/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-12 20:26:35,726] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f1c1bd9e6b0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-12 20:26:38,966] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-12 20:26:38,966] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-12 20:26:38,993] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.04812979698181152 seconds
[2025-01-12 20:26:39,261] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-12 20:26:39,261] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-12 20:26:39,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-12 20:26:39,264] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-12 20:26:39,270] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-12 20:26:39,270] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-12 20:26:39,270] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-12 20:26:39,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 20:26:39,271] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1c1a4bae30>
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-12 20:26:39,273] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train baseline
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4034304 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 4034273 got signal: 1
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-12 20:26:53,268] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f102c49e6b0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-12 20:26:57,045] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-12 20:26:57,045] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-12 20:26:57,074] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.04883146286010742 seconds
[2025-01-12 20:26:57,444] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-12 20:26:57,444] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-12 20:26:57,447] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-12 20:26:57,447] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-12 20:26:57,455] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-12 20:26:57,455] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-12 20:26:57,455] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-12 20:26:57,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 20:26:57,456] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1000e1ee30>
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-12 20:26:57,458] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train baseline
Epoch: [0]  [   0/2809]  eta: 13:01:14  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 16.6872  data: 5.4961  max mem: 15572
Epoch: [0]  [  10/2809]  eta: 1:28:17  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.8928  data: 0.5005  max mem: 15572
Epoch: [0]  [  20/2809]  eta: 0:56:19  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4379  data: 0.0009  max mem: 15572
Epoch: [0]  [  30/2809]  eta: 0:45:15  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4734  data: 0.0010  max mem: 15572
Epoch: [0]  [  40/2809]  eta: 0:39:27  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0011  max mem: 15572
Epoch: [0]  [  50/2809]  eta: 0:36:22  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0338  max mem: 15572
Epoch: [0]  [  60/2809]  eta: 0:34:38  lr: 0.000000  min_lr: 0.000000  loss: 5.1600 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0937  max mem: 15572
Epoch: [0]  [  70/2809]  eta: 0:34:07  lr: 0.000000  min_lr: 0.000000  loss: 5.1599 (5.1600)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1856  max mem: 15572
Epoch: [0]  [  80/2809]  eta: 0:33:12  lr: 0.000000  min_lr: 0.000000  loss: 5.1598 (5.1600)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.2154  max mem: 15572
Epoch: [0]  [  90/2809]  eta: 0:32:21  lr: 0.000000  min_lr: 0.000000  loss: 5.1597 (5.1599)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1495  max mem: 15572
Epoch: [0]  [ 100/2809]  eta: 0:31:43  lr: 0.000000  min_lr: 0.000000  loss: 5.1594 (5.1599)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1238  max mem: 15572
Epoch: [0]  [ 110/2809]  eta: 0:31:33  lr: 0.000000  min_lr: 0.000000  loss: 5.1590 (5.1598)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.1839  max mem: 15572
Epoch: [0]  [ 120/2809]  eta: 0:31:02  lr: 0.000000  min_lr: 0.000000  loss: 5.1588 (5.1597)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1963  max mem: 15572
[2025-01-12 20:28:25,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:28:25,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/2809]  eta: 0:30:17  lr: 0.000000  min_lr: 0.000000  loss: 5.1586 (5.1596)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1066  max mem: 15572
Epoch: [0]  [ 140/2809]  eta: 0:29:45  lr: 0.000000  min_lr: 0.000000  loss: 5.1583 (5.1595)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0836  max mem: 15572
Epoch: [0]  [ 150/2809]  eta: 0:29:47  lr: 0.000001  min_lr: 0.000000  loss: 5.1583 (5.1595)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1767  max mem: 15572
Epoch: [0]  [ 160/2809]  eta: 0:29:16  lr: 0.000001  min_lr: 0.000000  loss: 5.1584 (5.1594)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1713  max mem: 15572
Epoch: [0]  [ 170/2809]  eta: 0:29:06  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1593)  loss_scale: 256.0000 (160.1871)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1351  max mem: 15572
Epoch: [0]  [ 180/2809]  eta: 0:28:48  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1431  max mem: 15572
Epoch: [0]  [ 190/2809]  eta: 0:28:32  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  loss_scale: 256.0000 (170.2199)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1185  max mem: 15572
Epoch: [0]  [ 200/2809]  eta: 0:28:14  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1591)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1013  max mem: 15572
Epoch: [0]  [ 210/2809]  eta: 0:28:06  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1591)  loss_scale: 256.0000 (178.3507)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1259  max mem: 15572
Epoch: [0]  [ 220/2809]  eta: 0:27:50  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1590)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1210  max mem: 15572
Epoch: [0]  [ 230/2809]  eta: 0:27:37  lr: 0.000001  min_lr: 0.000000  loss: 5.1576 (5.1590)  loss_scale: 256.0000 (185.0736)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0623  max mem: 15572
Epoch: [0]  [ 240/2809]  eta: 0:27:23  lr: 0.000001  min_lr: 0.000000  loss: 5.1574 (5.1589)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0705  max mem: 15572
Epoch: [0]  [ 250/2809]  eta: 0:27:19  lr: 0.000001  min_lr: 0.000000  loss: 5.1574 (5.1588)  loss_scale: 256.0000 (190.7251)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1564  max mem: 15572
[2025-01-12 20:29:41,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:29:41,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [ 260/2809]  eta: 0:27:10  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1588)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.2117  max mem: 15572
Epoch: [0]  [ 270/2809]  eta: 0:26:57  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1587)  loss_scale: 512.0000 (209.7122)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1697  max mem: 15572
Epoch: [0]  [ 280/2809]  eta: 0:26:43  lr: 0.000001  min_lr: 0.000000  loss: 5.1571 (5.1587)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1330  max mem: 15572
Epoch: [0]  [ 290/2809]  eta: 0:26:32  lr: 0.000001  min_lr: 0.000000  loss: 5.1567 (5.1586)  loss_scale: 512.0000 (230.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1421  max mem: 15572
Epoch: [0]  [ 300/2809]  eta: 0:26:15  lr: 0.000001  min_lr: 0.000000  loss: 5.1563 (5.1585)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.1073  max mem: 15572
Epoch: [0]  [ 310/2809]  eta: 0:26:08  lr: 0.000001  min_lr: 0.000000  loss: 5.1563 (5.1584)  loss_scale: 512.0000 (248.5916)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1070  max mem: 15572
Epoch: [0]  [ 320/2809]  eta: 0:25:59  lr: 0.000001  min_lr: 0.000000  loss: 5.1562 (5.1584)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1476  max mem: 15572
Epoch: [0]  [ 330/2809]  eta: 0:25:45  lr: 0.000001  min_lr: 0.000000  loss: 5.1555 (5.1583)  loss_scale: 512.0000 (264.5076)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0822  max mem: 15572
Epoch: [0]  [ 340/2809]  eta: 0:25:43  lr: 0.000001  min_lr: 0.000000  loss: 5.1554 (5.1582)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1339  max mem: 15572
Epoch: [0]  [ 350/2809]  eta: 0:25:37  lr: 0.000001  min_lr: 0.000000  loss: 5.1553 (5.1581)  loss_scale: 512.0000 (278.6097)  weight_decay: 0.0500 (0.0500)  time: 0.6584  data: 0.2076  max mem: 15572
Epoch: [0]  [ 360/2809]  eta: 0:25:20  lr: 0.000001  min_lr: 0.000000  loss: 5.1548 (5.1580)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0894  max mem: 15572
Epoch: [0]  [ 370/2809]  eta: 0:25:15  lr: 0.000001  min_lr: 0.000000  loss: 5.1541 (5.1579)  loss_scale: 512.0000 (291.1914)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0978  max mem: 15572
Epoch: [0]  [ 380/2809]  eta: 0:25:07  lr: 0.000001  min_lr: 0.000000  loss: 5.1541 (5.1578)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1757  max mem: 15572
[2025-01-12 20:30:56,074] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:30:56,075] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [ 390/2809]  eta: 0:24:56  lr: 0.000001  min_lr: 0.000000  loss: 5.1535 (5.1577)  loss_scale: 512.0000 (311.6522)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1324  max mem: 15572
Epoch: [0]  [ 400/2809]  eta: 0:24:48  lr: 0.000001  min_lr: 0.000000  loss: 5.1520 (5.1576)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1183  max mem: 15572
Epoch: [0]  [ 410/2809]  eta: 0:24:40  lr: 0.000001  min_lr: 0.000000  loss: 5.1539 (5.1575)  loss_scale: 1024.0000 (346.3163)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1185  max mem: 15572
Epoch: [0]  [ 420/2809]  eta: 0:24:33  lr: 0.000001  min_lr: 0.000000  loss: 5.1532 (5.1573)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1199  max mem: 15572
Epoch: [0]  [ 430/2809]  eta: 0:24:28  lr: 0.000001  min_lr: 0.000000  loss: 5.1515 (5.1572)  loss_scale: 1024.0000 (377.7633)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1563  max mem: 15572
Epoch: [0]  [ 440/2809]  eta: 0:24:17  lr: 0.000001  min_lr: 0.000000  loss: 5.1515 (5.1571)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0907  max mem: 15572
Epoch: [0]  [ 450/2809]  eta: 0:24:09  lr: 0.000002  min_lr: 0.000000  loss: 5.1515 (5.1569)  loss_scale: 1024.0000 (406.4213)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0476  max mem: 15572
Epoch: [0]  [ 460/2809]  eta: 0:24:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1496 (5.1568)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1226  max mem: 15572
Epoch: [0]  [ 470/2809]  eta: 0:23:53  lr: 0.000002  min_lr: 0.000000  loss: 5.1490 (5.1566)  loss_scale: 1024.0000 (432.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1311  max mem: 15572
Epoch: [0]  [ 480/2809]  eta: 0:23:44  lr: 0.000002  min_lr: 0.000000  loss: 5.1484 (5.1564)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1097  max mem: 15572
Epoch: [0]  [ 490/2809]  eta: 0:23:38  lr: 0.000002  min_lr: 0.000000  loss: 5.1474 (5.1562)  loss_scale: 1024.0000 (456.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1478  max mem: 15572
Epoch: [0]  [ 500/2809]  eta: 0:23:31  lr: 0.000002  min_lr: 0.000000  loss: 5.1458 (5.1560)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1630  max mem: 15572
Epoch: [0]  [ 510/2809]  eta: 0:23:21  lr: 0.000002  min_lr: 0.000000  loss: 5.1456 (5.1558)  loss_scale: 1024.0000 (478.9354)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0952  max mem: 15572
[2025-01-12 20:32:10,207] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:32:10,208] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [0]  [ 520/2809]  eta: 0:23:15  lr: 0.000002  min_lr: 0.000000  loss: 5.1441 (5.1556)  loss_scale: 1024.0000 (507.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1014  max mem: 15572
Epoch: [0]  [ 530/2809]  eta: 0:23:12  lr: 0.000002  min_lr: 0.000000  loss: 5.1440 (5.1553)  loss_scale: 2048.0000 (536.1055)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.1862  max mem: 15572
Epoch: [0]  [ 540/2809]  eta: 0:23:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1419 (5.1551)  loss_scale: 2048.0000 (564.0518)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1556  max mem: 15572
Epoch: [0]  [ 550/2809]  eta: 0:22:56  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1549)  loss_scale: 2048.0000 (590.9837)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1249  max mem: 15572
Epoch: [0]  [ 560/2809]  eta: 0:22:50  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1547)  loss_scale: 2048.0000 (616.9554)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1623  max mem: 15572
Epoch: [0]  [ 570/2809]  eta: 0:22:46  lr: 0.000002  min_lr: 0.000000  loss: 5.1395 (5.1545)  loss_scale: 2048.0000 (642.0175)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.2005  max mem: 15572
Epoch: [0]  [ 580/2809]  eta: 0:22:37  lr: 0.000002  min_lr: 0.000000  loss: 5.1396 (5.1543)  loss_scale: 2048.0000 (666.2169)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1718  max mem: 15572
Epoch: [0]  [ 590/2809]  eta: 0:22:32  lr: 0.000002  min_lr: 0.000000  loss: 5.1376 (5.1540)  loss_scale: 2048.0000 (689.5973)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1316  max mem: 15572
Epoch: [0]  [ 600/2809]  eta: 0:22:25  lr: 0.000002  min_lr: 0.000000  loss: 5.1369 (5.1537)  loss_scale: 2048.0000 (712.1997)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1457  max mem: 15572
Epoch: [0]  [ 610/2809]  eta: 0:22:24  lr: 0.000002  min_lr: 0.000000  loss: 5.1369 (5.1535)  loss_scale: 2048.0000 (734.0622)  weight_decay: 0.0500 (0.0500)  time: 0.6594  data: 0.1998  max mem: 15572
Epoch: [0]  [ 620/2809]  eta: 0:22:14  lr: 0.000002  min_lr: 0.000000  loss: 5.1334 (5.1531)  loss_scale: 2048.0000 (755.2206)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1384  max mem: 15572
Epoch: [0]  [ 630/2809]  eta: 0:22:05  lr: 0.000002  min_lr: 0.000000  loss: 5.1308 (5.1529)  loss_scale: 2048.0000 (775.7084)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0321  max mem: 15572
[2025-01-12 20:33:27,259] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:33:27,259] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 640/2809]  eta: 0:21:57  lr: 0.000002  min_lr: 0.000000  loss: 5.1382 (5.1527)  loss_scale: 2048.0000 (798.7520)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0911  max mem: 15572
Epoch: [0]  [ 650/2809]  eta: 0:21:51  lr: 0.000002  min_lr: 0.000000  loss: 5.1335 (5.1523)  loss_scale: 4096.0000 (849.4009)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1509  max mem: 15572
Epoch: [0]  [ 660/2809]  eta: 0:21:45  lr: 0.000002  min_lr: 0.000000  loss: 5.1286 (5.1520)  loss_scale: 4096.0000 (898.5174)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1769  max mem: 15572
Epoch: [0]  [ 670/2809]  eta: 0:21:35  lr: 0.000002  min_lr: 0.000000  loss: 5.1264 (5.1516)  loss_scale: 4096.0000 (946.1699)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1142  max mem: 15572
Epoch: [0]  [ 680/2809]  eta: 0:21:26  lr: 0.000002  min_lr: 0.000000  loss: 5.1207 (5.1512)  loss_scale: 4096.0000 (992.4229)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0570  max mem: 15572
Epoch: [0]  [ 690/2809]  eta: 0:21:20  lr: 0.000002  min_lr: 0.000000  loss: 5.1207 (5.1509)  loss_scale: 4096.0000 (1037.3372)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0977  max mem: 15572
Epoch: [0]  [ 700/2809]  eta: 0:21:15  lr: 0.000002  min_lr: 0.000000  loss: 5.1347 (5.1506)  loss_scale: 4096.0000 (1080.9700)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1521  max mem: 15572
Epoch: [0]  [ 710/2809]  eta: 0:21:09  lr: 0.000002  min_lr: 0.000000  loss: 5.1334 (5.1503)  loss_scale: 4096.0000 (1123.3755)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1495  max mem: 15572
Epoch: [0]  [ 720/2809]  eta: 0:21:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1228 (5.1499)  loss_scale: 4096.0000 (1164.6047)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1094  max mem: 15572
Epoch: [0]  [ 730/2809]  eta: 0:21:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1228 (5.1495)  loss_scale: 4096.0000 (1204.7059)  weight_decay: 0.0500 (0.0500)  time: 0.7010  data: 0.1808  max mem: 15572
Epoch: [0]  [ 740/2809]  eta: 0:20:52  lr: 0.000002  min_lr: 0.000000  loss: 5.1246 (5.1492)  loss_scale: 4096.0000 (1243.7247)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.1330  max mem: 15572
Epoch: [0]  [ 750/2809]  eta: 0:20:42  lr: 0.000003  min_lr: 0.000000  loss: 5.1155 (5.1487)  loss_scale: 4096.0000 (1281.7044)  weight_decay: 0.0500 (0.0500)  time: 0.4649  data: 0.0011  max mem: 15572
Epoch: [0]  [ 760/2809]  eta: 0:20:32  lr: 0.000003  min_lr: 0.000000  loss: 5.1155 (5.1483)  loss_scale: 4096.0000 (1318.6859)  weight_decay: 0.0500 (0.0500)  time: 0.4742  data: 0.0292  max mem: 15572
[2025-01-12 20:34:38,978] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:34:38,979] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [0]  [ 770/2809]  eta: 0:20:21  lr: 0.000003  min_lr: 0.000000  loss: 5.1169 (5.1479)  loss_scale: 4096.0000 (1370.6459)  weight_decay: 0.0500 (0.0500)  time: 0.4246  data: 0.0287  max mem: 15572
Epoch: [0]  [ 780/2809]  eta: 0:20:12  lr: 0.000003  min_lr: 0.000000  loss: 5.1170 (5.1475)  loss_scale: 8192.0000 (1457.9872)  weight_decay: 0.0500 (0.0500)  time: 0.4316  data: 0.0006  max mem: 15572
Epoch: [0]  [ 790/2809]  eta: 0:20:05  lr: 0.000003  min_lr: 0.000000  loss: 5.1164 (5.1472)  loss_scale: 8192.0000 (1543.1201)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0595  max mem: 15572
Epoch: [0]  [ 800/2809]  eta: 0:20:01  lr: 0.000003  min_lr: 0.000000  loss: 5.1091 (5.1467)  loss_scale: 8192.0000 (1626.1273)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1679  max mem: 15572
Epoch: [0]  [ 810/2809]  eta: 0:19:56  lr: 0.000003  min_lr: 0.000000  loss: 5.1091 (5.1463)  loss_scale: 8192.0000 (1707.0875)  weight_decay: 0.0500 (0.0500)  time: 0.6633  data: 0.1806  max mem: 15572
Epoch: [0]  [ 820/2809]  eta: 0:19:53  lr: 0.000003  min_lr: 0.000000  loss: 5.1146 (5.1460)  loss_scale: 8192.0000 (1786.0755)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.1935  max mem: 15572
Epoch: [0]  [ 830/2809]  eta: 0:19:49  lr: 0.000003  min_lr: 0.000000  loss: 5.1068 (5.1455)  loss_scale: 8192.0000 (1863.1625)  weight_decay: 0.0500 (0.0500)  time: 0.7002  data: 0.2319  max mem: 15572
Epoch: [0]  [ 840/2809]  eta: 0:19:45  lr: 0.000003  min_lr: 0.000000  loss: 5.1024 (5.1450)  loss_scale: 8192.0000 (1938.4162)  weight_decay: 0.0500 (0.0500)  time: 0.6832  data: 0.1943  max mem: 15572
Epoch: [0]  [ 850/2809]  eta: 0:19:40  lr: 0.000003  min_lr: 0.000000  loss: 5.1057 (5.1446)  loss_scale: 8192.0000 (2011.9013)  weight_decay: 0.0500 (0.0500)  time: 0.6595  data: 0.1757  max mem: 15572
Epoch: [0]  [ 860/2809]  eta: 0:19:37  lr: 0.000003  min_lr: 0.000000  loss: 5.1015 (5.1442)  loss_scale: 8192.0000 (2083.6794)  weight_decay: 0.0500 (0.0500)  time: 0.6899  data: 0.1922  max mem: 15572
Epoch: [0]  [ 870/2809]  eta: 0:19:31  lr: 0.000003  min_lr: 0.000000  loss: 5.1044 (5.1439)  loss_scale: 8192.0000 (2153.8094)  weight_decay: 0.0500 (0.0500)  time: 0.6692  data: 0.1868  max mem: 15572
Epoch: [0]  [ 880/2809]  eta: 0:19:26  lr: 0.000003  min_lr: 0.000000  loss: 5.1025 (5.1433)  loss_scale: 8192.0000 (2222.3473)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.1949  max mem: 15572
Epoch: [0]  [ 890/2809]  eta: 0:19:22  lr: 0.000003  min_lr: 0.000000  loss: 5.0929 (5.1429)  loss_scale: 8192.0000 (2289.3468)  weight_decay: 0.0500 (0.0500)  time: 0.6834  data: 0.2097  max mem: 15572
[2025-01-12 20:36:00,639] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:36:00,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [0]  [ 900/2809]  eta: 0:19:18  lr: 0.000003  min_lr: 0.000000  loss: 5.1061 (5.1425)  loss_scale: 8192.0000 (2400.3196)  weight_decay: 0.0500 (0.0500)  time: 0.6882  data: 0.2100  max mem: 15572
Epoch: [0]  [ 910/2809]  eta: 0:19:08  lr: 0.000003  min_lr: 0.000000  loss: 5.1046 (5.1421)  loss_scale: 16384.0000 (2553.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1171  max mem: 15572
Epoch: [0]  [ 920/2809]  eta: 0:18:57  lr: 0.000003  min_lr: 0.000000  loss: 5.1040 (5.1418)  loss_scale: 16384.0000 (2703.9826)  weight_decay: 0.0500 (0.0500)  time: 0.3997  data: 0.0082  max mem: 15572
Epoch: [0]  [ 930/2809]  eta: 0:18:48  lr: 0.000003  min_lr: 0.000000  loss: 5.1048 (5.1414)  loss_scale: 16384.0000 (2850.9216)  weight_decay: 0.0500 (0.0500)  time: 0.4158  data: 0.0005  max mem: 15572
Epoch: [0]  [ 940/2809]  eta: 0:18:41  lr: 0.000003  min_lr: 0.000000  loss: 5.1055 (5.1410)  loss_scale: 16384.0000 (2994.7375)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0568  max mem: 15572
Epoch: [0]  [ 950/2809]  eta: 0:18:34  lr: 0.000003  min_lr: 0.000000  loss: 5.1055 (5.1407)  loss_scale: 16384.0000 (3135.5289)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1224  max mem: 15572
Epoch: [0]  [ 960/2809]  eta: 0:18:30  lr: 0.000003  min_lr: 0.000000  loss: 5.1052 (5.1402)  loss_scale: 16384.0000 (3273.3902)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1715  max mem: 15572
Epoch: [0]  [ 970/2809]  eta: 0:18:24  lr: 0.000003  min_lr: 0.000000  loss: 5.1052 (5.1399)  loss_scale: 16384.0000 (3408.4119)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1812  max mem: 15572
Epoch: [0]  [ 980/2809]  eta: 0:18:18  lr: 0.000003  min_lr: 0.000000  loss: 5.1015 (5.1394)  loss_scale: 16384.0000 (3540.6809)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1648  max mem: 15572
Epoch: [0]  [ 990/2809]  eta: 0:18:12  lr: 0.000003  min_lr: 0.000000  loss: 5.0866 (5.1387)  loss_scale: 16384.0000 (3670.2805)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1824  max mem: 15572
[2025-01-12 20:36:59,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[3.2306541515702744e-08, 3.2306541515702744e-08, 4.615220216528964e-08, 4.615220216528964e-08, 6.59317173789852e-08, 6.59317173789852e-08, 9.418816768426457e-08, 9.418816768426457e-08, 1.3455452526323513e-07, 1.3455452526323513e-07, 1.9222075037605018e-07, 1.9222075037605018e-07, 2.7460107196578597e-07, 2.7460107196578597e-07, 3.922872456654086e-07, 3.922872456654086e-07, 5.604103509505837e-07, 5.604103509505837e-07, 8.005862156436911e-07, 8.005862156436911e-07, 1.1436945937767016e-06, 1.1436945937767016e-06, 1.6338494196810024e-06, 1.6338494196810024e-06, 2.3340705995442894e-06, 2.3340705995442894e-06, 3.3343865707775563e-06, 3.3343865707775563e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 20:36:59,466] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=27.714561948353577, CurrSamplesPerSec=27.97767862354106, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [1000/2809]  eta: 0:18:07  lr: 0.000003  min_lr: 0.000000  loss: 5.0871 (5.1383)  loss_scale: 16384.0000 (3797.2907)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1917  max mem: 15572
Epoch: [0]  [1010/2809]  eta: 0:18:02  lr: 0.000003  min_lr: 0.000000  loss: 5.0940 (5.1380)  loss_scale: 16384.0000 (3921.7883)  weight_decay: 0.0500 (0.0500)  time: 0.6454  data: 0.2193  max mem: 15572
Epoch: [0]  [1020/2809]  eta: 0:17:53  lr: 0.000003  min_lr: 0.000000  loss: 5.0888 (5.1375)  loss_scale: 16384.0000 (4043.8472)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1347  max mem: 15572
[2025-01-12 20:37:12,666] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:37:12,667] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [0]  [1030/2809]  eta: 0:17:48  lr: 0.000003  min_lr: 0.000000  loss: 5.0857 (5.1370)  loss_scale: 16384.0000 (4274.7779)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0901  max mem: 15572
Epoch: [0]  [1040/2809]  eta: 0:17:42  lr: 0.000003  min_lr: 0.000000  loss: 5.0919 (5.1368)  loss_scale: 32768.0000 (4548.4880)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1481  max mem: 15572
Epoch: [0]  [1050/2809]  eta: 0:17:34  lr: 0.000004  min_lr: 0.000000  loss: 5.1028 (5.1364)  loss_scale: 32768.0000 (4816.9895)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0997  max mem: 15572
Epoch: [0]  [1060/2809]  eta: 0:17:30  lr: 0.000004  min_lr: 0.000000  loss: 5.0846 (5.1359)  loss_scale: 32768.0000 (5080.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1413  max mem: 15572
Epoch: [0]  [1070/2809]  eta: 0:17:24  lr: 0.000004  min_lr: 0.000000  loss: 5.0781 (5.1354)  loss_scale: 32768.0000 (5338.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.1842  max mem: 15572
Epoch: [0]  [1080/2809]  eta: 0:17:19  lr: 0.000004  min_lr: 0.000000  loss: 5.0781 (5.1348)  loss_scale: 32768.0000 (5592.6883)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.1699  max mem: 15572
Epoch: [0]  [1090/2809]  eta: 0:17:11  lr: 0.000004  min_lr: 0.000000  loss: 5.0771 (5.1344)  loss_scale: 32768.0000 (5841.7745)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1099  max mem: 15572
Epoch: [0]  [1100/2809]  eta: 0:17:04  lr: 0.000004  min_lr: 0.000000  loss: 5.0976 (5.1341)  loss_scale: 32768.0000 (6086.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0327  max mem: 15572
Epoch: [0]  [1110/2809]  eta: 0:16:58  lr: 0.000004  min_lr: 0.000000  loss: 5.0843 (5.1335)  loss_scale: 32768.0000 (6326.4950)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1071  max mem: 15572
Epoch: [0]  [1120/2809]  eta: 0:16:52  lr: 0.000004  min_lr: 0.000000  loss: 5.0730 (5.1331)  loss_scale: 32768.0000 (6562.3693)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1873  max mem: 15572
Epoch: [0]  [1130/2809]  eta: 0:16:47  lr: 0.000004  min_lr: 0.000000  loss: 5.0699 (5.1326)  loss_scale: 32768.0000 (6794.0725)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.2024  max mem: 15572
Epoch: [0]  [1140/2809]  eta: 0:16:40  lr: 0.000004  min_lr: 0.000000  loss: 5.0726 (5.1321)  loss_scale: 32768.0000 (7021.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1274  max mem: 15572
Epoch: [0]  [1150/2809]  eta: 0:16:34  lr: 0.000004  min_lr: 0.000000  loss: 5.0859 (5.1317)  loss_scale: 32768.0000 (7245.4005)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0838  max mem: 15572
[2025-01-12 20:38:28,751] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:38:28,751] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [0]  [1160/2809]  eta: 0:16:26  lr: 0.000004  min_lr: 0.000000  loss: 5.0859 (5.1313)  loss_scale: 32768.0000 (7719.2489)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0749  max mem: 15572
Epoch: [0]  [1170/2809]  eta: 0:16:20  lr: 0.000004  min_lr: 0.000000  loss: 5.0784 (5.1307)  loss_scale: 65536.0000 (8212.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.1044  max mem: 15572
Epoch: [0]  [1180/2809]  eta: 0:16:15  lr: 0.000004  min_lr: 0.000000  loss: 5.0701 (5.1303)  loss_scale: 65536.0000 (8698.3641)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1657  max mem: 15572
Epoch: [0]  [1190/2809]  eta: 0:16:09  lr: 0.000004  min_lr: 0.000000  loss: 5.0956 (5.1299)  loss_scale: 65536.0000 (9175.5903)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1430  max mem: 15572
Epoch: [0]  [1200/2809]  eta: 0:16:02  lr: 0.000004  min_lr: 0.000000  loss: 5.0828 (5.1295)  loss_scale: 65536.0000 (9644.8693)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0890  max mem: 15572
Epoch: [0]  [1210/2809]  eta: 0:15:55  lr: 0.000004  min_lr: 0.000000  loss: 5.0916 (5.1291)  loss_scale: 65536.0000 (10106.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0762  max mem: 15572
Epoch: [0]  [1220/2809]  eta: 0:15:49  lr: 0.000004  min_lr: 0.000000  loss: 5.0774 (5.1286)  loss_scale: 65536.0000 (10560.3669)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1127  max mem: 15572
Epoch: [0]  [1230/2809]  eta: 0:15:43  lr: 0.000004  min_lr: 0.000000  loss: 5.0682 (5.1283)  loss_scale: 65536.0000 (11006.9602)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1623  max mem: 15572
Epoch: [0]  [1240/2809]  eta: 0:15:36  lr: 0.000004  min_lr: 0.000000  loss: 5.0855 (5.1280)  loss_scale: 65536.0000 (11446.3562)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1469  max mem: 15572
Epoch: [0]  [1250/2809]  eta: 0:15:30  lr: 0.000004  min_lr: 0.000000  loss: 5.0822 (5.1276)  loss_scale: 65536.0000 (11878.7274)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1215  max mem: 15572
Epoch: [0]  [1260/2809]  eta: 0:15:23  lr: 0.000004  min_lr: 0.000000  loss: 5.0856 (5.1273)  loss_scale: 65536.0000 (12304.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1177  max mem: 15572
Epoch: [0]  [1270/2809]  eta: 0:15:18  lr: 0.000004  min_lr: 0.000000  loss: 5.0768 (5.1269)  loss_scale: 65536.0000 (12723.0590)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1170  max mem: 15572
[2025-01-12 20:39:43,078] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:39:43,078] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [0]  [1280/2809]  eta: 0:15:12  lr: 0.000004  min_lr: 0.000000  loss: 5.0672 (5.1266)  loss_scale: 65536.0000 (13186.4980)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.1635  max mem: 15572
Epoch: [0]  [1290/2809]  eta: 0:15:07  lr: 0.000004  min_lr: 0.000000  loss: 5.0922 (5.1264)  loss_scale: 131072.0000 (14099.6313)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1530  max mem: 15572
Epoch: [0]  [1300/2809]  eta: 0:15:00  lr: 0.000004  min_lr: 0.000000  loss: 5.0757 (5.1259)  loss_scale: 131072.0000 (14998.7271)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1137  max mem: 15572
Epoch: [0]  [1310/2809]  eta: 0:14:54  lr: 0.000004  min_lr: 0.000000  loss: 5.0489 (5.1253)  loss_scale: 131072.0000 (15884.1068)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1049  max mem: 15572
Epoch: [0]  [1320/2809]  eta: 0:14:49  lr: 0.000004  min_lr: 0.000000  loss: 5.0517 (5.1248)  loss_scale: 131072.0000 (16756.0818)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.1744  max mem: 15572
Epoch: [0]  [1330/2809]  eta: 0:14:42  lr: 0.000004  min_lr: 0.000000  loss: 5.0613 (5.1244)  loss_scale: 131072.0000 (17614.9542)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1519  max mem: 15572
Epoch: [0]  [1340/2809]  eta: 0:14:36  lr: 0.000004  min_lr: 0.000000  loss: 5.0613 (5.1239)  loss_scale: 131072.0000 (18461.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0975  max mem: 15572
Epoch: [0]  [1350/2809]  eta: 0:14:30  lr: 0.000005  min_lr: 0.000000  loss: 5.0655 (5.1235)  loss_scale: 131072.0000 (19294.5551)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1599  max mem: 15572
Epoch: [0]  [1360/2809]  eta: 0:14:24  lr: 0.000005  min_lr: 0.000000  loss: 5.0695 (5.1231)  loss_scale: 131072.0000 (20115.8442)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1708  max mem: 15572
Epoch: [0]  [1370/2809]  eta: 0:14:18  lr: 0.000005  min_lr: 0.000000  loss: 5.0580 (5.1226)  loss_scale: 131072.0000 (20925.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1208  max mem: 15572
Epoch: [0]  [1380/2809]  eta: 0:14:12  lr: 0.000005  min_lr: 0.000000  loss: 5.0384 (5.1220)  loss_scale: 131072.0000 (21722.7400)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1317  max mem: 15572
Epoch: [0]  [1390/2809]  eta: 0:14:06  lr: 0.000005  min_lr: 0.000000  loss: 5.0521 (5.1216)  loss_scale: 131072.0000 (22508.8598)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1653  max mem: 15572
Epoch: [0]  [1400/2809]  eta: 0:13:59  lr: 0.000005  min_lr: 0.000000  loss: 5.0647 (5.1213)  loss_scale: 131072.0000 (23283.7573)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1289  max mem: 15572
[2025-01-12 20:40:59,689] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:40:59,690] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
Epoch: [0]  [1410/2809]  eta: 0:13:55  lr: 0.000005  min_lr: 0.000000  loss: 5.0692 (5.1209)  loss_scale: 131072.0000 (24326.3501)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1936  max mem: 15572
Epoch: [0]  [1420/2809]  eta: 0:13:49  lr: 0.000005  min_lr: 0.000000  loss: 5.0708 (5.1204)  loss_scale: 262144.0000 (25999.9437)  weight_decay: 0.0500 (0.0500)  time: 0.6865  data: 0.2539  max mem: 15572
[2025-01-12 20:41:12,078] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1430
[2025-01-12 20:41:12,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
[2025-01-12 20:41:12,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144, reducing to 131072.0
Epoch: [0]  [1430/2809]  eta: 0:13:42  lr: 0.000005  min_lr: 0.000000  loss: 5.0685 (5.1199)  loss_scale: 262144.0000 (27558.5521)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1441  max mem: 15572
Epoch: [0]  [1440/2809]  eta: 0:13:36  lr: 0.000005  min_lr: 0.000000  loss: 5.0702 (5.1197)  loss_scale: 131072.0000 (28276.8966)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0818  max mem: 15572
Epoch: [0]  [1450/2809]  eta: 0:13:30  lr: 0.000005  min_lr: 0.000000  loss: 5.0620 (5.1192)  loss_scale: 131072.0000 (28985.3398)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1270  max mem: 15572
Epoch: [0]  [1460/2809]  eta: 0:13:24  lr: 0.000005  min_lr: 0.000000  loss: 5.0597 (5.1188)  loss_scale: 131072.0000 (29684.0849)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1772  max mem: 15572
Epoch: [0]  [1470/2809]  eta: 0:13:18  lr: 0.000005  min_lr: 0.000000  loss: 5.0780 (5.1186)  loss_scale: 131072.0000 (30373.3297)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1692  max mem: 15572
Epoch: [0]  [1480/2809]  eta: 0:13:12  lr: 0.000005  min_lr: 0.000000  loss: 5.0553 (5.1182)  loss_scale: 131072.0000 (31053.2667)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1697  max mem: 15572
Epoch: [0]  [1490/2809]  eta: 0:13:06  lr: 0.000005  min_lr: 0.000000  loss: 5.0651 (5.1178)  loss_scale: 131072.0000 (31724.0832)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1593  max mem: 15572
Epoch: [0]  [1500/2809]  eta: 0:13:01  lr: 0.000005  min_lr: 0.000000  loss: 5.0768 (5.1175)  loss_scale: 131072.0000 (32385.9614)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.1762  max mem: 15572
Epoch: [0]  [1510/2809]  eta: 0:12:54  lr: 0.000005  min_lr: 0.000000  loss: 5.0635 (5.1170)  loss_scale: 131072.0000 (33039.0788)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1507  max mem: 15572
Epoch: [0]  [1520/2809]  eta: 0:12:48  lr: 0.000005  min_lr: 0.000000  loss: 5.0512 (5.1166)  loss_scale: 131072.0000 (33683.6082)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1056  max mem: 15572
Epoch: [0]  [1530/2809]  eta: 0:12:42  lr: 0.000005  min_lr: 0.000000  loss: 5.0512 (5.1163)  loss_scale: 131072.0000 (34319.7178)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1467  max mem: 15572
Epoch: [0]  [1540/2809]  eta: 0:12:36  lr: 0.000005  min_lr: 0.000000  loss: 5.0472 (5.1159)  loss_scale: 131072.0000 (34947.5717)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1336  max mem: 15572
Epoch: [0]  [1550/2809]  eta: 0:12:30  lr: 0.000005  min_lr: 0.000000  loss: 5.0354 (5.1154)  loss_scale: 131072.0000 (35567.3295)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0890  max mem: 15572
[2025-01-12 20:42:28,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:42:28,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1560/2809]  eta: 0:12:24  lr: 0.000005  min_lr: 0.000000  loss: 5.0520 (5.1152)  loss_scale: 131072.0000 (36347.0801)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1016  max mem: 15572
Epoch: [0]  [1570/2809]  eta: 0:12:18  lr: 0.000005  min_lr: 0.000000  loss: 5.0554 (5.1148)  loss_scale: 262144.0000 (37784.3616)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1374  max mem: 15572
Epoch: [0]  [1580/2809]  eta: 0:12:12  lr: 0.000005  min_lr: 0.000000  loss: 5.0238 (5.1142)  loss_scale: 262144.0000 (39203.4611)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1190  max mem: 15572
Epoch: [0]  [1590/2809]  eta: 0:12:06  lr: 0.000005  min_lr: 0.000000  loss: 5.0186 (5.1136)  loss_scale: 262144.0000 (40604.7216)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1656  max mem: 15572
[2025-01-12 20:42:52,412] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1599
[2025-01-12 20:42:52,412] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:42:52,412] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1600/2809]  eta: 0:12:00  lr: 0.000005  min_lr: 0.000000  loss: 5.0195 (5.1131)  loss_scale: 262144.0000 (41824.7395)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1625  max mem: 15572
Epoch: [0]  [1610/2809]  eta: 0:11:54  lr: 0.000005  min_lr: 0.000000  loss: 5.0479 (5.1127)  loss_scale: 131072.0000 (42378.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0884  max mem: 15572
Epoch: [0]  [1620/2809]  eta: 0:11:48  lr: 0.000005  min_lr: 0.000000  loss: 5.0526 (5.1123)  loss_scale: 131072.0000 (42925.8779)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1208  max mem: 15572
Epoch: [0]  [1630/2809]  eta: 0:11:42  lr: 0.000005  min_lr: 0.000000  loss: 5.0335 (5.1119)  loss_scale: 131072.0000 (43466.3200)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1126  max mem: 15572
Epoch: [0]  [1640/2809]  eta: 0:11:36  lr: 0.000005  min_lr: 0.000000  loss: 5.0410 (5.1115)  loss_scale: 131072.0000 (44000.1755)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1159  max mem: 15572
Epoch: [0]  [1650/2809]  eta: 0:11:30  lr: 0.000006  min_lr: 0.000000  loss: 5.0617 (5.1111)  loss_scale: 131072.0000 (44527.5639)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1568  max mem: 15572
Epoch: [0]  [1660/2809]  eta: 0:11:24  lr: 0.000006  min_lr: 0.000000  loss: 5.0134 (5.1104)  loss_scale: 131072.0000 (45048.6020)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1240  max mem: 15572
Epoch: [0]  [1670/2809]  eta: 0:11:18  lr: 0.000006  min_lr: 0.000000  loss: 5.0045 (5.1100)  loss_scale: 131072.0000 (45563.4039)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1084  max mem: 15572
Epoch: [0]  [1680/2809]  eta: 0:11:12  lr: 0.000006  min_lr: 0.000000  loss: 5.0359 (5.1098)  loss_scale: 131072.0000 (46072.0809)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1420  max mem: 15572
Epoch: [0]  [1690/2809]  eta: 0:11:06  lr: 0.000006  min_lr: 0.000000  loss: 5.0668 (5.1095)  loss_scale: 131072.0000 (46574.7416)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1668  max mem: 15572
Epoch: [0]  [1700/2809]  eta: 0:11:00  lr: 0.000006  min_lr: 0.000000  loss: 5.0668 (5.1094)  loss_scale: 131072.0000 (47071.4921)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1539  max mem: 15572
Epoch: [0]  [1710/2809]  eta: 0:10:54  lr: 0.000006  min_lr: 0.000000  loss: 5.0553 (5.1089)  loss_scale: 131072.0000 (47562.4360)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1643  max mem: 15572
Epoch: [0]  [1720/2809]  eta: 0:10:48  lr: 0.000006  min_lr: 0.000000  loss: 5.0238 (5.1084)  loss_scale: 131072.0000 (48047.6746)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1228  max mem: 15572
[2025-01-12 20:44:08,659] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:44:08,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1730/2809]  eta: 0:10:42  lr: 0.000006  min_lr: 0.000000  loss: 5.0297 (5.1080)  loss_scale: 131072.0000 (48754.4679)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1307  max mem: 15572
Epoch: [0]  [1740/2809]  eta: 0:10:36  lr: 0.000006  min_lr: 0.000000  loss: 5.0292 (5.1075)  loss_scale: 262144.0000 (49980.1401)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1672  max mem: 15572
Epoch: [0]  [1750/2809]  eta: 0:10:30  lr: 0.000006  min_lr: 0.000000  loss: 5.0127 (5.1070)  loss_scale: 262144.0000 (51191.8127)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0942  max mem: 15572
Epoch: [0]  [1760/2809]  eta: 0:10:23  lr: 0.000006  min_lr: 0.000000  loss: 5.0273 (5.1067)  loss_scale: 262144.0000 (52389.7240)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0310  max mem: 15572
Epoch: [0]  [1770/2809]  eta: 0:10:17  lr: 0.000006  min_lr: 0.000000  loss: 5.0398 (5.1062)  loss_scale: 262144.0000 (53574.1073)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0488  max mem: 15572
[2025-01-12 20:44:33,265] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1776
[2025-01-12 20:44:33,266] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:44:33,266] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1780/2809]  eta: 0:10:11  lr: 0.000006  min_lr: 0.000000  loss: 5.0520 (5.1059)  loss_scale: 262144.0000 (54377.2173)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1422  max mem: 15572
Epoch: [0]  [1790/2809]  eta: 0:10:06  lr: 0.000006  min_lr: 0.000000  loss: 5.0756 (5.1056)  loss_scale: 131072.0000 (54805.4405)  weight_decay: 0.0500 (0.0500)  time: 0.6736  data: 0.2419  max mem: 15572
Epoch: [0]  [1800/2809]  eta: 0:10:00  lr: 0.000006  min_lr: 0.000000  loss: 5.0756 (5.1054)  loss_scale: 131072.0000 (55228.9084)  weight_decay: 0.0500 (0.0500)  time: 0.6589  data: 0.2152  max mem: 15572
Epoch: [0]  [1810/2809]  eta: 0:09:54  lr: 0.000006  min_lr: 0.000000  loss: 5.0073 (5.1048)  loss_scale: 131072.0000 (55647.6996)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1233  max mem: 15572
Epoch: [0]  [1820/2809]  eta: 0:09:48  lr: 0.000006  min_lr: 0.000000  loss: 5.0470 (5.1046)  loss_scale: 131072.0000 (56061.8913)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1282  max mem: 15572
Epoch: [0]  [1830/2809]  eta: 0:09:42  lr: 0.000006  min_lr: 0.000000  loss: 5.0751 (5.1045)  loss_scale: 131072.0000 (56471.5587)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1243  max mem: 15572
Epoch: [0]  [1840/2809]  eta: 0:09:35  lr: 0.000006  min_lr: 0.000000  loss: 5.0669 (5.1042)  loss_scale: 131072.0000 (56876.7757)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0853  max mem: 15572
Epoch: [0]  [1850/2809]  eta: 0:09:29  lr: 0.000006  min_lr: 0.000000  loss: 5.0295 (5.1039)  loss_scale: 131072.0000 (57277.6143)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0967  max mem: 15572
Epoch: [0]  [1860/2809]  eta: 0:09:24  lr: 0.000006  min_lr: 0.000000  loss: 5.0140 (5.1034)  loss_scale: 131072.0000 (57674.1451)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1806  max mem: 15572
Epoch: [0]  [1870/2809]  eta: 0:09:18  lr: 0.000006  min_lr: 0.000000  loss: 5.0332 (5.1030)  loss_scale: 131072.0000 (58066.4372)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1577  max mem: 15572
Epoch: [0]  [1880/2809]  eta: 0:09:11  lr: 0.000006  min_lr: 0.000000  loss: 5.0503 (5.1027)  loss_scale: 131072.0000 (58454.5582)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0723  max mem: 15572
Epoch: [0]  [1890/2809]  eta: 0:09:05  lr: 0.000006  min_lr: 0.000000  loss: 5.0270 (5.1022)  loss_scale: 131072.0000 (58838.5743)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1006  max mem: 15572
Epoch: [0]  [1900/2809]  eta: 0:09:00  lr: 0.000006  min_lr: 0.000000  loss: 5.0328 (5.1021)  loss_scale: 131072.0000 (59218.5502)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1547  max mem: 15572
[2025-01-12 20:45:50,656] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:45:50,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-12 20:45:54,893] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1909
[2025-01-12 20:45:54,893] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:45:54,894] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1910/2809]  eta: 0:08:54  lr: 0.000006  min_lr: 0.000000  loss: 5.0328 (5.1017)  loss_scale: 131072.0000 (59868.9021)  weight_decay: 0.0500 (0.0500)  time: 0.6821  data: 0.2104  max mem: 15572
Epoch: [0]  [1920/2809]  eta: 0:08:48  lr: 0.000006  min_lr: 0.000000  loss: 5.0367 (5.1014)  loss_scale: 131072.0000 (60239.5586)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1941  max mem: 15572
Epoch: [0]  [1930/2809]  eta: 0:08:42  lr: 0.000006  min_lr: 0.000000  loss: 4.9997 (5.1008)  loss_scale: 131072.0000 (60606.3760)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1288  max mem: 15572
Epoch: [0]  [1940/2809]  eta: 0:08:36  lr: 0.000006  min_lr: 0.000000  loss: 4.9997 (5.1004)  loss_scale: 131072.0000 (60969.4137)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1372  max mem: 15572
Epoch: [0]  [1950/2809]  eta: 0:08:30  lr: 0.000007  min_lr: 0.000000  loss: 5.0086 (5.1000)  loss_scale: 131072.0000 (61328.7299)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0819  max mem: 15572
Epoch: [0]  [1960/2809]  eta: 0:08:24  lr: 0.000007  min_lr: 0.000000  loss: 4.9853 (5.0996)  loss_scale: 131072.0000 (61684.3814)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.0502  max mem: 15572
Epoch: [0]  [1970/2809]  eta: 0:08:18  lr: 0.000007  min_lr: 0.000000  loss: 4.9785 (5.0991)  loss_scale: 131072.0000 (62036.4242)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1788  max mem: 15572
Epoch: [0]  [1980/2809]  eta: 0:08:13  lr: 0.000007  min_lr: 0.000000  loss: 5.0137 (5.0989)  loss_scale: 131072.0000 (62384.9127)  weight_decay: 0.0500 (0.0500)  time: 0.6915  data: 0.2507  max mem: 15572
Epoch: [0]  [1990/2809]  eta: 0:08:07  lr: 0.000007  min_lr: 0.000000  loss: 5.0183 (5.0985)  loss_scale: 131072.0000 (62729.9006)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1573  max mem: 15572
[2025-01-12 20:46:48,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=4, lr=[6.464542191180159e-08, 6.464542191180159e-08, 9.235060273114513e-08, 9.235060273114513e-08, 1.3192943247306448e-07, 1.3192943247306448e-07, 1.8847061781866357e-07, 1.8847061781866357e-07, 2.6924373974094795e-07, 2.6924373974094795e-07, 3.8463391391563995e-07, 3.8463391391563995e-07, 5.494770198794857e-07, 5.494770198794857e-07, 7.849671712564082e-07, 7.849671712564082e-07, 1.1213816732234404e-06, 1.1213816732234404e-06, 1.6019738188906293e-06, 1.6019738188906293e-06, 2.288534026986613e-06, 2.288534026986613e-06, 3.2693343242665907e-06, 3.2693343242665907e-06, 4.670477606095129e-06, 4.670477606095129e-06, 6.6721108658501856e-06, 6.6721108658501856e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 20:46:48,011] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=27.87239847586939, CurrSamplesPerSec=23.329024889813017, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [2000/2809]  eta: 0:08:01  lr: 0.000007  min_lr: 0.000000  loss: 5.0114 (5.0981)  loss_scale: 131072.0000 (63071.4403)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0979  max mem: 15572
Epoch: [0]  [2010/2809]  eta: 0:07:54  lr: 0.000007  min_lr: 0.000000  loss: 4.9854 (5.0975)  loss_scale: 131072.0000 (63409.5833)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1121  max mem: 15572
Epoch: [0]  [2020/2809]  eta: 0:07:48  lr: 0.000007  min_lr: 0.000000  loss: 4.9888 (5.0973)  loss_scale: 131072.0000 (63744.3800)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1064  max mem: 15572
Epoch: [0]  [2030/2809]  eta: 0:07:42  lr: 0.000007  min_lr: 0.000000  loss: 5.0313 (5.0971)  loss_scale: 131072.0000 (64075.8799)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1278  max mem: 15572
[2025-01-12 20:47:10,678] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:47:10,678] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2040/2809]  eta: 0:07:37  lr: 0.000007  min_lr: 0.000000  loss: 4.9939 (5.0964)  loss_scale: 131072.0000 (64596.7898)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1411  max mem: 15572
Epoch: [0]  [2050/2809]  eta: 0:07:31  lr: 0.000007  min_lr: 0.000000  loss: 4.9703 (5.0960)  loss_scale: 262144.0000 (65559.9649)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1564  max mem: 15572
Epoch: [0]  [2060/2809]  eta: 0:07:25  lr: 0.000007  min_lr: 0.000000  loss: 5.0084 (5.0957)  loss_scale: 262144.0000 (66513.7933)  weight_decay: 0.0500 (0.0500)  time: 0.6808  data: 0.2500  max mem: 15572
Epoch: [0]  [2070/2809]  eta: 0:07:19  lr: 0.000007  min_lr: 0.000000  loss: 5.0252 (5.0954)  loss_scale: 262144.0000 (67458.4104)  weight_decay: 0.0500 (0.0500)  time: 0.6669  data: 0.2414  max mem: 15572
Epoch: [0]  [2080/2809]  eta: 0:07:13  lr: 0.000007  min_lr: 0.000000  loss: 5.0044 (5.0949)  loss_scale: 262144.0000 (68393.9491)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1128  max mem: 15572
[2025-01-12 20:47:39,112] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2086
[2025-01-12 20:47:39,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:47:39,113] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2090/2809]  eta: 0:07:07  lr: 0.000007  min_lr: 0.000000  loss: 4.9915 (5.0944)  loss_scale: 262144.0000 (69007.1200)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1028  max mem: 15572
Epoch: [0]  [2100/2809]  eta: 0:07:01  lr: 0.000007  min_lr: 0.000000  loss: 5.0105 (5.0942)  loss_scale: 131072.0000 (69302.5264)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1147  max mem: 15572
Epoch: [0]  [2110/2809]  eta: 0:06:55  lr: 0.000007  min_lr: 0.000000  loss: 5.0079 (5.0937)  loss_scale: 131072.0000 (69595.1341)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1583  max mem: 15572
Epoch: [0]  [2120/2809]  eta: 0:06:49  lr: 0.000007  min_lr: 0.000000  loss: 4.9933 (5.0933)  loss_scale: 131072.0000 (69884.9826)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1399  max mem: 15572
Epoch: [0]  [2130/2809]  eta: 0:06:43  lr: 0.000007  min_lr: 0.000000  loss: 4.9997 (5.0930)  loss_scale: 131072.0000 (70172.1107)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1104  max mem: 15572
Epoch: [0]  [2140/2809]  eta: 0:06:37  lr: 0.000007  min_lr: 0.000000  loss: 5.0188 (5.0929)  loss_scale: 131072.0000 (70456.5567)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1376  max mem: 15572
Epoch: [0]  [2150/2809]  eta: 0:06:31  lr: 0.000007  min_lr: 0.000000  loss: 5.0121 (5.0926)  loss_scale: 131072.0000 (70738.3580)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1146  max mem: 15572
Epoch: [0]  [2160/2809]  eta: 0:06:25  lr: 0.000007  min_lr: 0.000000  loss: 5.0001 (5.0922)  loss_scale: 131072.0000 (71017.5511)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0826  max mem: 15572
Epoch: [0]  [2170/2809]  eta: 0:06:19  lr: 0.000007  min_lr: 0.000000  loss: 5.0220 (5.0919)  loss_scale: 131072.0000 (71294.1723)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0594  max mem: 15572
Epoch: [0]  [2180/2809]  eta: 0:06:13  lr: 0.000007  min_lr: 0.000000  loss: 5.0319 (5.0918)  loss_scale: 131072.0000 (71568.2568)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0858  max mem: 15572
Epoch: [0]  [2190/2809]  eta: 0:06:07  lr: 0.000007  min_lr: 0.000000  loss: 5.0153 (5.0913)  loss_scale: 131072.0000 (71839.8393)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0977  max mem: 15572
Epoch: [0]  [2200/2809]  eta: 0:06:01  lr: 0.000007  min_lr: 0.000000  loss: 5.0022 (5.0910)  loss_scale: 131072.0000 (72108.9541)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0639  max mem: 15572
Epoch: [0]  [2210/2809]  eta: 0:05:55  lr: 0.000007  min_lr: 0.000000  loss: 5.0012 (5.0904)  loss_scale: 131072.0000 (72375.6346)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1260  max mem: 15572
[2025-01-12 20:48:52,997] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:48:52,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2220/2809]  eta: 0:05:49  lr: 0.000007  min_lr: 0.000000  loss: 4.9886 (5.0900)  loss_scale: 131072.0000 (72994.0027)  weight_decay: 0.0500 (0.0500)  time: 0.6476  data: 0.2006  max mem: 15572
Epoch: [0]  [2230/2809]  eta: 0:05:43  lr: 0.000007  min_lr: 0.000000  loss: 4.9899 (5.0894)  loss_scale: 262144.0000 (73841.8288)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0972  max mem: 15572
Epoch: [0]  [2240/2809]  eta: 0:05:37  lr: 0.000007  min_lr: 0.000000  loss: 5.0007 (5.0892)  loss_scale: 262144.0000 (74682.0884)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1404  max mem: 15572
Epoch: [0]  [2250/2809]  eta: 0:05:32  lr: 0.000008  min_lr: 0.000000  loss: 5.0239 (5.0888)  loss_scale: 262144.0000 (75514.8823)  weight_decay: 0.0500 (0.0500)  time: 0.6713  data: 0.2018  max mem: 15572
Epoch: [0]  [2260/2809]  eta: 0:05:26  lr: 0.000008  min_lr: 0.000000  loss: 5.0211 (5.0886)  loss_scale: 262144.0000 (76340.3096)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1253  max mem: 15572
[2025-01-12 20:49:22,684] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2263
[2025-01-12 20:49:22,684] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:49:22,684] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2270/2809]  eta: 0:05:20  lr: 0.000008  min_lr: 0.000000  loss: 4.9858 (5.0880)  loss_scale: 262144.0000 (76696.7433)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1251  max mem: 15572
Epoch: [0]  [2280/2809]  eta: 0:05:14  lr: 0.000008  min_lr: 0.000000  loss: 4.9672 (5.0876)  loss_scale: 131072.0000 (76935.1267)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1398  max mem: 15572
Epoch: [0]  [2290/2809]  eta: 0:05:08  lr: 0.000008  min_lr: 0.000000  loss: 5.0193 (5.0872)  loss_scale: 131072.0000 (77171.4291)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1562  max mem: 15572
Epoch: [0]  [2300/2809]  eta: 0:05:02  lr: 0.000008  min_lr: 0.000000  loss: 4.9886 (5.0867)  loss_scale: 131072.0000 (77405.6775)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1309  max mem: 15572
Epoch: [0]  [2310/2809]  eta: 0:04:56  lr: 0.000008  min_lr: 0.000000  loss: 4.9738 (5.0864)  loss_scale: 131072.0000 (77637.8987)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1047  max mem: 15572
Epoch: [0]  [2320/2809]  eta: 0:04:50  lr: 0.000008  min_lr: 0.000000  loss: 4.9738 (5.0861)  loss_scale: 131072.0000 (77868.1189)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1246  max mem: 15572
Epoch: [0]  [2330/2809]  eta: 0:04:44  lr: 0.000008  min_lr: 0.000000  loss: 4.9869 (5.0857)  loss_scale: 131072.0000 (78096.3638)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1630  max mem: 15572
Epoch: [0]  [2340/2809]  eta: 0:04:38  lr: 0.000008  min_lr: 0.000000  loss: 5.0125 (5.0856)  loss_scale: 131072.0000 (78322.6587)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.2041  max mem: 15572
Epoch: [0]  [2350/2809]  eta: 0:04:32  lr: 0.000008  min_lr: 0.000000  loss: 5.0449 (5.0854)  loss_scale: 131072.0000 (78547.0285)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1501  max mem: 15572
Epoch: [0]  [2360/2809]  eta: 0:04:26  lr: 0.000008  min_lr: 0.000000  loss: 5.0186 (5.0851)  loss_scale: 131072.0000 (78769.4977)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0853  max mem: 15572
Epoch: [0]  [2370/2809]  eta: 0:04:20  lr: 0.000008  min_lr: 0.000000  loss: 5.0500 (5.0852)  loss_scale: 131072.0000 (78990.0903)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1412  max mem: 15572
Epoch: [0]  [2380/2809]  eta: 0:04:14  lr: 0.000008  min_lr: 0.000000  loss: 5.0313 (5.0848)  loss_scale: 131072.0000 (79208.8299)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1601  max mem: 15572
Epoch: [0]  [2390/2809]  eta: 0:04:08  lr: 0.000008  min_lr: 0.000000  loss: 5.0198 (5.0846)  loss_scale: 131072.0000 (79425.7399)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1116  max mem: 15572
[2025-01-12 20:50:39,079] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:50:39,079] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-12 20:50:43,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2399
[2025-01-12 20:50:43,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:50:43,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2400/2809]  eta: 0:04:02  lr: 0.000008  min_lr: 0.000000  loss: 5.0043 (5.0841)  loss_scale: 131072.0000 (80022.9771)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1259  max mem: 15572
Epoch: [0]  [2410/2809]  eta: 0:03:56  lr: 0.000008  min_lr: 0.000000  loss: 4.9684 (5.0835)  loss_scale: 131072.0000 (80234.7109)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1700  max mem: 15572
Epoch: [0]  [2420/2809]  eta: 0:03:50  lr: 0.000008  min_lr: 0.000000  loss: 4.9712 (5.0834)  loss_scale: 131072.0000 (80444.6956)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1257  max mem: 15572
Epoch: [0]  [2430/2809]  eta: 0:03:44  lr: 0.000008  min_lr: 0.000000  loss: 5.0326 (5.0833)  loss_scale: 131072.0000 (80652.9527)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0893  max mem: 15572
Epoch: [0]  [2440/2809]  eta: 0:03:38  lr: 0.000008  min_lr: 0.000000  loss: 5.0418 (5.0831)  loss_scale: 131072.0000 (80859.5035)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1331  max mem: 15572
Epoch: [0]  [2450/2809]  eta: 0:03:32  lr: 0.000008  min_lr: 0.000000  loss: 5.0418 (5.0828)  loss_scale: 131072.0000 (81064.3688)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1229  max mem: 15572
Epoch: [0]  [2460/2809]  eta: 0:03:26  lr: 0.000008  min_lr: 0.000000  loss: 5.0174 (5.0826)  loss_scale: 131072.0000 (81267.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1359  max mem: 15572
Epoch: [0]  [2470/2809]  eta: 0:03:21  lr: 0.000008  min_lr: 0.000000  loss: 5.0133 (5.0823)  loss_scale: 131072.0000 (81469.1251)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1573  max mem: 15572
Epoch: [0]  [2480/2809]  eta: 0:03:15  lr: 0.000008  min_lr: 0.000000  loss: 4.9949 (5.0819)  loss_scale: 131072.0000 (81669.0560)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1018  max mem: 15572
Epoch: [0]  [2490/2809]  eta: 0:03:09  lr: 0.000008  min_lr: 0.000000  loss: 4.9702 (5.0815)  loss_scale: 131072.0000 (81867.3818)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0725  max mem: 15572
Epoch: [0]  [2500/2809]  eta: 0:03:03  lr: 0.000008  min_lr: 0.000000  loss: 5.0015 (5.0812)  loss_scale: 131072.0000 (82064.1216)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1654  max mem: 15572
Epoch: [0]  [2510/2809]  eta: 0:02:57  lr: 0.000008  min_lr: 0.000000  loss: 4.9884 (5.0809)  loss_scale: 131072.0000 (82259.2943)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.2026  max mem: 15572
Epoch: [0]  [2520/2809]  eta: 0:02:51  lr: 0.000008  min_lr: 0.000000  loss: 4.9615 (5.0804)  loss_scale: 131072.0000 (82452.9187)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1597  max mem: 15572
[2025-01-12 20:51:58,231] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:51:58,231] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2530/2809]  eta: 0:02:45  lr: 0.000008  min_lr: 0.000000  loss: 4.9685 (5.0801)  loss_scale: 131072.0000 (82800.3730)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0838  max mem: 15572
Epoch: [0]  [2540/2809]  eta: 0:02:39  lr: 0.000008  min_lr: 0.000000  loss: 5.0214 (5.0798)  loss_scale: 262144.0000 (83506.1724)  weight_decay: 0.0500 (0.0500)  time: 0.4768  data: 0.0350  max mem: 15572
Epoch: [0]  [2550/2809]  eta: 0:02:33  lr: 0.000009  min_lr: 0.000000  loss: 5.0318 (5.0798)  loss_scale: 262144.0000 (84206.4383)  weight_decay: 0.0500 (0.0500)  time: 0.4903  data: 0.0648  max mem: 15572
Epoch: [0]  [2560/2809]  eta: 0:02:27  lr: 0.000009  min_lr: 0.000000  loss: 4.9903 (5.0794)  loss_scale: 262144.0000 (84901.2355)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1137  max mem: 15572
[2025-01-12 20:52:17,282] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2565
[2025-01-12 20:52:17,283] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:52:17,283] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2570/2809]  eta: 0:02:21  lr: 0.000009  min_lr: 0.000000  loss: 4.9903 (5.0792)  loss_scale: 262144.0000 (85284.7421)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1319  max mem: 15572
Epoch: [0]  [2580/2809]  eta: 0:02:15  lr: 0.000009  min_lr: 0.000000  loss: 4.9850 (5.0788)  loss_scale: 131072.0000 (85462.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1465  max mem: 15572
Epoch: [0]  [2590/2809]  eta: 0:02:09  lr: 0.000009  min_lr: 0.000000  loss: 4.9850 (5.0786)  loss_scale: 131072.0000 (85638.1752)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.1874  max mem: 15572
Epoch: [0]  [2600/2809]  eta: 0:02:03  lr: 0.000009  min_lr: 0.000000  loss: 4.9907 (5.0780)  loss_scale: 131072.0000 (85812.8535)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1332  max mem: 15572
Epoch: [0]  [2610/2809]  eta: 0:01:57  lr: 0.000009  min_lr: 0.000000  loss: 4.9891 (5.0776)  loss_scale: 131072.0000 (85986.1938)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1512  max mem: 15572
Epoch: [0]  [2620/2809]  eta: 0:01:51  lr: 0.000009  min_lr: 0.000000  loss: 5.0390 (5.0776)  loss_scale: 131072.0000 (86158.2114)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1473  max mem: 15572
Epoch: [0]  [2630/2809]  eta: 0:01:45  lr: 0.000009  min_lr: 0.000000  loss: 5.0639 (5.0774)  loss_scale: 131072.0000 (86328.9213)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1120  max mem: 15572
Epoch: [0]  [2640/2809]  eta: 0:01:40  lr: 0.000009  min_lr: 0.000000  loss: 4.9870 (5.0772)  loss_scale: 131072.0000 (86498.3385)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1342  max mem: 15572
Epoch: [0]  [2650/2809]  eta: 0:01:34  lr: 0.000009  min_lr: 0.000000  loss: 5.0347 (5.0771)  loss_scale: 131072.0000 (86666.4776)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1733  max mem: 15572
Epoch: [0]  [2660/2809]  eta: 0:01:28  lr: 0.000009  min_lr: 0.000000  loss: 5.0035 (5.0768)  loss_scale: 131072.0000 (86833.3529)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1869  max mem: 15572
Epoch: [0]  [2670/2809]  eta: 0:01:22  lr: 0.000009  min_lr: 0.000000  loss: 4.9875 (5.0765)  loss_scale: 131072.0000 (86998.9787)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1524  max mem: 15572
Epoch: [0]  [2680/2809]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000000  loss: 4.9701 (5.0761)  loss_scale: 131072.0000 (87163.3689)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1147  max mem: 15572
Epoch: [0]  [2690/2809]  eta: 0:01:10  lr: 0.000009  min_lr: 0.000000  loss: 4.9704 (5.0759)  loss_scale: 131072.0000 (87326.5373)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1658  max mem: 15572
[2025-01-12 20:53:35,199] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:53:35,200] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2700/2809]  eta: 0:01:04  lr: 0.000009  min_lr: 0.000000  loss: 5.0239 (5.0758)  loss_scale: 131072.0000 (87828.1881)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.1881  max mem: 15572
Epoch: [0]  [2710/2809]  eta: 0:00:58  lr: 0.000009  min_lr: 0.000000  loss: 5.0170 (5.0756)  loss_scale: 262144.0000 (88471.1826)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0972  max mem: 15572
Epoch: [0]  [2720/2809]  eta: 0:00:52  lr: 0.000009  min_lr: 0.000000  loss: 5.0047 (5.0755)  loss_scale: 262144.0000 (89109.4509)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1152  max mem: 15572
Epoch: [0]  [2730/2809]  eta: 0:00:46  lr: 0.000009  min_lr: 0.000000  loss: 4.9830 (5.0752)  loss_scale: 262144.0000 (89743.0450)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1354  max mem: 15572
Epoch: [0]  [2740/2809]  eta: 0:00:40  lr: 0.000009  min_lr: 0.000000  loss: 4.9566 (5.0748)  loss_scale: 262144.0000 (90372.0161)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1326  max mem: 15572
[2025-01-12 20:54:06,652] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2750
[2025-01-12 20:54:06,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:54:06,653] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2750/2809]  eta: 0:00:34  lr: 0.000009  min_lr: 0.000000  loss: 4.9600 (5.0745)  loss_scale: 262144.0000 (90948.7692)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0859  max mem: 15572
Epoch: [0]  [2760/2809]  eta: 0:00:29  lr: 0.000009  min_lr: 0.000000  loss: 5.0024 (5.0742)  loss_scale: 131072.0000 (91094.0905)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1087  max mem: 15572
Epoch: [0]  [2770/2809]  eta: 0:00:23  lr: 0.000009  min_lr: 0.000000  loss: 4.9535 (5.0738)  loss_scale: 131072.0000 (91238.3630)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.1647  max mem: 15572
Epoch: [0]  [2780/2809]  eta: 0:00:17  lr: 0.000009  min_lr: 0.000000  loss: 5.0012 (5.0736)  loss_scale: 131072.0000 (91381.5980)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1100  max mem: 15572
Epoch: [0]  [2790/2809]  eta: 0:00:11  lr: 0.000009  min_lr: 0.000000  loss: 4.9633 (5.0731)  loss_scale: 131072.0000 (91523.8065)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1508  max mem: 15572
Epoch: [0]  [2800/2809]  eta: 0:00:05  lr: 0.000009  min_lr: 0.000000  loss: 4.9705 (5.0731)  loss_scale: 131072.0000 (91664.9996)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.2025  max mem: 15572
Epoch: [0]  [2808/2809]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 5.0420 (5.0730)  loss_scale: 131072.0000 (91777.2303)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0957  max mem: 15572
Epoch: [0] Total time: 0:27:43 (0.5922 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 5.0420 (5.0730)  loss_scale: 131072.0000 (91777.2303)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:26:37  loss: 4.9922 (4.9922)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 5.8724  data: 5.4049  max mem: 15572
Val:  [ 10/272]  eta: 0:03:33  loss: 5.1506 (5.0917)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.8131  data: 0.5724  max mem: 15572
Val:  [ 20/272]  eta: 0:02:26  loss: 5.1836 (5.0610)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3147  data: 0.0988  max mem: 15572
Val:  [ 30/272]  eta: 0:01:51  loss: 5.1020 (5.0353)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2689  data: 0.0648  max mem: 15572
Val:  [ 40/272]  eta: 0:01:40  loss: 4.8739 (4.9999)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2762  data: 0.0739  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 4.8283 (5.0215)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3410  data: 0.1366  max mem: 15572
Val:  [ 60/272]  eta: 0:01:25  loss: 4.8307 (5.0150)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3469  data: 0.1458  max mem: 15572
Val:  [ 70/272]  eta: 0:01:19  loss: 4.8320 (4.9990)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (4.8513)  time: 0.3374  data: 0.1459  max mem: 15572
Val:  [ 80/272]  eta: 0:01:14  loss: 4.9648 (4.9579)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (9.1221)  time: 0.3371  data: 0.1409  max mem: 15572
Val:  [ 90/272]  eta: 0:01:09  loss: 5.0204 (4.9828)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (8.1197)  time: 0.3426  data: 0.1380  max mem: 15572
Val:  [100/272]  eta: 0:01:04  loss: 5.1758 (5.0141)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (7.3157)  time: 0.3381  data: 0.1325  max mem: 15572
Val:  [110/272]  eta: 0:01:00  loss: 5.2500 (5.0352)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (6.6567)  time: 0.3421  data: 0.1330  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 5.1777 (5.0578)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (6.1065)  time: 0.3192  data: 0.1179  max mem: 15572
Val:  [130/272]  eta: 0:00:52  loss: 5.1120 (5.0564)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (7.5912)  time: 0.3322  data: 0.1307  max mem: 15572
Val:  [140/272]  eta: 0:00:48  loss: 4.8459 (5.0422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (9.0623)  time: 0.3384  data: 0.1431  max mem: 15572
Val:  [150/272]  eta: 0:00:44  loss: 4.8459 (5.0327)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (8.4621)  time: 0.3119  data: 0.1086  max mem: 15572
Val:  [160/272]  eta: 0:00:40  loss: 4.6328 (5.0027)  acc1: 0.0000 (1.8634)  acc5: 0.0000 (9.7999)  time: 0.3280  data: 0.1236  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 4.6250 (5.0105)  acc1: 0.0000 (1.9493)  acc5: 0.0000 (9.4217)  time: 0.3338  data: 0.1337  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 5.1155 (5.0132)  acc1: 0.0000 (1.8416)  acc5: 0.0000 (8.9012)  time: 0.3320  data: 0.1298  max mem: 15572
Val:  [190/272]  eta: 0:00:29  loss: 5.1745 (5.0205)  acc1: 0.0000 (1.7452)  acc5: 0.0000 (8.4351)  time: 0.3501  data: 0.1520  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 5.0760 (5.0293)  acc1: 0.0000 (1.6584)  acc5: 0.0000 (8.0155)  time: 0.3403  data: 0.1345  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 5.0757 (5.0345)  acc1: 0.0000 (1.5798)  acc5: 0.0000 (7.6356)  time: 0.3235  data: 0.1278  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 4.8477 (5.0207)  acc1: 0.0000 (1.5083)  acc5: 0.0000 (7.2901)  time: 0.3259  data: 0.1406  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.7385 (5.0101)  acc1: 0.0000 (1.4430)  acc5: 0.0000 (6.9745)  time: 0.3088  data: 0.1132  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 4.7630 (5.0087)  acc1: 0.0000 (1.3831)  acc5: 0.0000 (6.6851)  time: 0.3007  data: 0.1019  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 5.2500 (5.0253)  acc1: 0.0000 (1.3280)  acc5: 0.0000 (6.4188)  time: 0.3341  data: 0.1394  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 5.1847 (5.0217)  acc1: 0.0000 (1.2771)  acc5: 0.0000 (6.1728)  time: 0.2919  data: 0.1096  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 5.0894 (5.0235)  acc1: 0.0000 (1.2300)  acc5: 0.0000 (5.9451)  time: 0.1989  data: 0.0386  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 5.0948 (5.0255)  acc1: 0.0000 (1.2288)  acc5: 0.0000 (5.9390)  time: 0.1939  data: 0.0386  max mem: 15572
Val: Total time: 0:01:31 (0.3381 s / it)
* Acc@1 1.229 Acc@5 5.939 loss 5.025
Accuracy of the network on the 4883 val videos: 1.2%
[2025-01-12 20:56:13,078] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2025-01-12 20:56:13,083] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 20:56:13,083] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 20:56:13,603] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 20:56:13,604] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 1.23%
Epoch: [1]  [   0/2809]  eta: 7:10:05  lr: 0.000009  min_lr: 0.000000  loss: 4.8862 (4.8862)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 9.1867  data: 8.7039  max mem: 15572
Epoch: [1]  [  10/2809]  eta: 1:03:31  lr: 0.000009  min_lr: 0.000000  loss: 4.9537 (4.9949)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3618  data: 0.9331  max mem: 15572
Epoch: [1]  [  20/2809]  eta: 0:46:42  lr: 0.000009  min_lr: 0.000000  loss: 5.0245 (5.0164)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1552  max mem: 15572
Epoch: [1]  [  30/2809]  eta: 0:41:00  lr: 0.000009  min_lr: 0.000000  loss: 5.0256 (5.0102)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1677  max mem: 15572
Epoch: [1]  [  40/2809]  eta: 0:37:12  lr: 0.000010  min_lr: 0.000000  loss: 4.9364 (4.9983)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1551  max mem: 15572
Epoch: [1]  [  50/2809]  eta: 0:34:58  lr: 0.000010  min_lr: 0.000000  loss: 4.9784 (5.0038)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1357  max mem: 15572
Epoch: [1]  [  60/2809]  eta: 0:32:54  lr: 0.000010  min_lr: 0.000000  loss: 4.9952 (5.0033)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.1195  max mem: 15572
[2025-01-12 20:57:02,971] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:57:02,971] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [  70/2809]  eta: 0:31:43  lr: 0.000010  min_lr: 0.000000  loss: 4.9961 (5.0075)  loss_scale: 131072.0000 (132918.0845)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.1050  max mem: 15572
Epoch: [1]  [  80/2809]  eta: 0:30:57  lr: 0.000010  min_lr: 0.000000  loss: 5.0365 (5.0107)  loss_scale: 262144.0000 (148871.9012)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1312  max mem: 15572
Epoch: [1]  [  90/2809]  eta: 0:30:26  lr: 0.000010  min_lr: 0.000000  loss: 4.9996 (5.0053)  loss_scale: 262144.0000 (161319.3846)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1469  max mem: 15572
Epoch: [1]  [ 100/2809]  eta: 0:30:10  lr: 0.000010  min_lr: 0.000000  loss: 4.9581 (5.0023)  loss_scale: 262144.0000 (171302.0198)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1603  max mem: 15572
[2025-01-12 20:57:25,401] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2916
[2025-01-12 20:57:25,401] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:57:25,401] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 110/2809]  eta: 0:29:41  lr: 0.000010  min_lr: 0.000000  loss: 4.9358 (4.9972)  loss_scale: 262144.0000 (174762.6667)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1472  max mem: 15572
Epoch: [1]  [ 120/2809]  eta: 0:29:37  lr: 0.000010  min_lr: 0.000000  loss: 4.9595 (4.9947)  loss_scale: 131072.0000 (171151.8678)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1715  max mem: 15572
Epoch: [1]  [ 130/2809]  eta: 0:29:09  lr: 0.000010  min_lr: 0.000000  loss: 4.9894 (4.9951)  loss_scale: 131072.0000 (168092.3359)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1806  max mem: 15572
Epoch: [1]  [ 140/2809]  eta: 0:28:52  lr: 0.000010  min_lr: 0.000000  loss: 5.0020 (5.0010)  loss_scale: 131072.0000 (165466.7801)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1325  max mem: 15572
Epoch: [1]  [ 150/2809]  eta: 0:28:38  lr: 0.000010  min_lr: 0.000000  loss: 5.0018 (4.9999)  loss_scale: 131072.0000 (163188.9801)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1436  max mem: 15572
Epoch: [1]  [ 160/2809]  eta: 0:28:25  lr: 0.000010  min_lr: 0.000000  loss: 4.9573 (4.9968)  loss_scale: 131072.0000 (161194.1366)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1619  max mem: 15572
Epoch: [1]  [ 170/2809]  eta: 0:28:14  lr: 0.000010  min_lr: 0.000000  loss: 4.9466 (4.9949)  loss_scale: 131072.0000 (159432.6082)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1671  max mem: 15572
Epoch: [1]  [ 180/2809]  eta: 0:28:08  lr: 0.000010  min_lr: 0.000000  loss: 5.0301 (4.9993)  loss_scale: 131072.0000 (157865.7238)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1660  max mem: 15572
[2025-01-12 20:58:15,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=10, lr=[9.698430230790043e-08, 9.698430230790043e-08, 1.3854900329700063e-07, 1.3854900329700063e-07, 1.9792714756714377e-07, 1.9792714756714377e-07, 2.8275306795306254e-07, 2.8275306795306254e-07, 4.0393295421866083e-07, 4.0393295421866083e-07, 5.770470774552297e-07, 5.770470774552297e-07, 8.243529677931854e-07, 8.243529677931854e-07, 1.1776470968474079e-06, 1.1776470968474079e-06, 1.682352995496297e-06, 1.682352995496297e-06, 2.4033614221375676e-06, 2.4033614221375676e-06, 3.4333734601965247e-06, 3.4333734601965247e-06, 4.9048192288521786e-06, 4.9048192288521786e-06, 7.0068846126459705e-06, 7.0068846126459705e-06, 1.0009835160922815e-05, 1.0009835160922815e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 20:58:15,964] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=27.940445903034206, CurrSamplesPerSec=31.495133213230652, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [ 190/2809]  eta: 0:27:57  lr: 0.000010  min_lr: 0.000000  loss: 5.0354 (4.9997)  loss_scale: 131072.0000 (156462.9110)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1314  max mem: 15572
Epoch: [1]  [ 200/2809]  eta: 0:27:41  lr: 0.000010  min_lr: 0.000000  loss: 4.9873 (4.9998)  loss_scale: 131072.0000 (155199.6816)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0953  max mem: 15572
Epoch: [1]  [ 210/2809]  eta: 0:27:31  lr: 0.000010  min_lr: 0.000000  loss: 4.9873 (4.9965)  loss_scale: 131072.0000 (154056.1896)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1227  max mem: 15572
Epoch: [1]  [ 220/2809]  eta: 0:27:17  lr: 0.000010  min_lr: 0.000000  loss: 4.9731 (4.9941)  loss_scale: 131072.0000 (153016.1810)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1375  max mem: 15572
Epoch: [1]  [ 230/2809]  eta: 0:27:14  lr: 0.000010  min_lr: 0.000000  loss: 4.9888 (4.9956)  loss_scale: 131072.0000 (152066.2165)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1557  max mem: 15572
[2025-01-12 20:58:43,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:58:43,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 240/2809]  eta: 0:27:02  lr: 0.000010  min_lr: 0.000000  loss: 4.9923 (4.9966)  loss_scale: 131072.0000 (153914.4232)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1405  max mem: 15572
Epoch: [1]  [ 250/2809]  eta: 0:26:43  lr: 0.000010  min_lr: 0.000000  loss: 5.0413 (4.9993)  loss_scale: 262144.0000 (158226.3586)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0693  max mem: 15572
Epoch: [1]  [ 260/2809]  eta: 0:26:43  lr: 0.000010  min_lr: 0.000000  loss: 4.9883 (4.9990)  loss_scale: 262144.0000 (162207.8774)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1363  max mem: 15572
Epoch: [1]  [ 270/2809]  eta: 0:26:24  lr: 0.000010  min_lr: 0.000000  loss: 4.9802 (5.0000)  loss_scale: 262144.0000 (165895.5572)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1291  max mem: 15572
[2025-01-12 20:59:06,409] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3084
[2025-01-12 20:59:06,410] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:59:06,411] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 280/2809]  eta: 0:26:17  lr: 0.000010  min_lr: 0.000000  loss: 5.0546 (5.0022)  loss_scale: 262144.0000 (166522.0783)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0878  max mem: 15572
Epoch: [1]  [ 290/2809]  eta: 0:26:07  lr: 0.000010  min_lr: 0.000000  loss: 5.0647 (5.0041)  loss_scale: 131072.0000 (165303.8625)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1462  max mem: 15572
Epoch: [1]  [ 300/2809]  eta: 0:25:58  lr: 0.000010  min_lr: 0.000000  loss: 5.0291 (5.0049)  loss_scale: 131072.0000 (164166.5914)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1466  max mem: 15572
Epoch: [1]  [ 310/2809]  eta: 0:25:45  lr: 0.000010  min_lr: 0.000000  loss: 4.9723 (5.0040)  loss_scale: 131072.0000 (163102.4566)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1052  max mem: 15572
Epoch: [1]  [ 320/2809]  eta: 0:25:33  lr: 0.000010  min_lr: 0.000000  loss: 4.9245 (5.0017)  loss_scale: 131072.0000 (162104.6231)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0778  max mem: 15572
Epoch: [1]  [ 330/2809]  eta: 0:25:29  lr: 0.000010  min_lr: 0.000000  loss: 4.8942 (5.0002)  loss_scale: 131072.0000 (161167.0816)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1151  max mem: 15572
Epoch: [1]  [ 340/2809]  eta: 0:25:24  lr: 0.000011  min_lr: 0.000000  loss: 4.9083 (4.9975)  loss_scale: 131072.0000 (160284.5279)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1329  max mem: 15572
Epoch: [1]  [ 350/2809]  eta: 0:25:18  lr: 0.000011  min_lr: 0.000000  loss: 4.9450 (4.9963)  loss_scale: 131072.0000 (159452.2621)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.1203  max mem: 15572
Epoch: [1]  [ 360/2809]  eta: 0:25:04  lr: 0.000011  min_lr: 0.000000  loss: 4.9546 (4.9968)  loss_scale: 131072.0000 (158666.1053)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0877  max mem: 15572
Epoch: [1]  [ 370/2809]  eta: 0:24:52  lr: 0.000011  min_lr: 0.000000  loss: 4.9727 (4.9959)  loss_scale: 131072.0000 (157922.3288)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0667  max mem: 15572
Epoch: [1]  [ 380/2809]  eta: 0:24:46  lr: 0.000011  min_lr: 0.000000  loss: 4.9719 (4.9965)  loss_scale: 131072.0000 (157217.5958)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1410  max mem: 15572
Epoch: [1]  [ 390/2809]  eta: 0:24:40  lr: 0.000011  min_lr: 0.000000  loss: 5.0120 (4.9967)  loss_scale: 131072.0000 (156548.9105)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1938  max mem: 15572
Epoch: [1]  [ 400/2809]  eta: 0:24:32  lr: 0.000011  min_lr: 0.000000  loss: 5.0018 (4.9959)  loss_scale: 131072.0000 (155913.5761)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1630  max mem: 15572
[2025-01-12 21:00:21,963] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:00:21,963] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 410/2809]  eta: 0:24:24  lr: 0.000011  min_lr: 0.000000  loss: 4.9906 (4.9958)  loss_scale: 131072.0000 (157541.5280)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1464  max mem: 15572
[2025-01-12 21:00:26,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3220
[2025-01-12 21:00:26,482] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:00:26,482] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 420/2809]  eta: 0:24:18  lr: 0.000011  min_lr: 0.000000  loss: 5.0235 (4.9959)  loss_scale: 131072.0000 (156912.7981)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1391  max mem: 15572
Epoch: [1]  [ 430/2809]  eta: 0:24:06  lr: 0.000011  min_lr: 0.000000  loss: 5.0095 (4.9954)  loss_scale: 131072.0000 (156313.2436)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0883  max mem: 15572
Epoch: [1]  [ 440/2809]  eta: 0:24:02  lr: 0.000011  min_lr: 0.000000  loss: 4.9743 (4.9946)  loss_scale: 131072.0000 (155740.8798)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1008  max mem: 15572
Epoch: [1]  [ 450/2809]  eta: 0:23:52  lr: 0.000011  min_lr: 0.000000  loss: 4.9470 (4.9936)  loss_scale: 131072.0000 (155193.8980)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1286  max mem: 15572
Epoch: [1]  [ 460/2809]  eta: 0:23:50  lr: 0.000011  min_lr: 0.000000  loss: 4.9470 (4.9938)  loss_scale: 131072.0000 (154670.6464)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1620  max mem: 15572
Epoch: [1]  [ 470/2809]  eta: 0:23:42  lr: 0.000011  min_lr: 0.000000  loss: 4.9925 (4.9946)  loss_scale: 131072.0000 (154169.6136)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1779  max mem: 15572
Epoch: [1]  [ 480/2809]  eta: 0:23:34  lr: 0.000011  min_lr: 0.000000  loss: 5.0116 (4.9946)  loss_scale: 131072.0000 (153689.4137)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1344  max mem: 15572
Epoch: [1]  [ 490/2809]  eta: 0:23:26  lr: 0.000011  min_lr: 0.000000  loss: 4.9737 (4.9938)  loss_scale: 131072.0000 (153228.7739)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1383  max mem: 15572
Epoch: [1]  [ 500/2809]  eta: 0:23:19  lr: 0.000011  min_lr: 0.000000  loss: 4.9396 (4.9938)  loss_scale: 131072.0000 (152786.5230)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1583  max mem: 15572
Epoch: [1]  [ 510/2809]  eta: 0:23:15  lr: 0.000011  min_lr: 0.000000  loss: 4.9434 (4.9925)  loss_scale: 131072.0000 (152361.5812)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1752  max mem: 15572
Epoch: [1]  [ 520/2809]  eta: 0:23:07  lr: 0.000011  min_lr: 0.000000  loss: 4.9867 (4.9935)  loss_scale: 131072.0000 (151952.9520)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1323  max mem: 15572
Epoch: [1]  [ 530/2809]  eta: 0:23:01  lr: 0.000011  min_lr: 0.000000  loss: 5.0357 (4.9937)  loss_scale: 131072.0000 (151559.7137)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1120  max mem: 15572
[2025-01-12 21:01:40,880] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:01:40,881] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 540/2809]  eta: 0:22:51  lr: 0.000011  min_lr: 0.000000  loss: 5.0357 (4.9952)  loss_scale: 131072.0000 (151423.2902)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1069  max mem: 15572
Epoch: [1]  [ 550/2809]  eta: 0:22:42  lr: 0.000011  min_lr: 0.000000  loss: 5.0675 (4.9961)  loss_scale: 262144.0000 (153432.7405)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0674  max mem: 15572
[2025-01-12 21:01:53,005] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3369
[2025-01-12 21:01:53,006] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:01:53,006] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 560/2809]  eta: 0:22:39  lr: 0.000011  min_lr: 0.000000  loss: 5.0528 (4.9968)  loss_scale: 262144.0000 (155136.9127)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1433  max mem: 15572
Epoch: [1]  [ 570/2809]  eta: 0:22:30  lr: 0.000011  min_lr: 0.000000  loss: 5.0045 (4.9961)  loss_scale: 131072.0000 (154715.4606)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1518  max mem: 15572
Epoch: [1]  [ 580/2809]  eta: 0:22:26  lr: 0.000011  min_lr: 0.000000  loss: 5.0087 (4.9969)  loss_scale: 131072.0000 (154308.5164)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1288  max mem: 15572
Epoch: [1]  [ 590/2809]  eta: 0:22:22  lr: 0.000011  min_lr: 0.000000  loss: 5.0087 (4.9968)  loss_scale: 131072.0000 (153915.3435)  weight_decay: 0.0500 (0.0500)  time: 0.6602  data: 0.2041  max mem: 15572
Epoch: [1]  [ 600/2809]  eta: 0:22:15  lr: 0.000011  min_lr: 0.000000  loss: 4.9798 (4.9968)  loss_scale: 131072.0000 (153535.2546)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1820  max mem: 15572
Epoch: [1]  [ 610/2809]  eta: 0:22:09  lr: 0.000011  min_lr: 0.000000  loss: 5.0114 (4.9972)  loss_scale: 131072.0000 (153167.6072)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1534  max mem: 15572
Epoch: [1]  [ 620/2809]  eta: 0:22:02  lr: 0.000011  min_lr: 0.000000  loss: 5.0114 (4.9967)  loss_scale: 131072.0000 (152811.8003)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1429  max mem: 15572
Epoch: [1]  [ 630/2809]  eta: 0:21:58  lr: 0.000011  min_lr: 0.000000  loss: 5.0298 (4.9976)  loss_scale: 131072.0000 (152467.2710)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1684  max mem: 15572
Epoch: [1]  [ 640/2809]  eta: 0:21:50  lr: 0.000012  min_lr: 0.000000  loss: 5.0298 (4.9985)  loss_scale: 131072.0000 (152133.4914)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1288  max mem: 15572
Epoch: [1]  [ 650/2809]  eta: 0:21:43  lr: 0.000012  min_lr: 0.000000  loss: 5.0424 (4.9994)  loss_scale: 131072.0000 (151809.9662)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0610  max mem: 15572
Epoch: [1]  [ 660/2809]  eta: 0:21:36  lr: 0.000012  min_lr: 0.000000  loss: 5.0207 (4.9996)  loss_scale: 131072.0000 (151496.2300)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0758  max mem: 15572
Epoch: [1]  [ 670/2809]  eta: 0:21:30  lr: 0.000012  min_lr: 0.000000  loss: 4.9962 (5.0007)  loss_scale: 131072.0000 (151191.8450)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1021  max mem: 15572
Epoch: [1]  [ 680/2809]  eta: 0:21:23  lr: 0.000012  min_lr: 0.000000  loss: 5.0603 (5.0016)  loss_scale: 131072.0000 (150896.3994)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1306  max mem: 15572
[2025-01-12 21:03:08,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:03:08,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 690/2809]  eta: 0:21:14  lr: 0.000012  min_lr: 0.000000  loss: 5.0687 (5.0027)  loss_scale: 131072.0000 (150988.8741)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0798  max mem: 15572
[2025-01-12 21:03:09,903] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3500
[2025-01-12 21:03:09,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:03:09,904] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-01-12 21:03:12,057] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3503
[2025-01-12 21:03:12,057] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:03:12,057] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [ 700/2809]  eta: 0:21:05  lr: 0.000012  min_lr: 0.000000  loss: 4.9768 (5.0019)  loss_scale: 131072.0000 (150050.3281)  weight_decay: 0.0500 (0.0500)  time: 0.5176  data: 0.0607  max mem: 15572
Epoch: [1]  [ 710/2809]  eta: 0:20:53  lr: 0.000012  min_lr: 0.000000  loss: 4.9768 (5.0022)  loss_scale: 65536.0000 (148861.6596)  weight_decay: 0.0500 (0.0500)  time: 0.4649  data: 0.0445  max mem: 15572
Epoch: [1]  [ 720/2809]  eta: 0:20:45  lr: 0.000012  min_lr: 0.000000  loss: 4.9902 (5.0015)  loss_scale: 65536.0000 (147705.9639)  weight_decay: 0.0500 (0.0500)  time: 0.4474  data: 0.0011  max mem: 15572
Epoch: [1]  [ 730/2809]  eta: 0:20:35  lr: 0.000012  min_lr: 0.000000  loss: 4.9634 (5.0019)  loss_scale: 65536.0000 (146581.8878)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0013  max mem: 15572
Epoch: [1]  [ 740/2809]  eta: 0:20:25  lr: 0.000012  min_lr: 0.000000  loss: 4.9552 (5.0011)  loss_scale: 65536.0000 (145488.1511)  weight_decay: 0.0500 (0.0500)  time: 0.4665  data: 0.0009  max mem: 15572
Epoch: [1]  [ 750/2809]  eta: 0:20:20  lr: 0.000012  min_lr: 0.000000  loss: 4.9283 (5.0004)  loss_scale: 65536.0000 (144423.5419)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0589  max mem: 15572
Epoch: [1]  [ 760/2809]  eta: 0:20:16  lr: 0.000012  min_lr: 0.000000  loss: 4.9603 (5.0008)  loss_scale: 65536.0000 (143386.9120)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.1384  max mem: 15572
Epoch: [1]  [ 770/2809]  eta: 0:20:15  lr: 0.000012  min_lr: 0.000000  loss: 4.9921 (5.0001)  loss_scale: 65536.0000 (142377.1725)  weight_decay: 0.0500 (0.0500)  time: 0.7163  data: 0.2231  max mem: 15572
Epoch: [1]  [ 780/2809]  eta: 0:20:11  lr: 0.000012  min_lr: 0.000000  loss: 4.9785 (5.0000)  loss_scale: 65536.0000 (141393.2907)  weight_decay: 0.0500 (0.0500)  time: 0.7307  data: 0.2474  max mem: 15572
Epoch: [1]  [ 790/2809]  eta: 0:20:09  lr: 0.000012  min_lr: 0.000000  loss: 4.9424 (4.9993)  loss_scale: 65536.0000 (140434.2857)  weight_decay: 0.0500 (0.0500)  time: 0.7159  data: 0.2249  max mem: 15572
Epoch: [1]  [ 800/2809]  eta: 0:20:05  lr: 0.000012  min_lr: 0.000000  loss: 4.9774 (4.9993)  loss_scale: 65536.0000 (139499.2260)  weight_decay: 0.0500 (0.0500)  time: 0.7122  data: 0.2178  max mem: 15572
Epoch: [1]  [ 810/2809]  eta: 0:20:00  lr: 0.000012  min_lr: 0.000000  loss: 5.0090 (4.9998)  loss_scale: 65536.0000 (138587.2256)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.1779  max mem: 15572
Epoch: [1]  [ 820/2809]  eta: 0:19:56  lr: 0.000012  min_lr: 0.000000  loss: 5.0156 (5.0001)  loss_scale: 65536.0000 (137697.4421)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.1733  max mem: 15572
[2025-01-12 21:04:29,705] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:04:29,705] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [ 830/2809]  eta: 0:19:51  lr: 0.000012  min_lr: 0.000000  loss: 5.0141 (5.0004)  loss_scale: 65536.0000 (137459.9856)  weight_decay: 0.0500 (0.0500)  time: 0.6784  data: 0.1931  max mem: 15572
Epoch: [1]  [ 840/2809]  eta: 0:19:47  lr: 0.000012  min_lr: 0.000000  loss: 5.0072 (5.0007)  loss_scale: 131072.0000 (137384.0285)  weight_decay: 0.0500 (0.0500)  time: 0.6808  data: 0.2157  max mem: 15572
Epoch: [1]  [ 850/2809]  eta: 0:19:41  lr: 0.000012  min_lr: 0.000000  loss: 5.0431 (5.0012)  loss_scale: 131072.0000 (137309.8566)  weight_decay: 0.0500 (0.0500)  time: 0.6500  data: 0.2058  max mem: 15572
Epoch: [1]  [ 860/2809]  eta: 0:19:31  lr: 0.000012  min_lr: 0.000000  loss: 5.0151 (5.0006)  loss_scale: 131072.0000 (137237.4077)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0909  max mem: 15572
Epoch: [1]  [ 870/2809]  eta: 0:19:22  lr: 0.000012  min_lr: 0.000000  loss: 4.9353 (5.0006)  loss_scale: 131072.0000 (137166.6223)  weight_decay: 0.0500 (0.0500)  time: 0.4408  data: 0.0024  max mem: 15572
Epoch: [1]  [ 880/2809]  eta: 0:19:12  lr: 0.000012  min_lr: 0.000000  loss: 4.9321 (5.0004)  loss_scale: 131072.0000 (137097.4438)  weight_decay: 0.0500 (0.0500)  time: 0.4382  data: 0.0026  max mem: 15572
Epoch: [1]  [ 890/2809]  eta: 0:19:05  lr: 0.000012  min_lr: 0.000000  loss: 4.9883 (5.0007)  loss_scale: 131072.0000 (137029.8182)  weight_decay: 0.0500 (0.0500)  time: 0.4778  data: 0.0436  max mem: 15572
Epoch: [1]  [ 900/2809]  eta: 0:18:59  lr: 0.000012  min_lr: 0.000000  loss: 5.0071 (5.0008)  loss_scale: 131072.0000 (136963.6937)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1245  max mem: 15572
Epoch: [1]  [ 910/2809]  eta: 0:18:53  lr: 0.000012  min_lr: 0.000000  loss: 5.0133 (5.0014)  loss_scale: 131072.0000 (136899.0209)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1586  max mem: 15572
Epoch: [1]  [ 920/2809]  eta: 0:18:47  lr: 0.000012  min_lr: 0.000000  loss: 4.9953 (5.0014)  loss_scale: 131072.0000 (136835.7524)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1703  max mem: 15572
Epoch: [1]  [ 930/2809]  eta: 0:18:41  lr: 0.000012  min_lr: 0.000000  loss: 5.0012 (5.0017)  loss_scale: 131072.0000 (136773.8432)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1703  max mem: 15572
Epoch: [1]  [ 940/2809]  eta: 0:18:35  lr: 0.000013  min_lr: 0.000000  loss: 5.0077 (5.0015)  loss_scale: 131072.0000 (136713.2497)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1427  max mem: 15572
Epoch: [1]  [ 950/2809]  eta: 0:18:30  lr: 0.000013  min_lr: 0.000000  loss: 4.9236 (5.0005)  loss_scale: 131072.0000 (136653.9306)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1746  max mem: 15572
[2025-01-12 21:05:42,525] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:05:42,525] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 960/2809]  eta: 0:18:25  lr: 0.000013  min_lr: 0.000000  loss: 4.9377 (5.0008)  loss_scale: 131072.0000 (137959.7586)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.2084  max mem: 15572
Epoch: [1]  [ 970/2809]  eta: 0:18:21  lr: 0.000013  min_lr: 0.000000  loss: 4.9631 (5.0011)  loss_scale: 262144.0000 (139238.6900)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.2171  max mem: 15572
Epoch: [1]  [ 980/2809]  eta: 0:18:13  lr: 0.000013  min_lr: 0.000000  loss: 4.9497 (5.0006)  loss_scale: 262144.0000 (140491.5474)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1730  max mem: 15572
Epoch: [1]  [ 990/2809]  eta: 0:18:06  lr: 0.000013  min_lr: 0.000000  loss: 4.9084 (5.0002)  loss_scale: 262144.0000 (141719.1201)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.1140  max mem: 15572
Epoch: [1]  [1000/2809]  eta: 0:18:01  lr: 0.000013  min_lr: 0.000000  loss: 4.9223 (5.0001)  loss_scale: 262144.0000 (142922.1658)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1404  max mem: 15572
Epoch: [1]  [1010/2809]  eta: 0:17:57  lr: 0.000013  min_lr: 0.000000  loss: 5.0424 (5.0009)  loss_scale: 262144.0000 (144101.4125)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.2004  max mem: 15572
[2025-01-12 21:06:25,173] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3828
[2025-01-12 21:06:25,173] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:06:25,173] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [1020/2809]  eta: 0:17:51  lr: 0.000013  min_lr: 0.000000  loss: 5.0424 (5.0011)  loss_scale: 262144.0000 (145000.8071)  weight_decay: 0.0500 (0.0500)  time: 0.6654  data: 0.1925  max mem: 15572
Epoch: [1]  [1030/2809]  eta: 0:17:45  lr: 0.000013  min_lr: 0.000000  loss: 4.9455 (5.0007)  loss_scale: 131072.0000 (144865.7071)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1153  max mem: 15572
Epoch: [1]  [1040/2809]  eta: 0:17:39  lr: 0.000013  min_lr: 0.000000  loss: 4.9711 (5.0003)  loss_scale: 131072.0000 (144733.2027)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1136  max mem: 15572
Epoch: [1]  [1050/2809]  eta: 0:17:33  lr: 0.000013  min_lr: 0.000000  loss: 4.9333 (4.9996)  loss_scale: 131072.0000 (144603.2198)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1452  max mem: 15572
Epoch: [1]  [1060/2809]  eta: 0:17:28  lr: 0.000013  min_lr: 0.000000  loss: 4.9158 (4.9990)  loss_scale: 131072.0000 (144475.6871)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1554  max mem: 15572
Epoch: [1]  [1070/2809]  eta: 0:17:19  lr: 0.000013  min_lr: 0.000000  loss: 4.9246 (4.9985)  loss_scale: 131072.0000 (144350.5359)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0825  max mem: 15572
Epoch: [1]  [1080/2809]  eta: 0:17:13  lr: 0.000013  min_lr: 0.000000  loss: 4.9418 (4.9980)  loss_scale: 131072.0000 (144227.7003)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0589  max mem: 15572
Epoch: [1]  [1090/2809]  eta: 0:17:06  lr: 0.000013  min_lr: 0.000000  loss: 4.9421 (4.9979)  loss_scale: 131072.0000 (144107.1164)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0790  max mem: 15572
Epoch: [1]  [1100/2809]  eta: 0:17:01  lr: 0.000013  min_lr: 0.000000  loss: 4.9770 (4.9979)  loss_scale: 131072.0000 (143988.7230)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1116  max mem: 15572
[2025-01-12 21:07:16,048] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3917
[2025-01-12 21:07:16,048] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:07:16,048] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1110/2809]  eta: 0:16:54  lr: 0.000013  min_lr: 0.000000  loss: 4.9963 (4.9978)  loss_scale: 131072.0000 (143695.4959)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0915  max mem: 15572
Epoch: [1]  [1120/2809]  eta: 0:16:46  lr: 0.000013  min_lr: 0.000000  loss: 5.0379 (4.9986)  loss_scale: 65536.0000 (142998.2658)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0461  max mem: 15572
Epoch: [1]  [1130/2809]  eta: 0:16:39  lr: 0.000013  min_lr: 0.000000  loss: 5.0168 (4.9986)  loss_scale: 65536.0000 (142313.3652)  weight_decay: 0.0500 (0.0500)  time: 0.4913  data: 0.0607  max mem: 15572
Epoch: [1]  [1140/2809]  eta: 0:16:33  lr: 0.000013  min_lr: 0.000000  loss: 4.9625 (4.9984)  loss_scale: 65536.0000 (141640.4698)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1188  max mem: 15572
Epoch: [1]  [1150/2809]  eta: 0:16:26  lr: 0.000013  min_lr: 0.000000  loss: 5.0072 (4.9988)  loss_scale: 65536.0000 (140979.2667)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1244  max mem: 15572
Epoch: [1]  [1160/2809]  eta: 0:16:20  lr: 0.000013  min_lr: 0.000000  loss: 4.9998 (4.9985)  loss_scale: 65536.0000 (140329.4539)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0848  max mem: 15572
Epoch: [1]  [1170/2809]  eta: 0:16:14  lr: 0.000013  min_lr: 0.000000  loss: 4.9958 (4.9987)  loss_scale: 65536.0000 (139690.7395)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1322  max mem: 15572
Epoch: [1]  [1180/2809]  eta: 0:16:07  lr: 0.000013  min_lr: 0.000000  loss: 5.0764 (4.9994)  loss_scale: 65536.0000 (139062.8417)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1254  max mem: 15572
[2025-01-12 21:08:01,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=17, lr=[1.2932318270399928e-07, 1.2932318270399928e-07, 1.8474740386285612e-07, 1.8474740386285612e-07, 2.6392486266122304e-07, 2.6392486266122304e-07, 3.7703551808746154e-07, 3.7703551808746154e-07, 5.386221686963737e-07, 5.386221686963737e-07, 7.694602409948195e-07, 7.694602409948195e-07, 1.0992289157068852e-06, 1.0992289157068852e-06, 1.5703270224384075e-06, 1.5703270224384075e-06, 2.2433243177691536e-06, 2.2433243177691536e-06, 3.2047490253845054e-06, 3.2047490253845054e-06, 4.578212893406436e-06, 4.578212893406436e-06, 6.5403041334377665e-06, 6.5403041334377665e-06, 9.34329161919681e-06, 9.34329161919681e-06, 1.3347559455995444e-05, 1.3347559455995444e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:08:01,851] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=27.86961265917859, CurrSamplesPerSec=27.337946902135744, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [1190/2809]  eta: 0:16:02  lr: 0.000013  min_lr: 0.000000  loss: 4.9669 (4.9988)  loss_scale: 65536.0000 (138445.4878)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1479  max mem: 15572
Epoch: [1]  [1200/2809]  eta: 0:15:56  lr: 0.000013  min_lr: 0.000000  loss: 4.9558 (4.9986)  loss_scale: 65536.0000 (137838.4147)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1732  max mem: 15572
Epoch: [1]  [1210/2809]  eta: 0:15:49  lr: 0.000013  min_lr: 0.000000  loss: 4.9985 (4.9988)  loss_scale: 65536.0000 (137241.3675)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1102  max mem: 15572
Epoch: [1]  [1220/2809]  eta: 0:15:42  lr: 0.000013  min_lr: 0.000000  loss: 4.9965 (4.9988)  loss_scale: 65536.0000 (136654.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0759  max mem: 15572
Epoch: [1]  [1230/2809]  eta: 0:15:37  lr: 0.000013  min_lr: 0.000000  loss: 5.0077 (4.9988)  loss_scale: 65536.0000 (136076.3737)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1361  max mem: 15572
[2025-01-12 21:08:30,347] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:08:30,347] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1240/2809]  eta: 0:15:32  lr: 0.000014  min_lr: 0.000000  loss: 5.0098 (4.9993)  loss_scale: 65536.0000 (135719.1942)  weight_decay: 0.0500 (0.0500)  time: 0.6588  data: 0.1906  max mem: 15572
Epoch: [1]  [1250/2809]  eta: 0:15:25  lr: 0.000014  min_lr: 0.000000  loss: 5.0595 (4.9997)  loss_scale: 131072.0000 (135682.0464)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1035  max mem: 15572
Epoch: [1]  [1260/2809]  eta: 0:15:18  lr: 0.000014  min_lr: 0.000000  loss: 5.0337 (4.9994)  loss_scale: 131072.0000 (135645.4877)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0180  max mem: 15572
[2025-01-12 21:08:42,569] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4070
[2025-01-12 21:08:42,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:08:42,570] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1270/2809]  eta: 0:15:12  lr: 0.000014  min_lr: 0.000000  loss: 4.9074 (4.9990)  loss_scale: 65536.0000 (135093.8788)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0540  max mem: 15572
Epoch: [1]  [1280/2809]  eta: 0:15:06  lr: 0.000014  min_lr: 0.000000  loss: 4.9494 (4.9987)  loss_scale: 65536.0000 (134550.8821)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1013  max mem: 15572
Epoch: [1]  [1290/2809]  eta: 0:15:00  lr: 0.000014  min_lr: 0.000000  loss: 4.9967 (4.9991)  loss_scale: 65536.0000 (134016.2974)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1859  max mem: 15572
Epoch: [1]  [1300/2809]  eta: 0:14:54  lr: 0.000014  min_lr: 0.000000  loss: 5.0098 (4.9993)  loss_scale: 65536.0000 (133489.9308)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1647  max mem: 15572
Epoch: [1]  [1310/2809]  eta: 0:14:48  lr: 0.000014  min_lr: 0.000000  loss: 5.0345 (4.9997)  loss_scale: 65536.0000 (132971.5942)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1200  max mem: 15572
Epoch: [1]  [1320/2809]  eta: 0:14:42  lr: 0.000014  min_lr: 0.000000  loss: 5.0168 (4.9994)  loss_scale: 65536.0000 (132461.1052)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1705  max mem: 15572
Epoch: [1]  [1330/2809]  eta: 0:14:36  lr: 0.000014  min_lr: 0.000000  loss: 4.9587 (4.9990)  loss_scale: 65536.0000 (131958.2870)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1663  max mem: 15572
Epoch: [1]  [1340/2809]  eta: 0:14:30  lr: 0.000014  min_lr: 0.000000  loss: 4.9883 (4.9997)  loss_scale: 65536.0000 (131462.9679)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1555  max mem: 15572
Epoch: [1]  [1350/2809]  eta: 0:14:25  lr: 0.000014  min_lr: 0.000000  loss: 4.9995 (4.9997)  loss_scale: 65536.0000 (130974.9815)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1778  max mem: 15572
Epoch: [1]  [1360/2809]  eta: 0:14:18  lr: 0.000014  min_lr: 0.000000  loss: 4.9995 (4.9996)  loss_scale: 65536.0000 (130494.1661)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1259  max mem: 15572
Epoch: [1]  [1370/2809]  eta: 0:14:12  lr: 0.000014  min_lr: 0.000000  loss: 4.9777 (4.9994)  loss_scale: 65536.0000 (130020.3647)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1108  max mem: 15572
Epoch: [1]  [1380/2809]  eta: 0:14:07  lr: 0.000014  min_lr: 0.000000  loss: 4.9560 (4.9990)  loss_scale: 65536.0000 (129553.4251)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1667  max mem: 15572
[2025-01-12 21:09:58,576] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:09:58,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1390/2809]  eta: 0:14:01  lr: 0.000014  min_lr: 0.000000  loss: 4.9686 (4.9988)  loss_scale: 65536.0000 (129140.3134)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1661  max mem: 15572
Epoch: [1]  [1400/2809]  eta: 0:13:55  lr: 0.000014  min_lr: 0.000000  loss: 4.9686 (4.9985)  loss_scale: 131072.0000 (129154.1014)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1572  max mem: 15572
Epoch: [1]  [1410/2809]  eta: 0:13:49  lr: 0.000014  min_lr: 0.000000  loss: 4.9262 (4.9981)  loss_scale: 131072.0000 (129167.6938)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1515  max mem: 15572
Epoch: [1]  [1420/2809]  eta: 0:13:43  lr: 0.000014  min_lr: 0.000000  loss: 4.9153 (4.9979)  loss_scale: 131072.0000 (129181.0950)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1501  max mem: 15572
[2025-01-12 21:10:21,893] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4234
[2025-01-12 21:10:21,893] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:10:21,893] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1430/2809]  eta: 0:13:38  lr: 0.000014  min_lr: 0.000000  loss: 4.9406 (4.9980)  loss_scale: 131072.0000 (128919.5248)  weight_decay: 0.0500 (0.0500)  time: 0.6766  data: 0.2120  max mem: 15572
Epoch: [1]  [1440/2809]  eta: 0:13:33  lr: 0.000014  min_lr: 0.000000  loss: 5.0458 (4.9987)  loss_scale: 65536.0000 (128479.6669)  weight_decay: 0.0500 (0.0500)  time: 0.6704  data: 0.2245  max mem: 15572
Epoch: [1]  [1450/2809]  eta: 0:13:26  lr: 0.000014  min_lr: 0.000000  loss: 5.0131 (4.9986)  loss_scale: 65536.0000 (128045.8718)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.1012  max mem: 15572
Epoch: [1]  [1460/2809]  eta: 0:13:20  lr: 0.000014  min_lr: 0.000000  loss: 4.9675 (4.9987)  loss_scale: 65536.0000 (127618.0151)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0806  max mem: 15572
Epoch: [1]  [1470/2809]  eta: 0:13:14  lr: 0.000014  min_lr: 0.000000  loss: 4.9800 (4.9988)  loss_scale: 65536.0000 (127195.9755)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1262  max mem: 15572
Epoch: [1]  [1480/2809]  eta: 0:13:08  lr: 0.000014  min_lr: 0.000000  loss: 4.9800 (4.9987)  loss_scale: 65536.0000 (126779.6354)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1269  max mem: 15572
Epoch: [1]  [1490/2809]  eta: 0:13:02  lr: 0.000014  min_lr: 0.000000  loss: 4.9972 (4.9988)  loss_scale: 65536.0000 (126368.8799)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1823  max mem: 15572
Epoch: [1]  [1500/2809]  eta: 0:12:56  lr: 0.000014  min_lr: 0.000000  loss: 5.0328 (4.9990)  loss_scale: 65536.0000 (125963.5976)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1602  max mem: 15572
Epoch: [1]  [1510/2809]  eta: 0:12:50  lr: 0.000014  min_lr: 0.000000  loss: 4.9441 (4.9987)  loss_scale: 65536.0000 (125563.6797)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0949  max mem: 15572
Epoch: [1]  [1520/2809]  eta: 0:12:44  lr: 0.000014  min_lr: 0.000000  loss: 4.9295 (4.9989)  loss_scale: 65536.0000 (125169.0204)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0831  max mem: 15572
Epoch: [1]  [1530/2809]  eta: 0:12:37  lr: 0.000014  min_lr: 0.000000  loss: 5.0137 (4.9988)  loss_scale: 65536.0000 (124779.5167)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0635  max mem: 15572
Epoch: [1]  [1540/2809]  eta: 0:12:32  lr: 0.000015  min_lr: 0.000000  loss: 4.9685 (4.9987)  loss_scale: 65536.0000 (124395.0681)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1328  max mem: 15572
Epoch: [1]  [1550/2809]  eta: 0:12:26  lr: 0.000015  min_lr: 0.000000  loss: 5.0386 (4.9987)  loss_scale: 65536.0000 (124015.5770)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1624  max mem: 15572
[2025-01-12 21:11:36,352] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:11:36,352] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1560/2809]  eta: 0:12:19  lr: 0.000015  min_lr: 0.000000  loss: 5.0386 (4.9990)  loss_scale: 65536.0000 (123934.8315)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1139  max mem: 15572
Epoch: [1]  [1570/2809]  eta: 0:12:13  lr: 0.000015  min_lr: 0.000000  loss: 5.0313 (4.9989)  loss_scale: 131072.0000 (123980.2623)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1170  max mem: 15572
Epoch: [1]  [1580/2809]  eta: 0:12:08  lr: 0.000015  min_lr: 0.000000  loss: 4.9970 (4.9992)  loss_scale: 131072.0000 (124025.1183)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1634  max mem: 15572
Epoch: [1]  [1590/2809]  eta: 0:12:01  lr: 0.000015  min_lr: 0.000000  loss: 4.9959 (4.9992)  loss_scale: 131072.0000 (124069.4104)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1508  max mem: 15572
Epoch: [1]  [1600/2809]  eta: 0:11:55  lr: 0.000015  min_lr: 0.000000  loss: 5.0116 (4.9994)  loss_scale: 131072.0000 (124113.1493)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0949  max mem: 15572
Epoch: [1]  [1610/2809]  eta: 0:11:50  lr: 0.000015  min_lr: 0.000000  loss: 5.0116 (4.9991)  loss_scale: 131072.0000 (124156.3451)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1750  max mem: 15572
Epoch: [1]  [1620/2809]  eta: 0:11:43  lr: 0.000015  min_lr: 0.000000  loss: 4.9515 (4.9988)  loss_scale: 131072.0000 (124199.0080)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1315  max mem: 15572
[2025-01-12 21:12:16,825] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4433
[2025-01-12 21:12:16,826] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:12:16,826] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1630/2809]  eta: 0:11:38  lr: 0.000015  min_lr: 0.000000  loss: 4.9242 (4.9983)  loss_scale: 131072.0000 (123959.8774)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1731  max mem: 15572
Epoch: [1]  [1640/2809]  eta: 0:11:32  lr: 0.000015  min_lr: 0.000000  loss: 4.9647 (4.9984)  loss_scale: 65536.0000 (123603.8513)  weight_decay: 0.0500 (0.0500)  time: 0.6596  data: 0.2193  max mem: 15572
Epoch: [1]  [1650/2809]  eta: 0:11:27  lr: 0.000015  min_lr: 0.000000  loss: 4.9772 (4.9982)  loss_scale: 65536.0000 (123252.1381)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1602  max mem: 15572
Epoch: [1]  [1660/2809]  eta: 0:11:20  lr: 0.000015  min_lr: 0.000000  loss: 4.9385 (4.9980)  loss_scale: 65536.0000 (122904.6598)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1326  max mem: 15572
Epoch: [1]  [1670/2809]  eta: 0:11:14  lr: 0.000015  min_lr: 0.000000  loss: 4.9437 (4.9976)  loss_scale: 65536.0000 (122561.3405)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1202  max mem: 15572
Epoch: [1]  [1680/2809]  eta: 0:11:09  lr: 0.000015  min_lr: 0.000000  loss: 4.9766 (4.9976)  loss_scale: 65536.0000 (122222.1059)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1753  max mem: 15572
Epoch: [1]  [1690/2809]  eta: 0:11:03  lr: 0.000015  min_lr: 0.000000  loss: 4.9701 (4.9973)  loss_scale: 65536.0000 (121886.8835)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1552  max mem: 15572
Epoch: [1]  [1700/2809]  eta: 0:10:56  lr: 0.000015  min_lr: 0.000000  loss: 4.9874 (4.9973)  loss_scale: 65536.0000 (121555.6026)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1302  max mem: 15572
Epoch: [1]  [1710/2809]  eta: 0:10:50  lr: 0.000015  min_lr: 0.000000  loss: 4.9949 (4.9970)  loss_scale: 65536.0000 (121228.1940)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1188  max mem: 15572
Epoch: [1]  [1720/2809]  eta: 0:10:45  lr: 0.000015  min_lr: 0.000000  loss: 4.9817 (4.9967)  loss_scale: 65536.0000 (120904.5904)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1315  max mem: 15572
Epoch: [1]  [1730/2809]  eta: 0:10:40  lr: 0.000015  min_lr: 0.000000  loss: 4.9203 (4.9964)  loss_scale: 65536.0000 (120584.7256)  weight_decay: 0.0500 (0.0500)  time: 0.6666  data: 0.2286  max mem: 15572
Epoch: [1]  [1740/2809]  eta: 0:10:33  lr: 0.000015  min_lr: 0.000000  loss: 4.9584 (4.9962)  loss_scale: 65536.0000 (120268.5353)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1718  max mem: 15572
Epoch: [1]  [1750/2809]  eta: 0:10:27  lr: 0.000015  min_lr: 0.000000  loss: 4.9867 (4.9959)  loss_scale: 65536.0000 (119955.9566)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1200  max mem: 15572
[2025-01-12 21:13:35,565] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:13:35,565] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1760/2809]  eta: 0:10:21  lr: 0.000015  min_lr: 0.000000  loss: 4.9874 (4.9961)  loss_scale: 65536.0000 (119944.6496)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.2011  max mem: 15572
Epoch: [1]  [1770/2809]  eta: 0:10:15  lr: 0.000015  min_lr: 0.000000  loss: 5.0192 (4.9961)  loss_scale: 131072.0000 (120007.4805)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1379  max mem: 15572
[2025-01-12 21:13:47,196] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4584
[2025-01-12 21:13:47,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:13:47,197] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1780/2809]  eta: 0:10:09  lr: 0.000015  min_lr: 0.000000  loss: 4.9692 (4.9960)  loss_scale: 131072.0000 (119848.8220)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1011  max mem: 15572
Epoch: [1]  [1790/2809]  eta: 0:10:03  lr: 0.000015  min_lr: 0.000000  loss: 4.9769 (4.9960)  loss_scale: 65536.0000 (119545.5678)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1358  max mem: 15572
Epoch: [1]  [1800/2809]  eta: 0:09:58  lr: 0.000015  min_lr: 0.000000  loss: 4.9557 (4.9958)  loss_scale: 65536.0000 (119245.6813)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1995  max mem: 15572
Epoch: [1]  [1810/2809]  eta: 0:09:51  lr: 0.000015  min_lr: 0.000000  loss: 4.9514 (4.9957)  loss_scale: 65536.0000 (118949.1066)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1588  max mem: 15572
Epoch: [1]  [1820/2809]  eta: 0:09:45  lr: 0.000015  min_lr: 0.000000  loss: 4.9959 (4.9961)  loss_scale: 65536.0000 (118655.7891)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1058  max mem: 15572
Epoch: [1]  [1830/2809]  eta: 0:09:39  lr: 0.000015  min_lr: 0.000000  loss: 5.0412 (4.9962)  loss_scale: 65536.0000 (118365.6756)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1386  max mem: 15572
Epoch: [1]  [1840/2809]  eta: 0:09:33  lr: 0.000016  min_lr: 0.000000  loss: 5.0040 (4.9961)  loss_scale: 65536.0000 (118078.7137)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1391  max mem: 15572
Epoch: [1]  [1850/2809]  eta: 0:09:28  lr: 0.000016  min_lr: 0.000000  loss: 4.9427 (4.9959)  loss_scale: 65536.0000 (117794.8525)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1587  max mem: 15572
Epoch: [1]  [1860/2809]  eta: 0:09:21  lr: 0.000016  min_lr: 0.000000  loss: 4.9491 (4.9961)  loss_scale: 65536.0000 (117514.0419)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0956  max mem: 15572
Epoch: [1]  [1870/2809]  eta: 0:09:15  lr: 0.000016  min_lr: 0.000000  loss: 5.0058 (4.9959)  loss_scale: 65536.0000 (117236.2330)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0554  max mem: 15572
Epoch: [1]  [1880/2809]  eta: 0:09:09  lr: 0.000016  min_lr: 0.000000  loss: 4.9893 (4.9960)  loss_scale: 65536.0000 (116961.3780)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1060  max mem: 15572
Epoch: [1]  [1890/2809]  eta: 0:09:03  lr: 0.000016  min_lr: 0.000000  loss: 4.9609 (4.9959)  loss_scale: 65536.0000 (116689.4299)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0517  max mem: 15572
Epoch: [1]  [1900/2809]  eta: 0:08:57  lr: 0.000016  min_lr: 0.000000  loss: 4.9566 (4.9959)  loss_scale: 65536.0000 (116420.3430)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0676  max mem: 15572
[2025-01-12 21:15:01,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:15:01,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 21:15:02,307] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4715
[2025-01-12 21:15:02,308] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:15:02,308] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1910/2809]  eta: 0:08:51  lr: 0.000016  min_lr: 0.000000  loss: 5.0075 (4.9960)  loss_scale: 65536.0000 (116222.6604)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1267  max mem: 15572
Epoch: [1]  [1920/2809]  eta: 0:08:45  lr: 0.000016  min_lr: 0.000000  loss: 4.9993 (4.9959)  loss_scale: 65536.0000 (115958.8048)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1272  max mem: 15572
Epoch: [1]  [1930/2809]  eta: 0:08:39  lr: 0.000016  min_lr: 0.000000  loss: 5.0251 (4.9960)  loss_scale: 65536.0000 (115697.6820)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1157  max mem: 15572
Epoch: [1]  [1940/2809]  eta: 0:08:33  lr: 0.000016  min_lr: 0.000000  loss: 5.0146 (4.9959)  loss_scale: 65536.0000 (115439.2499)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1491  max mem: 15572
Epoch: [1]  [1950/2809]  eta: 0:08:27  lr: 0.000016  min_lr: 0.000000  loss: 4.9636 (4.9954)  loss_scale: 65536.0000 (115183.4669)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.2104  max mem: 15572
Epoch: [1]  [1960/2809]  eta: 0:08:21  lr: 0.000016  min_lr: 0.000000  loss: 4.9788 (4.9954)  loss_scale: 65536.0000 (114930.2927)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1622  max mem: 15572
Epoch: [1]  [1970/2809]  eta: 0:08:16  lr: 0.000016  min_lr: 0.000000  loss: 4.9705 (4.9951)  loss_scale: 65536.0000 (114679.6875)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1718  max mem: 15572
Epoch: [1]  [1980/2809]  eta: 0:08:09  lr: 0.000016  min_lr: 0.000000  loss: 4.9705 (4.9952)  loss_scale: 65536.0000 (114431.6123)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1358  max mem: 15572
Epoch: [1]  [1990/2809]  eta: 0:08:04  lr: 0.000016  min_lr: 0.000000  loss: 5.0016 (4.9951)  loss_scale: 65536.0000 (114186.0291)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0667  max mem: 15572
Epoch: [1]  [2000/2809]  eta: 0:07:58  lr: 0.000016  min_lr: 0.000000  loss: 4.9506 (4.9949)  loss_scale: 65536.0000 (113942.9005)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1539  max mem: 15572
Epoch: [1]  [2010/2809]  eta: 0:07:52  lr: 0.000016  min_lr: 0.000000  loss: 4.9547 (4.9950)  loss_scale: 65536.0000 (113702.1900)  weight_decay: 0.0500 (0.0500)  time: 0.6652  data: 0.2140  max mem: 15572
Epoch: [1]  [2020/2809]  eta: 0:07:46  lr: 0.000016  min_lr: 0.000000  loss: 4.9621 (4.9949)  loss_scale: 65536.0000 (113463.8615)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1835  max mem: 15572
Epoch: [1]  [2030/2809]  eta: 0:07:40  lr: 0.000016  min_lr: 0.000000  loss: 4.9621 (4.9949)  loss_scale: 65536.0000 (113227.8799)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0835  max mem: 15572
[2025-01-12 21:16:17,290] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:16:17,290] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 21:16:21,093] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4848
[2025-01-12 21:16:21,093] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:16:21,094] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2040/2809]  eta: 0:07:34  lr: 0.000016  min_lr: 0.000000  loss: 4.9641 (4.9946)  loss_scale: 65536.0000 (113122.6497)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1067  max mem: 15572
Epoch: [1]  [2050/2809]  eta: 0:07:28  lr: 0.000016  min_lr: 0.000000  loss: 4.9641 (4.9948)  loss_scale: 65536.0000 (112890.6329)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1527  max mem: 15572
Epoch: [1]  [2060/2809]  eta: 0:07:23  lr: 0.000016  min_lr: 0.000000  loss: 4.9646 (4.9947)  loss_scale: 65536.0000 (112660.8675)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1386  max mem: 15572
Epoch: [1]  [2070/2809]  eta: 0:07:16  lr: 0.000016  min_lr: 0.000000  loss: 4.9925 (4.9947)  loss_scale: 65536.0000 (112433.3211)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1025  max mem: 15572
[2025-01-12 21:16:40,153] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4882
[2025-01-12 21:16:40,153] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 21:16:40,156] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [2080/2809]  eta: 0:07:11  lr: 0.000016  min_lr: 0.000000  loss: 5.0045 (4.9947)  loss_scale: 65536.0000 (112081.9914)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0955  max mem: 15572
Epoch: [1]  [2090/2809]  eta: 0:07:05  lr: 0.000016  min_lr: 0.000000  loss: 4.9630 (4.9945)  loss_scale: 32768.0000 (111702.6801)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1432  max mem: 15572
Epoch: [1]  [2100/2809]  eta: 0:06:59  lr: 0.000016  min_lr: 0.000000  loss: 4.9746 (4.9946)  loss_scale: 32768.0000 (111326.9795)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1963  max mem: 15572
Epoch: [1]  [2110/2809]  eta: 0:06:53  lr: 0.000016  min_lr: 0.000000  loss: 5.0337 (4.9947)  loss_scale: 32768.0000 (110954.8385)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1551  max mem: 15572
Epoch: [1]  [2120/2809]  eta: 0:06:47  lr: 0.000016  min_lr: 0.000000  loss: 4.9544 (4.9945)  loss_scale: 32768.0000 (110586.2065)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.0736  max mem: 15572
Epoch: [1]  [2130/2809]  eta: 0:06:41  lr: 0.000016  min_lr: 0.000000  loss: 4.9653 (4.9948)  loss_scale: 32768.0000 (110221.0343)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1016  max mem: 15572
Epoch: [1]  [2140/2809]  eta: 0:06:35  lr: 0.000017  min_lr: 0.000000  loss: 4.9736 (4.9946)  loss_scale: 32768.0000 (109859.2732)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1496  max mem: 15572
Epoch: [1]  [2150/2809]  eta: 0:06:29  lr: 0.000017  min_lr: 0.000000  loss: 4.9736 (4.9947)  loss_scale: 32768.0000 (109500.8759)  weight_decay: 0.0500 (0.0500)  time: 0.6240  data: 0.1442  max mem: 15572
Epoch: [1]  [2160/2809]  eta: 0:06:23  lr: 0.000017  min_lr: 0.000000  loss: 5.0416 (4.9950)  loss_scale: 32768.0000 (109145.7955)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1550  max mem: 15572
Epoch: [1]  [2170/2809]  eta: 0:06:17  lr: 0.000017  min_lr: 0.000000  loss: 4.9807 (4.9949)  loss_scale: 32768.0000 (108793.9862)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1591  max mem: 15572
Epoch: [1]  [2180/2809]  eta: 0:06:11  lr: 0.000017  min_lr: 0.000000  loss: 4.9807 (4.9949)  loss_scale: 32768.0000 (108445.4030)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1441  max mem: 15572
[2025-01-12 21:17:50,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[1.6166206310009812e-07, 1.6166206310009812e-07, 2.3094580442871163e-07, 2.3094580442871163e-07, 3.2992257775530236e-07, 3.2992257775530236e-07, 4.7131796822186054e-07, 4.7131796822186054e-07, 6.733113831740865e-07, 6.733113831740865e-07, 9.618734045344093e-07, 9.618734045344093e-07, 1.3741048636205847e-06, 1.3741048636205847e-06, 1.963006948029407e-06, 1.963006948029407e-06, 2.80429564004201e-06, 2.80429564004201e-06, 4.0061366286314435e-06, 4.0061366286314435e-06, 5.723052326616347e-06, 5.723052326616347e-06, 8.175789038023355e-06, 8.175789038023355e-06, 1.167969862574765e-05, 1.167969862574765e-05, 1.6685283751068073e-05, 1.6685283751068073e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:17:50,524] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=27.92310121983637, CurrSamplesPerSec=31.029518691390006, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [2190/2809]  eta: 0:06:06  lr: 0.000017  min_lr: 0.000000  loss: 5.0170 (4.9950)  loss_scale: 32768.0000 (108100.0018)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1796  max mem: 15572
Epoch: [1]  [2200/2809]  eta: 0:06:00  lr: 0.000017  min_lr: 0.000000  loss: 4.9662 (4.9948)  loss_scale: 32768.0000 (107757.7392)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1656  max mem: 15572
[2025-01-12 21:17:57,473] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:17:57,475] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2210/2809]  eta: 0:05:54  lr: 0.000017  min_lr: 0.000000  loss: 4.9382 (4.9944)  loss_scale: 32768.0000 (107551.9566)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1548  max mem: 15572
Epoch: [1]  [2220/2809]  eta: 0:05:48  lr: 0.000017  min_lr: 0.000000  loss: 4.9111 (4.9942)  loss_scale: 65536.0000 (107362.7807)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1449  max mem: 15572
Epoch: [1]  [2230/2809]  eta: 0:05:42  lr: 0.000017  min_lr: 0.000000  loss: 4.9521 (4.9940)  loss_scale: 65536.0000 (107175.3008)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1572  max mem: 15572
[2025-01-12 21:18:20,664] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5049
[2025-01-12 21:18:20,665] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 21:18:20,665] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [2240/2809]  eta: 0:05:36  lr: 0.000017  min_lr: 0.000000  loss: 4.9521 (4.9937)  loss_scale: 65536.0000 (106974.8719)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1638  max mem: 15572
Epoch: [1]  [2250/2809]  eta: 0:05:30  lr: 0.000017  min_lr: 0.000000  loss: 4.9245 (4.9934)  loss_scale: 32768.0000 (106645.2101)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0990  max mem: 15572
Epoch: [1]  [2260/2809]  eta: 0:05:24  lr: 0.000017  min_lr: 0.000000  loss: 4.9285 (4.9934)  loss_scale: 32768.0000 (106318.4644)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0987  max mem: 15572
Epoch: [1]  [2270/2809]  eta: 0:05:18  lr: 0.000017  min_lr: 0.000000  loss: 4.9067 (4.9930)  loss_scale: 32768.0000 (105994.5962)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1588  max mem: 15572
Epoch: [1]  [2280/2809]  eta: 0:05:12  lr: 0.000017  min_lr: 0.000000  loss: 4.9214 (4.9929)  loss_scale: 32768.0000 (105673.5677)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1370  max mem: 15572
Epoch: [1]  [2290/2809]  eta: 0:05:06  lr: 0.000017  min_lr: 0.000000  loss: 4.9410 (4.9926)  loss_scale: 32768.0000 (105355.3418)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1388  max mem: 15572
Epoch: [1]  [2300/2809]  eta: 0:05:01  lr: 0.000017  min_lr: 0.000000  loss: 4.9860 (4.9926)  loss_scale: 32768.0000 (105039.8818)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1636  max mem: 15572
Epoch: [1]  [2310/2809]  eta: 0:04:55  lr: 0.000017  min_lr: 0.000000  loss: 5.0122 (4.9924)  loss_scale: 32768.0000 (104727.1519)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1133  max mem: 15572
Epoch: [1]  [2320/2809]  eta: 0:04:49  lr: 0.000017  min_lr: 0.000000  loss: 4.9263 (4.9921)  loss_scale: 32768.0000 (104417.1168)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1014  max mem: 15572
Epoch: [1]  [2330/2809]  eta: 0:04:43  lr: 0.000017  min_lr: 0.000000  loss: 4.9459 (4.9923)  loss_scale: 32768.0000 (104109.7417)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1487  max mem: 15572
Epoch: [1]  [2340/2809]  eta: 0:04:37  lr: 0.000017  min_lr: 0.000000  loss: 4.9495 (4.9920)  loss_scale: 32768.0000 (103804.9927)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1306  max mem: 15572
Epoch: [1]  [2350/2809]  eta: 0:04:31  lr: 0.000017  min_lr: 0.000000  loss: 4.9495 (4.9922)  loss_scale: 32768.0000 (103502.8362)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1115  max mem: 15572
Epoch: [1]  [2360/2809]  eta: 0:04:25  lr: 0.000017  min_lr: 0.000000  loss: 4.9719 (4.9921)  loss_scale: 32768.0000 (103203.2393)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.1966  max mem: 15572
[2025-01-12 21:19:37,425] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:19:37,426] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2370/2809]  eta: 0:04:19  lr: 0.000017  min_lr: 0.000000  loss: 4.9719 (4.9920)  loss_scale: 32768.0000 (102933.8102)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.1848  max mem: 15572
Epoch: [1]  [2380/2809]  eta: 0:04:14  lr: 0.000017  min_lr: 0.000000  loss: 4.8816 (4.9915)  loss_scale: 65536.0000 (102776.7425)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1190  max mem: 15572
Epoch: [1]  [2390/2809]  eta: 0:04:08  lr: 0.000017  min_lr: 0.000000  loss: 4.8638 (4.9910)  loss_scale: 65536.0000 (102620.9887)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1422  max mem: 15572
Epoch: [1]  [2400/2809]  eta: 0:04:02  lr: 0.000017  min_lr: 0.000000  loss: 4.8923 (4.9906)  loss_scale: 65536.0000 (102466.5323)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1290  max mem: 15572
Epoch: [1]  [2410/2809]  eta: 0:03:56  lr: 0.000017  min_lr: 0.000000  loss: 4.9440 (4.9907)  loss_scale: 65536.0000 (102313.3571)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1311  max mem: 15572
Epoch: [1]  [2420/2809]  eta: 0:03:50  lr: 0.000017  min_lr: 0.000000  loss: 4.9617 (4.9906)  loss_scale: 65536.0000 (102161.4473)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1015  max mem: 15572
Epoch: [1]  [2430/2809]  eta: 0:03:44  lr: 0.000017  min_lr: 0.000000  loss: 4.9517 (4.9906)  loss_scale: 65536.0000 (102010.7873)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0526  max mem: 15572
Epoch: [1]  [2440/2809]  eta: 0:03:38  lr: 0.000018  min_lr: 0.000000  loss: 4.9348 (4.9903)  loss_scale: 65536.0000 (101861.3617)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0617  max mem: 15572
Epoch: [1]  [2450/2809]  eta: 0:03:32  lr: 0.000018  min_lr: 0.000000  loss: 4.9592 (4.9904)  loss_scale: 65536.0000 (101713.1554)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0768  max mem: 15572
Epoch: [1]  [2460/2809]  eta: 0:03:26  lr: 0.000018  min_lr: 0.000000  loss: 5.0032 (4.9903)  loss_scale: 65536.0000 (101566.1536)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1171  max mem: 15572
Epoch: [1]  [2470/2809]  eta: 0:03:20  lr: 0.000018  min_lr: 0.000000  loss: 4.9282 (4.9901)  loss_scale: 65536.0000 (101420.3416)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1090  max mem: 15572
Epoch: [1]  [2480/2809]  eta: 0:03:14  lr: 0.000018  min_lr: 0.000000  loss: 4.9123 (4.9899)  loss_scale: 65536.0000 (101275.7050)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0796  max mem: 15572
Epoch: [1]  [2490/2809]  eta: 0:03:08  lr: 0.000018  min_lr: 0.000000  loss: 4.9232 (4.9896)  loss_scale: 65536.0000 (101132.2296)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1401  max mem: 15572
[2025-01-12 21:20:49,750] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:20:49,751] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2500/2809]  eta: 0:03:02  lr: 0.000018  min_lr: 0.000000  loss: 4.9236 (4.9896)  loss_scale: 65536.0000 (101094.7173)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.1841  max mem: 15572
[2025-01-12 21:20:54,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5313
[2025-01-12 21:20:54,463] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:20:54,463] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2510/2809]  eta: 0:02:56  lr: 0.000018  min_lr: 0.000000  loss: 4.9559 (4.9895)  loss_scale: 65536.0000 (101031.4042)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1751  max mem: 15572
Epoch: [1]  [2520/2809]  eta: 0:02:50  lr: 0.000018  min_lr: 0.000000  loss: 4.9559 (4.9893)  loss_scale: 65536.0000 (100890.6053)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1781  max mem: 15572
Epoch: [1]  [2530/2809]  eta: 0:02:44  lr: 0.000018  min_lr: 0.000000  loss: 4.9441 (4.9892)  loss_scale: 65536.0000 (100750.9190)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1158  max mem: 15572
Epoch: [1]  [2540/2809]  eta: 0:02:38  lr: 0.000018  min_lr: 0.000000  loss: 5.0081 (4.9893)  loss_scale: 65536.0000 (100612.3322)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0779  max mem: 15572
Epoch: [1]  [2550/2809]  eta: 0:02:33  lr: 0.000018  min_lr: 0.000000  loss: 4.9990 (4.9892)  loss_scale: 65536.0000 (100474.8318)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1346  max mem: 15572
Epoch: [1]  [2560/2809]  eta: 0:02:27  lr: 0.000018  min_lr: 0.000000  loss: 4.9625 (4.9891)  loss_scale: 65536.0000 (100338.4053)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1078  max mem: 15572
Epoch: [1]  [2570/2809]  eta: 0:02:21  lr: 0.000018  min_lr: 0.000000  loss: 4.9747 (4.9889)  loss_scale: 65536.0000 (100203.0401)  weight_decay: 0.0500 (0.0500)  time: 0.4629  data: 0.0255  max mem: 15572
Epoch: [1]  [2580/2809]  eta: 0:02:14  lr: 0.000018  min_lr: 0.000000  loss: 4.9203 (4.9886)  loss_scale: 65536.0000 (100068.7238)  weight_decay: 0.0500 (0.0500)  time: 0.3938  data: 0.0004  max mem: 15572
Epoch: [1]  [2590/2809]  eta: 0:02:08  lr: 0.000018  min_lr: 0.000000  loss: 4.9469 (4.9890)  loss_scale: 65536.0000 (99935.4442)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
Epoch: [1]  [2600/2809]  eta: 0:02:02  lr: 0.000018  min_lr: 0.000000  loss: 4.9588 (4.9888)  loss_scale: 65536.0000 (99803.1895)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0003  max mem: 15572
Epoch: [1]  [2610/2809]  eta: 0:01:56  lr: 0.000018  min_lr: 0.000000  loss: 4.9404 (4.9887)  loss_scale: 65536.0000 (99671.9479)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
Epoch: [1]  [2620/2809]  eta: 0:01:50  lr: 0.000018  min_lr: 0.000000  loss: 4.9255 (4.9884)  loss_scale: 65536.0000 (99541.7077)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [1]  [2630/2809]  eta: 0:01:44  lr: 0.000018  min_lr: 0.000000  loss: 4.8699 (4.9881)  loss_scale: 65536.0000 (99412.4576)  weight_decay: 0.0500 (0.0500)  time: 0.3823  data: 0.0004  max mem: 15572
[2025-01-12 21:21:55,548] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:21:55,549] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2640/2809]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 4.9297 (4.9879)  loss_scale: 65536.0000 (99482.7050)  weight_decay: 0.0500 (0.0500)  time: 0.4048  data: 0.0005  max mem: 15572
[2025-01-12 21:22:01,439] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5456
[2025-01-12 21:22:01,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:22:01,439] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2650/2809]  eta: 0:01:32  lr: 0.000018  min_lr: 0.000000  loss: 4.9297 (4.9876)  loss_scale: 131072.0000 (99502.9800)  weight_decay: 0.0500 (0.0500)  time: 0.4089  data: 0.0006  max mem: 15572
Epoch: [1]  [2660/2809]  eta: 0:01:26  lr: 0.000018  min_lr: 0.000000  loss: 4.9021 (4.9874)  loss_scale: 65536.0000 (99375.3326)  weight_decay: 0.0500 (0.0500)  time: 0.3995  data: 0.0006  max mem: 15572
Epoch: [1]  [2670/2809]  eta: 0:01:21  lr: 0.000018  min_lr: 0.000000  loss: 4.9021 (4.9871)  loss_scale: 65536.0000 (99248.6410)  weight_decay: 0.0500 (0.0500)  time: 0.4408  data: 0.0007  max mem: 15572
Epoch: [1]  [2680/2809]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 4.9349 (4.9870)  loss_scale: 65536.0000 (99122.8944)  weight_decay: 0.0500 (0.0500)  time: 0.4702  data: 0.0010  max mem: 15572
Epoch: [1]  [2690/2809]  eta: 0:01:09  lr: 0.000018  min_lr: 0.000000  loss: 4.9522 (4.9869)  loss_scale: 65536.0000 (98998.0825)  weight_decay: 0.0500 (0.0500)  time: 0.4521  data: 0.0010  max mem: 15572
Epoch: [1]  [2700/2809]  eta: 0:01:03  lr: 0.000018  min_lr: 0.000000  loss: 4.9120 (4.9864)  loss_scale: 65536.0000 (98874.1947)  weight_decay: 0.0500 (0.0500)  time: 0.4298  data: 0.0006  max mem: 15572
Epoch: [1]  [2710/2809]  eta: 0:00:57  lr: 0.000018  min_lr: 0.000000  loss: 4.9120 (4.9862)  loss_scale: 65536.0000 (98751.2210)  weight_decay: 0.0500 (0.0500)  time: 0.4352  data: 0.0007  max mem: 15572
Epoch: [1]  [2720/2809]  eta: 0:00:51  lr: 0.000018  min_lr: 0.000000  loss: 4.8782 (4.9858)  loss_scale: 65536.0000 (98629.1510)  weight_decay: 0.0500 (0.0500)  time: 0.4554  data: 0.0009  max mem: 15572
Epoch: [1]  [2730/2809]  eta: 0:00:45  lr: 0.000018  min_lr: 0.000000  loss: 4.8772 (4.9857)  loss_scale: 65536.0000 (98507.9751)  weight_decay: 0.0500 (0.0500)  time: 0.4528  data: 0.0009  max mem: 15572
Epoch: [1]  [2740/2809]  eta: 0:00:40  lr: 0.000019  min_lr: 0.000000  loss: 4.9237 (4.9855)  loss_scale: 65536.0000 (98387.6833)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0937  max mem: 15572
Epoch: [1]  [2750/2809]  eta: 0:00:34  lr: 0.000019  min_lr: 0.000000  loss: 4.9309 (4.9854)  loss_scale: 65536.0000 (98268.2661)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1653  max mem: 15572
Epoch: [1]  [2760/2809]  eta: 0:00:28  lr: 0.000019  min_lr: 0.000000  loss: 4.8917 (4.9851)  loss_scale: 65536.0000 (98149.7139)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1415  max mem: 15572
Epoch: [1]  [2770/2809]  eta: 0:00:22  lr: 0.000019  min_lr: 0.000000  loss: 4.8863 (4.9849)  loss_scale: 65536.0000 (98032.0173)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.1454  max mem: 15572
[2025-01-12 21:23:06,942] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:23:06,942] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2780/2809]  eta: 0:00:16  lr: 0.000019  min_lr: 0.000000  loss: 4.9066 (4.9845)  loss_scale: 65536.0000 (98032.9953)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1112  max mem: 15572
Epoch: [1]  [2790/2809]  eta: 0:00:11  lr: 0.000019  min_lr: 0.000000  loss: 4.9092 (4.9845)  loss_scale: 131072.0000 (98151.3723)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0913  max mem: 15572
[2025-01-12 21:23:18,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5606
[2025-01-12 21:23:18,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:23:18,481] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2800/2809]  eta: 0:00:05  lr: 0.000019  min_lr: 0.000000  loss: 4.9303 (4.9843)  loss_scale: 131072.0000 (98175.3145)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0920  max mem: 15572
Epoch: [1]  [2808/2809]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 4.9376 (4.9843)  loss_scale: 65536.0000 (98082.3581)  weight_decay: 0.0500 (0.0500)  time: 0.4740  data: 0.0616  max mem: 15572
Epoch: [1] Total time: 0:27:09 (0.5803 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 4.9376 (4.9843)  loss_scale: 65536.0000 (98082.3581)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:18:43  loss: 5.0872 (5.0872)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 4.1298  data: 3.9588  max mem: 15572
Val:  [ 10/272]  eta: 0:03:17  loss: 5.2079 (5.0102)  acc1: 0.0000 (2.5253)  acc5: 0.0000 (18.1818)  time: 0.7519  data: 0.5394  max mem: 15572
Val:  [ 20/272]  eta: 0:02:19  loss: 5.0015 (4.9185)  acc1: 0.0000 (3.4392)  acc5: 0.0000 (17.9894)  time: 0.3768  data: 0.1664  max mem: 15572
Val:  [ 30/272]  eta: 0:01:50  loss: 4.9735 (4.8957)  acc1: 0.0000 (2.3297)  acc5: 0.0000 (17.7419)  time: 0.2961  data: 0.0950  max mem: 15572
Val:  [ 40/272]  eta: 0:01:42  loss: 4.5089 (4.7925)  acc1: 0.0000 (6.7751)  acc5: 27.7778 (28.5908)  time: 0.3230  data: 0.1276  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 4.6248 (4.8251)  acc1: 0.0000 (5.4466)  acc5: 0.0000 (22.9847)  time: 0.3458  data: 0.1546  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 4.7122 (4.8287)  acc1: 0.0000 (4.5537)  acc5: 0.0000 (19.3078)  time: 0.3024  data: 0.1015  max mem: 15572
Val:  [ 70/272]  eta: 0:01:20  loss: 4.7806 (4.8047)  acc1: 0.0000 (6.5728)  acc5: 0.0000 (20.6573)  time: 0.3630  data: 0.1469  max mem: 15572
Val:  [ 80/272]  eta: 0:01:17  loss: 4.9269 (4.7836)  acc1: 0.0000 (6.3100)  acc5: 0.0000 (22.1536)  time: 0.4284  data: 0.1994  max mem: 15572
Val:  [ 90/272]  eta: 0:01:09  loss: 4.9622 (4.8199)  acc1: 0.0000 (5.6166)  acc5: 0.0000 (19.7192)  time: 0.3215  data: 0.1000  max mem: 15572
Val:  [100/272]  eta: 0:01:04  loss: 4.9603 (4.8372)  acc1: 0.0000 (5.0605)  acc5: 0.0000 (19.3619)  time: 0.2518  data: 0.0311  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 4.8600 (4.8606)  acc1: 0.0000 (4.6046)  acc5: 0.0000 (17.6176)  time: 0.3043  data: 0.0943  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 5.0471 (4.8823)  acc1: 0.0000 (4.2241)  acc5: 0.0000 (16.1616)  time: 0.3118  data: 0.1204  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 4.9312 (4.8661)  acc1: 0.0000 (5.5980)  acc5: 0.0000 (18.2782)  time: 0.2796  data: 0.0855  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 4.6678 (4.8551)  acc1: 0.0000 (5.3586)  acc5: 0.0000 (18.8731)  time: 0.2826  data: 0.1011  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 4.7878 (4.8553)  acc1: 0.0000 (5.0037)  acc5: 0.0000 (17.6233)  time: 0.3374  data: 0.1485  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 4.5775 (4.8342)  acc1: 0.0000 (5.4865)  acc5: 0.0000 (18.8751)  time: 0.3120  data: 0.1164  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.6400 (4.8488)  acc1: 0.0000 (5.2632)  acc5: 0.0000 (17.9662)  time: 0.2983  data: 0.1085  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 4.9416 (4.8517)  acc1: 0.0000 (4.9724)  acc5: 0.0000 (16.9736)  time: 0.3304  data: 0.1365  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.9375 (4.8615)  acc1: 0.0000 (4.7120)  acc5: 0.0000 (16.0849)  time: 0.3333  data: 0.1367  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 4.9900 (4.8737)  acc1: 0.0000 (4.4776)  acc5: 0.0000 (15.2847)  time: 0.3015  data: 0.1091  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 5.0091 (4.8826)  acc1: 0.0000 (4.2654)  acc5: 0.0000 (14.5866)  time: 0.2888  data: 0.1014  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.9151 (4.8769)  acc1: 0.0000 (4.0724)  acc5: 0.0000 (14.0523)  time: 0.3600  data: 0.1635  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.7648 (4.8730)  acc1: 0.0000 (3.8961)  acc5: 0.0000 (13.4440)  time: 0.4074  data: 0.1993  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 4.7689 (4.8746)  acc1: 0.0000 (3.7344)  acc5: 0.0000 (13.2089)  time: 0.3457  data: 0.1389  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 5.1458 (4.8933)  acc1: 0.0000 (3.5857)  acc5: 0.0000 (12.6826)  time: 0.2817  data: 0.0756  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.8583 (4.8860)  acc1: 0.0000 (3.4483)  acc5: 0.0000 (12.3670)  time: 0.2777  data: 0.0813  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 4.7574 (4.8861)  acc1: 0.0000 (3.3210)  acc5: 0.0000 (12.6486)  time: 0.2453  data: 0.0794  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 4.7574 (4.8885)  acc1: 0.0000 (3.3176)  acc5: 0.0000 (12.6357)  time: 0.2130  data: 0.0544  max mem: 15572
Val: Total time: 0:01:30 (0.3335 s / it)
* Acc@1 3.318 Acc@5 12.636 loss 4.889
Accuracy of the network on the 4883 val videos: 3.3%
[2025-01-12 21:24:54,257] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-12 21:24:54,263] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 21:24:54,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 21:24:57,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 21:24:57,054] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 3.32%
Epoch: [2]  [   0/2809]  eta: 4:34:19  lr: 0.000019  min_lr: 0.000000  loss: 4.9339 (4.9339)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 5.8595  data: 5.3021  max mem: 15572
Epoch: [2]  [  10/2809]  eta: 0:56:07  lr: 0.000019  min_lr: 0.000000  loss: 4.9615 (4.9631)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2030  data: 0.7605  max mem: 15572
Epoch: [2]  [  20/2809]  eta: 0:41:58  lr: 0.000019  min_lr: 0.000000  loss: 4.9594 (4.9462)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6551  data: 0.2174  max mem: 15572
Epoch: [2]  [  30/2809]  eta: 0:37:37  lr: 0.000019  min_lr: 0.000000  loss: 4.8484 (4.9134)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1621  max mem: 15572
Epoch: [2]  [  40/2809]  eta: 0:37:13  lr: 0.000019  min_lr: 0.000000  loss: 4.8770 (4.9293)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7053  data: 0.2675  max mem: 15572
Epoch: [2]  [  50/2809]  eta: 0:34:39  lr: 0.000019  min_lr: 0.000000  loss: 4.9400 (4.9242)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.2233  max mem: 15572
Epoch: [2]  [  60/2809]  eta: 0:32:53  lr: 0.000019  min_lr: 0.000000  loss: 4.9353 (4.9343)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0881  max mem: 15572
Epoch: [2]  [  70/2809]  eta: 0:31:32  lr: 0.000019  min_lr: 0.000000  loss: 4.9544 (4.9413)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0825  max mem: 15572
Epoch: [2]  [  80/2809]  eta: 0:31:07  lr: 0.000019  min_lr: 0.000000  loss: 4.9544 (4.9471)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1399  max mem: 15572
Epoch: [2]  [  90/2809]  eta: 0:30:21  lr: 0.000019  min_lr: 0.000000  loss: 4.9108 (4.9414)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1403  max mem: 15572
Epoch: [2]  [ 100/2809]  eta: 0:29:41  lr: 0.000019  min_lr: 0.000000  loss: 4.8750 (4.9395)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0969  max mem: 15572
Epoch: [2]  [ 110/2809]  eta: 0:29:14  lr: 0.000019  min_lr: 0.000000  loss: 4.9562 (4.9419)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1232  max mem: 15572
[2025-01-12 21:26:13,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:26:13,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 120/2809]  eta: 0:28:51  lr: 0.000019  min_lr: 0.000000  loss: 4.9713 (4.9398)  loss_scale: 65536.0000 (67702.4793)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1470  max mem: 15572
Epoch: [2]  [ 130/2809]  eta: 0:28:35  lr: 0.000019  min_lr: 0.000000  loss: 4.9297 (4.9352)  loss_scale: 131072.0000 (72539.8473)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1510  max mem: 15572
[2025-01-12 21:26:23,736] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5750
[2025-01-12 21:26:23,737] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:26:23,737] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 140/2809]  eta: 0:28:23  lr: 0.000019  min_lr: 0.000000  loss: 4.9297 (4.9407)  loss_scale: 131072.0000 (72507.9149)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1704  max mem: 15572
Epoch: [2]  [ 150/2809]  eta: 0:28:20  lr: 0.000019  min_lr: 0.000000  loss: 4.9181 (4.9358)  loss_scale: 65536.0000 (72046.1987)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1755  max mem: 15572
Epoch: [2]  [ 160/2809]  eta: 0:28:11  lr: 0.000019  min_lr: 0.000000  loss: 4.9211 (4.9362)  loss_scale: 65536.0000 (71641.8385)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.1534  max mem: 15572
Epoch: [2]  [ 170/2809]  eta: 0:27:56  lr: 0.000019  min_lr: 0.000000  loss: 4.9567 (4.9348)  loss_scale: 65536.0000 (71284.7719)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1359  max mem: 15572
Epoch: [2]  [ 180/2809]  eta: 0:27:52  lr: 0.000019  min_lr: 0.000000  loss: 4.8716 (4.9342)  loss_scale: 65536.0000 (70967.1602)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1563  max mem: 15572
Epoch: [2]  [ 190/2809]  eta: 0:27:25  lr: 0.000019  min_lr: 0.000000  loss: 4.8898 (4.9311)  loss_scale: 65536.0000 (70682.8063)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1066  max mem: 15572
Epoch: [2]  [ 200/2809]  eta: 0:27:17  lr: 0.000019  min_lr: 0.000000  loss: 4.8781 (4.9291)  loss_scale: 65536.0000 (70426.7463)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0985  max mem: 15572
Epoch: [2]  [ 210/2809]  eta: 0:27:06  lr: 0.000019  min_lr: 0.000000  loss: 4.8732 (4.9276)  loss_scale: 65536.0000 (70194.9573)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1583  max mem: 15572
Epoch: [2]  [ 220/2809]  eta: 0:26:44  lr: 0.000019  min_lr: 0.000000  loss: 4.8676 (4.9235)  loss_scale: 65536.0000 (69984.1448)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.1060  max mem: 15572
Epoch: [2]  [ 230/2809]  eta: 0:26:45  lr: 0.000020  min_lr: 0.000000  loss: 4.8241 (4.9224)  loss_scale: 65536.0000 (69791.5844)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1487  max mem: 15572
Epoch: [2]  [ 240/2809]  eta: 0:26:44  lr: 0.000020  min_lr: 0.000000  loss: 4.9405 (4.9234)  loss_scale: 65536.0000 (69615.0041)  weight_decay: 0.0500 (0.0500)  time: 0.6769  data: 0.2184  max mem: 15572
Epoch: [2]  [ 250/2809]  eta: 0:26:34  lr: 0.000020  min_lr: 0.000000  loss: 4.9062 (4.9230)  loss_scale: 65536.0000 (69452.4940)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.1571  max mem: 15572
Epoch: [2]  [ 260/2809]  eta: 0:26:16  lr: 0.000020  min_lr: 0.000000  loss: 4.8917 (4.9249)  loss_scale: 65536.0000 (69302.4368)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0965  max mem: 15572
[2025-01-12 21:27:40,156] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:27:40,156] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 270/2809]  eta: 0:26:08  lr: 0.000020  min_lr: 0.000000  loss: 4.8917 (4.9244)  loss_scale: 65536.0000 (71581.7565)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0922  max mem: 15572
Epoch: [2]  [ 280/2809]  eta: 0:26:04  lr: 0.000020  min_lr: 0.000000  loss: 4.8561 (4.9219)  loss_scale: 131072.0000 (73698.8470)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1321  max mem: 15572
Epoch: [2]  [ 290/2809]  eta: 0:25:59  lr: 0.000020  min_lr: 0.000000  loss: 4.8549 (4.9221)  loss_scale: 131072.0000 (75670.4330)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1428  max mem: 15572
Epoch: [2]  [ 300/2809]  eta: 0:25:41  lr: 0.000020  min_lr: 0.000000  loss: 4.9173 (4.9221)  loss_scale: 131072.0000 (77511.0166)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0676  max mem: 15572
Epoch: [2]  [ 310/2809]  eta: 0:25:30  lr: 0.000020  min_lr: 0.000000  loss: 4.9816 (4.9230)  loss_scale: 131072.0000 (79233.2347)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0658  max mem: 15572
[2025-01-12 21:28:09,318] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5929
[2025-01-12 21:28:09,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:28:09,319] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 320/2809]  eta: 0:25:21  lr: 0.000020  min_lr: 0.000000  loss: 4.9054 (4.9222)  loss_scale: 65536.0000 (78806.5296)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1209  max mem: 15572
Epoch: [2]  [ 330/2809]  eta: 0:25:17  lr: 0.000020  min_lr: 0.000000  loss: 4.8715 (4.9210)  loss_scale: 65536.0000 (78405.6073)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1702  max mem: 15572
Epoch: [2]  [ 340/2809]  eta: 0:25:03  lr: 0.000020  min_lr: 0.000000  loss: 4.8488 (4.9198)  loss_scale: 65536.0000 (78028.1994)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1388  max mem: 15572
Epoch: [2]  [ 350/2809]  eta: 0:24:51  lr: 0.000020  min_lr: 0.000000  loss: 4.8466 (4.9196)  loss_scale: 65536.0000 (77672.2963)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0760  max mem: 15572
Epoch: [2]  [ 360/2809]  eta: 0:24:43  lr: 0.000020  min_lr: 0.000000  loss: 4.9178 (4.9196)  loss_scale: 65536.0000 (77336.1108)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1311  max mem: 15572
Epoch: [2]  [ 370/2809]  eta: 0:24:38  lr: 0.000020  min_lr: 0.000000  loss: 4.8991 (4.9177)  loss_scale: 65536.0000 (77018.0485)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1719  max mem: 15572
Epoch: [2]  [ 380/2809]  eta: 0:24:32  lr: 0.000020  min_lr: 0.000000  loss: 4.8928 (4.9171)  loss_scale: 65536.0000 (76716.6824)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1551  max mem: 15572
[2025-01-12 21:28:49,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=30, lr=[1.9400094349619698e-07, 1.9400094349619698e-07, 2.7714420499456714e-07, 2.7714420499456714e-07, 3.959202928493817e-07, 3.959202928493817e-07, 5.656004183562596e-07, 5.656004183562596e-07, 8.080005976517993e-07, 8.080005976517993e-07, 1.1542865680739992e-06, 1.1542865680739992e-06, 1.6489808115342846e-06, 1.6489808115342846e-06, 2.355686873620407e-06, 2.355686873620407e-06, 3.3652669623148667e-06, 3.3652669623148667e-06, 4.807524231878382e-06, 4.807524231878382e-06, 6.86789175982626e-06, 6.86789175982626e-06, 9.811273942608943e-06, 9.811273942608943e-06, 1.401610563229849e-05, 1.401610563229849e-05, 2.0023008046140703e-05, 2.0023008046140703e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:28:49,510] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=27.98507027920085, CurrSamplesPerSec=29.842024471674723, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [ 390/2809]  eta: 0:24:21  lr: 0.000020  min_lr: 0.000000  loss: 4.9028 (4.9168)  loss_scale: 65536.0000 (76430.7315)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1091  max mem: 15572
Epoch: [2]  [ 400/2809]  eta: 0:24:17  lr: 0.000020  min_lr: 0.000000  loss: 4.9028 (4.9176)  loss_scale: 65536.0000 (76159.0424)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1245  max mem: 15572
Epoch: [2]  [ 410/2809]  eta: 0:24:08  lr: 0.000020  min_lr: 0.000000  loss: 4.9013 (4.9161)  loss_scale: 65536.0000 (75900.5742)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1480  max mem: 15572
[2025-01-12 21:29:06,513] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6029
[2025-01-12 21:29:06,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 21:29:06,514] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 420/2809]  eta: 0:23:59  lr: 0.000020  min_lr: 0.000000  loss: 4.8742 (4.9166)  loss_scale: 32768.0000 (74876.0475)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1217  max mem: 15572
Epoch: [2]  [ 430/2809]  eta: 0:23:55  lr: 0.000020  min_lr: 0.000000  loss: 4.9590 (4.9162)  loss_scale: 32768.0000 (73899.0626)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1608  max mem: 15572
Epoch: [2]  [ 440/2809]  eta: 0:23:47  lr: 0.000020  min_lr: 0.000000  loss: 4.8703 (4.9145)  loss_scale: 32768.0000 (72966.3855)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1720  max mem: 15572
Epoch: [2]  [ 450/2809]  eta: 0:23:39  lr: 0.000020  min_lr: 0.000000  loss: 4.8926 (4.9149)  loss_scale: 32768.0000 (72075.0687)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1351  max mem: 15572
Epoch: [2]  [ 460/2809]  eta: 0:23:35  lr: 0.000020  min_lr: 0.000000  loss: 4.9141 (4.9141)  loss_scale: 32768.0000 (71222.4208)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1705  max mem: 15572
Epoch: [2]  [ 470/2809]  eta: 0:23:33  lr: 0.000020  min_lr: 0.000000  loss: 4.8844 (4.9135)  loss_scale: 32768.0000 (70405.9788)  weight_decay: 0.0500 (0.0500)  time: 0.6587  data: 0.2013  max mem: 15572
Epoch: [2]  [ 480/2809]  eta: 0:23:23  lr: 0.000020  min_lr: 0.000000  loss: 4.8816 (4.9139)  loss_scale: 32768.0000 (69623.4844)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1376  max mem: 15572
Epoch: [2]  [ 490/2809]  eta: 0:23:15  lr: 0.000020  min_lr: 0.000000  loss: 4.9980 (4.9155)  loss_scale: 32768.0000 (68872.8635)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0942  max mem: 15572
Epoch: [2]  [ 500/2809]  eta: 0:23:09  lr: 0.000020  min_lr: 0.000000  loss: 4.9328 (4.9151)  loss_scale: 32768.0000 (68152.2076)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1354  max mem: 15572
Epoch: [2]  [ 510/2809]  eta: 0:23:05  lr: 0.000020  min_lr: 0.000000  loss: 4.8678 (4.9132)  loss_scale: 32768.0000 (67459.7573)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1835  max mem: 15572
Epoch: [2]  [ 520/2809]  eta: 0:22:52  lr: 0.000020  min_lr: 0.000000  loss: 4.8441 (4.9121)  loss_scale: 32768.0000 (66793.8887)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1051  max mem: 15572
Epoch: [2]  [ 530/2809]  eta: 0:22:45  lr: 0.000021  min_lr: 0.000000  loss: 4.8396 (4.9109)  loss_scale: 32768.0000 (66153.0998)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0698  max mem: 15572
[2025-01-12 21:30:21,187] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:30:21,188] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 540/2809]  eta: 0:22:38  lr: 0.000021  min_lr: 0.000000  loss: 4.8469 (4.9116)  loss_scale: 32768.0000 (65596.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1252  max mem: 15572
Epoch: [2]  [ 550/2809]  eta: 0:22:29  lr: 0.000021  min_lr: 0.000000  loss: 4.9559 (4.9128)  loss_scale: 65536.0000 (65595.4701)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0922  max mem: 15572
Epoch: [2]  [ 560/2809]  eta: 0:22:21  lr: 0.000021  min_lr: 0.000000  loss: 4.9283 (4.9115)  loss_scale: 65536.0000 (65594.4100)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0924  max mem: 15572
Epoch: [2]  [ 570/2809]  eta: 0:22:17  lr: 0.000021  min_lr: 0.000000  loss: 4.8689 (4.9107)  loss_scale: 65536.0000 (65593.3870)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1553  max mem: 15572
Epoch: [2]  [ 580/2809]  eta: 0:22:09  lr: 0.000021  min_lr: 0.000000  loss: 4.8887 (4.9109)  loss_scale: 65536.0000 (65592.3993)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1533  max mem: 15572
Epoch: [2]  [ 590/2809]  eta: 0:22:03  lr: 0.000021  min_lr: 0.000000  loss: 4.9192 (4.9122)  loss_scale: 65536.0000 (65591.4450)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1295  max mem: 15572
Epoch: [2]  [ 600/2809]  eta: 0:21:57  lr: 0.000021  min_lr: 0.000000  loss: 4.9484 (4.9122)  loss_scale: 65536.0000 (65590.5225)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1492  max mem: 15572
Epoch: [2]  [ 610/2809]  eta: 0:21:56  lr: 0.000021  min_lr: 0.000000  loss: 4.8911 (4.9109)  loss_scale: 65536.0000 (65589.6301)  weight_decay: 0.0500 (0.0500)  time: 0.6685  data: 0.2116  max mem: 15572
Epoch: [2]  [ 620/2809]  eta: 0:21:50  lr: 0.000021  min_lr: 0.000000  loss: 4.8661 (4.9115)  loss_scale: 65536.0000 (65588.7665)  weight_decay: 0.0500 (0.0500)  time: 0.6605  data: 0.1968  max mem: 15572
Epoch: [2]  [ 630/2809]  eta: 0:21:43  lr: 0.000021  min_lr: 0.000000  loss: 4.8823 (4.9109)  loss_scale: 65536.0000 (65587.9303)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0982  max mem: 15572
Epoch: [2]  [ 640/2809]  eta: 0:21:39  lr: 0.000021  min_lr: 0.000000  loss: 4.8975 (4.9103)  loss_scale: 65536.0000 (65587.1201)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1419  max mem: 15572
Epoch: [2]  [ 650/2809]  eta: 0:21:33  lr: 0.000021  min_lr: 0.000000  loss: 4.9071 (4.9102)  loss_scale: 65536.0000 (65586.3349)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1764  max mem: 15572
Epoch: [2]  [ 660/2809]  eta: 0:21:27  lr: 0.000021  min_lr: 0.000000  loss: 4.8876 (4.9092)  loss_scale: 65536.0000 (65585.5734)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1376  max mem: 15572
[2025-01-12 21:31:39,357] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:31:39,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 670/2809]  eta: 0:21:24  lr: 0.000021  min_lr: 0.000000  loss: 4.8269 (4.9085)  loss_scale: 65536.0000 (65877.8420)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.1689  max mem: 15572
[2025-01-12 21:31:43,796] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6295
[2025-01-12 21:31:43,797] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:31:43,797] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 680/2809]  eta: 0:21:14  lr: 0.000021  min_lr: 0.000000  loss: 4.9009 (4.9100)  loss_scale: 65536.0000 (66450.2320)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1054  max mem: 15572
Epoch: [2]  [ 690/2809]  eta: 0:21:06  lr: 0.000021  min_lr: 0.000000  loss: 4.9043 (4.9102)  loss_scale: 65536.0000 (66437.0014)  weight_decay: 0.0500 (0.0500)  time: 0.4924  data: 0.0203  max mem: 15572
Epoch: [2]  [ 700/2809]  eta: 0:21:01  lr: 0.000021  min_lr: 0.000000  loss: 4.9031 (4.9098)  loss_scale: 65536.0000 (66424.1484)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0922  max mem: 15572
Epoch: [2]  [ 710/2809]  eta: 0:20:54  lr: 0.000021  min_lr: 0.000000  loss: 4.8791 (4.9086)  loss_scale: 65536.0000 (66411.6568)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1244  max mem: 15572
Epoch: [2]  [ 720/2809]  eta: 0:20:47  lr: 0.000021  min_lr: 0.000000  loss: 4.8384 (4.9078)  loss_scale: 65536.0000 (66399.5118)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0987  max mem: 15572
Epoch: [2]  [ 730/2809]  eta: 0:20:43  lr: 0.000021  min_lr: 0.000000  loss: 4.8847 (4.9072)  loss_scale: 65536.0000 (66387.6990)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1439  max mem: 15572
Epoch: [2]  [ 740/2809]  eta: 0:20:35  lr: 0.000021  min_lr: 0.000000  loss: 4.9264 (4.9082)  loss_scale: 65536.0000 (66376.2051)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1366  max mem: 15572
Epoch: [2]  [ 750/2809]  eta: 0:20:28  lr: 0.000021  min_lr: 0.000000  loss: 4.9347 (4.9083)  loss_scale: 65536.0000 (66365.0173)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0855  max mem: 15572
Epoch: [2]  [ 760/2809]  eta: 0:20:24  lr: 0.000021  min_lr: 0.000000  loss: 4.8749 (4.9077)  loss_scale: 65536.0000 (66354.1235)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1455  max mem: 15572
Epoch: [2]  [ 770/2809]  eta: 0:20:16  lr: 0.000021  min_lr: 0.000000  loss: 4.8749 (4.9067)  loss_scale: 65536.0000 (66343.5123)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1239  max mem: 15572
Epoch: [2]  [ 780/2809]  eta: 0:20:10  lr: 0.000021  min_lr: 0.000000  loss: 4.7919 (4.9048)  loss_scale: 65536.0000 (66333.1729)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0998  max mem: 15572
Epoch: [2]  [ 790/2809]  eta: 0:20:04  lr: 0.000021  min_lr: 0.000000  loss: 4.7849 (4.9041)  loss_scale: 65536.0000 (66323.0948)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1217  max mem: 15572
Epoch: [2]  [ 800/2809]  eta: 0:20:00  lr: 0.000021  min_lr: 0.000000  loss: 4.7867 (4.9030)  loss_scale: 65536.0000 (66313.2684)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1454  max mem: 15572
[2025-01-12 21:32:59,427] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:32:59,428] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 810/2809]  eta: 0:19:55  lr: 0.000021  min_lr: 0.000000  loss: 4.7773 (4.9020)  loss_scale: 65536.0000 (66707.7287)  weight_decay: 0.0500 (0.0500)  time: 0.6553  data: 0.1537  max mem: 15572
[2025-01-12 21:33:04,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6432
[2025-01-12 21:33:04,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:33:04,228] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 820/2809]  eta: 0:19:47  lr: 0.000021  min_lr: 0.000000  loss: 4.8445 (4.9020)  loss_scale: 65536.0000 (66932.9306)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0827  max mem: 15572
Epoch: [2]  [ 830/2809]  eta: 0:19:40  lr: 0.000022  min_lr: 0.000000  loss: 4.8445 (4.9009)  loss_scale: 65536.0000 (66916.1203)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0840  max mem: 15572
Epoch: [2]  [ 840/2809]  eta: 0:19:34  lr: 0.000022  min_lr: 0.000000  loss: 4.7387 (4.8997)  loss_scale: 65536.0000 (66899.7099)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1245  max mem: 15572
Epoch: [2]  [ 850/2809]  eta: 0:19:28  lr: 0.000022  min_lr: 0.000000  loss: 4.8404 (4.8997)  loss_scale: 65536.0000 (66883.6851)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1299  max mem: 15572
Epoch: [2]  [ 860/2809]  eta: 0:19:22  lr: 0.000022  min_lr: 0.000000  loss: 4.8822 (4.8996)  loss_scale: 65536.0000 (66868.0325)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1375  max mem: 15572
Epoch: [2]  [ 870/2809]  eta: 0:19:17  lr: 0.000022  min_lr: 0.000000  loss: 4.8690 (4.8990)  loss_scale: 65536.0000 (66852.7394)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1708  max mem: 15572
Epoch: [2]  [ 880/2809]  eta: 0:19:11  lr: 0.000022  min_lr: 0.000000  loss: 4.9291 (4.8990)  loss_scale: 65536.0000 (66837.7934)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1862  max mem: 15572
Epoch: [2]  [ 890/2809]  eta: 0:19:03  lr: 0.000022  min_lr: 0.000000  loss: 4.9292 (4.8989)  loss_scale: 65536.0000 (66823.1829)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1038  max mem: 15572
Epoch: [2]  [ 900/2809]  eta: 0:18:57  lr: 0.000022  min_lr: 0.000000  loss: 4.8916 (4.8990)  loss_scale: 65536.0000 (66808.8968)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0703  max mem: 15572
Epoch: [2]  [ 910/2809]  eta: 0:18:51  lr: 0.000022  min_lr: 0.000000  loss: 4.9033 (4.8989)  loss_scale: 65536.0000 (66794.9243)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1441  max mem: 15572
Epoch: [2]  [ 920/2809]  eta: 0:18:47  lr: 0.000022  min_lr: 0.000000  loss: 4.9033 (4.8990)  loss_scale: 65536.0000 (66781.2552)  weight_decay: 0.0500 (0.0500)  time: 0.6488  data: 0.2095  max mem: 15572
Epoch: [2]  [ 930/2809]  eta: 0:18:41  lr: 0.000022  min_lr: 0.000000  loss: 4.9418 (4.8988)  loss_scale: 65536.0000 (66767.8797)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.2009  max mem: 15572
Epoch: [2]  [ 940/2809]  eta: 0:18:36  lr: 0.000022  min_lr: 0.000000  loss: 4.8763 (4.8981)  loss_scale: 65536.0000 (66754.7885)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1476  max mem: 15572
[2025-01-12 21:34:21,008] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:34:21,008] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 950/2809]  eta: 0:18:29  lr: 0.000022  min_lr: 0.000000  loss: 4.8334 (4.8977)  loss_scale: 65536.0000 (67293.2744)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1319  max mem: 15572
[2025-01-12 21:34:25,744] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6569
[2025-01-12 21:34:25,744] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:34:25,744] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 960/2809]  eta: 0:18:24  lr: 0.000022  min_lr: 0.000000  loss: 4.7518 (4.8963)  loss_scale: 65536.0000 (67274.9886)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1685  max mem: 15572
Epoch: [2]  [ 970/2809]  eta: 0:18:19  lr: 0.000022  min_lr: 0.000000  loss: 4.7672 (4.8957)  loss_scale: 65536.0000 (67257.0793)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.2034  max mem: 15572
Epoch: [2]  [ 980/2809]  eta: 0:18:14  lr: 0.000022  min_lr: 0.000000  loss: 4.8581 (4.8954)  loss_scale: 65536.0000 (67239.5352)  weight_decay: 0.0500 (0.0500)  time: 0.6596  data: 0.2209  max mem: 15572
Epoch: [2]  [ 990/2809]  eta: 0:18:08  lr: 0.000022  min_lr: 0.000000  loss: 4.9023 (4.8957)  loss_scale: 65536.0000 (67222.3451)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.2101  max mem: 15572
Epoch: [2]  [1000/2809]  eta: 0:18:01  lr: 0.000022  min_lr: 0.000000  loss: 4.9421 (4.8958)  loss_scale: 65536.0000 (67205.4985)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1473  max mem: 15572
Epoch: [2]  [1010/2809]  eta: 0:17:55  lr: 0.000022  min_lr: 0.000000  loss: 4.9777 (4.8963)  loss_scale: 65536.0000 (67188.9852)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1331  max mem: 15572
Epoch: [2]  [1020/2809]  eta: 0:17:49  lr: 0.000022  min_lr: 0.000000  loss: 4.8275 (4.8953)  loss_scale: 65536.0000 (67172.7953)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1188  max mem: 15572
Epoch: [2]  [1030/2809]  eta: 0:17:43  lr: 0.000022  min_lr: 0.000000  loss: 4.7568 (4.8946)  loss_scale: 65536.0000 (67156.9195)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0902  max mem: 15572
Epoch: [2]  [1040/2809]  eta: 0:17:37  lr: 0.000022  min_lr: 0.000000  loss: 4.8268 (4.8946)  loss_scale: 65536.0000 (67141.3487)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1334  max mem: 15572
Epoch: [2]  [1050/2809]  eta: 0:17:29  lr: 0.000022  min_lr: 0.000000  loss: 4.8619 (4.8941)  loss_scale: 65536.0000 (67126.0742)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0906  max mem: 15572
Epoch: [2]  [1060/2809]  eta: 0:17:24  lr: 0.000022  min_lr: 0.000000  loss: 4.8519 (4.8937)  loss_scale: 65536.0000 (67111.0877)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1094  max mem: 15572
Epoch: [2]  [1070/2809]  eta: 0:17:20  lr: 0.000022  min_lr: 0.000000  loss: 4.8280 (4.8925)  loss_scale: 65536.0000 (67096.3810)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.2408  max mem: 15572
[2025-01-12 21:35:44,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:35:44,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1080/2809]  eta: 0:17:14  lr: 0.000022  min_lr: 0.000000  loss: 4.8373 (4.8922)  loss_scale: 65536.0000 (67142.5717)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.2013  max mem: 15572
Epoch: [2]  [1090/2809]  eta: 0:17:08  lr: 0.000022  min_lr: 0.000000  loss: 4.8517 (4.8907)  loss_scale: 131072.0000 (67728.5426)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1378  max mem: 15572
Epoch: [2]  [1100/2809]  eta: 0:17:02  lr: 0.000022  min_lr: 0.000000  loss: 4.8650 (4.8909)  loss_scale: 131072.0000 (68303.8692)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1399  max mem: 15572
Epoch: [2]  [1110/2809]  eta: 0:16:56  lr: 0.000022  min_lr: 0.000000  loss: 4.9127 (4.8902)  loss_scale: 131072.0000 (68868.8389)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1331  max mem: 15572
Epoch: [2]  [1120/2809]  eta: 0:16:48  lr: 0.000022  min_lr: 0.000000  loss: 4.8213 (4.8895)  loss_scale: 131072.0000 (69423.7288)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0741  max mem: 15572
[2025-01-12 21:36:08,145] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6740
[2025-01-12 21:36:08,146] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:36:08,146] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1130/2809]  eta: 0:16:43  lr: 0.000023  min_lr: 0.000000  loss: 4.8200 (4.8894)  loss_scale: 131072.0000 (69447.2997)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1038  max mem: 15572
Epoch: [2]  [1140/2809]  eta: 0:16:37  lr: 0.000023  min_lr: 0.000000  loss: 4.9093 (4.8895)  loss_scale: 65536.0000 (69413.0202)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1079  max mem: 15572
Epoch: [2]  [1150/2809]  eta: 0:16:32  lr: 0.000023  min_lr: 0.000000  loss: 4.8590 (4.8889)  loss_scale: 65536.0000 (69379.3362)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1519  max mem: 15572
Epoch: [2]  [1160/2809]  eta: 0:16:26  lr: 0.000023  min_lr: 0.000000  loss: 4.8184 (4.8888)  loss_scale: 65536.0000 (69346.2326)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.2189  max mem: 15572
Epoch: [2]  [1170/2809]  eta: 0:16:19  lr: 0.000023  min_lr: 0.000000  loss: 4.8621 (4.8884)  loss_scale: 65536.0000 (69313.6943)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.1054  max mem: 15572
Epoch: [2]  [1180/2809]  eta: 0:16:12  lr: 0.000023  min_lr: 0.000000  loss: 4.8735 (4.8884)  loss_scale: 65536.0000 (69281.7070)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0991  max mem: 15572
Epoch: [2]  [1190/2809]  eta: 0:16:07  lr: 0.000023  min_lr: 0.000000  loss: 4.8735 (4.8878)  loss_scale: 65536.0000 (69250.2569)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1773  max mem: 15572
Epoch: [2]  [1200/2809]  eta: 0:16:01  lr: 0.000023  min_lr: 0.000000  loss: 4.8292 (4.8875)  loss_scale: 65536.0000 (69219.3306)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1757  max mem: 15572
Epoch: [2]  [1210/2809]  eta: 0:15:56  lr: 0.000023  min_lr: 0.000000  loss: 4.8669 (4.8873)  loss_scale: 65536.0000 (69188.9149)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1620  max mem: 15572
Epoch: [2]  [1220/2809]  eta: 0:15:51  lr: 0.000023  min_lr: 0.000000  loss: 4.8669 (4.8870)  loss_scale: 65536.0000 (69158.9975)  weight_decay: 0.0500 (0.0500)  time: 0.6547  data: 0.2037  max mem: 15572
Epoch: [2]  [1230/2809]  eta: 0:15:44  lr: 0.000023  min_lr: 0.000000  loss: 4.9199 (4.8869)  loss_scale: 65536.0000 (69129.5662)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1670  max mem: 15572
Epoch: [2]  [1240/2809]  eta: 0:15:38  lr: 0.000023  min_lr: 0.000000  loss: 4.9199 (4.8865)  loss_scale: 65536.0000 (69100.6092)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1245  max mem: 15572
Epoch: [2]  [1250/2809]  eta: 0:15:32  lr: 0.000023  min_lr: 0.000000  loss: 4.8233 (4.8863)  loss_scale: 65536.0000 (69072.1151)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1525  max mem: 15572
[2025-01-12 21:37:26,808] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:37:26,808] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1260/2809]  eta: 0:15:26  lr: 0.000023  min_lr: 0.000000  loss: 4.8455 (4.8864)  loss_scale: 65536.0000 (69563.7875)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1584  max mem: 15572
[2025-01-12 21:37:32,721] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6879
[2025-01-12 21:37:32,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:37:32,721] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1270/2809]  eta: 0:15:20  lr: 0.000023  min_lr: 0.000000  loss: 4.8455 (4.8862)  loss_scale: 65536.0000 (69532.0976)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1715  max mem: 15572
Epoch: [2]  [1280/2809]  eta: 0:15:14  lr: 0.000023  min_lr: 0.000000  loss: 4.8563 (4.8860)  loss_scale: 65536.0000 (69500.9024)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1538  max mem: 15572
Epoch: [2]  [1290/2809]  eta: 0:15:08  lr: 0.000023  min_lr: 0.000000  loss: 4.8423 (4.8853)  loss_scale: 65536.0000 (69470.1905)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1286  max mem: 15572
Epoch: [2]  [1300/2809]  eta: 0:15:02  lr: 0.000023  min_lr: 0.000000  loss: 4.7573 (4.8844)  loss_scale: 65536.0000 (69439.9508)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1522  max mem: 15572
Epoch: [2]  [1310/2809]  eta: 0:14:56  lr: 0.000023  min_lr: 0.000000  loss: 4.8284 (4.8840)  loss_scale: 65536.0000 (69410.1724)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1645  max mem: 15572
Epoch: [2]  [1320/2809]  eta: 0:14:51  lr: 0.000023  min_lr: 0.000000  loss: 4.8870 (4.8841)  loss_scale: 65536.0000 (69380.8448)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.2193  max mem: 15572
Epoch: [2]  [1330/2809]  eta: 0:14:44  lr: 0.000023  min_lr: 0.000000  loss: 5.0063 (4.8849)  loss_scale: 65536.0000 (69351.9579)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1601  max mem: 15572
Epoch: [2]  [1340/2809]  eta: 0:14:37  lr: 0.000023  min_lr: 0.000000  loss: 4.8612 (4.8845)  loss_scale: 65536.0000 (69323.5019)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0503  max mem: 15572
Epoch: [2]  [1350/2809]  eta: 0:14:31  lr: 0.000023  min_lr: 0.000000  loss: 4.8264 (4.8843)  loss_scale: 65536.0000 (69295.4671)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1211  max mem: 15572
Epoch: [2]  [1360/2809]  eta: 0:14:25  lr: 0.000023  min_lr: 0.000000  loss: 4.8366 (4.8835)  loss_scale: 65536.0000 (69267.8442)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1559  max mem: 15572
Epoch: [2]  [1370/2809]  eta: 0:14:19  lr: 0.000023  min_lr: 0.000000  loss: 4.8196 (4.8831)  loss_scale: 65536.0000 (69240.6244)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1315  max mem: 15572
Epoch: [2]  [1380/2809]  eta: 0:14:13  lr: 0.000023  min_lr: 0.000000  loss: 4.8275 (4.8827)  loss_scale: 65536.0000 (69213.7987)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1413  max mem: 15572
[2025-01-12 21:38:43,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=36, lr=[2.263398238922958e-07, 2.263398238922958e-07, 3.233426055604226e-07, 3.233426055604226e-07, 4.619180079434609e-07, 4.619180079434609e-07, 6.598828684906585e-07, 6.598828684906585e-07, 9.426898121295122e-07, 9.426898121295122e-07, 1.346699731613589e-06, 1.346699731613589e-06, 1.923856759447984e-06, 1.923856759447984e-06, 2.748366799211406e-06, 2.748366799211406e-06, 3.926238284587723e-06, 3.926238284587723e-06, 5.60891183512532e-06, 5.60891183512532e-06, 8.012731193036171e-06, 8.012731193036171e-06, 1.1446758847194531e-05, 1.1446758847194531e-05, 1.635251263884933e-05, 1.635251263884933e-05, 2.336073234121333e-05, 2.336073234121333e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:38:43,357] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=27.975610059738596, CurrSamplesPerSec=26.965068173903727, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-12 21:38:49,131] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:38:49,131] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1390/2809]  eta: 0:14:07  lr: 0.000023  min_lr: 0.000000  loss: 4.8281 (4.8825)  loss_scale: 65536.0000 (69234.4730)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1759  max mem: 15572
[2025-01-12 21:38:50,070] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7010
[2025-01-12 21:38:50,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:38:50,071] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1400/2809]  eta: 0:14:02  lr: 0.000023  min_lr: 0.000000  loss: 4.8278 (4.8823)  loss_scale: 65536.0000 (69254.8522)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1933  max mem: 15572
Epoch: [2]  [1410/2809]  eta: 0:13:55  lr: 0.000023  min_lr: 0.000000  loss: 4.8222 (4.8821)  loss_scale: 65536.0000 (69228.4961)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1556  max mem: 15572
Epoch: [2]  [1420/2809]  eta: 0:13:49  lr: 0.000023  min_lr: 0.000000  loss: 4.8086 (4.8813)  loss_scale: 65536.0000 (69202.5109)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1525  max mem: 15572
Epoch: [2]  [1430/2809]  eta: 0:13:44  lr: 0.000024  min_lr: 0.000000  loss: 4.7492 (4.8802)  loss_scale: 65536.0000 (69176.8889)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1822  max mem: 15572
Epoch: [2]  [1440/2809]  eta: 0:13:39  lr: 0.000024  min_lr: 0.000000  loss: 4.7346 (4.8798)  loss_scale: 65536.0000 (69151.6225)  weight_decay: 0.0500 (0.0500)  time: 0.6970  data: 0.2145  max mem: 15572
Epoch: [2]  [1450/2809]  eta: 0:13:33  lr: 0.000024  min_lr: 0.000000  loss: 4.8763 (4.8797)  loss_scale: 65536.0000 (69126.7043)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1494  max mem: 15572
Epoch: [2]  [1460/2809]  eta: 0:13:27  lr: 0.000024  min_lr: 0.000000  loss: 4.8070 (4.8785)  loss_scale: 65536.0000 (69102.1273)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1139  max mem: 15572
Epoch: [2]  [1470/2809]  eta: 0:13:21  lr: 0.000024  min_lr: 0.000000  loss: 4.8390 (4.8788)  loss_scale: 65536.0000 (69077.8844)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1232  max mem: 15572
Epoch: [2]  [1480/2809]  eta: 0:13:16  lr: 0.000024  min_lr: 0.000000  loss: 4.8861 (4.8786)  loss_scale: 65536.0000 (69053.9689)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1457  max mem: 15572
Epoch: [2]  [1490/2809]  eta: 0:13:09  lr: 0.000024  min_lr: 0.000000  loss: 4.7952 (4.8776)  loss_scale: 65536.0000 (69030.3742)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1358  max mem: 15572
Epoch: [2]  [1500/2809]  eta: 0:13:03  lr: 0.000024  min_lr: 0.000000  loss: 4.7952 (4.8771)  loss_scale: 65536.0000 (69007.0939)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1318  max mem: 15572
Epoch: [2]  [1510/2809]  eta: 0:12:57  lr: 0.000024  min_lr: 0.000000  loss: 4.8425 (4.8772)  loss_scale: 65536.0000 (68984.1218)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1750  max mem: 15572
Epoch: [2]  [1520/2809]  eta: 0:12:51  lr: 0.000024  min_lr: 0.000000  loss: 4.8906 (4.8774)  loss_scale: 65536.0000 (68961.4517)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0945  max mem: 15572
[2025-01-12 21:40:08,476] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:40:08,476] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 21:40:09,631] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7141
[2025-01-12 21:40:09,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:40:09,632] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1530/2809]  eta: 0:12:45  lr: 0.000024  min_lr: 0.000000  loss: 4.8437 (4.8770)  loss_scale: 65536.0000 (69024.6897)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0885  max mem: 15572
Epoch: [2]  [1540/2809]  eta: 0:12:39  lr: 0.000024  min_lr: 0.000000  loss: 4.8437 (4.8772)  loss_scale: 65536.0000 (69002.0506)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1232  max mem: 15572
Epoch: [2]  [1550/2809]  eta: 0:12:32  lr: 0.000024  min_lr: 0.000000  loss: 4.8526 (4.8771)  loss_scale: 65536.0000 (68979.7034)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0950  max mem: 15572
Epoch: [2]  [1560/2809]  eta: 0:12:26  lr: 0.000024  min_lr: 0.000000  loss: 4.8184 (4.8765)  loss_scale: 65536.0000 (68957.6425)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0864  max mem: 15572
Epoch: [2]  [1570/2809]  eta: 0:12:20  lr: 0.000024  min_lr: 0.000000  loss: 4.7791 (4.8763)  loss_scale: 65536.0000 (68935.8625)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1448  max mem: 15572
Epoch: [2]  [1580/2809]  eta: 0:12:14  lr: 0.000024  min_lr: 0.000000  loss: 4.7998 (4.8758)  loss_scale: 65536.0000 (68914.3580)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1470  max mem: 15572
Epoch: [2]  [1590/2809]  eta: 0:12:08  lr: 0.000024  min_lr: 0.000000  loss: 4.8650 (4.8764)  loss_scale: 65536.0000 (68893.1238)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1365  max mem: 15572
Epoch: [2]  [1600/2809]  eta: 0:12:02  lr: 0.000024  min_lr: 0.000000  loss: 4.9435 (4.8760)  loss_scale: 65536.0000 (68872.1549)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1605  max mem: 15572
Epoch: [2]  [1610/2809]  eta: 0:11:56  lr: 0.000024  min_lr: 0.000000  loss: 4.8233 (4.8756)  loss_scale: 65536.0000 (68851.4463)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1837  max mem: 15572
Epoch: [2]  [1620/2809]  eta: 0:11:51  lr: 0.000024  min_lr: 0.000000  loss: 4.8152 (4.8749)  loss_scale: 65536.0000 (68830.9932)  weight_decay: 0.0500 (0.0500)  time: 0.6580  data: 0.2090  max mem: 15572
Epoch: [2]  [1630/2809]  eta: 0:11:45  lr: 0.000024  min_lr: 0.000000  loss: 4.7990 (4.8745)  loss_scale: 65536.0000 (68810.7909)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1611  max mem: 15572
Epoch: [2]  [1640/2809]  eta: 0:11:38  lr: 0.000024  min_lr: 0.000000  loss: 4.7393 (4.8740)  loss_scale: 65536.0000 (68790.8349)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0930  max mem: 15572
Epoch: [2]  [1650/2809]  eta: 0:11:32  lr: 0.000024  min_lr: 0.000000  loss: 4.7754 (4.8741)  loss_scale: 65536.0000 (68771.1205)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1100  max mem: 15572
[2025-01-12 21:41:25,670] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:41:25,670] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 21:41:30,215] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7276
[2025-01-12 21:41:30,216] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:41:30,216] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1660/2809]  eta: 0:11:26  lr: 0.000024  min_lr: 0.000000  loss: 4.7877 (4.8733)  loss_scale: 65536.0000 (68988.3781)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1788  max mem: 15572
Epoch: [2]  [1670/2809]  eta: 0:11:20  lr: 0.000024  min_lr: 0.000000  loss: 4.7933 (4.8729)  loss_scale: 65536.0000 (68967.7175)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1460  max mem: 15572
Epoch: [2]  [1680/2809]  eta: 0:11:14  lr: 0.000024  min_lr: 0.000000  loss: 4.7939 (4.8724)  loss_scale: 65536.0000 (68947.3028)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0988  max mem: 15572
Epoch: [2]  [1690/2809]  eta: 0:11:08  lr: 0.000024  min_lr: 0.000000  loss: 4.8805 (4.8728)  loss_scale: 65536.0000 (68927.1295)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1586  max mem: 15572
Epoch: [2]  [1700/2809]  eta: 0:11:02  lr: 0.000024  min_lr: 0.000000  loss: 4.8865 (4.8727)  loss_scale: 65536.0000 (68907.1934)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1897  max mem: 15572
Epoch: [2]  [1710/2809]  eta: 0:10:56  lr: 0.000024  min_lr: 0.000000  loss: 4.8510 (4.8727)  loss_scale: 65536.0000 (68887.4904)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.1829  max mem: 15572
Epoch: [2]  [1720/2809]  eta: 0:10:51  lr: 0.000024  min_lr: 0.000000  loss: 4.8650 (4.8727)  loss_scale: 65536.0000 (68868.0163)  weight_decay: 0.0500 (0.0500)  time: 0.6623  data: 0.1839  max mem: 15572
Epoch: [2]  [1730/2809]  eta: 0:10:44  lr: 0.000025  min_lr: 0.000000  loss: 4.8650 (4.8723)  loss_scale: 65536.0000 (68848.7672)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1157  max mem: 15572
Epoch: [2]  [1740/2809]  eta: 0:10:39  lr: 0.000025  min_lr: 0.000000  loss: 4.8011 (4.8716)  loss_scale: 65536.0000 (68829.7392)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1401  max mem: 15572
Epoch: [2]  [1750/2809]  eta: 0:10:32  lr: 0.000025  min_lr: 0.000000  loss: 4.7846 (4.8712)  loss_scale: 65536.0000 (68810.9286)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1403  max mem: 15572
Epoch: [2]  [1760/2809]  eta: 0:10:26  lr: 0.000025  min_lr: 0.000000  loss: 4.8559 (4.8713)  loss_scale: 65536.0000 (68792.3316)  weight_decay: 0.0500 (0.0500)  time: 0.5036  data: 0.0591  max mem: 15572
Epoch: [2]  [1770/2809]  eta: 0:10:20  lr: 0.000025  min_lr: 0.000000  loss: 4.8559 (4.8714)  loss_scale: 65536.0000 (68773.9447)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0906  max mem: 15572
Epoch: [2]  [1780/2809]  eta: 0:10:13  lr: 0.000025  min_lr: 0.000000  loss: 4.7712 (4.8713)  loss_scale: 65536.0000 (68755.7642)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.1142  max mem: 15572
[2025-01-12 21:42:43,111] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:42:43,111] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1790/2809]  eta: 0:10:07  lr: 0.000025  min_lr: 0.000000  loss: 4.7038 (4.8702)  loss_scale: 65536.0000 (68884.1541)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.1241  max mem: 15572
[2025-01-12 21:42:48,070] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7413
[2025-01-12 21:42:48,070] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:42:48,071] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1800/2809]  eta: 0:10:01  lr: 0.000025  min_lr: 0.000000  loss: 4.7692 (4.8700)  loss_scale: 65536.0000 (69011.1183)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1519  max mem: 15572
Epoch: [2]  [1810/2809]  eta: 0:09:55  lr: 0.000025  min_lr: 0.000000  loss: 4.8487 (4.8701)  loss_scale: 65536.0000 (68991.9293)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1442  max mem: 15572
Epoch: [2]  [1820/2809]  eta: 0:09:49  lr: 0.000025  min_lr: 0.000000  loss: 4.8388 (4.8694)  loss_scale: 65536.0000 (68972.9511)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.1007  max mem: 15572
Epoch: [2]  [1830/2809]  eta: 0:09:43  lr: 0.000025  min_lr: 0.000000  loss: 4.7185 (4.8685)  loss_scale: 65536.0000 (68954.1802)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1213  max mem: 15572
Epoch: [2]  [1840/2809]  eta: 0:09:37  lr: 0.000025  min_lr: 0.000000  loss: 4.7475 (4.8680)  loss_scale: 65536.0000 (68935.6133)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1987  max mem: 15572
Epoch: [2]  [1850/2809]  eta: 0:09:31  lr: 0.000025  min_lr: 0.000000  loss: 4.7890 (4.8677)  loss_scale: 65536.0000 (68917.2469)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.2121  max mem: 15572
Epoch: [2]  [1860/2809]  eta: 0:09:26  lr: 0.000025  min_lr: 0.000000  loss: 4.7570 (4.8674)  loss_scale: 65536.0000 (68899.0779)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1992  max mem: 15572
Epoch: [2]  [1870/2809]  eta: 0:09:20  lr: 0.000025  min_lr: 0.000000  loss: 4.7570 (4.8668)  loss_scale: 65536.0000 (68881.1032)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1848  max mem: 15572
Epoch: [2]  [1880/2809]  eta: 0:09:14  lr: 0.000025  min_lr: 0.000000  loss: 4.7675 (4.8666)  loss_scale: 65536.0000 (68863.3195)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1651  max mem: 15572
Epoch: [2]  [1890/2809]  eta: 0:09:08  lr: 0.000025  min_lr: 0.000000  loss: 4.8321 (4.8665)  loss_scale: 65536.0000 (68845.7240)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1806  max mem: 15572
Epoch: [2]  [1900/2809]  eta: 0:09:02  lr: 0.000025  min_lr: 0.000000  loss: 4.8685 (4.8659)  loss_scale: 65536.0000 (68828.3135)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1705  max mem: 15572
Epoch: [2]  [1910/2809]  eta: 0:08:56  lr: 0.000025  min_lr: 0.000000  loss: 4.8246 (4.8655)  loss_scale: 65536.0000 (68811.0853)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.1833  max mem: 15572
Epoch: [2]  [1920/2809]  eta: 0:08:50  lr: 0.000025  min_lr: 0.000000  loss: 4.8074 (4.8654)  loss_scale: 65536.0000 (68794.0364)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1869  max mem: 15572
[2025-01-12 21:44:06,768] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:44:06,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 21:44:10,707] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7547
[2025-01-12 21:44:10,708] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:44:10,708] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1930/2809]  eta: 0:08:44  lr: 0.000025  min_lr: 0.000000  loss: 4.8126 (4.8651)  loss_scale: 65536.0000 (68946.8586)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1709  max mem: 15572
Epoch: [2]  [1940/2809]  eta: 0:08:39  lr: 0.000025  min_lr: 0.000000  loss: 4.8405 (4.8651)  loss_scale: 65536.0000 (68929.2859)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.2103  max mem: 15572
Epoch: [2]  [1950/2809]  eta: 0:08:33  lr: 0.000025  min_lr: 0.000000  loss: 4.8089 (4.8643)  loss_scale: 65536.0000 (68911.8934)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1500  max mem: 15572
Epoch: [2]  [1960/2809]  eta: 0:08:26  lr: 0.000025  min_lr: 0.000000  loss: 4.7976 (4.8643)  loss_scale: 65536.0000 (68894.6782)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0687  max mem: 15572
Epoch: [2]  [1970/2809]  eta: 0:08:21  lr: 0.000025  min_lr: 0.000000  loss: 4.8803 (4.8642)  loss_scale: 65536.0000 (68877.6377)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1483  max mem: 15572
Epoch: [2]  [1980/2809]  eta: 0:08:15  lr: 0.000025  min_lr: 0.000000  loss: 4.7697 (4.8637)  loss_scale: 65536.0000 (68860.7693)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1575  max mem: 15572
Epoch: [2]  [1990/2809]  eta: 0:08:09  lr: 0.000025  min_lr: 0.000000  loss: 4.8323 (4.8635)  loss_scale: 65536.0000 (68844.0703)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1062  max mem: 15572
Epoch: [2]  [2000/2809]  eta: 0:08:03  lr: 0.000025  min_lr: 0.000000  loss: 4.8527 (4.8631)  loss_scale: 65536.0000 (68827.5382)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1191  max mem: 15572
[2025-01-12 21:44:57,986] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7627
[2025-01-12 21:44:57,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 21:44:57,987] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [2010/2809]  eta: 0:07:56  lr: 0.000025  min_lr: 0.000000  loss: 4.8237 (4.8628)  loss_scale: 65536.0000 (68778.5818)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0713  max mem: 15572
Epoch: [2]  [2020/2809]  eta: 0:07:50  lr: 0.000025  min_lr: 0.000000  loss: 4.8237 (4.8625)  loss_scale: 32768.0000 (68600.3998)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0670  max mem: 15572
Epoch: [2]  [2030/2809]  eta: 0:07:44  lr: 0.000026  min_lr: 0.000000  loss: 4.7358 (4.8622)  loss_scale: 32768.0000 (68423.9724)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0714  max mem: 15572
Epoch: [2]  [2040/2809]  eta: 0:07:38  lr: 0.000026  min_lr: 0.000000  loss: 4.8377 (4.8623)  loss_scale: 32768.0000 (68249.2739)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0793  max mem: 15572
Epoch: [2]  [2050/2809]  eta: 0:07:32  lr: 0.000026  min_lr: 0.000000  loss: 4.8408 (4.8622)  loss_scale: 32768.0000 (68076.2789)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1390  max mem: 15572
Epoch: [2]  [2060/2809]  eta: 0:07:26  lr: 0.000026  min_lr: 0.000000  loss: 4.8333 (4.8622)  loss_scale: 32768.0000 (67904.9626)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1595  max mem: 15572
Epoch: [2]  [2070/2809]  eta: 0:07:20  lr: 0.000026  min_lr: 0.000000  loss: 4.7660 (4.8616)  loss_scale: 32768.0000 (67735.3008)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1203  max mem: 15572
Epoch: [2]  [2080/2809]  eta: 0:07:14  lr: 0.000026  min_lr: 0.000000  loss: 4.7527 (4.8611)  loss_scale: 32768.0000 (67567.2696)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1244  max mem: 15572
Epoch: [2]  [2090/2809]  eta: 0:07:08  lr: 0.000026  min_lr: 0.000000  loss: 4.8679 (4.8613)  loss_scale: 32768.0000 (67400.8455)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1532  max mem: 15572
Epoch: [2]  [2100/2809]  eta: 0:07:02  lr: 0.000026  min_lr: 0.000000  loss: 4.8265 (4.8607)  loss_scale: 32768.0000 (67236.0057)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1723  max mem: 15572
Epoch: [2]  [2110/2809]  eta: 0:06:56  lr: 0.000026  min_lr: 0.000000  loss: 4.8062 (4.8608)  loss_scale: 32768.0000 (67072.7276)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1697  max mem: 15572
Epoch: [2]  [2120/2809]  eta: 0:06:50  lr: 0.000026  min_lr: 0.000000  loss: 4.8470 (4.8604)  loss_scale: 32768.0000 (66910.9892)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.1017  max mem: 15572
Epoch: [2]  [2130/2809]  eta: 0:06:44  lr: 0.000026  min_lr: 0.000000  loss: 4.7227 (4.8596)  loss_scale: 32768.0000 (66750.7687)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0858  max mem: 15572
[2025-01-12 21:46:11,792] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:46:11,793] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [2140/2809]  eta: 0:06:38  lr: 0.000026  min_lr: 0.000000  loss: 4.7178 (4.8594)  loss_scale: 32768.0000 (66637.9598)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0913  max mem: 15572
[2025-01-12 21:46:14,911] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7763
[2025-01-12 21:46:14,911] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 21:46:14,911] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [2150/2809]  eta: 0:06:32  lr: 0.000026  min_lr: 0.000000  loss: 4.8508 (4.8594)  loss_scale: 32768.0000 (66541.4338)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1427  max mem: 15572
Epoch: [2]  [2160/2809]  eta: 0:06:26  lr: 0.000026  min_lr: 0.000000  loss: 4.7956 (4.8589)  loss_scale: 32768.0000 (66385.1476)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1569  max mem: 15572
Epoch: [2]  [2170/2809]  eta: 0:06:20  lr: 0.000026  min_lr: 0.000000  loss: 4.7482 (4.8585)  loss_scale: 32768.0000 (66230.3012)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1146  max mem: 15572
Epoch: [2]  [2180/2809]  eta: 0:06:14  lr: 0.000026  min_lr: 0.000000  loss: 4.7630 (4.8585)  loss_scale: 32768.0000 (66076.8748)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1232  max mem: 15572
Epoch: [2]  [2190/2809]  eta: 0:06:08  lr: 0.000026  min_lr: 0.000000  loss: 4.8726 (4.8586)  loss_scale: 32768.0000 (65924.8489)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1628  max mem: 15572
Epoch: [2]  [2200/2809]  eta: 0:06:02  lr: 0.000026  min_lr: 0.000000  loss: 4.7837 (4.8584)  loss_scale: 32768.0000 (65774.2045)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1669  max mem: 15572
Epoch: [2]  [2210/2809]  eta: 0:05:56  lr: 0.000026  min_lr: 0.000000  loss: 4.7642 (4.8582)  loss_scale: 32768.0000 (65624.9227)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1242  max mem: 15572
Epoch: [2]  [2220/2809]  eta: 0:05:50  lr: 0.000026  min_lr: 0.000000  loss: 4.7746 (4.8582)  loss_scale: 32768.0000 (65476.9851)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1504  max mem: 15572
Epoch: [2]  [2230/2809]  eta: 0:05:44  lr: 0.000026  min_lr: 0.000000  loss: 4.7300 (4.8575)  loss_scale: 32768.0000 (65330.3738)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1467  max mem: 15572
Epoch: [2]  [2240/2809]  eta: 0:05:38  lr: 0.000026  min_lr: 0.000000  loss: 4.7300 (4.8571)  loss_scale: 32768.0000 (65185.0710)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.1029  max mem: 15572
Epoch: [2]  [2250/2809]  eta: 0:05:32  lr: 0.000026  min_lr: 0.000000  loss: 4.8214 (4.8569)  loss_scale: 32768.0000 (65041.0591)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1605  max mem: 15572
Epoch: [2]  [2260/2809]  eta: 0:05:26  lr: 0.000026  min_lr: 0.000000  loss: 4.8024 (4.8568)  loss_scale: 32768.0000 (64898.3211)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.2241  max mem: 15572
Epoch: [2]  [2270/2809]  eta: 0:05:20  lr: 0.000026  min_lr: 0.000000  loss: 4.7329 (4.8561)  loss_scale: 32768.0000 (64756.8402)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1274  max mem: 15572
[2025-01-12 21:47:30,941] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:47:30,942] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [2280/2809]  eta: 0:05:14  lr: 0.000026  min_lr: 0.000000  loss: 4.7162 (4.8557)  loss_scale: 32768.0000 (64717.1591)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0767  max mem: 15572
Epoch: [2]  [2290/2809]  eta: 0:05:08  lr: 0.000026  min_lr: 0.000000  loss: 4.8132 (4.8557)  loss_scale: 65536.0000 (64720.7333)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1655  max mem: 15572
Epoch: [2]  [2300/2809]  eta: 0:05:02  lr: 0.000026  min_lr: 0.000000  loss: 4.8132 (4.8553)  loss_scale: 65536.0000 (64724.2764)  weight_decay: 0.0500 (0.0500)  time: 0.6496  data: 0.2260  max mem: 15572
Epoch: [2]  [2310/2809]  eta: 0:04:56  lr: 0.000026  min_lr: 0.000000  loss: 4.8171 (4.8552)  loss_scale: 65536.0000 (64727.7888)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.2022  max mem: 15572
Epoch: [2]  [2320/2809]  eta: 0:04:51  lr: 0.000026  min_lr: 0.000000  loss: 4.8299 (4.8553)  loss_scale: 65536.0000 (64731.2710)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.2019  max mem: 15572
Epoch: [2]  [2330/2809]  eta: 0:04:45  lr: 0.000027  min_lr: 0.000000  loss: 4.8892 (4.8555)  loss_scale: 65536.0000 (64734.7233)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.2074  max mem: 15572
Epoch: [2]  [2340/2809]  eta: 0:04:39  lr: 0.000027  min_lr: 0.000000  loss: 4.8892 (4.8555)  loss_scale: 65536.0000 (64738.1461)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1481  max mem: 15572
Epoch: [2]  [2350/2809]  eta: 0:04:33  lr: 0.000027  min_lr: 0.000000  loss: 4.8689 (4.8556)  loss_scale: 65536.0000 (64741.5398)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0898  max mem: 15572
Epoch: [2]  [2360/2809]  eta: 0:04:27  lr: 0.000027  min_lr: 0.000000  loss: 4.8415 (4.8555)  loss_scale: 65536.0000 (64744.9047)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0837  max mem: 15572
Epoch: [2]  [2370/2809]  eta: 0:04:21  lr: 0.000027  min_lr: 0.000000  loss: 4.7484 (4.8549)  loss_scale: 65536.0000 (64748.2412)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1667  max mem: 15572
Epoch: [2]  [2380/2809]  eta: 0:04:15  lr: 0.000027  min_lr: 0.000000  loss: 4.7326 (4.8546)  loss_scale: 65536.0000 (64751.5498)  weight_decay: 0.0500 (0.0500)  time: 0.6612  data: 0.1806  max mem: 15572
[2025-01-12 21:48:37,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=43, lr=[2.5867870428839465e-07, 2.5867870428839465e-07, 3.695410061262781e-07, 3.695410061262781e-07, 5.279157230375402e-07, 5.279157230375402e-07, 7.541653186250575e-07, 7.541653186250575e-07, 1.077379026607225e-06, 1.077379026607225e-06, 1.5391128951531786e-06, 1.5391128951531786e-06, 2.198732707361684e-06, 2.198732707361684e-06, 3.141046724802406e-06, 3.141046724802406e-06, 4.48720960686058e-06, 4.48720960686058e-06, 6.4102994383722576e-06, 6.4102994383722576e-06, 9.157570626246083e-06, 9.157570626246083e-06, 1.3082243751780119e-05, 1.3082243751780119e-05, 1.868891964540017e-05, 1.868891964540017e-05, 2.669845663628596e-05, 2.669845663628596e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:48:37,189] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=28.003400498515926, CurrSamplesPerSec=31.77135974821091, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [2390/2809]  eta: 0:04:09  lr: 0.000027  min_lr: 0.000000  loss: 4.7195 (4.8540)  loss_scale: 65536.0000 (64754.8306)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1334  max mem: 15572
Epoch: [2]  [2400/2809]  eta: 0:04:03  lr: 0.000027  min_lr: 0.000000  loss: 4.7195 (4.8536)  loss_scale: 65536.0000 (64758.0841)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0738  max mem: 15572
[2025-01-12 21:48:48,449] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:48:48,450] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 21:48:50,788] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8025
[2025-01-12 21:48:50,789] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:48:50,789] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2410/2809]  eta: 0:03:57  lr: 0.000027  min_lr: 0.000000  loss: 4.7342 (4.8531)  loss_scale: 65536.0000 (64897.2211)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0784  max mem: 15572
Epoch: [2]  [2420/2809]  eta: 0:03:51  lr: 0.000027  min_lr: 0.000000  loss: 4.8210 (4.8528)  loss_scale: 65536.0000 (64899.8596)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1970  max mem: 15572
Epoch: [2]  [2430/2809]  eta: 0:03:45  lr: 0.000027  min_lr: 0.000000  loss: 4.8342 (4.8525)  loss_scale: 65536.0000 (64902.4763)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1867  max mem: 15572
Epoch: [2]  [2440/2809]  eta: 0:03:39  lr: 0.000027  min_lr: 0.000000  loss: 4.7208 (4.8522)  loss_scale: 65536.0000 (64905.0717)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1674  max mem: 15572
[2025-01-12 21:49:17,684] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8068
[2025-01-12 21:49:17,685] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 21:49:17,686] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [2450/2809]  eta: 0:03:33  lr: 0.000027  min_lr: 0.000000  loss: 4.8032 (4.8522)  loss_scale: 65536.0000 (64894.2766)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1519  max mem: 15572
Epoch: [2]  [2460/2809]  eta: 0:03:27  lr: 0.000027  min_lr: 0.000000  loss: 4.8108 (4.8517)  loss_scale: 32768.0000 (64763.7351)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1085  max mem: 15572
Epoch: [2]  [2470/2809]  eta: 0:03:21  lr: 0.000027  min_lr: 0.000000  loss: 4.7842 (4.8516)  loss_scale: 32768.0000 (64634.2501)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1154  max mem: 15572
Epoch: [2]  [2480/2809]  eta: 0:03:15  lr: 0.000027  min_lr: 0.000000  loss: 4.7794 (4.8515)  loss_scale: 32768.0000 (64505.8089)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0752  max mem: 15572
Epoch: [2]  [2490/2809]  eta: 0:03:09  lr: 0.000027  min_lr: 0.000000  loss: 4.7794 (4.8512)  loss_scale: 32768.0000 (64378.3990)  weight_decay: 0.0500 (0.0500)  time: 0.4565  data: 0.0302  max mem: 15572
Epoch: [2]  [2500/2809]  eta: 0:03:03  lr: 0.000027  min_lr: 0.000000  loss: 4.7519 (4.8510)  loss_scale: 32768.0000 (64252.0080)  weight_decay: 0.0500 (0.0500)  time: 0.4142  data: 0.0005  max mem: 15572
Epoch: [2]  [2510/2809]  eta: 0:02:57  lr: 0.000027  min_lr: 0.000000  loss: 4.8407 (4.8510)  loss_scale: 32768.0000 (64126.6237)  weight_decay: 0.0500 (0.0500)  time: 0.4425  data: 0.0006  max mem: 15572
Epoch: [2]  [2520/2809]  eta: 0:02:51  lr: 0.000027  min_lr: 0.000000  loss: 4.7974 (4.8504)  loss_scale: 32768.0000 (64002.2340)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0010  max mem: 15572
Epoch: [2]  [2530/2809]  eta: 0:02:45  lr: 0.000027  min_lr: 0.000000  loss: 4.7852 (4.8501)  loss_scale: 32768.0000 (63878.8273)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.0964  max mem: 15572
Epoch: [2]  [2540/2809]  eta: 0:02:39  lr: 0.000027  min_lr: 0.000000  loss: 4.8211 (4.8500)  loss_scale: 32768.0000 (63756.3920)  weight_decay: 0.0500 (0.0500)  time: 0.6762  data: 0.2015  max mem: 15572
Epoch: [2]  [2550/2809]  eta: 0:02:33  lr: 0.000027  min_lr: 0.000000  loss: 4.8409 (4.8498)  loss_scale: 32768.0000 (63634.9165)  weight_decay: 0.0500 (0.0500)  time: 0.6900  data: 0.2090  max mem: 15572
Epoch: [2]  [2560/2809]  eta: 0:02:27  lr: 0.000027  min_lr: 0.000000  loss: 4.7571 (4.8493)  loss_scale: 32768.0000 (63514.3897)  weight_decay: 0.0500 (0.0500)  time: 0.7068  data: 0.1970  max mem: 15572
Epoch: [2]  [2570/2809]  eta: 0:02:22  lr: 0.000027  min_lr: 0.000000  loss: 4.6736 (4.8489)  loss_scale: 32768.0000 (63394.8005)  weight_decay: 0.0500 (0.0500)  time: 0.6990  data: 0.2046  max mem: 15572
[2025-01-12 21:50:33,381] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:50:33,382] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [2580/2809]  eta: 0:02:16  lr: 0.000027  min_lr: 0.000000  loss: 4.7574 (4.8488)  loss_scale: 32768.0000 (63301.5296)  weight_decay: 0.0500 (0.0500)  time: 0.6837  data: 0.2071  max mem: 15572
Epoch: [2]  [2590/2809]  eta: 0:02:10  lr: 0.000027  min_lr: 0.000000  loss: 4.7545 (4.8481)  loss_scale: 65536.0000 (63310.1536)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.1698  max mem: 15572
Epoch: [2]  [2600/2809]  eta: 0:02:04  lr: 0.000027  min_lr: 0.000000  loss: 4.6863 (4.8479)  loss_scale: 65536.0000 (63318.7113)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.1880  max mem: 15572
Epoch: [2]  [2610/2809]  eta: 0:01:58  lr: 0.000027  min_lr: 0.000000  loss: 4.7714 (4.8479)  loss_scale: 65536.0000 (63327.2034)  weight_decay: 0.0500 (0.0500)  time: 0.7177  data: 0.2611  max mem: 15572
Epoch: [2]  [2620/2809]  eta: 0:01:52  lr: 0.000027  min_lr: 0.000000  loss: 4.7714 (4.8477)  loss_scale: 65536.0000 (63335.6307)  weight_decay: 0.0500 (0.0500)  time: 0.7189  data: 0.2776  max mem: 15572
Epoch: [2]  [2630/2809]  eta: 0:01:46  lr: 0.000028  min_lr: 0.000000  loss: 4.8172 (4.8476)  loss_scale: 65536.0000 (63343.9939)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.2395  max mem: 15572
Epoch: [2]  [2640/2809]  eta: 0:01:40  lr: 0.000028  min_lr: 0.000000  loss: 4.7919 (4.8472)  loss_scale: 65536.0000 (63352.2938)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.1088  max mem: 15572
Epoch: [2]  [2650/2809]  eta: 0:01:34  lr: 0.000028  min_lr: 0.000000  loss: 4.7459 (4.8471)  loss_scale: 65536.0000 (63360.5311)  weight_decay: 0.0500 (0.0500)  time: 0.4110  data: 0.0005  max mem: 15572
Epoch: [2]  [2660/2809]  eta: 0:01:28  lr: 0.000028  min_lr: 0.000000  loss: 4.7421 (4.8468)  loss_scale: 65536.0000 (63368.7065)  weight_decay: 0.0500 (0.0500)  time: 0.4354  data: 0.0008  max mem: 15572
Epoch: [2]  [2670/2809]  eta: 0:01:22  lr: 0.000028  min_lr: 0.000000  loss: 4.7264 (4.8468)  loss_scale: 65536.0000 (63376.8207)  weight_decay: 0.0500 (0.0500)  time: 0.4614  data: 0.0262  max mem: 15572
Epoch: [2]  [2680/2809]  eta: 0:01:16  lr: 0.000028  min_lr: 0.000000  loss: 4.7488 (4.8465)  loss_scale: 65536.0000 (63384.8743)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0936  max mem: 15572
Epoch: [2]  [2690/2809]  eta: 0:01:10  lr: 0.000028  min_lr: 0.000000  loss: 4.7119 (4.8459)  loss_scale: 65536.0000 (63392.8681)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1562  max mem: 15572
Epoch: [2]  [2700/2809]  eta: 0:01:04  lr: 0.000028  min_lr: 0.000000  loss: 4.6398 (4.8454)  loss_scale: 65536.0000 (63400.8027)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1504  max mem: 15572
[2025-01-12 21:51:47,372] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:51:47,372] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [2710/2809]  eta: 0:00:58  lr: 0.000028  min_lr: 0.000000  loss: 4.7002 (4.8453)  loss_scale: 65536.0000 (63505.3751)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1687  max mem: 15572
[2025-01-12 21:51:54,751] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8334
[2025-01-12 21:51:54,752] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:51:54,752] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2720/2809]  eta: 0:00:52  lr: 0.000028  min_lr: 0.000000  loss: 4.7178 (4.8448)  loss_scale: 65536.0000 (63633.2642)  weight_decay: 0.0500 (0.0500)  time: 0.6806  data: 0.2259  max mem: 15572
Epoch: [2]  [2730/2809]  eta: 0:00:46  lr: 0.000028  min_lr: 0.000000  loss: 4.6612 (4.8441)  loss_scale: 65536.0000 (63640.2314)  weight_decay: 0.0500 (0.0500)  time: 0.6751  data: 0.1788  max mem: 15572
Epoch: [2]  [2740/2809]  eta: 0:00:41  lr: 0.000028  min_lr: 0.000000  loss: 4.6925 (4.8438)  loss_scale: 65536.0000 (63647.1478)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1163  max mem: 15572
Epoch: [2]  [2750/2809]  eta: 0:00:35  lr: 0.000028  min_lr: 0.000000  loss: 4.6929 (4.8435)  loss_scale: 65536.0000 (63654.0138)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1650  max mem: 15572
Epoch: [2]  [2760/2809]  eta: 0:00:29  lr: 0.000028  min_lr: 0.000000  loss: 4.8138 (4.8435)  loss_scale: 65536.0000 (63660.8301)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1734  max mem: 15572
Epoch: [2]  [2770/2809]  eta: 0:00:23  lr: 0.000028  min_lr: 0.000000  loss: 4.7892 (4.8432)  loss_scale: 65536.0000 (63667.5973)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0986  max mem: 15572
Epoch: [2]  [2780/2809]  eta: 0:00:17  lr: 0.000028  min_lr: 0.000000  loss: 4.7239 (4.8429)  loss_scale: 65536.0000 (63674.3157)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0636  max mem: 15572
Epoch: [2]  [2790/2809]  eta: 0:00:11  lr: 0.000028  min_lr: 0.000000  loss: 4.7583 (4.8426)  loss_scale: 65536.0000 (63680.9860)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0306  max mem: 15572
Epoch: [2]  [2800/2809]  eta: 0:00:05  lr: 0.000028  min_lr: 0.000000  loss: 4.7190 (4.8422)  loss_scale: 65536.0000 (63687.6087)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0719  max mem: 15572
Epoch: [2]  [2808/2809]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000000  loss: 4.7298 (4.8420)  loss_scale: 65536.0000 (63692.8729)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0717  max mem: 15572
Epoch: [2] Total time: 0:27:49 (0.5943 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000000  loss: 4.7298 (4.8420)  loss_scale: 65536.0000 (63692.8729)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:24:43  loss: 3.9638 (3.9638)  acc1: 0.0000 (0.0000)  acc5: 38.8889 (38.8889)  time: 5.4530  data: 5.2575  max mem: 15572
Val:  [ 10/272]  eta: 0:03:34  loss: 5.1801 (4.7569)  acc1: 0.0000 (13.6364)  acc5: 0.0000 (20.7071)  time: 0.8199  data: 0.6257  max mem: 15572
Val:  [ 20/272]  eta: 0:02:23  loss: 4.8175 (4.7032)  acc1: 0.0000 (11.3757)  acc5: 0.0000 (17.9894)  time: 0.3269  data: 0.1351  max mem: 15572
Val:  [ 30/272]  eta: 0:01:54  loss: 4.6363 (4.6469)  acc1: 0.0000 (8.0645)  acc5: 0.0000 (18.6380)  time: 0.2855  data: 0.0907  max mem: 15572
Val:  [ 40/272]  eta: 0:01:44  loss: 4.1763 (4.4920)  acc1: 0.0000 (10.1626)  acc5: 27.7778 (28.1843)  time: 0.3203  data: 0.1148  max mem: 15572
Val:  [ 50/272]  eta: 0:01:35  loss: 3.9988 (4.4780)  acc1: 0.0000 (9.2593)  acc5: 66.6667 (30.3922)  time: 0.3555  data: 0.1446  max mem: 15572
Val:  [ 60/272]  eta: 0:01:26  loss: 4.0394 (4.4284)  acc1: 0.0000 (8.2878)  acc5: 27.7778 (31.4208)  time: 0.3211  data: 0.1161  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 4.1554 (4.3770)  acc1: 0.0000 (7.6682)  acc5: 27.7778 (31.6119)  time: 0.2770  data: 0.0810  max mem: 15572
Val:  [ 80/272]  eta: 0:01:12  loss: 4.1901 (4.3725)  acc1: 0.0000 (10.1509)  acc5: 0.0000 (31.6187)  time: 0.2924  data: 0.1028  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 4.8854 (4.4369)  acc1: 0.0000 (9.0354)  acc5: 0.0000 (28.1441)  time: 0.3464  data: 0.1628  max mem: 15572
Val:  [100/272]  eta: 0:01:04  loss: 4.8880 (4.4876)  acc1: 0.0000 (8.7459)  acc5: 0.0000 (26.7327)  time: 0.3472  data: 0.1626  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 4.8639 (4.5253)  acc1: 0.0000 (7.9580)  acc5: 0.0000 (24.4745)  time: 0.3319  data: 0.1378  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 4.8307 (4.5625)  acc1: 0.0000 (7.3003)  acc5: 0.0000 (22.5895)  time: 0.3255  data: 0.1162  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 4.8305 (4.5219)  acc1: 0.0000 (8.5666)  acc5: 0.0000 (25.2332)  time: 0.3439  data: 0.1444  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 4.2857 (4.5036)  acc1: 0.0000 (9.7715)  acc5: 5.5556 (25.3743)  time: 0.3240  data: 0.1319  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 4.4084 (4.5098)  acc1: 0.0000 (9.1244)  acc5: 0.0000 (24.0618)  time: 0.3099  data: 0.1213  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 4.3186 (4.4976)  acc1: 0.0000 (9.2823)  acc5: 11.1111 (25.0863)  time: 0.3422  data: 0.1725  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.4173 (4.5247)  acc1: 0.0000 (8.8694)  acc5: 0.0000 (23.8142)  time: 0.2883  data: 0.1168  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 4.8201 (4.5278)  acc1: 0.0000 (8.3794)  acc5: 0.0000 (22.6519)  time: 0.2901  data: 0.1134  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.8201 (4.5436)  acc1: 0.0000 (7.9407)  acc5: 0.0000 (21.6405)  time: 0.3021  data: 0.1155  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 4.8392 (4.5683)  acc1: 0.0000 (7.5456)  acc5: 0.0000 (20.5638)  time: 0.2871  data: 0.0832  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 4.8240 (4.5757)  acc1: 0.0000 (7.6619)  acc5: 0.0000 (20.3528)  time: 0.2878  data: 0.0921  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.5262 (4.5683)  acc1: 0.0000 (7.3906)  acc5: 0.0000 (20.4123)  time: 0.2845  data: 0.1058  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.3689 (4.5606)  acc1: 0.0000 (7.3112)  acc5: 27.7778 (20.9716)  time: 0.3364  data: 0.1400  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 4.3689 (4.5601)  acc1: 0.0000 (7.0770)  acc5: 22.2222 (20.8391)  time: 0.3542  data: 0.1504  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 4.7641 (4.5830)  acc1: 0.0000 (6.8172)  acc5: 0.0000 (20.1859)  time: 0.3160  data: 0.1224  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.3776 (4.5534)  acc1: 0.0000 (7.2158)  acc5: 22.2222 (21.7752)  time: 0.2960  data: 0.1044  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.9537 (4.5508)  acc1: 0.0000 (6.9906)  acc5: 55.5556 (22.0377)  time: 0.2261  data: 0.0585  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.9537 (4.5524)  acc1: 0.0000 (6.9834)  acc5: 38.8889 (22.0152)  time: 0.2198  data: 0.0584  max mem: 15572
Val: Total time: 0:01:29 (0.3292 s / it)
* Acc@1 6.983 Acc@5 22.015 loss 4.552
Accuracy of the network on the 4883 val videos: 7.0%
[2025-01-12 21:54:16,041] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-12 21:54:16,045] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 21:54:16,045] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 21:54:19,267] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 21:54:19,268] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 6.98%
Epoch: [3]  [   0/2809]  eta: 7:02:53  lr: 0.000028  min_lr: 0.000000  loss: 4.5312 (4.5312)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.0330  data: 8.6215  max mem: 15572
Epoch: [3]  [  10/2809]  eta: 1:06:47  lr: 0.000028  min_lr: 0.000000  loss: 4.6669 (4.6944)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4316  data: 1.0106  max mem: 15572
Epoch: [3]  [  20/2809]  eta: 0:48:23  lr: 0.000028  min_lr: 0.000000  loss: 4.7198 (4.6935)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.2064  max mem: 15572
Epoch: [3]  [  30/2809]  eta: 0:41:28  lr: 0.000028  min_lr: 0.000000  loss: 4.7190 (4.6921)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1632  max mem: 15572
[2025-01-12 21:54:49,636] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:54:49,637] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [  40/2809]  eta: 0:37:47  lr: 0.000028  min_lr: 0.000000  loss: 4.7274 (4.7071)  loss_scale: 65536.0000 (73528.1951)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1548  max mem: 15572
[2025-01-12 21:54:55,325] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8473
[2025-01-12 21:54:55,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:54:55,327] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [  50/2809]  eta: 0:34:57  lr: 0.000028  min_lr: 0.000000  loss: 4.7559 (4.7143)  loss_scale: 65536.0000 (78386.1961)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1287  max mem: 15572
Epoch: [3]  [  60/2809]  eta: 0:33:22  lr: 0.000028  min_lr: 0.000000  loss: 4.7817 (4.7190)  loss_scale: 65536.0000 (76279.6066)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1076  max mem: 15572
Epoch: [3]  [  70/2809]  eta: 0:31:55  lr: 0.000028  min_lr: 0.000000  loss: 4.8012 (4.7387)  loss_scale: 65536.0000 (74766.4225)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0682  max mem: 15572
Epoch: [3]  [  80/2809]  eta: 0:31:35  lr: 0.000028  min_lr: 0.000000  loss: 4.7851 (4.7327)  loss_scale: 65536.0000 (73626.8642)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1170  max mem: 15572
Epoch: [3]  [  90/2809]  eta: 0:31:33  lr: 0.000028  min_lr: 0.000000  loss: 4.7273 (4.7364)  loss_scale: 65536.0000 (72737.7582)  weight_decay: 0.0500 (0.0500)  time: 0.6862  data: 0.2481  max mem: 15572
Epoch: [3]  [ 100/2809]  eta: 0:30:59  lr: 0.000028  min_lr: 0.000000  loss: 4.7742 (4.7417)  loss_scale: 65536.0000 (72024.7129)  weight_decay: 0.0500 (0.0500)  time: 0.6529  data: 0.2281  max mem: 15572
Epoch: [3]  [ 110/2809]  eta: 0:30:27  lr: 0.000028  min_lr: 0.000000  loss: 4.7405 (4.7462)  loss_scale: 65536.0000 (71440.1441)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1471  max mem: 15572
Epoch: [3]  [ 120/2809]  eta: 0:30:11  lr: 0.000029  min_lr: 0.000000  loss: 4.7650 (4.7516)  loss_scale: 65536.0000 (70952.1983)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1709  max mem: 15572
Epoch: [3]  [ 130/2809]  eta: 0:29:56  lr: 0.000029  min_lr: 0.000000  loss: 4.8559 (4.7550)  loss_scale: 65536.0000 (70538.7481)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.2011  max mem: 15572
Epoch: [3]  [ 140/2809]  eta: 0:29:40  lr: 0.000029  min_lr: 0.000000  loss: 4.8301 (4.7608)  loss_scale: 65536.0000 (70183.9433)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1730  max mem: 15572
Epoch: [3]  [ 150/2809]  eta: 0:29:28  lr: 0.000029  min_lr: 0.000000  loss: 4.7607 (4.7551)  loss_scale: 65536.0000 (69876.1325)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1533  max mem: 15572
Epoch: [3]  [ 160/2809]  eta: 0:28:58  lr: 0.000029  min_lr: 0.000000  loss: 4.6861 (4.7528)  loss_scale: 65536.0000 (69606.5590)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1192  max mem: 15572
Epoch: [3]  [ 170/2809]  eta: 0:28:33  lr: 0.000029  min_lr: 0.000000  loss: 4.8233 (4.7616)  loss_scale: 65536.0000 (69368.5146)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0830  max mem: 15572
[2025-01-12 21:56:12,401] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:56:12,402] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 180/2809]  eta: 0:28:27  lr: 0.000029  min_lr: 0.000000  loss: 4.8828 (4.7647)  loss_scale: 65536.0000 (71329.2376)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1421  max mem: 15572
[2025-01-12 21:56:18,083] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8610
[2025-01-12 21:56:18,084] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:56:18,084] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 190/2809]  eta: 0:28:10  lr: 0.000029  min_lr: 0.000000  loss: 4.8318 (4.7661)  loss_scale: 65536.0000 (71712.1675)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1742  max mem: 15572
Epoch: [3]  [ 200/2809]  eta: 0:27:53  lr: 0.000029  min_lr: 0.000000  loss: 4.7084 (4.7647)  loss_scale: 65536.0000 (71404.8955)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1323  max mem: 15572
Epoch: [3]  [ 210/2809]  eta: 0:27:30  lr: 0.000029  min_lr: 0.000000  loss: 4.7346 (4.7646)  loss_scale: 65536.0000 (71126.7488)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0707  max mem: 15572
Epoch: [3]  [ 220/2809]  eta: 0:27:22  lr: 0.000029  min_lr: 0.000000  loss: 4.8109 (4.7693)  loss_scale: 65536.0000 (70873.7738)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0757  max mem: 15572
Epoch: [3]  [ 230/2809]  eta: 0:27:10  lr: 0.000029  min_lr: 0.000000  loss: 4.8777 (4.7691)  loss_scale: 65536.0000 (70642.7013)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1222  max mem: 15572
Epoch: [3]  [ 240/2809]  eta: 0:26:51  lr: 0.000029  min_lr: 0.000000  loss: 4.8742 (4.7740)  loss_scale: 65536.0000 (70430.8050)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0974  max mem: 15572
Epoch: [3]  [ 250/2809]  eta: 0:26:45  lr: 0.000029  min_lr: 0.000000  loss: 4.8276 (4.7730)  loss_scale: 65536.0000 (70235.7928)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1157  max mem: 15572
Epoch: [3]  [ 260/2809]  eta: 0:26:37  lr: 0.000029  min_lr: 0.000000  loss: 4.7051 (4.7670)  loss_scale: 65536.0000 (70055.7241)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1388  max mem: 15572
Epoch: [3]  [ 270/2809]  eta: 0:26:30  lr: 0.000029  min_lr: 0.000000  loss: 4.6951 (4.7666)  loss_scale: 65536.0000 (69888.9446)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1402  max mem: 15572
Epoch: [3]  [ 280/2809]  eta: 0:26:20  lr: 0.000029  min_lr: 0.000000  loss: 4.7903 (4.7680)  loss_scale: 65536.0000 (69734.0356)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1291  max mem: 15572
Epoch: [3]  [ 290/2809]  eta: 0:26:16  lr: 0.000029  min_lr: 0.000000  loss: 4.7704 (4.7680)  loss_scale: 65536.0000 (69589.7732)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1520  max mem: 15572
Epoch: [3]  [ 300/2809]  eta: 0:26:01  lr: 0.000029  min_lr: 0.000000  loss: 4.7882 (4.7693)  loss_scale: 65536.0000 (69455.0963)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1249  max mem: 15572
Epoch: [3]  [ 310/2809]  eta: 0:25:51  lr: 0.000029  min_lr: 0.000000  loss: 4.7953 (4.7689)  loss_scale: 65536.0000 (69329.0804)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0868  max mem: 15572
[2025-01-12 21:57:33,310] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:57:33,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 21:57:35,447] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8744
[2025-01-12 21:57:35,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:57:35,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 320/2809]  eta: 0:25:52  lr: 0.000029  min_lr: 0.000000  loss: 4.7854 (4.7695)  loss_scale: 65536.0000 (70231.7259)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.2084  max mem: 15572
Epoch: [3]  [ 330/2809]  eta: 0:25:38  lr: 0.000029  min_lr: 0.000000  loss: 4.7854 (4.7701)  loss_scale: 65536.0000 (70089.8610)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1962  max mem: 15572
Epoch: [3]  [ 340/2809]  eta: 0:25:30  lr: 0.000029  min_lr: 0.000000  loss: 4.7312 (4.7695)  loss_scale: 65536.0000 (69956.3167)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1350  max mem: 15572
Epoch: [3]  [ 350/2809]  eta: 0:25:14  lr: 0.000029  min_lr: 0.000000  loss: 4.7065 (4.7676)  loss_scale: 65536.0000 (69830.3818)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0889  max mem: 15572
Epoch: [3]  [ 360/2809]  eta: 0:25:04  lr: 0.000029  min_lr: 0.000000  loss: 4.8268 (4.7690)  loss_scale: 65536.0000 (69711.4238)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0343  max mem: 15572
Epoch: [3]  [ 370/2809]  eta: 0:25:04  lr: 0.000029  min_lr: 0.000000  loss: 4.8576 (4.7700)  loss_scale: 65536.0000 (69598.8787)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1505  max mem: 15572
Epoch: [3]  [ 380/2809]  eta: 0:24:57  lr: 0.000029  min_lr: 0.000000  loss: 4.8098 (4.7679)  loss_scale: 65536.0000 (69492.2415)  weight_decay: 0.0500 (0.0500)  time: 0.6561  data: 0.2055  max mem: 15572
Epoch: [3]  [ 390/2809]  eta: 0:24:47  lr: 0.000029  min_lr: 0.000000  loss: 4.7010 (4.7666)  loss_scale: 65536.0000 (69391.0588)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0978  max mem: 15572
Epoch: [3]  [ 400/2809]  eta: 0:24:31  lr: 0.000029  min_lr: 0.000000  loss: 4.7022 (4.7682)  loss_scale: 65536.0000 (69294.9227)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0182  max mem: 15572
Epoch: [3]  [ 410/2809]  eta: 0:24:18  lr: 0.000029  min_lr: 0.000000  loss: 4.8163 (4.7692)  loss_scale: 65536.0000 (69203.4647)  weight_decay: 0.0500 (0.0500)  time: 0.4750  data: 0.0405  max mem: 15572
Epoch: [3]  [ 420/2809]  eta: 0:24:05  lr: 0.000030  min_lr: 0.000000  loss: 4.7846 (4.7708)  loss_scale: 65536.0000 (69116.3515)  weight_decay: 0.0500 (0.0500)  time: 0.4880  data: 0.0629  max mem: 15572
Epoch: [3]  [ 430/2809]  eta: 0:23:58  lr: 0.000030  min_lr: 0.000000  loss: 4.7454 (4.7700)  loss_scale: 65536.0000 (69033.2807)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.1129  max mem: 15572
Epoch: [3]  [ 440/2809]  eta: 0:23:48  lr: 0.000030  min_lr: 0.000000  loss: 4.7304 (4.7684)  loss_scale: 65536.0000 (68953.9773)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1328  max mem: 15572
[2025-01-12 21:58:48,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:58:48,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 450/2809]  eta: 0:23:52  lr: 0.000030  min_lr: 0.000000  loss: 4.7819 (4.7694)  loss_scale: 65536.0000 (69604.7539)  weight_decay: 0.0500 (0.0500)  time: 0.6590  data: 0.2284  max mem: 15572
[2025-01-12 21:58:55,148] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8881
[2025-01-12 21:58:55,148] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:58:55,148] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 460/2809]  eta: 0:23:43  lr: 0.000030  min_lr: 0.000000  loss: 4.7379 (4.7695)  loss_scale: 65536.0000 (69942.9761)  weight_decay: 0.0500 (0.0500)  time: 0.6771  data: 0.2385  max mem: 15572
Epoch: [3]  [ 470/2809]  eta: 0:23:40  lr: 0.000030  min_lr: 0.000000  loss: 4.7233 (4.7675)  loss_scale: 65536.0000 (69849.4098)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1651  max mem: 15572
Epoch: [3]  [ 480/2809]  eta: 0:23:36  lr: 0.000030  min_lr: 0.000000  loss: 4.7929 (4.7665)  loss_scale: 65536.0000 (69759.7339)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.1933  max mem: 15572
Epoch: [3]  [ 490/2809]  eta: 0:23:28  lr: 0.000030  min_lr: 0.000000  loss: 4.7162 (4.7653)  loss_scale: 65536.0000 (69673.7108)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1429  max mem: 15572
Epoch: [3]  [ 500/2809]  eta: 0:23:22  lr: 0.000030  min_lr: 0.000000  loss: 4.7162 (4.7650)  loss_scale: 65536.0000 (69591.1218)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1440  max mem: 15572
Epoch: [3]  [ 510/2809]  eta: 0:23:15  lr: 0.000030  min_lr: 0.000000  loss: 4.8074 (4.7642)  loss_scale: 65536.0000 (69511.7652)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1670  max mem: 15572
Epoch: [3]  [ 520/2809]  eta: 0:23:06  lr: 0.000030  min_lr: 0.000000  loss: 4.7686 (4.7626)  loss_scale: 65536.0000 (69435.4549)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1092  max mem: 15572
Epoch: [3]  [ 530/2809]  eta: 0:22:57  lr: 0.000030  min_lr: 0.000000  loss: 4.6848 (4.7620)  loss_scale: 65536.0000 (69362.0188)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0801  max mem: 15572
Epoch: [3]  [ 540/2809]  eta: 0:22:50  lr: 0.000030  min_lr: 0.000000  loss: 4.7247 (4.7633)  loss_scale: 65536.0000 (69291.2976)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1299  max mem: 15572
Epoch: [3]  [ 550/2809]  eta: 0:22:47  lr: 0.000030  min_lr: 0.000000  loss: 4.7604 (4.7619)  loss_scale: 65536.0000 (69223.1434)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.1944  max mem: 15572
Epoch: [3]  [ 560/2809]  eta: 0:22:43  lr: 0.000030  min_lr: 0.000000  loss: 4.7424 (4.7621)  loss_scale: 65536.0000 (69157.4189)  weight_decay: 0.0500 (0.0500)  time: 0.6612  data: 0.2177  max mem: 15572
Epoch: [3]  [ 570/2809]  eta: 0:22:36  lr: 0.000030  min_lr: 0.000000  loss: 4.7424 (4.7603)  loss_scale: 65536.0000 (69093.9965)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1554  max mem: 15572
[2025-01-12 22:00:06,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=50, lr=[2.9101758468449346e-07, 2.9101758468449346e-07, 4.157394066921336e-07, 4.157394066921336e-07, 5.939134381316195e-07, 5.939134381316195e-07, 8.484477687594565e-07, 8.484477687594565e-07, 1.2120682410849378e-06, 1.2120682410849378e-06, 1.7315260586927683e-06, 1.7315260586927683e-06, 2.4736086552753836e-06, 2.4736086552753836e-06, 3.5337266503934053e-06, 3.5337266503934053e-06, 5.0481809291334365e-06, 5.0481809291334365e-06, 7.211687041619195e-06, 7.211687041619195e-06, 1.0302410059455993e-05, 1.0302410059455993e-05, 1.4717728656365707e-05, 1.4717728656365707e-05, 2.102532665195101e-05, 2.102532665195101e-05, 3.0036180931358588e-05, 3.0036180931358588e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:00:06,295] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=27.99716105366473, CurrSamplesPerSec=25.54546797226989, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [ 580/2809]  eta: 0:22:30  lr: 0.000030  min_lr: 0.000000  loss: 4.6350 (4.7585)  loss_scale: 65536.0000 (69032.7573)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1333  max mem: 15572
[2025-01-12 22:00:12,711] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:00:12,711] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 590/2809]  eta: 0:22:23  lr: 0.000030  min_lr: 0.000000  loss: 4.6992 (4.7584)  loss_scale: 65536.0000 (69860.7107)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1646  max mem: 15572
Epoch: [3]  [ 600/2809]  eta: 0:22:18  lr: 0.000030  min_lr: 0.000000  loss: 4.7399 (4.7590)  loss_scale: 131072.0000 (70879.2013)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1722  max mem: 15572
[2025-01-12 22:00:26,943] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9034
[2025-01-12 22:00:26,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:00:26,943] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 610/2809]  eta: 0:22:09  lr: 0.000030  min_lr: 0.000000  loss: 4.7242 (4.7576)  loss_scale: 131072.0000 (71435.3126)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1189  max mem: 15572
Epoch: [3]  [ 620/2809]  eta: 0:22:04  lr: 0.000030  min_lr: 0.000000  loss: 4.7598 (4.7580)  loss_scale: 65536.0000 (71340.3156)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1206  max mem: 15572
Epoch: [3]  [ 630/2809]  eta: 0:21:55  lr: 0.000030  min_lr: 0.000000  loss: 4.7778 (4.7586)  loss_scale: 65536.0000 (71248.3296)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1234  max mem: 15572
Epoch: [3]  [ 640/2809]  eta: 0:21:50  lr: 0.000030  min_lr: 0.000000  loss: 4.7062 (4.7579)  loss_scale: 65536.0000 (71159.2137)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0894  max mem: 15572
Epoch: [3]  [ 650/2809]  eta: 0:21:38  lr: 0.000030  min_lr: 0.000000  loss: 4.7046 (4.7571)  loss_scale: 65536.0000 (71072.8356)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0565  max mem: 15572
Epoch: [3]  [ 660/2809]  eta: 0:21:32  lr: 0.000030  min_lr: 0.000000  loss: 4.7099 (4.7575)  loss_scale: 65536.0000 (70989.0711)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0617  max mem: 15572
Epoch: [3]  [ 670/2809]  eta: 0:21:25  lr: 0.000030  min_lr: 0.000000  loss: 4.7437 (4.7567)  loss_scale: 65536.0000 (70907.8033)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1188  max mem: 15572
Epoch: [3]  [ 680/2809]  eta: 0:21:22  lr: 0.000030  min_lr: 0.000000  loss: 4.7731 (4.7574)  loss_scale: 65536.0000 (70828.9222)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.1709  max mem: 15572
Epoch: [3]  [ 690/2809]  eta: 0:21:13  lr: 0.000030  min_lr: 0.000000  loss: 4.7357 (4.7559)  loss_scale: 65536.0000 (70752.3242)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1308  max mem: 15572
Epoch: [3]  [ 700/2809]  eta: 0:21:05  lr: 0.000030  min_lr: 0.000000  loss: 4.6491 (4.7554)  loss_scale: 65536.0000 (70677.9116)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0359  max mem: 15572
Epoch: [3]  [ 710/2809]  eta: 0:20:59  lr: 0.000030  min_lr: 0.000000  loss: 4.6537 (4.7552)  loss_scale: 65536.0000 (70605.5921)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0846  max mem: 15572
Epoch: [3]  [ 720/2809]  eta: 0:20:57  lr: 0.000031  min_lr: 0.000000  loss: 4.7597 (4.7557)  loss_scale: 65536.0000 (70535.2788)  weight_decay: 0.0500 (0.0500)  time: 0.6686  data: 0.1895  max mem: 15572
Epoch: [3]  [ 730/2809]  eta: 0:20:50  lr: 0.000031  min_lr: 0.000000  loss: 4.7981 (4.7563)  loss_scale: 65536.0000 (70466.8892)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.1769  max mem: 15572
[2025-01-12 22:01:42,105] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:01:42,106] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:01:43,053] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9165
[2025-01-12 22:01:43,054] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:01:43,054] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 740/2809]  eta: 0:20:43  lr: 0.000031  min_lr: 0.000000  loss: 4.7981 (4.7563)  loss_scale: 65536.0000 (70577.2308)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1116  max mem: 15572
Epoch: [3]  [ 750/2809]  eta: 0:20:38  lr: 0.000031  min_lr: 0.000000  loss: 4.7697 (4.7565)  loss_scale: 65536.0000 (70510.1039)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1612  max mem: 15572
Epoch: [3]  [ 760/2809]  eta: 0:20:30  lr: 0.000031  min_lr: 0.000000  loss: 4.8031 (4.7564)  loss_scale: 65536.0000 (70444.7411)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1515  max mem: 15572
Epoch: [3]  [ 770/2809]  eta: 0:20:22  lr: 0.000031  min_lr: 0.000000  loss: 4.7926 (4.7560)  loss_scale: 65536.0000 (70381.0739)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0808  max mem: 15572
Epoch: [3]  [ 780/2809]  eta: 0:20:16  lr: 0.000031  min_lr: 0.000000  loss: 4.6737 (4.7549)  loss_scale: 65536.0000 (70319.0371)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1219  max mem: 15572
Epoch: [3]  [ 790/2809]  eta: 0:20:12  lr: 0.000031  min_lr: 0.000000  loss: 4.6737 (4.7539)  loss_scale: 65536.0000 (70258.5689)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.1849  max mem: 15572
Epoch: [3]  [ 800/2809]  eta: 0:20:07  lr: 0.000031  min_lr: 0.000000  loss: 4.7139 (4.7536)  loss_scale: 65536.0000 (70199.6105)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.1801  max mem: 15572
Epoch: [3]  [ 810/2809]  eta: 0:20:00  lr: 0.000031  min_lr: 0.000000  loss: 4.6796 (4.7529)  loss_scale: 65536.0000 (70142.1060)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1320  max mem: 15572
Epoch: [3]  [ 820/2809]  eta: 0:19:55  lr: 0.000031  min_lr: 0.000000  loss: 4.6684 (4.7525)  loss_scale: 65536.0000 (70086.0024)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1425  max mem: 15572
Epoch: [3]  [ 830/2809]  eta: 0:19:48  lr: 0.000031  min_lr: 0.000000  loss: 4.6659 (4.7513)  loss_scale: 65536.0000 (70031.2491)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1345  max mem: 15572
Epoch: [3]  [ 840/2809]  eta: 0:19:41  lr: 0.000031  min_lr: 0.000000  loss: 4.6687 (4.7503)  loss_scale: 65536.0000 (69977.7979)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0956  max mem: 15572
Epoch: [3]  [ 850/2809]  eta: 0:19:32  lr: 0.000031  min_lr: 0.000000  loss: 4.7036 (4.7504)  loss_scale: 65536.0000 (69925.6028)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0715  max mem: 15572
Epoch: [3]  [ 860/2809]  eta: 0:19:23  lr: 0.000031  min_lr: 0.000000  loss: 4.5974 (4.7494)  loss_scale: 65536.0000 (69874.6202)  weight_decay: 0.0500 (0.0500)  time: 0.4741  data: 0.0446  max mem: 15572
[2025-01-12 22:02:56,670] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:02:56,670] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:02:57,533] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9296
[2025-01-12 22:02:57,533] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:02:57,533] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 870/2809]  eta: 0:19:18  lr: 0.000031  min_lr: 0.000000  loss: 4.6244 (4.7484)  loss_scale: 65536.0000 (69975.2928)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1234  max mem: 15572
Epoch: [3]  [ 880/2809]  eta: 0:19:11  lr: 0.000031  min_lr: 0.000000  loss: 4.6918 (4.7482)  loss_scale: 65536.0000 (69924.9035)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1585  max mem: 15572
Epoch: [3]  [ 890/2809]  eta: 0:19:04  lr: 0.000031  min_lr: 0.000000  loss: 4.6412 (4.7475)  loss_scale: 65536.0000 (69875.6453)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1112  max mem: 15572
Epoch: [3]  [ 900/2809]  eta: 0:18:58  lr: 0.000031  min_lr: 0.000000  loss: 4.6051 (4.7463)  loss_scale: 65536.0000 (69827.4806)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1252  max mem: 15572
Epoch: [3]  [ 910/2809]  eta: 0:18:53  lr: 0.000031  min_lr: 0.000000  loss: 4.7046 (4.7465)  loss_scale: 65536.0000 (69780.3732)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1560  max mem: 15572
Epoch: [3]  [ 920/2809]  eta: 0:18:45  lr: 0.000031  min_lr: 0.000000  loss: 4.7074 (4.7461)  loss_scale: 65536.0000 (69734.2888)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1112  max mem: 15572
Epoch: [3]  [ 930/2809]  eta: 0:18:38  lr: 0.000031  min_lr: 0.000000  loss: 4.7074 (4.7452)  loss_scale: 65536.0000 (69689.1944)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0761  max mem: 15572
Epoch: [3]  [ 940/2809]  eta: 0:18:33  lr: 0.000031  min_lr: 0.000000  loss: 4.7283 (4.7458)  loss_scale: 65536.0000 (69645.0584)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1553  max mem: 15572
Epoch: [3]  [ 950/2809]  eta: 0:18:26  lr: 0.000031  min_lr: 0.000000  loss: 4.7559 (4.7462)  loss_scale: 65536.0000 (69601.8507)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1752  max mem: 15572
Epoch: [3]  [ 960/2809]  eta: 0:18:20  lr: 0.000031  min_lr: 0.000000  loss: 4.7170 (4.7459)  loss_scale: 65536.0000 (69559.5421)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1192  max mem: 15572
Epoch: [3]  [ 970/2809]  eta: 0:18:15  lr: 0.000031  min_lr: 0.000000  loss: 4.7273 (4.7459)  loss_scale: 65536.0000 (69518.1050)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1618  max mem: 15572
Epoch: [3]  [ 980/2809]  eta: 0:18:07  lr: 0.000031  min_lr: 0.000000  loss: 4.6833 (4.7450)  loss_scale: 65536.0000 (69477.5127)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1478  max mem: 15572
Epoch: [3]  [ 990/2809]  eta: 0:18:02  lr: 0.000031  min_lr: 0.000000  loss: 4.6660 (4.7446)  loss_scale: 65536.0000 (69437.7397)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1145  max mem: 15572
[2025-01-12 22:04:13,978] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:04:13,979] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1000/2809]  eta: 0:17:56  lr: 0.000031  min_lr: 0.000000  loss: 4.6660 (4.7440)  loss_scale: 65536.0000 (69595.1728)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1386  max mem: 15572
Epoch: [3]  [1010/2809]  eta: 0:17:49  lr: 0.000031  min_lr: 0.000000  loss: 4.7111 (4.7440)  loss_scale: 131072.0000 (70203.2522)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1308  max mem: 15572
Epoch: [3]  [1020/2809]  eta: 0:17:43  lr: 0.000032  min_lr: 0.000000  loss: 4.6919 (4.7440)  loss_scale: 131072.0000 (70799.4202)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1357  max mem: 15572
Epoch: [3]  [1030/2809]  eta: 0:17:37  lr: 0.000032  min_lr: 0.000000  loss: 4.6702 (4.7434)  loss_scale: 131072.0000 (71384.0233)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1228  max mem: 15572
[2025-01-12 22:04:34,934] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9460
[2025-01-12 22:04:34,934] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:04:34,934] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1040/2809]  eta: 0:17:30  lr: 0.000032  min_lr: 0.000000  loss: 4.6150 (4.7425)  loss_scale: 131072.0000 (71453.7560)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1129  max mem: 15572
Epoch: [3]  [1050/2809]  eta: 0:17:25  lr: 0.000032  min_lr: 0.000000  loss: 4.6412 (4.7429)  loss_scale: 65536.0000 (71397.4500)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1543  max mem: 15572
Epoch: [3]  [1060/2809]  eta: 0:17:20  lr: 0.000032  min_lr: 0.000000  loss: 4.7887 (4.7437)  loss_scale: 65536.0000 (71342.2055)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1860  max mem: 15572
Epoch: [3]  [1070/2809]  eta: 0:17:14  lr: 0.000032  min_lr: 0.000000  loss: 4.7858 (4.7435)  loss_scale: 65536.0000 (71287.9925)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1629  max mem: 15572
Epoch: [3]  [1080/2809]  eta: 0:17:07  lr: 0.000032  min_lr: 0.000000  loss: 4.7066 (4.7434)  loss_scale: 65536.0000 (71234.7826)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0981  max mem: 15572
Epoch: [3]  [1090/2809]  eta: 0:17:00  lr: 0.000032  min_lr: 0.000000  loss: 4.7066 (4.7432)  loss_scale: 65536.0000 (71182.5481)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0766  max mem: 15572
Epoch: [3]  [1100/2809]  eta: 0:16:55  lr: 0.000032  min_lr: 0.000000  loss: 4.6627 (4.7424)  loss_scale: 65536.0000 (71131.2625)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1492  max mem: 15572
Epoch: [3]  [1110/2809]  eta: 0:16:50  lr: 0.000032  min_lr: 0.000000  loss: 4.6627 (4.7423)  loss_scale: 65536.0000 (71080.9001)  weight_decay: 0.0500 (0.0500)  time: 0.6534  data: 0.1986  max mem: 15572
Epoch: [3]  [1120/2809]  eta: 0:16:42  lr: 0.000032  min_lr: 0.000000  loss: 4.7220 (4.7421)  loss_scale: 65536.0000 (71031.4362)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1085  max mem: 15572
Epoch: [3]  [1130/2809]  eta: 0:16:36  lr: 0.000032  min_lr: 0.000000  loss: 4.7299 (4.7415)  loss_scale: 65536.0000 (70982.8470)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0843  max mem: 15572
Epoch: [3]  [1140/2809]  eta: 0:16:32  lr: 0.000032  min_lr: 0.000000  loss: 4.7692 (4.7420)  loss_scale: 65536.0000 (70935.1096)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.1877  max mem: 15572
Epoch: [3]  [1150/2809]  eta: 0:16:25  lr: 0.000032  min_lr: 0.000000  loss: 4.7819 (4.7420)  loss_scale: 65536.0000 (70888.2016)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1253  max mem: 15572
Epoch: [3]  [1160/2809]  eta: 0:16:19  lr: 0.000032  min_lr: 0.000000  loss: 4.7041 (4.7411)  loss_scale: 65536.0000 (70842.1016)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0894  max mem: 15572
[2025-01-12 22:05:51,050] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:05:51,050] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:05:53,915] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9595
[2025-01-12 22:05:53,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:05:53,915] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1170/2809]  eta: 0:16:12  lr: 0.000032  min_lr: 0.000000  loss: 4.6788 (4.7406)  loss_scale: 65536.0000 (71132.5841)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1342  max mem: 15572
Epoch: [3]  [1180/2809]  eta: 0:16:08  lr: 0.000032  min_lr: 0.000000  loss: 4.7684 (4.7405)  loss_scale: 65536.0000 (71085.1956)  weight_decay: 0.0500 (0.0500)  time: 0.6376  data: 0.1758  max mem: 15572
Epoch: [3]  [1190/2809]  eta: 0:16:01  lr: 0.000032  min_lr: 0.000000  loss: 4.7131 (4.7398)  loss_scale: 65536.0000 (71038.6029)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1506  max mem: 15572
Epoch: [3]  [1200/2809]  eta: 0:15:54  lr: 0.000032  min_lr: 0.000000  loss: 4.7131 (4.7393)  loss_scale: 65536.0000 (70992.7860)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0901  max mem: 15572
Epoch: [3]  [1210/2809]  eta: 0:15:48  lr: 0.000032  min_lr: 0.000000  loss: 4.7500 (4.7397)  loss_scale: 65536.0000 (70947.7258)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1395  max mem: 15572
Epoch: [3]  [1220/2809]  eta: 0:15:42  lr: 0.000032  min_lr: 0.000000  loss: 4.7249 (4.7394)  loss_scale: 65536.0000 (70903.4038)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1507  max mem: 15572
Epoch: [3]  [1230/2809]  eta: 0:15:36  lr: 0.000032  min_lr: 0.000000  loss: 4.7522 (4.7397)  loss_scale: 65536.0000 (70859.8018)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1235  max mem: 15572
Epoch: [3]  [1240/2809]  eta: 0:15:29  lr: 0.000032  min_lr: 0.000000  loss: 4.7876 (4.7406)  loss_scale: 65536.0000 (70816.9025)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0611  max mem: 15572
Epoch: [3]  [1250/2809]  eta: 0:15:24  lr: 0.000032  min_lr: 0.000000  loss: 4.7644 (4.7411)  loss_scale: 65536.0000 (70774.6890)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0792  max mem: 15572
Epoch: [3]  [1260/2809]  eta: 0:15:17  lr: 0.000032  min_lr: 0.000000  loss: 4.7644 (4.7416)  loss_scale: 65536.0000 (70733.1451)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0987  max mem: 15572
Epoch: [3]  [1270/2809]  eta: 0:15:12  lr: 0.000032  min_lr: 0.000000  loss: 4.6796 (4.7407)  loss_scale: 65536.0000 (70692.2549)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1066  max mem: 15572
Epoch: [3]  [1280/2809]  eta: 0:15:05  lr: 0.000032  min_lr: 0.000000  loss: 4.7828 (4.7420)  loss_scale: 65536.0000 (70652.0031)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1356  max mem: 15572
Epoch: [3]  [1290/2809]  eta: 0:14:58  lr: 0.000032  min_lr: 0.000000  loss: 4.8773 (4.7419)  loss_scale: 65536.0000 (70612.3749)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0872  max mem: 15572
[2025-01-12 22:07:07,600] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:07:07,601] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:07:10,033] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9727
[2025-01-12 22:07:10,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:07:10,033] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1300/2809]  eta: 0:14:53  lr: 0.000032  min_lr: 0.000000  loss: 4.7157 (4.7415)  loss_scale: 65536.0000 (70724.4766)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1310  max mem: 15572
Epoch: [3]  [1310/2809]  eta: 0:14:46  lr: 0.000032  min_lr: 0.000000  loss: 4.7599 (4.7417)  loss_scale: 65536.0000 (70684.9001)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1486  max mem: 15572
Epoch: [3]  [1320/2809]  eta: 0:14:40  lr: 0.000033  min_lr: 0.000000  loss: 4.7244 (4.7410)  loss_scale: 65536.0000 (70645.9228)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1300  max mem: 15572
Epoch: [3]  [1330/2809]  eta: 0:14:35  lr: 0.000033  min_lr: 0.000000  loss: 4.7162 (4.7415)  loss_scale: 65536.0000 (70607.5312)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1562  max mem: 15572
Epoch: [3]  [1340/2809]  eta: 0:14:29  lr: 0.000033  min_lr: 0.000000  loss: 4.7687 (4.7411)  loss_scale: 65536.0000 (70569.7122)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1534  max mem: 15572
Epoch: [3]  [1350/2809]  eta: 0:14:22  lr: 0.000033  min_lr: 0.000000  loss: 4.7748 (4.7419)  loss_scale: 65536.0000 (70532.4530)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1061  max mem: 15572
Epoch: [3]  [1360/2809]  eta: 0:14:16  lr: 0.000033  min_lr: 0.000000  loss: 4.7779 (4.7421)  loss_scale: 65536.0000 (70495.7414)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0730  max mem: 15572
Epoch: [3]  [1370/2809]  eta: 0:14:10  lr: 0.000033  min_lr: 0.000000  loss: 4.7631 (4.7424)  loss_scale: 65536.0000 (70459.5653)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1256  max mem: 15572
Epoch: [3]  [1380/2809]  eta: 0:14:04  lr: 0.000033  min_lr: 0.000000  loss: 4.8337 (4.7427)  loss_scale: 65536.0000 (70423.9131)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1310  max mem: 15572
Epoch: [3]  [1390/2809]  eta: 0:13:58  lr: 0.000033  min_lr: 0.000000  loss: 4.6011 (4.7413)  loss_scale: 65536.0000 (70388.7735)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1244  max mem: 15572
Epoch: [3]  [1400/2809]  eta: 0:13:52  lr: 0.000033  min_lr: 0.000000  loss: 4.5919 (4.7409)  loss_scale: 65536.0000 (70354.1356)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1606  max mem: 15572
Epoch: [3]  [1410/2809]  eta: 0:13:47  lr: 0.000033  min_lr: 0.000000  loss: 4.6709 (4.7407)  loss_scale: 65536.0000 (70319.9887)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.2016  max mem: 15572
Epoch: [3]  [1420/2809]  eta: 0:13:41  lr: 0.000033  min_lr: 0.000000  loss: 4.7050 (4.7407)  loss_scale: 65536.0000 (70286.3223)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1890  max mem: 15572
[2025-01-12 22:08:26,175] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:08:26,176] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1430/2809]  eta: 0:13:36  lr: 0.000033  min_lr: 0.000000  loss: 4.7189 (4.7405)  loss_scale: 65536.0000 (70344.7212)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1548  max mem: 15572
[2025-01-12 22:08:32,234] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9865
[2025-01-12 22:08:32,234] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:08:32,235] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1440/2809]  eta: 0:13:30  lr: 0.000033  min_lr: 0.000000  loss: 4.7160 (4.7402)  loss_scale: 65536.0000 (70629.7071)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.1814  max mem: 15572
Epoch: [3]  [1450/2809]  eta: 0:13:24  lr: 0.000033  min_lr: 0.000000  loss: 4.6798 (4.7399)  loss_scale: 65536.0000 (70594.6023)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1503  max mem: 15572
Epoch: [3]  [1460/2809]  eta: 0:13:17  lr: 0.000033  min_lr: 0.000000  loss: 4.6798 (4.7396)  loss_scale: 65536.0000 (70559.9781)  weight_decay: 0.0500 (0.0500)  time: 0.4920  data: 0.0486  max mem: 15572
Epoch: [3]  [1470/2809]  eta: 0:13:12  lr: 0.000033  min_lr: 0.000000  loss: 4.6786 (4.7393)  loss_scale: 65536.0000 (70525.8246)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1220  max mem: 15572
Epoch: [3]  [1480/2809]  eta: 0:13:06  lr: 0.000033  min_lr: 0.000000  loss: 4.6476 (4.7389)  loss_scale: 65536.0000 (70492.1323)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.2126  max mem: 15572
Epoch: [3]  [1490/2809]  eta: 0:12:59  lr: 0.000033  min_lr: 0.000000  loss: 4.6846 (4.7391)  loss_scale: 65536.0000 (70458.8920)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1278  max mem: 15572
Epoch: [3]  [1500/2809]  eta: 0:12:53  lr: 0.000033  min_lr: 0.000000  loss: 4.6554 (4.7380)  loss_scale: 65536.0000 (70426.0946)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0759  max mem: 15572
Epoch: [3]  [1510/2809]  eta: 0:12:47  lr: 0.000033  min_lr: 0.000000  loss: 4.5943 (4.7375)  loss_scale: 65536.0000 (70393.7313)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1042  max mem: 15572
Epoch: [3]  [1520/2809]  eta: 0:12:41  lr: 0.000033  min_lr: 0.000000  loss: 4.6953 (4.7374)  loss_scale: 65536.0000 (70361.7936)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1190  max mem: 15572
Epoch: [3]  [1530/2809]  eta: 0:12:35  lr: 0.000033  min_lr: 0.000000  loss: 4.6951 (4.7363)  loss_scale: 65536.0000 (70330.2730)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1285  max mem: 15572
Epoch: [3]  [1540/2809]  eta: 0:12:29  lr: 0.000033  min_lr: 0.000000  loss: 4.6176 (4.7358)  loss_scale: 65536.0000 (70299.1616)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1192  max mem: 15572
Epoch: [3]  [1550/2809]  eta: 0:12:24  lr: 0.000033  min_lr: 0.000000  loss: 4.6925 (4.7354)  loss_scale: 65536.0000 (70268.4513)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1319  max mem: 15572
Epoch: [3]  [1560/2809]  eta: 0:12:19  lr: 0.000033  min_lr: 0.000000  loss: 4.6531 (4.7347)  loss_scale: 65536.0000 (70238.1345)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.1947  max mem: 15572
[2025-01-12 22:09:46,665] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:09:46,666] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1570/2809]  eta: 0:12:11  lr: 0.000033  min_lr: 0.000000  loss: 4.6145 (4.7338)  loss_scale: 65536.0000 (70375.0681)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1079  max mem: 15572
[2025-01-12 22:09:50,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=57, lr=[3.233564650805923e-07, 3.233564650805923e-07, 4.6193780725798904e-07, 4.6193780725798904e-07, 6.599111532256987e-07, 6.599111532256987e-07, 9.427302188938554e-07, 9.427302188938554e-07, 1.3467574555626506e-06, 1.3467574555626506e-06, 1.923939222232358e-06, 1.923939222232358e-06, 2.748484603189083e-06, 2.748484603189083e-06, 3.926406575984405e-06, 3.926406575984405e-06, 5.609152251406292e-06, 5.609152251406292e-06, 8.013074644866134e-06, 8.013074644866134e-06, 1.1447249492665904e-05, 1.1447249492665904e-05, 1.6353213560951293e-05, 1.6353213560951293e-05, 2.336173365850185e-05, 2.336173365850185e-05, 3.3373905226431215e-05, 3.3373905226431215e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:09:50,306] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=27.987495908456957, CurrSamplesPerSec=25.650450792039397, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-12 22:09:52,643] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10004
[2025-01-12 22:09:52,643] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:09:52,644] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1580/2809]  eta: 0:12:06  lr: 0.000033  min_lr: 0.000000  loss: 4.6233 (4.7338)  loss_scale: 65536.0000 (70593.1739)  weight_decay: 0.0500 (0.0500)  time: 0.5188  data: 0.0689  max mem: 15572
Epoch: [3]  [1590/2809]  eta: 0:11:59  lr: 0.000033  min_lr: 0.000000  loss: 4.6833 (4.7335)  loss_scale: 65536.0000 (70561.3878)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1176  max mem: 15572
Epoch: [3]  [1600/2809]  eta: 0:11:53  lr: 0.000033  min_lr: 0.000000  loss: 4.7120 (4.7338)  loss_scale: 65536.0000 (70529.9988)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1217  max mem: 15572
Epoch: [3]  [1610/2809]  eta: 0:11:48  lr: 0.000034  min_lr: 0.000000  loss: 4.7218 (4.7337)  loss_scale: 65536.0000 (70498.9994)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.1817  max mem: 15572
Epoch: [3]  [1620/2809]  eta: 0:11:42  lr: 0.000034  min_lr: 0.000000  loss: 4.7317 (4.7336)  loss_scale: 65536.0000 (70468.3825)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1326  max mem: 15572
Epoch: [3]  [1630/2809]  eta: 0:11:36  lr: 0.000034  min_lr: 0.000000  loss: 4.7644 (4.7336)  loss_scale: 65536.0000 (70438.1410)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1188  max mem: 15572
Epoch: [3]  [1640/2809]  eta: 0:11:30  lr: 0.000034  min_lr: 0.000000  loss: 4.6583 (4.7330)  loss_scale: 65536.0000 (70408.2681)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1653  max mem: 15572
Epoch: [3]  [1650/2809]  eta: 0:11:24  lr: 0.000034  min_lr: 0.000000  loss: 4.6583 (4.7330)  loss_scale: 65536.0000 (70378.7571)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1162  max mem: 15572
Epoch: [3]  [1660/2809]  eta: 0:11:18  lr: 0.000034  min_lr: 0.000000  loss: 4.7281 (4.7337)  loss_scale: 65536.0000 (70349.6014)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1005  max mem: 15572
Epoch: [3]  [1670/2809]  eta: 0:11:12  lr: 0.000034  min_lr: 0.000000  loss: 4.7243 (4.7330)  loss_scale: 65536.0000 (70320.7947)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1076  max mem: 15572
Epoch: [3]  [1680/2809]  eta: 0:11:05  lr: 0.000034  min_lr: 0.000000  loss: 4.7031 (4.7333)  loss_scale: 65536.0000 (70292.3308)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0668  max mem: 15572
Epoch: [3]  [1690/2809]  eta: 0:11:00  lr: 0.000034  min_lr: 0.000000  loss: 4.7031 (4.7329)  loss_scale: 65536.0000 (70264.2034)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1033  max mem: 15572
Epoch: [3]  [1700/2809]  eta: 0:10:54  lr: 0.000034  min_lr: 0.000000  loss: 4.7380 (4.7331)  loss_scale: 65536.0000 (70236.4068)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.1907  max mem: 15572
[2025-01-12 22:11:08,178] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:11:08,178] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1710/2809]  eta: 0:10:48  lr: 0.000034  min_lr: 0.000000  loss: 4.7444 (4.7324)  loss_scale: 65536.0000 (70400.4489)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.1471  max mem: 15572
[2025-01-12 22:11:14,330] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10144
[2025-01-12 22:11:14,330] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:11:14,331] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1720/2809]  eta: 0:10:42  lr: 0.000034  min_lr: 0.000000  loss: 4.6946 (4.7326)  loss_scale: 131072.0000 (70600.6647)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0936  max mem: 15572
Epoch: [3]  [1730/2809]  eta: 0:10:36  lr: 0.000034  min_lr: 0.000000  loss: 4.7510 (4.7327)  loss_scale: 65536.0000 (70571.4061)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1178  max mem: 15572
Epoch: [3]  [1740/2809]  eta: 0:10:31  lr: 0.000034  min_lr: 0.000000  loss: 4.6530 (4.7320)  loss_scale: 65536.0000 (70542.4836)  weight_decay: 0.0500 (0.0500)  time: 0.6712  data: 0.2051  max mem: 15572
Epoch: [3]  [1750/2809]  eta: 0:10:26  lr: 0.000034  min_lr: 0.000000  loss: 4.5826 (4.7316)  loss_scale: 65536.0000 (70513.8915)  weight_decay: 0.0500 (0.0500)  time: 0.6897  data: 0.2209  max mem: 15572
Epoch: [3]  [1760/2809]  eta: 0:10:20  lr: 0.000034  min_lr: 0.000000  loss: 4.5826 (4.7305)  loss_scale: 65536.0000 (70485.6241)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1467  max mem: 15572
Epoch: [3]  [1770/2809]  eta: 0:10:14  lr: 0.000034  min_lr: 0.000000  loss: 4.6697 (4.7305)  loss_scale: 65536.0000 (70457.6759)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1280  max mem: 15572
Epoch: [3]  [1780/2809]  eta: 0:10:08  lr: 0.000034  min_lr: 0.000000  loss: 4.7480 (4.7309)  loss_scale: 65536.0000 (70430.0415)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1572  max mem: 15572
Epoch: [3]  [1790/2809]  eta: 0:10:02  lr: 0.000034  min_lr: 0.000000  loss: 4.7480 (4.7307)  loss_scale: 65536.0000 (70402.7158)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1617  max mem: 15572
Epoch: [3]  [1800/2809]  eta: 0:09:56  lr: 0.000034  min_lr: 0.000000  loss: 4.7000 (4.7305)  loss_scale: 65536.0000 (70375.6935)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1196  max mem: 15572
Epoch: [3]  [1810/2809]  eta: 0:09:50  lr: 0.000034  min_lr: 0.000000  loss: 4.6985 (4.7304)  loss_scale: 65536.0000 (70348.9696)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1182  max mem: 15572
Epoch: [3]  [1820/2809]  eta: 0:09:44  lr: 0.000034  min_lr: 0.000000  loss: 4.7076 (4.7302)  loss_scale: 65536.0000 (70322.5393)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1514  max mem: 15572
Epoch: [3]  [1830/2809]  eta: 0:09:38  lr: 0.000034  min_lr: 0.000000  loss: 4.7336 (4.7302)  loss_scale: 65536.0000 (70296.3976)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1031  max mem: 15572
Epoch: [3]  [1840/2809]  eta: 0:09:32  lr: 0.000034  min_lr: 0.000000  loss: 4.7428 (4.7304)  loss_scale: 65536.0000 (70270.5399)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0538  max mem: 15572
[2025-01-12 22:12:30,716] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:12:30,717] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1850/2809]  eta: 0:09:26  lr: 0.000034  min_lr: 0.000000  loss: 4.6974 (4.7299)  loss_scale: 65536.0000 (70421.9903)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0919  max mem: 15572
[2025-01-12 22:12:35,760] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10278
[2025-01-12 22:12:35,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:12:35,760] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1860/2809]  eta: 0:09:20  lr: 0.000034  min_lr: 0.000000  loss: 4.6309 (4.7295)  loss_scale: 65536.0000 (70395.7356)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.2017  max mem: 15572
Epoch: [3]  [1870/2809]  eta: 0:09:14  lr: 0.000034  min_lr: 0.000000  loss: 4.6320 (4.7292)  loss_scale: 65536.0000 (70369.7616)  weight_decay: 0.0500 (0.0500)  time: 0.6537  data: 0.2204  max mem: 15572
Epoch: [3]  [1880/2809]  eta: 0:09:08  lr: 0.000034  min_lr: 0.000000  loss: 4.7092 (4.7293)  loss_scale: 65536.0000 (70344.0638)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1447  max mem: 15572
Epoch: [3]  [1890/2809]  eta: 0:09:02  lr: 0.000034  min_lr: 0.000000  loss: 4.6542 (4.7284)  loss_scale: 65536.0000 (70318.6378)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0998  max mem: 15572
Epoch: [3]  [1900/2809]  eta: 0:08:57  lr: 0.000034  min_lr: 0.000000  loss: 4.6208 (4.7280)  loss_scale: 65536.0000 (70293.4792)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1255  max mem: 15572
Epoch: [3]  [1910/2809]  eta: 0:08:51  lr: 0.000035  min_lr: 0.000000  loss: 4.5608 (4.7273)  loss_scale: 65536.0000 (70268.5840)  weight_decay: 0.0500 (0.0500)  time: 0.6925  data: 0.2350  max mem: 15572
Epoch: [3]  [1920/2809]  eta: 0:08:45  lr: 0.000035  min_lr: 0.000000  loss: 4.6461 (4.7276)  loss_scale: 65536.0000 (70243.9479)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1442  max mem: 15572
Epoch: [3]  [1930/2809]  eta: 0:08:39  lr: 0.000035  min_lr: 0.000000  loss: 4.6939 (4.7277)  loss_scale: 65536.0000 (70219.5671)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0987  max mem: 15572
Epoch: [3]  [1940/2809]  eta: 0:08:33  lr: 0.000035  min_lr: 0.000000  loss: 4.7497 (4.7281)  loss_scale: 65536.0000 (70195.4374)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0988  max mem: 15572
Epoch: [3]  [1950/2809]  eta: 0:08:27  lr: 0.000035  min_lr: 0.000000  loss: 4.8010 (4.7285)  loss_scale: 65536.0000 (70171.5551)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0144  max mem: 15572
Epoch: [3]  [1960/2809]  eta: 0:08:21  lr: 0.000035  min_lr: 0.000000  loss: 4.7970 (4.7285)  loss_scale: 65536.0000 (70147.9164)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0874  max mem: 15572
Epoch: [3]  [1970/2809]  eta: 0:08:15  lr: 0.000035  min_lr: 0.000000  loss: 4.7970 (4.7287)  loss_scale: 65536.0000 (70124.5175)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1706  max mem: 15572
[2025-01-12 22:13:49,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:13:49,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1980/2809]  eta: 0:08:09  lr: 0.000035  min_lr: 0.000000  loss: 4.6554 (4.7280)  loss_scale: 65536.0000 (70134.4372)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1114  max mem: 15572
[2025-01-12 22:13:51,835] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10410
[2025-01-12 22:13:51,835] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:13:51,835] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1990/2809]  eta: 0:08:03  lr: 0.000035  min_lr: 0.000000  loss: 4.6498 (4.7278)  loss_scale: 65536.0000 (70177.1733)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0475  max mem: 15572
Epoch: [3]  [2000/2809]  eta: 0:07:57  lr: 0.000035  min_lr: 0.000000  loss: 4.6683 (4.7272)  loss_scale: 65536.0000 (70153.9790)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0918  max mem: 15572
Epoch: [3]  [2010/2809]  eta: 0:07:52  lr: 0.000035  min_lr: 0.000000  loss: 4.6000 (4.7265)  loss_scale: 65536.0000 (70131.0154)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1741  max mem: 15572
Epoch: [3]  [2020/2809]  eta: 0:07:46  lr: 0.000035  min_lr: 0.000000  loss: 4.5032 (4.7261)  loss_scale: 65536.0000 (70108.2791)  weight_decay: 0.0500 (0.0500)  time: 0.6447  data: 0.1906  max mem: 15572
Epoch: [3]  [2030/2809]  eta: 0:07:40  lr: 0.000035  min_lr: 0.000000  loss: 4.5229 (4.7255)  loss_scale: 65536.0000 (70085.7666)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1398  max mem: 15572
Epoch: [3]  [2040/2809]  eta: 0:07:34  lr: 0.000035  min_lr: 0.000000  loss: 4.5975 (4.7249)  loss_scale: 65536.0000 (70063.4748)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1333  max mem: 15572
Epoch: [3]  [2050/2809]  eta: 0:07:28  lr: 0.000035  min_lr: 0.000000  loss: 4.5987 (4.7247)  loss_scale: 65536.0000 (70041.4003)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0968  max mem: 15572
Epoch: [3]  [2060/2809]  eta: 0:07:22  lr: 0.000035  min_lr: 0.000000  loss: 4.6345 (4.7242)  loss_scale: 65536.0000 (70019.5400)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0874  max mem: 15572
Epoch: [3]  [2070/2809]  eta: 0:07:16  lr: 0.000035  min_lr: 0.000000  loss: 4.6623 (4.7237)  loss_scale: 65536.0000 (69997.8909)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1425  max mem: 15572
Epoch: [3]  [2080/2809]  eta: 0:07:10  lr: 0.000035  min_lr: 0.000000  loss: 4.7025 (4.7233)  loss_scale: 65536.0000 (69976.4498)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1220  max mem: 15572
Epoch: [3]  [2090/2809]  eta: 0:07:04  lr: 0.000035  min_lr: 0.000000  loss: 4.6717 (4.7227)  loss_scale: 65536.0000 (69955.2138)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0986  max mem: 15572
Epoch: [3]  [2100/2809]  eta: 0:06:58  lr: 0.000035  min_lr: 0.000000  loss: 4.6717 (4.7226)  loss_scale: 65536.0000 (69934.1799)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1537  max mem: 15572
Epoch: [3]  [2110/2809]  eta: 0:06:52  lr: 0.000035  min_lr: 0.000000  loss: 4.6414 (4.7222)  loss_scale: 65536.0000 (69913.3453)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1541  max mem: 15572
[2025-01-12 22:15:08,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:15:08,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2120/2809]  eta: 0:06:46  lr: 0.000035  min_lr: 0.000000  loss: 4.6267 (4.7219)  loss_scale: 65536.0000 (70170.7949)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1208  max mem: 15572
Epoch: [3]  [2130/2809]  eta: 0:06:40  lr: 0.000035  min_lr: 0.000000  loss: 4.6472 (4.7214)  loss_scale: 131072.0000 (70456.5819)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0923  max mem: 15572
[2025-01-12 22:15:21,003] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10563
[2025-01-12 22:15:21,003] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:15:21,003] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2140/2809]  eta: 0:06:34  lr: 0.000035  min_lr: 0.000000  loss: 4.5541 (4.7203)  loss_scale: 131072.0000 (70586.6492)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0869  max mem: 15572
Epoch: [3]  [2150/2809]  eta: 0:06:29  lr: 0.000035  min_lr: 0.000000  loss: 4.5768 (4.7202)  loss_scale: 65536.0000 (70563.1688)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.1910  max mem: 15572
Epoch: [3]  [2160/2809]  eta: 0:06:23  lr: 0.000035  min_lr: 0.000000  loss: 4.6987 (4.7206)  loss_scale: 65536.0000 (70539.9056)  weight_decay: 0.0500 (0.0500)  time: 0.7151  data: 0.2088  max mem: 15572
Epoch: [3]  [2170/2809]  eta: 0:06:17  lr: 0.000035  min_lr: 0.000000  loss: 4.6335 (4.7195)  loss_scale: 65536.0000 (70516.8567)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0898  max mem: 15572
Epoch: [3]  [2180/2809]  eta: 0:06:11  lr: 0.000035  min_lr: 0.000000  loss: 4.4740 (4.7195)  loss_scale: 65536.0000 (70494.0193)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0873  max mem: 15572
Epoch: [3]  [2190/2809]  eta: 0:06:05  lr: 0.000035  min_lr: 0.000000  loss: 4.6849 (4.7192)  loss_scale: 65536.0000 (70471.3902)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0988  max mem: 15572
Epoch: [3]  [2200/2809]  eta: 0:05:59  lr: 0.000035  min_lr: 0.000000  loss: 4.6247 (4.7191)  loss_scale: 65536.0000 (70448.9668)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0925  max mem: 15572
Epoch: [3]  [2210/2809]  eta: 0:05:53  lr: 0.000036  min_lr: 0.000000  loss: 4.6411 (4.7190)  loss_scale: 65536.0000 (70426.7463)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1716  max mem: 15572
Epoch: [3]  [2220/2809]  eta: 0:05:47  lr: 0.000036  min_lr: 0.000000  loss: 4.6781 (4.7185)  loss_scale: 65536.0000 (70404.7258)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.1958  max mem: 15572
Epoch: [3]  [2230/2809]  eta: 0:05:41  lr: 0.000036  min_lr: 0.000000  loss: 4.5662 (4.7176)  loss_scale: 65536.0000 (70382.9027)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1813  max mem: 15572
Epoch: [3]  [2240/2809]  eta: 0:05:36  lr: 0.000036  min_lr: 0.000000  loss: 4.7013 (4.7176)  loss_scale: 65536.0000 (70361.2744)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1618  max mem: 15572
Epoch: [3]  [2250/2809]  eta: 0:05:30  lr: 0.000036  min_lr: 0.000000  loss: 4.7258 (4.7175)  loss_scale: 65536.0000 (70339.8383)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1166  max mem: 15572
Epoch: [3]  [2260/2809]  eta: 0:05:24  lr: 0.000036  min_lr: 0.000000  loss: 4.6679 (4.7175)  loss_scale: 65536.0000 (70318.5918)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1439  max mem: 15572
[2025-01-12 22:16:38,010] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:16:38,011] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2270/2809]  eta: 0:05:18  lr: 0.000036  min_lr: 0.000000  loss: 4.6815 (4.7178)  loss_scale: 65536.0000 (70470.6790)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1565  max mem: 15572
Epoch: [3]  [2280/2809]  eta: 0:05:12  lr: 0.000036  min_lr: 0.000000  loss: 4.8175 (4.7179)  loss_scale: 131072.0000 (70736.3577)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1528  max mem: 15572
Epoch: [3]  [2290/2809]  eta: 0:05:06  lr: 0.000036  min_lr: 0.000000  loss: 4.7350 (4.7176)  loss_scale: 131072.0000 (70999.7172)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1318  max mem: 15572
Epoch: [3]  [2300/2809]  eta: 0:05:00  lr: 0.000036  min_lr: 0.000000  loss: 4.6384 (4.7172)  loss_scale: 131072.0000 (71260.7875)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1426  max mem: 15572
[2025-01-12 22:17:02,168] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10732
[2025-01-12 22:17:02,168] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:17:02,168] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2310/2809]  eta: 0:04:54  lr: 0.000036  min_lr: 0.000000  loss: 4.6245 (4.7166)  loss_scale: 131072.0000 (71349.4487)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.1881  max mem: 15572
Epoch: [3]  [2320/2809]  eta: 0:04:49  lr: 0.000036  min_lr: 0.000000  loss: 4.6245 (4.7161)  loss_scale: 65536.0000 (71324.4016)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1637  max mem: 15572
Epoch: [3]  [2330/2809]  eta: 0:04:43  lr: 0.000036  min_lr: 0.000000  loss: 4.6295 (4.7159)  loss_scale: 65536.0000 (71299.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1256  max mem: 15572
Epoch: [3]  [2340/2809]  eta: 0:04:37  lr: 0.000036  min_lr: 0.000000  loss: 4.6339 (4.7157)  loss_scale: 65536.0000 (71274.9492)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1264  max mem: 15572
Epoch: [3]  [2350/2809]  eta: 0:04:31  lr: 0.000036  min_lr: 0.000000  loss: 4.7245 (4.7155)  loss_scale: 65536.0000 (71250.5385)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0800  max mem: 15572
Epoch: [3]  [2360/2809]  eta: 0:04:24  lr: 0.000036  min_lr: 0.000000  loss: 4.6496 (4.7154)  loss_scale: 65536.0000 (71226.3346)  weight_decay: 0.0500 (0.0500)  time: 0.4785  data: 0.0091  max mem: 15572
Epoch: [3]  [2370/2809]  eta: 0:04:18  lr: 0.000036  min_lr: 0.000000  loss: 4.6443 (4.7151)  loss_scale: 65536.0000 (71202.3349)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0541  max mem: 15572
[2025-01-12 22:17:43,177] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10805
[2025-01-12 22:17:43,177] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:17:43,177] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [2380/2809]  eta: 0:04:12  lr: 0.000036  min_lr: 0.000000  loss: 4.7045 (4.7149)  loss_scale: 65536.0000 (71137.2499)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0964  max mem: 15572
Epoch: [3]  [2390/2809]  eta: 0:04:07  lr: 0.000036  min_lr: 0.000000  loss: 4.7440 (4.7150)  loss_scale: 32768.0000 (70976.7762)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1141  max mem: 15572
Epoch: [3]  [2400/2809]  eta: 0:04:01  lr: 0.000036  min_lr: 0.000000  loss: 4.7018 (4.7145)  loss_scale: 32768.0000 (70817.6393)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1086  max mem: 15572
Epoch: [3]  [2410/2809]  eta: 0:03:55  lr: 0.000036  min_lr: 0.000000  loss: 4.6258 (4.7141)  loss_scale: 32768.0000 (70659.8225)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1288  max mem: 15572
Epoch: [3]  [2420/2809]  eta: 0:03:49  lr: 0.000036  min_lr: 0.000000  loss: 4.6228 (4.7136)  loss_scale: 32768.0000 (70503.3094)  weight_decay: 0.0500 (0.0500)  time: 0.6825  data: 0.2323  max mem: 15572
Epoch: [3]  [2430/2809]  eta: 0:03:43  lr: 0.000036  min_lr: 0.000000  loss: 4.6904 (4.7136)  loss_scale: 32768.0000 (70348.0839)  weight_decay: 0.0500 (0.0500)  time: 0.6775  data: 0.2196  max mem: 15572
Epoch: [3]  [2440/2809]  eta: 0:03:37  lr: 0.000036  min_lr: 0.000000  loss: 4.7214 (4.7138)  loss_scale: 32768.0000 (70194.1303)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1039  max mem: 15572
Epoch: [3]  [2450/2809]  eta: 0:03:31  lr: 0.000036  min_lr: 0.000000  loss: 4.6542 (4.7135)  loss_scale: 32768.0000 (70041.4329)  weight_decay: 0.0500 (0.0500)  time: 0.4532  data: 0.0337  max mem: 15572
Epoch: [3]  [2460/2809]  eta: 0:03:25  lr: 0.000036  min_lr: 0.000000  loss: 4.6122 (4.7134)  loss_scale: 32768.0000 (69889.9764)  weight_decay: 0.0500 (0.0500)  time: 0.4072  data: 0.0007  max mem: 15572
Epoch: [3]  [2470/2809]  eta: 0:03:19  lr: 0.000036  min_lr: 0.000000  loss: 4.6028 (4.7128)  loss_scale: 32768.0000 (69739.7459)  weight_decay: 0.0500 (0.0500)  time: 0.4733  data: 0.0181  max mem: 15572
Epoch: [3]  [2480/2809]  eta: 0:03:13  lr: 0.000036  min_lr: 0.000000  loss: 4.6365 (4.7126)  loss_scale: 32768.0000 (69590.7263)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1331  max mem: 15572
Epoch: [3]  [2490/2809]  eta: 0:03:07  lr: 0.000036  min_lr: 0.000000  loss: 4.6999 (4.7126)  loss_scale: 32768.0000 (69442.9033)  weight_decay: 0.0500 (0.0500)  time: 0.6814  data: 0.2320  max mem: 15572
Epoch: [3]  [2500/2809]  eta: 0:03:02  lr: 0.000036  min_lr: 0.000000  loss: 4.6902 (4.7125)  loss_scale: 32768.0000 (69296.2623)  weight_decay: 0.0500 (0.0500)  time: 0.6689  data: 0.2148  max mem: 15572
[2025-01-12 22:18:57,902] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:18:57,903] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [2510/2809]  eta: 0:02:56  lr: 0.000037  min_lr: 0.000000  loss: 4.6232 (4.7123)  loss_scale: 32768.0000 (69202.9885)  weight_decay: 0.0500 (0.0500)  time: 0.7003  data: 0.2207  max mem: 15572
Epoch: [3]  [2520/2809]  eta: 0:02:50  lr: 0.000037  min_lr: 0.000000  loss: 4.6850 (4.7122)  loss_scale: 65536.0000 (69188.4427)  weight_decay: 0.0500 (0.0500)  time: 0.7013  data: 0.1999  max mem: 15572
Epoch: [3]  [2530/2809]  eta: 0:02:44  lr: 0.000037  min_lr: 0.000000  loss: 4.7235 (4.7122)  loss_scale: 65536.0000 (69174.0119)  weight_decay: 0.0500 (0.0500)  time: 0.7060  data: 0.1956  max mem: 15572
Epoch: [3]  [2540/2809]  eta: 0:02:38  lr: 0.000037  min_lr: 0.000000  loss: 4.7462 (4.7122)  loss_scale: 65536.0000 (69159.6946)  weight_decay: 0.0500 (0.0500)  time: 0.6713  data: 0.1656  max mem: 15572
Epoch: [3]  [2550/2809]  eta: 0:02:33  lr: 0.000037  min_lr: 0.000000  loss: 4.7097 (4.7115)  loss_scale: 65536.0000 (69145.4896)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1337  max mem: 15572
Epoch: [3]  [2560/2809]  eta: 0:02:27  lr: 0.000037  min_lr: 0.000000  loss: 4.4977 (4.7109)  loss_scale: 65536.0000 (69131.3955)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.1664  max mem: 15572
Epoch: [3]  [2570/2809]  eta: 0:02:21  lr: 0.000037  min_lr: 0.000000  loss: 4.4977 (4.7104)  loss_scale: 65536.0000 (69117.4111)  weight_decay: 0.0500 (0.0500)  time: 0.6559  data: 0.1849  max mem: 15572
[2025-01-12 22:19:42,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=64, lr=[3.556953454766912e-07, 3.556953454766912e-07, 5.081362078238446e-07, 5.081362078238446e-07, 7.259088683197781e-07, 7.259088683197781e-07, 1.0370126690282546e-06, 1.0370126690282546e-06, 1.4814466700403637e-06, 1.4814466700403637e-06, 2.116352385771948e-06, 2.116352385771948e-06, 3.023360551102783e-06, 3.023360551102783e-06, 4.319086501575405e-06, 4.319086501575405e-06, 6.17012357367915e-06, 6.17012357367915e-06, 8.814462248113072e-06, 8.814462248113072e-06, 1.2592088925875817e-05, 1.2592088925875817e-05, 1.7988698465536884e-05, 1.7988698465536884e-05, 2.5698140665052692e-05, 2.5698140665052692e-05, 3.671162952150385e-05, 3.671162952150385e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:19:42,941] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=27.94750635849804, CurrSamplesPerSec=21.713778371403464, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [2580/2809]  eta: 0:02:15  lr: 0.000037  min_lr: 0.000000  loss: 4.5448 (4.7098)  loss_scale: 65536.0000 (69103.5351)  weight_decay: 0.0500 (0.0500)  time: 0.7004  data: 0.2405  max mem: 15572
Epoch: [3]  [2590/2809]  eta: 0:02:09  lr: 0.000037  min_lr: 0.000000  loss: 4.5632 (4.7098)  loss_scale: 65536.0000 (69089.7661)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.1880  max mem: 15572
Epoch: [3]  [2600/2809]  eta: 0:02:03  lr: 0.000037  min_lr: 0.000000  loss: 4.6736 (4.7096)  loss_scale: 65536.0000 (69076.1030)  weight_decay: 0.0500 (0.0500)  time: 0.4599  data: 0.0523  max mem: 15572
Epoch: [3]  [2610/2809]  eta: 0:01:57  lr: 0.000037  min_lr: 0.000000  loss: 4.6146 (4.7091)  loss_scale: 65536.0000 (69062.5446)  weight_decay: 0.0500 (0.0500)  time: 0.3978  data: 0.0005  max mem: 15572
Epoch: [3]  [2620/2809]  eta: 0:01:51  lr: 0.000037  min_lr: 0.000000  loss: 4.6575 (4.7090)  loss_scale: 65536.0000 (69049.0897)  weight_decay: 0.0500 (0.0500)  time: 0.4291  data: 0.0006  max mem: 15572
Epoch: [3]  [2630/2809]  eta: 0:01:45  lr: 0.000037  min_lr: 0.000000  loss: 4.6575 (4.7086)  loss_scale: 65536.0000 (69035.7370)  weight_decay: 0.0500 (0.0500)  time: 0.4681  data: 0.0046  max mem: 15572
[2025-01-12 22:20:14,370] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:20:14,370] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:20:17,881] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11067
[2025-01-12 22:20:17,882] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:20:17,882] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2640/2809]  eta: 0:01:39  lr: 0.000037  min_lr: 0.000000  loss: 4.6489 (4.7086)  loss_scale: 65536.0000 (69146.5596)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0693  max mem: 15572
Epoch: [3]  [2650/2809]  eta: 0:01:33  lr: 0.000037  min_lr: 0.000000  loss: 4.6489 (4.7083)  loss_scale: 65536.0000 (69132.9400)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.1753  max mem: 15572
Epoch: [3]  [2660/2809]  eta: 0:01:27  lr: 0.000037  min_lr: 0.000000  loss: 4.5469 (4.7081)  loss_scale: 65536.0000 (69119.4228)  weight_decay: 0.0500 (0.0500)  time: 0.6502  data: 0.1707  max mem: 15572
Epoch: [3]  [2670/2809]  eta: 0:01:22  lr: 0.000037  min_lr: 0.000000  loss: 4.6908 (4.7081)  loss_scale: 65536.0000 (69106.0067)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1460  max mem: 15572
Epoch: [3]  [2680/2809]  eta: 0:01:16  lr: 0.000037  min_lr: 0.000000  loss: 4.6909 (4.7077)  loss_scale: 65536.0000 (69092.6908)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1885  max mem: 15572
Epoch: [3]  [2690/2809]  eta: 0:01:10  lr: 0.000037  min_lr: 0.000000  loss: 4.5986 (4.7079)  loss_scale: 65536.0000 (69079.4738)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1597  max mem: 15572
Epoch: [3]  [2700/2809]  eta: 0:01:04  lr: 0.000037  min_lr: 0.000000  loss: 4.7534 (4.7081)  loss_scale: 65536.0000 (69066.3547)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1673  max mem: 15572
Epoch: [3]  [2710/2809]  eta: 0:00:58  lr: 0.000037  min_lr: 0.000000  loss: 4.7249 (4.7080)  loss_scale: 65536.0000 (69053.3323)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.1993  max mem: 15572
Epoch: [3]  [2720/2809]  eta: 0:00:52  lr: 0.000037  min_lr: 0.000000  loss: 4.7799 (4.7083)  loss_scale: 65536.0000 (69040.4057)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1327  max mem: 15572
Epoch: [3]  [2730/2809]  eta: 0:00:46  lr: 0.000037  min_lr: 0.000000  loss: 4.7685 (4.7081)  loss_scale: 65536.0000 (69027.5738)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1504  max mem: 15572
Epoch: [3]  [2740/2809]  eta: 0:00:40  lr: 0.000037  min_lr: 0.000000  loss: 4.6772 (4.7076)  loss_scale: 65536.0000 (69014.8355)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1637  max mem: 15572
Epoch: [3]  [2750/2809]  eta: 0:00:34  lr: 0.000037  min_lr: 0.000000  loss: 4.5842 (4.7070)  loss_scale: 65536.0000 (69002.1897)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.1954  max mem: 15572
Epoch: [3]  [2760/2809]  eta: 0:00:28  lr: 0.000037  min_lr: 0.000000  loss: 4.5816 (4.7063)  loss_scale: 65536.0000 (68989.6356)  weight_decay: 0.0500 (0.0500)  time: 0.6575  data: 0.2083  max mem: 15572
[2025-01-12 22:21:34,463] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11192
[2025-01-12 22:21:34,463] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:21:34,463] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [2770/2809]  eta: 0:00:23  lr: 0.000037  min_lr: 0.000000  loss: 4.6157 (4.7058)  loss_scale: 65536.0000 (68906.2201)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1694  max mem: 15572
Epoch: [3]  [2780/2809]  eta: 0:00:17  lr: 0.000037  min_lr: 0.000000  loss: 4.5829 (4.7056)  loss_scale: 32768.0000 (68776.2733)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1053  max mem: 15572
Epoch: [3]  [2790/2809]  eta: 0:00:11  lr: 0.000037  min_lr: 0.000000  loss: 4.6820 (4.7055)  loss_scale: 32768.0000 (68647.2576)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1057  max mem: 15572
Epoch: [3]  [2800/2809]  eta: 0:00:05  lr: 0.000037  min_lr: 0.000000  loss: 4.6820 (4.7055)  loss_scale: 32768.0000 (68519.1632)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1069  max mem: 15572
Epoch: [3]  [2808/2809]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000000  loss: 4.6100 (4.7054)  loss_scale: 32768.0000 (68417.3443)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.1066  max mem: 15572
Epoch: [3] Total time: 0:27:38 (0.5904 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000000  loss: 4.6100 (4.7054)  loss_scale: 32768.0000 (68417.3443)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:17:44  loss: 2.5095 (2.5095)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 3.9139  data: 3.7141  max mem: 15572
Val:  [ 10/272]  eta: 0:02:59  loss: 4.8101 (4.4050)  acc1: 0.0000 (21.2121)  acc5: 0.0000 (28.2828)  time: 0.6863  data: 0.4657  max mem: 15572
Val:  [ 20/272]  eta: 0:02:14  loss: 4.4890 (4.3695)  acc1: 0.0000 (15.3439)  acc5: 0.0000 (24.0741)  time: 0.3653  data: 0.1499  max mem: 15572
Val:  [ 30/272]  eta: 0:01:53  loss: 4.4835 (4.3582)  acc1: 0.0000 (12.1864)  acc5: 11.1111 (25.0896)  time: 0.3478  data: 0.1440  max mem: 15572
Val:  [ 40/272]  eta: 0:01:41  loss: 3.8483 (4.2204)  acc1: 0.0000 (12.4661)  acc5: 27.7778 (30.6233)  time: 0.3365  data: 0.1302  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 3.7167 (4.2083)  acc1: 0.0000 (12.4183)  acc5: 50.0000 (32.4619)  time: 0.3383  data: 0.1288  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 3.5570 (4.1171)  acc1: 5.5556 (14.6630)  acc5: 55.5556 (37.1585)  time: 0.3215  data: 0.1172  max mem: 15572
Val:  [ 70/272]  eta: 0:01:18  loss: 3.6334 (4.0398)  acc1: 22.2222 (16.1189)  acc5: 55.5556 (37.8717)  time: 0.3148  data: 0.1080  max mem: 15572
Val:  [ 80/272]  eta: 0:01:12  loss: 3.8067 (4.0684)  acc1: 5.5556 (16.3237)  acc5: 33.3333 (37.3114)  time: 0.3110  data: 0.1173  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 4.8286 (4.1538)  acc1: 0.0000 (14.5299)  acc5: 0.0000 (33.2112)  time: 0.3217  data: 0.1422  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 4.7810 (4.2026)  acc1: 0.0000 (13.7514)  acc5: 0.0000 (31.4631)  time: 0.3312  data: 0.1405  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 4.6525 (4.2444)  acc1: 0.0000 (12.6126)  acc5: 0.0000 (30.0300)  time: 0.3428  data: 0.1374  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 4.6451 (4.2797)  acc1: 0.0000 (11.7539)  acc5: 5.5556 (28.6042)  time: 0.3201  data: 0.1180  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 4.5167 (4.2179)  acc1: 0.0000 (13.2740)  acc5: 22.2222 (31.1705)  time: 0.3055  data: 0.1141  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 4.1678 (4.2013)  acc1: 0.0000 (14.1844)  acc5: 33.3333 (31.3633)  time: 0.3416  data: 0.1517  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 4.3234 (4.2190)  acc1: 0.0000 (13.2818)  acc5: 5.5556 (29.8013)  time: 0.3539  data: 0.1596  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 4.2796 (4.2137)  acc1: 0.0000 (13.6646)  acc5: 11.1111 (30.9179)  time: 0.3318  data: 0.1292  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.2892 (4.2401)  acc1: 0.0000 (12.9955)  acc5: 33.3333 (30.7018)  time: 0.2656  data: 0.0714  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 4.2955 (4.2439)  acc1: 0.0000 (12.5230)  acc5: 11.1111 (29.8343)  time: 0.2943  data: 0.1022  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.5694 (4.2617)  acc1: 0.0000 (11.8674)  acc5: 5.5556 (28.8540)  time: 0.3572  data: 0.1654  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 4.4731 (4.2682)  acc1: 0.0000 (11.3875)  acc5: 11.1111 (29.1874)  time: 0.3089  data: 0.1317  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 4.3821 (4.2855)  acc1: 0.0000 (10.9531)  acc5: 22.2222 (28.3307)  time: 0.2703  data: 0.0780  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.4633 (4.2904)  acc1: 0.0000 (10.5329)  acc5: 5.5556 (28.0794)  time: 0.2867  data: 0.0837  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.3409 (4.2858)  acc1: 0.0000 (11.3516)  acc5: 22.2222 (28.8841)  time: 0.3177  data: 0.1259  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.9946 (4.2759)  acc1: 16.6667 (11.3877)  acc5: 55.5556 (29.7372)  time: 0.3227  data: 0.1216  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 4.3626 (4.2997)  acc1: 0.0000 (11.2660)  acc5: 22.2222 (29.0615)  time: 0.2997  data: 0.0967  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.0382 (4.2507)  acc1: 11.1111 (12.2180)  acc5: 55.5556 (31.0132)  time: 0.2828  data: 0.0903  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.8373 (4.2467)  acc1: 16.6667 (12.1566)  acc5: 66.6667 (31.5293)  time: 0.2549  data: 0.0822  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.8373 (4.2490)  acc1: 16.6667 (12.1442)  acc5: 66.6667 (31.5175)  time: 0.2472  data: 0.0822  max mem: 15572
Val: Total time: 0:01:29 (0.3304 s / it)
* Acc@1 12.144 Acc@5 31.518 loss 4.249
Accuracy of the network on the 4883 val videos: 12.1%
[2025-01-12 22:23:27,624] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-12 22:23:27,628] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 22:23:27,628] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 22:23:30,533] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 22:23:30,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 12.14%
Epoch: [4]  [   0/2809]  eta: 5:49:10  lr: 0.000038  min_lr: 0.000000  loss: 4.1983 (4.1983)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.4585  data: 6.9718  max mem: 15572
Epoch: [4]  [  10/2809]  eta: 0:57:36  lr: 0.000038  min_lr: 0.000000  loss: 4.5496 (4.5045)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2348  data: 0.7792  max mem: 15572
Epoch: [4]  [  20/2809]  eta: 0:39:51  lr: 0.000038  min_lr: 0.000000  loss: 4.5793 (4.5834)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0802  max mem: 15572
Epoch: [4]  [  30/2809]  eta: 0:35:40  lr: 0.000038  min_lr: 0.000000  loss: 4.6792 (4.6124)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0659  max mem: 15572
Epoch: [4]  [  40/2809]  eta: 0:32:55  lr: 0.000038  min_lr: 0.000000  loss: 4.7000 (4.6207)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1241  max mem: 15572
Epoch: [4]  [  50/2809]  eta: 0:32:03  lr: 0.000038  min_lr: 0.000000  loss: 4.7337 (4.6370)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1370  max mem: 15572
Epoch: [4]  [  60/2809]  eta: 0:31:20  lr: 0.000038  min_lr: 0.000000  loss: 4.7189 (4.6515)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1750  max mem: 15572
Epoch: [4]  [  70/2809]  eta: 0:30:39  lr: 0.000038  min_lr: 0.000000  loss: 4.6673 (4.6530)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1676  max mem: 15572
Epoch: [4]  [  80/2809]  eta: 0:30:05  lr: 0.000038  min_lr: 0.000000  loss: 4.5531 (4.6490)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1339  max mem: 15572
[2025-01-12 22:24:28,461] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:24:28,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [  90/2809]  eta: 0:30:00  lr: 0.000038  min_lr: 0.000000  loss: 4.5565 (4.6352)  loss_scale: 32768.0000 (34928.5275)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1541  max mem: 15572
Epoch: [4]  [ 100/2809]  eta: 0:29:41  lr: 0.000038  min_lr: 0.000000  loss: 4.5565 (4.6323)  loss_scale: 65536.0000 (37958.9703)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.1587  max mem: 15572
Epoch: [4]  [ 110/2809]  eta: 0:29:25  lr: 0.000038  min_lr: 0.000000  loss: 4.5023 (4.6273)  loss_scale: 65536.0000 (40443.3874)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1563  max mem: 15572
Epoch: [4]  [ 120/2809]  eta: 0:28:46  lr: 0.000038  min_lr: 0.000000  loss: 4.6232 (4.6252)  loss_scale: 65536.0000 (42517.1570)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1268  max mem: 15572
Epoch: [4]  [ 130/2809]  eta: 0:28:29  lr: 0.000038  min_lr: 0.000000  loss: 4.6544 (4.6276)  loss_scale: 65536.0000 (44274.3206)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1106  max mem: 15572
Epoch: [4]  [ 140/2809]  eta: 0:28:05  lr: 0.000038  min_lr: 0.000000  loss: 4.5832 (4.6201)  loss_scale: 65536.0000 (45782.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1128  max mem: 15572
Epoch: [4]  [ 150/2809]  eta: 0:28:30  lr: 0.000038  min_lr: 0.000000  loss: 4.5227 (4.6163)  loss_scale: 65536.0000 (47090.4371)  weight_decay: 0.0500 (0.0500)  time: 0.6768  data: 0.2196  max mem: 15572
Epoch: [4]  [ 160/2809]  eta: 0:27:54  lr: 0.000038  min_lr: 0.000000  loss: 4.6142 (4.6166)  loss_scale: 65536.0000 (48236.1242)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.1801  max mem: 15572
Epoch: [4]  [ 170/2809]  eta: 0:27:35  lr: 0.000038  min_lr: 0.000000  loss: 4.5087 (4.6059)  loss_scale: 65536.0000 (49247.8129)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0439  max mem: 15572
Epoch: [4]  [ 180/2809]  eta: 0:27:24  lr: 0.000038  min_lr: 0.000000  loss: 4.5648 (4.6130)  loss_scale: 65536.0000 (50147.7127)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1276  max mem: 15572
Epoch: [4]  [ 190/2809]  eta: 0:27:09  lr: 0.000038  min_lr: 0.000000  loss: 4.7331 (4.6223)  loss_scale: 65536.0000 (50953.3822)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1439  max mem: 15572
Epoch: [4]  [ 200/2809]  eta: 0:27:03  lr: 0.000038  min_lr: 0.000000  loss: 4.7013 (4.6229)  loss_scale: 65536.0000 (51678.8856)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1565  max mem: 15572
Epoch: [4]  [ 210/2809]  eta: 0:26:50  lr: 0.000038  min_lr: 0.000000  loss: 4.7013 (4.6256)  loss_scale: 65536.0000 (52335.6209)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1541  max mem: 15572
[2025-01-12 22:25:43,106] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:25:43,107] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:25:45,436] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11454
[2025-01-12 22:25:45,436] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:25:45,437] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 220/2809]  eta: 0:26:30  lr: 0.000038  min_lr: 0.000000  loss: 4.6657 (4.6227)  loss_scale: 65536.0000 (54415.6380)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0778  max mem: 15572
Epoch: [4]  [ 230/2809]  eta: 0:26:12  lr: 0.000038  min_lr: 0.000000  loss: 4.6657 (4.6262)  loss_scale: 65536.0000 (54897.0390)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0334  max mem: 15572
Epoch: [4]  [ 240/2809]  eta: 0:26:06  lr: 0.000038  min_lr: 0.000000  loss: 4.6719 (4.6243)  loss_scale: 65536.0000 (55338.4896)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0747  max mem: 15572
Epoch: [4]  [ 250/2809]  eta: 0:25:58  lr: 0.000038  min_lr: 0.000000  loss: 4.6484 (4.6285)  loss_scale: 65536.0000 (55744.7649)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1364  max mem: 15572
Epoch: [4]  [ 260/2809]  eta: 0:25:56  lr: 0.000038  min_lr: 0.000000  loss: 4.7667 (4.6356)  loss_scale: 65536.0000 (56119.9080)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1679  max mem: 15572
Epoch: [4]  [ 270/2809]  eta: 0:25:51  lr: 0.000038  min_lr: 0.000000  loss: 4.7667 (4.6376)  loss_scale: 65536.0000 (56467.3653)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.1758  max mem: 15572
Epoch: [4]  [ 280/2809]  eta: 0:25:41  lr: 0.000038  min_lr: 0.000000  loss: 4.6850 (4.6415)  loss_scale: 65536.0000 (56790.0925)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1396  max mem: 15572
Epoch: [4]  [ 290/2809]  eta: 0:25:27  lr: 0.000038  min_lr: 0.000000  loss: 4.6295 (4.6380)  loss_scale: 65536.0000 (57090.6392)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1001  max mem: 15572
Epoch: [4]  [ 300/2809]  eta: 0:25:24  lr: 0.000039  min_lr: 0.000000  loss: 4.5673 (4.6404)  loss_scale: 65536.0000 (57371.2159)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1434  max mem: 15572
Epoch: [4]  [ 310/2809]  eta: 0:25:16  lr: 0.000039  min_lr: 0.000000  loss: 4.6251 (4.6406)  loss_scale: 65536.0000 (57633.7492)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1684  max mem: 15572
Epoch: [4]  [ 320/2809]  eta: 0:25:16  lr: 0.000039  min_lr: 0.000000  loss: 4.6258 (4.6417)  loss_scale: 65536.0000 (57879.9252)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1730  max mem: 15572
Epoch: [4]  [ 330/2809]  eta: 0:25:08  lr: 0.000039  min_lr: 0.000000  loss: 4.6435 (4.6408)  loss_scale: 65536.0000 (58111.2266)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1730  max mem: 15572
Epoch: [4]  [ 340/2809]  eta: 0:24:57  lr: 0.000039  min_lr: 0.000000  loss: 4.5870 (4.6390)  loss_scale: 65536.0000 (58328.9619)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1112  max mem: 15572
[2025-01-12 22:27:01,166] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:27:01,168] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 350/2809]  eta: 0:24:53  lr: 0.000039  min_lr: 0.000000  loss: 4.4451 (4.6350)  loss_scale: 65536.0000 (59281.1396)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0968  max mem: 15572
Epoch: [4]  [ 360/2809]  eta: 0:24:49  lr: 0.000039  min_lr: 0.000000  loss: 4.4423 (4.6336)  loss_scale: 131072.0000 (61269.8061)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1522  max mem: 15572
[2025-01-12 22:27:12,892] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11602
[2025-01-12 22:27:12,893] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:27:12,893] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 370/2809]  eta: 0:24:38  lr: 0.000039  min_lr: 0.000000  loss: 4.6639 (4.6336)  loss_scale: 131072.0000 (62268.0323)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1331  max mem: 15572
Epoch: [4]  [ 380/2809]  eta: 0:24:31  lr: 0.000039  min_lr: 0.000000  loss: 4.7490 (4.6343)  loss_scale: 65536.0000 (62353.8058)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1057  max mem: 15572
Epoch: [4]  [ 390/2809]  eta: 0:24:20  lr: 0.000039  min_lr: 0.000000  loss: 4.7326 (4.6363)  loss_scale: 65536.0000 (62435.1918)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1074  max mem: 15572
Epoch: [4]  [ 400/2809]  eta: 0:24:10  lr: 0.000039  min_lr: 0.000000  loss: 4.6183 (4.6346)  loss_scale: 65536.0000 (62512.5187)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0770  max mem: 15572
Epoch: [4]  [ 410/2809]  eta: 0:24:02  lr: 0.000039  min_lr: 0.000000  loss: 4.6060 (4.6356)  loss_scale: 65536.0000 (62586.0827)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1180  max mem: 15572
Epoch: [4]  [ 420/2809]  eta: 0:24:05  lr: 0.000039  min_lr: 0.000000  loss: 4.5939 (4.6334)  loss_scale: 65536.0000 (62656.1520)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.2291  max mem: 15572
Epoch: [4]  [ 430/2809]  eta: 0:23:50  lr: 0.000039  min_lr: 0.000000  loss: 4.4611 (4.6291)  loss_scale: 65536.0000 (62722.9698)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1618  max mem: 15572
Epoch: [4]  [ 440/2809]  eta: 0:23:45  lr: 0.000039  min_lr: 0.000000  loss: 4.5427 (4.6279)  loss_scale: 65536.0000 (62786.7574)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.1023  max mem: 15572
Epoch: [4]  [ 450/2809]  eta: 0:23:38  lr: 0.000039  min_lr: 0.000000  loss: 4.5992 (4.6270)  loss_scale: 65536.0000 (62847.7162)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1495  max mem: 15572
Epoch: [4]  [ 460/2809]  eta: 0:23:29  lr: 0.000039  min_lr: 0.000000  loss: 4.6117 (4.6275)  loss_scale: 65536.0000 (62906.0304)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0942  max mem: 15572
Epoch: [4]  [ 470/2809]  eta: 0:23:22  lr: 0.000039  min_lr: 0.000000  loss: 4.6117 (4.6257)  loss_scale: 65536.0000 (62961.8684)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1197  max mem: 15572
Epoch: [4]  [ 480/2809]  eta: 0:23:15  lr: 0.000039  min_lr: 0.000000  loss: 4.6256 (4.6256)  loss_scale: 65536.0000 (63015.3846)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1468  max mem: 15572
Epoch: [4]  [ 490/2809]  eta: 0:23:03  lr: 0.000039  min_lr: 0.000000  loss: 4.6110 (4.6259)  loss_scale: 65536.0000 (63066.7210)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0643  max mem: 15572
[2025-01-12 22:28:26,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:28:26,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:28:26,857] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11733
[2025-01-12 22:28:26,857] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:28:26,857] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 500/2809]  eta: 0:22:58  lr: 0.000039  min_lr: 0.000000  loss: 4.6364 (4.6265)  loss_scale: 65536.0000 (63377.6287)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0802  max mem: 15572
Epoch: [4]  [ 510/2809]  eta: 0:22:51  lr: 0.000039  min_lr: 0.000000  loss: 4.5423 (4.6225)  loss_scale: 65536.0000 (63419.8669)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1519  max mem: 15572
Epoch: [4]  [ 520/2809]  eta: 0:22:45  lr: 0.000039  min_lr: 0.000000  loss: 4.5288 (4.6255)  loss_scale: 65536.0000 (63460.4837)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1535  max mem: 15572
Epoch: [4]  [ 530/2809]  eta: 0:22:35  lr: 0.000039  min_lr: 0.000000  loss: 4.7708 (4.6264)  loss_scale: 65536.0000 (63499.5706)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1020  max mem: 15572
Epoch: [4]  [ 540/2809]  eta: 0:22:29  lr: 0.000039  min_lr: 0.000000  loss: 4.6425 (4.6271)  loss_scale: 65536.0000 (63537.2126)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1089  max mem: 15572
Epoch: [4]  [ 550/2809]  eta: 0:22:22  lr: 0.000039  min_lr: 0.000000  loss: 4.6966 (4.6287)  loss_scale: 65536.0000 (63573.4882)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1506  max mem: 15572
Epoch: [4]  [ 560/2809]  eta: 0:22:17  lr: 0.000039  min_lr: 0.000000  loss: 4.6632 (4.6265)  loss_scale: 65536.0000 (63608.4706)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1439  max mem: 15572
Epoch: [4]  [ 570/2809]  eta: 0:22:13  lr: 0.000039  min_lr: 0.000000  loss: 4.4830 (4.6257)  loss_scale: 65536.0000 (63642.2277)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1883  max mem: 15572
Epoch: [4]  [ 580/2809]  eta: 0:22:09  lr: 0.000039  min_lr: 0.000000  loss: 4.5534 (4.6246)  loss_scale: 65536.0000 (63674.8227)  weight_decay: 0.0500 (0.0500)  time: 0.6513  data: 0.2105  max mem: 15572
Epoch: [4]  [ 590/2809]  eta: 0:22:04  lr: 0.000039  min_lr: 0.000000  loss: 4.6007 (4.6239)  loss_scale: 65536.0000 (63706.3147)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1899  max mem: 15572
Epoch: [4]  [ 600/2809]  eta: 0:22:00  lr: 0.000040  min_lr: 0.000000  loss: 4.5933 (4.6231)  loss_scale: 65536.0000 (63736.7587)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1640  max mem: 15572
Epoch: [4]  [ 610/2809]  eta: 0:21:51  lr: 0.000040  min_lr: 0.000000  loss: 4.5933 (4.6232)  loss_scale: 65536.0000 (63766.2062)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1139  max mem: 15572
Epoch: [4]  [ 620/2809]  eta: 0:21:45  lr: 0.000040  min_lr: 0.000000  loss: 4.6202 (4.6234)  loss_scale: 65536.0000 (63794.7053)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1017  max mem: 15572
[2025-01-12 22:29:43,791] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:29:43,792] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 630/2809]  eta: 0:21:38  lr: 0.000040  min_lr: 0.000000  loss: 4.6185 (4.6227)  loss_scale: 65536.0000 (64341.6038)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1457  max mem: 15572
[2025-01-12 22:29:48,980] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11871
[2025-01-12 22:29:48,980] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:29:48,981] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 640/2809]  eta: 0:21:32  lr: 0.000040  min_lr: 0.000000  loss: 4.5233 (4.6212)  loss_scale: 65536.0000 (64769.1981)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1628  max mem: 15572
Epoch: [4]  [ 650/2809]  eta: 0:21:26  lr: 0.000040  min_lr: 0.000000  loss: 4.3852 (4.6179)  loss_scale: 65536.0000 (64780.9770)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1284  max mem: 15572
Epoch: [4]  [ 660/2809]  eta: 0:21:23  lr: 0.000040  min_lr: 0.000000  loss: 4.5184 (4.6175)  loss_scale: 65536.0000 (64792.3994)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.1592  max mem: 15572
Epoch: [4]  [ 670/2809]  eta: 0:21:17  lr: 0.000040  min_lr: 0.000000  loss: 4.6205 (4.6182)  loss_scale: 65536.0000 (64803.4814)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.1800  max mem: 15572
Epoch: [4]  [ 680/2809]  eta: 0:21:14  lr: 0.000040  min_lr: 0.000000  loss: 4.5875 (4.6173)  loss_scale: 65536.0000 (64814.2379)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1841  max mem: 15572
Epoch: [4]  [ 690/2809]  eta: 0:21:06  lr: 0.000040  min_lr: 0.000000  loss: 4.5638 (4.6176)  loss_scale: 65536.0000 (64824.6831)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1481  max mem: 15572
Epoch: [4]  [ 700/2809]  eta: 0:21:01  lr: 0.000040  min_lr: 0.000000  loss: 4.5525 (4.6171)  loss_scale: 65536.0000 (64834.8302)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1136  max mem: 15572
Epoch: [4]  [ 710/2809]  eta: 0:20:55  lr: 0.000040  min_lr: 0.000000  loss: 4.5525 (4.6169)  loss_scale: 65536.0000 (64844.6920)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1445  max mem: 15572
Epoch: [4]  [ 720/2809]  eta: 0:20:49  lr: 0.000040  min_lr: 0.000000  loss: 4.6396 (4.6164)  loss_scale: 65536.0000 (64854.2802)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1184  max mem: 15572
Epoch: [4]  [ 730/2809]  eta: 0:20:43  lr: 0.000040  min_lr: 0.000000  loss: 4.6396 (4.6164)  loss_scale: 65536.0000 (64863.6060)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1133  max mem: 15572
Epoch: [4]  [ 740/2809]  eta: 0:20:35  lr: 0.000040  min_lr: 0.000000  loss: 4.6421 (4.6168)  loss_scale: 65536.0000 (64872.6802)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.0769  max mem: 15572
Epoch: [4]  [ 750/2809]  eta: 0:20:26  lr: 0.000040  min_lr: 0.000000  loss: 4.4861 (4.6148)  loss_scale: 65536.0000 (64881.5126)  weight_decay: 0.0500 (0.0500)  time: 0.5111  data: 0.0544  max mem: 15572
Epoch: [4]  [ 760/2809]  eta: 0:20:22  lr: 0.000040  min_lr: 0.000000  loss: 4.5063 (4.6157)  loss_scale: 65536.0000 (64890.1130)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1464  max mem: 15572
[2025-01-12 22:31:07,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=70, lr=[3.8803422587279e-07, 3.8803422587279e-07, 5.543346083897001e-07, 5.543346083897001e-07, 7.919065834138574e-07, 7.919065834138574e-07, 1.1312951191626535e-06, 1.1312951191626535e-06, 1.6161358845180763e-06, 1.6161358845180763e-06, 2.3087655493115375e-06, 2.3087655493115375e-06, 3.298236499016483e-06, 3.298236499016483e-06, 4.7117664271664045e-06, 4.7117664271664045e-06, 6.731094895952006e-06, 6.731094895952006e-06, 9.61584985136001e-06, 9.61584985136001e-06, 1.3736928359085727e-05, 1.3736928359085727e-05, 1.9624183370122472e-05, 1.9624183370122472e-05, 2.8034547671603533e-05, 2.8034547671603533e-05, 4.0049353816576476e-05, 4.0049353816576476e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:31:07,356] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=27.960514978014185, CurrSamplesPerSec=28.125199700261014, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-12 22:31:07,774] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:31:07,774] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 770/2809]  eta: 0:20:15  lr: 0.000040  min_lr: 0.000000  loss: 4.5834 (4.6151)  loss_scale: 65536.0000 (65493.4994)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1721  max mem: 15572
[2025-01-12 22:31:12,971] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12009
[2025-01-12 22:31:12,971] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:31:12,971] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 780/2809]  eta: 0:20:08  lr: 0.000040  min_lr: 0.000000  loss: 4.5509 (4.6136)  loss_scale: 65536.0000 (65661.8694)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1172  max mem: 15572
Epoch: [4]  [ 790/2809]  eta: 0:19:59  lr: 0.000040  min_lr: 0.000000  loss: 4.5699 (4.6146)  loss_scale: 65536.0000 (65660.2781)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0823  max mem: 15572
Epoch: [4]  [ 800/2809]  eta: 0:19:57  lr: 0.000040  min_lr: 0.000000  loss: 4.5629 (4.6137)  loss_scale: 65536.0000 (65658.7266)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1715  max mem: 15572
Epoch: [4]  [ 810/2809]  eta: 0:19:50  lr: 0.000040  min_lr: 0.000000  loss: 4.5031 (4.6126)  loss_scale: 65536.0000 (65657.2133)  weight_decay: 0.0500 (0.0500)  time: 0.6587  data: 0.1976  max mem: 15572
Epoch: [4]  [ 820/2809]  eta: 0:19:44  lr: 0.000040  min_lr: 0.000000  loss: 4.5943 (4.6118)  loss_scale: 65536.0000 (65655.7369)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1334  max mem: 15572
Epoch: [4]  [ 830/2809]  eta: 0:19:36  lr: 0.000040  min_lr: 0.000000  loss: 4.5943 (4.6109)  loss_scale: 65536.0000 (65654.2960)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0948  max mem: 15572
Epoch: [4]  [ 840/2809]  eta: 0:19:30  lr: 0.000040  min_lr: 0.000000  loss: 4.4687 (4.6102)  loss_scale: 65536.0000 (65652.8894)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0680  max mem: 15572
Epoch: [4]  [ 850/2809]  eta: 0:19:23  lr: 0.000040  min_lr: 0.000000  loss: 4.6558 (4.6114)  loss_scale: 65536.0000 (65651.5159)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1161  max mem: 15572
Epoch: [4]  [ 860/2809]  eta: 0:19:19  lr: 0.000040  min_lr: 0.000000  loss: 4.6233 (4.6093)  loss_scale: 65536.0000 (65650.1742)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1765  max mem: 15572
Epoch: [4]  [ 870/2809]  eta: 0:19:14  lr: 0.000040  min_lr: 0.000000  loss: 4.4879 (4.6092)  loss_scale: 65536.0000 (65648.8634)  weight_decay: 0.0500 (0.0500)  time: 0.6640  data: 0.2059  max mem: 15572
Epoch: [4]  [ 880/2809]  eta: 0:19:06  lr: 0.000040  min_lr: 0.000000  loss: 4.5325 (4.6069)  loss_scale: 65536.0000 (65647.5823)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1183  max mem: 15572
Epoch: [4]  [ 890/2809]  eta: 0:19:00  lr: 0.000040  min_lr: 0.000000  loss: 4.4572 (4.6063)  loss_scale: 65536.0000 (65646.3300)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1271  max mem: 15572
Epoch: [4]  [ 900/2809]  eta: 0:18:54  lr: 0.000041  min_lr: 0.000000  loss: 4.5267 (4.6059)  loss_scale: 65536.0000 (65645.1054)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1567  max mem: 15572
[2025-01-12 22:32:27,102] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:32:27,102] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:32:30,151] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12140
[2025-01-12 22:32:30,151] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:32:30,151] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 910/2809]  eta: 0:18:50  lr: 0.000041  min_lr: 0.000000  loss: 4.5421 (4.6052)  loss_scale: 65536.0000 (65787.7849)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1707  max mem: 15572
Epoch: [4]  [ 920/2809]  eta: 0:18:44  lr: 0.000041  min_lr: 0.000000  loss: 4.5748 (4.6051)  loss_scale: 65536.0000 (65785.0510)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.1436  max mem: 15572
Epoch: [4]  [ 930/2809]  eta: 0:18:38  lr: 0.000041  min_lr: 0.000000  loss: 4.6368 (4.6053)  loss_scale: 65536.0000 (65782.3759)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1067  max mem: 15572
Epoch: [4]  [ 940/2809]  eta: 0:18:31  lr: 0.000041  min_lr: 0.000000  loss: 4.5675 (4.6039)  loss_scale: 65536.0000 (65779.7577)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1238  max mem: 15572
Epoch: [4]  [ 950/2809]  eta: 0:18:25  lr: 0.000041  min_lr: 0.000000  loss: 4.5675 (4.6038)  loss_scale: 65536.0000 (65777.1945)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1216  max mem: 15572
Epoch: [4]  [ 960/2809]  eta: 0:18:19  lr: 0.000041  min_lr: 0.000000  loss: 4.6376 (4.6035)  loss_scale: 65536.0000 (65774.6847)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1329  max mem: 15572
Epoch: [4]  [ 970/2809]  eta: 0:18:13  lr: 0.000041  min_lr: 0.000000  loss: 4.6238 (4.6034)  loss_scale: 65536.0000 (65772.2266)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1446  max mem: 15572
Epoch: [4]  [ 980/2809]  eta: 0:18:06  lr: 0.000041  min_lr: 0.000000  loss: 4.5325 (4.6036)  loss_scale: 65536.0000 (65769.8186)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1400  max mem: 15572
Epoch: [4]  [ 990/2809]  eta: 0:18:00  lr: 0.000041  min_lr: 0.000000  loss: 4.4896 (4.6026)  loss_scale: 65536.0000 (65767.4591)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1426  max mem: 15572
[2025-01-12 22:33:22,536] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12230
[2025-01-12 22:33:22,537] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:33:22,537] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [1000/2809]  eta: 0:17:53  lr: 0.000041  min_lr: 0.000000  loss: 4.5026 (4.6022)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1439  max mem: 15572
Epoch: [4]  [1010/2809]  eta: 0:17:48  lr: 0.000041  min_lr: 0.000000  loss: 4.5232 (4.6017)  loss_scale: 32768.0000 (65211.8853)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1662  max mem: 15572
Epoch: [4]  [1020/2809]  eta: 0:17:41  lr: 0.000041  min_lr: 0.000000  loss: 4.5540 (4.6009)  loss_scale: 32768.0000 (64894.1195)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1407  max mem: 15572
Epoch: [4]  [1030/2809]  eta: 0:17:36  lr: 0.000041  min_lr: 0.000000  loss: 4.4927 (4.6008)  loss_scale: 32768.0000 (64582.5179)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1042  max mem: 15572
Epoch: [4]  [1040/2809]  eta: 0:17:29  lr: 0.000041  min_lr: 0.000000  loss: 4.4622 (4.6000)  loss_scale: 32768.0000 (64276.9030)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0860  max mem: 15572
Epoch: [4]  [1050/2809]  eta: 0:17:23  lr: 0.000041  min_lr: 0.000000  loss: 4.5369 (4.6002)  loss_scale: 32768.0000 (63977.1037)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.0833  max mem: 15572
Epoch: [4]  [1060/2809]  eta: 0:17:15  lr: 0.000041  min_lr: 0.000000  loss: 4.6520 (4.6011)  loss_scale: 32768.0000 (63682.9557)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0871  max mem: 15572
Epoch: [4]  [1070/2809]  eta: 0:17:10  lr: 0.000041  min_lr: 0.000000  loss: 4.6795 (4.6011)  loss_scale: 32768.0000 (63394.3007)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0953  max mem: 15572
Epoch: [4]  [1080/2809]  eta: 0:17:03  lr: 0.000041  min_lr: 0.000000  loss: 4.5899 (4.6013)  loss_scale: 32768.0000 (63110.9861)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1422  max mem: 15572
Epoch: [4]  [1090/2809]  eta: 0:16:57  lr: 0.000041  min_lr: 0.000000  loss: 4.7356 (4.6019)  loss_scale: 32768.0000 (62832.8653)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1330  max mem: 15572
Epoch: [4]  [1100/2809]  eta: 0:16:53  lr: 0.000041  min_lr: 0.000000  loss: 4.6150 (4.6001)  loss_scale: 32768.0000 (62559.7965)  weight_decay: 0.0500 (0.0500)  time: 0.6549  data: 0.2195  max mem: 15572
Epoch: [4]  [1110/2809]  eta: 0:16:49  lr: 0.000041  min_lr: 0.000000  loss: 4.5366 (4.6007)  loss_scale: 32768.0000 (62291.6436)  weight_decay: 0.0500 (0.0500)  time: 0.7137  data: 0.2510  max mem: 15572
Epoch: [4]  [1120/2809]  eta: 0:16:43  lr: 0.000041  min_lr: 0.000000  loss: 4.5696 (4.6009)  loss_scale: 32768.0000 (62028.2748)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1845  max mem: 15572
[2025-01-12 22:34:40,101] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:34:40,102] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [1130/2809]  eta: 0:16:38  lr: 0.000041  min_lr: 0.000000  loss: 4.5396 (4.6009)  loss_scale: 32768.0000 (62001.3439)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1637  max mem: 15572
Epoch: [4]  [1140/2809]  eta: 0:16:30  lr: 0.000041  min_lr: 0.000000  loss: 4.5396 (4.6000)  loss_scale: 65536.0000 (62032.3225)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0815  max mem: 15572
Epoch: [4]  [1150/2809]  eta: 0:16:24  lr: 0.000041  min_lr: 0.000000  loss: 4.5647 (4.5998)  loss_scale: 65536.0000 (62062.7628)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0581  max mem: 15572
Epoch: [4]  [1160/2809]  eta: 0:16:18  lr: 0.000041  min_lr: 0.000000  loss: 4.5575 (4.5995)  loss_scale: 65536.0000 (62092.6787)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1087  max mem: 15572
Epoch: [4]  [1170/2809]  eta: 0:16:11  lr: 0.000041  min_lr: 0.000000  loss: 4.5575 (4.6004)  loss_scale: 65536.0000 (62122.0837)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0875  max mem: 15572
Epoch: [4]  [1180/2809]  eta: 0:16:05  lr: 0.000041  min_lr: 0.000000  loss: 4.5372 (4.5998)  loss_scale: 65536.0000 (62150.9907)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0628  max mem: 15572
Epoch: [4]  [1190/2809]  eta: 0:15:58  lr: 0.000041  min_lr: 0.000000  loss: 4.5355 (4.5998)  loss_scale: 65536.0000 (62179.4123)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0775  max mem: 15572
Epoch: [4]  [1200/2809]  eta: 0:15:52  lr: 0.000042  min_lr: 0.000000  loss: 4.5988 (4.5991)  loss_scale: 65536.0000 (62207.3605)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1254  max mem: 15572
Epoch: [4]  [1210/2809]  eta: 0:15:47  lr: 0.000042  min_lr: 0.000000  loss: 4.5821 (4.5992)  loss_scale: 65536.0000 (62234.8472)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1314  max mem: 15572
Epoch: [4]  [1220/2809]  eta: 0:15:39  lr: 0.000042  min_lr: 0.000000  loss: 4.5821 (4.5987)  loss_scale: 65536.0000 (62261.8837)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0849  max mem: 15572
Epoch: [4]  [1230/2809]  eta: 0:15:34  lr: 0.000042  min_lr: 0.000000  loss: 4.4997 (4.5970)  loss_scale: 65536.0000 (62288.4809)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1277  max mem: 15572
Epoch: [4]  [1240/2809]  eta: 0:15:28  lr: 0.000042  min_lr: 0.000000  loss: 4.4997 (4.5973)  loss_scale: 65536.0000 (62314.6495)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1818  max mem: 15572
Epoch: [4]  [1250/2809]  eta: 0:15:23  lr: 0.000042  min_lr: 0.000000  loss: 4.6666 (4.5979)  loss_scale: 65536.0000 (62340.3997)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1803  max mem: 15572
[2025-01-12 22:35:52,478] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:35:52,479] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:35:52,946] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12488
[2025-01-12 22:35:52,946] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:35:52,947] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1260/2809]  eta: 0:15:16  lr: 0.000042  min_lr: 0.000000  loss: 4.7100 (4.5986)  loss_scale: 65536.0000 (62417.7129)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1383  max mem: 15572
Epoch: [4]  [1270/2809]  eta: 0:15:13  lr: 0.000042  min_lr: 0.000000  loss: 4.6248 (4.5980)  loss_scale: 65536.0000 (62442.2470)  weight_decay: 0.0500 (0.0500)  time: 0.6578  data: 0.2103  max mem: 15572
Epoch: [4]  [1280/2809]  eta: 0:15:07  lr: 0.000042  min_lr: 0.000000  loss: 4.5812 (4.5979)  loss_scale: 65536.0000 (62466.3981)  weight_decay: 0.0500 (0.0500)  time: 0.7069  data: 0.2344  max mem: 15572
Epoch: [4]  [1290/2809]  eta: 0:15:01  lr: 0.000042  min_lr: 0.000000  loss: 4.6259 (4.5981)  loss_scale: 65536.0000 (62490.1751)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1192  max mem: 15572
Epoch: [4]  [1300/2809]  eta: 0:14:55  lr: 0.000042  min_lr: 0.000000  loss: 4.5282 (4.5984)  loss_scale: 65536.0000 (62513.5865)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0978  max mem: 15572
Epoch: [4]  [1310/2809]  eta: 0:14:48  lr: 0.000042  min_lr: 0.000000  loss: 4.5831 (4.5981)  loss_scale: 65536.0000 (62536.6407)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0693  max mem: 15572
Epoch: [4]  [1320/2809]  eta: 0:14:41  lr: 0.000042  min_lr: 0.000000  loss: 4.5568 (4.5973)  loss_scale: 65536.0000 (62559.3460)  weight_decay: 0.0500 (0.0500)  time: 0.5227  data: 0.0786  max mem: 15572
Epoch: [4]  [1330/2809]  eta: 0:14:35  lr: 0.000042  min_lr: 0.000000  loss: 4.5321 (4.5967)  loss_scale: 65536.0000 (62581.7100)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1172  max mem: 15572
Epoch: [4]  [1340/2809]  eta: 0:14:29  lr: 0.000042  min_lr: 0.000000  loss: 4.6038 (4.5971)  loss_scale: 65536.0000 (62603.7405)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1416  max mem: 15572
Epoch: [4]  [1350/2809]  eta: 0:14:23  lr: 0.000042  min_lr: 0.000000  loss: 4.6608 (4.5970)  loss_scale: 65536.0000 (62625.4449)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1249  max mem: 15572
Epoch: [4]  [1360/2809]  eta: 0:14:17  lr: 0.000042  min_lr: 0.000000  loss: 4.4760 (4.5963)  loss_scale: 65536.0000 (62646.8303)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1153  max mem: 15572
Epoch: [4]  [1370/2809]  eta: 0:14:11  lr: 0.000042  min_lr: 0.000000  loss: 4.4761 (4.5957)  loss_scale: 65536.0000 (62667.9037)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1281  max mem: 15572
Epoch: [4]  [1380/2809]  eta: 0:14:05  lr: 0.000042  min_lr: 0.000000  loss: 4.5338 (4.5961)  loss_scale: 65536.0000 (62688.6720)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1417  max mem: 15572
[2025-01-12 22:37:08,886] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:37:08,887] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:37:11,813] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12621
[2025-01-12 22:37:11,813] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:37:11,813] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1390/2809]  eta: 0:13:59  lr: 0.000042  min_lr: 0.000000  loss: 4.5726 (4.5958)  loss_scale: 65536.0000 (62897.5988)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1339  max mem: 15572
Epoch: [4]  [1400/2809]  eta: 0:13:53  lr: 0.000042  min_lr: 0.000000  loss: 4.5746 (4.5956)  loss_scale: 65536.0000 (62916.4311)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1042  max mem: 15572
Epoch: [4]  [1410/2809]  eta: 0:13:47  lr: 0.000042  min_lr: 0.000000  loss: 4.6537 (4.5958)  loss_scale: 65536.0000 (62934.9965)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1509  max mem: 15572
Epoch: [4]  [1420/2809]  eta: 0:13:41  lr: 0.000042  min_lr: 0.000000  loss: 4.5954 (4.5958)  loss_scale: 65536.0000 (62953.3005)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1484  max mem: 15572
Epoch: [4]  [1430/2809]  eta: 0:13:36  lr: 0.000042  min_lr: 0.000000  loss: 4.5864 (4.5958)  loss_scale: 65536.0000 (62971.3487)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1573  max mem: 15572
Epoch: [4]  [1440/2809]  eta: 0:13:29  lr: 0.000042  min_lr: 0.000000  loss: 4.5820 (4.5956)  loss_scale: 65536.0000 (62989.1464)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1649  max mem: 15572
Epoch: [4]  [1450/2809]  eta: 0:13:23  lr: 0.000042  min_lr: 0.000000  loss: 4.6160 (4.5958)  loss_scale: 65536.0000 (63006.6988)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1098  max mem: 15572
Epoch: [4]  [1460/2809]  eta: 0:13:17  lr: 0.000042  min_lr: 0.000000  loss: 4.5936 (4.5957)  loss_scale: 65536.0000 (63024.0110)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1434  max mem: 15572
Epoch: [4]  [1470/2809]  eta: 0:13:12  lr: 0.000042  min_lr: 0.000000  loss: 4.5646 (4.5958)  loss_scale: 65536.0000 (63041.0877)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1927  max mem: 15572
Epoch: [4]  [1480/2809]  eta: 0:13:06  lr: 0.000042  min_lr: 0.000000  loss: 4.6192 (4.5960)  loss_scale: 65536.0000 (63057.9338)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1702  max mem: 15572
Epoch: [4]  [1490/2809]  eta: 0:12:59  lr: 0.000042  min_lr: 0.000000  loss: 4.6114 (4.5956)  loss_scale: 65536.0000 (63074.5540)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0825  max mem: 15572
Epoch: [4]  [1500/2809]  eta: 0:12:54  lr: 0.000043  min_lr: 0.000000  loss: 4.5555 (4.5952)  loss_scale: 65536.0000 (63090.9527)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0987  max mem: 15572
Epoch: [4]  [1510/2809]  eta: 0:12:47  lr: 0.000043  min_lr: 0.000000  loss: 4.5611 (4.5949)  loss_scale: 65536.0000 (63107.1343)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1223  max mem: 15572
[2025-01-12 22:38:28,612] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:38:28,612] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:38:30,600] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12754
[2025-01-12 22:38:30,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:38:30,601] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1520/2809]  eta: 0:12:43  lr: 0.000043  min_lr: 0.000000  loss: 4.5485 (4.5942)  loss_scale: 65536.0000 (63295.4530)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.1592  max mem: 15572
Epoch: [4]  [1530/2809]  eta: 0:12:36  lr: 0.000043  min_lr: 0.000000  loss: 4.4743 (4.5941)  loss_scale: 65536.0000 (63310.0875)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1523  max mem: 15572
Epoch: [4]  [1540/2809]  eta: 0:12:29  lr: 0.000043  min_lr: 0.000000  loss: 4.6206 (4.5948)  loss_scale: 65536.0000 (63324.5321)  weight_decay: 0.0500 (0.0500)  time: 0.5166  data: 0.0807  max mem: 15572
Epoch: [4]  [1550/2809]  eta: 0:12:23  lr: 0.000043  min_lr: 0.000000  loss: 4.6253 (4.5945)  loss_scale: 65536.0000 (63338.7905)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1020  max mem: 15572
Epoch: [4]  [1560/2809]  eta: 0:12:18  lr: 0.000043  min_lr: 0.000000  loss: 4.6298 (4.5945)  loss_scale: 65536.0000 (63352.8661)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1141  max mem: 15572
Epoch: [4]  [1570/2809]  eta: 0:12:12  lr: 0.000043  min_lr: 0.000000  loss: 4.6809 (4.5954)  loss_scale: 65536.0000 (63366.7626)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1089  max mem: 15572
Epoch: [4]  [1580/2809]  eta: 0:12:06  lr: 0.000043  min_lr: 0.000000  loss: 4.7048 (4.5960)  loss_scale: 65536.0000 (63380.4832)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1310  max mem: 15572
Epoch: [4]  [1590/2809]  eta: 0:12:01  lr: 0.000043  min_lr: 0.000000  loss: 4.7041 (4.5961)  loss_scale: 65536.0000 (63394.0314)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.2001  max mem: 15572
Epoch: [4]  [1600/2809]  eta: 0:11:55  lr: 0.000043  min_lr: 0.000000  loss: 4.6053 (4.5962)  loss_scale: 65536.0000 (63407.4104)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.1634  max mem: 15572
Epoch: [4]  [1610/2809]  eta: 0:11:49  lr: 0.000043  min_lr: 0.000000  loss: 4.6351 (4.5959)  loss_scale: 65536.0000 (63420.6232)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0609  max mem: 15572
Epoch: [4]  [1620/2809]  eta: 0:11:43  lr: 0.000043  min_lr: 0.000000  loss: 4.5440 (4.5950)  loss_scale: 65536.0000 (63433.6730)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0641  max mem: 15572
Epoch: [4]  [1630/2809]  eta: 0:11:37  lr: 0.000043  min_lr: 0.000000  loss: 4.5355 (4.5956)  loss_scale: 65536.0000 (63446.5628)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1012  max mem: 15572
Epoch: [4]  [1640/2809]  eta: 0:11:31  lr: 0.000043  min_lr: 0.000000  loss: 4.5647 (4.5956)  loss_scale: 65536.0000 (63459.2956)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1239  max mem: 15572
[2025-01-12 22:39:45,797] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:39:45,797] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1650/2809]  eta: 0:11:25  lr: 0.000043  min_lr: 0.000000  loss: 4.5735 (4.5959)  loss_scale: 65536.0000 (63630.6529)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1207  max mem: 15572
[2025-01-12 22:39:52,230] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12896
[2025-01-12 22:39:52,230] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:39:52,230] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1660/2809]  eta: 0:11:18  lr: 0.000043  min_lr: 0.000000  loss: 4.5753 (4.5957)  loss_scale: 131072.0000 (63997.2258)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0793  max mem: 15572
Epoch: [4]  [1670/2809]  eta: 0:11:12  lr: 0.000043  min_lr: 0.000000  loss: 4.5788 (4.5962)  loss_scale: 65536.0000 (64006.4345)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1243  max mem: 15572
Epoch: [4]  [1680/2809]  eta: 0:11:07  lr: 0.000043  min_lr: 0.000000  loss: 4.5627 (4.5953)  loss_scale: 65536.0000 (64015.5336)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1712  max mem: 15572
Epoch: [4]  [1690/2809]  eta: 0:11:01  lr: 0.000043  min_lr: 0.000000  loss: 4.6312 (4.5960)  loss_scale: 65536.0000 (64024.5251)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1581  max mem: 15572
Epoch: [4]  [1700/2809]  eta: 0:10:55  lr: 0.000043  min_lr: 0.000000  loss: 4.5898 (4.5949)  loss_scale: 65536.0000 (64033.4109)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1703  max mem: 15572
Epoch: [4]  [1710/2809]  eta: 0:10:49  lr: 0.000043  min_lr: 0.000000  loss: 4.4489 (4.5948)  loss_scale: 65536.0000 (64042.1929)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1664  max mem: 15572
Epoch: [4]  [1720/2809]  eta: 0:10:44  lr: 0.000043  min_lr: 0.000000  loss: 4.4489 (4.5943)  loss_scale: 65536.0000 (64050.8727)  weight_decay: 0.0500 (0.0500)  time: 0.6509  data: 0.1813  max mem: 15572
Epoch: [4]  [1730/2809]  eta: 0:10:38  lr: 0.000043  min_lr: 0.000000  loss: 4.5409 (4.5946)  loss_scale: 65536.0000 (64059.4523)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1429  max mem: 15572
Epoch: [4]  [1740/2809]  eta: 0:10:32  lr: 0.000043  min_lr: 0.000000  loss: 4.6580 (4.5947)  loss_scale: 65536.0000 (64067.9334)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1020  max mem: 15572
Epoch: [4]  [1750/2809]  eta: 0:10:26  lr: 0.000043  min_lr: 0.000000  loss: 4.5541 (4.5943)  loss_scale: 65536.0000 (64076.3175)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1145  max mem: 15572
Epoch: [4]  [1760/2809]  eta: 0:10:19  lr: 0.000043  min_lr: 0.000000  loss: 4.4354 (4.5932)  loss_scale: 65536.0000 (64084.6065)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0910  max mem: 15572
[2025-01-12 22:40:55,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=77, lr=[4.2037310626888885e-07, 4.2037310626888885e-07, 6.005330089555555e-07, 6.005330089555555e-07, 8.579042985079365e-07, 8.579042985079365e-07, 1.2255775692970524e-06, 1.2255775692970524e-06, 1.750825098995789e-06, 1.750825098995789e-06, 2.5011787128511272e-06, 2.5011787128511272e-06, 3.573112446930182e-06, 3.573112446930182e-06, 5.1044463527574035e-06, 5.1044463527574035e-06, 7.292066218224862e-06, 7.292066218224862e-06, 1.0417237454606947e-05, 1.0417237454606947e-05, 1.4881767792295639e-05, 1.4881767792295639e-05, 2.1259668274708057e-05, 2.1259668274708057e-05, 3.037095467815437e-05, 3.037095467815437e-05, 4.33870781116491e-05, 4.33870781116491e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:40:55,453] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=27.947502007141455, CurrSamplesPerSec=26.105694257505853, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [1770/2809]  eta: 0:10:14  lr: 0.000043  min_lr: 0.000000  loss: 4.4516 (4.5931)  loss_scale: 65536.0000 (64092.8018)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1474  max mem: 15572
Epoch: [4]  [1780/2809]  eta: 0:10:07  lr: 0.000043  min_lr: 0.000000  loss: 4.5454 (4.5927)  loss_scale: 65536.0000 (64100.9051)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1368  max mem: 15572
[2025-01-12 22:41:08,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:41:08,729] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1790/2809]  eta: 0:10:01  lr: 0.000043  min_lr: 0.000000  loss: 4.5447 (4.5927)  loss_scale: 65536.0000 (64182.1016)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.1030  max mem: 15572
[2025-01-12 22:41:15,311] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13036
[2025-01-12 22:41:15,311] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:41:15,311] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1800/2809]  eta: 0:09:56  lr: 0.000044  min_lr: 0.000000  loss: 4.5969 (4.5930)  loss_scale: 131072.0000 (64517.1172)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1598  max mem: 15572
Epoch: [4]  [1810/2809]  eta: 0:09:50  lr: 0.000044  min_lr: 0.000000  loss: 4.6075 (4.5931)  loss_scale: 65536.0000 (64522.7432)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1798  max mem: 15572
Epoch: [4]  [1820/2809]  eta: 0:09:44  lr: 0.000044  min_lr: 0.000000  loss: 4.6075 (4.5932)  loss_scale: 65536.0000 (64528.3075)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1907  max mem: 15572
Epoch: [4]  [1830/2809]  eta: 0:09:39  lr: 0.000044  min_lr: 0.000000  loss: 4.4612 (4.5919)  loss_scale: 65536.0000 (64533.8110)  weight_decay: 0.0500 (0.0500)  time: 0.6661  data: 0.2116  max mem: 15572
Epoch: [4]  [1840/2809]  eta: 0:09:32  lr: 0.000044  min_lr: 0.000000  loss: 4.4628 (4.5917)  loss_scale: 65536.0000 (64539.2548)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1453  max mem: 15572
Epoch: [4]  [1850/2809]  eta: 0:09:27  lr: 0.000044  min_lr: 0.000000  loss: 4.6136 (4.5916)  loss_scale: 65536.0000 (64544.6397)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.1006  max mem: 15572
Epoch: [4]  [1860/2809]  eta: 0:09:20  lr: 0.000044  min_lr: 0.000000  loss: 4.6238 (4.5918)  loss_scale: 65536.0000 (64549.9667)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0935  max mem: 15572
Epoch: [4]  [1870/2809]  eta: 0:09:14  lr: 0.000044  min_lr: 0.000000  loss: 4.5896 (4.5920)  loss_scale: 65536.0000 (64555.2368)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1134  max mem: 15572
Epoch: [4]  [1880/2809]  eta: 0:09:08  lr: 0.000044  min_lr: 0.000000  loss: 4.5241 (4.5913)  loss_scale: 65536.0000 (64560.4508)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1358  max mem: 15572
Epoch: [4]  [1890/2809]  eta: 0:09:02  lr: 0.000044  min_lr: 0.000000  loss: 4.4713 (4.5912)  loss_scale: 65536.0000 (64565.6097)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0955  max mem: 15572
Epoch: [4]  [1900/2809]  eta: 0:08:56  lr: 0.000044  min_lr: 0.000000  loss: 4.5949 (4.5914)  loss_scale: 65536.0000 (64570.7144)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1279  max mem: 15572
Epoch: [4]  [1910/2809]  eta: 0:08:51  lr: 0.000044  min_lr: 0.000000  loss: 4.6307 (4.5913)  loss_scale: 65536.0000 (64575.7656)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1555  max mem: 15572
Epoch: [4]  [1920/2809]  eta: 0:08:44  lr: 0.000044  min_lr: 0.000000  loss: 4.6307 (4.5914)  loss_scale: 65536.0000 (64580.7642)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0985  max mem: 15572
[2025-01-12 22:42:30,605] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:42:30,607] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1930/2809]  eta: 0:08:38  lr: 0.000044  min_lr: 0.000000  loss: 4.4590 (4.5903)  loss_scale: 65536.0000 (64653.5888)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.0873  max mem: 15572
[2025-01-12 22:42:36,026] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13174
[2025-01-12 22:42:36,026] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:42:36,026] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1940/2809]  eta: 0:08:32  lr: 0.000044  min_lr: 0.000000  loss: 4.3501 (4.5899)  loss_scale: 65536.0000 (64894.4833)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1359  max mem: 15572
Epoch: [4]  [1950/2809]  eta: 0:08:27  lr: 0.000044  min_lr: 0.000000  loss: 4.5617 (4.5895)  loss_scale: 65536.0000 (64897.7714)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1415  max mem: 15572
Epoch: [4]  [1960/2809]  eta: 0:08:21  lr: 0.000044  min_lr: 0.000000  loss: 4.5173 (4.5888)  loss_scale: 65536.0000 (64901.0260)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1598  max mem: 15572
Epoch: [4]  [1970/2809]  eta: 0:08:15  lr: 0.000044  min_lr: 0.000000  loss: 4.4271 (4.5881)  loss_scale: 65536.0000 (64904.2476)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1345  max mem: 15572
Epoch: [4]  [1980/2809]  eta: 0:08:09  lr: 0.000044  min_lr: 0.000000  loss: 4.5596 (4.5883)  loss_scale: 65536.0000 (64907.4366)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1032  max mem: 15572
Epoch: [4]  [1990/2809]  eta: 0:08:03  lr: 0.000044  min_lr: 0.000000  loss: 4.6807 (4.5884)  loss_scale: 65536.0000 (64910.5937)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1626  max mem: 15572
Epoch: [4]  [2000/2809]  eta: 0:07:58  lr: 0.000044  min_lr: 0.000000  loss: 4.4378 (4.5877)  loss_scale: 65536.0000 (64913.7191)  weight_decay: 0.0500 (0.0500)  time: 0.6773  data: 0.2378  max mem: 15572
Epoch: [4]  [2010/2809]  eta: 0:07:52  lr: 0.000044  min_lr: 0.000000  loss: 4.4240 (4.5865)  loss_scale: 65536.0000 (64916.8135)  weight_decay: 0.0500 (0.0500)  time: 0.6767  data: 0.2160  max mem: 15572
Epoch: [4]  [2020/2809]  eta: 0:07:46  lr: 0.000044  min_lr: 0.000000  loss: 4.4891 (4.5866)  loss_scale: 65536.0000 (64919.8773)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1467  max mem: 15572
Epoch: [4]  [2030/2809]  eta: 0:07:40  lr: 0.000044  min_lr: 0.000000  loss: 4.5601 (4.5861)  loss_scale: 65536.0000 (64922.9109)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0870  max mem: 15572
Epoch: [4]  [2040/2809]  eta: 0:07:34  lr: 0.000044  min_lr: 0.000000  loss: 4.4404 (4.5854)  loss_scale: 65536.0000 (64925.9147)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0796  max mem: 15572
Epoch: [4]  [2050/2809]  eta: 0:07:28  lr: 0.000044  min_lr: 0.000000  loss: 4.4404 (4.5851)  loss_scale: 65536.0000 (64928.8893)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0977  max mem: 15572
Epoch: [4]  [2060/2809]  eta: 0:07:22  lr: 0.000044  min_lr: 0.000000  loss: 4.6034 (4.5856)  loss_scale: 65536.0000 (64931.8350)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0947  max mem: 15572
[2025-01-12 22:43:52,252] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:43:52,252] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2070/2809]  eta: 0:07:16  lr: 0.000044  min_lr: 0.000000  loss: 4.7169 (4.5859)  loss_scale: 65536.0000 (65061.3308)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0531  max mem: 15572
[2025-01-12 22:43:59,041] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13315
[2025-01-12 22:43:59,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:43:59,042] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2080/2809]  eta: 0:07:10  lr: 0.000044  min_lr: 0.000000  loss: 4.4963 (4.5851)  loss_scale: 131072.0000 (65315.5521)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0605  max mem: 15572
Epoch: [4]  [2090/2809]  eta: 0:07:04  lr: 0.000044  min_lr: 0.000000  loss: 4.4697 (4.5851)  loss_scale: 65536.0000 (65316.6064)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1711  max mem: 15572
Epoch: [4]  [2100/2809]  eta: 0:06:58  lr: 0.000045  min_lr: 0.000000  loss: 4.5032 (4.5847)  loss_scale: 65536.0000 (65317.6506)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1735  max mem: 15572
Epoch: [4]  [2110/2809]  eta: 0:06:52  lr: 0.000045  min_lr: 0.000000  loss: 4.5000 (4.5847)  loss_scale: 65536.0000 (65318.6850)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0906  max mem: 15572
Epoch: [4]  [2120/2809]  eta: 0:06:46  lr: 0.000045  min_lr: 0.000000  loss: 4.5368 (4.5846)  loss_scale: 65536.0000 (65319.7096)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1081  max mem: 15572
Epoch: [4]  [2130/2809]  eta: 0:06:40  lr: 0.000045  min_lr: 0.000000  loss: 4.6162 (4.5847)  loss_scale: 65536.0000 (65320.7245)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1199  max mem: 15572
Epoch: [4]  [2140/2809]  eta: 0:06:34  lr: 0.000045  min_lr: 0.000000  loss: 4.6671 (4.5850)  loss_scale: 65536.0000 (65321.7300)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1018  max mem: 15572
Epoch: [4]  [2150/2809]  eta: 0:06:28  lr: 0.000045  min_lr: 0.000000  loss: 4.6173 (4.5849)  loss_scale: 65536.0000 (65322.7262)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1083  max mem: 15572
Epoch: [4]  [2160/2809]  eta: 0:06:23  lr: 0.000045  min_lr: 0.000000  loss: 4.5917 (4.5846)  loss_scale: 65536.0000 (65323.7131)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1463  max mem: 15572
Epoch: [4]  [2170/2809]  eta: 0:06:17  lr: 0.000045  min_lr: 0.000000  loss: 4.4804 (4.5841)  loss_scale: 65536.0000 (65324.6909)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.1596  max mem: 15572
Epoch: [4]  [2180/2809]  eta: 0:06:11  lr: 0.000045  min_lr: 0.000000  loss: 4.4533 (4.5838)  loss_scale: 65536.0000 (65325.6598)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0713  max mem: 15572
Epoch: [4]  [2190/2809]  eta: 0:06:05  lr: 0.000045  min_lr: 0.000000  loss: 4.6060 (4.5842)  loss_scale: 65536.0000 (65326.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0815  max mem: 15572
Epoch: [4]  [2200/2809]  eta: 0:05:59  lr: 0.000045  min_lr: 0.000000  loss: 4.6670 (4.5846)  loss_scale: 65536.0000 (65327.5711)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1126  max mem: 15572
[2025-01-12 22:45:14,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:45:14,265] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2210/2809]  eta: 0:05:53  lr: 0.000045  min_lr: 0.000000  loss: 4.6081 (4.5844)  loss_scale: 65536.0000 (65417.4365)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1161  max mem: 15572
[2025-01-12 22:45:17,742] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13448
[2025-01-12 22:45:17,743] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:45:17,743] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2220/2809]  eta: 0:05:47  lr: 0.000045  min_lr: 0.000000  loss: 4.6081 (4.5845)  loss_scale: 65536.0000 (65447.4777)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1589  max mem: 15572
Epoch: [4]  [2230/2809]  eta: 0:05:41  lr: 0.000045  min_lr: 0.000000  loss: 4.6177 (4.5846)  loss_scale: 65536.0000 (65447.8745)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1581  max mem: 15572
Epoch: [4]  [2240/2809]  eta: 0:05:35  lr: 0.000045  min_lr: 0.000000  loss: 4.5419 (4.5841)  loss_scale: 65536.0000 (65448.2677)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0958  max mem: 15572
Epoch: [4]  [2250/2809]  eta: 0:05:29  lr: 0.000045  min_lr: 0.000000  loss: 4.6165 (4.5841)  loss_scale: 65536.0000 (65448.6575)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1419  max mem: 15572
Epoch: [4]  [2260/2809]  eta: 0:05:23  lr: 0.000045  min_lr: 0.000000  loss: 4.6250 (4.5843)  loss_scale: 65536.0000 (65449.0438)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1514  max mem: 15572
Epoch: [4]  [2270/2809]  eta: 0:05:17  lr: 0.000045  min_lr: 0.000000  loss: 4.6168 (4.5845)  loss_scale: 65536.0000 (65449.4267)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0767  max mem: 15572
Epoch: [4]  [2280/2809]  eta: 0:05:11  lr: 0.000045  min_lr: 0.000000  loss: 4.6759 (4.5846)  loss_scale: 65536.0000 (65449.8062)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1064  max mem: 15572
Epoch: [4]  [2290/2809]  eta: 0:05:05  lr: 0.000045  min_lr: 0.000000  loss: 4.5506 (4.5841)  loss_scale: 65536.0000 (65450.1825)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1120  max mem: 15572
Epoch: [4]  [2300/2809]  eta: 0:05:00  lr: 0.000045  min_lr: 0.000000  loss: 4.5391 (4.5841)  loss_scale: 65536.0000 (65450.5554)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.1719  max mem: 15572
Epoch: [4]  [2310/2809]  eta: 0:04:54  lr: 0.000045  min_lr: 0.000000  loss: 4.5358 (4.5836)  loss_scale: 65536.0000 (65450.9251)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1231  max mem: 15572
Epoch: [4]  [2320/2809]  eta: 0:04:48  lr: 0.000045  min_lr: 0.000000  loss: 4.3291 (4.5821)  loss_scale: 65536.0000 (65451.2917)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0666  max mem: 15572
Epoch: [4]  [2330/2809]  eta: 0:04:42  lr: 0.000045  min_lr: 0.000000  loss: 4.3173 (4.5814)  loss_scale: 65536.0000 (65451.6551)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.1058  max mem: 15572
Epoch: [4]  [2340/2809]  eta: 0:04:35  lr: 0.000045  min_lr: 0.000000  loss: 4.5081 (4.5811)  loss_scale: 65536.0000 (65452.0154)  weight_decay: 0.0500 (0.0500)  time: 0.4640  data: 0.0402  max mem: 15572
[2025-01-12 22:46:28,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:46:28,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:46:30,981] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13583
[2025-01-12 22:46:30,982] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:46:30,982] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2350/2809]  eta: 0:04:29  lr: 0.000045  min_lr: 0.000000  loss: 4.5757 (4.5812)  loss_scale: 65536.0000 (65619.6274)  weight_decay: 0.0500 (0.0500)  time: 0.4051  data: 0.0006  max mem: 15572
Epoch: [4]  [2360/2809]  eta: 0:04:23  lr: 0.000045  min_lr: 0.000000  loss: 4.5669 (4.5810)  loss_scale: 65536.0000 (65619.2732)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0773  max mem: 15572
Epoch: [4]  [2370/2809]  eta: 0:04:18  lr: 0.000045  min_lr: 0.000000  loss: 4.4591 (4.5809)  loss_scale: 65536.0000 (65618.9220)  weight_decay: 0.0500 (0.0500)  time: 0.6593  data: 0.1940  max mem: 15572
Epoch: [4]  [2380/2809]  eta: 0:04:12  lr: 0.000045  min_lr: 0.000000  loss: 4.4675 (4.5810)  loss_scale: 65536.0000 (65618.5737)  weight_decay: 0.0500 (0.0500)  time: 0.6844  data: 0.2242  max mem: 15572
Epoch: [4]  [2390/2809]  eta: 0:04:06  lr: 0.000045  min_lr: 0.000000  loss: 4.4889 (4.5806)  loss_scale: 65536.0000 (65618.2284)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.1491  max mem: 15572
Epoch: [4]  [2400/2809]  eta: 0:04:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4894 (4.5807)  loss_scale: 65536.0000 (65617.8859)  weight_decay: 0.0500 (0.0500)  time: 0.6589  data: 0.1633  max mem: 15572
Epoch: [4]  [2410/2809]  eta: 0:03:55  lr: 0.000046  min_lr: 0.000000  loss: 4.4055 (4.5797)  loss_scale: 65536.0000 (65617.5462)  weight_decay: 0.0500 (0.0500)  time: 0.6893  data: 0.2224  max mem: 15572
[2025-01-12 22:47:17,657] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13654
[2025-01-12 22:47:17,658] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:47:17,658] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [2420/2809]  eta: 0:03:49  lr: 0.000046  min_lr: 0.000000  loss: 4.3563 (4.5789)  loss_scale: 65536.0000 (65576.6047)  weight_decay: 0.0500 (0.0500)  time: 0.6570  data: 0.1970  max mem: 15572
Epoch: [4]  [2430/2809]  eta: 0:03:43  lr: 0.000046  min_lr: 0.000000  loss: 4.4660 (4.5788)  loss_scale: 32768.0000 (65441.6454)  weight_decay: 0.0500 (0.0500)  time: 0.7180  data: 0.2358  max mem: 15572
Epoch: [4]  [2440/2809]  eta: 0:03:38  lr: 0.000046  min_lr: 0.000000  loss: 4.5742 (4.5789)  loss_scale: 32768.0000 (65307.7919)  weight_decay: 0.0500 (0.0500)  time: 0.7945  data: 0.3012  max mem: 15572
Epoch: [4]  [2450/2809]  eta: 0:03:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4893 (4.5783)  loss_scale: 32768.0000 (65175.0306)  weight_decay: 0.0500 (0.0500)  time: 0.6783  data: 0.1785  max mem: 15572
Epoch: [4]  [2460/2809]  eta: 0:03:26  lr: 0.000046  min_lr: 0.000000  loss: 4.3672 (4.5773)  loss_scale: 32768.0000 (65043.3482)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1047  max mem: 15572
Epoch: [4]  [2470/2809]  eta: 0:03:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3710 (4.5770)  loss_scale: 32768.0000 (64912.7317)  weight_decay: 0.0500 (0.0500)  time: 0.6792  data: 0.1887  max mem: 15572
Epoch: [4]  [2480/2809]  eta: 0:03:14  lr: 0.000046  min_lr: 0.000000  loss: 4.4732 (4.5770)  loss_scale: 32768.0000 (64783.1681)  weight_decay: 0.0500 (0.0500)  time: 0.6500  data: 0.1759  max mem: 15572
Epoch: [4]  [2490/2809]  eta: 0:03:08  lr: 0.000046  min_lr: 0.000000  loss: 4.5940 (4.5773)  loss_scale: 32768.0000 (64654.6447)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0749  max mem: 15572
Epoch: [4]  [2500/2809]  eta: 0:03:02  lr: 0.000046  min_lr: 0.000000  loss: 4.5839 (4.5769)  loss_scale: 32768.0000 (64527.1491)  weight_decay: 0.0500 (0.0500)  time: 0.4295  data: 0.0005  max mem: 15572
Epoch: [4]  [2510/2809]  eta: 0:02:56  lr: 0.000046  min_lr: 0.000000  loss: 4.4788 (4.5769)  loss_scale: 32768.0000 (64400.6691)  weight_decay: 0.0500 (0.0500)  time: 0.4653  data: 0.0009  max mem: 15572
Epoch: [4]  [2520/2809]  eta: 0:02:50  lr: 0.000046  min_lr: 0.000000  loss: 4.6072 (4.5773)  loss_scale: 32768.0000 (64275.1924)  weight_decay: 0.0500 (0.0500)  time: 0.4730  data: 0.0012  max mem: 15572
Epoch: [4]  [2530/2809]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000000  loss: 4.6464 (4.5771)  loss_scale: 32768.0000 (64150.7072)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0520  max mem: 15572
Epoch: [4]  [2540/2809]  eta: 0:02:38  lr: 0.000046  min_lr: 0.000000  loss: 4.6038 (4.5771)  loss_scale: 32768.0000 (64027.2019)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1205  max mem: 15572
[2025-01-12 22:48:33,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:48:33,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [2550/2809]  eta: 0:02:32  lr: 0.000046  min_lr: 0.000000  loss: 4.6208 (4.5772)  loss_scale: 32768.0000 (63956.0455)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0977  max mem: 15572
Epoch: [4]  [2560/2809]  eta: 0:02:26  lr: 0.000046  min_lr: 0.000000  loss: 4.6000 (4.5770)  loss_scale: 65536.0000 (63962.2148)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1044  max mem: 15572
Epoch: [4]  [2570/2809]  eta: 0:02:20  lr: 0.000046  min_lr: 0.000000  loss: 4.4646 (4.5767)  loss_scale: 65536.0000 (63968.3361)  weight_decay: 0.0500 (0.0500)  time: 0.6603  data: 0.1973  max mem: 15572
Epoch: [4]  [2580/2809]  eta: 0:02:14  lr: 0.000046  min_lr: 0.000000  loss: 4.4283 (4.5760)  loss_scale: 65536.0000 (63974.4099)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1322  max mem: 15572
Epoch: [4]  [2590/2809]  eta: 0:02:09  lr: 0.000046  min_lr: 0.000000  loss: 4.4049 (4.5760)  loss_scale: 65536.0000 (63980.4369)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0775  max mem: 15572
Epoch: [4]  [2600/2809]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000000  loss: 4.5976 (4.5759)  loss_scale: 65536.0000 (63986.4175)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1443  max mem: 15572
Epoch: [4]  [2610/2809]  eta: 0:01:57  lr: 0.000046  min_lr: 0.000000  loss: 4.5468 (4.5758)  loss_scale: 65536.0000 (63992.3524)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0887  max mem: 15572
Epoch: [4]  [2620/2809]  eta: 0:01:51  lr: 0.000046  min_lr: 0.000000  loss: 4.5468 (4.5759)  loss_scale: 65536.0000 (63998.2419)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0647  max mem: 15572
Epoch: [4]  [2630/2809]  eta: 0:01:45  lr: 0.000046  min_lr: 0.000000  loss: 4.5451 (4.5757)  loss_scale: 65536.0000 (64004.0867)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1054  max mem: 15572
Epoch: [4]  [2640/2809]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000000  loss: 4.4255 (4.5749)  loss_scale: 65536.0000 (64009.8872)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1231  max mem: 15572
Epoch: [4]  [2650/2809]  eta: 0:01:33  lr: 0.000046  min_lr: 0.000000  loss: 4.5397 (4.5747)  loss_scale: 65536.0000 (64015.6439)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1324  max mem: 15572
Epoch: [4]  [2660/2809]  eta: 0:01:27  lr: 0.000046  min_lr: 0.000000  loss: 4.6022 (4.5747)  loss_scale: 65536.0000 (64021.3574)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1461  max mem: 15572
Epoch: [4]  [2670/2809]  eta: 0:01:21  lr: 0.000046  min_lr: 0.000000  loss: 4.5963 (4.5747)  loss_scale: 65536.0000 (64027.0281)  weight_decay: 0.0500 (0.0500)  time: 0.6477  data: 0.1511  max mem: 15572
[2025-01-12 22:49:48,693] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:49:48,694] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2680/2809]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000000  loss: 4.6991 (4.5752)  loss_scale: 65536.0000 (64179.3241)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0796  max mem: 15572
[2025-01-12 22:49:55,776] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13922
[2025-01-12 22:49:55,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:49:55,777] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2690/2809]  eta: 0:01:10  lr: 0.000046  min_lr: 0.000000  loss: 4.5299 (4.5745)  loss_scale: 131072.0000 (64306.1345)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0881  max mem: 15572
Epoch: [4]  [2700/2809]  eta: 0:01:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4509 (4.5742)  loss_scale: 65536.0000 (64310.6879)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1452  max mem: 15572
Epoch: [4]  [2710/2809]  eta: 0:00:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4840 (4.5739)  loss_scale: 65536.0000 (64315.2077)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1556  max mem: 15572
Epoch: [4]  [2720/2809]  eta: 0:00:52  lr: 0.000047  min_lr: 0.000000  loss: 4.4703 (4.5735)  loss_scale: 65536.0000 (64319.6942)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1397  max mem: 15572
Epoch: [4]  [2730/2809]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4432 (4.5734)  loss_scale: 65536.0000 (64324.1479)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0721  max mem: 15572
Epoch: [4]  [2740/2809]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4432 (4.5727)  loss_scale: 65536.0000 (64328.5691)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0689  max mem: 15572
Epoch: [4]  [2750/2809]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4983 (4.5728)  loss_scale: 65536.0000 (64332.9582)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1190  max mem: 15572
Epoch: [4]  [2760/2809]  eta: 0:00:28  lr: 0.000047  min_lr: 0.000000  loss: 4.5804 (4.5729)  loss_scale: 65536.0000 (64337.3155)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1540  max mem: 15572
[2025-01-12 22:50:41,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=84, lr=[4.527119866649877e-07, 4.527119866649877e-07, 6.467314095214111e-07, 6.467314095214111e-07, 9.239020136020159e-07, 9.239020136020159e-07, 1.3198600194314515e-06, 1.3198600194314515e-06, 1.8855143134735021e-06, 1.8855143134735021e-06, 2.6935918763907173e-06, 2.6935918763907173e-06, 3.847988394843882e-06, 3.847988394843882e-06, 5.497126278348404e-06, 5.497126278348404e-06, 7.85303754049772e-06, 7.85303754049772e-06, 1.1218625057853886e-05, 1.1218625057853886e-05, 1.6026607225505554e-05, 1.6026607225505554e-05, 2.2895153179293648e-05, 2.2895153179293648e-05, 3.270736168470521e-05, 3.270736168470521e-05, 4.672480240672174e-05, 4.672480240672174e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:50:41,180] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=27.914490155816292, CurrSamplesPerSec=28.544751918895408, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [2770/2809]  eta: 0:00:22  lr: 0.000047  min_lr: 0.000000  loss: 4.4705 (4.5728)  loss_scale: 65536.0000 (64341.6413)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.1548  max mem: 15572
Epoch: [4]  [2780/2809]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000000  loss: 4.4775 (4.5728)  loss_scale: 65536.0000 (64345.9360)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1083  max mem: 15572
Epoch: [4]  [2790/2809]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.5182 (4.5727)  loss_scale: 65536.0000 (64350.1999)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0883  max mem: 15572
Epoch: [4]  [2800/2809]  eta: 0:00:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5262 (4.5726)  loss_scale: 65536.0000 (64354.4334)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1012  max mem: 15572
Epoch: [4]  [2808/2809]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4897 (4.5723)  loss_scale: 65536.0000 (64357.7985)  weight_decay: 0.0500 (0.0500)  time: 0.4713  data: 0.0583  max mem: 15572
Epoch: [4] Total time: 0:27:34 (0.5889 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000000  loss: 4.4897 (4.5723)  loss_scale: 65536.0000 (64357.7985)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:28:18  loss: 1.1023 (1.1023)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.2447  data: 6.0399  max mem: 15572
Val:  [ 10/272]  eta: 0:03:31  loss: 4.3635 (3.9955)  acc1: 0.0000 (20.2020)  acc5: 5.5556 (27.7778)  time: 0.8079  data: 0.6141  max mem: 15572
Val:  [ 20/272]  eta: 0:02:09  loss: 4.2286 (4.0507)  acc1: 0.0000 (15.0794)  acc5: 22.2222 (28.5714)  time: 0.2278  data: 0.0406  max mem: 15572
Val:  [ 30/272]  eta: 0:01:49  loss: 4.2042 (4.1102)  acc1: 0.0000 (10.3943)  acc5: 27.7778 (30.1075)  time: 0.2559  data: 0.0668  max mem: 15572
Val:  [ 40/272]  eta: 0:01:36  loss: 3.9857 (4.0088)  acc1: 0.0000 (12.1951)  acc5: 38.8889 (34.8238)  time: 0.3091  data: 0.1261  max mem: 15572
Val:  [ 50/272]  eta: 0:01:30  loss: 3.6531 (3.9780)  acc1: 11.1111 (12.9630)  acc5: 55.5556 (37.0370)  time: 0.3338  data: 0.1485  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 3.0781 (3.8368)  acc1: 11.1111 (17.3042)  acc5: 72.2222 (41.9854)  time: 0.3249  data: 0.1210  max mem: 15572
Val:  [ 70/272]  eta: 0:01:19  loss: 3.0456 (3.7659)  acc1: 27.7778 (18.5446)  acc5: 66.6667 (43.5055)  time: 0.3645  data: 0.1723  max mem: 15572
Val:  [ 80/272]  eta: 0:01:13  loss: 3.6258 (3.7829)  acc1: 11.1111 (18.1756)  acc5: 50.0000 (42.6612)  time: 0.3856  data: 0.2109  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 4.3622 (3.8520)  acc1: 0.0000 (16.2393)  acc5: 5.5556 (39.0720)  time: 0.3261  data: 0.1386  max mem: 15572
Val:  [100/272]  eta: 0:01:06  loss: 4.3622 (3.9093)  acc1: 0.0000 (15.7316)  acc5: 5.5556 (37.1837)  time: 0.3882  data: 0.1805  max mem: 15572
Val:  [110/272]  eta: 0:01:02  loss: 4.4086 (3.9607)  acc1: 0.0000 (14.4144)  acc5: 11.1111 (35.4855)  time: 0.4145  data: 0.2013  max mem: 15572
Val:  [120/272]  eta: 0:00:56  loss: 4.3994 (3.9972)  acc1: 0.0000 (13.4068)  acc5: 11.1111 (34.4353)  time: 0.2892  data: 0.0846  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 4.3121 (3.9251)  acc1: 0.0000 (14.9279)  acc5: 27.7778 (36.4292)  time: 0.2492  data: 0.0606  max mem: 15572
Val:  [140/272]  eta: 0:00:48  loss: 3.8288 (3.9133)  acc1: 11.1111 (15.7210)  acc5: 44.4444 (36.6036)  time: 0.3425  data: 0.1659  max mem: 15572
Val:  [150/272]  eta: 0:00:44  loss: 4.0206 (3.9252)  acc1: 0.0000 (14.7535)  acc5: 27.7778 (36.0927)  time: 0.3587  data: 0.1780  max mem: 15572
Val:  [160/272]  eta: 0:00:40  loss: 3.8945 (3.9135)  acc1: 5.5556 (15.6660)  acc5: 38.8889 (37.6121)  time: 0.3470  data: 0.1582  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 4.0425 (3.9478)  acc1: 0.0000 (14.9123)  acc5: 33.3333 (36.4198)  time: 0.3481  data: 0.1563  max mem: 15572
Val:  [180/272]  eta: 0:00:33  loss: 4.2241 (3.9513)  acc1: 0.0000 (15.1934)  acc5: 11.1111 (36.2492)  time: 0.3313  data: 0.1402  max mem: 15572
Val:  [190/272]  eta: 0:00:29  loss: 4.2875 (3.9763)  acc1: 0.0000 (14.3979)  acc5: 11.1111 (35.3403)  time: 0.3223  data: 0.1389  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 4.2335 (3.9833)  acc1: 0.0000 (14.4002)  acc5: 27.7778 (35.9867)  time: 0.3338  data: 0.1535  max mem: 15572
Val:  [210/272]  eta: 0:00:22  loss: 3.8848 (3.9946)  acc1: 0.0000 (14.5076)  acc5: 44.4444 (35.9136)  time: 0.3353  data: 0.1442  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 4.1169 (4.0023)  acc1: 0.0000 (14.2534)  acc5: 27.7778 (35.7466)  time: 0.3581  data: 0.1511  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.9526 (3.9810)  acc1: 11.1111 (15.7528)  acc5: 50.0000 (37.2054)  time: 0.3708  data: 0.1593  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 3.5805 (3.9689)  acc1: 11.1111 (15.7215)  acc5: 77.7778 (38.4740)  time: 0.3242  data: 0.1069  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.9295 (3.9977)  acc1: 0.0000 (15.2722)  acc5: 33.3333 (37.5609)  time: 0.2980  data: 0.0971  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.8130 (3.9340)  acc1: 27.7778 (17.4117)  acc5: 66.6667 (39.5275)  time: 0.3018  data: 0.1304  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.3755 (3.9323)  acc1: 33.3333 (17.3432)  acc5: 77.7778 (39.7294)  time: 0.2309  data: 0.0722  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.3755 (3.9348)  acc1: 33.3333 (17.3254)  acc5: 77.7778 (39.7092)  time: 0.1934  data: 0.0392  max mem: 15572
Val: Total time: 0:01:33 (0.3434 s / it)
* Acc@1 17.325 Acc@5 39.709 loss 3.935
Accuracy of the network on the 4883 val videos: 17.3%
[2025-01-12 22:52:38,174] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-12 22:52:38,177] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 22:52:38,177] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 22:52:41,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 22:52:41,024] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 17.33%
Epoch: [5]  [   0/2809]  eta: 6:40:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6067 (4.6067)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.5513  data: 8.0385  max mem: 15572
[2025-01-12 22:52:52,258] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:52:52,259] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [  10/2809]  eta: 0:55:24  lr: 0.000047  min_lr: 0.000000  loss: 4.4319 (4.5251)  loss_scale: 65536.0000 (95325.0909)  weight_decay: 0.0500 (0.0500)  time: 1.1878  data: 0.7313  max mem: 15572
[2025-01-12 22:52:57,145] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14058
[2025-01-12 22:52:57,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:52:57,145] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [  20/2809]  eta: 0:42:39  lr: 0.000047  min_lr: 0.000000  loss: 4.4077 (4.4868)  loss_scale: 65536.0000 (87381.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0927  max mem: 15572
Epoch: [5]  [  30/2809]  eta: 0:37:08  lr: 0.000047  min_lr: 0.000000  loss: 4.4374 (4.4884)  loss_scale: 65536.0000 (80334.4516)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1490  max mem: 15572
Epoch: [5]  [  40/2809]  eta: 0:34:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5582 (4.5051)  loss_scale: 65536.0000 (76725.0732)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1381  max mem: 15572
Epoch: [5]  [  50/2809]  eta: 0:33:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4894 (4.4817)  loss_scale: 65536.0000 (74531.1373)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1501  max mem: 15572
Epoch: [5]  [  60/2809]  eta: 0:31:56  lr: 0.000047  min_lr: 0.000000  loss: 4.3225 (4.4749)  loss_scale: 65536.0000 (73056.5246)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1207  max mem: 15572
Epoch: [5]  [  70/2809]  eta: 0:30:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4216 (4.4884)  loss_scale: 65536.0000 (71997.2958)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1041  max mem: 15572
Epoch: [5]  [  80/2809]  eta: 0:30:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5790 (4.4962)  loss_scale: 65536.0000 (71199.6049)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1409  max mem: 15572
Epoch: [5]  [  90/2809]  eta: 0:30:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4209 (4.4909)  loss_scale: 65536.0000 (70577.2308)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1491  max mem: 15572
Epoch: [5]  [ 100/2809]  eta: 0:29:30  lr: 0.000047  min_lr: 0.000000  loss: 4.6042 (4.5139)  loss_scale: 65536.0000 (70078.0990)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1352  max mem: 15572
Epoch: [5]  [ 110/2809]  eta: 0:28:55  lr: 0.000047  min_lr: 0.000000  loss: 4.6078 (4.5142)  loss_scale: 65536.0000 (69668.9009)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1231  max mem: 15572
Epoch: [5]  [ 120/2809]  eta: 0:29:09  lr: 0.000047  min_lr: 0.000000  loss: 4.4839 (4.5080)  loss_scale: 65536.0000 (69327.3388)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.1969  max mem: 15572
Epoch: [5]  [ 130/2809]  eta: 0:29:01  lr: 0.000047  min_lr: 0.000000  loss: 4.3842 (4.5034)  loss_scale: 65536.0000 (69037.9237)  weight_decay: 0.0500 (0.0500)  time: 0.6896  data: 0.2283  max mem: 15572
Epoch: [5]  [ 140/2809]  eta: 0:28:46  lr: 0.000047  min_lr: 0.000000  loss: 4.5383 (4.5103)  loss_scale: 65536.0000 (68789.5603)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1680  max mem: 15572
[2025-01-12 22:54:13,098] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:54:13,098] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:54:13,955] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14189
[2025-01-12 22:54:13,956] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:54:13,957] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 150/2809]  eta: 0:28:41  lr: 0.000047  min_lr: 0.000000  loss: 4.5826 (4.5171)  loss_scale: 65536.0000 (69442.1192)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1899  max mem: 15572
Epoch: [5]  [ 160/2809]  eta: 0:28:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5213 (4.5074)  loss_scale: 65536.0000 (69199.5031)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1330  max mem: 15572
Epoch: [5]  [ 170/2809]  eta: 0:28:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4524 (4.5079)  loss_scale: 65536.0000 (68985.2632)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1021  max mem: 15572
Epoch: [5]  [ 180/2809]  eta: 0:27:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4524 (4.4996)  loss_scale: 65536.0000 (68794.6961)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1472  max mem: 15572
Epoch: [5]  [ 190/2809]  eta: 0:27:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4869 (4.5020)  loss_scale: 65536.0000 (68624.0838)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1561  max mem: 15572
Epoch: [5]  [ 200/2809]  eta: 0:27:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5563 (4.5095)  loss_scale: 65536.0000 (68470.4478)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.1153  max mem: 15572
Epoch: [5]  [ 210/2809]  eta: 0:26:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5260 (4.5000)  loss_scale: 65536.0000 (68331.3744)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.1018  max mem: 15572
Epoch: [5]  [ 220/2809]  eta: 0:26:47  lr: 0.000047  min_lr: 0.000000  loss: 4.3007 (4.5002)  loss_scale: 65536.0000 (68204.8869)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1417  max mem: 15572
Epoch: [5]  [ 230/2809]  eta: 0:26:36  lr: 0.000047  min_lr: 0.000000  loss: 4.4995 (4.5064)  loss_scale: 65536.0000 (68089.3506)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1323  max mem: 15572
Epoch: [5]  [ 240/2809]  eta: 0:26:26  lr: 0.000047  min_lr: 0.000000  loss: 4.5105 (4.5094)  loss_scale: 65536.0000 (67983.4025)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1200  max mem: 15572
Epoch: [5]  [ 250/2809]  eta: 0:26:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5105 (4.5116)  loss_scale: 65536.0000 (67885.8964)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1385  max mem: 15572
Epoch: [5]  [ 260/2809]  eta: 0:26:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5079 (4.5085)  loss_scale: 65536.0000 (67795.8621)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1345  max mem: 15572
Epoch: [5]  [ 270/2809]  eta: 0:25:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5079 (4.5093)  loss_scale: 65536.0000 (67712.4723)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.1123  max mem: 15572
[2025-01-12 22:55:29,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:55:29,901] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:55:31,834] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14322
[2025-01-12 22:55:31,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:55:31,834] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 280/2809]  eta: 0:25:50  lr: 0.000047  min_lr: 0.000000  loss: 4.5404 (4.5095)  loss_scale: 65536.0000 (68567.9146)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1522  max mem: 15572
Epoch: [5]  [ 290/2809]  eta: 0:25:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4823 (4.5098)  loss_scale: 65536.0000 (68463.7251)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1671  max mem: 15572
Epoch: [5]  [ 300/2809]  eta: 0:25:38  lr: 0.000047  min_lr: 0.000000  loss: 4.5038 (4.5095)  loss_scale: 65536.0000 (68366.4585)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1762  max mem: 15572
Epoch: [5]  [ 310/2809]  eta: 0:25:35  lr: 0.000047  min_lr: 0.000000  loss: 4.5448 (4.5093)  loss_scale: 65536.0000 (68275.4469)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.1982  max mem: 15572
Epoch: [5]  [ 320/2809]  eta: 0:25:30  lr: 0.000047  min_lr: 0.000000  loss: 4.5187 (4.5091)  loss_scale: 65536.0000 (68190.1059)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1690  max mem: 15572
Epoch: [5]  [ 330/2809]  eta: 0:25:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6049 (4.5121)  loss_scale: 65536.0000 (68109.9215)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1331  max mem: 15572
Epoch: [5]  [ 340/2809]  eta: 0:25:09  lr: 0.000047  min_lr: 0.000000  loss: 4.6013 (4.5134)  loss_scale: 65536.0000 (68034.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0859  max mem: 15572
Epoch: [5]  [ 350/2809]  eta: 0:24:55  lr: 0.000047  min_lr: 0.000000  loss: 4.5065 (4.5129)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.0612  max mem: 15572
Epoch: [5]  [ 360/2809]  eta: 0:24:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4993 (4.5134)  loss_scale: 65536.0000 (67896.0222)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0805  max mem: 15572
Epoch: [5]  [ 370/2809]  eta: 0:24:29  lr: 0.000047  min_lr: 0.000000  loss: 4.5022 (4.5104)  loss_scale: 65536.0000 (67832.4097)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0516  max mem: 15572
Epoch: [5]  [ 380/2809]  eta: 0:24:24  lr: 0.000047  min_lr: 0.000000  loss: 4.4703 (4.5114)  loss_scale: 65536.0000 (67772.1365)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0610  max mem: 15572
Epoch: [5]  [ 390/2809]  eta: 0:24:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5945 (4.5147)  loss_scale: 65536.0000 (67714.9463)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1366  max mem: 15572
Epoch: [5]  [ 400/2809]  eta: 0:24:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5742 (4.5117)  loss_scale: 65536.0000 (67660.6085)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1163  max mem: 15572
[2025-01-12 22:56:47,316] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:56:47,317] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:56:47,836] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14452
[2025-01-12 22:56:47,837] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:56:47,837] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 410/2809]  eta: 0:24:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5106 (4.5141)  loss_scale: 65536.0000 (67768.3698)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1146  max mem: 15572
Epoch: [5]  [ 420/2809]  eta: 0:23:59  lr: 0.000047  min_lr: 0.000000  loss: 4.6595 (4.5164)  loss_scale: 65536.0000 (67715.3444)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1273  max mem: 15572
Epoch: [5]  [ 430/2809]  eta: 0:23:53  lr: 0.000047  min_lr: 0.000000  loss: 4.6119 (4.5193)  loss_scale: 65536.0000 (67664.7796)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1254  max mem: 15572
Epoch: [5]  [ 440/2809]  eta: 0:23:47  lr: 0.000047  min_lr: 0.000000  loss: 4.4939 (4.5171)  loss_scale: 65536.0000 (67616.5079)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1501  max mem: 15572
Epoch: [5]  [ 450/2809]  eta: 0:23:36  lr: 0.000047  min_lr: 0.000000  loss: 4.4445 (4.5166)  loss_scale: 65536.0000 (67570.3769)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0934  max mem: 15572
Epoch: [5]  [ 460/2809]  eta: 0:23:26  lr: 0.000047  min_lr: 0.000000  loss: 4.4786 (4.5161)  loss_scale: 65536.0000 (67526.2473)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0386  max mem: 15572
Epoch: [5]  [ 470/2809]  eta: 0:23:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5430 (4.5151)  loss_scale: 65536.0000 (67483.9915)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1136  max mem: 15572
Epoch: [5]  [ 480/2809]  eta: 0:23:13  lr: 0.000047  min_lr: 0.000000  loss: 4.4896 (4.5142)  loss_scale: 65536.0000 (67443.4927)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1474  max mem: 15572
Epoch: [5]  [ 490/2809]  eta: 0:23:06  lr: 0.000047  min_lr: 0.000000  loss: 4.4774 (4.5113)  loss_scale: 65536.0000 (67404.6436)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1203  max mem: 15572
Epoch: [5]  [ 500/2809]  eta: 0:23:01  lr: 0.000047  min_lr: 0.000000  loss: 4.4621 (4.5095)  loss_scale: 65536.0000 (67367.3453)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1337  max mem: 15572
Epoch: [5]  [ 510/2809]  eta: 0:22:56  lr: 0.000047  min_lr: 0.000000  loss: 4.4863 (4.5106)  loss_scale: 65536.0000 (67331.5068)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1610  max mem: 15572
Epoch: [5]  [ 520/2809]  eta: 0:22:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3638 (4.5078)  loss_scale: 65536.0000 (67297.0441)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1554  max mem: 15572
Epoch: [5]  [ 530/2809]  eta: 0:22:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5203 (4.5092)  loss_scale: 65536.0000 (67263.8795)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1791  max mem: 15572
[2025-01-12 22:58:03,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:58:03,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 540/2809]  eta: 0:22:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5203 (4.5069)  loss_scale: 65536.0000 (67837.6340)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1697  max mem: 15572
[2025-01-12 22:58:09,277] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14592
[2025-01-12 22:58:09,277] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:58:09,278] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 550/2809]  eta: 0:22:30  lr: 0.000047  min_lr: 0.000000  loss: 4.6498 (4.5119)  loss_scale: 131072.0000 (68509.5027)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1152  max mem: 15572
Epoch: [5]  [ 560/2809]  eta: 0:22:23  lr: 0.000047  min_lr: 0.000000  loss: 4.7782 (4.5160)  loss_scale: 65536.0000 (68456.4991)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1231  max mem: 15572
Epoch: [5]  [ 570/2809]  eta: 0:22:15  lr: 0.000047  min_lr: 0.000000  loss: 4.6349 (4.5164)  loss_scale: 65536.0000 (68405.3520)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1017  max mem: 15572
Epoch: [5]  [ 580/2809]  eta: 0:22:04  lr: 0.000047  min_lr: 0.000000  loss: 4.5218 (4.5161)  loss_scale: 65536.0000 (68355.9656)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0508  max mem: 15572
Epoch: [5]  [ 590/2809]  eta: 0:22:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4893 (4.5161)  loss_scale: 65536.0000 (68308.2504)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1119  max mem: 15572
Epoch: [5]  [ 600/2809]  eta: 0:21:52  lr: 0.000047  min_lr: 0.000000  loss: 4.4726 (4.5149)  loss_scale: 65536.0000 (68262.1231)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1397  max mem: 15572
Epoch: [5]  [ 610/2809]  eta: 0:21:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4109 (4.5128)  loss_scale: 65536.0000 (68217.5057)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0679  max mem: 15572
Epoch: [5]  [ 620/2809]  eta: 0:21:39  lr: 0.000047  min_lr: 0.000000  loss: 4.4487 (4.5134)  loss_scale: 65536.0000 (68174.3253)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1334  max mem: 15572
Epoch: [5]  [ 630/2809]  eta: 0:21:31  lr: 0.000047  min_lr: 0.000000  loss: 4.4140 (4.5100)  loss_scale: 65536.0000 (68132.5135)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1492  max mem: 15572
Epoch: [5]  [ 640/2809]  eta: 0:21:25  lr: 0.000047  min_lr: 0.000000  loss: 4.3131 (4.5080)  loss_scale: 65536.0000 (68092.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1232  max mem: 15572
Epoch: [5]  [ 650/2809]  eta: 0:21:19  lr: 0.000047  min_lr: 0.000000  loss: 4.3131 (4.5066)  loss_scale: 65536.0000 (68052.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1524  max mem: 15572
Epoch: [5]  [ 660/2809]  eta: 0:21:12  lr: 0.000047  min_lr: 0.000000  loss: 4.4640 (4.5060)  loss_scale: 65536.0000 (68014.6687)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1459  max mem: 15572
Epoch: [5]  [ 670/2809]  eta: 0:21:07  lr: 0.000047  min_lr: 0.000000  loss: 4.4640 (4.5056)  loss_scale: 65536.0000 (67977.7288)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1268  max mem: 15572
[2025-01-12 22:59:22,778] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:59:22,779] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:59:23,214] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14722
[2025-01-12 22:59:23,215] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:59:23,215] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 680/2809]  eta: 0:21:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4456 (4.5039)  loss_scale: 65536.0000 (68038.1087)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1012  max mem: 15572
Epoch: [5]  [ 690/2809]  eta: 0:20:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4666 (4.5037)  loss_scale: 65536.0000 (68001.8987)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1460  max mem: 15572
Epoch: [5]  [ 700/2809]  eta: 0:20:51  lr: 0.000047  min_lr: 0.000000  loss: 4.4666 (4.5035)  loss_scale: 65536.0000 (67966.7218)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.1722  max mem: 15572
Epoch: [5]  [ 710/2809]  eta: 0:20:48  lr: 0.000047  min_lr: 0.000000  loss: 4.4874 (4.5036)  loss_scale: 65536.0000 (67932.5345)  weight_decay: 0.0500 (0.0500)  time: 0.6352  data: 0.1924  max mem: 15572
Epoch: [5]  [ 720/2809]  eta: 0:20:39  lr: 0.000047  min_lr: 0.000000  loss: 4.4762 (4.5030)  loss_scale: 65536.0000 (67899.2954)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1328  max mem: 15572
Epoch: [5]  [ 730/2809]  eta: 0:20:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4890 (4.5031)  loss_scale: 65536.0000 (67866.9658)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0475  max mem: 15572
Epoch: [5]  [ 740/2809]  eta: 0:20:25  lr: 0.000047  min_lr: 0.000000  loss: 4.3887 (4.5011)  loss_scale: 65536.0000 (67835.5088)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1197  max mem: 15572
Epoch: [5]  [ 750/2809]  eta: 0:20:16  lr: 0.000047  min_lr: 0.000000  loss: 4.3887 (4.5014)  loss_scale: 65536.0000 (67804.8895)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1123  max mem: 15572
Epoch: [5]  [ 760/2809]  eta: 0:20:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5205 (4.5023)  loss_scale: 65536.0000 (67775.0749)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0956  max mem: 15572
Epoch: [5]  [ 770/2809]  eta: 0:20:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5109 (4.5008)  loss_scale: 65536.0000 (67746.0337)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1136  max mem: 15572
Epoch: [5]  [ 780/2809]  eta: 0:20:00  lr: 0.000047  min_lr: 0.000000  loss: 4.2708 (4.4982)  loss_scale: 65536.0000 (67717.7362)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0949  max mem: 15572
Epoch: [5]  [ 790/2809]  eta: 0:19:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5387 (4.4994)  loss_scale: 65536.0000 (67690.1542)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0850  max mem: 15572
Epoch: [5]  [ 800/2809]  eta: 0:19:48  lr: 0.000047  min_lr: 0.000000  loss: 4.5388 (4.4997)  loss_scale: 65536.0000 (67663.2609)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1138  max mem: 15572
[2025-01-12 23:00:37,995] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:00:37,996] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 810/2809]  eta: 0:19:42  lr: 0.000047  min_lr: 0.000000  loss: 4.4734 (4.4985)  loss_scale: 65536.0000 (68041.0752)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.1605  max mem: 15572
Epoch: [5]  [ 820/2809]  eta: 0:19:34  lr: 0.000047  min_lr: 0.000000  loss: 4.5184 (4.5000)  loss_scale: 131072.0000 (68808.8088)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0865  max mem: 15572
[2025-01-12 23:00:51,859] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14874
[2025-01-12 23:00:51,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:00:51,860] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 830/2809]  eta: 0:19:29  lr: 0.000047  min_lr: 0.000000  loss: 4.4593 (4.5002)  loss_scale: 131072.0000 (69400.3369)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0910  max mem: 15572
Epoch: [5]  [ 840/2809]  eta: 0:19:25  lr: 0.000047  min_lr: 0.000000  loss: 4.4454 (4.5000)  loss_scale: 65536.0000 (69354.3876)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.2065  max mem: 15572
Epoch: [5]  [ 850/2809]  eta: 0:19:18  lr: 0.000047  min_lr: 0.000000  loss: 4.4303 (4.4995)  loss_scale: 65536.0000 (69309.5182)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1714  max mem: 15572
Epoch: [5]  [ 860/2809]  eta: 0:19:14  lr: 0.000047  min_lr: 0.000000  loss: 4.4785 (4.4993)  loss_scale: 65536.0000 (69265.6911)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1276  max mem: 15572
Epoch: [5]  [ 870/2809]  eta: 0:19:07  lr: 0.000047  min_lr: 0.000000  loss: 4.6340 (4.5012)  loss_scale: 65536.0000 (69222.8703)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1078  max mem: 15572
Epoch: [5]  [ 880/2809]  eta: 0:19:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5151 (4.4993)  loss_scale: 65536.0000 (69181.0216)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1104  max mem: 15572
Epoch: [5]  [ 890/2809]  eta: 0:18:57  lr: 0.000047  min_lr: 0.000000  loss: 4.3829 (4.4987)  loss_scale: 65536.0000 (69140.1122)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.1477  max mem: 15572
Epoch: [5]  [ 900/2809]  eta: 0:18:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4949 (4.4985)  loss_scale: 65536.0000 (69100.1110)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1147  max mem: 15572
Epoch: [5]  [ 910/2809]  eta: 0:18:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4937 (4.4987)  loss_scale: 65536.0000 (69060.9879)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0880  max mem: 15572
Epoch: [5]  [ 920/2809]  eta: 0:18:38  lr: 0.000047  min_lr: 0.000000  loss: 4.4937 (4.4992)  loss_scale: 65536.0000 (69022.7144)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1150  max mem: 15572
Epoch: [5]  [ 930/2809]  eta: 0:18:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4557 (4.4980)  loss_scale: 65536.0000 (68985.2632)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1046  max mem: 15572
Epoch: [5]  [ 940/2809]  eta: 0:18:23  lr: 0.000047  min_lr: 0.000000  loss: 4.4557 (4.4983)  loss_scale: 65536.0000 (68948.6079)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0849  max mem: 15572
Epoch: [5]  [ 950/2809]  eta: 0:18:17  lr: 0.000047  min_lr: 0.000000  loss: 4.5190 (4.4991)  loss_scale: 65536.0000 (68912.7234)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1270  max mem: 15572
[2025-01-12 23:02:06,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=91, lr=[4.540618353164525e-07, 4.540618353164525e-07, 6.486597647377894e-07, 6.486597647377894e-07, 9.266568067682706e-07, 9.266568067682706e-07, 1.3237954382403866e-06, 1.3237954382403866e-06, 1.8911363403434095e-06, 1.8911363403434095e-06, 2.701623343347728e-06, 2.701623343347728e-06, 3.859461919068183e-06, 3.859461919068183e-06, 5.5135170272402624e-06, 5.5135170272402624e-06, 7.876452896057517e-06, 7.876452896057517e-06, 1.1252075565796455e-05, 1.1252075565796455e-05, 1.6074393665423507e-05, 1.6074393665423507e-05, 2.2963419522033585e-05, 2.2963419522033585e-05, 3.280488503147655e-05, 3.280488503147655e-05, 4.686412147353793e-05, 4.686412147353793e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:02:06,183] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=27.908609131440382, CurrSamplesPerSec=28.23174966611566, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-12 23:02:08,140] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:02:08,140] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 960/2809]  eta: 0:18:11  lr: 0.000047  min_lr: 0.000000  loss: 4.5097 (4.4991)  loss_scale: 65536.0000 (69082.1727)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1753  max mem: 15572
Epoch: [5]  [ 970/2809]  eta: 0:18:06  lr: 0.000047  min_lr: 0.000000  loss: 4.3868 (4.4977)  loss_scale: 131072.0000 (69720.5850)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1855  max mem: 15572
Epoch: [5]  [ 980/2809]  eta: 0:18:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4485 (4.4986)  loss_scale: 131072.0000 (70345.9817)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1473  max mem: 15572
[2025-01-12 23:02:24,424] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15029
[2025-01-12 23:02:24,425] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:02:24,425] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 990/2809]  eta: 0:17:55  lr: 0.000047  min_lr: 0.000000  loss: 4.5073 (4.4981)  loss_scale: 131072.0000 (70495.8385)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1441  max mem: 15572
Epoch: [5]  [1000/2809]  eta: 0:17:49  lr: 0.000047  min_lr: 0.000000  loss: 4.4247 (4.4975)  loss_scale: 65536.0000 (70446.2897)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1560  max mem: 15572
Epoch: [5]  [1010/2809]  eta: 0:17:43  lr: 0.000047  min_lr: 0.000000  loss: 4.3503 (4.4970)  loss_scale: 65536.0000 (70397.7211)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1304  max mem: 15572
Epoch: [5]  [1020/2809]  eta: 0:17:37  lr: 0.000047  min_lr: 0.000000  loss: 4.3607 (4.4955)  loss_scale: 65536.0000 (70350.1038)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1314  max mem: 15572
Epoch: [5]  [1030/2809]  eta: 0:17:31  lr: 0.000047  min_lr: 0.000000  loss: 4.3607 (4.4953)  loss_scale: 65536.0000 (70303.4103)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1259  max mem: 15572
[2025-01-12 23:02:54,030] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15080
[2025-01-12 23:02:54,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 23:02:54,031] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [1040/2809]  eta: 0:17:24  lr: 0.000047  min_lr: 0.000000  loss: 4.3408 (4.4941)  loss_scale: 65536.0000 (70068.7493)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0982  max mem: 15572
Epoch: [5]  [1050/2809]  eta: 0:17:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5631 (4.4948)  loss_scale: 32768.0000 (69713.8421)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1370  max mem: 15572
Epoch: [5]  [1060/2809]  eta: 0:17:12  lr: 0.000047  min_lr: 0.000000  loss: 4.4835 (4.4935)  loss_scale: 32768.0000 (69365.6249)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1434  max mem: 15572
Epoch: [5]  [1070/2809]  eta: 0:17:07  lr: 0.000047  min_lr: 0.000000  loss: 4.3636 (4.4928)  loss_scale: 32768.0000 (69023.9104)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1572  max mem: 15572
Epoch: [5]  [1080/2809]  eta: 0:17:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5418 (4.4936)  loss_scale: 32768.0000 (68688.5180)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.1937  max mem: 15572
Epoch: [5]  [1090/2809]  eta: 0:16:57  lr: 0.000047  min_lr: 0.000000  loss: 4.5816 (4.4942)  loss_scale: 32768.0000 (68359.2741)  weight_decay: 0.0500 (0.0500)  time: 0.6584  data: 0.1955  max mem: 15572
Epoch: [5]  [1100/2809]  eta: 0:16:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5423 (4.4940)  loss_scale: 32768.0000 (68036.0109)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.1769  max mem: 15572
Epoch: [5]  [1110/2809]  eta: 0:16:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4826 (4.4946)  loss_scale: 32768.0000 (67718.5671)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1489  max mem: 15572
Epoch: [5]  [1120/2809]  eta: 0:16:41  lr: 0.000047  min_lr: 0.000000  loss: 4.5440 (4.4947)  loss_scale: 32768.0000 (67406.7868)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1809  max mem: 15572
Epoch: [5]  [1130/2809]  eta: 0:16:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5160 (4.4934)  loss_scale: 32768.0000 (67100.5199)  weight_decay: 0.0500 (0.0500)  time: 0.6568  data: 0.2180  max mem: 15572
Epoch: [5]  [1140/2809]  eta: 0:16:29  lr: 0.000047  min_lr: 0.000000  loss: 4.4807 (4.4944)  loss_scale: 32768.0000 (66799.6214)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1561  max mem: 15572
Epoch: [5]  [1150/2809]  eta: 0:16:25  lr: 0.000047  min_lr: 0.000000  loss: 4.5113 (4.4932)  loss_scale: 32768.0000 (66503.9513)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1572  max mem: 15572
Epoch: [5]  [1160/2809]  eta: 0:16:18  lr: 0.000047  min_lr: 0.000000  loss: 4.4849 (4.4926)  loss_scale: 32768.0000 (66213.3747)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1458  max mem: 15572
[2025-01-12 23:04:13,299] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:04:13,299] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1170/2809]  eta: 0:16:12  lr: 0.000047  min_lr: 0.000000  loss: 4.4648 (4.4924)  loss_scale: 32768.0000 (66123.6413)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0939  max mem: 15572
Epoch: [5]  [1180/2809]  eta: 0:16:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4727 (4.4920)  loss_scale: 65536.0000 (66118.6655)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0818  max mem: 15572
Epoch: [5]  [1190/2809]  eta: 0:16:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4002 (4.4908)  loss_scale: 65536.0000 (66113.7733)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1175  max mem: 15572
Epoch: [5]  [1200/2809]  eta: 0:15:52  lr: 0.000047  min_lr: 0.000000  loss: 4.4155 (4.4915)  loss_scale: 65536.0000 (66108.9625)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1045  max mem: 15572
Epoch: [5]  [1210/2809]  eta: 0:15:45  lr: 0.000047  min_lr: 0.000000  loss: 4.6228 (4.4912)  loss_scale: 65536.0000 (66104.2312)  weight_decay: 0.0500 (0.0500)  time: 0.4939  data: 0.0083  max mem: 15572
Epoch: [5]  [1220/2809]  eta: 0:15:39  lr: 0.000047  min_lr: 0.000000  loss: 4.5966 (4.4912)  loss_scale: 65536.0000 (66099.5774)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0352  max mem: 15572
Epoch: [5]  [1230/2809]  eta: 0:15:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5454 (4.4911)  loss_scale: 65536.0000 (66094.9992)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0572  max mem: 15572
Epoch: [5]  [1240/2809]  eta: 0:15:26  lr: 0.000047  min_lr: 0.000000  loss: 4.5454 (4.4901)  loss_scale: 65536.0000 (66090.4948)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0644  max mem: 15572
Epoch: [5]  [1250/2809]  eta: 0:15:20  lr: 0.000047  min_lr: 0.000000  loss: 4.5402 (4.4910)  loss_scale: 65536.0000 (66086.0624)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0947  max mem: 15572
Epoch: [5]  [1260/2809]  eta: 0:15:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5124 (4.4909)  loss_scale: 65536.0000 (66081.7002)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0751  max mem: 15572
Epoch: [5]  [1270/2809]  eta: 0:15:07  lr: 0.000047  min_lr: 0.000000  loss: 4.4938 (4.4911)  loss_scale: 65536.0000 (66077.4068)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0894  max mem: 15572
Epoch: [5]  [1280/2809]  eta: 0:15:02  lr: 0.000047  min_lr: 0.000000  loss: 4.4051 (4.4891)  loss_scale: 65536.0000 (66073.1803)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1464  max mem: 15572
Epoch: [5]  [1290/2809]  eta: 0:14:56  lr: 0.000047  min_lr: 0.000000  loss: 4.3471 (4.4886)  loss_scale: 65536.0000 (66069.0194)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1606  max mem: 15572
[2025-01-12 23:05:25,942] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:05:25,943] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:05:29,063] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15344
[2025-01-12 23:05:29,063] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:05:29,064] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1300/2809]  eta: 0:14:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4134 (4.4886)  loss_scale: 65536.0000 (66417.5373)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1566  max mem: 15572
Epoch: [5]  [1310/2809]  eta: 0:14:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4469 (4.4885)  loss_scale: 65536.0000 (66410.8131)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0978  max mem: 15572
Epoch: [5]  [1320/2809]  eta: 0:14:37  lr: 0.000047  min_lr: 0.000000  loss: 4.4543 (4.4886)  loss_scale: 65536.0000 (66404.1908)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1060  max mem: 15572
Epoch: [5]  [1330/2809]  eta: 0:14:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5462 (4.4895)  loss_scale: 65536.0000 (66397.6679)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1423  max mem: 15572
Epoch: [5]  [1340/2809]  eta: 0:14:26  lr: 0.000047  min_lr: 0.000000  loss: 4.5543 (4.4888)  loss_scale: 65536.0000 (66391.2424)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1550  max mem: 15572
Epoch: [5]  [1350/2809]  eta: 0:14:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4134 (4.4880)  loss_scale: 65536.0000 (66384.9119)  weight_decay: 0.0500 (0.0500)  time: 0.6546  data: 0.1794  max mem: 15572
Epoch: [5]  [1360/2809]  eta: 0:14:15  lr: 0.000047  min_lr: 0.000000  loss: 4.3265 (4.4870)  loss_scale: 65536.0000 (66378.6745)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0954  max mem: 15572
Epoch: [5]  [1370/2809]  eta: 0:14:08  lr: 0.000047  min_lr: 0.000000  loss: 4.4483 (4.4874)  loss_scale: 65536.0000 (66372.5281)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0460  max mem: 15572
Epoch: [5]  [1380/2809]  eta: 0:14:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4067 (4.4863)  loss_scale: 65536.0000 (66366.4707)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1276  max mem: 15572
Epoch: [5]  [1390/2809]  eta: 0:13:57  lr: 0.000047  min_lr: 0.000000  loss: 4.3163 (4.4850)  loss_scale: 65536.0000 (66360.5004)  weight_decay: 0.0500 (0.0500)  time: 0.6564  data: 0.1751  max mem: 15572
Epoch: [5]  [1400/2809]  eta: 0:13:51  lr: 0.000047  min_lr: 0.000000  loss: 4.4221 (4.4854)  loss_scale: 65536.0000 (66354.6153)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1107  max mem: 15572
Epoch: [5]  [1410/2809]  eta: 0:13:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4212 (4.4849)  loss_scale: 65536.0000 (66348.8136)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1330  max mem: 15572
Epoch: [5]  [1420/2809]  eta: 0:13:39  lr: 0.000047  min_lr: 0.000000  loss: 4.3779 (4.4853)  loss_scale: 65536.0000 (66343.0936)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1612  max mem: 15572
[2025-01-12 23:06:44,568] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:06:44,568] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1430/2809]  eta: 0:13:32  lr: 0.000047  min_lr: 0.000000  loss: 4.4975 (4.4844)  loss_scale: 65536.0000 (66474.8456)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0785  max mem: 15572
[2025-01-12 23:06:50,555] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15482
[2025-01-12 23:06:50,555] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:06:50,555] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1440/2809]  eta: 0:13:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4249 (4.4836)  loss_scale: 65536.0000 (66741.2075)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1286  max mem: 15572
Epoch: [5]  [1450/2809]  eta: 0:13:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4965 (4.4848)  loss_scale: 65536.0000 (66732.9014)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.2040  max mem: 15572
Epoch: [5]  [1460/2809]  eta: 0:13:15  lr: 0.000047  min_lr: 0.000000  loss: 4.4480 (4.4840)  loss_scale: 65536.0000 (66724.7091)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1371  max mem: 15572
Epoch: [5]  [1470/2809]  eta: 0:13:09  lr: 0.000047  min_lr: 0.000000  loss: 4.4309 (4.4840)  loss_scale: 65536.0000 (66716.6281)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0892  max mem: 15572
Epoch: [5]  [1480/2809]  eta: 0:13:03  lr: 0.000047  min_lr: 0.000000  loss: 4.5115 (4.4842)  loss_scale: 65536.0000 (66708.6563)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1145  max mem: 15572
Epoch: [5]  [1490/2809]  eta: 0:12:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4955 (4.4841)  loss_scale: 65536.0000 (66700.7914)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1366  max mem: 15572
Epoch: [5]  [1500/2809]  eta: 0:12:52  lr: 0.000047  min_lr: 0.000000  loss: 4.4771 (4.4838)  loss_scale: 65536.0000 (66693.0313)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.1836  max mem: 15572
Epoch: [5]  [1510/2809]  eta: 0:12:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4441 (4.4843)  loss_scale: 65536.0000 (66685.3739)  weight_decay: 0.0500 (0.0500)  time: 0.6480  data: 0.1766  max mem: 15572
Epoch: [5]  [1520/2809]  eta: 0:12:41  lr: 0.000047  min_lr: 0.000000  loss: 4.4595 (4.4833)  loss_scale: 65536.0000 (66677.8172)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1128  max mem: 15572
Epoch: [5]  [1530/2809]  eta: 0:12:35  lr: 0.000047  min_lr: 0.000000  loss: 4.4394 (4.4831)  loss_scale: 65536.0000 (66670.3592)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1333  max mem: 15572
Epoch: [5]  [1540/2809]  eta: 0:12:29  lr: 0.000047  min_lr: 0.000000  loss: 4.3964 (4.4824)  loss_scale: 65536.0000 (66662.9981)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1709  max mem: 15572
Epoch: [5]  [1550/2809]  eta: 0:12:23  lr: 0.000047  min_lr: 0.000000  loss: 4.3311 (4.4810)  loss_scale: 65536.0000 (66655.7318)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1467  max mem: 15572
Epoch: [5]  [1560/2809]  eta: 0:12:18  lr: 0.000047  min_lr: 0.000000  loss: 4.4305 (4.4809)  loss_scale: 65536.0000 (66648.5586)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1304  max mem: 15572
[2025-01-12 23:08:09,149] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:08:09,149] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1570/2809]  eta: 0:12:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5341 (4.4809)  loss_scale: 65536.0000 (66850.0573)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1674  max mem: 15572
Epoch: [5]  [1580/2809]  eta: 0:12:06  lr: 0.000047  min_lr: 0.000000  loss: 4.4885 (4.4813)  loss_scale: 131072.0000 (67256.2682)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1560  max mem: 15572
[2025-01-12 23:08:22,564] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15635
[2025-01-12 23:08:22,564] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:08:22,565] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1590/2809]  eta: 0:12:00  lr: 0.000047  min_lr: 0.000000  loss: 4.3824 (4.4805)  loss_scale: 131072.0000 (67616.1810)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1375  max mem: 15572
Epoch: [5]  [1600/2809]  eta: 0:11:55  lr: 0.000047  min_lr: 0.000000  loss: 4.3257 (4.4795)  loss_scale: 65536.0000 (67603.1880)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1579  max mem: 15572
Epoch: [5]  [1610/2809]  eta: 0:11:49  lr: 0.000047  min_lr: 0.000000  loss: 4.4432 (4.4798)  loss_scale: 65536.0000 (67590.3563)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.1563  max mem: 15572
Epoch: [5]  [1620/2809]  eta: 0:11:43  lr: 0.000047  min_lr: 0.000000  loss: 4.3668 (4.4791)  loss_scale: 65536.0000 (67577.6829)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1244  max mem: 15572
Epoch: [5]  [1630/2809]  eta: 0:11:37  lr: 0.000047  min_lr: 0.000000  loss: 4.3715 (4.4794)  loss_scale: 65536.0000 (67565.1649)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0748  max mem: 15572
Epoch: [5]  [1640/2809]  eta: 0:11:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4187 (4.4777)  loss_scale: 65536.0000 (67552.7995)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0783  max mem: 15572
Epoch: [5]  [1650/2809]  eta: 0:11:25  lr: 0.000047  min_lr: 0.000000  loss: 4.3676 (4.4772)  loss_scale: 65536.0000 (67540.5839)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1298  max mem: 15572
Epoch: [5]  [1660/2809]  eta: 0:11:19  lr: 0.000047  min_lr: 0.000000  loss: 4.3722 (4.4775)  loss_scale: 65536.0000 (67528.5154)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1527  max mem: 15572
Epoch: [5]  [1670/2809]  eta: 0:11:13  lr: 0.000047  min_lr: 0.000000  loss: 4.3180 (4.4773)  loss_scale: 65536.0000 (67516.5913)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1499  max mem: 15572
Epoch: [5]  [1680/2809]  eta: 0:11:07  lr: 0.000047  min_lr: 0.000000  loss: 4.3550 (4.4759)  loss_scale: 65536.0000 (67504.8090)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1045  max mem: 15572
Epoch: [5]  [1690/2809]  eta: 0:11:01  lr: 0.000047  min_lr: 0.000000  loss: 4.3975 (4.4759)  loss_scale: 65536.0000 (67493.1662)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0847  max mem: 15572
Epoch: [5]  [1700/2809]  eta: 0:10:54  lr: 0.000047  min_lr: 0.000000  loss: 4.4516 (4.4757)  loss_scale: 65536.0000 (67481.6602)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0687  max mem: 15572
Epoch: [5]  [1710/2809]  eta: 0:10:48  lr: 0.000047  min_lr: 0.000000  loss: 4.5302 (4.4759)  loss_scale: 65536.0000 (67470.2887)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0877  max mem: 15572
[2025-01-12 23:09:36,341] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15764
[2025-01-12 23:09:36,341] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 23:09:36,341] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [1720/2809]  eta: 0:10:42  lr: 0.000047  min_lr: 0.000000  loss: 4.4054 (4.4750)  loss_scale: 65536.0000 (67420.9692)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.1165  max mem: 15572
Epoch: [5]  [1730/2809]  eta: 0:10:36  lr: 0.000047  min_lr: 0.000000  loss: 4.4403 (4.4750)  loss_scale: 32768.0000 (67220.7787)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1581  max mem: 15572
Epoch: [5]  [1740/2809]  eta: 0:10:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4506 (4.4742)  loss_scale: 32768.0000 (67022.8880)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1580  max mem: 15572
Epoch: [5]  [1750/2809]  eta: 0:10:25  lr: 0.000047  min_lr: 0.000000  loss: 4.4506 (4.4747)  loss_scale: 32768.0000 (66827.2576)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1509  max mem: 15572
Epoch: [5]  [1760/2809]  eta: 0:10:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5549 (4.4750)  loss_scale: 32768.0000 (66633.8489)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1882  max mem: 15572
Epoch: [5]  [1770/2809]  eta: 0:10:14  lr: 0.000047  min_lr: 0.000000  loss: 4.4858 (4.4753)  loss_scale: 32768.0000 (66442.6245)  weight_decay: 0.0500 (0.0500)  time: 0.6765  data: 0.2131  max mem: 15572
Epoch: [5]  [1780/2809]  eta: 0:10:08  lr: 0.000047  min_lr: 0.000000  loss: 4.4002 (4.4754)  loss_scale: 32768.0000 (66253.5474)  weight_decay: 0.0500 (0.0500)  time: 0.6465  data: 0.1918  max mem: 15572
Epoch: [5]  [1790/2809]  eta: 0:10:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5704 (4.4764)  loss_scale: 32768.0000 (66066.5818)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1243  max mem: 15572
Epoch: [5]  [1800/2809]  eta: 0:09:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6667 (4.4769)  loss_scale: 32768.0000 (65881.6924)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1581  max mem: 15572
Epoch: [5]  [1810/2809]  eta: 0:09:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4759 (4.4761)  loss_scale: 32768.0000 (65698.8448)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1272  max mem: 15572
Epoch: [5]  [1820/2809]  eta: 0:09:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3053 (4.4760)  loss_scale: 32768.0000 (65518.0055)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0959  max mem: 15572
Epoch: [5]  [1830/2809]  eta: 0:09:38  lr: 0.000047  min_lr: 0.000000  loss: 4.4512 (4.4755)  loss_scale: 32768.0000 (65339.1415)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1098  max mem: 15572
Epoch: [5]  [1840/2809]  eta: 0:09:33  lr: 0.000047  min_lr: 0.000000  loss: 4.4132 (4.4755)  loss_scale: 32768.0000 (65162.2205)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.1898  max mem: 15572
[2025-01-12 23:10:56,696] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:10:56,697] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1850/2809]  eta: 0:09:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4753 (4.4756)  loss_scale: 32768.0000 (65040.3198)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.2275  max mem: 15572
Epoch: [5]  [1860/2809]  eta: 0:09:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4880 (4.4760)  loss_scale: 65536.0000 (65042.9833)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1698  max mem: 15572
Epoch: [5]  [1870/2809]  eta: 0:09:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4284 (4.4754)  loss_scale: 65536.0000 (65045.6184)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.1803  max mem: 15572
Epoch: [5]  [1880/2809]  eta: 0:09:09  lr: 0.000047  min_lr: 0.000000  loss: 4.3146 (4.4747)  loss_scale: 65536.0000 (65048.2254)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1458  max mem: 15572
Epoch: [5]  [1890/2809]  eta: 0:09:04  lr: 0.000047  min_lr: 0.000000  loss: 4.3488 (4.4746)  loss_scale: 65536.0000 (65050.8049)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1170  max mem: 15572
Epoch: [5]  [1900/2809]  eta: 0:08:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4558 (4.4744)  loss_scale: 65536.0000 (65053.3572)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1110  max mem: 15572
Epoch: [5]  [1910/2809]  eta: 0:08:52  lr: 0.000047  min_lr: 0.000000  loss: 4.4111 (4.4740)  loss_scale: 65536.0000 (65055.8828)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1162  max mem: 15572
Epoch: [5]  [1920/2809]  eta: 0:08:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3315 (4.4739)  loss_scale: 65536.0000 (65058.3821)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0750  max mem: 15572
Epoch: [5]  [1930/2809]  eta: 0:08:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6253 (4.4742)  loss_scale: 65536.0000 (65060.8555)  weight_decay: 0.0500 (0.0500)  time: 0.4672  data: 0.0248  max mem: 15572
Epoch: [5]  [1940/2809]  eta: 0:08:33  lr: 0.000047  min_lr: 0.000000  loss: 4.4383 (4.4734)  loss_scale: 65536.0000 (65063.3035)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.1022  max mem: 15572
Epoch: [5]  [1950/2809]  eta: 0:08:27  lr: 0.000047  min_lr: 0.000000  loss: 4.2896 (4.4731)  loss_scale: 65536.0000 (65065.7263)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1718  max mem: 15572
[2025-01-12 23:11:57,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=97, lr=[4.537251678590611e-07, 4.537251678590611e-07, 6.481788112272302e-07, 6.481788112272302e-07, 9.259697303246147e-07, 9.259697303246147e-07, 1.3228139004637354e-06, 1.3228139004637354e-06, 1.889734143519622e-06, 1.889734143519622e-06, 2.6996202050280316e-06, 2.6996202050280316e-06, 3.856600292897189e-06, 3.856600292897189e-06, 5.509428989853127e-06, 5.509428989853127e-06, 7.870612842647324e-06, 7.870612842647324e-06, 1.124373263235332e-05, 1.124373263235332e-05, 1.606247518907617e-05, 1.606247518907617e-05, 2.2946393127251676e-05, 2.2946393127251676e-05, 3.2780561610359544e-05, 3.2780561610359544e-05, 4.682937372908506e-05, 4.682937372908506e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:11:57,003] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=27.889243689097754, CurrSamplesPerSec=31.359243166052128, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [5]  [1960/2809]  eta: 0:08:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4053 (4.4730)  loss_scale: 65536.0000 (65068.1244)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1272  max mem: 15572
Epoch: [5]  [1970/2809]  eta: 0:08:15  lr: 0.000047  min_lr: 0.000000  loss: 4.4437 (4.4730)  loss_scale: 65536.0000 (65070.4982)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1339  max mem: 15572
[2025-01-12 23:12:09,528] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:12:09,528] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:12:11,821] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16024
[2025-01-12 23:12:11,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:12:11,821] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1980/2809]  eta: 0:08:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5375 (4.4737)  loss_scale: 65536.0000 (65172.0949)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1540  max mem: 15572
Epoch: [5]  [1990/2809]  eta: 0:08:03  lr: 0.000047  min_lr: 0.000000  loss: 4.5527 (4.4739)  loss_scale: 65536.0000 (65173.9227)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1564  max mem: 15572
Epoch: [5]  [2000/2809]  eta: 0:07:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4905 (4.4738)  loss_scale: 65536.0000 (65175.7321)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1487  max mem: 15572
Epoch: [5]  [2010/2809]  eta: 0:07:51  lr: 0.000047  min_lr: 0.000000  loss: 4.3764 (4.4731)  loss_scale: 65536.0000 (65177.5236)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1024  max mem: 15572
Epoch: [5]  [2020/2809]  eta: 0:07:46  lr: 0.000047  min_lr: 0.000000  loss: 4.3815 (4.4729)  loss_scale: 65536.0000 (65179.2974)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1837  max mem: 15572
Epoch: [5]  [2030/2809]  eta: 0:07:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4970 (4.4725)  loss_scale: 65536.0000 (65181.0537)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.1927  max mem: 15572
Epoch: [5]  [2040/2809]  eta: 0:07:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4970 (4.4726)  loss_scale: 65536.0000 (65182.7927)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1505  max mem: 15572
Epoch: [5]  [2050/2809]  eta: 0:07:28  lr: 0.000047  min_lr: 0.000000  loss: 4.3868 (4.4721)  loss_scale: 65536.0000 (65184.5149)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1183  max mem: 15572
Epoch: [5]  [2060/2809]  eta: 0:07:22  lr: 0.000047  min_lr: 0.000000  loss: 4.3868 (4.4710)  loss_scale: 65536.0000 (65186.2203)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1221  max mem: 15572
Epoch: [5]  [2070/2809]  eta: 0:07:16  lr: 0.000047  min_lr: 0.000000  loss: 4.2963 (4.4704)  loss_scale: 65536.0000 (65187.9092)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1055  max mem: 15572
Epoch: [5]  [2080/2809]  eta: 0:07:09  lr: 0.000047  min_lr: 0.000000  loss: 4.4109 (4.4708)  loss_scale: 65536.0000 (65189.5819)  weight_decay: 0.0500 (0.0500)  time: 0.4748  data: 0.0280  max mem: 15572
Epoch: [5]  [2090/2809]  eta: 0:07:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4547 (4.4708)  loss_scale: 65536.0000 (65191.2386)  weight_decay: 0.0500 (0.0500)  time: 0.4605  data: 0.0113  max mem: 15572
Epoch: [5]  [2100/2809]  eta: 0:06:57  lr: 0.000047  min_lr: 0.000000  loss: 4.5139 (4.4709)  loss_scale: 65536.0000 (65192.8796)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.0648  max mem: 15572
[2025-01-12 23:13:25,311] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:13:25,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [2110/2809]  eta: 0:06:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5052 (4.4713)  loss_scale: 65536.0000 (65287.6400)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1300  max mem: 15572
[2025-01-12 23:13:29,529] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16161
[2025-01-12 23:13:29,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:13:29,530] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2120/2809]  eta: 0:06:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4957 (4.4714)  loss_scale: 65536.0000 (65443.3041)  weight_decay: 0.0500 (0.0500)  time: 0.6984  data: 0.1999  max mem: 15572
Epoch: [5]  [2130/2809]  eta: 0:06:41  lr: 0.000047  min_lr: 0.000000  loss: 4.2583 (4.4705)  loss_scale: 65536.0000 (65443.7391)  weight_decay: 0.0500 (0.0500)  time: 0.7385  data: 0.2592  max mem: 15572
Epoch: [5]  [2140/2809]  eta: 0:06:35  lr: 0.000047  min_lr: 0.000000  loss: 4.2583 (4.4695)  loss_scale: 65536.0000 (65444.1700)  weight_decay: 0.0500 (0.0500)  time: 0.6638  data: 0.1921  max mem: 15572
Epoch: [5]  [2150/2809]  eta: 0:06:29  lr: 0.000047  min_lr: 0.000000  loss: 4.4175 (4.4697)  loss_scale: 65536.0000 (65444.5969)  weight_decay: 0.0500 (0.0500)  time: 0.6573  data: 0.1673  max mem: 15572
Epoch: [5]  [2160/2809]  eta: 0:06:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5445 (4.4699)  loss_scale: 65536.0000 (65445.0199)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1197  max mem: 15572
Epoch: [5]  [2170/2809]  eta: 0:06:18  lr: 0.000047  min_lr: 0.000000  loss: 4.5657 (4.4703)  loss_scale: 65536.0000 (65445.4390)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1858  max mem: 15572
Epoch: [5]  [2180/2809]  eta: 0:06:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5843 (4.4706)  loss_scale: 65536.0000 (65445.8542)  weight_decay: 0.0500 (0.0500)  time: 0.7381  data: 0.2434  max mem: 15572
Epoch: [5]  [2190/2809]  eta: 0:06:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5366 (4.4711)  loss_scale: 65536.0000 (65446.2656)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.1247  max mem: 15572
Epoch: [5]  [2200/2809]  eta: 0:06:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4651 (4.4709)  loss_scale: 65536.0000 (65446.6733)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1464  max mem: 15572
Epoch: [5]  [2210/2809]  eta: 0:05:55  lr: 0.000047  min_lr: 0.000000  loss: 4.3792 (4.4705)  loss_scale: 65536.0000 (65447.0773)  weight_decay: 0.0500 (0.0500)  time: 0.6707  data: 0.2053  max mem: 15572
Epoch: [5]  [2220/2809]  eta: 0:05:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3792 (4.4701)  loss_scale: 65536.0000 (65447.4777)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1759  max mem: 15572
Epoch: [5]  [2230/2809]  eta: 0:05:42  lr: 0.000047  min_lr: 0.000000  loss: 4.3505 (4.4691)  loss_scale: 65536.0000 (65447.8745)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.0699  max mem: 15572
Epoch: [5]  [2240/2809]  eta: 0:05:36  lr: 0.000047  min_lr: 0.000000  loss: 4.3505 (4.4694)  loss_scale: 65536.0000 (65448.2677)  weight_decay: 0.0500 (0.0500)  time: 0.4207  data: 0.0006  max mem: 15572
[2025-01-12 23:14:49,509] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:14:49,510] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [2250/2809]  eta: 0:05:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4973 (4.4696)  loss_scale: 65536.0000 (65623.3425)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0284  max mem: 15572
[2025-01-12 23:14:56,580] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16302
[2025-01-12 23:14:56,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:14:56,580] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2260/2809]  eta: 0:05:24  lr: 0.000047  min_lr: 0.000000  loss: 4.2948 (4.4687)  loss_scale: 131072.0000 (65796.8686)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0767  max mem: 15572
Epoch: [5]  [2270/2809]  eta: 0:05:18  lr: 0.000047  min_lr: 0.000000  loss: 4.4571 (4.4691)  loss_scale: 65536.0000 (65795.7199)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1376  max mem: 15572
Epoch: [5]  [2280/2809]  eta: 0:05:12  lr: 0.000047  min_lr: 0.000000  loss: 4.4452 (4.4684)  loss_scale: 65536.0000 (65794.5813)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1482  max mem: 15572
Epoch: [5]  [2290/2809]  eta: 0:05:06  lr: 0.000047  min_lr: 0.000000  loss: 4.3968 (4.4685)  loss_scale: 65536.0000 (65793.4526)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1102  max mem: 15572
Epoch: [5]  [2300/2809]  eta: 0:05:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4865 (4.4684)  loss_scale: 65536.0000 (65792.3338)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1012  max mem: 15572
Epoch: [5]  [2310/2809]  eta: 0:04:54  lr: 0.000047  min_lr: 0.000000  loss: 4.3472 (4.4680)  loss_scale: 65536.0000 (65791.2246)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1494  max mem: 15572
Epoch: [5]  [2320/2809]  eta: 0:04:48  lr: 0.000047  min_lr: 0.000000  loss: 4.3472 (4.4679)  loss_scale: 65536.0000 (65790.1249)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.1838  max mem: 15572
Epoch: [5]  [2330/2809]  eta: 0:04:43  lr: 0.000047  min_lr: 0.000000  loss: 4.3946 (4.4675)  loss_scale: 65536.0000 (65789.0347)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1593  max mem: 15572
Epoch: [5]  [2340/2809]  eta: 0:04:37  lr: 0.000047  min_lr: 0.000000  loss: 4.3492 (4.4668)  loss_scale: 65536.0000 (65787.9539)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1807  max mem: 15572
Epoch: [5]  [2350/2809]  eta: 0:04:31  lr: 0.000047  min_lr: 0.000000  loss: 4.3625 (4.4668)  loss_scale: 65536.0000 (65786.8822)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1692  max mem: 15572
Epoch: [5]  [2360/2809]  eta: 0:04:25  lr: 0.000047  min_lr: 0.000000  loss: 4.3970 (4.4666)  loss_scale: 65536.0000 (65785.8196)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0834  max mem: 15572
Epoch: [5]  [2370/2809]  eta: 0:04:19  lr: 0.000047  min_lr: 0.000000  loss: 4.3930 (4.4660)  loss_scale: 65536.0000 (65784.7659)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0874  max mem: 15572
Epoch: [5]  [2380/2809]  eta: 0:04:13  lr: 0.000047  min_lr: 0.000000  loss: 4.3010 (4.4651)  loss_scale: 65536.0000 (65783.7211)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1740  max mem: 15572
[2025-01-12 23:16:11,342] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:16:11,343] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [2390/2809]  eta: 0:04:07  lr: 0.000047  min_lr: 0.000000  loss: 4.3010 (4.4652)  loss_scale: 65536.0000 (65919.7323)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1926  max mem: 15572
[2025-01-12 23:16:15,870] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16437
[2025-01-12 23:16:15,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:16:15,870] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2400/2809]  eta: 0:04:01  lr: 0.000047  min_lr: 0.000000  loss: 4.4381 (4.4655)  loss_scale: 65536.0000 (65945.4294)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1206  max mem: 15572
Epoch: [5]  [2410/2809]  eta: 0:03:55  lr: 0.000047  min_lr: 0.000000  loss: 4.4277 (4.4652)  loss_scale: 65536.0000 (65943.7312)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1513  max mem: 15572
Epoch: [5]  [2420/2809]  eta: 0:03:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3799 (4.4641)  loss_scale: 65536.0000 (65942.0471)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1595  max mem: 15572
Epoch: [5]  [2430/2809]  eta: 0:03:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4365 (4.4647)  loss_scale: 65536.0000 (65940.3768)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0930  max mem: 15572
Epoch: [5]  [2440/2809]  eta: 0:03:38  lr: 0.000047  min_lr: 0.000000  loss: 4.4720 (4.4644)  loss_scale: 65536.0000 (65938.7202)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1265  max mem: 15572
Epoch: [5]  [2450/2809]  eta: 0:03:32  lr: 0.000047  min_lr: 0.000000  loss: 4.3587 (4.4639)  loss_scale: 65536.0000 (65937.0771)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.1661  max mem: 15572
Epoch: [5]  [2460/2809]  eta: 0:03:26  lr: 0.000047  min_lr: 0.000000  loss: 4.4526 (4.4647)  loss_scale: 65536.0000 (65935.4474)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0896  max mem: 15572
Epoch: [5]  [2470/2809]  eta: 0:03:20  lr: 0.000047  min_lr: 0.000000  loss: 4.5257 (4.4648)  loss_scale: 65536.0000 (65933.8308)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0400  max mem: 15572
Epoch: [5]  [2480/2809]  eta: 0:03:14  lr: 0.000047  min_lr: 0.000000  loss: 4.4945 (4.4646)  loss_scale: 65536.0000 (65932.2273)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1067  max mem: 15572
Epoch: [5]  [2490/2809]  eta: 0:03:08  lr: 0.000047  min_lr: 0.000000  loss: 4.2816 (4.4635)  loss_scale: 65536.0000 (65930.6367)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1779  max mem: 15572
Epoch: [5]  [2500/2809]  eta: 0:03:02  lr: 0.000047  min_lr: 0.000000  loss: 4.2816 (4.4638)  loss_scale: 65536.0000 (65929.0588)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1608  max mem: 15572
Epoch: [5]  [2510/2809]  eta: 0:02:56  lr: 0.000047  min_lr: 0.000000  loss: 4.4743 (4.4633)  loss_scale: 65536.0000 (65927.4934)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1581  max mem: 15572
Epoch: [5]  [2520/2809]  eta: 0:02:50  lr: 0.000047  min_lr: 0.000000  loss: 4.3909 (4.4629)  loss_scale: 65536.0000 (65925.9405)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.1835  max mem: 15572
[2025-01-12 23:17:32,826] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:17:32,826] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:17:33,987] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16568
[2025-01-12 23:17:33,988] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:17:33,988] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2530/2809]  eta: 0:02:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3357 (4.4621)  loss_scale: 65536.0000 (65976.1865)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0900  max mem: 15572
Epoch: [5]  [2540/2809]  eta: 0:02:38  lr: 0.000047  min_lr: 0.000000  loss: 4.4756 (4.4624)  loss_scale: 65536.0000 (65974.4542)  weight_decay: 0.0500 (0.0500)  time: 0.4930  data: 0.0344  max mem: 15572
Epoch: [5]  [2550/2809]  eta: 0:02:32  lr: 0.000047  min_lr: 0.000000  loss: 4.4993 (4.4624)  loss_scale: 65536.0000 (65972.7354)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1053  max mem: 15572
Epoch: [5]  [2560/2809]  eta: 0:02:26  lr: 0.000047  min_lr: 0.000000  loss: 4.4214 (4.4623)  loss_scale: 65536.0000 (65971.0301)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1425  max mem: 15572
Epoch: [5]  [2570/2809]  eta: 0:02:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4010 (4.4625)  loss_scale: 65536.0000 (65969.3380)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1290  max mem: 15572
Epoch: [5]  [2580/2809]  eta: 0:02:15  lr: 0.000047  min_lr: 0.000000  loss: 4.3753 (4.4624)  loss_scale: 65536.0000 (65967.6590)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1780  max mem: 15572
Epoch: [5]  [2590/2809]  eta: 0:02:09  lr: 0.000047  min_lr: 0.000000  loss: 4.4815 (4.4628)  loss_scale: 65536.0000 (65965.9931)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1591  max mem: 15572
Epoch: [5]  [2600/2809]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4998 (4.4630)  loss_scale: 65536.0000 (65964.3399)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0950  max mem: 15572
Epoch: [5]  [2610/2809]  eta: 0:01:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4452 (4.4629)  loss_scale: 65536.0000 (65962.6993)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0727  max mem: 15572
Epoch: [5]  [2620/2809]  eta: 0:01:51  lr: 0.000047  min_lr: 0.000000  loss: 4.4281 (4.4629)  loss_scale: 65536.0000 (65961.0713)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1400  max mem: 15572
Epoch: [5]  [2630/2809]  eta: 0:01:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3842 (4.4625)  loss_scale: 65536.0000 (65959.4557)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1959  max mem: 15572
Epoch: [5]  [2640/2809]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000000  loss: 4.3639 (4.4626)  loss_scale: 65536.0000 (65957.8523)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1317  max mem: 15572
Epoch: [5]  [2650/2809]  eta: 0:01:33  lr: 0.000047  min_lr: 0.000000  loss: 4.3639 (4.4623)  loss_scale: 65536.0000 (65956.2610)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1252  max mem: 15572
[2025-01-12 23:18:47,752] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:18:47,753] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:18:48,208] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16698
[2025-01-12 23:18:48,209] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:18:48,209] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2660/2809]  eta: 0:01:27  lr: 0.000047  min_lr: 0.000000  loss: 4.3506 (4.4622)  loss_scale: 65536.0000 (65979.3100)  weight_decay: 0.0500 (0.0500)  time: 0.6492  data: 0.2107  max mem: 15572
Epoch: [5]  [2670/2809]  eta: 0:01:22  lr: 0.000047  min_lr: 0.000000  loss: 4.4476 (4.4623)  loss_scale: 65536.0000 (65977.6503)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1532  max mem: 15572
Epoch: [5]  [2680/2809]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4287 (4.4617)  loss_scale: 65536.0000 (65976.0030)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0850  max mem: 15572
Epoch: [5]  [2690/2809]  eta: 0:01:10  lr: 0.000047  min_lr: 0.000000  loss: 4.2589 (4.4607)  loss_scale: 65536.0000 (65974.3679)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1251  max mem: 15572
Epoch: [5]  [2700/2809]  eta: 0:01:04  lr: 0.000047  min_lr: 0.000000  loss: 4.1956 (4.4598)  loss_scale: 65536.0000 (65972.7449)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0826  max mem: 15572
Epoch: [5]  [2710/2809]  eta: 0:00:58  lr: 0.000047  min_lr: 0.000000  loss: 4.3976 (4.4604)  loss_scale: 65536.0000 (65971.1339)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1092  max mem: 15572
Epoch: [5]  [2720/2809]  eta: 0:00:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5446 (4.4606)  loss_scale: 65536.0000 (65969.5347)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1483  max mem: 15572
Epoch: [5]  [2730/2809]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.5343 (4.4610)  loss_scale: 65536.0000 (65967.9473)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1384  max mem: 15572
Epoch: [5]  [2740/2809]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000000  loss: 4.3946 (4.4604)  loss_scale: 65536.0000 (65966.3714)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1338  max mem: 15572
Epoch: [5]  [2750/2809]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.2064 (4.4596)  loss_scale: 65536.0000 (65964.8070)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1389  max mem: 15572
Epoch: [5]  [2760/2809]  eta: 0:00:28  lr: 0.000047  min_lr: 0.000000  loss: 4.3062 (4.4592)  loss_scale: 65536.0000 (65963.2539)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1863  max mem: 15572
Epoch: [5]  [2770/2809]  eta: 0:00:23  lr: 0.000047  min_lr: 0.000000  loss: 4.3687 (4.4589)  loss_scale: 65536.0000 (65961.7120)  weight_decay: 0.0500 (0.0500)  time: 0.6823  data: 0.2211  max mem: 15572
Epoch: [5]  [2780/2809]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000000  loss: 4.4218 (4.4587)  loss_scale: 65536.0000 (65960.1812)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1557  max mem: 15572
[2025-01-12 23:20:04,793] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:20:04,793] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [2790/2809]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.4492 (4.4588)  loss_scale: 65536.0000 (66169.9921)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0964  max mem: 15572
[2025-01-12 23:20:11,087] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16837
[2025-01-12 23:20:11,087] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:20:11,087] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2800/2809]  eta: 0:00:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4422 (4.4589)  loss_scale: 65536.0000 (66191.1260)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1439  max mem: 15572
Epoch: [5]  [2808/2809]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4422 (4.4590)  loss_scale: 65536.0000 (66189.2602)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0836  max mem: 15572
Epoch: [5] Total time: 0:27:37 (0.5901 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000000  loss: 4.4422 (4.4590)  loss_scale: 65536.0000 (66189.2602)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:21:00  loss: 0.8775 (0.8775)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.6337  data: 4.4205  max mem: 15572
Val:  [ 10/272]  eta: 0:03:27  loss: 4.2028 (3.7985)  acc1: 0.0000 (24.2424)  acc5: 11.1111 (30.8081)  time: 0.7912  data: 0.6009  max mem: 15572
Val:  [ 20/272]  eta: 0:02:22  loss: 3.9893 (3.8341)  acc1: 0.0000 (18.5185)  acc5: 27.7778 (31.7460)  time: 0.3618  data: 0.1671  max mem: 15572
Val:  [ 30/272]  eta: 0:01:56  loss: 3.9725 (3.9221)  acc1: 0.0000 (12.9032)  acc5: 27.7778 (31.3620)  time: 0.3079  data: 0.1091  max mem: 15572
Val:  [ 40/272]  eta: 0:01:41  loss: 3.6354 (3.7916)  acc1: 0.0000 (13.8211)  acc5: 38.8889 (36.9919)  time: 0.3053  data: 0.1113  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 3.2219 (3.7570)  acc1: 11.1111 (13.2898)  acc5: 50.0000 (39.1068)  time: 0.3111  data: 0.1125  max mem: 15572
Val:  [ 60/272]  eta: 0:01:25  loss: 2.8790 (3.6041)  acc1: 16.6667 (19.3078)  acc5: 72.2222 (43.6248)  time: 0.3316  data: 0.1279  max mem: 15572
Val:  [ 70/272]  eta: 0:01:20  loss: 2.9498 (3.5127)  acc1: 33.3333 (21.2050)  acc5: 72.2222 (46.9484)  time: 0.3566  data: 0.1543  max mem: 15572
Val:  [ 80/272]  eta: 0:01:17  loss: 3.1914 (3.5131)  acc1: 11.1111 (22.0165)  acc5: 61.1111 (46.6392)  time: 0.4139  data: 0.2105  max mem: 15572
Val:  [ 90/272]  eta: 0:01:10  loss: 4.4144 (3.6139)  acc1: 0.0000 (19.7192)  acc5: 11.1111 (42.4908)  time: 0.3534  data: 0.1497  max mem: 15572
Val:  [100/272]  eta: 0:01:04  loss: 4.3701 (3.6711)  acc1: 0.0000 (18.9769)  acc5: 11.1111 (40.7591)  time: 0.2476  data: 0.0438  max mem: 15572
Val:  [110/272]  eta: 0:01:00  loss: 4.1317 (3.7314)  acc1: 0.0000 (17.4675)  acc5: 16.6667 (39.1391)  time: 0.3056  data: 0.1095  max mem: 15572
Val:  [120/272]  eta: 0:00:56  loss: 4.2056 (3.7796)  acc1: 0.0000 (16.0239)  acc5: 16.6667 (38.1084)  time: 0.3512  data: 0.1617  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 4.1712 (3.7172)  acc1: 0.0000 (18.1934)  acc5: 33.3333 (39.8643)  time: 0.3239  data: 0.1149  max mem: 15572
Val:  [140/272]  eta: 0:00:48  loss: 3.4243 (3.7012)  acc1: 22.2222 (19.0701)  acc5: 55.5556 (40.2679)  time: 0.3342  data: 0.1200  max mem: 15572
Val:  [150/272]  eta: 0:00:44  loss: 3.9414 (3.7174)  acc1: 5.5556 (18.1015)  acc5: 27.7778 (39.5511)  time: 0.3415  data: 0.1297  max mem: 15572
Val:  [160/272]  eta: 0:00:40  loss: 3.7544 (3.7030)  acc1: 11.1111 (19.2892)  acc5: 38.8889 (41.0973)  time: 0.3479  data: 0.1213  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 3.6570 (3.7338)  acc1: 11.1111 (18.4860)  acc5: 50.0000 (40.5133)  time: 0.3618  data: 0.1500  max mem: 15572
Val:  [180/272]  eta: 0:00:33  loss: 3.8984 (3.7325)  acc1: 0.0000 (18.0172)  acc5: 33.3333 (40.7612)  time: 0.3577  data: 0.1562  max mem: 15572
Val:  [190/272]  eta: 0:00:29  loss: 3.9176 (3.7518)  acc1: 0.0000 (17.4229)  acc5: 33.3333 (40.0233)  time: 0.3285  data: 0.1117  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.7037 (3.7498)  acc1: 0.0000 (17.5235)  acc5: 44.4444 (40.9619)  time: 0.3031  data: 0.0747  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.5532 (3.7642)  acc1: 5.5556 (17.5355)  acc5: 55.5556 (41.0479)  time: 0.3120  data: 0.1060  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 3.8646 (3.7695)  acc1: 11.1111 (17.5968)  acc5: 38.8889 (40.9502)  time: 0.3125  data: 0.1319  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.4410 (3.7387)  acc1: 27.7778 (19.5286)  acc5: 61.1111 (42.4483)  time: 0.3129  data: 0.1364  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 3.0215 (3.7184)  acc1: 44.4444 (19.9862)  acc5: 83.3333 (43.7990)  time: 0.2874  data: 0.1041  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.5735 (3.7457)  acc1: 11.1111 (19.4112)  acc5: 38.8889 (43.1828)  time: 0.2764  data: 0.0877  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.4616 (3.6772)  acc1: 33.3333 (21.5837)  acc5: 72.2222 (44.9340)  time: 0.3138  data: 0.1316  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.8898 (3.6745)  acc1: 55.5556 (21.5662)  acc5: 77.7778 (44.9775)  time: 0.2467  data: 0.0812  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.8898 (3.6776)  acc1: 55.5556 (21.5646)  acc5: 77.7778 (44.9519)  time: 0.2336  data: 0.0737  max mem: 15572
Val: Total time: 0:01:31 (0.3380 s / it)
* Acc@1 21.565 Acc@5 44.952 loss 3.678
Accuracy of the network on the 4883 val videos: 21.6%
[2025-01-12 23:21:50,526] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-12 23:21:50,528] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 23:21:50,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 23:21:53,247] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 23:21:53,248] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 21.56%
Epoch: [6]  [   0/2809]  eta: 5:26:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3507 (4.3507)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.9684  data: 6.5498  max mem: 15572
Epoch: [6]  [  10/2809]  eta: 0:59:57  lr: 0.000047  min_lr: 0.000000  loss: 4.3507 (4.4300)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2852  data: 0.8461  max mem: 15572
Epoch: [6]  [  20/2809]  eta: 0:42:01  lr: 0.000047  min_lr: 0.000000  loss: 4.3900 (4.4289)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1465  max mem: 15572
Epoch: [6]  [  30/2809]  eta: 0:38:59  lr: 0.000047  min_lr: 0.000000  loss: 4.4347 (4.4452)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1400  max mem: 15572
Epoch: [6]  [  40/2809]  eta: 0:35:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4106 (4.4164)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1542  max mem: 15572
Epoch: [6]  [  50/2809]  eta: 0:33:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4620 (4.4302)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1130  max mem: 15572
Epoch: [6]  [  60/2809]  eta: 0:32:44  lr: 0.000047  min_lr: 0.000000  loss: 4.4604 (4.4289)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1575  max mem: 15572
Epoch: [6]  [  70/2809]  eta: 0:32:24  lr: 0.000047  min_lr: 0.000000  loss: 4.3388 (4.4208)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6477  data: 0.1707  max mem: 15572
Epoch: [6]  [  80/2809]  eta: 0:31:26  lr: 0.000047  min_lr: 0.000000  loss: 4.3832 (4.4248)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1644  max mem: 15572
Epoch: [6]  [  90/2809]  eta: 0:31:08  lr: 0.000047  min_lr: 0.000000  loss: 4.4423 (4.4288)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1830  max mem: 15572
Epoch: [6]  [ 100/2809]  eta: 0:30:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5365 (4.4304)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1572  max mem: 15572
Epoch: [6]  [ 110/2809]  eta: 0:29:41  lr: 0.000047  min_lr: 0.000000  loss: 4.4408 (4.4114)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0907  max mem: 15572
[2025-01-12 23:23:08,135] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:23:08,135] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 120/2809]  eta: 0:29:01  lr: 0.000047  min_lr: 0.000000  loss: 4.4384 (4.4121)  loss_scale: 65536.0000 (70410.5785)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0889  max mem: 15572
Epoch: [6]  [ 130/2809]  eta: 0:28:35  lr: 0.000047  min_lr: 0.000000  loss: 4.3960 (4.4016)  loss_scale: 131072.0000 (75041.2214)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0797  max mem: 15572
[2025-01-12 23:23:19,861] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16988
[2025-01-12 23:23:19,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:23:19,862] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 140/2809]  eta: 0:28:12  lr: 0.000047  min_lr: 0.000000  loss: 4.3106 (4.4007)  loss_scale: 131072.0000 (75761.4752)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0929  max mem: 15572
[2025-01-12 23:23:26,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=105, lr=[4.531573325722194e-07, 4.531573325722194e-07, 6.473676179603135e-07, 6.473676179603135e-07, 9.248108828004479e-07, 9.248108828004479e-07, 1.32115840400064e-06, 1.32115840400064e-06, 1.887369148572343e-06, 1.887369148572343e-06, 2.696241640817633e-06, 2.696241640817633e-06, 3.851773772596618e-06, 3.851773772596618e-06, 5.502533960852313e-06, 5.502533960852313e-06, 7.86076280121759e-06, 7.86076280121759e-06, 1.1229661144596558e-05, 1.1229661144596558e-05, 1.6042373063709367e-05, 1.6042373063709367e-05, 2.29176758052991e-05, 2.29176758052991e-05, 3.2739536864713004e-05, 3.2739536864713004e-05, 4.6770766949590006e-05, 4.6770766949590006e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:23:26,011] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=27.897779800533904, CurrSamplesPerSec=30.159732891668206, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [ 150/2809]  eta: 0:27:49  lr: 0.000047  min_lr: 0.000000  loss: 4.2992 (4.3949)  loss_scale: 65536.0000 (75084.2914)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0969  max mem: 15572
Epoch: [6]  [ 160/2809]  eta: 0:27:59  lr: 0.000047  min_lr: 0.000000  loss: 4.3363 (4.3890)  loss_scale: 65536.0000 (74491.2298)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1655  max mem: 15572
Epoch: [6]  [ 170/2809]  eta: 0:27:31  lr: 0.000047  min_lr: 0.000000  loss: 4.4205 (4.3898)  loss_scale: 65536.0000 (73967.5322)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1402  max mem: 15572
Epoch: [6]  [ 180/2809]  eta: 0:27:36  lr: 0.000047  min_lr: 0.000000  loss: 4.2998 (4.3822)  loss_scale: 65536.0000 (73501.7017)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1586  max mem: 15572
Epoch: [6]  [ 190/2809]  eta: 0:27:18  lr: 0.000047  min_lr: 0.000000  loss: 4.2998 (4.3829)  loss_scale: 65536.0000 (73084.6492)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1687  max mem: 15572
Epoch: [6]  [ 200/2809]  eta: 0:27:05  lr: 0.000047  min_lr: 0.000000  loss: 4.3476 (4.3805)  loss_scale: 65536.0000 (72709.0945)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0979  max mem: 15572
Epoch: [6]  [ 210/2809]  eta: 0:26:57  lr: 0.000047  min_lr: 0.000000  loss: 4.3176 (4.3811)  loss_scale: 65536.0000 (72369.1374)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1517  max mem: 15572
Epoch: [6]  [ 220/2809]  eta: 0:26:50  lr: 0.000047  min_lr: 0.000000  loss: 4.3176 (4.3780)  loss_scale: 65536.0000 (72059.9457)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1541  max mem: 15572
Epoch: [6]  [ 230/2809]  eta: 0:26:38  lr: 0.000047  min_lr: 0.000000  loss: 4.3750 (4.3824)  loss_scale: 65536.0000 (71777.5238)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1192  max mem: 15572
Epoch: [6]  [ 240/2809]  eta: 0:26:35  lr: 0.000047  min_lr: 0.000000  loss: 4.3356 (4.3797)  loss_scale: 65536.0000 (71518.5394)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1413  max mem: 15572
[2025-01-12 23:24:26,502] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17097
[2025-01-12 23:24:26,503] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 23:24:26,503] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 250/2809]  eta: 0:26:32  lr: 0.000047  min_lr: 0.000000  loss: 4.2848 (4.3811)  loss_scale: 65536.0000 (70235.7928)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.1859  max mem: 15572
Epoch: [6]  [ 260/2809]  eta: 0:26:17  lr: 0.000047  min_lr: 0.000000  loss: 4.3666 (4.3837)  loss_scale: 32768.0000 (68800.2452)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1509  max mem: 15572
Epoch: [6]  [ 270/2809]  eta: 0:26:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4865 (4.3871)  loss_scale: 32768.0000 (67470.6421)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0985  max mem: 15572
Epoch: [6]  [ 280/2809]  eta: 0:25:53  lr: 0.000047  min_lr: 0.000000  loss: 4.4879 (4.3908)  loss_scale: 32768.0000 (66235.6726)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1124  max mem: 15572
Epoch: [6]  [ 290/2809]  eta: 0:25:41  lr: 0.000047  min_lr: 0.000000  loss: 4.3980 (4.3882)  loss_scale: 32768.0000 (65085.5808)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1252  max mem: 15572
Epoch: [6]  [ 300/2809]  eta: 0:25:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3757 (4.3910)  loss_scale: 32768.0000 (64011.9070)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.1953  max mem: 15572
Epoch: [6]  [ 310/2809]  eta: 0:25:37  lr: 0.000047  min_lr: 0.000000  loss: 4.4042 (4.3876)  loss_scale: 32768.0000 (63007.2797)  weight_decay: 0.0500 (0.0500)  time: 0.6594  data: 0.2076  max mem: 15572
Epoch: [6]  [ 320/2809]  eta: 0:25:29  lr: 0.000047  min_lr: 0.000000  loss: 4.3076 (4.3844)  loss_scale: 32768.0000 (62065.2461)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1414  max mem: 15572
Epoch: [6]  [ 330/2809]  eta: 0:25:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3469 (4.3846)  loss_scale: 32768.0000 (61180.1329)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0708  max mem: 15572
Epoch: [6]  [ 340/2809]  eta: 0:25:09  lr: 0.000047  min_lr: 0.000000  loss: 4.3705 (4.3818)  loss_scale: 32768.0000 (60346.9326)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0921  max mem: 15572
Epoch: [6]  [ 350/2809]  eta: 0:24:58  lr: 0.000047  min_lr: 0.000000  loss: 4.3705 (4.3846)  loss_scale: 32768.0000 (59561.2080)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1333  max mem: 15572
Epoch: [6]  [ 360/2809]  eta: 0:24:47  lr: 0.000047  min_lr: 0.000000  loss: 4.4058 (4.3854)  loss_scale: 32768.0000 (58819.0139)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0874  max mem: 15572
Epoch: [6]  [ 370/2809]  eta: 0:24:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3975 (4.3829)  loss_scale: 32768.0000 (58116.8302)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1202  max mem: 15572
[2025-01-12 23:25:40,008] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:25:40,008] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 380/2809]  eta: 0:24:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3831 (4.3825)  loss_scale: 32768.0000 (58225.5538)  weight_decay: 0.0500 (0.0500)  time: 0.6781  data: 0.2021  max mem: 15572
Epoch: [6]  [ 390/2809]  eta: 0:24:36  lr: 0.000047  min_lr: 0.000000  loss: 4.3150 (4.3811)  loss_scale: 65536.0000 (58412.5217)  weight_decay: 0.0500 (0.0500)  time: 0.6475  data: 0.1868  max mem: 15572
Epoch: [6]  [ 400/2809]  eta: 0:24:27  lr: 0.000047  min_lr: 0.000000  loss: 4.2867 (4.3787)  loss_scale: 65536.0000 (58590.1646)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1131  max mem: 15572
Epoch: [6]  [ 410/2809]  eta: 0:24:24  lr: 0.000047  min_lr: 0.000000  loss: 4.2707 (4.3780)  loss_scale: 65536.0000 (58759.1630)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1571  max mem: 15572
Epoch: [6]  [ 420/2809]  eta: 0:24:15  lr: 0.000047  min_lr: 0.000000  loss: 4.2345 (4.3751)  loss_scale: 65536.0000 (58920.1330)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1481  max mem: 15572
Epoch: [6]  [ 430/2809]  eta: 0:24:03  lr: 0.000047  min_lr: 0.000000  loss: 4.1451 (4.3742)  loss_scale: 65536.0000 (59073.6334)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0841  max mem: 15572
Epoch: [6]  [ 440/2809]  eta: 0:23:55  lr: 0.000047  min_lr: 0.000000  loss: 4.2591 (4.3728)  loss_scale: 65536.0000 (59220.1723)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0996  max mem: 15572
Epoch: [6]  [ 450/2809]  eta: 0:23:46  lr: 0.000047  min_lr: 0.000000  loss: 4.3437 (4.3730)  loss_scale: 65536.0000 (59360.2129)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1103  max mem: 15572
Epoch: [6]  [ 460/2809]  eta: 0:23:37  lr: 0.000047  min_lr: 0.000000  loss: 4.4740 (4.3756)  loss_scale: 65536.0000 (59494.1779)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1104  max mem: 15572
Epoch: [6]  [ 470/2809]  eta: 0:23:31  lr: 0.000047  min_lr: 0.000000  loss: 4.4900 (4.3741)  loss_scale: 65536.0000 (59622.4544)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1152  max mem: 15572
Epoch: [6]  [ 480/2809]  eta: 0:23:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4013 (4.3760)  loss_scale: 65536.0000 (59745.3971)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1250  max mem: 15572
Epoch: [6]  [ 490/2809]  eta: 0:23:15  lr: 0.000047  min_lr: 0.000000  loss: 4.4013 (4.3745)  loss_scale: 65536.0000 (59863.3320)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0707  max mem: 15572
[2025-01-12 23:26:54,997] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:26:54,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 500/2809]  eta: 0:23:09  lr: 0.000047  min_lr: 0.000000  loss: 4.3968 (4.3744)  loss_scale: 65536.0000 (60107.3693)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0644  max mem: 15572
[2025-01-12 23:26:57,705] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17360
[2025-01-12 23:26:57,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:26:57,706] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 510/2809]  eta: 0:23:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4150 (4.3734)  loss_scale: 65536.0000 (60854.8571)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1387  max mem: 15572
Epoch: [6]  [ 520/2809]  eta: 0:22:58  lr: 0.000047  min_lr: 0.000000  loss: 4.2765 (4.3722)  loss_scale: 65536.0000 (60944.7063)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1578  max mem: 15572
Epoch: [6]  [ 530/2809]  eta: 0:22:52  lr: 0.000047  min_lr: 0.000000  loss: 4.2562 (4.3699)  loss_scale: 65536.0000 (61031.1714)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1436  max mem: 15572
Epoch: [6]  [ 540/2809]  eta: 0:22:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3015 (4.3688)  loss_scale: 65536.0000 (61114.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1203  max mem: 15572
Epoch: [6]  [ 550/2809]  eta: 0:22:38  lr: 0.000047  min_lr: 0.000000  loss: 4.3583 (4.3672)  loss_scale: 65536.0000 (61194.6860)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1292  max mem: 15572
Epoch: [6]  [ 560/2809]  eta: 0:22:28  lr: 0.000047  min_lr: 0.000000  loss: 4.3529 (4.3675)  loss_scale: 65536.0000 (61272.0713)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1016  max mem: 15572
Epoch: [6]  [ 570/2809]  eta: 0:22:22  lr: 0.000047  min_lr: 0.000000  loss: 4.4327 (4.3687)  loss_scale: 65536.0000 (61346.7461)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1117  max mem: 15572
Epoch: [6]  [ 580/2809]  eta: 0:22:17  lr: 0.000047  min_lr: 0.000000  loss: 4.4507 (4.3692)  loss_scale: 65536.0000 (61418.8503)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1735  max mem: 15572
Epoch: [6]  [ 590/2809]  eta: 0:22:08  lr: 0.000047  min_lr: 0.000000  loss: 4.4159 (4.3689)  loss_scale: 65536.0000 (61488.5144)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1463  max mem: 15572
Epoch: [6]  [ 600/2809]  eta: 0:22:01  lr: 0.000047  min_lr: 0.000000  loss: 4.3861 (4.3681)  loss_scale: 65536.0000 (61555.8602)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.1042  max mem: 15572
Epoch: [6]  [ 610/2809]  eta: 0:21:53  lr: 0.000047  min_lr: 0.000000  loss: 4.3861 (4.3682)  loss_scale: 65536.0000 (61621.0016)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1025  max mem: 15572
Epoch: [6]  [ 620/2809]  eta: 0:21:48  lr: 0.000047  min_lr: 0.000000  loss: 4.3899 (4.3690)  loss_scale: 65536.0000 (61684.0451)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1452  max mem: 15572
Epoch: [6]  [ 630/2809]  eta: 0:21:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4504 (4.3694)  loss_scale: 65536.0000 (61745.0903)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1789  max mem: 15572
[2025-01-12 23:28:13,114] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:28:13,114] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:28:15,452] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17491
[2025-01-12 23:28:15,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:28:15,452] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 640/2809]  eta: 0:21:37  lr: 0.000047  min_lr: 0.000000  loss: 4.4178 (4.3677)  loss_scale: 65536.0000 (62008.7114)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1533  max mem: 15572
Epoch: [6]  [ 650/2809]  eta: 0:21:32  lr: 0.000047  min_lr: 0.000000  loss: 4.3337 (4.3665)  loss_scale: 65536.0000 (62062.8940)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1489  max mem: 15572
Epoch: [6]  [ 660/2809]  eta: 0:21:26  lr: 0.000047  min_lr: 0.000000  loss: 4.3501 (4.3694)  loss_scale: 65536.0000 (62115.4372)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1533  max mem: 15572
Epoch: [6]  [ 670/2809]  eta: 0:21:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5952 (4.3705)  loss_scale: 65536.0000 (62166.4143)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1399  max mem: 15572
Epoch: [6]  [ 680/2809]  eta: 0:21:13  lr: 0.000047  min_lr: 0.000000  loss: 4.4792 (4.3694)  loss_scale: 65536.0000 (62215.8943)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1428  max mem: 15572
Epoch: [6]  [ 690/2809]  eta: 0:21:06  lr: 0.000047  min_lr: 0.000000  loss: 4.3633 (4.3676)  loss_scale: 65536.0000 (62263.9421)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1488  max mem: 15572
Epoch: [6]  [ 700/2809]  eta: 0:21:02  lr: 0.000047  min_lr: 0.000000  loss: 4.3097 (4.3646)  loss_scale: 65536.0000 (62310.6191)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1584  max mem: 15572
Epoch: [6]  [ 710/2809]  eta: 0:20:57  lr: 0.000047  min_lr: 0.000000  loss: 4.2123 (4.3627)  loss_scale: 65536.0000 (62355.9831)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.1911  max mem: 15572
Epoch: [6]  [ 720/2809]  eta: 0:20:53  lr: 0.000047  min_lr: 0.000000  loss: 4.2237 (4.3631)  loss_scale: 65536.0000 (62400.0888)  weight_decay: 0.0500 (0.0500)  time: 0.6644  data: 0.2280  max mem: 15572
Epoch: [6]  [ 730/2809]  eta: 0:20:47  lr: 0.000047  min_lr: 0.000000  loss: 4.3421 (4.3629)  loss_scale: 65536.0000 (62442.9877)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.1910  max mem: 15572
Epoch: [6]  [ 740/2809]  eta: 0:20:41  lr: 0.000047  min_lr: 0.000000  loss: 4.3421 (4.3631)  loss_scale: 65536.0000 (62484.7287)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1284  max mem: 15572
Epoch: [6]  [ 750/2809]  eta: 0:20:35  lr: 0.000047  min_lr: 0.000000  loss: 4.4187 (4.3640)  loss_scale: 65536.0000 (62525.3582)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1386  max mem: 15572
Epoch: [6]  [ 760/2809]  eta: 0:20:29  lr: 0.000047  min_lr: 0.000000  loss: 4.3326 (4.3633)  loss_scale: 65536.0000 (62564.9198)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1480  max mem: 15572
[2025-01-12 23:29:32,711] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:29:32,712] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 770/2809]  eta: 0:20:23  lr: 0.000047  min_lr: 0.000000  loss: 4.4055 (4.3642)  loss_scale: 65536.0000 (63028.4617)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1533  max mem: 15572
[2025-01-12 23:29:36,558] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17625
[2025-01-12 23:29:36,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:29:36,559] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 780/2809]  eta: 0:20:18  lr: 0.000047  min_lr: 0.000000  loss: 4.4055 (4.3629)  loss_scale: 65536.0000 (63060.5685)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1868  max mem: 15572
Epoch: [6]  [ 790/2809]  eta: 0:20:12  lr: 0.000047  min_lr: 0.000000  loss: 4.2890 (4.3630)  loss_scale: 65536.0000 (63091.8635)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1795  max mem: 15572
Epoch: [6]  [ 800/2809]  eta: 0:20:05  lr: 0.000047  min_lr: 0.000000  loss: 4.2078 (4.3609)  loss_scale: 65536.0000 (63122.3770)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1303  max mem: 15572
Epoch: [6]  [ 810/2809]  eta: 0:19:56  lr: 0.000047  min_lr: 0.000000  loss: 4.2125 (4.3611)  loss_scale: 65536.0000 (63152.1381)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0541  max mem: 15572
Epoch: [6]  [ 820/2809]  eta: 0:19:48  lr: 0.000047  min_lr: 0.000000  loss: 4.3831 (4.3630)  loss_scale: 65536.0000 (63181.1742)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0489  max mem: 15572
Epoch: [6]  [ 830/2809]  eta: 0:19:41  lr: 0.000047  min_lr: 0.000000  loss: 4.3132 (4.3599)  loss_scale: 65536.0000 (63209.5114)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0807  max mem: 15572
Epoch: [6]  [ 840/2809]  eta: 0:19:38  lr: 0.000047  min_lr: 0.000000  loss: 4.1031 (4.3577)  loss_scale: 65536.0000 (63237.1748)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.1747  max mem: 15572
Epoch: [6]  [ 850/2809]  eta: 0:19:31  lr: 0.000047  min_lr: 0.000000  loss: 4.3719 (4.3588)  loss_scale: 65536.0000 (63264.1880)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.1989  max mem: 15572
Epoch: [6]  [ 860/2809]  eta: 0:19:23  lr: 0.000047  min_lr: 0.000000  loss: 4.2955 (4.3569)  loss_scale: 65536.0000 (63290.5738)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0885  max mem: 15572
Epoch: [6]  [ 870/2809]  eta: 0:19:17  lr: 0.000047  min_lr: 0.000000  loss: 4.2454 (4.3585)  loss_scale: 65536.0000 (63316.3536)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0702  max mem: 15572
Epoch: [6]  [ 880/2809]  eta: 0:19:10  lr: 0.000047  min_lr: 0.000000  loss: 4.3803 (4.3590)  loss_scale: 65536.0000 (63341.5482)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0886  max mem: 15572
Epoch: [6]  [ 890/2809]  eta: 0:19:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4537 (4.3603)  loss_scale: 65536.0000 (63366.1773)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1188  max mem: 15572
[2025-01-12 23:30:51,441] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:30:51,442] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 900/2809]  eta: 0:18:59  lr: 0.000047  min_lr: 0.000000  loss: 4.4718 (4.3619)  loss_scale: 65536.0000 (63462.9967)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1621  max mem: 15572
[2025-01-12 23:30:57,515] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17762
[2025-01-12 23:30:57,515] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:30:57,515] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 910/2809]  eta: 0:18:55  lr: 0.000047  min_lr: 0.000000  loss: 4.4425 (4.3614)  loss_scale: 65536.0000 (63989.3216)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.2044  max mem: 15572
Epoch: [6]  [ 920/2809]  eta: 0:18:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4276 (4.3630)  loss_scale: 65536.0000 (64006.1151)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.2125  max mem: 15572
Epoch: [6]  [ 930/2809]  eta: 0:18:44  lr: 0.000047  min_lr: 0.000000  loss: 4.4096 (4.3621)  loss_scale: 65536.0000 (64022.5478)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1643  max mem: 15572
[2025-01-12 23:31:12,265] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17787
[2025-01-12 23:31:12,266] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 23:31:12,266] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 940/2809]  eta: 0:18:36  lr: 0.000047  min_lr: 0.000000  loss: 4.3716 (4.3624)  loss_scale: 65536.0000 (63760.0510)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0630  max mem: 15572
Epoch: [6]  [ 950/2809]  eta: 0:18:29  lr: 0.000047  min_lr: 0.000000  loss: 4.3716 (4.3619)  loss_scale: 32768.0000 (63434.1619)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0395  max mem: 15572
Epoch: [6]  [ 960/2809]  eta: 0:18:22  lr: 0.000047  min_lr: 0.000000  loss: 4.3004 (4.3615)  loss_scale: 32768.0000 (63115.0552)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0863  max mem: 15572
Epoch: [6]  [ 970/2809]  eta: 0:18:18  lr: 0.000047  min_lr: 0.000000  loss: 4.2549 (4.3602)  loss_scale: 32768.0000 (62802.5211)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1679  max mem: 15572
Epoch: [6]  [ 980/2809]  eta: 0:18:10  lr: 0.000047  min_lr: 0.000000  loss: 4.3304 (4.3607)  loss_scale: 32768.0000 (62496.3588)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1305  max mem: 15572
Epoch: [6]  [ 990/2809]  eta: 0:18:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4203 (4.3605)  loss_scale: 32768.0000 (62196.3754)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.0940  max mem: 15572
Epoch: [6]  [1000/2809]  eta: 0:17:58  lr: 0.000047  min_lr: 0.000000  loss: 4.3701 (4.3599)  loss_scale: 32768.0000 (61902.3856)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1430  max mem: 15572
Epoch: [6]  [1010/2809]  eta: 0:17:52  lr: 0.000047  min_lr: 0.000000  loss: 4.3854 (4.3606)  loss_scale: 32768.0000 (61614.2117)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1148  max mem: 15572
Epoch: [6]  [1020/2809]  eta: 0:17:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4093 (4.3610)  loss_scale: 32768.0000 (61331.6827)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1147  max mem: 15572
Epoch: [6]  [1030/2809]  eta: 0:17:40  lr: 0.000047  min_lr: 0.000000  loss: 4.3232 (4.3599)  loss_scale: 32768.0000 (61054.6343)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1268  max mem: 15572
Epoch: [6]  [1040/2809]  eta: 0:17:34  lr: 0.000047  min_lr: 0.000000  loss: 4.3437 (4.3595)  loss_scale: 32768.0000 (60782.9087)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1353  max mem: 15572
Epoch: [6]  [1050/2809]  eta: 0:17:28  lr: 0.000047  min_lr: 0.000000  loss: 4.3437 (4.3599)  loss_scale: 32768.0000 (60516.3539)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1491  max mem: 15572
Epoch: [6]  [1060/2809]  eta: 0:17:22  lr: 0.000047  min_lr: 0.000000  loss: 4.3124 (4.3603)  loss_scale: 32768.0000 (60254.8238)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1396  max mem: 15572
[2025-01-12 23:32:26,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:32:26,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [1070/2809]  eta: 0:17:15  lr: 0.000047  min_lr: 0.000000  loss: 4.4021 (4.3619)  loss_scale: 32768.0000 (60273.5387)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1047  max mem: 15572
Epoch: [6]  [1080/2809]  eta: 0:17:10  lr: 0.000047  min_lr: 0.000000  loss: 4.4021 (4.3621)  loss_scale: 65536.0000 (60322.2202)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1615  max mem: 15572
Epoch: [6]  [1090/2809]  eta: 0:17:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5337 (4.3642)  loss_scale: 65536.0000 (60370.0092)  weight_decay: 0.0500 (0.0500)  time: 0.7085  data: 0.2545  max mem: 15572
Epoch: [6]  [1100/2809]  eta: 0:16:59  lr: 0.000047  min_lr: 0.000000  loss: 4.2754 (4.3614)  loss_scale: 65536.0000 (60416.9301)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1403  max mem: 15572
Epoch: [6]  [1110/2809]  eta: 0:16:53  lr: 0.000047  min_lr: 0.000000  loss: 4.2338 (4.3604)  loss_scale: 65536.0000 (60463.0063)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0621  max mem: 15572
Epoch: [6]  [1120/2809]  eta: 0:16:47  lr: 0.000047  min_lr: 0.000000  loss: 4.3695 (4.3616)  loss_scale: 65536.0000 (60508.2605)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1338  max mem: 15572
Epoch: [6]  [1130/2809]  eta: 0:16:40  lr: 0.000047  min_lr: 0.000000  loss: 4.3596 (4.3593)  loss_scale: 65536.0000 (60552.7144)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0937  max mem: 15572
Epoch: [6]  [1140/2809]  eta: 0:16:34  lr: 0.000047  min_lr: 0.000000  loss: 4.2468 (4.3571)  loss_scale: 65536.0000 (60596.3891)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1128  max mem: 15572
[2025-01-12 23:33:15,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=111, lr=[4.523589092123934e-07, 4.523589092123934e-07, 6.462270131605621e-07, 6.462270131605621e-07, 9.231814473722315e-07, 9.231814473722315e-07, 1.318830639103188e-06, 1.318830639103188e-06, 1.8840437701474115e-06, 1.8840437701474115e-06, 2.691491100210588e-06, 2.691491100210588e-06, 3.844987286015126e-06, 3.844987286015126e-06, 5.492838980021609e-06, 5.492838980021609e-06, 7.846912828602299e-06, 7.846912828602299e-06, 1.1209875469431857e-05, 1.1209875469431857e-05, 1.601410781347408e-05, 1.601410781347408e-05, 2.2877296876391547e-05, 2.2877296876391547e-05, 3.268185268055935e-05, 3.268185268055935e-05, 4.668836097222765e-05, 4.668836097222765e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:33:15,843] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=27.89124249098753, CurrSamplesPerSec=30.109323235404965, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [1150/2809]  eta: 0:16:28  lr: 0.000047  min_lr: 0.000000  loss: 4.4134 (4.3578)  loss_scale: 65536.0000 (60639.3050)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1390  max mem: 15572
Epoch: [6]  [1160/2809]  eta: 0:16:22  lr: 0.000047  min_lr: 0.000000  loss: 4.4036 (4.3583)  loss_scale: 65536.0000 (60681.4815)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1083  max mem: 15572
Epoch: [6]  [1170/2809]  eta: 0:16:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4036 (4.3596)  loss_scale: 65536.0000 (60722.9377)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1464  max mem: 15572
Epoch: [6]  [1180/2809]  eta: 0:16:10  lr: 0.000047  min_lr: 0.000000  loss: 4.4263 (4.3589)  loss_scale: 65536.0000 (60763.6918)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1445  max mem: 15572
[2025-01-12 23:33:42,638] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:33:42,638] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1190/2809]  eta: 0:16:03  lr: 0.000047  min_lr: 0.000000  loss: 4.2993 (4.3585)  loss_scale: 65536.0000 (60858.7876)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1181  max mem: 15572
[2025-01-12 23:33:46,806] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18053
[2025-01-12 23:33:46,806] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:33:46,806] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1200/2809]  eta: 0:15:57  lr: 0.000047  min_lr: 0.000000  loss: 4.2993 (4.3579)  loss_scale: 65536.0000 (61334.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1241  max mem: 15572
Epoch: [6]  [1210/2809]  eta: 0:15:52  lr: 0.000047  min_lr: 0.000000  loss: 4.3139 (4.3580)  loss_scale: 65536.0000 (61368.9711)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1785  max mem: 15572
Epoch: [6]  [1220/2809]  eta: 0:15:46  lr: 0.000047  min_lr: 0.000000  loss: 4.3620 (4.3578)  loss_scale: 65536.0000 (61403.0991)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1590  max mem: 15572
Epoch: [6]  [1230/2809]  eta: 0:15:40  lr: 0.000047  min_lr: 0.000000  loss: 4.3810 (4.3588)  loss_scale: 65536.0000 (61436.6726)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1284  max mem: 15572
Epoch: [6]  [1240/2809]  eta: 0:15:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4431 (4.3592)  loss_scale: 65536.0000 (61469.7051)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1658  max mem: 15572
Epoch: [6]  [1250/2809]  eta: 0:15:28  lr: 0.000047  min_lr: 0.000000  loss: 4.4431 (4.3605)  loss_scale: 65536.0000 (61502.2094)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1416  max mem: 15572
Epoch: [6]  [1260/2809]  eta: 0:15:22  lr: 0.000047  min_lr: 0.000000  loss: 4.3354 (4.3597)  loss_scale: 65536.0000 (61534.1983)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1173  max mem: 15572
Epoch: [6]  [1270/2809]  eta: 0:15:16  lr: 0.000047  min_lr: 0.000000  loss: 4.3088 (4.3592)  loss_scale: 65536.0000 (61565.6837)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1571  max mem: 15572
Epoch: [6]  [1280/2809]  eta: 0:15:09  lr: 0.000047  min_lr: 0.000000  loss: 4.3910 (4.3595)  loss_scale: 65536.0000 (61596.6776)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1244  max mem: 15572
Epoch: [6]  [1290/2809]  eta: 0:15:03  lr: 0.000047  min_lr: 0.000000  loss: 4.3069 (4.3584)  loss_scale: 65536.0000 (61627.1913)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0823  max mem: 15572
Epoch: [6]  [1300/2809]  eta: 0:14:57  lr: 0.000047  min_lr: 0.000000  loss: 4.3069 (4.3581)  loss_scale: 65536.0000 (61657.2360)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1150  max mem: 15572
Epoch: [6]  [1310/2809]  eta: 0:14:51  lr: 0.000047  min_lr: 0.000000  loss: 4.2986 (4.3571)  loss_scale: 65536.0000 (61686.8223)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1243  max mem: 15572
Epoch: [6]  [1320/2809]  eta: 0:14:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3660 (4.3578)  loss_scale: 65536.0000 (61715.9606)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0940  max mem: 15572
[2025-01-12 23:35:03,326] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:35:03,326] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1330/2809]  eta: 0:14:39  lr: 0.000047  min_lr: 0.000000  loss: 4.4247 (4.3583)  loss_scale: 65536.0000 (61892.3757)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0924  max mem: 15572
Epoch: [6]  [1340/2809]  eta: 0:14:34  lr: 0.000047  min_lr: 0.000000  loss: 4.2592 (4.3579)  loss_scale: 131072.0000 (62408.2565)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1453  max mem: 15572
Epoch: [6]  [1350/2809]  eta: 0:14:26  lr: 0.000047  min_lr: 0.000000  loss: 4.2592 (4.3573)  loss_scale: 131072.0000 (62916.5004)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.0814  max mem: 15572
Epoch: [6]  [1360/2809]  eta: 0:14:20  lr: 0.000047  min_lr: 0.000000  loss: 4.3515 (4.3577)  loss_scale: 131072.0000 (63417.2755)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0524  max mem: 15572
[2025-01-12 23:35:22,199] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18215
[2025-01-12 23:35:22,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:35:22,199] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1370/2809]  eta: 0:14:13  lr: 0.000047  min_lr: 0.000000  loss: 4.3594 (4.3583)  loss_scale: 65536.0000 (63432.7294)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0966  max mem: 15572
Epoch: [6]  [1380/2809]  eta: 0:14:08  lr: 0.000047  min_lr: 0.000000  loss: 4.3238 (4.3582)  loss_scale: 65536.0000 (63447.9594)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1676  max mem: 15572
Epoch: [6]  [1390/2809]  eta: 0:14:02  lr: 0.000047  min_lr: 0.000000  loss: 4.3849 (4.3590)  loss_scale: 65536.0000 (63462.9705)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1830  max mem: 15572
Epoch: [6]  [1400/2809]  eta: 0:13:56  lr: 0.000047  min_lr: 0.000000  loss: 4.5046 (4.3603)  loss_scale: 65536.0000 (63477.7673)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1355  max mem: 15572
Epoch: [6]  [1410/2809]  eta: 0:13:50  lr: 0.000047  min_lr: 0.000000  loss: 4.5273 (4.3619)  loss_scale: 65536.0000 (63492.3544)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1216  max mem: 15572
Epoch: [6]  [1420/2809]  eta: 0:13:44  lr: 0.000047  min_lr: 0.000000  loss: 4.5078 (4.3633)  loss_scale: 65536.0000 (63506.7361)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1338  max mem: 15572
Epoch: [6]  [1430/2809]  eta: 0:13:37  lr: 0.000047  min_lr: 0.000000  loss: 4.3946 (4.3631)  loss_scale: 65536.0000 (63520.9168)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1290  max mem: 15572
Epoch: [6]  [1440/2809]  eta: 0:13:32  lr: 0.000047  min_lr: 0.000000  loss: 4.1865 (4.3619)  loss_scale: 65536.0000 (63534.9008)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1700  max mem: 15572
Epoch: [6]  [1450/2809]  eta: 0:13:26  lr: 0.000047  min_lr: 0.000000  loss: 4.1865 (4.3621)  loss_scale: 65536.0000 (63548.6919)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1492  max mem: 15572
Epoch: [6]  [1460/2809]  eta: 0:13:20  lr: 0.000047  min_lr: 0.000000  loss: 4.2302 (4.3614)  loss_scale: 65536.0000 (63562.2943)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1278  max mem: 15572
Epoch: [6]  [1470/2809]  eta: 0:13:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3710 (4.3626)  loss_scale: 65536.0000 (63575.7118)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1308  max mem: 15572
Epoch: [6]  [1480/2809]  eta: 0:13:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5073 (4.3629)  loss_scale: 65536.0000 (63588.9480)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0846  max mem: 15572
[2025-01-12 23:36:38,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:36:38,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1490/2809]  eta: 0:13:02  lr: 0.000047  min_lr: 0.000000  loss: 4.3600 (4.3625)  loss_scale: 65536.0000 (63645.9611)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1477  max mem: 15572
Epoch: [6]  [1500/2809]  eta: 0:12:56  lr: 0.000047  min_lr: 0.000000  loss: 4.2207 (4.3621)  loss_scale: 131072.0000 (64095.1686)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1439  max mem: 15572
Epoch: [6]  [1510/2809]  eta: 0:12:50  lr: 0.000047  min_lr: 0.000000  loss: 4.1784 (4.3617)  loss_scale: 131072.0000 (64538.4302)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1435  max mem: 15572
[2025-01-12 23:36:52,616] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18368
[2025-01-12 23:36:52,617] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:36:52,617] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1520/2809]  eta: 0:12:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3291 (4.3618)  loss_scale: 131072.0000 (64674.2512)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1730  max mem: 15572
Epoch: [6]  [1530/2809]  eta: 0:12:38  lr: 0.000047  min_lr: 0.000000  loss: 4.3780 (4.3629)  loss_scale: 65536.0000 (64679.8798)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1659  max mem: 15572
Epoch: [6]  [1540/2809]  eta: 0:12:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5362 (4.3632)  loss_scale: 65536.0000 (64685.4354)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1335  max mem: 15572
Epoch: [6]  [1550/2809]  eta: 0:12:26  lr: 0.000047  min_lr: 0.000000  loss: 4.4710 (4.3635)  loss_scale: 65536.0000 (64690.9194)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1529  max mem: 15572
Epoch: [6]  [1560/2809]  eta: 0:12:20  lr: 0.000047  min_lr: 0.000000  loss: 4.3873 (4.3632)  loss_scale: 65536.0000 (64696.3331)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1586  max mem: 15572
Epoch: [6]  [1570/2809]  eta: 0:12:14  lr: 0.000047  min_lr: 0.000000  loss: 4.2761 (4.3630)  loss_scale: 65536.0000 (64701.6779)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1207  max mem: 15572
Epoch: [6]  [1580/2809]  eta: 0:12:09  lr: 0.000047  min_lr: 0.000000  loss: 4.4546 (4.3631)  loss_scale: 65536.0000 (64706.9551)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1480  max mem: 15572
Epoch: [6]  [1590/2809]  eta: 0:12:02  lr: 0.000047  min_lr: 0.000000  loss: 4.4793 (4.3625)  loss_scale: 65536.0000 (64712.1659)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1277  max mem: 15572
Epoch: [6]  [1600/2809]  eta: 0:11:55  lr: 0.000047  min_lr: 0.000000  loss: 4.3598 (4.3625)  loss_scale: 65536.0000 (64717.3117)  weight_decay: 0.0500 (0.0500)  time: 0.4427  data: 0.0405  max mem: 15572
Epoch: [6]  [1610/2809]  eta: 0:11:48  lr: 0.000047  min_lr: 0.000000  loss: 4.3705 (4.3622)  loss_scale: 65536.0000 (64722.3935)  weight_decay: 0.0500 (0.0500)  time: 0.4170  data: 0.0006  max mem: 15572
Epoch: [6]  [1620/2809]  eta: 0:11:41  lr: 0.000047  min_lr: 0.000000  loss: 4.4237 (4.3630)  loss_scale: 65536.0000 (64727.4127)  weight_decay: 0.0500 (0.0500)  time: 0.4445  data: 0.0008  max mem: 15572
Epoch: [6]  [1630/2809]  eta: 0:11:36  lr: 0.000047  min_lr: 0.000000  loss: 4.4237 (4.3627)  loss_scale: 65536.0000 (64732.3703)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1186  max mem: 15572
Epoch: [6]  [1640/2809]  eta: 0:11:31  lr: 0.000047  min_lr: 0.000000  loss: 4.3189 (4.3625)  loss_scale: 65536.0000 (64737.2675)  weight_decay: 0.0500 (0.0500)  time: 0.7244  data: 0.2579  max mem: 15572
[2025-01-12 23:38:06,036] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:38:06,037] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:38:10,996] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18502
[2025-01-12 23:38:10,998] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:38:10,999] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1650/2809]  eta: 0:11:26  lr: 0.000047  min_lr: 0.000000  loss: 4.3045 (4.3621)  loss_scale: 65536.0000 (64940.5790)  weight_decay: 0.0500 (0.0500)  time: 0.7275  data: 0.2636  max mem: 15572
Epoch: [6]  [1660/2809]  eta: 0:11:21  lr: 0.000047  min_lr: 0.000000  loss: 4.2819 (4.3618)  loss_scale: 65536.0000 (64944.1638)  weight_decay: 0.0500 (0.0500)  time: 0.7406  data: 0.2414  max mem: 15572
Epoch: [6]  [1670/2809]  eta: 0:11:15  lr: 0.000047  min_lr: 0.000000  loss: 4.2771 (4.3613)  loss_scale: 65536.0000 (64947.7056)  weight_decay: 0.0500 (0.0500)  time: 0.6765  data: 0.1635  max mem: 15572
Epoch: [6]  [1680/2809]  eta: 0:11:11  lr: 0.000047  min_lr: 0.000000  loss: 4.4008 (4.3614)  loss_scale: 65536.0000 (64951.2052)  weight_decay: 0.0500 (0.0500)  time: 0.6933  data: 0.1896  max mem: 15572
Epoch: [6]  [1690/2809]  eta: 0:11:05  lr: 0.000047  min_lr: 0.000000  loss: 4.3492 (4.3603)  loss_scale: 65536.0000 (64954.6635)  weight_decay: 0.0500 (0.0500)  time: 0.7371  data: 0.2327  max mem: 15572
Epoch: [6]  [1700/2809]  eta: 0:11:01  lr: 0.000047  min_lr: 0.000000  loss: 4.2749 (4.3602)  loss_scale: 65536.0000 (64958.0811)  weight_decay: 0.0500 (0.0500)  time: 0.7188  data: 0.2237  max mem: 15572
Epoch: [6]  [1710/2809]  eta: 0:10:55  lr: 0.000047  min_lr: 0.000000  loss: 4.2654 (4.3597)  loss_scale: 65536.0000 (64961.4588)  weight_decay: 0.0500 (0.0500)  time: 0.6797  data: 0.2068  max mem: 15572
Epoch: [6]  [1720/2809]  eta: 0:10:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3423 (4.3601)  loss_scale: 65536.0000 (64964.7972)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1229  max mem: 15572
Epoch: [6]  [1730/2809]  eta: 0:10:43  lr: 0.000047  min_lr: 0.000000  loss: 4.3732 (4.3598)  loss_scale: 65536.0000 (64968.0971)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1360  max mem: 15572
Epoch: [6]  [1740/2809]  eta: 0:10:36  lr: 0.000047  min_lr: 0.000000  loss: 4.3270 (4.3593)  loss_scale: 65536.0000 (64971.3590)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1123  max mem: 15572
Epoch: [6]  [1750/2809]  eta: 0:10:29  lr: 0.000047  min_lr: 0.000000  loss: 4.3768 (4.3594)  loss_scale: 65536.0000 (64974.5837)  weight_decay: 0.0500 (0.0500)  time: 0.4443  data: 0.0274  max mem: 15572
Epoch: [6]  [1760/2809]  eta: 0:10:23  lr: 0.000047  min_lr: 0.000000  loss: 4.3768 (4.3595)  loss_scale: 65536.0000 (64977.7717)  weight_decay: 0.0500 (0.0500)  time: 0.4563  data: 0.0007  max mem: 15572
Epoch: [6]  [1770/2809]  eta: 0:10:16  lr: 0.000047  min_lr: 0.000000  loss: 4.2362 (4.3592)  loss_scale: 65536.0000 (64980.9238)  weight_decay: 0.0500 (0.0500)  time: 0.4699  data: 0.0007  max mem: 15572
[2025-01-12 23:39:28,226] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:39:28,226] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1780/2809]  eta: 0:10:09  lr: 0.000047  min_lr: 0.000000  loss: 4.2321 (4.3584)  loss_scale: 65536.0000 (65131.2296)  weight_decay: 0.0500 (0.0500)  time: 0.4609  data: 0.0008  max mem: 15572
[2025-01-12 23:39:30,832] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18637
[2025-01-12 23:39:30,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:39:30,833] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1790/2809]  eta: 0:10:03  lr: 0.000047  min_lr: 0.000000  loss: 4.3475 (4.3589)  loss_scale: 65536.0000 (65206.6734)  weight_decay: 0.0500 (0.0500)  time: 0.4798  data: 0.0165  max mem: 15572
Epoch: [6]  [1800/2809]  eta: 0:09:57  lr: 0.000047  min_lr: 0.000000  loss: 4.2182 (4.3576)  loss_scale: 65536.0000 (65208.5019)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0830  max mem: 15572
Epoch: [6]  [1810/2809]  eta: 0:09:51  lr: 0.000047  min_lr: 0.000000  loss: 4.2011 (4.3572)  loss_scale: 65536.0000 (65210.3103)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1696  max mem: 15572
Epoch: [6]  [1820/2809]  eta: 0:09:45  lr: 0.000047  min_lr: 0.000000  loss: 4.2011 (4.3555)  loss_scale: 65536.0000 (65212.0988)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1978  max mem: 15572
Epoch: [6]  [1830/2809]  eta: 0:09:40  lr: 0.000047  min_lr: 0.000000  loss: 4.2661 (4.3558)  loss_scale: 65536.0000 (65213.8678)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1803  max mem: 15572
Epoch: [6]  [1840/2809]  eta: 0:09:33  lr: 0.000047  min_lr: 0.000000  loss: 4.4697 (4.3565)  loss_scale: 65536.0000 (65215.6176)  weight_decay: 0.0500 (0.0500)  time: 0.5231  data: 0.0959  max mem: 15572
Epoch: [6]  [1850/2809]  eta: 0:09:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4931 (4.3571)  loss_scale: 65536.0000 (65217.3485)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.1065  max mem: 15572
Epoch: [6]  [1860/2809]  eta: 0:09:21  lr: 0.000047  min_lr: 0.000000  loss: 4.1157 (4.3560)  loss_scale: 65536.0000 (65219.0607)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1770  max mem: 15572
Epoch: [6]  [1870/2809]  eta: 0:09:15  lr: 0.000047  min_lr: 0.000000  loss: 4.1157 (4.3553)  loss_scale: 65536.0000 (65220.7547)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1085  max mem: 15572
Epoch: [6]  [1880/2809]  eta: 0:09:10  lr: 0.000047  min_lr: 0.000000  loss: 4.2076 (4.3551)  loss_scale: 65536.0000 (65222.4306)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1360  max mem: 15572
Epoch: [6]  [1890/2809]  eta: 0:09:04  lr: 0.000047  min_lr: 0.000000  loss: 4.3382 (4.3551)  loss_scale: 65536.0000 (65224.0888)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.1554  max mem: 15572
Epoch: [6]  [1900/2809]  eta: 0:08:58  lr: 0.000047  min_lr: 0.000000  loss: 4.3382 (4.3545)  loss_scale: 65536.0000 (65225.7296)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0786  max mem: 15572
Epoch: [6]  [1910/2809]  eta: 0:08:52  lr: 0.000047  min_lr: 0.000000  loss: 4.3524 (4.3549)  loss_scale: 65536.0000 (65227.3532)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0806  max mem: 15572
[2025-01-12 23:40:46,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:40:46,003] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1920/2809]  eta: 0:08:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4164 (4.3553)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1449  max mem: 15572
[2025-01-12 23:40:52,223] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18776
[2025-01-12 23:40:52,223] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:40:52,223] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1930/2809]  eta: 0:08:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4695 (4.3558)  loss_scale: 65536.0000 (65569.9389)  weight_decay: 0.0500 (0.0500)  time: 0.6542  data: 0.2330  max mem: 15572
Epoch: [6]  [1940/2809]  eta: 0:08:34  lr: 0.000047  min_lr: 0.000000  loss: 4.3737 (4.3557)  loss_scale: 65536.0000 (65569.7640)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1726  max mem: 15572
Epoch: [6]  [1950/2809]  eta: 0:08:28  lr: 0.000047  min_lr: 0.000000  loss: 4.3428 (4.3563)  loss_scale: 65536.0000 (65569.5910)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1506  max mem: 15572
Epoch: [6]  [1960/2809]  eta: 0:08:22  lr: 0.000047  min_lr: 0.000000  loss: 4.5648 (4.3565)  loss_scale: 65536.0000 (65569.4197)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1853  max mem: 15572
Epoch: [6]  [1970/2809]  eta: 0:08:17  lr: 0.000047  min_lr: 0.000000  loss: 4.3228 (4.3564)  loss_scale: 65536.0000 (65569.2501)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1963  max mem: 15572
Epoch: [6]  [1980/2809]  eta: 0:08:11  lr: 0.000047  min_lr: 0.000000  loss: 4.3228 (4.3562)  loss_scale: 65536.0000 (65569.0823)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1855  max mem: 15572
Epoch: [6]  [1990/2809]  eta: 0:08:05  lr: 0.000047  min_lr: 0.000000  loss: 4.3068 (4.3557)  loss_scale: 65536.0000 (65568.9161)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1509  max mem: 15572
Epoch: [6]  [2000/2809]  eta: 0:07:59  lr: 0.000047  min_lr: 0.000000  loss: 4.2449 (4.3552)  loss_scale: 65536.0000 (65568.7516)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1218  max mem: 15572
Epoch: [6]  [2010/2809]  eta: 0:07:53  lr: 0.000047  min_lr: 0.000000  loss: 4.3604 (4.3560)  loss_scale: 65536.0000 (65568.5888)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0913  max mem: 15572
Epoch: [6]  [2020/2809]  eta: 0:07:47  lr: 0.000047  min_lr: 0.000000  loss: 4.4347 (4.3561)  loss_scale: 65536.0000 (65568.4275)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1292  max mem: 15572
Epoch: [6]  [2030/2809]  eta: 0:07:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4260 (4.3569)  loss_scale: 65536.0000 (65568.2678)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1212  max mem: 15572
Epoch: [6]  [2040/2809]  eta: 0:07:34  lr: 0.000047  min_lr: 0.000000  loss: 4.5613 (4.3578)  loss_scale: 65536.0000 (65568.1098)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0886  max mem: 15572
Epoch: [6]  [2050/2809]  eta: 0:07:28  lr: 0.000047  min_lr: 0.000000  loss: 4.3049 (4.3569)  loss_scale: 65536.0000 (65567.9532)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1018  max mem: 15572
[2025-01-12 23:42:07,666] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:42:07,666] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:42:09,968] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18907
[2025-01-12 23:42:09,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:42:09,968] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2060/2809]  eta: 0:07:23  lr: 0.000047  min_lr: 0.000000  loss: 4.1908 (4.3563)  loss_scale: 65536.0000 (65631.3945)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1358  max mem: 15572
Epoch: [6]  [2070/2809]  eta: 0:07:17  lr: 0.000047  min_lr: 0.000000  loss: 4.3348 (4.3562)  loss_scale: 65536.0000 (65630.9338)  weight_decay: 0.0500 (0.0500)  time: 0.6814  data: 0.2143  max mem: 15572
Epoch: [6]  [2080/2809]  eta: 0:07:11  lr: 0.000047  min_lr: 0.000000  loss: 4.3058 (4.3555)  loss_scale: 65536.0000 (65630.4777)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1513  max mem: 15572
Epoch: [6]  [2090/2809]  eta: 0:07:05  lr: 0.000047  min_lr: 0.000000  loss: 4.2560 (4.3551)  loss_scale: 65536.0000 (65630.0258)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1044  max mem: 15572
Epoch: [6]  [2100/2809]  eta: 0:06:59  lr: 0.000047  min_lr: 0.000000  loss: 4.3756 (4.3549)  loss_scale: 65536.0000 (65629.5783)  weight_decay: 0.0500 (0.0500)  time: 0.6458  data: 0.1955  max mem: 15572
Epoch: [6]  [2110/2809]  eta: 0:06:54  lr: 0.000047  min_lr: 0.000000  loss: 4.4044 (4.3550)  loss_scale: 65536.0000 (65629.1350)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.2029  max mem: 15572
Epoch: [6]  [2120/2809]  eta: 0:06:48  lr: 0.000047  min_lr: 0.000000  loss: 4.3609 (4.3552)  loss_scale: 65536.0000 (65628.6959)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1734  max mem: 15572
Epoch: [6]  [2130/2809]  eta: 0:06:42  lr: 0.000047  min_lr: 0.000000  loss: 4.2472 (4.3547)  loss_scale: 65536.0000 (65628.2609)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1110  max mem: 15572
Epoch: [6]  [2140/2809]  eta: 0:06:36  lr: 0.000047  min_lr: 0.000000  loss: 4.2214 (4.3537)  loss_scale: 65536.0000 (65627.8300)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0879  max mem: 15572
[2025-01-12 23:43:05,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=118, lr=[4.5133071296510653e-07, 4.5133071296510653e-07, 6.447581613787237e-07, 6.447581613787237e-07, 9.21083087683891e-07, 9.21083087683891e-07, 1.3158329824055587e-06, 1.3158329824055587e-06, 1.8797614034365126e-06, 1.8797614034365126e-06, 2.685373433480732e-06, 2.685373433480732e-06, 3.836247762115332e-06, 3.836247762115332e-06, 5.480353945879047e-06, 5.480353945879047e-06, 7.829077065541495e-06, 7.829077065541495e-06, 1.1184395807916422e-05, 1.1184395807916422e-05, 1.597770829702346e-05, 1.597770829702346e-05, 2.2825297567176373e-05, 2.2825297567176373e-05, 3.2607567953109105e-05, 3.2607567953109105e-05, 4.6582239933013015e-05, 4.6582239933013015e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:43:05,442] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=27.890263956586058, CurrSamplesPerSec=31.47317395284619, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [2150/2809]  eta: 0:06:30  lr: 0.000047  min_lr: 0.000000  loss: 4.1667 (4.3533)  loss_scale: 65536.0000 (65627.4031)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1110  max mem: 15572
Epoch: [6]  [2160/2809]  eta: 0:06:24  lr: 0.000047  min_lr: 0.000000  loss: 4.3595 (4.3537)  loss_scale: 65536.0000 (65626.9801)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1012  max mem: 15572
Epoch: [6]  [2170/2809]  eta: 0:06:18  lr: 0.000047  min_lr: 0.000000  loss: 4.3595 (4.3534)  loss_scale: 65536.0000 (65626.5610)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1343  max mem: 15572
Epoch: [6]  [2180/2809]  eta: 0:06:12  lr: 0.000047  min_lr: 0.000000  loss: 4.2042 (4.3527)  loss_scale: 65536.0000 (65626.1458)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1484  max mem: 15572
[2025-01-12 23:43:25,930] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:43:25,931] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:43:27,969] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19038
[2025-01-12 23:43:27,971] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:43:27,973] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2190/2809]  eta: 0:06:06  lr: 0.000047  min_lr: 0.000000  loss: 4.2042 (4.3525)  loss_scale: 65536.0000 (65685.5573)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0961  max mem: 15572
Epoch: [6]  [2200/2809]  eta: 0:06:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4406 (4.3529)  loss_scale: 65536.0000 (65684.8778)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1148  max mem: 15572
Epoch: [6]  [2210/2809]  eta: 0:05:54  lr: 0.000047  min_lr: 0.000000  loss: 4.3971 (4.3529)  loss_scale: 65536.0000 (65684.2044)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1051  max mem: 15572
Epoch: [6]  [2220/2809]  eta: 0:05:48  lr: 0.000047  min_lr: 0.000000  loss: 4.3787 (4.3530)  loss_scale: 65536.0000 (65683.5371)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1248  max mem: 15572
Epoch: [6]  [2230/2809]  eta: 0:05:42  lr: 0.000047  min_lr: 0.000000  loss: 4.4448 (4.3538)  loss_scale: 65536.0000 (65682.8758)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1589  max mem: 15572
Epoch: [6]  [2240/2809]  eta: 0:05:36  lr: 0.000047  min_lr: 0.000000  loss: 4.4448 (4.3530)  loss_scale: 65536.0000 (65682.2204)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1229  max mem: 15572
Epoch: [6]  [2250/2809]  eta: 0:05:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4710 (4.3539)  loss_scale: 65536.0000 (65681.5709)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1244  max mem: 15572
Epoch: [6]  [2260/2809]  eta: 0:05:24  lr: 0.000047  min_lr: 0.000000  loss: 4.5240 (4.3545)  loss_scale: 65536.0000 (65680.9270)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1265  max mem: 15572
Epoch: [6]  [2270/2809]  eta: 0:05:19  lr: 0.000047  min_lr: 0.000000  loss: 4.3338 (4.3538)  loss_scale: 65536.0000 (65680.2889)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1772  max mem: 15572
Epoch: [6]  [2280/2809]  eta: 0:05:13  lr: 0.000047  min_lr: 0.000000  loss: 4.1821 (4.3534)  loss_scale: 65536.0000 (65679.6563)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1661  max mem: 15572
Epoch: [6]  [2290/2809]  eta: 0:05:06  lr: 0.000047  min_lr: 0.000000  loss: 4.2193 (4.3531)  loss_scale: 65536.0000 (65679.0292)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0653  max mem: 15572
Epoch: [6]  [2300/2809]  eta: 0:05:01  lr: 0.000047  min_lr: 0.000000  loss: 4.3757 (4.3531)  loss_scale: 65536.0000 (65678.4076)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1042  max mem: 15572
Epoch: [6]  [2310/2809]  eta: 0:04:55  lr: 0.000047  min_lr: 0.000000  loss: 4.3461 (4.3526)  loss_scale: 65536.0000 (65677.7914)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1578  max mem: 15572
[2025-01-12 23:44:42,960] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:44:42,960] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:44:45,742] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19173
[2025-01-12 23:44:45,742] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:44:45,742] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2320/2809]  eta: 0:04:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3504 (4.3527)  loss_scale: 65536.0000 (65846.5972)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1333  max mem: 15572
Epoch: [6]  [2330/2809]  eta: 0:04:43  lr: 0.000047  min_lr: 0.000000  loss: 4.3755 (4.3528)  loss_scale: 65536.0000 (65845.2647)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1223  max mem: 15572
Epoch: [6]  [2340/2809]  eta: 0:04:37  lr: 0.000047  min_lr: 0.000000  loss: 4.3485 (4.3529)  loss_scale: 65536.0000 (65843.9436)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1526  max mem: 15572
Epoch: [6]  [2350/2809]  eta: 0:04:31  lr: 0.000047  min_lr: 0.000000  loss: 4.5357 (4.3534)  loss_scale: 65536.0000 (65842.6338)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1655  max mem: 15572
Epoch: [6]  [2360/2809]  eta: 0:04:25  lr: 0.000047  min_lr: 0.000000  loss: 4.4768 (4.3538)  loss_scale: 65536.0000 (65841.3350)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1348  max mem: 15572
Epoch: [6]  [2370/2809]  eta: 0:04:19  lr: 0.000047  min_lr: 0.000000  loss: 4.4031 (4.3540)  loss_scale: 65536.0000 (65840.0472)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1513  max mem: 15572
Epoch: [6]  [2380/2809]  eta: 0:04:13  lr: 0.000047  min_lr: 0.000000  loss: 4.3870 (4.3538)  loss_scale: 65536.0000 (65838.7703)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1125  max mem: 15572
Epoch: [6]  [2390/2809]  eta: 0:04:07  lr: 0.000047  min_lr: 0.000000  loss: 4.1313 (4.3534)  loss_scale: 65536.0000 (65837.5040)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0690  max mem: 15572
Epoch: [6]  [2400/2809]  eta: 0:04:01  lr: 0.000047  min_lr: 0.000000  loss: 4.2045 (4.3527)  loss_scale: 65536.0000 (65836.2482)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1280  max mem: 15572
Epoch: [6]  [2410/2809]  eta: 0:03:55  lr: 0.000047  min_lr: 0.000000  loss: 4.2202 (4.3525)  loss_scale: 65536.0000 (65835.0029)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1119  max mem: 15572
Epoch: [6]  [2420/2809]  eta: 0:03:49  lr: 0.000047  min_lr: 0.000000  loss: 4.2855 (4.3520)  loss_scale: 65536.0000 (65833.7679)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0538  max mem: 15572
Epoch: [6]  [2430/2809]  eta: 0:03:43  lr: 0.000047  min_lr: 0.000000  loss: 4.3757 (4.3517)  loss_scale: 65536.0000 (65832.5430)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1037  max mem: 15572
Epoch: [6]  [2440/2809]  eta: 0:03:38  lr: 0.000047  min_lr: 0.000000  loss: 4.4008 (4.3517)  loss_scale: 65536.0000 (65831.3281)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1682  max mem: 15572
[2025-01-12 23:46:01,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:46:01,578] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [2450/2809]  eta: 0:03:32  lr: 0.000047  min_lr: 0.000000  loss: 4.3025 (4.3510)  loss_scale: 65536.0000 (65910.3386)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1562  max mem: 15572
Epoch: [6]  [2460/2809]  eta: 0:03:26  lr: 0.000047  min_lr: 0.000000  loss: 4.2816 (4.3511)  loss_scale: 131072.0000 (66175.1158)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1669  max mem: 15572
Epoch: [6]  [2470/2809]  eta: 0:03:20  lr: 0.000047  min_lr: 0.000000  loss: 4.3572 (4.3511)  loss_scale: 131072.0000 (66437.7499)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1777  max mem: 15572
[2025-01-12 23:46:17,317] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19329
[2025-01-12 23:46:17,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:46:17,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2480/2809]  eta: 0:03:14  lr: 0.000047  min_lr: 0.000000  loss: 4.2824 (4.3509)  loss_scale: 131072.0000 (66539.7759)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1305  max mem: 15572
Epoch: [6]  [2490/2809]  eta: 0:03:08  lr: 0.000047  min_lr: 0.000000  loss: 4.2837 (4.3513)  loss_scale: 65536.0000 (66535.7463)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1559  max mem: 15572
Epoch: [6]  [2500/2809]  eta: 0:03:02  lr: 0.000047  min_lr: 0.000000  loss: 4.3761 (4.3516)  loss_scale: 65536.0000 (66531.7489)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1379  max mem: 15572
Epoch: [6]  [2510/2809]  eta: 0:02:56  lr: 0.000047  min_lr: 0.000000  loss: 4.4777 (4.3523)  loss_scale: 65536.0000 (66527.7834)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1445  max mem: 15572
Epoch: [6]  [2520/2809]  eta: 0:02:50  lr: 0.000047  min_lr: 0.000000  loss: 4.3907 (4.3519)  loss_scale: 65536.0000 (66523.8493)  weight_decay: 0.0500 (0.0500)  time: 0.6469  data: 0.1907  max mem: 15572
Epoch: [6]  [2530/2809]  eta: 0:02:45  lr: 0.000047  min_lr: 0.000000  loss: 4.1818 (4.3511)  loss_scale: 65536.0000 (66519.9463)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1474  max mem: 15572
Epoch: [6]  [2540/2809]  eta: 0:02:39  lr: 0.000047  min_lr: 0.000000  loss: 4.1818 (4.3502)  loss_scale: 65536.0000 (66516.0740)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1403  max mem: 15572
Epoch: [6]  [2550/2809]  eta: 0:02:33  lr: 0.000047  min_lr: 0.000000  loss: 4.2906 (4.3504)  loss_scale: 65536.0000 (66512.2321)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1710  max mem: 15572
Epoch: [6]  [2560/2809]  eta: 0:02:27  lr: 0.000047  min_lr: 0.000000  loss: 4.3755 (4.3503)  loss_scale: 65536.0000 (66508.4201)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.1641  max mem: 15572
Epoch: [6]  [2570/2809]  eta: 0:02:21  lr: 0.000047  min_lr: 0.000000  loss: 4.3755 (4.3509)  loss_scale: 65536.0000 (66504.6379)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0968  max mem: 15572
Epoch: [6]  [2580/2809]  eta: 0:02:15  lr: 0.000047  min_lr: 0.000000  loss: 4.3429 (4.3507)  loss_scale: 65536.0000 (66500.8849)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0420  max mem: 15572
Epoch: [6]  [2590/2809]  eta: 0:02:09  lr: 0.000047  min_lr: 0.000000  loss: 4.3429 (4.3510)  loss_scale: 65536.0000 (66497.1609)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1080  max mem: 15572
Epoch: [6]  [2600/2809]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4230 (4.3515)  loss_scale: 65536.0000 (66493.4656)  weight_decay: 0.0500 (0.0500)  time: 0.6765  data: 0.2119  max mem: 15572
[2025-01-12 23:47:36,021] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:47:36,021] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:47:37,772] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19462
[2025-01-12 23:47:37,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:47:37,773] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2610/2809]  eta: 0:01:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4618 (4.3516)  loss_scale: 65536.0000 (66590.1984)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1110  max mem: 15572
Epoch: [6]  [2620/2809]  eta: 0:01:51  lr: 0.000047  min_lr: 0.000000  loss: 4.4831 (4.3521)  loss_scale: 65536.0000 (66586.1763)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0690  max mem: 15572
Epoch: [6]  [2630/2809]  eta: 0:01:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3399 (4.3521)  loss_scale: 65536.0000 (66582.1847)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1466  max mem: 15572
Epoch: [6]  [2640/2809]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000000  loss: 4.3399 (4.3518)  loss_scale: 65536.0000 (66578.2234)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1062  max mem: 15572
Epoch: [6]  [2650/2809]  eta: 0:01:34  lr: 0.000047  min_lr: 0.000000  loss: 4.2748 (4.3511)  loss_scale: 65536.0000 (66574.2920)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0550  max mem: 15572
Epoch: [6]  [2660/2809]  eta: 0:01:28  lr: 0.000047  min_lr: 0.000000  loss: 4.2785 (4.3511)  loss_scale: 65536.0000 (66570.3901)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0804  max mem: 15572
Epoch: [6]  [2670/2809]  eta: 0:01:22  lr: 0.000047  min_lr: 0.000000  loss: 4.4296 (4.3511)  loss_scale: 65536.0000 (66566.5174)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1000  max mem: 15572
Epoch: [6]  [2680/2809]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4296 (4.3511)  loss_scale: 65536.0000 (66562.6736)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.1883  max mem: 15572
Epoch: [6]  [2690/2809]  eta: 0:01:10  lr: 0.000047  min_lr: 0.000000  loss: 4.2449 (4.3508)  loss_scale: 65536.0000 (66558.8584)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1422  max mem: 15572
Epoch: [6]  [2700/2809]  eta: 0:01:04  lr: 0.000047  min_lr: 0.000000  loss: 4.2168 (4.3506)  loss_scale: 65536.0000 (66555.0715)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0585  max mem: 15572
Epoch: [6]  [2710/2809]  eta: 0:00:58  lr: 0.000047  min_lr: 0.000000  loss: 4.2876 (4.3508)  loss_scale: 65536.0000 (66551.3124)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1159  max mem: 15572
Epoch: [6]  [2720/2809]  eta: 0:00:52  lr: 0.000047  min_lr: 0.000000  loss: 4.4485 (4.3509)  loss_scale: 65536.0000 (66547.5810)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1084  max mem: 15572
Epoch: [6]  [2730/2809]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.2509 (4.3503)  loss_scale: 65536.0000 (66543.8770)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1406  max mem: 15572
[2025-01-12 23:48:53,826] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:48:53,826] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [2740/2809]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000000  loss: 4.1773 (4.3498)  loss_scale: 65536.0000 (66635.8380)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1395  max mem: 15572
Epoch: [6]  [2750/2809]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.2523 (4.3501)  loss_scale: 131072.0000 (66870.0662)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1169  max mem: 15572
[2025-01-12 23:49:01,698] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19606
[2025-01-12 23:49:01,698] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:49:01,698] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2760/2809]  eta: 0:00:28  lr: 0.000047  min_lr: 0.000000  loss: 4.2937 (4.3499)  loss_scale: 131072.0000 (66888.9707)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1411  max mem: 15572
Epoch: [6]  [2770/2809]  eta: 0:00:23  lr: 0.000047  min_lr: 0.000000  loss: 4.2964 (4.3499)  loss_scale: 65536.0000 (66884.0881)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1345  max mem: 15572
Epoch: [6]  [2780/2809]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000000  loss: 4.3308 (4.3501)  loss_scale: 65536.0000 (66879.2406)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1534  max mem: 15572
Epoch: [6]  [2790/2809]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.2326 (4.3492)  loss_scale: 65536.0000 (66874.4278)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1472  max mem: 15572
Epoch: [6]  [2800/2809]  eta: 0:00:05  lr: 0.000046  min_lr: 0.000000  loss: 4.0912 (4.3485)  loss_scale: 65536.0000 (66869.6494)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1015  max mem: 15572
Epoch: [6]  [2808/2809]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000000  loss: 4.1123 (4.3481)  loss_scale: 65536.0000 (66865.8512)  weight_decay: 0.0500 (0.0500)  time: 0.4711  data: 0.0475  max mem: 15572
Epoch: [6] Total time: 0:27:40 (0.5910 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 4.1123 (4.3481)  loss_scale: 65536.0000 (66865.8512)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:27:59  loss: 0.6630 (0.6630)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.1746  data: 5.9460  max mem: 15572
Val:  [ 10/272]  eta: 0:03:55  loss: 4.2227 (3.5768)  acc1: 0.0000 (25.7576)  acc5: 16.6667 (32.3232)  time: 0.8970  data: 0.6757  max mem: 15572
Val:  [ 20/272]  eta: 0:02:21  loss: 3.7753 (3.5594)  acc1: 11.1111 (22.2222)  acc5: 33.3333 (39.1534)  time: 0.2820  data: 0.0750  max mem: 15572
Val:  [ 30/272]  eta: 0:01:50  loss: 3.7575 (3.6992)  acc1: 5.5556 (16.4875)  acc5: 38.8889 (37.8136)  time: 0.2121  data: 0.0190  max mem: 15572
Val:  [ 40/272]  eta: 0:01:41  loss: 3.6599 (3.6592)  acc1: 5.5556 (16.2602)  acc5: 38.8889 (40.2439)  time: 0.3042  data: 0.1007  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 3.5183 (3.5884)  acc1: 16.6667 (17.8649)  acc5: 44.4444 (42.8105)  time: 0.3462  data: 0.1394  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 2.2841 (3.4130)  acc1: 44.4444 (24.2259)  acc5: 72.2222 (46.8124)  time: 0.3264  data: 0.1296  max mem: 15572
Val:  [ 70/272]  eta: 0:01:18  loss: 2.4524 (3.2988)  acc1: 50.0000 (25.7433)  acc5: 77.7778 (51.4867)  time: 0.3235  data: 0.1385  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 3.0421 (3.3054)  acc1: 22.2222 (25.9259)  acc5: 72.2222 (51.0974)  time: 0.2676  data: 0.0856  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 4.2743 (3.4125)  acc1: 5.5556 (23.4432)  acc5: 16.6667 (47.3138)  time: 0.2739  data: 0.0953  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 4.2207 (3.4852)  acc1: 5.5556 (22.5523)  acc5: 22.2222 (45.5996)  time: 0.3256  data: 0.1519  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 4.1034 (3.5549)  acc1: 0.0000 (20.5205)  acc5: 27.7778 (43.8939)  time: 0.3048  data: 0.1278  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 4.1570 (3.5959)  acc1: 0.0000 (19.1460)  acc5: 22.2222 (43.2966)  time: 0.3045  data: 0.1181  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 3.7399 (3.5372)  acc1: 5.5556 (21.0772)  acc5: 50.0000 (44.8261)  time: 0.3535  data: 0.1672  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 3.4278 (3.5241)  acc1: 16.6667 (21.6706)  acc5: 50.0000 (44.8385)  time: 0.3767  data: 0.1940  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 3.6059 (3.5248)  acc1: 11.1111 (21.0817)  acc5: 44.4444 (45.1803)  time: 0.3507  data: 0.1639  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.5286 (3.5000)  acc1: 16.6667 (21.9117)  acc5: 55.5556 (46.6874)  time: 0.3125  data: 0.1222  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.5891 (3.5292)  acc1: 11.1111 (21.0526)  acc5: 55.5556 (46.0364)  time: 0.3257  data: 0.1404  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 3.6437 (3.5188)  acc1: 5.5556 (21.0252)  acc5: 44.4444 (46.7158)  time: 0.3503  data: 0.1665  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.7243 (3.5459)  acc1: 0.0000 (20.0989)  acc5: 44.4444 (45.8697)  time: 0.3202  data: 0.1266  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.4617 (3.5476)  acc1: 0.0000 (19.8176)  acc5: 44.4444 (46.4345)  time: 0.2753  data: 0.0792  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2742 (3.5568)  acc1: 5.5556 (19.9579)  acc5: 72.2222 (46.7351)  time: 0.2679  data: 0.0771  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.4925 (3.5513)  acc1: 11.1111 (20.3620)  acc5: 55.5556 (46.8829)  time: 0.3093  data: 0.1248  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.9661 (3.5165)  acc1: 50.0000 (21.9577)  acc5: 72.2222 (48.1722)  time: 0.3647  data: 0.1873  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.7923 (3.4919)  acc1: 44.4444 (22.4758)  acc5: 77.7778 (49.4698)  time: 0.3545  data: 0.1743  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2314 (3.5227)  acc1: 11.1111 (21.7353)  acc5: 66.6667 (48.7384)  time: 0.3180  data: 0.1301  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 2.9455 (3.4461)  acc1: 16.6667 (24.1805)  acc5: 77.7778 (50.3831)  time: 0.3085  data: 0.1188  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.2547 (3.4457)  acc1: 61.1111 (24.0672)  acc5: 77.7778 (50.2870)  time: 0.2531  data: 0.0696  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.2547 (3.4486)  acc1: 55.5556 (24.0631)  acc5: 77.7778 (50.2765)  time: 0.2439  data: 0.0695  max mem: 15572
Val: Total time: 0:01:30 (0.3329 s / it)
* Acc@1 24.063 Acc@5 50.276 loss 3.449
Accuracy of the network on the 4883 val videos: 24.1%
[2025-01-12 23:51:03,986] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-12 23:51:03,990] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 23:51:03,991] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 23:51:06,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 23:51:06,978] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 24.06%
Epoch: [7]  [   0/2809]  eta: 7:16:48  lr: 0.000046  min_lr: 0.000000  loss: 4.2309 (4.2309)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.3300  data: 8.8203  max mem: 15572
Epoch: [7]  [  10/2809]  eta: 1:05:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2127 (4.1673)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3965  data: 0.9382  max mem: 15572
Epoch: [7]  [  20/2809]  eta: 0:46:09  lr: 0.000046  min_lr: 0.000000  loss: 4.2007 (4.2504)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1195  max mem: 15572
Epoch: [7]  [  30/2809]  eta: 0:41:16  lr: 0.000046  min_lr: 0.000000  loss: 4.1984 (4.2724)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1718  max mem: 15572
Epoch: [7]  [  40/2809]  eta: 0:36:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2241 (4.3033)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1423  max mem: 15572
Epoch: [7]  [  50/2809]  eta: 0:34:18  lr: 0.000046  min_lr: 0.000000  loss: 4.1879 (4.2771)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0632  max mem: 15572
Epoch: [7]  [  60/2809]  eta: 0:33:14  lr: 0.000046  min_lr: 0.000000  loss: 4.1939 (4.2860)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1197  max mem: 15572
Epoch: [7]  [  70/2809]  eta: 0:32:23  lr: 0.000046  min_lr: 0.000000  loss: 4.3813 (4.3102)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1519  max mem: 15572
[2025-01-12 23:51:58,195] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:51:58,196] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:51:58,584] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19736
[2025-01-12 23:51:58,584] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:51:58,584] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [  80/2809]  eta: 0:31:35  lr: 0.000046  min_lr: 0.000000  loss: 4.3935 (4.3025)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1524  max mem: 15572
Epoch: [7]  [  90/2809]  eta: 0:30:29  lr: 0.000046  min_lr: 0.000000  loss: 4.3805 (4.3041)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0847  max mem: 15572
Epoch: [7]  [ 100/2809]  eta: 0:29:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2931 (4.2981)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0500  max mem: 15572
Epoch: [7]  [ 110/2809]  eta: 0:29:01  lr: 0.000046  min_lr: 0.000000  loss: 4.2379 (4.2901)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0731  max mem: 15572
Epoch: [7]  [ 120/2809]  eta: 0:28:49  lr: 0.000046  min_lr: 0.000000  loss: 4.1811 (4.2818)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1144  max mem: 15572
Epoch: [7]  [ 130/2809]  eta: 0:28:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2207 (4.2689)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1719  max mem: 15572
Epoch: [7]  [ 140/2809]  eta: 0:28:27  lr: 0.000046  min_lr: 0.000000  loss: 4.2452 (4.2584)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1667  max mem: 15572
Epoch: [7]  [ 150/2809]  eta: 0:28:13  lr: 0.000046  min_lr: 0.000000  loss: 4.2452 (4.2621)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1494  max mem: 15572
Epoch: [7]  [ 160/2809]  eta: 0:27:57  lr: 0.000046  min_lr: 0.000000  loss: 4.2739 (4.2660)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1338  max mem: 15572
Epoch: [7]  [ 170/2809]  eta: 0:27:52  lr: 0.000046  min_lr: 0.000000  loss: 4.2736 (4.2683)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1476  max mem: 15572
Epoch: [7]  [ 180/2809]  eta: 0:27:30  lr: 0.000046  min_lr: 0.000000  loss: 4.2984 (4.2686)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1218  max mem: 15572
Epoch: [7]  [ 190/2809]  eta: 0:27:07  lr: 0.000046  min_lr: 0.000000  loss: 4.2547 (4.2649)  loss_scale: 65536.0000 (65879.1204)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0688  max mem: 15572
Epoch: [7]  [ 200/2809]  eta: 0:27:05  lr: 0.000046  min_lr: 0.000000  loss: 4.2699 (4.2730)  loss_scale: 65536.0000 (65862.0498)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1227  max mem: 15572
[2025-01-12 23:53:14,267] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:53:14,267] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:53:15,159] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19867
[2025-01-12 23:53:15,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:53:15,159] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 210/2809]  eta: 0:26:54  lr: 0.000046  min_lr: 0.000000  loss: 4.3205 (4.2720)  loss_scale: 65536.0000 (66467.7915)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1579  max mem: 15572
Epoch: [7]  [ 220/2809]  eta: 0:26:54  lr: 0.000046  min_lr: 0.000000  loss: 4.3720 (4.2758)  loss_scale: 65536.0000 (66425.6290)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1782  max mem: 15572
Epoch: [7]  [ 230/2809]  eta: 0:26:49  lr: 0.000046  min_lr: 0.000000  loss: 4.4216 (4.2821)  loss_scale: 65536.0000 (66387.1169)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.2081  max mem: 15572
Epoch: [7]  [ 240/2809]  eta: 0:26:40  lr: 0.000046  min_lr: 0.000000  loss: 4.4717 (4.2832)  loss_scale: 65536.0000 (66351.8008)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1799  max mem: 15572
Epoch: [7]  [ 250/2809]  eta: 0:26:33  lr: 0.000046  min_lr: 0.000000  loss: 4.3584 (4.2764)  loss_scale: 65536.0000 (66319.2988)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1708  max mem: 15572
Epoch: [7]  [ 260/2809]  eta: 0:26:25  lr: 0.000046  min_lr: 0.000000  loss: 4.2711 (4.2820)  loss_scale: 65536.0000 (66289.2874)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1864  max mem: 15572
Epoch: [7]  [ 270/2809]  eta: 0:26:16  lr: 0.000046  min_lr: 0.000000  loss: 4.4105 (4.2905)  loss_scale: 65536.0000 (66261.4908)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1511  max mem: 15572
Epoch: [7]  [ 280/2809]  eta: 0:26:17  lr: 0.000046  min_lr: 0.000000  loss: 4.3587 (4.2875)  loss_scale: 65536.0000 (66235.6726)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.1928  max mem: 15572
Epoch: [7]  [ 290/2809]  eta: 0:26:01  lr: 0.000046  min_lr: 0.000000  loss: 4.2863 (4.2914)  loss_scale: 65536.0000 (66211.6289)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1655  max mem: 15572
Epoch: [7]  [ 300/2809]  eta: 0:25:54  lr: 0.000046  min_lr: 0.000000  loss: 4.2555 (4.2874)  loss_scale: 65536.0000 (66189.1827)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0997  max mem: 15572
Epoch: [7]  [ 310/2809]  eta: 0:25:44  lr: 0.000046  min_lr: 0.000000  loss: 4.2325 (4.2883)  loss_scale: 65536.0000 (66168.1801)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1350  max mem: 15572
Epoch: [7]  [ 320/2809]  eta: 0:25:30  lr: 0.000046  min_lr: 0.000000  loss: 4.4022 (4.2930)  loss_scale: 65536.0000 (66148.4860)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.1209  max mem: 15572
Epoch: [7]  [ 330/2809]  eta: 0:25:26  lr: 0.000046  min_lr: 0.000000  loss: 4.2661 (4.2885)  loss_scale: 65536.0000 (66129.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1443  max mem: 15572
[2025-01-12 23:54:32,293] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:54:32,294] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:54:33,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=125, lr=[4.500737936126402e-07, 4.500737936126402e-07, 6.429625623037718e-07, 6.429625623037718e-07, 9.185179461482454e-07, 9.185179461482454e-07, 1.3121684944974935e-06, 1.3121684944974935e-06, 1.8745264207107052e-06, 1.8745264207107052e-06, 2.677894886729579e-06, 2.677894886729579e-06, 3.825564123899399e-06, 3.825564123899399e-06, 5.46509160557057e-06, 5.46509160557057e-06, 7.807273722243671e-06, 7.807273722243671e-06, 1.1153248174633818e-05, 1.1153248174633818e-05, 1.593321167804831e-05, 1.593321167804831e-05, 2.2761730968640446e-05, 2.2761730968640446e-05, 3.2516758526629215e-05, 3.2516758526629215e-05, 4.645251218089888e-05, 4.645251218089888e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:54:33,496] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=27.883994607127796, CurrSamplesPerSec=31.344010581760163, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [ 340/2809]  eta: 0:25:20  lr: 0.000046  min_lr: 0.000000  loss: 4.2259 (4.2896)  loss_scale: 65536.0000 (67650.0645)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1818  max mem: 15572
[2025-01-12 23:54:38,299] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20006
[2025-01-12 23:54:38,299] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:54:38,300] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 350/2809]  eta: 0:25:09  lr: 0.000046  min_lr: 0.000000  loss: 4.2259 (4.2884)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1509  max mem: 15572
Epoch: [7]  [ 360/2809]  eta: 0:25:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2771 (4.2879)  loss_scale: 65536.0000 (67896.0222)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1840  max mem: 15572
Epoch: [7]  [ 370/2809]  eta: 0:25:00  lr: 0.000046  min_lr: 0.000000  loss: 4.5826 (4.2959)  loss_scale: 65536.0000 (67832.4097)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.1930  max mem: 15572
Epoch: [7]  [ 380/2809]  eta: 0:24:57  lr: 0.000046  min_lr: 0.000000  loss: 4.4241 (4.2967)  loss_scale: 65536.0000 (67772.1365)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1823  max mem: 15572
Epoch: [7]  [ 390/2809]  eta: 0:24:50  lr: 0.000046  min_lr: 0.000000  loss: 4.4007 (4.2993)  loss_scale: 65536.0000 (67714.9463)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1761  max mem: 15572
Epoch: [7]  [ 400/2809]  eta: 0:24:44  lr: 0.000046  min_lr: 0.000000  loss: 4.3556 (4.3000)  loss_scale: 65536.0000 (67660.6085)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1473  max mem: 15572
Epoch: [7]  [ 410/2809]  eta: 0:24:30  lr: 0.000046  min_lr: 0.000000  loss: 4.2884 (4.3025)  loss_scale: 65536.0000 (67608.9148)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1028  max mem: 15572
Epoch: [7]  [ 420/2809]  eta: 0:24:30  lr: 0.000046  min_lr: 0.000000  loss: 4.2532 (4.3017)  loss_scale: 65536.0000 (67559.6770)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1467  max mem: 15572
Epoch: [7]  [ 430/2809]  eta: 0:24:23  lr: 0.000046  min_lr: 0.000000  loss: 4.2532 (4.3065)  loss_scale: 65536.0000 (67512.7239)  weight_decay: 0.0500 (0.0500)  time: 0.6629  data: 0.1971  max mem: 15572
Epoch: [7]  [ 440/2809]  eta: 0:24:21  lr: 0.000046  min_lr: 0.000000  loss: 4.3730 (4.3061)  loss_scale: 65536.0000 (67467.9002)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.1785  max mem: 15572
Epoch: [7]  [ 450/2809]  eta: 0:24:07  lr: 0.000046  min_lr: 0.000000  loss: 4.3379 (4.3057)  loss_scale: 65536.0000 (67425.0643)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1122  max mem: 15572
Epoch: [7]  [ 460/2809]  eta: 0:24:03  lr: 0.000046  min_lr: 0.000000  loss: 4.2092 (4.3034)  loss_scale: 65536.0000 (67384.0868)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1102  max mem: 15572
Epoch: [7]  [ 470/2809]  eta: 0:23:55  lr: 0.000046  min_lr: 0.000000  loss: 4.2563 (4.3043)  loss_scale: 65536.0000 (67344.8493)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1676  max mem: 15572
[2025-01-12 23:55:57,145] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:55:57,145] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:55:58,658] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20138
[2025-01-12 23:55:58,659] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:55:58,659] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 480/2809]  eta: 0:23:48  lr: 0.000046  min_lr: 0.000000  loss: 4.2598 (4.3011)  loss_scale: 65536.0000 (67715.9917)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1163  max mem: 15572
Epoch: [7]  [ 490/2809]  eta: 0:23:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2804 (4.3007)  loss_scale: 65536.0000 (67671.5927)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1156  max mem: 15572
Epoch: [7]  [ 500/2809]  eta: 0:23:33  lr: 0.000046  min_lr: 0.000000  loss: 4.3213 (4.2987)  loss_scale: 65536.0000 (67628.9661)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1250  max mem: 15572
Epoch: [7]  [ 510/2809]  eta: 0:23:27  lr: 0.000046  min_lr: 0.000000  loss: 4.3213 (4.2972)  loss_scale: 65536.0000 (67588.0078)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1416  max mem: 15572
Epoch: [7]  [ 520/2809]  eta: 0:23:20  lr: 0.000046  min_lr: 0.000000  loss: 4.1031 (4.2902)  loss_scale: 65536.0000 (67548.6219)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1656  max mem: 15572
Epoch: [7]  [ 530/2809]  eta: 0:23:09  lr: 0.000046  min_lr: 0.000000  loss: 4.1254 (4.2895)  loss_scale: 65536.0000 (67510.7194)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.1062  max mem: 15572
Epoch: [7]  [ 540/2809]  eta: 0:23:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2261 (4.2879)  loss_scale: 65536.0000 (67474.2181)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0411  max mem: 15572
Epoch: [7]  [ 550/2809]  eta: 0:22:59  lr: 0.000046  min_lr: 0.000000  loss: 4.2063 (4.2873)  loss_scale: 65536.0000 (67439.0417)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1382  max mem: 15572
Epoch: [7]  [ 560/2809]  eta: 0:22:53  lr: 0.000046  min_lr: 0.000000  loss: 4.3411 (4.2887)  loss_scale: 65536.0000 (67405.1194)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.1901  max mem: 15572
Epoch: [7]  [ 570/2809]  eta: 0:22:44  lr: 0.000046  min_lr: 0.000000  loss: 4.2886 (4.2881)  loss_scale: 65536.0000 (67372.3853)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1220  max mem: 15572
Epoch: [7]  [ 580/2809]  eta: 0:22:37  lr: 0.000046  min_lr: 0.000000  loss: 4.4041 (4.2893)  loss_scale: 65536.0000 (67340.7780)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1052  max mem: 15572
Epoch: [7]  [ 590/2809]  eta: 0:22:30  lr: 0.000046  min_lr: 0.000000  loss: 4.4041 (4.2878)  loss_scale: 65536.0000 (67310.2403)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1126  max mem: 15572
Epoch: [7]  [ 600/2809]  eta: 0:22:24  lr: 0.000046  min_lr: 0.000000  loss: 4.2285 (4.2891)  loss_scale: 65536.0000 (67280.7188)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1364  max mem: 15572
[2025-01-12 23:57:14,814] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:57:14,815] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [ 610/2809]  eta: 0:22:16  lr: 0.000046  min_lr: 0.000000  loss: 4.3086 (4.2894)  loss_scale: 65536.0000 (68002.9853)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1464  max mem: 15572
[2025-01-12 23:57:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20276
[2025-01-12 23:57:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:57:19,834] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 620/2809]  eta: 0:22:07  lr: 0.000046  min_lr: 0.000000  loss: 4.2991 (4.2892)  loss_scale: 65536.0000 (68174.3253)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.1027  max mem: 15572
Epoch: [7]  [ 630/2809]  eta: 0:22:02  lr: 0.000046  min_lr: 0.000000  loss: 4.2869 (4.2924)  loss_scale: 65536.0000 (68132.5135)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1338  max mem: 15572
Epoch: [7]  [ 640/2809]  eta: 0:21:54  lr: 0.000046  min_lr: 0.000000  loss: 4.4047 (4.2928)  loss_scale: 65536.0000 (68092.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1414  max mem: 15572
Epoch: [7]  [ 650/2809]  eta: 0:21:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2987 (4.2931)  loss_scale: 65536.0000 (68052.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0704  max mem: 15572
Epoch: [7]  [ 660/2809]  eta: 0:21:39  lr: 0.000046  min_lr: 0.000000  loss: 4.3778 (4.2930)  loss_scale: 65536.0000 (68014.6687)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1006  max mem: 15572
Epoch: [7]  [ 670/2809]  eta: 0:21:30  lr: 0.000046  min_lr: 0.000000  loss: 4.3582 (4.2922)  loss_scale: 65536.0000 (67977.7288)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1026  max mem: 15572
Epoch: [7]  [ 680/2809]  eta: 0:21:24  lr: 0.000046  min_lr: 0.000000  loss: 4.2936 (4.2930)  loss_scale: 65536.0000 (67941.8737)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0983  max mem: 15572
Epoch: [7]  [ 690/2809]  eta: 0:21:18  lr: 0.000046  min_lr: 0.000000  loss: 4.1211 (4.2896)  loss_scale: 65536.0000 (67907.0564)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1345  max mem: 15572
Epoch: [7]  [ 700/2809]  eta: 0:21:14  lr: 0.000046  min_lr: 0.000000  loss: 4.0699 (4.2887)  loss_scale: 65536.0000 (67873.2325)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1597  max mem: 15572
Epoch: [7]  [ 710/2809]  eta: 0:21:05  lr: 0.000046  min_lr: 0.000000  loss: 4.1622 (4.2878)  loss_scale: 65536.0000 (67840.3601)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1422  max mem: 15572
Epoch: [7]  [ 720/2809]  eta: 0:21:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2297 (4.2884)  loss_scale: 65536.0000 (67808.3994)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1323  max mem: 15572
Epoch: [7]  [ 730/2809]  eta: 0:20:57  lr: 0.000046  min_lr: 0.000000  loss: 4.4219 (4.2895)  loss_scale: 65536.0000 (67777.3133)  weight_decay: 0.0500 (0.0500)  time: 0.6627  data: 0.2231  max mem: 15572
Epoch: [7]  [ 740/2809]  eta: 0:20:50  lr: 0.000046  min_lr: 0.000000  loss: 4.4533 (4.2905)  loss_scale: 65536.0000 (67747.0661)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.1967  max mem: 15572
[2025-01-12 23:58:36,013] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:58:36,014] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:58:36,819] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20407
[2025-01-12 23:58:36,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:58:36,820] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 750/2809]  eta: 0:20:45  lr: 0.000046  min_lr: 0.000000  loss: 4.0508 (4.2862)  loss_scale: 65536.0000 (67892.1545)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1692  max mem: 15572
Epoch: [7]  [ 760/2809]  eta: 0:20:37  lr: 0.000046  min_lr: 0.000000  loss: 4.3336 (4.2884)  loss_scale: 65536.0000 (67861.1932)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1354  max mem: 15572
Epoch: [7]  [ 770/2809]  eta: 0:20:33  lr: 0.000046  min_lr: 0.000000  loss: 4.3733 (4.2895)  loss_scale: 65536.0000 (67831.0350)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1209  max mem: 15572
Epoch: [7]  [ 780/2809]  eta: 0:20:29  lr: 0.000046  min_lr: 0.000000  loss: 4.1496 (4.2873)  loss_scale: 65536.0000 (67801.6492)  weight_decay: 0.0500 (0.0500)  time: 0.6664  data: 0.2010  max mem: 15572
Epoch: [7]  [ 790/2809]  eta: 0:20:21  lr: 0.000046  min_lr: 0.000000  loss: 4.0636 (4.2862)  loss_scale: 65536.0000 (67773.0063)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1666  max mem: 15572
Epoch: [7]  [ 800/2809]  eta: 0:20:13  lr: 0.000046  min_lr: 0.000000  loss: 4.2724 (4.2862)  loss_scale: 65536.0000 (67745.0787)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.0803  max mem: 15572
[2025-01-12 23:59:19,021] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20473
[2025-01-12 23:59:19,022] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 23:59:19,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 810/2809]  eta: 0:20:12  lr: 0.000046  min_lr: 0.000000  loss: 4.3207 (4.2863)  loss_scale: 65536.0000 (67677.4353)  weight_decay: 0.0500 (0.0500)  time: 0.6550  data: 0.1911  max mem: 15572
Epoch: [7]  [ 820/2809]  eta: 0:20:03  lr: 0.000046  min_lr: 0.000000  loss: 4.3207 (4.2855)  loss_scale: 32768.0000 (67252.2290)  weight_decay: 0.0500 (0.0500)  time: 0.6539  data: 0.2032  max mem: 15572
Epoch: [7]  [ 830/2809]  eta: 0:19:55  lr: 0.000046  min_lr: 0.000000  loss: 4.2010 (4.2854)  loss_scale: 32768.0000 (66837.2563)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0833  max mem: 15572
Epoch: [7]  [ 840/2809]  eta: 0:19:47  lr: 0.000046  min_lr: 0.000000  loss: 4.2005 (4.2852)  loss_scale: 32768.0000 (66432.1522)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0949  max mem: 15572
Epoch: [7]  [ 850/2809]  eta: 0:19:38  lr: 0.000046  min_lr: 0.000000  loss: 4.2294 (4.2845)  loss_scale: 32768.0000 (66036.5687)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0774  max mem: 15572
Epoch: [7]  [ 860/2809]  eta: 0:19:27  lr: 0.000046  min_lr: 0.000000  loss: 4.2428 (4.2831)  loss_scale: 32768.0000 (65650.1742)  weight_decay: 0.0500 (0.0500)  time: 0.4369  data: 0.0293  max mem: 15572
Epoch: [7]  [ 870/2809]  eta: 0:19:20  lr: 0.000046  min_lr: 0.000000  loss: 4.2316 (4.2821)  loss_scale: 32768.0000 (65272.6521)  weight_decay: 0.0500 (0.0500)  time: 0.4713  data: 0.0313  max mem: 15572
Epoch: [7]  [ 880/2809]  eta: 0:19:16  lr: 0.000046  min_lr: 0.000000  loss: 4.1040 (4.2795)  loss_scale: 32768.0000 (64903.7003)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1335  max mem: 15572
Epoch: [7]  [ 890/2809]  eta: 0:19:13  lr: 0.000046  min_lr: 0.000000  loss: 4.1288 (4.2786)  loss_scale: 32768.0000 (64543.0303)  weight_decay: 0.0500 (0.0500)  time: 0.7130  data: 0.2289  max mem: 15572
Epoch: [7]  [ 900/2809]  eta: 0:19:10  lr: 0.000046  min_lr: 0.000000  loss: 4.3083 (4.2784)  loss_scale: 32768.0000 (64190.3663)  weight_decay: 0.0500 (0.0500)  time: 0.7319  data: 0.2244  max mem: 15572
Epoch: [7]  [ 910/2809]  eta: 0:19:04  lr: 0.000046  min_lr: 0.000000  loss: 4.3401 (4.2798)  loss_scale: 32768.0000 (63845.4446)  weight_decay: 0.0500 (0.0500)  time: 0.6770  data: 0.1564  max mem: 15572
Epoch: [7]  [ 920/2809]  eta: 0:18:59  lr: 0.000046  min_lr: 0.000000  loss: 4.4199 (4.2807)  loss_scale: 32768.0000 (63508.0130)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.1427  max mem: 15572
Epoch: [7]  [ 930/2809]  eta: 0:18:57  lr: 0.000046  min_lr: 0.000000  loss: 4.4832 (4.2812)  loss_scale: 32768.0000 (63177.8303)  weight_decay: 0.0500 (0.0500)  time: 0.7128  data: 0.2116  max mem: 15572
[2025-01-13 00:00:35,343] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:00:35,343] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 940/2809]  eta: 0:18:55  lr: 0.000046  min_lr: 0.000000  loss: 4.4453 (4.2829)  loss_scale: 32768.0000 (62924.3103)  weight_decay: 0.0500 (0.0500)  time: 0.7987  data: 0.2925  max mem: 15572
Epoch: [7]  [ 950/2809]  eta: 0:18:49  lr: 0.000046  min_lr: 0.000000  loss: 4.4453 (4.2837)  loss_scale: 65536.0000 (62951.7729)  weight_decay: 0.0500 (0.0500)  time: 0.7091  data: 0.2255  max mem: 15572
Epoch: [7]  [ 960/2809]  eta: 0:18:44  lr: 0.000046  min_lr: 0.000000  loss: 4.4291 (4.2847)  loss_scale: 65536.0000 (62978.6639)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.1715  max mem: 15572
Epoch: [7]  [ 970/2809]  eta: 0:18:39  lr: 0.000046  min_lr: 0.000000  loss: 4.2607 (4.2848)  loss_scale: 65536.0000 (63005.0010)  weight_decay: 0.0500 (0.0500)  time: 0.6710  data: 0.1853  max mem: 15572
Epoch: [7]  [ 980/2809]  eta: 0:18:35  lr: 0.000046  min_lr: 0.000000  loss: 4.2339 (4.2853)  loss_scale: 65536.0000 (63030.8012)  weight_decay: 0.0500 (0.0500)  time: 0.6800  data: 0.2087  max mem: 15572
Epoch: [7]  [ 990/2809]  eta: 0:18:27  lr: 0.000046  min_lr: 0.000000  loss: 4.2902 (4.2860)  loss_scale: 65536.0000 (63056.0807)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1647  max mem: 15572
Epoch: [7]  [1000/2809]  eta: 0:18:16  lr: 0.000046  min_lr: 0.000000  loss: 4.2687 (4.2852)  loss_scale: 65536.0000 (63080.8551)  weight_decay: 0.0500 (0.0500)  time: 0.4339  data: 0.0304  max mem: 15572
Epoch: [7]  [1010/2809]  eta: 0:18:07  lr: 0.000046  min_lr: 0.000000  loss: 4.0261 (4.2826)  loss_scale: 65536.0000 (63105.1395)  weight_decay: 0.0500 (0.0500)  time: 0.3902  data: 0.0003  max mem: 15572
Epoch: [7]  [1020/2809]  eta: 0:17:59  lr: 0.000046  min_lr: 0.000000  loss: 4.2114 (4.2829)  loss_scale: 65536.0000 (63128.9481)  weight_decay: 0.0500 (0.0500)  time: 0.4475  data: 0.0256  max mem: 15572
Epoch: [7]  [1030/2809]  eta: 0:17:52  lr: 0.000046  min_lr: 0.000000  loss: 4.3460 (4.2835)  loss_scale: 65536.0000 (63152.2949)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1074  max mem: 15572
Epoch: [7]  [1040/2809]  eta: 0:17:47  lr: 0.000046  min_lr: 0.000000  loss: 4.3407 (4.2846)  loss_scale: 65536.0000 (63175.1931)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1918  max mem: 15572
Epoch: [7]  [1050/2809]  eta: 0:17:40  lr: 0.000046  min_lr: 0.000000  loss: 4.3369 (4.2845)  loss_scale: 65536.0000 (63197.6556)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1594  max mem: 15572
Epoch: [7]  [1060/2809]  eta: 0:17:33  lr: 0.000046  min_lr: 0.000000  loss: 4.2993 (4.2845)  loss_scale: 65536.0000 (63219.6946)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1075  max mem: 15572
[2025-01-13 00:01:49,762] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:01:49,763] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:01:50,601] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20732
[2025-01-13 00:01:50,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:01:50,602] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1070/2809]  eta: 0:17:27  lr: 0.000046  min_lr: 0.000000  loss: 4.1464 (4.2829)  loss_scale: 65536.0000 (63363.7049)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1346  max mem: 15572
Epoch: [7]  [1080/2809]  eta: 0:17:21  lr: 0.000046  min_lr: 0.000000  loss: 4.1464 (4.2828)  loss_scale: 65536.0000 (63383.8002)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1289  max mem: 15572
Epoch: [7]  [1090/2809]  eta: 0:17:16  lr: 0.000046  min_lr: 0.000000  loss: 4.3005 (4.2823)  loss_scale: 65536.0000 (63403.5270)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.1735  max mem: 15572
Epoch: [7]  [1100/2809]  eta: 0:17:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2131 (4.2816)  loss_scale: 65536.0000 (63422.8955)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1455  max mem: 15572
Epoch: [7]  [1110/2809]  eta: 0:17:03  lr: 0.000046  min_lr: 0.000000  loss: 4.2954 (4.2826)  loss_scale: 65536.0000 (63441.9154)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1366  max mem: 15572
Epoch: [7]  [1120/2809]  eta: 0:16:55  lr: 0.000046  min_lr: 0.000000  loss: 4.4668 (4.2837)  loss_scale: 65536.0000 (63460.5959)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1195  max mem: 15572
Epoch: [7]  [1130/2809]  eta: 0:16:50  lr: 0.000046  min_lr: 0.000000  loss: 4.3324 (4.2828)  loss_scale: 65536.0000 (63478.9461)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1196  max mem: 15572
Epoch: [7]  [1140/2809]  eta: 0:16:44  lr: 0.000046  min_lr: 0.000000  loss: 4.1145 (4.2813)  loss_scale: 65536.0000 (63496.9746)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.2006  max mem: 15572
Epoch: [7]  [1150/2809]  eta: 0:16:36  lr: 0.000046  min_lr: 0.000000  loss: 4.2153 (4.2815)  loss_scale: 65536.0000 (63514.6898)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0959  max mem: 15572
Epoch: [7]  [1160/2809]  eta: 0:16:30  lr: 0.000046  min_lr: 0.000000  loss: 4.3577 (4.2823)  loss_scale: 65536.0000 (63532.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0805  max mem: 15572
Epoch: [7]  [1170/2809]  eta: 0:16:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4180 (4.2837)  loss_scale: 65536.0000 (63549.2126)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1110  max mem: 15572
Epoch: [7]  [1180/2809]  eta: 0:16:17  lr: 0.000046  min_lr: 0.000000  loss: 4.4857 (4.2844)  loss_scale: 65536.0000 (63566.0356)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1253  max mem: 15572
Epoch: [7]  [1190/2809]  eta: 0:16:11  lr: 0.000046  min_lr: 0.000000  loss: 4.4121 (4.2838)  loss_scale: 65536.0000 (63582.5760)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1460  max mem: 15572
[2025-01-13 00:03:06,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:03:06,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1200/2809]  eta: 0:16:04  lr: 0.000046  min_lr: 0.000000  loss: 4.2485 (4.2849)  loss_scale: 65536.0000 (63762.5445)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1187  max mem: 15572
[2025-01-13 00:03:10,782] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20868
[2025-01-13 00:03:10,783] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:03:10,783] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1210/2809]  eta: 0:15:59  lr: 0.000046  min_lr: 0.000000  loss: 4.2892 (4.2833)  loss_scale: 65536.0000 (63993.6581)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1463  max mem: 15572
Epoch: [7]  [1220/2809]  eta: 0:15:53  lr: 0.000046  min_lr: 0.000000  loss: 4.3006 (4.2828)  loss_scale: 65536.0000 (64006.2899)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1674  max mem: 15572
Epoch: [7]  [1230/2809]  eta: 0:15:46  lr: 0.000046  min_lr: 0.000000  loss: 4.3535 (4.2830)  loss_scale: 65536.0000 (64018.7165)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1340  max mem: 15572
Epoch: [7]  [1240/2809]  eta: 0:15:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2186 (4.2827)  loss_scale: 65536.0000 (64030.9428)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1308  max mem: 15572
Epoch: [7]  [1250/2809]  eta: 0:15:34  lr: 0.000046  min_lr: 0.000000  loss: 4.2186 (4.2824)  loss_scale: 65536.0000 (64042.9736)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1287  max mem: 15572
Epoch: [7]  [1260/2809]  eta: 0:15:29  lr: 0.000046  min_lr: 0.000000  loss: 4.4308 (4.2841)  loss_scale: 65536.0000 (64054.8136)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1698  max mem: 15572
Epoch: [7]  [1270/2809]  eta: 0:15:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4054 (4.2834)  loss_scale: 65536.0000 (64066.4673)  weight_decay: 0.0500 (0.0500)  time: 0.6535  data: 0.1974  max mem: 15572
Epoch: [7]  [1280/2809]  eta: 0:15:17  lr: 0.000046  min_lr: 0.000000  loss: 4.1212 (4.2818)  loss_scale: 65536.0000 (64077.9391)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1357  max mem: 15572
Epoch: [7]  [1290/2809]  eta: 0:15:11  lr: 0.000046  min_lr: 0.000000  loss: 4.4339 (4.2829)  loss_scale: 65536.0000 (64089.2332)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1254  max mem: 15572
Epoch: [7]  [1300/2809]  eta: 0:15:04  lr: 0.000046  min_lr: 0.000000  loss: 4.4339 (4.2826)  loss_scale: 65536.0000 (64100.3536)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0856  max mem: 15572
Epoch: [7]  [1310/2809]  eta: 0:14:57  lr: 0.000046  min_lr: 0.000000  loss: 4.1939 (4.2813)  loss_scale: 65536.0000 (64111.3043)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0350  max mem: 15572
Epoch: [7]  [1320/2809]  eta: 0:14:52  lr: 0.000046  min_lr: 0.000000  loss: 4.1703 (4.2803)  loss_scale: 65536.0000 (64122.0893)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1188  max mem: 15572
Epoch: [7]  [1330/2809]  eta: 0:14:46  lr: 0.000046  min_lr: 0.000000  loss: 4.1925 (4.2805)  loss_scale: 65536.0000 (64132.7122)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1875  max mem: 15572
[2025-01-13 00:04:29,096] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:04:29,096] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:04:30,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=132, lr=[4.485894344622119e-07, 4.485894344622119e-07, 6.408420492317313e-07, 6.408420492317313e-07, 9.154886417596163e-07, 9.154886417596163e-07, 1.307840916799452e-06, 1.307840916799452e-06, 1.86834416685636e-06, 1.86834416685636e-06, 2.669063095509086e-06, 2.669063095509086e-06, 3.812947279298694e-06, 3.812947279298694e-06, 5.447067541855278e-06, 5.447067541855278e-06, 7.781525059793253e-06, 7.781525059793253e-06, 1.1116464371133221e-05, 1.1116464371133221e-05, 1.588066338733317e-05, 1.588066338733317e-05, 2.2686661981904537e-05, 2.2686661981904537e-05, 3.2409517117006484e-05, 3.2409517117006484e-05, 4.629931016715212e-05, 4.629931016715212e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 00:04:30,194] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=27.880816705021207, CurrSamplesPerSec=21.309614796762975, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [1340/2809]  eta: 0:14:41  lr: 0.000046  min_lr: 0.000000  loss: 4.2410 (4.2799)  loss_scale: 65536.0000 (64485.2737)  weight_decay: 0.0500 (0.0500)  time: 0.6561  data: 0.1988  max mem: 15572
[2025-01-13 00:04:37,665] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21012
[2025-01-13 00:04:37,666] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:04:37,666] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1350/2809]  eta: 0:14:35  lr: 0.000046  min_lr: 0.000000  loss: 4.2410 (4.2788)  loss_scale: 131072.0000 (64881.1251)  weight_decay: 0.0500 (0.0500)  time: 0.6555  data: 0.1780  max mem: 15572
Epoch: [7]  [1360/2809]  eta: 0:14:29  lr: 0.000046  min_lr: 0.000000  loss: 4.3062 (4.2783)  loss_scale: 65536.0000 (64885.9368)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1490  max mem: 15572
Epoch: [7]  [1370/2809]  eta: 0:14:23  lr: 0.000046  min_lr: 0.000000  loss: 4.3742 (4.2792)  loss_scale: 65536.0000 (64890.6783)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1744  max mem: 15572
Epoch: [7]  [1380/2809]  eta: 0:14:15  lr: 0.000046  min_lr: 0.000000  loss: 4.3756 (4.2796)  loss_scale: 65536.0000 (64895.3512)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1071  max mem: 15572
Epoch: [7]  [1390/2809]  eta: 0:14:08  lr: 0.000046  min_lr: 0.000000  loss: 4.3426 (4.2797)  loss_scale: 65536.0000 (64899.9569)  weight_decay: 0.0500 (0.0500)  time: 0.4741  data: 0.0373  max mem: 15572
Epoch: [7]  [1400/2809]  eta: 0:14:03  lr: 0.000046  min_lr: 0.000000  loss: 4.3426 (4.2790)  loss_scale: 65536.0000 (64904.4968)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1051  max mem: 15572
Epoch: [7]  [1410/2809]  eta: 0:13:57  lr: 0.000046  min_lr: 0.000000  loss: 4.2343 (4.2791)  loss_scale: 65536.0000 (64908.9724)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1657  max mem: 15572
Epoch: [7]  [1420/2809]  eta: 0:13:51  lr: 0.000046  min_lr: 0.000000  loss: 4.2343 (4.2792)  loss_scale: 65536.0000 (64913.3849)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1556  max mem: 15572
Epoch: [7]  [1430/2809]  eta: 0:13:45  lr: 0.000046  min_lr: 0.000000  loss: 4.1886 (4.2783)  loss_scale: 65536.0000 (64917.7358)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1353  max mem: 15572
Epoch: [7]  [1440/2809]  eta: 0:13:39  lr: 0.000046  min_lr: 0.000000  loss: 4.1886 (4.2779)  loss_scale: 65536.0000 (64922.0264)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1322  max mem: 15572
Epoch: [7]  [1450/2809]  eta: 0:13:33  lr: 0.000046  min_lr: 0.000000  loss: 4.2037 (4.2771)  loss_scale: 65536.0000 (64926.2578)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1292  max mem: 15572
Epoch: [7]  [1460/2809]  eta: 0:13:27  lr: 0.000046  min_lr: 0.000000  loss: 4.2423 (4.2760)  loss_scale: 65536.0000 (64930.4312)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1045  max mem: 15572
Epoch: [7]  [1470/2809]  eta: 0:13:20  lr: 0.000046  min_lr: 0.000000  loss: 4.2423 (4.2764)  loss_scale: 65536.0000 (64934.5479)  weight_decay: 0.0500 (0.0500)  time: 0.5241  data: 0.0712  max mem: 15572
[2025-01-13 00:05:51,987] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:05:51,987] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1480/2809]  eta: 0:13:16  lr: 0.000046  min_lr: 0.000000  loss: 4.2992 (4.2761)  loss_scale: 65536.0000 (65071.3626)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.1869  max mem: 15572
[2025-01-13 00:05:57,912] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21149
[2025-01-13 00:05:57,912] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:05:57,912] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1490/2809]  eta: 0:13:10  lr: 0.000046  min_lr: 0.000000  loss: 4.3255 (4.2763)  loss_scale: 65536.0000 (65294.2508)  weight_decay: 0.0500 (0.0500)  time: 0.6932  data: 0.2130  max mem: 15572
Epoch: [7]  [1500/2809]  eta: 0:13:02  lr: 0.000046  min_lr: 0.000000  loss: 4.3255 (4.2760)  loss_scale: 65536.0000 (65295.8614)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0518  max mem: 15572
Epoch: [7]  [1510/2809]  eta: 0:12:56  lr: 0.000046  min_lr: 0.000000  loss: 4.3900 (4.2774)  loss_scale: 65536.0000 (65297.4507)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0654  max mem: 15572
Epoch: [7]  [1520/2809]  eta: 0:12:50  lr: 0.000046  min_lr: 0.000000  loss: 4.2543 (4.2771)  loss_scale: 65536.0000 (65299.0191)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1258  max mem: 15572
Epoch: [7]  [1530/2809]  eta: 0:12:44  lr: 0.000046  min_lr: 0.000000  loss: 4.3730 (4.2783)  loss_scale: 65536.0000 (65300.5669)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1260  max mem: 15572
Epoch: [7]  [1540/2809]  eta: 0:12:38  lr: 0.000046  min_lr: 0.000000  loss: 4.4644 (4.2786)  loss_scale: 65536.0000 (65302.0947)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1355  max mem: 15572
Epoch: [7]  [1550/2809]  eta: 0:12:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4203 (4.2794)  loss_scale: 65536.0000 (65303.6028)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1866  max mem: 15572
Epoch: [7]  [1560/2809]  eta: 0:12:27  lr: 0.000046  min_lr: 0.000000  loss: 4.4415 (4.2798)  loss_scale: 65536.0000 (65305.0916)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.2134  max mem: 15572
Epoch: [7]  [1570/2809]  eta: 0:12:21  lr: 0.000046  min_lr: 0.000000  loss: 4.2846 (4.2802)  loss_scale: 65536.0000 (65306.5614)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.2118  max mem: 15572
Epoch: [7]  [1580/2809]  eta: 0:12:15  lr: 0.000046  min_lr: 0.000000  loss: 4.5009 (4.2813)  loss_scale: 65536.0000 (65308.0127)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.1636  max mem: 15572
Epoch: [7]  [1590/2809]  eta: 0:12:09  lr: 0.000046  min_lr: 0.000000  loss: 4.4511 (4.2813)  loss_scale: 65536.0000 (65309.4456)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1118  max mem: 15572
Epoch: [7]  [1600/2809]  eta: 0:12:03  lr: 0.000046  min_lr: 0.000000  loss: 4.3480 (4.2813)  loss_scale: 65536.0000 (65310.8607)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1364  max mem: 15572
Epoch: [7]  [1610/2809]  eta: 0:11:57  lr: 0.000046  min_lr: 0.000000  loss: 4.2650 (4.2821)  loss_scale: 65536.0000 (65312.2582)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1232  max mem: 15572
[2025-01-13 00:07:16,424] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:07:16,425] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:07:16,921] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21279
[2025-01-13 00:07:16,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:07:16,922] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1620/2809]  eta: 0:11:52  lr: 0.000046  min_lr: 0.000000  loss: 4.4357 (4.2832)  loss_scale: 65536.0000 (65354.0679)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1772  max mem: 15572
Epoch: [7]  [1630/2809]  eta: 0:11:46  lr: 0.000046  min_lr: 0.000000  loss: 4.3904 (4.2825)  loss_scale: 65536.0000 (65355.1833)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.2038  max mem: 15572
Epoch: [7]  [1640/2809]  eta: 0:11:39  lr: 0.000046  min_lr: 0.000000  loss: 4.3303 (4.2829)  loss_scale: 65536.0000 (65356.2852)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1401  max mem: 15572
Epoch: [7]  [1650/2809]  eta: 0:11:34  lr: 0.000046  min_lr: 0.000000  loss: 4.0504 (4.2812)  loss_scale: 65536.0000 (65357.3737)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1523  max mem: 15572
Epoch: [7]  [1660/2809]  eta: 0:11:28  lr: 0.000046  min_lr: 0.000000  loss: 4.0501 (4.2801)  loss_scale: 65536.0000 (65358.4491)  weight_decay: 0.0500 (0.0500)  time: 0.6522  data: 0.1885  max mem: 15572
Epoch: [7]  [1670/2809]  eta: 0:11:22  lr: 0.000046  min_lr: 0.000000  loss: 4.1732 (4.2793)  loss_scale: 65536.0000 (65359.5117)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1323  max mem: 15572
Epoch: [7]  [1680/2809]  eta: 0:11:17  lr: 0.000046  min_lr: 0.000000  loss: 4.2349 (4.2798)  loss_scale: 65536.0000 (65360.5616)  weight_decay: 0.0500 (0.0500)  time: 0.6659  data: 0.1895  max mem: 15572
Epoch: [7]  [1690/2809]  eta: 0:11:10  lr: 0.000046  min_lr: 0.000000  loss: 4.2995 (4.2796)  loss_scale: 65536.0000 (65361.5991)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1576  max mem: 15572
Epoch: [7]  [1700/2809]  eta: 0:11:04  lr: 0.000046  min_lr: 0.000000  loss: 4.2151 (4.2787)  loss_scale: 65536.0000 (65362.6243)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0561  max mem: 15572
Epoch: [7]  [1710/2809]  eta: 0:10:58  lr: 0.000046  min_lr: 0.000000  loss: 4.1722 (4.2784)  loss_scale: 65536.0000 (65363.6376)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0564  max mem: 15572
Epoch: [7]  [1720/2809]  eta: 0:10:51  lr: 0.000046  min_lr: 0.000000  loss: 4.2145 (4.2785)  loss_scale: 65536.0000 (65364.6392)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0183  max mem: 15572
Epoch: [7]  [1730/2809]  eta: 0:10:45  lr: 0.000046  min_lr: 0.000000  loss: 4.3961 (4.2791)  loss_scale: 65536.0000 (65365.6291)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0667  max mem: 15572
Epoch: [7]  [1740/2809]  eta: 0:10:39  lr: 0.000046  min_lr: 0.000000  loss: 4.4803 (4.2795)  loss_scale: 65536.0000 (65366.6077)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0606  max mem: 15572
[2025-01-13 00:08:32,231] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:08:32,232] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1750/2809]  eta: 0:10:33  lr: 0.000046  min_lr: 0.000000  loss: 4.3939 (4.2792)  loss_scale: 65536.0000 (65592.1416)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0687  max mem: 15572
Epoch: [7]  [1760/2809]  eta: 0:10:26  lr: 0.000046  min_lr: 0.000000  loss: 4.2023 (4.2795)  loss_scale: 131072.0000 (65963.9750)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1052  max mem: 15572
[2025-01-13 00:08:40,956] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21425
[2025-01-13 00:08:40,957] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:08:40,957] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1770/2809]  eta: 0:10:21  lr: 0.000046  min_lr: 0.000000  loss: 4.2554 (4.2795)  loss_scale: 131072.0000 (65998.5635)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1586  max mem: 15572
Epoch: [7]  [1780/2809]  eta: 0:10:15  lr: 0.000046  min_lr: 0.000000  loss: 4.2874 (4.2792)  loss_scale: 65536.0000 (65995.9663)  weight_decay: 0.0500 (0.0500)  time: 0.6569  data: 0.2119  max mem: 15572
Epoch: [7]  [1790/2809]  eta: 0:10:09  lr: 0.000046  min_lr: 0.000000  loss: 4.4018 (4.2798)  loss_scale: 65536.0000 (65993.3981)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1556  max mem: 15572
Epoch: [7]  [1800/2809]  eta: 0:10:03  lr: 0.000046  min_lr: 0.000000  loss: 4.3818 (4.2789)  loss_scale: 65536.0000 (65990.8584)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1175  max mem: 15572
Epoch: [7]  [1810/2809]  eta: 0:09:56  lr: 0.000046  min_lr: 0.000000  loss: 4.2779 (4.2794)  loss_scale: 65536.0000 (65988.3468)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1071  max mem: 15572
Epoch: [7]  [1820/2809]  eta: 0:09:51  lr: 0.000046  min_lr: 0.000000  loss: 4.2779 (4.2789)  loss_scale: 65536.0000 (65985.8627)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1486  max mem: 15572
Epoch: [7]  [1830/2809]  eta: 0:09:45  lr: 0.000046  min_lr: 0.000000  loss: 4.0612 (4.2780)  loss_scale: 65536.0000 (65983.4058)  weight_decay: 0.0500 (0.0500)  time: 0.6518  data: 0.1813  max mem: 15572
Epoch: [7]  [1840/2809]  eta: 0:09:39  lr: 0.000046  min_lr: 0.000000  loss: 4.1610 (4.2777)  loss_scale: 65536.0000 (65980.9756)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1287  max mem: 15572
Epoch: [7]  [1850/2809]  eta: 0:09:32  lr: 0.000046  min_lr: 0.000000  loss: 4.0323 (4.2768)  loss_scale: 65536.0000 (65978.5716)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0741  max mem: 15572
Epoch: [7]  [1860/2809]  eta: 0:09:26  lr: 0.000046  min_lr: 0.000000  loss: 4.1972 (4.2767)  loss_scale: 65536.0000 (65976.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1197  max mem: 15572
Epoch: [7]  [1870/2809]  eta: 0:09:21  lr: 0.000046  min_lr: 0.000000  loss: 4.2992 (4.2770)  loss_scale: 65536.0000 (65973.8407)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1862  max mem: 15572
Epoch: [7]  [1880/2809]  eta: 0:09:14  lr: 0.000046  min_lr: 0.000000  loss: 4.1873 (4.2762)  loss_scale: 65536.0000 (65971.5130)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1263  max mem: 15572
Epoch: [7]  [1890/2809]  eta: 0:09:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2491 (4.2766)  loss_scale: 65536.0000 (65969.2099)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1064  max mem: 15572
[2025-01-13 00:09:57,724] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:09:57,724] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:10:02,388] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21561
[2025-01-13 00:10:02,388] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:10:02,388] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1900/2809]  eta: 0:09:03  lr: 0.000046  min_lr: 0.000000  loss: 4.2491 (4.2762)  loss_scale: 65536.0000 (66208.2525)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1292  max mem: 15572
Epoch: [7]  [1910/2809]  eta: 0:08:56  lr: 0.000046  min_lr: 0.000000  loss: 4.2187 (4.2760)  loss_scale: 65536.0000 (66204.7347)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1115  max mem: 15572
Epoch: [7]  [1920/2809]  eta: 0:08:50  lr: 0.000046  min_lr: 0.000000  loss: 4.2928 (4.2766)  loss_scale: 65536.0000 (66201.2535)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.1048  max mem: 15572
Epoch: [7]  [1930/2809]  eta: 0:08:44  lr: 0.000046  min_lr: 0.000000  loss: 4.3157 (4.2759)  loss_scale: 65536.0000 (66197.8084)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0899  max mem: 15572
Epoch: [7]  [1940/2809]  eta: 0:08:38  lr: 0.000046  min_lr: 0.000000  loss: 4.0644 (4.2752)  loss_scale: 65536.0000 (66194.3988)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1154  max mem: 15572
Epoch: [7]  [1950/2809]  eta: 0:08:32  lr: 0.000046  min_lr: 0.000000  loss: 3.9362 (4.2745)  loss_scale: 65536.0000 (66191.0241)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1596  max mem: 15572
Epoch: [7]  [1960/2809]  eta: 0:08:26  lr: 0.000046  min_lr: 0.000000  loss: 4.2444 (4.2751)  loss_scale: 65536.0000 (66187.6838)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.1845  max mem: 15572
Epoch: [7]  [1970/2809]  eta: 0:08:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3780 (4.2753)  loss_scale: 65536.0000 (66184.3775)  weight_decay: 0.0500 (0.0500)  time: 0.6523  data: 0.2206  max mem: 15572
Epoch: [7]  [1980/2809]  eta: 0:08:15  lr: 0.000046  min_lr: 0.000000  loss: 4.3116 (4.2755)  loss_scale: 65536.0000 (66181.1045)  weight_decay: 0.0500 (0.0500)  time: 0.6943  data: 0.2347  max mem: 15572
Epoch: [7]  [1990/2809]  eta: 0:08:09  lr: 0.000046  min_lr: 0.000000  loss: 4.2824 (4.2756)  loss_scale: 65536.0000 (66177.8644)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.1768  max mem: 15572
Epoch: [7]  [2000/2809]  eta: 0:08:03  lr: 0.000046  min_lr: 0.000000  loss: 4.2279 (4.2747)  loss_scale: 65536.0000 (66174.6567)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1185  max mem: 15572
Epoch: [7]  [2010/2809]  eta: 0:07:56  lr: 0.000046  min_lr: 0.000000  loss: 4.2731 (4.2750)  loss_scale: 65536.0000 (66171.4809)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0764  max mem: 15572
Epoch: [7]  [2020/2809]  eta: 0:07:50  lr: 0.000046  min_lr: 0.000000  loss: 4.4187 (4.2760)  loss_scale: 65536.0000 (66168.3365)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0968  max mem: 15572
[2025-01-13 00:11:17,108] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:11:17,109] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [2030/2809]  eta: 0:07:44  lr: 0.000046  min_lr: 0.000000  loss: 4.5326 (4.2767)  loss_scale: 65536.0000 (66294.2944)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1476  max mem: 15572
[2025-01-13 00:11:22,962] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21700
[2025-01-13 00:11:22,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:11:22,963] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2040/2809]  eta: 0:07:38  lr: 0.000046  min_lr: 0.000000  loss: 4.3504 (4.2765)  loss_scale: 65536.0000 (66483.2376)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0965  max mem: 15572
Epoch: [7]  [2050/2809]  eta: 0:07:32  lr: 0.000046  min_lr: 0.000000  loss: 4.3319 (4.2770)  loss_scale: 65536.0000 (66478.6192)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1052  max mem: 15572
Epoch: [7]  [2060/2809]  eta: 0:07:26  lr: 0.000046  min_lr: 0.000000  loss: 4.3183 (4.2769)  loss_scale: 65536.0000 (66474.0456)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1295  max mem: 15572
Epoch: [7]  [2070/2809]  eta: 0:07:20  lr: 0.000046  min_lr: 0.000000  loss: 4.2968 (4.2772)  loss_scale: 65536.0000 (66469.5162)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1145  max mem: 15572
Epoch: [7]  [2080/2809]  eta: 0:07:15  lr: 0.000046  min_lr: 0.000000  loss: 4.3251 (4.2771)  loss_scale: 65536.0000 (66465.0303)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.1725  max mem: 15572
Epoch: [7]  [2090/2809]  eta: 0:07:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2437 (4.2766)  loss_scale: 65536.0000 (66460.5873)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1335  max mem: 15572
Epoch: [7]  [2100/2809]  eta: 0:07:02  lr: 0.000046  min_lr: 0.000000  loss: 4.3643 (4.2770)  loss_scale: 65536.0000 (66456.1866)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.1068  max mem: 15572
Epoch: [7]  [2110/2809]  eta: 0:06:56  lr: 0.000046  min_lr: 0.000000  loss: 4.3643 (4.2769)  loss_scale: 65536.0000 (66451.8276)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1684  max mem: 15572
Epoch: [7]  [2120/2809]  eta: 0:06:50  lr: 0.000046  min_lr: 0.000000  loss: 4.3282 (4.2769)  loss_scale: 65536.0000 (66447.5097)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1203  max mem: 15572
Epoch: [7]  [2130/2809]  eta: 0:06:44  lr: 0.000046  min_lr: 0.000000  loss: 4.1459 (4.2756)  loss_scale: 65536.0000 (66443.2323)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1086  max mem: 15572
Epoch: [7]  [2140/2809]  eta: 0:06:38  lr: 0.000046  min_lr: 0.000000  loss: 4.2664 (4.2767)  loss_scale: 65536.0000 (66438.9949)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1019  max mem: 15572
Epoch: [7]  [2150/2809]  eta: 0:06:32  lr: 0.000046  min_lr: 0.000000  loss: 4.3691 (4.2771)  loss_scale: 65536.0000 (66434.7968)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0825  max mem: 15572
Epoch: [7]  [2160/2809]  eta: 0:06:26  lr: 0.000046  min_lr: 0.000000  loss: 4.3691 (4.2776)  loss_scale: 65536.0000 (66430.6377)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0527  max mem: 15572
[2025-01-13 00:12:36,591] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:12:36,591] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [2170/2809]  eta: 0:06:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3772 (4.2775)  loss_scale: 65536.0000 (66577.4519)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0417  max mem: 15572
[2025-01-13 00:12:39,758] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21834
[2025-01-13 00:12:39,759] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:12:39,759] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-13 00:12:40,152] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21835
[2025-01-13 00:12:40,153] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 00:12:40,153] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [2180/2809]  eta: 0:06:14  lr: 0.000046  min_lr: 0.000000  loss: 4.2634 (4.2769)  loss_scale: 65536.0000 (66437.4580)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1392  max mem: 15572
Epoch: [7]  [2190/2809]  eta: 0:06:08  lr: 0.000046  min_lr: 0.000000  loss: 4.1605 (4.2765)  loss_scale: 32768.0000 (66283.7864)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1305  max mem: 15572
Epoch: [7]  [2200/2809]  eta: 0:06:02  lr: 0.000046  min_lr: 0.000000  loss: 4.2183 (4.2762)  loss_scale: 32768.0000 (66131.5111)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1433  max mem: 15572
Epoch: [7]  [2210/2809]  eta: 0:05:56  lr: 0.000046  min_lr: 0.000000  loss: 4.2410 (4.2762)  loss_scale: 32768.0000 (65980.6133)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1254  max mem: 15572
Epoch: [7]  [2220/2809]  eta: 0:05:50  lr: 0.000046  min_lr: 0.000000  loss: 4.1717 (4.2753)  loss_scale: 32768.0000 (65831.0743)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0511  max mem: 15572
Epoch: [7]  [2230/2809]  eta: 0:05:44  lr: 0.000046  min_lr: 0.000000  loss: 3.9557 (4.2744)  loss_scale: 32768.0000 (65682.8758)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1453  max mem: 15572
Epoch: [7]  [2240/2809]  eta: 0:05:38  lr: 0.000046  min_lr: 0.000000  loss: 4.1880 (4.2743)  loss_scale: 32768.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1419  max mem: 15572
Epoch: [7]  [2250/2809]  eta: 0:05:32  lr: 0.000046  min_lr: 0.000000  loss: 4.2407 (4.2737)  loss_scale: 32768.0000 (65390.4291)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0414  max mem: 15572
Epoch: [7]  [2260/2809]  eta: 0:05:26  lr: 0.000046  min_lr: 0.000000  loss: 4.0939 (4.2732)  loss_scale: 32768.0000 (65246.1460)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0886  max mem: 15572
Epoch: [7]  [2270/2809]  eta: 0:05:20  lr: 0.000046  min_lr: 0.000000  loss: 4.0939 (4.2730)  loss_scale: 32768.0000 (65103.1334)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1145  max mem: 15572
Epoch: [7]  [2280/2809]  eta: 0:05:14  lr: 0.000046  min_lr: 0.000000  loss: 4.4132 (4.2737)  loss_scale: 32768.0000 (64961.3748)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1060  max mem: 15572
Epoch: [7]  [2290/2809]  eta: 0:05:08  lr: 0.000046  min_lr: 0.000000  loss: 4.3985 (4.2735)  loss_scale: 32768.0000 (64820.8538)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0981  max mem: 15572
Epoch: [7]  [2300/2809]  eta: 0:05:02  lr: 0.000046  min_lr: 0.000000  loss: 4.2310 (4.2731)  loss_scale: 32768.0000 (64681.5541)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0918  max mem: 15572
[2025-01-13 00:13:55,269] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:13:55,270] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [2310/2809]  eta: 0:04:56  lr: 0.000046  min_lr: 0.000000  loss: 4.0951 (4.2733)  loss_scale: 32768.0000 (64685.2514)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.1613  max mem: 15572
Epoch: [7]  [2320/2809]  eta: 0:04:50  lr: 0.000046  min_lr: 0.000000  loss: 4.3997 (4.2735)  loss_scale: 65536.0000 (64688.9168)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1138  max mem: 15572
Epoch: [7]  [2330/2809]  eta: 0:04:44  lr: 0.000046  min_lr: 0.000000  loss: 4.2776 (4.2736)  loss_scale: 65536.0000 (64692.5508)  weight_decay: 0.0500 (0.0500)  time: 0.5119  data: 0.0629  max mem: 15572
[2025-01-13 00:14:15,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=140, lr=[4.4687915103572624e-07, 4.4687915103572624e-07, 6.383987871938948e-07, 6.383987871938948e-07, 9.119982674198497e-07, 9.119982674198497e-07, 1.3028546677426425e-06, 1.3028546677426425e-06, 1.8612209539180608e-06, 1.8612209539180608e-06, 2.658887077025801e-06, 2.658887077025801e-06, 3.798410110036859e-06, 3.798410110036859e-06, 5.4263001571955135e-06, 5.4263001571955135e-06, 7.751857367422162e-06, 7.751857367422162e-06, 1.1074081953460234e-05, 1.1074081953460234e-05, 1.582011707637176e-05, 1.582011707637176e-05, 2.260016725195966e-05, 2.260016725195966e-05, 3.2285953217085234e-05, 3.2285953217085234e-05, 4.6122790310121765e-05, 4.6122790310121765e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 00:14:15,355] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=27.86693540015533, CurrSamplesPerSec=23.95312517846469, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [2340/2809]  eta: 0:04:38  lr: 0.000046  min_lr: 0.000000  loss: 4.2406 (4.2735)  loss_scale: 65536.0000 (64696.1538)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0947  max mem: 15572
Epoch: [7]  [2350/2809]  eta: 0:04:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4126 (4.2740)  loss_scale: 65536.0000 (64699.7261)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1306  max mem: 15572
Epoch: [7]  [2360/2809]  eta: 0:04:26  lr: 0.000046  min_lr: 0.000000  loss: 4.1591 (4.2734)  loss_scale: 65536.0000 (64703.2681)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1754  max mem: 15572
Epoch: [7]  [2370/2809]  eta: 0:04:20  lr: 0.000046  min_lr: 0.000000  loss: 4.1109 (4.2723)  loss_scale: 65536.0000 (64706.7803)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1527  max mem: 15572
Epoch: [7]  [2380/2809]  eta: 0:04:14  lr: 0.000046  min_lr: 0.000000  loss: 4.2935 (4.2726)  loss_scale: 65536.0000 (64710.2629)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.1002  max mem: 15572
Epoch: [7]  [2390/2809]  eta: 0:04:08  lr: 0.000046  min_lr: 0.000000  loss: 4.3679 (4.2726)  loss_scale: 65536.0000 (64713.7164)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1464  max mem: 15572
Epoch: [7]  [2400/2809]  eta: 0:04:02  lr: 0.000046  min_lr: 0.000000  loss: 4.3679 (4.2730)  loss_scale: 65536.0000 (64717.1412)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1566  max mem: 15572
Epoch: [7]  [2410/2809]  eta: 0:03:56  lr: 0.000046  min_lr: 0.000000  loss: 4.3750 (4.2733)  loss_scale: 65536.0000 (64720.5375)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1244  max mem: 15572
Epoch: [7]  [2420/2809]  eta: 0:03:50  lr: 0.000046  min_lr: 0.000000  loss: 4.1940 (4.2724)  loss_scale: 65536.0000 (64723.9058)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1507  max mem: 15572
[2025-01-13 00:15:08,882] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:15:08,883] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [2430/2809]  eta: 0:03:44  lr: 0.000046  min_lr: 0.000000  loss: 4.1367 (4.2724)  loss_scale: 65536.0000 (64781.1633)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1374  max mem: 15572
[2025-01-13 00:15:10,306] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22095
[2025-01-13 00:15:10,307] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:15:10,307] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2440/2809]  eta: 0:03:38  lr: 0.000046  min_lr: 0.000000  loss: 4.1801 (4.2721)  loss_scale: 65536.0000 (64811.1036)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1608  max mem: 15572
Epoch: [7]  [2450/2809]  eta: 0:03:32  lr: 0.000046  min_lr: 0.000000  loss: 4.1474 (4.2714)  loss_scale: 65536.0000 (64814.0612)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1848  max mem: 15572
Epoch: [7]  [2460/2809]  eta: 0:03:26  lr: 0.000046  min_lr: 0.000000  loss: 3.9534 (4.2705)  loss_scale: 65536.0000 (64816.9947)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1517  max mem: 15572
Epoch: [7]  [2470/2809]  eta: 0:03:20  lr: 0.000046  min_lr: 0.000000  loss: 4.1056 (4.2706)  loss_scale: 65536.0000 (64819.9045)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1219  max mem: 15572
Epoch: [7]  [2480/2809]  eta: 0:03:14  lr: 0.000046  min_lr: 0.000000  loss: 4.1892 (4.2700)  loss_scale: 65536.0000 (64822.7908)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0846  max mem: 15572
Epoch: [7]  [2490/2809]  eta: 0:03:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2311 (4.2701)  loss_scale: 65536.0000 (64825.6540)  weight_decay: 0.0500 (0.0500)  time: 0.5014  data: 0.0512  max mem: 15572
Epoch: [7]  [2500/2809]  eta: 0:03:03  lr: 0.000046  min_lr: 0.000000  loss: 4.2523 (4.2692)  loss_scale: 65536.0000 (64828.4942)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1159  max mem: 15572
Epoch: [7]  [2510/2809]  eta: 0:02:57  lr: 0.000046  min_lr: 0.000000  loss: 4.2639 (4.2688)  loss_scale: 65536.0000 (64831.3118)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1235  max mem: 15572
Epoch: [7]  [2520/2809]  eta: 0:02:51  lr: 0.000046  min_lr: 0.000000  loss: 4.3679 (4.2693)  loss_scale: 65536.0000 (64834.1071)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0956  max mem: 15572
Epoch: [7]  [2530/2809]  eta: 0:02:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2943 (4.2688)  loss_scale: 65536.0000 (64836.8803)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1450  max mem: 15572
Epoch: [7]  [2540/2809]  eta: 0:02:39  lr: 0.000046  min_lr: 0.000000  loss: 4.2486 (4.2686)  loss_scale: 65536.0000 (64839.6316)  weight_decay: 0.0500 (0.0500)  time: 0.6927  data: 0.2400  max mem: 15572
Epoch: [7]  [2550/2809]  eta: 0:02:33  lr: 0.000046  min_lr: 0.000000  loss: 4.2670 (4.2681)  loss_scale: 65536.0000 (64842.3614)  weight_decay: 0.0500 (0.0500)  time: 0.6705  data: 0.2436  max mem: 15572
Epoch: [7]  [2560/2809]  eta: 0:02:27  lr: 0.000046  min_lr: 0.000000  loss: 4.2860 (4.2684)  loss_scale: 65536.0000 (64845.0699)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1819  max mem: 15572
[2025-01-13 00:16:27,697] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:16:27,697] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:16:29,550] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22228
[2025-01-13 00:16:29,551] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:16:29,551] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2570/2809]  eta: 0:02:21  lr: 0.000046  min_lr: 0.000000  loss: 4.2634 (4.2678)  loss_scale: 65536.0000 (64949.7192)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1026  max mem: 15572
Epoch: [7]  [2580/2809]  eta: 0:02:15  lr: 0.000046  min_lr: 0.000000  loss: 4.2394 (4.2673)  loss_scale: 65536.0000 (64951.9907)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0623  max mem: 15572
Epoch: [7]  [2590/2809]  eta: 0:02:09  lr: 0.000046  min_lr: 0.000000  loss: 4.2577 (4.2676)  loss_scale: 65536.0000 (64954.2447)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1785  max mem: 15572
Epoch: [7]  [2600/2809]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000000  loss: 4.4001 (4.2685)  loss_scale: 65536.0000 (64956.4814)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1173  max mem: 15572
Epoch: [7]  [2610/2809]  eta: 0:01:57  lr: 0.000046  min_lr: 0.000000  loss: 4.4120 (4.2693)  loss_scale: 65536.0000 (64958.7009)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1156  max mem: 15572
Epoch: [7]  [2620/2809]  eta: 0:01:51  lr: 0.000046  min_lr: 0.000000  loss: 4.3987 (4.2697)  loss_scale: 65536.0000 (64960.9035)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1464  max mem: 15572
Epoch: [7]  [2630/2809]  eta: 0:01:46  lr: 0.000046  min_lr: 0.000000  loss: 4.2572 (4.2694)  loss_scale: 65536.0000 (64963.0893)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1298  max mem: 15572
Epoch: [7]  [2640/2809]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000000  loss: 4.1899 (4.2689)  loss_scale: 65536.0000 (64965.2586)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1480  max mem: 15572
Epoch: [7]  [2650/2809]  eta: 0:01:34  lr: 0.000046  min_lr: 0.000000  loss: 4.2292 (4.2689)  loss_scale: 65536.0000 (64967.4115)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0788  max mem: 15572
Epoch: [7]  [2660/2809]  eta: 0:01:28  lr: 0.000046  min_lr: 0.000000  loss: 4.2955 (4.2686)  loss_scale: 65536.0000 (64969.5483)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0876  max mem: 15572
Epoch: [7]  [2670/2809]  eta: 0:01:22  lr: 0.000046  min_lr: 0.000000  loss: 4.2227 (4.2685)  loss_scale: 65536.0000 (64971.6690)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0957  max mem: 15572
Epoch: [7]  [2680/2809]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000000  loss: 4.1618 (4.2683)  loss_scale: 65536.0000 (64973.7740)  weight_decay: 0.0500 (0.0500)  time: 0.6678  data: 0.2211  max mem: 15572
Epoch: [7]  [2690/2809]  eta: 0:01:10  lr: 0.000046  min_lr: 0.000000  loss: 4.1597 (4.2682)  loss_scale: 65536.0000 (64975.8632)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.2298  max mem: 15572
[2025-01-13 00:17:44,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:17:44,221] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [2700/2809]  eta: 0:01:04  lr: 0.000046  min_lr: 0.000000  loss: 4.4147 (4.2690)  loss_scale: 65536.0000 (65147.7823)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1306  max mem: 15572
[2025-01-13 00:17:51,117] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22369
[2025-01-13 00:17:51,118] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:17:51,118] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2710/2809]  eta: 0:00:58  lr: 0.000046  min_lr: 0.000000  loss: 4.3213 (4.2688)  loss_scale: 131072.0000 (65270.0848)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1418  max mem: 15572
Epoch: [7]  [2720/2809]  eta: 0:00:52  lr: 0.000046  min_lr: 0.000000  loss: 4.2891 (4.2692)  loss_scale: 65536.0000 (65271.0621)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1110  max mem: 15572
Epoch: [7]  [2730/2809]  eta: 0:00:46  lr: 0.000046  min_lr: 0.000000  loss: 4.2891 (4.2688)  loss_scale: 65536.0000 (65272.0322)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1322  max mem: 15572
Epoch: [7]  [2740/2809]  eta: 0:00:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2647 (4.2691)  loss_scale: 65536.0000 (65272.9953)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0791  max mem: 15572
[2025-01-13 00:18:12,587] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22407
[2025-01-13 00:18:12,588] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 00:18:12,588] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [2750/2809]  eta: 0:00:34  lr: 0.000046  min_lr: 0.000000  loss: 4.3886 (4.2692)  loss_scale: 65536.0000 (65190.5722)  weight_decay: 0.0500 (0.0500)  time: 0.4851  data: 0.0112  max mem: 15572
Epoch: [7]  [2760/2809]  eta: 0:00:28  lr: 0.000046  min_lr: 0.000000  loss: 4.2882 (4.2692)  loss_scale: 32768.0000 (65073.1416)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0636  max mem: 15572
Epoch: [7]  [2770/2809]  eta: 0:00:23  lr: 0.000046  min_lr: 0.000000  loss: 4.3972 (4.2698)  loss_scale: 32768.0000 (64956.5586)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1395  max mem: 15572
Epoch: [7]  [2780/2809]  eta: 0:00:17  lr: 0.000046  min_lr: 0.000000  loss: 4.4488 (4.2704)  loss_scale: 32768.0000 (64840.8141)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1211  max mem: 15572
Epoch: [7]  [2790/2809]  eta: 0:00:11  lr: 0.000046  min_lr: 0.000000  loss: 4.3933 (4.2706)  loss_scale: 32768.0000 (64725.8990)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0979  max mem: 15572
Epoch: [7]  [2800/2809]  eta: 0:00:05  lr: 0.000046  min_lr: 0.000000  loss: 4.1957 (4.2703)  loss_scale: 32768.0000 (64611.8044)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0891  max mem: 15572
Epoch: [7]  [2808/2809]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2035 (4.2707)  loss_scale: 32768.0000 (64521.1136)  weight_decay: 0.0500 (0.0500)  time: 0.4900  data: 0.0641  max mem: 15572
Epoch: [7] Total time: 0:27:39 (0.5910 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 4.2035 (4.2707)  loss_scale: 32768.0000 (64521.1136)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:20:53  loss: 0.5063 (0.5063)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.6085  data: 4.3751  max mem: 15572
Val:  [ 10/272]  eta: 0:03:21  loss: 4.0960 (3.4501)  acc1: 0.0000 (24.7475)  acc5: 11.1111 (31.3131)  time: 0.7708  data: 0.5684  max mem: 15572
Val:  [ 20/272]  eta: 0:02:24  loss: 3.6253 (3.4290)  acc1: 11.1111 (22.7513)  acc5: 38.8889 (41.5344)  time: 0.3709  data: 0.1873  max mem: 15572
Val:  [ 30/272]  eta: 0:01:50  loss: 3.6253 (3.5695)  acc1: 5.5556 (16.6667)  acc5: 44.4444 (41.0394)  time: 0.2830  data: 0.1136  max mem: 15572
Val:  [ 40/272]  eta: 0:01:43  loss: 3.4916 (3.5317)  acc1: 5.5556 (16.3957)  acc5: 50.0000 (43.7669)  time: 0.3123  data: 0.1275  max mem: 15572
Val:  [ 50/272]  eta: 0:01:33  loss: 3.3479 (3.4767)  acc1: 11.1111 (16.9935)  acc5: 55.5556 (46.1874)  time: 0.3622  data: 0.1675  max mem: 15572
Val:  [ 60/272]  eta: 0:01:23  loss: 2.2224 (3.3056)  acc1: 38.8889 (23.6794)  acc5: 72.2222 (49.7268)  time: 0.2843  data: 0.0916  max mem: 15572
Val:  [ 70/272]  eta: 0:01:12  loss: 2.2224 (3.2020)  acc1: 50.0000 (25.3521)  acc5: 72.2222 (54.1471)  time: 0.2068  data: 0.0316  max mem: 15572
Val:  [ 80/272]  eta: 0:01:04  loss: 2.8030 (3.1856)  acc1: 27.7778 (26.2689)  acc5: 72.2222 (54.1152)  time: 0.1657  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:58  loss: 3.8875 (3.2666)  acc1: 5.5556 (23.9316)  acc5: 33.3333 (51.5263)  time: 0.1765  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:53  loss: 3.8875 (3.3313)  acc1: 5.5556 (23.1573)  acc5: 33.3333 (50.2200)  time: 0.1972  data: 0.0007  max mem: 15572
Val:  [110/272]  eta: 0:00:50  loss: 3.8402 (3.3904)  acc1: 5.5556 (21.3714)  acc5: 38.8889 (48.5986)  time: 0.2811  data: 0.0797  max mem: 15572
Val:  [120/272]  eta: 0:00:47  loss: 3.8402 (3.4293)  acc1: 5.5556 (20.2479)  acc5: 38.8889 (48.0716)  time: 0.3263  data: 0.1290  max mem: 15572
Val:  [130/272]  eta: 0:00:45  loss: 3.6740 (3.3741)  acc1: 11.1111 (22.3494)  acc5: 44.4444 (49.0246)  time: 0.3653  data: 0.1555  max mem: 15572
Val:  [140/272]  eta: 0:00:42  loss: 3.0900 (3.3602)  acc1: 22.2222 (23.0496)  acc5: 55.5556 (49.2514)  time: 0.4044  data: 0.1989  max mem: 15572
Val:  [150/272]  eta: 0:00:39  loss: 3.3213 (3.3614)  acc1: 11.1111 (22.2958)  acc5: 61.1111 (50.0000)  time: 0.3762  data: 0.1715  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.2514 (3.3310)  acc1: 22.2222 (23.4645)  acc5: 66.6667 (51.4493)  time: 0.3738  data: 0.1652  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 3.3734 (3.3628)  acc1: 22.2222 (22.5796)  acc5: 55.5556 (50.8122)  time: 0.3624  data: 0.1600  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.4529 (3.3553)  acc1: 5.5556 (22.0381)  acc5: 55.5556 (51.7188)  time: 0.3918  data: 0.1955  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.4196 (3.3868)  acc1: 11.1111 (21.6987)  acc5: 55.5556 (50.9017)  time: 0.4085  data: 0.2128  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.1279 (3.3817)  acc1: 5.5556 (21.7523)  acc5: 61.1111 (51.5755)  time: 0.3513  data: 0.1572  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.9595 (3.3916)  acc1: 11.1111 (21.9326)  acc5: 72.2222 (51.6324)  time: 0.3355  data: 0.1442  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.4847 (3.3805)  acc1: 22.2222 (22.4233)  acc5: 55.5556 (52.0613)  time: 0.4146  data: 0.2183  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.5636 (3.3466)  acc1: 38.8889 (23.8336)  acc5: 66.6667 (53.0544)  time: 0.4400  data: 0.2266  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 2.3593 (3.3209)  acc1: 55.5556 (24.7118)  acc5: 77.7778 (53.9419)  time: 0.3668  data: 0.1627  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.0073 (3.3467)  acc1: 16.6667 (24.3471)  acc5: 61.1111 (53.3643)  time: 0.3519  data: 0.1648  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 2.7515 (3.2697)  acc1: 55.5556 (26.7348)  acc5: 72.2222 (54.7893)  time: 0.3617  data: 0.1530  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.1934 (3.2716)  acc1: 55.5556 (26.5273)  acc5: 77.7778 (54.5920)  time: 0.2681  data: 0.0653  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.1934 (3.2763)  acc1: 55.5556 (26.5206)  acc5: 77.7778 (54.5566)  time: 0.2622  data: 0.0652  max mem: 15572
Val: Total time: 0:01:33 (0.3430 s / it)
* Acc@1 26.521 Acc@5 54.557 loss 3.276
Accuracy of the network on the 4883 val videos: 26.5%
[2025-01-13 00:20:20,284] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 00:20:20,288] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 00:20:20,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 00:20:23,492] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 00:20:23,493] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 26.52%
Epoch: [8]  [   0/2809]  eta: 6:58:33  lr: 0.000046  min_lr: 0.000000  loss: 4.2523 (4.2523)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.9403  data: 8.3597  max mem: 15572
Epoch: [8]  [  10/2809]  eta: 1:08:15  lr: 0.000046  min_lr: 0.000000  loss: 4.0880 (4.1867)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4632  data: 0.9561  max mem: 15572
Epoch: [8]  [  20/2809]  eta: 0:49:04  lr: 0.000046  min_lr: 0.000000  loss: 4.0880 (4.1931)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6616  data: 0.1823  max mem: 15572
Epoch: [8]  [  30/2809]  eta: 0:39:19  lr: 0.000046  min_lr: 0.000000  loss: 4.2966 (4.2836)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5113  data: 0.0747  max mem: 15572
Epoch: [8]  [  40/2809]  eta: 0:34:53  lr: 0.000046  min_lr: 0.000000  loss: 4.3171 (4.2904)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4412  data: 0.0006  max mem: 15572
Epoch: [8]  [  50/2809]  eta: 0:31:53  lr: 0.000046  min_lr: 0.000000  loss: 4.3289 (4.2961)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4527  data: 0.0007  max mem: 15572
Epoch: [8]  [  60/2809]  eta: 0:30:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3289 (4.2930)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4696  data: 0.0008  max mem: 15572
[2025-01-13 00:21:06,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:21:06,677] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [  70/2809]  eta: 0:29:27  lr: 0.000046  min_lr: 0.000000  loss: 4.2505 (4.2575)  loss_scale: 32768.0000 (35998.6479)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0388  max mem: 15572
Epoch: [8]  [  80/2809]  eta: 0:29:01  lr: 0.000046  min_lr: 0.000000  loss: 4.2505 (4.2616)  loss_scale: 65536.0000 (39645.2346)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1113  max mem: 15572
Epoch: [8]  [  90/2809]  eta: 0:29:12  lr: 0.000046  min_lr: 0.000000  loss: 4.2792 (4.2721)  loss_scale: 65536.0000 (42490.3736)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.1844  max mem: 15572
Epoch: [8]  [ 100/2809]  eta: 0:29:00  lr: 0.000046  min_lr: 0.000000  loss: 4.3031 (4.2737)  loss_scale: 65536.0000 (44772.1188)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.1745  max mem: 15572
Epoch: [8]  [ 110/2809]  eta: 0:28:07  lr: 0.000046  min_lr: 0.000000  loss: 4.1679 (4.2541)  loss_scale: 65536.0000 (46642.7387)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0638  max mem: 15572
Epoch: [8]  [ 120/2809]  eta: 0:28:20  lr: 0.000046  min_lr: 0.000000  loss: 3.9789 (4.2228)  loss_scale: 65536.0000 (48204.1653)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1013  max mem: 15572
Epoch: [8]  [ 130/2809]  eta: 0:28:04  lr: 0.000046  min_lr: 0.000000  loss: 4.0508 (4.2196)  loss_scale: 65536.0000 (49527.2061)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.1592  max mem: 15572
Epoch: [8]  [ 140/2809]  eta: 0:27:50  lr: 0.000046  min_lr: 0.000000  loss: 4.1263 (4.2203)  loss_scale: 65536.0000 (50662.5816)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1036  max mem: 15572
Epoch: [8]  [ 150/2809]  eta: 0:27:30  lr: 0.000046  min_lr: 0.000000  loss: 4.1516 (4.2239)  loss_scale: 65536.0000 (51647.5762)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1029  max mem: 15572
Epoch: [8]  [ 160/2809]  eta: 0:27:12  lr: 0.000046  min_lr: 0.000000  loss: 4.3138 (4.2324)  loss_scale: 65536.0000 (52510.2112)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1106  max mem: 15572
Epoch: [8]  [ 170/2809]  eta: 0:26:57  lr: 0.000046  min_lr: 0.000000  loss: 4.3138 (4.2348)  loss_scale: 65536.0000 (53271.9532)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1131  max mem: 15572
Epoch: [8]  [ 180/2809]  eta: 0:26:49  lr: 0.000046  min_lr: 0.000000  loss: 4.0481 (4.2255)  loss_scale: 65536.0000 (53949.5249)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1367  max mem: 15572
Epoch: [8]  [ 190/2809]  eta: 0:26:35  lr: 0.000046  min_lr: 0.000000  loss: 4.1196 (4.2319)  loss_scale: 65536.0000 (54556.1466)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1411  max mem: 15572
[2025-01-13 00:22:21,585] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:22:21,586] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 200/2809]  eta: 0:26:17  lr: 0.000046  min_lr: 0.000000  loss: 4.2379 (4.2292)  loss_scale: 65536.0000 (58036.8557)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.1048  max mem: 15572
[2025-01-13 00:22:30,809] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22678
[2025-01-13 00:22:30,811] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:22:30,812] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 210/2809]  eta: 0:26:31  lr: 0.000046  min_lr: 0.000000  loss: 4.1387 (4.2206)  loss_scale: 131072.0000 (59945.2512)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.1833  max mem: 15572
Epoch: [8]  [ 220/2809]  eta: 0:26:28  lr: 0.000046  min_lr: 0.000000  loss: 4.0550 (4.2148)  loss_scale: 65536.0000 (60198.2262)  weight_decay: 0.0500 (0.0500)  time: 0.7051  data: 0.2241  max mem: 15572
Epoch: [8]  [ 230/2809]  eta: 0:26:09  lr: 0.000046  min_lr: 0.000000  loss: 4.1178 (4.2098)  loss_scale: 65536.0000 (60429.2987)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1080  max mem: 15572
Epoch: [8]  [ 240/2809]  eta: 0:26:01  lr: 0.000046  min_lr: 0.000000  loss: 4.2050 (4.2136)  loss_scale: 65536.0000 (60641.1950)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0800  max mem: 15572
Epoch: [8]  [ 250/2809]  eta: 0:25:56  lr: 0.000046  min_lr: 0.000000  loss: 4.3006 (4.2140)  loss_scale: 65536.0000 (60836.2072)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1324  max mem: 15572
Epoch: [8]  [ 260/2809]  eta: 0:25:46  lr: 0.000046  min_lr: 0.000000  loss: 4.2321 (4.2153)  loss_scale: 65536.0000 (61016.2759)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1324  max mem: 15572
Epoch: [8]  [ 270/2809]  eta: 0:25:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2416 (4.2153)  loss_scale: 65536.0000 (61183.0554)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1323  max mem: 15572
Epoch: [8]  [ 280/2809]  eta: 0:25:35  lr: 0.000046  min_lr: 0.000000  loss: 4.2650 (4.2190)  loss_scale: 65536.0000 (61337.9644)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1638  max mem: 15572
Epoch: [8]  [ 290/2809]  eta: 0:25:24  lr: 0.000046  min_lr: 0.000000  loss: 4.2242 (4.2218)  loss_scale: 65536.0000 (61482.2268)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1559  max mem: 15572
Epoch: [8]  [ 300/2809]  eta: 0:25:21  lr: 0.000046  min_lr: 0.000000  loss: 4.0960 (4.2132)  loss_scale: 65536.0000 (61616.9037)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1809  max mem: 15572
Epoch: [8]  [ 310/2809]  eta: 0:25:15  lr: 0.000046  min_lr: 0.000000  loss: 4.2149 (4.2154)  loss_scale: 65536.0000 (61742.9196)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1879  max mem: 15572
Epoch: [8]  [ 320/2809]  eta: 0:25:08  lr: 0.000046  min_lr: 0.000000  loss: 4.3021 (4.2150)  loss_scale: 65536.0000 (61861.0841)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1522  max mem: 15572
Epoch: [8]  [ 330/2809]  eta: 0:24:57  lr: 0.000046  min_lr: 0.000000  loss: 4.3305 (4.2203)  loss_scale: 65536.0000 (61972.1088)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1068  max mem: 15572
[2025-01-13 00:23:46,857] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:23:46,858] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:23:49,939] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22810
[2025-01-13 00:23:49,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:23:49,941] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 340/2809]  eta: 0:25:01  lr: 0.000046  min_lr: 0.000000  loss: 4.3923 (4.2203)  loss_scale: 65536.0000 (62653.1848)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1702  max mem: 15572
Epoch: [8]  [ 350/2809]  eta: 0:24:52  lr: 0.000046  min_lr: 0.000000  loss: 4.2851 (4.2191)  loss_scale: 65536.0000 (62735.3162)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.1976  max mem: 15572
Epoch: [8]  [ 360/2809]  eta: 0:24:45  lr: 0.000046  min_lr: 0.000000  loss: 4.1189 (4.2148)  loss_scale: 65536.0000 (62812.8975)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1446  max mem: 15572
Epoch: [8]  [ 370/2809]  eta: 0:24:40  lr: 0.000046  min_lr: 0.000000  loss: 4.1137 (4.2126)  loss_scale: 65536.0000 (62886.2965)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1711  max mem: 15572
Epoch: [8]  [ 380/2809]  eta: 0:24:32  lr: 0.000046  min_lr: 0.000000  loss: 4.2573 (4.2157)  loss_scale: 65536.0000 (62955.8425)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1501  max mem: 15572
Epoch: [8]  [ 390/2809]  eta: 0:24:26  lr: 0.000046  min_lr: 0.000000  loss: 4.3545 (4.2178)  loss_scale: 65536.0000 (63021.8312)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1448  max mem: 15572
Epoch: [8]  [ 400/2809]  eta: 0:24:19  lr: 0.000046  min_lr: 0.000000  loss: 4.1235 (4.2155)  loss_scale: 65536.0000 (63084.5287)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1335  max mem: 15572
Epoch: [8]  [ 410/2809]  eta: 0:24:09  lr: 0.000046  min_lr: 0.000000  loss: 4.0780 (4.2105)  loss_scale: 65536.0000 (63144.1752)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0880  max mem: 15572
Epoch: [8]  [ 420/2809]  eta: 0:24:04  lr: 0.000046  min_lr: 0.000000  loss: 4.1421 (4.2128)  loss_scale: 65536.0000 (63200.9881)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1015  max mem: 15572
Epoch: [8]  [ 430/2809]  eta: 0:23:54  lr: 0.000046  min_lr: 0.000000  loss: 4.2461 (4.2123)  loss_scale: 65536.0000 (63255.1647)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0971  max mem: 15572
Epoch: [8]  [ 440/2809]  eta: 0:23:50  lr: 0.000046  min_lr: 0.000000  loss: 4.3291 (4.2163)  loss_scale: 65536.0000 (63306.8844)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1192  max mem: 15572
Epoch: [8]  [ 450/2809]  eta: 0:23:38  lr: 0.000046  min_lr: 0.000000  loss: 4.3412 (4.2170)  loss_scale: 65536.0000 (63356.3104)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1005  max mem: 15572
Epoch: [8]  [ 460/2809]  eta: 0:23:30  lr: 0.000046  min_lr: 0.000000  loss: 4.3056 (4.2178)  loss_scale: 65536.0000 (63403.5922)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0848  max mem: 15572
[2025-01-13 00:25:03,943] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:25:03,944] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 470/2809]  eta: 0:23:22  lr: 0.000046  min_lr: 0.000000  loss: 4.2432 (4.2181)  loss_scale: 65536.0000 (64005.4352)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1071  max mem: 15572
[2025-01-13 00:25:06,596] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22943
[2025-01-13 00:25:06,596] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:25:06,596] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 480/2809]  eta: 0:23:12  lr: 0.000046  min_lr: 0.000000  loss: 4.2360 (4.2198)  loss_scale: 65536.0000 (64037.2557)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0656  max mem: 15572
Epoch: [8]  [ 490/2809]  eta: 0:23:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2360 (4.2200)  loss_scale: 65536.0000 (64067.7800)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1384  max mem: 15572
Epoch: [8]  [ 500/2809]  eta: 0:23:04  lr: 0.000046  min_lr: 0.000000  loss: 4.2087 (4.2196)  loss_scale: 65536.0000 (64097.0858)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.2040  max mem: 15572
Epoch: [8]  [ 510/2809]  eta: 0:22:54  lr: 0.000046  min_lr: 0.000000  loss: 4.2087 (4.2223)  loss_scale: 65536.0000 (64125.2446)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1289  max mem: 15572
Epoch: [8]  [ 520/2809]  eta: 0:22:47  lr: 0.000046  min_lr: 0.000000  loss: 4.3391 (4.2237)  loss_scale: 65536.0000 (64152.3225)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0819  max mem: 15572
[2025-01-13 00:25:37,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=147, lr=[4.449446895224366e-07, 4.449446895224366e-07, 6.356352707463381e-07, 6.356352707463381e-07, 9.080503867804831e-07, 9.080503867804831e-07, 1.297214838257833e-06, 1.297214838257833e-06, 1.8531640546540472e-06, 1.8531640546540472e-06, 2.6473772209343534e-06, 2.6473772209343534e-06, 3.7819674584776477e-06, 3.7819674584776477e-06, 5.402810654968069e-06, 5.402810654968069e-06, 7.718300935668669e-06, 7.718300935668669e-06, 1.1026144193812387e-05, 1.1026144193812387e-05, 1.5751634562589122e-05, 1.5751634562589122e-05, 2.2502335089413037e-05, 2.2502335089413037e-05, 3.214619298487577e-05, 3.214619298487577e-05, 4.5923132835536815e-05, 4.5923132835536815e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 00:25:37,830] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=27.872501305316415, CurrSamplesPerSec=29.721647256403262, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [ 530/2809]  eta: 0:22:45  lr: 0.000046  min_lr: 0.000000  loss: 4.3391 (4.2242)  loss_scale: 65536.0000 (64178.3804)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.1893  max mem: 15572
Epoch: [8]  [ 540/2809]  eta: 0:22:41  lr: 0.000046  min_lr: 0.000000  loss: 4.3881 (4.2271)  loss_scale: 65536.0000 (64203.4750)  weight_decay: 0.0500 (0.0500)  time: 0.6686  data: 0.2465  max mem: 15572
Epoch: [8]  [ 550/2809]  eta: 0:22:32  lr: 0.000046  min_lr: 0.000000  loss: 4.3412 (4.2288)  loss_scale: 65536.0000 (64227.6588)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1321  max mem: 15572
Epoch: [8]  [ 560/2809]  eta: 0:22:24  lr: 0.000046  min_lr: 0.000000  loss: 4.2000 (4.2266)  loss_scale: 65536.0000 (64250.9804)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0720  max mem: 15572
Epoch: [8]  [ 570/2809]  eta: 0:22:20  lr: 0.000046  min_lr: 0.000000  loss: 4.2173 (4.2252)  loss_scale: 65536.0000 (64273.4851)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1329  max mem: 15572
Epoch: [8]  [ 580/2809]  eta: 0:22:11  lr: 0.000046  min_lr: 0.000000  loss: 4.2173 (4.2229)  loss_scale: 65536.0000 (64295.2151)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1168  max mem: 15572
Epoch: [8]  [ 590/2809]  eta: 0:22:04  lr: 0.000046  min_lr: 0.000000  loss: 4.0940 (4.2204)  loss_scale: 65536.0000 (64316.2098)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0903  max mem: 15572
[2025-01-13 00:26:21,502] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:26:21,502] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 600/2809]  eta: 0:21:54  lr: 0.000046  min_lr: 0.000000  loss: 4.1834 (4.2195)  loss_scale: 65536.0000 (64445.5507)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0888  max mem: 15572
[2025-01-13 00:26:22,474] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23074
[2025-01-13 00:26:22,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:26:22,475] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 610/2809]  eta: 0:21:51  lr: 0.000046  min_lr: 0.000000  loss: 4.1841 (4.2210)  loss_scale: 65536.0000 (64570.6579)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1105  max mem: 15572
Epoch: [8]  [ 620/2809]  eta: 0:21:44  lr: 0.000046  min_lr: 0.000000  loss: 4.3274 (4.2233)  loss_scale: 65536.0000 (64586.2029)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1429  max mem: 15572
Epoch: [8]  [ 630/2809]  eta: 0:21:36  lr: 0.000046  min_lr: 0.000000  loss: 4.3274 (4.2249)  loss_scale: 65536.0000 (64601.2552)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1240  max mem: 15572
Epoch: [8]  [ 640/2809]  eta: 0:21:29  lr: 0.000046  min_lr: 0.000000  loss: 4.1478 (4.2218)  loss_scale: 65536.0000 (64615.8378)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1067  max mem: 15572
Epoch: [8]  [ 650/2809]  eta: 0:21:26  lr: 0.000046  min_lr: 0.000000  loss: 3.9962 (4.2204)  loss_scale: 65536.0000 (64629.9724)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1666  max mem: 15572
[2025-01-13 00:26:53,319] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23126
[2025-01-13 00:26:53,319] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 00:26:53,319] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 660/2809]  eta: 0:21:17  lr: 0.000046  min_lr: 0.000000  loss: 4.2113 (4.2205)  loss_scale: 65536.0000 (64296.6657)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1620  max mem: 15572
Epoch: [8]  [ 670/2809]  eta: 0:21:12  lr: 0.000046  min_lr: 0.000000  loss: 4.2493 (4.2211)  loss_scale: 32768.0000 (63826.7899)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1221  max mem: 15572
Epoch: [8]  [ 680/2809]  eta: 0:21:05  lr: 0.000046  min_lr: 0.000000  loss: 4.3177 (4.2240)  loss_scale: 32768.0000 (63370.7137)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1324  max mem: 15572
Epoch: [8]  [ 690/2809]  eta: 0:21:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2356 (4.2227)  loss_scale: 32768.0000 (62927.8379)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1325  max mem: 15572
Epoch: [8]  [ 700/2809]  eta: 0:20:58  lr: 0.000046  min_lr: 0.000000  loss: 4.0057 (4.2201)  loss_scale: 32768.0000 (62497.5977)  weight_decay: 0.0500 (0.0500)  time: 0.6736  data: 0.1929  max mem: 15572
Epoch: [8]  [ 710/2809]  eta: 0:20:51  lr: 0.000046  min_lr: 0.000000  loss: 4.0743 (4.2200)  loss_scale: 32768.0000 (62079.4599)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.1815  max mem: 15572
Epoch: [8]  [ 720/2809]  eta: 0:20:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2546 (4.2233)  loss_scale: 32768.0000 (61672.9209)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1342  max mem: 15572
Epoch: [8]  [ 730/2809]  eta: 0:20:39  lr: 0.000046  min_lr: 0.000000  loss: 4.3583 (4.2240)  loss_scale: 32768.0000 (61277.5048)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1458  max mem: 15572
Epoch: [8]  [ 740/2809]  eta: 0:20:31  lr: 0.000046  min_lr: 0.000000  loss: 4.1651 (4.2234)  loss_scale: 32768.0000 (60892.7611)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1154  max mem: 15572
Epoch: [8]  [ 750/2809]  eta: 0:20:26  lr: 0.000046  min_lr: 0.000000  loss: 4.1651 (4.2235)  loss_scale: 32768.0000 (60518.2636)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1103  max mem: 15572
Epoch: [8]  [ 760/2809]  eta: 0:20:22  lr: 0.000046  min_lr: 0.000000  loss: 4.2650 (4.2228)  loss_scale: 32768.0000 (60153.6084)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.1825  max mem: 15572
Epoch: [8]  [ 770/2809]  eta: 0:20:16  lr: 0.000046  min_lr: 0.000000  loss: 4.2650 (4.2236)  loss_scale: 32768.0000 (59798.4125)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1890  max mem: 15572
Epoch: [8]  [ 780/2809]  eta: 0:20:11  lr: 0.000046  min_lr: 0.000000  loss: 4.2755 (4.2239)  loss_scale: 32768.0000 (59452.3124)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1675  max mem: 15572
[2025-01-13 00:28:11,717] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:28:11,717] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 790/2809]  eta: 0:20:07  lr: 0.000046  min_lr: 0.000000  loss: 4.3579 (4.2246)  loss_scale: 32768.0000 (59446.3717)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.1831  max mem: 15572
Epoch: [8]  [ 800/2809]  eta: 0:19:59  lr: 0.000046  min_lr: 0.000000  loss: 4.3579 (4.2258)  loss_scale: 65536.0000 (59522.3970)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1329  max mem: 15572
Epoch: [8]  [ 810/2809]  eta: 0:19:53  lr: 0.000046  min_lr: 0.000000  loss: 4.4069 (4.2271)  loss_scale: 65536.0000 (59596.5475)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1088  max mem: 15572
Epoch: [8]  [ 820/2809]  eta: 0:19:46  lr: 0.000046  min_lr: 0.000000  loss: 4.2197 (4.2244)  loss_scale: 65536.0000 (59668.8916)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1393  max mem: 15572
Epoch: [8]  [ 830/2809]  eta: 0:19:42  lr: 0.000046  min_lr: 0.000000  loss: 4.1580 (4.2238)  loss_scale: 65536.0000 (59739.4946)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1777  max mem: 15572
Epoch: [8]  [ 840/2809]  eta: 0:19:34  lr: 0.000046  min_lr: 0.000000  loss: 4.2968 (4.2268)  loss_scale: 65536.0000 (59808.4185)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1521  max mem: 15572
Epoch: [8]  [ 850/2809]  eta: 0:19:28  lr: 0.000046  min_lr: 0.000000  loss: 4.2968 (4.2259)  loss_scale: 65536.0000 (59875.7227)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1171  max mem: 15572
Epoch: [8]  [ 860/2809]  eta: 0:19:25  lr: 0.000046  min_lr: 0.000000  loss: 4.2203 (4.2259)  loss_scale: 65536.0000 (59941.4634)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.2161  max mem: 15572
Epoch: [8]  [ 870/2809]  eta: 0:19:16  lr: 0.000046  min_lr: 0.000000  loss: 4.2777 (4.2257)  loss_scale: 65536.0000 (60005.6946)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1365  max mem: 15572
Epoch: [8]  [ 880/2809]  eta: 0:19:09  lr: 0.000046  min_lr: 0.000000  loss: 4.2360 (4.2256)  loss_scale: 65536.0000 (60068.4677)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0185  max mem: 15572
Epoch: [8]  [ 890/2809]  eta: 0:19:01  lr: 0.000046  min_lr: 0.000000  loss: 4.2613 (4.2265)  loss_scale: 65536.0000 (60129.8316)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0371  max mem: 15572
Epoch: [8]  [ 900/2809]  eta: 0:18:54  lr: 0.000046  min_lr: 0.000000  loss: 4.2642 (4.2255)  loss_scale: 65536.0000 (60189.8335)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0682  max mem: 15572
Epoch: [8]  [ 910/2809]  eta: 0:18:48  lr: 0.000046  min_lr: 0.000000  loss: 4.1715 (4.2237)  loss_scale: 65536.0000 (60248.5181)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1166  max mem: 15572
[2025-01-13 00:29:25,644] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:29:25,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:29:26,879] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23386
[2025-01-13 00:29:26,880] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:29:26,880] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 920/2809]  eta: 0:18:41  lr: 0.000046  min_lr: 0.000000  loss: 4.2730 (4.2243)  loss_scale: 65536.0000 (60519.4007)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1167  max mem: 15572
Epoch: [8]  [ 930/2809]  eta: 0:18:36  lr: 0.000046  min_lr: 0.000000  loss: 4.0687 (4.2201)  loss_scale: 65536.0000 (60573.2846)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1344  max mem: 15572
Epoch: [8]  [ 940/2809]  eta: 0:18:29  lr: 0.000046  min_lr: 0.000000  loss: 4.0482 (4.2199)  loss_scale: 65536.0000 (60626.0234)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1272  max mem: 15572
Epoch: [8]  [ 950/2809]  eta: 0:18:23  lr: 0.000046  min_lr: 0.000000  loss: 4.1178 (4.2180)  loss_scale: 65536.0000 (60677.6530)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1325  max mem: 15572
Epoch: [8]  [ 960/2809]  eta: 0:18:19  lr: 0.000046  min_lr: 0.000000  loss: 4.1818 (4.2176)  loss_scale: 65536.0000 (60728.2081)  weight_decay: 0.0500 (0.0500)  time: 0.6394  data: 0.1999  max mem: 15572
Epoch: [8]  [ 970/2809]  eta: 0:18:11  lr: 0.000046  min_lr: 0.000000  loss: 4.1938 (4.2172)  loss_scale: 65536.0000 (60777.7219)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1509  max mem: 15572
Epoch: [8]  [ 980/2809]  eta: 0:18:05  lr: 0.000046  min_lr: 0.000000  loss: 4.3767 (4.2184)  loss_scale: 65536.0000 (60826.2263)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1291  max mem: 15572
Epoch: [8]  [ 990/2809]  eta: 0:17:59  lr: 0.000046  min_lr: 0.000000  loss: 4.4160 (4.2206)  loss_scale: 65536.0000 (60873.7518)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1677  max mem: 15572
Epoch: [8]  [1000/2809]  eta: 0:17:53  lr: 0.000046  min_lr: 0.000000  loss: 4.2667 (4.2205)  loss_scale: 65536.0000 (60920.3277)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1562  max mem: 15572
Epoch: [8]  [1010/2809]  eta: 0:17:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2262 (4.2210)  loss_scale: 65536.0000 (60965.9822)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0905  max mem: 15572
Epoch: [8]  [1020/2809]  eta: 0:17:38  lr: 0.000046  min_lr: 0.000000  loss: 4.2546 (4.2200)  loss_scale: 65536.0000 (61010.7424)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0255  max mem: 15572
Epoch: [8]  [1030/2809]  eta: 0:17:32  lr: 0.000046  min_lr: 0.000000  loss: 4.1778 (4.2194)  loss_scale: 65536.0000 (61054.6343)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0858  max mem: 15572
Epoch: [8]  [1040/2809]  eta: 0:17:27  lr: 0.000046  min_lr: 0.000000  loss: 4.4434 (4.2207)  loss_scale: 65536.0000 (61097.6830)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1536  max mem: 15572
[2025-01-13 00:30:41,937] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:30:41,938] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:30:45,341] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23522
[2025-01-13 00:30:45,341] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:30:45,341] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1050/2809]  eta: 0:17:19  lr: 0.000046  min_lr: 0.000000  loss: 4.2750 (4.2201)  loss_scale: 65536.0000 (61576.4034)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0880  max mem: 15572
Epoch: [8]  [1060/2809]  eta: 0:17:14  lr: 0.000046  min_lr: 0.000000  loss: 4.1138 (4.2194)  loss_scale: 65536.0000 (61613.7229)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.0938  max mem: 15572
Epoch: [8]  [1070/2809]  eta: 0:17:07  lr: 0.000046  min_lr: 0.000000  loss: 4.1842 (4.2202)  loss_scale: 65536.0000 (61650.3455)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1397  max mem: 15572
[2025-01-13 00:31:02,201] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23550
[2025-01-13 00:31:02,202] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 00:31:02,203] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [1080/2809]  eta: 0:17:02  lr: 0.000046  min_lr: 0.000000  loss: 4.1838 (4.2203)  loss_scale: 65536.0000 (61595.3525)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1396  max mem: 15572
Epoch: [8]  [1090/2809]  eta: 0:16:58  lr: 0.000046  min_lr: 0.000000  loss: 4.2744 (4.2234)  loss_scale: 32768.0000 (61331.1237)  weight_decay: 0.0500 (0.0500)  time: 0.6720  data: 0.2115  max mem: 15572
Epoch: [8]  [1100/2809]  eta: 0:16:51  lr: 0.000046  min_lr: 0.000000  loss: 4.3400 (4.2215)  loss_scale: 32768.0000 (61071.6948)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.1720  max mem: 15572
Epoch: [8]  [1110/2809]  eta: 0:16:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2481 (4.2217)  loss_scale: 32768.0000 (60816.9361)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0890  max mem: 15572
Epoch: [8]  [1120/2809]  eta: 0:16:39  lr: 0.000046  min_lr: 0.000000  loss: 4.2245 (4.2199)  loss_scale: 32768.0000 (60566.7226)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1168  max mem: 15572
Epoch: [8]  [1130/2809]  eta: 0:16:34  lr: 0.000046  min_lr: 0.000000  loss: 4.1687 (4.2202)  loss_scale: 32768.0000 (60320.9337)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1527  max mem: 15572
Epoch: [8]  [1140/2809]  eta: 0:16:28  lr: 0.000046  min_lr: 0.000000  loss: 4.3522 (4.2221)  loss_scale: 32768.0000 (60079.4531)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1540  max mem: 15572
Epoch: [8]  [1150/2809]  eta: 0:16:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4099 (4.2235)  loss_scale: 32768.0000 (59842.1685)  weight_decay: 0.0500 (0.0500)  time: 0.6351  data: 0.1828  max mem: 15572
Epoch: [8]  [1160/2809]  eta: 0:16:17  lr: 0.000046  min_lr: 0.000000  loss: 4.2606 (4.2231)  loss_scale: 32768.0000 (59608.9716)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1709  max mem: 15572
Epoch: [8]  [1170/2809]  eta: 0:16:11  lr: 0.000046  min_lr: 0.000000  loss: 4.2606 (4.2242)  loss_scale: 32768.0000 (59379.7575)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1588  max mem: 15572
Epoch: [8]  [1180/2809]  eta: 0:16:04  lr: 0.000046  min_lr: 0.000000  loss: 4.3006 (4.2242)  loss_scale: 32768.0000 (59154.4251)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1009  max mem: 15572
Epoch: [8]  [1190/2809]  eta: 0:15:57  lr: 0.000046  min_lr: 0.000000  loss: 4.3014 (4.2239)  loss_scale: 32768.0000 (58932.8766)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0590  max mem: 15572
Epoch: [8]  [1200/2809]  eta: 0:15:52  lr: 0.000046  min_lr: 0.000000  loss: 4.3108 (4.2249)  loss_scale: 32768.0000 (58715.0175)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1543  max mem: 15572
[2025-01-13 00:32:18,068] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:32:18,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [1210/2809]  eta: 0:15:47  lr: 0.000046  min_lr: 0.000000  loss: 4.2993 (4.2247)  loss_scale: 32768.0000 (58608.9909)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1992  max mem: 15572
Epoch: [8]  [1220/2809]  eta: 0:15:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2123 (4.2242)  loss_scale: 65536.0000 (58665.7232)  weight_decay: 0.0500 (0.0500)  time: 0.6710  data: 0.2318  max mem: 15572
Epoch: [8]  [1230/2809]  eta: 0:15:35  lr: 0.000046  min_lr: 0.000000  loss: 4.3015 (4.2250)  loss_scale: 65536.0000 (58721.5337)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1376  max mem: 15572
Epoch: [8]  [1240/2809]  eta: 0:15:30  lr: 0.000046  min_lr: 0.000000  loss: 4.3870 (4.2249)  loss_scale: 65536.0000 (58776.4448)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1017  max mem: 15572
Epoch: [8]  [1250/2809]  eta: 0:15:24  lr: 0.000046  min_lr: 0.000000  loss: 4.3481 (4.2255)  loss_scale: 65536.0000 (58830.4780)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1801  max mem: 15572
Epoch: [8]  [1260/2809]  eta: 0:15:19  lr: 0.000046  min_lr: 0.000000  loss: 4.2331 (4.2259)  loss_scale: 65536.0000 (58883.6542)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.1787  max mem: 15572
Epoch: [8]  [1270/2809]  eta: 0:15:12  lr: 0.000046  min_lr: 0.000000  loss: 4.1285 (4.2252)  loss_scale: 65536.0000 (58935.9937)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1659  max mem: 15572
Epoch: [8]  [1280/2809]  eta: 0:15:06  lr: 0.000046  min_lr: 0.000000  loss: 4.2287 (4.2256)  loss_scale: 65536.0000 (58987.5160)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1358  max mem: 15572
Epoch: [8]  [1290/2809]  eta: 0:15:01  lr: 0.000046  min_lr: 0.000000  loss: 4.2517 (4.2248)  loss_scale: 65536.0000 (59038.2401)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1686  max mem: 15572
Epoch: [8]  [1300/2809]  eta: 0:14:55  lr: 0.000046  min_lr: 0.000000  loss: 4.2984 (4.2250)  loss_scale: 65536.0000 (59088.1845)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1494  max mem: 15572
Epoch: [8]  [1310/2809]  eta: 0:14:49  lr: 0.000046  min_lr: 0.000000  loss: 4.2253 (4.2231)  loss_scale: 65536.0000 (59137.3669)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0902  max mem: 15572
Epoch: [8]  [1320/2809]  eta: 0:14:43  lr: 0.000046  min_lr: 0.000000  loss: 4.0509 (4.2226)  loss_scale: 65536.0000 (59185.8047)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1020  max mem: 15572
Epoch: [8]  [1330/2809]  eta: 0:14:37  lr: 0.000046  min_lr: 0.000000  loss: 4.0916 (4.2222)  loss_scale: 65536.0000 (59233.5147)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1281  max mem: 15572
[2025-01-13 00:33:35,802] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:33:35,803] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:33:36,665] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23809
[2025-01-13 00:33:36,666] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:33:36,666] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1340/2809]  eta: 0:14:31  lr: 0.000046  min_lr: 0.000000  loss: 4.1522 (4.2215)  loss_scale: 65536.0000 (59378.2550)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1710  max mem: 15572
Epoch: [8]  [1350/2809]  eta: 0:14:24  lr: 0.000046  min_lr: 0.000000  loss: 4.1522 (4.2209)  loss_scale: 65536.0000 (59423.8342)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1333  max mem: 15572
Epoch: [8]  [1360/2809]  eta: 0:14:20  lr: 0.000046  min_lr: 0.000000  loss: 4.1485 (4.2209)  loss_scale: 65536.0000 (59468.7436)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1515  max mem: 15572
Epoch: [8]  [1370/2809]  eta: 0:14:13  lr: 0.000046  min_lr: 0.000000  loss: 4.1349 (4.2204)  loss_scale: 65536.0000 (59512.9978)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1692  max mem: 15572
Epoch: [8]  [1380/2809]  eta: 0:14:07  lr: 0.000046  min_lr: 0.000000  loss: 4.1520 (4.2200)  loss_scale: 65536.0000 (59556.6112)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0972  max mem: 15572
Epoch: [8]  [1390/2809]  eta: 0:14:01  lr: 0.000046  min_lr: 0.000000  loss: 4.3226 (4.2219)  loss_scale: 65536.0000 (59599.5974)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1123  max mem: 15572
Epoch: [8]  [1400/2809]  eta: 0:13:55  lr: 0.000046  min_lr: 0.000000  loss: 4.4239 (4.2226)  loss_scale: 65536.0000 (59641.9700)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1574  max mem: 15572
Epoch: [8]  [1410/2809]  eta: 0:13:48  lr: 0.000046  min_lr: 0.000000  loss: 4.2389 (4.2225)  loss_scale: 65536.0000 (59683.7420)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1155  max mem: 15572
Epoch: [8]  [1420/2809]  eta: 0:13:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2975 (4.2232)  loss_scale: 65536.0000 (59724.9261)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0740  max mem: 15572
Epoch: [8]  [1430/2809]  eta: 0:13:35  lr: 0.000046  min_lr: 0.000000  loss: 4.3855 (4.2245)  loss_scale: 65536.0000 (59765.5346)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0900  max mem: 15572
Epoch: [8]  [1440/2809]  eta: 0:13:29  lr: 0.000046  min_lr: 0.000000  loss: 4.2546 (4.2244)  loss_scale: 65536.0000 (59805.5795)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0999  max mem: 15572
Epoch: [8]  [1450/2809]  eta: 0:13:23  lr: 0.000046  min_lr: 0.000000  loss: 4.2695 (4.2251)  loss_scale: 65536.0000 (59845.0724)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.1016  max mem: 15572
Epoch: [8]  [1460/2809]  eta: 0:13:17  lr: 0.000046  min_lr: 0.000000  loss: 4.2695 (4.2247)  loss_scale: 65536.0000 (59884.0246)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1043  max mem: 15572
[2025-01-13 00:34:50,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:34:50,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:34:52,084] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23940
[2025-01-13 00:34:52,084] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:34:52,084] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1470/2809]  eta: 0:13:10  lr: 0.000046  min_lr: 0.000000  loss: 4.1644 (4.2246)  loss_scale: 65536.0000 (60011.5513)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0901  max mem: 15572
Epoch: [8]  [1480/2809]  eta: 0:13:04  lr: 0.000046  min_lr: 0.000000  loss: 4.1611 (4.2252)  loss_scale: 65536.0000 (60048.8535)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0991  max mem: 15572
Epoch: [8]  [1490/2809]  eta: 0:12:59  lr: 0.000046  min_lr: 0.000000  loss: 4.2483 (4.2249)  loss_scale: 65536.0000 (60085.6553)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1316  max mem: 15572
Epoch: [8]  [1500/2809]  eta: 0:12:52  lr: 0.000046  min_lr: 0.000000  loss: 4.2880 (4.2248)  loss_scale: 65536.0000 (60121.9667)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0998  max mem: 15572
Epoch: [8]  [1510/2809]  eta: 0:12:47  lr: 0.000046  min_lr: 0.000000  loss: 4.3744 (4.2259)  loss_scale: 65536.0000 (60157.7975)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1113  max mem: 15572
Epoch: [8]  [1520/2809]  eta: 0:12:41  lr: 0.000046  min_lr: 0.000000  loss: 4.2113 (4.2242)  loss_scale: 65536.0000 (60193.1571)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1534  max mem: 15572
[2025-01-13 00:35:26,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=154, lr=[4.427880249960953e-07, 4.427880249960953e-07, 6.325543214229933e-07, 6.325543214229933e-07, 9.036490306042763e-07, 9.036490306042763e-07, 1.2909271865775375e-06, 1.2909271865775375e-06, 1.844181695110768e-06, 1.844181695110768e-06, 2.6345452787296686e-06, 2.6345452787296686e-06, 3.7636361124709554e-06, 3.7636361124709554e-06, 5.3766230178156515e-06, 5.3766230178156515e-06, 7.68089002545093e-06, 7.68089002545093e-06, 1.0972700036358473e-05, 1.0972700036358473e-05, 1.567528576622639e-05, 1.567528576622639e-05, 2.2393265380323416e-05, 2.2393265380323416e-05, 3.1990379114747744e-05, 3.1990379114747744e-05, 4.5700541592496775e-05, 4.5700541592496775e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 00:35:26,683] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=27.876543624201926, CurrSamplesPerSec=27.729499408846042, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [1530/2809]  eta: 0:12:35  lr: 0.000046  min_lr: 0.000000  loss: 4.2227 (4.2249)  loss_scale: 65536.0000 (60228.0549)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1818  max mem: 15572
Epoch: [8]  [1540/2809]  eta: 0:12:30  lr: 0.000046  min_lr: 0.000000  loss: 4.4065 (4.2254)  loss_scale: 65536.0000 (60262.4997)  weight_decay: 0.0500 (0.0500)  time: 0.6772  data: 0.2379  max mem: 15572
Epoch: [8]  [1550/2809]  eta: 0:12:25  lr: 0.000046  min_lr: 0.000000  loss: 4.2099 (4.2251)  loss_scale: 65536.0000 (60296.5003)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.2211  max mem: 15572
Epoch: [8]  [1560/2809]  eta: 0:12:19  lr: 0.000046  min_lr: 0.000000  loss: 4.1144 (4.2244)  loss_scale: 65536.0000 (60330.0653)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1676  max mem: 15572
Epoch: [8]  [1570/2809]  eta: 0:12:13  lr: 0.000046  min_lr: 0.000000  loss: 4.1270 (4.2243)  loss_scale: 65536.0000 (60363.2031)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1305  max mem: 15572
Epoch: [8]  [1580/2809]  eta: 0:12:07  lr: 0.000046  min_lr: 0.000000  loss: 4.2952 (4.2255)  loss_scale: 65536.0000 (60395.9216)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1170  max mem: 15572
Epoch: [8]  [1590/2809]  eta: 0:12:00  lr: 0.000046  min_lr: 0.000000  loss: 4.3265 (4.2260)  loss_scale: 65536.0000 (60428.2288)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.1032  max mem: 15572
[2025-01-13 00:36:08,119] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:36:08,120] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:36:09,612] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24071
[2025-01-13 00:36:09,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:36:09,613] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1600/2809]  eta: 0:11:54  lr: 0.000046  min_lr: 0.000000  loss: 4.3434 (4.2262)  loss_scale: 65536.0000 (60542.0012)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0646  max mem: 15572
Epoch: [8]  [1610/2809]  eta: 0:11:48  lr: 0.000046  min_lr: 0.000000  loss: 4.1313 (4.2255)  loss_scale: 65536.0000 (60573.0006)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1316  max mem: 15572
Epoch: [8]  [1620/2809]  eta: 0:11:41  lr: 0.000046  min_lr: 0.000000  loss: 4.2348 (4.2264)  loss_scale: 65536.0000 (60603.6175)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0997  max mem: 15572
Epoch: [8]  [1630/2809]  eta: 0:11:34  lr: 0.000046  min_lr: 0.000000  loss: 4.5058 (4.2282)  loss_scale: 65536.0000 (60633.8590)  weight_decay: 0.0500 (0.0500)  time: 0.3894  data: 0.0004  max mem: 15572
Epoch: [8]  [1640/2809]  eta: 0:11:27  lr: 0.000046  min_lr: 0.000000  loss: 4.4089 (4.2275)  loss_scale: 65536.0000 (60663.7319)  weight_decay: 0.0500 (0.0500)  time: 0.4193  data: 0.0005  max mem: 15572
Epoch: [8]  [1650/2809]  eta: 0:11:20  lr: 0.000046  min_lr: 0.000000  loss: 4.2093 (4.2269)  loss_scale: 65536.0000 (60693.2429)  weight_decay: 0.0500 (0.0500)  time: 0.4699  data: 0.0007  max mem: 15572
Epoch: [8]  [1660/2809]  eta: 0:11:15  lr: 0.000046  min_lr: 0.000000  loss: 4.1112 (4.2262)  loss_scale: 65536.0000 (60722.3986)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1168  max mem: 15572
Epoch: [8]  [1670/2809]  eta: 0:11:10  lr: 0.000046  min_lr: 0.000000  loss: 4.1516 (4.2262)  loss_scale: 65536.0000 (60751.2053)  weight_decay: 0.0500 (0.0500)  time: 0.7058  data: 0.2374  max mem: 15572
Epoch: [8]  [1680/2809]  eta: 0:11:04  lr: 0.000046  min_lr: 0.000000  loss: 4.2821 (4.2265)  loss_scale: 65536.0000 (60779.6692)  weight_decay: 0.0500 (0.0500)  time: 0.6725  data: 0.1926  max mem: 15572
Epoch: [8]  [1690/2809]  eta: 0:11:00  lr: 0.000046  min_lr: 0.000000  loss: 4.3525 (4.2274)  loss_scale: 65536.0000 (60807.7966)  weight_decay: 0.0500 (0.0500)  time: 0.6953  data: 0.2095  max mem: 15572
Epoch: [8]  [1700/2809]  eta: 0:10:54  lr: 0.000046  min_lr: 0.000000  loss: 4.2205 (4.2271)  loss_scale: 65536.0000 (60835.5932)  weight_decay: 0.0500 (0.0500)  time: 0.7058  data: 0.2208  max mem: 15572
Epoch: [8]  [1710/2809]  eta: 0:10:49  lr: 0.000046  min_lr: 0.000000  loss: 4.4507 (4.2282)  loss_scale: 65536.0000 (60863.0649)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.1940  max mem: 15572
Epoch: [8]  [1720/2809]  eta: 0:10:43  lr: 0.000046  min_lr: 0.000000  loss: 4.2166 (4.2273)  loss_scale: 65536.0000 (60890.2173)  weight_decay: 0.0500 (0.0500)  time: 0.6641  data: 0.1845  max mem: 15572
[2025-01-13 00:37:28,494] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:37:28,495] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [1730/2809]  eta: 0:10:39  lr: 0.000046  min_lr: 0.000000  loss: 3.9735 (4.2274)  loss_scale: 65536.0000 (61030.6366)  weight_decay: 0.0500 (0.0500)  time: 0.7102  data: 0.2223  max mem: 15572
[2025-01-13 00:37:34,267] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24207
[2025-01-13 00:37:34,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:37:34,269] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1740/2809]  eta: 0:10:33  lr: 0.000046  min_lr: 0.000000  loss: 4.3182 (4.2280)  loss_scale: 65536.0000 (61207.0856)  weight_decay: 0.0500 (0.0500)  time: 0.7473  data: 0.2622  max mem: 15572
Epoch: [8]  [1750/2809]  eta: 0:10:28  lr: 0.000046  min_lr: 0.000000  loss: 4.4138 (4.2282)  loss_scale: 65536.0000 (61231.8081)  weight_decay: 0.0500 (0.0500)  time: 0.6958  data: 0.2165  max mem: 15572
Epoch: [8]  [1760/2809]  eta: 0:10:23  lr: 0.000046  min_lr: 0.000000  loss: 4.2598 (4.2271)  loss_scale: 65536.0000 (61256.2499)  weight_decay: 0.0500 (0.0500)  time: 0.7176  data: 0.2438  max mem: 15572
Epoch: [8]  [1770/2809]  eta: 0:10:16  lr: 0.000046  min_lr: 0.000000  loss: 4.1843 (4.2263)  loss_scale: 65536.0000 (61280.4156)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1408  max mem: 15572
Epoch: [8]  [1780/2809]  eta: 0:10:09  lr: 0.000046  min_lr: 0.000000  loss: 4.2425 (4.2268)  loss_scale: 65536.0000 (61304.3099)  weight_decay: 0.0500 (0.0500)  time: 0.4101  data: 0.0004  max mem: 15572
Epoch: [8]  [1790/2809]  eta: 0:10:02  lr: 0.000046  min_lr: 0.000000  loss: 4.2930 (4.2273)  loss_scale: 65536.0000 (61327.9375)  weight_decay: 0.0500 (0.0500)  time: 0.4235  data: 0.0005  max mem: 15572
Epoch: [8]  [1800/2809]  eta: 0:09:56  lr: 0.000046  min_lr: 0.000000  loss: 4.2805 (4.2274)  loss_scale: 65536.0000 (61351.3026)  weight_decay: 0.0500 (0.0500)  time: 0.4673  data: 0.0030  max mem: 15572
Epoch: [8]  [1810/2809]  eta: 0:09:50  lr: 0.000046  min_lr: 0.000000  loss: 4.2476 (4.2269)  loss_scale: 65536.0000 (61374.4097)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0825  max mem: 15572
Epoch: [8]  [1820/2809]  eta: 0:09:43  lr: 0.000046  min_lr: 0.000000  loss: 4.1196 (4.2251)  loss_scale: 65536.0000 (61397.2630)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.1001  max mem: 15572
Epoch: [8]  [1830/2809]  eta: 0:09:37  lr: 0.000046  min_lr: 0.000000  loss: 4.2582 (4.2264)  loss_scale: 65536.0000 (61419.8667)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0980  max mem: 15572
Epoch: [8]  [1840/2809]  eta: 0:09:32  lr: 0.000046  min_lr: 0.000000  loss: 4.3379 (4.2266)  loss_scale: 65536.0000 (61442.2249)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1565  max mem: 15572
Epoch: [8]  [1850/2809]  eta: 0:09:26  lr: 0.000046  min_lr: 0.000000  loss: 4.2040 (4.2261)  loss_scale: 65536.0000 (61464.3414)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1585  max mem: 15572
Epoch: [8]  [1860/2809]  eta: 0:09:20  lr: 0.000046  min_lr: 0.000000  loss: 4.1869 (4.2261)  loss_scale: 65536.0000 (61486.2203)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1333  max mem: 15572
[2025-01-13 00:38:44,840] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:38:44,840] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:38:47,621] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24340
[2025-01-13 00:38:47,622] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:38:47,622] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1870/2809]  eta: 0:09:14  lr: 0.000046  min_lr: 0.000000  loss: 4.2919 (4.2259)  loss_scale: 65536.0000 (61647.9743)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1012  max mem: 15572
Epoch: [8]  [1880/2809]  eta: 0:09:08  lr: 0.000046  min_lr: 0.000000  loss: 3.9975 (4.2251)  loss_scale: 65536.0000 (61668.6443)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1248  max mem: 15572
Epoch: [8]  [1890/2809]  eta: 0:09:02  lr: 0.000046  min_lr: 0.000000  loss: 3.9975 (4.2242)  loss_scale: 65536.0000 (61689.0957)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1398  max mem: 15572
Epoch: [8]  [1900/2809]  eta: 0:08:56  lr: 0.000046  min_lr: 0.000000  loss: 4.1874 (4.2238)  loss_scale: 65536.0000 (61709.3319)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1030  max mem: 15572
Epoch: [8]  [1910/2809]  eta: 0:08:50  lr: 0.000046  min_lr: 0.000000  loss: 4.0655 (4.2229)  loss_scale: 65536.0000 (61729.3564)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1112  max mem: 15572
Epoch: [8]  [1920/2809]  eta: 0:08:45  lr: 0.000046  min_lr: 0.000000  loss: 4.1161 (4.2233)  loss_scale: 65536.0000 (61749.1723)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1587  max mem: 15572
Epoch: [8]  [1930/2809]  eta: 0:08:38  lr: 0.000046  min_lr: 0.000000  loss: 4.1972 (4.2231)  loss_scale: 65536.0000 (61768.7830)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1361  max mem: 15572
Epoch: [8]  [1940/2809]  eta: 0:08:32  lr: 0.000046  min_lr: 0.000000  loss: 4.3402 (4.2237)  loss_scale: 65536.0000 (61788.1917)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1185  max mem: 15572
Epoch: [8]  [1950/2809]  eta: 0:08:27  lr: 0.000046  min_lr: 0.000000  loss: 4.3413 (4.2236)  loss_scale: 65536.0000 (61807.4013)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1684  max mem: 15572
Epoch: [8]  [1960/2809]  eta: 0:08:21  lr: 0.000046  min_lr: 0.000000  loss: 4.3413 (4.2240)  loss_scale: 65536.0000 (61826.4151)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1876  max mem: 15572
Epoch: [8]  [1970/2809]  eta: 0:08:15  lr: 0.000046  min_lr: 0.000000  loss: 4.0296 (4.2234)  loss_scale: 65536.0000 (61845.2359)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1532  max mem: 15572
Epoch: [8]  [1980/2809]  eta: 0:08:09  lr: 0.000046  min_lr: 0.000000  loss: 3.9992 (4.2227)  loss_scale: 65536.0000 (61863.8667)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1520  max mem: 15572
Epoch: [8]  [1990/2809]  eta: 0:08:03  lr: 0.000046  min_lr: 0.000000  loss: 4.0612 (4.2223)  loss_scale: 65536.0000 (61882.3104)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1806  max mem: 15572
[2025-01-13 00:40:04,634] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:40:04,635] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [2000/2809]  eta: 0:07:57  lr: 0.000046  min_lr: 0.000000  loss: 4.2521 (4.2231)  loss_scale: 65536.0000 (62031.5762)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1446  max mem: 15572
[2025-01-13 00:40:08,214] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24476
[2025-01-13 00:40:08,215] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:40:08,215] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2010/2809]  eta: 0:07:52  lr: 0.000046  min_lr: 0.000000  loss: 4.3999 (4.2231)  loss_scale: 65536.0000 (62146.7688)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1218  max mem: 15572
Epoch: [8]  [2020/2809]  eta: 0:07:46  lr: 0.000046  min_lr: 0.000000  loss: 4.2476 (4.2232)  loss_scale: 65536.0000 (62163.5388)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1143  max mem: 15572
Epoch: [8]  [2030/2809]  eta: 0:07:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2249 (4.2232)  loss_scale: 65536.0000 (62180.1438)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1188  max mem: 15572
Epoch: [8]  [2040/2809]  eta: 0:07:34  lr: 0.000046  min_lr: 0.000000  loss: 4.2526 (4.2235)  loss_scale: 65536.0000 (62196.5860)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1249  max mem: 15572
Epoch: [8]  [2050/2809]  eta: 0:07:28  lr: 0.000046  min_lr: 0.000000  loss: 4.1167 (4.2228)  loss_scale: 65536.0000 (62212.8679)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1528  max mem: 15572
Epoch: [8]  [2060/2809]  eta: 0:07:22  lr: 0.000046  min_lr: 0.000000  loss: 4.1211 (4.2229)  loss_scale: 65536.0000 (62228.9918)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.1969  max mem: 15572
Epoch: [8]  [2070/2809]  eta: 0:07:16  lr: 0.000046  min_lr: 0.000000  loss: 4.2435 (4.2230)  loss_scale: 65536.0000 (62244.9599)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1427  max mem: 15572
Epoch: [8]  [2080/2809]  eta: 0:07:10  lr: 0.000046  min_lr: 0.000000  loss: 4.2632 (4.2237)  loss_scale: 65536.0000 (62260.7746)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1500  max mem: 15572
Epoch: [8]  [2090/2809]  eta: 0:07:05  lr: 0.000046  min_lr: 0.000000  loss: 4.1905 (4.2229)  loss_scale: 65536.0000 (62276.4381)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1956  max mem: 15572
Epoch: [8]  [2100/2809]  eta: 0:06:58  lr: 0.000046  min_lr: 0.000000  loss: 4.0148 (4.2223)  loss_scale: 65536.0000 (62291.9524)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1176  max mem: 15572
Epoch: [8]  [2110/2809]  eta: 0:06:53  lr: 0.000046  min_lr: 0.000000  loss: 4.0181 (4.2217)  loss_scale: 65536.0000 (62307.3198)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0883  max mem: 15572
Epoch: [8]  [2120/2809]  eta: 0:06:46  lr: 0.000046  min_lr: 0.000000  loss: 4.0861 (4.2209)  loss_scale: 65536.0000 (62322.5422)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1030  max mem: 15572
Epoch: [8]  [2130/2809]  eta: 0:06:40  lr: 0.000046  min_lr: 0.000000  loss: 4.1031 (4.2202)  loss_scale: 65536.0000 (62337.6218)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0946  max mem: 15572
[2025-01-13 00:41:24,388] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:41:24,388] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [2140/2809]  eta: 0:06:35  lr: 0.000046  min_lr: 0.000000  loss: 4.0448 (4.2198)  loss_scale: 65536.0000 (62597.4404)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1439  max mem: 15572
[2025-01-13 00:41:30,140] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24614
[2025-01-13 00:41:30,140] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:41:30,140] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2150/2809]  eta: 0:06:29  lr: 0.000046  min_lr: 0.000000  loss: 4.2006 (4.2202)  loss_scale: 65536.0000 (62641.5695)  weight_decay: 0.0500 (0.0500)  time: 0.6729  data: 0.2182  max mem: 15572
Epoch: [8]  [2160/2809]  eta: 0:06:23  lr: 0.000046  min_lr: 0.000000  loss: 4.3228 (4.2203)  loss_scale: 65536.0000 (62654.9634)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1984  max mem: 15572
Epoch: [8]  [2170/2809]  eta: 0:06:17  lr: 0.000046  min_lr: 0.000000  loss: 4.1977 (4.2197)  loss_scale: 65536.0000 (62668.2340)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1200  max mem: 15572
Epoch: [8]  [2180/2809]  eta: 0:06:11  lr: 0.000046  min_lr: 0.000000  loss: 4.1671 (4.2202)  loss_scale: 65536.0000 (62681.3829)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1216  max mem: 15572
Epoch: [8]  [2190/2809]  eta: 0:06:05  lr: 0.000046  min_lr: 0.000000  loss: 4.2911 (4.2206)  loss_scale: 65536.0000 (62694.4117)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1015  max mem: 15572
Epoch: [8]  [2200/2809]  eta: 0:05:59  lr: 0.000046  min_lr: 0.000000  loss: 4.1633 (4.2202)  loss_scale: 65536.0000 (62707.3221)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0615  max mem: 15572
Epoch: [8]  [2210/2809]  eta: 0:05:53  lr: 0.000046  min_lr: 0.000000  loss: 4.2589 (4.2203)  loss_scale: 65536.0000 (62720.1158)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0618  max mem: 15572
Epoch: [8]  [2220/2809]  eta: 0:05:47  lr: 0.000046  min_lr: 0.000000  loss: 4.2101 (4.2196)  loss_scale: 65536.0000 (62732.7942)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1021  max mem: 15572
Epoch: [8]  [2230/2809]  eta: 0:05:41  lr: 0.000046  min_lr: 0.000000  loss: 4.1505 (4.2197)  loss_scale: 65536.0000 (62745.3590)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1451  max mem: 15572
Epoch: [8]  [2240/2809]  eta: 0:05:36  lr: 0.000046  min_lr: 0.000000  loss: 4.2307 (4.2194)  loss_scale: 65536.0000 (62757.8117)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1313  max mem: 15572
Epoch: [8]  [2250/2809]  eta: 0:05:30  lr: 0.000046  min_lr: 0.000000  loss: 4.1721 (4.2194)  loss_scale: 65536.0000 (62770.1537)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1288  max mem: 15572
Epoch: [8]  [2260/2809]  eta: 0:05:24  lr: 0.000046  min_lr: 0.000000  loss: 4.2725 (4.2201)  loss_scale: 65536.0000 (62782.3866)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1098  max mem: 15572
Epoch: [8]  [2270/2809]  eta: 0:05:18  lr: 0.000046  min_lr: 0.000000  loss: 4.3118 (4.2207)  loss_scale: 65536.0000 (62794.5117)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0443  max mem: 15572
[2025-01-13 00:42:45,234] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:42:45,234] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:42:47,887] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24747
[2025-01-13 00:42:47,887] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:42:47,887] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2280/2809]  eta: 0:05:12  lr: 0.000046  min_lr: 0.000000  loss: 4.2453 (4.2208)  loss_scale: 65536.0000 (62921.4555)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0529  max mem: 15572
Epoch: [8]  [2290/2809]  eta: 0:05:06  lr: 0.000046  min_lr: 0.000000  loss: 4.1969 (4.2205)  loss_scale: 65536.0000 (62932.8677)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1623  max mem: 15572
Epoch: [8]  [2300/2809]  eta: 0:05:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2684 (4.2205)  loss_scale: 65536.0000 (62944.1808)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1586  max mem: 15572
Epoch: [8]  [2310/2809]  eta: 0:04:54  lr: 0.000046  min_lr: 0.000000  loss: 4.0638 (4.2196)  loss_scale: 65536.0000 (62955.3959)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0710  max mem: 15572
Epoch: [8]  [2320/2809]  eta: 0:04:48  lr: 0.000046  min_lr: 0.000000  loss: 4.0638 (4.2193)  loss_scale: 65536.0000 (62966.5144)  weight_decay: 0.0500 (0.0500)  time: 0.5111  data: 0.0856  max mem: 15572
[2025-01-13 00:43:15,236] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24797
[2025-01-13 00:43:15,236] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 00:43:15,237] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [2330/2809]  eta: 0:04:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2631 (4.2189)  loss_scale: 65536.0000 (62893.1926)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.1178  max mem: 15572
Epoch: [8]  [2340/2809]  eta: 0:04:36  lr: 0.000046  min_lr: 0.000000  loss: 4.1203 (4.2182)  loss_scale: 32768.0000 (62764.5075)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1696  max mem: 15572
Epoch: [8]  [2350/2809]  eta: 0:04:30  lr: 0.000046  min_lr: 0.000000  loss: 4.0763 (4.2178)  loss_scale: 32768.0000 (62636.9171)  weight_decay: 0.0500 (0.0500)  time: 0.6747  data: 0.2229  max mem: 15572
Epoch: [8]  [2360/2809]  eta: 0:04:24  lr: 0.000045  min_lr: 0.000000  loss: 4.1199 (4.2173)  loss_scale: 32768.0000 (62510.4075)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.2039  max mem: 15572
Epoch: [8]  [2370/2809]  eta: 0:04:18  lr: 0.000045  min_lr: 0.000000  loss: 4.1450 (4.2171)  loss_scale: 32768.0000 (62384.9650)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1447  max mem: 15572
Epoch: [8]  [2380/2809]  eta: 0:04:13  lr: 0.000045  min_lr: 0.000000  loss: 4.1904 (4.2176)  loss_scale: 32768.0000 (62260.5762)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1535  max mem: 15572
Epoch: [8]  [2390/2809]  eta: 0:04:07  lr: 0.000045  min_lr: 0.000000  loss: 4.2203 (4.2173)  loss_scale: 32768.0000 (62137.2279)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1544  max mem: 15572
Epoch: [8]  [2400/2809]  eta: 0:04:01  lr: 0.000045  min_lr: 0.000000  loss: 4.2203 (4.2178)  loss_scale: 32768.0000 (62014.9071)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0734  max mem: 15572
Epoch: [8]  [2410/2809]  eta: 0:03:55  lr: 0.000045  min_lr: 0.000000  loss: 4.3189 (4.2174)  loss_scale: 32768.0000 (61893.6010)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0599  max mem: 15572
Epoch: [8]  [2420/2809]  eta: 0:03:49  lr: 0.000045  min_lr: 0.000000  loss: 4.2975 (4.2174)  loss_scale: 32768.0000 (61773.2970)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1697  max mem: 15572
Epoch: [8]  [2430/2809]  eta: 0:03:43  lr: 0.000045  min_lr: 0.000000  loss: 4.1982 (4.2164)  loss_scale: 32768.0000 (61653.9827)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1429  max mem: 15572
Epoch: [8]  [2440/2809]  eta: 0:03:37  lr: 0.000045  min_lr: 0.000000  loss: 4.0328 (4.2153)  loss_scale: 32768.0000 (61535.6460)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.1149  max mem: 15572
Epoch: [8]  [2450/2809]  eta: 0:03:31  lr: 0.000045  min_lr: 0.000000  loss: 4.2097 (4.2155)  loss_scale: 32768.0000 (61418.2750)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1940  max mem: 15572
[2025-01-13 00:44:31,159] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:44:31,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [2460/2809]  eta: 0:03:25  lr: 0.000045  min_lr: 0.000000  loss: 4.2966 (4.2158)  loss_scale: 32768.0000 (61395.0622)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.2254  max mem: 15572
Epoch: [8]  [2470/2809]  eta: 0:03:19  lr: 0.000045  min_lr: 0.000000  loss: 4.2966 (4.2163)  loss_scale: 65536.0000 (61411.8203)  weight_decay: 0.0500 (0.0500)  time: 0.6513  data: 0.2012  max mem: 15572
Epoch: [8]  [2480/2809]  eta: 0:03:13  lr: 0.000045  min_lr: 0.000000  loss: 4.2907 (4.2168)  loss_scale: 65536.0000 (61428.4434)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0748  max mem: 15572
Epoch: [8]  [2490/2809]  eta: 0:03:08  lr: 0.000045  min_lr: 0.000000  loss: 4.2907 (4.2173)  loss_scale: 65536.0000 (61444.9330)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0696  max mem: 15572
Epoch: [8]  [2500/2809]  eta: 0:03:02  lr: 0.000045  min_lr: 0.000000  loss: 4.2914 (4.2174)  loss_scale: 65536.0000 (61461.2907)  weight_decay: 0.0500 (0.0500)  time: 0.6406  data: 0.1898  max mem: 15572
Epoch: [8]  [2510/2809]  eta: 0:02:56  lr: 0.000045  min_lr: 0.000000  loss: 4.2985 (4.2178)  loss_scale: 65536.0000 (61477.5181)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1849  max mem: 15572
Epoch: [8]  [2520/2809]  eta: 0:02:50  lr: 0.000045  min_lr: 0.000000  loss: 4.1826 (4.2171)  loss_scale: 65536.0000 (61493.6168)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1738  max mem: 15572
[2025-01-13 00:45:15,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=161, lr=[4.404113593984157e-07, 4.404113593984157e-07, 6.291590848548796e-07, 6.291590848548796e-07, 8.987986926498281e-07, 8.987986926498281e-07, 1.2839981323568972e-06, 1.2839981323568972e-06, 1.8342830462241392e-06, 1.8342830462241392e-06, 2.62040435174877e-06, 2.62040435174877e-06, 3.743434788212529e-06, 3.743434788212529e-06, 5.347763983160757e-06, 5.347763983160757e-06, 7.639662833086796e-06, 7.639662833086796e-06, 1.0913804047266852e-05, 1.0913804047266852e-05, 1.5591148638952645e-05, 1.5591148638952645e-05, 2.2273069484218067e-05, 2.2273069484218067e-05, 3.1818670691740094e-05, 3.1818670691740094e-05, 4.5455243845343e-05, 4.5455243845343e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 00:45:15,087] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=27.880409201952983, CurrSamplesPerSec=32.354989402850464, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [2530/2809]  eta: 0:02:44  lr: 0.000045  min_lr: 0.000000  loss: 4.2601 (4.2179)  loss_scale: 65536.0000 (61509.5883)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.1937  max mem: 15572
Epoch: [8]  [2540/2809]  eta: 0:02:38  lr: 0.000045  min_lr: 0.000000  loss: 4.3434 (4.2181)  loss_scale: 65536.0000 (61525.4341)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0806  max mem: 15572
Epoch: [8]  [2550/2809]  eta: 0:02:32  lr: 0.000045  min_lr: 0.000000  loss: 4.2149 (4.2173)  loss_scale: 65536.0000 (61541.1556)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0962  max mem: 15572
Epoch: [8]  [2560/2809]  eta: 0:02:26  lr: 0.000045  min_lr: 0.000000  loss: 4.2446 (4.2178)  loss_scale: 65536.0000 (61556.7544)  weight_decay: 0.0500 (0.0500)  time: 0.6711  data: 0.2217  max mem: 15572
Epoch: [8]  [2570/2809]  eta: 0:02:21  lr: 0.000045  min_lr: 0.000000  loss: 4.2985 (4.2177)  loss_scale: 65536.0000 (61572.2318)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1575  max mem: 15572
Epoch: [8]  [2580/2809]  eta: 0:02:15  lr: 0.000045  min_lr: 0.000000  loss: 4.2137 (4.2172)  loss_scale: 65536.0000 (61587.5893)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0937  max mem: 15572
[2025-01-13 00:45:48,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:45:48,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:45:52,330] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25061
[2025-01-13 00:45:52,331] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:45:52,331] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2590/2809]  eta: 0:02:09  lr: 0.000045  min_lr: 0.000000  loss: 4.0884 (4.2168)  loss_scale: 65536.0000 (61779.8842)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1097  max mem: 15572
Epoch: [8]  [2600/2809]  eta: 0:02:03  lr: 0.000045  min_lr: 0.000000  loss: 4.1374 (4.2167)  loss_scale: 65536.0000 (61794.3253)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0875  max mem: 15572
Epoch: [8]  [2610/2809]  eta: 0:01:57  lr: 0.000045  min_lr: 0.000000  loss: 4.2476 (4.2169)  loss_scale: 65536.0000 (61808.6557)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1224  max mem: 15572
Epoch: [8]  [2620/2809]  eta: 0:01:51  lr: 0.000045  min_lr: 0.000000  loss: 4.1535 (4.2170)  loss_scale: 65536.0000 (61822.8768)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1601  max mem: 15572
Epoch: [8]  [2630/2809]  eta: 0:01:45  lr: 0.000045  min_lr: 0.000000  loss: 4.1095 (4.2170)  loss_scale: 65536.0000 (61836.9897)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1249  max mem: 15572
Epoch: [8]  [2640/2809]  eta: 0:01:39  lr: 0.000045  min_lr: 0.000000  loss: 4.0266 (4.2169)  loss_scale: 65536.0000 (61850.9958)  weight_decay: 0.0500 (0.0500)  time: 0.4975  data: 0.0478  max mem: 15572
Epoch: [8]  [2650/2809]  eta: 0:01:33  lr: 0.000045  min_lr: 0.000000  loss: 4.0266 (4.2158)  loss_scale: 65536.0000 (61864.8963)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0685  max mem: 15572
Epoch: [8]  [2660/2809]  eta: 0:01:27  lr: 0.000045  min_lr: 0.000000  loss: 4.0778 (4.2155)  loss_scale: 65536.0000 (61878.6922)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1588  max mem: 15572
Epoch: [8]  [2670/2809]  eta: 0:01:21  lr: 0.000045  min_lr: 0.000000  loss: 4.2117 (4.2157)  loss_scale: 65536.0000 (61892.3849)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1227  max mem: 15572
Epoch: [8]  [2680/2809]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000000  loss: 4.2113 (4.2152)  loss_scale: 65536.0000 (61905.9754)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1083  max mem: 15572
Epoch: [8]  [2690/2809]  eta: 0:01:10  lr: 0.000045  min_lr: 0.000000  loss: 4.0708 (4.2150)  loss_scale: 65536.0000 (61919.4649)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0880  max mem: 15572
Epoch: [8]  [2700/2809]  eta: 0:01:04  lr: 0.000045  min_lr: 0.000000  loss: 4.2590 (4.2153)  loss_scale: 65536.0000 (61932.8545)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1065  max mem: 15572
Epoch: [8]  [2710/2809]  eta: 0:00:58  lr: 0.000045  min_lr: 0.000000  loss: 4.3782 (4.2155)  loss_scale: 65536.0000 (61946.1453)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1045  max mem: 15572
[2025-01-13 00:47:05,306] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:47:05,306] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [2720/2809]  eta: 0:00:52  lr: 0.000045  min_lr: 0.000000  loss: 4.1404 (4.2143)  loss_scale: 65536.0000 (62031.5943)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0493  max mem: 15572
[2025-01-13 00:47:09,790] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25198
[2025-01-13 00:47:09,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:47:09,791] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2730/2809]  eta: 0:00:46  lr: 0.000045  min_lr: 0.000000  loss: 4.1404 (4.2144)  loss_scale: 65536.0000 (62164.4116)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1003  max mem: 15572
Epoch: [8]  [2740/2809]  eta: 0:00:40  lr: 0.000045  min_lr: 0.000000  loss: 4.4129 (4.2151)  loss_scale: 65536.0000 (62176.7121)  weight_decay: 0.0500 (0.0500)  time: 0.6510  data: 0.1840  max mem: 15572
Epoch: [8]  [2750/2809]  eta: 0:00:34  lr: 0.000045  min_lr: 0.000000  loss: 4.4129 (4.2153)  loss_scale: 65536.0000 (62188.9233)  weight_decay: 0.0500 (0.0500)  time: 0.6704  data: 0.2021  max mem: 15572
Epoch: [8]  [2760/2809]  eta: 0:00:28  lr: 0.000045  min_lr: 0.000000  loss: 4.3114 (4.2152)  loss_scale: 65536.0000 (62201.0460)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1119  max mem: 15572
Epoch: [8]  [2770/2809]  eta: 0:00:22  lr: 0.000045  min_lr: 0.000000  loss: 4.2618 (4.2153)  loss_scale: 65536.0000 (62213.0812)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0922  max mem: 15572
Epoch: [8]  [2780/2809]  eta: 0:00:17  lr: 0.000045  min_lr: 0.000000  loss: 4.2618 (4.2155)  loss_scale: 65536.0000 (62225.0298)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.1772  max mem: 15572
Epoch: [8]  [2790/2809]  eta: 0:00:11  lr: 0.000045  min_lr: 0.000000  loss: 4.2512 (4.2156)  loss_scale: 65536.0000 (62236.8929)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1683  max mem: 15572
Epoch: [8]  [2800/2809]  eta: 0:00:05  lr: 0.000045  min_lr: 0.000000  loss: 4.2452 (4.2153)  loss_scale: 65536.0000 (62248.6712)  weight_decay: 0.0500 (0.0500)  time: 0.4898  data: 0.0503  max mem: 15572
Epoch: [8]  [2808/2809]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 4.2811 (4.2157)  loss_scale: 65536.0000 (62258.0335)  weight_decay: 0.0500 (0.0500)  time: 0.4563  data: 0.0502  max mem: 15572
Epoch: [8] Total time: 0:27:34 (0.5889 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 4.2811 (4.2157)  loss_scale: 65536.0000 (62258.0335)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:25:34  loss: 0.4292 (0.4292)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.6427  data: 5.4423  max mem: 15572
Val:  [ 10/272]  eta: 0:03:44  loss: 3.9259 (3.3780)  acc1: 0.0000 (23.7374)  acc5: 22.2222 (33.3333)  time: 0.8572  data: 0.6816  max mem: 15572
Val:  [ 20/272]  eta: 0:02:18  loss: 3.4287 (3.3305)  acc1: 11.1111 (24.3386)  acc5: 44.4444 (44.7090)  time: 0.2947  data: 0.1031  max mem: 15572
Val:  [ 30/272]  eta: 0:01:53  loss: 3.4058 (3.4445)  acc1: 11.1111 (19.1756)  acc5: 50.0000 (45.8781)  time: 0.2547  data: 0.0548  max mem: 15572
Val:  [ 40/272]  eta: 0:01:39  loss: 3.3060 (3.3970)  acc1: 11.1111 (18.4282)  acc5: 55.5556 (49.1870)  time: 0.3012  data: 0.1181  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 3.1689 (3.3144)  acc1: 16.6667 (20.5882)  acc5: 61.1111 (52.7233)  time: 0.3272  data: 0.1424  max mem: 15572
Val:  [ 60/272]  eta: 0:01:30  loss: 2.2364 (3.1614)  acc1: 44.4444 (26.0474)  acc5: 72.2222 (55.6466)  time: 0.4172  data: 0.2043  max mem: 15572
Val:  [ 70/272]  eta: 0:01:20  loss: 2.2364 (3.0634)  acc1: 44.4444 (27.9343)  acc5: 77.7778 (59.2332)  time: 0.3630  data: 0.1555  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 2.8536 (3.0694)  acc1: 27.7778 (28.0521)  acc5: 77.7778 (58.7106)  time: 0.2142  data: 0.0305  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 3.6410 (3.1365)  acc1: 11.1111 (26.4957)  acc5: 38.8889 (56.8987)  time: 0.2750  data: 0.0803  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 3.6446 (3.1955)  acc1: 11.1111 (25.7426)  acc5: 38.8889 (56.1056)  time: 0.3413  data: 0.1443  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 3.6446 (3.2565)  acc1: 11.1111 (24.1241)  acc5: 44.4444 (54.3544)  time: 0.2937  data: 0.1038  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 3.8200 (3.2891)  acc1: 11.1111 (23.4619)  acc5: 44.4444 (53.7649)  time: 0.3038  data: 0.1040  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.4511 (3.2332)  acc1: 16.6667 (25.2332)  acc5: 50.0000 (54.6650)  time: 0.3057  data: 0.1085  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 2.8475 (3.2201)  acc1: 33.3333 (26.0047)  acc5: 55.5556 (54.6493)  time: 0.3049  data: 0.1139  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 3.1929 (3.2200)  acc1: 16.6667 (25.3495)  acc5: 66.6667 (55.5556)  time: 0.3239  data: 0.1337  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.0635 (3.1925)  acc1: 27.7778 (26.8806)  acc5: 72.2222 (56.7633)  time: 0.3327  data: 0.1471  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.2491 (3.2222)  acc1: 33.3333 (26.0234)  acc5: 61.1111 (55.9779)  time: 0.3470  data: 0.1455  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.2815 (3.2107)  acc1: 11.1111 (25.8748)  acc5: 50.0000 (56.6605)  time: 0.3497  data: 0.1446  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.3039 (3.2441)  acc1: 16.6667 (25.3636)  acc5: 50.0000 (55.5265)  time: 0.3611  data: 0.1597  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.0948 (3.2411)  acc1: 22.2222 (25.5666)  acc5: 44.4444 (55.9425)  time: 0.3300  data: 0.1226  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.8556 (3.2456)  acc1: 33.3333 (25.9874)  acc5: 72.2222 (56.2138)  time: 0.2915  data: 0.0927  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.1407 (3.2402)  acc1: 33.3333 (26.0935)  acc5: 66.6667 (56.3600)  time: 0.2981  data: 0.1092  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.5818 (3.2008)  acc1: 44.4444 (27.5613)  acc5: 72.2222 (57.3112)  time: 0.3021  data: 0.1199  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.0931 (3.1693)  acc1: 61.1111 (28.5615)  acc5: 83.3333 (58.3218)  time: 0.3254  data: 0.1385  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.8294 (3.1924)  acc1: 27.7778 (28.1541)  acc5: 66.6667 (57.7468)  time: 0.3162  data: 0.1177  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 2.4274 (3.1192)  acc1: 66.6667 (30.3746)  acc5: 77.7778 (59.0464)  time: 0.3064  data: 0.1119  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.8877 (3.1223)  acc1: 66.6667 (30.1968)  acc5: 83.3333 (58.7126)  time: 0.2693  data: 0.0939  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8877 (3.1266)  acc1: 61.1111 (30.1659)  acc5: 83.3333 (58.6729)  time: 0.2605  data: 0.0939  max mem: 15572
Val: Total time: 0:01:30 (0.3324 s / it)
* Acc@1 30.166 Acc@5 58.673 loss 3.127
Accuracy of the network on the 4883 val videos: 30.2%
[2025-01-13 00:49:28,085] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 00:49:28,089] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 00:49:28,089] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 00:49:30,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 00:49:30,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 30.17%
Epoch: [9]  [   0/2809]  eta: 4:48:49  lr: 0.000045  min_lr: 0.000000  loss: 3.8829 (3.8829)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.1693  data: 5.7446  max mem: 15572
Epoch: [9]  [  10/2809]  eta: 0:59:13  lr: 0.000045  min_lr: 0.000000  loss: 4.1393 (4.1209)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2695  data: 0.7923  max mem: 15572
Epoch: [9]  [  20/2809]  eta: 0:41:26  lr: 0.000045  min_lr: 0.000000  loss: 4.2370 (4.1504)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1490  max mem: 15572
Epoch: [9]  [  30/2809]  eta: 0:39:04  lr: 0.000045  min_lr: 0.000000  loss: 4.2556 (4.1736)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1523  max mem: 15572
Epoch: [9]  [  40/2809]  eta: 0:35:31  lr: 0.000045  min_lr: 0.000000  loss: 4.2556 (4.1812)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1813  max mem: 15572
[2025-01-13 00:50:05,481] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:50:05,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:50:07,807] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25331
[2025-01-13 00:50:07,808] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:50:07,808] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [  50/2809]  eta: 0:33:12  lr: 0.000045  min_lr: 0.000000  loss: 3.9989 (4.1487)  loss_scale: 65536.0000 (70676.0784)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0704  max mem: 15572
Epoch: [9]  [  60/2809]  eta: 0:32:55  lr: 0.000045  min_lr: 0.000000  loss: 4.2777 (4.1799)  loss_scale: 65536.0000 (69833.4426)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1518  max mem: 15572
Epoch: [9]  [  70/2809]  eta: 0:31:21  lr: 0.000045  min_lr: 0.000000  loss: 4.4059 (4.1980)  loss_scale: 65536.0000 (69228.1690)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1111  max mem: 15572
Epoch: [9]  [  80/2809]  eta: 0:30:43  lr: 0.000045  min_lr: 0.000000  loss: 4.1748 (4.2018)  loss_scale: 65536.0000 (68772.3457)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0608  max mem: 15572
Epoch: [9]  [  90/2809]  eta: 0:30:16  lr: 0.000045  min_lr: 0.000000  loss: 4.1479 (4.1887)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1432  max mem: 15572
Epoch: [9]  [ 100/2809]  eta: 0:29:53  lr: 0.000045  min_lr: 0.000000  loss: 4.1956 (4.1822)  loss_scale: 65536.0000 (68131.4851)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1413  max mem: 15572
Epoch: [9]  [ 110/2809]  eta: 0:29:44  lr: 0.000045  min_lr: 0.000000  loss: 4.2124 (4.1795)  loss_scale: 65536.0000 (67897.6577)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1771  max mem: 15572
Epoch: [9]  [ 120/2809]  eta: 0:29:16  lr: 0.000045  min_lr: 0.000000  loss: 4.3223 (4.1913)  loss_scale: 65536.0000 (67702.4793)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1834  max mem: 15572
Epoch: [9]  [ 130/2809]  eta: 0:29:03  lr: 0.000045  min_lr: 0.000000  loss: 4.2621 (4.1733)  loss_scale: 65536.0000 (67537.0992)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1407  max mem: 15572
Epoch: [9]  [ 140/2809]  eta: 0:28:46  lr: 0.000045  min_lr: 0.000000  loss: 4.0895 (4.1708)  loss_scale: 65536.0000 (67395.1773)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1582  max mem: 15572
Epoch: [9]  [ 150/2809]  eta: 0:28:29  lr: 0.000045  min_lr: 0.000000  loss: 4.3260 (4.1879)  loss_scale: 65536.0000 (67272.0530)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1702  max mem: 15572
Epoch: [9]  [ 160/2809]  eta: 0:28:14  lr: 0.000045  min_lr: 0.000000  loss: 4.3813 (4.1917)  loss_scale: 65536.0000 (67164.2236)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1589  max mem: 15572
Epoch: [9]  [ 170/2809]  eta: 0:28:12  lr: 0.000045  min_lr: 0.000000  loss: 4.2051 (4.1977)  loss_scale: 65536.0000 (67069.0058)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1874  max mem: 15572
[2025-01-13 00:51:24,756] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:51:24,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 180/2809]  eta: 0:27:37  lr: 0.000045  min_lr: 0.000000  loss: 4.1595 (4.1854)  loss_scale: 65536.0000 (67708.4641)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1166  max mem: 15572
Epoch: [9]  [ 190/2809]  eta: 0:27:00  lr: 0.000045  min_lr: 0.000000  loss: 4.0218 (4.1706)  loss_scale: 131072.0000 (71025.9267)  weight_decay: 0.0500 (0.0500)  time: 0.4272  data: 0.0005  max mem: 15572
[2025-01-13 00:51:32,067] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25477
[2025-01-13 00:51:32,068] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:51:32,068] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 200/2809]  eta: 0:26:35  lr: 0.000045  min_lr: 0.000000  loss: 4.2282 (4.1729)  loss_scale: 131072.0000 (72383.0448)  weight_decay: 0.0500 (0.0500)  time: 0.4388  data: 0.0005  max mem: 15572
Epoch: [9]  [ 210/2809]  eta: 0:26:16  lr: 0.000045  min_lr: 0.000000  loss: 4.3476 (4.1829)  loss_scale: 65536.0000 (72058.5403)  weight_decay: 0.0500 (0.0500)  time: 0.4895  data: 0.0011  max mem: 15572
Epoch: [9]  [ 220/2809]  eta: 0:25:53  lr: 0.000045  min_lr: 0.000000  loss: 4.0341 (4.1713)  loss_scale: 65536.0000 (71763.4027)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0011  max mem: 15572
Epoch: [9]  [ 230/2809]  eta: 0:25:44  lr: 0.000045  min_lr: 0.000000  loss: 3.9717 (4.1685)  loss_scale: 65536.0000 (71493.8182)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0260  max mem: 15572
Epoch: [9]  [ 240/2809]  eta: 0:25:43  lr: 0.000045  min_lr: 0.000000  loss: 4.0153 (4.1599)  loss_scale: 65536.0000 (71246.6058)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0988  max mem: 15572
Epoch: [9]  [ 250/2809]  eta: 0:25:48  lr: 0.000045  min_lr: 0.000000  loss: 4.0611 (4.1512)  loss_scale: 65536.0000 (71019.0916)  weight_decay: 0.0500 (0.0500)  time: 0.6797  data: 0.1904  max mem: 15572
Epoch: [9]  [ 260/2809]  eta: 0:25:53  lr: 0.000045  min_lr: 0.000000  loss: 4.0733 (4.1512)  loss_scale: 65536.0000 (70809.0115)  weight_decay: 0.0500 (0.0500)  time: 0.7116  data: 0.2346  max mem: 15572
Epoch: [9]  [ 270/2809]  eta: 0:25:49  lr: 0.000045  min_lr: 0.000000  loss: 4.2377 (4.1528)  loss_scale: 65536.0000 (70614.4354)  weight_decay: 0.0500 (0.0500)  time: 0.6735  data: 0.1981  max mem: 15572
Epoch: [9]  [ 280/2809]  eta: 0:25:50  lr: 0.000045  min_lr: 0.000000  loss: 4.3506 (4.1535)  loss_scale: 65536.0000 (70433.7082)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.1837  max mem: 15572
Epoch: [9]  [ 290/2809]  eta: 0:25:58  lr: 0.000045  min_lr: 0.000000  loss: 4.2561 (4.1574)  loss_scale: 65536.0000 (70265.4021)  weight_decay: 0.0500 (0.0500)  time: 0.7350  data: 0.2634  max mem: 15572
Epoch: [9]  [ 300/2809]  eta: 0:26:05  lr: 0.000045  min_lr: 0.000000  loss: 4.2205 (4.1568)  loss_scale: 65536.0000 (70108.2791)  weight_decay: 0.0500 (0.0500)  time: 0.7784  data: 0.3110  max mem: 15572
Epoch: [9]  [ 310/2809]  eta: 0:26:03  lr: 0.000045  min_lr: 0.000000  loss: 4.2406 (4.1587)  loss_scale: 65536.0000 (69961.2605)  weight_decay: 0.0500 (0.0500)  time: 0.7252  data: 0.2679  max mem: 15572
Epoch: [9]  [ 320/2809]  eta: 0:25:59  lr: 0.000045  min_lr: 0.000000  loss: 4.0099 (4.1574)  loss_scale: 65536.0000 (69823.4019)  weight_decay: 0.0500 (0.0500)  time: 0.6629  data: 0.1916  max mem: 15572
[2025-01-13 00:52:56,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:52:56,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:52:57,096] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25607
[2025-01-13 00:52:57,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:52:57,097] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 330/2809]  eta: 0:25:57  lr: 0.000045  min_lr: 0.000000  loss: 4.0099 (4.1561)  loss_scale: 65536.0000 (69891.8671)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.1516  max mem: 15572
Epoch: [9]  [ 340/2809]  eta: 0:25:40  lr: 0.000045  min_lr: 0.000000  loss: 4.1052 (4.1597)  loss_scale: 65536.0000 (69764.1290)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1029  max mem: 15572
Epoch: [9]  [ 350/2809]  eta: 0:25:21  lr: 0.000045  min_lr: 0.000000  loss: 4.4023 (4.1638)  loss_scale: 65536.0000 (69643.6695)  weight_decay: 0.0500 (0.0500)  time: 0.4609  data: 0.0260  max mem: 15572
Epoch: [9]  [ 360/2809]  eta: 0:25:05  lr: 0.000045  min_lr: 0.000000  loss: 4.2087 (4.1603)  loss_scale: 65536.0000 (69529.8837)  weight_decay: 0.0500 (0.0500)  time: 0.4513  data: 0.0062  max mem: 15572
Epoch: [9]  [ 370/2809]  eta: 0:24:57  lr: 0.000045  min_lr: 0.000000  loss: 3.9437 (4.1571)  loss_scale: 65536.0000 (69422.2318)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0665  max mem: 15572
Epoch: [9]  [ 380/2809]  eta: 0:24:44  lr: 0.000045  min_lr: 0.000000  loss: 4.0484 (4.1603)  loss_scale: 65536.0000 (69320.2310)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0656  max mem: 15572
Epoch: [9]  [ 390/2809]  eta: 0:24:35  lr: 0.000045  min_lr: 0.000000  loss: 4.1383 (4.1586)  loss_scale: 65536.0000 (69223.4476)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0623  max mem: 15572
Epoch: [9]  [ 400/2809]  eta: 0:24:28  lr: 0.000045  min_lr: 0.000000  loss: 3.9406 (4.1547)  loss_scale: 65536.0000 (69131.4913)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1200  max mem: 15572
Epoch: [9]  [ 410/2809]  eta: 0:24:22  lr: 0.000045  min_lr: 0.000000  loss: 3.9646 (4.1511)  loss_scale: 65536.0000 (69044.0097)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1290  max mem: 15572
Epoch: [9]  [ 420/2809]  eta: 0:24:19  lr: 0.000045  min_lr: 0.000000  loss: 4.0564 (4.1522)  loss_scale: 65536.0000 (68960.6841)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.1790  max mem: 15572
Epoch: [9]  [ 430/2809]  eta: 0:24:16  lr: 0.000045  min_lr: 0.000000  loss: 4.2139 (4.1557)  loss_scale: 65536.0000 (68881.2251)  weight_decay: 0.0500 (0.0500)  time: 0.6656  data: 0.2046  max mem: 15572
Epoch: [9]  [ 440/2809]  eta: 0:24:04  lr: 0.000045  min_lr: 0.000000  loss: 4.3302 (4.1566)  loss_scale: 65536.0000 (68805.3696)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1183  max mem: 15572
Epoch: [9]  [ 450/2809]  eta: 0:23:54  lr: 0.000045  min_lr: 0.000000  loss: 4.2286 (4.1561)  loss_scale: 65536.0000 (68732.8780)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0560  max mem: 15572
[2025-01-13 00:54:07,447] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:54:07,447] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 460/2809]  eta: 0:23:48  lr: 0.000045  min_lr: 0.000000  loss: 4.1642 (4.1586)  loss_scale: 65536.0000 (69516.4946)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1141  max mem: 15572
[2025-01-13 00:54:11,786] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25742
[2025-01-13 00:54:11,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:54:11,787] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 470/2809]  eta: 0:23:41  lr: 0.000045  min_lr: 0.000000  loss: 4.1642 (4.1606)  loss_scale: 65536.0000 (69431.9830)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1766  max mem: 15572
Epoch: [9]  [ 480/2809]  eta: 0:23:26  lr: 0.000045  min_lr: 0.000000  loss: 4.0303 (4.1600)  loss_scale: 65536.0000 (69350.9854)  weight_decay: 0.0500 (0.0500)  time: 0.5141  data: 0.0923  max mem: 15572
Epoch: [9]  [ 490/2809]  eta: 0:23:19  lr: 0.000045  min_lr: 0.000000  loss: 3.9739 (4.1549)  loss_scale: 65536.0000 (69273.2872)  weight_decay: 0.0500 (0.0500)  time: 0.4999  data: 0.0618  max mem: 15572
Epoch: [9]  [ 500/2809]  eta: 0:23:14  lr: 0.000045  min_lr: 0.000000  loss: 3.9274 (4.1500)  loss_scale: 65536.0000 (69198.6906)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1372  max mem: 15572
Epoch: [9]  [ 510/2809]  eta: 0:23:10  lr: 0.000045  min_lr: 0.000000  loss: 3.9274 (4.1475)  loss_scale: 65536.0000 (69127.0137)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.1721  max mem: 15572
Epoch: [9]  [ 520/2809]  eta: 0:23:05  lr: 0.000045  min_lr: 0.000000  loss: 4.0035 (4.1447)  loss_scale: 65536.0000 (69058.0883)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1856  max mem: 15572
Epoch: [9]  [ 530/2809]  eta: 0:23:00  lr: 0.000045  min_lr: 0.000000  loss: 4.0035 (4.1446)  loss_scale: 65536.0000 (68991.7589)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.1792  max mem: 15572
Epoch: [9]  [ 540/2809]  eta: 0:22:50  lr: 0.000045  min_lr: 0.000000  loss: 3.9155 (4.1439)  loss_scale: 65536.0000 (68927.8817)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1232  max mem: 15572
Epoch: [9]  [ 550/2809]  eta: 0:22:42  lr: 0.000045  min_lr: 0.000000  loss: 3.9705 (4.1420)  loss_scale: 65536.0000 (68866.3230)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0855  max mem: 15572
Epoch: [9]  [ 560/2809]  eta: 0:22:33  lr: 0.000045  min_lr: 0.000000  loss: 3.9705 (4.1421)  loss_scale: 65536.0000 (68806.9590)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0892  max mem: 15572
Epoch: [9]  [ 570/2809]  eta: 0:22:26  lr: 0.000045  min_lr: 0.000000  loss: 4.2416 (4.1463)  loss_scale: 65536.0000 (68749.6743)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1029  max mem: 15572
Epoch: [9]  [ 580/2809]  eta: 0:22:20  lr: 0.000045  min_lr: 0.000000  loss: 4.2416 (4.1466)  loss_scale: 65536.0000 (68694.3614)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1409  max mem: 15572
[2025-01-13 00:55:25,343] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:55:25,344] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 590/2809]  eta: 0:22:09  lr: 0.000045  min_lr: 0.000000  loss: 4.2679 (4.1500)  loss_scale: 65536.0000 (68751.8105)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0889  max mem: 15572
Epoch: [9]  [ 600/2809]  eta: 0:22:02  lr: 0.000045  min_lr: 0.000000  loss: 4.2170 (4.1468)  loss_scale: 131072.0000 (69788.7521)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0765  max mem: 15572
[2025-01-13 00:55:34,805] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25887
[2025-01-13 00:55:34,806] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:55:34,806] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 610/2809]  eta: 0:21:54  lr: 0.000045  min_lr: 0.000000  loss: 3.9539 (4.1452)  loss_scale: 131072.0000 (70255.4501)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1183  max mem: 15572
Epoch: [9]  [ 620/2809]  eta: 0:21:46  lr: 0.000045  min_lr: 0.000000  loss: 4.0773 (4.1442)  loss_scale: 65536.0000 (70179.4525)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0979  max mem: 15572
Epoch: [9]  [ 630/2809]  eta: 0:21:41  lr: 0.000045  min_lr: 0.000000  loss: 4.0773 (4.1423)  loss_scale: 65536.0000 (70105.8637)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1039  max mem: 15572
Epoch: [9]  [ 640/2809]  eta: 0:21:35  lr: 0.000045  min_lr: 0.000000  loss: 4.0307 (4.1406)  loss_scale: 65536.0000 (70034.5710)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1489  max mem: 15572
Epoch: [9]  [ 650/2809]  eta: 0:21:24  lr: 0.000045  min_lr: 0.000000  loss: 4.0688 (4.1423)  loss_scale: 65536.0000 (69965.4685)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0971  max mem: 15572
Epoch: [9]  [ 660/2809]  eta: 0:21:17  lr: 0.000045  min_lr: 0.000000  loss: 4.1897 (4.1427)  loss_scale: 65536.0000 (69898.4569)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0745  max mem: 15572
Epoch: [9]  [ 670/2809]  eta: 0:21:12  lr: 0.000045  min_lr: 0.000000  loss: 4.1050 (4.1423)  loss_scale: 65536.0000 (69833.4426)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1494  max mem: 15572
Epoch: [9]  [ 680/2809]  eta: 0:21:03  lr: 0.000045  min_lr: 0.000000  loss: 4.0492 (4.1410)  loss_scale: 65536.0000 (69770.3377)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1103  max mem: 15572
Epoch: [9]  [ 690/2809]  eta: 0:21:01  lr: 0.000045  min_lr: 0.000000  loss: 4.2382 (4.1432)  loss_scale: 65536.0000 (69709.0593)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1624  max mem: 15572
Epoch: [9]  [ 700/2809]  eta: 0:20:55  lr: 0.000045  min_lr: 0.000000  loss: 4.2303 (4.1412)  loss_scale: 65536.0000 (69649.5292)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.2212  max mem: 15572
Epoch: [9]  [ 710/2809]  eta: 0:20:47  lr: 0.000045  min_lr: 0.000000  loss: 3.9685 (4.1389)  loss_scale: 65536.0000 (69591.6737)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1213  max mem: 15572
[2025-01-13 00:56:37,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=168, lr=[4.3781711929090247e-07, 4.3781711929090247e-07, 6.254530275584321e-07, 6.254530275584321e-07, 8.935043250834747e-07, 8.935043250834747e-07, 1.2764347501192496e-06, 1.2764347501192496e-06, 1.8234782144560709e-06, 1.8234782144560709e-06, 2.604968877794387e-06, 2.604968877794387e-06, 3.721384111134839e-06, 3.721384111134839e-06, 5.3162630159069135e-06, 5.3162630159069135e-06, 7.59466145129559e-06, 7.59466145129559e-06, 1.0849516358993702e-05, 1.0849516358993702e-05, 1.5499309084276716e-05, 1.5499309084276716e-05, 2.2141870120395312e-05, 2.2141870120395312e-05, 3.163124302913616e-05, 3.163124302913616e-05, 4.518749004162309e-05, 4.518749004162309e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 00:56:37,821] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=27.869929234127866, CurrSamplesPerSec=31.874517433200594, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [ 720/2809]  eta: 0:20:41  lr: 0.000045  min_lr: 0.000000  loss: 3.9897 (4.1371)  loss_scale: 65536.0000 (69535.4230)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0991  max mem: 15572
Epoch: [9]  [ 730/2809]  eta: 0:20:34  lr: 0.000045  min_lr: 0.000000  loss: 4.1493 (4.1392)  loss_scale: 65536.0000 (69480.7114)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1176  max mem: 15572
[2025-01-13 00:56:47,442] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:56:47,442] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 740/2809]  eta: 0:20:24  lr: 0.000045  min_lr: 0.000000  loss: 4.1493 (4.1376)  loss_scale: 65536.0000 (69958.1323)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0729  max mem: 15572
[2025-01-13 00:56:50,236] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26022
[2025-01-13 00:56:50,236] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:56:50,237] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 750/2809]  eta: 0:20:16  lr: 0.000045  min_lr: 0.000000  loss: 4.1924 (4.1414)  loss_scale: 65536.0000 (69899.2490)  weight_decay: 0.0500 (0.0500)  time: 0.4807  data: 0.0383  max mem: 15572
Epoch: [9]  [ 760/2809]  eta: 0:20:08  lr: 0.000045  min_lr: 0.000000  loss: 4.0911 (4.1394)  loss_scale: 65536.0000 (69841.9133)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0886  max mem: 15572
Epoch: [9]  [ 770/2809]  eta: 0:20:01  lr: 0.000045  min_lr: 0.000000  loss: 4.0911 (4.1395)  loss_scale: 65536.0000 (69786.0649)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1185  max mem: 15572
Epoch: [9]  [ 780/2809]  eta: 0:19:54  lr: 0.000045  min_lr: 0.000000  loss: 4.1173 (4.1376)  loss_scale: 65536.0000 (69731.6466)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0977  max mem: 15572
Epoch: [9]  [ 790/2809]  eta: 0:19:47  lr: 0.000045  min_lr: 0.000000  loss: 3.9638 (4.1374)  loss_scale: 65536.0000 (69678.6043)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.1004  max mem: 15572
Epoch: [9]  [ 800/2809]  eta: 0:19:38  lr: 0.000045  min_lr: 0.000000  loss: 4.2095 (4.1377)  loss_scale: 65536.0000 (69626.8864)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0877  max mem: 15572
Epoch: [9]  [ 810/2809]  eta: 0:19:32  lr: 0.000045  min_lr: 0.000000  loss: 4.2095 (4.1387)  loss_scale: 65536.0000 (69576.4439)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.1198  max mem: 15572
Epoch: [9]  [ 820/2809]  eta: 0:19:28  lr: 0.000045  min_lr: 0.000000  loss: 4.2027 (4.1388)  loss_scale: 65536.0000 (69527.2302)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1858  max mem: 15572
Epoch: [9]  [ 830/2809]  eta: 0:19:21  lr: 0.000045  min_lr: 0.000000  loss: 4.2027 (4.1390)  loss_scale: 65536.0000 (69479.2010)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1511  max mem: 15572
Epoch: [9]  [ 840/2809]  eta: 0:19:15  lr: 0.000045  min_lr: 0.000000  loss: 3.9303 (4.1347)  loss_scale: 65536.0000 (69432.3139)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1321  max mem: 15572
Epoch: [9]  [ 850/2809]  eta: 0:19:12  lr: 0.000045  min_lr: 0.000000  loss: 3.9765 (4.1354)  loss_scale: 65536.0000 (69386.5288)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.2011  max mem: 15572
Epoch: [9]  [ 860/2809]  eta: 0:19:05  lr: 0.000045  min_lr: 0.000000  loss: 4.1764 (4.1372)  loss_scale: 65536.0000 (69341.8072)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1869  max mem: 15572
[2025-01-13 00:58:03,543] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:58:03,544] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 870/2809]  eta: 0:19:00  lr: 0.000045  min_lr: 0.000000  loss: 4.1371 (4.1360)  loss_scale: 65536.0000 (69373.3548)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1548  max mem: 15572
[2025-01-13 00:58:05,850] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26156
[2025-01-13 00:58:05,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:58:05,850] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 880/2809]  eta: 0:18:51  lr: 0.000045  min_lr: 0.000000  loss: 4.0315 (4.1347)  loss_scale: 65536.0000 (69627.3507)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0884  max mem: 15572
Epoch: [9]  [ 890/2809]  eta: 0:18:46  lr: 0.000045  min_lr: 0.000000  loss: 4.1677 (4.1354)  loss_scale: 65536.0000 (69581.4321)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0873  max mem: 15572
Epoch: [9]  [ 900/2809]  eta: 0:18:39  lr: 0.000045  min_lr: 0.000000  loss: 4.1824 (4.1342)  loss_scale: 65536.0000 (69536.5327)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1377  max mem: 15572
Epoch: [9]  [ 910/2809]  eta: 0:18:35  lr: 0.000045  min_lr: 0.000000  loss: 4.1570 (4.1352)  loss_scale: 65536.0000 (69492.6191)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1562  max mem: 15572
Epoch: [9]  [ 920/2809]  eta: 0:18:27  lr: 0.000045  min_lr: 0.000000  loss: 4.2220 (4.1359)  loss_scale: 65536.0000 (69449.6591)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1349  max mem: 15572
Epoch: [9]  [ 930/2809]  eta: 0:18:20  lr: 0.000045  min_lr: 0.000000  loss: 4.1833 (4.1341)  loss_scale: 65536.0000 (69407.6219)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0616  max mem: 15572
Epoch: [9]  [ 940/2809]  eta: 0:18:15  lr: 0.000045  min_lr: 0.000000  loss: 4.1132 (4.1337)  loss_scale: 65536.0000 (69366.4782)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1283  max mem: 15572
Epoch: [9]  [ 950/2809]  eta: 0:18:09  lr: 0.000045  min_lr: 0.000000  loss: 4.0411 (4.1321)  loss_scale: 65536.0000 (69326.1998)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1422  max mem: 15572
Epoch: [9]  [ 960/2809]  eta: 0:18:04  lr: 0.000045  min_lr: 0.000000  loss: 4.0679 (4.1319)  loss_scale: 65536.0000 (69286.7596)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1478  max mem: 15572
Epoch: [9]  [ 970/2809]  eta: 0:17:57  lr: 0.000045  min_lr: 0.000000  loss: 4.0752 (4.1329)  loss_scale: 65536.0000 (69248.1318)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1697  max mem: 15572
Epoch: [9]  [ 980/2809]  eta: 0:17:49  lr: 0.000045  min_lr: 0.000000  loss: 4.3139 (4.1346)  loss_scale: 65536.0000 (69210.2915)  weight_decay: 0.0500 (0.0500)  time: 0.5165  data: 0.0853  max mem: 15572
Epoch: [9]  [ 990/2809]  eta: 0:17:42  lr: 0.000045  min_lr: 0.000000  loss: 4.2462 (4.1352)  loss_scale: 65536.0000 (69173.2149)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0646  max mem: 15572
Epoch: [9]  [1000/2809]  eta: 0:17:37  lr: 0.000045  min_lr: 0.000000  loss: 4.2154 (4.1365)  loss_scale: 65536.0000 (69136.8791)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1304  max mem: 15572
[2025-01-13 00:59:19,188] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:59:19,188] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [1010/2809]  eta: 0:17:30  lr: 0.000045  min_lr: 0.000000  loss: 4.2145 (4.1375)  loss_scale: 65536.0000 (69555.0227)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1388  max mem: 15572
[2025-01-13 00:59:24,549] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26296
[2025-01-13 00:59:24,550] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:59:24,550] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1020/2809]  eta: 0:17:23  lr: 0.000045  min_lr: 0.000000  loss: 4.2145 (4.1382)  loss_scale: 131072.0000 (69772.4114)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0876  max mem: 15572
Epoch: [9]  [1030/2809]  eta: 0:17:18  lr: 0.000045  min_lr: 0.000000  loss: 4.0872 (4.1382)  loss_scale: 65536.0000 (69731.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1159  max mem: 15572
Epoch: [9]  [1040/2809]  eta: 0:17:13  lr: 0.000045  min_lr: 0.000000  loss: 4.1243 (4.1385)  loss_scale: 65536.0000 (69691.0202)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1857  max mem: 15572
Epoch: [9]  [1050/2809]  eta: 0:17:08  lr: 0.000045  min_lr: 0.000000  loss: 4.1921 (4.1384)  loss_scale: 65536.0000 (69651.4862)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1608  max mem: 15572
Epoch: [9]  [1060/2809]  eta: 0:17:01  lr: 0.000045  min_lr: 0.000000  loss: 4.2776 (4.1406)  loss_scale: 65536.0000 (69612.6975)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0997  max mem: 15572
Epoch: [9]  [1070/2809]  eta: 0:16:56  lr: 0.000045  min_lr: 0.000000  loss: 4.1846 (4.1401)  loss_scale: 65536.0000 (69574.6331)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1434  max mem: 15572
Epoch: [9]  [1080/2809]  eta: 0:16:49  lr: 0.000045  min_lr: 0.000000  loss: 4.0310 (4.1400)  loss_scale: 65536.0000 (69537.2729)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1333  max mem: 15572
Epoch: [9]  [1090/2809]  eta: 0:16:42  lr: 0.000045  min_lr: 0.000000  loss: 4.1749 (4.1395)  loss_scale: 65536.0000 (69500.5976)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0888  max mem: 15572
Epoch: [9]  [1100/2809]  eta: 0:16:36  lr: 0.000045  min_lr: 0.000000  loss: 4.1053 (4.1394)  loss_scale: 65536.0000 (69464.5886)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1003  max mem: 15572
Epoch: [9]  [1110/2809]  eta: 0:16:30  lr: 0.000045  min_lr: 0.000000  loss: 4.1053 (4.1395)  loss_scale: 65536.0000 (69429.2277)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0946  max mem: 15572
Epoch: [9]  [1120/2809]  eta: 0:16:23  lr: 0.000045  min_lr: 0.000000  loss: 4.0798 (4.1389)  loss_scale: 65536.0000 (69394.4978)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.0711  max mem: 15572
Epoch: [9]  [1130/2809]  eta: 0:16:16  lr: 0.000045  min_lr: 0.000000  loss: 4.0664 (4.1388)  loss_scale: 65536.0000 (69360.3820)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0192  max mem: 15572
Epoch: [9]  [1140/2809]  eta: 0:16:10  lr: 0.000045  min_lr: 0.000000  loss: 4.1632 (4.1394)  loss_scale: 65536.0000 (69326.8642)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0563  max mem: 15572
[2025-01-13 01:00:37,335] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:00:37,335] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:00:39,561] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26430
[2025-01-13 01:00:39,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:00:39,563] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1150/2809]  eta: 0:16:03  lr: 0.000045  min_lr: 0.000000  loss: 4.1398 (4.1387)  loss_scale: 65536.0000 (69578.6203)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0967  max mem: 15572
Epoch: [9]  [1160/2809]  eta: 0:15:59  lr: 0.000045  min_lr: 0.000000  loss: 3.9729 (4.1374)  loss_scale: 65536.0000 (69543.8002)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1351  max mem: 15572
Epoch: [9]  [1170/2809]  eta: 0:15:53  lr: 0.000045  min_lr: 0.000000  loss: 4.0417 (4.1382)  loss_scale: 65536.0000 (69509.5747)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1393  max mem: 15572
Epoch: [9]  [1180/2809]  eta: 0:15:48  lr: 0.000045  min_lr: 0.000000  loss: 4.1809 (4.1383)  loss_scale: 65536.0000 (69475.9289)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1073  max mem: 15572
Epoch: [9]  [1190/2809]  eta: 0:15:41  lr: 0.000045  min_lr: 0.000000  loss: 4.0597 (4.1369)  loss_scale: 65536.0000 (69442.8480)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1117  max mem: 15572
Epoch: [9]  [1200/2809]  eta: 0:15:36  lr: 0.000045  min_lr: 0.000000  loss: 3.9297 (4.1363)  loss_scale: 65536.0000 (69410.3181)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1374  max mem: 15572
Epoch: [9]  [1210/2809]  eta: 0:15:29  lr: 0.000045  min_lr: 0.000000  loss: 4.0547 (4.1367)  loss_scale: 65536.0000 (69378.3254)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1077  max mem: 15572
Epoch: [9]  [1220/2809]  eta: 0:15:24  lr: 0.000045  min_lr: 0.000000  loss: 4.2839 (4.1391)  loss_scale: 65536.0000 (69346.8567)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1045  max mem: 15572
Epoch: [9]  [1230/2809]  eta: 0:15:18  lr: 0.000045  min_lr: 0.000000  loss: 4.2724 (4.1396)  loss_scale: 65536.0000 (69315.8993)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1360  max mem: 15572
Epoch: [9]  [1240/2809]  eta: 0:15:11  lr: 0.000045  min_lr: 0.000000  loss: 4.2240 (4.1402)  loss_scale: 65536.0000 (69285.4408)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0724  max mem: 15572
Epoch: [9]  [1250/2809]  eta: 0:15:05  lr: 0.000045  min_lr: 0.000000  loss: 4.2281 (4.1417)  loss_scale: 65536.0000 (69255.4692)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0979  max mem: 15572
Epoch: [9]  [1260/2809]  eta: 0:14:59  lr: 0.000045  min_lr: 0.000000  loss: 4.4042 (4.1441)  loss_scale: 65536.0000 (69225.9730)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1054  max mem: 15572
Epoch: [9]  [1270/2809]  eta: 0:14:53  lr: 0.000045  min_lr: 0.000000  loss: 4.3606 (4.1446)  loss_scale: 65536.0000 (69196.9410)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1164  max mem: 15572
[2025-01-13 01:01:53,925] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:01:53,925] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [1280/2809]  eta: 0:14:47  lr: 0.000045  min_lr: 0.000000  loss: 4.0508 (4.1431)  loss_scale: 65536.0000 (69321.8423)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1439  max mem: 15572
[2025-01-13 01:01:58,245] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26565
[2025-01-13 01:01:58,245] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:01:58,245] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1290/2809]  eta: 0:14:41  lr: 0.000045  min_lr: 0.000000  loss: 4.1345 (4.1442)  loss_scale: 65536.0000 (69444.8087)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1417  max mem: 15572
Epoch: [9]  [1300/2809]  eta: 0:14:35  lr: 0.000045  min_lr: 0.000000  loss: 4.2326 (4.1447)  loss_scale: 65536.0000 (69414.7640)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1549  max mem: 15572
Epoch: [9]  [1310/2809]  eta: 0:14:30  lr: 0.000045  min_lr: 0.000000  loss: 4.1702 (4.1444)  loss_scale: 65536.0000 (69385.1777)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1424  max mem: 15572
Epoch: [9]  [1320/2809]  eta: 0:14:25  lr: 0.000045  min_lr: 0.000000  loss: 4.1765 (4.1449)  loss_scale: 65536.0000 (69356.0394)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.1693  max mem: 15572
Epoch: [9]  [1330/2809]  eta: 0:14:19  lr: 0.000045  min_lr: 0.000000  loss: 4.1792 (4.1453)  loss_scale: 65536.0000 (69327.3388)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1401  max mem: 15572
Epoch: [9]  [1340/2809]  eta: 0:14:13  lr: 0.000045  min_lr: 0.000000  loss: 3.9953 (4.1442)  loss_scale: 65536.0000 (69299.0664)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.1090  max mem: 15572
[2025-01-13 01:02:33,409] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26626
[2025-01-13 01:02:33,409] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:02:33,409] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [1350/2809]  eta: 0:14:06  lr: 0.000045  min_lr: 0.000000  loss: 3.9642 (4.1440)  loss_scale: 65536.0000 (69125.6847)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.1115  max mem: 15572
Epoch: [9]  [1360/2809]  eta: 0:14:01  lr: 0.000045  min_lr: 0.000000  loss: 3.8585 (4.1415)  loss_scale: 32768.0000 (68858.5452)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1323  max mem: 15572
Epoch: [9]  [1370/2809]  eta: 0:13:53  lr: 0.000045  min_lr: 0.000000  loss: 4.1297 (4.1429)  loss_scale: 32768.0000 (68595.3027)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0915  max mem: 15572
Epoch: [9]  [1380/2809]  eta: 0:13:48  lr: 0.000045  min_lr: 0.000000  loss: 4.3085 (4.1442)  loss_scale: 32768.0000 (68335.8726)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0865  max mem: 15572
Epoch: [9]  [1390/2809]  eta: 0:13:41  lr: 0.000045  min_lr: 0.000000  loss: 4.2485 (4.1444)  loss_scale: 32768.0000 (68080.1725)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0814  max mem: 15572
Epoch: [9]  [1400/2809]  eta: 0:13:35  lr: 0.000045  min_lr: 0.000000  loss: 4.2505 (4.1463)  loss_scale: 32768.0000 (67828.1228)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0590  max mem: 15572
Epoch: [9]  [1410/2809]  eta: 0:13:29  lr: 0.000045  min_lr: 0.000000  loss: 4.2126 (4.1466)  loss_scale: 32768.0000 (67579.6456)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1331  max mem: 15572
Epoch: [9]  [1420/2809]  eta: 0:13:23  lr: 0.000045  min_lr: 0.000000  loss: 4.0813 (4.1467)  loss_scale: 32768.0000 (67334.6657)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1425  max mem: 15572
Epoch: [9]  [1430/2809]  eta: 0:13:18  lr: 0.000045  min_lr: 0.000000  loss: 4.1316 (4.1470)  loss_scale: 32768.0000 (67093.1097)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1533  max mem: 15572
Epoch: [9]  [1440/2809]  eta: 0:13:12  lr: 0.000045  min_lr: 0.000000  loss: 4.2926 (4.1480)  loss_scale: 32768.0000 (66854.9063)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1688  max mem: 15572
Epoch: [9]  [1450/2809]  eta: 0:13:08  lr: 0.000045  min_lr: 0.000000  loss: 4.1882 (4.1476)  loss_scale: 32768.0000 (66619.9862)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.1991  max mem: 15572
Epoch: [9]  [1460/2809]  eta: 0:13:01  lr: 0.000045  min_lr: 0.000000  loss: 4.0356 (4.1467)  loss_scale: 32768.0000 (66388.2820)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1674  max mem: 15572
Epoch: [9]  [1470/2809]  eta: 0:12:55  lr: 0.000045  min_lr: 0.000000  loss: 4.1095 (4.1465)  loss_scale: 32768.0000 (66159.7281)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0966  max mem: 15572
[2025-01-13 01:03:46,367] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:03:46,368] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [1480/2809]  eta: 0:12:49  lr: 0.000045  min_lr: 0.000000  loss: 4.0433 (4.1448)  loss_scale: 32768.0000 (66089.1398)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0793  max mem: 15572
Epoch: [9]  [1490/2809]  eta: 0:12:43  lr: 0.000045  min_lr: 0.000000  loss: 4.0845 (4.1453)  loss_scale: 65536.0000 (66085.4299)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0661  max mem: 15572
Epoch: [9]  [1500/2809]  eta: 0:12:36  lr: 0.000045  min_lr: 0.000000  loss: 4.2530 (4.1456)  loss_scale: 65536.0000 (66081.7695)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0727  max mem: 15572
Epoch: [9]  [1510/2809]  eta: 0:12:30  lr: 0.000045  min_lr: 0.000000  loss: 4.1568 (4.1453)  loss_scale: 65536.0000 (66078.1575)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0800  max mem: 15572
Epoch: [9]  [1520/2809]  eta: 0:12:24  lr: 0.000045  min_lr: 0.000000  loss: 4.0549 (4.1452)  loss_scale: 65536.0000 (66074.5930)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0619  max mem: 15572
Epoch: [9]  [1530/2809]  eta: 0:12:18  lr: 0.000045  min_lr: 0.000000  loss: 4.1968 (4.1463)  loss_scale: 65536.0000 (66071.0751)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0888  max mem: 15572
Epoch: [9]  [1540/2809]  eta: 0:12:12  lr: 0.000045  min_lr: 0.000000  loss: 4.2811 (4.1474)  loss_scale: 65536.0000 (66067.6029)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1325  max mem: 15572
Epoch: [9]  [1550/2809]  eta: 0:12:06  lr: 0.000045  min_lr: 0.000000  loss: 4.3304 (4.1482)  loss_scale: 65536.0000 (66064.1754)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1420  max mem: 15572
Epoch: [9]  [1560/2809]  eta: 0:11:59  lr: 0.000045  min_lr: 0.000000  loss: 4.2326 (4.1474)  loss_scale: 65536.0000 (66060.7918)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0971  max mem: 15572
Epoch: [9]  [1570/2809]  eta: 0:11:52  lr: 0.000045  min_lr: 0.000000  loss: 4.3467 (4.1485)  loss_scale: 65536.0000 (66057.4513)  weight_decay: 0.0500 (0.0500)  time: 0.4272  data: 0.0223  max mem: 15572
Epoch: [9]  [1580/2809]  eta: 0:11:46  lr: 0.000045  min_lr: 0.000000  loss: 4.3805 (4.1502)  loss_scale: 65536.0000 (66054.1531)  weight_decay: 0.0500 (0.0500)  time: 0.4426  data: 0.0007  max mem: 15572
Epoch: [9]  [1590/2809]  eta: 0:11:39  lr: 0.000045  min_lr: 0.000000  loss: 4.3677 (4.1511)  loss_scale: 65536.0000 (66050.8963)  weight_decay: 0.0500 (0.0500)  time: 0.4682  data: 0.0102  max mem: 15572
Epoch: [9]  [1600/2809]  eta: 0:11:34  lr: 0.000045  min_lr: 0.000000  loss: 4.3421 (4.1517)  loss_scale: 65536.0000 (66047.6802)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0960  max mem: 15572
[2025-01-13 01:04:52,236] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:04:52,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:04:58,413] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26891
[2025-01-13 01:04:58,413] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:04:58,413] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1610/2809]  eta: 0:11:29  lr: 0.000045  min_lr: 0.000000  loss: 4.2888 (4.1519)  loss_scale: 65536.0000 (66369.9466)  weight_decay: 0.0500 (0.0500)  time: 0.6768  data: 0.2156  max mem: 15572
Epoch: [9]  [1620/2809]  eta: 0:11:25  lr: 0.000045  min_lr: 0.000000  loss: 4.1868 (4.1525)  loss_scale: 65536.0000 (66364.8020)  weight_decay: 0.0500 (0.0500)  time: 0.7286  data: 0.2629  max mem: 15572
Epoch: [9]  [1630/2809]  eta: 0:11:19  lr: 0.000045  min_lr: 0.000000  loss: 4.1486 (4.1520)  loss_scale: 65536.0000 (66359.7204)  weight_decay: 0.0500 (0.0500)  time: 0.6765  data: 0.2113  max mem: 15572
Epoch: [9]  [1640/2809]  eta: 0:11:14  lr: 0.000045  min_lr: 0.000000  loss: 4.1486 (4.1526)  loss_scale: 65536.0000 (66354.7008)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.1725  max mem: 15572
Epoch: [9]  [1650/2809]  eta: 0:11:09  lr: 0.000045  min_lr: 0.000000  loss: 4.0677 (4.1513)  loss_scale: 65536.0000 (66349.7420)  weight_decay: 0.0500 (0.0500)  time: 0.6693  data: 0.2047  max mem: 15572
Epoch: [9]  [1660/2809]  eta: 0:11:05  lr: 0.000045  min_lr: 0.000000  loss: 3.9945 (4.1509)  loss_scale: 65536.0000 (66344.8429)  weight_decay: 0.0500 (0.0500)  time: 0.7342  data: 0.2842  max mem: 15572
Epoch: [9]  [1670/2809]  eta: 0:11:00  lr: 0.000045  min_lr: 0.000000  loss: 4.1838 (4.1508)  loss_scale: 65536.0000 (66340.0024)  weight_decay: 0.0500 (0.0500)  time: 0.7453  data: 0.2683  max mem: 15572
Epoch: [9]  [1680/2809]  eta: 0:10:54  lr: 0.000045  min_lr: 0.000000  loss: 4.3375 (4.1518)  loss_scale: 65536.0000 (66335.2195)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.1804  max mem: 15572
Epoch: [9]  [1690/2809]  eta: 0:10:49  lr: 0.000045  min_lr: 0.000000  loss: 4.1400 (4.1508)  loss_scale: 65536.0000 (66330.4932)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.1767  max mem: 15572
Epoch: [9]  [1700/2809]  eta: 0:10:44  lr: 0.000045  min_lr: 0.000000  loss: 4.0075 (4.1502)  loss_scale: 65536.0000 (66325.8225)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.2115  max mem: 15572
Epoch: [9]  [1710/2809]  eta: 0:10:37  lr: 0.000045  min_lr: 0.000000  loss: 4.1213 (4.1505)  loss_scale: 65536.0000 (66321.2063)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.1213  max mem: 15572
[2025-01-13 01:06:06,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=175, lr=[4.3500795337734684e-07, 4.3500795337734684e-07, 6.214399333962098e-07, 6.214399333962098e-07, 8.877713334231571e-07, 8.877713334231571e-07, 1.2682447620330817e-06, 1.2682447620330817e-06, 1.811778231475831e-06, 1.811778231475831e-06, 2.5882546163940444e-06, 2.5882546163940444e-06, 3.697506594848635e-06, 3.697506594848635e-06, 5.282152278355193e-06, 5.282152278355193e-06, 7.545931826221704e-06, 7.545931826221704e-06, 1.077990260888815e-05, 1.077990260888815e-05, 1.5399860869840216e-05, 1.5399860869840216e-05, 2.199980124262888e-05, 2.199980124262888e-05, 3.142828748946983e-05, 3.142828748946983e-05, 4.489755355638548e-05, 4.489755355638548e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 01:06:06,621] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=27.88961604063406, CurrSamplesPerSec=33.030155354115905, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [1720/2809]  eta: 0:10:30  lr: 0.000045  min_lr: 0.000000  loss: 4.0812 (4.1501)  loss_scale: 65536.0000 (66316.6438)  weight_decay: 0.0500 (0.0500)  time: 0.3935  data: 0.0003  max mem: 15572
Epoch: [9]  [1730/2809]  eta: 0:10:23  lr: 0.000045  min_lr: 0.000000  loss: 3.9226 (4.1492)  loss_scale: 65536.0000 (66312.1340)  weight_decay: 0.0500 (0.0500)  time: 0.4440  data: 0.0006  max mem: 15572
[2025-01-13 01:06:16,357] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:06:16,358] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [1740/2809]  eta: 0:10:17  lr: 0.000045  min_lr: 0.000000  loss: 3.9630 (4.1492)  loss_scale: 65536.0000 (66382.9615)  weight_decay: 0.0500 (0.0500)  time: 0.4741  data: 0.0008  max mem: 15572
[2025-01-13 01:06:20,941] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27030
[2025-01-13 01:06:20,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:06:20,941] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1750/2809]  eta: 0:10:11  lr: 0.000045  min_lr: 0.000000  loss: 4.2445 (4.1496)  loss_scale: 65536.0000 (66677.5465)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0529  max mem: 15572
Epoch: [9]  [1760/2809]  eta: 0:10:05  lr: 0.000045  min_lr: 0.000000  loss: 4.1573 (4.1488)  loss_scale: 65536.0000 (66671.0642)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1197  max mem: 15572
Epoch: [9]  [1770/2809]  eta: 0:09:59  lr: 0.000045  min_lr: 0.000000  loss: 4.0434 (4.1491)  loss_scale: 65536.0000 (66664.6550)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1370  max mem: 15572
Epoch: [9]  [1780/2809]  eta: 0:09:54  lr: 0.000045  min_lr: 0.000000  loss: 4.1622 (4.1498)  loss_scale: 65536.0000 (66658.3178)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1388  max mem: 15572
Epoch: [9]  [1790/2809]  eta: 0:09:48  lr: 0.000045  min_lr: 0.000000  loss: 4.1483 (4.1491)  loss_scale: 65536.0000 (66652.0514)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1336  max mem: 15572
Epoch: [9]  [1800/2809]  eta: 0:09:42  lr: 0.000045  min_lr: 0.000000  loss: 4.1208 (4.1488)  loss_scale: 65536.0000 (66645.8545)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1075  max mem: 15572
Epoch: [9]  [1810/2809]  eta: 0:09:36  lr: 0.000045  min_lr: 0.000000  loss: 4.2676 (4.1499)  loss_scale: 65536.0000 (66639.7261)  weight_decay: 0.0500 (0.0500)  time: 0.5224  data: 0.0797  max mem: 15572
Epoch: [9]  [1820/2809]  eta: 0:09:30  lr: 0.000045  min_lr: 0.000000  loss: 4.2222 (4.1487)  loss_scale: 65536.0000 (66633.6650)  weight_decay: 0.0500 (0.0500)  time: 0.5055  data: 0.0735  max mem: 15572
Epoch: [9]  [1830/2809]  eta: 0:09:23  lr: 0.000045  min_lr: 0.000000  loss: 4.2164 (4.1491)  loss_scale: 65536.0000 (66627.6701)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0652  max mem: 15572
Epoch: [9]  [1840/2809]  eta: 0:09:18  lr: 0.000045  min_lr: 0.000000  loss: 4.2868 (4.1493)  loss_scale: 65536.0000 (66621.7404)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1377  max mem: 15572
Epoch: [9]  [1850/2809]  eta: 0:09:12  lr: 0.000045  min_lr: 0.000000  loss: 4.0908 (4.1487)  loss_scale: 65536.0000 (66615.8747)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.2110  max mem: 15572
Epoch: [9]  [1860/2809]  eta: 0:09:06  lr: 0.000045  min_lr: 0.000000  loss: 4.1582 (4.1485)  loss_scale: 65536.0000 (66610.0720)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1270  max mem: 15572
Epoch: [9]  [1870/2809]  eta: 0:09:00  lr: 0.000045  min_lr: 0.000000  loss: 4.1582 (4.1493)  loss_scale: 65536.0000 (66604.3314)  weight_decay: 0.0500 (0.0500)  time: 0.5095  data: 0.0515  max mem: 15572
[2025-01-13 01:07:34,137] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:07:34,137] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:07:34,968] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27161
[2025-01-13 01:07:34,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:07:34,969] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1880/2809]  eta: 0:08:55  lr: 0.000045  min_lr: 0.000000  loss: 4.1190 (4.1499)  loss_scale: 65536.0000 (66668.3339)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0924  max mem: 15572
Epoch: [9]  [1890/2809]  eta: 0:08:48  lr: 0.000045  min_lr: 0.000000  loss: 4.2306 (4.1502)  loss_scale: 65536.0000 (66662.3458)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.1080  max mem: 15572
Epoch: [9]  [1900/2809]  eta: 0:08:42  lr: 0.000045  min_lr: 0.000000  loss: 4.2306 (4.1504)  loss_scale: 65536.0000 (66656.4208)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0853  max mem: 15572
Epoch: [9]  [1910/2809]  eta: 0:08:37  lr: 0.000045  min_lr: 0.000000  loss: 4.0009 (4.1502)  loss_scale: 65536.0000 (66650.5578)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.1048  max mem: 15572
Epoch: [9]  [1920/2809]  eta: 0:08:31  lr: 0.000045  min_lr: 0.000000  loss: 4.2050 (4.1510)  loss_scale: 65536.0000 (66644.7559)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0945  max mem: 15572
Epoch: [9]  [1930/2809]  eta: 0:08:25  lr: 0.000045  min_lr: 0.000000  loss: 4.3664 (4.1514)  loss_scale: 65536.0000 (66639.0140)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0830  max mem: 15572
Epoch: [9]  [1940/2809]  eta: 0:08:19  lr: 0.000045  min_lr: 0.000000  loss: 4.1820 (4.1512)  loss_scale: 65536.0000 (66633.3313)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0744  max mem: 15572
Epoch: [9]  [1950/2809]  eta: 0:08:13  lr: 0.000045  min_lr: 0.000000  loss: 4.0340 (4.1505)  loss_scale: 65536.0000 (66627.7068)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.1100  max mem: 15572
Epoch: [9]  [1960/2809]  eta: 0:08:07  lr: 0.000045  min_lr: 0.000000  loss: 4.0502 (4.1502)  loss_scale: 65536.0000 (66622.1397)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1911  max mem: 15572
Epoch: [9]  [1970/2809]  eta: 0:08:01  lr: 0.000045  min_lr: 0.000000  loss: 3.9258 (4.1487)  loss_scale: 65536.0000 (66616.6291)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1237  max mem: 15572
Epoch: [9]  [1980/2809]  eta: 0:07:56  lr: 0.000045  min_lr: 0.000000  loss: 4.0006 (4.1501)  loss_scale: 65536.0000 (66611.1742)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0744  max mem: 15572
Epoch: [9]  [1990/2809]  eta: 0:07:50  lr: 0.000045  min_lr: 0.000000  loss: 4.3736 (4.1503)  loss_scale: 65536.0000 (66605.7740)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0945  max mem: 15572
Epoch: [9]  [2000/2809]  eta: 0:07:44  lr: 0.000045  min_lr: 0.000000  loss: 4.2223 (4.1508)  loss_scale: 65536.0000 (66600.4278)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0888  max mem: 15572
[2025-01-13 01:08:44,829] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:08:44,829] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [2010/2809]  eta: 0:07:38  lr: 0.000045  min_lr: 0.000000  loss: 4.2223 (4.1508)  loss_scale: 65536.0000 (66660.3123)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1215  max mem: 15572
[2025-01-13 01:08:48,923] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27297
[2025-01-13 01:08:48,923] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:08:48,924] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [2020/2809]  eta: 0:07:32  lr: 0.000045  min_lr: 0.000000  loss: 4.0663 (4.1511)  loss_scale: 65536.0000 (66816.8867)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1340  max mem: 15572
[2025-01-13 01:08:54,738] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27308
[2025-01-13 01:08:54,738] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:08:54,738] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [2030/2809]  eta: 0:07:26  lr: 0.000045  min_lr: 0.000000  loss: 4.0649 (4.1508)  loss_scale: 65536.0000 (66746.0443)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1164  max mem: 15572
Epoch: [9]  [2040/2809]  eta: 0:07:21  lr: 0.000045  min_lr: 0.000000  loss: 4.1859 (4.1517)  loss_scale: 32768.0000 (66579.5669)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.1001  max mem: 15572
Epoch: [9]  [2050/2809]  eta: 0:07:15  lr: 0.000045  min_lr: 0.000000  loss: 4.3323 (4.1522)  loss_scale: 32768.0000 (66414.7128)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1538  max mem: 15572
Epoch: [9]  [2060/2809]  eta: 0:07:09  lr: 0.000045  min_lr: 0.000000  loss: 4.1450 (4.1524)  loss_scale: 32768.0000 (66251.4585)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1568  max mem: 15572
Epoch: [9]  [2070/2809]  eta: 0:07:04  lr: 0.000045  min_lr: 0.000000  loss: 4.2723 (4.1526)  loss_scale: 32768.0000 (66089.7808)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1242  max mem: 15572
Epoch: [9]  [2080/2809]  eta: 0:06:58  lr: 0.000045  min_lr: 0.000000  loss: 4.2723 (4.1529)  loss_scale: 32768.0000 (65929.6569)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0879  max mem: 15572
Epoch: [9]  [2090/2809]  eta: 0:06:53  lr: 0.000045  min_lr: 0.000000  loss: 4.1733 (4.1522)  loss_scale: 32768.0000 (65771.0646)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1161  max mem: 15572
Epoch: [9]  [2100/2809]  eta: 0:06:46  lr: 0.000045  min_lr: 0.000000  loss: 4.0794 (4.1524)  loss_scale: 32768.0000 (65613.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1027  max mem: 15572
Epoch: [9]  [2110/2809]  eta: 0:06:41  lr: 0.000045  min_lr: 0.000000  loss: 4.3159 (4.1533)  loss_scale: 32768.0000 (65458.3875)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0309  max mem: 15572
Epoch: [9]  [2120/2809]  eta: 0:06:35  lr: 0.000045  min_lr: 0.000000  loss: 4.2624 (4.1532)  loss_scale: 32768.0000 (65304.2603)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1140  max mem: 15572
Epoch: [9]  [2130/2809]  eta: 0:06:29  lr: 0.000045  min_lr: 0.000000  loss: 4.1675 (4.1540)  loss_scale: 32768.0000 (65151.5795)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1501  max mem: 15572
Epoch: [9]  [2140/2809]  eta: 0:06:24  lr: 0.000045  min_lr: 0.000000  loss: 4.1310 (4.1533)  loss_scale: 32768.0000 (65000.3251)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1383  max mem: 15572
Epoch: [9]  [2150/2809]  eta: 0:06:18  lr: 0.000045  min_lr: 0.000000  loss: 4.0051 (4.1532)  loss_scale: 32768.0000 (64850.4770)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1319  max mem: 15572
[2025-01-13 01:10:10,284] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:10:10,284] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [2160/2809]  eta: 0:06:12  lr: 0.000045  min_lr: 0.000000  loss: 4.2117 (4.1536)  loss_scale: 32768.0000 (64777.8325)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0970  max mem: 15572
Epoch: [9]  [2170/2809]  eta: 0:06:06  lr: 0.000045  min_lr: 0.000000  loss: 4.1629 (4.1540)  loss_scale: 65536.0000 (64781.3247)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1281  max mem: 15572
Epoch: [9]  [2180/2809]  eta: 0:06:01  lr: 0.000045  min_lr: 0.000000  loss: 4.0880 (4.1533)  loss_scale: 65536.0000 (64784.7850)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1402  max mem: 15572
Epoch: [9]  [2190/2809]  eta: 0:05:55  lr: 0.000045  min_lr: 0.000000  loss: 4.0932 (4.1538)  loss_scale: 65536.0000 (64788.2136)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1235  max mem: 15572
Epoch: [9]  [2200/2809]  eta: 0:05:49  lr: 0.000045  min_lr: 0.000000  loss: 4.3078 (4.1549)  loss_scale: 65536.0000 (64791.6111)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1255  max mem: 15572
Epoch: [9]  [2210/2809]  eta: 0:05:43  lr: 0.000045  min_lr: 0.000000  loss: 4.2766 (4.1544)  loss_scale: 65536.0000 (64794.9778)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1174  max mem: 15572
Epoch: [9]  [2220/2809]  eta: 0:05:38  lr: 0.000045  min_lr: 0.000000  loss: 4.0388 (4.1539)  loss_scale: 65536.0000 (64798.3143)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1762  max mem: 15572
Epoch: [9]  [2230/2809]  eta: 0:05:32  lr: 0.000045  min_lr: 0.000000  loss: 4.1458 (4.1539)  loss_scale: 65536.0000 (64801.6208)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.1988  max mem: 15572
Epoch: [9]  [2240/2809]  eta: 0:05:26  lr: 0.000045  min_lr: 0.000000  loss: 4.1458 (4.1531)  loss_scale: 65536.0000 (64804.8978)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1544  max mem: 15572
Epoch: [9]  [2250/2809]  eta: 0:05:21  lr: 0.000045  min_lr: 0.000000  loss: 4.0554 (4.1532)  loss_scale: 65536.0000 (64808.1457)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.1183  max mem: 15572
Epoch: [9]  [2260/2809]  eta: 0:05:15  lr: 0.000045  min_lr: 0.000000  loss: 4.2283 (4.1547)  loss_scale: 65536.0000 (64811.3649)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1496  max mem: 15572
Epoch: [9]  [2270/2809]  eta: 0:05:09  lr: 0.000045  min_lr: 0.000000  loss: 4.5250 (4.1555)  loss_scale: 65536.0000 (64814.5557)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1538  max mem: 15572
Epoch: [9]  [2280/2809]  eta: 0:05:03  lr: 0.000045  min_lr: 0.000000  loss: 4.3144 (4.1554)  loss_scale: 65536.0000 (64817.7185)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1197  max mem: 15572
[2025-01-13 01:11:24,801] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:11:24,801] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:11:26,137] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27568
[2025-01-13 01:11:26,137] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:11:26,137] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [2290/2809]  eta: 0:04:58  lr: 0.000045  min_lr: 0.000000  loss: 4.0780 (4.1549)  loss_scale: 65536.0000 (64906.6713)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0831  max mem: 15572
Epoch: [9]  [2300/2809]  eta: 0:04:52  lr: 0.000045  min_lr: 0.000000  loss: 4.0831 (4.1548)  loss_scale: 65536.0000 (64909.4063)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1252  max mem: 15572
Epoch: [9]  [2310/2809]  eta: 0:04:46  lr: 0.000045  min_lr: 0.000000  loss: 4.1709 (4.1549)  loss_scale: 65536.0000 (64912.1177)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1264  max mem: 15572
Epoch: [9]  [2320/2809]  eta: 0:04:40  lr: 0.000045  min_lr: 0.000000  loss: 4.1544 (4.1542)  loss_scale: 65536.0000 (64914.8057)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0781  max mem: 15572
[2025-01-13 01:11:48,674] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27605
[2025-01-13 01:11:48,675] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:11:48,675] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [2330/2809]  eta: 0:04:35  lr: 0.000045  min_lr: 0.000000  loss: 4.0199 (4.1542)  loss_scale: 65536.0000 (64819.0682)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1937  max mem: 15572
Epoch: [9]  [2340/2809]  eta: 0:04:29  lr: 0.000045  min_lr: 0.000000  loss: 4.0575 (4.1537)  loss_scale: 32768.0000 (64682.1563)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1698  max mem: 15572
Epoch: [9]  [2350/2809]  eta: 0:04:23  lr: 0.000045  min_lr: 0.000000  loss: 4.0297 (4.1537)  loss_scale: 32768.0000 (64546.4092)  weight_decay: 0.0500 (0.0500)  time: 0.4879  data: 0.0544  max mem: 15572
Epoch: [9]  [2360/2809]  eta: 0:04:17  lr: 0.000045  min_lr: 0.000000  loss: 4.0297 (4.1539)  loss_scale: 32768.0000 (64411.8119)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.1060  max mem: 15572
Epoch: [9]  [2370/2809]  eta: 0:04:12  lr: 0.000045  min_lr: 0.000000  loss: 4.0907 (4.1534)  loss_scale: 32768.0000 (64278.3501)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1497  max mem: 15572
Epoch: [9]  [2380/2809]  eta: 0:04:06  lr: 0.000045  min_lr: 0.000000  loss: 4.1538 (4.1544)  loss_scale: 32768.0000 (64146.0092)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1292  max mem: 15572
Epoch: [9]  [2390/2809]  eta: 0:04:00  lr: 0.000045  min_lr: 0.000000  loss: 4.3145 (4.1544)  loss_scale: 32768.0000 (64014.7754)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1392  max mem: 15572
Epoch: [9]  [2400/2809]  eta: 0:03:54  lr: 0.000045  min_lr: 0.000000  loss: 4.0383 (4.1537)  loss_scale: 32768.0000 (63884.6347)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1361  max mem: 15572
Epoch: [9]  [2410/2809]  eta: 0:03:49  lr: 0.000045  min_lr: 0.000000  loss: 3.9874 (4.1538)  loss_scale: 32768.0000 (63755.5736)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1095  max mem: 15572
Epoch: [9]  [2420/2809]  eta: 0:03:43  lr: 0.000045  min_lr: 0.000000  loss: 4.3457 (4.1549)  loss_scale: 32768.0000 (63627.5787)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0740  max mem: 15572
Epoch: [9]  [2430/2809]  eta: 0:03:37  lr: 0.000045  min_lr: 0.000000  loss: 4.3682 (4.1551)  loss_scale: 32768.0000 (63500.6368)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0997  max mem: 15572
Epoch: [9]  [2440/2809]  eta: 0:03:31  lr: 0.000045  min_lr: 0.000000  loss: 4.1772 (4.1548)  loss_scale: 32768.0000 (63374.7349)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0854  max mem: 15572
Epoch: [9]  [2450/2809]  eta: 0:03:26  lr: 0.000045  min_lr: 0.000000  loss: 4.1438 (4.1546)  loss_scale: 32768.0000 (63249.8605)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0520  max mem: 15572
[2025-01-13 01:12:59,952] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:12:59,953] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [2460/2809]  eta: 0:03:20  lr: 0.000045  min_lr: 0.000000  loss: 3.9751 (4.1532)  loss_scale: 32768.0000 (63232.5201)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0703  max mem: 15572
Epoch: [9]  [2470/2809]  eta: 0:03:14  lr: 0.000045  min_lr: 0.000000  loss: 3.9961 (4.1540)  loss_scale: 65536.0000 (63241.8422)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1232  max mem: 15572
Epoch: [9]  [2480/2809]  eta: 0:03:08  lr: 0.000045  min_lr: 0.000000  loss: 4.1765 (4.1537)  loss_scale: 65536.0000 (63251.0891)  weight_decay: 0.0500 (0.0500)  time: 0.6381  data: 0.1771  max mem: 15572
Epoch: [9]  [2490/2809]  eta: 0:03:03  lr: 0.000045  min_lr: 0.000000  loss: 4.0923 (4.1535)  loss_scale: 65536.0000 (63260.2617)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0834  max mem: 15572
Epoch: [9]  [2500/2809]  eta: 0:02:57  lr: 0.000045  min_lr: 0.000000  loss: 3.9402 (4.1522)  loss_scale: 65536.0000 (63269.3611)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1459  max mem: 15572
Epoch: [9]  [2510/2809]  eta: 0:02:51  lr: 0.000045  min_lr: 0.000000  loss: 3.8959 (4.1519)  loss_scale: 65536.0000 (63278.3879)  weight_decay: 0.0500 (0.0500)  time: 0.7019  data: 0.2433  max mem: 15572
Epoch: [9]  [2520/2809]  eta: 0:02:45  lr: 0.000045  min_lr: 0.000000  loss: 4.1824 (4.1517)  loss_scale: 65536.0000 (63287.3431)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0980  max mem: 15572
Epoch: [9]  [2530/2809]  eta: 0:02:40  lr: 0.000045  min_lr: 0.000000  loss: 4.1843 (4.1517)  loss_scale: 65536.0000 (63296.2276)  weight_decay: 0.0500 (0.0500)  time: 0.4865  data: 0.0520  max mem: 15572
Epoch: [9]  [2540/2809]  eta: 0:02:34  lr: 0.000045  min_lr: 0.000000  loss: 4.2707 (4.1529)  loss_scale: 65536.0000 (63305.0421)  weight_decay: 0.0500 (0.0500)  time: 0.5161  data: 0.0737  max mem: 15572
Epoch: [9]  [2550/2809]  eta: 0:02:28  lr: 0.000045  min_lr: 0.000000  loss: 4.3621 (4.1531)  loss_scale: 65536.0000 (63313.7875)  weight_decay: 0.0500 (0.0500)  time: 0.4924  data: 0.0345  max mem: 15572
Epoch: [9]  [2560/2809]  eta: 0:02:22  lr: 0.000045  min_lr: 0.000000  loss: 4.2547 (4.1533)  loss_scale: 65536.0000 (63322.4647)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0489  max mem: 15572
Epoch: [9]  [2570/2809]  eta: 0:02:16  lr: 0.000045  min_lr: 0.000000  loss: 4.3906 (4.1542)  loss_scale: 65536.0000 (63331.0743)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0617  max mem: 15572
Epoch: [9]  [2580/2809]  eta: 0:02:11  lr: 0.000045  min_lr: 0.000000  loss: 4.3906 (4.1544)  loss_scale: 65536.0000 (63339.6172)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0618  max mem: 15572
[2025-01-13 01:14:11,172] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:14:11,173] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:14:13,991] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27866
[2025-01-13 01:14:13,991] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:14:13,992] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [2590/2809]  eta: 0:02:05  lr: 0.000045  min_lr: 0.000000  loss: 4.3324 (4.1550)  loss_scale: 65536.0000 (63449.2690)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0844  max mem: 15572
Epoch: [9]  [2600/2809]  eta: 0:01:59  lr: 0.000045  min_lr: 0.000000  loss: 4.2930 (4.1546)  loss_scale: 65536.0000 (63457.2918)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.1111  max mem: 15572
Epoch: [9]  [2610/2809]  eta: 0:01:53  lr: 0.000045  min_lr: 0.000000  loss: 4.1504 (4.1548)  loss_scale: 65536.0000 (63465.2532)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1193  max mem: 15572
Epoch: [9]  [2620/2809]  eta: 0:01:48  lr: 0.000045  min_lr: 0.000000  loss: 4.1504 (4.1538)  loss_scale: 65536.0000 (63473.1538)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1162  max mem: 15572
Epoch: [9]  [2630/2809]  eta: 0:01:42  lr: 0.000045  min_lr: 0.000000  loss: 3.8887 (4.1529)  loss_scale: 65536.0000 (63480.9943)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1563  max mem: 15572
Epoch: [9]  [2640/2809]  eta: 0:01:36  lr: 0.000045  min_lr: 0.000000  loss: 3.9971 (4.1529)  loss_scale: 65536.0000 (63488.7755)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1118  max mem: 15572
Epoch: [9]  [2650/2809]  eta: 0:01:31  lr: 0.000045  min_lr: 0.000000  loss: 4.2619 (4.1531)  loss_scale: 65536.0000 (63496.4979)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0875  max mem: 15572
Epoch: [9]  [2660/2809]  eta: 0:01:25  lr: 0.000045  min_lr: 0.000000  loss: 4.1194 (4.1528)  loss_scale: 65536.0000 (63504.1623)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0880  max mem: 15572
Epoch: [9]  [2670/2809]  eta: 0:01:19  lr: 0.000045  min_lr: 0.000000  loss: 4.1392 (4.1531)  loss_scale: 65536.0000 (63511.7694)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0631  max mem: 15572
Epoch: [9]  [2680/2809]  eta: 0:01:13  lr: 0.000045  min_lr: 0.000000  loss: 4.1392 (4.1527)  loss_scale: 65536.0000 (63519.3197)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1372  max mem: 15572
Epoch: [9]  [2690/2809]  eta: 0:01:08  lr: 0.000045  min_lr: 0.000000  loss: 4.2475 (4.1533)  loss_scale: 65536.0000 (63526.8138)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1217  max mem: 15572
Epoch: [9]  [2700/2809]  eta: 0:01:02  lr: 0.000045  min_lr: 0.000000  loss: 4.0543 (4.1521)  loss_scale: 65536.0000 (63534.2525)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0748  max mem: 15572
Epoch: [9]  [2710/2809]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000000  loss: 3.8680 (4.1515)  loss_scale: 65536.0000 (63541.6363)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1532  max mem: 15572
[2025-01-13 01:15:29,310] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:15:29,310] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:15:31,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=182, lr=[4.319867297995155e-07, 4.319867297995155e-07, 6.171238997135936e-07, 6.171238997135936e-07, 8.816055710194195e-07, 8.816055710194195e-07, 1.2594365300277421e-06, 1.2594365300277421e-06, 1.7991950428967748e-06, 1.7991950428967748e-06, 2.5702786327096783e-06, 2.5702786327096783e-06, 3.6718266181566832e-06, 3.6718266181566832e-06, 5.2454665973666914e-06, 5.2454665973666914e-06, 7.493523710523844e-06, 7.493523710523844e-06, 1.0705033872176922e-05, 1.0705033872176922e-05, 1.5292905531681318e-05, 1.5292905531681318e-05, 2.1847007902401884e-05, 2.1847007902401884e-05, 3.121001128914555e-05, 3.121001128914555e-05, 4.4585730413065074e-05, 4.4585730413065074e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 01:15:31,130] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=27.89766974443709, CurrSamplesPerSec=30.934201978421136, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [2720/2809]  eta: 0:00:51  lr: 0.000045  min_lr: 0.000000  loss: 4.0189 (4.1514)  loss_scale: 65536.0000 (63717.5627)  weight_decay: 0.0500 (0.0500)  time: 0.7228  data: 0.2387  max mem: 15572
[2025-01-13 01:15:35,746] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28008
[2025-01-13 01:15:35,746] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:15:35,747] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [2730/2809]  eta: 0:00:45  lr: 0.000045  min_lr: 0.000000  loss: 4.0425 (4.1507)  loss_scale: 131072.0000 (63868.2036)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1293  max mem: 15572
Epoch: [9]  [2740/2809]  eta: 0:00:39  lr: 0.000045  min_lr: 0.000000  loss: 4.1103 (4.1506)  loss_scale: 65536.0000 (63874.2882)  weight_decay: 0.0500 (0.0500)  time: 0.4668  data: 0.0010  max mem: 15572
Epoch: [9]  [2750/2809]  eta: 0:00:33  lr: 0.000045  min_lr: 0.000000  loss: 4.0973 (4.1499)  loss_scale: 65536.0000 (63880.3286)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0702  max mem: 15572
Epoch: [9]  [2760/2809]  eta: 0:00:28  lr: 0.000045  min_lr: 0.000000  loss: 4.2004 (4.1501)  loss_scale: 65536.0000 (63886.3252)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1363  max mem: 15572
Epoch: [9]  [2770/2809]  eta: 0:00:22  lr: 0.000045  min_lr: 0.000000  loss: 4.2195 (4.1498)  loss_scale: 65536.0000 (63892.2786)  weight_decay: 0.0500 (0.0500)  time: 0.4609  data: 0.0667  max mem: 15572
[2025-01-13 01:16:01,221] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28061
[2025-01-13 01:16:01,221] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:16:01,222] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [2780/2809]  eta: 0:00:16  lr: 0.000045  min_lr: 0.000000  loss: 4.1782 (4.1499)  loss_scale: 65536.0000 (63886.4063)  weight_decay: 0.0500 (0.0500)  time: 0.4194  data: 0.0007  max mem: 15572
Epoch: [9]  [2790/2809]  eta: 0:00:10  lr: 0.000045  min_lr: 0.000000  loss: 4.0801 (4.1493)  loss_scale: 32768.0000 (63774.9108)  weight_decay: 0.0500 (0.0500)  time: 0.4553  data: 0.0010  max mem: 15572
Epoch: [9]  [2800/2809]  eta: 0:00:05  lr: 0.000045  min_lr: 0.000000  loss: 4.1829 (4.1498)  loss_scale: 32768.0000 (63664.2114)  weight_decay: 0.0500 (0.0500)  time: 0.4692  data: 0.0008  max mem: 15572
Epoch: [9]  [2808/2809]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 4.2271 (4.1503)  loss_scale: 32768.0000 (63576.2193)  weight_decay: 0.0500 (0.0500)  time: 0.4574  data: 0.0007  max mem: 15572
Epoch: [9] Total time: 0:26:43 (0.5708 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 4.2271 (4.1503)  loss_scale: 32768.0000 (63576.2193)  weight_decay: 0.0500 (0.0500)
[2025-01-13 01:16:14,388] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-9 is about to be saved!
[2025-01-13 01:16:14,393] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-9/mp_rank_00_model_states.pt
[2025-01-13 01:16:14,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-9/mp_rank_00_model_states.pt...
[2025-01-13 01:16:15,220] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-9/mp_rank_00_model_states.pt.
[2025-01-13 01:16:15,221] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
Val:  [  0/272]  eta: 0:23:30  loss: 0.4021 (0.4021)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.1857  data: 5.0089  max mem: 15572
Val:  [ 10/272]  eta: 0:03:34  loss: 3.8947 (3.2759)  acc1: 5.5556 (24.7475)  acc5: 33.3333 (40.9091)  time: 0.8186  data: 0.6311  max mem: 15572
Val:  [ 20/272]  eta: 0:02:34  loss: 3.3791 (3.2198)  acc1: 22.2222 (27.5132)  acc5: 55.5556 (50.5291)  time: 0.3836  data: 0.1834  max mem: 15572
Val:  [ 30/272]  eta: 0:02:01  loss: 3.3318 (3.3237)  acc1: 16.6667 (22.0430)  acc5: 61.1111 (49.6416)  time: 0.3285  data: 0.1273  max mem: 15572
Val:  [ 40/272]  eta: 0:01:47  loss: 3.1367 (3.2566)  acc1: 16.6667 (21.5447)  acc5: 61.1111 (53.7940)  time: 0.3075  data: 0.1140  max mem: 15572
Val:  [ 50/272]  eta: 0:01:37  loss: 2.9695 (3.2017)  acc1: 16.6667 (22.9847)  acc5: 66.6667 (55.7734)  time: 0.3465  data: 0.1523  max mem: 15572
Val:  [ 60/272]  eta: 0:01:32  loss: 2.0696 (3.0204)  acc1: 44.4444 (28.8707)  acc5: 77.7778 (58.5610)  time: 0.3862  data: 0.1954  max mem: 15572
Val:  [ 70/272]  eta: 0:01:25  loss: 2.0158 (2.9304)  acc1: 50.0000 (29.9687)  acc5: 83.3333 (61.8153)  time: 0.3744  data: 0.1765  max mem: 15572
Val:  [ 80/272]  eta: 0:01:19  loss: 2.5363 (2.9280)  acc1: 22.2222 (30.5213)  acc5: 77.7778 (61.5912)  time: 0.3319  data: 0.1238  max mem: 15572
Val:  [ 90/272]  eta: 0:01:13  loss: 3.5317 (3.0025)  acc1: 11.1111 (28.5714)  acc5: 50.0000 (59.7680)  time: 0.3287  data: 0.1280  max mem: 15572
Val:  [100/272]  eta: 0:01:08  loss: 3.6634 (3.0693)  acc1: 11.1111 (27.4477)  acc5: 50.0000 (58.8559)  time: 0.3477  data: 0.1554  max mem: 15572
Val:  [110/272]  eta: 0:01:03  loss: 3.6657 (3.1425)  acc1: 0.0000 (25.3754)  acc5: 44.4444 (57.2573)  time: 0.3585  data: 0.1639  max mem: 15572
Val:  [120/272]  eta: 0:01:00  loss: 3.7151 (3.1784)  acc1: 0.0000 (24.6097)  acc5: 44.4444 (56.5197)  time: 0.3822  data: 0.1801  max mem: 15572
Val:  [130/272]  eta: 0:00:55  loss: 3.1985 (3.1239)  acc1: 22.2222 (26.3359)  acc5: 55.5556 (57.4640)  time: 0.3822  data: 0.1649  max mem: 15572
Val:  [140/272]  eta: 0:00:50  loss: 2.5342 (3.1138)  acc1: 50.0000 (27.3444)  acc5: 61.1111 (57.4862)  time: 0.3187  data: 0.1158  max mem: 15572
Val:  [150/272]  eta: 0:00:46  loss: 3.1656 (3.1183)  acc1: 22.2222 (26.8580)  acc5: 61.1111 (57.7263)  time: 0.3311  data: 0.1394  max mem: 15572
Val:  [160/272]  eta: 0:00:43  loss: 3.0107 (3.0914)  acc1: 27.7778 (27.9848)  acc5: 66.6667 (58.9027)  time: 0.3771  data: 0.1692  max mem: 15572
Val:  [170/272]  eta: 0:00:39  loss: 2.9810 (3.1178)  acc1: 33.3333 (27.4204)  acc5: 66.6667 (58.2846)  time: 0.3754  data: 0.1766  max mem: 15572
Val:  [180/272]  eta: 0:00:35  loss: 3.0040 (3.1080)  acc1: 16.6667 (27.2867)  acc5: 61.1111 (58.8398)  time: 0.3796  data: 0.1716  max mem: 15572
Val:  [190/272]  eta: 0:00:31  loss: 3.2912 (3.1487)  acc1: 11.1111 (26.4689)  acc5: 55.5556 (57.4753)  time: 0.3414  data: 0.1323  max mem: 15572
Val:  [200/272]  eta: 0:00:26  loss: 3.2912 (3.1605)  acc1: 11.1111 (26.2300)  acc5: 44.4444 (57.3521)  time: 0.2419  data: 0.0574  max mem: 15572
Val:  [210/272]  eta: 0:00:22  loss: 3.0752 (3.1689)  acc1: 27.7778 (26.4086)  acc5: 66.6667 (57.4250)  time: 0.1922  data: 0.0088  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 3.1189 (3.1592)  acc1: 27.7778 (26.5963)  acc5: 61.1111 (57.6923)  time: 0.1999  data: 0.0006  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.2666 (3.1213)  acc1: 44.4444 (27.9942)  acc5: 77.7778 (58.5859)  time: 0.2159  data: 0.0158  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 1.9698 (3.0874)  acc1: 55.5556 (28.8382)  acc5: 83.3333 (59.5666)  time: 0.2721  data: 0.0732  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.6553 (3.1083)  acc1: 33.3333 (28.6189)  acc5: 72.2222 (59.0084)  time: 0.3009  data: 0.0886  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 2.1436 (3.0262)  acc1: 72.2222 (30.9706)  acc5: 77.7778 (60.3874)  time: 0.2977  data: 0.1052  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.0307 (3.0311)  acc1: 61.1111 (30.7708)  acc5: 83.3333 (60.2091)  time: 0.2574  data: 0.0906  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.0307 (3.0353)  acc1: 55.5556 (30.7803)  acc5: 83.3333 (60.1884)  time: 0.2419  data: 0.0809  max mem: 15572
Val: Total time: 0:01:32 (0.3385 s / it)
* Acc@1 30.780 Acc@5 60.188 loss 3.035
Accuracy of the network on the 4883 val videos: 30.8%
[2025-01-13 01:17:47,290] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 01:17:47,293] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 01:17:47,293] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 01:17:50,043] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 01:17:50,076] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 30.78%
Epoch: [10]  [   0/2809]  eta: 5:16:33  lr: 0.000045  min_lr: 0.000000  loss: 4.4041 (4.4041)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.7615  data: 6.2455  max mem: 15572
Epoch: [10]  [  10/2809]  eta: 0:52:33  lr: 0.000045  min_lr: 0.000000  loss: 3.9856 (4.0164)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1265  data: 0.7002  max mem: 15572
Epoch: [10]  [  20/2809]  eta: 0:41:24  lr: 0.000045  min_lr: 0.000000  loss: 3.9856 (4.0478)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1495  max mem: 15572
Epoch: [10]  [  30/2809]  eta: 0:35:54  lr: 0.000045  min_lr: 0.000000  loss: 4.0969 (4.0483)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1109  max mem: 15572
Epoch: [10]  [  40/2809]  eta: 0:33:31  lr: 0.000045  min_lr: 0.000000  loss: 4.0969 (4.0486)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0971  max mem: 15572
Epoch: [10]  [  50/2809]  eta: 0:32:00  lr: 0.000045  min_lr: 0.000000  loss: 4.0140 (4.0171)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1229  max mem: 15572
Epoch: [10]  [  60/2809]  eta: 0:30:23  lr: 0.000045  min_lr: 0.000000  loss: 4.1029 (4.0729)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0785  max mem: 15572
Epoch: [10]  [  70/2809]  eta: 0:29:56  lr: 0.000045  min_lr: 0.000000  loss: 4.1342 (4.0579)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1066  max mem: 15572
Epoch: [10]  [  80/2809]  eta: 0:29:22  lr: 0.000045  min_lr: 0.000000  loss: 4.0226 (4.0586)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1604  max mem: 15572
Epoch: [10]  [  90/2809]  eta: 0:28:59  lr: 0.000045  min_lr: 0.000000  loss: 4.0076 (4.0604)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1468  max mem: 15572
[2025-01-13 01:18:54,444] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:18:54,444] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 100/2809]  eta: 0:28:45  lr: 0.000045  min_lr: 0.000000  loss: 3.9420 (4.0428)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1545  max mem: 15572
Epoch: [10]  [ 110/2809]  eta: 0:29:03  lr: 0.000045  min_lr: 0.000000  loss: 3.9467 (4.0385)  loss_scale: 65536.0000 (36015.2793)  weight_decay: 0.0500 (0.0500)  time: 0.6739  data: 0.2097  max mem: 15572
Epoch: [10]  [ 120/2809]  eta: 0:28:37  lr: 0.000045  min_lr: 0.000000  loss: 4.0844 (4.0449)  loss_scale: 65536.0000 (38455.0083)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1921  max mem: 15572
Epoch: [10]  [ 130/2809]  eta: 0:28:11  lr: 0.000045  min_lr: 0.000000  loss: 4.0330 (4.0266)  loss_scale: 65536.0000 (40522.2595)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1163  max mem: 15572
Epoch: [10]  [ 140/2809]  eta: 0:28:00  lr: 0.000045  min_lr: 0.000000  loss: 4.0826 (4.0409)  loss_scale: 65536.0000 (42296.2837)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1362  max mem: 15572
Epoch: [10]  [ 150/2809]  eta: 0:27:40  lr: 0.000045  min_lr: 0.000000  loss: 4.1557 (4.0385)  loss_scale: 65536.0000 (43835.3377)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1421  max mem: 15572
Epoch: [10]  [ 160/2809]  eta: 0:27:34  lr: 0.000045  min_lr: 0.000000  loss: 4.0908 (4.0341)  loss_scale: 65536.0000 (45183.2050)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1482  max mem: 15572
Epoch: [10]  [ 170/2809]  eta: 0:27:04  lr: 0.000045  min_lr: 0.000000  loss: 4.0908 (4.0452)  loss_scale: 65536.0000 (46373.4269)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.1035  max mem: 15572
Epoch: [10]  [ 180/2809]  eta: 0:26:48  lr: 0.000044  min_lr: 0.000000  loss: 4.1629 (4.0536)  loss_scale: 65536.0000 (47432.1326)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0508  max mem: 15572
Epoch: [10]  [ 190/2809]  eta: 0:26:26  lr: 0.000044  min_lr: 0.000000  loss: 4.1138 (4.0571)  loss_scale: 65536.0000 (48379.9791)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0609  max mem: 15572
Epoch: [10]  [ 200/2809]  eta: 0:26:02  lr: 0.000044  min_lr: 0.000000  loss: 4.0597 (4.0540)  loss_scale: 65536.0000 (49233.5124)  weight_decay: 0.0500 (0.0500)  time: 0.4809  data: 0.0464  max mem: 15572
Epoch: [10]  [ 210/2809]  eta: 0:25:47  lr: 0.000044  min_lr: 0.000000  loss: 3.9932 (4.0555)  loss_scale: 65536.0000 (50006.1422)  weight_decay: 0.0500 (0.0500)  time: 0.4936  data: 0.0475  max mem: 15572
Epoch: [10]  [ 220/2809]  eta: 0:25:38  lr: 0.000044  min_lr: 0.000000  loss: 4.0931 (4.0628)  loss_scale: 65536.0000 (50708.8507)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0890  max mem: 15572
[2025-01-13 01:20:05,869] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:20:05,869] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:20:06,397] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28319
[2025-01-13 01:20:06,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:20:06,398] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 230/2809]  eta: 0:25:27  lr: 0.000044  min_lr: 0.000000  loss: 4.0447 (4.0558)  loss_scale: 65536.0000 (51634.4242)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1154  max mem: 15572
Epoch: [10]  [ 240/2809]  eta: 0:25:21  lr: 0.000044  min_lr: 0.000000  loss: 3.9111 (4.0566)  loss_scale: 65536.0000 (52211.2531)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1163  max mem: 15572
Epoch: [10]  [ 250/2809]  eta: 0:25:22  lr: 0.000044  min_lr: 0.000000  loss: 4.0868 (4.0589)  loss_scale: 65536.0000 (52742.1195)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1751  max mem: 15572
Epoch: [10]  [ 260/2809]  eta: 0:25:16  lr: 0.000044  min_lr: 0.000000  loss: 4.0868 (4.0592)  loss_scale: 65536.0000 (53232.3065)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.2030  max mem: 15572
Epoch: [10]  [ 270/2809]  eta: 0:25:02  lr: 0.000044  min_lr: 0.000000  loss: 4.3346 (4.0687)  loss_scale: 65536.0000 (53686.3173)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1330  max mem: 15572
Epoch: [10]  [ 280/2809]  eta: 0:25:02  lr: 0.000044  min_lr: 0.000000  loss: 4.3903 (4.0729)  loss_scale: 65536.0000 (54108.0142)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1447  max mem: 15572
Epoch: [10]  [ 290/2809]  eta: 0:24:44  lr: 0.000044  min_lr: 0.000000  loss: 4.1623 (4.0734)  loss_scale: 65536.0000 (54500.7285)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1032  max mem: 15572
Epoch: [10]  [ 300/2809]  eta: 0:24:34  lr: 0.000044  min_lr: 0.000000  loss: 4.1623 (4.0758)  loss_scale: 65536.0000 (54867.3488)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0608  max mem: 15572
Epoch: [10]  [ 310/2809]  eta: 0:24:29  lr: 0.000044  min_lr: 0.000000  loss: 3.9170 (4.0691)  loss_scale: 65536.0000 (55210.3923)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1376  max mem: 15572
Epoch: [10]  [ 320/2809]  eta: 0:24:18  lr: 0.000044  min_lr: 0.000000  loss: 3.7865 (4.0669)  loss_scale: 65536.0000 (55532.0623)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1145  max mem: 15572
Epoch: [10]  [ 330/2809]  eta: 0:24:20  lr: 0.000044  min_lr: 0.000000  loss: 4.0492 (4.0702)  loss_scale: 65536.0000 (55834.2961)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1422  max mem: 15572
Epoch: [10]  [ 340/2809]  eta: 0:24:02  lr: 0.000044  min_lr: 0.000000  loss: 4.1205 (4.0707)  loss_scale: 65536.0000 (56118.8035)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1051  max mem: 15572
Epoch: [10]  [ 350/2809]  eta: 0:23:54  lr: 0.000044  min_lr: 0.000000  loss: 4.0947 (4.0666)  loss_scale: 65536.0000 (56387.0997)  weight_decay: 0.0500 (0.0500)  time: 0.4911  data: 0.0550  max mem: 15572
[2025-01-13 01:21:19,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:21:19,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [ 360/2809]  eta: 0:23:45  lr: 0.000044  min_lr: 0.000000  loss: 4.1486 (4.0699)  loss_scale: 65536.0000 (57185.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0968  max mem: 15572
[2025-01-13 01:21:24,008] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28455
[2025-01-13 01:21:24,008] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:21:24,008] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 370/2809]  eta: 0:23:40  lr: 0.000044  min_lr: 0.000000  loss: 4.2426 (4.0779)  loss_scale: 65536.0000 (58116.8302)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1082  max mem: 15572
Epoch: [10]  [ 380/2809]  eta: 0:23:33  lr: 0.000044  min_lr: 0.000000  loss: 4.3624 (4.0807)  loss_scale: 65536.0000 (58311.5591)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1288  max mem: 15572
Epoch: [10]  [ 390/2809]  eta: 0:23:23  lr: 0.000044  min_lr: 0.000000  loss: 4.2144 (4.0855)  loss_scale: 65536.0000 (58496.3274)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0968  max mem: 15572
Epoch: [10]  [ 400/2809]  eta: 0:23:15  lr: 0.000044  min_lr: 0.000000  loss: 4.2070 (4.0870)  loss_scale: 65536.0000 (58671.8803)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0805  max mem: 15572
Epoch: [10]  [ 410/2809]  eta: 0:23:08  lr: 0.000044  min_lr: 0.000000  loss: 4.0332 (4.0860)  loss_scale: 65536.0000 (58838.8905)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1114  max mem: 15572
Epoch: [10]  [ 420/2809]  eta: 0:23:03  lr: 0.000044  min_lr: 0.000000  loss: 4.2102 (4.0896)  loss_scale: 65536.0000 (58997.9667)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1545  max mem: 15572
Epoch: [10]  [ 430/2809]  eta: 0:22:57  lr: 0.000044  min_lr: 0.000000  loss: 4.2226 (4.0916)  loss_scale: 65536.0000 (59149.6613)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1519  max mem: 15572
Epoch: [10]  [ 440/2809]  eta: 0:22:47  lr: 0.000044  min_lr: 0.000000  loss: 4.1090 (4.0881)  loss_scale: 65536.0000 (59294.4762)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0964  max mem: 15572
Epoch: [10]  [ 450/2809]  eta: 0:22:38  lr: 0.000044  min_lr: 0.000000  loss: 4.1636 (4.0904)  loss_scale: 65536.0000 (59432.8692)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0702  max mem: 15572
Epoch: [10]  [ 460/2809]  eta: 0:22:34  lr: 0.000044  min_lr: 0.000000  loss: 4.1993 (4.0894)  loss_scale: 65536.0000 (59565.2581)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1164  max mem: 15572
Epoch: [10]  [ 470/2809]  eta: 0:22:28  lr: 0.000044  min_lr: 0.000000  loss: 4.1558 (4.0913)  loss_scale: 65536.0000 (59692.0255)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1464  max mem: 15572
Epoch: [10]  [ 480/2809]  eta: 0:22:20  lr: 0.000044  min_lr: 0.000000  loss: 4.1937 (4.0932)  loss_scale: 65536.0000 (59813.5218)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1267  max mem: 15572
Epoch: [10]  [ 490/2809]  eta: 0:22:15  lr: 0.000044  min_lr: 0.000000  loss: 4.1529 (4.0907)  loss_scale: 65536.0000 (59930.0692)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1452  max mem: 15572
[2025-01-13 01:22:35,558] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:22:35,558] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:22:35,976] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28585
[2025-01-13 01:22:35,977] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:22:35,977] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 500/2809]  eta: 0:22:06  lr: 0.000044  min_lr: 0.000000  loss: 4.1812 (4.0931)  loss_scale: 65536.0000 (60172.7745)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1240  max mem: 15572
Epoch: [10]  [ 510/2809]  eta: 0:21:59  lr: 0.000044  min_lr: 0.000000  loss: 4.1847 (4.0938)  loss_scale: 65536.0000 (60277.7299)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0995  max mem: 15572
Epoch: [10]  [ 520/2809]  eta: 0:21:56  lr: 0.000044  min_lr: 0.000000  loss: 4.1214 (4.0940)  loss_scale: 65536.0000 (60378.6564)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1772  max mem: 15572
Epoch: [10]  [ 530/2809]  eta: 0:21:51  lr: 0.000044  min_lr: 0.000000  loss: 4.1966 (4.0990)  loss_scale: 65536.0000 (60475.7815)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.2047  max mem: 15572
Epoch: [10]  [ 540/2809]  eta: 0:21:47  lr: 0.000044  min_lr: 0.000000  loss: 4.3164 (4.0985)  loss_scale: 65536.0000 (60569.3161)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1729  max mem: 15572
Epoch: [10]  [ 550/2809]  eta: 0:21:40  lr: 0.000044  min_lr: 0.000000  loss: 4.2894 (4.1007)  loss_scale: 65536.0000 (60659.4555)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1430  max mem: 15572
Epoch: [10]  [ 560/2809]  eta: 0:21:30  lr: 0.000044  min_lr: 0.000000  loss: 4.2396 (4.1001)  loss_scale: 65536.0000 (60746.3815)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0701  max mem: 15572
Epoch: [10]  [ 570/2809]  eta: 0:21:26  lr: 0.000044  min_lr: 0.000000  loss: 4.0752 (4.1017)  loss_scale: 65536.0000 (60830.2627)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1046  max mem: 15572
Epoch: [10]  [ 580/2809]  eta: 0:21:21  lr: 0.000044  min_lr: 0.000000  loss: 4.0752 (4.0990)  loss_scale: 65536.0000 (60911.2565)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1780  max mem: 15572
Epoch: [10]  [ 590/2809]  eta: 0:21:17  lr: 0.000044  min_lr: 0.000000  loss: 4.1767 (4.1016)  loss_scale: 65536.0000 (60989.5093)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1825  max mem: 15572
Epoch: [10]  [ 600/2809]  eta: 0:21:12  lr: 0.000044  min_lr: 0.000000  loss: 4.1659 (4.1004)  loss_scale: 65536.0000 (61065.1581)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1892  max mem: 15572
Epoch: [10]  [ 610/2809]  eta: 0:21:09  lr: 0.000044  min_lr: 0.000000  loss: 4.0634 (4.1022)  loss_scale: 65536.0000 (61138.3306)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1859  max mem: 15572
Epoch: [10]  [ 620/2809]  eta: 0:21:01  lr: 0.000044  min_lr: 0.000000  loss: 4.1886 (4.1032)  loss_scale: 65536.0000 (61209.1465)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1471  max mem: 15572
[2025-01-13 01:23:51,271] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:23:51,271] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [ 630/2809]  eta: 0:20:55  lr: 0.000044  min_lr: 0.000000  loss: 4.1742 (4.1027)  loss_scale: 65536.0000 (62004.7417)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1228  max mem: 15572
[2025-01-13 01:23:56,181] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28722
[2025-01-13 01:23:56,181] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:23:56,181] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 640/2809]  eta: 0:20:49  lr: 0.000044  min_lr: 0.000000  loss: 4.1724 (4.1029)  loss_scale: 65536.0000 (62162.0718)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1392  max mem: 15572
Epoch: [10]  [ 650/2809]  eta: 0:20:41  lr: 0.000044  min_lr: 0.000000  loss: 4.1619 (4.1018)  loss_scale: 65536.0000 (62213.8986)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0993  max mem: 15572
Epoch: [10]  [ 660/2809]  eta: 0:20:39  lr: 0.000044  min_lr: 0.000000  loss: 4.1551 (4.1005)  loss_scale: 65536.0000 (62264.1573)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1476  max mem: 15572
Epoch: [10]  [ 670/2809]  eta: 0:20:31  lr: 0.000044  min_lr: 0.000000  loss: 3.9939 (4.0985)  loss_scale: 65536.0000 (62312.9180)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1476  max mem: 15572
Epoch: [10]  [ 680/2809]  eta: 0:20:24  lr: 0.000044  min_lr: 0.000000  loss: 3.9939 (4.0956)  loss_scale: 65536.0000 (62360.2467)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0804  max mem: 15572
Epoch: [10]  [ 690/2809]  eta: 0:20:16  lr: 0.000044  min_lr: 0.000000  loss: 4.0217 (4.0940)  loss_scale: 65536.0000 (62406.2055)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0685  max mem: 15572
Epoch: [10]  [ 700/2809]  eta: 0:20:09  lr: 0.000044  min_lr: 0.000000  loss: 4.0541 (4.0948)  loss_scale: 65536.0000 (62450.8531)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0678  max mem: 15572
Epoch: [10]  [ 710/2809]  eta: 0:20:03  lr: 0.000044  min_lr: 0.000000  loss: 4.1094 (4.0956)  loss_scale: 65536.0000 (62494.2447)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1323  max mem: 15572
[2025-01-13 01:24:44,377] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28810
[2025-01-13 01:24:44,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:24:44,378] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 720/2809]  eta: 0:19:59  lr: 0.000044  min_lr: 0.000000  loss: 4.0141 (4.0952)  loss_scale: 65536.0000 (62490.9847)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1680  max mem: 15572
Epoch: [10]  [ 730/2809]  eta: 0:19:53  lr: 0.000044  min_lr: 0.000000  loss: 4.1185 (4.0963)  loss_scale: 32768.0000 (62084.3776)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1430  max mem: 15572
Epoch: [10]  [ 740/2809]  eta: 0:19:47  lr: 0.000044  min_lr: 0.000000  loss: 4.1376 (4.0952)  loss_scale: 32768.0000 (61688.7449)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1335  max mem: 15572
Epoch: [10]  [ 750/2809]  eta: 0:19:39  lr: 0.000044  min_lr: 0.000000  loss: 4.1373 (4.0958)  loss_scale: 32768.0000 (61303.6485)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.1013  max mem: 15572
Epoch: [10]  [ 760/2809]  eta: 0:19:34  lr: 0.000044  min_lr: 0.000000  loss: 4.2247 (4.0965)  loss_scale: 32768.0000 (60928.6728)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0911  max mem: 15572
Epoch: [10]  [ 770/2809]  eta: 0:19:29  lr: 0.000044  min_lr: 0.000000  loss: 4.3038 (4.0979)  loss_scale: 32768.0000 (60563.4241)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1417  max mem: 15572
Epoch: [10]  [ 780/2809]  eta: 0:19:23  lr: 0.000044  min_lr: 0.000000  loss: 4.3196 (4.1010)  loss_scale: 32768.0000 (60207.5288)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1365  max mem: 15572
Epoch: [10]  [ 790/2809]  eta: 0:19:17  lr: 0.000044  min_lr: 0.000000  loss: 4.1531 (4.0989)  loss_scale: 32768.0000 (59860.6321)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1309  max mem: 15572
Epoch: [10]  [ 800/2809]  eta: 0:19:15  lr: 0.000044  min_lr: 0.000000  loss: 4.0411 (4.0988)  loss_scale: 32768.0000 (59522.3970)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.2071  max mem: 15572
Epoch: [10]  [ 810/2809]  eta: 0:19:09  lr: 0.000044  min_lr: 0.000000  loss: 4.0411 (4.0979)  loss_scale: 32768.0000 (59192.5031)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.2147  max mem: 15572
Epoch: [10]  [ 820/2809]  eta: 0:19:01  lr: 0.000044  min_lr: 0.000000  loss: 3.8995 (4.0962)  loss_scale: 32768.0000 (58870.6456)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0920  max mem: 15572
Epoch: [10]  [ 830/2809]  eta: 0:18:56  lr: 0.000044  min_lr: 0.000000  loss: 3.9768 (4.0963)  loss_scale: 32768.0000 (58556.5343)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0954  max mem: 15572
Epoch: [10]  [ 840/2809]  eta: 0:18:48  lr: 0.000044  min_lr: 0.000000  loss: 4.1358 (4.0964)  loss_scale: 32768.0000 (58249.8930)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1179  max mem: 15572
[2025-01-13 01:25:55,959] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:25:55,959] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 850/2809]  eta: 0:18:38  lr: 0.000044  min_lr: 0.000000  loss: 4.1884 (4.0973)  loss_scale: 32768.0000 (58027.4689)  weight_decay: 0.0500 (0.0500)  time: 0.4341  data: 0.0373  max mem: 15572
Epoch: [10]  [ 860/2809]  eta: 0:18:29  lr: 0.000044  min_lr: 0.000000  loss: 3.9952 (4.0947)  loss_scale: 65536.0000 (58114.6760)  weight_decay: 0.0500 (0.0500)  time: 0.3994  data: 0.0005  max mem: 15572
Epoch: [10]  [ 870/2809]  eta: 0:18:20  lr: 0.000044  min_lr: 0.000000  loss: 3.9811 (4.0950)  loss_scale: 65536.0000 (58199.8806)  weight_decay: 0.0500 (0.0500)  time: 0.4267  data: 0.0011  max mem: 15572
Epoch: [10]  [ 880/2809]  eta: 0:18:13  lr: 0.000044  min_lr: 0.000000  loss: 4.1684 (4.0958)  loss_scale: 65536.0000 (58283.1510)  weight_decay: 0.0500 (0.0500)  time: 0.4622  data: 0.0175  max mem: 15572
Epoch: [10]  [ 890/2809]  eta: 0:18:09  lr: 0.000044  min_lr: 0.000000  loss: 4.2548 (4.0984)  loss_scale: 65536.0000 (58364.5522)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1070  max mem: 15572
Epoch: [10]  [ 900/2809]  eta: 0:18:06  lr: 0.000044  min_lr: 0.000000  loss: 4.2128 (4.0978)  loss_scale: 65536.0000 (58444.1465)  weight_decay: 0.0500 (0.0500)  time: 0.6669  data: 0.1838  max mem: 15572
[2025-01-13 01:26:28,844] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=189, lr=[4.287565332087948e-07, 4.287565332087948e-07, 6.125093331554213e-07, 6.125093331554213e-07, 8.750133330791733e-07, 8.750133330791733e-07, 1.250019047255962e-06, 1.250019047255962e-06, 1.7857414960799456e-06, 1.7857414960799456e-06, 2.551059280114208e-06, 2.551059280114208e-06, 3.6443704001631546e-06, 3.6443704001631546e-06, 5.2062434288045075e-06, 5.2062434288045075e-06, 7.437490612577868e-06, 7.437490612577868e-06, 1.0624986589396956e-05, 1.0624986589396956e-05, 1.5178552270567078e-05, 1.5178552270567078e-05, 2.1683646100810113e-05, 2.1683646100810113e-05, 3.097663728687159e-05, 3.097663728687159e-05, 4.4252338981245135e-05, 4.4252338981245135e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 01:26:28,844] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=27.926489327273025, CurrSamplesPerSec=30.913435387937707, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [10]  [ 910/2809]  eta: 0:18:01  lr: 0.000044  min_lr: 0.000000  loss: 4.1050 (4.0989)  loss_scale: 65536.0000 (58521.9934)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.1692  max mem: 15572
Epoch: [10]  [ 920/2809]  eta: 0:18:00  lr: 0.000044  min_lr: 0.000000  loss: 4.1782 (4.0993)  loss_scale: 65536.0000 (58598.1498)  weight_decay: 0.0500 (0.0500)  time: 0.6946  data: 0.2426  max mem: 15572
Epoch: [10]  [ 930/2809]  eta: 0:17:57  lr: 0.000044  min_lr: 0.000000  loss: 4.1032 (4.0988)  loss_scale: 65536.0000 (58672.6702)  weight_decay: 0.0500 (0.0500)  time: 0.7433  data: 0.3018  max mem: 15572
Epoch: [10]  [ 940/2809]  eta: 0:17:54  lr: 0.000044  min_lr: 0.000000  loss: 4.0532 (4.0990)  loss_scale: 65536.0000 (58745.6068)  weight_decay: 0.0500 (0.0500)  time: 0.7174  data: 0.2657  max mem: 15572
Epoch: [10]  [ 950/2809]  eta: 0:17:51  lr: 0.000044  min_lr: 0.000000  loss: 4.1658 (4.0990)  loss_scale: 65536.0000 (58817.0095)  weight_decay: 0.0500 (0.0500)  time: 0.7147  data: 0.2478  max mem: 15572
Epoch: [10]  [ 960/2809]  eta: 0:17:47  lr: 0.000044  min_lr: 0.000000  loss: 4.0745 (4.0983)  loss_scale: 65536.0000 (58886.9261)  weight_decay: 0.0500 (0.0500)  time: 0.6909  data: 0.2413  max mem: 15572
Epoch: [10]  [ 970/2809]  eta: 0:17:45  lr: 0.000044  min_lr: 0.000000  loss: 4.0651 (4.0975)  loss_scale: 65536.0000 (58955.4027)  weight_decay: 0.0500 (0.0500)  time: 0.7389  data: 0.2845  max mem: 15572
[2025-01-13 01:27:17,474] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:27:17,474] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [ 980/2809]  eta: 0:17:41  lr: 0.000044  min_lr: 0.000000  loss: 4.1687 (4.0986)  loss_scale: 65536.0000 (59289.7044)  weight_decay: 0.0500 (0.0500)  time: 0.7242  data: 0.2515  max mem: 15572
[2025-01-13 01:27:21,622] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29074
[2025-01-13 01:27:21,622] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:27:21,622] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 990/2809]  eta: 0:17:32  lr: 0.000044  min_lr: 0.000000  loss: 4.2729 (4.0998)  loss_scale: 65536.0000 (59551.1282)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.1078  max mem: 15572
Epoch: [10]  [1000/2809]  eta: 0:17:23  lr: 0.000044  min_lr: 0.000000  loss: 4.3125 (4.1014)  loss_scale: 65536.0000 (59610.9171)  weight_decay: 0.0500 (0.0500)  time: 0.4205  data: 0.0166  max mem: 15572
Epoch: [10]  [1010/2809]  eta: 0:17:15  lr: 0.000044  min_lr: 0.000000  loss: 4.2601 (4.1012)  loss_scale: 65536.0000 (59669.5232)  weight_decay: 0.0500 (0.0500)  time: 0.4152  data: 0.0003  max mem: 15572
Epoch: [10]  [1020/2809]  eta: 0:17:07  lr: 0.000044  min_lr: 0.000000  loss: 4.1698 (4.1019)  loss_scale: 65536.0000 (59726.9814)  weight_decay: 0.0500 (0.0500)  time: 0.4367  data: 0.0101  max mem: 15572
Epoch: [10]  [1030/2809]  eta: 0:17:02  lr: 0.000044  min_lr: 0.000000  loss: 4.0107 (4.1006)  loss_scale: 65536.0000 (59783.3249)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0920  max mem: 15572
Epoch: [10]  [1040/2809]  eta: 0:16:56  lr: 0.000044  min_lr: 0.000000  loss: 3.8595 (4.0983)  loss_scale: 65536.0000 (59838.5860)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1428  max mem: 15572
Epoch: [10]  [1050/2809]  eta: 0:16:50  lr: 0.000044  min_lr: 0.000000  loss: 3.9843 (4.0989)  loss_scale: 65536.0000 (59892.7954)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1185  max mem: 15572
Epoch: [10]  [1060/2809]  eta: 0:16:45  lr: 0.000044  min_lr: 0.000000  loss: 4.1441 (4.1003)  loss_scale: 65536.0000 (59945.9830)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1599  max mem: 15572
Epoch: [10]  [1070/2809]  eta: 0:16:40  lr: 0.000044  min_lr: 0.000000  loss: 4.0729 (4.1004)  loss_scale: 65536.0000 (59998.1774)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1780  max mem: 15572
Epoch: [10]  [1080/2809]  eta: 0:16:34  lr: 0.000044  min_lr: 0.000000  loss: 4.0479 (4.0999)  loss_scale: 65536.0000 (60049.4061)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1350  max mem: 15572
Epoch: [10]  [1090/2809]  eta: 0:16:28  lr: 0.000044  min_lr: 0.000000  loss: 4.0577 (4.0992)  loss_scale: 65536.0000 (60099.6957)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1055  max mem: 15572
Epoch: [10]  [1100/2809]  eta: 0:16:22  lr: 0.000044  min_lr: 0.000000  loss: 4.1815 (4.0997)  loss_scale: 65536.0000 (60149.0718)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0989  max mem: 15572
Epoch: [10]  [1110/2809]  eta: 0:16:16  lr: 0.000044  min_lr: 0.000000  loss: 4.0896 (4.0993)  loss_scale: 65536.0000 (60197.5590)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1235  max mem: 15572
[2025-01-13 01:28:30,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:28:30,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [1120/2809]  eta: 0:16:12  lr: 0.000044  min_lr: 0.000000  loss: 4.0733 (4.0982)  loss_scale: 65536.0000 (60712.8778)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1827  max mem: 15572
Epoch: [10]  [1130/2809]  eta: 0:16:06  lr: 0.000044  min_lr: 0.000000  loss: 4.1332 (4.0987)  loss_scale: 131072.0000 (61334.9744)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1826  max mem: 15572
[2025-01-13 01:28:44,359] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29227
[2025-01-13 01:28:44,360] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:28:44,360] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1140/2809]  eta: 0:16:00  lr: 0.000044  min_lr: 0.000000  loss: 4.1043 (4.0980)  loss_scale: 131072.0000 (61716.4172)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1287  max mem: 15572
Epoch: [10]  [1150/2809]  eta: 0:15:56  lr: 0.000044  min_lr: 0.000000  loss: 4.0508 (4.0976)  loss_scale: 65536.0000 (61749.6021)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1880  max mem: 15572
Epoch: [10]  [1160/2809]  eta: 0:15:49  lr: 0.000044  min_lr: 0.000000  loss: 4.2247 (4.0981)  loss_scale: 65536.0000 (61782.2153)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1874  max mem: 15572
Epoch: [10]  [1170/2809]  eta: 0:15:44  lr: 0.000044  min_lr: 0.000000  loss: 4.2826 (4.0995)  loss_scale: 65536.0000 (61814.2716)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1354  max mem: 15572
Epoch: [10]  [1180/2809]  eta: 0:15:37  lr: 0.000044  min_lr: 0.000000  loss: 4.0590 (4.0986)  loss_scale: 65536.0000 (61845.7849)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0858  max mem: 15572
Epoch: [10]  [1190/2809]  eta: 0:15:31  lr: 0.000044  min_lr: 0.000000  loss: 4.0156 (4.0993)  loss_scale: 65536.0000 (61876.7691)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0785  max mem: 15572
Epoch: [10]  [1200/2809]  eta: 0:15:25  lr: 0.000044  min_lr: 0.000000  loss: 4.2068 (4.1012)  loss_scale: 65536.0000 (61907.2373)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1361  max mem: 15572
Epoch: [10]  [1210/2809]  eta: 0:15:20  lr: 0.000044  min_lr: 0.000000  loss: 4.2740 (4.1027)  loss_scale: 65536.0000 (61937.2023)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1363  max mem: 15572
Epoch: [10]  [1220/2809]  eta: 0:15:13  lr: 0.000044  min_lr: 0.000000  loss: 4.2740 (4.1035)  loss_scale: 65536.0000 (61966.6765)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1257  max mem: 15572
Epoch: [10]  [1230/2809]  eta: 0:15:09  lr: 0.000044  min_lr: 0.000000  loss: 4.2140 (4.1047)  loss_scale: 65536.0000 (61995.6718)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1870  max mem: 15572
Epoch: [10]  [1240/2809]  eta: 0:15:02  lr: 0.000044  min_lr: 0.000000  loss: 4.0550 (4.1049)  loss_scale: 65536.0000 (62024.1998)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1545  max mem: 15572
Epoch: [10]  [1250/2809]  eta: 0:14:57  lr: 0.000044  min_lr: 0.000000  loss: 4.3804 (4.1071)  loss_scale: 65536.0000 (62052.2718)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0873  max mem: 15572
Epoch: [10]  [1260/2809]  eta: 0:14:51  lr: 0.000044  min_lr: 0.000000  loss: 4.2036 (4.1067)  loss_scale: 65536.0000 (62079.8985)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1267  max mem: 15572
[2025-01-13 01:29:58,959] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:29:58,959] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:29:59,397] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29357
[2025-01-13 01:29:59,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:29:59,398] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1270/2809]  eta: 0:14:45  lr: 0.000044  min_lr: 0.000000  loss: 3.9841 (4.1056)  loss_scale: 65536.0000 (62158.6530)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1135  max mem: 15572
Epoch: [10]  [1280/2809]  eta: 0:14:39  lr: 0.000044  min_lr: 0.000000  loss: 3.9097 (4.1053)  loss_scale: 65536.0000 (62185.0180)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0983  max mem: 15572
Epoch: [10]  [1290/2809]  eta: 0:14:33  lr: 0.000044  min_lr: 0.000000  loss: 4.1506 (4.1055)  loss_scale: 65536.0000 (62210.9744)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0984  max mem: 15572
Epoch: [10]  [1300/2809]  eta: 0:14:27  lr: 0.000044  min_lr: 0.000000  loss: 3.9266 (4.1044)  loss_scale: 65536.0000 (62236.5319)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1154  max mem: 15572
Epoch: [10]  [1310/2809]  eta: 0:14:21  lr: 0.000044  min_lr: 0.000000  loss: 3.9266 (4.1043)  loss_scale: 65536.0000 (62261.6995)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1218  max mem: 15572
Epoch: [10]  [1320/2809]  eta: 0:14:14  lr: 0.000044  min_lr: 0.000000  loss: 4.1438 (4.1050)  loss_scale: 65536.0000 (62286.4860)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0926  max mem: 15572
Epoch: [10]  [1330/2809]  eta: 0:14:08  lr: 0.000044  min_lr: 0.000000  loss: 4.0936 (4.1044)  loss_scale: 65536.0000 (62310.9001)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.1242  max mem: 15572
Epoch: [10]  [1340/2809]  eta: 0:14:02  lr: 0.000044  min_lr: 0.000000  loss: 4.0469 (4.1046)  loss_scale: 65536.0000 (62334.9500)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1506  max mem: 15572
Epoch: [10]  [1350/2809]  eta: 0:13:56  lr: 0.000044  min_lr: 0.000000  loss: 4.0875 (4.1047)  loss_scale: 65536.0000 (62358.6440)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0899  max mem: 15572
Epoch: [10]  [1360/2809]  eta: 0:13:51  lr: 0.000044  min_lr: 0.000000  loss: 4.0963 (4.1031)  loss_scale: 65536.0000 (62381.9897)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1162  max mem: 15572
Epoch: [10]  [1370/2809]  eta: 0:13:44  lr: 0.000044  min_lr: 0.000000  loss: 4.1055 (4.1034)  loss_scale: 65536.0000 (62404.9949)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1221  max mem: 15572
Epoch: [10]  [1380/2809]  eta: 0:13:39  lr: 0.000044  min_lr: 0.000000  loss: 4.1055 (4.1030)  loss_scale: 65536.0000 (62427.6669)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1313  max mem: 15572
Epoch: [10]  [1390/2809]  eta: 0:13:33  lr: 0.000044  min_lr: 0.000000  loss: 4.1339 (4.1036)  loss_scale: 65536.0000 (62450.0129)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1221  max mem: 15572
[2025-01-13 01:31:10,896] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:31:10,896] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:31:12,082] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29489
[2025-01-13 01:31:12,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:31:12,082] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1400/2809]  eta: 0:13:27  lr: 0.000044  min_lr: 0.000000  loss: 4.1339 (4.1037)  loss_scale: 65536.0000 (62612.3740)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0994  max mem: 15572
Epoch: [10]  [1410/2809]  eta: 0:13:21  lr: 0.000044  min_lr: 0.000000  loss: 4.2756 (4.1055)  loss_scale: 65536.0000 (62633.0943)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1331  max mem: 15572
Epoch: [10]  [1420/2809]  eta: 0:13:15  lr: 0.000044  min_lr: 0.000000  loss: 4.2934 (4.1061)  loss_scale: 65536.0000 (62653.5229)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1214  max mem: 15572
Epoch: [10]  [1430/2809]  eta: 0:13:10  lr: 0.000044  min_lr: 0.000000  loss: 4.0888 (4.1049)  loss_scale: 65536.0000 (62673.6660)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1194  max mem: 15572
Epoch: [10]  [1440/2809]  eta: 0:13:04  lr: 0.000044  min_lr: 0.000000  loss: 3.9537 (4.1045)  loss_scale: 65536.0000 (62693.5295)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1492  max mem: 15572
Epoch: [10]  [1450/2809]  eta: 0:12:59  lr: 0.000044  min_lr: 0.000000  loss: 4.1949 (4.1048)  loss_scale: 65536.0000 (62713.1192)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1542  max mem: 15572
Epoch: [10]  [1460/2809]  eta: 0:12:53  lr: 0.000044  min_lr: 0.000000  loss: 4.2057 (4.1050)  loss_scale: 65536.0000 (62732.4408)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0877  max mem: 15572
Epoch: [10]  [1470/2809]  eta: 0:12:47  lr: 0.000044  min_lr: 0.000000  loss: 4.2057 (4.1055)  loss_scale: 65536.0000 (62751.4997)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0918  max mem: 15572
Epoch: [10]  [1480/2809]  eta: 0:12:41  lr: 0.000044  min_lr: 0.000000  loss: 4.1849 (4.1062)  loss_scale: 65536.0000 (62770.3011)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1149  max mem: 15572
Epoch: [10]  [1490/2809]  eta: 0:12:36  lr: 0.000044  min_lr: 0.000000  loss: 4.1160 (4.1069)  loss_scale: 65536.0000 (62788.8504)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1524  max mem: 15572
Epoch: [10]  [1500/2809]  eta: 0:12:30  lr: 0.000044  min_lr: 0.000000  loss: 4.1160 (4.1066)  loss_scale: 65536.0000 (62807.1526)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1480  max mem: 15572
Epoch: [10]  [1510/2809]  eta: 0:12:23  lr: 0.000044  min_lr: 0.000000  loss: 4.0788 (4.1069)  loss_scale: 65536.0000 (62825.2124)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0693  max mem: 15572
Epoch: [10]  [1520/2809]  eta: 0:12:17  lr: 0.000044  min_lr: 0.000000  loss: 4.2472 (4.1086)  loss_scale: 65536.0000 (62843.0348)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0624  max mem: 15572
[2025-01-13 01:32:24,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:32:24,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [1530/2809]  eta: 0:12:10  lr: 0.000044  min_lr: 0.000000  loss: 4.1298 (4.1068)  loss_scale: 65536.0000 (62989.0425)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0421  max mem: 15572
[2025-01-13 01:32:29,169] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29628
[2025-01-13 01:32:29,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:32:29,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1540/2809]  eta: 0:12:05  lr: 0.000044  min_lr: 0.000000  loss: 3.9975 (4.1059)  loss_scale: 65536.0000 (63303.2680)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0710  max mem: 15572
Epoch: [10]  [1550/2809]  eta: 0:11:58  lr: 0.000044  min_lr: 0.000000  loss: 4.2673 (4.1075)  loss_scale: 65536.0000 (63317.6634)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0987  max mem: 15572
Epoch: [10]  [1560/2809]  eta: 0:11:52  lr: 0.000044  min_lr: 0.000000  loss: 4.2885 (4.1077)  loss_scale: 65536.0000 (63331.8744)  weight_decay: 0.0500 (0.0500)  time: 0.4902  data: 0.0461  max mem: 15572
Epoch: [10]  [1570/2809]  eta: 0:11:46  lr: 0.000044  min_lr: 0.000000  loss: 3.9651 (4.1069)  loss_scale: 65536.0000 (63345.9045)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0691  max mem: 15572
Epoch: [10]  [1580/2809]  eta: 0:11:41  lr: 0.000044  min_lr: 0.000000  loss: 4.0011 (4.1081)  loss_scale: 65536.0000 (63359.7571)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1547  max mem: 15572
Epoch: [10]  [1590/2809]  eta: 0:11:35  lr: 0.000044  min_lr: 0.000000  loss: 4.1565 (4.1091)  loss_scale: 65536.0000 (63373.4356)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1560  max mem: 15572
Epoch: [10]  [1600/2809]  eta: 0:11:29  lr: 0.000044  min_lr: 0.000000  loss: 4.2355 (4.1101)  loss_scale: 65536.0000 (63386.9432)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1139  max mem: 15572
Epoch: [10]  [1610/2809]  eta: 0:11:23  lr: 0.000044  min_lr: 0.000000  loss: 4.2355 (4.1097)  loss_scale: 65536.0000 (63400.2831)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0851  max mem: 15572
Epoch: [10]  [1620/2809]  eta: 0:11:18  lr: 0.000044  min_lr: 0.000000  loss: 4.2011 (4.1104)  loss_scale: 65536.0000 (63413.4584)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0880  max mem: 15572
Epoch: [10]  [1630/2809]  eta: 0:11:13  lr: 0.000044  min_lr: 0.000000  loss: 4.3039 (4.1110)  loss_scale: 65536.0000 (63426.4721)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1668  max mem: 15572
Epoch: [10]  [1640/2809]  eta: 0:11:06  lr: 0.000044  min_lr: 0.000000  loss: 3.9592 (4.1101)  loss_scale: 65536.0000 (63439.3272)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1397  max mem: 15572
Epoch: [10]  [1650/2809]  eta: 0:11:01  lr: 0.000044  min_lr: 0.000000  loss: 3.8668 (4.1082)  loss_scale: 65536.0000 (63452.0267)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.1050  max mem: 15572
Epoch: [10]  [1660/2809]  eta: 0:10:55  lr: 0.000044  min_lr: 0.000000  loss: 3.9481 (4.1079)  loss_scale: 65536.0000 (63464.5731)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1217  max mem: 15572
[2025-01-13 01:33:43,044] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:33:43,044] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:33:43,841] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29759
[2025-01-13 01:33:43,841] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:33:43,841] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1670/2809]  eta: 0:10:49  lr: 0.000044  min_lr: 0.000000  loss: 4.0581 (4.1078)  loss_scale: 65536.0000 (63555.4087)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1388  max mem: 15572
Epoch: [10]  [1680/2809]  eta: 0:10:44  lr: 0.000044  min_lr: 0.000000  loss: 4.0581 (4.1074)  loss_scale: 65536.0000 (63567.1910)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1901  max mem: 15572
Epoch: [10]  [1690/2809]  eta: 0:10:38  lr: 0.000044  min_lr: 0.000000  loss: 3.9857 (4.1062)  loss_scale: 65536.0000 (63578.8338)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1311  max mem: 15572
Epoch: [10]  [1700/2809]  eta: 0:10:32  lr: 0.000044  min_lr: 0.000000  loss: 3.9989 (4.1060)  loss_scale: 65536.0000 (63590.3398)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.1019  max mem: 15572
Epoch: [10]  [1710/2809]  eta: 0:10:26  lr: 0.000044  min_lr: 0.000000  loss: 3.9348 (4.1052)  loss_scale: 65536.0000 (63601.7113)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.1101  max mem: 15572
Epoch: [10]  [1720/2809]  eta: 0:10:21  lr: 0.000044  min_lr: 0.000000  loss: 3.9636 (4.1050)  loss_scale: 65536.0000 (63612.9506)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1816  max mem: 15572
Epoch: [10]  [1730/2809]  eta: 0:10:15  lr: 0.000044  min_lr: 0.000000  loss: 4.1789 (4.1052)  loss_scale: 65536.0000 (63624.0601)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1430  max mem: 15572
Epoch: [10]  [1740/2809]  eta: 0:10:09  lr: 0.000044  min_lr: 0.000000  loss: 4.1908 (4.1046)  loss_scale: 65536.0000 (63635.0419)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0614  max mem: 15572
Epoch: [10]  [1750/2809]  eta: 0:10:03  lr: 0.000044  min_lr: 0.000000  loss: 4.2037 (4.1058)  loss_scale: 65536.0000 (63645.8983)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1405  max mem: 15572
Epoch: [10]  [1760/2809]  eta: 0:09:58  lr: 0.000044  min_lr: 0.000000  loss: 4.1666 (4.1055)  loss_scale: 65536.0000 (63656.6315)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1781  max mem: 15572
Epoch: [10]  [1770/2809]  eta: 0:09:52  lr: 0.000044  min_lr: 0.000000  loss: 4.0964 (4.1055)  loss_scale: 65536.0000 (63667.2434)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1333  max mem: 15572
Epoch: [10]  [1780/2809]  eta: 0:09:46  lr: 0.000044  min_lr: 0.000000  loss: 4.0966 (4.1057)  loss_scale: 65536.0000 (63677.7361)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0760  max mem: 15572
Epoch: [10]  [1790/2809]  eta: 0:09:40  lr: 0.000044  min_lr: 0.000000  loss: 4.0535 (4.1041)  loss_scale: 65536.0000 (63688.1117)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0508  max mem: 15572
[2025-01-13 01:34:53,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:34:53,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:34:53,919] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29889
[2025-01-13 01:34:53,919] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:34:53,919] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1800/2809]  eta: 0:09:33  lr: 0.000044  min_lr: 0.000000  loss: 3.7836 (4.1039)  loss_scale: 65536.0000 (63734.7607)  weight_decay: 0.0500 (0.0500)  time: 0.4083  data: 0.0094  max mem: 15572
Epoch: [10]  [1810/2809]  eta: 0:09:27  lr: 0.000044  min_lr: 0.000000  loss: 4.2127 (4.1035)  loss_scale: 65536.0000 (63744.7068)  weight_decay: 0.0500 (0.0500)  time: 0.4400  data: 0.0006  max mem: 15572
Epoch: [10]  [1820/2809]  eta: 0:09:21  lr: 0.000044  min_lr: 0.000000  loss: 4.1637 (4.1032)  loss_scale: 65536.0000 (63754.5437)  weight_decay: 0.0500 (0.0500)  time: 0.4662  data: 0.0010  max mem: 15572
Epoch: [10]  [1830/2809]  eta: 0:09:16  lr: 0.000044  min_lr: 0.000000  loss: 4.0145 (4.1031)  loss_scale: 65536.0000 (63764.2731)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1395  max mem: 15572
Epoch: [10]  [1840/2809]  eta: 0:09:11  lr: 0.000044  min_lr: 0.000000  loss: 4.1667 (4.1037)  loss_scale: 65536.0000 (63773.8968)  weight_decay: 0.0500 (0.0500)  time: 0.6970  data: 0.2515  max mem: 15572
Epoch: [10]  [1850/2809]  eta: 0:09:05  lr: 0.000044  min_lr: 0.000000  loss: 4.1295 (4.1036)  loss_scale: 65536.0000 (63783.4165)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.2116  max mem: 15572
Epoch: [10]  [1860/2809]  eta: 0:09:00  lr: 0.000044  min_lr: 0.000000  loss: 4.0836 (4.1040)  loss_scale: 65536.0000 (63792.8340)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1787  max mem: 15572
Epoch: [10]  [1870/2809]  eta: 0:08:54  lr: 0.000044  min_lr: 0.000000  loss: 4.1818 (4.1047)  loss_scale: 65536.0000 (63802.1507)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1541  max mem: 15572
Epoch: [10]  [1880/2809]  eta: 0:08:49  lr: 0.000044  min_lr: 0.000000  loss: 4.1807 (4.1052)  loss_scale: 65536.0000 (63811.3684)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.1735  max mem: 15572
Epoch: [10]  [1890/2809]  eta: 0:08:44  lr: 0.000044  min_lr: 0.000000  loss: 4.0615 (4.1052)  loss_scale: 65536.0000 (63820.4886)  weight_decay: 0.0500 (0.0500)  time: 0.7163  data: 0.2594  max mem: 15572
Epoch: [10]  [1900/2809]  eta: 0:08:39  lr: 0.000044  min_lr: 0.000000  loss: 4.2147 (4.1054)  loss_scale: 65536.0000 (63829.5129)  weight_decay: 0.0500 (0.0500)  time: 0.6669  data: 0.1876  max mem: 15572
[2025-01-13 01:36:04,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=196, lr=[4.2532066161678034e-07, 4.2532066161678034e-07, 6.076009451668291e-07, 6.076009451668291e-07, 8.680013502383275e-07, 8.680013502383275e-07, 1.2400019289118964e-06, 1.2400019289118964e-06, 1.771431327016995e-06, 1.771431327016995e-06, 2.53061618145285e-06, 2.53061618145285e-06, 3.6151659735040717e-06, 3.6151659735040717e-06, 5.164522819291532e-06, 5.164522819291532e-06, 7.377889741845045e-06, 7.377889741845045e-06, 1.0539842488350066e-05, 1.0539842488350066e-05, 1.5056917840500093e-05, 1.5056917840500093e-05, 2.150988262928585e-05, 2.150988262928585e-05, 3.0728403756122646e-05, 3.0728403756122646e-05, 4.389771965160378e-05, 4.389771965160378e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 01:36:04,212] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=27.949562398635706, CurrSamplesPerSec=26.337762237396305, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [10]  [1910/2809]  eta: 0:08:34  lr: 0.000044  min_lr: 0.000000  loss: 4.1869 (4.1054)  loss_scale: 65536.0000 (63838.4427)  weight_decay: 0.0500 (0.0500)  time: 0.6828  data: 0.2066  max mem: 15572
Epoch: [10]  [1920/2809]  eta: 0:08:29  lr: 0.000044  min_lr: 0.000000  loss: 4.1124 (4.1054)  loss_scale: 65536.0000 (63847.2795)  weight_decay: 0.0500 (0.0500)  time: 0.7284  data: 0.2651  max mem: 15572
[2025-01-13 01:36:17,124] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:36:17,125] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [1930/2809]  eta: 0:08:24  lr: 0.000044  min_lr: 0.000000  loss: 4.1093 (4.1049)  loss_scale: 65536.0000 (63957.8415)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.2072  max mem: 15572
Epoch: [10]  [1940/2809]  eta: 0:08:17  lr: 0.000044  min_lr: 0.000000  loss: 4.1093 (4.1049)  loss_scale: 131072.0000 (64303.6126)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1217  max mem: 15572
Epoch: [10]  [1950/2809]  eta: 0:08:11  lr: 0.000044  min_lr: 0.000000  loss: 3.9946 (4.1038)  loss_scale: 131072.0000 (64645.8391)  weight_decay: 0.0500 (0.0500)  time: 0.4317  data: 0.0103  max mem: 15572
[2025-01-13 01:36:28,051] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30043
[2025-01-13 01:36:28,052] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:36:28,052] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1960/2809]  eta: 0:08:05  lr: 0.000044  min_lr: 0.000000  loss: 4.1683 (4.1045)  loss_scale: 131072.0000 (64717.2177)  weight_decay: 0.0500 (0.0500)  time: 0.4554  data: 0.0191  max mem: 15572
Epoch: [10]  [1970/2809]  eta: 0:07:59  lr: 0.000044  min_lr: 0.000000  loss: 4.2308 (4.1051)  loss_scale: 65536.0000 (64721.3719)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0801  max mem: 15572
Epoch: [10]  [1980/2809]  eta: 0:07:53  lr: 0.000044  min_lr: 0.000000  loss: 4.2139 (4.1051)  loss_scale: 65536.0000 (64725.4841)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1489  max mem: 15572
Epoch: [10]  [1990/2809]  eta: 0:07:47  lr: 0.000044  min_lr: 0.000000  loss: 3.9786 (4.1045)  loss_scale: 65536.0000 (64729.5550)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0943  max mem: 15572
Epoch: [10]  [2000/2809]  eta: 0:07:42  lr: 0.000044  min_lr: 0.000000  loss: 3.9786 (4.1042)  loss_scale: 65536.0000 (64733.5852)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1027  max mem: 15572
Epoch: [10]  [2010/2809]  eta: 0:07:36  lr: 0.000044  min_lr: 0.000000  loss: 4.0579 (4.1043)  loss_scale: 65536.0000 (64737.5753)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1575  max mem: 15572
[2025-01-13 01:37:05,050] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30110
[2025-01-13 01:37:05,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:37:05,051] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [2020/2809]  eta: 0:07:30  lr: 0.000044  min_lr: 0.000000  loss: 4.1018 (4.1048)  loss_scale: 65536.0000 (64725.3122)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1288  max mem: 15572
Epoch: [10]  [2030/2809]  eta: 0:07:25  lr: 0.000044  min_lr: 0.000000  loss: 4.2323 (4.1048)  loss_scale: 32768.0000 (64567.9645)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1425  max mem: 15572
Epoch: [10]  [2040/2809]  eta: 0:07:19  lr: 0.000044  min_lr: 0.000000  loss: 4.2447 (4.1056)  loss_scale: 32768.0000 (64412.1587)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1562  max mem: 15572
Epoch: [10]  [2050/2809]  eta: 0:07:14  lr: 0.000044  min_lr: 0.000000  loss: 4.3654 (4.1066)  loss_scale: 32768.0000 (64257.8723)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1841  max mem: 15572
Epoch: [10]  [2060/2809]  eta: 0:07:08  lr: 0.000044  min_lr: 0.000000  loss: 4.1552 (4.1065)  loss_scale: 32768.0000 (64105.0830)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1539  max mem: 15572
Epoch: [10]  [2070/2809]  eta: 0:07:02  lr: 0.000044  min_lr: 0.000000  loss: 4.1377 (4.1071)  loss_scale: 32768.0000 (63953.7692)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1319  max mem: 15572
Epoch: [10]  [2080/2809]  eta: 0:06:56  lr: 0.000044  min_lr: 0.000000  loss: 4.2333 (4.1073)  loss_scale: 32768.0000 (63803.9097)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1488  max mem: 15572
Epoch: [10]  [2090/2809]  eta: 0:06:51  lr: 0.000044  min_lr: 0.000000  loss: 4.1752 (4.1075)  loss_scale: 32768.0000 (63655.4835)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0893  max mem: 15572
Epoch: [10]  [2100/2809]  eta: 0:06:45  lr: 0.000044  min_lr: 0.000000  loss: 4.1752 (4.1081)  loss_scale: 32768.0000 (63508.4703)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0879  max mem: 15572
Epoch: [10]  [2110/2809]  eta: 0:06:39  lr: 0.000044  min_lr: 0.000000  loss: 4.1630 (4.1085)  loss_scale: 32768.0000 (63362.8498)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1509  max mem: 15572
Epoch: [10]  [2120/2809]  eta: 0:06:34  lr: 0.000044  min_lr: 0.000000  loss: 4.1674 (4.1088)  loss_scale: 32768.0000 (63218.6025)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1668  max mem: 15572
Epoch: [10]  [2130/2809]  eta: 0:06:28  lr: 0.000044  min_lr: 0.000000  loss: 4.0555 (4.1085)  loss_scale: 32768.0000 (63075.7091)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1242  max mem: 15572
Epoch: [10]  [2140/2809]  eta: 0:06:22  lr: 0.000044  min_lr: 0.000000  loss: 3.8921 (4.1075)  loss_scale: 32768.0000 (62934.1504)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0858  max mem: 15572
[2025-01-13 01:38:20,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:38:20,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [2150/2809]  eta: 0:06:16  lr: 0.000044  min_lr: 0.000000  loss: 3.8659 (4.1076)  loss_scale: 32768.0000 (62824.3756)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1210  max mem: 15572
Epoch: [10]  [2160/2809]  eta: 0:06:11  lr: 0.000044  min_lr: 0.000000  loss: 4.0764 (4.1076)  loss_scale: 65536.0000 (62836.9236)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1649  max mem: 15572
Epoch: [10]  [2170/2809]  eta: 0:06:04  lr: 0.000044  min_lr: 0.000000  loss: 4.0857 (4.1085)  loss_scale: 65536.0000 (62849.3561)  weight_decay: 0.0500 (0.0500)  time: 0.5116  data: 0.0784  max mem: 15572
Epoch: [10]  [2180/2809]  eta: 0:05:59  lr: 0.000044  min_lr: 0.000000  loss: 4.1307 (4.1079)  loss_scale: 65536.0000 (62861.6745)  weight_decay: 0.0500 (0.0500)  time: 0.4957  data: 0.0473  max mem: 15572
Epoch: [10]  [2190/2809]  eta: 0:05:53  lr: 0.000044  min_lr: 0.000000  loss: 4.0106 (4.1078)  loss_scale: 65536.0000 (62873.8804)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0934  max mem: 15572
Epoch: [10]  [2200/2809]  eta: 0:05:47  lr: 0.000044  min_lr: 0.000000  loss: 4.0296 (4.1072)  loss_scale: 65536.0000 (62885.9755)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0468  max mem: 15572
Epoch: [10]  [2210/2809]  eta: 0:05:42  lr: 0.000044  min_lr: 0.000000  loss: 4.0296 (4.1070)  loss_scale: 65536.0000 (62897.9611)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1039  max mem: 15572
Epoch: [10]  [2220/2809]  eta: 0:05:36  lr: 0.000044  min_lr: 0.000000  loss: 4.2272 (4.1074)  loss_scale: 65536.0000 (62909.8388)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1545  max mem: 15572
Epoch: [10]  [2230/2809]  eta: 0:05:30  lr: 0.000044  min_lr: 0.000000  loss: 4.1935 (4.1077)  loss_scale: 65536.0000 (62921.6100)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1203  max mem: 15572
Epoch: [10]  [2240/2809]  eta: 0:05:24  lr: 0.000044  min_lr: 0.000000  loss: 4.1890 (4.1080)  loss_scale: 65536.0000 (62933.2762)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1210  max mem: 15572
Epoch: [10]  [2250/2809]  eta: 0:05:19  lr: 0.000044  min_lr: 0.000000  loss: 4.1139 (4.1076)  loss_scale: 65536.0000 (62944.8387)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1403  max mem: 15572
[2025-01-13 01:39:17,556] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30343
[2025-01-13 01:39:17,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:39:17,557] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [2260/2809]  eta: 0:05:13  lr: 0.000044  min_lr: 0.000000  loss: 4.1449 (4.1081)  loss_scale: 65536.0000 (62840.3574)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1779  max mem: 15572
Epoch: [10]  [2270/2809]  eta: 0:05:07  lr: 0.000044  min_lr: 0.000000  loss: 4.1185 (4.1077)  loss_scale: 32768.0000 (62707.9384)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1841  max mem: 15572
Epoch: [10]  [2280/2809]  eta: 0:05:02  lr: 0.000044  min_lr: 0.000000  loss: 4.0727 (4.1075)  loss_scale: 32768.0000 (62576.6804)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1121  max mem: 15572
Epoch: [10]  [2290/2809]  eta: 0:04:56  lr: 0.000044  min_lr: 0.000000  loss: 4.1025 (4.1074)  loss_scale: 32768.0000 (62446.5683)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0838  max mem: 15572
Epoch: [10]  [2300/2809]  eta: 0:04:50  lr: 0.000044  min_lr: 0.000000  loss: 4.1389 (4.1071)  loss_scale: 32768.0000 (62317.5871)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.1988  max mem: 15572
Epoch: [10]  [2310/2809]  eta: 0:04:45  lr: 0.000044  min_lr: 0.000000  loss: 3.9679 (4.1070)  loss_scale: 32768.0000 (62189.7222)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1413  max mem: 15572
Epoch: [10]  [2320/2809]  eta: 0:04:39  lr: 0.000044  min_lr: 0.000000  loss: 4.0423 (4.1073)  loss_scale: 32768.0000 (62062.9591)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0915  max mem: 15572
Epoch: [10]  [2330/2809]  eta: 0:04:33  lr: 0.000044  min_lr: 0.000000  loss: 4.0605 (4.1072)  loss_scale: 32768.0000 (61937.2836)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.1078  max mem: 15572
Epoch: [10]  [2340/2809]  eta: 0:04:27  lr: 0.000044  min_lr: 0.000000  loss: 4.0157 (4.1075)  loss_scale: 32768.0000 (61812.6818)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1153  max mem: 15572
Epoch: [10]  [2350/2809]  eta: 0:04:22  lr: 0.000044  min_lr: 0.000000  loss: 4.1383 (4.1077)  loss_scale: 32768.0000 (61689.1399)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1427  max mem: 15572
Epoch: [10]  [2360/2809]  eta: 0:04:16  lr: 0.000044  min_lr: 0.000000  loss: 4.0409 (4.1071)  loss_scale: 32768.0000 (61566.6446)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0860  max mem: 15572
Epoch: [10]  [2370/2809]  eta: 0:04:10  lr: 0.000044  min_lr: 0.000000  loss: 4.0452 (4.1072)  loss_scale: 32768.0000 (61445.1826)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1168  max mem: 15572
Epoch: [10]  [2380/2809]  eta: 0:04:05  lr: 0.000044  min_lr: 0.000000  loss: 4.0403 (4.1072)  loss_scale: 32768.0000 (61324.7409)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1415  max mem: 15572
[2025-01-13 01:40:31,940] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:40:31,941] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [2390/2809]  eta: 0:03:59  lr: 0.000044  min_lr: 0.000000  loss: 4.0776 (4.1076)  loss_scale: 32768.0000 (61328.6491)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1017  max mem: 15572
Epoch: [10]  [2400/2809]  eta: 0:03:53  lr: 0.000044  min_lr: 0.000000  loss: 4.2111 (4.1079)  loss_scale: 65536.0000 (61346.1724)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1240  max mem: 15572
Epoch: [10]  [2410/2809]  eta: 0:03:47  lr: 0.000044  min_lr: 0.000000  loss: 4.1880 (4.1079)  loss_scale: 65536.0000 (61363.5504)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1311  max mem: 15572
Epoch: [10]  [2420/2809]  eta: 0:03:42  lr: 0.000044  min_lr: 0.000000  loss: 4.1497 (4.1076)  loss_scale: 65536.0000 (61380.7848)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1283  max mem: 15572
Epoch: [10]  [2430/2809]  eta: 0:03:36  lr: 0.000044  min_lr: 0.000000  loss: 3.9014 (4.1062)  loss_scale: 65536.0000 (61397.8774)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1180  max mem: 15572
Epoch: [10]  [2440/2809]  eta: 0:03:30  lr: 0.000044  min_lr: 0.000000  loss: 3.7463 (4.1053)  loss_scale: 65536.0000 (61414.8300)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1060  max mem: 15572
Epoch: [10]  [2450/2809]  eta: 0:03:24  lr: 0.000044  min_lr: 0.000000  loss: 3.8906 (4.1050)  loss_scale: 65536.0000 (61431.6442)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0946  max mem: 15572
Epoch: [10]  [2460/2809]  eta: 0:03:19  lr: 0.000044  min_lr: 0.000000  loss: 3.9226 (4.1046)  loss_scale: 65536.0000 (61448.3218)  weight_decay: 0.0500 (0.0500)  time: 0.4978  data: 0.0576  max mem: 15572
Epoch: [10]  [2470/2809]  eta: 0:03:13  lr: 0.000044  min_lr: 0.000000  loss: 3.9226 (4.1044)  loss_scale: 65536.0000 (61464.8644)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.0703  max mem: 15572
Epoch: [10]  [2480/2809]  eta: 0:03:07  lr: 0.000044  min_lr: 0.000000  loss: 4.0382 (4.1042)  loss_scale: 65536.0000 (61481.2737)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1018  max mem: 15572
[2025-01-13 01:41:28,478] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30574
[2025-01-13 01:41:28,479] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:41:28,479] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [2490/2809]  eta: 0:03:01  lr: 0.000044  min_lr: 0.000000  loss: 4.0382 (4.1042)  loss_scale: 65536.0000 (61405.4693)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1165  max mem: 15572
Epoch: [10]  [2500/2809]  eta: 0:02:56  lr: 0.000044  min_lr: 0.000000  loss: 4.0932 (4.1044)  loss_scale: 32768.0000 (61290.9652)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0893  max mem: 15572
Epoch: [10]  [2510/2809]  eta: 0:02:50  lr: 0.000044  min_lr: 0.000000  loss: 4.1476 (4.1050)  loss_scale: 32768.0000 (61177.3732)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.1055  max mem: 15572
Epoch: [10]  [2520/2809]  eta: 0:02:44  lr: 0.000044  min_lr: 0.000000  loss: 4.1476 (4.1051)  loss_scale: 32768.0000 (61064.6823)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1213  max mem: 15572
Epoch: [10]  [2530/2809]  eta: 0:02:39  lr: 0.000044  min_lr: 0.000000  loss: 4.1795 (4.1054)  loss_scale: 32768.0000 (60952.8819)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1569  max mem: 15572
Epoch: [10]  [2540/2809]  eta: 0:02:33  lr: 0.000044  min_lr: 0.000000  loss: 4.1970 (4.1054)  loss_scale: 32768.0000 (60841.9614)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.2054  max mem: 15572
Epoch: [10]  [2550/2809]  eta: 0:02:27  lr: 0.000044  min_lr: 0.000000  loss: 4.0712 (4.1056)  loss_scale: 32768.0000 (60731.9106)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1544  max mem: 15572
Epoch: [10]  [2560/2809]  eta: 0:02:22  lr: 0.000044  min_lr: 0.000000  loss: 4.0668 (4.1054)  loss_scale: 32768.0000 (60622.7193)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1261  max mem: 15572
Epoch: [10]  [2570/2809]  eta: 0:02:16  lr: 0.000044  min_lr: 0.000000  loss: 4.0410 (4.1056)  loss_scale: 32768.0000 (60514.3773)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1240  max mem: 15572
Epoch: [10]  [2580/2809]  eta: 0:02:10  lr: 0.000044  min_lr: 0.000000  loss: 4.0719 (4.1053)  loss_scale: 32768.0000 (60406.8749)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1643  max mem: 15572
Epoch: [10]  [2590/2809]  eta: 0:02:05  lr: 0.000044  min_lr: 0.000000  loss: 4.0330 (4.1046)  loss_scale: 32768.0000 (60300.2022)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1698  max mem: 15572
Epoch: [10]  [2600/2809]  eta: 0:01:59  lr: 0.000044  min_lr: 0.000000  loss: 3.7898 (4.1037)  loss_scale: 32768.0000 (60194.3499)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1105  max mem: 15572
Epoch: [10]  [2610/2809]  eta: 0:01:53  lr: 0.000044  min_lr: 0.000000  loss: 3.8735 (4.1035)  loss_scale: 32768.0000 (60089.3083)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1162  max mem: 15572
[2025-01-13 01:42:43,345] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:42:43,346] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [2620/2809]  eta: 0:01:47  lr: 0.000044  min_lr: 0.000000  loss: 4.1530 (4.1045)  loss_scale: 32768.0000 (60085.0851)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1640  max mem: 15572
Epoch: [10]  [2630/2809]  eta: 0:01:42  lr: 0.000044  min_lr: 0.000000  loss: 4.2575 (4.1046)  loss_scale: 65536.0000 (60105.8031)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0977  max mem: 15572
Epoch: [10]  [2640/2809]  eta: 0:01:36  lr: 0.000044  min_lr: 0.000000  loss: 4.2260 (4.1050)  loss_scale: 65536.0000 (60126.3643)  weight_decay: 0.0500 (0.0500)  time: 0.4343  data: 0.0005  max mem: 15572
Epoch: [10]  [2650/2809]  eta: 0:01:30  lr: 0.000044  min_lr: 0.000000  loss: 4.1759 (4.1049)  loss_scale: 65536.0000 (60146.7703)  weight_decay: 0.0500 (0.0500)  time: 0.4782  data: 0.0007  max mem: 15572
Epoch: [10]  [2660/2809]  eta: 0:01:24  lr: 0.000044  min_lr: 0.000000  loss: 4.0863 (4.1049)  loss_scale: 65536.0000 (60167.0229)  weight_decay: 0.0500 (0.0500)  time: 0.4784  data: 0.0009  max mem: 15572
Epoch: [10]  [2670/2809]  eta: 0:01:19  lr: 0.000044  min_lr: 0.000000  loss: 4.1074 (4.1051)  loss_scale: 65536.0000 (60187.1239)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0361  max mem: 15572
Epoch: [10]  [2680/2809]  eta: 0:01:13  lr: 0.000044  min_lr: 0.000000  loss: 4.2755 (4.1057)  loss_scale: 65536.0000 (60207.0750)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1015  max mem: 15572
Epoch: [10]  [2690/2809]  eta: 0:01:07  lr: 0.000044  min_lr: 0.000000  loss: 4.0976 (4.1056)  loss_scale: 65536.0000 (60226.8777)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.1544  max mem: 15572
Epoch: [10]  [2700/2809]  eta: 0:01:02  lr: 0.000044  min_lr: 0.000000  loss: 4.1310 (4.1061)  loss_scale: 65536.0000 (60246.5339)  weight_decay: 0.0500 (0.0500)  time: 0.6904  data: 0.1987  max mem: 15572
Epoch: [10]  [2710/2809]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000000  loss: 4.2556 (4.1070)  loss_scale: 65536.0000 (60266.0450)  weight_decay: 0.0500 (0.0500)  time: 0.7021  data: 0.2035  max mem: 15572
Epoch: [10]  [2720/2809]  eta: 0:00:50  lr: 0.000044  min_lr: 0.000000  loss: 4.2504 (4.1070)  loss_scale: 65536.0000 (60285.4127)  weight_decay: 0.0500 (0.0500)  time: 0.6758  data: 0.1761  max mem: 15572
Epoch: [10]  [2730/2809]  eta: 0:00:45  lr: 0.000044  min_lr: 0.000000  loss: 4.2504 (4.1079)  loss_scale: 65536.0000 (60304.6386)  weight_decay: 0.0500 (0.0500)  time: 0.6831  data: 0.1732  max mem: 15572
Epoch: [10]  [2740/2809]  eta: 0:00:39  lr: 0.000044  min_lr: 0.000000  loss: 4.3115 (4.1082)  loss_scale: 65536.0000 (60323.7242)  weight_decay: 0.0500 (0.0500)  time: 0.6838  data: 0.1792  max mem: 15572
[2025-01-13 01:43:59,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:43:59,980] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:44:04,344] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30836
[2025-01-13 01:44:04,344] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:44:04,344] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [2750/2809]  eta: 0:00:33  lr: 0.000044  min_lr: 0.000000  loss: 4.1715 (4.1081)  loss_scale: 65536.0000 (60461.7841)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.2017  max mem: 15572
Epoch: [10]  [2760/2809]  eta: 0:00:28  lr: 0.000044  min_lr: 0.000000  loss: 4.1266 (4.1081)  loss_scale: 65536.0000 (60480.1623)  weight_decay: 0.0500 (0.0500)  time: 0.7022  data: 0.2420  max mem: 15572
Epoch: [10]  [2770/2809]  eta: 0:00:22  lr: 0.000044  min_lr: 0.000000  loss: 4.2989 (4.1092)  loss_scale: 65536.0000 (60498.4078)  weight_decay: 0.0500 (0.0500)  time: 0.7112  data: 0.2452  max mem: 15572
Epoch: [10]  [2780/2809]  eta: 0:00:16  lr: 0.000044  min_lr: 0.000000  loss: 4.2979 (4.1097)  loss_scale: 65536.0000 (60516.5221)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.1161  max mem: 15572
Epoch: [10]  [2790/2809]  eta: 0:00:10  lr: 0.000044  min_lr: 0.000000  loss: 4.2817 (4.1096)  loss_scale: 65536.0000 (60534.5066)  weight_decay: 0.0500 (0.0500)  time: 0.4256  data: 0.0008  max mem: 15572
Epoch: [10]  [2800/2809]  eta: 0:00:05  lr: 0.000044  min_lr: 0.000000  loss: 4.2817 (4.1099)  loss_scale: 65536.0000 (60552.3627)  weight_decay: 0.0500 (0.0500)  time: 0.4321  data: 0.0009  max mem: 15572
Epoch: [10]  [2808/2809]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000000  loss: 4.1205 (4.1097)  loss_scale: 65536.0000 (60566.5561)  weight_decay: 0.0500 (0.0500)  time: 0.4063  data: 0.0006  max mem: 15572
Epoch: [10] Total time: 0:26:46 (0.5718 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000000  loss: 4.1205 (4.1097)  loss_scale: 65536.0000 (60566.5561)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:17:50  loss: 0.3830 (0.3830)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.9370  data: 3.7667  max mem: 15572
Val:  [ 10/272]  eta: 0:03:11  loss: 3.8433 (3.1981)  acc1: 0.0000 (24.2424)  acc5: 33.3333 (42.9293)  time: 0.7299  data: 0.5501  max mem: 15572
Val:  [ 20/272]  eta: 0:02:16  loss: 3.2184 (3.1392)  acc1: 27.7778 (27.5132)  acc5: 50.0000 (52.3810)  time: 0.3718  data: 0.1567  max mem: 15572
Val:  [ 30/272]  eta: 0:01:54  loss: 3.1282 (3.2063)  acc1: 22.2222 (25.0896)  acc5: 61.1111 (53.9427)  time: 0.3312  data: 0.0993  max mem: 15572
Val:  [ 40/272]  eta: 0:01:38  loss: 3.0553 (3.1579)  acc1: 16.6667 (24.5257)  acc5: 61.1111 (57.1816)  time: 0.3014  data: 0.1009  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 2.9999 (3.0787)  acc1: 16.6667 (26.9063)  acc5: 66.6667 (60.1307)  time: 0.2530  data: 0.0544  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 2.0483 (2.9003)  acc1: 55.5556 (32.5137)  acc5: 83.3333 (62.4772)  time: 0.2785  data: 0.0686  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 1.8284 (2.8246)  acc1: 55.5556 (33.5681)  acc5: 77.7778 (64.6322)  time: 0.2974  data: 0.1034  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 2.5237 (2.8271)  acc1: 27.7778 (33.9506)  acc5: 77.7778 (64.6091)  time: 0.3051  data: 0.1168  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 3.2163 (2.8780)  acc1: 22.2222 (32.2344)  acc5: 61.1111 (64.0415)  time: 0.3472  data: 0.1523  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 3.2163 (2.9240)  acc1: 22.2222 (31.6832)  acc5: 61.1111 (63.4763)  time: 0.2995  data: 0.1083  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.3834 (2.9973)  acc1: 0.0000 (29.3293)  acc5: 50.0000 (61.4615)  time: 0.3144  data: 0.1299  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.5487 (3.0347)  acc1: 5.5556 (28.3287)  acc5: 50.0000 (60.7438)  time: 0.3378  data: 0.1532  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 3.1615 (2.9903)  acc1: 22.2222 (29.6014)  acc5: 61.1111 (61.4080)  time: 0.3103  data: 0.1223  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 2.6588 (2.9794)  acc1: 38.8889 (30.6541)  acc5: 61.1111 (61.3081)  time: 0.3094  data: 0.1251  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.0160 (2.9788)  acc1: 27.7778 (30.2428)  acc5: 61.1111 (61.6998)  time: 0.2880  data: 0.0994  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.8226 (2.9595)  acc1: 33.3333 (31.3665)  acc5: 72.2222 (62.6294)  time: 0.2867  data: 0.1058  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.8882 (2.9829)  acc1: 33.3333 (30.6693)  acc5: 72.2222 (62.0533)  time: 0.3024  data: 0.1304  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.9012 (2.9701)  acc1: 22.2222 (30.7244)  acc5: 66.6667 (62.7686)  time: 0.3189  data: 0.1268  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.9551 (3.0099)  acc1: 22.2222 (29.9011)  acc5: 55.5556 (61.4892)  time: 0.3324  data: 0.1342  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.0771 (3.0168)  acc1: 5.5556 (29.5191)  acc5: 55.5556 (61.6363)  time: 0.2995  data: 0.1101  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.7733 (3.0170)  acc1: 27.7778 (30.1738)  acc5: 72.2222 (61.8747)  time: 0.2550  data: 0.0707  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.9069 (3.0062)  acc1: 33.3333 (30.4424)  acc5: 72.2222 (62.2675)  time: 0.2787  data: 0.0948  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.1192 (2.9724)  acc1: 50.0000 (31.8182)  acc5: 77.7778 (63.0351)  time: 0.3561  data: 0.1630  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.0379 (2.9486)  acc1: 55.5556 (32.6187)  acc5: 83.3333 (63.6468)  time: 0.3677  data: 0.1652  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.8657 (2.9661)  acc1: 33.3333 (32.2709)  acc5: 66.6667 (63.1031)  time: 0.3117  data: 0.1054  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.0989 (2.8934)  acc1: 66.6667 (34.4615)  acc5: 77.7778 (64.2401)  time: 0.2618  data: 0.0707  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.8757 (2.8944)  acc1: 55.5556 (34.0918)  acc5: 83.3333 (64.1451)  time: 0.2258  data: 0.0599  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8757 (2.8993)  acc1: 50.0000 (34.0569)  acc5: 83.3333 (64.1204)  time: 0.2106  data: 0.0505  max mem: 15572
Val: Total time: 0:01:26 (0.3198 s / it)
* Acc@1 34.057 Acc@5 64.120 loss 2.899
Accuracy of the network on the 4883 val videos: 34.1%
[2025-01-13 01:46:03,394] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 01:46:03,397] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 01:46:03,397] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 01:46:06,290] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 01:46:06,291] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 34.06%
Epoch: [11]  [   0/2809]  eta: 5:01:06  lr: 0.000044  min_lr: 0.000000  loss: 4.1017 (4.1017)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.4318  data: 6.0055  max mem: 15572
Epoch: [11]  [  10/2809]  eta: 0:51:48  lr: 0.000044  min_lr: 0.000000  loss: 4.2237 (4.2677)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1105  data: 0.6692  max mem: 15572
Epoch: [11]  [  20/2809]  eta: 0:39:11  lr: 0.000044  min_lr: 0.000000  loss: 4.2237 (4.2181)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1188  max mem: 15572
Epoch: [11]  [  30/2809]  eta: 0:34:18  lr: 0.000044  min_lr: 0.000000  loss: 4.0096 (4.1642)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.1016  max mem: 15572
Epoch: [11]  [  40/2809]  eta: 0:32:04  lr: 0.000044  min_lr: 0.000000  loss: 4.1060 (4.1453)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.1072  max mem: 15572
Epoch: [11]  [  50/2809]  eta: 0:31:09  lr: 0.000044  min_lr: 0.000000  loss: 4.1508 (4.1202)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1445  max mem: 15572
Epoch: [11]  [  60/2809]  eta: 0:30:18  lr: 0.000044  min_lr: 0.000000  loss: 4.0594 (4.0909)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1654  max mem: 15572
[2025-01-13 01:46:49,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:46:49,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:46:50,732] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30967
[2025-01-13 01:46:50,732] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:46:50,733] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [  70/2809]  eta: 0:29:25  lr: 0.000044  min_lr: 0.000000  loss: 4.0132 (4.0924)  loss_scale: 65536.0000 (67382.0845)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1320  max mem: 15572
Epoch: [11]  [  80/2809]  eta: 0:29:07  lr: 0.000044  min_lr: 0.000000  loss: 4.0132 (4.0986)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1226  max mem: 15572
Epoch: [11]  [  90/2809]  eta: 0:27:59  lr: 0.000044  min_lr: 0.000000  loss: 3.9796 (4.0735)  loss_scale: 65536.0000 (66976.3516)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0684  max mem: 15572
[2025-01-13 01:47:08,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=202, lr=[4.216826230280262e-07, 4.216826230280262e-07, 6.024037471828946e-07, 6.024037471828946e-07, 8.605767816898495e-07, 8.605767816898495e-07, 1.2293954024140709e-06, 1.2293954024140709e-06, 1.7562791463058155e-06, 1.7562791463058155e-06, 2.508970209008308e-06, 2.508970209008308e-06, 3.5842431557261544e-06, 3.5842431557261544e-06, 5.120347365323078e-06, 5.120347365323078e-06, 7.31478195046154e-06, 7.31478195046154e-06, 1.0449688500659345e-05, 1.0449688500659345e-05, 1.492812642951335e-05, 1.492812642951335e-05, 2.1325894899304786e-05, 2.1325894899304786e-05, 3.046556414186398e-05, 3.046556414186398e-05, 4.352223448837712e-05, 4.352223448837712e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 01:47:08,332] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=27.959495558722647, CurrSamplesPerSec=24.18951114084438, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [ 100/2809]  eta: 0:27:43  lr: 0.000044  min_lr: 0.000000  loss: 3.9775 (4.0787)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0652  max mem: 15572
Epoch: [11]  [ 110/2809]  eta: 0:27:22  lr: 0.000044  min_lr: 0.000000  loss: 3.9710 (4.0554)  loss_scale: 65536.0000 (66716.8288)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1303  max mem: 15572
Epoch: [11]  [ 120/2809]  eta: 0:27:26  lr: 0.000044  min_lr: 0.000000  loss: 3.9710 (4.0449)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1707  max mem: 15572
Epoch: [11]  [ 130/2809]  eta: 0:27:09  lr: 0.000044  min_lr: 0.000000  loss: 4.0947 (4.0601)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1583  max mem: 15572
Epoch: [11]  [ 140/2809]  eta: 0:26:54  lr: 0.000044  min_lr: 0.000000  loss: 4.0947 (4.0600)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1234  max mem: 15572
Epoch: [11]  [ 150/2809]  eta: 0:26:28  lr: 0.000044  min_lr: 0.000000  loss: 4.2227 (4.0724)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0970  max mem: 15572
Epoch: [11]  [ 160/2809]  eta: 0:26:21  lr: 0.000043  min_lr: 0.000000  loss: 4.3559 (4.0834)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.1079  max mem: 15572
Epoch: [11]  [ 170/2809]  eta: 0:26:15  lr: 0.000043  min_lr: 0.000000  loss: 4.0499 (4.0741)  loss_scale: 65536.0000 (66302.5029)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1452  max mem: 15572
Epoch: [11]  [ 180/2809]  eta: 0:25:56  lr: 0.000043  min_lr: 0.000000  loss: 3.9604 (4.0759)  loss_scale: 65536.0000 (66260.1547)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0974  max mem: 15572
Epoch: [11]  [ 190/2809]  eta: 0:25:48  lr: 0.000043  min_lr: 0.000000  loss: 4.0792 (4.0785)  loss_scale: 65536.0000 (66222.2408)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0977  max mem: 15572
[2025-01-13 01:48:04,242] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:48:04,243] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:48:05,111] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31098
[2025-01-13 01:48:05,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:48:05,113] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [ 200/2809]  eta: 0:25:47  lr: 0.000043  min_lr: 0.000000  loss: 4.2324 (4.0911)  loss_scale: 65536.0000 (66840.1990)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1580  max mem: 15572
Epoch: [11]  [ 210/2809]  eta: 0:25:33  lr: 0.000043  min_lr: 0.000000  loss: 4.2324 (4.0935)  loss_scale: 65536.0000 (66778.3886)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1429  max mem: 15572
Epoch: [11]  [ 220/2809]  eta: 0:25:29  lr: 0.000043  min_lr: 0.000000  loss: 4.1852 (4.0957)  loss_scale: 65536.0000 (66722.1719)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1314  max mem: 15572
Epoch: [11]  [ 230/2809]  eta: 0:25:31  lr: 0.000043  min_lr: 0.000000  loss: 4.3010 (4.1043)  loss_scale: 65536.0000 (66670.8225)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1860  max mem: 15572
Epoch: [11]  [ 240/2809]  eta: 0:25:23  lr: 0.000043  min_lr: 0.000000  loss: 4.2710 (4.1091)  loss_scale: 65536.0000 (66623.7344)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1691  max mem: 15572
Epoch: [11]  [ 250/2809]  eta: 0:25:16  lr: 0.000043  min_lr: 0.000000  loss: 4.0752 (4.1028)  loss_scale: 65536.0000 (66580.3984)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1365  max mem: 15572
Epoch: [11]  [ 260/2809]  eta: 0:24:58  lr: 0.000043  min_lr: 0.000000  loss: 3.9948 (4.1027)  loss_scale: 65536.0000 (66540.3831)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0904  max mem: 15572
Epoch: [11]  [ 270/2809]  eta: 0:24:54  lr: 0.000043  min_lr: 0.000000  loss: 4.0232 (4.1002)  loss_scale: 65536.0000 (66503.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.1062  max mem: 15572
Epoch: [11]  [ 280/2809]  eta: 0:24:45  lr: 0.000043  min_lr: 0.000000  loss: 4.0232 (4.0968)  loss_scale: 65536.0000 (66468.8968)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1530  max mem: 15572
Epoch: [11]  [ 290/2809]  eta: 0:24:38  lr: 0.000043  min_lr: 0.000000  loss: 4.1175 (4.0937)  loss_scale: 65536.0000 (66436.8385)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1359  max mem: 15572
Epoch: [11]  [ 300/2809]  eta: 0:24:37  lr: 0.000043  min_lr: 0.000000  loss: 3.9045 (4.0923)  loss_scale: 65536.0000 (66406.9103)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1789  max mem: 15572
Epoch: [11]  [ 310/2809]  eta: 0:24:27  lr: 0.000043  min_lr: 0.000000  loss: 3.9045 (4.0887)  loss_scale: 65536.0000 (66378.9068)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1576  max mem: 15572
[2025-01-13 01:49:12,973] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31213
[2025-01-13 01:49:12,974] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:49:12,974] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 320/2809]  eta: 0:24:27  lr: 0.000043  min_lr: 0.000000  loss: 3.9827 (4.0843)  loss_scale: 65536.0000 (65638.0810)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1448  max mem: 15572
Epoch: [11]  [ 330/2809]  eta: 0:24:10  lr: 0.000043  min_lr: 0.000000  loss: 3.9827 (4.0845)  loss_scale: 32768.0000 (64645.0272)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0967  max mem: 15572
Epoch: [11]  [ 340/2809]  eta: 0:24:01  lr: 0.000043  min_lr: 0.000000  loss: 4.0806 (4.0834)  loss_scale: 32768.0000 (63710.2170)  weight_decay: 0.0500 (0.0500)  time: 0.4914  data: 0.0520  max mem: 15572
Epoch: [11]  [ 350/2809]  eta: 0:23:57  lr: 0.000043  min_lr: 0.000000  loss: 4.1085 (4.0837)  loss_scale: 32768.0000 (62828.6724)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1106  max mem: 15572
Epoch: [11]  [ 360/2809]  eta: 0:23:46  lr: 0.000043  min_lr: 0.000000  loss: 4.1124 (4.0866)  loss_scale: 32768.0000 (61995.9668)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0837  max mem: 15572
Epoch: [11]  [ 370/2809]  eta: 0:23:37  lr: 0.000043  min_lr: 0.000000  loss: 4.0549 (4.0825)  loss_scale: 32768.0000 (61208.1509)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0804  max mem: 15572
Epoch: [11]  [ 380/2809]  eta: 0:23:26  lr: 0.000043  min_lr: 0.000000  loss: 4.0952 (4.0868)  loss_scale: 32768.0000 (60461.6903)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0883  max mem: 15572
Epoch: [11]  [ 390/2809]  eta: 0:23:20  lr: 0.000043  min_lr: 0.000000  loss: 4.1043 (4.0871)  loss_scale: 32768.0000 (59753.4118)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.1038  max mem: 15572
Epoch: [11]  [ 400/2809]  eta: 0:23:13  lr: 0.000043  min_lr: 0.000000  loss: 3.9414 (4.0831)  loss_scale: 32768.0000 (59080.4589)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1103  max mem: 15572
Epoch: [11]  [ 410/2809]  eta: 0:23:08  lr: 0.000043  min_lr: 0.000000  loss: 3.9951 (4.0839)  loss_scale: 32768.0000 (58440.2530)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1089  max mem: 15572
Epoch: [11]  [ 420/2809]  eta: 0:22:58  lr: 0.000043  min_lr: 0.000000  loss: 4.0510 (4.0807)  loss_scale: 32768.0000 (57830.4608)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1213  max mem: 15572
Epoch: [11]  [ 430/2809]  eta: 0:22:43  lr: 0.000043  min_lr: 0.000000  loss: 4.0131 (4.0837)  loss_scale: 32768.0000 (57248.9652)  weight_decay: 0.0500 (0.0500)  time: 0.4575  data: 0.0592  max mem: 15572
Epoch: [11]  [ 440/2809]  eta: 0:22:29  lr: 0.000043  min_lr: 0.000000  loss: 4.1789 (4.0862)  loss_scale: 32768.0000 (56693.8413)  weight_decay: 0.0500 (0.0500)  time: 0.4102  data: 0.0078  max mem: 15572
[2025-01-13 01:50:19,011] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:50:19,011] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 450/2809]  eta: 0:22:17  lr: 0.000043  min_lr: 0.000000  loss: 4.0891 (4.0826)  loss_scale: 32768.0000 (56744.5854)  weight_decay: 0.0500 (0.0500)  time: 0.4402  data: 0.0005  max mem: 15572
Epoch: [11]  [ 460/2809]  eta: 0:22:10  lr: 0.000043  min_lr: 0.000000  loss: 4.0865 (4.0840)  loss_scale: 65536.0000 (56935.2885)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0427  max mem: 15572
Epoch: [11]  [ 470/2809]  eta: 0:22:08  lr: 0.000043  min_lr: 0.000000  loss: 4.1674 (4.0846)  loss_scale: 65536.0000 (57117.8938)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1412  max mem: 15572
Epoch: [11]  [ 480/2809]  eta: 0:22:07  lr: 0.000043  min_lr: 0.000000  loss: 4.1455 (4.0822)  loss_scale: 65536.0000 (57292.9064)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.1929  max mem: 15572
Epoch: [11]  [ 490/2809]  eta: 0:22:08  lr: 0.000043  min_lr: 0.000000  loss: 4.2305 (4.0847)  loss_scale: 65536.0000 (57460.7902)  weight_decay: 0.0500 (0.0500)  time: 0.6895  data: 0.2218  max mem: 15572
Epoch: [11]  [ 500/2809]  eta: 0:22:05  lr: 0.000043  min_lr: 0.000000  loss: 4.1958 (4.0835)  loss_scale: 65536.0000 (57621.9721)  weight_decay: 0.0500 (0.0500)  time: 0.6679  data: 0.2046  max mem: 15572
Epoch: [11]  [ 510/2809]  eta: 0:22:11  lr: 0.000043  min_lr: 0.000000  loss: 3.9832 (4.0823)  loss_scale: 65536.0000 (57776.8454)  weight_decay: 0.0500 (0.0500)  time: 0.7248  data: 0.2654  max mem: 15572
Epoch: [11]  [ 520/2809]  eta: 0:22:05  lr: 0.000043  min_lr: 0.000000  loss: 3.9440 (4.0810)  loss_scale: 65536.0000 (57925.7735)  weight_decay: 0.0500 (0.0500)  time: 0.7094  data: 0.2672  max mem: 15572
Epoch: [11]  [ 530/2809]  eta: 0:22:09  lr: 0.000043  min_lr: 0.000000  loss: 4.0729 (4.0818)  loss_scale: 65536.0000 (58069.0923)  weight_decay: 0.0500 (0.0500)  time: 0.7000  data: 0.2489  max mem: 15572
Epoch: [11]  [ 540/2809]  eta: 0:22:02  lr: 0.000043  min_lr: 0.000000  loss: 4.1855 (4.0833)  loss_scale: 65536.0000 (58207.1128)  weight_decay: 0.0500 (0.0500)  time: 0.6762  data: 0.2202  max mem: 15572
Epoch: [11]  [ 550/2809]  eta: 0:22:03  lr: 0.000043  min_lr: 0.000000  loss: 4.0370 (4.0808)  loss_scale: 65536.0000 (58340.1234)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1886  max mem: 15572
Epoch: [11]  [ 560/2809]  eta: 0:22:02  lr: 0.000043  min_lr: 0.000000  loss: 3.9759 (4.0791)  loss_scale: 65536.0000 (58468.3922)  weight_decay: 0.0500 (0.0500)  time: 0.7259  data: 0.2770  max mem: 15572
Epoch: [11]  [ 570/2809]  eta: 0:21:52  lr: 0.000043  min_lr: 0.000000  loss: 4.0231 (4.0814)  loss_scale: 65536.0000 (58592.1681)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1874  max mem: 15572
[2025-01-13 01:51:41,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:51:41,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:51:43,357] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31474
[2025-01-13 01:51:43,357] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:51:43,358] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [ 580/2809]  eta: 0:21:40  lr: 0.000043  min_lr: 0.000000  loss: 4.0262 (4.0794)  loss_scale: 65536.0000 (59162.8778)  weight_decay: 0.0500 (0.0500)  time: 0.4578  data: 0.0491  max mem: 15572
Epoch: [11]  [ 590/2809]  eta: 0:21:29  lr: 0.000043  min_lr: 0.000000  loss: 4.1066 (4.0811)  loss_scale: 65536.0000 (59270.7140)  weight_decay: 0.0500 (0.0500)  time: 0.4300  data: 0.0004  max mem: 15572
Epoch: [11]  [ 600/2809]  eta: 0:21:19  lr: 0.000043  min_lr: 0.000000  loss: 4.0390 (4.0803)  loss_scale: 65536.0000 (59374.9617)  weight_decay: 0.0500 (0.0500)  time: 0.4523  data: 0.0007  max mem: 15572
Epoch: [11]  [ 610/2809]  eta: 0:21:09  lr: 0.000043  min_lr: 0.000000  loss: 4.0564 (4.0811)  loss_scale: 65536.0000 (59475.7971)  weight_decay: 0.0500 (0.0500)  time: 0.4693  data: 0.0130  max mem: 15572
[2025-01-13 01:52:01,792] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31514
[2025-01-13 01:52:01,792] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:52:01,792] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 620/2809]  eta: 0:21:05  lr: 0.000043  min_lr: 0.000000  loss: 4.1388 (4.0829)  loss_scale: 65536.0000 (59256.7858)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1017  max mem: 15572
Epoch: [11]  [ 630/2809]  eta: 0:20:59  lr: 0.000043  min_lr: 0.000000  loss: 4.2982 (4.0874)  loss_scale: 32768.0000 (58836.9952)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1500  max mem: 15572
Epoch: [11]  [ 640/2809]  eta: 0:20:55  lr: 0.000043  min_lr: 0.000000  loss: 4.2407 (4.0873)  loss_scale: 32768.0000 (58430.3027)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1583  max mem: 15572
Epoch: [11]  [ 650/2809]  eta: 0:20:49  lr: 0.000043  min_lr: 0.000000  loss: 4.0795 (4.0861)  loss_scale: 32768.0000 (58036.1045)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1519  max mem: 15572
Epoch: [11]  [ 660/2809]  eta: 0:20:44  lr: 0.000043  min_lr: 0.000000  loss: 3.8695 (4.0812)  loss_scale: 32768.0000 (57653.8336)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1342  max mem: 15572
Epoch: [11]  [ 670/2809]  eta: 0:20:37  lr: 0.000043  min_lr: 0.000000  loss: 4.1181 (4.0859)  loss_scale: 32768.0000 (57282.9568)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1308  max mem: 15572
Epoch: [11]  [ 680/2809]  eta: 0:20:28  lr: 0.000043  min_lr: 0.000000  loss: 4.3530 (4.0913)  loss_scale: 32768.0000 (56922.9721)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0647  max mem: 15572
Epoch: [11]  [ 690/2809]  eta: 0:20:22  lr: 0.000043  min_lr: 0.000000  loss: 4.2115 (4.0882)  loss_scale: 32768.0000 (56573.4067)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0858  max mem: 15572
Epoch: [11]  [ 700/2809]  eta: 0:20:14  lr: 0.000043  min_lr: 0.000000  loss: 4.0325 (4.0888)  loss_scale: 32768.0000 (56233.8146)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.1131  max mem: 15572
Epoch: [11]  [ 710/2809]  eta: 0:20:08  lr: 0.000043  min_lr: 0.000000  loss: 3.7359 (4.0849)  loss_scale: 32768.0000 (55903.7750)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1118  max mem: 15572
Epoch: [11]  [ 720/2809]  eta: 0:20:04  lr: 0.000043  min_lr: 0.000000  loss: 3.7283 (4.0852)  loss_scale: 32768.0000 (55582.8904)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1597  max mem: 15572
Epoch: [11]  [ 730/2809]  eta: 0:19:59  lr: 0.000043  min_lr: 0.000000  loss: 4.0105 (4.0825)  loss_scale: 32768.0000 (55270.7852)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1675  max mem: 15572
Epoch: [11]  [ 740/2809]  eta: 0:19:53  lr: 0.000043  min_lr: 0.000000  loss: 4.1011 (4.0842)  loss_scale: 32768.0000 (54967.1039)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1412  max mem: 15572
[2025-01-13 01:53:15,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:53:15,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 750/2809]  eta: 0:19:48  lr: 0.000043  min_lr: 0.000000  loss: 4.1293 (4.0834)  loss_scale: 32768.0000 (54976.9374)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1457  max mem: 15572
Epoch: [11]  [ 760/2809]  eta: 0:19:39  lr: 0.000043  min_lr: 0.000000  loss: 4.0099 (4.0819)  loss_scale: 65536.0000 (55115.6899)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0934  max mem: 15572
Epoch: [11]  [ 770/2809]  eta: 0:19:33  lr: 0.000043  min_lr: 0.000000  loss: 4.0099 (4.0840)  loss_scale: 65536.0000 (55250.8431)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0892  max mem: 15572
Epoch: [11]  [ 780/2809]  eta: 0:19:27  lr: 0.000043  min_lr: 0.000000  loss: 4.2304 (4.0840)  loss_scale: 65536.0000 (55382.5352)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1397  max mem: 15572
Epoch: [11]  [ 790/2809]  eta: 0:19:19  lr: 0.000043  min_lr: 0.000000  loss: 4.1910 (4.0837)  loss_scale: 65536.0000 (55510.8976)  weight_decay: 0.0500 (0.0500)  time: 0.5113  data: 0.0697  max mem: 15572
Epoch: [11]  [ 800/2809]  eta: 0:19:13  lr: 0.000043  min_lr: 0.000000  loss: 4.0528 (4.0837)  loss_scale: 65536.0000 (55636.0549)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0856  max mem: 15572
Epoch: [11]  [ 810/2809]  eta: 0:19:06  lr: 0.000043  min_lr: 0.000000  loss: 4.1117 (4.0849)  loss_scale: 65536.0000 (55758.1258)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1104  max mem: 15572
Epoch: [11]  [ 820/2809]  eta: 0:19:00  lr: 0.000043  min_lr: 0.000000  loss: 4.3081 (4.0847)  loss_scale: 65536.0000 (55877.2229)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0906  max mem: 15572
Epoch: [11]  [ 830/2809]  eta: 0:18:52  lr: 0.000043  min_lr: 0.000000  loss: 4.0907 (4.0856)  loss_scale: 65536.0000 (55993.4537)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0846  max mem: 15572
Epoch: [11]  [ 840/2809]  eta: 0:18:47  lr: 0.000043  min_lr: 0.000000  loss: 4.0907 (4.0842)  loss_scale: 65536.0000 (56106.9203)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.1015  max mem: 15572
Epoch: [11]  [ 850/2809]  eta: 0:18:41  lr: 0.000043  min_lr: 0.000000  loss: 4.0631 (4.0833)  loss_scale: 65536.0000 (56217.7203)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1331  max mem: 15572
Epoch: [11]  [ 860/2809]  eta: 0:18:35  lr: 0.000043  min_lr: 0.000000  loss: 4.0834 (4.0830)  loss_scale: 65536.0000 (56325.9466)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1174  max mem: 15572
Epoch: [11]  [ 870/2809]  eta: 0:18:29  lr: 0.000043  min_lr: 0.000000  loss: 4.3118 (4.0850)  loss_scale: 65536.0000 (56431.6877)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1197  max mem: 15572
[2025-01-13 01:54:25,987] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:54:25,988] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:54:26,890] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31772
[2025-01-13 01:54:26,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:54:26,891] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [ 880/2809]  eta: 0:18:23  lr: 0.000043  min_lr: 0.000000  loss: 4.2187 (4.0855)  loss_scale: 65536.0000 (56609.4166)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1135  max mem: 15572
Epoch: [11]  [ 890/2809]  eta: 0:18:17  lr: 0.000043  min_lr: 0.000000  loss: 4.2187 (4.0878)  loss_scale: 65536.0000 (56709.6027)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1331  max mem: 15572
Epoch: [11]  [ 900/2809]  eta: 0:18:13  lr: 0.000043  min_lr: 0.000000  loss: 4.3145 (4.0897)  loss_scale: 65536.0000 (56807.5649)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1739  max mem: 15572
Epoch: [11]  [ 910/2809]  eta: 0:18:06  lr: 0.000043  min_lr: 0.000000  loss: 4.0970 (4.0875)  loss_scale: 65536.0000 (56903.3765)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1164  max mem: 15572
Epoch: [11]  [ 920/2809]  eta: 0:18:00  lr: 0.000043  min_lr: 0.000000  loss: 4.0875 (4.0877)  loss_scale: 65536.0000 (56997.1075)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.1003  max mem: 15572
Epoch: [11]  [ 930/2809]  eta: 0:17:53  lr: 0.000043  min_lr: 0.000000  loss: 4.2272 (4.0898)  loss_scale: 65536.0000 (57088.8249)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0901  max mem: 15572
Epoch: [11]  [ 940/2809]  eta: 0:17:46  lr: 0.000043  min_lr: 0.000000  loss: 4.0605 (4.0887)  loss_scale: 65536.0000 (57178.5930)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0594  max mem: 15572
Epoch: [11]  [ 950/2809]  eta: 0:17:40  lr: 0.000043  min_lr: 0.000000  loss: 4.0618 (4.0900)  loss_scale: 65536.0000 (57266.4732)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0812  max mem: 15572
Epoch: [11]  [ 960/2809]  eta: 0:17:34  lr: 0.000043  min_lr: 0.000000  loss: 4.0381 (4.0870)  loss_scale: 65536.0000 (57352.5245)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0973  max mem: 15572
Epoch: [11]  [ 970/2809]  eta: 0:17:28  lr: 0.000043  min_lr: 0.000000  loss: 4.0619 (4.0871)  loss_scale: 65536.0000 (57436.8033)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1290  max mem: 15572
Epoch: [11]  [ 980/2809]  eta: 0:17:21  lr: 0.000043  min_lr: 0.000000  loss: 4.2015 (4.0893)  loss_scale: 65536.0000 (57519.3639)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1216  max mem: 15572
Epoch: [11]  [ 990/2809]  eta: 0:17:17  lr: 0.000043  min_lr: 0.000000  loss: 4.2015 (4.0892)  loss_scale: 65536.0000 (57600.2583)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1259  max mem: 15572
Epoch: [11]  [1000/2809]  eta: 0:17:12  lr: 0.000043  min_lr: 0.000000  loss: 3.8690 (4.0865)  loss_scale: 65536.0000 (57679.5365)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1613  max mem: 15572
[2025-01-13 01:55:39,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:55:39,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:55:43,551] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31908
[2025-01-13 01:55:43,551] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:55:43,551] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1010/2809]  eta: 0:17:07  lr: 0.000043  min_lr: 0.000000  loss: 3.9357 (4.0884)  loss_scale: 65536.0000 (58211.0069)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1475  max mem: 15572
Epoch: [11]  [1020/2809]  eta: 0:17:01  lr: 0.000043  min_lr: 0.000000  loss: 4.2026 (4.0895)  loss_scale: 65536.0000 (58282.7502)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1196  max mem: 15572
Epoch: [11]  [1030/2809]  eta: 0:16:55  lr: 0.000043  min_lr: 0.000000  loss: 4.0592 (4.0894)  loss_scale: 65536.0000 (58353.1018)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1413  max mem: 15572
Epoch: [11]  [1040/2809]  eta: 0:16:49  lr: 0.000043  min_lr: 0.000000  loss: 4.0592 (4.0897)  loss_scale: 65536.0000 (58422.1018)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1323  max mem: 15572
Epoch: [11]  [1050/2809]  eta: 0:16:43  lr: 0.000043  min_lr: 0.000000  loss: 4.1440 (4.0886)  loss_scale: 65536.0000 (58489.7888)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0959  max mem: 15572
Epoch: [11]  [1060/2809]  eta: 0:16:36  lr: 0.000043  min_lr: 0.000000  loss: 4.1759 (4.0887)  loss_scale: 65536.0000 (58556.1998)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0564  max mem: 15572
Epoch: [11]  [1070/2809]  eta: 0:16:30  lr: 0.000043  min_lr: 0.000000  loss: 4.0568 (4.0868)  loss_scale: 65536.0000 (58621.3707)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0875  max mem: 15572
Epoch: [11]  [1080/2809]  eta: 0:16:26  lr: 0.000043  min_lr: 0.000000  loss: 3.9666 (4.0856)  loss_scale: 65536.0000 (58685.3358)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1688  max mem: 15572
Epoch: [11]  [1090/2809]  eta: 0:16:21  lr: 0.000043  min_lr: 0.000000  loss: 3.9666 (4.0841)  loss_scale: 65536.0000 (58748.1283)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.2162  max mem: 15572
[2025-01-13 01:56:36,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=208, lr=[4.1784613185839324e-07, 4.1784613185839324e-07, 5.969230455119903e-07, 5.969230455119903e-07, 8.527472078742721e-07, 8.527472078742721e-07, 1.218210296963246e-06, 1.218210296963246e-06, 1.7403004242332084e-06, 1.7403004242332084e-06, 2.486143463190298e-06, 2.486143463190298e-06, 3.551633518843283e-06, 3.551633518843283e-06, 5.073762169776119e-06, 5.073762169776119e-06, 7.248231671108742e-06, 7.248231671108742e-06, 1.035461667301249e-05, 1.035461667301249e-05, 1.4792309532874983e-05, 1.4792309532874983e-05, 2.1131870761249978e-05, 2.1131870761249978e-05, 3.0188386801785686e-05, 3.0188386801785686e-05, 4.312626685969384e-05, 4.312626685969384e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 01:56:36,044] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=27.981190537913093, CurrSamplesPerSec=34.34196186959352, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [1100/2809]  eta: 0:16:16  lr: 0.000043  min_lr: 0.000000  loss: 3.9721 (4.0828)  loss_scale: 65536.0000 (58809.7802)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.2171  max mem: 15572
Epoch: [11]  [1110/2809]  eta: 0:16:10  lr: 0.000043  min_lr: 0.000000  loss: 4.0537 (4.0829)  loss_scale: 65536.0000 (58870.3222)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1572  max mem: 15572
Epoch: [11]  [1120/2809]  eta: 0:16:04  lr: 0.000043  min_lr: 0.000000  loss: 4.3011 (4.0853)  loss_scale: 65536.0000 (58929.7841)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1096  max mem: 15572
Epoch: [11]  [1130/2809]  eta: 0:15:56  lr: 0.000043  min_lr: 0.000000  loss: 4.2581 (4.0846)  loss_scale: 65536.0000 (58988.1945)  weight_decay: 0.0500 (0.0500)  time: 0.4868  data: 0.0598  max mem: 15572
[2025-01-13 01:56:54,298] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:56:54,298] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:56:55,116] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32039
[2025-01-13 01:56:55,117] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:56:55,117] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1140/2809]  eta: 0:15:48  lr: 0.000043  min_lr: 0.000000  loss: 3.9542 (4.0833)  loss_scale: 65536.0000 (59160.4557)  weight_decay: 0.0500 (0.0500)  time: 0.4076  data: 0.0119  max mem: 15572
Epoch: [11]  [1150/2809]  eta: 0:15:41  lr: 0.000043  min_lr: 0.000000  loss: 4.0089 (4.0827)  loss_scale: 65536.0000 (59215.8471)  weight_decay: 0.0500 (0.0500)  time: 0.4217  data: 0.0006  max mem: 15572
Epoch: [11]  [1160/2809]  eta: 0:15:36  lr: 0.000043  min_lr: 0.000000  loss: 3.9906 (4.0825)  loss_scale: 65536.0000 (59270.2842)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1063  max mem: 15572
Epoch: [11]  [1170/2809]  eta: 0:15:32  lr: 0.000043  min_lr: 0.000000  loss: 4.1276 (4.0834)  loss_scale: 65536.0000 (59323.7916)  weight_decay: 0.0500 (0.0500)  time: 0.6726  data: 0.2164  max mem: 15572
Epoch: [11]  [1180/2809]  eta: 0:15:28  lr: 0.000043  min_lr: 0.000000  loss: 4.2785 (4.0854)  loss_scale: 65536.0000 (59376.3929)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.2335  max mem: 15572
Epoch: [11]  [1190/2809]  eta: 0:15:24  lr: 0.000043  min_lr: 0.000000  loss: 4.1065 (4.0839)  loss_scale: 65536.0000 (59428.1108)  weight_decay: 0.0500 (0.0500)  time: 0.6854  data: 0.2341  max mem: 15572
Epoch: [11]  [1200/2809]  eta: 0:15:20  lr: 0.000043  min_lr: 0.000000  loss: 3.9722 (4.0832)  loss_scale: 65536.0000 (59478.9675)  weight_decay: 0.0500 (0.0500)  time: 0.6802  data: 0.1974  max mem: 15572
Epoch: [11]  [1210/2809]  eta: 0:15:15  lr: 0.000043  min_lr: 0.000000  loss: 3.9803 (4.0828)  loss_scale: 65536.0000 (59528.9843)  weight_decay: 0.0500 (0.0500)  time: 0.6689  data: 0.1779  max mem: 15572
Epoch: [11]  [1220/2809]  eta: 0:15:11  lr: 0.000043  min_lr: 0.000000  loss: 4.0832 (4.0842)  loss_scale: 65536.0000 (59578.1818)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.2050  max mem: 15572
Epoch: [11]  [1230/2809]  eta: 0:15:07  lr: 0.000043  min_lr: 0.000000  loss: 4.2688 (4.0854)  loss_scale: 65536.0000 (59626.5800)  weight_decay: 0.0500 (0.0500)  time: 0.7010  data: 0.2487  max mem: 15572
Epoch: [11]  [1240/2809]  eta: 0:15:02  lr: 0.000043  min_lr: 0.000000  loss: 4.1678 (4.0854)  loss_scale: 65536.0000 (59674.1982)  weight_decay: 0.0500 (0.0500)  time: 0.6745  data: 0.2029  max mem: 15572
Epoch: [11]  [1250/2809]  eta: 0:14:57  lr: 0.000043  min_lr: 0.000000  loss: 4.1030 (4.0857)  loss_scale: 65536.0000 (59721.0552)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1673  max mem: 15572
Epoch: [11]  [1260/2809]  eta: 0:14:53  lr: 0.000043  min_lr: 0.000000  loss: 4.0443 (4.0845)  loss_scale: 65536.0000 (59767.1689)  weight_decay: 0.0500 (0.0500)  time: 0.6843  data: 0.2383  max mem: 15572
[2025-01-13 01:58:19,017] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:58:19,017] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [1270/2809]  eta: 0:14:47  lr: 0.000043  min_lr: 0.000000  loss: 3.9255 (4.0854)  loss_scale: 65536.0000 (59915.6821)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.2044  max mem: 15572
[2025-01-13 01:58:20,536] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32172
[2025-01-13 01:58:20,536] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:58:20,536] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1280/2809]  eta: 0:14:39  lr: 0.000043  min_lr: 0.000000  loss: 4.3743 (4.0865)  loss_scale: 65536.0000 (60061.8767)  weight_decay: 0.0500 (0.0500)  time: 0.4698  data: 0.0660  max mem: 15572
Epoch: [11]  [1290/2809]  eta: 0:14:31  lr: 0.000043  min_lr: 0.000000  loss: 4.2784 (4.0870)  loss_scale: 65536.0000 (60104.2789)  weight_decay: 0.0500 (0.0500)  time: 0.4101  data: 0.0004  max mem: 15572
Epoch: [11]  [1300/2809]  eta: 0:14:26  lr: 0.000043  min_lr: 0.000000  loss: 4.0051 (4.0857)  loss_scale: 65536.0000 (60146.0292)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0815  max mem: 15572
Epoch: [11]  [1310/2809]  eta: 0:14:20  lr: 0.000043  min_lr: 0.000000  loss: 3.9897 (4.0867)  loss_scale: 65536.0000 (60187.1426)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1643  max mem: 15572
Epoch: [11]  [1320/2809]  eta: 0:14:16  lr: 0.000043  min_lr: 0.000000  loss: 4.1505 (4.0874)  loss_scale: 65536.0000 (60227.6336)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.2177  max mem: 15572
Epoch: [11]  [1330/2809]  eta: 0:14:10  lr: 0.000043  min_lr: 0.000000  loss: 4.1764 (4.0874)  loss_scale: 65536.0000 (60267.5162)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.1956  max mem: 15572
Epoch: [11]  [1340/2809]  eta: 0:14:05  lr: 0.000043  min_lr: 0.000000  loss: 4.0818 (4.0855)  loss_scale: 65536.0000 (60306.8039)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1236  max mem: 15572
Epoch: [11]  [1350/2809]  eta: 0:13:58  lr: 0.000043  min_lr: 0.000000  loss: 4.0357 (4.0847)  loss_scale: 65536.0000 (60345.5100)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0866  max mem: 15572
Epoch: [11]  [1360/2809]  eta: 0:13:52  lr: 0.000043  min_lr: 0.000000  loss: 4.1998 (4.0850)  loss_scale: 65536.0000 (60383.6473)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0977  max mem: 15572
Epoch: [11]  [1370/2809]  eta: 0:13:46  lr: 0.000043  min_lr: 0.000000  loss: 4.3082 (4.0859)  loss_scale: 65536.0000 (60421.2283)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1406  max mem: 15572
Epoch: [11]  [1380/2809]  eta: 0:13:40  lr: 0.000043  min_lr: 0.000000  loss: 4.1752 (4.0870)  loss_scale: 65536.0000 (60458.2650)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.1073  max mem: 15572
Epoch: [11]  [1390/2809]  eta: 0:13:36  lr: 0.000043  min_lr: 0.000000  loss: 4.0741 (4.0868)  loss_scale: 65536.0000 (60494.7692)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1709  max mem: 15572
Epoch: [11]  [1400/2809]  eta: 0:13:30  lr: 0.000043  min_lr: 0.000000  loss: 4.2018 (4.0877)  loss_scale: 65536.0000 (60530.7523)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1825  max mem: 15572
[2025-01-13 01:59:33,427] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:59:33,427] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:59:34,236] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32303
[2025-01-13 01:59:34,236] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:59:34,236] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1410/2809]  eta: 0:13:25  lr: 0.000043  min_lr: 0.000000  loss: 4.2000 (4.0874)  loss_scale: 65536.0000 (60659.1184)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1549  max mem: 15572
Epoch: [11]  [1420/2809]  eta: 0:13:18  lr: 0.000043  min_lr: 0.000000  loss: 4.2000 (4.0884)  loss_scale: 65536.0000 (60693.4384)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1355  max mem: 15572
Epoch: [11]  [1430/2809]  eta: 0:13:13  lr: 0.000043  min_lr: 0.000000  loss: 4.1960 (4.0884)  loss_scale: 65536.0000 (60727.2788)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1209  max mem: 15572
Epoch: [11]  [1440/2809]  eta: 0:13:07  lr: 0.000043  min_lr: 0.000000  loss: 4.0832 (4.0882)  loss_scale: 65536.0000 (60760.6495)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1423  max mem: 15572
Epoch: [11]  [1450/2809]  eta: 0:13:01  lr: 0.000043  min_lr: 0.000000  loss: 4.0832 (4.0887)  loss_scale: 65536.0000 (60793.5603)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1004  max mem: 15572
Epoch: [11]  [1460/2809]  eta: 0:12:56  lr: 0.000043  min_lr: 0.000000  loss: 4.1892 (4.0888)  loss_scale: 65536.0000 (60826.0205)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1246  max mem: 15572
Epoch: [11]  [1470/2809]  eta: 0:12:49  lr: 0.000043  min_lr: 0.000000  loss: 4.0023 (4.0870)  loss_scale: 65536.0000 (60858.0394)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1182  max mem: 15572
Epoch: [11]  [1480/2809]  eta: 0:12:44  lr: 0.000043  min_lr: 0.000000  loss: 3.8737 (4.0871)  loss_scale: 65536.0000 (60889.6259)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0821  max mem: 15572
Epoch: [11]  [1490/2809]  eta: 0:12:38  lr: 0.000043  min_lr: 0.000000  loss: 4.0121 (4.0880)  loss_scale: 65536.0000 (60920.7887)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1328  max mem: 15572
Epoch: [11]  [1500/2809]  eta: 0:12:32  lr: 0.000043  min_lr: 0.000000  loss: 4.0121 (4.0869)  loss_scale: 65536.0000 (60951.5363)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1454  max mem: 15572
Epoch: [11]  [1510/2809]  eta: 0:12:26  lr: 0.000043  min_lr: 0.000000  loss: 4.1566 (4.0879)  loss_scale: 65536.0000 (60981.8769)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.0903  max mem: 15572
Epoch: [11]  [1520/2809]  eta: 0:12:20  lr: 0.000043  min_lr: 0.000000  loss: 4.2055 (4.0867)  loss_scale: 65536.0000 (61011.8185)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.1086  max mem: 15572
Epoch: [11]  [1530/2809]  eta: 0:12:14  lr: 0.000043  min_lr: 0.000000  loss: 3.9239 (4.0853)  loss_scale: 65536.0000 (61041.3690)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0902  max mem: 15572
[2025-01-13 02:00:47,106] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:00:47,106] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:00:51,038] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32436
[2025-01-13 02:00:51,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:00:51,039] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1540/2809]  eta: 0:12:09  lr: 0.000043  min_lr: 0.000000  loss: 3.9239 (4.0849)  loss_scale: 65536.0000 (61240.6489)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1330  max mem: 15572
Epoch: [11]  [1550/2809]  eta: 0:12:02  lr: 0.000043  min_lr: 0.000000  loss: 3.9769 (4.0848)  loss_scale: 65536.0000 (61268.3430)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1288  max mem: 15572
Epoch: [11]  [1560/2809]  eta: 0:11:56  lr: 0.000043  min_lr: 0.000000  loss: 4.0167 (4.0847)  loss_scale: 65536.0000 (61295.6823)  weight_decay: 0.0500 (0.0500)  time: 0.4875  data: 0.0406  max mem: 15572
Epoch: [11]  [1570/2809]  eta: 0:11:51  lr: 0.000043  min_lr: 0.000000  loss: 4.0473 (4.0846)  loss_scale: 65536.0000 (61322.6735)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1292  max mem: 15572
Epoch: [11]  [1580/2809]  eta: 0:11:45  lr: 0.000043  min_lr: 0.000000  loss: 4.2254 (4.0851)  loss_scale: 65536.0000 (61349.3232)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1610  max mem: 15572
Epoch: [11]  [1590/2809]  eta: 0:11:39  lr: 0.000043  min_lr: 0.000000  loss: 4.2254 (4.0852)  loss_scale: 65536.0000 (61375.6380)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1106  max mem: 15572
Epoch: [11]  [1600/2809]  eta: 0:11:33  lr: 0.000043  min_lr: 0.000000  loss: 4.0927 (4.0856)  loss_scale: 65536.0000 (61401.6240)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1043  max mem: 15572
Epoch: [11]  [1610/2809]  eta: 0:11:28  lr: 0.000043  min_lr: 0.000000  loss: 4.1812 (4.0863)  loss_scale: 65536.0000 (61427.2874)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1412  max mem: 15572
Epoch: [11]  [1620/2809]  eta: 0:11:21  lr: 0.000043  min_lr: 0.000000  loss: 4.0459 (4.0853)  loss_scale: 65536.0000 (61452.6342)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1220  max mem: 15572
Epoch: [11]  [1630/2809]  eta: 0:11:16  lr: 0.000043  min_lr: 0.000000  loss: 3.8835 (4.0833)  loss_scale: 65536.0000 (61477.6701)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1211  max mem: 15572
Epoch: [11]  [1640/2809]  eta: 0:11:10  lr: 0.000043  min_lr: 0.000000  loss: 3.9274 (4.0836)  loss_scale: 65536.0000 (61502.4010)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1158  max mem: 15572
Epoch: [11]  [1650/2809]  eta: 0:11:04  lr: 0.000043  min_lr: 0.000000  loss: 4.0628 (4.0835)  loss_scale: 65536.0000 (61526.8322)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0694  max mem: 15572
Epoch: [11]  [1660/2809]  eta: 0:10:58  lr: 0.000043  min_lr: 0.000000  loss: 4.0401 (4.0824)  loss_scale: 65536.0000 (61550.9693)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0755  max mem: 15572
[2025-01-13 02:02:02,481] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:02:02,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:02:03,063] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32566
[2025-01-13 02:02:03,063] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:02:03,063] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1670/2809]  eta: 0:10:52  lr: 0.000043  min_lr: 0.000000  loss: 3.9683 (4.0829)  loss_scale: 65536.0000 (61614.0371)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1111  max mem: 15572
Epoch: [11]  [1680/2809]  eta: 0:10:46  lr: 0.000043  min_lr: 0.000000  loss: 4.3893 (4.0844)  loss_scale: 65536.0000 (61637.3682)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1046  max mem: 15572
Epoch: [11]  [1690/2809]  eta: 0:10:41  lr: 0.000043  min_lr: 0.000000  loss: 4.1498 (4.0832)  loss_scale: 65536.0000 (61660.4234)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1225  max mem: 15572
Epoch: [11]  [1700/2809]  eta: 0:10:35  lr: 0.000043  min_lr: 0.000000  loss: 3.8675 (4.0822)  loss_scale: 65536.0000 (61683.2075)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1169  max mem: 15572
Epoch: [11]  [1710/2809]  eta: 0:10:29  lr: 0.000043  min_lr: 0.000000  loss: 4.0686 (4.0824)  loss_scale: 65536.0000 (61705.7253)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0794  max mem: 15572
Epoch: [11]  [1720/2809]  eta: 0:10:23  lr: 0.000043  min_lr: 0.000000  loss: 4.0686 (4.0814)  loss_scale: 65536.0000 (61727.9814)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1386  max mem: 15572
Epoch: [11]  [1730/2809]  eta: 0:10:17  lr: 0.000043  min_lr: 0.000000  loss: 3.9314 (4.0804)  loss_scale: 65536.0000 (61749.9804)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1276  max mem: 15572
Epoch: [11]  [1740/2809]  eta: 0:10:12  lr: 0.000043  min_lr: 0.000000  loss: 4.2420 (4.0816)  loss_scale: 65536.0000 (61771.7266)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1703  max mem: 15572
Epoch: [11]  [1750/2809]  eta: 0:10:06  lr: 0.000043  min_lr: 0.000000  loss: 4.3064 (4.0811)  loss_scale: 65536.0000 (61793.2244)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1645  max mem: 15572
Epoch: [11]  [1760/2809]  eta: 0:09:59  lr: 0.000043  min_lr: 0.000000  loss: 3.9986 (4.0804)  loss_scale: 65536.0000 (61814.4781)  weight_decay: 0.0500 (0.0500)  time: 0.4528  data: 0.0400  max mem: 15572
Epoch: [11]  [1770/2809]  eta: 0:09:53  lr: 0.000043  min_lr: 0.000000  loss: 4.0196 (4.0812)  loss_scale: 65536.0000 (61835.4918)  weight_decay: 0.0500 (0.0500)  time: 0.3984  data: 0.0005  max mem: 15572
Epoch: [11]  [1780/2809]  eta: 0:09:46  lr: 0.000043  min_lr: 0.000000  loss: 4.1580 (4.0812)  loss_scale: 65536.0000 (61856.2695)  weight_decay: 0.0500 (0.0500)  time: 0.4300  data: 0.0008  max mem: 15572
Epoch: [11]  [1790/2809]  eta: 0:09:41  lr: 0.000043  min_lr: 0.000000  loss: 3.9613 (4.0798)  loss_scale: 65536.0000 (61876.8152)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1127  max mem: 15572
[2025-01-13 02:03:13,645] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:03:13,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [1800/2809]  eta: 0:09:36  lr: 0.000043  min_lr: 0.000000  loss: 3.8215 (4.0774)  loss_scale: 65536.0000 (62079.0761)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.1696  max mem: 15572
[2025-01-13 02:03:20,445] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32705
[2025-01-13 02:03:20,445] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:03:20,445] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1810/2809]  eta: 0:09:31  lr: 0.000043  min_lr: 0.000000  loss: 3.7635 (4.0766)  loss_scale: 65536.0000 (62279.1033)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1643  max mem: 15572
Epoch: [11]  [1820/2809]  eta: 0:09:26  lr: 0.000043  min_lr: 0.000000  loss: 3.9561 (4.0770)  loss_scale: 65536.0000 (62296.9885)  weight_decay: 0.0500 (0.0500)  time: 0.6969  data: 0.2379  max mem: 15572
Epoch: [11]  [1830/2809]  eta: 0:09:20  lr: 0.000043  min_lr: 0.000000  loss: 4.2209 (4.0775)  loss_scale: 65536.0000 (62314.6783)  weight_decay: 0.0500 (0.0500)  time: 0.6885  data: 0.2139  max mem: 15572
Epoch: [11]  [1840/2809]  eta: 0:09:15  lr: 0.000043  min_lr: 0.000000  loss: 4.1912 (4.0772)  loss_scale: 65536.0000 (62332.1760)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1599  max mem: 15572
Epoch: [11]  [1850/2809]  eta: 0:09:09  lr: 0.000043  min_lr: 0.000000  loss: 4.0314 (4.0770)  loss_scale: 65536.0000 (62349.4846)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1615  max mem: 15572
Epoch: [11]  [1860/2809]  eta: 0:09:04  lr: 0.000043  min_lr: 0.000000  loss: 4.0115 (4.0774)  loss_scale: 65536.0000 (62366.6072)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1749  max mem: 15572
Epoch: [11]  [1870/2809]  eta: 0:08:59  lr: 0.000043  min_lr: 0.000000  loss: 4.0247 (4.0772)  loss_scale: 65536.0000 (62383.5468)  weight_decay: 0.0500 (0.0500)  time: 0.6686  data: 0.2268  max mem: 15572
Epoch: [11]  [1880/2809]  eta: 0:08:53  lr: 0.000043  min_lr: 0.000000  loss: 4.0247 (4.0770)  loss_scale: 65536.0000 (62400.3062)  weight_decay: 0.0500 (0.0500)  time: 0.6686  data: 0.2327  max mem: 15572
Epoch: [11]  [1890/2809]  eta: 0:08:49  lr: 0.000043  min_lr: 0.000000  loss: 3.9688 (4.0773)  loss_scale: 65536.0000 (62416.8884)  weight_decay: 0.0500 (0.0500)  time: 0.7071  data: 0.2600  max mem: 15572
Epoch: [11]  [1900/2809]  eta: 0:08:43  lr: 0.000043  min_lr: 0.000000  loss: 3.9688 (4.0772)  loss_scale: 65536.0000 (62433.2962)  weight_decay: 0.0500 (0.0500)  time: 0.6988  data: 0.2617  max mem: 15572
Epoch: [11]  [1910/2809]  eta: 0:08:37  lr: 0.000043  min_lr: 0.000000  loss: 4.0830 (4.0775)  loss_scale: 65536.0000 (62449.5322)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0982  max mem: 15572
Epoch: [11]  [1920/2809]  eta: 0:08:30  lr: 0.000043  min_lr: 0.000000  loss: 4.0541 (4.0772)  loss_scale: 65536.0000 (62465.5992)  weight_decay: 0.0500 (0.0500)  time: 0.4210  data: 0.0006  max mem: 15572
Epoch: [11]  [1930/2809]  eta: 0:08:24  lr: 0.000043  min_lr: 0.000000  loss: 4.0600 (4.0783)  loss_scale: 65536.0000 (62481.4997)  weight_decay: 0.0500 (0.0500)  time: 0.4608  data: 0.0290  max mem: 15572
[2025-01-13 02:04:37,791] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:04:37,791] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:04:40,237] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32837
[2025-01-13 02:04:40,238] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:04:40,239] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1940/2809]  eta: 0:08:18  lr: 0.000043  min_lr: 0.000000  loss: 4.2490 (4.0780)  loss_scale: 65536.0000 (62598.5286)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0854  max mem: 15572
Epoch: [11]  [1950/2809]  eta: 0:08:13  lr: 0.000043  min_lr: 0.000000  loss: 3.9370 (4.0775)  loss_scale: 65536.0000 (62613.5848)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1551  max mem: 15572
Epoch: [11]  [1960/2809]  eta: 0:08:07  lr: 0.000043  min_lr: 0.000000  loss: 3.9590 (4.0773)  loss_scale: 65536.0000 (62628.4875)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1323  max mem: 15572
Epoch: [11]  [1970/2809]  eta: 0:08:02  lr: 0.000043  min_lr: 0.000000  loss: 4.0569 (4.0775)  loss_scale: 65536.0000 (62643.2390)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1161  max mem: 15572
Epoch: [11]  [1980/2809]  eta: 0:07:56  lr: 0.000043  min_lr: 0.000000  loss: 4.0271 (4.0775)  loss_scale: 65536.0000 (62657.8415)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1622  max mem: 15572
Epoch: [11]  [1990/2809]  eta: 0:07:51  lr: 0.000043  min_lr: 0.000000  loss: 3.9951 (4.0769)  loss_scale: 65536.0000 (62672.2973)  weight_decay: 0.0500 (0.0500)  time: 0.6456  data: 0.1792  max mem: 15572
Epoch: [11]  [2000/2809]  eta: 0:07:45  lr: 0.000043  min_lr: 0.000000  loss: 4.1801 (4.0784)  loss_scale: 65536.0000 (62686.6087)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1613  max mem: 15572
Epoch: [11]  [2010/2809]  eta: 0:07:39  lr: 0.000043  min_lr: 0.000000  loss: 4.1801 (4.0780)  loss_scale: 65536.0000 (62700.7777)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1273  max mem: 15572
Epoch: [11]  [2020/2809]  eta: 0:07:33  lr: 0.000043  min_lr: 0.000000  loss: 3.9870 (4.0770)  loss_scale: 65536.0000 (62714.8065)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1670  max mem: 15572
Epoch: [11]  [2030/2809]  eta: 0:07:28  lr: 0.000043  min_lr: 0.000000  loss: 3.8829 (4.0759)  loss_scale: 65536.0000 (62728.6972)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.2035  max mem: 15572
Epoch: [11]  [2040/2809]  eta: 0:07:22  lr: 0.000043  min_lr: 0.000000  loss: 3.9972 (4.0767)  loss_scale: 65536.0000 (62742.4517)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1677  max mem: 15572
Epoch: [11]  [2050/2809]  eta: 0:07:16  lr: 0.000043  min_lr: 0.000000  loss: 4.2039 (4.0769)  loss_scale: 65536.0000 (62756.0722)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1476  max mem: 15572
Epoch: [11]  [2060/2809]  eta: 0:07:10  lr: 0.000043  min_lr: 0.000000  loss: 4.0028 (4.0768)  loss_scale: 65536.0000 (62769.5604)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1077  max mem: 15572
[2025-01-13 02:05:57,514] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:05:57,514] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [2070/2809]  eta: 0:07:05  lr: 0.000043  min_lr: 0.000000  loss: 4.0028 (4.0769)  loss_scale: 65536.0000 (62909.4969)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0943  max mem: 15572
[2025-01-13 02:06:02,076] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32975
[2025-01-13 02:06:02,077] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:06:02,077] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2080/2809]  eta: 0:06:59  lr: 0.000043  min_lr: 0.000000  loss: 4.0345 (4.0771)  loss_scale: 65536.0000 (63079.5810)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0844  max mem: 15572
[2025-01-13 02:06:05,040] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32982
[2025-01-13 02:06:05,041] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 02:06:05,041] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [2090/2809]  eta: 0:06:53  lr: 0.000043  min_lr: 0.000000  loss: 4.0175 (4.0764)  loss_scale: 65536.0000 (62965.9608)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0617  max mem: 15572
[2025-01-13 02:06:14,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=33000, skipped=217, lr=[4.1381510514265254e-07, 4.1381510514265254e-07, 5.911644359180752e-07, 5.911644359180752e-07, 8.445206227401074e-07, 8.445206227401074e-07, 1.2064580324858679e-06, 1.2064580324858679e-06, 1.7235114749798113e-06, 1.7235114749798113e-06, 2.462159249971159e-06, 2.462159249971159e-06, 3.517370357101656e-06, 3.517370357101656e-06, 5.024814795859509e-06, 5.024814795859509e-06, 7.17830685122787e-06, 7.17830685122787e-06, 1.0254724073182673e-05, 1.0254724073182673e-05, 1.464960581883239e-05, 1.464960581883239e-05, 2.09280083126177e-05, 2.09280083126177e-05, 2.9897154732311005e-05, 2.9897154732311005e-05, 4.271022104615858e-05, 4.271022104615858e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 02:06:14,959] [INFO] [timer.py:260:stop] epoch=0/micro_step=33000/global_step=33000, RunningAvgSamplesPerSec=27.996027591516803, CurrSamplesPerSec=26.58258261430321, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [2100/2809]  eta: 0:06:47  lr: 0.000043  min_lr: 0.000000  loss: 3.9668 (4.0760)  loss_scale: 32768.0000 (62822.2294)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1235  max mem: 15572
Epoch: [11]  [2110/2809]  eta: 0:06:41  lr: 0.000043  min_lr: 0.000000  loss: 4.1266 (4.0766)  loss_scale: 32768.0000 (62679.8598)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1629  max mem: 15572
Epoch: [11]  [2120/2809]  eta: 0:06:35  lr: 0.000043  min_lr: 0.000000  loss: 3.9859 (4.0762)  loss_scale: 32768.0000 (62538.8326)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0980  max mem: 15572
Epoch: [11]  [2130/2809]  eta: 0:06:30  lr: 0.000043  min_lr: 0.000000  loss: 3.9586 (4.0765)  loss_scale: 32768.0000 (62399.1290)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1066  max mem: 15572
Epoch: [11]  [2140/2809]  eta: 0:06:24  lr: 0.000043  min_lr: 0.000000  loss: 4.0883 (4.0765)  loss_scale: 32768.0000 (62260.7305)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1114  max mem: 15572
Epoch: [11]  [2150/2809]  eta: 0:06:18  lr: 0.000043  min_lr: 0.000000  loss: 4.1708 (4.0769)  loss_scale: 32768.0000 (62123.6188)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1403  max mem: 15572
Epoch: [11]  [2160/2809]  eta: 0:06:13  lr: 0.000043  min_lr: 0.000000  loss: 4.1589 (4.0774)  loss_scale: 32768.0000 (61987.7760)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.2098  max mem: 15572
Epoch: [11]  [2170/2809]  eta: 0:06:07  lr: 0.000043  min_lr: 0.000000  loss: 4.1104 (4.0766)  loss_scale: 32768.0000 (61853.1847)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1563  max mem: 15572
Epoch: [11]  [2180/2809]  eta: 0:06:01  lr: 0.000043  min_lr: 0.000000  loss: 4.0802 (4.0767)  loss_scale: 32768.0000 (61719.8276)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1112  max mem: 15572
Epoch: [11]  [2190/2809]  eta: 0:05:55  lr: 0.000043  min_lr: 0.000000  loss: 4.1781 (4.0769)  loss_scale: 32768.0000 (61587.6878)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0978  max mem: 15572
Epoch: [11]  [2200/2809]  eta: 0:05:50  lr: 0.000043  min_lr: 0.000000  loss: 4.1918 (4.0766)  loss_scale: 32768.0000 (61456.7488)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1511  max mem: 15572
Epoch: [11]  [2210/2809]  eta: 0:05:44  lr: 0.000043  min_lr: 0.000000  loss: 4.2594 (4.0774)  loss_scale: 32768.0000 (61326.9941)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1412  max mem: 15572
[2025-01-13 02:07:19,670] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:07:19,670] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [2220/2809]  eta: 0:05:38  lr: 0.000043  min_lr: 0.000000  loss: 4.3316 (4.0782)  loss_scale: 32768.0000 (61331.1914)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0948  max mem: 15572
Epoch: [11]  [2230/2809]  eta: 0:05:32  lr: 0.000043  min_lr: 0.000000  loss: 4.0984 (4.0777)  loss_scale: 65536.0000 (61350.0385)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1334  max mem: 15572
Epoch: [11]  [2240/2809]  eta: 0:05:26  lr: 0.000043  min_lr: 0.000000  loss: 3.9231 (4.0762)  loss_scale: 65536.0000 (61368.7175)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1111  max mem: 15572
Epoch: [11]  [2250/2809]  eta: 0:05:21  lr: 0.000043  min_lr: 0.000000  loss: 3.7976 (4.0756)  loss_scale: 65536.0000 (61387.2306)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0656  max mem: 15572
Epoch: [11]  [2260/2809]  eta: 0:05:15  lr: 0.000043  min_lr: 0.000000  loss: 3.8089 (4.0751)  loss_scale: 65536.0000 (61405.5798)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1216  max mem: 15572
Epoch: [11]  [2270/2809]  eta: 0:05:09  lr: 0.000043  min_lr: 0.000000  loss: 3.9821 (4.0750)  loss_scale: 65536.0000 (61423.7675)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1118  max mem: 15572
Epoch: [11]  [2280/2809]  eta: 0:05:04  lr: 0.000043  min_lr: 0.000000  loss: 4.0182 (4.0744)  loss_scale: 65536.0000 (61441.7957)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1260  max mem: 15572
Epoch: [11]  [2290/2809]  eta: 0:04:58  lr: 0.000043  min_lr: 0.000000  loss: 4.1366 (4.0744)  loss_scale: 65536.0000 (61459.6665)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1316  max mem: 15572
Epoch: [11]  [2300/2809]  eta: 0:04:52  lr: 0.000043  min_lr: 0.000000  loss: 4.0802 (4.0741)  loss_scale: 65536.0000 (61477.3820)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0713  max mem: 15572
Epoch: [11]  [2310/2809]  eta: 0:04:46  lr: 0.000043  min_lr: 0.000000  loss: 3.8378 (4.0741)  loss_scale: 65536.0000 (61494.9442)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0707  max mem: 15572
Epoch: [11]  [2320/2809]  eta: 0:04:40  lr: 0.000043  min_lr: 0.000000  loss: 3.8766 (4.0736)  loss_scale: 65536.0000 (61512.3550)  weight_decay: 0.0500 (0.0500)  time: 0.4875  data: 0.0396  max mem: 15572
Epoch: [11]  [2330/2809]  eta: 0:04:34  lr: 0.000043  min_lr: 0.000000  loss: 3.8620 (4.0732)  loss_scale: 65536.0000 (61529.6165)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0866  max mem: 15572
[2025-01-13 02:08:29,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:08:29,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [2340/2809]  eta: 0:04:28  lr: 0.000043  min_lr: 0.000000  loss: 4.0211 (4.0735)  loss_scale: 65536.0000 (61574.7253)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0978  max mem: 15572
[2025-01-13 02:08:30,036] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33240
[2025-01-13 02:08:30,036] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:08:30,037] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2350/2809]  eta: 0:04:22  lr: 0.000043  min_lr: 0.000000  loss: 4.1082 (4.0734)  loss_scale: 65536.0000 (61591.5746)  weight_decay: 0.0500 (0.0500)  time: 0.4292  data: 0.0173  max mem: 15572
Epoch: [11]  [2360/2809]  eta: 0:04:17  lr: 0.000043  min_lr: 0.000000  loss: 4.0660 (4.0734)  loss_scale: 65536.0000 (61608.2812)  weight_decay: 0.0500 (0.0500)  time: 0.4619  data: 0.0007  max mem: 15572
Epoch: [11]  [2370/2809]  eta: 0:04:11  lr: 0.000043  min_lr: 0.000000  loss: 4.1324 (4.0748)  loss_scale: 65536.0000 (61624.8469)  weight_decay: 0.0500 (0.0500)  time: 0.4985  data: 0.0010  max mem: 15572
Epoch: [11]  [2380/2809]  eta: 0:04:05  lr: 0.000043  min_lr: 0.000000  loss: 4.1845 (4.0747)  loss_scale: 65536.0000 (61641.2734)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0177  max mem: 15572
Epoch: [11]  [2390/2809]  eta: 0:03:59  lr: 0.000043  min_lr: 0.000000  loss: 4.0985 (4.0749)  loss_scale: 65536.0000 (61657.5625)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0541  max mem: 15572
Epoch: [11]  [2400/2809]  eta: 0:03:54  lr: 0.000043  min_lr: 0.000000  loss: 4.1629 (4.0754)  loss_scale: 65536.0000 (61673.7160)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1660  max mem: 15572
Epoch: [11]  [2410/2809]  eta: 0:03:48  lr: 0.000043  min_lr: 0.000000  loss: 4.1489 (4.0757)  loss_scale: 65536.0000 (61689.7354)  weight_decay: 0.0500 (0.0500)  time: 0.7203  data: 0.2589  max mem: 15572
Epoch: [11]  [2420/2809]  eta: 0:03:42  lr: 0.000043  min_lr: 0.000000  loss: 4.0913 (4.0760)  loss_scale: 65536.0000 (61705.6225)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.2019  max mem: 15572
Epoch: [11]  [2430/2809]  eta: 0:03:37  lr: 0.000043  min_lr: 0.000000  loss: 4.1358 (4.0764)  loss_scale: 65536.0000 (61721.3789)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1440  max mem: 15572
Epoch: [11]  [2440/2809]  eta: 0:03:31  lr: 0.000043  min_lr: 0.000000  loss: 4.0427 (4.0758)  loss_scale: 65536.0000 (61737.0061)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0919  max mem: 15572
Epoch: [11]  [2450/2809]  eta: 0:03:25  lr: 0.000043  min_lr: 0.000000  loss: 3.9028 (4.0755)  loss_scale: 65536.0000 (61752.5059)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1144  max mem: 15572
Epoch: [11]  [2460/2809]  eta: 0:03:19  lr: 0.000043  min_lr: 0.000000  loss: 3.8768 (4.0751)  loss_scale: 65536.0000 (61767.8797)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1605  max mem: 15572
[2025-01-13 02:09:43,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:09:43,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [2470/2809]  eta: 0:03:14  lr: 0.000043  min_lr: 0.000000  loss: 3.9670 (4.0747)  loss_scale: 65536.0000 (61809.6512)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1605  max mem: 15572
[2025-01-13 02:09:45,693] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33371
[2025-01-13 02:09:45,693] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:09:45,694] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2480/2809]  eta: 0:03:08  lr: 0.000043  min_lr: 0.000000  loss: 4.0398 (4.0743)  loss_scale: 65536.0000 (61851.0859)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1719  max mem: 15572
Epoch: [11]  [2490/2809]  eta: 0:03:03  lr: 0.000043  min_lr: 0.000000  loss: 4.1828 (4.0746)  loss_scale: 65536.0000 (61865.8788)  weight_decay: 0.0500 (0.0500)  time: 0.6508  data: 0.1669  max mem: 15572
Epoch: [11]  [2500/2809]  eta: 0:02:57  lr: 0.000043  min_lr: 0.000000  loss: 4.1367 (4.0747)  loss_scale: 65536.0000 (61880.5534)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.1884  max mem: 15572
Epoch: [11]  [2510/2809]  eta: 0:02:51  lr: 0.000043  min_lr: 0.000000  loss: 4.0451 (4.0746)  loss_scale: 65536.0000 (61895.1111)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0994  max mem: 15572
Epoch: [11]  [2520/2809]  eta: 0:02:45  lr: 0.000043  min_lr: 0.000000  loss: 4.0775 (4.0745)  loss_scale: 65536.0000 (61909.5534)  weight_decay: 0.0500 (0.0500)  time: 0.4486  data: 0.0007  max mem: 15572
Epoch: [11]  [2530/2809]  eta: 0:02:39  lr: 0.000043  min_lr: 0.000000  loss: 4.0775 (4.0748)  loss_scale: 65536.0000 (61923.8815)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0463  max mem: 15572
Epoch: [11]  [2540/2809]  eta: 0:02:34  lr: 0.000043  min_lr: 0.000000  loss: 4.1186 (4.0740)  loss_scale: 65536.0000 (61938.0968)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1055  max mem: 15572
Epoch: [11]  [2550/2809]  eta: 0:02:28  lr: 0.000043  min_lr: 0.000000  loss: 4.0652 (4.0739)  loss_scale: 65536.0000 (61952.2007)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0995  max mem: 15572
Epoch: [11]  [2560/2809]  eta: 0:02:22  lr: 0.000043  min_lr: 0.000000  loss: 4.0528 (4.0735)  loss_scale: 65536.0000 (61966.1945)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1163  max mem: 15572
Epoch: [11]  [2570/2809]  eta: 0:02:17  lr: 0.000043  min_lr: 0.000000  loss: 4.0528 (4.0741)  loss_scale: 65536.0000 (61980.0793)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1715  max mem: 15572
Epoch: [11]  [2580/2809]  eta: 0:02:11  lr: 0.000043  min_lr: 0.000000  loss: 4.1639 (4.0742)  loss_scale: 65536.0000 (61993.8566)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1739  max mem: 15572
Epoch: [11]  [2590/2809]  eta: 0:02:05  lr: 0.000042  min_lr: 0.000000  loss: 4.1359 (4.0746)  loss_scale: 65536.0000 (62007.5276)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1013  max mem: 15572
Epoch: [11]  [2600/2809]  eta: 0:01:59  lr: 0.000042  min_lr: 0.000000  loss: 4.1486 (4.0748)  loss_scale: 65536.0000 (62021.0934)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0626  max mem: 15572
[2025-01-13 02:10:57,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:10:57,742] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [2610/2809]  eta: 0:01:53  lr: 0.000042  min_lr: 0.000000  loss: 4.1955 (4.0751)  loss_scale: 65536.0000 (62285.5550)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0708  max mem: 15572
[2025-01-13 02:11:06,753] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33515
[2025-01-13 02:11:06,753] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:11:06,753] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2620/2809]  eta: 0:01:48  lr: 0.000042  min_lr: 0.000000  loss: 4.2550 (4.0757)  loss_scale: 131072.0000 (62422.9775)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1160  max mem: 15572
Epoch: [11]  [2630/2809]  eta: 0:01:42  lr: 0.000042  min_lr: 0.000000  loss: 4.2567 (4.0764)  loss_scale: 65536.0000 (62434.8096)  weight_decay: 0.0500 (0.0500)  time: 0.6755  data: 0.1979  max mem: 15572
Epoch: [11]  [2640/2809]  eta: 0:01:36  lr: 0.000042  min_lr: 0.000000  loss: 4.0571 (4.0759)  loss_scale: 65536.0000 (62446.5521)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1417  max mem: 15572
Epoch: [11]  [2650/2809]  eta: 0:01:31  lr: 0.000042  min_lr: 0.000000  loss: 4.0890 (4.0757)  loss_scale: 65536.0000 (62458.2060)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0774  max mem: 15572
Epoch: [11]  [2660/2809]  eta: 0:01:25  lr: 0.000042  min_lr: 0.000000  loss: 4.1554 (4.0764)  loss_scale: 65536.0000 (62469.7723)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0748  max mem: 15572
Epoch: [11]  [2670/2809]  eta: 0:01:19  lr: 0.000042  min_lr: 0.000000  loss: 4.2873 (4.0761)  loss_scale: 65536.0000 (62481.2520)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.1051  max mem: 15572
Epoch: [11]  [2680/2809]  eta: 0:01:13  lr: 0.000042  min_lr: 0.000000  loss: 4.1257 (4.0763)  loss_scale: 65536.0000 (62492.6460)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1558  max mem: 15572
Epoch: [11]  [2690/2809]  eta: 0:01:08  lr: 0.000042  min_lr: 0.000000  loss: 4.1116 (4.0763)  loss_scale: 65536.0000 (62503.9554)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1428  max mem: 15572
Epoch: [11]  [2700/2809]  eta: 0:01:02  lr: 0.000042  min_lr: 0.000000  loss: 4.0954 (4.0765)  loss_scale: 65536.0000 (62515.1810)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1291  max mem: 15572
Epoch: [11]  [2710/2809]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000000  loss: 4.0792 (4.0762)  loss_scale: 65536.0000 (62526.3239)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1459  max mem: 15572
Epoch: [11]  [2720/2809]  eta: 0:00:51  lr: 0.000042  min_lr: 0.000000  loss: 4.0060 (4.0763)  loss_scale: 65536.0000 (62537.3848)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1398  max mem: 15572
Epoch: [11]  [2730/2809]  eta: 0:00:45  lr: 0.000042  min_lr: 0.000000  loss: 4.3049 (4.0772)  loss_scale: 65536.0000 (62548.3647)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0805  max mem: 15572
Epoch: [11]  [2740/2809]  eta: 0:00:39  lr: 0.000042  min_lr: 0.000000  loss: 3.8867 (4.0763)  loss_scale: 65536.0000 (62559.2645)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0678  max mem: 15572
[2025-01-13 02:12:19,809] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:12:19,809] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:12:22,311] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33646
[2025-01-13 02:12:22,311] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:12:22,311] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2750/2809]  eta: 0:00:33  lr: 0.000042  min_lr: 0.000000  loss: 3.8037 (4.0756)  loss_scale: 65536.0000 (62617.7303)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1192  max mem: 15572
Epoch: [11]  [2760/2809]  eta: 0:00:28  lr: 0.000042  min_lr: 0.000000  loss: 3.9598 (4.0757)  loss_scale: 65536.0000 (62628.2999)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1562  max mem: 15572
Epoch: [11]  [2770/2809]  eta: 0:00:22  lr: 0.000042  min_lr: 0.000000  loss: 3.9598 (4.0745)  loss_scale: 65536.0000 (62638.7932)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1660  max mem: 15572
Epoch: [11]  [2780/2809]  eta: 0:00:16  lr: 0.000042  min_lr: 0.000000  loss: 4.0161 (4.0749)  loss_scale: 65536.0000 (62649.2111)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1480  max mem: 15572
Epoch: [11]  [2790/2809]  eta: 0:00:10  lr: 0.000042  min_lr: 0.000000  loss: 3.8034 (4.0735)  loss_scale: 65536.0000 (62659.5543)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1393  max mem: 15572
Epoch: [11]  [2800/2809]  eta: 0:00:05  lr: 0.000042  min_lr: 0.000000  loss: 3.7512 (4.0741)  loss_scale: 65536.0000 (62669.8236)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.1182  max mem: 15572
Epoch: [11]  [2808/2809]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000000  loss: 4.0217 (4.0739)  loss_scale: 65536.0000 (62677.9865)  weight_decay: 0.0500 (0.0500)  time: 0.4414  data: 0.0432  max mem: 15572
Epoch: [11] Total time: 0:26:49 (0.5731 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000000  loss: 4.0217 (4.0739)  loss_scale: 65536.0000 (62677.9865)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:27:07  loss: 0.4568 (0.4568)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.9828  data: 5.6726  max mem: 15572
Val:  [ 10/272]  eta: 0:03:49  loss: 3.6130 (3.1578)  acc1: 5.5556 (26.2626)  acc5: 33.3333 (44.4444)  time: 0.8768  data: 0.6555  max mem: 15572
Val:  [ 20/272]  eta: 0:02:20  loss: 3.2946 (3.1208)  acc1: 16.6667 (25.6614)  acc5: 50.0000 (51.3228)  time: 0.2883  data: 0.0772  max mem: 15572
Val:  [ 30/272]  eta: 0:01:49  loss: 3.2946 (3.2343)  acc1: 16.6667 (21.6846)  acc5: 55.5556 (52.6882)  time: 0.2168  data: 0.0007  max mem: 15572
Val:  [ 40/272]  eta: 0:01:37  loss: 3.0308 (3.1688)  acc1: 16.6667 (22.6287)  acc5: 61.1111 (57.0461)  time: 0.2770  data: 0.0520  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 2.8811 (3.0664)  acc1: 22.2222 (25.0545)  acc5: 72.2222 (60.8932)  time: 0.3666  data: 0.1482  max mem: 15572
Val:  [ 60/272]  eta: 0:01:26  loss: 1.9412 (2.9093)  acc1: 44.4444 (30.6922)  acc5: 83.3333 (63.4791)  time: 0.3806  data: 0.1704  max mem: 15572
Val:  [ 70/272]  eta: 0:01:18  loss: 1.8960 (2.8126)  acc1: 61.1111 (32.7074)  acc5: 77.7778 (65.5712)  time: 0.3178  data: 0.1240  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 2.5211 (2.8269)  acc1: 33.3333 (33.0590)  acc5: 72.2222 (65.2949)  time: 0.2608  data: 0.0817  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 3.1032 (2.8685)  acc1: 22.2222 (32.4176)  acc5: 66.6667 (64.7131)  time: 0.3049  data: 0.1208  max mem: 15572
Val:  [100/272]  eta: 0:01:02  loss: 3.1994 (2.9243)  acc1: 22.2222 (31.6282)  acc5: 61.1111 (63.8614)  time: 0.3355  data: 0.1499  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 3.3624 (2.9927)  acc1: 11.1111 (29.7798)  acc5: 50.0000 (61.9620)  time: 0.3206  data: 0.1403  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 3.5905 (3.0239)  acc1: 5.5556 (29.1552)  acc5: 44.4444 (61.3407)  time: 0.3442  data: 0.1447  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 3.0277 (2.9720)  acc1: 27.7778 (30.7888)  acc5: 66.6667 (62.1713)  time: 0.3121  data: 0.0892  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 2.4187 (2.9549)  acc1: 38.8889 (31.5209)  acc5: 72.2222 (62.0567)  time: 0.2319  data: 0.0269  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 3.0844 (2.9605)  acc1: 22.2222 (30.8315)  acc5: 66.6667 (62.5460)  time: 0.1904  data: 0.0031  max mem: 15572
Val:  [160/272]  eta: 0:00:36  loss: 2.8893 (2.9412)  acc1: 27.7778 (32.0221)  acc5: 77.7778 (63.4231)  time: 0.1845  data: 0.0008  max mem: 15572
Val:  [170/272]  eta: 0:00:32  loss: 3.0806 (2.9773)  acc1: 22.2222 (30.9942)  acc5: 66.6667 (62.6381)  time: 0.2044  data: 0.0008  max mem: 15572
Val:  [180/272]  eta: 0:00:28  loss: 3.0529 (2.9607)  acc1: 22.2222 (31.2462)  acc5: 66.6667 (63.4745)  time: 0.2493  data: 0.0255  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.9233 (2.9987)  acc1: 22.2222 (30.3956)  acc5: 61.1111 (62.0419)  time: 0.3184  data: 0.0885  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.9233 (3.0037)  acc1: 16.6667 (30.2101)  acc5: 50.0000 (61.9679)  time: 0.3754  data: 0.1516  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.7631 (3.0019)  acc1: 38.8889 (30.9637)  acc5: 72.2222 (62.2170)  time: 0.4084  data: 0.1994  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.7749 (2.9929)  acc1: 44.4444 (31.1966)  acc5: 72.2222 (62.4686)  time: 0.4035  data: 0.1923  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.2850 (2.9558)  acc1: 50.0000 (32.6840)  acc5: 72.2222 (63.1313)  time: 0.4099  data: 0.1843  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1044 (2.9255)  acc1: 50.0000 (33.2411)  acc5: 83.3333 (64.1309)  time: 0.4018  data: 0.1738  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4420 (2.9402)  acc1: 38.8889 (32.9128)  acc5: 77.7778 (63.7672)  time: 0.2900  data: 0.0862  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.8531 (2.8621)  acc1: 77.7778 (35.1213)  acc5: 77.7778 (64.9212)  time: 0.2561  data: 0.0690  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.8531 (2.8641)  acc1: 61.1111 (34.9323)  acc5: 83.3333 (64.7396)  time: 0.2481  data: 0.0758  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8768 (2.8688)  acc1: 61.1111 (34.9171)  acc5: 83.3333 (64.7143)  time: 0.2401  data: 0.0757  max mem: 15572
Val: Total time: 0:01:28 (0.3249 s / it)
* Acc@1 34.917 Acc@5 64.714 loss 2.869
Accuracy of the network on the 4883 val videos: 34.9%
[2025-01-13 02:14:24,469] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 02:14:24,473] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 02:14:24,473] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 02:14:27,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 02:14:27,535] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 34.92%
Epoch: [12]  [   0/2809]  eta: 7:39:45  lr: 0.000042  min_lr: 0.000000  loss: 4.3040 (4.3040)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.8203  data: 9.3543  max mem: 15572
Epoch: [12]  [  10/2809]  eta: 1:02:14  lr: 0.000042  min_lr: 0.000000  loss: 4.0382 (3.9625)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3342  data: 0.9084  max mem: 15572
Epoch: [12]  [  20/2809]  eta: 0:47:46  lr: 0.000042  min_lr: 0.000000  loss: 3.9385 (3.9284)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1621  max mem: 15572
Epoch: [12]  [  30/2809]  eta: 0:39:22  lr: 0.000042  min_lr: 0.000000  loss: 4.0558 (3.9836)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1434  max mem: 15572
Epoch: [12]  [  40/2809]  eta: 0:38:29  lr: 0.000042  min_lr: 0.000000  loss: 4.1445 (3.9947)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1765  max mem: 15572
Epoch: [12]  [  50/2809]  eta: 0:36:27  lr: 0.000042  min_lr: 0.000000  loss: 4.0184 (3.9833)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7043  data: 0.2479  max mem: 15572
Epoch: [12]  [  60/2809]  eta: 0:34:11  lr: 0.000042  min_lr: 0.000000  loss: 3.8369 (3.9720)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1274  max mem: 15572
[2025-01-13 02:15:16,058] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:15:16,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:15:16,626] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33776
[2025-01-13 02:15:16,626] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:15:16,627] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [  70/2809]  eta: 0:32:00  lr: 0.000042  min_lr: 0.000000  loss: 3.9144 (3.9792)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.4677  data: 0.0467  max mem: 15572
Epoch: [12]  [  80/2809]  eta: 0:30:37  lr: 0.000042  min_lr: 0.000000  loss: 3.9144 (3.9750)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.4511  data: 0.0285  max mem: 15572
Epoch: [12]  [  90/2809]  eta: 0:29:53  lr: 0.000042  min_lr: 0.000000  loss: 3.9497 (3.9749)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0864  max mem: 15572
Epoch: [12]  [ 100/2809]  eta: 0:29:09  lr: 0.000042  min_lr: 0.000000  loss: 4.1694 (4.0075)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.1063  max mem: 15572
Epoch: [12]  [ 110/2809]  eta: 0:28:48  lr: 0.000042  min_lr: 0.000000  loss: 4.2931 (4.0211)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1214  max mem: 15572
Epoch: [12]  [ 120/2809]  eta: 0:28:22  lr: 0.000042  min_lr: 0.000000  loss: 4.1125 (4.0139)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1333  max mem: 15572
Epoch: [12]  [ 130/2809]  eta: 0:27:57  lr: 0.000042  min_lr: 0.000000  loss: 4.0188 (4.0210)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.1100  max mem: 15572
Epoch: [12]  [ 140/2809]  eta: 0:27:32  lr: 0.000042  min_lr: 0.000000  loss: 4.1785 (4.0338)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.1051  max mem: 15572
Epoch: [12]  [ 150/2809]  eta: 0:27:29  lr: 0.000042  min_lr: 0.000000  loss: 4.2302 (4.0493)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1359  max mem: 15572
Epoch: [12]  [ 160/2809]  eta: 0:26:54  lr: 0.000042  min_lr: 0.000000  loss: 4.2006 (4.0509)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0884  max mem: 15572
Epoch: [12]  [ 170/2809]  eta: 0:26:50  lr: 0.000042  min_lr: 0.000000  loss: 4.1923 (4.0550)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0939  max mem: 15572
Epoch: [12]  [ 180/2809]  eta: 0:26:33  lr: 0.000042  min_lr: 0.000000  loss: 4.2078 (4.0575)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1405  max mem: 15572
Epoch: [12]  [ 190/2809]  eta: 0:26:19  lr: 0.000042  min_lr: 0.000000  loss: 4.1315 (4.0565)  loss_scale: 65536.0000 (65879.1204)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1135  max mem: 15572
[2025-01-13 02:16:27,588] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:16:27,588] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:16:27,996] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33906
[2025-01-13 02:16:27,996] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:16:27,998] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 200/2809]  eta: 0:26:13  lr: 0.000042  min_lr: 0.000000  loss: 3.9741 (4.0514)  loss_scale: 65536.0000 (66188.0995)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1510  max mem: 15572
[2025-01-13 02:16:35,501] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33918
[2025-01-13 02:16:35,502] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 02:16:35,503] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 210/2809]  eta: 0:26:15  lr: 0.000042  min_lr: 0.000000  loss: 4.0144 (4.0458)  loss_scale: 65536.0000 (66001.8957)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1963  max mem: 15572
Epoch: [12]  [ 220/2809]  eta: 0:25:57  lr: 0.000042  min_lr: 0.000000  loss: 4.0144 (4.0462)  loss_scale: 32768.0000 (64498.0995)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1341  max mem: 15572
Epoch: [12]  [ 230/2809]  eta: 0:25:45  lr: 0.000042  min_lr: 0.000000  loss: 4.0367 (4.0461)  loss_scale: 32768.0000 (63124.5022)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.0887  max mem: 15572
Epoch: [12]  [ 240/2809]  eta: 0:25:34  lr: 0.000042  min_lr: 0.000000  loss: 3.9679 (4.0429)  loss_scale: 32768.0000 (61864.8963)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1193  max mem: 15572
Epoch: [12]  [ 250/2809]  eta: 0:25:26  lr: 0.000042  min_lr: 0.000000  loss: 3.8350 (4.0317)  loss_scale: 32768.0000 (60705.6574)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1233  max mem: 15572
Epoch: [12]  [ 260/2809]  eta: 0:25:14  lr: 0.000042  min_lr: 0.000000  loss: 3.7493 (4.0284)  loss_scale: 32768.0000 (59635.2490)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1151  max mem: 15572
Epoch: [12]  [ 270/2809]  eta: 0:25:06  lr: 0.000042  min_lr: 0.000000  loss: 4.2413 (4.0383)  loss_scale: 32768.0000 (58643.8376)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1145  max mem: 15572
Epoch: [12]  [ 280/2809]  eta: 0:24:55  lr: 0.000042  min_lr: 0.000000  loss: 4.2392 (4.0361)  loss_scale: 32768.0000 (57722.9893)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1232  max mem: 15572
Epoch: [12]  [ 290/2809]  eta: 0:24:48  lr: 0.000042  min_lr: 0.000000  loss: 4.0457 (4.0384)  loss_scale: 32768.0000 (56865.4296)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1261  max mem: 15572
[2025-01-13 02:17:20,133] [INFO] [logging.py:96:log_dist] [Rank 0] step=34000, skipped=224, lr=[4.095936585352162e-07, 4.095936585352162e-07, 5.851337979074517e-07, 5.851337979074517e-07, 8.359054255820739e-07, 8.359054255820739e-07, 1.1941506079743914e-06, 1.1941506079743914e-06, 1.7059294399634163e-06, 1.7059294399634163e-06, 2.437042057090595e-06, 2.437042057090595e-06, 3.4814886529865642e-06, 3.4814886529865642e-06, 4.973555218552235e-06, 4.973555218552235e-06, 7.10507888364605e-06, 7.10507888364605e-06, 1.0150112690922931e-05, 1.0150112690922931e-05, 1.4500160987032758e-05, 1.4500160987032758e-05, 2.0714515695761083e-05, 2.0714515695761083e-05, 2.9592165279658694e-05, 2.9592165279658694e-05, 4.227452182808385e-05, 4.227452182808385e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 02:17:20,135] [INFO] [timer.py:260:stop] epoch=0/micro_step=34000/global_step=34000, RunningAvgSamplesPerSec=28.005797816550864, CurrSamplesPerSec=27.47636662197513, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [ 300/2809]  eta: 0:24:41  lr: 0.000042  min_lr: 0.000000  loss: 4.0957 (4.0367)  loss_scale: 32768.0000 (56064.8505)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1298  max mem: 15572
Epoch: [12]  [ 310/2809]  eta: 0:24:41  lr: 0.000042  min_lr: 0.000000  loss: 4.0178 (4.0355)  loss_scale: 32768.0000 (55315.7556)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1705  max mem: 15572
Epoch: [12]  [ 320/2809]  eta: 0:24:37  lr: 0.000042  min_lr: 0.000000  loss: 3.8364 (4.0302)  loss_scale: 32768.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.1862  max mem: 15572
Epoch: [12]  [ 330/2809]  eta: 0:24:25  lr: 0.000042  min_lr: 0.000000  loss: 4.0458 (4.0354)  loss_scale: 32768.0000 (53953.3535)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1195  max mem: 15572
[2025-01-13 02:17:48,869] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:17:48,870] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 340/2809]  eta: 0:24:20  lr: 0.000042  min_lr: 0.000000  loss: 4.0987 (4.0373)  loss_scale: 32768.0000 (53524.2698)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1224  max mem: 15572
Epoch: [12]  [ 350/2809]  eta: 0:24:13  lr: 0.000042  min_lr: 0.000000  loss: 3.9198 (4.0342)  loss_scale: 65536.0000 (53866.4843)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1572  max mem: 15572
Epoch: [12]  [ 360/2809]  eta: 0:24:08  lr: 0.000042  min_lr: 0.000000  loss: 4.0155 (4.0378)  loss_scale: 65536.0000 (54189.7396)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1641  max mem: 15572
Epoch: [12]  [ 370/2809]  eta: 0:24:06  lr: 0.000042  min_lr: 0.000000  loss: 4.0986 (4.0364)  loss_scale: 65536.0000 (54495.5687)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1769  max mem: 15572
Epoch: [12]  [ 380/2809]  eta: 0:23:56  lr: 0.000042  min_lr: 0.000000  loss: 3.9199 (4.0330)  loss_scale: 65536.0000 (54785.3438)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1401  max mem: 15572
Epoch: [12]  [ 390/2809]  eta: 0:23:44  lr: 0.000042  min_lr: 0.000000  loss: 4.1068 (4.0360)  loss_scale: 65536.0000 (55060.2967)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0676  max mem: 15572
Epoch: [12]  [ 400/2809]  eta: 0:23:38  lr: 0.000042  min_lr: 0.000000  loss: 3.8752 (4.0304)  loss_scale: 65536.0000 (55321.5362)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0821  max mem: 15572
Epoch: [12]  [ 410/2809]  eta: 0:23:27  lr: 0.000042  min_lr: 0.000000  loss: 4.1036 (4.0344)  loss_scale: 65536.0000 (55570.0633)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0909  max mem: 15572
Epoch: [12]  [ 420/2809]  eta: 0:23:10  lr: 0.000042  min_lr: 0.000000  loss: 4.1374 (4.0369)  loss_scale: 65536.0000 (55806.7838)  weight_decay: 0.0500 (0.0500)  time: 0.4474  data: 0.0268  max mem: 15572
Epoch: [12]  [ 430/2809]  eta: 0:22:54  lr: 0.000042  min_lr: 0.000000  loss: 4.0676 (4.0380)  loss_scale: 65536.0000 (56032.5197)  weight_decay: 0.0500 (0.0500)  time: 0.3968  data: 0.0003  max mem: 15572
Epoch: [12]  [ 440/2809]  eta: 0:22:43  lr: 0.000042  min_lr: 0.000000  loss: 3.9499 (4.0345)  loss_scale: 65536.0000 (56248.0181)  weight_decay: 0.0500 (0.0500)  time: 0.4362  data: 0.0004  max mem: 15572
Epoch: [12]  [ 450/2809]  eta: 0:22:32  lr: 0.000042  min_lr: 0.000000  loss: 4.0841 (4.0395)  loss_scale: 65536.0000 (56453.9601)  weight_decay: 0.0500 (0.0500)  time: 0.4732  data: 0.0062  max mem: 15572
Epoch: [12]  [ 460/2809]  eta: 0:22:32  lr: 0.000042  min_lr: 0.000000  loss: 4.0893 (4.0355)  loss_scale: 65536.0000 (56650.9675)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1122  max mem: 15572
[2025-01-13 02:18:56,849] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:18:56,850] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [ 470/2809]  eta: 0:22:31  lr: 0.000042  min_lr: 0.000000  loss: 3.8192 (4.0295)  loss_scale: 65536.0000 (57396.1783)  weight_decay: 0.0500 (0.0500)  time: 0.6761  data: 0.2174  max mem: 15572
[2025-01-13 02:19:01,107] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34181
[2025-01-13 02:19:01,107] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:19:01,107] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 480/2809]  eta: 0:22:26  lr: 0.000042  min_lr: 0.000000  loss: 4.1046 (4.0340)  loss_scale: 65536.0000 (57837.9044)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.2022  max mem: 15572
Epoch: [12]  [ 490/2809]  eta: 0:22:26  lr: 0.000042  min_lr: 0.000000  loss: 4.0897 (4.0272)  loss_scale: 65536.0000 (57994.6884)  weight_decay: 0.0500 (0.0500)  time: 0.6528  data: 0.2091  max mem: 15572
Epoch: [12]  [ 500/2809]  eta: 0:22:25  lr: 0.000042  min_lr: 0.000000  loss: 3.8076 (4.0261)  loss_scale: 65536.0000 (58145.2136)  weight_decay: 0.0500 (0.0500)  time: 0.6927  data: 0.2499  max mem: 15572
Epoch: [12]  [ 510/2809]  eta: 0:22:22  lr: 0.000042  min_lr: 0.000000  loss: 3.9429 (4.0220)  loss_scale: 65536.0000 (58289.8474)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.2225  max mem: 15572
Epoch: [12]  [ 520/2809]  eta: 0:22:17  lr: 0.000042  min_lr: 0.000000  loss: 3.8441 (4.0177)  loss_scale: 65536.0000 (58428.9290)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1741  max mem: 15572
Epoch: [12]  [ 530/2809]  eta: 0:22:15  lr: 0.000042  min_lr: 0.000000  loss: 3.9911 (4.0197)  loss_scale: 65536.0000 (58562.7721)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.1691  max mem: 15572
Epoch: [12]  [ 540/2809]  eta: 0:22:16  lr: 0.000042  min_lr: 0.000000  loss: 4.0710 (4.0196)  loss_scale: 65536.0000 (58691.6673)  weight_decay: 0.0500 (0.0500)  time: 0.7155  data: 0.2501  max mem: 15572
Epoch: [12]  [ 550/2809]  eta: 0:22:09  lr: 0.000042  min_lr: 0.000000  loss: 3.9967 (4.0170)  loss_scale: 65536.0000 (58815.8838)  weight_decay: 0.0500 (0.0500)  time: 0.6636  data: 0.2144  max mem: 15572
Epoch: [12]  [ 560/2809]  eta: 0:22:07  lr: 0.000042  min_lr: 0.000000  loss: 3.9640 (4.0191)  loss_scale: 65536.0000 (58935.6720)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1426  max mem: 15572
Epoch: [12]  [ 570/2809]  eta: 0:21:55  lr: 0.000042  min_lr: 0.000000  loss: 4.0618 (4.0184)  loss_scale: 65536.0000 (59051.2644)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1112  max mem: 15572
Epoch: [12]  [ 580/2809]  eta: 0:21:42  lr: 0.000042  min_lr: 0.000000  loss: 4.0005 (4.0197)  loss_scale: 65536.0000 (59162.8778)  weight_decay: 0.0500 (0.0500)  time: 0.4240  data: 0.0187  max mem: 15572
Epoch: [12]  [ 590/2809]  eta: 0:21:37  lr: 0.000042  min_lr: 0.000000  loss: 4.0541 (4.0178)  loss_scale: 65536.0000 (59270.7140)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0727  max mem: 15572
Epoch: [12]  [ 600/2809]  eta: 0:21:31  lr: 0.000042  min_lr: 0.000000  loss: 4.0925 (4.0215)  loss_scale: 65536.0000 (59374.9617)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1379  max mem: 15572
[2025-01-13 02:20:20,189] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:20:20,189] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:20:24,051] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34317
[2025-01-13 02:20:24,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:20:24,051] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 610/2809]  eta: 0:21:23  lr: 0.000042  min_lr: 0.000000  loss: 4.0925 (4.0210)  loss_scale: 65536.0000 (60226.6187)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0975  max mem: 15572
Epoch: [12]  [ 620/2809]  eta: 0:21:15  lr: 0.000042  min_lr: 0.000000  loss: 4.2003 (4.0240)  loss_scale: 65536.0000 (60312.1159)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0649  max mem: 15572
Epoch: [12]  [ 630/2809]  eta: 0:21:06  lr: 0.000042  min_lr: 0.000000  loss: 4.2095 (4.0240)  loss_scale: 65536.0000 (60394.9033)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0665  max mem: 15572
Epoch: [12]  [ 640/2809]  eta: 0:21:02  lr: 0.000042  min_lr: 0.000000  loss: 4.0405 (4.0223)  loss_scale: 65536.0000 (60475.1076)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1251  max mem: 15572
Epoch: [12]  [ 650/2809]  eta: 0:20:54  lr: 0.000042  min_lr: 0.000000  loss: 4.0405 (4.0253)  loss_scale: 65536.0000 (60552.8479)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1280  max mem: 15572
Epoch: [12]  [ 660/2809]  eta: 0:20:48  lr: 0.000042  min_lr: 0.000000  loss: 4.1221 (4.0277)  loss_scale: 65536.0000 (60628.2360)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.1046  max mem: 15572
Epoch: [12]  [ 670/2809]  eta: 0:20:41  lr: 0.000042  min_lr: 0.000000  loss: 4.1922 (4.0285)  loss_scale: 65536.0000 (60701.3770)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1267  max mem: 15572
Epoch: [12]  [ 680/2809]  eta: 0:20:33  lr: 0.000042  min_lr: 0.000000  loss: 4.0206 (4.0291)  loss_scale: 65536.0000 (60772.3700)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0919  max mem: 15572
Epoch: [12]  [ 690/2809]  eta: 0:20:26  lr: 0.000042  min_lr: 0.000000  loss: 4.0186 (4.0263)  loss_scale: 65536.0000 (60841.3082)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0930  max mem: 15572
Epoch: [12]  [ 700/2809]  eta: 0:20:19  lr: 0.000042  min_lr: 0.000000  loss: 4.0398 (4.0274)  loss_scale: 65536.0000 (60908.2796)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1116  max mem: 15572
Epoch: [12]  [ 710/2809]  eta: 0:20:12  lr: 0.000042  min_lr: 0.000000  loss: 4.0340 (4.0248)  loss_scale: 65536.0000 (60973.3671)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.1005  max mem: 15572
Epoch: [12]  [ 720/2809]  eta: 0:20:04  lr: 0.000042  min_lr: 0.000000  loss: 4.0340 (4.0244)  loss_scale: 65536.0000 (61036.6491)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0706  max mem: 15572
Epoch: [12]  [ 730/2809]  eta: 0:20:01  lr: 0.000042  min_lr: 0.000000  loss: 4.0786 (4.0267)  loss_scale: 65536.0000 (61098.1997)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1156  max mem: 15572
[2025-01-13 02:21:33,581] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:21:33,582] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [ 740/2809]  eta: 0:19:51  lr: 0.000042  min_lr: 0.000000  loss: 4.1741 (4.0276)  loss_scale: 65536.0000 (61423.4170)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1113  max mem: 15572
[2025-01-13 02:21:36,295] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34449
[2025-01-13 02:21:36,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:21:36,295] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 750/2809]  eta: 0:19:46  lr: 0.000042  min_lr: 0.000000  loss: 4.0048 (4.0278)  loss_scale: 65536.0000 (61478.1784)  weight_decay: 0.0500 (0.0500)  time: 0.5178  data: 0.0911  max mem: 15572
Epoch: [12]  [ 760/2809]  eta: 0:19:39  lr: 0.000042  min_lr: 0.000000  loss: 3.9511 (4.0252)  loss_scale: 65536.0000 (61531.5007)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1359  max mem: 15572
Epoch: [12]  [ 770/2809]  eta: 0:19:37  lr: 0.000042  min_lr: 0.000000  loss: 3.8482 (4.0231)  loss_scale: 65536.0000 (61583.4397)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1842  max mem: 15572
Epoch: [12]  [ 780/2809]  eta: 0:19:30  lr: 0.000042  min_lr: 0.000000  loss: 4.0279 (4.0252)  loss_scale: 65536.0000 (61634.0487)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.1819  max mem: 15572
Epoch: [12]  [ 790/2809]  eta: 0:19:23  lr: 0.000042  min_lr: 0.000000  loss: 4.2074 (4.0271)  loss_scale: 65536.0000 (61683.3780)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.1037  max mem: 15572
Epoch: [12]  [ 800/2809]  eta: 0:19:16  lr: 0.000042  min_lr: 0.000000  loss: 4.0520 (4.0256)  loss_scale: 65536.0000 (61731.4757)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0911  max mem: 15572
Epoch: [12]  [ 810/2809]  eta: 0:19:10  lr: 0.000042  min_lr: 0.000000  loss: 4.0520 (4.0276)  loss_scale: 65536.0000 (61778.3872)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.1016  max mem: 15572
Epoch: [12]  [ 820/2809]  eta: 0:19:04  lr: 0.000042  min_lr: 0.000000  loss: 3.9960 (4.0246)  loss_scale: 65536.0000 (61824.1559)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1122  max mem: 15572
Epoch: [12]  [ 830/2809]  eta: 0:18:59  lr: 0.000042  min_lr: 0.000000  loss: 3.8984 (4.0265)  loss_scale: 65536.0000 (61868.8231)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1569  max mem: 15572
[2025-01-13 02:22:29,877] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34543
[2025-01-13 02:22:29,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 02:22:29,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 840/2809]  eta: 0:18:54  lr: 0.000042  min_lr: 0.000000  loss: 4.0517 (4.0249)  loss_scale: 65536.0000 (61678.6492)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1643  max mem: 15572
Epoch: [12]  [ 850/2809]  eta: 0:18:46  lr: 0.000042  min_lr: 0.000000  loss: 4.0517 (4.0267)  loss_scale: 32768.0000 (61338.9236)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1036  max mem: 15572
Epoch: [12]  [ 860/2809]  eta: 0:18:42  lr: 0.000042  min_lr: 0.000000  loss: 3.9464 (4.0252)  loss_scale: 32768.0000 (61007.0894)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1460  max mem: 15572
Epoch: [12]  [ 870/2809]  eta: 0:18:37  lr: 0.000042  min_lr: 0.000000  loss: 3.8854 (4.0260)  loss_scale: 32768.0000 (60682.8749)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1736  max mem: 15572
Epoch: [12]  [ 880/2809]  eta: 0:18:33  lr: 0.000042  min_lr: 0.000000  loss: 4.0384 (4.0252)  loss_scale: 32768.0000 (60366.0204)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1763  max mem: 15572
Epoch: [12]  [ 890/2809]  eta: 0:18:25  lr: 0.000042  min_lr: 0.000000  loss: 4.1374 (4.0243)  loss_scale: 32768.0000 (60056.2783)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1307  max mem: 15572
Epoch: [12]  [ 900/2809]  eta: 0:18:16  lr: 0.000042  min_lr: 0.000000  loss: 4.0711 (4.0226)  loss_scale: 32768.0000 (59753.4118)  weight_decay: 0.0500 (0.0500)  time: 0.4455  data: 0.0203  max mem: 15572
Epoch: [12]  [ 910/2809]  eta: 0:18:06  lr: 0.000042  min_lr: 0.000000  loss: 3.9558 (4.0208)  loss_scale: 32768.0000 (59457.1943)  weight_decay: 0.0500 (0.0500)  time: 0.3908  data: 0.0004  max mem: 15572
Epoch: [12]  [ 920/2809]  eta: 0:17:58  lr: 0.000042  min_lr: 0.000000  loss: 3.9443 (4.0192)  loss_scale: 32768.0000 (59167.4093)  weight_decay: 0.0500 (0.0500)  time: 0.4284  data: 0.0005  max mem: 15572
Epoch: [12]  [ 930/2809]  eta: 0:17:50  lr: 0.000042  min_lr: 0.000000  loss: 4.0254 (4.0195)  loss_scale: 32768.0000 (58883.8496)  weight_decay: 0.0500 (0.0500)  time: 0.4653  data: 0.0008  max mem: 15572
Epoch: [12]  [ 940/2809]  eta: 0:17:44  lr: 0.000042  min_lr: 0.000000  loss: 3.9676 (4.0183)  loss_scale: 32768.0000 (58606.3167)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0486  max mem: 15572
Epoch: [12]  [ 950/2809]  eta: 0:17:41  lr: 0.000042  min_lr: 0.000000  loss: 3.7576 (4.0158)  loss_scale: 32768.0000 (58334.6204)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1617  max mem: 15572
Epoch: [12]  [ 960/2809]  eta: 0:17:37  lr: 0.000042  min_lr: 0.000000  loss: 4.0486 (4.0178)  loss_scale: 32768.0000 (58068.5786)  weight_decay: 0.0500 (0.0500)  time: 0.6756  data: 0.2237  max mem: 15572
[2025-01-13 02:23:40,751] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:23:40,751] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 970/2809]  eta: 0:17:32  lr: 0.000042  min_lr: 0.000000  loss: 4.0486 (4.0163)  loss_scale: 32768.0000 (58044.2430)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.2056  max mem: 15572
Epoch: [12]  [ 980/2809]  eta: 0:17:30  lr: 0.000042  min_lr: 0.000000  loss: 3.9389 (4.0172)  loss_scale: 65536.0000 (58120.6116)  weight_decay: 0.0500 (0.0500)  time: 0.7041  data: 0.2622  max mem: 15572
Epoch: [12]  [ 990/2809]  eta: 0:17:27  lr: 0.000042  min_lr: 0.000000  loss: 4.1344 (4.0186)  loss_scale: 65536.0000 (58195.4390)  weight_decay: 0.0500 (0.0500)  time: 0.7558  data: 0.2978  max mem: 15572
Epoch: [12]  [1000/2809]  eta: 0:17:23  lr: 0.000042  min_lr: 0.000000  loss: 4.1760 (4.0218)  loss_scale: 65536.0000 (58268.7712)  weight_decay: 0.0500 (0.0500)  time: 0.6907  data: 0.2121  max mem: 15572
Epoch: [12]  [1010/2809]  eta: 0:17:19  lr: 0.000042  min_lr: 0.000000  loss: 4.1755 (4.0234)  loss_scale: 65536.0000 (58340.6528)  weight_decay: 0.0500 (0.0500)  time: 0.6778  data: 0.2108  max mem: 15572
Epoch: [12]  [1020/2809]  eta: 0:17:15  lr: 0.000042  min_lr: 0.000000  loss: 4.1139 (4.0235)  loss_scale: 65536.0000 (58411.1263)  weight_decay: 0.0500 (0.0500)  time: 0.6850  data: 0.2485  max mem: 15572
Epoch: [12]  [1030/2809]  eta: 0:17:10  lr: 0.000042  min_lr: 0.000000  loss: 4.0241 (4.0223)  loss_scale: 65536.0000 (58480.2328)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1960  max mem: 15572
Epoch: [12]  [1040/2809]  eta: 0:17:05  lr: 0.000042  min_lr: 0.000000  loss: 4.0420 (4.0235)  loss_scale: 65536.0000 (58548.0115)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1799  max mem: 15572
Epoch: [12]  [1050/2809]  eta: 0:16:58  lr: 0.000042  min_lr: 0.000000  loss: 4.1325 (4.0242)  loss_scale: 65536.0000 (58614.5005)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1372  max mem: 15572
Epoch: [12]  [1060/2809]  eta: 0:16:49  lr: 0.000042  min_lr: 0.000000  loss: 4.0638 (4.0246)  loss_scale: 65536.0000 (58679.7361)  weight_decay: 0.0500 (0.0500)  time: 0.4506  data: 0.0348  max mem: 15572
Epoch: [12]  [1070/2809]  eta: 0:16:44  lr: 0.000042  min_lr: 0.000000  loss: 4.0638 (4.0256)  loss_scale: 65536.0000 (58743.7535)  weight_decay: 0.0500 (0.0500)  time: 0.5002  data: 0.0895  max mem: 15572
Epoch: [12]  [1080/2809]  eta: 0:16:37  lr: 0.000042  min_lr: 0.000000  loss: 4.2553 (4.0260)  loss_scale: 65536.0000 (58806.5865)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1247  max mem: 15572
Epoch: [12]  [1090/2809]  eta: 0:16:31  lr: 0.000042  min_lr: 0.000000  loss: 4.0049 (4.0250)  loss_scale: 65536.0000 (58868.2676)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.1031  max mem: 15572
[2025-01-13 02:24:59,692] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:24:59,692] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:25:02,334] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34806
[2025-01-13 02:25:02,335] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:25:02,335] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1100/2809]  eta: 0:16:26  lr: 0.000042  min_lr: 0.000000  loss: 3.9075 (4.0244)  loss_scale: 65536.0000 (59285.9728)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1556  max mem: 15572
Epoch: [12]  [1110/2809]  eta: 0:16:20  lr: 0.000042  min_lr: 0.000000  loss: 3.9984 (4.0250)  loss_scale: 65536.0000 (59342.2286)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1721  max mem: 15572
Epoch: [12]  [1120/2809]  eta: 0:16:14  lr: 0.000042  min_lr: 0.000000  loss: 4.0796 (4.0242)  loss_scale: 65536.0000 (59397.4808)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1586  max mem: 15572
Epoch: [12]  [1130/2809]  eta: 0:16:09  lr: 0.000042  min_lr: 0.000000  loss: 4.0796 (4.0248)  loss_scale: 65536.0000 (59451.7560)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1506  max mem: 15572
Epoch: [12]  [1140/2809]  eta: 0:16:01  lr: 0.000042  min_lr: 0.000000  loss: 4.1154 (4.0245)  loss_scale: 65536.0000 (59505.0798)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0942  max mem: 15572
Epoch: [12]  [1150/2809]  eta: 0:15:56  lr: 0.000042  min_lr: 0.000000  loss: 4.0272 (4.0253)  loss_scale: 65536.0000 (59557.4770)  weight_decay: 0.0500 (0.0500)  time: 0.5174  data: 0.0947  max mem: 15572
Epoch: [12]  [1160/2809]  eta: 0:15:51  lr: 0.000042  min_lr: 0.000000  loss: 4.1104 (4.0260)  loss_scale: 65536.0000 (59608.9716)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1698  max mem: 15572
Epoch: [12]  [1170/2809]  eta: 0:15:46  lr: 0.000042  min_lr: 0.000000  loss: 4.1268 (4.0270)  loss_scale: 65536.0000 (59659.5867)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1766  max mem: 15572
Epoch: [12]  [1180/2809]  eta: 0:15:39  lr: 0.000042  min_lr: 0.000000  loss: 4.0935 (4.0257)  loss_scale: 65536.0000 (59709.3446)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1251  max mem: 15572
Epoch: [12]  [1190/2809]  eta: 0:15:32  lr: 0.000042  min_lr: 0.000000  loss: 4.1199 (4.0268)  loss_scale: 65536.0000 (59758.2670)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0805  max mem: 15572
Epoch: [12]  [1200/2809]  eta: 0:15:26  lr: 0.000042  min_lr: 0.000000  loss: 4.0240 (4.0261)  loss_scale: 65536.0000 (59806.3747)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0822  max mem: 15572
Epoch: [12]  [1210/2809]  eta: 0:15:20  lr: 0.000042  min_lr: 0.000000  loss: 3.8911 (4.0263)  loss_scale: 65536.0000 (59853.6879)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0918  max mem: 15572
Epoch: [12]  [1220/2809]  eta: 0:15:14  lr: 0.000042  min_lr: 0.000000  loss: 3.9763 (4.0258)  loss_scale: 65536.0000 (59900.2260)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1234  max mem: 15572
[2025-01-13 02:26:15,108] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:26:15,108] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [1230/2809]  eta: 0:15:08  lr: 0.000042  min_lr: 0.000000  loss: 4.0507 (4.0261)  loss_scale: 65536.0000 (60158.9602)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1479  max mem: 15572
[2025-01-13 02:26:19,808] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34941
[2025-01-13 02:26:19,809] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:26:19,809] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1240/2809]  eta: 0:15:03  lr: 0.000042  min_lr: 0.000000  loss: 4.0738 (4.0276)  loss_scale: 65536.0000 (60307.9065)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1758  max mem: 15572
Epoch: [12]  [1250/2809]  eta: 0:14:57  lr: 0.000042  min_lr: 0.000000  loss: 4.2095 (4.0294)  loss_scale: 65536.0000 (60349.6978)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1529  max mem: 15572
Epoch: [12]  [1260/2809]  eta: 0:14:51  lr: 0.000042  min_lr: 0.000000  loss: 3.9923 (4.0288)  loss_scale: 65536.0000 (60390.8263)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.1185  max mem: 15572
Epoch: [12]  [1270/2809]  eta: 0:14:45  lr: 0.000042  min_lr: 0.000000  loss: 3.9570 (4.0273)  loss_scale: 65536.0000 (60431.3076)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1601  max mem: 15572
Epoch: [12]  [1280/2809]  eta: 0:14:39  lr: 0.000042  min_lr: 0.000000  loss: 4.0798 (4.0292)  loss_scale: 65536.0000 (60471.1569)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1503  max mem: 15572
Epoch: [12]  [1290/2809]  eta: 0:14:33  lr: 0.000042  min_lr: 0.000000  loss: 4.3075 (4.0310)  loss_scale: 65536.0000 (60510.3888)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.1179  max mem: 15572
[2025-01-13 02:26:52,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=35000, skipped=230, lr=[4.051861021080781e-07, 4.051861021080781e-07, 5.788372887258259e-07, 5.788372887258259e-07, 8.269104124654657e-07, 8.269104124654657e-07, 1.1813005892363797e-06, 1.1813005892363797e-06, 1.6875722703376853e-06, 1.6875722703376853e-06, 2.4108175290538364e-06, 2.4108175290538364e-06, 3.4440250415054805e-06, 3.4440250415054805e-06, 4.920035773579258e-06, 4.920035773579258e-06, 7.028622533684655e-06, 7.028622533684655e-06, 1.0040889333835223e-05, 1.0040889333835223e-05, 1.4344127619764603e-05, 1.4344127619764603e-05, 2.0491610885378006e-05, 2.0491610885378006e-05, 2.9273729836254297e-05, 2.9273729836254297e-05, 4.181961405179186e-05, 4.181961405179186e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 02:26:52,271] [INFO] [timer.py:260:stop] epoch=0/micro_step=35000/global_step=35000, RunningAvgSamplesPerSec=28.029606319182463, CurrSamplesPerSec=31.695730945872164, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [1300/2809]  eta: 0:14:27  lr: 0.000042  min_lr: 0.000000  loss: 4.0960 (4.0296)  loss_scale: 65536.0000 (60549.0177)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1364  max mem: 15572
Epoch: [12]  [1310/2809]  eta: 0:14:22  lr: 0.000042  min_lr: 0.000000  loss: 3.7449 (4.0284)  loss_scale: 65536.0000 (60587.0572)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1505  max mem: 15572
Epoch: [12]  [1320/2809]  eta: 0:14:15  lr: 0.000042  min_lr: 0.000000  loss: 4.0661 (4.0291)  loss_scale: 65536.0000 (60624.5208)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1120  max mem: 15572
Epoch: [12]  [1330/2809]  eta: 0:14:09  lr: 0.000042  min_lr: 0.000000  loss: 4.2016 (4.0296)  loss_scale: 65536.0000 (60661.4215)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.1118  max mem: 15572
Epoch: [12]  [1340/2809]  eta: 0:14:02  lr: 0.000042  min_lr: 0.000000  loss: 3.9257 (4.0279)  loss_scale: 65536.0000 (60697.7718)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0973  max mem: 15572
Epoch: [12]  [1350/2809]  eta: 0:13:54  lr: 0.000042  min_lr: 0.000000  loss: 3.9546 (4.0281)  loss_scale: 65536.0000 (60733.5840)  weight_decay: 0.0500 (0.0500)  time: 0.4187  data: 0.0243  max mem: 15572
Epoch: [12]  [1360/2809]  eta: 0:13:48  lr: 0.000042  min_lr: 0.000000  loss: 4.0550 (4.0280)  loss_scale: 65536.0000 (60768.8699)  weight_decay: 0.0500 (0.0500)  time: 0.4260  data: 0.0007  max mem: 15572
[2025-01-13 02:27:26,703] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:27:26,703] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:27:28,530] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35074
[2025-01-13 02:27:28,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:27:28,531] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1370/2809]  eta: 0:13:41  lr: 0.000042  min_lr: 0.000000  loss: 4.0033 (4.0274)  loss_scale: 65536.0000 (60994.8476)  weight_decay: 0.0500 (0.0500)  time: 0.4614  data: 0.0008  max mem: 15572
Epoch: [12]  [1380/2809]  eta: 0:13:34  lr: 0.000042  min_lr: 0.000000  loss: 4.0697 (4.0284)  loss_scale: 65536.0000 (61027.7306)  weight_decay: 0.0500 (0.0500)  time: 0.4763  data: 0.0241  max mem: 15572
Epoch: [12]  [1390/2809]  eta: 0:13:30  lr: 0.000042  min_lr: 0.000000  loss: 4.0697 (4.0286)  loss_scale: 65536.0000 (61060.1409)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1444  max mem: 15572
Epoch: [12]  [1400/2809]  eta: 0:13:26  lr: 0.000042  min_lr: 0.000000  loss: 3.9944 (4.0284)  loss_scale: 65536.0000 (61092.0885)  weight_decay: 0.0500 (0.0500)  time: 0.7059  data: 0.2357  max mem: 15572
Epoch: [12]  [1410/2809]  eta: 0:13:20  lr: 0.000042  min_lr: 0.000000  loss: 4.1480 (4.0293)  loss_scale: 65536.0000 (61123.5833)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.1902  max mem: 15572
Epoch: [12]  [1420/2809]  eta: 0:13:16  lr: 0.000042  min_lr: 0.000000  loss: 4.1694 (4.0297)  loss_scale: 65536.0000 (61154.6348)  weight_decay: 0.0500 (0.0500)  time: 0.6832  data: 0.2320  max mem: 15572
Epoch: [12]  [1430/2809]  eta: 0:13:11  lr: 0.000042  min_lr: 0.000000  loss: 4.2742 (4.0311)  loss_scale: 65536.0000 (61185.2523)  weight_decay: 0.0500 (0.0500)  time: 0.7016  data: 0.2431  max mem: 15572
Epoch: [12]  [1440/2809]  eta: 0:13:07  lr: 0.000042  min_lr: 0.000000  loss: 4.3064 (4.0320)  loss_scale: 65536.0000 (61215.4448)  weight_decay: 0.0500 (0.0500)  time: 0.6658  data: 0.2117  max mem: 15572
Epoch: [12]  [1450/2809]  eta: 0:13:01  lr: 0.000042  min_lr: 0.000000  loss: 4.2137 (4.0317)  loss_scale: 65536.0000 (61245.2212)  weight_decay: 0.0500 (0.0500)  time: 0.6704  data: 0.2092  max mem: 15572
Epoch: [12]  [1460/2809]  eta: 0:12:57  lr: 0.000042  min_lr: 0.000000  loss: 4.1176 (4.0327)  loss_scale: 65536.0000 (61274.5900)  weight_decay: 0.0500 (0.0500)  time: 0.6647  data: 0.2114  max mem: 15572
Epoch: [12]  [1470/2809]  eta: 0:12:51  lr: 0.000042  min_lr: 0.000000  loss: 4.0710 (4.0327)  loss_scale: 65536.0000 (61303.5595)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.2236  max mem: 15572
Epoch: [12]  [1480/2809]  eta: 0:12:47  lr: 0.000042  min_lr: 0.000000  loss: 4.1960 (4.0343)  loss_scale: 65536.0000 (61332.1377)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.2111  max mem: 15572
Epoch: [12]  [1490/2809]  eta: 0:12:40  lr: 0.000042  min_lr: 0.000000  loss: 4.1748 (4.0356)  loss_scale: 65536.0000 (61360.3327)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1734  max mem: 15572
[2025-01-13 02:28:50,106] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:28:50,107] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:28:51,806] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35207
[2025-01-13 02:28:51,806] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:28:51,807] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1500/2809]  eta: 0:12:33  lr: 0.000042  min_lr: 0.000000  loss: 4.1748 (4.0363)  loss_scale: 65536.0000 (61562.7981)  weight_decay: 0.0500 (0.0500)  time: 0.4722  data: 0.0588  max mem: 15572
Epoch: [12]  [1510/2809]  eta: 0:12:26  lr: 0.000042  min_lr: 0.000000  loss: 4.1643 (4.0375)  loss_scale: 65536.0000 (61589.0933)  weight_decay: 0.0500 (0.0500)  time: 0.4242  data: 0.0008  max mem: 15572
Epoch: [12]  [1520/2809]  eta: 0:12:20  lr: 0.000042  min_lr: 0.000000  loss: 4.1145 (4.0372)  loss_scale: 65536.0000 (61615.0427)  weight_decay: 0.0500 (0.0500)  time: 0.4943  data: 0.0597  max mem: 15572
Epoch: [12]  [1530/2809]  eta: 0:12:15  lr: 0.000042  min_lr: 0.000000  loss: 4.0003 (4.0363)  loss_scale: 65536.0000 (61640.6532)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1369  max mem: 15572
Epoch: [12]  [1540/2809]  eta: 0:12:10  lr: 0.000042  min_lr: 0.000000  loss: 4.0982 (4.0377)  loss_scale: 65536.0000 (61665.9312)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1732  max mem: 15572
Epoch: [12]  [1550/2809]  eta: 0:12:04  lr: 0.000042  min_lr: 0.000000  loss: 4.2027 (4.0381)  loss_scale: 65536.0000 (61690.8833)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1405  max mem: 15572
Epoch: [12]  [1560/2809]  eta: 0:11:58  lr: 0.000042  min_lr: 0.000000  loss: 4.1103 (4.0387)  loss_scale: 65536.0000 (61715.5157)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1376  max mem: 15572
Epoch: [12]  [1570/2809]  eta: 0:11:53  lr: 0.000042  min_lr: 0.000000  loss: 3.9585 (4.0374)  loss_scale: 65536.0000 (61739.8345)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1609  max mem: 15572
Epoch: [12]  [1580/2809]  eta: 0:11:47  lr: 0.000042  min_lr: 0.000000  loss: 3.9268 (4.0374)  loss_scale: 65536.0000 (61763.8457)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1487  max mem: 15572
Epoch: [12]  [1590/2809]  eta: 0:11:41  lr: 0.000042  min_lr: 0.000000  loss: 3.9297 (4.0374)  loss_scale: 65536.0000 (61787.5550)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1229  max mem: 15572
Epoch: [12]  [1600/2809]  eta: 0:11:34  lr: 0.000042  min_lr: 0.000000  loss: 3.9297 (4.0378)  loss_scale: 65536.0000 (61810.9681)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0557  max mem: 15572
Epoch: [12]  [1610/2809]  eta: 0:11:28  lr: 0.000042  min_lr: 0.000000  loss: 3.8962 (4.0368)  loss_scale: 65536.0000 (61834.0906)  weight_decay: 0.0500 (0.0500)  time: 0.4733  data: 0.0530  max mem: 15572
Epoch: [12]  [1620/2809]  eta: 0:11:22  lr: 0.000042  min_lr: 0.000000  loss: 4.0893 (4.0376)  loss_scale: 65536.0000 (61856.9278)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0975  max mem: 15572
[2025-01-13 02:30:04,619] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:30:04,619] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:30:05,615] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35338
[2025-01-13 02:30:05,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:30:05,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1630/2809]  eta: 0:11:17  lr: 0.000042  min_lr: 0.000000  loss: 4.1382 (4.0367)  loss_scale: 65536.0000 (61959.8479)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1833  max mem: 15572
Epoch: [12]  [1640/2809]  eta: 0:11:11  lr: 0.000042  min_lr: 0.000000  loss: 3.9403 (4.0360)  loss_scale: 65536.0000 (61981.6405)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1900  max mem: 15572
Epoch: [12]  [1650/2809]  eta: 0:11:05  lr: 0.000042  min_lr: 0.000000  loss: 3.9403 (4.0353)  loss_scale: 65536.0000 (62003.1690)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0883  max mem: 15572
Epoch: [12]  [1660/2809]  eta: 0:10:59  lr: 0.000042  min_lr: 0.000000  loss: 3.9016 (4.0353)  loss_scale: 65536.0000 (62024.4383)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.1079  max mem: 15572
Epoch: [12]  [1670/2809]  eta: 0:10:53  lr: 0.000042  min_lr: 0.000000  loss: 3.9016 (4.0347)  loss_scale: 65536.0000 (62045.4530)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.1110  max mem: 15572
Epoch: [12]  [1680/2809]  eta: 0:10:48  lr: 0.000042  min_lr: 0.000000  loss: 4.0239 (4.0346)  loss_scale: 65536.0000 (62066.2177)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1343  max mem: 15572
Epoch: [12]  [1690/2809]  eta: 0:10:42  lr: 0.000042  min_lr: 0.000000  loss: 3.7439 (4.0330)  loss_scale: 65536.0000 (62086.7368)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1877  max mem: 15572
Epoch: [12]  [1700/2809]  eta: 0:10:36  lr: 0.000042  min_lr: 0.000000  loss: 3.7227 (4.0319)  loss_scale: 65536.0000 (62107.0147)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.1066  max mem: 15572
Epoch: [12]  [1710/2809]  eta: 0:10:30  lr: 0.000042  min_lr: 0.000000  loss: 4.0911 (4.0323)  loss_scale: 65536.0000 (62127.0555)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0944  max mem: 15572
Epoch: [12]  [1720/2809]  eta: 0:10:24  lr: 0.000042  min_lr: 0.000000  loss: 4.0953 (4.0318)  loss_scale: 65536.0000 (62146.8635)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1339  max mem: 15572
Epoch: [12]  [1730/2809]  eta: 0:10:18  lr: 0.000042  min_lr: 0.000000  loss: 4.1895 (4.0323)  loss_scale: 65536.0000 (62166.4425)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.1228  max mem: 15572
Epoch: [12]  [1740/2809]  eta: 0:10:12  lr: 0.000042  min_lr: 0.000000  loss: 4.2122 (4.0318)  loss_scale: 65536.0000 (62185.7967)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1293  max mem: 15572
Epoch: [12]  [1750/2809]  eta: 0:10:07  lr: 0.000042  min_lr: 0.000000  loss: 4.0415 (4.0312)  loss_scale: 65536.0000 (62204.9298)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.2012  max mem: 15572
[2025-01-13 02:31:16,679] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:31:16,679] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [1760/2809]  eta: 0:10:00  lr: 0.000042  min_lr: 0.000000  loss: 3.9924 (4.0305)  loss_scale: 65536.0000 (62298.2760)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.1363  max mem: 15572
[2025-01-13 02:31:19,527] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35474
[2025-01-13 02:31:19,527] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:31:19,527] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1770/2809]  eta: 0:09:54  lr: 0.000042  min_lr: 0.000000  loss: 3.9781 (4.0309)  loss_scale: 65536.0000 (62501.5833)  weight_decay: 0.0500 (0.0500)  time: 0.4187  data: 0.0077  max mem: 15572
Epoch: [12]  [1780/2809]  eta: 0:09:48  lr: 0.000042  min_lr: 0.000000  loss: 4.1272 (4.0325)  loss_scale: 65536.0000 (62518.6210)  weight_decay: 0.0500 (0.0500)  time: 0.4540  data: 0.0007  max mem: 15572
Epoch: [12]  [1790/2809]  eta: 0:09:42  lr: 0.000042  min_lr: 0.000000  loss: 4.2366 (4.0323)  loss_scale: 65536.0000 (62535.4685)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0789  max mem: 15572
Epoch: [12]  [1800/2809]  eta: 0:09:37  lr: 0.000042  min_lr: 0.000000  loss: 4.0112 (4.0324)  loss_scale: 65536.0000 (62552.1288)  weight_decay: 0.0500 (0.0500)  time: 0.6833  data: 0.2249  max mem: 15572
Epoch: [12]  [1810/2809]  eta: 0:09:33  lr: 0.000042  min_lr: 0.000000  loss: 4.0359 (4.0330)  loss_scale: 65536.0000 (62568.6052)  weight_decay: 0.0500 (0.0500)  time: 0.7553  data: 0.2868  max mem: 15572
Epoch: [12]  [1820/2809]  eta: 0:09:28  lr: 0.000042  min_lr: 0.000000  loss: 4.0158 (4.0327)  loss_scale: 65536.0000 (62584.9006)  weight_decay: 0.0500 (0.0500)  time: 0.7440  data: 0.2851  max mem: 15572
Epoch: [12]  [1830/2809]  eta: 0:09:23  lr: 0.000042  min_lr: 0.000000  loss: 3.9612 (4.0320)  loss_scale: 65536.0000 (62601.0180)  weight_decay: 0.0500 (0.0500)  time: 0.7312  data: 0.2734  max mem: 15572
Epoch: [12]  [1840/2809]  eta: 0:09:18  lr: 0.000042  min_lr: 0.000000  loss: 3.9926 (4.0322)  loss_scale: 65536.0000 (62616.9603)  weight_decay: 0.0500 (0.0500)  time: 0.7499  data: 0.2687  max mem: 15572
Epoch: [12]  [1850/2809]  eta: 0:09:13  lr: 0.000042  min_lr: 0.000000  loss: 4.0491 (4.0317)  loss_scale: 65536.0000 (62632.7304)  weight_decay: 0.0500 (0.0500)  time: 0.7451  data: 0.2601  max mem: 15572
Epoch: [12]  [1860/2809]  eta: 0:09:07  lr: 0.000042  min_lr: 0.000000  loss: 3.9299 (4.0317)  loss_scale: 65536.0000 (62648.3310)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1331  max mem: 15572
Epoch: [12]  [1870/2809]  eta: 0:09:02  lr: 0.000042  min_lr: 0.000000  loss: 4.0680 (4.0328)  loss_scale: 65536.0000 (62663.7648)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1365  max mem: 15572
Epoch: [12]  [1880/2809]  eta: 0:08:57  lr: 0.000042  min_lr: 0.000000  loss: 4.1017 (4.0324)  loss_scale: 65536.0000 (62679.0346)  weight_decay: 0.0500 (0.0500)  time: 0.7053  data: 0.2672  max mem: 15572
Epoch: [12]  [1890/2809]  eta: 0:08:51  lr: 0.000042  min_lr: 0.000000  loss: 4.1003 (4.0331)  loss_scale: 65536.0000 (62694.1428)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.2021  max mem: 15572
[2025-01-13 02:32:43,610] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:32:43,610] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:32:44,377] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35605
[2025-01-13 02:32:44,378] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:32:44,378] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1900/2809]  eta: 0:08:44  lr: 0.000042  min_lr: 0.000000  loss: 4.2866 (4.0347)  loss_scale: 65536.0000 (62778.0410)  weight_decay: 0.0500 (0.0500)  time: 0.4683  data: 0.0584  max mem: 15572
Epoch: [12]  [1910/2809]  eta: 0:08:38  lr: 0.000042  min_lr: 0.000000  loss: 4.2735 (4.0351)  loss_scale: 65536.0000 (62792.4731)  weight_decay: 0.0500 (0.0500)  time: 0.4233  data: 0.0005  max mem: 15572
Epoch: [12]  [1920/2809]  eta: 0:08:32  lr: 0.000042  min_lr: 0.000000  loss: 4.1839 (4.0363)  loss_scale: 65536.0000 (62806.7548)  weight_decay: 0.0500 (0.0500)  time: 0.4562  data: 0.0007  max mem: 15572
Epoch: [12]  [1930/2809]  eta: 0:08:25  lr: 0.000042  min_lr: 0.000000  loss: 4.1940 (4.0369)  loss_scale: 65536.0000 (62820.8887)  weight_decay: 0.0500 (0.0500)  time: 0.4653  data: 0.0008  max mem: 15572
Epoch: [12]  [1940/2809]  eta: 0:08:19  lr: 0.000042  min_lr: 0.000000  loss: 3.8387 (4.0356)  loss_scale: 65536.0000 (62834.8769)  weight_decay: 0.0500 (0.0500)  time: 0.4394  data: 0.0006  max mem: 15572
Epoch: [12]  [1950/2809]  eta: 0:08:13  lr: 0.000042  min_lr: 0.000000  loss: 3.8387 (4.0361)  loss_scale: 65536.0000 (62848.7217)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0835  max mem: 15572
Epoch: [12]  [1960/2809]  eta: 0:08:08  lr: 0.000042  min_lr: 0.000000  loss: 3.9760 (4.0345)  loss_scale: 65536.0000 (62862.4253)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1650  max mem: 15572
Epoch: [12]  [1970/2809]  eta: 0:08:02  lr: 0.000042  min_lr: 0.000000  loss: 3.6170 (4.0335)  loss_scale: 65536.0000 (62875.9899)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1584  max mem: 15572
Epoch: [12]  [1980/2809]  eta: 0:07:56  lr: 0.000041  min_lr: 0.000000  loss: 3.9537 (4.0335)  loss_scale: 65536.0000 (62889.4175)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1347  max mem: 15572
Epoch: [12]  [1990/2809]  eta: 0:07:50  lr: 0.000041  min_lr: 0.000000  loss: 4.1633 (4.0341)  loss_scale: 65536.0000 (62902.7102)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.1162  max mem: 15572
Epoch: [12]  [2000/2809]  eta: 0:07:45  lr: 0.000041  min_lr: 0.000000  loss: 4.1633 (4.0344)  loss_scale: 65536.0000 (62915.8701)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1408  max mem: 15572
Epoch: [12]  [2010/2809]  eta: 0:07:39  lr: 0.000041  min_lr: 0.000000  loss: 3.8680 (4.0335)  loss_scale: 65536.0000 (62928.8991)  weight_decay: 0.0500 (0.0500)  time: 0.6501  data: 0.2157  max mem: 15572
Epoch: [12]  [2020/2809]  eta: 0:07:34  lr: 0.000041  min_lr: 0.000000  loss: 4.0094 (4.0344)  loss_scale: 65536.0000 (62941.7991)  weight_decay: 0.0500 (0.0500)  time: 0.6936  data: 0.2345  max mem: 15572
[2025-01-13 02:33:55,449] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:33:55,449] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [2030/2809]  eta: 0:07:28  lr: 0.000041  min_lr: 0.000000  loss: 3.9552 (4.0328)  loss_scale: 65536.0000 (63115.9114)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1228  max mem: 15572
[2025-01-13 02:33:59,035] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35742
[2025-01-13 02:33:59,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:33:59,035] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2040/2809]  eta: 0:07:22  lr: 0.000041  min_lr: 0.000000  loss: 3.9552 (4.0335)  loss_scale: 65536.0000 (63224.0980)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1131  max mem: 15572
Epoch: [12]  [2050/2809]  eta: 0:07:16  lr: 0.000041  min_lr: 0.000000  loss: 4.1784 (4.0336)  loss_scale: 65536.0000 (63235.3701)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1370  max mem: 15572
Epoch: [12]  [2060/2809]  eta: 0:07:11  lr: 0.000041  min_lr: 0.000000  loss: 3.8567 (4.0316)  loss_scale: 65536.0000 (63246.5328)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1331  max mem: 15572
Epoch: [12]  [2070/2809]  eta: 0:07:05  lr: 0.000041  min_lr: 0.000000  loss: 3.9945 (4.0320)  loss_scale: 65536.0000 (63257.5876)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0969  max mem: 15572
Epoch: [12]  [2080/2809]  eta: 0:06:59  lr: 0.000041  min_lr: 0.000000  loss: 4.1530 (4.0326)  loss_scale: 65536.0000 (63268.5363)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1447  max mem: 15572
Epoch: [12]  [2090/2809]  eta: 0:06:53  lr: 0.000041  min_lr: 0.000000  loss: 4.0659 (4.0330)  loss_scale: 65536.0000 (63279.3802)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1977  max mem: 15572
Epoch: [12]  [2100/2809]  eta: 0:06:47  lr: 0.000041  min_lr: 0.000000  loss: 4.0039 (4.0322)  loss_scale: 65536.0000 (63290.1209)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1013  max mem: 15572
Epoch: [12]  [2110/2809]  eta: 0:06:42  lr: 0.000041  min_lr: 0.000000  loss: 4.1008 (4.0322)  loss_scale: 65536.0000 (63300.7598)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1125  max mem: 15572
Epoch: [12]  [2120/2809]  eta: 0:06:36  lr: 0.000041  min_lr: 0.000000  loss: 4.1080 (4.0330)  loss_scale: 65536.0000 (63311.2984)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.1022  max mem: 15572
Epoch: [12]  [2130/2809]  eta: 0:06:30  lr: 0.000041  min_lr: 0.000000  loss: 4.1061 (4.0330)  loss_scale: 65536.0000 (63321.7382)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0917  max mem: 15572
Epoch: [12]  [2140/2809]  eta: 0:06:24  lr: 0.000041  min_lr: 0.000000  loss: 4.0176 (4.0324)  loss_scale: 65536.0000 (63332.0803)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0856  max mem: 15572
Epoch: [12]  [2150/2809]  eta: 0:06:17  lr: 0.000041  min_lr: 0.000000  loss: 3.8992 (4.0316)  loss_scale: 65536.0000 (63342.3264)  weight_decay: 0.0500 (0.0500)  time: 0.4209  data: 0.0228  max mem: 15572
Epoch: [12]  [2160/2809]  eta: 0:06:11  lr: 0.000041  min_lr: 0.000000  loss: 3.9296 (4.0311)  loss_scale: 65536.0000 (63352.4776)  weight_decay: 0.0500 (0.0500)  time: 0.4072  data: 0.0004  max mem: 15572
[2025-01-13 02:35:07,632] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:35:07,632] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:35:08,560] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35873
[2025-01-13 02:35:08,560] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:35:08,561] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2170/2809]  eta: 0:06:05  lr: 0.000041  min_lr: 0.000000  loss: 4.0188 (4.0315)  loss_scale: 65536.0000 (63422.9093)  weight_decay: 0.0500 (0.0500)  time: 0.4464  data: 0.0009  max mem: 15572
Epoch: [12]  [2180/2809]  eta: 0:05:59  lr: 0.000041  min_lr: 0.000000  loss: 4.0052 (4.0318)  loss_scale: 65536.0000 (63432.5979)  weight_decay: 0.0500 (0.0500)  time: 0.4731  data: 0.0211  max mem: 15572
Epoch: [12]  [2190/2809]  eta: 0:05:54  lr: 0.000041  min_lr: 0.000000  loss: 3.8742 (4.0314)  loss_scale: 65536.0000 (63442.1981)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1089  max mem: 15572
Epoch: [12]  [2200/2809]  eta: 0:05:48  lr: 0.000041  min_lr: 0.000000  loss: 4.0208 (4.0318)  loss_scale: 65536.0000 (63451.7110)  weight_decay: 0.0500 (0.0500)  time: 0.6745  data: 0.1954  max mem: 15572
Epoch: [12]  [2210/2809]  eta: 0:05:43  lr: 0.000041  min_lr: 0.000000  loss: 3.9715 (4.0311)  loss_scale: 65536.0000 (63461.1379)  weight_decay: 0.0500 (0.0500)  time: 0.7311  data: 0.2599  max mem: 15572
Epoch: [12]  [2220/2809]  eta: 0:05:38  lr: 0.000041  min_lr: 0.000000  loss: 3.9161 (4.0308)  loss_scale: 65536.0000 (63470.4800)  weight_decay: 0.0500 (0.0500)  time: 0.7841  data: 0.3321  max mem: 15572
Epoch: [12]  [2230/2809]  eta: 0:05:32  lr: 0.000041  min_lr: 0.000000  loss: 4.0116 (4.0307)  loss_scale: 65536.0000 (63479.7382)  weight_decay: 0.0500 (0.0500)  time: 0.7159  data: 0.2626  max mem: 15572
Epoch: [12]  [2240/2809]  eta: 0:05:27  lr: 0.000041  min_lr: 0.000000  loss: 4.1151 (4.0313)  loss_scale: 65536.0000 (63488.9139)  weight_decay: 0.0500 (0.0500)  time: 0.6409  data: 0.1760  max mem: 15572
Epoch: [12]  [2250/2809]  eta: 0:05:21  lr: 0.000041  min_lr: 0.000000  loss: 4.1151 (4.0313)  loss_scale: 65536.0000 (63498.0080)  weight_decay: 0.0500 (0.0500)  time: 0.6709  data: 0.2154  max mem: 15572
Epoch: [12]  [2260/2809]  eta: 0:05:16  lr: 0.000041  min_lr: 0.000000  loss: 3.9751 (4.0303)  loss_scale: 65536.0000 (63507.0217)  weight_decay: 0.0500 (0.0500)  time: 0.6855  data: 0.2387  max mem: 15572
Epoch: [12]  [2270/2809]  eta: 0:05:10  lr: 0.000041  min_lr: 0.000000  loss: 3.7809 (4.0293)  loss_scale: 65536.0000 (63515.9560)  weight_decay: 0.0500 (0.0500)  time: 0.6921  data: 0.2430  max mem: 15572
Epoch: [12]  [2280/2809]  eta: 0:05:05  lr: 0.000041  min_lr: 0.000000  loss: 3.9565 (4.0286)  loss_scale: 65536.0000 (63524.8119)  weight_decay: 0.0500 (0.0500)  time: 0.7105  data: 0.2580  max mem: 15572
Epoch: [12]  [2290/2809]  eta: 0:04:59  lr: 0.000041  min_lr: 0.000000  loss: 3.9951 (4.0283)  loss_scale: 65536.0000 (63533.5906)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1311  max mem: 15572
[2025-01-13 02:36:30,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=36000, skipped=237, lr=[4.0059693595025643e-07, 4.0059693595025643e-07, 5.72281337071795e-07, 5.72281337071795e-07, 8.175447672454215e-07, 8.175447672454215e-07, 1.167921096064888e-06, 1.167921096064888e-06, 1.6684587086641256e-06, 1.6684587086641256e-06, 2.383512440948751e-06, 2.383512440948751e-06, 3.4050177727839304e-06, 3.4050177727839304e-06, 4.8643111039770435e-06, 4.8643111039770435e-06, 6.949015862824348e-06, 6.949015862824348e-06, 9.927165518320499e-06, 9.927165518320499e-06, 1.418166502617214e-05, 1.418166502617214e-05, 2.0259521465960202e-05, 2.0259521465960202e-05, 2.894217352280029e-05, 2.894217352280029e-05, 4.134596217542899e-05, 4.134596217542899e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 02:36:30,116] [INFO] [timer.py:260:stop] epoch=0/micro_step=36000/global_step=36000, RunningAvgSamplesPerSec=28.049011901492868, CurrSamplesPerSec=34.23323133302545, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-13 02:36:31,416] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:36:31,417] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [2300/2809]  eta: 0:04:53  lr: 0.000041  min_lr: 0.000000  loss: 3.9975 (4.0273)  loss_scale: 65536.0000 (63741.6636)  weight_decay: 0.0500 (0.0500)  time: 0.4131  data: 0.0007  max mem: 15572
[2025-01-13 02:36:34,404] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36009
[2025-01-13 02:36:34,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:36:34,405] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2310/2809]  eta: 0:04:47  lr: 0.000041  min_lr: 0.000000  loss: 3.9012 (4.0270)  loss_scale: 65536.0000 (63749.4280)  weight_decay: 0.0500 (0.0500)  time: 0.4557  data: 0.0227  max mem: 15572
Epoch: [12]  [2320/2809]  eta: 0:04:41  lr: 0.000041  min_lr: 0.000000  loss: 3.9065 (4.0266)  loss_scale: 65536.0000 (63757.1254)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0978  max mem: 15572
[2025-01-13 02:36:46,535] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36032
[2025-01-13 02:36:46,535] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 02:36:46,535] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [2330/2809]  eta: 0:04:35  lr: 0.000041  min_lr: 0.000000  loss: 4.1216 (4.0262)  loss_scale: 65536.0000 (63666.3544)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1700  max mem: 15572
Epoch: [12]  [2340/2809]  eta: 0:04:30  lr: 0.000041  min_lr: 0.000000  loss: 4.1694 (4.0274)  loss_scale: 32768.0000 (63534.3665)  weight_decay: 0.0500 (0.0500)  time: 0.6351  data: 0.2193  max mem: 15572
Epoch: [12]  [2350/2809]  eta: 0:04:24  lr: 0.000041  min_lr: 0.000000  loss: 4.1894 (4.0268)  loss_scale: 32768.0000 (63403.5015)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1864  max mem: 15572
Epoch: [12]  [2360/2809]  eta: 0:04:18  lr: 0.000041  min_lr: 0.000000  loss: 4.0297 (4.0268)  loss_scale: 32768.0000 (63273.7450)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1239  max mem: 15572
Epoch: [12]  [2370/2809]  eta: 0:04:12  lr: 0.000041  min_lr: 0.000000  loss: 4.1616 (4.0270)  loss_scale: 32768.0000 (63145.0831)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0917  max mem: 15572
Epoch: [12]  [2380/2809]  eta: 0:04:06  lr: 0.000041  min_lr: 0.000000  loss: 4.1217 (4.0275)  loss_scale: 32768.0000 (63017.5019)  weight_decay: 0.0500 (0.0500)  time: 0.4979  data: 0.0627  max mem: 15572
Epoch: [12]  [2390/2809]  eta: 0:04:01  lr: 0.000041  min_lr: 0.000000  loss: 3.9410 (4.0267)  loss_scale: 32768.0000 (62890.9879)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1350  max mem: 15572
Epoch: [12]  [2400/2809]  eta: 0:03:55  lr: 0.000041  min_lr: 0.000000  loss: 3.9410 (4.0266)  loss_scale: 32768.0000 (62765.5277)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1613  max mem: 15572
Epoch: [12]  [2410/2809]  eta: 0:03:49  lr: 0.000041  min_lr: 0.000000  loss: 3.9556 (4.0255)  loss_scale: 32768.0000 (62641.1083)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1246  max mem: 15572
Epoch: [12]  [2420/2809]  eta: 0:03:43  lr: 0.000041  min_lr: 0.000000  loss: 3.8638 (4.0251)  loss_scale: 32768.0000 (62517.7166)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0829  max mem: 15572
Epoch: [12]  [2430/2809]  eta: 0:03:37  lr: 0.000041  min_lr: 0.000000  loss: 3.9765 (4.0249)  loss_scale: 32768.0000 (62395.3402)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0977  max mem: 15572
Epoch: [12]  [2440/2809]  eta: 0:03:32  lr: 0.000041  min_lr: 0.000000  loss: 4.0055 (4.0249)  loss_scale: 32768.0000 (62273.9664)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1259  max mem: 15572
Epoch: [12]  [2450/2809]  eta: 0:03:26  lr: 0.000041  min_lr: 0.000000  loss: 4.0452 (4.0252)  loss_scale: 32768.0000 (62153.5830)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0998  max mem: 15572
[2025-01-13 02:37:58,659] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:37:58,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [2460/2809]  eta: 0:03:20  lr: 0.000041  min_lr: 0.000000  loss: 4.0372 (4.0248)  loss_scale: 32768.0000 (62140.6973)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1439  max mem: 15572
Epoch: [12]  [2470/2809]  eta: 0:03:14  lr: 0.000041  min_lr: 0.000000  loss: 4.0047 (4.0248)  loss_scale: 65536.0000 (62154.4379)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1769  max mem: 15572
Epoch: [12]  [2480/2809]  eta: 0:03:09  lr: 0.000041  min_lr: 0.000000  loss: 4.0584 (4.0246)  loss_scale: 65536.0000 (62168.0677)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1208  max mem: 15572
Epoch: [12]  [2490/2809]  eta: 0:03:03  lr: 0.000041  min_lr: 0.000000  loss: 4.1056 (4.0248)  loss_scale: 65536.0000 (62181.5881)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0696  max mem: 15572
Epoch: [12]  [2500/2809]  eta: 0:02:57  lr: 0.000041  min_lr: 0.000000  loss: 3.9411 (4.0244)  loss_scale: 65536.0000 (62195.0004)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0665  max mem: 15572
Epoch: [12]  [2510/2809]  eta: 0:02:51  lr: 0.000041  min_lr: 0.000000  loss: 3.9245 (4.0242)  loss_scale: 65536.0000 (62208.3059)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0867  max mem: 15572
Epoch: [12]  [2520/2809]  eta: 0:02:45  lr: 0.000041  min_lr: 0.000000  loss: 3.9011 (4.0238)  loss_scale: 65536.0000 (62221.5058)  weight_decay: 0.0500 (0.0500)  time: 0.4484  data: 0.0558  max mem: 15572
Epoch: [12]  [2530/2809]  eta: 0:02:39  lr: 0.000041  min_lr: 0.000000  loss: 3.9757 (4.0232)  loss_scale: 65536.0000 (62234.6013)  weight_decay: 0.0500 (0.0500)  time: 0.4048  data: 0.0004  max mem: 15572
Epoch: [12]  [2540/2809]  eta: 0:02:33  lr: 0.000041  min_lr: 0.000000  loss: 4.0388 (4.0238)  loss_scale: 65536.0000 (62247.5939)  weight_decay: 0.0500 (0.0500)  time: 0.4525  data: 0.0005  max mem: 15572
Epoch: [12]  [2550/2809]  eta: 0:02:28  lr: 0.000041  min_lr: 0.000000  loss: 4.2528 (4.0237)  loss_scale: 65536.0000 (62260.4845)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0475  max mem: 15572
Epoch: [12]  [2560/2809]  eta: 0:02:22  lr: 0.000041  min_lr: 0.000000  loss: 4.0647 (4.0247)  loss_scale: 65536.0000 (62273.2745)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1713  max mem: 15572
Epoch: [12]  [2570/2809]  eta: 0:02:16  lr: 0.000041  min_lr: 0.000000  loss: 3.9661 (4.0243)  loss_scale: 65536.0000 (62285.9650)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.1943  max mem: 15572
Epoch: [12]  [2580/2809]  eta: 0:02:11  lr: 0.000041  min_lr: 0.000000  loss: 3.9773 (4.0250)  loss_scale: 65536.0000 (62298.5571)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1491  max mem: 15572
[2025-01-13 02:39:08,447] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:39:08,448] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:39:09,477] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36291
[2025-01-13 02:39:09,478] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:39:09,478] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2590/2809]  eta: 0:02:05  lr: 0.000041  min_lr: 0.000000  loss: 3.9107 (4.0241)  loss_scale: 65536.0000 (62361.6395)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1808  max mem: 15572
Epoch: [12]  [2600/2809]  eta: 0:01:59  lr: 0.000041  min_lr: 0.000000  loss: 3.7754 (4.0238)  loss_scale: 65536.0000 (62373.8439)  weight_decay: 0.0500 (0.0500)  time: 0.6741  data: 0.2068  max mem: 15572
Epoch: [12]  [2610/2809]  eta: 0:01:54  lr: 0.000041  min_lr: 0.000000  loss: 3.8343 (4.0236)  loss_scale: 65536.0000 (62385.9548)  weight_decay: 0.0500 (0.0500)  time: 0.6569  data: 0.2044  max mem: 15572
Epoch: [12]  [2620/2809]  eta: 0:01:48  lr: 0.000041  min_lr: 0.000000  loss: 4.0189 (4.0236)  loss_scale: 65536.0000 (62397.9733)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1800  max mem: 15572
Epoch: [12]  [2630/2809]  eta: 0:01:42  lr: 0.000041  min_lr: 0.000000  loss: 3.8690 (4.0222)  loss_scale: 65536.0000 (62409.9004)  weight_decay: 0.0500 (0.0500)  time: 0.6522  data: 0.1991  max mem: 15572
Epoch: [12]  [2640/2809]  eta: 0:01:37  lr: 0.000041  min_lr: 0.000000  loss: 3.8690 (4.0219)  loss_scale: 65536.0000 (62421.7372)  weight_decay: 0.0500 (0.0500)  time: 0.7769  data: 0.3082  max mem: 15572
Epoch: [12]  [2650/2809]  eta: 0:01:31  lr: 0.000041  min_lr: 0.000000  loss: 4.0465 (4.0218)  loss_scale: 65536.0000 (62433.4847)  weight_decay: 0.0500 (0.0500)  time: 0.7329  data: 0.2726  max mem: 15572
Epoch: [12]  [2660/2809]  eta: 0:01:25  lr: 0.000041  min_lr: 0.000000  loss: 4.1460 (4.0221)  loss_scale: 65536.0000 (62445.1439)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1309  max mem: 15572
Epoch: [12]  [2670/2809]  eta: 0:01:19  lr: 0.000041  min_lr: 0.000000  loss: 4.1216 (4.0220)  loss_scale: 65536.0000 (62456.7158)  weight_decay: 0.0500 (0.0500)  time: 0.4755  data: 0.0481  max mem: 15572
Epoch: [12]  [2680/2809]  eta: 0:01:14  lr: 0.000041  min_lr: 0.000000  loss: 3.8676 (4.0214)  loss_scale: 65536.0000 (62468.2014)  weight_decay: 0.0500 (0.0500)  time: 0.4530  data: 0.0388  max mem: 15572
Epoch: [12]  [2690/2809]  eta: 0:01:08  lr: 0.000041  min_lr: 0.000000  loss: 3.8939 (4.0218)  loss_scale: 65536.0000 (62479.6016)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0847  max mem: 15572
Epoch: [12]  [2700/2809]  eta: 0:01:02  lr: 0.000041  min_lr: 0.000000  loss: 4.2358 (4.0225)  loss_scale: 65536.0000 (62490.9174)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0972  max mem: 15572
Epoch: [12]  [2710/2809]  eta: 0:00:56  lr: 0.000041  min_lr: 0.000000  loss: 4.0646 (4.0219)  loss_scale: 65536.0000 (62502.1498)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1298  max mem: 15572
[2025-01-13 02:40:27,789] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:40:27,789] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:40:28,665] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36422
[2025-01-13 02:40:28,665] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:40:28,666] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2720/2809]  eta: 0:00:51  lr: 0.000041  min_lr: 0.000000  loss: 4.0523 (4.0220)  loss_scale: 65536.0000 (62561.4700)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1612  max mem: 15572
Epoch: [12]  [2730/2809]  eta: 0:00:45  lr: 0.000041  min_lr: 0.000000  loss: 4.1242 (4.0230)  loss_scale: 65536.0000 (62572.3618)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1393  max mem: 15572
Epoch: [12]  [2740/2809]  eta: 0:00:39  lr: 0.000041  min_lr: 0.000000  loss: 4.1242 (4.0229)  loss_scale: 65536.0000 (62583.1740)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1369  max mem: 15572
Epoch: [12]  [2750/2809]  eta: 0:00:33  lr: 0.000041  min_lr: 0.000000  loss: 3.9584 (4.0227)  loss_scale: 65536.0000 (62593.9077)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1662  max mem: 15572
Epoch: [12]  [2760/2809]  eta: 0:00:28  lr: 0.000041  min_lr: 0.000000  loss: 3.9253 (4.0223)  loss_scale: 65536.0000 (62604.5636)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1414  max mem: 15572
Epoch: [12]  [2770/2809]  eta: 0:00:22  lr: 0.000041  min_lr: 0.000000  loss: 3.8960 (4.0216)  loss_scale: 65536.0000 (62615.1425)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1549  max mem: 15572
Epoch: [12]  [2780/2809]  eta: 0:00:16  lr: 0.000041  min_lr: 0.000000  loss: 3.8324 (4.0210)  loss_scale: 65536.0000 (62625.6455)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1593  max mem: 15572
Epoch: [12]  [2790/2809]  eta: 0:00:10  lr: 0.000041  min_lr: 0.000000  loss: 3.9891 (4.0214)  loss_scale: 65536.0000 (62636.0731)  weight_decay: 0.0500 (0.0500)  time: 0.5036  data: 0.0603  max mem: 15572
Epoch: [12]  [2800/2809]  eta: 0:00:05  lr: 0.000041  min_lr: 0.000000  loss: 4.1113 (4.0217)  loss_scale: 65536.0000 (62646.4263)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0860  max mem: 15572
Epoch: [12]  [2808/2809]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000000  loss: 4.1503 (4.0218)  loss_scale: 65536.0000 (62654.6557)  weight_decay: 0.0500 (0.0500)  time: 0.4767  data: 0.0858  max mem: 15572
Epoch: [12] Total time: 0:26:53 (0.5743 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000000  loss: 4.1503 (4.0218)  loss_scale: 65536.0000 (62654.6557)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:18:57  loss: 0.4260 (0.4260)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.1805  data: 3.9717  max mem: 15572
Val:  [ 10/272]  eta: 0:03:07  loss: 3.3503 (2.9670)  acc1: 22.2222 (29.7980)  acc5: 44.4444 (50.0000)  time: 0.7168  data: 0.5376  max mem: 15572
Val:  [ 20/272]  eta: 0:02:06  loss: 3.0539 (2.9522)  acc1: 27.7778 (32.5397)  acc5: 55.5556 (56.6138)  time: 0.3167  data: 0.1406  max mem: 15572
Val:  [ 30/272]  eta: 0:01:47  loss: 2.9921 (3.0119)  acc1: 33.3333 (30.4660)  acc5: 66.6667 (58.9606)  time: 0.2911  data: 0.1146  max mem: 15572
Val:  [ 40/272]  eta: 0:01:37  loss: 2.9376 (2.9840)  acc1: 27.7778 (29.2683)  acc5: 72.2222 (61.5176)  time: 0.3336  data: 0.1425  max mem: 15572
Val:  [ 50/272]  eta: 0:01:27  loss: 2.7740 (2.8943)  acc1: 27.7778 (31.9172)  acc5: 72.2222 (63.9434)  time: 0.3202  data: 0.1224  max mem: 15572
Val:  [ 60/272]  eta: 0:01:22  loss: 1.7277 (2.7456)  acc1: 55.5556 (36.1566)  acc5: 83.3333 (66.0291)  time: 0.3335  data: 0.1429  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 1.7399 (2.6641)  acc1: 55.5556 (37.5587)  acc5: 83.3333 (68.2316)  time: 0.3148  data: 0.1223  max mem: 15572
Val:  [ 80/272]  eta: 0:01:07  loss: 2.4323 (2.6642)  acc1: 38.8889 (37.7915)  acc5: 77.7778 (68.0384)  time: 0.2380  data: 0.0591  max mem: 15572
Val:  [ 90/272]  eta: 0:01:00  loss: 2.9519 (2.7051)  acc1: 27.7778 (37.0574)  acc5: 66.6667 (67.7045)  time: 0.1956  data: 0.0359  max mem: 15572
Val:  [100/272]  eta: 0:00:54  loss: 2.9519 (2.7499)  acc1: 27.7778 (36.5237)  acc5: 72.2222 (67.2717)  time: 0.1775  data: 0.0215  max mem: 15572
Val:  [110/272]  eta: 0:00:49  loss: 3.0718 (2.8259)  acc1: 5.5556 (33.9840)  acc5: 55.5556 (65.5155)  time: 0.1858  data: 0.0146  max mem: 15572
Val:  [120/272]  eta: 0:00:46  loss: 3.5026 (2.8556)  acc1: 5.5556 (33.2415)  acc5: 50.0000 (64.8760)  time: 0.2300  data: 0.0425  max mem: 15572
Val:  [130/272]  eta: 0:00:43  loss: 2.8868 (2.8195)  acc1: 33.3333 (34.6480)  acc5: 66.6667 (65.4368)  time: 0.3143  data: 0.1231  max mem: 15572
Val:  [140/272]  eta: 0:00:41  loss: 2.5239 (2.8151)  acc1: 33.3333 (34.9488)  acc5: 72.2222 (65.4058)  time: 0.3568  data: 0.1477  max mem: 15572
Val:  [150/272]  eta: 0:00:38  loss: 2.8058 (2.8052)  acc1: 33.3333 (34.8418)  acc5: 72.2222 (66.1516)  time: 0.3405  data: 0.1267  max mem: 15572
Val:  [160/272]  eta: 0:00:35  loss: 2.5095 (2.7909)  acc1: 38.8889 (35.6453)  acc5: 77.7778 (66.7702)  time: 0.3315  data: 0.1319  max mem: 15572
Val:  [170/272]  eta: 0:00:31  loss: 2.8040 (2.8237)  acc1: 27.7778 (34.7953)  acc5: 72.2222 (66.0169)  time: 0.3249  data: 0.1308  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 2.8040 (2.8115)  acc1: 27.7778 (34.9908)  acc5: 72.2222 (66.6053)  time: 0.3348  data: 0.1405  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.7627 (2.8517)  acc1: 27.7778 (34.0605)  acc5: 66.6667 (65.2414)  time: 0.3619  data: 0.1684  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.0035 (2.8683)  acc1: 22.2222 (33.7479)  acc5: 55.5556 (64.8701)  time: 0.3621  data: 0.1686  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.6809 (2.8730)  acc1: 33.3333 (34.1232)  acc5: 72.2222 (64.8763)  time: 0.3569  data: 0.1678  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.6529 (2.8618)  acc1: 44.4444 (34.5902)  acc5: 72.2222 (65.0578)  time: 0.3691  data: 0.1855  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.2210 (2.8259)  acc1: 50.0000 (35.9548)  acc5: 72.2222 (65.5123)  time: 0.3778  data: 0.1784  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.9954 (2.8011)  acc1: 50.0000 (36.3762)  acc5: 83.3333 (66.2056)  time: 0.3829  data: 0.1812  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.5532 (2.8189)  acc1: 22.2222 (35.6131)  acc5: 72.2222 (65.8256)  time: 0.3705  data: 0.1667  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.7933 (2.7477)  acc1: 72.2222 (37.7182)  acc5: 83.3333 (66.8582)  time: 0.3466  data: 0.1385  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.7933 (2.7471)  acc1: 55.5556 (37.3924)  acc5: 83.3333 (67.0152)  time: 0.2819  data: 0.1036  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8303 (2.7509)  acc1: 55.5556 (37.3746)  acc5: 83.3333 (67.0080)  time: 0.2745  data: 0.1035  max mem: 15572
Val: Total time: 0:01:29 (0.3277 s / it)
* Acc@1 37.375 Acc@5 67.008 loss 2.751
Accuracy of the network on the 4883 val videos: 37.4%
[2025-01-13 02:42:49,970] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 02:42:49,987] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 02:42:49,988] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 02:42:53,099] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 02:42:53,100] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 37.37%
Epoch: [13]  [   0/2809]  eta: 7:18:56  lr: 0.000041  min_lr: 0.000000  loss: 4.2783 (4.2783)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.3758  data: 8.7436  max mem: 15572
Epoch: [13]  [  10/2809]  eta: 1:06:25  lr: 0.000041  min_lr: 0.000000  loss: 4.2783 (4.1600)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4239  data: 0.9716  max mem: 15572
Epoch: [13]  [  20/2809]  eta: 0:49:27  lr: 0.000041  min_lr: 0.000000  loss: 4.0971 (4.0398)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.2144  max mem: 15572
Epoch: [13]  [  30/2809]  eta: 0:42:03  lr: 0.000041  min_lr: 0.000000  loss: 3.9976 (4.0078)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.2133  max mem: 15572
[2025-01-13 02:43:22,918] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:43:22,919] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:43:25,231] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36556
[2025-01-13 02:43:25,231] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:43:25,232] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [  40/2809]  eta: 0:36:36  lr: 0.000041  min_lr: 0.000000  loss: 3.8694 (3.9789)  loss_scale: 65536.0000 (73528.1951)  weight_decay: 0.0500 (0.0500)  time: 0.5090  data: 0.0965  max mem: 15572
Epoch: [13]  [  50/2809]  eta: 0:33:08  lr: 0.000041  min_lr: 0.000000  loss: 3.8474 (3.9755)  loss_scale: 65536.0000 (71961.0980)  weight_decay: 0.0500 (0.0500)  time: 0.4305  data: 0.0005  max mem: 15572
Epoch: [13]  [  60/2809]  eta: 0:30:53  lr: 0.000041  min_lr: 0.000000  loss: 4.0041 (3.9783)  loss_scale: 65536.0000 (70907.8033)  weight_decay: 0.0500 (0.0500)  time: 0.4305  data: 0.0005  max mem: 15572
Epoch: [13]  [  70/2809]  eta: 0:29:55  lr: 0.000041  min_lr: 0.000000  loss: 4.0985 (4.0049)  loss_scale: 65536.0000 (70151.2113)  weight_decay: 0.0500 (0.0500)  time: 0.4892  data: 0.0400  max mem: 15572
Epoch: [13]  [  80/2809]  eta: 0:29:25  lr: 0.000041  min_lr: 0.000000  loss: 4.1032 (4.0128)  loss_scale: 65536.0000 (69581.4321)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1265  max mem: 15572
Epoch: [13]  [  90/2809]  eta: 0:28:52  lr: 0.000041  min_lr: 0.000000  loss: 4.1716 (4.0175)  loss_scale: 65536.0000 (69136.8791)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1506  max mem: 15572
Epoch: [13]  [ 100/2809]  eta: 0:28:25  lr: 0.000041  min_lr: 0.000000  loss: 4.0080 (4.0149)  loss_scale: 65536.0000 (68780.3564)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1232  max mem: 15572
Epoch: [13]  [ 110/2809]  eta: 0:28:11  lr: 0.000041  min_lr: 0.000000  loss: 3.8204 (3.9835)  loss_scale: 65536.0000 (68488.0721)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1298  max mem: 15572
Epoch: [13]  [ 120/2809]  eta: 0:27:39  lr: 0.000041  min_lr: 0.000000  loss: 3.7887 (3.9742)  loss_scale: 65536.0000 (68244.0992)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1150  max mem: 15572
Epoch: [13]  [ 130/2809]  eta: 0:27:20  lr: 0.000041  min_lr: 0.000000  loss: 3.8093 (3.9682)  loss_scale: 65536.0000 (68037.3740)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.1098  max mem: 15572
Epoch: [13]  [ 140/2809]  eta: 0:27:11  lr: 0.000041  min_lr: 0.000000  loss: 4.1475 (3.9947)  loss_scale: 65536.0000 (67859.9716)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1414  max mem: 15572
Epoch: [13]  [ 150/2809]  eta: 0:26:46  lr: 0.000041  min_lr: 0.000000  loss: 4.0943 (3.9889)  loss_scale: 65536.0000 (67706.0662)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1223  max mem: 15572
Epoch: [13]  [ 160/2809]  eta: 0:26:42  lr: 0.000041  min_lr: 0.000000  loss: 3.9036 (3.9754)  loss_scale: 65536.0000 (67571.2795)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1384  max mem: 15572
[2025-01-13 02:44:36,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:44:36,134] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:44:36,677] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36686
[2025-01-13 02:44:36,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:44:36,678] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 170/2809]  eta: 0:26:44  lr: 0.000041  min_lr: 0.000000  loss: 3.7841 (3.9678)  loss_scale: 65536.0000 (67835.5088)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.1920  max mem: 15572
Epoch: [13]  [ 180/2809]  eta: 0:26:43  lr: 0.000041  min_lr: 0.000000  loss: 3.7590 (3.9622)  loss_scale: 65536.0000 (67708.4641)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.2057  max mem: 15572
Epoch: [13]  [ 190/2809]  eta: 0:26:31  lr: 0.000041  min_lr: 0.000000  loss: 3.7670 (3.9605)  loss_scale: 65536.0000 (67594.7225)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1744  max mem: 15572
Epoch: [13]  [ 200/2809]  eta: 0:26:11  lr: 0.000041  min_lr: 0.000000  loss: 3.7749 (3.9534)  loss_scale: 65536.0000 (67492.2985)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.1079  max mem: 15572
Epoch: [13]  [ 210/2809]  eta: 0:25:54  lr: 0.000041  min_lr: 0.000000  loss: 3.9336 (3.9622)  loss_scale: 65536.0000 (67399.5829)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0957  max mem: 15572
Epoch: [13]  [ 220/2809]  eta: 0:25:24  lr: 0.000041  min_lr: 0.000000  loss: 4.0939 (3.9545)  loss_scale: 65536.0000 (67315.2579)  weight_decay: 0.0500 (0.0500)  time: 0.4523  data: 0.0562  max mem: 15572
Epoch: [13]  [ 230/2809]  eta: 0:25:04  lr: 0.000041  min_lr: 0.000000  loss: 3.8230 (3.9528)  loss_scale: 65536.0000 (67238.2338)  weight_decay: 0.0500 (0.0500)  time: 0.4280  data: 0.0005  max mem: 15572
Epoch: [13]  [ 240/2809]  eta: 0:24:41  lr: 0.000041  min_lr: 0.000000  loss: 4.1157 (3.9625)  loss_scale: 65536.0000 (67167.6017)  weight_decay: 0.0500 (0.0500)  time: 0.4451  data: 0.0007  max mem: 15572
Epoch: [13]  [ 250/2809]  eta: 0:24:38  lr: 0.000041  min_lr: 0.000000  loss: 4.1910 (3.9694)  loss_scale: 65536.0000 (67102.5976)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0657  max mem: 15572
Epoch: [13]  [ 260/2809]  eta: 0:24:44  lr: 0.000041  min_lr: 0.000000  loss: 4.0454 (3.9667)  loss_scale: 65536.0000 (67042.5747)  weight_decay: 0.0500 (0.0500)  time: 0.6509  data: 0.1823  max mem: 15572
Epoch: [13]  [ 270/2809]  eta: 0:24:46  lr: 0.000041  min_lr: 0.000000  loss: 3.8551 (3.9632)  loss_scale: 65536.0000 (66986.9815)  weight_decay: 0.0500 (0.0500)  time: 0.6834  data: 0.2264  max mem: 15572
Epoch: [13]  [ 280/2809]  eta: 0:24:47  lr: 0.000041  min_lr: 0.000000  loss: 3.8012 (3.9611)  loss_scale: 65536.0000 (66935.3452)  weight_decay: 0.0500 (0.0500)  time: 0.6632  data: 0.2134  max mem: 15572
Epoch: [13]  [ 290/2809]  eta: 0:24:49  lr: 0.000041  min_lr: 0.000000  loss: 3.8534 (3.9633)  loss_scale: 65536.0000 (66887.2577)  weight_decay: 0.0500 (0.0500)  time: 0.6721  data: 0.2197  max mem: 15572
[2025-01-13 02:45:51,267] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:45:51,267] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:45:51,680] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36816
[2025-01-13 02:45:51,680] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:45:51,681] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 300/2809]  eta: 0:24:51  lr: 0.000041  min_lr: 0.000000  loss: 4.0102 (3.9576)  loss_scale: 65536.0000 (67060.0930)  weight_decay: 0.0500 (0.0500)  time: 0.6834  data: 0.2343  max mem: 15572
Epoch: [13]  [ 310/2809]  eta: 0:24:51  lr: 0.000041  min_lr: 0.000000  loss: 4.0725 (3.9669)  loss_scale: 65536.0000 (67011.0868)  weight_decay: 0.0500 (0.0500)  time: 0.6740  data: 0.2294  max mem: 15572
[2025-01-13 02:46:05,171] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36836
[2025-01-13 02:46:05,171] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 02:46:05,171] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 320/2809]  eta: 0:24:52  lr: 0.000041  min_lr: 0.000000  loss: 4.0111 (3.9612)  loss_scale: 65536.0000 (66760.9720)  weight_decay: 0.0500 (0.0500)  time: 0.6791  data: 0.2254  max mem: 15572
Epoch: [13]  [ 330/2809]  eta: 0:24:54  lr: 0.000041  min_lr: 0.000000  loss: 3.8918 (3.9627)  loss_scale: 32768.0000 (65733.9940)  weight_decay: 0.0500 (0.0500)  time: 0.6981  data: 0.2365  max mem: 15572
Epoch: [13]  [ 340/2809]  eta: 0:24:54  lr: 0.000041  min_lr: 0.000000  loss: 3.9677 (3.9649)  loss_scale: 32768.0000 (64767.2493)  weight_decay: 0.0500 (0.0500)  time: 0.6910  data: 0.2376  max mem: 15572
Epoch: [13]  [ 350/2809]  eta: 0:24:53  lr: 0.000041  min_lr: 0.000000  loss: 3.9246 (3.9635)  loss_scale: 32768.0000 (63855.5897)  weight_decay: 0.0500 (0.0500)  time: 0.6844  data: 0.2398  max mem: 15572
Epoch: [13]  [ 360/2809]  eta: 0:24:35  lr: 0.000041  min_lr: 0.000000  loss: 3.9294 (3.9617)  loss_scale: 32768.0000 (62994.4377)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1387  max mem: 15572
Epoch: [13]  [ 370/2809]  eta: 0:24:19  lr: 0.000041  min_lr: 0.000000  loss: 4.0032 (3.9689)  loss_scale: 32768.0000 (62179.7089)  weight_decay: 0.0500 (0.0500)  time: 0.4392  data: 0.0148  max mem: 15572
Epoch: [13]  [ 380/2809]  eta: 0:24:10  lr: 0.000041  min_lr: 0.000000  loss: 4.1848 (3.9696)  loss_scale: 32768.0000 (61407.7480)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0693  max mem: 15572
Epoch: [13]  [ 390/2809]  eta: 0:24:04  lr: 0.000041  min_lr: 0.000000  loss: 3.9111 (3.9691)  loss_scale: 32768.0000 (60675.2737)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1400  max mem: 15572
Epoch: [13]  [ 400/2809]  eta: 0:23:58  lr: 0.000041  min_lr: 0.000000  loss: 3.9905 (3.9698)  loss_scale: 32768.0000 (59979.3317)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1547  max mem: 15572
Epoch: [13]  [ 410/2809]  eta: 0:23:50  lr: 0.000041  min_lr: 0.000000  loss: 3.9905 (3.9685)  loss_scale: 32768.0000 (59317.2555)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1494  max mem: 15572
Epoch: [13]  [ 420/2809]  eta: 0:23:38  lr: 0.000041  min_lr: 0.000000  loss: 4.1522 (3.9720)  loss_scale: 32768.0000 (58686.6318)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0912  max mem: 15572
Epoch: [13]  [ 430/2809]  eta: 0:23:34  lr: 0.000041  min_lr: 0.000000  loss: 4.1778 (3.9711)  loss_scale: 32768.0000 (58085.2715)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1151  max mem: 15572
Epoch: [13]  [ 440/2809]  eta: 0:23:25  lr: 0.000041  min_lr: 0.000000  loss: 3.8704 (3.9676)  loss_scale: 32768.0000 (57511.1837)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1410  max mem: 15572
[2025-01-13 02:47:19,131] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:47:19,132] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 450/2809]  eta: 0:23:15  lr: 0.000041  min_lr: 0.000000  loss: 3.9548 (3.9718)  loss_scale: 32768.0000 (57180.5233)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0986  max mem: 15572
Epoch: [13]  [ 460/2809]  eta: 0:23:07  lr: 0.000041  min_lr: 0.000000  loss: 3.9164 (3.9666)  loss_scale: 65536.0000 (57361.7701)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0915  max mem: 15572
Epoch: [13]  [ 470/2809]  eta: 0:23:01  lr: 0.000041  min_lr: 0.000000  loss: 4.0155 (3.9679)  loss_scale: 65536.0000 (57535.3206)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1270  max mem: 15572
Epoch: [13]  [ 480/2809]  eta: 0:22:56  lr: 0.000041  min_lr: 0.000000  loss: 4.1196 (3.9703)  loss_scale: 65536.0000 (57701.6549)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1765  max mem: 15572
[2025-01-13 02:47:38,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=37000, skipped=245, lr=[3.958308455732292e-07, 3.958308455732292e-07, 5.654726365331847e-07, 5.654726365331847e-07, 8.078180521902639e-07, 8.078180521902639e-07, 1.1540257888432343e-06, 1.1540257888432343e-06, 1.6486082697760488e-06, 1.6486082697760488e-06, 2.355154671108641e-06, 2.355154671108641e-06, 3.3645066730123448e-06, 3.3645066730123448e-06, 4.806438104303351e-06, 4.806438104303351e-06, 6.866340149004786e-06, 6.866340149004786e-06, 9.809057355721125e-06, 9.809057355721125e-06, 1.4012939079601606e-05, 1.4012939079601606e-05, 2.001848439943087e-05, 2.001848439943087e-05, 2.8597834856329814e-05, 2.8597834856329814e-05, 4.085404979475688e-05, 4.085404979475688e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 02:47:38,459] [INFO] [timer.py:260:stop] epoch=0/micro_step=37000/global_step=37000, RunningAvgSamplesPerSec=28.07547701994894, CurrSamplesPerSec=22.948494558744947, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [ 490/2809]  eta: 0:22:53  lr: 0.000041  min_lr: 0.000000  loss: 4.0315 (3.9704)  loss_scale: 65536.0000 (57861.2138)  weight_decay: 0.0500 (0.0500)  time: 0.6351  data: 0.2057  max mem: 15572
Epoch: [13]  [ 500/2809]  eta: 0:22:45  lr: 0.000041  min_lr: 0.000000  loss: 3.9310 (3.9691)  loss_scale: 65536.0000 (58014.4032)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1518  max mem: 15572
Epoch: [13]  [ 510/2809]  eta: 0:22:38  lr: 0.000041  min_lr: 0.000000  loss: 3.9368 (3.9681)  loss_scale: 65536.0000 (58161.5969)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1069  max mem: 15572
Epoch: [13]  [ 520/2809]  eta: 0:22:23  lr: 0.000041  min_lr: 0.000000  loss: 3.9717 (3.9709)  loss_scale: 65536.0000 (58303.1401)  weight_decay: 0.0500 (0.0500)  time: 0.4764  data: 0.0674  max mem: 15572
Epoch: [13]  [ 530/2809]  eta: 0:22:09  lr: 0.000041  min_lr: 0.000000  loss: 3.8115 (3.9663)  loss_scale: 65536.0000 (58439.3522)  weight_decay: 0.0500 (0.0500)  time: 0.4014  data: 0.0005  max mem: 15572
Epoch: [13]  [ 540/2809]  eta: 0:21:57  lr: 0.000041  min_lr: 0.000000  loss: 3.8115 (3.9692)  loss_scale: 65536.0000 (58570.5287)  weight_decay: 0.0500 (0.0500)  time: 0.4150  data: 0.0005  max mem: 15572
Epoch: [13]  [ 550/2809]  eta: 0:21:54  lr: 0.000041  min_lr: 0.000000  loss: 4.0347 (3.9706)  loss_scale: 65536.0000 (58696.9437)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0925  max mem: 15572
Epoch: [13]  [ 560/2809]  eta: 0:21:52  lr: 0.000041  min_lr: 0.000000  loss: 4.1906 (3.9730)  loss_scale: 65536.0000 (58818.8520)  weight_decay: 0.0500 (0.0500)  time: 0.6550  data: 0.1846  max mem: 15572
Epoch: [13]  [ 570/2809]  eta: 0:21:52  lr: 0.000041  min_lr: 0.000000  loss: 3.9076 (3.9706)  loss_scale: 65536.0000 (58936.4904)  weight_decay: 0.0500 (0.0500)  time: 0.7036  data: 0.2289  max mem: 15572
[2025-01-13 02:48:31,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:48:31,266] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:48:32,187] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37095
[2025-01-13 02:48:32,187] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:48:32,188] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 580/2809]  eta: 0:21:43  lr: 0.000041  min_lr: 0.000000  loss: 3.8396 (3.9684)  loss_scale: 65536.0000 (59275.6764)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1416  max mem: 15572
Epoch: [13]  [ 590/2809]  eta: 0:21:37  lr: 0.000041  min_lr: 0.000000  loss: 3.8784 (3.9694)  loss_scale: 65536.0000 (59381.6041)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0562  max mem: 15572
Epoch: [13]  [ 600/2809]  eta: 0:21:37  lr: 0.000041  min_lr: 0.000000  loss: 4.0858 (3.9711)  loss_scale: 65536.0000 (59484.0067)  weight_decay: 0.0500 (0.0500)  time: 0.6595  data: 0.1965  max mem: 15572
Epoch: [13]  [ 610/2809]  eta: 0:21:35  lr: 0.000041  min_lr: 0.000000  loss: 4.1228 (3.9713)  loss_scale: 65536.0000 (59583.0573)  weight_decay: 0.0500 (0.0500)  time: 0.7213  data: 0.2788  max mem: 15572
Epoch: [13]  [ 620/2809]  eta: 0:21:31  lr: 0.000041  min_lr: 0.000000  loss: 4.1397 (3.9741)  loss_scale: 65536.0000 (59678.9179)  weight_decay: 0.0500 (0.0500)  time: 0.6759  data: 0.2306  max mem: 15572
Epoch: [13]  [ 630/2809]  eta: 0:21:29  lr: 0.000041  min_lr: 0.000000  loss: 4.1849 (3.9752)  loss_scale: 65536.0000 (59771.7401)  weight_decay: 0.0500 (0.0500)  time: 0.6731  data: 0.2057  max mem: 15572
[2025-01-13 02:49:13,806] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37156
[2025-01-13 02:49:13,806] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 02:49:13,806] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 640/2809]  eta: 0:21:29  lr: 0.000041  min_lr: 0.000000  loss: 4.0637 (3.9774)  loss_scale: 65536.0000 (59759.4259)  weight_decay: 0.0500 (0.0500)  time: 0.7303  data: 0.2606  max mem: 15572
Epoch: [13]  [ 650/2809]  eta: 0:21:23  lr: 0.000041  min_lr: 0.000000  loss: 3.9826 (3.9764)  loss_scale: 32768.0000 (59344.8111)  weight_decay: 0.0500 (0.0500)  time: 0.6765  data: 0.2151  max mem: 15572
Epoch: [13]  [ 660/2809]  eta: 0:21:17  lr: 0.000041  min_lr: 0.000000  loss: 3.9826 (3.9777)  loss_scale: 32768.0000 (58942.7413)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1324  max mem: 15572
Epoch: [13]  [ 670/2809]  eta: 0:21:04  lr: 0.000041  min_lr: 0.000000  loss: 4.0560 (3.9775)  loss_scale: 32768.0000 (58552.6557)  weight_decay: 0.0500 (0.0500)  time: 0.4887  data: 0.0695  max mem: 15572
Epoch: [13]  [ 680/2809]  eta: 0:20:54  lr: 0.000041  min_lr: 0.000000  loss: 3.9044 (3.9763)  loss_scale: 32768.0000 (58174.0264)  weight_decay: 0.0500 (0.0500)  time: 0.4178  data: 0.0005  max mem: 15572
Epoch: [13]  [ 690/2809]  eta: 0:20:45  lr: 0.000041  min_lr: 0.000000  loss: 3.7691 (3.9762)  loss_scale: 32768.0000 (57806.3560)  weight_decay: 0.0500 (0.0500)  time: 0.4698  data: 0.0442  max mem: 15572
Epoch: [13]  [ 700/2809]  eta: 0:20:37  lr: 0.000041  min_lr: 0.000000  loss: 4.1059 (3.9788)  loss_scale: 32768.0000 (57449.1755)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0837  max mem: 15572
Epoch: [13]  [ 710/2809]  eta: 0:20:32  lr: 0.000041  min_lr: 0.000000  loss: 4.0726 (3.9783)  loss_scale: 32768.0000 (57102.0422)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1203  max mem: 15572
Epoch: [13]  [ 720/2809]  eta: 0:20:29  lr: 0.000041  min_lr: 0.000000  loss: 3.8895 (3.9768)  loss_scale: 32768.0000 (56764.5381)  weight_decay: 0.0500 (0.0500)  time: 0.6545  data: 0.2128  max mem: 15572
Epoch: [13]  [ 730/2809]  eta: 0:20:21  lr: 0.000041  min_lr: 0.000000  loss: 3.7823 (3.9750)  loss_scale: 32768.0000 (56436.2681)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1888  max mem: 15572
Epoch: [13]  [ 740/2809]  eta: 0:20:13  lr: 0.000041  min_lr: 0.000000  loss: 3.7823 (3.9754)  loss_scale: 32768.0000 (56116.8583)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.1004  max mem: 15572
Epoch: [13]  [ 750/2809]  eta: 0:20:06  lr: 0.000041  min_lr: 0.000000  loss: 4.2335 (3.9802)  loss_scale: 32768.0000 (55805.9547)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.1009  max mem: 15572
Epoch: [13]  [ 760/2809]  eta: 0:20:02  lr: 0.000041  min_lr: 0.000000  loss: 4.3015 (3.9801)  loss_scale: 32768.0000 (55503.2221)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1617  max mem: 15572
[2025-01-13 02:50:23,130] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:50:23,131] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 770/2809]  eta: 0:19:56  lr: 0.000041  min_lr: 0.000000  loss: 4.0633 (3.9817)  loss_scale: 32768.0000 (55335.8444)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1872  max mem: 15572
Epoch: [13]  [ 780/2809]  eta: 0:19:52  lr: 0.000041  min_lr: 0.000000  loss: 4.0968 (3.9842)  loss_scale: 65536.0000 (55466.4481)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1991  max mem: 15572
Epoch: [13]  [ 790/2809]  eta: 0:19:42  lr: 0.000041  min_lr: 0.000000  loss: 4.0607 (3.9847)  loss_scale: 65536.0000 (55593.7497)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1170  max mem: 15572
Epoch: [13]  [ 800/2809]  eta: 0:19:36  lr: 0.000041  min_lr: 0.000000  loss: 4.1195 (3.9873)  loss_scale: 65536.0000 (55717.8727)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0758  max mem: 15572
Epoch: [13]  [ 810/2809]  eta: 0:19:29  lr: 0.000041  min_lr: 0.000000  loss: 4.2203 (3.9893)  loss_scale: 65536.0000 (55838.9346)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1217  max mem: 15572
Epoch: [13]  [ 820/2809]  eta: 0:19:19  lr: 0.000041  min_lr: 0.000000  loss: 4.2203 (3.9907)  loss_scale: 65536.0000 (55957.0475)  weight_decay: 0.0500 (0.0500)  time: 0.4644  data: 0.0510  max mem: 15572
Epoch: [13]  [ 830/2809]  eta: 0:19:08  lr: 0.000041  min_lr: 0.000000  loss: 4.2442 (3.9919)  loss_scale: 65536.0000 (56072.3177)  weight_decay: 0.0500 (0.0500)  time: 0.4024  data: 0.0049  max mem: 15572
Epoch: [13]  [ 840/2809]  eta: 0:19:02  lr: 0.000041  min_lr: 0.000000  loss: 4.2576 (3.9935)  loss_scale: 65536.0000 (56184.8466)  weight_decay: 0.0500 (0.0500)  time: 0.4621  data: 0.0414  max mem: 15572
Epoch: [13]  [ 850/2809]  eta: 0:18:55  lr: 0.000041  min_lr: 0.000000  loss: 4.0155 (3.9919)  loss_scale: 65536.0000 (56294.7309)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0856  max mem: 15572
Epoch: [13]  [ 860/2809]  eta: 0:18:50  lr: 0.000041  min_lr: 0.000000  loss: 3.8368 (3.9895)  loss_scale: 65536.0000 (56402.0627)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1430  max mem: 15572
Epoch: [13]  [ 870/2809]  eta: 0:18:47  lr: 0.000041  min_lr: 0.000000  loss: 3.8368 (3.9903)  loss_scale: 65536.0000 (56506.9300)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.2140  max mem: 15572
Epoch: [13]  [ 880/2809]  eta: 0:18:45  lr: 0.000041  min_lr: 0.000000  loss: 4.1643 (3.9922)  loss_scale: 65536.0000 (56609.4166)  weight_decay: 0.0500 (0.0500)  time: 0.7131  data: 0.2521  max mem: 15572
Epoch: [13]  [ 890/2809]  eta: 0:18:41  lr: 0.000041  min_lr: 0.000000  loss: 3.9790 (3.9913)  loss_scale: 65536.0000 (56709.6027)  weight_decay: 0.0500 (0.0500)  time: 0.7114  data: 0.2507  max mem: 15572
[2025-01-13 02:51:36,765] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:51:36,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:51:37,612] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37415
[2025-01-13 02:51:37,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:51:37,613] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 900/2809]  eta: 0:18:35  lr: 0.000041  min_lr: 0.000000  loss: 3.8325 (3.9908)  loss_scale: 65536.0000 (56953.0388)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.1828  max mem: 15572
Epoch: [13]  [ 910/2809]  eta: 0:18:30  lr: 0.000041  min_lr: 0.000000  loss: 3.9747 (3.9920)  loss_scale: 65536.0000 (57047.2536)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1460  max mem: 15572
Epoch: [13]  [ 920/2809]  eta: 0:18:26  lr: 0.000041  min_lr: 0.000000  loss: 3.9353 (3.9903)  loss_scale: 65536.0000 (57139.4224)  weight_decay: 0.0500 (0.0500)  time: 0.6591  data: 0.1898  max mem: 15572
Epoch: [13]  [ 930/2809]  eta: 0:18:22  lr: 0.000041  min_lr: 0.000000  loss: 3.9431 (3.9911)  loss_scale: 65536.0000 (57229.6112)  weight_decay: 0.0500 (0.0500)  time: 0.6843  data: 0.2162  max mem: 15572
Epoch: [13]  [ 940/2809]  eta: 0:18:18  lr: 0.000041  min_lr: 0.000000  loss: 3.9894 (3.9895)  loss_scale: 65536.0000 (57317.8831)  weight_decay: 0.0500 (0.0500)  time: 0.6824  data: 0.2259  max mem: 15572
Epoch: [13]  [ 950/2809]  eta: 0:18:15  lr: 0.000041  min_lr: 0.000000  loss: 3.9458 (3.9900)  loss_scale: 65536.0000 (57404.2986)  weight_decay: 0.0500 (0.0500)  time: 0.7011  data: 0.2213  max mem: 15572
Epoch: [13]  [ 960/2809]  eta: 0:18:09  lr: 0.000041  min_lr: 0.000000  loss: 3.8615 (3.9868)  loss_scale: 65536.0000 (57488.9157)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1643  max mem: 15572
Epoch: [13]  [ 970/2809]  eta: 0:17:59  lr: 0.000041  min_lr: 0.000000  loss: 3.8345 (3.9864)  loss_scale: 65536.0000 (57571.7899)  weight_decay: 0.0500 (0.0500)  time: 0.4776  data: 0.0656  max mem: 15572
Epoch: [13]  [ 980/2809]  eta: 0:17:50  lr: 0.000041  min_lr: 0.000000  loss: 3.9705 (3.9840)  loss_scale: 65536.0000 (57652.9745)  weight_decay: 0.0500 (0.0500)  time: 0.4077  data: 0.0004  max mem: 15572
Epoch: [13]  [ 990/2809]  eta: 0:17:44  lr: 0.000041  min_lr: 0.000000  loss: 3.9969 (3.9844)  loss_scale: 65536.0000 (57732.5207)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0657  max mem: 15572
Epoch: [13]  [1000/2809]  eta: 0:17:37  lr: 0.000041  min_lr: 0.000000  loss: 4.0523 (3.9829)  loss_scale: 65536.0000 (57810.4775)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.1079  max mem: 15572
Epoch: [13]  [1010/2809]  eta: 0:17:31  lr: 0.000041  min_lr: 0.000000  loss: 3.8272 (3.9822)  loss_scale: 65536.0000 (57886.8922)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.1152  max mem: 15572
Epoch: [13]  [1020/2809]  eta: 0:17:25  lr: 0.000041  min_lr: 0.000000  loss: 3.8884 (3.9822)  loss_scale: 65536.0000 (57961.8100)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1351  max mem: 15572
[2025-01-13 02:52:54,523] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:52:54,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [1030/2809]  eta: 0:17:19  lr: 0.000041  min_lr: 0.000000  loss: 4.1832 (3.9831)  loss_scale: 65536.0000 (58289.5364)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1392  max mem: 15572
[2025-01-13 02:52:59,563] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37553
[2025-01-13 02:52:59,564] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:52:59,564] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1040/2809]  eta: 0:17:12  lr: 0.000041  min_lr: 0.000000  loss: 3.9023 (3.9820)  loss_scale: 65536.0000 (58673.9212)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1288  max mem: 15572
Epoch: [13]  [1050/2809]  eta: 0:17:07  lr: 0.000041  min_lr: 0.000000  loss: 3.9023 (3.9825)  loss_scale: 65536.0000 (58739.2122)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1331  max mem: 15572
Epoch: [13]  [1060/2809]  eta: 0:17:01  lr: 0.000041  min_lr: 0.000000  loss: 4.0377 (3.9814)  loss_scale: 65536.0000 (58803.2724)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1420  max mem: 15572
Epoch: [13]  [1070/2809]  eta: 0:16:55  lr: 0.000041  min_lr: 0.000000  loss: 3.8096 (3.9802)  loss_scale: 65536.0000 (58866.1363)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1179  max mem: 15572
Epoch: [13]  [1080/2809]  eta: 0:16:48  lr: 0.000041  min_lr: 0.000000  loss: 3.8608 (3.9793)  loss_scale: 65536.0000 (58927.8372)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0981  max mem: 15572
Epoch: [13]  [1090/2809]  eta: 0:16:42  lr: 0.000041  min_lr: 0.000000  loss: 4.0160 (3.9800)  loss_scale: 65536.0000 (58988.4070)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1211  max mem: 15572
Epoch: [13]  [1100/2809]  eta: 0:16:34  lr: 0.000041  min_lr: 0.000000  loss: 3.9084 (3.9786)  loss_scale: 65536.0000 (59047.8765)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0941  max mem: 15572
Epoch: [13]  [1110/2809]  eta: 0:16:27  lr: 0.000041  min_lr: 0.000000  loss: 3.9153 (3.9781)  loss_scale: 65536.0000 (59106.2754)  weight_decay: 0.0500 (0.0500)  time: 0.4843  data: 0.0764  max mem: 15572
Epoch: [13]  [1120/2809]  eta: 0:16:19  lr: 0.000041  min_lr: 0.000000  loss: 4.1715 (3.9800)  loss_scale: 65536.0000 (59163.6325)  weight_decay: 0.0500 (0.0500)  time: 0.4539  data: 0.0626  max mem: 15572
Epoch: [13]  [1130/2809]  eta: 0:16:11  lr: 0.000041  min_lr: 0.000000  loss: 4.2939 (3.9808)  loss_scale: 65536.0000 (59219.9752)  weight_decay: 0.0500 (0.0500)  time: 0.4124  data: 0.0005  max mem: 15572
Epoch: [13]  [1140/2809]  eta: 0:16:03  lr: 0.000041  min_lr: 0.000000  loss: 4.2182 (3.9828)  loss_scale: 65536.0000 (59275.3304)  weight_decay: 0.0500 (0.0500)  time: 0.4490  data: 0.0006  max mem: 15572
Epoch: [13]  [1150/2809]  eta: 0:15:58  lr: 0.000041  min_lr: 0.000000  loss: 4.0483 (3.9816)  loss_scale: 65536.0000 (59329.7237)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0650  max mem: 15572
Epoch: [13]  [1160/2809]  eta: 0:15:54  lr: 0.000041  min_lr: 0.000000  loss: 3.8468 (3.9822)  loss_scale: 65536.0000 (59383.1800)  weight_decay: 0.0500 (0.0500)  time: 0.6488  data: 0.1838  max mem: 15572
[2025-01-13 02:54:08,196] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:54:08,197] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [1170/2809]  eta: 0:15:48  lr: 0.000041  min_lr: 0.000000  loss: 3.9385 (3.9819)  loss_scale: 65536.0000 (59771.5184)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.1656  max mem: 15572
[2025-01-13 02:54:14,891] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37690
[2025-01-13 02:54:14,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:54:14,891] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1180/2809]  eta: 0:15:44  lr: 0.000041  min_lr: 0.000000  loss: 4.0173 (3.9832)  loss_scale: 65536.0000 (59931.3124)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.1929  max mem: 15572
Epoch: [13]  [1190/2809]  eta: 0:15:40  lr: 0.000040  min_lr: 0.000000  loss: 4.2108 (3.9859)  loss_scale: 65536.0000 (59978.3711)  weight_decay: 0.0500 (0.0500)  time: 0.7077  data: 0.2619  max mem: 15572
Epoch: [13]  [1200/2809]  eta: 0:15:35  lr: 0.000040  min_lr: 0.000000  loss: 4.2576 (3.9870)  loss_scale: 65536.0000 (60024.6461)  weight_decay: 0.0500 (0.0500)  time: 0.6641  data: 0.2113  max mem: 15572
Epoch: [13]  [1210/2809]  eta: 0:15:32  lr: 0.000040  min_lr: 0.000000  loss: 4.1187 (3.9871)  loss_scale: 65536.0000 (60070.1569)  weight_decay: 0.0500 (0.0500)  time: 0.7203  data: 0.2553  max mem: 15572
Epoch: [13]  [1220/2809]  eta: 0:15:26  lr: 0.000040  min_lr: 0.000000  loss: 3.9108 (3.9854)  loss_scale: 65536.0000 (60114.9222)  weight_decay: 0.0500 (0.0500)  time: 0.6776  data: 0.1925  max mem: 15572
Epoch: [13]  [1230/2809]  eta: 0:15:21  lr: 0.000040  min_lr: 0.000000  loss: 3.6873 (3.9817)  loss_scale: 65536.0000 (60158.9602)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.1552  max mem: 15572
Epoch: [13]  [1240/2809]  eta: 0:15:17  lr: 0.000040  min_lr: 0.000000  loss: 3.6873 (3.9808)  loss_scale: 65536.0000 (60202.2885)  weight_decay: 0.0500 (0.0500)  time: 0.6924  data: 0.2277  max mem: 15572
Epoch: [13]  [1250/2809]  eta: 0:15:11  lr: 0.000040  min_lr: 0.000000  loss: 4.0483 (3.9828)  loss_scale: 65536.0000 (60244.9241)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1745  max mem: 15572
Epoch: [13]  [1260/2809]  eta: 0:15:03  lr: 0.000040  min_lr: 0.000000  loss: 4.1081 (3.9836)  loss_scale: 65536.0000 (60286.8834)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0945  max mem: 15572
Epoch: [13]  [1270/2809]  eta: 0:14:56  lr: 0.000040  min_lr: 0.000000  loss: 3.7422 (3.9819)  loss_scale: 65536.0000 (60328.1825)  weight_decay: 0.0500 (0.0500)  time: 0.4416  data: 0.0258  max mem: 15572
Epoch: [13]  [1280/2809]  eta: 0:14:48  lr: 0.000040  min_lr: 0.000000  loss: 4.0348 (3.9850)  loss_scale: 65536.0000 (60368.8368)  weight_decay: 0.0500 (0.0500)  time: 0.4526  data: 0.0143  max mem: 15572
Epoch: [13]  [1290/2809]  eta: 0:14:43  lr: 0.000040  min_lr: 0.000000  loss: 4.2208 (3.9832)  loss_scale: 65536.0000 (60408.8613)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.1034  max mem: 15572
Epoch: [13]  [1300/2809]  eta: 0:14:36  lr: 0.000040  min_lr: 0.000000  loss: 3.9816 (3.9833)  loss_scale: 65536.0000 (60448.2706)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1275  max mem: 15572
[2025-01-13 02:55:32,535] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:55:32,535] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:55:32,977] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37820
[2025-01-13 02:55:32,977] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:55:32,977] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1310/2809]  eta: 0:14:31  lr: 0.000040  min_lr: 0.000000  loss: 3.9135 (3.9821)  loss_scale: 65536.0000 (60537.0679)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1458  max mem: 15572
Epoch: [13]  [1320/2809]  eta: 0:14:26  lr: 0.000040  min_lr: 0.000000  loss: 3.7967 (3.9810)  loss_scale: 65536.0000 (60574.9099)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1958  max mem: 15572
Epoch: [13]  [1330/2809]  eta: 0:14:20  lr: 0.000040  min_lr: 0.000000  loss: 3.9796 (3.9819)  loss_scale: 65536.0000 (60612.1833)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1571  max mem: 15572
Epoch: [13]  [1340/2809]  eta: 0:14:13  lr: 0.000040  min_lr: 0.000000  loss: 4.1435 (3.9832)  loss_scale: 65536.0000 (60648.9008)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1048  max mem: 15572
Epoch: [13]  [1350/2809]  eta: 0:14:07  lr: 0.000040  min_lr: 0.000000  loss: 4.1440 (3.9831)  loss_scale: 65536.0000 (60685.0748)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.1128  max mem: 15572
Epoch: [13]  [1360/2809]  eta: 0:14:01  lr: 0.000040  min_lr: 0.000000  loss: 4.0640 (3.9843)  loss_scale: 65536.0000 (60720.7171)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1481  max mem: 15572
Epoch: [13]  [1370/2809]  eta: 0:13:55  lr: 0.000040  min_lr: 0.000000  loss: 4.2400 (3.9857)  loss_scale: 65536.0000 (60755.8395)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1049  max mem: 15572
Epoch: [13]  [1380/2809]  eta: 0:13:50  lr: 0.000040  min_lr: 0.000000  loss: 4.1090 (3.9859)  loss_scale: 65536.0000 (60790.4533)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1253  max mem: 15572
Epoch: [13]  [1390/2809]  eta: 0:13:43  lr: 0.000040  min_lr: 0.000000  loss: 4.0647 (3.9856)  loss_scale: 65536.0000 (60824.5694)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1223  max mem: 15572
Epoch: [13]  [1400/2809]  eta: 0:13:35  lr: 0.000040  min_lr: 0.000000  loss: 4.0121 (3.9844)  loss_scale: 65536.0000 (60858.1984)  weight_decay: 0.0500 (0.0500)  time: 0.4406  data: 0.0312  max mem: 15572
Epoch: [13]  [1410/2809]  eta: 0:13:28  lr: 0.000040  min_lr: 0.000000  loss: 4.0276 (3.9849)  loss_scale: 65536.0000 (60891.3508)  weight_decay: 0.0500 (0.0500)  time: 0.4202  data: 0.0004  max mem: 15572
Epoch: [13]  [1420/2809]  eta: 0:13:21  lr: 0.000040  min_lr: 0.000000  loss: 3.9806 (3.9847)  loss_scale: 65536.0000 (60924.0366)  weight_decay: 0.0500 (0.0500)  time: 0.4607  data: 0.0040  max mem: 15572
Epoch: [13]  [1430/2809]  eta: 0:13:17  lr: 0.000040  min_lr: 0.000000  loss: 3.9285 (3.9837)  loss_scale: 65536.0000 (60956.2655)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1069  max mem: 15572
[2025-01-13 02:56:43,466] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:56:43,467] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:56:43,924] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37950
[2025-01-13 02:56:43,925] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:56:43,928] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1440/2809]  eta: 0:13:11  lr: 0.000040  min_lr: 0.000000  loss: 3.9086 (3.9825)  loss_scale: 65536.0000 (61033.5267)  weight_decay: 0.0500 (0.0500)  time: 0.6691  data: 0.1926  max mem: 15572
Epoch: [13]  [1450/2809]  eta: 0:13:06  lr: 0.000040  min_lr: 0.000000  loss: 3.9393 (3.9830)  loss_scale: 65536.0000 (61064.5569)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.1822  max mem: 15572
Epoch: [13]  [1460/2809]  eta: 0:13:01  lr: 0.000040  min_lr: 0.000000  loss: 4.0337 (3.9837)  loss_scale: 65536.0000 (61095.1622)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.1642  max mem: 15572
Epoch: [13]  [1470/2809]  eta: 0:12:56  lr: 0.000040  min_lr: 0.000000  loss: 4.0263 (3.9837)  loss_scale: 65536.0000 (61125.3515)  weight_decay: 0.0500 (0.0500)  time: 0.6523  data: 0.1769  max mem: 15572
Epoch: [13]  [1480/2809]  eta: 0:12:51  lr: 0.000040  min_lr: 0.000000  loss: 4.0263 (3.9844)  loss_scale: 65536.0000 (61155.1330)  weight_decay: 0.0500 (0.0500)  time: 0.6775  data: 0.2258  max mem: 15572
[2025-01-13 02:57:17,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=38000, skipped=252, lr=[3.9089269712705475e-07, 3.9089269712705475e-07, 5.584181387529355e-07, 5.584181387529355e-07, 7.977401982184794e-07, 7.977401982184794e-07, 1.1396288545978277e-06, 1.1396288545978277e-06, 1.6280412208540395e-06, 1.6280412208540395e-06, 2.325773172648628e-06, 2.325773172648628e-06, 3.3225331037837544e-06, 3.3225331037837544e-06, 4.746475862548221e-06, 4.746475862548221e-06, 6.780679803640316e-06, 6.780679803640316e-06, 9.686685433771882e-06, 9.686685433771882e-06, 1.3838122048245544e-05, 1.3838122048245544e-05, 1.9768745783207923e-05, 1.9768745783207923e-05, 2.8241065404582747e-05, 2.8241065404582747e-05, 4.034437914940393e-05, 4.034437914940393e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 02:57:17,778] [INFO] [timer.py:260:stop] epoch=0/micro_step=38000/global_step=38000, RunningAvgSamplesPerSec=28.090214253603843, CurrSamplesPerSec=25.52709622938639, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [1490/2809]  eta: 0:12:47  lr: 0.000040  min_lr: 0.000000  loss: 3.9613 (3.9833)  loss_scale: 65536.0000 (61184.5151)  weight_decay: 0.0500 (0.0500)  time: 0.7391  data: 0.2883  max mem: 15572
Epoch: [13]  [1500/2809]  eta: 0:12:42  lr: 0.000040  min_lr: 0.000000  loss: 3.9177 (3.9828)  loss_scale: 65536.0000 (61213.5057)  weight_decay: 0.0500 (0.0500)  time: 0.7273  data: 0.2715  max mem: 15572
Epoch: [13]  [1510/2809]  eta: 0:12:37  lr: 0.000040  min_lr: 0.000000  loss: 4.0256 (3.9832)  loss_scale: 65536.0000 (61242.1125)  weight_decay: 0.0500 (0.0500)  time: 0.6637  data: 0.2105  max mem: 15572
Epoch: [13]  [1520/2809]  eta: 0:12:33  lr: 0.000040  min_lr: 0.000000  loss: 4.1686 (3.9832)  loss_scale: 65536.0000 (61270.3432)  weight_decay: 0.0500 (0.0500)  time: 0.7417  data: 0.2810  max mem: 15572
Epoch: [13]  [1530/2809]  eta: 0:12:28  lr: 0.000040  min_lr: 0.000000  loss: 3.8770 (3.9829)  loss_scale: 65536.0000 (61298.2051)  weight_decay: 0.0500 (0.0500)  time: 0.7175  data: 0.2830  max mem: 15572
Epoch: [13]  [1540/2809]  eta: 0:12:20  lr: 0.000040  min_lr: 0.000000  loss: 3.9370 (3.9834)  loss_scale: 65536.0000 (61325.7054)  weight_decay: 0.0500 (0.0500)  time: 0.5184  data: 0.1092  max mem: 15572
Epoch: [13]  [1550/2809]  eta: 0:12:14  lr: 0.000040  min_lr: 0.000000  loss: 3.9370 (3.9829)  loss_scale: 65536.0000 (61352.8511)  weight_decay: 0.0500 (0.0500)  time: 0.4847  data: 0.0451  max mem: 15572
Epoch: [13]  [1560/2809]  eta: 0:12:08  lr: 0.000040  min_lr: 0.000000  loss: 3.9298 (3.9823)  loss_scale: 65536.0000 (61379.6489)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0857  max mem: 15572
[2025-01-13 02:58:06,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:58:06,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:58:08,651] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38084
[2025-01-13 02:58:08,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:58:08,651] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1570/2809]  eta: 0:12:02  lr: 0.000040  min_lr: 0.000000  loss: 4.0175 (3.9822)  loss_scale: 65536.0000 (61614.6862)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1092  max mem: 15572
Epoch: [13]  [1580/2809]  eta: 0:11:56  lr: 0.000040  min_lr: 0.000000  loss: 4.0175 (3.9833)  loss_scale: 65536.0000 (61639.4889)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1186  max mem: 15572
Epoch: [13]  [1590/2809]  eta: 0:11:50  lr: 0.000040  min_lr: 0.000000  loss: 4.0293 (3.9837)  loss_scale: 65536.0000 (61663.9799)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0978  max mem: 15572
Epoch: [13]  [1600/2809]  eta: 0:11:44  lr: 0.000040  min_lr: 0.000000  loss: 4.0445 (3.9844)  loss_scale: 65536.0000 (61688.1649)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0886  max mem: 15572
Epoch: [13]  [1610/2809]  eta: 0:11:38  lr: 0.000040  min_lr: 0.000000  loss: 4.1504 (3.9850)  loss_scale: 65536.0000 (61712.0497)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.1094  max mem: 15572
Epoch: [13]  [1620/2809]  eta: 0:11:32  lr: 0.000040  min_lr: 0.000000  loss: 4.0862 (3.9850)  loss_scale: 65536.0000 (61735.6397)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1246  max mem: 15572
Epoch: [13]  [1630/2809]  eta: 0:11:26  lr: 0.000040  min_lr: 0.000000  loss: 4.0806 (3.9853)  loss_scale: 65536.0000 (61758.9405)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1300  max mem: 15572
Epoch: [13]  [1640/2809]  eta: 0:11:20  lr: 0.000040  min_lr: 0.000000  loss: 4.0573 (3.9858)  loss_scale: 65536.0000 (61781.9573)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1799  max mem: 15572
Epoch: [13]  [1650/2809]  eta: 0:11:14  lr: 0.000040  min_lr: 0.000000  loss: 4.1073 (3.9866)  loss_scale: 65536.0000 (61804.6953)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1490  max mem: 15572
Epoch: [13]  [1660/2809]  eta: 0:11:06  lr: 0.000040  min_lr: 0.000000  loss: 3.9983 (3.9851)  loss_scale: 65536.0000 (61827.1595)  weight_decay: 0.0500 (0.0500)  time: 0.4233  data: 0.0430  max mem: 15572
Epoch: [13]  [1670/2809]  eta: 0:11:00  lr: 0.000040  min_lr: 0.000000  loss: 3.9613 (3.9848)  loss_scale: 65536.0000 (61849.3549)  weight_decay: 0.0500 (0.0500)  time: 0.4156  data: 0.0005  max mem: 15572
Epoch: [13]  [1680/2809]  eta: 0:10:53  lr: 0.000040  min_lr: 0.000000  loss: 4.0791 (3.9852)  loss_scale: 65536.0000 (61871.2861)  weight_decay: 0.0500 (0.0500)  time: 0.4530  data: 0.0008  max mem: 15572
Epoch: [13]  [1690/2809]  eta: 0:10:47  lr: 0.000040  min_lr: 0.000000  loss: 4.1905 (3.9857)  loss_scale: 65536.0000 (61892.9580)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0832  max mem: 15572
[2025-01-13 02:59:17,389] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:59:17,389] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [1700/2809]  eta: 0:10:42  lr: 0.000040  min_lr: 0.000000  loss: 4.1339 (3.9864)  loss_scale: 65536.0000 (62107.0147)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.1830  max mem: 15572
[2025-01-13 02:59:19,838] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38218
[2025-01-13 02:59:19,838] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:59:19,838] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1710/2809]  eta: 0:10:37  lr: 0.000040  min_lr: 0.000000  loss: 4.1339 (3.9865)  loss_scale: 65536.0000 (62127.0555)  weight_decay: 0.0500 (0.0500)  time: 0.6876  data: 0.2398  max mem: 15572
Epoch: [13]  [1720/2809]  eta: 0:10:32  lr: 0.000040  min_lr: 0.000000  loss: 4.1311 (3.9877)  loss_scale: 65536.0000 (62146.8635)  weight_decay: 0.0500 (0.0500)  time: 0.7235  data: 0.2537  max mem: 15572
Epoch: [13]  [1730/2809]  eta: 0:10:27  lr: 0.000040  min_lr: 0.000000  loss: 4.0635 (3.9886)  loss_scale: 65536.0000 (62166.4425)  weight_decay: 0.0500 (0.0500)  time: 0.7196  data: 0.2523  max mem: 15572
Epoch: [13]  [1740/2809]  eta: 0:10:22  lr: 0.000040  min_lr: 0.000000  loss: 4.0184 (3.9893)  loss_scale: 65536.0000 (62185.7967)  weight_decay: 0.0500 (0.0500)  time: 0.6618  data: 0.2191  max mem: 15572
Epoch: [13]  [1750/2809]  eta: 0:10:16  lr: 0.000040  min_lr: 0.000000  loss: 4.0019 (3.9893)  loss_scale: 65536.0000 (62204.9298)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1392  max mem: 15572
Epoch: [13]  [1760/2809]  eta: 0:10:11  lr: 0.000040  min_lr: 0.000000  loss: 4.0293 (3.9902)  loss_scale: 65536.0000 (62223.8455)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.1906  max mem: 15572
Epoch: [13]  [1770/2809]  eta: 0:10:06  lr: 0.000040  min_lr: 0.000000  loss: 4.1685 (3.9910)  loss_scale: 65536.0000 (62242.5477)  weight_decay: 0.0500 (0.0500)  time: 0.7211  data: 0.2663  max mem: 15572
Epoch: [13]  [1780/2809]  eta: 0:10:01  lr: 0.000040  min_lr: 0.000000  loss: 4.0347 (3.9897)  loss_scale: 65536.0000 (62261.0399)  weight_decay: 0.0500 (0.0500)  time: 0.7040  data: 0.2535  max mem: 15572
Epoch: [13]  [1790/2809]  eta: 0:09:55  lr: 0.000040  min_lr: 0.000000  loss: 3.9008 (3.9890)  loss_scale: 65536.0000 (62279.3255)  weight_decay: 0.0500 (0.0500)  time: 0.6590  data: 0.2188  max mem: 15572
Epoch: [13]  [1800/2809]  eta: 0:09:48  lr: 0.000040  min_lr: 0.000000  loss: 3.9008 (3.9889)  loss_scale: 65536.0000 (62297.4081)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.1039  max mem: 15572
Epoch: [13]  [1810/2809]  eta: 0:09:42  lr: 0.000040  min_lr: 0.000000  loss: 3.7516 (3.9879)  loss_scale: 65536.0000 (62315.2910)  weight_decay: 0.0500 (0.0500)  time: 0.4214  data: 0.0051  max mem: 15572
Epoch: [13]  [1820/2809]  eta: 0:09:35  lr: 0.000040  min_lr: 0.000000  loss: 3.8430 (3.9873)  loss_scale: 65536.0000 (62332.9775)  weight_decay: 0.0500 (0.0500)  time: 0.4245  data: 0.0041  max mem: 15572
[2025-01-13 03:00:39,525] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:00:39,526] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [1830/2809]  eta: 0:09:29  lr: 0.000040  min_lr: 0.000000  loss: 3.9564 (3.9874)  loss_scale: 65536.0000 (62386.2632)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0818  max mem: 15572
[2025-01-13 03:00:39,926] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38348
[2025-01-13 03:00:39,926] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:00:39,927] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1840/2809]  eta: 0:09:23  lr: 0.000040  min_lr: 0.000000  loss: 4.0166 (3.9878)  loss_scale: 65536.0000 (62403.3721)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1218  max mem: 15572
Epoch: [13]  [1850/2809]  eta: 0:09:18  lr: 0.000040  min_lr: 0.000000  loss: 3.9052 (3.9873)  loss_scale: 65536.0000 (62420.2961)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1645  max mem: 15572
Epoch: [13]  [1860/2809]  eta: 0:09:12  lr: 0.000040  min_lr: 0.000000  loss: 4.0286 (3.9874)  loss_scale: 65536.0000 (62437.0382)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1644  max mem: 15572
Epoch: [13]  [1870/2809]  eta: 0:09:06  lr: 0.000040  min_lr: 0.000000  loss: 3.9893 (3.9872)  loss_scale: 65536.0000 (62453.6013)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1232  max mem: 15572
Epoch: [13]  [1880/2809]  eta: 0:09:00  lr: 0.000040  min_lr: 0.000000  loss: 3.9893 (3.9879)  loss_scale: 65536.0000 (62469.9883)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1640  max mem: 15572
Epoch: [13]  [1890/2809]  eta: 0:08:54  lr: 0.000040  min_lr: 0.000000  loss: 4.1187 (3.9882)  loss_scale: 65536.0000 (62486.2020)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.0910  max mem: 15572
Epoch: [13]  [1900/2809]  eta: 0:08:48  lr: 0.000040  min_lr: 0.000000  loss: 4.0984 (3.9893)  loss_scale: 65536.0000 (62502.2451)  weight_decay: 0.0500 (0.0500)  time: 0.5116  data: 0.0573  max mem: 15572
Epoch: [13]  [1910/2809]  eta: 0:08:42  lr: 0.000040  min_lr: 0.000000  loss: 4.0984 (3.9899)  loss_scale: 65536.0000 (62518.1204)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0610  max mem: 15572
Epoch: [13]  [1920/2809]  eta: 0:08:35  lr: 0.000040  min_lr: 0.000000  loss: 4.0740 (3.9903)  loss_scale: 65536.0000 (62533.8303)  weight_decay: 0.0500 (0.0500)  time: 0.4206  data: 0.0106  max mem: 15572
Epoch: [13]  [1930/2809]  eta: 0:08:29  lr: 0.000040  min_lr: 0.000000  loss: 3.9873 (3.9901)  loss_scale: 65536.0000 (62549.3775)  weight_decay: 0.0500 (0.0500)  time: 0.4219  data: 0.0006  max mem: 15572
Epoch: [13]  [1940/2809]  eta: 0:08:22  lr: 0.000040  min_lr: 0.000000  loss: 3.9310 (3.9893)  loss_scale: 65536.0000 (62564.7646)  weight_decay: 0.0500 (0.0500)  time: 0.4503  data: 0.0006  max mem: 15572
Epoch: [13]  [1950/2809]  eta: 0:08:17  lr: 0.000040  min_lr: 0.000000  loss: 3.8231 (3.9891)  loss_scale: 65536.0000 (62579.9938)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1013  max mem: 15572
[2025-01-13 03:01:48,801] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:01:48,801] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [1960/2809]  eta: 0:08:11  lr: 0.000040  min_lr: 0.000000  loss: 3.8348 (3.9889)  loss_scale: 65536.0000 (62628.4875)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1631  max mem: 15572
[2025-01-13 03:01:50,130] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38479
[2025-01-13 03:01:50,131] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:01:50,131] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1970/2809]  eta: 0:08:06  lr: 0.000040  min_lr: 0.000000  loss: 3.9633 (3.9890)  loss_scale: 65536.0000 (62676.4891)  weight_decay: 0.0500 (0.0500)  time: 0.6479  data: 0.1909  max mem: 15572
Epoch: [13]  [1980/2809]  eta: 0:08:01  lr: 0.000040  min_lr: 0.000000  loss: 4.0836 (3.9883)  loss_scale: 65536.0000 (62690.9238)  weight_decay: 0.0500 (0.0500)  time: 0.7546  data: 0.2965  max mem: 15572
Epoch: [13]  [1990/2809]  eta: 0:07:55  lr: 0.000040  min_lr: 0.000000  loss: 4.1989 (3.9896)  loss_scale: 65536.0000 (62705.2135)  weight_decay: 0.0500 (0.0500)  time: 0.7321  data: 0.2855  max mem: 15572
Epoch: [13]  [2000/2809]  eta: 0:07:50  lr: 0.000040  min_lr: 0.000000  loss: 4.4181 (3.9913)  loss_scale: 65536.0000 (62719.3603)  weight_decay: 0.0500 (0.0500)  time: 0.6824  data: 0.2224  max mem: 15572
Epoch: [13]  [2010/2809]  eta: 0:07:45  lr: 0.000040  min_lr: 0.000000  loss: 4.2612 (3.9909)  loss_scale: 65536.0000 (62733.3665)  weight_decay: 0.0500 (0.0500)  time: 0.7070  data: 0.2477  max mem: 15572
Epoch: [13]  [2020/2809]  eta: 0:07:39  lr: 0.000040  min_lr: 0.000000  loss: 3.8834 (3.9911)  loss_scale: 65536.0000 (62747.2340)  weight_decay: 0.0500 (0.0500)  time: 0.6775  data: 0.2188  max mem: 15572
Epoch: [13]  [2030/2809]  eta: 0:07:34  lr: 0.000040  min_lr: 0.000000  loss: 3.8788 (3.9909)  loss_scale: 65536.0000 (62760.9650)  weight_decay: 0.0500 (0.0500)  time: 0.6823  data: 0.2180  max mem: 15572
Epoch: [13]  [2040/2809]  eta: 0:07:28  lr: 0.000040  min_lr: 0.000000  loss: 3.8438 (3.9903)  loss_scale: 65536.0000 (62774.5615)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.1908  max mem: 15572
Epoch: [13]  [2050/2809]  eta: 0:07:22  lr: 0.000040  min_lr: 0.000000  loss: 3.9261 (3.9893)  loss_scale: 65536.0000 (62788.0254)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1114  max mem: 15572
Epoch: [13]  [2060/2809]  eta: 0:07:16  lr: 0.000040  min_lr: 0.000000  loss: 3.9628 (3.9898)  loss_scale: 65536.0000 (62801.3586)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0754  max mem: 15572
Epoch: [13]  [2070/2809]  eta: 0:07:09  lr: 0.000040  min_lr: 0.000000  loss: 3.9947 (3.9891)  loss_scale: 65536.0000 (62814.5630)  weight_decay: 0.0500 (0.0500)  time: 0.4288  data: 0.0126  max mem: 15572
Epoch: [13]  [2080/2809]  eta: 0:07:03  lr: 0.000040  min_lr: 0.000000  loss: 3.9053 (3.9883)  loss_scale: 65536.0000 (62827.6406)  weight_decay: 0.0500 (0.0500)  time: 0.4592  data: 0.0211  max mem: 15572
Epoch: [13]  [2090/2809]  eta: 0:06:57  lr: 0.000040  min_lr: 0.000000  loss: 3.9409 (3.9881)  loss_scale: 65536.0000 (62840.5930)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0943  max mem: 15572
[2025-01-13 03:03:09,855] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:03:09,855] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [2100/2809]  eta: 0:06:52  lr: 0.000040  min_lr: 0.000000  loss: 3.9409 (3.9883)  loss_scale: 65536.0000 (63165.3498)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.2044  max mem: 15572
[2025-01-13 03:03:16,952] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38618
[2025-01-13 03:03:16,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:03:16,953] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2110/2809]  eta: 0:06:47  lr: 0.000040  min_lr: 0.000000  loss: 3.9570 (3.9880)  loss_scale: 65536.0000 (63176.5798)  weight_decay: 0.0500 (0.0500)  time: 0.6863  data: 0.2414  max mem: 15572
Epoch: [13]  [2120/2809]  eta: 0:06:40  lr: 0.000040  min_lr: 0.000000  loss: 4.0021 (3.9880)  loss_scale: 65536.0000 (63187.7039)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1502  max mem: 15572
Epoch: [13]  [2130/2809]  eta: 0:06:34  lr: 0.000040  min_lr: 0.000000  loss: 3.9742 (3.9880)  loss_scale: 65536.0000 (63198.7236)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0745  max mem: 15572
Epoch: [13]  [2140/2809]  eta: 0:06:29  lr: 0.000040  min_lr: 0.000000  loss: 3.9532 (3.9878)  loss_scale: 65536.0000 (63209.6404)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1006  max mem: 15572
Epoch: [13]  [2150/2809]  eta: 0:06:23  lr: 0.000040  min_lr: 0.000000  loss: 3.9532 (3.9868)  loss_scale: 65536.0000 (63220.4556)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1034  max mem: 15572
Epoch: [13]  [2160/2809]  eta: 0:06:16  lr: 0.000040  min_lr: 0.000000  loss: 3.9333 (3.9866)  loss_scale: 65536.0000 (63231.1708)  weight_decay: 0.0500 (0.0500)  time: 0.4582  data: 0.0377  max mem: 15572
Epoch: [13]  [2170/2809]  eta: 0:06:10  lr: 0.000040  min_lr: 0.000000  loss: 4.0195 (3.9866)  loss_scale: 65536.0000 (63241.7872)  weight_decay: 0.0500 (0.0500)  time: 0.4136  data: 0.0004  max mem: 15572
Epoch: [13]  [2180/2809]  eta: 0:06:04  lr: 0.000040  min_lr: 0.000000  loss: 4.0195 (3.9861)  loss_scale: 65536.0000 (63252.3063)  weight_decay: 0.0500 (0.0500)  time: 0.4379  data: 0.0005  max mem: 15572
Epoch: [13]  [2190/2809]  eta: 0:05:58  lr: 0.000040  min_lr: 0.000000  loss: 4.0296 (3.9866)  loss_scale: 65536.0000 (63262.7293)  weight_decay: 0.0500 (0.0500)  time: 0.4482  data: 0.0005  max mem: 15572
Epoch: [13]  [2200/2809]  eta: 0:05:52  lr: 0.000040  min_lr: 0.000000  loss: 4.0296 (3.9860)  loss_scale: 65536.0000 (63273.0577)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1325  max mem: 15572
Epoch: [13]  [2210/2809]  eta: 0:05:46  lr: 0.000040  min_lr: 0.000000  loss: 3.8999 (3.9862)  loss_scale: 65536.0000 (63283.2926)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1893  max mem: 15572
Epoch: [13]  [2220/2809]  eta: 0:05:41  lr: 0.000040  min_lr: 0.000000  loss: 4.1642 (3.9873)  loss_scale: 65536.0000 (63293.4354)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1582  max mem: 15572
[2025-01-13 03:04:28,459] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:04:28,459] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [2230/2809]  eta: 0:05:35  lr: 0.000040  min_lr: 0.000000  loss: 4.1142 (3.9866)  loss_scale: 65536.0000 (63332.8624)  weight_decay: 0.0500 (0.0500)  time: 0.7120  data: 0.2656  max mem: 15572
[2025-01-13 03:04:30,035] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38749
[2025-01-13 03:04:30,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:04:30,036] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2240/2809]  eta: 0:05:30  lr: 0.000040  min_lr: 0.000000  loss: 4.0297 (3.9872)  loss_scale: 65536.0000 (63371.9375)  weight_decay: 0.0500 (0.0500)  time: 0.7631  data: 0.3222  max mem: 15572
Epoch: [13]  [2250/2809]  eta: 0:05:24  lr: 0.000040  min_lr: 0.000000  loss: 4.0297 (3.9869)  loss_scale: 65536.0000 (63381.5513)  weight_decay: 0.0500 (0.0500)  time: 0.6992  data: 0.2579  max mem: 15572
Epoch: [13]  [2260/2809]  eta: 0:05:19  lr: 0.000040  min_lr: 0.000000  loss: 3.8450 (3.9860)  loss_scale: 65536.0000 (63391.0801)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1791  max mem: 15572
Epoch: [13]  [2270/2809]  eta: 0:05:13  lr: 0.000040  min_lr: 0.000000  loss: 3.8735 (3.9857)  loss_scale: 65536.0000 (63400.5249)  weight_decay: 0.0500 (0.0500)  time: 0.6755  data: 0.2183  max mem: 15572
Epoch: [13]  [2280/2809]  eta: 0:05:08  lr: 0.000040  min_lr: 0.000000  loss: 4.2027 (3.9864)  loss_scale: 65536.0000 (63409.8869)  weight_decay: 0.0500 (0.0500)  time: 0.7090  data: 0.2409  max mem: 15572
Epoch: [13]  [2290/2809]  eta: 0:05:02  lr: 0.000040  min_lr: 0.000000  loss: 4.1879 (3.9862)  loss_scale: 65536.0000 (63419.1672)  weight_decay: 0.0500 (0.0500)  time: 0.6580  data: 0.1952  max mem: 15572
Epoch: [13]  [2300/2809]  eta: 0:04:56  lr: 0.000040  min_lr: 0.000000  loss: 3.8145 (3.9853)  loss_scale: 65536.0000 (63428.3668)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1853  max mem: 15572
Epoch: [13]  [2310/2809]  eta: 0:04:50  lr: 0.000040  min_lr: 0.000000  loss: 3.6177 (3.9840)  loss_scale: 65536.0000 (63437.4868)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.1200  max mem: 15572
Epoch: [13]  [2320/2809]  eta: 0:04:44  lr: 0.000040  min_lr: 0.000000  loss: 3.7842 (3.9841)  loss_scale: 65536.0000 (63446.5282)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0741  max mem: 15572
Epoch: [13]  [2330/2809]  eta: 0:04:38  lr: 0.000040  min_lr: 0.000000  loss: 3.9013 (3.9837)  loss_scale: 65536.0000 (63455.4921)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1137  max mem: 15572
Epoch: [13]  [2340/2809]  eta: 0:04:32  lr: 0.000040  min_lr: 0.000000  loss: 3.8941 (3.9832)  loss_scale: 65536.0000 (63464.3793)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1321  max mem: 15572
Epoch: [13]  [2350/2809]  eta: 0:04:27  lr: 0.000040  min_lr: 0.000000  loss: 3.8628 (3.9836)  loss_scale: 65536.0000 (63473.1910)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.1145  max mem: 15572
Epoch: [13]  [2360/2809]  eta: 0:04:21  lr: 0.000040  min_lr: 0.000000  loss: 4.1653 (3.9842)  loss_scale: 65536.0000 (63481.9280)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.1112  max mem: 15572
[2025-01-13 03:05:47,507] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:05:47,507] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:05:48,346] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38880
[2025-01-13 03:05:48,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:05:48,347] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2370/2809]  eta: 0:04:15  lr: 0.000040  min_lr: 0.000000  loss: 4.2364 (3.9847)  loss_scale: 65536.0000 (63545.8726)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1649  max mem: 15572
Epoch: [13]  [2380/2809]  eta: 0:04:09  lr: 0.000040  min_lr: 0.000000  loss: 4.0776 (3.9852)  loss_scale: 65536.0000 (63554.2310)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.1866  max mem: 15572
Epoch: [13]  [2390/2809]  eta: 0:04:03  lr: 0.000040  min_lr: 0.000000  loss: 4.0450 (3.9857)  loss_scale: 65536.0000 (63562.5194)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0960  max mem: 15572
Epoch: [13]  [2400/2809]  eta: 0:03:57  lr: 0.000040  min_lr: 0.000000  loss: 4.0783 (3.9860)  loss_scale: 65536.0000 (63570.7389)  weight_decay: 0.0500 (0.0500)  time: 0.4451  data: 0.0392  max mem: 15572
Epoch: [13]  [2410/2809]  eta: 0:03:51  lr: 0.000040  min_lr: 0.000000  loss: 4.0534 (3.9858)  loss_scale: 65536.0000 (63578.8901)  weight_decay: 0.0500 (0.0500)  time: 0.4205  data: 0.0259  max mem: 15572
Epoch: [13]  [2420/2809]  eta: 0:03:45  lr: 0.000040  min_lr: 0.000000  loss: 3.8928 (3.9858)  loss_scale: 65536.0000 (63586.9740)  weight_decay: 0.0500 (0.0500)  time: 0.4145  data: 0.0006  max mem: 15572
Epoch: [13]  [2430/2809]  eta: 0:03:39  lr: 0.000040  min_lr: 0.000000  loss: 3.8796 (3.9859)  loss_scale: 65536.0000 (63594.9914)  weight_decay: 0.0500 (0.0500)  time: 0.4524  data: 0.0007  max mem: 15572
Epoch: [13]  [2440/2809]  eta: 0:03:33  lr: 0.000040  min_lr: 0.000000  loss: 4.1101 (3.9861)  loss_scale: 65536.0000 (63602.9431)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0885  max mem: 15572
Epoch: [13]  [2450/2809]  eta: 0:03:28  lr: 0.000040  min_lr: 0.000000  loss: 4.0124 (3.9857)  loss_scale: 65536.0000 (63610.8299)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.1826  max mem: 15572
Epoch: [13]  [2460/2809]  eta: 0:03:22  lr: 0.000040  min_lr: 0.000000  loss: 3.9286 (3.9857)  loss_scale: 65536.0000 (63618.6526)  weight_decay: 0.0500 (0.0500)  time: 0.6650  data: 0.2171  max mem: 15572
Epoch: [13]  [2470/2809]  eta: 0:03:16  lr: 0.000040  min_lr: 0.000000  loss: 4.0074 (3.9858)  loss_scale: 65536.0000 (63626.4120)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.2142  max mem: 15572
Epoch: [13]  [2480/2809]  eta: 0:03:10  lr: 0.000040  min_lr: 0.000000  loss: 3.9555 (3.9858)  loss_scale: 65536.0000 (63634.1088)  weight_decay: 0.0500 (0.0500)  time: 0.6559  data: 0.1989  max mem: 15572
[2025-01-13 03:06:55,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=39000, skipped=259, lr=[3.8578753243206146e-07, 3.8578753243206146e-07, 5.511250463315164e-07, 5.511250463315164e-07, 7.873214947593093e-07, 7.873214947593093e-07, 1.1247449925132992e-06, 1.1247449925132992e-06, 1.6067785607332844e-06, 1.6067785607332844e-06, 2.295397943904692e-06, 2.295397943904692e-06, 3.2791399198638463e-06, 3.2791399198638463e-06, 4.684485599805495e-06, 4.684485599805495e-06, 6.692122285436421e-06, 6.692122285436421e-06, 9.560174693480603e-06, 9.560174693480603e-06, 1.3657392419258004e-05, 1.3657392419258004e-05, 1.951056059894001e-05, 1.951056059894001e-05, 2.7872229427057156e-05, 2.7872229427057156e-05, 3.9817470610081654e-05, 3.9817470610081654e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 03:06:55,186] [INFO] [timer.py:260:stop] epoch=0/micro_step=39000/global_step=39000, RunningAvgSamplesPerSec=28.107378958364208, CurrSamplesPerSec=25.196374604207602, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [2490/2809]  eta: 0:03:05  lr: 0.000040  min_lr: 0.000000  loss: 3.8230 (3.9856)  loss_scale: 65536.0000 (63641.7439)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.1887  max mem: 15572
[2025-01-13 03:07:01,388] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:07:01,388] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:07:03,976] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39012
[2025-01-13 03:07:03,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:07:03,976] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2500/2809]  eta: 0:02:59  lr: 0.000040  min_lr: 0.000000  loss: 3.8230 (3.9853)  loss_scale: 65536.0000 (63727.9296)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.1890  max mem: 15572
Epoch: [13]  [2510/2809]  eta: 0:02:53  lr: 0.000040  min_lr: 0.000000  loss: 4.0343 (3.9857)  loss_scale: 65536.0000 (63735.1302)  weight_decay: 0.0500 (0.0500)  time: 0.6853  data: 0.2472  max mem: 15572
Epoch: [13]  [2520/2809]  eta: 0:02:48  lr: 0.000040  min_lr: 0.000000  loss: 4.3446 (3.9863)  loss_scale: 65536.0000 (63742.2737)  weight_decay: 0.0500 (0.0500)  time: 0.6979  data: 0.2316  max mem: 15572
Epoch: [13]  [2530/2809]  eta: 0:02:42  lr: 0.000040  min_lr: 0.000000  loss: 4.2029 (3.9869)  loss_scale: 65536.0000 (63749.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1582  max mem: 15572
Epoch: [13]  [2540/2809]  eta: 0:02:36  lr: 0.000040  min_lr: 0.000000  loss: 3.9772 (3.9856)  loss_scale: 65536.0000 (63756.3920)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1584  max mem: 15572
Epoch: [13]  [2550/2809]  eta: 0:02:30  lr: 0.000040  min_lr: 0.000000  loss: 3.9772 (3.9857)  loss_scale: 65536.0000 (63763.3681)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.1751  max mem: 15572
Epoch: [13]  [2560/2809]  eta: 0:02:24  lr: 0.000040  min_lr: 0.000000  loss: 4.0144 (3.9857)  loss_scale: 65536.0000 (63770.2897)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0829  max mem: 15572
Epoch: [13]  [2570/2809]  eta: 0:02:18  lr: 0.000040  min_lr: 0.000000  loss: 3.9586 (3.9854)  loss_scale: 65536.0000 (63777.1575)  weight_decay: 0.0500 (0.0500)  time: 0.4490  data: 0.0181  max mem: 15572
Epoch: [13]  [2580/2809]  eta: 0:02:13  lr: 0.000040  min_lr: 0.000000  loss: 3.8712 (3.9854)  loss_scale: 65536.0000 (63783.9721)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0554  max mem: 15572
Epoch: [13]  [2590/2809]  eta: 0:02:07  lr: 0.000040  min_lr: 0.000000  loss: 4.1603 (3.9862)  loss_scale: 65536.0000 (63790.7341)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1239  max mem: 15572
Epoch: [13]  [2600/2809]  eta: 0:02:01  lr: 0.000040  min_lr: 0.000000  loss: 4.2623 (3.9865)  loss_scale: 65536.0000 (63797.4441)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1518  max mem: 15572
Epoch: [13]  [2610/2809]  eta: 0:01:55  lr: 0.000040  min_lr: 0.000000  loss: 4.1193 (3.9867)  loss_scale: 65536.0000 (63804.1026)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1531  max mem: 15572
Epoch: [13]  [2620/2809]  eta: 0:01:49  lr: 0.000040  min_lr: 0.000000  loss: 4.1193 (3.9863)  loss_scale: 65536.0000 (63810.7104)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1462  max mem: 15572
[2025-01-13 03:08:19,380] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:08:19,380] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [2630/2809]  eta: 0:01:43  lr: 0.000040  min_lr: 0.000000  loss: 4.0301 (3.9862)  loss_scale: 65536.0000 (63991.6321)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0929  max mem: 15572
[2025-01-13 03:08:25,386] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39152
[2025-01-13 03:08:25,387] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:08:25,387] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2640/2809]  eta: 0:01:38  lr: 0.000040  min_lr: 0.000000  loss: 3.8817 (3.9854)  loss_scale: 131072.0000 (64096.7391)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0961  max mem: 15572
Epoch: [13]  [2650/2809]  eta: 0:01:32  lr: 0.000040  min_lr: 0.000000  loss: 3.8293 (3.9854)  loss_scale: 65536.0000 (64102.1682)  weight_decay: 0.0500 (0.0500)  time: 0.4712  data: 0.0618  max mem: 15572
Epoch: [13]  [2660/2809]  eta: 0:01:26  lr: 0.000040  min_lr: 0.000000  loss: 4.1306 (3.9860)  loss_scale: 65536.0000 (64107.5566)  weight_decay: 0.0500 (0.0500)  time: 0.4152  data: 0.0005  max mem: 15572
Epoch: [13]  [2670/2809]  eta: 0:01:20  lr: 0.000040  min_lr: 0.000000  loss: 4.1306 (3.9856)  loss_scale: 65536.0000 (64112.9045)  weight_decay: 0.0500 (0.0500)  time: 0.4439  data: 0.0005  max mem: 15572
Epoch: [13]  [2680/2809]  eta: 0:01:14  lr: 0.000040  min_lr: 0.000000  loss: 3.9056 (3.9857)  loss_scale: 65536.0000 (64118.2126)  weight_decay: 0.0500 (0.0500)  time: 0.5043  data: 0.0504  max mem: 15572
Epoch: [13]  [2690/2809]  eta: 0:01:08  lr: 0.000040  min_lr: 0.000000  loss: 3.9056 (3.9856)  loss_scale: 65536.0000 (64123.4812)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1864  max mem: 15572
Epoch: [13]  [2700/2809]  eta: 0:01:03  lr: 0.000040  min_lr: 0.000000  loss: 4.1776 (3.9864)  loss_scale: 65536.0000 (64128.7108)  weight_decay: 0.0500 (0.0500)  time: 0.6735  data: 0.2103  max mem: 15572
Epoch: [13]  [2710/2809]  eta: 0:00:57  lr: 0.000040  min_lr: 0.000000  loss: 4.0419 (3.9859)  loss_scale: 65536.0000 (64133.9019)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1582  max mem: 15572
Epoch: [13]  [2720/2809]  eta: 0:00:51  lr: 0.000040  min_lr: 0.000000  loss: 3.9080 (3.9856)  loss_scale: 65536.0000 (64139.0548)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.1731  max mem: 15572
Epoch: [13]  [2730/2809]  eta: 0:00:45  lr: 0.000040  min_lr: 0.000000  loss: 3.8797 (3.9850)  loss_scale: 65536.0000 (64144.1699)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.1880  max mem: 15572
Epoch: [13]  [2740/2809]  eta: 0:00:40  lr: 0.000040  min_lr: 0.000000  loss: 3.8374 (3.9855)  loss_scale: 65536.0000 (64149.2477)  weight_decay: 0.0500 (0.0500)  time: 0.7108  data: 0.2329  max mem: 15572
Epoch: [13]  [2750/2809]  eta: 0:00:34  lr: 0.000040  min_lr: 0.000000  loss: 3.9116 (3.9854)  loss_scale: 65536.0000 (64154.2886)  weight_decay: 0.0500 (0.0500)  time: 0.7331  data: 0.2647  max mem: 15572
Epoch: [13]  [2760/2809]  eta: 0:00:28  lr: 0.000040  min_lr: 0.000000  loss: 4.0958 (3.9858)  loss_scale: 65536.0000 (64159.2930)  weight_decay: 0.0500 (0.0500)  time: 0.7214  data: 0.2675  max mem: 15572
[2025-01-13 03:09:44,152] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:09:44,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [2770/2809]  eta: 0:00:22  lr: 0.000040  min_lr: 0.000000  loss: 3.9549 (3.9853)  loss_scale: 65536.0000 (64329.8160)  weight_decay: 0.0500 (0.0500)  time: 0.6603  data: 0.2009  max mem: 15572
[2025-01-13 03:09:51,052] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39290
[2025-01-13 03:09:51,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:09:51,054] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2780/2809]  eta: 0:00:16  lr: 0.000040  min_lr: 0.000000  loss: 3.7402 (3.9837)  loss_scale: 65536.0000 (64381.2844)  weight_decay: 0.0500 (0.0500)  time: 0.6729  data: 0.2050  max mem: 15572
Epoch: [13]  [2790/2809]  eta: 0:00:11  lr: 0.000040  min_lr: 0.000000  loss: 3.8061 (3.9839)  loss_scale: 65536.0000 (64385.4217)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1655  max mem: 15572
[2025-01-13 03:09:59,418] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39308
[2025-01-13 03:09:59,418] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 03:09:59,418] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [2800/2809]  eta: 0:00:05  lr: 0.000040  min_lr: 0.000000  loss: 3.9943 (3.9835)  loss_scale: 32768.0000 (64272.5427)  weight_decay: 0.0500 (0.0500)  time: 0.4280  data: 0.0250  max mem: 15572
Epoch: [13]  [2808/2809]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000000  loss: 3.8941 (3.9832)  loss_scale: 32768.0000 (64182.8181)  weight_decay: 0.0500 (0.0500)  time: 0.3934  data: 0.0005  max mem: 15572
Epoch: [13] Total time: 0:27:13 (0.5814 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000000  loss: 3.8941 (3.9832)  loss_scale: 32768.0000 (64182.8181)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:24:50  loss: 0.4848 (0.4848)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.4804  data: 5.3019  max mem: 15572
Val:  [ 10/272]  eta: 0:03:21  loss: 3.0986 (2.8388)  acc1: 16.6667 (29.7980)  acc5: 55.5556 (54.0404)  time: 0.7677  data: 0.5702  max mem: 15572
Val:  [ 20/272]  eta: 0:02:21  loss: 3.0262 (2.8008)  acc1: 27.7778 (33.3333)  acc5: 61.1111 (60.5820)  time: 0.3151  data: 0.1227  max mem: 15572
Val:  [ 30/272]  eta: 0:01:50  loss: 3.0095 (2.9028)  acc1: 27.7778 (29.2115)  acc5: 61.1111 (60.5735)  time: 0.2873  data: 0.0977  max mem: 15572
Val:  [ 40/272]  eta: 0:01:39  loss: 2.7882 (2.8823)  acc1: 22.2222 (28.8618)  acc5: 66.6667 (63.2791)  time: 0.2939  data: 0.0980  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 2.7303 (2.8225)  acc1: 33.3333 (30.2832)  acc5: 72.2222 (65.6863)  time: 0.3386  data: 0.1417  max mem: 15572
Val:  [ 60/272]  eta: 0:01:23  loss: 1.8743 (2.6800)  acc1: 50.0000 (34.7905)  acc5: 77.7778 (67.4863)  time: 0.3154  data: 0.1077  max mem: 15572
Val:  [ 70/272]  eta: 0:01:18  loss: 1.8185 (2.5900)  acc1: 61.1111 (37.0110)  acc5: 83.3333 (69.9531)  time: 0.3260  data: 0.1166  max mem: 15572
Val:  [ 80/272]  eta: 0:01:13  loss: 2.3354 (2.5914)  acc1: 44.4444 (37.3800)  acc5: 83.3333 (69.7531)  time: 0.3496  data: 0.1457  max mem: 15572
Val:  [ 90/272]  eta: 0:01:06  loss: 3.0452 (2.6532)  acc1: 33.3333 (36.5690)  acc5: 66.6667 (68.6813)  time: 0.2917  data: 0.0764  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 3.0452 (2.7013)  acc1: 27.7778 (36.2486)  acc5: 66.6667 (68.0968)  time: 0.2345  data: 0.0373  max mem: 15572
Val:  [110/272]  eta: 0:00:54  loss: 3.1457 (2.7750)  acc1: 16.6667 (34.1341)  acc5: 55.5556 (66.5666)  time: 0.2080  data: 0.0393  max mem: 15572
Val:  [120/272]  eta: 0:00:49  loss: 3.2201 (2.8122)  acc1: 11.1111 (33.1956)  acc5: 55.5556 (65.8861)  time: 0.1755  data: 0.0091  max mem: 15572
Val:  [130/272]  eta: 0:00:44  loss: 2.8280 (2.7711)  acc1: 27.7778 (34.6904)  acc5: 66.6667 (66.2426)  time: 0.1793  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:41  loss: 2.0693 (2.7592)  acc1: 44.4444 (35.3428)  acc5: 66.6667 (66.2333)  time: 0.2738  data: 0.0812  max mem: 15572
Val:  [150/272]  eta: 0:00:39  loss: 2.9153 (2.7762)  acc1: 27.7778 (34.5107)  acc5: 61.1111 (66.3355)  time: 0.3989  data: 0.2077  max mem: 15572
Val:  [160/272]  eta: 0:00:36  loss: 2.7978 (2.7592)  acc1: 33.3333 (35.3692)  acc5: 72.2222 (67.0807)  time: 0.4187  data: 0.2254  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.6994 (2.7830)  acc1: 38.8889 (34.6329)  acc5: 72.2222 (66.5367)  time: 0.3884  data: 0.1897  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.7059 (2.7760)  acc1: 22.2222 (34.5611)  acc5: 66.6667 (66.8201)  time: 0.4061  data: 0.2046  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.7059 (2.8158)  acc1: 22.2222 (33.4788)  acc5: 66.6667 (65.7068)  time: 0.3264  data: 0.1319  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.7298 (2.8233)  acc1: 22.2222 (33.0293)  acc5: 66.6667 (65.7822)  time: 0.3010  data: 0.1127  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.4353 (2.8193)  acc1: 38.8889 (33.7019)  acc5: 77.7778 (66.0348)  time: 0.3678  data: 0.1728  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.6096 (2.8062)  acc1: 44.4444 (33.9869)  acc5: 77.7778 (66.1388)  time: 0.3680  data: 0.1663  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 1.9731 (2.7653)  acc1: 55.5556 (35.3776)  acc5: 77.7778 (66.8350)  time: 0.3995  data: 0.2021  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.8499 (2.7408)  acc1: 50.0000 (35.7077)  acc5: 83.3333 (67.3582)  time: 0.4050  data: 0.2046  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.7628 (2.7574)  acc1: 33.3333 (35.3918)  acc5: 66.6667 (66.9323)  time: 0.3746  data: 0.1784  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.6042 (2.6874)  acc1: 72.2222 (37.4202)  acc5: 77.7778 (67.8799)  time: 0.3060  data: 0.1351  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4964 (2.6841)  acc1: 66.6667 (37.3104)  acc5: 83.3333 (67.9787)  time: 0.2008  data: 0.0471  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4964 (2.6881)  acc1: 61.1111 (37.3131)  acc5: 83.3333 (67.9500)  time: 0.1888  data: 0.0409  max mem: 15572
Val: Total time: 0:01:30 (0.3332 s / it)
* Acc@1 37.313 Acc@5 67.950 loss 2.688
Accuracy of the network on the 4883 val videos: 37.3%
Max accuracy: 37.37%
Epoch: [14]  [   0/2809]  eta: 6:05:47  lr: 0.000040  min_lr: 0.000000  loss: 4.0762 (4.0762)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.8132  data: 7.3348  max mem: 15572
Epoch: [14]  [  10/2809]  eta: 1:03:36  lr: 0.000040  min_lr: 0.000000  loss: 4.0828 (4.0645)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3634  data: 0.9102  max mem: 15572
Epoch: [14]  [  20/2809]  eta: 0:45:24  lr: 0.000040  min_lr: 0.000000  loss: 3.7295 (3.8931)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1826  max mem: 15572
Epoch: [14]  [  30/2809]  eta: 0:41:01  lr: 0.000040  min_lr: 0.000000  loss: 3.7215 (3.9357)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1652  max mem: 15572
Epoch: [14]  [  40/2809]  eta: 0:37:29  lr: 0.000040  min_lr: 0.000000  loss: 4.0173 (3.9694)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.1858  max mem: 15572
Epoch: [14]  [  50/2809]  eta: 0:34:44  lr: 0.000040  min_lr: 0.000000  loss: 3.9623 (3.9028)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1153  max mem: 15572
Epoch: [14]  [  60/2809]  eta: 0:31:51  lr: 0.000040  min_lr: 0.000000  loss: 3.8535 (3.9054)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4551  data: 0.0462  max mem: 15572
Epoch: [14]  [  70/2809]  eta: 0:30:01  lr: 0.000040  min_lr: 0.000000  loss: 3.9512 (3.9194)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4083  data: 0.0005  max mem: 15572
Epoch: [14]  [  80/2809]  eta: 0:28:57  lr: 0.000040  min_lr: 0.000000  loss: 4.2109 (3.9627)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4576  data: 0.0264  max mem: 15572
Epoch: [14]  [  90/2809]  eta: 0:28:19  lr: 0.000040  min_lr: 0.000000  loss: 4.1446 (3.9607)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5091  data: 0.0849  max mem: 15572
Epoch: [14]  [ 100/2809]  eta: 0:27:58  lr: 0.000040  min_lr: 0.000000  loss: 4.0414 (3.9726)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1334  max mem: 15572
Epoch: [14]  [ 110/2809]  eta: 0:27:37  lr: 0.000040  min_lr: 0.000000  loss: 4.0756 (3.9762)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1256  max mem: 15572
[2025-01-13 03:12:45,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:12:45,561] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 120/2809]  eta: 0:27:34  lr: 0.000040  min_lr: 0.000000  loss: 4.1189 (3.9813)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1567  max mem: 15572
Epoch: [14]  [ 130/2809]  eta: 0:27:13  lr: 0.000040  min_lr: 0.000000  loss: 4.1494 (3.9879)  loss_scale: 65536.0000 (37770.7481)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1438  max mem: 15572
Epoch: [14]  [ 140/2809]  eta: 0:26:26  lr: 0.000040  min_lr: 0.000000  loss: 4.1298 (3.9974)  loss_scale: 65536.0000 (39739.9149)  weight_decay: 0.0500 (0.0500)  time: 0.4680  data: 0.0382  max mem: 15572
Epoch: [14]  [ 150/2809]  eta: 0:25:43  lr: 0.000040  min_lr: 0.000000  loss: 3.9472 (4.0000)  loss_scale: 65536.0000 (41448.2649)  weight_decay: 0.0500 (0.0500)  time: 0.3885  data: 0.0003  max mem: 15572
Epoch: [14]  [ 160/2809]  eta: 0:25:16  lr: 0.000040  min_lr: 0.000000  loss: 3.9464 (3.9980)  loss_scale: 65536.0000 (42944.3975)  weight_decay: 0.0500 (0.0500)  time: 0.4161  data: 0.0006  max mem: 15572
Epoch: [14]  [ 170/2809]  eta: 0:24:51  lr: 0.000040  min_lr: 0.000000  loss: 3.8679 (3.9889)  loss_scale: 65536.0000 (44265.5439)  weight_decay: 0.0500 (0.0500)  time: 0.4507  data: 0.0008  max mem: 15572
Epoch: [14]  [ 180/2809]  eta: 0:25:08  lr: 0.000040  min_lr: 0.000000  loss: 3.9954 (3.9932)  loss_scale: 65536.0000 (45440.7072)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1243  max mem: 15572
Epoch: [14]  [ 190/2809]  eta: 0:25:17  lr: 0.000040  min_lr: 0.000000  loss: 4.1149 (4.0037)  loss_scale: 65536.0000 (46492.8168)  weight_decay: 0.0500 (0.0500)  time: 0.7006  data: 0.2382  max mem: 15572
Epoch: [14]  [ 200/2809]  eta: 0:25:30  lr: 0.000040  min_lr: 0.000000  loss: 4.0792 (4.0093)  loss_scale: 65536.0000 (47440.2388)  weight_decay: 0.0500 (0.0500)  time: 0.7018  data: 0.2447  max mem: 15572
Epoch: [14]  [ 210/2809]  eta: 0:25:25  lr: 0.000040  min_lr: 0.000000  loss: 4.0934 (4.0075)  loss_scale: 65536.0000 (48297.8578)  weight_decay: 0.0500 (0.0500)  time: 0.6593  data: 0.1999  max mem: 15572
Epoch: [14]  [ 220/2809]  eta: 0:25:22  lr: 0.000040  min_lr: 0.000000  loss: 3.6708 (3.9948)  loss_scale: 65536.0000 (49077.8643)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1578  max mem: 15572
Epoch: [14]  [ 230/2809]  eta: 0:25:28  lr: 0.000040  min_lr: 0.000000  loss: 3.6829 (3.9869)  loss_scale: 65536.0000 (49790.3377)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.2126  max mem: 15572
[2025-01-13 03:13:58,161] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:13:58,161] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [ 240/2809]  eta: 0:25:36  lr: 0.000040  min_lr: 0.000000  loss: 4.0450 (3.9864)  loss_scale: 65536.0000 (50987.5519)  weight_decay: 0.0500 (0.0500)  time: 0.7089  data: 0.2515  max mem: 15572
[2025-01-13 03:14:02,415] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39569
[2025-01-13 03:14:02,415] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:14:02,416] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 250/2809]  eta: 0:25:34  lr: 0.000040  min_lr: 0.000000  loss: 4.0896 (3.9894)  loss_scale: 65536.0000 (52089.3705)  weight_decay: 0.0500 (0.0500)  time: 0.6801  data: 0.2110  max mem: 15572
Epoch: [14]  [ 260/2809]  eta: 0:25:29  lr: 0.000040  min_lr: 0.000000  loss: 4.1878 (3.9958)  loss_scale: 65536.0000 (52604.5670)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1551  max mem: 15572
Epoch: [14]  [ 270/2809]  eta: 0:25:38  lr: 0.000039  min_lr: 0.000000  loss: 4.1932 (3.9980)  loss_scale: 65536.0000 (53081.7417)  weight_decay: 0.0500 (0.0500)  time: 0.6860  data: 0.2188  max mem: 15572
Epoch: [14]  [ 280/2809]  eta: 0:25:52  lr: 0.000039  min_lr: 0.000000  loss: 4.1284 (4.0011)  loss_scale: 65536.0000 (53524.9537)  weight_decay: 0.0500 (0.0500)  time: 0.7948  data: 0.3136  max mem: 15572
Epoch: [14]  [ 290/2809]  eta: 0:25:26  lr: 0.000039  min_lr: 0.000000  loss: 4.0808 (4.0043)  loss_scale: 65536.0000 (53937.7045)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1664  max mem: 15572
Epoch: [14]  [ 300/2809]  eta: 0:25:05  lr: 0.000039  min_lr: 0.000000  loss: 4.0087 (4.0017)  loss_scale: 65536.0000 (54323.0299)  weight_decay: 0.0500 (0.0500)  time: 0.4043  data: 0.0005  max mem: 15572
Epoch: [14]  [ 310/2809]  eta: 0:24:48  lr: 0.000039  min_lr: 0.000000  loss: 4.0052 (4.0061)  loss_scale: 65536.0000 (54683.5756)  weight_decay: 0.0500 (0.0500)  time: 0.4426  data: 0.0006  max mem: 15572
Epoch: [14]  [ 320/2809]  eta: 0:24:32  lr: 0.000039  min_lr: 0.000000  loss: 4.1257 (4.0094)  loss_scale: 65536.0000 (55021.6573)  weight_decay: 0.0500 (0.0500)  time: 0.4643  data: 0.0160  max mem: 15572
Epoch: [14]  [ 330/2809]  eta: 0:24:18  lr: 0.000039  min_lr: 0.000000  loss: 4.1781 (4.0101)  loss_scale: 65536.0000 (55339.3112)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0609  max mem: 15572
Epoch: [14]  [ 340/2809]  eta: 0:24:14  lr: 0.000039  min_lr: 0.000000  loss: 4.1179 (4.0104)  loss_scale: 65536.0000 (55638.3343)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1205  max mem: 15572
Epoch: [14]  [ 350/2809]  eta: 0:24:01  lr: 0.000039  min_lr: 0.000000  loss: 4.1389 (4.0152)  loss_scale: 65536.0000 (55920.3191)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1080  max mem: 15572
Epoch: [14]  [ 360/2809]  eta: 0:23:55  lr: 0.000039  min_lr: 0.000000  loss: 4.0144 (4.0148)  loss_scale: 65536.0000 (56186.6814)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.1002  max mem: 15572
Epoch: [14]  [ 370/2809]  eta: 0:23:45  lr: 0.000039  min_lr: 0.000000  loss: 4.0089 (4.0167)  loss_scale: 65536.0000 (56438.6846)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1366  max mem: 15572
[2025-01-13 03:15:14,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:15:14,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:15:16,180] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39702
[2025-01-13 03:15:16,180] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:15:16,181] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 380/2809]  eta: 0:23:28  lr: 0.000039  min_lr: 0.000000  loss: 3.9366 (4.0089)  loss_scale: 65536.0000 (57365.5013)  weight_decay: 0.0500 (0.0500)  time: 0.4689  data: 0.0695  max mem: 15572
Epoch: [14]  [ 390/2809]  eta: 0:23:14  lr: 0.000039  min_lr: 0.000000  loss: 3.9156 (4.0084)  loss_scale: 65536.0000 (57574.4655)  weight_decay: 0.0500 (0.0500)  time: 0.4316  data: 0.0007  max mem: 15572
Epoch: [14]  [ 400/2809]  eta: 0:23:00  lr: 0.000039  min_lr: 0.000000  loss: 4.0288 (4.0069)  loss_scale: 65536.0000 (57773.0075)  weight_decay: 0.0500 (0.0500)  time: 0.4413  data: 0.0011  max mem: 15572
Epoch: [14]  [ 410/2809]  eta: 0:22:53  lr: 0.000039  min_lr: 0.000000  loss: 3.9641 (4.0024)  loss_scale: 65536.0000 (57961.8881)  weight_decay: 0.0500 (0.0500)  time: 0.4903  data: 0.0345  max mem: 15572
Epoch: [14]  [ 420/2809]  eta: 0:22:55  lr: 0.000039  min_lr: 0.000000  loss: 3.9802 (4.0032)  loss_scale: 65536.0000 (58141.7957)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1673  max mem: 15572
Epoch: [14]  [ 430/2809]  eta: 0:22:59  lr: 0.000039  min_lr: 0.000000  loss: 3.9802 (3.9994)  loss_scale: 65536.0000 (58313.3550)  weight_decay: 0.0500 (0.0500)  time: 0.7358  data: 0.2718  max mem: 15572
Epoch: [14]  [ 440/2809]  eta: 0:23:00  lr: 0.000039  min_lr: 0.000000  loss: 3.8818 (3.9981)  loss_scale: 65536.0000 (58477.1338)  weight_decay: 0.0500 (0.0500)  time: 0.7328  data: 0.2488  max mem: 15572
Epoch: [14]  [ 450/2809]  eta: 0:22:59  lr: 0.000039  min_lr: 0.000000  loss: 3.8622 (3.9951)  loss_scale: 65536.0000 (58633.6497)  weight_decay: 0.0500 (0.0500)  time: 0.6919  data: 0.2150  max mem: 15572
Epoch: [14]  [ 460/2809]  eta: 0:23:00  lr: 0.000039  min_lr: 0.000000  loss: 3.9075 (3.9900)  loss_scale: 65536.0000 (58783.3753)  weight_decay: 0.0500 (0.0500)  time: 0.6981  data: 0.2236  max mem: 15572
Epoch: [14]  [ 470/2809]  eta: 0:23:01  lr: 0.000039  min_lr: 0.000000  loss: 4.0257 (3.9902)  loss_scale: 65536.0000 (58926.7431)  weight_decay: 0.0500 (0.0500)  time: 0.7178  data: 0.2514  max mem: 15572
Epoch: [14]  [ 480/2809]  eta: 0:23:00  lr: 0.000039  min_lr: 0.000000  loss: 4.0257 (3.9885)  loss_scale: 65536.0000 (59064.1497)  weight_decay: 0.0500 (0.0500)  time: 0.7126  data: 0.2706  max mem: 15572
Epoch: [14]  [ 490/2809]  eta: 0:22:55  lr: 0.000039  min_lr: 0.000000  loss: 4.1328 (3.9937)  loss_scale: 65536.0000 (59195.9593)  weight_decay: 0.0500 (0.0500)  time: 0.6509  data: 0.2166  max mem: 15572
Epoch: [14]  [ 500/2809]  eta: 0:22:52  lr: 0.000039  min_lr: 0.000000  loss: 4.0318 (3.9895)  loss_scale: 65536.0000 (59322.5070)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1803  max mem: 15572
[2025-01-13 03:16:37,971] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:16:37,971] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:16:39,506] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39834
[2025-01-13 03:16:39,506] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:16:39,506] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 510/2809]  eta: 0:22:45  lr: 0.000039  min_lr: 0.000000  loss: 3.9933 (3.9919)  loss_scale: 65536.0000 (59828.8532)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1760  max mem: 15572
Epoch: [14]  [ 520/2809]  eta: 0:22:31  lr: 0.000039  min_lr: 0.000000  loss: 3.9933 (3.9893)  loss_scale: 65536.0000 (59938.3954)  weight_decay: 0.0500 (0.0500)  time: 0.4889  data: 0.0812  max mem: 15572
Epoch: [14]  [ 530/2809]  eta: 0:22:19  lr: 0.000039  min_lr: 0.000000  loss: 3.9071 (3.9896)  loss_scale: 65536.0000 (60043.8117)  weight_decay: 0.0500 (0.0500)  time: 0.4285  data: 0.0205  max mem: 15572
Epoch: [14]  [ 540/2809]  eta: 0:22:15  lr: 0.000039  min_lr: 0.000000  loss: 4.0699 (3.9912)  loss_scale: 65536.0000 (60145.3309)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.1081  max mem: 15572
Epoch: [14]  [ 550/2809]  eta: 0:22:09  lr: 0.000039  min_lr: 0.000000  loss: 4.0698 (3.9922)  loss_scale: 65536.0000 (60243.1652)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1623  max mem: 15572
Epoch: [14]  [ 560/2809]  eta: 0:21:59  lr: 0.000039  min_lr: 0.000000  loss: 3.9245 (3.9871)  loss_scale: 65536.0000 (60337.5116)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0962  max mem: 15572
Epoch: [14]  [ 570/2809]  eta: 0:21:53  lr: 0.000039  min_lr: 0.000000  loss: 3.7396 (3.9832)  loss_scale: 65536.0000 (60428.5534)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0902  max mem: 15572
Epoch: [14]  [ 580/2809]  eta: 0:21:45  lr: 0.000039  min_lr: 0.000000  loss: 3.7476 (3.9829)  loss_scale: 65536.0000 (60516.4613)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1077  max mem: 15572
Epoch: [14]  [ 590/2809]  eta: 0:21:32  lr: 0.000039  min_lr: 0.000000  loss: 4.0578 (3.9829)  loss_scale: 65536.0000 (60601.3942)  weight_decay: 0.0500 (0.0500)  time: 0.4680  data: 0.0488  max mem: 15572
Epoch: [14]  [ 600/2809]  eta: 0:21:19  lr: 0.000039  min_lr: 0.000000  loss: 4.0395 (3.9852)  loss_scale: 65536.0000 (60683.5008)  weight_decay: 0.0500 (0.0500)  time: 0.3947  data: 0.0029  max mem: 15572
Epoch: [14]  [ 610/2809]  eta: 0:21:09  lr: 0.000039  min_lr: 0.000000  loss: 4.0395 (3.9844)  loss_scale: 65536.0000 (60762.9198)  weight_decay: 0.0500 (0.0500)  time: 0.4288  data: 0.0006  max mem: 15572
Epoch: [14]  [ 620/2809]  eta: 0:20:59  lr: 0.000039  min_lr: 0.000000  loss: 4.0133 (3.9871)  loss_scale: 65536.0000 (60839.7810)  weight_decay: 0.0500 (0.0500)  time: 0.4606  data: 0.0007  max mem: 15572
Epoch: [14]  [ 630/2809]  eta: 0:20:53  lr: 0.000039  min_lr: 0.000000  loss: 4.0263 (3.9872)  loss_scale: 65536.0000 (60914.2060)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0449  max mem: 15572
[2025-01-13 03:17:43,368] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:17:43,369] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:17:44,199] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39965
[2025-01-13 03:17:44,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:17:44,201] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 640/2809]  eta: 0:20:50  lr: 0.000039  min_lr: 0.000000  loss: 4.0252 (3.9845)  loss_scale: 65536.0000 (61190.7894)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1603  max mem: 15572
Epoch: [14]  [ 650/2809]  eta: 0:20:45  lr: 0.000039  min_lr: 0.000000  loss: 3.9815 (3.9848)  loss_scale: 65536.0000 (61257.5361)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1737  max mem: 15572
Epoch: [14]  [ 660/2809]  eta: 0:20:42  lr: 0.000039  min_lr: 0.000000  loss: 4.1233 (3.9872)  loss_scale: 65536.0000 (61322.2632)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1499  max mem: 15572
Epoch: [14]  [ 670/2809]  eta: 0:20:38  lr: 0.000039  min_lr: 0.000000  loss: 4.0335 (3.9872)  loss_scale: 65536.0000 (61385.0611)  weight_decay: 0.0500 (0.0500)  time: 0.6506  data: 0.1863  max mem: 15572
[2025-01-13 03:18:07,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=40000, skipped=267, lr=[3.805205638311788e-07, 3.805205638311788e-07, 5.436008054731127e-07, 5.436008054731127e-07, 7.765725792473039e-07, 7.765725792473039e-07, 1.10938939892472e-06, 1.10938939892472e-06, 1.5848419984638858e-06, 1.5848419984638858e-06, 2.2640599978055513e-06, 2.2640599978055513e-06, 3.234371425436502e-06, 3.234371425436502e-06, 4.620530607766432e-06, 4.620530607766432e-06, 6.600758011094902e-06, 6.600758011094902e-06, 9.429654301564147e-06, 9.429654301564147e-06, 1.347093471652021e-05, 1.347093471652021e-05, 1.924419245217173e-05, 1.924419245217173e-05, 2.7491703503102474e-05, 2.7491703503102474e-05, 3.927386214728925e-05, 3.927386214728925e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 03:18:07,242] [INFO] [timer.py:260:stop] epoch=0/micro_step=40000/global_step=40000, RunningAvgSamplesPerSec=28.11884645607692, CurrSamplesPerSec=24.203027563523055, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [ 680/2809]  eta: 0:20:34  lr: 0.000039  min_lr: 0.000000  loss: 4.0384 (3.9879)  loss_scale: 65536.0000 (61446.0147)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.1774  max mem: 15572
Epoch: [14]  [ 690/2809]  eta: 0:20:31  lr: 0.000039  min_lr: 0.000000  loss: 4.0578 (3.9847)  loss_scale: 65536.0000 (61505.2041)  weight_decay: 0.0500 (0.0500)  time: 0.6591  data: 0.1905  max mem: 15572
Epoch: [14]  [ 700/2809]  eta: 0:20:30  lr: 0.000039  min_lr: 0.000000  loss: 3.8120 (3.9819)  loss_scale: 65536.0000 (61562.7047)  weight_decay: 0.0500 (0.0500)  time: 0.6981  data: 0.2223  max mem: 15572
Epoch: [14]  [ 710/2809]  eta: 0:20:27  lr: 0.000039  min_lr: 0.000000  loss: 3.5788 (3.9791)  loss_scale: 65536.0000 (61618.5879)  weight_decay: 0.0500 (0.0500)  time: 0.6993  data: 0.2240  max mem: 15572
Epoch: [14]  [ 720/2809]  eta: 0:20:24  lr: 0.000039  min_lr: 0.000000  loss: 3.8740 (3.9787)  loss_scale: 65536.0000 (61672.9209)  weight_decay: 0.0500 (0.0500)  time: 0.6876  data: 0.2279  max mem: 15572
Epoch: [14]  [ 730/2809]  eta: 0:20:20  lr: 0.000039  min_lr: 0.000000  loss: 3.9599 (3.9777)  loss_scale: 65536.0000 (61725.7674)  weight_decay: 0.0500 (0.0500)  time: 0.6745  data: 0.2092  max mem: 15572
Epoch: [14]  [ 740/2809]  eta: 0:20:13  lr: 0.000039  min_lr: 0.000000  loss: 3.9886 (3.9774)  loss_scale: 65536.0000 (61777.1876)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1521  max mem: 15572
Epoch: [14]  [ 750/2809]  eta: 0:20:02  lr: 0.000039  min_lr: 0.000000  loss: 4.0356 (3.9783)  loss_scale: 65536.0000 (61827.2383)  weight_decay: 0.0500 (0.0500)  time: 0.4737  data: 0.0620  max mem: 15572
Epoch: [14]  [ 760/2809]  eta: 0:19:53  lr: 0.000039  min_lr: 0.000000  loss: 3.9915 (3.9775)  loss_scale: 65536.0000 (61875.9737)  weight_decay: 0.0500 (0.0500)  time: 0.4455  data: 0.0007  max mem: 15572
[2025-01-13 03:19:06,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:19:06,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:19:06,576] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40095
[2025-01-13 03:19:06,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:19:06,576] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 770/2809]  eta: 0:19:49  lr: 0.000039  min_lr: 0.000000  loss: 3.8615 (3.9773)  loss_scale: 65536.0000 (62008.4462)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0951  max mem: 15572
Epoch: [14]  [ 780/2809]  eta: 0:19:46  lr: 0.000039  min_lr: 0.000000  loss: 4.0574 (3.9767)  loss_scale: 65536.0000 (62053.6133)  weight_decay: 0.0500 (0.0500)  time: 0.6663  data: 0.2128  max mem: 15572
Epoch: [14]  [ 790/2809]  eta: 0:19:44  lr: 0.000039  min_lr: 0.000000  loss: 4.1052 (3.9778)  loss_scale: 65536.0000 (62097.6384)  weight_decay: 0.0500 (0.0500)  time: 0.7102  data: 0.2604  max mem: 15572
Epoch: [14]  [ 800/2809]  eta: 0:19:42  lr: 0.000039  min_lr: 0.000000  loss: 4.0296 (3.9765)  loss_scale: 65536.0000 (62140.5643)  weight_decay: 0.0500 (0.0500)  time: 0.7430  data: 0.2987  max mem: 15572
Epoch: [14]  [ 810/2809]  eta: 0:19:38  lr: 0.000039  min_lr: 0.000000  loss: 3.9425 (3.9766)  loss_scale: 65536.0000 (62182.4316)  weight_decay: 0.0500 (0.0500)  time: 0.7030  data: 0.2647  max mem: 15572
Epoch: [14]  [ 820/2809]  eta: 0:19:33  lr: 0.000039  min_lr: 0.000000  loss: 4.0739 (3.9784)  loss_scale: 65536.0000 (62223.2789)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1850  max mem: 15572
Epoch: [14]  [ 830/2809]  eta: 0:19:28  lr: 0.000039  min_lr: 0.000000  loss: 4.0909 (3.9778)  loss_scale: 65536.0000 (62263.1432)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1781  max mem: 15572
Epoch: [14]  [ 840/2809]  eta: 0:19:24  lr: 0.000039  min_lr: 0.000000  loss: 4.0748 (3.9785)  loss_scale: 65536.0000 (62302.0595)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.2230  max mem: 15572
Epoch: [14]  [ 850/2809]  eta: 0:19:22  lr: 0.000039  min_lr: 0.000000  loss: 3.9376 (3.9789)  loss_scale: 65536.0000 (62340.0611)  weight_decay: 0.0500 (0.0500)  time: 0.7136  data: 0.2863  max mem: 15572
Epoch: [14]  [ 860/2809]  eta: 0:19:16  lr: 0.000039  min_lr: 0.000000  loss: 4.0721 (3.9801)  loss_scale: 65536.0000 (62377.1800)  weight_decay: 0.0500 (0.0500)  time: 0.6881  data: 0.2504  max mem: 15572
Epoch: [14]  [ 870/2809]  eta: 0:19:13  lr: 0.000039  min_lr: 0.000000  loss: 3.9072 (3.9780)  loss_scale: 65536.0000 (62413.4466)  weight_decay: 0.0500 (0.0500)  time: 0.6732  data: 0.2281  max mem: 15572
Epoch: [14]  [ 880/2809]  eta: 0:19:09  lr: 0.000039  min_lr: 0.000000  loss: 3.8725 (3.9786)  loss_scale: 65536.0000 (62448.8899)  weight_decay: 0.0500 (0.0500)  time: 0.6944  data: 0.2360  max mem: 15572
Epoch: [14]  [ 890/2809]  eta: 0:19:03  lr: 0.000039  min_lr: 0.000000  loss: 4.0371 (3.9787)  loss_scale: 65536.0000 (62483.5376)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1876  max mem: 15572
[2025-01-13 03:20:34,297] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:20:34,298] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:20:35,140] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40226
[2025-01-13 03:20:35,140] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:20:35,140] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 900/2809]  eta: 0:18:59  lr: 0.000039  min_lr: 0.000000  loss: 3.8834 (3.9771)  loss_scale: 65536.0000 (62662.8901)  weight_decay: 0.0500 (0.0500)  time: 0.6518  data: 0.2076  max mem: 15572
Epoch: [14]  [ 910/2809]  eta: 0:18:53  lr: 0.000039  min_lr: 0.000000  loss: 3.8566 (3.9769)  loss_scale: 65536.0000 (62694.4281)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.1888  max mem: 15572
Epoch: [14]  [ 920/2809]  eta: 0:18:49  lr: 0.000039  min_lr: 0.000000  loss: 3.8433 (3.9747)  loss_scale: 65536.0000 (62725.2812)  weight_decay: 0.0500 (0.0500)  time: 0.6477  data: 0.2114  max mem: 15572
Epoch: [14]  [ 930/2809]  eta: 0:18:41  lr: 0.000039  min_lr: 0.000000  loss: 3.9610 (3.9756)  loss_scale: 65536.0000 (62755.4715)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1462  max mem: 15572
Epoch: [14]  [ 940/2809]  eta: 0:18:36  lr: 0.000039  min_lr: 0.000000  loss: 4.2200 (3.9776)  loss_scale: 65536.0000 (62785.0202)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1035  max mem: 15572
Epoch: [14]  [ 950/2809]  eta: 0:18:33  lr: 0.000039  min_lr: 0.000000  loss: 4.1307 (3.9791)  loss_scale: 65536.0000 (62813.9474)  weight_decay: 0.0500 (0.0500)  time: 0.6856  data: 0.2303  max mem: 15572
Epoch: [14]  [ 960/2809]  eta: 0:18:27  lr: 0.000039  min_lr: 0.000000  loss: 4.1274 (3.9809)  loss_scale: 65536.0000 (62842.2726)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.2228  max mem: 15572
Epoch: [14]  [ 970/2809]  eta: 0:18:23  lr: 0.000039  min_lr: 0.000000  loss: 4.1307 (3.9821)  loss_scale: 65536.0000 (62870.0144)  weight_decay: 0.0500 (0.0500)  time: 0.6631  data: 0.2267  max mem: 15572
Epoch: [14]  [ 980/2809]  eta: 0:18:19  lr: 0.000039  min_lr: 0.000000  loss: 4.0903 (3.9823)  loss_scale: 65536.0000 (62897.1906)  weight_decay: 0.0500 (0.0500)  time: 0.7142  data: 0.2595  max mem: 15572
Epoch: [14]  [ 990/2809]  eta: 0:18:15  lr: 0.000039  min_lr: 0.000000  loss: 4.0515 (3.9833)  loss_scale: 65536.0000 (62923.8184)  weight_decay: 0.0500 (0.0500)  time: 0.7093  data: 0.2662  max mem: 15572
Epoch: [14]  [1000/2809]  eta: 0:18:09  lr: 0.000039  min_lr: 0.000000  loss: 3.9504 (3.9810)  loss_scale: 65536.0000 (62949.9141)  weight_decay: 0.0500 (0.0500)  time: 0.6678  data: 0.2292  max mem: 15572
Epoch: [14]  [1010/2809]  eta: 0:18:04  lr: 0.000039  min_lr: 0.000000  loss: 3.9173 (3.9809)  loss_scale: 65536.0000 (62975.4936)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1817  max mem: 15572
Epoch: [14]  [1020/2809]  eta: 0:17:58  lr: 0.000039  min_lr: 0.000000  loss: 3.9204 (3.9797)  loss_scale: 65536.0000 (63000.5720)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1832  max mem: 15572
[2025-01-13 03:21:57,871] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:21:57,871] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [1030/2809]  eta: 0:17:52  lr: 0.000039  min_lr: 0.000000  loss: 3.8983 (3.9788)  loss_scale: 65536.0000 (63152.2949)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1690  max mem: 15572
[2025-01-13 03:22:01,403] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40361
[2025-01-13 03:22:01,403] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:22:01,403] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [1040/2809]  eta: 0:17:48  lr: 0.000039  min_lr: 0.000000  loss: 3.9453 (3.9791)  loss_scale: 65536.0000 (63427.0125)  weight_decay: 0.0500 (0.0500)  time: 0.6694  data: 0.2145  max mem: 15572
Epoch: [14]  [1050/2809]  eta: 0:17:42  lr: 0.000039  min_lr: 0.000000  loss: 3.9890 (3.9801)  loss_scale: 65536.0000 (63447.0790)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.2060  max mem: 15572
Epoch: [14]  [1060/2809]  eta: 0:17:38  lr: 0.000039  min_lr: 0.000000  loss: 3.9960 (3.9814)  loss_scale: 65536.0000 (63466.7672)  weight_decay: 0.0500 (0.0500)  time: 0.6696  data: 0.2138  max mem: 15572
Epoch: [14]  [1070/2809]  eta: 0:17:31  lr: 0.000039  min_lr: 0.000000  loss: 3.9119 (3.9805)  loss_scale: 65536.0000 (63486.0878)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.1849  max mem: 15572
Epoch: [14]  [1080/2809]  eta: 0:17:26  lr: 0.000039  min_lr: 0.000000  loss: 3.8069 (3.9789)  loss_scale: 65536.0000 (63505.0509)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1612  max mem: 15572
Epoch: [14]  [1090/2809]  eta: 0:17:22  lr: 0.000039  min_lr: 0.000000  loss: 3.7549 (3.9775)  loss_scale: 65536.0000 (63523.6664)  weight_decay: 0.0500 (0.0500)  time: 0.6826  data: 0.2392  max mem: 15572
Epoch: [14]  [1100/2809]  eta: 0:17:16  lr: 0.000039  min_lr: 0.000000  loss: 3.8663 (3.9781)  loss_scale: 65536.0000 (63541.9437)  weight_decay: 0.0500 (0.0500)  time: 0.6813  data: 0.2302  max mem: 15572
Epoch: [14]  [1110/2809]  eta: 0:17:11  lr: 0.000039  min_lr: 0.000000  loss: 4.0476 (3.9771)  loss_scale: 65536.0000 (63559.8920)  weight_decay: 0.0500 (0.0500)  time: 0.6651  data: 0.2146  max mem: 15572
Epoch: [14]  [1120/2809]  eta: 0:17:07  lr: 0.000039  min_lr: 0.000000  loss: 3.7316 (3.9770)  loss_scale: 65536.0000 (63577.5201)  weight_decay: 0.0500 (0.0500)  time: 0.6895  data: 0.2258  max mem: 15572
Epoch: [14]  [1130/2809]  eta: 0:17:02  lr: 0.000039  min_lr: 0.000000  loss: 4.0815 (3.9784)  loss_scale: 65536.0000 (63594.8364)  weight_decay: 0.0500 (0.0500)  time: 0.6853  data: 0.2230  max mem: 15572
Epoch: [14]  [1140/2809]  eta: 0:16:56  lr: 0.000039  min_lr: 0.000000  loss: 4.1279 (3.9802)  loss_scale: 65536.0000 (63611.8493)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.1931  max mem: 15572
Epoch: [14]  [1150/2809]  eta: 0:16:50  lr: 0.000039  min_lr: 0.000000  loss: 4.0536 (3.9795)  loss_scale: 65536.0000 (63628.5665)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1615  max mem: 15572
Epoch: [14]  [1160/2809]  eta: 0:16:44  lr: 0.000039  min_lr: 0.000000  loss: 4.0536 (3.9812)  loss_scale: 65536.0000 (63644.9957)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.1753  max mem: 15572
[2025-01-13 03:23:26,673] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:23:26,674] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:23:29,341] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40493
[2025-01-13 03:23:29,341] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:23:29,341] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [1170/2809]  eta: 0:16:40  lr: 0.000039  min_lr: 0.000000  loss: 4.0684 (3.9818)  loss_scale: 65536.0000 (63829.0418)  weight_decay: 0.0500 (0.0500)  time: 0.6829  data: 0.2232  max mem: 15572
Epoch: [14]  [1180/2809]  eta: 0:16:35  lr: 0.000039  min_lr: 0.000000  loss: 3.9853 (3.9816)  loss_scale: 65536.0000 (63843.4953)  weight_decay: 0.0500 (0.0500)  time: 0.7109  data: 0.2475  max mem: 15572
Epoch: [14]  [1190/2809]  eta: 0:16:29  lr: 0.000039  min_lr: 0.000000  loss: 3.9853 (3.9824)  loss_scale: 65536.0000 (63857.7061)  weight_decay: 0.0500 (0.0500)  time: 0.6737  data: 0.2071  max mem: 15572
Epoch: [14]  [1200/2809]  eta: 0:16:25  lr: 0.000039  min_lr: 0.000000  loss: 4.1164 (3.9836)  loss_scale: 65536.0000 (63871.6803)  weight_decay: 0.0500 (0.0500)  time: 0.6833  data: 0.2240  max mem: 15572
Epoch: [14]  [1210/2809]  eta: 0:16:19  lr: 0.000039  min_lr: 0.000000  loss: 4.0260 (3.9822)  loss_scale: 65536.0000 (63885.4236)  weight_decay: 0.0500 (0.0500)  time: 0.6731  data: 0.2194  max mem: 15572
Epoch: [14]  [1220/2809]  eta: 0:16:14  lr: 0.000039  min_lr: 0.000000  loss: 3.8982 (3.9807)  loss_scale: 65536.0000 (63898.9419)  weight_decay: 0.0500 (0.0500)  time: 0.6679  data: 0.2053  max mem: 15572
Epoch: [14]  [1230/2809]  eta: 0:16:07  lr: 0.000039  min_lr: 0.000000  loss: 3.9147 (3.9803)  loss_scale: 65536.0000 (63912.2405)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.1839  max mem: 15572
Epoch: [14]  [1240/2809]  eta: 0:16:01  lr: 0.000039  min_lr: 0.000000  loss: 4.0286 (3.9817)  loss_scale: 65536.0000 (63925.3247)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1315  max mem: 15572
Epoch: [14]  [1250/2809]  eta: 0:15:55  lr: 0.000039  min_lr: 0.000000  loss: 3.9474 (3.9818)  loss_scale: 65536.0000 (63938.1998)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1557  max mem: 15572
Epoch: [14]  [1260/2809]  eta: 0:15:50  lr: 0.000039  min_lr: 0.000000  loss: 4.0656 (3.9834)  loss_scale: 65536.0000 (63950.8707)  weight_decay: 0.0500 (0.0500)  time: 0.6592  data: 0.1941  max mem: 15572
Epoch: [14]  [1270/2809]  eta: 0:15:44  lr: 0.000039  min_lr: 0.000000  loss: 4.1661 (3.9840)  loss_scale: 65536.0000 (63963.3423)  weight_decay: 0.0500 (0.0500)  time: 0.6605  data: 0.2177  max mem: 15572
Epoch: [14]  [1280/2809]  eta: 0:15:38  lr: 0.000039  min_lr: 0.000000  loss: 3.8684 (3.9825)  loss_scale: 65536.0000 (63975.6190)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1762  max mem: 15572
Epoch: [14]  [1290/2809]  eta: 0:15:33  lr: 0.000039  min_lr: 0.000000  loss: 4.0391 (3.9827)  loss_scale: 65536.0000 (63987.7057)  weight_decay: 0.0500 (0.0500)  time: 0.6811  data: 0.2133  max mem: 15572
[2025-01-13 03:24:53,808] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:24:53,809] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:24:54,261] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40623
[2025-01-13 03:24:54,261] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:24:54,261] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [1300/2809]  eta: 0:15:28  lr: 0.000039  min_lr: 0.000000  loss: 4.0635 (3.9833)  loss_scale: 65536.0000 (64049.9800)  weight_decay: 0.0500 (0.0500)  time: 0.6910  data: 0.2454  max mem: 15572
Epoch: [14]  [1310/2809]  eta: 0:15:22  lr: 0.000039  min_lr: 0.000000  loss: 4.0243 (3.9829)  loss_scale: 65536.0000 (64061.3150)  weight_decay: 0.0500 (0.0500)  time: 0.6677  data: 0.2187  max mem: 15572
Epoch: [14]  [1320/2809]  eta: 0:15:18  lr: 0.000039  min_lr: 0.000000  loss: 4.0393 (3.9821)  loss_scale: 65536.0000 (64072.4784)  weight_decay: 0.0500 (0.0500)  time: 0.7389  data: 0.2699  max mem: 15572
Epoch: [14]  [1330/2809]  eta: 0:15:11  lr: 0.000039  min_lr: 0.000000  loss: 4.0393 (3.9831)  loss_scale: 65536.0000 (64083.4741)  weight_decay: 0.0500 (0.0500)  time: 0.6678  data: 0.2070  max mem: 15572
Epoch: [14]  [1340/2809]  eta: 0:15:04  lr: 0.000039  min_lr: 0.000000  loss: 4.0980 (3.9837)  loss_scale: 65536.0000 (64094.3057)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1073  max mem: 15572
Epoch: [14]  [1350/2809]  eta: 0:14:59  lr: 0.000039  min_lr: 0.000000  loss: 3.9500 (3.9835)  loss_scale: 65536.0000 (64104.9771)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1559  max mem: 15572
Epoch: [14]  [1360/2809]  eta: 0:14:53  lr: 0.000039  min_lr: 0.000000  loss: 3.6716 (3.9816)  loss_scale: 65536.0000 (64115.4916)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.1951  max mem: 15572
Epoch: [14]  [1370/2809]  eta: 0:14:47  lr: 0.000039  min_lr: 0.000000  loss: 3.6716 (3.9801)  loss_scale: 65536.0000 (64125.8527)  weight_decay: 0.0500 (0.0500)  time: 0.6645  data: 0.2100  max mem: 15572
Epoch: [14]  [1380/2809]  eta: 0:14:41  lr: 0.000039  min_lr: 0.000000  loss: 3.7576 (3.9779)  loss_scale: 65536.0000 (64136.0637)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1955  max mem: 15572
Epoch: [14]  [1390/2809]  eta: 0:14:36  lr: 0.000039  min_lr: 0.000000  loss: 3.8096 (3.9780)  loss_scale: 65536.0000 (64146.1280)  weight_decay: 0.0500 (0.0500)  time: 0.6644  data: 0.1774  max mem: 15572
[2025-01-13 03:26:00,394] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40724
[2025-01-13 03:26:00,394] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 03:26:00,395] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1400/2809]  eta: 0:14:30  lr: 0.000039  min_lr: 0.000000  loss: 3.9261 (3.9786)  loss_scale: 65536.0000 (64085.8815)  weight_decay: 0.0500 (0.0500)  time: 0.6769  data: 0.1907  max mem: 15572
Epoch: [14]  [1410/2809]  eta: 0:14:25  lr: 0.000039  min_lr: 0.000000  loss: 3.8017 (3.9766)  loss_scale: 32768.0000 (63863.9263)  weight_decay: 0.0500 (0.0500)  time: 0.7096  data: 0.2627  max mem: 15572
Epoch: [14]  [1420/2809]  eta: 0:14:20  lr: 0.000039  min_lr: 0.000000  loss: 3.6600 (3.9771)  loss_scale: 32768.0000 (63645.0950)  weight_decay: 0.0500 (0.0500)  time: 0.7087  data: 0.2709  max mem: 15572
Epoch: [14]  [1430/2809]  eta: 0:14:13  lr: 0.000039  min_lr: 0.000000  loss: 4.1186 (3.9762)  loss_scale: 32768.0000 (63429.3222)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1875  max mem: 15572
Epoch: [14]  [1440/2809]  eta: 0:14:07  lr: 0.000039  min_lr: 0.000000  loss: 3.8256 (3.9755)  loss_scale: 32768.0000 (63216.5441)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.1930  max mem: 15572
Epoch: [14]  [1450/2809]  eta: 0:14:02  lr: 0.000039  min_lr: 0.000000  loss: 3.7758 (3.9740)  loss_scale: 32768.0000 (63006.6988)  weight_decay: 0.0500 (0.0500)  time: 0.6709  data: 0.2106  max mem: 15572
Epoch: [14]  [1460/2809]  eta: 0:13:56  lr: 0.000039  min_lr: 0.000000  loss: 3.9406 (3.9730)  loss_scale: 32768.0000 (62799.7262)  weight_decay: 0.0500 (0.0500)  time: 0.6820  data: 0.2067  max mem: 15572
Epoch: [14]  [1470/2809]  eta: 0:13:51  lr: 0.000039  min_lr: 0.000000  loss: 3.9406 (3.9731)  loss_scale: 32768.0000 (62595.5676)  weight_decay: 0.0500 (0.0500)  time: 0.7002  data: 0.2312  max mem: 15572
Epoch: [14]  [1480/2809]  eta: 0:13:45  lr: 0.000039  min_lr: 0.000000  loss: 3.9377 (3.9743)  loss_scale: 32768.0000 (62394.1661)  weight_decay: 0.0500 (0.0500)  time: 0.6765  data: 0.2186  max mem: 15572
Epoch: [14]  [1490/2809]  eta: 0:13:39  lr: 0.000039  min_lr: 0.000000  loss: 3.9066 (3.9739)  loss_scale: 32768.0000 (62195.4661)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1922  max mem: 15572
Epoch: [14]  [1500/2809]  eta: 0:13:33  lr: 0.000039  min_lr: 0.000000  loss: 3.8982 (3.9733)  loss_scale: 32768.0000 (61999.4137)  weight_decay: 0.0500 (0.0500)  time: 0.6763  data: 0.2102  max mem: 15572
Epoch: [14]  [1510/2809]  eta: 0:13:28  lr: 0.000039  min_lr: 0.000000  loss: 3.9402 (3.9737)  loss_scale: 32768.0000 (61805.9563)  weight_decay: 0.0500 (0.0500)  time: 0.6857  data: 0.2243  max mem: 15572
Epoch: [14]  [1520/2809]  eta: 0:13:23  lr: 0.000039  min_lr: 0.000000  loss: 3.9466 (3.9728)  loss_scale: 32768.0000 (61615.0427)  weight_decay: 0.0500 (0.0500)  time: 0.7148  data: 0.2515  max mem: 15572
[2025-01-13 03:27:28,447] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:27:28,447] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [1530/2809]  eta: 0:13:17  lr: 0.000039  min_lr: 0.000000  loss: 3.9734 (3.9734)  loss_scale: 32768.0000 (61512.2351)  weight_decay: 0.0500 (0.0500)  time: 0.7049  data: 0.2309  max mem: 15572
Epoch: [14]  [1540/2809]  eta: 0:13:10  lr: 0.000039  min_lr: 0.000000  loss: 3.9734 (3.9737)  loss_scale: 65536.0000 (61538.3465)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1493  max mem: 15572
Epoch: [14]  [1550/2809]  eta: 0:13:04  lr: 0.000039  min_lr: 0.000000  loss: 3.9161 (3.9733)  loss_scale: 65536.0000 (61564.1212)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1900  max mem: 15572
Epoch: [14]  [1560/2809]  eta: 0:12:58  lr: 0.000039  min_lr: 0.000000  loss: 3.8300 (3.9720)  loss_scale: 65536.0000 (61589.5657)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.2166  max mem: 15572
Epoch: [14]  [1570/2809]  eta: 0:12:52  lr: 0.000039  min_lr: 0.000000  loss: 3.7825 (3.9709)  loss_scale: 65536.0000 (61614.6862)  weight_decay: 0.0500 (0.0500)  time: 0.6500  data: 0.1872  max mem: 15572
Epoch: [14]  [1580/2809]  eta: 0:12:47  lr: 0.000039  min_lr: 0.000000  loss: 3.9740 (3.9713)  loss_scale: 65536.0000 (61639.4889)  weight_decay: 0.0500 (0.0500)  time: 0.6856  data: 0.2313  max mem: 15572
Epoch: [14]  [1590/2809]  eta: 0:12:40  lr: 0.000039  min_lr: 0.000000  loss: 4.1221 (3.9719)  loss_scale: 65536.0000 (61663.9799)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1610  max mem: 15572
Epoch: [14]  [1600/2809]  eta: 0:12:33  lr: 0.000039  min_lr: 0.000000  loss: 4.1159 (3.9711)  loss_scale: 65536.0000 (61688.1649)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1164  max mem: 15572
Epoch: [14]  [1610/2809]  eta: 0:12:27  lr: 0.000039  min_lr: 0.000000  loss: 3.5983 (3.9701)  loss_scale: 65536.0000 (61712.0497)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1809  max mem: 15572
Epoch: [14]  [1620/2809]  eta: 0:12:21  lr: 0.000039  min_lr: 0.000000  loss: 3.7650 (3.9692)  loss_scale: 65536.0000 (61735.6397)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.1775  max mem: 15572
Epoch: [14]  [1630/2809]  eta: 0:12:15  lr: 0.000039  min_lr: 0.000000  loss: 3.8142 (3.9684)  loss_scale: 65536.0000 (61758.9405)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.1775  max mem: 15572
Epoch: [14]  [1640/2809]  eta: 0:12:09  lr: 0.000039  min_lr: 0.000000  loss: 3.8373 (3.9687)  loss_scale: 65536.0000 (61781.9573)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1438  max mem: 15572
Epoch: [14]  [1650/2809]  eta: 0:12:03  lr: 0.000039  min_lr: 0.000000  loss: 3.9304 (3.9688)  loss_scale: 65536.0000 (61804.6953)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1211  max mem: 15572
[2025-01-13 03:28:50,133] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:28:50,134] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:28:50,553] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40982
[2025-01-13 03:28:50,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:28:50,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [1660/2809]  eta: 0:11:56  lr: 0.000039  min_lr: 0.000000  loss: 3.8768 (3.9677)  loss_scale: 65536.0000 (61866.6153)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1408  max mem: 15572
Epoch: [14]  [1670/2809]  eta: 0:11:49  lr: 0.000039  min_lr: 0.000000  loss: 3.8768 (3.9679)  loss_scale: 65536.0000 (61888.5745)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0653  max mem: 15572
[2025-01-13 03:28:59,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=41000, skipped=274, lr=[3.750971688681664e-07, 3.750971688681664e-07, 5.358530983830948e-07, 5.358530983830948e-07, 7.655044262615642e-07, 7.655044262615642e-07, 1.0935777518022345e-06, 1.0935777518022345e-06, 1.5622539311460496e-06, 1.5622539311460496e-06, 2.231791330208642e-06, 2.231791330208642e-06, 3.188273328869489e-06, 3.188273328869489e-06, 4.55467618409927e-06, 4.55467618409927e-06, 6.506680262998958e-06, 6.506680262998958e-06, 9.295257518569941e-06, 9.295257518569941e-06, 1.3278939312242773e-05, 1.3278939312242773e-05, 1.8969913303203963e-05, 1.8969913303203963e-05, 2.7099876147434235e-05, 2.7099876147434235e-05, 3.871410878204891e-05, 3.871410878204891e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 03:28:59,350] [INFO] [timer.py:260:stop] epoch=0/micro_step=41000/global_step=41000, RunningAvgSamplesPerSec=28.112147849261518, CurrSamplesPerSec=29.73655562349897, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [1680/2809]  eta: 0:11:41  lr: 0.000039  min_lr: 0.000000  loss: 3.9602 (3.9673)  loss_scale: 65536.0000 (61910.2725)  weight_decay: 0.0500 (0.0500)  time: 0.4180  data: 0.0038  max mem: 15572
Epoch: [14]  [1690/2809]  eta: 0:11:33  lr: 0.000039  min_lr: 0.000000  loss: 3.9710 (3.9673)  loss_scale: 65536.0000 (61931.7138)  weight_decay: 0.0500 (0.0500)  time: 0.3881  data: 0.0003  max mem: 15572
Epoch: [14]  [1700/2809]  eta: 0:11:26  lr: 0.000039  min_lr: 0.000000  loss: 4.1431 (3.9679)  loss_scale: 65536.0000 (61952.9030)  weight_decay: 0.0500 (0.0500)  time: 0.3873  data: 0.0003  max mem: 15572
[2025-01-13 03:29:13,676] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41036
[2025-01-13 03:29:13,676] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 03:29:13,676] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1710/2809]  eta: 0:11:18  lr: 0.000039  min_lr: 0.000000  loss: 4.0448 (3.9679)  loss_scale: 65536.0000 (61954.6932)  weight_decay: 0.0500 (0.0500)  time: 0.3877  data: 0.0004  max mem: 15572
Epoch: [14]  [1720/2809]  eta: 0:11:10  lr: 0.000039  min_lr: 0.000000  loss: 4.0448 (3.9687)  loss_scale: 32768.0000 (61785.1017)  weight_decay: 0.0500 (0.0500)  time: 0.3813  data: 0.0003  max mem: 15572
Epoch: [14]  [1730/2809]  eta: 0:11:03  lr: 0.000039  min_lr: 0.000000  loss: 4.0366 (3.9679)  loss_scale: 32768.0000 (61617.4697)  weight_decay: 0.0500 (0.0500)  time: 0.3796  data: 0.0003  max mem: 15572
Epoch: [14]  [1740/2809]  eta: 0:10:55  lr: 0.000039  min_lr: 0.000000  loss: 4.0366 (3.9684)  loss_scale: 32768.0000 (61451.7634)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [14]  [1750/2809]  eta: 0:10:47  lr: 0.000039  min_lr: 0.000000  loss: 4.1092 (3.9692)  loss_scale: 32768.0000 (61287.9497)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [14]  [1760/2809]  eta: 0:10:40  lr: 0.000039  min_lr: 0.000000  loss: 4.0744 (3.9693)  loss_scale: 32768.0000 (61125.9966)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [14]  [1770/2809]  eta: 0:10:32  lr: 0.000039  min_lr: 0.000000  loss: 4.1421 (3.9708)  loss_scale: 32768.0000 (60965.8724)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [14]  [1780/2809]  eta: 0:10:25  lr: 0.000039  min_lr: 0.000000  loss: 4.0745 (3.9710)  loss_scale: 32768.0000 (60807.5463)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [14]  [1790/2809]  eta: 0:10:17  lr: 0.000039  min_lr: 0.000000  loss: 4.0745 (3.9719)  loss_scale: 32768.0000 (60650.9883)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [14]  [1800/2809]  eta: 0:10:10  lr: 0.000039  min_lr: 0.000000  loss: 4.1413 (3.9725)  loss_scale: 32768.0000 (60496.1688)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [14]  [1810/2809]  eta: 0:10:03  lr: 0.000039  min_lr: 0.000000  loss: 4.1764 (3.9739)  loss_scale: 32768.0000 (60343.0591)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [14]  [1820/2809]  eta: 0:09:55  lr: 0.000039  min_lr: 0.000000  loss: 4.1764 (3.9735)  loss_scale: 32768.0000 (60191.6310)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [14]  [1830/2809]  eta: 0:09:48  lr: 0.000039  min_lr: 0.000000  loss: 3.8912 (3.9726)  loss_scale: 32768.0000 (60041.8569)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
[2025-01-13 03:30:01,684] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:30:01,684] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [1840/2809]  eta: 0:09:41  lr: 0.000039  min_lr: 0.000000  loss: 3.7767 (3.9713)  loss_scale: 32768.0000 (59929.3080)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [14]  [1850/2809]  eta: 0:09:34  lr: 0.000039  min_lr: 0.000000  loss: 3.8425 (3.9720)  loss_scale: 65536.0000 (59959.5981)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [14]  [1860/2809]  eta: 0:09:26  lr: 0.000039  min_lr: 0.000000  loss: 4.0565 (3.9717)  loss_scale: 65536.0000 (59989.5626)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [14]  [1870/2809]  eta: 0:09:19  lr: 0.000039  min_lr: 0.000000  loss: 4.0565 (3.9726)  loss_scale: 65536.0000 (60019.2068)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [14]  [1880/2809]  eta: 0:09:12  lr: 0.000039  min_lr: 0.000000  loss: 4.1130 (3.9719)  loss_scale: 65536.0000 (60048.5359)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [14]  [1890/2809]  eta: 0:09:05  lr: 0.000039  min_lr: 0.000000  loss: 3.9258 (3.9720)  loss_scale: 65536.0000 (60077.5547)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [14]  [1900/2809]  eta: 0:08:58  lr: 0.000039  min_lr: 0.000000  loss: 3.9258 (3.9717)  loss_scale: 65536.0000 (60106.2683)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [14]  [1910/2809]  eta: 0:08:51  lr: 0.000039  min_lr: 0.000000  loss: 3.6575 (3.9698)  loss_scale: 65536.0000 (60134.6813)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [14]  [1920/2809]  eta: 0:08:44  lr: 0.000039  min_lr: 0.000000  loss: 3.9037 (3.9701)  loss_scale: 65536.0000 (60162.7985)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0001  max mem: 15572
Epoch: [14]  [1930/2809]  eta: 0:08:37  lr: 0.000039  min_lr: 0.000000  loss: 3.9699 (3.9695)  loss_scale: 65536.0000 (60190.6245)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [14]  [1940/2809]  eta: 0:08:30  lr: 0.000039  min_lr: 0.000000  loss: 3.9285 (3.9696)  loss_scale: 65536.0000 (60218.1638)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [14]  [1950/2809]  eta: 0:08:24  lr: 0.000039  min_lr: 0.000000  loss: 4.0983 (3.9705)  loss_scale: 65536.0000 (60245.4208)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [14]  [1960/2809]  eta: 0:08:17  lr: 0.000039  min_lr: 0.000000  loss: 4.0983 (3.9710)  loss_scale: 65536.0000 (60272.3998)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
[2025-01-13 03:30:48,937] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:30:48,937] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [1970/2809]  eta: 0:08:10  lr: 0.000039  min_lr: 0.000000  loss: 4.1331 (3.9711)  loss_scale: 65536.0000 (60432.1055)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
[2025-01-13 03:30:52,206] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41302
[2025-01-13 03:30:52,206] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:30:52,206] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [1980/2809]  eta: 0:08:03  lr: 0.000039  min_lr: 0.000000  loss: 4.1306 (3.9708)  loss_scale: 65536.0000 (60623.2812)  weight_decay: 0.0500 (0.0500)  time: 0.3640  data: 0.0002  max mem: 15572
Epoch: [14]  [1990/2809]  eta: 0:07:57  lr: 0.000039  min_lr: 0.000000  loss: 3.9878 (3.9701)  loss_scale: 65536.0000 (60647.9558)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [14]  [2000/2809]  eta: 0:07:50  lr: 0.000039  min_lr: 0.000000  loss: 4.0025 (3.9695)  loss_scale: 65536.0000 (60672.3838)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [14]  [2010/2809]  eta: 0:07:43  lr: 0.000039  min_lr: 0.000000  loss: 3.9109 (3.9688)  loss_scale: 65536.0000 (60696.5689)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [14]  [2020/2809]  eta: 0:07:37  lr: 0.000039  min_lr: 0.000000  loss: 3.7370 (3.9674)  loss_scale: 65536.0000 (60720.5146)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [14]  [2030/2809]  eta: 0:07:30  lr: 0.000039  min_lr: 0.000000  loss: 3.8804 (3.9667)  loss_scale: 65536.0000 (60744.2245)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [14]  [2040/2809]  eta: 0:07:23  lr: 0.000039  min_lr: 0.000000  loss: 4.0238 (3.9670)  loss_scale: 65536.0000 (60767.7021)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [14]  [2050/2809]  eta: 0:07:17  lr: 0.000038  min_lr: 0.000000  loss: 4.3384 (3.9683)  loss_scale: 65536.0000 (60790.9508)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [14]  [2060/2809]  eta: 0:07:10  lr: 0.000038  min_lr: 0.000000  loss: 4.1557 (3.9673)  loss_scale: 65536.0000 (60813.9738)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [14]  [2070/2809]  eta: 0:07:04  lr: 0.000038  min_lr: 0.000000  loss: 3.7006 (3.9662)  loss_scale: 65536.0000 (60836.7745)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [14]  [2080/2809]  eta: 0:06:57  lr: 0.000038  min_lr: 0.000000  loss: 3.7006 (3.9651)  loss_scale: 65536.0000 (60859.3561)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
[2025-01-13 03:31:31,050] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41407
[2025-01-13 03:31:31,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 03:31:31,051] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [2090/2809]  eta: 0:06:51  lr: 0.000038  min_lr: 0.000000  loss: 3.9188 (3.9648)  loss_scale: 32768.0000 (60725.0120)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [14]  [2100/2809]  eta: 0:06:45  lr: 0.000038  min_lr: 0.000000  loss: 3.8833 (3.9642)  loss_scale: 32768.0000 (60591.9467)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [14]  [2110/2809]  eta: 0:06:38  lr: 0.000038  min_lr: 0.000000  loss: 3.8762 (3.9639)  loss_scale: 32768.0000 (60460.1421)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [14]  [2120/2809]  eta: 0:06:32  lr: 0.000038  min_lr: 0.000000  loss: 3.9111 (3.9644)  loss_scale: 32768.0000 (60329.5804)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0003  max mem: 15572
Epoch: [14]  [2130/2809]  eta: 0:06:26  lr: 0.000038  min_lr: 0.000000  loss: 3.9442 (3.9638)  loss_scale: 32768.0000 (60200.2440)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [14]  [2140/2809]  eta: 0:06:19  lr: 0.000038  min_lr: 0.000000  loss: 3.8356 (3.9633)  loss_scale: 32768.0000 (60072.1158)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [14]  [2150/2809]  eta: 0:06:13  lr: 0.000038  min_lr: 0.000000  loss: 3.9585 (3.9627)  loss_scale: 32768.0000 (59945.1790)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [14]  [2160/2809]  eta: 0:06:07  lr: 0.000038  min_lr: 0.000000  loss: 4.0116 (3.9630)  loss_scale: 32768.0000 (59819.4169)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [14]  [2170/2809]  eta: 0:06:00  lr: 0.000038  min_lr: 0.000000  loss: 3.8882 (3.9621)  loss_scale: 32768.0000 (59694.8135)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [14]  [2180/2809]  eta: 0:05:54  lr: 0.000038  min_lr: 0.000000  loss: 3.8882 (3.9619)  loss_scale: 32768.0000 (59571.3526)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [14]  [2190/2809]  eta: 0:05:48  lr: 0.000038  min_lr: 0.000000  loss: 4.0972 (3.9629)  loss_scale: 32768.0000 (59449.0187)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [14]  [2200/2809]  eta: 0:05:42  lr: 0.000038  min_lr: 0.000000  loss: 3.9989 (3.9626)  loss_scale: 32768.0000 (59327.7965)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
[2025-01-13 03:32:18,733] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:32:18,733] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [2210/2809]  eta: 0:05:36  lr: 0.000038  min_lr: 0.000000  loss: 3.8517 (3.9625)  loss_scale: 32768.0000 (59222.4912)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [14]  [2220/2809]  eta: 0:05:30  lr: 0.000038  min_lr: 0.000000  loss: 3.9184 (3.9626)  loss_scale: 65536.0000 (59250.9176)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [14]  [2230/2809]  eta: 0:05:24  lr: 0.000038  min_lr: 0.000000  loss: 4.0408 (3.9628)  loss_scale: 65536.0000 (59279.0892)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [14]  [2240/2809]  eta: 0:05:17  lr: 0.000038  min_lr: 0.000000  loss: 4.1393 (3.9639)  loss_scale: 65536.0000 (59307.0094)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [14]  [2250/2809]  eta: 0:05:11  lr: 0.000038  min_lr: 0.000000  loss: 4.1229 (3.9641)  loss_scale: 65536.0000 (59334.6815)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [14]  [2260/2809]  eta: 0:05:05  lr: 0.000038  min_lr: 0.000000  loss: 4.0685 (3.9650)  loss_scale: 65536.0000 (59362.1088)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [14]  [2270/2809]  eta: 0:04:59  lr: 0.000038  min_lr: 0.000000  loss: 4.0685 (3.9647)  loss_scale: 65536.0000 (59389.2946)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0001  max mem: 15572
Epoch: [14]  [2280/2809]  eta: 0:04:53  lr: 0.000038  min_lr: 0.000000  loss: 3.8304 (3.9638)  loss_scale: 65536.0000 (59416.2420)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [14]  [2290/2809]  eta: 0:04:47  lr: 0.000038  min_lr: 0.000000  loss: 3.8052 (3.9643)  loss_scale: 65536.0000 (59442.9542)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [14]  [2300/2809]  eta: 0:04:41  lr: 0.000038  min_lr: 0.000000  loss: 3.9443 (3.9645)  loss_scale: 65536.0000 (59469.4342)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [14]  [2310/2809]  eta: 0:04:35  lr: 0.000038  min_lr: 0.000000  loss: 3.8935 (3.9640)  loss_scale: 65536.0000 (59495.6850)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0001  max mem: 15572
Epoch: [14]  [2320/2809]  eta: 0:04:30  lr: 0.000038  min_lr: 0.000000  loss: 3.8555 (3.9634)  loss_scale: 65536.0000 (59521.7096)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [14]  [2330/2809]  eta: 0:04:24  lr: 0.000038  min_lr: 0.000000  loss: 3.8922 (3.9630)  loss_scale: 65536.0000 (59547.5109)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
[2025-01-13 03:33:06,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:33:06,122] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [2340/2809]  eta: 0:04:18  lr: 0.000038  min_lr: 0.000000  loss: 3.8922 (3.9624)  loss_scale: 65536.0000 (59657.0765)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
[2025-01-13 03:33:08,331] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41670
[2025-01-13 03:33:08,331] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:33:08,331] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [2350/2809]  eta: 0:04:12  lr: 0.000038  min_lr: 0.000000  loss: 3.9563 (3.9624)  loss_scale: 65536.0000 (59765.7099)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [14]  [2360/2809]  eta: 0:04:06  lr: 0.000038  min_lr: 0.000000  loss: 3.9832 (3.9624)  loss_scale: 65536.0000 (59790.1499)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [14]  [2370/2809]  eta: 0:04:00  lr: 0.000038  min_lr: 0.000000  loss: 3.9799 (3.9624)  loss_scale: 65536.0000 (59814.3838)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0003  max mem: 15572
Epoch: [14]  [2380/2809]  eta: 0:03:54  lr: 0.000038  min_lr: 0.000000  loss: 3.7654 (3.9621)  loss_scale: 65536.0000 (59838.4141)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [14]  [2390/2809]  eta: 0:03:49  lr: 0.000038  min_lr: 0.000000  loss: 3.7179 (3.9613)  loss_scale: 65536.0000 (59862.2434)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [14]  [2400/2809]  eta: 0:03:43  lr: 0.000038  min_lr: 0.000000  loss: 3.9120 (3.9611)  loss_scale: 65536.0000 (59885.8742)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [14]  [2410/2809]  eta: 0:03:37  lr: 0.000038  min_lr: 0.000000  loss: 3.7831 (3.9605)  loss_scale: 65536.0000 (59909.3090)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [14]  [2420/2809]  eta: 0:03:31  lr: 0.000038  min_lr: 0.000000  loss: 3.8031 (3.9597)  loss_scale: 65536.0000 (59932.5502)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [14]  [2430/2809]  eta: 0:03:26  lr: 0.000038  min_lr: 0.000000  loss: 3.8066 (3.9586)  loss_scale: 65536.0000 (59955.6002)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [14]  [2440/2809]  eta: 0:03:20  lr: 0.000038  min_lr: 0.000000  loss: 3.8344 (3.9580)  loss_scale: 65536.0000 (59978.4613)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [14]  [2450/2809]  eta: 0:03:14  lr: 0.000038  min_lr: 0.000000  loss: 3.8637 (3.9570)  loss_scale: 65536.0000 (60001.1359)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [14]  [2460/2809]  eta: 0:03:09  lr: 0.000038  min_lr: 0.000000  loss: 3.8694 (3.9574)  loss_scale: 65536.0000 (60023.6262)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [14]  [2470/2809]  eta: 0:03:03  lr: 0.000038  min_lr: 0.000000  loss: 3.8928 (3.9566)  loss_scale: 65536.0000 (60045.9344)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
[2025-01-13 03:33:56,124] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:33:56,124] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:33:56,514] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41800
[2025-01-13 03:33:56,514] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:33:56,514] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [2480/2809]  eta: 0:02:57  lr: 0.000038  min_lr: 0.000000  loss: 3.6460 (3.9561)  loss_scale: 65536.0000 (60094.4780)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0001  max mem: 15572
Epoch: [14]  [2490/2809]  eta: 0:02:52  lr: 0.000038  min_lr: 0.000000  loss: 3.8663 (3.9565)  loss_scale: 65536.0000 (60116.3228)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0001  max mem: 15572
Epoch: [14]  [2500/2809]  eta: 0:02:46  lr: 0.000038  min_lr: 0.000000  loss: 3.8663 (3.9555)  loss_scale: 65536.0000 (60137.9928)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [14]  [2510/2809]  eta: 0:02:40  lr: 0.000038  min_lr: 0.000000  loss: 3.8647 (3.9554)  loss_scale: 65536.0000 (60159.4902)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [14]  [2520/2809]  eta: 0:02:35  lr: 0.000038  min_lr: 0.000000  loss: 3.9339 (3.9557)  loss_scale: 65536.0000 (60180.8171)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [14]  [2530/2809]  eta: 0:02:29  lr: 0.000038  min_lr: 0.000000  loss: 4.0657 (3.9558)  loss_scale: 65536.0000 (60201.9755)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0002  max mem: 15572
Epoch: [14]  [2540/2809]  eta: 0:02:24  lr: 0.000038  min_lr: 0.000000  loss: 3.9689 (3.9553)  loss_scale: 65536.0000 (60222.9673)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [14]  [2550/2809]  eta: 0:02:18  lr: 0.000038  min_lr: 0.000000  loss: 3.9886 (3.9556)  loss_scale: 65536.0000 (60243.7946)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [14]  [2560/2809]  eta: 0:02:13  lr: 0.000038  min_lr: 0.000000  loss: 4.0988 (3.9560)  loss_scale: 65536.0000 (60264.4592)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [14]  [2570/2809]  eta: 0:02:07  lr: 0.000038  min_lr: 0.000000  loss: 3.8095 (3.9555)  loss_scale: 65536.0000 (60284.9630)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [14]  [2580/2809]  eta: 0:02:02  lr: 0.000038  min_lr: 0.000000  loss: 4.0417 (3.9562)  loss_scale: 65536.0000 (60305.3080)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [14]  [2590/2809]  eta: 0:01:56  lr: 0.000038  min_lr: 0.000000  loss: 4.0326 (3.9555)  loss_scale: 65536.0000 (60325.4959)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [14]  [2600/2809]  eta: 0:01:51  lr: 0.000038  min_lr: 0.000000  loss: 3.8629 (3.9549)  loss_scale: 65536.0000 (60345.5286)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
[2025-01-13 03:34:44,213] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:34:44,213] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:34:44,572] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41930
[2025-01-13 03:34:44,573] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:34:44,573] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [2610/2809]  eta: 0:01:45  lr: 0.000038  min_lr: 0.000000  loss: 3.8629 (3.9546)  loss_scale: 65536.0000 (60390.5079)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [14]  [2620/2809]  eta: 0:01:40  lr: 0.000038  min_lr: 0.000000  loss: 3.8649 (3.9545)  loss_scale: 65536.0000 (60410.1396)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [14]  [2630/2809]  eta: 0:01:35  lr: 0.000038  min_lr: 0.000000  loss: 3.8622 (3.9543)  loss_scale: 65536.0000 (60429.6222)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [14]  [2640/2809]  eta: 0:01:29  lr: 0.000038  min_lr: 0.000000  loss: 3.8682 (3.9542)  loss_scale: 65536.0000 (60448.9572)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [14]  [2650/2809]  eta: 0:01:24  lr: 0.000038  min_lr: 0.000000  loss: 3.8682 (3.9539)  loss_scale: 65536.0000 (60468.1464)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [14]  [2660/2809]  eta: 0:01:18  lr: 0.000038  min_lr: 0.000000  loss: 4.1106 (3.9544)  loss_scale: 65536.0000 (60487.1913)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [14]  [2670/2809]  eta: 0:01:13  lr: 0.000038  min_lr: 0.000000  loss: 4.1643 (3.9547)  loss_scale: 65536.0000 (60506.0936)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
[2025-01-13 03:35:10,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=42000, skipped=280, lr=[3.695228847971735e-07, 3.695228847971735e-07, 5.278898354245336e-07, 5.278898354245336e-07, 7.541283363207624e-07, 7.541283363207624e-07, 1.0773261947439463e-06, 1.0773261947439463e-06, 1.5390374210627805e-06, 1.5390374210627805e-06, 2.1986248872325438e-06, 2.1986248872325438e-06, 3.140892696046491e-06, 3.140892696046491e-06, 4.4869895657807015e-06, 4.4869895657807015e-06, 6.409985093972431e-06, 6.409985093972431e-06, 9.15712156281776e-06, 9.15712156281776e-06, 1.30816022325968e-05, 1.30816022325968e-05, 1.8688003189424e-05, 1.8688003189424e-05, 2.669714741346286e-05, 2.669714741346286e-05, 3.813878201923266e-05, 3.813878201923266e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 03:35:10,092] [INFO] [timer.py:260:stop] epoch=0/micro_step=42000/global_step=42000, RunningAvgSamplesPerSec=28.232510958513032, CurrSamplesPerSec=34.92146777762282, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [2680/2809]  eta: 0:01:08  lr: 0.000038  min_lr: 0.000000  loss: 4.0307 (3.9545)  loss_scale: 65536.0000 (60524.8549)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [14]  [2690/2809]  eta: 0:01:02  lr: 0.000038  min_lr: 0.000000  loss: 3.9215 (3.9541)  loss_scale: 65536.0000 (60543.4768)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [14]  [2700/2809]  eta: 0:00:57  lr: 0.000038  min_lr: 0.000000  loss: 4.0217 (3.9546)  loss_scale: 65536.0000 (60561.9608)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [14]  [2710/2809]  eta: 0:00:52  lr: 0.000038  min_lr: 0.000000  loss: 4.0389 (3.9548)  loss_scale: 65536.0000 (60580.3084)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [14]  [2720/2809]  eta: 0:00:46  lr: 0.000038  min_lr: 0.000000  loss: 4.0389 (3.9558)  loss_scale: 65536.0000 (60598.5211)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [14]  [2730/2809]  eta: 0:00:41  lr: 0.000038  min_lr: 0.000000  loss: 4.1905 (3.9567)  loss_scale: 65536.0000 (60616.6005)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
[2025-01-13 03:35:32,318] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:35:32,318] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:35:32,682] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42060
[2025-01-13 03:35:32,682] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:35:32,682] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [2740/2809]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000000  loss: 3.9383 (3.9559)  loss_scale: 65536.0000 (60658.4575)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [14]  [2750/2809]  eta: 0:00:30  lr: 0.000038  min_lr: 0.000000  loss: 3.9383 (3.9561)  loss_scale: 65536.0000 (60676.1876)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [14]  [2760/2809]  eta: 0:00:25  lr: 0.000038  min_lr: 0.000000  loss: 3.9535 (3.9549)  loss_scale: 65536.0000 (60693.7892)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [14]  [2770/2809]  eta: 0:00:20  lr: 0.000038  min_lr: 0.000000  loss: 3.9102 (3.9551)  loss_scale: 65536.0000 (60711.2638)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [14]  [2780/2809]  eta: 0:00:15  lr: 0.000038  min_lr: 0.000000  loss: 3.9102 (3.9550)  loss_scale: 65536.0000 (60728.6127)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [14]  [2790/2809]  eta: 0:00:09  lr: 0.000038  min_lr: 0.000000  loss: 3.9411 (3.9554)  loss_scale: 65536.0000 (60745.8373)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [14]  [2800/2809]  eta: 0:00:04  lr: 0.000038  min_lr: 0.000000  loss: 4.0879 (3.9557)  loss_scale: 65536.0000 (60762.9390)  weight_decay: 0.0500 (0.0500)  time: 0.3649  data: 0.0002  max mem: 15572
Epoch: [14]  [2808/2809]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000000  loss: 4.1195 (3.9560)  loss_scale: 65536.0000 (60776.5326)  weight_decay: 0.0500 (0.0500)  time: 0.3610  data: 0.0001  max mem: 15572
Epoch: [14] Total time: 0:24:22 (0.5208 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000000  loss: 4.1195 (3.9560)  loss_scale: 65536.0000 (60776.5326)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:10:56  loss: 0.4037 (0.4037)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4126  data: 2.2143  max mem: 15572
Val:  [ 10/272]  eta: 0:01:48  loss: 3.0202 (2.6542)  acc1: 22.2222 (31.8182)  acc5: 66.6667 (62.6263)  time: 0.4152  data: 0.2534  max mem: 15572
Val:  [ 20/272]  eta: 0:01:14  loss: 2.8860 (2.7369)  acc1: 27.7778 (34.1270)  acc5: 66.6667 (65.3439)  time: 0.1887  data: 0.0289  max mem: 15572
Val:  [ 30/272]  eta: 0:01:00  loss: 2.8860 (2.8174)  acc1: 27.7778 (31.8996)  acc5: 66.6667 (64.6953)  time: 0.1600  data: 0.0005  max mem: 15572
Val:  [ 40/272]  eta: 0:00:52  loss: 3.0026 (2.8452)  acc1: 22.2222 (30.2168)  acc5: 66.6667 (65.8537)  time: 0.1578  data: 0.0005  max mem: 15572
Val:  [ 50/272]  eta: 0:00:47  loss: 2.8909 (2.7676)  acc1: 27.7778 (32.4619)  acc5: 72.2222 (68.0828)  time: 0.1549  data: 0.0005  max mem: 15572
Val:  [ 60/272]  eta: 0:00:43  loss: 1.6947 (2.6108)  acc1: 55.5556 (37.2495)  acc5: 83.3333 (69.4900)  time: 0.1587  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:40  loss: 1.6489 (2.5297)  acc1: 66.6667 (39.2019)  acc5: 83.3333 (71.3615)  time: 0.1613  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:36  loss: 2.3767 (2.5399)  acc1: 38.8889 (39.0261)  acc5: 77.7778 (71.3306)  time: 0.1533  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.8300 (2.5838)  acc1: 33.3333 (38.2784)  acc5: 72.2222 (71.0012)  time: 0.1531  data: 0.0003  max mem: 15572
Val:  [100/272]  eta: 0:00:31  loss: 2.8300 (2.6267)  acc1: 33.3333 (37.9538)  acc5: 72.2222 (70.4620)  time: 0.1551  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:29  loss: 2.9029 (2.7084)  acc1: 11.1111 (35.7357)  acc5: 61.1111 (68.7688)  time: 0.1611  data: 0.0003  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 3.3472 (2.7491)  acc1: 11.1111 (34.6648)  acc5: 55.5556 (67.8145)  time: 0.1640  data: 0.0003  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.7021 (2.7064)  acc1: 27.7778 (35.7930)  acc5: 72.2222 (68.4478)  time: 0.1570  data: 0.0003  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.8849 (2.6883)  acc1: 55.5556 (36.6430)  acc5: 72.2222 (68.7155)  time: 0.1641  data: 0.0019  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.7110 (2.6939)  acc1: 33.3333 (36.1295)  acc5: 72.2222 (68.9478)  time: 0.1729  data: 0.0035  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.7021 (2.6719)  acc1: 33.3333 (37.3361)  acc5: 72.2222 (69.5307)  time: 0.1631  data: 0.0020  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.7503 (2.7011)  acc1: 33.3333 (36.8096)  acc5: 72.2222 (68.8434)  time: 0.1579  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 2.6689 (2.6877)  acc1: 33.3333 (37.0780)  acc5: 72.2222 (69.3063)  time: 0.1604  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.7642 (2.7333)  acc1: 22.2222 (35.7475)  acc5: 66.6667 (67.9174)  time: 0.1570  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.8099 (2.7436)  acc1: 22.2222 (35.6274)  acc5: 55.5556 (67.8552)  time: 0.1569  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.2702 (2.7359)  acc1: 38.8889 (36.3086)  acc5: 77.7778 (68.0885)  time: 0.1572  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.2576 (2.7225)  acc1: 44.4444 (36.6767)  acc5: 77.7778 (68.2504)  time: 0.1565  data: 0.0003  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.9536 (2.6879)  acc1: 55.5556 (37.9028)  acc5: 77.7778 (68.8312)  time: 0.1561  data: 0.0003  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.7448 (2.6695)  acc1: 61.1111 (38.4740)  acc5: 83.3333 (69.2485)  time: 0.1567  data: 0.0003  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.7699 (2.6830)  acc1: 38.8889 (37.9150)  acc5: 72.2222 (68.9464)  time: 0.1555  data: 0.0003  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.6669 (2.6126)  acc1: 72.2222 (39.9106)  acc5: 83.3333 (69.8595)  time: 0.1502  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4602 (2.6117)  acc1: 66.6667 (39.6679)  acc5: 88.8889 (69.8442)  time: 0.1403  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4602 (2.6159)  acc1: 61.1111 (39.6682)  acc5: 88.8889 (69.8136)  time: 0.1346  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1679 s / it)
* Acc@1 39.668 Acc@5 69.814 loss 2.616
Accuracy of the network on the 4883 val videos: 39.7%
[2025-01-13 03:36:45,592] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 03:36:45,593] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 03:36:45,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 03:36:47,924] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 03:36:47,924] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 39.67%
Epoch: [15]  [   0/2809]  eta: 2:52:09  lr: 0.000038  min_lr: 0.000000  loss: 3.9184 (3.9184)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.6771  data: 3.2906  max mem: 15572
Epoch: [15]  [  10/2809]  eta: 0:31:51  lr: 0.000038  min_lr: 0.000000  loss: 3.8813 (3.8361)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6830  data: 0.2994  max mem: 15572
Epoch: [15]  [  20/2809]  eta: 0:24:56  lr: 0.000038  min_lr: 0.000000  loss: 4.0915 (4.0054)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3795  data: 0.0003  max mem: 15572
Epoch: [15]  [  30/2809]  eta: 0:22:28  lr: 0.000038  min_lr: 0.000000  loss: 3.9428 (3.9103)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
Epoch: [15]  [  40/2809]  eta: 0:21:06  lr: 0.000038  min_lr: 0.000000  loss: 3.8394 (3.9253)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [15]  [  50/2809]  eta: 0:20:16  lr: 0.000038  min_lr: 0.000000  loss: 3.9143 (3.9319)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
[2025-01-13 03:37:11,903] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:37:11,903] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:37:12,986] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42192
[2025-01-13 03:37:12,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:37:12,987] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [  60/2809]  eta: 0:19:38  lr: 0.000038  min_lr: 0.000000  loss: 4.1613 (3.9828)  loss_scale: 65536.0000 (68759.0820)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [15]  [  70/2809]  eta: 0:19:10  lr: 0.000038  min_lr: 0.000000  loss: 4.2429 (4.0028)  loss_scale: 65536.0000 (68305.1268)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [15]  [  80/2809]  eta: 0:18:50  lr: 0.000038  min_lr: 0.000000  loss: 3.8678 (3.9907)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [15]  [  90/2809]  eta: 0:18:32  lr: 0.000038  min_lr: 0.000000  loss: 3.8584 (3.9896)  loss_scale: 65536.0000 (67696.5275)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [15]  [ 100/2809]  eta: 0:18:16  lr: 0.000038  min_lr: 0.000000  loss: 4.1444 (4.0067)  loss_scale: 65536.0000 (67482.6139)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [15]  [ 110/2809]  eta: 0:18:03  lr: 0.000038  min_lr: 0.000000  loss: 4.1413 (3.9894)  loss_scale: 65536.0000 (67307.2432)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [15]  [ 120/2809]  eta: 0:17:51  lr: 0.000038  min_lr: 0.000000  loss: 3.8334 (3.9803)  loss_scale: 65536.0000 (67160.8595)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [15]  [ 130/2809]  eta: 0:17:41  lr: 0.000038  min_lr: 0.000000  loss: 3.9234 (3.9734)  loss_scale: 65536.0000 (67036.8244)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [15]  [ 140/2809]  eta: 0:17:31  lr: 0.000038  min_lr: 0.000000  loss: 3.8518 (3.9533)  loss_scale: 65536.0000 (66930.3830)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0001  max mem: 15572
Epoch: [15]  [ 150/2809]  eta: 0:17:22  lr: 0.000038  min_lr: 0.000000  loss: 3.8518 (3.9524)  loss_scale: 65536.0000 (66838.0397)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [15]  [ 160/2809]  eta: 0:17:15  lr: 0.000038  min_lr: 0.000000  loss: 3.9037 (3.9439)  loss_scale: 65536.0000 (66757.1677)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [15]  [ 170/2809]  eta: 0:17:07  lr: 0.000038  min_lr: 0.000000  loss: 3.8766 (3.9480)  loss_scale: 65536.0000 (66685.7544)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [15]  [ 180/2809]  eta: 0:17:00  lr: 0.000038  min_lr: 0.000000  loss: 3.8564 (3.9447)  loss_scale: 65536.0000 (66622.2320)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
[2025-01-13 03:38:00,419] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:38:00,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:38:01,877] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42325
[2025-01-13 03:38:01,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:38:01,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 190/2809]  eta: 0:16:53  lr: 0.000038  min_lr: 0.000000  loss: 3.9255 (3.9527)  loss_scale: 65536.0000 (67937.8429)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [15]  [ 200/2809]  eta: 0:16:46  lr: 0.000038  min_lr: 0.000000  loss: 3.8849 (3.9479)  loss_scale: 65536.0000 (67818.3483)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [15]  [ 210/2809]  eta: 0:16:40  lr: 0.000038  min_lr: 0.000000  loss: 3.9166 (3.9511)  loss_scale: 65536.0000 (67710.1801)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [15]  [ 220/2809]  eta: 0:16:34  lr: 0.000038  min_lr: 0.000000  loss: 3.9626 (3.9421)  loss_scale: 65536.0000 (67611.8009)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [15]  [ 230/2809]  eta: 0:16:29  lr: 0.000038  min_lr: 0.000000  loss: 3.9197 (3.9329)  loss_scale: 65536.0000 (67521.9394)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [15]  [ 240/2809]  eta: 0:16:23  lr: 0.000038  min_lr: 0.000000  loss: 3.9395 (3.9336)  loss_scale: 65536.0000 (67439.5353)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0001  max mem: 15572
Epoch: [15]  [ 250/2809]  eta: 0:16:18  lr: 0.000038  min_lr: 0.000000  loss: 4.0031 (3.9316)  loss_scale: 65536.0000 (67363.6972)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [15]  [ 260/2809]  eta: 0:16:13  lr: 0.000038  min_lr: 0.000000  loss: 4.0449 (3.9386)  loss_scale: 65536.0000 (67293.6705)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [15]  [ 270/2809]  eta: 0:16:07  lr: 0.000038  min_lr: 0.000000  loss: 4.0329 (3.9387)  loss_scale: 65536.0000 (67228.8118)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [15]  [ 280/2809]  eta: 0:16:02  lr: 0.000038  min_lr: 0.000000  loss: 3.8458 (3.9345)  loss_scale: 65536.0000 (67168.5694)  weight_decay: 0.0500 (0.0500)  time: 0.3650  data: 0.0002  max mem: 15572
Epoch: [15]  [ 290/2809]  eta: 0:15:57  lr: 0.000038  min_lr: 0.000000  loss: 3.7642 (3.9355)  loss_scale: 65536.0000 (67112.4674)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [15]  [ 300/2809]  eta: 0:15:52  lr: 0.000038  min_lr: 0.000000  loss: 4.0180 (3.9330)  loss_scale: 65536.0000 (67060.0930)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [15]  [ 310/2809]  eta: 0:15:47  lr: 0.000038  min_lr: 0.000000  loss: 3.8019 (3.9267)  loss_scale: 65536.0000 (67011.0868)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
[2025-01-13 03:38:49,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:38:49,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:38:49,607] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42455
[2025-01-13 03:38:49,607] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:38:49,607] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 320/2809]  eta: 0:15:43  lr: 0.000038  min_lr: 0.000000  loss: 3.8019 (3.9282)  loss_scale: 65536.0000 (67169.2960)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [15]  [ 330/2809]  eta: 0:15:38  lr: 0.000038  min_lr: 0.000000  loss: 4.0986 (3.9368)  loss_scale: 65536.0000 (67119.9517)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [15]  [ 340/2809]  eta: 0:15:33  lr: 0.000038  min_lr: 0.000000  loss: 4.0737 (3.9335)  loss_scale: 65536.0000 (67073.5015)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [15]  [ 350/2809]  eta: 0:15:29  lr: 0.000038  min_lr: 0.000000  loss: 3.9291 (3.9377)  loss_scale: 65536.0000 (67029.6980)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [15]  [ 360/2809]  eta: 0:15:24  lr: 0.000038  min_lr: 0.000000  loss: 4.0723 (3.9353)  loss_scale: 65536.0000 (66988.3213)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [15]  [ 370/2809]  eta: 0:15:20  lr: 0.000038  min_lr: 0.000000  loss: 4.2475 (3.9459)  loss_scale: 65536.0000 (66949.1752)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0001  max mem: 15572
Epoch: [15]  [ 380/2809]  eta: 0:15:16  lr: 0.000038  min_lr: 0.000000  loss: 4.2058 (3.9459)  loss_scale: 65536.0000 (66912.0840)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [15]  [ 390/2809]  eta: 0:15:11  lr: 0.000038  min_lr: 0.000000  loss: 3.9988 (3.9480)  loss_scale: 65536.0000 (66876.8900)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [15]  [ 400/2809]  eta: 0:15:07  lr: 0.000038  min_lr: 0.000000  loss: 3.9967 (3.9461)  loss_scale: 65536.0000 (66843.4514)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [15]  [ 410/2809]  eta: 0:15:03  lr: 0.000038  min_lr: 0.000000  loss: 4.1346 (3.9492)  loss_scale: 65536.0000 (66811.6399)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [15]  [ 420/2809]  eta: 0:14:59  lr: 0.000038  min_lr: 0.000000  loss: 3.8148 (3.9421)  loss_scale: 65536.0000 (66781.3397)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [15]  [ 430/2809]  eta: 0:14:54  lr: 0.000038  min_lr: 0.000000  loss: 3.4902 (3.9301)  loss_scale: 65536.0000 (66752.4455)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [15]  [ 440/2809]  eta: 0:14:50  lr: 0.000038  min_lr: 0.000000  loss: 3.6711 (3.9308)  loss_scale: 65536.0000 (66724.8617)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 03:39:37,111] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:39:37,111] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [ 450/2809]  eta: 0:14:46  lr: 0.000038  min_lr: 0.000000  loss: 3.9640 (3.9320)  loss_scale: 65536.0000 (66989.1264)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
[2025-01-13 03:39:37,862] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42586
[2025-01-13 03:39:37,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:39:37,862] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 460/2809]  eta: 0:14:42  lr: 0.000038  min_lr: 0.000000  loss: 3.8872 (3.9302)  loss_scale: 65536.0000 (66957.6052)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [15]  [ 470/2809]  eta: 0:14:38  lr: 0.000038  min_lr: 0.000000  loss: 3.8295 (3.9262)  loss_scale: 65536.0000 (66927.4225)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [15]  [ 480/2809]  eta: 0:14:33  lr: 0.000038  min_lr: 0.000000  loss: 3.7412 (3.9228)  loss_scale: 65536.0000 (66898.4948)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [15]  [ 490/2809]  eta: 0:14:29  lr: 0.000038  min_lr: 0.000000  loss: 3.8260 (3.9235)  loss_scale: 65536.0000 (66870.7454)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0014  max mem: 15572
Epoch: [15]  [ 500/2809]  eta: 0:14:25  lr: 0.000038  min_lr: 0.000000  loss: 4.0996 (3.9283)  loss_scale: 65536.0000 (66844.1038)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0014  max mem: 15572
Epoch: [15]  [ 510/2809]  eta: 0:14:21  lr: 0.000038  min_lr: 0.000000  loss: 4.0996 (3.9284)  loss_scale: 65536.0000 (66818.5049)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [15]  [ 520/2809]  eta: 0:14:17  lr: 0.000038  min_lr: 0.000000  loss: 4.1509 (3.9321)  loss_scale: 65536.0000 (66793.8887)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0003  max mem: 15572
Epoch: [15]  [ 530/2809]  eta: 0:14:13  lr: 0.000038  min_lr: 0.000000  loss: 4.0657 (3.9302)  loss_scale: 65536.0000 (66770.1996)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [15]  [ 540/2809]  eta: 0:14:09  lr: 0.000038  min_lr: 0.000000  loss: 3.7401 (3.9283)  loss_scale: 65536.0000 (66747.3863)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [15]  [ 550/2809]  eta: 0:14:05  lr: 0.000038  min_lr: 0.000000  loss: 3.8867 (3.9292)  loss_scale: 65536.0000 (66725.4011)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [15]  [ 560/2809]  eta: 0:14:01  lr: 0.000038  min_lr: 0.000000  loss: 3.9034 (3.9253)  loss_scale: 65536.0000 (66704.1996)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [15]  [ 570/2809]  eta: 0:13:57  lr: 0.000038  min_lr: 0.000000  loss: 3.8639 (3.9258)  loss_scale: 65536.0000 (66683.7408)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
[2025-01-13 03:40:25,299] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:40:25,299] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [ 580/2809]  eta: 0:13:53  lr: 0.000038  min_lr: 0.000000  loss: 3.8639 (3.9276)  loss_scale: 65536.0000 (66776.7849)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
[2025-01-13 03:40:26,026] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42717
[2025-01-13 03:40:26,026] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:40:26,026] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 590/2809]  eta: 0:13:49  lr: 0.000038  min_lr: 0.000000  loss: 3.8191 (3.9260)  loss_scale: 65536.0000 (66866.6802)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [15]  [ 600/2809]  eta: 0:13:45  lr: 0.000038  min_lr: 0.000000  loss: 4.0991 (3.9304)  loss_scale: 65536.0000 (66844.5391)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0014  max mem: 15572
Epoch: [15]  [ 610/2809]  eta: 0:13:42  lr: 0.000038  min_lr: 0.000000  loss: 4.1260 (3.9321)  loss_scale: 65536.0000 (66823.1227)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0014  max mem: 15572
Epoch: [15]  [ 620/2809]  eta: 0:13:38  lr: 0.000038  min_lr: 0.000000  loss: 4.0150 (3.9326)  loss_scale: 65536.0000 (66802.3961)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [15]  [ 630/2809]  eta: 0:13:34  lr: 0.000038  min_lr: 0.000000  loss: 4.0208 (3.9341)  loss_scale: 65536.0000 (66782.3265)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [15]  [ 640/2809]  eta: 0:13:30  lr: 0.000038  min_lr: 0.000000  loss: 3.9857 (3.9346)  loss_scale: 65536.0000 (66762.8830)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [15]  [ 650/2809]  eta: 0:13:26  lr: 0.000038  min_lr: 0.000000  loss: 3.9024 (3.9315)  loss_scale: 65536.0000 (66744.0369)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [15]  [ 660/2809]  eta: 0:13:22  lr: 0.000038  min_lr: 0.000000  loss: 3.8210 (3.9298)  loss_scale: 65536.0000 (66725.7610)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [15]  [ 670/2809]  eta: 0:13:18  lr: 0.000038  min_lr: 0.000000  loss: 4.0146 (3.9323)  loss_scale: 65536.0000 (66708.0298)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [15]  [ 680/2809]  eta: 0:13:14  lr: 0.000038  min_lr: 0.000000  loss: 4.1180 (3.9318)  loss_scale: 65536.0000 (66690.8194)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0003  max mem: 15572
Epoch: [15]  [ 690/2809]  eta: 0:13:10  lr: 0.000038  min_lr: 0.000000  loss: 4.1803 (3.9359)  loss_scale: 65536.0000 (66674.1071)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [15]  [ 700/2809]  eta: 0:13:07  lr: 0.000038  min_lr: 0.000000  loss: 4.1398 (3.9369)  loss_scale: 65536.0000 (66657.8716)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [15]  [ 710/2809]  eta: 0:13:03  lr: 0.000038  min_lr: 0.000000  loss: 3.9021 (3.9348)  loss_scale: 65536.0000 (66642.0928)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
[2025-01-13 03:41:13,674] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:41:13,674] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:41:14,032] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42847
[2025-01-13 03:41:14,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:41:14,032] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 720/2809]  eta: 0:12:59  lr: 0.000038  min_lr: 0.000000  loss: 3.8641 (3.9343)  loss_scale: 65536.0000 (66717.6477)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [15]  [ 730/2809]  eta: 0:12:55  lr: 0.000038  min_lr: 0.000000  loss: 4.0584 (3.9337)  loss_scale: 65536.0000 (66701.4829)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [15]  [ 740/2809]  eta: 0:12:51  lr: 0.000038  min_lr: 0.000000  loss: 4.0809 (3.9348)  loss_scale: 65536.0000 (66685.7544)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [15]  [ 750/2809]  eta: 0:12:47  lr: 0.000038  min_lr: 0.000000  loss: 3.7410 (3.9320)  loss_scale: 65536.0000 (66670.4447)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
[2025-01-13 03:41:30,693] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42892
[2025-01-13 03:41:30,693] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 03:41:30,693] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [ 760/2809]  eta: 0:12:43  lr: 0.000038  min_lr: 0.000000  loss: 3.7222 (3.9321)  loss_scale: 65536.0000 (66483.3009)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [15]  [ 770/2809]  eta: 0:12:40  lr: 0.000038  min_lr: 0.000000  loss: 4.0203 (3.9342)  loss_scale: 32768.0000 (66046.0078)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [15]  [ 780/2809]  eta: 0:12:36  lr: 0.000038  min_lr: 0.000000  loss: 4.0987 (3.9327)  loss_scale: 32768.0000 (65619.9129)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [15]  [ 790/2809]  eta: 0:12:32  lr: 0.000038  min_lr: 0.000000  loss: 3.5410 (3.9299)  loss_scale: 32768.0000 (65204.5917)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [15]  [ 800/2809]  eta: 0:12:28  lr: 0.000038  min_lr: 0.000000  loss: 3.7281 (3.9302)  loss_scale: 32768.0000 (64799.6404)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [15]  [ 810/2809]  eta: 0:12:24  lr: 0.000038  min_lr: 0.000000  loss: 3.5556 (3.9247)  loss_scale: 32768.0000 (64404.6757)  weight_decay: 0.0500 (0.0500)  time: 0.3644  data: 0.0002  max mem: 15572
Epoch: [15]  [ 820/2809]  eta: 0:12:20  lr: 0.000038  min_lr: 0.000000  loss: 3.5988 (3.9229)  loss_scale: 32768.0000 (64019.3325)  weight_decay: 0.0500 (0.0500)  time: 0.3641  data: 0.0002  max mem: 15572
Epoch: [15]  [ 830/2809]  eta: 0:12:16  lr: 0.000038  min_lr: 0.000000  loss: 3.8121 (3.9227)  loss_scale: 32768.0000 (63643.2635)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [15]  [ 840/2809]  eta: 0:12:12  lr: 0.000038  min_lr: 0.000000  loss: 3.9417 (3.9212)  loss_scale: 32768.0000 (63276.1379)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [15]  [ 850/2809]  eta: 0:12:09  lr: 0.000038  min_lr: 0.000000  loss: 3.9279 (3.9213)  loss_scale: 32768.0000 (62917.6404)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [15]  [ 860/2809]  eta: 0:12:05  lr: 0.000038  min_lr: 0.000000  loss: 3.9279 (3.9183)  loss_scale: 32768.0000 (62567.4704)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 03:42:09,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=43000, skipped=288, lr=[3.6380340292923513e-07, 3.6380340292923513e-07, 5.197191470417645e-07, 5.197191470417645e-07, 7.42455924345378e-07, 7.42455924345378e-07, 1.0606513204933972e-06, 1.0606513204933972e-06, 1.5152161721334248e-06, 1.5152161721334248e-06, 2.1645945316191783e-06, 2.1645945316191783e-06, 3.0922779023131118e-06, 3.0922779023131118e-06, 4.417539860447303e-06, 4.417539860447303e-06, 6.310771229210433e-06, 6.310771229210433e-06, 9.01538747030062e-06, 9.01538747030062e-06, 1.2879124957572313e-05, 1.2879124957572313e-05, 1.839874993938902e-05, 1.839874993938902e-05, 2.628392848484146e-05, 2.628392848484146e-05, 3.754846926405923e-05, 3.754846926405923e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 03:42:09,993] [INFO] [timer.py:260:stop] epoch=0/micro_step=43000/global_step=43000, RunningAvgSamplesPerSec=28.351664254407133, CurrSamplesPerSec=33.1254137250424, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [15]  [ 870/2809]  eta: 0:12:01  lr: 0.000038  min_lr: 0.000000  loss: 4.0186 (3.9210)  loss_scale: 32768.0000 (62225.3410)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [15]  [ 880/2809]  eta: 0:11:57  lr: 0.000038  min_lr: 0.000000  loss: 4.0183 (3.9213)  loss_scale: 32768.0000 (61890.9784)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 03:42:18,080] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:42:18,080] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [ 890/2809]  eta: 0:11:53  lr: 0.000038  min_lr: 0.000000  loss: 3.9600 (3.9201)  loss_scale: 32768.0000 (61748.0045)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [15]  [ 900/2809]  eta: 0:11:50  lr: 0.000038  min_lr: 0.000000  loss: 4.1723 (3.9210)  loss_scale: 65536.0000 (61790.0466)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [15]  [ 910/2809]  eta: 0:11:46  lr: 0.000038  min_lr: 0.000000  loss: 4.1793 (3.9241)  loss_scale: 65536.0000 (61831.1658)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [15]  [ 920/2809]  eta: 0:11:42  lr: 0.000038  min_lr: 0.000000  loss: 4.0970 (3.9235)  loss_scale: 65536.0000 (61871.3920)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [15]  [ 930/2809]  eta: 0:11:39  lr: 0.000038  min_lr: 0.000000  loss: 3.9331 (3.9238)  loss_scale: 65536.0000 (61910.7540)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [15]  [ 940/2809]  eta: 0:11:35  lr: 0.000038  min_lr: 0.000000  loss: 4.0412 (3.9249)  loss_scale: 65536.0000 (61949.2795)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [15]  [ 950/2809]  eta: 0:11:31  lr: 0.000037  min_lr: 0.000000  loss: 4.1426 (3.9254)  loss_scale: 65536.0000 (61986.9947)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [15]  [ 960/2809]  eta: 0:11:27  lr: 0.000037  min_lr: 0.000000  loss: 3.9578 (3.9246)  loss_scale: 65536.0000 (62023.9251)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [15]  [ 970/2809]  eta: 0:11:23  lr: 0.000037  min_lr: 0.000000  loss: 3.8708 (3.9241)  loss_scale: 65536.0000 (62060.0947)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [15]  [ 980/2809]  eta: 0:11:20  lr: 0.000037  min_lr: 0.000000  loss: 4.1286 (3.9270)  loss_scale: 65536.0000 (62095.5270)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [15]  [ 990/2809]  eta: 0:11:16  lr: 0.000037  min_lr: 0.000000  loss: 3.9291 (3.9250)  loss_scale: 65536.0000 (62130.2442)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [15]  [1000/2809]  eta: 0:11:12  lr: 0.000037  min_lr: 0.000000  loss: 3.6918 (3.9225)  loss_scale: 65536.0000 (62164.2677)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [15]  [1010/2809]  eta: 0:11:08  lr: 0.000037  min_lr: 0.000000  loss: 3.9291 (3.9234)  loss_scale: 65536.0000 (62197.6182)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 03:43:05,407] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:43:05,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:43:06,858] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43153
[2025-01-13 03:43:06,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:43:06,858] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1020/2809]  eta: 0:11:04  lr: 0.000037  min_lr: 0.000000  loss: 4.0878 (3.9230)  loss_scale: 65536.0000 (62487.0676)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [15]  [1030/2809]  eta: 0:11:01  lr: 0.000037  min_lr: 0.000000  loss: 3.9127 (3.9216)  loss_scale: 65536.0000 (62516.6402)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [15]  [1040/2809]  eta: 0:10:57  lr: 0.000037  min_lr: 0.000000  loss: 4.1323 (3.9248)  loss_scale: 65536.0000 (62545.6446)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [15]  [1050/2809]  eta: 0:10:53  lr: 0.000037  min_lr: 0.000000  loss: 4.2243 (3.9263)  loss_scale: 65536.0000 (62574.0971)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [15]  [1060/2809]  eta: 0:10:49  lr: 0.000037  min_lr: 0.000000  loss: 4.1336 (3.9273)  loss_scale: 65536.0000 (62602.0132)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0001  max mem: 15572
Epoch: [15]  [1070/2809]  eta: 0:10:46  lr: 0.000037  min_lr: 0.000000  loss: 4.1295 (3.9283)  loss_scale: 65536.0000 (62629.4080)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [15]  [1080/2809]  eta: 0:10:42  lr: 0.000037  min_lr: 0.000000  loss: 3.8748 (3.9286)  loss_scale: 65536.0000 (62656.2960)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [15]  [1090/2809]  eta: 0:10:38  lr: 0.000037  min_lr: 0.000000  loss: 3.9855 (3.9292)  loss_scale: 65536.0000 (62682.6911)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [15]  [1100/2809]  eta: 0:10:34  lr: 0.000037  min_lr: 0.000000  loss: 4.1042 (3.9298)  loss_scale: 65536.0000 (62708.6067)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [15]  [1110/2809]  eta: 0:10:31  lr: 0.000037  min_lr: 0.000000  loss: 4.1561 (3.9316)  loss_scale: 65536.0000 (62734.0558)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [15]  [1120/2809]  eta: 0:10:27  lr: 0.000037  min_lr: 0.000000  loss: 4.1576 (3.9336)  loss_scale: 65536.0000 (62759.0508)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [15]  [1130/2809]  eta: 0:10:23  lr: 0.000037  min_lr: 0.000000  loss: 4.0745 (3.9342)  loss_scale: 65536.0000 (62783.6039)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [15]  [1140/2809]  eta: 0:10:19  lr: 0.000037  min_lr: 0.000000  loss: 3.9634 (3.9342)  loss_scale: 65536.0000 (62807.7266)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
[2025-01-13 03:43:54,488] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:43:54,488] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [1150/2809]  eta: 0:10:16  lr: 0.000037  min_lr: 0.000000  loss: 3.9059 (3.9356)  loss_scale: 65536.0000 (63059.1833)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
[2025-01-13 03:43:55,970] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43286
[2025-01-13 03:43:55,971] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:43:55,971] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1160/2809]  eta: 0:10:12  lr: 0.000037  min_lr: 0.000000  loss: 4.2395 (3.9376)  loss_scale: 65536.0000 (63080.5168)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [15]  [1170/2809]  eta: 0:10:08  lr: 0.000037  min_lr: 0.000000  loss: 4.1998 (3.9359)  loss_scale: 65536.0000 (63101.4859)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [15]  [1180/2809]  eta: 0:10:04  lr: 0.000037  min_lr: 0.000000  loss: 4.1743 (3.9382)  loss_scale: 65536.0000 (63122.0999)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [15]  [1190/2809]  eta: 0:10:01  lr: 0.000037  min_lr: 0.000000  loss: 4.0691 (3.9369)  loss_scale: 65536.0000 (63142.3678)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [15]  [1200/2809]  eta: 0:09:57  lr: 0.000037  min_lr: 0.000000  loss: 4.0152 (3.9372)  loss_scale: 65536.0000 (63162.2981)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [15]  [1210/2809]  eta: 0:09:53  lr: 0.000037  min_lr: 0.000000  loss: 4.0967 (3.9387)  loss_scale: 65536.0000 (63181.8993)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [15]  [1220/2809]  eta: 0:09:50  lr: 0.000037  min_lr: 0.000000  loss: 4.0578 (3.9380)  loss_scale: 65536.0000 (63201.1794)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [15]  [1230/2809]  eta: 0:09:46  lr: 0.000037  min_lr: 0.000000  loss: 3.9140 (3.9376)  loss_scale: 65536.0000 (63220.1462)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [15]  [1240/2809]  eta: 0:09:42  lr: 0.000037  min_lr: 0.000000  loss: 3.8539 (3.9367)  loss_scale: 65536.0000 (63238.8074)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [15]  [1250/2809]  eta: 0:09:38  lr: 0.000037  min_lr: 0.000000  loss: 3.9462 (3.9374)  loss_scale: 65536.0000 (63257.1703)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [15]  [1260/2809]  eta: 0:09:35  lr: 0.000037  min_lr: 0.000000  loss: 3.9634 (3.9364)  loss_scale: 65536.0000 (63275.2419)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [15]  [1270/2809]  eta: 0:09:31  lr: 0.000037  min_lr: 0.000000  loss: 3.9792 (3.9374)  loss_scale: 65536.0000 (63293.0291)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
[2025-01-13 03:44:43,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:44:43,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [1280/2809]  eta: 0:09:27  lr: 0.000037  min_lr: 0.000000  loss: 4.0027 (3.9369)  loss_scale: 65536.0000 (63361.6987)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
[2025-01-13 03:44:44,386] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43417
[2025-01-13 03:44:44,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:44:44,386] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1290/2809]  eta: 0:09:23  lr: 0.000037  min_lr: 0.000000  loss: 4.0775 (3.9384)  loss_scale: 65536.0000 (63429.3044)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [15]  [1300/2809]  eta: 0:09:20  lr: 0.000037  min_lr: 0.000000  loss: 4.1314 (3.9397)  loss_scale: 65536.0000 (63445.4973)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [15]  [1310/2809]  eta: 0:09:16  lr: 0.000037  min_lr: 0.000000  loss: 3.9372 (3.9396)  loss_scale: 65536.0000 (63461.4432)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [15]  [1320/2809]  eta: 0:09:12  lr: 0.000037  min_lr: 0.000000  loss: 3.9706 (3.9409)  loss_scale: 65536.0000 (63477.1476)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [15]  [1330/2809]  eta: 0:09:08  lr: 0.000037  min_lr: 0.000000  loss: 3.9380 (3.9400)  loss_scale: 65536.0000 (63492.6161)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
[2025-01-13 03:45:04,618] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43472
[2025-01-13 03:45:04,619] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 03:45:04,619] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [1340/2809]  eta: 0:09:05  lr: 0.000037  min_lr: 0.000000  loss: 3.8863 (3.9400)  loss_scale: 65536.0000 (63410.1119)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [15]  [1350/2809]  eta: 0:09:01  lr: 0.000037  min_lr: 0.000000  loss: 3.8897 (3.9383)  loss_scale: 32768.0000 (63183.3013)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [15]  [1360/2809]  eta: 0:08:57  lr: 0.000037  min_lr: 0.000000  loss: 3.6118 (3.9379)  loss_scale: 32768.0000 (62959.8237)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [15]  [1370/2809]  eta: 0:08:53  lr: 0.000037  min_lr: 0.000000  loss: 3.7439 (3.9378)  loss_scale: 32768.0000 (62739.6061)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [15]  [1380/2809]  eta: 0:08:50  lr: 0.000037  min_lr: 0.000000  loss: 3.8962 (3.9383)  loss_scale: 32768.0000 (62522.5778)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [15]  [1390/2809]  eta: 0:08:46  lr: 0.000037  min_lr: 0.000000  loss: 3.9663 (3.9390)  loss_scale: 32768.0000 (62308.6700)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [15]  [1400/2809]  eta: 0:08:42  lr: 0.000037  min_lr: 0.000000  loss: 3.8002 (3.9383)  loss_scale: 32768.0000 (62097.8158)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [15]  [1410/2809]  eta: 0:08:39  lr: 0.000037  min_lr: 0.000000  loss: 3.6799 (3.9375)  loss_scale: 32768.0000 (61889.9504)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [15]  [1420/2809]  eta: 0:08:35  lr: 0.000037  min_lr: 0.000000  loss: 4.0372 (3.9395)  loss_scale: 32768.0000 (61685.0106)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [15]  [1430/2809]  eta: 0:08:31  lr: 0.000037  min_lr: 0.000000  loss: 4.0942 (3.9388)  loss_scale: 32768.0000 (61482.9350)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [15]  [1440/2809]  eta: 0:08:27  lr: 0.000037  min_lr: 0.000000  loss: 3.9653 (3.9382)  loss_scale: 32768.0000 (61283.6641)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [15]  [1450/2809]  eta: 0:08:24  lr: 0.000037  min_lr: 0.000000  loss: 3.9657 (3.9376)  loss_scale: 32768.0000 (61087.1399)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [15]  [1460/2809]  eta: 0:08:20  lr: 0.000037  min_lr: 0.000000  loss: 3.9631 (3.9377)  loss_scale: 32768.0000 (60893.3060)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
[2025-01-13 03:45:52,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:45:52,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [1470/2809]  eta: 0:08:16  lr: 0.000037  min_lr: 0.000000  loss: 3.9631 (3.9381)  loss_scale: 32768.0000 (60813.4874)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [15]  [1480/2809]  eta: 0:08:13  lr: 0.000037  min_lr: 0.000000  loss: 3.9162 (3.9373)  loss_scale: 65536.0000 (60845.3747)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [15]  [1490/2809]  eta: 0:08:09  lr: 0.000037  min_lr: 0.000000  loss: 3.6195 (3.9343)  loss_scale: 65536.0000 (60876.8343)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [15]  [1500/2809]  eta: 0:08:05  lr: 0.000037  min_lr: 0.000000  loss: 3.7482 (3.9355)  loss_scale: 65536.0000 (60907.8748)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [15]  [1510/2809]  eta: 0:08:01  lr: 0.000037  min_lr: 0.000000  loss: 4.0328 (3.9348)  loss_scale: 65536.0000 (60938.5043)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [15]  [1520/2809]  eta: 0:07:58  lr: 0.000037  min_lr: 0.000000  loss: 3.9831 (3.9358)  loss_scale: 65536.0000 (60968.7311)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [15]  [1530/2809]  eta: 0:07:54  lr: 0.000037  min_lr: 0.000000  loss: 3.9831 (3.9357)  loss_scale: 65536.0000 (60998.5630)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [15]  [1540/2809]  eta: 0:07:50  lr: 0.000037  min_lr: 0.000000  loss: 4.0868 (3.9368)  loss_scale: 65536.0000 (61028.0078)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [15]  [1550/2809]  eta: 0:07:46  lr: 0.000037  min_lr: 0.000000  loss: 4.0868 (3.9381)  loss_scale: 65536.0000 (61057.0729)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [15]  [1560/2809]  eta: 0:07:43  lr: 0.000037  min_lr: 0.000000  loss: 4.3304 (3.9396)  loss_scale: 65536.0000 (61085.7655)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [15]  [1570/2809]  eta: 0:07:39  lr: 0.000037  min_lr: 0.000000  loss: 4.1200 (3.9388)  loss_scale: 65536.0000 (61114.0929)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [15]  [1580/2809]  eta: 0:07:35  lr: 0.000037  min_lr: 0.000000  loss: 4.1003 (3.9389)  loss_scale: 65536.0000 (61142.0620)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [15]  [1590/2809]  eta: 0:07:32  lr: 0.000037  min_lr: 0.000000  loss: 4.0513 (3.9389)  loss_scale: 65536.0000 (61169.6794)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
[2025-01-13 03:46:39,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:46:39,613] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:46:40,758] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43732
[2025-01-13 03:46:40,758] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:46:40,758] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1600/2809]  eta: 0:07:28  lr: 0.000037  min_lr: 0.000000  loss: 3.8854 (3.9378)  loss_scale: 65536.0000 (61319.7552)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [15]  [1610/2809]  eta: 0:07:24  lr: 0.000037  min_lr: 0.000000  loss: 3.9486 (3.9386)  loss_scale: 65536.0000 (61345.9268)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [15]  [1620/2809]  eta: 0:07:20  lr: 0.000037  min_lr: 0.000000  loss: 3.9904 (3.9385)  loss_scale: 65536.0000 (61371.7754)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [15]  [1630/2809]  eta: 0:07:17  lr: 0.000037  min_lr: 0.000000  loss: 3.8471 (3.9379)  loss_scale: 65536.0000 (61397.3072)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [15]  [1640/2809]  eta: 0:07:13  lr: 0.000037  min_lr: 0.000000  loss: 3.8759 (3.9379)  loss_scale: 65536.0000 (61422.5277)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0003  max mem: 15572
Epoch: [15]  [1650/2809]  eta: 0:07:09  lr: 0.000037  min_lr: 0.000000  loss: 3.9386 (3.9384)  loss_scale: 65536.0000 (61447.4428)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [15]  [1660/2809]  eta: 0:07:05  lr: 0.000037  min_lr: 0.000000  loss: 3.8998 (3.9373)  loss_scale: 65536.0000 (61472.0578)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0017  max mem: 15572
Epoch: [15]  [1670/2809]  eta: 0:07:02  lr: 0.000037  min_lr: 0.000000  loss: 3.8307 (3.9364)  loss_scale: 65536.0000 (61496.3782)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0017  max mem: 15572
Epoch: [15]  [1680/2809]  eta: 0:06:58  lr: 0.000037  min_lr: 0.000000  loss: 3.9660 (3.9368)  loss_scale: 65536.0000 (61520.4093)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [15]  [1690/2809]  eta: 0:06:54  lr: 0.000037  min_lr: 0.000000  loss: 3.9853 (3.9381)  loss_scale: 65536.0000 (61544.1561)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [15]  [1700/2809]  eta: 0:06:51  lr: 0.000037  min_lr: 0.000000  loss: 3.9639 (3.9373)  loss_scale: 65536.0000 (61567.6238)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [15]  [1710/2809]  eta: 0:06:47  lr: 0.000037  min_lr: 0.000000  loss: 3.9804 (3.9378)  loss_scale: 65536.0000 (61590.8171)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [15]  [1720/2809]  eta: 0:06:43  lr: 0.000037  min_lr: 0.000000  loss: 4.0551 (3.9374)  loss_scale: 65536.0000 (61613.7408)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
[2025-01-13 03:47:28,370] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:47:28,370] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:47:28,757] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43862
[2025-01-13 03:47:28,757] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:47:28,757] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1730/2809]  eta: 0:06:39  lr: 0.000037  min_lr: 0.000000  loss: 3.7992 (3.9363)  loss_scale: 65536.0000 (61674.2600)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [15]  [1740/2809]  eta: 0:06:36  lr: 0.000037  min_lr: 0.000000  loss: 3.7562 (3.9361)  loss_scale: 65536.0000 (61696.4411)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [15]  [1750/2809]  eta: 0:06:32  lr: 0.000037  min_lr: 0.000000  loss: 3.6657 (3.9356)  loss_scale: 65536.0000 (61718.3689)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [15]  [1760/2809]  eta: 0:06:28  lr: 0.000037  min_lr: 0.000000  loss: 3.5957 (3.9334)  loss_scale: 65536.0000 (61740.0477)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [15]  [1770/2809]  eta: 0:06:25  lr: 0.000037  min_lr: 0.000000  loss: 3.7549 (3.9340)  loss_scale: 65536.0000 (61761.4816)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [15]  [1780/2809]  eta: 0:06:21  lr: 0.000037  min_lr: 0.000000  loss: 4.0049 (3.9351)  loss_scale: 65536.0000 (61782.6749)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [15]  [1790/2809]  eta: 0:06:17  lr: 0.000037  min_lr: 0.000000  loss: 3.9199 (3.9351)  loss_scale: 65536.0000 (61803.6315)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [15]  [1800/2809]  eta: 0:06:14  lr: 0.000037  min_lr: 0.000000  loss: 3.9986 (3.9358)  loss_scale: 65536.0000 (61824.3554)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [15]  [1810/2809]  eta: 0:06:10  lr: 0.000037  min_lr: 0.000000  loss: 4.0745 (3.9359)  loss_scale: 65536.0000 (61844.8504)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [15]  [1820/2809]  eta: 0:06:06  lr: 0.000037  min_lr: 0.000000  loss: 3.9366 (3.9358)  loss_scale: 65536.0000 (61865.1203)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [15]  [1830/2809]  eta: 0:06:02  lr: 0.000037  min_lr: 0.000000  loss: 3.8149 (3.9359)  loss_scale: 65536.0000 (61885.1688)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [15]  [1840/2809]  eta: 0:05:59  lr: 0.000037  min_lr: 0.000000  loss: 4.0459 (3.9365)  loss_scale: 65536.0000 (61904.9995)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [15]  [1850/2809]  eta: 0:05:55  lr: 0.000037  min_lr: 0.000000  loss: 4.0448 (3.9368)  loss_scale: 65536.0000 (61924.6159)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
[2025-01-13 03:48:16,467] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:48:16,467] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [1860/2809]  eta: 0:05:51  lr: 0.000037  min_lr: 0.000000  loss: 4.1056 (3.9379)  loss_scale: 65536.0000 (62120.0989)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
[2025-01-13 03:48:18,694] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43997
[2025-01-13 03:48:18,694] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:48:18,695] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-13 03:48:19,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=44000, skipped=295, lr=[3.5794456282147766e-07, 3.5794456282147766e-07, 5.113493754592539e-07, 5.113493754592539e-07, 7.304991077989342e-07, 7.304991077989342e-07, 1.0435701539984775e-06, 1.0435701539984775e-06, 1.4908145057121107e-06, 1.4908145057121107e-06, 2.1297350081601583e-06, 2.1297350081601583e-06, 3.0424785830859404e-06, 3.0424785830859404e-06, 4.346397975837058e-06, 4.346397975837058e-06, 6.2091399654815116e-06, 6.2091399654815116e-06, 8.870199950687875e-06, 8.870199950687875e-06, 1.2671714215268393e-05, 1.2671714215268393e-05, 1.8102448878954847e-05, 1.8102448878954847e-05, 2.5860641255649787e-05, 2.5860641255649787e-05, 3.694377322235684e-05, 3.694377322235684e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 03:48:19,430] [INFO] [timer.py:260:stop] epoch=0/micro_step=44000/global_step=44000, RunningAvgSamplesPerSec=28.46485918848821, CurrSamplesPerSec=35.04650176619394, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [15]  [1870/2809]  eta: 0:05:48  lr: 0.000037  min_lr: 0.000000  loss: 4.1472 (3.9384)  loss_scale: 65536.0000 (62173.3832)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [15]  [1880/2809]  eta: 0:05:44  lr: 0.000037  min_lr: 0.000000  loss: 3.9674 (3.9376)  loss_scale: 65536.0000 (62191.2600)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [15]  [1890/2809]  eta: 0:05:40  lr: 0.000037  min_lr: 0.000000  loss: 3.8261 (3.9371)  loss_scale: 65536.0000 (62208.9476)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [15]  [1900/2809]  eta: 0:05:36  lr: 0.000037  min_lr: 0.000000  loss: 3.8737 (3.9375)  loss_scale: 65536.0000 (62226.4492)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0001  max mem: 15572
Epoch: [15]  [1910/2809]  eta: 0:05:33  lr: 0.000037  min_lr: 0.000000  loss: 3.9137 (3.9375)  loss_scale: 65536.0000 (62243.7677)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0001  max mem: 15572
Epoch: [15]  [1920/2809]  eta: 0:05:29  lr: 0.000037  min_lr: 0.000000  loss: 3.9361 (3.9387)  loss_scale: 65536.0000 (62260.9058)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [15]  [1930/2809]  eta: 0:05:25  lr: 0.000037  min_lr: 0.000000  loss: 3.9974 (3.9390)  loss_scale: 65536.0000 (62277.8664)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [15]  [1940/2809]  eta: 0:05:22  lr: 0.000037  min_lr: 0.000000  loss: 3.9974 (3.9395)  loss_scale: 65536.0000 (62294.6522)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [15]  [1950/2809]  eta: 0:05:18  lr: 0.000037  min_lr: 0.000000  loss: 3.9626 (3.9398)  loss_scale: 65536.0000 (62311.2660)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [15]  [1960/2809]  eta: 0:05:14  lr: 0.000037  min_lr: 0.000000  loss: 3.8697 (3.9391)  loss_scale: 65536.0000 (62327.7104)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [15]  [1970/2809]  eta: 0:05:10  lr: 0.000037  min_lr: 0.000000  loss: 3.9635 (3.9398)  loss_scale: 65536.0000 (62343.9878)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [15]  [1980/2809]  eta: 0:05:07  lr: 0.000037  min_lr: 0.000000  loss: 4.1185 (3.9411)  loss_scale: 65536.0000 (62360.1010)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0001  max mem: 15572
Epoch: [15]  [1990/2809]  eta: 0:05:03  lr: 0.000037  min_lr: 0.000000  loss: 4.0590 (3.9414)  loss_scale: 65536.0000 (62376.0522)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
[2025-01-13 03:49:06,389] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:49:06,389] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:49:06,792] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44127
[2025-01-13 03:49:06,792] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:49:06,792] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2000/2809]  eta: 0:04:59  lr: 0.000037  min_lr: 0.000000  loss: 3.8988 (3.9403)  loss_scale: 65536.0000 (62424.5957)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [15]  [2010/2809]  eta: 0:04:56  lr: 0.000037  min_lr: 0.000000  loss: 3.8686 (3.9396)  loss_scale: 65536.0000 (62440.0676)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [15]  [2020/2809]  eta: 0:04:52  lr: 0.000037  min_lr: 0.000000  loss: 3.8967 (3.9396)  loss_scale: 65536.0000 (62455.3864)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [15]  [2030/2809]  eta: 0:04:48  lr: 0.000037  min_lr: 0.000000  loss: 4.1216 (3.9403)  loss_scale: 65536.0000 (62470.5544)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [15]  [2040/2809]  eta: 0:04:44  lr: 0.000037  min_lr: 0.000000  loss: 4.1644 (3.9411)  loss_scale: 65536.0000 (62485.5737)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0001  max mem: 15572
Epoch: [15]  [2050/2809]  eta: 0:04:41  lr: 0.000037  min_lr: 0.000000  loss: 4.0383 (3.9410)  loss_scale: 65536.0000 (62500.4466)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0001  max mem: 15572
Epoch: [15]  [2060/2809]  eta: 0:04:37  lr: 0.000037  min_lr: 0.000000  loss: 4.0105 (3.9420)  loss_scale: 65536.0000 (62515.1752)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [15]  [2070/2809]  eta: 0:04:33  lr: 0.000037  min_lr: 0.000000  loss: 4.0541 (3.9419)  loss_scale: 65536.0000 (62529.7615)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [15]  [2080/2809]  eta: 0:04:30  lr: 0.000037  min_lr: 0.000000  loss: 3.8722 (3.9412)  loss_scale: 65536.0000 (62544.2076)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [15]  [2090/2809]  eta: 0:04:26  lr: 0.000037  min_lr: 0.000000  loss: 3.8722 (3.9418)  loss_scale: 65536.0000 (62558.5155)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [15]  [2100/2809]  eta: 0:04:22  lr: 0.000037  min_lr: 0.000000  loss: 3.9929 (3.9418)  loss_scale: 65536.0000 (62572.6873)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [15]  [2110/2809]  eta: 0:04:18  lr: 0.000037  min_lr: 0.000000  loss: 3.9351 (3.9412)  loss_scale: 65536.0000 (62586.7248)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [15]  [2120/2809]  eta: 0:04:15  lr: 0.000037  min_lr: 0.000000  loss: 3.9108 (3.9413)  loss_scale: 65536.0000 (62600.6299)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
[2025-01-13 03:49:54,377] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:49:54,377] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:49:56,251] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44261
[2025-01-13 03:49:56,252] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:49:56,252] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2130/2809]  eta: 0:04:11  lr: 0.000037  min_lr: 0.000000  loss: 3.8374 (3.9401)  loss_scale: 65536.0000 (62768.1727)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [15]  [2140/2809]  eta: 0:04:07  lr: 0.000037  min_lr: 0.000000  loss: 3.8374 (3.9401)  loss_scale: 65536.0000 (62781.1004)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [15]  [2150/2809]  eta: 0:04:04  lr: 0.000037  min_lr: 0.000000  loss: 4.0038 (3.9397)  loss_scale: 65536.0000 (62793.9079)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
Epoch: [15]  [2160/2809]  eta: 0:04:00  lr: 0.000037  min_lr: 0.000000  loss: 3.9705 (3.9397)  loss_scale: 65536.0000 (62806.5969)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [15]  [2170/2809]  eta: 0:03:56  lr: 0.000037  min_lr: 0.000000  loss: 4.1404 (3.9405)  loss_scale: 65536.0000 (62819.1690)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [15]  [2180/2809]  eta: 0:03:52  lr: 0.000037  min_lr: 0.000000  loss: 4.0528 (3.9403)  loss_scale: 65536.0000 (62831.6259)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [15]  [2190/2809]  eta: 0:03:49  lr: 0.000037  min_lr: 0.000000  loss: 3.7999 (3.9397)  loss_scale: 65536.0000 (62843.9690)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [15]  [2200/2809]  eta: 0:03:45  lr: 0.000037  min_lr: 0.000000  loss: 3.7999 (3.9398)  loss_scale: 65536.0000 (62856.1999)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [15]  [2210/2809]  eta: 0:03:41  lr: 0.000037  min_lr: 0.000000  loss: 3.9257 (3.9393)  loss_scale: 65536.0000 (62868.3202)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [15]  [2220/2809]  eta: 0:03:38  lr: 0.000037  min_lr: 0.000000  loss: 4.0957 (3.9406)  loss_scale: 65536.0000 (62880.3314)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [15]  [2230/2809]  eta: 0:03:34  lr: 0.000037  min_lr: 0.000000  loss: 4.1857 (3.9412)  loss_scale: 65536.0000 (62892.2349)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [15]  [2240/2809]  eta: 0:03:30  lr: 0.000037  min_lr: 0.000000  loss: 4.0833 (3.9410)  loss_scale: 65536.0000 (62904.0321)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [15]  [2250/2809]  eta: 0:03:27  lr: 0.000037  min_lr: 0.000000  loss: 4.0086 (3.9415)  loss_scale: 65536.0000 (62915.7246)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
[2025-01-13 03:50:43,887] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:50:43,887] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:50:44,248] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44391
[2025-01-13 03:50:44,248] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:50:44,248] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2260/2809]  eta: 0:03:23  lr: 0.000037  min_lr: 0.000000  loss: 3.9928 (3.9419)  loss_scale: 65536.0000 (62956.2990)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0002  max mem: 15572
Epoch: [15]  [2270/2809]  eta: 0:03:19  lr: 0.000037  min_lr: 0.000000  loss: 3.9763 (3.9417)  loss_scale: 65536.0000 (62967.6583)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [15]  [2280/2809]  eta: 0:03:15  lr: 0.000037  min_lr: 0.000000  loss: 3.8539 (3.9410)  loss_scale: 65536.0000 (62978.9180)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [15]  [2290/2809]  eta: 0:03:12  lr: 0.000037  min_lr: 0.000000  loss: 3.8539 (3.9410)  loss_scale: 65536.0000 (62990.0794)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [15]  [2300/2809]  eta: 0:03:08  lr: 0.000037  min_lr: 0.000000  loss: 3.9854 (3.9411)  loss_scale: 65536.0000 (63001.1439)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [15]  [2310/2809]  eta: 0:03:04  lr: 0.000037  min_lr: 0.000000  loss: 3.8352 (3.9404)  loss_scale: 65536.0000 (63012.1125)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0002  max mem: 15572
Epoch: [15]  [2320/2809]  eta: 0:03:01  lr: 0.000037  min_lr: 0.000000  loss: 3.6955 (3.9398)  loss_scale: 65536.0000 (63022.9866)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [15]  [2330/2809]  eta: 0:02:57  lr: 0.000037  min_lr: 0.000000  loss: 3.8376 (3.9398)  loss_scale: 65536.0000 (63033.7675)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [15]  [2340/2809]  eta: 0:02:53  lr: 0.000037  min_lr: 0.000000  loss: 4.0932 (3.9400)  loss_scale: 65536.0000 (63044.4562)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0014  max mem: 15572
Epoch: [15]  [2350/2809]  eta: 0:02:50  lr: 0.000037  min_lr: 0.000000  loss: 3.9023 (3.9396)  loss_scale: 65536.0000 (63055.0540)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0014  max mem: 15572
Epoch: [15]  [2360/2809]  eta: 0:02:46  lr: 0.000037  min_lr: 0.000000  loss: 3.9037 (3.9400)  loss_scale: 65536.0000 (63065.5620)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [15]  [2370/2809]  eta: 0:02:42  lr: 0.000037  min_lr: 0.000000  loss: 4.0081 (3.9399)  loss_scale: 65536.0000 (63075.9814)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [15]  [2380/2809]  eta: 0:02:38  lr: 0.000037  min_lr: 0.000000  loss: 3.8882 (3.9393)  loss_scale: 65536.0000 (63086.3133)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
[2025-01-13 03:51:32,075] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:51:32,075] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:51:33,204] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44523
[2025-01-13 03:51:33,205] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:51:33,206] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2390/2809]  eta: 0:02:35  lr: 0.000037  min_lr: 0.000000  loss: 3.8033 (3.9386)  loss_scale: 65536.0000 (63178.7871)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [15]  [2400/2809]  eta: 0:02:31  lr: 0.000037  min_lr: 0.000000  loss: 3.7813 (3.9379)  loss_scale: 65536.0000 (63188.6047)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [15]  [2410/2809]  eta: 0:02:27  lr: 0.000037  min_lr: 0.000000  loss: 4.1403 (3.9390)  loss_scale: 65536.0000 (63198.3409)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [15]  [2420/2809]  eta: 0:02:24  lr: 0.000037  min_lr: 0.000000  loss: 4.1984 (3.9386)  loss_scale: 65536.0000 (63207.9967)  weight_decay: 0.0500 (0.0500)  time: 0.3638  data: 0.0002  max mem: 15572
Epoch: [15]  [2430/2809]  eta: 0:02:20  lr: 0.000037  min_lr: 0.000000  loss: 3.9036 (3.9389)  loss_scale: 65536.0000 (63217.5730)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [15]  [2440/2809]  eta: 0:02:16  lr: 0.000037  min_lr: 0.000000  loss: 3.9036 (3.9385)  loss_scale: 65536.0000 (63227.0709)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [15]  [2450/2809]  eta: 0:02:12  lr: 0.000037  min_lr: 0.000000  loss: 3.8450 (3.9389)  loss_scale: 65536.0000 (63236.4912)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [15]  [2460/2809]  eta: 0:02:09  lr: 0.000037  min_lr: 0.000000  loss: 3.9805 (3.9392)  loss_scale: 65536.0000 (63245.8350)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [15]  [2470/2809]  eta: 0:02:05  lr: 0.000037  min_lr: 0.000000  loss: 3.9693 (3.9391)  loss_scale: 65536.0000 (63255.1032)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [15]  [2480/2809]  eta: 0:02:01  lr: 0.000037  min_lr: 0.000000  loss: 3.9622 (3.9391)  loss_scale: 65536.0000 (63264.2967)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [15]  [2490/2809]  eta: 0:01:58  lr: 0.000037  min_lr: 0.000000  loss: 3.9622 (3.9388)  loss_scale: 65536.0000 (63273.4163)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [15]  [2500/2809]  eta: 0:01:54  lr: 0.000037  min_lr: 0.000000  loss: 3.9264 (3.9387)  loss_scale: 65536.0000 (63282.4630)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [15]  [2510/2809]  eta: 0:01:50  lr: 0.000037  min_lr: 0.000000  loss: 4.0334 (3.9391)  loss_scale: 65536.0000 (63291.4377)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
[2025-01-13 03:52:20,836] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:52:20,836] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:52:21,568] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44654
[2025-01-13 03:52:21,568] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:52:21,568] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2520/2809]  eta: 0:01:47  lr: 0.000037  min_lr: 0.000000  loss: 4.0703 (3.9395)  loss_scale: 65536.0000 (63352.3332)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [15]  [2530/2809]  eta: 0:01:43  lr: 0.000037  min_lr: 0.000000  loss: 4.1519 (3.9395)  loss_scale: 65536.0000 (63360.9609)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [15]  [2540/2809]  eta: 0:01:39  lr: 0.000037  min_lr: 0.000000  loss: 3.6115 (3.9380)  loss_scale: 65536.0000 (63369.5207)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [15]  [2550/2809]  eta: 0:01:35  lr: 0.000037  min_lr: 0.000000  loss: 3.6928 (3.9384)  loss_scale: 65536.0000 (63378.0133)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [15]  [2560/2809]  eta: 0:01:32  lr: 0.000037  min_lr: 0.000000  loss: 3.9896 (3.9381)  loss_scale: 65536.0000 (63386.4397)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [15]  [2570/2809]  eta: 0:01:28  lr: 0.000037  min_lr: 0.000000  loss: 3.8912 (3.9382)  loss_scale: 65536.0000 (63394.8005)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [15]  [2580/2809]  eta: 0:01:24  lr: 0.000037  min_lr: 0.000000  loss: 3.9524 (3.9383)  loss_scale: 65536.0000 (63403.0965)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [15]  [2590/2809]  eta: 0:01:21  lr: 0.000036  min_lr: 0.000000  loss: 3.9524 (3.9385)  loss_scale: 65536.0000 (63411.3284)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [15]  [2600/2809]  eta: 0:01:17  lr: 0.000036  min_lr: 0.000000  loss: 4.0583 (3.9388)  loss_scale: 65536.0000 (63419.4971)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0001  max mem: 15572
Epoch: [15]  [2610/2809]  eta: 0:01:13  lr: 0.000036  min_lr: 0.000000  loss: 4.0583 (3.9387)  loss_scale: 65536.0000 (63427.6032)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [15]  [2620/2809]  eta: 0:01:09  lr: 0.000036  min_lr: 0.000000  loss: 3.8764 (3.9382)  loss_scale: 65536.0000 (63435.6475)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0003  max mem: 15572
Epoch: [15]  [2630/2809]  eta: 0:01:06  lr: 0.000036  min_lr: 0.000000  loss: 3.8764 (3.9384)  loss_scale: 65536.0000 (63443.6306)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [15]  [2640/2809]  eta: 0:01:02  lr: 0.000036  min_lr: 0.000000  loss: 3.9529 (3.9383)  loss_scale: 65536.0000 (63451.5532)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
[2025-01-13 03:53:09,345] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:53:09,345] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [2650/2809]  eta: 0:00:58  lr: 0.000036  min_lr: 0.000000  loss: 4.0174 (3.9387)  loss_scale: 65536.0000 (63533.5798)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
[2025-01-13 03:53:11,573] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44789
[2025-01-13 03:53:11,573] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:53:11,573] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2660/2809]  eta: 0:00:55  lr: 0.000036  min_lr: 0.000000  loss: 3.9399 (3.9390)  loss_scale: 65536.0000 (63614.9899)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [15]  [2670/2809]  eta: 0:00:51  lr: 0.000036  min_lr: 0.000000  loss: 3.9764 (3.9391)  loss_scale: 65536.0000 (63622.1820)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [15]  [2680/2809]  eta: 0:00:47  lr: 0.000036  min_lr: 0.000000  loss: 3.9498 (3.9384)  loss_scale: 65536.0000 (63629.3204)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [15]  [2690/2809]  eta: 0:00:44  lr: 0.000036  min_lr: 0.000000  loss: 3.6788 (3.9375)  loss_scale: 65536.0000 (63636.4058)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [15]  [2700/2809]  eta: 0:00:40  lr: 0.000036  min_lr: 0.000000  loss: 3.9297 (3.9382)  loss_scale: 65536.0000 (63643.4387)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [15]  [2710/2809]  eta: 0:00:36  lr: 0.000036  min_lr: 0.000000  loss: 3.8955 (3.9373)  loss_scale: 65536.0000 (63650.4198)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
[2025-01-13 03:53:34,934] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44852
[2025-01-13 03:53:34,934] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 03:53:34,934] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [2720/2809]  eta: 0:00:32  lr: 0.000036  min_lr: 0.000000  loss: 3.6256 (3.9372)  loss_scale: 65536.0000 (63609.1790)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [15]  [2730/2809]  eta: 0:00:29  lr: 0.000036  min_lr: 0.000000  loss: 3.5761 (3.9361)  loss_scale: 32768.0000 (63496.2490)  weight_decay: 0.0500 (0.0500)  time: 0.3649  data: 0.0002  max mem: 15572
Epoch: [15]  [2740/2809]  eta: 0:00:25  lr: 0.000036  min_lr: 0.000000  loss: 3.6354 (3.9357)  loss_scale: 32768.0000 (63384.1430)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [15]  [2750/2809]  eta: 0:00:21  lr: 0.000036  min_lr: 0.000000  loss: 3.8776 (3.9357)  loss_scale: 32768.0000 (63272.8521)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [15]  [2760/2809]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000000  loss: 3.8754 (3.9351)  loss_scale: 32768.0000 (63162.3673)  weight_decay: 0.0500 (0.0500)  time: 0.3643  data: 0.0002  max mem: 15572
Epoch: [15]  [2770/2809]  eta: 0:00:14  lr: 0.000036  min_lr: 0.000000  loss: 3.7916 (3.9348)  loss_scale: 32768.0000 (63052.6799)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [15]  [2780/2809]  eta: 0:00:10  lr: 0.000036  min_lr: 0.000000  loss: 3.8436 (3.9342)  loss_scale: 32768.0000 (62943.7814)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [15]  [2790/2809]  eta: 0:00:07  lr: 0.000036  min_lr: 0.000000  loss: 3.8436 (3.9338)  loss_scale: 32768.0000 (62835.6632)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [15]  [2800/2809]  eta: 0:00:03  lr: 0.000036  min_lr: 0.000000  loss: 3.7670 (3.9336)  loss_scale: 32768.0000 (62728.3170)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [15]  [2808/2809]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000000  loss: 3.8194 (3.9333)  loss_scale: 32768.0000 (62642.9904)  weight_decay: 0.0500 (0.0500)  time: 0.3592  data: 0.0001  max mem: 15572
Epoch: [15] Total time: 0:17:20 (0.3704 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000000  loss: 3.8194 (3.9333)  loss_scale: 32768.0000 (62642.9904)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:08:50  loss: 0.3207 (0.3207)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.9507  data: 1.7907  max mem: 15572
Val:  [ 10/272]  eta: 0:01:47  loss: 3.0839 (2.8092)  acc1: 22.2222 (31.3131)  acc5: 61.1111 (58.0808)  time: 0.4085  data: 0.2486  max mem: 15572
Val:  [ 20/272]  eta: 0:01:12  loss: 2.9797 (2.7860)  acc1: 27.7778 (34.3915)  acc5: 61.1111 (62.9630)  time: 0.2064  data: 0.0474  max mem: 15572
Val:  [ 30/272]  eta: 0:00:59  loss: 2.8490 (2.8470)  acc1: 33.3333 (31.1828)  acc5: 66.6667 (64.5161)  time: 0.1591  data: 0.0003  max mem: 15572
Val:  [ 40/272]  eta: 0:00:52  loss: 2.8490 (2.8422)  acc1: 22.2222 (29.9458)  acc5: 72.2222 (66.3957)  time: 0.1567  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:47  loss: 2.7163 (2.7501)  acc1: 33.3333 (33.3333)  acc5: 72.2222 (68.3007)  time: 0.1587  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:43  loss: 1.6938 (2.5981)  acc1: 55.5556 (37.5228)  acc5: 77.7778 (69.9454)  time: 0.1632  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:40  loss: 1.7694 (2.5139)  acc1: 55.5556 (38.9671)  acc5: 83.3333 (71.8310)  time: 0.1649  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:37  loss: 2.3699 (2.5366)  acc1: 38.8889 (39.0261)  acc5: 77.7778 (71.1248)  time: 0.1656  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.9916 (2.5933)  acc1: 33.3333 (38.4005)  acc5: 66.6667 (70.1465)  time: 0.1639  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:32  loss: 2.8761 (2.6282)  acc1: 33.3333 (38.3938)  acc5: 66.6667 (69.5270)  time: 0.1611  data: 0.0005  max mem: 15572
Val:  [110/272]  eta: 0:00:30  loss: 2.8862 (2.6985)  acc1: 16.6667 (36.5866)  acc5: 61.1111 (68.0681)  time: 0.1605  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 3.2083 (2.7374)  acc1: 16.6667 (35.4913)  acc5: 55.5556 (67.3095)  time: 0.1572  data: 0.0003  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.6296 (2.6885)  acc1: 33.3333 (37.0653)  acc5: 72.2222 (67.9389)  time: 0.1552  data: 0.0003  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.9848 (2.6721)  acc1: 44.4444 (37.6281)  acc5: 77.7778 (68.0851)  time: 0.1571  data: 0.0003  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.7999 (2.6790)  acc1: 27.7778 (36.7550)  acc5: 72.2222 (68.3591)  time: 0.1558  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.6727 (2.6579)  acc1: 33.3333 (37.7157)  acc5: 77.7778 (69.0131)  time: 0.1547  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.6727 (2.6865)  acc1: 38.8889 (37.0370)  acc5: 66.6667 (68.3236)  time: 0.1562  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 2.6778 (2.6722)  acc1: 27.7778 (36.8324)  acc5: 66.6667 (68.9994)  time: 0.1617  data: 0.0003  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.5557 (2.7088)  acc1: 27.7778 (36.0966)  acc5: 72.2222 (67.8592)  time: 0.1603  data: 0.0003  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.5384 (2.7137)  acc1: 27.7778 (35.9315)  acc5: 72.2222 (67.8275)  time: 0.1518  data: 0.0003  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.2396 (2.7011)  acc1: 50.0000 (36.7825)  acc5: 77.7778 (68.1148)  time: 0.1517  data: 0.0003  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.1852 (2.6886)  acc1: 50.0000 (37.0789)  acc5: 77.7778 (68.4012)  time: 0.1586  data: 0.0003  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.9531 (2.6567)  acc1: 55.5556 (38.2636)  acc5: 83.3333 (68.9274)  time: 0.1584  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.8014 (2.6378)  acc1: 61.1111 (38.7045)  acc5: 83.3333 (69.4099)  time: 0.1543  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.6947 (2.6529)  acc1: 27.7778 (38.0257)  acc5: 72.2222 (69.2342)  time: 0.1545  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.6154 (2.5816)  acc1: 77.7778 (40.0383)  acc5: 83.3333 (70.2214)  time: 0.1510  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.5589 (2.5816)  acc1: 72.2222 (39.7909)  acc5: 88.8889 (70.1722)  time: 0.1407  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.5589 (2.5867)  acc1: 55.5556 (39.7706)  acc5: 88.8889 (70.1413)  time: 0.1344  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1674 s / it)
* Acc@1 39.771 Acc@5 70.141 loss 2.587
Accuracy of the network on the 4883 val videos: 39.8%
[2025-01-13 03:54:53,867] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 03:54:53,869] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 03:54:53,869] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 03:54:56,239] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 03:54:56,239] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 39.77%
Epoch: [16]  [   0/2809]  eta: 2:37:33  lr: 0.000036  min_lr: 0.000000  loss: 2.8334 (2.8334)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 3.3655  data: 2.9681  max mem: 15572
Epoch: [16]  [  10/2809]  eta: 0:30:28  lr: 0.000036  min_lr: 0.000000  loss: 4.0878 (3.8427)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6534  data: 0.2701  max mem: 15572
Epoch: [16]  [  20/2809]  eta: 0:24:17  lr: 0.000036  min_lr: 0.000000  loss: 4.0878 (3.8514)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3804  data: 0.0003  max mem: 15572
Epoch: [16]  [  30/2809]  eta: 0:21:55  lr: 0.000036  min_lr: 0.000000  loss: 3.9471 (3.9054)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
[2025-01-13 03:55:13,533] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:55:13,533] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [  40/2809]  eta: 0:20:41  lr: 0.000036  min_lr: 0.000000  loss: 3.9020 (3.8986)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [16]  [  50/2809]  eta: 0:19:52  lr: 0.000036  min_lr: 0.000000  loss: 3.8513 (3.8719)  loss_scale: 65536.0000 (41763.1373)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
[2025-01-13 03:55:20,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=45000, skipped=302, lr=[3.519523463149651e-07, 3.519523463149651e-07, 5.027890661642359e-07, 5.027890661642359e-07, 7.18270094520337e-07, 7.18270094520337e-07, 1.026100135029053e-06, 1.026100135029053e-06, 1.46585733575579e-06, 1.46585733575579e-06, 2.094081908222557e-06, 2.094081908222557e-06, 2.991545583175082e-06, 2.991545583175082e-06, 4.273636547392975e-06, 4.273636547392975e-06, 6.105195067704249e-06, 6.105195067704249e-06, 8.7217072395775e-06, 8.7217072395775e-06, 1.2459581770825e-05, 1.2459581770825e-05, 1.779940252975e-05, 1.779940252975e-05, 2.5427717899642862e-05, 2.5427717899642862e-05, 3.632531128520409e-05, 3.632531128520409e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 03:55:20,115] [INFO] [timer.py:260:stop] epoch=0/micro_step=45000/global_step=45000, RunningAvgSamplesPerSec=28.57349618589402, CurrSamplesPerSec=34.469930370692, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [  60/2809]  eta: 0:19:17  lr: 0.000036  min_lr: 0.000000  loss: 3.9992 (3.8925)  loss_scale: 65536.0000 (45660.3279)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [16]  [  70/2809]  eta: 0:18:51  lr: 0.000036  min_lr: 0.000000  loss: 4.0259 (3.9184)  loss_scale: 65536.0000 (48459.7183)  weight_decay: 0.0500 (0.0500)  time: 0.3647  data: 0.0002  max mem: 15572
Epoch: [16]  [  80/2809]  eta: 0:18:31  lr: 0.000036  min_lr: 0.000000  loss: 4.0259 (3.9086)  loss_scale: 65536.0000 (50567.9012)  weight_decay: 0.0500 (0.0500)  time: 0.3647  data: 0.0002  max mem: 15572
Epoch: [16]  [  90/2809]  eta: 0:18:13  lr: 0.000036  min_lr: 0.000000  loss: 3.8522 (3.9084)  loss_scale: 65536.0000 (52212.7473)  weight_decay: 0.0500 (0.0500)  time: 0.3640  data: 0.0002  max mem: 15572
Epoch: [16]  [ 100/2809]  eta: 0:18:00  lr: 0.000036  min_lr: 0.000000  loss: 3.8965 (3.9056)  loss_scale: 65536.0000 (53531.8812)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [16]  [ 110/2809]  eta: 0:17:49  lr: 0.000036  min_lr: 0.000000  loss: 3.9756 (3.9043)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [16]  [ 120/2809]  eta: 0:17:38  lr: 0.000036  min_lr: 0.000000  loss: 3.9756 (3.9040)  loss_scale: 65536.0000 (55516.0331)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [16]  [ 130/2809]  eta: 0:17:28  lr: 0.000036  min_lr: 0.000000  loss: 4.0227 (3.9106)  loss_scale: 65536.0000 (56280.9160)  weight_decay: 0.0500 (0.0500)  time: 0.3645  data: 0.0002  max mem: 15572
Epoch: [16]  [ 140/2809]  eta: 0:17:20  lr: 0.000036  min_lr: 0.000000  loss: 4.0912 (3.9206)  loss_scale: 65536.0000 (56937.3050)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [16]  [ 150/2809]  eta: 0:17:13  lr: 0.000036  min_lr: 0.000000  loss: 4.0251 (3.9214)  loss_scale: 65536.0000 (57506.7550)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [16]  [ 160/2809]  eta: 0:17:05  lr: 0.000036  min_lr: 0.000000  loss: 4.0012 (3.9166)  loss_scale: 65536.0000 (58005.4658)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
[2025-01-13 03:56:00,411] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:56:00,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:56:01,133] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45111
[2025-01-13 03:56:01,133] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:56:01,133] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 170/2809]  eta: 0:16:58  lr: 0.000036  min_lr: 0.000000  loss: 4.0154 (3.9265)  loss_scale: 65536.0000 (59212.3509)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0002  max mem: 15572
Epoch: [16]  [ 180/2809]  eta: 0:16:51  lr: 0.000036  min_lr: 0.000000  loss: 4.1381 (3.9343)  loss_scale: 65536.0000 (59561.7238)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [16]  [ 190/2809]  eta: 0:16:45  lr: 0.000036  min_lr: 0.000000  loss: 4.1223 (3.9361)  loss_scale: 65536.0000 (59874.5131)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [16]  [ 200/2809]  eta: 0:16:39  lr: 0.000036  min_lr: 0.000000  loss: 4.0619 (3.9405)  loss_scale: 65536.0000 (60156.1791)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [16]  [ 210/2809]  eta: 0:16:34  lr: 0.000036  min_lr: 0.000000  loss: 4.0236 (3.9472)  loss_scale: 65536.0000 (60411.1469)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [16]  [ 220/2809]  eta: 0:16:28  lr: 0.000036  min_lr: 0.000000  loss: 4.0236 (3.9392)  loss_scale: 65536.0000 (60643.0407)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [16]  [ 230/2809]  eta: 0:16:22  lr: 0.000036  min_lr: 0.000000  loss: 4.0834 (3.9468)  loss_scale: 65536.0000 (60854.8571)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [16]  [ 240/2809]  eta: 0:16:17  lr: 0.000036  min_lr: 0.000000  loss: 4.1321 (3.9549)  loss_scale: 65536.0000 (61049.0954)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [16]  [ 250/2809]  eta: 0:16:12  lr: 0.000036  min_lr: 0.000000  loss: 4.0978 (3.9488)  loss_scale: 65536.0000 (61227.8566)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [16]  [ 260/2809]  eta: 0:16:07  lr: 0.000036  min_lr: 0.000000  loss: 3.8394 (3.9473)  loss_scale: 65536.0000 (61392.9195)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [16]  [ 270/2809]  eta: 0:16:02  lr: 0.000036  min_lr: 0.000000  loss: 3.8793 (3.9500)  loss_scale: 65536.0000 (61545.8007)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [16]  [ 280/2809]  eta: 0:15:57  lr: 0.000036  min_lr: 0.000000  loss: 4.0788 (3.9505)  loss_scale: 65536.0000 (61687.8007)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [16]  [ 290/2809]  eta: 0:15:53  lr: 0.000036  min_lr: 0.000000  loss: 4.0788 (3.9488)  loss_scale: 65536.0000 (61820.0412)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
[2025-01-13 03:56:48,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:56:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:56:49,731] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45243
[2025-01-13 03:56:49,731] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:56:49,733] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 300/2809]  eta: 0:15:48  lr: 0.000036  min_lr: 0.000000  loss: 3.6128 (3.9390)  loss_scale: 65536.0000 (62596.6777)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [16]  [ 310/2809]  eta: 0:15:44  lr: 0.000036  min_lr: 0.000000  loss: 3.6128 (3.9352)  loss_scale: 65536.0000 (62691.1897)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [16]  [ 320/2809]  eta: 0:15:39  lr: 0.000036  min_lr: 0.000000  loss: 3.9923 (3.9394)  loss_scale: 65536.0000 (62779.8131)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0013  max mem: 15572
Epoch: [16]  [ 330/2809]  eta: 0:15:34  lr: 0.000036  min_lr: 0.000000  loss: 3.9923 (3.9354)  loss_scale: 65536.0000 (62863.0816)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0013  max mem: 15572
Epoch: [16]  [ 340/2809]  eta: 0:15:30  lr: 0.000036  min_lr: 0.000000  loss: 4.0107 (3.9377)  loss_scale: 65536.0000 (62941.4663)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [16]  [ 350/2809]  eta: 0:15:26  lr: 0.000036  min_lr: 0.000000  loss: 4.0511 (3.9425)  loss_scale: 65536.0000 (63015.3846)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [16]  [ 360/2809]  eta: 0:15:21  lr: 0.000036  min_lr: 0.000000  loss: 4.2280 (3.9515)  loss_scale: 65536.0000 (63085.2078)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [16]  [ 370/2809]  eta: 0:15:17  lr: 0.000036  min_lr: 0.000000  loss: 4.2677 (3.9555)  loss_scale: 65536.0000 (63151.2668)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0001  max mem: 15572
Epoch: [16]  [ 380/2809]  eta: 0:15:13  lr: 0.000036  min_lr: 0.000000  loss: 3.8760 (3.9515)  loss_scale: 65536.0000 (63213.8583)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [16]  [ 390/2809]  eta: 0:15:08  lr: 0.000036  min_lr: 0.000000  loss: 3.9051 (3.9515)  loss_scale: 65536.0000 (63273.2481)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [16]  [ 400/2809]  eta: 0:15:04  lr: 0.000036  min_lr: 0.000000  loss: 4.0236 (3.9532)  loss_scale: 65536.0000 (63329.6758)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [16]  [ 410/2809]  eta: 0:15:00  lr: 0.000036  min_lr: 0.000000  loss: 3.8885 (3.9431)  loss_scale: 65536.0000 (63383.3577)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [16]  [ 420/2809]  eta: 0:14:56  lr: 0.000036  min_lr: 0.000000  loss: 3.5805 (3.9411)  loss_scale: 65536.0000 (63434.4893)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
[2025-01-13 03:57:37,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:57:37,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:57:37,580] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45373
[2025-01-13 03:57:37,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:57:37,580] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 430/2809]  eta: 0:14:52  lr: 0.000036  min_lr: 0.000000  loss: 3.7760 (3.9376)  loss_scale: 65536.0000 (63635.3039)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [16]  [ 440/2809]  eta: 0:14:48  lr: 0.000036  min_lr: 0.000000  loss: 3.6563 (3.9350)  loss_scale: 65536.0000 (63678.4036)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0001  max mem: 15572
Epoch: [16]  [ 450/2809]  eta: 0:14:44  lr: 0.000036  min_lr: 0.000000  loss: 4.0922 (3.9399)  loss_scale: 65536.0000 (63719.5920)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [16]  [ 460/2809]  eta: 0:14:40  lr: 0.000036  min_lr: 0.000000  loss: 4.1493 (3.9428)  loss_scale: 65536.0000 (63758.9935)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [16]  [ 470/2809]  eta: 0:14:36  lr: 0.000036  min_lr: 0.000000  loss: 3.8467 (3.9388)  loss_scale: 65536.0000 (63796.7219)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [16]  [ 480/2809]  eta: 0:14:31  lr: 0.000036  min_lr: 0.000000  loss: 3.8245 (3.9394)  loss_scale: 65536.0000 (63832.8815)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [16]  [ 490/2809]  eta: 0:14:27  lr: 0.000036  min_lr: 0.000000  loss: 3.9228 (3.9368)  loss_scale: 65536.0000 (63867.5682)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0001  max mem: 15572
Epoch: [16]  [ 500/2809]  eta: 0:14:23  lr: 0.000036  min_lr: 0.000000  loss: 3.9228 (3.9369)  loss_scale: 65536.0000 (63900.8703)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0001  max mem: 15572
Epoch: [16]  [ 510/2809]  eta: 0:14:19  lr: 0.000036  min_lr: 0.000000  loss: 3.8293 (3.9341)  loss_scale: 65536.0000 (63932.8689)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [16]  [ 520/2809]  eta: 0:14:15  lr: 0.000036  min_lr: 0.000000  loss: 3.9180 (3.9328)  loss_scale: 65536.0000 (63963.6392)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [16]  [ 530/2809]  eta: 0:14:11  lr: 0.000036  min_lr: 0.000000  loss: 3.9180 (3.9282)  loss_scale: 65536.0000 (63993.2505)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [16]  [ 540/2809]  eta: 0:14:07  lr: 0.000036  min_lr: 0.000000  loss: 4.0201 (3.9315)  loss_scale: 65536.0000 (64021.7671)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [16]  [ 550/2809]  eta: 0:14:03  lr: 0.000036  min_lr: 0.000000  loss: 4.0412 (3.9294)  loss_scale: 65536.0000 (64049.2486)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
[2025-01-13 03:58:25,104] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:58:25,104] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [ 560/2809]  eta: 0:14:00  lr: 0.000036  min_lr: 0.000000  loss: 3.8040 (3.9257)  loss_scale: 65536.0000 (64426.2103)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
[2025-01-13 03:58:26,222] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45505
[2025-01-13 03:58:26,222] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:58:26,224] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 570/2809]  eta: 0:13:56  lr: 0.000036  min_lr: 0.000000  loss: 3.6791 (3.9265)  loss_scale: 65536.0000 (64445.6462)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [16]  [ 580/2809]  eta: 0:13:52  lr: 0.000036  min_lr: 0.000000  loss: 3.9509 (3.9264)  loss_scale: 65536.0000 (64464.4131)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [16]  [ 590/2809]  eta: 0:13:48  lr: 0.000036  min_lr: 0.000000  loss: 3.4051 (3.9178)  loss_scale: 65536.0000 (64482.5448)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [16]  [ 600/2809]  eta: 0:13:44  lr: 0.000036  min_lr: 0.000000  loss: 3.6737 (3.9185)  loss_scale: 65536.0000 (64500.0732)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [16]  [ 610/2809]  eta: 0:13:40  lr: 0.000036  min_lr: 0.000000  loss: 3.8610 (3.9189)  loss_scale: 65536.0000 (64517.0278)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [16]  [ 620/2809]  eta: 0:13:36  lr: 0.000036  min_lr: 0.000000  loss: 4.0179 (3.9200)  loss_scale: 65536.0000 (64533.4364)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [16]  [ 630/2809]  eta: 0:13:32  lr: 0.000036  min_lr: 0.000000  loss: 3.8445 (3.9184)  loss_scale: 65536.0000 (64549.3249)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [16]  [ 640/2809]  eta: 0:13:28  lr: 0.000036  min_lr: 0.000000  loss: 3.6647 (3.9159)  loss_scale: 65536.0000 (64564.7176)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [16]  [ 650/2809]  eta: 0:13:24  lr: 0.000036  min_lr: 0.000000  loss: 3.9144 (3.9163)  loss_scale: 65536.0000 (64579.6375)  weight_decay: 0.0500 (0.0500)  time: 0.3653  data: 0.0002  max mem: 15572
Epoch: [16]  [ 660/2809]  eta: 0:13:20  lr: 0.000036  min_lr: 0.000000  loss: 3.9878 (3.9175)  loss_scale: 65536.0000 (64594.1059)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [16]  [ 670/2809]  eta: 0:13:16  lr: 0.000036  min_lr: 0.000000  loss: 4.0718 (3.9175)  loss_scale: 65536.0000 (64608.1431)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [16]  [ 680/2809]  eta: 0:13:13  lr: 0.000036  min_lr: 0.000000  loss: 3.8957 (3.9157)  loss_scale: 65536.0000 (64621.7680)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0001  max mem: 15572
[2025-01-13 03:59:13,692] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:59:13,692] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [ 690/2809]  eta: 0:13:09  lr: 0.000036  min_lr: 0.000000  loss: 3.9229 (3.9173)  loss_scale: 65536.0000 (64729.8408)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
[2025-01-13 03:59:16,273] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45641
[2025-01-13 03:59:16,273] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:59:16,273] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 700/2809]  eta: 0:13:05  lr: 0.000036  min_lr: 0.000000  loss: 3.8064 (3.9130)  loss_scale: 65536.0000 (65302.2767)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [16]  [ 710/2809]  eta: 0:13:01  lr: 0.000036  min_lr: 0.000000  loss: 3.7530 (3.9131)  loss_scale: 65536.0000 (65305.5640)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [16]  [ 720/2809]  eta: 0:12:57  lr: 0.000036  min_lr: 0.000000  loss: 4.0256 (3.9160)  loss_scale: 65536.0000 (65308.7601)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [16]  [ 730/2809]  eta: 0:12:53  lr: 0.000036  min_lr: 0.000000  loss: 4.2656 (3.9208)  loss_scale: 65536.0000 (65311.8687)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0001  max mem: 15572
Epoch: [16]  [ 740/2809]  eta: 0:12:49  lr: 0.000036  min_lr: 0.000000  loss: 4.2332 (3.9220)  loss_scale: 65536.0000 (65314.8934)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [16]  [ 750/2809]  eta: 0:12:46  lr: 0.000036  min_lr: 0.000000  loss: 3.9676 (3.9222)  loss_scale: 65536.0000 (65317.8375)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [16]  [ 760/2809]  eta: 0:12:42  lr: 0.000036  min_lr: 0.000000  loss: 4.0170 (3.9245)  loss_scale: 65536.0000 (65320.7043)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [16]  [ 770/2809]  eta: 0:12:38  lr: 0.000036  min_lr: 0.000000  loss: 3.9977 (3.9200)  loss_scale: 65536.0000 (65323.4968)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0001  max mem: 15572
Epoch: [16]  [ 780/2809]  eta: 0:12:34  lr: 0.000036  min_lr: 0.000000  loss: 3.9349 (3.9212)  loss_scale: 65536.0000 (65326.2177)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [16]  [ 790/2809]  eta: 0:12:30  lr: 0.000036  min_lr: 0.000000  loss: 3.9963 (3.9213)  loss_scale: 65536.0000 (65328.8698)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [16]  [ 800/2809]  eta: 0:12:26  lr: 0.000036  min_lr: 0.000000  loss: 4.0269 (3.9222)  loss_scale: 65536.0000 (65331.4557)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [16]  [ 810/2809]  eta: 0:12:23  lr: 0.000036  min_lr: 0.000000  loss: 3.9024 (3.9208)  loss_scale: 65536.0000 (65333.9778)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [16]  [ 820/2809]  eta: 0:12:19  lr: 0.000036  min_lr: 0.000000  loss: 3.8989 (3.9213)  loss_scale: 65536.0000 (65336.4385)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
[2025-01-13 04:00:03,683] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:00:03,683] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [ 830/2809]  eta: 0:12:15  lr: 0.000036  min_lr: 0.000000  loss: 3.8989 (3.9198)  loss_scale: 65536.0000 (65733.1600)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
[2025-01-13 04:00:05,511] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45775
[2025-01-13 04:00:05,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:00:05,511] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 840/2809]  eta: 0:12:11  lr: 0.000036  min_lr: 0.000000  loss: 3.8477 (3.9201)  loss_scale: 65536.0000 (65730.8157)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [16]  [ 850/2809]  eta: 0:12:07  lr: 0.000036  min_lr: 0.000000  loss: 3.8157 (3.9198)  loss_scale: 65536.0000 (65728.5264)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [16]  [ 860/2809]  eta: 0:12:04  lr: 0.000036  min_lr: 0.000000  loss: 3.7810 (3.9179)  loss_scale: 65536.0000 (65726.2904)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [16]  [ 870/2809]  eta: 0:12:00  lr: 0.000036  min_lr: 0.000000  loss: 3.7810 (3.9174)  loss_scale: 65536.0000 (65724.1056)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [16]  [ 880/2809]  eta: 0:11:56  lr: 0.000036  min_lr: 0.000000  loss: 3.7282 (3.9169)  loss_scale: 65536.0000 (65721.9705)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [16]  [ 890/2809]  eta: 0:11:52  lr: 0.000036  min_lr: 0.000000  loss: 3.8494 (3.9188)  loss_scale: 65536.0000 (65719.8833)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [16]  [ 900/2809]  eta: 0:11:49  lr: 0.000036  min_lr: 0.000000  loss: 3.9191 (3.9178)  loss_scale: 65536.0000 (65717.8424)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [16]  [ 910/2809]  eta: 0:11:45  lr: 0.000036  min_lr: 0.000000  loss: 3.9504 (3.9178)  loss_scale: 65536.0000 (65715.8463)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [16]  [ 920/2809]  eta: 0:11:41  lr: 0.000036  min_lr: 0.000000  loss: 3.8039 (3.9141)  loss_scale: 65536.0000 (65713.8936)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [16]  [ 930/2809]  eta: 0:11:37  lr: 0.000036  min_lr: 0.000000  loss: 3.5895 (3.9116)  loss_scale: 65536.0000 (65711.9828)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [16]  [ 940/2809]  eta: 0:11:34  lr: 0.000036  min_lr: 0.000000  loss: 3.9542 (3.9132)  loss_scale: 65536.0000 (65710.1126)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [16]  [ 950/2809]  eta: 0:11:30  lr: 0.000036  min_lr: 0.000000  loss: 3.9906 (3.9126)  loss_scale: 65536.0000 (65708.2818)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
[2025-01-13 04:00:53,090] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:00:53,090] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [ 960/2809]  eta: 0:11:26  lr: 0.000036  min_lr: 0.000000  loss: 3.8742 (3.9105)  loss_scale: 65536.0000 (65774.6847)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
[2025-01-13 04:00:53,815] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45906
[2025-01-13 04:00:53,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:00:53,815] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 970/2809]  eta: 0:11:22  lr: 0.000036  min_lr: 0.000000  loss: 3.9027 (3.9087)  loss_scale: 65536.0000 (65839.7199)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [16]  [ 980/2809]  eta: 0:11:18  lr: 0.000036  min_lr: 0.000000  loss: 3.9717 (3.9112)  loss_scale: 65536.0000 (65836.6239)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [16]  [ 990/2809]  eta: 0:11:15  lr: 0.000036  min_lr: 0.000000  loss: 4.1156 (3.9132)  loss_scale: 65536.0000 (65833.5903)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [16]  [1000/2809]  eta: 0:11:11  lr: 0.000036  min_lr: 0.000000  loss: 4.1330 (3.9149)  loss_scale: 65536.0000 (65830.6174)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [16]  [1010/2809]  eta: 0:11:07  lr: 0.000036  min_lr: 0.000000  loss: 4.0723 (3.9146)  loss_scale: 65536.0000 (65827.7033)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [16]  [1020/2809]  eta: 0:11:03  lr: 0.000036  min_lr: 0.000000  loss: 3.9583 (3.9139)  loss_scale: 65536.0000 (65824.8462)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [16]  [1030/2809]  eta: 0:11:00  lr: 0.000036  min_lr: 0.000000  loss: 4.1164 (3.9174)  loss_scale: 65536.0000 (65822.0446)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [16]  [1040/2809]  eta: 0:10:56  lr: 0.000036  min_lr: 0.000000  loss: 4.3562 (3.9171)  loss_scale: 65536.0000 (65819.2968)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [16]  [1050/2809]  eta: 0:10:52  lr: 0.000036  min_lr: 0.000000  loss: 3.9266 (3.9181)  loss_scale: 65536.0000 (65816.6013)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
[2025-01-13 04:01:28,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=46000, skipped=309, lr=[3.458328714272749e-07, 3.458328714272749e-07, 4.940469591818214e-07, 4.940469591818214e-07, 7.057813702597449e-07, 7.057813702597449e-07, 1.0082591003710642e-06, 1.0082591003710642e-06, 1.4403701433872347e-06, 1.4403701433872347e-06, 2.057671633410335e-06, 2.057671633410335e-06, 2.9395309048719076e-06, 2.9395309048719076e-06, 4.199329864102725e-06, 4.199329864102725e-06, 5.999042663003894e-06, 5.999042663003894e-06, 8.570060947148422e-06, 8.570060947148422e-06, 1.2242944210212029e-05, 1.2242944210212029e-05, 1.7489920300302902e-05, 1.7489920300302902e-05, 2.4985600429004146e-05, 2.4985600429004146e-05, 3.5693714898577354e-05, 3.5693714898577354e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 04:01:28,136] [INFO] [timer.py:260:stop] epoch=0/micro_step=46000/global_step=46000, RunningAvgSamplesPerSec=28.680479528878884, CurrSamplesPerSec=32.58217975607862, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [1060/2809]  eta: 0:10:48  lr: 0.000036  min_lr: 0.000000  loss: 3.9577 (3.9181)  loss_scale: 65536.0000 (65813.9566)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [16]  [1070/2809]  eta: 0:10:45  lr: 0.000036  min_lr: 0.000000  loss: 3.9577 (3.9170)  loss_scale: 65536.0000 (65811.3613)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [16]  [1080/2809]  eta: 0:10:41  lr: 0.000036  min_lr: 0.000000  loss: 3.8527 (3.9167)  loss_scale: 65536.0000 (65808.8141)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [16]  [1090/2809]  eta: 0:10:37  lr: 0.000036  min_lr: 0.000000  loss: 3.9106 (3.9177)  loss_scale: 65536.0000 (65806.3135)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
[2025-01-13 04:01:41,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:01:41,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:01:43,731] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46041
[2025-01-13 04:01:43,731] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:01:43,731] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-13 04:01:44,842] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46044
[2025-01-13 04:01:44,843] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 04:01:44,843] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1100/2809]  eta: 0:10:34  lr: 0.000036  min_lr: 0.000000  loss: 3.9106 (3.9158)  loss_scale: 65536.0000 (66131.2407)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [16]  [1110/2809]  eta: 0:10:30  lr: 0.000036  min_lr: 0.000000  loss: 3.7071 (3.9152)  loss_scale: 32768.0000 (65830.9415)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [16]  [1120/2809]  eta: 0:10:26  lr: 0.000036  min_lr: 0.000000  loss: 3.8157 (3.9147)  loss_scale: 32768.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [16]  [1130/2809]  eta: 0:10:22  lr: 0.000036  min_lr: 0.000000  loss: 3.8512 (3.9135)  loss_scale: 32768.0000 (65246.2741)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0001  max mem: 15572
Epoch: [16]  [1140/2809]  eta: 0:10:19  lr: 0.000036  min_lr: 0.000000  loss: 4.0777 (3.9156)  loss_scale: 32768.0000 (64961.6266)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [16]  [1150/2809]  eta: 0:10:15  lr: 0.000036  min_lr: 0.000000  loss: 4.1295 (3.9168)  loss_scale: 32768.0000 (64681.9253)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [16]  [1160/2809]  eta: 0:10:11  lr: 0.000036  min_lr: 0.000000  loss: 4.0811 (3.9171)  loss_scale: 32768.0000 (64407.0422)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [16]  [1170/2809]  eta: 0:10:07  lr: 0.000036  min_lr: 0.000000  loss: 3.9119 (3.9176)  loss_scale: 32768.0000 (64136.8540)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [16]  [1180/2809]  eta: 0:10:04  lr: 0.000036  min_lr: 0.000000  loss: 3.8923 (3.9174)  loss_scale: 32768.0000 (63871.2413)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [16]  [1190/2809]  eta: 0:10:00  lr: 0.000036  min_lr: 0.000000  loss: 3.9101 (3.9170)  loss_scale: 32768.0000 (63610.0890)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [16]  [1200/2809]  eta: 0:09:56  lr: 0.000036  min_lr: 0.000000  loss: 3.8997 (3.9183)  loss_scale: 32768.0000 (63353.2856)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0001  max mem: 15572
Epoch: [16]  [1210/2809]  eta: 0:09:52  lr: 0.000036  min_lr: 0.000000  loss: 3.7441 (3.9160)  loss_scale: 32768.0000 (63100.7234)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [16]  [1220/2809]  eta: 0:09:49  lr: 0.000036  min_lr: 0.000000  loss: 3.6184 (3.9145)  loss_scale: 32768.0000 (62852.2981)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
[2025-01-13 04:02:32,370] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:02:32,370] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [1230/2809]  eta: 0:09:45  lr: 0.000036  min_lr: 0.000000  loss: 3.8710 (3.9162)  loss_scale: 32768.0000 (62661.1470)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 04:02:35,342] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46181
[2025-01-13 04:02:35,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 04:02:35,342] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1240/2809]  eta: 0:09:41  lr: 0.000036  min_lr: 0.000000  loss: 4.0403 (3.9156)  loss_scale: 32768.0000 (62578.6946)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [16]  [1250/2809]  eta: 0:09:37  lr: 0.000036  min_lr: 0.000000  loss: 4.1318 (3.9180)  loss_scale: 32768.0000 (62340.3997)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [16]  [1260/2809]  eta: 0:09:34  lr: 0.000036  min_lr: 0.000000  loss: 3.8349 (3.9164)  loss_scale: 32768.0000 (62105.8842)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [16]  [1270/2809]  eta: 0:09:30  lr: 0.000036  min_lr: 0.000000  loss: 3.7430 (3.9169)  loss_scale: 32768.0000 (61875.0590)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [16]  [1280/2809]  eta: 0:09:26  lr: 0.000036  min_lr: 0.000000  loss: 3.9614 (3.9167)  loss_scale: 32768.0000 (61647.8376)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [16]  [1290/2809]  eta: 0:09:23  lr: 0.000036  min_lr: 0.000000  loss: 3.8558 (3.9171)  loss_scale: 32768.0000 (61424.1363)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [16]  [1300/2809]  eta: 0:09:19  lr: 0.000036  min_lr: 0.000000  loss: 3.8558 (3.9168)  loss_scale: 32768.0000 (61203.8739)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [16]  [1310/2809]  eta: 0:09:15  lr: 0.000036  min_lr: 0.000000  loss: 3.9157 (3.9164)  loss_scale: 32768.0000 (60986.9718)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [16]  [1320/2809]  eta: 0:09:11  lr: 0.000036  min_lr: 0.000000  loss: 3.9858 (3.9169)  loss_scale: 32768.0000 (60773.3535)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [16]  [1330/2809]  eta: 0:09:08  lr: 0.000036  min_lr: 0.000000  loss: 3.9235 (3.9166)  loss_scale: 32768.0000 (60562.9452)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [16]  [1340/2809]  eta: 0:09:04  lr: 0.000036  min_lr: 0.000000  loss: 3.9325 (3.9180)  loss_scale: 32768.0000 (60355.6749)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [16]  [1350/2809]  eta: 0:09:00  lr: 0.000036  min_lr: 0.000000  loss: 4.1263 (3.9193)  loss_scale: 32768.0000 (60151.4730)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [16]  [1360/2809]  eta: 0:08:56  lr: 0.000035  min_lr: 0.000000  loss: 3.9100 (3.9187)  loss_scale: 32768.0000 (59950.2719)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
[2025-01-13 04:03:23,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:03:23,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [1370/2809]  eta: 0:08:53  lr: 0.000035  min_lr: 0.000000  loss: 3.6200 (3.9178)  loss_scale: 32768.0000 (59871.5098)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [16]  [1380/2809]  eta: 0:08:49  lr: 0.000035  min_lr: 0.000000  loss: 3.7993 (3.9180)  loss_scale: 65536.0000 (59912.5272)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [16]  [1390/2809]  eta: 0:08:45  lr: 0.000035  min_lr: 0.000000  loss: 3.9867 (3.9190)  loss_scale: 65536.0000 (59952.9547)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [16]  [1400/2809]  eta: 0:08:42  lr: 0.000035  min_lr: 0.000000  loss: 4.0004 (3.9192)  loss_scale: 65536.0000 (59992.8051)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [16]  [1410/2809]  eta: 0:08:38  lr: 0.000035  min_lr: 0.000000  loss: 3.9990 (3.9194)  loss_scale: 65536.0000 (60032.0907)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [16]  [1420/2809]  eta: 0:08:34  lr: 0.000035  min_lr: 0.000000  loss: 3.9990 (3.9201)  loss_scale: 65536.0000 (60070.8234)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [16]  [1430/2809]  eta: 0:08:31  lr: 0.000035  min_lr: 0.000000  loss: 4.0104 (3.9217)  loss_scale: 65536.0000 (60109.0147)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [16]  [1440/2809]  eta: 0:08:27  lr: 0.000035  min_lr: 0.000000  loss: 3.8861 (3.9217)  loss_scale: 65536.0000 (60146.6759)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [16]  [1450/2809]  eta: 0:08:23  lr: 0.000035  min_lr: 0.000000  loss: 3.8808 (3.9221)  loss_scale: 65536.0000 (60183.8181)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [16]  [1460/2809]  eta: 0:08:19  lr: 0.000035  min_lr: 0.000000  loss: 3.8975 (3.9213)  loss_scale: 65536.0000 (60220.4517)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [16]  [1470/2809]  eta: 0:08:16  lr: 0.000035  min_lr: 0.000000  loss: 4.0661 (3.9218)  loss_scale: 65536.0000 (60256.5874)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [16]  [1480/2809]  eta: 0:08:12  lr: 0.000035  min_lr: 0.000000  loss: 4.0058 (3.9225)  loss_scale: 65536.0000 (60292.2350)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [16]  [1490/2809]  eta: 0:08:08  lr: 0.000035  min_lr: 0.000000  loss: 3.9168 (3.9225)  loss_scale: 65536.0000 (60327.4044)  weight_decay: 0.0500 (0.0500)  time: 0.3648  data: 0.0002  max mem: 15572
[2025-01-13 04:04:10,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:04:10,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:04:10,666] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46439
[2025-01-13 04:04:10,666] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:04:10,666] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [1500/2809]  eta: 0:08:04  lr: 0.000035  min_lr: 0.000000  loss: 3.7416 (3.9210)  loss_scale: 65536.0000 (60405.7668)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [16]  [1510/2809]  eta: 0:08:01  lr: 0.000035  min_lr: 0.000000  loss: 3.6796 (3.9219)  loss_scale: 65536.0000 (60439.7194)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [16]  [1520/2809]  eta: 0:07:57  lr: 0.000035  min_lr: 0.000000  loss: 4.0184 (3.9225)  loss_scale: 65536.0000 (60473.2255)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [16]  [1530/2809]  eta: 0:07:53  lr: 0.000035  min_lr: 0.000000  loss: 4.0184 (3.9232)  loss_scale: 65536.0000 (60506.2939)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [16]  [1540/2809]  eta: 0:07:50  lr: 0.000035  min_lr: 0.000000  loss: 3.8544 (3.9216)  loss_scale: 65536.0000 (60538.9332)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [16]  [1550/2809]  eta: 0:07:46  lr: 0.000035  min_lr: 0.000000  loss: 3.6539 (3.9193)  loss_scale: 65536.0000 (60571.1515)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [16]  [1560/2809]  eta: 0:07:42  lr: 0.000035  min_lr: 0.000000  loss: 3.6832 (3.9204)  loss_scale: 65536.0000 (60602.9571)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [16]  [1570/2809]  eta: 0:07:38  lr: 0.000035  min_lr: 0.000000  loss: 4.0027 (3.9194)  loss_scale: 65536.0000 (60634.3577)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [16]  [1580/2809]  eta: 0:07:35  lr: 0.000035  min_lr: 0.000000  loss: 3.8830 (3.9190)  loss_scale: 65536.0000 (60665.3612)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [16]  [1590/2809]  eta: 0:07:31  lr: 0.000035  min_lr: 0.000000  loss: 4.0968 (3.9203)  loss_scale: 65536.0000 (60695.9749)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [16]  [1600/2809]  eta: 0:07:27  lr: 0.000035  min_lr: 0.000000  loss: 4.0931 (3.9204)  loss_scale: 65536.0000 (60726.2061)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [16]  [1610/2809]  eta: 0:07:24  lr: 0.000035  min_lr: 0.000000  loss: 4.0368 (3.9200)  loss_scale: 65536.0000 (60756.0621)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [16]  [1620/2809]  eta: 0:07:20  lr: 0.000035  min_lr: 0.000000  loss: 4.0364 (3.9187)  loss_scale: 65536.0000 (60785.5497)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
[2025-01-13 04:04:58,267] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:04:58,267] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:04:58,629] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46569
[2025-01-13 04:04:58,629] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:04:58,629] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [1630/2809]  eta: 0:07:16  lr: 0.000035  min_lr: 0.000000  loss: 4.1525 (3.9203)  loss_scale: 65536.0000 (60854.8571)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [16]  [1640/2809]  eta: 0:07:12  lr: 0.000035  min_lr: 0.000000  loss: 4.1361 (3.9205)  loss_scale: 65536.0000 (60883.3833)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [16]  [1650/2809]  eta: 0:07:09  lr: 0.000035  min_lr: 0.000000  loss: 3.9421 (3.9201)  loss_scale: 65536.0000 (60911.5639)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [16]  [1660/2809]  eta: 0:07:05  lr: 0.000035  min_lr: 0.000000  loss: 3.8815 (3.9204)  loss_scale: 65536.0000 (60939.4052)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [16]  [1670/2809]  eta: 0:07:01  lr: 0.000035  min_lr: 0.000000  loss: 3.9148 (3.9202)  loss_scale: 65536.0000 (60966.9132)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [16]  [1680/2809]  eta: 0:06:58  lr: 0.000035  min_lr: 0.000000  loss: 3.6393 (3.9190)  loss_scale: 65536.0000 (60994.0940)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [16]  [1690/2809]  eta: 0:06:54  lr: 0.000035  min_lr: 0.000000  loss: 3.7551 (3.9186)  loss_scale: 65536.0000 (61020.9533)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [16]  [1700/2809]  eta: 0:06:50  lr: 0.000035  min_lr: 0.000000  loss: 3.9257 (3.9197)  loss_scale: 65536.0000 (61047.4968)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [16]  [1710/2809]  eta: 0:06:46  lr: 0.000035  min_lr: 0.000000  loss: 3.9257 (3.9193)  loss_scale: 65536.0000 (61073.7300)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [16]  [1720/2809]  eta: 0:06:43  lr: 0.000035  min_lr: 0.000000  loss: 3.8052 (3.9193)  loss_scale: 65536.0000 (61099.6583)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [16]  [1730/2809]  eta: 0:06:39  lr: 0.000035  min_lr: 0.000000  loss: 4.0396 (3.9196)  loss_scale: 65536.0000 (61125.2871)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
Epoch: [16]  [1740/2809]  eta: 0:06:35  lr: 0.000035  min_lr: 0.000000  loss: 4.0110 (3.9198)  loss_scale: 65536.0000 (61150.6215)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [16]  [1750/2809]  eta: 0:06:32  lr: 0.000035  min_lr: 0.000000  loss: 3.8647 (3.9200)  loss_scale: 65536.0000 (61175.6665)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 04:05:46,166] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:05:46,166] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:05:47,293] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46701
[2025-01-13 04:05:47,293] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:05:47,293] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [1760/2809]  eta: 0:06:28  lr: 0.000035  min_lr: 0.000000  loss: 3.8480 (3.9183)  loss_scale: 65536.0000 (61312.0727)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [16]  [1770/2809]  eta: 0:06:24  lr: 0.000035  min_lr: 0.000000  loss: 3.8480 (3.9178)  loss_scale: 65536.0000 (61335.9232)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [16]  [1780/2809]  eta: 0:06:20  lr: 0.000035  min_lr: 0.000000  loss: 3.9042 (3.9179)  loss_scale: 65536.0000 (61359.5059)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [16]  [1790/2809]  eta: 0:06:17  lr: 0.000035  min_lr: 0.000000  loss: 3.9837 (3.9177)  loss_scale: 65536.0000 (61382.8252)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [16]  [1800/2809]  eta: 0:06:13  lr: 0.000035  min_lr: 0.000000  loss: 3.7229 (3.9171)  loss_scale: 65536.0000 (61405.8856)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [16]  [1810/2809]  eta: 0:06:09  lr: 0.000035  min_lr: 0.000000  loss: 3.7137 (3.9148)  loss_scale: 65536.0000 (61428.6913)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [16]  [1820/2809]  eta: 0:06:06  lr: 0.000035  min_lr: 0.000000  loss: 3.9367 (3.9155)  loss_scale: 65536.0000 (61451.2466)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [16]  [1830/2809]  eta: 0:06:02  lr: 0.000035  min_lr: 0.000000  loss: 4.1373 (3.9159)  loss_scale: 65536.0000 (61473.5554)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [16]  [1840/2809]  eta: 0:05:58  lr: 0.000035  min_lr: 0.000000  loss: 4.0849 (3.9166)  loss_scale: 65536.0000 (61495.6219)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [16]  [1850/2809]  eta: 0:05:54  lr: 0.000035  min_lr: 0.000000  loss: 4.1455 (3.9179)  loss_scale: 65536.0000 (61517.4500)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [16]  [1860/2809]  eta: 0:05:51  lr: 0.000035  min_lr: 0.000000  loss: 3.9624 (3.9175)  loss_scale: 65536.0000 (61539.0435)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [16]  [1870/2809]  eta: 0:05:47  lr: 0.000035  min_lr: 0.000000  loss: 3.8620 (3.9174)  loss_scale: 65536.0000 (61560.4062)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [16]  [1880/2809]  eta: 0:05:43  lr: 0.000035  min_lr: 0.000000  loss: 4.0823 (3.9189)  loss_scale: 65536.0000 (61581.5417)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
[2025-01-13 04:06:34,886] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:06:34,886] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [1890/2809]  eta: 0:05:40  lr: 0.000035  min_lr: 0.000000  loss: 4.1181 (3.9195)  loss_scale: 65536.0000 (61775.7377)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
[2025-01-13 04:06:36,724] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46835
[2025-01-13 04:06:36,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:06:36,725] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [1900/2809]  eta: 0:05:36  lr: 0.000035  min_lr: 0.000000  loss: 3.9292 (3.9193)  loss_scale: 65536.0000 (61795.5181)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [16]  [1910/2809]  eta: 0:05:32  lr: 0.000035  min_lr: 0.000000  loss: 3.8294 (3.9185)  loss_scale: 65536.0000 (61815.0916)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [16]  [1920/2809]  eta: 0:05:29  lr: 0.000035  min_lr: 0.000000  loss: 3.8371 (3.9192)  loss_scale: 65536.0000 (61834.4612)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [16]  [1930/2809]  eta: 0:05:25  lr: 0.000035  min_lr: 0.000000  loss: 4.1194 (3.9189)  loss_scale: 65536.0000 (61853.6302)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [16]  [1940/2809]  eta: 0:05:21  lr: 0.000035  min_lr: 0.000000  loss: 4.0942 (3.9195)  loss_scale: 65536.0000 (61872.6018)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
[2025-01-13 04:06:57,134] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46890
[2025-01-13 04:06:57,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 04:06:57,134] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1950/2809]  eta: 0:05:17  lr: 0.000035  min_lr: 0.000000  loss: 4.1900 (3.9207)  loss_scale: 65536.0000 (61807.4013)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [16]  [1960/2809]  eta: 0:05:14  lr: 0.000035  min_lr: 0.000000  loss: 3.9486 (3.9199)  loss_scale: 32768.0000 (61659.3167)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0016  max mem: 15572
Epoch: [16]  [1970/2809]  eta: 0:05:10  lr: 0.000035  min_lr: 0.000000  loss: 3.7765 (3.9193)  loss_scale: 32768.0000 (61512.7347)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0016  max mem: 15572
Epoch: [16]  [1980/2809]  eta: 0:05:06  lr: 0.000035  min_lr: 0.000000  loss: 3.8242 (3.9195)  loss_scale: 32768.0000 (61367.6325)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [16]  [1990/2809]  eta: 0:05:03  lr: 0.000035  min_lr: 0.000000  loss: 3.9164 (3.9189)  loss_scale: 32768.0000 (61223.9879)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [16]  [2000/2809]  eta: 0:04:59  lr: 0.000035  min_lr: 0.000000  loss: 4.0875 (3.9202)  loss_scale: 32768.0000 (61081.7791)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [16]  [2010/2809]  eta: 0:04:55  lr: 0.000035  min_lr: 0.000000  loss: 4.2152 (3.9211)  loss_scale: 32768.0000 (60940.9846)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [16]  [2020/2809]  eta: 0:04:52  lr: 0.000035  min_lr: 0.000000  loss: 4.1318 (3.9218)  loss_scale: 32768.0000 (60801.5834)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [16]  [2030/2809]  eta: 0:04:48  lr: 0.000035  min_lr: 0.000000  loss: 3.7074 (3.9202)  loss_scale: 32768.0000 (60663.5549)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [16]  [2040/2809]  eta: 0:04:44  lr: 0.000035  min_lr: 0.000000  loss: 3.7593 (3.9205)  loss_scale: 32768.0000 (60526.8790)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [16]  [2050/2809]  eta: 0:04:40  lr: 0.000035  min_lr: 0.000000  loss: 3.9560 (3.9193)  loss_scale: 32768.0000 (60391.5358)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0001  max mem: 15572
[2025-01-13 04:07:37,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=47000, skipped=317, lr=[3.395923861060386e-07, 3.395923861060386e-07, 4.851319801514837e-07, 4.851319801514837e-07, 6.930456859306911e-07, 6.930456859306911e-07, 9.90065265615273e-07, 9.90065265615273e-07, 1.4143789508789615e-06, 1.4143789508789615e-06, 2.0205413583985166e-06, 2.0205413583985166e-06, 2.886487654855024e-06, 2.886487654855024e-06, 4.123553792650034e-06, 4.123553792650034e-06, 5.890791132357192e-06, 5.890791132357192e-06, 8.415415903367418e-06, 8.415415903367418e-06, 1.2022022719096311e-05, 1.2022022719096311e-05, 1.717431817013759e-05, 1.717431817013759e-05, 2.4534740243053702e-05, 2.4534740243053702e-05, 3.504962891864815e-05, 3.504962891864815e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 04:07:37,386] [INFO] [timer.py:260:stop] epoch=0/micro_step=47000/global_step=47000, RunningAvgSamplesPerSec=28.78209434282381, CurrSamplesPerSec=35.07461247062693, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [2060/2809]  eta: 0:04:37  lr: 0.000035  min_lr: 0.000000  loss: 3.9693 (3.9205)  loss_scale: 32768.0000 (60257.5061)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [16]  [2070/2809]  eta: 0:04:33  lr: 0.000035  min_lr: 0.000000  loss: 4.0277 (3.9209)  loss_scale: 32768.0000 (60124.7706)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
[2025-01-13 04:07:44,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:07:44,822] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [2080/2809]  eta: 0:04:29  lr: 0.000035  min_lr: 0.000000  loss: 3.9666 (3.9215)  loss_scale: 32768.0000 (60087.7886)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [16]  [2090/2809]  eta: 0:04:26  lr: 0.000035  min_lr: 0.000000  loss: 3.9106 (3.9217)  loss_scale: 65536.0000 (60113.8441)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [16]  [2100/2809]  eta: 0:04:22  lr: 0.000035  min_lr: 0.000000  loss: 3.9453 (3.9220)  loss_scale: 65536.0000 (60139.6516)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [16]  [2110/2809]  eta: 0:04:18  lr: 0.000035  min_lr: 0.000000  loss: 3.8283 (3.9216)  loss_scale: 65536.0000 (60165.2146)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [16]  [2120/2809]  eta: 0:04:14  lr: 0.000035  min_lr: 0.000000  loss: 3.7644 (3.9212)  loss_scale: 65536.0000 (60190.5365)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0001  max mem: 15572
Epoch: [16]  [2130/2809]  eta: 0:04:11  lr: 0.000035  min_lr: 0.000000  loss: 3.7644 (3.9205)  loss_scale: 65536.0000 (60215.6208)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [16]  [2140/2809]  eta: 0:04:07  lr: 0.000035  min_lr: 0.000000  loss: 3.8330 (3.9203)  loss_scale: 65536.0000 (60240.4708)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [16]  [2150/2809]  eta: 0:04:03  lr: 0.000035  min_lr: 0.000000  loss: 3.9587 (3.9202)  loss_scale: 65536.0000 (60265.0897)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0002  max mem: 15572
Epoch: [16]  [2160/2809]  eta: 0:04:00  lr: 0.000035  min_lr: 0.000000  loss: 3.9037 (3.9193)  loss_scale: 65536.0000 (60289.4808)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [16]  [2170/2809]  eta: 0:03:56  lr: 0.000035  min_lr: 0.000000  loss: 3.9519 (3.9202)  loss_scale: 65536.0000 (60313.6472)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [16]  [2180/2809]  eta: 0:03:52  lr: 0.000035  min_lr: 0.000000  loss: 3.5970 (3.9184)  loss_scale: 65536.0000 (60337.5919)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [16]  [2190/2809]  eta: 0:03:49  lr: 0.000035  min_lr: 0.000000  loss: 3.6629 (3.9190)  loss_scale: 65536.0000 (60361.3181)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0001  max mem: 15572
Epoch: [16]  [2200/2809]  eta: 0:03:45  lr: 0.000035  min_lr: 0.000000  loss: 4.0820 (3.9201)  loss_scale: 65536.0000 (60384.8287)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0002  max mem: 15572
[2025-01-13 04:08:32,128] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:08:32,128] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:08:33,976] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47152
[2025-01-13 04:08:33,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:08:33,977] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [2210/2809]  eta: 0:03:41  lr: 0.000035  min_lr: 0.000000  loss: 4.0177 (3.9193)  loss_scale: 65536.0000 (60556.3311)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [16]  [2220/2809]  eta: 0:03:37  lr: 0.000035  min_lr: 0.000000  loss: 3.9342 (3.9200)  loss_scale: 65536.0000 (60578.7519)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [16]  [2230/2809]  eta: 0:03:34  lr: 0.000035  min_lr: 0.000000  loss: 3.9286 (3.9190)  loss_scale: 65536.0000 (60600.9718)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [16]  [2240/2809]  eta: 0:03:30  lr: 0.000035  min_lr: 0.000000  loss: 3.8498 (3.9200)  loss_scale: 65536.0000 (60622.9933)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [16]  [2250/2809]  eta: 0:03:26  lr: 0.000035  min_lr: 0.000000  loss: 3.8176 (3.9175)  loss_scale: 65536.0000 (60644.8192)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [16]  [2260/2809]  eta: 0:03:23  lr: 0.000035  min_lr: 0.000000  loss: 3.5987 (3.9173)  loss_scale: 65536.0000 (60666.4520)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [16]  [2270/2809]  eta: 0:03:19  lr: 0.000035  min_lr: 0.000000  loss: 3.9356 (3.9177)  loss_scale: 65536.0000 (60687.8943)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [16]  [2280/2809]  eta: 0:03:15  lr: 0.000035  min_lr: 0.000000  loss: 4.1372 (3.9189)  loss_scale: 65536.0000 (60709.1486)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [16]  [2290/2809]  eta: 0:03:12  lr: 0.000035  min_lr: 0.000000  loss: 4.1264 (3.9183)  loss_scale: 65536.0000 (60730.2174)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [16]  [2300/2809]  eta: 0:03:08  lr: 0.000035  min_lr: 0.000000  loss: 3.9824 (3.9186)  loss_scale: 65536.0000 (60751.1030)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [16]  [2310/2809]  eta: 0:03:04  lr: 0.000035  min_lr: 0.000000  loss: 3.9225 (3.9180)  loss_scale: 65536.0000 (60771.8079)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [16]  [2320/2809]  eta: 0:03:00  lr: 0.000035  min_lr: 0.000000  loss: 4.0259 (3.9190)  loss_scale: 65536.0000 (60792.3343)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
[2025-01-13 04:09:16,793] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47268
[2025-01-13 04:09:16,793] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 04:09:16,793] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [2330/2809]  eta: 0:02:57  lr: 0.000035  min_lr: 0.000000  loss: 4.1111 (3.9190)  loss_scale: 65536.0000 (60714.2823)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [16]  [2340/2809]  eta: 0:02:53  lr: 0.000035  min_lr: 0.000000  loss: 4.0683 (3.9188)  loss_scale: 32768.0000 (60594.9047)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [16]  [2350/2809]  eta: 0:02:49  lr: 0.000035  min_lr: 0.000000  loss: 4.0683 (3.9191)  loss_scale: 32768.0000 (60476.5427)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [16]  [2360/2809]  eta: 0:02:46  lr: 0.000035  min_lr: 0.000000  loss: 3.9912 (3.9183)  loss_scale: 32768.0000 (60359.1834)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [16]  [2370/2809]  eta: 0:02:42  lr: 0.000035  min_lr: 0.000000  loss: 3.9728 (3.9188)  loss_scale: 32768.0000 (60242.8140)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [16]  [2380/2809]  eta: 0:02:38  lr: 0.000035  min_lr: 0.000000  loss: 4.0179 (3.9187)  loss_scale: 32768.0000 (60127.4221)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [16]  [2390/2809]  eta: 0:02:35  lr: 0.000035  min_lr: 0.000000  loss: 4.0434 (3.9188)  loss_scale: 32768.0000 (60012.9954)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [16]  [2400/2809]  eta: 0:02:31  lr: 0.000035  min_lr: 0.000000  loss: 4.0814 (3.9197)  loss_scale: 32768.0000 (59899.5219)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [16]  [2410/2809]  eta: 0:02:27  lr: 0.000035  min_lr: 0.000000  loss: 4.0879 (3.9203)  loss_scale: 32768.0000 (59786.9896)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [16]  [2420/2809]  eta: 0:02:23  lr: 0.000035  min_lr: 0.000000  loss: 4.0251 (3.9206)  loss_scale: 32768.0000 (59675.3870)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [16]  [2430/2809]  eta: 0:02:20  lr: 0.000035  min_lr: 0.000000  loss: 3.7867 (3.9198)  loss_scale: 32768.0000 (59564.7026)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [16]  [2440/2809]  eta: 0:02:16  lr: 0.000035  min_lr: 0.000000  loss: 3.7867 (3.9195)  loss_scale: 32768.0000 (59454.9250)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [16]  [2450/2809]  eta: 0:02:12  lr: 0.000035  min_lr: 0.000000  loss: 3.9320 (3.9196)  loss_scale: 32768.0000 (59346.0432)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
[2025-01-13 04:10:04,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:10:04,351] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [2460/2809]  eta: 0:02:09  lr: 0.000035  min_lr: 0.000000  loss: 4.1549 (3.9199)  loss_scale: 32768.0000 (59344.5656)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [16]  [2470/2809]  eta: 0:02:05  lr: 0.000035  min_lr: 0.000000  loss: 3.9954 (3.9194)  loss_scale: 65536.0000 (59369.6220)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [16]  [2480/2809]  eta: 0:02:01  lr: 0.000035  min_lr: 0.000000  loss: 3.9338 (3.9197)  loss_scale: 65536.0000 (59394.4764)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [16]  [2490/2809]  eta: 0:01:58  lr: 0.000035  min_lr: 0.000000  loss: 4.1589 (3.9202)  loss_scale: 65536.0000 (59419.1313)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [16]  [2500/2809]  eta: 0:01:54  lr: 0.000035  min_lr: 0.000000  loss: 4.1818 (3.9204)  loss_scale: 65536.0000 (59443.5890)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [16]  [2510/2809]  eta: 0:01:50  lr: 0.000035  min_lr: 0.000000  loss: 3.8153 (3.9199)  loss_scale: 65536.0000 (59467.8519)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [16]  [2520/2809]  eta: 0:01:46  lr: 0.000035  min_lr: 0.000000  loss: 3.7354 (3.9191)  loss_scale: 65536.0000 (59491.9223)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [16]  [2530/2809]  eta: 0:01:43  lr: 0.000035  min_lr: 0.000000  loss: 4.0350 (3.9200)  loss_scale: 65536.0000 (59515.8024)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
[2025-01-13 04:10:35,773] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47482
[2025-01-13 04:10:35,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 04:10:35,773] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [2540/2809]  eta: 0:01:39  lr: 0.000035  min_lr: 0.000000  loss: 4.0690 (3.9198)  loss_scale: 65536.0000 (59500.8076)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [16]  [2550/2809]  eta: 0:01:35  lr: 0.000035  min_lr: 0.000000  loss: 4.0769 (3.9199)  loss_scale: 32768.0000 (59396.0141)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [16]  [2560/2809]  eta: 0:01:32  lr: 0.000035  min_lr: 0.000000  loss: 4.0399 (3.9198)  loss_scale: 32768.0000 (59292.0390)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [16]  [2570/2809]  eta: 0:01:28  lr: 0.000035  min_lr: 0.000000  loss: 3.9920 (3.9199)  loss_scale: 32768.0000 (59188.8728)  weight_decay: 0.0500 (0.0500)  time: 0.3638  data: 0.0001  max mem: 15572
Epoch: [16]  [2580/2809]  eta: 0:01:24  lr: 0.000035  min_lr: 0.000000  loss: 4.0624 (3.9202)  loss_scale: 32768.0000 (59086.5060)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0001  max mem: 15572
Epoch: [16]  [2590/2809]  eta: 0:01:21  lr: 0.000035  min_lr: 0.000000  loss: 3.9758 (3.9205)  loss_scale: 32768.0000 (58984.9294)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [16]  [2600/2809]  eta: 0:01:17  lr: 0.000035  min_lr: 0.000000  loss: 3.9256 (3.9202)  loss_scale: 32768.0000 (58884.1338)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [16]  [2610/2809]  eta: 0:01:13  lr: 0.000035  min_lr: 0.000000  loss: 3.7998 (3.9198)  loss_scale: 32768.0000 (58784.1103)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [16]  [2620/2809]  eta: 0:01:09  lr: 0.000035  min_lr: 0.000000  loss: 4.1188 (3.9204)  loss_scale: 32768.0000 (58684.8501)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [16]  [2630/2809]  eta: 0:01:06  lr: 0.000035  min_lr: 0.000000  loss: 4.1279 (3.9210)  loss_scale: 32768.0000 (58586.3444)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [16]  [2640/2809]  eta: 0:01:02  lr: 0.000035  min_lr: 0.000000  loss: 4.0618 (3.9207)  loss_scale: 32768.0000 (58488.5846)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [16]  [2650/2809]  eta: 0:00:58  lr: 0.000035  min_lr: 0.000000  loss: 3.8469 (3.9204)  loss_scale: 32768.0000 (58391.5624)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [16]  [2660/2809]  eta: 0:00:55  lr: 0.000035  min_lr: 0.000000  loss: 3.8469 (3.9209)  loss_scale: 32768.0000 (58295.2694)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
[2025-01-13 04:11:23,329] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:11:23,329] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [2670/2809]  eta: 0:00:51  lr: 0.000035  min_lr: 0.000000  loss: 4.0625 (3.9209)  loss_scale: 32768.0000 (58248.7697)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [16]  [2680/2809]  eta: 0:00:47  lr: 0.000035  min_lr: 0.000000  loss: 4.0141 (3.9214)  loss_scale: 65536.0000 (58275.9508)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0001  max mem: 15572
Epoch: [16]  [2690/2809]  eta: 0:00:44  lr: 0.000035  min_lr: 0.000000  loss: 3.9901 (3.9209)  loss_scale: 65536.0000 (58302.9298)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [16]  [2700/2809]  eta: 0:00:40  lr: 0.000035  min_lr: 0.000000  loss: 3.9901 (3.9214)  loss_scale: 65536.0000 (58329.7090)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [16]  [2710/2809]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000000  loss: 3.6867 (3.9202)  loss_scale: 65536.0000 (58356.2907)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [16]  [2720/2809]  eta: 0:00:32  lr: 0.000035  min_lr: 0.000000  loss: 3.8153 (3.9201)  loss_scale: 65536.0000 (58382.6770)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [16]  [2730/2809]  eta: 0:00:29  lr: 0.000035  min_lr: 0.000000  loss: 3.9212 (3.9200)  loss_scale: 65536.0000 (58408.8700)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [16]  [2740/2809]  eta: 0:00:25  lr: 0.000035  min_lr: 0.000000  loss: 3.9212 (3.9195)  loss_scale: 65536.0000 (58434.8719)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0002  max mem: 15572
Epoch: [16]  [2750/2809]  eta: 0:00:21  lr: 0.000035  min_lr: 0.000000  loss: 4.2515 (3.9202)  loss_scale: 65536.0000 (58460.6848)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [16]  [2760/2809]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000000  loss: 4.2593 (3.9207)  loss_scale: 65536.0000 (58486.3108)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0001  max mem: 15572
Epoch: [16]  [2770/2809]  eta: 0:00:14  lr: 0.000035  min_lr: 0.000000  loss: 4.1803 (3.9215)  loss_scale: 65536.0000 (58511.7517)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0001  max mem: 15572
Epoch: [16]  [2780/2809]  eta: 0:00:10  lr: 0.000035  min_lr: 0.000000  loss: 4.2322 (3.9221)  loss_scale: 65536.0000 (58537.0097)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [16]  [2790/2809]  eta: 0:00:07  lr: 0.000035  min_lr: 0.000000  loss: 4.0071 (3.9212)  loss_scale: 65536.0000 (58562.0867)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0003  max mem: 15572
[2025-01-13 04:12:10,835] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:12:10,835] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:12:12,621] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47744
[2025-01-13 04:12:12,621] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:12:12,621] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [2800/2809]  eta: 0:00:03  lr: 0.000035  min_lr: 0.000000  loss: 3.8957 (3.9214)  loss_scale: 65536.0000 (58703.9714)  weight_decay: 0.0500 (0.0500)  time: 0.3650  data: 0.0002  max mem: 15572
Epoch: [16]  [2808/2809]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000000  loss: 3.9961 (3.9220)  loss_scale: 65536.0000 (58723.4290)  weight_decay: 0.0500 (0.0500)  time: 0.3605  data: 0.0001  max mem: 15572
Epoch: [16] Total time: 0:17:19 (0.3700 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000000  loss: 3.9961 (3.9220)  loss_scale: 65536.0000 (58723.4290)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:08:37  loss: 0.3531 (0.3531)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.9010  data: 1.7419  max mem: 15572
Val:  [ 10/272]  eta: 0:01:44  loss: 2.9312 (2.6618)  acc1: 27.7778 (32.3232)  acc5: 66.6667 (62.1212)  time: 0.3972  data: 0.2368  max mem: 15572
Val:  [ 20/272]  eta: 0:01:12  loss: 2.8403 (2.6530)  acc1: 38.8889 (37.0370)  acc5: 66.6667 (66.1376)  time: 0.2065  data: 0.0433  max mem: 15572
Val:  [ 30/272]  eta: 0:00:58  loss: 2.7386 (2.7597)  acc1: 38.8889 (32.9749)  acc5: 66.6667 (65.4122)  time: 0.1583  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:51  loss: 2.7386 (2.7308)  acc1: 27.7778 (32.6558)  acc5: 72.2222 (67.4797)  time: 0.1560  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:46  loss: 2.6067 (2.6313)  acc1: 33.3333 (35.4031)  acc5: 72.2222 (70.0436)  time: 0.1573  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:42  loss: 1.6940 (2.5151)  acc1: 55.5556 (39.0710)  acc5: 83.3333 (71.2204)  time: 0.1589  data: 0.0024  max mem: 15572
Val:  [ 70/272]  eta: 0:00:40  loss: 1.6940 (2.4377)  acc1: 61.1111 (41.8623)  acc5: 83.3333 (72.6135)  time: 0.1700  data: 0.0122  max mem: 15572
Val:  [ 80/272]  eta: 0:00:37  loss: 2.1668 (2.4449)  acc1: 50.0000 (42.1125)  acc5: 77.7778 (72.4280)  time: 0.1691  data: 0.0103  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.7110 (2.4875)  acc1: 38.8889 (41.2698)  acc5: 72.2222 (72.3443)  time: 0.1585  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:32  loss: 2.7088 (2.5234)  acc1: 38.8889 (41.0891)  acc5: 72.2222 (72.0022)  time: 0.1615  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:29  loss: 2.8958 (2.6066)  acc1: 11.1111 (38.7888)  acc5: 61.1111 (70.4204)  time: 0.1647  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 3.3175 (2.6553)  acc1: 11.1111 (37.6033)  acc5: 50.0000 (69.4674)  time: 0.1584  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.6774 (2.6152)  acc1: 33.3333 (38.9313)  acc5: 72.2222 (69.9746)  time: 0.1528  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.8668 (2.6054)  acc1: 50.0000 (39.4011)  acc5: 77.7778 (70.0158)  time: 0.1551  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.6641 (2.6070)  acc1: 33.3333 (38.8153)  acc5: 72.2222 (70.4930)  time: 0.1645  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.6329 (2.5903)  acc1: 38.8889 (39.8206)  acc5: 72.2222 (71.0145)  time: 0.1622  data: 0.0005  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.6883 (2.6162)  acc1: 38.8889 (38.9864)  acc5: 72.2222 (70.5003)  time: 0.1565  data: 0.0005  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 2.7415 (2.6093)  acc1: 22.2222 (38.5820)  acc5: 66.6667 (70.9331)  time: 0.1600  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.7146 (2.6524)  acc1: 22.2222 (37.5800)  acc5: 66.6667 (69.5462)  time: 0.1593  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.7146 (2.6536)  acc1: 27.7778 (37.7833)  acc5: 66.6667 (69.6241)  time: 0.1556  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.2187 (2.6473)  acc1: 44.4444 (38.3360)  acc5: 77.7778 (69.6156)  time: 0.1593  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.3659 (2.6411)  acc1: 44.4444 (38.5872)  acc5: 72.2222 (69.6581)  time: 0.1589  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 2.0700 (2.6059)  acc1: 55.5556 (39.7787)  acc5: 77.7778 (70.2501)  time: 0.1563  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.9258 (2.5866)  acc1: 55.5556 (39.9262)  acc5: 83.3333 (70.7469)  time: 0.1562  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.4729 (2.5983)  acc1: 27.7778 (39.1988)  acc5: 77.7778 (70.6065)  time: 0.1536  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.3804 (2.5323)  acc1: 66.6667 (41.1665)  acc5: 88.8889 (71.5198)  time: 0.1458  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4621 (2.5309)  acc1: 66.6667 (40.8979)  acc5: 88.8889 (71.5457)  time: 0.1364  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4621 (2.5353)  acc1: 55.5556 (40.8765)  acc5: 88.8889 (71.5134)  time: 0.1311  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1674 s / it)
* Acc@1 40.877 Acc@5 71.513 loss 2.535
Accuracy of the network on the 4883 val videos: 40.9%
[2025-01-13 04:13:01,164] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 04:13:01,166] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 04:13:01,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 04:13:03,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 04:13:03,558] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 40.88%
Epoch: [17]  [   0/2809]  eta: 3:29:43  lr: 0.000035  min_lr: 0.000000  loss: 3.9302 (3.9302)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.4799  data: 4.1012  max mem: 15572
Epoch: [17]  [  10/2809]  eta: 0:34:48  lr: 0.000035  min_lr: 0.000000  loss: 3.9302 (3.9464)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7461  data: 0.3731  max mem: 15572
Epoch: [17]  [  20/2809]  eta: 0:26:18  lr: 0.000035  min_lr: 0.000000  loss: 3.8763 (3.9165)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [17]  [  30/2809]  eta: 0:23:11  lr: 0.000035  min_lr: 0.000000  loss: 3.7958 (3.9044)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [17]  [  40/2809]  eta: 0:21:35  lr: 0.000035  min_lr: 0.000000  loss: 4.0419 (3.9374)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3648  data: 0.0001  max mem: 15572
Epoch: [17]  [  50/2809]  eta: 0:20:38  lr: 0.000035  min_lr: 0.000000  loss: 3.9624 (3.9223)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [17]  [  60/2809]  eta: 0:19:56  lr: 0.000035  min_lr: 0.000000  loss: 3.9163 (3.9192)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [17]  [  70/2809]  eta: 0:19:25  lr: 0.000035  min_lr: 0.000000  loss: 3.9646 (3.9214)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [17]  [  80/2809]  eta: 0:19:02  lr: 0.000035  min_lr: 0.000000  loss: 4.0451 (3.9174)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [17]  [  90/2809]  eta: 0:18:41  lr: 0.000034  min_lr: 0.000000  loss: 3.9814 (3.8996)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [17]  [ 100/2809]  eta: 0:18:26  lr: 0.000034  min_lr: 0.000000  loss: 3.9814 (3.9132)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [17]  [ 110/2809]  eta: 0:18:12  lr: 0.000034  min_lr: 0.000000  loss: 4.0393 (3.9286)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
[2025-01-13 04:13:52,147] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:13:52,147] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [ 120/2809]  eta: 0:17:59  lr: 0.000034  min_lr: 0.000000  loss: 4.1195 (3.9318)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
[2025-01-13 04:13:55,103] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47881
[2025-01-13 04:13:55,104] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:13:55,104] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 130/2809]  eta: 0:17:48  lr: 0.000034  min_lr: 0.000000  loss: 4.0697 (3.9319)  loss_scale: 65536.0000 (69538.1985)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [17]  [ 140/2809]  eta: 0:17:39  lr: 0.000034  min_lr: 0.000000  loss: 3.8902 (3.9271)  loss_scale: 65536.0000 (69254.3546)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [17]  [ 150/2809]  eta: 0:17:30  lr: 0.000034  min_lr: 0.000000  loss: 3.9090 (3.9321)  loss_scale: 65536.0000 (69008.1060)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [17]  [ 160/2809]  eta: 0:17:22  lr: 0.000034  min_lr: 0.000000  loss: 4.0352 (3.9322)  loss_scale: 65536.0000 (68792.4472)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [17]  [ 170/2809]  eta: 0:17:14  lr: 0.000034  min_lr: 0.000000  loss: 3.9405 (3.9194)  loss_scale: 65536.0000 (68602.0117)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [17]  [ 180/2809]  eta: 0:17:06  lr: 0.000034  min_lr: 0.000000  loss: 3.9052 (3.9212)  loss_scale: 65536.0000 (68432.6188)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [17]  [ 190/2809]  eta: 0:16:59  lr: 0.000034  min_lr: 0.000000  loss: 4.1240 (3.9294)  loss_scale: 65536.0000 (68280.9634)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [17]  [ 200/2809]  eta: 0:16:52  lr: 0.000034  min_lr: 0.000000  loss: 3.9515 (3.9221)  loss_scale: 65536.0000 (68144.3980)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [17]  [ 210/2809]  eta: 0:16:46  lr: 0.000034  min_lr: 0.000000  loss: 3.8240 (3.9196)  loss_scale: 65536.0000 (68020.7773)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [17]  [ 220/2809]  eta: 0:16:40  lr: 0.000034  min_lr: 0.000000  loss: 3.8305 (3.9089)  loss_scale: 65536.0000 (67908.3439)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [17]  [ 230/2809]  eta: 0:16:34  lr: 0.000034  min_lr: 0.000000  loss: 3.9144 (3.9098)  loss_scale: 65536.0000 (67805.6450)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0001  max mem: 15572
Epoch: [17]  [ 240/2809]  eta: 0:16:28  lr: 0.000034  min_lr: 0.000000  loss: 3.9544 (3.9202)  loss_scale: 65536.0000 (67711.4689)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0001  max mem: 15572
[2025-01-13 04:14:38,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=48000, skipped=322, lr=[3.332372618498235e-07, 3.332372618498235e-07, 4.7605323121403363e-07, 4.7605323121403363e-07, 6.800760445914767e-07, 6.800760445914767e-07, 9.715372065592525e-07, 9.715372065592525e-07, 1.3879102950846464e-06, 1.3879102950846464e-06, 1.9827289929780666e-06, 1.9827289929780666e-06, 2.8324699899686663e-06, 2.8324699899686663e-06, 4.046385699955239e-06, 4.046385699955239e-06, 5.780550999936054e-06, 5.780550999936054e-06, 8.25792999990865e-06, 8.25792999990865e-06, 1.1797042857012358e-05, 1.1797042857012358e-05, 1.6852918367160514e-05, 1.6852918367160514e-05, 2.4075597667372162e-05, 2.4075597667372162e-05, 3.4393710953388806e-05, 3.4393710953388806e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 04:14:38,500] [INFO] [timer.py:260:stop] epoch=0/micro_step=48000/global_step=48000, RunningAvgSamplesPerSec=28.88051870000626, CurrSamplesPerSec=32.342618760289476, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [ 250/2809]  eta: 0:16:23  lr: 0.000034  min_lr: 0.000000  loss: 4.0356 (3.9197)  loss_scale: 65536.0000 (67624.7968)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
[2025-01-13 04:14:42,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:14:42,578] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [ 260/2809]  eta: 0:16:17  lr: 0.000034  min_lr: 0.000000  loss: 4.1309 (3.9272)  loss_scale: 65536.0000 (68549.1494)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
[2025-01-13 04:14:44,393] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48015
[2025-01-13 04:14:44,393] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:14:44,393] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 270/2809]  eta: 0:16:12  lr: 0.000034  min_lr: 0.000000  loss: 3.9614 (3.9277)  loss_scale: 65536.0000 (68679.7934)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [17]  [ 280/2809]  eta: 0:16:07  lr: 0.000034  min_lr: 0.000000  loss: 3.8987 (3.9265)  loss_scale: 65536.0000 (68567.9146)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [17]  [ 290/2809]  eta: 0:16:01  lr: 0.000034  min_lr: 0.000000  loss: 3.8881 (3.9204)  loss_scale: 65536.0000 (68463.7251)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [17]  [ 300/2809]  eta: 0:15:57  lr: 0.000034  min_lr: 0.000000  loss: 3.9713 (3.9298)  loss_scale: 65536.0000 (68366.4585)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [17]  [ 310/2809]  eta: 0:15:52  lr: 0.000034  min_lr: 0.000000  loss: 4.1414 (3.9320)  loss_scale: 65536.0000 (68275.4469)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [17]  [ 320/2809]  eta: 0:15:47  lr: 0.000034  min_lr: 0.000000  loss: 4.0091 (3.9278)  loss_scale: 65536.0000 (68190.1059)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [17]  [ 330/2809]  eta: 0:15:42  lr: 0.000034  min_lr: 0.000000  loss: 3.8223 (3.9214)  loss_scale: 65536.0000 (68109.9215)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [17]  [ 340/2809]  eta: 0:15:38  lr: 0.000034  min_lr: 0.000000  loss: 3.8596 (3.9175)  loss_scale: 65536.0000 (68034.4399)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [17]  [ 350/2809]  eta: 0:15:33  lr: 0.000034  min_lr: 0.000000  loss: 3.9550 (3.9156)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [17]  [ 360/2809]  eta: 0:15:29  lr: 0.000034  min_lr: 0.000000  loss: 4.0456 (3.9196)  loss_scale: 65536.0000 (67896.0222)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [17]  [ 370/2809]  eta: 0:15:24  lr: 0.000034  min_lr: 0.000000  loss: 4.0296 (3.9184)  loss_scale: 65536.0000 (67832.4097)  weight_decay: 0.0500 (0.0500)  time: 0.3647  data: 0.0001  max mem: 15572
Epoch: [17]  [ 380/2809]  eta: 0:15:19  lr: 0.000034  min_lr: 0.000000  loss: 3.9886 (3.9195)  loss_scale: 65536.0000 (67772.1365)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [17]  [ 390/2809]  eta: 0:15:15  lr: 0.000034  min_lr: 0.000000  loss: 3.9886 (3.9219)  loss_scale: 65536.0000 (67714.9463)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
[2025-01-13 04:15:31,889] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:15:31,889] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:15:32,621] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48146
[2025-01-13 04:15:32,621] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:15:32,622] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 400/2809]  eta: 0:15:10  lr: 0.000034  min_lr: 0.000000  loss: 4.0268 (3.9192)  loss_scale: 65536.0000 (67987.4713)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [17]  [ 410/2809]  eta: 0:15:06  lr: 0.000034  min_lr: 0.000000  loss: 3.8898 (3.9191)  loss_scale: 65536.0000 (67927.8248)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [17]  [ 420/2809]  eta: 0:15:02  lr: 0.000034  min_lr: 0.000000  loss: 3.8282 (3.9161)  loss_scale: 65536.0000 (67871.0119)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [17]  [ 430/2809]  eta: 0:14:57  lr: 0.000034  min_lr: 0.000000  loss: 3.7366 (3.9109)  loss_scale: 65536.0000 (67816.8353)  weight_decay: 0.0500 (0.0500)  time: 0.3647  data: 0.0002  max mem: 15572
Epoch: [17]  [ 440/2809]  eta: 0:14:53  lr: 0.000034  min_lr: 0.000000  loss: 3.9607 (3.9122)  loss_scale: 65536.0000 (67765.1156)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [17]  [ 450/2809]  eta: 0:14:49  lr: 0.000034  min_lr: 0.000000  loss: 3.8411 (3.9035)  loss_scale: 65536.0000 (67715.6896)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [17]  [ 460/2809]  eta: 0:14:44  lr: 0.000034  min_lr: 0.000000  loss: 3.5799 (3.8992)  loss_scale: 65536.0000 (67668.4078)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [17]  [ 470/2809]  eta: 0:14:40  lr: 0.000034  min_lr: 0.000000  loss: 3.8625 (3.8998)  loss_scale: 65536.0000 (67623.1338)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [17]  [ 480/2809]  eta: 0:14:36  lr: 0.000034  min_lr: 0.000000  loss: 4.0081 (3.9023)  loss_scale: 65536.0000 (67579.7422)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [17]  [ 490/2809]  eta: 0:14:32  lr: 0.000034  min_lr: 0.000000  loss: 3.9550 (3.9042)  loss_scale: 65536.0000 (67538.1181)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [17]  [ 500/2809]  eta: 0:14:28  lr: 0.000034  min_lr: 0.000000  loss: 4.1690 (3.9098)  loss_scale: 65536.0000 (67498.1557)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [17]  [ 510/2809]  eta: 0:14:24  lr: 0.000034  min_lr: 0.000000  loss: 4.1715 (3.9053)  loss_scale: 65536.0000 (67459.7573)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [17]  [ 520/2809]  eta: 0:14:20  lr: 0.000034  min_lr: 0.000000  loss: 4.0491 (3.9091)  loss_scale: 65536.0000 (67422.8330)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
[2025-01-13 04:16:20,130] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:16:20,130] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:16:20,519] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48276
[2025-01-13 04:16:20,519] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:16:20,519] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 530/2809]  eta: 0:14:16  lr: 0.000034  min_lr: 0.000000  loss: 4.1930 (3.9108)  loss_scale: 65536.0000 (67510.7194)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [17]  [ 540/2809]  eta: 0:14:11  lr: 0.000034  min_lr: 0.000000  loss: 4.1861 (3.9144)  loss_scale: 65536.0000 (67474.2181)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [17]  [ 550/2809]  eta: 0:14:07  lr: 0.000034  min_lr: 0.000000  loss: 3.7723 (3.9092)  loss_scale: 65536.0000 (67439.0417)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [17]  [ 560/2809]  eta: 0:14:03  lr: 0.000034  min_lr: 0.000000  loss: 3.6351 (3.9065)  loss_scale: 65536.0000 (67405.1194)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
[2025-01-13 04:16:34,844] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48315
[2025-01-13 04:16:34,844] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 04:16:34,844] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 570/2809]  eta: 0:13:59  lr: 0.000034  min_lr: 0.000000  loss: 3.6180 (3.9006)  loss_scale: 65536.0000 (66855.9019)  weight_decay: 0.0500 (0.0500)  time: 0.3653  data: 0.0002  max mem: 15572
Epoch: [17]  [ 580/2809]  eta: 0:13:55  lr: 0.000034  min_lr: 0.000000  loss: 3.8306 (3.9033)  loss_scale: 32768.0000 (66269.1910)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [17]  [ 590/2809]  eta: 0:13:51  lr: 0.000034  min_lr: 0.000000  loss: 4.1309 (3.9068)  loss_scale: 32768.0000 (65702.3350)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [17]  [ 600/2809]  eta: 0:13:47  lr: 0.000034  min_lr: 0.000000  loss: 4.0261 (3.9052)  loss_scale: 32768.0000 (65154.3428)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [17]  [ 610/2809]  eta: 0:13:43  lr: 0.000034  min_lr: 0.000000  loss: 3.8531 (3.9030)  loss_scale: 32768.0000 (64624.2881)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [17]  [ 620/2809]  eta: 0:13:39  lr: 0.000034  min_lr: 0.000000  loss: 3.8723 (3.9037)  loss_scale: 32768.0000 (64111.3043)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [17]  [ 630/2809]  eta: 0:13:35  lr: 0.000034  min_lr: 0.000000  loss: 3.8723 (3.9031)  loss_scale: 32768.0000 (63614.5800)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [17]  [ 640/2809]  eta: 0:13:31  lr: 0.000034  min_lr: 0.000000  loss: 3.9643 (3.9037)  loss_scale: 32768.0000 (63133.3541)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [17]  [ 650/2809]  eta: 0:13:27  lr: 0.000034  min_lr: 0.000000  loss: 4.0038 (3.9071)  loss_scale: 32768.0000 (62666.9124)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [17]  [ 660/2809]  eta: 0:13:23  lr: 0.000034  min_lr: 0.000000  loss: 3.9973 (3.9072)  loss_scale: 32768.0000 (62214.5840)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [17]  [ 670/2809]  eta: 0:13:20  lr: 0.000034  min_lr: 0.000000  loss: 4.0879 (3.9094)  loss_scale: 32768.0000 (61775.7377)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [17]  [ 680/2809]  eta: 0:13:16  lr: 0.000034  min_lr: 0.000000  loss: 4.1053 (3.9088)  loss_scale: 32768.0000 (61349.7797)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [17]  [ 690/2809]  eta: 0:13:12  lr: 0.000034  min_lr: 0.000000  loss: 4.1752 (3.9136)  loss_scale: 32768.0000 (60936.1505)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
[2025-01-13 04:17:22,386] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:17:22,386] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [ 700/2809]  eta: 0:13:08  lr: 0.000034  min_lr: 0.000000  loss: 4.1173 (3.9158)  loss_scale: 32768.0000 (61001.7689)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [17]  [ 710/2809]  eta: 0:13:04  lr: 0.000034  min_lr: 0.000000  loss: 3.9705 (3.9160)  loss_scale: 65536.0000 (61065.5415)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [17]  [ 720/2809]  eta: 0:13:00  lr: 0.000034  min_lr: 0.000000  loss: 3.9670 (3.9171)  loss_scale: 65536.0000 (61127.5451)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [17]  [ 730/2809]  eta: 0:12:56  lr: 0.000034  min_lr: 0.000000  loss: 4.1109 (3.9164)  loss_scale: 65536.0000 (61187.8523)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [17]  [ 740/2809]  eta: 0:12:52  lr: 0.000034  min_lr: 0.000000  loss: 4.0155 (3.9172)  loss_scale: 65536.0000 (61246.5317)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [17]  [ 750/2809]  eta: 0:12:48  lr: 0.000034  min_lr: 0.000000  loss: 3.8456 (3.9194)  loss_scale: 65536.0000 (61303.6485)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [17]  [ 760/2809]  eta: 0:12:45  lr: 0.000034  min_lr: 0.000000  loss: 3.9276 (3.9183)  loss_scale: 65536.0000 (61359.2641)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [17]  [ 770/2809]  eta: 0:12:41  lr: 0.000034  min_lr: 0.000000  loss: 3.8111 (3.9173)  loss_scale: 65536.0000 (61413.4371)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [17]  [ 780/2809]  eta: 0:12:37  lr: 0.000034  min_lr: 0.000000  loss: 3.6705 (3.9155)  loss_scale: 65536.0000 (61466.2228)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [17]  [ 790/2809]  eta: 0:12:33  lr: 0.000034  min_lr: 0.000000  loss: 3.7704 (3.9162)  loss_scale: 65536.0000 (61517.6738)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0001  max mem: 15572
Epoch: [17]  [ 800/2809]  eta: 0:12:29  lr: 0.000034  min_lr: 0.000000  loss: 3.8077 (3.9137)  loss_scale: 65536.0000 (61567.8402)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [17]  [ 810/2809]  eta: 0:12:26  lr: 0.000034  min_lr: 0.000000  loss: 3.8746 (3.9149)  loss_scale: 65536.0000 (61616.7694)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
[2025-01-13 04:18:09,647] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:18:09,647] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [ 820/2809]  eta: 0:12:22  lr: 0.000034  min_lr: 0.000000  loss: 3.8477 (3.9120)  loss_scale: 65536.0000 (61824.1559)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
[2025-01-13 04:18:10,376] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48574
[2025-01-13 04:18:10,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:18:10,376] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 830/2809]  eta: 0:12:18  lr: 0.000034  min_lr: 0.000000  loss: 3.8686 (3.9127)  loss_scale: 65536.0000 (61868.8231)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [17]  [ 840/2809]  eta: 0:12:14  lr: 0.000034  min_lr: 0.000000  loss: 3.9711 (3.9125)  loss_scale: 65536.0000 (61912.4281)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [17]  [ 850/2809]  eta: 0:12:10  lr: 0.000034  min_lr: 0.000000  loss: 3.9380 (3.9129)  loss_scale: 65536.0000 (61955.0082)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [17]  [ 860/2809]  eta: 0:12:07  lr: 0.000034  min_lr: 0.000000  loss: 3.9015 (3.9125)  loss_scale: 65536.0000 (61996.5993)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [17]  [ 870/2809]  eta: 0:12:03  lr: 0.000034  min_lr: 0.000000  loss: 4.1759 (3.9137)  loss_scale: 65536.0000 (62037.2354)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [17]  [ 880/2809]  eta: 0:11:59  lr: 0.000034  min_lr: 0.000000  loss: 4.0042 (3.9153)  loss_scale: 65536.0000 (62076.9489)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [17]  [ 890/2809]  eta: 0:11:55  lr: 0.000034  min_lr: 0.000000  loss: 3.9514 (3.9131)  loss_scale: 65536.0000 (62115.7710)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [17]  [ 900/2809]  eta: 0:11:51  lr: 0.000034  min_lr: 0.000000  loss: 3.8260 (3.9149)  loss_scale: 65536.0000 (62153.7314)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [17]  [ 910/2809]  eta: 0:11:47  lr: 0.000034  min_lr: 0.000000  loss: 3.9615 (3.9123)  loss_scale: 65536.0000 (62190.8584)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [17]  [ 920/2809]  eta: 0:11:44  lr: 0.000034  min_lr: 0.000000  loss: 3.8533 (3.9108)  loss_scale: 65536.0000 (62227.1792)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [17]  [ 930/2809]  eta: 0:11:40  lr: 0.000034  min_lr: 0.000000  loss: 3.9275 (3.9106)  loss_scale: 65536.0000 (62262.7197)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [17]  [ 940/2809]  eta: 0:11:36  lr: 0.000034  min_lr: 0.000000  loss: 3.9275 (3.9091)  loss_scale: 65536.0000 (62297.5048)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
[2025-01-13 04:18:58,055] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:18:58,055] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [ 950/2809]  eta: 0:11:32  lr: 0.000034  min_lr: 0.000000  loss: 3.9430 (3.9098)  loss_scale: 65536.0000 (62400.4711)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
[2025-01-13 04:18:59,174] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48706
[2025-01-13 04:18:59,174] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:18:59,174] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 960/2809]  eta: 0:11:29  lr: 0.000034  min_lr: 0.000000  loss: 3.7957 (3.9069)  loss_scale: 65536.0000 (62569.4901)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [17]  [ 970/2809]  eta: 0:11:25  lr: 0.000034  min_lr: 0.000000  loss: 3.7085 (3.9044)  loss_scale: 65536.0000 (62600.0412)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [17]  [ 980/2809]  eta: 0:11:21  lr: 0.000034  min_lr: 0.000000  loss: 3.8295 (3.9043)  loss_scale: 65536.0000 (62629.9694)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [17]  [ 990/2809]  eta: 0:11:17  lr: 0.000034  min_lr: 0.000000  loss: 3.8295 (3.9036)  loss_scale: 65536.0000 (62659.2936)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [17]  [1000/2809]  eta: 0:11:13  lr: 0.000034  min_lr: 0.000000  loss: 4.0261 (3.9066)  loss_scale: 65536.0000 (62688.0320)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [17]  [1010/2809]  eta: 0:11:10  lr: 0.000034  min_lr: 0.000000  loss: 4.0453 (3.9065)  loss_scale: 65536.0000 (62716.2018)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [17]  [1020/2809]  eta: 0:11:06  lr: 0.000034  min_lr: 0.000000  loss: 3.8554 (3.9026)  loss_scale: 65536.0000 (62743.8198)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [17]  [1030/2809]  eta: 0:11:02  lr: 0.000034  min_lr: 0.000000  loss: 3.7857 (3.9030)  loss_scale: 65536.0000 (62770.9020)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [17]  [1040/2809]  eta: 0:10:58  lr: 0.000034  min_lr: 0.000000  loss: 3.8414 (3.9011)  loss_scale: 65536.0000 (62797.4640)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [17]  [1050/2809]  eta: 0:10:54  lr: 0.000034  min_lr: 0.000000  loss: 3.8960 (3.9034)  loss_scale: 65536.0000 (62823.5205)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [17]  [1060/2809]  eta: 0:10:51  lr: 0.000034  min_lr: 0.000000  loss: 4.0774 (3.9045)  loss_scale: 65536.0000 (62849.0858)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [17]  [1070/2809]  eta: 0:10:47  lr: 0.000034  min_lr: 0.000000  loss: 3.7986 (3.9023)  loss_scale: 65536.0000 (62874.1737)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [17]  [1080/2809]  eta: 0:10:43  lr: 0.000034  min_lr: 0.000000  loss: 3.6789 (3.8999)  loss_scale: 65536.0000 (62898.7974)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
[2025-01-13 04:19:46,825] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:19:46,825] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:19:47,553] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48837
[2025-01-13 04:19:47,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:19:47,553] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1090/2809]  eta: 0:10:39  lr: 0.000034  min_lr: 0.000000  loss: 3.7006 (3.8990)  loss_scale: 65536.0000 (63043.1091)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [17]  [1100/2809]  eta: 0:10:36  lr: 0.000034  min_lr: 0.000000  loss: 3.7821 (3.8971)  loss_scale: 65536.0000 (63065.7511)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0002  max mem: 15572
Epoch: [17]  [1110/2809]  eta: 0:10:32  lr: 0.000034  min_lr: 0.000000  loss: 3.8154 (3.8970)  loss_scale: 65536.0000 (63087.9856)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [17]  [1120/2809]  eta: 0:10:28  lr: 0.000034  min_lr: 0.000000  loss: 3.7488 (3.8958)  loss_scale: 65536.0000 (63109.8234)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [17]  [1130/2809]  eta: 0:10:24  lr: 0.000034  min_lr: 0.000000  loss: 3.9126 (3.8984)  loss_scale: 65536.0000 (63131.2750)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [17]  [1140/2809]  eta: 0:10:21  lr: 0.000034  min_lr: 0.000000  loss: 4.1814 (3.8997)  loss_scale: 65536.0000 (63152.3506)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [17]  [1150/2809]  eta: 0:10:17  lr: 0.000034  min_lr: 0.000000  loss: 4.0847 (3.8994)  loss_scale: 65536.0000 (63173.0599)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [17]  [1160/2809]  eta: 0:10:13  lr: 0.000034  min_lr: 0.000000  loss: 4.0127 (3.9009)  loss_scale: 65536.0000 (63193.4126)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [17]  [1170/2809]  eta: 0:10:09  lr: 0.000034  min_lr: 0.000000  loss: 4.0070 (3.9021)  loss_scale: 65536.0000 (63213.4176)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [17]  [1180/2809]  eta: 0:10:06  lr: 0.000034  min_lr: 0.000000  loss: 3.9780 (3.9035)  loss_scale: 65536.0000 (63233.0838)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [17]  [1190/2809]  eta: 0:10:02  lr: 0.000034  min_lr: 0.000000  loss: 4.0335 (3.9043)  loss_scale: 65536.0000 (63252.4198)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [17]  [1200/2809]  eta: 0:09:58  lr: 0.000034  min_lr: 0.000000  loss: 3.9788 (3.9025)  loss_scale: 65536.0000 (63271.4338)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [17]  [1210/2809]  eta: 0:09:54  lr: 0.000034  min_lr: 0.000000  loss: 3.9219 (3.9019)  loss_scale: 65536.0000 (63290.1338)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
[2025-01-13 04:20:35,195] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:20:35,195] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:20:36,298] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48969
[2025-01-13 04:20:36,298] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:20:36,298] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1220/2809]  eta: 0:09:50  lr: 0.000034  min_lr: 0.000000  loss: 3.8444 (3.9012)  loss_scale: 65536.0000 (63469.5495)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [17]  [1230/2809]  eta: 0:09:47  lr: 0.000034  min_lr: 0.000000  loss: 4.0062 (3.9042)  loss_scale: 65536.0000 (63486.3363)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [17]  [1240/2809]  eta: 0:09:43  lr: 0.000034  min_lr: 0.000000  loss: 3.9894 (3.9024)  loss_scale: 65536.0000 (63502.8525)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
[2025-01-13 04:20:47,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=49000, skipped=330, lr=[3.267739872028712e-07, 3.267739872028712e-07, 4.6681998171838747e-07, 4.6681998171838747e-07, 6.66885688169125e-07, 6.66885688169125e-07, 9.526938402416073e-07, 9.526938402416073e-07, 1.3609912003451532e-06, 1.3609912003451532e-06, 1.944273143350219e-06, 1.944273143350219e-06, 2.7775330619288843e-06, 2.7775330619288843e-06, 3.967904374184121e-06, 3.967904374184121e-06, 5.66843482026303e-06, 5.66843482026303e-06, 8.097764028947187e-06, 8.097764028947187e-06, 1.1568234327067409e-05, 1.1568234327067409e-05, 1.652604903866773e-05, 1.652604903866773e-05, 2.3608641483811045e-05, 2.3608641483811045e-05, 3.3726630691158636e-05, 3.3726630691158636e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 04:20:47,320] [INFO] [timer.py:260:stop] epoch=0/micro_step=49000/global_step=49000, RunningAvgSamplesPerSec=28.975740475966468, CurrSamplesPerSec=34.59590719004924, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [1250/2809]  eta: 0:09:39  lr: 0.000034  min_lr: 0.000000  loss: 3.6490 (3.9017)  loss_scale: 65536.0000 (63519.1047)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [17]  [1260/2809]  eta: 0:09:35  lr: 0.000034  min_lr: 0.000000  loss: 4.0713 (3.9042)  loss_scale: 65536.0000 (63535.0991)  weight_decay: 0.0500 (0.0500)  time: 0.3649  data: 0.0002  max mem: 15572
Epoch: [17]  [1270/2809]  eta: 0:09:32  lr: 0.000034  min_lr: 0.000000  loss: 4.0910 (3.9038)  loss_scale: 65536.0000 (63550.8419)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [17]  [1280/2809]  eta: 0:09:28  lr: 0.000034  min_lr: 0.000000  loss: 3.7261 (3.9020)  loss_scale: 65536.0000 (63566.3388)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [17]  [1290/2809]  eta: 0:09:24  lr: 0.000034  min_lr: 0.000000  loss: 3.6894 (3.9010)  loss_scale: 65536.0000 (63581.5957)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [17]  [1300/2809]  eta: 0:09:20  lr: 0.000034  min_lr: 0.000000  loss: 4.0669 (3.9024)  loss_scale: 65536.0000 (63596.6180)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [17]  [1310/2809]  eta: 0:09:17  lr: 0.000034  min_lr: 0.000000  loss: 4.0504 (3.9024)  loss_scale: 65536.0000 (63611.4111)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [17]  [1320/2809]  eta: 0:09:13  lr: 0.000034  min_lr: 0.000000  loss: 3.7010 (3.9013)  loss_scale: 65536.0000 (63625.9803)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [17]  [1330/2809]  eta: 0:09:09  lr: 0.000034  min_lr: 0.000000  loss: 3.7768 (3.9014)  loss_scale: 65536.0000 (63640.3306)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [17]  [1340/2809]  eta: 0:09:05  lr: 0.000034  min_lr: 0.000000  loss: 3.8718 (3.9015)  loss_scale: 65536.0000 (63654.4668)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
[2025-01-13 04:21:23,894] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:21:23,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:21:24,258] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49099
[2025-01-13 04:21:24,258] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:21:24,258] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1350/2809]  eta: 0:09:02  lr: 0.000034  min_lr: 0.000000  loss: 3.9858 (3.9017)  loss_scale: 65536.0000 (63716.9030)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [17]  [1360/2809]  eta: 0:08:58  lr: 0.000034  min_lr: 0.000000  loss: 4.0520 (3.9035)  loss_scale: 65536.0000 (63730.2689)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [17]  [1370/2809]  eta: 0:08:54  lr: 0.000034  min_lr: 0.000000  loss: 3.8951 (3.9007)  loss_scale: 65536.0000 (63743.4398)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [17]  [1380/2809]  eta: 0:08:50  lr: 0.000034  min_lr: 0.000000  loss: 3.7357 (3.9009)  loss_scale: 65536.0000 (63756.4200)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [17]  [1390/2809]  eta: 0:08:47  lr: 0.000034  min_lr: 0.000000  loss: 3.9295 (3.9011)  loss_scale: 65536.0000 (63769.2135)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [17]  [1400/2809]  eta: 0:08:43  lr: 0.000034  min_lr: 0.000000  loss: 3.9915 (3.8999)  loss_scale: 65536.0000 (63781.8244)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [17]  [1410/2809]  eta: 0:08:39  lr: 0.000034  min_lr: 0.000000  loss: 4.1262 (3.9017)  loss_scale: 65536.0000 (63794.2566)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [17]  [1420/2809]  eta: 0:08:35  lr: 0.000034  min_lr: 0.000000  loss: 4.0813 (3.9018)  loss_scale: 65536.0000 (63806.5137)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [17]  [1430/2809]  eta: 0:08:32  lr: 0.000034  min_lr: 0.000000  loss: 3.9468 (3.9014)  loss_scale: 65536.0000 (63818.5996)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [17]  [1440/2809]  eta: 0:08:28  lr: 0.000034  min_lr: 0.000000  loss: 4.0059 (3.9027)  loss_scale: 65536.0000 (63830.5177)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [17]  [1450/2809]  eta: 0:08:24  lr: 0.000034  min_lr: 0.000000  loss: 3.9793 (3.9021)  loss_scale: 65536.0000 (63842.2715)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [17]  [1460/2809]  eta: 0:08:21  lr: 0.000034  min_lr: 0.000000  loss: 3.9793 (3.9041)  loss_scale: 65536.0000 (63853.8645)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [17]  [1470/2809]  eta: 0:08:17  lr: 0.000034  min_lr: 0.000000  loss: 3.9465 (3.9034)  loss_scale: 65536.0000 (63865.2998)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
[2025-01-13 04:22:12,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:22:12,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:22:12,426] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49229
[2025-01-13 04:22:12,426] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:22:12,426] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1480/2809]  eta: 0:08:13  lr: 0.000034  min_lr: 0.000000  loss: 3.8006 (3.9027)  loss_scale: 65536.0000 (63920.8319)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [17]  [1490/2809]  eta: 0:08:09  lr: 0.000034  min_lr: 0.000000  loss: 3.6881 (3.9017)  loss_scale: 65536.0000 (63931.6647)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [17]  [1500/2809]  eta: 0:08:06  lr: 0.000034  min_lr: 0.000000  loss: 3.6881 (3.9005)  loss_scale: 65536.0000 (63942.3531)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [17]  [1510/2809]  eta: 0:08:02  lr: 0.000034  min_lr: 0.000000  loss: 3.8720 (3.9011)  loss_scale: 65536.0000 (63952.9001)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [17]  [1520/2809]  eta: 0:07:58  lr: 0.000034  min_lr: 0.000000  loss: 4.0739 (3.9019)  loss_scale: 65536.0000 (63963.3083)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [17]  [1530/2809]  eta: 0:07:55  lr: 0.000034  min_lr: 0.000000  loss: 4.0916 (3.9019)  loss_scale: 65536.0000 (63973.5807)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [17]  [1540/2809]  eta: 0:07:51  lr: 0.000034  min_lr: 0.000000  loss: 3.8672 (3.9014)  loss_scale: 65536.0000 (63983.7197)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [17]  [1550/2809]  eta: 0:07:47  lr: 0.000034  min_lr: 0.000000  loss: 3.8672 (3.9019)  loss_scale: 65536.0000 (63993.7279)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [17]  [1560/2809]  eta: 0:07:43  lr: 0.000034  min_lr: 0.000000  loss: 3.9024 (3.9014)  loss_scale: 65536.0000 (64003.6079)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [17]  [1570/2809]  eta: 0:07:40  lr: 0.000034  min_lr: 0.000000  loss: 3.7175 (3.9006)  loss_scale: 65536.0000 (64013.3622)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [17]  [1580/2809]  eta: 0:07:36  lr: 0.000034  min_lr: 0.000000  loss: 3.8241 (3.8999)  loss_scale: 65536.0000 (64022.9930)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0001  max mem: 15572
Epoch: [17]  [1590/2809]  eta: 0:07:32  lr: 0.000033  min_lr: 0.000000  loss: 3.8669 (3.8998)  loss_scale: 65536.0000 (64032.5028)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [17]  [1600/2809]  eta: 0:07:28  lr: 0.000033  min_lr: 0.000000  loss: 3.8563 (3.8996)  loss_scale: 65536.0000 (64041.8938)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
[2025-01-13 04:23:00,062] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:23:00,062] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [1610/2809]  eta: 0:07:25  lr: 0.000033  min_lr: 0.000000  loss: 3.8610 (3.9010)  loss_scale: 65536.0000 (64295.2502)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
[2025-01-13 04:23:03,022] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49366
[2025-01-13 04:23:03,022] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:23:03,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1620/2809]  eta: 0:07:21  lr: 0.000033  min_lr: 0.000000  loss: 4.0401 (3.9030)  loss_scale: 65536.0000 (64383.7631)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [17]  [1630/2809]  eta: 0:07:17  lr: 0.000033  min_lr: 0.000000  loss: 4.1032 (3.9036)  loss_scale: 65536.0000 (64390.8277)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [17]  [1640/2809]  eta: 0:07:14  lr: 0.000033  min_lr: 0.000000  loss: 4.0950 (3.9039)  loss_scale: 65536.0000 (64397.8062)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [17]  [1650/2809]  eta: 0:07:10  lr: 0.000033  min_lr: 0.000000  loss: 3.9529 (3.9048)  loss_scale: 65536.0000 (64404.7002)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [17]  [1660/2809]  eta: 0:07:06  lr: 0.000033  min_lr: 0.000000  loss: 4.0030 (3.9052)  loss_scale: 65536.0000 (64411.5111)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [17]  [1670/2809]  eta: 0:07:02  lr: 0.000033  min_lr: 0.000000  loss: 4.0030 (3.9051)  loss_scale: 65536.0000 (64418.2406)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [17]  [1680/2809]  eta: 0:06:59  lr: 0.000033  min_lr: 0.000000  loss: 3.7335 (3.9035)  loss_scale: 65536.0000 (64424.8899)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [17]  [1690/2809]  eta: 0:06:55  lr: 0.000033  min_lr: 0.000000  loss: 3.7739 (3.9041)  loss_scale: 65536.0000 (64431.4607)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [17]  [1700/2809]  eta: 0:06:51  lr: 0.000033  min_lr: 0.000000  loss: 4.1247 (3.9053)  loss_scale: 65536.0000 (64437.9541)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [17]  [1710/2809]  eta: 0:06:47  lr: 0.000033  min_lr: 0.000000  loss: 3.9529 (3.9057)  loss_scale: 65536.0000 (64444.3717)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [17]  [1720/2809]  eta: 0:06:44  lr: 0.000033  min_lr: 0.000000  loss: 3.8473 (3.9044)  loss_scale: 65536.0000 (64450.7147)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [17]  [1730/2809]  eta: 0:06:40  lr: 0.000033  min_lr: 0.000000  loss: 3.8543 (3.9049)  loss_scale: 65536.0000 (64456.9844)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [17]  [1740/2809]  eta: 0:06:36  lr: 0.000033  min_lr: 0.000000  loss: 3.9644 (3.9043)  loss_scale: 65536.0000 (64463.1821)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
[2025-01-13 04:23:50,703] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:23:50,703] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:23:51,439] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49497
[2025-01-13 04:23:51,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:23:51,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1750/2809]  eta: 0:06:33  lr: 0.000033  min_lr: 0.000000  loss: 3.9843 (3.9044)  loss_scale: 65536.0000 (64544.1645)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [17]  [1760/2809]  eta: 0:06:29  lr: 0.000033  min_lr: 0.000000  loss: 4.1405 (3.9058)  loss_scale: 65536.0000 (64549.7967)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [17]  [1770/2809]  eta: 0:06:25  lr: 0.000033  min_lr: 0.000000  loss: 4.0264 (3.9059)  loss_scale: 65536.0000 (64555.3653)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [17]  [1780/2809]  eta: 0:06:21  lr: 0.000033  min_lr: 0.000000  loss: 3.8242 (3.9043)  loss_scale: 65536.0000 (64560.8714)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0001  max mem: 15572
Epoch: [17]  [1790/2809]  eta: 0:06:18  lr: 0.000033  min_lr: 0.000000  loss: 3.7588 (3.9037)  loss_scale: 65536.0000 (64566.3160)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [17]  [1800/2809]  eta: 0:06:14  lr: 0.000033  min_lr: 0.000000  loss: 3.8282 (3.9039)  loss_scale: 65536.0000 (64571.7002)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [17]  [1810/2809]  eta: 0:06:10  lr: 0.000033  min_lr: 0.000000  loss: 3.6204 (3.9016)  loss_scale: 65536.0000 (64577.0248)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [17]  [1820/2809]  eta: 0:06:07  lr: 0.000033  min_lr: 0.000000  loss: 3.5769 (3.9011)  loss_scale: 65536.0000 (64582.2910)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [17]  [1830/2809]  eta: 0:06:03  lr: 0.000033  min_lr: 0.000000  loss: 3.9129 (3.9015)  loss_scale: 65536.0000 (64587.4997)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [17]  [1840/2809]  eta: 0:05:59  lr: 0.000033  min_lr: 0.000000  loss: 3.8783 (3.9015)  loss_scale: 65536.0000 (64592.6518)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [17]  [1850/2809]  eta: 0:05:55  lr: 0.000033  min_lr: 0.000000  loss: 3.8242 (3.9015)  loss_scale: 65536.0000 (64597.7482)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [17]  [1860/2809]  eta: 0:05:52  lr: 0.000033  min_lr: 0.000000  loss: 3.8975 (3.9024)  loss_scale: 65536.0000 (64602.7899)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [17]  [1870/2809]  eta: 0:05:48  lr: 0.000033  min_lr: 0.000000  loss: 3.8962 (3.9009)  loss_scale: 65536.0000 (64607.7777)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 04:24:39,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:24:39,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:24:39,496] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49627
[2025-01-13 04:24:39,496] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:24:39,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1880/2809]  eta: 0:05:44  lr: 0.000033  min_lr: 0.000000  loss: 3.7675 (3.8997)  loss_scale: 65536.0000 (64647.5534)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [17]  [1890/2809]  eta: 0:05:40  lr: 0.000033  min_lr: 0.000000  loss: 3.8039 (3.8993)  loss_scale: 65536.0000 (64652.2517)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0003  max mem: 15572
Epoch: [17]  [1900/2809]  eta: 0:05:37  lr: 0.000033  min_lr: 0.000000  loss: 3.8969 (3.8999)  loss_scale: 65536.0000 (64656.9006)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [17]  [1910/2809]  eta: 0:05:33  lr: 0.000033  min_lr: 0.000000  loss: 3.7902 (3.8992)  loss_scale: 65536.0000 (64661.5008)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [17]  [1920/2809]  eta: 0:05:29  lr: 0.000033  min_lr: 0.000000  loss: 3.8282 (3.8995)  loss_scale: 65536.0000 (64666.0531)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [17]  [1930/2809]  eta: 0:05:26  lr: 0.000033  min_lr: 0.000000  loss: 3.9410 (3.8993)  loss_scale: 65536.0000 (64670.5583)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [17]  [1940/2809]  eta: 0:05:22  lr: 0.000033  min_lr: 0.000000  loss: 3.9650 (3.8994)  loss_scale: 65536.0000 (64675.0170)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0001  max mem: 15572
Epoch: [17]  [1950/2809]  eta: 0:05:18  lr: 0.000033  min_lr: 0.000000  loss: 3.8871 (3.8989)  loss_scale: 65536.0000 (64679.4300)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [17]  [1960/2809]  eta: 0:05:14  lr: 0.000033  min_lr: 0.000000  loss: 3.6249 (3.8970)  loss_scale: 65536.0000 (64683.7981)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [17]  [1970/2809]  eta: 0:05:11  lr: 0.000033  min_lr: 0.000000  loss: 3.6248 (3.8962)  loss_scale: 65536.0000 (64688.1218)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [17]  [1980/2809]  eta: 0:05:07  lr: 0.000033  min_lr: 0.000000  loss: 3.7513 (3.8963)  loss_scale: 65536.0000 (64692.4018)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [17]  [1990/2809]  eta: 0:05:03  lr: 0.000033  min_lr: 0.000000  loss: 3.8513 (3.8963)  loss_scale: 65536.0000 (64696.6389)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [17]  [2000/2809]  eta: 0:05:00  lr: 0.000033  min_lr: 0.000000  loss: 3.8296 (3.8956)  loss_scale: 65536.0000 (64700.8336)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
[2025-01-13 04:25:27,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:25:27,265] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:25:28,389] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49759
[2025-01-13 04:25:28,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:25:28,389] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2010/2809]  eta: 0:04:56  lr: 0.000033  min_lr: 0.000000  loss: 3.6302 (3.8945)  loss_scale: 65536.0000 (64802.7529)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [17]  [2020/2809]  eta: 0:04:52  lr: 0.000033  min_lr: 0.000000  loss: 3.6610 (3.8948)  loss_scale: 65536.0000 (64806.3810)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [17]  [2030/2809]  eta: 0:04:48  lr: 0.000033  min_lr: 0.000000  loss: 3.8706 (3.8950)  loss_scale: 65536.0000 (64809.9734)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [17]  [2040/2809]  eta: 0:04:45  lr: 0.000033  min_lr: 0.000000  loss: 3.7691 (3.8953)  loss_scale: 65536.0000 (64813.5306)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [17]  [2050/2809]  eta: 0:04:41  lr: 0.000033  min_lr: 0.000000  loss: 3.7387 (3.8938)  loss_scale: 65536.0000 (64817.0531)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [17]  [2060/2809]  eta: 0:04:37  lr: 0.000033  min_lr: 0.000000  loss: 3.7853 (3.8940)  loss_scale: 65536.0000 (64820.5415)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [17]  [2070/2809]  eta: 0:04:34  lr: 0.000033  min_lr: 0.000000  loss: 3.9409 (3.8946)  loss_scale: 65536.0000 (64823.9961)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [17]  [2080/2809]  eta: 0:04:30  lr: 0.000033  min_lr: 0.000000  loss: 3.9026 (3.8941)  loss_scale: 65536.0000 (64827.4176)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [17]  [2090/2809]  eta: 0:04:26  lr: 0.000033  min_lr: 0.000000  loss: 3.8334 (3.8942)  loss_scale: 65536.0000 (64830.8063)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [17]  [2100/2809]  eta: 0:04:22  lr: 0.000033  min_lr: 0.000000  loss: 3.9258 (3.8935)  loss_scale: 65536.0000 (64834.1628)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [17]  [2110/2809]  eta: 0:04:19  lr: 0.000033  min_lr: 0.000000  loss: 3.9917 (3.8933)  loss_scale: 65536.0000 (64837.4874)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [17]  [2120/2809]  eta: 0:04:15  lr: 0.000033  min_lr: 0.000000  loss: 3.9752 (3.8938)  loss_scale: 65536.0000 (64840.7808)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [17]  [2130/2809]  eta: 0:04:11  lr: 0.000033  min_lr: 0.000000  loss: 3.8910 (3.8940)  loss_scale: 65536.0000 (64844.0432)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
[2025-01-13 04:26:15,925] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:26:15,925] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:26:16,292] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49889
[2025-01-13 04:26:16,292] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:26:16,292] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2140/2809]  eta: 0:04:08  lr: 0.000033  min_lr: 0.000000  loss: 4.0399 (3.8951)  loss_scale: 65536.0000 (64877.8851)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [17]  [2150/2809]  eta: 0:04:04  lr: 0.000033  min_lr: 0.000000  loss: 4.1132 (3.8961)  loss_scale: 65536.0000 (64880.9447)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [17]  [2160/2809]  eta: 0:04:00  lr: 0.000033  min_lr: 0.000000  loss: 4.0600 (3.8960)  loss_scale: 65536.0000 (64883.9759)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [17]  [2170/2809]  eta: 0:03:56  lr: 0.000033  min_lr: 0.000000  loss: 4.0600 (3.8974)  loss_scale: 65536.0000 (64886.9793)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [17]  [2180/2809]  eta: 0:03:53  lr: 0.000033  min_lr: 0.000000  loss: 4.1939 (3.8994)  loss_scale: 65536.0000 (64889.9551)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [17]  [2190/2809]  eta: 0:03:49  lr: 0.000033  min_lr: 0.000000  loss: 4.1523 (3.9003)  loss_scale: 65536.0000 (64892.9037)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [17]  [2200/2809]  eta: 0:03:45  lr: 0.000033  min_lr: 0.000000  loss: 3.9552 (3.9000)  loss_scale: 65536.0000 (64895.8255)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [17]  [2210/2809]  eta: 0:03:42  lr: 0.000033  min_lr: 0.000000  loss: 4.0028 (3.9004)  loss_scale: 65536.0000 (64898.7209)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [17]  [2220/2809]  eta: 0:03:38  lr: 0.000033  min_lr: 0.000000  loss: 3.6946 (3.8987)  loss_scale: 65536.0000 (64901.5903)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [17]  [2230/2809]  eta: 0:03:34  lr: 0.000033  min_lr: 0.000000  loss: 3.7811 (3.8995)  loss_scale: 65536.0000 (64904.4339)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [17]  [2240/2809]  eta: 0:03:30  lr: 0.000033  min_lr: 0.000000  loss: 4.0255 (3.8993)  loss_scale: 65536.0000 (64907.2521)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
[2025-01-13 04:26:56,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=50000, skipped=337, lr=[3.202091611303319e-07, 3.202091611303319e-07, 4.5744165875761706e-07, 4.5744165875761706e-07, 6.534880839394531e-07, 6.534880839394531e-07, 9.3355440562779e-07, 9.3355440562779e-07, 1.333649150896843e-06, 1.333649150896843e-06, 1.9052130727097759e-06, 1.9052130727097759e-06, 2.7217329610139656e-06, 2.7217329610139656e-06, 3.888189944305666e-06, 3.888189944305666e-06, 5.554557063293808e-06, 5.554557063293808e-06, 7.935081518991155e-06, 7.935081518991155e-06, 1.1335830741415936e-05, 1.1335830741415936e-05, 1.619404391630848e-05, 1.619404391630848e-05, 2.313434845186926e-05, 2.313434845186926e-05, 3.304906921695609e-05, 3.304906921695609e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 04:26:56,926] [INFO] [timer.py:260:stop] epoch=0/micro_step=50000/global_step=50000, RunningAvgSamplesPerSec=29.066650946546957, CurrSamplesPerSec=34.179437188427585, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [2250/2809]  eta: 0:03:27  lr: 0.000033  min_lr: 0.000000  loss: 3.7314 (3.8979)  loss_scale: 65536.0000 (64910.0453)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [17]  [2260/2809]  eta: 0:03:23  lr: 0.000033  min_lr: 0.000000  loss: 3.7314 (3.8977)  loss_scale: 65536.0000 (64912.8138)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
[2025-01-13 04:27:03,948] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:27:03,948] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [2270/2809]  eta: 0:03:19  lr: 0.000033  min_lr: 0.000000  loss: 3.8580 (3.8972)  loss_scale: 65536.0000 (65088.7045)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
[2025-01-13 04:27:06,524] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50025
[2025-01-13 04:27:06,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:27:06,524] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2280/2809]  eta: 0:03:16  lr: 0.000033  min_lr: 0.000000  loss: 3.7850 (3.8964)  loss_scale: 65536.0000 (65119.3968)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [17]  [2290/2809]  eta: 0:03:12  lr: 0.000033  min_lr: 0.000000  loss: 3.6132 (3.8955)  loss_scale: 65536.0000 (65121.2152)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0001  max mem: 15572
Epoch: [17]  [2300/2809]  eta: 0:03:08  lr: 0.000033  min_lr: 0.000000  loss: 3.9399 (3.8965)  loss_scale: 65536.0000 (65123.0178)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [17]  [2310/2809]  eta: 0:03:04  lr: 0.000033  min_lr: 0.000000  loss: 4.1184 (3.8963)  loss_scale: 65536.0000 (65124.8048)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [17]  [2320/2809]  eta: 0:03:01  lr: 0.000033  min_lr: 0.000000  loss: 4.1400 (3.8977)  loss_scale: 65536.0000 (65126.5765)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0001  max mem: 15572
Epoch: [17]  [2330/2809]  eta: 0:02:57  lr: 0.000033  min_lr: 0.000000  loss: 3.9816 (3.8974)  loss_scale: 65536.0000 (65128.3329)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0001  max mem: 15572
Epoch: [17]  [2340/2809]  eta: 0:02:53  lr: 0.000033  min_lr: 0.000000  loss: 3.6607 (3.8964)  loss_scale: 65536.0000 (65130.0743)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [17]  [2350/2809]  eta: 0:02:50  lr: 0.000033  min_lr: 0.000000  loss: 3.7002 (3.8962)  loss_scale: 65536.0000 (65131.8009)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [17]  [2360/2809]  eta: 0:02:46  lr: 0.000033  min_lr: 0.000000  loss: 3.6880 (3.8962)  loss_scale: 65536.0000 (65133.5129)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [17]  [2370/2809]  eta: 0:02:42  lr: 0.000033  min_lr: 0.000000  loss: 3.8865 (3.8964)  loss_scale: 65536.0000 (65135.2105)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [17]  [2380/2809]  eta: 0:02:38  lr: 0.000033  min_lr: 0.000000  loss: 3.8865 (3.8959)  loss_scale: 65536.0000 (65136.8937)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [17]  [2390/2809]  eta: 0:02:35  lr: 0.000033  min_lr: 0.000000  loss: 3.9994 (3.8960)  loss_scale: 65536.0000 (65138.5629)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [17]  [2400/2809]  eta: 0:02:31  lr: 0.000033  min_lr: 0.000000  loss: 4.0138 (3.8964)  loss_scale: 65536.0000 (65140.2182)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
[2025-01-13 04:27:54,015] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:27:54,015] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:27:55,479] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50158
[2025-01-13 04:27:55,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:27:55,480] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2410/2809]  eta: 0:02:27  lr: 0.000033  min_lr: 0.000000  loss: 3.8988 (3.8954)  loss_scale: 65536.0000 (65250.5881)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [17]  [2420/2809]  eta: 0:02:24  lr: 0.000033  min_lr: 0.000000  loss: 3.9041 (3.8959)  loss_scale: 65536.0000 (65251.7670)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [17]  [2430/2809]  eta: 0:02:20  lr: 0.000033  min_lr: 0.000000  loss: 3.8853 (3.8953)  loss_scale: 65536.0000 (65252.9362)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0001  max mem: 15572
Epoch: [17]  [2440/2809]  eta: 0:02:16  lr: 0.000033  min_lr: 0.000000  loss: 3.8569 (3.8956)  loss_scale: 65536.0000 (65254.0959)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [17]  [2450/2809]  eta: 0:02:13  lr: 0.000033  min_lr: 0.000000  loss: 4.0537 (3.8954)  loss_scale: 65536.0000 (65255.2460)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [17]  [2460/2809]  eta: 0:02:09  lr: 0.000033  min_lr: 0.000000  loss: 4.1212 (3.8960)  loss_scale: 65536.0000 (65256.3868)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [17]  [2470/2809]  eta: 0:02:05  lr: 0.000033  min_lr: 0.000000  loss: 3.8559 (3.8956)  loss_scale: 65536.0000 (65257.5184)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [17]  [2480/2809]  eta: 0:02:01  lr: 0.000033  min_lr: 0.000000  loss: 3.7953 (3.8951)  loss_scale: 65536.0000 (65258.6409)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [17]  [2490/2809]  eta: 0:01:58  lr: 0.000033  min_lr: 0.000000  loss: 3.7953 (3.8950)  loss_scale: 65536.0000 (65259.7543)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [17]  [2500/2809]  eta: 0:01:54  lr: 0.000033  min_lr: 0.000000  loss: 3.8950 (3.8950)  loss_scale: 65536.0000 (65260.8589)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [17]  [2510/2809]  eta: 0:01:50  lr: 0.000033  min_lr: 0.000000  loss: 3.8927 (3.8951)  loss_scale: 65536.0000 (65261.9546)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [17]  [2520/2809]  eta: 0:01:47  lr: 0.000033  min_lr: 0.000000  loss: 3.8934 (3.8954)  loss_scale: 65536.0000 (65263.0417)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [17]  [2530/2809]  eta: 0:01:43  lr: 0.000033  min_lr: 0.000000  loss: 3.8934 (3.8947)  loss_scale: 65536.0000 (65264.1201)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
[2025-01-13 04:28:43,335] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:28:43,335] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:28:44,067] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50289
[2025-01-13 04:28:44,067] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:28:44,069] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2540/2809]  eta: 0:01:39  lr: 0.000033  min_lr: 0.000000  loss: 3.9465 (3.8955)  loss_scale: 65536.0000 (65316.7729)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [17]  [2550/2809]  eta: 0:01:35  lr: 0.000033  min_lr: 0.000000  loss: 3.8384 (3.8943)  loss_scale: 65536.0000 (65317.6323)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [17]  [2560/2809]  eta: 0:01:32  lr: 0.000033  min_lr: 0.000000  loss: 3.6523 (3.8939)  loss_scale: 65536.0000 (65318.4850)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [17]  [2570/2809]  eta: 0:01:28  lr: 0.000033  min_lr: 0.000000  loss: 3.6523 (3.8935)  loss_scale: 65536.0000 (65319.3310)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0001  max mem: 15572
Epoch: [17]  [2580/2809]  eta: 0:01:24  lr: 0.000033  min_lr: 0.000000  loss: 3.7682 (3.8942)  loss_scale: 65536.0000 (65320.1705)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [17]  [2590/2809]  eta: 0:01:21  lr: 0.000033  min_lr: 0.000000  loss: 4.0413 (3.8950)  loss_scale: 65536.0000 (65321.0035)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [17]  [2600/2809]  eta: 0:01:17  lr: 0.000033  min_lr: 0.000000  loss: 3.9446 (3.8945)  loss_scale: 65536.0000 (65321.8301)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [17]  [2610/2809]  eta: 0:01:13  lr: 0.000033  min_lr: 0.000000  loss: 3.8585 (3.8937)  loss_scale: 65536.0000 (65322.6503)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [17]  [2620/2809]  eta: 0:01:10  lr: 0.000033  min_lr: 0.000000  loss: 3.8509 (3.8935)  loss_scale: 65536.0000 (65323.4643)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0001  max mem: 15572
Epoch: [17]  [2630/2809]  eta: 0:01:06  lr: 0.000033  min_lr: 0.000000  loss: 3.8509 (3.8937)  loss_scale: 65536.0000 (65324.2721)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [17]  [2640/2809]  eta: 0:01:02  lr: 0.000033  min_lr: 0.000000  loss: 3.9631 (3.8939)  loss_scale: 65536.0000 (65325.0738)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [17]  [2650/2809]  eta: 0:00:58  lr: 0.000033  min_lr: 0.000000  loss: 4.1035 (3.8951)  loss_scale: 65536.0000 (65325.8695)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [17]  [2660/2809]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000000  loss: 4.1162 (3.8954)  loss_scale: 65536.0000 (65326.6592)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
[2025-01-13 04:29:31,720] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:29:31,720] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:29:32,848] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50421
[2025-01-13 04:29:32,848] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:29:32,848] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2670/2809]  eta: 0:00:51  lr: 0.000033  min_lr: 0.000000  loss: 4.0372 (3.8959)  loss_scale: 65536.0000 (65401.0513)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [17]  [2680/2809]  eta: 0:00:47  lr: 0.000033  min_lr: 0.000000  loss: 3.9160 (3.8958)  loss_scale: 65536.0000 (65401.5546)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [17]  [2690/2809]  eta: 0:00:44  lr: 0.000033  min_lr: 0.000000  loss: 3.9030 (3.8962)  loss_scale: 65536.0000 (65402.0543)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [17]  [2700/2809]  eta: 0:00:40  lr: 0.000033  min_lr: 0.000000  loss: 3.9030 (3.8958)  loss_scale: 65536.0000 (65402.5502)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [17]  [2710/2809]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000000  loss: 3.7969 (3.8958)  loss_scale: 65536.0000 (65403.0424)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [17]  [2720/2809]  eta: 0:00:32  lr: 0.000033  min_lr: 0.000000  loss: 4.0236 (3.8967)  loss_scale: 65536.0000 (65403.5311)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [17]  [2730/2809]  eta: 0:00:29  lr: 0.000033  min_lr: 0.000000  loss: 3.9880 (3.8964)  loss_scale: 65536.0000 (65404.0161)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [17]  [2740/2809]  eta: 0:00:25  lr: 0.000033  min_lr: 0.000000  loss: 3.8830 (3.8965)  loss_scale: 65536.0000 (65404.4976)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0003  max mem: 15572
Epoch: [17]  [2750/2809]  eta: 0:00:21  lr: 0.000033  min_lr: 0.000000  loss: 3.9858 (3.8965)  loss_scale: 65536.0000 (65404.9756)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [17]  [2760/2809]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000000  loss: 4.0073 (3.8962)  loss_scale: 65536.0000 (65405.4502)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [17]  [2770/2809]  eta: 0:00:14  lr: 0.000033  min_lr: 0.000000  loss: 3.5924 (3.8954)  loss_scale: 65536.0000 (65405.9213)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [17]  [2780/2809]  eta: 0:00:10  lr: 0.000033  min_lr: 0.000000  loss: 3.6998 (3.8956)  loss_scale: 65536.0000 (65406.3891)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [17]  [2790/2809]  eta: 0:00:07  lr: 0.000033  min_lr: 0.000000  loss: 3.8566 (3.8959)  loss_scale: 65536.0000 (65406.8535)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0003  max mem: 15572
[2025-01-13 04:30:20,656] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:30:20,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:30:21,372] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50552
[2025-01-13 04:30:21,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:30:21,372] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2800/2809]  eta: 0:00:03  lr: 0.000033  min_lr: 0.000000  loss: 3.9018 (3.8957)  loss_scale: 65536.0000 (65454.1092)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [17]  [2808/2809]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000000  loss: 3.9247 (3.8960)  loss_scale: 65536.0000 (65454.3425)  weight_decay: 0.0500 (0.0500)  time: 0.3603  data: 0.0001  max mem: 15572
Epoch: [17] Total time: 0:17:21 (0.3707 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000000  loss: 3.9247 (3.8960)  loss_scale: 65536.0000 (65454.3425)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:08:04  loss: 0.4301 (0.4301)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.7818  data: 1.6094  max mem: 15572
Val:  [ 10/272]  eta: 0:01:46  loss: 2.6609 (2.5825)  acc1: 38.8889 (37.8788)  acc5: 66.6667 (66.6667)  time: 0.4048  data: 0.2448  max mem: 15572
Val:  [ 20/272]  eta: 0:01:12  loss: 2.6714 (2.6165)  acc1: 38.8889 (39.6825)  acc5: 66.6667 (69.5767)  time: 0.2120  data: 0.0543  max mem: 15572
Val:  [ 30/272]  eta: 0:00:58  loss: 2.8482 (2.7316)  acc1: 38.8889 (34.7670)  acc5: 66.6667 (68.1004)  time: 0.1551  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:51  loss: 2.6813 (2.7283)  acc1: 27.7778 (33.3333)  acc5: 66.6667 (68.8347)  time: 0.1550  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:46  loss: 2.6693 (2.6490)  acc1: 27.7778 (35.6209)  acc5: 72.2222 (70.4793)  time: 0.1558  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:42  loss: 1.7000 (2.5222)  acc1: 55.5556 (39.4353)  acc5: 77.7778 (71.9490)  time: 0.1529  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:39  loss: 1.7094 (2.4441)  acc1: 61.1111 (42.3318)  acc5: 83.3333 (73.0829)  time: 0.1590  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:36  loss: 2.1260 (2.4445)  acc1: 55.5556 (42.2497)  acc5: 77.7778 (72.8395)  time: 0.1647  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.3491 (2.4555)  acc1: 44.4444 (42.7350)  acc5: 77.7778 (73.5043)  time: 0.1595  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:32  loss: 2.3974 (2.4923)  acc1: 50.0000 (42.0792)  acc5: 77.7778 (72.9373)  time: 0.1718  data: 0.0100  max mem: 15572
Val:  [110/272]  eta: 0:00:29  loss: 2.8766 (2.5784)  acc1: 16.6667 (39.6897)  acc5: 55.5556 (71.0711)  time: 0.1702  data: 0.0099  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 3.2390 (2.6256)  acc1: 11.1111 (38.4757)  acc5: 55.5556 (70.1561)  time: 0.1562  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.6629 (2.5916)  acc1: 38.8889 (39.6947)  acc5: 72.2222 (70.5259)  time: 0.1647  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.7774 (2.5821)  acc1: 55.5556 (40.2679)  acc5: 72.2222 (70.4492)  time: 0.1611  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.5998 (2.5827)  acc1: 33.3333 (39.8087)  acc5: 72.2222 (70.8241)  time: 0.1541  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.4854 (2.5577)  acc1: 50.0000 (40.9938)  acc5: 77.7778 (71.4631)  time: 0.1539  data: 0.0003  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.4854 (2.5808)  acc1: 44.4444 (40.1235)  acc5: 72.2222 (70.9877)  time: 0.1573  data: 0.0003  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 2.5966 (2.5713)  acc1: 33.3333 (39.8404)  acc5: 66.6667 (71.4549)  time: 0.1666  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.6554 (2.6142)  acc1: 27.7778 (38.5398)  acc5: 72.2222 (70.3898)  time: 0.1709  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.7334 (2.6246)  acc1: 22.2222 (38.0873)  acc5: 61.1111 (70.1493)  time: 0.1640  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.4249 (2.6292)  acc1: 33.3333 (38.2570)  acc5: 77.7778 (70.1159)  time: 0.1565  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.4720 (2.6208)  acc1: 50.0000 (38.4615)  acc5: 72.2222 (70.2866)  time: 0.1605  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.9729 (2.5860)  acc1: 55.5556 (39.6104)  acc5: 72.2222 (70.6830)  time: 0.1692  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.8369 (2.5668)  acc1: 55.5556 (39.8801)  acc5: 77.7778 (71.0696)  time: 0.1677  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.5138 (2.5806)  acc1: 33.3333 (39.1988)  acc5: 72.2222 (70.9163)  time: 0.1590  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.3794 (2.5115)  acc1: 72.2222 (41.1452)  acc5: 88.8889 (71.7752)  time: 0.1476  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.5532 (2.5151)  acc1: 61.1111 (40.9594)  acc5: 83.3333 (71.7302)  time: 0.1356  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.6041 (2.5202)  acc1: 55.5556 (40.9175)  acc5: 83.3333 (71.7182)  time: 0.1303  data: 0.0001  max mem: 15572
Val: Total time: 0:00:46 (0.1691 s / it)
* Acc@1 40.917 Acc@5 71.718 loss 2.520
Accuracy of the network on the 4883 val videos: 40.9%
[2025-01-13 04:31:10,735] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 04:31:10,736] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 04:31:10,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 04:31:13,159] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 04:31:13,160] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 40.92%
Epoch: [18]  [   0/2809]  eta: 3:00:14  lr: 0.000033  min_lr: 0.000000  loss: 4.2060 (4.2060)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.8500  data: 3.4608  max mem: 15572
Epoch: [18]  [  10/2809]  eta: 0:32:12  lr: 0.000033  min_lr: 0.000000  loss: 4.0272 (4.0285)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6904  data: 0.3149  max mem: 15572
Epoch: [18]  [  20/2809]  eta: 0:25:06  lr: 0.000033  min_lr: 0.000000  loss: 4.0053 (4.0072)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0003  max mem: 15572
Epoch: [18]  [  30/2809]  eta: 0:22:25  lr: 0.000033  min_lr: 0.000000  loss: 4.0093 (3.9922)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [18]  [  40/2809]  eta: 0:21:02  lr: 0.000033  min_lr: 0.000000  loss: 3.7305 (3.8652)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [18]  [  50/2809]  eta: 0:20:12  lr: 0.000033  min_lr: 0.000000  loss: 3.5897 (3.8414)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [18]  [  60/2809]  eta: 0:19:33  lr: 0.000033  min_lr: 0.000000  loss: 3.8514 (3.8317)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [18]  [  70/2809]  eta: 0:19:04  lr: 0.000033  min_lr: 0.000000  loss: 3.5473 (3.7907)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3635  data: 0.0002  max mem: 15572
Epoch: [18]  [  80/2809]  eta: 0:18:44  lr: 0.000033  min_lr: 0.000000  loss: 3.6730 (3.8088)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [18]  [  90/2809]  eta: 0:18:27  lr: 0.000033  min_lr: 0.000000  loss: 3.7638 (3.8120)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [18]  [ 100/2809]  eta: 0:18:12  lr: 0.000033  min_lr: 0.000000  loss: 3.7839 (3.8056)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [18]  [ 110/2809]  eta: 0:18:00  lr: 0.000033  min_lr: 0.000000  loss: 3.8734 (3.8197)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
[2025-01-13 04:32:00,915] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:32:00,915] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:32:01,303] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50682
[2025-01-13 04:32:01,303] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:32:01,303] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 120/2809]  eta: 0:17:49  lr: 0.000033  min_lr: 0.000000  loss: 4.0445 (3.8314)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [18]  [ 130/2809]  eta: 0:17:38  lr: 0.000033  min_lr: 0.000000  loss: 4.1295 (3.8562)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [18]  [ 140/2809]  eta: 0:17:30  lr: 0.000033  min_lr: 0.000000  loss: 4.0587 (3.8538)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [18]  [ 150/2809]  eta: 0:17:21  lr: 0.000033  min_lr: 0.000000  loss: 3.8396 (3.8561)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [18]  [ 160/2809]  eta: 0:17:13  lr: 0.000033  min_lr: 0.000000  loss: 4.0274 (3.8626)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [18]  [ 170/2809]  eta: 0:17:06  lr: 0.000033  min_lr: 0.000000  loss: 4.1484 (3.8830)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [18]  [ 180/2809]  eta: 0:16:59  lr: 0.000033  min_lr: 0.000000  loss: 4.1073 (3.8835)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0001  max mem: 15572
Epoch: [18]  [ 190/2809]  eta: 0:16:52  lr: 0.000033  min_lr: 0.000000  loss: 3.6760 (3.8715)  loss_scale: 65536.0000 (65879.1204)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [18]  [ 200/2809]  eta: 0:16:46  lr: 0.000033  min_lr: 0.000000  loss: 3.4218 (3.8638)  loss_scale: 65536.0000 (65862.0498)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [18]  [ 210/2809]  eta: 0:16:40  lr: 0.000033  min_lr: 0.000000  loss: 3.7318 (3.8650)  loss_scale: 65536.0000 (65846.5972)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [18]  [ 220/2809]  eta: 0:16:34  lr: 0.000033  min_lr: 0.000000  loss: 3.7248 (3.8525)  loss_scale: 65536.0000 (65832.5430)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [18]  [ 230/2809]  eta: 0:16:29  lr: 0.000033  min_lr: 0.000000  loss: 3.5958 (3.8453)  loss_scale: 65536.0000 (65819.7056)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [18]  [ 240/2809]  eta: 0:16:24  lr: 0.000032  min_lr: 0.000000  loss: 3.5958 (3.8359)  loss_scale: 65536.0000 (65807.9336)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0015  max mem: 15572
[2025-01-13 04:32:48,867] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:32:48,867] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [18]  [ 250/2809]  eta: 0:16:19  lr: 0.000032  min_lr: 0.000000  loss: 3.8924 (3.8442)  loss_scale: 65536.0000 (66319.2988)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0014  max mem: 15572
[2025-01-13 04:32:50,696] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50816
[2025-01-13 04:32:50,696] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:32:50,697] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 260/2809]  eta: 0:16:14  lr: 0.000032  min_lr: 0.000000  loss: 3.8924 (3.8453)  loss_scale: 65536.0000 (67042.5747)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [18]  [ 270/2809]  eta: 0:16:08  lr: 0.000032  min_lr: 0.000000  loss: 3.9199 (3.8493)  loss_scale: 65536.0000 (66986.9815)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [18]  [ 280/2809]  eta: 0:16:03  lr: 0.000032  min_lr: 0.000000  loss: 3.9199 (3.8507)  loss_scale: 65536.0000 (66935.3452)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [18]  [ 290/2809]  eta: 0:15:58  lr: 0.000032  min_lr: 0.000000  loss: 3.9073 (3.8548)  loss_scale: 65536.0000 (66887.2577)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [18]  [ 300/2809]  eta: 0:15:53  lr: 0.000032  min_lr: 0.000000  loss: 4.0582 (3.8598)  loss_scale: 65536.0000 (66842.3654)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [18]  [ 310/2809]  eta: 0:15:49  lr: 0.000032  min_lr: 0.000000  loss: 4.0582 (3.8649)  loss_scale: 65536.0000 (66800.3601)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [18]  [ 320/2809]  eta: 0:15:44  lr: 0.000032  min_lr: 0.000000  loss: 3.9650 (3.8703)  loss_scale: 65536.0000 (66760.9720)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [18]  [ 330/2809]  eta: 0:15:40  lr: 0.000032  min_lr: 0.000000  loss: 3.9845 (3.8738)  loss_scale: 65536.0000 (66723.9637)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [18]  [ 340/2809]  eta: 0:15:35  lr: 0.000032  min_lr: 0.000000  loss: 3.9623 (3.8748)  loss_scale: 65536.0000 (66689.1261)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [18]  [ 350/2809]  eta: 0:15:31  lr: 0.000032  min_lr: 0.000000  loss: 3.8669 (3.8734)  loss_scale: 65536.0000 (66656.2735)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [18]  [ 360/2809]  eta: 0:15:26  lr: 0.000032  min_lr: 0.000000  loss: 3.7219 (3.8741)  loss_scale: 65536.0000 (66625.2410)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [18]  [ 370/2809]  eta: 0:15:22  lr: 0.000032  min_lr: 0.000000  loss: 3.7781 (3.8729)  loss_scale: 65536.0000 (66595.8814)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [18]  [ 380/2809]  eta: 0:15:17  lr: 0.000032  min_lr: 0.000000  loss: 3.8335 (3.8707)  loss_scale: 65536.0000 (66568.0630)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0003  max mem: 15572
[2025-01-13 04:33:38,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:33:38,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:33:39,007] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50947
[2025-01-13 04:33:39,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:33:39,009] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 390/2809]  eta: 0:15:13  lr: 0.000032  min_lr: 0.000000  loss: 3.8509 (3.8687)  loss_scale: 65536.0000 (66876.8900)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [18]  [ 400/2809]  eta: 0:15:08  lr: 0.000032  min_lr: 0.000000  loss: 3.8852 (3.8723)  loss_scale: 65536.0000 (66843.4514)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [18]  [ 410/2809]  eta: 0:15:04  lr: 0.000032  min_lr: 0.000000  loss: 4.0400 (3.8760)  loss_scale: 65536.0000 (66811.6399)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [18]  [ 420/2809]  eta: 0:15:00  lr: 0.000032  min_lr: 0.000000  loss: 3.8643 (3.8723)  loss_scale: 65536.0000 (66781.3397)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [18]  [ 430/2809]  eta: 0:14:55  lr: 0.000032  min_lr: 0.000000  loss: 3.7958 (3.8689)  loss_scale: 65536.0000 (66752.4455)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
[2025-01-13 04:33:58,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=51000, skipped=345, lr=[3.1354948628075986e-07, 3.1354948628075986e-07, 4.4792783754394275e-07, 4.4792783754394275e-07, 6.398969107770612e-07, 6.398969107770612e-07, 9.141384439672303e-07, 9.141384439672303e-07, 1.3059120628103289e-06, 1.3059120628103289e-06, 1.865588661157613e-06, 1.865588661157613e-06, 2.66512665879659e-06, 2.66512665879659e-06, 3.807323798280843e-06, 3.807323798280843e-06, 5.4390339975440615e-06, 5.4390339975440615e-06, 7.770048567920089e-06, 7.770048567920089e-06, 1.1100069382742983e-05, 1.1100069382742983e-05, 1.5857241975347122e-05, 1.5857241975347122e-05, 2.2653202821924462e-05, 2.2653202821924462e-05, 3.236171831703495e-05, 3.236171831703495e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 04:33:58,087] [INFO] [timer.py:260:stop] epoch=0/micro_step=51000/global_step=51000, RunningAvgSamplesPerSec=29.15521517683427, CurrSamplesPerSec=34.74450063577662, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [18]  [ 440/2809]  eta: 0:14:51  lr: 0.000032  min_lr: 0.000000  loss: 3.7958 (3.8679)  loss_scale: 65536.0000 (66724.8617)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [18]  [ 450/2809]  eta: 0:14:47  lr: 0.000032  min_lr: 0.000000  loss: 3.8039 (3.8650)  loss_scale: 65536.0000 (66698.5011)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [18]  [ 460/2809]  eta: 0:14:43  lr: 0.000032  min_lr: 0.000000  loss: 3.8640 (3.8686)  loss_scale: 65536.0000 (66673.2842)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [18]  [ 470/2809]  eta: 0:14:39  lr: 0.000032  min_lr: 0.000000  loss: 4.0934 (3.8726)  loss_scale: 65536.0000 (66649.1380)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [18]  [ 480/2809]  eta: 0:14:35  lr: 0.000032  min_lr: 0.000000  loss: 4.0216 (3.8735)  loss_scale: 65536.0000 (66625.9958)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [18]  [ 490/2809]  eta: 0:14:31  lr: 0.000032  min_lr: 0.000000  loss: 4.1466 (3.8755)  loss_scale: 65536.0000 (66603.7963)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0001  max mem: 15572
Epoch: [18]  [ 500/2809]  eta: 0:14:26  lr: 0.000032  min_lr: 0.000000  loss: 4.0315 (3.8758)  loss_scale: 65536.0000 (66582.4830)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [18]  [ 510/2809]  eta: 0:14:22  lr: 0.000032  min_lr: 0.000000  loss: 3.8819 (3.8711)  loss_scale: 65536.0000 (66562.0039)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0001  max mem: 15572
[2025-01-13 04:34:26,461] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:34:26,461] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:34:26,828] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51077
[2025-01-13 04:34:26,828] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:34:26,828] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 520/2809]  eta: 0:14:18  lr: 0.000032  min_lr: 0.000000  loss: 3.8876 (3.8720)  loss_scale: 65536.0000 (66668.0998)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [18]  [ 530/2809]  eta: 0:14:14  lr: 0.000032  min_lr: 0.000000  loss: 4.0229 (3.8729)  loss_scale: 65536.0000 (66646.7797)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [18]  [ 540/2809]  eta: 0:14:10  lr: 0.000032  min_lr: 0.000000  loss: 3.9483 (3.8743)  loss_scale: 65536.0000 (66626.2477)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [18]  [ 550/2809]  eta: 0:14:06  lr: 0.000032  min_lr: 0.000000  loss: 4.1131 (3.8767)  loss_scale: 65536.0000 (66606.4610)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [18]  [ 560/2809]  eta: 0:14:02  lr: 0.000032  min_lr: 0.000000  loss: 3.8347 (3.8753)  loss_scale: 65536.0000 (66587.3797)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [18]  [ 570/2809]  eta: 0:13:58  lr: 0.000032  min_lr: 0.000000  loss: 3.6884 (3.8740)  loss_scale: 65536.0000 (66568.9667)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [18]  [ 580/2809]  eta: 0:13:54  lr: 0.000032  min_lr: 0.000000  loss: 3.7962 (3.8738)  loss_scale: 65536.0000 (66551.1876)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [18]  [ 590/2809]  eta: 0:13:50  lr: 0.000032  min_lr: 0.000000  loss: 3.7332 (3.8693)  loss_scale: 65536.0000 (66534.0102)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [18]  [ 600/2809]  eta: 0:13:46  lr: 0.000032  min_lr: 0.000000  loss: 3.9682 (3.8748)  loss_scale: 65536.0000 (66517.4043)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [18]  [ 610/2809]  eta: 0:13:42  lr: 0.000032  min_lr: 0.000000  loss: 3.9960 (3.8748)  loss_scale: 65536.0000 (66501.3421)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [18]  [ 620/2809]  eta: 0:13:38  lr: 0.000032  min_lr: 0.000000  loss: 3.9300 (3.8732)  loss_scale: 65536.0000 (66485.7971)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [18]  [ 630/2809]  eta: 0:13:34  lr: 0.000032  min_lr: 0.000000  loss: 3.9247 (3.8722)  loss_scale: 65536.0000 (66470.7448)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [18]  [ 640/2809]  eta: 0:13:30  lr: 0.000032  min_lr: 0.000000  loss: 3.7830 (3.8711)  loss_scale: 65536.0000 (66456.1622)  weight_decay: 0.0500 (0.0500)  time: 0.3643  data: 0.0002  max mem: 15572
[2025-01-13 04:35:14,308] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:35:14,309] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:35:14,716] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51207
[2025-01-13 04:35:14,716] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:35:14,716] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 650/2809]  eta: 0:13:26  lr: 0.000032  min_lr: 0.000000  loss: 3.9361 (3.8713)  loss_scale: 65536.0000 (66542.6974)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [18]  [ 660/2809]  eta: 0:13:22  lr: 0.000032  min_lr: 0.000000  loss: 3.8507 (3.8701)  loss_scale: 65536.0000 (66527.4675)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [18]  [ 670/2809]  eta: 0:13:19  lr: 0.000032  min_lr: 0.000000  loss: 3.7249 (3.8692)  loss_scale: 65536.0000 (66512.6915)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [18]  [ 680/2809]  eta: 0:13:15  lr: 0.000032  min_lr: 0.000000  loss: 3.8708 (3.8673)  loss_scale: 65536.0000 (66498.3495)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [18]  [ 690/2809]  eta: 0:13:11  lr: 0.000032  min_lr: 0.000000  loss: 3.7853 (3.8629)  loss_scale: 65536.0000 (66484.4226)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [18]  [ 700/2809]  eta: 0:13:07  lr: 0.000032  min_lr: 0.000000  loss: 3.7465 (3.8614)  loss_scale: 65536.0000 (66470.8930)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [18]  [ 710/2809]  eta: 0:13:03  lr: 0.000032  min_lr: 0.000000  loss: 3.8909 (3.8641)  loss_scale: 65536.0000 (66457.7440)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [18]  [ 720/2809]  eta: 0:12:59  lr: 0.000032  min_lr: 0.000000  loss: 3.7849 (3.8618)  loss_scale: 65536.0000 (66444.9598)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [18]  [ 730/2809]  eta: 0:12:56  lr: 0.000032  min_lr: 0.000000  loss: 3.7173 (3.8634)  loss_scale: 65536.0000 (66432.5253)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0001  max mem: 15572
Epoch: [18]  [ 740/2809]  eta: 0:12:52  lr: 0.000032  min_lr: 0.000000  loss: 4.0421 (3.8634)  loss_scale: 65536.0000 (66420.4265)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [18]  [ 750/2809]  eta: 0:12:48  lr: 0.000032  min_lr: 0.000000  loss: 4.0421 (3.8639)  loss_scale: 65536.0000 (66408.6498)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [18]  [ 760/2809]  eta: 0:12:44  lr: 0.000032  min_lr: 0.000000  loss: 3.4224 (3.8581)  loss_scale: 65536.0000 (66397.1827)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
Epoch: [18]  [ 770/2809]  eta: 0:12:40  lr: 0.000032  min_lr: 0.000000  loss: 3.7536 (3.8597)  loss_scale: 65536.0000 (66386.0130)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
[2025-01-13 04:36:02,435] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:36:02,435] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:36:02,798] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51337
[2025-01-13 04:36:02,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:36:02,798] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 780/2809]  eta: 0:12:36  lr: 0.000032  min_lr: 0.000000  loss: 3.8376 (3.8588)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [18]  [ 790/2809]  eta: 0:12:33  lr: 0.000032  min_lr: 0.000000  loss: 3.8406 (3.8576)  loss_scale: 65536.0000 (66447.3729)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [18]  [ 800/2809]  eta: 0:12:29  lr: 0.000032  min_lr: 0.000000  loss: 3.9000 (3.8584)  loss_scale: 65536.0000 (66435.9950)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [18]  [ 810/2809]  eta: 0:12:25  lr: 0.000032  min_lr: 0.000000  loss: 3.9624 (3.8603)  loss_scale: 65536.0000 (66424.8977)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [18]  [ 820/2809]  eta: 0:12:21  lr: 0.000032  min_lr: 0.000000  loss: 3.8840 (3.8570)  loss_scale: 65536.0000 (66414.0706)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [18]  [ 830/2809]  eta: 0:12:17  lr: 0.000032  min_lr: 0.000000  loss: 3.6183 (3.8563)  loss_scale: 65536.0000 (66403.5042)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [18]  [ 840/2809]  eta: 0:12:14  lr: 0.000032  min_lr: 0.000000  loss: 3.8237 (3.8560)  loss_scale: 65536.0000 (66393.1891)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [18]  [ 850/2809]  eta: 0:12:10  lr: 0.000032  min_lr: 0.000000  loss: 3.8237 (3.8561)  loss_scale: 65536.0000 (66383.1163)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [18]  [ 860/2809]  eta: 0:12:06  lr: 0.000032  min_lr: 0.000000  loss: 3.9981 (3.8582)  loss_scale: 65536.0000 (66373.2776)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [18]  [ 870/2809]  eta: 0:12:02  lr: 0.000032  min_lr: 0.000000  loss: 3.9981 (3.8599)  loss_scale: 65536.0000 (66363.6648)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [18]  [ 880/2809]  eta: 0:11:58  lr: 0.000032  min_lr: 0.000000  loss: 4.0791 (3.8621)  loss_scale: 65536.0000 (66354.2701)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [18]  [ 890/2809]  eta: 0:11:55  lr: 0.000032  min_lr: 0.000000  loss: 4.0501 (3.8636)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [18]  [ 900/2809]  eta: 0:11:51  lr: 0.000032  min_lr: 0.000000  loss: 3.8251 (3.8630)  loss_scale: 65536.0000 (66336.1065)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
[2025-01-13 04:36:50,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:36:50,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:36:52,690] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51472
[2025-01-13 04:36:52,690] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:36:52,690] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 910/2809]  eta: 0:11:47  lr: 0.000032  min_lr: 0.000000  loss: 3.7688 (3.8637)  loss_scale: 65536.0000 (66758.9550)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [18]  [ 920/2809]  eta: 0:11:43  lr: 0.000032  min_lr: 0.000000  loss: 3.7688 (3.8595)  loss_scale: 65536.0000 (66745.6764)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [18]  [ 930/2809]  eta: 0:11:40  lr: 0.000032  min_lr: 0.000000  loss: 3.6964 (3.8593)  loss_scale: 65536.0000 (66732.6831)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [18]  [ 940/2809]  eta: 0:11:36  lr: 0.000032  min_lr: 0.000000  loss: 4.0368 (3.8608)  loss_scale: 65536.0000 (66719.9660)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [18]  [ 950/2809]  eta: 0:11:32  lr: 0.000032  min_lr: 0.000000  loss: 4.0100 (3.8599)  loss_scale: 65536.0000 (66707.5163)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [18]  [ 960/2809]  eta: 0:11:28  lr: 0.000032  min_lr: 0.000000  loss: 3.7390 (3.8588)  loss_scale: 65536.0000 (66695.3257)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [18]  [ 970/2809]  eta: 0:11:25  lr: 0.000032  min_lr: 0.000000  loss: 3.7390 (3.8593)  loss_scale: 65536.0000 (66683.3862)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [18]  [ 980/2809]  eta: 0:11:21  lr: 0.000032  min_lr: 0.000000  loss: 3.9180 (3.8594)  loss_scale: 65536.0000 (66671.6901)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [18]  [ 990/2809]  eta: 0:11:17  lr: 0.000032  min_lr: 0.000000  loss: 3.9180 (3.8591)  loss_scale: 65536.0000 (66660.2301)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [18]  [1000/2809]  eta: 0:11:13  lr: 0.000032  min_lr: 0.000000  loss: 3.8552 (3.8608)  loss_scale: 65536.0000 (66648.9990)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [18]  [1010/2809]  eta: 0:11:10  lr: 0.000032  min_lr: 0.000000  loss: 3.7225 (3.8584)  loss_scale: 65536.0000 (66637.9901)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [18]  [1020/2809]  eta: 0:11:06  lr: 0.000032  min_lr: 0.000000  loss: 3.7225 (3.8585)  loss_scale: 65536.0000 (66627.1969)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0001  max mem: 15572
Epoch: [18]  [1030/2809]  eta: 0:11:02  lr: 0.000032  min_lr: 0.000000  loss: 3.8658 (3.8596)  loss_scale: 65536.0000 (66616.6130)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0001  max mem: 15572
[2025-01-13 04:37:40,550] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:37:40,550] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:37:40,913] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51602
[2025-01-13 04:37:40,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:37:40,913] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [1040/2809]  eta: 0:10:58  lr: 0.000032  min_lr: 0.000000  loss: 4.0451 (3.8606)  loss_scale: 65536.0000 (66669.1873)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0001  max mem: 15572
Epoch: [18]  [1050/2809]  eta: 0:10:54  lr: 0.000032  min_lr: 0.000000  loss: 4.0017 (3.8617)  loss_scale: 65536.0000 (66658.4053)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [18]  [1060/2809]  eta: 0:10:51  lr: 0.000032  min_lr: 0.000000  loss: 3.8994 (3.8617)  loss_scale: 65536.0000 (66647.8266)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [18]  [1070/2809]  eta: 0:10:47  lr: 0.000032  min_lr: 0.000000  loss: 3.8090 (3.8602)  loss_scale: 65536.0000 (66637.4454)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [18]  [1080/2809]  eta: 0:10:43  lr: 0.000032  min_lr: 0.000000  loss: 3.9138 (3.8616)  loss_scale: 65536.0000 (66627.2562)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [18]  [1090/2809]  eta: 0:10:39  lr: 0.000032  min_lr: 0.000000  loss: 3.8806 (3.8601)  loss_scale: 65536.0000 (66617.2539)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [18]  [1100/2809]  eta: 0:10:36  lr: 0.000032  min_lr: 0.000000  loss: 3.8405 (3.8605)  loss_scale: 65536.0000 (66607.4332)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [18]  [1110/2809]  eta: 0:10:32  lr: 0.000032  min_lr: 0.000000  loss: 3.9068 (3.8616)  loss_scale: 65536.0000 (66597.7894)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0003  max mem: 15572
Epoch: [18]  [1120/2809]  eta: 0:10:28  lr: 0.000032  min_lr: 0.000000  loss: 4.0888 (3.8614)  loss_scale: 65536.0000 (66588.3176)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [18]  [1130/2809]  eta: 0:10:24  lr: 0.000032  min_lr: 0.000000  loss: 3.8761 (3.8619)  loss_scale: 65536.0000 (66579.0133)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [18]  [1140/2809]  eta: 0:10:21  lr: 0.000032  min_lr: 0.000000  loss: 3.7813 (3.8592)  loss_scale: 65536.0000 (66569.8720)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [18]  [1150/2809]  eta: 0:10:17  lr: 0.000032  min_lr: 0.000000  loss: 3.6578 (3.8576)  loss_scale: 65536.0000 (66560.8897)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [18]  [1160/2809]  eta: 0:10:13  lr: 0.000032  min_lr: 0.000000  loss: 3.7787 (3.8579)  loss_scale: 65536.0000 (66552.0620)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
[2025-01-13 04:38:28,628] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:38:28,628] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:38:28,992] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51732
[2025-01-13 04:38:28,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:38:28,992] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [1170/2809]  eta: 0:10:09  lr: 0.000032  min_lr: 0.000000  loss: 3.8805 (3.8581)  loss_scale: 65536.0000 (66599.3510)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [18]  [1180/2809]  eta: 0:10:05  lr: 0.000032  min_lr: 0.000000  loss: 3.9474 (3.8586)  loss_scale: 65536.0000 (66590.3472)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [18]  [1190/2809]  eta: 0:10:02  lr: 0.000032  min_lr: 0.000000  loss: 3.9474 (3.8601)  loss_scale: 65536.0000 (66581.4945)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [18]  [1200/2809]  eta: 0:09:58  lr: 0.000032  min_lr: 0.000000  loss: 3.8741 (3.8586)  loss_scale: 65536.0000 (66572.7893)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [18]  [1210/2809]  eta: 0:09:54  lr: 0.000032  min_lr: 0.000000  loss: 3.5711 (3.8567)  loss_scale: 65536.0000 (66564.2279)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [18]  [1220/2809]  eta: 0:09:51  lr: 0.000032  min_lr: 0.000000  loss: 3.8144 (3.8569)  loss_scale: 65536.0000 (66555.8067)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [18]  [1230/2809]  eta: 0:09:47  lr: 0.000032  min_lr: 0.000000  loss: 3.8144 (3.8563)  loss_scale: 65536.0000 (66547.5223)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [18]  [1240/2809]  eta: 0:09:43  lr: 0.000032  min_lr: 0.000000  loss: 3.8123 (3.8555)  loss_scale: 65536.0000 (66539.3715)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [18]  [1250/2809]  eta: 0:09:39  lr: 0.000032  min_lr: 0.000000  loss: 3.8450 (3.8555)  loss_scale: 65536.0000 (66531.3509)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [18]  [1260/2809]  eta: 0:09:36  lr: 0.000032  min_lr: 0.000000  loss: 3.8464 (3.8558)  loss_scale: 65536.0000 (66523.4576)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [18]  [1270/2809]  eta: 0:09:32  lr: 0.000032  min_lr: 0.000000  loss: 3.8426 (3.8554)  loss_scale: 65536.0000 (66515.6884)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [18]  [1280/2809]  eta: 0:09:28  lr: 0.000032  min_lr: 0.000000  loss: 3.8642 (3.8560)  loss_scale: 65536.0000 (66508.0406)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [18]  [1290/2809]  eta: 0:09:24  lr: 0.000032  min_lr: 0.000000  loss: 3.8642 (3.8559)  loss_scale: 65536.0000 (66500.5112)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
[2025-01-13 04:39:16,569] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:39:16,569] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [18]  [1300/2809]  eta: 0:09:20  lr: 0.000032  min_lr: 0.000000  loss: 3.8255 (3.8556)  loss_scale: 65536.0000 (66593.8447)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 04:39:18,033] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51865
[2025-01-13 04:39:18,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:39:18,033] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [1310/2809]  eta: 0:09:17  lr: 0.000032  min_lr: 0.000000  loss: 3.8245 (3.8551)  loss_scale: 65536.0000 (66685.7544)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [18]  [1320/2809]  eta: 0:09:13  lr: 0.000032  min_lr: 0.000000  loss: 3.6773 (3.8544)  loss_scale: 65536.0000 (66677.0507)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [18]  [1330/2809]  eta: 0:09:09  lr: 0.000032  min_lr: 0.000000  loss: 4.0160 (3.8561)  loss_scale: 65536.0000 (66668.4778)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [18]  [1340/2809]  eta: 0:09:05  lr: 0.000032  min_lr: 0.000000  loss: 4.0617 (3.8565)  loss_scale: 65536.0000 (66660.0328)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0003  max mem: 15572
Epoch: [18]  [1350/2809]  eta: 0:09:02  lr: 0.000032  min_lr: 0.000000  loss: 3.9359 (3.8568)  loss_scale: 65536.0000 (66651.7128)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [18]  [1360/2809]  eta: 0:08:58  lr: 0.000032  min_lr: 0.000000  loss: 3.9409 (3.8573)  loss_scale: 65536.0000 (66643.5151)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0001  max mem: 15572
Epoch: [18]  [1370/2809]  eta: 0:08:54  lr: 0.000032  min_lr: 0.000000  loss: 4.0186 (3.8590)  loss_scale: 65536.0000 (66635.4369)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [18]  [1380/2809]  eta: 0:08:50  lr: 0.000032  min_lr: 0.000000  loss: 3.9888 (3.8599)  loss_scale: 65536.0000 (66627.4757)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [18]  [1390/2809]  eta: 0:08:47  lr: 0.000032  min_lr: 0.000000  loss: 3.9280 (3.8595)  loss_scale: 65536.0000 (66619.6290)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
Epoch: [18]  [1400/2809]  eta: 0:08:43  lr: 0.000032  min_lr: 0.000000  loss: 3.9475 (3.8599)  loss_scale: 65536.0000 (66611.8944)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [18]  [1410/2809]  eta: 0:08:39  lr: 0.000032  min_lr: 0.000000  loss: 3.9507 (3.8607)  loss_scale: 65536.0000 (66604.2693)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [18]  [1420/2809]  eta: 0:08:35  lr: 0.000032  min_lr: 0.000000  loss: 3.9429 (3.8604)  loss_scale: 65536.0000 (66596.7516)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [18]  [1430/2809]  eta: 0:08:32  lr: 0.000032  min_lr: 0.000000  loss: 3.9031 (3.8601)  loss_scale: 65536.0000 (66589.3389)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
[2025-01-13 04:40:05,600] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:40:05,601] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:40:05,965] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51995
[2025-01-13 04:40:05,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:40:05,965] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-13 04:40:07,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=52000, skipped=353, lr=[3.068017621427496e-07, 3.068017621427496e-07, 4.382882316324995e-07, 4.382882316324995e-07, 6.26126045189285e-07, 6.26126045189285e-07, 8.944657788418358e-07, 8.944657788418358e-07, 1.277808255488337e-06, 1.277808255488337e-06, 1.8254403649833386e-06, 1.8254403649833386e-06, 2.607771949976198e-06, 2.607771949976198e-06, 3.7253884999659977e-06, 3.7253884999659977e-06, 5.321983571379997e-06, 5.321983571379997e-06, 7.602833673399996e-06, 7.602833673399996e-06, 1.0861190961999994e-05, 1.0861190961999994e-05, 1.5515987088571423e-05, 1.5515987088571423e-05, 2.2165695840816318e-05, 2.2165695840816318e-05, 3.166527977259474e-05, 3.166527977259474e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 04:40:07,424] [INFO] [timer.py:260:stop] epoch=0/micro_step=52000/global_step=52000, RunningAvgSamplesPerSec=29.240490987882737, CurrSamplesPerSec=35.004505300230484, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [18]  [1440/2809]  eta: 0:08:28  lr: 0.000032  min_lr: 0.000000  loss: 3.9616 (3.8611)  loss_scale: 65536.0000 (66627.5087)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [18]  [1450/2809]  eta: 0:08:24  lr: 0.000032  min_lr: 0.000000  loss: 3.9554 (3.8596)  loss_scale: 65536.0000 (66619.9862)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [18]  [1460/2809]  eta: 0:08:20  lr: 0.000032  min_lr: 0.000000  loss: 3.6579 (3.8588)  loss_scale: 65536.0000 (66612.5667)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [18]  [1470/2809]  eta: 0:08:17  lr: 0.000032  min_lr: 0.000000  loss: 3.7872 (3.8583)  loss_scale: 65536.0000 (66605.2481)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0001  max mem: 15572
Epoch: [18]  [1480/2809]  eta: 0:08:13  lr: 0.000032  min_lr: 0.000000  loss: 3.9840 (3.8598)  loss_scale: 65536.0000 (66598.0284)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [18]  [1490/2809]  eta: 0:08:09  lr: 0.000032  min_lr: 0.000000  loss: 3.8106 (3.8568)  loss_scale: 65536.0000 (66590.9054)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [18]  [1500/2809]  eta: 0:08:06  lr: 0.000032  min_lr: 0.000000  loss: 3.7152 (3.8577)  loss_scale: 65536.0000 (66583.8774)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [18]  [1510/2809]  eta: 0:08:02  lr: 0.000032  min_lr: 0.000000  loss: 3.9215 (3.8578)  loss_scale: 65536.0000 (66576.9424)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [18]  [1520/2809]  eta: 0:07:58  lr: 0.000032  min_lr: 0.000000  loss: 3.8611 (3.8592)  loss_scale: 65536.0000 (66570.0986)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [18]  [1530/2809]  eta: 0:07:54  lr: 0.000032  min_lr: 0.000000  loss: 3.8611 (3.8583)  loss_scale: 65536.0000 (66563.3442)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [18]  [1540/2809]  eta: 0:07:51  lr: 0.000032  min_lr: 0.000000  loss: 3.8639 (3.8582)  loss_scale: 65536.0000 (66556.6775)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0001  max mem: 15572
Epoch: [18]  [1550/2809]  eta: 0:07:47  lr: 0.000032  min_lr: 0.000000  loss: 3.8526 (3.8581)  loss_scale: 65536.0000 (66550.0967)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [18]  [1560/2809]  eta: 0:07:43  lr: 0.000032  min_lr: 0.000000  loss: 3.9491 (3.8595)  loss_scale: 65536.0000 (66543.6003)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
[2025-01-13 04:40:53,752] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:40:53,752] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:40:54,512] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52126
[2025-01-13 04:40:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:40:54,513] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [1570/2809]  eta: 0:07:40  lr: 0.000032  min_lr: 0.000000  loss: 4.2027 (3.8620)  loss_scale: 65536.0000 (66620.6187)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [18]  [1580/2809]  eta: 0:07:36  lr: 0.000032  min_lr: 0.000000  loss: 4.1083 (3.8627)  loss_scale: 65536.0000 (66613.7584)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0003  max mem: 15572
Epoch: [18]  [1590/2809]  eta: 0:07:32  lr: 0.000032  min_lr: 0.000000  loss: 3.9396 (3.8630)  loss_scale: 65536.0000 (66606.9843)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0002  max mem: 15572
Epoch: [18]  [1600/2809]  eta: 0:07:28  lr: 0.000032  min_lr: 0.000000  loss: 4.0228 (3.8639)  loss_scale: 65536.0000 (66600.2948)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [18]  [1610/2809]  eta: 0:07:25  lr: 0.000032  min_lr: 0.000000  loss: 3.9986 (3.8644)  loss_scale: 65536.0000 (66593.6884)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
Epoch: [18]  [1620/2809]  eta: 0:07:21  lr: 0.000032  min_lr: 0.000000  loss: 3.9376 (3.8645)  loss_scale: 65536.0000 (66587.1635)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [18]  [1630/2809]  eta: 0:07:17  lr: 0.000032  min_lr: 0.000000  loss: 3.9207 (3.8646)  loss_scale: 65536.0000 (66580.7186)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [18]  [1640/2809]  eta: 0:07:13  lr: 0.000032  min_lr: 0.000000  loss: 3.8404 (3.8640)  loss_scale: 65536.0000 (66574.3522)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [18]  [1650/2809]  eta: 0:07:10  lr: 0.000032  min_lr: 0.000000  loss: 3.9116 (3.8651)  loss_scale: 65536.0000 (66568.0630)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [18]  [1660/2809]  eta: 0:07:06  lr: 0.000032  min_lr: 0.000000  loss: 4.1543 (3.8660)  loss_scale: 65536.0000 (66561.8495)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [18]  [1670/2809]  eta: 0:07:02  lr: 0.000032  min_lr: 0.000000  loss: 4.0478 (3.8657)  loss_scale: 65536.0000 (66555.7104)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [18]  [1680/2809]  eta: 0:06:58  lr: 0.000031  min_lr: 0.000000  loss: 3.9203 (3.8652)  loss_scale: 65536.0000 (66549.6443)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [18]  [1690/2809]  eta: 0:06:55  lr: 0.000031  min_lr: 0.000000  loss: 3.7687 (3.8659)  loss_scale: 65536.0000 (66543.6499)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
[2025-01-13 04:41:41,992] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:41:41,992] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:41:43,089] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52258
[2025-01-13 04:41:43,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:41:43,090] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [1700/2809]  eta: 0:06:51  lr: 0.000031  min_lr: 0.000000  loss: 3.8872 (3.8640)  loss_scale: 65536.0000 (66653.3098)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0003  max mem: 15572
Epoch: [18]  [1710/2809]  eta: 0:06:47  lr: 0.000031  min_lr: 0.000000  loss: 3.8292 (3.8641)  loss_scale: 65536.0000 (66646.7797)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [18]  [1720/2809]  eta: 0:06:44  lr: 0.000031  min_lr: 0.000000  loss: 3.8292 (3.8641)  loss_scale: 65536.0000 (66640.3254)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [18]  [1730/2809]  eta: 0:06:40  lr: 0.000031  min_lr: 0.000000  loss: 3.7565 (3.8634)  loss_scale: 65536.0000 (66633.9457)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [18]  [1740/2809]  eta: 0:06:36  lr: 0.000031  min_lr: 0.000000  loss: 3.8195 (3.8641)  loss_scale: 65536.0000 (66627.6393)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [18]  [1750/2809]  eta: 0:06:32  lr: 0.000031  min_lr: 0.000000  loss: 3.9141 (3.8647)  loss_scale: 65536.0000 (66621.4049)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [18]  [1760/2809]  eta: 0:06:29  lr: 0.000031  min_lr: 0.000000  loss: 3.9141 (3.8641)  loss_scale: 65536.0000 (66615.2413)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [18]  [1770/2809]  eta: 0:06:25  lr: 0.000031  min_lr: 0.000000  loss: 3.9467 (3.8648)  loss_scale: 65536.0000 (66609.1474)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0001  max mem: 15572
[2025-01-13 04:42:11,552] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52335
[2025-01-13 04:42:11,552] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 04:42:11,552] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [1780/2809]  eta: 0:06:21  lr: 0.000031  min_lr: 0.000000  loss: 3.9955 (3.8634)  loss_scale: 65536.0000 (66455.9326)  weight_decay: 0.0500 (0.0500)  time: 0.3650  data: 0.0001  max mem: 15572
Epoch: [18]  [1790/2809]  eta: 0:06:18  lr: 0.000031  min_lr: 0.000000  loss: 3.4414 (3.8618)  loss_scale: 32768.0000 (66267.8370)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [18]  [1800/2809]  eta: 0:06:14  lr: 0.000031  min_lr: 0.000000  loss: 3.6423 (3.8615)  loss_scale: 32768.0000 (66081.8301)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [18]  [1810/2809]  eta: 0:06:10  lr: 0.000031  min_lr: 0.000000  loss: 3.9196 (3.8632)  loss_scale: 32768.0000 (65897.8774)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [18]  [1820/2809]  eta: 0:06:06  lr: 0.000031  min_lr: 0.000000  loss: 3.9743 (3.8631)  loss_scale: 32768.0000 (65715.9451)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [18]  [1830/2809]  eta: 0:06:03  lr: 0.000031  min_lr: 0.000000  loss: 3.8025 (3.8633)  loss_scale: 32768.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [18]  [1840/2809]  eta: 0:05:59  lr: 0.000031  min_lr: 0.000000  loss: 3.8232 (3.8634)  loss_scale: 32768.0000 (65358.0098)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [18]  [1850/2809]  eta: 0:05:55  lr: 0.000031  min_lr: 0.000000  loss: 3.9407 (3.8641)  loss_scale: 32768.0000 (65181.9427)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [18]  [1860/2809]  eta: 0:05:52  lr: 0.000031  min_lr: 0.000000  loss: 4.0992 (3.8654)  loss_scale: 32768.0000 (65007.7679)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [18]  [1870/2809]  eta: 0:05:48  lr: 0.000031  min_lr: 0.000000  loss: 4.1051 (3.8655)  loss_scale: 32768.0000 (64835.4548)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [18]  [1880/2809]  eta: 0:05:44  lr: 0.000031  min_lr: 0.000000  loss: 3.8330 (3.8646)  loss_scale: 32768.0000 (64664.9740)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [18]  [1890/2809]  eta: 0:05:40  lr: 0.000031  min_lr: 0.000000  loss: 3.8657 (3.8648)  loss_scale: 32768.0000 (64496.2961)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [18]  [1900/2809]  eta: 0:05:37  lr: 0.000031  min_lr: 0.000000  loss: 3.9271 (3.8656)  loss_scale: 32768.0000 (64329.3930)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
[2025-01-13 04:42:59,273] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:42:59,273] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [1910/2809]  eta: 0:05:33  lr: 0.000031  min_lr: 0.000000  loss: 4.0604 (3.8667)  loss_scale: 32768.0000 (64318.5599)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [18]  [1920/2809]  eta: 0:05:29  lr: 0.000031  min_lr: 0.000000  loss: 3.9282 (3.8666)  loss_scale: 65536.0000 (64324.8974)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [18]  [1930/2809]  eta: 0:05:26  lr: 0.000031  min_lr: 0.000000  loss: 3.7942 (3.8656)  loss_scale: 65536.0000 (64331.1693)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [18]  [1940/2809]  eta: 0:05:22  lr: 0.000031  min_lr: 0.000000  loss: 3.6033 (3.8650)  loss_scale: 65536.0000 (64337.3766)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [18]  [1950/2809]  eta: 0:05:18  lr: 0.000031  min_lr: 0.000000  loss: 3.9932 (3.8655)  loss_scale: 65536.0000 (64343.5202)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [18]  [1960/2809]  eta: 0:05:14  lr: 0.000031  min_lr: 0.000000  loss: 4.0327 (3.8651)  loss_scale: 65536.0000 (64349.6012)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [18]  [1970/2809]  eta: 0:05:11  lr: 0.000031  min_lr: 0.000000  loss: 4.0154 (3.8655)  loss_scale: 65536.0000 (64355.6205)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0003  max mem: 15572
Epoch: [18]  [1980/2809]  eta: 0:05:07  lr: 0.000031  min_lr: 0.000000  loss: 3.9923 (3.8656)  loss_scale: 65536.0000 (64361.5790)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [18]  [1990/2809]  eta: 0:05:03  lr: 0.000031  min_lr: 0.000000  loss: 3.9964 (3.8664)  loss_scale: 65536.0000 (64367.4776)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [18]  [2000/2809]  eta: 0:05:00  lr: 0.000031  min_lr: 0.000000  loss: 4.1679 (3.8677)  loss_scale: 65536.0000 (64373.3173)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [18]  [2010/2809]  eta: 0:04:56  lr: 0.000031  min_lr: 0.000000  loss: 4.2199 (3.8683)  loss_scale: 65536.0000 (64379.0990)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [18]  [2020/2809]  eta: 0:04:52  lr: 0.000031  min_lr: 0.000000  loss: 4.0600 (3.8691)  loss_scale: 65536.0000 (64384.8234)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
[2025-01-13 04:43:46,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:43:46,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [18]  [2030/2809]  eta: 0:04:48  lr: 0.000031  min_lr: 0.000000  loss: 4.0126 (3.8692)  loss_scale: 65536.0000 (64422.7592)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
[2025-01-13 04:43:47,757] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52595
[2025-01-13 04:43:47,757] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:43:47,757] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [2040/2809]  eta: 0:04:45  lr: 0.000031  min_lr: 0.000000  loss: 4.0204 (3.8694)  loss_scale: 65536.0000 (64492.4331)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [18]  [2050/2809]  eta: 0:04:41  lr: 0.000031  min_lr: 0.000000  loss: 3.9572 (3.8691)  loss_scale: 65536.0000 (64497.5212)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [18]  [2060/2809]  eta: 0:04:37  lr: 0.000031  min_lr: 0.000000  loss: 3.9362 (3.8696)  loss_scale: 65536.0000 (64502.5599)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [18]  [2070/2809]  eta: 0:04:34  lr: 0.000031  min_lr: 0.000000  loss: 3.7781 (3.8688)  loss_scale: 65536.0000 (64507.5500)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [18]  [2080/2809]  eta: 0:04:30  lr: 0.000031  min_lr: 0.000000  loss: 3.7122 (3.8690)  loss_scale: 65536.0000 (64512.4921)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0001  max mem: 15572
[2025-01-13 04:44:08,927] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52652
[2025-01-13 04:44:08,927] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 04:44:08,929] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [2090/2809]  eta: 0:04:26  lr: 0.000031  min_lr: 0.000000  loss: 3.9425 (3.8685)  loss_scale: 65536.0000 (64501.7159)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [18]  [2100/2809]  eta: 0:04:22  lr: 0.000031  min_lr: 0.000000  loss: 3.6098 (3.8670)  loss_scale: 32768.0000 (64350.6749)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [18]  [2110/2809]  eta: 0:04:19  lr: 0.000031  min_lr: 0.000000  loss: 3.7108 (3.8671)  loss_scale: 32768.0000 (64201.0649)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [18]  [2120/2809]  eta: 0:04:15  lr: 0.000031  min_lr: 0.000000  loss: 3.8591 (3.8675)  loss_scale: 32768.0000 (64052.8656)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [18]  [2130/2809]  eta: 0:04:11  lr: 0.000031  min_lr: 0.000000  loss: 3.9062 (3.8676)  loss_scale: 32768.0000 (63906.0573)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [18]  [2140/2809]  eta: 0:04:08  lr: 0.000031  min_lr: 0.000000  loss: 3.9220 (3.8677)  loss_scale: 32768.0000 (63760.6203)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [18]  [2150/2809]  eta: 0:04:04  lr: 0.000031  min_lr: 0.000000  loss: 4.0380 (3.8684)  loss_scale: 32768.0000 (63616.5356)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [18]  [2160/2809]  eta: 0:04:00  lr: 0.000031  min_lr: 0.000000  loss: 4.0560 (3.8691)  loss_scale: 32768.0000 (63473.7844)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [18]  [2170/2809]  eta: 0:03:56  lr: 0.000031  min_lr: 0.000000  loss: 3.8629 (3.8689)  loss_scale: 32768.0000 (63332.3482)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [18]  [2180/2809]  eta: 0:03:53  lr: 0.000031  min_lr: 0.000000  loss: 3.7312 (3.8680)  loss_scale: 32768.0000 (63192.2091)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [18]  [2190/2809]  eta: 0:03:49  lr: 0.000031  min_lr: 0.000000  loss: 3.7984 (3.8682)  loss_scale: 32768.0000 (63053.3492)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [18]  [2200/2809]  eta: 0:03:45  lr: 0.000031  min_lr: 0.000000  loss: 3.8562 (3.8682)  loss_scale: 32768.0000 (62915.7510)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [18]  [2210/2809]  eta: 0:03:42  lr: 0.000031  min_lr: 0.000000  loss: 3.8403 (3.8671)  loss_scale: 32768.0000 (62779.3976)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
[2025-01-13 04:44:56,474] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:44:56,474] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [2220/2809]  eta: 0:03:38  lr: 0.000031  min_lr: 0.000000  loss: 3.7006 (3.8668)  loss_scale: 32768.0000 (62673.7794)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [18]  [2230/2809]  eta: 0:03:34  lr: 0.000031  min_lr: 0.000000  loss: 3.8068 (3.8676)  loss_scale: 65536.0000 (62686.6087)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [18]  [2240/2809]  eta: 0:03:30  lr: 0.000031  min_lr: 0.000000  loss: 3.9173 (3.8667)  loss_scale: 65536.0000 (62699.3235)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [18]  [2250/2809]  eta: 0:03:27  lr: 0.000031  min_lr: 0.000000  loss: 3.9173 (3.8677)  loss_scale: 65536.0000 (62711.9254)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [18]  [2260/2809]  eta: 0:03:23  lr: 0.000031  min_lr: 0.000000  loss: 3.9187 (3.8667)  loss_scale: 65536.0000 (62724.4157)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [18]  [2270/2809]  eta: 0:03:19  lr: 0.000031  min_lr: 0.000000  loss: 3.7716 (3.8666)  loss_scale: 65536.0000 (62736.7961)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0003  max mem: 15572
Epoch: [18]  [2280/2809]  eta: 0:03:16  lr: 0.000031  min_lr: 0.000000  loss: 3.7078 (3.8657)  loss_scale: 65536.0000 (62749.0680)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [18]  [2290/2809]  eta: 0:03:12  lr: 0.000031  min_lr: 0.000000  loss: 3.6712 (3.8655)  loss_scale: 65536.0000 (62761.2326)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [18]  [2300/2809]  eta: 0:03:08  lr: 0.000031  min_lr: 0.000000  loss: 4.0205 (3.8659)  loss_scale: 65536.0000 (62773.2916)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [18]  [2310/2809]  eta: 0:03:05  lr: 0.000031  min_lr: 0.000000  loss: 3.8392 (3.8652)  loss_scale: 65536.0000 (62785.2462)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [18]  [2320/2809]  eta: 0:03:01  lr: 0.000031  min_lr: 0.000000  loss: 3.7920 (3.8655)  loss_scale: 65536.0000 (62797.0978)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [18]  [2330/2809]  eta: 0:02:57  lr: 0.000031  min_lr: 0.000000  loss: 3.8673 (3.8651)  loss_scale: 65536.0000 (62808.8477)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [18]  [2340/2809]  eta: 0:02:53  lr: 0.000031  min_lr: 0.000000  loss: 3.7510 (3.8643)  loss_scale: 65536.0000 (62820.4972)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
[2025-01-13 04:45:43,891] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:45:43,891] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:45:44,984] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52912
[2025-01-13 04:45:44,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:45:44,984] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [2350/2809]  eta: 0:02:50  lr: 0.000031  min_lr: 0.000000  loss: 3.7045 (3.8633)  loss_scale: 65536.0000 (62915.6750)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [18]  [2360/2809]  eta: 0:02:46  lr: 0.000031  min_lr: 0.000000  loss: 3.7274 (3.8634)  loss_scale: 65536.0000 (62926.7734)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [18]  [2370/2809]  eta: 0:02:42  lr: 0.000031  min_lr: 0.000000  loss: 4.0266 (3.8644)  loss_scale: 65536.0000 (62937.7782)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [18]  [2380/2809]  eta: 0:02:39  lr: 0.000031  min_lr: 0.000000  loss: 4.0681 (3.8650)  loss_scale: 65536.0000 (62948.6905)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [18]  [2390/2809]  eta: 0:02:35  lr: 0.000031  min_lr: 0.000000  loss: 3.9560 (3.8647)  loss_scale: 65536.0000 (62959.5115)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [18]  [2400/2809]  eta: 0:02:31  lr: 0.000031  min_lr: 0.000000  loss: 3.8222 (3.8643)  loss_scale: 65536.0000 (62970.2424)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0002  max mem: 15572
Epoch: [18]  [2410/2809]  eta: 0:02:27  lr: 0.000031  min_lr: 0.000000  loss: 3.6907 (3.8637)  loss_scale: 65536.0000 (62980.8843)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0002  max mem: 15572
Epoch: [18]  [2420/2809]  eta: 0:02:24  lr: 0.000031  min_lr: 0.000000  loss: 4.0068 (3.8646)  loss_scale: 65536.0000 (62991.4382)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [18]  [2430/2809]  eta: 0:02:20  lr: 0.000031  min_lr: 0.000000  loss: 4.0770 (3.8650)  loss_scale: 65536.0000 (63001.9054)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
[2025-01-13 04:46:16,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=53000, skipped=359, lr=[2.9997287810269747e-07, 2.9997287810269747e-07, 4.2853268300385355e-07, 4.2853268300385355e-07, 6.121895471483623e-07, 6.121895471483623e-07, 8.745564959262318e-07, 8.745564959262318e-07, 1.2493664227517598e-06, 1.2493664227517598e-06, 1.784809175359657e-06, 1.784809175359657e-06, 2.5497273933709387e-06, 2.5497273933709387e-06, 3.642467704815627e-06, 3.642467704815627e-06, 5.203525292593753e-06, 5.203525292593753e-06, 7.433607560848219e-06, 7.433607560848219e-06, 1.0619439372640312e-05, 1.0619439372640312e-05, 1.517062767520045e-05, 1.517062767520045e-05, 2.1672325250286358e-05, 2.1672325250286358e-05, 3.096046464326623e-05, 3.096046464326623e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 04:46:16,993] [INFO] [timer.py:260:stop] epoch=0/micro_step=53000/global_step=53000, RunningAvgSamplesPerSec=29.32270875705308, CurrSamplesPerSec=35.053091890694766, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [18]  [2440/2809]  eta: 0:02:16  lr: 0.000031  min_lr: 0.000000  loss: 4.0789 (3.8657)  loss_scale: 65536.0000 (63012.2868)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [18]  [2450/2809]  eta: 0:02:13  lr: 0.000031  min_lr: 0.000000  loss: 3.8096 (3.8653)  loss_scale: 65536.0000 (63022.5834)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [18]  [2460/2809]  eta: 0:02:09  lr: 0.000031  min_lr: 0.000000  loss: 3.8096 (3.8655)  loss_scale: 65536.0000 (63032.7964)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [18]  [2470/2809]  eta: 0:02:05  lr: 0.000031  min_lr: 0.000000  loss: 4.0883 (3.8663)  loss_scale: 65536.0000 (63042.9268)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 04:46:32,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:46:32,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:46:32,888] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53042
[2025-01-13 04:46:32,889] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:46:32,889] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [2480/2809]  eta: 0:02:01  lr: 0.000031  min_lr: 0.000000  loss: 3.9149 (3.8661)  loss_scale: 65536.0000 (63079.3906)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [18]  [2490/2809]  eta: 0:01:58  lr: 0.000031  min_lr: 0.000000  loss: 3.7908 (3.8657)  loss_scale: 65536.0000 (63089.2525)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [18]  [2500/2809]  eta: 0:01:54  lr: 0.000031  min_lr: 0.000000  loss: 3.7716 (3.8658)  loss_scale: 65536.0000 (63099.0356)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [18]  [2510/2809]  eta: 0:01:50  lr: 0.000031  min_lr: 0.000000  loss: 3.7664 (3.8649)  loss_scale: 65536.0000 (63108.7407)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [18]  [2520/2809]  eta: 0:01:47  lr: 0.000031  min_lr: 0.000000  loss: 3.5635 (3.8642)  loss_scale: 65536.0000 (63118.3689)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [18]  [2530/2809]  eta: 0:01:43  lr: 0.000031  min_lr: 0.000000  loss: 3.7487 (3.8641)  loss_scale: 65536.0000 (63127.9210)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [18]  [2540/2809]  eta: 0:01:39  lr: 0.000031  min_lr: 0.000000  loss: 3.8775 (3.8646)  loss_scale: 65536.0000 (63137.3979)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [18]  [2550/2809]  eta: 0:01:35  lr: 0.000031  min_lr: 0.000000  loss: 3.7816 (3.8641)  loss_scale: 65536.0000 (63146.8005)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [18]  [2560/2809]  eta: 0:01:32  lr: 0.000031  min_lr: 0.000000  loss: 3.9995 (3.8652)  loss_scale: 65536.0000 (63156.1296)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [18]  [2570/2809]  eta: 0:01:28  lr: 0.000031  min_lr: 0.000000  loss: 3.9995 (3.8650)  loss_scale: 65536.0000 (63165.3862)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [18]  [2580/2809]  eta: 0:01:24  lr: 0.000031  min_lr: 0.000000  loss: 3.7006 (3.8645)  loss_scale: 65536.0000 (63174.5711)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [18]  [2590/2809]  eta: 0:01:21  lr: 0.000031  min_lr: 0.000000  loss: 3.7109 (3.8645)  loss_scale: 65536.0000 (63183.6851)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [18]  [2600/2809]  eta: 0:01:17  lr: 0.000031  min_lr: 0.000000  loss: 3.9800 (3.8650)  loss_scale: 65536.0000 (63192.7290)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
[2025-01-13 04:47:20,708] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:47:20,708] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:47:21,073] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53172
[2025-01-13 04:47:21,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:47:21,073] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [2610/2809]  eta: 0:01:13  lr: 0.000031  min_lr: 0.000000  loss: 3.7237 (3.8637)  loss_scale: 65536.0000 (63226.8035)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [18]  [2620/2809]  eta: 0:01:10  lr: 0.000031  min_lr: 0.000000  loss: 3.6594 (3.8645)  loss_scale: 65536.0000 (63235.6139)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [18]  [2630/2809]  eta: 0:01:06  lr: 0.000031  min_lr: 0.000000  loss: 4.1045 (3.8650)  loss_scale: 65536.0000 (63244.3573)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [18]  [2640/2809]  eta: 0:01:02  lr: 0.000031  min_lr: 0.000000  loss: 3.8084 (3.8642)  loss_scale: 65536.0000 (63253.0345)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [18]  [2650/2809]  eta: 0:00:58  lr: 0.000031  min_lr: 0.000000  loss: 3.7745 (3.8638)  loss_scale: 65536.0000 (63261.6462)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [18]  [2660/2809]  eta: 0:00:55  lr: 0.000031  min_lr: 0.000000  loss: 3.8429 (3.8641)  loss_scale: 65536.0000 (63270.1932)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [18]  [2670/2809]  eta: 0:00:51  lr: 0.000031  min_lr: 0.000000  loss: 3.9864 (3.8643)  loss_scale: 65536.0000 (63278.6762)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [18]  [2680/2809]  eta: 0:00:47  lr: 0.000031  min_lr: 0.000000  loss: 3.8566 (3.8636)  loss_scale: 65536.0000 (63287.0959)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [18]  [2690/2809]  eta: 0:00:44  lr: 0.000031  min_lr: 0.000000  loss: 3.6579 (3.8634)  loss_scale: 65536.0000 (63295.4530)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [18]  [2700/2809]  eta: 0:00:40  lr: 0.000031  min_lr: 0.000000  loss: 3.7475 (3.8626)  loss_scale: 65536.0000 (63303.7482)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [18]  [2710/2809]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000000  loss: 3.6318 (3.8626)  loss_scale: 65536.0000 (63311.9823)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [18]  [2720/2809]  eta: 0:00:32  lr: 0.000031  min_lr: 0.000000  loss: 3.9197 (3.8630)  loss_scale: 65536.0000 (63320.1558)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [18]  [2730/2809]  eta: 0:00:29  lr: 0.000031  min_lr: 0.000000  loss: 3.9197 (3.8630)  loss_scale: 65536.0000 (63328.2695)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
[2025-01-13 04:48:08,831] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:48:08,831] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [18]  [2740/2809]  eta: 0:00:25  lr: 0.000031  min_lr: 0.000000  loss: 3.8984 (3.8629)  loss_scale: 65536.0000 (63384.1430)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
[2025-01-13 04:48:11,052] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53307
[2025-01-13 04:48:11,052] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:48:11,052] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [2750/2809]  eta: 0:00:21  lr: 0.000031  min_lr: 0.000000  loss: 3.8984 (3.8629)  loss_scale: 65536.0000 (63487.2555)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [18]  [2760/2809]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000000  loss: 3.8242 (3.8623)  loss_scale: 65536.0000 (63494.6758)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [18]  [2770/2809]  eta: 0:00:14  lr: 0.000031  min_lr: 0.000000  loss: 3.9290 (3.8625)  loss_scale: 65536.0000 (63502.0426)  weight_decay: 0.0500 (0.0500)  time: 0.3779  data: 0.0002  max mem: 15572
Epoch: [18]  [2780/2809]  eta: 0:00:10  lr: 0.000031  min_lr: 0.000000  loss: 3.9433 (3.8624)  loss_scale: 65536.0000 (63509.3563)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
[2025-01-13 04:48:27,432] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53351
[2025-01-13 04:48:27,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 04:48:27,432] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [2790/2809]  eta: 0:00:07  lr: 0.000031  min_lr: 0.000000  loss: 3.9433 (3.8627)  loss_scale: 65536.0000 (63493.1365)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [18]  [2800/2809]  eta: 0:00:03  lr: 0.000031  min_lr: 0.000000  loss: 3.9583 (3.8626)  loss_scale: 32768.0000 (63383.4431)  weight_decay: 0.0500 (0.0500)  time: 0.3628  data: 0.0002  max mem: 15572
Epoch: [18]  [2808/2809]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000000  loss: 3.8305 (3.8625)  loss_scale: 32768.0000 (63296.2506)  weight_decay: 0.0500 (0.0500)  time: 0.3581  data: 0.0001  max mem: 15572
Epoch: [18] Total time: 0:17:21 (0.3707 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000000  loss: 3.8305 (3.8625)  loss_scale: 32768.0000 (63296.2506)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:10:53  loss: 0.3089 (0.3089)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4025  data: 2.2276  max mem: 15572
Val:  [ 10/272]  eta: 0:01:49  loss: 2.7424 (2.5583)  acc1: 27.7778 (33.8384)  acc5: 66.6667 (64.6465)  time: 0.4179  data: 0.2617  max mem: 15572
Val:  [ 20/272]  eta: 0:01:13  loss: 2.6891 (2.6030)  acc1: 33.3333 (37.3016)  acc5: 66.6667 (67.1958)  time: 0.1877  data: 0.0327  max mem: 15572
Val:  [ 30/272]  eta: 0:01:00  loss: 2.6973 (2.6977)  acc1: 38.8889 (34.2294)  acc5: 66.6667 (67.5627)  time: 0.1590  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:53  loss: 2.7195 (2.7074)  acc1: 27.7778 (33.7398)  acc5: 72.2222 (68.6992)  time: 0.1625  data: 0.0018  max mem: 15572
Val:  [ 50/272]  eta: 0:00:47  loss: 2.6080 (2.6491)  acc1: 33.3333 (34.7495)  acc5: 72.2222 (70.6972)  time: 0.1582  data: 0.0018  max mem: 15572
Val:  [ 60/272]  eta: 0:00:43  loss: 1.7891 (2.5099)  acc1: 50.0000 (39.3443)  acc5: 83.3333 (72.3133)  time: 0.1587  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:40  loss: 1.5703 (2.4229)  acc1: 66.6667 (42.0188)  acc5: 88.8889 (73.7089)  time: 0.1641  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:37  loss: 2.0862 (2.4225)  acc1: 50.0000 (42.0439)  acc5: 77.7778 (73.3196)  time: 0.1603  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.5225 (2.4532)  acc1: 38.8889 (41.7582)  acc5: 72.2222 (73.2601)  time: 0.1575  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:32  loss: 2.5225 (2.4873)  acc1: 38.8889 (40.9791)  acc5: 77.7778 (72.5523)  time: 0.1555  data: 0.0003  max mem: 15572
Val:  [110/272]  eta: 0:00:29  loss: 2.7627 (2.5504)  acc1: 16.6667 (39.1892)  acc5: 66.6667 (71.4715)  time: 0.1538  data: 0.0003  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 2.9273 (2.5822)  acc1: 16.6667 (38.5216)  acc5: 66.6667 (70.8448)  time: 0.1561  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.4950 (2.5450)  acc1: 38.8889 (39.6947)  acc5: 72.2222 (71.4165)  time: 0.1570  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.9971 (2.5407)  acc1: 44.4444 (40.1103)  acc5: 77.7778 (71.1978)  time: 0.1532  data: 0.0005  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.5024 (2.5427)  acc1: 38.8889 (39.6247)  acc5: 72.2222 (71.6336)  time: 0.1518  data: 0.0006  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.4788 (2.5194)  acc1: 44.4444 (40.5797)  acc5: 77.7778 (72.2222)  time: 0.1559  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.6067 (2.5437)  acc1: 44.4444 (39.7661)  acc5: 72.2222 (71.8324)  time: 0.1586  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:15  loss: 2.5864 (2.5338)  acc1: 27.7778 (39.7790)  acc5: 72.2222 (72.3450)  time: 0.1570  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.5859 (2.5822)  acc1: 22.2222 (38.4235)  acc5: 72.2222 (71.0588)  time: 0.1616  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.7222 (2.5901)  acc1: 22.2222 (38.4190)  acc5: 66.6667 (70.9508)  time: 0.1614  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.3040 (2.5950)  acc1: 38.8889 (38.5993)  acc5: 72.2222 (70.8268)  time: 0.1552  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.5078 (2.5820)  acc1: 38.8889 (38.7883)  acc5: 72.2222 (70.9653)  time: 0.1575  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.8374 (2.5427)  acc1: 61.1111 (40.0433)  acc5: 83.3333 (71.6210)  time: 0.1579  data: 0.0003  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.7657 (2.5240)  acc1: 61.1111 (40.5256)  acc5: 88.8889 (72.0148)  time: 0.1557  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.5082 (2.5361)  acc1: 33.3333 (39.9956)  acc5: 72.2222 (71.9124)  time: 0.1537  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.4030 (2.4681)  acc1: 72.2222 (41.9115)  acc5: 88.8889 (72.7331)  time: 0.1523  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.2470 (2.4675)  acc1: 72.2222 (41.6974)  acc5: 88.8889 (72.7142)  time: 0.1420  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.2470 (2.4717)  acc1: 72.2222 (41.6752)  acc5: 88.8889 (72.6807)  time: 0.1362  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1672 s / it)
* Acc@1 41.675 Acc@5 72.681 loss 2.472
Accuracy of the network on the 4883 val videos: 41.7%
[2025-01-13 04:49:19,848] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 04:49:19,849] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 04:49:19,849] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 04:49:22,237] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 04:49:22,237] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.68%
Epoch: [19]  [   0/2809]  eta: 2:53:34  lr: 0.000031  min_lr: 0.000000  loss: 4.3869 (4.3869)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 3.7076  data: 3.3136  max mem: 15572
Epoch: [19]  [  10/2809]  eta: 0:32:36  lr: 0.000031  min_lr: 0.000000  loss: 3.7931 (3.7382)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6989  data: 0.3186  max mem: 15572
Epoch: [19]  [  20/2809]  eta: 0:25:13  lr: 0.000031  min_lr: 0.000000  loss: 3.7597 (3.7111)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3843  data: 0.0097  max mem: 15572
Epoch: [19]  [  30/2809]  eta: 0:22:29  lr: 0.000031  min_lr: 0.000000  loss: 3.8120 (3.7358)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [19]  [  40/2809]  eta: 0:21:04  lr: 0.000031  min_lr: 0.000000  loss: 3.8120 (3.7016)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [19]  [  50/2809]  eta: 0:20:11  lr: 0.000031  min_lr: 0.000000  loss: 3.9237 (3.7526)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [19]  [  60/2809]  eta: 0:19:34  lr: 0.000031  min_lr: 0.000000  loss: 3.7035 (3.7224)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [19]  [  70/2809]  eta: 0:19:06  lr: 0.000031  min_lr: 0.000000  loss: 3.6493 (3.7234)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [19]  [  80/2809]  eta: 0:18:46  lr: 0.000031  min_lr: 0.000000  loss: 3.7340 (3.7529)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [19]  [  90/2809]  eta: 0:18:28  lr: 0.000031  min_lr: 0.000000  loss: 3.9139 (3.7470)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [19]  [ 100/2809]  eta: 0:18:13  lr: 0.000031  min_lr: 0.000000  loss: 3.8484 (3.7525)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
[2025-01-13 04:50:06,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:50:06,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [ 110/2809]  eta: 0:18:01  lr: 0.000031  min_lr: 0.000000  loss: 3.8406 (3.7569)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [19]  [ 120/2809]  eta: 0:17:49  lr: 0.000031  min_lr: 0.000000  loss: 4.0009 (3.7777)  loss_scale: 65536.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [19]  [ 130/2809]  eta: 0:17:38  lr: 0.000031  min_lr: 0.000000  loss: 4.0098 (3.7853)  loss_scale: 65536.0000 (38271.0229)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [19]  [ 140/2809]  eta: 0:17:29  lr: 0.000031  min_lr: 0.000000  loss: 3.9577 (3.7967)  loss_scale: 65536.0000 (40204.7092)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0002  max mem: 15572
Epoch: [19]  [ 150/2809]  eta: 0:17:20  lr: 0.000031  min_lr: 0.000000  loss: 3.8554 (3.7977)  loss_scale: 65536.0000 (41882.2781)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [19]  [ 160/2809]  eta: 0:17:12  lr: 0.000031  min_lr: 0.000000  loss: 3.8554 (3.8057)  loss_scale: 65536.0000 (43351.4534)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [19]  [ 170/2809]  eta: 0:17:05  lr: 0.000031  min_lr: 0.000000  loss: 3.9255 (3.8108)  loss_scale: 65536.0000 (44648.7953)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [19]  [ 180/2809]  eta: 0:16:58  lr: 0.000031  min_lr: 0.000000  loss: 3.9169 (3.8144)  loss_scale: 65536.0000 (45802.7845)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [19]  [ 190/2809]  eta: 0:16:52  lr: 0.000031  min_lr: 0.000000  loss: 3.9079 (3.8212)  loss_scale: 65536.0000 (46835.9372)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0001  max mem: 15572
Epoch: [19]  [ 200/2809]  eta: 0:16:46  lr: 0.000031  min_lr: 0.000000  loss: 4.0725 (3.8257)  loss_scale: 65536.0000 (47766.2886)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0001  max mem: 15572
Epoch: [19]  [ 210/2809]  eta: 0:16:40  lr: 0.000031  min_lr: 0.000000  loss: 4.0725 (3.8320)  loss_scale: 65536.0000 (48608.4550)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [19]  [ 220/2809]  eta: 0:16:34  lr: 0.000031  min_lr: 0.000000  loss: 4.0299 (3.8434)  loss_scale: 65536.0000 (49374.4072)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [19]  [ 230/2809]  eta: 0:16:29  lr: 0.000031  min_lr: 0.000000  loss: 3.9696 (3.8388)  loss_scale: 65536.0000 (50074.0433)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
[2025-01-13 04:50:53,446] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:50:53,446] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:50:53,807] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53609
[2025-01-13 04:50:53,808] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:50:53,808] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 240/2809]  eta: 0:16:23  lr: 0.000031  min_lr: 0.000000  loss: 3.8859 (3.8378)  loss_scale: 65536.0000 (50987.5519)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [19]  [ 250/2809]  eta: 0:16:18  lr: 0.000031  min_lr: 0.000000  loss: 3.6918 (3.8290)  loss_scale: 65536.0000 (51567.1713)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [19]  [ 260/2809]  eta: 0:16:12  lr: 0.000031  min_lr: 0.000000  loss: 3.8530 (3.8349)  loss_scale: 65536.0000 (52102.3755)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [19]  [ 270/2809]  eta: 0:16:07  lr: 0.000031  min_lr: 0.000000  loss: 3.7584 (3.8253)  loss_scale: 65536.0000 (52598.0812)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [19]  [ 280/2809]  eta: 0:16:02  lr: 0.000030  min_lr: 0.000000  loss: 3.7209 (3.8324)  loss_scale: 65536.0000 (53058.5053)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [19]  [ 290/2809]  eta: 0:15:57  lr: 0.000030  min_lr: 0.000000  loss: 3.8284 (3.8235)  loss_scale: 65536.0000 (53487.2852)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [19]  [ 300/2809]  eta: 0:15:52  lr: 0.000030  min_lr: 0.000000  loss: 3.5685 (3.8240)  loss_scale: 65536.0000 (53887.5748)  weight_decay: 0.0500 (0.0500)  time: 0.3648  data: 0.0002  max mem: 15572
Epoch: [19]  [ 310/2809]  eta: 0:15:48  lr: 0.000030  min_lr: 0.000000  loss: 3.5831 (3.8220)  loss_scale: 65536.0000 (54262.1222)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [19]  [ 320/2809]  eta: 0:15:43  lr: 0.000030  min_lr: 0.000000  loss: 4.0299 (3.8286)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [19]  [ 330/2809]  eta: 0:15:38  lr: 0.000030  min_lr: 0.000000  loss: 4.0476 (3.8348)  loss_scale: 65536.0000 (54943.3233)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0001  max mem: 15572
Epoch: [19]  [ 340/2809]  eta: 0:15:34  lr: 0.000030  min_lr: 0.000000  loss: 3.9722 (3.8350)  loss_scale: 65536.0000 (55253.9589)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [19]  [ 350/2809]  eta: 0:15:30  lr: 0.000030  min_lr: 0.000000  loss: 3.8601 (3.8358)  loss_scale: 65536.0000 (55546.8946)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0001  max mem: 15572
Epoch: [19]  [ 360/2809]  eta: 0:15:25  lr: 0.000030  min_lr: 0.000000  loss: 3.7985 (3.8346)  loss_scale: 65536.0000 (55823.6011)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0001  max mem: 15572
[2025-01-13 04:51:41,251] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:51:41,251] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:51:41,986] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53740
[2025-01-13 04:51:41,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:51:41,986] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 370/2809]  eta: 0:15:20  lr: 0.000030  min_lr: 0.000000  loss: 3.9606 (3.8382)  loss_scale: 65536.0000 (56438.6846)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [19]  [ 380/2809]  eta: 0:15:16  lr: 0.000030  min_lr: 0.000000  loss: 3.9606 (3.8423)  loss_scale: 65536.0000 (56677.4593)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [19]  [ 390/2809]  eta: 0:15:12  lr: 0.000030  min_lr: 0.000000  loss: 4.0041 (3.8458)  loss_scale: 65536.0000 (56904.0205)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [19]  [ 400/2809]  eta: 0:15:08  lr: 0.000030  min_lr: 0.000000  loss: 3.9151 (3.8418)  loss_scale: 65536.0000 (57119.2818)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [19]  [ 410/2809]  eta: 0:15:04  lr: 0.000030  min_lr: 0.000000  loss: 3.8001 (3.8416)  loss_scale: 65536.0000 (57324.0681)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [19]  [ 420/2809]  eta: 0:15:00  lr: 0.000030  min_lr: 0.000000  loss: 3.8041 (3.8370)  loss_scale: 65536.0000 (57519.1259)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0001  max mem: 15572
Epoch: [19]  [ 430/2809]  eta: 0:14:56  lr: 0.000030  min_lr: 0.000000  loss: 3.8492 (3.8380)  loss_scale: 65536.0000 (57705.1323)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [19]  [ 440/2809]  eta: 0:14:52  lr: 0.000030  min_lr: 0.000000  loss: 3.9331 (3.8371)  loss_scale: 65536.0000 (57882.7029)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [19]  [ 450/2809]  eta: 0:14:48  lr: 0.000030  min_lr: 0.000000  loss: 3.9334 (3.8428)  loss_scale: 65536.0000 (58052.3991)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [19]  [ 460/2809]  eta: 0:14:43  lr: 0.000030  min_lr: 0.000000  loss: 4.0884 (3.8494)  loss_scale: 65536.0000 (58214.7332)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0001  max mem: 15572
Epoch: [19]  [ 470/2809]  eta: 0:14:39  lr: 0.000030  min_lr: 0.000000  loss: 3.8592 (3.8467)  loss_scale: 65536.0000 (58370.1741)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0001  max mem: 15572
Epoch: [19]  [ 480/2809]  eta: 0:14:35  lr: 0.000030  min_lr: 0.000000  loss: 3.7837 (3.8480)  loss_scale: 65536.0000 (58519.1518)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [19]  [ 490/2809]  eta: 0:14:31  lr: 0.000030  min_lr: 0.000000  loss: 3.9554 (3.8465)  loss_scale: 65536.0000 (58662.0611)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
[2025-01-13 04:52:29,756] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:52:29,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [ 500/2809]  eta: 0:14:27  lr: 0.000030  min_lr: 0.000000  loss: 3.9380 (3.8471)  loss_scale: 65536.0000 (59191.6966)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
[2025-01-13 04:52:31,580] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53874
[2025-01-13 04:52:31,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:52:31,580] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 510/2809]  eta: 0:14:23  lr: 0.000030  min_lr: 0.000000  loss: 3.9380 (3.8492)  loss_scale: 65536.0000 (59572.3523)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [19]  [ 520/2809]  eta: 0:14:18  lr: 0.000030  min_lr: 0.000000  loss: 3.9846 (3.8497)  loss_scale: 65536.0000 (59686.8177)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [19]  [ 530/2809]  eta: 0:14:14  lr: 0.000030  min_lr: 0.000000  loss: 4.0027 (3.8537)  loss_scale: 65536.0000 (59796.9718)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [19]  [ 540/2809]  eta: 0:14:10  lr: 0.000030  min_lr: 0.000000  loss: 4.1035 (3.8569)  loss_scale: 65536.0000 (59903.0536)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [19]  [ 550/2809]  eta: 0:14:06  lr: 0.000030  min_lr: 0.000000  loss: 3.8377 (3.8510)  loss_scale: 65536.0000 (60005.2849)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [19]  [ 560/2809]  eta: 0:14:02  lr: 0.000030  min_lr: 0.000000  loss: 3.8377 (3.8537)  loss_scale: 65536.0000 (60103.8717)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [19]  [ 570/2809]  eta: 0:13:58  lr: 0.000030  min_lr: 0.000000  loss: 4.1218 (3.8571)  loss_scale: 65536.0000 (60199.0053)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [19]  [ 580/2809]  eta: 0:13:54  lr: 0.000030  min_lr: 0.000000  loss: 3.9308 (3.8585)  loss_scale: 65536.0000 (60290.8640)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [19]  [ 590/2809]  eta: 0:13:50  lr: 0.000030  min_lr: 0.000000  loss: 3.7890 (3.8569)  loss_scale: 65536.0000 (60379.6142)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [19]  [ 600/2809]  eta: 0:13:46  lr: 0.000030  min_lr: 0.000000  loss: 3.7890 (3.8560)  loss_scale: 65536.0000 (60465.4110)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [19]  [ 610/2809]  eta: 0:13:42  lr: 0.000030  min_lr: 0.000000  loss: 3.8448 (3.8537)  loss_scale: 65536.0000 (60548.3993)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [19]  [ 620/2809]  eta: 0:13:39  lr: 0.000030  min_lr: 0.000000  loss: 3.8399 (3.8543)  loss_scale: 65536.0000 (60628.7150)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 04:53:17,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=54000, skipped=366, lr=[2.930698064107794e-07, 2.930698064107794e-07, 4.186711520153992e-07, 4.186711520153992e-07, 5.981016457362846e-07, 5.981016457362846e-07, 8.544309224804067e-07, 8.544309224804067e-07, 1.220615603543438e-06, 1.220615603543438e-06, 1.7437365764906259e-06, 1.7437365764906259e-06, 2.4910522521294658e-06, 2.4910522521294658e-06, 3.558646074470666e-06, 3.558646074470666e-06, 5.0837801063866654e-06, 5.0837801063866654e-06, 7.262543009123808e-06, 7.262543009123808e-06, 1.037506144160544e-05, 1.037506144160544e-05, 1.4821516345150631e-05, 1.4821516345150631e-05, 2.1173594778786615e-05, 2.1173594778786615e-05, 3.024799254112374e-05, 3.024799254112374e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 04:53:17,582] [INFO] [timer.py:260:stop] epoch=0/micro_step=54000/global_step=54000, RunningAvgSamplesPerSec=29.402982656003907, CurrSamplesPerSec=34.51924489480994, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [ 630/2809]  eta: 0:13:34  lr: 0.000030  min_lr: 0.000000  loss: 3.8648 (3.8538)  loss_scale: 65536.0000 (60706.4849)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
[2025-01-13 04:53:19,026] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:53:19,026] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:53:20,125] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54006
[2025-01-13 04:53:20,125] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:53:20,127] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 640/2809]  eta: 0:13:30  lr: 0.000030  min_lr: 0.000000  loss: 3.7087 (3.8504)  loss_scale: 65536.0000 (61088.5491)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [19]  [ 650/2809]  eta: 0:13:27  lr: 0.000030  min_lr: 0.000000  loss: 3.7087 (3.8493)  loss_scale: 65536.0000 (61156.8664)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [19]  [ 660/2809]  eta: 0:13:23  lr: 0.000030  min_lr: 0.000000  loss: 3.8253 (3.8507)  loss_scale: 65536.0000 (61223.1165)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [19]  [ 670/2809]  eta: 0:13:19  lr: 0.000030  min_lr: 0.000000  loss: 4.1013 (3.8539)  loss_scale: 65536.0000 (61287.3920)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0003  max mem: 15572
Epoch: [19]  [ 680/2809]  eta: 0:13:15  lr: 0.000030  min_lr: 0.000000  loss: 4.0447 (3.8537)  loss_scale: 65536.0000 (61349.7797)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
Epoch: [19]  [ 690/2809]  eta: 0:13:11  lr: 0.000030  min_lr: 0.000000  loss: 4.0447 (3.8559)  loss_scale: 65536.0000 (61410.3618)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [19]  [ 700/2809]  eta: 0:13:07  lr: 0.000030  min_lr: 0.000000  loss: 4.0406 (3.8556)  loss_scale: 65536.0000 (61469.2154)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [19]  [ 710/2809]  eta: 0:13:03  lr: 0.000030  min_lr: 0.000000  loss: 4.0102 (3.8583)  loss_scale: 65536.0000 (61526.4135)  weight_decay: 0.0500 (0.0500)  time: 0.3646  data: 0.0002  max mem: 15572
Epoch: [19]  [ 720/2809]  eta: 0:12:59  lr: 0.000030  min_lr: 0.000000  loss: 4.0997 (3.8637)  loss_scale: 65536.0000 (61582.0250)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [19]  [ 730/2809]  eta: 0:12:56  lr: 0.000030  min_lr: 0.000000  loss: 3.9410 (3.8593)  loss_scale: 65536.0000 (61636.1149)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [19]  [ 740/2809]  eta: 0:12:52  lr: 0.000030  min_lr: 0.000000  loss: 3.5513 (3.8583)  loss_scale: 65536.0000 (61688.7449)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0003  max mem: 15572
Epoch: [19]  [ 750/2809]  eta: 0:12:48  lr: 0.000030  min_lr: 0.000000  loss: 3.6725 (3.8568)  loss_scale: 65536.0000 (61739.9734)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [19]  [ 760/2809]  eta: 0:12:44  lr: 0.000030  min_lr: 0.000000  loss: 3.5918 (3.8528)  loss_scale: 65536.0000 (61789.8555)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
[2025-01-13 04:54:07,780] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:54:07,781] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:54:08,507] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54137
[2025-01-13 04:54:08,507] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:54:08,507] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 770/2809]  eta: 0:12:40  lr: 0.000030  min_lr: 0.000000  loss: 3.7693 (3.8540)  loss_scale: 65536.0000 (62008.4462)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [19]  [ 780/2809]  eta: 0:12:36  lr: 0.000030  min_lr: 0.000000  loss: 3.8765 (3.8507)  loss_scale: 65536.0000 (62053.6133)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [19]  [ 790/2809]  eta: 0:12:32  lr: 0.000030  min_lr: 0.000000  loss: 3.6185 (3.8506)  loss_scale: 65536.0000 (62097.6384)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [19]  [ 800/2809]  eta: 0:12:29  lr: 0.000030  min_lr: 0.000000  loss: 3.6550 (3.8500)  loss_scale: 65536.0000 (62140.5643)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [19]  [ 810/2809]  eta: 0:12:25  lr: 0.000030  min_lr: 0.000000  loss: 3.6906 (3.8497)  loss_scale: 65536.0000 (62182.4316)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [19]  [ 820/2809]  eta: 0:12:21  lr: 0.000030  min_lr: 0.000000  loss: 3.9604 (3.8525)  loss_scale: 65536.0000 (62223.2789)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [19]  [ 830/2809]  eta: 0:12:17  lr: 0.000030  min_lr: 0.000000  loss: 4.0079 (3.8533)  loss_scale: 65536.0000 (62263.1432)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [19]  [ 840/2809]  eta: 0:12:13  lr: 0.000030  min_lr: 0.000000  loss: 3.7949 (3.8529)  loss_scale: 65536.0000 (62302.0595)  weight_decay: 0.0500 (0.0500)  time: 0.3643  data: 0.0002  max mem: 15572
Epoch: [19]  [ 850/2809]  eta: 0:12:09  lr: 0.000030  min_lr: 0.000000  loss: 3.8643 (3.8536)  loss_scale: 65536.0000 (62340.0611)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [19]  [ 860/2809]  eta: 0:12:06  lr: 0.000030  min_lr: 0.000000  loss: 3.8643 (3.8524)  loss_scale: 65536.0000 (62377.1800)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [19]  [ 870/2809]  eta: 0:12:02  lr: 0.000030  min_lr: 0.000000  loss: 3.7765 (3.8512)  loss_scale: 65536.0000 (62413.4466)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [19]  [ 880/2809]  eta: 0:11:58  lr: 0.000030  min_lr: 0.000000  loss: 3.9582 (3.8536)  loss_scale: 65536.0000 (62448.8899)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [19]  [ 890/2809]  eta: 0:11:54  lr: 0.000030  min_lr: 0.000000  loss: 4.0194 (3.8542)  loss_scale: 65536.0000 (62483.5376)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
[2025-01-13 04:54:55,959] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:54:55,959] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:54:56,322] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54267
[2025-01-13 04:54:56,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:54:56,322] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 900/2809]  eta: 0:11:50  lr: 0.000030  min_lr: 0.000000  loss: 3.9595 (3.8568)  loss_scale: 65536.0000 (62590.1532)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [19]  [ 910/2809]  eta: 0:11:46  lr: 0.000030  min_lr: 0.000000  loss: 3.9938 (3.8593)  loss_scale: 65536.0000 (62622.4896)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0003  max mem: 15572
Epoch: [19]  [ 920/2809]  eta: 0:11:43  lr: 0.000030  min_lr: 0.000000  loss: 3.8996 (3.8582)  loss_scale: 65536.0000 (62654.1238)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [19]  [ 930/2809]  eta: 0:11:39  lr: 0.000030  min_lr: 0.000000  loss: 3.8013 (3.8578)  loss_scale: 65536.0000 (62685.0784)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [19]  [ 940/2809]  eta: 0:11:35  lr: 0.000030  min_lr: 0.000000  loss: 3.8706 (3.8574)  loss_scale: 65536.0000 (62715.3751)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [19]  [ 950/2809]  eta: 0:11:31  lr: 0.000030  min_lr: 0.000000  loss: 3.8243 (3.8569)  loss_scale: 65536.0000 (62745.0347)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [19]  [ 960/2809]  eta: 0:11:27  lr: 0.000030  min_lr: 0.000000  loss: 3.8243 (3.8565)  loss_scale: 65536.0000 (62774.0770)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [19]  [ 970/2809]  eta: 0:11:24  lr: 0.000030  min_lr: 0.000000  loss: 3.8724 (3.8575)  loss_scale: 65536.0000 (62802.5211)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [19]  [ 980/2809]  eta: 0:11:20  lr: 0.000030  min_lr: 0.000000  loss: 3.6414 (3.8563)  loss_scale: 65536.0000 (62830.3853)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [19]  [ 990/2809]  eta: 0:11:16  lr: 0.000030  min_lr: 0.000000  loss: 3.5620 (3.8561)  loss_scale: 65536.0000 (62857.6872)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [19]  [1000/2809]  eta: 0:11:12  lr: 0.000030  min_lr: 0.000000  loss: 3.5594 (3.8525)  loss_scale: 65536.0000 (62884.4436)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
Epoch: [19]  [1010/2809]  eta: 0:11:08  lr: 0.000030  min_lr: 0.000000  loss: 3.6527 (3.8540)  loss_scale: 65536.0000 (62910.6706)  weight_decay: 0.0500 (0.0500)  time: 0.3641  data: 0.0002  max mem: 15572
Epoch: [19]  [1020/2809]  eta: 0:11:04  lr: 0.000030  min_lr: 0.000000  loss: 4.0528 (3.8557)  loss_scale: 65536.0000 (62936.3839)  weight_decay: 0.0500 (0.0500)  time: 0.3653  data: 0.0002  max mem: 15572
[2025-01-13 04:55:43,740] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:55:43,740] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:55:45,227] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54400
[2025-01-13 04:55:45,227] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:55:45,227] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1030/2809]  eta: 0:11:01  lr: 0.000030  min_lr: 0.000000  loss: 3.8283 (3.8553)  loss_scale: 65536.0000 (63215.8603)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [19]  [1040/2809]  eta: 0:10:57  lr: 0.000030  min_lr: 0.000000  loss: 3.5996 (3.8534)  loss_scale: 65536.0000 (63238.1479)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [19]  [1050/2809]  eta: 0:10:53  lr: 0.000030  min_lr: 0.000000  loss: 3.5996 (3.8524)  loss_scale: 65536.0000 (63260.0114)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [19]  [1060/2809]  eta: 0:10:49  lr: 0.000030  min_lr: 0.000000  loss: 3.8280 (3.8529)  loss_scale: 65536.0000 (63281.4628)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [19]  [1070/2809]  eta: 0:10:46  lr: 0.000030  min_lr: 0.000000  loss: 3.8240 (3.8525)  loss_scale: 65536.0000 (63302.5135)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [19]  [1080/2809]  eta: 0:10:42  lr: 0.000030  min_lr: 0.000000  loss: 3.6307 (3.8497)  loss_scale: 65536.0000 (63323.1748)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [19]  [1090/2809]  eta: 0:10:38  lr: 0.000030  min_lr: 0.000000  loss: 3.6186 (3.8501)  loss_scale: 65536.0000 (63343.4574)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [19]  [1100/2809]  eta: 0:10:34  lr: 0.000030  min_lr: 0.000000  loss: 3.8838 (3.8507)  loss_scale: 65536.0000 (63363.3715)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [19]  [1110/2809]  eta: 0:10:31  lr: 0.000030  min_lr: 0.000000  loss: 4.0225 (3.8513)  loss_scale: 65536.0000 (63382.9271)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [19]  [1120/2809]  eta: 0:10:27  lr: 0.000030  min_lr: 0.000000  loss: 3.8387 (3.8490)  loss_scale: 65536.0000 (63402.1338)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [19]  [1130/2809]  eta: 0:10:23  lr: 0.000030  min_lr: 0.000000  loss: 3.8387 (3.8505)  loss_scale: 65536.0000 (63421.0009)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0001  max mem: 15572
Epoch: [19]  [1140/2809]  eta: 0:10:19  lr: 0.000030  min_lr: 0.000000  loss: 3.8562 (3.8501)  loss_scale: 65536.0000 (63439.5372)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0001  max mem: 15572
Epoch: [19]  [1150/2809]  eta: 0:10:16  lr: 0.000030  min_lr: 0.000000  loss: 3.7089 (3.8493)  loss_scale: 65536.0000 (63457.7515)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
[2025-01-13 04:56:32,734] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:56:32,735] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [1160/2809]  eta: 0:10:12  lr: 0.000030  min_lr: 0.000000  loss: 3.7827 (3.8491)  loss_scale: 65536.0000 (63644.9957)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
[2025-01-13 04:56:34,961] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54535
[2025-01-13 04:56:34,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:56:34,961] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1170/2809]  eta: 0:10:08  lr: 0.000030  min_lr: 0.000000  loss: 3.8194 (3.8477)  loss_scale: 65536.0000 (63829.0418)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0003  max mem: 15572
Epoch: [19]  [1180/2809]  eta: 0:10:04  lr: 0.000030  min_lr: 0.000000  loss: 3.8726 (3.8498)  loss_scale: 65536.0000 (63843.4953)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0003  max mem: 15572
Epoch: [19]  [1190/2809]  eta: 0:10:01  lr: 0.000030  min_lr: 0.000000  loss: 4.1509 (3.8505)  loss_scale: 65536.0000 (63857.7061)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [19]  [1200/2809]  eta: 0:09:57  lr: 0.000030  min_lr: 0.000000  loss: 3.9421 (3.8507)  loss_scale: 65536.0000 (63871.6803)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [19]  [1210/2809]  eta: 0:09:53  lr: 0.000030  min_lr: 0.000000  loss: 3.8788 (3.8496)  loss_scale: 65536.0000 (63885.4236)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [19]  [1220/2809]  eta: 0:09:49  lr: 0.000030  min_lr: 0.000000  loss: 3.8871 (3.8514)  loss_scale: 65536.0000 (63898.9419)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [19]  [1230/2809]  eta: 0:09:46  lr: 0.000030  min_lr: 0.000000  loss: 3.9933 (3.8516)  loss_scale: 65536.0000 (63912.2405)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0001  max mem: 15572
Epoch: [19]  [1240/2809]  eta: 0:09:42  lr: 0.000030  min_lr: 0.000000  loss: 3.9140 (3.8502)  loss_scale: 65536.0000 (63925.3247)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [19]  [1250/2809]  eta: 0:09:38  lr: 0.000030  min_lr: 0.000000  loss: 3.9140 (3.8504)  loss_scale: 65536.0000 (63938.1998)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [19]  [1260/2809]  eta: 0:09:35  lr: 0.000030  min_lr: 0.000000  loss: 3.9778 (3.8509)  loss_scale: 65536.0000 (63950.8707)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [19]  [1270/2809]  eta: 0:09:31  lr: 0.000030  min_lr: 0.000000  loss: 3.7919 (3.8516)  loss_scale: 65536.0000 (63963.3423)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [19]  [1280/2809]  eta: 0:09:27  lr: 0.000030  min_lr: 0.000000  loss: 3.6552 (3.8498)  loss_scale: 65536.0000 (63975.6190)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [19]  [1290/2809]  eta: 0:09:23  lr: 0.000030  min_lr: 0.000000  loss: 3.4404 (3.8472)  loss_scale: 65536.0000 (63987.7057)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
[2025-01-13 04:57:22,758] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:57:22,758] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:57:23,862] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54667
[2025-01-13 04:57:23,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:57:23,862] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1300/2809]  eta: 0:09:20  lr: 0.000030  min_lr: 0.000000  loss: 3.5043 (3.8462)  loss_scale: 65536.0000 (64150.7271)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [19]  [1310/2809]  eta: 0:09:16  lr: 0.000030  min_lr: 0.000000  loss: 3.8110 (3.8477)  loss_scale: 65536.0000 (64161.2937)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [19]  [1320/2809]  eta: 0:09:12  lr: 0.000030  min_lr: 0.000000  loss: 3.8586 (3.8467)  loss_scale: 65536.0000 (64171.7002)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [19]  [1330/2809]  eta: 0:09:08  lr: 0.000030  min_lr: 0.000000  loss: 3.5111 (3.8445)  loss_scale: 65536.0000 (64181.9504)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [19]  [1340/2809]  eta: 0:09:05  lr: 0.000030  min_lr: 0.000000  loss: 3.6762 (3.8446)  loss_scale: 65536.0000 (64192.0477)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [19]  [1350/2809]  eta: 0:09:01  lr: 0.000030  min_lr: 0.000000  loss: 3.7372 (3.8436)  loss_scale: 65536.0000 (64201.9956)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [19]  [1360/2809]  eta: 0:08:57  lr: 0.000030  min_lr: 0.000000  loss: 3.7082 (3.8423)  loss_scale: 65536.0000 (64211.7972)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [19]  [1370/2809]  eta: 0:08:54  lr: 0.000030  min_lr: 0.000000  loss: 3.7082 (3.8419)  loss_scale: 65536.0000 (64221.4559)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [19]  [1380/2809]  eta: 0:08:50  lr: 0.000030  min_lr: 0.000000  loss: 3.9590 (3.8417)  loss_scale: 65536.0000 (64230.9747)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [19]  [1390/2809]  eta: 0:08:46  lr: 0.000030  min_lr: 0.000000  loss: 3.7005 (3.8400)  loss_scale: 65536.0000 (64240.3566)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [19]  [1400/2809]  eta: 0:08:42  lr: 0.000030  min_lr: 0.000000  loss: 3.5193 (3.8387)  loss_scale: 65536.0000 (64249.6046)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0001  max mem: 15572
Epoch: [19]  [1410/2809]  eta: 0:08:39  lr: 0.000030  min_lr: 0.000000  loss: 3.7469 (3.8386)  loss_scale: 65536.0000 (64258.7215)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0001  max mem: 15572
Epoch: [19]  [1420/2809]  eta: 0:08:35  lr: 0.000030  min_lr: 0.000000  loss: 3.8173 (3.8391)  loss_scale: 65536.0000 (64267.7101)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
[2025-01-13 04:58:11,384] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:58:11,384] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 04:58:12,107] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54798
[2025-01-13 04:58:12,107] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:58:12,107] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1430/2809]  eta: 0:08:31  lr: 0.000030  min_lr: 0.000000  loss: 3.8694 (3.8391)  loss_scale: 65536.0000 (64368.1677)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [19]  [1440/2809]  eta: 0:08:27  lr: 0.000030  min_lr: 0.000000  loss: 3.8694 (3.8390)  loss_scale: 65536.0000 (64376.2720)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [19]  [1450/2809]  eta: 0:08:24  lr: 0.000030  min_lr: 0.000000  loss: 3.7739 (3.8389)  loss_scale: 65536.0000 (64384.2646)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0003  max mem: 15572
Epoch: [19]  [1460/2809]  eta: 0:08:20  lr: 0.000030  min_lr: 0.000000  loss: 3.8470 (3.8401)  loss_scale: 65536.0000 (64392.1478)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [19]  [1470/2809]  eta: 0:08:16  lr: 0.000030  min_lr: 0.000000  loss: 3.8302 (3.8384)  loss_scale: 65536.0000 (64399.9239)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [19]  [1480/2809]  eta: 0:08:13  lr: 0.000030  min_lr: 0.000000  loss: 3.8484 (3.8385)  loss_scale: 65536.0000 (64407.5949)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [19]  [1490/2809]  eta: 0:08:09  lr: 0.000030  min_lr: 0.000000  loss: 3.9414 (3.8385)  loss_scale: 65536.0000 (64415.1630)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0001  max mem: 15572
Epoch: [19]  [1500/2809]  eta: 0:08:05  lr: 0.000030  min_lr: 0.000000  loss: 3.8024 (3.8385)  loss_scale: 65536.0000 (64422.6302)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0001  max mem: 15572
Epoch: [19]  [1510/2809]  eta: 0:08:02  lr: 0.000030  min_lr: 0.000000  loss: 4.0461 (3.8405)  loss_scale: 65536.0000 (64429.9987)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [19]  [1520/2809]  eta: 0:07:58  lr: 0.000030  min_lr: 0.000000  loss: 4.0483 (3.8401)  loss_scale: 65536.0000 (64437.2702)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [19]  [1530/2809]  eta: 0:07:54  lr: 0.000030  min_lr: 0.000000  loss: 3.9707 (3.8419)  loss_scale: 65536.0000 (64444.4468)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [19]  [1540/2809]  eta: 0:07:50  lr: 0.000030  min_lr: 0.000000  loss: 4.0251 (3.8417)  loss_scale: 65536.0000 (64451.5302)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [19]  [1550/2809]  eta: 0:07:47  lr: 0.000030  min_lr: 0.000000  loss: 3.8797 (3.8418)  loss_scale: 65536.0000 (64458.5222)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0001  max mem: 15572
[2025-01-13 04:58:59,998] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 04:58:59,999] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [1560/2809]  eta: 0:07:43  lr: 0.000030  min_lr: 0.000000  loss: 3.7311 (3.8422)  loss_scale: 65536.0000 (64675.3414)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
[2025-01-13 04:59:01,853] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54932
[2025-01-13 04:59:01,853] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 04:59:01,853] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1570/2809]  eta: 0:07:39  lr: 0.000030  min_lr: 0.000000  loss: 4.1491 (3.8437)  loss_scale: 65536.0000 (64680.8199)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [19]  [1580/2809]  eta: 0:07:35  lr: 0.000030  min_lr: 0.000000  loss: 3.9802 (3.8424)  loss_scale: 65536.0000 (64686.2290)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [19]  [1590/2809]  eta: 0:07:32  lr: 0.000030  min_lr: 0.000000  loss: 3.8889 (3.8422)  loss_scale: 65536.0000 (64691.5701)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [19]  [1600/2809]  eta: 0:07:28  lr: 0.000030  min_lr: 0.000000  loss: 3.8889 (3.8409)  loss_scale: 65536.0000 (64696.8445)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
[2025-01-13 04:59:17,113] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54973
[2025-01-13 04:59:17,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 04:59:17,113] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [1610/2809]  eta: 0:07:24  lr: 0.000030  min_lr: 0.000000  loss: 3.7668 (3.8413)  loss_scale: 65536.0000 (64518.9919)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [19]  [1620/2809]  eta: 0:07:21  lr: 0.000030  min_lr: 0.000000  loss: 3.9714 (3.8419)  loss_scale: 32768.0000 (64323.1191)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
[2025-01-13 04:59:26,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=55000, skipped=375, lr=[2.860995950623248e-07, 2.860995950623248e-07, 4.087137072318926e-07, 4.087137072318926e-07, 5.838767246169895e-07, 5.838767246169895e-07, 8.341096065956993e-07, 8.341096065956993e-07, 1.1915851522795705e-06, 1.1915851522795705e-06, 1.7022645032565293e-06, 1.7022645032565293e-06, 2.4318064332236134e-06, 2.4318064332236134e-06, 3.474009190319448e-06, 3.474009190319448e-06, 4.962870271884926e-06, 4.962870271884926e-06, 7.089814674121324e-06, 7.089814674121324e-06, 1.0128306677316177e-05, 1.0128306677316177e-05, 1.446900953902311e-05, 1.446900953902311e-05, 2.0670013627175875e-05, 2.0670013627175875e-05, 2.9528590895965536e-05, 2.9528590895965536e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 04:59:26,650] [INFO] [timer.py:260:stop] epoch=0/micro_step=55000/global_step=55000, RunningAvgSamplesPerSec=29.480724974604556, CurrSamplesPerSec=35.2042190521821, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [1630/2809]  eta: 0:07:17  lr: 0.000030  min_lr: 0.000000  loss: 3.6965 (3.8409)  loss_scale: 32768.0000 (64129.6481)  weight_decay: 0.0500 (0.0500)  time: 0.3653  data: 0.0002  max mem: 15572
Epoch: [19]  [1640/2809]  eta: 0:07:13  lr: 0.000030  min_lr: 0.000000  loss: 3.6889 (3.8409)  loss_scale: 32768.0000 (63938.5350)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [19]  [1650/2809]  eta: 0:07:09  lr: 0.000030  min_lr: 0.000000  loss: 3.9007 (3.8409)  loss_scale: 32768.0000 (63749.7371)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [19]  [1660/2809]  eta: 0:07:06  lr: 0.000030  min_lr: 0.000000  loss: 4.0175 (3.8411)  loss_scale: 32768.0000 (63563.2125)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [19]  [1670/2809]  eta: 0:07:02  lr: 0.000029  min_lr: 0.000000  loss: 3.9046 (3.8400)  loss_scale: 32768.0000 (63378.9204)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [19]  [1680/2809]  eta: 0:06:58  lr: 0.000029  min_lr: 0.000000  loss: 3.7255 (3.8405)  loss_scale: 32768.0000 (63196.8209)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [19]  [1690/2809]  eta: 0:06:55  lr: 0.000029  min_lr: 0.000000  loss: 3.9612 (3.8405)  loss_scale: 32768.0000 (63016.8752)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [19]  [1700/2809]  eta: 0:06:51  lr: 0.000029  min_lr: 0.000000  loss: 3.8877 (3.8408)  loss_scale: 32768.0000 (62839.0453)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [19]  [1710/2809]  eta: 0:06:47  lr: 0.000029  min_lr: 0.000000  loss: 3.8356 (3.8402)  loss_scale: 32768.0000 (62663.2940)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [19]  [1720/2809]  eta: 0:06:43  lr: 0.000029  min_lr: 0.000000  loss: 3.7569 (3.8390)  loss_scale: 32768.0000 (62489.5851)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [19]  [1730/2809]  eta: 0:06:40  lr: 0.000029  min_lr: 0.000000  loss: 3.7569 (3.8394)  loss_scale: 32768.0000 (62317.8833)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
[2025-01-13 05:00:04,811] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:00:04,811] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [1740/2809]  eta: 0:06:36  lr: 0.000029  min_lr: 0.000000  loss: 4.0799 (3.8409)  loss_scale: 32768.0000 (62336.3676)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [19]  [1750/2809]  eta: 0:06:32  lr: 0.000029  min_lr: 0.000000  loss: 4.0239 (3.8398)  loss_scale: 65536.0000 (62354.6408)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [19]  [1760/2809]  eta: 0:06:29  lr: 0.000029  min_lr: 0.000000  loss: 3.8156 (3.8396)  loss_scale: 65536.0000 (62372.7064)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [19]  [1770/2809]  eta: 0:06:25  lr: 0.000029  min_lr: 0.000000  loss: 3.8156 (3.8402)  loss_scale: 65536.0000 (62390.5680)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [19]  [1780/2809]  eta: 0:06:21  lr: 0.000029  min_lr: 0.000000  loss: 3.8126 (3.8402)  loss_scale: 65536.0000 (62408.2291)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [19]  [1790/2809]  eta: 0:06:17  lr: 0.000029  min_lr: 0.000000  loss: 3.8428 (3.8407)  loss_scale: 65536.0000 (62425.6929)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [19]  [1800/2809]  eta: 0:06:14  lr: 0.000029  min_lr: 0.000000  loss: 3.8475 (3.8401)  loss_scale: 65536.0000 (62442.9628)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [19]  [1810/2809]  eta: 0:06:10  lr: 0.000029  min_lr: 0.000000  loss: 3.8475 (3.8403)  loss_scale: 65536.0000 (62460.0420)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [19]  [1820/2809]  eta: 0:06:06  lr: 0.000029  min_lr: 0.000000  loss: 3.9840 (3.8409)  loss_scale: 65536.0000 (62476.9336)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [19]  [1830/2809]  eta: 0:06:03  lr: 0.000029  min_lr: 0.000000  loss: 3.9975 (3.8414)  loss_scale: 65536.0000 (62493.6406)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [19]  [1840/2809]  eta: 0:05:59  lr: 0.000029  min_lr: 0.000000  loss: 3.9169 (3.8416)  loss_scale: 65536.0000 (62510.1662)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [19]  [1850/2809]  eta: 0:05:55  lr: 0.000029  min_lr: 0.000000  loss: 3.8611 (3.8417)  loss_scale: 65536.0000 (62526.5132)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
[2025-01-13 05:00:52,241] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:00:52,241] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [1860/2809]  eta: 0:05:51  lr: 0.000029  min_lr: 0.000000  loss: 3.8028 (3.8418)  loss_scale: 65536.0000 (62613.1155)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
[2025-01-13 05:00:53,342] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55233
[2025-01-13 05:00:53,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:00:53,342] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1870/2809]  eta: 0:05:48  lr: 0.000029  min_lr: 0.000000  loss: 3.7258 (3.8413)  loss_scale: 65536.0000 (62663.7648)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [19]  [1880/2809]  eta: 0:05:44  lr: 0.000029  min_lr: 0.000000  loss: 3.6626 (3.8407)  loss_scale: 65536.0000 (62679.0346)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [19]  [1890/2809]  eta: 0:05:40  lr: 0.000029  min_lr: 0.000000  loss: 3.6854 (3.8403)  loss_scale: 65536.0000 (62694.1428)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [19]  [1900/2809]  eta: 0:05:37  lr: 0.000029  min_lr: 0.000000  loss: 3.8804 (3.8401)  loss_scale: 65536.0000 (62709.0921)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [19]  [1910/2809]  eta: 0:05:33  lr: 0.000029  min_lr: 0.000000  loss: 3.9070 (3.8399)  loss_scale: 65536.0000 (62723.8849)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [19]  [1920/2809]  eta: 0:05:29  lr: 0.000029  min_lr: 0.000000  loss: 3.9092 (3.8395)  loss_scale: 65536.0000 (62738.5237)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [19]  [1930/2809]  eta: 0:05:25  lr: 0.000029  min_lr: 0.000000  loss: 3.7132 (3.8388)  loss_scale: 65536.0000 (62753.0109)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [19]  [1940/2809]  eta: 0:05:22  lr: 0.000029  min_lr: 0.000000  loss: 3.7425 (3.8389)  loss_scale: 65536.0000 (62767.3488)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [19]  [1950/2809]  eta: 0:05:18  lr: 0.000029  min_lr: 0.000000  loss: 3.8478 (3.8388)  loss_scale: 65536.0000 (62781.5397)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [19]  [1960/2809]  eta: 0:05:14  lr: 0.000029  min_lr: 0.000000  loss: 4.0026 (3.8399)  loss_scale: 65536.0000 (62795.5859)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [19]  [1970/2809]  eta: 0:05:11  lr: 0.000029  min_lr: 0.000000  loss: 3.9771 (3.8404)  loss_scale: 65536.0000 (62809.4896)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [19]  [1980/2809]  eta: 0:05:07  lr: 0.000029  min_lr: 0.000000  loss: 4.0017 (3.8410)  loss_scale: 65536.0000 (62823.2529)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [19]  [1990/2809]  eta: 0:05:03  lr: 0.000029  min_lr: 0.000000  loss: 3.9667 (3.8405)  loss_scale: 65536.0000 (62836.8780)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
[2025-01-13 05:01:40,944] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:01:40,944] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:01:41,699] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55364
[2025-01-13 05:01:41,700] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:01:41,700] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [2000/2809]  eta: 0:04:59  lr: 0.000029  min_lr: 0.000000  loss: 3.9035 (3.8412)  loss_scale: 65536.0000 (62915.8701)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [19]  [2010/2809]  eta: 0:04:56  lr: 0.000029  min_lr: 0.000000  loss: 3.9966 (3.8419)  loss_scale: 65536.0000 (62928.8991)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [19]  [2020/2809]  eta: 0:04:52  lr: 0.000029  min_lr: 0.000000  loss: 3.7556 (3.8416)  loss_scale: 65536.0000 (62941.7991)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [19]  [2030/2809]  eta: 0:04:48  lr: 0.000029  min_lr: 0.000000  loss: 3.7485 (3.8418)  loss_scale: 65536.0000 (62954.5721)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [19]  [2040/2809]  eta: 0:04:45  lr: 0.000029  min_lr: 0.000000  loss: 3.8723 (3.8428)  loss_scale: 65536.0000 (62967.2200)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [19]  [2050/2809]  eta: 0:04:41  lr: 0.000029  min_lr: 0.000000  loss: 4.1234 (3.8436)  loss_scale: 65536.0000 (62979.7445)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [19]  [2060/2809]  eta: 0:04:37  lr: 0.000029  min_lr: 0.000000  loss: 4.1091 (3.8441)  loss_scale: 65536.0000 (62992.1475)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [19]  [2070/2809]  eta: 0:04:33  lr: 0.000029  min_lr: 0.000000  loss: 4.0016 (3.8445)  loss_scale: 65536.0000 (63004.4307)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [19]  [2080/2809]  eta: 0:04:30  lr: 0.000029  min_lr: 0.000000  loss: 3.9326 (3.8440)  loss_scale: 65536.0000 (63016.5959)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [19]  [2090/2809]  eta: 0:04:26  lr: 0.000029  min_lr: 0.000000  loss: 3.6376 (3.8435)  loss_scale: 65536.0000 (63028.6447)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [19]  [2100/2809]  eta: 0:04:22  lr: 0.000029  min_lr: 0.000000  loss: 3.5628 (3.8426)  loss_scale: 65536.0000 (63040.5788)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [19]  [2110/2809]  eta: 0:04:19  lr: 0.000029  min_lr: 0.000000  loss: 3.9208 (3.8438)  loss_scale: 65536.0000 (63052.3998)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [19]  [2120/2809]  eta: 0:04:15  lr: 0.000029  min_lr: 0.000000  loss: 4.0244 (3.8445)  loss_scale: 65536.0000 (63064.1094)  weight_decay: 0.0500 (0.0500)  time: 0.3646  data: 0.0002  max mem: 15572
[2025-01-13 05:02:29,474] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:02:29,474] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:02:29,864] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55494
[2025-01-13 05:02:29,865] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:02:29,865] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [2130/2809]  eta: 0:04:11  lr: 0.000029  min_lr: 0.000000  loss: 3.9041 (3.8434)  loss_scale: 65536.0000 (63106.4627)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [19]  [2140/2809]  eta: 0:04:07  lr: 0.000029  min_lr: 0.000000  loss: 3.5039 (3.8421)  loss_scale: 65536.0000 (63117.8104)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
[2025-01-13 05:02:36,805] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55513
[2025-01-13 05:02:36,805] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:02:36,805] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [2150/2809]  eta: 0:04:04  lr: 0.000029  min_lr: 0.000000  loss: 3.9225 (3.8423)  loss_scale: 65536.0000 (62991.9479)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [19]  [2160/2809]  eta: 0:04:00  lr: 0.000029  min_lr: 0.000000  loss: 4.0263 (3.8436)  loss_scale: 32768.0000 (62852.0870)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [19]  [2170/2809]  eta: 0:03:56  lr: 0.000029  min_lr: 0.000000  loss: 3.9588 (3.8431)  loss_scale: 32768.0000 (62713.5145)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [19]  [2180/2809]  eta: 0:03:53  lr: 0.000029  min_lr: 0.000000  loss: 3.6993 (3.8427)  loss_scale: 32768.0000 (62576.2127)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [19]  [2190/2809]  eta: 0:03:49  lr: 0.000029  min_lr: 0.000000  loss: 3.9559 (3.8445)  loss_scale: 32768.0000 (62440.1643)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [19]  [2200/2809]  eta: 0:03:45  lr: 0.000029  min_lr: 0.000000  loss: 4.0219 (3.8442)  loss_scale: 32768.0000 (62305.3521)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [19]  [2210/2809]  eta: 0:03:42  lr: 0.000029  min_lr: 0.000000  loss: 3.8325 (3.8438)  loss_scale: 32768.0000 (62171.7594)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [19]  [2220/2809]  eta: 0:03:38  lr: 0.000029  min_lr: 0.000000  loss: 3.9209 (3.8438)  loss_scale: 32768.0000 (62039.3697)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [19]  [2230/2809]  eta: 0:03:34  lr: 0.000029  min_lr: 0.000000  loss: 3.8390 (3.8441)  loss_scale: 32768.0000 (61908.1667)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [19]  [2240/2809]  eta: 0:03:30  lr: 0.000029  min_lr: 0.000000  loss: 3.8390 (3.8441)  loss_scale: 32768.0000 (61778.1348)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0017  max mem: 15572
Epoch: [19]  [2250/2809]  eta: 0:03:27  lr: 0.000029  min_lr: 0.000000  loss: 3.8747 (3.8442)  loss_scale: 32768.0000 (61649.2581)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0017  max mem: 15572
Epoch: [19]  [2260/2809]  eta: 0:03:23  lr: 0.000029  min_lr: 0.000000  loss: 3.9587 (3.8447)  loss_scale: 32768.0000 (61521.5215)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [19]  [2270/2809]  eta: 0:03:19  lr: 0.000029  min_lr: 0.000000  loss: 3.9507 (3.8441)  loss_scale: 32768.0000 (61394.9097)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0003  max mem: 15572
[2025-01-13 05:03:24,753] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:03:24,753] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [2280/2809]  eta: 0:03:16  lr: 0.000029  min_lr: 0.000000  loss: 3.8510 (3.8436)  loss_scale: 32768.0000 (61413.0644)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [19]  [2290/2809]  eta: 0:03:12  lr: 0.000029  min_lr: 0.000000  loss: 3.6002 (3.8416)  loss_scale: 65536.0000 (61431.0607)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [19]  [2300/2809]  eta: 0:03:08  lr: 0.000029  min_lr: 0.000000  loss: 3.5556 (3.8414)  loss_scale: 65536.0000 (61448.9005)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [19]  [2310/2809]  eta: 0:03:04  lr: 0.000029  min_lr: 0.000000  loss: 3.7704 (3.8409)  loss_scale: 65536.0000 (61466.5859)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [19]  [2320/2809]  eta: 0:03:01  lr: 0.000029  min_lr: 0.000000  loss: 3.8427 (3.8412)  loss_scale: 65536.0000 (61484.1189)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [19]  [2330/2809]  eta: 0:02:57  lr: 0.000029  min_lr: 0.000000  loss: 4.0026 (3.8418)  loss_scale: 65536.0000 (61501.5015)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [19]  [2340/2809]  eta: 0:02:53  lr: 0.000029  min_lr: 0.000000  loss: 4.0399 (3.8417)  loss_scale: 65536.0000 (61518.7356)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [19]  [2350/2809]  eta: 0:02:50  lr: 0.000029  min_lr: 0.000000  loss: 3.7126 (3.8410)  loss_scale: 65536.0000 (61535.8231)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [19]  [2360/2809]  eta: 0:02:46  lr: 0.000029  min_lr: 0.000000  loss: 3.7126 (3.8404)  loss_scale: 65536.0000 (61552.7658)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [19]  [2370/2809]  eta: 0:02:42  lr: 0.000029  min_lr: 0.000000  loss: 3.7796 (3.8409)  loss_scale: 65536.0000 (61569.5656)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [19]  [2380/2809]  eta: 0:02:39  lr: 0.000029  min_lr: 0.000000  loss: 3.9475 (3.8413)  loss_scale: 65536.0000 (61586.2243)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [19]  [2390/2809]  eta: 0:02:35  lr: 0.000029  min_lr: 0.000000  loss: 3.8152 (3.8406)  loss_scale: 65536.0000 (61602.7436)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
[2025-01-13 05:04:12,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:04:12,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:04:12,631] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55771
[2025-01-13 05:04:12,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:04:12,631] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [2400/2809]  eta: 0:02:31  lr: 0.000029  min_lr: 0.000000  loss: 3.5540 (3.8391)  loss_scale: 65536.0000 (61646.4207)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [19]  [2410/2809]  eta: 0:02:27  lr: 0.000029  min_lr: 0.000000  loss: 3.8556 (3.8397)  loss_scale: 65536.0000 (61662.5533)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [19]  [2420/2809]  eta: 0:02:24  lr: 0.000029  min_lr: 0.000000  loss: 3.9746 (3.8402)  loss_scale: 65536.0000 (61678.5527)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0003  max mem: 15572
Epoch: [19]  [2430/2809]  eta: 0:02:20  lr: 0.000029  min_lr: 0.000000  loss: 3.8195 (3.8392)  loss_scale: 65536.0000 (61694.4204)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [19]  [2440/2809]  eta: 0:02:16  lr: 0.000029  min_lr: 0.000000  loss: 3.7423 (3.8392)  loss_scale: 65536.0000 (61710.1581)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [19]  [2450/2809]  eta: 0:02:13  lr: 0.000029  min_lr: 0.000000  loss: 3.9127 (3.8391)  loss_scale: 65536.0000 (61725.7674)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [19]  [2460/2809]  eta: 0:02:09  lr: 0.000029  min_lr: 0.000000  loss: 3.6142 (3.8383)  loss_scale: 65536.0000 (61741.2499)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [19]  [2470/2809]  eta: 0:02:05  lr: 0.000029  min_lr: 0.000000  loss: 3.7868 (3.8380)  loss_scale: 65536.0000 (61756.6070)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [19]  [2480/2809]  eta: 0:02:01  lr: 0.000029  min_lr: 0.000000  loss: 3.8070 (3.8376)  loss_scale: 65536.0000 (61771.8404)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [19]  [2490/2809]  eta: 0:01:58  lr: 0.000029  min_lr: 0.000000  loss: 3.8494 (3.8391)  loss_scale: 65536.0000 (61786.9514)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [19]  [2500/2809]  eta: 0:01:54  lr: 0.000029  min_lr: 0.000000  loss: 3.8494 (3.8384)  loss_scale: 65536.0000 (61801.9416)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [19]  [2510/2809]  eta: 0:01:50  lr: 0.000029  min_lr: 0.000000  loss: 3.7764 (3.8387)  loss_scale: 65536.0000 (61816.8124)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [19]  [2520/2809]  eta: 0:01:47  lr: 0.000029  min_lr: 0.000000  loss: 3.8531 (3.8385)  loss_scale: 65536.0000 (61831.5653)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
[2025-01-13 05:05:00,272] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:05:00,273] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [2530/2809]  eta: 0:01:43  lr: 0.000029  min_lr: 0.000000  loss: 3.7259 (3.8380)  loss_scale: 65536.0000 (61897.9881)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
[2025-01-13 05:05:01,746] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55904
[2025-01-13 05:05:01,746] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:05:01,746] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [2540/2809]  eta: 0:01:39  lr: 0.000029  min_lr: 0.000000  loss: 3.9584 (3.8380)  loss_scale: 65536.0000 (61963.8882)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [19]  [2550/2809]  eta: 0:01:35  lr: 0.000029  min_lr: 0.000000  loss: 3.9958 (3.8388)  loss_scale: 65536.0000 (61977.8910)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [19]  [2560/2809]  eta: 0:01:32  lr: 0.000029  min_lr: 0.000000  loss: 3.9647 (3.8392)  loss_scale: 65536.0000 (61991.7845)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [19]  [2570/2809]  eta: 0:01:28  lr: 0.000029  min_lr: 0.000000  loss: 3.7455 (3.8382)  loss_scale: 65536.0000 (62005.5698)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [19]  [2580/2809]  eta: 0:01:24  lr: 0.000029  min_lr: 0.000000  loss: 3.7192 (3.8385)  loss_scale: 65536.0000 (62019.2484)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [19]  [2590/2809]  eta: 0:01:21  lr: 0.000029  min_lr: 0.000000  loss: 4.0179 (3.8390)  loss_scale: 65536.0000 (62032.8213)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [19]  [2600/2809]  eta: 0:01:17  lr: 0.000029  min_lr: 0.000000  loss: 3.9128 (3.8384)  loss_scale: 65536.0000 (62046.2899)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [19]  [2610/2809]  eta: 0:01:13  lr: 0.000029  min_lr: 0.000000  loss: 4.0864 (3.8397)  loss_scale: 65536.0000 (62059.6553)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [19]  [2620/2809]  eta: 0:01:10  lr: 0.000029  min_lr: 0.000000  loss: 4.0116 (3.8393)  loss_scale: 65536.0000 (62072.9187)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
[2025-01-13 05:05:37,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=56000, skipped=381, lr=[2.790693606018548e-07, 2.790693606018548e-07, 3.9867051514550694e-07, 3.9867051514550694e-07, 5.695293073507243e-07, 5.695293073507243e-07, 8.136132962153204e-07, 8.136132962153204e-07, 1.1623047088790292e-06, 1.1623047088790292e-06, 1.6604352983986132e-06, 1.6604352983986132e-06, 2.3720504262837332e-06, 2.3720504262837332e-06, 3.3886434661196192e-06, 3.3886434661196192e-06, 4.840919237313742e-06, 4.840919237313742e-06, 6.915598910448203e-06, 6.915598910448203e-06, 9.879427014926004e-06, 9.879427014926004e-06, 1.4113467164180008e-05, 1.4113467164180008e-05, 2.0162095948828585e-05, 2.0162095948828585e-05, 2.8802994212612265e-05, 2.8802994212612265e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 05:05:37,007] [INFO] [timer.py:260:stop] epoch=0/micro_step=56000/global_step=56000, RunningAvgSamplesPerSec=29.55475782245124, CurrSamplesPerSec=33.84277242102715, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [2630/2809]  eta: 0:01:06  lr: 0.000029  min_lr: 0.000000  loss: 3.8730 (3.8394)  loss_scale: 65536.0000 (62086.0813)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0002  max mem: 15572
Epoch: [19]  [2640/2809]  eta: 0:01:02  lr: 0.000029  min_lr: 0.000000  loss: 3.8798 (3.8392)  loss_scale: 65536.0000 (62099.1443)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [19]  [2650/2809]  eta: 0:00:58  lr: 0.000029  min_lr: 0.000000  loss: 3.7937 (3.8398)  loss_scale: 65536.0000 (62112.1086)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [19]  [2660/2809]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000000  loss: 3.7782 (3.8396)  loss_scale: 65536.0000 (62124.9756)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0001  max mem: 15572
[2025-01-13 05:05:49,426] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:05:49,426] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:05:51,298] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56038
[2025-01-13 05:05:51,298] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:05:51,298] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [2670/2809]  eta: 0:00:51  lr: 0.000029  min_lr: 0.000000  loss: 3.7217 (3.8392)  loss_scale: 65536.0000 (62260.4268)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0001  max mem: 15572
Epoch: [19]  [2680/2809]  eta: 0:00:47  lr: 0.000029  min_lr: 0.000000  loss: 3.9626 (3.8396)  loss_scale: 65536.0000 (62272.6445)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0001  max mem: 15572
Epoch: [19]  [2690/2809]  eta: 0:00:44  lr: 0.000029  min_lr: 0.000000  loss: 3.9651 (3.8397)  loss_scale: 65536.0000 (62284.7715)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [19]  [2700/2809]  eta: 0:00:40  lr: 0.000029  min_lr: 0.000000  loss: 3.8688 (3.8393)  loss_scale: 65536.0000 (62296.8086)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [19]  [2710/2809]  eta: 0:00:36  lr: 0.000029  min_lr: 0.000000  loss: 3.8308 (3.8389)  loss_scale: 65536.0000 (62308.7569)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [19]  [2720/2809]  eta: 0:00:32  lr: 0.000029  min_lr: 0.000000  loss: 3.7785 (3.8389)  loss_scale: 65536.0000 (62320.6174)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [19]  [2730/2809]  eta: 0:00:29  lr: 0.000029  min_lr: 0.000000  loss: 3.7785 (3.8385)  loss_scale: 65536.0000 (62332.3911)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [19]  [2740/2809]  eta: 0:00:25  lr: 0.000029  min_lr: 0.000000  loss: 3.9475 (3.8388)  loss_scale: 65536.0000 (62344.0788)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [19]  [2750/2809]  eta: 0:00:21  lr: 0.000029  min_lr: 0.000000  loss: 3.9250 (3.8384)  loss_scale: 65536.0000 (62355.6816)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [19]  [2760/2809]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000000  loss: 3.6746 (3.8380)  loss_scale: 65536.0000 (62367.2003)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [19]  [2770/2809]  eta: 0:00:14  lr: 0.000029  min_lr: 0.000000  loss: 3.7283 (3.8370)  loss_scale: 65536.0000 (62378.6359)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [19]  [2780/2809]  eta: 0:00:10  lr: 0.000029  min_lr: 0.000000  loss: 3.6673 (3.8367)  loss_scale: 65536.0000 (62389.9892)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [19]  [2790/2809]  eta: 0:00:07  lr: 0.000029  min_lr: 0.000000  loss: 3.6673 (3.8363)  loss_scale: 65536.0000 (62401.2612)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
[2025-01-13 05:06:38,933] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:06:38,933] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [2800/2809]  eta: 0:00:03  lr: 0.000029  min_lr: 0.000000  loss: 3.8499 (3.8366)  loss_scale: 65536.0000 (62529.4395)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0001  max mem: 15572
[2025-01-13 05:06:41,443] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56174
[2025-01-13 05:06:41,443] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:06:41,443] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [2808/2809]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000000  loss: 3.7427 (3.8361)  loss_scale: 65536.0000 (62584.6636)  weight_decay: 0.0500 (0.0500)  time: 0.3623  data: 0.0001  max mem: 15572
Epoch: [19] Total time: 0:17:21 (0.3706 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000000  loss: 3.7427 (3.8361)  loss_scale: 65536.0000 (62584.6636)  weight_decay: 0.0500 (0.0500)
[2025-01-13 05:06:43,379] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2025-01-13 05:06:43,381] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-19/mp_rank_00_model_states.pt
[2025-01-13 05:06:43,381] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-19/mp_rank_00_model_states.pt...
[2025-01-13 05:06:43,599] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-19/mp_rank_00_model_states.pt.
[2025-01-13 05:06:43,599] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [  0/272]  eta: 0:11:09  loss: 0.6131 (0.6131)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4609  data: 2.2859  max mem: 15572
Val:  [ 10/272]  eta: 0:01:48  loss: 2.6199 (2.4453)  acc1: 38.8889 (38.8889)  acc5: 77.7778 (69.1919)  time: 0.4132  data: 0.2567  max mem: 15572
Val:  [ 20/272]  eta: 0:01:12  loss: 2.5757 (2.5025)  acc1: 38.8889 (41.5344)  acc5: 72.2222 (70.3704)  time: 0.1786  data: 0.0271  max mem: 15572
Val:  [ 30/272]  eta: 0:01:00  loss: 2.4976 (2.6060)  acc1: 38.8889 (37.4552)  acc5: 72.2222 (68.8172)  time: 0.1576  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:52  loss: 2.7344 (2.6295)  acc1: 27.7778 (35.5014)  acc5: 72.2222 (69.6477)  time: 0.1615  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:47  loss: 2.6983 (2.5504)  acc1: 27.7778 (37.5817)  acc5: 77.7778 (71.8954)  time: 0.1619  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:43  loss: 1.5749 (2.4259)  acc1: 61.1111 (41.3479)  acc5: 88.8889 (73.1330)  time: 0.1611  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:40  loss: 1.5695 (2.3381)  acc1: 66.6667 (44.4444)  acc5: 88.8889 (74.5696)  time: 0.1567  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:37  loss: 1.9844 (2.3450)  acc1: 55.5556 (44.5816)  acc5: 77.7778 (74.7599)  time: 0.1645  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.3837 (2.3672)  acc1: 44.4444 (43.9560)  acc5: 77.7778 (74.9695)  time: 0.1666  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:32  loss: 2.3837 (2.3992)  acc1: 38.8889 (43.2893)  acc5: 72.2222 (74.3674)  time: 0.1614  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:30  loss: 2.6833 (2.4755)  acc1: 16.6667 (41.0410)  acc5: 66.6667 (73.1732)  time: 0.1585  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 3.0065 (2.5039)  acc1: 22.2222 (40.4959)  acc5: 66.6667 (72.7273)  time: 0.1546  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.2187 (2.4741)  acc1: 44.4444 (41.7727)  acc5: 72.2222 (73.1552)  time: 0.1557  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.8867 (2.4668)  acc1: 50.0000 (42.1592)  acc5: 77.7778 (73.0496)  time: 0.1565  data: 0.0003  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.4281 (2.4707)  acc1: 38.8889 (41.6115)  acc5: 72.2222 (73.1420)  time: 0.1527  data: 0.0003  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.4022 (2.4530)  acc1: 44.4444 (42.4776)  acc5: 77.7778 (73.6715)  time: 0.1525  data: 0.0003  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.5231 (2.4779)  acc1: 38.8889 (41.6504)  acc5: 72.2222 (73.1969)  time: 0.1556  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:15  loss: 2.5231 (2.4717)  acc1: 33.3333 (41.4979)  acc5: 72.2222 (73.5114)  time: 0.1558  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.5474 (2.5211)  acc1: 27.7778 (40.3432)  acc5: 66.6667 (72.0768)  time: 0.1573  data: 0.0035  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.7784 (2.5306)  acc1: 27.7778 (40.1879)  acc5: 61.1111 (71.9182)  time: 0.1610  data: 0.0035  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.2204 (2.5261)  acc1: 44.4444 (40.7320)  acc5: 77.7778 (72.0379)  time: 0.1610  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.0358 (2.5126)  acc1: 55.5556 (41.1262)  acc5: 77.7778 (72.2474)  time: 0.1649  data: 0.0005  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.9300 (2.4831)  acc1: 55.5556 (42.1597)  acc5: 83.3333 (72.6551)  time: 0.1643  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.7439 (2.4655)  acc1: 61.1111 (42.5311)  acc5: 83.3333 (73.0752)  time: 0.1650  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.4829 (2.4791)  acc1: 38.8889 (41.9655)  acc5: 77.7778 (72.9748)  time: 0.1653  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.4432 (2.4172)  acc1: 72.2222 (43.7633)  acc5: 88.8889 (73.7548)  time: 0.1532  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.5348 (2.4170)  acc1: 61.1111 (43.5424)  acc5: 88.8889 (73.8417)  time: 0.1402  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.5348 (2.4232)  acc1: 61.1111 (43.5183)  acc5: 88.8889 (73.8071)  time: 0.1346  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1685 s / it)
* Acc@1 43.518 Acc@5 73.807 loss 2.423
Accuracy of the network on the 4883 val videos: 43.5%
[2025-01-13 05:07:29,436] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 05:07:29,437] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 05:07:29,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 05:07:31,837] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 05:07:31,837] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 43.52%
Epoch: [20]  [   0/2809]  eta: 3:10:31  lr: 0.000029  min_lr: 0.000000  loss: 3.7727 (3.7727)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.0695  data: 3.6723  max mem: 15572
Epoch: [20]  [  10/2809]  eta: 0:33:27  lr: 0.000029  min_lr: 0.000000  loss: 4.0022 (3.9947)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7171  data: 0.3342  max mem: 15572
Epoch: [20]  [  20/2809]  eta: 0:25:41  lr: 0.000029  min_lr: 0.000000  loss: 3.9554 (3.9178)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0003  max mem: 15572
Epoch: [20]  [  30/2809]  eta: 0:22:51  lr: 0.000029  min_lr: 0.000000  loss: 3.7934 (3.8302)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [20]  [  40/2809]  eta: 0:21:22  lr: 0.000029  min_lr: 0.000000  loss: 3.9669 (3.8606)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [20]  [  50/2809]  eta: 0:20:25  lr: 0.000029  min_lr: 0.000000  loss: 3.7194 (3.8277)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [20]  [  60/2809]  eta: 0:19:45  lr: 0.000029  min_lr: 0.000000  loss: 3.8460 (3.8580)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [20]  [  70/2809]  eta: 0:19:17  lr: 0.000029  min_lr: 0.000000  loss: 3.9838 (3.8775)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [20]  [  80/2809]  eta: 0:18:55  lr: 0.000029  min_lr: 0.000000  loss: 3.7750 (3.8419)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [20]  [  90/2809]  eta: 0:18:36  lr: 0.000029  min_lr: 0.000000  loss: 3.7280 (3.8433)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [20]  [ 100/2809]  eta: 0:18:20  lr: 0.000029  min_lr: 0.000000  loss: 3.8253 (3.8457)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [20]  [ 110/2809]  eta: 0:18:06  lr: 0.000029  min_lr: 0.000000  loss: 3.8253 (3.8523)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [20]  [ 120/2809]  eta: 0:17:54  lr: 0.000029  min_lr: 0.000000  loss: 3.8091 (3.8490)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
[2025-01-13 05:08:21,311] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:08:21,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:08:23,162] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56308
[2025-01-13 05:08:23,162] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:08:23,162] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [ 130/2809]  eta: 0:17:43  lr: 0.000029  min_lr: 0.000000  loss: 3.8378 (3.8517)  loss_scale: 65536.0000 (68037.3740)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [20]  [ 140/2809]  eta: 0:17:35  lr: 0.000029  min_lr: 0.000000  loss: 4.0559 (3.8742)  loss_scale: 65536.0000 (67859.9716)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [20]  [ 150/2809]  eta: 0:17:26  lr: 0.000029  min_lr: 0.000000  loss: 4.1083 (3.8850)  loss_scale: 65536.0000 (67706.0662)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [20]  [ 160/2809]  eta: 0:17:17  lr: 0.000029  min_lr: 0.000000  loss: 4.0692 (3.8850)  loss_scale: 65536.0000 (67571.2795)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [20]  [ 170/2809]  eta: 0:17:10  lr: 0.000029  min_lr: 0.000000  loss: 3.8997 (3.8795)  loss_scale: 65536.0000 (67452.2573)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [20]  [ 180/2809]  eta: 0:17:03  lr: 0.000029  min_lr: 0.000000  loss: 4.0227 (3.8894)  loss_scale: 65536.0000 (67346.3867)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [20]  [ 190/2809]  eta: 0:16:57  lr: 0.000029  min_lr: 0.000000  loss: 4.0940 (3.8912)  loss_scale: 65536.0000 (67251.6021)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [20]  [ 200/2809]  eta: 0:16:50  lr: 0.000029  min_lr: 0.000000  loss: 3.9242 (3.8942)  loss_scale: 65536.0000 (67166.2488)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [20]  [ 210/2809]  eta: 0:16:43  lr: 0.000029  min_lr: 0.000000  loss: 4.0336 (3.8901)  loss_scale: 65536.0000 (67088.9858)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [20]  [ 220/2809]  eta: 0:16:37  lr: 0.000029  min_lr: 0.000000  loss: 3.9612 (3.8886)  loss_scale: 65536.0000 (67018.7149)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [20]  [ 230/2809]  eta: 0:16:31  lr: 0.000029  min_lr: 0.000000  loss: 3.8969 (3.8920)  loss_scale: 65536.0000 (66954.5281)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [20]  [ 240/2809]  eta: 0:16:25  lr: 0.000028  min_lr: 0.000000  loss: 3.9373 (3.8884)  loss_scale: 65536.0000 (66895.6680)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [20]  [ 250/2809]  eta: 0:16:20  lr: 0.000028  min_lr: 0.000000  loss: 3.7778 (3.8806)  loss_scale: 65536.0000 (66841.4980)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
[2025-01-13 05:09:10,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:09:10,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:09:11,322] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56439
[2025-01-13 05:09:11,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:09:11,322] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [ 260/2809]  eta: 0:16:14  lr: 0.000028  min_lr: 0.000000  loss: 3.6175 (3.8690)  loss_scale: 65536.0000 (67293.6705)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [20]  [ 270/2809]  eta: 0:16:09  lr: 0.000028  min_lr: 0.000000  loss: 3.8682 (3.8711)  loss_scale: 65536.0000 (67228.8118)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
[2025-01-13 05:09:17,569] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56456
[2025-01-13 05:09:17,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:09:17,569] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 280/2809]  eta: 0:16:04  lr: 0.000028  min_lr: 0.000000  loss: 3.6990 (3.8575)  loss_scale: 65536.0000 (66585.5089)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [20]  [ 290/2809]  eta: 0:15:59  lr: 0.000028  min_lr: 0.000000  loss: 3.7030 (3.8573)  loss_scale: 32768.0000 (65423.3952)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [20]  [ 300/2809]  eta: 0:15:54  lr: 0.000028  min_lr: 0.000000  loss: 3.7815 (3.8481)  loss_scale: 32768.0000 (64338.4983)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [20]  [ 310/2809]  eta: 0:15:49  lr: 0.000028  min_lr: 0.000000  loss: 3.8910 (3.8526)  loss_scale: 32768.0000 (63323.3698)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [20]  [ 320/2809]  eta: 0:15:44  lr: 0.000028  min_lr: 0.000000  loss: 3.9100 (3.8490)  loss_scale: 32768.0000 (62371.4891)  weight_decay: 0.0500 (0.0500)  time: 0.3649  data: 0.0002  max mem: 15572
Epoch: [20]  [ 330/2809]  eta: 0:15:40  lr: 0.000028  min_lr: 0.000000  loss: 3.6916 (3.8466)  loss_scale: 32768.0000 (61477.1239)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [20]  [ 340/2809]  eta: 0:15:35  lr: 0.000028  min_lr: 0.000000  loss: 3.8372 (3.8471)  loss_scale: 32768.0000 (60635.2141)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [20]  [ 350/2809]  eta: 0:15:31  lr: 0.000028  min_lr: 0.000000  loss: 3.9284 (3.8437)  loss_scale: 32768.0000 (59841.2764)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [20]  [ 360/2809]  eta: 0:15:26  lr: 0.000028  min_lr: 0.000000  loss: 3.8319 (3.8408)  loss_scale: 32768.0000 (59091.3241)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [20]  [ 370/2809]  eta: 0:15:22  lr: 0.000028  min_lr: 0.000000  loss: 3.6764 (3.8354)  loss_scale: 32768.0000 (58381.8005)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [20]  [ 380/2809]  eta: 0:15:17  lr: 0.000028  min_lr: 0.000000  loss: 3.8506 (3.8369)  loss_scale: 32768.0000 (57709.5223)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [20]  [ 390/2809]  eta: 0:15:13  lr: 0.000028  min_lr: 0.000000  loss: 3.7884 (3.8299)  loss_scale: 32768.0000 (57071.6317)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [20]  [ 400/2809]  eta: 0:15:08  lr: 0.000028  min_lr: 0.000000  loss: 3.7027 (3.8302)  loss_scale: 32768.0000 (56465.5561)  weight_decay: 0.0500 (0.0500)  time: 0.3644  data: 0.0002  max mem: 15572
[2025-01-13 05:10:04,995] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:10:04,995] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 410/2809]  eta: 0:15:04  lr: 0.000028  min_lr: 0.000000  loss: 4.0502 (3.8330)  loss_scale: 32768.0000 (56367.3382)  weight_decay: 0.0500 (0.0500)  time: 0.3648  data: 0.0002  max mem: 15572
Epoch: [20]  [ 420/2809]  eta: 0:15:00  lr: 0.000028  min_lr: 0.000000  loss: 3.9527 (3.8355)  loss_scale: 65536.0000 (56585.1211)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [20]  [ 430/2809]  eta: 0:14:55  lr: 0.000028  min_lr: 0.000000  loss: 3.8236 (3.8326)  loss_scale: 65536.0000 (56792.7981)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [20]  [ 440/2809]  eta: 0:14:51  lr: 0.000028  min_lr: 0.000000  loss: 3.9766 (3.8388)  loss_scale: 65536.0000 (56991.0567)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [20]  [ 450/2809]  eta: 0:14:47  lr: 0.000028  min_lr: 0.000000  loss: 3.9806 (3.8366)  loss_scale: 65536.0000 (57180.5233)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [20]  [ 460/2809]  eta: 0:14:43  lr: 0.000028  min_lr: 0.000000  loss: 3.8686 (3.8387)  loss_scale: 65536.0000 (57361.7701)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [20]  [ 470/2809]  eta: 0:14:39  lr: 0.000028  min_lr: 0.000000  loss: 3.8343 (3.8364)  loss_scale: 65536.0000 (57535.3206)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [20]  [ 480/2809]  eta: 0:14:34  lr: 0.000028  min_lr: 0.000000  loss: 3.7016 (3.8363)  loss_scale: 65536.0000 (57701.6549)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [20]  [ 490/2809]  eta: 0:14:30  lr: 0.000028  min_lr: 0.000000  loss: 3.8800 (3.8371)  loss_scale: 65536.0000 (57861.2138)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [20]  [ 500/2809]  eta: 0:14:26  lr: 0.000028  min_lr: 0.000000  loss: 3.8993 (3.8376)  loss_scale: 65536.0000 (58014.4032)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [20]  [ 510/2809]  eta: 0:14:22  lr: 0.000028  min_lr: 0.000000  loss: 3.9754 (3.8356)  loss_scale: 65536.0000 (58161.5969)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [20]  [ 520/2809]  eta: 0:14:18  lr: 0.000028  min_lr: 0.000000  loss: 3.9754 (3.8367)  loss_scale: 65536.0000 (58303.1401)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [20]  [ 530/2809]  eta: 0:14:14  lr: 0.000028  min_lr: 0.000000  loss: 3.9659 (3.8328)  loss_scale: 65536.0000 (58439.3522)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
[2025-01-13 05:10:52,089] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:10:52,089] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:10:52,818] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56715
[2025-01-13 05:10:52,818] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:10:52,818] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [ 540/2809]  eta: 0:14:10  lr: 0.000028  min_lr: 0.000000  loss: 3.9748 (3.8343)  loss_scale: 65536.0000 (58812.8059)  weight_decay: 0.0500 (0.0500)  time: 0.3642  data: 0.0002  max mem: 15572
Epoch: [20]  [ 550/2809]  eta: 0:14:06  lr: 0.000028  min_lr: 0.000000  loss: 3.9979 (3.8315)  loss_scale: 65536.0000 (58934.8240)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [20]  [ 560/2809]  eta: 0:14:02  lr: 0.000028  min_lr: 0.000000  loss: 3.7757 (3.8320)  loss_scale: 65536.0000 (59052.4920)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [20]  [ 570/2809]  eta: 0:13:58  lr: 0.000028  min_lr: 0.000000  loss: 3.7992 (3.8320)  loss_scale: 65536.0000 (59166.0385)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [20]  [ 580/2809]  eta: 0:13:54  lr: 0.000028  min_lr: 0.000000  loss: 3.8601 (3.8329)  loss_scale: 65536.0000 (59275.6764)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [20]  [ 590/2809]  eta: 0:13:50  lr: 0.000028  min_lr: 0.000000  loss: 3.7814 (3.8323)  loss_scale: 65536.0000 (59381.6041)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [20]  [ 600/2809]  eta: 0:13:46  lr: 0.000028  min_lr: 0.000000  loss: 3.8699 (3.8333)  loss_scale: 65536.0000 (59484.0067)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [20]  [ 610/2809]  eta: 0:13:42  lr: 0.000028  min_lr: 0.000000  loss: 3.9545 (3.8335)  loss_scale: 65536.0000 (59583.0573)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [20]  [ 620/2809]  eta: 0:13:38  lr: 0.000028  min_lr: 0.000000  loss: 3.8730 (3.8321)  loss_scale: 65536.0000 (59678.9179)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [20]  [ 630/2809]  eta: 0:13:34  lr: 0.000028  min_lr: 0.000000  loss: 3.7963 (3.8339)  loss_scale: 65536.0000 (59771.7401)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [20]  [ 640/2809]  eta: 0:13:30  lr: 0.000028  min_lr: 0.000000  loss: 3.7963 (3.8312)  loss_scale: 65536.0000 (59861.6661)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [20]  [ 650/2809]  eta: 0:13:26  lr: 0.000028  min_lr: 0.000000  loss: 3.7627 (3.8319)  loss_scale: 65536.0000 (59948.8295)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [20]  [ 660/2809]  eta: 0:13:22  lr: 0.000028  min_lr: 0.000000  loss: 4.0575 (3.8367)  loss_scale: 65536.0000 (60033.3555)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
[2025-01-13 05:11:40,368] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:11:40,368] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:11:40,728] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56845
[2025-01-13 05:11:40,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:11:40,728] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [ 670/2809]  eta: 0:13:19  lr: 0.000028  min_lr: 0.000000  loss: 4.0959 (3.8400)  loss_scale: 65536.0000 (60213.0313)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [20]  [ 680/2809]  eta: 0:13:15  lr: 0.000028  min_lr: 0.000000  loss: 3.9978 (3.8384)  loss_scale: 65536.0000 (60291.1953)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [20]  [ 690/2809]  eta: 0:13:11  lr: 0.000028  min_lr: 0.000000  loss: 3.7902 (3.8377)  loss_scale: 65536.0000 (60367.0970)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [20]  [ 700/2809]  eta: 0:13:07  lr: 0.000028  min_lr: 0.000000  loss: 3.8616 (3.8388)  loss_scale: 65536.0000 (60440.8331)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [20]  [ 710/2809]  eta: 0:13:03  lr: 0.000028  min_lr: 0.000000  loss: 3.9087 (3.8385)  loss_scale: 65536.0000 (60512.4951)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [20]  [ 720/2809]  eta: 0:12:59  lr: 0.000028  min_lr: 0.000000  loss: 3.8775 (3.8377)  loss_scale: 65536.0000 (60582.1692)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [20]  [ 730/2809]  eta: 0:12:55  lr: 0.000028  min_lr: 0.000000  loss: 4.0444 (3.8386)  loss_scale: 65536.0000 (60649.9371)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
[2025-01-13 05:12:07,961] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56919
[2025-01-13 05:12:07,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:12:07,961] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 740/2809]  eta: 0:12:51  lr: 0.000028  min_lr: 0.000000  loss: 3.8687 (3.8392)  loss_scale: 65536.0000 (60627.4332)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [20]  [ 750/2809]  eta: 0:12:47  lr: 0.000028  min_lr: 0.000000  loss: 3.9014 (3.8407)  loss_scale: 32768.0000 (60256.4687)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [20]  [ 760/2809]  eta: 0:12:43  lr: 0.000028  min_lr: 0.000000  loss: 3.9322 (3.8391)  loss_scale: 32768.0000 (59895.2536)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [20]  [ 770/2809]  eta: 0:12:40  lr: 0.000028  min_lr: 0.000000  loss: 3.7889 (3.8369)  loss_scale: 32768.0000 (59543.4086)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [20]  [ 780/2809]  eta: 0:12:36  lr: 0.000028  min_lr: 0.000000  loss: 3.7642 (3.8370)  loss_scale: 32768.0000 (59200.5736)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [20]  [ 790/2809]  eta: 0:12:32  lr: 0.000028  min_lr: 0.000000  loss: 4.0559 (3.8386)  loss_scale: 32768.0000 (58866.4071)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [20]  [ 800/2809]  eta: 0:12:28  lr: 0.000028  min_lr: 0.000000  loss: 4.1641 (3.8422)  loss_scale: 32768.0000 (58540.5843)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [20]  [ 810/2809]  eta: 0:12:24  lr: 0.000028  min_lr: 0.000000  loss: 4.1165 (3.8421)  loss_scale: 32768.0000 (58222.7965)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
[2025-01-13 05:12:37,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=57000, skipped=389, lr=[2.7198628085713264e-07, 2.7198628085713264e-07, 3.885518297959038e-07, 3.885518297959038e-07, 5.550740425655769e-07, 5.550740425655769e-07, 7.929629179508242e-07, 7.929629179508242e-07, 1.1328041685011774e-06, 1.1328041685011774e-06, 1.6182916692873964e-06, 1.6182916692873964e-06, 2.311845241839138e-06, 2.311845241839138e-06, 3.3026360597701973e-06, 3.3026360597701973e-06, 4.718051513957425e-06, 4.718051513957425e-06, 6.740073591367751e-06, 6.740073591367751e-06, 9.628676559096786e-06, 9.628676559096786e-06, 1.3755252227281124e-05, 1.3755252227281124e-05, 1.9650360324687324e-05, 1.9650360324687324e-05, 2.807194332098189e-05, 2.807194332098189e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 05:12:37,316] [INFO] [timer.py:260:stop] epoch=0/micro_step=57000/global_step=57000, RunningAvgSamplesPerSec=29.629259056454615, CurrSamplesPerSec=35.174671152402745, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [20]  [ 820/2809]  eta: 0:12:20  lr: 0.000028  min_lr: 0.000000  loss: 3.6469 (3.8415)  loss_scale: 32768.0000 (57912.7503)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [20]  [ 830/2809]  eta: 0:12:16  lr: 0.000028  min_lr: 0.000000  loss: 4.1223 (3.8427)  loss_scale: 32768.0000 (57610.1661)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [20]  [ 840/2809]  eta: 0:12:13  lr: 0.000028  min_lr: 0.000000  loss: 3.8306 (3.8419)  loss_scale: 32768.0000 (57314.7776)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [20]  [ 850/2809]  eta: 0:12:09  lr: 0.000028  min_lr: 0.000000  loss: 3.6151 (3.8380)  loss_scale: 32768.0000 (57026.3314)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [20]  [ 860/2809]  eta: 0:12:05  lr: 0.000028  min_lr: 0.000000  loss: 3.4802 (3.8368)  loss_scale: 32768.0000 (56744.5854)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
[2025-01-13 05:12:55,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:12:55,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 870/2809]  eta: 0:12:01  lr: 0.000028  min_lr: 0.000000  loss: 3.9280 (3.8360)  loss_scale: 32768.0000 (56582.1722)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [20]  [ 880/2809]  eta: 0:11:57  lr: 0.000028  min_lr: 0.000000  loss: 4.0641 (3.8372)  loss_scale: 65536.0000 (56683.8048)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [20]  [ 890/2809]  eta: 0:11:54  lr: 0.000028  min_lr: 0.000000  loss: 3.9710 (3.8352)  loss_scale: 65536.0000 (56783.1560)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [20]  [ 900/2809]  eta: 0:11:50  lr: 0.000028  min_lr: 0.000000  loss: 3.7148 (3.8334)  loss_scale: 65536.0000 (56880.3019)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [20]  [ 910/2809]  eta: 0:11:46  lr: 0.000028  min_lr: 0.000000  loss: 3.7148 (3.8342)  loss_scale: 65536.0000 (56975.3150)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [20]  [ 920/2809]  eta: 0:11:42  lr: 0.000028  min_lr: 0.000000  loss: 3.9445 (3.8359)  loss_scale: 65536.0000 (57068.2649)  weight_decay: 0.0500 (0.0500)  time: 0.3650  data: 0.0002  max mem: 15572
Epoch: [20]  [ 930/2809]  eta: 0:11:38  lr: 0.000028  min_lr: 0.000000  loss: 3.8853 (3.8363)  loss_scale: 65536.0000 (57159.2180)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [20]  [ 940/2809]  eta: 0:11:34  lr: 0.000028  min_lr: 0.000000  loss: 3.8010 (3.8362)  loss_scale: 65536.0000 (57248.2380)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [20]  [ 950/2809]  eta: 0:11:31  lr: 0.000028  min_lr: 0.000000  loss: 3.6433 (3.8367)  loss_scale: 65536.0000 (57335.3859)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [20]  [ 960/2809]  eta: 0:11:27  lr: 0.000028  min_lr: 0.000000  loss: 3.7520 (3.8371)  loss_scale: 65536.0000 (57420.7201)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [20]  [ 970/2809]  eta: 0:11:23  lr: 0.000028  min_lr: 0.000000  loss: 3.8205 (3.8379)  loss_scale: 65536.0000 (57504.2966)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [20]  [ 980/2809]  eta: 0:11:19  lr: 0.000028  min_lr: 0.000000  loss: 3.8086 (3.8358)  loss_scale: 65536.0000 (57586.1692)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [20]  [ 990/2809]  eta: 0:11:16  lr: 0.000028  min_lr: 0.000000  loss: 3.5809 (3.8355)  loss_scale: 65536.0000 (57666.3895)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 05:13:42,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:13:42,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:13:43,673] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57179
[2025-01-13 05:13:43,673] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:13:43,673] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1000/2809]  eta: 0:11:12  lr: 0.000028  min_lr: 0.000000  loss: 3.5986 (3.8350)  loss_scale: 65536.0000 (57941.4186)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [20]  [1010/2809]  eta: 0:11:08  lr: 0.000028  min_lr: 0.000000  loss: 3.6126 (3.8345)  loss_scale: 65536.0000 (58016.5381)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [20]  [1020/2809]  eta: 0:11:04  lr: 0.000028  min_lr: 0.000000  loss: 3.7979 (3.8357)  loss_scale: 65536.0000 (58090.1861)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [20]  [1030/2809]  eta: 0:11:01  lr: 0.000028  min_lr: 0.000000  loss: 3.7979 (3.8348)  loss_scale: 65536.0000 (58162.4054)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [20]  [1040/2809]  eta: 0:10:57  lr: 0.000028  min_lr: 0.000000  loss: 3.7434 (3.8346)  loss_scale: 65536.0000 (58233.2373)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [20]  [1050/2809]  eta: 0:10:53  lr: 0.000028  min_lr: 0.000000  loss: 3.7434 (3.8336)  loss_scale: 65536.0000 (58302.7212)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [20]  [1060/2809]  eta: 0:10:49  lr: 0.000028  min_lr: 0.000000  loss: 3.5147 (3.8299)  loss_scale: 65536.0000 (58370.8954)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [20]  [1070/2809]  eta: 0:10:46  lr: 0.000028  min_lr: 0.000000  loss: 3.6584 (3.8293)  loss_scale: 65536.0000 (58437.7965)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [20]  [1080/2809]  eta: 0:10:42  lr: 0.000028  min_lr: 0.000000  loss: 3.9862 (3.8316)  loss_scale: 65536.0000 (58503.4598)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [20]  [1090/2809]  eta: 0:10:38  lr: 0.000028  min_lr: 0.000000  loss: 3.9611 (3.8308)  loss_scale: 65536.0000 (58567.9193)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [20]  [1100/2809]  eta: 0:10:34  lr: 0.000028  min_lr: 0.000000  loss: 3.7161 (3.8296)  loss_scale: 65536.0000 (58631.2080)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [20]  [1110/2809]  eta: 0:10:30  lr: 0.000028  min_lr: 0.000000  loss: 3.7318 (3.8291)  loss_scale: 65536.0000 (58693.3573)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [20]  [1120/2809]  eta: 0:10:27  lr: 0.000028  min_lr: 0.000000  loss: 3.7733 (3.8291)  loss_scale: 65536.0000 (58754.3979)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
[2025-01-13 05:14:31,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:14:31,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [1130/2809]  eta: 0:10:23  lr: 0.000028  min_lr: 0.000000  loss: 3.7733 (3.8294)  loss_scale: 65536.0000 (58988.1945)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
[2025-01-13 05:14:32,647] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57312
[2025-01-13 05:14:32,647] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:14:32,649] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1140/2809]  eta: 0:10:19  lr: 0.000028  min_lr: 0.000000  loss: 3.9053 (3.8313)  loss_scale: 65536.0000 (59103.0184)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [20]  [1150/2809]  eta: 0:10:15  lr: 0.000028  min_lr: 0.000000  loss: 3.9940 (3.8307)  loss_scale: 65536.0000 (59158.9088)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [20]  [1160/2809]  eta: 0:10:12  lr: 0.000028  min_lr: 0.000000  loss: 4.0274 (3.8329)  loss_scale: 65536.0000 (59213.8363)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [20]  [1170/2809]  eta: 0:10:08  lr: 0.000028  min_lr: 0.000000  loss: 3.9544 (3.8328)  loss_scale: 65536.0000 (59267.8258)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [20]  [1180/2809]  eta: 0:10:04  lr: 0.000028  min_lr: 0.000000  loss: 3.6416 (3.8310)  loss_scale: 65536.0000 (59320.9009)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [20]  [1190/2809]  eta: 0:10:00  lr: 0.000028  min_lr: 0.000000  loss: 3.5778 (3.8301)  loss_scale: 65536.0000 (59373.0848)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [20]  [1200/2809]  eta: 0:09:57  lr: 0.000028  min_lr: 0.000000  loss: 3.9248 (3.8303)  loss_scale: 65536.0000 (59424.3997)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [20]  [1210/2809]  eta: 0:09:53  lr: 0.000028  min_lr: 0.000000  loss: 3.9517 (3.8306)  loss_scale: 65536.0000 (59474.8671)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [20]  [1220/2809]  eta: 0:09:49  lr: 0.000028  min_lr: 0.000000  loss: 3.6899 (3.8282)  loss_scale: 65536.0000 (59524.5078)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [20]  [1230/2809]  eta: 0:09:45  lr: 0.000028  min_lr: 0.000000  loss: 3.6945 (3.8285)  loss_scale: 65536.0000 (59573.3420)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0001  max mem: 15572
Epoch: [20]  [1240/2809]  eta: 0:09:42  lr: 0.000028  min_lr: 0.000000  loss: 3.9020 (3.8286)  loss_scale: 65536.0000 (59621.3892)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0001  max mem: 15572
Epoch: [20]  [1250/2809]  eta: 0:09:38  lr: 0.000028  min_lr: 0.000000  loss: 3.8041 (3.8286)  loss_scale: 65536.0000 (59668.6683)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [20]  [1260/2809]  eta: 0:09:34  lr: 0.000028  min_lr: 0.000000  loss: 3.6856 (3.8277)  loss_scale: 65536.0000 (59715.1975)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
[2025-01-13 05:15:20,190] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:15:20,190] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:15:20,555] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57442
[2025-01-13 05:15:20,555] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:15:20,555] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1270/2809]  eta: 0:09:30  lr: 0.000028  min_lr: 0.000000  loss: 3.9214 (3.8288)  loss_scale: 65536.0000 (59812.5570)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [20]  [1280/2809]  eta: 0:09:27  lr: 0.000028  min_lr: 0.000000  loss: 3.8007 (3.8275)  loss_scale: 65536.0000 (59857.2365)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [20]  [1290/2809]  eta: 0:09:23  lr: 0.000028  min_lr: 0.000000  loss: 3.4834 (3.8253)  loss_scale: 65536.0000 (59901.2239)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [20]  [1300/2809]  eta: 0:09:19  lr: 0.000028  min_lr: 0.000000  loss: 3.5715 (3.8249)  loss_scale: 65536.0000 (59944.5350)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [20]  [1310/2809]  eta: 0:09:16  lr: 0.000028  min_lr: 0.000000  loss: 3.8560 (3.8242)  loss_scale: 65536.0000 (59987.1854)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [20]  [1320/2809]  eta: 0:09:12  lr: 0.000028  min_lr: 0.000000  loss: 3.7227 (3.8231)  loss_scale: 65536.0000 (60029.1900)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [20]  [1330/2809]  eta: 0:09:08  lr: 0.000028  min_lr: 0.000000  loss: 3.8301 (3.8238)  loss_scale: 65536.0000 (60070.5635)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [20]  [1340/2809]  eta: 0:09:04  lr: 0.000028  min_lr: 0.000000  loss: 3.9882 (3.8252)  loss_scale: 65536.0000 (60111.3199)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [20]  [1350/2809]  eta: 0:09:01  lr: 0.000028  min_lr: 0.000000  loss: 3.9478 (3.8248)  loss_scale: 65536.0000 (60151.4730)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0001  max mem: 15572
Epoch: [20]  [1360/2809]  eta: 0:08:57  lr: 0.000028  min_lr: 0.000000  loss: 3.8076 (3.8250)  loss_scale: 65536.0000 (60191.0360)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [20]  [1370/2809]  eta: 0:08:53  lr: 0.000028  min_lr: 0.000000  loss: 3.8249 (3.8241)  loss_scale: 65536.0000 (60230.0219)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [20]  [1380/2809]  eta: 0:08:49  lr: 0.000028  min_lr: 0.000000  loss: 4.0021 (3.8253)  loss_scale: 65536.0000 (60268.4432)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [20]  [1390/2809]  eta: 0:08:46  lr: 0.000028  min_lr: 0.000000  loss: 4.0021 (3.8255)  loss_scale: 65536.0000 (60306.3120)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
[2025-01-13 05:16:08,120] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:16:08,120] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:16:08,843] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57573
[2025-01-13 05:16:08,843] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:16:08,843] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1400/2809]  eta: 0:08:42  lr: 0.000028  min_lr: 0.000000  loss: 3.8085 (3.8253)  loss_scale: 65536.0000 (60437.1963)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [20]  [1410/2809]  eta: 0:08:38  lr: 0.000028  min_lr: 0.000000  loss: 3.9353 (3.8268)  loss_scale: 65536.0000 (60473.3324)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [20]  [1420/2809]  eta: 0:08:34  lr: 0.000028  min_lr: 0.000000  loss: 4.0639 (3.8281)  loss_scale: 65536.0000 (60508.9599)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [20]  [1430/2809]  eta: 0:08:31  lr: 0.000028  min_lr: 0.000000  loss: 3.8560 (3.8274)  loss_scale: 65536.0000 (60544.0894)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0001  max mem: 15572
Epoch: [20]  [1440/2809]  eta: 0:08:27  lr: 0.000028  min_lr: 0.000000  loss: 3.7257 (3.8278)  loss_scale: 65536.0000 (60578.7314)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [20]  [1450/2809]  eta: 0:08:23  lr: 0.000028  min_lr: 0.000000  loss: 3.7802 (3.8279)  loss_scale: 65536.0000 (60612.8959)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [20]  [1460/2809]  eta: 0:08:19  lr: 0.000028  min_lr: 0.000000  loss: 3.8441 (3.8281)  loss_scale: 65536.0000 (60646.5927)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [20]  [1470/2809]  eta: 0:08:16  lr: 0.000028  min_lr: 0.000000  loss: 3.8441 (3.8284)  loss_scale: 65536.0000 (60679.8314)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [20]  [1480/2809]  eta: 0:08:12  lr: 0.000028  min_lr: 0.000000  loss: 3.8813 (3.8281)  loss_scale: 65536.0000 (60712.6212)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [20]  [1490/2809]  eta: 0:08:08  lr: 0.000028  min_lr: 0.000000  loss: 3.7700 (3.8285)  loss_scale: 65536.0000 (60744.9712)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [20]  [1500/2809]  eta: 0:08:05  lr: 0.000028  min_lr: 0.000000  loss: 3.7597 (3.8292)  loss_scale: 65536.0000 (60776.8901)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [20]  [1510/2809]  eta: 0:08:01  lr: 0.000028  min_lr: 0.000000  loss: 4.0206 (3.8306)  loss_scale: 65536.0000 (60808.3865)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [20]  [1520/2809]  eta: 0:07:57  lr: 0.000028  min_lr: 0.000000  loss: 4.0202 (3.8310)  loss_scale: 65536.0000 (60839.4688)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
[2025-01-13 05:16:56,377] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:16:56,377] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:16:56,759] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57703
[2025-01-13 05:16:56,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:16:56,760] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1530/2809]  eta: 0:07:53  lr: 0.000028  min_lr: 0.000000  loss: 3.6141 (3.8287)  loss_scale: 65536.0000 (60912.9510)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0003  max mem: 15572
Epoch: [20]  [1540/2809]  eta: 0:07:50  lr: 0.000028  min_lr: 0.000000  loss: 3.6141 (3.8277)  loss_scale: 65536.0000 (60942.9513)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
Epoch: [20]  [1550/2809]  eta: 0:07:46  lr: 0.000028  min_lr: 0.000000  loss: 3.8563 (3.8274)  loss_scale: 65536.0000 (60972.5648)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [20]  [1560/2809]  eta: 0:07:42  lr: 0.000028  min_lr: 0.000000  loss: 3.7974 (3.8256)  loss_scale: 65536.0000 (61001.7988)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [20]  [1570/2809]  eta: 0:07:39  lr: 0.000028  min_lr: 0.000000  loss: 3.7438 (3.8256)  loss_scale: 65536.0000 (61030.6607)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [20]  [1580/2809]  eta: 0:07:35  lr: 0.000028  min_lr: 0.000000  loss: 3.8263 (3.8251)  loss_scale: 65536.0000 (61059.1575)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [20]  [1590/2809]  eta: 0:07:31  lr: 0.000028  min_lr: 0.000000  loss: 3.7633 (3.8259)  loss_scale: 65536.0000 (61087.2960)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [20]  [1600/2809]  eta: 0:07:28  lr: 0.000027  min_lr: 0.000000  loss: 3.7005 (3.8249)  loss_scale: 65536.0000 (61115.0831)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [20]  [1610/2809]  eta: 0:07:24  lr: 0.000027  min_lr: 0.000000  loss: 3.8199 (3.8265)  loss_scale: 65536.0000 (61142.5251)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [20]  [1620/2809]  eta: 0:07:20  lr: 0.000027  min_lr: 0.000000  loss: 3.9127 (3.8269)  loss_scale: 65536.0000 (61169.6286)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [20]  [1630/2809]  eta: 0:07:16  lr: 0.000027  min_lr: 0.000000  loss: 3.9093 (3.8267)  loss_scale: 65536.0000 (61196.3998)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
[2025-01-13 05:17:37,094] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57812
[2025-01-13 05:17:37,094] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:17:37,094] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1640/2809]  eta: 0:07:13  lr: 0.000027  min_lr: 0.000000  loss: 3.8475 (3.8258)  loss_scale: 65536.0000 (61043.1298)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [20]  [1650/2809]  eta: 0:07:09  lr: 0.000027  min_lr: 0.000000  loss: 3.6488 (3.8257)  loss_scale: 32768.0000 (60871.8692)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [20]  [1660/2809]  eta: 0:07:05  lr: 0.000027  min_lr: 0.000000  loss: 3.7285 (3.8250)  loss_scale: 32768.0000 (60702.6707)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [20]  [1670/2809]  eta: 0:07:01  lr: 0.000027  min_lr: 0.000000  loss: 3.7285 (3.8256)  loss_scale: 32768.0000 (60535.4973)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [20]  [1680/2809]  eta: 0:06:58  lr: 0.000027  min_lr: 0.000000  loss: 3.6045 (3.8244)  loss_scale: 32768.0000 (60370.3129)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [20]  [1690/2809]  eta: 0:06:54  lr: 0.000027  min_lr: 0.000000  loss: 3.4474 (3.8229)  loss_scale: 32768.0000 (60207.0822)  weight_decay: 0.0500 (0.0500)  time: 0.3653  data: 0.0002  max mem: 15572
Epoch: [20]  [1700/2809]  eta: 0:06:50  lr: 0.000027  min_lr: 0.000000  loss: 3.4647 (3.8206)  loss_scale: 32768.0000 (60045.7707)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [20]  [1710/2809]  eta: 0:06:47  lr: 0.000027  min_lr: 0.000000  loss: 3.6944 (3.8204)  loss_scale: 32768.0000 (59886.3448)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [20]  [1720/2809]  eta: 0:06:43  lr: 0.000027  min_lr: 0.000000  loss: 3.8180 (3.8198)  loss_scale: 32768.0000 (59728.7716)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [20]  [1730/2809]  eta: 0:06:39  lr: 0.000027  min_lr: 0.000000  loss: 3.8463 (3.8196)  loss_scale: 32768.0000 (59573.0191)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [20]  [1740/2809]  eta: 0:06:35  lr: 0.000027  min_lr: 0.000000  loss: 3.8252 (3.8194)  loss_scale: 32768.0000 (59419.0557)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [20]  [1750/2809]  eta: 0:06:32  lr: 0.000027  min_lr: 0.000000  loss: 3.8252 (3.8188)  loss_scale: 32768.0000 (59266.8509)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [20]  [1760/2809]  eta: 0:06:28  lr: 0.000027  min_lr: 0.000000  loss: 3.7436 (3.8186)  loss_scale: 32768.0000 (59116.3748)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
[2025-01-13 05:18:24,571] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:18:24,571] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1770/2809]  eta: 0:06:24  lr: 0.000027  min_lr: 0.000000  loss: 3.9510 (3.8195)  loss_scale: 32768.0000 (59152.6234)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [20]  [1780/2809]  eta: 0:06:21  lr: 0.000027  min_lr: 0.000000  loss: 4.0473 (3.8203)  loss_scale: 65536.0000 (59188.4649)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [20]  [1790/2809]  eta: 0:06:17  lr: 0.000027  min_lr: 0.000000  loss: 4.0820 (3.8217)  loss_scale: 65536.0000 (59223.9062)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [20]  [1800/2809]  eta: 0:06:13  lr: 0.000027  min_lr: 0.000000  loss: 4.1376 (3.8223)  loss_scale: 65536.0000 (59258.9539)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [20]  [1810/2809]  eta: 0:06:09  lr: 0.000027  min_lr: 0.000000  loss: 3.9218 (3.8217)  loss_scale: 65536.0000 (59293.6146)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
[2025-01-13 05:18:45,990] [INFO] [logging.py:96:log_dist] [Rank 0] step=58000, skipped=395, lr=[2.648575876106438e-07, 2.648575876106438e-07, 3.783679823009198e-07, 3.783679823009198e-07, 5.40525689001314e-07, 5.40525689001314e-07, 7.721795557161629e-07, 7.721795557161629e-07, 1.1031136510230898e-06, 1.1031136510230898e-06, 1.5758766443187e-06, 1.5758766443187e-06, 2.2512523490267144e-06, 2.2512523490267144e-06, 3.216074784323878e-06, 3.216074784323878e-06, 4.594392549034111e-06, 4.594392549034111e-06, 6.563417927191589e-06, 6.563417927191589e-06, 9.376311324559412e-06, 9.376311324559412e-06, 1.3394730463656304e-05, 1.3394730463656304e-05, 1.9135329233794723e-05, 1.9135329233794723e-05, 2.7336184619706747e-05, 2.7336184619706747e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 05:18:45,990] [INFO] [timer.py:260:stop] epoch=0/micro_step=58000/global_step=58000, RunningAvgSamplesPerSec=29.700686879156365, CurrSamplesPerSec=34.947776552635894, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [20]  [1820/2809]  eta: 0:06:06  lr: 0.000027  min_lr: 0.000000  loss: 3.8252 (3.8205)  loss_scale: 65536.0000 (59327.8946)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [20]  [1830/2809]  eta: 0:06:02  lr: 0.000027  min_lr: 0.000000  loss: 3.8616 (3.8209)  loss_scale: 65536.0000 (59361.8001)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [20]  [1840/2809]  eta: 0:05:58  lr: 0.000027  min_lr: 0.000000  loss: 3.8756 (3.8212)  loss_scale: 65536.0000 (59395.3373)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [20]  [1850/2809]  eta: 0:05:55  lr: 0.000027  min_lr: 0.000000  loss: 3.7745 (3.8206)  loss_scale: 65536.0000 (59428.5122)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [20]  [1860/2809]  eta: 0:05:51  lr: 0.000027  min_lr: 0.000000  loss: 3.7786 (3.8213)  loss_scale: 65536.0000 (59461.3305)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [20]  [1870/2809]  eta: 0:05:47  lr: 0.000027  min_lr: 0.000000  loss: 3.9079 (3.8223)  loss_scale: 65536.0000 (59493.7980)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [20]  [1880/2809]  eta: 0:05:43  lr: 0.000027  min_lr: 0.000000  loss: 3.8811 (3.8225)  loss_scale: 65536.0000 (59525.9203)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
[2025-01-13 05:19:11,773] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:19:11,773] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [1890/2809]  eta: 0:05:40  lr: 0.000027  min_lr: 0.000000  loss: 3.6971 (3.8233)  loss_scale: 65536.0000 (59627.0164)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
[2025-01-13 05:19:15,817] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58080
[2025-01-13 05:19:15,817] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:19:15,817] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1900/2809]  eta: 0:05:36  lr: 0.000027  min_lr: 0.000000  loss: 3.7609 (3.8223)  loss_scale: 131072.0000 (59968.3703)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [20]  [1910/2809]  eta: 0:05:32  lr: 0.000027  min_lr: 0.000000  loss: 3.7023 (3.8217)  loss_scale: 65536.0000 (59997.5050)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [20]  [1920/2809]  eta: 0:05:29  lr: 0.000027  min_lr: 0.000000  loss: 4.0474 (3.8233)  loss_scale: 65536.0000 (60026.3363)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [20]  [1930/2809]  eta: 0:05:25  lr: 0.000027  min_lr: 0.000000  loss: 4.1466 (3.8247)  loss_scale: 65536.0000 (60054.8690)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [20]  [1940/2809]  eta: 0:05:21  lr: 0.000027  min_lr: 0.000000  loss: 4.0967 (3.8250)  loss_scale: 65536.0000 (60083.1077)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [20]  [1950/2809]  eta: 0:05:18  lr: 0.000027  min_lr: 0.000000  loss: 3.9280 (3.8257)  loss_scale: 65536.0000 (60111.0569)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [20]  [1960/2809]  eta: 0:05:14  lr: 0.000027  min_lr: 0.000000  loss: 3.7904 (3.8250)  loss_scale: 65536.0000 (60138.7211)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [20]  [1970/2809]  eta: 0:05:10  lr: 0.000027  min_lr: 0.000000  loss: 3.4898 (3.8241)  loss_scale: 65536.0000 (60166.1045)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [20]  [1980/2809]  eta: 0:05:06  lr: 0.000027  min_lr: 0.000000  loss: 3.7882 (3.8245)  loss_scale: 65536.0000 (60193.2115)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [20]  [1990/2809]  eta: 0:05:03  lr: 0.000027  min_lr: 0.000000  loss: 3.8431 (3.8244)  loss_scale: 65536.0000 (60220.0462)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [20]  [2000/2809]  eta: 0:04:59  lr: 0.000027  min_lr: 0.000000  loss: 3.7744 (3.8235)  loss_scale: 65536.0000 (60246.6127)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [20]  [2010/2809]  eta: 0:04:55  lr: 0.000027  min_lr: 0.000000  loss: 3.8132 (3.8244)  loss_scale: 65536.0000 (60272.9150)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [20]  [2020/2809]  eta: 0:04:52  lr: 0.000027  min_lr: 0.000000  loss: 4.1196 (3.8251)  loss_scale: 65536.0000 (60298.9570)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
[2025-01-13 05:20:03,453] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:20:03,453] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [2030/2809]  eta: 0:04:48  lr: 0.000027  min_lr: 0.000000  loss: 3.8133 (3.8246)  loss_scale: 65536.0000 (60389.2782)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
[2025-01-13 05:20:04,559] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58212
[2025-01-13 05:20:04,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:20:04,559] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [2040/2809]  eta: 0:04:44  lr: 0.000027  min_lr: 0.000000  loss: 3.6044 (3.8240)  loss_scale: 65536.0000 (60446.6046)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [20]  [2050/2809]  eta: 0:04:40  lr: 0.000027  min_lr: 0.000000  loss: 3.7650 (3.8238)  loss_scale: 65536.0000 (60471.4188)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [20]  [2060/2809]  eta: 0:04:37  lr: 0.000027  min_lr: 0.000000  loss: 3.8635 (3.8237)  loss_scale: 65536.0000 (60495.9922)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [20]  [2070/2809]  eta: 0:04:33  lr: 0.000027  min_lr: 0.000000  loss: 3.7235 (3.8228)  loss_scale: 65536.0000 (60520.3283)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [20]  [2080/2809]  eta: 0:04:29  lr: 0.000027  min_lr: 0.000000  loss: 3.7516 (3.8227)  loss_scale: 65536.0000 (60544.4306)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [20]  [2090/2809]  eta: 0:04:26  lr: 0.000027  min_lr: 0.000000  loss: 3.7812 (3.8222)  loss_scale: 65536.0000 (60568.3022)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [20]  [2100/2809]  eta: 0:04:22  lr: 0.000027  min_lr: 0.000000  loss: 3.8692 (3.8229)  loss_scale: 65536.0000 (60591.9467)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [20]  [2110/2809]  eta: 0:04:18  lr: 0.000027  min_lr: 0.000000  loss: 3.6537 (3.8210)  loss_scale: 65536.0000 (60615.3671)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [20]  [2120/2809]  eta: 0:04:14  lr: 0.000027  min_lr: 0.000000  loss: 3.6537 (3.8218)  loss_scale: 65536.0000 (60638.5667)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [20]  [2130/2809]  eta: 0:04:11  lr: 0.000027  min_lr: 0.000000  loss: 3.9263 (3.8219)  loss_scale: 65536.0000 (60661.5486)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [20]  [2140/2809]  eta: 0:04:07  lr: 0.000027  min_lr: 0.000000  loss: 3.9263 (3.8229)  loss_scale: 65536.0000 (60684.3157)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [20]  [2150/2809]  eta: 0:04:03  lr: 0.000027  min_lr: 0.000000  loss: 3.9009 (3.8226)  loss_scale: 65536.0000 (60706.8712)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [20]  [2160/2809]  eta: 0:04:00  lr: 0.000027  min_lr: 0.000000  loss: 3.7070 (3.8221)  loss_scale: 65536.0000 (60729.2180)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
[2025-01-13 05:20:52,195] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:20:52,195] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:20:52,920] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58343
[2025-01-13 05:20:52,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:20:52,920] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [2170/2809]  eta: 0:03:56  lr: 0.000027  min_lr: 0.000000  loss: 3.8042 (3.8223)  loss_scale: 65536.0000 (60811.7328)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [20]  [2180/2809]  eta: 0:03:52  lr: 0.000027  min_lr: 0.000000  loss: 3.8015 (3.8211)  loss_scale: 65536.0000 (60833.3939)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [20]  [2190/2809]  eta: 0:03:49  lr: 0.000027  min_lr: 0.000000  loss: 3.7015 (3.8210)  loss_scale: 65536.0000 (60854.8571)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [20]  [2200/2809]  eta: 0:03:45  lr: 0.000027  min_lr: 0.000000  loss: 3.7488 (3.8209)  loss_scale: 65536.0000 (60876.1254)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [20]  [2210/2809]  eta: 0:03:41  lr: 0.000027  min_lr: 0.000000  loss: 3.8001 (3.8206)  loss_scale: 65536.0000 (60897.2013)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [20]  [2220/2809]  eta: 0:03:37  lr: 0.000027  min_lr: 0.000000  loss: 3.8123 (3.8199)  loss_scale: 65536.0000 (60918.0873)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [20]  [2230/2809]  eta: 0:03:34  lr: 0.000027  min_lr: 0.000000  loss: 3.8123 (3.8193)  loss_scale: 65536.0000 (60938.7862)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [20]  [2240/2809]  eta: 0:03:30  lr: 0.000027  min_lr: 0.000000  loss: 3.7171 (3.8188)  loss_scale: 65536.0000 (60959.3003)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [20]  [2250/2809]  eta: 0:03:26  lr: 0.000027  min_lr: 0.000000  loss: 3.8231 (3.8191)  loss_scale: 65536.0000 (60979.6322)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [20]  [2260/2809]  eta: 0:03:23  lr: 0.000027  min_lr: 0.000000  loss: 3.9093 (3.8190)  loss_scale: 65536.0000 (60999.7842)  weight_decay: 0.0500 (0.0500)  time: 0.3643  data: 0.0002  max mem: 15572
Epoch: [20]  [2270/2809]  eta: 0:03:19  lr: 0.000027  min_lr: 0.000000  loss: 3.7985 (3.8188)  loss_scale: 65536.0000 (61019.7587)  weight_decay: 0.0500 (0.0500)  time: 0.3648  data: 0.0002  max mem: 15572
Epoch: [20]  [2280/2809]  eta: 0:03:15  lr: 0.000027  min_lr: 0.000000  loss: 3.8424 (3.8187)  loss_scale: 65536.0000 (61039.5581)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [20]  [2290/2809]  eta: 0:03:12  lr: 0.000027  min_lr: 0.000000  loss: 3.8783 (3.8192)  loss_scale: 65536.0000 (61059.1846)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
[2025-01-13 05:21:40,571] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:21:40,571] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:21:40,945] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58473
[2025-01-13 05:21:40,945] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:21:40,945] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [2300/2809]  eta: 0:03:08  lr: 0.000027  min_lr: 0.000000  loss: 3.7497 (3.8182)  loss_scale: 65536.0000 (61107.1221)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [20]  [2310/2809]  eta: 0:03:04  lr: 0.000027  min_lr: 0.000000  loss: 3.7497 (3.8190)  loss_scale: 65536.0000 (61126.2865)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [20]  [2320/2809]  eta: 0:03:00  lr: 0.000027  min_lr: 0.000000  loss: 3.8706 (3.8186)  loss_scale: 65536.0000 (61145.2857)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [20]  [2330/2809]  eta: 0:02:57  lr: 0.000027  min_lr: 0.000000  loss: 3.8084 (3.8187)  loss_scale: 65536.0000 (61164.1218)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [20]  [2340/2809]  eta: 0:02:53  lr: 0.000027  min_lr: 0.000000  loss: 3.6651 (3.8173)  loss_scale: 65536.0000 (61182.7971)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [20]  [2350/2809]  eta: 0:02:49  lr: 0.000027  min_lr: 0.000000  loss: 3.5239 (3.8166)  loss_scale: 65536.0000 (61201.3135)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [20]  [2360/2809]  eta: 0:02:46  lr: 0.000027  min_lr: 0.000000  loss: 3.7406 (3.8163)  loss_scale: 65536.0000 (61219.6730)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [20]  [2370/2809]  eta: 0:02:42  lr: 0.000027  min_lr: 0.000000  loss: 3.8459 (3.8163)  loss_scale: 65536.0000 (61237.8777)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [20]  [2380/2809]  eta: 0:02:38  lr: 0.000027  min_lr: 0.000000  loss: 3.8103 (3.8162)  loss_scale: 65536.0000 (61255.9294)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [20]  [2390/2809]  eta: 0:02:35  lr: 0.000027  min_lr: 0.000000  loss: 3.7581 (3.8162)  loss_scale: 65536.0000 (61273.8302)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [20]  [2400/2809]  eta: 0:02:31  lr: 0.000027  min_lr: 0.000000  loss: 3.7840 (3.8162)  loss_scale: 65536.0000 (61291.5818)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [20]  [2410/2809]  eta: 0:02:27  lr: 0.000027  min_lr: 0.000000  loss: 3.7840 (3.8156)  loss_scale: 65536.0000 (61309.1862)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [20]  [2420/2809]  eta: 0:02:23  lr: 0.000027  min_lr: 0.000000  loss: 3.7635 (3.8150)  loss_scale: 65536.0000 (61326.6452)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
[2025-01-13 05:22:28,719] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:22:28,719] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:22:30,589] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58607
[2025-01-13 05:22:30,589] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:22:30,589] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [2430/2809]  eta: 0:02:20  lr: 0.000027  min_lr: 0.000000  loss: 3.7028 (3.8150)  loss_scale: 65536.0000 (61478.7528)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [20]  [2440/2809]  eta: 0:02:16  lr: 0.000027  min_lr: 0.000000  loss: 3.7060 (3.8151)  loss_scale: 65536.0000 (61495.3740)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [20]  [2450/2809]  eta: 0:02:12  lr: 0.000027  min_lr: 0.000000  loss: 3.6832 (3.8145)  loss_scale: 65536.0000 (61511.8596)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [20]  [2460/2809]  eta: 0:02:09  lr: 0.000027  min_lr: 0.000000  loss: 3.6832 (3.8141)  loss_scale: 65536.0000 (61528.2113)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [20]  [2470/2809]  eta: 0:02:05  lr: 0.000027  min_lr: 0.000000  loss: 3.8046 (3.8140)  loss_scale: 65536.0000 (61544.4306)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [20]  [2480/2809]  eta: 0:02:01  lr: 0.000027  min_lr: 0.000000  loss: 3.8181 (3.8144)  loss_scale: 65536.0000 (61560.5191)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [20]  [2490/2809]  eta: 0:01:58  lr: 0.000027  min_lr: 0.000000  loss: 4.0650 (3.8148)  loss_scale: 65536.0000 (61576.4785)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [20]  [2500/2809]  eta: 0:01:54  lr: 0.000027  min_lr: 0.000000  loss: 3.9837 (3.8156)  loss_scale: 65536.0000 (61592.3103)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [20]  [2510/2809]  eta: 0:01:50  lr: 0.000027  min_lr: 0.000000  loss: 4.0682 (3.8165)  loss_scale: 65536.0000 (61608.0159)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0002  max mem: 15572
Epoch: [20]  [2520/2809]  eta: 0:01:46  lr: 0.000027  min_lr: 0.000000  loss: 3.9348 (3.8164)  loss_scale: 65536.0000 (61623.5970)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [20]  [2530/2809]  eta: 0:01:43  lr: 0.000027  min_lr: 0.000000  loss: 3.9183 (3.8165)  loss_scale: 65536.0000 (61639.0549)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
[2025-01-13 05:23:12,448] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58720
[2025-01-13 05:23:12,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:23:12,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [2540/2809]  eta: 0:01:39  lr: 0.000027  min_lr: 0.000000  loss: 3.8495 (3.8158)  loss_scale: 65536.0000 (61641.4955)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [20]  [2550/2809]  eta: 0:01:35  lr: 0.000027  min_lr: 0.000000  loss: 3.8008 (3.8155)  loss_scale: 32768.0000 (61528.3105)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [20]  [2560/2809]  eta: 0:01:32  lr: 0.000027  min_lr: 0.000000  loss: 3.8368 (3.8155)  loss_scale: 32768.0000 (61416.0094)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [20]  [2570/2809]  eta: 0:01:28  lr: 0.000027  min_lr: 0.000000  loss: 4.0021 (3.8152)  loss_scale: 32768.0000 (61304.5819)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [20]  [2580/2809]  eta: 0:01:24  lr: 0.000027  min_lr: 0.000000  loss: 3.9557 (3.8155)  loss_scale: 32768.0000 (61194.0178)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [20]  [2590/2809]  eta: 0:01:21  lr: 0.000027  min_lr: 0.000000  loss: 3.9557 (3.8162)  loss_scale: 32768.0000 (61084.3072)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [20]  [2600/2809]  eta: 0:01:17  lr: 0.000027  min_lr: 0.000000  loss: 3.9211 (3.8167)  loss_scale: 32768.0000 (60975.4402)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [20]  [2610/2809]  eta: 0:01:13  lr: 0.000027  min_lr: 0.000000  loss: 3.8818 (3.8164)  loss_scale: 32768.0000 (60867.4071)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [20]  [2620/2809]  eta: 0:01:09  lr: 0.000027  min_lr: 0.000000  loss: 3.8878 (3.8169)  loss_scale: 32768.0000 (60760.1984)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [20]  [2630/2809]  eta: 0:01:06  lr: 0.000027  min_lr: 0.000000  loss: 3.8544 (3.8164)  loss_scale: 32768.0000 (60653.8046)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [20]  [2640/2809]  eta: 0:01:02  lr: 0.000027  min_lr: 0.000000  loss: 3.7588 (3.8163)  loss_scale: 32768.0000 (60548.2166)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0002  max mem: 15572
Epoch: [20]  [2650/2809]  eta: 0:00:58  lr: 0.000027  min_lr: 0.000000  loss: 3.9197 (3.8170)  loss_scale: 32768.0000 (60443.4251)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0003  max mem: 15572
Epoch: [20]  [2660/2809]  eta: 0:00:55  lr: 0.000027  min_lr: 0.000000  loss: 3.8327 (3.8166)  loss_scale: 32768.0000 (60339.4213)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
[2025-01-13 05:24:00,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:24:00,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [2670/2809]  eta: 0:00:51  lr: 0.000027  min_lr: 0.000000  loss: 3.7076 (3.8155)  loss_scale: 32768.0000 (60260.7323)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [20]  [2680/2809]  eta: 0:00:47  lr: 0.000027  min_lr: 0.000000  loss: 3.8250 (3.8160)  loss_scale: 65536.0000 (60280.4088)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [20]  [2690/2809]  eta: 0:00:44  lr: 0.000027  min_lr: 0.000000  loss: 3.9979 (3.8166)  loss_scale: 65536.0000 (60299.9391)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0003  max mem: 15572
Epoch: [20]  [2700/2809]  eta: 0:00:40  lr: 0.000027  min_lr: 0.000000  loss: 3.9832 (3.8172)  loss_scale: 65536.0000 (60319.3247)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [20]  [2710/2809]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000000  loss: 3.9368 (3.8179)  loss_scale: 65536.0000 (60338.5673)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [20]  [2720/2809]  eta: 0:00:32  lr: 0.000027  min_lr: 0.000000  loss: 3.6375 (3.8166)  loss_scale: 65536.0000 (60357.6685)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [20]  [2730/2809]  eta: 0:00:29  lr: 0.000027  min_lr: 0.000000  loss: 3.4689 (3.8166)  loss_scale: 65536.0000 (60376.6298)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0002  max mem: 15572
Epoch: [20]  [2740/2809]  eta: 0:00:25  lr: 0.000027  min_lr: 0.000000  loss: 3.6347 (3.8158)  loss_scale: 65536.0000 (60395.4528)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [20]  [2750/2809]  eta: 0:00:21  lr: 0.000027  min_lr: 0.000000  loss: 3.8733 (3.8162)  loss_scale: 65536.0000 (60414.1389)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [20]  [2760/2809]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000000  loss: 3.7549 (3.8154)  loss_scale: 65536.0000 (60432.6896)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [20]  [2770/2809]  eta: 0:00:14  lr: 0.000027  min_lr: 0.000000  loss: 3.6078 (3.8150)  loss_scale: 65536.0000 (60451.1065)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [20]  [2780/2809]  eta: 0:00:10  lr: 0.000027  min_lr: 0.000000  loss: 3.8922 (3.8157)  loss_scale: 65536.0000 (60469.3909)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [20]  [2790/2809]  eta: 0:00:07  lr: 0.000027  min_lr: 0.000000  loss: 3.9397 (3.8164)  loss_scale: 65536.0000 (60487.5442)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
[2025-01-13 05:24:47,442] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:24:47,442] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:24:47,799] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58978
[2025-01-13 05:24:47,799] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:24:47,799] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [2800/2809]  eta: 0:00:03  lr: 0.000027  min_lr: 0.000000  loss: 3.8624 (3.8170)  loss_scale: 65536.0000 (60528.9654)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0001  max mem: 15572
Epoch: [20]  [2808/2809]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000000  loss: 3.8624 (3.8175)  loss_scale: 65536.0000 (60543.2253)  weight_decay: 0.0500 (0.0500)  time: 0.3601  data: 0.0001  max mem: 15572
Epoch: [20] Total time: 0:17:19 (0.3701 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000000  loss: 3.8624 (3.8175)  loss_scale: 65536.0000 (60543.2253)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:11:18  loss: 0.4315 (0.4315)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4938  data: 2.3176  max mem: 15572
Val:  [ 10/272]  eta: 0:01:42  loss: 2.6855 (2.4523)  acc1: 38.8889 (38.8889)  acc5: 66.6667 (67.1717)  time: 0.3913  data: 0.2316  max mem: 15572
Val:  [ 20/272]  eta: 0:01:12  loss: 2.6855 (2.5143)  acc1: 38.8889 (39.6825)  acc5: 66.6667 (68.5185)  time: 0.1794  data: 0.0251  max mem: 15572
Val:  [ 30/272]  eta: 0:01:00  loss: 2.5747 (2.5885)  acc1: 38.8889 (36.3799)  acc5: 66.6667 (68.2796)  time: 0.1717  data: 0.0137  max mem: 15572
Val:  [ 40/272]  eta: 0:00:53  loss: 2.6691 (2.6123)  acc1: 27.7778 (34.9594)  acc5: 72.2222 (69.3767)  time: 0.1648  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:47  loss: 2.6691 (2.5579)  acc1: 38.8889 (35.9477)  acc5: 77.7778 (71.0240)  time: 0.1601  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:43  loss: 1.6160 (2.4264)  acc1: 61.1111 (40.0729)  acc5: 88.8889 (72.5865)  time: 0.1629  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:40  loss: 1.5530 (2.3280)  acc1: 61.1111 (43.4272)  acc5: 88.8889 (74.1002)  time: 0.1638  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:37  loss: 1.9379 (2.3214)  acc1: 55.5556 (43.5528)  acc5: 77.7778 (74.1427)  time: 0.1586  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.3127 (2.3347)  acc1: 50.0000 (43.8339)  acc5: 77.7778 (74.7253)  time: 0.1610  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:32  loss: 2.2934 (2.3637)  acc1: 50.0000 (43.3443)  acc5: 77.7778 (74.5325)  time: 0.1589  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:30  loss: 2.6835 (2.4322)  acc1: 22.2222 (41.4414)  acc5: 72.2222 (73.3233)  time: 0.1581  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 2.8862 (2.4794)  acc1: 22.2222 (40.5877)  acc5: 66.6667 (72.2681)  time: 0.1601  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.4070 (2.4505)  acc1: 38.8889 (41.6031)  acc5: 72.2222 (72.8584)  time: 0.1596  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 2.0729 (2.4420)  acc1: 50.0000 (42.2774)  acc5: 77.7778 (72.7344)  time: 0.1586  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.5315 (2.4497)  acc1: 38.8889 (41.6851)  acc5: 72.2222 (73.0316)  time: 0.1584  data: 0.0005  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.4845 (2.4401)  acc1: 38.8889 (42.2015)  acc5: 72.2222 (73.3264)  time: 0.1579  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.5572 (2.4633)  acc1: 38.8889 (41.4880)  acc5: 72.2222 (72.9695)  time: 0.1587  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 2.5648 (2.4556)  acc1: 33.3333 (41.3444)  acc5: 72.2222 (73.3579)  time: 0.1609  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.5461 (2.5049)  acc1: 33.3333 (40.1978)  acc5: 72.2222 (71.9895)  time: 0.1609  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.6614 (2.5156)  acc1: 27.7778 (39.9116)  acc5: 55.5556 (71.7523)  time: 0.1575  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.2572 (2.5139)  acc1: 38.8889 (40.3370)  acc5: 77.7778 (71.7483)  time: 0.1525  data: 0.0003  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.0980 (2.5076)  acc1: 55.5556 (40.5480)  acc5: 77.7778 (71.7949)  time: 0.1581  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.9347 (2.4780)  acc1: 61.1111 (41.5103)  acc5: 83.3333 (72.3184)  time: 0.1659  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.6922 (2.4617)  acc1: 61.1111 (41.9087)  acc5: 83.3333 (72.5911)  time: 0.1614  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.5481 (2.4721)  acc1: 33.3333 (41.5228)  acc5: 72.2222 (72.5321)  time: 0.1591  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.2697 (2.4119)  acc1: 66.6667 (43.3163)  acc5: 88.8889 (73.2865)  time: 0.1530  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.5684 (2.4134)  acc1: 61.1111 (43.0504)  acc5: 88.8889 (73.3087)  time: 0.1385  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.5684 (2.4193)  acc1: 61.1111 (43.0268)  acc5: 88.8889 (73.2746)  time: 0.1327  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1687 s / it)
* Acc@1 43.027 Acc@5 73.275 loss 2.419
Accuracy of the network on the 4883 val videos: 43.0%
Max accuracy: 43.52%
Epoch: [21]  [   0/2809]  eta: 2:56:44  lr: 0.000027  min_lr: 0.000000  loss: 4.1191 (4.1191)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.7753  data: 3.3796  max mem: 15572
[2025-01-13 05:25:44,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=59000, skipped=402, lr=[2.57690559215989e-07, 2.57690559215989e-07, 3.681293703085558e-07, 3.681293703085558e-07, 5.25899100440794e-07, 5.25899100440794e-07, 7.512844292011343e-07, 7.512844292011343e-07, 1.0732634702873348e-06, 1.0732634702873348e-06, 1.533233528981907e-06, 1.533233528981907e-06, 2.1903336128312955e-06, 2.1903336128312955e-06, 3.129048018330423e-06, 3.129048018330423e-06, 4.470068597614889e-06, 4.470068597614889e-06, 6.385812282306986e-06, 6.385812282306986e-06, 9.122588974724265e-06, 9.122588974724265e-06, 1.3032269963891809e-05, 1.3032269963891809e-05, 1.8617528519845444e-05, 1.8617528519845444e-05, 2.659646931406492e-05, 2.659646931406492e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 05:25:44,973] [INFO] [timer.py:260:stop] epoch=0/micro_step=59000/global_step=59000, RunningAvgSamplesPerSec=29.768957731736432, CurrSamplesPerSec=33.39123789001885, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [21]  [  10/2809]  eta: 0:32:07  lr: 0.000027  min_lr: 0.000000  loss: 3.7325 (3.6335)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6885  data: 0.3076  max mem: 15572
Epoch: [21]  [  20/2809]  eta: 0:25:06  lr: 0.000027  min_lr: 0.000000  loss: 3.7325 (3.7829)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0003  max mem: 15572
Epoch: [21]  [  30/2809]  eta: 0:22:26  lr: 0.000027  min_lr: 0.000000  loss: 3.5959 (3.7273)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [21]  [  40/2809]  eta: 0:21:01  lr: 0.000027  min_lr: 0.000000  loss: 3.6311 (3.7341)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [21]  [  50/2809]  eta: 0:20:14  lr: 0.000027  min_lr: 0.000000  loss: 3.8763 (3.8094)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [21]  [  60/2809]  eta: 0:19:36  lr: 0.000027  min_lr: 0.000000  loss: 3.9817 (3.8041)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [21]  [  70/2809]  eta: 0:19:09  lr: 0.000027  min_lr: 0.000000  loss: 3.8417 (3.7967)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [21]  [  80/2809]  eta: 0:18:48  lr: 0.000027  min_lr: 0.000000  loss: 3.9123 (3.8041)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [21]  [  90/2809]  eta: 0:18:31  lr: 0.000027  min_lr: 0.000000  loss: 3.9123 (3.8099)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [21]  [ 100/2809]  eta: 0:18:17  lr: 0.000027  min_lr: 0.000000  loss: 3.8284 (3.8010)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [21]  [ 110/2809]  eta: 0:18:05  lr: 0.000027  min_lr: 0.000000  loss: 3.7654 (3.7884)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0014  max mem: 15572
[2025-01-13 05:26:24,993] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:26:24,993] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:26:25,758] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59109
[2025-01-13 05:26:25,758] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:26:25,758] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [ 120/2809]  eta: 0:17:54  lr: 0.000027  min_lr: 0.000000  loss: 3.7461 (3.7802)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0014  max mem: 15572
Epoch: [21]  [ 130/2809]  eta: 0:17:45  lr: 0.000027  min_lr: 0.000000  loss: 3.7663 (3.7828)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [21]  [ 140/2809]  eta: 0:17:36  lr: 0.000027  min_lr: 0.000000  loss: 3.8182 (3.7946)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [21]  [ 150/2809]  eta: 0:17:28  lr: 0.000026  min_lr: 0.000000  loss: 3.9811 (3.7949)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [21]  [ 160/2809]  eta: 0:17:20  lr: 0.000026  min_lr: 0.000000  loss: 3.8662 (3.7889)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [21]  [ 170/2809]  eta: 0:17:13  lr: 0.000026  min_lr: 0.000000  loss: 3.8556 (3.8008)  loss_scale: 65536.0000 (66302.5029)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0003  max mem: 15572
Epoch: [21]  [ 180/2809]  eta: 0:17:06  lr: 0.000026  min_lr: 0.000000  loss: 3.7376 (3.7944)  loss_scale: 65536.0000 (66260.1547)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [21]  [ 190/2809]  eta: 0:16:58  lr: 0.000026  min_lr: 0.000000  loss: 3.8041 (3.7962)  loss_scale: 65536.0000 (66222.2408)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [21]  [ 200/2809]  eta: 0:16:52  lr: 0.000026  min_lr: 0.000000  loss: 3.8409 (3.7931)  loss_scale: 65536.0000 (66188.0995)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [21]  [ 210/2809]  eta: 0:16:45  lr: 0.000026  min_lr: 0.000000  loss: 3.8598 (3.8034)  loss_scale: 65536.0000 (66157.1943)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [21]  [ 220/2809]  eta: 0:16:40  lr: 0.000026  min_lr: 0.000000  loss: 4.0788 (3.8099)  loss_scale: 65536.0000 (66129.0860)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [21]  [ 230/2809]  eta: 0:16:34  lr: 0.000026  min_lr: 0.000000  loss: 3.8941 (3.8123)  loss_scale: 65536.0000 (66103.4113)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [21]  [ 240/2809]  eta: 0:16:29  lr: 0.000026  min_lr: 0.000000  loss: 3.8693 (3.8082)  loss_scale: 65536.0000 (66079.8672)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
[2025-01-13 05:27:13,572] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:27:13,572] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [ 250/2809]  eta: 0:16:23  lr: 0.000026  min_lr: 0.000000  loss: 3.7071 (3.8049)  loss_scale: 65536.0000 (66580.3984)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
[2025-01-13 05:27:14,302] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59240
[2025-01-13 05:27:14,302] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:27:14,302] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [ 260/2809]  eta: 0:16:18  lr: 0.000026  min_lr: 0.000000  loss: 3.6604 (3.7940)  loss_scale: 65536.0000 (66540.3831)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [21]  [ 270/2809]  eta: 0:16:13  lr: 0.000026  min_lr: 0.000000  loss: 3.6357 (3.7913)  loss_scale: 65536.0000 (66503.3210)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [21]  [ 280/2809]  eta: 0:16:08  lr: 0.000026  min_lr: 0.000000  loss: 3.7911 (3.7949)  loss_scale: 65536.0000 (66468.8968)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [21]  [ 290/2809]  eta: 0:16:04  lr: 0.000026  min_lr: 0.000000  loss: 3.7148 (3.7934)  loss_scale: 65536.0000 (66436.8385)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
Epoch: [21]  [ 300/2809]  eta: 0:15:59  lr: 0.000026  min_lr: 0.000000  loss: 3.6440 (3.7934)  loss_scale: 65536.0000 (66406.9103)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0003  max mem: 15572
Epoch: [21]  [ 310/2809]  eta: 0:15:54  lr: 0.000026  min_lr: 0.000000  loss: 3.6170 (3.7844)  loss_scale: 65536.0000 (66378.9068)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [21]  [ 320/2809]  eta: 0:15:49  lr: 0.000026  min_lr: 0.000000  loss: 3.8548 (3.7936)  loss_scale: 65536.0000 (66352.6480)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [21]  [ 330/2809]  eta: 0:15:44  lr: 0.000026  min_lr: 0.000000  loss: 4.0595 (3.7992)  loss_scale: 65536.0000 (66327.9758)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [21]  [ 340/2809]  eta: 0:15:39  lr: 0.000026  min_lr: 0.000000  loss: 4.0166 (3.7998)  loss_scale: 65536.0000 (66304.7507)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [21]  [ 350/2809]  eta: 0:15:35  lr: 0.000026  min_lr: 0.000000  loss: 3.9875 (3.8049)  loss_scale: 65536.0000 (66282.8490)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0003  max mem: 15572
Epoch: [21]  [ 360/2809]  eta: 0:15:30  lr: 0.000026  min_lr: 0.000000  loss: 4.1402 (3.8163)  loss_scale: 65536.0000 (66262.1607)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [21]  [ 370/2809]  eta: 0:15:26  lr: 0.000026  min_lr: 0.000000  loss: 4.1197 (3.8223)  loss_scale: 65536.0000 (66242.5876)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
[2025-01-13 05:28:01,968] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:28:01,968] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [ 380/2809]  eta: 0:15:21  lr: 0.000026  min_lr: 0.000000  loss: 3.9619 (3.8224)  loss_scale: 65536.0000 (66396.0525)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
[2025-01-13 05:28:02,333] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59370
[2025-01-13 05:28:02,333] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:28:02,333] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [ 390/2809]  eta: 0:15:17  lr: 0.000026  min_lr: 0.000000  loss: 3.7165 (3.8163)  loss_scale: 65536.0000 (66374.0563)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [21]  [ 400/2809]  eta: 0:15:12  lr: 0.000026  min_lr: 0.000000  loss: 3.7383 (3.8230)  loss_scale: 65536.0000 (66353.1571)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [21]  [ 410/2809]  eta: 0:15:08  lr: 0.000026  min_lr: 0.000000  loss: 3.9519 (3.8246)  loss_scale: 65536.0000 (66333.2749)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [21]  [ 420/2809]  eta: 0:15:04  lr: 0.000026  min_lr: 0.000000  loss: 3.9519 (3.8266)  loss_scale: 65536.0000 (66314.3373)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [21]  [ 430/2809]  eta: 0:15:00  lr: 0.000026  min_lr: 0.000000  loss: 3.8298 (3.8236)  loss_scale: 65536.0000 (66296.2784)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [21]  [ 440/2809]  eta: 0:14:55  lr: 0.000026  min_lr: 0.000000  loss: 3.7635 (3.8232)  loss_scale: 65536.0000 (66279.0385)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [21]  [ 450/2809]  eta: 0:14:51  lr: 0.000026  min_lr: 0.000000  loss: 3.8314 (3.8239)  loss_scale: 65536.0000 (66262.5632)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [21]  [ 460/2809]  eta: 0:14:47  lr: 0.000026  min_lr: 0.000000  loss: 3.9614 (3.8261)  loss_scale: 65536.0000 (66246.8026)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [21]  [ 470/2809]  eta: 0:14:42  lr: 0.000026  min_lr: 0.000000  loss: 3.7158 (3.8165)  loss_scale: 65536.0000 (66231.7113)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0003  max mem: 15572
Epoch: [21]  [ 480/2809]  eta: 0:14:38  lr: 0.000026  min_lr: 0.000000  loss: 3.7016 (3.8148)  loss_scale: 65536.0000 (66217.2474)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [21]  [ 490/2809]  eta: 0:14:34  lr: 0.000026  min_lr: 0.000000  loss: 3.8001 (3.8122)  loss_scale: 65536.0000 (66203.3727)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [21]  [ 500/2809]  eta: 0:14:30  lr: 0.000026  min_lr: 0.000000  loss: 3.8580 (3.8155)  loss_scale: 65536.0000 (66190.0519)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
[2025-01-13 05:28:50,099] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:28:50,100] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [ 510/2809]  eta: 0:14:26  lr: 0.000026  min_lr: 0.000000  loss: 3.8942 (3.8137)  loss_scale: 65536.0000 (66305.5029)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
[2025-01-13 05:28:50,464] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59500
[2025-01-13 05:28:50,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:28:50,466] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [ 520/2809]  eta: 0:14:22  lr: 0.000026  min_lr: 0.000000  loss: 3.8674 (3.8156)  loss_scale: 65536.0000 (66290.7332)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [21]  [ 530/2809]  eta: 0:14:18  lr: 0.000026  min_lr: 0.000000  loss: 3.8961 (3.8186)  loss_scale: 65536.0000 (66276.5198)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [21]  [ 540/2809]  eta: 0:14:14  lr: 0.000026  min_lr: 0.000000  loss: 3.8960 (3.8182)  loss_scale: 65536.0000 (66262.8318)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [21]  [ 550/2809]  eta: 0:14:10  lr: 0.000026  min_lr: 0.000000  loss: 3.7079 (3.8164)  loss_scale: 65536.0000 (66249.6407)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [21]  [ 560/2809]  eta: 0:14:06  lr: 0.000026  min_lr: 0.000000  loss: 3.6555 (3.8134)  loss_scale: 65536.0000 (66236.9198)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [21]  [ 570/2809]  eta: 0:14:02  lr: 0.000026  min_lr: 0.000000  loss: 3.9000 (3.8185)  loss_scale: 65536.0000 (66224.6445)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [21]  [ 580/2809]  eta: 0:13:58  lr: 0.000026  min_lr: 0.000000  loss: 3.7975 (3.8165)  loss_scale: 65536.0000 (66212.7917)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [21]  [ 590/2809]  eta: 0:13:54  lr: 0.000026  min_lr: 0.000000  loss: 3.7646 (3.8167)  loss_scale: 65536.0000 (66201.3401)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [21]  [ 600/2809]  eta: 0:13:50  lr: 0.000026  min_lr: 0.000000  loss: 3.8532 (3.8170)  loss_scale: 65536.0000 (66190.2696)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [21]  [ 610/2809]  eta: 0:13:46  lr: 0.000026  min_lr: 0.000000  loss: 4.0321 (3.8207)  loss_scale: 65536.0000 (66179.5614)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [21]  [ 620/2809]  eta: 0:13:42  lr: 0.000026  min_lr: 0.000000  loss: 3.8697 (3.8181)  loss_scale: 65536.0000 (66169.1981)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [21]  [ 630/2809]  eta: 0:13:38  lr: 0.000026  min_lr: 0.000000  loss: 3.6122 (3.8157)  loss_scale: 65536.0000 (66159.1632)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
[2025-01-13 05:29:38,183] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:29:38,183] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [ 640/2809]  eta: 0:13:34  lr: 0.000026  min_lr: 0.000000  loss: 3.9799 (3.8189)  loss_scale: 65536.0000 (66251.6817)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
[2025-01-13 05:29:38,555] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59630
[2025-01-13 05:29:38,555] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:29:38,555] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [ 650/2809]  eta: 0:13:30  lr: 0.000026  min_lr: 0.000000  loss: 3.8799 (3.8149)  loss_scale: 65536.0000 (66240.6882)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [21]  [ 660/2809]  eta: 0:13:26  lr: 0.000026  min_lr: 0.000000  loss: 3.6893 (3.8139)  loss_scale: 65536.0000 (66230.0272)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [21]  [ 670/2809]  eta: 0:13:22  lr: 0.000026  min_lr: 0.000000  loss: 3.7637 (3.8131)  loss_scale: 65536.0000 (66219.6841)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [21]  [ 680/2809]  eta: 0:13:18  lr: 0.000026  min_lr: 0.000000  loss: 3.9116 (3.8150)  loss_scale: 65536.0000 (66209.6446)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [21]  [ 690/2809]  eta: 0:13:14  lr: 0.000026  min_lr: 0.000000  loss: 3.9654 (3.8149)  loss_scale: 65536.0000 (66199.8958)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [21]  [ 700/2809]  eta: 0:13:10  lr: 0.000026  min_lr: 0.000000  loss: 3.5001 (3.8100)  loss_scale: 65536.0000 (66190.4251)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [21]  [ 710/2809]  eta: 0:13:06  lr: 0.000026  min_lr: 0.000000  loss: 3.5755 (3.8090)  loss_scale: 65536.0000 (66181.2208)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [21]  [ 720/2809]  eta: 0:13:02  lr: 0.000026  min_lr: 0.000000  loss: 3.8408 (3.8087)  loss_scale: 65536.0000 (66172.2718)  weight_decay: 0.0500 (0.0500)  time: 0.3653  data: 0.0002  max mem: 15572
Epoch: [21]  [ 730/2809]  eta: 0:12:58  lr: 0.000026  min_lr: 0.000000  loss: 3.8546 (3.8086)  loss_scale: 65536.0000 (66163.5677)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [21]  [ 740/2809]  eta: 0:12:55  lr: 0.000026  min_lr: 0.000000  loss: 3.9057 (3.8085)  loss_scale: 65536.0000 (66155.0985)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [21]  [ 750/2809]  eta: 0:12:51  lr: 0.000026  min_lr: 0.000000  loss: 3.5973 (3.8036)  loss_scale: 65536.0000 (66146.8549)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [21]  [ 760/2809]  eta: 0:12:47  lr: 0.000026  min_lr: 0.000000  loss: 3.6635 (3.8048)  loss_scale: 65536.0000 (66138.8279)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
[2025-01-13 05:30:26,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:30:26,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [ 770/2809]  eta: 0:12:43  lr: 0.000026  min_lr: 0.000000  loss: 3.9210 (3.8040)  loss_scale: 65536.0000 (66216.0104)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
[2025-01-13 05:30:27,339] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59762
[2025-01-13 05:30:27,339] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:30:27,339] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-13 05:30:29,582] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59768
[2025-01-13 05:30:29,582] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:30:29,582] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 780/2809]  eta: 0:12:39  lr: 0.000026  min_lr: 0.000000  loss: 3.7668 (3.8012)  loss_scale: 65536.0000 (66291.2164)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0002  max mem: 15572
Epoch: [21]  [ 790/2809]  eta: 0:12:35  lr: 0.000026  min_lr: 0.000000  loss: 3.6634 (3.7999)  loss_scale: 32768.0000 (65867.4083)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [21]  [ 800/2809]  eta: 0:12:32  lr: 0.000026  min_lr: 0.000000  loss: 3.6298 (3.7995)  loss_scale: 32768.0000 (65454.1823)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [21]  [ 810/2809]  eta: 0:12:28  lr: 0.000026  min_lr: 0.000000  loss: 3.7698 (3.8003)  loss_scale: 32768.0000 (65051.1467)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [21]  [ 820/2809]  eta: 0:12:24  lr: 0.000026  min_lr: 0.000000  loss: 4.0480 (3.8053)  loss_scale: 32768.0000 (64657.9294)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [21]  [ 830/2809]  eta: 0:12:20  lr: 0.000026  min_lr: 0.000000  loss: 3.9498 (3.8056)  loss_scale: 32768.0000 (64274.1757)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [21]  [ 840/2809]  eta: 0:12:16  lr: 0.000026  min_lr: 0.000000  loss: 3.7229 (3.8062)  loss_scale: 32768.0000 (63899.5482)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [21]  [ 850/2809]  eta: 0:12:12  lr: 0.000026  min_lr: 0.000000  loss: 3.8267 (3.8055)  loss_scale: 32768.0000 (63533.7250)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [21]  [ 860/2809]  eta: 0:12:09  lr: 0.000026  min_lr: 0.000000  loss: 3.8275 (3.8056)  loss_scale: 32768.0000 (63176.3995)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [21]  [ 870/2809]  eta: 0:12:05  lr: 0.000026  min_lr: 0.000000  loss: 3.8275 (3.8054)  loss_scale: 32768.0000 (62827.2790)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
Epoch: [21]  [ 880/2809]  eta: 0:12:01  lr: 0.000026  min_lr: 0.000000  loss: 3.9101 (3.8077)  loss_scale: 32768.0000 (62486.0840)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0003  max mem: 15572
Epoch: [21]  [ 890/2809]  eta: 0:11:57  lr: 0.000026  min_lr: 0.000000  loss: 3.8026 (3.8058)  loss_scale: 32768.0000 (62152.5477)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [21]  [ 900/2809]  eta: 0:11:53  lr: 0.000026  min_lr: 0.000000  loss: 3.6863 (3.8035)  loss_scale: 32768.0000 (61826.4151)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
[2025-01-13 05:31:17,372] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:31:17,372] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [ 910/2809]  eta: 0:11:49  lr: 0.000026  min_lr: 0.000000  loss: 3.7747 (3.8032)  loss_scale: 32768.0000 (61615.3502)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [21]  [ 920/2809]  eta: 0:11:46  lr: 0.000026  min_lr: 0.000000  loss: 3.7391 (3.8027)  loss_scale: 65536.0000 (61657.9197)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [21]  [ 930/2809]  eta: 0:11:42  lr: 0.000026  min_lr: 0.000000  loss: 3.6017 (3.8013)  loss_scale: 65536.0000 (61699.5747)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [21]  [ 940/2809]  eta: 0:11:38  lr: 0.000026  min_lr: 0.000000  loss: 3.6849 (3.8005)  loss_scale: 65536.0000 (61740.3443)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [21]  [ 950/2809]  eta: 0:11:34  lr: 0.000026  min_lr: 0.000000  loss: 3.6849 (3.7995)  loss_scale: 65536.0000 (61780.2566)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [21]  [ 960/2809]  eta: 0:11:30  lr: 0.000026  min_lr: 0.000000  loss: 3.6956 (3.8003)  loss_scale: 65536.0000 (61819.3382)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [21]  [ 970/2809]  eta: 0:11:27  lr: 0.000026  min_lr: 0.000000  loss: 3.8242 (3.7990)  loss_scale: 65536.0000 (61857.6148)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [21]  [ 980/2809]  eta: 0:11:23  lr: 0.000026  min_lr: 0.000000  loss: 3.8514 (3.8010)  loss_scale: 65536.0000 (61895.1111)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [21]  [ 990/2809]  eta: 0:11:19  lr: 0.000026  min_lr: 0.000000  loss: 4.1017 (3.8036)  loss_scale: 65536.0000 (61931.8507)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [21]  [1000/2809]  eta: 0:11:15  lr: 0.000026  min_lr: 0.000000  loss: 3.9878 (3.8029)  loss_scale: 65536.0000 (61967.8561)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
[2025-01-13 05:31:55,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=60000, skipped=409, lr=[2.5049251316672856e-07, 2.5049251316672856e-07, 3.5784644738104086e-07, 3.5784644738104086e-07, 5.112092105443442e-07, 5.112092105443442e-07, 7.30298872206206e-07, 7.30298872206206e-07, 1.0432841031517227e-06, 1.0432841031517227e-06, 1.4904058616453183e-06, 1.4904058616453183e-06, 2.1291512309218833e-06, 2.1291512309218833e-06, 3.041644615602691e-06, 3.041644615602691e-06, 4.34520659371813e-06, 4.34520659371813e-06, 6.207437991025901e-06, 6.207437991025901e-06, 8.86776855860843e-06, 8.86776855860843e-06, 1.2668240798012043e-05, 1.2668240798012043e-05, 1.8097486854302922e-05, 1.8097486854302922e-05, 2.5853552649004174e-05, 2.5853552649004174e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 05:31:55,069] [INFO] [timer.py:260:stop] epoch=0/micro_step=60000/global_step=60000, RunningAvgSamplesPerSec=29.834675750653695, CurrSamplesPerSec=35.10063852756492, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [21]  [1010/2809]  eta: 0:11:11  lr: 0.000026  min_lr: 0.000000  loss: 4.0022 (3.8050)  loss_scale: 65536.0000 (62003.1494)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [21]  [1020/2809]  eta: 0:11:07  lr: 0.000026  min_lr: 0.000000  loss: 4.0143 (3.8046)  loss_scale: 65536.0000 (62037.7512)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [21]  [1030/2809]  eta: 0:11:04  lr: 0.000026  min_lr: 0.000000  loss: 3.9162 (3.8057)  loss_scale: 65536.0000 (62071.6819)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
[2025-01-13 05:32:04,718] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:32:04,718] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:32:06,188] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60029
[2025-01-13 05:32:06,188] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:32:06,189] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [1040/2809]  eta: 0:11:00  lr: 0.000026  min_lr: 0.000000  loss: 3.9029 (3.8068)  loss_scale: 65536.0000 (62356.7800)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [21]  [1050/2809]  eta: 0:10:56  lr: 0.000026  min_lr: 0.000000  loss: 3.9486 (3.8081)  loss_scale: 65536.0000 (62387.0295)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [21]  [1060/2809]  eta: 0:10:52  lr: 0.000026  min_lr: 0.000000  loss: 3.9670 (3.8089)  loss_scale: 65536.0000 (62416.7088)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [21]  [1070/2809]  eta: 0:10:49  lr: 0.000026  min_lr: 0.000000  loss: 3.8859 (3.8095)  loss_scale: 65536.0000 (62445.8338)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
[2025-01-13 05:32:20,221] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60067
[2025-01-13 05:32:20,221] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:32:20,221] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1080/2809]  eta: 0:10:45  lr: 0.000026  min_lr: 0.000000  loss: 3.7671 (3.8080)  loss_scale: 65536.0000 (62383.4820)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [21]  [1090/2809]  eta: 0:10:41  lr: 0.000026  min_lr: 0.000000  loss: 3.6481 (3.8076)  loss_scale: 32768.0000 (62112.0293)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [21]  [1100/2809]  eta: 0:10:37  lr: 0.000026  min_lr: 0.000000  loss: 3.7445 (3.8075)  loss_scale: 32768.0000 (61845.5077)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [21]  [1110/2809]  eta: 0:10:33  lr: 0.000026  min_lr: 0.000000  loss: 3.8298 (3.8074)  loss_scale: 32768.0000 (61583.7840)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [21]  [1120/2809]  eta: 0:10:30  lr: 0.000026  min_lr: 0.000000  loss: 3.8415 (3.8081)  loss_scale: 32768.0000 (61326.7297)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [21]  [1130/2809]  eta: 0:10:26  lr: 0.000026  min_lr: 0.000000  loss: 3.8215 (3.8084)  loss_scale: 32768.0000 (61074.2210)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [21]  [1140/2809]  eta: 0:10:22  lr: 0.000026  min_lr: 0.000000  loss: 3.8164 (3.8091)  loss_scale: 32768.0000 (60826.1385)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [21]  [1150/2809]  eta: 0:10:18  lr: 0.000026  min_lr: 0.000000  loss: 3.9587 (3.8105)  loss_scale: 32768.0000 (60582.3666)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [21]  [1160/2809]  eta: 0:10:14  lr: 0.000026  min_lr: 0.000000  loss: 3.9703 (3.8097)  loss_scale: 32768.0000 (60342.7941)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [21]  [1170/2809]  eta: 0:10:11  lr: 0.000026  min_lr: 0.000000  loss: 3.8647 (3.8100)  loss_scale: 32768.0000 (60107.3134)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [21]  [1180/2809]  eta: 0:10:07  lr: 0.000026  min_lr: 0.000000  loss: 3.8647 (3.8097)  loss_scale: 32768.0000 (59875.8205)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [21]  [1190/2809]  eta: 0:10:03  lr: 0.000026  min_lr: 0.000000  loss: 3.9652 (3.8103)  loss_scale: 32768.0000 (59648.2149)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [21]  [1200/2809]  eta: 0:09:59  lr: 0.000026  min_lr: 0.000000  loss: 3.8805 (3.8109)  loss_scale: 32768.0000 (59424.3997)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
[2025-01-13 05:33:07,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:33:07,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [1210/2809]  eta: 0:09:56  lr: 0.000026  min_lr: 0.000000  loss: 3.6976 (3.8079)  loss_scale: 32768.0000 (59312.5153)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [21]  [1220/2809]  eta: 0:09:52  lr: 0.000026  min_lr: 0.000000  loss: 3.8266 (3.8097)  loss_scale: 65536.0000 (59363.4857)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [21]  [1230/2809]  eta: 0:09:48  lr: 0.000026  min_lr: 0.000000  loss: 3.9685 (3.8112)  loss_scale: 65536.0000 (59413.6279)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [21]  [1240/2809]  eta: 0:09:44  lr: 0.000026  min_lr: 0.000000  loss: 3.8761 (3.8112)  loss_scale: 65536.0000 (59462.9621)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [21]  [1250/2809]  eta: 0:09:41  lr: 0.000026  min_lr: 0.000000  loss: 3.7977 (3.8107)  loss_scale: 65536.0000 (59511.5076)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [21]  [1260/2809]  eta: 0:09:37  lr: 0.000026  min_lr: 0.000000  loss: 3.8303 (3.8111)  loss_scale: 65536.0000 (59559.2831)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [21]  [1270/2809]  eta: 0:09:33  lr: 0.000026  min_lr: 0.000000  loss: 3.7046 (3.8096)  loss_scale: 65536.0000 (59606.3068)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [21]  [1280/2809]  eta: 0:09:29  lr: 0.000026  min_lr: 0.000000  loss: 3.7046 (3.8095)  loss_scale: 65536.0000 (59652.5964)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [21]  [1290/2809]  eta: 0:09:26  lr: 0.000026  min_lr: 0.000000  loss: 3.9220 (3.8092)  loss_scale: 65536.0000 (59698.1689)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [21]  [1300/2809]  eta: 0:09:22  lr: 0.000026  min_lr: 0.000000  loss: 3.7315 (3.8098)  loss_scale: 65536.0000 (59743.0407)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [21]  [1310/2809]  eta: 0:09:18  lr: 0.000026  min_lr: 0.000000  loss: 3.8618 (3.8108)  loss_scale: 65536.0000 (59787.2281)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [21]  [1320/2809]  eta: 0:09:14  lr: 0.000026  min_lr: 0.000000  loss: 3.8587 (3.8097)  loss_scale: 65536.0000 (59830.7464)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
[2025-01-13 05:33:53,164] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60318
[2025-01-13 05:33:53,165] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:33:53,165] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1330/2809]  eta: 0:09:11  lr: 0.000026  min_lr: 0.000000  loss: 3.7438 (3.8085)  loss_scale: 65536.0000 (59824.3727)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [21]  [1340/2809]  eta: 0:09:07  lr: 0.000026  min_lr: 0.000000  loss: 3.7633 (3.8081)  loss_scale: 32768.0000 (59622.6100)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [21]  [1350/2809]  eta: 0:09:03  lr: 0.000026  min_lr: 0.000000  loss: 3.7633 (3.8073)  loss_scale: 32768.0000 (59423.8342)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [21]  [1360/2809]  eta: 0:08:59  lr: 0.000026  min_lr: 0.000000  loss: 3.9259 (3.8080)  loss_scale: 32768.0000 (59227.9794)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [21]  [1370/2809]  eta: 0:08:56  lr: 0.000026  min_lr: 0.000000  loss: 3.9989 (3.8078)  loss_scale: 32768.0000 (59034.9818)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [21]  [1380/2809]  eta: 0:08:52  lr: 0.000026  min_lr: 0.000000  loss: 3.9530 (3.8092)  loss_scale: 32768.0000 (58844.7791)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [21]  [1390/2809]  eta: 0:08:48  lr: 0.000026  min_lr: 0.000000  loss: 3.9227 (3.8093)  loss_scale: 32768.0000 (58657.3113)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [21]  [1400/2809]  eta: 0:08:44  lr: 0.000026  min_lr: 0.000000  loss: 3.6229 (3.8096)  loss_scale: 32768.0000 (58472.5196)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [21]  [1410/2809]  eta: 0:08:41  lr: 0.000026  min_lr: 0.000000  loss: 3.7176 (3.8091)  loss_scale: 32768.0000 (58290.3473)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [21]  [1420/2809]  eta: 0:08:37  lr: 0.000026  min_lr: 0.000000  loss: 3.8152 (3.8094)  loss_scale: 32768.0000 (58110.7389)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [21]  [1430/2809]  eta: 0:08:33  lr: 0.000026  min_lr: 0.000000  loss: 3.8046 (3.8086)  loss_scale: 32768.0000 (57933.6408)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [21]  [1440/2809]  eta: 0:08:29  lr: 0.000026  min_lr: 0.000000  loss: 3.7521 (3.8087)  loss_scale: 32768.0000 (57759.0007)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [21]  [1450/2809]  eta: 0:08:26  lr: 0.000026  min_lr: 0.000000  loss: 3.9912 (3.8096)  loss_scale: 32768.0000 (57586.7677)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
[2025-01-13 05:34:40,975] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:34:40,975] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [1460/2809]  eta: 0:08:22  lr: 0.000026  min_lr: 0.000000  loss: 3.9912 (3.8091)  loss_scale: 32768.0000 (57484.1780)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [21]  [1470/2809]  eta: 0:08:18  lr: 0.000026  min_lr: 0.000000  loss: 3.7439 (3.8088)  loss_scale: 65536.0000 (57538.9150)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [21]  [1480/2809]  eta: 0:08:14  lr: 0.000026  min_lr: 0.000000  loss: 3.9658 (3.8089)  loss_scale: 65536.0000 (57592.9129)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [21]  [1490/2809]  eta: 0:08:11  lr: 0.000025  min_lr: 0.000000  loss: 3.7701 (3.8090)  loss_scale: 65536.0000 (57646.1865)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [21]  [1500/2809]  eta: 0:08:07  lr: 0.000025  min_lr: 0.000000  loss: 3.6805 (3.8082)  loss_scale: 65536.0000 (57698.7502)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [21]  [1510/2809]  eta: 0:08:03  lr: 0.000025  min_lr: 0.000000  loss: 3.6730 (3.8075)  loss_scale: 65536.0000 (57750.6181)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [21]  [1520/2809]  eta: 0:07:59  lr: 0.000025  min_lr: 0.000000  loss: 3.8817 (3.8081)  loss_scale: 65536.0000 (57801.8041)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [21]  [1530/2809]  eta: 0:07:56  lr: 0.000025  min_lr: 0.000000  loss: 3.9322 (3.8082)  loss_scale: 65536.0000 (57852.3214)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [21]  [1540/2809]  eta: 0:07:52  lr: 0.000025  min_lr: 0.000000  loss: 4.0252 (3.8104)  loss_scale: 65536.0000 (57902.1830)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [21]  [1550/2809]  eta: 0:07:48  lr: 0.000025  min_lr: 0.000000  loss: 4.1567 (3.8118)  loss_scale: 65536.0000 (57951.4017)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [21]  [1560/2809]  eta: 0:07:44  lr: 0.000025  min_lr: 0.000000  loss: 4.1055 (3.8133)  loss_scale: 65536.0000 (57999.9898)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [21]  [1570/2809]  eta: 0:07:41  lr: 0.000025  min_lr: 0.000000  loss: 3.8644 (3.8131)  loss_scale: 65536.0000 (58047.9593)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
[2025-01-13 05:35:25,014] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60566
[2025-01-13 05:35:25,015] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:35:25,015] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1580/2809]  eta: 0:07:37  lr: 0.000025  min_lr: 0.000000  loss: 3.8213 (3.8119)  loss_scale: 65536.0000 (58012.4175)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [21]  [1590/2809]  eta: 0:07:33  lr: 0.000025  min_lr: 0.000000  loss: 3.5773 (3.8112)  loss_scale: 32768.0000 (57853.7473)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
Epoch: [21]  [1600/2809]  eta: 0:07:30  lr: 0.000025  min_lr: 0.000000  loss: 3.7789 (3.8119)  loss_scale: 32768.0000 (57697.0593)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [21]  [1610/2809]  eta: 0:07:26  lr: 0.000025  min_lr: 0.000000  loss: 3.8472 (3.8119)  loss_scale: 32768.0000 (57542.3166)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0001  max mem: 15572
Epoch: [21]  [1620/2809]  eta: 0:07:22  lr: 0.000025  min_lr: 0.000000  loss: 3.8097 (3.8115)  loss_scale: 32768.0000 (57389.4830)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [21]  [1630/2809]  eta: 0:07:18  lr: 0.000025  min_lr: 0.000000  loss: 3.9004 (3.8117)  loss_scale: 32768.0000 (57238.5236)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [21]  [1640/2809]  eta: 0:07:15  lr: 0.000025  min_lr: 0.000000  loss: 3.6701 (3.8102)  loss_scale: 32768.0000 (57089.4040)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [21]  [1650/2809]  eta: 0:07:11  lr: 0.000025  min_lr: 0.000000  loss: 3.7734 (3.8118)  loss_scale: 32768.0000 (56942.0909)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [21]  [1660/2809]  eta: 0:07:07  lr: 0.000025  min_lr: 0.000000  loss: 3.9917 (3.8125)  loss_scale: 32768.0000 (56796.5515)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [21]  [1670/2809]  eta: 0:07:04  lr: 0.000025  min_lr: 0.000000  loss: 3.9917 (3.8138)  loss_scale: 32768.0000 (56652.7540)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
Epoch: [21]  [1680/2809]  eta: 0:07:00  lr: 0.000025  min_lr: 0.000000  loss: 3.8981 (3.8143)  loss_scale: 32768.0000 (56510.6675)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [21]  [1690/2809]  eta: 0:06:56  lr: 0.000025  min_lr: 0.000000  loss: 4.0080 (3.8149)  loss_scale: 32768.0000 (56370.2614)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [21]  [1700/2809]  eta: 0:06:52  lr: 0.000025  min_lr: 0.000000  loss: 4.0080 (3.8159)  loss_scale: 32768.0000 (56231.5062)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
[2025-01-13 05:36:13,070] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:36:13,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [1710/2809]  eta: 0:06:49  lr: 0.000025  min_lr: 0.000000  loss: 3.8716 (3.8151)  loss_scale: 32768.0000 (56190.1297)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [21]  [1720/2809]  eta: 0:06:45  lr: 0.000025  min_lr: 0.000000  loss: 3.9707 (3.8158)  loss_scale: 65536.0000 (56244.4346)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [21]  [1730/2809]  eta: 0:06:41  lr: 0.000025  min_lr: 0.000000  loss: 3.9979 (3.8161)  loss_scale: 65536.0000 (56298.1121)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [21]  [1740/2809]  eta: 0:06:37  lr: 0.000025  min_lr: 0.000000  loss: 3.9453 (3.8155)  loss_scale: 65536.0000 (56351.1729)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [21]  [1750/2809]  eta: 0:06:34  lr: 0.000025  min_lr: 0.000000  loss: 3.9147 (3.8169)  loss_scale: 65536.0000 (56403.6276)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [21]  [1760/2809]  eta: 0:06:30  lr: 0.000025  min_lr: 0.000000  loss: 4.0797 (3.8174)  loss_scale: 65536.0000 (56455.4867)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [21]  [1770/2809]  eta: 0:06:26  lr: 0.000025  min_lr: 0.000000  loss: 4.0797 (3.8180)  loss_scale: 65536.0000 (56506.7600)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [21]  [1780/2809]  eta: 0:06:23  lr: 0.000025  min_lr: 0.000000  loss: 3.6515 (3.8168)  loss_scale: 65536.0000 (56557.4576)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
Epoch: [21]  [1790/2809]  eta: 0:06:19  lr: 0.000025  min_lr: 0.000000  loss: 3.6035 (3.8160)  loss_scale: 65536.0000 (56607.5891)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [21]  [1800/2809]  eta: 0:06:15  lr: 0.000025  min_lr: 0.000000  loss: 3.6035 (3.8152)  loss_scale: 65536.0000 (56657.1638)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [21]  [1810/2809]  eta: 0:06:11  lr: 0.000025  min_lr: 0.000000  loss: 3.8554 (3.8163)  loss_scale: 65536.0000 (56706.1911)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [21]  [1820/2809]  eta: 0:06:08  lr: 0.000025  min_lr: 0.000000  loss: 3.9847 (3.8165)  loss_scale: 65536.0000 (56754.6798)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [21]  [1830/2809]  eta: 0:06:04  lr: 0.000025  min_lr: 0.000000  loss: 3.7952 (3.8168)  loss_scale: 65536.0000 (56802.6390)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
[2025-01-13 05:37:00,648] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:37:00,648] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:37:02,519] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60828
[2025-01-13 05:37:02,519] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:37:02,520] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [1840/2809]  eta: 0:06:00  lr: 0.000025  min_lr: 0.000000  loss: 3.8697 (3.8171)  loss_scale: 65536.0000 (57028.0674)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [21]  [1850/2809]  eta: 0:05:56  lr: 0.000025  min_lr: 0.000000  loss: 3.8697 (3.8186)  loss_scale: 65536.0000 (57074.0313)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0002  max mem: 15572
Epoch: [21]  [1860/2809]  eta: 0:05:53  lr: 0.000025  min_lr: 0.000000  loss: 3.8371 (3.8179)  loss_scale: 65536.0000 (57119.5013)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0003  max mem: 15572
Epoch: [21]  [1870/2809]  eta: 0:05:49  lr: 0.000025  min_lr: 0.000000  loss: 3.6142 (3.8156)  loss_scale: 65536.0000 (57164.4853)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [21]  [1880/2809]  eta: 0:05:45  lr: 0.000025  min_lr: 0.000000  loss: 3.5218 (3.8156)  loss_scale: 65536.0000 (57208.9910)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [21]  [1890/2809]  eta: 0:05:42  lr: 0.000025  min_lr: 0.000000  loss: 3.7385 (3.8147)  loss_scale: 65536.0000 (57253.0259)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [21]  [1900/2809]  eta: 0:05:38  lr: 0.000025  min_lr: 0.000000  loss: 3.5139 (3.8144)  loss_scale: 65536.0000 (57296.5976)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [21]  [1910/2809]  eta: 0:05:34  lr: 0.000025  min_lr: 0.000000  loss: 3.7786 (3.8144)  loss_scale: 65536.0000 (57339.7132)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [21]  [1920/2809]  eta: 0:05:30  lr: 0.000025  min_lr: 0.000000  loss: 3.9437 (3.8152)  loss_scale: 65536.0000 (57382.3800)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [21]  [1930/2809]  eta: 0:05:27  lr: 0.000025  min_lr: 0.000000  loss: 3.8612 (3.8151)  loss_scale: 65536.0000 (57424.6049)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [21]  [1940/2809]  eta: 0:05:23  lr: 0.000025  min_lr: 0.000000  loss: 3.7539 (3.8149)  loss_scale: 65536.0000 (57466.3946)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0002  max mem: 15572
[2025-01-13 05:37:41,949] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60934
[2025-01-13 05:37:41,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:37:41,949] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1950/2809]  eta: 0:05:19  lr: 0.000025  min_lr: 0.000000  loss: 3.5978 (3.8142)  loss_scale: 65536.0000 (57406.9831)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [21]  [1960/2809]  eta: 0:05:15  lr: 0.000025  min_lr: 0.000000  loss: 3.6413 (3.8138)  loss_scale: 32768.0000 (57281.3381)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [21]  [1970/2809]  eta: 0:05:12  lr: 0.000025  min_lr: 0.000000  loss: 3.6483 (3.8134)  loss_scale: 32768.0000 (57156.9680)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0003  max mem: 15572
Epoch: [21]  [1980/2809]  eta: 0:05:08  lr: 0.000025  min_lr: 0.000000  loss: 3.7311 (3.8137)  loss_scale: 32768.0000 (57033.8536)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [21]  [1990/2809]  eta: 0:05:04  lr: 0.000025  min_lr: 0.000000  loss: 3.8603 (3.8144)  loss_scale: 32768.0000 (56911.9759)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0003  max mem: 15572
Epoch: [21]  [2000/2809]  eta: 0:05:01  lr: 0.000025  min_lr: 0.000000  loss: 3.7920 (3.8135)  loss_scale: 32768.0000 (56791.3163)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0002  max mem: 15572
[2025-01-13 05:38:06,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=61000, skipped=415, lr=[2.432707986252642e-07, 2.432707986252642e-07, 3.4752971232180603e-07, 3.4752971232180603e-07, 4.964710176025801e-07, 4.964710176025801e-07, 7.092443108608287e-07, 7.092443108608287e-07, 1.0132061583726125e-06, 1.0132061583726125e-06, 1.4474373691037321e-06, 1.4474373691037321e-06, 2.067767670148189e-06, 2.067767670148189e-06, 2.9539538144974133e-06, 2.9539538144974133e-06, 4.2199340207105905e-06, 4.2199340207105905e-06, 6.028477172443701e-06, 6.028477172443701e-06, 8.612110246348144e-06, 8.612110246348144e-06, 1.2303014637640208e-05, 1.2303014637640208e-05, 1.7575735196628867e-05, 1.7575735196628867e-05, 2.5108193138041243e-05, 2.5108193138041243e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 05:38:06,170] [INFO] [timer.py:260:stop] epoch=0/micro_step=61000/global_step=61000, RunningAvgSamplesPerSec=29.897293474119856, CurrSamplesPerSec=32.54080753903088, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [21]  [2010/2809]  eta: 0:04:57  lr: 0.000025  min_lr: 0.000000  loss: 3.7920 (3.8133)  loss_scale: 32768.0000 (56671.8568)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [21]  [2020/2809]  eta: 0:04:53  lr: 0.000025  min_lr: 0.000000  loss: 3.8483 (3.8137)  loss_scale: 32768.0000 (56553.5794)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [21]  [2030/2809]  eta: 0:04:49  lr: 0.000025  min_lr: 0.000000  loss: 3.8583 (3.8134)  loss_scale: 32768.0000 (56436.4668)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [21]  [2040/2809]  eta: 0:04:46  lr: 0.000025  min_lr: 0.000000  loss: 3.7084 (3.8131)  loss_scale: 32768.0000 (56320.5017)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [21]  [2050/2809]  eta: 0:04:42  lr: 0.000025  min_lr: 0.000000  loss: 4.0015 (3.8146)  loss_scale: 32768.0000 (56205.6675)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [21]  [2060/2809]  eta: 0:04:38  lr: 0.000025  min_lr: 0.000000  loss: 3.9912 (3.8135)  loss_scale: 32768.0000 (56091.9476)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [21]  [2070/2809]  eta: 0:04:35  lr: 0.000025  min_lr: 0.000000  loss: 3.8892 (3.8141)  loss_scale: 32768.0000 (55979.3259)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
[2025-01-13 05:38:29,831] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:38:29,831] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [2080/2809]  eta: 0:04:31  lr: 0.000025  min_lr: 0.000000  loss: 3.8892 (3.8133)  loss_scale: 32768.0000 (55978.0106)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [21]  [2090/2809]  eta: 0:04:27  lr: 0.000025  min_lr: 0.000000  loss: 3.6074 (3.8122)  loss_scale: 65536.0000 (56023.7207)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [21]  [2100/2809]  eta: 0:04:23  lr: 0.000025  min_lr: 0.000000  loss: 3.7724 (3.8122)  loss_scale: 65536.0000 (56068.9957)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [21]  [2110/2809]  eta: 0:04:20  lr: 0.000025  min_lr: 0.000000  loss: 3.7055 (3.8120)  loss_scale: 65536.0000 (56113.8418)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [21]  [2120/2809]  eta: 0:04:16  lr: 0.000025  min_lr: 0.000000  loss: 3.8128 (3.8124)  loss_scale: 65536.0000 (56158.2650)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [21]  [2130/2809]  eta: 0:04:12  lr: 0.000025  min_lr: 0.000000  loss: 3.9400 (3.8125)  loss_scale: 65536.0000 (56202.2712)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [21]  [2140/2809]  eta: 0:04:08  lr: 0.000025  min_lr: 0.000000  loss: 3.7950 (3.8121)  loss_scale: 65536.0000 (56245.8664)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [21]  [2150/2809]  eta: 0:04:05  lr: 0.000025  min_lr: 0.000000  loss: 3.7950 (3.8115)  loss_scale: 65536.0000 (56289.0563)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0002  max mem: 15572
Epoch: [21]  [2160/2809]  eta: 0:04:01  lr: 0.000025  min_lr: 0.000000  loss: 3.8611 (3.8115)  loss_scale: 65536.0000 (56331.8464)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [21]  [2170/2809]  eta: 0:03:57  lr: 0.000025  min_lr: 0.000000  loss: 3.9152 (3.8126)  loss_scale: 65536.0000 (56374.2423)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [21]  [2180/2809]  eta: 0:03:54  lr: 0.000025  min_lr: 0.000000  loss: 3.8370 (3.8107)  loss_scale: 65536.0000 (56416.2494)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [21]  [2190/2809]  eta: 0:03:50  lr: 0.000025  min_lr: 0.000000  loss: 3.6194 (3.8116)  loss_scale: 65536.0000 (56457.8731)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [21]  [2200/2809]  eta: 0:03:46  lr: 0.000025  min_lr: 0.000000  loss: 3.8300 (3.8113)  loss_scale: 65536.0000 (56499.1186)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
[2025-01-13 05:39:17,441] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:39:17,441] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:39:18,183] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61193
[2025-01-13 05:39:18,183] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:39:18,183] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [2210/2809]  eta: 0:03:42  lr: 0.000025  min_lr: 0.000000  loss: 3.7829 (3.8110)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
Epoch: [21]  [2220/2809]  eta: 0:03:39  lr: 0.000025  min_lr: 0.000000  loss: 3.7784 (3.8109)  loss_scale: 65536.0000 (56639.5101)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [21]  [2230/2809]  eta: 0:03:35  lr: 0.000025  min_lr: 0.000000  loss: 3.8499 (3.8117)  loss_scale: 65536.0000 (56679.3868)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [21]  [2240/2809]  eta: 0:03:31  lr: 0.000025  min_lr: 0.000000  loss: 3.9221 (3.8116)  loss_scale: 65536.0000 (56718.9076)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [21]  [2250/2809]  eta: 0:03:27  lr: 0.000025  min_lr: 0.000000  loss: 3.8914 (3.8123)  loss_scale: 65536.0000 (56758.0773)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [21]  [2260/2809]  eta: 0:03:24  lr: 0.000025  min_lr: 0.000000  loss: 3.8914 (3.8124)  loss_scale: 65536.0000 (56796.9005)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [21]  [2270/2809]  eta: 0:03:20  lr: 0.000025  min_lr: 0.000000  loss: 3.9601 (3.8130)  loss_scale: 65536.0000 (56835.3818)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [21]  [2280/2809]  eta: 0:03:16  lr: 0.000025  min_lr: 0.000000  loss: 3.7668 (3.8126)  loss_scale: 65536.0000 (56873.5256)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [21]  [2290/2809]  eta: 0:03:13  lr: 0.000025  min_lr: 0.000000  loss: 3.8687 (3.8135)  loss_scale: 65536.0000 (56911.3365)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [21]  [2300/2809]  eta: 0:03:09  lr: 0.000025  min_lr: 0.000000  loss: 3.8863 (3.8124)  loss_scale: 65536.0000 (56948.8188)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
Epoch: [21]  [2310/2809]  eta: 0:03:05  lr: 0.000025  min_lr: 0.000000  loss: 3.8405 (3.8125)  loss_scale: 65536.0000 (56985.9766)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [21]  [2320/2809]  eta: 0:03:01  lr: 0.000025  min_lr: 0.000000  loss: 3.8001 (3.8124)  loss_scale: 65536.0000 (57022.8143)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [21]  [2330/2809]  eta: 0:02:58  lr: 0.000025  min_lr: 0.000000  loss: 3.7305 (3.8122)  loss_scale: 65536.0000 (57059.3359)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
[2025-01-13 05:40:06,107] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:40:06,107] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:40:06,483] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61323
[2025-01-13 05:40:06,483] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:40:06,483] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [2340/2809]  eta: 0:02:54  lr: 0.000025  min_lr: 0.000000  loss: 3.7948 (3.8122)  loss_scale: 65536.0000 (57123.5404)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0003  max mem: 15572
Epoch: [21]  [2350/2809]  eta: 0:02:50  lr: 0.000025  min_lr: 0.000000  loss: 3.7692 (3.8118)  loss_scale: 65536.0000 (57159.3228)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [21]  [2360/2809]  eta: 0:02:47  lr: 0.000025  min_lr: 0.000000  loss: 3.8143 (3.8123)  loss_scale: 65536.0000 (57194.8022)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [21]  [2370/2809]  eta: 0:02:43  lr: 0.000025  min_lr: 0.000000  loss: 3.8260 (3.8118)  loss_scale: 65536.0000 (57229.9823)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [21]  [2380/2809]  eta: 0:02:39  lr: 0.000025  min_lr: 0.000000  loss: 3.9029 (3.8122)  loss_scale: 65536.0000 (57264.8669)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [21]  [2390/2809]  eta: 0:02:35  lr: 0.000025  min_lr: 0.000000  loss: 3.8807 (3.8114)  loss_scale: 65536.0000 (57299.4596)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [21]  [2400/2809]  eta: 0:02:32  lr: 0.000025  min_lr: 0.000000  loss: 3.8009 (3.8116)  loss_scale: 65536.0000 (57333.7643)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [21]  [2410/2809]  eta: 0:02:28  lr: 0.000025  min_lr: 0.000000  loss: 3.8079 (3.8116)  loss_scale: 65536.0000 (57367.7843)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [21]  [2420/2809]  eta: 0:02:24  lr: 0.000025  min_lr: 0.000000  loss: 3.7980 (3.8117)  loss_scale: 65536.0000 (57401.5233)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [21]  [2430/2809]  eta: 0:02:21  lr: 0.000025  min_lr: 0.000000  loss: 3.6402 (3.8107)  loss_scale: 65536.0000 (57434.9848)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [21]  [2440/2809]  eta: 0:02:17  lr: 0.000025  min_lr: 0.000000  loss: 3.7040 (3.8112)  loss_scale: 65536.0000 (57468.1721)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [21]  [2450/2809]  eta: 0:02:13  lr: 0.000025  min_lr: 0.000000  loss: 3.9971 (3.8119)  loss_scale: 65536.0000 (57501.0885)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [21]  [2460/2809]  eta: 0:02:09  lr: 0.000025  min_lr: 0.000000  loss: 3.9159 (3.8114)  loss_scale: 65536.0000 (57533.7375)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
[2025-01-13 05:40:54,439] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:40:54,440] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:40:55,167] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61454
[2025-01-13 05:40:55,167] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:40:55,167] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [2470/2809]  eta: 0:02:06  lr: 0.000025  min_lr: 0.000000  loss: 3.7841 (3.8111)  loss_scale: 65536.0000 (57619.1663)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0003  max mem: 15572
Epoch: [21]  [2480/2809]  eta: 0:02:02  lr: 0.000025  min_lr: 0.000000  loss: 3.6664 (3.8099)  loss_scale: 65536.0000 (57651.0762)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [21]  [2490/2809]  eta: 0:01:58  lr: 0.000025  min_lr: 0.000000  loss: 3.7606 (3.8101)  loss_scale: 65536.0000 (57682.7298)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [21]  [2500/2809]  eta: 0:01:54  lr: 0.000025  min_lr: 0.000000  loss: 3.8548 (3.8095)  loss_scale: 65536.0000 (57714.1303)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [21]  [2510/2809]  eta: 0:01:51  lr: 0.000025  min_lr: 0.000000  loss: 3.7288 (3.8088)  loss_scale: 65536.0000 (57745.2808)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [21]  [2520/2809]  eta: 0:01:47  lr: 0.000025  min_lr: 0.000000  loss: 3.6280 (3.8079)  loss_scale: 65536.0000 (57776.1841)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [21]  [2530/2809]  eta: 0:01:43  lr: 0.000025  min_lr: 0.000000  loss: 3.8237 (3.8083)  loss_scale: 65536.0000 (57806.8431)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [21]  [2540/2809]  eta: 0:01:40  lr: 0.000025  min_lr: 0.000000  loss: 3.8669 (3.8082)  loss_scale: 65536.0000 (57837.2609)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [21]  [2550/2809]  eta: 0:01:36  lr: 0.000025  min_lr: 0.000000  loss: 3.6874 (3.8083)  loss_scale: 65536.0000 (57867.4402)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [21]  [2560/2809]  eta: 0:01:32  lr: 0.000025  min_lr: 0.000000  loss: 3.8484 (3.8090)  loss_scale: 65536.0000 (57897.3838)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
Epoch: [21]  [2570/2809]  eta: 0:01:28  lr: 0.000025  min_lr: 0.000000  loss: 3.8502 (3.8088)  loss_scale: 65536.0000 (57927.0945)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [21]  [2580/2809]  eta: 0:01:25  lr: 0.000025  min_lr: 0.000000  loss: 3.8502 (3.8092)  loss_scale: 65536.0000 (57956.5750)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [21]  [2590/2809]  eta: 0:01:21  lr: 0.000025  min_lr: 0.000000  loss: 3.9971 (3.8093)  loss_scale: 65536.0000 (57985.8279)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
[2025-01-13 05:41:43,214] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:41:43,214] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [2600/2809]  eta: 0:01:17  lr: 0.000025  min_lr: 0.000000  loss: 3.7902 (3.8093)  loss_scale: 65536.0000 (58191.2311)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
[2025-01-13 05:41:46,134] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61591
[2025-01-13 05:41:46,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:41:46,135] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [2610/2809]  eta: 0:01:14  lr: 0.000025  min_lr: 0.000000  loss: 3.7533 (3.8088)  loss_scale: 65536.0000 (58244.4611)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [21]  [2620/2809]  eta: 0:01:10  lr: 0.000025  min_lr: 0.000000  loss: 3.7010 (3.8088)  loss_scale: 65536.0000 (58272.2808)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [21]  [2630/2809]  eta: 0:01:06  lr: 0.000025  min_lr: 0.000000  loss: 3.7302 (3.8087)  loss_scale: 65536.0000 (58299.8890)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [21]  [2640/2809]  eta: 0:01:02  lr: 0.000025  min_lr: 0.000000  loss: 3.7687 (3.8090)  loss_scale: 65536.0000 (58327.2881)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [21]  [2650/2809]  eta: 0:00:59  lr: 0.000025  min_lr: 0.000000  loss: 3.8764 (3.8092)  loss_scale: 65536.0000 (58354.4806)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [21]  [2660/2809]  eta: 0:00:55  lr: 0.000025  min_lr: 0.000000  loss: 3.8618 (3.8093)  loss_scale: 65536.0000 (58381.4686)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [21]  [2670/2809]  eta: 0:00:51  lr: 0.000025  min_lr: 0.000000  loss: 4.0069 (3.8099)  loss_scale: 65536.0000 (58408.2546)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [21]  [2680/2809]  eta: 0:00:47  lr: 0.000025  min_lr: 0.000000  loss: 4.0733 (3.8101)  loss_scale: 65536.0000 (58434.8407)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [21]  [2690/2809]  eta: 0:00:44  lr: 0.000025  min_lr: 0.000000  loss: 3.9381 (3.8101)  loss_scale: 65536.0000 (58461.2293)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [21]  [2700/2809]  eta: 0:00:40  lr: 0.000025  min_lr: 0.000000  loss: 3.6570 (3.8088)  loss_scale: 65536.0000 (58487.4224)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [21]  [2710/2809]  eta: 0:00:36  lr: 0.000025  min_lr: 0.000000  loss: 3.7662 (3.8093)  loss_scale: 65536.0000 (58513.4224)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [21]  [2720/2809]  eta: 0:00:33  lr: 0.000025  min_lr: 0.000000  loss: 3.9481 (3.8091)  loss_scale: 65536.0000 (58539.2312)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [21]  [2730/2809]  eta: 0:00:29  lr: 0.000025  min_lr: 0.000000  loss: 3.6921 (3.8086)  loss_scale: 65536.0000 (58564.8510)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
[2025-01-13 05:42:34,091] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:42:34,091] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:42:34,828] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61722
[2025-01-13 05:42:34,828] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:42:34,828] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [2740/2809]  eta: 0:00:25  lr: 0.000025  min_lr: 0.000000  loss: 3.8044 (3.8089)  loss_scale: 65536.0000 (58638.1029)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0002  max mem: 15572
Epoch: [21]  [2750/2809]  eta: 0:00:21  lr: 0.000025  min_lr: 0.000000  loss: 3.8449 (3.8088)  loss_scale: 65536.0000 (58663.1770)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [21]  [2760/2809]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000000  loss: 3.8916 (3.8094)  loss_scale: 65536.0000 (58688.0695)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [21]  [2770/2809]  eta: 0:00:14  lr: 0.000025  min_lr: 0.000000  loss: 3.8692 (3.8094)  loss_scale: 65536.0000 (58712.7824)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
[2025-01-13 05:42:50,532] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61764
[2025-01-13 05:42:50,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:42:50,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [2780/2809]  eta: 0:00:10  lr: 0.000025  min_lr: 0.000000  loss: 3.5914 (3.8079)  loss_scale: 65536.0000 (58666.6206)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
Epoch: [21]  [2790/2809]  eta: 0:00:07  lr: 0.000025  min_lr: 0.000000  loss: 3.6217 (3.8076)  loss_scale: 32768.0000 (58573.8273)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0003  max mem: 15572
Epoch: [21]  [2800/2809]  eta: 0:00:03  lr: 0.000025  min_lr: 0.000000  loss: 3.8207 (3.8082)  loss_scale: 32768.0000 (58481.6965)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [21]  [2808/2809]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000000  loss: 3.9301 (3.8081)  loss_scale: 32768.0000 (58408.4642)  weight_decay: 0.0500 (0.0500)  time: 0.3604  data: 0.0001  max mem: 15572
Epoch: [21] Total time: 0:17:25 (0.3721 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000000  loss: 3.9301 (3.8081)  loss_scale: 32768.0000 (58408.4642)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:08:54  loss: 0.3545 (0.3545)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.9633  data: 1.7961  max mem: 15572
Val:  [ 10/272]  eta: 0:01:42  loss: 2.6452 (2.4766)  acc1: 38.8889 (39.3939)  acc5: 66.6667 (67.6768)  time: 0.3918  data: 0.2370  max mem: 15572
Val:  [ 20/272]  eta: 0:01:12  loss: 2.5834 (2.4911)  acc1: 44.4444 (43.6508)  acc5: 66.6667 (69.8413)  time: 0.2039  data: 0.0517  max mem: 15572
Val:  [ 30/272]  eta: 0:00:59  loss: 2.4376 (2.5542)  acc1: 44.4444 (40.3226)  acc5: 72.2222 (69.5341)  time: 0.1661  data: 0.0113  max mem: 15572
Val:  [ 40/272]  eta: 0:00:52  loss: 2.4931 (2.5753)  acc1: 27.7778 (37.6694)  acc5: 72.2222 (70.4607)  time: 0.1605  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:47  loss: 2.4897 (2.5103)  acc1: 27.7778 (38.8889)  acc5: 77.7778 (72.6580)  time: 0.1612  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:43  loss: 1.7270 (2.3876)  acc1: 61.1111 (42.6230)  acc5: 88.8889 (74.0437)  time: 0.1653  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:40  loss: 1.5457 (2.2994)  acc1: 66.6667 (45.3052)  acc5: 83.3333 (75.2739)  time: 0.1678  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:37  loss: 1.8841 (2.3161)  acc1: 55.5556 (44.9246)  acc5: 77.7778 (74.5542)  time: 0.1651  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:35  loss: 2.3567 (2.3412)  acc1: 44.4444 (44.4444)  acc5: 77.7778 (74.6032)  time: 0.1652  data: 0.0026  max mem: 15572
Val:  [100/272]  eta: 0:00:32  loss: 2.3464 (2.3690)  acc1: 44.4444 (43.8944)  acc5: 77.7778 (74.2574)  time: 0.1633  data: 0.0026  max mem: 15572
Val:  [110/272]  eta: 0:00:30  loss: 2.5447 (2.4390)  acc1: 33.3333 (42.3924)  acc5: 72.2222 (73.3734)  time: 0.1607  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 3.0513 (2.4771)  acc1: 27.7778 (41.6896)  acc5: 66.6667 (72.7273)  time: 0.1577  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.3941 (2.4333)  acc1: 50.0000 (42.9177)  acc5: 77.7778 (73.6641)  time: 0.1588  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.6815 (2.4222)  acc1: 55.5556 (43.4988)  acc5: 83.3333 (73.7195)  time: 0.1608  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.4002 (2.4258)  acc1: 33.3333 (42.7888)  acc5: 77.7778 (74.1722)  time: 0.1569  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.4002 (2.4128)  acc1: 38.8889 (43.4438)  acc5: 77.7778 (74.5687)  time: 0.1506  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.4754 (2.4339)  acc1: 44.4444 (42.7225)  acc5: 72.2222 (74.0741)  time: 0.1483  data: 0.0003  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 2.4087 (2.4217)  acc1: 27.7778 (42.6028)  acc5: 72.2222 (74.5242)  time: 0.1506  data: 0.0003  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.4720 (2.4737)  acc1: 27.7778 (41.1867)  acc5: 72.2222 (73.0948)  time: 0.1536  data: 0.0003  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.6360 (2.4791)  acc1: 27.7778 (41.1553)  acc5: 61.1111 (72.9685)  time: 0.1563  data: 0.0003  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.0406 (2.4774)  acc1: 55.5556 (41.6798)  acc5: 77.7778 (72.8805)  time: 0.1614  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.2792 (2.4662)  acc1: 55.5556 (42.1569)  acc5: 72.2222 (73.0015)  time: 0.1669  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.8912 (2.4355)  acc1: 61.1111 (43.2900)  acc5: 77.7778 (73.4007)  time: 0.1712  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.8002 (2.4219)  acc1: 55.5556 (43.4302)  acc5: 83.3333 (73.6976)  time: 0.1632  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.4762 (2.4353)  acc1: 33.3333 (42.6516)  acc5: 72.2222 (73.5945)  time: 0.1568  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.3123 (2.3743)  acc1: 72.2222 (44.4232)  acc5: 88.8889 (74.3934)  time: 0.1535  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4045 (2.3699)  acc1: 61.1111 (44.2189)  acc5: 88.8889 (74.6822)  time: 0.1390  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4045 (2.3748)  acc1: 61.1111 (44.1941)  acc5: 88.8889 (74.6467)  time: 0.1338  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1686 s / it)
* Acc@1 44.194 Acc@5 74.647 loss 2.375
Accuracy of the network on the 4883 val videos: 44.2%
[2025-01-13 05:43:48,569] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 05:43:48,571] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 05:43:48,571] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 05:43:50,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 05:43:50,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.19%
Epoch: [22]  [   0/2809]  eta: 2:56:27  lr: 0.000025  min_lr: 0.000000  loss: 3.7753 (3.7753)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 3.7691  data: 3.3889  max mem: 15572
Epoch: [22]  [  10/2809]  eta: 0:32:50  lr: 0.000025  min_lr: 0.000000  loss: 3.8281 (3.8518)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7038  data: 0.3255  max mem: 15572
Epoch: [22]  [  20/2809]  eta: 0:25:21  lr: 0.000024  min_lr: 0.000000  loss: 3.7133 (3.7720)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3843  data: 0.0097  max mem: 15572
Epoch: [22]  [  30/2809]  eta: 0:22:42  lr: 0.000024  min_lr: 0.000000  loss: 3.7133 (3.7379)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [22]  [  40/2809]  eta: 0:21:16  lr: 0.000024  min_lr: 0.000000  loss: 3.7618 (3.7064)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [22]  [  50/2809]  eta: 0:20:21  lr: 0.000024  min_lr: 0.000000  loss: 3.5832 (3.6778)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [22]  [  60/2809]  eta: 0:19:42  lr: 0.000024  min_lr: 0.000000  loss: 3.6541 (3.6890)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [22]  [  70/2809]  eta: 0:19:13  lr: 0.000024  min_lr: 0.000000  loss: 3.8798 (3.7408)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [22]  [  80/2809]  eta: 0:18:53  lr: 0.000024  min_lr: 0.000000  loss: 3.9839 (3.7486)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [22]  [  90/2809]  eta: 0:18:35  lr: 0.000024  min_lr: 0.000000  loss: 3.7602 (3.7614)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
[2025-01-13 05:44:30,167] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:44:30,168] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 100/2809]  eta: 0:18:21  lr: 0.000024  min_lr: 0.000000  loss: 4.0040 (3.7804)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [22]  [ 110/2809]  eta: 0:18:08  lr: 0.000024  min_lr: 0.000000  loss: 3.8743 (3.7874)  loss_scale: 65536.0000 (37491.3153)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [22]  [ 120/2809]  eta: 0:17:56  lr: 0.000024  min_lr: 0.000000  loss: 3.8301 (3.8005)  loss_scale: 65536.0000 (39809.0579)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [22]  [ 130/2809]  eta: 0:17:46  lr: 0.000024  min_lr: 0.000000  loss: 3.8301 (3.8084)  loss_scale: 65536.0000 (41772.9466)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [22]  [ 140/2809]  eta: 0:17:36  lr: 0.000024  min_lr: 0.000000  loss: 3.9895 (3.8183)  loss_scale: 65536.0000 (43458.2695)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [22]  [ 150/2809]  eta: 0:17:27  lr: 0.000024  min_lr: 0.000000  loss: 4.0321 (3.8365)  loss_scale: 65536.0000 (44920.3709)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [22]  [ 160/2809]  eta: 0:17:19  lr: 0.000024  min_lr: 0.000000  loss: 3.8446 (3.8371)  loss_scale: 65536.0000 (46200.8447)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [22]  [ 170/2809]  eta: 0:17:11  lr: 0.000024  min_lr: 0.000000  loss: 3.7677 (3.8389)  loss_scale: 65536.0000 (47331.5556)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [22]  [ 180/2809]  eta: 0:17:04  lr: 0.000024  min_lr: 0.000000  loss: 3.8931 (3.8290)  loss_scale: 65536.0000 (48337.3260)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [22]  [ 190/2809]  eta: 0:16:57  lr: 0.000024  min_lr: 0.000000  loss: 3.7179 (3.8156)  loss_scale: 65536.0000 (49237.7801)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [22]  [ 200/2809]  eta: 0:16:51  lr: 0.000024  min_lr: 0.000000  loss: 3.7179 (3.8162)  loss_scale: 65536.0000 (50048.6368)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
[2025-01-13 05:45:09,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=62000, skipped=421, lr=[2.360327889193882e-07, 2.360327889193882e-07, 3.371896984562689e-07, 3.371896984562689e-07, 4.816995692232414e-07, 4.816995692232414e-07, 6.881422417474877e-07, 6.881422417474877e-07, 9.830603453535539e-07, 9.830603453535539e-07, 1.4043719219336484e-06, 1.4043719219336484e-06, 2.006245602762355e-06, 2.006245602762355e-06, 2.8660651468033645e-06, 2.8660651468033645e-06, 4.094378781147663e-06, 4.094378781147663e-06, 5.849112544496663e-06, 5.849112544496663e-06, 8.35587506356666e-06, 8.35587506356666e-06, 1.1936964376523803e-05, 1.1936964376523803e-05, 1.705280625217686e-05, 1.705280625217686e-05, 2.436115178882409e-05, 2.436115178882409e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 05:45:09,270] [INFO] [timer.py:260:stop] epoch=0/micro_step=62000/global_step=62000, RunningAvgSamplesPerSec=29.95798759934048, CurrSamplesPerSec=34.80658737875491, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [ 210/2809]  eta: 0:16:45  lr: 0.000024  min_lr: 0.000000  loss: 3.9053 (3.8177)  loss_scale: 65536.0000 (50782.6351)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [22]  [ 220/2809]  eta: 0:16:39  lr: 0.000024  min_lr: 0.000000  loss: 3.9053 (3.8152)  loss_scale: 65536.0000 (51450.2081)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
[2025-01-13 05:45:17,449] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:45:17,449] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:45:17,815] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62022
[2025-01-13 05:45:17,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:45:17,815] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [ 230/2809]  eta: 0:16:34  lr: 0.000024  min_lr: 0.000000  loss: 3.6752 (3.8079)  loss_scale: 65536.0000 (52343.6883)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [22]  [ 240/2809]  eta: 0:16:28  lr: 0.000024  min_lr: 0.000000  loss: 3.7365 (3.8086)  loss_scale: 65536.0000 (52891.0871)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [22]  [ 250/2809]  eta: 0:16:23  lr: 0.000024  min_lr: 0.000000  loss: 3.8170 (3.8079)  loss_scale: 65536.0000 (53394.8685)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [22]  [ 260/2809]  eta: 0:16:18  lr: 0.000024  min_lr: 0.000000  loss: 3.8648 (3.8164)  loss_scale: 65536.0000 (53860.0460)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [22]  [ 270/2809]  eta: 0:16:12  lr: 0.000024  min_lr: 0.000000  loss: 3.7999 (3.8141)  loss_scale: 65536.0000 (54290.8930)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [22]  [ 280/2809]  eta: 0:16:08  lr: 0.000024  min_lr: 0.000000  loss: 3.6236 (3.8131)  loss_scale: 65536.0000 (54691.0747)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [22]  [ 290/2809]  eta: 0:16:03  lr: 0.000024  min_lr: 0.000000  loss: 3.9547 (3.8169)  loss_scale: 65536.0000 (55063.7526)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0003  max mem: 15572
Epoch: [22]  [ 300/2809]  eta: 0:15:57  lr: 0.000024  min_lr: 0.000000  loss: 3.9547 (3.8119)  loss_scale: 65536.0000 (55411.6678)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [22]  [ 310/2809]  eta: 0:15:53  lr: 0.000024  min_lr: 0.000000  loss: 3.5826 (3.8038)  loss_scale: 65536.0000 (55737.2090)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [22]  [ 320/2809]  eta: 0:15:48  lr: 0.000024  min_lr: 0.000000  loss: 3.6707 (3.8079)  loss_scale: 65536.0000 (56042.4673)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [22]  [ 330/2809]  eta: 0:15:44  lr: 0.000024  min_lr: 0.000000  loss: 3.7722 (3.8068)  loss_scale: 65536.0000 (56329.2810)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0003  max mem: 15572
Epoch: [22]  [ 340/2809]  eta: 0:15:39  lr: 0.000024  min_lr: 0.000000  loss: 3.7393 (3.8052)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0003  max mem: 15572
Epoch: [22]  [ 350/2809]  eta: 0:15:34  lr: 0.000024  min_lr: 0.000000  loss: 3.7393 (3.8065)  loss_scale: 65536.0000 (56853.8803)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
[2025-01-13 05:46:05,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:46:05,538] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:46:07,789] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62157
[2025-01-13 05:46:07,789] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:46:07,789] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [ 360/2809]  eta: 0:15:30  lr: 0.000024  min_lr: 0.000000  loss: 3.5854 (3.8049)  loss_scale: 65536.0000 (58183.6233)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [22]  [ 370/2809]  eta: 0:15:25  lr: 0.000024  min_lr: 0.000000  loss: 3.6667 (3.8031)  loss_scale: 65536.0000 (58381.8005)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [22]  [ 380/2809]  eta: 0:15:20  lr: 0.000024  min_lr: 0.000000  loss: 3.6948 (3.8014)  loss_scale: 65536.0000 (58569.5748)  weight_decay: 0.0500 (0.0500)  time: 0.3648  data: 0.0002  max mem: 15572
Epoch: [22]  [ 390/2809]  eta: 0:15:17  lr: 0.000024  min_lr: 0.000000  loss: 3.7662 (3.8019)  loss_scale: 65536.0000 (58747.7442)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [22]  [ 400/2809]  eta: 0:15:12  lr: 0.000024  min_lr: 0.000000  loss: 3.5007 (3.7950)  loss_scale: 65536.0000 (58917.0274)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [22]  [ 410/2809]  eta: 0:15:08  lr: 0.000024  min_lr: 0.000000  loss: 3.6723 (3.7923)  loss_scale: 65536.0000 (59078.0730)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [22]  [ 420/2809]  eta: 0:15:04  lr: 0.000024  min_lr: 0.000000  loss: 3.7143 (3.7876)  loss_scale: 65536.0000 (59231.4679)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [22]  [ 430/2809]  eta: 0:15:00  lr: 0.000024  min_lr: 0.000000  loss: 3.8347 (3.7884)  loss_scale: 65536.0000 (59377.7448)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [22]  [ 440/2809]  eta: 0:14:55  lr: 0.000024  min_lr: 0.000000  loss: 3.8574 (3.7866)  loss_scale: 65536.0000 (59517.3878)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [22]  [ 450/2809]  eta: 0:14:51  lr: 0.000024  min_lr: 0.000000  loss: 3.9078 (3.7889)  loss_scale: 65536.0000 (59650.8381)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [22]  [ 460/2809]  eta: 0:14:47  lr: 0.000024  min_lr: 0.000000  loss: 3.7749 (3.7847)  loss_scale: 65536.0000 (59778.4989)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [22]  [ 470/2809]  eta: 0:14:43  lr: 0.000024  min_lr: 0.000000  loss: 3.7512 (3.7869)  loss_scale: 65536.0000 (59900.7389)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [22]  [ 480/2809]  eta: 0:14:39  lr: 0.000024  min_lr: 0.000000  loss: 3.6695 (3.7794)  loss_scale: 65536.0000 (60017.8960)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
[2025-01-13 05:46:55,523] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:46:55,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:46:55,914] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62287
[2025-01-13 05:46:55,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:46:55,914] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [ 490/2809]  eta: 0:14:35  lr: 0.000024  min_lr: 0.000000  loss: 3.5990 (3.7813)  loss_scale: 65536.0000 (60263.7556)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0003  max mem: 15572
Epoch: [22]  [ 500/2809]  eta: 0:14:31  lr: 0.000024  min_lr: 0.000000  loss: 3.8891 (3.7814)  loss_scale: 65536.0000 (60368.9900)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [22]  [ 510/2809]  eta: 0:14:27  lr: 0.000024  min_lr: 0.000000  loss: 3.7881 (3.7764)  loss_scale: 65536.0000 (60470.1057)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [22]  [ 520/2809]  eta: 0:14:23  lr: 0.000024  min_lr: 0.000000  loss: 3.7248 (3.7744)  loss_scale: 65536.0000 (60567.3397)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [22]  [ 530/2809]  eta: 0:14:18  lr: 0.000024  min_lr: 0.000000  loss: 3.9410 (3.7766)  loss_scale: 65536.0000 (60660.9115)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [22]  [ 540/2809]  eta: 0:14:14  lr: 0.000024  min_lr: 0.000000  loss: 3.9945 (3.7791)  loss_scale: 65536.0000 (60751.0240)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [22]  [ 550/2809]  eta: 0:14:10  lr: 0.000024  min_lr: 0.000000  loss: 3.9839 (3.7768)  loss_scale: 65536.0000 (60837.8657)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [22]  [ 560/2809]  eta: 0:14:06  lr: 0.000024  min_lr: 0.000000  loss: 3.8986 (3.7762)  loss_scale: 65536.0000 (60921.6114)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [22]  [ 570/2809]  eta: 0:14:02  lr: 0.000024  min_lr: 0.000000  loss: 3.8904 (3.7799)  loss_scale: 65536.0000 (61002.4238)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [22]  [ 580/2809]  eta: 0:13:58  lr: 0.000024  min_lr: 0.000000  loss: 3.6883 (3.7726)  loss_scale: 65536.0000 (61080.4544)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [22]  [ 590/2809]  eta: 0:13:54  lr: 0.000024  min_lr: 0.000000  loss: 3.7032 (3.7715)  loss_scale: 65536.0000 (61155.8443)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [22]  [ 600/2809]  eta: 0:13:50  lr: 0.000024  min_lr: 0.000000  loss: 3.8027 (3.7734)  loss_scale: 65536.0000 (61228.7255)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [22]  [ 610/2809]  eta: 0:13:46  lr: 0.000024  min_lr: 0.000000  loss: 3.7790 (3.7736)  loss_scale: 65536.0000 (61299.2209)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
[2025-01-13 05:47:43,722] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:47:43,722] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [22]  [ 620/2809]  eta: 0:13:42  lr: 0.000024  min_lr: 0.000000  loss: 3.7790 (3.7723)  loss_scale: 65536.0000 (61684.0451)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
[2025-01-13 05:47:45,948] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62422
[2025-01-13 05:47:45,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:47:45,948] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [ 630/2809]  eta: 0:13:38  lr: 0.000024  min_lr: 0.000000  loss: 3.9588 (3.7732)  loss_scale: 65536.0000 (62056.6719)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [22]  [ 640/2809]  eta: 0:13:34  lr: 0.000024  min_lr: 0.000000  loss: 3.8446 (3.7757)  loss_scale: 65536.0000 (62110.9516)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [22]  [ 650/2809]  eta: 0:13:30  lr: 0.000024  min_lr: 0.000000  loss: 3.8446 (3.7767)  loss_scale: 65536.0000 (62163.5637)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [22]  [ 660/2809]  eta: 0:13:27  lr: 0.000024  min_lr: 0.000000  loss: 3.9701 (3.7825)  loss_scale: 65536.0000 (62214.5840)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0002  max mem: 15572
Epoch: [22]  [ 670/2809]  eta: 0:13:23  lr: 0.000024  min_lr: 0.000000  loss: 3.9180 (3.7805)  loss_scale: 65536.0000 (62264.0835)  weight_decay: 0.0500 (0.0500)  time: 0.3791  data: 0.0015  max mem: 15572
Epoch: [22]  [ 680/2809]  eta: 0:13:19  lr: 0.000024  min_lr: 0.000000  loss: 3.8111 (3.7804)  loss_scale: 65536.0000 (62312.1292)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0015  max mem: 15572
Epoch: [22]  [ 690/2809]  eta: 0:13:15  lr: 0.000024  min_lr: 0.000000  loss: 3.8200 (3.7791)  loss_scale: 65536.0000 (62358.7844)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
[2025-01-13 05:48:11,311] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62490
[2025-01-13 05:48:11,311] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:48:11,311] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 700/2809]  eta: 0:13:11  lr: 0.000024  min_lr: 0.000000  loss: 3.8200 (3.7781)  loss_scale: 65536.0000 (61983.4066)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [22]  [ 710/2809]  eta: 0:13:07  lr: 0.000024  min_lr: 0.000000  loss: 3.9127 (3.7802)  loss_scale: 32768.0000 (61572.5007)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [22]  [ 720/2809]  eta: 0:13:04  lr: 0.000024  min_lr: 0.000000  loss: 3.7661 (3.7789)  loss_scale: 32768.0000 (61172.9931)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [22]  [ 730/2809]  eta: 0:13:00  lr: 0.000024  min_lr: 0.000000  loss: 3.7661 (3.7800)  loss_scale: 32768.0000 (60784.4159)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [22]  [ 740/2809]  eta: 0:12:56  lr: 0.000024  min_lr: 0.000000  loss: 3.8336 (3.7821)  loss_scale: 32768.0000 (60406.3266)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [22]  [ 750/2809]  eta: 0:12:52  lr: 0.000024  min_lr: 0.000000  loss: 3.8850 (3.7832)  loss_scale: 32768.0000 (60038.3063)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [22]  [ 760/2809]  eta: 0:12:48  lr: 0.000024  min_lr: 0.000000  loss: 3.8356 (3.7809)  loss_scale: 32768.0000 (59679.9580)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [22]  [ 770/2809]  eta: 0:12:44  lr: 0.000024  min_lr: 0.000000  loss: 3.5618 (3.7800)  loss_scale: 32768.0000 (59330.9053)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [22]  [ 780/2809]  eta: 0:12:41  lr: 0.000024  min_lr: 0.000000  loss: 3.6615 (3.7797)  loss_scale: 32768.0000 (58990.7913)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [22]  [ 790/2809]  eta: 0:12:37  lr: 0.000024  min_lr: 0.000000  loss: 3.6615 (3.7772)  loss_scale: 32768.0000 (58659.2769)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [22]  [ 800/2809]  eta: 0:12:33  lr: 0.000024  min_lr: 0.000000  loss: 3.5389 (3.7761)  loss_scale: 32768.0000 (58336.0400)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [22]  [ 810/2809]  eta: 0:12:29  lr: 0.000024  min_lr: 0.000000  loss: 3.7861 (3.7762)  loss_scale: 32768.0000 (58020.7744)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [22]  [ 820/2809]  eta: 0:12:25  lr: 0.000024  min_lr: 0.000000  loss: 3.8406 (3.7746)  loss_scale: 32768.0000 (57713.1888)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
[2025-01-13 05:48:59,111] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:48:59,111] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 830/2809]  eta: 0:12:21  lr: 0.000024  min_lr: 0.000000  loss: 3.4377 (3.7717)  loss_scale: 32768.0000 (57807.3261)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [22]  [ 840/2809]  eta: 0:12:17  lr: 0.000024  min_lr: 0.000000  loss: 3.7347 (3.7735)  loss_scale: 65536.0000 (57899.2247)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
[2025-01-13 05:49:07,922] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62643
[2025-01-13 05:49:07,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:49:07,922] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 850/2809]  eta: 0:12:13  lr: 0.000024  min_lr: 0.000000  loss: 3.8080 (3.7717)  loss_scale: 65536.0000 (57757.9318)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [22]  [ 860/2809]  eta: 0:12:09  lr: 0.000024  min_lr: 0.000000  loss: 3.8080 (3.7737)  loss_scale: 32768.0000 (57467.6887)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [22]  [ 870/2809]  eta: 0:12:06  lr: 0.000024  min_lr: 0.000000  loss: 3.9758 (3.7771)  loss_scale: 32768.0000 (57184.1102)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [22]  [ 880/2809]  eta: 0:12:02  lr: 0.000024  min_lr: 0.000000  loss: 4.0116 (3.7788)  loss_scale: 32768.0000 (56906.9694)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [22]  [ 890/2809]  eta: 0:11:58  lr: 0.000024  min_lr: 0.000000  loss: 3.8837 (3.7787)  loss_scale: 32768.0000 (56636.0494)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [22]  [ 900/2809]  eta: 0:11:54  lr: 0.000024  min_lr: 0.000000  loss: 3.8407 (3.7789)  loss_scale: 32768.0000 (56371.1432)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [22]  [ 910/2809]  eta: 0:11:50  lr: 0.000024  min_lr: 0.000000  loss: 3.7410 (3.7776)  loss_scale: 32768.0000 (56112.0527)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [22]  [ 920/2809]  eta: 0:11:46  lr: 0.000024  min_lr: 0.000000  loss: 3.8114 (3.7799)  loss_scale: 32768.0000 (55858.5885)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [22]  [ 930/2809]  eta: 0:11:43  lr: 0.000024  min_lr: 0.000000  loss: 3.9096 (3.7791)  loss_scale: 32768.0000 (55610.5693)  weight_decay: 0.0500 (0.0500)  time: 0.3647  data: 0.0002  max mem: 15572
Epoch: [22]  [ 940/2809]  eta: 0:11:39  lr: 0.000024  min_lr: 0.000000  loss: 3.6299 (3.7780)  loss_scale: 32768.0000 (55367.8215)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [22]  [ 950/2809]  eta: 0:11:35  lr: 0.000024  min_lr: 0.000000  loss: 3.6607 (3.7769)  loss_scale: 32768.0000 (55130.1788)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [22]  [ 960/2809]  eta: 0:11:31  lr: 0.000024  min_lr: 0.000000  loss: 3.5747 (3.7756)  loss_scale: 32768.0000 (54897.4818)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [22]  [ 970/2809]  eta: 0:11:27  lr: 0.000024  min_lr: 0.000000  loss: 3.9014 (3.7760)  loss_scale: 32768.0000 (54669.5778)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
[2025-01-13 05:49:55,610] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:49:55,611] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 980/2809]  eta: 0:11:23  lr: 0.000024  min_lr: 0.000000  loss: 4.0129 (3.7779)  loss_scale: 32768.0000 (54680.1386)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [22]  [ 990/2809]  eta: 0:11:19  lr: 0.000024  min_lr: 0.000000  loss: 4.0260 (3.7813)  loss_scale: 65536.0000 (54789.6831)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [22]  [1000/2809]  eta: 0:11:16  lr: 0.000024  min_lr: 0.000000  loss: 3.9284 (3.7802)  loss_scale: 65536.0000 (54897.0390)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [22]  [1010/2809]  eta: 0:11:12  lr: 0.000024  min_lr: 0.000000  loss: 3.7901 (3.7799)  loss_scale: 65536.0000 (55002.2710)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [22]  [1020/2809]  eta: 0:11:08  lr: 0.000024  min_lr: 0.000000  loss: 3.9090 (3.7799)  loss_scale: 65536.0000 (55105.4417)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [22]  [1030/2809]  eta: 0:11:04  lr: 0.000024  min_lr: 0.000000  loss: 3.9090 (3.7818)  loss_scale: 65536.0000 (55206.6111)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [22]  [1040/2809]  eta: 0:11:01  lr: 0.000024  min_lr: 0.000000  loss: 3.9893 (3.7830)  loss_scale: 65536.0000 (55305.8367)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [22]  [1050/2809]  eta: 0:10:57  lr: 0.000024  min_lr: 0.000000  loss: 3.8500 (3.7811)  loss_scale: 65536.0000 (55403.1741)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [22]  [1060/2809]  eta: 0:10:53  lr: 0.000024  min_lr: 0.000000  loss: 3.6129 (3.7814)  loss_scale: 65536.0000 (55498.6767)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [22]  [1070/2809]  eta: 0:10:49  lr: 0.000024  min_lr: 0.000000  loss: 3.8881 (3.7811)  loss_scale: 65536.0000 (55592.3959)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [22]  [1080/2809]  eta: 0:10:45  lr: 0.000024  min_lr: 0.000000  loss: 4.0357 (3.7827)  loss_scale: 65536.0000 (55684.3811)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [22]  [1090/2809]  eta: 0:10:42  lr: 0.000024  min_lr: 0.000000  loss: 3.7417 (3.7810)  loss_scale: 65536.0000 (55774.6801)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
Epoch: [22]  [1100/2809]  eta: 0:10:38  lr: 0.000024  min_lr: 0.000000  loss: 3.9439 (3.7839)  loss_scale: 65536.0000 (55863.3388)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
[2025-01-13 05:50:43,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:50:43,135] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:50:43,870] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62902
[2025-01-13 05:50:43,871] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:50:43,871] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [1110/2809]  eta: 0:10:34  lr: 0.000024  min_lr: 0.000000  loss: 4.0063 (3.7847)  loss_scale: 65536.0000 (56068.3780)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [22]  [1120/2809]  eta: 0:10:30  lr: 0.000024  min_lr: 0.000000  loss: 3.7967 (3.7836)  loss_scale: 65536.0000 (56152.8350)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 05:50:53,134] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62927
[2025-01-13 05:50:53,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:50:53,134] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1130/2809]  eta: 0:10:27  lr: 0.000024  min_lr: 0.000000  loss: 3.9037 (3.7868)  loss_scale: 65536.0000 (56177.8532)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [22]  [1140/2809]  eta: 0:10:23  lr: 0.000024  min_lr: 0.000000  loss: 4.0412 (3.7873)  loss_scale: 32768.0000 (55972.6836)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [22]  [1150/2809]  eta: 0:10:19  lr: 0.000024  min_lr: 0.000000  loss: 3.8748 (3.7877)  loss_scale: 32768.0000 (55771.0791)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [22]  [1160/2809]  eta: 0:10:15  lr: 0.000024  min_lr: 0.000000  loss: 3.7262 (3.7882)  loss_scale: 32768.0000 (55572.9475)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [22]  [1170/2809]  eta: 0:10:11  lr: 0.000024  min_lr: 0.000000  loss: 3.7699 (3.7877)  loss_scale: 32768.0000 (55378.1998)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [22]  [1180/2809]  eta: 0:10:08  lr: 0.000024  min_lr: 0.000000  loss: 3.8221 (3.7873)  loss_scale: 32768.0000 (55186.7502)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [22]  [1190/2809]  eta: 0:10:04  lr: 0.000024  min_lr: 0.000000  loss: 3.8203 (3.7862)  loss_scale: 32768.0000 (54998.5155)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
Epoch: [22]  [1200/2809]  eta: 0:10:00  lr: 0.000024  min_lr: 0.000000  loss: 3.7269 (3.7855)  loss_scale: 32768.0000 (54813.4155)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
[2025-01-13 05:51:19,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=63000, skipped=429, lr=[2.2878587401415915e-07, 2.2878587401415915e-07, 3.2683696287737025e-07, 3.2683696287737025e-07, 4.6690994696767183e-07, 4.6690994696767183e-07, 6.670142099538169e-07, 6.670142099538169e-07, 9.528774427911671e-07, 9.528774427911671e-07, 1.3612534897016674e-06, 1.3612534897016674e-06, 1.9446478424309535e-06, 1.9446478424309535e-06, 2.778068346329934e-06, 2.778068346329934e-06, 3.96866906618562e-06, 3.96866906618562e-06, 5.6695272374080294e-06, 5.6695272374080294e-06, 8.099324624868612e-06, 8.099324624868612e-06, 1.1570463749812306e-05, 1.1570463749812306e-05, 1.6529233928303293e-05, 1.6529233928303293e-05, 2.3613191326147565e-05, 2.3613191326147565e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 05:51:19,902] [INFO] [timer.py:260:stop] epoch=0/micro_step=63000/global_step=63000, RunningAvgSamplesPerSec=30.017656940544565, CurrSamplesPerSec=34.92682331061857, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [1210/2809]  eta: 0:09:57  lr: 0.000024  min_lr: 0.000000  loss: 3.5344 (3.7846)  loss_scale: 32768.0000 (54631.3724)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0014  max mem: 15572
Epoch: [22]  [1220/2809]  eta: 0:09:53  lr: 0.000024  min_lr: 0.000000  loss: 3.7594 (3.7853)  loss_scale: 32768.0000 (54452.3112)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0014  max mem: 15572
Epoch: [22]  [1230/2809]  eta: 0:09:49  lr: 0.000024  min_lr: 0.000000  loss: 3.9313 (3.7863)  loss_scale: 32768.0000 (54276.1592)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [22]  [1240/2809]  eta: 0:09:45  lr: 0.000024  min_lr: 0.000000  loss: 3.7427 (3.7856)  loss_scale: 32768.0000 (54102.8461)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [22]  [1250/2809]  eta: 0:09:41  lr: 0.000024  min_lr: 0.000000  loss: 3.7427 (3.7861)  loss_scale: 32768.0000 (53932.3038)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
[2025-01-13 05:51:41,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:51:41,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [1260/2809]  eta: 0:09:38  lr: 0.000024  min_lr: 0.000000  loss: 3.9304 (3.7863)  loss_scale: 32768.0000 (53842.4235)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [22]  [1270/2809]  eta: 0:09:34  lr: 0.000024  min_lr: 0.000000  loss: 3.8446 (3.7840)  loss_scale: 65536.0000 (53934.4264)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [22]  [1280/2809]  eta: 0:09:30  lr: 0.000024  min_lr: 0.000000  loss: 3.8446 (3.7859)  loss_scale: 65536.0000 (54024.9930)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [22]  [1290/2809]  eta: 0:09:26  lr: 0.000024  min_lr: 0.000000  loss: 4.0331 (3.7871)  loss_scale: 65536.0000 (54114.1565)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [22]  [1300/2809]  eta: 0:09:23  lr: 0.000024  min_lr: 0.000000  loss: 3.9812 (3.7892)  loss_scale: 65536.0000 (54201.9493)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [22]  [1310/2809]  eta: 0:09:19  lr: 0.000024  min_lr: 0.000000  loss: 3.7387 (3.7881)  loss_scale: 65536.0000 (54288.4027)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [22]  [1320/2809]  eta: 0:09:15  lr: 0.000024  min_lr: 0.000000  loss: 3.7387 (3.7887)  loss_scale: 65536.0000 (54373.5473)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [22]  [1330/2809]  eta: 0:09:11  lr: 0.000024  min_lr: 0.000000  loss: 3.9804 (3.7892)  loss_scale: 65536.0000 (54457.4125)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [22]  [1340/2809]  eta: 0:09:07  lr: 0.000024  min_lr: 0.000000  loss: 3.7614 (3.7893)  loss_scale: 65536.0000 (54540.0268)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [22]  [1350/2809]  eta: 0:09:04  lr: 0.000024  min_lr: 0.000000  loss: 3.6320 (3.7867)  loss_scale: 65536.0000 (54621.4182)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [22]  [1360/2809]  eta: 0:09:00  lr: 0.000023  min_lr: 0.000000  loss: 3.5132 (3.7857)  loss_scale: 65536.0000 (54701.6135)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [22]  [1370/2809]  eta: 0:08:56  lr: 0.000023  min_lr: 0.000000  loss: 3.8867 (3.7865)  loss_scale: 65536.0000 (54780.6389)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [22]  [1380/2809]  eta: 0:08:52  lr: 0.000023  min_lr: 0.000000  loss: 3.9334 (3.7871)  loss_scale: 65536.0000 (54858.5199)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
[2025-01-13 05:52:28,315] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:52:28,315] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:52:28,684] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63185
[2025-01-13 05:52:28,684] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:52:28,684] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [1390/2809]  eta: 0:08:49  lr: 0.000023  min_lr: 0.000000  loss: 3.9334 (3.7877)  loss_scale: 65536.0000 (54982.3954)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [22]  [1400/2809]  eta: 0:08:45  lr: 0.000023  min_lr: 0.000000  loss: 3.9432 (3.7881)  loss_scale: 65536.0000 (55057.7245)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [22]  [1410/2809]  eta: 0:08:41  lr: 0.000023  min_lr: 0.000000  loss: 3.7606 (3.7871)  loss_scale: 65536.0000 (55131.9858)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [22]  [1420/2809]  eta: 0:08:37  lr: 0.000023  min_lr: 0.000000  loss: 3.6872 (3.7874)  loss_scale: 65536.0000 (55205.2020)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [22]  [1430/2809]  eta: 0:08:34  lr: 0.000023  min_lr: 0.000000  loss: 3.8156 (3.7877)  loss_scale: 65536.0000 (55277.3948)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [22]  [1440/2809]  eta: 0:08:30  lr: 0.000023  min_lr: 0.000000  loss: 3.7897 (3.7882)  loss_scale: 65536.0000 (55348.5857)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [22]  [1450/2809]  eta: 0:08:26  lr: 0.000023  min_lr: 0.000000  loss: 3.7789 (3.7888)  loss_scale: 65536.0000 (55418.7953)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [22]  [1460/2809]  eta: 0:08:22  lr: 0.000023  min_lr: 0.000000  loss: 3.7403 (3.7893)  loss_scale: 65536.0000 (55488.0438)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [22]  [1470/2809]  eta: 0:08:19  lr: 0.000023  min_lr: 0.000000  loss: 3.9903 (3.7909)  loss_scale: 65536.0000 (55556.3508)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [22]  [1480/2809]  eta: 0:08:15  lr: 0.000023  min_lr: 0.000000  loss: 3.9903 (3.7899)  loss_scale: 65536.0000 (55623.7353)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
[2025-01-13 05:53:04,908] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63283
[2025-01-13 05:53:04,908] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:53:04,908] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1490/2809]  eta: 0:08:11  lr: 0.000023  min_lr: 0.000000  loss: 3.8115 (3.7892)  loss_scale: 65536.0000 (55558.3528)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [22]  [1500/2809]  eta: 0:08:07  lr: 0.000023  min_lr: 0.000000  loss: 3.8115 (3.7894)  loss_scale: 32768.0000 (55406.5183)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [22]  [1510/2809]  eta: 0:08:04  lr: 0.000023  min_lr: 0.000000  loss: 3.8651 (3.7911)  loss_scale: 32768.0000 (55256.6936)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [22]  [1520/2809]  eta: 0:08:00  lr: 0.000023  min_lr: 0.000000  loss: 3.7703 (3.7899)  loss_scale: 32768.0000 (55108.8389)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [22]  [1530/2809]  eta: 0:07:56  lr: 0.000023  min_lr: 0.000000  loss: 3.8245 (3.7905)  loss_scale: 32768.0000 (54962.9157)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [22]  [1540/2809]  eta: 0:07:52  lr: 0.000023  min_lr: 0.000000  loss: 3.9504 (3.7920)  loss_scale: 32768.0000 (54818.8864)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [22]  [1550/2809]  eta: 0:07:49  lr: 0.000023  min_lr: 0.000000  loss: 3.8763 (3.7927)  loss_scale: 32768.0000 (54676.7144)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [22]  [1560/2809]  eta: 0:07:45  lr: 0.000023  min_lr: 0.000000  loss: 3.7846 (3.7908)  loss_scale: 32768.0000 (54536.3639)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [22]  [1570/2809]  eta: 0:07:41  lr: 0.000023  min_lr: 0.000000  loss: 3.4904 (3.7906)  loss_scale: 32768.0000 (54397.8001)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [22]  [1580/2809]  eta: 0:07:37  lr: 0.000023  min_lr: 0.000000  loss: 3.9247 (3.7915)  loss_scale: 32768.0000 (54260.9892)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [22]  [1590/2809]  eta: 0:07:34  lr: 0.000023  min_lr: 0.000000  loss: 3.9584 (3.7919)  loss_scale: 32768.0000 (54125.8982)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [22]  [1600/2809]  eta: 0:07:30  lr: 0.000023  min_lr: 0.000000  loss: 3.9584 (3.7913)  loss_scale: 32768.0000 (53992.4947)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [22]  [1610/2809]  eta: 0:07:26  lr: 0.000023  min_lr: 0.000000  loss: 3.7934 (3.7916)  loss_scale: 32768.0000 (53860.7474)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
[2025-01-13 05:53:52,709] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:53:52,710] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [1620/2809]  eta: 0:07:22  lr: 0.000023  min_lr: 0.000000  loss: 3.7768 (3.7907)  loss_scale: 32768.0000 (53872.1283)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [22]  [1630/2809]  eta: 0:07:19  lr: 0.000023  min_lr: 0.000000  loss: 3.8328 (3.7918)  loss_scale: 65536.0000 (53943.6419)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [22]  [1640/2809]  eta: 0:07:15  lr: 0.000023  min_lr: 0.000000  loss: 3.9176 (3.7915)  loss_scale: 65536.0000 (54014.2840)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [22]  [1650/2809]  eta: 0:07:11  lr: 0.000023  min_lr: 0.000000  loss: 3.8533 (3.7924)  loss_scale: 65536.0000 (54084.0703)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [22]  [1660/2809]  eta: 0:07:07  lr: 0.000023  min_lr: 0.000000  loss: 3.9203 (3.7929)  loss_scale: 65536.0000 (54153.0163)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [22]  [1670/2809]  eta: 0:07:04  lr: 0.000023  min_lr: 0.000000  loss: 3.7788 (3.7921)  loss_scale: 65536.0000 (54221.1370)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [22]  [1680/2809]  eta: 0:07:00  lr: 0.000023  min_lr: 0.000000  loss: 3.6814 (3.7926)  loss_scale: 65536.0000 (54288.4474)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [22]  [1690/2809]  eta: 0:06:56  lr: 0.000023  min_lr: 0.000000  loss: 3.6785 (3.7922)  loss_scale: 65536.0000 (54354.9616)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [22]  [1700/2809]  eta: 0:06:52  lr: 0.000023  min_lr: 0.000000  loss: 3.8925 (3.7921)  loss_scale: 65536.0000 (54420.6937)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [22]  [1710/2809]  eta: 0:06:49  lr: 0.000023  min_lr: 0.000000  loss: 3.9576 (3.7933)  loss_scale: 65536.0000 (54485.6575)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [22]  [1720/2809]  eta: 0:06:45  lr: 0.000023  min_lr: 0.000000  loss: 3.8327 (3.7928)  loss_scale: 65536.0000 (54549.8664)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [22]  [1730/2809]  eta: 0:06:41  lr: 0.000023  min_lr: 0.000000  loss: 3.7342 (3.7925)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [22]  [1740/2809]  eta: 0:06:38  lr: 0.000023  min_lr: 0.000000  loss: 3.8116 (3.7931)  loss_scale: 65536.0000 (54676.0712)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
[2025-01-13 05:54:40,135] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:54:40,135] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:54:41,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63544
[2025-01-13 05:54:41,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:54:41,590] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [1750/2809]  eta: 0:06:34  lr: 0.000023  min_lr: 0.000000  loss: 3.8967 (3.7931)  loss_scale: 65536.0000 (54887.8035)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [22]  [1760/2809]  eta: 0:06:30  lr: 0.000023  min_lr: 0.000000  loss: 3.8409 (3.7937)  loss_scale: 65536.0000 (54948.2703)  weight_decay: 0.0500 (0.0500)  time: 0.3650  data: 0.0002  max mem: 15572
Epoch: [22]  [1770/2809]  eta: 0:06:26  lr: 0.000023  min_lr: 0.000000  loss: 3.9514 (3.7943)  loss_scale: 65536.0000 (55008.0542)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [22]  [1780/2809]  eta: 0:06:22  lr: 0.000023  min_lr: 0.000000  loss: 3.9372 (3.7948)  loss_scale: 65536.0000 (55067.1668)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [22]  [1790/2809]  eta: 0:06:19  lr: 0.000023  min_lr: 0.000000  loss: 3.7780 (3.7947)  loss_scale: 65536.0000 (55125.6192)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [22]  [1800/2809]  eta: 0:06:15  lr: 0.000023  min_lr: 0.000000  loss: 3.6860 (3.7938)  loss_scale: 65536.0000 (55183.4225)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [22]  [1810/2809]  eta: 0:06:11  lr: 0.000023  min_lr: 0.000000  loss: 3.7073 (3.7932)  loss_scale: 65536.0000 (55240.5875)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [22]  [1820/2809]  eta: 0:06:08  lr: 0.000023  min_lr: 0.000000  loss: 3.8419 (3.7943)  loss_scale: 65536.0000 (55297.1247)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [22]  [1830/2809]  eta: 0:06:04  lr: 0.000023  min_lr: 0.000000  loss: 3.8705 (3.7940)  loss_scale: 65536.0000 (55353.0442)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [22]  [1840/2809]  eta: 0:06:00  lr: 0.000023  min_lr: 0.000000  loss: 3.9262 (3.7940)  loss_scale: 65536.0000 (55408.3563)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [22]  [1850/2809]  eta: 0:05:56  lr: 0.000023  min_lr: 0.000000  loss: 3.7652 (3.7939)  loss_scale: 65536.0000 (55463.0708)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [22]  [1860/2809]  eta: 0:05:53  lr: 0.000023  min_lr: 0.000000  loss: 3.7528 (3.7938)  loss_scale: 65536.0000 (55517.1972)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [22]  [1870/2809]  eta: 0:05:49  lr: 0.000023  min_lr: 0.000000  loss: 4.0490 (3.7957)  loss_scale: 65536.0000 (55570.7451)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
[2025-01-13 05:55:29,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:55:29,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:55:30,336] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63676
[2025-01-13 05:55:30,337] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:55:30,337] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [1880/2809]  eta: 0:05:45  lr: 0.000023  min_lr: 0.000000  loss: 4.0490 (3.7963)  loss_scale: 65536.0000 (55728.2467)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [22]  [1890/2809]  eta: 0:05:41  lr: 0.000023  min_lr: 0.000000  loss: 3.9054 (3.7960)  loss_scale: 65536.0000 (55780.1121)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [22]  [1900/2809]  eta: 0:05:38  lr: 0.000023  min_lr: 0.000000  loss: 3.7466 (3.7957)  loss_scale: 65536.0000 (55831.4319)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
Epoch: [22]  [1910/2809]  eta: 0:05:34  lr: 0.000023  min_lr: 0.000000  loss: 3.8065 (3.7958)  loss_scale: 65536.0000 (55882.2145)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [22]  [1920/2809]  eta: 0:05:30  lr: 0.000023  min_lr: 0.000000  loss: 3.9812 (3.7968)  loss_scale: 65536.0000 (55932.4685)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0003  max mem: 15572
Epoch: [22]  [1930/2809]  eta: 0:05:27  lr: 0.000023  min_lr: 0.000000  loss: 3.9812 (3.7967)  loss_scale: 65536.0000 (55982.2020)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
[2025-01-13 05:55:50,385] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63730
[2025-01-13 05:55:50,385] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:55:50,385] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1940/2809]  eta: 0:05:23  lr: 0.000023  min_lr: 0.000000  loss: 3.9741 (3.7974)  loss_scale: 65536.0000 (55879.4848)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [22]  [1950/2809]  eta: 0:05:19  lr: 0.000023  min_lr: 0.000000  loss: 3.9175 (3.7974)  loss_scale: 32768.0000 (55761.0251)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [22]  [1960/2809]  eta: 0:05:15  lr: 0.000023  min_lr: 0.000000  loss: 3.9140 (3.7980)  loss_scale: 32768.0000 (55643.7736)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [22]  [1970/2809]  eta: 0:05:12  lr: 0.000023  min_lr: 0.000000  loss: 3.8488 (3.7981)  loss_scale: 32768.0000 (55527.7118)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [22]  [1980/2809]  eta: 0:05:08  lr: 0.000023  min_lr: 0.000000  loss: 3.6572 (3.7980)  loss_scale: 32768.0000 (55412.8218)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [22]  [1990/2809]  eta: 0:05:04  lr: 0.000023  min_lr: 0.000000  loss: 3.9472 (3.7985)  loss_scale: 32768.0000 (55299.0859)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [22]  [2000/2809]  eta: 0:05:00  lr: 0.000023  min_lr: 0.000000  loss: 3.9445 (3.7982)  loss_scale: 32768.0000 (55186.4868)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [22]  [2010/2809]  eta: 0:04:57  lr: 0.000023  min_lr: 0.000000  loss: 3.7770 (3.7978)  loss_scale: 32768.0000 (55075.0075)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [22]  [2020/2809]  eta: 0:04:53  lr: 0.000023  min_lr: 0.000000  loss: 3.6545 (3.7982)  loss_scale: 32768.0000 (54964.6314)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [22]  [2030/2809]  eta: 0:04:49  lr: 0.000023  min_lr: 0.000000  loss: 3.6601 (3.7979)  loss_scale: 32768.0000 (54855.3422)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [22]  [2040/2809]  eta: 0:04:45  lr: 0.000023  min_lr: 0.000000  loss: 3.6601 (3.7969)  loss_scale: 32768.0000 (54747.1240)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [22]  [2050/2809]  eta: 0:04:42  lr: 0.000023  min_lr: 0.000000  loss: 3.8438 (3.7979)  loss_scale: 32768.0000 (54639.9610)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [22]  [2060/2809]  eta: 0:04:38  lr: 0.000023  min_lr: 0.000000  loss: 3.8931 (3.7964)  loss_scale: 32768.0000 (54533.8379)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
[2025-01-13 05:56:38,076] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:56:38,076] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [2070/2809]  eta: 0:04:34  lr: 0.000023  min_lr: 0.000000  loss: 3.9106 (3.7985)  loss_scale: 32768.0000 (54586.9628)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [22]  [2080/2809]  eta: 0:04:31  lr: 0.000023  min_lr: 0.000000  loss: 4.0324 (3.7993)  loss_scale: 65536.0000 (54639.5771)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [22]  [2090/2809]  eta: 0:04:27  lr: 0.000023  min_lr: 0.000000  loss: 3.9716 (3.7997)  loss_scale: 65536.0000 (54691.6882)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [22]  [2100/2809]  eta: 0:04:23  lr: 0.000023  min_lr: 0.000000  loss: 3.9338 (3.8006)  loss_scale: 65536.0000 (54743.3032)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
[2025-01-13 05:56:55,225] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63905
[2025-01-13 05:56:55,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:56:55,226] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [2110/2809]  eta: 0:04:19  lr: 0.000023  min_lr: 0.000000  loss: 3.7972 (3.8005)  loss_scale: 65536.0000 (54732.3392)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [22]  [2120/2809]  eta: 0:04:16  lr: 0.000023  min_lr: 0.000000  loss: 3.8052 (3.8005)  loss_scale: 32768.0000 (54628.7826)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [22]  [2130/2809]  eta: 0:04:12  lr: 0.000023  min_lr: 0.000000  loss: 3.8581 (3.8004)  loss_scale: 32768.0000 (54526.1980)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [22]  [2140/2809]  eta: 0:04:08  lr: 0.000023  min_lr: 0.000000  loss: 3.8788 (3.8006)  loss_scale: 32768.0000 (54424.5717)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [22]  [2150/2809]  eta: 0:04:05  lr: 0.000023  min_lr: 0.000000  loss: 3.7760 (3.8001)  loss_scale: 32768.0000 (54323.8903)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [22]  [2160/2809]  eta: 0:04:01  lr: 0.000023  min_lr: 0.000000  loss: 3.8850 (3.8004)  loss_scale: 32768.0000 (54224.1407)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [22]  [2170/2809]  eta: 0:03:57  lr: 0.000023  min_lr: 0.000000  loss: 3.9804 (3.8006)  loss_scale: 32768.0000 (54125.3100)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [22]  [2180/2809]  eta: 0:03:53  lr: 0.000023  min_lr: 0.000000  loss: 3.7876 (3.7999)  loss_scale: 32768.0000 (54027.3856)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [22]  [2190/2809]  eta: 0:03:50  lr: 0.000023  min_lr: 0.000000  loss: 3.7465 (3.7995)  loss_scale: 32768.0000 (53930.3551)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [22]  [2200/2809]  eta: 0:03:46  lr: 0.000023  min_lr: 0.000000  loss: 3.8318 (3.7994)  loss_scale: 32768.0000 (53834.2063)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
[2025-01-13 05:57:30,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=64000, skipped=435, lr=[2.2153745296679163e-07, 2.2153745296679163e-07, 3.164820756668452e-07, 3.164820756668452e-07, 4.5211725095263607e-07, 4.5211725095263607e-07, 6.458817870751944e-07, 6.458817870751944e-07, 9.226882672502778e-07, 9.226882672502778e-07, 1.3181260960718254e-06, 1.3181260960718254e-06, 1.8830372801026078e-06, 1.8830372801026078e-06, 2.69005325728944e-06, 2.69005325728944e-06, 3.8429332246992e-06, 3.8429332246992e-06, 5.489904606713144e-06, 5.489904606713144e-06, 7.842720866733062e-06, 7.842720866733062e-06, 1.1203886952475804e-05, 1.1203886952475804e-05, 1.600555278925115e-05, 1.600555278925115e-05, 2.286507541321593e-05, 2.286507541321593e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 05:57:30,060] [INFO] [timer.py:260:stop] epoch=0/micro_step=64000/global_step=64000, RunningAvgSamplesPerSec=30.07629449031097, CurrSamplesPerSec=34.946611824107585, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [2210/2809]  eta: 0:03:42  lr: 0.000023  min_lr: 0.000000  loss: 3.8318 (3.8001)  loss_scale: 32768.0000 (53738.9272)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [22]  [2220/2809]  eta: 0:03:39  lr: 0.000023  min_lr: 0.000000  loss: 3.8826 (3.8008)  loss_scale: 32768.0000 (53644.5061)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [22]  [2230/2809]  eta: 0:03:35  lr: 0.000023  min_lr: 0.000000  loss: 3.8532 (3.8013)  loss_scale: 32768.0000 (53550.9314)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
[2025-01-13 05:57:43,022] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:57:43,022] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [2240/2809]  eta: 0:03:31  lr: 0.000023  min_lr: 0.000000  loss: 3.7207 (3.7989)  loss_scale: 32768.0000 (53531.3021)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [22]  [2250/2809]  eta: 0:03:27  lr: 0.000023  min_lr: 0.000000  loss: 3.7345 (3.7991)  loss_scale: 65536.0000 (53584.6326)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [22]  [2260/2809]  eta: 0:03:24  lr: 0.000023  min_lr: 0.000000  loss: 3.8678 (3.7993)  loss_scale: 65536.0000 (53637.4914)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [22]  [2270/2809]  eta: 0:03:20  lr: 0.000023  min_lr: 0.000000  loss: 3.7786 (3.7994)  loss_scale: 65536.0000 (53689.8846)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [22]  [2280/2809]  eta: 0:03:16  lr: 0.000023  min_lr: 0.000000  loss: 3.7838 (3.7999)  loss_scale: 65536.0000 (53741.8185)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [22]  [2290/2809]  eta: 0:03:12  lr: 0.000023  min_lr: 0.000000  loss: 3.8268 (3.7998)  loss_scale: 65536.0000 (53793.2990)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [22]  [2300/2809]  eta: 0:03:09  lr: 0.000023  min_lr: 0.000000  loss: 3.8268 (3.7997)  loss_scale: 65536.0000 (53844.3320)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [22]  [2310/2809]  eta: 0:03:05  lr: 0.000023  min_lr: 0.000000  loss: 3.7191 (3.7991)  loss_scale: 65536.0000 (53894.9234)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [22]  [2320/2809]  eta: 0:03:01  lr: 0.000023  min_lr: 0.000000  loss: 3.7062 (3.7986)  loss_scale: 65536.0000 (53945.0788)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [22]  [2330/2809]  eta: 0:02:58  lr: 0.000023  min_lr: 0.000000  loss: 3.4280 (3.7978)  loss_scale: 65536.0000 (53994.8039)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [22]  [2340/2809]  eta: 0:02:54  lr: 0.000023  min_lr: 0.000000  loss: 3.5971 (3.7973)  loss_scale: 65536.0000 (54044.1042)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [22]  [2350/2809]  eta: 0:02:50  lr: 0.000023  min_lr: 0.000000  loss: 3.7245 (3.7970)  loss_scale: 65536.0000 (54092.9851)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [22]  [2360/2809]  eta: 0:02:46  lr: 0.000023  min_lr: 0.000000  loss: 3.7434 (3.7962)  loss_scale: 65536.0000 (54141.4519)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
[2025-01-13 05:58:30,558] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 05:58:30,558] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 05:58:31,295] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64164
[2025-01-13 05:58:31,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 05:58:31,296] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [2370/2809]  eta: 0:02:43  lr: 0.000023  min_lr: 0.000000  loss: 3.7434 (3.7960)  loss_scale: 65536.0000 (54244.7912)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [22]  [2380/2809]  eta: 0:02:39  lr: 0.000023  min_lr: 0.000000  loss: 4.0708 (3.7976)  loss_scale: 65536.0000 (54292.2134)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [22]  [2390/2809]  eta: 0:02:35  lr: 0.000023  min_lr: 0.000000  loss: 4.1442 (3.7978)  loss_scale: 65536.0000 (54339.2388)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [22]  [2400/2809]  eta: 0:02:32  lr: 0.000023  min_lr: 0.000000  loss: 3.5618 (3.7965)  loss_scale: 65536.0000 (54385.8726)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [22]  [2410/2809]  eta: 0:02:28  lr: 0.000023  min_lr: 0.000000  loss: 3.5618 (3.7965)  loss_scale: 65536.0000 (54432.1195)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [22]  [2420/2809]  eta: 0:02:24  lr: 0.000023  min_lr: 0.000000  loss: 3.7603 (3.7965)  loss_scale: 65536.0000 (54477.9843)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [22]  [2430/2809]  eta: 0:02:20  lr: 0.000023  min_lr: 0.000000  loss: 3.7408 (3.7960)  loss_scale: 65536.0000 (54523.4718)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [22]  [2440/2809]  eta: 0:02:17  lr: 0.000023  min_lr: 0.000000  loss: 3.7488 (3.7957)  loss_scale: 65536.0000 (54568.5866)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [22]  [2450/2809]  eta: 0:02:13  lr: 0.000023  min_lr: 0.000000  loss: 3.8094 (3.7957)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [22]  [2460/2809]  eta: 0:02:09  lr: 0.000023  min_lr: 0.000000  loss: 3.6011 (3.7947)  loss_scale: 65536.0000 (54657.7164)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [22]  [2470/2809]  eta: 0:02:06  lr: 0.000023  min_lr: 0.000000  loss: 3.7591 (3.7950)  loss_scale: 65536.0000 (54701.7402)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [22]  [2480/2809]  eta: 0:02:02  lr: 0.000023  min_lr: 0.000000  loss: 3.8102 (3.7948)  loss_scale: 65536.0000 (54745.4091)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
[2025-01-13 05:59:14,199] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64280
[2025-01-13 05:59:14,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 05:59:14,199] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [2490/2809]  eta: 0:01:58  lr: 0.000023  min_lr: 0.000000  loss: 3.7634 (3.7946)  loss_scale: 65536.0000 (54670.3364)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [22]  [2500/2809]  eta: 0:01:54  lr: 0.000023  min_lr: 0.000000  loss: 3.8398 (3.7948)  loss_scale: 32768.0000 (54582.7621)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [22]  [2510/2809]  eta: 0:01:51  lr: 0.000023  min_lr: 0.000000  loss: 3.8564 (3.7949)  loss_scale: 32768.0000 (54495.8853)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [22]  [2520/2809]  eta: 0:01:47  lr: 0.000023  min_lr: 0.000000  loss: 3.9391 (3.7953)  loss_scale: 32768.0000 (54409.6977)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [22]  [2530/2809]  eta: 0:01:43  lr: 0.000023  min_lr: 0.000000  loss: 3.6937 (3.7948)  loss_scale: 32768.0000 (54324.1912)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0003  max mem: 15572
Epoch: [22]  [2540/2809]  eta: 0:01:39  lr: 0.000023  min_lr: 0.000000  loss: 3.7197 (3.7949)  loss_scale: 32768.0000 (54239.3577)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [22]  [2550/2809]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000000  loss: 3.6861 (3.7937)  loss_scale: 32768.0000 (54155.1893)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [22]  [2560/2809]  eta: 0:01:32  lr: 0.000023  min_lr: 0.000000  loss: 3.6728 (3.7939)  loss_scale: 32768.0000 (54071.6783)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [22]  [2570/2809]  eta: 0:01:28  lr: 0.000023  min_lr: 0.000000  loss: 3.8235 (3.7940)  loss_scale: 32768.0000 (53988.8168)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [22]  [2580/2809]  eta: 0:01:25  lr: 0.000023  min_lr: 0.000000  loss: 3.8106 (3.7942)  loss_scale: 32768.0000 (53906.5974)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [22]  [2590/2809]  eta: 0:01:21  lr: 0.000023  min_lr: 0.000000  loss: 3.7323 (3.7937)  loss_scale: 32768.0000 (53825.0127)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [22]  [2600/2809]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000000  loss: 3.6809 (3.7939)  loss_scale: 32768.0000 (53744.0554)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [22]  [2610/2809]  eta: 0:01:13  lr: 0.000023  min_lr: 0.000000  loss: 3.8993 (3.7937)  loss_scale: 32768.0000 (53663.7181)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
[2025-01-13 06:00:02,340] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:00:02,340] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [2620/2809]  eta: 0:01:10  lr: 0.000023  min_lr: 0.000000  loss: 4.0096 (3.7948)  loss_scale: 32768.0000 (53709.0149)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [22]  [2630/2809]  eta: 0:01:06  lr: 0.000023  min_lr: 0.000000  loss: 3.9106 (3.7941)  loss_scale: 65536.0000 (53753.9673)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [22]  [2640/2809]  eta: 0:01:02  lr: 0.000023  min_lr: 0.000000  loss: 3.5166 (3.7935)  loss_scale: 65536.0000 (53798.5793)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [22]  [2650/2809]  eta: 0:00:59  lr: 0.000023  min_lr: 0.000000  loss: 3.6427 (3.7929)  loss_scale: 65536.0000 (53842.8548)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [22]  [2660/2809]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000000  loss: 3.8318 (3.7932)  loss_scale: 65536.0000 (53886.7974)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0001  max mem: 15572
Epoch: [22]  [2670/2809]  eta: 0:00:51  lr: 0.000023  min_lr: 0.000000  loss: 3.8318 (3.7931)  loss_scale: 65536.0000 (53930.4111)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [22]  [2680/2809]  eta: 0:00:47  lr: 0.000023  min_lr: 0.000000  loss: 3.7018 (3.7928)  loss_scale: 65536.0000 (53973.6994)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [22]  [2690/2809]  eta: 0:00:44  lr: 0.000022  min_lr: 0.000000  loss: 3.7519 (3.7935)  loss_scale: 65536.0000 (54016.6659)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [22]  [2700/2809]  eta: 0:00:40  lr: 0.000022  min_lr: 0.000000  loss: 3.7304 (3.7929)  loss_scale: 65536.0000 (54059.3143)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0001  max mem: 15572
Epoch: [22]  [2710/2809]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000000  loss: 3.7238 (3.7926)  loss_scale: 65536.0000 (54101.6481)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [22]  [2720/2809]  eta: 0:00:33  lr: 0.000022  min_lr: 0.000000  loss: 3.7420 (3.7927)  loss_scale: 65536.0000 (54143.6707)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [22]  [2730/2809]  eta: 0:00:29  lr: 0.000022  min_lr: 0.000000  loss: 3.7048 (3.7919)  loss_scale: 65536.0000 (54185.3856)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
[2025-01-13 06:00:49,760] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:00:49,760] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [22]  [2740/2809]  eta: 0:00:25  lr: 0.000022  min_lr: 0.000000  loss: 3.7281 (3.7917)  loss_scale: 65536.0000 (54274.6151)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
[2025-01-13 06:00:50,503] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64539
[2025-01-13 06:00:50,503] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:00:50,503] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [2750/2809]  eta: 0:00:21  lr: 0.000022  min_lr: 0.000000  loss: 3.7403 (3.7916)  loss_scale: 65536.0000 (54315.5507)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [22]  [2760/2809]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000000  loss: 3.7458 (3.7914)  loss_scale: 65536.0000 (54356.1898)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
Epoch: [22]  [2770/2809]  eta: 0:00:14  lr: 0.000022  min_lr: 0.000000  loss: 3.7458 (3.7914)  loss_scale: 65536.0000 (54396.5355)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [22]  [2780/2809]  eta: 0:00:10  lr: 0.000022  min_lr: 0.000000  loss: 4.0617 (3.7923)  loss_scale: 65536.0000 (54436.5912)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [22]  [2790/2809]  eta: 0:00:07  lr: 0.000022  min_lr: 0.000000  loss: 4.0191 (3.7925)  loss_scale: 65536.0000 (54476.3597)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0003  max mem: 15572
Epoch: [22]  [2800/2809]  eta: 0:00:03  lr: 0.000022  min_lr: 0.000000  loss: 3.8667 (3.7930)  loss_scale: 65536.0000 (54515.8443)  weight_decay: 0.0500 (0.0500)  time: 0.3640  data: 0.0002  max mem: 15572
Epoch: [22]  [2808/2809]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000000  loss: 3.8803 (3.7935)  loss_scale: 65536.0000 (54547.2296)  weight_decay: 0.0500 (0.0500)  time: 0.3586  data: 0.0001  max mem: 15572
Epoch: [22] Total time: 0:17:24 (0.3718 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000000  loss: 3.8803 (3.7935)  loss_scale: 65536.0000 (54547.2296)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:09:01  loss: 0.3450 (0.3450)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.9916  data: 1.8145  max mem: 15572
Val:  [ 10/272]  eta: 0:01:48  loss: 2.5208 (2.4461)  acc1: 38.8889 (41.4141)  acc5: 72.2222 (68.1818)  time: 0.4140  data: 0.2589  max mem: 15572
Val:  [ 20/272]  eta: 0:01:13  loss: 2.4554 (2.4331)  acc1: 50.0000 (43.9153)  acc5: 72.2222 (71.9577)  time: 0.2053  data: 0.0519  max mem: 15572
Val:  [ 30/272]  eta: 0:01:00  loss: 2.4488 (2.5212)  acc1: 50.0000 (39.7849)  acc5: 72.2222 (70.7885)  time: 0.1583  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:52  loss: 2.5654 (2.5497)  acc1: 27.7778 (38.0759)  acc5: 72.2222 (71.1382)  time: 0.1602  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:47  loss: 2.4899 (2.4731)  acc1: 33.3333 (39.6514)  acc5: 77.7778 (73.3115)  time: 0.1635  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:43  loss: 1.5773 (2.3656)  acc1: 55.5556 (42.9872)  acc5: 88.8889 (74.5902)  time: 0.1638  data: 0.0003  max mem: 15572
Val:  [ 70/272]  eta: 0:00:40  loss: 1.5993 (2.2764)  acc1: 61.1111 (45.6182)  acc5: 88.8889 (75.8998)  time: 0.1573  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:37  loss: 1.9676 (2.2993)  acc1: 55.5556 (45.1989)  acc5: 83.3333 (75.5830)  time: 0.1561  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.3187 (2.3117)  acc1: 50.0000 (45.7875)  acc5: 77.7778 (76.0073)  time: 0.1546  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:31  loss: 2.3177 (2.3321)  acc1: 50.0000 (45.1595)  acc5: 83.3333 (75.8526)  time: 0.1544  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:29  loss: 2.5791 (2.4075)  acc1: 16.6667 (43.0430)  acc5: 66.6667 (74.9249)  time: 0.1587  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 3.0406 (2.4497)  acc1: 16.6667 (42.0569)  acc5: 66.6667 (74.1965)  time: 0.1573  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.2858 (2.4118)  acc1: 44.4444 (43.1298)  acc5: 77.7778 (74.8940)  time: 0.1569  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.7784 (2.4033)  acc1: 55.5556 (43.6170)  acc5: 83.3333 (74.5469)  time: 0.1653  data: 0.0005  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.3537 (2.4036)  acc1: 33.3333 (42.8992)  acc5: 77.7778 (74.9080)  time: 0.1619  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.3679 (2.3974)  acc1: 44.4444 (43.5473)  acc5: 77.7778 (75.1208)  time: 0.1574  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.5277 (2.4227)  acc1: 38.8889 (42.7225)  acc5: 72.2222 (74.4964)  time: 0.1597  data: 0.0005  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 2.4690 (2.4150)  acc1: 33.3333 (42.3266)  acc5: 72.2222 (74.8312)  time: 0.1577  data: 0.0005  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.4852 (2.4675)  acc1: 27.7778 (40.9831)  acc5: 77.7778 (73.4730)  time: 0.1554  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.5970 (2.4702)  acc1: 27.7778 (40.8789)  acc5: 66.6667 (73.3554)  time: 0.1525  data: 0.0003  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.0888 (2.4639)  acc1: 50.0000 (41.3112)  acc5: 77.7778 (73.4860)  time: 0.1580  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.1259 (2.4552)  acc1: 50.0000 (41.6792)  acc5: 77.7778 (73.6551)  time: 0.1577  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.8815 (2.4257)  acc1: 61.1111 (42.8090)  acc5: 77.7778 (73.9779)  time: 0.1539  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.6432 (2.4085)  acc1: 61.1111 (43.2227)  acc5: 83.3333 (74.3430)  time: 0.1559  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.1984 (2.4173)  acc1: 44.4444 (42.6737)  acc5: 72.2222 (74.2364)  time: 0.1605  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.1000 (2.3517)  acc1: 72.2222 (44.5296)  acc5: 88.8889 (75.0319)  time: 0.1563  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3677 (2.3485)  acc1: 72.2222 (44.5264)  acc5: 88.8889 (75.1948)  time: 0.1393  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3677 (2.3536)  acc1: 66.6667 (44.5013)  acc5: 88.8889 (75.1587)  time: 0.1333  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1675 s / it)
* Acc@1 44.501 Acc@5 75.159 loss 2.354
Accuracy of the network on the 4883 val videos: 44.5%
[2025-01-13 06:02:00,844] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 06:02:00,846] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 06:02:00,846] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 06:02:03,285] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 06:02:03,285] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.50%
Epoch: [23]  [   0/2809]  eta: 3:00:34  lr: 0.000022  min_lr: 0.000000  loss: 3.7756 (3.7756)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.8569  data: 3.4674  max mem: 15572
Epoch: [23]  [  10/2809]  eta: 0:32:52  lr: 0.000022  min_lr: 0.000000  loss: 3.8074 (3.8433)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7046  data: 0.3250  max mem: 15572
Epoch: [23]  [  20/2809]  eta: 0:25:28  lr: 0.000022  min_lr: 0.000000  loss: 3.8322 (3.8858)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0055  max mem: 15572
Epoch: [23]  [  30/2809]  eta: 0:22:40  lr: 0.000022  min_lr: 0.000000  loss: 3.7988 (3.7634)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [23]  [  40/2809]  eta: 0:21:18  lr: 0.000022  min_lr: 0.000000  loss: 3.5008 (3.7552)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [23]  [  50/2809]  eta: 0:20:20  lr: 0.000022  min_lr: 0.000000  loss: 3.6955 (3.7321)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [23]  [  60/2809]  eta: 0:19:44  lr: 0.000022  min_lr: 0.000000  loss: 3.5401 (3.6798)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
[2025-01-13 06:02:29,947] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:02:29,947] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:02:30,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64670
[2025-01-13 06:02:30,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:02:30,688] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [  70/2809]  eta: 0:19:16  lr: 0.000022  min_lr: 0.000000  loss: 3.5619 (3.6904)  loss_scale: 65536.0000 (67382.0845)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [23]  [  80/2809]  eta: 0:18:53  lr: 0.000022  min_lr: 0.000000  loss: 3.8168 (3.7209)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [23]  [  90/2809]  eta: 0:18:35  lr: 0.000022  min_lr: 0.000000  loss: 3.8331 (3.7169)  loss_scale: 65536.0000 (66976.3516)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [23]  [ 100/2809]  eta: 0:18:20  lr: 0.000022  min_lr: 0.000000  loss: 3.7819 (3.7316)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [23]  [ 110/2809]  eta: 0:18:08  lr: 0.000022  min_lr: 0.000000  loss: 3.7819 (3.7267)  loss_scale: 65536.0000 (66716.8288)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0003  max mem: 15572
Epoch: [23]  [ 120/2809]  eta: 0:17:56  lr: 0.000022  min_lr: 0.000000  loss: 3.6264 (3.7261)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0003  max mem: 15572
Epoch: [23]  [ 130/2809]  eta: 0:17:46  lr: 0.000022  min_lr: 0.000000  loss: 3.6466 (3.7303)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [23]  [ 140/2809]  eta: 0:17:37  lr: 0.000022  min_lr: 0.000000  loss: 3.9422 (3.7386)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [ 150/2809]  eta: 0:17:28  lr: 0.000022  min_lr: 0.000000  loss: 3.7438 (3.7301)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [ 160/2809]  eta: 0:17:19  lr: 0.000022  min_lr: 0.000000  loss: 3.6690 (3.7392)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [23]  [ 170/2809]  eta: 0:17:12  lr: 0.000022  min_lr: 0.000000  loss: 3.6633 (3.7297)  loss_scale: 65536.0000 (66302.5029)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [23]  [ 180/2809]  eta: 0:17:06  lr: 0.000022  min_lr: 0.000000  loss: 3.5930 (3.7380)  loss_scale: 65536.0000 (66260.1547)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [23]  [ 190/2809]  eta: 0:16:59  lr: 0.000022  min_lr: 0.000000  loss: 3.5382 (3.7250)  loss_scale: 65536.0000 (66222.2408)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
[2025-01-13 06:03:18,396] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:03:18,396] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:03:19,151] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64801
[2025-01-13 06:03:19,151] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:03:19,151] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 200/2809]  eta: 0:16:52  lr: 0.000022  min_lr: 0.000000  loss: 3.4419 (3.7188)  loss_scale: 65536.0000 (66840.1990)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [23]  [ 210/2809]  eta: 0:16:46  lr: 0.000022  min_lr: 0.000000  loss: 3.5632 (3.7130)  loss_scale: 65536.0000 (66778.3886)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [23]  [ 220/2809]  eta: 0:16:41  lr: 0.000022  min_lr: 0.000000  loss: 3.6305 (3.7023)  loss_scale: 65536.0000 (66722.1719)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [23]  [ 230/2809]  eta: 0:16:35  lr: 0.000022  min_lr: 0.000000  loss: 3.6896 (3.7034)  loss_scale: 65536.0000 (66670.8225)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [23]  [ 240/2809]  eta: 0:16:29  lr: 0.000022  min_lr: 0.000000  loss: 3.7929 (3.7058)  loss_scale: 65536.0000 (66623.7344)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [23]  [ 250/2809]  eta: 0:16:24  lr: 0.000022  min_lr: 0.000000  loss: 3.8000 (3.7139)  loss_scale: 65536.0000 (66580.3984)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [23]  [ 260/2809]  eta: 0:16:19  lr: 0.000022  min_lr: 0.000000  loss: 3.7814 (3.7193)  loss_scale: 65536.0000 (66540.3831)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [23]  [ 270/2809]  eta: 0:16:13  lr: 0.000022  min_lr: 0.000000  loss: 3.8471 (3.7224)  loss_scale: 65536.0000 (66503.3210)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [23]  [ 280/2809]  eta: 0:16:08  lr: 0.000022  min_lr: 0.000000  loss: 3.7870 (3.7235)  loss_scale: 65536.0000 (66468.8968)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [23]  [ 290/2809]  eta: 0:16:04  lr: 0.000022  min_lr: 0.000000  loss: 3.7838 (3.7303)  loss_scale: 65536.0000 (66436.8385)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [23]  [ 300/2809]  eta: 0:15:59  lr: 0.000022  min_lr: 0.000000  loss: 3.7544 (3.7280)  loss_scale: 65536.0000 (66406.9103)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [23]  [ 310/2809]  eta: 0:15:54  lr: 0.000022  min_lr: 0.000000  loss: 3.7544 (3.7270)  loss_scale: 65536.0000 (66378.9068)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [23]  [ 320/2809]  eta: 0:15:50  lr: 0.000022  min_lr: 0.000000  loss: 3.8818 (3.7293)  loss_scale: 65536.0000 (66352.6480)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
[2025-01-13 06:04:06,997] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:04:06,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:04:07,366] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64931
[2025-01-13 06:04:07,366] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:04:07,366] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 330/2809]  eta: 0:15:45  lr: 0.000022  min_lr: 0.000000  loss: 4.0693 (3.7374)  loss_scale: 65536.0000 (66525.9698)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [23]  [ 340/2809]  eta: 0:15:41  lr: 0.000022  min_lr: 0.000000  loss: 3.9455 (3.7378)  loss_scale: 65536.0000 (66496.9384)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [23]  [ 350/2809]  eta: 0:15:36  lr: 0.000022  min_lr: 0.000000  loss: 3.7878 (3.7397)  loss_scale: 65536.0000 (66469.5613)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [23]  [ 360/2809]  eta: 0:15:31  lr: 0.000022  min_lr: 0.000000  loss: 3.7815 (3.7417)  loss_scale: 65536.0000 (66443.7008)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [23]  [ 370/2809]  eta: 0:15:26  lr: 0.000022  min_lr: 0.000000  loss: 3.6548 (3.7409)  loss_scale: 65536.0000 (66419.2345)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [23]  [ 380/2809]  eta: 0:15:22  lr: 0.000022  min_lr: 0.000000  loss: 3.5814 (3.7384)  loss_scale: 65536.0000 (66396.0525)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [23]  [ 390/2809]  eta: 0:15:18  lr: 0.000022  min_lr: 0.000000  loss: 3.8081 (3.7389)  loss_scale: 65536.0000 (66374.0563)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
[2025-01-13 06:04:32,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=65000, skipped=441, lr=[2.1429492637226224e-07, 2.1429492637226224e-07, 3.061356091032318e-07, 3.061356091032318e-07, 4.3733658443318834e-07, 4.3733658443318834e-07, 6.24766549190269e-07, 6.24766549190269e-07, 8.925236417003845e-07, 8.925236417003845e-07, 1.275033773857692e-06, 1.275033773857692e-06, 1.821476819796703e-06, 1.821476819796703e-06, 2.602109742566719e-06, 2.602109742566719e-06, 3.71729963223817e-06, 3.71729963223817e-06, 5.310428046054529e-06, 5.310428046054529e-06, 7.586325780077899e-06, 7.586325780077899e-06, 1.0837608257254143e-05, 1.0837608257254143e-05, 1.548229751036306e-05, 1.548229751036306e-05, 2.211756787194723e-05, 2.211756787194723e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 06:04:32,493] [INFO] [timer.py:260:stop] epoch=0/micro_step=65000/global_step=65000, RunningAvgSamplesPerSec=30.132661197564943, CurrSamplesPerSec=34.167998131785176, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [23]  [ 400/2809]  eta: 0:15:13  lr: 0.000022  min_lr: 0.000000  loss: 3.9626 (3.7417)  loss_scale: 65536.0000 (66353.1571)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [23]  [ 410/2809]  eta: 0:15:09  lr: 0.000022  min_lr: 0.000000  loss: 3.7833 (3.7399)  loss_scale: 65536.0000 (66333.2749)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [ 420/2809]  eta: 0:15:05  lr: 0.000022  min_lr: 0.000000  loss: 3.8722 (3.7474)  loss_scale: 65536.0000 (66314.3373)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [23]  [ 430/2809]  eta: 0:15:01  lr: 0.000022  min_lr: 0.000000  loss: 3.8532 (3.7438)  loss_scale: 65536.0000 (66296.2784)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [23]  [ 440/2809]  eta: 0:14:57  lr: 0.000022  min_lr: 0.000000  loss: 3.7213 (3.7481)  loss_scale: 65536.0000 (66279.0385)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [23]  [ 450/2809]  eta: 0:14:52  lr: 0.000022  min_lr: 0.000000  loss: 3.8915 (3.7496)  loss_scale: 65536.0000 (66262.5632)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
[2025-01-13 06:04:55,181] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:04:55,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:04:57,413] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65066
[2025-01-13 06:04:57,413] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:04:57,415] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 460/2809]  eta: 0:14:48  lr: 0.000022  min_lr: 0.000000  loss: 3.8329 (3.7500)  loss_scale: 65536.0000 (67099.7657)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [23]  [ 470/2809]  eta: 0:14:44  lr: 0.000022  min_lr: 0.000000  loss: 3.7309 (3.7471)  loss_scale: 65536.0000 (67066.5648)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0013  max mem: 15572
Epoch: [23]  [ 480/2809]  eta: 0:14:40  lr: 0.000022  min_lr: 0.000000  loss: 3.7456 (3.7491)  loss_scale: 65536.0000 (67034.7443)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0013  max mem: 15572
Epoch: [23]  [ 490/2809]  eta: 0:14:36  lr: 0.000022  min_lr: 0.000000  loss: 3.7855 (3.7492)  loss_scale: 65536.0000 (67004.2200)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
Epoch: [23]  [ 500/2809]  eta: 0:14:32  lr: 0.000022  min_lr: 0.000000  loss: 3.8409 (3.7529)  loss_scale: 65536.0000 (66974.9142)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [23]  [ 510/2809]  eta: 0:14:28  lr: 0.000022  min_lr: 0.000000  loss: 3.8996 (3.7547)  loss_scale: 65536.0000 (66946.7554)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [23]  [ 520/2809]  eta: 0:14:24  lr: 0.000022  min_lr: 0.000000  loss: 3.8996 (3.7543)  loss_scale: 65536.0000 (66919.6775)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [23]  [ 530/2809]  eta: 0:14:20  lr: 0.000022  min_lr: 0.000000  loss: 3.4507 (3.7484)  loss_scale: 65536.0000 (66893.6196)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [23]  [ 540/2809]  eta: 0:14:16  lr: 0.000022  min_lr: 0.000000  loss: 3.5346 (3.7471)  loss_scale: 65536.0000 (66868.5250)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [23]  [ 550/2809]  eta: 0:14:12  lr: 0.000022  min_lr: 0.000000  loss: 3.6615 (3.7462)  loss_scale: 65536.0000 (66844.3412)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [23]  [ 560/2809]  eta: 0:14:08  lr: 0.000022  min_lr: 0.000000  loss: 3.8587 (3.7458)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0002  max mem: 15572
Epoch: [23]  [ 570/2809]  eta: 0:14:04  lr: 0.000022  min_lr: 0.000000  loss: 3.9908 (3.7501)  loss_scale: 65536.0000 (66798.5149)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [23]  [ 580/2809]  eta: 0:14:00  lr: 0.000022  min_lr: 0.000000  loss: 4.0176 (3.7545)  loss_scale: 65536.0000 (66776.7849)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
[2025-01-13 06:05:45,391] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:05:45,391] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:05:45,754] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65196
[2025-01-13 06:05:45,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:05:45,754] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 590/2809]  eta: 0:13:56  lr: 0.000022  min_lr: 0.000000  loss: 4.2090 (3.7592)  loss_scale: 65536.0000 (66866.6802)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [23]  [ 600/2809]  eta: 0:13:52  lr: 0.000022  min_lr: 0.000000  loss: 4.1736 (3.7652)  loss_scale: 65536.0000 (66844.5391)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [23]  [ 610/2809]  eta: 0:13:48  lr: 0.000022  min_lr: 0.000000  loss: 3.9496 (3.7650)  loss_scale: 65536.0000 (66823.1227)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [23]  [ 620/2809]  eta: 0:13:44  lr: 0.000022  min_lr: 0.000000  loss: 3.8881 (3.7708)  loss_scale: 65536.0000 (66802.3961)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [23]  [ 630/2809]  eta: 0:13:40  lr: 0.000022  min_lr: 0.000000  loss: 3.8881 (3.7653)  loss_scale: 65536.0000 (66782.3265)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [23]  [ 640/2809]  eta: 0:13:36  lr: 0.000022  min_lr: 0.000000  loss: 3.6132 (3.7659)  loss_scale: 65536.0000 (66762.8830)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
[2025-01-13 06:06:06,567] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65252
[2025-01-13 06:06:06,568] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:06:06,568] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 650/2809]  eta: 0:13:32  lr: 0.000022  min_lr: 0.000000  loss: 3.7271 (3.7656)  loss_scale: 65536.0000 (66442.0276)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [23]  [ 660/2809]  eta: 0:13:28  lr: 0.000022  min_lr: 0.000000  loss: 3.8414 (3.7654)  loss_scale: 32768.0000 (65932.5870)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [23]  [ 670/2809]  eta: 0:13:24  lr: 0.000022  min_lr: 0.000000  loss: 3.9064 (3.7665)  loss_scale: 32768.0000 (65438.3308)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [23]  [ 680/2809]  eta: 0:13:20  lr: 0.000022  min_lr: 0.000000  loss: 4.0446 (3.7682)  loss_scale: 32768.0000 (64958.5903)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [23]  [ 690/2809]  eta: 0:13:17  lr: 0.000022  min_lr: 0.000000  loss: 3.7425 (3.7679)  loss_scale: 32768.0000 (64492.7352)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [23]  [ 700/2809]  eta: 0:13:13  lr: 0.000022  min_lr: 0.000000  loss: 3.6346 (3.7640)  loss_scale: 32768.0000 (64040.1712)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [23]  [ 710/2809]  eta: 0:13:09  lr: 0.000022  min_lr: 0.000000  loss: 3.7263 (3.7661)  loss_scale: 32768.0000 (63600.3376)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [23]  [ 720/2809]  eta: 0:13:05  lr: 0.000022  min_lr: 0.000000  loss: 3.8773 (3.7664)  loss_scale: 32768.0000 (63172.7046)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [23]  [ 730/2809]  eta: 0:13:01  lr: 0.000022  min_lr: 0.000000  loss: 3.7003 (3.7649)  loss_scale: 32768.0000 (62756.7715)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [23]  [ 740/2809]  eta: 0:12:57  lr: 0.000022  min_lr: 0.000000  loss: 3.7554 (3.7651)  loss_scale: 32768.0000 (62352.0648)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [23]  [ 750/2809]  eta: 0:12:53  lr: 0.000022  min_lr: 0.000000  loss: 3.8936 (3.7669)  loss_scale: 32768.0000 (61958.1358)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [23]  [ 760/2809]  eta: 0:12:49  lr: 0.000022  min_lr: 0.000000  loss: 3.4575 (3.7604)  loss_scale: 32768.0000 (61574.5598)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [23]  [ 770/2809]  eta: 0:12:45  lr: 0.000022  min_lr: 0.000000  loss: 3.4575 (3.7632)  loss_scale: 32768.0000 (61200.9339)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
[2025-01-13 06:06:54,484] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:06:54,484] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [ 780/2809]  eta: 0:12:42  lr: 0.000022  min_lr: 0.000000  loss: 4.0481 (3.7636)  loss_scale: 32768.0000 (61130.5711)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [23]  [ 790/2809]  eta: 0:12:38  lr: 0.000022  min_lr: 0.000000  loss: 3.8883 (3.7641)  loss_scale: 65536.0000 (61186.2655)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [23]  [ 800/2809]  eta: 0:12:34  lr: 0.000022  min_lr: 0.000000  loss: 3.7599 (3.7653)  loss_scale: 65536.0000 (61240.5693)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [23]  [ 810/2809]  eta: 0:12:30  lr: 0.000022  min_lr: 0.000000  loss: 3.7473 (3.7650)  loss_scale: 65536.0000 (61293.5339)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [23]  [ 820/2809]  eta: 0:12:26  lr: 0.000022  min_lr: 0.000000  loss: 3.8760 (3.7686)  loss_scale: 65536.0000 (61345.2083)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [23]  [ 830/2809]  eta: 0:12:22  lr: 0.000022  min_lr: 0.000000  loss: 4.1504 (3.7708)  loss_scale: 65536.0000 (61395.6390)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [23]  [ 840/2809]  eta: 0:12:18  lr: 0.000022  min_lr: 0.000000  loss: 3.7833 (3.7714)  loss_scale: 65536.0000 (61444.8704)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
Epoch: [23]  [ 850/2809]  eta: 0:12:14  lr: 0.000022  min_lr: 0.000000  loss: 3.7685 (3.7722)  loss_scale: 65536.0000 (61492.9448)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [23]  [ 860/2809]  eta: 0:12:10  lr: 0.000022  min_lr: 0.000000  loss: 3.7393 (3.7735)  loss_scale: 65536.0000 (61539.9024)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [23]  [ 870/2809]  eta: 0:12:06  lr: 0.000022  min_lr: 0.000000  loss: 3.7393 (3.7729)  loss_scale: 65536.0000 (61585.7819)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [23]  [ 880/2809]  eta: 0:12:02  lr: 0.000022  min_lr: 0.000000  loss: 3.7781 (3.7732)  loss_scale: 65536.0000 (61630.6198)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [23]  [ 890/2809]  eta: 0:11:59  lr: 0.000022  min_lr: 0.000000  loss: 3.9009 (3.7746)  loss_scale: 65536.0000 (61674.4512)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [23]  [ 900/2809]  eta: 0:11:55  lr: 0.000022  min_lr: 0.000000  loss: 3.9500 (3.7760)  loss_scale: 65536.0000 (61717.3097)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
[2025-01-13 06:07:41,721] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:07:41,721] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:07:44,019] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65515
[2025-01-13 06:07:44,019] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:07:44,019] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 910/2809]  eta: 0:11:51  lr: 0.000022  min_lr: 0.000000  loss: 4.0912 (3.7787)  loss_scale: 65536.0000 (62190.8584)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [23]  [ 920/2809]  eta: 0:11:47  lr: 0.000022  min_lr: 0.000000  loss: 3.9064 (3.7774)  loss_scale: 65536.0000 (62227.1792)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [23]  [ 930/2809]  eta: 0:11:43  lr: 0.000022  min_lr: 0.000000  loss: 3.9942 (3.7819)  loss_scale: 65536.0000 (62262.7197)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0003  max mem: 15572
Epoch: [23]  [ 940/2809]  eta: 0:11:39  lr: 0.000022  min_lr: 0.000000  loss: 3.9942 (3.7807)  loss_scale: 65536.0000 (62297.5048)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0003  max mem: 15572
Epoch: [23]  [ 950/2809]  eta: 0:11:36  lr: 0.000022  min_lr: 0.000000  loss: 3.8399 (3.7823)  loss_scale: 65536.0000 (62331.5584)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [23]  [ 960/2809]  eta: 0:11:32  lr: 0.000022  min_lr: 0.000000  loss: 3.9583 (3.7835)  loss_scale: 65536.0000 (62364.9032)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [23]  [ 970/2809]  eta: 0:11:28  lr: 0.000022  min_lr: 0.000000  loss: 3.9065 (3.7835)  loss_scale: 65536.0000 (62397.5613)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [23]  [ 980/2809]  eta: 0:11:24  lr: 0.000022  min_lr: 0.000000  loss: 3.8245 (3.7816)  loss_scale: 65536.0000 (62429.5535)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [23]  [ 990/2809]  eta: 0:11:20  lr: 0.000022  min_lr: 0.000000  loss: 3.5473 (3.7793)  loss_scale: 65536.0000 (62460.9001)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [23]  [1000/2809]  eta: 0:11:16  lr: 0.000022  min_lr: 0.000000  loss: 3.7538 (3.7817)  loss_scale: 65536.0000 (62491.6204)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0003  max mem: 15572
Epoch: [23]  [1010/2809]  eta: 0:11:13  lr: 0.000022  min_lr: 0.000000  loss: 3.7538 (3.7789)  loss_scale: 65536.0000 (62521.7329)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [23]  [1020/2809]  eta: 0:11:09  lr: 0.000022  min_lr: 0.000000  loss: 3.6126 (3.7793)  loss_scale: 65536.0000 (62551.2556)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [23]  [1030/2809]  eta: 0:11:05  lr: 0.000022  min_lr: 0.000000  loss: 3.9555 (3.7805)  loss_scale: 65536.0000 (62580.2056)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
[2025-01-13 06:08:31,739] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:08:31,739] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:08:32,461] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65646
[2025-01-13 06:08:32,461] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:08:32,462] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [1040/2809]  eta: 0:11:01  lr: 0.000022  min_lr: 0.000000  loss: 3.8460 (3.7810)  loss_scale: 65536.0000 (62734.5091)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [23]  [1050/2809]  eta: 0:10:57  lr: 0.000022  min_lr: 0.000000  loss: 3.7701 (3.7795)  loss_scale: 65536.0000 (62761.1646)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [23]  [1060/2809]  eta: 0:10:54  lr: 0.000022  min_lr: 0.000000  loss: 3.8544 (3.7820)  loss_scale: 65536.0000 (62787.3176)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [23]  [1070/2809]  eta: 0:10:50  lr: 0.000022  min_lr: 0.000000  loss: 4.0468 (3.7828)  loss_scale: 65536.0000 (62812.9823)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [23]  [1080/2809]  eta: 0:10:46  lr: 0.000022  min_lr: 0.000000  loss: 3.6959 (3.7812)  loss_scale: 65536.0000 (62838.1721)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0003  max mem: 15572
Epoch: [23]  [1090/2809]  eta: 0:10:42  lr: 0.000022  min_lr: 0.000000  loss: 3.8733 (3.7825)  loss_scale: 65536.0000 (62862.9001)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [23]  [1100/2809]  eta: 0:10:38  lr: 0.000022  min_lr: 0.000000  loss: 4.0051 (3.7817)  loss_scale: 65536.0000 (62887.1789)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [1110/2809]  eta: 0:10:35  lr: 0.000022  min_lr: 0.000000  loss: 3.6443 (3.7810)  loss_scale: 65536.0000 (62911.0207)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [23]  [1120/2809]  eta: 0:10:31  lr: 0.000022  min_lr: 0.000000  loss: 3.8786 (3.7827)  loss_scale: 65536.0000 (62934.4371)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [23]  [1130/2809]  eta: 0:10:27  lr: 0.000022  min_lr: 0.000000  loss: 3.8066 (3.7801)  loss_scale: 65536.0000 (62957.4394)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [23]  [1140/2809]  eta: 0:10:23  lr: 0.000022  min_lr: 0.000000  loss: 3.6948 (3.7801)  loss_scale: 65536.0000 (62980.0386)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [23]  [1150/2809]  eta: 0:10:19  lr: 0.000022  min_lr: 0.000000  loss: 3.9020 (3.7806)  loss_scale: 65536.0000 (63002.2450)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [23]  [1160/2809]  eta: 0:10:16  lr: 0.000022  min_lr: 0.000000  loss: 3.9315 (3.7824)  loss_scale: 65536.0000 (63024.0689)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
[2025-01-13 06:09:20,170] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:09:20,170] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:09:20,545] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65776
[2025-01-13 06:09:20,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:09:20,545] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [1170/2809]  eta: 0:10:12  lr: 0.000022  min_lr: 0.000000  loss: 3.7068 (3.7789)  loss_scale: 65536.0000 (63101.4859)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [23]  [1180/2809]  eta: 0:10:08  lr: 0.000022  min_lr: 0.000000  loss: 3.6853 (3.7789)  loss_scale: 65536.0000 (63122.0999)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [23]  [1190/2809]  eta: 0:10:04  lr: 0.000022  min_lr: 0.000000  loss: 4.0417 (3.7809)  loss_scale: 65536.0000 (63142.3678)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [23]  [1200/2809]  eta: 0:10:00  lr: 0.000022  min_lr: 0.000000  loss: 4.0010 (3.7821)  loss_scale: 65536.0000 (63162.2981)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
Epoch: [23]  [1210/2809]  eta: 0:09:57  lr: 0.000022  min_lr: 0.000000  loss: 3.6024 (3.7796)  loss_scale: 65536.0000 (63181.8993)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [23]  [1220/2809]  eta: 0:09:53  lr: 0.000021  min_lr: 0.000000  loss: 3.5203 (3.7770)  loss_scale: 65536.0000 (63201.1794)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0003  max mem: 15572
Epoch: [23]  [1230/2809]  eta: 0:09:49  lr: 0.000021  min_lr: 0.000000  loss: 3.5339 (3.7746)  loss_scale: 65536.0000 (63220.1462)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0003  max mem: 15572
Epoch: [23]  [1240/2809]  eta: 0:09:45  lr: 0.000021  min_lr: 0.000000  loss: 3.5912 (3.7752)  loss_scale: 65536.0000 (63238.8074)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [23]  [1250/2809]  eta: 0:09:42  lr: 0.000021  min_lr: 0.000000  loss: 3.8772 (3.7763)  loss_scale: 65536.0000 (63257.1703)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [23]  [1260/2809]  eta: 0:09:38  lr: 0.000021  min_lr: 0.000000  loss: 3.9594 (3.7773)  loss_scale: 65536.0000 (63275.2419)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [23]  [1270/2809]  eta: 0:09:34  lr: 0.000021  min_lr: 0.000000  loss: 3.9771 (3.7782)  loss_scale: 65536.0000 (63293.0291)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
[2025-01-13 06:10:01,325] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65886
[2025-01-13 06:10:01,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:10:01,325] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [1280/2809]  eta: 0:09:30  lr: 0.000021  min_lr: 0.000000  loss: 4.0459 (3.7789)  loss_scale: 65536.0000 (63259.3786)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [23]  [1290/2809]  eta: 0:09:27  lr: 0.000021  min_lr: 0.000000  loss: 3.7813 (3.7778)  loss_scale: 32768.0000 (63023.1944)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [23]  [1300/2809]  eta: 0:09:23  lr: 0.000021  min_lr: 0.000000  loss: 3.7883 (3.7796)  loss_scale: 32768.0000 (62790.6410)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [23]  [1310/2809]  eta: 0:09:19  lr: 0.000021  min_lr: 0.000000  loss: 4.0786 (3.7803)  loss_scale: 32768.0000 (62561.6354)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [23]  [1320/2809]  eta: 0:09:15  lr: 0.000021  min_lr: 0.000000  loss: 4.0617 (3.7806)  loss_scale: 32768.0000 (62336.0969)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [1330/2809]  eta: 0:09:12  lr: 0.000021  min_lr: 0.000000  loss: 3.8461 (3.7811)  loss_scale: 32768.0000 (62113.9474)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [1340/2809]  eta: 0:09:08  lr: 0.000021  min_lr: 0.000000  loss: 3.7357 (3.7808)  loss_scale: 32768.0000 (61895.1111)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [23]  [1350/2809]  eta: 0:09:04  lr: 0.000021  min_lr: 0.000000  loss: 3.6589 (3.7800)  loss_scale: 32768.0000 (61679.5144)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [23]  [1360/2809]  eta: 0:09:00  lr: 0.000021  min_lr: 0.000000  loss: 3.5487 (3.7784)  loss_scale: 32768.0000 (61467.0860)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [23]  [1370/2809]  eta: 0:08:57  lr: 0.000021  min_lr: 0.000000  loss: 3.4533 (3.7745)  loss_scale: 32768.0000 (61257.7564)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [23]  [1380/2809]  eta: 0:08:53  lr: 0.000021  min_lr: 0.000000  loss: 3.6036 (3.7751)  loss_scale: 32768.0000 (61051.4584)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [23]  [1390/2809]  eta: 0:08:49  lr: 0.000021  min_lr: 0.000000  loss: 3.8601 (3.7760)  loss_scale: 32768.0000 (60848.1265)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
[2025-01-13 06:10:43,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=66000, skipped=448, lr=[2.0706568880734621e-07, 2.0706568880734621e-07, 2.958081268676375e-07, 2.958081268676375e-07, 4.2258303838233926e-07, 4.2258303838233926e-07, 6.036900548319133e-07, 6.036900548319133e-07, 8.624143640455905e-07, 8.624143640455905e-07, 1.2320205200651293e-06, 1.2320205200651293e-06, 1.7600293143787562e-06, 1.7600293143787562e-06, 2.514327591969652e-06, 2.514327591969652e-06, 3.5918965599566456e-06, 3.5918965599566456e-06, 5.131280799938066e-06, 5.131280799938066e-06, 7.330401142768665e-06, 7.330401142768665e-06, 1.0472001632526666e-05, 1.0472001632526666e-05, 1.4960002332180953e-05, 1.4960002332180953e-05, 2.1371431903115647e-05, 2.1371431903115647e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 06:10:43,306] [INFO] [timer.py:260:stop] epoch=0/micro_step=66000/global_step=66000, RunningAvgSamplesPerSec=30.18739960343596, CurrSamplesPerSec=32.515832941407524, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [23]  [1400/2809]  eta: 0:08:45  lr: 0.000021  min_lr: 0.000000  loss: 3.8783 (3.7758)  loss_scale: 32768.0000 (60647.6974)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
[2025-01-13 06:10:49,312] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:10:49,312] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [1410/2809]  eta: 0:08:42  lr: 0.000021  min_lr: 0.000000  loss: 3.7380 (3.7757)  loss_scale: 32768.0000 (60519.7789)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0002  max mem: 15572
Epoch: [23]  [1420/2809]  eta: 0:08:38  lr: 0.000021  min_lr: 0.000000  loss: 3.6372 (3.7751)  loss_scale: 65536.0000 (60555.0795)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [23]  [1430/2809]  eta: 0:08:34  lr: 0.000021  min_lr: 0.000000  loss: 3.6698 (3.7739)  loss_scale: 65536.0000 (60589.8868)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [23]  [1440/2809]  eta: 0:08:30  lr: 0.000021  min_lr: 0.000000  loss: 3.7809 (3.7739)  loss_scale: 65536.0000 (60624.2110)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [23]  [1450/2809]  eta: 0:08:27  lr: 0.000021  min_lr: 0.000000  loss: 3.7809 (3.7741)  loss_scale: 65536.0000 (60658.0620)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [23]  [1460/2809]  eta: 0:08:23  lr: 0.000021  min_lr: 0.000000  loss: 3.9732 (3.7753)  loss_scale: 65536.0000 (60691.4497)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [23]  [1470/2809]  eta: 0:08:19  lr: 0.000021  min_lr: 0.000000  loss: 4.1087 (3.7769)  loss_scale: 65536.0000 (60724.3834)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [1480/2809]  eta: 0:08:15  lr: 0.000021  min_lr: 0.000000  loss: 4.0944 (3.7785)  loss_scale: 65536.0000 (60756.8724)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [23]  [1490/2809]  eta: 0:08:12  lr: 0.000021  min_lr: 0.000000  loss: 3.8332 (3.7786)  loss_scale: 65536.0000 (60788.9256)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [1500/2809]  eta: 0:08:08  lr: 0.000021  min_lr: 0.000000  loss: 4.0228 (3.7807)  loss_scale: 65536.0000 (60820.5516)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [23]  [1510/2809]  eta: 0:08:04  lr: 0.000021  min_lr: 0.000000  loss: 3.9912 (3.7807)  loss_scale: 65536.0000 (60851.7591)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0003  max mem: 15572
Epoch: [23]  [1520/2809]  eta: 0:08:00  lr: 0.000021  min_lr: 0.000000  loss: 3.8065 (3.7813)  loss_scale: 65536.0000 (60882.5562)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [23]  [1530/2809]  eta: 0:07:57  lr: 0.000021  min_lr: 0.000000  loss: 3.7785 (3.7813)  loss_scale: 65536.0000 (60912.9510)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0002  max mem: 15572
[2025-01-13 06:11:37,002] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:11:37,003] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [1540/2809]  eta: 0:07:53  lr: 0.000021  min_lr: 0.000000  loss: 3.7127 (3.7809)  loss_scale: 65536.0000 (61155.5925)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0003  max mem: 15572
[2025-01-13 06:11:38,906] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66148
[2025-01-13 06:11:38,906] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:11:38,906] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [1550/2809]  eta: 0:07:49  lr: 0.000021  min_lr: 0.000000  loss: 3.6747 (3.7813)  loss_scale: 65536.0000 (61183.8349)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0003  max mem: 15572
Epoch: [23]  [1560/2809]  eta: 0:07:46  lr: 0.000021  min_lr: 0.000000  loss: 3.6789 (3.7803)  loss_scale: 65536.0000 (61211.7156)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0003  max mem: 15572
Epoch: [23]  [1570/2809]  eta: 0:07:42  lr: 0.000021  min_lr: 0.000000  loss: 3.7029 (3.7796)  loss_scale: 65536.0000 (61239.2412)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [23]  [1580/2809]  eta: 0:07:38  lr: 0.000021  min_lr: 0.000000  loss: 3.7160 (3.7785)  loss_scale: 65536.0000 (61266.4187)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [23]  [1590/2809]  eta: 0:07:34  lr: 0.000021  min_lr: 0.000000  loss: 3.5821 (3.7770)  loss_scale: 65536.0000 (61293.2546)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [1600/2809]  eta: 0:07:31  lr: 0.000021  min_lr: 0.000000  loss: 3.7815 (3.7775)  loss_scale: 65536.0000 (61319.7552)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [23]  [1610/2809]  eta: 0:07:27  lr: 0.000021  min_lr: 0.000000  loss: 3.7833 (3.7766)  loss_scale: 65536.0000 (61345.9268)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [23]  [1620/2809]  eta: 0:07:23  lr: 0.000021  min_lr: 0.000000  loss: 3.7075 (3.7759)  loss_scale: 65536.0000 (61371.7754)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [23]  [1630/2809]  eta: 0:07:19  lr: 0.000021  min_lr: 0.000000  loss: 3.9375 (3.7764)  loss_scale: 65536.0000 (61397.3072)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [23]  [1640/2809]  eta: 0:07:16  lr: 0.000021  min_lr: 0.000000  loss: 3.8995 (3.7758)  loss_scale: 65536.0000 (61422.5277)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [23]  [1650/2809]  eta: 0:07:12  lr: 0.000021  min_lr: 0.000000  loss: 3.8764 (3.7758)  loss_scale: 65536.0000 (61447.4428)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [23]  [1660/2809]  eta: 0:07:08  lr: 0.000021  min_lr: 0.000000  loss: 3.8568 (3.7753)  loss_scale: 65536.0000 (61472.0578)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
[2025-01-13 06:12:26,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:12:26,862] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [1670/2809]  eta: 0:07:04  lr: 0.000021  min_lr: 0.000000  loss: 3.6949 (3.7744)  loss_scale: 65536.0000 (61535.5978)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
[2025-01-13 06:12:27,601] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66279
[2025-01-13 06:12:27,602] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:12:27,603] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [1680/2809]  eta: 0:07:01  lr: 0.000021  min_lr: 0.000000  loss: 3.5926 (3.7751)  loss_scale: 65536.0000 (61598.3819)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [23]  [1690/2809]  eta: 0:06:57  lr: 0.000021  min_lr: 0.000000  loss: 3.8221 (3.7751)  loss_scale: 65536.0000 (61621.6677)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [23]  [1700/2809]  eta: 0:06:53  lr: 0.000021  min_lr: 0.000000  loss: 3.9693 (3.7753)  loss_scale: 65536.0000 (61644.6796)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [23]  [1710/2809]  eta: 0:06:49  lr: 0.000021  min_lr: 0.000000  loss: 3.6910 (3.7739)  loss_scale: 65536.0000 (61667.4226)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [23]  [1720/2809]  eta: 0:06:46  lr: 0.000021  min_lr: 0.000000  loss: 3.5437 (3.7730)  loss_scale: 65536.0000 (61689.9012)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [23]  [1730/2809]  eta: 0:06:42  lr: 0.000021  min_lr: 0.000000  loss: 3.5689 (3.7729)  loss_scale: 65536.0000 (61712.1202)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [23]  [1740/2809]  eta: 0:06:38  lr: 0.000021  min_lr: 0.000000  loss: 3.8125 (3.7730)  loss_scale: 65536.0000 (61734.0839)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [23]  [1750/2809]  eta: 0:06:35  lr: 0.000021  min_lr: 0.000000  loss: 3.8265 (3.7742)  loss_scale: 65536.0000 (61755.7967)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [23]  [1760/2809]  eta: 0:06:31  lr: 0.000021  min_lr: 0.000000  loss: 3.9475 (3.7754)  loss_scale: 65536.0000 (61777.2629)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [23]  [1770/2809]  eta: 0:06:27  lr: 0.000021  min_lr: 0.000000  loss: 4.0818 (3.7755)  loss_scale: 65536.0000 (61798.4867)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [23]  [1780/2809]  eta: 0:06:23  lr: 0.000021  min_lr: 0.000000  loss: 3.9143 (3.7762)  loss_scale: 65536.0000 (61819.4722)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0003  max mem: 15572
Epoch: [23]  [1790/2809]  eta: 0:06:20  lr: 0.000021  min_lr: 0.000000  loss: 3.9377 (3.7773)  loss_scale: 65536.0000 (61840.2233)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [23]  [1800/2809]  eta: 0:06:16  lr: 0.000021  min_lr: 0.000000  loss: 3.9377 (3.7771)  loss_scale: 65536.0000 (61860.7440)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
[2025-01-13 06:13:15,618] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:13:15,619] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:13:15,978] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66409
[2025-01-13 06:13:15,978] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:13:15,978] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [1810/2809]  eta: 0:06:12  lr: 0.000021  min_lr: 0.000000  loss: 3.7748 (3.7768)  loss_scale: 65536.0000 (61917.2258)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [23]  [1820/2809]  eta: 0:06:08  lr: 0.000021  min_lr: 0.000000  loss: 3.8426 (3.7768)  loss_scale: 65536.0000 (61937.0983)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [23]  [1830/2809]  eta: 0:06:05  lr: 0.000021  min_lr: 0.000000  loss: 3.8723 (3.7771)  loss_scale: 65536.0000 (61956.7537)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [23]  [1840/2809]  eta: 0:06:01  lr: 0.000021  min_lr: 0.000000  loss: 3.7084 (3.7757)  loss_scale: 65536.0000 (61976.1955)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [23]  [1850/2809]  eta: 0:05:57  lr: 0.000021  min_lr: 0.000000  loss: 3.8238 (3.7760)  loss_scale: 65536.0000 (61995.4273)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [1860/2809]  eta: 0:05:53  lr: 0.000021  min_lr: 0.000000  loss: 3.9011 (3.7759)  loss_scale: 65536.0000 (62014.4524)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
[2025-01-13 06:13:37,837] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66468
[2025-01-13 06:13:37,837] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:13:37,837] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [1870/2809]  eta: 0:05:50  lr: 0.000021  min_lr: 0.000000  loss: 3.4986 (3.7746)  loss_scale: 32768.0000 (61858.1379)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [23]  [1880/2809]  eta: 0:05:46  lr: 0.000021  min_lr: 0.000000  loss: 3.9007 (3.7759)  loss_scale: 32768.0000 (61703.4854)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [23]  [1890/2809]  eta: 0:05:42  lr: 0.000021  min_lr: 0.000000  loss: 3.7524 (3.7740)  loss_scale: 32768.0000 (61550.4685)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [23]  [1900/2809]  eta: 0:05:38  lr: 0.000021  min_lr: 0.000000  loss: 3.6925 (3.7743)  loss_scale: 32768.0000 (61399.0615)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
Epoch: [23]  [1910/2809]  eta: 0:05:35  lr: 0.000021  min_lr: 0.000000  loss: 3.7811 (3.7742)  loss_scale: 32768.0000 (61249.2391)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [23]  [1920/2809]  eta: 0:05:31  lr: 0.000021  min_lr: 0.000000  loss: 3.7390 (3.7744)  loss_scale: 32768.0000 (61100.9766)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [23]  [1930/2809]  eta: 0:05:27  lr: 0.000021  min_lr: 0.000000  loss: 3.7318 (3.7744)  loss_scale: 32768.0000 (60954.2496)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [23]  [1940/2809]  eta: 0:05:23  lr: 0.000021  min_lr: 0.000000  loss: 3.7620 (3.7735)  loss_scale: 32768.0000 (60809.0345)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0017  max mem: 15572
Epoch: [23]  [1950/2809]  eta: 0:05:20  lr: 0.000021  min_lr: 0.000000  loss: 3.8044 (3.7745)  loss_scale: 32768.0000 (60665.3080)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0017  max mem: 15572
Epoch: [23]  [1960/2809]  eta: 0:05:16  lr: 0.000021  min_lr: 0.000000  loss: 4.0501 (3.7754)  loss_scale: 32768.0000 (60523.0474)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [23]  [1970/2809]  eta: 0:05:12  lr: 0.000021  min_lr: 0.000000  loss: 4.0501 (3.7767)  loss_scale: 32768.0000 (60382.2303)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [23]  [1980/2809]  eta: 0:05:09  lr: 0.000021  min_lr: 0.000000  loss: 3.8405 (3.7763)  loss_scale: 32768.0000 (60242.8349)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
[2025-01-13 06:14:25,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:14:25,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [1990/2809]  eta: 0:05:05  lr: 0.000021  min_lr: 0.000000  loss: 3.7879 (3.7762)  loss_scale: 32768.0000 (60121.2978)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [23]  [2000/2809]  eta: 0:05:01  lr: 0.000021  min_lr: 0.000000  loss: 3.6133 (3.7757)  loss_scale: 65536.0000 (60148.3578)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [23]  [2010/2809]  eta: 0:04:57  lr: 0.000021  min_lr: 0.000000  loss: 3.6403 (3.7759)  loss_scale: 65536.0000 (60175.1487)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [23]  [2020/2809]  eta: 0:04:54  lr: 0.000021  min_lr: 0.000000  loss: 3.7005 (3.7745)  loss_scale: 65536.0000 (60201.6744)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [23]  [2030/2809]  eta: 0:04:50  lr: 0.000021  min_lr: 0.000000  loss: 3.7005 (3.7744)  loss_scale: 65536.0000 (60227.9389)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [23]  [2040/2809]  eta: 0:04:46  lr: 0.000021  min_lr: 0.000000  loss: 3.8912 (3.7744)  loss_scale: 65536.0000 (60253.9461)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [23]  [2050/2809]  eta: 0:04:42  lr: 0.000021  min_lr: 0.000000  loss: 3.8284 (3.7752)  loss_scale: 65536.0000 (60279.6997)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [23]  [2060/2809]  eta: 0:04:39  lr: 0.000021  min_lr: 0.000000  loss: 3.8254 (3.7748)  loss_scale: 65536.0000 (60305.2033)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [23]  [2070/2809]  eta: 0:04:35  lr: 0.000021  min_lr: 0.000000  loss: 3.7585 (3.7747)  loss_scale: 65536.0000 (60330.4606)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [2080/2809]  eta: 0:04:31  lr: 0.000021  min_lr: 0.000000  loss: 3.7735 (3.7754)  loss_scale: 65536.0000 (60355.4753)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [23]  [2090/2809]  eta: 0:04:27  lr: 0.000021  min_lr: 0.000000  loss: 3.7143 (3.7743)  loss_scale: 65536.0000 (60380.2506)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [23]  [2100/2809]  eta: 0:04:24  lr: 0.000021  min_lr: 0.000000  loss: 3.6871 (3.7748)  loss_scale: 65536.0000 (60404.7901)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [23]  [2110/2809]  eta: 0:04:20  lr: 0.000021  min_lr: 0.000000  loss: 3.8278 (3.7747)  loss_scale: 65536.0000 (60429.0971)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
[2025-01-13 06:15:13,269] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:15:13,269] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [2120/2809]  eta: 0:04:16  lr: 0.000021  min_lr: 0.000000  loss: 3.8043 (3.7750)  loss_scale: 65536.0000 (60545.8708)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
[2025-01-13 06:15:14,426] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66728
[2025-01-13 06:15:14,426] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:15:14,426] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2130/2809]  eta: 0:04:13  lr: 0.000021  min_lr: 0.000000  loss: 3.6874 (3.7746)  loss_scale: 65536.0000 (60569.2877)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [23]  [2140/2809]  eta: 0:04:09  lr: 0.000021  min_lr: 0.000000  loss: 3.5966 (3.7750)  loss_scale: 65536.0000 (60592.4858)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [23]  [2150/2809]  eta: 0:04:05  lr: 0.000021  min_lr: 0.000000  loss: 3.7740 (3.7751)  loss_scale: 65536.0000 (60615.4682)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [23]  [2160/2809]  eta: 0:04:01  lr: 0.000021  min_lr: 0.000000  loss: 3.8905 (3.7760)  loss_scale: 65536.0000 (60638.2379)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [23]  [2170/2809]  eta: 0:03:58  lr: 0.000021  min_lr: 0.000000  loss: 4.0482 (3.7762)  loss_scale: 65536.0000 (60660.7978)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
Epoch: [23]  [2180/2809]  eta: 0:03:54  lr: 0.000021  min_lr: 0.000000  loss: 4.0408 (3.7770)  loss_scale: 65536.0000 (60683.1508)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [23]  [2190/2809]  eta: 0:03:50  lr: 0.000021  min_lr: 0.000000  loss: 3.8999 (3.7770)  loss_scale: 65536.0000 (60705.2999)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [23]  [2200/2809]  eta: 0:03:46  lr: 0.000021  min_lr: 0.000000  loss: 3.7796 (3.7769)  loss_scale: 65536.0000 (60727.2476)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [23]  [2210/2809]  eta: 0:03:43  lr: 0.000021  min_lr: 0.000000  loss: 3.7796 (3.7770)  loss_scale: 65536.0000 (60748.9968)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [23]  [2220/2809]  eta: 0:03:39  lr: 0.000021  min_lr: 0.000000  loss: 3.9201 (3.7776)  loss_scale: 65536.0000 (60770.5502)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0002  max mem: 15572
Epoch: [23]  [2230/2809]  eta: 0:03:35  lr: 0.000021  min_lr: 0.000000  loss: 3.9201 (3.7785)  loss_scale: 65536.0000 (60791.9104)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0002  max mem: 15572
Epoch: [23]  [2240/2809]  eta: 0:03:32  lr: 0.000021  min_lr: 0.000000  loss: 4.0265 (3.7790)  loss_scale: 65536.0000 (60813.0799)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
[2025-01-13 06:16:02,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:16:02,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [2250/2809]  eta: 0:03:28  lr: 0.000021  min_lr: 0.000000  loss: 4.0480 (3.7793)  loss_scale: 65536.0000 (60863.1755)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
[2025-01-13 06:16:03,334] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66859
[2025-01-13 06:16:03,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:16:03,334] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2260/2809]  eta: 0:03:24  lr: 0.000021  min_lr: 0.000000  loss: 3.8276 (3.7796)  loss_scale: 65536.0000 (60912.8280)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0002  max mem: 15572
Epoch: [23]  [2270/2809]  eta: 0:03:20  lr: 0.000021  min_lr: 0.000000  loss: 3.7881 (3.7792)  loss_scale: 65536.0000 (60933.1854)  weight_decay: 0.0500 (0.0500)  time: 0.3795  data: 0.0003  max mem: 15572
Epoch: [23]  [2280/2809]  eta: 0:03:17  lr: 0.000021  min_lr: 0.000000  loss: 3.7670 (3.7785)  loss_scale: 65536.0000 (60953.3643)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
Epoch: [23]  [2290/2809]  eta: 0:03:13  lr: 0.000021  min_lr: 0.000000  loss: 3.8023 (3.7785)  loss_scale: 65536.0000 (60973.3671)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [23]  [2300/2809]  eta: 0:03:09  lr: 0.000021  min_lr: 0.000000  loss: 3.7334 (3.7776)  loss_scale: 65536.0000 (60993.1960)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [23]  [2310/2809]  eta: 0:03:05  lr: 0.000021  min_lr: 0.000000  loss: 3.7831 (3.7779)  loss_scale: 65536.0000 (61012.8533)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [23]  [2320/2809]  eta: 0:03:02  lr: 0.000021  min_lr: 0.000000  loss: 3.9040 (3.7786)  loss_scale: 65536.0000 (61032.3412)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [23]  [2330/2809]  eta: 0:02:58  lr: 0.000021  min_lr: 0.000000  loss: 3.8961 (3.7778)  loss_scale: 65536.0000 (61051.6619)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [23]  [2340/2809]  eta: 0:02:54  lr: 0.000021  min_lr: 0.000000  loss: 3.8064 (3.7781)  loss_scale: 65536.0000 (61070.8176)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [23]  [2350/2809]  eta: 0:02:51  lr: 0.000021  min_lr: 0.000000  loss: 3.8415 (3.7783)  loss_scale: 65536.0000 (61089.8103)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
Epoch: [23]  [2360/2809]  eta: 0:02:47  lr: 0.000021  min_lr: 0.000000  loss: 3.8845 (3.7787)  loss_scale: 65536.0000 (61108.6421)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0002  max mem: 15572
Epoch: [23]  [2370/2809]  eta: 0:02:43  lr: 0.000021  min_lr: 0.000000  loss: 3.8718 (3.7785)  loss_scale: 65536.0000 (61127.3151)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [23]  [2380/2809]  eta: 0:02:39  lr: 0.000021  min_lr: 0.000000  loss: 3.7828 (3.7788)  loss_scale: 65536.0000 (61145.8312)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
[2025-01-13 06:16:51,473] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:16:51,473] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:16:54,025] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66995
[2025-01-13 06:16:54,025] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:16:54,025] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2390/2809]  eta: 0:02:36  lr: 0.000021  min_lr: 0.000000  loss: 3.6820 (3.7780)  loss_scale: 65536.0000 (61356.0586)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 06:16:55,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=67000, skipped=455, lr=[1.998571212807982e-07, 1.998571212807982e-07, 2.855101732582832e-07, 2.855101732582832e-07, 4.0787167608326175e-07, 4.0787167608326175e-07, 5.826738229760882e-07, 5.826738229760882e-07, 8.323911756801261e-07, 8.323911756801261e-07, 1.1891302509716086e-06, 1.1891302509716086e-06, 1.6987575013880125e-06, 1.6987575013880125e-06, 2.426796430554304e-06, 2.426796430554304e-06, 3.4668520436490055e-06, 3.4668520436490055e-06, 4.952645776641437e-06, 4.952645776641437e-06, 7.07520825234491e-06, 7.07520825234491e-06, 1.010744036049273e-05, 1.010744036049273e-05, 1.4439200514989614e-05, 1.4439200514989614e-05, 2.0627429307128022e-05, 2.0627429307128022e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 06:16:55,516] [INFO] [timer.py:260:stop] epoch=0/micro_step=67000/global_step=67000, RunningAvgSamplesPerSec=30.23916937474102, CurrSamplesPerSec=34.30268796685023, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [23]  [2400/2809]  eta: 0:02:32  lr: 0.000021  min_lr: 0.000000  loss: 3.6565 (3.7772)  loss_scale: 65536.0000 (61373.4677)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [23]  [2410/2809]  eta: 0:02:28  lr: 0.000021  min_lr: 0.000000  loss: 3.7525 (3.7775)  loss_scale: 65536.0000 (61390.7325)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [23]  [2420/2809]  eta: 0:02:24  lr: 0.000021  min_lr: 0.000000  loss: 3.7714 (3.7764)  loss_scale: 65536.0000 (61407.8546)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0001  max mem: 15572
Epoch: [23]  [2430/2809]  eta: 0:02:21  lr: 0.000021  min_lr: 0.000000  loss: 3.6528 (3.7764)  loss_scale: 65536.0000 (61424.8359)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [23]  [2440/2809]  eta: 0:02:17  lr: 0.000021  min_lr: 0.000000  loss: 3.6528 (3.7767)  loss_scale: 65536.0000 (61441.6780)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [23]  [2450/2809]  eta: 0:02:13  lr: 0.000021  min_lr: 0.000000  loss: 3.6937 (3.7764)  loss_scale: 65536.0000 (61458.3827)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [23]  [2460/2809]  eta: 0:02:10  lr: 0.000021  min_lr: 0.000000  loss: 3.8426 (3.7772)  loss_scale: 65536.0000 (61474.9516)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [23]  [2470/2809]  eta: 0:02:06  lr: 0.000021  min_lr: 0.000000  loss: 3.9395 (3.7778)  loss_scale: 65536.0000 (61491.3865)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [23]  [2480/2809]  eta: 0:02:02  lr: 0.000021  min_lr: 0.000000  loss: 3.5986 (3.7763)  loss_scale: 65536.0000 (61507.6888)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [23]  [2490/2809]  eta: 0:01:58  lr: 0.000021  min_lr: 0.000000  loss: 3.5986 (3.7760)  loss_scale: 65536.0000 (61523.8603)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
Epoch: [23]  [2500/2809]  eta: 0:01:55  lr: 0.000021  min_lr: 0.000000  loss: 3.6593 (3.7759)  loss_scale: 65536.0000 (61539.9024)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0003  max mem: 15572
Epoch: [23]  [2510/2809]  eta: 0:01:51  lr: 0.000021  min_lr: 0.000000  loss: 3.6337 (3.7749)  loss_scale: 65536.0000 (61555.8168)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
[2025-01-13 06:17:41,644] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:17:41,644] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [2520/2809]  eta: 0:01:47  lr: 0.000021  min_lr: 0.000000  loss: 3.7546 (3.7757)  loss_scale: 65536.0000 (61675.5891)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0003  max mem: 15572
[2025-01-13 06:17:43,862] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67130
[2025-01-13 06:17:43,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:17:43,862] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2530/2809]  eta: 0:01:43  lr: 0.000021  min_lr: 0.000000  loss: 3.9869 (3.7761)  loss_scale: 65536.0000 (61742.6282)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0003  max mem: 15572
Epoch: [23]  [2540/2809]  eta: 0:01:40  lr: 0.000021  min_lr: 0.000000  loss: 3.8127 (3.7760)  loss_scale: 65536.0000 (61757.5569)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [23]  [2550/2809]  eta: 0:01:36  lr: 0.000021  min_lr: 0.000000  loss: 3.4299 (3.7751)  loss_scale: 65536.0000 (61772.3685)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [23]  [2560/2809]  eta: 0:01:32  lr: 0.000021  min_lr: 0.000000  loss: 3.4901 (3.7752)  loss_scale: 65536.0000 (61787.0644)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [23]  [2570/2809]  eta: 0:01:29  lr: 0.000020  min_lr: 0.000000  loss: 3.8233 (3.7757)  loss_scale: 65536.0000 (61801.6461)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [23]  [2580/2809]  eta: 0:01:25  lr: 0.000020  min_lr: 0.000000  loss: 3.8425 (3.7765)  loss_scale: 65536.0000 (61816.1147)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [23]  [2590/2809]  eta: 0:01:21  lr: 0.000020  min_lr: 0.000000  loss: 3.7362 (3.7758)  loss_scale: 65536.0000 (61830.4716)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [23]  [2600/2809]  eta: 0:01:17  lr: 0.000020  min_lr: 0.000000  loss: 3.5625 (3.7753)  loss_scale: 65536.0000 (61844.7182)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [23]  [2610/2809]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000000  loss: 3.8475 (3.7762)  loss_scale: 65536.0000 (61858.8556)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [23]  [2620/2809]  eta: 0:01:10  lr: 0.000020  min_lr: 0.000000  loss: 3.7301 (3.7754)  loss_scale: 65536.0000 (61872.8852)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [23]  [2630/2809]  eta: 0:01:06  lr: 0.000020  min_lr: 0.000000  loss: 3.6752 (3.7763)  loss_scale: 65536.0000 (61886.8081)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [23]  [2640/2809]  eta: 0:01:02  lr: 0.000020  min_lr: 0.000000  loss: 3.6752 (3.7755)  loss_scale: 65536.0000 (61900.6255)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [23]  [2650/2809]  eta: 0:00:59  lr: 0.000020  min_lr: 0.000000  loss: 3.6353 (3.7751)  loss_scale: 65536.0000 (61914.3387)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0001  max mem: 15572
[2025-01-13 06:18:31,734] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:18:31,734] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:18:32,813] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67262
[2025-01-13 06:18:32,813] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:18:32,813] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2660/2809]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 3.6353 (3.7739)  loss_scale: 65536.0000 (62001.8339)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [23]  [2670/2809]  eta: 0:00:51  lr: 0.000020  min_lr: 0.000000  loss: 3.5079 (3.7734)  loss_scale: 65536.0000 (62015.0655)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [23]  [2680/2809]  eta: 0:00:48  lr: 0.000020  min_lr: 0.000000  loss: 3.7979 (3.7740)  loss_scale: 65536.0000 (62028.1984)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [23]  [2690/2809]  eta: 0:00:44  lr: 0.000020  min_lr: 0.000000  loss: 3.8944 (3.7742)  loss_scale: 65536.0000 (62041.2337)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0001  max mem: 15572
Epoch: [23]  [2700/2809]  eta: 0:00:40  lr: 0.000020  min_lr: 0.000000  loss: 3.8944 (3.7745)  loss_scale: 65536.0000 (62054.1725)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [23]  [2710/2809]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 3.6124 (3.7737)  loss_scale: 65536.0000 (62067.0159)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [23]  [2720/2809]  eta: 0:00:33  lr: 0.000020  min_lr: 0.000000  loss: 3.5298 (3.7726)  loss_scale: 65536.0000 (62079.7648)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [23]  [2730/2809]  eta: 0:00:29  lr: 0.000020  min_lr: 0.000000  loss: 3.6099 (3.7723)  loss_scale: 65536.0000 (62092.4204)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [23]  [2740/2809]  eta: 0:00:25  lr: 0.000020  min_lr: 0.000000  loss: 3.8824 (3.7730)  loss_scale: 65536.0000 (62104.9836)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [23]  [2750/2809]  eta: 0:00:21  lr: 0.000020  min_lr: 0.000000  loss: 3.9228 (3.7735)  loss_scale: 65536.0000 (62117.4555)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [23]  [2760/2809]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 3.9484 (3.7741)  loss_scale: 65536.0000 (62129.8370)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [23]  [2770/2809]  eta: 0:00:14  lr: 0.000020  min_lr: 0.000000  loss: 3.9226 (3.7741)  loss_scale: 65536.0000 (62142.1292)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [23]  [2780/2809]  eta: 0:00:10  lr: 0.000020  min_lr: 0.000000  loss: 3.6806 (3.7736)  loss_scale: 65536.0000 (62154.3330)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
[2025-01-13 06:19:20,680] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:19:20,681] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:19:21,045] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67392
[2025-01-13 06:19:21,045] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:19:21,045] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2790/2809]  eta: 0:00:07  lr: 0.000020  min_lr: 0.000000  loss: 3.6562 (3.7737)  loss_scale: 65536.0000 (62189.9305)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0003  max mem: 15572
Epoch: [23]  [2800/2809]  eta: 0:00:03  lr: 0.000020  min_lr: 0.000000  loss: 3.8311 (3.7736)  loss_scale: 65536.0000 (62201.8765)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [23]  [2808/2809]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 3.8760 (3.7736)  loss_scale: 65536.0000 (62211.3720)  weight_decay: 0.0500 (0.0500)  time: 0.3610  data: 0.0001  max mem: 15572
Epoch: [23] Total time: 0:17:26 (0.3725 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 3.8760 (3.7736)  loss_scale: 65536.0000 (62211.3720)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:11:02  loss: 0.4880 (0.4880)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4345  data: 2.2491  max mem: 15572
Val:  [ 10/272]  eta: 0:01:48  loss: 2.6335 (2.4047)  acc1: 50.0000 (41.9192)  acc5: 72.2222 (68.1818)  time: 0.4154  data: 0.2595  max mem: 15572
Val:  [ 20/272]  eta: 0:01:13  loss: 2.4405 (2.4187)  acc1: 50.0000 (45.2381)  acc5: 72.2222 (70.1058)  time: 0.1847  data: 0.0305  max mem: 15572
Val:  [ 30/272]  eta: 0:01:00  loss: 2.4405 (2.5010)  acc1: 44.4444 (41.5771)  acc5: 72.2222 (69.8925)  time: 0.1602  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:53  loss: 2.5570 (2.5293)  acc1: 27.7778 (38.7534)  acc5: 72.2222 (71.0027)  time: 0.1661  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:47  loss: 2.4226 (2.4510)  acc1: 33.3333 (40.9586)  acc5: 77.7778 (73.0937)  time: 0.1607  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:43  loss: 1.6374 (2.3351)  acc1: 66.6667 (44.2623)  acc5: 83.3333 (74.4080)  time: 0.1526  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:40  loss: 1.5583 (2.2556)  acc1: 66.6667 (46.9484)  acc5: 83.3333 (75.4304)  time: 0.1562  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:36  loss: 2.0093 (2.2858)  acc1: 50.0000 (46.5706)  acc5: 77.7778 (75.1029)  time: 0.1542  data: 0.0003  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.4053 (2.3072)  acc1: 44.4444 (46.6422)  acc5: 72.2222 (74.8474)  time: 0.1534  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:31  loss: 2.3399 (2.3318)  acc1: 44.4444 (46.0946)  acc5: 77.7778 (74.9175)  time: 0.1573  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:29  loss: 2.3763 (2.4074)  acc1: 27.7778 (44.1441)  acc5: 77.7778 (73.8238)  time: 0.1588  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 2.9485 (2.4471)  acc1: 27.7778 (43.2966)  acc5: 66.6667 (73.2323)  time: 0.1597  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.3048 (2.4099)  acc1: 44.4444 (44.1476)  acc5: 77.7778 (73.9186)  time: 0.1548  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.9047 (2.3982)  acc1: 50.0000 (44.6020)  acc5: 77.7778 (73.8771)  time: 0.1569  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.2908 (2.3988)  acc1: 38.8889 (44.0765)  acc5: 72.2222 (74.2090)  time: 0.1598  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.2908 (2.3842)  acc1: 44.4444 (44.5480)  acc5: 77.7778 (74.6032)  time: 0.1567  data: 0.0013  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.4882 (2.4021)  acc1: 44.4444 (43.8596)  acc5: 72.2222 (74.2690)  time: 0.1584  data: 0.0014  max mem: 15572
Val:  [180/272]  eta: 0:00:15  loss: 2.2596 (2.3875)  acc1: 33.3333 (43.7078)  acc5: 77.7778 (74.7391)  time: 0.1619  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.3124 (2.4341)  acc1: 33.3333 (42.5247)  acc5: 77.7778 (73.5311)  time: 0.1598  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.5869 (2.4413)  acc1: 33.3333 (42.2609)  acc5: 66.6667 (73.3278)  time: 0.1534  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.1950 (2.4443)  acc1: 50.0000 (42.4961)  acc5: 77.7778 (73.3544)  time: 0.1486  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.2969 (2.4337)  acc1: 55.5556 (42.9110)  acc5: 77.7778 (73.5797)  time: 0.1553  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.9467 (2.4078)  acc1: 61.1111 (43.8913)  acc5: 77.7778 (73.8817)  time: 0.1644  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.7749 (2.3937)  acc1: 55.5556 (44.0987)  acc5: 83.3333 (74.2508)  time: 0.1648  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.3127 (2.4042)  acc1: 38.8889 (43.5148)  acc5: 83.3333 (74.1700)  time: 0.1571  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.2691 (2.3421)  acc1: 77.7778 (45.3597)  acc5: 88.8889 (74.9468)  time: 0.1493  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4140 (2.3367)  acc1: 77.7778 (45.4900)  acc5: 83.3333 (75.1538)  time: 0.1404  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4140 (2.3414)  acc1: 61.1111 (45.4639)  acc5: 83.3333 (75.1178)  time: 0.1339  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1670 s / it)
* Acc@1 45.464 Acc@5 75.118 loss 2.341
Accuracy of the network on the 4883 val videos: 45.5%
[2025-01-13 06:20:14,961] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 06:20:14,963] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 06:20:14,963] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 06:20:17,384] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 06:20:17,384] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.46%
Epoch: [24]  [   0/2809]  eta: 2:33:33  lr: 0.000020  min_lr: 0.000000  loss: 3.4855 (3.4855)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.2801  data: 2.8875  max mem: 15572
Epoch: [24]  [  10/2809]  eta: 0:31:31  lr: 0.000020  min_lr: 0.000000  loss: 3.8828 (3.7069)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6759  data: 0.2913  max mem: 15572
Epoch: [24]  [  20/2809]  eta: 0:24:51  lr: 0.000020  min_lr: 0.000000  loss: 3.7429 (3.6978)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3974  data: 0.0160  max mem: 15572
Epoch: [24]  [  30/2809]  eta: 0:22:18  lr: 0.000020  min_lr: 0.000000  loss: 3.8081 (3.8043)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [24]  [  40/2809]  eta: 0:20:59  lr: 0.000020  min_lr: 0.000000  loss: 4.0605 (3.7636)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0001  max mem: 15572
Epoch: [24]  [  50/2809]  eta: 0:20:05  lr: 0.000020  min_lr: 0.000000  loss: 3.5880 (3.7533)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [24]  [  60/2809]  eta: 0:19:29  lr: 0.000020  min_lr: 0.000000  loss: 3.6396 (3.7297)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3653  data: 0.0002  max mem: 15572
Epoch: [24]  [  70/2809]  eta: 0:19:05  lr: 0.000020  min_lr: 0.000000  loss: 3.6911 (3.7400)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [24]  [  80/2809]  eta: 0:18:44  lr: 0.000020  min_lr: 0.000000  loss: 3.5415 (3.7198)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [24]  [  90/2809]  eta: 0:18:26  lr: 0.000020  min_lr: 0.000000  loss: 3.6907 (3.7334)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [24]  [ 100/2809]  eta: 0:18:12  lr: 0.000020  min_lr: 0.000000  loss: 3.8067 (3.7212)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
[2025-01-13 06:20:59,974] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:20:59,974] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:21:00,691] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67523
[2025-01-13 06:21:00,691] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:21:00,692] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [ 110/2809]  eta: 0:17:59  lr: 0.000020  min_lr: 0.000000  loss: 3.6927 (3.7188)  loss_scale: 65536.0000 (66716.8288)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [24]  [ 120/2809]  eta: 0:17:49  lr: 0.000020  min_lr: 0.000000  loss: 3.5992 (3.7080)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [24]  [ 130/2809]  eta: 0:17:40  lr: 0.000020  min_lr: 0.000000  loss: 3.5528 (3.7057)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [24]  [ 140/2809]  eta: 0:17:30  lr: 0.000020  min_lr: 0.000000  loss: 3.4851 (3.6858)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [24]  [ 150/2809]  eta: 0:17:21  lr: 0.000020  min_lr: 0.000000  loss: 3.5699 (3.7001)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [24]  [ 160/2809]  eta: 0:17:13  lr: 0.000020  min_lr: 0.000000  loss: 3.7944 (3.7026)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [24]  [ 170/2809]  eta: 0:17:06  lr: 0.000020  min_lr: 0.000000  loss: 3.8531 (3.7119)  loss_scale: 65536.0000 (66302.5029)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [24]  [ 180/2809]  eta: 0:17:00  lr: 0.000020  min_lr: 0.000000  loss: 3.8720 (3.7103)  loss_scale: 65536.0000 (66260.1547)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [24]  [ 190/2809]  eta: 0:16:54  lr: 0.000020  min_lr: 0.000000  loss: 3.7632 (3.7126)  loss_scale: 65536.0000 (66222.2408)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [24]  [ 200/2809]  eta: 0:16:48  lr: 0.000020  min_lr: 0.000000  loss: 3.7994 (3.7144)  loss_scale: 65536.0000 (66188.0995)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [24]  [ 210/2809]  eta: 0:16:42  lr: 0.000020  min_lr: 0.000000  loss: 3.7591 (3.7069)  loss_scale: 65536.0000 (66157.1943)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [24]  [ 220/2809]  eta: 0:16:37  lr: 0.000020  min_lr: 0.000000  loss: 3.7591 (3.7120)  loss_scale: 65536.0000 (66129.0860)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [24]  [ 230/2809]  eta: 0:16:31  lr: 0.000020  min_lr: 0.000000  loss: 3.8750 (3.7172)  loss_scale: 65536.0000 (66103.4113)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
[2025-01-13 06:21:48,410] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:21:48,410] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:21:49,500] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67655
[2025-01-13 06:21:49,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:21:49,500] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [ 240/2809]  eta: 0:16:25  lr: 0.000020  min_lr: 0.000000  loss: 3.8162 (3.7178)  loss_scale: 65536.0000 (66895.6680)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [24]  [ 250/2809]  eta: 0:16:19  lr: 0.000020  min_lr: 0.000000  loss: 3.9334 (3.7269)  loss_scale: 65536.0000 (66841.4980)  weight_decay: 0.0500 (0.0500)  time: 0.3640  data: 0.0002  max mem: 15572
Epoch: [24]  [ 260/2809]  eta: 0:16:14  lr: 0.000020  min_lr: 0.000000  loss: 3.8783 (3.7290)  loss_scale: 65536.0000 (66791.4789)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [24]  [ 270/2809]  eta: 0:16:09  lr: 0.000020  min_lr: 0.000000  loss: 3.8783 (3.7356)  loss_scale: 65536.0000 (66745.1513)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [24]  [ 280/2809]  eta: 0:16:05  lr: 0.000020  min_lr: 0.000000  loss: 4.0263 (3.7381)  loss_scale: 65536.0000 (66702.1210)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [24]  [ 290/2809]  eta: 0:16:00  lr: 0.000020  min_lr: 0.000000  loss: 4.0263 (3.7424)  loss_scale: 65536.0000 (66662.0481)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [24]  [ 300/2809]  eta: 0:15:55  lr: 0.000020  min_lr: 0.000000  loss: 3.9306 (3.7409)  loss_scale: 65536.0000 (66624.6379)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
[2025-01-13 06:22:13,527] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67720
[2025-01-13 06:22:13,528] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:22:13,528] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 310/2809]  eta: 0:15:50  lr: 0.000020  min_lr: 0.000000  loss: 3.7118 (3.7402)  loss_scale: 65536.0000 (65852.0900)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [24]  [ 320/2809]  eta: 0:15:46  lr: 0.000020  min_lr: 0.000000  loss: 3.7169 (3.7441)  loss_scale: 32768.0000 (64821.4330)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0003  max mem: 15572
Epoch: [24]  [ 330/2809]  eta: 0:15:41  lr: 0.000020  min_lr: 0.000000  loss: 3.6516 (3.7453)  loss_scale: 32768.0000 (63853.0514)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [24]  [ 340/2809]  eta: 0:15:36  lr: 0.000020  min_lr: 0.000000  loss: 3.5244 (3.7389)  loss_scale: 32768.0000 (62941.4663)  weight_decay: 0.0500 (0.0500)  time: 0.3653  data: 0.0002  max mem: 15572
Epoch: [24]  [ 350/2809]  eta: 0:15:32  lr: 0.000020  min_lr: 0.000000  loss: 3.6020 (3.7394)  loss_scale: 32768.0000 (62081.8234)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [24]  [ 360/2809]  eta: 0:15:28  lr: 0.000020  min_lr: 0.000000  loss: 3.6392 (3.7381)  loss_scale: 32768.0000 (61269.8061)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [24]  [ 370/2809]  eta: 0:15:23  lr: 0.000020  min_lr: 0.000000  loss: 3.8123 (3.7386)  loss_scale: 32768.0000 (60501.5633)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [24]  [ 380/2809]  eta: 0:15:18  lr: 0.000020  min_lr: 0.000000  loss: 3.8923 (3.7447)  loss_scale: 32768.0000 (59773.6483)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [24]  [ 390/2809]  eta: 0:15:14  lr: 0.000020  min_lr: 0.000000  loss: 3.8558 (3.7454)  loss_scale: 32768.0000 (59082.9668)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [24]  [ 400/2809]  eta: 0:15:10  lr: 0.000020  min_lr: 0.000000  loss: 3.7342 (3.7454)  loss_scale: 32768.0000 (58426.7332)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [24]  [ 410/2809]  eta: 0:15:05  lr: 0.000020  min_lr: 0.000000  loss: 3.7253 (3.7445)  loss_scale: 32768.0000 (57802.4331)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [24]  [ 420/2809]  eta: 0:15:01  lr: 0.000020  min_lr: 0.000000  loss: 3.6555 (3.7425)  loss_scale: 32768.0000 (57207.7910)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [24]  [ 430/2809]  eta: 0:14:57  lr: 0.000020  min_lr: 0.000000  loss: 3.6555 (3.7407)  loss_scale: 32768.0000 (56640.7425)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
[2025-01-13 06:23:01,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:23:01,246] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 440/2809]  eta: 0:14:53  lr: 0.000020  min_lr: 0.000000  loss: 3.4160 (3.7337)  loss_scale: 32768.0000 (56693.8413)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [24]  [ 450/2809]  eta: 0:14:49  lr: 0.000020  min_lr: 0.000000  loss: 3.4160 (3.7313)  loss_scale: 65536.0000 (56889.8980)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [24]  [ 460/2809]  eta: 0:14:45  lr: 0.000020  min_lr: 0.000000  loss: 3.7273 (3.7339)  loss_scale: 65536.0000 (57077.4490)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [24]  [ 470/2809]  eta: 0:14:41  lr: 0.000020  min_lr: 0.000000  loss: 3.8290 (3.7363)  loss_scale: 65536.0000 (57257.0361)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [24]  [ 480/2809]  eta: 0:14:36  lr: 0.000020  min_lr: 0.000000  loss: 3.7456 (3.7346)  loss_scale: 65536.0000 (57429.1559)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [24]  [ 490/2809]  eta: 0:14:32  lr: 0.000020  min_lr: 0.000000  loss: 3.7519 (3.7380)  loss_scale: 65536.0000 (57594.2648)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [24]  [ 500/2809]  eta: 0:14:28  lr: 0.000020  min_lr: 0.000000  loss: 3.9644 (3.7424)  loss_scale: 65536.0000 (57752.7824)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [24]  [ 510/2809]  eta: 0:14:24  lr: 0.000020  min_lr: 0.000000  loss: 3.9806 (3.7450)  loss_scale: 65536.0000 (57905.0959)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [24]  [ 520/2809]  eta: 0:14:20  lr: 0.000020  min_lr: 0.000000  loss: 3.9147 (3.7458)  loss_scale: 65536.0000 (58051.5624)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [24]  [ 530/2809]  eta: 0:14:16  lr: 0.000020  min_lr: 0.000000  loss: 3.7266 (3.7426)  loss_scale: 65536.0000 (58192.5122)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [24]  [ 540/2809]  eta: 0:14:12  lr: 0.000020  min_lr: 0.000000  loss: 3.6812 (3.7443)  loss_scale: 65536.0000 (58328.2514)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [24]  [ 550/2809]  eta: 0:14:08  lr: 0.000020  min_lr: 0.000000  loss: 3.8820 (3.7436)  loss_scale: 65536.0000 (58459.0635)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [24]  [ 560/2809]  eta: 0:14:04  lr: 0.000020  min_lr: 0.000000  loss: 3.7086 (3.7428)  loss_scale: 65536.0000 (58585.2121)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0001  max mem: 15572
[2025-01-13 06:23:48,489] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:23:48,489] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:23:49,243] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67979
[2025-01-13 06:23:49,243] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:23:49,243] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [ 570/2809]  eta: 0:14:00  lr: 0.000020  min_lr: 0.000000  loss: 3.8242 (3.7440)  loss_scale: 65536.0000 (58936.4904)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
[2025-01-13 06:23:54,384] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67993
[2025-01-13 06:23:54,384] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:23:54,384] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 580/2809]  eta: 0:13:56  lr: 0.000020  min_lr: 0.000000  loss: 3.7539 (3.7403)  loss_scale: 65536.0000 (58824.4819)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
[2025-01-13 06:23:56,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=68000, skipped=463, lr=[1.926765836973862e-07, 1.926765836973862e-07, 2.7525226242483745e-07, 2.7525226242483745e-07, 3.932175177497678e-07, 3.932175177497678e-07, 5.617393110710969e-07, 5.617393110710969e-07, 8.024847301015671e-07, 8.024847301015671e-07, 1.146406757287953e-06, 1.146406757287953e-06, 1.6377239389827901e-06, 1.6377239389827901e-06, 2.339605627118272e-06, 2.339605627118272e-06, 3.3422937530261024e-06, 3.3422937530261024e-06, 4.774705361465862e-06, 4.774705361465862e-06, 6.821007659236944e-06, 6.821007659236944e-06, 9.74429665605278e-06, 9.74429665605278e-06, 1.3920423794361114e-05, 1.3920423794361114e-05, 1.9886319706230164e-05, 1.9886319706230164e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 06:23:56,587] [INFO] [timer.py:260:stop] epoch=0/micro_step=68000/global_step=68000, RunningAvgSamplesPerSec=30.292152813675347, CurrSamplesPerSec=34.551899262509636, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [ 590/2809]  eta: 0:13:52  lr: 0.000020  min_lr: 0.000000  loss: 3.7495 (3.7418)  loss_scale: 32768.0000 (58383.5939)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [24]  [ 600/2809]  eta: 0:13:48  lr: 0.000020  min_lr: 0.000000  loss: 3.7486 (3.7386)  loss_scale: 32768.0000 (57957.3777)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [24]  [ 610/2809]  eta: 0:13:44  lr: 0.000020  min_lr: 0.000000  loss: 3.6088 (3.7415)  loss_scale: 32768.0000 (57545.1129)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [24]  [ 620/2809]  eta: 0:13:40  lr: 0.000020  min_lr: 0.000000  loss: 3.8698 (3.7445)  loss_scale: 32768.0000 (57146.1256)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [24]  [ 630/2809]  eta: 0:13:36  lr: 0.000020  min_lr: 0.000000  loss: 3.5952 (3.7402)  loss_scale: 32768.0000 (56759.7845)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [24]  [ 640/2809]  eta: 0:13:32  lr: 0.000020  min_lr: 0.000000  loss: 3.5651 (3.7395)  loss_scale: 32768.0000 (56385.4977)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [24]  [ 650/2809]  eta: 0:13:28  lr: 0.000020  min_lr: 0.000000  loss: 3.5772 (3.7390)  loss_scale: 32768.0000 (56022.7097)  weight_decay: 0.0500 (0.0500)  time: 0.3649  data: 0.0002  max mem: 15572
Epoch: [24]  [ 660/2809]  eta: 0:13:24  lr: 0.000020  min_lr: 0.000000  loss: 3.7120 (3.7373)  loss_scale: 32768.0000 (55670.8986)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [24]  [ 670/2809]  eta: 0:13:20  lr: 0.000020  min_lr: 0.000000  loss: 3.7506 (3.7378)  loss_scale: 32768.0000 (55329.5738)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [24]  [ 680/2809]  eta: 0:13:16  lr: 0.000020  min_lr: 0.000000  loss: 3.7732 (3.7336)  loss_scale: 32768.0000 (54998.2731)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [24]  [ 690/2809]  eta: 0:13:12  lr: 0.000020  min_lr: 0.000000  loss: 3.7295 (3.7345)  loss_scale: 32768.0000 (54676.5615)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [24]  [ 700/2809]  eta: 0:13:08  lr: 0.000020  min_lr: 0.000000  loss: 3.7453 (3.7343)  loss_scale: 32768.0000 (54364.0285)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0003  max mem: 15572
[2025-01-13 06:24:41,868] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:24:41,868] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 710/2809]  eta: 0:13:04  lr: 0.000020  min_lr: 0.000000  loss: 3.6412 (3.7312)  loss_scale: 32768.0000 (54290.7229)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
Epoch: [24]  [ 720/2809]  eta: 0:13:01  lr: 0.000020  min_lr: 0.000000  loss: 3.7009 (3.7340)  loss_scale: 65536.0000 (54446.6907)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [24]  [ 730/2809]  eta: 0:12:57  lr: 0.000020  min_lr: 0.000000  loss: 3.7119 (3.7306)  loss_scale: 65536.0000 (54598.3912)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [24]  [ 740/2809]  eta: 0:12:53  lr: 0.000020  min_lr: 0.000000  loss: 3.4700 (3.7301)  loss_scale: 65536.0000 (54745.9973)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0001  max mem: 15572
Epoch: [24]  [ 750/2809]  eta: 0:12:49  lr: 0.000020  min_lr: 0.000000  loss: 3.6805 (3.7290)  loss_scale: 65536.0000 (54889.6724)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [24]  [ 760/2809]  eta: 0:12:45  lr: 0.000020  min_lr: 0.000000  loss: 3.7764 (3.7317)  loss_scale: 65536.0000 (55029.5716)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [24]  [ 770/2809]  eta: 0:12:42  lr: 0.000020  min_lr: 0.000000  loss: 4.0077 (3.7354)  loss_scale: 65536.0000 (55165.8418)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [24]  [ 780/2809]  eta: 0:12:38  lr: 0.000020  min_lr: 0.000000  loss: 4.0271 (3.7387)  loss_scale: 65536.0000 (55298.6223)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
Epoch: [24]  [ 790/2809]  eta: 0:12:34  lr: 0.000020  min_lr: 0.000000  loss: 3.8391 (3.7385)  loss_scale: 65536.0000 (55428.0455)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [24]  [ 800/2809]  eta: 0:12:30  lr: 0.000020  min_lr: 0.000000  loss: 3.7349 (3.7378)  loss_scale: 65536.0000 (55554.2372)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [24]  [ 810/2809]  eta: 0:12:26  lr: 0.000020  min_lr: 0.000000  loss: 3.6267 (3.7394)  loss_scale: 65536.0000 (55677.3169)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [24]  [ 820/2809]  eta: 0:12:23  lr: 0.000020  min_lr: 0.000000  loss: 3.8810 (3.7428)  loss_scale: 65536.0000 (55797.3983)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [24]  [ 830/2809]  eta: 0:12:19  lr: 0.000020  min_lr: 0.000000  loss: 3.8918 (3.7447)  loss_scale: 65536.0000 (55914.5897)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
[2025-01-13 06:25:29,369] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:25:29,369] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:25:29,747] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68251
[2025-01-13 06:25:29,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:25:29,747] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [ 840/2809]  eta: 0:12:15  lr: 0.000020  min_lr: 0.000000  loss: 3.8859 (3.7473)  loss_scale: 65536.0000 (56106.9203)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [24]  [ 850/2809]  eta: 0:12:11  lr: 0.000020  min_lr: 0.000000  loss: 3.9460 (3.7487)  loss_scale: 65536.0000 (56217.7203)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [24]  [ 860/2809]  eta: 0:12:07  lr: 0.000020  min_lr: 0.000000  loss: 3.9460 (3.7503)  loss_scale: 65536.0000 (56325.9466)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [24]  [ 870/2809]  eta: 0:12:04  lr: 0.000020  min_lr: 0.000000  loss: 3.7050 (3.7482)  loss_scale: 65536.0000 (56431.6877)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [24]  [ 880/2809]  eta: 0:12:00  lr: 0.000020  min_lr: 0.000000  loss: 3.6000 (3.7479)  loss_scale: 65536.0000 (56535.0284)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [24]  [ 890/2809]  eta: 0:11:56  lr: 0.000020  min_lr: 0.000000  loss: 3.8320 (3.7486)  loss_scale: 65536.0000 (56636.0494)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [24]  [ 900/2809]  eta: 0:11:52  lr: 0.000020  min_lr: 0.000000  loss: 3.9628 (3.7498)  loss_scale: 65536.0000 (56734.8280)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [24]  [ 910/2809]  eta: 0:11:48  lr: 0.000020  min_lr: 0.000000  loss: 3.7877 (3.7498)  loss_scale: 65536.0000 (56831.4380)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [24]  [ 920/2809]  eta: 0:11:45  lr: 0.000020  min_lr: 0.000000  loss: 3.6937 (3.7508)  loss_scale: 65536.0000 (56925.9501)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [24]  [ 930/2809]  eta: 0:11:41  lr: 0.000020  min_lr: 0.000000  loss: 3.9279 (3.7540)  loss_scale: 65536.0000 (57018.4318)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [24]  [ 940/2809]  eta: 0:11:37  lr: 0.000020  min_lr: 0.000000  loss: 3.9279 (3.7555)  loss_scale: 65536.0000 (57108.9479)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [24]  [ 950/2809]  eta: 0:11:33  lr: 0.000020  min_lr: 0.000000  loss: 3.9642 (3.7576)  loss_scale: 65536.0000 (57197.5605)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
[2025-01-13 06:26:15,226] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68374
[2025-01-13 06:26:15,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:26:15,228] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 960/2809]  eta: 0:11:29  lr: 0.000020  min_lr: 0.000000  loss: 4.0717 (3.7602)  loss_scale: 65536.0000 (57182.0354)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [24]  [ 970/2809]  eta: 0:11:25  lr: 0.000020  min_lr: 0.000000  loss: 3.8549 (3.7606)  loss_scale: 32768.0000 (56930.6035)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [24]  [ 980/2809]  eta: 0:11:22  lr: 0.000020  min_lr: 0.000000  loss: 3.7767 (3.7610)  loss_scale: 32768.0000 (56684.2977)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [24]  [ 990/2809]  eta: 0:11:18  lr: 0.000020  min_lr: 0.000000  loss: 3.8970 (3.7630)  loss_scale: 32768.0000 (56442.9627)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [24]  [1000/2809]  eta: 0:11:14  lr: 0.000020  min_lr: 0.000000  loss: 3.9205 (3.7638)  loss_scale: 32768.0000 (56206.4496)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [24]  [1010/2809]  eta: 0:11:10  lr: 0.000020  min_lr: 0.000000  loss: 3.7201 (3.7640)  loss_scale: 32768.0000 (55974.6152)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [24]  [1020/2809]  eta: 0:11:06  lr: 0.000020  min_lr: 0.000000  loss: 3.9806 (3.7674)  loss_scale: 32768.0000 (55747.3222)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [24]  [1030/2809]  eta: 0:11:03  lr: 0.000020  min_lr: 0.000000  loss: 3.9895 (3.7679)  loss_scale: 32768.0000 (55524.4384)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0001  max mem: 15572
Epoch: [24]  [1040/2809]  eta: 0:10:59  lr: 0.000020  min_lr: 0.000000  loss: 3.8181 (3.7668)  loss_scale: 32768.0000 (55305.8367)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [24]  [1050/2809]  eta: 0:10:55  lr: 0.000020  min_lr: 0.000000  loss: 3.8181 (3.7694)  loss_scale: 32768.0000 (55091.3949)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [24]  [1060/2809]  eta: 0:10:51  lr: 0.000020  min_lr: 0.000000  loss: 3.9183 (3.7696)  loss_scale: 32768.0000 (54880.9953)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [24]  [1070/2809]  eta: 0:10:48  lr: 0.000020  min_lr: 0.000000  loss: 3.6349 (3.7675)  loss_scale: 32768.0000 (54674.5247)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [24]  [1080/2809]  eta: 0:10:44  lr: 0.000020  min_lr: 0.000000  loss: 3.5325 (3.7660)  loss_scale: 32768.0000 (54471.8742)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
[2025-01-13 06:27:02,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:27:02,980] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [1090/2809]  eta: 0:10:40  lr: 0.000020  min_lr: 0.000000  loss: 3.5552 (3.7633)  loss_scale: 32768.0000 (54393.0779)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [24]  [1100/2809]  eta: 0:10:36  lr: 0.000020  min_lr: 0.000000  loss: 3.5552 (3.7614)  loss_scale: 65536.0000 (54494.2852)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [24]  [1110/2809]  eta: 0:10:33  lr: 0.000019  min_lr: 0.000000  loss: 3.6274 (3.7628)  loss_scale: 65536.0000 (54593.6706)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [24]  [1120/2809]  eta: 0:10:29  lr: 0.000019  min_lr: 0.000000  loss: 3.7020 (3.7620)  loss_scale: 65536.0000 (54691.2828)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [24]  [1130/2809]  eta: 0:10:25  lr: 0.000019  min_lr: 0.000000  loss: 3.6949 (3.7615)  loss_scale: 65536.0000 (54787.1689)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0001  max mem: 15572
Epoch: [24]  [1140/2809]  eta: 0:10:21  lr: 0.000019  min_lr: 0.000000  loss: 3.7631 (3.7623)  loss_scale: 65536.0000 (54881.3742)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [24]  [1150/2809]  eta: 0:10:18  lr: 0.000019  min_lr: 0.000000  loss: 4.0531 (3.7649)  loss_scale: 65536.0000 (54973.9427)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [24]  [1160/2809]  eta: 0:10:14  lr: 0.000019  min_lr: 0.000000  loss: 3.8476 (3.7629)  loss_scale: 65536.0000 (55064.9165)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [24]  [1170/2809]  eta: 0:10:10  lr: 0.000019  min_lr: 0.000000  loss: 3.4092 (3.7616)  loss_scale: 65536.0000 (55154.3365)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [24]  [1180/2809]  eta: 0:10:06  lr: 0.000019  min_lr: 0.000000  loss: 3.8461 (3.7632)  loss_scale: 65536.0000 (55242.2422)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [24]  [1190/2809]  eta: 0:10:03  lr: 0.000019  min_lr: 0.000000  loss: 3.8629 (3.7617)  loss_scale: 65536.0000 (55328.6717)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0001  max mem: 15572
Epoch: [24]  [1200/2809]  eta: 0:09:59  lr: 0.000019  min_lr: 0.000000  loss: 3.7159 (3.7617)  loss_scale: 65536.0000 (55413.6619)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [24]  [1210/2809]  eta: 0:09:55  lr: 0.000019  min_lr: 0.000000  loss: 3.8317 (3.7626)  loss_scale: 65536.0000 (55497.2486)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
[2025-01-13 06:27:50,402] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:27:50,402] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:27:50,769] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68632
[2025-01-13 06:27:50,769] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:27:50,769] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1220/2809]  eta: 0:09:51  lr: 0.000019  min_lr: 0.000000  loss: 3.7437 (3.7603)  loss_scale: 65536.0000 (55633.1400)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [24]  [1230/2809]  eta: 0:09:48  lr: 0.000019  min_lr: 0.000000  loss: 3.5936 (3.7626)  loss_scale: 65536.0000 (55713.5857)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [24]  [1240/2809]  eta: 0:09:44  lr: 0.000019  min_lr: 0.000000  loss: 3.9706 (3.7637)  loss_scale: 65536.0000 (55792.7349)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [24]  [1250/2809]  eta: 0:09:40  lr: 0.000019  min_lr: 0.000000  loss: 3.9804 (3.7659)  loss_scale: 65536.0000 (55870.6187)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [24]  [1260/2809]  eta: 0:09:36  lr: 0.000019  min_lr: 0.000000  loss: 3.9048 (3.7650)  loss_scale: 65536.0000 (55947.2672)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [24]  [1270/2809]  eta: 0:09:33  lr: 0.000019  min_lr: 0.000000  loss: 3.6754 (3.7622)  loss_scale: 65536.0000 (56022.7097)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [24]  [1280/2809]  eta: 0:09:29  lr: 0.000019  min_lr: 0.000000  loss: 3.7124 (3.7622)  loss_scale: 65536.0000 (56096.9742)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [24]  [1290/2809]  eta: 0:09:25  lr: 0.000019  min_lr: 0.000000  loss: 3.7874 (3.7619)  loss_scale: 65536.0000 (56170.0883)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [24]  [1300/2809]  eta: 0:09:21  lr: 0.000019  min_lr: 0.000000  loss: 3.7874 (3.7623)  loss_scale: 65536.0000 (56242.0784)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [24]  [1310/2809]  eta: 0:09:18  lr: 0.000019  min_lr: 0.000000  loss: 3.8451 (3.7637)  loss_scale: 65536.0000 (56312.9703)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [24]  [1320/2809]  eta: 0:09:14  lr: 0.000019  min_lr: 0.000000  loss: 3.8312 (3.7622)  loss_scale: 65536.0000 (56382.7888)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [24]  [1330/2809]  eta: 0:09:10  lr: 0.000019  min_lr: 0.000000  loss: 3.6384 (3.7615)  loss_scale: 65536.0000 (56451.5582)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [24]  [1340/2809]  eta: 0:09:06  lr: 0.000019  min_lr: 0.000000  loss: 3.7191 (3.7620)  loss_scale: 65536.0000 (56519.3020)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
[2025-01-13 06:28:38,727] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:28:38,727] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:28:39,814] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68764
[2025-01-13 06:28:39,814] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:28:39,814] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1350/2809]  eta: 0:09:03  lr: 0.000019  min_lr: 0.000000  loss: 3.7191 (3.7610)  loss_scale: 65536.0000 (56731.5707)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [24]  [1360/2809]  eta: 0:08:59  lr: 0.000019  min_lr: 0.000000  loss: 3.6608 (3.7597)  loss_scale: 65536.0000 (56796.2616)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [24]  [1370/2809]  eta: 0:08:55  lr: 0.000019  min_lr: 0.000000  loss: 3.7376 (3.7609)  loss_scale: 65536.0000 (56860.0088)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [24]  [1380/2809]  eta: 0:08:51  lr: 0.000019  min_lr: 0.000000  loss: 3.9308 (3.7607)  loss_scale: 65536.0000 (56922.8327)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [24]  [1390/2809]  eta: 0:08:48  lr: 0.000019  min_lr: 0.000000  loss: 3.8071 (3.7609)  loss_scale: 65536.0000 (56984.7534)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [24]  [1400/2809]  eta: 0:08:44  lr: 0.000019  min_lr: 0.000000  loss: 3.8071 (3.7604)  loss_scale: 65536.0000 (57045.7901)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [24]  [1410/2809]  eta: 0:08:40  lr: 0.000019  min_lr: 0.000000  loss: 3.9236 (3.7606)  loss_scale: 65536.0000 (57105.9617)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [24]  [1420/2809]  eta: 0:08:36  lr: 0.000019  min_lr: 0.000000  loss: 3.8665 (3.7616)  loss_scale: 65536.0000 (57165.2864)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [24]  [1430/2809]  eta: 0:08:33  lr: 0.000019  min_lr: 0.000000  loss: 3.8696 (3.7617)  loss_scale: 65536.0000 (57223.7820)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [24]  [1440/2809]  eta: 0:08:29  lr: 0.000019  min_lr: 0.000000  loss: 3.7371 (3.7609)  loss_scale: 65536.0000 (57281.4656)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0001  max mem: 15572
Epoch: [24]  [1450/2809]  eta: 0:08:25  lr: 0.000019  min_lr: 0.000000  loss: 3.5848 (3.7598)  loss_scale: 65536.0000 (57338.3542)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [24]  [1460/2809]  eta: 0:08:21  lr: 0.000019  min_lr: 0.000000  loss: 3.5143 (3.7595)  loss_scale: 65536.0000 (57394.4641)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [24]  [1470/2809]  eta: 0:08:18  lr: 0.000019  min_lr: 0.000000  loss: 3.9004 (3.7605)  loss_scale: 65536.0000 (57449.8110)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
[2025-01-13 06:29:27,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:29:27,405] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:29:27,765] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68894
[2025-01-13 06:29:27,765] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:29:27,765] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1480/2809]  eta: 0:08:14  lr: 0.000019  min_lr: 0.000000  loss: 3.9023 (3.7610)  loss_scale: 65536.0000 (57548.6617)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [24]  [1490/2809]  eta: 0:08:10  lr: 0.000019  min_lr: 0.000000  loss: 3.8210 (3.7600)  loss_scale: 65536.0000 (57602.2321)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [24]  [1500/2809]  eta: 0:08:06  lr: 0.000019  min_lr: 0.000000  loss: 3.7931 (3.7605)  loss_scale: 65536.0000 (57655.0886)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [24]  [1510/2809]  eta: 0:08:03  lr: 0.000019  min_lr: 0.000000  loss: 4.0446 (3.7616)  loss_scale: 65536.0000 (57707.2455)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0001  max mem: 15572
Epoch: [24]  [1520/2809]  eta: 0:07:59  lr: 0.000019  min_lr: 0.000000  loss: 3.6914 (3.7608)  loss_scale: 65536.0000 (57758.7166)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0001  max mem: 15572
Epoch: [24]  [1530/2809]  eta: 0:07:55  lr: 0.000019  min_lr: 0.000000  loss: 3.6733 (3.7610)  loss_scale: 65536.0000 (57809.5153)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [24]  [1540/2809]  eta: 0:07:52  lr: 0.000019  min_lr: 0.000000  loss: 3.7568 (3.7599)  loss_scale: 65536.0000 (57859.6548)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [24]  [1550/2809]  eta: 0:07:48  lr: 0.000019  min_lr: 0.000000  loss: 3.9164 (3.7625)  loss_scale: 65536.0000 (57909.1476)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [24]  [1560/2809]  eta: 0:07:44  lr: 0.000019  min_lr: 0.000000  loss: 4.0350 (3.7631)  loss_scale: 65536.0000 (57958.0064)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [24]  [1570/2809]  eta: 0:07:40  lr: 0.000019  min_lr: 0.000000  loss: 3.7068 (3.7618)  loss_scale: 65536.0000 (58006.2432)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [24]  [1580/2809]  eta: 0:07:37  lr: 0.000019  min_lr: 0.000000  loss: 3.7096 (3.7624)  loss_scale: 65536.0000 (58053.8697)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
[2025-01-13 06:30:06,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=69000, skipped=468, lr=[1.8553140734347188e-07, 1.8553140734347188e-07, 2.650448676335313e-07, 2.650448676335313e-07, 3.7863552519075905e-07, 3.7863552519075905e-07, 5.409078931296558e-07, 5.409078931296558e-07, 7.72725561613794e-07, 7.72725561613794e-07, 1.1038936594482773e-06, 1.1038936594482773e-06, 1.5769909420689674e-06, 1.5769909420689674e-06, 2.252844202955668e-06, 2.252844202955668e-06, 3.21834886136524e-06, 3.21834886136524e-06, 4.597641230521772e-06, 4.597641230521772e-06, 6.568058900745388e-06, 6.568058900745388e-06, 9.382941286779127e-06, 9.382941286779127e-06, 1.3404201838255898e-05, 1.3404201838255898e-05, 1.9148859768936997e-05, 1.9148859768936997e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 06:30:06,749] [INFO] [timer.py:260:stop] epoch=0/micro_step=69000/global_step=69000, RunningAvgSamplesPerSec=30.34342494139288, CurrSamplesPerSec=32.82326103713674, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [1590/2809]  eta: 0:07:33  lr: 0.000019  min_lr: 0.000000  loss: 3.9857 (3.7652)  loss_scale: 65536.0000 (58100.8975)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [24]  [1600/2809]  eta: 0:07:29  lr: 0.000019  min_lr: 0.000000  loss: 3.8424 (3.7645)  loss_scale: 65536.0000 (58147.3379)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
[2025-01-13 06:30:15,578] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:30:15,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:30:15,939] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69024
[2025-01-13 06:30:15,939] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:30:15,939] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1610/2809]  eta: 0:07:25  lr: 0.000019  min_lr: 0.000000  loss: 3.7323 (3.7655)  loss_scale: 65536.0000 (58233.8821)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [24]  [1620/2809]  eta: 0:07:22  lr: 0.000019  min_lr: 0.000000  loss: 3.7957 (3.7652)  loss_scale: 65536.0000 (58278.9291)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [24]  [1630/2809]  eta: 0:07:18  lr: 0.000019  min_lr: 0.000000  loss: 3.5913 (3.7633)  loss_scale: 65536.0000 (58323.4237)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [24]  [1640/2809]  eta: 0:07:14  lr: 0.000019  min_lr: 0.000000  loss: 3.7062 (3.7642)  loss_scale: 65536.0000 (58367.3760)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [24]  [1650/2809]  eta: 0:07:10  lr: 0.000019  min_lr: 0.000000  loss: 3.8489 (3.7651)  loss_scale: 65536.0000 (58410.7959)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [24]  [1660/2809]  eta: 0:07:07  lr: 0.000019  min_lr: 0.000000  loss: 3.7850 (3.7639)  loss_scale: 65536.0000 (58453.6930)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [24]  [1670/2809]  eta: 0:07:03  lr: 0.000019  min_lr: 0.000000  loss: 3.4804 (3.7621)  loss_scale: 65536.0000 (58496.0766)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [24]  [1680/2809]  eta: 0:06:59  lr: 0.000019  min_lr: 0.000000  loss: 3.3277 (3.7604)  loss_scale: 65536.0000 (58537.9560)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [24]  [1690/2809]  eta: 0:06:55  lr: 0.000019  min_lr: 0.000000  loss: 3.5992 (3.7610)  loss_scale: 65536.0000 (58579.3400)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [24]  [1700/2809]  eta: 0:06:52  lr: 0.000019  min_lr: 0.000000  loss: 3.7675 (3.7613)  loss_scale: 65536.0000 (58620.2375)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [24]  [1710/2809]  eta: 0:06:48  lr: 0.000019  min_lr: 0.000000  loss: 3.7093 (3.7601)  loss_scale: 65536.0000 (58660.6569)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [24]  [1720/2809]  eta: 0:06:44  lr: 0.000019  min_lr: 0.000000  loss: 3.3601 (3.7586)  loss_scale: 65536.0000 (58700.6066)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [24]  [1730/2809]  eta: 0:06:41  lr: 0.000019  min_lr: 0.000000  loss: 3.6745 (3.7581)  loss_scale: 65536.0000 (58740.0947)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
[2025-01-13 06:31:03,608] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:31:03,608] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [24]  [1740/2809]  eta: 0:06:37  lr: 0.000019  min_lr: 0.000000  loss: 3.6910 (3.7586)  loss_scale: 65536.0000 (58929.7002)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
[2025-01-13 06:31:05,445] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69158
[2025-01-13 06:31:05,446] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:31:05,446] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1750/2809]  eta: 0:06:33  lr: 0.000019  min_lr: 0.000000  loss: 3.7905 (3.7584)  loss_scale: 65536.0000 (59004.8567)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [24]  [1760/2809]  eta: 0:06:29  lr: 0.000019  min_lr: 0.000000  loss: 3.8601 (3.7587)  loss_scale: 65536.0000 (59041.9443)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [24]  [1770/2809]  eta: 0:06:26  lr: 0.000019  min_lr: 0.000000  loss: 3.7266 (3.7581)  loss_scale: 65536.0000 (59078.6132)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [24]  [1780/2809]  eta: 0:06:22  lr: 0.000019  min_lr: 0.000000  loss: 3.7266 (3.7580)  loss_scale: 65536.0000 (59114.8703)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [24]  [1790/2809]  eta: 0:06:18  lr: 0.000019  min_lr: 0.000000  loss: 3.6955 (3.7577)  loss_scale: 65536.0000 (59150.7225)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [24]  [1800/2809]  eta: 0:06:14  lr: 0.000019  min_lr: 0.000000  loss: 3.7583 (3.7579)  loss_scale: 65536.0000 (59186.1766)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0003  max mem: 15572
Epoch: [24]  [1810/2809]  eta: 0:06:11  lr: 0.000019  min_lr: 0.000000  loss: 3.8833 (3.7582)  loss_scale: 65536.0000 (59221.2391)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [24]  [1820/2809]  eta: 0:06:07  lr: 0.000019  min_lr: 0.000000  loss: 3.8274 (3.7574)  loss_scale: 65536.0000 (59255.9165)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [24]  [1830/2809]  eta: 0:06:03  lr: 0.000019  min_lr: 0.000000  loss: 3.6964 (3.7577)  loss_scale: 65536.0000 (59290.2152)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [24]  [1840/2809]  eta: 0:06:00  lr: 0.000019  min_lr: 0.000000  loss: 3.6964 (3.7569)  loss_scale: 65536.0000 (59324.1412)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [24]  [1850/2809]  eta: 0:05:56  lr: 0.000019  min_lr: 0.000000  loss: 3.6403 (3.7561)  loss_scale: 65536.0000 (59357.7007)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [24]  [1860/2809]  eta: 0:05:52  lr: 0.000019  min_lr: 0.000000  loss: 3.7214 (3.7569)  loss_scale: 65536.0000 (59390.8995)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [24]  [1870/2809]  eta: 0:05:48  lr: 0.000019  min_lr: 0.000000  loss: 3.7597 (3.7568)  loss_scale: 65536.0000 (59423.7435)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
[2025-01-13 06:31:53,228] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:31:53,228] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:31:54,703] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69291
[2025-01-13 06:31:54,704] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:31:54,704] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1880/2809]  eta: 0:05:45  lr: 0.000019  min_lr: 0.000000  loss: 3.7597 (3.7571)  loss_scale: 65536.0000 (59595.6023)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [24]  [1890/2809]  eta: 0:05:41  lr: 0.000019  min_lr: 0.000000  loss: 3.7795 (3.7562)  loss_scale: 65536.0000 (59627.0164)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [24]  [1900/2809]  eta: 0:05:37  lr: 0.000019  min_lr: 0.000000  loss: 3.6835 (3.7562)  loss_scale: 65536.0000 (59658.0999)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [24]  [1910/2809]  eta: 0:05:34  lr: 0.000019  min_lr: 0.000000  loss: 3.8162 (3.7568)  loss_scale: 65536.0000 (59688.8582)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [24]  [1920/2809]  eta: 0:05:30  lr: 0.000019  min_lr: 0.000000  loss: 3.7795 (3.7568)  loss_scale: 65536.0000 (59719.2962)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0003  max mem: 15572
Epoch: [24]  [1930/2809]  eta: 0:05:26  lr: 0.000019  min_lr: 0.000000  loss: 3.7033 (3.7558)  loss_scale: 65536.0000 (59749.4190)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [24]  [1940/2809]  eta: 0:05:22  lr: 0.000019  min_lr: 0.000000  loss: 3.6891 (3.7571)  loss_scale: 65536.0000 (59779.2313)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [24]  [1950/2809]  eta: 0:05:19  lr: 0.000019  min_lr: 0.000000  loss: 4.0764 (3.7573)  loss_scale: 65536.0000 (59808.7381)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [24]  [1960/2809]  eta: 0:05:15  lr: 0.000019  min_lr: 0.000000  loss: 3.8397 (3.7580)  loss_scale: 65536.0000 (59837.9439)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0002  max mem: 15572
Epoch: [24]  [1970/2809]  eta: 0:05:11  lr: 0.000019  min_lr: 0.000000  loss: 3.8397 (3.7582)  loss_scale: 65536.0000 (59866.8534)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [24]  [1980/2809]  eta: 0:05:08  lr: 0.000019  min_lr: 0.000000  loss: 3.7591 (3.7577)  loss_scale: 65536.0000 (59895.4710)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [24]  [1990/2809]  eta: 0:05:04  lr: 0.000019  min_lr: 0.000000  loss: 3.5859 (3.7572)  loss_scale: 65536.0000 (59923.8011)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [24]  [2000/2809]  eta: 0:05:00  lr: 0.000019  min_lr: 0.000000  loss: 3.5859 (3.7570)  loss_scale: 65536.0000 (59951.8481)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
[2025-01-13 06:32:42,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:32:42,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:32:44,829] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69426
[2025-01-13 06:32:44,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:32:44,831] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [2010/2809]  eta: 0:04:56  lr: 0.000019  min_lr: 0.000000  loss: 3.8976 (3.7577)  loss_scale: 65536.0000 (60175.1487)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [24]  [2020/2809]  eta: 0:04:53  lr: 0.000019  min_lr: 0.000000  loss: 3.9627 (3.7586)  loss_scale: 65536.0000 (60201.6744)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0002  max mem: 15572
Epoch: [24]  [2030/2809]  eta: 0:04:49  lr: 0.000019  min_lr: 0.000000  loss: 3.9586 (3.7587)  loss_scale: 65536.0000 (60227.9389)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0002  max mem: 15572
Epoch: [24]  [2040/2809]  eta: 0:04:45  lr: 0.000019  min_lr: 0.000000  loss: 3.7527 (3.7591)  loss_scale: 65536.0000 (60253.9461)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [24]  [2050/2809]  eta: 0:04:42  lr: 0.000019  min_lr: 0.000000  loss: 3.9002 (3.7595)  loss_scale: 65536.0000 (60279.6997)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
[2025-01-13 06:33:01,963] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69472
[2025-01-13 06:33:01,964] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:33:01,964] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [2060/2809]  eta: 0:04:38  lr: 0.000019  min_lr: 0.000000  loss: 3.8249 (3.7594)  loss_scale: 65536.0000 (60225.7079)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [24]  [2070/2809]  eta: 0:04:34  lr: 0.000019  min_lr: 0.000000  loss: 3.8249 (3.7603)  loss_scale: 32768.0000 (60093.1260)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [24]  [2080/2809]  eta: 0:04:30  lr: 0.000019  min_lr: 0.000000  loss: 3.7799 (3.7598)  loss_scale: 32768.0000 (59961.8184)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [24]  [2090/2809]  eta: 0:04:27  lr: 0.000019  min_lr: 0.000000  loss: 3.4174 (3.7596)  loss_scale: 32768.0000 (59831.7666)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [24]  [2100/2809]  eta: 0:04:23  lr: 0.000019  min_lr: 0.000000  loss: 3.7843 (3.7596)  loss_scale: 32768.0000 (59702.9529)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0001  max mem: 15572
Epoch: [24]  [2110/2809]  eta: 0:04:19  lr: 0.000019  min_lr: 0.000000  loss: 3.7089 (3.7593)  loss_scale: 32768.0000 (59575.3595)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [24]  [2120/2809]  eta: 0:04:15  lr: 0.000019  min_lr: 0.000000  loss: 3.8068 (3.7601)  loss_scale: 32768.0000 (59448.9694)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0003  max mem: 15572
Epoch: [24]  [2130/2809]  eta: 0:04:12  lr: 0.000019  min_lr: 0.000000  loss: 3.8234 (3.7605)  loss_scale: 32768.0000 (59323.7654)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [24]  [2140/2809]  eta: 0:04:08  lr: 0.000019  min_lr: 0.000000  loss: 3.7312 (3.7604)  loss_scale: 32768.0000 (59199.7310)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [24]  [2150/2809]  eta: 0:04:04  lr: 0.000019  min_lr: 0.000000  loss: 3.6173 (3.7595)  loss_scale: 32768.0000 (59076.8498)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [24]  [2160/2809]  eta: 0:04:01  lr: 0.000019  min_lr: 0.000000  loss: 3.5000 (3.7581)  loss_scale: 32768.0000 (58955.1060)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [24]  [2170/2809]  eta: 0:03:57  lr: 0.000019  min_lr: 0.000000  loss: 3.6577 (3.7579)  loss_scale: 32768.0000 (58834.4836)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0018  max mem: 15572
Epoch: [24]  [2180/2809]  eta: 0:03:53  lr: 0.000019  min_lr: 0.000000  loss: 3.8767 (3.7575)  loss_scale: 32768.0000 (58714.9674)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0018  max mem: 15572
[2025-01-13 06:33:49,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:33:49,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [2190/2809]  eta: 0:03:49  lr: 0.000019  min_lr: 0.000000  loss: 3.8269 (3.7570)  loss_scale: 32768.0000 (58686.2766)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [24]  [2200/2809]  eta: 0:03:46  lr: 0.000019  min_lr: 0.000000  loss: 3.8269 (3.7575)  loss_scale: 65536.0000 (58717.3975)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [24]  [2210/2809]  eta: 0:03:42  lr: 0.000019  min_lr: 0.000000  loss: 3.6488 (3.7560)  loss_scale: 65536.0000 (58748.2370)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [24]  [2220/2809]  eta: 0:03:38  lr: 0.000019  min_lr: 0.000000  loss: 3.4383 (3.7547)  loss_scale: 65536.0000 (58778.7987)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [24]  [2230/2809]  eta: 0:03:35  lr: 0.000019  min_lr: 0.000000  loss: 3.4593 (3.7540)  loss_scale: 65536.0000 (58809.0865)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [24]  [2240/2809]  eta: 0:03:31  lr: 0.000019  min_lr: 0.000000  loss: 3.6672 (3.7537)  loss_scale: 65536.0000 (58839.1040)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [24]  [2250/2809]  eta: 0:03:27  lr: 0.000019  min_lr: 0.000000  loss: 3.7360 (3.7541)  loss_scale: 65536.0000 (58868.8547)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [24]  [2260/2809]  eta: 0:03:23  lr: 0.000019  min_lr: 0.000000  loss: 3.7804 (3.7535)  loss_scale: 65536.0000 (58898.3423)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [24]  [2270/2809]  eta: 0:03:20  lr: 0.000019  min_lr: 0.000000  loss: 3.8465 (3.7545)  loss_scale: 65536.0000 (58927.5702)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
[2025-01-13 06:34:22,152] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69688
[2025-01-13 06:34:22,152] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:34:22,152] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [2280/2809]  eta: 0:03:16  lr: 0.000019  min_lr: 0.000000  loss: 3.8559 (3.7548)  loss_scale: 65536.0000 (58827.2512)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [24]  [2290/2809]  eta: 0:03:12  lr: 0.000019  min_lr: 0.000000  loss: 3.8189 (3.7553)  loss_scale: 32768.0000 (58713.5050)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
Epoch: [24]  [2300/2809]  eta: 0:03:09  lr: 0.000019  min_lr: 0.000000  loss: 3.8281 (3.7556)  loss_scale: 32768.0000 (58600.7475)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [24]  [2310/2809]  eta: 0:03:05  lr: 0.000019  min_lr: 0.000000  loss: 3.8128 (3.7556)  loss_scale: 32768.0000 (58488.9658)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [24]  [2320/2809]  eta: 0:03:01  lr: 0.000019  min_lr: 0.000000  loss: 3.8905 (3.7562)  loss_scale: 32768.0000 (58378.1474)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [24]  [2330/2809]  eta: 0:02:57  lr: 0.000019  min_lr: 0.000000  loss: 3.8905 (3.7566)  loss_scale: 32768.0000 (58268.2797)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [24]  [2340/2809]  eta: 0:02:54  lr: 0.000019  min_lr: 0.000000  loss: 3.7845 (3.7565)  loss_scale: 32768.0000 (58159.3507)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [24]  [2350/2809]  eta: 0:02:50  lr: 0.000019  min_lr: 0.000000  loss: 3.8132 (3.7574)  loss_scale: 32768.0000 (58051.3484)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [24]  [2360/2809]  eta: 0:02:46  lr: 0.000019  min_lr: 0.000000  loss: 3.8699 (3.7578)  loss_scale: 32768.0000 (57944.2609)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [24]  [2370/2809]  eta: 0:02:43  lr: 0.000019  min_lr: 0.000000  loss: 3.6318 (3.7565)  loss_scale: 32768.0000 (57838.0768)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [24]  [2380/2809]  eta: 0:02:39  lr: 0.000019  min_lr: 0.000000  loss: 3.6318 (3.7563)  loss_scale: 32768.0000 (57732.7845)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [24]  [2390/2809]  eta: 0:02:35  lr: 0.000019  min_lr: 0.000000  loss: 3.8176 (3.7568)  loss_scale: 32768.0000 (57628.3731)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [24]  [2400/2809]  eta: 0:02:31  lr: 0.000019  min_lr: 0.000000  loss: 3.8157 (3.7560)  loss_scale: 32768.0000 (57524.8313)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
[2025-01-13 06:35:10,075] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:35:10,075] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [2410/2809]  eta: 0:02:28  lr: 0.000019  min_lr: 0.000000  loss: 3.7507 (3.7564)  loss_scale: 32768.0000 (57558.0589)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [24]  [2420/2809]  eta: 0:02:24  lr: 0.000019  min_lr: 0.000000  loss: 3.8796 (3.7570)  loss_scale: 65536.0000 (57591.0120)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [24]  [2430/2809]  eta: 0:02:20  lr: 0.000019  min_lr: 0.000000  loss: 3.9020 (3.7578)  loss_scale: 65536.0000 (57623.6940)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [24]  [2440/2809]  eta: 0:02:17  lr: 0.000019  min_lr: 0.000000  loss: 3.9131 (3.7570)  loss_scale: 65536.0000 (57656.1082)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [24]  [2450/2809]  eta: 0:02:13  lr: 0.000019  min_lr: 0.000000  loss: 3.9497 (3.7581)  loss_scale: 65536.0000 (57688.2579)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [24]  [2460/2809]  eta: 0:02:09  lr: 0.000019  min_lr: 0.000000  loss: 3.8200 (3.7563)  loss_scale: 65536.0000 (57720.1463)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [24]  [2470/2809]  eta: 0:02:05  lr: 0.000018  min_lr: 0.000000  loss: 3.3597 (3.7558)  loss_scale: 65536.0000 (57751.7766)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0001  max mem: 15572
Epoch: [24]  [2480/2809]  eta: 0:02:02  lr: 0.000018  min_lr: 0.000000  loss: 3.6905 (3.7561)  loss_scale: 65536.0000 (57783.1520)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [24]  [2490/2809]  eta: 0:01:58  lr: 0.000018  min_lr: 0.000000  loss: 3.7153 (3.7559)  loss_scale: 65536.0000 (57814.2754)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [24]  [2500/2809]  eta: 0:01:54  lr: 0.000018  min_lr: 0.000000  loss: 3.7270 (3.7554)  loss_scale: 65536.0000 (57845.1499)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [24]  [2510/2809]  eta: 0:01:51  lr: 0.000018  min_lr: 0.000000  loss: 3.8645 (3.7559)  loss_scale: 65536.0000 (57875.7786)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [24]  [2520/2809]  eta: 0:01:47  lr: 0.000018  min_lr: 0.000000  loss: 3.8969 (3.7568)  loss_scale: 65536.0000 (57906.1642)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
[2025-01-13 06:35:57,481] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:35:57,481] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:35:57,857] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69946
[2025-01-13 06:35:57,857] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:35:57,858] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [2530/2809]  eta: 0:01:43  lr: 0.000018  min_lr: 0.000000  loss: 3.8875 (3.7571)  loss_scale: 65536.0000 (57962.2031)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [24]  [2540/2809]  eta: 0:01:39  lr: 0.000018  min_lr: 0.000000  loss: 3.7854 (3.7571)  loss_scale: 65536.0000 (57992.0094)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0001  max mem: 15572
Epoch: [24]  [2550/2809]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 3.6642 (3.7567)  loss_scale: 65536.0000 (58021.5821)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [24]  [2560/2809]  eta: 0:01:32  lr: 0.000018  min_lr: 0.000000  loss: 3.6975 (3.7562)  loss_scale: 65536.0000 (58050.9239)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [24]  [2570/2809]  eta: 0:01:28  lr: 0.000018  min_lr: 0.000000  loss: 3.6975 (3.7547)  loss_scale: 65536.0000 (58080.0373)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [24]  [2580/2809]  eta: 0:01:25  lr: 0.000018  min_lr: 0.000000  loss: 3.5041 (3.7543)  loss_scale: 65536.0000 (58108.9252)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
[2025-01-13 06:36:17,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=70000, skipped=475, lr=[1.7842888740181126e-07, 1.7842888740181126e-07, 2.548984105740161e-07, 2.548984105740161e-07, 3.6414058653430874e-07, 3.6414058653430874e-07, 5.202008379061554e-07, 5.202008379061554e-07, 7.431440541516506e-07, 7.431440541516506e-07, 1.0616343630737865e-06, 1.0616343630737865e-06, 1.516620518676838e-06, 1.516620518676838e-06, 2.1666007409669117e-06, 2.1666007409669117e-06, 3.095143915667017e-06, 3.095143915667017e-06, 4.421634165238596e-06, 4.421634165238596e-06, 6.316620236055137e-06, 6.316620236055137e-06, 9.023743194364482e-06, 9.023743194364482e-06, 1.2891061706234976e-05, 1.2891061706234976e-05, 1.8415802437478537e-05, 1.8415802437478537e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 06:36:17,531] [INFO] [timer.py:260:stop] epoch=0/micro_step=70000/global_step=70000, RunningAvgSamplesPerSec=30.392792739699992, CurrSamplesPerSec=34.89420669755485, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [2590/2809]  eta: 0:01:21  lr: 0.000018  min_lr: 0.000000  loss: 3.5702 (3.7536)  loss_scale: 65536.0000 (58137.5901)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [24]  [2600/2809]  eta: 0:01:17  lr: 0.000018  min_lr: 0.000000  loss: 3.7593 (3.7536)  loss_scale: 65536.0000 (58166.0346)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [24]  [2610/2809]  eta: 0:01:13  lr: 0.000018  min_lr: 0.000000  loss: 3.6835 (3.7528)  loss_scale: 65536.0000 (58194.2612)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [24]  [2620/2809]  eta: 0:01:10  lr: 0.000018  min_lr: 0.000000  loss: 3.5818 (3.7522)  loss_scale: 65536.0000 (58222.2724)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [24]  [2630/2809]  eta: 0:01:06  lr: 0.000018  min_lr: 0.000000  loss: 3.7473 (3.7528)  loss_scale: 65536.0000 (58250.0707)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [24]  [2640/2809]  eta: 0:01:02  lr: 0.000018  min_lr: 0.000000  loss: 3.7920 (3.7528)  loss_scale: 65536.0000 (58277.6585)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [24]  [2650/2809]  eta: 0:00:59  lr: 0.000018  min_lr: 0.000000  loss: 3.8013 (3.7523)  loss_scale: 65536.0000 (58305.0381)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0002  max mem: 15572
[2025-01-13 06:36:45,714] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:36:45,714] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:36:46,077] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70076
[2025-01-13 06:36:46,077] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:36:46,077] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [2660/2809]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 3.7719 (3.7532)  loss_scale: 65536.0000 (58356.8403)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [24]  [2670/2809]  eta: 0:00:51  lr: 0.000018  min_lr: 0.000000  loss: 3.7775 (3.7527)  loss_scale: 65536.0000 (58383.7185)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [24]  [2680/2809]  eta: 0:00:47  lr: 0.000018  min_lr: 0.000000  loss: 3.7885 (3.7524)  loss_scale: 65536.0000 (58410.3961)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [24]  [2690/2809]  eta: 0:00:44  lr: 0.000018  min_lr: 0.000000  loss: 3.5235 (3.7517)  loss_scale: 65536.0000 (58436.8755)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [24]  [2700/2809]  eta: 0:00:40  lr: 0.000018  min_lr: 0.000000  loss: 3.5687 (3.7523)  loss_scale: 65536.0000 (58463.1588)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [24]  [2710/2809]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 3.7392 (3.7516)  loss_scale: 65536.0000 (58489.2482)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [24]  [2720/2809]  eta: 0:00:33  lr: 0.000018  min_lr: 0.000000  loss: 3.7243 (3.7511)  loss_scale: 65536.0000 (58515.1459)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [24]  [2730/2809]  eta: 0:00:29  lr: 0.000018  min_lr: 0.000000  loss: 3.7473 (3.7514)  loss_scale: 65536.0000 (58540.8539)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [24]  [2740/2809]  eta: 0:00:25  lr: 0.000018  min_lr: 0.000000  loss: 3.8956 (3.7518)  loss_scale: 65536.0000 (58566.3743)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [24]  [2750/2809]  eta: 0:00:21  lr: 0.000018  min_lr: 0.000000  loss: 3.7187 (3.7523)  loss_scale: 65536.0000 (58591.7092)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [24]  [2760/2809]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 3.6732 (3.7514)  loss_scale: 65536.0000 (58616.8606)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [24]  [2770/2809]  eta: 0:00:14  lr: 0.000018  min_lr: 0.000000  loss: 3.4326 (3.7515)  loss_scale: 65536.0000 (58641.8304)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [24]  [2780/2809]  eta: 0:00:10  lr: 0.000018  min_lr: 0.000000  loss: 3.8811 (3.7524)  loss_scale: 65536.0000 (58666.6206)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0002  max mem: 15572
[2025-01-13 06:37:33,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:37:33,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [24]  [2790/2809]  eta: 0:00:07  lr: 0.000018  min_lr: 0.000000  loss: 4.0037 (3.7528)  loss_scale: 65536.0000 (58738.1956)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
[2025-01-13 06:37:37,739] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70216
[2025-01-13 06:37:37,739] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:37:37,739] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [2800/2809]  eta: 0:00:03  lr: 0.000018  min_lr: 0.000000  loss: 3.7976 (3.7518)  loss_scale: 131072.0000 (58973.0411)  weight_decay: 0.0500 (0.0500)  time: 0.3627  data: 0.0002  max mem: 15572
Epoch: [24]  [2808/2809]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 3.7728 (3.7518)  loss_scale: 131072.0000 (58991.7323)  weight_decay: 0.0500 (0.0500)  time: 0.3593  data: 0.0002  max mem: 15572
Epoch: [24] Total time: 0:17:23 (0.3714 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 3.7728 (3.7518)  loss_scale: 131072.0000 (58991.7323)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:08:48  loss: 0.3579 (0.3579)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.9445  data: 1.7690  max mem: 15572
Val:  [ 10/272]  eta: 0:01:42  loss: 2.5540 (2.4141)  acc1: 38.8889 (39.3939)  acc5: 72.2222 (71.2121)  time: 0.3911  data: 0.2375  max mem: 15572
Val:  [ 20/272]  eta: 0:01:10  loss: 2.5310 (2.4530)  acc1: 44.4444 (42.3280)  acc5: 66.6667 (70.3704)  time: 0.1946  data: 0.0423  max mem: 15572
Val:  [ 30/272]  eta: 0:00:58  loss: 2.5310 (2.5347)  acc1: 44.4444 (38.5305)  acc5: 66.6667 (70.2509)  time: 0.1567  data: 0.0003  max mem: 15572
Val:  [ 40/272]  eta: 0:00:50  loss: 2.5194 (2.5402)  acc1: 33.3333 (36.9919)  acc5: 72.2222 (71.0027)  time: 0.1568  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:47  loss: 2.3266 (2.4173)  acc1: 33.3333 (40.1961)  acc5: 77.7778 (73.9651)  time: 0.1704  data: 0.0179  max mem: 15572
Val:  [ 60/272]  eta: 0:00:43  loss: 1.5573 (2.3106)  acc1: 66.6667 (43.8980)  acc5: 83.3333 (74.8634)  time: 0.1743  data: 0.0179  max mem: 15572
Val:  [ 70/272]  eta: 0:00:39  loss: 1.6986 (2.2396)  acc1: 61.1111 (46.3224)  acc5: 83.3333 (76.1346)  time: 0.1588  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:37  loss: 1.9705 (2.2630)  acc1: 50.0000 (45.8848)  acc5: 77.7778 (75.5144)  time: 0.1636  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.2115 (2.2605)  acc1: 44.4444 (45.9707)  acc5: 77.7778 (76.4347)  time: 0.1682  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:32  loss: 2.2115 (2.2982)  acc1: 38.8889 (45.2145)  acc5: 83.3333 (75.9076)  time: 0.1635  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:30  loss: 2.5878 (2.3787)  acc1: 27.7778 (43.2432)  acc5: 61.1111 (74.5746)  time: 0.1624  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 3.0692 (2.4202)  acc1: 27.7778 (42.2406)  acc5: 61.1111 (73.5996)  time: 0.1588  data: 0.0005  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.3162 (2.3892)  acc1: 44.4444 (43.2570)  acc5: 83.3333 (74.2578)  time: 0.1551  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.9506 (2.3738)  acc1: 55.5556 (43.8140)  acc5: 83.3333 (74.2711)  time: 0.1562  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.4527 (2.3772)  acc1: 44.4444 (43.4511)  acc5: 72.2222 (74.3561)  time: 0.1583  data: 0.0005  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.3292 (2.3645)  acc1: 44.4444 (44.0649)  acc5: 72.2222 (74.6377)  time: 0.1623  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.4704 (2.3875)  acc1: 38.8889 (43.2424)  acc5: 72.2222 (74.1715)  time: 0.1623  data: 0.0003  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 2.3769 (2.3761)  acc1: 33.3333 (42.8484)  acc5: 72.2222 (74.6777)  time: 0.1611  data: 0.0003  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.3769 (2.4260)  acc1: 33.3333 (41.7103)  acc5: 77.7778 (73.3275)  time: 0.1550  data: 0.0003  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.5782 (2.4358)  acc1: 33.3333 (41.4870)  acc5: 66.6667 (73.1067)  time: 0.1481  data: 0.0003  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.2741 (2.4403)  acc1: 44.4444 (41.8115)  acc5: 77.7778 (73.0121)  time: 0.1490  data: 0.0003  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.3502 (2.4297)  acc1: 44.4444 (42.0563)  acc5: 77.7778 (73.2026)  time: 0.1545  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.9013 (2.3969)  acc1: 66.6667 (43.2179)  acc5: 83.3333 (73.6412)  time: 0.1596  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.6294 (2.3779)  acc1: 66.6667 (43.5915)  acc5: 83.3333 (74.0664)  time: 0.1616  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.2018 (2.3864)  acc1: 44.4444 (43.0722)  acc5: 77.7778 (74.1479)  time: 0.1585  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.2671 (2.3252)  acc1: 72.2222 (44.8063)  acc5: 88.8889 (74.8616)  time: 0.1491  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4293 (2.3219)  acc1: 72.2222 (44.8954)  acc5: 88.8889 (75.0718)  time: 0.1391  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4293 (2.3260)  acc1: 72.2222 (44.8700)  acc5: 88.8889 (75.0563)  time: 0.1335  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1677 s / it)
* Acc@1 44.870 Acc@5 75.056 loss 2.326
Accuracy of the network on the 4883 val videos: 44.9%
Max accuracy: 45.46%
Epoch: [25]  [   0/2809]  eta: 3:15:48  lr: 0.000018  min_lr: 0.000000  loss: 3.2176 (3.2176)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.1826  data: 3.7843  max mem: 15572
Epoch: [25]  [  10/2809]  eta: 0:33:57  lr: 0.000018  min_lr: 0.000000  loss: 3.9452 (3.7784)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7279  data: 0.3444  max mem: 15572
Epoch: [25]  [  20/2809]  eta: 0:25:57  lr: 0.000018  min_lr: 0.000000  loss: 3.7985 (3.7821)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0003  max mem: 15572
Epoch: [25]  [  30/2809]  eta: 0:23:03  lr: 0.000018  min_lr: 0.000000  loss: 3.7875 (3.7636)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [25]  [  40/2809]  eta: 0:21:30  lr: 0.000018  min_lr: 0.000000  loss: 3.8430 (3.7699)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [25]  [  50/2809]  eta: 0:20:35  lr: 0.000018  min_lr: 0.000000  loss: 3.8155 (3.7470)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [25]  [  60/2809]  eta: 0:19:54  lr: 0.000018  min_lr: 0.000000  loss: 3.6421 (3.7340)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [25]  [  70/2809]  eta: 0:19:24  lr: 0.000018  min_lr: 0.000000  loss: 3.9796 (3.7654)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [25]  [  80/2809]  eta: 0:19:00  lr: 0.000018  min_lr: 0.000000  loss: 3.9038 (3.7671)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [25]  [  90/2809]  eta: 0:18:41  lr: 0.000018  min_lr: 0.000000  loss: 3.7938 (3.7575)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0003  max mem: 15572
Epoch: [25]  [ 100/2809]  eta: 0:18:25  lr: 0.000018  min_lr: 0.000000  loss: 3.6202 (3.7500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [25]  [ 110/2809]  eta: 0:18:11  lr: 0.000018  min_lr: 0.000000  loss: 3.7411 (3.7551)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
[2025-01-13 06:39:14,934] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:39:14,934] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [ 120/2809]  eta: 0:17:59  lr: 0.000018  min_lr: 0.000000  loss: 3.8080 (3.7597)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
[2025-01-13 06:39:15,297] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70346
[2025-01-13 06:39:15,298] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:39:15,298] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 130/2809]  eta: 0:17:48  lr: 0.000018  min_lr: 0.000000  loss: 3.8316 (3.7648)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [25]  [ 140/2809]  eta: 0:17:39  lr: 0.000018  min_lr: 0.000000  loss: 3.8849 (3.7696)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0003  max mem: 15572
Epoch: [25]  [ 150/2809]  eta: 0:17:30  lr: 0.000018  min_lr: 0.000000  loss: 3.7545 (3.7598)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [25]  [ 160/2809]  eta: 0:17:22  lr: 0.000018  min_lr: 0.000000  loss: 3.8249 (3.7723)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [25]  [ 170/2809]  eta: 0:17:14  lr: 0.000018  min_lr: 0.000000  loss: 3.9439 (3.7795)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [25]  [ 180/2809]  eta: 0:17:07  lr: 0.000018  min_lr: 0.000000  loss: 3.9439 (3.7808)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0003  max mem: 15572
Epoch: [25]  [ 190/2809]  eta: 0:17:00  lr: 0.000018  min_lr: 0.000000  loss: 3.7247 (3.7804)  loss_scale: 65536.0000 (65879.1204)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [25]  [ 200/2809]  eta: 0:16:54  lr: 0.000018  min_lr: 0.000000  loss: 3.6701 (3.7721)  loss_scale: 65536.0000 (65862.0498)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [25]  [ 210/2809]  eta: 0:16:48  lr: 0.000018  min_lr: 0.000000  loss: 3.7496 (3.7810)  loss_scale: 65536.0000 (65846.5972)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [25]  [ 220/2809]  eta: 0:16:42  lr: 0.000018  min_lr: 0.000000  loss: 3.9796 (3.7851)  loss_scale: 65536.0000 (65832.5430)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [25]  [ 230/2809]  eta: 0:16:37  lr: 0.000018  min_lr: 0.000000  loss: 3.8978 (3.7842)  loss_scale: 65536.0000 (65819.7056)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [25]  [ 240/2809]  eta: 0:16:31  lr: 0.000018  min_lr: 0.000000  loss: 3.6842 (3.7798)  loss_scale: 65536.0000 (65807.9336)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
[2025-01-13 06:40:03,039] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:40:03,039] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [ 250/2809]  eta: 0:16:25  lr: 0.000018  min_lr: 0.000000  loss: 3.7322 (3.7734)  loss_scale: 65536.0000 (66058.1992)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [25]  [ 260/2809]  eta: 0:16:20  lr: 0.000018  min_lr: 0.000000  loss: 3.9164 (3.7803)  loss_scale: 131072.0000 (68549.1494)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
[2025-01-13 06:40:07,801] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70488
[2025-01-13 06:40:07,801] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:40:07,802] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 270/2809]  eta: 0:16:14  lr: 0.000018  min_lr: 0.000000  loss: 3.5772 (3.7666)  loss_scale: 131072.0000 (68921.6236)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0003  max mem: 15572
Epoch: [25]  [ 280/2809]  eta: 0:16:09  lr: 0.000018  min_lr: 0.000000  loss: 3.6529 (3.7705)  loss_scale: 65536.0000 (68801.1388)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0003  max mem: 15572
Epoch: [25]  [ 290/2809]  eta: 0:16:05  lr: 0.000018  min_lr: 0.000000  loss: 3.7279 (3.7701)  loss_scale: 65536.0000 (68688.9347)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [25]  [ 300/2809]  eta: 0:16:00  lr: 0.000018  min_lr: 0.000000  loss: 3.7133 (3.7692)  loss_scale: 65536.0000 (68584.1860)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0003  max mem: 15572
Epoch: [25]  [ 310/2809]  eta: 0:15:55  lr: 0.000018  min_lr: 0.000000  loss: 3.8644 (3.7753)  loss_scale: 65536.0000 (68486.1736)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
Epoch: [25]  [ 320/2809]  eta: 0:15:50  lr: 0.000018  min_lr: 0.000000  loss: 3.8472 (3.7671)  loss_scale: 65536.0000 (68394.2679)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [25]  [ 330/2809]  eta: 0:15:46  lr: 0.000018  min_lr: 0.000000  loss: 3.6238 (3.7653)  loss_scale: 65536.0000 (68307.9154)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [25]  [ 340/2809]  eta: 0:15:41  lr: 0.000018  min_lr: 0.000000  loss: 3.6238 (3.7600)  loss_scale: 65536.0000 (68226.6276)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [25]  [ 350/2809]  eta: 0:15:37  lr: 0.000018  min_lr: 0.000000  loss: 3.9582 (3.7649)  loss_scale: 65536.0000 (68149.9715)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [25]  [ 360/2809]  eta: 0:15:32  lr: 0.000018  min_lr: 0.000000  loss: 3.9582 (3.7637)  loss_scale: 65536.0000 (68077.5623)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [25]  [ 370/2809]  eta: 0:15:28  lr: 0.000018  min_lr: 0.000000  loss: 3.8367 (3.7647)  loss_scale: 65536.0000 (68009.0566)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [25]  [ 380/2809]  eta: 0:15:23  lr: 0.000018  min_lr: 0.000000  loss: 4.0031 (3.7714)  loss_scale: 65536.0000 (67944.1470)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [25]  [ 390/2809]  eta: 0:15:19  lr: 0.000018  min_lr: 0.000000  loss: 3.9237 (3.7700)  loss_scale: 65536.0000 (67882.5575)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
[2025-01-13 06:40:55,772] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:40:55,772] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:40:56,862] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70620
[2025-01-13 06:40:56,863] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:40:56,863] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 400/2809]  eta: 0:15:15  lr: 0.000018  min_lr: 0.000000  loss: 3.6688 (3.7677)  loss_scale: 65536.0000 (68314.3342)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [25]  [ 410/2809]  eta: 0:15:10  lr: 0.000018  min_lr: 0.000000  loss: 3.7173 (3.7684)  loss_scale: 65536.0000 (68246.7348)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [25]  [ 420/2809]  eta: 0:15:06  lr: 0.000018  min_lr: 0.000000  loss: 3.7173 (3.7661)  loss_scale: 65536.0000 (68182.3468)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [25]  [ 430/2809]  eta: 0:15:02  lr: 0.000018  min_lr: 0.000000  loss: 3.6894 (3.7669)  loss_scale: 65536.0000 (68120.9466)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [25]  [ 440/2809]  eta: 0:14:58  lr: 0.000018  min_lr: 0.000000  loss: 3.7426 (3.7691)  loss_scale: 65536.0000 (68062.3311)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [25]  [ 450/2809]  eta: 0:14:53  lr: 0.000018  min_lr: 0.000000  loss: 3.9353 (3.7709)  loss_scale: 65536.0000 (68006.3149)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
Epoch: [25]  [ 460/2809]  eta: 0:14:49  lr: 0.000018  min_lr: 0.000000  loss: 3.9353 (3.7731)  loss_scale: 65536.0000 (67952.7289)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0003  max mem: 15572
Epoch: [25]  [ 470/2809]  eta: 0:14:45  lr: 0.000018  min_lr: 0.000000  loss: 3.8925 (3.7726)  loss_scale: 65536.0000 (67901.4183)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0003  max mem: 15572
Epoch: [25]  [ 480/2809]  eta: 0:14:41  lr: 0.000018  min_lr: 0.000000  loss: 3.9418 (3.7760)  loss_scale: 65536.0000 (67852.2412)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [25]  [ 490/2809]  eta: 0:14:37  lr: 0.000018  min_lr: 0.000000  loss: 3.9472 (3.7775)  loss_scale: 65536.0000 (67805.0672)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [25]  [ 500/2809]  eta: 0:14:33  lr: 0.000018  min_lr: 0.000000  loss: 3.9953 (3.7796)  loss_scale: 65536.0000 (67759.7764)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [25]  [ 510/2809]  eta: 0:14:29  lr: 0.000018  min_lr: 0.000000  loss: 3.9950 (3.7814)  loss_scale: 65536.0000 (67716.2583)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [25]  [ 520/2809]  eta: 0:14:25  lr: 0.000018  min_lr: 0.000000  loss: 3.5285 (3.7736)  loss_scale: 65536.0000 (67674.4107)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
[2025-01-13 06:41:44,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:41:44,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [ 530/2809]  eta: 0:14:20  lr: 0.000018  min_lr: 0.000000  loss: 3.4036 (3.7659)  loss_scale: 65536.0000 (68498.0791)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
[2025-01-13 06:41:48,092] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70758
[2025-01-13 06:41:48,092] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:41:48,092] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 540/2809]  eta: 0:14:16  lr: 0.000018  min_lr: 0.000000  loss: 3.5418 (3.7658)  loss_scale: 65536.0000 (68685.6044)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [25]  [ 550/2809]  eta: 0:14:12  lr: 0.000018  min_lr: 0.000000  loss: 3.9187 (3.7652)  loss_scale: 65536.0000 (68628.4428)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [25]  [ 560/2809]  eta: 0:14:08  lr: 0.000018  min_lr: 0.000000  loss: 3.7977 (3.7679)  loss_scale: 65536.0000 (68573.3191)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [25]  [ 570/2809]  eta: 0:14:04  lr: 0.000018  min_lr: 0.000000  loss: 3.6040 (3.7586)  loss_scale: 65536.0000 (68520.1261)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [25]  [ 580/2809]  eta: 0:14:00  lr: 0.000018  min_lr: 0.000000  loss: 3.4201 (3.7585)  loss_scale: 65536.0000 (68468.7642)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0003  max mem: 15572
Epoch: [25]  [ 590/2809]  eta: 0:13:56  lr: 0.000018  min_lr: 0.000000  loss: 3.8935 (3.7601)  loss_scale: 65536.0000 (68419.1404)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [25]  [ 600/2809]  eta: 0:13:52  lr: 0.000018  min_lr: 0.000000  loss: 3.9664 (3.7612)  loss_scale: 65536.0000 (68371.1681)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [25]  [ 610/2809]  eta: 0:13:48  lr: 0.000018  min_lr: 0.000000  loss: 3.9622 (3.7635)  loss_scale: 65536.0000 (68324.7660)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [25]  [ 620/2809]  eta: 0:13:44  lr: 0.000018  min_lr: 0.000000  loss: 3.9622 (3.7650)  loss_scale: 65536.0000 (68279.8583)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [25]  [ 630/2809]  eta: 0:13:40  lr: 0.000018  min_lr: 0.000000  loss: 3.8962 (3.7657)  loss_scale: 65536.0000 (68236.3740)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [25]  [ 640/2809]  eta: 0:13:36  lr: 0.000018  min_lr: 0.000000  loss: 3.9126 (3.7711)  loss_scale: 65536.0000 (68194.2465)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [25]  [ 650/2809]  eta: 0:13:32  lr: 0.000018  min_lr: 0.000000  loss: 3.8590 (3.7679)  loss_scale: 65536.0000 (68153.4132)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [25]  [ 660/2809]  eta: 0:13:28  lr: 0.000018  min_lr: 0.000000  loss: 3.6748 (3.7673)  loss_scale: 65536.0000 (68113.8154)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0003  max mem: 15572
[2025-01-13 06:42:35,811] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:42:35,812] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:42:36,538] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70889
[2025-01-13 06:42:36,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:42:36,538] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 670/2809]  eta: 0:13:24  lr: 0.000018  min_lr: 0.000000  loss: 3.6748 (3.7669)  loss_scale: 65536.0000 (68270.7362)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [25]  [ 680/2809]  eta: 0:13:20  lr: 0.000018  min_lr: 0.000000  loss: 3.6473 (3.7659)  loss_scale: 65536.0000 (68230.5786)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [25]  [ 690/2809]  eta: 0:13:16  lr: 0.000018  min_lr: 0.000000  loss: 3.9120 (3.7688)  loss_scale: 65536.0000 (68191.5832)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0003  max mem: 15572
Epoch: [25]  [ 700/2809]  eta: 0:13:12  lr: 0.000018  min_lr: 0.000000  loss: 3.7843 (3.7635)  loss_scale: 65536.0000 (68153.7004)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0003  max mem: 15572
Epoch: [25]  [ 710/2809]  eta: 0:13:09  lr: 0.000018  min_lr: 0.000000  loss: 3.4943 (3.7605)  loss_scale: 65536.0000 (68116.8833)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [25]  [ 720/2809]  eta: 0:13:05  lr: 0.000018  min_lr: 0.000000  loss: 3.7935 (3.7645)  loss_scale: 65536.0000 (68081.0874)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [25]  [ 730/2809]  eta: 0:13:01  lr: 0.000018  min_lr: 0.000000  loss: 4.0956 (3.7686)  loss_scale: 65536.0000 (68046.2709)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [25]  [ 740/2809]  eta: 0:12:57  lr: 0.000018  min_lr: 0.000000  loss: 4.1221 (3.7734)  loss_scale: 65536.0000 (68012.3941)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [25]  [ 750/2809]  eta: 0:12:53  lr: 0.000018  min_lr: 0.000000  loss: 3.9590 (3.7737)  loss_scale: 65536.0000 (67979.4194)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [25]  [ 760/2809]  eta: 0:12:49  lr: 0.000018  min_lr: 0.000000  loss: 3.6485 (3.7715)  loss_scale: 65536.0000 (67947.3114)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0003  max mem: 15572
Epoch: [25]  [ 770/2809]  eta: 0:12:45  lr: 0.000018  min_lr: 0.000000  loss: 3.7385 (3.7727)  loss_scale: 65536.0000 (67916.0363)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0003  max mem: 15572
[2025-01-13 06:43:17,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=71000, skipped=482, lr=[1.7137627550321582e-07, 1.7137627550321582e-07, 2.4482325071887975e-07, 2.4482325071887975e-07, 3.497475010269711e-07, 3.497475010269711e-07, 4.996392871813873e-07, 4.996392871813873e-07, 7.137704102591248e-07, 7.137704102591248e-07, 1.0196720146558926e-06, 1.0196720146558926e-06, 1.4566743066512753e-06, 1.4566743066512753e-06, 2.0809632952161078e-06, 2.0809632952161078e-06, 2.9728047074515825e-06, 2.9728047074515825e-06, 4.2468638677879755e-06, 4.2468638677879755e-06, 6.06694838255425e-06, 6.06694838255425e-06, 8.667069117934644e-06, 8.667069117934644e-06, 1.2381527311335207e-05, 1.2381527311335207e-05, 1.7687896159050297e-05, 1.7687896159050297e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 06:43:17,351] [INFO] [timer.py:260:stop] epoch=0/micro_step=71000/global_step=71000, RunningAvgSamplesPerSec=30.44137471478176, CurrSamplesPerSec=34.25326715652747, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [25]  [ 780/2809]  eta: 0:12:41  lr: 0.000018  min_lr: 0.000000  loss: 3.7385 (3.7723)  loss_scale: 65536.0000 (67885.5621)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [25]  [ 790/2809]  eta: 0:12:37  lr: 0.000018  min_lr: 0.000000  loss: 3.7166 (3.7724)  loss_scale: 65536.0000 (67855.8584)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
[2025-01-13 06:43:24,386] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:43:24,386] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [ 800/2809]  eta: 0:12:33  lr: 0.000018  min_lr: 0.000000  loss: 3.7522 (3.7743)  loss_scale: 65536.0000 (68481.4382)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0003  max mem: 15572
[2025-01-13 06:43:27,722] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71027
[2025-01-13 06:43:27,722] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:43:27,724] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 810/2809]  eta: 0:12:30  lr: 0.000018  min_lr: 0.000000  loss: 3.8260 (3.7749)  loss_scale: 65536.0000 (68525.9285)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
Epoch: [25]  [ 820/2809]  eta: 0:12:26  lr: 0.000018  min_lr: 0.000000  loss: 3.8260 (3.7758)  loss_scale: 65536.0000 (68489.5104)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [25]  [ 830/2809]  eta: 0:12:22  lr: 0.000018  min_lr: 0.000000  loss: 3.9757 (3.7800)  loss_scale: 65536.0000 (68453.9687)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0003  max mem: 15572
Epoch: [25]  [ 840/2809]  eta: 0:12:18  lr: 0.000018  min_lr: 0.000000  loss: 3.8100 (3.7778)  loss_scale: 65536.0000 (68419.2723)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0003  max mem: 15572
Epoch: [25]  [ 850/2809]  eta: 0:12:14  lr: 0.000018  min_lr: 0.000000  loss: 3.7373 (3.7771)  loss_scale: 65536.0000 (68385.3913)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [25]  [ 860/2809]  eta: 0:12:10  lr: 0.000018  min_lr: 0.000000  loss: 3.6355 (3.7748)  loss_scale: 65536.0000 (68352.2973)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [25]  [ 870/2809]  eta: 0:12:06  lr: 0.000018  min_lr: 0.000000  loss: 3.6026 (3.7754)  loss_scale: 65536.0000 (68319.9633)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [25]  [ 880/2809]  eta: 0:12:02  lr: 0.000018  min_lr: 0.000000  loss: 3.7493 (3.7752)  loss_scale: 65536.0000 (68288.3632)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [25]  [ 890/2809]  eta: 0:11:59  lr: 0.000018  min_lr: 0.000000  loss: 3.7915 (3.7748)  loss_scale: 65536.0000 (68257.4725)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [25]  [ 900/2809]  eta: 0:11:55  lr: 0.000018  min_lr: 0.000000  loss: 3.8162 (3.7773)  loss_scale: 65536.0000 (68227.2675)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [25]  [ 910/2809]  eta: 0:11:51  lr: 0.000018  min_lr: 0.000000  loss: 3.8511 (3.7766)  loss_scale: 65536.0000 (68197.7256)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [25]  [ 920/2809]  eta: 0:11:47  lr: 0.000018  min_lr: 0.000000  loss: 3.8348 (3.7768)  loss_scale: 65536.0000 (68168.8252)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [25]  [ 930/2809]  eta: 0:11:43  lr: 0.000018  min_lr: 0.000000  loss: 3.7710 (3.7765)  loss_scale: 65536.0000 (68140.5456)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
[2025-01-13 06:44:15,467] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:44:15,468] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:44:15,826] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71157
[2025-01-13 06:44:15,826] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:44:15,826] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 940/2809]  eta: 0:11:39  lr: 0.000018  min_lr: 0.000000  loss: 3.7520 (3.7760)  loss_scale: 65536.0000 (68182.5122)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0003  max mem: 15572
Epoch: [25]  [ 950/2809]  eta: 0:11:36  lr: 0.000018  min_lr: 0.000000  loss: 3.7748 (3.7759)  loss_scale: 65536.0000 (68154.6835)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [25]  [ 960/2809]  eta: 0:11:32  lr: 0.000018  min_lr: 0.000000  loss: 3.8190 (3.7767)  loss_scale: 65536.0000 (68127.4339)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [25]  [ 970/2809]  eta: 0:11:28  lr: 0.000018  min_lr: 0.000000  loss: 3.8961 (3.7770)  loss_scale: 65536.0000 (68100.7456)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [25]  [ 980/2809]  eta: 0:11:24  lr: 0.000018  min_lr: 0.000000  loss: 3.7752 (3.7753)  loss_scale: 65536.0000 (68074.6014)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [25]  [ 990/2809]  eta: 0:11:21  lr: 0.000018  min_lr: 0.000000  loss: 3.6444 (3.7743)  loss_scale: 65536.0000 (68048.9849)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [25]  [1000/2809]  eta: 0:11:17  lr: 0.000018  min_lr: 0.000000  loss: 3.8271 (3.7741)  loss_scale: 65536.0000 (68023.8801)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [25]  [1010/2809]  eta: 0:11:13  lr: 0.000018  min_lr: 0.000000  loss: 3.9750 (3.7751)  loss_scale: 65536.0000 (67999.2720)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [25]  [1020/2809]  eta: 0:11:09  lr: 0.000018  min_lr: 0.000000  loss: 3.9091 (3.7758)  loss_scale: 65536.0000 (67975.1459)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [25]  [1030/2809]  eta: 0:11:05  lr: 0.000018  min_lr: 0.000000  loss: 3.8488 (3.7736)  loss_scale: 65536.0000 (67951.4879)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [25]  [1040/2809]  eta: 0:11:01  lr: 0.000017  min_lr: 0.000000  loss: 3.8183 (3.7738)  loss_scale: 65536.0000 (67928.2843)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [25]  [1050/2809]  eta: 0:10:58  lr: 0.000017  min_lr: 0.000000  loss: 3.9373 (3.7760)  loss_scale: 65536.0000 (67905.5224)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [25]  [1060/2809]  eta: 0:10:54  lr: 0.000017  min_lr: 0.000000  loss: 3.7478 (3.7742)  loss_scale: 65536.0000 (67883.1894)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
[2025-01-13 06:45:03,759] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:45:03,759] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [1070/2809]  eta: 0:10:50  lr: 0.000017  min_lr: 0.000000  loss: 3.7069 (3.7731)  loss_scale: 65536.0000 (68473.1877)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0003  max mem: 15572
[2025-01-13 06:45:08,237] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71298
[2025-01-13 06:45:08,238] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:45:08,238] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1080/2809]  eta: 0:10:46  lr: 0.000017  min_lr: 0.000000  loss: 3.7728 (3.7738)  loss_scale: 131072.0000 (68567.2673)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [25]  [1090/2809]  eta: 0:10:43  lr: 0.000017  min_lr: 0.000000  loss: 3.9230 (3.7757)  loss_scale: 65536.0000 (68539.4830)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0003  max mem: 15572
Epoch: [25]  [1100/2809]  eta: 0:10:39  lr: 0.000017  min_lr: 0.000000  loss: 3.6402 (3.7719)  loss_scale: 65536.0000 (68512.2035)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [25]  [1110/2809]  eta: 0:10:35  lr: 0.000017  min_lr: 0.000000  loss: 3.3911 (3.7694)  loss_scale: 65536.0000 (68485.4149)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [25]  [1120/2809]  eta: 0:10:31  lr: 0.000017  min_lr: 0.000000  loss: 3.5878 (3.7684)  loss_scale: 65536.0000 (68459.1044)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0003  max mem: 15572
Epoch: [25]  [1130/2809]  eta: 0:10:27  lr: 0.000017  min_lr: 0.000000  loss: 3.3316 (3.7653)  loss_scale: 65536.0000 (68433.2591)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [25]  [1140/2809]  eta: 0:10:24  lr: 0.000017  min_lr: 0.000000  loss: 3.7800 (3.7659)  loss_scale: 65536.0000 (68407.8668)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [25]  [1150/2809]  eta: 0:10:20  lr: 0.000017  min_lr: 0.000000  loss: 3.9800 (3.7646)  loss_scale: 65536.0000 (68382.9157)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [25]  [1160/2809]  eta: 0:10:16  lr: 0.000017  min_lr: 0.000000  loss: 3.8965 (3.7657)  loss_scale: 65536.0000 (68358.3945)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0003  max mem: 15572
Epoch: [25]  [1170/2809]  eta: 0:10:12  lr: 0.000017  min_lr: 0.000000  loss: 4.0293 (3.7651)  loss_scale: 65536.0000 (68334.2921)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
Epoch: [25]  [1180/2809]  eta: 0:10:08  lr: 0.000017  min_lr: 0.000000  loss: 4.0293 (3.7654)  loss_scale: 65536.0000 (68310.5978)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [25]  [1190/2809]  eta: 0:10:05  lr: 0.000017  min_lr: 0.000000  loss: 3.9000 (3.7664)  loss_scale: 65536.0000 (68287.3014)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0003  max mem: 15572
Epoch: [25]  [1200/2809]  eta: 0:10:01  lr: 0.000017  min_lr: 0.000000  loss: 3.8368 (3.7635)  loss_scale: 65536.0000 (68264.3930)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
[2025-01-13 06:45:56,174] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:45:56,174] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:45:58,005] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71432
[2025-01-13 06:45:58,006] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:45:58,006] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1210/2809]  eta: 0:09:57  lr: 0.000017  min_lr: 0.000000  loss: 3.8368 (3.7642)  loss_scale: 65536.0000 (68512.4492)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0003  max mem: 15572
Epoch: [25]  [1220/2809]  eta: 0:09:53  lr: 0.000017  min_lr: 0.000000  loss: 3.9143 (3.7649)  loss_scale: 65536.0000 (68488.0721)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [25]  [1230/2809]  eta: 0:09:50  lr: 0.000017  min_lr: 0.000000  loss: 3.9143 (3.7655)  loss_scale: 65536.0000 (68464.0910)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [25]  [1240/2809]  eta: 0:09:46  lr: 0.000017  min_lr: 0.000000  loss: 3.7486 (3.7646)  loss_scale: 65536.0000 (68440.4964)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [25]  [1250/2809]  eta: 0:09:42  lr: 0.000017  min_lr: 0.000000  loss: 3.6038 (3.7622)  loss_scale: 65536.0000 (68417.2790)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [25]  [1260/2809]  eta: 0:09:38  lr: 0.000017  min_lr: 0.000000  loss: 3.7615 (3.7647)  loss_scale: 65536.0000 (68394.4298)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [25]  [1270/2809]  eta: 0:09:35  lr: 0.000017  min_lr: 0.000000  loss: 3.9680 (3.7646)  loss_scale: 65536.0000 (68371.9402)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [25]  [1280/2809]  eta: 0:09:31  lr: 0.000017  min_lr: 0.000000  loss: 3.9206 (3.7655)  loss_scale: 65536.0000 (68349.8017)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [25]  [1290/2809]  eta: 0:09:27  lr: 0.000017  min_lr: 0.000000  loss: 3.6336 (3.7635)  loss_scale: 65536.0000 (68328.0062)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [25]  [1300/2809]  eta: 0:09:23  lr: 0.000017  min_lr: 0.000000  loss: 3.7692 (3.7637)  loss_scale: 65536.0000 (68306.5457)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [25]  [1310/2809]  eta: 0:09:19  lr: 0.000017  min_lr: 0.000000  loss: 3.8676 (3.7636)  loss_scale: 65536.0000 (68285.4127)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [25]  [1320/2809]  eta: 0:09:16  lr: 0.000017  min_lr: 0.000000  loss: 3.7534 (3.7632)  loss_scale: 65536.0000 (68264.5995)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [25]  [1330/2809]  eta: 0:09:12  lr: 0.000017  min_lr: 0.000000  loss: 3.7127 (3.7615)  loss_scale: 65536.0000 (68244.0992)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
[2025-01-13 06:46:45,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:46:45,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [1340/2809]  eta: 0:09:08  lr: 0.000017  min_lr: 0.000000  loss: 3.7034 (3.7613)  loss_scale: 65536.0000 (68468.2595)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
[2025-01-13 06:46:50,677] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71574
[2025-01-13 06:46:50,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:46:50,679] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1350/2809]  eta: 0:09:04  lr: 0.000017  min_lr: 0.000000  loss: 3.7342 (3.7616)  loss_scale: 131072.0000 (68834.6292)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [25]  [1360/2809]  eta: 0:09:01  lr: 0.000017  min_lr: 0.000000  loss: 3.7565 (3.7612)  loss_scale: 65536.0000 (68810.3924)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0003  max mem: 15572
Epoch: [25]  [1370/2809]  eta: 0:08:57  lr: 0.000017  min_lr: 0.000000  loss: 3.7723 (3.7633)  loss_scale: 65536.0000 (68786.5091)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0003  max mem: 15572
Epoch: [25]  [1380/2809]  eta: 0:08:53  lr: 0.000017  min_lr: 0.000000  loss: 3.9181 (3.7622)  loss_scale: 65536.0000 (68762.9718)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [25]  [1390/2809]  eta: 0:08:49  lr: 0.000017  min_lr: 0.000000  loss: 3.4370 (3.7616)  loss_scale: 65536.0000 (68739.7728)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0003  max mem: 15572
Epoch: [25]  [1400/2809]  eta: 0:08:46  lr: 0.000017  min_lr: 0.000000  loss: 3.7709 (3.7614)  loss_scale: 65536.0000 (68716.9051)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [25]  [1410/2809]  eta: 0:08:42  lr: 0.000017  min_lr: 0.000000  loss: 3.7811 (3.7618)  loss_scale: 65536.0000 (68694.3614)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0003  max mem: 15572
Epoch: [25]  [1420/2809]  eta: 0:08:38  lr: 0.000017  min_lr: 0.000000  loss: 3.7811 (3.7609)  loss_scale: 65536.0000 (68672.1351)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [25]  [1430/2809]  eta: 0:08:34  lr: 0.000017  min_lr: 0.000000  loss: 3.4603 (3.7592)  loss_scale: 65536.0000 (68650.2194)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [25]  [1440/2809]  eta: 0:08:30  lr: 0.000017  min_lr: 0.000000  loss: 3.6616 (3.7590)  loss_scale: 65536.0000 (68628.6079)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [25]  [1450/2809]  eta: 0:08:27  lr: 0.000017  min_lr: 0.000000  loss: 3.8911 (3.7606)  loss_scale: 65536.0000 (68607.2943)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [25]  [1460/2809]  eta: 0:08:23  lr: 0.000017  min_lr: 0.000000  loss: 3.8660 (3.7603)  loss_scale: 65536.0000 (68586.2724)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0003  max mem: 15572
Epoch: [25]  [1470/2809]  eta: 0:08:19  lr: 0.000017  min_lr: 0.000000  loss: 3.8740 (3.7613)  loss_scale: 65536.0000 (68565.5364)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0003  max mem: 15572
[2025-01-13 06:47:38,418] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:47:38,418] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [1480/2809]  eta: 0:08:15  lr: 0.000017  min_lr: 0.000000  loss: 3.8991 (3.7619)  loss_scale: 65536.0000 (68677.8339)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
[2025-01-13 06:47:39,926] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71707
[2025-01-13 06:47:39,926] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:47:39,926] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1490/2809]  eta: 0:08:12  lr: 0.000017  min_lr: 0.000000  loss: 3.6510 (3.7608)  loss_scale: 65536.0000 (68700.7163)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [25]  [1500/2809]  eta: 0:08:08  lr: 0.000017  min_lr: 0.000000  loss: 3.6510 (3.7614)  loss_scale: 65536.0000 (68679.6322)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [25]  [1510/2809]  eta: 0:08:04  lr: 0.000017  min_lr: 0.000000  loss: 3.8643 (3.7611)  loss_scale: 65536.0000 (68658.8273)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [25]  [1520/2809]  eta: 0:08:00  lr: 0.000017  min_lr: 0.000000  loss: 3.7161 (3.7605)  loss_scale: 65536.0000 (68638.2959)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [25]  [1530/2809]  eta: 0:07:57  lr: 0.000017  min_lr: 0.000000  loss: 3.7464 (3.7613)  loss_scale: 65536.0000 (68618.0327)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0003  max mem: 15572
Epoch: [25]  [1540/2809]  eta: 0:07:53  lr: 0.000017  min_lr: 0.000000  loss: 3.8755 (3.7623)  loss_scale: 65536.0000 (68598.0324)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0003  max mem: 15572
Epoch: [25]  [1550/2809]  eta: 0:07:49  lr: 0.000017  min_lr: 0.000000  loss: 3.8755 (3.7626)  loss_scale: 65536.0000 (68578.2901)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [25]  [1560/2809]  eta: 0:07:45  lr: 0.000017  min_lr: 0.000000  loss: 3.8423 (3.7620)  loss_scale: 65536.0000 (68558.8008)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [25]  [1570/2809]  eta: 0:07:42  lr: 0.000017  min_lr: 0.000000  loss: 3.6813 (3.7608)  loss_scale: 65536.0000 (68539.5595)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [25]  [1580/2809]  eta: 0:07:38  lr: 0.000017  min_lr: 0.000000  loss: 3.7216 (3.7603)  loss_scale: 65536.0000 (68520.5617)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [25]  [1590/2809]  eta: 0:07:34  lr: 0.000017  min_lr: 0.000000  loss: 3.9681 (3.7622)  loss_scale: 65536.0000 (68501.8026)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [25]  [1600/2809]  eta: 0:07:30  lr: 0.000017  min_lr: 0.000000  loss: 3.9531 (3.7621)  loss_scale: 65536.0000 (68483.2780)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0003  max mem: 15572
Epoch: [25]  [1610/2809]  eta: 0:07:27  lr: 0.000017  min_lr: 0.000000  loss: 3.8984 (3.7628)  loss_scale: 65536.0000 (68464.9832)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
[2025-01-13 06:48:27,744] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:48:27,744] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:48:29,246] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71840
[2025-01-13 06:48:29,246] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:48:29,246] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1620/2809]  eta: 0:07:23  lr: 0.000017  min_lr: 0.000000  loss: 3.7535 (3.7617)  loss_scale: 65536.0000 (68608.6317)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0003  max mem: 15572
Epoch: [25]  [1630/2809]  eta: 0:07:19  lr: 0.000017  min_lr: 0.000000  loss: 3.6688 (3.7616)  loss_scale: 65536.0000 (68589.7928)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0003  max mem: 15572
Epoch: [25]  [1640/2809]  eta: 0:07:15  lr: 0.000017  min_lr: 0.000000  loss: 3.8140 (3.7620)  loss_scale: 65536.0000 (68571.1834)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0003  max mem: 15572
Epoch: [25]  [1650/2809]  eta: 0:07:12  lr: 0.000017  min_lr: 0.000000  loss: 3.6991 (3.7606)  loss_scale: 65536.0000 (68552.7995)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [25]  [1660/2809]  eta: 0:07:08  lr: 0.000017  min_lr: 0.000000  loss: 3.6874 (3.7601)  loss_scale: 65536.0000 (68534.6370)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [25]  [1670/2809]  eta: 0:07:04  lr: 0.000017  min_lr: 0.000000  loss: 3.7594 (3.7598)  loss_scale: 65536.0000 (68516.6918)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [25]  [1680/2809]  eta: 0:07:00  lr: 0.000017  min_lr: 0.000000  loss: 3.5997 (3.7585)  loss_scale: 65536.0000 (68498.9601)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [25]  [1690/2809]  eta: 0:06:57  lr: 0.000017  min_lr: 0.000000  loss: 3.5997 (3.7583)  loss_scale: 65536.0000 (68481.4382)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [25]  [1700/2809]  eta: 0:06:53  lr: 0.000017  min_lr: 0.000000  loss: 3.6111 (3.7575)  loss_scale: 65536.0000 (68464.1223)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [25]  [1710/2809]  eta: 0:06:49  lr: 0.000017  min_lr: 0.000000  loss: 3.6300 (3.7576)  loss_scale: 65536.0000 (68447.0088)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [25]  [1720/2809]  eta: 0:06:46  lr: 0.000017  min_lr: 0.000000  loss: 3.7114 (3.7570)  loss_scale: 65536.0000 (68430.0941)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [25]  [1730/2809]  eta: 0:06:42  lr: 0.000017  min_lr: 0.000000  loss: 3.7728 (3.7578)  loss_scale: 65536.0000 (68413.3749)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [25]  [1740/2809]  eta: 0:06:38  lr: 0.000017  min_lr: 0.000000  loss: 3.8583 (3.7572)  loss_scale: 65536.0000 (68396.8478)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
[2025-01-13 06:49:17,126] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:49:17,126] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:49:17,502] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71970
[2025-01-13 06:49:17,502] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:49:17,502] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1750/2809]  eta: 0:06:34  lr: 0.000017  min_lr: 0.000000  loss: 3.8882 (3.7578)  loss_scale: 65536.0000 (68417.9372)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0003  max mem: 15572
Epoch: [25]  [1760/2809]  eta: 0:06:31  lr: 0.000017  min_lr: 0.000000  loss: 3.9926 (3.7585)  loss_scale: 65536.0000 (68401.5718)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [25]  [1770/2809]  eta: 0:06:27  lr: 0.000017  min_lr: 0.000000  loss: 3.9451 (3.7578)  loss_scale: 65536.0000 (68385.3913)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
[2025-01-13 06:49:28,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=72000, skipped=490, lr=[1.6438077232268055e-07, 1.6438077232268055e-07, 2.3482967474668654e-07, 2.3482967474668654e-07, 3.3547096392383795e-07, 3.3547096392383795e-07, 4.792442341769114e-07, 4.792442341769114e-07, 6.846346202527306e-07, 6.846346202527306e-07, 9.78049457503901e-07, 9.78049457503901e-07, 1.3972135107198584e-06, 1.3972135107198584e-06, 1.9960193010283693e-06, 1.9960193010283693e-06, 2.851456144326242e-06, 2.851456144326242e-06, 4.073508777608918e-06, 4.073508777608918e-06, 5.819298253727025e-06, 5.819298253727025e-06, 8.313283219610036e-06, 8.313283219610036e-06, 1.1876118885157196e-05, 1.1876118885157196e-05, 1.6965884121653138e-05, 1.6965884121653138e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 06:49:28,275] [INFO] [timer.py:260:stop] epoch=0/micro_step=72000/global_step=72000, RunningAvgSamplesPerSec=30.488136020241587, CurrSamplesPerSec=34.73325553743692, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [25]  [1780/2809]  eta: 0:06:23  lr: 0.000017  min_lr: 0.000000  loss: 3.9083 (3.7586)  loss_scale: 65536.0000 (68369.3925)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0002  max mem: 15572
Epoch: [25]  [1790/2809]  eta: 0:06:19  lr: 0.000017  min_lr: 0.000000  loss: 3.9276 (3.7596)  loss_scale: 65536.0000 (68353.5723)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [25]  [1800/2809]  eta: 0:06:16  lr: 0.000017  min_lr: 0.000000  loss: 3.7176 (3.7585)  loss_scale: 65536.0000 (68337.9278)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0004  max mem: 15572
Epoch: [25]  [1810/2809]  eta: 0:06:12  lr: 0.000017  min_lr: 0.000000  loss: 3.6290 (3.7577)  loss_scale: 65536.0000 (68322.4561)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [25]  [1820/2809]  eta: 0:06:08  lr: 0.000017  min_lr: 0.000000  loss: 3.6290 (3.7568)  loss_scale: 65536.0000 (68307.1543)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [25]  [1830/2809]  eta: 0:06:04  lr: 0.000017  min_lr: 0.000000  loss: 3.8968 (3.7571)  loss_scale: 65536.0000 (68292.0197)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [25]  [1840/2809]  eta: 0:06:01  lr: 0.000017  min_lr: 0.000000  loss: 3.5111 (3.7561)  loss_scale: 65536.0000 (68277.0494)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [25]  [1850/2809]  eta: 0:05:57  lr: 0.000017  min_lr: 0.000000  loss: 3.8189 (3.7563)  loss_scale: 65536.0000 (68262.2410)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [25]  [1860/2809]  eta: 0:05:53  lr: 0.000017  min_lr: 0.000000  loss: 3.9915 (3.7571)  loss_scale: 65536.0000 (68247.5916)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [25]  [1870/2809]  eta: 0:05:50  lr: 0.000017  min_lr: 0.000000  loss: 3.9864 (3.7574)  loss_scale: 65536.0000 (68233.0989)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
[2025-01-13 06:50:05,485] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:50:05,485] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:50:05,851] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72100
[2025-01-13 06:50:05,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:50:05,851] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1880/2809]  eta: 0:05:46  lr: 0.000017  min_lr: 0.000000  loss: 3.5119 (3.7571)  loss_scale: 65536.0000 (68253.6013)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [25]  [1890/2809]  eta: 0:05:42  lr: 0.000017  min_lr: 0.000000  loss: 3.7517 (3.7577)  loss_scale: 65536.0000 (68239.2300)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [25]  [1900/2809]  eta: 0:05:38  lr: 0.000017  min_lr: 0.000000  loss: 3.8449 (3.7574)  loss_scale: 65536.0000 (68225.0100)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [25]  [1910/2809]  eta: 0:05:35  lr: 0.000017  min_lr: 0.000000  loss: 3.8449 (3.7572)  loss_scale: 65536.0000 (68210.9388)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [25]  [1920/2809]  eta: 0:05:31  lr: 0.000017  min_lr: 0.000000  loss: 3.8436 (3.7573)  loss_scale: 65536.0000 (68197.0141)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [25]  [1930/2809]  eta: 0:05:27  lr: 0.000017  min_lr: 0.000000  loss: 3.8107 (3.7568)  loss_scale: 65536.0000 (68183.2336)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0003  max mem: 15572
Epoch: [25]  [1940/2809]  eta: 0:05:23  lr: 0.000017  min_lr: 0.000000  loss: 3.5096 (3.7559)  loss_scale: 65536.0000 (68169.5951)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
Epoch: [25]  [1950/2809]  eta: 0:05:20  lr: 0.000017  min_lr: 0.000000  loss: 3.8125 (3.7563)  loss_scale: 65536.0000 (68156.0964)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0003  max mem: 15572
Epoch: [25]  [1960/2809]  eta: 0:05:16  lr: 0.000017  min_lr: 0.000000  loss: 3.6685 (3.7550)  loss_scale: 65536.0000 (68142.7353)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [25]  [1970/2809]  eta: 0:05:12  lr: 0.000017  min_lr: 0.000000  loss: 3.5647 (3.7549)  loss_scale: 65536.0000 (68129.5099)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [25]  [1980/2809]  eta: 0:05:08  lr: 0.000017  min_lr: 0.000000  loss: 3.6146 (3.7536)  loss_scale: 65536.0000 (68116.4180)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0003  max mem: 15572
Epoch: [25]  [1990/2809]  eta: 0:05:05  lr: 0.000017  min_lr: 0.000000  loss: 3.8504 (3.7544)  loss_scale: 65536.0000 (68103.4576)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [25]  [2000/2809]  eta: 0:05:01  lr: 0.000017  min_lr: 0.000000  loss: 3.8504 (3.7537)  loss_scale: 65536.0000 (68090.6267)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
[2025-01-13 06:50:53,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:50:53,781] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:50:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72231
[2025-01-13 06:50:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:50:54,514] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [2010/2809]  eta: 0:04:57  lr: 0.000017  min_lr: 0.000000  loss: 4.0156 (3.7536)  loss_scale: 65536.0000 (68143.1009)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [25]  [2020/2809]  eta: 0:04:54  lr: 0.000017  min_lr: 0.000000  loss: 4.0156 (3.7535)  loss_scale: 65536.0000 (68130.2009)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [25]  [2030/2809]  eta: 0:04:50  lr: 0.000017  min_lr: 0.000000  loss: 3.6783 (3.7520)  loss_scale: 65536.0000 (68117.4279)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [25]  [2040/2809]  eta: 0:04:46  lr: 0.000017  min_lr: 0.000000  loss: 3.7273 (3.7531)  loss_scale: 65536.0000 (68104.7800)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [25]  [2050/2809]  eta: 0:04:42  lr: 0.000017  min_lr: 0.000000  loss: 3.9951 (3.7538)  loss_scale: 65536.0000 (68092.2555)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [25]  [2060/2809]  eta: 0:04:39  lr: 0.000017  min_lr: 0.000000  loss: 4.0459 (3.7549)  loss_scale: 65536.0000 (68079.8525)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [25]  [2070/2809]  eta: 0:04:35  lr: 0.000017  min_lr: 0.000000  loss: 3.8937 (3.7548)  loss_scale: 65536.0000 (68067.5693)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
[2025-01-13 06:51:21,510] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72304
[2025-01-13 06:51:21,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:51:21,512] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [2080/2809]  eta: 0:04:31  lr: 0.000017  min_lr: 0.000000  loss: 3.8019 (3.7549)  loss_scale: 65536.0000 (68023.9116)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [25]  [2090/2809]  eta: 0:04:27  lr: 0.000017  min_lr: 0.000000  loss: 3.7561 (3.7538)  loss_scale: 32768.0000 (67855.3037)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
Epoch: [25]  [2100/2809]  eta: 0:04:24  lr: 0.000017  min_lr: 0.000000  loss: 3.6685 (3.7537)  loss_scale: 32768.0000 (67688.3008)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0003  max mem: 15572
Epoch: [25]  [2110/2809]  eta: 0:04:20  lr: 0.000017  min_lr: 0.000000  loss: 3.6409 (3.7523)  loss_scale: 32768.0000 (67522.8802)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [25]  [2120/2809]  eta: 0:04:16  lr: 0.000017  min_lr: 0.000000  loss: 3.6184 (3.7516)  loss_scale: 32768.0000 (67359.0193)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [25]  [2130/2809]  eta: 0:04:12  lr: 0.000017  min_lr: 0.000000  loss: 3.7813 (3.7512)  loss_scale: 32768.0000 (67196.6964)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [25]  [2140/2809]  eta: 0:04:09  lr: 0.000017  min_lr: 0.000000  loss: 3.6087 (3.7513)  loss_scale: 32768.0000 (67035.8898)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0003  max mem: 15572
Epoch: [25]  [2150/2809]  eta: 0:04:05  lr: 0.000017  min_lr: 0.000000  loss: 3.7389 (3.7512)  loss_scale: 32768.0000 (66876.5783)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [25]  [2160/2809]  eta: 0:04:01  lr: 0.000017  min_lr: 0.000000  loss: 3.7389 (3.7512)  loss_scale: 32768.0000 (66718.7413)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [25]  [2170/2809]  eta: 0:03:58  lr: 0.000017  min_lr: 0.000000  loss: 3.6334 (3.7517)  loss_scale: 32768.0000 (66562.3584)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [25]  [2180/2809]  eta: 0:03:54  lr: 0.000017  min_lr: 0.000000  loss: 3.6396 (3.7507)  loss_scale: 32768.0000 (66407.4094)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [25]  [2190/2809]  eta: 0:03:50  lr: 0.000017  min_lr: 0.000000  loss: 3.7402 (3.7503)  loss_scale: 32768.0000 (66253.8749)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [25]  [2200/2809]  eta: 0:03:46  lr: 0.000017  min_lr: 0.000000  loss: 3.6151 (3.7498)  loss_scale: 32768.0000 (66101.7356)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
[2025-01-13 06:52:09,351] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:52:09,352] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [2210/2809]  eta: 0:03:43  lr: 0.000017  min_lr: 0.000000  loss: 3.6151 (3.7497)  loss_scale: 32768.0000 (65995.4337)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [25]  [2220/2809]  eta: 0:03:39  lr: 0.000017  min_lr: 0.000000  loss: 3.7460 (3.7494)  loss_scale: 65536.0000 (65993.3652)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [25]  [2230/2809]  eta: 0:03:35  lr: 0.000017  min_lr: 0.000000  loss: 3.7769 (3.7493)  loss_scale: 65536.0000 (65991.3151)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
Epoch: [25]  [2240/2809]  eta: 0:03:31  lr: 0.000017  min_lr: 0.000000  loss: 3.8702 (3.7502)  loss_scale: 65536.0000 (65989.2834)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
Epoch: [25]  [2250/2809]  eta: 0:03:28  lr: 0.000017  min_lr: 0.000000  loss: 3.7846 (3.7496)  loss_scale: 65536.0000 (65987.2697)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [25]  [2260/2809]  eta: 0:03:24  lr: 0.000017  min_lr: 0.000000  loss: 3.5361 (3.7494)  loss_scale: 65536.0000 (65985.2738)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0003  max mem: 15572
Epoch: [25]  [2270/2809]  eta: 0:03:20  lr: 0.000017  min_lr: 0.000000  loss: 3.7857 (3.7494)  loss_scale: 65536.0000 (65983.2955)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [25]  [2280/2809]  eta: 0:03:17  lr: 0.000017  min_lr: 0.000000  loss: 3.7320 (3.7491)  loss_scale: 65536.0000 (65981.3345)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [25]  [2290/2809]  eta: 0:03:13  lr: 0.000017  min_lr: 0.000000  loss: 3.4480 (3.7484)  loss_scale: 65536.0000 (65979.3907)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
[2025-01-13 06:52:42,639] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72522
[2025-01-13 06:52:42,639] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:52:42,639] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [2300/2809]  eta: 0:03:09  lr: 0.000017  min_lr: 0.000000  loss: 3.4739 (3.7474)  loss_scale: 65536.0000 (65920.5007)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [25]  [2310/2809]  eta: 0:03:05  lr: 0.000017  min_lr: 0.000000  loss: 3.5066 (3.7466)  loss_scale: 32768.0000 (65777.0454)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [25]  [2320/2809]  eta: 0:03:02  lr: 0.000017  min_lr: 0.000000  loss: 3.6665 (3.7466)  loss_scale: 32768.0000 (65634.8264)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [25]  [2330/2809]  eta: 0:02:58  lr: 0.000017  min_lr: 0.000000  loss: 3.7297 (3.7462)  loss_scale: 32768.0000 (65493.8275)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [25]  [2340/2809]  eta: 0:02:54  lr: 0.000017  min_lr: 0.000000  loss: 3.6024 (3.7451)  loss_scale: 32768.0000 (65354.0333)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0003  max mem: 15572
Epoch: [25]  [2350/2809]  eta: 0:02:50  lr: 0.000017  min_lr: 0.000000  loss: 3.3740 (3.7436)  loss_scale: 32768.0000 (65215.4283)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [25]  [2360/2809]  eta: 0:02:47  lr: 0.000017  min_lr: 0.000000  loss: 3.5221 (3.7434)  loss_scale: 32768.0000 (65077.9975)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0003  max mem: 15572
Epoch: [25]  [2370/2809]  eta: 0:02:43  lr: 0.000017  min_lr: 0.000000  loss: 3.7190 (3.7435)  loss_scale: 32768.0000 (64941.7259)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0003  max mem: 15572
Epoch: [25]  [2380/2809]  eta: 0:02:39  lr: 0.000017  min_lr: 0.000000  loss: 3.5343 (3.7421)  loss_scale: 32768.0000 (64806.5989)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [25]  [2390/2809]  eta: 0:02:36  lr: 0.000017  min_lr: 0.000000  loss: 3.4936 (3.7414)  loss_scale: 32768.0000 (64672.6023)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [25]  [2400/2809]  eta: 0:02:32  lr: 0.000017  min_lr: 0.000000  loss: 3.5880 (3.7416)  loss_scale: 32768.0000 (64539.7218)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [25]  [2410/2809]  eta: 0:02:28  lr: 0.000017  min_lr: 0.000000  loss: 3.9423 (3.7420)  loss_scale: 32768.0000 (64407.9436)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [25]  [2420/2809]  eta: 0:02:24  lr: 0.000017  min_lr: 0.000000  loss: 3.9791 (3.7426)  loss_scale: 32768.0000 (64277.2540)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
[2025-01-13 06:53:30,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:53:30,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [2430/2809]  eta: 0:02:21  lr: 0.000016  min_lr: 0.000000  loss: 4.0065 (3.7431)  loss_scale: 32768.0000 (64215.0358)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [25]  [2440/2809]  eta: 0:02:17  lr: 0.000016  min_lr: 0.000000  loss: 3.8718 (3.7432)  loss_scale: 65536.0000 (64220.4474)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
Epoch: [25]  [2450/2809]  eta: 0:02:13  lr: 0.000016  min_lr: 0.000000  loss: 3.6185 (3.7423)  loss_scale: 65536.0000 (64225.8148)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [25]  [2460/2809]  eta: 0:02:10  lr: 0.000016  min_lr: 0.000000  loss: 3.6498 (3.7421)  loss_scale: 65536.0000 (64231.1386)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [25]  [2470/2809]  eta: 0:02:06  lr: 0.000016  min_lr: 0.000000  loss: 3.6498 (3.7416)  loss_scale: 65536.0000 (64236.4193)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [25]  [2480/2809]  eta: 0:02:02  lr: 0.000016  min_lr: 0.000000  loss: 3.7973 (3.7425)  loss_scale: 65536.0000 (64241.6574)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [25]  [2490/2809]  eta: 0:01:58  lr: 0.000016  min_lr: 0.000000  loss: 3.8517 (3.7420)  loss_scale: 65536.0000 (64246.8535)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [25]  [2500/2809]  eta: 0:01:55  lr: 0.000016  min_lr: 0.000000  loss: 3.6549 (3.7423)  loss_scale: 65536.0000 (64252.0080)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [25]  [2510/2809]  eta: 0:01:51  lr: 0.000016  min_lr: 0.000000  loss: 3.8183 (3.7427)  loss_scale: 65536.0000 (64257.1215)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [25]  [2520/2809]  eta: 0:01:47  lr: 0.000016  min_lr: 0.000000  loss: 3.8297 (3.7428)  loss_scale: 65536.0000 (64262.1944)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [25]  [2530/2809]  eta: 0:01:43  lr: 0.000016  min_lr: 0.000000  loss: 3.5573 (3.7418)  loss_scale: 65536.0000 (64267.2272)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [25]  [2540/2809]  eta: 0:01:40  lr: 0.000016  min_lr: 0.000000  loss: 3.4589 (3.7423)  loss_scale: 65536.0000 (64272.2204)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0003  max mem: 15572
Epoch: [25]  [2550/2809]  eta: 0:01:36  lr: 0.000016  min_lr: 0.000000  loss: 3.5305 (3.7419)  loss_scale: 65536.0000 (64277.1744)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0003  max mem: 15572
[2025-01-13 06:54:18,324] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:54:18,324] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:54:18,717] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72780
[2025-01-13 06:54:18,717] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:54:18,717] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [2560/2809]  eta: 0:01:32  lr: 0.000016  min_lr: 0.000000  loss: 3.6295 (3.7424)  loss_scale: 65536.0000 (64307.6798)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
[2025-01-13 06:54:22,024] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72789
[2025-01-13 06:54:22,024] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:54:22,025] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [2570/2809]  eta: 0:01:29  lr: 0.000016  min_lr: 0.000000  loss: 3.9330 (3.7429)  loss_scale: 65536.0000 (64223.2408)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [25]  [2580/2809]  eta: 0:01:25  lr: 0.000016  min_lr: 0.000000  loss: 3.8324 (3.7432)  loss_scale: 32768.0000 (64101.3685)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
Epoch: [25]  [2590/2809]  eta: 0:01:21  lr: 0.000016  min_lr: 0.000000  loss: 3.9395 (3.7433)  loss_scale: 32768.0000 (63980.4369)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [25]  [2600/2809]  eta: 0:01:17  lr: 0.000016  min_lr: 0.000000  loss: 3.9180 (3.7431)  loss_scale: 32768.0000 (63860.4352)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
Epoch: [25]  [2610/2809]  eta: 0:01:14  lr: 0.000016  min_lr: 0.000000  loss: 3.7413 (3.7432)  loss_scale: 32768.0000 (63741.3527)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [25]  [2620/2809]  eta: 0:01:10  lr: 0.000016  min_lr: 0.000000  loss: 3.9180 (3.7435)  loss_scale: 32768.0000 (63623.1789)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [25]  [2630/2809]  eta: 0:01:06  lr: 0.000016  min_lr: 0.000000  loss: 3.8230 (3.7429)  loss_scale: 32768.0000 (63505.9035)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [25]  [2640/2809]  eta: 0:01:02  lr: 0.000016  min_lr: 0.000000  loss: 3.6563 (3.7435)  loss_scale: 32768.0000 (63389.5161)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [25]  [2650/2809]  eta: 0:00:59  lr: 0.000016  min_lr: 0.000000  loss: 3.7627 (3.7428)  loss_scale: 32768.0000 (63274.0068)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [25]  [2660/2809]  eta: 0:00:55  lr: 0.000016  min_lr: 0.000000  loss: 3.8869 (3.7443)  loss_scale: 32768.0000 (63159.3657)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0003  max mem: 15572
Epoch: [25]  [2670/2809]  eta: 0:00:51  lr: 0.000016  min_lr: 0.000000  loss: 4.1440 (3.7450)  loss_scale: 32768.0000 (63045.5829)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [25]  [2680/2809]  eta: 0:00:48  lr: 0.000016  min_lr: 0.000000  loss: 3.7378 (3.7446)  loss_scale: 32768.0000 (62932.6490)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [25]  [2690/2809]  eta: 0:00:44  lr: 0.000016  min_lr: 0.000000  loss: 3.6171 (3.7444)  loss_scale: 32768.0000 (62820.5544)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
[2025-01-13 06:55:10,022] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:55:10,022] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [2700/2809]  eta: 0:00:40  lr: 0.000016  min_lr: 0.000000  loss: 3.7417 (3.7444)  loss_scale: 32768.0000 (62806.3443)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
Epoch: [25]  [2710/2809]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 3.8164 (3.7447)  loss_scale: 65536.0000 (62816.4131)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [25]  [2720/2809]  eta: 0:00:33  lr: 0.000016  min_lr: 0.000000  loss: 3.8164 (3.7453)  loss_scale: 65536.0000 (62826.4079)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0003  max mem: 15572
Epoch: [25]  [2730/2809]  eta: 0:00:29  lr: 0.000016  min_lr: 0.000000  loss: 3.7209 (3.7450)  loss_scale: 65536.0000 (62836.3295)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0003  max mem: 15572
Epoch: [25]  [2740/2809]  eta: 0:00:25  lr: 0.000016  min_lr: 0.000000  loss: 3.6781 (3.7447)  loss_scale: 65536.0000 (62846.1788)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0003  max mem: 15572
Epoch: [25]  [2750/2809]  eta: 0:00:21  lr: 0.000016  min_lr: 0.000000  loss: 3.8217 (3.7449)  loss_scale: 65536.0000 (62855.9564)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0003  max mem: 15572
Epoch: [25]  [2760/2809]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 3.9099 (3.7453)  loss_scale: 65536.0000 (62865.6632)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [25]  [2770/2809]  eta: 0:00:14  lr: 0.000016  min_lr: 0.000000  loss: 3.9099 (3.7457)  loss_scale: 65536.0000 (62875.2999)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
[2025-01-13 06:55:40,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=73000, skipped=496, lr=[1.5744952022753748e-07, 1.5744952022753748e-07, 2.2492788603933928e-07, 2.2492788603933928e-07, 3.2132555148477044e-07, 3.2132555148477044e-07, 4.5903650212110065e-07, 4.5903650212110065e-07, 6.557664316015724e-07, 6.557664316015724e-07, 9.368091880022463e-07, 9.368091880022463e-07, 1.338298840003209e-06, 1.338298840003209e-06, 1.91185548571887e-06, 1.91185548571887e-06, 2.731222122455529e-06, 2.731222122455529e-06, 3.9017458892221845e-06, 3.9017458892221845e-06, 5.573922698888835e-06, 5.573922698888835e-06, 7.962746712698336e-06, 7.962746712698336e-06, 1.137535244671191e-05, 1.137535244671191e-05, 1.625050349530273e-05, 1.625050349530273e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 06:55:40,030] [INFO] [timer.py:260:stop] epoch=0/micro_step=73000/global_step=73000, RunningAvgSamplesPerSec=30.532966841817764, CurrSamplesPerSec=33.2298867725217, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [25]  [2780/2809]  eta: 0:00:10  lr: 0.000016  min_lr: 0.000000  loss: 3.7875 (3.7459)  loss_scale: 65536.0000 (62884.8673)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [25]  [2790/2809]  eta: 0:00:07  lr: 0.000016  min_lr: 0.000000  loss: 3.7218 (3.7457)  loss_scale: 65536.0000 (62894.3662)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0003  max mem: 15572
Epoch: [25]  [2800/2809]  eta: 0:00:03  lr: 0.000016  min_lr: 0.000000  loss: 3.6126 (3.7457)  loss_scale: 65536.0000 (62903.7972)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0003  max mem: 15572
Epoch: [25]  [2808/2809]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 3.8122 (3.7468)  loss_scale: 65536.0000 (62911.2937)  weight_decay: 0.0500 (0.0500)  time: 0.3608  data: 0.0002  max mem: 15572
Epoch: [25] Total time: 0:17:26 (0.3725 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 3.8122 (3.7468)  loss_scale: 65536.0000 (62911.2937)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:10:29  loss: 0.4068 (0.4068)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3138  data: 2.1356  max mem: 15572
Val:  [ 10/272]  eta: 0:01:46  loss: 2.5270 (2.4164)  acc1: 44.4444 (38.8889)  acc5: 72.2222 (70.2020)  time: 0.4059  data: 0.2478  max mem: 15572
Val:  [ 20/272]  eta: 0:01:11  loss: 2.5270 (2.4351)  acc1: 44.4444 (43.9153)  acc5: 72.2222 (70.8995)  time: 0.1825  data: 0.0297  max mem: 15572
Val:  [ 30/272]  eta: 0:00:58  loss: 2.5575 (2.5361)  acc1: 44.4444 (39.9642)  acc5: 72.2222 (69.7133)  time: 0.1538  data: 0.0003  max mem: 15572
Val:  [ 40/272]  eta: 0:00:51  loss: 2.6070 (2.5602)  acc1: 33.3333 (38.0759)  acc5: 72.2222 (70.3252)  time: 0.1560  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:46  loss: 2.5264 (2.4654)  acc1: 38.8889 (40.8497)  acc5: 77.7778 (72.8758)  time: 0.1597  data: 0.0017  max mem: 15572
Val:  [ 60/272]  eta: 0:00:42  loss: 1.5566 (2.3557)  acc1: 61.1111 (43.8980)  acc5: 88.8889 (74.1348)  time: 0.1620  data: 0.0017  max mem: 15572
Val:  [ 70/272]  eta: 0:00:39  loss: 1.5907 (2.2691)  acc1: 61.1111 (46.4789)  acc5: 83.3333 (75.1956)  time: 0.1628  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:36  loss: 1.9569 (2.2818)  acc1: 55.5556 (46.2963)  acc5: 77.7778 (74.9657)  time: 0.1591  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.1307 (2.2738)  acc1: 50.0000 (47.0085)  acc5: 77.7778 (75.9463)  time: 0.1514  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:31  loss: 2.1666 (2.3024)  acc1: 50.0000 (46.2046)  acc5: 83.3333 (75.5226)  time: 0.1533  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:29  loss: 2.5035 (2.3694)  acc1: 33.3333 (44.4945)  acc5: 77.7778 (74.4244)  time: 0.1582  data: 0.0003  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 2.8108 (2.3975)  acc1: 27.7778 (43.7557)  acc5: 72.2222 (74.1047)  time: 0.1577  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.0783 (2.3608)  acc1: 38.8889 (44.6565)  acc5: 83.3333 (74.9364)  time: 0.1524  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.8178 (2.3533)  acc1: 55.5556 (44.9567)  acc5: 83.3333 (74.9015)  time: 0.1534  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.4334 (2.3579)  acc1: 38.8889 (44.3709)  acc5: 77.7778 (75.0552)  time: 0.1602  data: 0.0024  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.4096 (2.3525)  acc1: 38.8889 (44.6515)  acc5: 77.7778 (75.2243)  time: 0.1654  data: 0.0060  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.3279 (2.3686)  acc1: 38.8889 (44.1196)  acc5: 72.2222 (74.8863)  time: 0.1643  data: 0.0040  max mem: 15572
Val:  [180/272]  eta: 0:00:15  loss: 2.3279 (2.3564)  acc1: 38.8889 (44.0761)  acc5: 77.7778 (75.3837)  time: 0.1651  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.4703 (2.4153)  acc1: 33.3333 (42.6702)  acc5: 77.7778 (73.8802)  time: 0.1673  data: 0.0021  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.7088 (2.4242)  acc1: 33.3333 (42.3715)  acc5: 61.1111 (73.6595)  time: 0.1612  data: 0.0021  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.1900 (2.4263)  acc1: 44.4444 (42.4961)  acc5: 72.2222 (73.5387)  time: 0.1536  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.4523 (2.4149)  acc1: 44.4444 (42.7602)  acc5: 72.2222 (73.7808)  time: 0.1488  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.8043 (2.3827)  acc1: 66.6667 (43.8672)  acc5: 77.7778 (74.1462)  time: 0.1702  data: 0.0156  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.6506 (2.3649)  acc1: 66.6667 (44.2139)  acc5: 88.8889 (74.5274)  time: 0.1718  data: 0.0156  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.2409 (2.3758)  acc1: 44.4444 (43.7140)  acc5: 77.7778 (74.5684)  time: 0.1530  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.1756 (2.3149)  acc1: 77.7778 (45.4449)  acc5: 88.8889 (75.3299)  time: 0.1613  data: 0.0065  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4458 (2.3103)  acc1: 66.6667 (45.3670)  acc5: 88.8889 (75.5638)  time: 0.1494  data: 0.0064  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4458 (2.3148)  acc1: 66.6667 (45.3410)  acc5: 88.8889 (75.5273)  time: 0.1402  data: 0.0063  max mem: 15572
Val: Total time: 0:00:45 (0.1684 s / it)
* Acc@1 45.341 Acc@5 75.527 loss 2.315
Accuracy of the network on the 4883 val videos: 45.3%
Max accuracy: 45.46%
Epoch: [26]  [   0/2809]  eta: 3:13:19  lr: 0.000016  min_lr: 0.000000  loss: 4.0020 (4.0020)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.1296  data: 3.7319  max mem: 15572
Epoch: [26]  [  10/2809]  eta: 0:33:31  lr: 0.000016  min_lr: 0.000000  loss: 4.0020 (3.8951)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7186  data: 0.3395  max mem: 15572
[2025-01-13 06:56:47,055] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:56:47,055] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 06:56:48,579] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73050
[2025-01-13 06:56:48,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:56:48,579] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [  20/2809]  eta: 0:25:48  lr: 0.000016  min_lr: 0.000000  loss: 3.7758 (3.7547)  loss_scale: 65536.0000 (78019.0476)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0003  max mem: 15572
Epoch: [26]  [  30/2809]  eta: 0:22:53  lr: 0.000016  min_lr: 0.000000  loss: 3.5808 (3.7724)  loss_scale: 65536.0000 (73992.2581)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0003  max mem: 15572
Epoch: [26]  [  40/2809]  eta: 0:21:24  lr: 0.000016  min_lr: 0.000000  loss: 3.6844 (3.7741)  loss_scale: 65536.0000 (71929.7561)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [26]  [  50/2809]  eta: 0:20:28  lr: 0.000016  min_lr: 0.000000  loss: 3.8285 (3.7790)  loss_scale: 65536.0000 (70676.0784)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [26]  [  60/2809]  eta: 0:19:49  lr: 0.000016  min_lr: 0.000000  loss: 3.8285 (3.7847)  loss_scale: 65536.0000 (69833.4426)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [26]  [  70/2809]  eta: 0:19:19  lr: 0.000016  min_lr: 0.000000  loss: 3.7889 (3.7706)  loss_scale: 65536.0000 (69228.1690)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0001  max mem: 15572
Epoch: [26]  [  80/2809]  eta: 0:18:55  lr: 0.000016  min_lr: 0.000000  loss: 3.6577 (3.7753)  loss_scale: 65536.0000 (68772.3457)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
Epoch: [26]  [  90/2809]  eta: 0:18:36  lr: 0.000016  min_lr: 0.000000  loss: 3.7892 (3.7792)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [26]  [ 100/2809]  eta: 0:18:21  lr: 0.000016  min_lr: 0.000000  loss: 3.7648 (3.7692)  loss_scale: 65536.0000 (68131.4851)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [26]  [ 110/2809]  eta: 0:18:08  lr: 0.000016  min_lr: 0.000000  loss: 3.7133 (3.7774)  loss_scale: 65536.0000 (67897.6577)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [26]  [ 120/2809]  eta: 0:17:56  lr: 0.000016  min_lr: 0.000000  loss: 3.8083 (3.7769)  loss_scale: 65536.0000 (67702.4793)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [26]  [ 130/2809]  eta: 0:17:45  lr: 0.000016  min_lr: 0.000000  loss: 3.9193 (3.7829)  loss_scale: 65536.0000 (67537.0992)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [26]  [ 140/2809]  eta: 0:17:35  lr: 0.000016  min_lr: 0.000000  loss: 3.8289 (3.7700)  loss_scale: 65536.0000 (67395.1773)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
[2025-01-13 06:57:36,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:57:36,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [26]  [ 150/2809]  eta: 0:17:26  lr: 0.000016  min_lr: 0.000000  loss: 3.7098 (3.7727)  loss_scale: 65536.0000 (69876.1325)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0002  max mem: 15572
[2025-01-13 06:57:38,917] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73187
[2025-01-13 06:57:38,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 06:57:38,917] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [ 160/2809]  eta: 0:17:17  lr: 0.000016  min_lr: 0.000000  loss: 3.8195 (3.7772)  loss_scale: 65536.0000 (70420.6708)  weight_decay: 0.0500 (0.0500)  time: 0.3649  data: 0.0002  max mem: 15572
Epoch: [26]  [ 170/2809]  eta: 0:17:10  lr: 0.000016  min_lr: 0.000000  loss: 3.9232 (3.7856)  loss_scale: 65536.0000 (70135.0175)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [26]  [ 180/2809]  eta: 0:17:02  lr: 0.000016  min_lr: 0.000000  loss: 3.8728 (3.7813)  loss_scale: 65536.0000 (69880.9282)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [26]  [ 190/2809]  eta: 0:16:56  lr: 0.000016  min_lr: 0.000000  loss: 3.6968 (3.7705)  loss_scale: 65536.0000 (69653.4450)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [26]  [ 200/2809]  eta: 0:16:50  lr: 0.000016  min_lr: 0.000000  loss: 3.6430 (3.7689)  loss_scale: 65536.0000 (69448.5970)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [26]  [ 210/2809]  eta: 0:16:44  lr: 0.000016  min_lr: 0.000000  loss: 3.7108 (3.7591)  loss_scale: 65536.0000 (69263.1659)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [26]  [ 220/2809]  eta: 0:16:38  lr: 0.000016  min_lr: 0.000000  loss: 3.7712 (3.7582)  loss_scale: 65536.0000 (69094.5158)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
[2025-01-13 06:58:05,519] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73259
[2025-01-13 06:58:05,519] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:58:05,519] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 230/2809]  eta: 0:16:32  lr: 0.000016  min_lr: 0.000000  loss: 3.7712 (3.7578)  loss_scale: 65536.0000 (68089.3506)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0002  max mem: 15572
Epoch: [26]  [ 240/2809]  eta: 0:16:26  lr: 0.000016  min_lr: 0.000000  loss: 3.8411 (3.7644)  loss_scale: 32768.0000 (66623.7344)  weight_decay: 0.0500 (0.0500)  time: 0.3641  data: 0.0002  max mem: 15572
Epoch: [26]  [ 250/2809]  eta: 0:16:20  lr: 0.000016  min_lr: 0.000000  loss: 3.8176 (3.7601)  loss_scale: 32768.0000 (65274.9004)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [26]  [ 260/2809]  eta: 0:16:15  lr: 0.000016  min_lr: 0.000000  loss: 3.7723 (3.7575)  loss_scale: 32768.0000 (64029.4253)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [26]  [ 270/2809]  eta: 0:16:10  lr: 0.000016  min_lr: 0.000000  loss: 3.8075 (3.7615)  loss_scale: 32768.0000 (62875.8672)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [26]  [ 280/2809]  eta: 0:16:05  lr: 0.000016  min_lr: 0.000000  loss: 3.7010 (3.7564)  loss_scale: 32768.0000 (61804.4128)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [26]  [ 290/2809]  eta: 0:16:01  lr: 0.000016  min_lr: 0.000000  loss: 3.7010 (3.7576)  loss_scale: 32768.0000 (60806.5979)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0016  max mem: 15572
Epoch: [26]  [ 300/2809]  eta: 0:15:56  lr: 0.000016  min_lr: 0.000000  loss: 3.7286 (3.7547)  loss_scale: 32768.0000 (59875.0831)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0016  max mem: 15572
Epoch: [26]  [ 310/2809]  eta: 0:15:51  lr: 0.000016  min_lr: 0.000000  loss: 3.8089 (3.7543)  loss_scale: 32768.0000 (59003.4727)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [26]  [ 320/2809]  eta: 0:15:46  lr: 0.000016  min_lr: 0.000000  loss: 3.8692 (3.7540)  loss_scale: 32768.0000 (58186.1682)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [26]  [ 330/2809]  eta: 0:15:41  lr: 0.000016  min_lr: 0.000000  loss: 3.8692 (3.7522)  loss_scale: 32768.0000 (57418.2477)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [26]  [ 340/2809]  eta: 0:15:37  lr: 0.000016  min_lr: 0.000000  loss: 3.8549 (3.7525)  loss_scale: 32768.0000 (56695.3666)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [26]  [ 350/2809]  eta: 0:15:33  lr: 0.000016  min_lr: 0.000000  loss: 3.8241 (3.7458)  loss_scale: 32768.0000 (56013.6752)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
[2025-01-13 06:58:53,120] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 06:58:53,120] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 360/2809]  eta: 0:15:28  lr: 0.000016  min_lr: 0.000000  loss: 3.8734 (3.7461)  loss_scale: 32768.0000 (56005.1413)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0001  max mem: 15572
Epoch: [26]  [ 370/2809]  eta: 0:15:23  lr: 0.000016  min_lr: 0.000000  loss: 3.4593 (3.7411)  loss_scale: 65536.0000 (56262.0377)  weight_decay: 0.0500 (0.0500)  time: 0.3646  data: 0.0002  max mem: 15572
Epoch: [26]  [ 380/2809]  eta: 0:15:19  lr: 0.000016  min_lr: 0.000000  loss: 3.5514 (3.7402)  loss_scale: 65536.0000 (56505.4488)  weight_decay: 0.0500 (0.0500)  time: 0.3648  data: 0.0003  max mem: 15572
Epoch: [26]  [ 390/2809]  eta: 0:15:14  lr: 0.000016  min_lr: 0.000000  loss: 3.6262 (3.7373)  loss_scale: 65536.0000 (56736.4092)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0003  max mem: 15572
Epoch: [26]  [ 400/2809]  eta: 0:15:10  lr: 0.000016  min_lr: 0.000000  loss: 3.8888 (3.7424)  loss_scale: 65536.0000 (56955.8504)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [26]  [ 410/2809]  eta: 0:15:05  lr: 0.000016  min_lr: 0.000000  loss: 3.8293 (3.7380)  loss_scale: 65536.0000 (57164.6131)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [26]  [ 420/2809]  eta: 0:15:01  lr: 0.000016  min_lr: 0.000000  loss: 3.6014 (3.7372)  loss_scale: 65536.0000 (57363.4584)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0001  max mem: 15572
[2025-01-13 06:59:19,844] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73461
[2025-01-13 06:59:19,844] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 06:59:19,844] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 430/2809]  eta: 0:14:56  lr: 0.000016  min_lr: 0.000000  loss: 3.6032 (3.7366)  loss_scale: 65536.0000 (57248.9652)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [26]  [ 440/2809]  eta: 0:14:52  lr: 0.000016  min_lr: 0.000000  loss: 3.6627 (3.7325)  loss_scale: 32768.0000 (56693.8413)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0001  max mem: 15572
Epoch: [26]  [ 450/2809]  eta: 0:14:48  lr: 0.000016  min_lr: 0.000000  loss: 3.7248 (3.7338)  loss_scale: 32768.0000 (56163.3348)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [26]  [ 460/2809]  eta: 0:14:44  lr: 0.000016  min_lr: 0.000000  loss: 3.8743 (3.7319)  loss_scale: 32768.0000 (55655.8438)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [26]  [ 470/2809]  eta: 0:14:40  lr: 0.000016  min_lr: 0.000000  loss: 3.8744 (3.7325)  loss_scale: 32768.0000 (55169.9023)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [26]  [ 480/2809]  eta: 0:14:36  lr: 0.000016  min_lr: 0.000000  loss: 3.9586 (3.7341)  loss_scale: 32768.0000 (54704.1663)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [26]  [ 490/2809]  eta: 0:14:32  lr: 0.000016  min_lr: 0.000000  loss: 3.8780 (3.7334)  loss_scale: 32768.0000 (54257.4012)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [26]  [ 500/2809]  eta: 0:14:28  lr: 0.000016  min_lr: 0.000000  loss: 3.7217 (3.7347)  loss_scale: 32768.0000 (53828.4711)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [26]  [ 510/2809]  eta: 0:14:24  lr: 0.000016  min_lr: 0.000000  loss: 3.7246 (3.7326)  loss_scale: 32768.0000 (53416.3288)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [26]  [ 520/2809]  eta: 0:14:19  lr: 0.000016  min_lr: 0.000000  loss: 3.9299 (3.7350)  loss_scale: 32768.0000 (53020.0077)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [26]  [ 530/2809]  eta: 0:14:15  lr: 0.000016  min_lr: 0.000000  loss: 3.9181 (3.7333)  loss_scale: 32768.0000 (52638.6139)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [26]  [ 540/2809]  eta: 0:14:11  lr: 0.000016  min_lr: 0.000000  loss: 3.6091 (3.7332)  loss_scale: 32768.0000 (52271.3198)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [26]  [ 550/2809]  eta: 0:14:07  lr: 0.000016  min_lr: 0.000000  loss: 3.7543 (3.7325)  loss_scale: 32768.0000 (51917.3575)  weight_decay: 0.0500 (0.0500)  time: 0.3644  data: 0.0002  max mem: 15572
[2025-01-13 07:00:07,348] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:00:07,348] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 560/2809]  eta: 0:14:03  lr: 0.000016  min_lr: 0.000000  loss: 3.7914 (3.7310)  loss_scale: 32768.0000 (51868.0642)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [26]  [ 570/2809]  eta: 0:13:59  lr: 0.000016  min_lr: 0.000000  loss: 3.8293 (3.7316)  loss_scale: 65536.0000 (52107.4326)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [26]  [ 580/2809]  eta: 0:13:55  lr: 0.000016  min_lr: 0.000000  loss: 3.8105 (3.7310)  loss_scale: 65536.0000 (52338.5611)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [26]  [ 590/2809]  eta: 0:13:51  lr: 0.000016  min_lr: 0.000000  loss: 3.8496 (3.7330)  loss_scale: 65536.0000 (52561.8680)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [26]  [ 600/2809]  eta: 0:13:47  lr: 0.000016  min_lr: 0.000000  loss: 3.9346 (3.7336)  loss_scale: 65536.0000 (52777.7438)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [26]  [ 610/2809]  eta: 0:13:43  lr: 0.000016  min_lr: 0.000000  loss: 3.6861 (3.7309)  loss_scale: 65536.0000 (52986.5532)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
[2025-01-13 07:00:31,017] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73654
[2025-01-13 07:00:31,017] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:00:31,017] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 620/2809]  eta: 0:13:39  lr: 0.000016  min_lr: 0.000000  loss: 3.6861 (3.7318)  loss_scale: 65536.0000 (53135.8712)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [26]  [ 630/2809]  eta: 0:13:35  lr: 0.000016  min_lr: 0.000000  loss: 3.3955 (3.7252)  loss_scale: 32768.0000 (52813.0840)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [26]  [ 640/2809]  eta: 0:13:31  lr: 0.000016  min_lr: 0.000000  loss: 3.6048 (3.7275)  loss_scale: 32768.0000 (52500.3682)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0001  max mem: 15572
Epoch: [26]  [ 650/2809]  eta: 0:13:27  lr: 0.000016  min_lr: 0.000000  loss: 3.7968 (3.7246)  loss_scale: 32768.0000 (52197.2596)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [26]  [ 660/2809]  eta: 0:13:23  lr: 0.000016  min_lr: 0.000000  loss: 3.7968 (3.7253)  loss_scale: 32768.0000 (51903.3222)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [26]  [ 670/2809]  eta: 0:13:19  lr: 0.000016  min_lr: 0.000000  loss: 3.6081 (3.7226)  loss_scale: 32768.0000 (51618.1461)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [26]  [ 680/2809]  eta: 0:13:15  lr: 0.000016  min_lr: 0.000000  loss: 3.5371 (3.7205)  loss_scale: 32768.0000 (51341.3451)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [26]  [ 690/2809]  eta: 0:13:11  lr: 0.000016  min_lr: 0.000000  loss: 3.6026 (3.7177)  loss_scale: 32768.0000 (51072.5557)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [26]  [ 700/2809]  eta: 0:13:07  lr: 0.000016  min_lr: 0.000000  loss: 3.7708 (3.7189)  loss_scale: 32768.0000 (50811.4351)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [26]  [ 710/2809]  eta: 0:13:04  lr: 0.000016  min_lr: 0.000000  loss: 3.9782 (3.7185)  loss_scale: 32768.0000 (50557.6596)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [26]  [ 720/2809]  eta: 0:13:00  lr: 0.000016  min_lr: 0.000000  loss: 3.7356 (3.7177)  loss_scale: 32768.0000 (50310.9237)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [26]  [ 730/2809]  eta: 0:12:56  lr: 0.000016  min_lr: 0.000000  loss: 3.8979 (3.7194)  loss_scale: 32768.0000 (50070.9384)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [26]  [ 740/2809]  eta: 0:12:52  lr: 0.000016  min_lr: 0.000000  loss: 3.7421 (3.7187)  loss_scale: 32768.0000 (49837.4305)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0001  max mem: 15572
[2025-01-13 07:01:18,409] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:01:18,409] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 750/2809]  eta: 0:12:48  lr: 0.000016  min_lr: 0.000000  loss: 3.5945 (3.7165)  loss_scale: 32768.0000 (49697.4061)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0001  max mem: 15572
Epoch: [26]  [ 760/2809]  eta: 0:12:44  lr: 0.000016  min_lr: 0.000000  loss: 3.8086 (3.7182)  loss_scale: 65536.0000 (49905.5348)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0001  max mem: 15572
Epoch: [26]  [ 770/2809]  eta: 0:12:40  lr: 0.000016  min_lr: 0.000000  loss: 3.9546 (3.7201)  loss_scale: 65536.0000 (50108.2646)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [26]  [ 780/2809]  eta: 0:12:36  lr: 0.000016  min_lr: 0.000000  loss: 3.5701 (3.7171)  loss_scale: 65536.0000 (50305.8028)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [26]  [ 790/2809]  eta: 0:12:33  lr: 0.000016  min_lr: 0.000000  loss: 3.6690 (3.7189)  loss_scale: 65536.0000 (50498.3464)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [26]  [ 800/2809]  eta: 0:12:29  lr: 0.000016  min_lr: 0.000000  loss: 3.7366 (3.7177)  loss_scale: 65536.0000 (50686.0824)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [26]  [ 810/2809]  eta: 0:12:25  lr: 0.000016  min_lr: 0.000000  loss: 3.8373 (3.7197)  loss_scale: 65536.0000 (50869.1887)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [26]  [ 820/2809]  eta: 0:12:21  lr: 0.000016  min_lr: 0.000000  loss: 3.7656 (3.7176)  loss_scale: 65536.0000 (51047.8343)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [26]  [ 830/2809]  eta: 0:12:17  lr: 0.000016  min_lr: 0.000000  loss: 3.7656 (3.7185)  loss_scale: 65536.0000 (51222.1805)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [26]  [ 840/2809]  eta: 0:12:13  lr: 0.000016  min_lr: 0.000000  loss: 3.7946 (3.7173)  loss_scale: 65536.0000 (51392.3805)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [26]  [ 850/2809]  eta: 0:12:09  lr: 0.000016  min_lr: 0.000000  loss: 3.3983 (3.7128)  loss_scale: 65536.0000 (51558.5805)  weight_decay: 0.0500 (0.0500)  time: 0.3650  data: 0.0002  max mem: 15572
Epoch: [26]  [ 860/2809]  eta: 0:12:06  lr: 0.000016  min_lr: 0.000000  loss: 3.5397 (3.7127)  loss_scale: 65536.0000 (51720.9199)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [26]  [ 870/2809]  eta: 0:12:02  lr: 0.000016  min_lr: 0.000000  loss: 3.8779 (3.7131)  loss_scale: 65536.0000 (51879.5316)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
[2025-01-13 07:02:05,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:02:05,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [26]  [ 880/2809]  eta: 0:11:58  lr: 0.000016  min_lr: 0.000000  loss: 3.9444 (3.7151)  loss_scale: 65536.0000 (52332.0953)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 07:02:08,112] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73918
[2025-01-13 07:02:08,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:02:08,114] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [ 890/2809]  eta: 0:11:54  lr: 0.000016  min_lr: 0.000000  loss: 3.8366 (3.7152)  loss_scale: 65536.0000 (52700.9473)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [26]  [ 900/2809]  eta: 0:11:50  lr: 0.000016  min_lr: 0.000000  loss: 3.5192 (3.7125)  loss_scale: 65536.0000 (52843.4007)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [26]  [ 910/2809]  eta: 0:11:47  lr: 0.000016  min_lr: 0.000000  loss: 3.5463 (3.7137)  loss_scale: 65536.0000 (52982.7267)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [26]  [ 920/2809]  eta: 0:11:43  lr: 0.000016  min_lr: 0.000000  loss: 3.8190 (3.7135)  loss_scale: 65536.0000 (53119.0271)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [26]  [ 930/2809]  eta: 0:11:39  lr: 0.000016  min_lr: 0.000000  loss: 3.8190 (3.7146)  loss_scale: 65536.0000 (53252.3996)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [26]  [ 940/2809]  eta: 0:11:35  lr: 0.000016  min_lr: 0.000000  loss: 3.7105 (3.7149)  loss_scale: 65536.0000 (53382.9373)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [26]  [ 950/2809]  eta: 0:11:31  lr: 0.000016  min_lr: 0.000000  loss: 3.6398 (3.7132)  loss_scale: 65536.0000 (53510.7298)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [26]  [ 960/2809]  eta: 0:11:27  lr: 0.000016  min_lr: 0.000000  loss: 3.8375 (3.7153)  loss_scale: 65536.0000 (53635.8626)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
[2025-01-13 07:02:37,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=74000, skipped=502, lr=[1.505895959851408e-07, 1.505895959851408e-07, 2.151279942644869e-07, 2.151279942644869e-07, 3.073257060921242e-07, 3.073257060921242e-07, 4.390367229887489e-07, 4.390367229887489e-07, 6.271953185553555e-07, 6.271953185553555e-07, 8.959933122219366e-07, 8.959933122219366e-07, 1.279990446031338e-06, 1.279990446031338e-06, 1.8285577800447688e-06, 1.8285577800447688e-06, 2.612225400063955e-06, 2.612225400063955e-06, 3.7317505715199367e-06, 3.7317505715199367e-06, 5.331072245028481e-06, 5.331072245028481e-06, 7.615817492897831e-06, 7.615817492897831e-06, 1.087973927556833e-05, 1.087973927556833e-05, 1.554248467938333e-05, 1.554248467938333e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 07:02:37,949] [INFO] [timer.py:260:stop] epoch=0/micro_step=74000/global_step=74000, RunningAvgSamplesPerSec=30.58022004877051, CurrSamplesPerSec=34.65347862577603, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [26]  [ 970/2809]  eta: 0:11:24  lr: 0.000016  min_lr: 0.000000  loss: 3.8508 (3.7149)  loss_scale: 65536.0000 (53758.4181)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [26]  [ 980/2809]  eta: 0:11:20  lr: 0.000016  min_lr: 0.000000  loss: 3.8330 (3.7154)  loss_scale: 65536.0000 (53878.4750)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [26]  [ 990/2809]  eta: 0:11:16  lr: 0.000016  min_lr: 0.000000  loss: 3.8771 (3.7177)  loss_scale: 65536.0000 (53996.1090)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [26]  [1000/2809]  eta: 0:11:13  lr: 0.000016  min_lr: 0.000000  loss: 3.8504 (3.7177)  loss_scale: 65536.0000 (54111.3926)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [26]  [1010/2809]  eta: 0:11:09  lr: 0.000016  min_lr: 0.000000  loss: 3.7140 (3.7171)  loss_scale: 65536.0000 (54224.3956)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
[2025-01-13 07:02:55,770] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:02:55,770] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:02:56,501] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74049
[2025-01-13 07:02:56,501] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:02:56,503] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [1020/2809]  eta: 0:11:05  lr: 0.000016  min_lr: 0.000000  loss: 3.7045 (3.7171)  loss_scale: 65536.0000 (54463.5612)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [26]  [1030/2809]  eta: 0:11:01  lr: 0.000015  min_lr: 0.000000  loss: 3.6900 (3.7175)  loss_scale: 65536.0000 (54570.9564)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [26]  [1040/2809]  eta: 0:10:57  lr: 0.000015  min_lr: 0.000000  loss: 3.6465 (3.7181)  loss_scale: 65536.0000 (54676.2882)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [26]  [1050/2809]  eta: 0:10:54  lr: 0.000015  min_lr: 0.000000  loss: 3.6854 (3.7182)  loss_scale: 65536.0000 (54779.6156)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [26]  [1060/2809]  eta: 0:10:50  lr: 0.000015  min_lr: 0.000000  loss: 3.8145 (3.7203)  loss_scale: 65536.0000 (54880.9953)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [26]  [1070/2809]  eta: 0:10:46  lr: 0.000015  min_lr: 0.000000  loss: 3.9686 (3.7210)  loss_scale: 65536.0000 (54980.4818)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0001  max mem: 15572
Epoch: [26]  [1080/2809]  eta: 0:10:42  lr: 0.000015  min_lr: 0.000000  loss: 3.8366 (3.7233)  loss_scale: 65536.0000 (55078.1277)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [26]  [1090/2809]  eta: 0:10:39  lr: 0.000015  min_lr: 0.000000  loss: 3.9568 (3.7253)  loss_scale: 65536.0000 (55173.9835)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0001  max mem: 15572
Epoch: [26]  [1100/2809]  eta: 0:10:35  lr: 0.000015  min_lr: 0.000000  loss: 3.9469 (3.7256)  loss_scale: 65536.0000 (55268.0981)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [26]  [1110/2809]  eta: 0:10:31  lr: 0.000015  min_lr: 0.000000  loss: 3.8029 (3.7261)  loss_scale: 65536.0000 (55360.5185)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [26]  [1120/2809]  eta: 0:10:27  lr: 0.000015  min_lr: 0.000000  loss: 3.8029 (3.7265)  loss_scale: 65536.0000 (55451.2899)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [26]  [1130/2809]  eta: 0:10:24  lr: 0.000015  min_lr: 0.000000  loss: 3.8899 (3.7274)  loss_scale: 65536.0000 (55540.4562)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [26]  [1140/2809]  eta: 0:10:20  lr: 0.000015  min_lr: 0.000000  loss: 3.5148 (3.7253)  loss_scale: 65536.0000 (55628.0596)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 07:03:44,092] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:03:44,092] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [26]  [1150/2809]  eta: 0:10:16  lr: 0.000015  min_lr: 0.000000  loss: 3.5700 (3.7266)  loss_scale: 65536.0000 (56112.7089)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
[2025-01-13 07:03:47,747] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74188
[2025-01-13 07:03:47,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:03:47,748] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [1160/2809]  eta: 0:10:12  lr: 0.000015  min_lr: 0.000000  loss: 3.7885 (3.7265)  loss_scale: 65536.0000 (56363.2179)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [26]  [1170/2809]  eta: 0:10:08  lr: 0.000015  min_lr: 0.000000  loss: 3.8100 (3.7274)  loss_scale: 65536.0000 (56441.5508)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [26]  [1180/2809]  eta: 0:10:05  lr: 0.000015  min_lr: 0.000000  loss: 3.8100 (3.7262)  loss_scale: 65536.0000 (56518.5572)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [26]  [1190/2809]  eta: 0:10:01  lr: 0.000015  min_lr: 0.000000  loss: 3.5231 (3.7246)  loss_scale: 65536.0000 (56594.2704)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [26]  [1200/2809]  eta: 0:09:57  lr: 0.000015  min_lr: 0.000000  loss: 3.5231 (3.7231)  loss_scale: 65536.0000 (56668.7227)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
[2025-01-13 07:04:08,404] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74244
[2025-01-13 07:04:08,404] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:04:08,404] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1210/2809]  eta: 0:09:53  lr: 0.000015  min_lr: 0.000000  loss: 3.4453 (3.7204)  loss_scale: 65536.0000 (56714.8869)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [26]  [1220/2809]  eta: 0:09:50  lr: 0.000015  min_lr: 0.000000  loss: 3.4763 (3.7203)  loss_scale: 32768.0000 (56518.7617)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [26]  [1230/2809]  eta: 0:09:46  lr: 0.000015  min_lr: 0.000000  loss: 3.8034 (3.7213)  loss_scale: 32768.0000 (56325.8229)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [26]  [1240/2809]  eta: 0:09:42  lr: 0.000015  min_lr: 0.000000  loss: 3.9584 (3.7235)  loss_scale: 32768.0000 (56135.9936)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [26]  [1250/2809]  eta: 0:09:39  lr: 0.000015  min_lr: 0.000000  loss: 3.9623 (3.7247)  loss_scale: 32768.0000 (55949.1990)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [26]  [1260/2809]  eta: 0:09:35  lr: 0.000015  min_lr: 0.000000  loss: 4.0081 (3.7271)  loss_scale: 32768.0000 (55765.3672)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [26]  [1270/2809]  eta: 0:09:31  lr: 0.000015  min_lr: 0.000000  loss: 3.8983 (3.7270)  loss_scale: 32768.0000 (55584.4280)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [26]  [1280/2809]  eta: 0:09:27  lr: 0.000015  min_lr: 0.000000  loss: 3.9134 (3.7298)  loss_scale: 32768.0000 (55406.3138)  weight_decay: 0.0500 (0.0500)  time: 0.3637  data: 0.0002  max mem: 15572
Epoch: [26]  [1290/2809]  eta: 0:09:24  lr: 0.000015  min_lr: 0.000000  loss: 4.0737 (3.7321)  loss_scale: 32768.0000 (55230.9589)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [26]  [1300/2809]  eta: 0:09:20  lr: 0.000015  min_lr: 0.000000  loss: 4.0167 (3.7341)  loss_scale: 32768.0000 (55058.2998)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [26]  [1310/2809]  eta: 0:09:16  lr: 0.000015  min_lr: 0.000000  loss: 3.9901 (3.7328)  loss_scale: 32768.0000 (54888.2746)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [26]  [1320/2809]  eta: 0:09:12  lr: 0.000015  min_lr: 0.000000  loss: 3.9354 (3.7355)  loss_scale: 32768.0000 (54720.8236)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [26]  [1330/2809]  eta: 0:09:09  lr: 0.000015  min_lr: 0.000000  loss: 4.0403 (3.7368)  loss_scale: 32768.0000 (54555.8888)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
[2025-01-13 07:04:56,066] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:04:56,066] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1340/2809]  eta: 0:09:05  lr: 0.000015  min_lr: 0.000000  loss: 3.7117 (3.7360)  loss_scale: 32768.0000 (54442.2849)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [26]  [1350/2809]  eta: 0:09:01  lr: 0.000015  min_lr: 0.000000  loss: 3.6576 (3.7368)  loss_scale: 65536.0000 (54524.3997)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [26]  [1360/2809]  eta: 0:08:57  lr: 0.000015  min_lr: 0.000000  loss: 3.7611 (3.7382)  loss_scale: 65536.0000 (54605.3079)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [26]  [1370/2809]  eta: 0:08:54  lr: 0.000015  min_lr: 0.000000  loss: 3.7260 (3.7367)  loss_scale: 65536.0000 (54685.0357)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [26]  [1380/2809]  eta: 0:08:50  lr: 0.000015  min_lr: 0.000000  loss: 3.6843 (3.7367)  loss_scale: 65536.0000 (54763.6090)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [26]  [1390/2809]  eta: 0:08:46  lr: 0.000015  min_lr: 0.000000  loss: 3.7181 (3.7365)  loss_scale: 65536.0000 (54841.0525)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [26]  [1400/2809]  eta: 0:08:42  lr: 0.000015  min_lr: 0.000000  loss: 3.7019 (3.7360)  loss_scale: 65536.0000 (54917.3904)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [26]  [1410/2809]  eta: 0:08:39  lr: 0.000015  min_lr: 0.000000  loss: 3.8530 (3.7358)  loss_scale: 65536.0000 (54992.6464)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [26]  [1420/2809]  eta: 0:08:35  lr: 0.000015  min_lr: 0.000000  loss: 3.9020 (3.7363)  loss_scale: 65536.0000 (55066.8431)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [26]  [1430/2809]  eta: 0:08:31  lr: 0.000015  min_lr: 0.000000  loss: 3.6997 (3.7363)  loss_scale: 65536.0000 (55140.0028)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [26]  [1440/2809]  eta: 0:08:28  lr: 0.000015  min_lr: 0.000000  loss: 3.7660 (3.7371)  loss_scale: 65536.0000 (55212.1471)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [26]  [1450/2809]  eta: 0:08:24  lr: 0.000015  min_lr: 0.000000  loss: 3.8839 (3.7380)  loss_scale: 65536.0000 (55283.2970)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [26]  [1460/2809]  eta: 0:08:20  lr: 0.000015  min_lr: 0.000000  loss: 3.5780 (3.7372)  loss_scale: 65536.0000 (55353.4730)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
[2025-01-13 07:05:43,408] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:05:43,409] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [26]  [1470/2809]  eta: 0:08:16  lr: 0.000015  min_lr: 0.000000  loss: 3.6427 (3.7369)  loss_scale: 65536.0000 (55600.9028)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
[2025-01-13 07:05:45,254] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74506
[2025-01-13 07:05:45,254] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:05:45,255] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [1480/2809]  eta: 0:08:13  lr: 0.000015  min_lr: 0.000000  loss: 3.6549 (3.7367)  loss_scale: 65536.0000 (55712.2377)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [26]  [1490/2809]  eta: 0:08:09  lr: 0.000015  min_lr: 0.000000  loss: 3.6725 (3.7373)  loss_scale: 65536.0000 (55778.1247)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [26]  [1500/2809]  eta: 0:08:05  lr: 0.000015  min_lr: 0.000000  loss: 3.8285 (3.7373)  loss_scale: 65536.0000 (55843.1339)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [26]  [1510/2809]  eta: 0:08:02  lr: 0.000015  min_lr: 0.000000  loss: 3.6586 (3.7356)  loss_scale: 65536.0000 (55907.2826)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [26]  [1520/2809]  eta: 0:07:58  lr: 0.000015  min_lr: 0.000000  loss: 3.4532 (3.7345)  loss_scale: 65536.0000 (55970.5878)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [26]  [1530/2809]  eta: 0:07:54  lr: 0.000015  min_lr: 0.000000  loss: 3.6824 (3.7343)  loss_scale: 65536.0000 (56033.0660)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [26]  [1540/2809]  eta: 0:07:50  lr: 0.000015  min_lr: 0.000000  loss: 3.7685 (3.7354)  loss_scale: 65536.0000 (56094.7333)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [26]  [1550/2809]  eta: 0:07:47  lr: 0.000015  min_lr: 0.000000  loss: 3.9698 (3.7354)  loss_scale: 65536.0000 (56155.6054)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [26]  [1560/2809]  eta: 0:07:43  lr: 0.000015  min_lr: 0.000000  loss: 3.7515 (3.7361)  loss_scale: 65536.0000 (56215.6976)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [26]  [1570/2809]  eta: 0:07:39  lr: 0.000015  min_lr: 0.000000  loss: 3.9547 (3.7364)  loss_scale: 65536.0000 (56275.0248)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [26]  [1580/2809]  eta: 0:07:35  lr: 0.000015  min_lr: 0.000000  loss: 3.8205 (3.7358)  loss_scale: 65536.0000 (56333.6015)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
[2025-01-13 07:06:26,216] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74617
[2025-01-13 07:06:26,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:06:26,217] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1590/2809]  eta: 0:07:32  lr: 0.000015  min_lr: 0.000000  loss: 3.5295 (3.7344)  loss_scale: 65536.0000 (56226.6750)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [26]  [1600/2809]  eta: 0:07:28  lr: 0.000015  min_lr: 0.000000  loss: 3.6136 (3.7347)  loss_scale: 32768.0000 (56080.1499)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [26]  [1610/2809]  eta: 0:07:24  lr: 0.000015  min_lr: 0.000000  loss: 3.8812 (3.7362)  loss_scale: 32768.0000 (55935.4438)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [26]  [1620/2809]  eta: 0:07:21  lr: 0.000015  min_lr: 0.000000  loss: 4.0009 (3.7372)  loss_scale: 32768.0000 (55792.5231)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [26]  [1630/2809]  eta: 0:07:17  lr: 0.000015  min_lr: 0.000000  loss: 3.9659 (3.7380)  loss_scale: 32768.0000 (55651.3550)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [26]  [1640/2809]  eta: 0:07:13  lr: 0.000015  min_lr: 0.000000  loss: 3.8232 (3.7376)  loss_scale: 32768.0000 (55511.9074)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [26]  [1650/2809]  eta: 0:07:09  lr: 0.000015  min_lr: 0.000000  loss: 3.7871 (3.7374)  loss_scale: 32768.0000 (55374.1490)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [26]  [1660/2809]  eta: 0:07:06  lr: 0.000015  min_lr: 0.000000  loss: 3.8822 (3.7383)  loss_scale: 32768.0000 (55238.0494)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [26]  [1670/2809]  eta: 0:07:02  lr: 0.000015  min_lr: 0.000000  loss: 3.8822 (3.7374)  loss_scale: 32768.0000 (55103.5787)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [26]  [1680/2809]  eta: 0:06:58  lr: 0.000015  min_lr: 0.000000  loss: 3.7150 (3.7382)  loss_scale: 32768.0000 (54970.7079)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [26]  [1690/2809]  eta: 0:06:55  lr: 0.000015  min_lr: 0.000000  loss: 3.9314 (3.7391)  loss_scale: 32768.0000 (54839.4086)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [26]  [1700/2809]  eta: 0:06:51  lr: 0.000015  min_lr: 0.000000  loss: 3.8605 (3.7395)  loss_scale: 32768.0000 (54709.6531)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [26]  [1710/2809]  eta: 0:06:47  lr: 0.000015  min_lr: 0.000000  loss: 3.7345 (3.7396)  loss_scale: 32768.0000 (54581.4144)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
[2025-01-13 07:07:13,866] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:07:13,866] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1720/2809]  eta: 0:06:43  lr: 0.000015  min_lr: 0.000000  loss: 3.7348 (3.7396)  loss_scale: 32768.0000 (54626.0267)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [26]  [1730/2809]  eta: 0:06:40  lr: 0.000015  min_lr: 0.000000  loss: 3.7847 (3.7416)  loss_scale: 65536.0000 (54689.0537)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [26]  [1740/2809]  eta: 0:06:36  lr: 0.000015  min_lr: 0.000000  loss: 3.7722 (3.7401)  loss_scale: 65536.0000 (54751.3567)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [26]  [1750/2809]  eta: 0:06:32  lr: 0.000015  min_lr: 0.000000  loss: 3.6948 (3.7405)  loss_scale: 65536.0000 (54812.9480)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [26]  [1760/2809]  eta: 0:06:28  lr: 0.000015  min_lr: 0.000000  loss: 3.9132 (3.7411)  loss_scale: 65536.0000 (54873.8399)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
[2025-01-13 07:07:35,281] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74804
[2025-01-13 07:07:35,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:07:35,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1770/2809]  eta: 0:06:25  lr: 0.000015  min_lr: 0.000000  loss: 3.9146 (3.7426)  loss_scale: 65536.0000 (54915.5415)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [26]  [1780/2809]  eta: 0:06:21  lr: 0.000015  min_lr: 0.000000  loss: 4.0759 (3.7430)  loss_scale: 32768.0000 (54791.1870)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [26]  [1790/2809]  eta: 0:06:17  lr: 0.000015  min_lr: 0.000000  loss: 3.7609 (3.7424)  loss_scale: 32768.0000 (54668.2211)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [26]  [1800/2809]  eta: 0:06:14  lr: 0.000015  min_lr: 0.000000  loss: 3.7218 (3.7419)  loss_scale: 32768.0000 (54546.6208)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [26]  [1810/2809]  eta: 0:06:10  lr: 0.000015  min_lr: 0.000000  loss: 3.7611 (3.7422)  loss_scale: 32768.0000 (54426.3633)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [26]  [1820/2809]  eta: 0:06:06  lr: 0.000015  min_lr: 0.000000  loss: 3.9061 (3.7428)  loss_scale: 32768.0000 (54307.4267)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [26]  [1830/2809]  eta: 0:06:02  lr: 0.000015  min_lr: 0.000000  loss: 3.8284 (3.7428)  loss_scale: 32768.0000 (54189.7892)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [26]  [1840/2809]  eta: 0:05:59  lr: 0.000015  min_lr: 0.000000  loss: 3.7645 (3.7424)  loss_scale: 32768.0000 (54073.4297)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [26]  [1850/2809]  eta: 0:05:55  lr: 0.000015  min_lr: 0.000000  loss: 3.6623 (3.7415)  loss_scale: 32768.0000 (53958.3274)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0001  max mem: 15572
Epoch: [26]  [1860/2809]  eta: 0:05:51  lr: 0.000015  min_lr: 0.000000  loss: 3.8523 (3.7434)  loss_scale: 32768.0000 (53844.4621)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0002  max mem: 15572
Epoch: [26]  [1870/2809]  eta: 0:05:48  lr: 0.000015  min_lr: 0.000000  loss: 3.9462 (3.7432)  loss_scale: 32768.0000 (53731.8140)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [26]  [1880/2809]  eta: 0:05:44  lr: 0.000015  min_lr: 0.000000  loss: 3.8725 (3.7447)  loss_scale: 32768.0000 (53620.3636)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0001  max mem: 15572
Epoch: [26]  [1890/2809]  eta: 0:05:40  lr: 0.000015  min_lr: 0.000000  loss: 3.8725 (3.7446)  loss_scale: 32768.0000 (53510.0920)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
[2025-01-13 07:08:23,021] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:08:23,022] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1900/2809]  eta: 0:05:37  lr: 0.000015  min_lr: 0.000000  loss: 3.6753 (3.7448)  loss_scale: 32768.0000 (53435.4550)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [26]  [1910/2809]  eta: 0:05:33  lr: 0.000015  min_lr: 0.000000  loss: 3.7870 (3.7453)  loss_scale: 65536.0000 (53498.7755)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [26]  [1920/2809]  eta: 0:05:29  lr: 0.000015  min_lr: 0.000000  loss: 3.9863 (3.7458)  loss_scale: 65536.0000 (53561.4368)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [26]  [1930/2809]  eta: 0:05:25  lr: 0.000015  min_lr: 0.000000  loss: 3.7109 (3.7452)  loss_scale: 65536.0000 (53623.4490)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0003  max mem: 15572
Epoch: [26]  [1940/2809]  eta: 0:05:22  lr: 0.000015  min_lr: 0.000000  loss: 3.6010 (3.7451)  loss_scale: 65536.0000 (53684.8223)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [26]  [1950/2809]  eta: 0:05:18  lr: 0.000015  min_lr: 0.000000  loss: 3.7476 (3.7459)  loss_scale: 65536.0000 (53745.5664)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [26]  [1960/2809]  eta: 0:05:14  lr: 0.000015  min_lr: 0.000000  loss: 3.7476 (3.7461)  loss_scale: 65536.0000 (53805.6910)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
[2025-01-13 07:08:47,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=75000, skipped=508, lr=[1.4380800353752938e-07, 1.4380800353752938e-07, 2.0544000505361344e-07, 2.0544000505361344e-07, 2.934857215051621e-07, 2.934857215051621e-07, 4.1926531643594586e-07, 4.1926531643594586e-07, 5.989504520513512e-07, 5.989504520513512e-07, 8.556435029305019e-07, 8.556435029305019e-07, 1.2223478613292883e-06, 1.2223478613292883e-06, 1.7462112304704122e-06, 1.7462112304704122e-06, 2.4945874721005886e-06, 2.4945874721005886e-06, 3.5636963887151274e-06, 3.5636963887151274e-06, 5.090994841021611e-06, 5.090994841021611e-06, 7.2728497728880156e-06, 7.2728497728880156e-06, 1.0389785389840023e-05, 1.0389785389840023e-05, 1.484255055691432e-05, 1.484255055691432e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 07:08:47,437] [INFO] [timer.py:260:stop] epoch=0/micro_step=75000/global_step=75000, RunningAvgSamplesPerSec=30.625053540531486, CurrSamplesPerSec=34.9204259417036, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [26]  [1970/2809]  eta: 0:05:11  lr: 0.000015  min_lr: 0.000000  loss: 3.6674 (3.7446)  loss_scale: 65536.0000 (53865.2055)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [26]  [1980/2809]  eta: 0:05:07  lr: 0.000015  min_lr: 0.000000  loss: 3.6447 (3.7439)  loss_scale: 65536.0000 (53924.1191)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [26]  [1990/2809]  eta: 0:05:03  lr: 0.000015  min_lr: 0.000000  loss: 3.6681 (3.7441)  loss_scale: 65536.0000 (53982.4410)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [26]  [2000/2809]  eta: 0:04:59  lr: 0.000015  min_lr: 0.000000  loss: 3.6948 (3.7443)  loss_scale: 65536.0000 (54040.1799)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [26]  [2010/2809]  eta: 0:04:56  lr: 0.000015  min_lr: 0.000000  loss: 3.7514 (3.7450)  loss_scale: 65536.0000 (54097.3446)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0002  max mem: 15572
Epoch: [26]  [2020/2809]  eta: 0:04:52  lr: 0.000015  min_lr: 0.000000  loss: 3.8994 (3.7447)  loss_scale: 65536.0000 (54153.9436)  weight_decay: 0.0500 (0.0500)  time: 0.3653  data: 0.0002  max mem: 15572
[2025-01-13 07:09:10,250] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:09:10,250] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:09:10,615] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75062
[2025-01-13 07:09:10,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:09:10,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [2030/2809]  eta: 0:04:48  lr: 0.000015  min_lr: 0.000000  loss: 3.7074 (3.7450)  loss_scale: 65536.0000 (54242.2531)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [26]  [2040/2809]  eta: 0:04:45  lr: 0.000015  min_lr: 0.000000  loss: 3.6822 (3.7445)  loss_scale: 65536.0000 (54297.5875)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
Epoch: [26]  [2050/2809]  eta: 0:04:41  lr: 0.000015  min_lr: 0.000000  loss: 3.6780 (3.7440)  loss_scale: 65536.0000 (54352.3823)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [26]  [2060/2809]  eta: 0:04:37  lr: 0.000015  min_lr: 0.000000  loss: 3.7218 (3.7442)  loss_scale: 65536.0000 (54406.6453)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0002  max mem: 15572
Epoch: [26]  [2070/2809]  eta: 0:04:33  lr: 0.000015  min_lr: 0.000000  loss: 3.8149 (3.7442)  loss_scale: 65536.0000 (54460.3844)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [26]  [2080/2809]  eta: 0:04:30  lr: 0.000015  min_lr: 0.000000  loss: 3.8659 (3.7445)  loss_scale: 65536.0000 (54513.6069)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [26]  [2090/2809]  eta: 0:04:26  lr: 0.000015  min_lr: 0.000000  loss: 3.8648 (3.7449)  loss_scale: 65536.0000 (54566.3204)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [26]  [2100/2809]  eta: 0:04:22  lr: 0.000015  min_lr: 0.000000  loss: 3.6868 (3.7444)  loss_scale: 65536.0000 (54618.5321)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [26]  [2110/2809]  eta: 0:04:19  lr: 0.000015  min_lr: 0.000000  loss: 3.5411 (3.7425)  loss_scale: 65536.0000 (54670.2492)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [26]  [2120/2809]  eta: 0:04:15  lr: 0.000015  min_lr: 0.000000  loss: 3.6832 (3.7427)  loss_scale: 65536.0000 (54721.4785)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [26]  [2130/2809]  eta: 0:04:11  lr: 0.000015  min_lr: 0.000000  loss: 3.7520 (3.7430)  loss_scale: 65536.0000 (54772.2271)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [26]  [2140/2809]  eta: 0:04:07  lr: 0.000015  min_lr: 0.000000  loss: 3.6977 (3.7429)  loss_scale: 65536.0000 (54822.5016)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [26]  [2150/2809]  eta: 0:04:04  lr: 0.000015  min_lr: 0.000000  loss: 3.6977 (3.7433)  loss_scale: 65536.0000 (54872.3087)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
[2025-01-13 07:09:58,393] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:09:58,393] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [26]  [2160/2809]  eta: 0:04:00  lr: 0.000015  min_lr: 0.000000  loss: 3.7260 (3.7429)  loss_scale: 65536.0000 (55042.9616)  weight_decay: 0.0500 (0.0500)  time: 0.3645  data: 0.0002  max mem: 15572
[2025-01-13 07:10:00,977] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75198
[2025-01-13 07:10:00,977] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:10:00,978] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [2170/2809]  eta: 0:03:56  lr: 0.000015  min_lr: 0.000000  loss: 3.7100 (3.7426)  loss_scale: 65536.0000 (55181.8554)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [26]  [2180/2809]  eta: 0:03:53  lr: 0.000015  min_lr: 0.000000  loss: 3.7397 (3.7426)  loss_scale: 65536.0000 (55229.3297)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [26]  [2190/2809]  eta: 0:03:49  lr: 0.000015  min_lr: 0.000000  loss: 3.8639 (3.7438)  loss_scale: 65536.0000 (55276.3706)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [26]  [2200/2809]  eta: 0:03:45  lr: 0.000015  min_lr: 0.000000  loss: 4.0411 (3.7451)  loss_scale: 65536.0000 (55322.9841)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [26]  [2210/2809]  eta: 0:03:41  lr: 0.000015  min_lr: 0.000000  loss: 3.9381 (3.7448)  loss_scale: 65536.0000 (55369.1759)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [26]  [2220/2809]  eta: 0:03:38  lr: 0.000015  min_lr: 0.000000  loss: 3.7142 (3.7449)  loss_scale: 65536.0000 (55414.9518)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [26]  [2230/2809]  eta: 0:03:34  lr: 0.000015  min_lr: 0.000000  loss: 3.6660 (3.7437)  loss_scale: 65536.0000 (55460.3173)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [26]  [2240/2809]  eta: 0:03:30  lr: 0.000015  min_lr: 0.000000  loss: 3.6510 (3.7441)  loss_scale: 65536.0000 (55505.2780)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [26]  [2250/2809]  eta: 0:03:27  lr: 0.000015  min_lr: 0.000000  loss: 3.7435 (3.7443)  loss_scale: 65536.0000 (55549.8392)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [26]  [2260/2809]  eta: 0:03:23  lr: 0.000015  min_lr: 0.000000  loss: 3.7435 (3.7439)  loss_scale: 65536.0000 (55594.0062)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
[2025-01-13 07:10:39,493] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75302
[2025-01-13 07:10:39,493] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:10:39,493] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [2270/2809]  eta: 0:03:19  lr: 0.000015  min_lr: 0.000000  loss: 3.7728 (3.7448)  loss_scale: 65536.0000 (55594.4976)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [26]  [2280/2809]  eta: 0:03:16  lr: 0.000015  min_lr: 0.000000  loss: 3.8314 (3.7441)  loss_scale: 32768.0000 (55494.4253)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [26]  [2290/2809]  eta: 0:03:12  lr: 0.000015  min_lr: 0.000000  loss: 3.8614 (3.7449)  loss_scale: 32768.0000 (55395.2265)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [26]  [2300/2809]  eta: 0:03:08  lr: 0.000015  min_lr: 0.000000  loss: 3.8089 (3.7445)  loss_scale: 32768.0000 (55296.8900)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [26]  [2310/2809]  eta: 0:03:04  lr: 0.000015  min_lr: 0.000000  loss: 3.6279 (3.7441)  loss_scale: 32768.0000 (55199.4046)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [26]  [2320/2809]  eta: 0:03:01  lr: 0.000015  min_lr: 0.000000  loss: 3.5546 (3.7432)  loss_scale: 32768.0000 (55102.7592)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [26]  [2330/2809]  eta: 0:02:57  lr: 0.000015  min_lr: 0.000000  loss: 3.5546 (3.7431)  loss_scale: 32768.0000 (55006.9429)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
Epoch: [26]  [2340/2809]  eta: 0:02:53  lr: 0.000015  min_lr: 0.000000  loss: 3.6489 (3.7420)  loss_scale: 32768.0000 (54911.9453)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [26]  [2350/2809]  eta: 0:02:50  lr: 0.000015  min_lr: 0.000000  loss: 3.5070 (3.7412)  loss_scale: 32768.0000 (54817.7558)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [26]  [2360/2809]  eta: 0:02:46  lr: 0.000015  min_lr: 0.000000  loss: 3.5105 (3.7409)  loss_scale: 32768.0000 (54724.3643)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [26]  [2370/2809]  eta: 0:02:42  lr: 0.000015  min_lr: 0.000000  loss: 3.7751 (3.7414)  loss_scale: 32768.0000 (54631.7604)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [26]  [2380/2809]  eta: 0:02:38  lr: 0.000015  min_lr: 0.000000  loss: 3.8178 (3.7416)  loss_scale: 32768.0000 (54539.9345)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [26]  [2390/2809]  eta: 0:02:35  lr: 0.000015  min_lr: 0.000000  loss: 3.7758 (3.7420)  loss_scale: 32768.0000 (54448.8766)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
[2025-01-13 07:11:27,143] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:11:27,144] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [2400/2809]  eta: 0:02:31  lr: 0.000015  min_lr: 0.000000  loss: 3.6792 (3.7412)  loss_scale: 32768.0000 (54413.1678)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [26]  [2410/2809]  eta: 0:02:27  lr: 0.000015  min_lr: 0.000000  loss: 3.5168 (3.7412)  loss_scale: 65536.0000 (54459.3015)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0016  max mem: 15572
Epoch: [26]  [2420/2809]  eta: 0:02:24  lr: 0.000015  min_lr: 0.000000  loss: 3.7621 (3.7411)  loss_scale: 65536.0000 (54505.0541)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0016  max mem: 15572
Epoch: [26]  [2430/2809]  eta: 0:02:20  lr: 0.000015  min_lr: 0.000000  loss: 4.0164 (3.7420)  loss_scale: 65536.0000 (54550.4303)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [26]  [2440/2809]  eta: 0:02:16  lr: 0.000015  min_lr: 0.000000  loss: 4.0164 (3.7423)  loss_scale: 65536.0000 (54595.4347)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [26]  [2450/2809]  eta: 0:02:13  lr: 0.000015  min_lr: 0.000000  loss: 3.6703 (3.7419)  loss_scale: 65536.0000 (54640.0718)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [26]  [2460/2809]  eta: 0:02:09  lr: 0.000014  min_lr: 0.000000  loss: 3.8363 (3.7425)  loss_scale: 65536.0000 (54684.3462)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [26]  [2470/2809]  eta: 0:02:05  lr: 0.000014  min_lr: 0.000000  loss: 3.9289 (3.7425)  loss_scale: 65536.0000 (54728.2622)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [26]  [2480/2809]  eta: 0:02:01  lr: 0.000014  min_lr: 0.000000  loss: 3.9289 (3.7432)  loss_scale: 65536.0000 (54771.8243)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [26]  [2490/2809]  eta: 0:01:58  lr: 0.000014  min_lr: 0.000000  loss: 3.8929 (3.7442)  loss_scale: 65536.0000 (54815.0365)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [26]  [2500/2809]  eta: 0:01:54  lr: 0.000014  min_lr: 0.000000  loss: 3.8822 (3.7444)  loss_scale: 65536.0000 (54857.9032)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [26]  [2510/2809]  eta: 0:01:50  lr: 0.000014  min_lr: 0.000000  loss: 3.6452 (3.7435)  loss_scale: 65536.0000 (54900.4285)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [26]  [2520/2809]  eta: 0:01:47  lr: 0.000014  min_lr: 0.000000  loss: 3.6452 (3.7432)  loss_scale: 65536.0000 (54942.6164)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
[2025-01-13 07:12:14,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:12:14,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:12:14,718] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75560
[2025-01-13 07:12:14,718] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:12:14,718] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [2530/2809]  eta: 0:01:43  lr: 0.000014  min_lr: 0.000000  loss: 3.9981 (3.7439)  loss_scale: 65536.0000 (55010.3643)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [26]  [2540/2809]  eta: 0:01:39  lr: 0.000014  min_lr: 0.000000  loss: 3.9996 (3.7441)  loss_scale: 65536.0000 (55051.7875)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
[2025-01-13 07:12:22,475] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75581
[2025-01-13 07:12:22,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:12:22,475] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [2550/2809]  eta: 0:01:35  lr: 0.000014  min_lr: 0.000000  loss: 3.8777 (3.7450)  loss_scale: 65536.0000 (55041.5053)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [26]  [2560/2809]  eta: 0:01:32  lr: 0.000014  min_lr: 0.000000  loss: 3.9847 (3.7459)  loss_scale: 32768.0000 (54954.5334)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [26]  [2570/2809]  eta: 0:01:28  lr: 0.000014  min_lr: 0.000000  loss: 3.8137 (3.7461)  loss_scale: 32768.0000 (54868.2380)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [26]  [2580/2809]  eta: 0:01:24  lr: 0.000014  min_lr: 0.000000  loss: 3.8399 (3.7467)  loss_scale: 32768.0000 (54782.6114)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [26]  [2590/2809]  eta: 0:01:21  lr: 0.000014  min_lr: 0.000000  loss: 3.7889 (3.7471)  loss_scale: 32768.0000 (54697.6457)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [26]  [2600/2809]  eta: 0:01:17  lr: 0.000014  min_lr: 0.000000  loss: 3.6111 (3.7465)  loss_scale: 32768.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [26]  [2610/2809]  eta: 0:01:13  lr: 0.000014  min_lr: 0.000000  loss: 3.6266 (3.7468)  loss_scale: 32768.0000 (54529.6668)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [26]  [2620/2809]  eta: 0:01:10  lr: 0.000014  min_lr: 0.000000  loss: 3.7351 (3.7469)  loss_scale: 32768.0000 (54446.6387)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [26]  [2630/2809]  eta: 0:01:06  lr: 0.000014  min_lr: 0.000000  loss: 3.8521 (3.7471)  loss_scale: 32768.0000 (54364.2417)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [26]  [2640/2809]  eta: 0:01:02  lr: 0.000014  min_lr: 0.000000  loss: 3.9572 (3.7480)  loss_scale: 32768.0000 (54282.4688)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0001  max mem: 15572
Epoch: [26]  [2650/2809]  eta: 0:00:58  lr: 0.000014  min_lr: 0.000000  loss: 3.8923 (3.7477)  loss_scale: 32768.0000 (54201.3127)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [26]  [2660/2809]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 3.5811 (3.7471)  loss_scale: 32768.0000 (54120.7666)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [26]  [2670/2809]  eta: 0:00:51  lr: 0.000014  min_lr: 0.000000  loss: 3.7131 (3.7476)  loss_scale: 32768.0000 (54040.8237)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
[2025-01-13 07:13:10,256] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:13:10,256] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [2680/2809]  eta: 0:00:47  lr: 0.000014  min_lr: 0.000000  loss: 3.8174 (3.7476)  loss_scale: 32768.0000 (54022.5886)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [26]  [2690/2809]  eta: 0:00:44  lr: 0.000014  min_lr: 0.000000  loss: 3.9061 (3.7483)  loss_scale: 65536.0000 (54065.3735)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [26]  [2700/2809]  eta: 0:00:40  lr: 0.000014  min_lr: 0.000000  loss: 3.8646 (3.7480)  loss_scale: 65536.0000 (54107.8415)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [26]  [2710/2809]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 3.8646 (3.7489)  loss_scale: 65536.0000 (54149.9963)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [26]  [2720/2809]  eta: 0:00:32  lr: 0.000014  min_lr: 0.000000  loss: 3.7197 (3.7484)  loss_scale: 65536.0000 (54191.8412)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [26]  [2730/2809]  eta: 0:00:29  lr: 0.000014  min_lr: 0.000000  loss: 3.7197 (3.7486)  loss_scale: 65536.0000 (54233.3797)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [26]  [2740/2809]  eta: 0:00:25  lr: 0.000014  min_lr: 0.000000  loss: 3.7542 (3.7484)  loss_scale: 65536.0000 (54274.6151)  weight_decay: 0.0500 (0.0500)  time: 0.3780  data: 0.0002  max mem: 15572
Epoch: [26]  [2750/2809]  eta: 0:00:21  lr: 0.000014  min_lr: 0.000000  loss: 3.8091 (3.7488)  loss_scale: 65536.0000 (54315.5507)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [26]  [2760/2809]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 3.8345 (3.7485)  loss_scale: 65536.0000 (54356.1898)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [26]  [2770/2809]  eta: 0:00:14  lr: 0.000014  min_lr: 0.000000  loss: 3.8601 (3.7492)  loss_scale: 65536.0000 (54396.5355)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [26]  [2780/2809]  eta: 0:00:10  lr: 0.000014  min_lr: 0.000000  loss: 3.9411 (3.7489)  loss_scale: 65536.0000 (54436.5912)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [26]  [2790/2809]  eta: 0:00:07  lr: 0.000014  min_lr: 0.000000  loss: 3.5501 (3.7482)  loss_scale: 65536.0000 (54476.3597)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [26]  [2800/2809]  eta: 0:00:03  lr: 0.000014  min_lr: 0.000000  loss: 3.7177 (3.7479)  loss_scale: 65536.0000 (54515.8443)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0001  max mem: 15572
[2025-01-13 07:13:57,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:13:57,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [26]  [2808/2809]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 3.7220 (3.7476)  loss_scale: 65536.0000 (54663.8832)  weight_decay: 0.0500 (0.0500)  time: 0.3611  data: 0.0001  max mem: 15572
Epoch: [26] Total time: 0:17:20 (0.3706 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 3.7220 (3.7476)  loss_scale: 65536.0000 (54663.8832)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:08:37  loss: 0.3986 (0.3986)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.9039  data: 1.7287  max mem: 15572
Val:  [ 10/272]  eta: 0:01:43  loss: 2.5307 (2.4060)  acc1: 27.7778 (36.8687)  acc5: 72.2222 (70.7071)  time: 0.3944  data: 0.2351  max mem: 15572
Val:  [ 20/272]  eta: 0:01:11  loss: 2.5103 (2.4104)  acc1: 38.8889 (42.3280)  acc5: 72.2222 (72.2222)  time: 0.2023  data: 0.0431  max mem: 15572
Val:  [ 30/272]  eta: 0:00:58  loss: 2.3891 (2.5072)  acc1: 38.8889 (39.0681)  acc5: 72.2222 (71.5054)  time: 0.1581  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:51  loss: 2.5598 (2.5330)  acc1: 22.2222 (36.4499)  acc5: 77.7778 (72.0867)  time: 0.1544  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:46  loss: 2.4658 (2.4381)  acc1: 33.3333 (39.4336)  acc5: 77.7778 (74.0741)  time: 0.1554  data: 0.0003  max mem: 15572
Val:  [ 60/272]  eta: 0:00:42  loss: 1.4517 (2.3110)  acc1: 66.6667 (42.9872)  acc5: 88.8889 (75.4098)  time: 0.1533  data: 0.0003  max mem: 15572
Val:  [ 70/272]  eta: 0:00:39  loss: 1.4173 (2.2118)  acc1: 66.6667 (46.4006)  acc5: 88.8889 (76.6823)  time: 0.1692  data: 0.0133  max mem: 15572
Val:  [ 80/272]  eta: 0:00:36  loss: 1.8352 (2.2338)  acc1: 61.1111 (46.2277)  acc5: 77.7778 (75.9945)  time: 0.1730  data: 0.0133  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.2399 (2.2346)  acc1: 50.0000 (47.0085)  acc5: 77.7778 (76.7399)  time: 0.1545  data: 0.0003  max mem: 15572
Val:  [100/272]  eta: 0:00:31  loss: 2.0928 (2.2612)  acc1: 50.0000 (46.4246)  acc5: 77.7778 (76.1826)  time: 0.1554  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:29  loss: 2.6393 (2.3402)  acc1: 22.2222 (44.3443)  acc5: 66.6667 (74.7748)  time: 0.1597  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 2.9650 (2.3750)  acc1: 22.2222 (43.4803)  acc5: 66.6667 (74.0588)  time: 0.1647  data: 0.0047  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.1657 (2.3377)  acc1: 44.4444 (44.5717)  acc5: 77.7778 (74.9788)  time: 0.1650  data: 0.0047  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.6607 (2.3282)  acc1: 61.1111 (44.9567)  acc5: 88.8889 (74.9015)  time: 0.1572  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.3102 (2.3302)  acc1: 38.8889 (44.5180)  acc5: 77.7778 (75.2024)  time: 0.1549  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.3102 (2.3204)  acc1: 44.4444 (45.1691)  acc5: 77.7778 (75.5003)  time: 0.1548  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.3578 (2.3368)  acc1: 44.4444 (44.7044)  acc5: 72.2222 (75.1787)  time: 0.1561  data: 0.0005  max mem: 15572
Val:  [180/272]  eta: 0:00:15  loss: 2.3208 (2.3274)  acc1: 33.3333 (44.5979)  acc5: 72.2222 (75.5985)  time: 0.1619  data: 0.0005  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.5234 (2.3839)  acc1: 33.3333 (43.3973)  acc5: 72.2222 (74.1710)  time: 0.1566  data: 0.0003  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.6967 (2.3964)  acc1: 33.3333 (42.9795)  acc5: 61.1111 (73.9082)  time: 0.1528  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.3299 (2.4003)  acc1: 38.8889 (43.1543)  acc5: 77.7778 (73.8283)  time: 0.1536  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.4663 (2.3898)  acc1: 38.8889 (43.4389)  acc5: 77.7778 (73.9819)  time: 0.1513  data: 0.0003  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.8210 (2.3655)  acc1: 55.5556 (44.3242)  acc5: 83.3333 (74.3146)  time: 0.1604  data: 0.0070  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.7489 (2.3527)  acc1: 55.5556 (44.5136)  acc5: 83.3333 (74.5735)  time: 0.1757  data: 0.0143  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.3745 (2.3657)  acc1: 38.8889 (43.9132)  acc5: 77.7778 (74.4799)  time: 0.1758  data: 0.0077  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.2149 (2.3065)  acc1: 72.2222 (45.6577)  acc5: 83.3333 (75.2235)  time: 0.1612  data: 0.0058  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4786 (2.3058)  acc1: 72.2222 (45.5925)  acc5: 88.8889 (75.3588)  time: 0.1436  data: 0.0056  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4786 (2.3104)  acc1: 55.5556 (45.5663)  acc5: 88.8889 (75.3225)  time: 0.1370  data: 0.0052  max mem: 15572
Val: Total time: 0:00:45 (0.1684 s / it)
* Acc@1 45.566 Acc@5 75.323 loss 2.310
Accuracy of the network on the 4883 val videos: 45.6%
[2025-01-13 07:14:45,133] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 07:14:45,134] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 07:14:45,134] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 07:14:47,565] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 07:14:47,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.57%
Epoch: [27]  [   0/2809]  eta: 2:43:09  lr: 0.000014  min_lr: 0.000000  loss: 3.2758 (3.2758)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 3.4849  data: 3.0630  max mem: 15572
[2025-01-13 07:14:52,269] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75846
[2025-01-13 07:14:52,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:14:52,270] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [  10/2809]  eta: 0:31:13  lr: 0.000014  min_lr: 0.000000  loss: 3.7727 (3.7288)  loss_scale: 65536.0000 (83409.4545)  weight_decay: 0.0500 (0.0500)  time: 0.6693  data: 0.2869  max mem: 15572
Epoch: [27]  [  20/2809]  eta: 0:24:39  lr: 0.000014  min_lr: 0.000000  loss: 3.7627 (3.7155)  loss_scale: 65536.0000 (74898.2857)  weight_decay: 0.0500 (0.0500)  time: 0.3828  data: 0.0047  max mem: 15572
Epoch: [27]  [  30/2809]  eta: 0:22:14  lr: 0.000014  min_lr: 0.000000  loss: 3.7627 (3.7055)  loss_scale: 65536.0000 (71878.1935)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [27]  [  40/2809]  eta: 0:20:55  lr: 0.000014  min_lr: 0.000000  loss: 3.7289 (3.7010)  loss_scale: 65536.0000 (70331.3171)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [27]  [  50/2809]  eta: 0:20:05  lr: 0.000014  min_lr: 0.000000  loss: 3.7106 (3.7147)  loss_scale: 65536.0000 (69391.0588)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [27]  [  60/2809]  eta: 0:19:30  lr: 0.000014  min_lr: 0.000000  loss: 3.7526 (3.7133)  loss_scale: 65536.0000 (68759.0820)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [27]  [  70/2809]  eta: 0:19:04  lr: 0.000014  min_lr: 0.000000  loss: 3.7099 (3.6990)  loss_scale: 65536.0000 (68305.1268)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [27]  [  80/2809]  eta: 0:18:43  lr: 0.000014  min_lr: 0.000000  loss: 3.7891 (3.7191)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [27]  [  90/2809]  eta: 0:18:25  lr: 0.000014  min_lr: 0.000000  loss: 3.7891 (3.6981)  loss_scale: 65536.0000 (67696.5275)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [27]  [ 100/2809]  eta: 0:18:10  lr: 0.000014  min_lr: 0.000000  loss: 3.7784 (3.7157)  loss_scale: 65536.0000 (67482.6139)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [27]  [ 110/2809]  eta: 0:17:58  lr: 0.000014  min_lr: 0.000000  loss: 3.6671 (3.7076)  loss_scale: 65536.0000 (67307.2432)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [27]  [ 120/2809]  eta: 0:17:49  lr: 0.000014  min_lr: 0.000000  loss: 3.3338 (3.6803)  loss_scale: 65536.0000 (67160.8595)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [27]  [ 130/2809]  eta: 0:17:39  lr: 0.000014  min_lr: 0.000000  loss: 3.6329 (3.6937)  loss_scale: 65536.0000 (67036.8244)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
[2025-01-13 07:15:40,169] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:15:40,169] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:15:42,725] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75982
[2025-01-13 07:15:42,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:15:42,725] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 140/2809]  eta: 0:17:30  lr: 0.000014  min_lr: 0.000000  loss: 3.7036 (3.6856)  loss_scale: 65536.0000 (70183.9433)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [27]  [ 150/2809]  eta: 0:17:22  lr: 0.000014  min_lr: 0.000000  loss: 3.5733 (3.6711)  loss_scale: 65536.0000 (69876.1325)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 07:15:49,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=76000, skipped=515, lr=[1.3711166685044318e-07, 1.3711166685044318e-07, 1.9587380978634742e-07, 1.9587380978634742e-07, 2.7981972826621064e-07, 2.7981972826621064e-07, 3.997424689517295e-07, 3.997424689517295e-07, 5.710606699310422e-07, 5.710606699310422e-07, 8.15800957044346e-07, 8.15800957044346e-07, 1.1654299386347801e-06, 1.1654299386347801e-06, 1.6648999123354003e-06, 1.6648999123354003e-06, 2.378428446193429e-06, 2.378428446193429e-06, 3.3977549231334703e-06, 3.3977549231334703e-06, 4.853935604476386e-06, 4.853935604476386e-06, 6.9341937206805526e-06, 6.9341937206805526e-06, 9.905991029543647e-06, 9.905991029543647e-06, 1.4151415756490925e-05, 1.4151415756490925e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 07:15:49,027] [INFO] [timer.py:260:stop] epoch=0/micro_step=76000/global_step=76000, RunningAvgSamplesPerSec=30.66828718039706, CurrSamplesPerSec=33.4329811239039, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [27]  [ 160/2809]  eta: 0:17:15  lr: 0.000014  min_lr: 0.000000  loss: 3.6065 (3.6870)  loss_scale: 65536.0000 (69606.5590)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [27]  [ 170/2809]  eta: 0:17:08  lr: 0.000014  min_lr: 0.000000  loss: 3.7270 (3.6757)  loss_scale: 65536.0000 (69368.5146)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [27]  [ 180/2809]  eta: 0:17:01  lr: 0.000014  min_lr: 0.000000  loss: 3.4134 (3.6736)  loss_scale: 65536.0000 (69156.7735)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [27]  [ 190/2809]  eta: 0:16:54  lr: 0.000014  min_lr: 0.000000  loss: 3.7996 (3.6779)  loss_scale: 65536.0000 (68967.2042)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [27]  [ 200/2809]  eta: 0:16:48  lr: 0.000014  min_lr: 0.000000  loss: 3.6586 (3.6740)  loss_scale: 65536.0000 (68796.4975)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [27]  [ 210/2809]  eta: 0:16:42  lr: 0.000014  min_lr: 0.000000  loss: 3.4780 (3.6610)  loss_scale: 65536.0000 (68641.9716)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [27]  [ 220/2809]  eta: 0:16:36  lr: 0.000014  min_lr: 0.000000  loss: 3.4780 (3.6617)  loss_scale: 65536.0000 (68501.4299)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [27]  [ 230/2809]  eta: 0:16:31  lr: 0.000014  min_lr: 0.000000  loss: 3.7411 (3.6665)  loss_scale: 65536.0000 (68373.0563)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [27]  [ 240/2809]  eta: 0:16:25  lr: 0.000014  min_lr: 0.000000  loss: 3.7411 (3.6735)  loss_scale: 65536.0000 (68255.3361)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [27]  [ 250/2809]  eta: 0:16:21  lr: 0.000014  min_lr: 0.000000  loss: 3.7950 (3.6808)  loss_scale: 65536.0000 (68146.9960)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [27]  [ 260/2809]  eta: 0:16:15  lr: 0.000014  min_lr: 0.000000  loss: 3.7859 (3.6816)  loss_scale: 65536.0000 (68046.9579)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
[2025-01-13 07:16:30,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:16:30,462] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:16:31,198] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76113
[2025-01-13 07:16:31,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:16:31,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 270/2809]  eta: 0:16:10  lr: 0.000014  min_lr: 0.000000  loss: 3.6952 (3.6894)  loss_scale: 65536.0000 (68437.9631)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [27]  [ 280/2809]  eta: 0:16:05  lr: 0.000014  min_lr: 0.000000  loss: 3.7119 (3.6933)  loss_scale: 65536.0000 (68334.6904)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [27]  [ 290/2809]  eta: 0:16:00  lr: 0.000014  min_lr: 0.000000  loss: 3.7119 (3.6930)  loss_scale: 65536.0000 (68238.5155)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [27]  [ 300/2809]  eta: 0:15:56  lr: 0.000014  min_lr: 0.000000  loss: 3.6141 (3.6894)  loss_scale: 65536.0000 (68148.7309)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [27]  [ 310/2809]  eta: 0:15:51  lr: 0.000014  min_lr: 0.000000  loss: 3.6141 (3.6877)  loss_scale: 65536.0000 (68064.7203)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [27]  [ 320/2809]  eta: 0:15:46  lr: 0.000014  min_lr: 0.000000  loss: 3.5662 (3.6796)  loss_scale: 65536.0000 (67985.9439)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [27]  [ 330/2809]  eta: 0:15:41  lr: 0.000014  min_lr: 0.000000  loss: 3.5084 (3.6832)  loss_scale: 65536.0000 (67911.9275)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [27]  [ 340/2809]  eta: 0:15:37  lr: 0.000014  min_lr: 0.000000  loss: 3.5225 (3.6868)  loss_scale: 65536.0000 (67842.2522)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [27]  [ 350/2809]  eta: 0:15:33  lr: 0.000014  min_lr: 0.000000  loss: 3.8698 (3.6892)  loss_scale: 65536.0000 (67776.5470)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [27]  [ 360/2809]  eta: 0:15:28  lr: 0.000014  min_lr: 0.000000  loss: 3.8698 (3.6914)  loss_scale: 65536.0000 (67714.4820)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [27]  [ 370/2809]  eta: 0:15:24  lr: 0.000014  min_lr: 0.000000  loss: 3.6376 (3.6848)  loss_scale: 65536.0000 (67655.7628)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [27]  [ 380/2809]  eta: 0:15:19  lr: 0.000014  min_lr: 0.000000  loss: 3.7180 (3.6934)  loss_scale: 65536.0000 (67600.1260)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [27]  [ 390/2809]  eta: 0:15:15  lr: 0.000014  min_lr: 0.000000  loss: 4.0240 (3.6921)  loss_scale: 65536.0000 (67547.3350)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
[2025-01-13 07:17:18,916] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:17:18,916] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [27]  [ 400/2809]  eta: 0:15:11  lr: 0.000014  min_lr: 0.000000  loss: 3.6172 (3.6896)  loss_scale: 65536.0000 (67824.0399)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
[2025-01-13 07:17:20,027] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76245
[2025-01-13 07:17:20,027] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:17:20,027] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 410/2809]  eta: 0:15:06  lr: 0.000014  min_lr: 0.000000  loss: 3.5521 (3.6909)  loss_scale: 65536.0000 (67927.8248)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [27]  [ 420/2809]  eta: 0:15:02  lr: 0.000014  min_lr: 0.000000  loss: 3.7393 (3.6896)  loss_scale: 65536.0000 (67871.0119)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [27]  [ 430/2809]  eta: 0:14:58  lr: 0.000014  min_lr: 0.000000  loss: 3.7730 (3.6900)  loss_scale: 65536.0000 (67816.8353)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [27]  [ 440/2809]  eta: 0:14:54  lr: 0.000014  min_lr: 0.000000  loss: 3.7811 (3.6953)  loss_scale: 65536.0000 (67765.1156)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [27]  [ 450/2809]  eta: 0:14:50  lr: 0.000014  min_lr: 0.000000  loss: 3.9708 (3.6973)  loss_scale: 65536.0000 (67715.6896)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [27]  [ 460/2809]  eta: 0:14:45  lr: 0.000014  min_lr: 0.000000  loss: 3.7240 (3.6976)  loss_scale: 65536.0000 (67668.4078)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0001  max mem: 15572
Epoch: [27]  [ 470/2809]  eta: 0:14:41  lr: 0.000014  min_lr: 0.000000  loss: 3.6763 (3.6967)  loss_scale: 65536.0000 (67623.1338)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [27]  [ 480/2809]  eta: 0:14:37  lr: 0.000014  min_lr: 0.000000  loss: 3.5002 (3.6943)  loss_scale: 65536.0000 (67579.7422)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [27]  [ 490/2809]  eta: 0:14:33  lr: 0.000014  min_lr: 0.000000  loss: 3.7279 (3.6955)  loss_scale: 65536.0000 (67538.1181)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [27]  [ 500/2809]  eta: 0:14:29  lr: 0.000014  min_lr: 0.000000  loss: 3.7342 (3.6937)  loss_scale: 65536.0000 (67498.1557)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [27]  [ 510/2809]  eta: 0:14:25  lr: 0.000014  min_lr: 0.000000  loss: 3.7186 (3.6946)  loss_scale: 65536.0000 (67459.7573)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [27]  [ 520/2809]  eta: 0:14:21  lr: 0.000014  min_lr: 0.000000  loss: 3.7186 (3.6946)  loss_scale: 65536.0000 (67422.8330)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [27]  [ 530/2809]  eta: 0:14:17  lr: 0.000014  min_lr: 0.000000  loss: 3.5279 (3.6897)  loss_scale: 65536.0000 (67387.2994)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
[2025-01-13 07:18:07,783] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:18:07,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:18:08,514] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76376
[2025-01-13 07:18:08,515] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:18:08,515] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 540/2809]  eta: 0:14:13  lr: 0.000014  min_lr: 0.000000  loss: 3.5279 (3.6920)  loss_scale: 65536.0000 (67595.3567)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [27]  [ 550/2809]  eta: 0:14:09  lr: 0.000014  min_lr: 0.000000  loss: 3.6271 (3.6906)  loss_scale: 65536.0000 (67557.9819)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [27]  [ 560/2809]  eta: 0:14:05  lr: 0.000014  min_lr: 0.000000  loss: 3.6271 (3.6922)  loss_scale: 65536.0000 (67521.9394)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [27]  [ 570/2809]  eta: 0:14:01  lr: 0.000014  min_lr: 0.000000  loss: 3.9353 (3.6963)  loss_scale: 65536.0000 (67487.1594)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [27]  [ 580/2809]  eta: 0:13:57  lr: 0.000014  min_lr: 0.000000  loss: 3.7774 (3.6913)  loss_scale: 65536.0000 (67453.5766)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [27]  [ 590/2809]  eta: 0:13:53  lr: 0.000014  min_lr: 0.000000  loss: 3.7215 (3.6945)  loss_scale: 65536.0000 (67421.1303)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [27]  [ 600/2809]  eta: 0:13:49  lr: 0.000014  min_lr: 0.000000  loss: 3.8719 (3.6946)  loss_scale: 65536.0000 (67389.7637)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [27]  [ 610/2809]  eta: 0:13:45  lr: 0.000014  min_lr: 0.000000  loss: 3.6488 (3.6940)  loss_scale: 65536.0000 (67359.4239)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [27]  [ 620/2809]  eta: 0:13:41  lr: 0.000014  min_lr: 0.000000  loss: 3.5357 (3.6922)  loss_scale: 65536.0000 (67330.0612)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [27]  [ 630/2809]  eta: 0:13:37  lr: 0.000014  min_lr: 0.000000  loss: 3.5228 (3.6915)  loss_scale: 65536.0000 (67301.6292)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [27]  [ 640/2809]  eta: 0:13:33  lr: 0.000014  min_lr: 0.000000  loss: 3.4874 (3.6876)  loss_scale: 65536.0000 (67274.0842)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [27]  [ 650/2809]  eta: 0:13:29  lr: 0.000014  min_lr: 0.000000  loss: 3.4824 (3.6870)  loss_scale: 65536.0000 (67247.3856)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [27]  [ 660/2809]  eta: 0:13:25  lr: 0.000014  min_lr: 0.000000  loss: 3.7057 (3.6890)  loss_scale: 65536.0000 (67221.4947)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
[2025-01-13 07:18:56,223] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:18:56,223] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:18:57,347] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76508
[2025-01-13 07:18:57,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:18:57,347] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 670/2809]  eta: 0:13:21  lr: 0.000014  min_lr: 0.000000  loss: 3.8352 (3.6896)  loss_scale: 65536.0000 (67489.3830)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [27]  [ 680/2809]  eta: 0:13:18  lr: 0.000014  min_lr: 0.000000  loss: 3.6014 (3.6849)  loss_scale: 65536.0000 (67460.6990)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [27]  [ 690/2809]  eta: 0:13:14  lr: 0.000014  min_lr: 0.000000  loss: 3.6681 (3.6847)  loss_scale: 65536.0000 (67432.8452)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [27]  [ 700/2809]  eta: 0:13:10  lr: 0.000014  min_lr: 0.000000  loss: 3.8562 (3.6876)  loss_scale: 65536.0000 (67405.7860)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [27]  [ 710/2809]  eta: 0:13:06  lr: 0.000014  min_lr: 0.000000  loss: 3.9100 (3.6890)  loss_scale: 65536.0000 (67379.4880)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [27]  [ 720/2809]  eta: 0:13:02  lr: 0.000014  min_lr: 0.000000  loss: 3.7279 (3.6866)  loss_scale: 65536.0000 (67353.9196)  weight_decay: 0.0500 (0.0500)  time: 0.3650  data: 0.0002  max mem: 15572
Epoch: [27]  [ 730/2809]  eta: 0:12:58  lr: 0.000014  min_lr: 0.000000  loss: 3.6326 (3.6863)  loss_scale: 65536.0000 (67329.0506)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0001  max mem: 15572
Epoch: [27]  [ 740/2809]  eta: 0:12:54  lr: 0.000014  min_lr: 0.000000  loss: 3.8036 (3.6890)  loss_scale: 65536.0000 (67304.8529)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [27]  [ 750/2809]  eta: 0:12:50  lr: 0.000014  min_lr: 0.000000  loss: 3.8667 (3.6908)  loss_scale: 65536.0000 (67281.2996)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [27]  [ 760/2809]  eta: 0:12:46  lr: 0.000014  min_lr: 0.000000  loss: 3.7404 (3.6911)  loss_scale: 65536.0000 (67258.3653)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [27]  [ 770/2809]  eta: 0:12:42  lr: 0.000014  min_lr: 0.000000  loss: 3.7404 (3.6914)  loss_scale: 65536.0000 (67236.0259)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [27]  [ 780/2809]  eta: 0:12:38  lr: 0.000014  min_lr: 0.000000  loss: 3.7619 (3.6935)  loss_scale: 65536.0000 (67214.2586)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [27]  [ 790/2809]  eta: 0:12:35  lr: 0.000014  min_lr: 0.000000  loss: 4.0682 (3.6970)  loss_scale: 65536.0000 (67193.0417)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
[2025-01-13 07:19:44,946] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:19:44,947] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:19:45,327] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76638
[2025-01-13 07:19:45,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:19:45,327] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 800/2809]  eta: 0:12:31  lr: 0.000014  min_lr: 0.000000  loss: 3.7190 (3.6949)  loss_scale: 65536.0000 (67254.1723)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [27]  [ 810/2809]  eta: 0:12:27  lr: 0.000014  min_lr: 0.000000  loss: 3.6110 (3.6925)  loss_scale: 65536.0000 (67232.9864)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [27]  [ 820/2809]  eta: 0:12:23  lr: 0.000014  min_lr: 0.000000  loss: 3.6657 (3.6931)  loss_scale: 65536.0000 (67212.3167)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [27]  [ 830/2809]  eta: 0:12:19  lr: 0.000014  min_lr: 0.000000  loss: 3.7199 (3.6927)  loss_scale: 65536.0000 (67192.1444)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [27]  [ 840/2809]  eta: 0:12:15  lr: 0.000014  min_lr: 0.000000  loss: 3.9405 (3.6962)  loss_scale: 65536.0000 (67172.4518)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [27]  [ 850/2809]  eta: 0:12:11  lr: 0.000014  min_lr: 0.000000  loss: 3.8427 (3.6971)  loss_scale: 65536.0000 (67153.2221)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [27]  [ 860/2809]  eta: 0:12:08  lr: 0.000014  min_lr: 0.000000  loss: 3.8427 (3.6986)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [27]  [ 870/2809]  eta: 0:12:04  lr: 0.000014  min_lr: 0.000000  loss: 3.9411 (3.6984)  loss_scale: 65536.0000 (67116.0873)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [27]  [ 880/2809]  eta: 0:12:00  lr: 0.000014  min_lr: 0.000000  loss: 3.9411 (3.6996)  loss_scale: 65536.0000 (67098.1521)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0015  max mem: 15572
Epoch: [27]  [ 890/2809]  eta: 0:11:56  lr: 0.000014  min_lr: 0.000000  loss: 3.9247 (3.7000)  loss_scale: 65536.0000 (67080.6195)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0015  max mem: 15572
Epoch: [27]  [ 900/2809]  eta: 0:11:52  lr: 0.000014  min_lr: 0.000000  loss: 3.7149 (3.6982)  loss_scale: 65536.0000 (67063.4761)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [27]  [ 910/2809]  eta: 0:11:49  lr: 0.000014  min_lr: 0.000000  loss: 3.7293 (3.6994)  loss_scale: 65536.0000 (67046.7091)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [27]  [ 920/2809]  eta: 0:11:45  lr: 0.000014  min_lr: 0.000000  loss: 3.7640 (3.6980)  loss_scale: 65536.0000 (67030.3062)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
[2025-01-13 07:20:33,032] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:20:33,032] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [27]  [ 930/2809]  eta: 0:11:41  lr: 0.000014  min_lr: 0.000000  loss: 3.8115 (3.6988)  loss_scale: 65536.0000 (67507.0075)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
[2025-01-13 07:20:35,628] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76774
[2025-01-13 07:20:35,628] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:20:35,628] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 940/2809]  eta: 0:11:37  lr: 0.000014  min_lr: 0.000000  loss: 3.6835 (3.6986)  loss_scale: 65536.0000 (67486.0616)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [27]  [ 950/2809]  eta: 0:11:33  lr: 0.000014  min_lr: 0.000000  loss: 3.6905 (3.6998)  loss_scale: 65536.0000 (67465.5563)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [27]  [ 960/2809]  eta: 0:11:30  lr: 0.000014  min_lr: 0.000000  loss: 3.6905 (3.6994)  loss_scale: 65536.0000 (67445.4776)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [27]  [ 970/2809]  eta: 0:11:26  lr: 0.000014  min_lr: 0.000000  loss: 3.7035 (3.7016)  loss_scale: 65536.0000 (67425.8126)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [27]  [ 980/2809]  eta: 0:11:22  lr: 0.000014  min_lr: 0.000000  loss: 4.0249 (3.7027)  loss_scale: 65536.0000 (67406.5484)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [27]  [ 990/2809]  eta: 0:11:18  lr: 0.000014  min_lr: 0.000000  loss: 3.8371 (3.7036)  loss_scale: 65536.0000 (67387.6731)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0001  max mem: 15572
Epoch: [27]  [1000/2809]  eta: 0:11:14  lr: 0.000014  min_lr: 0.000000  loss: 3.7015 (3.7025)  loss_scale: 65536.0000 (67369.1748)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0001  max mem: 15572
Epoch: [27]  [1010/2809]  eta: 0:11:10  lr: 0.000014  min_lr: 0.000000  loss: 3.5389 (3.7005)  loss_scale: 65536.0000 (67351.0425)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [27]  [1020/2809]  eta: 0:11:07  lr: 0.000014  min_lr: 0.000000  loss: 3.6101 (3.7018)  loss_scale: 65536.0000 (67333.2654)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [27]  [1030/2809]  eta: 0:11:03  lr: 0.000014  min_lr: 0.000000  loss: 3.8505 (3.7009)  loss_scale: 65536.0000 (67315.8332)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [27]  [1040/2809]  eta: 0:10:59  lr: 0.000014  min_lr: 0.000000  loss: 3.7409 (3.7000)  loss_scale: 65536.0000 (67298.7358)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [27]  [1050/2809]  eta: 0:10:55  lr: 0.000014  min_lr: 0.000000  loss: 3.6489 (3.7003)  loss_scale: 65536.0000 (67281.9638)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
[2025-01-13 07:21:23,212] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:21:23,212] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [27]  [1060/2809]  eta: 0:10:52  lr: 0.000014  min_lr: 0.000000  loss: 3.7342 (3.7000)  loss_scale: 65536.0000 (67327.2762)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
[2025-01-13 07:21:26,575] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76912
[2025-01-13 07:21:26,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:21:26,575] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [1070/2809]  eta: 0:10:48  lr: 0.000014  min_lr: 0.000000  loss: 3.7342 (3.6984)  loss_scale: 65536.0000 (67800.0822)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [27]  [1080/2809]  eta: 0:10:44  lr: 0.000014  min_lr: 0.000000  loss: 3.7578 (3.6997)  loss_scale: 65536.0000 (67779.1378)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [27]  [1090/2809]  eta: 0:10:40  lr: 0.000014  min_lr: 0.000000  loss: 3.8233 (3.6996)  loss_scale: 65536.0000 (67758.5775)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [27]  [1100/2809]  eta: 0:10:37  lr: 0.000014  min_lr: 0.000000  loss: 3.7399 (3.6995)  loss_scale: 65536.0000 (67738.3906)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [27]  [1110/2809]  eta: 0:10:33  lr: 0.000014  min_lr: 0.000000  loss: 3.5805 (3.6997)  loss_scale: 65536.0000 (67718.5671)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [27]  [1120/2809]  eta: 0:10:29  lr: 0.000013  min_lr: 0.000000  loss: 3.5567 (3.6981)  loss_scale: 65536.0000 (67699.0972)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [27]  [1130/2809]  eta: 0:10:25  lr: 0.000013  min_lr: 0.000000  loss: 3.5847 (3.6977)  loss_scale: 65536.0000 (67679.9717)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [27]  [1140/2809]  eta: 0:10:21  lr: 0.000013  min_lr: 0.000000  loss: 3.7756 (3.6972)  loss_scale: 65536.0000 (67661.1814)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [27]  [1150/2809]  eta: 0:10:18  lr: 0.000013  min_lr: 0.000000  loss: 3.6355 (3.6971)  loss_scale: 65536.0000 (67642.7176)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
[2025-01-13 07:21:58,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=77000, skipped=522, lr=[1.3050742284399558e-07, 1.3050742284399558e-07, 1.8643917549142228e-07, 1.8643917549142228e-07, 2.663416792734604e-07, 2.663416792734604e-07, 3.804881132478006e-07, 3.804881132478006e-07, 5.435544474968581e-07, 5.435544474968581e-07, 7.765063535669401e-07, 7.765063535669401e-07, 1.1092947908099145e-06, 1.1092947908099145e-06, 1.5847068440141638e-06, 1.5847068440141638e-06, 2.263866920020234e-06, 2.263866920020234e-06, 3.234095600028906e-06, 3.234095600028906e-06, 4.620136571469866e-06, 4.620136571469866e-06, 6.600195102099809e-06, 6.600195102099809e-06, 9.42885014585687e-06, 9.42885014585687e-06, 1.3469785922652673e-05, 1.3469785922652673e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 07:21:58,807] [INFO] [timer.py:260:stop] epoch=0/micro_step=77000/global_step=77000, RunningAvgSamplesPerSec=30.710804656296908, CurrSamplesPerSec=34.59526514721924, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [27]  [1160/2809]  eta: 0:10:14  lr: 0.000013  min_lr: 0.000000  loss: 3.6413 (3.6970)  loss_scale: 65536.0000 (67624.5719)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0003  max mem: 15572
Epoch: [27]  [1170/2809]  eta: 0:10:10  lr: 0.000013  min_lr: 0.000000  loss: 3.7106 (3.6982)  loss_scale: 65536.0000 (67606.7361)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [27]  [1180/2809]  eta: 0:10:06  lr: 0.000013  min_lr: 0.000000  loss: 3.7566 (3.6988)  loss_scale: 65536.0000 (67589.2024)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [27]  [1190/2809]  eta: 0:10:03  lr: 0.000013  min_lr: 0.000000  loss: 3.8556 (3.7010)  loss_scale: 65536.0000 (67571.9631)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
[2025-01-13 07:22:14,351] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:22:14,351] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [27]  [1200/2809]  eta: 0:09:59  lr: 0.000013  min_lr: 0.000000  loss: 3.8556 (3.6993)  loss_scale: 65536.0000 (67718.7144)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0001  max mem: 15572
[2025-01-13 07:22:15,831] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77045
[2025-01-13 07:22:15,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:22:15,831] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [1210/2809]  eta: 0:09:55  lr: 0.000013  min_lr: 0.000000  loss: 3.5928 (3.6994)  loss_scale: 65536.0000 (67754.8076)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [27]  [1220/2809]  eta: 0:09:51  lr: 0.000013  min_lr: 0.000000  loss: 3.7623 (3.7001)  loss_scale: 65536.0000 (67736.6355)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [27]  [1230/2809]  eta: 0:09:47  lr: 0.000013  min_lr: 0.000000  loss: 3.8257 (3.7019)  loss_scale: 65536.0000 (67718.7587)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [27]  [1240/2809]  eta: 0:09:44  lr: 0.000013  min_lr: 0.000000  loss: 4.0318 (3.7052)  loss_scale: 65536.0000 (67701.1700)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0001  max mem: 15572
Epoch: [27]  [1250/2809]  eta: 0:09:40  lr: 0.000013  min_lr: 0.000000  loss: 4.0007 (3.7068)  loss_scale: 65536.0000 (67683.8625)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [27]  [1260/2809]  eta: 0:09:36  lr: 0.000013  min_lr: 0.000000  loss: 3.9177 (3.7071)  loss_scale: 65536.0000 (67666.8295)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [27]  [1270/2809]  eta: 0:09:33  lr: 0.000013  min_lr: 0.000000  loss: 3.8826 (3.7082)  loss_scale: 65536.0000 (67650.0645)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [27]  [1280/2809]  eta: 0:09:29  lr: 0.000013  min_lr: 0.000000  loss: 3.9089 (3.7085)  loss_scale: 65536.0000 (67633.5613)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [27]  [1290/2809]  eta: 0:09:25  lr: 0.000013  min_lr: 0.000000  loss: 3.6166 (3.7059)  loss_scale: 65536.0000 (67617.3137)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [27]  [1300/2809]  eta: 0:09:21  lr: 0.000013  min_lr: 0.000000  loss: 3.6166 (3.7063)  loss_scale: 65536.0000 (67601.3159)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [27]  [1310/2809]  eta: 0:09:18  lr: 0.000013  min_lr: 0.000000  loss: 4.0207 (3.7088)  loss_scale: 65536.0000 (67585.5622)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [27]  [1320/2809]  eta: 0:09:14  lr: 0.000013  min_lr: 0.000000  loss: 3.9435 (3.7092)  loss_scale: 65536.0000 (67570.0469)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [27]  [1330/2809]  eta: 0:09:10  lr: 0.000013  min_lr: 0.000000  loss: 3.7622 (3.7108)  loss_scale: 65536.0000 (67554.7648)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
[2025-01-13 07:23:03,733] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:23:03,733] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:23:05,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77178
[2025-01-13 07:23:05,229] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:23:05,229] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [1340/2809]  eta: 0:09:06  lr: 0.000013  min_lr: 0.000000  loss: 3.8768 (3.7122)  loss_scale: 65536.0000 (67735.1946)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [27]  [1350/2809]  eta: 0:09:03  lr: 0.000013  min_lr: 0.000000  loss: 3.8684 (3.7125)  loss_scale: 65536.0000 (67718.9164)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [27]  [1360/2809]  eta: 0:08:59  lr: 0.000013  min_lr: 0.000000  loss: 3.7658 (3.7132)  loss_scale: 65536.0000 (67702.8773)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [27]  [1370/2809]  eta: 0:08:55  lr: 0.000013  min_lr: 0.000000  loss: 3.8030 (3.7143)  loss_scale: 65536.0000 (67687.0722)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [27]  [1380/2809]  eta: 0:08:51  lr: 0.000013  min_lr: 0.000000  loss: 3.9731 (3.7140)  loss_scale: 65536.0000 (67671.4960)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [27]  [1390/2809]  eta: 0:08:48  lr: 0.000013  min_lr: 0.000000  loss: 3.7903 (3.7146)  loss_scale: 65536.0000 (67656.1438)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [27]  [1400/2809]  eta: 0:08:44  lr: 0.000013  min_lr: 0.000000  loss: 3.7903 (3.7154)  loss_scale: 65536.0000 (67641.0107)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [27]  [1410/2809]  eta: 0:08:40  lr: 0.000013  min_lr: 0.000000  loss: 3.7226 (3.7158)  loss_scale: 65536.0000 (67626.0921)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [27]  [1420/2809]  eta: 0:08:36  lr: 0.000013  min_lr: 0.000000  loss: 3.7226 (3.7164)  loss_scale: 65536.0000 (67611.3835)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [27]  [1430/2809]  eta: 0:08:33  lr: 0.000013  min_lr: 0.000000  loss: 3.5585 (3.7145)  loss_scale: 65536.0000 (67596.8805)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [27]  [1440/2809]  eta: 0:08:29  lr: 0.000013  min_lr: 0.000000  loss: 3.5530 (3.7146)  loss_scale: 65536.0000 (67582.5788)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [27]  [1450/2809]  eta: 0:08:25  lr: 0.000013  min_lr: 0.000000  loss: 3.6061 (3.7149)  loss_scale: 65536.0000 (67568.4742)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [27]  [1460/2809]  eta: 0:08:21  lr: 0.000013  min_lr: 0.000000  loss: 3.7346 (3.7154)  loss_scale: 65536.0000 (67554.5626)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
[2025-01-13 07:23:52,889] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:23:52,889] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:23:54,754] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77312
[2025-01-13 07:23:54,755] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:23:54,755] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [1470/2809]  eta: 0:08:18  lr: 0.000013  min_lr: 0.000000  loss: 3.7882 (3.7160)  loss_scale: 65536.0000 (67763.6003)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [27]  [1480/2809]  eta: 0:08:14  lr: 0.000013  min_lr: 0.000000  loss: 3.7743 (3.7166)  loss_scale: 65536.0000 (67748.5591)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [27]  [1490/2809]  eta: 0:08:10  lr: 0.000013  min_lr: 0.000000  loss: 3.7514 (3.7160)  loss_scale: 65536.0000 (67733.7197)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [27]  [1500/2809]  eta: 0:08:07  lr: 0.000013  min_lr: 0.000000  loss: 3.6840 (3.7163)  loss_scale: 65536.0000 (67719.0779)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [27]  [1510/2809]  eta: 0:08:03  lr: 0.000013  min_lr: 0.000000  loss: 3.9055 (3.7181)  loss_scale: 65536.0000 (67704.6300)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [27]  [1520/2809]  eta: 0:07:59  lr: 0.000013  min_lr: 0.000000  loss: 3.9055 (3.7181)  loss_scale: 65536.0000 (67690.3721)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [27]  [1530/2809]  eta: 0:07:55  lr: 0.000013  min_lr: 0.000000  loss: 3.7790 (3.7175)  loss_scale: 65536.0000 (67676.3005)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [27]  [1540/2809]  eta: 0:07:52  lr: 0.000013  min_lr: 0.000000  loss: 3.6204 (3.7171)  loss_scale: 65536.0000 (67662.4114)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
[2025-01-13 07:24:23,435] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77389
[2025-01-13 07:24:23,435] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:24:23,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1550/2809]  eta: 0:07:48  lr: 0.000013  min_lr: 0.000000  loss: 3.6047 (3.7168)  loss_scale: 65536.0000 (67543.0664)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [27]  [1560/2809]  eta: 0:07:44  lr: 0.000013  min_lr: 0.000000  loss: 3.9115 (3.7184)  loss_scale: 32768.0000 (67320.2921)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [27]  [1570/2809]  eta: 0:07:41  lr: 0.000013  min_lr: 0.000000  loss: 3.9241 (3.7194)  loss_scale: 32768.0000 (67100.3539)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [27]  [1580/2809]  eta: 0:07:37  lr: 0.000013  min_lr: 0.000000  loss: 3.9241 (3.7204)  loss_scale: 32768.0000 (66883.1980)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [27]  [1590/2809]  eta: 0:07:33  lr: 0.000013  min_lr: 0.000000  loss: 3.7759 (3.7196)  loss_scale: 32768.0000 (66668.7718)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [27]  [1600/2809]  eta: 0:07:29  lr: 0.000013  min_lr: 0.000000  loss: 3.4619 (3.7180)  loss_scale: 32768.0000 (66457.0244)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [27]  [1610/2809]  eta: 0:07:26  lr: 0.000013  min_lr: 0.000000  loss: 3.3707 (3.7171)  loss_scale: 32768.0000 (66247.9056)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0001  max mem: 15572
Epoch: [27]  [1620/2809]  eta: 0:07:22  lr: 0.000013  min_lr: 0.000000  loss: 3.5436 (3.7160)  loss_scale: 32768.0000 (66041.3671)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [27]  [1630/2809]  eta: 0:07:18  lr: 0.000013  min_lr: 0.000000  loss: 3.6486 (3.7161)  loss_scale: 32768.0000 (65837.3611)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [27]  [1640/2809]  eta: 0:07:14  lr: 0.000013  min_lr: 0.000000  loss: 3.7379 (3.7163)  loss_scale: 32768.0000 (65635.8416)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [27]  [1650/2809]  eta: 0:07:11  lr: 0.000013  min_lr: 0.000000  loss: 3.6523 (3.7152)  loss_scale: 32768.0000 (65436.7632)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [27]  [1660/2809]  eta: 0:07:07  lr: 0.000013  min_lr: 0.000000  loss: 3.6523 (3.7144)  loss_scale: 32768.0000 (65240.0819)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [27]  [1670/2809]  eta: 0:07:03  lr: 0.000013  min_lr: 0.000000  loss: 3.7255 (3.7141)  loss_scale: 32768.0000 (65045.7546)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
[2025-01-13 07:25:11,173] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:25:11,173] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1680/2809]  eta: 0:06:59  lr: 0.000013  min_lr: 0.000000  loss: 3.7255 (3.7140)  loss_scale: 32768.0000 (64970.6984)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [27]  [1690/2809]  eta: 0:06:56  lr: 0.000013  min_lr: 0.000000  loss: 3.4624 (3.7124)  loss_scale: 65536.0000 (64974.0414)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
[2025-01-13 07:25:17,786] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77536
[2025-01-13 07:25:17,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:25:17,787] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1700/2809]  eta: 0:06:52  lr: 0.000013  min_lr: 0.000000  loss: 3.4469 (3.7111)  loss_scale: 65536.0000 (64823.2334)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [27]  [1710/2809]  eta: 0:06:48  lr: 0.000013  min_lr: 0.000000  loss: 3.6134 (3.7116)  loss_scale: 32768.0000 (64635.8854)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [27]  [1720/2809]  eta: 0:06:44  lr: 0.000013  min_lr: 0.000000  loss: 3.6567 (3.7120)  loss_scale: 32768.0000 (64450.7147)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [27]  [1730/2809]  eta: 0:06:41  lr: 0.000013  min_lr: 0.000000  loss: 3.7444 (3.7120)  loss_scale: 32768.0000 (64267.6834)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [27]  [1740/2809]  eta: 0:06:37  lr: 0.000013  min_lr: 0.000000  loss: 3.6778 (3.7118)  loss_scale: 32768.0000 (64086.7547)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [27]  [1750/2809]  eta: 0:06:33  lr: 0.000013  min_lr: 0.000000  loss: 3.8487 (3.7127)  loss_scale: 32768.0000 (63907.8926)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [27]  [1760/2809]  eta: 0:06:30  lr: 0.000013  min_lr: 0.000000  loss: 3.9136 (3.7130)  loss_scale: 32768.0000 (63731.0619)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [27]  [1770/2809]  eta: 0:06:26  lr: 0.000013  min_lr: 0.000000  loss: 3.8193 (3.7128)  loss_scale: 32768.0000 (63556.2281)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [27]  [1780/2809]  eta: 0:06:22  lr: 0.000013  min_lr: 0.000000  loss: 3.5427 (3.7113)  loss_scale: 32768.0000 (63383.3577)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [27]  [1790/2809]  eta: 0:06:18  lr: 0.000013  min_lr: 0.000000  loss: 3.5601 (3.7120)  loss_scale: 32768.0000 (63212.4176)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [27]  [1800/2809]  eta: 0:06:15  lr: 0.000013  min_lr: 0.000000  loss: 3.8747 (3.7111)  loss_scale: 32768.0000 (63043.3759)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [27]  [1810/2809]  eta: 0:06:11  lr: 0.000013  min_lr: 0.000000  loss: 3.7815 (3.7111)  loss_scale: 32768.0000 (62876.2010)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [27]  [1820/2809]  eta: 0:06:07  lr: 0.000013  min_lr: 0.000000  loss: 3.7020 (3.7106)  loss_scale: 32768.0000 (62710.8622)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
[2025-01-13 07:26:05,646] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:26:05,646] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1830/2809]  eta: 0:06:04  lr: 0.000013  min_lr: 0.000000  loss: 3.6682 (3.7103)  loss_scale: 32768.0000 (62708.3954)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
Epoch: [27]  [1840/2809]  eta: 0:06:00  lr: 0.000013  min_lr: 0.000000  loss: 3.7607 (3.7102)  loss_scale: 65536.0000 (62723.7545)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [27]  [1850/2809]  eta: 0:05:56  lr: 0.000013  min_lr: 0.000000  loss: 3.5643 (3.7079)  loss_scale: 65536.0000 (62738.9476)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [27]  [1860/2809]  eta: 0:05:52  lr: 0.000013  min_lr: 0.000000  loss: 3.5112 (3.7078)  loss_scale: 65536.0000 (62753.9774)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [27]  [1870/2809]  eta: 0:05:49  lr: 0.000013  min_lr: 0.000000  loss: 3.6828 (3.7068)  loss_scale: 65536.0000 (62768.8466)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [27]  [1880/2809]  eta: 0:05:45  lr: 0.000013  min_lr: 0.000000  loss: 3.6828 (3.7066)  loss_scale: 65536.0000 (62783.5577)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [27]  [1890/2809]  eta: 0:05:41  lr: 0.000013  min_lr: 0.000000  loss: 3.7957 (3.7053)  loss_scale: 65536.0000 (62798.1132)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
[2025-01-13 07:26:33,372] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77740
[2025-01-13 07:26:33,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:26:33,372] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1900/2809]  eta: 0:05:37  lr: 0.000013  min_lr: 0.000000  loss: 3.7160 (3.7067)  loss_scale: 65536.0000 (62743.5665)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [27]  [1910/2809]  eta: 0:05:34  lr: 0.000013  min_lr: 0.000000  loss: 3.6465 (3.7059)  loss_scale: 32768.0000 (62586.7085)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [27]  [1920/2809]  eta: 0:05:30  lr: 0.000013  min_lr: 0.000000  loss: 3.8298 (3.7070)  loss_scale: 32768.0000 (62431.4836)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0001  max mem: 15572
Epoch: [27]  [1930/2809]  eta: 0:05:26  lr: 0.000013  min_lr: 0.000000  loss: 4.0256 (3.7092)  loss_scale: 32768.0000 (62277.8664)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [27]  [1940/2809]  eta: 0:05:23  lr: 0.000013  min_lr: 0.000000  loss: 3.6851 (3.7082)  loss_scale: 32768.0000 (62125.8320)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [27]  [1950/2809]  eta: 0:05:19  lr: 0.000013  min_lr: 0.000000  loss: 3.6179 (3.7079)  loss_scale: 32768.0000 (61975.3562)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [27]  [1960/2809]  eta: 0:05:15  lr: 0.000013  min_lr: 0.000000  loss: 3.8808 (3.7085)  loss_scale: 32768.0000 (61826.4151)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [27]  [1970/2809]  eta: 0:05:11  lr: 0.000013  min_lr: 0.000000  loss: 3.6763 (3.7075)  loss_scale: 32768.0000 (61678.9853)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [27]  [1980/2809]  eta: 0:05:08  lr: 0.000013  min_lr: 0.000000  loss: 3.6411 (3.7085)  loss_scale: 32768.0000 (61533.0439)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [27]  [1990/2809]  eta: 0:05:04  lr: 0.000013  min_lr: 0.000000  loss: 3.8660 (3.7086)  loss_scale: 32768.0000 (61388.5686)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [27]  [2000/2809]  eta: 0:05:00  lr: 0.000013  min_lr: 0.000000  loss: 3.6823 (3.7080)  loss_scale: 32768.0000 (61245.5372)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [27]  [2010/2809]  eta: 0:04:56  lr: 0.000013  min_lr: 0.000000  loss: 3.7257 (3.7088)  loss_scale: 32768.0000 (61103.9284)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [27]  [2020/2809]  eta: 0:04:53  lr: 0.000013  min_lr: 0.000000  loss: 3.9143 (3.7097)  loss_scale: 32768.0000 (60963.7209)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
[2025-01-13 07:27:21,046] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:27:21,046] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [2030/2809]  eta: 0:04:49  lr: 0.000013  min_lr: 0.000000  loss: 4.0821 (3.7109)  loss_scale: 32768.0000 (60905.5638)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [27]  [2040/2809]  eta: 0:04:45  lr: 0.000013  min_lr: 0.000000  loss: 3.9216 (3.7106)  loss_scale: 65536.0000 (60928.2509)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [27]  [2050/2809]  eta: 0:04:42  lr: 0.000013  min_lr: 0.000000  loss: 3.4907 (3.7104)  loss_scale: 65536.0000 (60950.7167)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0003  max mem: 15572
Epoch: [27]  [2060/2809]  eta: 0:04:38  lr: 0.000013  min_lr: 0.000000  loss: 3.7937 (3.7111)  loss_scale: 65536.0000 (60972.9646)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [27]  [2070/2809]  eta: 0:04:34  lr: 0.000013  min_lr: 0.000000  loss: 3.8823 (3.7107)  loss_scale: 65536.0000 (60994.9976)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [27]  [2080/2809]  eta: 0:04:30  lr: 0.000013  min_lr: 0.000000  loss: 3.7736 (3.7111)  loss_scale: 65536.0000 (61016.8188)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [27]  [2090/2809]  eta: 0:04:27  lr: 0.000013  min_lr: 0.000000  loss: 3.9786 (3.7123)  loss_scale: 65536.0000 (61038.4314)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [27]  [2100/2809]  eta: 0:04:23  lr: 0.000013  min_lr: 0.000000  loss: 3.9786 (3.7127)  loss_scale: 65536.0000 (61059.8382)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [27]  [2110/2809]  eta: 0:04:19  lr: 0.000013  min_lr: 0.000000  loss: 3.7396 (3.7124)  loss_scale: 65536.0000 (61081.0422)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [27]  [2120/2809]  eta: 0:04:15  lr: 0.000013  min_lr: 0.000000  loss: 3.5583 (3.7129)  loss_scale: 65536.0000 (61102.0462)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [27]  [2130/2809]  eta: 0:04:12  lr: 0.000013  min_lr: 0.000000  loss: 3.6534 (3.7123)  loss_scale: 65536.0000 (61122.8531)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [27]  [2140/2809]  eta: 0:04:08  lr: 0.000013  min_lr: 0.000000  loss: 3.6534 (3.7123)  loss_scale: 65536.0000 (61143.4657)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [27]  [2150/2809]  eta: 0:04:04  lr: 0.000013  min_lr: 0.000000  loss: 3.5641 (3.7119)  loss_scale: 65536.0000 (61163.8866)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
[2025-01-13 07:28:08,355] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:28:08,355] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:28:09,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=78000, skipped=528, lr=[1.2400201441221816e-07, 1.2400201441221816e-07, 1.7714573487459742e-07, 1.7714573487459742e-07, 2.5306533553513916e-07, 2.5306533553513916e-07, 3.615219079073417e-07, 3.615219079073417e-07, 5.164598684390596e-07, 5.164598684390596e-07, 7.377998120557995e-07, 7.377998120557995e-07, 1.053999731508285e-06, 1.053999731508285e-06, 1.505713902154693e-06, 1.505713902154693e-06, 2.15101986022099e-06, 2.15101986022099e-06, 3.0728855146014146e-06, 3.0728855146014146e-06, 4.389836449430592e-06, 4.389836449430592e-06, 6.2711949277579896e-06, 6.2711949277579896e-06, 8.95884989679713e-06, 8.95884989679713e-06, 1.279835699542447e-05, 1.279835699542447e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 07:28:09,104] [INFO] [timer.py:260:stop] epoch=0/micro_step=78000/global_step=78000, RunningAvgSamplesPerSec=30.751835123455038, CurrSamplesPerSec=35.20212618636303, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-13 07:28:10,569] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78003
[2025-01-13 07:28:10,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:28:10,569] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [2160/2809]  eta: 0:04:01  lr: 0.000013  min_lr: 0.000000  loss: 3.4977 (3.7117)  loss_scale: 65536.0000 (61366.0787)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [27]  [2170/2809]  eta: 0:03:57  lr: 0.000013  min_lr: 0.000000  loss: 3.5766 (3.7114)  loss_scale: 65536.0000 (61385.2860)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [27]  [2180/2809]  eta: 0:03:53  lr: 0.000013  min_lr: 0.000000  loss: 3.6355 (3.7116)  loss_scale: 65536.0000 (61404.3173)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [27]  [2190/2809]  eta: 0:03:49  lr: 0.000013  min_lr: 0.000000  loss: 3.9251 (3.7121)  loss_scale: 65536.0000 (61423.1748)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [27]  [2200/2809]  eta: 0:03:46  lr: 0.000013  min_lr: 0.000000  loss: 3.8895 (3.7119)  loss_scale: 65536.0000 (61441.8610)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [27]  [2210/2809]  eta: 0:03:42  lr: 0.000013  min_lr: 0.000000  loss: 3.6656 (3.7118)  loss_scale: 65536.0000 (61460.3781)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [27]  [2220/2809]  eta: 0:03:38  lr: 0.000013  min_lr: 0.000000  loss: 3.7616 (3.7125)  loss_scale: 65536.0000 (61478.7285)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [27]  [2230/2809]  eta: 0:03:35  lr: 0.000013  min_lr: 0.000000  loss: 3.6673 (3.7117)  loss_scale: 65536.0000 (61496.9144)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [27]  [2240/2809]  eta: 0:03:31  lr: 0.000013  min_lr: 0.000000  loss: 3.6561 (3.7126)  loss_scale: 65536.0000 (61514.9380)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [27]  [2250/2809]  eta: 0:03:27  lr: 0.000013  min_lr: 0.000000  loss: 3.9862 (3.7132)  loss_scale: 65536.0000 (61532.8014)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [27]  [2260/2809]  eta: 0:03:23  lr: 0.000013  min_lr: 0.000000  loss: 3.9284 (3.7142)  loss_scale: 65536.0000 (61550.5069)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [27]  [2270/2809]  eta: 0:03:20  lr: 0.000013  min_lr: 0.000000  loss: 3.7246 (3.7142)  loss_scale: 65536.0000 (61568.0564)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [27]  [2280/2809]  eta: 0:03:16  lr: 0.000013  min_lr: 0.000000  loss: 3.6602 (3.7143)  loss_scale: 65536.0000 (61585.4520)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 07:28:58,453] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:28:58,453] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:28:58,838] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78133
[2025-01-13 07:28:58,838] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:28:58,838] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [2290/2809]  eta: 0:03:12  lr: 0.000013  min_lr: 0.000000  loss: 3.6765 (3.7142)  loss_scale: 65536.0000 (61631.3016)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
Epoch: [27]  [2300/2809]  eta: 0:03:09  lr: 0.000013  min_lr: 0.000000  loss: 3.8240 (3.7149)  loss_scale: 65536.0000 (61648.2712)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
Epoch: [27]  [2310/2809]  eta: 0:03:05  lr: 0.000013  min_lr: 0.000000  loss: 3.8323 (3.7154)  loss_scale: 65536.0000 (61665.0939)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [27]  [2320/2809]  eta: 0:03:01  lr: 0.000013  min_lr: 0.000000  loss: 3.6265 (3.7154)  loss_scale: 65536.0000 (61681.7717)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [27]  [2330/2809]  eta: 0:02:57  lr: 0.000013  min_lr: 0.000000  loss: 3.7294 (3.7159)  loss_scale: 65536.0000 (61698.3063)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [27]  [2340/2809]  eta: 0:02:54  lr: 0.000013  min_lr: 0.000000  loss: 3.7708 (3.7164)  loss_scale: 65536.0000 (61714.6997)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [27]  [2350/2809]  eta: 0:02:50  lr: 0.000013  min_lr: 0.000000  loss: 3.8390 (3.7169)  loss_scale: 65536.0000 (61730.9536)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [27]  [2360/2809]  eta: 0:02:46  lr: 0.000013  min_lr: 0.000000  loss: 3.8924 (3.7167)  loss_scale: 65536.0000 (61747.0699)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [27]  [2370/2809]  eta: 0:02:43  lr: 0.000013  min_lr: 0.000000  loss: 3.8760 (3.7174)  loss_scale: 65536.0000 (61763.0502)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [27]  [2380/2809]  eta: 0:02:39  lr: 0.000013  min_lr: 0.000000  loss: 3.8070 (3.7180)  loss_scale: 65536.0000 (61778.8963)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [27]  [2390/2809]  eta: 0:02:35  lr: 0.000013  min_lr: 0.000000  loss: 3.6547 (3.7173)  loss_scale: 65536.0000 (61794.6098)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [27]  [2400/2809]  eta: 0:02:31  lr: 0.000013  min_lr: 0.000000  loss: 3.5966 (3.7174)  loss_scale: 65536.0000 (61810.1924)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [27]  [2410/2809]  eta: 0:02:28  lr: 0.000013  min_lr: 0.000000  loss: 3.5228 (3.7155)  loss_scale: 65536.0000 (61825.6458)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
[2025-01-13 07:29:46,851] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:29:46,851] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:29:47,212] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78263
[2025-01-13 07:29:47,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:29:47,212] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [2420/2809]  eta: 0:02:24  lr: 0.000013  min_lr: 0.000000  loss: 3.3322 (3.7155)  loss_scale: 65536.0000 (61868.0413)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [27]  [2430/2809]  eta: 0:02:20  lr: 0.000013  min_lr: 0.000000  loss: 3.5051 (3.7145)  loss_scale: 65536.0000 (61883.1296)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [27]  [2440/2809]  eta: 0:02:17  lr: 0.000013  min_lr: 0.000000  loss: 3.6397 (3.7146)  loss_scale: 65536.0000 (61898.0942)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [27]  [2450/2809]  eta: 0:02:13  lr: 0.000013  min_lr: 0.000000  loss: 3.7690 (3.7144)  loss_scale: 65536.0000 (61912.9368)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [27]  [2460/2809]  eta: 0:02:09  lr: 0.000013  min_lr: 0.000000  loss: 3.7690 (3.7150)  loss_scale: 65536.0000 (61927.6587)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [27]  [2470/2809]  eta: 0:02:05  lr: 0.000013  min_lr: 0.000000  loss: 3.8830 (3.7146)  loss_scale: 65536.0000 (61942.2614)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
[2025-01-13 07:30:07,926] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78319
[2025-01-13 07:30:07,926] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:30:07,926] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [2480/2809]  eta: 0:02:02  lr: 0.000013  min_lr: 0.000000  loss: 3.7983 (3.7148)  loss_scale: 65536.0000 (61890.7086)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [27]  [2490/2809]  eta: 0:01:58  lr: 0.000013  min_lr: 0.000000  loss: 3.7524 (3.7146)  loss_scale: 32768.0000 (61773.7969)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [27]  [2500/2809]  eta: 0:01:54  lr: 0.000013  min_lr: 0.000000  loss: 3.5425 (3.7137)  loss_scale: 32768.0000 (61657.8201)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [27]  [2510/2809]  eta: 0:01:51  lr: 0.000013  min_lr: 0.000000  loss: 3.5425 (3.7129)  loss_scale: 32768.0000 (61542.7670)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [27]  [2520/2809]  eta: 0:01:47  lr: 0.000013  min_lr: 0.000000  loss: 3.6670 (3.7136)  loss_scale: 32768.0000 (61428.6267)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0001  max mem: 15572
Epoch: [27]  [2530/2809]  eta: 0:01:43  lr: 0.000013  min_lr: 0.000000  loss: 3.9327 (3.7140)  loss_scale: 32768.0000 (61315.3884)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0001  max mem: 15572
Epoch: [27]  [2540/2809]  eta: 0:01:39  lr: 0.000013  min_lr: 0.000000  loss: 3.8444 (3.7141)  loss_scale: 32768.0000 (61203.0413)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0001  max mem: 15572
Epoch: [27]  [2550/2809]  eta: 0:01:36  lr: 0.000013  min_lr: 0.000000  loss: 3.7727 (3.7140)  loss_scale: 32768.0000 (61091.5751)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0001  max mem: 15572
Epoch: [27]  [2560/2809]  eta: 0:01:32  lr: 0.000013  min_lr: 0.000000  loss: 3.6038 (3.7127)  loss_scale: 32768.0000 (60980.9793)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [27]  [2570/2809]  eta: 0:01:28  lr: 0.000013  min_lr: 0.000000  loss: 3.6141 (3.7129)  loss_scale: 32768.0000 (60871.2439)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [27]  [2580/2809]  eta: 0:01:25  lr: 0.000013  min_lr: 0.000000  loss: 3.7574 (3.7127)  loss_scale: 32768.0000 (60762.3588)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [27]  [2590/2809]  eta: 0:01:21  lr: 0.000013  min_lr: 0.000000  loss: 3.4844 (3.7116)  loss_scale: 32768.0000 (60654.3142)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [27]  [2600/2809]  eta: 0:01:17  lr: 0.000013  min_lr: 0.000000  loss: 3.6913 (3.7120)  loss_scale: 32768.0000 (60547.1003)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
[2025-01-13 07:30:55,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:30:55,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [2610/2809]  eta: 0:01:13  lr: 0.000012  min_lr: 0.000000  loss: 4.0023 (3.7123)  loss_scale: 32768.0000 (60516.0077)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [27]  [2620/2809]  eta: 0:01:10  lr: 0.000012  min_lr: 0.000000  loss: 3.7701 (3.7115)  loss_scale: 65536.0000 (60535.1606)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [27]  [2630/2809]  eta: 0:01:06  lr: 0.000012  min_lr: 0.000000  loss: 3.6833 (3.7108)  loss_scale: 65536.0000 (60554.1680)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [27]  [2640/2809]  eta: 0:01:02  lr: 0.000012  min_lr: 0.000000  loss: 3.7242 (3.7110)  loss_scale: 65536.0000 (60573.0314)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [27]  [2650/2809]  eta: 0:00:59  lr: 0.000012  min_lr: 0.000000  loss: 3.7242 (3.7114)  loss_scale: 65536.0000 (60591.7525)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [27]  [2660/2809]  eta: 0:00:55  lr: 0.000012  min_lr: 0.000000  loss: 3.8074 (3.7120)  loss_scale: 65536.0000 (60610.3330)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [27]  [2670/2809]  eta: 0:00:51  lr: 0.000012  min_lr: 0.000000  loss: 3.9351 (3.7120)  loss_scale: 65536.0000 (60628.7742)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [27]  [2680/2809]  eta: 0:00:47  lr: 0.000012  min_lr: 0.000000  loss: 3.6208 (3.7118)  loss_scale: 65536.0000 (60647.0780)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [27]  [2690/2809]  eta: 0:00:44  lr: 0.000012  min_lr: 0.000000  loss: 3.7722 (3.7124)  loss_scale: 65536.0000 (60665.2456)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [27]  [2700/2809]  eta: 0:00:40  lr: 0.000012  min_lr: 0.000000  loss: 4.0042 (3.7132)  loss_scale: 65536.0000 (60683.2788)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [27]  [2710/2809]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 3.9953 (3.7136)  loss_scale: 65536.0000 (60701.1789)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [27]  [2720/2809]  eta: 0:00:33  lr: 0.000012  min_lr: 0.000000  loss: 3.9193 (3.7142)  loss_scale: 65536.0000 (60718.9474)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [27]  [2730/2809]  eta: 0:00:29  lr: 0.000012  min_lr: 0.000000  loss: 3.6704 (3.7143)  loss_scale: 65536.0000 (60736.5859)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
[2025-01-13 07:31:43,099] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:31:43,099] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:31:44,952] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78581
[2025-01-13 07:31:44,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:31:44,952] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [2740/2809]  eta: 0:00:25  lr: 0.000012  min_lr: 0.000000  loss: 4.0053 (3.7157)  loss_scale: 65536.0000 (60873.6432)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [27]  [2750/2809]  eta: 0:00:21  lr: 0.000012  min_lr: 0.000000  loss: 3.9342 (3.7160)  loss_scale: 65536.0000 (60890.5911)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [27]  [2760/2809]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 3.8126 (3.7163)  loss_scale: 65536.0000 (60907.4162)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [27]  [2770/2809]  eta: 0:00:14  lr: 0.000012  min_lr: 0.000000  loss: 3.8729 (3.7167)  loss_scale: 65536.0000 (60924.1198)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [27]  [2780/2809]  eta: 0:00:10  lr: 0.000012  min_lr: 0.000000  loss: 3.8239 (3.7166)  loss_scale: 65536.0000 (60940.7033)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [27]  [2790/2809]  eta: 0:00:07  lr: 0.000012  min_lr: 0.000000  loss: 3.8833 (3.7170)  loss_scale: 65536.0000 (60957.1680)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [27]  [2800/2809]  eta: 0:00:03  lr: 0.000012  min_lr: 0.000000  loss: 3.9343 (3.7177)  loss_scale: 65536.0000 (60973.5152)  weight_decay: 0.0500 (0.0500)  time: 0.3639  data: 0.0002  max mem: 15572
Epoch: [27]  [2808/2809]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 3.8833 (3.7179)  loss_scale: 65536.0000 (60986.5091)  weight_decay: 0.0500 (0.0500)  time: 0.3597  data: 0.0001  max mem: 15572
Epoch: [27] Total time: 0:17:23 (0.3714 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 3.8833 (3.7179)  loss_scale: 65536.0000 (60986.5091)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:10:51  loss: 0.3168 (0.3168)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3956  data: 2.1993  max mem: 15572
Val:  [ 10/272]  eta: 0:01:46  loss: 2.2762 (2.2681)  acc1: 44.4444 (42.4242)  acc5: 77.7778 (73.2323)  time: 0.4064  data: 0.2364  max mem: 15572
Val:  [ 20/272]  eta: 0:01:11  loss: 2.2762 (2.3359)  acc1: 44.4444 (45.7672)  acc5: 77.7778 (73.5450)  time: 0.1797  data: 0.0203  max mem: 15572
Val:  [ 30/272]  eta: 0:00:58  loss: 2.4696 (2.4313)  acc1: 44.4444 (42.2939)  acc5: 72.2222 (73.4767)  time: 0.1504  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:51  loss: 2.5404 (2.4676)  acc1: 27.7778 (40.2439)  acc5: 72.2222 (73.4417)  time: 0.1585  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:46  loss: 2.4038 (2.3913)  acc1: 33.3333 (41.9390)  acc5: 77.7778 (75.0545)  time: 0.1609  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:42  loss: 1.5815 (2.2859)  acc1: 61.1111 (45.0820)  acc5: 88.8889 (76.2295)  time: 0.1585  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:39  loss: 1.6108 (2.2139)  acc1: 66.6667 (47.4961)  acc5: 83.3333 (76.9953)  time: 0.1617  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:36  loss: 1.9245 (2.2231)  acc1: 61.1111 (47.3937)  acc5: 77.7778 (76.6804)  time: 0.1600  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.0357 (2.2181)  acc1: 50.0000 (47.6801)  acc5: 77.7778 (77.4115)  time: 0.1600  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:31  loss: 2.0515 (2.2462)  acc1: 50.0000 (46.9747)  acc5: 83.3333 (77.1727)  time: 0.1613  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:29  loss: 2.5678 (2.3187)  acc1: 22.2222 (44.7447)  acc5: 72.2222 (76.0260)  time: 0.1534  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 2.8162 (2.3565)  acc1: 22.2222 (43.9853)  acc5: 66.6667 (75.3903)  time: 0.1539  data: 0.0003  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.1413 (2.3219)  acc1: 50.0000 (44.9958)  acc5: 77.7778 (75.9966)  time: 0.1621  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.7273 (2.3152)  acc1: 61.1111 (45.3901)  acc5: 77.7778 (75.8865)  time: 0.1600  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.4453 (2.3240)  acc1: 38.8889 (44.7756)  acc5: 72.2222 (76.0118)  time: 0.1566  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.3966 (2.3149)  acc1: 44.4444 (45.1691)  acc5: 77.7778 (76.2595)  time: 0.1656  data: 0.0075  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.3966 (2.3353)  acc1: 44.4444 (44.5419)  acc5: 72.2222 (75.8609)  time: 0.1701  data: 0.0075  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 2.3883 (2.3255)  acc1: 38.8889 (44.5672)  acc5: 72.2222 (76.0589)  time: 0.1686  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.4881 (2.3798)  acc1: 38.8889 (43.4555)  acc5: 66.6667 (74.6655)  time: 0.1697  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.5671 (2.3844)  acc1: 38.8889 (43.2559)  acc5: 72.2222 (74.6545)  time: 0.1625  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.1369 (2.3875)  acc1: 50.0000 (43.3123)  acc5: 77.7778 (74.5919)  time: 0.1573  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.0516 (2.3763)  acc1: 50.0000 (43.5395)  acc5: 77.7778 (74.6355)  time: 0.1543  data: 0.0003  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.8751 (2.3487)  acc1: 61.1111 (44.4925)  acc5: 77.7778 (74.9158)  time: 0.1519  data: 0.0003  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.7190 (2.3334)  acc1: 55.5556 (44.6750)  acc5: 83.3333 (75.2651)  time: 0.1508  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.2647 (2.3446)  acc1: 38.8889 (44.0460)  acc5: 77.7778 (75.1660)  time: 0.1579  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.1948 (2.2853)  acc1: 72.2222 (45.7854)  acc5: 88.8889 (75.9259)  time: 0.1562  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.2939 (2.2816)  acc1: 72.2222 (45.8180)  acc5: 88.8889 (76.0968)  time: 0.1406  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.2939 (2.2860)  acc1: 66.6667 (45.7915)  acc5: 88.8889 (76.0803)  time: 0.1344  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1682 s / it)
* Acc@1 45.792 Acc@5 76.080 loss 2.286
Accuracy of the network on the 4883 val videos: 45.8%
[2025-01-13 07:32:56,575] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 07:32:56,577] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 07:32:56,577] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 07:32:58,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 07:32:58,906] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.79%
Epoch: [28]  [   0/2809]  eta: 2:40:27  lr: 0.000012  min_lr: 0.000000  loss: 3.9294 (3.9294)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.4274  data: 3.0385  max mem: 15572
Epoch: [28]  [  10/2809]  eta: 0:32:08  lr: 0.000012  min_lr: 0.000000  loss: 3.7802 (3.6211)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6891  data: 0.3039  max mem: 15572
Epoch: [28]  [  20/2809]  eta: 0:24:59  lr: 0.000012  min_lr: 0.000000  loss: 3.7025 (3.6675)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3931  data: 0.0153  max mem: 15572
Epoch: [28]  [  30/2809]  eta: 0:22:26  lr: 0.000012  min_lr: 0.000000  loss: 3.5960 (3.6597)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [28]  [  40/2809]  eta: 0:21:07  lr: 0.000012  min_lr: 0.000000  loss: 3.5787 (3.6489)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [28]  [  50/2809]  eta: 0:20:15  lr: 0.000012  min_lr: 0.000000  loss: 3.7253 (3.6725)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
[2025-01-13 07:33:24,311] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:33:24,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [  60/2809]  eta: 0:19:39  lr: 0.000012  min_lr: 0.000000  loss: 3.7340 (3.6700)  loss_scale: 65536.0000 (68759.0820)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
[2025-01-13 07:33:25,840] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78714
[2025-01-13 07:33:25,840] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:33:25,840] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [  70/2809]  eta: 0:19:10  lr: 0.000012  min_lr: 0.000000  loss: 3.5938 (3.6851)  loss_scale: 65536.0000 (69228.1690)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [28]  [  80/2809]  eta: 0:18:49  lr: 0.000012  min_lr: 0.000000  loss: 3.7685 (3.7076)  loss_scale: 65536.0000 (68772.3457)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [28]  [  90/2809]  eta: 0:18:30  lr: 0.000012  min_lr: 0.000000  loss: 3.6865 (3.6869)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [28]  [ 100/2809]  eta: 0:18:15  lr: 0.000012  min_lr: 0.000000  loss: 3.6865 (3.7014)  loss_scale: 65536.0000 (68131.4851)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [28]  [ 110/2809]  eta: 0:18:03  lr: 0.000012  min_lr: 0.000000  loss: 3.8700 (3.7028)  loss_scale: 65536.0000 (67897.6577)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [28]  [ 120/2809]  eta: 0:17:51  lr: 0.000012  min_lr: 0.000000  loss: 3.9944 (3.7277)  loss_scale: 65536.0000 (67702.4793)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [28]  [ 130/2809]  eta: 0:17:41  lr: 0.000012  min_lr: 0.000000  loss: 3.8947 (3.7184)  loss_scale: 65536.0000 (67537.0992)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [28]  [ 140/2809]  eta: 0:17:32  lr: 0.000012  min_lr: 0.000000  loss: 3.5613 (3.6996)  loss_scale: 65536.0000 (67395.1773)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [28]  [ 150/2809]  eta: 0:17:24  lr: 0.000012  min_lr: 0.000000  loss: 3.3954 (3.6779)  loss_scale: 65536.0000 (67272.0530)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [28]  [ 160/2809]  eta: 0:17:17  lr: 0.000012  min_lr: 0.000000  loss: 3.7462 (3.6867)  loss_scale: 65536.0000 (67164.2236)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [28]  [ 170/2809]  eta: 0:17:09  lr: 0.000012  min_lr: 0.000000  loss: 3.7501 (3.6805)  loss_scale: 65536.0000 (67069.0058)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [28]  [ 180/2809]  eta: 0:17:02  lr: 0.000012  min_lr: 0.000000  loss: 3.5549 (3.6675)  loss_scale: 65536.0000 (66984.3094)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [28]  [ 190/2809]  eta: 0:16:56  lr: 0.000012  min_lr: 0.000000  loss: 3.7036 (3.6831)  loss_scale: 65536.0000 (66908.4817)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 07:34:13,400] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:34:13,401] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:34:15,969] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78850
[2025-01-13 07:34:15,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:34:15,969] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 200/2809]  eta: 0:16:49  lr: 0.000012  min_lr: 0.000000  loss: 3.8940 (3.6870)  loss_scale: 65536.0000 (69122.5473)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [28]  [ 210/2809]  eta: 0:16:43  lr: 0.000012  min_lr: 0.000000  loss: 3.9298 (3.6914)  loss_scale: 65536.0000 (68952.5687)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [28]  [ 220/2809]  eta: 0:16:37  lr: 0.000012  min_lr: 0.000000  loss: 3.7468 (3.6929)  loss_scale: 65536.0000 (68797.9729)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [28]  [ 230/2809]  eta: 0:16:31  lr: 0.000012  min_lr: 0.000000  loss: 3.7130 (3.6953)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [28]  [ 240/2809]  eta: 0:16:25  lr: 0.000012  min_lr: 0.000000  loss: 3.5278 (3.6898)  loss_scale: 65536.0000 (68527.2697)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [28]  [ 250/2809]  eta: 0:16:19  lr: 0.000012  min_lr: 0.000000  loss: 3.6311 (3.6917)  loss_scale: 65536.0000 (68408.0956)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [28]  [ 260/2809]  eta: 0:16:14  lr: 0.000012  min_lr: 0.000000  loss: 3.8758 (3.6963)  loss_scale: 65536.0000 (68298.0536)  weight_decay: 0.0500 (0.0500)  time: 0.3643  data: 0.0002  max mem: 15572
Epoch: [28]  [ 270/2809]  eta: 0:16:09  lr: 0.000012  min_lr: 0.000000  loss: 3.8755 (3.7024)  loss_scale: 65536.0000 (68196.1328)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [28]  [ 280/2809]  eta: 0:16:04  lr: 0.000012  min_lr: 0.000000  loss: 3.8755 (3.7097)  loss_scale: 65536.0000 (68101.4662)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [28]  [ 290/2809]  eta: 0:15:59  lr: 0.000012  min_lr: 0.000000  loss: 3.7101 (3.7010)  loss_scale: 65536.0000 (68013.3058)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [28]  [ 300/2809]  eta: 0:15:54  lr: 0.000012  min_lr: 0.000000  loss: 3.7038 (3.7036)  loss_scale: 65536.0000 (67931.0033)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [28]  [ 310/2809]  eta: 0:15:49  lr: 0.000012  min_lr: 0.000000  loss: 3.8718 (3.7024)  loss_scale: 65536.0000 (67853.9936)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [28]  [ 320/2809]  eta: 0:15:45  lr: 0.000012  min_lr: 0.000000  loss: 3.4230 (3.6881)  loss_scale: 65536.0000 (67781.7819)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
[2025-01-13 07:35:03,458] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:35:03,458] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [ 330/2809]  eta: 0:15:40  lr: 0.000012  min_lr: 0.000000  loss: 3.2429 (3.6848)  loss_scale: 65536.0000 (68505.9094)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
[2025-01-13 07:35:05,678] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78985
[2025-01-13 07:35:05,679] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:35:05,679] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 340/2809]  eta: 0:15:36  lr: 0.000012  min_lr: 0.000000  loss: 3.6522 (3.6856)  loss_scale: 65536.0000 (68803.1906)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
[2025-01-13 07:35:10,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=79000, skipped=536, lr=[1.1760208353860594e-07, 1.1760208353860594e-07, 1.680029764837228e-07, 1.680029764837228e-07, 2.40004252119604e-07, 2.40004252119604e-07, 3.4286321731372005e-07, 3.4286321731372005e-07, 4.898045961624572e-07, 4.898045961624572e-07, 6.997208516606532e-07, 6.997208516606532e-07, 9.99601216658076e-07, 9.99601216658076e-07, 1.428001738082966e-06, 1.428001738082966e-06, 2.0400024829756658e-06, 2.0400024829756658e-06, 2.9142892613938086e-06, 2.9142892613938086e-06, 4.1632703734197264e-06, 4.1632703734197264e-06, 5.947529104885324e-06, 5.947529104885324e-06, 8.496470149836178e-06, 8.496470149836178e-06, 1.2137814499765969e-05, 1.2137814499765969e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 07:35:10,849] [INFO] [timer.py:260:stop] epoch=0/micro_step=79000/global_step=79000, RunningAvgSamplesPerSec=30.79209032394277, CurrSamplesPerSec=35.100834358497984, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [28]  [ 350/2809]  eta: 0:15:32  lr: 0.000012  min_lr: 0.000000  loss: 3.4371 (3.6837)  loss_scale: 65536.0000 (68710.1083)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [28]  [ 360/2809]  eta: 0:15:27  lr: 0.000012  min_lr: 0.000000  loss: 3.6482 (3.6844)  loss_scale: 65536.0000 (68622.1828)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [28]  [ 370/2809]  eta: 0:15:23  lr: 0.000012  min_lr: 0.000000  loss: 3.6892 (3.6832)  loss_scale: 65536.0000 (68538.9973)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [28]  [ 380/2809]  eta: 0:15:18  lr: 0.000012  min_lr: 0.000000  loss: 3.5458 (3.6819)  loss_scale: 65536.0000 (68460.1785)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [28]  [ 390/2809]  eta: 0:15:14  lr: 0.000012  min_lr: 0.000000  loss: 3.6943 (3.6876)  loss_scale: 65536.0000 (68385.3913)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [28]  [ 400/2809]  eta: 0:15:09  lr: 0.000012  min_lr: 0.000000  loss: 3.6943 (3.6831)  loss_scale: 65536.0000 (68314.3342)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [28]  [ 410/2809]  eta: 0:15:05  lr: 0.000012  min_lr: 0.000000  loss: 3.5026 (3.6835)  loss_scale: 65536.0000 (68246.7348)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [28]  [ 420/2809]  eta: 0:15:01  lr: 0.000012  min_lr: 0.000000  loss: 3.5915 (3.6842)  loss_scale: 65536.0000 (68182.3468)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [28]  [ 430/2809]  eta: 0:14:57  lr: 0.000012  min_lr: 0.000000  loss: 3.6386 (3.6807)  loss_scale: 65536.0000 (68120.9466)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [28]  [ 440/2809]  eta: 0:14:52  lr: 0.000012  min_lr: 0.000000  loss: 3.3776 (3.6764)  loss_scale: 65536.0000 (68062.3311)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [28]  [ 450/2809]  eta: 0:14:48  lr: 0.000012  min_lr: 0.000000  loss: 3.3776 (3.6719)  loss_scale: 65536.0000 (68006.3149)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [28]  [ 460/2809]  eta: 0:14:44  lr: 0.000012  min_lr: 0.000000  loss: 3.8007 (3.6736)  loss_scale: 65536.0000 (67952.7289)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
[2025-01-13 07:35:53,300] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:35:53,300] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:35:54,025] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79116
[2025-01-13 07:35:54,026] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:35:54,026] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 470/2809]  eta: 0:14:40  lr: 0.000012  min_lr: 0.000000  loss: 3.9199 (3.6739)  loss_scale: 65536.0000 (68179.7028)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [28]  [ 480/2809]  eta: 0:14:36  lr: 0.000012  min_lr: 0.000000  loss: 3.8342 (3.6763)  loss_scale: 65536.0000 (68124.7401)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [28]  [ 490/2809]  eta: 0:14:31  lr: 0.000012  min_lr: 0.000000  loss: 3.8342 (3.6768)  loss_scale: 65536.0000 (68072.0163)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [28]  [ 500/2809]  eta: 0:14:27  lr: 0.000012  min_lr: 0.000000  loss: 3.7953 (3.6756)  loss_scale: 65536.0000 (68021.3972)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [28]  [ 510/2809]  eta: 0:14:23  lr: 0.000012  min_lr: 0.000000  loss: 3.7953 (3.6774)  loss_scale: 65536.0000 (67972.7593)  weight_decay: 0.0500 (0.0500)  time: 0.3646  data: 0.0002  max mem: 15572
Epoch: [28]  [ 520/2809]  eta: 0:14:19  lr: 0.000012  min_lr: 0.000000  loss: 3.8624 (3.6792)  loss_scale: 65536.0000 (67925.9885)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [28]  [ 530/2809]  eta: 0:14:15  lr: 0.000012  min_lr: 0.000000  loss: 3.8326 (3.6786)  loss_scale: 65536.0000 (67880.9793)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [28]  [ 540/2809]  eta: 0:14:11  lr: 0.000012  min_lr: 0.000000  loss: 3.6277 (3.6803)  loss_scale: 65536.0000 (67837.6340)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [28]  [ 550/2809]  eta: 0:14:07  lr: 0.000012  min_lr: 0.000000  loss: 3.6790 (3.6821)  loss_scale: 65536.0000 (67795.8621)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [28]  [ 560/2809]  eta: 0:14:03  lr: 0.000012  min_lr: 0.000000  loss: 3.6952 (3.6823)  loss_scale: 65536.0000 (67755.5793)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [28]  [ 570/2809]  eta: 0:13:59  lr: 0.000012  min_lr: 0.000000  loss: 3.8189 (3.6855)  loss_scale: 65536.0000 (67716.7075)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [28]  [ 580/2809]  eta: 0:13:55  lr: 0.000012  min_lr: 0.000000  loss: 3.8947 (3.6910)  loss_scale: 65536.0000 (67679.1738)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [28]  [ 590/2809]  eta: 0:13:51  lr: 0.000012  min_lr: 0.000000  loss: 3.8301 (3.6907)  loss_scale: 65536.0000 (67642.9103)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
[2025-01-13 07:36:41,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:36:41,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:36:43,398] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79250
[2025-01-13 07:36:43,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:36:43,398] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 600/2809]  eta: 0:13:47  lr: 0.000012  min_lr: 0.000000  loss: 4.0261 (3.6979)  loss_scale: 65536.0000 (68153.0782)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [28]  [ 610/2809]  eta: 0:13:43  lr: 0.000012  min_lr: 0.000000  loss: 3.9194 (3.6965)  loss_scale: 65536.0000 (68110.2455)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [28]  [ 620/2809]  eta: 0:13:39  lr: 0.000012  min_lr: 0.000000  loss: 3.6409 (3.6969)  loss_scale: 65536.0000 (68068.7923)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [28]  [ 630/2809]  eta: 0:13:35  lr: 0.000012  min_lr: 0.000000  loss: 3.5080 (3.6918)  loss_scale: 65536.0000 (68028.6529)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [28]  [ 640/2809]  eta: 0:13:31  lr: 0.000012  min_lr: 0.000000  loss: 3.4450 (3.6921)  loss_scale: 65536.0000 (67989.7660)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [28]  [ 650/2809]  eta: 0:13:27  lr: 0.000012  min_lr: 0.000000  loss: 3.5916 (3.6908)  loss_scale: 65536.0000 (67952.0737)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [28]  [ 660/2809]  eta: 0:13:23  lr: 0.000012  min_lr: 0.000000  loss: 3.7463 (3.6934)  loss_scale: 65536.0000 (67915.5219)  weight_decay: 0.0500 (0.0500)  time: 0.3647  data: 0.0002  max mem: 15572
Epoch: [28]  [ 670/2809]  eta: 0:13:19  lr: 0.000012  min_lr: 0.000000  loss: 3.9765 (3.6923)  loss_scale: 65536.0000 (67880.0596)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0002  max mem: 15572
Epoch: [28]  [ 680/2809]  eta: 0:13:15  lr: 0.000012  min_lr: 0.000000  loss: 3.8009 (3.6923)  loss_scale: 65536.0000 (67845.6388)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [28]  [ 690/2809]  eta: 0:13:11  lr: 0.000012  min_lr: 0.000000  loss: 3.8009 (3.6940)  loss_scale: 65536.0000 (67812.2142)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [28]  [ 700/2809]  eta: 0:13:07  lr: 0.000012  min_lr: 0.000000  loss: 3.8578 (3.6948)  loss_scale: 65536.0000 (67779.7432)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [28]  [ 710/2809]  eta: 0:13:04  lr: 0.000012  min_lr: 0.000000  loss: 3.9024 (3.6973)  loss_scale: 65536.0000 (67748.1857)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [28]  [ 720/2809]  eta: 0:13:00  lr: 0.000012  min_lr: 0.000000  loss: 3.8958 (3.7006)  loss_scale: 65536.0000 (67717.5035)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
[2025-01-13 07:37:30,830] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:37:30,830] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [ 730/2809]  eta: 0:12:56  lr: 0.000012  min_lr: 0.000000  loss: 3.7348 (3.6990)  loss_scale: 65536.0000 (68046.2709)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [28]  [ 740/2809]  eta: 0:12:52  lr: 0.000012  min_lr: 0.000000  loss: 3.6809 (3.6984)  loss_scale: 131072.0000 (68896.8205)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
[2025-01-13 07:37:35,997] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79393
[2025-01-13 07:37:35,997] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:37:35,997] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 750/2809]  eta: 0:12:48  lr: 0.000012  min_lr: 0.000000  loss: 3.6716 (3.6960)  loss_scale: 65536.0000 (68852.0692)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [28]  [ 760/2809]  eta: 0:12:44  lr: 0.000012  min_lr: 0.000000  loss: 3.5422 (3.6943)  loss_scale: 65536.0000 (68808.4941)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [28]  [ 770/2809]  eta: 0:12:40  lr: 0.000012  min_lr: 0.000000  loss: 3.5422 (3.6949)  loss_scale: 65536.0000 (68766.0493)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [28]  [ 780/2809]  eta: 0:12:36  lr: 0.000012  min_lr: 0.000000  loss: 4.0298 (3.7000)  loss_scale: 65536.0000 (68724.6914)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [28]  [ 790/2809]  eta: 0:12:33  lr: 0.000012  min_lr: 0.000000  loss: 4.0456 (3.7008)  loss_scale: 65536.0000 (68684.3793)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0001  max mem: 15572
Epoch: [28]  [ 800/2809]  eta: 0:12:29  lr: 0.000012  min_lr: 0.000000  loss: 3.8662 (3.7022)  loss_scale: 65536.0000 (68645.0737)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [28]  [ 810/2809]  eta: 0:12:25  lr: 0.000012  min_lr: 0.000000  loss: 3.9741 (3.7075)  loss_scale: 65536.0000 (68606.7374)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [28]  [ 820/2809]  eta: 0:12:21  lr: 0.000012  min_lr: 0.000000  loss: 3.9634 (3.7069)  loss_scale: 65536.0000 (68569.3350)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [28]  [ 830/2809]  eta: 0:12:17  lr: 0.000012  min_lr: 0.000000  loss: 3.9081 (3.7101)  loss_scale: 65536.0000 (68532.8327)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [28]  [ 840/2809]  eta: 0:12:13  lr: 0.000012  min_lr: 0.000000  loss: 3.8925 (3.7103)  loss_scale: 65536.0000 (68497.1986)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0002  max mem: 15572
Epoch: [28]  [ 850/2809]  eta: 0:12:09  lr: 0.000012  min_lr: 0.000000  loss: 3.6120 (3.7097)  loss_scale: 65536.0000 (68462.4019)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [28]  [ 860/2809]  eta: 0:12:06  lr: 0.000012  min_lr: 0.000000  loss: 3.7492 (3.7115)  loss_scale: 65536.0000 (68428.4135)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
[2025-01-13 07:38:23,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:38:23,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [ 870/2809]  eta: 0:12:02  lr: 0.000012  min_lr: 0.000000  loss: 3.7713 (3.7121)  loss_scale: 65536.0000 (68470.4478)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
[2025-01-13 07:38:23,899] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79523
[2025-01-13 07:38:23,899] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:38:23,899] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 880/2809]  eta: 0:11:58  lr: 0.000012  min_lr: 0.000000  loss: 3.6459 (3.7087)  loss_scale: 65536.0000 (68437.1396)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [28]  [ 890/2809]  eta: 0:11:54  lr: 0.000012  min_lr: 0.000000  loss: 3.4799 (3.7077)  loss_scale: 65536.0000 (68404.5791)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [28]  [ 900/2809]  eta: 0:11:50  lr: 0.000012  min_lr: 0.000000  loss: 3.6463 (3.7069)  loss_scale: 65536.0000 (68372.7414)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [28]  [ 910/2809]  eta: 0:11:47  lr: 0.000012  min_lr: 0.000000  loss: 3.6444 (3.7060)  loss_scale: 65536.0000 (68341.6026)  weight_decay: 0.0500 (0.0500)  time: 0.3650  data: 0.0002  max mem: 15572
Epoch: [28]  [ 920/2809]  eta: 0:11:43  lr: 0.000012  min_lr: 0.000000  loss: 3.7997 (3.7075)  loss_scale: 65536.0000 (68311.1401)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [28]  [ 930/2809]  eta: 0:11:39  lr: 0.000012  min_lr: 0.000000  loss: 3.8144 (3.7073)  loss_scale: 65536.0000 (68281.3319)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [28]  [ 940/2809]  eta: 0:11:35  lr: 0.000012  min_lr: 0.000000  loss: 3.6386 (3.7063)  loss_scale: 65536.0000 (68252.1573)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [28]  [ 950/2809]  eta: 0:11:31  lr: 0.000012  min_lr: 0.000000  loss: 3.6042 (3.7049)  loss_scale: 65536.0000 (68223.5962)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [28]  [ 960/2809]  eta: 0:11:27  lr: 0.000012  min_lr: 0.000000  loss: 3.6042 (3.7024)  loss_scale: 65536.0000 (68195.6296)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [28]  [ 970/2809]  eta: 0:11:24  lr: 0.000012  min_lr: 0.000000  loss: 3.8215 (3.7053)  loss_scale: 65536.0000 (68168.2389)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [28]  [ 980/2809]  eta: 0:11:20  lr: 0.000012  min_lr: 0.000000  loss: 3.8309 (3.7042)  loss_scale: 65536.0000 (68141.4067)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [28]  [ 990/2809]  eta: 0:11:16  lr: 0.000012  min_lr: 0.000000  loss: 3.5341 (3.7019)  loss_scale: 65536.0000 (68115.1160)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
[2025-01-13 07:39:11,281] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:39:11,281] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [1000/2809]  eta: 0:11:12  lr: 0.000012  min_lr: 0.000000  loss: 3.7485 (3.7035)  loss_scale: 65536.0000 (68154.8212)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
[2025-01-13 07:39:12,362] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79655
[2025-01-13 07:39:12,363] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:39:12,363] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [1010/2809]  eta: 0:11:08  lr: 0.000012  min_lr: 0.000000  loss: 3.7485 (3.7019)  loss_scale: 65536.0000 (68258.5638)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [28]  [1020/2809]  eta: 0:11:05  lr: 0.000012  min_lr: 0.000000  loss: 3.6702 (3.6994)  loss_scale: 65536.0000 (68231.8981)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [28]  [1030/2809]  eta: 0:11:01  lr: 0.000012  min_lr: 0.000000  loss: 3.3711 (3.6979)  loss_scale: 65536.0000 (68205.7498)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [28]  [1040/2809]  eta: 0:10:57  lr: 0.000012  min_lr: 0.000000  loss: 3.6732 (3.7008)  loss_scale: 65536.0000 (68180.1037)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [28]  [1050/2809]  eta: 0:10:53  lr: 0.000012  min_lr: 0.000000  loss: 3.6732 (3.6976)  loss_scale: 65536.0000 (68154.9458)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [28]  [1060/2809]  eta: 0:10:50  lr: 0.000012  min_lr: 0.000000  loss: 3.4354 (3.6981)  loss_scale: 65536.0000 (68130.2620)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [28]  [1070/2809]  eta: 0:10:46  lr: 0.000012  min_lr: 0.000000  loss: 3.5432 (3.6953)  loss_scale: 65536.0000 (68106.0392)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
Epoch: [28]  [1080/2809]  eta: 0:10:42  lr: 0.000012  min_lr: 0.000000  loss: 3.5139 (3.6944)  loss_scale: 65536.0000 (68082.2646)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [28]  [1090/2809]  eta: 0:10:38  lr: 0.000012  min_lr: 0.000000  loss: 3.8142 (3.6973)  loss_scale: 65536.0000 (68058.9258)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [28]  [1100/2809]  eta: 0:10:34  lr: 0.000012  min_lr: 0.000000  loss: 3.8865 (3.6971)  loss_scale: 65536.0000 (68036.0109)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [28]  [1110/2809]  eta: 0:10:31  lr: 0.000012  min_lr: 0.000000  loss: 3.6153 (3.6974)  loss_scale: 65536.0000 (68013.5086)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [28]  [1120/2809]  eta: 0:10:27  lr: 0.000012  min_lr: 0.000000  loss: 3.4829 (3.6945)  loss_scale: 65536.0000 (67991.4077)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [28]  [1130/2809]  eta: 0:10:23  lr: 0.000012  min_lr: 0.000000  loss: 3.5417 (3.6939)  loss_scale: 65536.0000 (67969.6976)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
[2025-01-13 07:39:59,819] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:39:59,819] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:40:00,565] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79786
[2025-01-13 07:40:00,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:40:00,565] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [1140/2809]  eta: 0:10:19  lr: 0.000012  min_lr: 0.000000  loss: 3.5627 (3.6916)  loss_scale: 65536.0000 (68063.2428)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [28]  [1150/2809]  eta: 0:10:16  lr: 0.000012  min_lr: 0.000000  loss: 3.5971 (3.6919)  loss_scale: 65536.0000 (68041.2858)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [28]  [1160/2809]  eta: 0:10:12  lr: 0.000012  min_lr: 0.000000  loss: 3.9247 (3.6953)  loss_scale: 65536.0000 (68019.7071)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [28]  [1170/2809]  eta: 0:10:08  lr: 0.000012  min_lr: 0.000000  loss: 3.9845 (3.6960)  loss_scale: 65536.0000 (67998.4970)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [28]  [1180/2809]  eta: 0:10:04  lr: 0.000012  min_lr: 0.000000  loss: 3.6074 (3.6954)  loss_scale: 65536.0000 (67977.6461)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [28]  [1190/2809]  eta: 0:10:00  lr: 0.000012  min_lr: 0.000000  loss: 3.5633 (3.6950)  loss_scale: 65536.0000 (67957.1453)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [28]  [1200/2809]  eta: 0:09:57  lr: 0.000012  min_lr: 0.000000  loss: 3.7278 (3.6971)  loss_scale: 65536.0000 (67936.9858)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
Epoch: [28]  [1210/2809]  eta: 0:09:53  lr: 0.000012  min_lr: 0.000000  loss: 3.7278 (3.6958)  loss_scale: 65536.0000 (67917.1594)  weight_decay: 0.0500 (0.0500)  time: 0.3645  data: 0.0002  max mem: 15572
Epoch: [28]  [1220/2809]  eta: 0:09:49  lr: 0.000012  min_lr: 0.000000  loss: 3.6045 (3.6966)  loss_scale: 65536.0000 (67897.6577)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [28]  [1230/2809]  eta: 0:09:46  lr: 0.000012  min_lr: 0.000000  loss: 3.5985 (3.6954)  loss_scale: 65536.0000 (67878.4728)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [28]  [1240/2809]  eta: 0:09:42  lr: 0.000012  min_lr: 0.000000  loss: 3.3351 (3.6943)  loss_scale: 65536.0000 (67859.5971)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [28]  [1250/2809]  eta: 0:09:38  lr: 0.000012  min_lr: 0.000000  loss: 3.6924 (3.6954)  loss_scale: 65536.0000 (67841.0232)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [28]  [1260/2809]  eta: 0:09:34  lr: 0.000012  min_lr: 0.000000  loss: 3.7686 (3.6955)  loss_scale: 65536.0000 (67822.7439)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
[2025-01-13 07:40:48,077] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:40:48,077] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:40:49,532] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79919
[2025-01-13 07:40:49,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:40:49,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [1270/2809]  eta: 0:09:30  lr: 0.000012  min_lr: 0.000000  loss: 3.7752 (3.6964)  loss_scale: 65536.0000 (68011.0024)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [28]  [1280/2809]  eta: 0:09:27  lr: 0.000012  min_lr: 0.000000  loss: 3.9679 (3.6982)  loss_scale: 65536.0000 (67991.6815)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [28]  [1290/2809]  eta: 0:09:23  lr: 0.000012  min_lr: 0.000000  loss: 3.8264 (3.6985)  loss_scale: 65536.0000 (67972.6600)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [28]  [1300/2809]  eta: 0:09:19  lr: 0.000012  min_lr: 0.000000  loss: 3.8152 (3.6999)  loss_scale: 65536.0000 (67953.9308)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [28]  [1310/2809]  eta: 0:09:16  lr: 0.000012  min_lr: 0.000000  loss: 3.7522 (3.6999)  loss_scale: 65536.0000 (67935.4874)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [28]  [1320/2809]  eta: 0:09:12  lr: 0.000012  min_lr: 0.000000  loss: 3.8487 (3.7028)  loss_scale: 65536.0000 (67917.3232)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [28]  [1330/2809]  eta: 0:09:08  lr: 0.000011  min_lr: 0.000000  loss: 3.9311 (3.7033)  loss_scale: 65536.0000 (67899.4320)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [28]  [1340/2809]  eta: 0:09:04  lr: 0.000011  min_lr: 0.000000  loss: 3.7566 (3.7037)  loss_scale: 65536.0000 (67881.8076)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
[2025-01-13 07:41:19,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=80000, skipped=543, lr=[1.1131416451469152e-07, 1.1131416451469152e-07, 1.590202350209879e-07, 1.590202350209879e-07, 2.2717176431569702e-07, 2.2717176431569702e-07, 3.245310918795672e-07, 3.245310918795672e-07, 4.636158455422389e-07, 4.636158455422389e-07, 6.62308350774627e-07, 6.62308350774627e-07, 9.461547868208958e-07, 9.461547868208958e-07, 1.3516496954584228e-06, 1.3516496954584228e-06, 1.930928136369175e-06, 1.930928136369175e-06, 2.758468766241679e-06, 2.758468766241679e-06, 3.9406696660595414e-06, 3.9406696660595414e-06, 5.629528094370775e-06, 5.629528094370775e-06, 8.04218299195825e-06, 8.04218299195825e-06, 1.1488832845654643e-05, 1.1488832845654643e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 07:41:19,084] [INFO] [timer.py:260:stop] epoch=0/micro_step=80000/global_step=80000, RunningAvgSamplesPerSec=30.833227353312513, CurrSamplesPerSec=34.29039532472275, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [28]  [1350/2809]  eta: 0:09:01  lr: 0.000011  min_lr: 0.000000  loss: 3.7223 (3.7031)  loss_scale: 65536.0000 (67864.4441)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0001  max mem: 15572
Epoch: [28]  [1360/2809]  eta: 0:08:57  lr: 0.000011  min_lr: 0.000000  loss: 3.7423 (3.7031)  loss_scale: 65536.0000 (67847.3358)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [28]  [1370/2809]  eta: 0:08:53  lr: 0.000011  min_lr: 0.000000  loss: 3.7753 (3.7037)  loss_scale: 65536.0000 (67830.4770)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [28]  [1380/2809]  eta: 0:08:50  lr: 0.000011  min_lr: 0.000000  loss: 3.5721 (3.7030)  loss_scale: 65536.0000 (67813.8624)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [28]  [1390/2809]  eta: 0:08:46  lr: 0.000011  min_lr: 0.000000  loss: 3.6089 (3.7039)  loss_scale: 65536.0000 (67797.4867)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
[2025-01-13 07:41:37,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:41:37,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:41:37,526] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80049
[2025-01-13 07:41:37,526] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:41:37,526] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [1400/2809]  eta: 0:08:42  lr: 0.000011  min_lr: 0.000000  loss: 3.6184 (3.7031)  loss_scale: 65536.0000 (67828.1228)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [28]  [1410/2809]  eta: 0:08:38  lr: 0.000011  min_lr: 0.000000  loss: 3.4766 (3.7007)  loss_scale: 65536.0000 (67811.8781)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [28]  [1420/2809]  eta: 0:08:35  lr: 0.000011  min_lr: 0.000000  loss: 3.5264 (3.7012)  loss_scale: 65536.0000 (67795.8621)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
[2025-01-13 07:41:48,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80079
[2025-01-13 07:41:48,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:41:48,591] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1430/2809]  eta: 0:08:31  lr: 0.000011  min_lr: 0.000000  loss: 3.6812 (3.7012)  loss_scale: 65536.0000 (67688.4752)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [28]  [1440/2809]  eta: 0:08:27  lr: 0.000011  min_lr: 0.000000  loss: 3.7164 (3.7023)  loss_scale: 32768.0000 (67446.1402)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [28]  [1450/2809]  eta: 0:08:23  lr: 0.000011  min_lr: 0.000000  loss: 3.6778 (3.7002)  loss_scale: 32768.0000 (67207.1454)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [28]  [1460/2809]  eta: 0:08:20  lr: 0.000011  min_lr: 0.000000  loss: 3.6959 (3.7021)  loss_scale: 32768.0000 (66971.4223)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [28]  [1470/2809]  eta: 0:08:16  lr: 0.000011  min_lr: 0.000000  loss: 3.8362 (3.7023)  loss_scale: 32768.0000 (66738.9041)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [28]  [1480/2809]  eta: 0:08:12  lr: 0.000011  min_lr: 0.000000  loss: 3.6860 (3.7017)  loss_scale: 32768.0000 (66509.5260)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [28]  [1490/2809]  eta: 0:08:09  lr: 0.000011  min_lr: 0.000000  loss: 3.6784 (3.7013)  loss_scale: 32768.0000 (66283.2247)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [28]  [1500/2809]  eta: 0:08:05  lr: 0.000011  min_lr: 0.000000  loss: 3.7091 (3.7007)  loss_scale: 32768.0000 (66059.9387)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [28]  [1510/2809]  eta: 0:08:01  lr: 0.000011  min_lr: 0.000000  loss: 3.7473 (3.7012)  loss_scale: 32768.0000 (65839.6082)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0001  max mem: 15572
Epoch: [28]  [1520/2809]  eta: 0:07:57  lr: 0.000011  min_lr: 0.000000  loss: 3.7909 (3.6999)  loss_scale: 32768.0000 (65622.1749)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [28]  [1530/2809]  eta: 0:07:54  lr: 0.000011  min_lr: 0.000000  loss: 3.3061 (3.6974)  loss_scale: 32768.0000 (65407.5820)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [28]  [1540/2809]  eta: 0:07:50  lr: 0.000011  min_lr: 0.000000  loss: 3.4788 (3.6978)  loss_scale: 32768.0000 (65195.7742)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [28]  [1550/2809]  eta: 0:07:46  lr: 0.000011  min_lr: 0.000000  loss: 3.8087 (3.6973)  loss_scale: 32768.0000 (64986.6976)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
[2025-01-13 07:42:36,231] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:42:36,231] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1560/2809]  eta: 0:07:42  lr: 0.000011  min_lr: 0.000000  loss: 3.9117 (3.6988)  loss_scale: 32768.0000 (64885.2582)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [28]  [1570/2809]  eta: 0:07:39  lr: 0.000011  min_lr: 0.000000  loss: 3.9722 (3.6995)  loss_scale: 65536.0000 (64889.4004)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [28]  [1580/2809]  eta: 0:07:35  lr: 0.000011  min_lr: 0.000000  loss: 3.8099 (3.6994)  loss_scale: 65536.0000 (64893.4902)  weight_decay: 0.0500 (0.0500)  time: 0.3658  data: 0.0002  max mem: 15572
[2025-01-13 07:42:48,723] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80242
[2025-01-13 07:42:48,723] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:42:48,723] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1590/2809]  eta: 0:07:31  lr: 0.000011  min_lr: 0.000000  loss: 3.7260 (3.6998)  loss_scale: 65536.0000 (64876.9327)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [28]  [1600/2809]  eta: 0:07:28  lr: 0.000011  min_lr: 0.000000  loss: 3.7260 (3.6998)  loss_scale: 32768.0000 (64676.3773)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [28]  [1610/2809]  eta: 0:07:24  lr: 0.000011  min_lr: 0.000000  loss: 3.6647 (3.7002)  loss_scale: 32768.0000 (64478.3116)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [28]  [1620/2809]  eta: 0:07:20  lr: 0.000011  min_lr: 0.000000  loss: 3.6961 (3.7000)  loss_scale: 32768.0000 (64282.6897)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [28]  [1630/2809]  eta: 0:07:16  lr: 0.000011  min_lr: 0.000000  loss: 3.7662 (3.7002)  loss_scale: 32768.0000 (64089.4666)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [28]  [1640/2809]  eta: 0:07:13  lr: 0.000011  min_lr: 0.000000  loss: 3.6424 (3.7000)  loss_scale: 32768.0000 (63898.5984)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [28]  [1650/2809]  eta: 0:07:09  lr: 0.000011  min_lr: 0.000000  loss: 3.6704 (3.7017)  loss_scale: 32768.0000 (63710.0424)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [28]  [1660/2809]  eta: 0:07:05  lr: 0.000011  min_lr: 0.000000  loss: 3.6898 (3.7009)  loss_scale: 32768.0000 (63523.7568)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [28]  [1670/2809]  eta: 0:07:02  lr: 0.000011  min_lr: 0.000000  loss: 3.7186 (3.7018)  loss_scale: 32768.0000 (63339.7008)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [28]  [1680/2809]  eta: 0:06:58  lr: 0.000011  min_lr: 0.000000  loss: 3.9878 (3.7027)  loss_scale: 32768.0000 (63157.8346)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [28]  [1690/2809]  eta: 0:06:54  lr: 0.000011  min_lr: 0.000000  loss: 3.8129 (3.7032)  loss_scale: 32768.0000 (62978.1195)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [28]  [1700/2809]  eta: 0:06:50  lr: 0.000011  min_lr: 0.000000  loss: 3.6928 (3.7026)  loss_scale: 32768.0000 (62800.5173)  weight_decay: 0.0500 (0.0500)  time: 0.3652  data: 0.0002  max mem: 15572
Epoch: [28]  [1710/2809]  eta: 0:06:47  lr: 0.000011  min_lr: 0.000000  loss: 3.4697 (3.7019)  loss_scale: 32768.0000 (62624.9912)  weight_decay: 0.0500 (0.0500)  time: 0.3647  data: 0.0002  max mem: 15572
[2025-01-13 07:43:36,154] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:43:36,155] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1720/2809]  eta: 0:06:43  lr: 0.000011  min_lr: 0.000000  loss: 3.5116 (3.7020)  loss_scale: 32768.0000 (62489.5851)  weight_decay: 0.0500 (0.0500)  time: 0.3648  data: 0.0002  max mem: 15572
Epoch: [28]  [1730/2809]  eta: 0:06:39  lr: 0.000011  min_lr: 0.000000  loss: 3.6685 (3.7017)  loss_scale: 65536.0000 (62507.1843)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [28]  [1740/2809]  eta: 0:06:35  lr: 0.000011  min_lr: 0.000000  loss: 3.7554 (3.7024)  loss_scale: 65536.0000 (62524.5813)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [28]  [1750/2809]  eta: 0:06:32  lr: 0.000011  min_lr: 0.000000  loss: 3.7848 (3.7021)  loss_scale: 65536.0000 (62541.7796)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [28]  [1760/2809]  eta: 0:06:28  lr: 0.000011  min_lr: 0.000000  loss: 3.8304 (3.7032)  loss_scale: 65536.0000 (62558.7825)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [28]  [1770/2809]  eta: 0:06:24  lr: 0.000011  min_lr: 0.000000  loss: 3.7691 (3.7020)  loss_scale: 65536.0000 (62575.5935)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [28]  [1780/2809]  eta: 0:06:21  lr: 0.000011  min_lr: 0.000000  loss: 3.5153 (3.7016)  loss_scale: 65536.0000 (62592.2156)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [28]  [1790/2809]  eta: 0:06:17  lr: 0.000011  min_lr: 0.000000  loss: 3.6167 (3.7024)  loss_scale: 65536.0000 (62608.6521)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0001  max mem: 15572
[2025-01-13 07:44:06,060] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80452
[2025-01-13 07:44:06,060] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:44:06,060] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1800/2809]  eta: 0:06:13  lr: 0.000011  min_lr: 0.000000  loss: 3.5944 (3.7018)  loss_scale: 65536.0000 (62606.7118)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [28]  [1810/2809]  eta: 0:06:09  lr: 0.000011  min_lr: 0.000000  loss: 3.3282 (3.7014)  loss_scale: 32768.0000 (62441.9481)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
Epoch: [28]  [1820/2809]  eta: 0:06:06  lr: 0.000011  min_lr: 0.000000  loss: 3.8346 (3.7033)  loss_scale: 32768.0000 (62278.9940)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [28]  [1830/2809]  eta: 0:06:02  lr: 0.000011  min_lr: 0.000000  loss: 3.8034 (3.7022)  loss_scale: 32768.0000 (62117.8198)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [28]  [1840/2809]  eta: 0:05:58  lr: 0.000011  min_lr: 0.000000  loss: 3.6056 (3.7024)  loss_scale: 32768.0000 (61958.3965)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [28]  [1850/2809]  eta: 0:05:55  lr: 0.000011  min_lr: 0.000000  loss: 3.7067 (3.7021)  loss_scale: 32768.0000 (61800.6958)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [28]  [1860/2809]  eta: 0:05:51  lr: 0.000011  min_lr: 0.000000  loss: 3.8554 (3.7032)  loss_scale: 32768.0000 (61644.6900)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [28]  [1870/2809]  eta: 0:05:47  lr: 0.000011  min_lr: 0.000000  loss: 3.8769 (3.7029)  loss_scale: 32768.0000 (61490.3517)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [28]  [1880/2809]  eta: 0:05:43  lr: 0.000011  min_lr: 0.000000  loss: 3.6953 (3.7031)  loss_scale: 32768.0000 (61337.6544)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [28]  [1890/2809]  eta: 0:05:40  lr: 0.000011  min_lr: 0.000000  loss: 3.6953 (3.7037)  loss_scale: 32768.0000 (61186.5722)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [28]  [1900/2809]  eta: 0:05:36  lr: 0.000011  min_lr: 0.000000  loss: 3.6625 (3.7040)  loss_scale: 32768.0000 (61037.0794)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0003  max mem: 15572
Epoch: [28]  [1910/2809]  eta: 0:05:32  lr: 0.000011  min_lr: 0.000000  loss: 3.9663 (3.7045)  loss_scale: 32768.0000 (60889.1512)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [28]  [1920/2809]  eta: 0:05:29  lr: 0.000011  min_lr: 0.000000  loss: 3.8025 (3.7047)  loss_scale: 32768.0000 (60742.7631)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
[2025-01-13 07:44:53,742] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:44:53,742] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1930/2809]  eta: 0:05:25  lr: 0.000011  min_lr: 0.000000  loss: 3.7995 (3.7051)  loss_scale: 32768.0000 (60631.8301)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [28]  [1940/2809]  eta: 0:05:21  lr: 0.000011  min_lr: 0.000000  loss: 3.8564 (3.7057)  loss_scale: 65536.0000 (60657.0963)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [28]  [1950/2809]  eta: 0:05:18  lr: 0.000011  min_lr: 0.000000  loss: 3.9166 (3.7065)  loss_scale: 65536.0000 (60682.1035)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [28]  [1960/2809]  eta: 0:05:14  lr: 0.000011  min_lr: 0.000000  loss: 3.6670 (3.7061)  loss_scale: 65536.0000 (60706.8557)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [28]  [1970/2809]  eta: 0:05:10  lr: 0.000011  min_lr: 0.000000  loss: 3.6195 (3.7057)  loss_scale: 65536.0000 (60731.3567)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [28]  [1980/2809]  eta: 0:05:06  lr: 0.000011  min_lr: 0.000000  loss: 3.7713 (3.7047)  loss_scale: 65536.0000 (60755.6103)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [28]  [1990/2809]  eta: 0:05:03  lr: 0.000011  min_lr: 0.000000  loss: 3.6301 (3.7039)  loss_scale: 65536.0000 (60779.6203)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [28]  [2000/2809]  eta: 0:04:59  lr: 0.000011  min_lr: 0.000000  loss: 3.7618 (3.7056)  loss_scale: 65536.0000 (60803.3903)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [28]  [2010/2809]  eta: 0:04:55  lr: 0.000011  min_lr: 0.000000  loss: 3.9941 (3.7057)  loss_scale: 65536.0000 (60826.9239)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [28]  [2020/2809]  eta: 0:04:52  lr: 0.000011  min_lr: 0.000000  loss: 3.7708 (3.7061)  loss_scale: 65536.0000 (60850.2246)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [28]  [2030/2809]  eta: 0:04:48  lr: 0.000011  min_lr: 0.000000  loss: 3.7104 (3.7064)  loss_scale: 65536.0000 (60873.2959)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [28]  [2040/2809]  eta: 0:04:44  lr: 0.000011  min_lr: 0.000000  loss: 3.8242 (3.7068)  loss_scale: 65536.0000 (60896.1411)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [28]  [2050/2809]  eta: 0:04:41  lr: 0.000011  min_lr: 0.000000  loss: 3.8260 (3.7063)  loss_scale: 65536.0000 (60918.7635)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
[2025-01-13 07:45:41,240] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:45:41,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:45:41,966] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80711
[2025-01-13 07:45:41,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:45:41,966] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [2060/2809]  eta: 0:04:37  lr: 0.000011  min_lr: 0.000000  loss: 3.4701 (3.7057)  loss_scale: 65536.0000 (61004.7627)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [28]  [2070/2809]  eta: 0:04:33  lr: 0.000011  min_lr: 0.000000  loss: 3.5954 (3.7050)  loss_scale: 65536.0000 (61026.6422)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [28]  [2080/2809]  eta: 0:04:29  lr: 0.000011  min_lr: 0.000000  loss: 3.8321 (3.7055)  loss_scale: 65536.0000 (61048.3114)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [28]  [2090/2809]  eta: 0:04:26  lr: 0.000011  min_lr: 0.000000  loss: 3.8826 (3.7063)  loss_scale: 65536.0000 (61069.7733)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [28]  [2100/2809]  eta: 0:04:22  lr: 0.000011  min_lr: 0.000000  loss: 3.7862 (3.7064)  loss_scale: 65536.0000 (61091.0309)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0001  max mem: 15572
Epoch: [28]  [2110/2809]  eta: 0:04:18  lr: 0.000011  min_lr: 0.000000  loss: 3.7098 (3.7060)  loss_scale: 65536.0000 (61112.0872)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0002  max mem: 15572
Epoch: [28]  [2120/2809]  eta: 0:04:15  lr: 0.000011  min_lr: 0.000000  loss: 3.4868 (3.7058)  loss_scale: 65536.0000 (61132.9448)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [28]  [2130/2809]  eta: 0:04:11  lr: 0.000011  min_lr: 0.000000  loss: 3.6357 (3.7058)  loss_scale: 65536.0000 (61153.6068)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [28]  [2140/2809]  eta: 0:04:07  lr: 0.000011  min_lr: 0.000000  loss: 3.6620 (3.7057)  loss_scale: 65536.0000 (61174.0757)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0001  max mem: 15572
Epoch: [28]  [2150/2809]  eta: 0:04:04  lr: 0.000011  min_lr: 0.000000  loss: 3.7897 (3.7069)  loss_scale: 65536.0000 (61194.3543)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [28]  [2160/2809]  eta: 0:04:00  lr: 0.000011  min_lr: 0.000000  loss: 4.1411 (3.7076)  loss_scale: 65536.0000 (61214.4452)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [28]  [2170/2809]  eta: 0:03:56  lr: 0.000011  min_lr: 0.000000  loss: 3.9907 (3.7076)  loss_scale: 65536.0000 (61234.3510)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [28]  [2180/2809]  eta: 0:03:52  lr: 0.000011  min_lr: 0.000000  loss: 3.9191 (3.7084)  loss_scale: 65536.0000 (61254.0743)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
[2025-01-13 07:46:29,723] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:46:29,723] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:46:30,448] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80842
[2025-01-13 07:46:30,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:46:30,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [2190/2809]  eta: 0:03:49  lr: 0.000011  min_lr: 0.000000  loss: 3.6599 (3.7081)  loss_scale: 65536.0000 (61333.4404)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [28]  [2200/2809]  eta: 0:03:45  lr: 0.000011  min_lr: 0.000000  loss: 3.5909 (3.7084)  loss_scale: 65536.0000 (61352.5343)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [28]  [2210/2809]  eta: 0:03:41  lr: 0.000011  min_lr: 0.000000  loss: 3.7566 (3.7082)  loss_scale: 65536.0000 (61371.4555)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [28]  [2220/2809]  eta: 0:03:38  lr: 0.000011  min_lr: 0.000000  loss: 3.5264 (3.7070)  loss_scale: 65536.0000 (61390.2062)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [28]  [2230/2809]  eta: 0:03:34  lr: 0.000011  min_lr: 0.000000  loss: 3.6795 (3.7073)  loss_scale: 65536.0000 (61408.7889)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [28]  [2240/2809]  eta: 0:03:30  lr: 0.000011  min_lr: 0.000000  loss: 3.8420 (3.7078)  loss_scale: 65536.0000 (61427.2057)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [28]  [2250/2809]  eta: 0:03:26  lr: 0.000011  min_lr: 0.000000  loss: 3.8213 (3.7083)  loss_scale: 65536.0000 (61445.4589)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [28]  [2260/2809]  eta: 0:03:23  lr: 0.000011  min_lr: 0.000000  loss: 3.7360 (3.7078)  loss_scale: 65536.0000 (61463.5506)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [28]  [2270/2809]  eta: 0:03:19  lr: 0.000011  min_lr: 0.000000  loss: 3.6957 (3.7078)  loss_scale: 65536.0000 (61481.4830)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
Epoch: [28]  [2280/2809]  eta: 0:03:15  lr: 0.000011  min_lr: 0.000000  loss: 3.7098 (3.7081)  loss_scale: 65536.0000 (61499.2582)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0001  max mem: 15572
Epoch: [28]  [2290/2809]  eta: 0:03:12  lr: 0.000011  min_lr: 0.000000  loss: 3.7624 (3.7079)  loss_scale: 65536.0000 (61516.8782)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0001  max mem: 15572
Epoch: [28]  [2300/2809]  eta: 0:03:08  lr: 0.000011  min_lr: 0.000000  loss: 3.7678 (3.7087)  loss_scale: 65536.0000 (61534.3451)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [28]  [2310/2809]  eta: 0:03:04  lr: 0.000011  min_lr: 0.000000  loss: 3.7933 (3.7080)  loss_scale: 65536.0000 (61551.6608)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
[2025-01-13 07:47:18,197] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:47:18,197] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:47:18,566] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80972
[2025-01-13 07:47:18,567] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:47:18,567] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [2320/2809]  eta: 0:03:01  lr: 0.000011  min_lr: 0.000000  loss: 3.9021 (3.7089)  loss_scale: 65536.0000 (61597.0633)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [28]  [2330/2809]  eta: 0:02:57  lr: 0.000011  min_lr: 0.000000  loss: 3.9351 (3.7092)  loss_scale: 65536.0000 (61613.9614)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [28]  [2340/2809]  eta: 0:02:53  lr: 0.000011  min_lr: 0.000000  loss: 3.7099 (3.7092)  loss_scale: 65536.0000 (61630.7151)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0001  max mem: 15572
[2025-01-13 07:47:28,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=81000, skipped=550, lr=[1.0514467726857202e-07, 1.0514467726857202e-07, 1.5020668181224575e-07, 1.5020668181224575e-07, 2.1458097401749398e-07, 2.1458097401749398e-07, 3.0654424859642e-07, 3.0654424859642e-07, 4.379203551377428e-07, 4.379203551377428e-07, 6.256005073396326e-07, 6.256005073396326e-07, 8.937150104851895e-07, 8.937150104851895e-07, 1.2767357292645567e-06, 1.2767357292645567e-06, 1.8239081846636523e-06, 1.8239081846636523e-06, 2.605583120948075e-06, 2.605583120948075e-06, 3.722261601354393e-06, 3.722261601354393e-06, 5.317516573363419e-06, 5.317516573363419e-06, 7.5964522476620275e-06, 7.5964522476620275e-06, 1.0852074639517183e-05, 1.0852074639517183e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 07:47:28,633] [INFO] [timer.py:260:stop] epoch=0/micro_step=81000/global_step=81000, RunningAvgSamplesPerSec=30.872120530159858, CurrSamplesPerSec=34.73896109543204, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [28]  [2350/2809]  eta: 0:02:49  lr: 0.000011  min_lr: 0.000000  loss: 3.7099 (3.7095)  loss_scale: 65536.0000 (61647.3262)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [28]  [2360/2809]  eta: 0:02:46  lr: 0.000011  min_lr: 0.000000  loss: 3.7347 (3.7102)  loss_scale: 65536.0000 (61663.7967)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [28]  [2370/2809]  eta: 0:02:42  lr: 0.000011  min_lr: 0.000000  loss: 3.6845 (3.7089)  loss_scale: 65536.0000 (61680.1282)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [28]  [2380/2809]  eta: 0:02:38  lr: 0.000011  min_lr: 0.000000  loss: 3.5894 (3.7085)  loss_scale: 65536.0000 (61696.3226)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [28]  [2390/2809]  eta: 0:02:35  lr: 0.000011  min_lr: 0.000000  loss: 3.7724 (3.7093)  loss_scale: 65536.0000 (61712.3814)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [28]  [2400/2809]  eta: 0:02:31  lr: 0.000011  min_lr: 0.000000  loss: 3.7614 (3.7090)  loss_scale: 65536.0000 (61728.3065)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [28]  [2410/2809]  eta: 0:02:27  lr: 0.000011  min_lr: 0.000000  loss: 3.5597 (3.7082)  loss_scale: 65536.0000 (61744.0995)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [28]  [2420/2809]  eta: 0:02:24  lr: 0.000011  min_lr: 0.000000  loss: 3.7479 (3.7086)  loss_scale: 65536.0000 (61759.7621)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0001  max mem: 15572
Epoch: [28]  [2430/2809]  eta: 0:02:20  lr: 0.000011  min_lr: 0.000000  loss: 3.9101 (3.7085)  loss_scale: 65536.0000 (61775.2958)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [28]  [2440/2809]  eta: 0:02:16  lr: 0.000011  min_lr: 0.000000  loss: 3.6401 (3.7081)  loss_scale: 65536.0000 (61790.7022)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
[2025-01-13 07:48:06,197] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:48:06,197] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [2450/2809]  eta: 0:02:12  lr: 0.000011  min_lr: 0.000000  loss: 3.6401 (3.7078)  loss_scale: 65536.0000 (61859.4598)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
[2025-01-13 07:48:06,946] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81103
[2025-01-13 07:48:06,946] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:48:06,946] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [2460/2809]  eta: 0:02:09  lr: 0.000011  min_lr: 0.000000  loss: 3.7873 (3.7076)  loss_scale: 65536.0000 (61874.3990)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [28]  [2470/2809]  eta: 0:02:05  lr: 0.000011  min_lr: 0.000000  loss: 3.7873 (3.7073)  loss_scale: 65536.0000 (61889.2173)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [28]  [2480/2809]  eta: 0:02:01  lr: 0.000011  min_lr: 0.000000  loss: 3.6616 (3.7067)  loss_scale: 65536.0000 (61903.9162)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [28]  [2490/2809]  eta: 0:01:58  lr: 0.000011  min_lr: 0.000000  loss: 3.7069 (3.7072)  loss_scale: 65536.0000 (61918.4970)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [28]  [2500/2809]  eta: 0:01:54  lr: 0.000011  min_lr: 0.000000  loss: 3.9491 (3.7082)  loss_scale: 65536.0000 (61932.9612)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [28]  [2510/2809]  eta: 0:01:50  lr: 0.000011  min_lr: 0.000000  loss: 3.8790 (3.7083)  loss_scale: 65536.0000 (61947.3102)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [28]  [2520/2809]  eta: 0:01:46  lr: 0.000011  min_lr: 0.000000  loss: 3.6777 (3.7082)  loss_scale: 65536.0000 (61961.5454)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [28]  [2530/2809]  eta: 0:01:43  lr: 0.000011  min_lr: 0.000000  loss: 3.8701 (3.7086)  loss_scale: 65536.0000 (61975.6681)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [28]  [2540/2809]  eta: 0:01:39  lr: 0.000011  min_lr: 0.000000  loss: 3.8990 (3.7090)  loss_scale: 65536.0000 (61989.6797)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [28]  [2550/2809]  eta: 0:01:35  lr: 0.000011  min_lr: 0.000000  loss: 3.8589 (3.7089)  loss_scale: 65536.0000 (62003.5813)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [28]  [2560/2809]  eta: 0:01:32  lr: 0.000011  min_lr: 0.000000  loss: 3.8589 (3.7093)  loss_scale: 65536.0000 (62017.3745)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [28]  [2570/2809]  eta: 0:01:28  lr: 0.000011  min_lr: 0.000000  loss: 3.8109 (3.7089)  loss_scale: 65536.0000 (62031.0603)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
[2025-01-13 07:48:54,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:48:54,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [2580/2809]  eta: 0:01:24  lr: 0.000011  min_lr: 0.000000  loss: 3.7482 (3.7093)  loss_scale: 65536.0000 (62070.0318)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
[2025-01-13 07:48:55,346] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81234
[2025-01-13 07:48:55,346] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:48:55,346] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [2590/2809]  eta: 0:01:21  lr: 0.000011  min_lr: 0.000000  loss: 3.7482 (3.7095)  loss_scale: 65536.0000 (62108.7024)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [28]  [2600/2809]  eta: 0:01:17  lr: 0.000011  min_lr: 0.000000  loss: 3.6156 (3.7088)  loss_scale: 65536.0000 (62121.8793)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [28]  [2610/2809]  eta: 0:01:13  lr: 0.000011  min_lr: 0.000000  loss: 3.7257 (3.7098)  loss_scale: 65536.0000 (62134.9552)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [28]  [2620/2809]  eta: 0:01:09  lr: 0.000011  min_lr: 0.000000  loss: 3.7617 (3.7091)  loss_scale: 65536.0000 (62147.9313)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
[2025-01-13 07:49:10,125] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81274
[2025-01-13 07:49:10,125] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 07:49:10,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [2630/2809]  eta: 0:01:06  lr: 0.000011  min_lr: 0.000000  loss: 3.7617 (3.7091)  loss_scale: 65536.0000 (62048.7176)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [28]  [2640/2809]  eta: 0:01:02  lr: 0.000011  min_lr: 0.000000  loss: 3.9110 (3.7098)  loss_scale: 32768.0000 (61937.8478)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [28]  [2650/2809]  eta: 0:00:58  lr: 0.000011  min_lr: 0.000000  loss: 3.6146 (3.7085)  loss_scale: 32768.0000 (61827.8144)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [28]  [2660/2809]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 3.5086 (3.7077)  loss_scale: 32768.0000 (61718.6080)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [28]  [2670/2809]  eta: 0:00:51  lr: 0.000011  min_lr: 0.000000  loss: 3.5086 (3.7071)  loss_scale: 32768.0000 (61610.2194)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [28]  [2680/2809]  eta: 0:00:47  lr: 0.000011  min_lr: 0.000000  loss: 3.4248 (3.7064)  loss_scale: 32768.0000 (61502.6393)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0003  max mem: 15572
Epoch: [28]  [2690/2809]  eta: 0:00:44  lr: 0.000011  min_lr: 0.000000  loss: 3.4756 (3.7063)  loss_scale: 32768.0000 (61395.8588)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [28]  [2700/2809]  eta: 0:00:40  lr: 0.000011  min_lr: 0.000000  loss: 3.6651 (3.7061)  loss_scale: 32768.0000 (61289.8689)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [28]  [2710/2809]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 3.7926 (3.7074)  loss_scale: 32768.0000 (61184.6610)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [28]  [2720/2809]  eta: 0:00:32  lr: 0.000011  min_lr: 0.000000  loss: 3.7733 (3.7071)  loss_scale: 32768.0000 (61080.2264)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [28]  [2730/2809]  eta: 0:00:29  lr: 0.000011  min_lr: 0.000000  loss: 3.7131 (3.7066)  loss_scale: 32768.0000 (60976.5566)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [28]  [2740/2809]  eta: 0:00:25  lr: 0.000011  min_lr: 0.000000  loss: 3.6728 (3.7065)  loss_scale: 32768.0000 (60873.6432)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [28]  [2750/2809]  eta: 0:00:21  lr: 0.000011  min_lr: 0.000000  loss: 3.6066 (3.7058)  loss_scale: 32768.0000 (60771.4780)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
[2025-01-13 07:49:57,809] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:49:57,809] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [2760/2809]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 3.4502 (3.7052)  loss_scale: 32768.0000 (60788.7345)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [28]  [2770/2809]  eta: 0:00:14  lr: 0.000011  min_lr: 0.000000  loss: 3.5415 (3.7047)  loss_scale: 65536.0000 (60805.8665)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [28]  [2780/2809]  eta: 0:00:10  lr: 0.000011  min_lr: 0.000000  loss: 3.7077 (3.7050)  loss_scale: 65536.0000 (60822.8752)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [28]  [2790/2809]  eta: 0:00:07  lr: 0.000011  min_lr: 0.000000  loss: 3.6759 (3.7044)  loss_scale: 65536.0000 (60839.7621)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0003  max mem: 15572
Epoch: [28]  [2800/2809]  eta: 0:00:03  lr: 0.000011  min_lr: 0.000000  loss: 3.8154 (3.7055)  loss_scale: 65536.0000 (60856.5284)  weight_decay: 0.0500 (0.0500)  time: 0.3618  data: 0.0001  max mem: 15572
Epoch: [28]  [2808/2809]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 3.8874 (3.7055)  loss_scale: 65536.0000 (60869.8555)  weight_decay: 0.0500 (0.0500)  time: 0.3586  data: 0.0001  max mem: 15572
Epoch: [28] Total time: 0:17:19 (0.3702 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 3.8874 (3.7055)  loss_scale: 65536.0000 (60869.8555)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:09:16  loss: 0.3159 (0.3159)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0458  data: 1.8914  max mem: 15572
Val:  [ 10/272]  eta: 0:01:46  loss: 2.3787 (2.2575)  acc1: 44.4444 (43.4343)  acc5: 77.7778 (74.7475)  time: 0.4076  data: 0.2558  max mem: 15572
Val:  [ 20/272]  eta: 0:01:13  loss: 2.2769 (2.2959)  acc1: 44.4444 (46.2963)  acc5: 72.2222 (74.6032)  time: 0.2027  data: 0.0463  max mem: 15572
Val:  [ 30/272]  eta: 0:01:00  loss: 2.2815 (2.3898)  acc1: 50.0000 (42.8315)  acc5: 72.2222 (73.6559)  time: 0.1633  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:52  loss: 2.5219 (2.4448)  acc1: 33.3333 (40.6504)  acc5: 72.2222 (73.8482)  time: 0.1599  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:47  loss: 2.4458 (2.3610)  acc1: 33.3333 (42.5926)  acc5: 77.7778 (75.7081)  time: 0.1564  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:42  loss: 1.4858 (2.2626)  acc1: 61.1111 (45.1730)  acc5: 83.3333 (76.4117)  time: 0.1531  data: 0.0003  max mem: 15572
Val:  [ 70/272]  eta: 0:00:39  loss: 1.5678 (2.1906)  acc1: 66.6667 (47.6526)  acc5: 83.3333 (77.3083)  time: 0.1498  data: 0.0003  max mem: 15572
Val:  [ 80/272]  eta: 0:00:36  loss: 1.8395 (2.1974)  acc1: 55.5556 (47.5309)  acc5: 83.3333 (77.3663)  time: 0.1531  data: 0.0003  max mem: 15572
Val:  [ 90/272]  eta: 0:00:34  loss: 2.0460 (2.1914)  acc1: 50.0000 (48.0464)  acc5: 83.3333 (78.0830)  time: 0.1587  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:31  loss: 2.0337 (2.2105)  acc1: 61.1111 (47.7998)  acc5: 83.3333 (77.9978)  time: 0.1617  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:29  loss: 2.4136 (2.2893)  acc1: 22.2222 (45.6957)  acc5: 77.7778 (76.4765)  time: 0.1625  data: 0.0017  max mem: 15572
Val:  [120/272]  eta: 0:00:27  loss: 3.0528 (2.3319)  acc1: 16.6667 (44.7199)  acc5: 61.1111 (75.8494)  time: 0.1638  data: 0.0016  max mem: 15572
Val:  [130/272]  eta: 0:00:25  loss: 2.1556 (2.2966)  acc1: 44.4444 (45.7167)  acc5: 83.3333 (76.5479)  time: 0.1619  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:23  loss: 1.7505 (2.2904)  acc1: 50.0000 (46.0993)  acc5: 83.3333 (76.2805)  time: 0.1615  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:21  loss: 2.4012 (2.3003)  acc1: 33.3333 (45.4010)  acc5: 77.7778 (76.3061)  time: 0.1657  data: 0.0005  max mem: 15572
Val:  [160/272]  eta: 0:00:19  loss: 2.4012 (2.2900)  acc1: 44.4444 (46.0663)  acc5: 77.7778 (76.5010)  time: 0.1638  data: 0.0005  max mem: 15572
Val:  [170/272]  eta: 0:00:17  loss: 2.4442 (2.3155)  acc1: 44.4444 (45.3866)  acc5: 72.2222 (75.9584)  time: 0.1608  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 2.3511 (2.3039)  acc1: 33.3333 (45.1811)  acc5: 77.7778 (76.3966)  time: 0.1597  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 2.3364 (2.3554)  acc1: 33.3333 (44.0081)  acc5: 77.7778 (75.0727)  time: 0.1565  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:12  loss: 2.5135 (2.3642)  acc1: 27.7778 (43.5600)  acc5: 66.6667 (74.8480)  time: 0.1583  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:10  loss: 2.0529 (2.3691)  acc1: 44.4444 (43.7072)  acc5: 77.7778 (74.7499)  time: 0.1580  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:08  loss: 2.2518 (2.3581)  acc1: 55.5556 (44.0422)  acc5: 72.2222 (74.8617)  time: 0.1585  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.8056 (2.3265)  acc1: 61.1111 (45.0938)  acc5: 83.3333 (75.2285)  time: 0.1586  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.6558 (2.3114)  acc1: 61.1111 (45.3204)  acc5: 83.3333 (75.4034)  time: 0.1523  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.3619 (2.3256)  acc1: 38.8889 (44.6658)  acc5: 77.7778 (75.2103)  time: 0.1483  data: 0.0003  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.3301 (2.2686)  acc1: 66.6667 (46.3602)  acc5: 88.8889 (75.9685)  time: 0.1446  data: 0.0002  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3611 (2.2647)  acc1: 66.6667 (46.3305)  acc5: 88.8889 (76.1788)  time: 0.1368  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3611 (2.2694)  acc1: 66.6667 (46.3035)  acc5: 88.8889 (76.1417)  time: 0.1302  data: 0.0001  max mem: 15572
Val: Total time: 0:00:45 (0.1670 s / it)
* Acc@1 46.304 Acc@5 76.142 loss 2.269
Accuracy of the network on the 4883 val videos: 46.3%
[2025-01-13 07:51:04,225] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 07:51:04,227] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 07:51:04,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 07:51:06,618] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 07:51:06,618] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.30%
Epoch: [29]  [   0/2809]  eta: 3:23:36  lr: 0.000011  min_lr: 0.000000  loss: 4.1015 (4.1015)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.3490  data: 3.9629  max mem: 15572
Epoch: [29]  [  10/2809]  eta: 0:34:29  lr: 0.000011  min_lr: 0.000000  loss: 3.9307 (3.8734)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7392  data: 0.3605  max mem: 15572
Epoch: [29]  [  20/2809]  eta: 0:26:07  lr: 0.000011  min_lr: 0.000000  loss: 3.7636 (3.7300)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [29]  [  30/2809]  eta: 0:23:08  lr: 0.000011  min_lr: 0.000000  loss: 3.6546 (3.6434)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [29]  [  40/2809]  eta: 0:21:36  lr: 0.000011  min_lr: 0.000000  loss: 3.4536 (3.6053)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [29]  [  50/2809]  eta: 0:20:38  lr: 0.000011  min_lr: 0.000000  loss: 3.4440 (3.5821)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [29]  [  60/2809]  eta: 0:19:58  lr: 0.000011  min_lr: 0.000000  loss: 3.5315 (3.6027)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
[2025-01-13 07:51:36,946] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:51:36,946] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [29]  [  70/2809]  eta: 0:19:29  lr: 0.000011  min_lr: 0.000000  loss: 3.7384 (3.6042)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
[2025-01-13 07:51:38,409] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81535
[2025-01-13 07:51:38,409] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:51:38,409] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [  80/2809]  eta: 0:19:04  lr: 0.000011  min_lr: 0.000000  loss: 3.6420 (3.5922)  loss_scale: 65536.0000 (68772.3457)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [29]  [  90/2809]  eta: 0:18:44  lr: 0.000011  min_lr: 0.000000  loss: 3.5993 (3.6013)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [29]  [ 100/2809]  eta: 0:18:28  lr: 0.000010  min_lr: 0.000000  loss: 3.9722 (3.6329)  loss_scale: 65536.0000 (68131.4851)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [29]  [ 110/2809]  eta: 0:18:15  lr: 0.000010  min_lr: 0.000000  loss: 3.9425 (3.6372)  loss_scale: 65536.0000 (67897.6577)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [29]  [ 120/2809]  eta: 0:18:02  lr: 0.000010  min_lr: 0.000000  loss: 3.6494 (3.6334)  loss_scale: 65536.0000 (67702.4793)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [29]  [ 130/2809]  eta: 0:17:51  lr: 0.000010  min_lr: 0.000000  loss: 3.7034 (3.6454)  loss_scale: 65536.0000 (67537.0992)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [29]  [ 140/2809]  eta: 0:17:41  lr: 0.000010  min_lr: 0.000000  loss: 3.7602 (3.6563)  loss_scale: 65536.0000 (67395.1773)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [29]  [ 150/2809]  eta: 0:17:33  lr: 0.000010  min_lr: 0.000000  loss: 3.7340 (3.6510)  loss_scale: 65536.0000 (67272.0530)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0003  max mem: 15572
Epoch: [29]  [ 160/2809]  eta: 0:17:27  lr: 0.000010  min_lr: 0.000000  loss: 3.5791 (3.6472)  loss_scale: 65536.0000 (67164.2236)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0004  max mem: 15572
Epoch: [29]  [ 170/2809]  eta: 0:17:19  lr: 0.000010  min_lr: 0.000000  loss: 3.6926 (3.6551)  loss_scale: 65536.0000 (67069.0058)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0003  max mem: 15572
Epoch: [29]  [ 180/2809]  eta: 0:17:13  lr: 0.000010  min_lr: 0.000000  loss: 3.7724 (3.6550)  loss_scale: 65536.0000 (66984.3094)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0004  max mem: 15572
Epoch: [29]  [ 190/2809]  eta: 0:17:08  lr: 0.000010  min_lr: 0.000000  loss: 3.5964 (3.6562)  loss_scale: 65536.0000 (66908.4817)  weight_decay: 0.0500 (0.0500)  time: 0.3803  data: 0.0004  max mem: 15572
Epoch: [29]  [ 200/2809]  eta: 0:17:03  lr: 0.000010  min_lr: 0.000000  loss: 3.8301 (3.6737)  loss_scale: 65536.0000 (66840.1990)  weight_decay: 0.0500 (0.0500)  time: 0.3865  data: 0.0005  max mem: 15572
[2025-01-13 07:52:26,625] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:52:26,625] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:52:27,359] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81666
[2025-01-13 07:52:27,368] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:52:27,368] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 210/2809]  eta: 0:16:57  lr: 0.000010  min_lr: 0.000000  loss: 3.9203 (3.6838)  loss_scale: 65536.0000 (67399.5829)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0003  max mem: 15572
Epoch: [29]  [ 220/2809]  eta: 0:16:51  lr: 0.000010  min_lr: 0.000000  loss: 3.8985 (3.6950)  loss_scale: 65536.0000 (67315.2579)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [29]  [ 230/2809]  eta: 0:16:45  lr: 0.000010  min_lr: 0.000000  loss: 3.7182 (3.6914)  loss_scale: 65536.0000 (67238.2338)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0003  max mem: 15572
Epoch: [29]  [ 240/2809]  eta: 0:16:39  lr: 0.000010  min_lr: 0.000000  loss: 3.6885 (3.6934)  loss_scale: 65536.0000 (67167.6017)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
Epoch: [29]  [ 250/2809]  eta: 0:16:33  lr: 0.000010  min_lr: 0.000000  loss: 3.6230 (3.6939)  loss_scale: 65536.0000 (67102.5976)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0003  max mem: 15572
Epoch: [29]  [ 260/2809]  eta: 0:16:28  lr: 0.000010  min_lr: 0.000000  loss: 3.6230 (3.6999)  loss_scale: 65536.0000 (67042.5747)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0003  max mem: 15572
Epoch: [29]  [ 270/2809]  eta: 0:16:22  lr: 0.000010  min_lr: 0.000000  loss: 3.6348 (3.6961)  loss_scale: 65536.0000 (66986.9815)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [29]  [ 280/2809]  eta: 0:16:16  lr: 0.000010  min_lr: 0.000000  loss: 3.6348 (3.6945)  loss_scale: 65536.0000 (66935.3452)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [29]  [ 290/2809]  eta: 0:16:11  lr: 0.000010  min_lr: 0.000000  loss: 3.6251 (3.6965)  loss_scale: 65536.0000 (66887.2577)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0003  max mem: 15572
Epoch: [29]  [ 300/2809]  eta: 0:16:06  lr: 0.000010  min_lr: 0.000000  loss: 3.4282 (3.6909)  loss_scale: 65536.0000 (66842.3654)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [29]  [ 310/2809]  eta: 0:16:02  lr: 0.000010  min_lr: 0.000000  loss: 3.8182 (3.6931)  loss_scale: 65536.0000 (66800.3601)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0003  max mem: 15572
Epoch: [29]  [ 320/2809]  eta: 0:15:57  lr: 0.000010  min_lr: 0.000000  loss: 3.8199 (3.6919)  loss_scale: 65536.0000 (66760.9720)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [29]  [ 330/2809]  eta: 0:15:52  lr: 0.000010  min_lr: 0.000000  loss: 3.8354 (3.6934)  loss_scale: 65536.0000 (66723.9637)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
[2025-01-13 07:53:15,303] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:53:15,303] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:53:15,696] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81796
[2025-01-13 07:53:15,696] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:53:15,696] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 340/2809]  eta: 0:15:48  lr: 0.000010  min_lr: 0.000000  loss: 3.7570 (3.6948)  loss_scale: 65536.0000 (66881.3138)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0003  max mem: 15572
Epoch: [29]  [ 350/2809]  eta: 0:15:43  lr: 0.000010  min_lr: 0.000000  loss: 3.8565 (3.7003)  loss_scale: 65536.0000 (66842.9858)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0003  max mem: 15572
Epoch: [29]  [ 360/2809]  eta: 0:15:38  lr: 0.000010  min_lr: 0.000000  loss: 3.8521 (3.7000)  loss_scale: 65536.0000 (66806.7812)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0003  max mem: 15572
Epoch: [29]  [ 370/2809]  eta: 0:15:35  lr: 0.000010  min_lr: 0.000000  loss: 3.6336 (3.6982)  loss_scale: 65536.0000 (66772.5283)  weight_decay: 0.0500 (0.0500)  time: 0.3802  data: 0.0004  max mem: 15572
Epoch: [29]  [ 380/2809]  eta: 0:15:31  lr: 0.000010  min_lr: 0.000000  loss: 3.6336 (3.6966)  loss_scale: 65536.0000 (66740.0735)  weight_decay: 0.0500 (0.0500)  time: 0.3878  data: 0.0005  max mem: 15572
Epoch: [29]  [ 390/2809]  eta: 0:15:29  lr: 0.000010  min_lr: 0.000000  loss: 3.5817 (3.6944)  loss_scale: 65536.0000 (66709.2788)  weight_decay: 0.0500 (0.0500)  time: 0.3963  data: 0.0006  max mem: 15572
Epoch: [29]  [ 400/2809]  eta: 0:15:25  lr: 0.000010  min_lr: 0.000000  loss: 3.6403 (3.6993)  loss_scale: 65536.0000 (66680.0200)  weight_decay: 0.0500 (0.0500)  time: 0.3967  data: 0.0006  max mem: 15572
Epoch: [29]  [ 410/2809]  eta: 0:15:22  lr: 0.000010  min_lr: 0.000000  loss: 3.7708 (3.6972)  loss_scale: 65536.0000 (66652.1849)  weight_decay: 0.0500 (0.0500)  time: 0.3918  data: 0.0007  max mem: 15572
Epoch: [29]  [ 420/2809]  eta: 0:15:18  lr: 0.000010  min_lr: 0.000000  loss: 3.6367 (3.6952)  loss_scale: 65536.0000 (66625.6722)  weight_decay: 0.0500 (0.0500)  time: 0.3928  data: 0.0007  max mem: 15572
Epoch: [29]  [ 430/2809]  eta: 0:15:15  lr: 0.000010  min_lr: 0.000000  loss: 3.5406 (3.6907)  loss_scale: 65536.0000 (66600.3898)  weight_decay: 0.0500 (0.0500)  time: 0.3968  data: 0.0007  max mem: 15572
Epoch: [29]  [ 440/2809]  eta: 0:15:12  lr: 0.000010  min_lr: 0.000000  loss: 3.6188 (3.6925)  loss_scale: 65536.0000 (66576.2540)  weight_decay: 0.0500 (0.0500)  time: 0.3970  data: 0.0006  max mem: 15572
Epoch: [29]  [ 450/2809]  eta: 0:15:09  lr: 0.000010  min_lr: 0.000000  loss: 3.6188 (3.6895)  loss_scale: 65536.0000 (66553.1885)  weight_decay: 0.0500 (0.0500)  time: 0.3932  data: 0.0006  max mem: 15572
Epoch: [29]  [ 460/2809]  eta: 0:15:05  lr: 0.000010  min_lr: 0.000000  loss: 3.7079 (3.6909)  loss_scale: 65536.0000 (66531.1236)  weight_decay: 0.0500 (0.0500)  time: 0.3881  data: 0.0005  max mem: 15572
[2025-01-13 07:54:05,898] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:54:05,898] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:54:06,279] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81926
[2025-01-13 07:54:06,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:54:06,280] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 470/2809]  eta: 0:15:01  lr: 0.000010  min_lr: 0.000000  loss: 3.7080 (3.6903)  loss_scale: 65536.0000 (66649.1380)  weight_decay: 0.0500 (0.0500)  time: 0.3886  data: 0.0004  max mem: 15572
Epoch: [29]  [ 480/2809]  eta: 0:14:57  lr: 0.000010  min_lr: 0.000000  loss: 3.8458 (3.6997)  loss_scale: 65536.0000 (66625.9958)  weight_decay: 0.0500 (0.0500)  time: 0.3856  data: 0.0004  max mem: 15572
Epoch: [29]  [ 490/2809]  eta: 0:14:53  lr: 0.000010  min_lr: 0.000000  loss: 3.9122 (3.6996)  loss_scale: 65536.0000 (66603.7963)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0003  max mem: 15572
Epoch: [29]  [ 500/2809]  eta: 0:14:48  lr: 0.000010  min_lr: 0.000000  loss: 3.8126 (3.7024)  loss_scale: 65536.0000 (66582.4830)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0003  max mem: 15572
Epoch: [29]  [ 510/2809]  eta: 0:14:44  lr: 0.000010  min_lr: 0.000000  loss: 3.9252 (3.7060)  loss_scale: 65536.0000 (66562.0039)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [29]  [ 520/2809]  eta: 0:14:39  lr: 0.000010  min_lr: 0.000000  loss: 3.8401 (3.7074)  loss_scale: 65536.0000 (66542.3109)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [29]  [ 530/2809]  eta: 0:14:35  lr: 0.000010  min_lr: 0.000000  loss: 3.8401 (3.7096)  loss_scale: 65536.0000 (66523.3597)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0003  max mem: 15572
[2025-01-13 07:54:33,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=82000, skipped=557, lr=[9.909992081020063e-08, 9.909992081020063e-08, 1.4157131544314378e-07, 1.4157131544314378e-07, 2.0224473634734827e-07, 2.0224473634734827e-07, 2.889210519247833e-07, 2.889210519247833e-07, 4.1274435989254753e-07, 4.1274435989254753e-07, 5.896347998464965e-07, 5.896347998464965e-07, 8.423354283521379e-07, 8.423354283521379e-07, 1.20333632621734e-06, 1.20333632621734e-06, 1.7190518945962e-06, 1.7190518945962e-06, 2.4557884208517147e-06, 2.4557884208517147e-06, 3.5082691726453065e-06, 3.5082691726453065e-06, 5.01181310377901e-06, 5.01181310377901e-06, 7.159733005398586e-06, 7.159733005398586e-06, 1.0228190007712266e-05, 1.0228190007712266e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 07:54:33,556] [INFO] [timer.py:260:stop] epoch=0/micro_step=82000/global_step=82000, RunningAvgSamplesPerSec=30.907193782816083, CurrSamplesPerSec=34.934508097557995, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [ 540/2809]  eta: 0:14:30  lr: 0.000010  min_lr: 0.000000  loss: 3.9490 (3.7148)  loss_scale: 65536.0000 (66505.1091)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0003  max mem: 15572
Epoch: [29]  [ 550/2809]  eta: 0:14:26  lr: 0.000010  min_lr: 0.000000  loss: 3.9617 (3.7186)  loss_scale: 65536.0000 (66487.5209)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
Epoch: [29]  [ 560/2809]  eta: 0:14:21  lr: 0.000010  min_lr: 0.000000  loss: 3.8517 (3.7189)  loss_scale: 65536.0000 (66470.5597)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0003  max mem: 15572
Epoch: [29]  [ 570/2809]  eta: 0:14:17  lr: 0.000010  min_lr: 0.000000  loss: 3.6555 (3.7174)  loss_scale: 65536.0000 (66454.1926)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0003  max mem: 15572
Epoch: [29]  [ 580/2809]  eta: 0:14:13  lr: 0.000010  min_lr: 0.000000  loss: 3.7029 (3.7176)  loss_scale: 65536.0000 (66438.3890)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [29]  [ 590/2809]  eta: 0:14:08  lr: 0.000010  min_lr: 0.000000  loss: 3.8743 (3.7219)  loss_scale: 65536.0000 (66423.1201)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
[2025-01-13 07:54:54,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:54:54,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:54:54,639] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82056
[2025-01-13 07:54:54,640] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:54:54,640] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 600/2809]  eta: 0:14:04  lr: 0.000010  min_lr: 0.000000  loss: 3.8743 (3.7242)  loss_scale: 65536.0000 (66517.4043)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [29]  [ 610/2809]  eta: 0:14:00  lr: 0.000010  min_lr: 0.000000  loss: 3.8624 (3.7239)  loss_scale: 65536.0000 (66501.3421)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [29]  [ 620/2809]  eta: 0:13:56  lr: 0.000010  min_lr: 0.000000  loss: 3.9115 (3.7273)  loss_scale: 65536.0000 (66485.7971)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [29]  [ 630/2809]  eta: 0:13:51  lr: 0.000010  min_lr: 0.000000  loss: 3.9293 (3.7303)  loss_scale: 65536.0000 (66470.7448)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [29]  [ 640/2809]  eta: 0:13:47  lr: 0.000010  min_lr: 0.000000  loss: 3.8164 (3.7301)  loss_scale: 65536.0000 (66456.1622)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [29]  [ 650/2809]  eta: 0:13:43  lr: 0.000010  min_lr: 0.000000  loss: 3.8390 (3.7306)  loss_scale: 65536.0000 (66442.0276)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [29]  [ 660/2809]  eta: 0:13:39  lr: 0.000010  min_lr: 0.000000  loss: 3.8710 (3.7308)  loss_scale: 65536.0000 (66428.3207)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [29]  [ 670/2809]  eta: 0:13:34  lr: 0.000010  min_lr: 0.000000  loss: 3.8061 (3.7326)  loss_scale: 65536.0000 (66415.0224)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [29]  [ 680/2809]  eta: 0:13:30  lr: 0.000010  min_lr: 0.000000  loss: 3.8061 (3.7317)  loss_scale: 65536.0000 (66402.1145)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [29]  [ 690/2809]  eta: 0:13:26  lr: 0.000010  min_lr: 0.000000  loss: 3.5971 (3.7308)  loss_scale: 65536.0000 (66389.5803)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [29]  [ 700/2809]  eta: 0:13:22  lr: 0.000010  min_lr: 0.000000  loss: 3.5281 (3.7296)  loss_scale: 65536.0000 (66377.4037)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0013  max mem: 15572
Epoch: [29]  [ 710/2809]  eta: 0:13:18  lr: 0.000010  min_lr: 0.000000  loss: 3.6526 (3.7309)  loss_scale: 65536.0000 (66365.5696)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0013  max mem: 15572
Epoch: [29]  [ 720/2809]  eta: 0:13:14  lr: 0.000010  min_lr: 0.000000  loss: 3.7498 (3.7283)  loss_scale: 65536.0000 (66354.0638)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
[2025-01-13 07:55:42,481] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:55:42,481] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:55:44,708] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82191
[2025-01-13 07:55:44,708] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:55:44,708] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 730/2809]  eta: 0:13:10  lr: 0.000010  min_lr: 0.000000  loss: 3.6310 (3.7273)  loss_scale: 65536.0000 (66880.7880)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [29]  [ 740/2809]  eta: 0:13:06  lr: 0.000010  min_lr: 0.000000  loss: 3.5841 (3.7255)  loss_scale: 65536.0000 (66862.6397)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0002  max mem: 15572
Epoch: [29]  [ 750/2809]  eta: 0:13:02  lr: 0.000010  min_lr: 0.000000  loss: 3.5713 (3.7210)  loss_scale: 65536.0000 (66844.9747)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [29]  [ 760/2809]  eta: 0:12:58  lr: 0.000010  min_lr: 0.000000  loss: 3.5726 (3.7202)  loss_scale: 65536.0000 (66827.7740)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [29]  [ 770/2809]  eta: 0:12:54  lr: 0.000010  min_lr: 0.000000  loss: 3.5726 (3.7195)  loss_scale: 65536.0000 (66811.0195)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0003  max mem: 15572
Epoch: [29]  [ 780/2809]  eta: 0:12:50  lr: 0.000010  min_lr: 0.000000  loss: 3.4656 (3.7167)  loss_scale: 65536.0000 (66794.6940)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [29]  [ 790/2809]  eta: 0:12:46  lr: 0.000010  min_lr: 0.000000  loss: 3.4118 (3.7159)  loss_scale: 65536.0000 (66778.7813)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [29]  [ 800/2809]  eta: 0:12:42  lr: 0.000010  min_lr: 0.000000  loss: 3.7657 (3.7173)  loss_scale: 65536.0000 (66763.2659)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [29]  [ 810/2809]  eta: 0:12:38  lr: 0.000010  min_lr: 0.000000  loss: 3.7970 (3.7177)  loss_scale: 65536.0000 (66748.1332)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [29]  [ 820/2809]  eta: 0:12:34  lr: 0.000010  min_lr: 0.000000  loss: 3.6729 (3.7151)  loss_scale: 65536.0000 (66733.3691)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [29]  [ 830/2809]  eta: 0:12:30  lr: 0.000010  min_lr: 0.000000  loss: 3.5168 (3.7140)  loss_scale: 65536.0000 (66718.9603)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [29]  [ 840/2809]  eta: 0:12:26  lr: 0.000010  min_lr: 0.000000  loss: 3.8475 (3.7157)  loss_scale: 65536.0000 (66704.8942)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [29]  [ 850/2809]  eta: 0:12:22  lr: 0.000010  min_lr: 0.000000  loss: 3.8364 (3.7150)  loss_scale: 65536.0000 (66691.1586)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
[2025-01-13 07:56:32,707] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:56:32,707] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:56:33,089] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82321
[2025-01-13 07:56:33,089] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:56:33,089] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 860/2809]  eta: 0:12:18  lr: 0.000010  min_lr: 0.000000  loss: 3.8285 (3.7156)  loss_scale: 65536.0000 (66753.8583)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [29]  [ 870/2809]  eta: 0:12:14  lr: 0.000010  min_lr: 0.000000  loss: 3.8527 (3.7163)  loss_scale: 65536.0000 (66739.8760)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [29]  [ 880/2809]  eta: 0:12:10  lr: 0.000010  min_lr: 0.000000  loss: 3.7018 (3.7146)  loss_scale: 65536.0000 (66726.2111)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [29]  [ 890/2809]  eta: 0:12:06  lr: 0.000010  min_lr: 0.000000  loss: 3.5828 (3.7142)  loss_scale: 65536.0000 (66712.8530)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [29]  [ 900/2809]  eta: 0:12:02  lr: 0.000010  min_lr: 0.000000  loss: 3.7642 (3.7147)  loss_scale: 65536.0000 (66699.7913)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [29]  [ 910/2809]  eta: 0:11:58  lr: 0.000010  min_lr: 0.000000  loss: 3.9547 (3.7160)  loss_scale: 65536.0000 (66687.0165)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [29]  [ 920/2809]  eta: 0:11:54  lr: 0.000010  min_lr: 0.000000  loss: 3.8795 (3.7148)  loss_scale: 65536.0000 (66674.5190)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [29]  [ 930/2809]  eta: 0:11:50  lr: 0.000010  min_lr: 0.000000  loss: 3.8795 (3.7153)  loss_scale: 65536.0000 (66662.2900)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [29]  [ 940/2809]  eta: 0:11:46  lr: 0.000010  min_lr: 0.000000  loss: 3.8223 (3.7145)  loss_scale: 65536.0000 (66650.3209)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [29]  [ 950/2809]  eta: 0:11:43  lr: 0.000010  min_lr: 0.000000  loss: 3.6974 (3.7168)  loss_scale: 65536.0000 (66638.6036)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [29]  [ 960/2809]  eta: 0:11:39  lr: 0.000010  min_lr: 0.000000  loss: 3.8997 (3.7197)  loss_scale: 65536.0000 (66627.1301)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [29]  [ 970/2809]  eta: 0:11:35  lr: 0.000010  min_lr: 0.000000  loss: 3.8401 (3.7193)  loss_scale: 65536.0000 (66615.8929)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0002  max mem: 15572
Epoch: [29]  [ 980/2809]  eta: 0:11:31  lr: 0.000010  min_lr: 0.000000  loss: 3.6681 (3.7194)  loss_scale: 65536.0000 (66604.8848)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
[2025-01-13 07:57:21,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:57:21,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [29]  [ 990/2809]  eta: 0:11:27  lr: 0.000010  min_lr: 0.000000  loss: 3.9366 (3.7224)  loss_scale: 65536.0000 (66726.3613)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
[2025-01-13 07:57:23,959] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82458
[2025-01-13 07:57:23,959] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:57:23,959] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [1000/2809]  eta: 0:11:23  lr: 0.000010  min_lr: 0.000000  loss: 3.8900 (3.7237)  loss_scale: 65536.0000 (67107.2927)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [29]  [1010/2809]  eta: 0:11:19  lr: 0.000010  min_lr: 0.000000  loss: 3.8185 (3.7221)  loss_scale: 65536.0000 (67091.7507)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [29]  [1020/2809]  eta: 0:11:16  lr: 0.000010  min_lr: 0.000000  loss: 3.4672 (3.7209)  loss_scale: 65536.0000 (67076.5132)  weight_decay: 0.0500 (0.0500)  time: 0.3780  data: 0.0002  max mem: 15572
Epoch: [29]  [1030/2809]  eta: 0:11:12  lr: 0.000010  min_lr: 0.000000  loss: 3.5087 (3.7200)  loss_scale: 65536.0000 (67061.5713)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [29]  [1040/2809]  eta: 0:11:08  lr: 0.000010  min_lr: 0.000000  loss: 3.6208 (3.7204)  loss_scale: 65536.0000 (67046.9164)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [29]  [1050/2809]  eta: 0:11:04  lr: 0.000010  min_lr: 0.000000  loss: 3.8904 (3.7229)  loss_scale: 65536.0000 (67032.5404)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [29]  [1060/2809]  eta: 0:11:00  lr: 0.000010  min_lr: 0.000000  loss: 3.8429 (3.7235)  loss_scale: 65536.0000 (67018.4354)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [29]  [1070/2809]  eta: 0:10:56  lr: 0.000010  min_lr: 0.000000  loss: 3.7165 (3.7231)  loss_scale: 65536.0000 (67004.5938)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [29]  [1080/2809]  eta: 0:10:52  lr: 0.000010  min_lr: 0.000000  loss: 3.9881 (3.7243)  loss_scale: 65536.0000 (66991.0083)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0003  max mem: 15572
Epoch: [29]  [1090/2809]  eta: 0:10:48  lr: 0.000010  min_lr: 0.000000  loss: 3.9881 (3.7269)  loss_scale: 65536.0000 (66977.6719)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0002  max mem: 15572
Epoch: [29]  [1100/2809]  eta: 0:10:45  lr: 0.000010  min_lr: 0.000000  loss: 3.7661 (3.7251)  loss_scale: 65536.0000 (66964.5777)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [29]  [1110/2809]  eta: 0:10:41  lr: 0.000010  min_lr: 0.000000  loss: 3.6156 (3.7236)  loss_scale: 65536.0000 (66951.7192)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [29]  [1120/2809]  eta: 0:10:37  lr: 0.000010  min_lr: 0.000000  loss: 3.5460 (3.7224)  loss_scale: 65536.0000 (66939.0901)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0012  max mem: 15572
[2025-01-13 07:58:12,143] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:58:12,143] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:58:12,878] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82589
[2025-01-13 07:58:12,879] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:58:12,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [1130/2809]  eta: 0:10:33  lr: 0.000010  min_lr: 0.000000  loss: 3.7741 (3.7242)  loss_scale: 65536.0000 (67042.5747)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0012  max mem: 15572
Epoch: [29]  [1140/2809]  eta: 0:10:29  lr: 0.000010  min_lr: 0.000000  loss: 4.0275 (3.7254)  loss_scale: 65536.0000 (67029.3707)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [29]  [1150/2809]  eta: 0:10:25  lr: 0.000010  min_lr: 0.000000  loss: 3.9167 (3.7262)  loss_scale: 65536.0000 (67016.3962)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [29]  [1160/2809]  eta: 0:10:22  lr: 0.000010  min_lr: 0.000000  loss: 3.9004 (3.7271)  loss_scale: 65536.0000 (67003.6451)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [29]  [1170/2809]  eta: 0:10:18  lr: 0.000010  min_lr: 0.000000  loss: 3.9516 (3.7287)  loss_scale: 65536.0000 (66991.1119)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [29]  [1180/2809]  eta: 0:10:14  lr: 0.000010  min_lr: 0.000000  loss: 3.8496 (3.7279)  loss_scale: 65536.0000 (66978.7909)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [29]  [1190/2809]  eta: 0:10:10  lr: 0.000010  min_lr: 0.000000  loss: 3.7007 (3.7286)  loss_scale: 65536.0000 (66966.6767)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0003  max mem: 15572
Epoch: [29]  [1200/2809]  eta: 0:10:06  lr: 0.000010  min_lr: 0.000000  loss: 3.6031 (3.7266)  loss_scale: 65536.0000 (66954.7644)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [29]  [1210/2809]  eta: 0:10:02  lr: 0.000010  min_lr: 0.000000  loss: 3.5138 (3.7253)  loss_scale: 65536.0000 (66943.0487)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
Epoch: [29]  [1220/2809]  eta: 0:09:58  lr: 0.000010  min_lr: 0.000000  loss: 3.8026 (3.7264)  loss_scale: 65536.0000 (66931.5250)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [29]  [1230/2809]  eta: 0:09:54  lr: 0.000010  min_lr: 0.000000  loss: 3.8026 (3.7269)  loss_scale: 65536.0000 (66920.1885)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [29]  [1240/2809]  eta: 0:09:51  lr: 0.000010  min_lr: 0.000000  loss: 3.8472 (3.7280)  loss_scale: 65536.0000 (66909.0346)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0003  max mem: 15572
Epoch: [29]  [1250/2809]  eta: 0:09:47  lr: 0.000010  min_lr: 0.000000  loss: 3.7711 (3.7268)  loss_scale: 65536.0000 (66898.0592)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
[2025-01-13 07:59:00,610] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:59:00,610] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:59:00,981] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82719
[2025-01-13 07:59:00,981] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:59:00,981] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [1260/2809]  eta: 0:09:43  lr: 0.000010  min_lr: 0.000000  loss: 3.6898 (3.7272)  loss_scale: 65536.0000 (66939.2292)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [29]  [1270/2809]  eta: 0:09:39  lr: 0.000010  min_lr: 0.000000  loss: 3.7043 (3.7251)  loss_scale: 65536.0000 (66928.1888)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [29]  [1280/2809]  eta: 0:09:35  lr: 0.000010  min_lr: 0.000000  loss: 3.6814 (3.7239)  loss_scale: 65536.0000 (66917.3208)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [29]  [1290/2809]  eta: 0:09:31  lr: 0.000010  min_lr: 0.000000  loss: 3.6897 (3.7248)  loss_scale: 65536.0000 (66906.6212)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [29]  [1300/2809]  eta: 0:09:28  lr: 0.000010  min_lr: 0.000000  loss: 3.8673 (3.7255)  loss_scale: 65536.0000 (66896.0861)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [29]  [1310/2809]  eta: 0:09:24  lr: 0.000010  min_lr: 0.000000  loss: 3.7831 (3.7255)  loss_scale: 65536.0000 (66885.7117)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [29]  [1320/2809]  eta: 0:09:20  lr: 0.000010  min_lr: 0.000000  loss: 3.7479 (3.7248)  loss_scale: 65536.0000 (66875.4943)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0003  max mem: 15572
Epoch: [29]  [1330/2809]  eta: 0:09:16  lr: 0.000010  min_lr: 0.000000  loss: 3.7675 (3.7241)  loss_scale: 65536.0000 (66865.4305)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
Epoch: [29]  [1340/2809]  eta: 0:09:12  lr: 0.000010  min_lr: 0.000000  loss: 3.6734 (3.7243)  loss_scale: 65536.0000 (66855.5168)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0003  max mem: 15572
Epoch: [29]  [1350/2809]  eta: 0:09:08  lr: 0.000010  min_lr: 0.000000  loss: 3.6734 (3.7243)  loss_scale: 65536.0000 (66845.7498)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [29]  [1360/2809]  eta: 0:09:05  lr: 0.000010  min_lr: 0.000000  loss: 3.5549 (3.7233)  loss_scale: 65536.0000 (66836.1264)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [29]  [1370/2809]  eta: 0:09:01  lr: 0.000010  min_lr: 0.000000  loss: 3.7002 (3.7237)  loss_scale: 65536.0000 (66826.6433)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [29]  [1380/2809]  eta: 0:08:57  lr: 0.000010  min_lr: 0.000000  loss: 3.7156 (3.7245)  loss_scale: 65536.0000 (66817.2976)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
[2025-01-13 07:59:48,897] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 07:59:48,897] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 07:59:49,626] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82850
[2025-01-13 07:59:49,627] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 07:59:49,627] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [1390/2809]  eta: 0:08:53  lr: 0.000010  min_lr: 0.000000  loss: 3.9723 (3.7264)  loss_scale: 65536.0000 (66902.3149)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [29]  [1400/2809]  eta: 0:08:49  lr: 0.000010  min_lr: 0.000000  loss: 3.8039 (3.7258)  loss_scale: 65536.0000 (66892.5625)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [29]  [1410/2809]  eta: 0:08:46  lr: 0.000010  min_lr: 0.000000  loss: 3.5949 (3.7246)  loss_scale: 65536.0000 (66882.9483)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [29]  [1420/2809]  eta: 0:08:42  lr: 0.000010  min_lr: 0.000000  loss: 3.6630 (3.7256)  loss_scale: 65536.0000 (66873.4694)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [29]  [1430/2809]  eta: 0:08:38  lr: 0.000010  min_lr: 0.000000  loss: 3.7918 (3.7256)  loss_scale: 65536.0000 (66864.1230)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [29]  [1440/2809]  eta: 0:08:34  lr: 0.000010  min_lr: 0.000000  loss: 3.6007 (3.7253)  loss_scale: 65536.0000 (66854.9063)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [29]  [1450/2809]  eta: 0:08:30  lr: 0.000010  min_lr: 0.000000  loss: 3.6693 (3.7247)  loss_scale: 65536.0000 (66845.8167)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [29]  [1460/2809]  eta: 0:08:27  lr: 0.000010  min_lr: 0.000000  loss: 3.5521 (3.7234)  loss_scale: 65536.0000 (66836.8515)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
[2025-01-13 08:00:19,282] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82930
[2025-01-13 08:00:19,282] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:00:19,282] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1470/2809]  eta: 0:08:23  lr: 0.000010  min_lr: 0.000000  loss: 3.7593 (3.7245)  loss_scale: 65536.0000 (66783.4562)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [29]  [1480/2809]  eta: 0:08:19  lr: 0.000010  min_lr: 0.000000  loss: 3.8561 (3.7254)  loss_scale: 32768.0000 (66553.7772)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [29]  [1490/2809]  eta: 0:08:15  lr: 0.000010  min_lr: 0.000000  loss: 3.8561 (3.7255)  loss_scale: 32768.0000 (66327.1791)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [29]  [1500/2809]  eta: 0:08:11  lr: 0.000010  min_lr: 0.000000  loss: 3.5059 (3.7229)  loss_scale: 32768.0000 (66103.6003)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [29]  [1510/2809]  eta: 0:08:07  lr: 0.000010  min_lr: 0.000000  loss: 3.5059 (3.7235)  loss_scale: 32768.0000 (65882.9808)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [29]  [1520/2809]  eta: 0:08:04  lr: 0.000010  min_lr: 0.000000  loss: 3.6145 (3.7222)  loss_scale: 32768.0000 (65665.2623)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [29]  [1530/2809]  eta: 0:08:00  lr: 0.000010  min_lr: 0.000000  loss: 3.6896 (3.7218)  loss_scale: 32768.0000 (65450.3880)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
[2025-01-13 08:00:44,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=83000, skipped=565, lr=[9.318606680013476e-08, 9.318606680013476e-08, 1.331229525716211e-07, 1.331229525716211e-07, 1.901756465308873e-07, 1.901756465308873e-07, 2.7167949504412473e-07, 2.7167949504412473e-07, 3.8811356434874963e-07, 3.8811356434874963e-07, 5.544479490696423e-07, 5.544479490696423e-07, 7.920684986709176e-07, 7.920684986709176e-07, 1.1315264266727396e-06, 1.1315264266727396e-06, 1.6164663238181993e-06, 1.6164663238181993e-06, 2.309237605454571e-06, 2.309237605454571e-06, 3.2989108649351014e-06, 3.2989108649351014e-06, 4.712729807050145e-06, 4.712729807050145e-06, 6.732471152928779e-06, 6.732471152928779e-06, 9.6178159327554e-06, 9.6178159327554e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 08:00:44,721] [INFO] [timer.py:260:stop] epoch=0/micro_step=83000/global_step=83000, RunningAvgSamplesPerSec=30.943309254688415, CurrSamplesPerSec=34.583260957992984, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [1540/2809]  eta: 0:07:56  lr: 0.000010  min_lr: 0.000000  loss: 3.7380 (3.7231)  loss_scale: 32768.0000 (65238.3024)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [29]  [1550/2809]  eta: 0:07:52  lr: 0.000010  min_lr: 0.000000  loss: 3.7643 (3.7214)  loss_scale: 32768.0000 (65028.9516)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [29]  [1560/2809]  eta: 0:07:48  lr: 0.000010  min_lr: 0.000000  loss: 3.6339 (3.7214)  loss_scale: 32768.0000 (64822.2832)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [29]  [1570/2809]  eta: 0:07:45  lr: 0.000010  min_lr: 0.000000  loss: 3.5509 (3.7206)  loss_scale: 32768.0000 (64618.2457)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [29]  [1580/2809]  eta: 0:07:41  lr: 0.000010  min_lr: 0.000000  loss: 3.6926 (3.7205)  loss_scale: 32768.0000 (64416.7894)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [29]  [1590/2809]  eta: 0:07:37  lr: 0.000010  min_lr: 0.000000  loss: 3.6926 (3.7193)  loss_scale: 32768.0000 (64217.8655)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
[2025-01-13 08:01:06,986] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:01:06,986] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [1600/2809]  eta: 0:07:33  lr: 0.000010  min_lr: 0.000000  loss: 3.3824 (3.7182)  loss_scale: 32768.0000 (64082.8282)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0003  max mem: 15572
Epoch: [29]  [1610/2809]  eta: 0:07:29  lr: 0.000010  min_lr: 0.000000  loss: 3.5868 (3.7180)  loss_scale: 65536.0000 (64091.8485)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [29]  [1620/2809]  eta: 0:07:26  lr: 0.000010  min_lr: 0.000000  loss: 3.6562 (3.7180)  loss_scale: 65536.0000 (64100.7576)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0003  max mem: 15572
Epoch: [29]  [1630/2809]  eta: 0:07:22  lr: 0.000010  min_lr: 0.000000  loss: 3.7930 (3.7185)  loss_scale: 65536.0000 (64109.5573)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0003  max mem: 15572
Epoch: [29]  [1640/2809]  eta: 0:07:18  lr: 0.000010  min_lr: 0.000000  loss: 3.6663 (3.7170)  loss_scale: 65536.0000 (64118.2498)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
[2025-01-13 08:01:25,841] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83110
[2025-01-13 08:01:25,841] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:01:25,841] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1650/2809]  eta: 0:07:14  lr: 0.000010  min_lr: 0.000000  loss: 3.7344 (3.7180)  loss_scale: 65536.0000 (64087.1423)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [29]  [1660/2809]  eta: 0:07:11  lr: 0.000010  min_lr: 0.000000  loss: 3.8661 (3.7186)  loss_scale: 32768.0000 (63898.5864)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [29]  [1670/2809]  eta: 0:07:07  lr: 0.000010  min_lr: 0.000000  loss: 3.8805 (3.7194)  loss_scale: 32768.0000 (63712.2873)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [29]  [1680/2809]  eta: 0:07:03  lr: 0.000010  min_lr: 0.000000  loss: 3.8579 (3.7188)  loss_scale: 32768.0000 (63528.2046)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [29]  [1690/2809]  eta: 0:06:59  lr: 0.000010  min_lr: 0.000000  loss: 3.5384 (3.7177)  loss_scale: 32768.0000 (63346.2992)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [29]  [1700/2809]  eta: 0:06:55  lr: 0.000010  min_lr: 0.000000  loss: 3.5058 (3.7171)  loss_scale: 32768.0000 (63166.5326)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [29]  [1710/2809]  eta: 0:06:52  lr: 0.000010  min_lr: 0.000000  loss: 3.6923 (3.7171)  loss_scale: 32768.0000 (62988.8673)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [29]  [1720/2809]  eta: 0:06:48  lr: 0.000010  min_lr: 0.000000  loss: 3.6923 (3.7162)  loss_scale: 32768.0000 (62813.2667)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [29]  [1730/2809]  eta: 0:06:44  lr: 0.000010  min_lr: 0.000000  loss: 3.6939 (3.7165)  loss_scale: 32768.0000 (62639.6950)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0004  max mem: 15572
Epoch: [29]  [1740/2809]  eta: 0:06:40  lr: 0.000009  min_lr: 0.000000  loss: 3.6255 (3.7161)  loss_scale: 32768.0000 (62468.1172)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0004  max mem: 15572
Epoch: [29]  [1750/2809]  eta: 0:06:36  lr: 0.000009  min_lr: 0.000000  loss: 3.6958 (3.7172)  loss_scale: 32768.0000 (62298.4991)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [29]  [1760/2809]  eta: 0:06:33  lr: 0.000009  min_lr: 0.000000  loss: 3.8839 (3.7182)  loss_scale: 32768.0000 (62130.8075)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0003  max mem: 15572
Epoch: [29]  [1770/2809]  eta: 0:06:29  lr: 0.000009  min_lr: 0.000000  loss: 3.8155 (3.7182)  loss_scale: 32768.0000 (61965.0096)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
[2025-01-13 08:02:13,683] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:02:13,684] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [1780/2809]  eta: 0:06:25  lr: 0.000009  min_lr: 0.000000  loss: 3.7662 (3.7188)  loss_scale: 32768.0000 (61856.2695)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [29]  [1790/2809]  eta: 0:06:21  lr: 0.000009  min_lr: 0.000000  loss: 3.7098 (3.7174)  loss_scale: 65536.0000 (61876.8152)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
Epoch: [29]  [1800/2809]  eta: 0:06:18  lr: 0.000009  min_lr: 0.000000  loss: 3.9224 (3.7189)  loss_scale: 65536.0000 (61897.1327)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [29]  [1810/2809]  eta: 0:06:14  lr: 0.000009  min_lr: 0.000000  loss: 4.0676 (3.7197)  loss_scale: 65536.0000 (61917.2258)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [29]  [1820/2809]  eta: 0:06:10  lr: 0.000009  min_lr: 0.000000  loss: 3.9017 (3.7201)  loss_scale: 65536.0000 (61937.0983)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [29]  [1830/2809]  eta: 0:06:06  lr: 0.000009  min_lr: 0.000000  loss: 3.9017 (3.7211)  loss_scale: 65536.0000 (61956.7537)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [29]  [1840/2809]  eta: 0:06:03  lr: 0.000009  min_lr: 0.000000  loss: 3.7507 (3.7210)  loss_scale: 65536.0000 (61976.1955)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [29]  [1850/2809]  eta: 0:05:59  lr: 0.000009  min_lr: 0.000000  loss: 3.7861 (3.7214)  loss_scale: 65536.0000 (61995.4273)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [29]  [1860/2809]  eta: 0:05:55  lr: 0.000009  min_lr: 0.000000  loss: 3.5950 (3.7207)  loss_scale: 65536.0000 (62014.4524)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [29]  [1870/2809]  eta: 0:05:51  lr: 0.000009  min_lr: 0.000000  loss: 3.4932 (3.7210)  loss_scale: 65536.0000 (62033.2742)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [29]  [1880/2809]  eta: 0:05:47  lr: 0.000009  min_lr: 0.000000  loss: 3.6791 (3.7199)  loss_scale: 65536.0000 (62051.8958)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
[2025-01-13 08:02:54,797] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83350
[2025-01-13 08:02:54,797] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:02:54,797] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1890/2809]  eta: 0:05:44  lr: 0.000009  min_lr: 0.000000  loss: 3.6263 (3.7196)  loss_scale: 65536.0000 (62035.6637)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0003  max mem: 15572
Epoch: [29]  [1900/2809]  eta: 0:05:40  lr: 0.000009  min_lr: 0.000000  loss: 3.5688 (3.7189)  loss_scale: 32768.0000 (61881.7044)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [29]  [1910/2809]  eta: 0:05:36  lr: 0.000009  min_lr: 0.000000  loss: 3.5688 (3.7186)  loss_scale: 32768.0000 (61729.3564)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [29]  [1920/2809]  eta: 0:05:32  lr: 0.000009  min_lr: 0.000000  loss: 3.6081 (3.7178)  loss_scale: 32768.0000 (61578.5945)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [29]  [1930/2809]  eta: 0:05:29  lr: 0.000009  min_lr: 0.000000  loss: 3.5619 (3.7171)  loss_scale: 32768.0000 (61429.3941)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [29]  [1940/2809]  eta: 0:05:25  lr: 0.000009  min_lr: 0.000000  loss: 3.7686 (3.7171)  loss_scale: 32768.0000 (61281.7311)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [29]  [1950/2809]  eta: 0:05:21  lr: 0.000009  min_lr: 0.000000  loss: 3.7686 (3.7175)  loss_scale: 32768.0000 (61135.5818)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [29]  [1960/2809]  eta: 0:05:17  lr: 0.000009  min_lr: 0.000000  loss: 3.8480 (3.7181)  loss_scale: 32768.0000 (60990.9230)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0003  max mem: 15572
Epoch: [29]  [1970/2809]  eta: 0:05:14  lr: 0.000009  min_lr: 0.000000  loss: 3.6771 (3.7171)  loss_scale: 32768.0000 (60847.7321)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0003  max mem: 15572
Epoch: [29]  [1980/2809]  eta: 0:05:10  lr: 0.000009  min_lr: 0.000000  loss: 3.5735 (3.7169)  loss_scale: 32768.0000 (60705.9869)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [29]  [1990/2809]  eta: 0:05:06  lr: 0.000009  min_lr: 0.000000  loss: 3.7458 (3.7169)  loss_scale: 32768.0000 (60565.6655)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [29]  [2000/2809]  eta: 0:05:02  lr: 0.000009  min_lr: 0.000000  loss: 3.7754 (3.7173)  loss_scale: 32768.0000 (60426.7466)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [29]  [2010/2809]  eta: 0:04:59  lr: 0.000009  min_lr: 0.000000  loss: 3.7892 (3.7184)  loss_scale: 32768.0000 (60289.2093)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
[2025-01-13 08:03:42,583] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:03:42,583] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [2020/2809]  eta: 0:04:55  lr: 0.000009  min_lr: 0.000000  loss: 3.7635 (3.7177)  loss_scale: 32768.0000 (60201.6744)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [29]  [2030/2809]  eta: 0:04:51  lr: 0.000009  min_lr: 0.000000  loss: 3.6293 (3.7181)  loss_scale: 65536.0000 (60227.9389)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0003  max mem: 15572
Epoch: [29]  [2040/2809]  eta: 0:04:47  lr: 0.000009  min_lr: 0.000000  loss: 3.8403 (3.7185)  loss_scale: 65536.0000 (60253.9461)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
Epoch: [29]  [2050/2809]  eta: 0:04:44  lr: 0.000009  min_lr: 0.000000  loss: 3.8180 (3.7184)  loss_scale: 65536.0000 (60279.6997)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0002  max mem: 15572
Epoch: [29]  [2060/2809]  eta: 0:04:40  lr: 0.000009  min_lr: 0.000000  loss: 3.7007 (3.7190)  loss_scale: 65536.0000 (60305.2033)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0015  max mem: 15572
Epoch: [29]  [2070/2809]  eta: 0:04:36  lr: 0.000009  min_lr: 0.000000  loss: 3.7007 (3.7191)  loss_scale: 65536.0000 (60330.4606)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0015  max mem: 15572
Epoch: [29]  [2080/2809]  eta: 0:04:32  lr: 0.000009  min_lr: 0.000000  loss: 3.6165 (3.7190)  loss_scale: 65536.0000 (60355.4753)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [29]  [2090/2809]  eta: 0:04:29  lr: 0.000009  min_lr: 0.000000  loss: 3.6165 (3.7181)  loss_scale: 65536.0000 (60380.2506)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [29]  [2100/2809]  eta: 0:04:25  lr: 0.000009  min_lr: 0.000000  loss: 3.8428 (3.7192)  loss_scale: 65536.0000 (60404.7901)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [29]  [2110/2809]  eta: 0:04:21  lr: 0.000009  min_lr: 0.000000  loss: 3.8721 (3.7190)  loss_scale: 65536.0000 (60429.0971)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [29]  [2120/2809]  eta: 0:04:17  lr: 0.000009  min_lr: 0.000000  loss: 3.7491 (3.7192)  loss_scale: 65536.0000 (60453.1749)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [29]  [2130/2809]  eta: 0:04:14  lr: 0.000009  min_lr: 0.000000  loss: 3.7687 (3.7192)  loss_scale: 65536.0000 (60477.0267)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [29]  [2140/2809]  eta: 0:04:10  lr: 0.000009  min_lr: 0.000000  loss: 3.7320 (3.7191)  loss_scale: 65536.0000 (60500.6558)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0002  max mem: 15572
[2025-01-13 08:04:30,051] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:04:30,051] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [29]  [2150/2809]  eta: 0:04:06  lr: 0.000009  min_lr: 0.000000  loss: 3.7598 (3.7201)  loss_scale: 65536.0000 (60676.4035)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
[2025-01-13 08:04:33,343] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83616
[2025-01-13 08:04:33,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:04:33,343] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [2160/2809]  eta: 0:04:02  lr: 0.000009  min_lr: 0.000000  loss: 3.7717 (3.7205)  loss_scale: 65536.0000 (60820.1981)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [29]  [2170/2809]  eta: 0:03:58  lr: 0.000009  min_lr: 0.000000  loss: 3.7112 (3.7200)  loss_scale: 65536.0000 (60841.9199)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [29]  [2180/2809]  eta: 0:03:55  lr: 0.000009  min_lr: 0.000000  loss: 3.6623 (3.7201)  loss_scale: 65536.0000 (60863.4425)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [29]  [2190/2809]  eta: 0:03:51  lr: 0.000009  min_lr: 0.000000  loss: 3.5858 (3.7191)  loss_scale: 65536.0000 (60884.7686)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [29]  [2200/2809]  eta: 0:03:47  lr: 0.000009  min_lr: 0.000000  loss: 3.6689 (3.7194)  loss_scale: 65536.0000 (60905.9010)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [29]  [2210/2809]  eta: 0:03:43  lr: 0.000009  min_lr: 0.000000  loss: 3.6740 (3.7193)  loss_scale: 65536.0000 (60926.8422)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [29]  [2220/2809]  eta: 0:03:40  lr: 0.000009  min_lr: 0.000000  loss: 3.6514 (3.7196)  loss_scale: 65536.0000 (60947.5948)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [29]  [2230/2809]  eta: 0:03:36  lr: 0.000009  min_lr: 0.000000  loss: 3.8499 (3.7198)  loss_scale: 65536.0000 (60968.1614)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0002  max mem: 15572
Epoch: [29]  [2240/2809]  eta: 0:03:32  lr: 0.000009  min_lr: 0.000000  loss: 3.8597 (3.7201)  loss_scale: 65536.0000 (60988.5444)  weight_decay: 0.0500 (0.0500)  time: 0.3660  data: 0.0002  max mem: 15572
Epoch: [29]  [2250/2809]  eta: 0:03:28  lr: 0.000009  min_lr: 0.000000  loss: 3.7623 (3.7204)  loss_scale: 65536.0000 (61008.7463)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [29]  [2260/2809]  eta: 0:03:25  lr: 0.000009  min_lr: 0.000000  loss: 3.6659 (3.7200)  loss_scale: 65536.0000 (61028.7696)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0003  max mem: 15572
Epoch: [29]  [2270/2809]  eta: 0:03:21  lr: 0.000009  min_lr: 0.000000  loss: 3.6613 (3.7196)  loss_scale: 65536.0000 (61048.6165)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [29]  [2280/2809]  eta: 0:03:17  lr: 0.000009  min_lr: 0.000000  loss: 3.6784 (3.7193)  loss_scale: 65536.0000 (61068.2893)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
[2025-01-13 08:05:20,979] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:05:20,979] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [29]  [2290/2809]  eta: 0:03:13  lr: 0.000009  min_lr: 0.000000  loss: 3.7930 (3.7202)  loss_scale: 65536.0000 (61288.0314)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0003  max mem: 15572
[2025-01-13 08:05:23,960] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83753
[2025-01-13 08:05:23,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:05:23,961] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [2300/2809]  eta: 0:03:10  lr: 0.000009  min_lr: 0.000000  loss: 3.9415 (3.7204)  loss_scale: 65536.0000 (61334.9744)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [29]  [2310/2809]  eta: 0:03:06  lr: 0.000009  min_lr: 0.000000  loss: 3.4153 (3.7191)  loss_scale: 65536.0000 (61353.1527)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [29]  [2320/2809]  eta: 0:03:02  lr: 0.000009  min_lr: 0.000000  loss: 3.6111 (3.7190)  loss_scale: 65536.0000 (61371.1745)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [29]  [2330/2809]  eta: 0:02:59  lr: 0.000009  min_lr: 0.000000  loss: 3.6963 (3.7188)  loss_scale: 65536.0000 (61389.0416)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [29]  [2340/2809]  eta: 0:02:55  lr: 0.000009  min_lr: 0.000000  loss: 3.6963 (3.7195)  loss_scale: 65536.0000 (61406.7561)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [29]  [2350/2809]  eta: 0:02:51  lr: 0.000009  min_lr: 0.000000  loss: 3.7800 (3.7193)  loss_scale: 65536.0000 (61424.3199)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [29]  [2360/2809]  eta: 0:02:47  lr: 0.000009  min_lr: 0.000000  loss: 3.8030 (3.7198)  loss_scale: 65536.0000 (61441.7349)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0001  max mem: 15572
Epoch: [29]  [2370/2809]  eta: 0:02:44  lr: 0.000009  min_lr: 0.000000  loss: 3.8030 (3.7198)  loss_scale: 65536.0000 (61459.0030)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [29]  [2380/2809]  eta: 0:02:40  lr: 0.000009  min_lr: 0.000000  loss: 3.6945 (3.7199)  loss_scale: 65536.0000 (61476.1260)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [29]  [2390/2809]  eta: 0:02:36  lr: 0.000009  min_lr: 0.000000  loss: 3.9678 (3.7202)  loss_scale: 65536.0000 (61493.1058)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [29]  [2400/2809]  eta: 0:02:32  lr: 0.000009  min_lr: 0.000000  loss: 3.7369 (3.7192)  loss_scale: 65536.0000 (61509.9442)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [29]  [2410/2809]  eta: 0:02:29  lr: 0.000009  min_lr: 0.000000  loss: 3.6537 (3.7196)  loss_scale: 65536.0000 (61526.6429)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [29]  [2420/2809]  eta: 0:02:25  lr: 0.000009  min_lr: 0.000000  loss: 4.0181 (3.7206)  loss_scale: 65536.0000 (61543.2036)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
[2025-01-13 08:06:11,796] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:06:11,796] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:06:12,157] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83883
[2025-01-13 08:06:12,157] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:06:12,157] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [2430/2809]  eta: 0:02:21  lr: 0.000009  min_lr: 0.000000  loss: 3.8782 (3.7204)  loss_scale: 65536.0000 (61586.5866)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [29]  [2440/2809]  eta: 0:02:17  lr: 0.000009  min_lr: 0.000000  loss: 3.6129 (3.7194)  loss_scale: 65536.0000 (61602.7661)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [29]  [2450/2809]  eta: 0:02:14  lr: 0.000009  min_lr: 0.000000  loss: 3.5523 (3.7195)  loss_scale: 65536.0000 (61618.8135)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [29]  [2460/2809]  eta: 0:02:10  lr: 0.000009  min_lr: 0.000000  loss: 3.5923 (3.7188)  loss_scale: 65536.0000 (61634.7306)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [29]  [2470/2809]  eta: 0:02:06  lr: 0.000009  min_lr: 0.000000  loss: 3.5923 (3.7184)  loss_scale: 65536.0000 (61650.5188)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [29]  [2480/2809]  eta: 0:02:02  lr: 0.000009  min_lr: 0.000000  loss: 3.8134 (3.7189)  loss_scale: 65536.0000 (61666.1798)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [29]  [2490/2809]  eta: 0:01:59  lr: 0.000009  min_lr: 0.000000  loss: 3.6641 (3.7181)  loss_scale: 65536.0000 (61681.7150)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0003  max mem: 15572
Epoch: [29]  [2500/2809]  eta: 0:01:55  lr: 0.000009  min_lr: 0.000000  loss: 3.5663 (3.7180)  loss_scale: 65536.0000 (61697.1259)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [29]  [2510/2809]  eta: 0:01:51  lr: 0.000009  min_lr: 0.000000  loss: 3.5431 (3.7175)  loss_scale: 65536.0000 (61712.4142)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [29]  [2520/2809]  eta: 0:01:47  lr: 0.000009  min_lr: 0.000000  loss: 3.5692 (3.7174)  loss_scale: 65536.0000 (61727.5811)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [29]  [2530/2809]  eta: 0:01:44  lr: 0.000009  min_lr: 0.000000  loss: 3.6861 (3.7177)  loss_scale: 65536.0000 (61742.6282)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 08:06:55,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=84000, skipped=570, lr=[8.740915324830693e-08, 8.740915324830693e-08, 1.2487021892615276e-07, 1.2487021892615276e-07, 1.7838602703736113e-07, 1.7838602703736113e-07, 2.5483718148194445e-07, 2.5483718148194445e-07, 3.6405311640277783e-07, 3.6405311640277783e-07, 5.20075880575397e-07, 5.20075880575397e-07, 7.429655436791385e-07, 7.429655436791385e-07, 1.0613793481130551e-06, 1.0613793481130551e-06, 1.5162562115900786e-06, 1.5162562115900786e-06, 2.166080302271541e-06, 2.166080302271541e-06, 3.0944004318164876e-06, 3.0944004318164876e-06, 4.420572045452126e-06, 4.420572045452126e-06, 6.315102922074465e-06, 6.315102922074465e-06, 9.021575602963522e-06, 9.021575602963522e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 08:06:55,167] [INFO] [timer.py:260:stop] epoch=0/micro_step=84000/global_step=84000, RunningAvgSamplesPerSec=30.97919631002141, CurrSamplesPerSec=35.0060877856888, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [2540/2809]  eta: 0:01:40  lr: 0.000009  min_lr: 0.000000  loss: 3.8044 (3.7176)  loss_scale: 65536.0000 (61757.5569)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [29]  [2550/2809]  eta: 0:01:36  lr: 0.000009  min_lr: 0.000000  loss: 3.8813 (3.7181)  loss_scale: 65536.0000 (61772.3685)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 08:06:59,985] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:06:59,985] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:07:00,371] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84013
[2025-01-13 08:07:00,371] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:07:00,371] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [2560/2809]  eta: 0:01:32  lr: 0.000009  min_lr: 0.000000  loss: 3.8319 (3.7185)  loss_scale: 65536.0000 (61812.6544)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [29]  [2570/2809]  eta: 0:01:29  lr: 0.000009  min_lr: 0.000000  loss: 3.7545 (3.7175)  loss_scale: 65536.0000 (61827.1365)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [29]  [2580/2809]  eta: 0:01:25  lr: 0.000009  min_lr: 0.000000  loss: 3.8524 (3.7188)  loss_scale: 65536.0000 (61841.5064)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [29]  [2590/2809]  eta: 0:01:21  lr: 0.000009  min_lr: 0.000000  loss: 3.8763 (3.7182)  loss_scale: 65536.0000 (61855.7653)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [29]  [2600/2809]  eta: 0:01:18  lr: 0.000009  min_lr: 0.000000  loss: 3.6645 (3.7188)  loss_scale: 65536.0000 (61869.9146)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0003  max mem: 15572
Epoch: [29]  [2610/2809]  eta: 0:01:14  lr: 0.000009  min_lr: 0.000000  loss: 3.7241 (3.7185)  loss_scale: 65536.0000 (61883.9556)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [29]  [2620/2809]  eta: 0:01:10  lr: 0.000009  min_lr: 0.000000  loss: 3.7241 (3.7188)  loss_scale: 65536.0000 (61897.8894)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [29]  [2630/2809]  eta: 0:01:06  lr: 0.000009  min_lr: 0.000000  loss: 3.8918 (3.7199)  loss_scale: 65536.0000 (61911.7172)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [29]  [2640/2809]  eta: 0:01:03  lr: 0.000009  min_lr: 0.000000  loss: 3.8918 (3.7203)  loss_scale: 65536.0000 (61925.4404)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0002  max mem: 15572
Epoch: [29]  [2650/2809]  eta: 0:00:59  lr: 0.000009  min_lr: 0.000000  loss: 3.6547 (3.7193)  loss_scale: 65536.0000 (61939.0600)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0002  max mem: 15572
Epoch: [29]  [2660/2809]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 3.6547 (3.7197)  loss_scale: 65536.0000 (61952.5772)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [29]  [2670/2809]  eta: 0:00:51  lr: 0.000009  min_lr: 0.000000  loss: 3.9103 (3.7193)  loss_scale: 65536.0000 (61965.9933)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [29]  [2680/2809]  eta: 0:00:48  lr: 0.000009  min_lr: 0.000000  loss: 3.7346 (3.7192)  loss_scale: 65536.0000 (61979.3092)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
[2025-01-13 08:07:48,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:07:48,431] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:07:49,166] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84144
[2025-01-13 08:07:49,166] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:07:49,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [2690/2809]  eta: 0:00:44  lr: 0.000009  min_lr: 0.000000  loss: 3.6097 (3.7188)  loss_scale: 65536.0000 (62041.2337)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
Epoch: [29]  [2700/2809]  eta: 0:00:40  lr: 0.000009  min_lr: 0.000000  loss: 3.5833 (3.7191)  loss_scale: 65536.0000 (62054.1725)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [29]  [2710/2809]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 3.6613 (3.7185)  loss_scale: 65536.0000 (62067.0159)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [29]  [2720/2809]  eta: 0:00:33  lr: 0.000009  min_lr: 0.000000  loss: 3.6538 (3.7190)  loss_scale: 65536.0000 (62079.7648)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [29]  [2730/2809]  eta: 0:00:29  lr: 0.000009  min_lr: 0.000000  loss: 3.7248 (3.7192)  loss_scale: 65536.0000 (62092.4204)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [29]  [2740/2809]  eta: 0:00:25  lr: 0.000009  min_lr: 0.000000  loss: 3.6303 (3.7189)  loss_scale: 65536.0000 (62104.9836)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [29]  [2750/2809]  eta: 0:00:22  lr: 0.000009  min_lr: 0.000000  loss: 3.7361 (3.7188)  loss_scale: 65536.0000 (62117.4555)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [29]  [2760/2809]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 3.7361 (3.7182)  loss_scale: 65536.0000 (62129.8370)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [29]  [2770/2809]  eta: 0:00:14  lr: 0.000009  min_lr: 0.000000  loss: 3.8037 (3.7184)  loss_scale: 65536.0000 (62142.1292)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [29]  [2780/2809]  eta: 0:00:10  lr: 0.000009  min_lr: 0.000000  loss: 3.8111 (3.7191)  loss_scale: 65536.0000 (62154.3330)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [29]  [2790/2809]  eta: 0:00:07  lr: 0.000009  min_lr: 0.000000  loss: 3.9576 (3.7190)  loss_scale: 65536.0000 (62166.4493)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [29]  [2800/2809]  eta: 0:00:03  lr: 0.000009  min_lr: 0.000000  loss: 3.4956 (3.7185)  loss_scale: 65536.0000 (62178.4791)  weight_decay: 0.0500 (0.0500)  time: 0.3628  data: 0.0001  max mem: 15572
Epoch: [29]  [2808/2809]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 3.8380 (3.7187)  loss_scale: 65536.0000 (62188.0413)  weight_decay: 0.0500 (0.0500)  time: 0.3602  data: 0.0001  max mem: 15572
Epoch: [29] Total time: 0:17:28 (0.3734 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 3.8380 (3.7187)  loss_scale: 65536.0000 (62188.0413)  weight_decay: 0.0500 (0.0500)
[2025-01-13 08:08:35,429] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-29 is about to be saved!
[2025-01-13 08:08:35,431] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-29/mp_rank_00_model_states.pt
[2025-01-13 08:08:35,431] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-29/mp_rank_00_model_states.pt...
[2025-01-13 08:08:35,662] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-29/mp_rank_00_model_states.pt.
[2025-01-13 08:08:35,663] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-29 is ready now!
Val:  [  0/272]  eta: 0:11:14  loss: 0.4059 (0.4059)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4806  data: 2.3117  max mem: 15572
Val:  [ 10/272]  eta: 0:01:47  loss: 2.4151 (2.2237)  acc1: 44.4444 (45.4545)  acc5: 77.7778 (73.2323)  time: 0.4104  data: 0.2448  max mem: 15572
Val:  [ 20/272]  eta: 0:01:13  loss: 2.3673 (2.2488)  acc1: 44.4444 (47.8836)  acc5: 77.7778 (74.6032)  time: 0.1811  data: 0.0193  max mem: 15572
Val:  [ 30/272]  eta: 0:01:00  loss: 2.3673 (2.3532)  acc1: 44.4444 (43.9068)  acc5: 77.7778 (74.5520)  time: 0.1632  data: 0.0026  max mem: 15572
Val:  [ 40/272]  eta: 0:00:56  loss: 2.4604 (2.4008)  acc1: 33.3333 (41.8699)  acc5: 77.7778 (74.5257)  time: 0.1980  data: 0.0378  max mem: 15572
Val:  [ 50/272]  eta: 0:00:51  loss: 2.4523 (2.3323)  acc1: 33.3333 (43.6819)  acc5: 77.7778 (75.9259)  time: 0.1975  data: 0.0365  max mem: 15572
Val:  [ 60/272]  eta: 0:00:47  loss: 1.5159 (2.2359)  acc1: 66.6667 (46.6302)  acc5: 88.8889 (76.8670)  time: 0.1777  data: 0.0094  max mem: 15572
Val:  [ 70/272]  eta: 0:00:43  loss: 1.5871 (2.1645)  acc1: 66.6667 (48.9045)  acc5: 83.3333 (77.5430)  time: 0.1818  data: 0.0110  max mem: 15572
Val:  [ 80/272]  eta: 0:00:40  loss: 1.8724 (2.1821)  acc1: 55.5556 (48.6968)  acc5: 77.7778 (77.3663)  time: 0.1698  data: 0.0030  max mem: 15572
Val:  [ 90/272]  eta: 0:00:37  loss: 2.1810 (2.1964)  acc1: 50.0000 (48.7790)  acc5: 83.3333 (77.7778)  time: 0.1704  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 2.1810 (2.2250)  acc1: 50.0000 (48.1298)  acc5: 83.3333 (77.2827)  time: 0.1656  data: 0.0005  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 2.5047 (2.3095)  acc1: 22.2222 (45.7457)  acc5: 66.6667 (75.5756)  time: 0.1556  data: 0.0005  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 3.0533 (2.3506)  acc1: 22.2222 (44.9036)  acc5: 66.6667 (74.9770)  time: 0.1603  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 2.1229 (2.3180)  acc1: 50.0000 (45.7591)  acc5: 77.7778 (75.6997)  time: 0.1632  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 1.7348 (2.3119)  acc1: 55.5556 (46.1387)  acc5: 83.3333 (75.3349)  time: 0.1688  data: 0.0024  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 2.2832 (2.3185)  acc1: 38.8889 (45.5850)  acc5: 77.7778 (75.6439)  time: 0.1755  data: 0.0110  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 2.2832 (2.3044)  acc1: 44.4444 (46.2043)  acc5: 77.7778 (75.9834)  time: 0.1740  data: 0.0110  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 2.3717 (2.3232)  acc1: 44.4444 (45.6465)  acc5: 77.7778 (75.6660)  time: 0.1738  data: 0.0086  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 2.2814 (2.3135)  acc1: 38.8889 (45.5494)  acc5: 72.2222 (75.9975)  time: 0.1783  data: 0.0109  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 2.2814 (2.3630)  acc1: 38.8889 (44.4154)  acc5: 77.7778 (74.7237)  time: 0.1983  data: 0.0302  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 2.5007 (2.3684)  acc1: 38.8889 (44.2510)  acc5: 66.6667 (74.5163)  time: 0.1996  data: 0.0259  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.0878 (2.3738)  acc1: 44.4444 (44.2338)  acc5: 77.7778 (74.4339)  time: 0.1788  data: 0.0054  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 2.1767 (2.3615)  acc1: 50.0000 (44.4696)  acc5: 77.7778 (74.6606)  time: 0.1710  data: 0.0054  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.7267 (2.3333)  acc1: 61.1111 (45.4545)  acc5: 83.3333 (75.0120)  time: 0.1765  data: 0.0071  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.6605 (2.3189)  acc1: 61.1111 (45.6893)  acc5: 83.3333 (75.3573)  time: 0.1755  data: 0.0070  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 2.2174 (2.3300)  acc1: 38.8889 (45.0199)  acc5: 83.3333 (75.2988)  time: 0.1661  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.2620 (2.2730)  acc1: 72.2222 (46.6582)  acc5: 88.8889 (76.0324)  time: 0.1717  data: 0.0166  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.2958 (2.2661)  acc1: 72.2222 (46.6585)  acc5: 88.8889 (76.2608)  time: 0.1559  data: 0.0164  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.2958 (2.2709)  acc1: 66.6667 (46.6312)  acc5: 88.8889 (76.2236)  time: 0.1338  data: 0.0001  max mem: 15572
Val: Total time: 0:00:49 (0.1833 s / it)
* Acc@1 46.631 Acc@5 76.224 loss 2.271
Accuracy of the network on the 4883 val videos: 46.6%
[2025-01-13 08:09:25,528] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 08:09:25,530] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 08:09:25,530] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 08:09:28,148] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 08:09:28,148] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.63%
Epoch: [30]  [   0/2809]  eta: 3:42:29  lr: 0.000009  min_lr: 0.000000  loss: 2.8661 (2.8661)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.7526  data: 4.3626  max mem: 15572
[2025-01-13 08:09:34,038] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:09:34,038] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:09:35,924] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84278
[2025-01-13 08:09:35,924] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:09:35,925] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [  10/2809]  eta: 0:36:08  lr: 0.000009  min_lr: 0.000000  loss: 3.8237 (3.7350)  loss_scale: 65536.0000 (95325.0909)  weight_decay: 0.0500 (0.0500)  time: 0.7749  data: 0.3971  max mem: 15572
Epoch: [30]  [  20/2809]  eta: 0:27:18  lr: 0.000009  min_lr: 0.000000  loss: 3.8771 (3.7392)  loss_scale: 65536.0000 (81139.8095)  weight_decay: 0.0500 (0.0500)  time: 0.3791  data: 0.0004  max mem: 15572
Epoch: [30]  [  30/2809]  eta: 0:23:56  lr: 0.000009  min_lr: 0.000000  loss: 3.8771 (3.6837)  loss_scale: 65536.0000 (76106.3226)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
Epoch: [30]  [  40/2809]  eta: 0:22:10  lr: 0.000009  min_lr: 0.000000  loss: 3.8485 (3.7313)  loss_scale: 65536.0000 (73528.1951)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [30]  [  50/2809]  eta: 0:21:04  lr: 0.000009  min_lr: 0.000000  loss: 3.8153 (3.7406)  loss_scale: 65536.0000 (71961.0980)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [30]  [  60/2809]  eta: 0:20:18  lr: 0.000009  min_lr: 0.000000  loss: 3.7871 (3.7517)  loss_scale: 65536.0000 (70907.8033)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [30]  [  70/2809]  eta: 0:19:45  lr: 0.000009  min_lr: 0.000000  loss: 3.7167 (3.7346)  loss_scale: 65536.0000 (70151.2113)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [30]  [  80/2809]  eta: 0:19:20  lr: 0.000009  min_lr: 0.000000  loss: 3.7167 (3.7370)  loss_scale: 65536.0000 (69581.4321)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [30]  [  90/2809]  eta: 0:18:59  lr: 0.000009  min_lr: 0.000000  loss: 3.5989 (3.7206)  loss_scale: 65536.0000 (69136.8791)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [30]  [ 100/2809]  eta: 0:18:41  lr: 0.000009  min_lr: 0.000000  loss: 3.5427 (3.6995)  loss_scale: 65536.0000 (68780.3564)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [30]  [ 110/2809]  eta: 0:18:28  lr: 0.000009  min_lr: 0.000000  loss: 3.5427 (3.6831)  loss_scale: 65536.0000 (68488.0721)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [30]  [ 120/2809]  eta: 0:18:14  lr: 0.000009  min_lr: 0.000000  loss: 3.7288 (3.7024)  loss_scale: 65536.0000 (68244.0992)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [30]  [ 130/2809]  eta: 0:18:02  lr: 0.000009  min_lr: 0.000000  loss: 3.7336 (3.7007)  loss_scale: 65536.0000 (68037.3740)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
[2025-01-13 08:10:23,710] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:10:23,711] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:10:24,820] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84410
[2025-01-13 08:10:24,820] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:10:24,820] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 140/2809]  eta: 0:17:52  lr: 0.000009  min_lr: 0.000000  loss: 3.6291 (3.7091)  loss_scale: 65536.0000 (69254.3546)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [30]  [ 150/2809]  eta: 0:17:42  lr: 0.000009  min_lr: 0.000000  loss: 3.9097 (3.7268)  loss_scale: 65536.0000 (69008.1060)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [30]  [ 160/2809]  eta: 0:17:33  lr: 0.000009  min_lr: 0.000000  loss: 3.7437 (3.7199)  loss_scale: 65536.0000 (68792.4472)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [30]  [ 170/2809]  eta: 0:17:25  lr: 0.000009  min_lr: 0.000000  loss: 3.6663 (3.7089)  loss_scale: 65536.0000 (68602.0117)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [30]  [ 180/2809]  eta: 0:17:17  lr: 0.000009  min_lr: 0.000000  loss: 3.5044 (3.7065)  loss_scale: 65536.0000 (68432.6188)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [30]  [ 190/2809]  eta: 0:17:10  lr: 0.000009  min_lr: 0.000000  loss: 3.5915 (3.7112)  loss_scale: 65536.0000 (68280.9634)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [30]  [ 200/2809]  eta: 0:17:03  lr: 0.000009  min_lr: 0.000000  loss: 3.7603 (3.7293)  loss_scale: 65536.0000 (68144.3980)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0003  max mem: 15572
Epoch: [30]  [ 210/2809]  eta: 0:16:56  lr: 0.000009  min_lr: 0.000000  loss: 3.9119 (3.7288)  loss_scale: 65536.0000 (68020.7773)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0003  max mem: 15572
Epoch: [30]  [ 220/2809]  eta: 0:16:50  lr: 0.000009  min_lr: 0.000000  loss: 3.8384 (3.7294)  loss_scale: 65536.0000 (67908.3439)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
Epoch: [30]  [ 230/2809]  eta: 0:16:44  lr: 0.000009  min_lr: 0.000000  loss: 3.8578 (3.7423)  loss_scale: 65536.0000 (67805.6450)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [30]  [ 240/2809]  eta: 0:16:38  lr: 0.000009  min_lr: 0.000000  loss: 3.9243 (3.7433)  loss_scale: 65536.0000 (67711.4689)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [30]  [ 250/2809]  eta: 0:16:32  lr: 0.000009  min_lr: 0.000000  loss: 3.6466 (3.7337)  loss_scale: 65536.0000 (67624.7968)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0003  max mem: 15572
Epoch: [30]  [ 260/2809]  eta: 0:16:26  lr: 0.000009  min_lr: 0.000000  loss: 3.6466 (3.7357)  loss_scale: 65536.0000 (67544.7663)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
[2025-01-13 08:11:12,520] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:11:12,520] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [ 270/2809]  eta: 0:16:20  lr: 0.000009  min_lr: 0.000000  loss: 3.5931 (3.7234)  loss_scale: 65536.0000 (67954.3026)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 08:11:13,659] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84542
[2025-01-13 08:11:13,659] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:11:13,659] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 280/2809]  eta: 0:16:15  lr: 0.000009  min_lr: 0.000000  loss: 3.4960 (3.7209)  loss_scale: 65536.0000 (68101.4662)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [30]  [ 290/2809]  eta: 0:16:10  lr: 0.000009  min_lr: 0.000000  loss: 3.5392 (3.7142)  loss_scale: 65536.0000 (68013.3058)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [30]  [ 300/2809]  eta: 0:16:05  lr: 0.000009  min_lr: 0.000000  loss: 3.8229 (3.7188)  loss_scale: 65536.0000 (67931.0033)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0004  max mem: 15572
Epoch: [30]  [ 310/2809]  eta: 0:16:00  lr: 0.000009  min_lr: 0.000000  loss: 4.0054 (3.7211)  loss_scale: 65536.0000 (67853.9936)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [30]  [ 320/2809]  eta: 0:15:55  lr: 0.000009  min_lr: 0.000000  loss: 3.7283 (3.7209)  loss_scale: 65536.0000 (67781.7819)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0003  max mem: 15572
Epoch: [30]  [ 330/2809]  eta: 0:15:50  lr: 0.000009  min_lr: 0.000000  loss: 3.8201 (3.7234)  loss_scale: 65536.0000 (67713.9335)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0003  max mem: 15572
Epoch: [30]  [ 340/2809]  eta: 0:15:46  lr: 0.000009  min_lr: 0.000000  loss: 3.8470 (3.7245)  loss_scale: 65536.0000 (67650.0645)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [30]  [ 350/2809]  eta: 0:15:41  lr: 0.000009  min_lr: 0.000000  loss: 3.7708 (3.7210)  loss_scale: 65536.0000 (67589.8348)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [30]  [ 360/2809]  eta: 0:15:36  lr: 0.000009  min_lr: 0.000000  loss: 3.4977 (3.7150)  loss_scale: 65536.0000 (67532.9418)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [30]  [ 370/2809]  eta: 0:15:31  lr: 0.000009  min_lr: 0.000000  loss: 3.5421 (3.7130)  loss_scale: 65536.0000 (67479.1159)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [30]  [ 380/2809]  eta: 0:15:27  lr: 0.000009  min_lr: 0.000000  loss: 3.7025 (3.7124)  loss_scale: 65536.0000 (67428.1155)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [30]  [ 390/2809]  eta: 0:15:22  lr: 0.000009  min_lr: 0.000000  loss: 3.7025 (3.7123)  loss_scale: 65536.0000 (67379.7238)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [30]  [ 400/2809]  eta: 0:15:18  lr: 0.000009  min_lr: 0.000000  loss: 3.7985 (3.7170)  loss_scale: 65536.0000 (67333.7456)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
[2025-01-13 08:12:01,461] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:12:01,461] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:12:02,182] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84673
[2025-01-13 08:12:02,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:12:02,182] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 410/2809]  eta: 0:15:13  lr: 0.000009  min_lr: 0.000000  loss: 3.9434 (3.7228)  loss_scale: 65536.0000 (67608.9148)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [30]  [ 420/2809]  eta: 0:15:09  lr: 0.000009  min_lr: 0.000000  loss: 4.0100 (3.7296)  loss_scale: 65536.0000 (67559.6770)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [30]  [ 430/2809]  eta: 0:15:05  lr: 0.000009  min_lr: 0.000000  loss: 3.7423 (3.7199)  loss_scale: 65536.0000 (67512.7239)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [30]  [ 440/2809]  eta: 0:15:00  lr: 0.000009  min_lr: 0.000000  loss: 3.5313 (3.7229)  loss_scale: 65536.0000 (67467.9002)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [30]  [ 450/2809]  eta: 0:14:56  lr: 0.000009  min_lr: 0.000000  loss: 3.9043 (3.7241)  loss_scale: 65536.0000 (67425.0643)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [30]  [ 460/2809]  eta: 0:14:52  lr: 0.000009  min_lr: 0.000000  loss: 3.8500 (3.7203)  loss_scale: 65536.0000 (67384.0868)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [30]  [ 470/2809]  eta: 0:14:47  lr: 0.000009  min_lr: 0.000000  loss: 3.5398 (3.7177)  loss_scale: 65536.0000 (67344.8493)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [30]  [ 480/2809]  eta: 0:14:43  lr: 0.000009  min_lr: 0.000000  loss: 3.5481 (3.7129)  loss_scale: 65536.0000 (67307.2432)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [30]  [ 490/2809]  eta: 0:14:39  lr: 0.000009  min_lr: 0.000000  loss: 3.5777 (3.7138)  loss_scale: 65536.0000 (67271.1690)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [30]  [ 500/2809]  eta: 0:14:35  lr: 0.000009  min_lr: 0.000000  loss: 3.5833 (3.7108)  loss_scale: 65536.0000 (67236.5349)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [30]  [ 510/2809]  eta: 0:14:30  lr: 0.000009  min_lr: 0.000000  loss: 3.5591 (3.7069)  loss_scale: 65536.0000 (67203.2564)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [30]  [ 520/2809]  eta: 0:14:26  lr: 0.000009  min_lr: 0.000000  loss: 3.5591 (3.7067)  loss_scale: 65536.0000 (67171.2553)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [30]  [ 530/2809]  eta: 0:14:22  lr: 0.000009  min_lr: 0.000000  loss: 3.8134 (3.7073)  loss_scale: 65536.0000 (67140.4595)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
[2025-01-13 08:12:49,927] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:12:49,927] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:12:52,145] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84808
[2025-01-13 08:12:52,146] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:12:52,146] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 540/2809]  eta: 0:14:18  lr: 0.000009  min_lr: 0.000000  loss: 3.8019 (3.7092)  loss_scale: 65536.0000 (67837.6340)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0003  max mem: 15572
Epoch: [30]  [ 550/2809]  eta: 0:14:14  lr: 0.000009  min_lr: 0.000000  loss: 3.8019 (3.7073)  loss_scale: 65536.0000 (67795.8621)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0003  max mem: 15572
Epoch: [30]  [ 560/2809]  eta: 0:14:10  lr: 0.000009  min_lr: 0.000000  loss: 3.8166 (3.7067)  loss_scale: 65536.0000 (67755.5793)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0003  max mem: 15572
Epoch: [30]  [ 570/2809]  eta: 0:14:06  lr: 0.000009  min_lr: 0.000000  loss: 3.7865 (3.7076)  loss_scale: 65536.0000 (67716.7075)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [30]  [ 580/2809]  eta: 0:14:02  lr: 0.000009  min_lr: 0.000000  loss: 3.7601 (3.7059)  loss_scale: 65536.0000 (67679.1738)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
Epoch: [30]  [ 590/2809]  eta: 0:13:58  lr: 0.000009  min_lr: 0.000000  loss: 3.6995 (3.7064)  loss_scale: 65536.0000 (67642.9103)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [30]  [ 600/2809]  eta: 0:13:54  lr: 0.000009  min_lr: 0.000000  loss: 3.6202 (3.7045)  loss_scale: 65536.0000 (67607.8536)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [30]  [ 610/2809]  eta: 0:13:50  lr: 0.000009  min_lr: 0.000000  loss: 3.7222 (3.7073)  loss_scale: 65536.0000 (67573.9444)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0003  max mem: 15572
Epoch: [30]  [ 620/2809]  eta: 0:13:46  lr: 0.000009  min_lr: 0.000000  loss: 3.6288 (3.7051)  loss_scale: 65536.0000 (67541.1272)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [30]  [ 630/2809]  eta: 0:13:42  lr: 0.000008  min_lr: 0.000000  loss: 3.6582 (3.7071)  loss_scale: 65536.0000 (67509.3502)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [30]  [ 640/2809]  eta: 0:13:38  lr: 0.000008  min_lr: 0.000000  loss: 3.8037 (3.7090)  loss_scale: 65536.0000 (67478.5647)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0003  max mem: 15572
Epoch: [30]  [ 650/2809]  eta: 0:13:34  lr: 0.000008  min_lr: 0.000000  loss: 3.9026 (3.7103)  loss_scale: 65536.0000 (67448.7250)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [30]  [ 660/2809]  eta: 0:13:30  lr: 0.000008  min_lr: 0.000000  loss: 3.7730 (3.7099)  loss_scale: 65536.0000 (67419.7882)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
[2025-01-13 08:13:40,178] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:13:40,179] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [ 670/2809]  eta: 0:13:26  lr: 0.000008  min_lr: 0.000000  loss: 3.7901 (3.7106)  loss_scale: 65536.0000 (67782.3905)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
[2025-01-13 08:13:42,021] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84942
[2025-01-13 08:13:42,022] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:13:42,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 680/2809]  eta: 0:13:22  lr: 0.000008  min_lr: 0.000000  loss: 3.7901 (3.7108)  loss_scale: 65536.0000 (67845.6388)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0002  max mem: 15572
Epoch: [30]  [ 690/2809]  eta: 0:13:18  lr: 0.000008  min_lr: 0.000000  loss: 3.7668 (3.7119)  loss_scale: 65536.0000 (67812.2142)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [30]  [ 700/2809]  eta: 0:13:14  lr: 0.000008  min_lr: 0.000000  loss: 3.8755 (3.7134)  loss_scale: 65536.0000 (67779.7432)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [30]  [ 710/2809]  eta: 0:13:10  lr: 0.000008  min_lr: 0.000000  loss: 3.8310 (3.7140)  loss_scale: 65536.0000 (67748.1857)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [30]  [ 720/2809]  eta: 0:13:06  lr: 0.000008  min_lr: 0.000000  loss: 3.6796 (3.7119)  loss_scale: 65536.0000 (67717.5035)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
[2025-01-13 08:14:03,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=85000, skipped=578, lr=[8.177507834925266e-08, 8.177507834925266e-08, 1.1682154049893238e-07, 1.1682154049893238e-07, 1.6688791499847485e-07, 1.6688791499847485e-07, 2.3841130714067836e-07, 2.3841130714067836e-07, 3.4058758162954053e-07, 3.4058758162954053e-07, 4.865536880422007e-07, 4.865536880422007e-07, 6.95076697203144e-07, 6.95076697203144e-07, 9.929667102902058e-07, 9.929667102902058e-07, 1.4185238718431511e-06, 1.4185238718431511e-06, 2.026462674061645e-06, 2.026462674061645e-06, 2.894946677230921e-06, 2.894946677230921e-06, 4.135638110329887e-06, 4.135638110329887e-06, 5.908054443328411e-06, 5.908054443328411e-06, 8.440077776183445e-06, 8.440077776183445e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 08:14:03,086] [INFO] [timer.py:260:stop] epoch=0/micro_step=85000/global_step=85000, RunningAvgSamplesPerSec=31.014197664210382, CurrSamplesPerSec=33.994433281439434, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [30]  [ 730/2809]  eta: 0:13:02  lr: 0.000008  min_lr: 0.000000  loss: 3.6163 (3.7112)  loss_scale: 65536.0000 (67687.6607)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [30]  [ 740/2809]  eta: 0:12:58  lr: 0.000008  min_lr: 0.000000  loss: 3.6171 (3.7098)  loss_scale: 65536.0000 (67658.6235)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [30]  [ 750/2809]  eta: 0:12:54  lr: 0.000008  min_lr: 0.000000  loss: 3.6870 (3.7095)  loss_scale: 65536.0000 (67630.3595)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [30]  [ 760/2809]  eta: 0:12:50  lr: 0.000008  min_lr: 0.000000  loss: 3.7710 (3.7096)  loss_scale: 65536.0000 (67602.8384)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [30]  [ 770/2809]  eta: 0:12:46  lr: 0.000008  min_lr: 0.000000  loss: 3.6752 (3.7096)  loss_scale: 65536.0000 (67576.0311)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [30]  [ 780/2809]  eta: 0:12:43  lr: 0.000008  min_lr: 0.000000  loss: 3.6720 (3.7090)  loss_scale: 65536.0000 (67549.9104)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [30]  [ 790/2809]  eta: 0:12:39  lr: 0.000008  min_lr: 0.000000  loss: 3.5579 (3.7061)  loss_scale: 65536.0000 (67524.4501)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [30]  [ 800/2809]  eta: 0:12:35  lr: 0.000008  min_lr: 0.000000  loss: 3.5145 (3.7043)  loss_scale: 65536.0000 (67499.6255)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
[2025-01-13 08:14:29,800] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:14:29,801] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:14:30,529] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85073
[2025-01-13 08:14:30,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:14:30,529] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 810/2809]  eta: 0:12:31  lr: 0.000008  min_lr: 0.000000  loss: 3.7243 (3.7030)  loss_scale: 65536.0000 (67637.0308)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0003  max mem: 15572
Epoch: [30]  [ 820/2809]  eta: 0:12:27  lr: 0.000008  min_lr: 0.000000  loss: 3.5873 (3.7004)  loss_scale: 65536.0000 (67611.4397)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0003  max mem: 15572
Epoch: [30]  [ 830/2809]  eta: 0:12:23  lr: 0.000008  min_lr: 0.000000  loss: 3.7276 (3.7052)  loss_scale: 65536.0000 (67586.4645)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [30]  [ 840/2809]  eta: 0:12:19  lr: 0.000008  min_lr: 0.000000  loss: 3.9285 (3.7060)  loss_scale: 65536.0000 (67562.0832)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [30]  [ 850/2809]  eta: 0:12:15  lr: 0.000008  min_lr: 0.000000  loss: 3.8384 (3.7037)  loss_scale: 65536.0000 (67538.2750)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0003  max mem: 15572
Epoch: [30]  [ 860/2809]  eta: 0:12:12  lr: 0.000008  min_lr: 0.000000  loss: 3.3743 (3.7006)  loss_scale: 65536.0000 (67515.0197)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [30]  [ 870/2809]  eta: 0:12:08  lr: 0.000008  min_lr: 0.000000  loss: 3.6316 (3.7030)  loss_scale: 65536.0000 (67492.2985)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [30]  [ 880/2809]  eta: 0:12:04  lr: 0.000008  min_lr: 0.000000  loss: 3.9723 (3.7033)  loss_scale: 65536.0000 (67470.0931)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0003  max mem: 15572
Epoch: [30]  [ 890/2809]  eta: 0:12:00  lr: 0.000008  min_lr: 0.000000  loss: 3.7357 (3.7020)  loss_scale: 65536.0000 (67448.3861)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [30]  [ 900/2809]  eta: 0:11:56  lr: 0.000008  min_lr: 0.000000  loss: 3.7973 (3.7039)  loss_scale: 65536.0000 (67427.1609)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0003  max mem: 15572
Epoch: [30]  [ 910/2809]  eta: 0:11:52  lr: 0.000008  min_lr: 0.000000  loss: 3.8809 (3.7042)  loss_scale: 65536.0000 (67406.4018)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0003  max mem: 15572
Epoch: [30]  [ 920/2809]  eta: 0:11:49  lr: 0.000008  min_lr: 0.000000  loss: 3.6500 (3.7040)  loss_scale: 65536.0000 (67386.0934)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [30]  [ 930/2809]  eta: 0:11:45  lr: 0.000008  min_lr: 0.000000  loss: 3.5802 (3.7031)  loss_scale: 65536.0000 (67366.2213)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
[2025-01-13 08:15:18,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:15:18,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:15:20,355] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85207
[2025-01-13 08:15:20,355] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:15:20,355] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 940/2809]  eta: 0:11:41  lr: 0.000008  min_lr: 0.000000  loss: 3.5802 (3.7016)  loss_scale: 65536.0000 (67694.9968)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [30]  [ 950/2809]  eta: 0:11:37  lr: 0.000008  min_lr: 0.000000  loss: 3.6866 (3.7030)  loss_scale: 65536.0000 (67672.2944)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [30]  [ 960/2809]  eta: 0:11:33  lr: 0.000008  min_lr: 0.000000  loss: 3.9225 (3.7041)  loss_scale: 65536.0000 (67650.0645)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [30]  [ 970/2809]  eta: 0:11:30  lr: 0.000008  min_lr: 0.000000  loss: 3.6063 (3.7045)  loss_scale: 65536.0000 (67628.2925)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [30]  [ 980/2809]  eta: 0:11:26  lr: 0.000008  min_lr: 0.000000  loss: 3.5586 (3.7025)  loss_scale: 65536.0000 (67606.9643)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [30]  [ 990/2809]  eta: 0:11:22  lr: 0.000008  min_lr: 0.000000  loss: 3.6841 (3.7014)  loss_scale: 65536.0000 (67586.0666)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
Epoch: [30]  [1000/2809]  eta: 0:11:18  lr: 0.000008  min_lr: 0.000000  loss: 3.7257 (3.7014)  loss_scale: 65536.0000 (67565.5864)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [30]  [1010/2809]  eta: 0:11:14  lr: 0.000008  min_lr: 0.000000  loss: 3.7645 (3.6994)  loss_scale: 65536.0000 (67545.5114)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [30]  [1020/2809]  eta: 0:11:10  lr: 0.000008  min_lr: 0.000000  loss: 3.7405 (3.6998)  loss_scale: 65536.0000 (67525.8296)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [30]  [1030/2809]  eta: 0:11:06  lr: 0.000008  min_lr: 0.000000  loss: 3.6705 (3.6989)  loss_scale: 65536.0000 (67506.5296)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [30]  [1040/2809]  eta: 0:11:03  lr: 0.000008  min_lr: 0.000000  loss: 3.5606 (3.6971)  loss_scale: 65536.0000 (67487.6004)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [30]  [1050/2809]  eta: 0:10:59  lr: 0.000008  min_lr: 0.000000  loss: 3.5682 (3.6980)  loss_scale: 65536.0000 (67469.0314)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0003  max mem: 15572
Epoch: [30]  [1060/2809]  eta: 0:10:55  lr: 0.000008  min_lr: 0.000000  loss: 3.6969 (3.6981)  loss_scale: 65536.0000 (67450.8124)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0004  max mem: 15572
[2025-01-13 08:16:08,393] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:16:08,394] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:16:09,184] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85338
[2025-01-13 08:16:09,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:16:09,184] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [1070/2809]  eta: 0:10:52  lr: 0.000008  min_lr: 0.000000  loss: 3.6988 (3.6993)  loss_scale: 65536.0000 (67555.3165)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0004  max mem: 15572
Epoch: [30]  [1080/2809]  eta: 0:10:48  lr: 0.000008  min_lr: 0.000000  loss: 3.6174 (3.6968)  loss_scale: 65536.0000 (67536.6364)  weight_decay: 0.0500 (0.0500)  time: 0.3866  data: 0.0004  max mem: 15572
Epoch: [30]  [1090/2809]  eta: 0:10:45  lr: 0.000008  min_lr: 0.000000  loss: 3.7338 (3.6992)  loss_scale: 65536.0000 (67518.2988)  weight_decay: 0.0500 (0.0500)  time: 0.3862  data: 0.0004  max mem: 15572
Epoch: [30]  [1100/2809]  eta: 0:10:41  lr: 0.000008  min_lr: 0.000000  loss: 3.8397 (3.6981)  loss_scale: 65536.0000 (67500.2943)  weight_decay: 0.0500 (0.0500)  time: 0.3836  data: 0.0005  max mem: 15572
Epoch: [30]  [1110/2809]  eta: 0:10:37  lr: 0.000008  min_lr: 0.000000  loss: 3.6043 (3.6989)  loss_scale: 65536.0000 (67482.6139)  weight_decay: 0.0500 (0.0500)  time: 0.3827  data: 0.0005  max mem: 15572
Epoch: [30]  [1120/2809]  eta: 0:10:34  lr: 0.000008  min_lr: 0.000000  loss: 3.8665 (3.7004)  loss_scale: 65536.0000 (67465.2489)  weight_decay: 0.0500 (0.0500)  time: 0.3793  data: 0.0003  max mem: 15572
Epoch: [30]  [1130/2809]  eta: 0:10:30  lr: 0.000008  min_lr: 0.000000  loss: 3.6341 (3.6979)  loss_scale: 65536.0000 (67448.1910)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0002  max mem: 15572
Epoch: [30]  [1140/2809]  eta: 0:10:26  lr: 0.000008  min_lr: 0.000000  loss: 3.6341 (3.6989)  loss_scale: 65536.0000 (67431.4321)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [30]  [1150/2809]  eta: 0:10:22  lr: 0.000008  min_lr: 0.000000  loss: 3.5403 (3.6969)  loss_scale: 65536.0000 (67414.9644)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [30]  [1160/2809]  eta: 0:10:18  lr: 0.000008  min_lr: 0.000000  loss: 3.7651 (3.6973)  loss_scale: 65536.0000 (67398.7804)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [30]  [1170/2809]  eta: 0:10:15  lr: 0.000008  min_lr: 0.000000  loss: 3.9092 (3.7002)  loss_scale: 65536.0000 (67382.8728)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [30]  [1180/2809]  eta: 0:10:11  lr: 0.000008  min_lr: 0.000000  loss: 3.8714 (3.7006)  loss_scale: 65536.0000 (67367.2345)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [30]  [1190/2809]  eta: 0:10:07  lr: 0.000008  min_lr: 0.000000  loss: 3.7016 (3.7015)  loss_scale: 65536.0000 (67351.8589)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
[2025-01-13 08:16:57,789] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:16:57,789] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:16:58,154] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85468
[2025-01-13 08:16:58,154] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:16:58,154] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [1200/2809]  eta: 0:10:03  lr: 0.000008  min_lr: 0.000000  loss: 3.9118 (3.7027)  loss_scale: 65536.0000 (67391.3072)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [30]  [1210/2809]  eta: 0:10:00  lr: 0.000008  min_lr: 0.000000  loss: 3.5996 (3.6992)  loss_scale: 65536.0000 (67375.9868)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0003  max mem: 15572
Epoch: [30]  [1220/2809]  eta: 0:09:56  lr: 0.000008  min_lr: 0.000000  loss: 3.6059 (3.7014)  loss_scale: 65536.0000 (67360.9173)  weight_decay: 0.0500 (0.0500)  time: 0.3866  data: 0.0004  max mem: 15572
Epoch: [30]  [1230/2809]  eta: 0:09:52  lr: 0.000008  min_lr: 0.000000  loss: 3.7938 (3.7000)  loss_scale: 65536.0000 (67346.0926)  weight_decay: 0.0500 (0.0500)  time: 0.3899  data: 0.0004  max mem: 15572
Epoch: [30]  [1240/2809]  eta: 0:09:49  lr: 0.000008  min_lr: 0.000000  loss: 3.6146 (3.6985)  loss_scale: 65536.0000 (67331.5068)  weight_decay: 0.0500 (0.0500)  time: 0.3902  data: 0.0005  max mem: 15572
Epoch: [30]  [1250/2809]  eta: 0:09:45  lr: 0.000008  min_lr: 0.000000  loss: 3.8070 (3.6990)  loss_scale: 65536.0000 (67317.1543)  weight_decay: 0.0500 (0.0500)  time: 0.3811  data: 0.0004  max mem: 15572
Epoch: [30]  [1260/2809]  eta: 0:09:42  lr: 0.000008  min_lr: 0.000000  loss: 3.7425 (3.6980)  loss_scale: 65536.0000 (67303.0293)  weight_decay: 0.0500 (0.0500)  time: 0.3886  data: 0.0004  max mem: 15572
Epoch: [30]  [1270/2809]  eta: 0:09:38  lr: 0.000008  min_lr: 0.000000  loss: 3.7124 (3.6982)  loss_scale: 65536.0000 (67289.1267)  weight_decay: 0.0500 (0.0500)  time: 0.3943  data: 0.0005  max mem: 15572
Epoch: [30]  [1280/2809]  eta: 0:09:34  lr: 0.000008  min_lr: 0.000000  loss: 3.6448 (3.6992)  loss_scale: 65536.0000 (67275.4411)  weight_decay: 0.0500 (0.0500)  time: 0.3872  data: 0.0004  max mem: 15572
Epoch: [30]  [1290/2809]  eta: 0:09:31  lr: 0.000008  min_lr: 0.000000  loss: 4.0760 (3.7033)  loss_scale: 65536.0000 (67261.9675)  weight_decay: 0.0500 (0.0500)  time: 0.3918  data: 0.0004  max mem: 15572
Epoch: [30]  [1300/2809]  eta: 0:09:27  lr: 0.000008  min_lr: 0.000000  loss: 3.9640 (3.7036)  loss_scale: 65536.0000 (67248.7010)  weight_decay: 0.0500 (0.0500)  time: 0.3957  data: 0.0006  max mem: 15572
Epoch: [30]  [1310/2809]  eta: 0:09:24  lr: 0.000008  min_lr: 0.000000  loss: 3.8509 (3.7040)  loss_scale: 65536.0000 (67235.6369)  weight_decay: 0.0500 (0.0500)  time: 0.3878  data: 0.0005  max mem: 15572
Epoch: [30]  [1320/2809]  eta: 0:09:20  lr: 0.000008  min_lr: 0.000000  loss: 3.7428 (3.7036)  loss_scale: 65536.0000 (67222.7706)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0003  max mem: 15572
[2025-01-13 08:17:47,968] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:17:47,969] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:17:48,345] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85598
[2025-01-13 08:17:48,345] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:17:48,345] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [1330/2809]  eta: 0:09:16  lr: 0.000008  min_lr: 0.000000  loss: 3.7043 (3.7040)  loss_scale: 65536.0000 (67259.3358)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0003  max mem: 15572
[2025-01-13 08:17:50,180] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85603
[2025-01-13 08:17:50,180] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:17:50,180] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1340/2809]  eta: 0:09:12  lr: 0.000008  min_lr: 0.000000  loss: 3.7043 (3.7030)  loss_scale: 65536.0000 (67051.0007)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0003  max mem: 15572
Epoch: [30]  [1350/2809]  eta: 0:09:08  lr: 0.000008  min_lr: 0.000000  loss: 3.7035 (3.7031)  loss_scale: 32768.0000 (66797.2406)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [30]  [1360/2809]  eta: 0:09:04  lr: 0.000008  min_lr: 0.000000  loss: 3.8692 (3.7032)  loss_scale: 32768.0000 (66547.2094)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [30]  [1370/2809]  eta: 0:09:01  lr: 0.000008  min_lr: 0.000000  loss: 3.6426 (3.7025)  loss_scale: 32768.0000 (66300.8257)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [30]  [1380/2809]  eta: 0:08:57  lr: 0.000008  min_lr: 0.000000  loss: 3.6426 (3.7027)  loss_scale: 32768.0000 (66058.0101)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [30]  [1390/2809]  eta: 0:08:53  lr: 0.000008  min_lr: 0.000000  loss: 4.0077 (3.7054)  loss_scale: 32768.0000 (65818.6858)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [30]  [1400/2809]  eta: 0:08:49  lr: 0.000008  min_lr: 0.000000  loss: 3.9505 (3.7040)  loss_scale: 32768.0000 (65582.7780)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [30]  [1410/2809]  eta: 0:08:45  lr: 0.000008  min_lr: 0.000000  loss: 3.5093 (3.7039)  loss_scale: 32768.0000 (65350.2140)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0004  max mem: 15572
Epoch: [30]  [1420/2809]  eta: 0:08:42  lr: 0.000008  min_lr: 0.000000  loss: 3.5093 (3.7019)  loss_scale: 32768.0000 (65120.9233)  weight_decay: 0.0500 (0.0500)  time: 0.3818  data: 0.0005  max mem: 15572
Epoch: [30]  [1430/2809]  eta: 0:08:38  lr: 0.000008  min_lr: 0.000000  loss: 3.7150 (3.7027)  loss_scale: 32768.0000 (64894.8372)  weight_decay: 0.0500 (0.0500)  time: 0.3840  data: 0.0004  max mem: 15572
Epoch: [30]  [1440/2809]  eta: 0:08:34  lr: 0.000008  min_lr: 0.000000  loss: 3.6431 (3.7018)  loss_scale: 32768.0000 (64671.8890)  weight_decay: 0.0500 (0.0500)  time: 0.3835  data: 0.0005  max mem: 15572
Epoch: [30]  [1450/2809]  eta: 0:08:31  lr: 0.000008  min_lr: 0.000000  loss: 3.6431 (3.7029)  loss_scale: 32768.0000 (64452.0138)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0005  max mem: 15572
Epoch: [30]  [1460/2809]  eta: 0:08:27  lr: 0.000008  min_lr: 0.000000  loss: 3.7875 (3.7045)  loss_scale: 32768.0000 (64235.1485)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0004  max mem: 15572
[2025-01-13 08:18:38,664] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:18:38,664] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1470/2809]  eta: 0:08:23  lr: 0.000008  min_lr: 0.000000  loss: 3.7564 (3.7039)  loss_scale: 32768.0000 (64221.7158)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0003  max mem: 15572
Epoch: [30]  [1480/2809]  eta: 0:08:19  lr: 0.000008  min_lr: 0.000000  loss: 3.6967 (3.7042)  loss_scale: 65536.0000 (64230.5901)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [30]  [1490/2809]  eta: 0:08:16  lr: 0.000008  min_lr: 0.000000  loss: 3.8002 (3.7047)  loss_scale: 65536.0000 (64239.3454)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0002  max mem: 15572
Epoch: [30]  [1500/2809]  eta: 0:08:12  lr: 0.000008  min_lr: 0.000000  loss: 3.8002 (3.7056)  loss_scale: 65536.0000 (64247.9840)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [30]  [1510/2809]  eta: 0:08:08  lr: 0.000008  min_lr: 0.000000  loss: 3.7646 (3.7053)  loss_scale: 65536.0000 (64256.5083)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [30]  [1520/2809]  eta: 0:08:04  lr: 0.000008  min_lr: 0.000000  loss: 3.5132 (3.7044)  loss_scale: 65536.0000 (64264.9204)  weight_decay: 0.0500 (0.0500)  time: 0.3795  data: 0.0003  max mem: 15572
Epoch: [30]  [1530/2809]  eta: 0:08:01  lr: 0.000008  min_lr: 0.000000  loss: 3.5132 (3.7042)  loss_scale: 65536.0000 (64273.2227)  weight_decay: 0.0500 (0.0500)  time: 0.3903  data: 0.0005  max mem: 15572
[2025-01-13 08:19:05,205] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85802
[2025-01-13 08:19:05,206] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:19:05,206] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1540/2809]  eta: 0:07:57  lr: 0.000008  min_lr: 0.000000  loss: 3.6597 (3.7042)  loss_scale: 65536.0000 (64090.0402)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0005  max mem: 15572
Epoch: [30]  [1550/2809]  eta: 0:07:53  lr: 0.000008  min_lr: 0.000000  loss: 3.8398 (3.7052)  loss_scale: 32768.0000 (63888.0928)  weight_decay: 0.0500 (0.0500)  time: 0.3910  data: 0.0005  max mem: 15572
Epoch: [30]  [1560/2809]  eta: 0:07:50  lr: 0.000008  min_lr: 0.000000  loss: 3.8854 (3.7052)  loss_scale: 32768.0000 (63688.7329)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0004  max mem: 15572
Epoch: [30]  [1570/2809]  eta: 0:07:46  lr: 0.000008  min_lr: 0.000000  loss: 3.8854 (3.7058)  loss_scale: 32768.0000 (63491.9109)  weight_decay: 0.0500 (0.0500)  time: 0.3868  data: 0.0005  max mem: 15572
Epoch: [30]  [1580/2809]  eta: 0:07:42  lr: 0.000008  min_lr: 0.000000  loss: 3.9409 (3.7075)  loss_scale: 32768.0000 (63297.5787)  weight_decay: 0.0500 (0.0500)  time: 0.3822  data: 0.0004  max mem: 15572
Epoch: [30]  [1590/2809]  eta: 0:07:39  lr: 0.000008  min_lr: 0.000000  loss: 4.0355 (3.7089)  loss_scale: 32768.0000 (63105.6895)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [30]  [1600/2809]  eta: 0:07:35  lr: 0.000008  min_lr: 0.000000  loss: 3.7928 (3.7093)  loss_scale: 32768.0000 (62916.1974)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [30]  [1610/2809]  eta: 0:07:31  lr: 0.000008  min_lr: 0.000000  loss: 3.6877 (3.7092)  loss_scale: 32768.0000 (62729.0577)  weight_decay: 0.0500 (0.0500)  time: 0.3667  data: 0.0002  max mem: 15572
Epoch: [30]  [1620/2809]  eta: 0:07:27  lr: 0.000008  min_lr: 0.000000  loss: 3.6433 (3.7085)  loss_scale: 32768.0000 (62544.2270)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [30]  [1630/2809]  eta: 0:07:23  lr: 0.000008  min_lr: 0.000000  loss: 3.4096 (3.7074)  loss_scale: 32768.0000 (62361.6628)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [30]  [1640/2809]  eta: 0:07:19  lr: 0.000008  min_lr: 0.000000  loss: 3.6259 (3.7076)  loss_scale: 32768.0000 (62181.3236)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [30]  [1650/2809]  eta: 0:07:16  lr: 0.000008  min_lr: 0.000000  loss: 3.7545 (3.7076)  loss_scale: 32768.0000 (62003.1690)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [30]  [1660/2809]  eta: 0:07:12  lr: 0.000008  min_lr: 0.000000  loss: 3.6505 (3.7066)  loss_scale: 32768.0000 (61827.1595)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
[2025-01-13 08:19:53,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:19:53,863] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1670/2809]  eta: 0:07:08  lr: 0.000008  min_lr: 0.000000  loss: 3.5672 (3.7058)  loss_scale: 32768.0000 (61849.3549)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
Epoch: [30]  [1680/2809]  eta: 0:07:04  lr: 0.000008  min_lr: 0.000000  loss: 3.6830 (3.7050)  loss_scale: 65536.0000 (61871.2861)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0004  max mem: 15572
Epoch: [30]  [1690/2809]  eta: 0:07:01  lr: 0.000008  min_lr: 0.000000  loss: 3.7684 (3.7057)  loss_scale: 65536.0000 (61892.9580)  weight_decay: 0.0500 (0.0500)  time: 0.3883  data: 0.0004  max mem: 15572
Epoch: [30]  [1700/2809]  eta: 0:06:57  lr: 0.000008  min_lr: 0.000000  loss: 3.6246 (3.7047)  loss_scale: 65536.0000 (61914.3751)  weight_decay: 0.0500 (0.0500)  time: 0.3873  data: 0.0004  max mem: 15572
Epoch: [30]  [1710/2809]  eta: 0:06:53  lr: 0.000008  min_lr: 0.000000  loss: 3.7862 (3.7056)  loss_scale: 65536.0000 (61935.5418)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0005  max mem: 15572
Epoch: [30]  [1720/2809]  eta: 0:06:50  lr: 0.000008  min_lr: 0.000000  loss: 3.9034 (3.7062)  loss_scale: 65536.0000 (61956.4625)  weight_decay: 0.0500 (0.0500)  time: 0.3856  data: 0.0004  max mem: 15572
[2025-01-13 08:20:19,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=86000, skipped=585, lr=[7.628959446008848e-08, 7.628959446008848e-08, 1.0898513494298357e-07, 1.0898513494298357e-07, 1.5569304991854797e-07, 1.5569304991854797e-07, 2.2241864274078283e-07, 2.2241864274078283e-07, 3.177409182011183e-07, 3.177409182011183e-07, 4.5391559743016906e-07, 4.5391559743016906e-07, 6.484508534716701e-07, 6.484508534716701e-07, 9.26358362102386e-07, 9.26358362102386e-07, 1.3233690887176942e-06, 1.3233690887176942e-06, 1.8905272695967064e-06, 1.8905272695967064e-06, 2.700753242281009e-06, 2.700753242281009e-06, 3.858218917544299e-06, 3.858218917544299e-06, 5.5117413107775706e-06, 5.5117413107775706e-06, 7.873916158253672e-06, 7.873916158253672e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 08:20:19,934] [INFO] [timer.py:260:stop] epoch=0/micro_step=86000/global_step=86000, RunningAvgSamplesPerSec=31.042826258942537, CurrSamplesPerSec=34.90913933374486, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [30]  [1730/2809]  eta: 0:06:46  lr: 0.000008  min_lr: 0.000000  loss: 3.6386 (3.7062)  loss_scale: 65536.0000 (61977.1415)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [30]  [1740/2809]  eta: 0:06:42  lr: 0.000008  min_lr: 0.000000  loss: 3.7343 (3.7067)  loss_scale: 65536.0000 (61997.5830)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [30]  [1750/2809]  eta: 0:06:38  lr: 0.000008  min_lr: 0.000000  loss: 3.8213 (3.7066)  loss_scale: 65536.0000 (62017.7910)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
Epoch: [30]  [1760/2809]  eta: 0:06:35  lr: 0.000008  min_lr: 0.000000  loss: 3.6898 (3.7055)  loss_scale: 65536.0000 (62037.7694)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0003  max mem: 15572
Epoch: [30]  [1770/2809]  eta: 0:06:31  lr: 0.000008  min_lr: 0.000000  loss: 3.6898 (3.7048)  loss_scale: 65536.0000 (62057.5223)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [30]  [1780/2809]  eta: 0:06:27  lr: 0.000008  min_lr: 0.000000  loss: 3.4914 (3.7036)  loss_scale: 65536.0000 (62077.0533)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0003  max mem: 15572
[2025-01-13 08:20:42,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:20:42,288] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [1790/2809]  eta: 0:06:23  lr: 0.000008  min_lr: 0.000000  loss: 3.6073 (3.7046)  loss_scale: 65536.0000 (62169.5500)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
[2025-01-13 08:20:43,032] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86061
[2025-01-13 08:20:43,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:20:43,032] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [1800/2809]  eta: 0:06:19  lr: 0.000008  min_lr: 0.000000  loss: 3.8584 (3.7045)  loss_scale: 65536.0000 (62188.2421)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [30]  [1810/2809]  eta: 0:06:16  lr: 0.000008  min_lr: 0.000000  loss: 3.8113 (3.7044)  loss_scale: 65536.0000 (62206.7278)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0003  max mem: 15572
Epoch: [30]  [1820/2809]  eta: 0:06:12  lr: 0.000008  min_lr: 0.000000  loss: 3.9196 (3.7051)  loss_scale: 65536.0000 (62225.0104)  weight_decay: 0.0500 (0.0500)  time: 0.3860  data: 0.0004  max mem: 15572
Epoch: [30]  [1830/2809]  eta: 0:06:08  lr: 0.000008  min_lr: 0.000000  loss: 3.9196 (3.7044)  loss_scale: 65536.0000 (62243.0934)  weight_decay: 0.0500 (0.0500)  time: 0.3890  data: 0.0005  max mem: 15572
Epoch: [30]  [1840/2809]  eta: 0:06:05  lr: 0.000008  min_lr: 0.000000  loss: 3.6667 (3.7036)  loss_scale: 65536.0000 (62260.9799)  weight_decay: 0.0500 (0.0500)  time: 0.3940  data: 0.0005  max mem: 15572
Epoch: [30]  [1850/2809]  eta: 0:06:01  lr: 0.000008  min_lr: 0.000000  loss: 3.6440 (3.7032)  loss_scale: 65536.0000 (62278.6731)  weight_decay: 0.0500 (0.0500)  time: 0.3927  data: 0.0005  max mem: 15572
Epoch: [30]  [1860/2809]  eta: 0:05:57  lr: 0.000008  min_lr: 0.000000  loss: 3.6440 (3.7038)  loss_scale: 65536.0000 (62296.1762)  weight_decay: 0.0500 (0.0500)  time: 0.3855  data: 0.0005  max mem: 15572
Epoch: [30]  [1870/2809]  eta: 0:05:53  lr: 0.000008  min_lr: 0.000000  loss: 3.4918 (3.7027)  loss_scale: 65536.0000 (62313.4923)  weight_decay: 0.0500 (0.0500)  time: 0.3829  data: 0.0004  max mem: 15572
Epoch: [30]  [1880/2809]  eta: 0:05:50  lr: 0.000008  min_lr: 0.000000  loss: 3.3912 (3.7018)  loss_scale: 65536.0000 (62330.6241)  weight_decay: 0.0500 (0.0500)  time: 0.3779  data: 0.0003  max mem: 15572
Epoch: [30]  [1890/2809]  eta: 0:05:46  lr: 0.000008  min_lr: 0.000000  loss: 3.7316 (3.7025)  loss_scale: 65536.0000 (62347.5748)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [30]  [1900/2809]  eta: 0:05:42  lr: 0.000008  min_lr: 0.000000  loss: 3.7316 (3.7018)  loss_scale: 65536.0000 (62364.3472)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [30]  [1910/2809]  eta: 0:05:38  lr: 0.000008  min_lr: 0.000000  loss: 3.9588 (3.7038)  loss_scale: 65536.0000 (62380.9440)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
[2025-01-13 08:21:32,250] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:21:32,250] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [1920/2809]  eta: 0:05:34  lr: 0.000008  min_lr: 0.000000  loss: 3.9428 (3.7032)  loss_scale: 65536.0000 (62431.4836)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0003  max mem: 15572
[2025-01-13 08:21:32,624] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86191
[2025-01-13 08:21:32,624] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:21:32,624] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [1930/2809]  eta: 0:05:31  lr: 0.000008  min_lr: 0.000000  loss: 3.6097 (3.7029)  loss_scale: 65536.0000 (62447.5608)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0003  max mem: 15572
Epoch: [30]  [1940/2809]  eta: 0:05:27  lr: 0.000008  min_lr: 0.000000  loss: 3.6402 (3.7031)  loss_scale: 65536.0000 (62463.4724)  weight_decay: 0.0500 (0.0500)  time: 0.3823  data: 0.0004  max mem: 15572
Epoch: [30]  [1950/2809]  eta: 0:05:23  lr: 0.000008  min_lr: 0.000000  loss: 3.6317 (3.7026)  loss_scale: 65536.0000 (62479.2209)  weight_decay: 0.0500 (0.0500)  time: 0.3852  data: 0.0004  max mem: 15572
Epoch: [30]  [1960/2809]  eta: 0:05:20  lr: 0.000008  min_lr: 0.000000  loss: 3.5208 (3.7012)  loss_scale: 65536.0000 (62494.8088)  weight_decay: 0.0500 (0.0500)  time: 0.3858  data: 0.0003  max mem: 15572
Epoch: [30]  [1970/2809]  eta: 0:05:16  lr: 0.000008  min_lr: 0.000000  loss: 3.4866 (3.7009)  loss_scale: 65536.0000 (62510.2385)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0004  max mem: 15572
Epoch: [30]  [1980/2809]  eta: 0:05:12  lr: 0.000008  min_lr: 0.000000  loss: 3.4866 (3.6998)  loss_scale: 65536.0000 (62525.5124)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0004  max mem: 15572
Epoch: [30]  [1990/2809]  eta: 0:05:08  lr: 0.000008  min_lr: 0.000000  loss: 3.8378 (3.7016)  loss_scale: 65536.0000 (62540.6328)  weight_decay: 0.0500 (0.0500)  time: 0.3914  data: 0.0004  max mem: 15572
Epoch: [30]  [2000/2809]  eta: 0:05:05  lr: 0.000008  min_lr: 0.000000  loss: 3.5574 (3.6997)  loss_scale: 65536.0000 (62555.6022)  weight_decay: 0.0500 (0.0500)  time: 0.3841  data: 0.0003  max mem: 15572
Epoch: [30]  [2010/2809]  eta: 0:05:01  lr: 0.000008  min_lr: 0.000000  loss: 3.3831 (3.6992)  loss_scale: 65536.0000 (62570.4227)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [30]  [2020/2809]  eta: 0:04:57  lr: 0.000008  min_lr: 0.000000  loss: 3.5067 (3.6979)  loss_scale: 65536.0000 (62585.0965)  weight_decay: 0.0500 (0.0500)  time: 0.3773  data: 0.0003  max mem: 15572
Epoch: [30]  [2030/2809]  eta: 0:04:53  lr: 0.000008  min_lr: 0.000000  loss: 3.5668 (3.6988)  loss_scale: 65536.0000 (62599.6258)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0003  max mem: 15572
Epoch: [30]  [2040/2809]  eta: 0:04:49  lr: 0.000008  min_lr: 0.000000  loss: 3.7454 (3.6987)  loss_scale: 65536.0000 (62614.0127)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
[2025-01-13 08:22:21,851] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:22:21,851] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [2050/2809]  eta: 0:04:46  lr: 0.000008  min_lr: 0.000000  loss: 3.6578 (3.6992)  loss_scale: 65536.0000 (62660.2126)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
[2025-01-13 08:22:22,598] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86322
[2025-01-13 08:22:22,598] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:22:22,598] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [2060/2809]  eta: 0:04:42  lr: 0.000008  min_lr: 0.000000  loss: 3.6826 (3.6993)  loss_scale: 65536.0000 (62705.9641)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [30]  [2070/2809]  eta: 0:04:38  lr: 0.000008  min_lr: 0.000000  loss: 3.7407 (3.6993)  loss_scale: 65536.0000 (62719.6292)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [30]  [2080/2809]  eta: 0:04:34  lr: 0.000008  min_lr: 0.000000  loss: 3.7449 (3.6988)  loss_scale: 65536.0000 (62733.1629)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
Epoch: [30]  [2090/2809]  eta: 0:04:31  lr: 0.000008  min_lr: 0.000000  loss: 3.7096 (3.6976)  loss_scale: 65536.0000 (62746.5672)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0004  max mem: 15572
Epoch: [30]  [2100/2809]  eta: 0:04:27  lr: 0.000008  min_lr: 0.000000  loss: 3.7096 (3.6970)  loss_scale: 65536.0000 (62759.8439)  weight_decay: 0.0500 (0.0500)  time: 0.3895  data: 0.0006  max mem: 15572
Epoch: [30]  [2110/2809]  eta: 0:04:23  lr: 0.000008  min_lr: 0.000000  loss: 3.8142 (3.6972)  loss_scale: 65536.0000 (62772.9948)  weight_decay: 0.0500 (0.0500)  time: 0.3929  data: 0.0006  max mem: 15572
Epoch: [30]  [2120/2809]  eta: 0:04:19  lr: 0.000008  min_lr: 0.000000  loss: 3.5838 (3.6972)  loss_scale: 65536.0000 (62786.0217)  weight_decay: 0.0500 (0.0500)  time: 0.3904  data: 0.0006  max mem: 15572
Epoch: [30]  [2130/2809]  eta: 0:04:16  lr: 0.000008  min_lr: 0.000000  loss: 3.6442 (3.6972)  loss_scale: 65536.0000 (62798.9263)  weight_decay: 0.0500 (0.0500)  time: 0.3886  data: 0.0005  max mem: 15572
Epoch: [30]  [2140/2809]  eta: 0:04:12  lr: 0.000008  min_lr: 0.000000  loss: 3.8687 (3.6978)  loss_scale: 65536.0000 (62811.7104)  weight_decay: 0.0500 (0.0500)  time: 0.3814  data: 0.0003  max mem: 15572
Epoch: [30]  [2150/2809]  eta: 0:04:08  lr: 0.000008  min_lr: 0.000000  loss: 3.9520 (3.6985)  loss_scale: 65536.0000 (62824.3756)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [30]  [2160/2809]  eta: 0:04:04  lr: 0.000008  min_lr: 0.000000  loss: 3.7957 (3.6989)  loss_scale: 65536.0000 (62836.9236)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [30]  [2170/2809]  eta: 0:04:01  lr: 0.000008  min_lr: 0.000000  loss: 3.7885 (3.6992)  loss_scale: 65536.0000 (62849.3561)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [30]  [2180/2809]  eta: 0:03:57  lr: 0.000008  min_lr: 0.000000  loss: 3.7670 (3.6990)  loss_scale: 65536.0000 (62861.6745)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0003  max mem: 15572
[2025-01-13 08:23:11,496] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:23:11,496] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:23:11,853] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86452
[2025-01-13 08:23:11,853] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:23:11,853] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [2190/2809]  eta: 0:03:53  lr: 0.000008  min_lr: 0.000000  loss: 3.8655 (3.6994)  loss_scale: 65536.0000 (62903.7919)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [30]  [2200/2809]  eta: 0:03:49  lr: 0.000008  min_lr: 0.000000  loss: 3.7267 (3.6996)  loss_scale: 65536.0000 (62915.7510)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [30]  [2210/2809]  eta: 0:03:45  lr: 0.000008  min_lr: 0.000000  loss: 3.6715 (3.6988)  loss_scale: 65536.0000 (62927.6020)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [30]  [2220/2809]  eta: 0:03:42  lr: 0.000008  min_lr: 0.000000  loss: 3.5543 (3.6984)  loss_scale: 65536.0000 (62939.3462)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [30]  [2230/2809]  eta: 0:03:38  lr: 0.000008  min_lr: 0.000000  loss: 3.3964 (3.6953)  loss_scale: 65536.0000 (62950.9852)  weight_decay: 0.0500 (0.0500)  time: 0.3794  data: 0.0004  max mem: 15572
Epoch: [30]  [2240/2809]  eta: 0:03:34  lr: 0.000008  min_lr: 0.000000  loss: 3.5110 (3.6963)  loss_scale: 65536.0000 (62962.5203)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0005  max mem: 15572
Epoch: [30]  [2250/2809]  eta: 0:03:30  lr: 0.000008  min_lr: 0.000000  loss: 3.8277 (3.6969)  loss_scale: 65536.0000 (62973.9529)  weight_decay: 0.0500 (0.0500)  time: 0.3814  data: 0.0003  max mem: 15572
Epoch: [30]  [2260/2809]  eta: 0:03:27  lr: 0.000008  min_lr: 0.000000  loss: 3.7995 (3.6970)  loss_scale: 65536.0000 (62985.2844)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0003  max mem: 15572
Epoch: [30]  [2270/2809]  eta: 0:03:23  lr: 0.000008  min_lr: 0.000000  loss: 3.8014 (3.6965)  loss_scale: 65536.0000 (62996.5161)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [30]  [2280/2809]  eta: 0:03:19  lr: 0.000008  min_lr: 0.000000  loss: 3.8178 (3.6981)  loss_scale: 65536.0000 (63007.6493)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [30]  [2290/2809]  eta: 0:03:15  lr: 0.000008  min_lr: 0.000000  loss: 3.7781 (3.6970)  loss_scale: 65536.0000 (63018.6853)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [30]  [2300/2809]  eta: 0:03:11  lr: 0.000008  min_lr: 0.000000  loss: 3.6409 (3.6974)  loss_scale: 65536.0000 (63029.6254)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [30]  [2310/2809]  eta: 0:03:08  lr: 0.000008  min_lr: 0.000000  loss: 3.7724 (3.6982)  loss_scale: 65536.0000 (63040.4708)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0002  max mem: 15572
[2025-01-13 08:24:00,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:24:00,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:24:03,148] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86589
[2025-01-13 08:24:03,148] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:24:03,148] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [2320/2809]  eta: 0:03:04  lr: 0.000008  min_lr: 0.000000  loss: 3.6787 (3.6970)  loss_scale: 65536.0000 (63277.1116)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
Epoch: [30]  [2330/2809]  eta: 0:03:00  lr: 0.000008  min_lr: 0.000000  loss: 3.6787 (3.6976)  loss_scale: 65536.0000 (63286.8022)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0003  max mem: 15572
Epoch: [30]  [2340/2809]  eta: 0:02:56  lr: 0.000008  min_lr: 0.000000  loss: 3.9334 (3.6985)  loss_scale: 65536.0000 (63296.4101)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
[2025-01-13 08:24:14,097] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86618
[2025-01-13 08:24:14,098] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:24:14,098] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [2350/2809]  eta: 0:02:53  lr: 0.000008  min_lr: 0.000000  loss: 3.8748 (3.6982)  loss_scale: 65536.0000 (63264.1225)  weight_decay: 0.0500 (0.0500)  time: 0.3810  data: 0.0003  max mem: 15572
Epoch: [30]  [2360/2809]  eta: 0:02:49  lr: 0.000008  min_lr: 0.000000  loss: 3.8388 (3.6982)  loss_scale: 32768.0000 (63134.9564)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0003  max mem: 15572
Epoch: [30]  [2370/2809]  eta: 0:02:45  lr: 0.000008  min_lr: 0.000000  loss: 3.8388 (3.6980)  loss_scale: 32768.0000 (63006.8798)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [30]  [2380/2809]  eta: 0:02:41  lr: 0.000008  min_lr: 0.000000  loss: 3.7783 (3.6976)  loss_scale: 32768.0000 (62879.8790)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [30]  [2390/2809]  eta: 0:02:37  lr: 0.000008  min_lr: 0.000000  loss: 3.7538 (3.6976)  loss_scale: 32768.0000 (62753.9406)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [30]  [2400/2809]  eta: 0:02:34  lr: 0.000008  min_lr: 0.000000  loss: 3.5499 (3.6966)  loss_scale: 32768.0000 (62629.0512)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [30]  [2410/2809]  eta: 0:02:30  lr: 0.000007  min_lr: 0.000000  loss: 3.6951 (3.6963)  loss_scale: 32768.0000 (62505.1978)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [30]  [2420/2809]  eta: 0:02:26  lr: 0.000007  min_lr: 0.000000  loss: 3.7268 (3.6960)  loss_scale: 32768.0000 (62382.3676)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [30]  [2430/2809]  eta: 0:02:22  lr: 0.000007  min_lr: 0.000000  loss: 3.4160 (3.6948)  loss_scale: 32768.0000 (62260.5479)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [30]  [2440/2809]  eta: 0:02:19  lr: 0.000007  min_lr: 0.000000  loss: 3.6301 (3.6946)  loss_scale: 32768.0000 (62139.7263)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [30]  [2450/2809]  eta: 0:02:15  lr: 0.000007  min_lr: 0.000000  loss: 3.7216 (3.6947)  loss_scale: 32768.0000 (62019.8907)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [30]  [2460/2809]  eta: 0:02:11  lr: 0.000007  min_lr: 0.000000  loss: 3.5567 (3.6936)  loss_scale: 32768.0000 (61901.0289)  weight_decay: 0.0500 (0.0500)  time: 0.3941  data: 0.0005  max mem: 15572
Epoch: [30]  [2470/2809]  eta: 0:02:07  lr: 0.000007  min_lr: 0.000000  loss: 3.5121 (3.6929)  loss_scale: 32768.0000 (61783.1291)  weight_decay: 0.0500 (0.0500)  time: 0.3947  data: 0.0005  max mem: 15572
[2025-01-13 08:25:02,727] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:25:02,727] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [2480/2809]  eta: 0:02:04  lr: 0.000007  min_lr: 0.000000  loss: 3.5738 (3.6933)  loss_scale: 32768.0000 (61719.0101)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0003  max mem: 15572
Epoch: [30]  [2490/2809]  eta: 0:02:00  lr: 0.000007  min_lr: 0.000000  loss: 3.5738 (3.6928)  loss_scale: 65536.0000 (61734.3332)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [30]  [2500/2809]  eta: 0:01:56  lr: 0.000007  min_lr: 0.000000  loss: 3.5144 (3.6924)  loss_scale: 65536.0000 (61749.5338)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [30]  [2510/2809]  eta: 0:01:52  lr: 0.000007  min_lr: 0.000000  loss: 3.6882 (3.6921)  loss_scale: 65536.0000 (61764.6133)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [30]  [2520/2809]  eta: 0:01:48  lr: 0.000007  min_lr: 0.000000  loss: 3.7222 (3.6925)  loss_scale: 65536.0000 (61779.5732)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [30]  [2530/2809]  eta: 0:01:45  lr: 0.000007  min_lr: 0.000000  loss: 3.7222 (3.6923)  loss_scale: 65536.0000 (61794.4149)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [30]  [2540/2809]  eta: 0:01:41  lr: 0.000007  min_lr: 0.000000  loss: 3.5968 (3.6923)  loss_scale: 65536.0000 (61809.1397)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [30]  [2550/2809]  eta: 0:01:37  lr: 0.000007  min_lr: 0.000000  loss: 3.5979 (3.6922)  loss_scale: 65536.0000 (61823.7491)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [30]  [2560/2809]  eta: 0:01:33  lr: 0.000007  min_lr: 0.000000  loss: 3.7420 (3.6929)  loss_scale: 65536.0000 (61838.2444)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [30]  [2570/2809]  eta: 0:01:30  lr: 0.000007  min_lr: 0.000000  loss: 3.8198 (3.6930)  loss_scale: 65536.0000 (61852.6270)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0003  max mem: 15572
Epoch: [30]  [2580/2809]  eta: 0:01:26  lr: 0.000007  min_lr: 0.000000  loss: 3.7069 (3.6920)  loss_scale: 65536.0000 (61866.8981)  weight_decay: 0.0500 (0.0500)  time: 0.3819  data: 0.0003  max mem: 15572
Epoch: [30]  [2590/2809]  eta: 0:01:22  lr: 0.000007  min_lr: 0.000000  loss: 3.5761 (3.6915)  loss_scale: 65536.0000 (61881.0591)  weight_decay: 0.0500 (0.0500)  time: 0.3819  data: 0.0003  max mem: 15572
Epoch: [30]  [2600/2809]  eta: 0:01:18  lr: 0.000007  min_lr: 0.000000  loss: 3.5871 (3.6922)  loss_scale: 65536.0000 (61895.1111)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0003  max mem: 15572
[2025-01-13 08:25:50,396] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:25:50,396] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:25:50,758] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86876
[2025-01-13 08:25:50,758] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:25:50,759] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-13 08:25:51,130] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86877
[2025-01-13 08:25:51,130] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:25:51,130] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [2610/2809]  eta: 0:01:14  lr: 0.000007  min_lr: 0.000000  loss: 3.7887 (3.6920)  loss_scale: 65536.0000 (61883.9556)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0003  max mem: 15572
Epoch: [30]  [2620/2809]  eta: 0:01:11  lr: 0.000007  min_lr: 0.000000  loss: 3.7552 (3.6929)  loss_scale: 32768.0000 (61772.8684)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0003  max mem: 15572
Epoch: [30]  [2630/2809]  eta: 0:01:07  lr: 0.000007  min_lr: 0.000000  loss: 3.7552 (3.6931)  loss_scale: 32768.0000 (61662.6256)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [30]  [2640/2809]  eta: 0:01:03  lr: 0.000007  min_lr: 0.000000  loss: 3.9680 (3.6942)  loss_scale: 32768.0000 (61553.2177)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0003  max mem: 15572
Epoch: [30]  [2650/2809]  eta: 0:00:59  lr: 0.000007  min_lr: 0.000000  loss: 3.8895 (3.6945)  loss_scale: 32768.0000 (61444.6352)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [30]  [2660/2809]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 3.8376 (3.6947)  loss_scale: 32768.0000 (61336.8688)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [30]  [2670/2809]  eta: 0:00:52  lr: 0.000007  min_lr: 0.000000  loss: 3.9187 (3.6953)  loss_scale: 32768.0000 (61229.9094)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [30]  [2680/2809]  eta: 0:00:48  lr: 0.000007  min_lr: 0.000000  loss: 3.6630 (3.6951)  loss_scale: 32768.0000 (61123.7479)  weight_decay: 0.0500 (0.0500)  time: 0.3779  data: 0.0003  max mem: 15572
Epoch: [30]  [2690/2809]  eta: 0:00:44  lr: 0.000007  min_lr: 0.000000  loss: 3.6445 (3.6946)  loss_scale: 32768.0000 (61018.3753)  weight_decay: 0.0500 (0.0500)  time: 0.3815  data: 0.0004  max mem: 15572
Epoch: [30]  [2700/2809]  eta: 0:00:41  lr: 0.000007  min_lr: 0.000000  loss: 3.6883 (3.6944)  loss_scale: 32768.0000 (60913.7830)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [30]  [2710/2809]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 3.8214 (3.6947)  loss_scale: 32768.0000 (60809.9624)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [30]  [2720/2809]  eta: 0:00:33  lr: 0.000007  min_lr: 0.000000  loss: 3.7508 (3.6946)  loss_scale: 32768.0000 (60706.9048)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
[2025-01-13 08:26:36,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=87000, skipped=593, lr=[7.09583022273898e-08, 7.09583022273898e-08, 1.0136900318198544e-07, 1.0136900318198544e-07, 1.4481286168855063e-07, 1.4481286168855063e-07, 2.068755166979295e-07, 2.068755166979295e-07, 2.955364524256136e-07, 2.955364524256136e-07, 4.2219493203659086e-07, 4.2219493203659086e-07, 6.031356171951298e-07, 6.031356171951298e-07, 8.61622310278757e-07, 8.61622310278757e-07, 1.2308890146839386e-06, 1.2308890146839386e-06, 1.7584128781199124e-06, 1.7584128781199124e-06, 2.5120183973141607e-06, 2.5120183973141607e-06, 3.588597710448801e-06, 3.588597710448801e-06, 5.1265681577840024e-06, 5.1265681577840024e-06, 7.323668796834289e-06, 7.323668796834289e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 08:26:36,509] [INFO] [timer.py:260:stop] epoch=0/micro_step=87000/global_step=87000, RunningAvgSamplesPerSec=31.071145543697025, CurrSamplesPerSec=34.97145191739142, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [30]  [2730/2809]  eta: 0:00:29  lr: 0.000007  min_lr: 0.000000  loss: 3.7700 (3.6950)  loss_scale: 32768.0000 (60604.6020)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
[2025-01-13 08:26:39,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:26:39,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [2740/2809]  eta: 0:00:25  lr: 0.000007  min_lr: 0.000000  loss: 3.6228 (3.6938)  loss_scale: 32768.0000 (60562.8194)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [30]  [2750/2809]  eta: 0:00:22  lr: 0.000007  min_lr: 0.000000  loss: 3.3056 (3.6926)  loss_scale: 65536.0000 (60580.8971)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [30]  [2760/2809]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 3.5428 (3.6930)  loss_scale: 65536.0000 (60598.8439)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [30]  [2770/2809]  eta: 0:00:14  lr: 0.000007  min_lr: 0.000000  loss: 3.8079 (3.6930)  loss_scale: 65536.0000 (60616.6611)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [30]  [2780/2809]  eta: 0:00:10  lr: 0.000007  min_lr: 0.000000  loss: 3.8033 (3.6930)  loss_scale: 65536.0000 (60634.3502)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [30]  [2790/2809]  eta: 0:00:07  lr: 0.000007  min_lr: 0.000000  loss: 3.6362 (3.6927)  loss_scale: 65536.0000 (60651.9126)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [30]  [2800/2809]  eta: 0:00:03  lr: 0.000007  min_lr: 0.000000  loss: 3.6699 (3.6930)  loss_scale: 65536.0000 (60669.3495)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [30]  [2808/2809]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 3.6699 (3.6929)  loss_scale: 65536.0000 (60683.2097)  weight_decay: 0.0500 (0.0500)  time: 0.3602  data: 0.0001  max mem: 15572
Epoch: [30] Total time: 0:17:37 (0.3765 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 3.6699 (3.6929)  loss_scale: 65536.0000 (60683.2097)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:10:34  loss: 0.3898 (0.3898)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3338  data: 2.1599  max mem: 15572
Val:  [ 10/272]  eta: 0:02:01  loss: 2.4904 (2.2864)  acc1: 50.0000 (43.9394)  acc5: 72.2222 (72.2222)  time: 0.4643  data: 0.2920  max mem: 15572
Val:  [ 20/272]  eta: 0:01:21  loss: 2.2483 (2.2730)  acc1: 50.0000 (47.3545)  acc5: 72.2222 (73.5450)  time: 0.2216  data: 0.0529  max mem: 15572
Val:  [ 30/272]  eta: 0:01:05  loss: 2.3252 (2.3801)  acc1: 44.4444 (43.1900)  acc5: 77.7778 (72.9391)  time: 0.1672  data: 0.0006  max mem: 15572
Val:  [ 40/272]  eta: 0:00:57  loss: 2.5603 (2.4441)  acc1: 33.3333 (40.7859)  acc5: 77.7778 (73.1707)  time: 0.1689  data: 0.0027  max mem: 15572
Val:  [ 50/272]  eta: 0:00:52  loss: 2.4736 (2.3701)  acc1: 33.3333 (42.1569)  acc5: 77.7778 (74.8366)  time: 0.1796  data: 0.0027  max mem: 15572
Val:  [ 60/272]  eta: 0:00:47  loss: 1.4456 (2.2628)  acc1: 61.1111 (45.5373)  acc5: 83.3333 (75.8652)  time: 0.1788  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:43  loss: 1.5066 (2.1887)  acc1: 66.6667 (47.8873)  acc5: 83.3333 (76.6823)  time: 0.1615  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:40  loss: 1.8962 (2.2057)  acc1: 61.1111 (47.6680)  acc5: 77.7778 (76.2689)  time: 0.1579  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:36  loss: 2.1737 (2.2160)  acc1: 50.0000 (47.8632)  acc5: 77.7778 (76.8010)  time: 0.1606  data: 0.0059  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 2.1389 (2.2381)  acc1: 55.5556 (47.1947)  acc5: 83.3333 (76.7877)  time: 0.1716  data: 0.0196  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 2.3168 (2.3106)  acc1: 27.7778 (45.0951)  acc5: 77.7778 (75.7758)  time: 0.1771  data: 0.0142  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 2.8533 (2.3478)  acc1: 16.6667 (44.3067)  acc5: 72.2222 (75.4362)  time: 0.1802  data: 0.0088  max mem: 15572
Val:  [130/272]  eta: 0:00:28  loss: 2.2378 (2.3099)  acc1: 50.0000 (45.4623)  acc5: 83.3333 (76.1662)  time: 0.1958  data: 0.0324  max mem: 15572
Val:  [140/272]  eta: 0:00:26  loss: 1.7273 (2.2974)  acc1: 55.5556 (45.8235)  acc5: 83.3333 (76.1229)  time: 0.2161  data: 0.0604  max mem: 15572
Val:  [150/272]  eta: 0:00:24  loss: 2.2852 (2.3013)  acc1: 38.8889 (45.3274)  acc5: 77.7778 (76.3429)  time: 0.1956  data: 0.0367  max mem: 15572
Val:  [160/272]  eta: 0:00:22  loss: 2.2852 (2.2907)  acc1: 44.4444 (45.8937)  acc5: 77.7778 (76.6391)  time: 0.1726  data: 0.0005  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 2.4337 (2.3122)  acc1: 44.4444 (45.3216)  acc5: 72.2222 (76.1858)  time: 0.1650  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 2.3136 (2.3020)  acc1: 33.3333 (45.0890)  acc5: 77.7778 (76.6114)  time: 0.1544  data: 0.0003  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 2.3136 (2.3560)  acc1: 33.3333 (43.8918)  acc5: 77.7778 (75.2763)  time: 0.1582  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 2.5742 (2.3646)  acc1: 33.3333 (43.6429)  acc5: 72.2222 (74.9585)  time: 0.1635  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.0840 (2.3647)  acc1: 50.0000 (43.9442)  acc5: 77.7778 (74.9078)  time: 0.1692  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 2.1005 (2.3513)  acc1: 50.0000 (44.1679)  acc5: 77.7778 (75.0880)  time: 0.1688  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.8271 (2.3203)  acc1: 61.1111 (45.2381)  acc5: 83.3333 (75.4209)  time: 0.1653  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.6886 (2.3084)  acc1: 61.1111 (45.3435)  acc5: 83.3333 (75.6570)  time: 0.1626  data: 0.0056  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 2.2829 (2.3192)  acc1: 33.3333 (44.7100)  acc5: 77.7778 (75.6529)  time: 0.1681  data: 0.0102  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.1679 (2.2603)  acc1: 72.2222 (46.4453)  acc5: 88.8889 (76.3942)  time: 0.1629  data: 0.0049  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3385 (2.2548)  acc1: 72.2222 (46.5765)  acc5: 88.8889 (76.5888)  time: 0.1442  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3385 (2.2601)  acc1: 66.6667 (46.5493)  acc5: 88.8889 (76.5513)  time: 0.1384  data: 0.0001  max mem: 15572
Val: Total time: 0:00:49 (0.1820 s / it)
* Acc@1 46.549 Acc@5 76.551 loss 2.260
Accuracy of the network on the 4883 val videos: 46.5%
Max accuracy: 46.63%
Epoch: [31]  [   0/2809]  eta: 3:48:37  lr: 0.000007  min_lr: 0.000000  loss: 4.3883 (4.3883)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.8834  data: 4.4254  max mem: 15572
Epoch: [31]  [  10/2809]  eta: 0:36:47  lr: 0.000007  min_lr: 0.000000  loss: 3.4280 (3.5536)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7885  data: 0.4026  max mem: 15572
Epoch: [31]  [  20/2809]  eta: 0:28:10  lr: 0.000007  min_lr: 0.000000  loss: 3.4464 (3.6158)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3924  data: 0.0004  max mem: 15572
Epoch: [31]  [  30/2809]  eta: 0:24:42  lr: 0.000007  min_lr: 0.000000  loss: 3.8205 (3.6230)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3930  data: 0.0004  max mem: 15572
Epoch: [31]  [  40/2809]  eta: 0:22:48  lr: 0.000007  min_lr: 0.000000  loss: 3.9252 (3.7204)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0003  max mem: 15572
Epoch: [31]  [  50/2809]  eta: 0:21:35  lr: 0.000007  min_lr: 0.000000  loss: 4.0537 (3.7205)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
[2025-01-13 08:28:21,046] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:28:21,046] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:28:21,413] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87135
[2025-01-13 08:28:21,413] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:28:21,413] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [  60/2809]  eta: 0:20:49  lr: 0.000007  min_lr: 0.000000  loss: 3.7496 (3.6889)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [31]  [  70/2809]  eta: 0:20:15  lr: 0.000007  min_lr: 0.000000  loss: 3.6934 (3.6727)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0002  max mem: 15572
Epoch: [31]  [  80/2809]  eta: 0:19:49  lr: 0.000007  min_lr: 0.000000  loss: 3.6243 (3.6564)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0002  max mem: 15572
Epoch: [31]  [  90/2809]  eta: 0:19:27  lr: 0.000007  min_lr: 0.000000  loss: 3.4830 (3.6400)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.3786  data: 0.0003  max mem: 15572
Epoch: [31]  [ 100/2809]  eta: 0:19:06  lr: 0.000007  min_lr: 0.000000  loss: 3.7008 (3.6738)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0003  max mem: 15572
Epoch: [31]  [ 110/2809]  eta: 0:18:50  lr: 0.000007  min_lr: 0.000000  loss: 4.0751 (3.7044)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [31]  [ 120/2809]  eta: 0:18:34  lr: 0.000007  min_lr: 0.000000  loss: 3.7808 (3.7021)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [31]  [ 130/2809]  eta: 0:18:25  lr: 0.000007  min_lr: 0.000000  loss: 3.5468 (3.6882)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0003  max mem: 15572
Epoch: [31]  [ 140/2809]  eta: 0:18:17  lr: 0.000007  min_lr: 0.000000  loss: 3.6317 (3.6921)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.3902  data: 0.0005  max mem: 15572
Epoch: [31]  [ 150/2809]  eta: 0:18:08  lr: 0.000007  min_lr: 0.000000  loss: 3.6371 (3.6915)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.3855  data: 0.0005  max mem: 15572
Epoch: [31]  [ 160/2809]  eta: 0:17:58  lr: 0.000007  min_lr: 0.000000  loss: 3.6740 (3.6903)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.3786  data: 0.0004  max mem: 15572
Epoch: [31]  [ 170/2809]  eta: 0:17:49  lr: 0.000007  min_lr: 0.000000  loss: 3.7517 (3.6990)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [31]  [ 180/2809]  eta: 0:17:39  lr: 0.000007  min_lr: 0.000000  loss: 3.8039 (3.7026)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
[2025-01-13 08:29:09,974] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:29:09,974] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:29:11,444] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87268
[2025-01-13 08:29:11,444] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:29:11,444] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 190/2809]  eta: 0:17:30  lr: 0.000007  min_lr: 0.000000  loss: 3.8039 (3.7129)  loss_scale: 65536.0000 (67251.6021)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [31]  [ 200/2809]  eta: 0:17:22  lr: 0.000007  min_lr: 0.000000  loss: 3.7770 (3.7014)  loss_scale: 65536.0000 (67166.2488)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [31]  [ 210/2809]  eta: 0:17:14  lr: 0.000007  min_lr: 0.000000  loss: 3.6723 (3.7055)  loss_scale: 65536.0000 (67088.9858)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [31]  [ 220/2809]  eta: 0:17:07  lr: 0.000007  min_lr: 0.000000  loss: 3.6608 (3.6927)  loss_scale: 65536.0000 (67018.7149)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [31]  [ 230/2809]  eta: 0:17:00  lr: 0.000007  min_lr: 0.000000  loss: 3.6899 (3.7020)  loss_scale: 65536.0000 (66954.5281)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [31]  [ 240/2809]  eta: 0:16:53  lr: 0.000007  min_lr: 0.000000  loss: 3.9343 (3.7028)  loss_scale: 65536.0000 (66895.6680)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0003  max mem: 15572
Epoch: [31]  [ 250/2809]  eta: 0:16:47  lr: 0.000007  min_lr: 0.000000  loss: 3.7692 (3.7059)  loss_scale: 65536.0000 (66841.4980)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0003  max mem: 15572
Epoch: [31]  [ 260/2809]  eta: 0:16:41  lr: 0.000007  min_lr: 0.000000  loss: 3.9049 (3.7118)  loss_scale: 65536.0000 (66791.4789)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [31]  [ 270/2809]  eta: 0:16:35  lr: 0.000007  min_lr: 0.000000  loss: 3.6382 (3.7037)  loss_scale: 65536.0000 (66745.1513)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0003  max mem: 15572
Epoch: [31]  [ 280/2809]  eta: 0:16:30  lr: 0.000007  min_lr: 0.000000  loss: 3.4113 (3.6980)  loss_scale: 65536.0000 (66702.1210)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [31]  [ 290/2809]  eta: 0:16:25  lr: 0.000007  min_lr: 0.000000  loss: 3.7128 (3.7058)  loss_scale: 65536.0000 (66662.0481)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0003  max mem: 15572
Epoch: [31]  [ 300/2809]  eta: 0:16:20  lr: 0.000007  min_lr: 0.000000  loss: 3.7625 (3.7056)  loss_scale: 65536.0000 (66624.6379)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0003  max mem: 15572
Epoch: [31]  [ 310/2809]  eta: 0:16:14  lr: 0.000007  min_lr: 0.000000  loss: 3.7351 (3.7040)  loss_scale: 65536.0000 (66589.6334)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0003  max mem: 15572
[2025-01-13 08:29:59,558] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:29:59,558] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:30:00,294] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87399
[2025-01-13 08:30:00,294] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:30:00,294] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 320/2809]  eta: 0:16:09  lr: 0.000007  min_lr: 0.000000  loss: 3.6736 (3.6984)  loss_scale: 65536.0000 (66965.1340)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0003  max mem: 15572
Epoch: [31]  [ 330/2809]  eta: 0:16:04  lr: 0.000007  min_lr: 0.000000  loss: 3.5277 (3.6941)  loss_scale: 65536.0000 (66921.9577)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [31]  [ 340/2809]  eta: 0:15:58  lr: 0.000007  min_lr: 0.000000  loss: 3.6576 (3.6940)  loss_scale: 65536.0000 (66881.3138)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [31]  [ 350/2809]  eta: 0:15:54  lr: 0.000007  min_lr: 0.000000  loss: 3.9004 (3.7015)  loss_scale: 65536.0000 (66842.9858)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0002  max mem: 15572
Epoch: [31]  [ 360/2809]  eta: 0:15:49  lr: 0.000007  min_lr: 0.000000  loss: 3.8847 (3.6930)  loss_scale: 65536.0000 (66806.7812)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [31]  [ 370/2809]  eta: 0:15:44  lr: 0.000007  min_lr: 0.000000  loss: 3.6154 (3.6949)  loss_scale: 65536.0000 (66772.5283)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [31]  [ 380/2809]  eta: 0:15:39  lr: 0.000007  min_lr: 0.000000  loss: 3.6154 (3.6876)  loss_scale: 65536.0000 (66740.0735)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0003  max mem: 15572
Epoch: [31]  [ 390/2809]  eta: 0:15:34  lr: 0.000007  min_lr: 0.000000  loss: 3.4589 (3.6837)  loss_scale: 65536.0000 (66709.2788)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [31]  [ 400/2809]  eta: 0:15:30  lr: 0.000007  min_lr: 0.000000  loss: 3.7056 (3.6800)  loss_scale: 65536.0000 (66680.0200)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0003  max mem: 15572
Epoch: [31]  [ 410/2809]  eta: 0:15:26  lr: 0.000007  min_lr: 0.000000  loss: 3.8453 (3.6798)  loss_scale: 65536.0000 (66652.1849)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0004  max mem: 15572
Epoch: [31]  [ 420/2809]  eta: 0:15:22  lr: 0.000007  min_lr: 0.000000  loss: 3.8972 (3.6872)  loss_scale: 65536.0000 (66625.6722)  weight_decay: 0.0500 (0.0500)  time: 0.3860  data: 0.0004  max mem: 15572
Epoch: [31]  [ 430/2809]  eta: 0:15:18  lr: 0.000007  min_lr: 0.000000  loss: 3.5725 (3.6764)  loss_scale: 65536.0000 (66600.3898)  weight_decay: 0.0500 (0.0500)  time: 0.3841  data: 0.0004  max mem: 15572
Epoch: [31]  [ 440/2809]  eta: 0:15:14  lr: 0.000007  min_lr: 0.000000  loss: 3.5298 (3.6743)  loss_scale: 65536.0000 (66576.2540)  weight_decay: 0.0500 (0.0500)  time: 0.3825  data: 0.0004  max mem: 15572
[2025-01-13 08:30:48,815] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:30:48,815] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [31]  [ 450/2809]  eta: 0:15:09  lr: 0.000007  min_lr: 0.000000  loss: 3.7274 (3.6771)  loss_scale: 65536.0000 (66843.8137)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0003  max mem: 15572
[2025-01-13 08:30:49,565] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87530
[2025-01-13 08:30:49,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:30:49,565] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 460/2809]  eta: 0:15:05  lr: 0.000007  min_lr: 0.000000  loss: 3.7291 (3.6766)  loss_scale: 65536.0000 (66815.4447)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [31]  [ 470/2809]  eta: 0:15:00  lr: 0.000007  min_lr: 0.000000  loss: 3.7115 (3.6765)  loss_scale: 65536.0000 (66788.2803)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [31]  [ 480/2809]  eta: 0:14:56  lr: 0.000007  min_lr: 0.000000  loss: 3.7305 (3.6762)  loss_scale: 65536.0000 (66762.2453)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [31]  [ 490/2809]  eta: 0:14:51  lr: 0.000007  min_lr: 0.000000  loss: 3.7590 (3.6793)  loss_scale: 65536.0000 (66737.2709)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [31]  [ 500/2809]  eta: 0:14:47  lr: 0.000007  min_lr: 0.000000  loss: 3.9162 (3.6865)  loss_scale: 65536.0000 (66713.2934)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [31]  [ 510/2809]  eta: 0:14:42  lr: 0.000007  min_lr: 0.000000  loss: 3.8095 (3.6839)  loss_scale: 65536.0000 (66690.2544)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [31]  [ 520/2809]  eta: 0:14:38  lr: 0.000007  min_lr: 0.000000  loss: 3.5366 (3.6803)  loss_scale: 65536.0000 (66668.0998)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [31]  [ 530/2809]  eta: 0:14:33  lr: 0.000007  min_lr: 0.000000  loss: 3.8892 (3.6819)  loss_scale: 65536.0000 (66646.7797)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [31]  [ 540/2809]  eta: 0:14:29  lr: 0.000007  min_lr: 0.000000  loss: 3.8892 (3.6841)  loss_scale: 65536.0000 (66626.2477)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [31]  [ 550/2809]  eta: 0:14:25  lr: 0.000007  min_lr: 0.000000  loss: 3.8695 (3.6864)  loss_scale: 65536.0000 (66606.4610)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
Epoch: [31]  [ 560/2809]  eta: 0:14:20  lr: 0.000007  min_lr: 0.000000  loss: 3.6786 (3.6843)  loss_scale: 65536.0000 (66587.3797)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0003  max mem: 15572
Epoch: [31]  [ 570/2809]  eta: 0:14:16  lr: 0.000007  min_lr: 0.000000  loss: 3.5819 (3.6847)  loss_scale: 65536.0000 (66568.9667)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
[2025-01-13 08:31:37,382] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:31:37,382] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [31]  [ 580/2809]  eta: 0:14:12  lr: 0.000007  min_lr: 0.000000  loss: 3.4701 (3.6777)  loss_scale: 65536.0000 (66663.9862)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
[2025-01-13 08:31:40,340] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87667
[2025-01-13 08:31:40,340] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:31:40,340] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 590/2809]  eta: 0:14:07  lr: 0.000007  min_lr: 0.000000  loss: 3.5055 (3.6798)  loss_scale: 65536.0000 (67421.1303)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0003  max mem: 15572
Epoch: [31]  [ 600/2809]  eta: 0:14:03  lr: 0.000007  min_lr: 0.000000  loss: 3.7359 (3.6799)  loss_scale: 65536.0000 (67389.7637)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [31]  [ 610/2809]  eta: 0:13:59  lr: 0.000007  min_lr: 0.000000  loss: 3.7359 (3.6779)  loss_scale: 65536.0000 (67359.4239)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [31]  [ 620/2809]  eta: 0:13:55  lr: 0.000007  min_lr: 0.000000  loss: 3.6397 (3.6782)  loss_scale: 65536.0000 (67330.0612)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0003  max mem: 15572
Epoch: [31]  [ 630/2809]  eta: 0:13:50  lr: 0.000007  min_lr: 0.000000  loss: 3.7837 (3.6794)  loss_scale: 65536.0000 (67301.6292)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0004  max mem: 15572
Epoch: [31]  [ 640/2809]  eta: 0:13:46  lr: 0.000007  min_lr: 0.000000  loss: 3.7927 (3.6816)  loss_scale: 65536.0000 (67274.0842)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0003  max mem: 15572
Epoch: [31]  [ 650/2809]  eta: 0:13:42  lr: 0.000007  min_lr: 0.000000  loss: 3.7015 (3.6807)  loss_scale: 65536.0000 (67247.3856)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [31]  [ 660/2809]  eta: 0:13:38  lr: 0.000007  min_lr: 0.000000  loss: 3.7015 (3.6820)  loss_scale: 65536.0000 (67221.4947)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
[2025-01-13 08:32:07,505] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87740
[2025-01-13 08:32:07,505] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:32:07,505] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [ 670/2809]  eta: 0:13:34  lr: 0.000007  min_lr: 0.000000  loss: 3.7071 (3.6813)  loss_scale: 32768.0000 (66708.0298)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [31]  [ 680/2809]  eta: 0:13:30  lr: 0.000007  min_lr: 0.000000  loss: 3.7565 (3.6836)  loss_scale: 32768.0000 (66209.6446)  weight_decay: 0.0500 (0.0500)  time: 0.3786  data: 0.0003  max mem: 15572
Epoch: [31]  [ 690/2809]  eta: 0:13:26  lr: 0.000007  min_lr: 0.000000  loss: 4.0046 (3.6869)  loss_scale: 32768.0000 (65725.6845)  weight_decay: 0.0500 (0.0500)  time: 0.3789  data: 0.0004  max mem: 15572
Epoch: [31]  [ 700/2809]  eta: 0:13:23  lr: 0.000007  min_lr: 0.000000  loss: 3.9505 (3.6834)  loss_scale: 32768.0000 (65255.5321)  weight_decay: 0.0500 (0.0500)  time: 0.3838  data: 0.0004  max mem: 15572
Epoch: [31]  [ 710/2809]  eta: 0:13:19  lr: 0.000007  min_lr: 0.000000  loss: 3.7295 (3.6856)  loss_scale: 32768.0000 (64798.6048)  weight_decay: 0.0500 (0.0500)  time: 0.3879  data: 0.0004  max mem: 15572
Epoch: [31]  [ 720/2809]  eta: 0:13:15  lr: 0.000007  min_lr: 0.000000  loss: 3.8958 (3.6879)  loss_scale: 32768.0000 (64354.3523)  weight_decay: 0.0500 (0.0500)  time: 0.3840  data: 0.0006  max mem: 15572
Epoch: [31]  [ 730/2809]  eta: 0:13:12  lr: 0.000007  min_lr: 0.000000  loss: 3.7122 (3.6868)  loss_scale: 32768.0000 (63922.2544)  weight_decay: 0.0500 (0.0500)  time: 0.3857  data: 0.0008  max mem: 15572
Epoch: [31]  [ 740/2809]  eta: 0:13:08  lr: 0.000007  min_lr: 0.000000  loss: 3.5314 (3.6849)  loss_scale: 32768.0000 (63501.8192)  weight_decay: 0.0500 (0.0500)  time: 0.3900  data: 0.0009  max mem: 15572
Epoch: [31]  [ 750/2809]  eta: 0:13:05  lr: 0.000007  min_lr: 0.000000  loss: 3.6559 (3.6873)  loss_scale: 32768.0000 (63092.5806)  weight_decay: 0.0500 (0.0500)  time: 0.3927  data: 0.0010  max mem: 15572
Epoch: [31]  [ 760/2809]  eta: 0:13:02  lr: 0.000007  min_lr: 0.000000  loss: 3.7233 (3.6867)  loss_scale: 32768.0000 (62694.0972)  weight_decay: 0.0500 (0.0500)  time: 0.3969  data: 0.0008  max mem: 15572
Epoch: [31]  [ 770/2809]  eta: 0:12:58  lr: 0.000007  min_lr: 0.000000  loss: 3.7233 (3.6854)  loss_scale: 32768.0000 (62305.9507)  weight_decay: 0.0500 (0.0500)  time: 0.4012  data: 0.0008  max mem: 15572
Epoch: [31]  [ 780/2809]  eta: 0:12:55  lr: 0.000007  min_lr: 0.000000  loss: 3.6128 (3.6842)  loss_scale: 32768.0000 (61927.7439)  weight_decay: 0.0500 (0.0500)  time: 0.4002  data: 0.0008  max mem: 15572
[2025-01-13 08:32:57,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:32:57,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 790/2809]  eta: 0:12:51  lr: 0.000007  min_lr: 0.000000  loss: 3.7792 (3.6850)  loss_scale: 32768.0000 (61600.5259)  weight_decay: 0.0500 (0.0500)  time: 0.3983  data: 0.0006  max mem: 15572
Epoch: [31]  [ 800/2809]  eta: 0:12:48  lr: 0.000007  min_lr: 0.000000  loss: 3.8584 (3.6883)  loss_scale: 65536.0000 (61649.6579)  weight_decay: 0.0500 (0.0500)  time: 0.3988  data: 0.0006  max mem: 15572
Epoch: [31]  [ 810/2809]  eta: 0:12:45  lr: 0.000007  min_lr: 0.000000  loss: 3.7789 (3.6894)  loss_scale: 65536.0000 (61697.5783)  weight_decay: 0.0500 (0.0500)  time: 0.4020  data: 0.0006  max mem: 15572
Epoch: [31]  [ 820/2809]  eta: 0:12:42  lr: 0.000007  min_lr: 0.000000  loss: 3.6157 (3.6882)  loss_scale: 65536.0000 (61744.3313)  weight_decay: 0.0500 (0.0500)  time: 0.4049  data: 0.0009  max mem: 15572
Epoch: [31]  [ 830/2809]  eta: 0:12:38  lr: 0.000007  min_lr: 0.000000  loss: 3.8670 (3.6883)  loss_scale: 65536.0000 (61789.9591)  weight_decay: 0.0500 (0.0500)  time: 0.3954  data: 0.0008  max mem: 15572
Epoch: [31]  [ 840/2809]  eta: 0:12:35  lr: 0.000007  min_lr: 0.000000  loss: 3.3785 (3.6840)  loss_scale: 65536.0000 (61834.5018)  weight_decay: 0.0500 (0.0500)  time: 0.4022  data: 0.0005  max mem: 15572
Epoch: [31]  [ 850/2809]  eta: 0:12:31  lr: 0.000007  min_lr: 0.000000  loss: 3.5996 (3.6878)  loss_scale: 65536.0000 (61877.9976)  weight_decay: 0.0500 (0.0500)  time: 0.4028  data: 0.0005  max mem: 15572
Epoch: [31]  [ 860/2809]  eta: 0:12:27  lr: 0.000007  min_lr: 0.000000  loss: 3.9262 (3.6880)  loss_scale: 65536.0000 (61920.4832)  weight_decay: 0.0500 (0.0500)  time: 0.3834  data: 0.0003  max mem: 15572
Epoch: [31]  [ 870/2809]  eta: 0:12:23  lr: 0.000007  min_lr: 0.000000  loss: 3.7277 (3.6879)  loss_scale: 65536.0000 (61961.9931)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0003  max mem: 15572
Epoch: [31]  [ 880/2809]  eta: 0:12:19  lr: 0.000007  min_lr: 0.000000  loss: 3.8298 (3.6905)  loss_scale: 65536.0000 (62002.5607)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0003  max mem: 15572
Epoch: [31]  [ 890/2809]  eta: 0:12:15  lr: 0.000007  min_lr: 0.000000  loss: 3.8298 (3.6881)  loss_scale: 65536.0000 (62042.2177)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0003  max mem: 15572
Epoch: [31]  [ 900/2809]  eta: 0:12:11  lr: 0.000007  min_lr: 0.000000  loss: 3.8915 (3.6917)  loss_scale: 65536.0000 (62080.9945)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0002  max mem: 15572
Epoch: [31]  [ 910/2809]  eta: 0:12:07  lr: 0.000007  min_lr: 0.000000  loss: 3.9807 (3.6936)  loss_scale: 65536.0000 (62118.9199)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0003  max mem: 15572
[2025-01-13 08:33:47,310] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:33:47,310] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:33:48,047] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87999
[2025-01-13 08:33:48,047] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:33:48,047] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-13 08:33:48,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=88000, skipped=600, lr=[6.578664486896344e-08, 6.578664486896344e-08, 9.398092124137635e-08, 9.398092124137635e-08, 1.3425845891625195e-07, 1.3425845891625195e-07, 1.9179779845178852e-07, 1.9179779845178852e-07, 2.7399685493112645e-07, 2.7399685493112645e-07, 3.914240784730378e-07, 3.914240784730378e-07, 5.591772549614826e-07, 5.591772549614826e-07, 7.988246499449752e-07, 7.988246499449752e-07, 1.1411780713499645e-06, 1.1411780713499645e-06, 1.6302543876428069e-06, 1.6302543876428069e-06, 2.328934839489724e-06, 2.328934839489724e-06, 3.327049770699606e-06, 3.327049770699606e-06, 4.752928243856581e-06, 4.752928243856581e-06, 6.7898974912236865e-06, 6.7898974912236865e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 08:33:48,061] [INFO] [timer.py:260:stop] epoch=0/micro_step=88000/global_step=88000, RunningAvgSamplesPerSec=31.098300186678085, CurrSamplesPerSec=33.72413183137548, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [31]  [ 920/2809]  eta: 0:12:03  lr: 0.000007  min_lr: 0.000000  loss: 3.8849 (3.6935)  loss_scale: 65536.0000 (62298.3366)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
Epoch: [31]  [ 930/2809]  eta: 0:11:59  lr: 0.000007  min_lr: 0.000000  loss: 3.5893 (3.6907)  loss_scale: 65536.0000 (62333.1128)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [31]  [ 940/2809]  eta: 0:11:55  lr: 0.000007  min_lr: 0.000000  loss: 3.4141 (3.6885)  loss_scale: 65536.0000 (62367.1498)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0004  max mem: 15572
Epoch: [31]  [ 950/2809]  eta: 0:11:51  lr: 0.000007  min_lr: 0.000000  loss: 3.7316 (3.6903)  loss_scale: 65536.0000 (62400.4711)  weight_decay: 0.0500 (0.0500)  time: 0.3794  data: 0.0004  max mem: 15572
Epoch: [31]  [ 960/2809]  eta: 0:11:47  lr: 0.000007  min_lr: 0.000000  loss: 3.9090 (3.6890)  loss_scale: 65536.0000 (62433.0989)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0003  max mem: 15572
[2025-01-13 08:34:03,965] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88041
[2025-01-13 08:34:03,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:34:03,965] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [ 970/2809]  eta: 0:11:43  lr: 0.000007  min_lr: 0.000000  loss: 3.6804 (3.6879)  loss_scale: 65536.0000 (62161.3347)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0004  max mem: 15572
Epoch: [31]  [ 980/2809]  eta: 0:11:39  lr: 0.000007  min_lr: 0.000000  loss: 3.6558 (3.6860)  loss_scale: 32768.0000 (61861.7085)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0004  max mem: 15572
Epoch: [31]  [ 990/2809]  eta: 0:11:35  lr: 0.000007  min_lr: 0.000000  loss: 3.6558 (3.6869)  loss_scale: 32768.0000 (61568.1292)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0003  max mem: 15572
Epoch: [31]  [1000/2809]  eta: 0:11:32  lr: 0.000007  min_lr: 0.000000  loss: 3.7096 (3.6847)  loss_scale: 32768.0000 (61280.4156)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0003  max mem: 15572
Epoch: [31]  [1010/2809]  eta: 0:11:28  lr: 0.000007  min_lr: 0.000000  loss: 3.5308 (3.6839)  loss_scale: 32768.0000 (60998.3937)  weight_decay: 0.0500 (0.0500)  time: 0.3819  data: 0.0003  max mem: 15572
Epoch: [31]  [1020/2809]  eta: 0:11:24  lr: 0.000007  min_lr: 0.000000  loss: 3.7454 (3.6851)  loss_scale: 32768.0000 (60721.8962)  weight_decay: 0.0500 (0.0500)  time: 0.3883  data: 0.0003  max mem: 15572
Epoch: [31]  [1030/2809]  eta: 0:11:20  lr: 0.000007  min_lr: 0.000000  loss: 3.7698 (3.6848)  loss_scale: 32768.0000 (60450.7624)  weight_decay: 0.0500 (0.0500)  time: 0.3940  data: 0.0004  max mem: 15572
Epoch: [31]  [1040/2809]  eta: 0:11:17  lr: 0.000007  min_lr: 0.000000  loss: 3.6609 (3.6841)  loss_scale: 32768.0000 (60184.8377)  weight_decay: 0.0500 (0.0500)  time: 0.4029  data: 0.0005  max mem: 15572
Epoch: [31]  [1050/2809]  eta: 0:11:14  lr: 0.000007  min_lr: 0.000000  loss: 3.5342 (3.6825)  loss_scale: 32768.0000 (59923.9734)  weight_decay: 0.0500 (0.0500)  time: 0.4056  data: 0.0007  max mem: 15572
Epoch: [31]  [1060/2809]  eta: 0:11:10  lr: 0.000007  min_lr: 0.000000  loss: 3.4406 (3.6821)  loss_scale: 32768.0000 (59668.0264)  weight_decay: 0.0500 (0.0500)  time: 0.3969  data: 0.0007  max mem: 15572
Epoch: [31]  [1070/2809]  eta: 0:11:06  lr: 0.000007  min_lr: 0.000000  loss: 3.5264 (3.6828)  loss_scale: 32768.0000 (59416.8590)  weight_decay: 0.0500 (0.0500)  time: 0.3991  data: 0.0008  max mem: 15572
Epoch: [31]  [1080/2809]  eta: 0:11:03  lr: 0.000007  min_lr: 0.000000  loss: 3.5590 (3.6827)  loss_scale: 32768.0000 (59170.3386)  weight_decay: 0.0500 (0.0500)  time: 0.4112  data: 0.0008  max mem: 15572
Epoch: [31]  [1090/2809]  eta: 0:11:00  lr: 0.000007  min_lr: 0.000000  loss: 3.7848 (3.6828)  loss_scale: 32768.0000 (58928.3373)  weight_decay: 0.0500 (0.0500)  time: 0.4116  data: 0.0007  max mem: 15572
[2025-01-13 08:34:54,744] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:34:54,744] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1100/2809]  eta: 0:10:56  lr: 0.000007  min_lr: 0.000000  loss: 3.8427 (3.6821)  loss_scale: 32768.0000 (58988.3524)  weight_decay: 0.0500 (0.0500)  time: 0.3922  data: 0.0006  max mem: 15572
Epoch: [31]  [1110/2809]  eta: 0:10:52  lr: 0.000007  min_lr: 0.000000  loss: 3.8733 (3.6851)  loss_scale: 65536.0000 (59047.2871)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0003  max mem: 15572
[2025-01-13 08:35:03,420] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88193
[2025-01-13 08:35:03,420] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:35:03,420] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1120/2809]  eta: 0:10:48  lr: 0.000007  min_lr: 0.000000  loss: 4.0077 (3.6872)  loss_scale: 65536.0000 (58900.5531)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0003  max mem: 15572
Epoch: [31]  [1130/2809]  eta: 0:10:44  lr: 0.000007  min_lr: 0.000000  loss: 3.6044 (3.6845)  loss_scale: 32768.0000 (58669.4960)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
Epoch: [31]  [1140/2809]  eta: 0:10:40  lr: 0.000007  min_lr: 0.000000  loss: 3.5720 (3.6839)  loss_scale: 32768.0000 (58442.4890)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0003  max mem: 15572
Epoch: [31]  [1150/2809]  eta: 0:10:36  lr: 0.000007  min_lr: 0.000000  loss: 3.4116 (3.6807)  loss_scale: 32768.0000 (58219.4266)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [31]  [1160/2809]  eta: 0:10:32  lr: 0.000007  min_lr: 0.000000  loss: 3.4977 (3.6812)  loss_scale: 32768.0000 (58000.2067)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [31]  [1170/2809]  eta: 0:10:28  lr: 0.000007  min_lr: 0.000000  loss: 3.5659 (3.6805)  loss_scale: 32768.0000 (57784.7310)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0003  max mem: 15572
Epoch: [31]  [1180/2809]  eta: 0:10:24  lr: 0.000007  min_lr: 0.000000  loss: 3.4994 (3.6797)  loss_scale: 32768.0000 (57572.9043)  weight_decay: 0.0500 (0.0500)  time: 0.3802  data: 0.0003  max mem: 15572
Epoch: [31]  [1190/2809]  eta: 0:10:20  lr: 0.000007  min_lr: 0.000000  loss: 3.7624 (3.6816)  loss_scale: 32768.0000 (57364.6348)  weight_decay: 0.0500 (0.0500)  time: 0.3881  data: 0.0004  max mem: 15572
Epoch: [31]  [1200/2809]  eta: 0:10:17  lr: 0.000007  min_lr: 0.000000  loss: 3.7406 (3.6808)  loss_scale: 32768.0000 (57159.8335)  weight_decay: 0.0500 (0.0500)  time: 0.3898  data: 0.0004  max mem: 15572
Epoch: [31]  [1210/2809]  eta: 0:10:13  lr: 0.000007  min_lr: 0.000000  loss: 3.7406 (3.6820)  loss_scale: 32768.0000 (56958.4145)  weight_decay: 0.0500 (0.0500)  time: 0.3838  data: 0.0004  max mem: 15572
Epoch: [31]  [1220/2809]  eta: 0:10:09  lr: 0.000007  min_lr: 0.000000  loss: 3.6551 (3.6800)  loss_scale: 32768.0000 (56760.2948)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [31]  [1230/2809]  eta: 0:10:05  lr: 0.000007  min_lr: 0.000000  loss: 3.6551 (3.6797)  loss_scale: 32768.0000 (56565.3940)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0003  max mem: 15572
Epoch: [31]  [1240/2809]  eta: 0:10:01  lr: 0.000007  min_lr: 0.000000  loss: 3.7118 (3.6790)  loss_scale: 32768.0000 (56373.6342)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
[2025-01-13 08:35:52,241] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:35:52,241] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1250/2809]  eta: 0:09:57  lr: 0.000007  min_lr: 0.000000  loss: 3.5101 (3.6778)  loss_scale: 32768.0000 (56394.4876)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [31]  [1260/2809]  eta: 0:09:53  lr: 0.000007  min_lr: 0.000000  loss: 3.5043 (3.6754)  loss_scale: 65536.0000 (56466.9818)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0003  max mem: 15572
Epoch: [31]  [1270/2809]  eta: 0:09:49  lr: 0.000007  min_lr: 0.000000  loss: 3.5925 (3.6756)  loss_scale: 65536.0000 (56538.3352)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0003  max mem: 15572
Epoch: [31]  [1280/2809]  eta: 0:09:45  lr: 0.000007  min_lr: 0.000000  loss: 3.8421 (3.6770)  loss_scale: 65536.0000 (56608.5746)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [31]  [1290/2809]  eta: 0:09:41  lr: 0.000007  min_lr: 0.000000  loss: 3.8424 (3.6776)  loss_scale: 65536.0000 (56677.7258)  weight_decay: 0.0500 (0.0500)  time: 0.3773  data: 0.0003  max mem: 15572
Epoch: [31]  [1300/2809]  eta: 0:09:38  lr: 0.000007  min_lr: 0.000000  loss: 3.7767 (3.6785)  loss_scale: 65536.0000 (56745.8140)  weight_decay: 0.0500 (0.0500)  time: 0.3846  data: 0.0003  max mem: 15572
Epoch: [31]  [1310/2809]  eta: 0:09:34  lr: 0.000007  min_lr: 0.000000  loss: 3.7935 (3.6800)  loss_scale: 65536.0000 (56812.8635)  weight_decay: 0.0500 (0.0500)  time: 0.3907  data: 0.0004  max mem: 15572
Epoch: [31]  [1320/2809]  eta: 0:09:30  lr: 0.000007  min_lr: 0.000000  loss: 3.7935 (3.6814)  loss_scale: 65536.0000 (56878.8978)  weight_decay: 0.0500 (0.0500)  time: 0.3944  data: 0.0005  max mem: 15572
Epoch: [31]  [1330/2809]  eta: 0:09:26  lr: 0.000007  min_lr: 0.000000  loss: 3.7458 (3.6818)  loss_scale: 65536.0000 (56943.9399)  weight_decay: 0.0500 (0.0500)  time: 0.3862  data: 0.0004  max mem: 15572
Epoch: [31]  [1340/2809]  eta: 0:09:22  lr: 0.000007  min_lr: 0.000000  loss: 3.7246 (3.6809)  loss_scale: 65536.0000 (57008.0119)  weight_decay: 0.0500 (0.0500)  time: 0.3788  data: 0.0004  max mem: 15572
Epoch: [31]  [1350/2809]  eta: 0:09:18  lr: 0.000007  min_lr: 0.000000  loss: 3.8205 (3.6821)  loss_scale: 65536.0000 (57071.1355)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0003  max mem: 15572
Epoch: [31]  [1360/2809]  eta: 0:09:15  lr: 0.000007  min_lr: 0.000000  loss: 3.8297 (3.6826)  loss_scale: 65536.0000 (57133.3314)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [31]  [1370/2809]  eta: 0:09:11  lr: 0.000007  min_lr: 0.000000  loss: 3.6992 (3.6826)  loss_scale: 65536.0000 (57194.6200)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
[2025-01-13 08:36:40,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:36:40,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:36:41,166] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88451
[2025-01-13 08:36:41,166] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:36:41,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [1380/2809]  eta: 0:09:07  lr: 0.000007  min_lr: 0.000000  loss: 3.5018 (3.6819)  loss_scale: 65536.0000 (57302.4765)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0003  max mem: 15572
Epoch: [31]  [1390/2809]  eta: 0:09:03  lr: 0.000007  min_lr: 0.000000  loss: 3.4293 (3.6808)  loss_scale: 65536.0000 (57361.6679)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
Epoch: [31]  [1400/2809]  eta: 0:08:59  lr: 0.000007  min_lr: 0.000000  loss: 3.7275 (3.6817)  loss_scale: 65536.0000 (57420.0143)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0003  max mem: 15572
Epoch: [31]  [1410/2809]  eta: 0:08:55  lr: 0.000007  min_lr: 0.000000  loss: 3.7384 (3.6810)  loss_scale: 65536.0000 (57477.5337)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0003  max mem: 15572
Epoch: [31]  [1420/2809]  eta: 0:08:51  lr: 0.000007  min_lr: 0.000000  loss: 3.7316 (3.6808)  loss_scale: 65536.0000 (57534.2435)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0004  max mem: 15572
Epoch: [31]  [1430/2809]  eta: 0:08:47  lr: 0.000007  min_lr: 0.000000  loss: 3.8502 (3.6825)  loss_scale: 65536.0000 (57590.1607)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0004  max mem: 15572
Epoch: [31]  [1440/2809]  eta: 0:08:43  lr: 0.000007  min_lr: 0.000000  loss: 3.7749 (3.6817)  loss_scale: 65536.0000 (57645.3019)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
Epoch: [31]  [1450/2809]  eta: 0:08:39  lr: 0.000007  min_lr: 0.000000  loss: 3.7749 (3.6836)  loss_scale: 65536.0000 (57699.6830)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0004  max mem: 15572
Epoch: [31]  [1460/2809]  eta: 0:08:35  lr: 0.000007  min_lr: 0.000000  loss: 4.0338 (3.6842)  loss_scale: 65536.0000 (57753.3196)  weight_decay: 0.0500 (0.0500)  time: 0.3803  data: 0.0004  max mem: 15572
Epoch: [31]  [1470/2809]  eta: 0:08:32  lr: 0.000007  min_lr: 0.000000  loss: 3.7228 (3.6837)  loss_scale: 65536.0000 (57806.2271)  weight_decay: 0.0500 (0.0500)  time: 0.3829  data: 0.0004  max mem: 15572
Epoch: [31]  [1480/2809]  eta: 0:08:28  lr: 0.000006  min_lr: 0.000000  loss: 3.7228 (3.6853)  loss_scale: 65536.0000 (57858.4200)  weight_decay: 0.0500 (0.0500)  time: 0.3830  data: 0.0004  max mem: 15572
Epoch: [31]  [1490/2809]  eta: 0:08:24  lr: 0.000006  min_lr: 0.000000  loss: 3.8657 (3.6846)  loss_scale: 65536.0000 (57909.9128)  weight_decay: 0.0500 (0.0500)  time: 0.3970  data: 0.0004  max mem: 15572
Epoch: [31]  [1500/2809]  eta: 0:08:21  lr: 0.000006  min_lr: 0.000000  loss: 3.6356 (3.6841)  loss_scale: 65536.0000 (57960.7195)  weight_decay: 0.0500 (0.0500)  time: 0.4033  data: 0.0005  max mem: 15572
[2025-01-13 08:37:30,344] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:37:30,345] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:37:33,542] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88588
[2025-01-13 08:37:33,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:37:33,542] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [1510/2809]  eta: 0:08:17  lr: 0.000006  min_lr: 0.000000  loss: 3.6874 (3.6849)  loss_scale: 65536.0000 (58357.8345)  weight_decay: 0.0500 (0.0500)  time: 0.3967  data: 0.0006  max mem: 15572
Epoch: [31]  [1520/2809]  eta: 0:08:13  lr: 0.000006  min_lr: 0.000000  loss: 3.8734 (3.6859)  loss_scale: 65536.0000 (58405.0283)  weight_decay: 0.0500 (0.0500)  time: 0.3877  data: 0.0004  max mem: 15572
Epoch: [31]  [1530/2809]  eta: 0:08:09  lr: 0.000006  min_lr: 0.000000  loss: 3.8139 (3.6858)  loss_scale: 65536.0000 (58451.6055)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [31]  [1540/2809]  eta: 0:08:05  lr: 0.000006  min_lr: 0.000000  loss: 3.9227 (3.6879)  loss_scale: 65536.0000 (58497.5782)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0003  max mem: 15572
Epoch: [31]  [1550/2809]  eta: 0:08:01  lr: 0.000006  min_lr: 0.000000  loss: 3.9203 (3.6879)  loss_scale: 65536.0000 (58542.9581)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0003  max mem: 15572
Epoch: [31]  [1560/2809]  eta: 0:07:57  lr: 0.000006  min_lr: 0.000000  loss: 3.7867 (3.6880)  loss_scale: 65536.0000 (58587.7566)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [31]  [1570/2809]  eta: 0:07:53  lr: 0.000006  min_lr: 0.000000  loss: 3.6096 (3.6878)  loss_scale: 65536.0000 (58631.9847)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0003  max mem: 15572
Epoch: [31]  [1580/2809]  eta: 0:07:50  lr: 0.000006  min_lr: 0.000000  loss: 3.6751 (3.6886)  loss_scale: 65536.0000 (58675.6534)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
Epoch: [31]  [1590/2809]  eta: 0:07:46  lr: 0.000006  min_lr: 0.000000  loss: 3.7903 (3.6887)  loss_scale: 65536.0000 (58718.7731)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0003  max mem: 15572
Epoch: [31]  [1600/2809]  eta: 0:07:42  lr: 0.000006  min_lr: 0.000000  loss: 3.7903 (3.6892)  loss_scale: 65536.0000 (58761.3542)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [31]  [1610/2809]  eta: 0:07:38  lr: 0.000006  min_lr: 0.000000  loss: 3.9319 (3.6890)  loss_scale: 65536.0000 (58803.4066)  weight_decay: 0.0500 (0.0500)  time: 0.3789  data: 0.0003  max mem: 15572
Epoch: [31]  [1620/2809]  eta: 0:07:34  lr: 0.000006  min_lr: 0.000000  loss: 3.7393 (3.6893)  loss_scale: 65536.0000 (58844.9402)  weight_decay: 0.0500 (0.0500)  time: 0.3885  data: 0.0004  max mem: 15572
Epoch: [31]  [1630/2809]  eta: 0:07:30  lr: 0.000006  min_lr: 0.000000  loss: 3.7393 (3.6894)  loss_scale: 65536.0000 (58885.9644)  weight_decay: 0.0500 (0.0500)  time: 0.3853  data: 0.0004  max mem: 15572
[2025-01-13 08:38:22,058] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:38:22,058] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:38:22,427] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88718
[2025-01-13 08:38:22,427] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:38:22,427] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [1640/2809]  eta: 0:07:26  lr: 0.000006  min_lr: 0.000000  loss: 3.9536 (3.6894)  loss_scale: 65536.0000 (58966.4254)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0004  max mem: 15572
Epoch: [31]  [1650/2809]  eta: 0:07:22  lr: 0.000006  min_lr: 0.000000  loss: 3.9766 (3.6910)  loss_scale: 65536.0000 (59006.2168)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0003  max mem: 15572
Epoch: [31]  [1660/2809]  eta: 0:07:19  lr: 0.000006  min_lr: 0.000000  loss: 3.8021 (3.6891)  loss_scale: 65536.0000 (59045.5292)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [31]  [1670/2809]  eta: 0:07:15  lr: 0.000006  min_lr: 0.000000  loss: 3.6331 (3.6889)  loss_scale: 65536.0000 (59084.3710)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [31]  [1680/2809]  eta: 0:07:11  lr: 0.000006  min_lr: 0.000000  loss: 3.6721 (3.6887)  loss_scale: 65536.0000 (59122.7507)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0004  max mem: 15572
Epoch: [31]  [1690/2809]  eta: 0:07:07  lr: 0.000006  min_lr: 0.000000  loss: 3.6102 (3.6885)  loss_scale: 65536.0000 (59160.6765)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0004  max mem: 15572
Epoch: [31]  [1700/2809]  eta: 0:07:03  lr: 0.000006  min_lr: 0.000000  loss: 3.5997 (3.6887)  loss_scale: 65536.0000 (59198.1564)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
Epoch: [31]  [1710/2809]  eta: 0:06:59  lr: 0.000006  min_lr: 0.000000  loss: 3.5549 (3.6876)  loss_scale: 65536.0000 (59235.1981)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0003  max mem: 15572
Epoch: [31]  [1720/2809]  eta: 0:06:55  lr: 0.000006  min_lr: 0.000000  loss: 3.8355 (3.6880)  loss_scale: 65536.0000 (59271.8094)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0003  max mem: 15572
Epoch: [31]  [1730/2809]  eta: 0:06:51  lr: 0.000006  min_lr: 0.000000  loss: 3.5697 (3.6876)  loss_scale: 65536.0000 (59307.9977)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [31]  [1740/2809]  eta: 0:06:48  lr: 0.000006  min_lr: 0.000000  loss: 3.3458 (3.6866)  loss_scale: 65536.0000 (59343.7702)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0003  max mem: 15572
Epoch: [31]  [1750/2809]  eta: 0:06:44  lr: 0.000006  min_lr: 0.000000  loss: 3.6473 (3.6865)  loss_scale: 65536.0000 (59379.1342)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [31]  [1760/2809]  eta: 0:06:40  lr: 0.000006  min_lr: 0.000000  loss: 3.7937 (3.6879)  loss_scale: 65536.0000 (59414.0965)  weight_decay: 0.0500 (0.0500)  time: 0.3828  data: 0.0003  max mem: 15572
[2025-01-13 08:39:10,908] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:39:10,908] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:39:11,279] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88848
[2025-01-13 08:39:11,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:39:11,279] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [1770/2809]  eta: 0:06:36  lr: 0.000006  min_lr: 0.000000  loss: 3.6246 (3.6861)  loss_scale: 65536.0000 (59485.6691)  weight_decay: 0.0500 (0.0500)  time: 0.3855  data: 0.0005  max mem: 15572
Epoch: [31]  [1780/2809]  eta: 0:06:32  lr: 0.000006  min_lr: 0.000000  loss: 3.6644 (3.6871)  loss_scale: 65536.0000 (59519.6407)  weight_decay: 0.0500 (0.0500)  time: 0.3939  data: 0.0005  max mem: 15572
Epoch: [31]  [1790/2809]  eta: 0:06:29  lr: 0.000006  min_lr: 0.000000  loss: 3.8904 (3.6880)  loss_scale: 65536.0000 (59553.2328)  weight_decay: 0.0500 (0.0500)  time: 0.4047  data: 0.0005  max mem: 15572
Epoch: [31]  [1800/2809]  eta: 0:06:25  lr: 0.000006  min_lr: 0.000000  loss: 3.8713 (3.6892)  loss_scale: 65536.0000 (59586.4520)  weight_decay: 0.0500 (0.0500)  time: 0.3920  data: 0.0004  max mem: 15572
Epoch: [31]  [1810/2809]  eta: 0:06:21  lr: 0.000006  min_lr: 0.000000  loss: 3.8186 (3.6895)  loss_scale: 65536.0000 (59619.3043)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
Epoch: [31]  [1820/2809]  eta: 0:06:17  lr: 0.000006  min_lr: 0.000000  loss: 3.5598 (3.6887)  loss_scale: 65536.0000 (59651.7957)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [31]  [1830/2809]  eta: 0:06:13  lr: 0.000006  min_lr: 0.000000  loss: 3.4696 (3.6879)  loss_scale: 65536.0000 (59683.9323)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0004  max mem: 15572
Epoch: [31]  [1840/2809]  eta: 0:06:10  lr: 0.000006  min_lr: 0.000000  loss: 3.5127 (3.6874)  loss_scale: 65536.0000 (59715.7197)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0004  max mem: 15572
Epoch: [31]  [1850/2809]  eta: 0:06:06  lr: 0.000006  min_lr: 0.000000  loss: 3.7773 (3.6882)  loss_scale: 65536.0000 (59747.1637)  weight_decay: 0.0500 (0.0500)  time: 0.3814  data: 0.0003  max mem: 15572
Epoch: [31]  [1860/2809]  eta: 0:06:02  lr: 0.000006  min_lr: 0.000000  loss: 3.8727 (3.6882)  loss_scale: 65536.0000 (59778.2697)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
Epoch: [31]  [1870/2809]  eta: 0:05:58  lr: 0.000006  min_lr: 0.000000  loss: 3.7115 (3.6872)  loss_scale: 65536.0000 (59809.0433)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [31]  [1880/2809]  eta: 0:05:54  lr: 0.000006  min_lr: 0.000000  loss: 3.6446 (3.6869)  loss_scale: 65536.0000 (59839.4896)  weight_decay: 0.0500 (0.0500)  time: 0.3787  data: 0.0004  max mem: 15572
Epoch: [31]  [1890/2809]  eta: 0:05:50  lr: 0.000006  min_lr: 0.000000  loss: 3.7856 (3.6872)  loss_scale: 65536.0000 (59869.6140)  weight_decay: 0.0500 (0.0500)  time: 0.3884  data: 0.0006  max mem: 15572
[2025-01-13 08:40:00,627] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:40:00,627] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [31]  [1900/2809]  eta: 0:05:47  lr: 0.000006  min_lr: 0.000000  loss: 3.7856 (3.6872)  loss_scale: 65536.0000 (60002.8448)  weight_decay: 0.0500 (0.0500)  time: 0.3865  data: 0.0006  max mem: 15572
[2025-01-13 08:40:01,758] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88980
[2025-01-13 08:40:01,759] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:40:01,759] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [1910/2809]  eta: 0:05:43  lr: 0.000006  min_lr: 0.000000  loss: 3.8889 (3.6870)  loss_scale: 65536.0000 (60031.7991)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0005  max mem: 15572
[2025-01-13 08:40:08,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=89000, skipped=607, lr=[6.077990261635452e-08, 6.077990261635452e-08, 8.68284323090779e-08, 8.68284323090779e-08, 1.24040617584397e-07, 1.24040617584397e-07, 1.772008822634243e-07, 1.772008822634243e-07, 2.531441175191776e-07, 2.531441175191776e-07, 3.6163445359882513e-07, 3.6163445359882513e-07, 5.166206479983216e-07, 5.166206479983216e-07, 7.380294971404596e-07, 7.380294971404596e-07, 1.0543278530577993e-06, 1.0543278530577993e-06, 1.506182647225428e-06, 1.506182647225428e-06, 2.1516894960363255e-06, 2.1516894960363255e-06, 3.073842137194751e-06, 3.073842137194751e-06, 4.391203053135359e-06, 4.391203053135359e-06, 6.273147218764799e-06, 6.273147218764799e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 08:40:08,872] [INFO] [timer.py:260:stop] epoch=0/micro_step=89000/global_step=89000, RunningAvgSamplesPerSec=31.121929940207202, CurrSamplesPerSec=33.923359846113094, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [31]  [1920/2809]  eta: 0:05:39  lr: 0.000006  min_lr: 0.000000  loss: 3.8889 (3.6880)  loss_scale: 65536.0000 (60060.4518)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0004  max mem: 15572
Epoch: [31]  [1930/2809]  eta: 0:05:35  lr: 0.000006  min_lr: 0.000000  loss: 3.9203 (3.6884)  loss_scale: 65536.0000 (60088.8079)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0003  max mem: 15572
Epoch: [31]  [1940/2809]  eta: 0:05:31  lr: 0.000006  min_lr: 0.000000  loss: 3.6926 (3.6874)  loss_scale: 65536.0000 (60116.8717)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [31]  [1950/2809]  eta: 0:05:27  lr: 0.000006  min_lr: 0.000000  loss: 3.6424 (3.6874)  loss_scale: 65536.0000 (60144.6479)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0003  max mem: 15572
Epoch: [31]  [1960/2809]  eta: 0:05:23  lr: 0.000006  min_lr: 0.000000  loss: 3.8337 (3.6878)  loss_scale: 65536.0000 (60172.1407)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0003  max mem: 15572
Epoch: [31]  [1970/2809]  eta: 0:05:20  lr: 0.000006  min_lr: 0.000000  loss: 3.8731 (3.6872)  loss_scale: 65536.0000 (60199.3546)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [31]  [1980/2809]  eta: 0:05:16  lr: 0.000006  min_lr: 0.000000  loss: 3.8731 (3.6879)  loss_scale: 65536.0000 (60226.2938)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0003  max mem: 15572
[2025-01-13 08:40:33,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89064
[2025-01-13 08:40:33,171] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:40:33,171] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1990/2809]  eta: 0:05:12  lr: 0.000006  min_lr: 0.000000  loss: 3.8413 (3.6873)  loss_scale: 65536.0000 (60154.2140)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0003  max mem: 15572
Epoch: [31]  [2000/2809]  eta: 0:05:08  lr: 0.000006  min_lr: 0.000000  loss: 3.6451 (3.6870)  loss_scale: 32768.0000 (60017.3513)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0004  max mem: 15572
Epoch: [31]  [2010/2809]  eta: 0:05:04  lr: 0.000006  min_lr: 0.000000  loss: 3.5952 (3.6858)  loss_scale: 32768.0000 (59881.8498)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0003  max mem: 15572
Epoch: [31]  [2020/2809]  eta: 0:05:00  lr: 0.000006  min_lr: 0.000000  loss: 3.3787 (3.6849)  loss_scale: 32768.0000 (59747.6893)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
Epoch: [31]  [2030/2809]  eta: 0:04:57  lr: 0.000006  min_lr: 0.000000  loss: 3.3505 (3.6845)  loss_scale: 32768.0000 (59614.8498)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0013  max mem: 15572
Epoch: [31]  [2040/2809]  eta: 0:04:53  lr: 0.000006  min_lr: 0.000000  loss: 3.5903 (3.6841)  loss_scale: 32768.0000 (59483.3121)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0013  max mem: 15572
Epoch: [31]  [2050/2809]  eta: 0:04:49  lr: 0.000006  min_lr: 0.000000  loss: 3.7607 (3.6851)  loss_scale: 32768.0000 (59353.0570)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0003  max mem: 15572
Epoch: [31]  [2060/2809]  eta: 0:04:45  lr: 0.000006  min_lr: 0.000000  loss: 3.8116 (3.6851)  loss_scale: 32768.0000 (59224.0660)  weight_decay: 0.0500 (0.0500)  time: 0.3815  data: 0.0004  max mem: 15572
Epoch: [31]  [2070/2809]  eta: 0:04:41  lr: 0.000006  min_lr: 0.000000  loss: 3.7200 (3.6860)  loss_scale: 32768.0000 (59096.3206)  weight_decay: 0.0500 (0.0500)  time: 0.3842  data: 0.0005  max mem: 15572
Epoch: [31]  [2080/2809]  eta: 0:04:37  lr: 0.000006  min_lr: 0.000000  loss: 3.6756 (3.6856)  loss_scale: 32768.0000 (58969.8030)  weight_decay: 0.0500 (0.0500)  time: 0.3846  data: 0.0004  max mem: 15572
Epoch: [31]  [2090/2809]  eta: 0:04:34  lr: 0.000006  min_lr: 0.000000  loss: 3.3461 (3.6839)  loss_scale: 32768.0000 (58844.4955)  weight_decay: 0.0500 (0.0500)  time: 0.3830  data: 0.0004  max mem: 15572
Epoch: [31]  [2100/2809]  eta: 0:04:30  lr: 0.000006  min_lr: 0.000000  loss: 3.3461 (3.6834)  loss_scale: 32768.0000 (58720.3808)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [31]  [2110/2809]  eta: 0:04:26  lr: 0.000006  min_lr: 0.000000  loss: 3.5034 (3.6831)  loss_scale: 32768.0000 (58597.4420)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
[2025-01-13 08:41:21,797] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:41:21,797] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [2120/2809]  eta: 0:04:22  lr: 0.000006  min_lr: 0.000000  loss: 3.5401 (3.6828)  loss_scale: 32768.0000 (58583.8076)  weight_decay: 0.0500 (0.0500)  time: 0.3822  data: 0.0003  max mem: 15572
Epoch: [31]  [2130/2809]  eta: 0:04:18  lr: 0.000006  min_lr: 0.000000  loss: 3.6112 (3.6829)  loss_scale: 65536.0000 (58616.4317)  weight_decay: 0.0500 (0.0500)  time: 0.3804  data: 0.0003  max mem: 15572
Epoch: [31]  [2140/2809]  eta: 0:04:15  lr: 0.000006  min_lr: 0.000000  loss: 3.6112 (3.6825)  loss_scale: 65536.0000 (58648.7511)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
Epoch: [31]  [2150/2809]  eta: 0:04:11  lr: 0.000006  min_lr: 0.000000  loss: 3.6852 (3.6825)  loss_scale: 65536.0000 (58680.7699)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [31]  [2160/2809]  eta: 0:04:07  lr: 0.000006  min_lr: 0.000000  loss: 3.6852 (3.6816)  loss_scale: 65536.0000 (58712.4924)  weight_decay: 0.0500 (0.0500)  time: 0.3848  data: 0.0004  max mem: 15572
[2025-01-13 08:41:42,943] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89248
[2025-01-13 08:41:42,944] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:41:42,944] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [2170/2809]  eta: 0:04:03  lr: 0.000006  min_lr: 0.000000  loss: 3.4171 (3.6804)  loss_scale: 65536.0000 (58713.7356)  weight_decay: 0.0500 (0.0500)  time: 0.3953  data: 0.0005  max mem: 15572
Epoch: [31]  [2180/2809]  eta: 0:03:59  lr: 0.000006  min_lr: 0.000000  loss: 3.6260 (3.6811)  loss_scale: 32768.0000 (58594.7730)  weight_decay: 0.0500 (0.0500)  time: 0.3936  data: 0.0005  max mem: 15572
Epoch: [31]  [2190/2809]  eta: 0:03:56  lr: 0.000006  min_lr: 0.000000  loss: 3.8924 (3.6816)  loss_scale: 32768.0000 (58476.8964)  weight_decay: 0.0500 (0.0500)  time: 0.3868  data: 0.0004  max mem: 15572
Epoch: [31]  [2200/2809]  eta: 0:03:52  lr: 0.000006  min_lr: 0.000000  loss: 3.9358 (3.6825)  loss_scale: 32768.0000 (58360.0909)  weight_decay: 0.0500 (0.0500)  time: 0.3870  data: 0.0004  max mem: 15572
Epoch: [31]  [2210/2809]  eta: 0:03:48  lr: 0.000006  min_lr: 0.000000  loss: 3.9925 (3.6828)  loss_scale: 32768.0000 (58244.3419)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0004  max mem: 15572
Epoch: [31]  [2220/2809]  eta: 0:03:44  lr: 0.000006  min_lr: 0.000000  loss: 4.0007 (3.6841)  loss_scale: 32768.0000 (58129.6353)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
Epoch: [31]  [2230/2809]  eta: 0:03:40  lr: 0.000006  min_lr: 0.000000  loss: 3.7204 (3.6840)  loss_scale: 32768.0000 (58015.9570)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0003  max mem: 15572
Epoch: [31]  [2240/2809]  eta: 0:03:36  lr: 0.000006  min_lr: 0.000000  loss: 3.5879 (3.6838)  loss_scale: 32768.0000 (57903.2932)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
Epoch: [31]  [2250/2809]  eta: 0:03:33  lr: 0.000006  min_lr: 0.000000  loss: 3.6942 (3.6839)  loss_scale: 32768.0000 (57791.6304)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
Epoch: [31]  [2260/2809]  eta: 0:03:29  lr: 0.000006  min_lr: 0.000000  loss: 3.6888 (3.6839)  loss_scale: 32768.0000 (57680.9553)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0003  max mem: 15572
Epoch: [31]  [2270/2809]  eta: 0:03:25  lr: 0.000006  min_lr: 0.000000  loss: 3.6624 (3.6833)  loss_scale: 32768.0000 (57571.2550)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0004  max mem: 15572
Epoch: [31]  [2280/2809]  eta: 0:03:21  lr: 0.000006  min_lr: 0.000000  loss: 3.7604 (3.6838)  loss_scale: 32768.0000 (57462.5164)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0004  max mem: 15572
Epoch: [31]  [2290/2809]  eta: 0:03:17  lr: 0.000006  min_lr: 0.000000  loss: 3.7604 (3.6837)  loss_scale: 32768.0000 (57354.7272)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0004  max mem: 15572
[2025-01-13 08:42:31,594] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:42:31,594] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [2300/2809]  eta: 0:03:13  lr: 0.000006  min_lr: 0.000000  loss: 3.6466 (3.6830)  loss_scale: 32768.0000 (57290.5971)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [31]  [2310/2809]  eta: 0:03:10  lr: 0.000006  min_lr: 0.000000  loss: 3.7901 (3.6833)  loss_scale: 65536.0000 (57326.2761)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [31]  [2320/2809]  eta: 0:03:06  lr: 0.000006  min_lr: 0.000000  loss: 3.6240 (3.6827)  loss_scale: 65536.0000 (57361.6476)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0003  max mem: 15572
Epoch: [31]  [2330/2809]  eta: 0:03:02  lr: 0.000006  min_lr: 0.000000  loss: 3.6240 (3.6829)  loss_scale: 65536.0000 (57396.7156)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [31]  [2340/2809]  eta: 0:02:58  lr: 0.000006  min_lr: 0.000000  loss: 3.9720 (3.6834)  loss_scale: 65536.0000 (57431.4840)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0003  max mem: 15572
Epoch: [31]  [2350/2809]  eta: 0:02:54  lr: 0.000006  min_lr: 0.000000  loss: 3.9979 (3.6848)  loss_scale: 65536.0000 (57465.9566)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0003  max mem: 15572
Epoch: [31]  [2360/2809]  eta: 0:02:51  lr: 0.000006  min_lr: 0.000000  loss: 3.8705 (3.6849)  loss_scale: 65536.0000 (57500.1372)  weight_decay: 0.0500 (0.0500)  time: 0.3793  data: 0.0003  max mem: 15572
Epoch: [31]  [2370/2809]  eta: 0:02:47  lr: 0.000006  min_lr: 0.000000  loss: 3.7628 (3.6856)  loss_scale: 65536.0000 (57534.0295)  weight_decay: 0.0500 (0.0500)  time: 0.3793  data: 0.0003  max mem: 15572
Epoch: [31]  [2380/2809]  eta: 0:02:43  lr: 0.000006  min_lr: 0.000000  loss: 3.8925 (3.6861)  loss_scale: 65536.0000 (57567.6371)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0003  max mem: 15572
Epoch: [31]  [2390/2809]  eta: 0:02:39  lr: 0.000006  min_lr: 0.000000  loss: 3.7392 (3.6855)  loss_scale: 65536.0000 (57600.9636)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0004  max mem: 15572
[2025-01-13 08:43:07,711] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89473
[2025-01-13 08:43:07,711] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:43:07,711] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [2400/2809]  eta: 0:02:35  lr: 0.000006  min_lr: 0.000000  loss: 3.7588 (3.6865)  loss_scale: 65536.0000 (57538.4790)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0003  max mem: 15572
Epoch: [31]  [2410/2809]  eta: 0:02:31  lr: 0.000006  min_lr: 0.000000  loss: 3.8693 (3.6862)  loss_scale: 32768.0000 (57435.7395)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
Epoch: [31]  [2420/2809]  eta: 0:02:28  lr: 0.000006  min_lr: 0.000000  loss: 3.8046 (3.6865)  loss_scale: 32768.0000 (57333.8488)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0003  max mem: 15572
Epoch: [31]  [2430/2809]  eta: 0:02:24  lr: 0.000006  min_lr: 0.000000  loss: 3.8788 (3.6871)  loss_scale: 32768.0000 (57232.7964)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [31]  [2440/2809]  eta: 0:02:20  lr: 0.000006  min_lr: 0.000000  loss: 3.6122 (3.6868)  loss_scale: 32768.0000 (57132.5719)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0003  max mem: 15572
Epoch: [31]  [2450/2809]  eta: 0:02:16  lr: 0.000006  min_lr: 0.000000  loss: 3.5807 (3.6870)  loss_scale: 32768.0000 (57033.1652)  weight_decay: 0.0500 (0.0500)  time: 0.3819  data: 0.0003  max mem: 15572
Epoch: [31]  [2460/2809]  eta: 0:02:12  lr: 0.000006  min_lr: 0.000000  loss: 3.5807 (3.6863)  loss_scale: 32768.0000 (56934.5664)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0003  max mem: 15572
Epoch: [31]  [2470/2809]  eta: 0:02:09  lr: 0.000006  min_lr: 0.000000  loss: 3.6706 (3.6866)  loss_scale: 32768.0000 (56836.7657)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [31]  [2480/2809]  eta: 0:02:05  lr: 0.000006  min_lr: 0.000000  loss: 3.9263 (3.6878)  loss_scale: 32768.0000 (56739.7533)  weight_decay: 0.0500 (0.0500)  time: 0.3794  data: 0.0004  max mem: 15572
Epoch: [31]  [2490/2809]  eta: 0:02:01  lr: 0.000006  min_lr: 0.000000  loss: 3.9081 (3.6869)  loss_scale: 32768.0000 (56643.5199)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0004  max mem: 15572
Epoch: [31]  [2500/2809]  eta: 0:01:57  lr: 0.000006  min_lr: 0.000000  loss: 3.5246 (3.6873)  loss_scale: 32768.0000 (56548.0560)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
Epoch: [31]  [2510/2809]  eta: 0:01:53  lr: 0.000006  min_lr: 0.000000  loss: 3.8977 (3.6880)  loss_scale: 32768.0000 (56453.3524)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0003  max mem: 15572
Epoch: [31]  [2520/2809]  eta: 0:01:50  lr: 0.000006  min_lr: 0.000000  loss: 3.9462 (3.6885)  loss_scale: 32768.0000 (56359.4002)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0003  max mem: 15572
[2025-01-13 08:43:56,581] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:43:56,581] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [2530/2809]  eta: 0:01:46  lr: 0.000006  min_lr: 0.000000  loss: 3.7687 (3.6890)  loss_scale: 32768.0000 (56369.7637)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0003  max mem: 15572
Epoch: [31]  [2540/2809]  eta: 0:01:42  lr: 0.000006  min_lr: 0.000000  loss: 3.7755 (3.6896)  loss_scale: 65536.0000 (56405.8371)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0003  max mem: 15572
Epoch: [31]  [2550/2809]  eta: 0:01:38  lr: 0.000006  min_lr: 0.000000  loss: 3.7755 (3.6892)  loss_scale: 65536.0000 (56441.6276)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
Epoch: [31]  [2560/2809]  eta: 0:01:34  lr: 0.000006  min_lr: 0.000000  loss: 3.5899 (3.6893)  loss_scale: 65536.0000 (56477.1386)  weight_decay: 0.0500 (0.0500)  time: 0.3787  data: 0.0003  max mem: 15572
Epoch: [31]  [2570/2809]  eta: 0:01:30  lr: 0.000006  min_lr: 0.000000  loss: 3.8344 (3.6903)  loss_scale: 65536.0000 (56512.3734)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0003  max mem: 15572
Epoch: [31]  [2580/2809]  eta: 0:01:27  lr: 0.000006  min_lr: 0.000000  loss: 3.8986 (3.6906)  loss_scale: 65536.0000 (56547.3351)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0003  max mem: 15572
Epoch: [31]  [2590/2809]  eta: 0:01:23  lr: 0.000006  min_lr: 0.000000  loss: 3.7109 (3.6907)  loss_scale: 65536.0000 (56582.0270)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0003  max mem: 15572
Epoch: [31]  [2600/2809]  eta: 0:01:19  lr: 0.000006  min_lr: 0.000000  loss: 3.7168 (3.6906)  loss_scale: 65536.0000 (56616.4521)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
Epoch: [31]  [2610/2809]  eta: 0:01:15  lr: 0.000006  min_lr: 0.000000  loss: 3.7171 (3.6904)  loss_scale: 65536.0000 (56650.6136)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0004  max mem: 15572
Epoch: [31]  [2620/2809]  eta: 0:01:11  lr: 0.000006  min_lr: 0.000000  loss: 3.7171 (3.6910)  loss_scale: 65536.0000 (56684.5143)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0003  max mem: 15572
Epoch: [31]  [2630/2809]  eta: 0:01:08  lr: 0.000006  min_lr: 0.000000  loss: 3.6506 (3.6894)  loss_scale: 65536.0000 (56718.1574)  weight_decay: 0.0500 (0.0500)  time: 0.3872  data: 0.0005  max mem: 15572
Epoch: [31]  [2640/2809]  eta: 0:01:04  lr: 0.000006  min_lr: 0.000000  loss: 3.3832 (3.6895)  loss_scale: 65536.0000 (56751.5456)  weight_decay: 0.0500 (0.0500)  time: 0.3874  data: 0.0004  max mem: 15572
Epoch: [31]  [2650/2809]  eta: 0:01:00  lr: 0.000006  min_lr: 0.000000  loss: 3.8757 (3.6896)  loss_scale: 65536.0000 (56784.6820)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
[2025-01-13 08:44:44,970] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:44:44,970] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:44:46,447] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89734
[2025-01-13 08:44:46,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:44:46,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [2660/2809]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000000  loss: 3.7611 (3.6892)  loss_scale: 65536.0000 (56916.0827)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0003  max mem: 15572
Epoch: [31]  [2670/2809]  eta: 0:00:52  lr: 0.000006  min_lr: 0.000000  loss: 3.7611 (3.6895)  loss_scale: 65536.0000 (56948.3549)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [31]  [2680/2809]  eta: 0:00:49  lr: 0.000006  min_lr: 0.000000  loss: 3.9453 (3.6901)  loss_scale: 65536.0000 (56980.3864)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0003  max mem: 15572
Epoch: [31]  [2690/2809]  eta: 0:00:45  lr: 0.000006  min_lr: 0.000000  loss: 4.1289 (3.6920)  loss_scale: 65536.0000 (57012.1799)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [31]  [2700/2809]  eta: 0:00:41  lr: 0.000006  min_lr: 0.000000  loss: 3.9402 (3.6917)  loss_scale: 65536.0000 (57043.7379)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0003  max mem: 15572
Epoch: [31]  [2710/2809]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 3.5296 (3.6907)  loss_scale: 65536.0000 (57075.0631)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
Epoch: [31]  [2720/2809]  eta: 0:00:33  lr: 0.000006  min_lr: 0.000000  loss: 3.7139 (3.6914)  loss_scale: 65536.0000 (57106.1580)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0003  max mem: 15572
Epoch: [31]  [2730/2809]  eta: 0:00:30  lr: 0.000006  min_lr: 0.000000  loss: 3.9180 (3.6921)  loss_scale: 65536.0000 (57137.0253)  weight_decay: 0.0500 (0.0500)  time: 0.3805  data: 0.0004  max mem: 15572
Epoch: [31]  [2740/2809]  eta: 0:00:26  lr: 0.000006  min_lr: 0.000000  loss: 3.8163 (3.6922)  loss_scale: 65536.0000 (57167.6673)  weight_decay: 0.0500 (0.0500)  time: 0.3844  data: 0.0004  max mem: 15572
Epoch: [31]  [2750/2809]  eta: 0:00:22  lr: 0.000006  min_lr: 0.000000  loss: 3.7390 (3.6921)  loss_scale: 65536.0000 (57198.0865)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0003  max mem: 15572
Epoch: [31]  [2760/2809]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 3.8429 (3.6926)  loss_scale: 65536.0000 (57228.2854)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0003  max mem: 15572
Epoch: [31]  [2770/2809]  eta: 0:00:14  lr: 0.000006  min_lr: 0.000000  loss: 3.5468 (3.6915)  loss_scale: 65536.0000 (57258.2663)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0004  max mem: 15572
Epoch: [31]  [2780/2809]  eta: 0:00:11  lr: 0.000006  min_lr: 0.000000  loss: 3.5915 (3.6913)  loss_scale: 65536.0000 (57288.0316)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
[2025-01-13 08:45:34,965] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:45:34,965] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:45:36,510] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89867
[2025-01-13 08:45:36,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:45:36,511] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [2790/2809]  eta: 0:00:07  lr: 0.000006  min_lr: 0.000000  loss: 3.5915 (3.6900)  loss_scale: 65536.0000 (57411.5084)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0004  max mem: 15572
Epoch: [31]  [2800/2809]  eta: 0:00:03  lr: 0.000006  min_lr: 0.000000  loss: 3.6181 (3.6909)  loss_scale: 65536.0000 (57440.5141)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
Epoch: [31]  [2808/2809]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 3.7288 (3.6904)  loss_scale: 65536.0000 (57463.5700)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [31] Total time: 0:17:48 (0.3805 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 3.7288 (3.6904)  loss_scale: 65536.0000 (57463.5700)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:15:05  loss: 0.4201 (0.4201)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.3282  data: 3.1378  max mem: 15572
Val:  [ 10/272]  eta: 0:02:11  loss: 2.3202 (2.2851)  acc1: 50.0000 (43.4343)  acc5: 77.7778 (76.2626)  time: 0.5011  data: 0.3342  max mem: 15572
Val:  [ 20/272]  eta: 0:01:25  loss: 2.3202 (2.3388)  acc1: 50.0000 (45.7672)  acc5: 77.7778 (73.5450)  time: 0.1900  data: 0.0271  max mem: 15572
Val:  [ 30/272]  eta: 0:01:08  loss: 2.4104 (2.4298)  acc1: 44.4444 (41.5771)  acc5: 72.2222 (72.4014)  time: 0.1629  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:58  loss: 2.6058 (2.4672)  acc1: 27.7778 (39.2954)  acc5: 72.2222 (72.6287)  time: 0.1630  data: 0.0005  max mem: 15572
Val:  [ 50/272]  eta: 0:00:52  loss: 2.4252 (2.3821)  acc1: 33.3333 (41.6122)  acc5: 77.7778 (74.8366)  time: 0.1642  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:47  loss: 1.4732 (2.2729)  acc1: 61.1111 (44.8087)  acc5: 88.8889 (76.0474)  time: 0.1671  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:43  loss: 1.5407 (2.1873)  acc1: 66.6667 (47.2613)  acc5: 88.8889 (77.3865)  time: 0.1607  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:39  loss: 1.8210 (2.2023)  acc1: 61.1111 (46.8450)  acc5: 83.3333 (77.1605)  time: 0.1562  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:37  loss: 2.1750 (2.1989)  acc1: 44.4444 (47.2527)  acc5: 77.7778 (77.7778)  time: 0.1789  data: 0.0208  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 2.0880 (2.2245)  acc1: 50.0000 (46.7547)  acc5: 83.3333 (77.4477)  time: 0.1799  data: 0.0209  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 2.4900 (2.2991)  acc1: 22.2222 (44.6446)  acc5: 66.6667 (76.1762)  time: 0.1760  data: 0.0154  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 2.9622 (2.3357)  acc1: 16.6667 (44.0312)  acc5: 66.6667 (75.7576)  time: 0.1791  data: 0.0155  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 2.1759 (2.2974)  acc1: 44.4444 (45.1230)  acc5: 77.7778 (76.5055)  time: 0.1690  data: 0.0051  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 1.7054 (2.2896)  acc1: 50.0000 (45.5871)  acc5: 83.3333 (76.3199)  time: 0.1695  data: 0.0115  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 2.3226 (2.2951)  acc1: 44.4444 (45.2907)  acc5: 77.7778 (76.6004)  time: 0.1883  data: 0.0313  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 2.3226 (2.2866)  acc1: 50.0000 (45.8592)  acc5: 77.7778 (76.8461)  time: 0.1911  data: 0.0248  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 2.3470 (2.3051)  acc1: 44.4444 (45.3541)  acc5: 72.2222 (76.4782)  time: 0.1718  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 2.3382 (2.2982)  acc1: 38.8889 (45.2118)  acc5: 77.7778 (76.7956)  time: 0.1890  data: 0.0194  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 2.3382 (2.3532)  acc1: 38.8889 (44.0372)  acc5: 77.7778 (75.3636)  time: 0.1929  data: 0.0228  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 2.5461 (2.3617)  acc1: 38.8889 (43.6705)  acc5: 72.2222 (75.0967)  time: 0.1803  data: 0.0040  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.1734 (2.3613)  acc1: 44.4444 (43.8915)  acc5: 77.7778 (75.0395)  time: 0.1797  data: 0.0006  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 2.1909 (2.3508)  acc1: 44.4444 (44.1428)  acc5: 77.7778 (75.0628)  time: 0.1715  data: 0.0006  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.6880 (2.3205)  acc1: 61.1111 (45.1419)  acc5: 83.3333 (75.4209)  time: 0.1664  data: 0.0056  max mem: 15572
Val:  [240/272]  eta: 0:00:06  loss: 1.6564 (2.3056)  acc1: 61.1111 (45.3896)  acc5: 83.3333 (75.7261)  time: 0.1600  data: 0.0055  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 2.2772 (2.3189)  acc1: 44.4444 (44.7543)  acc5: 77.7778 (75.6308)  time: 0.1859  data: 0.0312  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.3540 (2.2612)  acc1: 72.2222 (46.4027)  acc5: 88.8889 (76.3942)  time: 0.1834  data: 0.0311  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4092 (2.2558)  acc1: 66.6667 (46.4740)  acc5: 88.8889 (76.5683)  time: 0.1405  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4092 (2.2600)  acc1: 66.6667 (46.4469)  acc5: 88.8889 (76.5308)  time: 0.1347  data: 0.0002  max mem: 15572
Val: Total time: 0:00:50 (0.1858 s / it)
* Acc@1 46.447 Acc@5 76.531 loss 2.260
Accuracy of the network on the 4883 val videos: 46.4%
Max accuracy: 46.63%
Epoch: [32]  [   0/2809]  eta: 3:35:03  lr: 0.000006  min_lr: 0.000000  loss: 3.7425 (3.7425)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.5937  data: 4.0867  max mem: 15572
Epoch: [32]  [  10/2809]  eta: 0:36:37  lr: 0.000006  min_lr: 0.000000  loss: 3.7425 (3.6293)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7850  data: 0.3834  max mem: 15572
Epoch: [32]  [  20/2809]  eta: 0:27:43  lr: 0.000006  min_lr: 0.000000  loss: 3.8056 (3.6968)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3964  data: 0.0068  max mem: 15572
Epoch: [32]  [  30/2809]  eta: 0:24:22  lr: 0.000006  min_lr: 0.000000  loss: 3.9129 (3.6609)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3837  data: 0.0004  max mem: 15572
Epoch: [32]  [  40/2809]  eta: 0:22:47  lr: 0.000006  min_lr: 0.000000  loss: 3.6567 (3.6182)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3859  data: 0.0003  max mem: 15572
Epoch: [32]  [  50/2809]  eta: 0:21:47  lr: 0.000006  min_lr: 0.000000  loss: 3.7635 (3.6561)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3926  data: 0.0004  max mem: 15572
Epoch: [32]  [  60/2809]  eta: 0:21:04  lr: 0.000006  min_lr: 0.000000  loss: 3.8517 (3.6726)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3914  data: 0.0004  max mem: 15572
Epoch: [32]  [  70/2809]  eta: 0:20:34  lr: 0.000006  min_lr: 0.000000  loss: 3.7269 (3.6635)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3923  data: 0.0004  max mem: 15572
Epoch: [32]  [  80/2809]  eta: 0:20:12  lr: 0.000006  min_lr: 0.000000  loss: 3.7864 (3.7127)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3958  data: 0.0005  max mem: 15572
Epoch: [32]  [  90/2809]  eta: 0:19:49  lr: 0.000006  min_lr: 0.000000  loss: 3.8691 (3.7138)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3906  data: 0.0004  max mem: 15572
Epoch: [32]  [ 100/2809]  eta: 0:19:27  lr: 0.000006  min_lr: 0.000000  loss: 3.6536 (3.7019)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
[2025-01-13 08:47:20,816] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89995
[2025-01-13 08:47:20,816] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:47:20,816] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 110/2809]  eta: 0:19:10  lr: 0.000006  min_lr: 0.000000  loss: 3.3843 (3.6849)  loss_scale: 65536.0000 (64355.1712)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0003  max mem: 15572
[2025-01-13 08:47:22,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=90000, skipped=613, lr=[5.594318732376175e-08, 5.594318732376175e-08, 7.991883903394536e-08, 7.991883903394536e-08, 1.141697700484934e-07, 1.141697700484934e-07, 1.6309967149784771e-07, 1.6309967149784771e-07, 2.3299953071121103e-07, 2.3299953071121103e-07, 3.328564724445872e-07, 3.328564724445872e-07, 4.755092463494103e-07, 4.755092463494103e-07, 6.792989233563005e-07, 6.792989233563005e-07, 9.704270333661435e-07, 9.704270333661435e-07, 1.3863243333802054e-06, 1.3863243333802054e-06, 1.980463333400293e-06, 1.980463333400293e-06, 2.8292333334289905e-06, 2.8292333334289905e-06, 4.041761904898559e-06, 4.041761904898559e-06, 5.773945578426512e-06, 5.773945578426512e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 08:47:22,295] [INFO] [timer.py:260:stop] epoch=0/micro_step=90000/global_step=90000, RunningAvgSamplesPerSec=31.147122792400324, CurrSamplesPerSec=34.62832563457958, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [32]  [ 120/2809]  eta: 0:18:53  lr: 0.000006  min_lr: 0.000000  loss: 3.6531 (3.6839)  loss_scale: 32768.0000 (61744.6612)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0004  max mem: 15572
Epoch: [32]  [ 130/2809]  eta: 0:18:40  lr: 0.000006  min_lr: 0.000000  loss: 3.6411 (3.6698)  loss_scale: 32768.0000 (59532.7023)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0004  max mem: 15572
Epoch: [32]  [ 140/2809]  eta: 0:18:28  lr: 0.000006  min_lr: 0.000000  loss: 3.5595 (3.6599)  loss_scale: 32768.0000 (57634.4965)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0004  max mem: 15572
Epoch: [32]  [ 150/2809]  eta: 0:18:18  lr: 0.000006  min_lr: 0.000000  loss: 3.7718 (3.6602)  loss_scale: 32768.0000 (55987.7086)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0003  max mem: 15572
Epoch: [32]  [ 160/2809]  eta: 0:18:08  lr: 0.000006  min_lr: 0.000000  loss: 3.5817 (3.6470)  loss_scale: 32768.0000 (54545.4907)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0003  max mem: 15572
Epoch: [32]  [ 170/2809]  eta: 0:17:59  lr: 0.000006  min_lr: 0.000000  loss: 3.4399 (3.6437)  loss_scale: 32768.0000 (53271.9532)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0004  max mem: 15572
Epoch: [32]  [ 180/2809]  eta: 0:17:51  lr: 0.000006  min_lr: 0.000000  loss: 3.6495 (3.6562)  loss_scale: 32768.0000 (52139.1381)  weight_decay: 0.0500 (0.0500)  time: 0.3810  data: 0.0003  max mem: 15572
Epoch: [32]  [ 190/2809]  eta: 0:17:45  lr: 0.000006  min_lr: 0.000000  loss: 3.9959 (3.6657)  loss_scale: 32768.0000 (51124.9424)  weight_decay: 0.0500 (0.0500)  time: 0.3901  data: 0.0004  max mem: 15572
Epoch: [32]  [ 200/2809]  eta: 0:17:39  lr: 0.000006  min_lr: 0.000000  loss: 3.6947 (3.6647)  loss_scale: 32768.0000 (50211.6617)  weight_decay: 0.0500 (0.0500)  time: 0.3906  data: 0.0005  max mem: 15572
Epoch: [32]  [ 210/2809]  eta: 0:17:32  lr: 0.000006  min_lr: 0.000000  loss: 3.6650 (3.6559)  loss_scale: 32768.0000 (49384.9479)  weight_decay: 0.0500 (0.0500)  time: 0.3850  data: 0.0005  max mem: 15572
Epoch: [32]  [ 220/2809]  eta: 0:17:26  lr: 0.000006  min_lr: 0.000000  loss: 3.7417 (3.6585)  loss_scale: 32768.0000 (48633.0498)  weight_decay: 0.0500 (0.0500)  time: 0.3852  data: 0.0005  max mem: 15572
Epoch: [32]  [ 230/2809]  eta: 0:17:18  lr: 0.000006  min_lr: 0.000000  loss: 3.7811 (3.6595)  loss_scale: 32768.0000 (47946.2511)  weight_decay: 0.0500 (0.0500)  time: 0.3788  data: 0.0003  max mem: 15572
[2025-01-13 08:48:09,884] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:48:09,884] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 240/2809]  eta: 0:17:11  lr: 0.000006  min_lr: 0.000000  loss: 3.7134 (3.6590)  loss_scale: 32768.0000 (47996.2822)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [32]  [ 250/2809]  eta: 0:17:04  lr: 0.000006  min_lr: 0.000000  loss: 3.6666 (3.6613)  loss_scale: 65536.0000 (48695.0757)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0003  max mem: 15572
Epoch: [32]  [ 260/2809]  eta: 0:16:58  lr: 0.000006  min_lr: 0.000000  loss: 3.5882 (3.6488)  loss_scale: 65536.0000 (49340.3218)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
Epoch: [32]  [ 270/2809]  eta: 0:16:51  lr: 0.000006  min_lr: 0.000000  loss: 3.3613 (3.6425)  loss_scale: 65536.0000 (49937.9483)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0003  max mem: 15572
Epoch: [32]  [ 280/2809]  eta: 0:16:45  lr: 0.000006  min_lr: 0.000000  loss: 3.5654 (3.6409)  loss_scale: 65536.0000 (50493.0391)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0003  max mem: 15572
Epoch: [32]  [ 290/2809]  eta: 0:16:40  lr: 0.000006  min_lr: 0.000000  loss: 3.8131 (3.6463)  loss_scale: 65536.0000 (51009.9794)  weight_decay: 0.0500 (0.0500)  time: 0.3788  data: 0.0003  max mem: 15572
Epoch: [32]  [ 300/2809]  eta: 0:16:34  lr: 0.000006  min_lr: 0.000000  loss: 3.8531 (3.6444)  loss_scale: 65536.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0003  max mem: 15572
Epoch: [32]  [ 310/2809]  eta: 0:16:28  lr: 0.000006  min_lr: 0.000000  loss: 3.8428 (3.6507)  loss_scale: 65536.0000 (51944.1286)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0002  max mem: 15572
Epoch: [32]  [ 320/2809]  eta: 0:16:24  lr: 0.000006  min_lr: 0.000000  loss: 3.8428 (3.6515)  loss_scale: 65536.0000 (52367.5514)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0002  max mem: 15572
Epoch: [32]  [ 330/2809]  eta: 0:16:18  lr: 0.000006  min_lr: 0.000000  loss: 3.5279 (3.6446)  loss_scale: 65536.0000 (52765.3897)  weight_decay: 0.0500 (0.0500)  time: 0.3815  data: 0.0002  max mem: 15572
Epoch: [32]  [ 340/2809]  eta: 0:16:13  lr: 0.000006  min_lr: 0.000000  loss: 3.5279 (3.6446)  loss_scale: 65536.0000 (53139.8944)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [32]  [ 350/2809]  eta: 0:16:08  lr: 0.000006  min_lr: 0.000000  loss: 3.7659 (3.6493)  loss_scale: 65536.0000 (53493.0598)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [32]  [ 360/2809]  eta: 0:16:02  lr: 0.000006  min_lr: 0.000000  loss: 3.8070 (3.6501)  loss_scale: 65536.0000 (53826.6593)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0003  max mem: 15572
[2025-01-13 08:48:58,060] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:48:58,060] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:48:58,422] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90253
[2025-01-13 08:48:58,422] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:48:58,422] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [ 370/2809]  eta: 0:15:57  lr: 0.000006  min_lr: 0.000000  loss: 3.5856 (3.6488)  loss_scale: 65536.0000 (54318.9218)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [32]  [ 380/2809]  eta: 0:15:52  lr: 0.000006  min_lr: 0.000000  loss: 3.6001 (3.6463)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0003  max mem: 15572
Epoch: [32]  [ 390/2809]  eta: 0:15:47  lr: 0.000006  min_lr: 0.000000  loss: 3.6499 (3.6483)  loss_scale: 65536.0000 (54892.6854)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0003  max mem: 15572
Epoch: [32]  [ 400/2809]  eta: 0:15:42  lr: 0.000006  min_lr: 0.000000  loss: 3.7815 (3.6520)  loss_scale: 65536.0000 (55158.1047)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0003  max mem: 15572
Epoch: [32]  [ 410/2809]  eta: 0:15:37  lr: 0.000006  min_lr: 0.000000  loss: 3.6726 (3.6498)  loss_scale: 65536.0000 (55410.6083)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
Epoch: [32]  [ 420/2809]  eta: 0:15:33  lr: 0.000006  min_lr: 0.000000  loss: 3.2509 (3.6452)  loss_scale: 65536.0000 (55651.1164)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0003  max mem: 15572
Epoch: [32]  [ 430/2809]  eta: 0:15:28  lr: 0.000006  min_lr: 0.000000  loss: 3.5225 (3.6458)  loss_scale: 65536.0000 (55880.4640)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0004  max mem: 15572
Epoch: [32]  [ 440/2809]  eta: 0:15:23  lr: 0.000006  min_lr: 0.000000  loss: 3.5661 (3.6445)  loss_scale: 65536.0000 (56099.4104)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0003  max mem: 15572
Epoch: [32]  [ 450/2809]  eta: 0:15:19  lr: 0.000006  min_lr: 0.000000  loss: 3.7321 (3.6451)  loss_scale: 65536.0000 (56308.6475)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0003  max mem: 15572
Epoch: [32]  [ 460/2809]  eta: 0:15:14  lr: 0.000006  min_lr: 0.000000  loss: 3.8476 (3.6522)  loss_scale: 65536.0000 (56508.8069)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0003  max mem: 15572
Epoch: [32]  [ 470/2809]  eta: 0:15:09  lr: 0.000006  min_lr: 0.000000  loss: 3.7319 (3.6496)  loss_scale: 65536.0000 (56700.4671)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0003  max mem: 15572
Epoch: [32]  [ 480/2809]  eta: 0:15:05  lr: 0.000006  min_lr: 0.000000  loss: 3.5172 (3.6467)  loss_scale: 65536.0000 (56884.1580)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
Epoch: [32]  [ 490/2809]  eta: 0:15:00  lr: 0.000006  min_lr: 0.000000  loss: 3.6576 (3.6491)  loss_scale: 65536.0000 (57060.3666)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
[2025-01-13 08:49:46,148] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90380
[2025-01-13 08:49:46,148] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:49:46,148] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 500/2809]  eta: 0:14:56  lr: 0.000006  min_lr: 0.000000  loss: 3.7473 (3.6489)  loss_scale: 65536.0000 (56640.8942)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0003  max mem: 15572
Epoch: [32]  [ 510/2809]  eta: 0:14:52  lr: 0.000006  min_lr: 0.000000  loss: 3.6645 (3.6467)  loss_scale: 32768.0000 (56173.7143)  weight_decay: 0.0500 (0.0500)  time: 0.3803  data: 0.0003  max mem: 15572
Epoch: [32]  [ 520/2809]  eta: 0:14:47  lr: 0.000006  min_lr: 0.000000  loss: 3.8257 (3.6526)  loss_scale: 32768.0000 (55724.4683)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [32]  [ 530/2809]  eta: 0:14:43  lr: 0.000006  min_lr: 0.000000  loss: 3.8815 (3.6586)  loss_scale: 32768.0000 (55292.1431)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0003  max mem: 15572
Epoch: [32]  [ 540/2809]  eta: 0:14:39  lr: 0.000006  min_lr: 0.000000  loss: 3.9526 (3.6630)  loss_scale: 32768.0000 (54875.8004)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0003  max mem: 15572
Epoch: [32]  [ 550/2809]  eta: 0:14:34  lr: 0.000006  min_lr: 0.000000  loss: 3.9754 (3.6661)  loss_scale: 32768.0000 (54474.5699)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [32]  [ 560/2809]  eta: 0:14:30  lr: 0.000006  min_lr: 0.000000  loss: 3.8528 (3.6667)  loss_scale: 32768.0000 (54087.6435)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0004  max mem: 15572
Epoch: [32]  [ 570/2809]  eta: 0:14:26  lr: 0.000006  min_lr: 0.000000  loss: 3.4837 (3.6625)  loss_scale: 32768.0000 (53714.2697)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0005  max mem: 15572
Epoch: [32]  [ 580/2809]  eta: 0:14:22  lr: 0.000006  min_lr: 0.000000  loss: 3.5459 (3.6628)  loss_scale: 32768.0000 (53353.7487)  weight_decay: 0.0500 (0.0500)  time: 0.3797  data: 0.0004  max mem: 15572
Epoch: [32]  [ 590/2809]  eta: 0:14:17  lr: 0.000006  min_lr: 0.000000  loss: 3.8357 (3.6663)  loss_scale: 32768.0000 (53005.4281)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0003  max mem: 15572
Epoch: [32]  [ 600/2809]  eta: 0:14:13  lr: 0.000006  min_lr: 0.000000  loss: 3.7604 (3.6652)  loss_scale: 32768.0000 (52668.6988)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0003  max mem: 15572
Epoch: [32]  [ 610/2809]  eta: 0:14:09  lr: 0.000006  min_lr: 0.000000  loss: 3.7126 (3.6677)  loss_scale: 32768.0000 (52342.9918)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
Epoch: [32]  [ 620/2809]  eta: 0:14:05  lr: 0.000006  min_lr: 0.000000  loss: 3.7867 (3.6681)  loss_scale: 32768.0000 (52027.7746)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
[2025-01-13 08:50:34,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:50:34,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 630/2809]  eta: 0:14:00  lr: 0.000006  min_lr: 0.000000  loss: 3.7493 (3.6645)  loss_scale: 32768.0000 (52241.8510)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0003  max mem: 15572
Epoch: [32]  [ 640/2809]  eta: 0:13:56  lr: 0.000006  min_lr: 0.000000  loss: 3.7701 (3.6671)  loss_scale: 65536.0000 (52449.2480)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
Epoch: [32]  [ 650/2809]  eta: 0:13:52  lr: 0.000006  min_lr: 0.000000  loss: 3.8162 (3.6680)  loss_scale: 65536.0000 (52650.2734)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0003  max mem: 15572
Epoch: [32]  [ 660/2809]  eta: 0:13:48  lr: 0.000006  min_lr: 0.000000  loss: 3.8162 (3.6682)  loss_scale: 65536.0000 (52845.2163)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0003  max mem: 15572
Epoch: [32]  [ 670/2809]  eta: 0:13:44  lr: 0.000006  min_lr: 0.000000  loss: 3.8644 (3.6686)  loss_scale: 65536.0000 (53034.3487)  weight_decay: 0.0500 (0.0500)  time: 0.3810  data: 0.0004  max mem: 15572
Epoch: [32]  [ 680/2809]  eta: 0:13:40  lr: 0.000005  min_lr: 0.000000  loss: 3.6047 (3.6655)  loss_scale: 65536.0000 (53217.9266)  weight_decay: 0.0500 (0.0500)  time: 0.3884  data: 0.0005  max mem: 15572
Epoch: [32]  [ 690/2809]  eta: 0:13:36  lr: 0.000005  min_lr: 0.000000  loss: 3.6047 (3.6646)  loss_scale: 65536.0000 (53396.1910)  weight_decay: 0.0500 (0.0500)  time: 0.3861  data: 0.0004  max mem: 15572
Epoch: [32]  [ 700/2809]  eta: 0:13:32  lr: 0.000005  min_lr: 0.000000  loss: 3.5913 (3.6617)  loss_scale: 65536.0000 (53569.3695)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0004  max mem: 15572
Epoch: [32]  [ 710/2809]  eta: 0:13:28  lr: 0.000005  min_lr: 0.000000  loss: 3.5164 (3.6609)  loss_scale: 65536.0000 (53737.6765)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [32]  [ 720/2809]  eta: 0:13:24  lr: 0.000005  min_lr: 0.000000  loss: 3.4342 (3.6572)  loss_scale: 65536.0000 (53901.3148)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0003  max mem: 15572
Epoch: [32]  [ 730/2809]  eta: 0:13:19  lr: 0.000005  min_lr: 0.000000  loss: 3.4697 (3.6555)  loss_scale: 65536.0000 (54060.4761)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0003  max mem: 15572
Epoch: [32]  [ 740/2809]  eta: 0:13:15  lr: 0.000005  min_lr: 0.000000  loss: 3.6882 (3.6567)  loss_scale: 65536.0000 (54215.3414)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
[2025-01-13 08:51:23,151] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:51:23,151] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [32]  [ 750/2809]  eta: 0:13:11  lr: 0.000005  min_lr: 0.000000  loss: 3.6882 (3.6559)  loss_scale: 65536.0000 (54540.6125)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0003  max mem: 15572
[2025-01-13 08:51:23,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90639
[2025-01-13 08:51:23,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:51:23,902] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [ 760/2809]  eta: 0:13:07  lr: 0.000005  min_lr: 0.000000  loss: 3.5949 (3.6538)  loss_scale: 65536.0000 (54685.0986)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0003  max mem: 15572
Epoch: [32]  [ 770/2809]  eta: 0:13:03  lr: 0.000005  min_lr: 0.000000  loss: 3.5949 (3.6535)  loss_scale: 65536.0000 (54825.8366)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [32]  [ 780/2809]  eta: 0:12:59  lr: 0.000005  min_lr: 0.000000  loss: 3.5736 (3.6500)  loss_scale: 65536.0000 (54962.9706)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0003  max mem: 15572
Epoch: [32]  [ 790/2809]  eta: 0:12:55  lr: 0.000005  min_lr: 0.000000  loss: 3.2718 (3.6484)  loss_scale: 65536.0000 (55096.6372)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0003  max mem: 15572
Epoch: [32]  [ 800/2809]  eta: 0:12:51  lr: 0.000005  min_lr: 0.000000  loss: 3.4674 (3.6482)  loss_scale: 65536.0000 (55226.9663)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0003  max mem: 15572
Epoch: [32]  [ 810/2809]  eta: 0:12:47  lr: 0.000005  min_lr: 0.000000  loss: 3.5907 (3.6503)  loss_scale: 65536.0000 (55354.0814)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0003  max mem: 15572
Epoch: [32]  [ 820/2809]  eta: 0:12:43  lr: 0.000005  min_lr: 0.000000  loss: 3.7101 (3.6525)  loss_scale: 65536.0000 (55478.0999)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [32]  [ 830/2809]  eta: 0:12:39  lr: 0.000005  min_lr: 0.000000  loss: 3.9719 (3.6568)  loss_scale: 65536.0000 (55599.1336)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0003  max mem: 15572
Epoch: [32]  [ 840/2809]  eta: 0:12:35  lr: 0.000005  min_lr: 0.000000  loss: 3.7335 (3.6547)  loss_scale: 65536.0000 (55717.2889)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0003  max mem: 15572
Epoch: [32]  [ 850/2809]  eta: 0:12:31  lr: 0.000005  min_lr: 0.000000  loss: 3.5479 (3.6515)  loss_scale: 65536.0000 (55832.6675)  weight_decay: 0.0500 (0.0500)  time: 0.3836  data: 0.0003  max mem: 15572
[2025-01-13 08:52:04,580] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90747
[2025-01-13 08:52:04,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:52:04,580] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 860/2809]  eta: 0:12:27  lr: 0.000005  min_lr: 0.000000  loss: 3.6451 (3.6531)  loss_scale: 65536.0000 (55869.2497)  weight_decay: 0.0500 (0.0500)  time: 0.3844  data: 0.0004  max mem: 15572
Epoch: [32]  [ 870/2809]  eta: 0:12:23  lr: 0.000005  min_lr: 0.000000  loss: 3.6938 (3.6534)  loss_scale: 32768.0000 (55604.0230)  weight_decay: 0.0500 (0.0500)  time: 0.3833  data: 0.0004  max mem: 15572
Epoch: [32]  [ 880/2809]  eta: 0:12:20  lr: 0.000005  min_lr: 0.000000  loss: 3.6938 (3.6533)  loss_scale: 32768.0000 (55344.8173)  weight_decay: 0.0500 (0.0500)  time: 0.3990  data: 0.0004  max mem: 15572
Epoch: [32]  [ 890/2809]  eta: 0:12:16  lr: 0.000005  min_lr: 0.000000  loss: 3.8273 (3.6544)  loss_scale: 32768.0000 (55091.4299)  weight_decay: 0.0500 (0.0500)  time: 0.4026  data: 0.0004  max mem: 15572
Epoch: [32]  [ 900/2809]  eta: 0:12:12  lr: 0.000005  min_lr: 0.000000  loss: 3.9088 (3.6589)  loss_scale: 32768.0000 (54843.6670)  weight_decay: 0.0500 (0.0500)  time: 0.3871  data: 0.0004  max mem: 15572
Epoch: [32]  [ 910/2809]  eta: 0:12:09  lr: 0.000005  min_lr: 0.000000  loss: 3.7783 (3.6535)  loss_scale: 32768.0000 (54601.3436)  weight_decay: 0.0500 (0.0500)  time: 0.3904  data: 0.0005  max mem: 15572
Epoch: [32]  [ 920/2809]  eta: 0:12:05  lr: 0.000005  min_lr: 0.000000  loss: 3.3427 (3.6541)  loss_scale: 32768.0000 (54364.2823)  weight_decay: 0.0500 (0.0500)  time: 0.3950  data: 0.0005  max mem: 15572
Epoch: [32]  [ 930/2809]  eta: 0:12:01  lr: 0.000005  min_lr: 0.000000  loss: 3.4727 (3.6531)  loss_scale: 32768.0000 (54132.3136)  weight_decay: 0.0500 (0.0500)  time: 0.3906  data: 0.0005  max mem: 15572
Epoch: [32]  [ 940/2809]  eta: 0:11:58  lr: 0.000005  min_lr: 0.000000  loss: 3.7530 (3.6565)  loss_scale: 32768.0000 (53905.2752)  weight_decay: 0.0500 (0.0500)  time: 0.3906  data: 0.0004  max mem: 15572
Epoch: [32]  [ 950/2809]  eta: 0:11:54  lr: 0.000005  min_lr: 0.000000  loss: 3.7684 (3.6552)  loss_scale: 32768.0000 (53683.0116)  weight_decay: 0.0500 (0.0500)  time: 0.3901  data: 0.0005  max mem: 15572
Epoch: [32]  [ 960/2809]  eta: 0:11:50  lr: 0.000005  min_lr: 0.000000  loss: 3.6798 (3.6543)  loss_scale: 32768.0000 (53465.3736)  weight_decay: 0.0500 (0.0500)  time: 0.3862  data: 0.0005  max mem: 15572
Epoch: [32]  [ 970/2809]  eta: 0:11:46  lr: 0.000005  min_lr: 0.000000  loss: 3.8437 (3.6588)  loss_scale: 32768.0000 (53252.2183)  weight_decay: 0.0500 (0.0500)  time: 0.3860  data: 0.0004  max mem: 15572
Epoch: [32]  [ 980/2809]  eta: 0:11:43  lr: 0.000005  min_lr: 0.000000  loss: 3.8256 (3.6586)  loss_scale: 32768.0000 (53043.4088)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0003  max mem: 15572
[2025-01-13 08:52:54,929] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:52:54,929] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 990/2809]  eta: 0:11:39  lr: 0.000005  min_lr: 0.000000  loss: 3.6101 (3.6596)  loss_scale: 32768.0000 (52938.0101)  weight_decay: 0.0500 (0.0500)  time: 0.3825  data: 0.0002  max mem: 15572
Epoch: [32]  [1000/2809]  eta: 0:11:35  lr: 0.000005  min_lr: 0.000000  loss: 3.8358 (3.6626)  loss_scale: 65536.0000 (53063.8641)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0002  max mem: 15572
Epoch: [32]  [1010/2809]  eta: 0:11:30  lr: 0.000005  min_lr: 0.000000  loss: 3.7421 (3.6607)  loss_scale: 65536.0000 (53187.2285)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [32]  [1020/2809]  eta: 0:11:26  lr: 0.000005  min_lr: 0.000000  loss: 3.3755 (3.6591)  loss_scale: 65536.0000 (53308.1763)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
Epoch: [32]  [1030/2809]  eta: 0:11:22  lr: 0.000005  min_lr: 0.000000  loss: 3.5899 (3.6616)  loss_scale: 65536.0000 (53426.7779)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0003  max mem: 15572
Epoch: [32]  [1040/2809]  eta: 0:11:18  lr: 0.000005  min_lr: 0.000000  loss: 3.6893 (3.6604)  loss_scale: 65536.0000 (53543.1009)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0003  max mem: 15572
Epoch: [32]  [1050/2809]  eta: 0:11:14  lr: 0.000005  min_lr: 0.000000  loss: 3.6893 (3.6626)  loss_scale: 65536.0000 (53657.2103)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [32]  [1060/2809]  eta: 0:11:10  lr: 0.000005  min_lr: 0.000000  loss: 3.8251 (3.6632)  loss_scale: 65536.0000 (53769.1687)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
Epoch: [32]  [1070/2809]  eta: 0:11:06  lr: 0.000005  min_lr: 0.000000  loss: 3.8100 (3.6640)  loss_scale: 65536.0000 (53879.0364)  weight_decay: 0.0500 (0.0500)  time: 0.3779  data: 0.0003  max mem: 15572
Epoch: [32]  [1080/2809]  eta: 0:11:02  lr: 0.000005  min_lr: 0.000000  loss: 3.6375 (3.6637)  loss_scale: 65536.0000 (53986.8714)  weight_decay: 0.0500 (0.0500)  time: 0.3788  data: 0.0003  max mem: 15572
Epoch: [32]  [1090/2809]  eta: 0:10:58  lr: 0.000005  min_lr: 0.000000  loss: 3.8305 (3.6635)  loss_scale: 65536.0000 (54092.7296)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0003  max mem: 15572
Epoch: [32]  [1100/2809]  eta: 0:10:54  lr: 0.000005  min_lr: 0.000000  loss: 3.8376 (3.6655)  loss_scale: 65536.0000 (54196.6649)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0003  max mem: 15572
Epoch: [32]  [1110/2809]  eta: 0:10:51  lr: 0.000005  min_lr: 0.000000  loss: 3.8251 (3.6651)  loss_scale: 65536.0000 (54298.7291)  weight_decay: 0.0500 (0.0500)  time: 0.3845  data: 0.0004  max mem: 15572
[2025-01-13 08:53:41,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=91000, skipped=617, lr=[5.128143724886411e-08, 5.128143724886411e-08, 7.325919606980588e-08, 7.325919606980588e-08, 1.0465599438543698e-07, 1.0465599438543698e-07, 1.4950856340776714e-07, 1.4950856340776714e-07, 2.135836620110959e-07, 2.135836620110959e-07, 3.051195171587084e-07, 3.051195171587084e-07, 4.3588502451244066e-07, 4.3588502451244066e-07, 6.226928921606296e-07, 6.226928921606296e-07, 8.895612745151851e-07, 8.895612745151851e-07, 1.2708018207359789e-06, 1.2708018207359789e-06, 1.8154311724799698e-06, 1.8154311724799698e-06, 2.593473103542814e-06, 2.593473103542814e-06, 3.704961576489735e-06, 3.704961576489735e-06, 5.292802252128193e-06, 5.292802252128193e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 08:53:41,032] [INFO] [timer.py:260:stop] epoch=0/micro_step=91000/global_step=91000, RunningAvgSamplesPerSec=31.171498382380072, CurrSamplesPerSec=32.2440344403444, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-13 08:53:42,949] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:53:42,950] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:53:44,452] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91008
[2025-01-13 08:53:44,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:53:44,452] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [1120/2809]  eta: 0:10:47  lr: 0.000005  min_lr: 0.000000  loss: 3.4293 (3.6644)  loss_scale: 65536.0000 (54632.8207)  weight_decay: 0.0500 (0.0500)  time: 0.3852  data: 0.0005  max mem: 15572
Epoch: [32]  [1130/2809]  eta: 0:10:43  lr: 0.000005  min_lr: 0.000000  loss: 3.3132 (3.6632)  loss_scale: 65536.0000 (54729.2237)  weight_decay: 0.0500 (0.0500)  time: 0.3820  data: 0.0004  max mem: 15572
Epoch: [32]  [1140/2809]  eta: 0:10:39  lr: 0.000005  min_lr: 0.000000  loss: 3.3404 (3.6625)  loss_scale: 65536.0000 (54823.9369)  weight_decay: 0.0500 (0.0500)  time: 0.3834  data: 0.0004  max mem: 15572
Epoch: [32]  [1150/2809]  eta: 0:10:35  lr: 0.000005  min_lr: 0.000000  loss: 3.6381 (3.6623)  loss_scale: 65536.0000 (54917.0043)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0004  max mem: 15572
Epoch: [32]  [1160/2809]  eta: 0:10:32  lr: 0.000005  min_lr: 0.000000  loss: 3.8061 (3.6628)  loss_scale: 65536.0000 (55008.4686)  weight_decay: 0.0500 (0.0500)  time: 0.3869  data: 0.0005  max mem: 15572
Epoch: [32]  [1170/2809]  eta: 0:10:28  lr: 0.000005  min_lr: 0.000000  loss: 3.8615 (3.6646)  loss_scale: 65536.0000 (55098.3706)  weight_decay: 0.0500 (0.0500)  time: 0.3854  data: 0.0005  max mem: 15572
Epoch: [32]  [1180/2809]  eta: 0:10:24  lr: 0.000005  min_lr: 0.000000  loss: 3.8615 (3.6651)  loss_scale: 65536.0000 (55186.7502)  weight_decay: 0.0500 (0.0500)  time: 0.3955  data: 0.0004  max mem: 15572
Epoch: [32]  [1190/2809]  eta: 0:10:21  lr: 0.000005  min_lr: 0.000000  loss: 3.9196 (3.6674)  loss_scale: 65536.0000 (55273.6457)  weight_decay: 0.0500 (0.0500)  time: 0.4033  data: 0.0005  max mem: 15572
Epoch: [32]  [1200/2809]  eta: 0:10:17  lr: 0.000005  min_lr: 0.000000  loss: 3.8322 (3.6669)  loss_scale: 65536.0000 (55359.0941)  weight_decay: 0.0500 (0.0500)  time: 0.3982  data: 0.0006  max mem: 15572
Epoch: [32]  [1210/2809]  eta: 0:10:13  lr: 0.000005  min_lr: 0.000000  loss: 3.7699 (3.6665)  loss_scale: 65536.0000 (55443.1313)  weight_decay: 0.0500 (0.0500)  time: 0.3951  data: 0.0006  max mem: 15572
Epoch: [32]  [1220/2809]  eta: 0:10:10  lr: 0.000005  min_lr: 0.000000  loss: 3.8007 (3.6667)  loss_scale: 65536.0000 (55525.7920)  weight_decay: 0.0500 (0.0500)  time: 0.3949  data: 0.0005  max mem: 15572
Epoch: [32]  [1230/2809]  eta: 0:10:06  lr: 0.000005  min_lr: 0.000000  loss: 3.5970 (3.6654)  loss_scale: 65536.0000 (55607.1097)  weight_decay: 0.0500 (0.0500)  time: 0.3912  data: 0.0005  max mem: 15572
Epoch: [32]  [1240/2809]  eta: 0:10:02  lr: 0.000005  min_lr: 0.000000  loss: 3.5957 (3.6654)  loss_scale: 65536.0000 (55687.1168)  weight_decay: 0.0500 (0.0500)  time: 0.3870  data: 0.0004  max mem: 15572
[2025-01-13 08:54:34,883] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:54:34,883] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [32]  [1250/2809]  eta: 0:09:58  lr: 0.000005  min_lr: 0.000000  loss: 3.8792 (3.6679)  loss_scale: 65536.0000 (55870.6187)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0005  max mem: 15572
[2025-01-13 08:54:36,070] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91140
[2025-01-13 08:54:36,070] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:54:36,070] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [1260/2809]  eta: 0:09:54  lr: 0.000005  min_lr: 0.000000  loss: 3.9575 (3.6697)  loss_scale: 65536.0000 (55999.2387)  weight_decay: 0.0500 (0.0500)  time: 0.3893  data: 0.0005  max mem: 15572
Epoch: [32]  [1270/2809]  eta: 0:09:51  lr: 0.000005  min_lr: 0.000000  loss: 3.7595 (3.6701)  loss_scale: 65536.0000 (56074.2722)  weight_decay: 0.0500 (0.0500)  time: 0.3943  data: 0.0004  max mem: 15572
Epoch: [32]  [1280/2809]  eta: 0:09:47  lr: 0.000005  min_lr: 0.000000  loss: 3.7665 (3.6715)  loss_scale: 65536.0000 (56148.1343)  weight_decay: 0.0500 (0.0500)  time: 0.4001  data: 0.0030  max mem: 15572
Epoch: [32]  [1290/2809]  eta: 0:09:43  lr: 0.000005  min_lr: 0.000000  loss: 3.7665 (3.6726)  loss_scale: 65536.0000 (56220.8521)  weight_decay: 0.0500 (0.0500)  time: 0.3981  data: 0.0031  max mem: 15572
Epoch: [32]  [1300/2809]  eta: 0:09:40  lr: 0.000005  min_lr: 0.000000  loss: 3.5950 (3.6714)  loss_scale: 65536.0000 (56292.4520)  weight_decay: 0.0500 (0.0500)  time: 0.3886  data: 0.0005  max mem: 15572
Epoch: [32]  [1310/2809]  eta: 0:09:36  lr: 0.000005  min_lr: 0.000000  loss: 3.5995 (3.6721)  loss_scale: 65536.0000 (56362.9596)  weight_decay: 0.0500 (0.0500)  time: 0.3791  data: 0.0004  max mem: 15572
Epoch: [32]  [1320/2809]  eta: 0:09:32  lr: 0.000005  min_lr: 0.000000  loss: 3.7351 (3.6725)  loss_scale: 65536.0000 (56432.3997)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0003  max mem: 15572
Epoch: [32]  [1330/2809]  eta: 0:09:28  lr: 0.000005  min_lr: 0.000000  loss: 3.7584 (3.6719)  loss_scale: 65536.0000 (56500.7964)  weight_decay: 0.0500 (0.0500)  time: 0.3780  data: 0.0003  max mem: 15572
Epoch: [32]  [1340/2809]  eta: 0:09:24  lr: 0.000005  min_lr: 0.000000  loss: 3.8898 (3.6730)  loss_scale: 65536.0000 (56568.1730)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0002  max mem: 15572
Epoch: [32]  [1350/2809]  eta: 0:09:20  lr: 0.000005  min_lr: 0.000000  loss: 3.6884 (3.6710)  loss_scale: 65536.0000 (56634.5522)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [32]  [1360/2809]  eta: 0:09:16  lr: 0.000005  min_lr: 0.000000  loss: 3.4908 (3.6710)  loss_scale: 65536.0000 (56699.9559)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [32]  [1370/2809]  eta: 0:09:12  lr: 0.000005  min_lr: 0.000000  loss: 3.7993 (3.6735)  loss_scale: 65536.0000 (56764.4055)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [32]  [1380/2809]  eta: 0:09:08  lr: 0.000005  min_lr: 0.000000  loss: 3.9024 (3.6750)  loss_scale: 65536.0000 (56827.9218)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
[2025-01-13 08:55:25,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:55:25,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:55:26,000] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91271
[2025-01-13 08:55:26,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:55:26,000] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [1390/2809]  eta: 0:09:04  lr: 0.000005  min_lr: 0.000000  loss: 3.7402 (3.6741)  loss_scale: 65536.0000 (56984.7534)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0003  max mem: 15572
Epoch: [32]  [1400/2809]  eta: 0:09:00  lr: 0.000005  min_lr: 0.000000  loss: 3.5907 (3.6732)  loss_scale: 65536.0000 (57045.7901)  weight_decay: 0.0500 (0.0500)  time: 0.3805  data: 0.0004  max mem: 15572
Epoch: [32]  [1410/2809]  eta: 0:08:57  lr: 0.000005  min_lr: 0.000000  loss: 3.6465 (3.6741)  loss_scale: 65536.0000 (57105.9617)  weight_decay: 0.0500 (0.0500)  time: 0.3906  data: 0.0004  max mem: 15572
Epoch: [32]  [1420/2809]  eta: 0:08:53  lr: 0.000005  min_lr: 0.000000  loss: 3.7213 (3.6746)  loss_scale: 65536.0000 (57165.2864)  weight_decay: 0.0500 (0.0500)  time: 0.3968  data: 0.0005  max mem: 15572
Epoch: [32]  [1430/2809]  eta: 0:08:49  lr: 0.000005  min_lr: 0.000000  loss: 3.7007 (3.6745)  loss_scale: 65536.0000 (57223.7820)  weight_decay: 0.0500 (0.0500)  time: 0.4005  data: 0.0006  max mem: 15572
Epoch: [32]  [1440/2809]  eta: 0:08:45  lr: 0.000005  min_lr: 0.000000  loss: 3.6609 (3.6752)  loss_scale: 65536.0000 (57281.4656)  weight_decay: 0.0500 (0.0500)  time: 0.3969  data: 0.0005  max mem: 15572
Epoch: [32]  [1450/2809]  eta: 0:08:42  lr: 0.000005  min_lr: 0.000000  loss: 3.5587 (3.6728)  loss_scale: 65536.0000 (57338.3542)  weight_decay: 0.0500 (0.0500)  time: 0.3965  data: 0.0005  max mem: 15572
Epoch: [32]  [1460/2809]  eta: 0:08:38  lr: 0.000005  min_lr: 0.000000  loss: 3.4979 (3.6732)  loss_scale: 65536.0000 (57394.4641)  weight_decay: 0.0500 (0.0500)  time: 0.4007  data: 0.0005  max mem: 15572
Epoch: [32]  [1470/2809]  eta: 0:08:34  lr: 0.000005  min_lr: 0.000000  loss: 3.8307 (3.6731)  loss_scale: 65536.0000 (57449.8110)  weight_decay: 0.0500 (0.0500)  time: 0.3938  data: 0.0004  max mem: 15572
Epoch: [32]  [1480/2809]  eta: 0:08:30  lr: 0.000005  min_lr: 0.000000  loss: 3.5039 (3.6723)  loss_scale: 65536.0000 (57504.4105)  weight_decay: 0.0500 (0.0500)  time: 0.3842  data: 0.0004  max mem: 15572
Epoch: [32]  [1490/2809]  eta: 0:08:27  lr: 0.000005  min_lr: 0.000000  loss: 3.8559 (3.6739)  loss_scale: 65536.0000 (57558.2777)  weight_decay: 0.0500 (0.0500)  time: 0.3950  data: 0.0005  max mem: 15572
Epoch: [32]  [1500/2809]  eta: 0:08:23  lr: 0.000005  min_lr: 0.000000  loss: 3.7973 (3.6731)  loss_scale: 65536.0000 (57611.4270)  weight_decay: 0.0500 (0.0500)  time: 0.4016  data: 0.0004  max mem: 15572
Epoch: [32]  [1510/2809]  eta: 0:08:19  lr: 0.000005  min_lr: 0.000000  loss: 3.6032 (3.6730)  loss_scale: 65536.0000 (57663.8729)  weight_decay: 0.0500 (0.0500)  time: 0.3919  data: 0.0005  max mem: 15572
[2025-01-13 08:56:16,778] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:56:16,778] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 08:56:18,371] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91404
[2025-01-13 08:56:18,371] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:56:18,371] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [1520/2809]  eta: 0:08:15  lr: 0.000005  min_lr: 0.000000  loss: 3.7277 (3.6729)  loss_scale: 65536.0000 (57887.9790)  weight_decay: 0.0500 (0.0500)  time: 0.3879  data: 0.0005  max mem: 15572
Epoch: [32]  [1530/2809]  eta: 0:08:12  lr: 0.000005  min_lr: 0.000000  loss: 3.7277 (3.6730)  loss_scale: 65536.0000 (57937.9334)  weight_decay: 0.0500 (0.0500)  time: 0.3968  data: 0.0004  max mem: 15572
Epoch: [32]  [1540/2809]  eta: 0:08:08  lr: 0.000005  min_lr: 0.000000  loss: 3.7448 (3.6742)  loss_scale: 65536.0000 (57987.2395)  weight_decay: 0.0500 (0.0500)  time: 0.4028  data: 0.0004  max mem: 15572
Epoch: [32]  [1550/2809]  eta: 0:08:04  lr: 0.000005  min_lr: 0.000000  loss: 3.7809 (3.6749)  loss_scale: 65536.0000 (58035.9097)  weight_decay: 0.0500 (0.0500)  time: 0.3965  data: 0.0004  max mem: 15572
Epoch: [32]  [1560/2809]  eta: 0:08:00  lr: 0.000005  min_lr: 0.000000  loss: 3.7062 (3.6747)  loss_scale: 65536.0000 (58083.9564)  weight_decay: 0.0500 (0.0500)  time: 0.3825  data: 0.0003  max mem: 15572
Epoch: [32]  [1570/2809]  eta: 0:07:56  lr: 0.000005  min_lr: 0.000000  loss: 3.7474 (3.6753)  loss_scale: 65536.0000 (58131.3915)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [32]  [1580/2809]  eta: 0:07:52  lr: 0.000005  min_lr: 0.000000  loss: 3.7061 (3.6739)  loss_scale: 65536.0000 (58178.2264)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0002  max mem: 15572
Epoch: [32]  [1590/2809]  eta: 0:07:48  lr: 0.000005  min_lr: 0.000000  loss: 3.7566 (3.6746)  loss_scale: 65536.0000 (58224.4727)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0002  max mem: 15572
Epoch: [32]  [1600/2809]  eta: 0:07:45  lr: 0.000005  min_lr: 0.000000  loss: 3.6188 (3.6731)  loss_scale: 65536.0000 (58270.1412)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0003  max mem: 15572
Epoch: [32]  [1610/2809]  eta: 0:07:41  lr: 0.000005  min_lr: 0.000000  loss: 3.3789 (3.6717)  loss_scale: 65536.0000 (58315.2427)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0003  max mem: 15572
Epoch: [32]  [1620/2809]  eta: 0:07:37  lr: 0.000005  min_lr: 0.000000  loss: 3.7367 (3.6719)  loss_scale: 65536.0000 (58359.7878)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [32]  [1630/2809]  eta: 0:07:33  lr: 0.000005  min_lr: 0.000000  loss: 3.7426 (3.6729)  loss_scale: 65536.0000 (58403.7866)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [32]  [1640/2809]  eta: 0:07:29  lr: 0.000005  min_lr: 0.000000  loss: 4.0011 (3.6747)  loss_scale: 65536.0000 (58447.2492)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
[2025-01-13 08:57:07,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:57:07,581] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [32]  [1650/2809]  eta: 0:07:25  lr: 0.000005  min_lr: 0.000000  loss: 3.8777 (3.6748)  loss_scale: 65536.0000 (58728.3537)  weight_decay: 0.0500 (0.0500)  time: 0.3839  data: 0.0003  max mem: 15572
[2025-01-13 08:57:09,908] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91539
[2025-01-13 08:57:09,908] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 08:57:09,908] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [1660/2809]  eta: 0:07:21  lr: 0.000005  min_lr: 0.000000  loss: 3.7448 (3.6754)  loss_scale: 65536.0000 (58769.3390)  weight_decay: 0.0500 (0.0500)  time: 0.3902  data: 0.0004  max mem: 15572
Epoch: [32]  [1670/2809]  eta: 0:07:17  lr: 0.000005  min_lr: 0.000000  loss: 3.7037 (3.6748)  loss_scale: 65536.0000 (58809.8336)  weight_decay: 0.0500 (0.0500)  time: 0.3946  data: 0.0004  max mem: 15572
Epoch: [32]  [1680/2809]  eta: 0:07:14  lr: 0.000005  min_lr: 0.000000  loss: 3.7037 (3.6747)  loss_scale: 65536.0000 (58849.8465)  weight_decay: 0.0500 (0.0500)  time: 0.3931  data: 0.0004  max mem: 15572
Epoch: [32]  [1690/2809]  eta: 0:07:10  lr: 0.000005  min_lr: 0.000000  loss: 3.8782 (3.6755)  loss_scale: 65536.0000 (58889.3862)  weight_decay: 0.0500 (0.0500)  time: 0.3975  data: 0.0005  max mem: 15572
Epoch: [32]  [1700/2809]  eta: 0:07:06  lr: 0.000005  min_lr: 0.000000  loss: 3.8627 (3.6755)  loss_scale: 65536.0000 (58928.4609)  weight_decay: 0.0500 (0.0500)  time: 0.4004  data: 0.0005  max mem: 15572
Epoch: [32]  [1710/2809]  eta: 0:07:02  lr: 0.000005  min_lr: 0.000000  loss: 3.8534 (3.6764)  loss_scale: 65536.0000 (58967.0789)  weight_decay: 0.0500 (0.0500)  time: 0.3898  data: 0.0004  max mem: 15572
Epoch: [32]  [1720/2809]  eta: 0:06:59  lr: 0.000005  min_lr: 0.000000  loss: 3.8287 (3.6764)  loss_scale: 65536.0000 (59005.2481)  weight_decay: 0.0500 (0.0500)  time: 0.3902  data: 0.0004  max mem: 15572
Epoch: [32]  [1730/2809]  eta: 0:06:55  lr: 0.000005  min_lr: 0.000000  loss: 3.8543 (3.6782)  loss_scale: 65536.0000 (59042.9763)  weight_decay: 0.0500 (0.0500)  time: 0.3930  data: 0.0005  max mem: 15572
Epoch: [32]  [1740/2809]  eta: 0:06:51  lr: 0.000005  min_lr: 0.000000  loss: 3.8778 (3.6799)  loss_scale: 65536.0000 (59080.2711)  weight_decay: 0.0500 (0.0500)  time: 0.3928  data: 0.0005  max mem: 15572
[2025-01-13 08:57:48,638] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91637
[2025-01-13 08:57:48,638] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:57:48,638] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [1750/2809]  eta: 0:06:47  lr: 0.000005  min_lr: 0.000000  loss: 3.7587 (3.6791)  loss_scale: 65536.0000 (59079.7122)  weight_decay: 0.0500 (0.0500)  time: 0.3978  data: 0.0005  max mem: 15572
Epoch: [32]  [1760/2809]  eta: 0:06:43  lr: 0.000005  min_lr: 0.000000  loss: 3.5287 (3.6790)  loss_scale: 32768.0000 (58930.2987)  weight_decay: 0.0500 (0.0500)  time: 0.3957  data: 0.0005  max mem: 15572
Epoch: [32]  [1770/2809]  eta: 0:06:40  lr: 0.000005  min_lr: 0.000000  loss: 3.6065 (3.6795)  loss_scale: 32768.0000 (58782.5726)  weight_decay: 0.0500 (0.0500)  time: 0.3892  data: 0.0005  max mem: 15572
Epoch: [32]  [1780/2809]  eta: 0:06:36  lr: 0.000005  min_lr: 0.000000  loss: 3.5959 (3.6791)  loss_scale: 32768.0000 (58636.5053)  weight_decay: 0.0500 (0.0500)  time: 0.3846  data: 0.0004  max mem: 15572
Epoch: [32]  [1790/2809]  eta: 0:06:32  lr: 0.000005  min_lr: 0.000000  loss: 3.5603 (3.6786)  loss_scale: 32768.0000 (58492.0692)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0004  max mem: 15572
Epoch: [32]  [1800/2809]  eta: 0:06:28  lr: 0.000005  min_lr: 0.000000  loss: 3.7654 (3.6797)  loss_scale: 32768.0000 (58349.2371)  weight_decay: 0.0500 (0.0500)  time: 0.3848  data: 0.0005  max mem: 15572
Epoch: [32]  [1810/2809]  eta: 0:06:24  lr: 0.000005  min_lr: 0.000000  loss: 4.0144 (3.6793)  loss_scale: 32768.0000 (58207.9823)  weight_decay: 0.0500 (0.0500)  time: 0.3942  data: 0.0005  max mem: 15572
Epoch: [32]  [1820/2809]  eta: 0:06:20  lr: 0.000005  min_lr: 0.000000  loss: 3.4195 (3.6792)  loss_scale: 32768.0000 (58068.2790)  weight_decay: 0.0500 (0.0500)  time: 0.3980  data: 0.0006  max mem: 15572
Epoch: [32]  [1830/2809]  eta: 0:06:17  lr: 0.000005  min_lr: 0.000000  loss: 3.8644 (3.6802)  loss_scale: 32768.0000 (57930.1016)  weight_decay: 0.0500 (0.0500)  time: 0.3894  data: 0.0005  max mem: 15572
Epoch: [32]  [1840/2809]  eta: 0:06:13  lr: 0.000005  min_lr: 0.000000  loss: 3.7109 (3.6803)  loss_scale: 32768.0000 (57793.4253)  weight_decay: 0.0500 (0.0500)  time: 0.3882  data: 0.0004  max mem: 15572
Epoch: [32]  [1850/2809]  eta: 0:06:09  lr: 0.000005  min_lr: 0.000000  loss: 3.6213 (3.6803)  loss_scale: 32768.0000 (57658.2258)  weight_decay: 0.0500 (0.0500)  time: 0.3829  data: 0.0004  max mem: 15572
Epoch: [32]  [1860/2809]  eta: 0:06:05  lr: 0.000005  min_lr: 0.000000  loss: 3.5646 (3.6790)  loss_scale: 32768.0000 (57524.4793)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
Epoch: [32]  [1870/2809]  eta: 0:06:01  lr: 0.000005  min_lr: 0.000000  loss: 3.6401 (3.6801)  loss_scale: 32768.0000 (57392.1625)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0014  max mem: 15572
[2025-01-13 08:58:38,281] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:58:38,282] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1880/2809]  eta: 0:05:57  lr: 0.000005  min_lr: 0.000000  loss: 3.6986 (3.6789)  loss_scale: 32768.0000 (57313.5141)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0014  max mem: 15572
Epoch: [32]  [1890/2809]  eta: 0:05:53  lr: 0.000005  min_lr: 0.000000  loss: 3.4115 (3.6782)  loss_scale: 65536.0000 (57356.9963)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [32]  [1900/2809]  eta: 0:05:49  lr: 0.000005  min_lr: 0.000000  loss: 3.6820 (3.6791)  loss_scale: 65536.0000 (57400.0210)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [32]  [1910/2809]  eta: 0:05:45  lr: 0.000005  min_lr: 0.000000  loss: 3.7382 (3.6777)  loss_scale: 65536.0000 (57442.5955)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
[2025-01-13 08:58:51,969] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91803
[2025-01-13 08:58:51,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 08:58:51,969] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [1920/2809]  eta: 0:05:41  lr: 0.000005  min_lr: 0.000000  loss: 3.5585 (3.6770)  loss_scale: 65536.0000 (57382.3800)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0003  max mem: 15572
Epoch: [32]  [1930/2809]  eta: 0:05:37  lr: 0.000005  min_lr: 0.000000  loss: 3.5199 (3.6763)  loss_scale: 32768.0000 (57254.9104)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0003  max mem: 15572
Epoch: [32]  [1940/2809]  eta: 0:05:34  lr: 0.000005  min_lr: 0.000000  loss: 3.6848 (3.6771)  loss_scale: 32768.0000 (57128.7543)  weight_decay: 0.0500 (0.0500)  time: 0.3853  data: 0.0005  max mem: 15572
Epoch: [32]  [1950/2809]  eta: 0:05:30  lr: 0.000005  min_lr: 0.000000  loss: 3.6848 (3.6769)  loss_scale: 32768.0000 (57003.8913)  weight_decay: 0.0500 (0.0500)  time: 0.3932  data: 0.0006  max mem: 15572
Epoch: [32]  [1960/2809]  eta: 0:05:26  lr: 0.000005  min_lr: 0.000000  loss: 3.7018 (3.6766)  loss_scale: 32768.0000 (56880.3019)  weight_decay: 0.0500 (0.0500)  time: 0.3796  data: 0.0004  max mem: 15572
Epoch: [32]  [1970/2809]  eta: 0:05:22  lr: 0.000005  min_lr: 0.000000  loss: 3.5685 (3.6754)  loss_scale: 32768.0000 (56757.9665)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0004  max mem: 15572
Epoch: [32]  [1980/2809]  eta: 0:05:18  lr: 0.000005  min_lr: 0.000000  loss: 3.5073 (3.6743)  loss_scale: 32768.0000 (56636.8662)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0002  max mem: 15572
Epoch: [32]  [1990/2809]  eta: 0:05:14  lr: 0.000005  min_lr: 0.000000  loss: 3.5073 (3.6736)  loss_scale: 32768.0000 (56516.9824)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [32]  [2000/2809]  eta: 0:05:10  lr: 0.000005  min_lr: 0.000000  loss: 3.7127 (3.6742)  loss_scale: 32768.0000 (56398.2969)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [32]  [2010/2809]  eta: 0:05:07  lr: 0.000005  min_lr: 0.000000  loss: 3.7411 (3.6750)  loss_scale: 32768.0000 (56280.7916)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [32]  [2020/2809]  eta: 0:05:03  lr: 0.000005  min_lr: 0.000000  loss: 3.7759 (3.6751)  loss_scale: 32768.0000 (56164.4493)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [32]  [2030/2809]  eta: 0:04:59  lr: 0.000005  min_lr: 0.000000  loss: 3.6452 (3.6749)  loss_scale: 32768.0000 (56049.2526)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [32]  [2040/2809]  eta: 0:04:55  lr: 0.000005  min_lr: 0.000000  loss: 3.3968 (3.6739)  loss_scale: 32768.0000 (55935.1847)  weight_decay: 0.0500 (0.0500)  time: 0.3796  data: 0.0002  max mem: 15572
[2025-01-13 08:59:40,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 08:59:40,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [2050/2809]  eta: 0:04:51  lr: 0.000005  min_lr: 0.000000  loss: 3.7669 (3.6750)  loss_scale: 32768.0000 (55934.0653)  weight_decay: 0.0500 (0.0500)  time: 0.3838  data: 0.0004  max mem: 15572
Epoch: [32]  [2060/2809]  eta: 0:04:47  lr: 0.000005  min_lr: 0.000000  loss: 3.8922 (3.6747)  loss_scale: 65536.0000 (55980.6541)  weight_decay: 0.0500 (0.0500)  time: 0.3886  data: 0.0004  max mem: 15572
Epoch: [32]  [2070/2809]  eta: 0:04:43  lr: 0.000005  min_lr: 0.000000  loss: 3.5943 (3.6744)  loss_scale: 65536.0000 (56026.7929)  weight_decay: 0.0500 (0.0500)  time: 0.3953  data: 0.0004  max mem: 15572
Epoch: [32]  [2080/2809]  eta: 0:04:40  lr: 0.000005  min_lr: 0.000000  loss: 3.6758 (3.6742)  loss_scale: 65536.0000 (56072.4882)  weight_decay: 0.0500 (0.0500)  time: 0.3907  data: 0.0005  max mem: 15572
Epoch: [32]  [2090/2809]  eta: 0:04:36  lr: 0.000005  min_lr: 0.000000  loss: 3.7020 (3.6741)  loss_scale: 65536.0000 (56117.7465)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0004  max mem: 15572
Epoch: [32]  [2100/2809]  eta: 0:04:32  lr: 0.000005  min_lr: 0.000000  loss: 3.7784 (3.6751)  loss_scale: 65536.0000 (56162.5740)  weight_decay: 0.0500 (0.0500)  time: 0.3907  data: 0.0004  max mem: 15572
Epoch: [32]  [2110/2809]  eta: 0:04:28  lr: 0.000005  min_lr: 0.000000  loss: 3.7529 (3.6747)  loss_scale: 65536.0000 (56206.9768)  weight_decay: 0.0500 (0.0500)  time: 0.3845  data: 0.0005  max mem: 15572
[2025-01-13 09:00:06,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=92000, skipped=624, lr=[4.679941201088924e-08, 4.679941201088924e-08, 6.685630287269892e-08, 6.685630287269892e-08, 9.550900410385561e-08, 9.550900410385561e-08, 1.3644143443407946e-07, 1.3644143443407946e-07, 1.949163349058278e-07, 1.949163349058278e-07, 2.784519070083254e-07, 2.784519070083254e-07, 3.9778843858332207e-07, 3.9778843858332207e-07, 5.682691979761744e-07, 5.682691979761744e-07, 8.118131399659635e-07, 8.118131399659635e-07, 1.1597330570942337e-06, 1.1597330570942337e-06, 1.6567615101346195e-06, 1.6567615101346195e-06, 2.366802157335171e-06, 2.366802157335171e-06, 3.3811459390502447e-06, 3.3811459390502447e-06, 4.830208484357493e-06, 4.830208484357493e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 09:00:06,733] [INFO] [timer.py:260:stop] epoch=0/micro_step=92000/global_step=92000, RunningAvgSamplesPerSec=31.189354732522446, CurrSamplesPerSec=33.06159384114139, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [32]  [2120/2809]  eta: 0:04:24  lr: 0.000005  min_lr: 0.000000  loss: 3.6725 (3.6753)  loss_scale: 65536.0000 (56250.9609)  weight_decay: 0.0500 (0.0500)  time: 0.3872  data: 0.0004  max mem: 15572
Epoch: [32]  [2130/2809]  eta: 0:04:21  lr: 0.000005  min_lr: 0.000000  loss: 3.8653 (3.6760)  loss_scale: 65536.0000 (56294.5321)  weight_decay: 0.0500 (0.0500)  time: 0.3930  data: 0.0004  max mem: 15572
[2025-01-13 09:00:15,765] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92022
[2025-01-13 09:00:15,765] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:00:15,766] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [2140/2809]  eta: 0:04:17  lr: 0.000005  min_lr: 0.000000  loss: 3.5605 (3.6748)  loss_scale: 65536.0000 (56230.5614)  weight_decay: 0.0500 (0.0500)  time: 0.3903  data: 0.0005  max mem: 15572
Epoch: [32]  [2150/2809]  eta: 0:04:13  lr: 0.000005  min_lr: 0.000000  loss: 3.4446 (3.6741)  loss_scale: 32768.0000 (56121.4840)  weight_decay: 0.0500 (0.0500)  time: 0.3837  data: 0.0005  max mem: 15572
Epoch: [32]  [2160/2809]  eta: 0:04:09  lr: 0.000005  min_lr: 0.000000  loss: 3.7993 (3.6744)  loss_scale: 32768.0000 (56013.4160)  weight_decay: 0.0500 (0.0500)  time: 0.3808  data: 0.0004  max mem: 15572
Epoch: [32]  [2170/2809]  eta: 0:04:05  lr: 0.000005  min_lr: 0.000000  loss: 3.7868 (3.6740)  loss_scale: 32768.0000 (55906.3436)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0003  max mem: 15572
Epoch: [32]  [2180/2809]  eta: 0:04:01  lr: 0.000005  min_lr: 0.000000  loss: 3.7168 (3.6744)  loss_scale: 32768.0000 (55800.2531)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [32]  [2190/2809]  eta: 0:03:57  lr: 0.000005  min_lr: 0.000000  loss: 3.8494 (3.6747)  loss_scale: 32768.0000 (55695.1310)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0003  max mem: 15572
Epoch: [32]  [2200/2809]  eta: 0:03:53  lr: 0.000005  min_lr: 0.000000  loss: 3.7807 (3.6754)  loss_scale: 32768.0000 (55590.9641)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [32]  [2210/2809]  eta: 0:03:50  lr: 0.000005  min_lr: 0.000000  loss: 3.5522 (3.6736)  loss_scale: 32768.0000 (55487.7395)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0003  max mem: 15572
Epoch: [32]  [2220/2809]  eta: 0:03:46  lr: 0.000005  min_lr: 0.000000  loss: 3.4852 (3.6731)  loss_scale: 32768.0000 (55385.4444)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
Epoch: [32]  [2230/2809]  eta: 0:03:42  lr: 0.000005  min_lr: 0.000000  loss: 3.6732 (3.6729)  loss_scale: 32768.0000 (55284.0663)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0003  max mem: 15572
Epoch: [32]  [2240/2809]  eta: 0:03:38  lr: 0.000005  min_lr: 0.000000  loss: 3.5895 (3.6721)  loss_scale: 32768.0000 (55183.5930)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0003  max mem: 15572
Epoch: [32]  [2250/2809]  eta: 0:03:34  lr: 0.000005  min_lr: 0.000000  loss: 3.6664 (3.6726)  loss_scale: 32768.0000 (55084.0124)  weight_decay: 0.0500 (0.0500)  time: 0.3845  data: 0.0003  max mem: 15572
Epoch: [32]  [2260/2809]  eta: 0:03:30  lr: 0.000005  min_lr: 0.000000  loss: 3.5069 (3.6716)  loss_scale: 32768.0000 (54985.3127)  weight_decay: 0.0500 (0.0500)  time: 0.3916  data: 0.0004  max mem: 15572
[2025-01-13 09:01:04,555] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:01:04,555] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [2270/2809]  eta: 0:03:27  lr: 0.000005  min_lr: 0.000000  loss: 3.7314 (3.6729)  loss_scale: 32768.0000 (55002.9133)  weight_decay: 0.0500 (0.0500)  time: 0.3922  data: 0.0005  max mem: 15572
Epoch: [32]  [2280/2809]  eta: 0:03:23  lr: 0.000005  min_lr: 0.000000  loss: 3.8451 (3.6726)  loss_scale: 65536.0000 (55049.0907)  weight_decay: 0.0500 (0.0500)  time: 0.3936  data: 0.0006  max mem: 15572
Epoch: [32]  [2290/2809]  eta: 0:03:19  lr: 0.000005  min_lr: 0.000000  loss: 3.7731 (3.6728)  loss_scale: 65536.0000 (55094.8651)  weight_decay: 0.0500 (0.0500)  time: 0.3933  data: 0.0005  max mem: 15572
Epoch: [32]  [2300/2809]  eta: 0:03:15  lr: 0.000005  min_lr: 0.000000  loss: 3.6129 (3.6724)  loss_scale: 65536.0000 (55140.2416)  weight_decay: 0.0500 (0.0500)  time: 0.3899  data: 0.0004  max mem: 15572
Epoch: [32]  [2310/2809]  eta: 0:03:11  lr: 0.000005  min_lr: 0.000000  loss: 3.6891 (3.6730)  loss_scale: 65536.0000 (55185.2254)  weight_decay: 0.0500 (0.0500)  time: 0.3850  data: 0.0004  max mem: 15572
Epoch: [32]  [2320/2809]  eta: 0:03:07  lr: 0.000005  min_lr: 0.000000  loss: 3.7110 (3.6733)  loss_scale: 65536.0000 (55229.8216)  weight_decay: 0.0500 (0.0500)  time: 0.3787  data: 0.0003  max mem: 15572
Epoch: [32]  [2330/2809]  eta: 0:03:03  lr: 0.000005  min_lr: 0.000000  loss: 3.8886 (3.6743)  loss_scale: 65536.0000 (55274.0352)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [32]  [2340/2809]  eta: 0:03:00  lr: 0.000005  min_lr: 0.000000  loss: 3.8886 (3.6746)  loss_scale: 65536.0000 (55317.8710)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [32]  [2350/2809]  eta: 0:02:56  lr: 0.000005  min_lr: 0.000000  loss: 3.7276 (3.6750)  loss_scale: 65536.0000 (55361.3339)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [32]  [2360/2809]  eta: 0:02:52  lr: 0.000005  min_lr: 0.000000  loss: 3.6898 (3.6755)  loss_scale: 65536.0000 (55404.4286)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [32]  [2370/2809]  eta: 0:02:48  lr: 0.000005  min_lr: 0.000000  loss: 3.8356 (3.6755)  loss_scale: 65536.0000 (55447.1598)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [32]  [2380/2809]  eta: 0:02:44  lr: 0.000005  min_lr: 0.000000  loss: 3.7319 (3.6759)  loss_scale: 65536.0000 (55489.5321)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [32]  [2390/2809]  eta: 0:02:40  lr: 0.000005  min_lr: 0.000000  loss: 3.6602 (3.6762)  loss_scale: 65536.0000 (55531.5500)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
[2025-01-13 09:01:52,854] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:01:52,854] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:01:53,577] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92281
[2025-01-13 09:01:53,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:01:53,577] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [2400/2809]  eta: 0:02:36  lr: 0.000005  min_lr: 0.000000  loss: 3.6008 (3.6750)  loss_scale: 65536.0000 (55627.8084)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0003  max mem: 15572
Epoch: [32]  [2410/2809]  eta: 0:02:33  lr: 0.000005  min_lr: 0.000000  loss: 3.6008 (3.6752)  loss_scale: 65536.0000 (55668.9042)  weight_decay: 0.0500 (0.0500)  time: 0.3805  data: 0.0004  max mem: 15572
Epoch: [32]  [2420/2809]  eta: 0:02:29  lr: 0.000005  min_lr: 0.000000  loss: 3.8339 (3.6759)  loss_scale: 65536.0000 (55709.6605)  weight_decay: 0.0500 (0.0500)  time: 0.3847  data: 0.0004  max mem: 15572
Epoch: [32]  [2430/2809]  eta: 0:02:25  lr: 0.000005  min_lr: 0.000000  loss: 3.8339 (3.6763)  loss_scale: 65536.0000 (55750.0814)  weight_decay: 0.0500 (0.0500)  time: 0.3876  data: 0.0004  max mem: 15572
Epoch: [32]  [2440/2809]  eta: 0:02:21  lr: 0.000005  min_lr: 0.000000  loss: 3.7247 (3.6766)  loss_scale: 65536.0000 (55790.1712)  weight_decay: 0.0500 (0.0500)  time: 0.3937  data: 0.0005  max mem: 15572
Epoch: [32]  [2450/2809]  eta: 0:02:17  lr: 0.000005  min_lr: 0.000000  loss: 3.7247 (3.6770)  loss_scale: 65536.0000 (55829.9339)  weight_decay: 0.0500 (0.0500)  time: 0.3892  data: 0.0005  max mem: 15572
Epoch: [32]  [2460/2809]  eta: 0:02:13  lr: 0.000005  min_lr: 0.000000  loss: 3.7739 (3.6768)  loss_scale: 65536.0000 (55869.3734)  weight_decay: 0.0500 (0.0500)  time: 0.3869  data: 0.0004  max mem: 15572
Epoch: [32]  [2470/2809]  eta: 0:02:10  lr: 0.000005  min_lr: 0.000000  loss: 3.6508 (3.6768)  loss_scale: 65536.0000 (55908.4937)  weight_decay: 0.0500 (0.0500)  time: 0.3867  data: 0.0004  max mem: 15572
Epoch: [32]  [2480/2809]  eta: 0:02:06  lr: 0.000005  min_lr: 0.000000  loss: 3.6776 (3.6768)  loss_scale: 65536.0000 (55947.2987)  weight_decay: 0.0500 (0.0500)  time: 0.3793  data: 0.0004  max mem: 15572
Epoch: [32]  [2490/2809]  eta: 0:02:02  lr: 0.000005  min_lr: 0.000000  loss: 3.4349 (3.6767)  loss_scale: 65536.0000 (55985.7921)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [32]  [2500/2809]  eta: 0:01:58  lr: 0.000005  min_lr: 0.000000  loss: 3.4349 (3.6755)  loss_scale: 65536.0000 (56023.9776)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [32]  [2510/2809]  eta: 0:01:54  lr: 0.000005  min_lr: 0.000000  loss: 3.4778 (3.6754)  loss_scale: 65536.0000 (56061.8590)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [32]  [2520/2809]  eta: 0:01:50  lr: 0.000005  min_lr: 0.000000  loss: 3.5122 (3.6758)  loss_scale: 65536.0000 (56099.4399)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
[2025-01-13 09:02:42,760] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:02:42,760] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:02:44,244] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92414
[2025-01-13 09:02:44,244] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:02:44,244] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [2530/2809]  eta: 0:01:46  lr: 0.000005  min_lr: 0.000000  loss: 3.5329 (3.6744)  loss_scale: 65536.0000 (56240.2971)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [32]  [2540/2809]  eta: 0:01:43  lr: 0.000005  min_lr: 0.000000  loss: 3.5329 (3.6742)  loss_scale: 65536.0000 (56276.8800)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0003  max mem: 15572
Epoch: [32]  [2550/2809]  eta: 0:01:39  lr: 0.000005  min_lr: 0.000000  loss: 3.5622 (3.6736)  loss_scale: 65536.0000 (56313.1760)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [32]  [2560/2809]  eta: 0:01:35  lr: 0.000005  min_lr: 0.000000  loss: 3.4999 (3.6736)  loss_scale: 65536.0000 (56349.1886)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0004  max mem: 15572
Epoch: [32]  [2570/2809]  eta: 0:01:31  lr: 0.000005  min_lr: 0.000000  loss: 3.8269 (3.6741)  loss_scale: 65536.0000 (56384.9210)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0005  max mem: 15572
Epoch: [32]  [2580/2809]  eta: 0:01:27  lr: 0.000005  min_lr: 0.000000  loss: 3.8523 (3.6741)  loss_scale: 65536.0000 (56420.3766)  weight_decay: 0.0500 (0.0500)  time: 0.3906  data: 0.0006  max mem: 15572
Epoch: [32]  [2590/2809]  eta: 0:01:23  lr: 0.000005  min_lr: 0.000000  loss: 3.8468 (3.6742)  loss_scale: 65536.0000 (56455.5585)  weight_decay: 0.0500 (0.0500)  time: 0.3916  data: 0.0006  max mem: 15572
Epoch: [32]  [2600/2809]  eta: 0:01:20  lr: 0.000005  min_lr: 0.000000  loss: 3.7055 (3.6739)  loss_scale: 65536.0000 (56490.4698)  weight_decay: 0.0500 (0.0500)  time: 0.3877  data: 0.0004  max mem: 15572
Epoch: [32]  [2610/2809]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000000  loss: 3.5617 (3.6732)  loss_scale: 65536.0000 (56525.1137)  weight_decay: 0.0500 (0.0500)  time: 0.3864  data: 0.0004  max mem: 15572
Epoch: [32]  [2620/2809]  eta: 0:01:12  lr: 0.000005  min_lr: 0.000000  loss: 3.6527 (3.6732)  loss_scale: 65536.0000 (56559.4933)  weight_decay: 0.0500 (0.0500)  time: 0.3954  data: 0.0004  max mem: 15572
Epoch: [32]  [2630/2809]  eta: 0:01:08  lr: 0.000005  min_lr: 0.000000  loss: 3.8911 (3.6741)  loss_scale: 65536.0000 (56593.6116)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0003  max mem: 15572
Epoch: [32]  [2640/2809]  eta: 0:01:04  lr: 0.000005  min_lr: 0.000000  loss: 3.8213 (3.6733)  loss_scale: 65536.0000 (56627.4714)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [32]  [2650/2809]  eta: 0:01:00  lr: 0.000005  min_lr: 0.000000  loss: 3.7631 (3.6741)  loss_scale: 65536.0000 (56661.0758)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0003  max mem: 15572
[2025-01-13 09:03:33,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:03:33,405] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:03:33,770] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92544
[2025-01-13 09:03:33,770] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:03:33,770] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [2660/2809]  eta: 0:00:57  lr: 0.000005  min_lr: 0.000000  loss: 3.7631 (3.6739)  loss_scale: 65536.0000 (56719.0560)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
Epoch: [32]  [2670/2809]  eta: 0:00:53  lr: 0.000005  min_lr: 0.000000  loss: 3.5848 (3.6737)  loss_scale: 65536.0000 (56752.0659)  weight_decay: 0.0500 (0.0500)  time: 0.3841  data: 0.0005  max mem: 15572
Epoch: [32]  [2680/2809]  eta: 0:00:49  lr: 0.000005  min_lr: 0.000000  loss: 3.8858 (3.6753)  loss_scale: 65536.0000 (56784.8295)  weight_decay: 0.0500 (0.0500)  time: 0.3849  data: 0.0006  max mem: 15572
Epoch: [32]  [2690/2809]  eta: 0:00:45  lr: 0.000005  min_lr: 0.000000  loss: 3.7546 (3.6744)  loss_scale: 65536.0000 (56817.3497)  weight_decay: 0.0500 (0.0500)  time: 0.3836  data: 0.0004  max mem: 15572
Epoch: [32]  [2700/2809]  eta: 0:00:41  lr: 0.000005  min_lr: 0.000000  loss: 3.4935 (3.6743)  loss_scale: 65536.0000 (56849.6290)  weight_decay: 0.0500 (0.0500)  time: 0.3856  data: 0.0004  max mem: 15572
Epoch: [32]  [2710/2809]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 3.5288 (3.6739)  loss_scale: 65536.0000 (56881.6702)  weight_decay: 0.0500 (0.0500)  time: 0.3814  data: 0.0004  max mem: 15572
Epoch: [32]  [2720/2809]  eta: 0:00:34  lr: 0.000005  min_lr: 0.000000  loss: 3.4808 (3.6739)  loss_scale: 65536.0000 (56913.4759)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
Epoch: [32]  [2730/2809]  eta: 0:00:30  lr: 0.000005  min_lr: 0.000000  loss: 3.5597 (3.6739)  loss_scale: 65536.0000 (56945.0487)  weight_decay: 0.0500 (0.0500)  time: 0.3810  data: 0.0004  max mem: 15572
Epoch: [32]  [2740/2809]  eta: 0:00:26  lr: 0.000005  min_lr: 0.000000  loss: 3.6736 (3.6740)  loss_scale: 65536.0000 (56976.3911)  weight_decay: 0.0500 (0.0500)  time: 0.3898  data: 0.0005  max mem: 15572
Epoch: [32]  [2750/2809]  eta: 0:00:22  lr: 0.000005  min_lr: 0.000000  loss: 3.6840 (3.6742)  loss_scale: 65536.0000 (57007.5056)  weight_decay: 0.0500 (0.0500)  time: 0.3939  data: 0.0006  max mem: 15572
Epoch: [32]  [2760/2809]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 3.7642 (3.6748)  loss_scale: 65536.0000 (57038.3948)  weight_decay: 0.0500 (0.0500)  time: 0.3944  data: 0.0006  max mem: 15572
Epoch: [32]  [2770/2809]  eta: 0:00:14  lr: 0.000005  min_lr: 0.000000  loss: 3.8123 (3.6752)  loss_scale: 65536.0000 (57069.0610)  weight_decay: 0.0500 (0.0500)  time: 0.3892  data: 0.0004  max mem: 15572
Epoch: [32]  [2780/2809]  eta: 0:00:11  lr: 0.000005  min_lr: 0.000000  loss: 3.9277 (3.6760)  loss_scale: 65536.0000 (57099.5067)  weight_decay: 0.0500 (0.0500)  time: 0.3870  data: 0.0004  max mem: 15572
[2025-01-13 09:04:23,651] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:04:23,651] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:04:24,017] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92674
[2025-01-13 09:04:24,017] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:04:24,017] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [2790/2809]  eta: 0:00:07  lr: 0.000005  min_lr: 0.000000  loss: 3.8720 (3.6755)  loss_scale: 65536.0000 (57153.2153)  weight_decay: 0.0500 (0.0500)  time: 0.3921  data: 0.0005  max mem: 15572
Epoch: [32]  [2800/2809]  eta: 0:00:03  lr: 0.000005  min_lr: 0.000000  loss: 3.2524 (3.6753)  loss_scale: 65536.0000 (57183.1432)  weight_decay: 0.0500 (0.0500)  time: 0.3854  data: 0.0003  max mem: 15572
Epoch: [32]  [2808/2809]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 3.6899 (3.6758)  loss_scale: 65536.0000 (57206.9320)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0003  max mem: 15572
Epoch: [32] Total time: 0:17:57 (0.3837 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 3.6899 (3.6758)  loss_scale: 65536.0000 (57206.9320)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:13:40  loss: 0.3695 (0.3695)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.0151  data: 2.7921  max mem: 15572
Val:  [ 10/272]  eta: 0:02:02  loss: 2.2967 (2.2468)  acc1: 50.0000 (45.9596)  acc5: 77.7778 (75.2525)  time: 0.4671  data: 0.2875  max mem: 15572
Val:  [ 20/272]  eta: 0:01:24  loss: 2.2963 (2.2741)  acc1: 50.0000 (47.8836)  acc5: 72.2222 (75.1323)  time: 0.2030  data: 0.0379  max mem: 15572
Val:  [ 30/272]  eta: 0:01:08  loss: 2.2963 (2.3689)  acc1: 50.0000 (44.2652)  acc5: 72.2222 (74.3728)  time: 0.1805  data: 0.0197  max mem: 15572
Val:  [ 40/272]  eta: 0:00:59  loss: 2.5199 (2.4305)  acc1: 27.7778 (41.3279)  acc5: 77.7778 (73.9837)  time: 0.1684  data: 0.0006  max mem: 15572
Val:  [ 50/272]  eta: 0:00:52  loss: 2.4232 (2.3583)  acc1: 33.3333 (43.1373)  acc5: 77.7778 (75.1634)  time: 0.1689  data: 0.0006  max mem: 15572
Val:  [ 60/272]  eta: 0:00:48  loss: 1.5007 (2.2541)  acc1: 61.1111 (46.1749)  acc5: 83.3333 (76.2295)  time: 0.1711  data: 0.0006  max mem: 15572
Val:  [ 70/272]  eta: 0:00:44  loss: 1.5007 (2.1772)  acc1: 66.6667 (48.5133)  acc5: 83.3333 (77.2300)  time: 0.1722  data: 0.0006  max mem: 15572
Val:  [ 80/272]  eta: 0:00:40  loss: 1.8046 (2.1887)  acc1: 55.5556 (48.2853)  acc5: 77.7778 (77.0233)  time: 0.1645  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:37  loss: 2.1214 (2.1956)  acc1: 50.0000 (48.2906)  acc5: 77.7778 (77.7167)  time: 0.1628  data: 0.0006  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 2.1015 (2.2182)  acc1: 50.0000 (47.7448)  acc5: 83.3333 (77.2827)  time: 0.1645  data: 0.0005  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 2.3961 (2.2930)  acc1: 22.2222 (45.4955)  acc5: 72.2222 (76.3764)  time: 0.1773  data: 0.0006  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 2.8591 (2.3284)  acc1: 22.2222 (44.6740)  acc5: 72.2222 (75.9412)  time: 0.1785  data: 0.0006  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 2.0933 (2.2934)  acc1: 44.4444 (45.5047)  acc5: 83.3333 (76.6327)  time: 0.1582  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 1.7455 (2.2850)  acc1: 55.5556 (45.9417)  acc5: 83.3333 (76.5169)  time: 0.1542  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 2.3850 (2.2903)  acc1: 38.8889 (45.4010)  acc5: 77.7778 (76.6740)  time: 0.1850  data: 0.0331  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 2.2988 (2.2782)  acc1: 44.4444 (46.1353)  acc5: 77.7778 (76.8806)  time: 0.1862  data: 0.0331  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 2.3171 (2.2956)  acc1: 44.4444 (45.7440)  acc5: 72.2222 (76.4457)  time: 0.1625  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 2.3014 (2.2861)  acc1: 38.8889 (45.6415)  acc5: 77.7778 (76.7649)  time: 0.1732  data: 0.0007  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 2.3014 (2.3389)  acc1: 33.3333 (44.3281)  acc5: 72.2222 (75.3054)  time: 0.1773  data: 0.0007  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 2.5833 (2.3485)  acc1: 33.3333 (44.0299)  acc5: 66.6667 (75.0138)  time: 0.1715  data: 0.0006  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.1270 (2.3516)  acc1: 44.4444 (44.2075)  acc5: 83.3333 (74.9868)  time: 0.1694  data: 0.0006  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 2.1270 (2.3386)  acc1: 50.0000 (44.4193)  acc5: 83.3333 (75.2388)  time: 0.1736  data: 0.0005  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.7738 (2.3071)  acc1: 61.1111 (45.4545)  acc5: 83.3333 (75.6133)  time: 0.1706  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.6337 (2.2931)  acc1: 61.1111 (45.6662)  acc5: 83.3333 (75.7492)  time: 0.1694  data: 0.0005  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 2.3598 (2.3040)  acc1: 44.4444 (45.0199)  acc5: 77.7778 (75.7193)  time: 0.1691  data: 0.0005  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.1898 (2.2474)  acc1: 72.2222 (46.6794)  acc5: 88.8889 (76.4155)  time: 0.1566  data: 0.0004  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3746 (2.2437)  acc1: 72.2222 (46.7200)  acc5: 88.8889 (76.5888)  time: 0.1441  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3746 (2.2488)  acc1: 66.6667 (46.6926)  acc5: 88.8889 (76.5513)  time: 0.1387  data: 0.0001  max mem: 15572
Val: Total time: 0:00:49 (0.1815 s / it)
* Acc@1 46.693 Acc@5 76.551 loss 2.249
Accuracy of the network on the 4883 val videos: 46.7%
[2025-01-13 09:05:21,895] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 09:05:21,898] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 09:05:21,899] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 09:05:24,810] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 09:05:24,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.69%
Epoch: [33]  [   0/2809]  eta: 3:12:01  lr: 0.000005  min_lr: 0.000000  loss: 3.5474 (3.5474)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.1017  data: 3.6964  max mem: 15572
Epoch: [33]  [  10/2809]  eta: 0:34:59  lr: 0.000005  min_lr: 0.000000  loss: 3.5474 (3.6873)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7502  data: 0.3656  max mem: 15572
Epoch: [33]  [  20/2809]  eta: 0:26:37  lr: 0.000005  min_lr: 0.000000  loss: 3.6803 (3.7561)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3964  data: 0.0164  max mem: 15572
Epoch: [33]  [  30/2809]  eta: 0:23:26  lr: 0.000005  min_lr: 0.000000  loss: 3.7157 (3.7920)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
Epoch: [33]  [  40/2809]  eta: 0:21:49  lr: 0.000005  min_lr: 0.000000  loss: 3.7157 (3.7272)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0002  max mem: 15572
Epoch: [33]  [  50/2809]  eta: 0:20:46  lr: 0.000004  min_lr: 0.000000  loss: 3.5791 (3.7197)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [33]  [  60/2809]  eta: 0:20:06  lr: 0.000004  min_lr: 0.000000  loss: 3.7133 (3.7378)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [33]  [  70/2809]  eta: 0:19:37  lr: 0.000004  min_lr: 0.000000  loss: 3.8415 (3.7161)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [33]  [  80/2809]  eta: 0:19:18  lr: 0.000004  min_lr: 0.000000  loss: 3.6970 (3.7087)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3804  data: 0.0003  max mem: 15572
Epoch: [33]  [  90/2809]  eta: 0:19:02  lr: 0.000004  min_lr: 0.000000  loss: 3.6687 (3.6995)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3864  data: 0.0003  max mem: 15572
Epoch: [33]  [ 100/2809]  eta: 0:18:51  lr: 0.000004  min_lr: 0.000000  loss: 3.6355 (3.6783)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3908  data: 0.0004  max mem: 15572
[2025-01-13 09:06:09,445] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:06:09,445] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [33]  [ 110/2809]  eta: 0:18:39  lr: 0.000004  min_lr: 0.000000  loss: 3.5394 (3.6775)  loss_scale: 65536.0000 (68488.0721)  weight_decay: 0.0500 (0.0500)  time: 0.3895  data: 0.0005  max mem: 15572
[2025-01-13 09:06:11,677] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92809
[2025-01-13 09:06:11,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:06:11,677] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 120/2809]  eta: 0:18:29  lr: 0.000004  min_lr: 0.000000  loss: 3.5203 (3.6653)  loss_scale: 65536.0000 (68785.7190)  weight_decay: 0.0500 (0.0500)  time: 0.3855  data: 0.0005  max mem: 15572
Epoch: [33]  [ 130/2809]  eta: 0:18:19  lr: 0.000004  min_lr: 0.000000  loss: 3.6936 (3.6678)  loss_scale: 65536.0000 (68537.6489)  weight_decay: 0.0500 (0.0500)  time: 0.3873  data: 0.0004  max mem: 15572
Epoch: [33]  [ 140/2809]  eta: 0:18:12  lr: 0.000004  min_lr: 0.000000  loss: 3.7278 (3.6624)  loss_scale: 65536.0000 (68324.7660)  weight_decay: 0.0500 (0.0500)  time: 0.3912  data: 0.0005  max mem: 15572
Epoch: [33]  [ 150/2809]  eta: 0:18:04  lr: 0.000004  min_lr: 0.000000  loss: 3.7278 (3.6686)  loss_scale: 65536.0000 (68140.0795)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0004  max mem: 15572
Epoch: [33]  [ 160/2809]  eta: 0:17:57  lr: 0.000004  min_lr: 0.000000  loss: 3.6833 (3.6651)  loss_scale: 65536.0000 (67978.3354)  weight_decay: 0.0500 (0.0500)  time: 0.3862  data: 0.0005  max mem: 15572
Epoch: [33]  [ 170/2809]  eta: 0:17:50  lr: 0.000004  min_lr: 0.000000  loss: 3.7501 (3.6653)  loss_scale: 65536.0000 (67835.5088)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0005  max mem: 15572
Epoch: [33]  [ 180/2809]  eta: 0:17:43  lr: 0.000004  min_lr: 0.000000  loss: 3.7615 (3.6582)  loss_scale: 65536.0000 (67708.4641)  weight_decay: 0.0500 (0.0500)  time: 0.3887  data: 0.0005  max mem: 15572
Epoch: [33]  [ 190/2809]  eta: 0:17:37  lr: 0.000004  min_lr: 0.000000  loss: 3.7280 (3.6587)  loss_scale: 65536.0000 (67594.7225)  weight_decay: 0.0500 (0.0500)  time: 0.3875  data: 0.0004  max mem: 15572
Epoch: [33]  [ 200/2809]  eta: 0:17:29  lr: 0.000004  min_lr: 0.000000  loss: 3.7424 (3.6606)  loss_scale: 65536.0000 (67492.2985)  weight_decay: 0.0500 (0.0500)  time: 0.3803  data: 0.0003  max mem: 15572
Epoch: [33]  [ 210/2809]  eta: 0:17:21  lr: 0.000004  min_lr: 0.000000  loss: 3.5287 (3.6486)  loss_scale: 65536.0000 (67399.5829)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [33]  [ 220/2809]  eta: 0:17:13  lr: 0.000004  min_lr: 0.000000  loss: 3.4640 (3.6501)  loss_scale: 65536.0000 (67315.2579)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [33]  [ 230/2809]  eta: 0:17:06  lr: 0.000004  min_lr: 0.000000  loss: 3.6804 (3.6528)  loss_scale: 65536.0000 (67238.2338)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [33]  [ 240/2809]  eta: 0:16:59  lr: 0.000004  min_lr: 0.000000  loss: 3.7839 (3.6529)  loss_scale: 65536.0000 (67167.6017)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
[2025-01-13 09:07:00,916] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:07:00,917] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:07:02,375] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92942
[2025-01-13 09:07:02,375] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:07:02,376] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 250/2809]  eta: 0:16:52  lr: 0.000004  min_lr: 0.000000  loss: 3.8593 (3.6669)  loss_scale: 65536.0000 (68146.9960)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [33]  [ 260/2809]  eta: 0:16:45  lr: 0.000004  min_lr: 0.000000  loss: 3.9456 (3.6698)  loss_scale: 65536.0000 (68046.9579)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [33]  [ 270/2809]  eta: 0:16:39  lr: 0.000004  min_lr: 0.000000  loss: 3.8965 (3.6746)  loss_scale: 65536.0000 (67954.3026)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [33]  [ 280/2809]  eta: 0:16:33  lr: 0.000004  min_lr: 0.000000  loss: 3.7767 (3.6701)  loss_scale: 65536.0000 (67868.2420)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [33]  [ 290/2809]  eta: 0:16:28  lr: 0.000004  min_lr: 0.000000  loss: 3.7671 (3.6732)  loss_scale: 65536.0000 (67788.0962)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0004  max mem: 15572
Epoch: [33]  [ 300/2809]  eta: 0:16:23  lr: 0.000004  min_lr: 0.000000  loss: 3.7703 (3.6758)  loss_scale: 65536.0000 (67713.2757)  weight_decay: 0.0500 (0.0500)  time: 0.3809  data: 0.0004  max mem: 15572
[2025-01-13 09:07:23,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=93000, skipped=631, lr=[4.25016877310702e-08, 4.25016877310702e-08, 6.071669675867172e-08, 6.071669675867172e-08, 8.67381382266739e-08, 8.67381382266739e-08, 1.2391162603810558e-07, 1.2391162603810558e-07, 1.770166086258651e-07, 1.770166086258651e-07, 2.528808694655216e-07, 2.528808694655216e-07, 3.6125838495074515e-07, 3.6125838495074515e-07, 5.160834070724932e-07, 5.160834070724932e-07, 7.372620101035616e-07, 7.372620101035616e-07, 1.053231443005088e-06, 1.053231443005088e-06, 1.5046163471501259e-06, 1.5046163471501259e-06, 2.14945192450018e-06, 2.14945192450018e-06, 3.070645606428829e-06, 3.070645606428829e-06, 4.386636580612613e-06, 4.386636580612613e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 09:07:23,665] [INFO] [timer.py:260:stop] epoch=0/micro_step=93000/global_step=93000, RunningAvgSamplesPerSec=31.211216803994215, CurrSamplesPerSec=34.649971705400205, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [33]  [ 310/2809]  eta: 0:16:18  lr: 0.000004  min_lr: 0.000000  loss: 3.7693 (3.6785)  loss_scale: 65536.0000 (67643.2669)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0003  max mem: 15572
Epoch: [33]  [ 320/2809]  eta: 0:16:13  lr: 0.000004  min_lr: 0.000000  loss: 3.9042 (3.6886)  loss_scale: 65536.0000 (67577.6199)  weight_decay: 0.0500 (0.0500)  time: 0.3794  data: 0.0003  max mem: 15572
Epoch: [33]  [ 330/2809]  eta: 0:16:08  lr: 0.000004  min_lr: 0.000000  loss: 3.9209 (3.6930)  loss_scale: 65536.0000 (67515.9396)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0003  max mem: 15572
Epoch: [33]  [ 340/2809]  eta: 0:16:04  lr: 0.000004  min_lr: 0.000000  loss: 3.7982 (3.6908)  loss_scale: 65536.0000 (67457.8768)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0003  max mem: 15572
Epoch: [33]  [ 350/2809]  eta: 0:16:00  lr: 0.000004  min_lr: 0.000000  loss: 3.5520 (3.6901)  loss_scale: 65536.0000 (67403.1225)  weight_decay: 0.0500 (0.0500)  time: 0.3871  data: 0.0004  max mem: 15572
Epoch: [33]  [ 360/2809]  eta: 0:15:56  lr: 0.000004  min_lr: 0.000000  loss: 3.7187 (3.6935)  loss_scale: 65536.0000 (67351.4017)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0006  max mem: 15572
Epoch: [33]  [ 370/2809]  eta: 0:15:52  lr: 0.000004  min_lr: 0.000000  loss: 3.5320 (3.6882)  loss_scale: 65536.0000 (67302.4690)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0006  max mem: 15572
[2025-01-13 09:07:51,413] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:07:51,414] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:07:51,798] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93072
[2025-01-13 09:07:51,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:07:51,798] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 380/2809]  eta: 0:15:49  lr: 0.000004  min_lr: 0.000000  loss: 3.4495 (3.6880)  loss_scale: 65536.0000 (67428.1155)  weight_decay: 0.0500 (0.0500)  time: 0.3995  data: 0.0005  max mem: 15572
Epoch: [33]  [ 390/2809]  eta: 0:15:45  lr: 0.000004  min_lr: 0.000000  loss: 3.4495 (3.6838)  loss_scale: 65536.0000 (67379.7238)  weight_decay: 0.0500 (0.0500)  time: 0.3971  data: 0.0005  max mem: 15572
Epoch: [33]  [ 400/2809]  eta: 0:15:41  lr: 0.000004  min_lr: 0.000000  loss: 3.5590 (3.6853)  loss_scale: 65536.0000 (67333.7456)  weight_decay: 0.0500 (0.0500)  time: 0.3879  data: 0.0004  max mem: 15572
Epoch: [33]  [ 410/2809]  eta: 0:15:36  lr: 0.000004  min_lr: 0.000000  loss: 3.8320 (3.6859)  loss_scale: 65536.0000 (67290.0049)  weight_decay: 0.0500 (0.0500)  time: 0.3817  data: 0.0003  max mem: 15572
Epoch: [33]  [ 420/2809]  eta: 0:15:31  lr: 0.000004  min_lr: 0.000000  loss: 3.8690 (3.6889)  loss_scale: 65536.0000 (67248.3420)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [33]  [ 430/2809]  eta: 0:15:27  lr: 0.000004  min_lr: 0.000000  loss: 3.8686 (3.6894)  loss_scale: 65536.0000 (67208.6125)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0002  max mem: 15572
Epoch: [33]  [ 440/2809]  eta: 0:15:22  lr: 0.000004  min_lr: 0.000000  loss: 3.7665 (3.6940)  loss_scale: 65536.0000 (67170.6848)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0003  max mem: 15572
Epoch: [33]  [ 450/2809]  eta: 0:15:17  lr: 0.000004  min_lr: 0.000000  loss: 3.7665 (3.6949)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [33]  [ 460/2809]  eta: 0:15:12  lr: 0.000004  min_lr: 0.000000  loss: 3.7563 (3.6926)  loss_scale: 65536.0000 (67099.7657)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
[2025-01-13 09:08:26,681] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93164
[2025-01-13 09:08:26,681] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:08:26,681] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 470/2809]  eta: 0:15:08  lr: 0.000004  min_lr: 0.000000  loss: 3.6137 (3.6880)  loss_scale: 65536.0000 (66788.2803)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0003  max mem: 15572
Epoch: [33]  [ 480/2809]  eta: 0:15:03  lr: 0.000004  min_lr: 0.000000  loss: 3.4725 (3.6822)  loss_scale: 32768.0000 (66080.9979)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [33]  [ 490/2809]  eta: 0:14:59  lr: 0.000004  min_lr: 0.000000  loss: 3.5413 (3.6831)  loss_scale: 32768.0000 (65402.5255)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0003  max mem: 15572
Epoch: [33]  [ 500/2809]  eta: 0:14:55  lr: 0.000004  min_lr: 0.000000  loss: 3.7502 (3.6781)  loss_scale: 32768.0000 (64751.1377)  weight_decay: 0.0500 (0.0500)  time: 0.3875  data: 0.0004  max mem: 15572
Epoch: [33]  [ 510/2809]  eta: 0:14:51  lr: 0.000004  min_lr: 0.000000  loss: 3.5502 (3.6786)  loss_scale: 32768.0000 (64125.2446)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0004  max mem: 15572
Epoch: [33]  [ 520/2809]  eta: 0:14:47  lr: 0.000004  min_lr: 0.000000  loss: 3.7311 (3.6804)  loss_scale: 32768.0000 (63523.3781)  weight_decay: 0.0500 (0.0500)  time: 0.3864  data: 0.0004  max mem: 15572
Epoch: [33]  [ 530/2809]  eta: 0:14:43  lr: 0.000004  min_lr: 0.000000  loss: 3.3783 (3.6762)  loss_scale: 32768.0000 (62944.1808)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0005  max mem: 15572
Epoch: [33]  [ 540/2809]  eta: 0:14:40  lr: 0.000004  min_lr: 0.000000  loss: 3.3931 (3.6744)  loss_scale: 32768.0000 (62386.3956)  weight_decay: 0.0500 (0.0500)  time: 0.3914  data: 0.0006  max mem: 15572
Epoch: [33]  [ 550/2809]  eta: 0:14:36  lr: 0.000004  min_lr: 0.000000  loss: 3.7430 (3.6728)  loss_scale: 32768.0000 (61848.8566)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0005  max mem: 15572
Epoch: [33]  [ 560/2809]  eta: 0:14:31  lr: 0.000004  min_lr: 0.000000  loss: 3.7359 (3.6738)  loss_scale: 32768.0000 (61330.4813)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0004  max mem: 15572
Epoch: [33]  [ 570/2809]  eta: 0:14:27  lr: 0.000004  min_lr: 0.000000  loss: 3.7400 (3.6749)  loss_scale: 32768.0000 (60830.2627)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0004  max mem: 15572
Epoch: [33]  [ 580/2809]  eta: 0:14:22  lr: 0.000004  min_lr: 0.000000  loss: 3.6504 (3.6743)  loss_scale: 32768.0000 (60347.2633)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0003  max mem: 15572
Epoch: [33]  [ 590/2809]  eta: 0:14:18  lr: 0.000004  min_lr: 0.000000  loss: 3.6471 (3.6736)  loss_scale: 32768.0000 (59880.6091)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0003  max mem: 15572
[2025-01-13 09:09:15,857] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:09:15,857] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 600/2809]  eta: 0:14:14  lr: 0.000004  min_lr: 0.000000  loss: 3.3034 (3.6706)  loss_scale: 32768.0000 (59702.0965)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
Epoch: [33]  [ 610/2809]  eta: 0:14:09  lr: 0.000004  min_lr: 0.000000  loss: 3.3034 (3.6703)  loss_scale: 65536.0000 (59797.5777)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [33]  [ 620/2809]  eta: 0:14:05  lr: 0.000004  min_lr: 0.000000  loss: 3.5647 (3.6666)  loss_scale: 65536.0000 (59889.9839)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [33]  [ 630/2809]  eta: 0:14:00  lr: 0.000004  min_lr: 0.000000  loss: 3.5306 (3.6644)  loss_scale: 65536.0000 (59979.4612)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [33]  [ 640/2809]  eta: 0:13:56  lr: 0.000004  min_lr: 0.000000  loss: 3.8864 (3.6673)  loss_scale: 65536.0000 (60066.1466)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [33]  [ 650/2809]  eta: 0:13:53  lr: 0.000004  min_lr: 0.000000  loss: 3.8864 (3.6690)  loss_scale: 65536.0000 (60150.1690)  weight_decay: 0.0500 (0.0500)  time: 0.3848  data: 0.0004  max mem: 15572
Epoch: [33]  [ 660/2809]  eta: 0:13:49  lr: 0.000004  min_lr: 0.000000  loss: 3.7107 (3.6682)  loss_scale: 65536.0000 (60231.6490)  weight_decay: 0.0500 (0.0500)  time: 0.3950  data: 0.0005  max mem: 15572
Epoch: [33]  [ 670/2809]  eta: 0:13:46  lr: 0.000004  min_lr: 0.000000  loss: 3.5813 (3.6684)  loss_scale: 65536.0000 (60310.7004)  weight_decay: 0.0500 (0.0500)  time: 0.3985  data: 0.0006  max mem: 15572
Epoch: [33]  [ 680/2809]  eta: 0:13:42  lr: 0.000004  min_lr: 0.000000  loss: 3.6789 (3.6685)  loss_scale: 65536.0000 (60387.4302)  weight_decay: 0.0500 (0.0500)  time: 0.3960  data: 0.0006  max mem: 15572
Epoch: [33]  [ 690/2809]  eta: 0:13:39  lr: 0.000004  min_lr: 0.000000  loss: 3.6789 (3.6684)  loss_scale: 65536.0000 (60461.9392)  weight_decay: 0.0500 (0.0500)  time: 0.3958  data: 0.0005  max mem: 15572
Epoch: [33]  [ 700/2809]  eta: 0:13:34  lr: 0.000004  min_lr: 0.000000  loss: 3.5409 (3.6663)  loss_scale: 65536.0000 (60534.3224)  weight_decay: 0.0500 (0.0500)  time: 0.3867  data: 0.0003  max mem: 15572
Epoch: [33]  [ 710/2809]  eta: 0:13:30  lr: 0.000004  min_lr: 0.000000  loss: 3.5327 (3.6654)  loss_scale: 65536.0000 (60604.6695)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [33]  [ 720/2809]  eta: 0:13:26  lr: 0.000004  min_lr: 0.000000  loss: 3.5967 (3.6655)  loss_scale: 65536.0000 (60673.0652)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0003  max mem: 15572
[2025-01-13 09:10:04,797] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:10:04,797] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:10:05,167] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93422
[2025-01-13 09:10:05,167] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:10:05,167] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 730/2809]  eta: 0:13:22  lr: 0.000004  min_lr: 0.000000  loss: 3.5864 (3.6634)  loss_scale: 65536.0000 (60829.2421)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [33]  [ 740/2809]  eta: 0:13:17  lr: 0.000004  min_lr: 0.000000  loss: 3.6576 (3.6642)  loss_scale: 65536.0000 (60892.7611)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [33]  [ 750/2809]  eta: 0:13:13  lr: 0.000004  min_lr: 0.000000  loss: 3.7445 (3.6675)  loss_scale: 65536.0000 (60954.5885)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [33]  [ 760/2809]  eta: 0:13:09  lr: 0.000004  min_lr: 0.000000  loss: 3.7661 (3.6686)  loss_scale: 65536.0000 (61014.7911)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [33]  [ 770/2809]  eta: 0:13:04  lr: 0.000004  min_lr: 0.000000  loss: 3.6636 (3.6683)  loss_scale: 65536.0000 (61073.4319)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0003  max mem: 15572
Epoch: [33]  [ 780/2809]  eta: 0:13:00  lr: 0.000004  min_lr: 0.000000  loss: 3.7560 (3.6699)  loss_scale: 65536.0000 (61130.5711)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0003  max mem: 15572
Epoch: [33]  [ 790/2809]  eta: 0:12:56  lr: 0.000004  min_lr: 0.000000  loss: 3.7247 (3.6692)  loss_scale: 65536.0000 (61186.2655)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
Epoch: [33]  [ 800/2809]  eta: 0:12:53  lr: 0.000004  min_lr: 0.000000  loss: 3.6341 (3.6690)  loss_scale: 65536.0000 (61240.5693)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0004  max mem: 15572
Epoch: [33]  [ 810/2809]  eta: 0:12:49  lr: 0.000004  min_lr: 0.000000  loss: 3.7087 (3.6690)  loss_scale: 65536.0000 (61293.5339)  weight_decay: 0.0500 (0.0500)  time: 0.3922  data: 0.0005  max mem: 15572
Epoch: [33]  [ 820/2809]  eta: 0:12:45  lr: 0.000004  min_lr: 0.000000  loss: 3.5591 (3.6661)  loss_scale: 65536.0000 (61345.2083)  weight_decay: 0.0500 (0.0500)  time: 0.3929  data: 0.0005  max mem: 15572
Epoch: [33]  [ 830/2809]  eta: 0:12:42  lr: 0.000004  min_lr: 0.000000  loss: 3.8018 (3.6685)  loss_scale: 65536.0000 (61395.6390)  weight_decay: 0.0500 (0.0500)  time: 0.3937  data: 0.0005  max mem: 15572
Epoch: [33]  [ 840/2809]  eta: 0:12:38  lr: 0.000004  min_lr: 0.000000  loss: 3.8174 (3.6693)  loss_scale: 65536.0000 (61444.8704)  weight_decay: 0.0500 (0.0500)  time: 0.3918  data: 0.0006  max mem: 15572
Epoch: [33]  [ 850/2809]  eta: 0:12:34  lr: 0.000004  min_lr: 0.000000  loss: 3.5701 (3.6682)  loss_scale: 65536.0000 (61492.9448)  weight_decay: 0.0500 (0.0500)  time: 0.3872  data: 0.0005  max mem: 15572
[2025-01-13 09:10:54,282] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:10:54,283] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:10:54,686] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93552
[2025-01-13 09:10:54,686] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:10:54,686] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 860/2809]  eta: 0:12:30  lr: 0.000004  min_lr: 0.000000  loss: 3.4393 (3.6670)  loss_scale: 65536.0000 (61616.0186)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0003  max mem: 15572
Epoch: [33]  [ 870/2809]  eta: 0:12:26  lr: 0.000004  min_lr: 0.000000  loss: 3.8593 (3.6670)  loss_scale: 65536.0000 (61661.0241)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0003  max mem: 15572
Epoch: [33]  [ 880/2809]  eta: 0:12:22  lr: 0.000004  min_lr: 0.000000  loss: 3.7852 (3.6680)  loss_scale: 65536.0000 (61705.0079)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0003  max mem: 15572
Epoch: [33]  [ 890/2809]  eta: 0:12:18  lr: 0.000004  min_lr: 0.000000  loss: 3.7583 (3.6676)  loss_scale: 65536.0000 (61748.0045)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [33]  [ 900/2809]  eta: 0:12:13  lr: 0.000004  min_lr: 0.000000  loss: 3.6671 (3.6661)  loss_scale: 65536.0000 (61790.0466)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0003  max mem: 15572
Epoch: [33]  [ 910/2809]  eta: 0:12:09  lr: 0.000004  min_lr: 0.000000  loss: 3.5105 (3.6639)  loss_scale: 65536.0000 (61831.1658)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0003  max mem: 15572
Epoch: [33]  [ 920/2809]  eta: 0:12:05  lr: 0.000004  min_lr: 0.000000  loss: 3.6421 (3.6642)  loss_scale: 65536.0000 (61871.3920)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [33]  [ 930/2809]  eta: 0:12:01  lr: 0.000004  min_lr: 0.000000  loss: 3.7217 (3.6643)  loss_scale: 65536.0000 (61910.7540)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [33]  [ 940/2809]  eta: 0:11:57  lr: 0.000004  min_lr: 0.000000  loss: 3.8091 (3.6655)  loss_scale: 65536.0000 (61949.2795)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0003  max mem: 15572
Epoch: [33]  [ 950/2809]  eta: 0:11:53  lr: 0.000004  min_lr: 0.000000  loss: 3.7889 (3.6663)  loss_scale: 65536.0000 (61986.9947)  weight_decay: 0.0500 (0.0500)  time: 0.3848  data: 0.0003  max mem: 15572
Epoch: [33]  [ 960/2809]  eta: 0:11:50  lr: 0.000004  min_lr: 0.000000  loss: 3.7552 (3.6671)  loss_scale: 65536.0000 (62023.9251)  weight_decay: 0.0500 (0.0500)  time: 0.3835  data: 0.0004  max mem: 15572
Epoch: [33]  [ 970/2809]  eta: 0:11:46  lr: 0.000004  min_lr: 0.000000  loss: 3.8113 (3.6688)  loss_scale: 65536.0000 (62060.0947)  weight_decay: 0.0500 (0.0500)  time: 0.3859  data: 0.0004  max mem: 15572
Epoch: [33]  [ 980/2809]  eta: 0:11:42  lr: 0.000004  min_lr: 0.000000  loss: 3.6975 (3.6691)  loss_scale: 65536.0000 (62095.5270)  weight_decay: 0.0500 (0.0500)  time: 0.3917  data: 0.0004  max mem: 15572
[2025-01-13 09:11:43,435] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:11:43,436] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:11:44,196] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93683
[2025-01-13 09:11:44,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:11:44,196] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 990/2809]  eta: 0:11:38  lr: 0.000004  min_lr: 0.000000  loss: 3.6320 (3.6688)  loss_scale: 65536.0000 (62262.5066)  weight_decay: 0.0500 (0.0500)  time: 0.3893  data: 0.0004  max mem: 15572
Epoch: [33]  [1000/2809]  eta: 0:11:34  lr: 0.000004  min_lr: 0.000000  loss: 3.6979 (3.6699)  loss_scale: 65536.0000 (62295.2088)  weight_decay: 0.0500 (0.0500)  time: 0.3800  data: 0.0003  max mem: 15572
Epoch: [33]  [1010/2809]  eta: 0:11:30  lr: 0.000004  min_lr: 0.000000  loss: 3.7927 (3.6709)  loss_scale: 65536.0000 (62327.2641)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [33]  [1020/2809]  eta: 0:11:26  lr: 0.000004  min_lr: 0.000000  loss: 3.8064 (3.6714)  loss_scale: 65536.0000 (62358.6915)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [33]  [1030/2809]  eta: 0:11:22  lr: 0.000004  min_lr: 0.000000  loss: 3.7066 (3.6696)  loss_scale: 65536.0000 (62389.5092)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0003  max mem: 15572
Epoch: [33]  [1040/2809]  eta: 0:11:18  lr: 0.000004  min_lr: 0.000000  loss: 3.5791 (3.6689)  loss_scale: 65536.0000 (62419.7349)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [33]  [1050/2809]  eta: 0:11:14  lr: 0.000004  min_lr: 0.000000  loss: 3.7657 (3.6699)  loss_scale: 65536.0000 (62449.3853)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [33]  [1060/2809]  eta: 0:11:10  lr: 0.000004  min_lr: 0.000000  loss: 3.7657 (3.6696)  loss_scale: 65536.0000 (62478.4769)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0003  max mem: 15572
Epoch: [33]  [1070/2809]  eta: 0:11:06  lr: 0.000004  min_lr: 0.000000  loss: 3.3576 (3.6674)  loss_scale: 65536.0000 (62507.0252)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0003  max mem: 15572
Epoch: [33]  [1080/2809]  eta: 0:11:02  lr: 0.000004  min_lr: 0.000000  loss: 3.4047 (3.6662)  loss_scale: 65536.0000 (62535.0453)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [33]  [1090/2809]  eta: 0:10:58  lr: 0.000004  min_lr: 0.000000  loss: 3.6832 (3.6695)  loss_scale: 65536.0000 (62562.5518)  weight_decay: 0.0500 (0.0500)  time: 0.3844  data: 0.0004  max mem: 15572
Epoch: [33]  [1100/2809]  eta: 0:10:54  lr: 0.000004  min_lr: 0.000000  loss: 3.7228 (3.6694)  loss_scale: 65536.0000 (62589.5586)  weight_decay: 0.0500 (0.0500)  time: 0.3926  data: 0.0004  max mem: 15572
Epoch: [33]  [1110/2809]  eta: 0:10:51  lr: 0.000004  min_lr: 0.000000  loss: 3.5413 (3.6682)  loss_scale: 65536.0000 (62616.0792)  weight_decay: 0.0500 (0.0500)  time: 0.3911  data: 0.0005  max mem: 15572
[2025-01-13 09:12:32,918] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:12:32,919] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:12:34,882] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93817
[2025-01-13 09:12:34,883] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:12:34,883] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [1120/2809]  eta: 0:10:47  lr: 0.000004  min_lr: 0.000000  loss: 3.6084 (3.6691)  loss_scale: 65536.0000 (62934.4371)  weight_decay: 0.0500 (0.0500)  time: 0.3971  data: 0.0006  max mem: 15572
Epoch: [33]  [1130/2809]  eta: 0:10:43  lr: 0.000004  min_lr: 0.000000  loss: 3.7160 (3.6695)  loss_scale: 65536.0000 (62957.4394)  weight_decay: 0.0500 (0.0500)  time: 0.3961  data: 0.0005  max mem: 15572
Epoch: [33]  [1140/2809]  eta: 0:10:40  lr: 0.000004  min_lr: 0.000000  loss: 3.8028 (3.6701)  loss_scale: 65536.0000 (62980.0386)  weight_decay: 0.0500 (0.0500)  time: 0.3902  data: 0.0005  max mem: 15572
[2025-01-13 09:12:44,995] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93843
[2025-01-13 09:12:44,995] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:12:44,995] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1150/2809]  eta: 0:10:36  lr: 0.000004  min_lr: 0.000000  loss: 3.6608 (3.6687)  loss_scale: 65536.0000 (62859.8992)  weight_decay: 0.0500 (0.0500)  time: 0.3856  data: 0.0005  max mem: 15572
Epoch: [33]  [1160/2809]  eta: 0:10:32  lr: 0.000004  min_lr: 0.000000  loss: 3.6608 (3.6679)  loss_scale: 32768.0000 (62600.7097)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0004  max mem: 15572
Epoch: [33]  [1170/2809]  eta: 0:10:28  lr: 0.000004  min_lr: 0.000000  loss: 3.7636 (3.6694)  loss_scale: 32768.0000 (62345.9471)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0004  max mem: 15572
Epoch: [33]  [1180/2809]  eta: 0:10:24  lr: 0.000004  min_lr: 0.000000  loss: 3.8283 (3.6708)  loss_scale: 32768.0000 (62095.4987)  weight_decay: 0.0500 (0.0500)  time: 0.3913  data: 0.0004  max mem: 15572
Epoch: [33]  [1190/2809]  eta: 0:10:21  lr: 0.000004  min_lr: 0.000000  loss: 3.8197 (3.6717)  loss_scale: 32768.0000 (61849.2561)  weight_decay: 0.0500 (0.0500)  time: 0.3914  data: 0.0004  max mem: 15572
Epoch: [33]  [1200/2809]  eta: 0:10:17  lr: 0.000004  min_lr: 0.000000  loss: 3.5912 (3.6680)  loss_scale: 32768.0000 (61607.1141)  weight_decay: 0.0500 (0.0500)  time: 0.3913  data: 0.0004  max mem: 15572
Epoch: [33]  [1210/2809]  eta: 0:10:13  lr: 0.000004  min_lr: 0.000000  loss: 3.2611 (3.6666)  loss_scale: 32768.0000 (61368.9711)  weight_decay: 0.0500 (0.0500)  time: 0.3944  data: 0.0004  max mem: 15572
Epoch: [33]  [1220/2809]  eta: 0:10:09  lr: 0.000004  min_lr: 0.000000  loss: 3.6135 (3.6660)  loss_scale: 32768.0000 (61134.7289)  weight_decay: 0.0500 (0.0500)  time: 0.3884  data: 0.0004  max mem: 15572
Epoch: [33]  [1230/2809]  eta: 0:10:05  lr: 0.000004  min_lr: 0.000000  loss: 3.7839 (3.6666)  loss_scale: 32768.0000 (60904.2924)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0003  max mem: 15572
Epoch: [33]  [1240/2809]  eta: 0:10:01  lr: 0.000004  min_lr: 0.000000  loss: 3.8111 (3.6672)  loss_scale: 32768.0000 (60677.5697)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [33]  [1250/2809]  eta: 0:09:57  lr: 0.000004  min_lr: 0.000000  loss: 3.6240 (3.6666)  loss_scale: 32768.0000 (60454.4716)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [33]  [1260/2809]  eta: 0:09:53  lr: 0.000004  min_lr: 0.000000  loss: 3.7625 (3.6658)  loss_scale: 32768.0000 (60234.9120)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0003  max mem: 15572
Epoch: [33]  [1270/2809]  eta: 0:09:49  lr: 0.000004  min_lr: 0.000000  loss: 3.9143 (3.6682)  loss_scale: 32768.0000 (60018.8072)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
[2025-01-13 09:13:34,085] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:13:34,085] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-13 09:13:34,813] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93974
[2025-01-13 09:13:34,814] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:13:34,814] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1280/2809]  eta: 0:09:45  lr: 0.000004  min_lr: 0.000000  loss: 3.9483 (3.6685)  loss_scale: 32768.0000 (59857.2365)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [33]  [1290/2809]  eta: 0:09:41  lr: 0.000004  min_lr: 0.000000  loss: 3.8824 (3.6704)  loss_scale: 32768.0000 (59647.4051)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [33]  [1300/2809]  eta: 0:09:37  lr: 0.000004  min_lr: 0.000000  loss: 3.7366 (3.6705)  loss_scale: 32768.0000 (59440.7994)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0003  max mem: 15572
[2025-01-13 09:13:44,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=94000, skipped=639, lr=[3.8392652360453044e-08, 3.8392652360453044e-08, 5.4846646229218644e-08, 5.4846646229218644e-08, 7.835235175602664e-08, 7.835235175602664e-08, 1.1193193108003806e-07, 1.1193193108003806e-07, 1.5990275868576867e-07, 1.5990275868576867e-07, 2.2843251240824097e-07, 2.2843251240824097e-07, 3.263321605832014e-07, 3.263321605832014e-07, 4.661888008331449e-07, 4.661888008331449e-07, 6.65984001190207e-07, 6.65984001190207e-07, 9.514057159860102e-07, 9.514057159860102e-07, 1.3591510228371572e-06, 1.3591510228371572e-06, 1.9416443183387965e-06, 1.9416443183387965e-06, 2.773777597626852e-06, 2.773777597626852e-06, 3.9625394251812176e-06, 3.9625394251812176e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 09:13:44,045] [INFO] [timer.py:260:stop] epoch=0/micro_step=94000/global_step=94000, RunningAvgSamplesPerSec=31.232561671632034, CurrSamplesPerSec=34.97733323418896, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [33]  [1310/2809]  eta: 0:09:33  lr: 0.000004  min_lr: 0.000000  loss: 3.7366 (3.6707)  loss_scale: 32768.0000 (59237.3455)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [33]  [1320/2809]  eta: 0:09:30  lr: 0.000004  min_lr: 0.000000  loss: 3.8599 (3.6720)  loss_scale: 32768.0000 (59036.9720)  weight_decay: 0.0500 (0.0500)  time: 0.3779  data: 0.0003  max mem: 15572
Epoch: [33]  [1330/2809]  eta: 0:09:26  lr: 0.000004  min_lr: 0.000000  loss: 3.7741 (3.6721)  loss_scale: 32768.0000 (58839.6093)  weight_decay: 0.0500 (0.0500)  time: 0.3856  data: 0.0018  max mem: 15572
Epoch: [33]  [1340/2809]  eta: 0:09:22  lr: 0.000004  min_lr: 0.000000  loss: 3.5295 (3.6713)  loss_scale: 32768.0000 (58645.1902)  weight_decay: 0.0500 (0.0500)  time: 0.3910  data: 0.0018  max mem: 15572
Epoch: [33]  [1350/2809]  eta: 0:09:18  lr: 0.000004  min_lr: 0.000000  loss: 3.5295 (3.6706)  loss_scale: 32768.0000 (58453.6491)  weight_decay: 0.0500 (0.0500)  time: 0.3924  data: 0.0005  max mem: 15572
Epoch: [33]  [1360/2809]  eta: 0:09:15  lr: 0.000004  min_lr: 0.000000  loss: 3.7110 (3.6709)  loss_scale: 32768.0000 (58264.9229)  weight_decay: 0.0500 (0.0500)  time: 0.3908  data: 0.0006  max mem: 15572
Epoch: [33]  [1370/2809]  eta: 0:09:11  lr: 0.000004  min_lr: 0.000000  loss: 3.7324 (3.6711)  loss_scale: 32768.0000 (58078.9497)  weight_decay: 0.0500 (0.0500)  time: 0.3881  data: 0.0004  max mem: 15572
Epoch: [33]  [1380/2809]  eta: 0:09:07  lr: 0.000004  min_lr: 0.000000  loss: 3.7521 (3.6713)  loss_scale: 32768.0000 (57895.6698)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0003  max mem: 15572
Epoch: [33]  [1390/2809]  eta: 0:09:03  lr: 0.000004  min_lr: 0.000000  loss: 3.7463 (3.6711)  loss_scale: 32768.0000 (57715.0252)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [33]  [1400/2809]  eta: 0:08:59  lr: 0.000004  min_lr: 0.000000  loss: 3.5161 (3.6694)  loss_scale: 32768.0000 (57536.9593)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0003  max mem: 15572
[2025-01-13 09:14:23,746] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:14:23,746] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [1410/2809]  eta: 0:08:55  lr: 0.000004  min_lr: 0.000000  loss: 3.5728 (3.6712)  loss_scale: 32768.0000 (57477.5337)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0003  max mem: 15572
Epoch: [33]  [1420/2809]  eta: 0:08:51  lr: 0.000004  min_lr: 0.000000  loss: 3.8902 (3.6733)  loss_scale: 65536.0000 (57534.2435)  weight_decay: 0.0500 (0.0500)  time: 0.3787  data: 0.0003  max mem: 15572
Epoch: [33]  [1430/2809]  eta: 0:08:47  lr: 0.000004  min_lr: 0.000000  loss: 3.6905 (3.6730)  loss_scale: 65536.0000 (57590.1607)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
Epoch: [33]  [1440/2809]  eta: 0:08:43  lr: 0.000004  min_lr: 0.000000  loss: 3.4989 (3.6715)  loss_scale: 65536.0000 (57645.3019)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [33]  [1450/2809]  eta: 0:08:40  lr: 0.000004  min_lr: 0.000000  loss: 3.5408 (3.6707)  loss_scale: 65536.0000 (57699.6830)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [33]  [1460/2809]  eta: 0:08:36  lr: 0.000004  min_lr: 0.000000  loss: 3.6282 (3.6705)  loss_scale: 65536.0000 (57753.3196)  weight_decay: 0.0500 (0.0500)  time: 0.3795  data: 0.0002  max mem: 15572
Epoch: [33]  [1470/2809]  eta: 0:08:32  lr: 0.000004  min_lr: 0.000000  loss: 3.8573 (3.6715)  loss_scale: 65536.0000 (57806.2271)  weight_decay: 0.0500 (0.0500)  time: 0.3927  data: 0.0003  max mem: 15572
Epoch: [33]  [1480/2809]  eta: 0:08:28  lr: 0.000004  min_lr: 0.000000  loss: 3.8238 (3.6707)  loss_scale: 65536.0000 (57858.4200)  weight_decay: 0.0500 (0.0500)  time: 0.3940  data: 0.0004  max mem: 15572
Epoch: [33]  [1490/2809]  eta: 0:08:25  lr: 0.000004  min_lr: 0.000000  loss: 3.6911 (3.6701)  loss_scale: 65536.0000 (57909.9128)  weight_decay: 0.0500 (0.0500)  time: 0.3907  data: 0.0005  max mem: 15572
Epoch: [33]  [1500/2809]  eta: 0:08:21  lr: 0.000004  min_lr: 0.000000  loss: 3.6911 (3.6703)  loss_scale: 65536.0000 (57960.7195)  weight_decay: 0.0500 (0.0500)  time: 0.3890  data: 0.0005  max mem: 15572
Epoch: [33]  [1510/2809]  eta: 0:08:17  lr: 0.000004  min_lr: 0.000000  loss: 3.5398 (3.6703)  loss_scale: 65536.0000 (58010.8537)  weight_decay: 0.0500 (0.0500)  time: 0.3857  data: 0.0005  max mem: 15572
Epoch: [33]  [1520/2809]  eta: 0:08:13  lr: 0.000004  min_lr: 0.000000  loss: 3.7111 (3.6715)  loss_scale: 65536.0000 (58060.3287)  weight_decay: 0.0500 (0.0500)  time: 0.3795  data: 0.0004  max mem: 15572
Epoch: [33]  [1530/2809]  eta: 0:08:09  lr: 0.000004  min_lr: 0.000000  loss: 3.8361 (3.6724)  loss_scale: 65536.0000 (58109.1574)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
[2025-01-13 09:15:12,633] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:15:12,633] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:15:13,722] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94234
[2025-01-13 09:15:13,722] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:15:13,722] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [1540/2809]  eta: 0:08:05  lr: 0.000004  min_lr: 0.000000  loss: 3.6970 (3.6713)  loss_scale: 65536.0000 (58284.9371)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
[2025-01-13 09:15:16,334] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94241
[2025-01-13 09:15:16,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:15:16,334] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1550/2809]  eta: 0:08:01  lr: 0.000004  min_lr: 0.000000  loss: 3.6258 (3.6717)  loss_scale: 65536.0000 (58183.7988)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [33]  [1560/2809]  eta: 0:07:57  lr: 0.000004  min_lr: 0.000000  loss: 3.7127 (3.6717)  loss_scale: 32768.0000 (58020.9814)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [33]  [1570/2809]  eta: 0:07:53  lr: 0.000004  min_lr: 0.000000  loss: 3.7127 (3.6708)  loss_scale: 32768.0000 (57860.2368)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [33]  [1580/2809]  eta: 0:07:49  lr: 0.000004  min_lr: 0.000000  loss: 3.7470 (3.6718)  loss_scale: 32768.0000 (57701.5256)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [33]  [1590/2809]  eta: 0:07:46  lr: 0.000004  min_lr: 0.000000  loss: 3.8968 (3.6721)  loss_scale: 32768.0000 (57544.8096)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [33]  [1600/2809]  eta: 0:07:42  lr: 0.000004  min_lr: 0.000000  loss: 3.9182 (3.6732)  loss_scale: 32768.0000 (57390.0512)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [33]  [1610/2809]  eta: 0:07:38  lr: 0.000004  min_lr: 0.000000  loss: 3.7436 (3.6722)  loss_scale: 32768.0000 (57237.2142)  weight_decay: 0.0500 (0.0500)  time: 0.3791  data: 0.0021  max mem: 15572
Epoch: [33]  [1620/2809]  eta: 0:07:34  lr: 0.000004  min_lr: 0.000000  loss: 3.7166 (3.6729)  loss_scale: 32768.0000 (57086.2628)  weight_decay: 0.0500 (0.0500)  time: 0.3863  data: 0.0022  max mem: 15572
Epoch: [33]  [1630/2809]  eta: 0:07:30  lr: 0.000004  min_lr: 0.000000  loss: 3.7352 (3.6725)  loss_scale: 32768.0000 (56937.1625)  weight_decay: 0.0500 (0.0500)  time: 0.3846  data: 0.0004  max mem: 15572
Epoch: [33]  [1640/2809]  eta: 0:07:26  lr: 0.000004  min_lr: 0.000000  loss: 3.7033 (3.6730)  loss_scale: 32768.0000 (56789.8793)  weight_decay: 0.0500 (0.0500)  time: 0.3886  data: 0.0005  max mem: 15572
Epoch: [33]  [1650/2809]  eta: 0:07:23  lr: 0.000004  min_lr: 0.000000  loss: 3.6027 (3.6729)  loss_scale: 32768.0000 (56644.3804)  weight_decay: 0.0500 (0.0500)  time: 0.3936  data: 0.0026  max mem: 15572
Epoch: [33]  [1660/2809]  eta: 0:07:19  lr: 0.000004  min_lr: 0.000000  loss: 3.8594 (3.6755)  loss_scale: 32768.0000 (56500.6334)  weight_decay: 0.0500 (0.0500)  time: 0.3843  data: 0.0025  max mem: 15572
Epoch: [33]  [1670/2809]  eta: 0:07:15  lr: 0.000004  min_lr: 0.000000  loss: 3.9992 (3.6759)  loss_scale: 32768.0000 (56358.6068)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
[2025-01-13 09:16:05,126] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:16:05,126] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [1680/2809]  eta: 0:07:11  lr: 0.000004  min_lr: 0.000000  loss: 3.6535 (3.6753)  loss_scale: 32768.0000 (56374.2153)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
Epoch: [33]  [1690/2809]  eta: 0:07:07  lr: 0.000004  min_lr: 0.000000  loss: 3.7801 (3.6753)  loss_scale: 65536.0000 (56428.3950)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0003  max mem: 15572
Epoch: [33]  [1700/2809]  eta: 0:07:03  lr: 0.000004  min_lr: 0.000000  loss: 3.9055 (3.6774)  loss_scale: 65536.0000 (56481.9377)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [33]  [1710/2809]  eta: 0:06:59  lr: 0.000004  min_lr: 0.000000  loss: 3.8929 (3.6780)  loss_scale: 65536.0000 (56534.8545)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0003  max mem: 15572
Epoch: [33]  [1720/2809]  eta: 0:06:56  lr: 0.000004  min_lr: 0.000000  loss: 3.7751 (3.6784)  loss_scale: 65536.0000 (56587.1563)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [33]  [1730/2809]  eta: 0:06:52  lr: 0.000004  min_lr: 0.000000  loss: 3.7751 (3.6787)  loss_scale: 65536.0000 (56638.8538)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [33]  [1740/2809]  eta: 0:06:48  lr: 0.000004  min_lr: 0.000000  loss: 3.9718 (3.6801)  loss_scale: 65536.0000 (56689.9575)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [33]  [1750/2809]  eta: 0:06:44  lr: 0.000004  min_lr: 0.000000  loss: 3.9542 (3.6803)  loss_scale: 65536.0000 (56740.4774)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0003  max mem: 15572
Epoch: [33]  [1760/2809]  eta: 0:06:40  lr: 0.000004  min_lr: 0.000000  loss: 3.7312 (3.6804)  loss_scale: 65536.0000 (56790.4236)  weight_decay: 0.0500 (0.0500)  time: 0.3870  data: 0.0003  max mem: 15572
Epoch: [33]  [1770/2809]  eta: 0:06:36  lr: 0.000004  min_lr: 0.000000  loss: 3.8880 (3.6816)  loss_scale: 65536.0000 (56839.8058)  weight_decay: 0.0500 (0.0500)  time: 0.3888  data: 0.0004  max mem: 15572
Epoch: [33]  [1780/2809]  eta: 0:06:33  lr: 0.000004  min_lr: 0.000000  loss: 3.8181 (3.6824)  loss_scale: 65536.0000 (56888.6334)  weight_decay: 0.0500 (0.0500)  time: 0.3825  data: 0.0005  max mem: 15572
Epoch: [33]  [1790/2809]  eta: 0:06:29  lr: 0.000004  min_lr: 0.000000  loss: 3.7771 (3.6823)  loss_scale: 65536.0000 (56936.9157)  weight_decay: 0.0500 (0.0500)  time: 0.3779  data: 0.0003  max mem: 15572
Epoch: [33]  [1800/2809]  eta: 0:06:25  lr: 0.000004  min_lr: 0.000000  loss: 3.8177 (3.6825)  loss_scale: 65536.0000 (56984.6619)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
[2025-01-13 09:16:53,318] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:16:53,318] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:16:54,076] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94500
[2025-01-13 09:16:54,076] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:16:54,076] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [1810/2809]  eta: 0:06:21  lr: 0.000004  min_lr: 0.000000  loss: 3.5280 (3.6814)  loss_scale: 65536.0000 (57104.2562)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0003  max mem: 15572
Epoch: [33]  [1820/2809]  eta: 0:06:17  lr: 0.000004  min_lr: 0.000000  loss: 3.5757 (3.6824)  loss_scale: 65536.0000 (57150.5590)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [33]  [1830/2809]  eta: 0:06:13  lr: 0.000004  min_lr: 0.000000  loss: 3.6078 (3.6818)  loss_scale: 65536.0000 (57196.3561)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0011  max mem: 15572
Epoch: [33]  [1840/2809]  eta: 0:06:09  lr: 0.000004  min_lr: 0.000000  loss: 3.5388 (3.6816)  loss_scale: 65536.0000 (57241.6556)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0011  max mem: 15572
Epoch: [33]  [1850/2809]  eta: 0:06:05  lr: 0.000004  min_lr: 0.000000  loss: 3.7896 (3.6819)  loss_scale: 65536.0000 (57286.4657)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [33]  [1860/2809]  eta: 0:06:02  lr: 0.000004  min_lr: 0.000000  loss: 3.8863 (3.6828)  loss_scale: 65536.0000 (57330.7942)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [33]  [1870/2809]  eta: 0:05:58  lr: 0.000004  min_lr: 0.000000  loss: 3.5398 (3.6802)  loss_scale: 65536.0000 (57374.6489)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [33]  [1880/2809]  eta: 0:05:54  lr: 0.000004  min_lr: 0.000000  loss: 3.3465 (3.6792)  loss_scale: 65536.0000 (57418.0372)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0003  max mem: 15572
Epoch: [33]  [1890/2809]  eta: 0:05:50  lr: 0.000004  min_lr: 0.000000  loss: 3.7200 (3.6790)  loss_scale: 65536.0000 (57460.9667)  weight_decay: 0.0500 (0.0500)  time: 0.3927  data: 0.0005  max mem: 15572
Epoch: [33]  [1900/2809]  eta: 0:05:46  lr: 0.000004  min_lr: 0.000000  loss: 3.7928 (3.6797)  loss_scale: 65536.0000 (57503.4445)  weight_decay: 0.0500 (0.0500)  time: 0.3952  data: 0.0006  max mem: 15572
Epoch: [33]  [1910/2809]  eta: 0:05:43  lr: 0.000004  min_lr: 0.000000  loss: 3.8707 (3.6809)  loss_scale: 65536.0000 (57545.4778)  weight_decay: 0.0500 (0.0500)  time: 0.3887  data: 0.0005  max mem: 15572
Epoch: [33]  [1920/2809]  eta: 0:05:39  lr: 0.000004  min_lr: 0.000000  loss: 3.9374 (3.6830)  loss_scale: 65536.0000 (57587.0734)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [33]  [1930/2809]  eta: 0:05:35  lr: 0.000004  min_lr: 0.000000  loss: 3.7954 (3.6824)  loss_scale: 65536.0000 (57628.2382)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
[2025-01-13 09:17:42,715] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:17:42,715] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:17:43,091] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94630
[2025-01-13 09:17:43,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:17:43,091] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [1940/2809]  eta: 0:05:31  lr: 0.000004  min_lr: 0.000000  loss: 3.5418 (3.6818)  loss_scale: 65536.0000 (57702.7429)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [33]  [1950/2809]  eta: 0:05:27  lr: 0.000004  min_lr: 0.000000  loss: 3.5418 (3.6819)  loss_scale: 65536.0000 (57742.8929)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
Epoch: [33]  [1960/2809]  eta: 0:05:23  lr: 0.000004  min_lr: 0.000000  loss: 3.7003 (3.6818)  loss_scale: 65536.0000 (57782.6334)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [33]  [1970/2809]  eta: 0:05:19  lr: 0.000004  min_lr: 0.000000  loss: 3.6089 (3.6813)  loss_scale: 65536.0000 (57821.9706)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [33]  [1980/2809]  eta: 0:05:16  lr: 0.000004  min_lr: 0.000000  loss: 3.6089 (3.6817)  loss_scale: 65536.0000 (57860.9107)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [33]  [1990/2809]  eta: 0:05:12  lr: 0.000004  min_lr: 0.000000  loss: 3.5417 (3.6813)  loss_scale: 65536.0000 (57899.4596)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [33]  [2000/2809]  eta: 0:05:08  lr: 0.000004  min_lr: 0.000000  loss: 3.6828 (3.6818)  loss_scale: 65536.0000 (57937.6232)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0003  max mem: 15572
Epoch: [33]  [2010/2809]  eta: 0:05:04  lr: 0.000004  min_lr: 0.000000  loss: 3.7108 (3.6828)  loss_scale: 65536.0000 (57975.4073)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0004  max mem: 15572
Epoch: [33]  [2020/2809]  eta: 0:05:00  lr: 0.000004  min_lr: 0.000000  loss: 4.0099 (3.6841)  loss_scale: 65536.0000 (58012.8174)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0004  max mem: 15572
Epoch: [33]  [2030/2809]  eta: 0:04:56  lr: 0.000004  min_lr: 0.000000  loss: 3.5289 (3.6822)  loss_scale: 65536.0000 (58049.8592)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0005  max mem: 15572
Epoch: [33]  [2040/2809]  eta: 0:04:53  lr: 0.000004  min_lr: 0.000000  loss: 3.6056 (3.6818)  loss_scale: 65536.0000 (58086.5380)  weight_decay: 0.0500 (0.0500)  time: 0.3820  data: 0.0004  max mem: 15572
Epoch: [33]  [2050/2809]  eta: 0:04:49  lr: 0.000004  min_lr: 0.000000  loss: 3.5819 (3.6801)  loss_scale: 65536.0000 (58122.8591)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0003  max mem: 15572
Epoch: [33]  [2060/2809]  eta: 0:04:45  lr: 0.000004  min_lr: 0.000000  loss: 3.6183 (3.6812)  loss_scale: 65536.0000 (58158.8278)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0003  max mem: 15572
[2025-01-13 09:18:31,317] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:18:31,317] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [33]  [2070/2809]  eta: 0:04:41  lr: 0.000004  min_lr: 0.000000  loss: 3.7670 (3.6803)  loss_scale: 65536.0000 (58479.2506)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 09:18:36,542] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94773
[2025-01-13 09:18:36,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:18:36,542] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [2080/2809]  eta: 0:04:37  lr: 0.000004  min_lr: 0.000000  loss: 3.8410 (3.6819)  loss_scale: 131072.0000 (58670.6237)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [33]  [2090/2809]  eta: 0:04:33  lr: 0.000004  min_lr: 0.000000  loss: 4.0160 (3.6831)  loss_scale: 65536.0000 (58703.4567)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [33]  [2100/2809]  eta: 0:04:30  lr: 0.000004  min_lr: 0.000000  loss: 3.8460 (3.6833)  loss_scale: 65536.0000 (58735.9772)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [33]  [2110/2809]  eta: 0:04:26  lr: 0.000004  min_lr: 0.000000  loss: 3.6220 (3.6823)  loss_scale: 65536.0000 (58768.1895)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [33]  [2120/2809]  eta: 0:04:22  lr: 0.000004  min_lr: 0.000000  loss: 3.5308 (3.6823)  loss_scale: 65536.0000 (58800.0981)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0003  max mem: 15572
Epoch: [33]  [2130/2809]  eta: 0:04:18  lr: 0.000004  min_lr: 0.000000  loss: 3.6334 (3.6817)  loss_scale: 65536.0000 (58831.7072)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
Epoch: [33]  [2140/2809]  eta: 0:04:14  lr: 0.000004  min_lr: 0.000000  loss: 3.7079 (3.6819)  loss_scale: 65536.0000 (58863.0210)  weight_decay: 0.0500 (0.0500)  time: 0.3873  data: 0.0004  max mem: 15572
Epoch: [33]  [2150/2809]  eta: 0:04:11  lr: 0.000004  min_lr: 0.000000  loss: 3.8832 (3.6827)  loss_scale: 65536.0000 (58894.0437)  weight_decay: 0.0500 (0.0500)  time: 0.3911  data: 0.0004  max mem: 15572
Epoch: [33]  [2160/2809]  eta: 0:04:07  lr: 0.000004  min_lr: 0.000000  loss: 3.9024 (3.6831)  loss_scale: 65536.0000 (58924.7793)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0003  max mem: 15572
Epoch: [33]  [2170/2809]  eta: 0:04:03  lr: 0.000004  min_lr: 0.000000  loss: 3.6645 (3.6833)  loss_scale: 65536.0000 (58955.2317)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [33]  [2180/2809]  eta: 0:03:59  lr: 0.000004  min_lr: 0.000000  loss: 3.7536 (3.6841)  loss_scale: 65536.0000 (58985.4049)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [33]  [2190/2809]  eta: 0:03:55  lr: 0.000004  min_lr: 0.000000  loss: 3.7622 (3.6835)  loss_scale: 65536.0000 (59015.3026)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [33]  [2200/2809]  eta: 0:03:51  lr: 0.000004  min_lr: 0.000000  loss: 3.7622 (3.6847)  loss_scale: 65536.0000 (59044.9287)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
[2025-01-13 09:19:24,888] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:19:24,888] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:19:25,249] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94903
[2025-01-13 09:19:25,249] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:19:25,249] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [2210/2809]  eta: 0:03:47  lr: 0.000004  min_lr: 0.000000  loss: 3.7705 (3.6845)  loss_scale: 65536.0000 (59103.9276)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
[2025-01-13 09:19:29,305] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94914
[2025-01-13 09:19:29,305] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:19:29,305] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2220/2809]  eta: 0:03:44  lr: 0.000004  min_lr: 0.000000  loss: 3.5302 (3.6833)  loss_scale: 65536.0000 (59073.8730)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0003  max mem: 15572
Epoch: [33]  [2230/2809]  eta: 0:03:40  lr: 0.000004  min_lr: 0.000000  loss: 3.6183 (3.6836)  loss_scale: 32768.0000 (58955.9623)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [33]  [2240/2809]  eta: 0:03:36  lr: 0.000004  min_lr: 0.000000  loss: 3.8293 (3.6835)  loss_scale: 32768.0000 (58839.1040)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [33]  [2250/2809]  eta: 0:03:32  lr: 0.000004  min_lr: 0.000000  loss: 3.6671 (3.6836)  loss_scale: 32768.0000 (58723.2839)  weight_decay: 0.0500 (0.0500)  time: 0.3817  data: 0.0004  max mem: 15572
Epoch: [33]  [2260/2809]  eta: 0:03:28  lr: 0.000004  min_lr: 0.000000  loss: 3.7761 (3.6839)  loss_scale: 32768.0000 (58608.4883)  weight_decay: 0.0500 (0.0500)  time: 0.3935  data: 0.0005  max mem: 15572
Epoch: [33]  [2270/2809]  eta: 0:03:25  lr: 0.000004  min_lr: 0.000000  loss: 3.6755 (3.6830)  loss_scale: 32768.0000 (58494.7037)  weight_decay: 0.0500 (0.0500)  time: 0.3901  data: 0.0004  max mem: 15572
Epoch: [33]  [2280/2809]  eta: 0:03:21  lr: 0.000004  min_lr: 0.000000  loss: 3.5162 (3.6830)  loss_scale: 32768.0000 (58381.9167)  weight_decay: 0.0500 (0.0500)  time: 0.3829  data: 0.0003  max mem: 15572
Epoch: [33]  [2290/2809]  eta: 0:03:17  lr: 0.000004  min_lr: 0.000000  loss: 3.6208 (3.6830)  loss_scale: 32768.0000 (58270.1144)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0002  max mem: 15572
Epoch: [33]  [2300/2809]  eta: 0:03:13  lr: 0.000004  min_lr: 0.000000  loss: 3.6060 (3.6837)  loss_scale: 32768.0000 (58159.2838)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
[2025-01-13 09:20:01,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=95000, skipped=646, lr=[3.4476501199824503e-08, 3.4476501199824503e-08, 4.925214457117787e-08, 4.925214457117787e-08, 7.036020653025411e-08, 7.036020653025411e-08, 1.0051458075750587e-07, 1.0051458075750587e-07, 1.435922582250084e-07, 1.435922582250084e-07, 2.0513179746429771e-07, 2.0513179746429771e-07, 2.930454249489968e-07, 2.930454249489968e-07, 4.186363213557097e-07, 4.186363213557097e-07, 5.980518876510139e-07, 5.980518876510139e-07, 8.543598395014485e-07, 8.543598395014485e-07, 1.2205140564306406e-06, 1.2205140564306406e-06, 1.7435915091866297e-06, 1.7435915091866297e-06, 2.4908450131237567e-06, 2.4908450131237567e-06, 3.5583500187482243e-06, 3.5583500187482243e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 09:20:01,551] [INFO] [timer.py:260:stop] epoch=0/micro_step=95000/global_step=95000, RunningAvgSamplesPerSec=31.256073652271066, CurrSamplesPerSec=32.867737320083926, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [33]  [2310/2809]  eta: 0:03:09  lr: 0.000004  min_lr: 0.000000  loss: 3.5806 (3.6837)  loss_scale: 32768.0000 (58049.4124)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [33]  [2320/2809]  eta: 0:03:06  lr: 0.000004  min_lr: 0.000000  loss: 3.4222 (3.6830)  loss_scale: 32768.0000 (57940.4877)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [33]  [2330/2809]  eta: 0:03:02  lr: 0.000004  min_lr: 0.000000  loss: 3.3390 (3.6823)  loss_scale: 32768.0000 (57832.4976)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [33]  [2340/2809]  eta: 0:02:58  lr: 0.000004  min_lr: 0.000000  loss: 3.6547 (3.6823)  loss_scale: 32768.0000 (57725.4302)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
[2025-01-13 09:20:17,939] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:20:17,939] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [2350/2809]  eta: 0:02:54  lr: 0.000004  min_lr: 0.000000  loss: 3.6883 (3.6821)  loss_scale: 32768.0000 (57688.9630)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [33]  [2360/2809]  eta: 0:02:50  lr: 0.000004  min_lr: 0.000000  loss: 3.9014 (3.6833)  loss_scale: 65536.0000 (57722.1991)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [33]  [2370/2809]  eta: 0:02:46  lr: 0.000004  min_lr: 0.000000  loss: 3.8871 (3.6833)  loss_scale: 65536.0000 (57755.1548)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [33]  [2380/2809]  eta: 0:02:43  lr: 0.000004  min_lr: 0.000000  loss: 3.6837 (3.6827)  loss_scale: 65536.0000 (57787.8337)  weight_decay: 0.0500 (0.0500)  time: 0.3842  data: 0.0003  max mem: 15572
Epoch: [33]  [2390/2809]  eta: 0:02:39  lr: 0.000004  min_lr: 0.000000  loss: 3.8705 (3.6830)  loss_scale: 65536.0000 (57820.2392)  weight_decay: 0.0500 (0.0500)  time: 0.3825  data: 0.0003  max mem: 15572
Epoch: [33]  [2400/2809]  eta: 0:02:35  lr: 0.000004  min_lr: 0.000000  loss: 3.6144 (3.6820)  loss_scale: 65536.0000 (57852.3748)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0002  max mem: 15572
Epoch: [33]  [2410/2809]  eta: 0:02:31  lr: 0.000004  min_lr: 0.000000  loss: 3.6433 (3.6826)  loss_scale: 65536.0000 (57884.2439)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [33]  [2420/2809]  eta: 0:02:27  lr: 0.000004  min_lr: 0.000000  loss: 3.7115 (3.6824)  loss_scale: 65536.0000 (57915.8496)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [33]  [2430/2809]  eta: 0:02:24  lr: 0.000004  min_lr: 0.000000  loss: 3.4605 (3.6808)  loss_scale: 65536.0000 (57947.1954)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [33]  [2440/2809]  eta: 0:02:20  lr: 0.000004  min_lr: 0.000000  loss: 3.4674 (3.6810)  loss_scale: 65536.0000 (57978.2843)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [33]  [2450/2809]  eta: 0:02:16  lr: 0.000004  min_lr: 0.000000  loss: 3.7060 (3.6809)  loss_scale: 65536.0000 (58009.1195)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [33]  [2460/2809]  eta: 0:02:12  lr: 0.000003  min_lr: 0.000000  loss: 3.7060 (3.6806)  loss_scale: 65536.0000 (58039.7042)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [33]  [2470/2809]  eta: 0:02:08  lr: 0.000003  min_lr: 0.000000  loss: 3.7901 (3.6813)  loss_scale: 65536.0000 (58070.0413)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
[2025-01-13 09:21:05,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:21:05,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:21:07,013] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95174
[2025-01-13 09:21:07,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:21:07,013] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [2480/2809]  eta: 0:02:05  lr: 0.000003  min_lr: 0.000000  loss: 3.7901 (3.6814)  loss_scale: 65536.0000 (58179.3793)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0004  max mem: 15572
Epoch: [33]  [2490/2809]  eta: 0:02:01  lr: 0.000003  min_lr: 0.000000  loss: 3.7028 (3.6820)  loss_scale: 65536.0000 (58208.9121)  weight_decay: 0.0500 (0.0500)  time: 0.3836  data: 0.0004  max mem: 15572
Epoch: [33]  [2500/2809]  eta: 0:01:57  lr: 0.000003  min_lr: 0.000000  loss: 3.7028 (3.6823)  loss_scale: 65536.0000 (58238.2087)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0004  max mem: 15572
Epoch: [33]  [2510/2809]  eta: 0:01:53  lr: 0.000003  min_lr: 0.000000  loss: 3.8300 (3.6826)  loss_scale: 65536.0000 (58267.2720)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0004  max mem: 15572
Epoch: [33]  [2520/2809]  eta: 0:01:49  lr: 0.000003  min_lr: 0.000000  loss: 3.8091 (3.6824)  loss_scale: 65536.0000 (58296.1047)  weight_decay: 0.0500 (0.0500)  time: 0.3796  data: 0.0003  max mem: 15572
Epoch: [33]  [2530/2809]  eta: 0:01:46  lr: 0.000003  min_lr: 0.000000  loss: 3.6770 (3.6822)  loss_scale: 65536.0000 (58324.7096)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [33]  [2540/2809]  eta: 0:01:42  lr: 0.000003  min_lr: 0.000000  loss: 3.6770 (3.6823)  loss_scale: 65536.0000 (58353.0893)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [33]  [2550/2809]  eta: 0:01:38  lr: 0.000003  min_lr: 0.000000  loss: 3.7737 (3.6827)  loss_scale: 65536.0000 (58381.2466)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
[2025-01-13 09:21:37,535] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95255
[2025-01-13 09:21:37,535] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:21:37,535] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2560/2809]  eta: 0:01:34  lr: 0.000003  min_lr: 0.000000  loss: 3.7585 (3.6823)  loss_scale: 65536.0000 (58370.7989)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [33]  [2570/2809]  eta: 0:01:30  lr: 0.000003  min_lr: 0.000000  loss: 3.6849 (3.6832)  loss_scale: 32768.0000 (58271.2159)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [33]  [2580/2809]  eta: 0:01:26  lr: 0.000003  min_lr: 0.000000  loss: 3.6861 (3.6829)  loss_scale: 32768.0000 (58172.4045)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [33]  [2590/2809]  eta: 0:01:23  lr: 0.000003  min_lr: 0.000000  loss: 3.4303 (3.6824)  loss_scale: 32768.0000 (58074.3558)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [33]  [2600/2809]  eta: 0:01:19  lr: 0.000003  min_lr: 0.000000  loss: 3.5216 (3.6826)  loss_scale: 32768.0000 (57977.0611)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [33]  [2610/2809]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000000  loss: 3.6881 (3.6821)  loss_scale: 32768.0000 (57880.5117)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [33]  [2620/2809]  eta: 0:01:11  lr: 0.000003  min_lr: 0.000000  loss: 3.7731 (3.6827)  loss_scale: 32768.0000 (57784.6990)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0003  max mem: 15572
Epoch: [33]  [2630/2809]  eta: 0:01:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5546 (3.6817)  loss_scale: 32768.0000 (57689.6146)  weight_decay: 0.0500 (0.0500)  time: 0.3842  data: 0.0003  max mem: 15572
Epoch: [33]  [2640/2809]  eta: 0:01:04  lr: 0.000003  min_lr: 0.000000  loss: 3.5040 (3.6817)  loss_scale: 32768.0000 (57595.2503)  weight_decay: 0.0500 (0.0500)  time: 0.3786  data: 0.0003  max mem: 15572
Epoch: [33]  [2650/2809]  eta: 0:01:00  lr: 0.000003  min_lr: 0.000000  loss: 3.7892 (3.6820)  loss_scale: 32768.0000 (57501.5979)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [33]  [2660/2809]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 3.5763 (3.6812)  loss_scale: 32768.0000 (57408.6494)  weight_decay: 0.0500 (0.0500)  time: 0.3787  data: 0.0004  max mem: 15572
Epoch: [33]  [2670/2809]  eta: 0:00:52  lr: 0.000003  min_lr: 0.000000  loss: 3.3493 (3.6806)  loss_scale: 32768.0000 (57316.3969)  weight_decay: 0.0500 (0.0500)  time: 0.3837  data: 0.0004  max mem: 15572
Epoch: [33]  [2680/2809]  eta: 0:00:48  lr: 0.000003  min_lr: 0.000000  loss: 3.5615 (3.6805)  loss_scale: 32768.0000 (57224.8325)  weight_decay: 0.0500 (0.0500)  time: 0.3836  data: 0.0004  max mem: 15572
[2025-01-13 09:22:26,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:22:26,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [2690/2809]  eta: 0:00:45  lr: 0.000003  min_lr: 0.000000  loss: 3.4498 (3.6799)  loss_scale: 32768.0000 (57182.6563)  weight_decay: 0.0500 (0.0500)  time: 0.3854  data: 0.0005  max mem: 15572
Epoch: [33]  [2700/2809]  eta: 0:00:41  lr: 0.000003  min_lr: 0.000000  loss: 3.5389 (3.6805)  loss_scale: 65536.0000 (57213.5831)  weight_decay: 0.0500 (0.0500)  time: 0.3907  data: 0.0004  max mem: 15572
Epoch: [33]  [2710/2809]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000000  loss: 3.8226 (3.6811)  loss_scale: 65536.0000 (57244.2818)  weight_decay: 0.0500 (0.0500)  time: 0.3962  data: 0.0005  max mem: 15572
Epoch: [33]  [2720/2809]  eta: 0:00:33  lr: 0.000003  min_lr: 0.000000  loss: 3.8220 (3.6808)  loss_scale: 65536.0000 (57274.7549)  weight_decay: 0.0500 (0.0500)  time: 0.3997  data: 0.0006  max mem: 15572
Epoch: [33]  [2730/2809]  eta: 0:00:30  lr: 0.000003  min_lr: 0.000000  loss: 3.8861 (3.6820)  loss_scale: 65536.0000 (57305.0048)  weight_decay: 0.0500 (0.0500)  time: 0.4056  data: 0.0006  max mem: 15572
Epoch: [33]  [2740/2809]  eta: 0:00:26  lr: 0.000003  min_lr: 0.000000  loss: 3.8862 (3.6821)  loss_scale: 65536.0000 (57335.0339)  weight_decay: 0.0500 (0.0500)  time: 0.4043  data: 0.0005  max mem: 15572
Epoch: [33]  [2750/2809]  eta: 0:00:22  lr: 0.000003  min_lr: 0.000000  loss: 3.5717 (3.6806)  loss_scale: 65536.0000 (57364.8448)  weight_decay: 0.0500 (0.0500)  time: 0.3915  data: 0.0005  max mem: 15572
[2025-01-13 09:22:53,952] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95454
[2025-01-13 09:22:53,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:22:53,952] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2760/2809]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 3.6157 (3.6812)  loss_scale: 65536.0000 (57346.9670)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0004  max mem: 15572
Epoch: [33]  [2770/2809]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 3.6599 (3.6798)  loss_scale: 32768.0000 (57258.2663)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0003  max mem: 15572
Epoch: [33]  [2780/2809]  eta: 0:00:11  lr: 0.000003  min_lr: 0.000000  loss: 3.5722 (3.6797)  loss_scale: 32768.0000 (57170.2035)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [33]  [2790/2809]  eta: 0:00:07  lr: 0.000003  min_lr: 0.000000  loss: 3.7622 (3.6802)  loss_scale: 32768.0000 (57082.7718)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [33]  [2800/2809]  eta: 0:00:03  lr: 0.000003  min_lr: 0.000000  loss: 3.8465 (3.6806)  loss_scale: 32768.0000 (56995.9643)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [33]  [2808/2809]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 4.0136 (3.6810)  loss_scale: 32768.0000 (56926.9633)  weight_decay: 0.0500 (0.0500)  time: 0.3605  data: 0.0001  max mem: 15572
Epoch: [33] Total time: 0:17:47 (0.3802 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 4.0136 (3.6810)  loss_scale: 32768.0000 (56926.9633)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:12:15  loss: 0.3657 (0.3657)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7049  data: 2.5305  max mem: 15572
Val:  [ 10/272]  eta: 0:01:49  loss: 2.4368 (2.2830)  acc1: 38.8889 (42.4242)  acc5: 77.7778 (73.2323)  time: 0.4190  data: 0.2638  max mem: 15572
Val:  [ 20/272]  eta: 0:01:13  loss: 2.3036 (2.3176)  acc1: 44.4444 (44.9735)  acc5: 77.7778 (74.6032)  time: 0.1721  data: 0.0188  max mem: 15572
Val:  [ 30/272]  eta: 0:00:59  loss: 2.3036 (2.3922)  acc1: 44.4444 (43.0108)  acc5: 77.7778 (73.6559)  time: 0.1531  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:52  loss: 2.5478 (2.4395)  acc1: 33.3333 (40.6504)  acc5: 77.7778 (73.7127)  time: 0.1526  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:48  loss: 2.4166 (2.3669)  acc1: 33.3333 (42.8105)  acc5: 77.7778 (75.1634)  time: 0.1679  data: 0.0099  max mem: 15572
Val:  [ 60/272]  eta: 0:00:43  loss: 1.5259 (2.2591)  acc1: 66.6667 (45.9016)  acc5: 88.8889 (76.4117)  time: 0.1705  data: 0.0100  max mem: 15572
Val:  [ 70/272]  eta: 0:00:41  loss: 1.5477 (2.1808)  acc1: 66.6667 (48.3568)  acc5: 83.3333 (77.4648)  time: 0.1725  data: 0.0034  max mem: 15572
Val:  [ 80/272]  eta: 0:00:39  loss: 1.8466 (2.1914)  acc1: 55.5556 (48.3539)  acc5: 83.3333 (77.2291)  time: 0.2048  data: 0.0331  max mem: 15572
Val:  [ 90/272]  eta: 0:00:36  loss: 2.1076 (2.1900)  acc1: 55.5556 (48.5958)  acc5: 77.7778 (77.7167)  time: 0.1937  data: 0.0302  max mem: 15572
Val:  [100/272]  eta: 0:00:33  loss: 2.1023 (2.2168)  acc1: 50.0000 (48.0748)  acc5: 83.3333 (77.3927)  time: 0.1617  data: 0.0005  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 2.4877 (2.2917)  acc1: 22.2222 (45.6456)  acc5: 72.2222 (76.1261)  time: 0.1633  data: 0.0005  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 2.8779 (2.3275)  acc1: 16.6667 (44.9954)  acc5: 72.2222 (75.4821)  time: 0.1661  data: 0.0006  max mem: 15572
Val:  [130/272]  eta: 0:00:26  loss: 2.1024 (2.2936)  acc1: 44.4444 (45.8439)  acc5: 77.7778 (76.1662)  time: 0.1632  data: 0.0007  max mem: 15572
Val:  [140/272]  eta: 0:00:24  loss: 1.6215 (2.2854)  acc1: 55.5556 (46.2569)  acc5: 83.3333 (76.0047)  time: 0.1717  data: 0.0026  max mem: 15572
Val:  [150/272]  eta: 0:00:22  loss: 2.3300 (2.2911)  acc1: 38.8889 (45.4746)  acc5: 77.7778 (76.2693)  time: 0.1750  data: 0.0024  max mem: 15572
Val:  [160/272]  eta: 0:00:20  loss: 2.2830 (2.2803)  acc1: 44.4444 (45.9972)  acc5: 77.7778 (76.3630)  time: 0.1742  data: 0.0005  max mem: 15572
Val:  [170/272]  eta: 0:00:18  loss: 2.4051 (2.3005)  acc1: 44.4444 (45.3866)  acc5: 72.2222 (75.9259)  time: 0.1666  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 2.3532 (2.2909)  acc1: 38.8889 (45.3039)  acc5: 77.7778 (76.2431)  time: 0.1589  data: 0.0057  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 2.3532 (2.3433)  acc1: 38.8889 (44.0954)  acc5: 77.7778 (74.8691)  time: 0.1642  data: 0.0057  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 2.4765 (2.3496)  acc1: 38.8889 (43.8917)  acc5: 66.6667 (74.6821)  time: 0.1648  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.1588 (2.3547)  acc1: 44.4444 (43.9179)  acc5: 72.2222 (74.5656)  time: 0.1706  data: 0.0005  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 2.3791 (2.3438)  acc1: 44.4444 (44.2433)  acc5: 77.7778 (74.7109)  time: 0.1687  data: 0.0005  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.7228 (2.3126)  acc1: 66.6667 (45.2621)  acc5: 83.3333 (75.1323)  time: 0.1636  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.5480 (2.2976)  acc1: 61.1111 (45.5970)  acc5: 83.3333 (75.4726)  time: 0.1665  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.1875 (2.3090)  acc1: 38.8889 (44.9757)  acc5: 83.3333 (75.4316)  time: 0.1791  data: 0.0097  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.2059 (2.2524)  acc1: 66.6667 (46.5517)  acc5: 88.8889 (76.1814)  time: 0.1761  data: 0.0154  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3747 (2.2468)  acc1: 66.6667 (46.7405)  acc5: 88.8889 (76.4043)  time: 0.1477  data: 0.0059  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3747 (2.2504)  acc1: 66.6667 (46.7131)  acc5: 88.8889 (76.3670)  time: 0.1421  data: 0.0059  max mem: 15572
Val: Total time: 0:00:48 (0.1781 s / it)
* Acc@1 46.713 Acc@5 76.367 loss 2.250
Accuracy of the network on the 4883 val videos: 46.7%
[2025-01-13 09:24:01,221] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 09:24:01,222] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 09:24:01,222] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 09:24:03,631] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 09:24:03,631] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.71%
Epoch: [34]  [   0/2809]  eta: 3:08:48  lr: 0.000003  min_lr: 0.000000  loss: 3.4312 (3.4312)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 4.0331  data: 3.6330  max mem: 15572
Epoch: [34]  [  10/2809]  eta: 0:33:43  lr: 0.000003  min_lr: 0.000000  loss: 3.7032 (3.6521)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7229  data: 0.3305  max mem: 15572
Epoch: [34]  [  20/2809]  eta: 0:25:59  lr: 0.000003  min_lr: 0.000000  loss: 3.7032 (3.5965)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3855  data: 0.0003  max mem: 15572
Epoch: [34]  [  30/2809]  eta: 0:23:27  lr: 0.000003  min_lr: 0.000000  loss: 3.7593 (3.6250)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3875  data: 0.0003  max mem: 15572
Epoch: [34]  [  40/2809]  eta: 0:22:07  lr: 0.000003  min_lr: 0.000000  loss: 3.6017 (3.5963)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3956  data: 0.0005  max mem: 15572
Epoch: [34]  [  50/2809]  eta: 0:21:09  lr: 0.000003  min_lr: 0.000000  loss: 3.5778 (3.6000)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3883  data: 0.0005  max mem: 15572
Epoch: [34]  [  60/2809]  eta: 0:20:31  lr: 0.000003  min_lr: 0.000000  loss: 3.8202 (3.6234)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3833  data: 0.0004  max mem: 15572
Epoch: [34]  [  70/2809]  eta: 0:19:58  lr: 0.000003  min_lr: 0.000000  loss: 3.6573 (3.6148)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3800  data: 0.0003  max mem: 15572
[2025-01-13 09:24:37,323] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:24:37,323] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [  80/2809]  eta: 0:19:31  lr: 0.000003  min_lr: 0.000000  loss: 3.6573 (3.6341)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [34]  [  90/2809]  eta: 0:19:10  lr: 0.000003  min_lr: 0.000000  loss: 3.6577 (3.6090)  loss_scale: 65536.0000 (37809.2308)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [34]  [ 100/2809]  eta: 0:18:52  lr: 0.000003  min_lr: 0.000000  loss: 3.6577 (3.6142)  loss_scale: 65536.0000 (40554.4554)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [34]  [ 110/2809]  eta: 0:18:36  lr: 0.000003  min_lr: 0.000000  loss: 3.6497 (3.6149)  loss_scale: 65536.0000 (42805.0450)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [34]  [ 120/2809]  eta: 0:18:21  lr: 0.000003  min_lr: 0.000000  loss: 3.6497 (3.6172)  loss_scale: 65536.0000 (44683.6364)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0003  max mem: 15572
Epoch: [34]  [ 130/2809]  eta: 0:18:09  lr: 0.000003  min_lr: 0.000000  loss: 3.6234 (3.6111)  loss_scale: 65536.0000 (46275.4198)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0003  max mem: 15572
Epoch: [34]  [ 140/2809]  eta: 0:17:57  lr: 0.000003  min_lr: 0.000000  loss: 3.3981 (3.6022)  loss_scale: 65536.0000 (47641.4184)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [34]  [ 150/2809]  eta: 0:17:47  lr: 0.000003  min_lr: 0.000000  loss: 3.6358 (3.6108)  loss_scale: 65536.0000 (48826.4901)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0003  max mem: 15572
Epoch: [34]  [ 160/2809]  eta: 0:17:38  lr: 0.000003  min_lr: 0.000000  loss: 3.7331 (3.6118)  loss_scale: 65536.0000 (49864.3478)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
Epoch: [34]  [ 170/2809]  eta: 0:17:32  lr: 0.000003  min_lr: 0.000000  loss: 3.7250 (3.6217)  loss_scale: 65536.0000 (50780.8187)  weight_decay: 0.0500 (0.0500)  time: 0.3800  data: 0.0003  max mem: 15572
Epoch: [34]  [ 180/2809]  eta: 0:17:27  lr: 0.000003  min_lr: 0.000000  loss: 3.6782 (3.6281)  loss_scale: 65536.0000 (51596.0221)  weight_decay: 0.0500 (0.0500)  time: 0.3906  data: 0.0005  max mem: 15572
Epoch: [34]  [ 190/2809]  eta: 0:17:22  lr: 0.000003  min_lr: 0.000000  loss: 3.8021 (3.6249)  loss_scale: 65536.0000 (52325.8639)  weight_decay: 0.0500 (0.0500)  time: 0.3913  data: 0.0006  max mem: 15572
Epoch: [34]  [ 200/2809]  eta: 0:17:15  lr: 0.000003  min_lr: 0.000000  loss: 3.8021 (3.6395)  loss_scale: 65536.0000 (52983.0846)  weight_decay: 0.0500 (0.0500)  time: 0.3818  data: 0.0004  max mem: 15572
[2025-01-13 09:25:25,293] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:25:25,294] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:25:27,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95716
[2025-01-13 09:25:27,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:25:27,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 210/2809]  eta: 0:17:08  lr: 0.000003  min_lr: 0.000000  loss: 3.6485 (3.6297)  loss_scale: 65536.0000 (55130.9953)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [34]  [ 220/2809]  eta: 0:17:01  lr: 0.000003  min_lr: 0.000000  loss: 3.6488 (3.6408)  loss_scale: 65536.0000 (55601.8100)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [34]  [ 230/2809]  eta: 0:16:54  lr: 0.000003  min_lr: 0.000000  loss: 3.9637 (3.6478)  loss_scale: 65536.0000 (56031.8615)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [34]  [ 240/2809]  eta: 0:16:48  lr: 0.000003  min_lr: 0.000000  loss: 3.7560 (3.6474)  loss_scale: 65536.0000 (56426.2241)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [34]  [ 250/2809]  eta: 0:16:43  lr: 0.000003  min_lr: 0.000000  loss: 3.9431 (3.6629)  loss_scale: 65536.0000 (56789.1633)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [34]  [ 260/2809]  eta: 0:16:37  lr: 0.000003  min_lr: 0.000000  loss: 3.7584 (3.6543)  loss_scale: 65536.0000 (57124.2912)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0003  max mem: 15572
Epoch: [34]  [ 270/2809]  eta: 0:16:31  lr: 0.000003  min_lr: 0.000000  loss: 3.6889 (3.6607)  loss_scale: 65536.0000 (57434.6863)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [34]  [ 280/2809]  eta: 0:16:26  lr: 0.000003  min_lr: 0.000000  loss: 3.9105 (3.6647)  loss_scale: 65536.0000 (57722.9893)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [34]  [ 290/2809]  eta: 0:16:22  lr: 0.000003  min_lr: 0.000000  loss: 3.6496 (3.6697)  loss_scale: 65536.0000 (57991.4777)  weight_decay: 0.0500 (0.0500)  time: 0.3814  data: 0.0004  max mem: 15572
Epoch: [34]  [ 300/2809]  eta: 0:16:18  lr: 0.000003  min_lr: 0.000000  loss: 3.7179 (3.6730)  loss_scale: 65536.0000 (58242.1262)  weight_decay: 0.0500 (0.0500)  time: 0.3916  data: 0.0005  max mem: 15572
Epoch: [34]  [ 310/2809]  eta: 0:16:14  lr: 0.000003  min_lr: 0.000000  loss: 3.6237 (3.6763)  loss_scale: 65536.0000 (58476.6559)  weight_decay: 0.0500 (0.0500)  time: 0.3917  data: 0.0005  max mem: 15572
Epoch: [34]  [ 320/2809]  eta: 0:16:09  lr: 0.000003  min_lr: 0.000000  loss: 3.6237 (3.6756)  loss_scale: 65536.0000 (58696.5732)  weight_decay: 0.0500 (0.0500)  time: 0.3845  data: 0.0005  max mem: 15572
Epoch: [34]  [ 330/2809]  eta: 0:16:05  lr: 0.000003  min_lr: 0.000000  loss: 3.8435 (3.6798)  loss_scale: 65536.0000 (58903.2024)  weight_decay: 0.0500 (0.0500)  time: 0.3818  data: 0.0004  max mem: 15572
[2025-01-13 09:26:15,907] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:26:15,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:26:16,267] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95846
[2025-01-13 09:26:16,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:26:16,267] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 340/2809]  eta: 0:15:59  lr: 0.000003  min_lr: 0.000000  loss: 3.7558 (3.6813)  loss_scale: 65536.0000 (59289.9003)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0003  max mem: 15572
Epoch: [34]  [ 350/2809]  eta: 0:15:54  lr: 0.000003  min_lr: 0.000000  loss: 3.7511 (3.6839)  loss_scale: 65536.0000 (59467.8519)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [34]  [ 360/2809]  eta: 0:15:49  lr: 0.000003  min_lr: 0.000000  loss: 3.7511 (3.6771)  loss_scale: 65536.0000 (59635.9446)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [34]  [ 370/2809]  eta: 0:15:44  lr: 0.000003  min_lr: 0.000000  loss: 3.7778 (3.6786)  loss_scale: 65536.0000 (59794.9757)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [34]  [ 380/2809]  eta: 0:15:40  lr: 0.000003  min_lr: 0.000000  loss: 3.7778 (3.6807)  loss_scale: 65536.0000 (59945.6588)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [34]  [ 390/2809]  eta: 0:15:35  lr: 0.000003  min_lr: 0.000000  loss: 3.6822 (3.6784)  loss_scale: 65536.0000 (60088.6343)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [34]  [ 400/2809]  eta: 0:15:30  lr: 0.000003  min_lr: 0.000000  loss: 3.8666 (3.6818)  loss_scale: 65536.0000 (60224.4788)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [34]  [ 410/2809]  eta: 0:15:25  lr: 0.000003  min_lr: 0.000000  loss: 3.8558 (3.6803)  loss_scale: 65536.0000 (60353.7129)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [34]  [ 420/2809]  eta: 0:15:20  lr: 0.000003  min_lr: 0.000000  loss: 3.7380 (3.6849)  loss_scale: 65536.0000 (60476.8076)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [34]  [ 430/2809]  eta: 0:15:17  lr: 0.000003  min_lr: 0.000000  loss: 3.6348 (3.6826)  loss_scale: 65536.0000 (60594.1903)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0003  max mem: 15572
Epoch: [34]  [ 440/2809]  eta: 0:15:13  lr: 0.000003  min_lr: 0.000000  loss: 3.5821 (3.6830)  loss_scale: 65536.0000 (60706.2494)  weight_decay: 0.0500 (0.0500)  time: 0.3885  data: 0.0004  max mem: 15572
Epoch: [34]  [ 450/2809]  eta: 0:15:09  lr: 0.000003  min_lr: 0.000000  loss: 3.5769 (3.6799)  loss_scale: 65536.0000 (60813.3392)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0004  max mem: 15572
Epoch: [34]  [ 460/2809]  eta: 0:15:04  lr: 0.000003  min_lr: 0.000000  loss: 3.5414 (3.6837)  loss_scale: 65536.0000 (60915.7831)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
[2025-01-13 09:27:04,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:27:04,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [ 470/2809]  eta: 0:15:00  lr: 0.000003  min_lr: 0.000000  loss: 4.0958 (3.6863)  loss_scale: 65536.0000 (61292.1614)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
[2025-01-13 09:27:05,689] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95978
[2025-01-13 09:27:05,689] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:27:05,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 480/2809]  eta: 0:14:55  lr: 0.000003  min_lr: 0.000000  loss: 3.7632 (3.6828)  loss_scale: 65536.0000 (61516.6403)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [34]  [ 490/2809]  eta: 0:14:50  lr: 0.000003  min_lr: 0.000000  loss: 3.4013 (3.6794)  loss_scale: 65536.0000 (61598.5010)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
[2025-01-13 09:27:13,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=96000, skipped=652, lr=[3.075723261633489e-08, 3.075723261633489e-08, 4.393890373762128e-08, 4.393890373762128e-08, 6.276986248231613e-08, 6.276986248231613e-08, 8.967123211759446e-08, 8.967123211759446e-08, 1.281017601679921e-07, 1.281017601679921e-07, 1.83002514525703e-07, 1.83002514525703e-07, 2.6143216360814715e-07, 2.6143216360814715e-07, 3.734745194402103e-07, 3.734745194402103e-07, 5.335350277717289e-07, 5.335350277717289e-07, 7.621928968167557e-07, 7.621928968167557e-07, 1.0888469954525083e-06, 1.0888469954525083e-06, 1.5554957077892975e-06, 1.5554957077892975e-06, 2.222136725413282e-06, 2.222136725413282e-06, 3.1744810363046893e-06, 3.1744810363046893e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 09:27:13,426] [INFO] [timer.py:260:stop] epoch=0/micro_step=96000/global_step=96000, RunningAvgSamplesPerSec=31.27918103661864, CurrSamplesPerSec=35.15216835612248, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [ 500/2809]  eta: 0:14:46  lr: 0.000003  min_lr: 0.000000  loss: 3.6495 (3.6809)  loss_scale: 65536.0000 (61677.0938)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [34]  [ 510/2809]  eta: 0:14:42  lr: 0.000003  min_lr: 0.000000  loss: 3.5231 (3.6768)  loss_scale: 65536.0000 (61752.6106)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [34]  [ 520/2809]  eta: 0:14:37  lr: 0.000003  min_lr: 0.000000  loss: 3.4395 (3.6756)  loss_scale: 65536.0000 (61825.2284)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [34]  [ 530/2809]  eta: 0:14:33  lr: 0.000003  min_lr: 0.000000  loss: 3.7111 (3.6790)  loss_scale: 65536.0000 (61895.1111)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [34]  [ 540/2809]  eta: 0:14:29  lr: 0.000003  min_lr: 0.000000  loss: 3.9698 (3.6823)  loss_scale: 65536.0000 (61962.4104)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0003  max mem: 15572
Epoch: [34]  [ 550/2809]  eta: 0:14:25  lr: 0.000003  min_lr: 0.000000  loss: 3.9283 (3.6856)  loss_scale: 65536.0000 (62027.2668)  weight_decay: 0.0500 (0.0500)  time: 0.3849  data: 0.0004  max mem: 15572
Epoch: [34]  [ 560/2809]  eta: 0:14:21  lr: 0.000003  min_lr: 0.000000  loss: 3.9059 (3.6873)  loss_scale: 65536.0000 (62089.8111)  weight_decay: 0.0500 (0.0500)  time: 0.3811  data: 0.0003  max mem: 15572
[2025-01-13 09:27:39,429] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96068
[2025-01-13 09:27:39,429] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:27:39,429] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 570/2809]  eta: 0:14:17  lr: 0.000003  min_lr: 0.000000  loss: 3.7194 (3.6854)  loss_scale: 65536.0000 (61633.6813)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0002  max mem: 15572
Epoch: [34]  [ 580/2809]  eta: 0:14:13  lr: 0.000003  min_lr: 0.000000  loss: 3.6428 (3.6869)  loss_scale: 32768.0000 (61136.8537)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [34]  [ 590/2809]  eta: 0:14:09  lr: 0.000003  min_lr: 0.000000  loss: 3.6428 (3.6856)  loss_scale: 32768.0000 (60656.8393)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0002  max mem: 15572
Epoch: [34]  [ 600/2809]  eta: 0:14:05  lr: 0.000003  min_lr: 0.000000  loss: 3.6042 (3.6840)  loss_scale: 32768.0000 (60192.7987)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0002  max mem: 15572
Epoch: [34]  [ 610/2809]  eta: 0:14:00  lr: 0.000003  min_lr: 0.000000  loss: 3.6741 (3.6834)  loss_scale: 32768.0000 (59743.9476)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [34]  [ 620/2809]  eta: 0:13:56  lr: 0.000003  min_lr: 0.000000  loss: 3.8028 (3.6879)  loss_scale: 32768.0000 (59309.5523)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [34]  [ 630/2809]  eta: 0:13:52  lr: 0.000003  min_lr: 0.000000  loss: 3.7865 (3.6858)  loss_scale: 32768.0000 (58888.9255)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [34]  [ 640/2809]  eta: 0:13:48  lr: 0.000003  min_lr: 0.000000  loss: 3.7076 (3.6877)  loss_scale: 32768.0000 (58481.4228)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0001  max mem: 15572
Epoch: [34]  [ 650/2809]  eta: 0:13:44  lr: 0.000003  min_lr: 0.000000  loss: 3.9392 (3.6892)  loss_scale: 32768.0000 (58086.4393)  weight_decay: 0.0500 (0.0500)  time: 0.3793  data: 0.0003  max mem: 15572
Epoch: [34]  [ 660/2809]  eta: 0:13:41  lr: 0.000003  min_lr: 0.000000  loss: 3.7887 (3.6904)  loss_scale: 32768.0000 (57703.4070)  weight_decay: 0.0500 (0.0500)  time: 0.3920  data: 0.0004  max mem: 15572
Epoch: [34]  [ 670/2809]  eta: 0:13:37  lr: 0.000003  min_lr: 0.000000  loss: 3.7864 (3.6914)  loss_scale: 32768.0000 (57331.7914)  weight_decay: 0.0500 (0.0500)  time: 0.3956  data: 0.0004  max mem: 15572
Epoch: [34]  [ 680/2809]  eta: 0:13:34  lr: 0.000003  min_lr: 0.000000  loss: 3.6803 (3.6918)  loss_scale: 32768.0000 (56971.0896)  weight_decay: 0.0500 (0.0500)  time: 0.3895  data: 0.0004  max mem: 15572
Epoch: [34]  [ 690/2809]  eta: 0:13:30  lr: 0.000003  min_lr: 0.000000  loss: 3.7071 (3.6922)  loss_scale: 32768.0000 (56620.8278)  weight_decay: 0.0500 (0.0500)  time: 0.3802  data: 0.0003  max mem: 15572
[2025-01-13 09:28:28,259] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:28:28,259] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 700/2809]  eta: 0:13:25  lr: 0.000003  min_lr: 0.000000  loss: 3.4782 (3.6867)  loss_scale: 32768.0000 (56748.0057)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [34]  [ 710/2809]  eta: 0:13:21  lr: 0.000003  min_lr: 0.000000  loss: 3.4323 (3.6846)  loss_scale: 65536.0000 (56871.6062)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [34]  [ 720/2809]  eta: 0:13:17  lr: 0.000003  min_lr: 0.000000  loss: 3.4323 (3.6824)  loss_scale: 65536.0000 (56991.7781)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [34]  [ 730/2809]  eta: 0:13:13  lr: 0.000003  min_lr: 0.000000  loss: 3.6635 (3.6827)  loss_scale: 65536.0000 (57108.6621)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [34]  [ 740/2809]  eta: 0:13:09  lr: 0.000003  min_lr: 0.000000  loss: 3.6635 (3.6811)  loss_scale: 65536.0000 (57222.3914)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [34]  [ 750/2809]  eta: 0:13:05  lr: 0.000003  min_lr: 0.000000  loss: 3.5295 (3.6795)  loss_scale: 65536.0000 (57333.0919)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0003  max mem: 15572
Epoch: [34]  [ 760/2809]  eta: 0:13:01  lr: 0.000003  min_lr: 0.000000  loss: 3.5570 (3.6791)  loss_scale: 65536.0000 (57440.8830)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0003  max mem: 15572
Epoch: [34]  [ 770/2809]  eta: 0:12:57  lr: 0.000003  min_lr: 0.000000  loss: 3.5715 (3.6789)  loss_scale: 65536.0000 (57545.8781)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [34]  [ 780/2809]  eta: 0:12:53  lr: 0.000003  min_lr: 0.000000  loss: 3.5715 (3.6777)  loss_scale: 65536.0000 (57648.1844)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0003  max mem: 15572
Epoch: [34]  [ 790/2809]  eta: 0:12:50  lr: 0.000003  min_lr: 0.000000  loss: 3.4929 (3.6751)  loss_scale: 65536.0000 (57747.9039)  weight_decay: 0.0500 (0.0500)  time: 0.3901  data: 0.0004  max mem: 15572
Epoch: [34]  [ 800/2809]  eta: 0:12:46  lr: 0.000003  min_lr: 0.000000  loss: 3.5586 (3.6770)  loss_scale: 65536.0000 (57845.1336)  weight_decay: 0.0500 (0.0500)  time: 0.3894  data: 0.0004  max mem: 15572
Epoch: [34]  [ 810/2809]  eta: 0:12:42  lr: 0.000003  min_lr: 0.000000  loss: 3.5753 (3.6758)  loss_scale: 65536.0000 (57939.9655)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0003  max mem: 15572
[2025-01-13 09:29:16,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:29:16,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [ 820/2809]  eta: 0:12:38  lr: 0.000003  min_lr: 0.000000  loss: 3.5700 (3.6736)  loss_scale: 65536.0000 (58192.1364)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
[2025-01-13 09:29:17,880] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96329
[2025-01-13 09:29:17,880] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:29:17,880] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 830/2809]  eta: 0:12:34  lr: 0.000003  min_lr: 0.000000  loss: 3.7074 (3.6747)  loss_scale: 65536.0000 (58438.2383)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
[2025-01-13 09:29:21,600] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96339
[2025-01-13 09:29:21,600] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:29:21,600] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 840/2809]  eta: 0:12:30  lr: 0.000003  min_lr: 0.000000  loss: 3.7074 (3.6738)  loss_scale: 65536.0000 (58210.9298)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [34]  [ 850/2809]  eta: 0:12:26  lr: 0.000003  min_lr: 0.000000  loss: 3.4419 (3.6682)  loss_scale: 32768.0000 (57911.9530)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [34]  [ 860/2809]  eta: 0:12:22  lr: 0.000003  min_lr: 0.000000  loss: 3.5081 (3.6695)  loss_scale: 32768.0000 (57619.9210)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [34]  [ 870/2809]  eta: 0:12:18  lr: 0.000003  min_lr: 0.000000  loss: 3.7871 (3.6724)  loss_scale: 32768.0000 (57334.5947)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [34]  [ 880/2809]  eta: 0:12:14  lr: 0.000003  min_lr: 0.000000  loss: 3.7515 (3.6710)  loss_scale: 32768.0000 (57055.7457)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [34]  [ 890/2809]  eta: 0:12:10  lr: 0.000003  min_lr: 0.000000  loss: 3.6818 (3.6717)  loss_scale: 32768.0000 (56783.1560)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [34]  [ 900/2809]  eta: 0:12:06  lr: 0.000003  min_lr: 0.000000  loss: 3.8830 (3.6741)  loss_scale: 32768.0000 (56516.6171)  weight_decay: 0.0500 (0.0500)  time: 0.3835  data: 0.0004  max mem: 15572
Epoch: [34]  [ 910/2809]  eta: 0:12:03  lr: 0.000003  min_lr: 0.000000  loss: 3.9093 (3.6758)  loss_scale: 32768.0000 (56255.9297)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0005  max mem: 15572
Epoch: [34]  [ 920/2809]  eta: 0:11:59  lr: 0.000003  min_lr: 0.000000  loss: 3.8063 (3.6765)  loss_scale: 32768.0000 (56000.9034)  weight_decay: 0.0500 (0.0500)  time: 0.3912  data: 0.0004  max mem: 15572
Epoch: [34]  [ 930/2809]  eta: 0:11:55  lr: 0.000003  min_lr: 0.000000  loss: 3.7424 (3.6761)  loss_scale: 32768.0000 (55751.3555)  weight_decay: 0.0500 (0.0500)  time: 0.3869  data: 0.0004  max mem: 15572
Epoch: [34]  [ 940/2809]  eta: 0:11:51  lr: 0.000003  min_lr: 0.000000  loss: 3.6314 (3.6773)  loss_scale: 32768.0000 (55507.1116)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0003  max mem: 15572
Epoch: [34]  [ 950/2809]  eta: 0:11:47  lr: 0.000003  min_lr: 0.000000  loss: 3.6314 (3.6787)  loss_scale: 32768.0000 (55268.0042)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [34]  [ 960/2809]  eta: 0:11:43  lr: 0.000003  min_lr: 0.000000  loss: 3.8797 (3.6801)  loss_scale: 32768.0000 (55033.8730)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
[2025-01-13 09:30:10,299] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:30:10,299] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 970/2809]  eta: 0:11:39  lr: 0.000003  min_lr: 0.000000  loss: 3.9840 (3.6819)  loss_scale: 32768.0000 (55108.2842)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
Epoch: [34]  [ 980/2809]  eta: 0:11:35  lr: 0.000003  min_lr: 0.000000  loss: 3.7964 (3.6822)  loss_scale: 65536.0000 (55214.5810)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0003  max mem: 15572
Epoch: [34]  [ 990/2809]  eta: 0:11:31  lr: 0.000003  min_lr: 0.000000  loss: 3.7726 (3.6830)  loss_scale: 65536.0000 (55318.7326)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [34]  [1000/2809]  eta: 0:11:27  lr: 0.000003  min_lr: 0.000000  loss: 3.8108 (3.6833)  loss_scale: 65536.0000 (55420.8032)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [34]  [1010/2809]  eta: 0:11:23  lr: 0.000003  min_lr: 0.000000  loss: 3.9484 (3.6831)  loss_scale: 65536.0000 (55520.8546)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [34]  [1020/2809]  eta: 0:11:20  lr: 0.000003  min_lr: 0.000000  loss: 3.8940 (3.6839)  loss_scale: 65536.0000 (55618.9461)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
Epoch: [34]  [1030/2809]  eta: 0:11:16  lr: 0.000003  min_lr: 0.000000  loss: 3.7831 (3.6850)  loss_scale: 65536.0000 (55715.1348)  weight_decay: 0.0500 (0.0500)  time: 0.3842  data: 0.0003  max mem: 15572
Epoch: [34]  [1040/2809]  eta: 0:11:12  lr: 0.000003  min_lr: 0.000000  loss: 3.6881 (3.6844)  loss_scale: 65536.0000 (55809.4755)  weight_decay: 0.0500 (0.0500)  time: 0.3866  data: 0.0003  max mem: 15572
Epoch: [34]  [1050/2809]  eta: 0:11:08  lr: 0.000003  min_lr: 0.000000  loss: 3.4853 (3.6845)  loss_scale: 65536.0000 (55902.0209)  weight_decay: 0.0500 (0.0500)  time: 0.3779  data: 0.0003  max mem: 15572
Epoch: [34]  [1060/2809]  eta: 0:11:04  lr: 0.000003  min_lr: 0.000000  loss: 3.7425 (3.6877)  loss_scale: 65536.0000 (55992.8219)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0003  max mem: 15572
Epoch: [34]  [1070/2809]  eta: 0:11:01  lr: 0.000003  min_lr: 0.000000  loss: 3.7339 (3.6863)  loss_scale: 65536.0000 (56081.9272)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0003  max mem: 15572
Epoch: [34]  [1080/2809]  eta: 0:10:57  lr: 0.000003  min_lr: 0.000000  loss: 3.7339 (3.6876)  loss_scale: 65536.0000 (56169.3839)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
[2025-01-13 09:30:58,368] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:30:58,368] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [1090/2809]  eta: 0:10:53  lr: 0.000003  min_lr: 0.000000  loss: 3.8700 (3.6902)  loss_scale: 65536.0000 (56315.3071)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0003  max mem: 15572
[2025-01-13 09:30:59,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96599
[2025-01-13 09:30:59,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:30:59,481] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [1100/2809]  eta: 0:10:49  lr: 0.000003  min_lr: 0.000000  loss: 3.8574 (3.6908)  loss_scale: 65536.0000 (56518.1035)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [34]  [1110/2809]  eta: 0:10:45  lr: 0.000003  min_lr: 0.000000  loss: 3.6420 (3.6878)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
Epoch: [34]  [1120/2809]  eta: 0:10:41  lr: 0.000003  min_lr: 0.000000  loss: 3.5808 (3.6885)  loss_scale: 65536.0000 (56678.9938)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0003  max mem: 15572
Epoch: [34]  [1130/2809]  eta: 0:10:37  lr: 0.000003  min_lr: 0.000000  loss: 3.7139 (3.6890)  loss_scale: 65536.0000 (56757.3050)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
Epoch: [34]  [1140/2809]  eta: 0:10:34  lr: 0.000003  min_lr: 0.000000  loss: 3.8123 (3.6890)  loss_scale: 65536.0000 (56834.2436)  weight_decay: 0.0500 (0.0500)  time: 0.3834  data: 0.0003  max mem: 15572
Epoch: [34]  [1150/2809]  eta: 0:10:30  lr: 0.000003  min_lr: 0.000000  loss: 3.8123 (3.6902)  loss_scale: 65536.0000 (56909.8454)  weight_decay: 0.0500 (0.0500)  time: 0.3926  data: 0.0004  max mem: 15572
Epoch: [34]  [1160/2809]  eta: 0:10:26  lr: 0.000003  min_lr: 0.000000  loss: 3.9675 (3.6924)  loss_scale: 65536.0000 (56984.1447)  weight_decay: 0.0500 (0.0500)  time: 0.3921  data: 0.0005  max mem: 15572
Epoch: [34]  [1170/2809]  eta: 0:10:23  lr: 0.000003  min_lr: 0.000000  loss: 3.6702 (3.6911)  loss_scale: 65536.0000 (57057.1751)  weight_decay: 0.0500 (0.0500)  time: 0.3870  data: 0.0004  max mem: 15572
Epoch: [34]  [1180/2809]  eta: 0:10:19  lr: 0.000003  min_lr: 0.000000  loss: 3.5591 (3.6908)  loss_scale: 65536.0000 (57128.9687)  weight_decay: 0.0500 (0.0500)  time: 0.3788  data: 0.0003  max mem: 15572
Epoch: [34]  [1190/2809]  eta: 0:10:15  lr: 0.000003  min_lr: 0.000000  loss: 3.6378 (3.6898)  loss_scale: 65536.0000 (57199.5567)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [34]  [1200/2809]  eta: 0:10:11  lr: 0.000003  min_lr: 0.000000  loss: 3.7270 (3.6906)  loss_scale: 65536.0000 (57268.9692)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [34]  [1210/2809]  eta: 0:10:07  lr: 0.000003  min_lr: 0.000000  loss: 3.6830 (3.6898)  loss_scale: 65536.0000 (57337.2353)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0003  max mem: 15572
Epoch: [34]  [1220/2809]  eta: 0:10:03  lr: 0.000003  min_lr: 0.000000  loss: 3.5971 (3.6902)  loss_scale: 65536.0000 (57404.3833)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 09:31:48,314] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:31:48,314] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:31:48,684] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96729
[2025-01-13 09:31:48,684] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:31:48,684] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [1230/2809]  eta: 0:09:59  lr: 0.000003  min_lr: 0.000000  loss: 3.5966 (3.6912)  loss_scale: 65536.0000 (57523.6783)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [34]  [1240/2809]  eta: 0:09:55  lr: 0.000003  min_lr: 0.000000  loss: 3.7726 (3.6918)  loss_scale: 65536.0000 (57588.2417)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0003  max mem: 15572
Epoch: [34]  [1250/2809]  eta: 0:09:51  lr: 0.000003  min_lr: 0.000000  loss: 3.7539 (3.6930)  loss_scale: 65536.0000 (57651.7730)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
[2025-01-13 09:31:59,440] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96758
[2025-01-13 09:31:59,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:31:59,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1260/2809]  eta: 0:09:47  lr: 0.000003  min_lr: 0.000000  loss: 3.7539 (3.6933)  loss_scale: 65536.0000 (57480.4251)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [34]  [1270/2809]  eta: 0:09:44  lr: 0.000003  min_lr: 0.000000  loss: 3.7155 (3.6926)  loss_scale: 32768.0000 (57285.9921)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0004  max mem: 15572
Epoch: [34]  [1280/2809]  eta: 0:09:40  lr: 0.000003  min_lr: 0.000000  loss: 3.6308 (3.6928)  loss_scale: 32768.0000 (57094.5948)  weight_decay: 0.0500 (0.0500)  time: 0.3912  data: 0.0005  max mem: 15572
Epoch: [34]  [1290/2809]  eta: 0:09:36  lr: 0.000003  min_lr: 0.000000  loss: 3.8454 (3.6943)  loss_scale: 32768.0000 (56906.1627)  weight_decay: 0.0500 (0.0500)  time: 0.3937  data: 0.0005  max mem: 15572
Epoch: [34]  [1300/2809]  eta: 0:09:33  lr: 0.000003  min_lr: 0.000000  loss: 3.6823 (3.6940)  loss_scale: 32768.0000 (56720.6272)  weight_decay: 0.0500 (0.0500)  time: 0.3828  data: 0.0004  max mem: 15572
Epoch: [34]  [1310/2809]  eta: 0:09:29  lr: 0.000003  min_lr: 0.000000  loss: 3.5834 (3.6938)  loss_scale: 32768.0000 (56537.9222)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [34]  [1320/2809]  eta: 0:09:25  lr: 0.000003  min_lr: 0.000000  loss: 3.8347 (3.6941)  loss_scale: 32768.0000 (56357.9833)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [34]  [1330/2809]  eta: 0:09:21  lr: 0.000003  min_lr: 0.000000  loss: 3.5312 (3.6939)  loss_scale: 32768.0000 (56180.7483)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [34]  [1340/2809]  eta: 0:09:17  lr: 0.000003  min_lr: 0.000000  loss: 3.5181 (3.6924)  loss_scale: 32768.0000 (56006.1566)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [34]  [1350/2809]  eta: 0:09:13  lr: 0.000003  min_lr: 0.000000  loss: 3.5277 (3.6910)  loss_scale: 32768.0000 (55834.1495)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0003  max mem: 15572
Epoch: [34]  [1360/2809]  eta: 0:09:09  lr: 0.000003  min_lr: 0.000000  loss: 3.7085 (3.6900)  loss_scale: 32768.0000 (55664.6701)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [34]  [1370/2809]  eta: 0:09:05  lr: 0.000003  min_lr: 0.000000  loss: 3.7411 (3.6905)  loss_scale: 32768.0000 (55497.6630)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0003  max mem: 15572
Epoch: [34]  [1380/2809]  eta: 0:09:01  lr: 0.000003  min_lr: 0.000000  loss: 3.8688 (3.6916)  loss_scale: 32768.0000 (55333.0746)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
[2025-01-13 09:32:47,946] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:32:47,947] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [1390/2809]  eta: 0:08:58  lr: 0.000003  min_lr: 0.000000  loss: 3.8010 (3.6911)  loss_scale: 32768.0000 (55406.4242)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0003  max mem: 15572
Epoch: [34]  [1400/2809]  eta: 0:08:54  lr: 0.000003  min_lr: 0.000000  loss: 3.5571 (3.6895)  loss_scale: 65536.0000 (55478.7266)  weight_decay: 0.0500 (0.0500)  time: 0.3885  data: 0.0004  max mem: 15572
Epoch: [34]  [1410/2809]  eta: 0:08:50  lr: 0.000003  min_lr: 0.000000  loss: 3.4214 (3.6884)  loss_scale: 65536.0000 (55550.0043)  weight_decay: 0.0500 (0.0500)  time: 0.3908  data: 0.0004  max mem: 15572
Epoch: [34]  [1420/2809]  eta: 0:08:47  lr: 0.000003  min_lr: 0.000000  loss: 3.7525 (3.6900)  loss_scale: 65536.0000 (55620.2787)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0004  max mem: 15572
Epoch: [34]  [1430/2809]  eta: 0:08:43  lr: 0.000003  min_lr: 0.000000  loss: 3.7711 (3.6887)  loss_scale: 65536.0000 (55689.5709)  weight_decay: 0.0500 (0.0500)  time: 0.3882  data: 0.0004  max mem: 15572
Epoch: [34]  [1440/2809]  eta: 0:08:39  lr: 0.000003  min_lr: 0.000000  loss: 3.5862 (3.6885)  loss_scale: 65536.0000 (55757.9015)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0003  max mem: 15572
Epoch: [34]  [1450/2809]  eta: 0:08:35  lr: 0.000003  min_lr: 0.000000  loss: 3.6211 (3.6891)  loss_scale: 65536.0000 (55825.2901)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [34]  [1460/2809]  eta: 0:08:31  lr: 0.000003  min_lr: 0.000000  loss: 3.5473 (3.6873)  loss_scale: 65536.0000 (55891.7563)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [34]  [1470/2809]  eta: 0:08:27  lr: 0.000003  min_lr: 0.000000  loss: 3.4422 (3.6862)  loss_scale: 65536.0000 (55957.3188)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [34]  [1480/2809]  eta: 0:08:24  lr: 0.000003  min_lr: 0.000000  loss: 3.5457 (3.6859)  loss_scale: 65536.0000 (56021.9959)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [34]  [1490/2809]  eta: 0:08:20  lr: 0.000003  min_lr: 0.000000  loss: 3.5995 (3.6869)  loss_scale: 65536.0000 (56085.8055)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
[2025-01-13 09:33:30,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=97000, skipped=658, lr=[2.723864396118874e-08, 2.723864396118874e-08, 3.891234851598392e-08, 3.891234851598392e-08, 5.558906930854846e-08, 5.558906930854846e-08, 7.941295615506924e-08, 7.941295615506924e-08, 1.1344708022152749e-07, 1.1344708022152749e-07, 1.62067257459325e-07, 1.62067257459325e-07, 2.3152465351332144e-07, 2.3152465351332144e-07, 3.3074950501903063e-07, 3.3074950501903063e-07, 4.724992928843295e-07, 4.724992928843295e-07, 6.749989898347565e-07, 6.749989898347565e-07, 9.642842711925092e-07, 9.642842711925092e-07, 1.377548958846442e-06, 1.377548958846442e-06, 1.967927084066346e-06, 1.967927084066346e-06, 2.8113244058090655e-06, 2.8113244058090655e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 09:33:30,380] [INFO] [timer.py:260:stop] epoch=0/micro_step=97000/global_step=97000, RunningAvgSamplesPerSec=31.302071430265368, CurrSamplesPerSec=34.90560468703188, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [1500/2809]  eta: 0:08:16  lr: 0.000003  min_lr: 0.000000  loss: 3.9129 (3.6868)  loss_scale: 65536.0000 (56148.7648)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
[2025-01-13 09:33:36,342] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:33:36,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [1510/2809]  eta: 0:08:12  lr: 0.000003  min_lr: 0.000000  loss: 3.9379 (3.6893)  loss_scale: 65536.0000 (56297.6360)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
[2025-01-13 09:33:37,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97018
[2025-01-13 09:33:37,462] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:33:37,462] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [1520/2809]  eta: 0:08:08  lr: 0.000003  min_lr: 0.000000  loss: 4.0515 (3.6903)  loss_scale: 65536.0000 (56401.4622)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [34]  [1530/2809]  eta: 0:08:04  lr: 0.000003  min_lr: 0.000000  loss: 3.8276 (3.6900)  loss_scale: 65536.0000 (56461.1261)  weight_decay: 0.0500 (0.0500)  time: 0.3835  data: 0.0003  max mem: 15572
Epoch: [34]  [1540/2809]  eta: 0:08:01  lr: 0.000003  min_lr: 0.000000  loss: 3.6221 (3.6887)  loss_scale: 65536.0000 (56520.0156)  weight_decay: 0.0500 (0.0500)  time: 0.3924  data: 0.0004  max mem: 15572
Epoch: [34]  [1550/2809]  eta: 0:07:57  lr: 0.000003  min_lr: 0.000000  loss: 3.7599 (3.6893)  loss_scale: 65536.0000 (56578.1457)  weight_decay: 0.0500 (0.0500)  time: 0.3868  data: 0.0004  max mem: 15572
Epoch: [34]  [1560/2809]  eta: 0:07:53  lr: 0.000003  min_lr: 0.000000  loss: 3.8335 (3.6906)  loss_scale: 65536.0000 (56635.5311)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
Epoch: [34]  [1570/2809]  eta: 0:07:49  lr: 0.000003  min_lr: 0.000000  loss: 3.8318 (3.6907)  loss_scale: 65536.0000 (56692.1859)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [34]  [1580/2809]  eta: 0:07:45  lr: 0.000003  min_lr: 0.000000  loss: 3.6588 (3.6906)  loss_scale: 65536.0000 (56748.1240)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [34]  [1590/2809]  eta: 0:07:42  lr: 0.000003  min_lr: 0.000000  loss: 3.6655 (3.6910)  loss_scale: 65536.0000 (56803.3589)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [34]  [1600/2809]  eta: 0:07:38  lr: 0.000003  min_lr: 0.000000  loss: 3.8916 (3.6924)  loss_scale: 65536.0000 (56857.9038)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0002  max mem: 15572
Epoch: [34]  [1610/2809]  eta: 0:07:34  lr: 0.000003  min_lr: 0.000000  loss: 3.8666 (3.6919)  loss_scale: 65536.0000 (56911.7716)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
Epoch: [34]  [1620/2809]  eta: 0:07:30  lr: 0.000003  min_lr: 0.000000  loss: 3.7376 (3.6912)  loss_scale: 65536.0000 (56964.9747)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [34]  [1630/2809]  eta: 0:07:26  lr: 0.000003  min_lr: 0.000000  loss: 3.6411 (3.6905)  loss_scale: 65536.0000 (57017.5254)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [34]  [1640/2809]  eta: 0:07:23  lr: 0.000003  min_lr: 0.000000  loss: 3.7342 (3.6915)  loss_scale: 65536.0000 (57069.4357)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0003  max mem: 15572
[2025-01-13 09:34:26,196] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:34:26,196] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:34:26,969] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97149
[2025-01-13 09:34:26,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:34:26,969] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [1650/2809]  eta: 0:07:19  lr: 0.000003  min_lr: 0.000000  loss: 3.5379 (3.6903)  loss_scale: 65536.0000 (57200.1066)  weight_decay: 0.0500 (0.0500)  time: 0.3841  data: 0.0004  max mem: 15572
Epoch: [34]  [1660/2809]  eta: 0:07:15  lr: 0.000003  min_lr: 0.000000  loss: 3.5145 (3.6899)  loss_scale: 65536.0000 (57250.2926)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0005  max mem: 15572
Epoch: [34]  [1670/2809]  eta: 0:07:11  lr: 0.000003  min_lr: 0.000000  loss: 3.6407 (3.6901)  loss_scale: 65536.0000 (57299.8779)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0006  max mem: 15572
Epoch: [34]  [1680/2809]  eta: 0:07:08  lr: 0.000003  min_lr: 0.000000  loss: 3.8897 (3.6912)  loss_scale: 65536.0000 (57348.8733)  weight_decay: 0.0500 (0.0500)  time: 0.3891  data: 0.0005  max mem: 15572
Epoch: [34]  [1690/2809]  eta: 0:07:04  lr: 0.000003  min_lr: 0.000000  loss: 3.7900 (3.6907)  loss_scale: 65536.0000 (57397.2892)  weight_decay: 0.0500 (0.0500)  time: 0.3773  data: 0.0003  max mem: 15572
Epoch: [34]  [1700/2809]  eta: 0:07:00  lr: 0.000003  min_lr: 0.000000  loss: 3.4258 (3.6900)  loss_scale: 65536.0000 (57445.1358)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [34]  [1710/2809]  eta: 0:06:56  lr: 0.000003  min_lr: 0.000000  loss: 3.7348 (3.6906)  loss_scale: 65536.0000 (57492.4231)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [34]  [1720/2809]  eta: 0:06:52  lr: 0.000003  min_lr: 0.000000  loss: 3.7348 (3.6911)  loss_scale: 65536.0000 (57539.1610)  weight_decay: 0.0500 (0.0500)  time: 0.3804  data: 0.0004  max mem: 15572
Epoch: [34]  [1730/2809]  eta: 0:06:49  lr: 0.000003  min_lr: 0.000000  loss: 3.6521 (3.6911)  loss_scale: 65536.0000 (57585.3588)  weight_decay: 0.0500 (0.0500)  time: 0.3835  data: 0.0004  max mem: 15572
Epoch: [34]  [1740/2809]  eta: 0:06:45  lr: 0.000003  min_lr: 0.000000  loss: 3.5508 (3.6912)  loss_scale: 65536.0000 (57631.0258)  weight_decay: 0.0500 (0.0500)  time: 0.3847  data: 0.0004  max mem: 15572
Epoch: [34]  [1750/2809]  eta: 0:06:41  lr: 0.000003  min_lr: 0.000000  loss: 3.7120 (3.6907)  loss_scale: 65536.0000 (57676.1713)  weight_decay: 0.0500 (0.0500)  time: 0.3787  data: 0.0003  max mem: 15572
Epoch: [34]  [1760/2809]  eta: 0:06:37  lr: 0.000003  min_lr: 0.000000  loss: 3.4548 (3.6890)  loss_scale: 65536.0000 (57720.8041)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [34]  [1770/2809]  eta: 0:06:33  lr: 0.000003  min_lr: 0.000000  loss: 3.6967 (3.6896)  loss_scale: 65536.0000 (57764.9328)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
[2025-01-13 09:35:15,969] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:35:15,969] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:35:16,764] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97280
[2025-01-13 09:35:16,764] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:35:16,764] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [1780/2809]  eta: 0:06:30  lr: 0.000003  min_lr: 0.000000  loss: 3.7439 (3.6894)  loss_scale: 65536.0000 (57882.1606)  weight_decay: 0.0500 (0.0500)  time: 0.3852  data: 0.0003  max mem: 15572
Epoch: [34]  [1790/2809]  eta: 0:06:26  lr: 0.000003  min_lr: 0.000000  loss: 3.8080 (3.6896)  loss_scale: 65536.0000 (57924.8956)  weight_decay: 0.0500 (0.0500)  time: 0.3909  data: 0.0004  max mem: 15572
Epoch: [34]  [1800/2809]  eta: 0:06:22  lr: 0.000003  min_lr: 0.000000  loss: 3.9912 (3.6900)  loss_scale: 65536.0000 (57967.1560)  weight_decay: 0.0500 (0.0500)  time: 0.3861  data: 0.0005  max mem: 15572
Epoch: [34]  [1810/2809]  eta: 0:06:18  lr: 0.000003  min_lr: 0.000000  loss: 3.8150 (3.6901)  loss_scale: 65536.0000 (58008.9498)  weight_decay: 0.0500 (0.0500)  time: 0.3825  data: 0.0004  max mem: 15572
Epoch: [34]  [1820/2809]  eta: 0:06:15  lr: 0.000003  min_lr: 0.000000  loss: 3.6373 (3.6895)  loss_scale: 65536.0000 (58050.2845)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0003  max mem: 15572
Epoch: [34]  [1830/2809]  eta: 0:06:11  lr: 0.000003  min_lr: 0.000000  loss: 3.5057 (3.6900)  loss_scale: 65536.0000 (58091.1677)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
Epoch: [34]  [1840/2809]  eta: 0:06:07  lr: 0.000003  min_lr: 0.000000  loss: 3.8669 (3.6902)  loss_scale: 65536.0000 (58131.6067)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [34]  [1850/2809]  eta: 0:06:03  lr: 0.000003  min_lr: 0.000000  loss: 3.8669 (3.6917)  loss_scale: 65536.0000 (58171.6089)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [34]  [1860/2809]  eta: 0:05:59  lr: 0.000003  min_lr: 0.000000  loss: 3.8734 (3.6921)  loss_scale: 65536.0000 (58211.1811)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0020  max mem: 15572
Epoch: [34]  [1870/2809]  eta: 0:05:55  lr: 0.000003  min_lr: 0.000000  loss: 3.7619 (3.6919)  loss_scale: 65536.0000 (58250.3303)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0020  max mem: 15572
Epoch: [34]  [1880/2809]  eta: 0:05:52  lr: 0.000003  min_lr: 0.000000  loss: 3.7196 (3.6920)  loss_scale: 65536.0000 (58289.0633)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [34]  [1890/2809]  eta: 0:05:48  lr: 0.000003  min_lr: 0.000000  loss: 3.8964 (3.6933)  loss_scale: 65536.0000 (58327.3866)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0003  max mem: 15572
Epoch: [34]  [1900/2809]  eta: 0:05:44  lr: 0.000003  min_lr: 0.000000  loss: 3.7982 (3.6922)  loss_scale: 65536.0000 (58365.3067)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0004  max mem: 15572
[2025-01-13 09:36:05,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:36:05,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:36:06,021] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97410
[2025-01-13 09:36:06,022] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:36:06,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [1910/2809]  eta: 0:05:40  lr: 0.000003  min_lr: 0.000000  loss: 3.7124 (3.6924)  loss_scale: 65536.0000 (58437.1240)  weight_decay: 0.0500 (0.0500)  time: 0.3903  data: 0.0005  max mem: 15572
Epoch: [34]  [1920/2809]  eta: 0:05:37  lr: 0.000003  min_lr: 0.000000  loss: 3.5842 (3.6904)  loss_scale: 65536.0000 (58474.0781)  weight_decay: 0.0500 (0.0500)  time: 0.3890  data: 0.0005  max mem: 15572
Epoch: [34]  [1930/2809]  eta: 0:05:33  lr: 0.000003  min_lr: 0.000000  loss: 3.6054 (3.6905)  loss_scale: 65536.0000 (58510.6494)  weight_decay: 0.0500 (0.0500)  time: 0.3867  data: 0.0004  max mem: 15572
Epoch: [34]  [1940/2809]  eta: 0:05:29  lr: 0.000003  min_lr: 0.000000  loss: 3.7273 (3.6907)  loss_scale: 65536.0000 (58546.8439)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0004  max mem: 15572
Epoch: [34]  [1950/2809]  eta: 0:05:25  lr: 0.000003  min_lr: 0.000000  loss: 3.6344 (3.6904)  loss_scale: 65536.0000 (58582.6674)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0003  max mem: 15572
Epoch: [34]  [1960/2809]  eta: 0:05:21  lr: 0.000003  min_lr: 0.000000  loss: 3.6344 (3.6894)  loss_scale: 65536.0000 (58618.1254)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0003  max mem: 15572
Epoch: [34]  [1970/2809]  eta: 0:05:18  lr: 0.000003  min_lr: 0.000000  loss: 3.4646 (3.6883)  loss_scale: 65536.0000 (58653.2237)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0003  max mem: 15572
Epoch: [34]  [1980/2809]  eta: 0:05:14  lr: 0.000003  min_lr: 0.000000  loss: 3.4241 (3.6870)  loss_scale: 65536.0000 (58687.9677)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
[2025-01-13 09:36:36,922] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97492
[2025-01-13 09:36:36,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:36:36,922] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1990/2809]  eta: 0:05:10  lr: 0.000003  min_lr: 0.000000  loss: 3.2864 (3.6847)  loss_scale: 65536.0000 (58640.0723)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [34]  [2000/2809]  eta: 0:05:06  lr: 0.000003  min_lr: 0.000000  loss: 3.5887 (3.6851)  loss_scale: 32768.0000 (58510.7766)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [34]  [2010/2809]  eta: 0:05:02  lr: 0.000003  min_lr: 0.000000  loss: 3.6966 (3.6843)  loss_scale: 32768.0000 (58382.7668)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [34]  [2020/2809]  eta: 0:04:58  lr: 0.000003  min_lr: 0.000000  loss: 3.4290 (3.6830)  loss_scale: 32768.0000 (58256.0238)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [34]  [2030/2809]  eta: 0:04:55  lr: 0.000003  min_lr: 0.000000  loss: 3.7773 (3.6837)  loss_scale: 32768.0000 (58130.5288)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0002  max mem: 15572
Epoch: [34]  [2040/2809]  eta: 0:04:51  lr: 0.000003  min_lr: 0.000000  loss: 3.8438 (3.6832)  loss_scale: 32768.0000 (58006.2636)  weight_decay: 0.0500 (0.0500)  time: 0.3910  data: 0.0003  max mem: 15572
Epoch: [34]  [2050/2809]  eta: 0:04:47  lr: 0.000003  min_lr: 0.000000  loss: 3.8348 (3.6840)  loss_scale: 32768.0000 (57883.2101)  weight_decay: 0.0500 (0.0500)  time: 0.3963  data: 0.0004  max mem: 15572
Epoch: [34]  [2060/2809]  eta: 0:04:43  lr: 0.000003  min_lr: 0.000000  loss: 3.5930 (3.6828)  loss_scale: 32768.0000 (57761.3508)  weight_decay: 0.0500 (0.0500)  time: 0.3856  data: 0.0003  max mem: 15572
Epoch: [34]  [2070/2809]  eta: 0:04:40  lr: 0.000003  min_lr: 0.000000  loss: 3.7040 (3.6837)  loss_scale: 32768.0000 (57640.6683)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0002  max mem: 15572
Epoch: [34]  [2080/2809]  eta: 0:04:36  lr: 0.000003  min_lr: 0.000000  loss: 3.8454 (3.6839)  loss_scale: 32768.0000 (57521.1456)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0002  max mem: 15572
Epoch: [34]  [2090/2809]  eta: 0:04:32  lr: 0.000003  min_lr: 0.000000  loss: 3.9114 (3.6851)  loss_scale: 32768.0000 (57402.7661)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0003  max mem: 15572
Epoch: [34]  [2100/2809]  eta: 0:04:28  lr: 0.000003  min_lr: 0.000000  loss: 3.7983 (3.6850)  loss_scale: 32768.0000 (57285.5136)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0003  max mem: 15572
Epoch: [34]  [2110/2809]  eta: 0:04:24  lr: 0.000003  min_lr: 0.000000  loss: 3.5653 (3.6846)  loss_scale: 32768.0000 (57169.3719)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
[2025-01-13 09:37:25,650] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:37:25,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [2120/2809]  eta: 0:04:21  lr: 0.000003  min_lr: 0.000000  loss: 3.6908 (3.6845)  loss_scale: 32768.0000 (57147.0212)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [34]  [2130/2809]  eta: 0:04:17  lr: 0.000003  min_lr: 0.000000  loss: 3.6233 (3.6833)  loss_scale: 65536.0000 (57186.3876)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [34]  [2140/2809]  eta: 0:04:13  lr: 0.000003  min_lr: 0.000000  loss: 3.4748 (3.6816)  loss_scale: 65536.0000 (57225.3863)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
Epoch: [34]  [2150/2809]  eta: 0:04:09  lr: 0.000003  min_lr: 0.000000  loss: 3.6969 (3.6818)  loss_scale: 65536.0000 (57264.0223)  weight_decay: 0.0500 (0.0500)  time: 0.3804  data: 0.0003  max mem: 15572
Epoch: [34]  [2160/2809]  eta: 0:04:05  lr: 0.000003  min_lr: 0.000000  loss: 3.8615 (3.6822)  loss_scale: 65536.0000 (57302.3008)  weight_decay: 0.0500 (0.0500)  time: 0.3873  data: 0.0004  max mem: 15572
Epoch: [34]  [2170/2809]  eta: 0:04:02  lr: 0.000003  min_lr: 0.000000  loss: 3.9402 (3.6833)  loss_scale: 65536.0000 (57340.2266)  weight_decay: 0.0500 (0.0500)  time: 0.3858  data: 0.0004  max mem: 15572
Epoch: [34]  [2180/2809]  eta: 0:03:58  lr: 0.000003  min_lr: 0.000000  loss: 3.9562 (3.6832)  loss_scale: 65536.0000 (57377.8047)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0003  max mem: 15572
Epoch: [34]  [2190/2809]  eta: 0:03:54  lr: 0.000003  min_lr: 0.000000  loss: 3.7147 (3.6826)  loss_scale: 65536.0000 (57415.0397)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [34]  [2200/2809]  eta: 0:03:50  lr: 0.000003  min_lr: 0.000000  loss: 3.7145 (3.6825)  loss_scale: 65536.0000 (57451.9364)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [34]  [2210/2809]  eta: 0:03:46  lr: 0.000003  min_lr: 0.000000  loss: 3.6639 (3.6823)  loss_scale: 65536.0000 (57488.4993)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [34]  [2220/2809]  eta: 0:03:43  lr: 0.000003  min_lr: 0.000000  loss: 3.6284 (3.6822)  loss_scale: 65536.0000 (57524.7330)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [34]  [2230/2809]  eta: 0:03:39  lr: 0.000003  min_lr: 0.000000  loss: 3.8622 (3.6825)  loss_scale: 65536.0000 (57560.6419)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [34]  [2240/2809]  eta: 0:03:35  lr: 0.000003  min_lr: 0.000000  loss: 3.8413 (3.6820)  loss_scale: 65536.0000 (57596.2303)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
[2025-01-13 09:38:13,610] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:38:13,610] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:38:14,352] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97751
[2025-01-13 09:38:14,352] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:38:14,352] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [2250/2809]  eta: 0:03:31  lr: 0.000003  min_lr: 0.000000  loss: 3.6185 (3.6817)  loss_scale: 65536.0000 (57689.7308)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [34]  [2260/2809]  eta: 0:03:27  lr: 0.000003  min_lr: 0.000000  loss: 3.2471 (3.6798)  loss_scale: 65536.0000 (57724.4334)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0002  max mem: 15572
Epoch: [34]  [2270/2809]  eta: 0:03:24  lr: 0.000003  min_lr: 0.000000  loss: 3.6950 (3.6808)  loss_scale: 65536.0000 (57758.8305)  weight_decay: 0.0500 (0.0500)  time: 0.3853  data: 0.0004  max mem: 15572
Epoch: [34]  [2280/2809]  eta: 0:03:20  lr: 0.000003  min_lr: 0.000000  loss: 3.9144 (3.6810)  loss_scale: 65536.0000 (57792.9259)  weight_decay: 0.0500 (0.0500)  time: 0.3834  data: 0.0004  max mem: 15572
Epoch: [34]  [2290/2809]  eta: 0:03:16  lr: 0.000003  min_lr: 0.000000  loss: 3.6670 (3.6810)  loss_scale: 65536.0000 (57826.7237)  weight_decay: 0.0500 (0.0500)  time: 0.3846  data: 0.0005  max mem: 15572
Epoch: [34]  [2300/2809]  eta: 0:03:12  lr: 0.000003  min_lr: 0.000000  loss: 3.7464 (3.6814)  loss_scale: 65536.0000 (57860.2277)  weight_decay: 0.0500 (0.0500)  time: 0.3838  data: 0.0004  max mem: 15572
Epoch: [34]  [2310/2809]  eta: 0:03:08  lr: 0.000003  min_lr: 0.000000  loss: 3.6073 (3.6806)  loss_scale: 65536.0000 (57893.4418)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0003  max mem: 15572
Epoch: [34]  [2320/2809]  eta: 0:03:05  lr: 0.000003  min_lr: 0.000000  loss: 3.4861 (3.6801)  loss_scale: 65536.0000 (57926.3697)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [34]  [2330/2809]  eta: 0:03:01  lr: 0.000003  min_lr: 0.000000  loss: 3.3972 (3.6784)  loss_scale: 65536.0000 (57959.0150)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [34]  [2340/2809]  eta: 0:02:57  lr: 0.000003  min_lr: 0.000000  loss: 3.5448 (3.6788)  loss_scale: 65536.0000 (57991.3815)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [34]  [2350/2809]  eta: 0:02:53  lr: 0.000003  min_lr: 0.000000  loss: 3.8472 (3.6790)  loss_scale: 65536.0000 (58023.4726)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [34]  [2360/2809]  eta: 0:02:49  lr: 0.000003  min_lr: 0.000000  loss: 3.7384 (3.6793)  loss_scale: 65536.0000 (58055.2918)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [34]  [2370/2809]  eta: 0:02:46  lr: 0.000003  min_lr: 0.000000  loss: 3.7359 (3.6796)  loss_scale: 65536.0000 (58086.8427)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
[2025-01-13 09:39:02,753] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:39:02,753] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:39:03,130] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97881
[2025-01-13 09:39:03,130] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:39:03,130] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [2380/2809]  eta: 0:02:42  lr: 0.000003  min_lr: 0.000000  loss: 3.7359 (3.6794)  loss_scale: 65536.0000 (58145.6531)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [34]  [2390/2809]  eta: 0:02:38  lr: 0.000003  min_lr: 0.000000  loss: 3.8134 (3.6797)  loss_scale: 65536.0000 (58176.5621)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0003  max mem: 15572
Epoch: [34]  [2400/2809]  eta: 0:02:34  lr: 0.000003  min_lr: 0.000000  loss: 3.7045 (3.6787)  loss_scale: 65536.0000 (58207.2137)  weight_decay: 0.0500 (0.0500)  time: 0.3808  data: 0.0004  max mem: 15572
Epoch: [34]  [2410/2809]  eta: 0:02:30  lr: 0.000002  min_lr: 0.000000  loss: 3.3268 (3.6779)  loss_scale: 65536.0000 (58237.6109)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0003  max mem: 15572
Epoch: [34]  [2420/2809]  eta: 0:02:27  lr: 0.000002  min_lr: 0.000000  loss: 3.7153 (3.6784)  loss_scale: 65536.0000 (58267.7571)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [34]  [2430/2809]  eta: 0:02:23  lr: 0.000002  min_lr: 0.000000  loss: 3.6744 (3.6781)  loss_scale: 65536.0000 (58297.6553)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [34]  [2440/2809]  eta: 0:02:19  lr: 0.000002  min_lr: 0.000000  loss: 3.6552 (3.6786)  loss_scale: 65536.0000 (58327.3085)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [34]  [2450/2809]  eta: 0:02:15  lr: 0.000002  min_lr: 0.000000  loss: 3.6746 (3.6783)  loss_scale: 65536.0000 (58356.7197)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [34]  [2460/2809]  eta: 0:02:12  lr: 0.000002  min_lr: 0.000000  loss: 3.5883 (3.6781)  loss_scale: 65536.0000 (58385.8919)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [34]  [2470/2809]  eta: 0:02:08  lr: 0.000002  min_lr: 0.000000  loss: 3.5883 (3.6773)  loss_scale: 65536.0000 (58414.8280)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [34]  [2480/2809]  eta: 0:02:04  lr: 0.000002  min_lr: 0.000000  loss: 3.6249 (3.6775)  loss_scale: 65536.0000 (58443.5308)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [34]  [2490/2809]  eta: 0:02:00  lr: 0.000002  min_lr: 0.000000  loss: 3.7504 (3.6783)  loss_scale: 65536.0000 (58472.0032)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0002  max mem: 15572
[2025-01-13 09:39:47,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=98000, skipped=665, lr=[2.3924327692572e-08, 2.3924327692572e-08, 3.417761098938857e-08, 3.417761098938857e-08, 4.88251585562694e-08, 4.88251585562694e-08, 6.975022650895629e-08, 6.975022650895629e-08, 9.964318072708041e-08, 9.964318072708041e-08, 1.4234740103868632e-07, 1.4234740103868632e-07, 2.0335343005526615e-07, 2.0335343005526615e-07, 2.905049000789517e-07, 2.905049000789517e-07, 4.1500700011278813e-07, 4.1500700011278813e-07, 5.928671430182688e-07, 5.928671430182688e-07, 8.469530614546697e-07, 8.469530614546697e-07, 1.2099329449352427e-06, 1.2099329449352427e-06, 1.7284756356217752e-06, 1.7284756356217752e-06, 2.4692509080311077e-06, 2.4692509080311077e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 09:39:47,381] [INFO] [timer.py:260:stop] epoch=0/micro_step=98000/global_step=98000, RunningAvgSamplesPerSec=31.32447849845687, CurrSamplesPerSec=34.925563037432205, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [2500/2809]  eta: 0:01:56  lr: 0.000002  min_lr: 0.000000  loss: 3.6887 (3.6778)  loss_scale: 65536.0000 (58500.2479)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0003  max mem: 15572
[2025-01-13 09:39:51,629] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:39:51,630] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:39:52,805] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98013
[2025-01-13 09:39:52,805] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:39:52,805] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [2510/2809]  eta: 0:01:53  lr: 0.000002  min_lr: 0.000000  loss: 3.6478 (3.6779)  loss_scale: 65536.0000 (58606.5663)  weight_decay: 0.0500 (0.0500)  time: 0.3845  data: 0.0004  max mem: 15572
Epoch: [34]  [2520/2809]  eta: 0:01:49  lr: 0.000002  min_lr: 0.000000  loss: 3.6958 (3.6781)  loss_scale: 65536.0000 (58634.0532)  weight_decay: 0.0500 (0.0500)  time: 0.3925  data: 0.0005  max mem: 15572
Epoch: [34]  [2530/2809]  eta: 0:01:45  lr: 0.000002  min_lr: 0.000000  loss: 3.4577 (3.6778)  loss_scale: 65536.0000 (58661.3228)  weight_decay: 0.0500 (0.0500)  time: 0.3935  data: 0.0005  max mem: 15572
Epoch: [34]  [2540/2809]  eta: 0:01:41  lr: 0.000002  min_lr: 0.000000  loss: 3.4577 (3.6769)  loss_scale: 65536.0000 (58688.3778)  weight_decay: 0.0500 (0.0500)  time: 0.3963  data: 0.0005  max mem: 15572
Epoch: [34]  [2550/2809]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000000  loss: 3.5124 (3.6765)  loss_scale: 65536.0000 (58715.2207)  weight_decay: 0.0500 (0.0500)  time: 0.3821  data: 0.0003  max mem: 15572
Epoch: [34]  [2560/2809]  eta: 0:01:34  lr: 0.000002  min_lr: 0.000000  loss: 3.7612 (3.6772)  loss_scale: 65536.0000 (58741.8540)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [34]  [2570/2809]  eta: 0:01:30  lr: 0.000002  min_lr: 0.000000  loss: 3.9324 (3.6780)  loss_scale: 65536.0000 (58768.2800)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [34]  [2580/2809]  eta: 0:01:26  lr: 0.000002  min_lr: 0.000000  loss: 3.8008 (3.6781)  loss_scale: 65536.0000 (58794.5014)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [34]  [2590/2809]  eta: 0:01:22  lr: 0.000002  min_lr: 0.000000  loss: 3.5254 (3.6773)  loss_scale: 65536.0000 (58820.5203)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [34]  [2600/2809]  eta: 0:01:19  lr: 0.000002  min_lr: 0.000000  loss: 3.3403 (3.6765)  loss_scale: 65536.0000 (58846.3391)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [34]  [2610/2809]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 3.5374 (3.6764)  loss_scale: 65536.0000 (58871.9602)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [34]  [2620/2809]  eta: 0:01:11  lr: 0.000002  min_lr: 0.000000  loss: 3.5374 (3.6754)  loss_scale: 65536.0000 (58897.3857)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [34]  [2630/2809]  eta: 0:01:07  lr: 0.000002  min_lr: 0.000000  loss: 3.3893 (3.6748)  loss_scale: 65536.0000 (58922.6180)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
[2025-01-13 09:40:41,507] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:40:41,507] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:40:41,885] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98143
[2025-01-13 09:40:41,886] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:40:41,886] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [2640/2809]  eta: 0:01:03  lr: 0.000002  min_lr: 0.000000  loss: 3.5710 (3.6740)  loss_scale: 65536.0000 (58972.4741)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0005  max mem: 15572
Epoch: [34]  [2650/2809]  eta: 0:01:00  lr: 0.000002  min_lr: 0.000000  loss: 3.6451 (3.6744)  loss_scale: 65536.0000 (58997.2327)  weight_decay: 0.0500 (0.0500)  time: 0.3839  data: 0.0005  max mem: 15572
Epoch: [34]  [2660/2809]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 3.7635 (3.6745)  loss_scale: 65536.0000 (59021.8053)  weight_decay: 0.0500 (0.0500)  time: 0.3830  data: 0.0004  max mem: 15572
Epoch: [34]  [2670/2809]  eta: 0:00:52  lr: 0.000002  min_lr: 0.000000  loss: 3.5928 (3.6730)  loss_scale: 65536.0000 (59046.1939)  weight_decay: 0.0500 (0.0500)  time: 0.3806  data: 0.0004  max mem: 15572
Epoch: [34]  [2680/2809]  eta: 0:00:48  lr: 0.000002  min_lr: 0.000000  loss: 3.5413 (3.6727)  loss_scale: 65536.0000 (59070.4006)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
Epoch: [34]  [2690/2809]  eta: 0:00:45  lr: 0.000002  min_lr: 0.000000  loss: 3.8033 (3.6729)  loss_scale: 65536.0000 (59094.4274)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [34]  [2700/2809]  eta: 0:00:41  lr: 0.000002  min_lr: 0.000000  loss: 3.6028 (3.6717)  loss_scale: 65536.0000 (59118.2762)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [34]  [2710/2809]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 3.6042 (3.6718)  loss_scale: 65536.0000 (59141.9491)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [34]  [2720/2809]  eta: 0:00:33  lr: 0.000002  min_lr: 0.000000  loss: 3.7076 (3.6716)  loss_scale: 65536.0000 (59165.4480)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0003  max mem: 15572
Epoch: [34]  [2730/2809]  eta: 0:00:29  lr: 0.000002  min_lr: 0.000000  loss: 3.6336 (3.6704)  loss_scale: 65536.0000 (59188.7748)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [34]  [2740/2809]  eta: 0:00:26  lr: 0.000002  min_lr: 0.000000  loss: 3.6336 (3.6699)  loss_scale: 65536.0000 (59211.9314)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0003  max mem: 15572
Epoch: [34]  [2750/2809]  eta: 0:00:22  lr: 0.000002  min_lr: 0.000000  loss: 3.9669 (3.6706)  loss_scale: 65536.0000 (59234.9197)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [34]  [2760/2809]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 3.7536 (3.6705)  loss_scale: 65536.0000 (59257.7414)  weight_decay: 0.0500 (0.0500)  time: 0.3864  data: 0.0004  max mem: 15572
[2025-01-13 09:41:30,460] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:41:30,460] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:41:30,840] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98273
[2025-01-13 09:41:30,840] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:41:30,840] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [2770/2809]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 3.7399 (3.6708)  loss_scale: 65536.0000 (59304.0491)  weight_decay: 0.0500 (0.0500)  time: 0.3953  data: 0.0004  max mem: 15572
Epoch: [34]  [2780/2809]  eta: 0:00:10  lr: 0.000002  min_lr: 0.000000  loss: 3.8158 (3.6707)  loss_scale: 65536.0000 (59326.4581)  weight_decay: 0.0500 (0.0500)  time: 0.3948  data: 0.0005  max mem: 15572
Epoch: [34]  [2790/2809]  eta: 0:00:07  lr: 0.000002  min_lr: 0.000000  loss: 3.7699 (3.6711)  loss_scale: 65536.0000 (59348.7066)  weight_decay: 0.0500 (0.0500)  time: 0.3949  data: 0.0006  max mem: 15572
Epoch: [34]  [2800/2809]  eta: 0:00:03  lr: 0.000002  min_lr: 0.000000  loss: 3.7737 (3.6714)  loss_scale: 65536.0000 (59370.7961)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0004  max mem: 15572
Epoch: [34]  [2808/2809]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 3.8502 (3.6718)  loss_scale: 65536.0000 (59388.3546)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0001  max mem: 15572
Epoch: [34] Total time: 0:17:42 (0.3784 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 3.8502 (3.6718)  loss_scale: 65536.0000 (59388.3546)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:12:04  loss: 0.3748 (0.3748)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6621  data: 2.4968  max mem: 15572
Val:  [ 10/272]  eta: 0:01:42  loss: 2.3235 (2.2309)  acc1: 50.0000 (45.4545)  acc5: 77.7778 (73.7374)  time: 0.3919  data: 0.2440  max mem: 15572
Val:  [ 20/272]  eta: 0:01:12  loss: 2.2804 (2.2737)  acc1: 50.0000 (47.3545)  acc5: 72.2222 (74.3386)  time: 0.1674  data: 0.0096  max mem: 15572
Val:  [ 30/272]  eta: 0:00:59  loss: 2.3442 (2.3621)  acc1: 44.4444 (43.9068)  acc5: 72.2222 (74.3728)  time: 0.1685  data: 0.0106  max mem: 15572
Val:  [ 40/272]  eta: 0:00:53  loss: 2.5264 (2.4131)  acc1: 27.7778 (41.0569)  acc5: 72.2222 (74.1192)  time: 0.1766  data: 0.0317  max mem: 15572
Val:  [ 50/272]  eta: 0:00:48  loss: 2.4827 (2.3427)  acc1: 33.3333 (43.2462)  acc5: 72.2222 (75.5991)  time: 0.1735  data: 0.0216  max mem: 15572
Val:  [ 60/272]  eta: 0:00:44  loss: 1.4737 (2.2374)  acc1: 61.1111 (46.6302)  acc5: 88.8889 (76.5938)  time: 0.1578  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:40  loss: 1.4737 (2.1541)  acc1: 66.6667 (49.3740)  acc5: 83.3333 (77.6995)  time: 0.1629  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:38  loss: 1.8636 (2.1689)  acc1: 61.1111 (49.3141)  acc5: 83.3333 (77.2291)  time: 0.1760  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:35  loss: 2.1321 (2.1705)  acc1: 50.0000 (49.4505)  acc5: 77.7778 (77.6557)  time: 0.1680  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:33  loss: 2.0876 (2.2000)  acc1: 50.0000 (48.6249)  acc5: 77.7778 (77.2827)  time: 0.1603  data: 0.0005  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 2.4217 (2.2758)  acc1: 22.2222 (46.4464)  acc5: 72.2222 (76.2262)  time: 0.1750  data: 0.0096  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 2.8862 (2.3136)  acc1: 16.6667 (45.7759)  acc5: 72.2222 (75.8035)  time: 0.1840  data: 0.0158  max mem: 15572
Val:  [130/272]  eta: 0:00:26  loss: 2.1075 (2.2803)  acc1: 50.0000 (46.7345)  acc5: 77.7778 (76.4631)  time: 0.1774  data: 0.0067  max mem: 15572
Val:  [140/272]  eta: 0:00:24  loss: 1.7369 (2.2737)  acc1: 55.5556 (47.0843)  acc5: 88.8889 (76.3987)  time: 0.1676  data: 0.0005  max mem: 15572
Val:  [150/272]  eta: 0:00:22  loss: 2.2739 (2.2795)  acc1: 44.4444 (46.4312)  acc5: 77.7778 (76.6004)  time: 0.1617  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:20  loss: 2.2739 (2.2701)  acc1: 44.4444 (46.8254)  acc5: 77.7778 (76.8116)  time: 0.1649  data: 0.0005  max mem: 15572
Val:  [170/272]  eta: 0:00:18  loss: 2.4189 (2.2898)  acc1: 44.4444 (46.4912)  acc5: 72.2222 (76.3808)  time: 0.1725  data: 0.0007  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 2.3521 (2.2831)  acc1: 38.8889 (46.1940)  acc5: 72.2222 (76.6421)  time: 0.1961  data: 0.0199  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 2.3428 (2.3347)  acc1: 33.3333 (44.8517)  acc5: 77.7778 (75.4508)  time: 0.1895  data: 0.0197  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 2.5487 (2.3423)  acc1: 33.3333 (44.6656)  acc5: 66.6667 (75.2626)  time: 0.1604  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.1191 (2.3435)  acc1: 50.0000 (44.8920)  acc5: 77.7778 (75.2238)  time: 0.1652  data: 0.0006  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 2.1848 (2.3331)  acc1: 55.5556 (45.0980)  acc5: 77.7778 (75.3645)  time: 0.1819  data: 0.0007  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.7476 (2.3003)  acc1: 61.1111 (46.1279)  acc5: 83.3333 (75.8057)  time: 0.1850  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.5921 (2.2852)  acc1: 66.6667 (46.4039)  acc5: 83.3333 (76.0719)  time: 0.1767  data: 0.0020  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.2695 (2.2967)  acc1: 44.4444 (45.7503)  acc5: 77.7778 (76.0514)  time: 0.1656  data: 0.0019  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.1975 (2.2391)  acc1: 66.6667 (47.3819)  acc5: 88.8889 (76.7348)  time: 0.1479  data: 0.0002  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3529 (2.2337)  acc1: 72.2222 (47.4785)  acc5: 88.8889 (76.8758)  time: 0.1375  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3529 (2.2380)  acc1: 72.2222 (47.4503)  acc5: 88.8889 (76.8585)  time: 0.1305  data: 0.0001  max mem: 15572
Val: Total time: 0:00:48 (0.1786 s / it)
* Acc@1 47.450 Acc@5 76.858 loss 2.238
Accuracy of the network on the 4883 val videos: 47.5%
[2025-01-13 09:42:35,218] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 09:42:35,220] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 09:42:35,220] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 09:42:37,636] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 09:42:37,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 47.45%
Epoch: [35]  [   0/2809]  eta: 3:31:55  lr: 0.000002  min_lr: 0.000000  loss: 3.2857 (3.2857)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.5267  data: 4.1198  max mem: 15572
Epoch: [35]  [  10/2809]  eta: 0:35:04  lr: 0.000002  min_lr: 0.000000  loss: 3.7540 (3.7811)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7520  data: 0.3748  max mem: 15572
Epoch: [35]  [  20/2809]  eta: 0:26:40  lr: 0.000002  min_lr: 0.000000  loss: 3.7540 (3.7367)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0004  max mem: 15572
Epoch: [35]  [  30/2809]  eta: 0:23:30  lr: 0.000002  min_lr: 0.000000  loss: 3.7179 (3.7002)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0004  max mem: 15572
Epoch: [35]  [  40/2809]  eta: 0:21:51  lr: 0.000002  min_lr: 0.000000  loss: 3.6482 (3.7126)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0004  max mem: 15572
Epoch: [35]  [  50/2809]  eta: 0:20:53  lr: 0.000002  min_lr: 0.000000  loss: 3.6482 (3.7455)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [35]  [  60/2809]  eta: 0:20:12  lr: 0.000002  min_lr: 0.000000  loss: 3.9993 (3.7825)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [35]  [  70/2809]  eta: 0:19:42  lr: 0.000002  min_lr: 0.000000  loss: 3.6831 (3.7423)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0003  max mem: 15572
Epoch: [35]  [  80/2809]  eta: 0:19:28  lr: 0.000002  min_lr: 0.000000  loss: 3.6368 (3.7303)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3882  data: 0.0005  max mem: 15572
[2025-01-13 09:43:15,005] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:43:15,005] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [  90/2809]  eta: 0:19:10  lr: 0.000002  min_lr: 0.000000  loss: 3.7105 (3.6947)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)  time: 0.3927  data: 0.0005  max mem: 15572
[2025-01-13 09:43:16,892] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98407
[2025-01-13 09:43:16,892] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:43:16,892] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 100/2809]  eta: 0:18:52  lr: 0.000002  min_lr: 0.000000  loss: 3.7105 (3.7029)  loss_scale: 65536.0000 (68780.3564)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [35]  [ 110/2809]  eta: 0:18:35  lr: 0.000002  min_lr: 0.000000  loss: 3.4380 (3.6648)  loss_scale: 65536.0000 (68488.0721)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [35]  [ 120/2809]  eta: 0:18:22  lr: 0.000002  min_lr: 0.000000  loss: 3.4380 (3.6647)  loss_scale: 65536.0000 (68244.0992)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [35]  [ 130/2809]  eta: 0:18:11  lr: 0.000002  min_lr: 0.000000  loss: 3.8021 (3.6766)  loss_scale: 65536.0000 (68037.3740)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [35]  [ 140/2809]  eta: 0:18:00  lr: 0.000002  min_lr: 0.000000  loss: 3.8278 (3.6775)  loss_scale: 65536.0000 (67859.9716)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0003  max mem: 15572
Epoch: [35]  [ 150/2809]  eta: 0:17:50  lr: 0.000002  min_lr: 0.000000  loss: 3.6927 (3.6738)  loss_scale: 65536.0000 (67706.0662)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0003  max mem: 15572
Epoch: [35]  [ 160/2809]  eta: 0:17:40  lr: 0.000002  min_lr: 0.000000  loss: 3.6070 (3.6713)  loss_scale: 65536.0000 (67571.2795)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [35]  [ 170/2809]  eta: 0:17:32  lr: 0.000002  min_lr: 0.000000  loss: 3.7183 (3.6796)  loss_scale: 65536.0000 (67452.2573)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0003  max mem: 15572
Epoch: [35]  [ 180/2809]  eta: 0:17:24  lr: 0.000002  min_lr: 0.000000  loss: 3.7392 (3.6855)  loss_scale: 65536.0000 (67346.3867)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0004  max mem: 15572
Epoch: [35]  [ 190/2809]  eta: 0:17:19  lr: 0.000002  min_lr: 0.000000  loss: 3.7373 (3.6826)  loss_scale: 65536.0000 (67251.6021)  weight_decay: 0.0500 (0.0500)  time: 0.3804  data: 0.0004  max mem: 15572
Epoch: [35]  [ 200/2809]  eta: 0:17:13  lr: 0.000002  min_lr: 0.000000  loss: 3.6770 (3.6803)  loss_scale: 65536.0000 (67166.2488)  weight_decay: 0.0500 (0.0500)  time: 0.3841  data: 0.0004  max mem: 15572
Epoch: [35]  [ 210/2809]  eta: 0:17:08  lr: 0.000002  min_lr: 0.000000  loss: 3.7571 (3.6858)  loss_scale: 65536.0000 (67088.9858)  weight_decay: 0.0500 (0.0500)  time: 0.3843  data: 0.0004  max mem: 15572
Epoch: [35]  [ 220/2809]  eta: 0:17:02  lr: 0.000002  min_lr: 0.000000  loss: 3.8005 (3.6809)  loss_scale: 65536.0000 (67018.7149)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0005  max mem: 15572
[2025-01-13 09:44:05,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:44:05,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:44:06,019] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98538
[2025-01-13 09:44:06,019] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:44:06,019] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 230/2809]  eta: 0:16:55  lr: 0.000002  min_lr: 0.000000  loss: 3.8005 (3.6944)  loss_scale: 65536.0000 (67521.9394)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0003  max mem: 15572
Epoch: [35]  [ 240/2809]  eta: 0:16:48  lr: 0.000002  min_lr: 0.000000  loss: 3.7314 (3.6918)  loss_scale: 65536.0000 (67439.5353)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0002  max mem: 15572
Epoch: [35]  [ 250/2809]  eta: 0:16:42  lr: 0.000002  min_lr: 0.000000  loss: 3.6262 (3.6985)  loss_scale: 65536.0000 (67363.6972)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [35]  [ 260/2809]  eta: 0:16:36  lr: 0.000002  min_lr: 0.000000  loss: 3.7405 (3.6907)  loss_scale: 65536.0000 (67293.6705)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [35]  [ 270/2809]  eta: 0:16:30  lr: 0.000002  min_lr: 0.000000  loss: 3.5467 (3.6836)  loss_scale: 65536.0000 (67228.8118)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [35]  [ 280/2809]  eta: 0:16:24  lr: 0.000002  min_lr: 0.000000  loss: 3.6400 (3.6880)  loss_scale: 65536.0000 (67168.5694)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [35]  [ 290/2809]  eta: 0:16:19  lr: 0.000002  min_lr: 0.000000  loss: 3.7282 (3.6852)  loss_scale: 65536.0000 (67112.4674)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [35]  [ 300/2809]  eta: 0:16:13  lr: 0.000002  min_lr: 0.000000  loss: 3.7536 (3.6916)  loss_scale: 65536.0000 (67060.0930)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [35]  [ 310/2809]  eta: 0:16:08  lr: 0.000002  min_lr: 0.000000  loss: 3.9284 (3.6983)  loss_scale: 65536.0000 (67011.0868)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [35]  [ 320/2809]  eta: 0:16:04  lr: 0.000002  min_lr: 0.000000  loss: 3.8332 (3.7014)  loss_scale: 65536.0000 (66965.1340)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0003  max mem: 15572
Epoch: [35]  [ 330/2809]  eta: 0:16:00  lr: 0.000002  min_lr: 0.000000  loss: 3.6841 (3.7027)  loss_scale: 65536.0000 (66921.9577)  weight_decay: 0.0500 (0.0500)  time: 0.3831  data: 0.0003  max mem: 15572
Epoch: [35]  [ 340/2809]  eta: 0:15:55  lr: 0.000002  min_lr: 0.000000  loss: 3.4904 (3.6943)  loss_scale: 65536.0000 (66881.3138)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0003  max mem: 15572
Epoch: [35]  [ 350/2809]  eta: 0:15:50  lr: 0.000002  min_lr: 0.000000  loss: 3.4904 (3.7010)  loss_scale: 65536.0000 (66842.9858)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0003  max mem: 15572
[2025-01-13 09:44:54,178] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:44:54,178] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:44:55,671] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98671
[2025-01-13 09:44:55,671] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:44:55,671] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 360/2809]  eta: 0:15:46  lr: 0.000002  min_lr: 0.000000  loss: 3.7513 (3.7039)  loss_scale: 65536.0000 (67532.9418)  weight_decay: 0.0500 (0.0500)  time: 0.3773  data: 0.0002  max mem: 15572
Epoch: [35]  [ 370/2809]  eta: 0:15:41  lr: 0.000002  min_lr: 0.000000  loss: 3.8275 (3.7056)  loss_scale: 65536.0000 (67479.1159)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [35]  [ 380/2809]  eta: 0:15:36  lr: 0.000002  min_lr: 0.000000  loss: 3.8933 (3.7070)  loss_scale: 65536.0000 (67428.1155)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [35]  [ 390/2809]  eta: 0:15:31  lr: 0.000002  min_lr: 0.000000  loss: 3.8010 (3.7090)  loss_scale: 65536.0000 (67379.7238)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [35]  [ 400/2809]  eta: 0:15:26  lr: 0.000002  min_lr: 0.000000  loss: 3.7277 (3.7032)  loss_scale: 65536.0000 (67333.7456)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [35]  [ 410/2809]  eta: 0:15:21  lr: 0.000002  min_lr: 0.000000  loss: 3.4957 (3.7008)  loss_scale: 65536.0000 (67290.0049)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [35]  [ 420/2809]  eta: 0:15:17  lr: 0.000002  min_lr: 0.000000  loss: 3.6021 (3.6992)  loss_scale: 65536.0000 (67248.3420)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0003  max mem: 15572
Epoch: [35]  [ 430/2809]  eta: 0:15:13  lr: 0.000002  min_lr: 0.000000  loss: 3.7740 (3.7014)  loss_scale: 65536.0000 (67208.6125)  weight_decay: 0.0500 (0.0500)  time: 0.3837  data: 0.0004  max mem: 15572
Epoch: [35]  [ 440/2809]  eta: 0:15:09  lr: 0.000002  min_lr: 0.000000  loss: 3.7740 (3.7012)  loss_scale: 65536.0000 (67170.6848)  weight_decay: 0.0500 (0.0500)  time: 0.3855  data: 0.0004  max mem: 15572
Epoch: [35]  [ 450/2809]  eta: 0:15:05  lr: 0.000002  min_lr: 0.000000  loss: 3.5577 (3.7001)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0003  max mem: 15572
Epoch: [35]  [ 460/2809]  eta: 0:15:00  lr: 0.000002  min_lr: 0.000000  loss: 3.6335 (3.6983)  loss_scale: 65536.0000 (67099.7657)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [35]  [ 470/2809]  eta: 0:14:56  lr: 0.000002  min_lr: 0.000000  loss: 3.6989 (3.6995)  loss_scale: 65536.0000 (67066.5648)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [35]  [ 480/2809]  eta: 0:14:51  lr: 0.000002  min_lr: 0.000000  loss: 3.6583 (3.6970)  loss_scale: 65536.0000 (67034.7443)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0003  max mem: 15572
[2025-01-13 09:45:43,766] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:45:43,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:45:44,896] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98803
[2025-01-13 09:45:44,896] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:45:44,896] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 490/2809]  eta: 0:14:47  lr: 0.000002  min_lr: 0.000000  loss: 3.6583 (3.6993)  loss_scale: 65536.0000 (67404.6436)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [35]  [ 500/2809]  eta: 0:14:43  lr: 0.000002  min_lr: 0.000000  loss: 3.6207 (3.6966)  loss_scale: 65536.0000 (67367.3453)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [35]  [ 510/2809]  eta: 0:14:38  lr: 0.000002  min_lr: 0.000000  loss: 3.4271 (3.6950)  loss_scale: 65536.0000 (67331.5068)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0003  max mem: 15572
Epoch: [35]  [ 520/2809]  eta: 0:14:34  lr: 0.000002  min_lr: 0.000000  loss: 3.6475 (3.6980)  loss_scale: 65536.0000 (67297.0441)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [35]  [ 530/2809]  eta: 0:14:29  lr: 0.000002  min_lr: 0.000000  loss: 3.7285 (3.6976)  loss_scale: 65536.0000 (67263.8795)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [35]  [ 540/2809]  eta: 0:14:25  lr: 0.000002  min_lr: 0.000000  loss: 3.7638 (3.6971)  loss_scale: 65536.0000 (67231.9409)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0004  max mem: 15572
Epoch: [35]  [ 550/2809]  eta: 0:14:22  lr: 0.000002  min_lr: 0.000000  loss: 3.8493 (3.7000)  loss_scale: 65536.0000 (67201.1615)  weight_decay: 0.0500 (0.0500)  time: 0.3810  data: 0.0005  max mem: 15572
Epoch: [35]  [ 560/2809]  eta: 0:14:18  lr: 0.000002  min_lr: 0.000000  loss: 3.6661 (3.6971)  loss_scale: 65536.0000 (67171.4795)  weight_decay: 0.0500 (0.0500)  time: 0.3876  data: 0.0005  max mem: 15572
Epoch: [35]  [ 570/2809]  eta: 0:14:14  lr: 0.000002  min_lr: 0.000000  loss: 3.6661 (3.6979)  loss_scale: 65536.0000 (67142.8371)  weight_decay: 0.0500 (0.0500)  time: 0.3813  data: 0.0004  max mem: 15572
Epoch: [35]  [ 580/2809]  eta: 0:14:10  lr: 0.000002  min_lr: 0.000000  loss: 3.6718 (3.6952)  loss_scale: 65536.0000 (67115.1807)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [35]  [ 590/2809]  eta: 0:14:05  lr: 0.000002  min_lr: 0.000000  loss: 3.6574 (3.6960)  loss_scale: 65536.0000 (67088.4602)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [35]  [ 600/2809]  eta: 0:14:02  lr: 0.000002  min_lr: 0.000000  loss: 3.6574 (3.6931)  loss_scale: 65536.0000 (67062.6290)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [35]  [ 610/2809]  eta: 0:13:57  lr: 0.000002  min_lr: 0.000000  loss: 3.6155 (3.6918)  loss_scale: 65536.0000 (67037.6432)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0002  max mem: 15572
[2025-01-13 09:46:33,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:46:33,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:46:33,885] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98934
[2025-01-13 09:46:33,885] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:46:33,885] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 620/2809]  eta: 0:13:53  lr: 0.000002  min_lr: 0.000000  loss: 3.6155 (3.6899)  loss_scale: 65536.0000 (67224.5282)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [35]  [ 630/2809]  eta: 0:13:49  lr: 0.000002  min_lr: 0.000000  loss: 3.7186 (3.6924)  loss_scale: 65536.0000 (67197.7686)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [35]  [ 640/2809]  eta: 0:13:45  lr: 0.000002  min_lr: 0.000000  loss: 3.8523 (3.6923)  loss_scale: 65536.0000 (67171.8440)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [35]  [ 650/2809]  eta: 0:13:41  lr: 0.000002  min_lr: 0.000000  loss: 3.4357 (3.6891)  loss_scale: 65536.0000 (67146.7158)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [35]  [ 660/2809]  eta: 0:13:37  lr: 0.000002  min_lr: 0.000000  loss: 3.6419 (3.6898)  loss_scale: 65536.0000 (67122.3480)  weight_decay: 0.0500 (0.0500)  time: 0.3812  data: 0.0003  max mem: 15572
Epoch: [35]  [ 670/2809]  eta: 0:13:34  lr: 0.000002  min_lr: 0.000000  loss: 3.6791 (3.6887)  loss_scale: 65536.0000 (67098.7064)  weight_decay: 0.0500 (0.0500)  time: 0.3921  data: 0.0004  max mem: 15572
Epoch: [35]  [ 680/2809]  eta: 0:13:30  lr: 0.000002  min_lr: 0.000000  loss: 3.6411 (3.6901)  loss_scale: 65536.0000 (67075.7592)  weight_decay: 0.0500 (0.0500)  time: 0.3894  data: 0.0005  max mem: 15572
[2025-01-13 09:46:58,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=99000, skipped=673, lr=[2.0817667707773205e-08, 2.0817667707773205e-08, 2.9739525296818868e-08, 2.9739525296818868e-08, 4.248503613831267e-08, 4.248503613831267e-08, 6.06929087690181e-08, 6.06929087690181e-08, 8.670415538431158e-08, 8.670415538431158e-08, 1.2386307912044512e-07, 1.2386307912044512e-07, 1.7694725588635018e-07, 1.7694725588635018e-07, 2.5278179412335745e-07, 2.5278179412335745e-07, 3.611168487476535e-07, 3.611168487476535e-07, 5.158812124966479e-07, 5.158812124966479e-07, 7.36973160709497e-07, 7.36973160709497e-07, 1.0528188010135672e-06, 1.0528188010135672e-06, 1.5040268585908103e-06, 1.5040268585908103e-06, 2.148609797986872e-06, 2.148609797986872e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 09:46:58,652] [INFO] [timer.py:260:stop] epoch=0/micro_step=99000/global_step=99000, RunningAvgSamplesPerSec=31.347423416466693, CurrSamplesPerSec=30.6707852207365, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [35]  [ 690/2809]  eta: 0:13:27  lr: 0.000002  min_lr: 0.000000  loss: 3.8186 (3.6935)  loss_scale: 65536.0000 (67053.4761)  weight_decay: 0.0500 (0.0500)  time: 0.3861  data: 0.0004  max mem: 15572
Epoch: [35]  [ 700/2809]  eta: 0:13:23  lr: 0.000002  min_lr: 0.000000  loss: 3.7098 (3.6929)  loss_scale: 65536.0000 (67031.8288)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0003  max mem: 15572
Epoch: [35]  [ 710/2809]  eta: 0:13:18  lr: 0.000002  min_lr: 0.000000  loss: 3.7651 (3.6960)  loss_scale: 65536.0000 (67010.7904)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [35]  [ 720/2809]  eta: 0:13:14  lr: 0.000002  min_lr: 0.000000  loss: 3.4928 (3.6929)  loss_scale: 65536.0000 (66990.3356)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0002  max mem: 15572
Epoch: [35]  [ 730/2809]  eta: 0:13:10  lr: 0.000002  min_lr: 0.000000  loss: 3.4608 (3.6921)  loss_scale: 65536.0000 (66970.4405)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [35]  [ 740/2809]  eta: 0:13:06  lr: 0.000002  min_lr: 0.000000  loss: 3.6066 (3.6915)  loss_scale: 65536.0000 (66951.0823)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
[2025-01-13 09:47:22,371] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:47:22,372] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [ 750/2809]  eta: 0:13:02  lr: 0.000002  min_lr: 0.000000  loss: 3.5147 (3.6898)  loss_scale: 65536.0000 (67194.0346)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
[2025-01-13 09:47:24,563] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99069
[2025-01-13 09:47:24,564] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:47:24,564] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 760/2809]  eta: 0:12:58  lr: 0.000002  min_lr: 0.000000  loss: 3.5348 (3.6898)  loss_scale: 65536.0000 (67430.6018)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0002  max mem: 15572
Epoch: [35]  [ 770/2809]  eta: 0:12:54  lr: 0.000002  min_lr: 0.000000  loss: 3.5337 (3.6852)  loss_scale: 65536.0000 (67406.0285)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [35]  [ 780/2809]  eta: 0:12:50  lr: 0.000002  min_lr: 0.000000  loss: 3.4513 (3.6870)  loss_scale: 65536.0000 (67382.0845)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0003  max mem: 15572
Epoch: [35]  [ 790/2809]  eta: 0:12:46  lr: 0.000002  min_lr: 0.000000  loss: 3.5103 (3.6834)  loss_scale: 65536.0000 (67358.7459)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0003  max mem: 15572
Epoch: [35]  [ 800/2809]  eta: 0:12:42  lr: 0.000002  min_lr: 0.000000  loss: 3.4795 (3.6817)  loss_scale: 65536.0000 (67335.9900)  weight_decay: 0.0500 (0.0500)  time: 0.3863  data: 0.0004  max mem: 15572
Epoch: [35]  [ 810/2809]  eta: 0:12:39  lr: 0.000002  min_lr: 0.000000  loss: 3.7095 (3.6841)  loss_scale: 65536.0000 (67313.7953)  weight_decay: 0.0500 (0.0500)  time: 0.3817  data: 0.0004  max mem: 15572
Epoch: [35]  [ 820/2809]  eta: 0:12:35  lr: 0.000002  min_lr: 0.000000  loss: 3.6674 (3.6815)  loss_scale: 65536.0000 (67292.1413)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [35]  [ 830/2809]  eta: 0:12:31  lr: 0.000002  min_lr: 0.000000  loss: 3.4730 (3.6793)  loss_scale: 65536.0000 (67271.0084)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [35]  [ 840/2809]  eta: 0:12:27  lr: 0.000002  min_lr: 0.000000  loss: 3.7871 (3.6790)  loss_scale: 65536.0000 (67250.3781)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [35]  [ 850/2809]  eta: 0:12:23  lr: 0.000002  min_lr: 0.000000  loss: 3.7943 (3.6791)  loss_scale: 65536.0000 (67230.2327)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [35]  [ 860/2809]  eta: 0:12:19  lr: 0.000002  min_lr: 0.000000  loss: 3.7085 (3.6787)  loss_scale: 65536.0000 (67210.5552)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [35]  [ 870/2809]  eta: 0:12:15  lr: 0.000002  min_lr: 0.000000  loss: 3.7081 (3.6780)  loss_scale: 65536.0000 (67191.3295)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [35]  [ 880/2809]  eta: 0:12:11  lr: 0.000002  min_lr: 0.000000  loss: 3.7778 (3.6803)  loss_scale: 65536.0000 (67172.5403)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
[2025-01-13 09:48:12,834] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:48:12,835] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:48:13,225] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99199
[2025-01-13 09:48:13,225] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:48:13,226] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 890/2809]  eta: 0:12:07  lr: 0.000002  min_lr: 0.000000  loss: 3.7778 (3.6794)  loss_scale: 65536.0000 (67227.7262)  weight_decay: 0.0500 (0.0500)  time: 0.3800  data: 0.0005  max mem: 15572
Epoch: [35]  [ 900/2809]  eta: 0:12:04  lr: 0.000002  min_lr: 0.000000  loss: 3.6412 (3.6812)  loss_scale: 65536.0000 (67208.9501)  weight_decay: 0.0500 (0.0500)  time: 0.3910  data: 0.0006  max mem: 15572
Epoch: [35]  [ 910/2809]  eta: 0:12:00  lr: 0.000002  min_lr: 0.000000  loss: 3.8811 (3.6809)  loss_scale: 65536.0000 (67190.5862)  weight_decay: 0.0500 (0.0500)  time: 0.3981  data: 0.0006  max mem: 15572
Epoch: [35]  [ 920/2809]  eta: 0:11:57  lr: 0.000002  min_lr: 0.000000  loss: 3.7834 (3.6806)  loss_scale: 65536.0000 (67172.6211)  weight_decay: 0.0500 (0.0500)  time: 0.4019  data: 0.0007  max mem: 15572
Epoch: [35]  [ 930/2809]  eta: 0:11:53  lr: 0.000002  min_lr: 0.000000  loss: 3.8486 (3.6839)  loss_scale: 65536.0000 (67155.0419)  weight_decay: 0.0500 (0.0500)  time: 0.3918  data: 0.0005  max mem: 15572
Epoch: [35]  [ 940/2809]  eta: 0:11:49  lr: 0.000002  min_lr: 0.000000  loss: 3.9729 (3.6853)  loss_scale: 65536.0000 (67137.8363)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0003  max mem: 15572
Epoch: [35]  [ 950/2809]  eta: 0:11:45  lr: 0.000002  min_lr: 0.000000  loss: 3.9660 (3.6874)  loss_scale: 65536.0000 (67120.9926)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0002  max mem: 15572
Epoch: [35]  [ 960/2809]  eta: 0:11:41  lr: 0.000002  min_lr: 0.000000  loss: 3.9153 (3.6859)  loss_scale: 65536.0000 (67104.4995)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [35]  [ 970/2809]  eta: 0:11:37  lr: 0.000002  min_lr: 0.000000  loss: 3.4596 (3.6839)  loss_scale: 65536.0000 (67088.3460)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0003  max mem: 15572
Epoch: [35]  [ 980/2809]  eta: 0:11:33  lr: 0.000002  min_lr: 0.000000  loss: 3.6011 (3.6841)  loss_scale: 65536.0000 (67072.5219)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0003  max mem: 15572
Epoch: [35]  [ 990/2809]  eta: 0:11:29  lr: 0.000002  min_lr: 0.000000  loss: 3.7796 (3.6857)  loss_scale: 65536.0000 (67057.0172)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [35]  [1000/2809]  eta: 0:11:26  lr: 0.000002  min_lr: 0.000000  loss: 3.8906 (3.6859)  loss_scale: 65536.0000 (67041.8222)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [35]  [1010/2809]  eta: 0:11:22  lr: 0.000002  min_lr: 0.000000  loss: 3.8540 (3.6857)  loss_scale: 65536.0000 (67026.9278)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0003  max mem: 15572
[2025-01-13 09:49:02,211] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:49:02,212] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:49:03,326] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99331
[2025-01-13 09:49:03,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:49:03,326] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [1020/2809]  eta: 0:11:18  lr: 0.000002  min_lr: 0.000000  loss: 3.5955 (3.6820)  loss_scale: 65536.0000 (67204.8893)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0003  max mem: 15572
Epoch: [35]  [1030/2809]  eta: 0:11:14  lr: 0.000002  min_lr: 0.000000  loss: 3.4537 (3.6801)  loss_scale: 65536.0000 (67188.7022)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0003  max mem: 15572
Epoch: [35]  [1040/2809]  eta: 0:11:10  lr: 0.000002  min_lr: 0.000000  loss: 3.6627 (3.6803)  loss_scale: 65536.0000 (67172.8261)  weight_decay: 0.0500 (0.0500)  time: 0.3857  data: 0.0004  max mem: 15572
Epoch: [35]  [1050/2809]  eta: 0:11:07  lr: 0.000002  min_lr: 0.000000  loss: 3.8574 (3.6795)  loss_scale: 65536.0000 (67157.2521)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0004  max mem: 15572
Epoch: [35]  [1060/2809]  eta: 0:11:03  lr: 0.000002  min_lr: 0.000000  loss: 3.9293 (3.6822)  loss_scale: 65536.0000 (67141.9717)  weight_decay: 0.0500 (0.0500)  time: 0.3820  data: 0.0003  max mem: 15572
Epoch: [35]  [1070/2809]  eta: 0:10:59  lr: 0.000002  min_lr: 0.000000  loss: 3.8453 (3.6833)  loss_scale: 65536.0000 (67126.9767)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [35]  [1080/2809]  eta: 0:10:55  lr: 0.000002  min_lr: 0.000000  loss: 3.6646 (3.6826)  loss_scale: 65536.0000 (67112.2590)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [35]  [1090/2809]  eta: 0:10:51  lr: 0.000002  min_lr: 0.000000  loss: 3.6646 (3.6836)  loss_scale: 65536.0000 (67097.8112)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [35]  [1100/2809]  eta: 0:10:47  lr: 0.000002  min_lr: 0.000000  loss: 3.7770 (3.6841)  loss_scale: 65536.0000 (67083.6258)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0003  max mem: 15572
Epoch: [35]  [1110/2809]  eta: 0:10:43  lr: 0.000002  min_lr: 0.000000  loss: 3.8986 (3.6870)  loss_scale: 65536.0000 (67069.6958)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0003  max mem: 15572
Epoch: [35]  [1120/2809]  eta: 0:10:39  lr: 0.000002  min_lr: 0.000000  loss: 3.8643 (3.6854)  loss_scale: 65536.0000 (67056.0143)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0003  max mem: 15572
Epoch: [35]  [1130/2809]  eta: 0:10:35  lr: 0.000002  min_lr: 0.000000  loss: 3.7033 (3.6853)  loss_scale: 65536.0000 (67042.5747)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [35]  [1140/2809]  eta: 0:10:32  lr: 0.000002  min_lr: 0.000000  loss: 3.7931 (3.6875)  loss_scale: 65536.0000 (67029.3707)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
[2025-01-13 09:49:51,964] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:49:51,964] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:49:53,862] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99465
[2025-01-13 09:49:53,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:49:53,862] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [1150/2809]  eta: 0:10:28  lr: 0.000002  min_lr: 0.000000  loss: 3.9236 (3.6885)  loss_scale: 65536.0000 (67301.0877)  weight_decay: 0.0500 (0.0500)  time: 0.3859  data: 0.0004  max mem: 15572
Epoch: [35]  [1160/2809]  eta: 0:10:24  lr: 0.000002  min_lr: 0.000000  loss: 3.6452 (3.6869)  loss_scale: 65536.0000 (67285.8846)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0004  max mem: 15572
Epoch: [35]  [1170/2809]  eta: 0:10:21  lr: 0.000002  min_lr: 0.000000  loss: 3.5690 (3.6857)  loss_scale: 65536.0000 (67270.9411)  weight_decay: 0.0500 (0.0500)  time: 0.3859  data: 0.0004  max mem: 15572
Epoch: [35]  [1180/2809]  eta: 0:10:17  lr: 0.000002  min_lr: 0.000000  loss: 3.7099 (3.6862)  loss_scale: 65536.0000 (67256.2506)  weight_decay: 0.0500 (0.0500)  time: 0.3844  data: 0.0004  max mem: 15572
Epoch: [35]  [1190/2809]  eta: 0:10:13  lr: 0.000002  min_lr: 0.000000  loss: 3.7473 (3.6877)  loss_scale: 65536.0000 (67241.8069)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [35]  [1200/2809]  eta: 0:10:09  lr: 0.000002  min_lr: 0.000000  loss: 3.7134 (3.6874)  loss_scale: 65536.0000 (67227.6037)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [35]  [1210/2809]  eta: 0:10:05  lr: 0.000002  min_lr: 0.000000  loss: 3.7046 (3.6867)  loss_scale: 65536.0000 (67213.6350)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0003  max mem: 15572
Epoch: [35]  [1220/2809]  eta: 0:10:01  lr: 0.000002  min_lr: 0.000000  loss: 3.4201 (3.6859)  loss_scale: 65536.0000 (67199.8952)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0003  max mem: 15572
Epoch: [35]  [1230/2809]  eta: 0:09:57  lr: 0.000002  min_lr: 0.000000  loss: 3.5633 (3.6859)  loss_scale: 65536.0000 (67186.3786)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [35]  [1240/2809]  eta: 0:09:53  lr: 0.000002  min_lr: 0.000000  loss: 3.5690 (3.6851)  loss_scale: 65536.0000 (67173.0798)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [35]  [1250/2809]  eta: 0:09:50  lr: 0.000002  min_lr: 0.000000  loss: 3.5164 (3.6843)  loss_scale: 65536.0000 (67159.9936)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [35]  [1260/2809]  eta: 0:09:46  lr: 0.000002  min_lr: 0.000000  loss: 3.5818 (3.6847)  loss_scale: 65536.0000 (67147.1150)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [35]  [1270/2809]  eta: 0:09:42  lr: 0.000002  min_lr: 0.000000  loss: 3.6366 (3.6846)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
[2025-01-13 09:50:42,082] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:50:42,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [1280/2809]  eta: 0:09:38  lr: 0.000002  min_lr: 0.000000  loss: 3.4350 (3.6831)  loss_scale: 65536.0000 (67224.2810)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0003  max mem: 15572
[2025-01-13 09:50:43,597] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99598
[2025-01-13 09:50:43,597] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:50:43,597] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-13 09:50:45,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99603
[2025-01-13 09:50:45,462] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:50:45,462] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1290/2809]  eta: 0:09:34  lr: 0.000002  min_lr: 0.000000  loss: 3.5564 (3.6823)  loss_scale: 65536.0000 (67236.5856)  weight_decay: 0.0500 (0.0500)  time: 0.3788  data: 0.0004  max mem: 15572
Epoch: [35]  [1300/2809]  eta: 0:09:30  lr: 0.000002  min_lr: 0.000000  loss: 3.8782 (3.6831)  loss_scale: 32768.0000 (66971.6464)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0004  max mem: 15572
Epoch: [35]  [1310/2809]  eta: 0:09:27  lr: 0.000002  min_lr: 0.000000  loss: 3.5739 (3.6814)  loss_scale: 32768.0000 (66710.7490)  weight_decay: 0.0500 (0.0500)  time: 0.3835  data: 0.0004  max mem: 15572
Epoch: [35]  [1320/2809]  eta: 0:09:23  lr: 0.000002  min_lr: 0.000000  loss: 3.7870 (3.6828)  loss_scale: 32768.0000 (66453.8017)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [35]  [1330/2809]  eta: 0:09:19  lr: 0.000002  min_lr: 0.000000  loss: 3.8555 (3.6826)  loss_scale: 32768.0000 (66200.7153)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [35]  [1340/2809]  eta: 0:09:15  lr: 0.000002  min_lr: 0.000000  loss: 3.7674 (3.6822)  loss_scale: 32768.0000 (65951.4034)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0003  max mem: 15572
Epoch: [35]  [1350/2809]  eta: 0:09:11  lr: 0.000002  min_lr: 0.000000  loss: 3.7674 (3.6814)  loss_scale: 32768.0000 (65705.7824)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0003  max mem: 15572
Epoch: [35]  [1360/2809]  eta: 0:09:07  lr: 0.000002  min_lr: 0.000000  loss: 3.7929 (3.6832)  loss_scale: 32768.0000 (65463.7708)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0003  max mem: 15572
Epoch: [35]  [1370/2809]  eta: 0:09:04  lr: 0.000002  min_lr: 0.000000  loss: 3.7907 (3.6819)  loss_scale: 32768.0000 (65225.2896)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0003  max mem: 15572
Epoch: [35]  [1380/2809]  eta: 0:09:00  lr: 0.000002  min_lr: 0.000000  loss: 3.6931 (3.6820)  loss_scale: 32768.0000 (64990.2621)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0003  max mem: 15572
Epoch: [35]  [1390/2809]  eta: 0:08:56  lr: 0.000002  min_lr: 0.000000  loss: 3.7722 (3.6834)  loss_scale: 32768.0000 (64758.6139)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0003  max mem: 15572
Epoch: [35]  [1400/2809]  eta: 0:08:52  lr: 0.000002  min_lr: 0.000000  loss: 3.5191 (3.6812)  loss_scale: 32768.0000 (64530.2727)  weight_decay: 0.0500 (0.0500)  time: 0.3844  data: 0.0003  max mem: 15572
Epoch: [35]  [1410/2809]  eta: 0:08:48  lr: 0.000002  min_lr: 0.000000  loss: 3.2908 (3.6802)  loss_scale: 32768.0000 (64305.1680)  weight_decay: 0.0500 (0.0500)  time: 0.3873  data: 0.0003  max mem: 15572
[2025-01-13 09:51:34,140] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:51:34,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1420/2809]  eta: 0:08:45  lr: 0.000002  min_lr: 0.000000  loss: 3.9261 (3.6825)  loss_scale: 32768.0000 (64175.4708)  weight_decay: 0.0500 (0.0500)  time: 0.3887  data: 0.0004  max mem: 15572
Epoch: [35]  [1430/2809]  eta: 0:08:41  lr: 0.000002  min_lr: 0.000000  loss: 3.9098 (3.6828)  loss_scale: 65536.0000 (64184.9783)  weight_decay: 0.0500 (0.0500)  time: 0.3940  data: 0.0004  max mem: 15572
Epoch: [35]  [1440/2809]  eta: 0:08:37  lr: 0.000002  min_lr: 0.000000  loss: 3.6580 (3.6834)  loss_scale: 65536.0000 (64194.3539)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0003  max mem: 15572
Epoch: [35]  [1450/2809]  eta: 0:08:34  lr: 0.000002  min_lr: 0.000000  loss: 3.6580 (3.6827)  loss_scale: 65536.0000 (64203.6003)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [35]  [1460/2809]  eta: 0:08:30  lr: 0.000002  min_lr: 0.000000  loss: 3.2895 (3.6809)  loss_scale: 65536.0000 (64212.7201)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [35]  [1470/2809]  eta: 0:08:26  lr: 0.000002  min_lr: 0.000000  loss: 3.6826 (3.6809)  loss_scale: 65536.0000 (64221.7158)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [35]  [1480/2809]  eta: 0:08:22  lr: 0.000002  min_lr: 0.000000  loss: 3.7102 (3.6813)  loss_scale: 65536.0000 (64230.5901)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [35]  [1490/2809]  eta: 0:08:18  lr: 0.000002  min_lr: 0.000000  loss: 3.6438 (3.6818)  loss_scale: 65536.0000 (64239.3454)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0003  max mem: 15572
Epoch: [35]  [1500/2809]  eta: 0:08:14  lr: 0.000002  min_lr: 0.000000  loss: 3.8744 (3.6833)  loss_scale: 65536.0000 (64247.9840)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [35]  [1510/2809]  eta: 0:08:10  lr: 0.000002  min_lr: 0.000000  loss: 3.8896 (3.6840)  loss_scale: 65536.0000 (64256.5083)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0003  max mem: 15572
Epoch: [35]  [1520/2809]  eta: 0:08:06  lr: 0.000002  min_lr: 0.000000  loss: 3.6652 (3.6837)  loss_scale: 65536.0000 (64264.9204)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
[2025-01-13 09:52:15,561] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99843
[2025-01-13 09:52:15,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:52:15,561] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1530/2809]  eta: 0:08:03  lr: 0.000002  min_lr: 0.000000  loss: 3.7472 (3.6847)  loss_scale: 65536.0000 (64209.0137)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0004  max mem: 15572
Epoch: [35]  [1540/2809]  eta: 0:07:59  lr: 0.000002  min_lr: 0.000000  loss: 3.6204 (3.6823)  loss_scale: 32768.0000 (64004.9838)  weight_decay: 0.0500 (0.0500)  time: 0.3825  data: 0.0005  max mem: 15572
Epoch: [35]  [1550/2809]  eta: 0:07:55  lr: 0.000002  min_lr: 0.000000  loss: 3.4340 (3.6817)  loss_scale: 32768.0000 (63803.5848)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0004  max mem: 15572
Epoch: [35]  [1560/2809]  eta: 0:07:51  lr: 0.000002  min_lr: 0.000000  loss: 3.6305 (3.6831)  loss_scale: 32768.0000 (63604.7662)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
Epoch: [35]  [1570/2809]  eta: 0:07:48  lr: 0.000002  min_lr: 0.000000  loss: 3.6386 (3.6829)  loss_scale: 32768.0000 (63408.4787)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [35]  [1580/2809]  eta: 0:07:44  lr: 0.000002  min_lr: 0.000000  loss: 3.7982 (3.6837)  loss_scale: 32768.0000 (63214.6743)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0003  max mem: 15572
Epoch: [35]  [1590/2809]  eta: 0:07:40  lr: 0.000002  min_lr: 0.000000  loss: 3.6432 (3.6835)  loss_scale: 32768.0000 (63023.3061)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [35]  [1600/2809]  eta: 0:07:36  lr: 0.000002  min_lr: 0.000000  loss: 3.6401 (3.6834)  loss_scale: 32768.0000 (62834.3285)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [35]  [1610/2809]  eta: 0:07:32  lr: 0.000002  min_lr: 0.000000  loss: 3.6098 (3.6823)  loss_scale: 32768.0000 (62647.6971)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [35]  [1620/2809]  eta: 0:07:28  lr: 0.000002  min_lr: 0.000000  loss: 3.5802 (3.6808)  loss_scale: 32768.0000 (62463.3683)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [35]  [1630/2809]  eta: 0:07:25  lr: 0.000002  min_lr: 0.000000  loss: 3.7735 (3.6818)  loss_scale: 32768.0000 (62281.2998)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
Epoch: [35]  [1640/2809]  eta: 0:07:21  lr: 0.000002  min_lr: 0.000000  loss: 3.8111 (3.6816)  loss_scale: 32768.0000 (62101.4503)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
Epoch: [35]  [1650/2809]  eta: 0:07:17  lr: 0.000002  min_lr: 0.000000  loss: 3.6806 (3.6811)  loss_scale: 32768.0000 (61923.7795)  weight_decay: 0.0500 (0.0500)  time: 0.3822  data: 0.0003  max mem: 15572
[2025-01-13 09:53:04,040] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:53:04,041] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1660/2809]  eta: 0:07:13  lr: 0.000002  min_lr: 0.000000  loss: 3.7311 (3.6814)  loss_scale: 32768.0000 (61827.1595)  weight_decay: 0.0500 (0.0500)  time: 0.3876  data: 0.0003  max mem: 15572
Epoch: [35]  [1670/2809]  eta: 0:07:10  lr: 0.000002  min_lr: 0.000000  loss: 3.7311 (3.6822)  loss_scale: 65536.0000 (61849.3549)  weight_decay: 0.0500 (0.0500)  time: 0.3858  data: 0.0004  max mem: 15572
Epoch: [35]  [1680/2809]  eta: 0:07:06  lr: 0.000002  min_lr: 0.000000  loss: 3.8130 (3.6833)  loss_scale: 65536.0000 (61871.2861)  weight_decay: 0.0500 (0.0500)  time: 0.3794  data: 0.0004  max mem: 15572
[2025-01-13 09:53:14,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=100000, skipped=680, lr=[1.7921835888244555e-08, 1.7921835888244555e-08, 2.5602622697492225e-08, 2.5602622697492225e-08, 3.6575175282131754e-08, 3.6575175282131754e-08, 5.225025040304537e-08, 5.225025040304537e-08, 7.464321486149339e-08, 7.464321486149339e-08, 1.0663316408784769e-07, 1.0663316408784769e-07, 1.5233309155406816e-07, 1.5233309155406816e-07, 2.1761870222009737e-07, 2.1761870222009737e-07, 3.1088386031442484e-07, 3.1088386031442484e-07, 4.441198004491784e-07, 4.441198004491784e-07, 6.344568577845405e-07, 6.344568577845405e-07, 9.063669396922008e-07, 9.063669396922008e-07, 1.2948099138460012e-06, 1.2948099138460012e-06, 1.8497284483514305e-06, 1.8497284483514305e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 09:53:14,275] [INFO] [timer.py:260:stop] epoch=0/micro_step=100000/global_step=100000, RunningAvgSamplesPerSec=31.370224636403577, CurrSamplesPerSec=34.79349805265101, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [35]  [1690/2809]  eta: 0:07:02  lr: 0.000002  min_lr: 0.000000  loss: 3.7482 (3.6828)  loss_scale: 65536.0000 (61892.9580)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0003  max mem: 15572
Epoch: [35]  [1700/2809]  eta: 0:06:58  lr: 0.000002  min_lr: 0.000000  loss: 3.5580 (3.6814)  loss_scale: 65536.0000 (61914.3751)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [35]  [1710/2809]  eta: 0:06:54  lr: 0.000002  min_lr: 0.000000  loss: 3.6327 (3.6811)  loss_scale: 65536.0000 (61935.5418)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0003  max mem: 15572
Epoch: [35]  [1720/2809]  eta: 0:06:51  lr: 0.000002  min_lr: 0.000000  loss: 3.7206 (3.6822)  loss_scale: 65536.0000 (61956.4625)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0003  max mem: 15572
Epoch: [35]  [1730/2809]  eta: 0:06:47  lr: 0.000002  min_lr: 0.000000  loss: 3.7206 (3.6813)  loss_scale: 65536.0000 (61977.1415)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0003  max mem: 15572
Epoch: [35]  [1740/2809]  eta: 0:06:43  lr: 0.000002  min_lr: 0.000000  loss: 3.5940 (3.6804)  loss_scale: 65536.0000 (61997.5830)  weight_decay: 0.0500 (0.0500)  time: 0.3655  data: 0.0003  max mem: 15572
Epoch: [35]  [1750/2809]  eta: 0:06:39  lr: 0.000002  min_lr: 0.000000  loss: 3.4896 (3.6783)  loss_scale: 65536.0000 (62017.7910)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0003  max mem: 15572
Epoch: [35]  [1760/2809]  eta: 0:06:35  lr: 0.000002  min_lr: 0.000000  loss: 3.4896 (3.6781)  loss_scale: 65536.0000 (62037.7694)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0003  max mem: 15572
[2025-01-13 09:53:44,094] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100080
[2025-01-13 09:53:44,094] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:53:44,094] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1770/2809]  eta: 0:06:31  lr: 0.000002  min_lr: 0.000000  loss: 3.7206 (3.6775)  loss_scale: 65536.0000 (61946.5071)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [35]  [1780/2809]  eta: 0:06:28  lr: 0.000002  min_lr: 0.000000  loss: 3.4684 (3.6764)  loss_scale: 32768.0000 (61782.6749)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0004  max mem: 15572
Epoch: [35]  [1790/2809]  eta: 0:06:24  lr: 0.000002  min_lr: 0.000000  loss: 3.3299 (3.6761)  loss_scale: 32768.0000 (61620.6723)  weight_decay: 0.0500 (0.0500)  time: 0.3862  data: 0.0004  max mem: 15572
Epoch: [35]  [1800/2809]  eta: 0:06:20  lr: 0.000002  min_lr: 0.000000  loss: 3.5931 (3.6759)  loss_scale: 32768.0000 (61460.4686)  weight_decay: 0.0500 (0.0500)  time: 0.3873  data: 0.0004  max mem: 15572
Epoch: [35]  [1810/2809]  eta: 0:06:16  lr: 0.000002  min_lr: 0.000000  loss: 3.5931 (3.6753)  loss_scale: 32768.0000 (61302.0342)  weight_decay: 0.0500 (0.0500)  time: 0.3796  data: 0.0004  max mem: 15572
Epoch: [35]  [1820/2809]  eta: 0:06:13  lr: 0.000002  min_lr: 0.000000  loss: 3.5799 (3.6753)  loss_scale: 32768.0000 (61145.3399)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [35]  [1830/2809]  eta: 0:06:09  lr: 0.000002  min_lr: 0.000000  loss: 3.5747 (3.6736)  loss_scale: 32768.0000 (60990.3572)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0003  max mem: 15572
Epoch: [35]  [1840/2809]  eta: 0:06:05  lr: 0.000002  min_lr: 0.000000  loss: 3.8827 (3.6739)  loss_scale: 32768.0000 (60837.0581)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0003  max mem: 15572
Epoch: [35]  [1850/2809]  eta: 0:06:01  lr: 0.000002  min_lr: 0.000000  loss: 3.9223 (3.6745)  loss_scale: 32768.0000 (60685.4155)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0003  max mem: 15572
Epoch: [35]  [1860/2809]  eta: 0:05:57  lr: 0.000002  min_lr: 0.000000  loss: 3.9356 (3.6757)  loss_scale: 32768.0000 (60535.4025)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0003  max mem: 15572
Epoch: [35]  [1870/2809]  eta: 0:05:54  lr: 0.000002  min_lr: 0.000000  loss: 3.7724 (3.6744)  loss_scale: 32768.0000 (60386.9931)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [35]  [1880/2809]  eta: 0:05:50  lr: 0.000002  min_lr: 0.000000  loss: 3.5627 (3.6745)  loss_scale: 32768.0000 (60240.1616)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
Epoch: [35]  [1890/2809]  eta: 0:05:46  lr: 0.000002  min_lr: 0.000000  loss: 3.8824 (3.6756)  loss_scale: 32768.0000 (60094.8831)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
[2025-01-13 09:54:32,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:54:32,349] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1900/2809]  eta: 0:05:42  lr: 0.000002  min_lr: 0.000000  loss: 3.9901 (3.6762)  loss_scale: 32768.0000 (60071.7938)  weight_decay: 0.0500 (0.0500)  time: 0.3793  data: 0.0003  max mem: 15572
Epoch: [35]  [1910/2809]  eta: 0:05:39  lr: 0.000002  min_lr: 0.000000  loss: 3.7091 (3.6763)  loss_scale: 65536.0000 (60100.3872)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0005  max mem: 15572
Epoch: [35]  [1920/2809]  eta: 0:05:35  lr: 0.000002  min_lr: 0.000000  loss: 3.5321 (3.6745)  loss_scale: 65536.0000 (60128.6830)  weight_decay: 0.0500 (0.0500)  time: 0.3917  data: 0.0004  max mem: 15572
[2025-01-13 09:54:46,014] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100244
[2025-01-13 09:54:46,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:54:46,014] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1930/2809]  eta: 0:05:31  lr: 0.000002  min_lr: 0.000000  loss: 3.5028 (3.6752)  loss_scale: 65536.0000 (60122.7468)  weight_decay: 0.0500 (0.0500)  time: 0.3896  data: 0.0004  max mem: 15572
Epoch: [35]  [1940/2809]  eta: 0:05:27  lr: 0.000002  min_lr: 0.000000  loss: 3.8165 (3.6757)  loss_scale: 32768.0000 (59981.8156)  weight_decay: 0.0500 (0.0500)  time: 0.3836  data: 0.0006  max mem: 15572
Epoch: [35]  [1950/2809]  eta: 0:05:24  lr: 0.000002  min_lr: 0.000000  loss: 3.6376 (3.6755)  loss_scale: 32768.0000 (59842.3291)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0004  max mem: 15572
Epoch: [35]  [1960/2809]  eta: 0:05:20  lr: 0.000002  min_lr: 0.000000  loss: 3.7688 (3.6765)  loss_scale: 32768.0000 (59704.2652)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [35]  [1970/2809]  eta: 0:05:16  lr: 0.000002  min_lr: 0.000000  loss: 4.0193 (3.6778)  loss_scale: 32768.0000 (59567.6022)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0002  max mem: 15572
Epoch: [35]  [1980/2809]  eta: 0:05:12  lr: 0.000002  min_lr: 0.000000  loss: 3.7541 (3.6764)  loss_scale: 32768.0000 (59432.3190)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0013  max mem: 15572
Epoch: [35]  [1990/2809]  eta: 0:05:08  lr: 0.000002  min_lr: 0.000000  loss: 3.5156 (3.6761)  loss_scale: 32768.0000 (59298.3948)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0013  max mem: 15572
Epoch: [35]  [2000/2809]  eta: 0:05:05  lr: 0.000002  min_lr: 0.000000  loss: 3.7655 (3.6770)  loss_scale: 32768.0000 (59165.8091)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [35]  [2010/2809]  eta: 0:05:01  lr: 0.000002  min_lr: 0.000000  loss: 3.8227 (3.6777)  loss_scale: 32768.0000 (59034.5420)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0003  max mem: 15572
Epoch: [35]  [2020/2809]  eta: 0:04:57  lr: 0.000002  min_lr: 0.000000  loss: 3.5970 (3.6771)  loss_scale: 32768.0000 (58904.5740)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [35]  [2030/2809]  eta: 0:04:53  lr: 0.000002  min_lr: 0.000000  loss: 3.5708 (3.6767)  loss_scale: 32768.0000 (58775.8858)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [35]  [2040/2809]  eta: 0:04:49  lr: 0.000002  min_lr: 0.000000  loss: 3.8327 (3.6769)  loss_scale: 32768.0000 (58648.4586)  weight_decay: 0.0500 (0.0500)  time: 0.3802  data: 0.0004  max mem: 15572
Epoch: [35]  [2050/2809]  eta: 0:04:46  lr: 0.000002  min_lr: 0.000000  loss: 3.8617 (3.6775)  loss_scale: 32768.0000 (58522.2740)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0004  max mem: 15572
[2025-01-13 09:55:34,381] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:55:34,382] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [2060/2809]  eta: 0:04:42  lr: 0.000002  min_lr: 0.000000  loss: 3.8972 (3.6783)  loss_scale: 32768.0000 (58445.0112)  weight_decay: 0.0500 (0.0500)  time: 0.3814  data: 0.0004  max mem: 15572
Epoch: [35]  [2070/2809]  eta: 0:04:38  lr: 0.000002  min_lr: 0.000000  loss: 3.5697 (3.6781)  loss_scale: 65536.0000 (58479.2506)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0003  max mem: 15572
Epoch: [35]  [2080/2809]  eta: 0:04:34  lr: 0.000002  min_lr: 0.000000  loss: 3.4515 (3.6774)  loss_scale: 65536.0000 (58513.1610)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
Epoch: [35]  [2090/2809]  eta: 0:04:31  lr: 0.000002  min_lr: 0.000000  loss: 3.5702 (3.6770)  loss_scale: 65536.0000 (58546.7470)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [35]  [2100/2809]  eta: 0:04:27  lr: 0.000002  min_lr: 0.000000  loss: 3.5702 (3.6767)  loss_scale: 65536.0000 (58580.0133)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0002  max mem: 15572
Epoch: [35]  [2110/2809]  eta: 0:04:23  lr: 0.000002  min_lr: 0.000000  loss: 3.2377 (3.6750)  loss_scale: 65536.0000 (58612.9645)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [35]  [2120/2809]  eta: 0:04:19  lr: 0.000002  min_lr: 0.000000  loss: 3.4594 (3.6753)  loss_scale: 65536.0000 (58645.6049)  weight_decay: 0.0500 (0.0500)  time: 0.3664  data: 0.0003  max mem: 15572
Epoch: [35]  [2130/2809]  eta: 0:04:15  lr: 0.000002  min_lr: 0.000000  loss: 3.7014 (3.6746)  loss_scale: 65536.0000 (58677.9390)  weight_decay: 0.0500 (0.0500)  time: 0.3670  data: 0.0003  max mem: 15572
Epoch: [35]  [2140/2809]  eta: 0:04:12  lr: 0.000002  min_lr: 0.000000  loss: 3.5672 (3.6738)  loss_scale: 65536.0000 (58709.9710)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [35]  [2150/2809]  eta: 0:04:08  lr: 0.000002  min_lr: 0.000000  loss: 3.6239 (3.6739)  loss_scale: 65536.0000 (58741.7053)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [35]  [2160/2809]  eta: 0:04:04  lr: 0.000002  min_lr: 0.000000  loss: 3.6521 (3.6737)  loss_scale: 65536.0000 (58773.1458)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0003  max mem: 15572
Epoch: [35]  [2170/2809]  eta: 0:04:00  lr: 0.000002  min_lr: 0.000000  loss: 3.5735 (3.6729)  loss_scale: 65536.0000 (58804.2966)  weight_decay: 0.0500 (0.0500)  time: 0.3875  data: 0.0019  max mem: 15572
Epoch: [35]  [2180/2809]  eta: 0:03:57  lr: 0.000002  min_lr: 0.000000  loss: 3.7186 (3.6739)  loss_scale: 65536.0000 (58835.1619)  weight_decay: 0.0500 (0.0500)  time: 0.3787  data: 0.0019  max mem: 15572
[2025-01-13 09:56:22,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:56:22,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:56:22,393] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100502
[2025-01-13 09:56:22,393] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:56:22,393] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [2190/2809]  eta: 0:03:53  lr: 0.000002  min_lr: 0.000000  loss: 3.9354 (3.6740)  loss_scale: 65536.0000 (58895.6568)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0003  max mem: 15572
Epoch: [35]  [2200/2809]  eta: 0:03:49  lr: 0.000002  min_lr: 0.000000  loss: 3.5866 (3.6732)  loss_scale: 65536.0000 (58925.8264)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [35]  [2210/2809]  eta: 0:03:45  lr: 0.000002  min_lr: 0.000000  loss: 3.6997 (3.6742)  loss_scale: 65536.0000 (58955.7232)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [35]  [2220/2809]  eta: 0:03:41  lr: 0.000002  min_lr: 0.000000  loss: 3.9233 (3.6746)  loss_scale: 65536.0000 (58985.3507)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [35]  [2230/2809]  eta: 0:03:38  lr: 0.000002  min_lr: 0.000000  loss: 3.6135 (3.6737)  loss_scale: 65536.0000 (59014.7127)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [35]  [2240/2809]  eta: 0:03:34  lr: 0.000002  min_lr: 0.000000  loss: 3.6787 (3.6744)  loss_scale: 65536.0000 (59043.8126)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [35]  [2250/2809]  eta: 0:03:30  lr: 0.000002  min_lr: 0.000000  loss: 3.7255 (3.6742)  loss_scale: 65536.0000 (59072.6539)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0002  max mem: 15572
Epoch: [35]  [2260/2809]  eta: 0:03:26  lr: 0.000002  min_lr: 0.000000  loss: 3.7255 (3.6748)  loss_scale: 65536.0000 (59101.2402)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0002  max mem: 15572
Epoch: [35]  [2270/2809]  eta: 0:03:22  lr: 0.000002  min_lr: 0.000000  loss: 3.7546 (3.6744)  loss_scale: 65536.0000 (59129.5746)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [35]  [2280/2809]  eta: 0:03:19  lr: 0.000002  min_lr: 0.000000  loss: 3.6129 (3.6742)  loss_scale: 65536.0000 (59157.6607)  weight_decay: 0.0500 (0.0500)  time: 0.3807  data: 0.0003  max mem: 15572
Epoch: [35]  [2290/2809]  eta: 0:03:15  lr: 0.000002  min_lr: 0.000000  loss: 3.5962 (3.6740)  loss_scale: 65536.0000 (59185.5015)  weight_decay: 0.0500 (0.0500)  time: 0.3870  data: 0.0004  max mem: 15572
Epoch: [35]  [2300/2809]  eta: 0:03:11  lr: 0.000002  min_lr: 0.000000  loss: 3.5962 (3.6744)  loss_scale: 65536.0000 (59213.1004)  weight_decay: 0.0500 (0.0500)  time: 0.3871  data: 0.0004  max mem: 15572
Epoch: [35]  [2310/2809]  eta: 0:03:07  lr: 0.000002  min_lr: 0.000000  loss: 3.7562 (3.6741)  loss_scale: 65536.0000 (59240.4604)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0003  max mem: 15572
[2025-01-13 09:57:10,653] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:57:10,654] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 09:57:11,022] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100632
[2025-01-13 09:57:11,022] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:57:11,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [2320/2809]  eta: 0:03:04  lr: 0.000002  min_lr: 0.000000  loss: 3.5276 (3.6741)  loss_scale: 65536.0000 (59295.8208)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [35]  [2330/2809]  eta: 0:03:00  lr: 0.000002  min_lr: 0.000000  loss: 3.7093 (3.6749)  loss_scale: 65536.0000 (59322.5912)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
Epoch: [35]  [2340/2809]  eta: 0:02:56  lr: 0.000002  min_lr: 0.000000  loss: 3.8341 (3.6747)  loss_scale: 65536.0000 (59349.1328)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0003  max mem: 15572
Epoch: [35]  [2350/2809]  eta: 0:02:52  lr: 0.000002  min_lr: 0.000000  loss: 3.7747 (3.6744)  loss_scale: 65536.0000 (59375.4487)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
Epoch: [35]  [2360/2809]  eta: 0:02:49  lr: 0.000002  min_lr: 0.000000  loss: 3.8306 (3.6745)  loss_scale: 65536.0000 (59401.5417)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
Epoch: [35]  [2370/2809]  eta: 0:02:45  lr: 0.000002  min_lr: 0.000000  loss: 3.7306 (3.6738)  loss_scale: 65536.0000 (59427.4146)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0003  max mem: 15572
Epoch: [35]  [2380/2809]  eta: 0:02:41  lr: 0.000002  min_lr: 0.000000  loss: 3.3000 (3.6730)  loss_scale: 65536.0000 (59453.0701)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0003  max mem: 15572
Epoch: [35]  [2390/2809]  eta: 0:02:37  lr: 0.000002  min_lr: 0.000000  loss: 3.4490 (3.6728)  loss_scale: 65536.0000 (59478.5111)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0003  max mem: 15572
Epoch: [35]  [2400/2809]  eta: 0:02:33  lr: 0.000002  min_lr: 0.000000  loss: 3.7805 (3.6731)  loss_scale: 65536.0000 (59503.7401)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0004  max mem: 15572
Epoch: [35]  [2410/2809]  eta: 0:02:30  lr: 0.000002  min_lr: 0.000000  loss: 3.6247 (3.6718)  loss_scale: 65536.0000 (59528.7599)  weight_decay: 0.0500 (0.0500)  time: 0.3827  data: 0.0005  max mem: 15572
Epoch: [35]  [2420/2809]  eta: 0:02:26  lr: 0.000002  min_lr: 0.000000  loss: 3.5713 (3.6715)  loss_scale: 65536.0000 (59553.5729)  weight_decay: 0.0500 (0.0500)  time: 0.3788  data: 0.0004  max mem: 15572
Epoch: [35]  [2430/2809]  eta: 0:02:22  lr: 0.000002  min_lr: 0.000000  loss: 3.5756 (3.6712)  loss_scale: 65536.0000 (59578.1818)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0003  max mem: 15572
Epoch: [35]  [2440/2809]  eta: 0:02:18  lr: 0.000002  min_lr: 0.000000  loss: 3.5475 (3.6710)  loss_scale: 65536.0000 (59602.5891)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0003  max mem: 15572
[2025-01-13 09:57:59,261] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:57:59,261] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [2450/2809]  eta: 0:02:15  lr: 0.000002  min_lr: 0.000000  loss: 3.6635 (3.6711)  loss_scale: 65536.0000 (59760.4896)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
[2025-01-13 09:58:02,273] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100769
[2025-01-13 09:58:02,273] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:58:02,273] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [2460/2809]  eta: 0:02:11  lr: 0.000002  min_lr: 0.000000  loss: 3.6521 (3.6713)  loss_scale: 65536.0000 (59863.8472)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
Epoch: [35]  [2470/2809]  eta: 0:02:07  lr: 0.000002  min_lr: 0.000000  loss: 3.5424 (3.6703)  loss_scale: 65536.0000 (59886.8021)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [35]  [2480/2809]  eta: 0:02:03  lr: 0.000002  min_lr: 0.000000  loss: 3.5424 (3.6703)  loss_scale: 65536.0000 (59909.5719)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [35]  [2490/2809]  eta: 0:02:00  lr: 0.000002  min_lr: 0.000000  loss: 3.7864 (3.6706)  loss_scale: 65536.0000 (59932.1590)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [35]  [2500/2809]  eta: 0:01:56  lr: 0.000002  min_lr: 0.000000  loss: 3.7134 (3.6706)  loss_scale: 65536.0000 (59954.5654)  weight_decay: 0.0500 (0.0500)  time: 0.3700  data: 0.0002  max mem: 15572
Epoch: [35]  [2510/2809]  eta: 0:01:52  lr: 0.000002  min_lr: 0.000000  loss: 3.7134 (3.6706)  loss_scale: 65536.0000 (59976.7933)  weight_decay: 0.0500 (0.0500)  time: 0.3794  data: 0.0003  max mem: 15572
Epoch: [35]  [2520/2809]  eta: 0:01:48  lr: 0.000002  min_lr: 0.000000  loss: 3.7718 (3.6705)  loss_scale: 65536.0000 (59998.8449)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0004  max mem: 15572
Epoch: [35]  [2530/2809]  eta: 0:01:45  lr: 0.000002  min_lr: 0.000000  loss: 3.9786 (3.6718)  loss_scale: 65536.0000 (60020.7222)  weight_decay: 0.0500 (0.0500)  time: 0.3859  data: 0.0003  max mem: 15572
Epoch: [35]  [2540/2809]  eta: 0:01:41  lr: 0.000002  min_lr: 0.000000  loss: 3.7419 (3.6722)  loss_scale: 65536.0000 (60042.4274)  weight_decay: 0.0500 (0.0500)  time: 0.3859  data: 0.0004  max mem: 15572
Epoch: [35]  [2550/2809]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 3.7419 (3.6727)  loss_scale: 65536.0000 (60063.9624)  weight_decay: 0.0500 (0.0500)  time: 0.3852  data: 0.0004  max mem: 15572
Epoch: [35]  [2560/2809]  eta: 0:01:33  lr: 0.000002  min_lr: 0.000000  loss: 3.8396 (3.6729)  loss_scale: 65536.0000 (60085.3292)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [35]  [2570/2809]  eta: 0:01:29  lr: 0.000002  min_lr: 0.000000  loss: 3.8396 (3.6725)  loss_scale: 65536.0000 (60106.5298)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [35]  [2580/2809]  eta: 0:01:26  lr: 0.000002  min_lr: 0.000000  loss: 3.4494 (3.6721)  loss_scale: 65536.0000 (60127.5661)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0002  max mem: 15572
[2025-01-13 09:58:50,818] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 09:58:50,818] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [2590/2809]  eta: 0:01:22  lr: 0.000002  min_lr: 0.000000  loss: 3.5490 (3.6715)  loss_scale: 65536.0000 (60350.7897)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
[2025-01-13 09:58:54,108] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100907
[2025-01-13 09:58:54,109] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 09:58:54,109] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [2600/2809]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 3.5490 (3.6707)  loss_scale: 65536.0000 (60395.9216)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [35]  [2610/2809]  eta: 0:01:14  lr: 0.000002  min_lr: 0.000000  loss: 3.4694 (3.6697)  loss_scale: 65536.0000 (60415.6078)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0002  max mem: 15572
Epoch: [35]  [2620/2809]  eta: 0:01:11  lr: 0.000002  min_lr: 0.000000  loss: 3.6231 (3.6698)  loss_scale: 65536.0000 (60435.1438)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0003  max mem: 15572
Epoch: [35]  [2630/2809]  eta: 0:01:07  lr: 0.000002  min_lr: 0.000000  loss: 3.6352 (3.6698)  loss_scale: 65536.0000 (60454.5314)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0003  max mem: 15572
Epoch: [35]  [2640/2809]  eta: 0:01:03  lr: 0.000002  min_lr: 0.000000  loss: 3.6267 (3.6693)  loss_scale: 65536.0000 (60473.7721)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0003  max mem: 15572
Epoch: [35]  [2650/2809]  eta: 0:00:59  lr: 0.000002  min_lr: 0.000000  loss: 3.7159 (3.6698)  loss_scale: 65536.0000 (60492.8676)  weight_decay: 0.0500 (0.0500)  time: 0.3792  data: 0.0003  max mem: 15572
Epoch: [35]  [2660/2809]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 3.7150 (3.6693)  loss_scale: 65536.0000 (60511.8196)  weight_decay: 0.0500 (0.0500)  time: 0.3849  data: 0.0004  max mem: 15572
Epoch: [35]  [2670/2809]  eta: 0:00:52  lr: 0.000002  min_lr: 0.000000  loss: 3.7150 (3.6696)  loss_scale: 65536.0000 (60530.6297)  weight_decay: 0.0500 (0.0500)  time: 0.3822  data: 0.0004  max mem: 15572
Epoch: [35]  [2680/2809]  eta: 0:00:48  lr: 0.000002  min_lr: 0.000000  loss: 3.8023 (3.6693)  loss_scale: 65536.0000 (60549.2995)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
[2025-01-13 09:59:28,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=101000, skipped=686, lr=[1.523978886112964e-08, 1.523978886112964e-08, 2.1771126944470916e-08, 2.1771126944470916e-08, 3.110160992067274e-08, 3.110160992067274e-08, 4.4430871315246775e-08, 4.4430871315246775e-08, 6.34726733074954e-08, 6.34726733074954e-08, 9.067524758213629e-08, 9.067524758213629e-08, 1.2953606797448042e-07, 1.2953606797448042e-07, 1.8505152567782917e-07, 1.8505152567782917e-07, 2.6435932239689883e-07, 2.6435932239689883e-07, 3.7765617485271267e-07, 3.7765617485271267e-07, 5.39508821218161e-07, 5.39508821218161e-07, 7.707268874545158e-07, 7.707268874545158e-07, 1.1010384106493083e-06, 1.1010384106493083e-06, 1.5729120152132976e-06, 1.5729120152132976e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 09:59:28,491] [INFO] [timer.py:260:stop] epoch=0/micro_step=101000/global_step=101000, RunningAvgSamplesPerSec=31.393745816648668, CurrSamplesPerSec=34.54156072232254, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [35]  [2690/2809]  eta: 0:00:44  lr: 0.000002  min_lr: 0.000000  loss: 3.5599 (3.6687)  loss_scale: 65536.0000 (60567.8305)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
Epoch: [35]  [2700/2809]  eta: 0:00:41  lr: 0.000002  min_lr: 0.000000  loss: 3.7077 (3.6698)  loss_scale: 65536.0000 (60586.2244)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [35]  [2710/2809]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 3.8141 (3.6691)  loss_scale: 65536.0000 (60604.4825)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
[2025-01-13 09:59:41,458] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101034
[2025-01-13 09:59:41,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 09:59:41,459] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [2720/2809]  eta: 0:00:33  lr: 0.000002  min_lr: 0.000000  loss: 3.6091 (3.6692)  loss_scale: 65536.0000 (60598.5211)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [35]  [2730/2809]  eta: 0:00:29  lr: 0.000002  min_lr: 0.000000  loss: 3.5998 (3.6684)  loss_scale: 32768.0000 (60496.6152)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0003  max mem: 15572
Epoch: [35]  [2740/2809]  eta: 0:00:25  lr: 0.000002  min_lr: 0.000000  loss: 3.5704 (3.6683)  loss_scale: 32768.0000 (60395.4528)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [35]  [2750/2809]  eta: 0:00:22  lr: 0.000002  min_lr: 0.000000  loss: 3.8308 (3.6690)  loss_scale: 32768.0000 (60295.0258)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [35]  [2760/2809]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 3.8014 (3.6694)  loss_scale: 32768.0000 (60195.3263)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0003  max mem: 15572
Epoch: [35]  [2770/2809]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 3.7295 (3.6690)  loss_scale: 32768.0000 (60096.3464)  weight_decay: 0.0500 (0.0500)  time: 0.3854  data: 0.0004  max mem: 15572
Epoch: [35]  [2780/2809]  eta: 0:00:10  lr: 0.000002  min_lr: 0.000000  loss: 3.5627 (3.6690)  loss_scale: 32768.0000 (59998.0784)  weight_decay: 0.0500 (0.0500)  time: 0.3873  data: 0.0004  max mem: 15572
Epoch: [35]  [2790/2809]  eta: 0:00:07  lr: 0.000002  min_lr: 0.000000  loss: 3.6722 (3.6693)  loss_scale: 32768.0000 (59900.5145)  weight_decay: 0.0500 (0.0500)  time: 0.3940  data: 0.0005  max mem: 15572
Epoch: [35]  [2800/2809]  eta: 0:00:03  lr: 0.000002  min_lr: 0.000000  loss: 3.7896 (3.6694)  loss_scale: 32768.0000 (59803.6473)  weight_decay: 0.0500 (0.0500)  time: 0.3843  data: 0.0004  max mem: 15572
Epoch: [35]  [2808/2809]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 3.5440 (3.6683)  loss_scale: 32768.0000 (59726.6501)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [35] Total time: 0:17:37 (0.3765 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 3.5440 (3.6683)  loss_scale: 32768.0000 (59726.6501)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:08:43  loss: 0.3830 (0.3830)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.9254  data: 1.7631  max mem: 15572
Val:  [ 10/272]  eta: 0:02:30  loss: 2.2908 (2.2280)  acc1: 50.0000 (44.9495)  acc5: 77.7778 (75.2525)  time: 0.5732  data: 0.4225  max mem: 15572
Val:  [ 20/272]  eta: 0:01:34  loss: 2.2908 (2.2689)  acc1: 44.4444 (45.2381)  acc5: 77.7778 (74.8677)  time: 0.2980  data: 0.1444  max mem: 15572
Val:  [ 30/272]  eta: 0:01:13  loss: 2.3190 (2.3551)  acc1: 44.4444 (43.0108)  acc5: 77.7778 (73.8351)  time: 0.1564  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:01:02  loss: 2.5490 (2.4237)  acc1: 27.7778 (39.5664)  acc5: 72.2222 (73.8482)  time: 0.1614  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:55  loss: 2.4578 (2.3470)  acc1: 33.3333 (41.3943)  acc5: 77.7778 (75.4902)  time: 0.1611  data: 0.0005  max mem: 15572
Val:  [ 60/272]  eta: 0:00:49  loss: 1.4923 (2.2464)  acc1: 61.1111 (44.7177)  acc5: 83.3333 (76.3206)  time: 0.1564  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:45  loss: 1.5936 (2.1740)  acc1: 66.6667 (47.3396)  acc5: 83.3333 (77.0736)  time: 0.1644  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:42  loss: 1.8539 (2.1828)  acc1: 55.5556 (47.4623)  acc5: 77.7778 (76.8176)  time: 0.1753  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:38  loss: 2.2135 (2.1873)  acc1: 50.0000 (47.7411)  acc5: 77.7778 (77.1673)  time: 0.1741  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:35  loss: 2.1284 (2.2124)  acc1: 44.4444 (46.9747)  acc5: 83.3333 (77.0627)  time: 0.1675  data: 0.0020  max mem: 15572
Val:  [110/272]  eta: 0:00:33  loss: 2.3416 (2.2840)  acc1: 27.7778 (45.0450)  acc5: 77.7778 (76.0761)  time: 0.1666  data: 0.0021  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 2.8435 (2.3195)  acc1: 16.6667 (44.3526)  acc5: 66.6667 (75.7117)  time: 0.1674  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:28  loss: 2.1652 (2.2854)  acc1: 50.0000 (45.2926)  acc5: 83.3333 (76.4631)  time: 0.1648  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 1.6788 (2.2785)  acc1: 55.5556 (45.7841)  acc5: 88.8889 (76.2805)  time: 0.1650  data: 0.0059  max mem: 15572
Val:  [150/272]  eta: 0:00:24  loss: 2.2693 (2.2827)  acc1: 38.8889 (45.3274)  acc5: 83.3333 (76.4533)  time: 0.1902  data: 0.0338  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 2.2693 (2.2716)  acc1: 44.4444 (45.7212)  acc5: 77.7778 (76.6046)  time: 0.1848  data: 0.0283  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 2.3974 (2.2901)  acc1: 44.4444 (45.3216)  acc5: 72.2222 (76.1209)  time: 0.1649  data: 0.0005  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 2.2971 (2.2799)  acc1: 38.8889 (45.2732)  acc5: 72.2222 (76.4886)  time: 0.1698  data: 0.0005  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 2.2971 (2.3321)  acc1: 33.3333 (44.1245)  acc5: 77.7778 (75.2472)  time: 0.1601  data: 0.0005  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 2.5620 (2.3418)  acc1: 38.8889 (43.8917)  acc5: 66.6667 (74.9862)  time: 0.1590  data: 0.0005  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.1388 (2.3442)  acc1: 44.4444 (44.0495)  acc5: 77.7778 (74.8815)  time: 0.1643  data: 0.0005  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 2.1042 (2.3314)  acc1: 50.0000 (44.3439)  acc5: 77.7778 (74.9874)  time: 0.1759  data: 0.0005  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 1.7143 (2.3027)  acc1: 61.1111 (45.3824)  acc5: 77.7778 (75.3728)  time: 0.1840  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 1.5822 (2.2867)  acc1: 61.1111 (45.7354)  acc5: 83.3333 (75.6800)  time: 0.1739  data: 0.0005  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 2.1968 (2.2980)  acc1: 50.0000 (45.1970)  acc5: 83.3333 (75.6751)  time: 0.1774  data: 0.0006  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 1.1460 (2.2413)  acc1: 66.6667 (46.8710)  acc5: 88.8889 (76.3729)  time: 0.1691  data: 0.0004  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.2783 (2.2348)  acc1: 66.6667 (47.0480)  acc5: 88.8889 (76.5888)  time: 0.1429  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.2783 (2.2393)  acc1: 66.6667 (47.0203)  acc5: 88.8889 (76.5513)  time: 0.1371  data: 0.0001  max mem: 15572
Val: Total time: 0:00:49 (0.1837 s / it)
* Acc@1 47.020 Acc@5 76.551 loss 2.239
Accuracy of the network on the 4883 val videos: 47.0%
Max accuracy: 47.45%
Epoch: [36]  [   0/2809]  eta: 2:50:00  lr: 0.000002  min_lr: 0.000000  loss: 3.7395 (3.7395)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 3.6313  data: 3.1939  max mem: 15572
Epoch: [36]  [  10/2809]  eta: 0:33:08  lr: 0.000002  min_lr: 0.000000  loss: 3.7214 (3.6374)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7103  data: 0.3330  max mem: 15572
Epoch: [36]  [  20/2809]  eta: 0:25:36  lr: 0.000002  min_lr: 0.000000  loss: 3.7256 (3.7591)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3969  data: 0.0236  max mem: 15572
Epoch: [36]  [  30/2809]  eta: 0:22:50  lr: 0.000002  min_lr: 0.000000  loss: 3.7925 (3.7648)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0003  max mem: 15572
[2025-01-13 10:01:23,951] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:01:23,952] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [  40/2809]  eta: 0:21:22  lr: 0.000002  min_lr: 0.000000  loss: 3.7745 (3.7507)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0003  max mem: 15572
Epoch: [36]  [  50/2809]  eta: 0:20:29  lr: 0.000002  min_lr: 0.000000  loss: 3.8685 (3.7655)  loss_scale: 65536.0000 (40478.1176)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
Epoch: [36]  [  60/2809]  eta: 0:19:52  lr: 0.000002  min_lr: 0.000000  loss: 3.8616 (3.7600)  loss_scale: 65536.0000 (44585.9672)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [36]  [  70/2809]  eta: 0:19:22  lr: 0.000002  min_lr: 0.000000  loss: 3.6880 (3.7190)  loss_scale: 65536.0000 (47536.6761)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [36]  [  80/2809]  eta: 0:19:06  lr: 0.000002  min_lr: 0.000000  loss: 3.4885 (3.6887)  loss_scale: 65536.0000 (49758.8148)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0003  max mem: 15572
Epoch: [36]  [  90/2809]  eta: 0:18:52  lr: 0.000002  min_lr: 0.000000  loss: 3.6572 (3.6952)  loss_scale: 65536.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  time: 0.3895  data: 0.0004  max mem: 15572
Epoch: [36]  [ 100/2809]  eta: 0:18:41  lr: 0.000002  min_lr: 0.000000  loss: 3.7689 (3.6813)  loss_scale: 65536.0000 (52883.0099)  weight_decay: 0.0500 (0.0500)  time: 0.3896  data: 0.0005  max mem: 15572
Epoch: [36]  [ 110/2809]  eta: 0:18:31  lr: 0.000002  min_lr: 0.000000  loss: 3.5596 (3.6694)  loss_scale: 65536.0000 (54022.9189)  weight_decay: 0.0500 (0.0500)  time: 0.3903  data: 0.0027  max mem: 15572
Epoch: [36]  [ 120/2809]  eta: 0:18:21  lr: 0.000002  min_lr: 0.000000  loss: 3.7115 (3.6761)  loss_scale: 65536.0000 (54974.4132)  weight_decay: 0.0500 (0.0500)  time: 0.3871  data: 0.0026  max mem: 15572
Epoch: [36]  [ 130/2809]  eta: 0:18:10  lr: 0.000002  min_lr: 0.000000  loss: 3.7415 (3.6867)  loss_scale: 65536.0000 (55780.6412)  weight_decay: 0.0500 (0.0500)  time: 0.3793  data: 0.0003  max mem: 15572
Epoch: [36]  [ 140/2809]  eta: 0:17:59  lr: 0.000002  min_lr: 0.000000  loss: 3.6723 (3.6822)  loss_scale: 65536.0000 (56472.5106)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [36]  [ 150/2809]  eta: 0:17:49  lr: 0.000002  min_lr: 0.000000  loss: 3.7691 (3.6828)  loss_scale: 65536.0000 (57072.7417)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [36]  [ 160/2809]  eta: 0:17:40  lr: 0.000001  min_lr: 0.000000  loss: 3.5940 (3.6665)  loss_scale: 65536.0000 (57598.4099)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
[2025-01-13 10:02:12,402] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:02:12,403] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 10:02:12,774] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101292
[2025-01-13 10:02:12,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:02:12,774] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [ 170/2809]  eta: 0:17:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7951 (3.6851)  loss_scale: 65536.0000 (58445.8480)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.0002  max mem: 15572
Epoch: [36]  [ 180/2809]  eta: 0:17:23  lr: 0.000001  min_lr: 0.000000  loss: 3.8666 (3.6800)  loss_scale: 65536.0000 (58837.5691)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [36]  [ 190/2809]  eta: 0:17:14  lr: 0.000001  min_lr: 0.000000  loss: 3.8874 (3.6995)  loss_scale: 65536.0000 (59188.2723)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0002  max mem: 15572
Epoch: [36]  [ 200/2809]  eta: 0:17:07  lr: 0.000001  min_lr: 0.000000  loss: 3.8879 (3.7032)  loss_scale: 65536.0000 (59504.0796)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0002  max mem: 15572
Epoch: [36]  [ 210/2809]  eta: 0:17:01  lr: 0.000001  min_lr: 0.000000  loss: 3.7392 (3.7036)  loss_scale: 65536.0000 (59789.9526)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0003  max mem: 15572
[2025-01-13 10:02:30,241] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101339
[2025-01-13 10:02:30,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 10:02:30,241] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 220/2809]  eta: 0:16:56  lr: 0.000001  min_lr: 0.000000  loss: 3.6864 (3.6997)  loss_scale: 65536.0000 (59160.3258)  weight_decay: 0.0500 (0.0500)  time: 0.3789  data: 0.0004  max mem: 15572
Epoch: [36]  [ 230/2809]  eta: 0:16:51  lr: 0.000001  min_lr: 0.000000  loss: 3.5342 (3.6988)  loss_scale: 32768.0000 (58017.8009)  weight_decay: 0.0500 (0.0500)  time: 0.3830  data: 0.0005  max mem: 15572
Epoch: [36]  [ 240/2809]  eta: 0:16:46  lr: 0.000001  min_lr: 0.000000  loss: 3.6917 (3.7018)  loss_scale: 32768.0000 (56970.0913)  weight_decay: 0.0500 (0.0500)  time: 0.3807  data: 0.0005  max mem: 15572
Epoch: [36]  [ 250/2809]  eta: 0:16:39  lr: 0.000001  min_lr: 0.000000  loss: 3.6400 (3.6948)  loss_scale: 32768.0000 (56005.8645)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [36]  [ 260/2809]  eta: 0:16:33  lr: 0.000001  min_lr: 0.000000  loss: 3.7609 (3.7034)  loss_scale: 32768.0000 (55115.5249)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [36]  [ 270/2809]  eta: 0:16:28  lr: 0.000001  min_lr: 0.000000  loss: 3.7609 (3.6995)  loss_scale: 32768.0000 (54290.8930)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0002  max mem: 15572
Epoch: [36]  [ 280/2809]  eta: 0:16:22  lr: 0.000001  min_lr: 0.000000  loss: 3.6755 (3.7025)  loss_scale: 32768.0000 (53524.9537)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [36]  [ 290/2809]  eta: 0:16:16  lr: 0.000001  min_lr: 0.000000  loss: 3.8203 (3.7089)  loss_scale: 32768.0000 (52811.6564)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [36]  [ 300/2809]  eta: 0:16:11  lr: 0.000001  min_lr: 0.000000  loss: 3.8616 (3.7097)  loss_scale: 32768.0000 (52145.7542)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [36]  [ 310/2809]  eta: 0:16:06  lr: 0.000001  min_lr: 0.000000  loss: 3.6062 (3.6985)  loss_scale: 32768.0000 (51522.6752)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [36]  [ 320/2809]  eta: 0:16:01  lr: 0.000001  min_lr: 0.000000  loss: 3.4143 (3.6953)  loss_scale: 32768.0000 (50938.4174)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [36]  [ 330/2809]  eta: 0:15:57  lr: 0.000001  min_lr: 0.000000  loss: 3.8038 (3.7052)  loss_scale: 32768.0000 (50389.4622)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0003  max mem: 15572
Epoch: [36]  [ 340/2809]  eta: 0:15:54  lr: 0.000001  min_lr: 0.000000  loss: 3.8412 (3.7086)  loss_scale: 32768.0000 (49872.7038)  weight_decay: 0.0500 (0.0500)  time: 0.3941  data: 0.0003  max mem: 15572
[2025-01-13 10:03:18,827] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:03:18,827] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 350/2809]  eta: 0:15:51  lr: 0.000001  min_lr: 0.000000  loss: 3.7983 (3.7071)  loss_scale: 32768.0000 (50038.8832)  weight_decay: 0.0500 (0.0500)  time: 0.4029  data: 0.0004  max mem: 15572
Epoch: [36]  [ 360/2809]  eta: 0:15:48  lr: 0.000001  min_lr: 0.000000  loss: 3.8183 (3.7156)  loss_scale: 65536.0000 (50468.1662)  weight_decay: 0.0500 (0.0500)  time: 0.3952  data: 0.0004  max mem: 15572
Epoch: [36]  [ 370/2809]  eta: 0:15:44  lr: 0.000001  min_lr: 0.000000  loss: 3.8463 (3.7163)  loss_scale: 65536.0000 (50874.3073)  weight_decay: 0.0500 (0.0500)  time: 0.3920  data: 0.0004  max mem: 15572
Epoch: [36]  [ 380/2809]  eta: 0:15:40  lr: 0.000001  min_lr: 0.000000  loss: 3.8463 (3.7222)  loss_scale: 65536.0000 (51259.1286)  weight_decay: 0.0500 (0.0500)  time: 0.3910  data: 0.0005  max mem: 15572
Epoch: [36]  [ 390/2809]  eta: 0:15:36  lr: 0.000001  min_lr: 0.000000  loss: 3.8495 (3.7254)  loss_scale: 65536.0000 (51624.2660)  weight_decay: 0.0500 (0.0500)  time: 0.3805  data: 0.0003  max mem: 15572
Epoch: [36]  [ 400/2809]  eta: 0:15:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7461 (3.7219)  loss_scale: 65536.0000 (51971.1920)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [36]  [ 410/2809]  eta: 0:15:26  lr: 0.000001  min_lr: 0.000000  loss: 3.5424 (3.7216)  loss_scale: 65536.0000 (52301.2360)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [36]  [ 420/2809]  eta: 0:15:21  lr: 0.000001  min_lr: 0.000000  loss: 3.6917 (3.7223)  loss_scale: 65536.0000 (52615.6010)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [36]  [ 430/2809]  eta: 0:15:16  lr: 0.000001  min_lr: 0.000000  loss: 3.5578 (3.7197)  loss_scale: 65536.0000 (52915.3782)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [36]  [ 440/2809]  eta: 0:15:12  lr: 0.000001  min_lr: 0.000000  loss: 3.5578 (3.7164)  loss_scale: 65536.0000 (53201.5601)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [36]  [ 450/2809]  eta: 0:15:07  lr: 0.000001  min_lr: 0.000000  loss: 3.6320 (3.7109)  loss_scale: 65536.0000 (53475.0510)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [36]  [ 460/2809]  eta: 0:15:02  lr: 0.000001  min_lr: 0.000000  loss: 3.7377 (3.7104)  loss_scale: 65536.0000 (53736.6768)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [36]  [ 470/2809]  eta: 0:14:58  lr: 0.000001  min_lr: 0.000000  loss: 3.9787 (3.7119)  loss_scale: 65536.0000 (53987.1932)  weight_decay: 0.0500 (0.0500)  time: 0.3682  data: 0.0002  max mem: 15572
[2025-01-13 10:04:07,071] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:04:07,071] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 10:04:09,717] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101603
[2025-01-13 10:04:09,717] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:04:09,717] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [ 480/2809]  eta: 0:14:54  lr: 0.000001  min_lr: 0.000000  loss: 3.7821 (3.7097)  loss_scale: 65536.0000 (55181.0395)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0003  max mem: 15572
Epoch: [36]  [ 490/2809]  eta: 0:14:50  lr: 0.000001  min_lr: 0.000000  loss: 3.7762 (3.7099)  loss_scale: 65536.0000 (55391.9348)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0004  max mem: 15572
Epoch: [36]  [ 500/2809]  eta: 0:14:46  lr: 0.000001  min_lr: 0.000000  loss: 3.4377 (3.7034)  loss_scale: 65536.0000 (55594.4112)  weight_decay: 0.0500 (0.0500)  time: 0.3829  data: 0.0005  max mem: 15572
Epoch: [36]  [ 510/2809]  eta: 0:14:42  lr: 0.000001  min_lr: 0.000000  loss: 3.8069 (3.7078)  loss_scale: 65536.0000 (55788.9628)  weight_decay: 0.0500 (0.0500)  time: 0.3773  data: 0.0004  max mem: 15572
Epoch: [36]  [ 520/2809]  eta: 0:14:37  lr: 0.000001  min_lr: 0.000000  loss: 3.6896 (3.7056)  loss_scale: 65536.0000 (55976.0461)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [36]  [ 530/2809]  eta: 0:14:33  lr: 0.000001  min_lr: 0.000000  loss: 3.7055 (3.7101)  loss_scale: 65536.0000 (56156.0829)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0003  max mem: 15572
Epoch: [36]  [ 540/2809]  eta: 0:14:29  lr: 0.000001  min_lr: 0.000000  loss: 3.7055 (3.7049)  loss_scale: 65536.0000 (56329.4640)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [36]  [ 550/2809]  eta: 0:14:24  lr: 0.000001  min_lr: 0.000000  loss: 3.3684 (3.7009)  loss_scale: 65536.0000 (56496.5517)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [36]  [ 560/2809]  eta: 0:14:20  lr: 0.000001  min_lr: 0.000000  loss: 3.6789 (3.7024)  loss_scale: 65536.0000 (56657.6827)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0003  max mem: 15572
Epoch: [36]  [ 570/2809]  eta: 0:14:16  lr: 0.000001  min_lr: 0.000000  loss: 3.9471 (3.7057)  loss_scale: 65536.0000 (56813.1699)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0003  max mem: 15572
Epoch: [36]  [ 580/2809]  eta: 0:14:12  lr: 0.000001  min_lr: 0.000000  loss: 3.8352 (3.7057)  loss_scale: 65536.0000 (56963.3046)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [36]  [ 590/2809]  eta: 0:14:08  lr: 0.000001  min_lr: 0.000000  loss: 3.8092 (3.7052)  loss_scale: 65536.0000 (57108.3587)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [36]  [ 600/2809]  eta: 0:14:04  lr: 0.000001  min_lr: 0.000000  loss: 3.8439 (3.7088)  loss_scale: 65536.0000 (57248.5857)  weight_decay: 0.0500 (0.0500)  time: 0.3809  data: 0.0003  max mem: 15572
[2025-01-13 10:04:58,339] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:04:58,340] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [ 610/2809]  eta: 0:14:00  lr: 0.000001  min_lr: 0.000000  loss: 3.9138 (3.7086)  loss_scale: 65536.0000 (57706.0033)  weight_decay: 0.0500 (0.0500)  time: 0.3866  data: 0.0004  max mem: 15572
[2025-01-13 10:04:59,851] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101736
[2025-01-13 10:04:59,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:04:59,851] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [ 620/2809]  eta: 0:13:57  lr: 0.000001  min_lr: 0.000000  loss: 3.8202 (3.7083)  loss_scale: 65536.0000 (57937.6232)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0005  max mem: 15572
Epoch: [36]  [ 630/2809]  eta: 0:13:53  lr: 0.000001  min_lr: 0.000000  loss: 3.6872 (3.7076)  loss_scale: 65536.0000 (58058.0412)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0005  max mem: 15572
Epoch: [36]  [ 640/2809]  eta: 0:13:49  lr: 0.000001  min_lr: 0.000000  loss: 3.5662 (3.7050)  loss_scale: 65536.0000 (58174.7020)  weight_decay: 0.0500 (0.0500)  time: 0.3812  data: 0.0004  max mem: 15572
Epoch: [36]  [ 650/2809]  eta: 0:13:45  lr: 0.000001  min_lr: 0.000000  loss: 3.7108 (3.7078)  loss_scale: 65536.0000 (58287.7788)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [36]  [ 660/2809]  eta: 0:13:41  lr: 0.000001  min_lr: 0.000000  loss: 3.9476 (3.7071)  loss_scale: 65536.0000 (58397.4342)  weight_decay: 0.0500 (0.0500)  time: 0.3702  data: 0.0002  max mem: 15572
Epoch: [36]  [ 670/2809]  eta: 0:13:37  lr: 0.000001  min_lr: 0.000000  loss: 3.7955 (3.7078)  loss_scale: 65536.0000 (58503.8212)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [36]  [ 680/2809]  eta: 0:13:33  lr: 0.000001  min_lr: 0.000000  loss: 3.6776 (3.7045)  loss_scale: 65536.0000 (58607.0837)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0003  max mem: 15572
Epoch: [36]  [ 690/2809]  eta: 0:13:28  lr: 0.000001  min_lr: 0.000000  loss: 3.7629 (3.7037)  loss_scale: 65536.0000 (58707.3575)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0003  max mem: 15572
Epoch: [36]  [ 700/2809]  eta: 0:13:24  lr: 0.000001  min_lr: 0.000000  loss: 3.6735 (3.7010)  loss_scale: 65536.0000 (58804.7703)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [36]  [ 710/2809]  eta: 0:13:20  lr: 0.000001  min_lr: 0.000000  loss: 3.5272 (3.6969)  loss_scale: 65536.0000 (58899.4430)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [36]  [ 720/2809]  eta: 0:13:16  lr: 0.000001  min_lr: 0.000000  loss: 3.4352 (3.6945)  loss_scale: 65536.0000 (58991.4896)  weight_decay: 0.0500 (0.0500)  time: 0.3699  data: 0.0002  max mem: 15572
Epoch: [36]  [ 730/2809]  eta: 0:13:12  lr: 0.000001  min_lr: 0.000000  loss: 3.4700 (3.6941)  loss_scale: 65536.0000 (59081.0178)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0003  max mem: 15572
Epoch: [36]  [ 740/2809]  eta: 0:13:08  lr: 0.000001  min_lr: 0.000000  loss: 3.5235 (3.6915)  loss_scale: 65536.0000 (59168.1296)  weight_decay: 0.0500 (0.0500)  time: 0.3849  data: 0.0004  max mem: 15572
[2025-01-13 10:05:48,388] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:05:48,389] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 10:05:49,571] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101868
[2025-01-13 10:05:49,571] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:05:49,571] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [ 750/2809]  eta: 0:13:05  lr: 0.000001  min_lr: 0.000000  loss: 3.6113 (3.6933)  loss_scale: 65536.0000 (59514.7164)  weight_decay: 0.0500 (0.0500)  time: 0.3865  data: 0.0004  max mem: 15572
Epoch: [36]  [ 760/2809]  eta: 0:13:01  lr: 0.000001  min_lr: 0.000000  loss: 3.7987 (3.6947)  loss_scale: 65536.0000 (59593.8397)  weight_decay: 0.0500 (0.0500)  time: 0.3837  data: 0.0004  max mem: 15572
Epoch: [36]  [ 770/2809]  eta: 0:12:57  lr: 0.000001  min_lr: 0.000000  loss: 3.7675 (3.6955)  loss_scale: 65536.0000 (59670.9105)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [36]  [ 780/2809]  eta: 0:12:53  lr: 0.000001  min_lr: 0.000000  loss: 3.6957 (3.6963)  loss_scale: 65536.0000 (59746.0077)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [36]  [ 790/2809]  eta: 0:12:49  lr: 0.000001  min_lr: 0.000000  loss: 3.8430 (3.7008)  loss_scale: 65536.0000 (59819.2061)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.0002  max mem: 15572
Epoch: [36]  [ 800/2809]  eta: 0:12:44  lr: 0.000001  min_lr: 0.000000  loss: 3.7456 (3.6974)  loss_scale: 65536.0000 (59890.5768)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0002  max mem: 15572
Epoch: [36]  [ 810/2809]  eta: 0:12:40  lr: 0.000001  min_lr: 0.000000  loss: 3.7357 (3.6994)  loss_scale: 65536.0000 (59960.1874)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0003  max mem: 15572
Epoch: [36]  [ 820/2809]  eta: 0:12:36  lr: 0.000001  min_lr: 0.000000  loss: 3.8469 (3.6995)  loss_scale: 65536.0000 (60028.1023)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0003  max mem: 15572
Epoch: [36]  [ 830/2809]  eta: 0:12:32  lr: 0.000001  min_lr: 0.000000  loss: 3.7733 (3.6973)  loss_scale: 65536.0000 (60094.3827)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0003  max mem: 15572
Epoch: [36]  [ 840/2809]  eta: 0:12:28  lr: 0.000001  min_lr: 0.000000  loss: 3.7878 (3.6988)  loss_scale: 65536.0000 (60159.0868)  weight_decay: 0.0500 (0.0500)  time: 0.3708  data: 0.0002  max mem: 15572
Epoch: [36]  [ 850/2809]  eta: 0:12:25  lr: 0.000001  min_lr: 0.000000  loss: 3.8091 (3.6996)  loss_scale: 65536.0000 (60222.2703)  weight_decay: 0.0500 (0.0500)  time: 0.3779  data: 0.0003  max mem: 15572
Epoch: [36]  [ 860/2809]  eta: 0:12:21  lr: 0.000001  min_lr: 0.000000  loss: 3.8939 (3.7029)  loss_scale: 65536.0000 (60283.9861)  weight_decay: 0.0500 (0.0500)  time: 0.3925  data: 0.0005  max mem: 15572
Epoch: [36]  [ 870/2809]  eta: 0:12:18  lr: 0.000001  min_lr: 0.000000  loss: 3.8672 (3.7037)  loss_scale: 65536.0000 (60344.2847)  weight_decay: 0.0500 (0.0500)  time: 0.3962  data: 0.0005  max mem: 15572
[2025-01-13 10:06:38,179] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:06:38,179] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 10:06:38,552] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101998
[2025-01-13 10:06:38,552] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:06:38,552] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-13 10:06:38,952] [INFO] [logging.py:96:log_dist] [Rank 0] step=102000, skipped=693, lr=[1.2774264980564707e-08, 1.2774264980564707e-08, 1.82489499722353e-08, 1.82489499722353e-08, 2.6069928531764718e-08, 2.6069928531764718e-08, 3.724275504537817e-08, 3.724275504537817e-08, 5.320393577911167e-08, 5.320393577911167e-08, 7.60056225415881e-08, 7.60056225415881e-08, 1.085794607736973e-07, 1.085794607736973e-07, 1.5511351539099616e-07, 1.5511351539099616e-07, 2.2159073627285164e-07, 2.2159073627285164e-07, 3.165581946755024e-07, 3.165581946755024e-07, 4.5222599239357485e-07, 4.5222599239357485e-07, 6.460371319908213e-07, 6.460371319908213e-07, 9.229101885583161e-07, 9.229101885583161e-07, 1.3184431265118803e-06, 1.3184431265118803e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 10:06:38,952] [INFO] [timer.py:260:stop] epoch=0/micro_step=102000/global_step=102000, RunningAvgSamplesPerSec=31.41481397637097, CurrSamplesPerSec=31.654726355643326, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [36]  [ 880/2809]  eta: 0:12:14  lr: 0.000001  min_lr: 0.000000  loss: 3.7658 (3.7039)  loss_scale: 65536.0000 (60477.6027)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0004  max mem: 15572
Epoch: [36]  [ 890/2809]  eta: 0:12:10  lr: 0.000001  min_lr: 0.000000  loss: 3.7025 (3.7042)  loss_scale: 65536.0000 (60534.3749)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0004  max mem: 15572
Epoch: [36]  [ 900/2809]  eta: 0:12:06  lr: 0.000001  min_lr: 0.000000  loss: 3.6928 (3.7021)  loss_scale: 65536.0000 (60589.8868)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0003  max mem: 15572
Epoch: [36]  [ 910/2809]  eta: 0:12:02  lr: 0.000001  min_lr: 0.000000  loss: 3.7853 (3.7013)  loss_scale: 65536.0000 (60644.1800)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0002  max mem: 15572
Epoch: [36]  [ 920/2809]  eta: 0:11:58  lr: 0.000001  min_lr: 0.000000  loss: 3.6174 (3.6992)  loss_scale: 65536.0000 (60697.2942)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0002  max mem: 15572
Epoch: [36]  [ 930/2809]  eta: 0:11:54  lr: 0.000001  min_lr: 0.000000  loss: 3.6694 (3.7019)  loss_scale: 65536.0000 (60749.2675)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0002  max mem: 15572
Epoch: [36]  [ 940/2809]  eta: 0:11:50  lr: 0.000001  min_lr: 0.000000  loss: 3.7134 (3.7014)  loss_scale: 65536.0000 (60800.1360)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [36]  [ 950/2809]  eta: 0:11:46  lr: 0.000001  min_lr: 0.000000  loss: 3.3643 (3.6971)  loss_scale: 65536.0000 (60849.9348)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0003  max mem: 15572
Epoch: [36]  [ 960/2809]  eta: 0:11:42  lr: 0.000001  min_lr: 0.000000  loss: 3.3740 (3.6974)  loss_scale: 65536.0000 (60898.6972)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [36]  [ 970/2809]  eta: 0:11:38  lr: 0.000001  min_lr: 0.000000  loss: 3.7380 (3.6983)  loss_scale: 65536.0000 (60946.4552)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [36]  [ 980/2809]  eta: 0:11:34  lr: 0.000001  min_lr: 0.000000  loss: 3.7067 (3.6949)  loss_scale: 65536.0000 (60993.2396)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
[2025-01-13 10:07:19,205] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102107
[2025-01-13 10:07:19,205] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 10:07:19,205] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 990/2809]  eta: 0:11:30  lr: 0.000001  min_lr: 0.000000  loss: 3.4149 (3.6925)  loss_scale: 65536.0000 (60774.5550)  weight_decay: 0.0500 (0.0500)  time: 0.3797  data: 0.0003  max mem: 15572
Epoch: [36]  [1000/2809]  eta: 0:11:27  lr: 0.000001  min_lr: 0.000000  loss: 3.6092 (3.6938)  loss_scale: 32768.0000 (60494.7692)  weight_decay: 0.0500 (0.0500)  time: 0.3806  data: 0.0004  max mem: 15572
Epoch: [36]  [1010/2809]  eta: 0:11:23  lr: 0.000001  min_lr: 0.000000  loss: 3.6539 (3.6924)  loss_scale: 32768.0000 (60220.5183)  weight_decay: 0.0500 (0.0500)  time: 0.3822  data: 0.0004  max mem: 15572
Epoch: [36]  [1020/2809]  eta: 0:11:19  lr: 0.000001  min_lr: 0.000000  loss: 4.0037 (3.6964)  loss_scale: 32768.0000 (59951.6396)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0003  max mem: 15572
Epoch: [36]  [1030/2809]  eta: 0:11:15  lr: 0.000001  min_lr: 0.000000  loss: 3.7213 (3.6958)  loss_scale: 32768.0000 (59687.9767)  weight_decay: 0.0500 (0.0500)  time: 0.3692  data: 0.0002  max mem: 15572
Epoch: [36]  [1040/2809]  eta: 0:11:11  lr: 0.000001  min_lr: 0.000000  loss: 3.6807 (3.6944)  loss_scale: 32768.0000 (59429.3794)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [36]  [1050/2809]  eta: 0:11:07  lr: 0.000001  min_lr: 0.000000  loss: 3.8179 (3.6962)  loss_scale: 32768.0000 (59175.7031)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [36]  [1060/2809]  eta: 0:11:03  lr: 0.000001  min_lr: 0.000000  loss: 3.7676 (3.6945)  loss_scale: 32768.0000 (58926.8087)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0002  max mem: 15572
Epoch: [36]  [1070/2809]  eta: 0:10:59  lr: 0.000001  min_lr: 0.000000  loss: 3.5541 (3.6942)  loss_scale: 32768.0000 (58682.5621)  weight_decay: 0.0500 (0.0500)  time: 0.3689  data: 0.0003  max mem: 15572
Epoch: [36]  [1080/2809]  eta: 0:10:55  lr: 0.000001  min_lr: 0.000000  loss: 3.6815 (3.6938)  loss_scale: 32768.0000 (58442.8344)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0003  max mem: 15572
Epoch: [36]  [1090/2809]  eta: 0:10:52  lr: 0.000001  min_lr: 0.000000  loss: 3.6588 (3.6916)  loss_scale: 32768.0000 (58207.5014)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [36]  [1100/2809]  eta: 0:10:48  lr: 0.000001  min_lr: 0.000000  loss: 3.6588 (3.6927)  loss_scale: 32768.0000 (57976.4432)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [36]  [1110/2809]  eta: 0:10:44  lr: 0.000001  min_lr: 0.000000  loss: 3.6389 (3.6902)  loss_scale: 32768.0000 (57749.5446)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0005  max mem: 15572
[2025-01-13 10:08:07,919] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:08:07,919] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1120/2809]  eta: 0:10:41  lr: 0.000001  min_lr: 0.000000  loss: 3.4627 (3.6909)  loss_scale: 32768.0000 (57789.7734)  weight_decay: 0.0500 (0.0500)  time: 0.3964  data: 0.0006  max mem: 15572
Epoch: [36]  [1130/2809]  eta: 0:10:37  lr: 0.000001  min_lr: 0.000000  loss: 3.5398 (3.6891)  loss_scale: 65536.0000 (57858.2635)  weight_decay: 0.0500 (0.0500)  time: 0.3936  data: 0.0005  max mem: 15572
Epoch: [36]  [1140/2809]  eta: 0:10:34  lr: 0.000001  min_lr: 0.000000  loss: 3.5515 (3.6902)  loss_scale: 65536.0000 (57925.5530)  weight_decay: 0.0500 (0.0500)  time: 0.3973  data: 0.0004  max mem: 15572
Epoch: [36]  [1150/2809]  eta: 0:10:30  lr: 0.000001  min_lr: 0.000000  loss: 3.8925 (3.6905)  loss_scale: 65536.0000 (57991.6733)  weight_decay: 0.0500 (0.0500)  time: 0.3948  data: 0.0004  max mem: 15572
Epoch: [36]  [1160/2809]  eta: 0:10:26  lr: 0.000001  min_lr: 0.000000  loss: 3.8925 (3.6914)  loss_scale: 65536.0000 (58056.6546)  weight_decay: 0.0500 (0.0500)  time: 0.3804  data: 0.0003  max mem: 15572
Epoch: [36]  [1170/2809]  eta: 0:10:22  lr: 0.000001  min_lr: 0.000000  loss: 3.7892 (3.6916)  loss_scale: 65536.0000 (58120.5260)  weight_decay: 0.0500 (0.0500)  time: 0.3685  data: 0.0003  max mem: 15572
Epoch: [36]  [1180/2809]  eta: 0:10:18  lr: 0.000001  min_lr: 0.000000  loss: 3.7869 (3.6923)  loss_scale: 65536.0000 (58183.3158)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [36]  [1190/2809]  eta: 0:10:14  lr: 0.000001  min_lr: 0.000000  loss: 3.6868 (3.6906)  loss_scale: 65536.0000 (58245.0512)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [36]  [1200/2809]  eta: 0:10:10  lr: 0.000001  min_lr: 0.000000  loss: 3.7393 (3.6912)  loss_scale: 65536.0000 (58305.7585)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0002  max mem: 15572
Epoch: [36]  [1210/2809]  eta: 0:10:06  lr: 0.000001  min_lr: 0.000000  loss: 3.7381 (3.6894)  loss_scale: 65536.0000 (58365.4633)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0002  max mem: 15572
Epoch: [36]  [1220/2809]  eta: 0:10:02  lr: 0.000001  min_lr: 0.000000  loss: 3.6920 (3.6907)  loss_scale: 65536.0000 (58424.1900)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0003  max mem: 15572
[2025-01-13 10:08:51,347] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102351
[2025-01-13 10:08:51,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 10:08:51,347] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [1230/2809]  eta: 0:09:58  lr: 0.000001  min_lr: 0.000000  loss: 4.0064 (3.6918)  loss_scale: 65536.0000 (58375.4866)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0003  max mem: 15572
Epoch: [36]  [1240/2809]  eta: 0:09:55  lr: 0.000001  min_lr: 0.000000  loss: 3.6237 (3.6903)  loss_scale: 32768.0000 (58169.1410)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [36]  [1250/2809]  eta: 0:09:51  lr: 0.000001  min_lr: 0.000000  loss: 3.5819 (3.6896)  loss_scale: 32768.0000 (57966.0943)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [36]  [1260/2809]  eta: 0:09:47  lr: 0.000001  min_lr: 0.000000  loss: 3.8496 (3.6917)  loss_scale: 32768.0000 (57766.2680)  weight_decay: 0.0500 (0.0500)  time: 0.3811  data: 0.0004  max mem: 15572
Epoch: [36]  [1270/2809]  eta: 0:09:43  lr: 0.000001  min_lr: 0.000000  loss: 3.7216 (3.6892)  loss_scale: 32768.0000 (57569.5862)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0003  max mem: 15572
Epoch: [36]  [1280/2809]  eta: 0:09:39  lr: 0.000001  min_lr: 0.000000  loss: 3.4810 (3.6861)  loss_scale: 32768.0000 (57375.9750)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0003  max mem: 15572
Epoch: [36]  [1290/2809]  eta: 0:09:35  lr: 0.000001  min_lr: 0.000000  loss: 3.6712 (3.6865)  loss_scale: 32768.0000 (57185.3633)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [36]  [1300/2809]  eta: 0:09:32  lr: 0.000001  min_lr: 0.000000  loss: 3.7543 (3.6876)  loss_scale: 32768.0000 (56997.6818)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [36]  [1310/2809]  eta: 0:09:28  lr: 0.000001  min_lr: 0.000000  loss: 3.7474 (3.6856)  loss_scale: 32768.0000 (56812.8635)  weight_decay: 0.0500 (0.0500)  time: 0.3679  data: 0.0002  max mem: 15572
Epoch: [36]  [1320/2809]  eta: 0:09:24  lr: 0.000001  min_lr: 0.000000  loss: 3.6116 (3.6841)  loss_scale: 32768.0000 (56630.8433)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [36]  [1330/2809]  eta: 0:09:20  lr: 0.000001  min_lr: 0.000000  loss: 3.5535 (3.6825)  loss_scale: 32768.0000 (56451.5582)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [36]  [1340/2809]  eta: 0:09:16  lr: 0.000001  min_lr: 0.000000  loss: 3.6806 (3.6830)  loss_scale: 32768.0000 (56274.9471)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [36]  [1350/2809]  eta: 0:09:12  lr: 0.000001  min_lr: 0.000000  loss: 3.8007 (3.6836)  loss_scale: 32768.0000 (56100.9504)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
[2025-01-13 10:09:39,561] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:09:39,561] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1360/2809]  eta: 0:09:08  lr: 0.000001  min_lr: 0.000000  loss: 3.8338 (3.6842)  loss_scale: 32768.0000 (56049.8927)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [36]  [1370/2809]  eta: 0:09:05  lr: 0.000001  min_lr: 0.000000  loss: 3.9568 (3.6844)  loss_scale: 65536.0000 (56119.0839)  weight_decay: 0.0500 (0.0500)  time: 0.3890  data: 0.0004  max mem: 15572
Epoch: [36]  [1380/2809]  eta: 0:09:01  lr: 0.000001  min_lr: 0.000000  loss: 3.8537 (3.6856)  loss_scale: 65536.0000 (56187.2730)  weight_decay: 0.0500 (0.0500)  time: 0.3898  data: 0.0005  max mem: 15572
Epoch: [36]  [1390/2809]  eta: 0:08:57  lr: 0.000001  min_lr: 0.000000  loss: 3.5805 (3.6843)  loss_scale: 65536.0000 (56254.4817)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0004  max mem: 15572
Epoch: [36]  [1400/2809]  eta: 0:08:53  lr: 0.000001  min_lr: 0.000000  loss: 3.5645 (3.6841)  loss_scale: 65536.0000 (56320.7309)  weight_decay: 0.0500 (0.0500)  time: 0.3715  data: 0.0002  max mem: 15572
Epoch: [36]  [1410/2809]  eta: 0:08:49  lr: 0.000001  min_lr: 0.000000  loss: 3.7748 (3.6852)  loss_scale: 65536.0000 (56386.0411)  weight_decay: 0.0500 (0.0500)  time: 0.3711  data: 0.0002  max mem: 15572
Epoch: [36]  [1420/2809]  eta: 0:08:46  lr: 0.000001  min_lr: 0.000000  loss: 3.7748 (3.6851)  loss_scale: 65536.0000 (56450.4321)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0002  max mem: 15572
Epoch: [36]  [1430/2809]  eta: 0:08:42  lr: 0.000001  min_lr: 0.000000  loss: 3.6820 (3.6852)  loss_scale: 65536.0000 (56513.9231)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [36]  [1440/2809]  eta: 0:08:38  lr: 0.000001  min_lr: 0.000000  loss: 3.8872 (3.6862)  loss_scale: 65536.0000 (56576.5330)  weight_decay: 0.0500 (0.0500)  time: 0.3717  data: 0.0002  max mem: 15572
Epoch: [36]  [1450/2809]  eta: 0:08:34  lr: 0.000001  min_lr: 0.000000  loss: 3.8808 (3.6858)  loss_scale: 65536.0000 (56638.2798)  weight_decay: 0.0500 (0.0500)  time: 0.3680  data: 0.0002  max mem: 15572
Epoch: [36]  [1460/2809]  eta: 0:08:30  lr: 0.000001  min_lr: 0.000000  loss: 3.8280 (3.6861)  loss_scale: 65536.0000 (56699.1814)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0003  max mem: 15572
Epoch: [36]  [1470/2809]  eta: 0:08:26  lr: 0.000001  min_lr: 0.000000  loss: 3.6878 (3.6860)  loss_scale: 65536.0000 (56759.2549)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0003  max mem: 15572
[2025-01-13 10:10:23,528] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102597
[2025-01-13 10:10:23,528] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 10:10:23,528] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [1480/2809]  eta: 0:08:23  lr: 0.000001  min_lr: 0.000000  loss: 3.6602 (3.6856)  loss_scale: 65536.0000 (56641.5125)  weight_decay: 0.0500 (0.0500)  time: 0.3806  data: 0.0004  max mem: 15572
Epoch: [36]  [1490/2809]  eta: 0:08:19  lr: 0.000001  min_lr: 0.000000  loss: 3.5682 (3.6835)  loss_scale: 32768.0000 (56481.3950)  weight_decay: 0.0500 (0.0500)  time: 0.3846  data: 0.0004  max mem: 15572
Epoch: [36]  [1500/2809]  eta: 0:08:15  lr: 0.000001  min_lr: 0.000000  loss: 3.8723 (3.6849)  loss_scale: 32768.0000 (56323.4111)  weight_decay: 0.0500 (0.0500)  time: 0.3895  data: 0.0004  max mem: 15572
Epoch: [36]  [1510/2809]  eta: 0:08:12  lr: 0.000001  min_lr: 0.000000  loss: 3.7064 (3.6843)  loss_scale: 32768.0000 (56167.5182)  weight_decay: 0.0500 (0.0500)  time: 0.3883  data: 0.0005  max mem: 15572
Epoch: [36]  [1520/2809]  eta: 0:08:08  lr: 0.000001  min_lr: 0.000000  loss: 3.6214 (3.6850)  loss_scale: 32768.0000 (56013.6752)  weight_decay: 0.0500 (0.0500)  time: 0.3786  data: 0.0003  max mem: 15572
Epoch: [36]  [1530/2809]  eta: 0:08:04  lr: 0.000001  min_lr: 0.000000  loss: 3.6214 (3.6840)  loss_scale: 32768.0000 (55861.8419)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [36]  [1540/2809]  eta: 0:08:00  lr: 0.000001  min_lr: 0.000000  loss: 3.4137 (3.6827)  loss_scale: 32768.0000 (55711.9792)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0002  max mem: 15572
Epoch: [36]  [1550/2809]  eta: 0:07:56  lr: 0.000001  min_lr: 0.000000  loss: 3.6533 (3.6829)  loss_scale: 32768.0000 (55564.0490)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [36]  [1560/2809]  eta: 0:07:52  lr: 0.000001  min_lr: 0.000000  loss: 3.7693 (3.6842)  loss_scale: 32768.0000 (55418.0141)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [36]  [1570/2809]  eta: 0:07:48  lr: 0.000001  min_lr: 0.000000  loss: 3.7643 (3.6843)  loss_scale: 32768.0000 (55273.8383)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0002  max mem: 15572
Epoch: [36]  [1580/2809]  eta: 0:07:44  lr: 0.000001  min_lr: 0.000000  loss: 3.7145 (3.6844)  loss_scale: 32768.0000 (55131.4864)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [36]  [1590/2809]  eta: 0:07:41  lr: 0.000001  min_lr: 0.000000  loss: 3.6886 (3.6853)  loss_scale: 32768.0000 (54990.9239)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [36]  [1600/2809]  eta: 0:07:37  lr: 0.000001  min_lr: 0.000000  loss: 3.8212 (3.6860)  loss_scale: 32768.0000 (54852.1174)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
[2025-01-13 10:11:11,979] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:11:11,980] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1610/2809]  eta: 0:07:33  lr: 0.000001  min_lr: 0.000000  loss: 3.8212 (3.6863)  loss_scale: 32768.0000 (54898.0956)  weight_decay: 0.0500 (0.0500)  time: 0.3850  data: 0.0004  max mem: 15572
Epoch: [36]  [1620/2809]  eta: 0:07:29  lr: 0.000001  min_lr: 0.000000  loss: 3.7558 (3.6872)  loss_scale: 65536.0000 (54963.7212)  weight_decay: 0.0500 (0.0500)  time: 0.3882  data: 0.0004  max mem: 15572
Epoch: [36]  [1630/2809]  eta: 0:07:26  lr: 0.000001  min_lr: 0.000000  loss: 3.7558 (3.6860)  loss_scale: 65536.0000 (55028.5420)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0003  max mem: 15572
Epoch: [36]  [1640/2809]  eta: 0:07:22  lr: 0.000001  min_lr: 0.000000  loss: 3.8136 (3.6868)  loss_scale: 65536.0000 (55092.5728)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [36]  [1650/2809]  eta: 0:07:18  lr: 0.000001  min_lr: 0.000000  loss: 3.8094 (3.6863)  loss_scale: 65536.0000 (55155.8280)  weight_decay: 0.0500 (0.0500)  time: 0.3684  data: 0.0002  max mem: 15572
Epoch: [36]  [1660/2809]  eta: 0:07:14  lr: 0.000001  min_lr: 0.000000  loss: 3.7388 (3.6864)  loss_scale: 65536.0000 (55218.3215)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [36]  [1670/2809]  eta: 0:07:10  lr: 0.000001  min_lr: 0.000000  loss: 3.7305 (3.6865)  loss_scale: 65536.0000 (55280.0670)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0002  max mem: 15572
Epoch: [36]  [1680/2809]  eta: 0:07:06  lr: 0.000001  min_lr: 0.000000  loss: 3.4962 (3.6858)  loss_scale: 65536.0000 (55341.0779)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0003  max mem: 15572
Epoch: [36]  [1690/2809]  eta: 0:07:03  lr: 0.000001  min_lr: 0.000000  loss: 3.5233 (3.6863)  loss_scale: 65536.0000 (55401.3672)  weight_decay: 0.0500 (0.0500)  time: 0.3678  data: 0.0003  max mem: 15572
Epoch: [36]  [1700/2809]  eta: 0:06:59  lr: 0.000001  min_lr: 0.000000  loss: 3.7766 (3.6863)  loss_scale: 65536.0000 (55460.9477)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0002  max mem: 15572
Epoch: [36]  [1710/2809]  eta: 0:06:55  lr: 0.000001  min_lr: 0.000000  loss: 3.7766 (3.6871)  loss_scale: 65536.0000 (55519.8317)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [36]  [1720/2809]  eta: 0:06:51  lr: 0.000001  min_lr: 0.000000  loss: 3.8612 (3.6878)  loss_scale: 65536.0000 (55578.0314)  weight_decay: 0.0500 (0.0500)  time: 0.3809  data: 0.0003  max mem: 15572
[2025-01-13 10:12:00,010] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:12:00,010] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [1730/2809]  eta: 0:06:47  lr: 0.000001  min_lr: 0.000000  loss: 3.6995 (3.6876)  loss_scale: 65536.0000 (55673.4188)  weight_decay: 0.0500 (0.0500)  time: 0.3854  data: 0.0004  max mem: 15572
[2025-01-13 10:12:00,386] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102855
[2025-01-13 10:12:00,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:12:00,386] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [1740/2809]  eta: 0:06:44  lr: 0.000001  min_lr: 0.000000  loss: 3.6608 (3.6874)  loss_scale: 65536.0000 (55730.0678)  weight_decay: 0.0500 (0.0500)  time: 0.3844  data: 0.0004  max mem: 15572
Epoch: [36]  [1750/2809]  eta: 0:06:40  lr: 0.000001  min_lr: 0.000000  loss: 3.7235 (3.6875)  loss_scale: 65536.0000 (55786.0697)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0003  max mem: 15572
Epoch: [36]  [1760/2809]  eta: 0:06:36  lr: 0.000001  min_lr: 0.000000  loss: 3.7033 (3.6870)  loss_scale: 65536.0000 (55841.4355)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [36]  [1770/2809]  eta: 0:06:32  lr: 0.000001  min_lr: 0.000000  loss: 3.6882 (3.6873)  loss_scale: 65536.0000 (55896.1762)  weight_decay: 0.0500 (0.0500)  time: 0.3837  data: 0.0019  max mem: 15572
Epoch: [36]  [1780/2809]  eta: 0:06:29  lr: 0.000001  min_lr: 0.000000  loss: 3.6375 (3.6864)  loss_scale: 65536.0000 (55950.3021)  weight_decay: 0.0500 (0.0500)  time: 0.3871  data: 0.0021  max mem: 15572
Epoch: [36]  [1790/2809]  eta: 0:06:25  lr: 0.000001  min_lr: 0.000000  loss: 3.6713 (3.6866)  loss_scale: 65536.0000 (56003.8236)  weight_decay: 0.0500 (0.0500)  time: 0.3805  data: 0.0005  max mem: 15572
Epoch: [36]  [1800/2809]  eta: 0:06:21  lr: 0.000001  min_lr: 0.000000  loss: 3.7531 (3.6861)  loss_scale: 65536.0000 (56056.7507)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [36]  [1810/2809]  eta: 0:06:17  lr: 0.000001  min_lr: 0.000000  loss: 3.6973 (3.6862)  loss_scale: 65536.0000 (56109.0933)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0003  max mem: 15572
Epoch: [36]  [1820/2809]  eta: 0:06:13  lr: 0.000001  min_lr: 0.000000  loss: 3.6656 (3.6862)  loss_scale: 65536.0000 (56160.8611)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0003  max mem: 15572
Epoch: [36]  [1830/2809]  eta: 0:06:10  lr: 0.000001  min_lr: 0.000000  loss: 3.6949 (3.6867)  loss_scale: 65536.0000 (56212.0634)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0003  max mem: 15572
Epoch: [36]  [1840/2809]  eta: 0:06:06  lr: 0.000001  min_lr: 0.000000  loss: 3.5474 (3.6846)  loss_scale: 65536.0000 (56262.7094)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0003  max mem: 15572
Epoch: [36]  [1850/2809]  eta: 0:06:02  lr: 0.000001  min_lr: 0.000000  loss: 3.5890 (3.6865)  loss_scale: 65536.0000 (56312.8082)  weight_decay: 0.0500 (0.0500)  time: 0.3876  data: 0.0004  max mem: 15572
[2025-01-13 10:12:49,106] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:12:49,106] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [1860/2809]  eta: 0:05:58  lr: 0.000001  min_lr: 0.000000  loss: 3.8844 (3.6862)  loss_scale: 65536.0000 (56397.5841)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0003  max mem: 15572
[2025-01-13 10:12:49,468] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102985
[2025-01-13 10:12:49,469] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:12:49,469] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [1870/2809]  eta: 0:05:54  lr: 0.000001  min_lr: 0.000000  loss: 3.7499 (3.6867)  loss_scale: 65536.0000 (56446.4265)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
[2025-01-13 10:12:54,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=103000, skipped=698, lr=[1.0527781531835162e-08, 1.0527781531835162e-08, 1.5039687902621662e-08, 1.5039687902621662e-08, 2.1485268432316662e-08, 2.1485268432316662e-08, 3.069324061759523e-08, 3.069324061759523e-08, 4.384748659656462e-08, 4.384748659656462e-08, 6.26392665665209e-08, 6.26392665665209e-08, 8.948466652360128e-08, 8.948466652360128e-08, 1.2783523789085898e-07, 1.2783523789085898e-07, 1.8262176841551284e-07, 1.8262176841551284e-07, 2.608882405935898e-07, 2.608882405935898e-07, 3.7269748656227115e-07, 3.7269748656227115e-07, 5.324249808032445e-07, 5.324249808032445e-07, 7.606071154332065e-07, 7.606071154332065e-07, 1.0865815934760094e-06, 1.0865815934760094e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 10:12:54,724] [INFO] [timer.py:260:stop] epoch=0/micro_step=103000/global_step=103000, RunningAvgSamplesPerSec=31.43627422429021, CurrSamplesPerSec=34.808368673078995, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [36]  [1880/2809]  eta: 0:05:51  lr: 0.000001  min_lr: 0.000000  loss: 3.7562 (3.6867)  loss_scale: 65536.0000 (56494.7496)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0003  max mem: 15572
Epoch: [36]  [1890/2809]  eta: 0:05:47  lr: 0.000001  min_lr: 0.000000  loss: 3.5105 (3.6858)  loss_scale: 65536.0000 (56542.5616)  weight_decay: 0.0500 (0.0500)  time: 0.3688  data: 0.0003  max mem: 15572
Epoch: [36]  [1900/2809]  eta: 0:05:43  lr: 0.000001  min_lr: 0.000000  loss: 3.4806 (3.6844)  loss_scale: 65536.0000 (56589.8706)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0003  max mem: 15572
Epoch: [36]  [1910/2809]  eta: 0:05:39  lr: 0.000001  min_lr: 0.000000  loss: 3.5643 (3.6845)  loss_scale: 65536.0000 (56636.6845)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0002  max mem: 15572
Epoch: [36]  [1920/2809]  eta: 0:05:35  lr: 0.000001  min_lr: 0.000000  loss: 3.6914 (3.6841)  loss_scale: 65536.0000 (56683.0109)  weight_decay: 0.0500 (0.0500)  time: 0.3681  data: 0.0002  max mem: 15572
Epoch: [36]  [1930/2809]  eta: 0:05:32  lr: 0.000001  min_lr: 0.000000  loss: 3.5877 (3.6826)  loss_scale: 65536.0000 (56728.8576)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
Epoch: [36]  [1940/2809]  eta: 0:05:28  lr: 0.000001  min_lr: 0.000000  loss: 3.4966 (3.6828)  loss_scale: 65536.0000 (56774.2318)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0003  max mem: 15572
Epoch: [36]  [1950/2809]  eta: 0:05:24  lr: 0.000001  min_lr: 0.000000  loss: 3.6423 (3.6823)  loss_scale: 65536.0000 (56819.1410)  weight_decay: 0.0500 (0.0500)  time: 0.3797  data: 0.0003  max mem: 15572
Epoch: [36]  [1960/2809]  eta: 0:05:20  lr: 0.000001  min_lr: 0.000000  loss: 3.6439 (3.6821)  loss_scale: 65536.0000 (56863.5920)  weight_decay: 0.0500 (0.0500)  time: 0.3821  data: 0.0004  max mem: 15572
Epoch: [36]  [1970/2809]  eta: 0:05:17  lr: 0.000001  min_lr: 0.000000  loss: 3.5981 (3.6809)  loss_scale: 65536.0000 (56907.5921)  weight_decay: 0.0500 (0.0500)  time: 0.3839  data: 0.0004  max mem: 15572
Epoch: [36]  [1980/2809]  eta: 0:05:13  lr: 0.000001  min_lr: 0.000000  loss: 3.6528 (3.6807)  loss_scale: 65536.0000 (56951.1479)  weight_decay: 0.0500 (0.0500)  time: 0.3923  data: 0.0005  max mem: 15572
[2025-01-13 10:13:38,129] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:13:38,129] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [1990/2809]  eta: 0:05:09  lr: 0.000001  min_lr: 0.000000  loss: 3.7627 (3.6811)  loss_scale: 65536.0000 (57027.1823)  weight_decay: 0.0500 (0.0500)  time: 0.3865  data: 0.0004  max mem: 15572
[2025-01-13 10:13:38,489] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103115
[2025-01-13 10:13:38,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:13:38,489] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [2000/2809]  eta: 0:05:05  lr: 0.000001  min_lr: 0.000000  loss: 3.7593 (3.6811)  loss_scale: 65536.0000 (57069.7051)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0003  max mem: 15572
Epoch: [36]  [2010/2809]  eta: 0:05:01  lr: 0.000001  min_lr: 0.000000  loss: 3.7176 (3.6814)  loss_scale: 65536.0000 (57111.8051)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0002  max mem: 15572
Epoch: [36]  [2020/2809]  eta: 0:04:58  lr: 0.000001  min_lr: 0.000000  loss: 3.6761 (3.6806)  loss_scale: 65536.0000 (57153.4884)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0002  max mem: 15572
Epoch: [36]  [2030/2809]  eta: 0:04:54  lr: 0.000001  min_lr: 0.000000  loss: 3.4098 (3.6790)  loss_scale: 65536.0000 (57194.7612)  weight_decay: 0.0500 (0.0500)  time: 0.3694  data: 0.0002  max mem: 15572
Epoch: [36]  [2040/2809]  eta: 0:04:50  lr: 0.000001  min_lr: 0.000000  loss: 3.5366 (3.6782)  loss_scale: 65536.0000 (57235.6296)  weight_decay: 0.0500 (0.0500)  time: 0.3718  data: 0.0002  max mem: 15572
Epoch: [36]  [2050/2809]  eta: 0:04:46  lr: 0.000001  min_lr: 0.000000  loss: 3.7813 (3.6794)  loss_scale: 65536.0000 (57276.0995)  weight_decay: 0.0500 (0.0500)  time: 0.3705  data: 0.0002  max mem: 15572
Epoch: [36]  [2060/2809]  eta: 0:04:42  lr: 0.000001  min_lr: 0.000000  loss: 3.8215 (3.6791)  loss_scale: 65536.0000 (57316.1766)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [36]  [2070/2809]  eta: 0:04:39  lr: 0.000001  min_lr: 0.000000  loss: 3.8570 (3.6803)  loss_scale: 65536.0000 (57355.8667)  weight_decay: 0.0500 (0.0500)  time: 0.3714  data: 0.0002  max mem: 15572
Epoch: [36]  [2080/2809]  eta: 0:04:35  lr: 0.000001  min_lr: 0.000000  loss: 3.7551 (3.6796)  loss_scale: 65536.0000 (57395.1754)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0003  max mem: 15572
Epoch: [36]  [2090/2809]  eta: 0:04:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7377 (3.6800)  loss_scale: 65536.0000 (57434.1081)  weight_decay: 0.0500 (0.0500)  time: 0.3830  data: 0.0004  max mem: 15572
Epoch: [36]  [2100/2809]  eta: 0:04:27  lr: 0.000001  min_lr: 0.000000  loss: 3.8226 (3.6807)  loss_scale: 65536.0000 (57472.6702)  weight_decay: 0.0500 (0.0500)  time: 0.3882  data: 0.0004  max mem: 15572
Epoch: [36]  [2110/2809]  eta: 0:04:24  lr: 0.000001  min_lr: 0.000000  loss: 3.7564 (3.6804)  loss_scale: 65536.0000 (57510.8669)  weight_decay: 0.0500 (0.0500)  time: 0.3809  data: 0.0003  max mem: 15572
[2025-01-13 10:14:26,709] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:14:26,709] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [2120/2809]  eta: 0:04:20  lr: 0.000001  min_lr: 0.000000  loss: 3.6755 (3.6799)  loss_scale: 65536.0000 (57579.6021)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0003  max mem: 15572
[2025-01-13 10:14:27,447] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103246
[2025-01-13 10:14:27,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:14:27,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [2130/2809]  eta: 0:04:16  lr: 0.000001  min_lr: 0.000000  loss: 3.5880 (3.6791)  loss_scale: 65536.0000 (57647.6922)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [36]  [2140/2809]  eta: 0:04:12  lr: 0.000001  min_lr: 0.000000  loss: 3.2911 (3.6783)  loss_scale: 65536.0000 (57684.5362)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [36]  [2150/2809]  eta: 0:04:08  lr: 0.000001  min_lr: 0.000000  loss: 3.5645 (3.6790)  loss_scale: 65536.0000 (57721.0377)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0003  max mem: 15572
Epoch: [36]  [2160/2809]  eta: 0:04:05  lr: 0.000001  min_lr: 0.000000  loss: 3.7710 (3.6790)  loss_scale: 65536.0000 (57757.2013)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [36]  [2170/2809]  eta: 0:04:01  lr: 0.000001  min_lr: 0.000000  loss: 3.6102 (3.6780)  loss_scale: 65536.0000 (57793.0318)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [36]  [2180/2809]  eta: 0:03:57  lr: 0.000001  min_lr: 0.000000  loss: 3.5214 (3.6778)  loss_scale: 65536.0000 (57828.5337)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0003  max mem: 15572
Epoch: [36]  [2190/2809]  eta: 0:03:53  lr: 0.000001  min_lr: 0.000000  loss: 3.5638 (3.6776)  loss_scale: 65536.0000 (57863.7115)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
Epoch: [36]  [2200/2809]  eta: 0:03:49  lr: 0.000001  min_lr: 0.000000  loss: 3.7590 (3.6782)  loss_scale: 65536.0000 (57898.5697)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0003  max mem: 15572
Epoch: [36]  [2210/2809]  eta: 0:03:46  lr: 0.000001  min_lr: 0.000000  loss: 3.7988 (3.6780)  loss_scale: 65536.0000 (57933.1126)  weight_decay: 0.0500 (0.0500)  time: 0.3903  data: 0.0003  max mem: 15572
Epoch: [36]  [2220/2809]  eta: 0:03:42  lr: 0.000001  min_lr: 0.000000  loss: 3.7338 (3.6780)  loss_scale: 65536.0000 (57967.3444)  weight_decay: 0.0500 (0.0500)  time: 0.3881  data: 0.0004  max mem: 15572
Epoch: [36]  [2230/2809]  eta: 0:03:38  lr: 0.000001  min_lr: 0.000000  loss: 3.6737 (3.6768)  loss_scale: 65536.0000 (58001.2694)  weight_decay: 0.0500 (0.0500)  time: 0.3873  data: 0.0005  max mem: 15572
Epoch: [36]  [2240/2809]  eta: 0:03:34  lr: 0.000001  min_lr: 0.000000  loss: 3.6773 (3.6771)  loss_scale: 65536.0000 (58034.8916)  weight_decay: 0.0500 (0.0500)  time: 0.3820  data: 0.0005  max mem: 15572
Epoch: [36]  [2250/2809]  eta: 0:03:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7930 (3.6773)  loss_scale: 65536.0000 (58068.2150)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0003  max mem: 15572
[2025-01-13 10:15:16,260] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:15:16,260] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 10:15:17,381] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103378
[2025-01-13 10:15:17,381] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:15:17,381] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [2260/2809]  eta: 0:03:27  lr: 0.000001  min_lr: 0.000000  loss: 3.8488 (3.6775)  loss_scale: 65536.0000 (58188.1999)  weight_decay: 0.0500 (0.0500)  time: 0.3683  data: 0.0002  max mem: 15572
Epoch: [36]  [2270/2809]  eta: 0:03:23  lr: 0.000001  min_lr: 0.000000  loss: 3.8488 (3.6785)  loss_scale: 65536.0000 (58220.5548)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0003  max mem: 15572
Epoch: [36]  [2280/2809]  eta: 0:03:19  lr: 0.000001  min_lr: 0.000000  loss: 3.7640 (3.6777)  loss_scale: 65536.0000 (58252.6260)  weight_decay: 0.0500 (0.0500)  time: 0.3704  data: 0.0002  max mem: 15572
Epoch: [36]  [2290/2809]  eta: 0:03:15  lr: 0.000001  min_lr: 0.000000  loss: 3.6486 (3.6783)  loss_scale: 65536.0000 (58284.4173)  weight_decay: 0.0500 (0.0500)  time: 0.3706  data: 0.0002  max mem: 15572
Epoch: [36]  [2300/2809]  eta: 0:03:12  lr: 0.000001  min_lr: 0.000000  loss: 3.8141 (3.6791)  loss_scale: 65536.0000 (58315.9322)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0002  max mem: 15572
Epoch: [36]  [2310/2809]  eta: 0:03:08  lr: 0.000001  min_lr: 0.000000  loss: 3.8467 (3.6787)  loss_scale: 65536.0000 (58347.1744)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [36]  [2320/2809]  eta: 0:03:04  lr: 0.000001  min_lr: 0.000000  loss: 3.5767 (3.6779)  loss_scale: 65536.0000 (58378.1474)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.0002  max mem: 15572
Epoch: [36]  [2330/2809]  eta: 0:03:00  lr: 0.000001  min_lr: 0.000000  loss: 3.3400 (3.6769)  loss_scale: 65536.0000 (58408.8546)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [36]  [2340/2809]  eta: 0:02:57  lr: 0.000001  min_lr: 0.000000  loss: 3.5777 (3.6763)  loss_scale: 65536.0000 (58439.2994)  weight_decay: 0.0500 (0.0500)  time: 0.3789  data: 0.0003  max mem: 15572
Epoch: [36]  [2350/2809]  eta: 0:02:53  lr: 0.000001  min_lr: 0.000000  loss: 3.6832 (3.6770)  loss_scale: 65536.0000 (58469.4853)  weight_decay: 0.0500 (0.0500)  time: 0.3860  data: 0.0004  max mem: 15572
Epoch: [36]  [2360/2809]  eta: 0:02:49  lr: 0.000001  min_lr: 0.000000  loss: 3.8184 (3.6775)  loss_scale: 65536.0000 (58499.4155)  weight_decay: 0.0500 (0.0500)  time: 0.3836  data: 0.0004  max mem: 15572
Epoch: [36]  [2370/2809]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 3.8187 (3.6781)  loss_scale: 65536.0000 (58529.0932)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0003  max mem: 15572
Epoch: [36]  [2380/2809]  eta: 0:02:41  lr: 0.000001  min_lr: 0.000000  loss: 3.6306 (3.6781)  loss_scale: 65536.0000 (58558.5216)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
[2025-01-13 10:16:05,621] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:16:05,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 10:16:05,987] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103508
[2025-01-13 10:16:05,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:16:05,987] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [2390/2809]  eta: 0:02:38  lr: 0.000001  min_lr: 0.000000  loss: 3.6520 (3.6784)  loss_scale: 65536.0000 (58615.1133)  weight_decay: 0.0500 (0.0500)  time: 0.3809  data: 0.0005  max mem: 15572
Epoch: [36]  [2400/2809]  eta: 0:02:34  lr: 0.000001  min_lr: 0.000000  loss: 3.6520 (3.6783)  loss_scale: 65536.0000 (58643.9384)  weight_decay: 0.0500 (0.0500)  time: 0.3990  data: 0.0006  max mem: 15572
Epoch: [36]  [2410/2809]  eta: 0:02:30  lr: 0.000001  min_lr: 0.000000  loss: 3.8583 (3.6789)  loss_scale: 65536.0000 (58672.5243)  weight_decay: 0.0500 (0.0500)  time: 0.4169  data: 0.0007  max mem: 15572
Epoch: [36]  [2420/2809]  eta: 0:02:27  lr: 0.000001  min_lr: 0.000000  loss: 3.9110 (3.6789)  loss_scale: 65536.0000 (58700.8740)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0870  max mem: 15572
Epoch: [36]  [2430/2809]  eta: 0:02:23  lr: 0.000001  min_lr: 0.000000  loss: 3.6132 (3.6775)  loss_scale: 65536.0000 (58728.9905)  weight_decay: 0.0500 (0.0500)  time: 0.4879  data: 0.0868  max mem: 15572
Epoch: [36]  [2440/2809]  eta: 0:02:19  lr: 0.000001  min_lr: 0.000000  loss: 3.4272 (3.6777)  loss_scale: 65536.0000 (58756.8767)  weight_decay: 0.0500 (0.0500)  time: 0.3892  data: 0.0003  max mem: 15572
Epoch: [36]  [2450/2809]  eta: 0:02:16  lr: 0.000001  min_lr: 0.000000  loss: 3.9400 (3.6787)  loss_scale: 65536.0000 (58784.5353)  weight_decay: 0.0500 (0.0500)  time: 0.4035  data: 0.0006  max mem: 15572
Epoch: [36]  [2460/2809]  eta: 0:02:12  lr: 0.000001  min_lr: 0.000000  loss: 3.8104 (3.6784)  loss_scale: 65536.0000 (58811.9691)  weight_decay: 0.0500 (0.0500)  time: 0.4394  data: 0.0170  max mem: 15572
Epoch: [36]  [2470/2809]  eta: 0:02:09  lr: 0.000001  min_lr: 0.000000  loss: 3.7153 (3.6786)  loss_scale: 65536.0000 (58839.1809)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1516  max mem: 15572
Epoch: [36]  [2480/2809]  eta: 0:02:05  lr: 0.000001  min_lr: 0.000000  loss: 3.6511 (3.6776)  loss_scale: 65536.0000 (58866.1733)  weight_decay: 0.0500 (0.0500)  time: 0.6746  data: 0.2250  max mem: 15572
Epoch: [36]  [2490/2809]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 3.3854 (3.6763)  loss_scale: 65536.0000 (58892.9490)  weight_decay: 0.0500 (0.0500)  time: 0.7188  data: 0.2817  max mem: 15572
Epoch: [36]  [2500/2809]  eta: 0:01:58  lr: 0.000001  min_lr: 0.000000  loss: 3.5121 (3.6770)  loss_scale: 65536.0000 (58919.5106)  weight_decay: 0.0500 (0.0500)  time: 0.6989  data: 0.2692  max mem: 15572
Epoch: [36]  [2510/2809]  eta: 0:01:55  lr: 0.000001  min_lr: 0.000000  loss: 3.5121 (3.6762)  loss_scale: 65536.0000 (58945.8606)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1227  max mem: 15572
[2025-01-13 10:17:13,550] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:17:13,551] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 10:17:13,994] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103638
[2025-01-13 10:17:13,995] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:17:13,995] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [2520/2809]  eta: 0:01:51  lr: 0.000001  min_lr: 0.000000  loss: 3.6161 (3.6766)  loss_scale: 65536.0000 (58997.9976)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1118  max mem: 15572
Epoch: [36]  [2530/2809]  eta: 0:01:47  lr: 0.000001  min_lr: 0.000000  loss: 3.8806 (3.6773)  loss_scale: 65536.0000 (59023.8293)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1616  max mem: 15572
Epoch: [36]  [2540/2809]  eta: 0:01:44  lr: 0.000001  min_lr: 0.000000  loss: 3.8279 (3.6767)  loss_scale: 65536.0000 (59049.4577)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1385  max mem: 15572
Epoch: [36]  [2550/2809]  eta: 0:01:40  lr: 0.000001  min_lr: 0.000000  loss: 3.7964 (3.6770)  loss_scale: 65536.0000 (59074.8851)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0993  max mem: 15572
Epoch: [36]  [2560/2809]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 3.5269 (3.6764)  loss_scale: 65536.0000 (59100.1140)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1504  max mem: 15572
Epoch: [36]  [2570/2809]  eta: 0:01:33  lr: 0.000001  min_lr: 0.000000  loss: 3.7129 (3.6769)  loss_scale: 65536.0000 (59125.1466)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0949  max mem: 15572
Epoch: [36]  [2580/2809]  eta: 0:01:29  lr: 0.000001  min_lr: 0.000000  loss: 3.8350 (3.6766)  loss_scale: 65536.0000 (59149.9853)  weight_decay: 0.0500 (0.0500)  time: 0.4140  data: 0.0004  max mem: 15572
Epoch: [36]  [2590/2809]  eta: 0:01:25  lr: 0.000001  min_lr: 0.000000  loss: 3.8441 (3.6771)  loss_scale: 65536.0000 (59174.6322)  weight_decay: 0.0500 (0.0500)  time: 0.4258  data: 0.0006  max mem: 15572
Epoch: [36]  [2600/2809]  eta: 0:01:21  lr: 0.000001  min_lr: 0.000000  loss: 3.5944 (3.6773)  loss_scale: 65536.0000 (59199.0896)  weight_decay: 0.0500 (0.0500)  time: 0.4415  data: 0.0007  max mem: 15572
