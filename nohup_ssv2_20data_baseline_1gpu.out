/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-12 20:26:35,726] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f1c1bd9e6b0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-12 20:26:38,966] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-12 20:26:38,966] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-12 20:26:38,993] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.04812979698181152 seconds
[2025-01-12 20:26:39,261] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-12 20:26:39,261] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-12 20:26:39,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-12 20:26:39,264] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-12 20:26:39,270] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-12 20:26:39,270] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-12 20:26:39,270] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-12 20:26:39,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 20:26:39,271] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1c1a4bae30>
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-12 20:26:39,271] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-12 20:26:39,272] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-12 20:26:39,273] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train baseline
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4034304 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 4034273 got signal: 1
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-12 20:26:53,268] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f102c49e6b0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-12 20:26:57,045] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-12 20:26:57,045] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-12 20:26:57,074] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.04883146286010742 seconds
[2025-01-12 20:26:57,444] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-12 20:26:57,444] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-12 20:26:57,447] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-12 20:26:57,447] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-12 20:26:57,455] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-12 20:26:57,455] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-12 20:26:57,455] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-12 20:26:57,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 20:26:57,456] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-12 20:26:57,456] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1000e1ee30>
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-12 20:26:57,457] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-12 20:26:57,458] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-12 20:26:57,458] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train baseline
Epoch: [0]  [   0/2809]  eta: 13:01:14  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 16.6872  data: 5.4961  max mem: 15572
Epoch: [0]  [  10/2809]  eta: 1:28:17  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.8928  data: 0.5005  max mem: 15572
Epoch: [0]  [  20/2809]  eta: 0:56:19  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4379  data: 0.0009  max mem: 15572
Epoch: [0]  [  30/2809]  eta: 0:45:15  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4734  data: 0.0010  max mem: 15572
Epoch: [0]  [  40/2809]  eta: 0:39:27  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0011  max mem: 15572
Epoch: [0]  [  50/2809]  eta: 0:36:22  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0338  max mem: 15572
Epoch: [0]  [  60/2809]  eta: 0:34:38  lr: 0.000000  min_lr: 0.000000  loss: 5.1600 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0937  max mem: 15572
Epoch: [0]  [  70/2809]  eta: 0:34:07  lr: 0.000000  min_lr: 0.000000  loss: 5.1599 (5.1600)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1856  max mem: 15572
Epoch: [0]  [  80/2809]  eta: 0:33:12  lr: 0.000000  min_lr: 0.000000  loss: 5.1598 (5.1600)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.2154  max mem: 15572
Epoch: [0]  [  90/2809]  eta: 0:32:21  lr: 0.000000  min_lr: 0.000000  loss: 5.1597 (5.1599)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1495  max mem: 15572
Epoch: [0]  [ 100/2809]  eta: 0:31:43  lr: 0.000000  min_lr: 0.000000  loss: 5.1594 (5.1599)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1238  max mem: 15572
Epoch: [0]  [ 110/2809]  eta: 0:31:33  lr: 0.000000  min_lr: 0.000000  loss: 5.1590 (5.1598)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.1839  max mem: 15572
Epoch: [0]  [ 120/2809]  eta: 0:31:02  lr: 0.000000  min_lr: 0.000000  loss: 5.1588 (5.1597)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1963  max mem: 15572
[2025-01-12 20:28:25,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:28:25,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/2809]  eta: 0:30:17  lr: 0.000000  min_lr: 0.000000  loss: 5.1586 (5.1596)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1066  max mem: 15572
Epoch: [0]  [ 140/2809]  eta: 0:29:45  lr: 0.000000  min_lr: 0.000000  loss: 5.1583 (5.1595)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0836  max mem: 15572
Epoch: [0]  [ 150/2809]  eta: 0:29:47  lr: 0.000001  min_lr: 0.000000  loss: 5.1583 (5.1595)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1767  max mem: 15572
Epoch: [0]  [ 160/2809]  eta: 0:29:16  lr: 0.000001  min_lr: 0.000000  loss: 5.1584 (5.1594)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1713  max mem: 15572
Epoch: [0]  [ 170/2809]  eta: 0:29:06  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1593)  loss_scale: 256.0000 (160.1871)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1351  max mem: 15572
Epoch: [0]  [ 180/2809]  eta: 0:28:48  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1431  max mem: 15572
Epoch: [0]  [ 190/2809]  eta: 0:28:32  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  loss_scale: 256.0000 (170.2199)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1185  max mem: 15572
Epoch: [0]  [ 200/2809]  eta: 0:28:14  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1591)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1013  max mem: 15572
Epoch: [0]  [ 210/2809]  eta: 0:28:06  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1591)  loss_scale: 256.0000 (178.3507)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1259  max mem: 15572
Epoch: [0]  [ 220/2809]  eta: 0:27:50  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1590)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1210  max mem: 15572
Epoch: [0]  [ 230/2809]  eta: 0:27:37  lr: 0.000001  min_lr: 0.000000  loss: 5.1576 (5.1590)  loss_scale: 256.0000 (185.0736)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0623  max mem: 15572
Epoch: [0]  [ 240/2809]  eta: 0:27:23  lr: 0.000001  min_lr: 0.000000  loss: 5.1574 (5.1589)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0705  max mem: 15572
Epoch: [0]  [ 250/2809]  eta: 0:27:19  lr: 0.000001  min_lr: 0.000000  loss: 5.1574 (5.1588)  loss_scale: 256.0000 (190.7251)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1564  max mem: 15572
[2025-01-12 20:29:41,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:29:41,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [ 260/2809]  eta: 0:27:10  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1588)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.2117  max mem: 15572
Epoch: [0]  [ 270/2809]  eta: 0:26:57  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1587)  loss_scale: 512.0000 (209.7122)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1697  max mem: 15572
Epoch: [0]  [ 280/2809]  eta: 0:26:43  lr: 0.000001  min_lr: 0.000000  loss: 5.1571 (5.1587)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1330  max mem: 15572
Epoch: [0]  [ 290/2809]  eta: 0:26:32  lr: 0.000001  min_lr: 0.000000  loss: 5.1567 (5.1586)  loss_scale: 512.0000 (230.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1421  max mem: 15572
Epoch: [0]  [ 300/2809]  eta: 0:26:15  lr: 0.000001  min_lr: 0.000000  loss: 5.1563 (5.1585)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.1073  max mem: 15572
Epoch: [0]  [ 310/2809]  eta: 0:26:08  lr: 0.000001  min_lr: 0.000000  loss: 5.1563 (5.1584)  loss_scale: 512.0000 (248.5916)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1070  max mem: 15572
Epoch: [0]  [ 320/2809]  eta: 0:25:59  lr: 0.000001  min_lr: 0.000000  loss: 5.1562 (5.1584)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1476  max mem: 15572
Epoch: [0]  [ 330/2809]  eta: 0:25:45  lr: 0.000001  min_lr: 0.000000  loss: 5.1555 (5.1583)  loss_scale: 512.0000 (264.5076)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0822  max mem: 15572
Epoch: [0]  [ 340/2809]  eta: 0:25:43  lr: 0.000001  min_lr: 0.000000  loss: 5.1554 (5.1582)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1339  max mem: 15572
Epoch: [0]  [ 350/2809]  eta: 0:25:37  lr: 0.000001  min_lr: 0.000000  loss: 5.1553 (5.1581)  loss_scale: 512.0000 (278.6097)  weight_decay: 0.0500 (0.0500)  time: 0.6584  data: 0.2076  max mem: 15572
Epoch: [0]  [ 360/2809]  eta: 0:25:20  lr: 0.000001  min_lr: 0.000000  loss: 5.1548 (5.1580)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0894  max mem: 15572
Epoch: [0]  [ 370/2809]  eta: 0:25:15  lr: 0.000001  min_lr: 0.000000  loss: 5.1541 (5.1579)  loss_scale: 512.0000 (291.1914)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0978  max mem: 15572
Epoch: [0]  [ 380/2809]  eta: 0:25:07  lr: 0.000001  min_lr: 0.000000  loss: 5.1541 (5.1578)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1757  max mem: 15572
[2025-01-12 20:30:56,074] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:30:56,075] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [ 390/2809]  eta: 0:24:56  lr: 0.000001  min_lr: 0.000000  loss: 5.1535 (5.1577)  loss_scale: 512.0000 (311.6522)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1324  max mem: 15572
Epoch: [0]  [ 400/2809]  eta: 0:24:48  lr: 0.000001  min_lr: 0.000000  loss: 5.1520 (5.1576)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1183  max mem: 15572
Epoch: [0]  [ 410/2809]  eta: 0:24:40  lr: 0.000001  min_lr: 0.000000  loss: 5.1539 (5.1575)  loss_scale: 1024.0000 (346.3163)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1185  max mem: 15572
Epoch: [0]  [ 420/2809]  eta: 0:24:33  lr: 0.000001  min_lr: 0.000000  loss: 5.1532 (5.1573)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1199  max mem: 15572
Epoch: [0]  [ 430/2809]  eta: 0:24:28  lr: 0.000001  min_lr: 0.000000  loss: 5.1515 (5.1572)  loss_scale: 1024.0000 (377.7633)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1563  max mem: 15572
Epoch: [0]  [ 440/2809]  eta: 0:24:17  lr: 0.000001  min_lr: 0.000000  loss: 5.1515 (5.1571)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0907  max mem: 15572
Epoch: [0]  [ 450/2809]  eta: 0:24:09  lr: 0.000002  min_lr: 0.000000  loss: 5.1515 (5.1569)  loss_scale: 1024.0000 (406.4213)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0476  max mem: 15572
Epoch: [0]  [ 460/2809]  eta: 0:24:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1496 (5.1568)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1226  max mem: 15572
Epoch: [0]  [ 470/2809]  eta: 0:23:53  lr: 0.000002  min_lr: 0.000000  loss: 5.1490 (5.1566)  loss_scale: 1024.0000 (432.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1311  max mem: 15572
Epoch: [0]  [ 480/2809]  eta: 0:23:44  lr: 0.000002  min_lr: 0.000000  loss: 5.1484 (5.1564)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1097  max mem: 15572
Epoch: [0]  [ 490/2809]  eta: 0:23:38  lr: 0.000002  min_lr: 0.000000  loss: 5.1474 (5.1562)  loss_scale: 1024.0000 (456.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1478  max mem: 15572
Epoch: [0]  [ 500/2809]  eta: 0:23:31  lr: 0.000002  min_lr: 0.000000  loss: 5.1458 (5.1560)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1630  max mem: 15572
Epoch: [0]  [ 510/2809]  eta: 0:23:21  lr: 0.000002  min_lr: 0.000000  loss: 5.1456 (5.1558)  loss_scale: 1024.0000 (478.9354)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0952  max mem: 15572
[2025-01-12 20:32:10,207] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:32:10,208] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [0]  [ 520/2809]  eta: 0:23:15  lr: 0.000002  min_lr: 0.000000  loss: 5.1441 (5.1556)  loss_scale: 1024.0000 (507.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1014  max mem: 15572
Epoch: [0]  [ 530/2809]  eta: 0:23:12  lr: 0.000002  min_lr: 0.000000  loss: 5.1440 (5.1553)  loss_scale: 2048.0000 (536.1055)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.1862  max mem: 15572
Epoch: [0]  [ 540/2809]  eta: 0:23:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1419 (5.1551)  loss_scale: 2048.0000 (564.0518)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1556  max mem: 15572
Epoch: [0]  [ 550/2809]  eta: 0:22:56  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1549)  loss_scale: 2048.0000 (590.9837)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1249  max mem: 15572
Epoch: [0]  [ 560/2809]  eta: 0:22:50  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1547)  loss_scale: 2048.0000 (616.9554)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1623  max mem: 15572
Epoch: [0]  [ 570/2809]  eta: 0:22:46  lr: 0.000002  min_lr: 0.000000  loss: 5.1395 (5.1545)  loss_scale: 2048.0000 (642.0175)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.2005  max mem: 15572
Epoch: [0]  [ 580/2809]  eta: 0:22:37  lr: 0.000002  min_lr: 0.000000  loss: 5.1396 (5.1543)  loss_scale: 2048.0000 (666.2169)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1718  max mem: 15572
Epoch: [0]  [ 590/2809]  eta: 0:22:32  lr: 0.000002  min_lr: 0.000000  loss: 5.1376 (5.1540)  loss_scale: 2048.0000 (689.5973)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1316  max mem: 15572
Epoch: [0]  [ 600/2809]  eta: 0:22:25  lr: 0.000002  min_lr: 0.000000  loss: 5.1369 (5.1537)  loss_scale: 2048.0000 (712.1997)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1457  max mem: 15572
Epoch: [0]  [ 610/2809]  eta: 0:22:24  lr: 0.000002  min_lr: 0.000000  loss: 5.1369 (5.1535)  loss_scale: 2048.0000 (734.0622)  weight_decay: 0.0500 (0.0500)  time: 0.6594  data: 0.1998  max mem: 15572
Epoch: [0]  [ 620/2809]  eta: 0:22:14  lr: 0.000002  min_lr: 0.000000  loss: 5.1334 (5.1531)  loss_scale: 2048.0000 (755.2206)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1384  max mem: 15572
Epoch: [0]  [ 630/2809]  eta: 0:22:05  lr: 0.000002  min_lr: 0.000000  loss: 5.1308 (5.1529)  loss_scale: 2048.0000 (775.7084)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0321  max mem: 15572
[2025-01-12 20:33:27,259] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:33:27,259] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 640/2809]  eta: 0:21:57  lr: 0.000002  min_lr: 0.000000  loss: 5.1382 (5.1527)  loss_scale: 2048.0000 (798.7520)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0911  max mem: 15572
Epoch: [0]  [ 650/2809]  eta: 0:21:51  lr: 0.000002  min_lr: 0.000000  loss: 5.1335 (5.1523)  loss_scale: 4096.0000 (849.4009)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1509  max mem: 15572
Epoch: [0]  [ 660/2809]  eta: 0:21:45  lr: 0.000002  min_lr: 0.000000  loss: 5.1286 (5.1520)  loss_scale: 4096.0000 (898.5174)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1769  max mem: 15572
Epoch: [0]  [ 670/2809]  eta: 0:21:35  lr: 0.000002  min_lr: 0.000000  loss: 5.1264 (5.1516)  loss_scale: 4096.0000 (946.1699)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1142  max mem: 15572
Epoch: [0]  [ 680/2809]  eta: 0:21:26  lr: 0.000002  min_lr: 0.000000  loss: 5.1207 (5.1512)  loss_scale: 4096.0000 (992.4229)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0570  max mem: 15572
Epoch: [0]  [ 690/2809]  eta: 0:21:20  lr: 0.000002  min_lr: 0.000000  loss: 5.1207 (5.1509)  loss_scale: 4096.0000 (1037.3372)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0977  max mem: 15572
Epoch: [0]  [ 700/2809]  eta: 0:21:15  lr: 0.000002  min_lr: 0.000000  loss: 5.1347 (5.1506)  loss_scale: 4096.0000 (1080.9700)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1521  max mem: 15572
Epoch: [0]  [ 710/2809]  eta: 0:21:09  lr: 0.000002  min_lr: 0.000000  loss: 5.1334 (5.1503)  loss_scale: 4096.0000 (1123.3755)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1495  max mem: 15572
Epoch: [0]  [ 720/2809]  eta: 0:21:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1228 (5.1499)  loss_scale: 4096.0000 (1164.6047)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1094  max mem: 15572
Epoch: [0]  [ 730/2809]  eta: 0:21:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1228 (5.1495)  loss_scale: 4096.0000 (1204.7059)  weight_decay: 0.0500 (0.0500)  time: 0.7010  data: 0.1808  max mem: 15572
Epoch: [0]  [ 740/2809]  eta: 0:20:52  lr: 0.000002  min_lr: 0.000000  loss: 5.1246 (5.1492)  loss_scale: 4096.0000 (1243.7247)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.1330  max mem: 15572
Epoch: [0]  [ 750/2809]  eta: 0:20:42  lr: 0.000003  min_lr: 0.000000  loss: 5.1155 (5.1487)  loss_scale: 4096.0000 (1281.7044)  weight_decay: 0.0500 (0.0500)  time: 0.4649  data: 0.0011  max mem: 15572
Epoch: [0]  [ 760/2809]  eta: 0:20:32  lr: 0.000003  min_lr: 0.000000  loss: 5.1155 (5.1483)  loss_scale: 4096.0000 (1318.6859)  weight_decay: 0.0500 (0.0500)  time: 0.4742  data: 0.0292  max mem: 15572
[2025-01-12 20:34:38,978] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:34:38,979] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [0]  [ 770/2809]  eta: 0:20:21  lr: 0.000003  min_lr: 0.000000  loss: 5.1169 (5.1479)  loss_scale: 4096.0000 (1370.6459)  weight_decay: 0.0500 (0.0500)  time: 0.4246  data: 0.0287  max mem: 15572
Epoch: [0]  [ 780/2809]  eta: 0:20:12  lr: 0.000003  min_lr: 0.000000  loss: 5.1170 (5.1475)  loss_scale: 8192.0000 (1457.9872)  weight_decay: 0.0500 (0.0500)  time: 0.4316  data: 0.0006  max mem: 15572
Epoch: [0]  [ 790/2809]  eta: 0:20:05  lr: 0.000003  min_lr: 0.000000  loss: 5.1164 (5.1472)  loss_scale: 8192.0000 (1543.1201)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0595  max mem: 15572
Epoch: [0]  [ 800/2809]  eta: 0:20:01  lr: 0.000003  min_lr: 0.000000  loss: 5.1091 (5.1467)  loss_scale: 8192.0000 (1626.1273)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1679  max mem: 15572
Epoch: [0]  [ 810/2809]  eta: 0:19:56  lr: 0.000003  min_lr: 0.000000  loss: 5.1091 (5.1463)  loss_scale: 8192.0000 (1707.0875)  weight_decay: 0.0500 (0.0500)  time: 0.6633  data: 0.1806  max mem: 15572
Epoch: [0]  [ 820/2809]  eta: 0:19:53  lr: 0.000003  min_lr: 0.000000  loss: 5.1146 (5.1460)  loss_scale: 8192.0000 (1786.0755)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.1935  max mem: 15572
Epoch: [0]  [ 830/2809]  eta: 0:19:49  lr: 0.000003  min_lr: 0.000000  loss: 5.1068 (5.1455)  loss_scale: 8192.0000 (1863.1625)  weight_decay: 0.0500 (0.0500)  time: 0.7002  data: 0.2319  max mem: 15572
Epoch: [0]  [ 840/2809]  eta: 0:19:45  lr: 0.000003  min_lr: 0.000000  loss: 5.1024 (5.1450)  loss_scale: 8192.0000 (1938.4162)  weight_decay: 0.0500 (0.0500)  time: 0.6832  data: 0.1943  max mem: 15572
Epoch: [0]  [ 850/2809]  eta: 0:19:40  lr: 0.000003  min_lr: 0.000000  loss: 5.1057 (5.1446)  loss_scale: 8192.0000 (2011.9013)  weight_decay: 0.0500 (0.0500)  time: 0.6595  data: 0.1757  max mem: 15572
Epoch: [0]  [ 860/2809]  eta: 0:19:37  lr: 0.000003  min_lr: 0.000000  loss: 5.1015 (5.1442)  loss_scale: 8192.0000 (2083.6794)  weight_decay: 0.0500 (0.0500)  time: 0.6899  data: 0.1922  max mem: 15572
Epoch: [0]  [ 870/2809]  eta: 0:19:31  lr: 0.000003  min_lr: 0.000000  loss: 5.1044 (5.1439)  loss_scale: 8192.0000 (2153.8094)  weight_decay: 0.0500 (0.0500)  time: 0.6692  data: 0.1868  max mem: 15572
Epoch: [0]  [ 880/2809]  eta: 0:19:26  lr: 0.000003  min_lr: 0.000000  loss: 5.1025 (5.1433)  loss_scale: 8192.0000 (2222.3473)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.1949  max mem: 15572
Epoch: [0]  [ 890/2809]  eta: 0:19:22  lr: 0.000003  min_lr: 0.000000  loss: 5.0929 (5.1429)  loss_scale: 8192.0000 (2289.3468)  weight_decay: 0.0500 (0.0500)  time: 0.6834  data: 0.2097  max mem: 15572
[2025-01-12 20:36:00,639] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:36:00,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [0]  [ 900/2809]  eta: 0:19:18  lr: 0.000003  min_lr: 0.000000  loss: 5.1061 (5.1425)  loss_scale: 8192.0000 (2400.3196)  weight_decay: 0.0500 (0.0500)  time: 0.6882  data: 0.2100  max mem: 15572
Epoch: [0]  [ 910/2809]  eta: 0:19:08  lr: 0.000003  min_lr: 0.000000  loss: 5.1046 (5.1421)  loss_scale: 16384.0000 (2553.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1171  max mem: 15572
Epoch: [0]  [ 920/2809]  eta: 0:18:57  lr: 0.000003  min_lr: 0.000000  loss: 5.1040 (5.1418)  loss_scale: 16384.0000 (2703.9826)  weight_decay: 0.0500 (0.0500)  time: 0.3997  data: 0.0082  max mem: 15572
Epoch: [0]  [ 930/2809]  eta: 0:18:48  lr: 0.000003  min_lr: 0.000000  loss: 5.1048 (5.1414)  loss_scale: 16384.0000 (2850.9216)  weight_decay: 0.0500 (0.0500)  time: 0.4158  data: 0.0005  max mem: 15572
Epoch: [0]  [ 940/2809]  eta: 0:18:41  lr: 0.000003  min_lr: 0.000000  loss: 5.1055 (5.1410)  loss_scale: 16384.0000 (2994.7375)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0568  max mem: 15572
Epoch: [0]  [ 950/2809]  eta: 0:18:34  lr: 0.000003  min_lr: 0.000000  loss: 5.1055 (5.1407)  loss_scale: 16384.0000 (3135.5289)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1224  max mem: 15572
Epoch: [0]  [ 960/2809]  eta: 0:18:30  lr: 0.000003  min_lr: 0.000000  loss: 5.1052 (5.1402)  loss_scale: 16384.0000 (3273.3902)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1715  max mem: 15572
Epoch: [0]  [ 970/2809]  eta: 0:18:24  lr: 0.000003  min_lr: 0.000000  loss: 5.1052 (5.1399)  loss_scale: 16384.0000 (3408.4119)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1812  max mem: 15572
Epoch: [0]  [ 980/2809]  eta: 0:18:18  lr: 0.000003  min_lr: 0.000000  loss: 5.1015 (5.1394)  loss_scale: 16384.0000 (3540.6809)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1648  max mem: 15572
Epoch: [0]  [ 990/2809]  eta: 0:18:12  lr: 0.000003  min_lr: 0.000000  loss: 5.0866 (5.1387)  loss_scale: 16384.0000 (3670.2805)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1824  max mem: 15572
[2025-01-12 20:36:59,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[3.2306541515702744e-08, 3.2306541515702744e-08, 4.615220216528964e-08, 4.615220216528964e-08, 6.59317173789852e-08, 6.59317173789852e-08, 9.418816768426457e-08, 9.418816768426457e-08, 1.3455452526323513e-07, 1.3455452526323513e-07, 1.9222075037605018e-07, 1.9222075037605018e-07, 2.7460107196578597e-07, 2.7460107196578597e-07, 3.922872456654086e-07, 3.922872456654086e-07, 5.604103509505837e-07, 5.604103509505837e-07, 8.005862156436911e-07, 8.005862156436911e-07, 1.1436945937767016e-06, 1.1436945937767016e-06, 1.6338494196810024e-06, 1.6338494196810024e-06, 2.3340705995442894e-06, 2.3340705995442894e-06, 3.3343865707775563e-06, 3.3343865707775563e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 20:36:59,466] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=27.714561948353577, CurrSamplesPerSec=27.97767862354106, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [1000/2809]  eta: 0:18:07  lr: 0.000003  min_lr: 0.000000  loss: 5.0871 (5.1383)  loss_scale: 16384.0000 (3797.2907)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1917  max mem: 15572
Epoch: [0]  [1010/2809]  eta: 0:18:02  lr: 0.000003  min_lr: 0.000000  loss: 5.0940 (5.1380)  loss_scale: 16384.0000 (3921.7883)  weight_decay: 0.0500 (0.0500)  time: 0.6454  data: 0.2193  max mem: 15572
Epoch: [0]  [1020/2809]  eta: 0:17:53  lr: 0.000003  min_lr: 0.000000  loss: 5.0888 (5.1375)  loss_scale: 16384.0000 (4043.8472)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1347  max mem: 15572
[2025-01-12 20:37:12,666] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:37:12,667] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [0]  [1030/2809]  eta: 0:17:48  lr: 0.000003  min_lr: 0.000000  loss: 5.0857 (5.1370)  loss_scale: 16384.0000 (4274.7779)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0901  max mem: 15572
Epoch: [0]  [1040/2809]  eta: 0:17:42  lr: 0.000003  min_lr: 0.000000  loss: 5.0919 (5.1368)  loss_scale: 32768.0000 (4548.4880)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1481  max mem: 15572
Epoch: [0]  [1050/2809]  eta: 0:17:34  lr: 0.000004  min_lr: 0.000000  loss: 5.1028 (5.1364)  loss_scale: 32768.0000 (4816.9895)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0997  max mem: 15572
Epoch: [0]  [1060/2809]  eta: 0:17:30  lr: 0.000004  min_lr: 0.000000  loss: 5.0846 (5.1359)  loss_scale: 32768.0000 (5080.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1413  max mem: 15572
Epoch: [0]  [1070/2809]  eta: 0:17:24  lr: 0.000004  min_lr: 0.000000  loss: 5.0781 (5.1354)  loss_scale: 32768.0000 (5338.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.1842  max mem: 15572
Epoch: [0]  [1080/2809]  eta: 0:17:19  lr: 0.000004  min_lr: 0.000000  loss: 5.0781 (5.1348)  loss_scale: 32768.0000 (5592.6883)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.1699  max mem: 15572
Epoch: [0]  [1090/2809]  eta: 0:17:11  lr: 0.000004  min_lr: 0.000000  loss: 5.0771 (5.1344)  loss_scale: 32768.0000 (5841.7745)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1099  max mem: 15572
Epoch: [0]  [1100/2809]  eta: 0:17:04  lr: 0.000004  min_lr: 0.000000  loss: 5.0976 (5.1341)  loss_scale: 32768.0000 (6086.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0327  max mem: 15572
Epoch: [0]  [1110/2809]  eta: 0:16:58  lr: 0.000004  min_lr: 0.000000  loss: 5.0843 (5.1335)  loss_scale: 32768.0000 (6326.4950)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1071  max mem: 15572
Epoch: [0]  [1120/2809]  eta: 0:16:52  lr: 0.000004  min_lr: 0.000000  loss: 5.0730 (5.1331)  loss_scale: 32768.0000 (6562.3693)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1873  max mem: 15572
Epoch: [0]  [1130/2809]  eta: 0:16:47  lr: 0.000004  min_lr: 0.000000  loss: 5.0699 (5.1326)  loss_scale: 32768.0000 (6794.0725)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.2024  max mem: 15572
Epoch: [0]  [1140/2809]  eta: 0:16:40  lr: 0.000004  min_lr: 0.000000  loss: 5.0726 (5.1321)  loss_scale: 32768.0000 (7021.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1274  max mem: 15572
Epoch: [0]  [1150/2809]  eta: 0:16:34  lr: 0.000004  min_lr: 0.000000  loss: 5.0859 (5.1317)  loss_scale: 32768.0000 (7245.4005)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0838  max mem: 15572
[2025-01-12 20:38:28,751] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:38:28,751] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [0]  [1160/2809]  eta: 0:16:26  lr: 0.000004  min_lr: 0.000000  loss: 5.0859 (5.1313)  loss_scale: 32768.0000 (7719.2489)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0749  max mem: 15572
Epoch: [0]  [1170/2809]  eta: 0:16:20  lr: 0.000004  min_lr: 0.000000  loss: 5.0784 (5.1307)  loss_scale: 65536.0000 (8212.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.1044  max mem: 15572
Epoch: [0]  [1180/2809]  eta: 0:16:15  lr: 0.000004  min_lr: 0.000000  loss: 5.0701 (5.1303)  loss_scale: 65536.0000 (8698.3641)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1657  max mem: 15572
Epoch: [0]  [1190/2809]  eta: 0:16:09  lr: 0.000004  min_lr: 0.000000  loss: 5.0956 (5.1299)  loss_scale: 65536.0000 (9175.5903)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1430  max mem: 15572
Epoch: [0]  [1200/2809]  eta: 0:16:02  lr: 0.000004  min_lr: 0.000000  loss: 5.0828 (5.1295)  loss_scale: 65536.0000 (9644.8693)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0890  max mem: 15572
Epoch: [0]  [1210/2809]  eta: 0:15:55  lr: 0.000004  min_lr: 0.000000  loss: 5.0916 (5.1291)  loss_scale: 65536.0000 (10106.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0762  max mem: 15572
Epoch: [0]  [1220/2809]  eta: 0:15:49  lr: 0.000004  min_lr: 0.000000  loss: 5.0774 (5.1286)  loss_scale: 65536.0000 (10560.3669)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1127  max mem: 15572
Epoch: [0]  [1230/2809]  eta: 0:15:43  lr: 0.000004  min_lr: 0.000000  loss: 5.0682 (5.1283)  loss_scale: 65536.0000 (11006.9602)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1623  max mem: 15572
Epoch: [0]  [1240/2809]  eta: 0:15:36  lr: 0.000004  min_lr: 0.000000  loss: 5.0855 (5.1280)  loss_scale: 65536.0000 (11446.3562)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1469  max mem: 15572
Epoch: [0]  [1250/2809]  eta: 0:15:30  lr: 0.000004  min_lr: 0.000000  loss: 5.0822 (5.1276)  loss_scale: 65536.0000 (11878.7274)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1215  max mem: 15572
Epoch: [0]  [1260/2809]  eta: 0:15:23  lr: 0.000004  min_lr: 0.000000  loss: 5.0856 (5.1273)  loss_scale: 65536.0000 (12304.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1177  max mem: 15572
Epoch: [0]  [1270/2809]  eta: 0:15:18  lr: 0.000004  min_lr: 0.000000  loss: 5.0768 (5.1269)  loss_scale: 65536.0000 (12723.0590)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1170  max mem: 15572
[2025-01-12 20:39:43,078] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:39:43,078] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [0]  [1280/2809]  eta: 0:15:12  lr: 0.000004  min_lr: 0.000000  loss: 5.0672 (5.1266)  loss_scale: 65536.0000 (13186.4980)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.1635  max mem: 15572
Epoch: [0]  [1290/2809]  eta: 0:15:07  lr: 0.000004  min_lr: 0.000000  loss: 5.0922 (5.1264)  loss_scale: 131072.0000 (14099.6313)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1530  max mem: 15572
Epoch: [0]  [1300/2809]  eta: 0:15:00  lr: 0.000004  min_lr: 0.000000  loss: 5.0757 (5.1259)  loss_scale: 131072.0000 (14998.7271)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1137  max mem: 15572
Epoch: [0]  [1310/2809]  eta: 0:14:54  lr: 0.000004  min_lr: 0.000000  loss: 5.0489 (5.1253)  loss_scale: 131072.0000 (15884.1068)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1049  max mem: 15572
Epoch: [0]  [1320/2809]  eta: 0:14:49  lr: 0.000004  min_lr: 0.000000  loss: 5.0517 (5.1248)  loss_scale: 131072.0000 (16756.0818)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.1744  max mem: 15572
Epoch: [0]  [1330/2809]  eta: 0:14:42  lr: 0.000004  min_lr: 0.000000  loss: 5.0613 (5.1244)  loss_scale: 131072.0000 (17614.9542)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1519  max mem: 15572
Epoch: [0]  [1340/2809]  eta: 0:14:36  lr: 0.000004  min_lr: 0.000000  loss: 5.0613 (5.1239)  loss_scale: 131072.0000 (18461.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0975  max mem: 15572
Epoch: [0]  [1350/2809]  eta: 0:14:30  lr: 0.000005  min_lr: 0.000000  loss: 5.0655 (5.1235)  loss_scale: 131072.0000 (19294.5551)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1599  max mem: 15572
Epoch: [0]  [1360/2809]  eta: 0:14:24  lr: 0.000005  min_lr: 0.000000  loss: 5.0695 (5.1231)  loss_scale: 131072.0000 (20115.8442)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1708  max mem: 15572
Epoch: [0]  [1370/2809]  eta: 0:14:18  lr: 0.000005  min_lr: 0.000000  loss: 5.0580 (5.1226)  loss_scale: 131072.0000 (20925.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1208  max mem: 15572
Epoch: [0]  [1380/2809]  eta: 0:14:12  lr: 0.000005  min_lr: 0.000000  loss: 5.0384 (5.1220)  loss_scale: 131072.0000 (21722.7400)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1317  max mem: 15572
Epoch: [0]  [1390/2809]  eta: 0:14:06  lr: 0.000005  min_lr: 0.000000  loss: 5.0521 (5.1216)  loss_scale: 131072.0000 (22508.8598)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1653  max mem: 15572
Epoch: [0]  [1400/2809]  eta: 0:13:59  lr: 0.000005  min_lr: 0.000000  loss: 5.0647 (5.1213)  loss_scale: 131072.0000 (23283.7573)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1289  max mem: 15572
[2025-01-12 20:40:59,689] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:40:59,690] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
Epoch: [0]  [1410/2809]  eta: 0:13:55  lr: 0.000005  min_lr: 0.000000  loss: 5.0692 (5.1209)  loss_scale: 131072.0000 (24326.3501)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1936  max mem: 15572
Epoch: [0]  [1420/2809]  eta: 0:13:49  lr: 0.000005  min_lr: 0.000000  loss: 5.0708 (5.1204)  loss_scale: 262144.0000 (25999.9437)  weight_decay: 0.0500 (0.0500)  time: 0.6865  data: 0.2539  max mem: 15572
[2025-01-12 20:41:12,078] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1430
[2025-01-12 20:41:12,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
[2025-01-12 20:41:12,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144, reducing to 131072.0
Epoch: [0]  [1430/2809]  eta: 0:13:42  lr: 0.000005  min_lr: 0.000000  loss: 5.0685 (5.1199)  loss_scale: 262144.0000 (27558.5521)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1441  max mem: 15572
Epoch: [0]  [1440/2809]  eta: 0:13:36  lr: 0.000005  min_lr: 0.000000  loss: 5.0702 (5.1197)  loss_scale: 131072.0000 (28276.8966)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0818  max mem: 15572
Epoch: [0]  [1450/2809]  eta: 0:13:30  lr: 0.000005  min_lr: 0.000000  loss: 5.0620 (5.1192)  loss_scale: 131072.0000 (28985.3398)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1270  max mem: 15572
Epoch: [0]  [1460/2809]  eta: 0:13:24  lr: 0.000005  min_lr: 0.000000  loss: 5.0597 (5.1188)  loss_scale: 131072.0000 (29684.0849)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1772  max mem: 15572
Epoch: [0]  [1470/2809]  eta: 0:13:18  lr: 0.000005  min_lr: 0.000000  loss: 5.0780 (5.1186)  loss_scale: 131072.0000 (30373.3297)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1692  max mem: 15572
Epoch: [0]  [1480/2809]  eta: 0:13:12  lr: 0.000005  min_lr: 0.000000  loss: 5.0553 (5.1182)  loss_scale: 131072.0000 (31053.2667)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1697  max mem: 15572
Epoch: [0]  [1490/2809]  eta: 0:13:06  lr: 0.000005  min_lr: 0.000000  loss: 5.0651 (5.1178)  loss_scale: 131072.0000 (31724.0832)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1593  max mem: 15572
Epoch: [0]  [1500/2809]  eta: 0:13:01  lr: 0.000005  min_lr: 0.000000  loss: 5.0768 (5.1175)  loss_scale: 131072.0000 (32385.9614)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.1762  max mem: 15572
Epoch: [0]  [1510/2809]  eta: 0:12:54  lr: 0.000005  min_lr: 0.000000  loss: 5.0635 (5.1170)  loss_scale: 131072.0000 (33039.0788)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1507  max mem: 15572
Epoch: [0]  [1520/2809]  eta: 0:12:48  lr: 0.000005  min_lr: 0.000000  loss: 5.0512 (5.1166)  loss_scale: 131072.0000 (33683.6082)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1056  max mem: 15572
Epoch: [0]  [1530/2809]  eta: 0:12:42  lr: 0.000005  min_lr: 0.000000  loss: 5.0512 (5.1163)  loss_scale: 131072.0000 (34319.7178)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1467  max mem: 15572
Epoch: [0]  [1540/2809]  eta: 0:12:36  lr: 0.000005  min_lr: 0.000000  loss: 5.0472 (5.1159)  loss_scale: 131072.0000 (34947.5717)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1336  max mem: 15572
Epoch: [0]  [1550/2809]  eta: 0:12:30  lr: 0.000005  min_lr: 0.000000  loss: 5.0354 (5.1154)  loss_scale: 131072.0000 (35567.3295)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0890  max mem: 15572
[2025-01-12 20:42:28,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:42:28,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1560/2809]  eta: 0:12:24  lr: 0.000005  min_lr: 0.000000  loss: 5.0520 (5.1152)  loss_scale: 131072.0000 (36347.0801)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1016  max mem: 15572
Epoch: [0]  [1570/2809]  eta: 0:12:18  lr: 0.000005  min_lr: 0.000000  loss: 5.0554 (5.1148)  loss_scale: 262144.0000 (37784.3616)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1374  max mem: 15572
Epoch: [0]  [1580/2809]  eta: 0:12:12  lr: 0.000005  min_lr: 0.000000  loss: 5.0238 (5.1142)  loss_scale: 262144.0000 (39203.4611)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1190  max mem: 15572
Epoch: [0]  [1590/2809]  eta: 0:12:06  lr: 0.000005  min_lr: 0.000000  loss: 5.0186 (5.1136)  loss_scale: 262144.0000 (40604.7216)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1656  max mem: 15572
[2025-01-12 20:42:52,412] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1599
[2025-01-12 20:42:52,412] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:42:52,412] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1600/2809]  eta: 0:12:00  lr: 0.000005  min_lr: 0.000000  loss: 5.0195 (5.1131)  loss_scale: 262144.0000 (41824.7395)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1625  max mem: 15572
Epoch: [0]  [1610/2809]  eta: 0:11:54  lr: 0.000005  min_lr: 0.000000  loss: 5.0479 (5.1127)  loss_scale: 131072.0000 (42378.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0884  max mem: 15572
Epoch: [0]  [1620/2809]  eta: 0:11:48  lr: 0.000005  min_lr: 0.000000  loss: 5.0526 (5.1123)  loss_scale: 131072.0000 (42925.8779)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1208  max mem: 15572
Epoch: [0]  [1630/2809]  eta: 0:11:42  lr: 0.000005  min_lr: 0.000000  loss: 5.0335 (5.1119)  loss_scale: 131072.0000 (43466.3200)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1126  max mem: 15572
Epoch: [0]  [1640/2809]  eta: 0:11:36  lr: 0.000005  min_lr: 0.000000  loss: 5.0410 (5.1115)  loss_scale: 131072.0000 (44000.1755)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1159  max mem: 15572
Epoch: [0]  [1650/2809]  eta: 0:11:30  lr: 0.000006  min_lr: 0.000000  loss: 5.0617 (5.1111)  loss_scale: 131072.0000 (44527.5639)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1568  max mem: 15572
Epoch: [0]  [1660/2809]  eta: 0:11:24  lr: 0.000006  min_lr: 0.000000  loss: 5.0134 (5.1104)  loss_scale: 131072.0000 (45048.6020)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1240  max mem: 15572
Epoch: [0]  [1670/2809]  eta: 0:11:18  lr: 0.000006  min_lr: 0.000000  loss: 5.0045 (5.1100)  loss_scale: 131072.0000 (45563.4039)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1084  max mem: 15572
Epoch: [0]  [1680/2809]  eta: 0:11:12  lr: 0.000006  min_lr: 0.000000  loss: 5.0359 (5.1098)  loss_scale: 131072.0000 (46072.0809)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1420  max mem: 15572
Epoch: [0]  [1690/2809]  eta: 0:11:06  lr: 0.000006  min_lr: 0.000000  loss: 5.0668 (5.1095)  loss_scale: 131072.0000 (46574.7416)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1668  max mem: 15572
Epoch: [0]  [1700/2809]  eta: 0:11:00  lr: 0.000006  min_lr: 0.000000  loss: 5.0668 (5.1094)  loss_scale: 131072.0000 (47071.4921)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1539  max mem: 15572
Epoch: [0]  [1710/2809]  eta: 0:10:54  lr: 0.000006  min_lr: 0.000000  loss: 5.0553 (5.1089)  loss_scale: 131072.0000 (47562.4360)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1643  max mem: 15572
Epoch: [0]  [1720/2809]  eta: 0:10:48  lr: 0.000006  min_lr: 0.000000  loss: 5.0238 (5.1084)  loss_scale: 131072.0000 (48047.6746)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1228  max mem: 15572
[2025-01-12 20:44:08,659] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:44:08,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1730/2809]  eta: 0:10:42  lr: 0.000006  min_lr: 0.000000  loss: 5.0297 (5.1080)  loss_scale: 131072.0000 (48754.4679)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1307  max mem: 15572
Epoch: [0]  [1740/2809]  eta: 0:10:36  lr: 0.000006  min_lr: 0.000000  loss: 5.0292 (5.1075)  loss_scale: 262144.0000 (49980.1401)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1672  max mem: 15572
Epoch: [0]  [1750/2809]  eta: 0:10:30  lr: 0.000006  min_lr: 0.000000  loss: 5.0127 (5.1070)  loss_scale: 262144.0000 (51191.8127)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0942  max mem: 15572
Epoch: [0]  [1760/2809]  eta: 0:10:23  lr: 0.000006  min_lr: 0.000000  loss: 5.0273 (5.1067)  loss_scale: 262144.0000 (52389.7240)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0310  max mem: 15572
Epoch: [0]  [1770/2809]  eta: 0:10:17  lr: 0.000006  min_lr: 0.000000  loss: 5.0398 (5.1062)  loss_scale: 262144.0000 (53574.1073)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0488  max mem: 15572
[2025-01-12 20:44:33,265] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1776
[2025-01-12 20:44:33,266] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:44:33,266] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1780/2809]  eta: 0:10:11  lr: 0.000006  min_lr: 0.000000  loss: 5.0520 (5.1059)  loss_scale: 262144.0000 (54377.2173)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1422  max mem: 15572
Epoch: [0]  [1790/2809]  eta: 0:10:06  lr: 0.000006  min_lr: 0.000000  loss: 5.0756 (5.1056)  loss_scale: 131072.0000 (54805.4405)  weight_decay: 0.0500 (0.0500)  time: 0.6736  data: 0.2419  max mem: 15572
Epoch: [0]  [1800/2809]  eta: 0:10:00  lr: 0.000006  min_lr: 0.000000  loss: 5.0756 (5.1054)  loss_scale: 131072.0000 (55228.9084)  weight_decay: 0.0500 (0.0500)  time: 0.6589  data: 0.2152  max mem: 15572
Epoch: [0]  [1810/2809]  eta: 0:09:54  lr: 0.000006  min_lr: 0.000000  loss: 5.0073 (5.1048)  loss_scale: 131072.0000 (55647.6996)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1233  max mem: 15572
Epoch: [0]  [1820/2809]  eta: 0:09:48  lr: 0.000006  min_lr: 0.000000  loss: 5.0470 (5.1046)  loss_scale: 131072.0000 (56061.8913)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1282  max mem: 15572
Epoch: [0]  [1830/2809]  eta: 0:09:42  lr: 0.000006  min_lr: 0.000000  loss: 5.0751 (5.1045)  loss_scale: 131072.0000 (56471.5587)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1243  max mem: 15572
Epoch: [0]  [1840/2809]  eta: 0:09:35  lr: 0.000006  min_lr: 0.000000  loss: 5.0669 (5.1042)  loss_scale: 131072.0000 (56876.7757)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0853  max mem: 15572
Epoch: [0]  [1850/2809]  eta: 0:09:29  lr: 0.000006  min_lr: 0.000000  loss: 5.0295 (5.1039)  loss_scale: 131072.0000 (57277.6143)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0967  max mem: 15572
Epoch: [0]  [1860/2809]  eta: 0:09:24  lr: 0.000006  min_lr: 0.000000  loss: 5.0140 (5.1034)  loss_scale: 131072.0000 (57674.1451)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1806  max mem: 15572
Epoch: [0]  [1870/2809]  eta: 0:09:18  lr: 0.000006  min_lr: 0.000000  loss: 5.0332 (5.1030)  loss_scale: 131072.0000 (58066.4372)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1577  max mem: 15572
Epoch: [0]  [1880/2809]  eta: 0:09:11  lr: 0.000006  min_lr: 0.000000  loss: 5.0503 (5.1027)  loss_scale: 131072.0000 (58454.5582)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0723  max mem: 15572
Epoch: [0]  [1890/2809]  eta: 0:09:05  lr: 0.000006  min_lr: 0.000000  loss: 5.0270 (5.1022)  loss_scale: 131072.0000 (58838.5743)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1006  max mem: 15572
Epoch: [0]  [1900/2809]  eta: 0:09:00  lr: 0.000006  min_lr: 0.000000  loss: 5.0328 (5.1021)  loss_scale: 131072.0000 (59218.5502)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1547  max mem: 15572
[2025-01-12 20:45:50,656] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:45:50,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-12 20:45:54,893] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1909
[2025-01-12 20:45:54,893] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:45:54,894] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1910/2809]  eta: 0:08:54  lr: 0.000006  min_lr: 0.000000  loss: 5.0328 (5.1017)  loss_scale: 131072.0000 (59868.9021)  weight_decay: 0.0500 (0.0500)  time: 0.6821  data: 0.2104  max mem: 15572
Epoch: [0]  [1920/2809]  eta: 0:08:48  lr: 0.000006  min_lr: 0.000000  loss: 5.0367 (5.1014)  loss_scale: 131072.0000 (60239.5586)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1941  max mem: 15572
Epoch: [0]  [1930/2809]  eta: 0:08:42  lr: 0.000006  min_lr: 0.000000  loss: 4.9997 (5.1008)  loss_scale: 131072.0000 (60606.3760)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1288  max mem: 15572
Epoch: [0]  [1940/2809]  eta: 0:08:36  lr: 0.000006  min_lr: 0.000000  loss: 4.9997 (5.1004)  loss_scale: 131072.0000 (60969.4137)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1372  max mem: 15572
Epoch: [0]  [1950/2809]  eta: 0:08:30  lr: 0.000007  min_lr: 0.000000  loss: 5.0086 (5.1000)  loss_scale: 131072.0000 (61328.7299)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0819  max mem: 15572
Epoch: [0]  [1960/2809]  eta: 0:08:24  lr: 0.000007  min_lr: 0.000000  loss: 4.9853 (5.0996)  loss_scale: 131072.0000 (61684.3814)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.0502  max mem: 15572
Epoch: [0]  [1970/2809]  eta: 0:08:18  lr: 0.000007  min_lr: 0.000000  loss: 4.9785 (5.0991)  loss_scale: 131072.0000 (62036.4242)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1788  max mem: 15572
Epoch: [0]  [1980/2809]  eta: 0:08:13  lr: 0.000007  min_lr: 0.000000  loss: 5.0137 (5.0989)  loss_scale: 131072.0000 (62384.9127)  weight_decay: 0.0500 (0.0500)  time: 0.6915  data: 0.2507  max mem: 15572
Epoch: [0]  [1990/2809]  eta: 0:08:07  lr: 0.000007  min_lr: 0.000000  loss: 5.0183 (5.0985)  loss_scale: 131072.0000 (62729.9006)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1573  max mem: 15572
[2025-01-12 20:46:48,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=4, lr=[6.464542191180159e-08, 6.464542191180159e-08, 9.235060273114513e-08, 9.235060273114513e-08, 1.3192943247306448e-07, 1.3192943247306448e-07, 1.8847061781866357e-07, 1.8847061781866357e-07, 2.6924373974094795e-07, 2.6924373974094795e-07, 3.8463391391563995e-07, 3.8463391391563995e-07, 5.494770198794857e-07, 5.494770198794857e-07, 7.849671712564082e-07, 7.849671712564082e-07, 1.1213816732234404e-06, 1.1213816732234404e-06, 1.6019738188906293e-06, 1.6019738188906293e-06, 2.288534026986613e-06, 2.288534026986613e-06, 3.2693343242665907e-06, 3.2693343242665907e-06, 4.670477606095129e-06, 4.670477606095129e-06, 6.6721108658501856e-06, 6.6721108658501856e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 20:46:48,011] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=27.87239847586939, CurrSamplesPerSec=23.329024889813017, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [2000/2809]  eta: 0:08:01  lr: 0.000007  min_lr: 0.000000  loss: 5.0114 (5.0981)  loss_scale: 131072.0000 (63071.4403)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0979  max mem: 15572
Epoch: [0]  [2010/2809]  eta: 0:07:54  lr: 0.000007  min_lr: 0.000000  loss: 4.9854 (5.0975)  loss_scale: 131072.0000 (63409.5833)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1121  max mem: 15572
Epoch: [0]  [2020/2809]  eta: 0:07:48  lr: 0.000007  min_lr: 0.000000  loss: 4.9888 (5.0973)  loss_scale: 131072.0000 (63744.3800)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1064  max mem: 15572
Epoch: [0]  [2030/2809]  eta: 0:07:42  lr: 0.000007  min_lr: 0.000000  loss: 5.0313 (5.0971)  loss_scale: 131072.0000 (64075.8799)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1278  max mem: 15572
[2025-01-12 20:47:10,678] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:47:10,678] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2040/2809]  eta: 0:07:37  lr: 0.000007  min_lr: 0.000000  loss: 4.9939 (5.0964)  loss_scale: 131072.0000 (64596.7898)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1411  max mem: 15572
Epoch: [0]  [2050/2809]  eta: 0:07:31  lr: 0.000007  min_lr: 0.000000  loss: 4.9703 (5.0960)  loss_scale: 262144.0000 (65559.9649)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1564  max mem: 15572
Epoch: [0]  [2060/2809]  eta: 0:07:25  lr: 0.000007  min_lr: 0.000000  loss: 5.0084 (5.0957)  loss_scale: 262144.0000 (66513.7933)  weight_decay: 0.0500 (0.0500)  time: 0.6808  data: 0.2500  max mem: 15572
Epoch: [0]  [2070/2809]  eta: 0:07:19  lr: 0.000007  min_lr: 0.000000  loss: 5.0252 (5.0954)  loss_scale: 262144.0000 (67458.4104)  weight_decay: 0.0500 (0.0500)  time: 0.6669  data: 0.2414  max mem: 15572
Epoch: [0]  [2080/2809]  eta: 0:07:13  lr: 0.000007  min_lr: 0.000000  loss: 5.0044 (5.0949)  loss_scale: 262144.0000 (68393.9491)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1128  max mem: 15572
[2025-01-12 20:47:39,112] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2086
[2025-01-12 20:47:39,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:47:39,113] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2090/2809]  eta: 0:07:07  lr: 0.000007  min_lr: 0.000000  loss: 4.9915 (5.0944)  loss_scale: 262144.0000 (69007.1200)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1028  max mem: 15572
Epoch: [0]  [2100/2809]  eta: 0:07:01  lr: 0.000007  min_lr: 0.000000  loss: 5.0105 (5.0942)  loss_scale: 131072.0000 (69302.5264)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1147  max mem: 15572
Epoch: [0]  [2110/2809]  eta: 0:06:55  lr: 0.000007  min_lr: 0.000000  loss: 5.0079 (5.0937)  loss_scale: 131072.0000 (69595.1341)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1583  max mem: 15572
Epoch: [0]  [2120/2809]  eta: 0:06:49  lr: 0.000007  min_lr: 0.000000  loss: 4.9933 (5.0933)  loss_scale: 131072.0000 (69884.9826)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1399  max mem: 15572
Epoch: [0]  [2130/2809]  eta: 0:06:43  lr: 0.000007  min_lr: 0.000000  loss: 4.9997 (5.0930)  loss_scale: 131072.0000 (70172.1107)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1104  max mem: 15572
Epoch: [0]  [2140/2809]  eta: 0:06:37  lr: 0.000007  min_lr: 0.000000  loss: 5.0188 (5.0929)  loss_scale: 131072.0000 (70456.5567)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1376  max mem: 15572
Epoch: [0]  [2150/2809]  eta: 0:06:31  lr: 0.000007  min_lr: 0.000000  loss: 5.0121 (5.0926)  loss_scale: 131072.0000 (70738.3580)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1146  max mem: 15572
Epoch: [0]  [2160/2809]  eta: 0:06:25  lr: 0.000007  min_lr: 0.000000  loss: 5.0001 (5.0922)  loss_scale: 131072.0000 (71017.5511)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0826  max mem: 15572
Epoch: [0]  [2170/2809]  eta: 0:06:19  lr: 0.000007  min_lr: 0.000000  loss: 5.0220 (5.0919)  loss_scale: 131072.0000 (71294.1723)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0594  max mem: 15572
Epoch: [0]  [2180/2809]  eta: 0:06:13  lr: 0.000007  min_lr: 0.000000  loss: 5.0319 (5.0918)  loss_scale: 131072.0000 (71568.2568)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0858  max mem: 15572
Epoch: [0]  [2190/2809]  eta: 0:06:07  lr: 0.000007  min_lr: 0.000000  loss: 5.0153 (5.0913)  loss_scale: 131072.0000 (71839.8393)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0977  max mem: 15572
Epoch: [0]  [2200/2809]  eta: 0:06:01  lr: 0.000007  min_lr: 0.000000  loss: 5.0022 (5.0910)  loss_scale: 131072.0000 (72108.9541)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0639  max mem: 15572
Epoch: [0]  [2210/2809]  eta: 0:05:55  lr: 0.000007  min_lr: 0.000000  loss: 5.0012 (5.0904)  loss_scale: 131072.0000 (72375.6346)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1260  max mem: 15572
[2025-01-12 20:48:52,997] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:48:52,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2220/2809]  eta: 0:05:49  lr: 0.000007  min_lr: 0.000000  loss: 4.9886 (5.0900)  loss_scale: 131072.0000 (72994.0027)  weight_decay: 0.0500 (0.0500)  time: 0.6476  data: 0.2006  max mem: 15572
Epoch: [0]  [2230/2809]  eta: 0:05:43  lr: 0.000007  min_lr: 0.000000  loss: 4.9899 (5.0894)  loss_scale: 262144.0000 (73841.8288)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0972  max mem: 15572
Epoch: [0]  [2240/2809]  eta: 0:05:37  lr: 0.000007  min_lr: 0.000000  loss: 5.0007 (5.0892)  loss_scale: 262144.0000 (74682.0884)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1404  max mem: 15572
Epoch: [0]  [2250/2809]  eta: 0:05:32  lr: 0.000008  min_lr: 0.000000  loss: 5.0239 (5.0888)  loss_scale: 262144.0000 (75514.8823)  weight_decay: 0.0500 (0.0500)  time: 0.6713  data: 0.2018  max mem: 15572
Epoch: [0]  [2260/2809]  eta: 0:05:26  lr: 0.000008  min_lr: 0.000000  loss: 5.0211 (5.0886)  loss_scale: 262144.0000 (76340.3096)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1253  max mem: 15572
[2025-01-12 20:49:22,684] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2263
[2025-01-12 20:49:22,684] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:49:22,684] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2270/2809]  eta: 0:05:20  lr: 0.000008  min_lr: 0.000000  loss: 4.9858 (5.0880)  loss_scale: 262144.0000 (76696.7433)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1251  max mem: 15572
Epoch: [0]  [2280/2809]  eta: 0:05:14  lr: 0.000008  min_lr: 0.000000  loss: 4.9672 (5.0876)  loss_scale: 131072.0000 (76935.1267)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1398  max mem: 15572
Epoch: [0]  [2290/2809]  eta: 0:05:08  lr: 0.000008  min_lr: 0.000000  loss: 5.0193 (5.0872)  loss_scale: 131072.0000 (77171.4291)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1562  max mem: 15572
Epoch: [0]  [2300/2809]  eta: 0:05:02  lr: 0.000008  min_lr: 0.000000  loss: 4.9886 (5.0867)  loss_scale: 131072.0000 (77405.6775)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1309  max mem: 15572
Epoch: [0]  [2310/2809]  eta: 0:04:56  lr: 0.000008  min_lr: 0.000000  loss: 4.9738 (5.0864)  loss_scale: 131072.0000 (77637.8987)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1047  max mem: 15572
Epoch: [0]  [2320/2809]  eta: 0:04:50  lr: 0.000008  min_lr: 0.000000  loss: 4.9738 (5.0861)  loss_scale: 131072.0000 (77868.1189)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1246  max mem: 15572
Epoch: [0]  [2330/2809]  eta: 0:04:44  lr: 0.000008  min_lr: 0.000000  loss: 4.9869 (5.0857)  loss_scale: 131072.0000 (78096.3638)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1630  max mem: 15572
Epoch: [0]  [2340/2809]  eta: 0:04:38  lr: 0.000008  min_lr: 0.000000  loss: 5.0125 (5.0856)  loss_scale: 131072.0000 (78322.6587)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.2041  max mem: 15572
Epoch: [0]  [2350/2809]  eta: 0:04:32  lr: 0.000008  min_lr: 0.000000  loss: 5.0449 (5.0854)  loss_scale: 131072.0000 (78547.0285)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1501  max mem: 15572
Epoch: [0]  [2360/2809]  eta: 0:04:26  lr: 0.000008  min_lr: 0.000000  loss: 5.0186 (5.0851)  loss_scale: 131072.0000 (78769.4977)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0853  max mem: 15572
Epoch: [0]  [2370/2809]  eta: 0:04:20  lr: 0.000008  min_lr: 0.000000  loss: 5.0500 (5.0852)  loss_scale: 131072.0000 (78990.0903)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1412  max mem: 15572
Epoch: [0]  [2380/2809]  eta: 0:04:14  lr: 0.000008  min_lr: 0.000000  loss: 5.0313 (5.0848)  loss_scale: 131072.0000 (79208.8299)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1601  max mem: 15572
Epoch: [0]  [2390/2809]  eta: 0:04:08  lr: 0.000008  min_lr: 0.000000  loss: 5.0198 (5.0846)  loss_scale: 131072.0000 (79425.7399)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1116  max mem: 15572
[2025-01-12 20:50:39,079] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:50:39,079] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-12 20:50:43,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2399
[2025-01-12 20:50:43,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:50:43,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2400/2809]  eta: 0:04:02  lr: 0.000008  min_lr: 0.000000  loss: 5.0043 (5.0841)  loss_scale: 131072.0000 (80022.9771)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1259  max mem: 15572
Epoch: [0]  [2410/2809]  eta: 0:03:56  lr: 0.000008  min_lr: 0.000000  loss: 4.9684 (5.0835)  loss_scale: 131072.0000 (80234.7109)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1700  max mem: 15572
Epoch: [0]  [2420/2809]  eta: 0:03:50  lr: 0.000008  min_lr: 0.000000  loss: 4.9712 (5.0834)  loss_scale: 131072.0000 (80444.6956)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1257  max mem: 15572
Epoch: [0]  [2430/2809]  eta: 0:03:44  lr: 0.000008  min_lr: 0.000000  loss: 5.0326 (5.0833)  loss_scale: 131072.0000 (80652.9527)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0893  max mem: 15572
Epoch: [0]  [2440/2809]  eta: 0:03:38  lr: 0.000008  min_lr: 0.000000  loss: 5.0418 (5.0831)  loss_scale: 131072.0000 (80859.5035)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1331  max mem: 15572
Epoch: [0]  [2450/2809]  eta: 0:03:32  lr: 0.000008  min_lr: 0.000000  loss: 5.0418 (5.0828)  loss_scale: 131072.0000 (81064.3688)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1229  max mem: 15572
Epoch: [0]  [2460/2809]  eta: 0:03:26  lr: 0.000008  min_lr: 0.000000  loss: 5.0174 (5.0826)  loss_scale: 131072.0000 (81267.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1359  max mem: 15572
Epoch: [0]  [2470/2809]  eta: 0:03:21  lr: 0.000008  min_lr: 0.000000  loss: 5.0133 (5.0823)  loss_scale: 131072.0000 (81469.1251)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1573  max mem: 15572
Epoch: [0]  [2480/2809]  eta: 0:03:15  lr: 0.000008  min_lr: 0.000000  loss: 4.9949 (5.0819)  loss_scale: 131072.0000 (81669.0560)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1018  max mem: 15572
Epoch: [0]  [2490/2809]  eta: 0:03:09  lr: 0.000008  min_lr: 0.000000  loss: 4.9702 (5.0815)  loss_scale: 131072.0000 (81867.3818)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0725  max mem: 15572
Epoch: [0]  [2500/2809]  eta: 0:03:03  lr: 0.000008  min_lr: 0.000000  loss: 5.0015 (5.0812)  loss_scale: 131072.0000 (82064.1216)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1654  max mem: 15572
Epoch: [0]  [2510/2809]  eta: 0:02:57  lr: 0.000008  min_lr: 0.000000  loss: 4.9884 (5.0809)  loss_scale: 131072.0000 (82259.2943)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.2026  max mem: 15572
Epoch: [0]  [2520/2809]  eta: 0:02:51  lr: 0.000008  min_lr: 0.000000  loss: 4.9615 (5.0804)  loss_scale: 131072.0000 (82452.9187)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1597  max mem: 15572
[2025-01-12 20:51:58,231] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:51:58,231] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2530/2809]  eta: 0:02:45  lr: 0.000008  min_lr: 0.000000  loss: 4.9685 (5.0801)  loss_scale: 131072.0000 (82800.3730)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0838  max mem: 15572
Epoch: [0]  [2540/2809]  eta: 0:02:39  lr: 0.000008  min_lr: 0.000000  loss: 5.0214 (5.0798)  loss_scale: 262144.0000 (83506.1724)  weight_decay: 0.0500 (0.0500)  time: 0.4768  data: 0.0350  max mem: 15572
Epoch: [0]  [2550/2809]  eta: 0:02:33  lr: 0.000009  min_lr: 0.000000  loss: 5.0318 (5.0798)  loss_scale: 262144.0000 (84206.4383)  weight_decay: 0.0500 (0.0500)  time: 0.4903  data: 0.0648  max mem: 15572
Epoch: [0]  [2560/2809]  eta: 0:02:27  lr: 0.000009  min_lr: 0.000000  loss: 4.9903 (5.0794)  loss_scale: 262144.0000 (84901.2355)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1137  max mem: 15572
[2025-01-12 20:52:17,282] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2565
[2025-01-12 20:52:17,283] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:52:17,283] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2570/2809]  eta: 0:02:21  lr: 0.000009  min_lr: 0.000000  loss: 4.9903 (5.0792)  loss_scale: 262144.0000 (85284.7421)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1319  max mem: 15572
Epoch: [0]  [2580/2809]  eta: 0:02:15  lr: 0.000009  min_lr: 0.000000  loss: 4.9850 (5.0788)  loss_scale: 131072.0000 (85462.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1465  max mem: 15572
Epoch: [0]  [2590/2809]  eta: 0:02:09  lr: 0.000009  min_lr: 0.000000  loss: 4.9850 (5.0786)  loss_scale: 131072.0000 (85638.1752)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.1874  max mem: 15572
Epoch: [0]  [2600/2809]  eta: 0:02:03  lr: 0.000009  min_lr: 0.000000  loss: 4.9907 (5.0780)  loss_scale: 131072.0000 (85812.8535)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1332  max mem: 15572
Epoch: [0]  [2610/2809]  eta: 0:01:57  lr: 0.000009  min_lr: 0.000000  loss: 4.9891 (5.0776)  loss_scale: 131072.0000 (85986.1938)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1512  max mem: 15572
Epoch: [0]  [2620/2809]  eta: 0:01:51  lr: 0.000009  min_lr: 0.000000  loss: 5.0390 (5.0776)  loss_scale: 131072.0000 (86158.2114)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1473  max mem: 15572
Epoch: [0]  [2630/2809]  eta: 0:01:45  lr: 0.000009  min_lr: 0.000000  loss: 5.0639 (5.0774)  loss_scale: 131072.0000 (86328.9213)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1120  max mem: 15572
Epoch: [0]  [2640/2809]  eta: 0:01:40  lr: 0.000009  min_lr: 0.000000  loss: 4.9870 (5.0772)  loss_scale: 131072.0000 (86498.3385)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1342  max mem: 15572
Epoch: [0]  [2650/2809]  eta: 0:01:34  lr: 0.000009  min_lr: 0.000000  loss: 5.0347 (5.0771)  loss_scale: 131072.0000 (86666.4776)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1733  max mem: 15572
Epoch: [0]  [2660/2809]  eta: 0:01:28  lr: 0.000009  min_lr: 0.000000  loss: 5.0035 (5.0768)  loss_scale: 131072.0000 (86833.3529)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1869  max mem: 15572
Epoch: [0]  [2670/2809]  eta: 0:01:22  lr: 0.000009  min_lr: 0.000000  loss: 4.9875 (5.0765)  loss_scale: 131072.0000 (86998.9787)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1524  max mem: 15572
Epoch: [0]  [2680/2809]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000000  loss: 4.9701 (5.0761)  loss_scale: 131072.0000 (87163.3689)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1147  max mem: 15572
Epoch: [0]  [2690/2809]  eta: 0:01:10  lr: 0.000009  min_lr: 0.000000  loss: 4.9704 (5.0759)  loss_scale: 131072.0000 (87326.5373)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1658  max mem: 15572
[2025-01-12 20:53:35,199] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:53:35,200] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2700/2809]  eta: 0:01:04  lr: 0.000009  min_lr: 0.000000  loss: 5.0239 (5.0758)  loss_scale: 131072.0000 (87828.1881)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.1881  max mem: 15572
Epoch: [0]  [2710/2809]  eta: 0:00:58  lr: 0.000009  min_lr: 0.000000  loss: 5.0170 (5.0756)  loss_scale: 262144.0000 (88471.1826)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0972  max mem: 15572
Epoch: [0]  [2720/2809]  eta: 0:00:52  lr: 0.000009  min_lr: 0.000000  loss: 5.0047 (5.0755)  loss_scale: 262144.0000 (89109.4509)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1152  max mem: 15572
Epoch: [0]  [2730/2809]  eta: 0:00:46  lr: 0.000009  min_lr: 0.000000  loss: 4.9830 (5.0752)  loss_scale: 262144.0000 (89743.0450)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1354  max mem: 15572
Epoch: [0]  [2740/2809]  eta: 0:00:40  lr: 0.000009  min_lr: 0.000000  loss: 4.9566 (5.0748)  loss_scale: 262144.0000 (90372.0161)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1326  max mem: 15572
[2025-01-12 20:54:06,652] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2750
[2025-01-12 20:54:06,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:54:06,653] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2750/2809]  eta: 0:00:34  lr: 0.000009  min_lr: 0.000000  loss: 4.9600 (5.0745)  loss_scale: 262144.0000 (90948.7692)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0859  max mem: 15572
Epoch: [0]  [2760/2809]  eta: 0:00:29  lr: 0.000009  min_lr: 0.000000  loss: 5.0024 (5.0742)  loss_scale: 131072.0000 (91094.0905)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1087  max mem: 15572
Epoch: [0]  [2770/2809]  eta: 0:00:23  lr: 0.000009  min_lr: 0.000000  loss: 4.9535 (5.0738)  loss_scale: 131072.0000 (91238.3630)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.1647  max mem: 15572
Epoch: [0]  [2780/2809]  eta: 0:00:17  lr: 0.000009  min_lr: 0.000000  loss: 5.0012 (5.0736)  loss_scale: 131072.0000 (91381.5980)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1100  max mem: 15572
Epoch: [0]  [2790/2809]  eta: 0:00:11  lr: 0.000009  min_lr: 0.000000  loss: 4.9633 (5.0731)  loss_scale: 131072.0000 (91523.8065)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1508  max mem: 15572
Epoch: [0]  [2800/2809]  eta: 0:00:05  lr: 0.000009  min_lr: 0.000000  loss: 4.9705 (5.0731)  loss_scale: 131072.0000 (91664.9996)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.2025  max mem: 15572
Epoch: [0]  [2808/2809]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 5.0420 (5.0730)  loss_scale: 131072.0000 (91777.2303)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0957  max mem: 15572
Epoch: [0] Total time: 0:27:43 (0.5922 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 5.0420 (5.0730)  loss_scale: 131072.0000 (91777.2303)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:26:37  loss: 4.9922 (4.9922)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 5.8724  data: 5.4049  max mem: 15572
Val:  [ 10/272]  eta: 0:03:33  loss: 5.1506 (5.0917)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.8131  data: 0.5724  max mem: 15572
Val:  [ 20/272]  eta: 0:02:26  loss: 5.1836 (5.0610)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3147  data: 0.0988  max mem: 15572
Val:  [ 30/272]  eta: 0:01:51  loss: 5.1020 (5.0353)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2689  data: 0.0648  max mem: 15572
Val:  [ 40/272]  eta: 0:01:40  loss: 4.8739 (4.9999)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2762  data: 0.0739  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 4.8283 (5.0215)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3410  data: 0.1366  max mem: 15572
Val:  [ 60/272]  eta: 0:01:25  loss: 4.8307 (5.0150)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3469  data: 0.1458  max mem: 15572
Val:  [ 70/272]  eta: 0:01:19  loss: 4.8320 (4.9990)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (4.8513)  time: 0.3374  data: 0.1459  max mem: 15572
Val:  [ 80/272]  eta: 0:01:14  loss: 4.9648 (4.9579)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (9.1221)  time: 0.3371  data: 0.1409  max mem: 15572
Val:  [ 90/272]  eta: 0:01:09  loss: 5.0204 (4.9828)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (8.1197)  time: 0.3426  data: 0.1380  max mem: 15572
Val:  [100/272]  eta: 0:01:04  loss: 5.1758 (5.0141)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (7.3157)  time: 0.3381  data: 0.1325  max mem: 15572
Val:  [110/272]  eta: 0:01:00  loss: 5.2500 (5.0352)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (6.6567)  time: 0.3421  data: 0.1330  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 5.1777 (5.0578)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (6.1065)  time: 0.3192  data: 0.1179  max mem: 15572
Val:  [130/272]  eta: 0:00:52  loss: 5.1120 (5.0564)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (7.5912)  time: 0.3322  data: 0.1307  max mem: 15572
Val:  [140/272]  eta: 0:00:48  loss: 4.8459 (5.0422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (9.0623)  time: 0.3384  data: 0.1431  max mem: 15572
Val:  [150/272]  eta: 0:00:44  loss: 4.8459 (5.0327)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (8.4621)  time: 0.3119  data: 0.1086  max mem: 15572
Val:  [160/272]  eta: 0:00:40  loss: 4.6328 (5.0027)  acc1: 0.0000 (1.8634)  acc5: 0.0000 (9.7999)  time: 0.3280  data: 0.1236  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 4.6250 (5.0105)  acc1: 0.0000 (1.9493)  acc5: 0.0000 (9.4217)  time: 0.3338  data: 0.1337  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 5.1155 (5.0132)  acc1: 0.0000 (1.8416)  acc5: 0.0000 (8.9012)  time: 0.3320  data: 0.1298  max mem: 15572
Val:  [190/272]  eta: 0:00:29  loss: 5.1745 (5.0205)  acc1: 0.0000 (1.7452)  acc5: 0.0000 (8.4351)  time: 0.3501  data: 0.1520  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 5.0760 (5.0293)  acc1: 0.0000 (1.6584)  acc5: 0.0000 (8.0155)  time: 0.3403  data: 0.1345  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 5.0757 (5.0345)  acc1: 0.0000 (1.5798)  acc5: 0.0000 (7.6356)  time: 0.3235  data: 0.1278  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 4.8477 (5.0207)  acc1: 0.0000 (1.5083)  acc5: 0.0000 (7.2901)  time: 0.3259  data: 0.1406  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.7385 (5.0101)  acc1: 0.0000 (1.4430)  acc5: 0.0000 (6.9745)  time: 0.3088  data: 0.1132  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 4.7630 (5.0087)  acc1: 0.0000 (1.3831)  acc5: 0.0000 (6.6851)  time: 0.3007  data: 0.1019  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 5.2500 (5.0253)  acc1: 0.0000 (1.3280)  acc5: 0.0000 (6.4188)  time: 0.3341  data: 0.1394  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 5.1847 (5.0217)  acc1: 0.0000 (1.2771)  acc5: 0.0000 (6.1728)  time: 0.2919  data: 0.1096  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 5.0894 (5.0235)  acc1: 0.0000 (1.2300)  acc5: 0.0000 (5.9451)  time: 0.1989  data: 0.0386  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 5.0948 (5.0255)  acc1: 0.0000 (1.2288)  acc5: 0.0000 (5.9390)  time: 0.1939  data: 0.0386  max mem: 15572
Val: Total time: 0:01:31 (0.3381 s / it)
* Acc@1 1.229 Acc@5 5.939 loss 5.025
Accuracy of the network on the 4883 val videos: 1.2%
[2025-01-12 20:56:13,078] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2025-01-12 20:56:13,083] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 20:56:13,083] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 20:56:13,603] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_baseline_1gpu/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 20:56:13,604] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 1.23%
Epoch: [1]  [   0/2809]  eta: 7:10:05  lr: 0.000009  min_lr: 0.000000  loss: 4.8862 (4.8862)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 9.1867  data: 8.7039  max mem: 15572
Epoch: [1]  [  10/2809]  eta: 1:03:31  lr: 0.000009  min_lr: 0.000000  loss: 4.9537 (4.9949)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3618  data: 0.9331  max mem: 15572
Epoch: [1]  [  20/2809]  eta: 0:46:42  lr: 0.000009  min_lr: 0.000000  loss: 5.0245 (5.0164)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1552  max mem: 15572
Epoch: [1]  [  30/2809]  eta: 0:41:00  lr: 0.000009  min_lr: 0.000000  loss: 5.0256 (5.0102)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1677  max mem: 15572
Epoch: [1]  [  40/2809]  eta: 0:37:12  lr: 0.000010  min_lr: 0.000000  loss: 4.9364 (4.9983)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1551  max mem: 15572
Epoch: [1]  [  50/2809]  eta: 0:34:58  lr: 0.000010  min_lr: 0.000000  loss: 4.9784 (5.0038)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1357  max mem: 15572
Epoch: [1]  [  60/2809]  eta: 0:32:54  lr: 0.000010  min_lr: 0.000000  loss: 4.9952 (5.0033)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.1195  max mem: 15572
[2025-01-12 20:57:02,971] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:57:02,971] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [  70/2809]  eta: 0:31:43  lr: 0.000010  min_lr: 0.000000  loss: 4.9961 (5.0075)  loss_scale: 131072.0000 (132918.0845)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.1050  max mem: 15572
Epoch: [1]  [  80/2809]  eta: 0:30:57  lr: 0.000010  min_lr: 0.000000  loss: 5.0365 (5.0107)  loss_scale: 262144.0000 (148871.9012)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1312  max mem: 15572
Epoch: [1]  [  90/2809]  eta: 0:30:26  lr: 0.000010  min_lr: 0.000000  loss: 4.9996 (5.0053)  loss_scale: 262144.0000 (161319.3846)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1469  max mem: 15572
Epoch: [1]  [ 100/2809]  eta: 0:30:10  lr: 0.000010  min_lr: 0.000000  loss: 4.9581 (5.0023)  loss_scale: 262144.0000 (171302.0198)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1603  max mem: 15572
[2025-01-12 20:57:25,401] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2916
[2025-01-12 20:57:25,401] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:57:25,401] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 110/2809]  eta: 0:29:41  lr: 0.000010  min_lr: 0.000000  loss: 4.9358 (4.9972)  loss_scale: 262144.0000 (174762.6667)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1472  max mem: 15572
Epoch: [1]  [ 120/2809]  eta: 0:29:37  lr: 0.000010  min_lr: 0.000000  loss: 4.9595 (4.9947)  loss_scale: 131072.0000 (171151.8678)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1715  max mem: 15572
Epoch: [1]  [ 130/2809]  eta: 0:29:09  lr: 0.000010  min_lr: 0.000000  loss: 4.9894 (4.9951)  loss_scale: 131072.0000 (168092.3359)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1806  max mem: 15572
Epoch: [1]  [ 140/2809]  eta: 0:28:52  lr: 0.000010  min_lr: 0.000000  loss: 5.0020 (5.0010)  loss_scale: 131072.0000 (165466.7801)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1325  max mem: 15572
Epoch: [1]  [ 150/2809]  eta: 0:28:38  lr: 0.000010  min_lr: 0.000000  loss: 5.0018 (4.9999)  loss_scale: 131072.0000 (163188.9801)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1436  max mem: 15572
Epoch: [1]  [ 160/2809]  eta: 0:28:25  lr: 0.000010  min_lr: 0.000000  loss: 4.9573 (4.9968)  loss_scale: 131072.0000 (161194.1366)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1619  max mem: 15572
Epoch: [1]  [ 170/2809]  eta: 0:28:14  lr: 0.000010  min_lr: 0.000000  loss: 4.9466 (4.9949)  loss_scale: 131072.0000 (159432.6082)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1671  max mem: 15572
Epoch: [1]  [ 180/2809]  eta: 0:28:08  lr: 0.000010  min_lr: 0.000000  loss: 5.0301 (4.9993)  loss_scale: 131072.0000 (157865.7238)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1660  max mem: 15572
[2025-01-12 20:58:15,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=10, lr=[9.698430230790043e-08, 9.698430230790043e-08, 1.3854900329700063e-07, 1.3854900329700063e-07, 1.9792714756714377e-07, 1.9792714756714377e-07, 2.8275306795306254e-07, 2.8275306795306254e-07, 4.0393295421866083e-07, 4.0393295421866083e-07, 5.770470774552297e-07, 5.770470774552297e-07, 8.243529677931854e-07, 8.243529677931854e-07, 1.1776470968474079e-06, 1.1776470968474079e-06, 1.682352995496297e-06, 1.682352995496297e-06, 2.4033614221375676e-06, 2.4033614221375676e-06, 3.4333734601965247e-06, 3.4333734601965247e-06, 4.9048192288521786e-06, 4.9048192288521786e-06, 7.0068846126459705e-06, 7.0068846126459705e-06, 1.0009835160922815e-05, 1.0009835160922815e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 20:58:15,964] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=27.940445903034206, CurrSamplesPerSec=31.495133213230652, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [ 190/2809]  eta: 0:27:57  lr: 0.000010  min_lr: 0.000000  loss: 5.0354 (4.9997)  loss_scale: 131072.0000 (156462.9110)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1314  max mem: 15572
Epoch: [1]  [ 200/2809]  eta: 0:27:41  lr: 0.000010  min_lr: 0.000000  loss: 4.9873 (4.9998)  loss_scale: 131072.0000 (155199.6816)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0953  max mem: 15572
Epoch: [1]  [ 210/2809]  eta: 0:27:31  lr: 0.000010  min_lr: 0.000000  loss: 4.9873 (4.9965)  loss_scale: 131072.0000 (154056.1896)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1227  max mem: 15572
Epoch: [1]  [ 220/2809]  eta: 0:27:17  lr: 0.000010  min_lr: 0.000000  loss: 4.9731 (4.9941)  loss_scale: 131072.0000 (153016.1810)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1375  max mem: 15572
Epoch: [1]  [ 230/2809]  eta: 0:27:14  lr: 0.000010  min_lr: 0.000000  loss: 4.9888 (4.9956)  loss_scale: 131072.0000 (152066.2165)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1557  max mem: 15572
[2025-01-12 20:58:43,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 20:58:43,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 240/2809]  eta: 0:27:02  lr: 0.000010  min_lr: 0.000000  loss: 4.9923 (4.9966)  loss_scale: 131072.0000 (153914.4232)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1405  max mem: 15572
Epoch: [1]  [ 250/2809]  eta: 0:26:43  lr: 0.000010  min_lr: 0.000000  loss: 5.0413 (4.9993)  loss_scale: 262144.0000 (158226.3586)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0693  max mem: 15572
Epoch: [1]  [ 260/2809]  eta: 0:26:43  lr: 0.000010  min_lr: 0.000000  loss: 4.9883 (4.9990)  loss_scale: 262144.0000 (162207.8774)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1363  max mem: 15572
Epoch: [1]  [ 270/2809]  eta: 0:26:24  lr: 0.000010  min_lr: 0.000000  loss: 4.9802 (5.0000)  loss_scale: 262144.0000 (165895.5572)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1291  max mem: 15572
[2025-01-12 20:59:06,409] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3084
[2025-01-12 20:59:06,410] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 20:59:06,411] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 280/2809]  eta: 0:26:17  lr: 0.000010  min_lr: 0.000000  loss: 5.0546 (5.0022)  loss_scale: 262144.0000 (166522.0783)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0878  max mem: 15572
Epoch: [1]  [ 290/2809]  eta: 0:26:07  lr: 0.000010  min_lr: 0.000000  loss: 5.0647 (5.0041)  loss_scale: 131072.0000 (165303.8625)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1462  max mem: 15572
Epoch: [1]  [ 300/2809]  eta: 0:25:58  lr: 0.000010  min_lr: 0.000000  loss: 5.0291 (5.0049)  loss_scale: 131072.0000 (164166.5914)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1466  max mem: 15572
Epoch: [1]  [ 310/2809]  eta: 0:25:45  lr: 0.000010  min_lr: 0.000000  loss: 4.9723 (5.0040)  loss_scale: 131072.0000 (163102.4566)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1052  max mem: 15572
Epoch: [1]  [ 320/2809]  eta: 0:25:33  lr: 0.000010  min_lr: 0.000000  loss: 4.9245 (5.0017)  loss_scale: 131072.0000 (162104.6231)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0778  max mem: 15572
Epoch: [1]  [ 330/2809]  eta: 0:25:29  lr: 0.000010  min_lr: 0.000000  loss: 4.8942 (5.0002)  loss_scale: 131072.0000 (161167.0816)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1151  max mem: 15572
Epoch: [1]  [ 340/2809]  eta: 0:25:24  lr: 0.000011  min_lr: 0.000000  loss: 4.9083 (4.9975)  loss_scale: 131072.0000 (160284.5279)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1329  max mem: 15572
Epoch: [1]  [ 350/2809]  eta: 0:25:18  lr: 0.000011  min_lr: 0.000000  loss: 4.9450 (4.9963)  loss_scale: 131072.0000 (159452.2621)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.1203  max mem: 15572
Epoch: [1]  [ 360/2809]  eta: 0:25:04  lr: 0.000011  min_lr: 0.000000  loss: 4.9546 (4.9968)  loss_scale: 131072.0000 (158666.1053)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0877  max mem: 15572
Epoch: [1]  [ 370/2809]  eta: 0:24:52  lr: 0.000011  min_lr: 0.000000  loss: 4.9727 (4.9959)  loss_scale: 131072.0000 (157922.3288)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0667  max mem: 15572
Epoch: [1]  [ 380/2809]  eta: 0:24:46  lr: 0.000011  min_lr: 0.000000  loss: 4.9719 (4.9965)  loss_scale: 131072.0000 (157217.5958)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1410  max mem: 15572
Epoch: [1]  [ 390/2809]  eta: 0:24:40  lr: 0.000011  min_lr: 0.000000  loss: 5.0120 (4.9967)  loss_scale: 131072.0000 (156548.9105)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1938  max mem: 15572
Epoch: [1]  [ 400/2809]  eta: 0:24:32  lr: 0.000011  min_lr: 0.000000  loss: 5.0018 (4.9959)  loss_scale: 131072.0000 (155913.5761)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1630  max mem: 15572
[2025-01-12 21:00:21,963] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:00:21,963] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 410/2809]  eta: 0:24:24  lr: 0.000011  min_lr: 0.000000  loss: 4.9906 (4.9958)  loss_scale: 131072.0000 (157541.5280)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1464  max mem: 15572
[2025-01-12 21:00:26,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3220
[2025-01-12 21:00:26,482] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:00:26,482] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 420/2809]  eta: 0:24:18  lr: 0.000011  min_lr: 0.000000  loss: 5.0235 (4.9959)  loss_scale: 131072.0000 (156912.7981)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1391  max mem: 15572
Epoch: [1]  [ 430/2809]  eta: 0:24:06  lr: 0.000011  min_lr: 0.000000  loss: 5.0095 (4.9954)  loss_scale: 131072.0000 (156313.2436)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0883  max mem: 15572
Epoch: [1]  [ 440/2809]  eta: 0:24:02  lr: 0.000011  min_lr: 0.000000  loss: 4.9743 (4.9946)  loss_scale: 131072.0000 (155740.8798)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1008  max mem: 15572
Epoch: [1]  [ 450/2809]  eta: 0:23:52  lr: 0.000011  min_lr: 0.000000  loss: 4.9470 (4.9936)  loss_scale: 131072.0000 (155193.8980)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1286  max mem: 15572
Epoch: [1]  [ 460/2809]  eta: 0:23:50  lr: 0.000011  min_lr: 0.000000  loss: 4.9470 (4.9938)  loss_scale: 131072.0000 (154670.6464)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1620  max mem: 15572
Epoch: [1]  [ 470/2809]  eta: 0:23:42  lr: 0.000011  min_lr: 0.000000  loss: 4.9925 (4.9946)  loss_scale: 131072.0000 (154169.6136)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1779  max mem: 15572
Epoch: [1]  [ 480/2809]  eta: 0:23:34  lr: 0.000011  min_lr: 0.000000  loss: 5.0116 (4.9946)  loss_scale: 131072.0000 (153689.4137)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1344  max mem: 15572
Epoch: [1]  [ 490/2809]  eta: 0:23:26  lr: 0.000011  min_lr: 0.000000  loss: 4.9737 (4.9938)  loss_scale: 131072.0000 (153228.7739)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1383  max mem: 15572
Epoch: [1]  [ 500/2809]  eta: 0:23:19  lr: 0.000011  min_lr: 0.000000  loss: 4.9396 (4.9938)  loss_scale: 131072.0000 (152786.5230)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1583  max mem: 15572
Epoch: [1]  [ 510/2809]  eta: 0:23:15  lr: 0.000011  min_lr: 0.000000  loss: 4.9434 (4.9925)  loss_scale: 131072.0000 (152361.5812)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1752  max mem: 15572
Epoch: [1]  [ 520/2809]  eta: 0:23:07  lr: 0.000011  min_lr: 0.000000  loss: 4.9867 (4.9935)  loss_scale: 131072.0000 (151952.9520)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1323  max mem: 15572
Epoch: [1]  [ 530/2809]  eta: 0:23:01  lr: 0.000011  min_lr: 0.000000  loss: 5.0357 (4.9937)  loss_scale: 131072.0000 (151559.7137)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1120  max mem: 15572
[2025-01-12 21:01:40,880] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:01:40,881] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 540/2809]  eta: 0:22:51  lr: 0.000011  min_lr: 0.000000  loss: 5.0357 (4.9952)  loss_scale: 131072.0000 (151423.2902)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1069  max mem: 15572
Epoch: [1]  [ 550/2809]  eta: 0:22:42  lr: 0.000011  min_lr: 0.000000  loss: 5.0675 (4.9961)  loss_scale: 262144.0000 (153432.7405)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0674  max mem: 15572
[2025-01-12 21:01:53,005] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3369
[2025-01-12 21:01:53,006] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:01:53,006] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 560/2809]  eta: 0:22:39  lr: 0.000011  min_lr: 0.000000  loss: 5.0528 (4.9968)  loss_scale: 262144.0000 (155136.9127)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1433  max mem: 15572
Epoch: [1]  [ 570/2809]  eta: 0:22:30  lr: 0.000011  min_lr: 0.000000  loss: 5.0045 (4.9961)  loss_scale: 131072.0000 (154715.4606)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1518  max mem: 15572
Epoch: [1]  [ 580/2809]  eta: 0:22:26  lr: 0.000011  min_lr: 0.000000  loss: 5.0087 (4.9969)  loss_scale: 131072.0000 (154308.5164)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1288  max mem: 15572
Epoch: [1]  [ 590/2809]  eta: 0:22:22  lr: 0.000011  min_lr: 0.000000  loss: 5.0087 (4.9968)  loss_scale: 131072.0000 (153915.3435)  weight_decay: 0.0500 (0.0500)  time: 0.6602  data: 0.2041  max mem: 15572
Epoch: [1]  [ 600/2809]  eta: 0:22:15  lr: 0.000011  min_lr: 0.000000  loss: 4.9798 (4.9968)  loss_scale: 131072.0000 (153535.2546)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1820  max mem: 15572
Epoch: [1]  [ 610/2809]  eta: 0:22:09  lr: 0.000011  min_lr: 0.000000  loss: 5.0114 (4.9972)  loss_scale: 131072.0000 (153167.6072)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1534  max mem: 15572
Epoch: [1]  [ 620/2809]  eta: 0:22:02  lr: 0.000011  min_lr: 0.000000  loss: 5.0114 (4.9967)  loss_scale: 131072.0000 (152811.8003)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1429  max mem: 15572
Epoch: [1]  [ 630/2809]  eta: 0:21:58  lr: 0.000011  min_lr: 0.000000  loss: 5.0298 (4.9976)  loss_scale: 131072.0000 (152467.2710)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1684  max mem: 15572
Epoch: [1]  [ 640/2809]  eta: 0:21:50  lr: 0.000012  min_lr: 0.000000  loss: 5.0298 (4.9985)  loss_scale: 131072.0000 (152133.4914)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1288  max mem: 15572
Epoch: [1]  [ 650/2809]  eta: 0:21:43  lr: 0.000012  min_lr: 0.000000  loss: 5.0424 (4.9994)  loss_scale: 131072.0000 (151809.9662)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0610  max mem: 15572
Epoch: [1]  [ 660/2809]  eta: 0:21:36  lr: 0.000012  min_lr: 0.000000  loss: 5.0207 (4.9996)  loss_scale: 131072.0000 (151496.2300)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0758  max mem: 15572
Epoch: [1]  [ 670/2809]  eta: 0:21:30  lr: 0.000012  min_lr: 0.000000  loss: 4.9962 (5.0007)  loss_scale: 131072.0000 (151191.8450)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1021  max mem: 15572
Epoch: [1]  [ 680/2809]  eta: 0:21:23  lr: 0.000012  min_lr: 0.000000  loss: 5.0603 (5.0016)  loss_scale: 131072.0000 (150896.3994)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1306  max mem: 15572
[2025-01-12 21:03:08,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:03:08,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 690/2809]  eta: 0:21:14  lr: 0.000012  min_lr: 0.000000  loss: 5.0687 (5.0027)  loss_scale: 131072.0000 (150988.8741)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0798  max mem: 15572
[2025-01-12 21:03:09,903] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3500
[2025-01-12 21:03:09,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:03:09,904] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-01-12 21:03:12,057] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3503
[2025-01-12 21:03:12,057] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:03:12,057] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [ 700/2809]  eta: 0:21:05  lr: 0.000012  min_lr: 0.000000  loss: 4.9768 (5.0019)  loss_scale: 131072.0000 (150050.3281)  weight_decay: 0.0500 (0.0500)  time: 0.5176  data: 0.0607  max mem: 15572
Epoch: [1]  [ 710/2809]  eta: 0:20:53  lr: 0.000012  min_lr: 0.000000  loss: 4.9768 (5.0022)  loss_scale: 65536.0000 (148861.6596)  weight_decay: 0.0500 (0.0500)  time: 0.4649  data: 0.0445  max mem: 15572
Epoch: [1]  [ 720/2809]  eta: 0:20:45  lr: 0.000012  min_lr: 0.000000  loss: 4.9902 (5.0015)  loss_scale: 65536.0000 (147705.9639)  weight_decay: 0.0500 (0.0500)  time: 0.4474  data: 0.0011  max mem: 15572
Epoch: [1]  [ 730/2809]  eta: 0:20:35  lr: 0.000012  min_lr: 0.000000  loss: 4.9634 (5.0019)  loss_scale: 65536.0000 (146581.8878)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0013  max mem: 15572
Epoch: [1]  [ 740/2809]  eta: 0:20:25  lr: 0.000012  min_lr: 0.000000  loss: 4.9552 (5.0011)  loss_scale: 65536.0000 (145488.1511)  weight_decay: 0.0500 (0.0500)  time: 0.4665  data: 0.0009  max mem: 15572
Epoch: [1]  [ 750/2809]  eta: 0:20:20  lr: 0.000012  min_lr: 0.000000  loss: 4.9283 (5.0004)  loss_scale: 65536.0000 (144423.5419)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0589  max mem: 15572
Epoch: [1]  [ 760/2809]  eta: 0:20:16  lr: 0.000012  min_lr: 0.000000  loss: 4.9603 (5.0008)  loss_scale: 65536.0000 (143386.9120)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.1384  max mem: 15572
Epoch: [1]  [ 770/2809]  eta: 0:20:15  lr: 0.000012  min_lr: 0.000000  loss: 4.9921 (5.0001)  loss_scale: 65536.0000 (142377.1725)  weight_decay: 0.0500 (0.0500)  time: 0.7163  data: 0.2231  max mem: 15572
Epoch: [1]  [ 780/2809]  eta: 0:20:11  lr: 0.000012  min_lr: 0.000000  loss: 4.9785 (5.0000)  loss_scale: 65536.0000 (141393.2907)  weight_decay: 0.0500 (0.0500)  time: 0.7307  data: 0.2474  max mem: 15572
Epoch: [1]  [ 790/2809]  eta: 0:20:09  lr: 0.000012  min_lr: 0.000000  loss: 4.9424 (4.9993)  loss_scale: 65536.0000 (140434.2857)  weight_decay: 0.0500 (0.0500)  time: 0.7159  data: 0.2249  max mem: 15572
Epoch: [1]  [ 800/2809]  eta: 0:20:05  lr: 0.000012  min_lr: 0.000000  loss: 4.9774 (4.9993)  loss_scale: 65536.0000 (139499.2260)  weight_decay: 0.0500 (0.0500)  time: 0.7122  data: 0.2178  max mem: 15572
Epoch: [1]  [ 810/2809]  eta: 0:20:00  lr: 0.000012  min_lr: 0.000000  loss: 5.0090 (4.9998)  loss_scale: 65536.0000 (138587.2256)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.1779  max mem: 15572
Epoch: [1]  [ 820/2809]  eta: 0:19:56  lr: 0.000012  min_lr: 0.000000  loss: 5.0156 (5.0001)  loss_scale: 65536.0000 (137697.4421)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.1733  max mem: 15572
[2025-01-12 21:04:29,705] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:04:29,705] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [ 830/2809]  eta: 0:19:51  lr: 0.000012  min_lr: 0.000000  loss: 5.0141 (5.0004)  loss_scale: 65536.0000 (137459.9856)  weight_decay: 0.0500 (0.0500)  time: 0.6784  data: 0.1931  max mem: 15572
Epoch: [1]  [ 840/2809]  eta: 0:19:47  lr: 0.000012  min_lr: 0.000000  loss: 5.0072 (5.0007)  loss_scale: 131072.0000 (137384.0285)  weight_decay: 0.0500 (0.0500)  time: 0.6808  data: 0.2157  max mem: 15572
Epoch: [1]  [ 850/2809]  eta: 0:19:41  lr: 0.000012  min_lr: 0.000000  loss: 5.0431 (5.0012)  loss_scale: 131072.0000 (137309.8566)  weight_decay: 0.0500 (0.0500)  time: 0.6500  data: 0.2058  max mem: 15572
Epoch: [1]  [ 860/2809]  eta: 0:19:31  lr: 0.000012  min_lr: 0.000000  loss: 5.0151 (5.0006)  loss_scale: 131072.0000 (137237.4077)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0909  max mem: 15572
Epoch: [1]  [ 870/2809]  eta: 0:19:22  lr: 0.000012  min_lr: 0.000000  loss: 4.9353 (5.0006)  loss_scale: 131072.0000 (137166.6223)  weight_decay: 0.0500 (0.0500)  time: 0.4408  data: 0.0024  max mem: 15572
Epoch: [1]  [ 880/2809]  eta: 0:19:12  lr: 0.000012  min_lr: 0.000000  loss: 4.9321 (5.0004)  loss_scale: 131072.0000 (137097.4438)  weight_decay: 0.0500 (0.0500)  time: 0.4382  data: 0.0026  max mem: 15572
Epoch: [1]  [ 890/2809]  eta: 0:19:05  lr: 0.000012  min_lr: 0.000000  loss: 4.9883 (5.0007)  loss_scale: 131072.0000 (137029.8182)  weight_decay: 0.0500 (0.0500)  time: 0.4778  data: 0.0436  max mem: 15572
Epoch: [1]  [ 900/2809]  eta: 0:18:59  lr: 0.000012  min_lr: 0.000000  loss: 5.0071 (5.0008)  loss_scale: 131072.0000 (136963.6937)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1245  max mem: 15572
Epoch: [1]  [ 910/2809]  eta: 0:18:53  lr: 0.000012  min_lr: 0.000000  loss: 5.0133 (5.0014)  loss_scale: 131072.0000 (136899.0209)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1586  max mem: 15572
Epoch: [1]  [ 920/2809]  eta: 0:18:47  lr: 0.000012  min_lr: 0.000000  loss: 4.9953 (5.0014)  loss_scale: 131072.0000 (136835.7524)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1703  max mem: 15572
Epoch: [1]  [ 930/2809]  eta: 0:18:41  lr: 0.000012  min_lr: 0.000000  loss: 5.0012 (5.0017)  loss_scale: 131072.0000 (136773.8432)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1703  max mem: 15572
Epoch: [1]  [ 940/2809]  eta: 0:18:35  lr: 0.000013  min_lr: 0.000000  loss: 5.0077 (5.0015)  loss_scale: 131072.0000 (136713.2497)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1427  max mem: 15572
Epoch: [1]  [ 950/2809]  eta: 0:18:30  lr: 0.000013  min_lr: 0.000000  loss: 4.9236 (5.0005)  loss_scale: 131072.0000 (136653.9306)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1746  max mem: 15572
[2025-01-12 21:05:42,525] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:05:42,525] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 960/2809]  eta: 0:18:25  lr: 0.000013  min_lr: 0.000000  loss: 4.9377 (5.0008)  loss_scale: 131072.0000 (137959.7586)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.2084  max mem: 15572
Epoch: [1]  [ 970/2809]  eta: 0:18:21  lr: 0.000013  min_lr: 0.000000  loss: 4.9631 (5.0011)  loss_scale: 262144.0000 (139238.6900)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.2171  max mem: 15572
Epoch: [1]  [ 980/2809]  eta: 0:18:13  lr: 0.000013  min_lr: 0.000000  loss: 4.9497 (5.0006)  loss_scale: 262144.0000 (140491.5474)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1730  max mem: 15572
Epoch: [1]  [ 990/2809]  eta: 0:18:06  lr: 0.000013  min_lr: 0.000000  loss: 4.9084 (5.0002)  loss_scale: 262144.0000 (141719.1201)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.1140  max mem: 15572
Epoch: [1]  [1000/2809]  eta: 0:18:01  lr: 0.000013  min_lr: 0.000000  loss: 4.9223 (5.0001)  loss_scale: 262144.0000 (142922.1658)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1404  max mem: 15572
Epoch: [1]  [1010/2809]  eta: 0:17:57  lr: 0.000013  min_lr: 0.000000  loss: 5.0424 (5.0009)  loss_scale: 262144.0000 (144101.4125)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.2004  max mem: 15572
[2025-01-12 21:06:25,173] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3828
[2025-01-12 21:06:25,173] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:06:25,173] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [1020/2809]  eta: 0:17:51  lr: 0.000013  min_lr: 0.000000  loss: 5.0424 (5.0011)  loss_scale: 262144.0000 (145000.8071)  weight_decay: 0.0500 (0.0500)  time: 0.6654  data: 0.1925  max mem: 15572
Epoch: [1]  [1030/2809]  eta: 0:17:45  lr: 0.000013  min_lr: 0.000000  loss: 4.9455 (5.0007)  loss_scale: 131072.0000 (144865.7071)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1153  max mem: 15572
Epoch: [1]  [1040/2809]  eta: 0:17:39  lr: 0.000013  min_lr: 0.000000  loss: 4.9711 (5.0003)  loss_scale: 131072.0000 (144733.2027)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1136  max mem: 15572
Epoch: [1]  [1050/2809]  eta: 0:17:33  lr: 0.000013  min_lr: 0.000000  loss: 4.9333 (4.9996)  loss_scale: 131072.0000 (144603.2198)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1452  max mem: 15572
Epoch: [1]  [1060/2809]  eta: 0:17:28  lr: 0.000013  min_lr: 0.000000  loss: 4.9158 (4.9990)  loss_scale: 131072.0000 (144475.6871)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1554  max mem: 15572
Epoch: [1]  [1070/2809]  eta: 0:17:19  lr: 0.000013  min_lr: 0.000000  loss: 4.9246 (4.9985)  loss_scale: 131072.0000 (144350.5359)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0825  max mem: 15572
Epoch: [1]  [1080/2809]  eta: 0:17:13  lr: 0.000013  min_lr: 0.000000  loss: 4.9418 (4.9980)  loss_scale: 131072.0000 (144227.7003)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0589  max mem: 15572
Epoch: [1]  [1090/2809]  eta: 0:17:06  lr: 0.000013  min_lr: 0.000000  loss: 4.9421 (4.9979)  loss_scale: 131072.0000 (144107.1164)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0790  max mem: 15572
Epoch: [1]  [1100/2809]  eta: 0:17:01  lr: 0.000013  min_lr: 0.000000  loss: 4.9770 (4.9979)  loss_scale: 131072.0000 (143988.7230)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1116  max mem: 15572
[2025-01-12 21:07:16,048] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3917
[2025-01-12 21:07:16,048] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:07:16,048] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1110/2809]  eta: 0:16:54  lr: 0.000013  min_lr: 0.000000  loss: 4.9963 (4.9978)  loss_scale: 131072.0000 (143695.4959)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0915  max mem: 15572
Epoch: [1]  [1120/2809]  eta: 0:16:46  lr: 0.000013  min_lr: 0.000000  loss: 5.0379 (4.9986)  loss_scale: 65536.0000 (142998.2658)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0461  max mem: 15572
Epoch: [1]  [1130/2809]  eta: 0:16:39  lr: 0.000013  min_lr: 0.000000  loss: 5.0168 (4.9986)  loss_scale: 65536.0000 (142313.3652)  weight_decay: 0.0500 (0.0500)  time: 0.4913  data: 0.0607  max mem: 15572
Epoch: [1]  [1140/2809]  eta: 0:16:33  lr: 0.000013  min_lr: 0.000000  loss: 4.9625 (4.9984)  loss_scale: 65536.0000 (141640.4698)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1188  max mem: 15572
Epoch: [1]  [1150/2809]  eta: 0:16:26  lr: 0.000013  min_lr: 0.000000  loss: 5.0072 (4.9988)  loss_scale: 65536.0000 (140979.2667)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1244  max mem: 15572
Epoch: [1]  [1160/2809]  eta: 0:16:20  lr: 0.000013  min_lr: 0.000000  loss: 4.9998 (4.9985)  loss_scale: 65536.0000 (140329.4539)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0848  max mem: 15572
Epoch: [1]  [1170/2809]  eta: 0:16:14  lr: 0.000013  min_lr: 0.000000  loss: 4.9958 (4.9987)  loss_scale: 65536.0000 (139690.7395)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1322  max mem: 15572
Epoch: [1]  [1180/2809]  eta: 0:16:07  lr: 0.000013  min_lr: 0.000000  loss: 5.0764 (4.9994)  loss_scale: 65536.0000 (139062.8417)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1254  max mem: 15572
[2025-01-12 21:08:01,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=17, lr=[1.2932318270399928e-07, 1.2932318270399928e-07, 1.8474740386285612e-07, 1.8474740386285612e-07, 2.6392486266122304e-07, 2.6392486266122304e-07, 3.7703551808746154e-07, 3.7703551808746154e-07, 5.386221686963737e-07, 5.386221686963737e-07, 7.694602409948195e-07, 7.694602409948195e-07, 1.0992289157068852e-06, 1.0992289157068852e-06, 1.5703270224384075e-06, 1.5703270224384075e-06, 2.2433243177691536e-06, 2.2433243177691536e-06, 3.2047490253845054e-06, 3.2047490253845054e-06, 4.578212893406436e-06, 4.578212893406436e-06, 6.5403041334377665e-06, 6.5403041334377665e-06, 9.34329161919681e-06, 9.34329161919681e-06, 1.3347559455995444e-05, 1.3347559455995444e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:08:01,851] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=27.86961265917859, CurrSamplesPerSec=27.337946902135744, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [1190/2809]  eta: 0:16:02  lr: 0.000013  min_lr: 0.000000  loss: 4.9669 (4.9988)  loss_scale: 65536.0000 (138445.4878)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1479  max mem: 15572
Epoch: [1]  [1200/2809]  eta: 0:15:56  lr: 0.000013  min_lr: 0.000000  loss: 4.9558 (4.9986)  loss_scale: 65536.0000 (137838.4147)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1732  max mem: 15572
Epoch: [1]  [1210/2809]  eta: 0:15:49  lr: 0.000013  min_lr: 0.000000  loss: 4.9985 (4.9988)  loss_scale: 65536.0000 (137241.3675)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1102  max mem: 15572
Epoch: [1]  [1220/2809]  eta: 0:15:42  lr: 0.000013  min_lr: 0.000000  loss: 4.9965 (4.9988)  loss_scale: 65536.0000 (136654.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0759  max mem: 15572
Epoch: [1]  [1230/2809]  eta: 0:15:37  lr: 0.000013  min_lr: 0.000000  loss: 5.0077 (4.9988)  loss_scale: 65536.0000 (136076.3737)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1361  max mem: 15572
[2025-01-12 21:08:30,347] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:08:30,347] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1240/2809]  eta: 0:15:32  lr: 0.000014  min_lr: 0.000000  loss: 5.0098 (4.9993)  loss_scale: 65536.0000 (135719.1942)  weight_decay: 0.0500 (0.0500)  time: 0.6588  data: 0.1906  max mem: 15572
Epoch: [1]  [1250/2809]  eta: 0:15:25  lr: 0.000014  min_lr: 0.000000  loss: 5.0595 (4.9997)  loss_scale: 131072.0000 (135682.0464)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1035  max mem: 15572
Epoch: [1]  [1260/2809]  eta: 0:15:18  lr: 0.000014  min_lr: 0.000000  loss: 5.0337 (4.9994)  loss_scale: 131072.0000 (135645.4877)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0180  max mem: 15572
[2025-01-12 21:08:42,569] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4070
[2025-01-12 21:08:42,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:08:42,570] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1270/2809]  eta: 0:15:12  lr: 0.000014  min_lr: 0.000000  loss: 4.9074 (4.9990)  loss_scale: 65536.0000 (135093.8788)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0540  max mem: 15572
Epoch: [1]  [1280/2809]  eta: 0:15:06  lr: 0.000014  min_lr: 0.000000  loss: 4.9494 (4.9987)  loss_scale: 65536.0000 (134550.8821)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1013  max mem: 15572
Epoch: [1]  [1290/2809]  eta: 0:15:00  lr: 0.000014  min_lr: 0.000000  loss: 4.9967 (4.9991)  loss_scale: 65536.0000 (134016.2974)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1859  max mem: 15572
Epoch: [1]  [1300/2809]  eta: 0:14:54  lr: 0.000014  min_lr: 0.000000  loss: 5.0098 (4.9993)  loss_scale: 65536.0000 (133489.9308)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1647  max mem: 15572
Epoch: [1]  [1310/2809]  eta: 0:14:48  lr: 0.000014  min_lr: 0.000000  loss: 5.0345 (4.9997)  loss_scale: 65536.0000 (132971.5942)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1200  max mem: 15572
Epoch: [1]  [1320/2809]  eta: 0:14:42  lr: 0.000014  min_lr: 0.000000  loss: 5.0168 (4.9994)  loss_scale: 65536.0000 (132461.1052)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1705  max mem: 15572
Epoch: [1]  [1330/2809]  eta: 0:14:36  lr: 0.000014  min_lr: 0.000000  loss: 4.9587 (4.9990)  loss_scale: 65536.0000 (131958.2870)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1663  max mem: 15572
Epoch: [1]  [1340/2809]  eta: 0:14:30  lr: 0.000014  min_lr: 0.000000  loss: 4.9883 (4.9997)  loss_scale: 65536.0000 (131462.9679)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1555  max mem: 15572
Epoch: [1]  [1350/2809]  eta: 0:14:25  lr: 0.000014  min_lr: 0.000000  loss: 4.9995 (4.9997)  loss_scale: 65536.0000 (130974.9815)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1778  max mem: 15572
Epoch: [1]  [1360/2809]  eta: 0:14:18  lr: 0.000014  min_lr: 0.000000  loss: 4.9995 (4.9996)  loss_scale: 65536.0000 (130494.1661)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1259  max mem: 15572
Epoch: [1]  [1370/2809]  eta: 0:14:12  lr: 0.000014  min_lr: 0.000000  loss: 4.9777 (4.9994)  loss_scale: 65536.0000 (130020.3647)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1108  max mem: 15572
Epoch: [1]  [1380/2809]  eta: 0:14:07  lr: 0.000014  min_lr: 0.000000  loss: 4.9560 (4.9990)  loss_scale: 65536.0000 (129553.4251)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1667  max mem: 15572
[2025-01-12 21:09:58,576] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:09:58,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1390/2809]  eta: 0:14:01  lr: 0.000014  min_lr: 0.000000  loss: 4.9686 (4.9988)  loss_scale: 65536.0000 (129140.3134)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1661  max mem: 15572
Epoch: [1]  [1400/2809]  eta: 0:13:55  lr: 0.000014  min_lr: 0.000000  loss: 4.9686 (4.9985)  loss_scale: 131072.0000 (129154.1014)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1572  max mem: 15572
Epoch: [1]  [1410/2809]  eta: 0:13:49  lr: 0.000014  min_lr: 0.000000  loss: 4.9262 (4.9981)  loss_scale: 131072.0000 (129167.6938)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1515  max mem: 15572
Epoch: [1]  [1420/2809]  eta: 0:13:43  lr: 0.000014  min_lr: 0.000000  loss: 4.9153 (4.9979)  loss_scale: 131072.0000 (129181.0950)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1501  max mem: 15572
[2025-01-12 21:10:21,893] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4234
[2025-01-12 21:10:21,893] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:10:21,893] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1430/2809]  eta: 0:13:38  lr: 0.000014  min_lr: 0.000000  loss: 4.9406 (4.9980)  loss_scale: 131072.0000 (128919.5248)  weight_decay: 0.0500 (0.0500)  time: 0.6766  data: 0.2120  max mem: 15572
Epoch: [1]  [1440/2809]  eta: 0:13:33  lr: 0.000014  min_lr: 0.000000  loss: 5.0458 (4.9987)  loss_scale: 65536.0000 (128479.6669)  weight_decay: 0.0500 (0.0500)  time: 0.6704  data: 0.2245  max mem: 15572
Epoch: [1]  [1450/2809]  eta: 0:13:26  lr: 0.000014  min_lr: 0.000000  loss: 5.0131 (4.9986)  loss_scale: 65536.0000 (128045.8718)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.1012  max mem: 15572
Epoch: [1]  [1460/2809]  eta: 0:13:20  lr: 0.000014  min_lr: 0.000000  loss: 4.9675 (4.9987)  loss_scale: 65536.0000 (127618.0151)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0806  max mem: 15572
Epoch: [1]  [1470/2809]  eta: 0:13:14  lr: 0.000014  min_lr: 0.000000  loss: 4.9800 (4.9988)  loss_scale: 65536.0000 (127195.9755)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1262  max mem: 15572
Epoch: [1]  [1480/2809]  eta: 0:13:08  lr: 0.000014  min_lr: 0.000000  loss: 4.9800 (4.9987)  loss_scale: 65536.0000 (126779.6354)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1269  max mem: 15572
Epoch: [1]  [1490/2809]  eta: 0:13:02  lr: 0.000014  min_lr: 0.000000  loss: 4.9972 (4.9988)  loss_scale: 65536.0000 (126368.8799)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1823  max mem: 15572
Epoch: [1]  [1500/2809]  eta: 0:12:56  lr: 0.000014  min_lr: 0.000000  loss: 5.0328 (4.9990)  loss_scale: 65536.0000 (125963.5976)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1602  max mem: 15572
Epoch: [1]  [1510/2809]  eta: 0:12:50  lr: 0.000014  min_lr: 0.000000  loss: 4.9441 (4.9987)  loss_scale: 65536.0000 (125563.6797)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0949  max mem: 15572
Epoch: [1]  [1520/2809]  eta: 0:12:44  lr: 0.000014  min_lr: 0.000000  loss: 4.9295 (4.9989)  loss_scale: 65536.0000 (125169.0204)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0831  max mem: 15572
Epoch: [1]  [1530/2809]  eta: 0:12:37  lr: 0.000014  min_lr: 0.000000  loss: 5.0137 (4.9988)  loss_scale: 65536.0000 (124779.5167)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0635  max mem: 15572
Epoch: [1]  [1540/2809]  eta: 0:12:32  lr: 0.000015  min_lr: 0.000000  loss: 4.9685 (4.9987)  loss_scale: 65536.0000 (124395.0681)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1328  max mem: 15572
Epoch: [1]  [1550/2809]  eta: 0:12:26  lr: 0.000015  min_lr: 0.000000  loss: 5.0386 (4.9987)  loss_scale: 65536.0000 (124015.5770)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1624  max mem: 15572
[2025-01-12 21:11:36,352] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:11:36,352] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1560/2809]  eta: 0:12:19  lr: 0.000015  min_lr: 0.000000  loss: 5.0386 (4.9990)  loss_scale: 65536.0000 (123934.8315)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1139  max mem: 15572
Epoch: [1]  [1570/2809]  eta: 0:12:13  lr: 0.000015  min_lr: 0.000000  loss: 5.0313 (4.9989)  loss_scale: 131072.0000 (123980.2623)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1170  max mem: 15572
Epoch: [1]  [1580/2809]  eta: 0:12:08  lr: 0.000015  min_lr: 0.000000  loss: 4.9970 (4.9992)  loss_scale: 131072.0000 (124025.1183)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1634  max mem: 15572
Epoch: [1]  [1590/2809]  eta: 0:12:01  lr: 0.000015  min_lr: 0.000000  loss: 4.9959 (4.9992)  loss_scale: 131072.0000 (124069.4104)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1508  max mem: 15572
Epoch: [1]  [1600/2809]  eta: 0:11:55  lr: 0.000015  min_lr: 0.000000  loss: 5.0116 (4.9994)  loss_scale: 131072.0000 (124113.1493)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0949  max mem: 15572
Epoch: [1]  [1610/2809]  eta: 0:11:50  lr: 0.000015  min_lr: 0.000000  loss: 5.0116 (4.9991)  loss_scale: 131072.0000 (124156.3451)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1750  max mem: 15572
Epoch: [1]  [1620/2809]  eta: 0:11:43  lr: 0.000015  min_lr: 0.000000  loss: 4.9515 (4.9988)  loss_scale: 131072.0000 (124199.0080)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1315  max mem: 15572
[2025-01-12 21:12:16,825] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4433
[2025-01-12 21:12:16,826] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:12:16,826] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1630/2809]  eta: 0:11:38  lr: 0.000015  min_lr: 0.000000  loss: 4.9242 (4.9983)  loss_scale: 131072.0000 (123959.8774)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1731  max mem: 15572
Epoch: [1]  [1640/2809]  eta: 0:11:32  lr: 0.000015  min_lr: 0.000000  loss: 4.9647 (4.9984)  loss_scale: 65536.0000 (123603.8513)  weight_decay: 0.0500 (0.0500)  time: 0.6596  data: 0.2193  max mem: 15572
Epoch: [1]  [1650/2809]  eta: 0:11:27  lr: 0.000015  min_lr: 0.000000  loss: 4.9772 (4.9982)  loss_scale: 65536.0000 (123252.1381)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1602  max mem: 15572
Epoch: [1]  [1660/2809]  eta: 0:11:20  lr: 0.000015  min_lr: 0.000000  loss: 4.9385 (4.9980)  loss_scale: 65536.0000 (122904.6598)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1326  max mem: 15572
Epoch: [1]  [1670/2809]  eta: 0:11:14  lr: 0.000015  min_lr: 0.000000  loss: 4.9437 (4.9976)  loss_scale: 65536.0000 (122561.3405)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1202  max mem: 15572
Epoch: [1]  [1680/2809]  eta: 0:11:09  lr: 0.000015  min_lr: 0.000000  loss: 4.9766 (4.9976)  loss_scale: 65536.0000 (122222.1059)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1753  max mem: 15572
Epoch: [1]  [1690/2809]  eta: 0:11:03  lr: 0.000015  min_lr: 0.000000  loss: 4.9701 (4.9973)  loss_scale: 65536.0000 (121886.8835)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1552  max mem: 15572
Epoch: [1]  [1700/2809]  eta: 0:10:56  lr: 0.000015  min_lr: 0.000000  loss: 4.9874 (4.9973)  loss_scale: 65536.0000 (121555.6026)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1302  max mem: 15572
Epoch: [1]  [1710/2809]  eta: 0:10:50  lr: 0.000015  min_lr: 0.000000  loss: 4.9949 (4.9970)  loss_scale: 65536.0000 (121228.1940)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1188  max mem: 15572
Epoch: [1]  [1720/2809]  eta: 0:10:45  lr: 0.000015  min_lr: 0.000000  loss: 4.9817 (4.9967)  loss_scale: 65536.0000 (120904.5904)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1315  max mem: 15572
Epoch: [1]  [1730/2809]  eta: 0:10:40  lr: 0.000015  min_lr: 0.000000  loss: 4.9203 (4.9964)  loss_scale: 65536.0000 (120584.7256)  weight_decay: 0.0500 (0.0500)  time: 0.6666  data: 0.2286  max mem: 15572
Epoch: [1]  [1740/2809]  eta: 0:10:33  lr: 0.000015  min_lr: 0.000000  loss: 4.9584 (4.9962)  loss_scale: 65536.0000 (120268.5353)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1718  max mem: 15572
Epoch: [1]  [1750/2809]  eta: 0:10:27  lr: 0.000015  min_lr: 0.000000  loss: 4.9867 (4.9959)  loss_scale: 65536.0000 (119955.9566)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1200  max mem: 15572
[2025-01-12 21:13:35,565] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:13:35,565] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1760/2809]  eta: 0:10:21  lr: 0.000015  min_lr: 0.000000  loss: 4.9874 (4.9961)  loss_scale: 65536.0000 (119944.6496)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.2011  max mem: 15572
Epoch: [1]  [1770/2809]  eta: 0:10:15  lr: 0.000015  min_lr: 0.000000  loss: 5.0192 (4.9961)  loss_scale: 131072.0000 (120007.4805)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1379  max mem: 15572
[2025-01-12 21:13:47,196] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4584
[2025-01-12 21:13:47,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:13:47,197] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1780/2809]  eta: 0:10:09  lr: 0.000015  min_lr: 0.000000  loss: 4.9692 (4.9960)  loss_scale: 131072.0000 (119848.8220)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1011  max mem: 15572
Epoch: [1]  [1790/2809]  eta: 0:10:03  lr: 0.000015  min_lr: 0.000000  loss: 4.9769 (4.9960)  loss_scale: 65536.0000 (119545.5678)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1358  max mem: 15572
Epoch: [1]  [1800/2809]  eta: 0:09:58  lr: 0.000015  min_lr: 0.000000  loss: 4.9557 (4.9958)  loss_scale: 65536.0000 (119245.6813)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1995  max mem: 15572
Epoch: [1]  [1810/2809]  eta: 0:09:51  lr: 0.000015  min_lr: 0.000000  loss: 4.9514 (4.9957)  loss_scale: 65536.0000 (118949.1066)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1588  max mem: 15572
Epoch: [1]  [1820/2809]  eta: 0:09:45  lr: 0.000015  min_lr: 0.000000  loss: 4.9959 (4.9961)  loss_scale: 65536.0000 (118655.7891)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1058  max mem: 15572
Epoch: [1]  [1830/2809]  eta: 0:09:39  lr: 0.000015  min_lr: 0.000000  loss: 5.0412 (4.9962)  loss_scale: 65536.0000 (118365.6756)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1386  max mem: 15572
Epoch: [1]  [1840/2809]  eta: 0:09:33  lr: 0.000016  min_lr: 0.000000  loss: 5.0040 (4.9961)  loss_scale: 65536.0000 (118078.7137)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1391  max mem: 15572
Epoch: [1]  [1850/2809]  eta: 0:09:28  lr: 0.000016  min_lr: 0.000000  loss: 4.9427 (4.9959)  loss_scale: 65536.0000 (117794.8525)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1587  max mem: 15572
Epoch: [1]  [1860/2809]  eta: 0:09:21  lr: 0.000016  min_lr: 0.000000  loss: 4.9491 (4.9961)  loss_scale: 65536.0000 (117514.0419)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0956  max mem: 15572
Epoch: [1]  [1870/2809]  eta: 0:09:15  lr: 0.000016  min_lr: 0.000000  loss: 5.0058 (4.9959)  loss_scale: 65536.0000 (117236.2330)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0554  max mem: 15572
Epoch: [1]  [1880/2809]  eta: 0:09:09  lr: 0.000016  min_lr: 0.000000  loss: 4.9893 (4.9960)  loss_scale: 65536.0000 (116961.3780)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1060  max mem: 15572
Epoch: [1]  [1890/2809]  eta: 0:09:03  lr: 0.000016  min_lr: 0.000000  loss: 4.9609 (4.9959)  loss_scale: 65536.0000 (116689.4299)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0517  max mem: 15572
Epoch: [1]  [1900/2809]  eta: 0:08:57  lr: 0.000016  min_lr: 0.000000  loss: 4.9566 (4.9959)  loss_scale: 65536.0000 (116420.3430)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0676  max mem: 15572
[2025-01-12 21:15:01,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:15:01,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 21:15:02,307] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4715
[2025-01-12 21:15:02,308] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:15:02,308] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1910/2809]  eta: 0:08:51  lr: 0.000016  min_lr: 0.000000  loss: 5.0075 (4.9960)  loss_scale: 65536.0000 (116222.6604)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1267  max mem: 15572
Epoch: [1]  [1920/2809]  eta: 0:08:45  lr: 0.000016  min_lr: 0.000000  loss: 4.9993 (4.9959)  loss_scale: 65536.0000 (115958.8048)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1272  max mem: 15572
Epoch: [1]  [1930/2809]  eta: 0:08:39  lr: 0.000016  min_lr: 0.000000  loss: 5.0251 (4.9960)  loss_scale: 65536.0000 (115697.6820)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1157  max mem: 15572
Epoch: [1]  [1940/2809]  eta: 0:08:33  lr: 0.000016  min_lr: 0.000000  loss: 5.0146 (4.9959)  loss_scale: 65536.0000 (115439.2499)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1491  max mem: 15572
Epoch: [1]  [1950/2809]  eta: 0:08:27  lr: 0.000016  min_lr: 0.000000  loss: 4.9636 (4.9954)  loss_scale: 65536.0000 (115183.4669)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.2104  max mem: 15572
Epoch: [1]  [1960/2809]  eta: 0:08:21  lr: 0.000016  min_lr: 0.000000  loss: 4.9788 (4.9954)  loss_scale: 65536.0000 (114930.2927)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1622  max mem: 15572
Epoch: [1]  [1970/2809]  eta: 0:08:16  lr: 0.000016  min_lr: 0.000000  loss: 4.9705 (4.9951)  loss_scale: 65536.0000 (114679.6875)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1718  max mem: 15572
Epoch: [1]  [1980/2809]  eta: 0:08:09  lr: 0.000016  min_lr: 0.000000  loss: 4.9705 (4.9952)  loss_scale: 65536.0000 (114431.6123)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1358  max mem: 15572
Epoch: [1]  [1990/2809]  eta: 0:08:04  lr: 0.000016  min_lr: 0.000000  loss: 5.0016 (4.9951)  loss_scale: 65536.0000 (114186.0291)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0667  max mem: 15572
Epoch: [1]  [2000/2809]  eta: 0:07:58  lr: 0.000016  min_lr: 0.000000  loss: 4.9506 (4.9949)  loss_scale: 65536.0000 (113942.9005)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1539  max mem: 15572
Epoch: [1]  [2010/2809]  eta: 0:07:52  lr: 0.000016  min_lr: 0.000000  loss: 4.9547 (4.9950)  loss_scale: 65536.0000 (113702.1900)  weight_decay: 0.0500 (0.0500)  time: 0.6652  data: 0.2140  max mem: 15572
Epoch: [1]  [2020/2809]  eta: 0:07:46  lr: 0.000016  min_lr: 0.000000  loss: 4.9621 (4.9949)  loss_scale: 65536.0000 (113463.8615)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1835  max mem: 15572
Epoch: [1]  [2030/2809]  eta: 0:07:40  lr: 0.000016  min_lr: 0.000000  loss: 4.9621 (4.9949)  loss_scale: 65536.0000 (113227.8799)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0835  max mem: 15572
[2025-01-12 21:16:17,290] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:16:17,290] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 21:16:21,093] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4848
[2025-01-12 21:16:21,093] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:16:21,094] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2040/2809]  eta: 0:07:34  lr: 0.000016  min_lr: 0.000000  loss: 4.9641 (4.9946)  loss_scale: 65536.0000 (113122.6497)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1067  max mem: 15572
Epoch: [1]  [2050/2809]  eta: 0:07:28  lr: 0.000016  min_lr: 0.000000  loss: 4.9641 (4.9948)  loss_scale: 65536.0000 (112890.6329)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1527  max mem: 15572
Epoch: [1]  [2060/2809]  eta: 0:07:23  lr: 0.000016  min_lr: 0.000000  loss: 4.9646 (4.9947)  loss_scale: 65536.0000 (112660.8675)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1386  max mem: 15572
Epoch: [1]  [2070/2809]  eta: 0:07:16  lr: 0.000016  min_lr: 0.000000  loss: 4.9925 (4.9947)  loss_scale: 65536.0000 (112433.3211)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1025  max mem: 15572
[2025-01-12 21:16:40,153] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4882
[2025-01-12 21:16:40,153] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 21:16:40,156] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [2080/2809]  eta: 0:07:11  lr: 0.000016  min_lr: 0.000000  loss: 5.0045 (4.9947)  loss_scale: 65536.0000 (112081.9914)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0955  max mem: 15572
Epoch: [1]  [2090/2809]  eta: 0:07:05  lr: 0.000016  min_lr: 0.000000  loss: 4.9630 (4.9945)  loss_scale: 32768.0000 (111702.6801)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1432  max mem: 15572
Epoch: [1]  [2100/2809]  eta: 0:06:59  lr: 0.000016  min_lr: 0.000000  loss: 4.9746 (4.9946)  loss_scale: 32768.0000 (111326.9795)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1963  max mem: 15572
Epoch: [1]  [2110/2809]  eta: 0:06:53  lr: 0.000016  min_lr: 0.000000  loss: 5.0337 (4.9947)  loss_scale: 32768.0000 (110954.8385)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1551  max mem: 15572
Epoch: [1]  [2120/2809]  eta: 0:06:47  lr: 0.000016  min_lr: 0.000000  loss: 4.9544 (4.9945)  loss_scale: 32768.0000 (110586.2065)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.0736  max mem: 15572
Epoch: [1]  [2130/2809]  eta: 0:06:41  lr: 0.000016  min_lr: 0.000000  loss: 4.9653 (4.9948)  loss_scale: 32768.0000 (110221.0343)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1016  max mem: 15572
Epoch: [1]  [2140/2809]  eta: 0:06:35  lr: 0.000017  min_lr: 0.000000  loss: 4.9736 (4.9946)  loss_scale: 32768.0000 (109859.2732)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1496  max mem: 15572
Epoch: [1]  [2150/2809]  eta: 0:06:29  lr: 0.000017  min_lr: 0.000000  loss: 4.9736 (4.9947)  loss_scale: 32768.0000 (109500.8759)  weight_decay: 0.0500 (0.0500)  time: 0.6240  data: 0.1442  max mem: 15572
Epoch: [1]  [2160/2809]  eta: 0:06:23  lr: 0.000017  min_lr: 0.000000  loss: 5.0416 (4.9950)  loss_scale: 32768.0000 (109145.7955)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1550  max mem: 15572
Epoch: [1]  [2170/2809]  eta: 0:06:17  lr: 0.000017  min_lr: 0.000000  loss: 4.9807 (4.9949)  loss_scale: 32768.0000 (108793.9862)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1591  max mem: 15572
Epoch: [1]  [2180/2809]  eta: 0:06:11  lr: 0.000017  min_lr: 0.000000  loss: 4.9807 (4.9949)  loss_scale: 32768.0000 (108445.4030)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1441  max mem: 15572
[2025-01-12 21:17:50,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[1.6166206310009812e-07, 1.6166206310009812e-07, 2.3094580442871163e-07, 2.3094580442871163e-07, 3.2992257775530236e-07, 3.2992257775530236e-07, 4.7131796822186054e-07, 4.7131796822186054e-07, 6.733113831740865e-07, 6.733113831740865e-07, 9.618734045344093e-07, 9.618734045344093e-07, 1.3741048636205847e-06, 1.3741048636205847e-06, 1.963006948029407e-06, 1.963006948029407e-06, 2.80429564004201e-06, 2.80429564004201e-06, 4.0061366286314435e-06, 4.0061366286314435e-06, 5.723052326616347e-06, 5.723052326616347e-06, 8.175789038023355e-06, 8.175789038023355e-06, 1.167969862574765e-05, 1.167969862574765e-05, 1.6685283751068073e-05, 1.6685283751068073e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:17:50,524] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=27.92310121983637, CurrSamplesPerSec=31.029518691390006, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [2190/2809]  eta: 0:06:06  lr: 0.000017  min_lr: 0.000000  loss: 5.0170 (4.9950)  loss_scale: 32768.0000 (108100.0018)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1796  max mem: 15572
Epoch: [1]  [2200/2809]  eta: 0:06:00  lr: 0.000017  min_lr: 0.000000  loss: 4.9662 (4.9948)  loss_scale: 32768.0000 (107757.7392)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1656  max mem: 15572
[2025-01-12 21:17:57,473] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:17:57,475] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2210/2809]  eta: 0:05:54  lr: 0.000017  min_lr: 0.000000  loss: 4.9382 (4.9944)  loss_scale: 32768.0000 (107551.9566)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1548  max mem: 15572
Epoch: [1]  [2220/2809]  eta: 0:05:48  lr: 0.000017  min_lr: 0.000000  loss: 4.9111 (4.9942)  loss_scale: 65536.0000 (107362.7807)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1449  max mem: 15572
Epoch: [1]  [2230/2809]  eta: 0:05:42  lr: 0.000017  min_lr: 0.000000  loss: 4.9521 (4.9940)  loss_scale: 65536.0000 (107175.3008)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1572  max mem: 15572
[2025-01-12 21:18:20,664] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5049
[2025-01-12 21:18:20,665] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 21:18:20,665] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [2240/2809]  eta: 0:05:36  lr: 0.000017  min_lr: 0.000000  loss: 4.9521 (4.9937)  loss_scale: 65536.0000 (106974.8719)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1638  max mem: 15572
Epoch: [1]  [2250/2809]  eta: 0:05:30  lr: 0.000017  min_lr: 0.000000  loss: 4.9245 (4.9934)  loss_scale: 32768.0000 (106645.2101)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0990  max mem: 15572
Epoch: [1]  [2260/2809]  eta: 0:05:24  lr: 0.000017  min_lr: 0.000000  loss: 4.9285 (4.9934)  loss_scale: 32768.0000 (106318.4644)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0987  max mem: 15572
Epoch: [1]  [2270/2809]  eta: 0:05:18  lr: 0.000017  min_lr: 0.000000  loss: 4.9067 (4.9930)  loss_scale: 32768.0000 (105994.5962)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1588  max mem: 15572
Epoch: [1]  [2280/2809]  eta: 0:05:12  lr: 0.000017  min_lr: 0.000000  loss: 4.9214 (4.9929)  loss_scale: 32768.0000 (105673.5677)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1370  max mem: 15572
Epoch: [1]  [2290/2809]  eta: 0:05:06  lr: 0.000017  min_lr: 0.000000  loss: 4.9410 (4.9926)  loss_scale: 32768.0000 (105355.3418)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1388  max mem: 15572
Epoch: [1]  [2300/2809]  eta: 0:05:01  lr: 0.000017  min_lr: 0.000000  loss: 4.9860 (4.9926)  loss_scale: 32768.0000 (105039.8818)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1636  max mem: 15572
Epoch: [1]  [2310/2809]  eta: 0:04:55  lr: 0.000017  min_lr: 0.000000  loss: 5.0122 (4.9924)  loss_scale: 32768.0000 (104727.1519)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1133  max mem: 15572
Epoch: [1]  [2320/2809]  eta: 0:04:49  lr: 0.000017  min_lr: 0.000000  loss: 4.9263 (4.9921)  loss_scale: 32768.0000 (104417.1168)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1014  max mem: 15572
Epoch: [1]  [2330/2809]  eta: 0:04:43  lr: 0.000017  min_lr: 0.000000  loss: 4.9459 (4.9923)  loss_scale: 32768.0000 (104109.7417)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1487  max mem: 15572
Epoch: [1]  [2340/2809]  eta: 0:04:37  lr: 0.000017  min_lr: 0.000000  loss: 4.9495 (4.9920)  loss_scale: 32768.0000 (103804.9927)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1306  max mem: 15572
Epoch: [1]  [2350/2809]  eta: 0:04:31  lr: 0.000017  min_lr: 0.000000  loss: 4.9495 (4.9922)  loss_scale: 32768.0000 (103502.8362)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1115  max mem: 15572
Epoch: [1]  [2360/2809]  eta: 0:04:25  lr: 0.000017  min_lr: 0.000000  loss: 4.9719 (4.9921)  loss_scale: 32768.0000 (103203.2393)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.1966  max mem: 15572
[2025-01-12 21:19:37,425] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:19:37,426] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2370/2809]  eta: 0:04:19  lr: 0.000017  min_lr: 0.000000  loss: 4.9719 (4.9920)  loss_scale: 32768.0000 (102933.8102)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.1848  max mem: 15572
Epoch: [1]  [2380/2809]  eta: 0:04:14  lr: 0.000017  min_lr: 0.000000  loss: 4.8816 (4.9915)  loss_scale: 65536.0000 (102776.7425)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1190  max mem: 15572
Epoch: [1]  [2390/2809]  eta: 0:04:08  lr: 0.000017  min_lr: 0.000000  loss: 4.8638 (4.9910)  loss_scale: 65536.0000 (102620.9887)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1422  max mem: 15572
Epoch: [1]  [2400/2809]  eta: 0:04:02  lr: 0.000017  min_lr: 0.000000  loss: 4.8923 (4.9906)  loss_scale: 65536.0000 (102466.5323)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1290  max mem: 15572
Epoch: [1]  [2410/2809]  eta: 0:03:56  lr: 0.000017  min_lr: 0.000000  loss: 4.9440 (4.9907)  loss_scale: 65536.0000 (102313.3571)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1311  max mem: 15572
Epoch: [1]  [2420/2809]  eta: 0:03:50  lr: 0.000017  min_lr: 0.000000  loss: 4.9617 (4.9906)  loss_scale: 65536.0000 (102161.4473)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1015  max mem: 15572
Epoch: [1]  [2430/2809]  eta: 0:03:44  lr: 0.000017  min_lr: 0.000000  loss: 4.9517 (4.9906)  loss_scale: 65536.0000 (102010.7873)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0526  max mem: 15572
Epoch: [1]  [2440/2809]  eta: 0:03:38  lr: 0.000018  min_lr: 0.000000  loss: 4.9348 (4.9903)  loss_scale: 65536.0000 (101861.3617)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0617  max mem: 15572
Epoch: [1]  [2450/2809]  eta: 0:03:32  lr: 0.000018  min_lr: 0.000000  loss: 4.9592 (4.9904)  loss_scale: 65536.0000 (101713.1554)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0768  max mem: 15572
Epoch: [1]  [2460/2809]  eta: 0:03:26  lr: 0.000018  min_lr: 0.000000  loss: 5.0032 (4.9903)  loss_scale: 65536.0000 (101566.1536)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1171  max mem: 15572
Epoch: [1]  [2470/2809]  eta: 0:03:20  lr: 0.000018  min_lr: 0.000000  loss: 4.9282 (4.9901)  loss_scale: 65536.0000 (101420.3416)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1090  max mem: 15572
Epoch: [1]  [2480/2809]  eta: 0:03:14  lr: 0.000018  min_lr: 0.000000  loss: 4.9123 (4.9899)  loss_scale: 65536.0000 (101275.7050)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0796  max mem: 15572
Epoch: [1]  [2490/2809]  eta: 0:03:08  lr: 0.000018  min_lr: 0.000000  loss: 4.9232 (4.9896)  loss_scale: 65536.0000 (101132.2296)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1401  max mem: 15572
[2025-01-12 21:20:49,750] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:20:49,751] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2500/2809]  eta: 0:03:02  lr: 0.000018  min_lr: 0.000000  loss: 4.9236 (4.9896)  loss_scale: 65536.0000 (101094.7173)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.1841  max mem: 15572
[2025-01-12 21:20:54,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5313
[2025-01-12 21:20:54,463] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:20:54,463] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2510/2809]  eta: 0:02:56  lr: 0.000018  min_lr: 0.000000  loss: 4.9559 (4.9895)  loss_scale: 65536.0000 (101031.4042)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1751  max mem: 15572
Epoch: [1]  [2520/2809]  eta: 0:02:50  lr: 0.000018  min_lr: 0.000000  loss: 4.9559 (4.9893)  loss_scale: 65536.0000 (100890.6053)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1781  max mem: 15572
Epoch: [1]  [2530/2809]  eta: 0:02:44  lr: 0.000018  min_lr: 0.000000  loss: 4.9441 (4.9892)  loss_scale: 65536.0000 (100750.9190)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1158  max mem: 15572
Epoch: [1]  [2540/2809]  eta: 0:02:38  lr: 0.000018  min_lr: 0.000000  loss: 5.0081 (4.9893)  loss_scale: 65536.0000 (100612.3322)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0779  max mem: 15572
Epoch: [1]  [2550/2809]  eta: 0:02:33  lr: 0.000018  min_lr: 0.000000  loss: 4.9990 (4.9892)  loss_scale: 65536.0000 (100474.8318)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1346  max mem: 15572
Epoch: [1]  [2560/2809]  eta: 0:02:27  lr: 0.000018  min_lr: 0.000000  loss: 4.9625 (4.9891)  loss_scale: 65536.0000 (100338.4053)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1078  max mem: 15572
Epoch: [1]  [2570/2809]  eta: 0:02:21  lr: 0.000018  min_lr: 0.000000  loss: 4.9747 (4.9889)  loss_scale: 65536.0000 (100203.0401)  weight_decay: 0.0500 (0.0500)  time: 0.4629  data: 0.0255  max mem: 15572
Epoch: [1]  [2580/2809]  eta: 0:02:14  lr: 0.000018  min_lr: 0.000000  loss: 4.9203 (4.9886)  loss_scale: 65536.0000 (100068.7238)  weight_decay: 0.0500 (0.0500)  time: 0.3938  data: 0.0004  max mem: 15572
Epoch: [1]  [2590/2809]  eta: 0:02:08  lr: 0.000018  min_lr: 0.000000  loss: 4.9469 (4.9890)  loss_scale: 65536.0000 (99935.4442)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.0003  max mem: 15572
Epoch: [1]  [2600/2809]  eta: 0:02:02  lr: 0.000018  min_lr: 0.000000  loss: 4.9588 (4.9888)  loss_scale: 65536.0000 (99803.1895)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0003  max mem: 15572
Epoch: [1]  [2610/2809]  eta: 0:01:56  lr: 0.000018  min_lr: 0.000000  loss: 4.9404 (4.9887)  loss_scale: 65536.0000 (99671.9479)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
Epoch: [1]  [2620/2809]  eta: 0:01:50  lr: 0.000018  min_lr: 0.000000  loss: 4.9255 (4.9884)  loss_scale: 65536.0000 (99541.7077)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [1]  [2630/2809]  eta: 0:01:44  lr: 0.000018  min_lr: 0.000000  loss: 4.8699 (4.9881)  loss_scale: 65536.0000 (99412.4576)  weight_decay: 0.0500 (0.0500)  time: 0.3823  data: 0.0004  max mem: 15572
[2025-01-12 21:21:55,548] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:21:55,549] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2640/2809]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 4.9297 (4.9879)  loss_scale: 65536.0000 (99482.7050)  weight_decay: 0.0500 (0.0500)  time: 0.4048  data: 0.0005  max mem: 15572
[2025-01-12 21:22:01,439] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5456
[2025-01-12 21:22:01,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:22:01,439] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2650/2809]  eta: 0:01:32  lr: 0.000018  min_lr: 0.000000  loss: 4.9297 (4.9876)  loss_scale: 131072.0000 (99502.9800)  weight_decay: 0.0500 (0.0500)  time: 0.4089  data: 0.0006  max mem: 15572
Epoch: [1]  [2660/2809]  eta: 0:01:26  lr: 0.000018  min_lr: 0.000000  loss: 4.9021 (4.9874)  loss_scale: 65536.0000 (99375.3326)  weight_decay: 0.0500 (0.0500)  time: 0.3995  data: 0.0006  max mem: 15572
Epoch: [1]  [2670/2809]  eta: 0:01:21  lr: 0.000018  min_lr: 0.000000  loss: 4.9021 (4.9871)  loss_scale: 65536.0000 (99248.6410)  weight_decay: 0.0500 (0.0500)  time: 0.4408  data: 0.0007  max mem: 15572
Epoch: [1]  [2680/2809]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 4.9349 (4.9870)  loss_scale: 65536.0000 (99122.8944)  weight_decay: 0.0500 (0.0500)  time: 0.4702  data: 0.0010  max mem: 15572
Epoch: [1]  [2690/2809]  eta: 0:01:09  lr: 0.000018  min_lr: 0.000000  loss: 4.9522 (4.9869)  loss_scale: 65536.0000 (98998.0825)  weight_decay: 0.0500 (0.0500)  time: 0.4521  data: 0.0010  max mem: 15572
Epoch: [1]  [2700/2809]  eta: 0:01:03  lr: 0.000018  min_lr: 0.000000  loss: 4.9120 (4.9864)  loss_scale: 65536.0000 (98874.1947)  weight_decay: 0.0500 (0.0500)  time: 0.4298  data: 0.0006  max mem: 15572
Epoch: [1]  [2710/2809]  eta: 0:00:57  lr: 0.000018  min_lr: 0.000000  loss: 4.9120 (4.9862)  loss_scale: 65536.0000 (98751.2210)  weight_decay: 0.0500 (0.0500)  time: 0.4352  data: 0.0007  max mem: 15572
Epoch: [1]  [2720/2809]  eta: 0:00:51  lr: 0.000018  min_lr: 0.000000  loss: 4.8782 (4.9858)  loss_scale: 65536.0000 (98629.1510)  weight_decay: 0.0500 (0.0500)  time: 0.4554  data: 0.0009  max mem: 15572
Epoch: [1]  [2730/2809]  eta: 0:00:45  lr: 0.000018  min_lr: 0.000000  loss: 4.8772 (4.9857)  loss_scale: 65536.0000 (98507.9751)  weight_decay: 0.0500 (0.0500)  time: 0.4528  data: 0.0009  max mem: 15572
Epoch: [1]  [2740/2809]  eta: 0:00:40  lr: 0.000019  min_lr: 0.000000  loss: 4.9237 (4.9855)  loss_scale: 65536.0000 (98387.6833)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0937  max mem: 15572
Epoch: [1]  [2750/2809]  eta: 0:00:34  lr: 0.000019  min_lr: 0.000000  loss: 4.9309 (4.9854)  loss_scale: 65536.0000 (98268.2661)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1653  max mem: 15572
Epoch: [1]  [2760/2809]  eta: 0:00:28  lr: 0.000019  min_lr: 0.000000  loss: 4.8917 (4.9851)  loss_scale: 65536.0000 (98149.7139)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1415  max mem: 15572
Epoch: [1]  [2770/2809]  eta: 0:00:22  lr: 0.000019  min_lr: 0.000000  loss: 4.8863 (4.9849)  loss_scale: 65536.0000 (98032.0173)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.1454  max mem: 15572
[2025-01-12 21:23:06,942] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:23:06,942] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2780/2809]  eta: 0:00:16  lr: 0.000019  min_lr: 0.000000  loss: 4.9066 (4.9845)  loss_scale: 65536.0000 (98032.9953)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1112  max mem: 15572
Epoch: [1]  [2790/2809]  eta: 0:00:11  lr: 0.000019  min_lr: 0.000000  loss: 4.9092 (4.9845)  loss_scale: 131072.0000 (98151.3723)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0913  max mem: 15572
[2025-01-12 21:23:18,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5606
[2025-01-12 21:23:18,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 21:23:18,481] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2800/2809]  eta: 0:00:05  lr: 0.000019  min_lr: 0.000000  loss: 4.9303 (4.9843)  loss_scale: 131072.0000 (98175.3145)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0920  max mem: 15572
Epoch: [1]  [2808/2809]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 4.9376 (4.9843)  loss_scale: 65536.0000 (98082.3581)  weight_decay: 0.0500 (0.0500)  time: 0.4740  data: 0.0616  max mem: 15572
Epoch: [1] Total time: 0:27:09 (0.5803 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 4.9376 (4.9843)  loss_scale: 65536.0000 (98082.3581)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:18:43  loss: 5.0872 (5.0872)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 4.1298  data: 3.9588  max mem: 15572
Val:  [ 10/272]  eta: 0:03:17  loss: 5.2079 (5.0102)  acc1: 0.0000 (2.5253)  acc5: 0.0000 (18.1818)  time: 0.7519  data: 0.5394  max mem: 15572
Val:  [ 20/272]  eta: 0:02:19  loss: 5.0015 (4.9185)  acc1: 0.0000 (3.4392)  acc5: 0.0000 (17.9894)  time: 0.3768  data: 0.1664  max mem: 15572
Val:  [ 30/272]  eta: 0:01:50  loss: 4.9735 (4.8957)  acc1: 0.0000 (2.3297)  acc5: 0.0000 (17.7419)  time: 0.2961  data: 0.0950  max mem: 15572
Val:  [ 40/272]  eta: 0:01:42  loss: 4.5089 (4.7925)  acc1: 0.0000 (6.7751)  acc5: 27.7778 (28.5908)  time: 0.3230  data: 0.1276  max mem: 15572
