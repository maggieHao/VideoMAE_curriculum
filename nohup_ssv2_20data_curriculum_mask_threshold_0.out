/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-15 14:22:09,638] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, mask_curriculum_threshold=0, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f1e305503d0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-15 14:22:12,948] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-15 14:22:12,948] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-15 14:22:12,974] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.06553912162780762 seconds
[2025-01-15 14:22:13,255] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-15 14:22:13,255] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-15 14:22:13,258] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-15 14:22:13,258] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-15 14:22:13,265] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-15 14:22:13,265] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-15 14:22:13,265] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-15 14:22:13,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 14:22:13,265] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1e14eb2e60>
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-15 14:22:13,266] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-15 14:22:13,267] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-15 14:22:13,267] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train curriculum learning mask.
Mask curriculum threshold = 0
Epoch: [0]  [   0/2809]  eta: 8:30:07  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  class_acc: 0.0000 (0.0000)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 10.8963  data: 7.2804  max mem: 15572
Epoch: [0]  [  10/2809]  eta: 1:03:20  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0152)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3579  data: 0.6622  max mem: 15572
Epoch: [0]  [  20/2809]  eta: 0:42:14  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0119)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4096  data: 0.0005  max mem: 15572
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3022794 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3022787 got signal: 1
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-15 14:22:36,187] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, mask_curriculum_threshold=0, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f6da12543d0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-15 14:22:38,956] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-15 14:22:38,956] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-15 14:22:38,979] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.039624691009521484 seconds
[2025-01-15 14:22:39,188] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-15 14:22:39,188] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-15 14:22:39,190] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-15 14:22:39,190] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-15 14:22:39,196] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-15 14:22:39,196] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-15 14:22:39,196] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-15 14:22:39,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 14:22:39,197] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6d84f6ee60>
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-15 14:22:39,197] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-15 14:22:39,198] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-15 14:22:39,198] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train curriculum learning mask.
Mask curriculum threshold = 0
Epoch: [0]  [   0/2809]  eta: 12:36:12  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  class_acc: 0.0000 (0.0000)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 16.1524  data: 6.0230  max mem: 15572
Epoch: [0]  [  10/2809]  eta: 1:25:47  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0152)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.8390  data: 0.5479  max mem: 15572
Epoch: [0]  [  20/2809]  eta: 0:54:42  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0119)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4280  data: 0.0005  max mem: 15572
Epoch: [0]  [  30/2809]  eta: 0:43:45  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0094)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4530  data: 0.0007  max mem: 15572
Epoch: [0]  [  40/2809]  eta: 0:37:54  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  class_acc: 0.0000 (0.0081)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4486  data: 0.0006  max mem: 15572
Epoch: [0]  [  50/2809]  eta: 0:34:39  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  class_acc: 0.0000 (0.0090)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4572  data: 0.0006  max mem: 15572
Epoch: [0]  [  60/2809]  eta: 0:33:30  lr: 0.000000  min_lr: 0.000000  loss: 5.1600 (5.1601)  class_acc: 0.0000 (0.0075)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0891  max mem: 15572
Epoch: [0]  [  70/2809]  eta: 0:32:21  lr: 0.000000  min_lr: 0.000000  loss: 5.1599 (5.1600)  class_acc: 0.0000 (0.0065)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1530  max mem: 15572
Epoch: [0]  [  80/2809]  eta: 0:31:27  lr: 0.000000  min_lr: 0.000000  loss: 5.1598 (5.1600)  class_acc: 0.0000 (0.0062)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1239  max mem: 15572
Epoch: [0]  [  90/2809]  eta: 0:31:09  lr: 0.000000  min_lr: 0.000000  loss: 5.1597 (5.1600)  class_acc: 0.0000 (0.0055)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1486  max mem: 15572
Epoch: [0]  [ 100/2809]  eta: 0:30:39  lr: 0.000000  min_lr: 0.000000  loss: 5.1594 (5.1599)  class_acc: 0.0000 (0.0062)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1533  max mem: 15572
Epoch: [0]  [ 110/2809]  eta: 0:30:01  lr: 0.000000  min_lr: 0.000000  loss: 5.1590 (5.1598)  class_acc: 0.0000 (0.0064)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1210  max mem: 15572
Epoch: [0]  [ 120/2809]  eta: 0:29:20  lr: 0.000000  min_lr: 0.000000  loss: 5.1588 (5.1597)  class_acc: 0.0000 (0.0069)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0936  max mem: 15572
[2025-01-15 14:24:03,339] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:24:03,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/2809]  eta: 0:28:57  lr: 0.000000  min_lr: 0.000000  loss: 5.1587 (5.1596)  class_acc: 0.0000 (0.0076)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0953  max mem: 15572
Epoch: [0]  [ 140/2809]  eta: 0:28:30  lr: 0.000000  min_lr: 0.000000  loss: 5.1583 (5.1595)  class_acc: 0.0000 (0.0074)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1077  max mem: 15572
Epoch: [0]  [ 150/2809]  eta: 0:28:23  lr: 0.000001  min_lr: 0.000000  loss: 5.1583 (5.1595)  class_acc: 0.0000 (0.0077)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1286  max mem: 15572
Epoch: [0]  [ 160/2809]  eta: 0:27:54  lr: 0.000001  min_lr: 0.000000  loss: 5.1584 (5.1594)  class_acc: 0.0000 (0.0072)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0990  max mem: 15572
Epoch: [0]  [ 170/2809]  eta: 0:27:49  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1593)  class_acc: 0.0000 (0.0078)  loss_scale: 256.0000 (160.1871)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1096  max mem: 15572
Epoch: [0]  [ 180/2809]  eta: 0:27:27  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1593)  class_acc: 0.0000 (0.0081)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1275  max mem: 15572
Epoch: [0]  [ 190/2809]  eta: 0:27:06  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  class_acc: 0.0000 (0.0087)  loss_scale: 256.0000 (170.2199)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0754  max mem: 15572
Epoch: [0]  [ 200/2809]  eta: 0:27:04  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  class_acc: 0.0000 (0.0112)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1379  max mem: 15572
Epoch: [0]  [ 210/2809]  eta: 0:26:52  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1591)  class_acc: 0.0000 (0.0107)  loss_scale: 256.0000 (178.3507)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1671  max mem: 15572
Epoch: [0]  [ 220/2809]  eta: 0:26:45  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1591)  class_acc: 0.0000 (0.0109)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1614  max mem: 15572
Epoch: [0]  [ 230/2809]  eta: 0:26:30  lr: 0.000001  min_lr: 0.000000  loss: 5.1577 (5.1590)  class_acc: 0.0000 (0.0110)  loss_scale: 256.0000 (185.0736)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1345  max mem: 15572
Epoch: [0]  [ 240/2809]  eta: 0:26:20  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1589)  class_acc: 0.0000 (0.0111)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0926  max mem: 15572
Epoch: [0]  [ 250/2809]  eta: 0:26:06  lr: 0.000001  min_lr: 0.000000  loss: 5.1574 (5.1589)  class_acc: 0.0000 (0.0121)  loss_scale: 256.0000 (190.7251)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0981  max mem: 15572
[2025-01-15 14:25:16,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:25:16,005] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [ 260/2809]  eta: 0:25:57  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1588)  class_acc: 0.0000 (0.0120)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1232  max mem: 15572
Epoch: [0]  [ 270/2809]  eta: 0:25:59  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1588)  class_acc: 0.0000 (0.0120)  loss_scale: 512.0000 (209.7122)  weight_decay: 0.0500 (0.0500)  time: 0.6406  data: 0.2066  max mem: 15572
Epoch: [0]  [ 280/2809]  eta: 0:25:43  lr: 0.000001  min_lr: 0.000000  loss: 5.1571 (5.1587)  class_acc: 0.0000 (0.0120)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1689  max mem: 15572
Epoch: [0]  [ 290/2809]  eta: 0:25:36  lr: 0.000001  min_lr: 0.000000  loss: 5.1571 (5.1586)  class_acc: 0.0000 (0.0120)  loss_scale: 512.0000 (230.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1213  max mem: 15572
Epoch: [0]  [ 300/2809]  eta: 0:25:29  lr: 0.000001  min_lr: 0.000000  loss: 5.1563 (5.1585)  class_acc: 0.0000 (0.0125)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1405  max mem: 15572
Epoch: [0]  [ 310/2809]  eta: 0:25:21  lr: 0.000001  min_lr: 0.000000  loss: 5.1564 (5.1585)  class_acc: 0.0000 (0.0122)  loss_scale: 512.0000 (248.5916)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1222  max mem: 15572
Epoch: [0]  [ 320/2809]  eta: 0:25:09  lr: 0.000001  min_lr: 0.000000  loss: 5.1562 (5.1584)  class_acc: 0.0000 (0.0129)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1043  max mem: 15572
Epoch: [0]  [ 330/2809]  eta: 0:25:02  lr: 0.000001  min_lr: 0.000000  loss: 5.1557 (5.1583)  class_acc: 0.0000 (0.0132)  loss_scale: 512.0000 (264.5076)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1149  max mem: 15572
Epoch: [0]  [ 340/2809]  eta: 0:24:52  lr: 0.000001  min_lr: 0.000000  loss: 5.1557 (5.1583)  class_acc: 0.0000 (0.0133)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1298  max mem: 15572
Epoch: [0]  [ 350/2809]  eta: 0:24:44  lr: 0.000001  min_lr: 0.000000  loss: 5.1553 (5.1582)  class_acc: 0.0000 (0.0135)  loss_scale: 512.0000 (278.6097)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1266  max mem: 15572
Epoch: [0]  [ 360/2809]  eta: 0:24:37  lr: 0.000001  min_lr: 0.000000  loss: 5.1548 (5.1581)  class_acc: 0.0000 (0.0139)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1367  max mem: 15572
Epoch: [0]  [ 370/2809]  eta: 0:24:29  lr: 0.000001  min_lr: 0.000000  loss: 5.1546 (5.1580)  class_acc: 0.0000 (0.0139)  loss_scale: 512.0000 (291.1914)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1164  max mem: 15572
Epoch: [0]  [ 380/2809]  eta: 0:24:29  lr: 0.000001  min_lr: 0.000000  loss: 5.1540 (5.1579)  class_acc: 0.0000 (0.0139)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1653  max mem: 15572
[2025-01-15 14:26:32,071] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:26:32,071] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [ 390/2809]  eta: 0:24:26  lr: 0.000001  min_lr: 0.000000  loss: 5.1535 (5.1578)  class_acc: 0.0000 (0.0143)  loss_scale: 512.0000 (311.6522)  weight_decay: 0.0500 (0.0500)  time: 0.6725  data: 0.2009  max mem: 15572
Epoch: [0]  [ 400/2809]  eta: 0:24:22  lr: 0.000001  min_lr: 0.000000  loss: 5.1528 (5.1577)  class_acc: 0.0000 (0.0148)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.1854  max mem: 15572
Epoch: [0]  [ 410/2809]  eta: 0:24:14  lr: 0.000001  min_lr: 0.000000  loss: 5.1539 (5.1576)  class_acc: 0.0000 (0.0148)  loss_scale: 1024.0000 (346.3163)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1501  max mem: 15572
Epoch: [0]  [ 420/2809]  eta: 0:24:07  lr: 0.000001  min_lr: 0.000000  loss: 5.1533 (5.1574)  class_acc: 0.0000 (0.0149)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1125  max mem: 15572
Epoch: [0]  [ 430/2809]  eta: 0:23:56  lr: 0.000001  min_lr: 0.000000  loss: 5.1515 (5.1573)  class_acc: 0.0000 (0.0147)  loss_scale: 1024.0000 (377.7633)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0924  max mem: 15572
Epoch: [0]  [ 440/2809]  eta: 0:23:54  lr: 0.000001  min_lr: 0.000000  loss: 5.1523 (5.1572)  class_acc: 0.0000 (0.0148)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1290  max mem: 15572
Epoch: [0]  [ 450/2809]  eta: 0:23:47  lr: 0.000002  min_lr: 0.000000  loss: 5.1521 (5.1571)  class_acc: 0.0000 (0.0150)  loss_scale: 1024.0000 (406.4213)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.1634  max mem: 15572
Epoch: [0]  [ 460/2809]  eta: 0:23:36  lr: 0.000002  min_lr: 0.000000  loss: 5.1498 (5.1569)  class_acc: 0.0000 (0.0150)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.1036  max mem: 15572
Epoch: [0]  [ 470/2809]  eta: 0:23:35  lr: 0.000002  min_lr: 0.000000  loss: 5.1495 (5.1567)  class_acc: 0.0000 (0.0154)  loss_scale: 1024.0000 (432.6454)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1606  max mem: 15572
Epoch: [0]  [ 480/2809]  eta: 0:23:29  lr: 0.000002  min_lr: 0.000000  loss: 5.1487 (5.1566)  class_acc: 0.0000 (0.0152)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.1939  max mem: 15572
Epoch: [0]  [ 490/2809]  eta: 0:23:21  lr: 0.000002  min_lr: 0.000000  loss: 5.1476 (5.1564)  class_acc: 0.0000 (0.0150)  loss_scale: 1024.0000 (456.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1197  max mem: 15572
Epoch: [0]  [ 500/2809]  eta: 0:23:11  lr: 0.000002  min_lr: 0.000000  loss: 5.1460 (5.1562)  class_acc: 0.0000 (0.0150)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0831  max mem: 15572
Epoch: [0]  [ 510/2809]  eta: 0:23:03  lr: 0.000002  min_lr: 0.000000  loss: 5.1458 (5.1560)  class_acc: 0.0000 (0.0149)  loss_scale: 1024.0000 (478.9354)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.1022  max mem: 15572
[2025-01-15 14:27:49,578] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:27:49,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [0]  [ 520/2809]  eta: 0:23:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1448 (5.1557)  class_acc: 0.0000 (0.0150)  loss_scale: 1024.0000 (507.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.2039  max mem: 15572
Epoch: [0]  [ 530/2809]  eta: 0:22:55  lr: 0.000002  min_lr: 0.000000  loss: 5.1448 (5.1555)  class_acc: 0.0000 (0.0148)  loss_scale: 2048.0000 (536.1055)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.1881  max mem: 15572
Epoch: [0]  [ 540/2809]  eta: 0:22:49  lr: 0.000002  min_lr: 0.000000  loss: 5.1446 (5.1553)  class_acc: 0.0000 (0.0149)  loss_scale: 2048.0000 (564.0518)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1152  max mem: 15572
Epoch: [0]  [ 550/2809]  eta: 0:22:46  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1551)  class_acc: 0.0000 (0.0151)  loss_scale: 2048.0000 (590.9837)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.1663  max mem: 15572
Epoch: [0]  [ 560/2809]  eta: 0:22:40  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1549)  class_acc: 0.0000 (0.0149)  loss_scale: 2048.0000 (616.9554)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.1706  max mem: 15572
Epoch: [0]  [ 570/2809]  eta: 0:22:33  lr: 0.000002  min_lr: 0.000000  loss: 5.1401 (5.1547)  class_acc: 0.0000 (0.0151)  loss_scale: 2048.0000 (642.0175)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1269  max mem: 15572
Epoch: [0]  [ 580/2809]  eta: 0:22:27  lr: 0.000002  min_lr: 0.000000  loss: 5.1412 (5.1545)  class_acc: 0.0000 (0.0150)  loss_scale: 2048.0000 (666.2169)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1157  max mem: 15572
Epoch: [0]  [ 590/2809]  eta: 0:22:20  lr: 0.000002  min_lr: 0.000000  loss: 5.1408 (5.1543)  class_acc: 0.0000 (0.0150)  loss_scale: 2048.0000 (689.5973)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0952  max mem: 15572
Epoch: [0]  [ 600/2809]  eta: 0:22:09  lr: 0.000002  min_lr: 0.000000  loss: 5.1378 (5.1540)  class_acc: 0.0000 (0.0149)  loss_scale: 2048.0000 (712.1997)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0381  max mem: 15572
Epoch: [0]  [ 610/2809]  eta: 0:21:59  lr: 0.000002  min_lr: 0.000000  loss: 5.1377 (5.1537)  class_acc: 0.0000 (0.0147)  loss_scale: 2048.0000 (734.0622)  weight_decay: 0.0500 (0.0500)  time: 0.4677  data: 0.0096  max mem: 15572
Epoch: [0]  [ 620/2809]  eta: 0:21:55  lr: 0.000002  min_lr: 0.000000  loss: 5.1336 (5.1534)  class_acc: 0.0000 (0.0149)  loss_scale: 2048.0000 (755.2206)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1039  max mem: 15572
Epoch: [0]  [ 630/2809]  eta: 0:21:49  lr: 0.000002  min_lr: 0.000000  loss: 5.1332 (5.1532)  class_acc: 0.0000 (0.0149)  loss_scale: 2048.0000 (775.7084)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.1647  max mem: 15572
[2025-01-15 14:29:06,293] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:29:06,294] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 640/2809]  eta: 0:21:48  lr: 0.000002  min_lr: 0.000000  loss: 5.1386 (5.1530)  class_acc: 0.0000 (0.0148)  loss_scale: 2048.0000 (798.7520)  weight_decay: 0.0500 (0.0500)  time: 0.6688  data: 0.2035  max mem: 15572
Epoch: [0]  [ 650/2809]  eta: 0:21:40  lr: 0.000002  min_lr: 0.000000  loss: 5.1342 (5.1527)  class_acc: 0.0000 (0.0150)  loss_scale: 4096.0000 (849.4009)  weight_decay: 0.0500 (0.0500)  time: 0.6475  data: 0.1894  max mem: 15572
Epoch: [0]  [ 660/2809]  eta: 0:21:34  lr: 0.000002  min_lr: 0.000000  loss: 5.1293 (5.1523)  class_acc: 0.0000 (0.0151)  loss_scale: 4096.0000 (898.5174)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1183  max mem: 15572
Epoch: [0]  [ 670/2809]  eta: 0:21:26  lr: 0.000002  min_lr: 0.000000  loss: 5.1294 (5.1520)  class_acc: 0.0000 (0.0153)  loss_scale: 4096.0000 (946.1699)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1116  max mem: 15572
Epoch: [0]  [ 680/2809]  eta: 0:21:21  lr: 0.000002  min_lr: 0.000000  loss: 5.1236 (5.1516)  class_acc: 0.0000 (0.0154)  loss_scale: 4096.0000 (992.4229)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1216  max mem: 15572
Epoch: [0]  [ 690/2809]  eta: 0:21:12  lr: 0.000002  min_lr: 0.000000  loss: 5.1250 (5.1513)  class_acc: 0.0000 (0.0151)  loss_scale: 4096.0000 (1037.3372)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0815  max mem: 15572
Epoch: [0]  [ 700/2809]  eta: 0:21:05  lr: 0.000002  min_lr: 0.000000  loss: 5.1352 (5.1511)  class_acc: 0.0000 (0.0150)  loss_scale: 4096.0000 (1080.9700)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0620  max mem: 15572
Epoch: [0]  [ 710/2809]  eta: 0:20:57  lr: 0.000002  min_lr: 0.000000  loss: 5.1336 (5.1507)  class_acc: 0.0000 (0.0150)  loss_scale: 4096.0000 (1123.3755)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0533  max mem: 15572
Epoch: [0]  [ 720/2809]  eta: 0:20:50  lr: 0.000002  min_lr: 0.000000  loss: 5.1255 (5.1504)  class_acc: 0.0000 (0.0154)  loss_scale: 4096.0000 (1164.6047)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0430  max mem: 15572
Epoch: [0]  [ 730/2809]  eta: 0:20:44  lr: 0.000002  min_lr: 0.000000  loss: 5.1248 (5.1500)  class_acc: 0.0000 (0.0156)  loss_scale: 4096.0000 (1204.7059)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1383  max mem: 15572
Epoch: [0]  [ 740/2809]  eta: 0:20:38  lr: 0.000002  min_lr: 0.000000  loss: 5.1248 (5.1497)  class_acc: 0.0000 (0.0154)  loss_scale: 4096.0000 (1243.7247)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1761  max mem: 15572
Epoch: [0]  [ 750/2809]  eta: 0:20:31  lr: 0.000003  min_lr: 0.000000  loss: 5.1158 (5.1492)  class_acc: 0.0000 (0.0153)  loss_scale: 4096.0000 (1281.7044)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1421  max mem: 15572
Epoch: [0]  [ 760/2809]  eta: 0:20:23  lr: 0.000003  min_lr: 0.000000  loss: 5.1160 (5.1489)  class_acc: 0.0000 (0.0153)  loss_scale: 4096.0000 (1318.6859)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0947  max mem: 15572
[2025-01-15 14:30:19,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:30:19,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [0]  [ 770/2809]  eta: 0:20:18  lr: 0.000003  min_lr: 0.000000  loss: 5.1200 (5.1485)  class_acc: 0.0000 (0.0156)  loss_scale: 4096.0000 (1370.6459)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1176  max mem: 15572
Epoch: [0]  [ 780/2809]  eta: 0:20:15  lr: 0.000003  min_lr: 0.000000  loss: 5.1198 (5.1481)  class_acc: 0.0000 (0.0155)  loss_scale: 8192.0000 (1457.9872)  weight_decay: 0.0500 (0.0500)  time: 0.6676  data: 0.1801  max mem: 15572
Epoch: [0]  [ 790/2809]  eta: 0:20:08  lr: 0.000003  min_lr: 0.000000  loss: 5.1174 (5.1478)  class_acc: 0.0000 (0.0155)  loss_scale: 8192.0000 (1543.1201)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1448  max mem: 15572
Epoch: [0]  [ 800/2809]  eta: 0:20:03  lr: 0.000003  min_lr: 0.000000  loss: 5.1111 (5.1473)  class_acc: 0.0000 (0.0154)  loss_scale: 8192.0000 (1626.1273)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1508  max mem: 15572
Epoch: [0]  [ 810/2809]  eta: 0:19:54  lr: 0.000003  min_lr: 0.000000  loss: 5.1104 (5.1469)  class_acc: 0.0000 (0.0154)  loss_scale: 8192.0000 (1707.0875)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1274  max mem: 15572
Epoch: [0]  [ 820/2809]  eta: 0:19:48  lr: 0.000003  min_lr: 0.000000  loss: 5.1177 (5.1466)  class_acc: 0.0000 (0.0153)  loss_scale: 8192.0000 (1786.0755)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0815  max mem: 15572
Epoch: [0]  [ 830/2809]  eta: 0:19:42  lr: 0.000003  min_lr: 0.000000  loss: 5.1069 (5.1461)  class_acc: 0.0000 (0.0152)  loss_scale: 8192.0000 (1863.1625)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1460  max mem: 15572
Epoch: [0]  [ 840/2809]  eta: 0:19:37  lr: 0.000003  min_lr: 0.000000  loss: 5.1029 (5.1457)  class_acc: 0.0000 (0.0153)  loss_scale: 8192.0000 (1938.4162)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1935  max mem: 15572
Epoch: [0]  [ 850/2809]  eta: 0:19:31  lr: 0.000003  min_lr: 0.000000  loss: 5.1073 (5.1453)  class_acc: 0.0000 (0.0156)  loss_scale: 8192.0000 (2011.9013)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1594  max mem: 15572
Epoch: [0]  [ 860/2809]  eta: 0:19:22  lr: 0.000003  min_lr: 0.000000  loss: 5.1060 (5.1449)  class_acc: 0.0000 (0.0155)  loss_scale: 8192.0000 (2083.6794)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0727  max mem: 15572
Epoch: [0]  [ 870/2809]  eta: 0:19:17  lr: 0.000003  min_lr: 0.000000  loss: 5.1059 (5.1446)  class_acc: 0.0000 (0.0156)  loss_scale: 8192.0000 (2153.8094)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1065  max mem: 15572
Epoch: [0]  [ 880/2809]  eta: 0:19:13  lr: 0.000003  min_lr: 0.000000  loss: 5.1020 (5.1441)  class_acc: 0.0000 (0.0162)  loss_scale: 8192.0000 (2222.3473)  weight_decay: 0.0500 (0.0500)  time: 0.6520  data: 0.2092  max mem: 15572
Epoch: [0]  [ 890/2809]  eta: 0:19:04  lr: 0.000003  min_lr: 0.000000  loss: 5.1010 (5.1438)  class_acc: 0.0000 (0.0164)  loss_scale: 8192.0000 (2289.3468)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1450  max mem: 15572
[2025-01-15 14:31:36,032] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:31:36,032] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [0]  [ 900/2809]  eta: 0:18:59  lr: 0.000003  min_lr: 0.000000  loss: 5.1123 (5.1435)  class_acc: 0.0000 (0.0166)  loss_scale: 8192.0000 (2400.3196)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1170  max mem: 15572
Epoch: [0]  [ 910/2809]  eta: 0:18:53  lr: 0.000003  min_lr: 0.000000  loss: 5.1084 (5.1431)  class_acc: 0.0000 (0.0165)  loss_scale: 16384.0000 (2553.8178)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1673  max mem: 15572
Epoch: [0]  [ 920/2809]  eta: 0:18:49  lr: 0.000003  min_lr: 0.000000  loss: 5.1062 (5.1427)  class_acc: 0.0000 (0.0164)  loss_scale: 16384.0000 (2703.9826)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.2012  max mem: 15572
Epoch: [0]  [ 930/2809]  eta: 0:18:43  lr: 0.000003  min_lr: 0.000000  loss: 5.1084 (5.1424)  class_acc: 0.0000 (0.0163)  loss_scale: 16384.0000 (2850.9216)  weight_decay: 0.0500 (0.0500)  time: 0.6488  data: 0.2018  max mem: 15572
Epoch: [0]  [ 940/2809]  eta: 0:18:39  lr: 0.000003  min_lr: 0.000000  loss: 5.1050 (5.1420)  class_acc: 0.0000 (0.0163)  loss_scale: 16384.0000 (2994.7375)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.1980  max mem: 15572
Epoch: [0]  [ 950/2809]  eta: 0:18:32  lr: 0.000003  min_lr: 0.000000  loss: 5.1058 (5.1417)  class_acc: 0.0000 (0.0163)  loss_scale: 16384.0000 (3135.5289)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1719  max mem: 15572
Epoch: [0]  [ 960/2809]  eta: 0:18:26  lr: 0.000003  min_lr: 0.000000  loss: 5.1060 (5.1412)  class_acc: 0.0000 (0.0163)  loss_scale: 16384.0000 (3273.3902)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1131  max mem: 15572
Epoch: [0]  [ 970/2809]  eta: 0:18:20  lr: 0.000003  min_lr: 0.000000  loss: 5.1060 (5.1410)  class_acc: 0.0000 (0.0165)  loss_scale: 16384.0000 (3408.4119)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1317  max mem: 15572
Epoch: [0]  [ 980/2809]  eta: 0:18:13  lr: 0.000003  min_lr: 0.000000  loss: 5.1072 (5.1405)  class_acc: 0.0000 (0.0166)  loss_scale: 16384.0000 (3540.6809)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1516  max mem: 15572
Epoch: [0]  [ 990/2809]  eta: 0:18:08  lr: 0.000003  min_lr: 0.000000  loss: 5.0871 (5.1399)  class_acc: 0.0000 (0.0166)  loss_scale: 16384.0000 (3670.2805)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1740  max mem: 15572
[2025-01-15 14:32:38,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[3.2306541515702744e-08, 3.2306541515702744e-08, 4.615220216528964e-08, 4.615220216528964e-08, 6.59317173789852e-08, 6.59317173789852e-08, 9.418816768426457e-08, 9.418816768426457e-08, 1.3455452526323513e-07, 1.3455452526323513e-07, 1.9222075037605018e-07, 1.9222075037605018e-07, 2.7460107196578597e-07, 2.7460107196578597e-07, 3.922872456654086e-07, 3.922872456654086e-07, 5.604103509505837e-07, 5.604103509505837e-07, 8.005862156436911e-07, 8.005862156436911e-07, 1.1436945937767016e-06, 1.1436945937767016e-06, 1.6338494196810024e-06, 1.6338494196810024e-06, 2.3340705995442894e-06, 2.3340705995442894e-06, 3.3343865707775563e-06, 3.3343865707775563e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 14:32:38,183] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=27.869532182237656, CurrSamplesPerSec=27.35220916336935, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [1000/2809]  eta: 0:18:01  lr: 0.000003  min_lr: 0.000000  loss: 5.0871 (5.1394)  class_acc: 0.0000 (0.0165)  loss_scale: 16384.0000 (3797.2907)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1601  max mem: 15572
Epoch: [0]  [1010/2809]  eta: 0:17:56  lr: 0.000003  min_lr: 0.000000  loss: 5.0946 (5.1391)  class_acc: 0.0000 (0.0164)  loss_scale: 16384.0000 (3921.7883)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1404  max mem: 15572
Epoch: [0]  [1020/2809]  eta: 0:17:49  lr: 0.000003  min_lr: 0.000000  loss: 5.0967 (5.1387)  class_acc: 0.0000 (0.0167)  loss_scale: 16384.0000 (4043.8472)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1122  max mem: 15572
[2025-01-15 14:32:52,355] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:32:52,355] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [0]  [1030/2809]  eta: 0:17:43  lr: 0.000003  min_lr: 0.000000  loss: 5.0918 (5.1383)  class_acc: 0.0000 (0.0168)  loss_scale: 16384.0000 (4274.7779)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.0925  max mem: 15572
Epoch: [0]  [1040/2809]  eta: 0:17:36  lr: 0.000003  min_lr: 0.000000  loss: 5.1019 (5.1380)  class_acc: 0.0000 (0.0168)  loss_scale: 32768.0000 (4548.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0837  max mem: 15572
Epoch: [0]  [1050/2809]  eta: 0:17:30  lr: 0.000004  min_lr: 0.000000  loss: 5.1019 (5.1377)  class_acc: 0.0000 (0.0167)  loss_scale: 32768.0000 (4816.9895)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1187  max mem: 15572
Epoch: [0]  [1060/2809]  eta: 0:17:25  lr: 0.000004  min_lr: 0.000000  loss: 5.0847 (5.1372)  class_acc: 0.0000 (0.0166)  loss_scale: 32768.0000 (5080.4298)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1636  max mem: 15572
Epoch: [0]  [1070/2809]  eta: 0:17:19  lr: 0.000004  min_lr: 0.000000  loss: 5.0797 (5.1367)  class_acc: 0.0000 (0.0166)  loss_scale: 32768.0000 (5338.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1543  max mem: 15572
Epoch: [0]  [1080/2809]  eta: 0:17:14  lr: 0.000004  min_lr: 0.000000  loss: 5.0797 (5.1362)  class_acc: 0.0000 (0.0167)  loss_scale: 32768.0000 (5592.6883)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1716  max mem: 15572
Epoch: [0]  [1090/2809]  eta: 0:17:08  lr: 0.000004  min_lr: 0.000000  loss: 5.0779 (5.1357)  class_acc: 0.0000 (0.0165)  loss_scale: 32768.0000 (5841.7745)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1541  max mem: 15572
Epoch: [0]  [1100/2809]  eta: 0:17:01  lr: 0.000004  min_lr: 0.000000  loss: 5.1011 (5.1355)  class_acc: 0.0000 (0.0167)  loss_scale: 32768.0000 (6086.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1143  max mem: 15572
Epoch: [0]  [1110/2809]  eta: 0:16:55  lr: 0.000004  min_lr: 0.000000  loss: 5.0970 (5.1349)  class_acc: 0.0000 (0.0166)  loss_scale: 32768.0000 (6326.4950)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1105  max mem: 15572
Epoch: [0]  [1120/2809]  eta: 0:16:49  lr: 0.000004  min_lr: 0.000000  loss: 5.0733 (5.1345)  class_acc: 0.0000 (0.0166)  loss_scale: 32768.0000 (6562.3693)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1201  max mem: 15572
Epoch: [0]  [1130/2809]  eta: 0:16:42  lr: 0.000004  min_lr: 0.000000  loss: 5.0808 (5.1340)  class_acc: 0.0000 (0.0167)  loss_scale: 32768.0000 (6794.0725)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0771  max mem: 15572
Epoch: [0]  [1140/2809]  eta: 0:16:36  lr: 0.000004  min_lr: 0.000000  loss: 5.0878 (5.1336)  class_acc: 0.0000 (0.0168)  loss_scale: 32768.0000 (7021.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0703  max mem: 15572
Epoch: [0]  [1150/2809]  eta: 0:16:29  lr: 0.000004  min_lr: 0.000000  loss: 5.0933 (5.1332)  class_acc: 0.0000 (0.0168)  loss_scale: 32768.0000 (7245.4005)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0884  max mem: 15572
[2025-01-15 14:34:07,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:34:07,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [0]  [1160/2809]  eta: 0:16:22  lr: 0.000004  min_lr: 0.000000  loss: 5.0942 (5.1329)  class_acc: 0.0000 (0.0168)  loss_scale: 32768.0000 (7719.2489)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0639  max mem: 15572
Epoch: [0]  [1170/2809]  eta: 0:16:16  lr: 0.000004  min_lr: 0.000000  loss: 5.0790 (5.1323)  class_acc: 0.0000 (0.0170)  loss_scale: 65536.0000 (8212.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1261  max mem: 15572
Epoch: [0]  [1180/2809]  eta: 0:16:09  lr: 0.000004  min_lr: 0.000000  loss: 5.0785 (5.1320)  class_acc: 0.0000 (0.0171)  loss_scale: 65536.0000 (8698.3641)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1115  max mem: 15572
Epoch: [0]  [1190/2809]  eta: 0:16:04  lr: 0.000004  min_lr: 0.000000  loss: 5.0943 (5.1315)  class_acc: 0.0000 (0.0171)  loss_scale: 65536.0000 (9175.5903)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1007  max mem: 15572
Epoch: [0]  [1200/2809]  eta: 0:15:58  lr: 0.000004  min_lr: 0.000000  loss: 5.0858 (5.1312)  class_acc: 0.0000 (0.0169)  loss_scale: 65536.0000 (9644.8693)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1606  max mem: 15572
Epoch: [0]  [1210/2809]  eta: 0:15:51  lr: 0.000004  min_lr: 0.000000  loss: 5.0906 (5.1308)  class_acc: 0.0000 (0.0169)  loss_scale: 65536.0000 (10106.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1130  max mem: 15572
Epoch: [0]  [1220/2809]  eta: 0:15:45  lr: 0.000004  min_lr: 0.000000  loss: 5.0849 (5.1303)  class_acc: 0.0000 (0.0168)  loss_scale: 65536.0000 (10560.3669)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0921  max mem: 15572
Epoch: [0]  [1230/2809]  eta: 0:15:39  lr: 0.000004  min_lr: 0.000000  loss: 5.0784 (5.1300)  class_acc: 0.0000 (0.0167)  loss_scale: 65536.0000 (11006.9602)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1297  max mem: 15572
Epoch: [0]  [1240/2809]  eta: 0:15:34  lr: 0.000004  min_lr: 0.000000  loss: 5.0918 (5.1297)  class_acc: 0.0000 (0.0167)  loss_scale: 65536.0000 (11446.3562)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1763  max mem: 15572
Epoch: [0]  [1250/2809]  eta: 0:15:28  lr: 0.000004  min_lr: 0.000000  loss: 5.0865 (5.1293)  class_acc: 0.0000 (0.0166)  loss_scale: 65536.0000 (11878.7274)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1589  max mem: 15572
Epoch: [0]  [1260/2809]  eta: 0:15:22  lr: 0.000004  min_lr: 0.000000  loss: 5.0849 (5.1291)  class_acc: 0.0000 (0.0166)  loss_scale: 65536.0000 (12304.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1304  max mem: 15572
Epoch: [0]  [1270/2809]  eta: 0:15:16  lr: 0.000004  min_lr: 0.000000  loss: 5.0753 (5.1286)  class_acc: 0.0000 (0.0165)  loss_scale: 65536.0000 (12723.0590)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1296  max mem: 15572
[2025-01-15 14:35:22,196] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:35:22,197] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [0]  [1280/2809]  eta: 0:15:09  lr: 0.000004  min_lr: 0.000000  loss: 5.0753 (5.1284)  class_acc: 0.0000 (0.0166)  loss_scale: 65536.0000 (13186.4980)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0956  max mem: 15572
Epoch: [0]  [1290/2809]  eta: 0:15:03  lr: 0.000004  min_lr: 0.000000  loss: 5.1068 (5.1282)  class_acc: 0.0000 (0.0167)  loss_scale: 131072.0000 (14099.6313)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0948  max mem: 15572
Epoch: [0]  [1300/2809]  eta: 0:14:57  lr: 0.000004  min_lr: 0.000000  loss: 5.0773 (5.1277)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (14998.7271)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1218  max mem: 15572
Epoch: [0]  [1310/2809]  eta: 0:14:51  lr: 0.000004  min_lr: 0.000000  loss: 5.0499 (5.1271)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (15884.1068)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1471  max mem: 15572
Epoch: [0]  [1320/2809]  eta: 0:14:46  lr: 0.000004  min_lr: 0.000000  loss: 5.0510 (5.1267)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (16756.0818)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1779  max mem: 15572
Epoch: [0]  [1330/2809]  eta: 0:14:41  lr: 0.000004  min_lr: 0.000000  loss: 5.0631 (5.1263)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (17614.9542)  weight_decay: 0.0500 (0.0500)  time: 0.6629  data: 0.2027  max mem: 15572
Epoch: [0]  [1340/2809]  eta: 0:14:34  lr: 0.000004  min_lr: 0.000000  loss: 5.0631 (5.1259)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (18461.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1371  max mem: 15572
Epoch: [0]  [1350/2809]  eta: 0:14:27  lr: 0.000005  min_lr: 0.000000  loss: 5.0743 (5.1255)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (19294.5551)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.0822  max mem: 15572
Epoch: [0]  [1360/2809]  eta: 0:14:20  lr: 0.000005  min_lr: 0.000000  loss: 5.0736 (5.1251)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (20115.8442)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0803  max mem: 15572
Epoch: [0]  [1370/2809]  eta: 0:14:14  lr: 0.000005  min_lr: 0.000000  loss: 5.0636 (5.1246)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (20925.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0790  max mem: 15572
Epoch: [0]  [1380/2809]  eta: 0:14:07  lr: 0.000005  min_lr: 0.000000  loss: 5.0508 (5.1241)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (21722.7400)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1101  max mem: 15572
Epoch: [0]  [1390/2809]  eta: 0:14:02  lr: 0.000005  min_lr: 0.000000  loss: 5.0567 (5.1237)  class_acc: 0.0000 (0.0167)  loss_scale: 131072.0000 (22508.8598)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1840  max mem: 15572
Epoch: [0]  [1400/2809]  eta: 0:13:56  lr: 0.000005  min_lr: 0.000000  loss: 5.0684 (5.1234)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (23283.7573)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.2039  max mem: 15572
[2025-01-15 14:36:37,403] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:36:37,404] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
Epoch: [0]  [1410/2809]  eta: 0:13:51  lr: 0.000005  min_lr: 0.000000  loss: 5.0707 (5.1230)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (24326.3501)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1509  max mem: 15572
Epoch: [0]  [1420/2809]  eta: 0:13:45  lr: 0.000005  min_lr: 0.000000  loss: 5.0748 (5.1226)  class_acc: 0.0000 (0.0166)  loss_scale: 262144.0000 (25999.9437)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1131  max mem: 15572
[2025-01-15 14:36:49,785] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1430
[2025-01-15 14:36:49,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
[2025-01-15 14:36:49,785] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144, reducing to 131072.0
Epoch: [0]  [1430/2809]  eta: 0:13:38  lr: 0.000005  min_lr: 0.000000  loss: 5.0665 (5.1220)  class_acc: 0.0000 (0.0166)  loss_scale: 262144.0000 (27558.5521)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0686  max mem: 15572
Epoch: [0]  [1440/2809]  eta: 0:13:33  lr: 0.000005  min_lr: 0.000000  loss: 5.0691 (5.1219)  class_acc: 0.0000 (0.0167)  loss_scale: 131072.0000 (28276.8966)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.0925  max mem: 15572
Epoch: [0]  [1450/2809]  eta: 0:13:26  lr: 0.000005  min_lr: 0.000000  loss: 5.0655 (5.1215)  class_acc: 0.0000 (0.0167)  loss_scale: 131072.0000 (28985.3398)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0985  max mem: 15572
Epoch: [0]  [1460/2809]  eta: 0:13:22  lr: 0.000005  min_lr: 0.000000  loss: 5.0614 (5.1210)  class_acc: 0.0000 (0.0167)  loss_scale: 131072.0000 (29684.0849)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.1374  max mem: 15572
Epoch: [0]  [1470/2809]  eta: 0:13:15  lr: 0.000005  min_lr: 0.000000  loss: 5.0787 (5.1208)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (30373.3297)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1079  max mem: 15572
Epoch: [0]  [1480/2809]  eta: 0:13:09  lr: 0.000005  min_lr: 0.000000  loss: 5.0530 (5.1204)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (31053.2667)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0793  max mem: 15572
Epoch: [0]  [1490/2809]  eta: 0:13:03  lr: 0.000005  min_lr: 0.000000  loss: 5.0665 (5.1201)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (31724.0832)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1234  max mem: 15572
Epoch: [0]  [1500/2809]  eta: 0:12:57  lr: 0.000005  min_lr: 0.000000  loss: 5.0822 (5.1198)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (32385.9614)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0920  max mem: 15572
Epoch: [0]  [1510/2809]  eta: 0:12:51  lr: 0.000005  min_lr: 0.000000  loss: 5.0609 (5.1194)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (33039.0788)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1353  max mem: 15572
Epoch: [0]  [1520/2809]  eta: 0:12:45  lr: 0.000005  min_lr: 0.000000  loss: 5.0577 (5.1189)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (33683.6082)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1540  max mem: 15572
Epoch: [0]  [1530/2809]  eta: 0:12:39  lr: 0.000005  min_lr: 0.000000  loss: 5.0544 (5.1187)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (34319.7178)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1483  max mem: 15572
Epoch: [0]  [1540/2809]  eta: 0:12:33  lr: 0.000005  min_lr: 0.000000  loss: 5.0452 (5.1182)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (34947.5717)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1416  max mem: 15572
Epoch: [0]  [1550/2809]  eta: 0:12:27  lr: 0.000005  min_lr: 0.000000  loss: 5.0521 (5.1179)  class_acc: 0.0000 (0.0168)  loss_scale: 131072.0000 (35567.3295)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1497  max mem: 15572
[2025-01-15 14:38:05,797] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:38:05,797] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1560/2809]  eta: 0:12:21  lr: 0.000005  min_lr: 0.000000  loss: 5.0700 (5.1176)  class_acc: 0.0000 (0.0167)  loss_scale: 131072.0000 (36347.0801)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1406  max mem: 15572
Epoch: [0]  [1570/2809]  eta: 0:12:15  lr: 0.000005  min_lr: 0.000000  loss: 5.0549 (5.1173)  class_acc: 0.0000 (0.0167)  loss_scale: 262144.0000 (37784.3616)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1022  max mem: 15572
Epoch: [0]  [1580/2809]  eta: 0:12:09  lr: 0.000005  min_lr: 0.000000  loss: 5.0298 (5.1167)  class_acc: 0.0000 (0.0166)  loss_scale: 262144.0000 (39203.4611)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1340  max mem: 15572
Epoch: [0]  [1590/2809]  eta: 0:12:03  lr: 0.000005  min_lr: 0.000000  loss: 5.0277 (5.1161)  class_acc: 0.0000 (0.0166)  loss_scale: 262144.0000 (40604.7216)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0825  max mem: 15572
Epoch: [0]  [1600/2809]  eta: 0:11:57  lr: 0.000005  min_lr: 0.000000  loss: 5.0462 (5.1157)  class_acc: 0.0000 (0.0168)  loss_scale: 262144.0000 (41988.4772)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0833  max mem: 15572
Epoch: [0]  [1610/2809]  eta: 0:11:51  lr: 0.000005  min_lr: 0.000000  loss: 5.0458 (5.1153)  class_acc: 0.0000 (0.0167)  loss_scale: 262144.0000 (43355.0540)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1346  max mem: 15572
Epoch: [0]  [1620/2809]  eta: 0:11:45  lr: 0.000005  min_lr: 0.000000  loss: 5.0514 (5.1149)  class_acc: 0.0000 (0.0167)  loss_scale: 262144.0000 (44704.7699)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1176  max mem: 15572
Epoch: [0]  [1630/2809]  eta: 0:11:38  lr: 0.000005  min_lr: 0.000000  loss: 5.0308 (5.1145)  class_acc: 0.0000 (0.0166)  loss_scale: 262144.0000 (46037.9350)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.0897  max mem: 15572
Epoch: [0]  [1640/2809]  eta: 0:11:32  lr: 0.000005  min_lr: 0.000000  loss: 5.0455 (5.1141)  class_acc: 0.0000 (0.0166)  loss_scale: 262144.0000 (47354.8519)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.1006  max mem: 15572
Epoch: [0]  [1650/2809]  eta: 0:11:27  lr: 0.000006  min_lr: 0.000000  loss: 5.0626 (5.1138)  class_acc: 0.0000 (0.0166)  loss_scale: 262144.0000 (48655.8159)  weight_decay: 0.0500 (0.0500)  time: 0.6456  data: 0.2090  max mem: 15572
[2025-01-15 14:39:03,060] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1656
[2025-01-15 14:39:03,061] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:39:03,061] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1660/2809]  eta: 0:11:21  lr: 0.000006  min_lr: 0.000000  loss: 5.0280 (5.1132)  class_acc: 0.0000 (0.0167)  loss_scale: 262144.0000 (49546.5575)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1588  max mem: 15572
Epoch: [0]  [1670/2809]  eta: 0:11:14  lr: 0.000006  min_lr: 0.000000  loss: 5.0205 (5.1128)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (50034.4417)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0425  max mem: 15572
Epoch: [0]  [1680/2809]  eta: 0:11:08  lr: 0.000006  min_lr: 0.000000  loss: 5.0664 (5.1125)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (50516.5211)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0579  max mem: 15572
Epoch: [0]  [1690/2809]  eta: 0:11:02  lr: 0.000006  min_lr: 0.000000  loss: 5.0667 (5.1123)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (50992.8989)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1105  max mem: 15572
Epoch: [0]  [1700/2809]  eta: 0:10:56  lr: 0.000006  min_lr: 0.000000  loss: 5.0720 (5.1121)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (51463.6755)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1504  max mem: 15572
Epoch: [0]  [1710/2809]  eta: 0:10:50  lr: 0.000006  min_lr: 0.000000  loss: 5.0639 (5.1117)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (51928.9492)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1659  max mem: 15572
Epoch: [0]  [1720/2809]  eta: 0:10:44  lr: 0.000006  min_lr: 0.000000  loss: 5.0317 (5.1112)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (52388.8158)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1519  max mem: 15572
Epoch: [0]  [1730/2809]  eta: 0:10:38  lr: 0.000006  min_lr: 0.000000  loss: 5.0317 (5.1109)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (52843.3692)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1426  max mem: 15572
Epoch: [0]  [1740/2809]  eta: 0:10:32  lr: 0.000006  min_lr: 0.000000  loss: 5.0318 (5.1104)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (53292.7007)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1333  max mem: 15572
Epoch: [0]  [1750/2809]  eta: 0:10:26  lr: 0.000006  min_lr: 0.000000  loss: 5.0231 (5.1099)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (53736.9001)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1257  max mem: 15572
Epoch: [0]  [1760/2809]  eta: 0:10:20  lr: 0.000006  min_lr: 0.000000  loss: 5.0386 (5.1096)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (54176.0545)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1020  max mem: 15572
Epoch: [0]  [1770/2809]  eta: 0:10:14  lr: 0.000006  min_lr: 0.000000  loss: 5.0386 (5.1091)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (54610.2496)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0911  max mem: 15572
Epoch: [0]  [1780/2809]  eta: 0:10:08  lr: 0.000006  min_lr: 0.000000  loss: 5.0477 (5.1088)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (55039.5688)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1365  max mem: 15572
[2025-01-15 14:40:17,195] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:40:17,195] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1790/2809]  eta: 0:10:02  lr: 0.000006  min_lr: 0.000000  loss: 5.0745 (5.1085)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (55903.1960)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1418  max mem: 15572
[2025-01-15 14:40:21,675] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1792
[2025-01-15 14:40:21,676] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:40:21,676] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1800/2809]  eta: 0:09:56  lr: 0.000006  min_lr: 0.000000  loss: 5.0745 (5.1083)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (56393.3459)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1233  max mem: 15572
Epoch: [0]  [1810/2809]  eta: 0:09:50  lr: 0.000006  min_lr: 0.000000  loss: 5.0198 (5.1078)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (56805.7073)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1417  max mem: 15572
Epoch: [0]  [1820/2809]  eta: 0:09:45  lr: 0.000006  min_lr: 0.000000  loss: 5.0479 (5.1077)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (57213.5398)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.1974  max mem: 15572
Epoch: [0]  [1830/2809]  eta: 0:09:39  lr: 0.000006  min_lr: 0.000000  loss: 5.0763 (5.1075)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (57616.9175)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.1703  max mem: 15572
Epoch: [0]  [1840/2809]  eta: 0:09:33  lr: 0.000006  min_lr: 0.000000  loss: 5.0624 (5.1072)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (58015.9131)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1272  max mem: 15572
Epoch: [0]  [1850/2809]  eta: 0:09:27  lr: 0.000006  min_lr: 0.000000  loss: 5.0424 (5.1069)  class_acc: 0.0000 (0.0163)  loss_scale: 131072.0000 (58410.5975)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1657  max mem: 15572
Epoch: [0]  [1860/2809]  eta: 0:09:21  lr: 0.000006  min_lr: 0.000000  loss: 5.0252 (5.1065)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (58801.0403)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1563  max mem: 15572
Epoch: [0]  [1870/2809]  eta: 0:09:15  lr: 0.000006  min_lr: 0.000000  loss: 5.0339 (5.1061)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (59187.3095)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1112  max mem: 15572
Epoch: [0]  [1880/2809]  eta: 0:09:09  lr: 0.000006  min_lr: 0.000000  loss: 5.0465 (5.1058)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (59569.4716)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0999  max mem: 15572
Epoch: [0]  [1890/2809]  eta: 0:09:04  lr: 0.000006  min_lr: 0.000000  loss: 5.0285 (5.1054)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (59947.5918)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1441  max mem: 15572
Epoch: [0]  [1900/2809]  eta: 0:08:58  lr: 0.000006  min_lr: 0.000000  loss: 5.0527 (5.1053)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (60321.7338)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.1548  max mem: 15572
Epoch: [0]  [1910/2809]  eta: 0:08:52  lr: 0.000006  min_lr: 0.000000  loss: 5.0521 (5.1050)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (60691.9602)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1012  max mem: 15572
Epoch: [0]  [1920/2809]  eta: 0:08:46  lr: 0.000006  min_lr: 0.000000  loss: 5.0453 (5.1047)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (61058.3321)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0720  max mem: 15572
[2025-01-15 14:41:38,396] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:41:38,397] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1930/2809]  eta: 0:08:40  lr: 0.000006  min_lr: 0.000000  loss: 4.9962 (5.1041)  class_acc: 0.0000 (0.0167)  loss_scale: 131072.0000 (62099.6872)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0844  max mem: 15572
Epoch: [0]  [1940/2809]  eta: 0:08:34  lr: 0.000006  min_lr: 0.000000  loss: 4.9962 (5.1037)  class_acc: 0.0000 (0.0166)  loss_scale: 262144.0000 (63130.3122)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0999  max mem: 15572
Epoch: [0]  [1950/2809]  eta: 0:08:28  lr: 0.000007  min_lr: 0.000000  loss: 5.0189 (5.1034)  class_acc: 0.0000 (0.0167)  loss_scale: 262144.0000 (64150.3721)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1470  max mem: 15572
Epoch: [0]  [1960/2809]  eta: 0:08:22  lr: 0.000007  min_lr: 0.000000  loss: 5.0057 (5.1030)  class_acc: 0.0000 (0.0167)  loss_scale: 262144.0000 (65160.0286)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1410  max mem: 15572
[2025-01-15 14:42:03,994] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1966
[2025-01-15 14:42:03,995] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:42:03,995] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1970/2809]  eta: 0:08:16  lr: 0.000007  min_lr: 0.000000  loss: 5.0015 (5.1026)  class_acc: 0.0000 (0.0168)  loss_scale: 262144.0000 (65826.9386)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0846  max mem: 15572
Epoch: [0]  [1980/2809]  eta: 0:08:10  lr: 0.000007  min_lr: 0.000000  loss: 5.0249 (5.1024)  class_acc: 0.0000 (0.0168)  loss_scale: 131072.0000 (66156.2928)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0903  max mem: 15572
Epoch: [0]  [1990/2809]  eta: 0:08:04  lr: 0.000007  min_lr: 0.000000  loss: 5.0255 (5.1021)  class_acc: 0.0000 (0.0169)  loss_scale: 131072.0000 (66482.3385)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1172  max mem: 15572
[2025-01-15 14:42:24,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=4, lr=[6.464542191180159e-08, 6.464542191180159e-08, 9.235060273114513e-08, 9.235060273114513e-08, 1.3192943247306448e-07, 1.3192943247306448e-07, 1.8847061781866357e-07, 1.8847061781866357e-07, 2.6924373974094795e-07, 2.6924373974094795e-07, 3.8463391391563995e-07, 3.8463391391563995e-07, 5.494770198794857e-07, 5.494770198794857e-07, 7.849671712564082e-07, 7.849671712564082e-07, 1.1213816732234404e-06, 1.1213816732234404e-06, 1.6019738188906293e-06, 1.6019738188906293e-06, 2.288534026986613e-06, 2.288534026986613e-06, 3.2693343242665907e-06, 3.2693343242665907e-06, 4.670477606095129e-06, 4.670477606095129e-06, 6.6721108658501856e-06, 6.6721108658501856e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 14:42:24,563] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=27.664634583358996, CurrSamplesPerSec=31.762076829777754, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [2000/2809]  eta: 0:07:58  lr: 0.000007  min_lr: 0.000000  loss: 5.0239 (5.1017)  class_acc: 0.0000 (0.0169)  loss_scale: 131072.0000 (66805.1254)  weight_decay: 0.0500 (0.0500)  time: 0.6549  data: 0.1755  max mem: 15572
Epoch: [0]  [2010/2809]  eta: 0:07:53  lr: 0.000007  min_lr: 0.000000  loss: 5.0163 (5.1012)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (67124.7021)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.1849  max mem: 15572
Epoch: [0]  [2020/2809]  eta: 0:07:47  lr: 0.000007  min_lr: 0.000000  loss: 5.0131 (5.1009)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (67441.1163)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1434  max mem: 15572
Epoch: [0]  [2030/2809]  eta: 0:07:41  lr: 0.000007  min_lr: 0.000000  loss: 5.0365 (5.1007)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (67754.4146)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1412  max mem: 15572
Epoch: [0]  [2040/2809]  eta: 0:07:35  lr: 0.000007  min_lr: 0.000000  loss: 5.0148 (5.1002)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (68064.6428)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1438  max mem: 15572
Epoch: [0]  [2050/2809]  eta: 0:07:29  lr: 0.000007  min_lr: 0.000000  loss: 4.9956 (5.0998)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (68371.8459)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1169  max mem: 15572
Epoch: [0]  [2060/2809]  eta: 0:07:22  lr: 0.000007  min_lr: 0.000000  loss: 5.0065 (5.0994)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (68676.0679)  weight_decay: 0.0500 (0.0500)  time: 0.5161  data: 0.0684  max mem: 15572
Epoch: [0]  [2070/2809]  eta: 0:07:17  lr: 0.000007  min_lr: 0.000000  loss: 5.0474 (5.0993)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (68977.3520)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1204  max mem: 15572
Epoch: [0]  [2080/2809]  eta: 0:07:11  lr: 0.000007  min_lr: 0.000000  loss: 5.0472 (5.0988)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (69275.7405)  weight_decay: 0.0500 (0.0500)  time: 0.7035  data: 0.2368  max mem: 15572
Epoch: [0]  [2090/2809]  eta: 0:07:05  lr: 0.000007  min_lr: 0.000000  loss: 5.0079 (5.0984)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (69571.2750)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1447  max mem: 15572
[2025-01-15 14:43:21,490] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:43:21,491] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2100/2809]  eta: 0:06:59  lr: 0.000007  min_lr: 0.000000  loss: 5.0152 (5.0981)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (70238.3094)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0431  max mem: 15572
Epoch: [0]  [2110/2809]  eta: 0:06:53  lr: 0.000007  min_lr: 0.000000  loss: 5.0159 (5.0977)  class_acc: 0.0000 (0.0173)  loss_scale: 262144.0000 (71147.3842)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0864  max mem: 15572
Epoch: [0]  [2120/2809]  eta: 0:06:47  lr: 0.000007  min_lr: 0.000000  loss: 5.0087 (5.0973)  class_acc: 0.0000 (0.0173)  loss_scale: 262144.0000 (72047.8868)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1299  max mem: 15572
Epoch: [0]  [2130/2809]  eta: 0:06:41  lr: 0.000007  min_lr: 0.000000  loss: 5.0087 (5.0969)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (72939.9381)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1378  max mem: 15572
Epoch: [0]  [2140/2809]  eta: 0:06:35  lr: 0.000007  min_lr: 0.000000  loss: 5.0178 (5.0969)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (73823.6562)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1547  max mem: 15572
Epoch: [0]  [2150/2809]  eta: 0:06:29  lr: 0.000007  min_lr: 0.000000  loss: 5.0167 (5.0966)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (74699.1576)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1615  max mem: 15572
Epoch: [0]  [2160/2809]  eta: 0:06:24  lr: 0.000007  min_lr: 0.000000  loss: 5.0172 (5.0962)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (75566.5562)  weight_decay: 0.0500 (0.0500)  time: 0.6381  data: 0.1677  max mem: 15572
[2025-01-15 14:44:02,606] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2164
[2025-01-15 14:44:02,606] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:44:02,607] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2170/2809]  eta: 0:06:18  lr: 0.000007  min_lr: 0.000000  loss: 5.0320 (5.0959)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (76003.3459)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1315  max mem: 15572
Epoch: [0]  [2180/2809]  eta: 0:06:12  lr: 0.000007  min_lr: 0.000000  loss: 5.0348 (5.0958)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (76255.8386)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0648  max mem: 15572
Epoch: [0]  [2190/2809]  eta: 0:06:05  lr: 0.000007  min_lr: 0.000000  loss: 5.0163 (5.0953)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (76506.0265)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0692  max mem: 15572
Epoch: [0]  [2200/2809]  eta: 0:05:59  lr: 0.000007  min_lr: 0.000000  loss: 5.0278 (5.0951)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (76753.9409)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0852  max mem: 15572
Epoch: [0]  [2210/2809]  eta: 0:05:53  lr: 0.000007  min_lr: 0.000000  loss: 5.0292 (5.0947)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (76999.6128)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0996  max mem: 15572
Epoch: [0]  [2220/2809]  eta: 0:05:47  lr: 0.000007  min_lr: 0.000000  loss: 4.9965 (5.0943)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (77243.0725)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1160  max mem: 15572
Epoch: [0]  [2230/2809]  eta: 0:05:42  lr: 0.000007  min_lr: 0.000000  loss: 4.9938 (5.0937)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (77484.3496)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1467  max mem: 15572
Epoch: [0]  [2240/2809]  eta: 0:05:36  lr: 0.000007  min_lr: 0.000000  loss: 5.0039 (5.0935)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (77723.4734)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1417  max mem: 15572
Epoch: [0]  [2250/2809]  eta: 0:05:30  lr: 0.000008  min_lr: 0.000000  loss: 5.0199 (5.0932)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (77960.4727)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1163  max mem: 15572
Epoch: [0]  [2260/2809]  eta: 0:05:24  lr: 0.000008  min_lr: 0.000000  loss: 5.0149 (5.0930)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (78195.3755)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.1367  max mem: 15572
Epoch: [0]  [2270/2809]  eta: 0:05:18  lr: 0.000008  min_lr: 0.000000  loss: 5.0000 (5.0925)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (78428.2096)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1574  max mem: 15572
Epoch: [0]  [2280/2809]  eta: 0:05:12  lr: 0.000008  min_lr: 0.000000  loss: 4.9945 (5.0921)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (78659.0022)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1455  max mem: 15572
Epoch: [0]  [2290/2809]  eta: 0:05:06  lr: 0.000008  min_lr: 0.000000  loss: 5.0172 (5.0916)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (78887.7800)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0776  max mem: 15572
[2025-01-15 14:45:15,617] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:45:15,617] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2300/2809]  eta: 0:05:00  lr: 0.000008  min_lr: 0.000000  loss: 4.9994 (5.0912)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (79570.2738)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0426  max mem: 15572
Epoch: [0]  [2310/2809]  eta: 0:04:55  lr: 0.000008  min_lr: 0.000000  loss: 4.9904 (5.0909)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (80360.2942)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.1286  max mem: 15572
Epoch: [0]  [2320/2809]  eta: 0:04:48  lr: 0.000008  min_lr: 0.000000  loss: 4.9957 (5.0906)  class_acc: 0.0000 (0.0176)  loss_scale: 262144.0000 (81143.5071)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.0917  max mem: 15572
Epoch: [0]  [2330/2809]  eta: 0:04:42  lr: 0.000008  min_lr: 0.000000  loss: 5.0006 (5.0903)  class_acc: 0.0000 (0.0176)  loss_scale: 262144.0000 (81920.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0009  max mem: 15572
Epoch: [0]  [2340/2809]  eta: 0:04:37  lr: 0.000008  min_lr: 0.000000  loss: 5.0350 (5.0902)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (82689.8590)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0733  max mem: 15572
Epoch: [0]  [2350/2809]  eta: 0:04:31  lr: 0.000008  min_lr: 0.000000  loss: 5.0468 (5.0899)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (83453.1689)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1389  max mem: 15572
Epoch: [0]  [2360/2809]  eta: 0:04:25  lr: 0.000008  min_lr: 0.000000  loss: 5.0284 (5.0897)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (84210.0127)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1594  max mem: 15572
Epoch: [0]  [2370/2809]  eta: 0:04:19  lr: 0.000008  min_lr: 0.000000  loss: 5.0409 (5.0897)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (84960.4724)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1749  max mem: 15572
Epoch: [0]  [2380/2809]  eta: 0:04:13  lr: 0.000008  min_lr: 0.000000  loss: 5.0282 (5.0894)  class_acc: 0.0000 (0.0174)  loss_scale: 262144.0000 (85704.6283)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1574  max mem: 15572
Epoch: [0]  [2390/2809]  eta: 0:04:07  lr: 0.000008  min_lr: 0.000000  loss: 5.0139 (5.0892)  class_acc: 0.0000 (0.0174)  loss_scale: 262144.0000 (86442.5596)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1562  max mem: 15572
[2025-01-15 14:46:19,982] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2399
[2025-01-15 14:46:19,982] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:46:19,982] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2400/2809]  eta: 0:04:01  lr: 0.000008  min_lr: 0.000000  loss: 5.0126 (5.0888)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (87065.1628)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1493  max mem: 15572
Epoch: [0]  [2410/2809]  eta: 0:03:55  lr: 0.000008  min_lr: 0.000000  loss: 4.9721 (5.0882)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (87247.6881)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.0963  max mem: 15572
Epoch: [0]  [2420/2809]  eta: 0:03:49  lr: 0.000008  min_lr: 0.000000  loss: 4.9851 (5.0881)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (87428.7055)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0693  max mem: 15572
Epoch: [0]  [2430/2809]  eta: 0:03:44  lr: 0.000008  min_lr: 0.000000  loss: 5.0347 (5.0879)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (87608.2336)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1004  max mem: 15572
Epoch: [0]  [2440/2809]  eta: 0:03:38  lr: 0.000008  min_lr: 0.000000  loss: 5.0477 (5.0878)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (87786.2909)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.0936  max mem: 15572
Epoch: [0]  [2450/2809]  eta: 0:03:32  lr: 0.000008  min_lr: 0.000000  loss: 5.0438 (5.0875)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (87962.8951)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1082  max mem: 15572
Epoch: [0]  [2460/2809]  eta: 0:03:26  lr: 0.000008  min_lr: 0.000000  loss: 5.0188 (5.0872)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (88138.0642)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1019  max mem: 15572
Epoch: [0]  [2470/2809]  eta: 0:03:20  lr: 0.000008  min_lr: 0.000000  loss: 5.0179 (5.0870)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (88311.8155)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0787  max mem: 15572
Epoch: [0]  [2480/2809]  eta: 0:03:14  lr: 0.000008  min_lr: 0.000000  loss: 5.0046 (5.0866)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (88484.1661)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1281  max mem: 15572
Epoch: [0]  [2490/2809]  eta: 0:03:08  lr: 0.000008  min_lr: 0.000000  loss: 4.9785 (5.0862)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (88655.1329)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1274  max mem: 15572
Epoch: [0]  [2500/2809]  eta: 0:03:02  lr: 0.000008  min_lr: 0.000000  loss: 4.9931 (5.0859)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (88824.7325)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1113  max mem: 15572
Epoch: [0]  [2510/2809]  eta: 0:02:56  lr: 0.000008  min_lr: 0.000000  loss: 4.9957 (5.0856)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (88992.9813)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1331  max mem: 15572
Epoch: [0]  [2520/2809]  eta: 0:02:50  lr: 0.000008  min_lr: 0.000000  loss: 4.9732 (5.0851)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (89159.8953)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1581  max mem: 15572
[2025-01-15 14:47:33,713] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:47:33,714] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2530/2809]  eta: 0:02:44  lr: 0.000008  min_lr: 0.000000  loss: 4.9662 (5.0848)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (89480.8503)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1408  max mem: 15572
Epoch: [0]  [2540/2809]  eta: 0:02:38  lr: 0.000008  min_lr: 0.000000  loss: 5.0167 (5.0846)  class_acc: 0.0000 (0.0173)  loss_scale: 262144.0000 (90160.3589)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1059  max mem: 15572
Epoch: [0]  [2550/2809]  eta: 0:02:32  lr: 0.000009  min_lr: 0.000000  loss: 5.0580 (5.0846)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (90834.5402)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1058  max mem: 15572
Epoch: [0]  [2560/2809]  eta: 0:02:26  lr: 0.000009  min_lr: 0.000000  loss: 5.0289 (5.0843)  class_acc: 0.0000 (0.0174)  loss_scale: 262144.0000 (91503.4565)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0923  max mem: 15572
[2025-01-15 14:47:54,086] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2565
[2025-01-15 14:47:54,087] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:47:54,087] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2570/2809]  eta: 0:02:21  lr: 0.000009  min_lr: 0.000000  loss: 5.0285 (5.0841)  class_acc: 0.0000 (0.0173)  loss_scale: 262144.0000 (91861.2835)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1162  max mem: 15572
Epoch: [0]  [2580/2809]  eta: 0:02:15  lr: 0.000009  min_lr: 0.000000  loss: 5.0133 (5.0838)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (92013.2042)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1815  max mem: 15572
Epoch: [0]  [2590/2809]  eta: 0:02:09  lr: 0.000009  min_lr: 0.000000  loss: 5.0160 (5.0836)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (92163.9521)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1793  max mem: 15572
Epoch: [0]  [2600/2809]  eta: 0:02:03  lr: 0.000009  min_lr: 0.000000  loss: 5.0070 (5.0831)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (92313.5409)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1456  max mem: 15572
Epoch: [0]  [2610/2809]  eta: 0:01:57  lr: 0.000009  min_lr: 0.000000  loss: 4.9880 (5.0827)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (92461.9839)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1414  max mem: 15572
Epoch: [0]  [2620/2809]  eta: 0:01:51  lr: 0.000009  min_lr: 0.000000  loss: 5.0409 (5.0827)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (92609.2942)  weight_decay: 0.0500 (0.0500)  time: 0.6716  data: 0.1880  max mem: 15572
Epoch: [0]  [2630/2809]  eta: 0:01:45  lr: 0.000009  min_lr: 0.000000  loss: 5.0615 (5.0824)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (92755.4846)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1589  max mem: 15572
Epoch: [0]  [2640/2809]  eta: 0:01:39  lr: 0.000009  min_lr: 0.000000  loss: 5.0248 (5.0823)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (92900.5680)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1298  max mem: 15572
Epoch: [0]  [2650/2809]  eta: 0:01:33  lr: 0.000009  min_lr: 0.000000  loss: 5.0564 (5.0822)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (93044.5568)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1602  max mem: 15572
Epoch: [0]  [2660/2809]  eta: 0:01:27  lr: 0.000009  min_lr: 0.000000  loss: 5.0158 (5.0819)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (93187.4634)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0989  max mem: 15572
Epoch: [0]  [2670/2809]  eta: 0:01:22  lr: 0.000009  min_lr: 0.000000  loss: 4.9963 (5.0816)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (93329.2999)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0942  max mem: 15572
Epoch: [0]  [2680/2809]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000000  loss: 4.9945 (5.0813)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (93470.0783)  weight_decay: 0.0500 (0.0500)  time: 0.6508  data: 0.1959  max mem: 15572
Epoch: [0]  [2690/2809]  eta: 0:01:10  lr: 0.000009  min_lr: 0.000000  loss: 5.0100 (5.0810)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (93609.8105)  weight_decay: 0.0500 (0.0500)  time: 0.7033  data: 0.2418  max mem: 15572
[2025-01-15 14:49:14,110] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:49:14,110] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2700/2809]  eta: 0:01:04  lr: 0.000009  min_lr: 0.000000  loss: 5.0253 (5.0810)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (94088.1984)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1453  max mem: 15572
Epoch: [0]  [2710/2809]  eta: 0:00:58  lr: 0.000009  min_lr: 0.000000  loss: 5.0170 (5.0808)  class_acc: 0.0000 (0.0174)  loss_scale: 262144.0000 (94708.1018)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0944  max mem: 15572
Epoch: [0]  [2720/2809]  eta: 0:00:52  lr: 0.000009  min_lr: 0.000000  loss: 5.0170 (5.0807)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (95323.4487)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1098  max mem: 15572
Epoch: [0]  [2730/2809]  eta: 0:00:46  lr: 0.000009  min_lr: 0.000000  loss: 5.0070 (5.0804)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (95934.2893)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1382  max mem: 15572
Epoch: [0]  [2740/2809]  eta: 0:00:40  lr: 0.000009  min_lr: 0.000000  loss: 4.9637 (5.0801)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (96540.6727)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1560  max mem: 15572
[2025-01-15 14:49:46,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2750
[2025-01-15 14:49:46,689] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:49:46,690] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2750/2809]  eta: 0:00:34  lr: 0.000009  min_lr: 0.000000  loss: 4.9868 (5.0798)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (97095.0025)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1242  max mem: 15572
Epoch: [0]  [2760/2809]  eta: 0:00:28  lr: 0.000009  min_lr: 0.000000  loss: 5.0144 (5.0795)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (97218.0630)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0591  max mem: 15572
Epoch: [0]  [2770/2809]  eta: 0:00:23  lr: 0.000009  min_lr: 0.000000  loss: 4.9761 (5.0791)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (97340.2353)  weight_decay: 0.0500 (0.0500)  time: 0.4372  data: 0.0006  max mem: 15572
Epoch: [0]  [2780/2809]  eta: 0:00:17  lr: 0.000009  min_lr: 0.000000  loss: 4.9995 (5.0789)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (97461.5289)  weight_decay: 0.0500 (0.0500)  time: 0.4551  data: 0.0007  max mem: 15572
Epoch: [0]  [2790/2809]  eta: 0:00:11  lr: 0.000009  min_lr: 0.000000  loss: 4.9823 (5.0784)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (97581.9534)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0675  max mem: 15572
Epoch: [0]  [2800/2809]  eta: 0:00:05  lr: 0.000009  min_lr: 0.000000  loss: 4.9883 (5.0784)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (97701.5180)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1673  max mem: 15572
Epoch: [0]  [2808/2809]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 5.0459 (5.0783)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (97796.5568)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1178  max mem: 15572
Epoch: [0] Total time: 0:27:37 (0.5899 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 5.0459 (5.0783)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (97796.5568)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:28:28  loss: 4.9961 (4.9961)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 6.2817  data: 5.9034  max mem: 15572
Val:  [ 10/272]  eta: 0:04:02  loss: 5.1398 (5.0970)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.9272  data: 0.7124  max mem: 15572
Val:  [ 20/272]  eta: 0:02:50  loss: 5.1810 (5.0652)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3946  data: 0.2022  max mem: 15572
Val:  [ 30/272]  eta: 0:02:26  loss: 5.0935 (5.0345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.4303  data: 0.2422  max mem: 15572
Val:  [ 40/272]  eta: 0:02:08  loss: 4.8557 (4.9965)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.4301  data: 0.2277  max mem: 15572
Val:  [ 50/272]  eta: 0:01:50  loss: 4.8203 (5.0164)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3317  data: 0.1323  max mem: 15572
Val:  [ 60/272]  eta: 0:01:41  loss: 4.8264 (5.0089)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3244  data: 0.1385  max mem: 15572
Val:  [ 70/272]  eta: 0:01:32  loss: 4.8264 (4.9943)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (4.8513)  time: 0.3573  data: 0.1574  max mem: 15572
Val:  [ 80/272]  eta: 0:01:27  loss: 4.9688 (4.9630)  acc1: 0.0000 (1.4403)  acc5: 0.0000 (9.1221)  time: 0.3800  data: 0.1783  max mem: 15572
Val:  [ 90/272]  eta: 0:01:21  loss: 5.0117 (4.9861)  acc1: 0.0000 (1.2821)  acc5: 0.0000 (8.1197)  time: 0.3998  data: 0.2035  max mem: 15572
Val:  [100/272]  eta: 0:01:13  loss: 5.1699 (5.0157)  acc1: 0.0000 (1.1551)  acc5: 0.0000 (7.3157)  time: 0.3259  data: 0.1166  max mem: 15572
Val:  [110/272]  eta: 0:01:09  loss: 5.2383 (5.0361)  acc1: 0.0000 (1.0511)  acc5: 0.0000 (6.6567)  time: 0.3497  data: 0.1407  max mem: 15572
Val:  [120/272]  eta: 0:01:03  loss: 5.1738 (5.0579)  acc1: 0.0000 (0.9642)  acc5: 0.0000 (6.1065)  time: 0.3776  data: 0.1759  max mem: 15572
Val:  [130/272]  eta: 0:00:59  loss: 5.1081 (5.0573)  acc1: 0.0000 (0.8906)  acc5: 0.0000 (7.5912)  time: 0.3463  data: 0.1358  max mem: 15572
Val:  [140/272]  eta: 0:00:55  loss: 4.8329 (5.0439)  acc1: 0.0000 (0.8274)  acc5: 0.0000 (9.0623)  time: 0.3969  data: 0.1744  max mem: 15572
Val:  [150/272]  eta: 0:00:50  loss: 4.8329 (5.0335)  acc1: 0.0000 (0.7726)  acc5: 0.0000 (8.4621)  time: 0.3812  data: 0.1607  max mem: 15572
Val:  [160/272]  eta: 0:00:45  loss: 4.6217 (5.0066)  acc1: 0.0000 (2.3810)  acc5: 0.0000 (9.7999)  time: 0.3399  data: 0.1233  max mem: 15572
Val:  [170/272]  eta: 0:00:40  loss: 4.6133 (5.0139)  acc1: 0.0000 (2.3717)  acc5: 0.0000 (9.4217)  time: 0.3297  data: 0.1231  max mem: 15572
Val:  [180/272]  eta: 0:00:36  loss: 5.1031 (5.0162)  acc1: 0.0000 (2.2406)  acc5: 0.0000 (8.9012)  time: 0.2623  data: 0.0803  max mem: 15572
Val:  [190/272]  eta: 0:00:31  loss: 5.1619 (5.0228)  acc1: 0.0000 (2.1233)  acc5: 0.0000 (8.4351)  time: 0.1892  data: 0.0251  max mem: 15572
Val:  [200/272]  eta: 0:00:26  loss: 5.0703 (5.0313)  acc1: 0.0000 (2.0177)  acc5: 0.0000 (8.0155)  time: 0.1707  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:22  loss: 5.0703 (5.0357)  acc1: 0.0000 (1.9221)  acc5: 0.0000 (7.6356)  time: 0.1800  data: 0.0005  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 4.8362 (5.0212)  acc1: 0.0000 (1.8351)  acc5: 0.0000 (7.2901)  time: 0.2190  data: 0.0274  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.7266 (5.0101)  acc1: 0.0000 (1.7557)  acc5: 0.0000 (6.9745)  time: 0.2866  data: 0.0928  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 4.7528 (5.0084)  acc1: 0.0000 (1.6828)  acc5: 0.0000 (6.6851)  time: 0.2888  data: 0.1083  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 5.2188 (5.0245)  acc1: 0.0000 (1.6158)  acc5: 0.0000 (6.4188)  time: 0.2680  data: 0.0827  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 5.1806 (5.0205)  acc1: 0.0000 (1.5539)  acc5: 0.0000 (6.1728)  time: 0.3080  data: 0.1192  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 5.0794 (5.0221)  acc1: 0.0000 (1.4965)  acc5: 0.0000 (5.9451)  time: 0.2587  data: 0.0848  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 5.0851 (5.0240)  acc1: 0.0000 (1.4950)  acc5: 0.0000 (5.9390)  time: 0.2017  data: 0.0333  max mem: 15572
Val: Total time: 0:01:32 (0.3410 s / it)
* Acc@1 1.495 Acc@5 5.939 loss 5.024
Accuracy of the network on the 4883 val videos: 1.5%
[2025-01-15 14:51:49,274] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2025-01-15 14:51:49,279] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 14:51:49,279] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 14:51:52,094] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 14:51:52,095] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 1.49%
Epoch: [1]  [   0/2809]  eta: 8:30:12  lr: 0.000009  min_lr: 0.000000  loss: 4.9111 (4.9111)  class_acc: 0.0417 (0.0417)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 10.8981  data: 10.4030  max mem: 15572
Epoch: [1]  [  10/2809]  eta: 1:06:24  lr: 0.000009  min_lr: 0.000000  loss: 5.0153 (5.0085)  class_acc: 0.0000 (0.0265)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4236  data: 0.9463  max mem: 15572
Epoch: [1]  [  20/2809]  eta: 0:47:46  lr: 0.000009  min_lr: 0.000000  loss: 5.0187 (5.0266)  class_acc: 0.0000 (0.0218)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0666  max mem: 15572
Epoch: [1]  [  30/2809]  eta: 0:39:58  lr: 0.000009  min_lr: 0.000000  loss: 5.0329 (5.0178)  class_acc: 0.0000 (0.0148)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1021  max mem: 15572
Epoch: [1]  [  40/2809]  eta: 0:36:49  lr: 0.000010  min_lr: 0.000000  loss: 4.9727 (5.0084)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0941  max mem: 15572
Epoch: [1]  [  50/2809]  eta: 0:35:18  lr: 0.000010  min_lr: 0.000000  loss: 4.9929 (5.0139)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1548  max mem: 15572
Epoch: [1]  [  60/2809]  eta: 0:33:49  lr: 0.000010  min_lr: 0.000000  loss: 4.9949 (5.0141)  class_acc: 0.0000 (0.0184)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1648  max mem: 15572
[2025-01-15 14:52:41,835] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:52:41,836] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [  70/2809]  eta: 0:31:58  lr: 0.000010  min_lr: 0.000000  loss: 5.0436 (5.0177)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (132918.0845)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0687  max mem: 15572
Epoch: [1]  [  80/2809]  eta: 0:31:41  lr: 0.000010  min_lr: 0.000000  loss: 5.0436 (5.0213)  class_acc: 0.0000 (0.0170)  loss_scale: 262144.0000 (148871.9012)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1195  max mem: 15572
Epoch: [1]  [  90/2809]  eta: 0:30:35  lr: 0.000010  min_lr: 0.000000  loss: 5.0041 (5.0165)  class_acc: 0.0000 (0.0188)  loss_scale: 262144.0000 (161319.3846)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1451  max mem: 15572
Epoch: [1]  [ 100/2809]  eta: 0:30:17  lr: 0.000010  min_lr: 0.000000  loss: 4.9952 (5.0135)  class_acc: 0.0000 (0.0186)  loss_scale: 262144.0000 (171302.0198)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1200  max mem: 15572
[2025-01-15 14:53:02,883] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2916
[2025-01-15 14:53:02,883] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:53:02,884] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 110/2809]  eta: 0:29:47  lr: 0.000010  min_lr: 0.000000  loss: 4.9427 (5.0077)  class_acc: 0.0000 (0.0184)  loss_scale: 262144.0000 (174762.6667)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1628  max mem: 15572
Epoch: [1]  [ 120/2809]  eta: 0:29:28  lr: 0.000010  min_lr: 0.000000  loss: 4.9598 (5.0042)  class_acc: 0.0000 (0.0169)  loss_scale: 131072.0000 (171151.8678)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1441  max mem: 15572
Epoch: [1]  [ 130/2809]  eta: 0:28:59  lr: 0.000010  min_lr: 0.000000  loss: 4.9860 (5.0038)  class_acc: 0.0000 (0.0156)  loss_scale: 131072.0000 (168092.3359)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1279  max mem: 15572
Epoch: [1]  [ 140/2809]  eta: 0:28:41  lr: 0.000010  min_lr: 0.000000  loss: 4.9986 (5.0089)  class_acc: 0.0000 (0.0145)  loss_scale: 131072.0000 (165466.7801)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0996  max mem: 15572
Epoch: [1]  [ 150/2809]  eta: 0:28:22  lr: 0.000010  min_lr: 0.000000  loss: 5.0000 (5.0078)  class_acc: 0.0000 (0.0152)  loss_scale: 131072.0000 (163188.9801)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1062  max mem: 15572
Epoch: [1]  [ 160/2809]  eta: 0:28:07  lr: 0.000010  min_lr: 0.000000  loss: 4.9776 (5.0053)  class_acc: 0.0000 (0.0153)  loss_scale: 131072.0000 (161194.1366)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1155  max mem: 15572
Epoch: [1]  [ 170/2809]  eta: 0:27:39  lr: 0.000010  min_lr: 0.000000  loss: 4.9464 (5.0036)  class_acc: 0.0000 (0.0158)  loss_scale: 131072.0000 (159432.6082)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0699  max mem: 15572
Epoch: [1]  [ 180/2809]  eta: 0:27:23  lr: 0.000010  min_lr: 0.000000  loss: 5.0185 (5.0076)  class_acc: 0.0000 (0.0154)  loss_scale: 131072.0000 (157865.7238)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0573  max mem: 15572
[2025-01-15 14:53:51,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=9, lr=[9.698430230790043e-08, 9.698430230790043e-08, 1.3854900329700063e-07, 1.3854900329700063e-07, 1.9792714756714377e-07, 1.9792714756714377e-07, 2.8275306795306254e-07, 2.8275306795306254e-07, 4.0393295421866083e-07, 4.0393295421866083e-07, 5.770470774552297e-07, 5.770470774552297e-07, 8.243529677931854e-07, 8.243529677931854e-07, 1.1776470968474079e-06, 1.1776470968474079e-06, 1.682352995496297e-06, 1.682352995496297e-06, 2.4033614221375676e-06, 2.4033614221375676e-06, 3.4333734601965247e-06, 3.4333734601965247e-06, 4.9048192288521786e-06, 4.9048192288521786e-06, 7.0068846126459705e-06, 7.0068846126459705e-06, 1.0009835160922815e-05, 1.0009835160922815e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 14:53:51,209] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=27.618809288722048, CurrSamplesPerSec=23.213038723873503, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [ 190/2809]  eta: 0:27:12  lr: 0.000010  min_lr: 0.000000  loss: 5.0269 (5.0076)  class_acc: 0.0000 (0.0151)  loss_scale: 131072.0000 (156462.9110)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1054  max mem: 15572
Epoch: [1]  [ 200/2809]  eta: 0:26:58  lr: 0.000010  min_lr: 0.000000  loss: 4.9828 (5.0076)  class_acc: 0.0000 (0.0149)  loss_scale: 131072.0000 (155199.6816)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1146  max mem: 15572
Epoch: [1]  [ 210/2809]  eta: 0:27:02  lr: 0.000010  min_lr: 0.000000  loss: 4.9828 (5.0054)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (154056.1896)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.1924  max mem: 15572
Epoch: [1]  [ 220/2809]  eta: 0:26:46  lr: 0.000010  min_lr: 0.000000  loss: 4.9874 (5.0038)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (153016.1810)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1794  max mem: 15572
Epoch: [1]  [ 230/2809]  eta: 0:26:34  lr: 0.000010  min_lr: 0.000000  loss: 4.9921 (5.0050)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (152066.2165)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1034  max mem: 15572
[2025-01-15 14:54:19,886] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:54:19,886] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 240/2809]  eta: 0:26:32  lr: 0.000010  min_lr: 0.000000  loss: 4.9921 (5.0060)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (153914.4232)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1627  max mem: 15572
Epoch: [1]  [ 250/2809]  eta: 0:26:16  lr: 0.000010  min_lr: 0.000000  loss: 5.0949 (5.0087)  class_acc: 0.0000 (0.0176)  loss_scale: 262144.0000 (158226.3586)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1257  max mem: 15572
Epoch: [1]  [ 260/2809]  eta: 0:26:00  lr: 0.000010  min_lr: 0.000000  loss: 5.0220 (5.0082)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (162207.8774)  weight_decay: 0.0500 (0.0500)  time: 0.5178  data: 0.0556  max mem: 15572
Epoch: [1]  [ 270/2809]  eta: 0:26:00  lr: 0.000010  min_lr: 0.000000  loss: 4.9790 (5.0094)  class_acc: 0.0000 (0.0174)  loss_scale: 262144.0000 (165895.5572)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1265  max mem: 15572
[2025-01-15 14:54:41,693] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3084
[2025-01-15 14:54:41,694] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:54:41,694] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 280/2809]  eta: 0:25:50  lr: 0.000010  min_lr: 0.000000  loss: 5.0541 (5.0112)  class_acc: 0.0000 (0.0168)  loss_scale: 262144.0000 (166522.0783)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1520  max mem: 15572
Epoch: [1]  [ 290/2809]  eta: 0:25:48  lr: 0.000010  min_lr: 0.000000  loss: 5.0697 (5.0127)  class_acc: 0.0000 (0.0162)  loss_scale: 131072.0000 (165303.8625)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1410  max mem: 15572
Epoch: [1]  [ 300/2809]  eta: 0:25:38  lr: 0.000010  min_lr: 0.000000  loss: 5.0354 (5.0139)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (164166.5914)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1390  max mem: 15572
Epoch: [1]  [ 310/2809]  eta: 0:25:30  lr: 0.000010  min_lr: 0.000000  loss: 5.0107 (5.0133)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (163102.4566)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1221  max mem: 15572
Epoch: [1]  [ 320/2809]  eta: 0:25:17  lr: 0.000010  min_lr: 0.000000  loss: 4.9561 (5.0110)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (162104.6231)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1026  max mem: 15572
Epoch: [1]  [ 330/2809]  eta: 0:25:07  lr: 0.000010  min_lr: 0.000000  loss: 4.9278 (5.0104)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (161167.0816)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0911  max mem: 15572
Epoch: [1]  [ 340/2809]  eta: 0:25:07  lr: 0.000011  min_lr: 0.000000  loss: 4.9467 (5.0080)  class_acc: 0.0000 (0.0187)  loss_scale: 131072.0000 (160284.5279)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1910  max mem: 15572
Epoch: [1]  [ 350/2809]  eta: 0:25:05  lr: 0.000011  min_lr: 0.000000  loss: 4.9467 (5.0071)  class_acc: 0.0000 (0.0189)  loss_scale: 131072.0000 (159452.2621)  weight_decay: 0.0500 (0.0500)  time: 0.6806  data: 0.2330  max mem: 15572
Epoch: [1]  [ 360/2809]  eta: 0:24:53  lr: 0.000011  min_lr: 0.000000  loss: 5.0137 (5.0080)  class_acc: 0.0000 (0.0197)  loss_scale: 131072.0000 (158666.1053)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1394  max mem: 15572
Epoch: [1]  [ 370/2809]  eta: 0:24:45  lr: 0.000011  min_lr: 0.000000  loss: 5.0137 (5.0069)  class_acc: 0.0000 (0.0194)  loss_scale: 131072.0000 (157922.3288)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1180  max mem: 15572
Epoch: [1]  [ 380/2809]  eta: 0:24:38  lr: 0.000011  min_lr: 0.000000  loss: 4.9701 (5.0074)  class_acc: 0.0000 (0.0191)  loss_scale: 131072.0000 (157217.5958)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1571  max mem: 15572
Epoch: [1]  [ 390/2809]  eta: 0:24:29  lr: 0.000011  min_lr: 0.000000  loss: 5.0217 (5.0073)  class_acc: 0.0000 (0.0186)  loss_scale: 131072.0000 (156548.9105)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1233  max mem: 15572
Epoch: [1]  [ 400/2809]  eta: 0:24:21  lr: 0.000011  min_lr: 0.000000  loss: 5.0090 (5.0065)  class_acc: 0.0000 (0.0184)  loss_scale: 131072.0000 (155913.5761)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0882  max mem: 15572
[2025-01-15 14:55:57,415] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:55:57,416] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 410/2809]  eta: 0:24:16  lr: 0.000011  min_lr: 0.000000  loss: 5.0160 (5.0064)  class_acc: 0.0000 (0.0186)  loss_scale: 131072.0000 (157541.5280)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1246  max mem: 15572
[2025-01-15 14:56:02,484] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3220
[2025-01-15 14:56:02,485] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:56:02,485] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 420/2809]  eta: 0:24:06  lr: 0.000011  min_lr: 0.000000  loss: 5.0325 (5.0064)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (156912.7981)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1209  max mem: 15572
Epoch: [1]  [ 430/2809]  eta: 0:24:04  lr: 0.000011  min_lr: 0.000000  loss: 5.0016 (5.0060)  class_acc: 0.0000 (0.0189)  loss_scale: 131072.0000 (156313.2436)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1594  max mem: 15572
Epoch: [1]  [ 440/2809]  eta: 0:23:57  lr: 0.000011  min_lr: 0.000000  loss: 4.9671 (5.0053)  class_acc: 0.0000 (0.0190)  loss_scale: 131072.0000 (155740.8798)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.1962  max mem: 15572
Epoch: [1]  [ 450/2809]  eta: 0:23:48  lr: 0.000011  min_lr: 0.000000  loss: 4.9671 (5.0042)  class_acc: 0.0000 (0.0188)  loss_scale: 131072.0000 (155193.8980)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1316  max mem: 15572
Epoch: [1]  [ 460/2809]  eta: 0:23:49  lr: 0.000011  min_lr: 0.000000  loss: 4.9678 (5.0046)  class_acc: 0.0000 (0.0189)  loss_scale: 131072.0000 (154670.6464)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.2062  max mem: 15572
Epoch: [1]  [ 470/2809]  eta: 0:23:40  lr: 0.000011  min_lr: 0.000000  loss: 5.0347 (5.0054)  class_acc: 0.0000 (0.0190)  loss_scale: 131072.0000 (154169.6136)  weight_decay: 0.0500 (0.0500)  time: 0.6381  data: 0.1918  max mem: 15572
Epoch: [1]  [ 480/2809]  eta: 0:23:33  lr: 0.000011  min_lr: 0.000000  loss: 5.0347 (5.0054)  class_acc: 0.0000 (0.0190)  loss_scale: 131072.0000 (153689.4137)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0970  max mem: 15572
Epoch: [1]  [ 490/2809]  eta: 0:23:23  lr: 0.000011  min_lr: 0.000000  loss: 4.9688 (5.0046)  class_acc: 0.0000 (0.0189)  loss_scale: 131072.0000 (153228.7739)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0914  max mem: 15572
Epoch: [1]  [ 500/2809]  eta: 0:23:19  lr: 0.000011  min_lr: 0.000000  loss: 4.9688 (5.0049)  class_acc: 0.0000 (0.0195)  loss_scale: 131072.0000 (152786.5230)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1267  max mem: 15572
Epoch: [1]  [ 510/2809]  eta: 0:23:14  lr: 0.000011  min_lr: 0.000000  loss: 4.9626 (5.0037)  class_acc: 0.0000 (0.0195)  loss_scale: 131072.0000 (152361.5812)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1765  max mem: 15572
Epoch: [1]  [ 520/2809]  eta: 0:23:06  lr: 0.000011  min_lr: 0.000000  loss: 4.9891 (5.0044)  class_acc: 0.0000 (0.0193)  loss_scale: 131072.0000 (151952.9520)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1338  max mem: 15572
Epoch: [1]  [ 530/2809]  eta: 0:23:04  lr: 0.000011  min_lr: 0.000000  loss: 5.0355 (5.0046)  class_acc: 0.0000 (0.0191)  loss_scale: 131072.0000 (151559.7137)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1618  max mem: 15572
[2025-01-15 14:57:19,666] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:57:19,666] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 540/2809]  eta: 0:22:52  lr: 0.000011  min_lr: 0.000000  loss: 5.0386 (5.0061)  class_acc: 0.0000 (0.0192)  loss_scale: 131072.0000 (151423.2902)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1286  max mem: 15572
Epoch: [1]  [ 550/2809]  eta: 0:22:39  lr: 0.000011  min_lr: 0.000000  loss: 5.0565 (5.0068)  class_acc: 0.0000 (0.0190)  loss_scale: 262144.0000 (153432.7405)  weight_decay: 0.0500 (0.0500)  time: 0.4597  data: 0.0159  max mem: 15572
Epoch: [1]  [ 560/2809]  eta: 0:22:33  lr: 0.000011  min_lr: 0.000000  loss: 5.0545 (5.0079)  class_acc: 0.0000 (0.0195)  loss_scale: 262144.0000 (155370.5526)  weight_decay: 0.0500 (0.0500)  time: 0.5225  data: 0.0631  max mem: 15572
Epoch: [1]  [ 570/2809]  eta: 0:22:26  lr: 0.000011  min_lr: 0.000000  loss: 5.0539 (5.0074)  class_acc: 0.0000 (0.0201)  loss_scale: 262144.0000 (157240.4904)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1218  max mem: 15572
Epoch: [1]  [ 580/2809]  eta: 0:22:21  lr: 0.000011  min_lr: 0.000000  loss: 5.0331 (5.0079)  class_acc: 0.0000 (0.0199)  loss_scale: 262144.0000 (159046.0585)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1495  max mem: 15572
Epoch: [1]  [ 590/2809]  eta: 0:22:16  lr: 0.000011  min_lr: 0.000000  loss: 5.0097 (5.0078)  class_acc: 0.0000 (0.0197)  loss_scale: 262144.0000 (160790.5245)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1952  max mem: 15572
[2025-01-15 14:57:51,350] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3405
[2025-01-15 14:57:51,350] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:57:51,351] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 600/2809]  eta: 0:22:12  lr: 0.000011  min_lr: 0.000000  loss: 4.9949 (5.0078)  class_acc: 0.0000 (0.0200)  loss_scale: 262144.0000 (161386.4892)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.1908  max mem: 15572
Epoch: [1]  [ 610/2809]  eta: 0:22:04  lr: 0.000011  min_lr: 0.000000  loss: 5.0030 (5.0081)  class_acc: 0.0000 (0.0200)  loss_scale: 131072.0000 (160890.3437)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1165  max mem: 15572
Epoch: [1]  [ 620/2809]  eta: 0:21:57  lr: 0.000011  min_lr: 0.000000  loss: 5.0230 (5.0076)  class_acc: 0.0000 (0.0199)  loss_scale: 131072.0000 (160410.1771)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0881  max mem: 15572
Epoch: [1]  [ 630/2809]  eta: 0:21:51  lr: 0.000011  min_lr: 0.000000  loss: 5.0276 (5.0082)  class_acc: 0.0000 (0.0196)  loss_scale: 131072.0000 (159945.2298)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1225  max mem: 15572
Epoch: [1]  [ 640/2809]  eta: 0:21:45  lr: 0.000012  min_lr: 0.000000  loss: 5.0246 (5.0091)  class_acc: 0.0000 (0.0195)  loss_scale: 131072.0000 (159494.7894)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1113  max mem: 15572
Epoch: [1]  [ 650/2809]  eta: 0:21:39  lr: 0.000012  min_lr: 0.000000  loss: 5.0558 (5.0100)  class_acc: 0.0000 (0.0197)  loss_scale: 131072.0000 (159058.1874)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1144  max mem: 15572
Epoch: [1]  [ 660/2809]  eta: 0:21:32  lr: 0.000012  min_lr: 0.000000  loss: 5.0398 (5.0101)  class_acc: 0.0000 (0.0194)  loss_scale: 131072.0000 (158634.7958)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1109  max mem: 15572
Epoch: [1]  [ 670/2809]  eta: 0:21:24  lr: 0.000012  min_lr: 0.000000  loss: 5.0131 (5.0112)  class_acc: 0.0000 (0.0194)  loss_scale: 131072.0000 (158224.0238)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1010  max mem: 15572
Epoch: [1]  [ 680/2809]  eta: 0:21:18  lr: 0.000012  min_lr: 0.000000  loss: 5.0580 (5.0119)  class_acc: 0.0000 (0.0191)  loss_scale: 131072.0000 (157825.3157)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1249  max mem: 15572
Epoch: [1]  [ 690/2809]  eta: 0:21:09  lr: 0.000012  min_lr: 0.000000  loss: 5.0597 (5.0128)  class_acc: 0.0000 (0.0188)  loss_scale: 131072.0000 (157438.1476)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1114  max mem: 15572
[2025-01-15 14:58:48,311] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3503
[2025-01-15 14:58:48,311] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 14:58:48,311] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [ 700/2809]  eta: 0:21:04  lr: 0.000012  min_lr: 0.000000  loss: 4.9761 (5.0121)  class_acc: 0.0000 (0.0189)  loss_scale: 131072.0000 (156407.6006)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1405  max mem: 15572
Epoch: [1]  [ 710/2809]  eta: 0:20:56  lr: 0.000012  min_lr: 0.000000  loss: 4.9723 (5.0123)  class_acc: 0.0000 (0.0188)  loss_scale: 65536.0000 (155129.5190)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1329  max mem: 15572
Epoch: [1]  [ 720/2809]  eta: 0:20:49  lr: 0.000012  min_lr: 0.000000  loss: 4.9864 (5.0116)  class_acc: 0.0000 (0.0187)  loss_scale: 65536.0000 (153886.8904)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1094  max mem: 15572
Epoch: [1]  [ 730/2809]  eta: 0:20:44  lr: 0.000012  min_lr: 0.000000  loss: 4.9643 (5.0118)  class_acc: 0.0000 (0.0186)  loss_scale: 65536.0000 (152678.2599)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1487  max mem: 15572
Epoch: [1]  [ 740/2809]  eta: 0:20:36  lr: 0.000012  min_lr: 0.000000  loss: 4.9599 (5.0111)  class_acc: 0.0000 (0.0184)  loss_scale: 65536.0000 (151502.2510)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1033  max mem: 15572
Epoch: [1]  [ 750/2809]  eta: 0:20:31  lr: 0.000012  min_lr: 0.000000  loss: 4.9439 (5.0104)  class_acc: 0.0000 (0.0184)  loss_scale: 65536.0000 (150357.5606)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1065  max mem: 15572
Epoch: [1]  [ 760/2809]  eta: 0:20:27  lr: 0.000012  min_lr: 0.000000  loss: 4.9853 (5.0106)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (149242.9540)  weight_decay: 0.0500 (0.0500)  time: 0.6479  data: 0.1694  max mem: 15572
Epoch: [1]  [ 770/2809]  eta: 0:20:17  lr: 0.000012  min_lr: 0.000000  loss: 5.0200 (5.0101)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (148157.2607)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0907  max mem: 15572
Epoch: [1]  [ 780/2809]  eta: 0:20:12  lr: 0.000012  min_lr: 0.000000  loss: 4.9876 (5.0100)  class_acc: 0.0000 (0.0185)  loss_scale: 65536.0000 (147099.3700)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0995  max mem: 15572
Epoch: [1]  [ 790/2809]  eta: 0:20:04  lr: 0.000012  min_lr: 0.000000  loss: 4.9686 (5.0093)  class_acc: 0.0000 (0.0186)  loss_scale: 65536.0000 (146068.2276)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1399  max mem: 15572
Epoch: [1]  [ 800/2809]  eta: 0:19:56  lr: 0.000012  min_lr: 0.000000  loss: 4.9895 (5.0095)  class_acc: 0.0000 (0.0187)  loss_scale: 65536.0000 (145062.8315)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0849  max mem: 15572
Epoch: [1]  [ 810/2809]  eta: 0:19:51  lr: 0.000012  min_lr: 0.000000  loss: 5.0181 (5.0099)  class_acc: 0.0000 (0.0187)  loss_scale: 65536.0000 (144082.2293)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1322  max mem: 15572
Epoch: [1]  [ 820/2809]  eta: 0:19:43  lr: 0.000012  min_lr: 0.000000  loss: 5.0213 (5.0101)  class_acc: 0.0000 (0.0186)  loss_scale: 65536.0000 (143125.5152)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1274  max mem: 15572
[2025-01-15 15:00:02,512] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:00:02,512] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [ 830/2809]  eta: 0:19:36  lr: 0.000012  min_lr: 0.000000  loss: 5.0112 (5.0103)  class_acc: 0.0000 (0.0185)  loss_scale: 65536.0000 (142822.7389)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0969  max mem: 15572
Epoch: [1]  [ 840/2809]  eta: 0:19:30  lr: 0.000012  min_lr: 0.000000  loss: 5.0025 (5.0106)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (142683.0155)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1320  max mem: 15572
Epoch: [1]  [ 850/2809]  eta: 0:19:26  lr: 0.000012  min_lr: 0.000000  loss: 5.0298 (5.0110)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (142546.5758)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1737  max mem: 15572
Epoch: [1]  [ 860/2809]  eta: 0:19:18  lr: 0.000012  min_lr: 0.000000  loss: 5.0212 (5.0104)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (142413.3055)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.0998  max mem: 15572
Epoch: [1]  [ 870/2809]  eta: 0:19:12  lr: 0.000012  min_lr: 0.000000  loss: 4.9831 (5.0104)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (142283.0953)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0688  max mem: 15572
Epoch: [1]  [ 880/2809]  eta: 0:19:07  lr: 0.000012  min_lr: 0.000000  loss: 4.9791 (5.0104)  class_acc: 0.0000 (0.0186)  loss_scale: 131072.0000 (142155.8411)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1357  max mem: 15572
Epoch: [1]  [ 890/2809]  eta: 0:19:00  lr: 0.000012  min_lr: 0.000000  loss: 4.9937 (5.0106)  class_acc: 0.0000 (0.0187)  loss_scale: 131072.0000 (142031.4433)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1270  max mem: 15572
Epoch: [1]  [ 900/2809]  eta: 0:18:54  lr: 0.000012  min_lr: 0.000000  loss: 5.0044 (5.0106)  class_acc: 0.0000 (0.0186)  loss_scale: 131072.0000 (141909.8069)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1398  max mem: 15572
Epoch: [1]  [ 910/2809]  eta: 0:18:50  lr: 0.000012  min_lr: 0.000000  loss: 5.0044 (5.0112)  class_acc: 0.0000 (0.0187)  loss_scale: 131072.0000 (141790.8408)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.2036  max mem: 15572
Epoch: [1]  [ 920/2809]  eta: 0:18:44  lr: 0.000012  min_lr: 0.000000  loss: 5.0244 (5.0113)  class_acc: 0.0000 (0.0188)  loss_scale: 131072.0000 (141674.4582)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1968  max mem: 15572
Epoch: [1]  [ 930/2809]  eta: 0:18:38  lr: 0.000012  min_lr: 0.000000  loss: 5.0029 (5.0116)  class_acc: 0.0000 (0.0189)  loss_scale: 131072.0000 (141560.5757)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1386  max mem: 15572
Epoch: [1]  [ 940/2809]  eta: 0:18:32  lr: 0.000013  min_lr: 0.000000  loss: 5.0029 (5.0113)  class_acc: 0.0000 (0.0188)  loss_scale: 131072.0000 (141449.1137)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1236  max mem: 15572
Epoch: [1]  [ 950/2809]  eta: 0:18:26  lr: 0.000013  min_lr: 0.000000  loss: 4.9411 (5.0103)  class_acc: 0.0000 (0.0188)  loss_scale: 131072.0000 (141339.9958)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1246  max mem: 15572
[2025-01-15 15:01:18,950] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:01:18,951] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-15 15:01:23,550] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3768
[2025-01-15 15:01:23,551] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 15:01:23,551] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 960/2809]  eta: 0:18:22  lr: 0.000013  min_lr: 0.000000  loss: 4.9505 (5.0105)  class_acc: 0.0000 (0.0186)  loss_scale: 131072.0000 (142324.2789)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.1753  max mem: 15572
Epoch: [1]  [ 970/2809]  eta: 0:18:14  lr: 0.000013  min_lr: 0.000000  loss: 4.9516 (5.0107)  class_acc: 0.0000 (0.0184)  loss_scale: 131072.0000 (142208.3955)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1094  max mem: 15572
Epoch: [1]  [ 980/2809]  eta: 0:18:10  lr: 0.000013  min_lr: 0.000000  loss: 4.9462 (5.0102)  class_acc: 0.0000 (0.0184)  loss_scale: 131072.0000 (142094.8746)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1133  max mem: 15572
Epoch: [1]  [ 990/2809]  eta: 0:18:02  lr: 0.000013  min_lr: 0.000000  loss: 4.9308 (5.0097)  class_acc: 0.0000 (0.0184)  loss_scale: 131072.0000 (141983.6448)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1275  max mem: 15572
Epoch: [1]  [1000/2809]  eta: 0:17:57  lr: 0.000013  min_lr: 0.000000  loss: 4.9537 (5.0097)  class_acc: 0.0000 (0.0186)  loss_scale: 131072.0000 (141874.6374)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1103  max mem: 15572
Epoch: [1]  [1010/2809]  eta: 0:17:51  lr: 0.000013  min_lr: 0.000000  loss: 5.0541 (5.0106)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (141767.7864)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1718  max mem: 15572
Epoch: [1]  [1020/2809]  eta: 0:17:44  lr: 0.000013  min_lr: 0.000000  loss: 5.0541 (5.0107)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (141663.0284)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0999  max mem: 15572
Epoch: [1]  [1030/2809]  eta: 0:17:37  lr: 0.000013  min_lr: 0.000000  loss: 4.9816 (5.0104)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (141560.3026)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0865  max mem: 15572
Epoch: [1]  [1040/2809]  eta: 0:17:32  lr: 0.000013  min_lr: 0.000000  loss: 4.9882 (5.0100)  class_acc: 0.0000 (0.0186)  loss_scale: 131072.0000 (141459.5504)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1371  max mem: 15572
Epoch: [1]  [1050/2809]  eta: 0:17:25  lr: 0.000013  min_lr: 0.000000  loss: 4.9476 (5.0094)  class_acc: 0.0000 (0.0186)  loss_scale: 131072.0000 (141360.7155)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1249  max mem: 15572
Epoch: [1]  [1060/2809]  eta: 0:17:18  lr: 0.000013  min_lr: 0.000000  loss: 4.9254 (5.0088)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (141263.7436)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.1024  max mem: 15572
Epoch: [1]  [1070/2809]  eta: 0:17:12  lr: 0.000013  min_lr: 0.000000  loss: 4.9436 (5.0085)  class_acc: 0.0000 (0.0188)  loss_scale: 131072.0000 (141168.5826)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1090  max mem: 15572
Epoch: [1]  [1080/2809]  eta: 0:17:05  lr: 0.000013  min_lr: 0.000000  loss: 4.9458 (5.0079)  class_acc: 0.0000 (0.0188)  loss_scale: 131072.0000 (141075.1822)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1065  max mem: 15572
[2025-01-15 15:02:37,448] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:02:37,449] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-15 15:02:38,351] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3898
[2025-01-15 15:02:38,351] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 15:02:38,352] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [1090/2809]  eta: 0:16:58  lr: 0.000013  min_lr: 0.000000  loss: 4.9458 (5.0078)  class_acc: 0.0000 (0.0189)  loss_scale: 131072.0000 (141103.6334)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0855  max mem: 15572
Epoch: [1]  [1100/2809]  eta: 0:16:52  lr: 0.000013  min_lr: 0.000000  loss: 4.9859 (5.0079)  class_acc: 0.0000 (0.0191)  loss_scale: 131072.0000 (141012.5195)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0748  max mem: 15572
[2025-01-15 15:02:48,805] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3917
[2025-01-15 15:02:48,806] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:02:48,806] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1110/2809]  eta: 0:16:45  lr: 0.000013  min_lr: 0.000000  loss: 4.9864 (5.0077)  class_acc: 0.0000 (0.0189)  loss_scale: 131072.0000 (140746.0810)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0773  max mem: 15572
Epoch: [1]  [1120/2809]  eta: 0:16:39  lr: 0.000013  min_lr: 0.000000  loss: 5.0376 (5.0085)  class_acc: 0.0000 (0.0189)  loss_scale: 65536.0000 (140075.1615)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1074  max mem: 15572
Epoch: [1]  [1130/2809]  eta: 0:16:35  lr: 0.000013  min_lr: 0.000000  loss: 5.0151 (5.0084)  class_acc: 0.0000 (0.0188)  loss_scale: 65536.0000 (139416.1061)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.1971  max mem: 15572
Epoch: [1]  [1140/2809]  eta: 0:16:28  lr: 0.000013  min_lr: 0.000000  loss: 4.9521 (5.0083)  class_acc: 0.0000 (0.0189)  loss_scale: 65536.0000 (138768.6030)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1565  max mem: 15572
Epoch: [1]  [1150/2809]  eta: 0:16:21  lr: 0.000013  min_lr: 0.000000  loss: 5.0329 (5.0087)  class_acc: 0.0000 (0.0189)  loss_scale: 65536.0000 (138132.3510)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0397  max mem: 15572
Epoch: [1]  [1160/2809]  eta: 0:16:15  lr: 0.000013  min_lr: 0.000000  loss: 4.9974 (5.0085)  class_acc: 0.0000 (0.0190)  loss_scale: 65536.0000 (137507.0594)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0801  max mem: 15572
Epoch: [1]  [1170/2809]  eta: 0:16:09  lr: 0.000013  min_lr: 0.000000  loss: 4.9955 (5.0086)  class_acc: 0.0000 (0.0190)  loss_scale: 65536.0000 (136892.4475)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1543  max mem: 15572
Epoch: [1]  [1180/2809]  eta: 0:16:03  lr: 0.000013  min_lr: 0.000000  loss: 5.0660 (5.0092)  class_acc: 0.0000 (0.0190)  loss_scale: 65536.0000 (136288.2439)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1497  max mem: 15572
[2025-01-15 15:03:37,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=16, lr=[1.2932318270399928e-07, 1.2932318270399928e-07, 1.8474740386285612e-07, 1.8474740386285612e-07, 2.6392486266122304e-07, 2.6392486266122304e-07, 3.7703551808746154e-07, 3.7703551808746154e-07, 5.386221686963737e-07, 5.386221686963737e-07, 7.694602409948195e-07, 7.694602409948195e-07, 1.0992289157068852e-06, 1.0992289157068852e-06, 1.5703270224384075e-06, 1.5703270224384075e-06, 2.2433243177691536e-06, 2.2433243177691536e-06, 3.2047490253845054e-06, 3.2047490253845054e-06, 4.578212893406436e-06, 4.578212893406436e-06, 6.5403041334377665e-06, 6.5403041334377665e-06, 9.34329161919681e-06, 9.34329161919681e-06, 1.3347559455995444e-05, 1.3347559455995444e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:03:37,078] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=27.66177295840561, CurrSamplesPerSec=24.921604734405197, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [1190/2809]  eta: 0:15:57  lr: 0.000013  min_lr: 0.000000  loss: 5.0261 (5.0088)  class_acc: 0.0000 (0.0192)  loss_scale: 65536.0000 (135694.1864)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1408  max mem: 15572
Epoch: [1]  [1200/2809]  eta: 0:15:51  lr: 0.000013  min_lr: 0.000000  loss: 4.9520 (5.0088)  class_acc: 0.0000 (0.0195)  loss_scale: 65536.0000 (135110.0216)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1381  max mem: 15572
Epoch: [1]  [1210/2809]  eta: 0:15:44  lr: 0.000013  min_lr: 0.000000  loss: 5.0172 (5.0090)  class_acc: 0.0000 (0.0195)  loss_scale: 65536.0000 (134535.5045)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1250  max mem: 15572
Epoch: [1]  [1220/2809]  eta: 0:15:39  lr: 0.000013  min_lr: 0.000000  loss: 5.0038 (5.0089)  class_acc: 0.0000 (0.0194)  loss_scale: 65536.0000 (133970.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1341  max mem: 15572
Epoch: [1]  [1230/2809]  eta: 0:15:34  lr: 0.000013  min_lr: 0.000000  loss: 4.9966 (5.0089)  class_acc: 0.0000 (0.0195)  loss_scale: 65536.0000 (133414.4728)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1534  max mem: 15572
[2025-01-15 15:04:06,086] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:04:06,086] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:04:06,550] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4047
[2025-01-15 15:04:06,551] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:04:06,552] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1240/2809]  eta: 0:15:28  lr: 0.000014  min_lr: 0.000000  loss: 5.0160 (5.0093)  class_acc: 0.0000 (0.0193)  loss_scale: 65536.0000 (132920.3159)  weight_decay: 0.0500 (0.0500)  time: 0.6538  data: 0.1699  max mem: 15572
Epoch: [1]  [1250/2809]  eta: 0:15:22  lr: 0.000014  min_lr: 0.000000  loss: 5.0421 (5.0096)  class_acc: 0.0000 (0.0193)  loss_scale: 65536.0000 (132381.6723)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1278  max mem: 15572
Epoch: [1]  [1260/2809]  eta: 0:15:17  lr: 0.000014  min_lr: 0.000000  loss: 5.0372 (5.0094)  class_acc: 0.0000 (0.0193)  loss_scale: 65536.0000 (131851.5718)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1521  max mem: 15572
Epoch: [1]  [1270/2809]  eta: 0:15:12  lr: 0.000014  min_lr: 0.000000  loss: 4.9159 (5.0089)  class_acc: 0.0000 (0.0192)  loss_scale: 65536.0000 (131329.8127)  weight_decay: 0.0500 (0.0500)  time: 0.6950  data: 0.2262  max mem: 15572
Epoch: [1]  [1280/2809]  eta: 0:15:06  lr: 0.000014  min_lr: 0.000000  loss: 4.9472 (5.0086)  class_acc: 0.0000 (0.0191)  loss_scale: 65536.0000 (130816.1998)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.1856  max mem: 15572
Epoch: [1]  [1290/2809]  eta: 0:15:01  lr: 0.000014  min_lr: 0.000000  loss: 4.9926 (5.0089)  class_acc: 0.0000 (0.0191)  loss_scale: 65536.0000 (130310.5438)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.1717  max mem: 15572
Epoch: [1]  [1300/2809]  eta: 0:14:56  lr: 0.000014  min_lr: 0.000000  loss: 5.0478 (5.0090)  class_acc: 0.0000 (0.0190)  loss_scale: 65536.0000 (129812.6610)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.1811  max mem: 15572
Epoch: [1]  [1310/2809]  eta: 0:14:49  lr: 0.000014  min_lr: 0.000000  loss: 5.0451 (5.0094)  class_acc: 0.0000 (0.0189)  loss_scale: 65536.0000 (129322.3738)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0975  max mem: 15572
Epoch: [1]  [1320/2809]  eta: 0:14:44  lr: 0.000014  min_lr: 0.000000  loss: 5.0138 (5.0090)  class_acc: 0.0000 (0.0189)  loss_scale: 65536.0000 (128839.5095)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1047  max mem: 15572
Epoch: [1]  [1330/2809]  eta: 0:14:37  lr: 0.000014  min_lr: 0.000000  loss: 4.9547 (5.0087)  class_acc: 0.0000 (0.0189)  loss_scale: 65536.0000 (128363.9008)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1154  max mem: 15572
Epoch: [1]  [1340/2809]  eta: 0:14:32  lr: 0.000014  min_lr: 0.000000  loss: 4.9783 (5.0093)  class_acc: 0.0000 (0.0189)  loss_scale: 65536.0000 (127895.3855)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1148  max mem: 15572
Epoch: [1]  [1350/2809]  eta: 0:14:26  lr: 0.000014  min_lr: 0.000000  loss: 5.0303 (5.0093)  class_acc: 0.0000 (0.0188)  loss_scale: 65536.0000 (127433.8061)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1536  max mem: 15572
Epoch: [1]  [1360/2809]  eta: 0:14:20  lr: 0.000014  min_lr: 0.000000  loss: 5.0103 (5.0091)  class_acc: 0.0000 (0.0188)  loss_scale: 65536.0000 (126979.0096)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1634  max mem: 15572
[2025-01-15 15:05:25,856] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:05:25,857] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1370/2809]  eta: 0:14:14  lr: 0.000014  min_lr: 0.000000  loss: 4.9563 (5.0088)  class_acc: 0.0000 (0.0187)  loss_scale: 65536.0000 (126722.0540)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1830  max mem: 15572
Epoch: [1]  [1380/2809]  eta: 0:14:09  lr: 0.000014  min_lr: 0.000000  loss: 4.9563 (5.0085)  class_acc: 0.0000 (0.0189)  loss_scale: 131072.0000 (126753.5525)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.1528  max mem: 15572
[2025-01-15 15:05:34,828] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4191
[2025-01-15 15:05:34,828] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:05:34,829] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1390/2809]  eta: 0:14:02  lr: 0.000014  min_lr: 0.000000  loss: 4.9844 (5.0085)  class_acc: 0.0417 (0.0190)  loss_scale: 131072.0000 (126360.5694)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0842  max mem: 15572
Epoch: [1]  [1400/2809]  eta: 0:13:57  lr: 0.000014  min_lr: 0.000000  loss: 4.9707 (5.0081)  class_acc: 0.0417 (0.0191)  loss_scale: 65536.0000 (125926.4183)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0990  max mem: 15572
Epoch: [1]  [1410/2809]  eta: 0:13:50  lr: 0.000014  min_lr: 0.000000  loss: 4.9540 (5.0077)  class_acc: 0.0000 (0.0191)  loss_scale: 65536.0000 (125498.4210)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1290  max mem: 15572
Epoch: [1]  [1420/2809]  eta: 0:13:43  lr: 0.000014  min_lr: 0.000000  loss: 4.9225 (5.0076)  class_acc: 0.0000 (0.0191)  loss_scale: 65536.0000 (125076.4476)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0482  max mem: 15572
[2025-01-15 15:05:58,075] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4234
[2025-01-15 15:05:58,076] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:05:58,076] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [1430/2809]  eta: 0:13:38  lr: 0.000014  min_lr: 0.000000  loss: 4.9984 (5.0077)  class_acc: 0.0000 (0.0192)  loss_scale: 65536.0000 (124522.9797)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0776  max mem: 15572
Epoch: [1]  [1440/2809]  eta: 0:13:32  lr: 0.000014  min_lr: 0.000000  loss: 5.0322 (5.0083)  class_acc: 0.0000 (0.0190)  loss_scale: 32768.0000 (123886.2346)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1295  max mem: 15572
Epoch: [1]  [1450/2809]  eta: 0:13:25  lr: 0.000014  min_lr: 0.000000  loss: 4.9977 (5.0081)  class_acc: 0.0000 (0.0190)  loss_scale: 32768.0000 (123258.2660)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0688  max mem: 15572
Epoch: [1]  [1460/2809]  eta: 0:13:19  lr: 0.000014  min_lr: 0.000000  loss: 4.9736 (5.0081)  class_acc: 0.0000 (0.0190)  loss_scale: 32768.0000 (122638.8939)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0946  max mem: 15572
Epoch: [1]  [1470/2809]  eta: 0:13:12  lr: 0.000014  min_lr: 0.000000  loss: 4.9890 (5.0082)  class_acc: 0.0000 (0.0191)  loss_scale: 32768.0000 (122027.9429)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1159  max mem: 15572
Epoch: [1]  [1480/2809]  eta: 0:13:06  lr: 0.000014  min_lr: 0.000000  loss: 4.9686 (5.0081)  class_acc: 0.0000 (0.0191)  loss_scale: 32768.0000 (121425.2424)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0633  max mem: 15572
Epoch: [1]  [1490/2809]  eta: 0:13:00  lr: 0.000014  min_lr: 0.000000  loss: 4.9839 (5.0082)  class_acc: 0.0000 (0.0192)  loss_scale: 32768.0000 (120830.6264)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1241  max mem: 15572
Epoch: [1]  [1500/2809]  eta: 0:12:55  lr: 0.000014  min_lr: 0.000000  loss: 5.0461 (5.0084)  class_acc: 0.0000 (0.0192)  loss_scale: 32768.0000 (120243.9334)  weight_decay: 0.0500 (0.0500)  time: 0.6506  data: 0.1832  max mem: 15572
Epoch: [1]  [1510/2809]  eta: 0:12:49  lr: 0.000014  min_lr: 0.000000  loss: 4.9592 (5.0080)  class_acc: 0.0000 (0.0192)  loss_scale: 32768.0000 (119665.0060)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1341  max mem: 15572
Epoch: [1]  [1520/2809]  eta: 0:12:42  lr: 0.000014  min_lr: 0.000000  loss: 4.9592 (5.0082)  class_acc: 0.0000 (0.0192)  loss_scale: 32768.0000 (119093.6910)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0933  max mem: 15572
Epoch: [1]  [1530/2809]  eta: 0:12:37  lr: 0.000014  min_lr: 0.000000  loss: 5.0092 (5.0082)  class_acc: 0.0000 (0.0192)  loss_scale: 32768.0000 (118529.8393)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1130  max mem: 15572
Epoch: [1]  [1540/2809]  eta: 0:12:30  lr: 0.000015  min_lr: 0.000000  loss: 4.9976 (5.0080)  class_acc: 0.0000 (0.0192)  loss_scale: 32768.0000 (117973.3056)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1036  max mem: 15572
Epoch: [1]  [1550/2809]  eta: 0:12:24  lr: 0.000015  min_lr: 0.000000  loss: 5.0268 (5.0080)  class_acc: 0.0000 (0.0192)  loss_scale: 32768.0000 (117423.9484)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0791  max mem: 15572
[2025-01-15 15:07:12,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:07:12,576] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [1560/2809]  eta: 0:12:18  lr: 0.000015  min_lr: 0.000000  loss: 5.0578 (5.0084)  class_acc: 0.0000 (0.0194)  loss_scale: 32768.0000 (117028.5714)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0932  max mem: 15572
Epoch: [1]  [1570/2809]  eta: 0:12:13  lr: 0.000015  min_lr: 0.000000  loss: 5.0334 (5.0083)  class_acc: 0.0000 (0.0194)  loss_scale: 65536.0000 (116700.8020)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1429  max mem: 15572
Epoch: [1]  [1580/2809]  eta: 0:12:07  lr: 0.000015  min_lr: 0.000000  loss: 4.9984 (5.0086)  class_acc: 0.0000 (0.0193)  loss_scale: 65536.0000 (116377.1790)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1819  max mem: 15572
Epoch: [1]  [1590/2809]  eta: 0:12:00  lr: 0.000015  min_lr: 0.000000  loss: 5.0182 (5.0086)  class_acc: 0.0000 (0.0194)  loss_scale: 65536.0000 (116057.6241)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1188  max mem: 15572
Epoch: [1]  [1600/2809]  eta: 0:11:54  lr: 0.000015  min_lr: 0.000000  loss: 5.0281 (5.0087)  class_acc: 0.0000 (0.0193)  loss_scale: 65536.0000 (115742.0612)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0835  max mem: 15572
Epoch: [1]  [1610/2809]  eta: 0:11:49  lr: 0.000015  min_lr: 0.000000  loss: 5.0144 (5.0085)  class_acc: 0.0000 (0.0194)  loss_scale: 65536.0000 (115430.4159)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1413  max mem: 15572
Epoch: [1]  [1620/2809]  eta: 0:11:43  lr: 0.000015  min_lr: 0.000000  loss: 4.9622 (5.0083)  class_acc: 0.0000 (0.0196)  loss_scale: 65536.0000 (115122.6157)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1521  max mem: 15572
[2025-01-15 15:07:53,646] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4433
[2025-01-15 15:07:53,647] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:07:53,647] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [1630/2809]  eta: 0:11:37  lr: 0.000015  min_lr: 0.000000  loss: 4.9445 (5.0078)  class_acc: 0.0000 (0.0198)  loss_scale: 65536.0000 (114677.9546)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1290  max mem: 15572
Epoch: [1]  [1640/2809]  eta: 0:11:32  lr: 0.000015  min_lr: 0.000000  loss: 4.9885 (5.0079)  class_acc: 0.0000 (0.0198)  loss_scale: 32768.0000 (114178.8080)  weight_decay: 0.0500 (0.0500)  time: 0.6579  data: 0.2114  max mem: 15572
Epoch: [1]  [1650/2809]  eta: 0:11:26  lr: 0.000015  min_lr: 0.000000  loss: 4.9992 (5.0078)  class_acc: 0.0000 (0.0198)  loss_scale: 32768.0000 (113685.7081)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.1928  max mem: 15572
Epoch: [1]  [1660/2809]  eta: 0:11:20  lr: 0.000015  min_lr: 0.000000  loss: 4.9469 (5.0075)  class_acc: 0.0000 (0.0198)  loss_scale: 32768.0000 (113198.5455)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1048  max mem: 15572
Epoch: [1]  [1670/2809]  eta: 0:11:14  lr: 0.000015  min_lr: 0.000000  loss: 4.9677 (5.0072)  class_acc: 0.0000 (0.0199)  loss_scale: 32768.0000 (112717.2136)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0842  max mem: 15572
Epoch: [1]  [1680/2809]  eta: 0:11:09  lr: 0.000015  min_lr: 0.000000  loss: 5.0029 (5.0072)  class_acc: 0.0000 (0.0201)  loss_scale: 32768.0000 (112241.6086)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1565  max mem: 15572
Epoch: [1]  [1690/2809]  eta: 0:11:03  lr: 0.000015  min_lr: 0.000000  loss: 4.9856 (5.0070)  class_acc: 0.0000 (0.0202)  loss_scale: 32768.0000 (111771.6286)  weight_decay: 0.0500 (0.0500)  time: 0.6510  data: 0.1881  max mem: 15572
Epoch: [1]  [1700/2809]  eta: 0:10:56  lr: 0.000015  min_lr: 0.000000  loss: 4.9856 (5.0070)  class_acc: 0.0000 (0.0202)  loss_scale: 32768.0000 (111307.1746)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0991  max mem: 15572
Epoch: [1]  [1710/2809]  eta: 0:10:51  lr: 0.000015  min_lr: 0.000000  loss: 4.9890 (5.0067)  class_acc: 0.0000 (0.0202)  loss_scale: 32768.0000 (110848.1496)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1759  max mem: 15572
Epoch: [1]  [1720/2809]  eta: 0:10:45  lr: 0.000015  min_lr: 0.000000  loss: 4.9883 (5.0065)  class_acc: 0.0000 (0.0202)  loss_scale: 32768.0000 (110394.4590)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1838  max mem: 15572
Epoch: [1]  [1730/2809]  eta: 0:10:39  lr: 0.000015  min_lr: 0.000000  loss: 4.9358 (5.0061)  class_acc: 0.0000 (0.0202)  loss_scale: 32768.0000 (109946.0104)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.0983  max mem: 15572
Epoch: [1]  [1740/2809]  eta: 0:10:33  lr: 0.000015  min_lr: 0.000000  loss: 4.9730 (5.0060)  class_acc: 0.0000 (0.0202)  loss_scale: 32768.0000 (109502.7134)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1217  max mem: 15572
Epoch: [1]  [1750/2809]  eta: 0:10:27  lr: 0.000015  min_lr: 0.000000  loss: 5.0025 (5.0057)  class_acc: 0.0417 (0.0204)  loss_scale: 32768.0000 (109064.4797)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1465  max mem: 15572
[2025-01-15 15:09:12,078] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:09:12,078] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [1760/2809]  eta: 0:10:21  lr: 0.000015  min_lr: 0.000000  loss: 5.0070 (5.0059)  class_acc: 0.0000 (0.0203)  loss_scale: 32768.0000 (108780.0840)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.1280  max mem: 15572
Epoch: [1]  [1770/2809]  eta: 0:10:15  lr: 0.000015  min_lr: 0.000000  loss: 5.0110 (5.0059)  class_acc: 0.0000 (0.0203)  loss_scale: 65536.0000 (108535.9051)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1132  max mem: 15572
Epoch: [1]  [1780/2809]  eta: 0:10:09  lr: 0.000015  min_lr: 0.000000  loss: 4.9750 (5.0057)  class_acc: 0.0417 (0.0205)  loss_scale: 65536.0000 (108294.4683)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1377  max mem: 15572
Epoch: [1]  [1790/2809]  eta: 0:10:03  lr: 0.000015  min_lr: 0.000000  loss: 4.9983 (5.0057)  class_acc: 0.0417 (0.0205)  loss_scale: 65536.0000 (108055.7275)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0879  max mem: 15572
Epoch: [1]  [1800/2809]  eta: 0:09:57  lr: 0.000015  min_lr: 0.000000  loss: 4.9768 (5.0056)  class_acc: 0.0000 (0.0205)  loss_scale: 65536.0000 (107819.6380)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.0896  max mem: 15572
Epoch: [1]  [1810/2809]  eta: 0:09:51  lr: 0.000015  min_lr: 0.000000  loss: 4.9466 (5.0055)  class_acc: 0.0000 (0.0205)  loss_scale: 65536.0000 (107586.1557)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1205  max mem: 15572
Epoch: [1]  [1820/2809]  eta: 0:09:45  lr: 0.000015  min_lr: 0.000000  loss: 4.9776 (5.0058)  class_acc: 0.0000 (0.0204)  loss_scale: 65536.0000 (107355.2378)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1087  max mem: 15572
Epoch: [1]  [1830/2809]  eta: 0:09:39  lr: 0.000015  min_lr: 0.000000  loss: 5.0259 (5.0059)  class_acc: 0.0000 (0.0205)  loss_scale: 65536.0000 (107126.8422)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1278  max mem: 15572
Epoch: [1]  [1840/2809]  eta: 0:09:33  lr: 0.000016  min_lr: 0.000000  loss: 5.0196 (5.0058)  class_acc: 0.0000 (0.0206)  loss_scale: 65536.0000 (106900.9278)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1196  max mem: 15572
Epoch: [1]  [1850/2809]  eta: 0:09:27  lr: 0.000016  min_lr: 0.000000  loss: 4.9648 (5.0056)  class_acc: 0.0417 (0.0206)  loss_scale: 65536.0000 (106677.4543)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1287  max mem: 15572
Epoch: [1]  [1860/2809]  eta: 0:09:21  lr: 0.000016  min_lr: 0.000000  loss: 4.9648 (5.0059)  class_acc: 0.0000 (0.0206)  loss_scale: 65536.0000 (106456.3826)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1316  max mem: 15572
[2025-01-15 15:10:19,144] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4675
[2025-01-15 15:10:19,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:10:19,145] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [1870/2809]  eta: 0:09:16  lr: 0.000016  min_lr: 0.000000  loss: 5.0077 (5.0057)  class_acc: 0.0000 (0.0207)  loss_scale: 65536.0000 (106150.1058)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1396  max mem: 15572
Epoch: [1]  [1880/2809]  eta: 0:09:10  lr: 0.000016  min_lr: 0.000000  loss: 5.0069 (5.0058)  class_acc: 0.0000 (0.0207)  loss_scale: 32768.0000 (105759.9830)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1707  max mem: 15572
Epoch: [1]  [1890/2809]  eta: 0:09:04  lr: 0.000016  min_lr: 0.000000  loss: 5.0090 (5.0057)  class_acc: 0.0000 (0.0207)  loss_scale: 32768.0000 (105373.9863)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1510  max mem: 15572
Epoch: [1]  [1900/2809]  eta: 0:08:57  lr: 0.000016  min_lr: 0.000000  loss: 5.0144 (5.0058)  class_acc: 0.0000 (0.0208)  loss_scale: 32768.0000 (104992.0505)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0657  max mem: 15572
Epoch: [1]  [1910/2809]  eta: 0:08:51  lr: 0.000016  min_lr: 0.000000  loss: 5.0223 (5.0059)  class_acc: 0.0000 (0.0208)  loss_scale: 32768.0000 (104614.1120)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0626  max mem: 15572
Epoch: [1]  [1920/2809]  eta: 0:08:45  lr: 0.000016  min_lr: 0.000000  loss: 4.9972 (5.0058)  class_acc: 0.0000 (0.0209)  loss_scale: 32768.0000 (104240.1083)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0988  max mem: 15572
Epoch: [1]  [1930/2809]  eta: 0:08:39  lr: 0.000016  min_lr: 0.000000  loss: 5.0190 (5.0059)  class_acc: 0.0417 (0.0209)  loss_scale: 32768.0000 (103869.9782)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1051  max mem: 15572
Epoch: [1]  [1940/2809]  eta: 0:08:33  lr: 0.000016  min_lr: 0.000000  loss: 5.0530 (5.0059)  class_acc: 0.0417 (0.0211)  loss_scale: 32768.0000 (103503.6620)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1314  max mem: 15572
Epoch: [1]  [1950/2809]  eta: 0:08:28  lr: 0.000016  min_lr: 0.000000  loss: 4.9981 (5.0056)  class_acc: 0.0417 (0.0214)  loss_scale: 32768.0000 (103141.1010)  weight_decay: 0.0500 (0.0500)  time: 0.6485  data: 0.2073  max mem: 15572
Epoch: [1]  [1960/2809]  eta: 0:08:22  lr: 0.000016  min_lr: 0.000000  loss: 4.9962 (5.0056)  class_acc: 0.0000 (0.0213)  loss_scale: 32768.0000 (102782.2376)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1696  max mem: 15572
Epoch: [1]  [1970/2809]  eta: 0:08:16  lr: 0.000016  min_lr: 0.000000  loss: 4.9863 (5.0053)  class_acc: 0.0000 (0.0214)  loss_scale: 32768.0000 (102427.0157)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.1121  max mem: 15572
Epoch: [1]  [1980/2809]  eta: 0:08:10  lr: 0.000016  min_lr: 0.000000  loss: 4.9827 (5.0054)  class_acc: 0.0000 (0.0215)  loss_scale: 32768.0000 (102075.3801)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1235  max mem: 15572
Epoch: [1]  [1990/2809]  eta: 0:08:03  lr: 0.000016  min_lr: 0.000000  loss: 5.0232 (5.0054)  class_acc: 0.0417 (0.0216)  loss_scale: 32768.0000 (101727.2767)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0960  max mem: 15572
[2025-01-15 15:11:31,602] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:11:31,602] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2000/2809]  eta: 0:07:58  lr: 0.000016  min_lr: 0.000000  loss: 4.9711 (5.0052)  class_acc: 0.0417 (0.0217)  loss_scale: 32768.0000 (101480.9075)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1433  max mem: 15572
Epoch: [1]  [2010/2809]  eta: 0:07:51  lr: 0.000016  min_lr: 0.000000  loss: 4.9718 (5.0053)  class_acc: 0.0000 (0.0217)  loss_scale: 65536.0000 (101302.1661)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1090  max mem: 15572
Epoch: [1]  [2020/2809]  eta: 0:07:45  lr: 0.000016  min_lr: 0.000000  loss: 4.9995 (5.0053)  class_acc: 0.0000 (0.0218)  loss_scale: 65536.0000 (101125.1935)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0947  max mem: 15572
Epoch: [1]  [2030/2809]  eta: 0:07:40  lr: 0.000016  min_lr: 0.000000  loss: 4.9995 (5.0052)  class_acc: 0.0000 (0.0218)  loss_scale: 65536.0000 (100949.9636)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1469  max mem: 15572
Epoch: [1]  [2040/2809]  eta: 0:07:34  lr: 0.000016  min_lr: 0.000000  loss: 4.9840 (5.0050)  class_acc: 0.0000 (0.0219)  loss_scale: 65536.0000 (100776.4508)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1173  max mem: 15572
Epoch: [1]  [2050/2809]  eta: 0:07:28  lr: 0.000016  min_lr: 0.000000  loss: 4.9841 (5.0052)  class_acc: 0.0000 (0.0219)  loss_scale: 65536.0000 (100604.6299)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1248  max mem: 15572
Epoch: [1]  [2060/2809]  eta: 0:07:22  lr: 0.000016  min_lr: 0.000000  loss: 5.0404 (5.0053)  class_acc: 0.0417 (0.0222)  loss_scale: 65536.0000 (100434.4765)  weight_decay: 0.0500 (0.0500)  time: 0.6411  data: 0.1572  max mem: 15572
Epoch: [1]  [2070/2809]  eta: 0:07:16  lr: 0.000016  min_lr: 0.000000  loss: 5.0157 (5.0053)  class_acc: 0.0000 (0.0221)  loss_scale: 65536.0000 (100265.9662)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1263  max mem: 15572
[2025-01-15 15:12:17,910] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4882
[2025-01-15 15:12:17,910] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:12:17,910] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [2080/2809]  eta: 0:07:10  lr: 0.000016  min_lr: 0.000000  loss: 4.9952 (5.0052)  class_acc: 0.0000 (0.0220)  loss_scale: 65536.0000 (99973.1052)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1223  max mem: 15572
Epoch: [1]  [2090/2809]  eta: 0:07:04  lr: 0.000016  min_lr: 0.000000  loss: 4.9684 (5.0051)  class_acc: 0.0000 (0.0221)  loss_scale: 32768.0000 (99651.7035)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1390  max mem: 15572
Epoch: [1]  [2100/2809]  eta: 0:06:58  lr: 0.000016  min_lr: 0.000000  loss: 4.9863 (5.0052)  class_acc: 0.0417 (0.0222)  loss_scale: 32768.0000 (99333.3613)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1148  max mem: 15572
Epoch: [1]  [2110/2809]  eta: 0:06:52  lr: 0.000016  min_lr: 0.000000  loss: 5.0441 (5.0053)  class_acc: 0.0000 (0.0222)  loss_scale: 32768.0000 (99018.0351)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1411  max mem: 15572
Epoch: [1]  [2120/2809]  eta: 0:06:46  lr: 0.000016  min_lr: 0.000000  loss: 5.0017 (5.0051)  class_acc: 0.0000 (0.0222)  loss_scale: 32768.0000 (98705.6822)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1585  max mem: 15572
Epoch: [1]  [2130/2809]  eta: 0:06:41  lr: 0.000016  min_lr: 0.000000  loss: 5.0162 (5.0054)  class_acc: 0.0000 (0.0222)  loss_scale: 32768.0000 (98396.2609)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.1701  max mem: 15572
Epoch: [1]  [2140/2809]  eta: 0:06:35  lr: 0.000017  min_lr: 0.000000  loss: 5.0122 (5.0052)  class_acc: 0.0000 (0.0222)  loss_scale: 32768.0000 (98089.7300)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1331  max mem: 15572
Epoch: [1]  [2150/2809]  eta: 0:06:29  lr: 0.000017  min_lr: 0.000000  loss: 4.9888 (5.0054)  class_acc: 0.0417 (0.0223)  loss_scale: 32768.0000 (97786.0493)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0802  max mem: 15572
Epoch: [1]  [2160/2809]  eta: 0:06:23  lr: 0.000017  min_lr: 0.000000  loss: 5.0542 (5.0057)  class_acc: 0.0000 (0.0223)  loss_scale: 32768.0000 (97485.1791)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1206  max mem: 15572
Epoch: [1]  [2170/2809]  eta: 0:06:17  lr: 0.000017  min_lr: 0.000000  loss: 4.9861 (5.0056)  class_acc: 0.0000 (0.0223)  loss_scale: 32768.0000 (97187.0806)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0816  max mem: 15572
Epoch: [1]  [2180/2809]  eta: 0:06:11  lr: 0.000017  min_lr: 0.000000  loss: 4.9918 (5.0056)  class_acc: 0.0000 (0.0223)  loss_scale: 32768.0000 (96891.7157)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0542  max mem: 15572
[2025-01-15 15:13:26,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=22, lr=[1.6166206310009812e-07, 1.6166206310009812e-07, 2.3094580442871163e-07, 2.3094580442871163e-07, 3.2992257775530236e-07, 3.2992257775530236e-07, 4.7131796822186054e-07, 4.7131796822186054e-07, 6.733113831740865e-07, 6.733113831740865e-07, 9.618734045344093e-07, 9.618734045344093e-07, 1.3741048636205847e-06, 1.3741048636205847e-06, 1.963006948029407e-06, 1.963006948029407e-06, 2.80429564004201e-06, 2.80429564004201e-06, 4.0061366286314435e-06, 4.0061366286314435e-06, 5.723052326616347e-06, 5.723052326616347e-06, 8.175789038023355e-06, 8.175789038023355e-06, 1.167969862574765e-05, 1.167969862574765e-05, 1.6685283751068073e-05, 1.6685283751068073e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:13:26,418] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=27.63640444223007, CurrSamplesPerSec=29.14495564992918, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [2190/2809]  eta: 0:06:05  lr: 0.000017  min_lr: 0.000000  loss: 5.0361 (5.0057)  class_acc: 0.0000 (0.0223)  loss_scale: 32768.0000 (96599.0470)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1278  max mem: 15572
Epoch: [1]  [2200/2809]  eta: 0:05:59  lr: 0.000017  min_lr: 0.000000  loss: 4.9787 (5.0055)  class_acc: 0.0000 (0.0223)  loss_scale: 32768.0000 (96309.0377)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1520  max mem: 15572
[2025-01-15 15:13:33,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:13:33,265] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2210/2809]  eta: 0:05:53  lr: 0.000017  min_lr: 0.000000  loss: 4.9436 (5.0051)  class_acc: 0.0000 (0.0224)  loss_scale: 32768.0000 (96155.0357)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1542  max mem: 15572
Epoch: [1]  [2220/2809]  eta: 0:05:47  lr: 0.000017  min_lr: 0.000000  loss: 4.9339 (5.0050)  class_acc: 0.0000 (0.0224)  loss_scale: 65536.0000 (96017.1742)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1208  max mem: 15572
Epoch: [1]  [2230/2809]  eta: 0:05:41  lr: 0.000017  min_lr: 0.000000  loss: 4.9512 (5.0048)  class_acc: 0.0000 (0.0224)  loss_scale: 65536.0000 (95880.5486)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0690  max mem: 15572
Epoch: [1]  [2240/2809]  eta: 0:05:35  lr: 0.000017  min_lr: 0.000000  loss: 4.9931 (5.0047)  class_acc: 0.0000 (0.0225)  loss_scale: 65536.0000 (95745.1423)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0495  max mem: 15572
Epoch: [1]  [2250/2809]  eta: 0:05:29  lr: 0.000017  min_lr: 0.000000  loss: 4.9370 (5.0044)  class_acc: 0.0000 (0.0225)  loss_scale: 65536.0000 (95610.9391)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1019  max mem: 15572
Epoch: [1]  [2260/2809]  eta: 0:05:23  lr: 0.000017  min_lr: 0.000000  loss: 4.9250 (5.0043)  class_acc: 0.0000 (0.0225)  loss_scale: 65536.0000 (95477.9230)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0853  max mem: 15572
Epoch: [1]  [2270/2809]  eta: 0:05:17  lr: 0.000017  min_lr: 0.000000  loss: 4.9306 (5.0039)  class_acc: 0.0000 (0.0225)  loss_scale: 65536.0000 (95346.0784)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0698  max mem: 15572
[2025-01-15 15:14:17,028] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5087
[2025-01-15 15:14:17,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:14:17,029] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [2280/2809]  eta: 0:05:11  lr: 0.000017  min_lr: 0.000000  loss: 4.9425 (5.0039)  class_acc: 0.0417 (0.0226)  loss_scale: 65536.0000 (95172.2929)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1177  max mem: 15572
Epoch: [1]  [2290/2809]  eta: 0:05:06  lr: 0.000017  min_lr: 0.000000  loss: 4.9698 (5.0037)  class_acc: 0.0417 (0.0228)  loss_scale: 32768.0000 (94899.9040)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1482  max mem: 15572
Epoch: [1]  [2300/2809]  eta: 0:05:00  lr: 0.000017  min_lr: 0.000000  loss: 4.9985 (5.0036)  class_acc: 0.0000 (0.0228)  loss_scale: 32768.0000 (94629.8827)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1470  max mem: 15572
Epoch: [1]  [2310/2809]  eta: 0:04:54  lr: 0.000017  min_lr: 0.000000  loss: 5.0173 (5.0035)  class_acc: 0.0000 (0.0228)  loss_scale: 32768.0000 (94362.1982)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1443  max mem: 15572
Epoch: [1]  [2320/2809]  eta: 0:04:48  lr: 0.000017  min_lr: 0.000000  loss: 4.9702 (5.0033)  class_acc: 0.0000 (0.0229)  loss_scale: 32768.0000 (94096.8203)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.1075  max mem: 15572
Epoch: [1]  [2330/2809]  eta: 0:04:42  lr: 0.000017  min_lr: 0.000000  loss: 4.9710 (5.0034)  class_acc: 0.0000 (0.0229)  loss_scale: 32768.0000 (93833.7194)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1145  max mem: 15572
Epoch: [1]  [2340/2809]  eta: 0:04:36  lr: 0.000017  min_lr: 0.000000  loss: 4.9716 (5.0033)  class_acc: 0.0000 (0.0229)  loss_scale: 32768.0000 (93572.8663)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1667  max mem: 15572
Epoch: [1]  [2350/2809]  eta: 0:04:30  lr: 0.000017  min_lr: 0.000000  loss: 4.9851 (5.0034)  class_acc: 0.0000 (0.0230)  loss_scale: 32768.0000 (93314.2322)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0634  max mem: 15572
Epoch: [1]  [2360/2809]  eta: 0:04:24  lr: 0.000017  min_lr: 0.000000  loss: 4.9793 (5.0034)  class_acc: 0.0000 (0.0230)  loss_scale: 32768.0000 (93057.7891)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0853  max mem: 15572
Epoch: [1]  [2370/2809]  eta: 0:04:18  lr: 0.000017  min_lr: 0.000000  loss: 4.9728 (5.0033)  class_acc: 0.0000 (0.0230)  loss_scale: 32768.0000 (92803.5091)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0996  max mem: 15572
Epoch: [1]  [2380/2809]  eta: 0:04:12  lr: 0.000017  min_lr: 0.000000  loss: 4.9360 (5.0029)  class_acc: 0.0417 (0.0232)  loss_scale: 32768.0000 (92551.3650)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0774  max mem: 15572
Epoch: [1]  [2390/2809]  eta: 0:04:06  lr: 0.000017  min_lr: 0.000000  loss: 4.9069 (5.0025)  class_acc: 0.0833 (0.0234)  loss_scale: 32768.0000 (92301.3300)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0991  max mem: 15572
Epoch: [1]  [2400/2809]  eta: 0:04:00  lr: 0.000017  min_lr: 0.000000  loss: 4.9290 (5.0022)  class_acc: 0.0417 (0.0234)  loss_scale: 32768.0000 (92053.3778)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0801  max mem: 15572
[2025-01-15 15:15:30,371] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:15:30,371] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2410/2809]  eta: 0:03:54  lr: 0.000017  min_lr: 0.000000  loss: 4.9536 (5.0022)  class_acc: 0.0000 (0.0234)  loss_scale: 32768.0000 (91861.8465)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0879  max mem: 15572
Epoch: [1]  [2420/2809]  eta: 0:03:48  lr: 0.000017  min_lr: 0.000000  loss: 4.9697 (5.0022)  class_acc: 0.0000 (0.0234)  loss_scale: 65536.0000 (91753.1070)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1124  max mem: 15572
Epoch: [1]  [2430/2809]  eta: 0:03:42  lr: 0.000017  min_lr: 0.000000  loss: 4.9697 (5.0022)  class_acc: 0.0417 (0.0235)  loss_scale: 65536.0000 (91645.2620)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0921  max mem: 15572
Epoch: [1]  [2440/2809]  eta: 0:03:37  lr: 0.000018  min_lr: 0.000000  loss: 4.9562 (5.0020)  class_acc: 0.0000 (0.0235)  loss_scale: 65536.0000 (91538.3007)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0789  max mem: 15572
Epoch: [1]  [2450/2809]  eta: 0:03:31  lr: 0.000018  min_lr: 0.000000  loss: 4.9562 (5.0021)  class_acc: 0.0000 (0.0235)  loss_scale: 65536.0000 (91432.2122)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1532  max mem: 15572
Epoch: [1]  [2460/2809]  eta: 0:03:25  lr: 0.000018  min_lr: 0.000000  loss: 5.0382 (5.0020)  class_acc: 0.0000 (0.0235)  loss_scale: 65536.0000 (91326.9858)  weight_decay: 0.0500 (0.0500)  time: 0.6588  data: 0.1871  max mem: 15572
Epoch: [1]  [2470/2809]  eta: 0:03:19  lr: 0.000018  min_lr: 0.000000  loss: 4.9510 (5.0017)  class_acc: 0.0000 (0.0235)  loss_scale: 65536.0000 (91222.6111)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0992  max mem: 15572
Epoch: [1]  [2480/2809]  eta: 0:03:13  lr: 0.000018  min_lr: 0.000000  loss: 4.9500 (5.0016)  class_acc: 0.0000 (0.0236)  loss_scale: 65536.0000 (91119.0778)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0699  max mem: 15572
Epoch: [1]  [2490/2809]  eta: 0:03:07  lr: 0.000018  min_lr: 0.000000  loss: 4.9500 (5.0014)  class_acc: 0.0417 (0.0237)  loss_scale: 65536.0000 (91016.3758)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1568  max mem: 15572
Epoch: [1]  [2500/2809]  eta: 0:03:01  lr: 0.000018  min_lr: 0.000000  loss: 4.9679 (5.0014)  class_acc: 0.0417 (0.0237)  loss_scale: 65536.0000 (90914.4950)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1426  max mem: 15572
Epoch: [1]  [2510/2809]  eta: 0:02:55  lr: 0.000018  min_lr: 0.000000  loss: 4.9937 (5.0014)  class_acc: 0.0417 (0.0238)  loss_scale: 65536.0000 (90813.4257)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1180  max mem: 15572
Epoch: [1]  [2520/2809]  eta: 0:02:50  lr: 0.000018  min_lr: 0.000000  loss: 4.9837 (5.0012)  class_acc: 0.0000 (0.0238)  loss_scale: 65536.0000 (90713.1583)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1656  max mem: 15572
Epoch: [1]  [2530/2809]  eta: 0:02:44  lr: 0.000018  min_lr: 0.000000  loss: 4.9659 (5.0011)  class_acc: 0.0000 (0.0238)  loss_scale: 65536.0000 (90613.6831)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1699  max mem: 15572
[2025-01-15 15:16:45,739] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:16:45,739] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2540/2809]  eta: 0:02:38  lr: 0.000018  min_lr: 0.000000  loss: 5.0029 (5.0012)  class_acc: 0.0000 (0.0237)  loss_scale: 65536.0000 (90669.7395)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1195  max mem: 15572
[2025-01-15 15:16:54,538] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5358
[2025-01-15 15:16:54,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:16:54,538] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2550/2809]  eta: 0:02:32  lr: 0.000018  min_lr: 0.000000  loss: 5.0029 (5.0011)  class_acc: 0.0000 (0.0237)  loss_scale: 131072.0000 (90776.7370)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1225  max mem: 15572
Epoch: [1]  [2560/2809]  eta: 0:02:26  lr: 0.000018  min_lr: 0.000000  loss: 4.9901 (5.0011)  class_acc: 0.0000 (0.0238)  loss_scale: 65536.0000 (90678.1788)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1753  max mem: 15572
Epoch: [1]  [2570/2809]  eta: 0:02:20  lr: 0.000018  min_lr: 0.000000  loss: 4.9734 (5.0009)  class_acc: 0.0417 (0.0238)  loss_scale: 65536.0000 (90580.3874)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1797  max mem: 15572
Epoch: [1]  [2580/2809]  eta: 0:02:14  lr: 0.000018  min_lr: 0.000000  loss: 4.9370 (5.0007)  class_acc: 0.0000 (0.0239)  loss_scale: 65536.0000 (90483.3537)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1615  max mem: 15572
Epoch: [1]  [2590/2809]  eta: 0:02:09  lr: 0.000018  min_lr: 0.000000  loss: 4.9732 (5.0010)  class_acc: 0.0000 (0.0239)  loss_scale: 65536.0000 (90387.0691)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1491  max mem: 15572
Epoch: [1]  [2600/2809]  eta: 0:02:03  lr: 0.000018  min_lr: 0.000000  loss: 5.0188 (5.0010)  class_acc: 0.0000 (0.0240)  loss_scale: 65536.0000 (90291.5248)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0806  max mem: 15572
Epoch: [1]  [2610/2809]  eta: 0:01:57  lr: 0.000018  min_lr: 0.000000  loss: 4.9838 (5.0009)  class_acc: 0.0000 (0.0240)  loss_scale: 65536.0000 (90196.7124)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0649  max mem: 15572
Epoch: [1]  [2620/2809]  eta: 0:01:51  lr: 0.000018  min_lr: 0.000000  loss: 4.9418 (5.0007)  class_acc: 0.0000 (0.0241)  loss_scale: 65536.0000 (90102.6234)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0979  max mem: 15572
Epoch: [1]  [2630/2809]  eta: 0:01:45  lr: 0.000018  min_lr: 0.000000  loss: 4.9418 (5.0005)  class_acc: 0.0417 (0.0243)  loss_scale: 65536.0000 (90009.2497)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1172  max mem: 15572
Epoch: [1]  [2640/2809]  eta: 0:01:39  lr: 0.000018  min_lr: 0.000000  loss: 4.9563 (5.0003)  class_acc: 0.0000 (0.0243)  loss_scale: 65536.0000 (89916.5831)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1338  max mem: 15572
Epoch: [1]  [2650/2809]  eta: 0:01:33  lr: 0.000018  min_lr: 0.000000  loss: 4.9322 (5.0001)  class_acc: 0.0000 (0.0244)  loss_scale: 65536.0000 (89824.6156)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1272  max mem: 15572
Epoch: [1]  [2660/2809]  eta: 0:01:27  lr: 0.000018  min_lr: 0.000000  loss: 4.9264 (4.9999)  class_acc: 0.0000 (0.0244)  loss_scale: 65536.0000 (89733.3393)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1195  max mem: 15572
Epoch: [1]  [2670/2809]  eta: 0:01:21  lr: 0.000018  min_lr: 0.000000  loss: 4.9426 (4.9997)  class_acc: 0.0417 (0.0245)  loss_scale: 65536.0000 (89642.7465)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1017  max mem: 15572
[2025-01-15 15:18:09,394] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:18:09,395] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2680/2809]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 4.9846 (4.9997)  class_acc: 0.0417 (0.0247)  loss_scale: 65536.0000 (89626.1634)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0659  max mem: 15572
[2025-01-15 15:18:14,965] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5495
[2025-01-15 15:18:14,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:18:14,966] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2690/2809]  eta: 0:01:10  lr: 0.000018  min_lr: 0.000000  loss: 4.9846 (4.9996)  class_acc: 0.0417 (0.0247)  loss_scale: 65536.0000 (89658.4110)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1193  max mem: 15572
Epoch: [1]  [2700/2809]  eta: 0:01:04  lr: 0.000018  min_lr: 0.000000  loss: 4.9425 (4.9993)  class_acc: 0.0417 (0.0249)  loss_scale: 65536.0000 (89569.1018)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1614  max mem: 15572
Epoch: [1]  [2710/2809]  eta: 0:00:58  lr: 0.000018  min_lr: 0.000000  loss: 4.9245 (4.9991)  class_acc: 0.0417 (0.0249)  loss_scale: 65536.0000 (89480.4515)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1346  max mem: 15572
Epoch: [1]  [2720/2809]  eta: 0:00:52  lr: 0.000018  min_lr: 0.000000  loss: 4.9026 (4.9987)  class_acc: 0.0000 (0.0249)  loss_scale: 65536.0000 (89392.4528)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1550  max mem: 15572
Epoch: [1]  [2730/2809]  eta: 0:00:46  lr: 0.000018  min_lr: 0.000000  loss: 4.9026 (4.9986)  class_acc: 0.0000 (0.0249)  loss_scale: 65536.0000 (89305.0985)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.1925  max mem: 15572
Epoch: [1]  [2740/2809]  eta: 0:00:40  lr: 0.000019  min_lr: 0.000000  loss: 4.9567 (4.9986)  class_acc: 0.0417 (0.0251)  loss_scale: 65536.0000 (89218.3816)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1608  max mem: 15572
Epoch: [1]  [2750/2809]  eta: 0:00:34  lr: 0.000019  min_lr: 0.000000  loss: 4.9756 (4.9986)  class_acc: 0.0417 (0.0251)  loss_scale: 65536.0000 (89132.2952)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1128  max mem: 15572
Epoch: [1]  [2760/2809]  eta: 0:00:28  lr: 0.000019  min_lr: 0.000000  loss: 4.9291 (4.9983)  class_acc: 0.0417 (0.0252)  loss_scale: 65536.0000 (89046.8323)  weight_decay: 0.0500 (0.0500)  time: 0.6518  data: 0.1605  max mem: 15572
Epoch: [1]  [2770/2809]  eta: 0:00:22  lr: 0.000019  min_lr: 0.000000  loss: 4.9327 (4.9981)  class_acc: 0.0417 (0.0252)  loss_scale: 65536.0000 (88961.9863)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0932  max mem: 15572
Epoch: [1]  [2780/2809]  eta: 0:00:17  lr: 0.000019  min_lr: 0.000000  loss: 4.9371 (4.9979)  class_acc: 0.0417 (0.0254)  loss_scale: 65536.0000 (88877.7504)  weight_decay: 0.0500 (0.0500)  time: 0.4875  data: 0.0041  max mem: 15572
Epoch: [1]  [2790/2809]  eta: 0:00:11  lr: 0.000019  min_lr: 0.000000  loss: 4.9120 (4.9979)  class_acc: 0.0000 (0.0254)  loss_scale: 65536.0000 (88794.1182)  weight_decay: 0.0500 (0.0500)  time: 0.4926  data: 0.0374  max mem: 15572
Epoch: [1]  [2800/2809]  eta: 0:00:05  lr: 0.000019  min_lr: 0.000000  loss: 4.9565 (4.9977)  class_acc: 0.0000 (0.0254)  loss_scale: 65536.0000 (88711.0832)  weight_decay: 0.0500 (0.0500)  time: 0.4498  data: 0.0340  max mem: 15572
Epoch: [1]  [2808/2809]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 4.9741 (4.9978)  class_acc: 0.0000 (0.0254)  loss_scale: 65536.0000 (88645.0808)  weight_decay: 0.0500 (0.0500)  time: 0.4057  data: 0.0005  max mem: 15572
Epoch: [1] Total time: 0:27:29 (0.5872 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 4.9741 (4.9978)  class_acc: 0.0000 (0.0254)  loss_scale: 65536.0000 (88645.0808)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:30:10  loss: 5.0328 (5.0328)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 6.6553  data: 6.4663  max mem: 15572
Val:  [ 10/272]  eta: 0:04:10  loss: 5.1842 (5.0422)  acc1: 0.0000 (5.5556)  acc5: 0.0000 (18.1818)  time: 0.9551  data: 0.7527  max mem: 15572
Val:  [ 20/272]  eta: 0:02:45  loss: 5.0004 (4.9486)  acc1: 0.0000 (5.5556)  acc5: 0.0000 (18.2540)  time: 0.3570  data: 0.1556  max mem: 15572
Val:  [ 30/272]  eta: 0:02:14  loss: 4.9538 (4.9100)  acc1: 0.0000 (3.7634)  acc5: 0.0000 (17.5627)  time: 0.3366  data: 0.1230  max mem: 15572
Val:  [ 40/272]  eta: 0:02:02  loss: 4.5781 (4.8132)  acc1: 0.0000 (8.2656)  acc5: 27.7778 (28.7263)  time: 0.3897  data: 0.1758  max mem: 15572
Val:  [ 50/272]  eta: 0:01:51  loss: 4.5675 (4.8342)  acc1: 0.0000 (6.6449)  acc5: 0.0000 (23.2026)  time: 0.4153  data: 0.2123  max mem: 15572
Val:  [ 60/272]  eta: 0:01:42  loss: 4.6471 (4.8322)  acc1: 0.0000 (5.5556)  acc5: 0.0000 (19.3989)  time: 0.3917  data: 0.1806  max mem: 15572
Val:  [ 70/272]  eta: 0:01:33  loss: 4.7478 (4.8175)  acc1: 0.0000 (6.1033)  acc5: 0.0000 (20.3443)  time: 0.3643  data: 0.1564  max mem: 15572
Val:  [ 80/272]  eta: 0:01:25  loss: 4.8676 (4.7999)  acc1: 0.0000 (7.8189)  acc5: 0.0000 (21.6049)  time: 0.3406  data: 0.1397  max mem: 15572
Val:  [ 90/272]  eta: 0:01:20  loss: 4.9499 (4.8329)  acc1: 0.0000 (6.9597)  acc5: 0.0000 (19.2308)  time: 0.3668  data: 0.1724  max mem: 15572
Val:  [100/272]  eta: 0:01:15  loss: 4.9499 (4.8535)  acc1: 0.0000 (6.3256)  acc5: 0.0000 (18.8119)  time: 0.3911  data: 0.1821  max mem: 15572
Val:  [110/272]  eta: 0:01:09  loss: 4.9434 (4.8778)  acc1: 0.0000 (5.7558)  acc5: 0.0000 (17.1171)  time: 0.3622  data: 0.1457  max mem: 15572
Val:  [120/272]  eta: 0:01:03  loss: 5.0729 (4.9001)  acc1: 0.0000 (5.2801)  acc5: 0.0000 (15.7025)  time: 0.3275  data: 0.1116  max mem: 15572
Val:  [130/272]  eta: 0:00:58  loss: 5.0030 (4.8888)  acc1: 0.0000 (6.4037)  acc5: 0.0000 (18.7447)  time: 0.3390  data: 0.1152  max mem: 15572
Val:  [140/272]  eta: 0:00:54  loss: 4.5942 (4.8804)  acc1: 0.0000 (7.6832)  acc5: 0.0000 (19.3065)  time: 0.3570  data: 0.1482  max mem: 15572
Val:  [150/272]  eta: 0:00:49  loss: 4.8045 (4.8773)  acc1: 0.0000 (7.1744)  acc5: 0.0000 (18.2487)  time: 0.3253  data: 0.1265  max mem: 15572
Val:  [160/272]  eta: 0:00:45  loss: 4.5729 (4.8585)  acc1: 0.0000 (7.5914)  acc5: 5.5556 (19.0821)  time: 0.3498  data: 0.1504  max mem: 15572
Val:  [170/272]  eta: 0:00:40  loss: 4.6187 (4.8717)  acc1: 0.0000 (7.2775)  acc5: 0.0000 (18.1287)  time: 0.3732  data: 0.1804  max mem: 15572
Val:  [180/272]  eta: 0:00:36  loss: 4.9442 (4.8711)  acc1: 0.0000 (6.8754)  acc5: 0.0000 (17.1271)  time: 0.3499  data: 0.1587  max mem: 15572
Val:  [190/272]  eta: 0:00:32  loss: 4.9295 (4.8784)  acc1: 0.0000 (6.5154)  acc5: 0.0000 (16.2304)  time: 0.3629  data: 0.1689  max mem: 15572
Val:  [200/272]  eta: 0:00:28  loss: 4.9737 (4.8895)  acc1: 0.0000 (6.1913)  acc5: 0.0000 (15.4229)  time: 0.3520  data: 0.1608  max mem: 15572
Val:  [210/272]  eta: 0:00:24  loss: 4.9746 (4.8965)  acc1: 0.0000 (5.8978)  acc5: 0.0000 (14.7446)  time: 0.3471  data: 0.1530  max mem: 15572
Val:  [220/272]  eta: 0:00:20  loss: 4.8989 (4.8895)  acc1: 0.0000 (5.6310)  acc5: 0.0000 (14.5048)  time: 0.3770  data: 0.1790  max mem: 15572
Val:  [230/272]  eta: 0:00:16  loss: 4.6825 (4.8820)  acc1: 0.0000 (5.3872)  acc5: 0.0000 (13.8769)  time: 0.3297  data: 0.1417  max mem: 15572
Val:  [240/272]  eta: 0:00:12  loss: 4.7335 (4.8837)  acc1: 0.0000 (5.1637)  acc5: 0.0000 (13.4163)  time: 0.2389  data: 0.0562  max mem: 15572
Val:  [250/272]  eta: 0:00:08  loss: 5.1220 (4.9016)  acc1: 0.0000 (4.9579)  acc5: 0.0000 (12.8818)  time: 0.2167  data: 0.0439  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.8257 (4.8919)  acc1: 0.0000 (4.7680)  acc5: 0.0000 (12.7288)  time: 0.1847  data: 0.0315  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 4.6944 (4.8911)  acc1: 0.0000 (4.6125)  acc5: 5.5556 (13.3251)  time: 0.1454  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 4.6944 (4.8930)  acc1: 0.0000 (4.6078)  acc5: 5.5556 (13.3115)  time: 0.1400  data: 0.0002  max mem: 15572
Val: Total time: 0:01:36 (0.3553 s / it)
* Acc@1 4.608 Acc@5 13.311 loss 4.893
Accuracy of the network on the 4883 val videos: 4.6%
[2025-01-15 15:20:58,063] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 15:20:58,066] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 15:20:58,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 15:21:00,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 15:21:00,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 4.61%
Epoch: [2]  [   0/2809]  eta: 4:52:23  lr: 0.000019  min_lr: 0.000000  loss: 5.0766 (5.0766)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.2453  data: 5.7776  max mem: 15572
[2025-01-15 15:21:12,084] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:21:12,085] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:21:12,478] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5625
[2025-01-15 15:21:12,479] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:21:12,479] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [  10/2809]  eta: 0:54:35  lr: 0.000019  min_lr: 0.000000  loss: 4.9630 (4.9813)  class_acc: 0.0000 (0.0227)  loss_scale: 65536.0000 (71493.8182)  weight_decay: 0.0500 (0.0500)  time: 1.1702  data: 0.7435  max mem: 15572
Epoch: [2]  [  20/2809]  eta: 0:41:45  lr: 0.000019  min_lr: 0.000000  loss: 4.9548 (4.9651)  class_acc: 0.0000 (0.0238)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1900  max mem: 15572
Epoch: [2]  [  30/2809]  eta: 0:35:06  lr: 0.000019  min_lr: 0.000000  loss: 4.9081 (4.9462)  class_acc: 0.0417 (0.0349)  loss_scale: 65536.0000 (67650.0645)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0810  max mem: 15572
Epoch: [2]  [  40/2809]  eta: 0:34:22  lr: 0.000019  min_lr: 0.000000  loss: 4.9444 (4.9603)  class_acc: 0.0417 (0.0335)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1281  max mem: 15572
Epoch: [2]  [  50/2809]  eta: 0:33:31  lr: 0.000019  min_lr: 0.000000  loss: 4.9534 (4.9523)  class_acc: 0.0000 (0.0310)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.6843  data: 0.2274  max mem: 15572
Epoch: [2]  [  60/2809]  eta: 0:33:12  lr: 0.000019  min_lr: 0.000000  loss: 4.9679 (4.9615)  class_acc: 0.0000 (0.0294)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6837  data: 0.2288  max mem: 15572
Epoch: [2]  [  70/2809]  eta: 0:31:40  lr: 0.000019  min_lr: 0.000000  loss: 5.0031 (4.9672)  class_acc: 0.0000 (0.0293)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1426  max mem: 15572
Epoch: [2]  [  80/2809]  eta: 0:30:21  lr: 0.000019  min_lr: 0.000000  loss: 4.9890 (4.9730)  class_acc: 0.0000 (0.0303)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.4922  data: 0.0417  max mem: 15572
Epoch: [2]  [  90/2809]  eta: 0:30:42  lr: 0.000019  min_lr: 0.000000  loss: 4.9684 (4.9675)  class_acc: 0.0417 (0.0311)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1446  max mem: 15572
Epoch: [2]  [ 100/2809]  eta: 0:29:53  lr: 0.000019  min_lr: 0.000000  loss: 4.9357 (4.9673)  class_acc: 0.0417 (0.0330)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.1494  max mem: 15572
Epoch: [2]  [ 110/2809]  eta: 0:29:37  lr: 0.000019  min_lr: 0.000000  loss: 4.9798 (4.9675)  class_acc: 0.0000 (0.0315)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0963  max mem: 15572
[2025-01-15 15:22:17,913] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5734
[2025-01-15 15:22:17,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:22:17,913] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 120/2809]  eta: 0:29:10  lr: 0.000019  min_lr: 0.000000  loss: 4.9614 (4.9658)  class_acc: 0.0000 (0.0320)  loss_scale: 65536.0000 (64723.5702)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1427  max mem: 15572
Epoch: [2]  [ 130/2809]  eta: 0:28:49  lr: 0.000019  min_lr: 0.000000  loss: 4.9217 (4.9613)  class_acc: 0.0000 (0.0328)  loss_scale: 32768.0000 (62284.2137)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1530  max mem: 15572
Epoch: [2]  [ 140/2809]  eta: 0:28:47  lr: 0.000019  min_lr: 0.000000  loss: 4.9581 (4.9675)  class_acc: 0.0417 (0.0343)  loss_scale: 32768.0000 (60190.8652)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1950  max mem: 15572
Epoch: [2]  [ 150/2809]  eta: 0:28:11  lr: 0.000019  min_lr: 0.000000  loss: 4.9382 (4.9633)  class_acc: 0.0417 (0.0339)  loss_scale: 32768.0000 (58374.7815)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1333  max mem: 15572
Epoch: [2]  [ 160/2809]  eta: 0:28:08  lr: 0.000019  min_lr: 0.000000  loss: 4.9453 (4.9639)  class_acc: 0.0000 (0.0360)  loss_scale: 32768.0000 (56784.2981)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1243  max mem: 15572
Epoch: [2]  [ 170/2809]  eta: 0:27:36  lr: 0.000019  min_lr: 0.000000  loss: 4.9962 (4.9624)  class_acc: 0.0417 (0.0358)  loss_scale: 32768.0000 (55379.8363)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1153  max mem: 15572
Epoch: [2]  [ 180/2809]  eta: 0:27:38  lr: 0.000019  min_lr: 0.000000  loss: 4.8819 (4.9613)  class_acc: 0.0000 (0.0355)  loss_scale: 32768.0000 (54130.5635)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1175  max mem: 15572
Epoch: [2]  [ 190/2809]  eta: 0:27:21  lr: 0.000019  min_lr: 0.000000  loss: 4.8987 (4.9576)  class_acc: 0.0000 (0.0349)  loss_scale: 32768.0000 (53012.1047)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1465  max mem: 15572
Epoch: [2]  [ 200/2809]  eta: 0:27:19  lr: 0.000019  min_lr: 0.000000  loss: 4.9165 (4.9562)  class_acc: 0.0417 (0.0361)  loss_scale: 32768.0000 (52004.9353)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1329  max mem: 15572
Epoch: [2]  [ 210/2809]  eta: 0:27:09  lr: 0.000019  min_lr: 0.000000  loss: 4.9195 (4.9554)  class_acc: 0.0417 (0.0361)  loss_scale: 32768.0000 (51093.2322)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.1461  max mem: 15572
Epoch: [2]  [ 220/2809]  eta: 0:26:55  lr: 0.000019  min_lr: 0.000000  loss: 4.8791 (4.9517)  class_acc: 0.0000 (0.0364)  loss_scale: 32768.0000 (50264.0362)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1077  max mem: 15572
Epoch: [2]  [ 230/2809]  eta: 0:26:47  lr: 0.000020  min_lr: 0.000000  loss: 4.8791 (4.9508)  class_acc: 0.0417 (0.0375)  loss_scale: 32768.0000 (49506.6320)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1190  max mem: 15572
Epoch: [2]  [ 240/2809]  eta: 0:26:29  lr: 0.000020  min_lr: 0.000000  loss: 4.9511 (4.9524)  class_acc: 0.0417 (0.0373)  loss_scale: 32768.0000 (48812.0830)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1008  max mem: 15572
[2025-01-15 15:23:34,042] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:23:34,042] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 250/2809]  eta: 0:26:23  lr: 0.000020  min_lr: 0.000000  loss: 4.9511 (4.9518)  class_acc: 0.0417 (0.0372)  loss_scale: 32768.0000 (48956.1753)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1219  max mem: 15572
Epoch: [2]  [ 260/2809]  eta: 0:26:12  lr: 0.000020  min_lr: 0.000000  loss: 4.9391 (4.9532)  class_acc: 0.0000 (0.0370)  loss_scale: 65536.0000 (49591.4176)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1486  max mem: 15572
Epoch: [2]  [ 270/2809]  eta: 0:25:55  lr: 0.000020  min_lr: 0.000000  loss: 4.9456 (4.9529)  class_acc: 0.0417 (0.0380)  loss_scale: 65536.0000 (50179.7786)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0947  max mem: 15572
Epoch: [2]  [ 280/2809]  eta: 0:25:58  lr: 0.000020  min_lr: 0.000000  loss: 4.9100 (4.9516)  class_acc: 0.0417 (0.0386)  loss_scale: 65536.0000 (50726.2633)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1644  max mem: 15572
Epoch: [2]  [ 290/2809]  eta: 0:25:53  lr: 0.000020  min_lr: 0.000000  loss: 4.9164 (4.9519)  class_acc: 0.0417 (0.0391)  loss_scale: 65536.0000 (51235.1890)  weight_decay: 0.0500 (0.0500)  time: 0.6732  data: 0.2205  max mem: 15572
Epoch: [2]  [ 300/2809]  eta: 0:25:47  lr: 0.000020  min_lr: 0.000000  loss: 4.9737 (4.9520)  class_acc: 0.0417 (0.0393)  loss_scale: 65536.0000 (51710.2990)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1583  max mem: 15572
Epoch: [2]  [ 310/2809]  eta: 0:25:34  lr: 0.000020  min_lr: 0.000000  loss: 4.9980 (4.9523)  class_acc: 0.0000 (0.0393)  loss_scale: 65536.0000 (52154.8553)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0952  max mem: 15572
Epoch: [2]  [ 320/2809]  eta: 0:25:27  lr: 0.000020  min_lr: 0.000000  loss: 4.8807 (4.9508)  class_acc: 0.0000 (0.0386)  loss_scale: 65536.0000 (52571.7134)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0888  max mem: 15572
Epoch: [2]  [ 330/2809]  eta: 0:25:27  lr: 0.000020  min_lr: 0.000000  loss: 4.8771 (4.9486)  class_acc: 0.0000 (0.0379)  loss_scale: 65536.0000 (52963.3837)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.1742  max mem: 15572
Epoch: [2]  [ 340/2809]  eta: 0:25:12  lr: 0.000020  min_lr: 0.000000  loss: 4.8774 (4.9475)  class_acc: 0.0000 (0.0380)  loss_scale: 65536.0000 (53332.0821)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1353  max mem: 15572
Epoch: [2]  [ 350/2809]  eta: 0:25:03  lr: 0.000020  min_lr: 0.000000  loss: 4.9097 (4.9467)  class_acc: 0.0000 (0.0373)  loss_scale: 65536.0000 (53679.7721)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0741  max mem: 15572
Epoch: [2]  [ 360/2809]  eta: 0:24:56  lr: 0.000020  min_lr: 0.000000  loss: 4.9292 (4.9465)  class_acc: 0.0000 (0.0368)  loss_scale: 65536.0000 (54008.1994)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1283  max mem: 15572
Epoch: [2]  [ 370/2809]  eta: 0:24:52  lr: 0.000020  min_lr: 0.000000  loss: 4.9155 (4.9453)  class_acc: 0.0000 (0.0377)  loss_scale: 65536.0000 (54318.9218)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1720  max mem: 15572
[2025-01-15 15:24:50,581] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:24:50,582] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:24:51,833] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5994
[2025-01-15 15:24:51,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:24:51,834] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 380/2809]  eta: 0:24:43  lr: 0.000020  min_lr: 0.000000  loss: 4.9102 (4.9445)  class_acc: 0.0000 (0.0372)  loss_scale: 65536.0000 (55129.3648)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1492  max mem: 15572
[2025-01-15 15:24:54,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=28, lr=[1.9400094349619698e-07, 1.9400094349619698e-07, 2.7714420499456714e-07, 2.7714420499456714e-07, 3.959202928493817e-07, 3.959202928493817e-07, 5.656004183562596e-07, 5.656004183562596e-07, 8.080005976517993e-07, 8.080005976517993e-07, 1.1542865680739992e-06, 1.1542865680739992e-06, 1.6489808115342846e-06, 1.6489808115342846e-06, 2.355686873620407e-06, 2.355686873620407e-06, 3.3652669623148667e-06, 3.3652669623148667e-06, 4.807524231878382e-06, 4.807524231878382e-06, 6.86789175982626e-06, 6.86789175982626e-06, 9.811273942608943e-06, 9.811273942608943e-06, 1.401610563229849e-05, 1.401610563229849e-05, 2.0023008046140703e-05, 2.0023008046140703e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:24:54,708] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=27.638316861163013, CurrSamplesPerSec=30.852059014813776, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [ 390/2809]  eta: 0:24:31  lr: 0.000020  min_lr: 0.000000  loss: 4.9131 (4.9444)  class_acc: 0.0000 (0.0373)  loss_scale: 65536.0000 (55395.5192)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0819  max mem: 15572
Epoch: [2]  [ 400/2809]  eta: 0:24:26  lr: 0.000020  min_lr: 0.000000  loss: 4.9276 (4.9449)  class_acc: 0.0000 (0.0369)  loss_scale: 65536.0000 (55648.3990)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1130  max mem: 15572
Epoch: [2]  [ 410/2809]  eta: 0:24:19  lr: 0.000020  min_lr: 0.000000  loss: 4.9218 (4.9440)  class_acc: 0.0000 (0.0375)  loss_scale: 65536.0000 (55888.9732)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1421  max mem: 15572
[2025-01-15 15:25:14,093] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6029
[2025-01-15 15:25:14,093] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:25:14,093] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 420/2809]  eta: 0:24:20  lr: 0.000020  min_lr: 0.000000  loss: 4.9110 (4.9445)  class_acc: 0.0417 (0.0374)  loss_scale: 32768.0000 (55339.7815)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.1870  max mem: 15572
Epoch: [2]  [ 430/2809]  eta: 0:24:14  lr: 0.000020  min_lr: 0.000000  loss: 5.0005 (4.9441)  class_acc: 0.0417 (0.0374)  loss_scale: 32768.0000 (54816.0742)  weight_decay: 0.0500 (0.0500)  time: 0.6711  data: 0.2082  max mem: 15572
Epoch: [2]  [ 440/2809]  eta: 0:24:07  lr: 0.000020  min_lr: 0.000000  loss: 4.9069 (4.9424)  class_acc: 0.0417 (0.0376)  loss_scale: 32768.0000 (54316.1179)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1496  max mem: 15572
Epoch: [2]  [ 450/2809]  eta: 0:23:57  lr: 0.000020  min_lr: 0.000000  loss: 4.9258 (4.9428)  class_acc: 0.0417 (0.0376)  loss_scale: 32768.0000 (53838.3326)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1179  max mem: 15572
Epoch: [2]  [ 460/2809]  eta: 0:23:47  lr: 0.000020  min_lr: 0.000000  loss: 4.9490 (4.9422)  class_acc: 0.0417 (0.0378)  loss_scale: 32768.0000 (53381.2755)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0932  max mem: 15572
Epoch: [2]  [ 470/2809]  eta: 0:23:44  lr: 0.000020  min_lr: 0.000000  loss: 4.9021 (4.9412)  class_acc: 0.0417 (0.0375)  loss_scale: 32768.0000 (52943.6263)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1527  max mem: 15572
Epoch: [2]  [ 480/2809]  eta: 0:23:42  lr: 0.000020  min_lr: 0.000000  loss: 4.9098 (4.9421)  class_acc: 0.0000 (0.0379)  loss_scale: 32768.0000 (52524.1746)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.2114  max mem: 15572
Epoch: [2]  [ 490/2809]  eta: 0:23:32  lr: 0.000020  min_lr: 0.000000  loss: 5.0425 (4.9438)  class_acc: 0.0417 (0.0378)  loss_scale: 32768.0000 (52121.8086)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1319  max mem: 15572
Epoch: [2]  [ 500/2809]  eta: 0:23:26  lr: 0.000020  min_lr: 0.000000  loss: 4.9492 (4.9433)  class_acc: 0.0417 (0.0378)  loss_scale: 32768.0000 (51735.5050)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.0957  max mem: 15572
Epoch: [2]  [ 510/2809]  eta: 0:23:17  lr: 0.000020  min_lr: 0.000000  loss: 4.9135 (4.9424)  class_acc: 0.0417 (0.0386)  loss_scale: 32768.0000 (51364.3209)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1038  max mem: 15572
Epoch: [2]  [ 520/2809]  eta: 0:23:09  lr: 0.000020  min_lr: 0.000000  loss: 4.8989 (4.9421)  class_acc: 0.0833 (0.0395)  loss_scale: 32768.0000 (51007.3858)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0507  max mem: 15572
Epoch: [2]  [ 530/2809]  eta: 0:23:00  lr: 0.000021  min_lr: 0.000000  loss: 4.8687 (4.9409)  class_acc: 0.0417 (0.0396)  loss_scale: 32768.0000 (50663.8945)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0483  max mem: 15572
[2025-01-15 15:26:27,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:26:27,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 540/2809]  eta: 0:22:50  lr: 0.000021  min_lr: 0.000000  loss: 4.8737 (4.9416)  class_acc: 0.0417 (0.0394)  loss_scale: 32768.0000 (50393.6710)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0325  max mem: 15572
Epoch: [2]  [ 550/2809]  eta: 0:22:40  lr: 0.000021  min_lr: 0.000000  loss: 4.9769 (4.9425)  class_acc: 0.0000 (0.0393)  loss_scale: 65536.0000 (50668.4864)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0174  max mem: 15572
Epoch: [2]  [ 560/2809]  eta: 0:22:35  lr: 0.000021  min_lr: 0.000000  loss: 4.9760 (4.9419)  class_acc: 0.0417 (0.0397)  loss_scale: 65536.0000 (50933.5045)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1109  max mem: 15572
Epoch: [2]  [ 570/2809]  eta: 0:22:29  lr: 0.000021  min_lr: 0.000000  loss: 4.8899 (4.9412)  class_acc: 0.0417 (0.0401)  loss_scale: 65536.0000 (51189.2399)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.1758  max mem: 15572
Epoch: [2]  [ 580/2809]  eta: 0:22:26  lr: 0.000021  min_lr: 0.000000  loss: 4.9413 (4.9416)  class_acc: 0.0417 (0.0404)  loss_scale: 65536.0000 (51436.1721)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.1988  max mem: 15572
Epoch: [2]  [ 590/2809]  eta: 0:22:20  lr: 0.000021  min_lr: 0.000000  loss: 4.9574 (4.9427)  class_acc: 0.0417 (0.0403)  loss_scale: 65536.0000 (51674.7479)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1886  max mem: 15572
Epoch: [2]  [ 600/2809]  eta: 0:22:12  lr: 0.000021  min_lr: 0.000000  loss: 4.9593 (4.9426)  class_acc: 0.0000 (0.0403)  loss_scale: 65536.0000 (51905.3844)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1243  max mem: 15572
Epoch: [2]  [ 610/2809]  eta: 0:22:04  lr: 0.000021  min_lr: 0.000000  loss: 4.9475 (4.9415)  class_acc: 0.0000 (0.0404)  loss_scale: 65536.0000 (52128.4714)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1025  max mem: 15572
Epoch: [2]  [ 620/2809]  eta: 0:22:04  lr: 0.000021  min_lr: 0.000000  loss: 4.9248 (4.9423)  class_acc: 0.0417 (0.0407)  loss_scale: 65536.0000 (52344.3736)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.2027  max mem: 15572
Epoch: [2]  [ 630/2809]  eta: 0:21:57  lr: 0.000021  min_lr: 0.000000  loss: 4.9352 (4.9420)  class_acc: 0.0417 (0.0409)  loss_scale: 65536.0000 (52553.4326)  weight_decay: 0.0500 (0.0500)  time: 0.6676  data: 0.1901  max mem: 15572
Epoch: [2]  [ 640/2809]  eta: 0:21:50  lr: 0.000021  min_lr: 0.000000  loss: 4.9076 (4.9411)  class_acc: 0.0000 (0.0408)  loss_scale: 65536.0000 (52755.9688)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0951  max mem: 15572
Epoch: [2]  [ 650/2809]  eta: 0:21:40  lr: 0.000021  min_lr: 0.000000  loss: 4.9161 (4.9411)  class_acc: 0.0417 (0.0408)  loss_scale: 65536.0000 (52952.2826)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0582  max mem: 15572
Epoch: [2]  [ 660/2809]  eta: 0:21:35  lr: 0.000021  min_lr: 0.000000  loss: 4.9306 (4.9402)  class_acc: 0.0417 (0.0410)  loss_scale: 65536.0000 (53142.6566)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0813  max mem: 15572
[2025-01-15 15:27:43,589] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:27:43,589] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 670/2809]  eta: 0:21:26  lr: 0.000021  min_lr: 0.000000  loss: 4.8643 (4.9394)  class_acc: 0.0417 (0.0408)  loss_scale: 65536.0000 (53620.3636)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0871  max mem: 15572
Epoch: [2]  [ 680/2809]  eta: 0:21:20  lr: 0.000021  min_lr: 0.000000  loss: 4.9583 (4.9409)  class_acc: 0.0417 (0.0407)  loss_scale: 131072.0000 (54757.6858)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0821  max mem: 15572
[2025-01-15 15:27:51,495] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6299
[2025-01-15 15:27:51,495] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:27:51,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 690/2809]  eta: 0:21:16  lr: 0.000021  min_lr: 0.000000  loss: 4.9762 (4.9408)  class_acc: 0.0417 (0.0404)  loss_scale: 65536.0000 (54913.6671)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1769  max mem: 15572
Epoch: [2]  [ 700/2809]  eta: 0:21:08  lr: 0.000021  min_lr: 0.000000  loss: 4.9401 (4.9402)  class_acc: 0.0000 (0.0401)  loss_scale: 65536.0000 (55065.1983)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1573  max mem: 15572
Epoch: [2]  [ 710/2809]  eta: 0:21:00  lr: 0.000021  min_lr: 0.000000  loss: 4.9183 (4.9396)  class_acc: 0.0000 (0.0409)  loss_scale: 65536.0000 (55212.4669)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0824  max mem: 15572
Epoch: [2]  [ 720/2809]  eta: 0:20:52  lr: 0.000021  min_lr: 0.000000  loss: 4.9037 (4.9392)  class_acc: 0.0417 (0.0410)  loss_scale: 65536.0000 (55355.6505)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0727  max mem: 15572
Epoch: [2]  [ 730/2809]  eta: 0:20:45  lr: 0.000021  min_lr: 0.000000  loss: 4.9393 (4.9390)  class_acc: 0.0417 (0.0413)  loss_scale: 65536.0000 (55494.9166)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1131  max mem: 15572
Epoch: [2]  [ 740/2809]  eta: 0:20:39  lr: 0.000021  min_lr: 0.000000  loss: 4.9871 (4.9403)  class_acc: 0.0417 (0.0414)  loss_scale: 65536.0000 (55630.4238)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1297  max mem: 15572
Epoch: [2]  [ 750/2809]  eta: 0:20:35  lr: 0.000021  min_lr: 0.000000  loss: 5.0039 (4.9404)  class_acc: 0.0417 (0.0414)  loss_scale: 65536.0000 (55762.3222)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1529  max mem: 15572
Epoch: [2]  [ 760/2809]  eta: 0:20:30  lr: 0.000021  min_lr: 0.000000  loss: 4.9124 (4.9399)  class_acc: 0.0000 (0.0413)  loss_scale: 65536.0000 (55890.7543)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.1638  max mem: 15572
Epoch: [2]  [ 770/2809]  eta: 0:20:25  lr: 0.000021  min_lr: 0.000000  loss: 4.9070 (4.9391)  class_acc: 0.0000 (0.0414)  loss_scale: 65536.0000 (56015.8547)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1617  max mem: 15572
Epoch: [2]  [ 780/2809]  eta: 0:20:19  lr: 0.000021  min_lr: 0.000000  loss: 4.8026 (4.9376)  class_acc: 0.0417 (0.0419)  loss_scale: 65536.0000 (56137.7516)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1440  max mem: 15572
Epoch: [2]  [ 790/2809]  eta: 0:20:11  lr: 0.000021  min_lr: 0.000000  loss: 4.8026 (4.9370)  class_acc: 0.0417 (0.0420)  loss_scale: 65536.0000 (56256.5664)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0904  max mem: 15572
Epoch: [2]  [ 800/2809]  eta: 0:20:06  lr: 0.000021  min_lr: 0.000000  loss: 4.8747 (4.9362)  class_acc: 0.0417 (0.0424)  loss_scale: 65536.0000 (56372.4145)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1213  max mem: 15572
[2025-01-15 15:29:07,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:29:07,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 810/2809]  eta: 0:19:59  lr: 0.000021  min_lr: 0.000000  loss: 4.8592 (4.9352)  class_acc: 0.0417 (0.0423)  loss_scale: 65536.0000 (56566.2145)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1266  max mem: 15572
[2025-01-15 15:29:10,367] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6432
[2025-01-15 15:29:10,367] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:29:10,367] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 820/2809]  eta: 0:19:54  lr: 0.000021  min_lr: 0.000000  loss: 4.8940 (4.9349)  class_acc: 0.0000 (0.0418)  loss_scale: 65536.0000 (56914.9428)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1246  max mem: 15572
Epoch: [2]  [ 830/2809]  eta: 0:19:50  lr: 0.000022  min_lr: 0.000000  loss: 4.8860 (4.9343)  class_acc: 0.0000 (0.0422)  loss_scale: 65536.0000 (57018.6859)  weight_decay: 0.0500 (0.0500)  time: 0.6710  data: 0.2193  max mem: 15572
Epoch: [2]  [ 840/2809]  eta: 0:19:42  lr: 0.000022  min_lr: 0.000000  loss: 4.8491 (4.9336)  class_acc: 0.0833 (0.0425)  loss_scale: 65536.0000 (57119.9620)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1739  max mem: 15572
Epoch: [2]  [ 850/2809]  eta: 0:19:40  lr: 0.000022  min_lr: 0.000000  loss: 4.8794 (4.9335)  class_acc: 0.0417 (0.0424)  loss_scale: 65536.0000 (57218.8578)  weight_decay: 0.0500 (0.0500)  time: 0.6497  data: 0.2008  max mem: 15572
Epoch: [2]  [ 860/2809]  eta: 0:19:33  lr: 0.000022  min_lr: 0.000000  loss: 4.9296 (4.9331)  class_acc: 0.0000 (0.0420)  loss_scale: 65536.0000 (57315.4564)  weight_decay: 0.0500 (0.0500)  time: 0.6633  data: 0.2064  max mem: 15572
Epoch: [2]  [ 870/2809]  eta: 0:19:24  lr: 0.000022  min_lr: 0.000000  loss: 4.9202 (4.9325)  class_acc: 0.0000 (0.0420)  loss_scale: 65536.0000 (57409.8370)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0688  max mem: 15572
Epoch: [2]  [ 880/2809]  eta: 0:19:20  lr: 0.000022  min_lr: 0.000000  loss: 4.9585 (4.9326)  class_acc: 0.0417 (0.0420)  loss_scale: 65536.0000 (57502.0749)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1285  max mem: 15572
Epoch: [2]  [ 890/2809]  eta: 0:19:10  lr: 0.000022  min_lr: 0.000000  loss: 4.9585 (4.9325)  class_acc: 0.0417 (0.0421)  loss_scale: 65536.0000 (57592.2424)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1109  max mem: 15572
Epoch: [2]  [ 900/2809]  eta: 0:19:04  lr: 0.000022  min_lr: 0.000000  loss: 4.9502 (4.9326)  class_acc: 0.0417 (0.0420)  loss_scale: 65536.0000 (57680.4084)  weight_decay: 0.0500 (0.0500)  time: 0.5049  data: 0.0734  max mem: 15572
Epoch: [2]  [ 910/2809]  eta: 0:18:58  lr: 0.000022  min_lr: 0.000000  loss: 4.9361 (4.9325)  class_acc: 0.0000 (0.0418)  loss_scale: 65536.0000 (57766.6389)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1373  max mem: 15572
Epoch: [2]  [ 920/2809]  eta: 0:18:52  lr: 0.000022  min_lr: 0.000000  loss: 4.9622 (4.9327)  class_acc: 0.0000 (0.0418)  loss_scale: 65536.0000 (57850.9967)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1584  max mem: 15572
Epoch: [2]  [ 930/2809]  eta: 0:18:45  lr: 0.000022  min_lr: 0.000000  loss: 4.9483 (4.9324)  class_acc: 0.0000 (0.0418)  loss_scale: 65536.0000 (57933.5424)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1356  max mem: 15572
Epoch: [2]  [ 940/2809]  eta: 0:18:38  lr: 0.000022  min_lr: 0.000000  loss: 4.9046 (4.9317)  class_acc: 0.0000 (0.0415)  loss_scale: 65536.0000 (58014.3337)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0958  max mem: 15572
[2025-01-15 15:30:27,530] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:30:27,530] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:30:28,373] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6563
[2025-01-15 15:30:28,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:30:28,374] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 950/2809]  eta: 0:18:33  lr: 0.000022  min_lr: 0.000000  loss: 4.8859 (4.9314)  class_acc: 0.0000 (0.0416)  loss_scale: 65536.0000 (58231.2513)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1297  max mem: 15572
Epoch: [2]  [ 960/2809]  eta: 0:18:27  lr: 0.000022  min_lr: 0.000000  loss: 4.8059 (4.9303)  class_acc: 0.0417 (0.0418)  loss_scale: 65536.0000 (58307.2633)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1441  max mem: 15572
Epoch: [2]  [ 970/2809]  eta: 0:18:21  lr: 0.000022  min_lr: 0.000000  loss: 4.8390 (4.9297)  class_acc: 0.0417 (0.0420)  loss_scale: 65536.0000 (58381.7096)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1209  max mem: 15572
Epoch: [2]  [ 980/2809]  eta: 0:18:15  lr: 0.000022  min_lr: 0.000000  loss: 4.8591 (4.9294)  class_acc: 0.0417 (0.0419)  loss_scale: 65536.0000 (58454.6381)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1239  max mem: 15572
Epoch: [2]  [ 990/2809]  eta: 0:18:07  lr: 0.000022  min_lr: 0.000000  loss: 4.9198 (4.9299)  class_acc: 0.0000 (0.0419)  loss_scale: 65536.0000 (58526.0949)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0669  max mem: 15572
Epoch: [2]  [1000/2809]  eta: 0:18:01  lr: 0.000022  min_lr: 0.000000  loss: 4.9800 (4.9299)  class_acc: 0.0000 (0.0418)  loss_scale: 65536.0000 (58596.1239)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0476  max mem: 15572
[2025-01-15 15:31:03,670] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6624
[2025-01-15 15:31:03,671] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:31:03,671] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [1010/2809]  eta: 0:17:55  lr: 0.000022  min_lr: 0.000000  loss: 4.9932 (4.9302)  class_acc: 0.0417 (0.0417)  loss_scale: 65536.0000 (58502.7102)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1010  max mem: 15572
Epoch: [2]  [1020/2809]  eta: 0:17:49  lr: 0.000022  min_lr: 0.000000  loss: 4.9192 (4.9295)  class_acc: 0.0417 (0.0418)  loss_scale: 32768.0000 (58250.6562)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1157  max mem: 15572
Epoch: [2]  [1030/2809]  eta: 0:17:44  lr: 0.000022  min_lr: 0.000000  loss: 4.8002 (4.9290)  class_acc: 0.0417 (0.0420)  loss_scale: 32768.0000 (58003.4918)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1498  max mem: 15572
Epoch: [2]  [1040/2809]  eta: 0:17:37  lr: 0.000022  min_lr: 0.000000  loss: 4.8680 (4.9288)  class_acc: 0.0417 (0.0417)  loss_scale: 32768.0000 (57761.0759)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1372  max mem: 15572
Epoch: [2]  [1050/2809]  eta: 0:17:31  lr: 0.000022  min_lr: 0.000000  loss: 4.8839 (4.9286)  class_acc: 0.0000 (0.0419)  loss_scale: 32768.0000 (57523.2731)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1224  max mem: 15572
Epoch: [2]  [1060/2809]  eta: 0:17:25  lr: 0.000022  min_lr: 0.000000  loss: 4.8770 (4.9283)  class_acc: 0.0000 (0.0419)  loss_scale: 32768.0000 (57289.9529)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1388  max mem: 15572
Epoch: [2]  [1070/2809]  eta: 0:17:19  lr: 0.000022  min_lr: 0.000000  loss: 4.8434 (4.9277)  class_acc: 0.0417 (0.0423)  loss_scale: 32768.0000 (57060.9897)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1412  max mem: 15572
Epoch: [2]  [1080/2809]  eta: 0:17:15  lr: 0.000022  min_lr: 0.000000  loss: 4.8635 (4.9272)  class_acc: 0.0417 (0.0422)  loss_scale: 32768.0000 (56836.2627)  weight_decay: 0.0500 (0.0500)  time: 0.6821  data: 0.2225  max mem: 15572
Epoch: [2]  [1090/2809]  eta: 0:17:08  lr: 0.000022  min_lr: 0.000000  loss: 4.8639 (4.9261)  class_acc: 0.0417 (0.0427)  loss_scale: 32768.0000 (56615.6554)  weight_decay: 0.0500 (0.0500)  time: 0.6509  data: 0.1697  max mem: 15572
Epoch: [2]  [1100/2809]  eta: 0:17:02  lr: 0.000022  min_lr: 0.000000  loss: 4.9158 (4.9265)  class_acc: 0.0417 (0.0428)  loss_scale: 32768.0000 (56399.0554)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0464  max mem: 15572
Epoch: [2]  [1110/2809]  eta: 0:16:56  lr: 0.000022  min_lr: 0.000000  loss: 4.9491 (4.9259)  class_acc: 0.0417 (0.0428)  loss_scale: 32768.0000 (56186.3546)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0979  max mem: 15572
Epoch: [2]  [1120/2809]  eta: 0:16:48  lr: 0.000022  min_lr: 0.000000  loss: 4.9115 (4.9252)  class_acc: 0.0417 (0.0428)  loss_scale: 32768.0000 (55977.4487)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0995  max mem: 15572
Epoch: [2]  [1130/2809]  eta: 0:16:44  lr: 0.000023  min_lr: 0.000000  loss: 4.9115 (4.9253)  class_acc: 0.0417 (0.0430)  loss_scale: 32768.0000 (55772.2370)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1345  max mem: 15572
[2025-01-15 15:32:20,361] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:32:20,361] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1140/2809]  eta: 0:16:38  lr: 0.000023  min_lr: 0.000000  loss: 4.9242 (4.9254)  class_acc: 0.0417 (0.0429)  loss_scale: 32768.0000 (55742.9343)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1687  max mem: 15572
Epoch: [2]  [1150/2809]  eta: 0:16:31  lr: 0.000023  min_lr: 0.000000  loss: 4.9242 (4.9252)  class_acc: 0.0417 (0.0431)  loss_scale: 65536.0000 (55828.0174)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0923  max mem: 15572
Epoch: [2]  [1160/2809]  eta: 0:16:25  lr: 0.000023  min_lr: 0.000000  loss: 4.9069 (4.9251)  class_acc: 0.0417 (0.0431)  loss_scale: 65536.0000 (55911.6348)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1178  max mem: 15572
Epoch: [2]  [1170/2809]  eta: 0:16:17  lr: 0.000023  min_lr: 0.000000  loss: 4.9069 (4.9249)  class_acc: 0.0417 (0.0434)  loss_scale: 65536.0000 (55993.8241)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.1019  max mem: 15572
Epoch: [2]  [1180/2809]  eta: 0:16:11  lr: 0.000023  min_lr: 0.000000  loss: 4.9066 (4.9249)  class_acc: 0.0417 (0.0434)  loss_scale: 65536.0000 (56074.6215)  weight_decay: 0.0500 (0.0500)  time: 0.5185  data: 0.0803  max mem: 15572
Epoch: [2]  [1190/2809]  eta: 0:16:05  lr: 0.000023  min_lr: 0.000000  loss: 4.9066 (4.9244)  class_acc: 0.0417 (0.0434)  loss_scale: 65536.0000 (56154.0621)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1388  max mem: 15572
Epoch: [2]  [1200/2809]  eta: 0:15:58  lr: 0.000023  min_lr: 0.000000  loss: 4.8952 (4.9240)  class_acc: 0.0000 (0.0433)  loss_scale: 65536.0000 (56232.1799)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1134  max mem: 15572
Epoch: [2]  [1210/2809]  eta: 0:15:54  lr: 0.000023  min_lr: 0.000000  loss: 4.8839 (4.9238)  class_acc: 0.0000 (0.0432)  loss_scale: 65536.0000 (56309.0074)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1681  max mem: 15572
Epoch: [2]  [1220/2809]  eta: 0:15:49  lr: 0.000023  min_lr: 0.000000  loss: 4.8854 (4.9236)  class_acc: 0.0417 (0.0434)  loss_scale: 65536.0000 (56384.5766)  weight_decay: 0.0500 (0.0500)  time: 0.6976  data: 0.2303  max mem: 15572
Epoch: [2]  [1230/2809]  eta: 0:15:43  lr: 0.000023  min_lr: 0.000000  loss: 4.8991 (4.9234)  class_acc: 0.0417 (0.0435)  loss_scale: 65536.0000 (56458.9180)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.1687  max mem: 15572
Epoch: [2]  [1240/2809]  eta: 0:15:37  lr: 0.000023  min_lr: 0.000000  loss: 4.8991 (4.9230)  class_acc: 0.0417 (0.0433)  loss_scale: 65536.0000 (56532.0612)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1549  max mem: 15572
Epoch: [2]  [1250/2809]  eta: 0:15:32  lr: 0.000023  min_lr: 0.000000  loss: 4.9005 (4.9229)  class_acc: 0.0417 (0.0435)  loss_scale: 65536.0000 (56604.0352)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1892  max mem: 15572
Epoch: [2]  [1260/2809]  eta: 0:15:26  lr: 0.000023  min_lr: 0.000000  loss: 4.9155 (4.9230)  class_acc: 0.0417 (0.0434)  loss_scale: 65536.0000 (56674.8676)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1731  max mem: 15572
[2025-01-15 15:33:37,347] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:33:37,347] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:33:37,879] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6882
[2025-01-15 15:33:37,879] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:33:37,880] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1270/2809]  eta: 0:15:20  lr: 0.000023  min_lr: 0.000000  loss: 4.9171 (4.9232)  class_acc: 0.0417 (0.0437)  loss_scale: 65536.0000 (56796.1479)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1330  max mem: 15572
Epoch: [2]  [1280/2809]  eta: 0:15:14  lr: 0.000023  min_lr: 0.000000  loss: 4.8835 (4.9231)  class_acc: 0.0417 (0.0437)  loss_scale: 65536.0000 (56864.3747)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1191  max mem: 15572
Epoch: [2]  [1290/2809]  eta: 0:15:09  lr: 0.000023  min_lr: 0.000000  loss: 4.8694 (4.9225)  class_acc: 0.0417 (0.0436)  loss_scale: 65536.0000 (56931.5445)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.1891  max mem: 15572
Epoch: [2]  [1300/2809]  eta: 0:15:02  lr: 0.000023  min_lr: 0.000000  loss: 4.8138 (4.9219)  class_acc: 0.0417 (0.0438)  loss_scale: 65536.0000 (56997.6818)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1879  max mem: 15572
Epoch: [2]  [1310/2809]  eta: 0:14:57  lr: 0.000023  min_lr: 0.000000  loss: 4.8567 (4.9217)  class_acc: 0.0417 (0.0439)  loss_scale: 65536.0000 (57062.8101)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1470  max mem: 15572
Epoch: [2]  [1320/2809]  eta: 0:14:51  lr: 0.000023  min_lr: 0.000000  loss: 4.8975 (4.9217)  class_acc: 0.0417 (0.0438)  loss_scale: 65536.0000 (57126.9523)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1627  max mem: 15572
Epoch: [2]  [1330/2809]  eta: 0:14:45  lr: 0.000023  min_lr: 0.000000  loss: 4.9771 (4.9221)  class_acc: 0.0000 (0.0435)  loss_scale: 65536.0000 (57190.1307)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1177  max mem: 15572
Epoch: [2]  [1340/2809]  eta: 0:14:38  lr: 0.000023  min_lr: 0.000000  loss: 4.8861 (4.9217)  class_acc: 0.0000 (0.0436)  loss_scale: 65536.0000 (57252.3669)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0474  max mem: 15572
Epoch: [2]  [1350/2809]  eta: 0:14:32  lr: 0.000023  min_lr: 0.000000  loss: 4.8674 (4.9214)  class_acc: 0.0000 (0.0434)  loss_scale: 65536.0000 (57313.6817)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0514  max mem: 15572
Epoch: [2]  [1360/2809]  eta: 0:14:26  lr: 0.000023  min_lr: 0.000000  loss: 4.8833 (4.9208)  class_acc: 0.0000 (0.0435)  loss_scale: 65536.0000 (57374.0955)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0514  max mem: 15572
Epoch: [2]  [1370/2809]  eta: 0:14:20  lr: 0.000023  min_lr: 0.000000  loss: 4.8872 (4.9207)  class_acc: 0.0417 (0.0438)  loss_scale: 65536.0000 (57433.6280)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0687  max mem: 15572
Epoch: [2]  [1380/2809]  eta: 0:14:13  lr: 0.000023  min_lr: 0.000000  loss: 4.8557 (4.9202)  class_acc: 0.0417 (0.0437)  loss_scale: 65536.0000 (57492.2983)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1152  max mem: 15572
[2025-01-15 15:34:46,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=34, lr=[2.263398238922958e-07, 2.263398238922958e-07, 3.233426055604226e-07, 3.233426055604226e-07, 4.619180079434609e-07, 4.619180079434609e-07, 6.598828684906585e-07, 6.598828684906585e-07, 9.426898121295122e-07, 9.426898121295122e-07, 1.346699731613589e-06, 1.346699731613589e-06, 1.923856759447984e-06, 1.923856759447984e-06, 2.748366799211406e-06, 2.748366799211406e-06, 3.926238284587723e-06, 3.926238284587723e-06, 5.60891183512532e-06, 5.60891183512532e-06, 8.012731193036171e-06, 8.012731193036171e-06, 1.1446758847194531e-05, 1.1446758847194531e-05, 1.635251263884933e-05, 1.635251263884933e-05, 2.336073234121333e-05, 2.336073234121333e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:34:46,963] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=27.587428444518167, CurrSamplesPerSec=29.62419062922122, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [1390/2809]  eta: 0:14:07  lr: 0.000023  min_lr: 0.000000  loss: 4.8962 (4.9201)  class_acc: 0.0417 (0.0438)  loss_scale: 65536.0000 (57550.1251)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1269  max mem: 15572
[2025-01-15 15:34:53,869] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:34:53,870] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:34:56,849] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7013
[2025-01-15 15:34:56,849] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:34:56,850] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1400/2809]  eta: 0:14:02  lr: 0.000023  min_lr: 0.000000  loss: 4.8962 (4.9199)  class_acc: 0.0417 (0.0438)  loss_scale: 65536.0000 (57700.6824)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1799  max mem: 15572
Epoch: [2]  [1410/2809]  eta: 0:13:55  lr: 0.000023  min_lr: 0.000000  loss: 4.8830 (4.9199)  class_acc: 0.0417 (0.0441)  loss_scale: 65536.0000 (57756.2126)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1458  max mem: 15572
Epoch: [2]  [1420/2809]  eta: 0:13:50  lr: 0.000023  min_lr: 0.000000  loss: 4.8830 (4.9194)  class_acc: 0.0833 (0.0442)  loss_scale: 65536.0000 (57810.9613)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1382  max mem: 15572
Epoch: [2]  [1430/2809]  eta: 0:13:44  lr: 0.000024  min_lr: 0.000000  loss: 4.8238 (4.9187)  class_acc: 0.0417 (0.0444)  loss_scale: 65536.0000 (57864.9448)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1451  max mem: 15572
Epoch: [2]  [1440/2809]  eta: 0:13:39  lr: 0.000024  min_lr: 0.000000  loss: 4.8165 (4.9183)  class_acc: 0.0833 (0.0446)  loss_scale: 65536.0000 (57918.1790)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1660  max mem: 15572
Epoch: [2]  [1450/2809]  eta: 0:13:32  lr: 0.000024  min_lr: 0.000000  loss: 4.9034 (4.9184)  class_acc: 0.0833 (0.0448)  loss_scale: 65536.0000 (57970.6795)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1258  max mem: 15572
Epoch: [2]  [1460/2809]  eta: 0:13:26  lr: 0.000024  min_lr: 0.000000  loss: 4.8971 (4.9175)  class_acc: 0.0417 (0.0450)  loss_scale: 65536.0000 (58022.4613)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0987  max mem: 15572
Epoch: [2]  [1470/2809]  eta: 0:13:19  lr: 0.000024  min_lr: 0.000000  loss: 4.8753 (4.9177)  class_acc: 0.0417 (0.0448)  loss_scale: 65536.0000 (58073.5391)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1184  max mem: 15572
Epoch: [2]  [1480/2809]  eta: 0:13:14  lr: 0.000024  min_lr: 0.000000  loss: 4.9355 (4.9176)  class_acc: 0.0417 (0.0449)  loss_scale: 65536.0000 (58123.9271)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1181  max mem: 15572
Epoch: [2]  [1490/2809]  eta: 0:13:07  lr: 0.000024  min_lr: 0.000000  loss: 4.8862 (4.9169)  class_acc: 0.0417 (0.0452)  loss_scale: 65536.0000 (58173.6392)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0862  max mem: 15572
Epoch: [2]  [1500/2809]  eta: 0:13:01  lr: 0.000024  min_lr: 0.000000  loss: 4.8161 (4.9164)  class_acc: 0.0417 (0.0451)  loss_scale: 65536.0000 (58222.6889)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0928  max mem: 15572
Epoch: [2]  [1510/2809]  eta: 0:12:55  lr: 0.000024  min_lr: 0.000000  loss: 4.9089 (4.9167)  class_acc: 0.0417 (0.0452)  loss_scale: 65536.0000 (58271.0893)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1490  max mem: 15572
Epoch: [2]  [1520/2809]  eta: 0:12:49  lr: 0.000024  min_lr: 0.000000  loss: 4.9148 (4.9166)  class_acc: 0.0000 (0.0450)  loss_scale: 65536.0000 (58318.8534)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1210  max mem: 15572
[2025-01-15 15:36:11,477] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:36:11,478] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:36:13,323] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7143
[2025-01-15 15:36:13,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:36:13,323] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1530/2809]  eta: 0:12:44  lr: 0.000024  min_lr: 0.000000  loss: 4.9148 (4.9163)  class_acc: 0.0000 (0.0451)  loss_scale: 65536.0000 (58408.7995)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1768  max mem: 15572
Epoch: [2]  [1540/2809]  eta: 0:12:37  lr: 0.000024  min_lr: 0.000000  loss: 4.9072 (4.9163)  class_acc: 0.0000 (0.0450)  loss_scale: 65536.0000 (58455.0500)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1790  max mem: 15572
Epoch: [2]  [1550/2809]  eta: 0:12:31  lr: 0.000024  min_lr: 0.000000  loss: 4.9220 (4.9162)  class_acc: 0.0000 (0.0450)  loss_scale: 65536.0000 (58500.7041)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1347  max mem: 15572
Epoch: [2]  [1560/2809]  eta: 0:12:26  lr: 0.000024  min_lr: 0.000000  loss: 4.8802 (4.9159)  class_acc: 0.0417 (0.0452)  loss_scale: 65536.0000 (58545.7732)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1677  max mem: 15572
Epoch: [2]  [1570/2809]  eta: 0:12:20  lr: 0.000024  min_lr: 0.000000  loss: 4.8589 (4.9158)  class_acc: 0.0417 (0.0452)  loss_scale: 65536.0000 (58590.2686)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.1959  max mem: 15572
Epoch: [2]  [1580/2809]  eta: 0:12:13  lr: 0.000024  min_lr: 0.000000  loss: 4.8616 (4.9156)  class_acc: 0.0417 (0.0454)  loss_scale: 65536.0000 (58634.2011)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1155  max mem: 15572
Epoch: [2]  [1590/2809]  eta: 0:12:07  lr: 0.000024  min_lr: 0.000000  loss: 4.9219 (4.9161)  class_acc: 0.0417 (0.0453)  loss_scale: 65536.0000 (58677.5814)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0856  max mem: 15572
Epoch: [2]  [1600/2809]  eta: 0:12:02  lr: 0.000024  min_lr: 0.000000  loss: 4.9337 (4.9157)  class_acc: 0.0000 (0.0453)  loss_scale: 65536.0000 (58720.4197)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1389  max mem: 15572
Epoch: [2]  [1610/2809]  eta: 0:11:55  lr: 0.000024  min_lr: 0.000000  loss: 4.8395 (4.9153)  class_acc: 0.0417 (0.0454)  loss_scale: 65536.0000 (58762.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1420  max mem: 15572
Epoch: [2]  [1620/2809]  eta: 0:11:50  lr: 0.000024  min_lr: 0.000000  loss: 4.8459 (4.9147)  class_acc: 0.0417 (0.0454)  loss_scale: 65536.0000 (58804.5108)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1770  max mem: 15572
Epoch: [2]  [1630/2809]  eta: 0:11:43  lr: 0.000024  min_lr: 0.000000  loss: 4.8519 (4.9143)  class_acc: 0.0417 (0.0455)  loss_scale: 65536.0000 (58845.7830)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1362  max mem: 15572
Epoch: [2]  [1640/2809]  eta: 0:11:37  lr: 0.000024  min_lr: 0.000000  loss: 4.8282 (4.9140)  class_acc: 0.0417 (0.0456)  loss_scale: 65536.0000 (58886.5521)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0615  max mem: 15572
Epoch: [2]  [1650/2809]  eta: 0:11:31  lr: 0.000024  min_lr: 0.000000  loss: 4.8282 (4.9141)  class_acc: 0.0417 (0.0456)  loss_scale: 65536.0000 (58926.8274)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1247  max mem: 15572
[2025-01-15 15:37:28,259] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:37:28,260] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:37:28,744] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7273
[2025-01-15 15:37:28,744] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:37:28,745] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1660/2809]  eta: 0:11:25  lr: 0.000024  min_lr: 0.000000  loss: 4.7984 (4.9134)  class_acc: 0.0417 (0.0456)  loss_scale: 65536.0000 (59006.0734)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1918  max mem: 15572
Epoch: [2]  [1670/2809]  eta: 0:11:19  lr: 0.000024  min_lr: 0.000000  loss: 4.7984 (4.9130)  class_acc: 0.0417 (0.0456)  loss_scale: 65536.0000 (59045.1514)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1518  max mem: 15572
Epoch: [2]  [1680/2809]  eta: 0:11:12  lr: 0.000024  min_lr: 0.000000  loss: 4.8150 (4.9126)  class_acc: 0.0417 (0.0458)  loss_scale: 65536.0000 (59083.7644)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0887  max mem: 15572
Epoch: [2]  [1690/2809]  eta: 0:11:07  lr: 0.000024  min_lr: 0.000000  loss: 4.8994 (4.9129)  class_acc: 0.0417 (0.0457)  loss_scale: 65536.0000 (59121.9208)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1646  max mem: 15572
Epoch: [2]  [1700/2809]  eta: 0:11:02  lr: 0.000024  min_lr: 0.000000  loss: 4.8956 (4.9126)  class_acc: 0.0000 (0.0456)  loss_scale: 65536.0000 (59159.6285)  weight_decay: 0.0500 (0.0500)  time: 0.6901  data: 0.2378  max mem: 15572
Epoch: [2]  [1710/2809]  eta: 0:10:55  lr: 0.000024  min_lr: 0.000000  loss: 4.8584 (4.9127)  class_acc: 0.0000 (0.0456)  loss_scale: 65536.0000 (59196.8954)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1380  max mem: 15572
Epoch: [2]  [1720/2809]  eta: 0:10:50  lr: 0.000024  min_lr: 0.000000  loss: 4.8534 (4.9127)  class_acc: 0.0417 (0.0457)  loss_scale: 65536.0000 (59233.7292)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.1386  max mem: 15572
Epoch: [2]  [1730/2809]  eta: 0:10:44  lr: 0.000025  min_lr: 0.000000  loss: 4.8439 (4.9123)  class_acc: 0.0417 (0.0457)  loss_scale: 65536.0000 (59270.1375)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1818  max mem: 15572
Epoch: [2]  [1740/2809]  eta: 0:10:38  lr: 0.000025  min_lr: 0.000000  loss: 4.8316 (4.9117)  class_acc: 0.0417 (0.0458)  loss_scale: 65536.0000 (59306.1275)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1217  max mem: 15572
Epoch: [2]  [1750/2809]  eta: 0:10:32  lr: 0.000025  min_lr: 0.000000  loss: 4.8788 (4.9113)  class_acc: 0.0417 (0.0457)  loss_scale: 65536.0000 (59341.7065)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1018  max mem: 15572
Epoch: [2]  [1760/2809]  eta: 0:10:26  lr: 0.000025  min_lr: 0.000000  loss: 4.8955 (4.9114)  class_acc: 0.0417 (0.0458)  loss_scale: 65536.0000 (59376.8813)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1176  max mem: 15572
Epoch: [2]  [1770/2809]  eta: 0:10:20  lr: 0.000025  min_lr: 0.000000  loss: 4.8888 (4.9114)  class_acc: 0.0000 (0.0456)  loss_scale: 65536.0000 (59411.6589)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1330  max mem: 15572
Epoch: [2]  [1780/2809]  eta: 0:10:14  lr: 0.000025  min_lr: 0.000000  loss: 4.8056 (4.9113)  class_acc: 0.0000 (0.0456)  loss_scale: 65536.0000 (59446.0460)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1374  max mem: 15572
[2025-01-15 15:38:46,579] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:38:46,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1790/2809]  eta: 0:10:08  lr: 0.000025  min_lr: 0.000000  loss: 4.8056 (4.9104)  class_acc: 0.0417 (0.0457)  loss_scale: 65536.0000 (59736.1921)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1385  max mem: 15572
[2025-01-15 15:38:52,157] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7411
[2025-01-15 15:38:52,157] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:38:52,157] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1800/2809]  eta: 0:10:02  lr: 0.000025  min_lr: 0.000000  loss: 4.8475 (4.9102)  class_acc: 0.0417 (0.0457)  loss_scale: 65536.0000 (59841.1727)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1368  max mem: 15572
Epoch: [2]  [1810/2809]  eta: 0:09:55  lr: 0.000025  min_lr: 0.000000  loss: 4.8779 (4.9102)  class_acc: 0.0000 (0.0457)  loss_scale: 65536.0000 (59872.6184)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0695  max mem: 15572
Epoch: [2]  [1820/2809]  eta: 0:09:49  lr: 0.000025  min_lr: 0.000000  loss: 4.8843 (4.9098)  class_acc: 0.0417 (0.0459)  loss_scale: 65536.0000 (59903.7188)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0276  max mem: 15572
[2025-01-15 15:39:11,499] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7446
[2025-01-15 15:39:11,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:39:11,499] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [1830/2809]  eta: 0:09:43  lr: 0.000025  min_lr: 0.000000  loss: 4.8287 (4.9091)  class_acc: 0.0833 (0.0461)  loss_scale: 65536.0000 (59880.7908)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0556  max mem: 15572
Epoch: [2]  [1840/2809]  eta: 0:09:37  lr: 0.000025  min_lr: 0.000000  loss: 4.8099 (4.9086)  class_acc: 0.0833 (0.0461)  loss_scale: 32768.0000 (59733.5187)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0771  max mem: 15572
Epoch: [2]  [1850/2809]  eta: 0:09:31  lr: 0.000025  min_lr: 0.000000  loss: 4.8099 (4.9083)  class_acc: 0.0417 (0.0461)  loss_scale: 32768.0000 (59587.8379)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1139  max mem: 15572
Epoch: [2]  [1860/2809]  eta: 0:09:25  lr: 0.000025  min_lr: 0.000000  loss: 4.8063 (4.9080)  class_acc: 0.0000 (0.0461)  loss_scale: 32768.0000 (59443.7227)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1181  max mem: 15572
Epoch: [2]  [1870/2809]  eta: 0:09:19  lr: 0.000025  min_lr: 0.000000  loss: 4.7889 (4.9074)  class_acc: 0.0417 (0.0461)  loss_scale: 32768.0000 (59301.1480)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1354  max mem: 15572
Epoch: [2]  [1880/2809]  eta: 0:09:13  lr: 0.000025  min_lr: 0.000000  loss: 4.7871 (4.9073)  class_acc: 0.0417 (0.0462)  loss_scale: 32768.0000 (59160.0893)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.1752  max mem: 15572
Epoch: [2]  [1890/2809]  eta: 0:09:07  lr: 0.000025  min_lr: 0.000000  loss: 4.9034 (4.9073)  class_acc: 0.0417 (0.0462)  loss_scale: 32768.0000 (59020.5225)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1419  max mem: 15572
Epoch: [2]  [1900/2809]  eta: 0:09:01  lr: 0.000025  min_lr: 0.000000  loss: 4.8445 (4.9068)  class_acc: 0.0417 (0.0463)  loss_scale: 32768.0000 (58882.4240)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1011  max mem: 15572
Epoch: [2]  [1910/2809]  eta: 0:08:56  lr: 0.000025  min_lr: 0.000000  loss: 4.8233 (4.9065)  class_acc: 0.0417 (0.0464)  loss_scale: 32768.0000 (58745.7708)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.1795  max mem: 15572
Epoch: [2]  [1920/2809]  eta: 0:08:49  lr: 0.000025  min_lr: 0.000000  loss: 4.8233 (4.9062)  class_acc: 0.0000 (0.0463)  loss_scale: 32768.0000 (58610.5403)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.1694  max mem: 15572
Epoch: [2]  [1930/2809]  eta: 0:08:43  lr: 0.000025  min_lr: 0.000000  loss: 4.8325 (4.9060)  class_acc: 0.0417 (0.0463)  loss_scale: 32768.0000 (58476.7105)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0734  max mem: 15572
Epoch: [2]  [1940/2809]  eta: 0:08:37  lr: 0.000025  min_lr: 0.000000  loss: 4.8912 (4.9060)  class_acc: 0.0417 (0.0463)  loss_scale: 32768.0000 (58344.2597)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0983  max mem: 15572
Epoch: [2]  [1950/2809]  eta: 0:08:32  lr: 0.000025  min_lr: 0.000000  loss: 4.8573 (4.9055)  class_acc: 0.0417 (0.0465)  loss_scale: 32768.0000 (58213.1666)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1167  max mem: 15572
[2025-01-15 15:40:28,174] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:40:28,175] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1960/2809]  eta: 0:08:25  lr: 0.000025  min_lr: 0.000000  loss: 4.8532 (4.9056)  class_acc: 0.0417 (0.0466)  loss_scale: 32768.0000 (58150.2499)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0504  max mem: 15572
Epoch: [2]  [1970/2809]  eta: 0:08:19  lr: 0.000025  min_lr: 0.000000  loss: 4.8911 (4.9055)  class_acc: 0.0417 (0.0466)  loss_scale: 65536.0000 (58187.7220)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1080  max mem: 15572
Epoch: [2]  [1980/2809]  eta: 0:08:14  lr: 0.000025  min_lr: 0.000000  loss: 4.8576 (4.9051)  class_acc: 0.0000 (0.0466)  loss_scale: 65536.0000 (58224.8157)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.1874  max mem: 15572
Epoch: [2]  [1990/2809]  eta: 0:08:07  lr: 0.000025  min_lr: 0.000000  loss: 4.8604 (4.9048)  class_acc: 0.0417 (0.0466)  loss_scale: 65536.0000 (58261.5369)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1058  max mem: 15572
Epoch: [2]  [2000/2809]  eta: 0:08:01  lr: 0.000025  min_lr: 0.000000  loss: 4.8343 (4.9045)  class_acc: 0.0417 (0.0466)  loss_scale: 65536.0000 (58297.8911)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.0957  max mem: 15572
Epoch: [2]  [2010/2809]  eta: 0:07:55  lr: 0.000025  min_lr: 0.000000  loss: 4.8160 (4.9041)  class_acc: 0.0417 (0.0466)  loss_scale: 65536.0000 (58333.8836)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1021  max mem: 15572
Epoch: [2]  [2020/2809]  eta: 0:07:49  lr: 0.000025  min_lr: 0.000000  loss: 4.8273 (4.9039)  class_acc: 0.0417 (0.0467)  loss_scale: 65536.0000 (58369.5200)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1053  max mem: 15572
Epoch: [2]  [2030/2809]  eta: 0:07:43  lr: 0.000026  min_lr: 0.000000  loss: 4.8695 (4.9038)  class_acc: 0.0833 (0.0468)  loss_scale: 65536.0000 (58404.8055)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1570  max mem: 15572
Epoch: [2]  [2040/2809]  eta: 0:07:37  lr: 0.000026  min_lr: 0.000000  loss: 4.8916 (4.9039)  class_acc: 0.0417 (0.0468)  loss_scale: 65536.0000 (58439.7452)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1299  max mem: 15572
Epoch: [2]  [2050/2809]  eta: 0:07:31  lr: 0.000026  min_lr: 0.000000  loss: 4.8916 (4.9037)  class_acc: 0.0417 (0.0468)  loss_scale: 65536.0000 (58474.3442)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1090  max mem: 15572
Epoch: [2]  [2060/2809]  eta: 0:07:25  lr: 0.000026  min_lr: 0.000000  loss: 4.8877 (4.9038)  class_acc: 0.0417 (0.0469)  loss_scale: 65536.0000 (58508.6075)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0918  max mem: 15572
Epoch: [2]  [2070/2809]  eta: 0:07:19  lr: 0.000026  min_lr: 0.000000  loss: 4.8554 (4.9033)  class_acc: 0.0417 (0.0470)  loss_scale: 65536.0000 (58542.5398)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0847  max mem: 15572
Epoch: [2]  [2080/2809]  eta: 0:07:13  lr: 0.000026  min_lr: 0.000000  loss: 4.7868 (4.9030)  class_acc: 0.0417 (0.0472)  loss_scale: 65536.0000 (58576.1461)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1315  max mem: 15572
[2025-01-15 15:41:42,309] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:41:42,310] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:41:43,659] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7704
[2025-01-15 15:41:43,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:41:43,660] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2090/2809]  eta: 0:07:07  lr: 0.000026  min_lr: 0.000000  loss: 4.9232 (4.9032)  class_acc: 0.0417 (0.0472)  loss_scale: 65536.0000 (58640.7728)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1201  max mem: 15572
Epoch: [2]  [2100/2809]  eta: 0:07:01  lr: 0.000026  min_lr: 0.000000  loss: 4.8312 (4.9028)  class_acc: 0.0417 (0.0473)  loss_scale: 65536.0000 (58673.5916)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1313  max mem: 15572
Epoch: [2]  [2110/2809]  eta: 0:06:55  lr: 0.000026  min_lr: 0.000000  loss: 4.8418 (4.9030)  class_acc: 0.0417 (0.0474)  loss_scale: 65536.0000 (58706.0995)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1411  max mem: 15572
Epoch: [2]  [2120/2809]  eta: 0:06:49  lr: 0.000026  min_lr: 0.000000  loss: 4.8845 (4.9027)  class_acc: 0.0833 (0.0476)  loss_scale: 65536.0000 (58738.3008)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1114  max mem: 15572
Epoch: [2]  [2130/2809]  eta: 0:06:43  lr: 0.000026  min_lr: 0.000000  loss: 4.8274 (4.9022)  class_acc: 0.0833 (0.0478)  loss_scale: 65536.0000 (58770.1999)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1156  max mem: 15572
Epoch: [2]  [2140/2809]  eta: 0:06:37  lr: 0.000026  min_lr: 0.000000  loss: 4.8298 (4.9023)  class_acc: 0.0833 (0.0480)  loss_scale: 65536.0000 (58801.8010)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1425  max mem: 15572
Epoch: [2]  [2150/2809]  eta: 0:06:31  lr: 0.000026  min_lr: 0.000000  loss: 4.8796 (4.9023)  class_acc: 0.0833 (0.0481)  loss_scale: 65536.0000 (58833.1083)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1528  max mem: 15572
Epoch: [2]  [2160/2809]  eta: 0:06:25  lr: 0.000026  min_lr: 0.000000  loss: 4.8774 (4.9019)  class_acc: 0.0000 (0.0479)  loss_scale: 65536.0000 (58864.1259)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1424  max mem: 15572
Epoch: [2]  [2170/2809]  eta: 0:06:20  lr: 0.000026  min_lr: 0.000000  loss: 4.8719 (4.9017)  class_acc: 0.0000 (0.0480)  loss_scale: 65536.0000 (58894.8577)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1550  max mem: 15572
Epoch: [2]  [2180/2809]  eta: 0:06:14  lr: 0.000026  min_lr: 0.000000  loss: 4.8870 (4.9016)  class_acc: 0.0417 (0.0479)  loss_scale: 65536.0000 (58925.3077)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1719  max mem: 15572
Epoch: [2]  [2190/2809]  eta: 0:06:08  lr: 0.000026  min_lr: 0.000000  loss: 4.8870 (4.9015)  class_acc: 0.0000 (0.0478)  loss_scale: 65536.0000 (58955.4797)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.0992  max mem: 15572
Epoch: [2]  [2200/2809]  eta: 0:06:02  lr: 0.000026  min_lr: 0.000000  loss: 4.8410 (4.9012)  class_acc: 0.0000 (0.0478)  loss_scale: 65536.0000 (58985.3776)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1024  max mem: 15572
Epoch: [2]  [2210/2809]  eta: 0:05:56  lr: 0.000026  min_lr: 0.000000  loss: 4.8291 (4.9011)  class_acc: 0.0417 (0.0479)  loss_scale: 65536.0000 (59015.0050)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.1540  max mem: 15572
[2025-01-15 15:43:00,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:43:00,351] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [2220/2809]  eta: 0:05:50  lr: 0.000026  min_lr: 0.000000  loss: 4.8795 (4.9011)  class_acc: 0.0417 (0.0480)  loss_scale: 65536.0000 (59221.4102)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0609  max mem: 15572
[2025-01-15 15:43:07,835] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7846
[2025-01-15 15:43:07,835] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:43:07,836] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2230/2809]  eta: 0:05:44  lr: 0.000026  min_lr: 0.000000  loss: 4.8304 (4.9006)  class_acc: 0.0833 (0.0481)  loss_scale: 131072.0000 (59455.3402)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0660  max mem: 15572
Epoch: [2]  [2240/2809]  eta: 0:05:38  lr: 0.000026  min_lr: 0.000000  loss: 4.7847 (4.9003)  class_acc: 0.0417 (0.0481)  loss_scale: 65536.0000 (59482.4739)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1237  max mem: 15572
Epoch: [2]  [2250/2809]  eta: 0:05:32  lr: 0.000026  min_lr: 0.000000  loss: 4.8494 (4.9001)  class_acc: 0.0417 (0.0481)  loss_scale: 65536.0000 (59509.3665)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0803  max mem: 15572
Epoch: [2]  [2260/2809]  eta: 0:05:26  lr: 0.000026  min_lr: 0.000000  loss: 4.8657 (4.8999)  class_acc: 0.0417 (0.0481)  loss_scale: 65536.0000 (59536.0212)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1048  max mem: 15572
Epoch: [2]  [2270/2809]  eta: 0:05:20  lr: 0.000026  min_lr: 0.000000  loss: 4.7767 (4.8993)  class_acc: 0.0417 (0.0481)  loss_scale: 65536.0000 (59562.4412)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.1932  max mem: 15572
Epoch: [2]  [2280/2809]  eta: 0:05:14  lr: 0.000026  min_lr: 0.000000  loss: 4.7361 (4.8990)  class_acc: 0.0417 (0.0482)  loss_scale: 65536.0000 (59588.6295)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.1875  max mem: 15572
Epoch: [2]  [2290/2809]  eta: 0:05:08  lr: 0.000026  min_lr: 0.000000  loss: 4.8042 (4.8989)  class_acc: 0.0417 (0.0482)  loss_scale: 65536.0000 (59614.5893)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1452  max mem: 15572
Epoch: [2]  [2300/2809]  eta: 0:05:03  lr: 0.000026  min_lr: 0.000000  loss: 4.8042 (4.8987)  class_acc: 0.0417 (0.0482)  loss_scale: 65536.0000 (59640.3233)  weight_decay: 0.0500 (0.0500)  time: 0.6675  data: 0.2207  max mem: 15572
Epoch: [2]  [2310/2809]  eta: 0:04:56  lr: 0.000026  min_lr: 0.000000  loss: 4.8198 (4.8986)  class_acc: 0.0417 (0.0483)  loss_scale: 65536.0000 (59665.8347)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1525  max mem: 15572
Epoch: [2]  [2320/2809]  eta: 0:04:50  lr: 0.000026  min_lr: 0.000000  loss: 4.8560 (4.8987)  class_acc: 0.0000 (0.0483)  loss_scale: 65536.0000 (59691.1262)  weight_decay: 0.0500 (0.0500)  time: 0.4669  data: 0.0008  max mem: 15572
Epoch: [2]  [2330/2809]  eta: 0:04:44  lr: 0.000027  min_lr: 0.000000  loss: 4.8933 (4.8989)  class_acc: 0.0000 (0.0482)  loss_scale: 65536.0000 (59716.2008)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1045  max mem: 15572
Epoch: [2]  [2340/2809]  eta: 0:04:38  lr: 0.000027  min_lr: 0.000000  loss: 4.8933 (4.8989)  class_acc: 0.0000 (0.0482)  loss_scale: 65536.0000 (59741.0611)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1542  max mem: 15572
Epoch: [2]  [2350/2809]  eta: 0:04:32  lr: 0.000027  min_lr: 0.000000  loss: 4.8900 (4.8989)  class_acc: 0.0000 (0.0481)  loss_scale: 65536.0000 (59765.7099)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1550  max mem: 15572
[2025-01-15 15:44:25,015] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:44:25,016] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [2360/2809]  eta: 0:04:27  lr: 0.000027  min_lr: 0.000000  loss: 4.8635 (4.8988)  class_acc: 0.0000 (0.0481)  loss_scale: 65536.0000 (59901.1809)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1749  max mem: 15572
[2025-01-15 15:44:29,961] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7983
[2025-01-15 15:44:29,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:44:29,962] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2370/2809]  eta: 0:04:21  lr: 0.000027  min_lr: 0.000000  loss: 4.8317 (4.8984)  class_acc: 0.0833 (0.0483)  loss_scale: 65536.0000 (60035.5091)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1405  max mem: 15572
Epoch: [2]  [2380/2809]  eta: 0:04:15  lr: 0.000027  min_lr: 0.000000  loss: 4.8317 (4.8982)  class_acc: 0.0833 (0.0484)  loss_scale: 65536.0000 (60058.6107)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1328  max mem: 15572
[2025-01-15 15:44:38,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=42, lr=[2.5867870428839465e-07, 2.5867870428839465e-07, 3.695410061262781e-07, 3.695410061262781e-07, 5.279157230375402e-07, 5.279157230375402e-07, 7.541653186250575e-07, 7.541653186250575e-07, 1.077379026607225e-06, 1.077379026607225e-06, 1.5391128951531786e-06, 1.5391128951531786e-06, 2.198732707361684e-06, 2.198732707361684e-06, 3.141046724802406e-06, 3.141046724802406e-06, 4.48720960686058e-06, 4.48720960686058e-06, 6.4102994383722576e-06, 6.4102994383722576e-06, 9.157570626246083e-06, 9.157570626246083e-06, 1.3082243751780119e-05, 1.3082243751780119e-05, 1.868891964540017e-05, 1.868891964540017e-05, 2.669845663628596e-05, 2.669845663628596e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:44:38,839] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=27.569256411374592, CurrSamplesPerSec=23.49378319677698, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [2390/2809]  eta: 0:04:09  lr: 0.000027  min_lr: 0.000000  loss: 4.7632 (4.8978)  class_acc: 0.0417 (0.0484)  loss_scale: 65536.0000 (60081.5190)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1014  max mem: 15572
Epoch: [2]  [2400/2809]  eta: 0:04:03  lr: 0.000027  min_lr: 0.000000  loss: 4.7598 (4.8975)  class_acc: 0.0417 (0.0485)  loss_scale: 65536.0000 (60104.2366)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1140  max mem: 15572
Epoch: [2]  [2410/2809]  eta: 0:03:57  lr: 0.000027  min_lr: 0.000000  loss: 4.8063 (4.8971)  class_acc: 0.0833 (0.0486)  loss_scale: 65536.0000 (60126.7657)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1550  max mem: 15572
Epoch: [2]  [2420/2809]  eta: 0:03:51  lr: 0.000027  min_lr: 0.000000  loss: 4.8467 (4.8968)  class_acc: 0.0417 (0.0487)  loss_scale: 65536.0000 (60149.1086)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1624  max mem: 15572
Epoch: [2]  [2430/2809]  eta: 0:03:45  lr: 0.000027  min_lr: 0.000000  loss: 4.8736 (4.8965)  class_acc: 0.0417 (0.0487)  loss_scale: 65536.0000 (60171.2678)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1368  max mem: 15572
Epoch: [2]  [2440/2809]  eta: 0:03:39  lr: 0.000027  min_lr: 0.000000  loss: 4.7788 (4.8962)  class_acc: 0.0417 (0.0486)  loss_scale: 65536.0000 (60193.2454)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1566  max mem: 15572
Epoch: [2]  [2450/2809]  eta: 0:03:33  lr: 0.000027  min_lr: 0.000000  loss: 4.8328 (4.8960)  class_acc: 0.0000 (0.0485)  loss_scale: 65536.0000 (60215.0437)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1642  max mem: 15572
Epoch: [2]  [2460/2809]  eta: 0:03:27  lr: 0.000027  min_lr: 0.000000  loss: 4.8641 (4.8957)  class_acc: 0.0417 (0.0486)  loss_scale: 65536.0000 (60236.6648)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1536  max mem: 15572
Epoch: [2]  [2470/2809]  eta: 0:03:21  lr: 0.000027  min_lr: 0.000000  loss: 4.8355 (4.8955)  class_acc: 0.0417 (0.0486)  loss_scale: 65536.0000 (60258.1109)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1043  max mem: 15572
Epoch: [2]  [2480/2809]  eta: 0:03:15  lr: 0.000027  min_lr: 0.000000  loss: 4.8721 (4.8956)  class_acc: 0.0417 (0.0488)  loss_scale: 65536.0000 (60279.3841)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0690  max mem: 15572
Epoch: [2]  [2490/2809]  eta: 0:03:09  lr: 0.000027  min_lr: 0.000000  loss: 4.8721 (4.8952)  class_acc: 0.0417 (0.0487)  loss_scale: 65536.0000 (60300.4866)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1023  max mem: 15572
[2025-01-15 15:45:44,705] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:45:44,705] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:45:45,562] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8114
[2025-01-15 15:45:45,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:45:45,562] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2500/2809]  eta: 0:03:03  lr: 0.000027  min_lr: 0.000000  loss: 4.8282 (4.8951)  class_acc: 0.0417 (0.0486)  loss_scale: 65536.0000 (60373.8281)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0464  max mem: 15572
Epoch: [2]  [2510/2809]  eta: 0:02:57  lr: 0.000027  min_lr: 0.000000  loss: 4.8594 (4.8950)  class_acc: 0.0417 (0.0487)  loss_scale: 65536.0000 (60394.3863)  weight_decay: 0.0500 (0.0500)  time: 0.4903  data: 0.0543  max mem: 15572
Epoch: [2]  [2520/2809]  eta: 0:02:51  lr: 0.000027  min_lr: 0.000000  loss: 4.8541 (4.8947)  class_acc: 0.0833 (0.0489)  loss_scale: 65536.0000 (60414.7814)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1608  max mem: 15572
Epoch: [2]  [2530/2809]  eta: 0:02:45  lr: 0.000027  min_lr: 0.000000  loss: 4.8301 (4.8945)  class_acc: 0.0833 (0.0490)  loss_scale: 65536.0000 (60435.0154)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1479  max mem: 15572
Epoch: [2]  [2540/2809]  eta: 0:02:39  lr: 0.000027  min_lr: 0.000000  loss: 4.9078 (4.8945)  class_acc: 0.0417 (0.0492)  loss_scale: 65536.0000 (60455.0901)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1190  max mem: 15572
Epoch: [2]  [2550/2809]  eta: 0:02:33  lr: 0.000027  min_lr: 0.000000  loss: 4.9078 (4.8944)  class_acc: 0.0417 (0.0492)  loss_scale: 65536.0000 (60475.0074)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1230  max mem: 15572
Epoch: [2]  [2560/2809]  eta: 0:02:27  lr: 0.000027  min_lr: 0.000000  loss: 4.8028 (4.8941)  class_acc: 0.0417 (0.0493)  loss_scale: 65536.0000 (60494.7692)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1733  max mem: 15572
Epoch: [2]  [2570/2809]  eta: 0:02:21  lr: 0.000027  min_lr: 0.000000  loss: 4.7422 (4.8938)  class_acc: 0.0417 (0.0494)  loss_scale: 65536.0000 (60514.3773)  weight_decay: 0.0500 (0.0500)  time: 0.6970  data: 0.2123  max mem: 15572
Epoch: [2]  [2580/2809]  eta: 0:02:16  lr: 0.000027  min_lr: 0.000000  loss: 4.8323 (4.8937)  class_acc: 0.0417 (0.0494)  loss_scale: 65536.0000 (60533.8334)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.1360  max mem: 15572
Epoch: [2]  [2590/2809]  eta: 0:02:10  lr: 0.000027  min_lr: 0.000000  loss: 4.8212 (4.8932)  class_acc: 0.0417 (0.0495)  loss_scale: 65536.0000 (60553.1393)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0730  max mem: 15572
Epoch: [2]  [2600/2809]  eta: 0:02:04  lr: 0.000027  min_lr: 0.000000  loss: 4.8209 (4.8930)  class_acc: 0.0417 (0.0495)  loss_scale: 65536.0000 (60572.2968)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0931  max mem: 15572
Epoch: [2]  [2610/2809]  eta: 0:01:58  lr: 0.000027  min_lr: 0.000000  loss: 4.8550 (4.8929)  class_acc: 0.0417 (0.0495)  loss_scale: 65536.0000 (60591.3075)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1017  max mem: 15572
Epoch: [2]  [2620/2809]  eta: 0:01:52  lr: 0.000027  min_lr: 0.000000  loss: 4.8483 (4.8927)  class_acc: 0.0417 (0.0495)  loss_scale: 65536.0000 (60610.1732)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0696  max mem: 15572
[2025-01-15 15:47:02,566] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:47:02,567] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:47:02,980] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8244
[2025-01-15 15:47:02,980] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:47:02,981] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2630/2809]  eta: 0:01:46  lr: 0.000028  min_lr: 0.000000  loss: 4.8362 (4.8926)  class_acc: 0.0417 (0.0495)  loss_scale: 65536.0000 (60653.8046)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1778  max mem: 15572
Epoch: [2]  [2640/2809]  eta: 0:01:40  lr: 0.000028  min_lr: 0.000000  loss: 4.8362 (4.8922)  class_acc: 0.0417 (0.0495)  loss_scale: 65536.0000 (60672.2908)  weight_decay: 0.0500 (0.0500)  time: 0.6645  data: 0.2073  max mem: 15572
Epoch: [2]  [2650/2809]  eta: 0:01:34  lr: 0.000028  min_lr: 0.000000  loss: 4.8204 (4.8921)  class_acc: 0.0417 (0.0495)  loss_scale: 65536.0000 (60690.6375)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0698  max mem: 15572
Epoch: [2]  [2660/2809]  eta: 0:01:28  lr: 0.000028  min_lr: 0.000000  loss: 4.7817 (4.8918)  class_acc: 0.0417 (0.0495)  loss_scale: 65536.0000 (60708.8463)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0885  max mem: 15572
Epoch: [2]  [2670/2809]  eta: 0:01:22  lr: 0.000028  min_lr: 0.000000  loss: 4.7698 (4.8918)  class_acc: 0.0417 (0.0495)  loss_scale: 65536.0000 (60726.9188)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1532  max mem: 15572
Epoch: [2]  [2680/2809]  eta: 0:01:16  lr: 0.000028  min_lr: 0.000000  loss: 4.8364 (4.8916)  class_acc: 0.0417 (0.0496)  loss_scale: 65536.0000 (60744.8564)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1105  max mem: 15572
Epoch: [2]  [2690/2809]  eta: 0:01:10  lr: 0.000028  min_lr: 0.000000  loss: 4.8101 (4.8911)  class_acc: 0.0833 (0.0497)  loss_scale: 65536.0000 (60762.6607)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0643  max mem: 15572
Epoch: [2]  [2700/2809]  eta: 0:01:04  lr: 0.000028  min_lr: 0.000000  loss: 4.7267 (4.8906)  class_acc: 0.0833 (0.0497)  loss_scale: 65536.0000 (60780.3332)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0457  max mem: 15572
Epoch: [2]  [2710/2809]  eta: 0:00:58  lr: 0.000028  min_lr: 0.000000  loss: 4.8017 (4.8906)  class_acc: 0.0417 (0.0497)  loss_scale: 65536.0000 (60797.8753)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1019  max mem: 15572
Epoch: [2]  [2720/2809]  eta: 0:00:52  lr: 0.000028  min_lr: 0.000000  loss: 4.8017 (4.8901)  class_acc: 0.0417 (0.0497)  loss_scale: 65536.0000 (60815.2885)  weight_decay: 0.0500 (0.0500)  time: 0.6802  data: 0.2160  max mem: 15572
Epoch: [2]  [2730/2809]  eta: 0:00:46  lr: 0.000028  min_lr: 0.000000  loss: 4.7250 (4.8896)  class_acc: 0.0417 (0.0499)  loss_scale: 65536.0000 (60832.5741)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1674  max mem: 15572
Epoch: [2]  [2740/2809]  eta: 0:00:40  lr: 0.000028  min_lr: 0.000000  loss: 4.7700 (4.8893)  class_acc: 0.0417 (0.0498)  loss_scale: 65536.0000 (60849.7337)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1184  max mem: 15572
Epoch: [2]  [2750/2809]  eta: 0:00:35  lr: 0.000028  min_lr: 0.000000  loss: 4.7700 (4.8890)  class_acc: 0.0000 (0.0498)  loss_scale: 65536.0000 (60866.7684)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1342  max mem: 15572
[2025-01-15 15:48:16,815] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:48:16,815] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:48:18,446] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8377
[2025-01-15 15:48:18,446] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:48:18,446] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2760/2809]  eta: 0:00:29  lr: 0.000028  min_lr: 0.000000  loss: 4.8059 (4.8889)  class_acc: 0.0000 (0.0497)  loss_scale: 65536.0000 (60978.6251)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0422  max mem: 15572
Epoch: [2]  [2770/2809]  eta: 0:00:23  lr: 0.000028  min_lr: 0.000000  loss: 4.8230 (4.8887)  class_acc: 0.0417 (0.0497)  loss_scale: 65536.0000 (60995.0718)  weight_decay: 0.0500 (0.0500)  time: 0.4075  data: 0.0004  max mem: 15572
Epoch: [2]  [2780/2809]  eta: 0:00:17  lr: 0.000028  min_lr: 0.000000  loss: 4.8173 (4.8885)  class_acc: 0.0417 (0.0498)  loss_scale: 65536.0000 (61011.4002)  weight_decay: 0.0500 (0.0500)  time: 0.4538  data: 0.0012  max mem: 15572
Epoch: [2]  [2790/2809]  eta: 0:00:11  lr: 0.000028  min_lr: 0.000000  loss: 4.8173 (4.8883)  class_acc: 0.0833 (0.0499)  loss_scale: 65536.0000 (61027.6116)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0013  max mem: 15572
Epoch: [2]  [2800/2809]  eta: 0:00:05  lr: 0.000028  min_lr: 0.000000  loss: 4.8015 (4.8880)  class_acc: 0.0833 (0.0500)  loss_scale: 65536.0000 (61043.7072)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0739  max mem: 15572
[2025-01-15 15:48:42,741] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8423
[2025-01-15 15:48:42,741] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:48:42,741] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [2808/2809]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000000  loss: 4.7998 (4.8877)  class_acc: 0.0833 (0.0500)  loss_scale: 65536.0000 (61009.8398)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1856  max mem: 15572
Epoch: [2] Total time: 0:27:43 (0.5921 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000000  loss: 4.7998 (4.8877)  class_acc: 0.0833 (0.0500)  loss_scale: 65536.0000 (61009.8398)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:35:59  loss: 3.9145 (3.9145)  acc1: 0.0000 (0.0000)  acc5: 55.5556 (55.5556)  time: 7.9387  data: 7.7240  max mem: 15572
Val:  [ 10/272]  eta: 0:04:14  loss: 5.1296 (4.7966)  acc1: 0.0000 (9.0909)  acc5: 5.5556 (21.7172)  time: 0.9715  data: 0.7649  max mem: 15572
Val:  [ 20/272]  eta: 0:02:52  loss: 4.8309 (4.7375)  acc1: 0.0000 (7.4074)  acc5: 0.0000 (17.7249)  time: 0.3212  data: 0.1110  max mem: 15572
Val:  [ 30/272]  eta: 0:02:10  loss: 4.6087 (4.6663)  acc1: 0.0000 (6.8100)  acc5: 11.1111 (16.8459)  time: 0.3009  data: 0.0771  max mem: 15572
Val:  [ 40/272]  eta: 0:01:56  loss: 4.1672 (4.5254)  acc1: 0.0000 (8.9431)  acc5: 22.2222 (26.1518)  time: 0.3084  data: 0.0811  max mem: 15572
Val:  [ 50/272]  eta: 0:01:49  loss: 3.9830 (4.4990)  acc1: 0.0000 (11.4379)  acc5: 66.6667 (29.3028)  time: 0.4192  data: 0.1975  max mem: 15572
Val:  [ 60/272]  eta: 0:01:37  loss: 3.9885 (4.4409)  acc1: 0.0000 (11.9308)  acc5: 44.4444 (31.3297)  time: 0.3808  data: 0.1696  max mem: 15572
Val:  [ 70/272]  eta: 0:01:30  loss: 4.0400 (4.3943)  acc1: 0.0000 (12.6761)  acc5: 38.8889 (31.3772)  time: 0.3355  data: 0.1206  max mem: 15572
Val:  [ 80/272]  eta: 0:01:26  loss: 4.1590 (4.4060)  acc1: 0.0000 (12.2771)  acc5: 5.5556 (31.2757)  time: 0.4068  data: 0.1805  max mem: 15572
Val:  [ 90/272]  eta: 0:01:19  loss: 4.8509 (4.4635)  acc1: 0.0000 (10.9280)  acc5: 0.0000 (27.8388)  time: 0.3924  data: 0.1776  max mem: 15572
Val:  [100/272]  eta: 0:01:13  loss: 4.8518 (4.5139)  acc1: 0.0000 (10.6711)  acc5: 0.0000 (26.4576)  time: 0.3464  data: 0.1373  max mem: 15572
Val:  [110/272]  eta: 0:01:08  loss: 4.8518 (4.5450)  acc1: 0.0000 (9.8098)  acc5: 0.0000 (24.6246)  time: 0.3728  data: 0.1670  max mem: 15572
Val:  [120/272]  eta: 0:01:03  loss: 4.8175 (4.5793)  acc1: 0.0000 (8.9991)  acc5: 0.0000 (22.7273)  time: 0.3603  data: 0.1684  max mem: 15572
Val:  [130/272]  eta: 0:00:58  loss: 4.8069 (4.5508)  acc1: 0.0000 (10.0933)  acc5: 0.0000 (25.3181)  time: 0.3440  data: 0.1550  max mem: 15572
Val:  [140/272]  eta: 0:00:53  loss: 4.2962 (4.5405)  acc1: 0.0000 (11.0323)  acc5: 11.1111 (25.5713)  time: 0.3331  data: 0.1236  max mem: 15572
Val:  [150/272]  eta: 0:00:48  loss: 4.3811 (4.5425)  acc1: 0.0000 (10.3385)  acc5: 5.5556 (24.3929)  time: 0.3173  data: 0.1025  max mem: 15572
Val:  [160/272]  eta: 0:00:44  loss: 4.2912 (4.5334)  acc1: 0.0000 (10.3865)  acc5: 11.1111 (25.7419)  time: 0.3740  data: 0.1626  max mem: 15572
Val:  [170/272]  eta: 0:00:40  loss: 4.3987 (4.5584)  acc1: 0.0000 (9.8765)  acc5: 22.2222 (24.6264)  time: 0.3907  data: 0.1846  max mem: 15572
Val:  [180/272]  eta: 0:00:35  loss: 4.8535 (4.5573)  acc1: 0.0000 (9.3616)  acc5: 0.0000 (23.5727)  time: 0.3109  data: 0.1136  max mem: 15572
Val:  [190/272]  eta: 0:00:31  loss: 4.7849 (4.5692)  acc1: 0.0000 (8.8714)  acc5: 0.0000 (22.9203)  time: 0.2255  data: 0.0316  max mem: 15572
Val:  [200/272]  eta: 0:00:26  loss: 4.8312 (4.5917)  acc1: 0.0000 (8.4301)  acc5: 0.0000 (21.7800)  time: 0.1843  data: 0.0006  max mem: 15572
Val:  [210/272]  eta: 0:00:22  loss: 4.8173 (4.5992)  acc1: 0.0000 (8.4255)  acc5: 0.0000 (21.4850)  time: 0.1816  data: 0.0006  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 4.5241 (4.5903)  acc1: 0.0000 (8.3459)  acc5: 11.1111 (21.5435)  time: 0.1987  data: 0.0142  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.3953 (4.5821)  acc1: 11.1111 (8.6821)  acc5: 27.7778 (22.1260)  time: 0.2823  data: 0.0937  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 4.4224 (4.5807)  acc1: 0.0000 (8.4601)  acc5: 22.2222 (22.0839)  time: 0.3374  data: 0.1373  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 4.7595 (4.6015)  acc1: 0.0000 (8.1673)  acc5: 5.5556 (21.4254)  time: 0.3029  data: 0.1012  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.4945 (4.5726)  acc1: 5.5556 (8.8123)  acc5: 22.2222 (23.0311)  time: 0.2873  data: 0.0902  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 4.1016 (4.5708)  acc1: 11.1111 (8.7741)  acc5: 50.0000 (23.1037)  time: 0.2442  data: 0.0634  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 4.1016 (4.5722)  acc1: 11.1111 (8.7651)  acc5: 50.0000 (23.0801)  time: 0.2253  data: 0.0510  max mem: 15572
Val: Total time: 0:01:33 (0.3430 s / it)
* Acc@1 8.765 Acc@5 23.080 loss 4.572
Accuracy of the network on the 4883 val videos: 8.8%
[2025-01-15 15:50:17,470] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 15:50:17,472] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 15:50:17,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 15:50:20,344] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 15:50:20,345] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 8.77%
Epoch: [3]  [   0/2809]  eta: 5:31:59  lr: 0.000028  min_lr: 0.000000  loss: 4.7881 (4.7881)  class_acc: 0.1667 (0.1667)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.0914  data: 6.6545  max mem: 15572
Epoch: [3]  [  10/2809]  eta: 0:59:04  lr: 0.000028  min_lr: 0.000000  loss: 4.7471 (4.7322)  class_acc: 0.0417 (0.0341)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2662  data: 0.8322  max mem: 15572
Epoch: [3]  [  20/2809]  eta: 0:45:10  lr: 0.000028  min_lr: 0.000000  loss: 4.7594 (4.7642)  class_acc: 0.0417 (0.0635)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6660  data: 0.2170  max mem: 15572
Epoch: [3]  [  30/2809]  eta: 0:37:07  lr: 0.000028  min_lr: 0.000000  loss: 4.7789 (4.7629)  class_acc: 0.0833 (0.0672)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0925  max mem: 15572
Epoch: [3]  [  40/2809]  eta: 0:34:11  lr: 0.000028  min_lr: 0.000000  loss: 4.7665 (4.7677)  class_acc: 0.0417 (0.0620)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0555  max mem: 15572
Epoch: [3]  [  50/2809]  eta: 0:32:40  lr: 0.000028  min_lr: 0.000000  loss: 4.7689 (4.7757)  class_acc: 0.0417 (0.0613)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1250  max mem: 15572
Epoch: [3]  [  60/2809]  eta: 0:31:24  lr: 0.000028  min_lr: 0.000000  loss: 4.8148 (4.7777)  class_acc: 0.0417 (0.0622)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1252  max mem: 15572
Epoch: [3]  [  70/2809]  eta: 0:30:11  lr: 0.000028  min_lr: 0.000000  loss: 4.8490 (4.7979)  class_acc: 0.0833 (0.0651)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0947  max mem: 15572
Epoch: [3]  [  80/2809]  eta: 0:29:37  lr: 0.000028  min_lr: 0.000000  loss: 4.8435 (4.8005)  class_acc: 0.0833 (0.0700)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.1016  max mem: 15572
Epoch: [3]  [  90/2809]  eta: 0:28:55  lr: 0.000028  min_lr: 0.000000  loss: 4.8170 (4.8026)  class_acc: 0.0417 (0.0701)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1150  max mem: 15572
Epoch: [3]  [ 100/2809]  eta: 0:29:46  lr: 0.000028  min_lr: 0.000000  loss: 4.8238 (4.8085)  class_acc: 0.0833 (0.0701)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6909  data: 0.2549  max mem: 15572
Epoch: [3]  [ 110/2809]  eta: 0:29:36  lr: 0.000028  min_lr: 0.000000  loss: 4.8283 (4.8121)  class_acc: 0.0417 (0.0702)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7483  data: 0.3004  max mem: 15572
Epoch: [3]  [ 120/2809]  eta: 0:29:15  lr: 0.000029  min_lr: 0.000000  loss: 4.8290 (4.8157)  class_acc: 0.0417 (0.0685)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1637  max mem: 15572
[2025-01-15 15:51:42,062] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:51:42,062] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 130/2809]  eta: 0:28:48  lr: 0.000029  min_lr: 0.000000  loss: 4.8616 (4.8185)  class_acc: 0.0417 (0.0687)  loss_scale: 32768.0000 (34268.8244)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1131  max mem: 15572
Epoch: [3]  [ 140/2809]  eta: 0:28:48  lr: 0.000029  min_lr: 0.000000  loss: 4.8876 (4.8247)  class_acc: 0.0417 (0.0683)  loss_scale: 65536.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1546  max mem: 15572
Epoch: [3]  [ 150/2809]  eta: 0:28:35  lr: 0.000029  min_lr: 0.000000  loss: 4.8039 (4.8197)  class_acc: 0.0417 (0.0687)  loss_scale: 65536.0000 (38410.1722)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.1796  max mem: 15572
Epoch: [3]  [ 160/2809]  eta: 0:28:12  lr: 0.000029  min_lr: 0.000000  loss: 4.7660 (4.8161)  class_acc: 0.0417 (0.0675)  loss_scale: 65536.0000 (40095.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1106  max mem: 15572
Epoch: [3]  [ 170/2809]  eta: 0:27:57  lr: 0.000029  min_lr: 0.000000  loss: 4.8310 (4.8243)  class_acc: 0.0417 (0.0663)  loss_scale: 65536.0000 (41582.7836)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1022  max mem: 15572
Epoch: [3]  [ 180/2809]  eta: 0:27:47  lr: 0.000029  min_lr: 0.000000  loss: 4.8947 (4.8262)  class_acc: 0.0417 (0.0656)  loss_scale: 65536.0000 (42906.1657)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1502  max mem: 15572
Epoch: [3]  [ 190/2809]  eta: 0:27:23  lr: 0.000029  min_lr: 0.000000  loss: 4.8815 (4.8281)  class_acc: 0.0417 (0.0659)  loss_scale: 65536.0000 (44090.9738)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1245  max mem: 15572
Epoch: [3]  [ 200/2809]  eta: 0:27:18  lr: 0.000029  min_lr: 0.000000  loss: 4.8050 (4.8286)  class_acc: 0.0833 (0.0674)  loss_scale: 65536.0000 (45157.8905)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1266  max mem: 15572
Epoch: [3]  [ 210/2809]  eta: 0:27:01  lr: 0.000029  min_lr: 0.000000  loss: 4.8102 (4.8301)  class_acc: 0.0833 (0.0679)  loss_scale: 65536.0000 (46123.6777)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1330  max mem: 15572
Epoch: [3]  [ 220/2809]  eta: 0:26:43  lr: 0.000029  min_lr: 0.000000  loss: 4.8987 (4.8334)  class_acc: 0.0417 (0.0666)  loss_scale: 65536.0000 (47002.0633)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0875  max mem: 15572
Epoch: [3]  [ 230/2809]  eta: 0:26:35  lr: 0.000029  min_lr: 0.000000  loss: 4.9110 (4.8332)  class_acc: 0.0417 (0.0658)  loss_scale: 65536.0000 (47804.3983)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1161  max mem: 15572
Epoch: [3]  [ 240/2809]  eta: 0:26:32  lr: 0.000029  min_lr: 0.000000  loss: 4.9181 (4.8367)  class_acc: 0.0417 (0.0647)  loss_scale: 65536.0000 (48540.1494)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1642  max mem: 15572
Epoch: [3]  [ 250/2809]  eta: 0:26:19  lr: 0.000029  min_lr: 0.000000  loss: 4.9119 (4.8363)  class_acc: 0.0417 (0.0646)  loss_scale: 65536.0000 (49217.2749)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1370  max mem: 15572
[2025-01-15 15:52:57,312] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:52:57,313] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:52:58,804] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8683
[2025-01-15 15:52:58,805] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:52:58,805] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 260/2809]  eta: 0:26:06  lr: 0.000029  min_lr: 0.000000  loss: 4.7617 (4.8315)  class_acc: 0.0417 (0.0645)  loss_scale: 65536.0000 (50595.8008)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0701  max mem: 15572
Epoch: [3]  [ 270/2809]  eta: 0:25:55  lr: 0.000029  min_lr: 0.000000  loss: 4.8080 (4.8301)  class_acc: 0.0417 (0.0635)  loss_scale: 65536.0000 (51147.0996)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0758  max mem: 15572
Epoch: [3]  [ 280/2809]  eta: 0:25:50  lr: 0.000029  min_lr: 0.000000  loss: 4.8255 (4.8304)  class_acc: 0.0417 (0.0627)  loss_scale: 65536.0000 (51659.1601)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1341  max mem: 15572
Epoch: [3]  [ 290/2809]  eta: 0:25:34  lr: 0.000029  min_lr: 0.000000  loss: 4.8540 (4.8312)  class_acc: 0.0417 (0.0627)  loss_scale: 65536.0000 (52136.0275)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1165  max mem: 15572
Epoch: [3]  [ 300/2809]  eta: 0:25:27  lr: 0.000029  min_lr: 0.000000  loss: 4.8643 (4.8327)  class_acc: 0.0833 (0.0628)  loss_scale: 65536.0000 (52581.2093)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1165  max mem: 15572
Epoch: [3]  [ 310/2809]  eta: 0:25:23  lr: 0.000029  min_lr: 0.000000  loss: 4.9181 (4.8335)  class_acc: 0.0833 (0.0638)  loss_scale: 65536.0000 (52997.7621)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1705  max mem: 15572
Epoch: [3]  [ 320/2809]  eta: 0:25:16  lr: 0.000029  min_lr: 0.000000  loss: 4.8416 (4.8340)  class_acc: 0.0833 (0.0639)  loss_scale: 65536.0000 (53388.3614)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1356  max mem: 15572
Epoch: [3]  [ 330/2809]  eta: 0:25:05  lr: 0.000029  min_lr: 0.000000  loss: 4.8416 (4.8358)  class_acc: 0.0417 (0.0646)  loss_scale: 65536.0000 (53755.3595)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1057  max mem: 15572
Epoch: [3]  [ 340/2809]  eta: 0:25:05  lr: 0.000029  min_lr: 0.000000  loss: 4.8394 (4.8350)  class_acc: 0.0417 (0.0644)  loss_scale: 65536.0000 (54100.8328)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1719  max mem: 15572
Epoch: [3]  [ 350/2809]  eta: 0:24:55  lr: 0.000029  min_lr: 0.000000  loss: 4.7466 (4.8321)  class_acc: 0.0417 (0.0635)  loss_scale: 65536.0000 (54426.6211)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1676  max mem: 15572
Epoch: [3]  [ 360/2809]  eta: 0:24:50  lr: 0.000029  min_lr: 0.000000  loss: 4.8504 (4.8331)  class_acc: 0.0417 (0.0637)  loss_scale: 65536.0000 (54734.3601)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1356  max mem: 15572
Epoch: [3]  [ 370/2809]  eta: 0:24:52  lr: 0.000029  min_lr: 0.000000  loss: 4.8736 (4.8331)  class_acc: 0.0417 (0.0631)  loss_scale: 65536.0000 (55025.5094)  weight_decay: 0.0500 (0.0500)  time: 0.6728  data: 0.2028  max mem: 15572
Epoch: [3]  [ 380/2809]  eta: 0:24:41  lr: 0.000029  min_lr: 0.000000  loss: 4.8198 (4.8310)  class_acc: 0.0417 (0.0632)  loss_scale: 65536.0000 (55301.3753)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1552  max mem: 15572
[2025-01-15 15:54:16,292] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:54:16,293] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 390/2809]  eta: 0:24:32  lr: 0.000029  min_lr: 0.000000  loss: 4.8015 (4.8302)  class_acc: 0.0417 (0.0636)  loss_scale: 65536.0000 (56568.7980)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0876  max mem: 15572
Epoch: [3]  [ 400/2809]  eta: 0:24:24  lr: 0.000029  min_lr: 0.000000  loss: 4.8306 (4.8314)  class_acc: 0.0417 (0.0632)  loss_scale: 131072.0000 (58426.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1159  max mem: 15572
[2025-01-15 15:54:29,074] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8833
[2025-01-15 15:54:29,074] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:54:29,075] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 410/2809]  eta: 0:24:21  lr: 0.000029  min_lr: 0.000000  loss: 4.8552 (4.8315)  class_acc: 0.0000 (0.0622)  loss_scale: 131072.0000 (59396.9830)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1481  max mem: 15572
Epoch: [3]  [ 420/2809]  eta: 0:24:17  lr: 0.000030  min_lr: 0.000000  loss: 4.8335 (4.8327)  class_acc: 0.0417 (0.0621)  loss_scale: 65536.0000 (59542.8029)  weight_decay: 0.0500 (0.0500)  time: 0.6579  data: 0.1803  max mem: 15572
Epoch: [3]  [ 430/2809]  eta: 0:24:08  lr: 0.000030  min_lr: 0.000000  loss: 4.8059 (4.8320)  class_acc: 0.0417 (0.0623)  loss_scale: 65536.0000 (59681.8561)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1535  max mem: 15572
Epoch: [3]  [ 440/2809]  eta: 0:24:02  lr: 0.000030  min_lr: 0.000000  loss: 4.7899 (4.8303)  class_acc: 0.0417 (0.0622)  loss_scale: 65536.0000 (59814.6032)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1319  max mem: 15572
Epoch: [3]  [ 450/2809]  eta: 0:23:53  lr: 0.000030  min_lr: 0.000000  loss: 4.8354 (4.8314)  class_acc: 0.0417 (0.0620)  loss_scale: 65536.0000 (59941.4634)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1203  max mem: 15572
Epoch: [3]  [ 460/2809]  eta: 0:23:45  lr: 0.000030  min_lr: 0.000000  loss: 4.8301 (4.8311)  class_acc: 0.0417 (0.0616)  loss_scale: 65536.0000 (60062.8200)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1058  max mem: 15572
Epoch: [3]  [ 470/2809]  eta: 0:23:39  lr: 0.000030  min_lr: 0.000000  loss: 4.8117 (4.8296)  class_acc: 0.0417 (0.0617)  loss_scale: 65536.0000 (60179.0234)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1338  max mem: 15572
Epoch: [3]  [ 480/2809]  eta: 0:23:34  lr: 0.000030  min_lr: 0.000000  loss: 4.8393 (4.8285)  class_acc: 0.0417 (0.0617)  loss_scale: 65536.0000 (60290.3950)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1684  max mem: 15572
Epoch: [3]  [ 490/2809]  eta: 0:23:28  lr: 0.000030  min_lr: 0.000000  loss: 4.7762 (4.8274)  class_acc: 0.0417 (0.0614)  loss_scale: 65536.0000 (60397.2301)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1688  max mem: 15572
Epoch: [3]  [ 500/2809]  eta: 0:23:23  lr: 0.000030  min_lr: 0.000000  loss: 4.7601 (4.8270)  class_acc: 0.0417 (0.0614)  loss_scale: 65536.0000 (60499.8004)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1583  max mem: 15572
Epoch: [3]  [ 510/2809]  eta: 0:23:14  lr: 0.000030  min_lr: 0.000000  loss: 4.8298 (4.8262)  class_acc: 0.0417 (0.0616)  loss_scale: 65536.0000 (60598.3562)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1397  max mem: 15572
Epoch: [3]  [ 520/2809]  eta: 0:23:06  lr: 0.000030  min_lr: 0.000000  loss: 4.8043 (4.8253)  class_acc: 0.0833 (0.0621)  loss_scale: 65536.0000 (60693.1286)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1052  max mem: 15572
Epoch: [3]  [ 530/2809]  eta: 0:22:56  lr: 0.000030  min_lr: 0.000000  loss: 4.7790 (4.8252)  class_acc: 0.0833 (0.0622)  loss_scale: 65536.0000 (60784.3315)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0877  max mem: 15572
[2025-01-15 15:55:45,092] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:55:45,093] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:55:45,980] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8964
[2025-01-15 15:55:45,981] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:55:45,981] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 540/2809]  eta: 0:22:50  lr: 0.000030  min_lr: 0.000000  loss: 4.8667 (4.8263)  class_acc: 0.0417 (0.0622)  loss_scale: 65536.0000 (61114.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1106  max mem: 15572
Epoch: [3]  [ 550/2809]  eta: 0:22:48  lr: 0.000030  min_lr: 0.000000  loss: 4.8556 (4.8257)  class_acc: 0.0417 (0.0622)  loss_scale: 65536.0000 (61194.6860)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.1946  max mem: 15572
Epoch: [3]  [ 560/2809]  eta: 0:22:43  lr: 0.000030  min_lr: 0.000000  loss: 4.8072 (4.8260)  class_acc: 0.0417 (0.0626)  loss_scale: 65536.0000 (61272.0713)  weight_decay: 0.0500 (0.0500)  time: 0.6645  data: 0.2208  max mem: 15572
Epoch: [3]  [ 570/2809]  eta: 0:22:36  lr: 0.000030  min_lr: 0.000000  loss: 4.8217 (4.8255)  class_acc: 0.0833 (0.0633)  loss_scale: 65536.0000 (61346.7461)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1588  max mem: 15572
[2025-01-15 15:56:07,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=49, lr=[2.9101758468449346e-07, 2.9101758468449346e-07, 4.157394066921336e-07, 4.157394066921336e-07, 5.939134381316195e-07, 5.939134381316195e-07, 8.484477687594565e-07, 8.484477687594565e-07, 1.2120682410849378e-06, 1.2120682410849378e-06, 1.7315260586927683e-06, 1.7315260586927683e-06, 2.4736086552753836e-06, 2.4736086552753836e-06, 3.5337266503934053e-06, 3.5337266503934053e-06, 5.0481809291334365e-06, 5.0481809291334365e-06, 7.211687041619195e-06, 7.211687041619195e-06, 1.0302410059455993e-05, 1.0302410059455993e-05, 1.4717728656365707e-05, 1.4717728656365707e-05, 2.102532665195101e-05, 2.102532665195101e-05, 3.0036180931358588e-05, 3.0036180931358588e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:56:07,522] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=27.601964346946776, CurrSamplesPerSec=27.832758963923112, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [ 580/2809]  eta: 0:22:28  lr: 0.000030  min_lr: 0.000000  loss: 4.8289 (4.8244)  class_acc: 0.0833 (0.0639)  loss_scale: 65536.0000 (61418.8503)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1167  max mem: 15572
Epoch: [3]  [ 590/2809]  eta: 0:22:21  lr: 0.000030  min_lr: 0.000000  loss: 4.8289 (4.8245)  class_acc: 0.0833 (0.0639)  loss_scale: 65536.0000 (61488.5144)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0973  max mem: 15572
Epoch: [3]  [ 600/2809]  eta: 0:22:12  lr: 0.000030  min_lr: 0.000000  loss: 4.7664 (4.8248)  class_acc: 0.0417 (0.0636)  loss_scale: 65536.0000 (61555.8602)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0660  max mem: 15572
Epoch: [3]  [ 610/2809]  eta: 0:22:05  lr: 0.000030  min_lr: 0.000000  loss: 4.7722 (4.8234)  class_acc: 0.0417 (0.0636)  loss_scale: 65536.0000 (61621.0016)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0914  max mem: 15572
Epoch: [3]  [ 620/2809]  eta: 0:21:56  lr: 0.000030  min_lr: 0.000000  loss: 4.8122 (4.8239)  class_acc: 0.0417 (0.0638)  loss_scale: 65536.0000 (61684.0451)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0910  max mem: 15572
Epoch: [3]  [ 630/2809]  eta: 0:21:47  lr: 0.000030  min_lr: 0.000000  loss: 4.8709 (4.8248)  class_acc: 0.0833 (0.0638)  loss_scale: 65536.0000 (61745.0903)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0623  max mem: 15572
Epoch: [3]  [ 640/2809]  eta: 0:21:42  lr: 0.000030  min_lr: 0.000000  loss: 4.8163 (4.8238)  class_acc: 0.0417 (0.0637)  loss_scale: 65536.0000 (61804.2309)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1248  max mem: 15572
Epoch: [3]  [ 650/2809]  eta: 0:21:37  lr: 0.000030  min_lr: 0.000000  loss: 4.7566 (4.8232)  class_acc: 0.0833 (0.0639)  loss_scale: 65536.0000 (61861.5545)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1683  max mem: 15572
Epoch: [3]  [ 660/2809]  eta: 0:21:29  lr: 0.000030  min_lr: 0.000000  loss: 4.7765 (4.8233)  class_acc: 0.0417 (0.0635)  loss_scale: 65536.0000 (61917.1437)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1218  max mem: 15572
[2025-01-15 15:57:02,099] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:57:02,100] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 670/2809]  eta: 0:21:25  lr: 0.000030  min_lr: 0.000000  loss: 4.7890 (4.8228)  class_acc: 0.0417 (0.0639)  loss_scale: 65536.0000 (62459.4218)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1386  max mem: 15572
[2025-01-15 15:57:04,435] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9098
[2025-01-15 15:57:04,436] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:57:04,436] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 680/2809]  eta: 0:21:18  lr: 0.000030  min_lr: 0.000000  loss: 4.8170 (4.8234)  class_acc: 0.0417 (0.0638)  loss_scale: 65536.0000 (62504.5991)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1492  max mem: 15572
Epoch: [3]  [ 690/2809]  eta: 0:21:11  lr: 0.000030  min_lr: 0.000000  loss: 4.8329 (4.8229)  class_acc: 0.0833 (0.0644)  loss_scale: 65536.0000 (62548.4689)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.0933  max mem: 15572
Epoch: [3]  [ 700/2809]  eta: 0:21:07  lr: 0.000030  min_lr: 0.000000  loss: 4.7783 (4.8221)  class_acc: 0.0417 (0.0641)  loss_scale: 65536.0000 (62591.0870)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1129  max mem: 15572
Epoch: [3]  [ 710/2809]  eta: 0:21:00  lr: 0.000030  min_lr: 0.000000  loss: 4.7626 (4.8218)  class_acc: 0.0417 (0.0641)  loss_scale: 65536.0000 (62632.5063)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1133  max mem: 15572
Epoch: [3]  [ 720/2809]  eta: 0:20:54  lr: 0.000031  min_lr: 0.000000  loss: 4.7800 (4.8218)  class_acc: 0.0417 (0.0639)  loss_scale: 65536.0000 (62672.7767)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1118  max mem: 15572
Epoch: [3]  [ 730/2809]  eta: 0:20:47  lr: 0.000031  min_lr: 0.000000  loss: 4.8531 (4.8226)  class_acc: 0.0417 (0.0639)  loss_scale: 65536.0000 (62711.9453)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1224  max mem: 15572
Epoch: [3]  [ 740/2809]  eta: 0:20:40  lr: 0.000031  min_lr: 0.000000  loss: 4.8426 (4.8228)  class_acc: 0.0417 (0.0639)  loss_scale: 65536.0000 (62750.0567)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0823  max mem: 15572
Epoch: [3]  [ 750/2809]  eta: 0:20:32  lr: 0.000031  min_lr: 0.000000  loss: 4.8039 (4.8226)  class_acc: 0.0417 (0.0635)  loss_scale: 65536.0000 (62787.1531)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0657  max mem: 15572
Epoch: [3]  [ 760/2809]  eta: 0:20:26  lr: 0.000031  min_lr: 0.000000  loss: 4.8232 (4.8228)  class_acc: 0.0417 (0.0636)  loss_scale: 65536.0000 (62823.2746)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1117  max mem: 15572
Epoch: [3]  [ 770/2809]  eta: 0:20:18  lr: 0.000031  min_lr: 0.000000  loss: 4.8454 (4.8227)  class_acc: 0.0833 (0.0638)  loss_scale: 65536.0000 (62858.4591)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1179  max mem: 15572
Epoch: [3]  [ 780/2809]  eta: 0:20:11  lr: 0.000031  min_lr: 0.000000  loss: 4.8163 (4.8224)  class_acc: 0.0833 (0.0645)  loss_scale: 65536.0000 (62892.7426)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1000  max mem: 15572
Epoch: [3]  [ 790/2809]  eta: 0:20:06  lr: 0.000031  min_lr: 0.000000  loss: 4.7676 (4.8216)  class_acc: 0.0833 (0.0647)  loss_scale: 65536.0000 (62926.1593)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1330  max mem: 15572
[2025-01-15 15:58:19,562] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:58:19,563] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 800/2809]  eta: 0:20:00  lr: 0.000031  min_lr: 0.000000  loss: 4.7758 (4.8213)  class_acc: 0.0833 (0.0648)  loss_scale: 65536.0000 (63040.5593)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1477  max mem: 15572
[2025-01-15 15:58:23,719] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9232
[2025-01-15 15:58:23,720] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:58:23,720] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 810/2809]  eta: 0:19:56  lr: 0.000031  min_lr: 0.000000  loss: 4.7957 (4.8208)  class_acc: 0.0833 (0.0647)  loss_scale: 65536.0000 (63394.5647)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.1630  max mem: 15572
Epoch: [3]  [ 820/2809]  eta: 0:19:48  lr: 0.000031  min_lr: 0.000000  loss: 4.7957 (4.8201)  class_acc: 0.0417 (0.0645)  loss_scale: 65536.0000 (63420.6480)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1271  max mem: 15572
Epoch: [3]  [ 830/2809]  eta: 0:19:43  lr: 0.000031  min_lr: 0.000000  loss: 4.7165 (4.8191)  class_acc: 0.0417 (0.0645)  loss_scale: 65536.0000 (63446.1035)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1077  max mem: 15572
Epoch: [3]  [ 840/2809]  eta: 0:19:38  lr: 0.000031  min_lr: 0.000000  loss: 4.6842 (4.8182)  class_acc: 0.0833 (0.0648)  loss_scale: 65536.0000 (63470.9536)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.1721  max mem: 15572
Epoch: [3]  [ 850/2809]  eta: 0:19:29  lr: 0.000031  min_lr: 0.000000  loss: 4.7633 (4.8185)  class_acc: 0.0833 (0.0648)  loss_scale: 65536.0000 (63495.2197)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0990  max mem: 15572
Epoch: [3]  [ 860/2809]  eta: 0:19:22  lr: 0.000031  min_lr: 0.000000  loss: 4.6485 (4.8173)  class_acc: 0.0417 (0.0647)  loss_scale: 65536.0000 (63518.9222)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0319  max mem: 15572
Epoch: [3]  [ 870/2809]  eta: 0:19:15  lr: 0.000031  min_lr: 0.000000  loss: 4.6882 (4.8165)  class_acc: 0.0417 (0.0649)  loss_scale: 65536.0000 (63542.0804)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0628  max mem: 15572
Epoch: [3]  [ 880/2809]  eta: 0:19:09  lr: 0.000031  min_lr: 0.000000  loss: 4.7671 (4.8164)  class_acc: 0.0417 (0.0649)  loss_scale: 65536.0000 (63564.7128)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1133  max mem: 15572
Epoch: [3]  [ 890/2809]  eta: 0:19:03  lr: 0.000031  min_lr: 0.000000  loss: 4.7650 (4.8155)  class_acc: 0.0417 (0.0650)  loss_scale: 65536.0000 (63586.8373)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1612  max mem: 15572
Epoch: [3]  [ 900/2809]  eta: 0:18:56  lr: 0.000031  min_lr: 0.000000  loss: 4.6952 (4.8143)  class_acc: 0.0417 (0.0649)  loss_scale: 65536.0000 (63608.4706)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1148  max mem: 15572
Epoch: [3]  [ 910/2809]  eta: 0:18:50  lr: 0.000031  min_lr: 0.000000  loss: 4.7815 (4.8145)  class_acc: 0.0417 (0.0647)  loss_scale: 65536.0000 (63629.6290)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0991  max mem: 15572
Epoch: [3]  [ 920/2809]  eta: 0:18:45  lr: 0.000031  min_lr: 0.000000  loss: 4.8163 (4.8143)  class_acc: 0.0417 (0.0649)  loss_scale: 65536.0000 (63650.3279)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1604  max mem: 15572
Epoch: [3]  [ 930/2809]  eta: 0:18:40  lr: 0.000031  min_lr: 0.000000  loss: 4.7623 (4.8133)  class_acc: 0.0417 (0.0646)  loss_scale: 65536.0000 (63670.5822)  weight_decay: 0.0500 (0.0500)  time: 0.6522  data: 0.1841  max mem: 15572
[2025-01-15 15:59:37,826] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:59:37,827] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:59:40,752] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9366
[2025-01-15 15:59:40,752] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:59:40,753] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 940/2809]  eta: 0:18:33  lr: 0.000031  min_lr: 0.000000  loss: 4.7758 (4.8136)  class_acc: 0.0000 (0.0645)  loss_scale: 65536.0000 (64038.6312)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1150  max mem: 15572
Epoch: [3]  [ 950/2809]  eta: 0:18:26  lr: 0.000031  min_lr: 0.000000  loss: 4.7758 (4.8137)  class_acc: 0.0000 (0.0641)  loss_scale: 65536.0000 (64054.3764)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0709  max mem: 15572
Epoch: [3]  [ 960/2809]  eta: 0:18:20  lr: 0.000031  min_lr: 0.000000  loss: 4.7362 (4.8132)  class_acc: 0.0417 (0.0642)  loss_scale: 65536.0000 (64069.7940)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1243  max mem: 15572
Epoch: [3]  [ 970/2809]  eta: 0:18:15  lr: 0.000031  min_lr: 0.000000  loss: 4.8044 (4.8136)  class_acc: 0.0833 (0.0645)  loss_scale: 65536.0000 (64084.8939)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1397  max mem: 15572
Epoch: [3]  [ 980/2809]  eta: 0:18:11  lr: 0.000031  min_lr: 0.000000  loss: 4.8249 (4.8130)  class_acc: 0.0833 (0.0647)  loss_scale: 65536.0000 (64099.6860)  weight_decay: 0.0500 (0.0500)  time: 0.6676  data: 0.1770  max mem: 15572
Epoch: [3]  [ 990/2809]  eta: 0:18:04  lr: 0.000031  min_lr: 0.000000  loss: 4.7631 (4.8127)  class_acc: 0.0833 (0.0648)  loss_scale: 65536.0000 (64114.1796)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1533  max mem: 15572
Epoch: [3]  [1000/2809]  eta: 0:17:59  lr: 0.000031  min_lr: 0.000000  loss: 4.7631 (4.8123)  class_acc: 0.0833 (0.0651)  loss_scale: 65536.0000 (64128.3836)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1278  max mem: 15572
Epoch: [3]  [1010/2809]  eta: 0:17:51  lr: 0.000031  min_lr: 0.000000  loss: 4.7910 (4.8120)  class_acc: 0.0833 (0.0652)  loss_scale: 65536.0000 (64142.3066)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1246  max mem: 15572
Epoch: [3]  [1020/2809]  eta: 0:17:47  lr: 0.000032  min_lr: 0.000000  loss: 4.8018 (4.8125)  class_acc: 0.0417 (0.0653)  loss_scale: 65536.0000 (64155.9569)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1367  max mem: 15572
Epoch: [3]  [1030/2809]  eta: 0:17:41  lr: 0.000032  min_lr: 0.000000  loss: 4.8195 (4.8123)  class_acc: 0.0833 (0.0656)  loss_scale: 65536.0000 (64169.3424)  weight_decay: 0.0500 (0.0500)  time: 0.6576  data: 0.1941  max mem: 15572
Epoch: [3]  [1040/2809]  eta: 0:17:37  lr: 0.000032  min_lr: 0.000000  loss: 4.7432 (4.8115)  class_acc: 0.0833 (0.0656)  loss_scale: 65536.0000 (64182.4707)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.2023  max mem: 15572
Epoch: [3]  [1050/2809]  eta: 0:17:31  lr: 0.000032  min_lr: 0.000000  loss: 4.7677 (4.8121)  class_acc: 0.0833 (0.0657)  loss_scale: 65536.0000 (64195.3492)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.1632  max mem: 15572
Epoch: [3]  [1060/2809]  eta: 0:17:26  lr: 0.000032  min_lr: 0.000000  loss: 4.8127 (4.8125)  class_acc: 0.0417 (0.0654)  loss_scale: 65536.0000 (64207.9849)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1348  max mem: 15572
[2025-01-15 16:01:00,355] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:01:00,355] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1070/2809]  eta: 0:17:19  lr: 0.000032  min_lr: 0.000000  loss: 4.7834 (4.8122)  class_acc: 0.0417 (0.0653)  loss_scale: 65536.0000 (64403.9589)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1275  max mem: 15572
[2025-01-15 16:01:03,232] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9498
[2025-01-15 16:01:03,233] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:01:03,233] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1080/2809]  eta: 0:17:13  lr: 0.000032  min_lr: 0.000000  loss: 4.7995 (4.8122)  class_acc: 0.0417 (0.0653)  loss_scale: 65536.0000 (64414.4311)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1204  max mem: 15572
Epoch: [3]  [1090/2809]  eta: 0:17:08  lr: 0.000032  min_lr: 0.000000  loss: 4.7995 (4.8125)  class_acc: 0.0833 (0.0655)  loss_scale: 65536.0000 (64424.7113)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1475  max mem: 15572
Epoch: [3]  [1100/2809]  eta: 0:17:02  lr: 0.000032  min_lr: 0.000000  loss: 4.7694 (4.8116)  class_acc: 0.0833 (0.0654)  loss_scale: 65536.0000 (64434.8047)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1199  max mem: 15572
Epoch: [3]  [1110/2809]  eta: 0:16:55  lr: 0.000032  min_lr: 0.000000  loss: 4.7737 (4.8115)  class_acc: 0.0833 (0.0656)  loss_scale: 65536.0000 (64444.7165)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0887  max mem: 15572
Epoch: [3]  [1120/2809]  eta: 0:16:48  lr: 0.000032  min_lr: 0.000000  loss: 4.7931 (4.8117)  class_acc: 0.0833 (0.0656)  loss_scale: 65536.0000 (64454.4514)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0683  max mem: 15572
Epoch: [3]  [1130/2809]  eta: 0:16:44  lr: 0.000032  min_lr: 0.000000  loss: 4.8021 (4.8112)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (64464.0141)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.1635  max mem: 15572
Epoch: [3]  [1140/2809]  eta: 0:16:38  lr: 0.000032  min_lr: 0.000000  loss: 4.8530 (4.8120)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (64473.4093)  weight_decay: 0.0500 (0.0500)  time: 0.6702  data: 0.1954  max mem: 15572
Epoch: [3]  [1150/2809]  eta: 0:16:32  lr: 0.000032  min_lr: 0.000000  loss: 4.8331 (4.8120)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (64482.6412)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1392  max mem: 15572
Epoch: [3]  [1160/2809]  eta: 0:16:26  lr: 0.000032  min_lr: 0.000000  loss: 4.7448 (4.8112)  class_acc: 0.0833 (0.0660)  loss_scale: 65536.0000 (64491.7140)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1203  max mem: 15572
Epoch: [3]  [1170/2809]  eta: 0:16:19  lr: 0.000032  min_lr: 0.000000  loss: 4.7448 (4.8108)  class_acc: 0.0417 (0.0660)  loss_scale: 65536.0000 (64500.6319)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0856  max mem: 15572
Epoch: [3]  [1180/2809]  eta: 0:16:14  lr: 0.000032  min_lr: 0.000000  loss: 4.7874 (4.8110)  class_acc: 0.0833 (0.0663)  loss_scale: 65536.0000 (64509.3988)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1421  max mem: 15572
Epoch: [3]  [1190/2809]  eta: 0:16:08  lr: 0.000032  min_lr: 0.000000  loss: 4.7753 (4.8106)  class_acc: 0.0417 (0.0664)  loss_scale: 65536.0000 (64518.0185)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1590  max mem: 15572
[2025-01-15 16:02:19,156] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:02:19,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1200/2809]  eta: 0:16:02  lr: 0.000032  min_lr: 0.000000  loss: 4.8138 (4.8105)  class_acc: 0.0417 (0.0667)  loss_scale: 65536.0000 (64581.0624)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1335  max mem: 15572
[2025-01-15 16:02:21,827] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9628
[2025-01-15 16:02:21,828] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:02:21,828] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1210/2809]  eta: 0:15:57  lr: 0.000032  min_lr: 0.000000  loss: 4.8494 (4.8109)  class_acc: 0.0833 (0.0668)  loss_scale: 65536.0000 (64588.9480)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.1857  max mem: 15572
Epoch: [3]  [1220/2809]  eta: 0:15:50  lr: 0.000032  min_lr: 0.000000  loss: 4.7508 (4.8107)  class_acc: 0.0417 (0.0667)  loss_scale: 65536.0000 (64596.7043)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1704  max mem: 15572
Epoch: [3]  [1230/2809]  eta: 0:15:45  lr: 0.000032  min_lr: 0.000000  loss: 4.8407 (4.8110)  class_acc: 0.0417 (0.0667)  loss_scale: 65536.0000 (64604.3347)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1402  max mem: 15572
Epoch: [3]  [1240/2809]  eta: 0:15:38  lr: 0.000032  min_lr: 0.000000  loss: 4.8881 (4.8118)  class_acc: 0.0417 (0.0667)  loss_scale: 65536.0000 (64611.8421)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1249  max mem: 15572
Epoch: [3]  [1250/2809]  eta: 0:15:31  lr: 0.000032  min_lr: 0.000000  loss: 4.8703 (4.8121)  class_acc: 0.0417 (0.0665)  loss_scale: 65536.0000 (64619.2294)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1053  max mem: 15572
Epoch: [3]  [1260/2809]  eta: 0:15:24  lr: 0.000032  min_lr: 0.000000  loss: 4.8306 (4.8122)  class_acc: 0.0417 (0.0663)  loss_scale: 65536.0000 (64626.4996)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0795  max mem: 15572
Epoch: [3]  [1270/2809]  eta: 0:15:19  lr: 0.000032  min_lr: 0.000000  loss: 4.7313 (4.8115)  class_acc: 0.0417 (0.0664)  loss_scale: 65536.0000 (64633.6554)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1326  max mem: 15572
Epoch: [3]  [1280/2809]  eta: 0:15:13  lr: 0.000032  min_lr: 0.000000  loss: 4.8010 (4.8124)  class_acc: 0.0417 (0.0661)  loss_scale: 65536.0000 (64640.6995)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1599  max mem: 15572
Epoch: [3]  [1290/2809]  eta: 0:15:08  lr: 0.000032  min_lr: 0.000000  loss: 4.8925 (4.8124)  class_acc: 0.0417 (0.0661)  loss_scale: 65536.0000 (64647.6344)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.1671  max mem: 15572
Epoch: [3]  [1300/2809]  eta: 0:15:01  lr: 0.000032  min_lr: 0.000000  loss: 4.7760 (4.8121)  class_acc: 0.0417 (0.0663)  loss_scale: 65536.0000 (64654.4627)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1228  max mem: 15572
Epoch: [3]  [1310/2809]  eta: 0:14:55  lr: 0.000032  min_lr: 0.000000  loss: 4.7760 (4.8121)  class_acc: 0.0417 (0.0660)  loss_scale: 65536.0000 (64661.1869)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0693  max mem: 15572
Epoch: [3]  [1320/2809]  eta: 0:14:49  lr: 0.000033  min_lr: 0.000000  loss: 4.7473 (4.8115)  class_acc: 0.0417 (0.0661)  loss_scale: 65536.0000 (64667.8092)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1397  max mem: 15572
[2025-01-15 16:03:36,618] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:03:36,618] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1330/2809]  eta: 0:14:44  lr: 0.000033  min_lr: 0.000000  loss: 4.7841 (4.8119)  class_acc: 0.0417 (0.0661)  loss_scale: 65536.0000 (64723.5702)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1559  max mem: 15572
[2025-01-15 16:03:40,692] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9762
[2025-01-15 16:03:40,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:03:40,692] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1340/2809]  eta: 0:14:38  lr: 0.000033  min_lr: 0.000000  loss: 4.7988 (4.8117)  class_acc: 0.0417 (0.0662)  loss_scale: 65536.0000 (64925.1126)  weight_decay: 0.0500 (0.0500)  time: 0.6381  data: 0.1698  max mem: 15572
Epoch: [3]  [1350/2809]  eta: 0:14:32  lr: 0.000033  min_lr: 0.000000  loss: 4.8116 (4.8123)  class_acc: 0.0417 (0.0659)  loss_scale: 65536.0000 (64929.6343)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1350  max mem: 15572
Epoch: [3]  [1360/2809]  eta: 0:14:26  lr: 0.000033  min_lr: 0.000000  loss: 4.8748 (4.8124)  class_acc: 0.0417 (0.0659)  loss_scale: 65536.0000 (64934.0896)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1012  max mem: 15572
Epoch: [3]  [1370/2809]  eta: 0:14:19  lr: 0.000033  min_lr: 0.000000  loss: 4.8294 (4.8128)  class_acc: 0.0417 (0.0659)  loss_scale: 65536.0000 (64938.4799)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1038  max mem: 15572
Epoch: [3]  [1380/2809]  eta: 0:14:14  lr: 0.000033  min_lr: 0.000000  loss: 4.8653 (4.8130)  class_acc: 0.0417 (0.0659)  loss_scale: 65536.0000 (64942.8067)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1826  max mem: 15572
Epoch: [3]  [1390/2809]  eta: 0:14:08  lr: 0.000033  min_lr: 0.000000  loss: 4.7443 (4.8121)  class_acc: 0.0833 (0.0663)  loss_scale: 65536.0000 (64947.0712)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.1825  max mem: 15572
Epoch: [3]  [1400/2809]  eta: 0:14:03  lr: 0.000033  min_lr: 0.000000  loss: 4.7160 (4.8120)  class_acc: 0.0833 (0.0664)  loss_scale: 65536.0000 (64951.2748)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.1422  max mem: 15572
Epoch: [3]  [1410/2809]  eta: 0:13:56  lr: 0.000033  min_lr: 0.000000  loss: 4.8054 (4.8120)  class_acc: 0.0833 (0.0666)  loss_scale: 65536.0000 (64955.4189)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1087  max mem: 15572
Epoch: [3]  [1420/2809]  eta: 0:13:49  lr: 0.000033  min_lr: 0.000000  loss: 4.8234 (4.8120)  class_acc: 0.0417 (0.0666)  loss_scale: 65536.0000 (64959.5046)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0449  max mem: 15572
Epoch: [3]  [1430/2809]  eta: 0:13:43  lr: 0.000033  min_lr: 0.000000  loss: 4.7971 (4.8121)  class_acc: 0.0417 (0.0667)  loss_scale: 65536.0000 (64963.5332)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0903  max mem: 15572
Epoch: [3]  [1440/2809]  eta: 0:13:37  lr: 0.000033  min_lr: 0.000000  loss: 4.7828 (4.8118)  class_acc: 0.0833 (0.0668)  loss_scale: 65536.0000 (64967.5059)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1027  max mem: 15572
Epoch: [3]  [1450/2809]  eta: 0:13:30  lr: 0.000033  min_lr: 0.000000  loss: 4.7278 (4.8116)  class_acc: 0.0417 (0.0669)  loss_scale: 65536.0000 (64971.4238)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0742  max mem: 15572
Epoch: [3]  [1460/2809]  eta: 0:13:23  lr: 0.000033  min_lr: 0.000000  loss: 4.7735 (4.8114)  class_acc: 0.0833 (0.0669)  loss_scale: 65536.0000 (64975.2882)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0402  max mem: 15572
[2025-01-15 16:04:53,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:04:53,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1470/2809]  eta: 0:13:17  lr: 0.000033  min_lr: 0.000000  loss: 4.7735 (4.8111)  class_acc: 0.0833 (0.0669)  loss_scale: 65536.0000 (65290.9640)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0621  max mem: 15572
[2025-01-15 16:05:02,702] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9906
[2025-01-15 16:05:02,702] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:05:02,702] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1480/2809]  eta: 0:13:11  lr: 0.000033  min_lr: 0.000000  loss: 4.7580 (4.8108)  class_acc: 0.0833 (0.0670)  loss_scale: 131072.0000 (65646.6280)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1384  max mem: 15572
Epoch: [3]  [1490/2809]  eta: 0:13:05  lr: 0.000033  min_lr: 0.000000  loss: 4.8098 (4.8110)  class_acc: 0.0417 (0.0670)  loss_scale: 65536.0000 (65645.8860)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1143  max mem: 15572
Epoch: [3]  [1500/2809]  eta: 0:12:59  lr: 0.000033  min_lr: 0.000000  loss: 4.7712 (4.8103)  class_acc: 0.0833 (0.0672)  loss_scale: 65536.0000 (65645.1539)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1363  max mem: 15572
Epoch: [3]  [1510/2809]  eta: 0:12:54  lr: 0.000033  min_lr: 0.000000  loss: 4.7040 (4.8101)  class_acc: 0.0833 (0.0674)  loss_scale: 65536.0000 (65644.4315)  weight_decay: 0.0500 (0.0500)  time: 0.6754  data: 0.2061  max mem: 15572
Epoch: [3]  [1520/2809]  eta: 0:12:47  lr: 0.000033  min_lr: 0.000000  loss: 4.7887 (4.8101)  class_acc: 0.0833 (0.0676)  loss_scale: 65536.0000 (65643.7186)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1291  max mem: 15572
Epoch: [3]  [1530/2809]  eta: 0:12:42  lr: 0.000033  min_lr: 0.000000  loss: 4.7649 (4.8093)  class_acc: 0.0833 (0.0678)  loss_scale: 65536.0000 (65643.0150)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.0847  max mem: 15572
Epoch: [3]  [1540/2809]  eta: 0:12:35  lr: 0.000033  min_lr: 0.000000  loss: 4.6432 (4.8088)  class_acc: 0.0833 (0.0678)  loss_scale: 65536.0000 (65642.3206)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1027  max mem: 15572
Epoch: [3]  [1550/2809]  eta: 0:12:29  lr: 0.000033  min_lr: 0.000000  loss: 4.8041 (4.8090)  class_acc: 0.0417 (0.0682)  loss_scale: 65536.0000 (65641.6351)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0795  max mem: 15572
Epoch: [3]  [1560/2809]  eta: 0:12:23  lr: 0.000033  min_lr: 0.000000  loss: 4.7780 (4.8086)  class_acc: 0.0833 (0.0683)  loss_scale: 65536.0000 (65640.9584)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0764  max mem: 15572
Epoch: [3]  [1570/2809]  eta: 0:12:17  lr: 0.000033  min_lr: 0.000000  loss: 4.7348 (4.8080)  class_acc: 0.0417 (0.0683)  loss_scale: 65536.0000 (65640.2903)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0972  max mem: 15572
[2025-01-15 16:05:57,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=56, lr=[3.233564650805923e-07, 3.233564650805923e-07, 4.6193780725798904e-07, 4.6193780725798904e-07, 6.599111532256987e-07, 6.599111532256987e-07, 9.427302188938554e-07, 9.427302188938554e-07, 1.3467574555626506e-06, 1.3467574555626506e-06, 1.923939222232358e-06, 1.923939222232358e-06, 2.748484603189083e-06, 2.748484603189083e-06, 3.926406575984405e-06, 3.926406575984405e-06, 5.609152251406292e-06, 5.609152251406292e-06, 8.013074644866134e-06, 8.013074644866134e-06, 1.1447249492665904e-05, 1.1447249492665904e-05, 1.6353213560951293e-05, 1.6353213560951293e-05, 2.336173365850185e-05, 2.336173365850185e-05, 3.3373905226431215e-05, 3.3373905226431215e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:05:57,424] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=27.56016130574133, CurrSamplesPerSec=21.966204662635807, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [1580/2809]  eta: 0:12:11  lr: 0.000033  min_lr: 0.000000  loss: 4.7643 (4.8077)  class_acc: 0.0417 (0.0682)  loss_scale: 65536.0000 (65639.6306)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1052  max mem: 15572
Epoch: [3]  [1590/2809]  eta: 0:12:04  lr: 0.000033  min_lr: 0.000000  loss: 4.7145 (4.8074)  class_acc: 0.0417 (0.0681)  loss_scale: 65536.0000 (65638.9793)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0833  max mem: 15572
Epoch: [3]  [1600/2809]  eta: 0:11:58  lr: 0.000033  min_lr: 0.000000  loss: 4.7520 (4.8075)  class_acc: 0.0417 (0.0680)  loss_scale: 65536.0000 (65638.3360)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1080  max mem: 15572
[2025-01-15 16:06:18,513] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:06:18,513] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:06:19,520] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10037
[2025-01-15 16:06:19,520] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:06:19,520] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1610/2809]  eta: 0:11:53  lr: 0.000034  min_lr: 0.000000  loss: 4.8020 (4.8073)  class_acc: 0.0417 (0.0680)  loss_scale: 65536.0000 (65719.0615)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1501  max mem: 15572
Epoch: [3]  [1620/2809]  eta: 0:11:47  lr: 0.000034  min_lr: 0.000000  loss: 4.7809 (4.8071)  class_acc: 0.0417 (0.0680)  loss_scale: 65536.0000 (65717.9321)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1526  max mem: 15572
Epoch: [3]  [1630/2809]  eta: 0:11:42  lr: 0.000034  min_lr: 0.000000  loss: 4.8215 (4.8070)  class_acc: 0.0417 (0.0679)  loss_scale: 65536.0000 (65716.8167)  weight_decay: 0.0500 (0.0500)  time: 0.6716  data: 0.2282  max mem: 15572
Epoch: [3]  [1640/2809]  eta: 0:11:36  lr: 0.000034  min_lr: 0.000000  loss: 4.8013 (4.8066)  class_acc: 0.0417 (0.0680)  loss_scale: 65536.0000 (65715.7148)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.1972  max mem: 15572
Epoch: [3]  [1650/2809]  eta: 0:11:30  lr: 0.000034  min_lr: 0.000000  loss: 4.7220 (4.8064)  class_acc: 0.0417 (0.0679)  loss_scale: 65536.0000 (65714.6263)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1272  max mem: 15572
Epoch: [3]  [1660/2809]  eta: 0:11:24  lr: 0.000034  min_lr: 0.000000  loss: 4.8139 (4.8073)  class_acc: 0.0417 (0.0680)  loss_scale: 65536.0000 (65713.5509)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1074  max mem: 15572
Epoch: [3]  [1670/2809]  eta: 0:11:18  lr: 0.000034  min_lr: 0.000000  loss: 4.7965 (4.8068)  class_acc: 0.0833 (0.0681)  loss_scale: 65536.0000 (65712.4883)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1108  max mem: 15572
Epoch: [3]  [1680/2809]  eta: 0:11:12  lr: 0.000034  min_lr: 0.000000  loss: 4.7965 (4.8067)  class_acc: 0.0417 (0.0679)  loss_scale: 65536.0000 (65711.4384)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1203  max mem: 15572
Epoch: [3]  [1690/2809]  eta: 0:11:06  lr: 0.000034  min_lr: 0.000000  loss: 4.8149 (4.8065)  class_acc: 0.0417 (0.0680)  loss_scale: 65536.0000 (65710.4009)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0994  max mem: 15572
Epoch: [3]  [1700/2809]  eta: 0:11:00  lr: 0.000034  min_lr: 0.000000  loss: 4.7744 (4.8067)  class_acc: 0.0833 (0.0681)  loss_scale: 65536.0000 (65709.3757)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1114  max mem: 15572
Epoch: [3]  [1710/2809]  eta: 0:10:54  lr: 0.000034  min_lr: 0.000000  loss: 4.7744 (4.8063)  class_acc: 0.0833 (0.0683)  loss_scale: 65536.0000 (65708.3624)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1156  max mem: 15572
Epoch: [3]  [1720/2809]  eta: 0:10:48  lr: 0.000034  min_lr: 0.000000  loss: 4.7681 (4.8064)  class_acc: 0.0417 (0.0682)  loss_scale: 65536.0000 (65707.3608)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1748  max mem: 15572
Epoch: [3]  [1730/2809]  eta: 0:10:42  lr: 0.000034  min_lr: 0.000000  loss: 4.8345 (4.8065)  class_acc: 0.0417 (0.0681)  loss_scale: 65536.0000 (65706.3709)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1632  max mem: 15572
[2025-01-15 16:07:36,807] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:07:36,807] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:07:37,224] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10167
[2025-01-15 16:07:37,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:07:37,225] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1740/2809]  eta: 0:10:36  lr: 0.000034  min_lr: 0.000000  loss: 4.7883 (4.8064)  class_acc: 0.1250 (0.0686)  loss_scale: 65536.0000 (65743.0350)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.1106  max mem: 15572
Epoch: [3]  [1750/2809]  eta: 0:10:30  lr: 0.000034  min_lr: 0.000000  loss: 4.7236 (4.8062)  class_acc: 0.1250 (0.0687)  loss_scale: 65536.0000 (65741.8527)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1229  max mem: 15572
Epoch: [3]  [1760/2809]  eta: 0:10:24  lr: 0.000034  min_lr: 0.000000  loss: 4.6814 (4.8057)  class_acc: 0.1250 (0.0691)  loss_scale: 65536.0000 (65740.6837)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1310  max mem: 15572
Epoch: [3]  [1770/2809]  eta: 0:10:18  lr: 0.000034  min_lr: 0.000000  loss: 4.7174 (4.8056)  class_acc: 0.0833 (0.0690)  loss_scale: 65536.0000 (65739.5280)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1739  max mem: 15572
Epoch: [3]  [1780/2809]  eta: 0:10:12  lr: 0.000034  min_lr: 0.000000  loss: 4.8094 (4.8058)  class_acc: 0.0417 (0.0689)  loss_scale: 65536.0000 (65738.3852)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.1717  max mem: 15572
Epoch: [3]  [1790/2809]  eta: 0:10:06  lr: 0.000034  min_lr: 0.000000  loss: 4.7783 (4.8057)  class_acc: 0.0417 (0.0689)  loss_scale: 65536.0000 (65737.2552)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1320  max mem: 15572
Epoch: [3]  [1800/2809]  eta: 0:10:01  lr: 0.000034  min_lr: 0.000000  loss: 4.7848 (4.8056)  class_acc: 0.0833 (0.0690)  loss_scale: 65536.0000 (65736.1377)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1569  max mem: 15572
Epoch: [3]  [1810/2809]  eta: 0:09:55  lr: 0.000034  min_lr: 0.000000  loss: 4.8302 (4.8057)  class_acc: 0.0833 (0.0690)  loss_scale: 65536.0000 (65735.0326)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1625  max mem: 15572
Epoch: [3]  [1820/2809]  eta: 0:09:49  lr: 0.000034  min_lr: 0.000000  loss: 4.7535 (4.8054)  class_acc: 0.0417 (0.0690)  loss_scale: 65536.0000 (65733.9396)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1285  max mem: 15572
Epoch: [3]  [1830/2809]  eta: 0:09:43  lr: 0.000034  min_lr: 0.000000  loss: 4.7515 (4.8053)  class_acc: 0.0417 (0.0690)  loss_scale: 65536.0000 (65732.8585)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1240  max mem: 15572
Epoch: [3]  [1840/2809]  eta: 0:09:37  lr: 0.000034  min_lr: 0.000000  loss: 4.8168 (4.8055)  class_acc: 0.0417 (0.0690)  loss_scale: 65536.0000 (65731.7892)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1464  max mem: 15572
Epoch: [3]  [1850/2809]  eta: 0:09:31  lr: 0.000034  min_lr: 0.000000  loss: 4.7450 (4.8050)  class_acc: 0.0417 (0.0691)  loss_scale: 65536.0000 (65730.7315)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1233  max mem: 15572
Epoch: [3]  [1860/2809]  eta: 0:09:25  lr: 0.000034  min_lr: 0.000000  loss: 4.6994 (4.8048)  class_acc: 0.0833 (0.0690)  loss_scale: 65536.0000 (65729.6851)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1553  max mem: 15572
[2025-01-15 16:08:54,225] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:08:54,226] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1870/2809]  eta: 0:09:18  lr: 0.000034  min_lr: 0.000000  loss: 4.7330 (4.8046)  class_acc: 0.0417 (0.0690)  loss_scale: 65536.0000 (65798.7044)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1041  max mem: 15572
[2025-01-15 16:08:55,289] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10298
[2025-01-15 16:08:55,289] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:08:55,290] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1880/2809]  eta: 0:09:13  lr: 0.000034  min_lr: 0.000000  loss: 4.7607 (4.8047)  class_acc: 0.0417 (0.0689)  loss_scale: 65536.0000 (65797.3078)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0934  max mem: 15572
Epoch: [3]  [1890/2809]  eta: 0:09:07  lr: 0.000034  min_lr: 0.000000  loss: 4.7435 (4.8040)  class_acc: 0.0833 (0.0691)  loss_scale: 65536.0000 (65795.9260)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1594  max mem: 15572
Epoch: [3]  [1900/2809]  eta: 0:09:01  lr: 0.000034  min_lr: 0.000000  loss: 4.7468 (4.8041)  class_acc: 0.0833 (0.0693)  loss_scale: 65536.0000 (65794.5587)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1422  max mem: 15572
Epoch: [3]  [1910/2809]  eta: 0:08:55  lr: 0.000035  min_lr: 0.000000  loss: 4.7996 (4.8037)  class_acc: 0.0833 (0.0695)  loss_scale: 65536.0000 (65793.2057)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1200  max mem: 15572
Epoch: [3]  [1920/2809]  eta: 0:08:49  lr: 0.000035  min_lr: 0.000000  loss: 4.7937 (4.8039)  class_acc: 0.0833 (0.0694)  loss_scale: 65536.0000 (65791.8667)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1270  max mem: 15572
Epoch: [3]  [1930/2809]  eta: 0:08:44  lr: 0.000035  min_lr: 0.000000  loss: 4.8155 (4.8041)  class_acc: 0.0833 (0.0695)  loss_scale: 65536.0000 (65790.5417)  weight_decay: 0.0500 (0.0500)  time: 0.6573  data: 0.1842  max mem: 15572
Epoch: [3]  [1940/2809]  eta: 0:08:38  lr: 0.000035  min_lr: 0.000000  loss: 4.8117 (4.8044)  class_acc: 0.0417 (0.0694)  loss_scale: 65536.0000 (65789.2303)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1397  max mem: 15572
Epoch: [3]  [1950/2809]  eta: 0:08:31  lr: 0.000035  min_lr: 0.000000  loss: 4.8303 (4.8047)  class_acc: 0.0833 (0.0694)  loss_scale: 65536.0000 (65787.9323)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.0677  max mem: 15572
Epoch: [3]  [1960/2809]  eta: 0:08:26  lr: 0.000035  min_lr: 0.000000  loss: 4.8303 (4.8047)  class_acc: 0.0833 (0.0693)  loss_scale: 65536.0000 (65786.6476)  weight_decay: 0.0500 (0.0500)  time: 0.6540  data: 0.1762  max mem: 15572
Epoch: [3]  [1970/2809]  eta: 0:08:20  lr: 0.000035  min_lr: 0.000000  loss: 4.7935 (4.8047)  class_acc: 0.0417 (0.0692)  loss_scale: 65536.0000 (65785.3760)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1566  max mem: 15572
Epoch: [3]  [1980/2809]  eta: 0:08:14  lr: 0.000035  min_lr: 0.000000  loss: 4.7052 (4.8044)  class_acc: 0.0417 (0.0693)  loss_scale: 65536.0000 (65784.1171)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0955  max mem: 15572
Epoch: [3]  [1990/2809]  eta: 0:08:08  lr: 0.000035  min_lr: 0.000000  loss: 4.7894 (4.8043)  class_acc: 0.0833 (0.0693)  loss_scale: 65536.0000 (65782.8709)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1246  max mem: 15572
[2025-01-15 16:10:13,670] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:10:13,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2000/2809]  eta: 0:08:02  lr: 0.000035  min_lr: 0.000000  loss: 4.7815 (4.8038)  class_acc: 0.0417 (0.0693)  loss_scale: 65536.0000 (65814.3888)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1024  max mem: 15572
Epoch: [3]  [2010/2809]  eta: 0:07:56  lr: 0.000035  min_lr: 0.000000  loss: 4.6661 (4.8033)  class_acc: 0.0417 (0.0694)  loss_scale: 131072.0000 (66138.8921)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1591  max mem: 15572
[2025-01-15 16:10:20,492] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10438
[2025-01-15 16:10:20,492] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:10:20,493] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2020/2809]  eta: 0:07:49  lr: 0.000035  min_lr: 0.000000  loss: 4.6497 (4.8031)  class_acc: 0.0833 (0.0695)  loss_scale: 65536.0000 (66135.9090)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1082  max mem: 15572
Epoch: [3]  [2030/2809]  eta: 0:07:43  lr: 0.000035  min_lr: 0.000000  loss: 4.6446 (4.8025)  class_acc: 0.0833 (0.0695)  loss_scale: 65536.0000 (66132.9552)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0573  max mem: 15572
Epoch: [3]  [2040/2809]  eta: 0:07:37  lr: 0.000035  min_lr: 0.000000  loss: 4.6755 (4.8022)  class_acc: 0.0833 (0.0696)  loss_scale: 65536.0000 (66130.0304)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0782  max mem: 15572
Epoch: [3]  [2050/2809]  eta: 0:07:32  lr: 0.000035  min_lr: 0.000000  loss: 4.6973 (4.8019)  class_acc: 0.0833 (0.0695)  loss_scale: 65536.0000 (66127.1341)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1739  max mem: 15572
Epoch: [3]  [2060/2809]  eta: 0:07:26  lr: 0.000035  min_lr: 0.000000  loss: 4.6924 (4.8015)  class_acc: 0.0417 (0.0695)  loss_scale: 65536.0000 (66124.2659)  weight_decay: 0.0500 (0.0500)  time: 0.6557  data: 0.1988  max mem: 15572
Epoch: [3]  [2070/2809]  eta: 0:07:20  lr: 0.000035  min_lr: 0.000000  loss: 4.6864 (4.8011)  class_acc: 0.0417 (0.0696)  loss_scale: 65536.0000 (66121.4254)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1400  max mem: 15572
Epoch: [3]  [2080/2809]  eta: 0:07:14  lr: 0.000035  min_lr: 0.000000  loss: 4.7226 (4.8006)  class_acc: 0.0417 (0.0695)  loss_scale: 65536.0000 (66118.6122)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1319  max mem: 15572
Epoch: [3]  [2090/2809]  eta: 0:07:08  lr: 0.000035  min_lr: 0.000000  loss: 4.7226 (4.8002)  class_acc: 0.0417 (0.0696)  loss_scale: 65536.0000 (66115.8259)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1299  max mem: 15572
Epoch: [3]  [2100/2809]  eta: 0:07:02  lr: 0.000035  min_lr: 0.000000  loss: 4.7362 (4.8002)  class_acc: 0.0833 (0.0697)  loss_scale: 65536.0000 (66113.0662)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1445  max mem: 15572
Epoch: [3]  [2110/2809]  eta: 0:06:56  lr: 0.000035  min_lr: 0.000000  loss: 4.7638 (4.7999)  class_acc: 0.0833 (0.0699)  loss_scale: 65536.0000 (66110.3325)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1506  max mem: 15572
Epoch: [3]  [2120/2809]  eta: 0:06:50  lr: 0.000035  min_lr: 0.000000  loss: 4.6953 (4.7996)  class_acc: 0.0833 (0.0698)  loss_scale: 65536.0000 (66107.6247)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1218  max mem: 15572
Epoch: [3]  [2130/2809]  eta: 0:06:44  lr: 0.000035  min_lr: 0.000000  loss: 4.6917 (4.7992)  class_acc: 0.0833 (0.0699)  loss_scale: 65536.0000 (66104.9423)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0670  max mem: 15572
[2025-01-15 16:11:35,520] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:11:35,521] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2140/2809]  eta: 0:06:38  lr: 0.000035  min_lr: 0.000000  loss: 4.6917 (4.7986)  class_acc: 0.1250 (0.0703)  loss_scale: 65536.0000 (66132.8949)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0528  max mem: 15572
[2025-01-15 16:11:35,990] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10568
[2025-01-15 16:11:35,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:11:35,991] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2150/2809]  eta: 0:06:32  lr: 0.000035  min_lr: 0.000000  loss: 4.7664 (4.7985)  class_acc: 0.0833 (0.0703)  loss_scale: 65536.0000 (66130.1199)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1497  max mem: 15572
Epoch: [3]  [2160/2809]  eta: 0:06:26  lr: 0.000035  min_lr: 0.000000  loss: 4.8356 (4.7990)  class_acc: 0.0417 (0.0702)  loss_scale: 65536.0000 (66127.3707)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.1849  max mem: 15572
Epoch: [3]  [2170/2809]  eta: 0:06:20  lr: 0.000035  min_lr: 0.000000  loss: 4.6924 (4.7983)  class_acc: 0.0833 (0.0703)  loss_scale: 65536.0000 (66124.6467)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0724  max mem: 15572
Epoch: [3]  [2180/2809]  eta: 0:06:14  lr: 0.000035  min_lr: 0.000000  loss: 4.6868 (4.7981)  class_acc: 0.0833 (0.0702)  loss_scale: 65536.0000 (66121.9477)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1116  max mem: 15572
Epoch: [3]  [2190/2809]  eta: 0:06:08  lr: 0.000035  min_lr: 0.000000  loss: 4.7119 (4.7980)  class_acc: 0.0417 (0.0703)  loss_scale: 65536.0000 (66119.2734)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1501  max mem: 15572
Epoch: [3]  [2200/2809]  eta: 0:06:02  lr: 0.000035  min_lr: 0.000000  loss: 4.7087 (4.7978)  class_acc: 0.0833 (0.0703)  loss_scale: 65536.0000 (66116.6234)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1422  max mem: 15572
Epoch: [3]  [2210/2809]  eta: 0:05:56  lr: 0.000036  min_lr: 0.000000  loss: 4.7077 (4.7977)  class_acc: 0.0417 (0.0702)  loss_scale: 65536.0000 (66113.9973)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1636  max mem: 15572
Epoch: [3]  [2220/2809]  eta: 0:05:50  lr: 0.000036  min_lr: 0.000000  loss: 4.7517 (4.7973)  class_acc: 0.0417 (0.0703)  loss_scale: 65536.0000 (66111.3949)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1057  max mem: 15572
Epoch: [3]  [2230/2809]  eta: 0:05:44  lr: 0.000036  min_lr: 0.000000  loss: 4.7343 (4.7966)  class_acc: 0.0833 (0.0704)  loss_scale: 65536.0000 (66108.8158)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1005  max mem: 15572
Epoch: [3]  [2240/2809]  eta: 0:05:38  lr: 0.000036  min_lr: 0.000000  loss: 4.7018 (4.7968)  class_acc: 0.0417 (0.0704)  loss_scale: 65536.0000 (66106.2597)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1326  max mem: 15572
Epoch: [3]  [2250/2809]  eta: 0:05:32  lr: 0.000036  min_lr: 0.000000  loss: 4.8031 (4.7968)  class_acc: 0.0417 (0.0705)  loss_scale: 65536.0000 (66103.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1261  max mem: 15572
Epoch: [3]  [2260/2809]  eta: 0:05:27  lr: 0.000036  min_lr: 0.000000  loss: 4.7854 (4.7968)  class_acc: 0.0833 (0.0705)  loss_scale: 65536.0000 (66101.2154)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1600  max mem: 15572
[2025-01-15 16:12:54,090] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:12:54,091] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2270/2809]  eta: 0:05:21  lr: 0.000036  min_lr: 0.000000  loss: 4.8131 (4.7969)  class_acc: 0.0417 (0.0704)  loss_scale: 65536.0000 (66127.5843)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1226  max mem: 15572
[2025-01-15 16:12:57,149] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10703
[2025-01-15 16:12:57,150] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:12:57,151] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2280/2809]  eta: 0:05:14  lr: 0.000036  min_lr: 0.000000  loss: 4.8505 (4.7968)  class_acc: 0.0417 (0.0703)  loss_scale: 65536.0000 (66268.6471)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0397  max mem: 15572
Epoch: [3]  [2290/2809]  eta: 0:05:08  lr: 0.000036  min_lr: 0.000000  loss: 4.7445 (4.7967)  class_acc: 0.0417 (0.0703)  loss_scale: 65536.0000 (66265.4491)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0606  max mem: 15572
Epoch: [3]  [2300/2809]  eta: 0:05:03  lr: 0.000036  min_lr: 0.000000  loss: 4.7183 (4.7964)  class_acc: 0.0417 (0.0704)  loss_scale: 65536.0000 (66262.2790)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1374  max mem: 15572
Epoch: [3]  [2310/2809]  eta: 0:04:57  lr: 0.000036  min_lr: 0.000000  loss: 4.7113 (4.7959)  class_acc: 0.0417 (0.0704)  loss_scale: 65536.0000 (66259.1363)  weight_decay: 0.0500 (0.0500)  time: 0.6496  data: 0.1951  max mem: 15572
Epoch: [3]  [2320/2809]  eta: 0:04:51  lr: 0.000036  min_lr: 0.000000  loss: 4.7243 (4.7956)  class_acc: 0.0417 (0.0705)  loss_scale: 65536.0000 (66256.0207)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1852  max mem: 15572
Epoch: [3]  [2330/2809]  eta: 0:04:45  lr: 0.000036  min_lr: 0.000000  loss: 4.7243 (4.7955)  class_acc: 0.0833 (0.0705)  loss_scale: 65536.0000 (66252.9318)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1411  max mem: 15572
Epoch: [3]  [2340/2809]  eta: 0:04:39  lr: 0.000036  min_lr: 0.000000  loss: 4.7182 (4.7953)  class_acc: 0.0417 (0.0705)  loss_scale: 65536.0000 (66249.8693)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1087  max mem: 15572
Epoch: [3]  [2350/2809]  eta: 0:04:33  lr: 0.000036  min_lr: 0.000000  loss: 4.7088 (4.7952)  class_acc: 0.0417 (0.0705)  loss_scale: 65536.0000 (66246.8328)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0959  max mem: 15572
Epoch: [3]  [2360/2809]  eta: 0:04:26  lr: 0.000036  min_lr: 0.000000  loss: 4.7572 (4.7952)  class_acc: 0.0833 (0.0706)  loss_scale: 65536.0000 (66243.8221)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0594  max mem: 15572
Epoch: [3]  [2370/2809]  eta: 0:04:21  lr: 0.000036  min_lr: 0.000000  loss: 4.7572 (4.7951)  class_acc: 0.0833 (0.0707)  loss_scale: 65536.0000 (66240.8368)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0970  max mem: 15572
Epoch: [3]  [2380/2809]  eta: 0:04:15  lr: 0.000036  min_lr: 0.000000  loss: 4.7755 (4.7952)  class_acc: 0.0417 (0.0708)  loss_scale: 65536.0000 (66237.8765)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1286  max mem: 15572
Epoch: [3]  [2390/2809]  eta: 0:04:09  lr: 0.000036  min_lr: 0.000000  loss: 4.8015 (4.7953)  class_acc: 0.0417 (0.0708)  loss_scale: 65536.0000 (66234.9410)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1553  max mem: 15572
Epoch: [3]  [2400/2809]  eta: 0:04:03  lr: 0.000036  min_lr: 0.000000  loss: 4.7520 (4.7949)  class_acc: 0.0417 (0.0709)  loss_scale: 65536.0000 (66232.0300)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1126  max mem: 15572
[2025-01-15 16:14:11,203] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:14:11,204] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2410/2809]  eta: 0:03:57  lr: 0.000036  min_lr: 0.000000  loss: 4.7041 (4.7948)  class_acc: 0.0833 (0.0710)  loss_scale: 65536.0000 (66392.2356)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0537  max mem: 15572
[2025-01-15 16:14:16,808] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10842
[2025-01-15 16:14:16,808] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:14:16,809] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2420/2809]  eta: 0:03:51  lr: 0.000036  min_lr: 0.000000  loss: 4.7060 (4.7946)  class_acc: 0.1250 (0.0713)  loss_scale: 65536.0000 (66496.9781)  weight_decay: 0.0500 (0.0500)  time: 0.6839  data: 0.2158  max mem: 15572
Epoch: [3]  [2430/2809]  eta: 0:03:45  lr: 0.000036  min_lr: 0.000000  loss: 4.7738 (4.7945)  class_acc: 0.0833 (0.0713)  loss_scale: 65536.0000 (66493.0251)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.1631  max mem: 15572
Epoch: [3]  [2440/2809]  eta: 0:03:39  lr: 0.000036  min_lr: 0.000000  loss: 4.7745 (4.7946)  class_acc: 0.0417 (0.0712)  loss_scale: 65536.0000 (66489.1045)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0107  max mem: 15572
Epoch: [3]  [2450/2809]  eta: 0:03:33  lr: 0.000036  min_lr: 0.000000  loss: 4.7460 (4.7943)  class_acc: 0.0417 (0.0712)  loss_scale: 65536.0000 (66485.2158)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1043  max mem: 15572
Epoch: [3]  [2460/2809]  eta: 0:03:27  lr: 0.000036  min_lr: 0.000000  loss: 4.7391 (4.7943)  class_acc: 0.0833 (0.0713)  loss_scale: 65536.0000 (66481.3588)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.1923  max mem: 15572
Epoch: [3]  [2470/2809]  eta: 0:03:21  lr: 0.000036  min_lr: 0.000000  loss: 4.7341 (4.7939)  class_acc: 0.0833 (0.0715)  loss_scale: 65536.0000 (66477.5330)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1335  max mem: 15572
Epoch: [3]  [2480/2809]  eta: 0:03:15  lr: 0.000036  min_lr: 0.000000  loss: 4.6938 (4.7937)  class_acc: 0.0833 (0.0715)  loss_scale: 65536.0000 (66473.7380)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0980  max mem: 15572
Epoch: [3]  [2490/2809]  eta: 0:03:09  lr: 0.000036  min_lr: 0.000000  loss: 4.7832 (4.7939)  class_acc: 0.0417 (0.0715)  loss_scale: 65536.0000 (66469.9735)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1443  max mem: 15572
Epoch: [3]  [2500/2809]  eta: 0:03:03  lr: 0.000036  min_lr: 0.000000  loss: 4.8482 (4.7939)  class_acc: 0.0833 (0.0716)  loss_scale: 65536.0000 (66466.2391)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.2076  max mem: 15572
Epoch: [3]  [2510/2809]  eta: 0:02:57  lr: 0.000037  min_lr: 0.000000  loss: 4.8314 (4.7937)  class_acc: 0.0833 (0.0716)  loss_scale: 65536.0000 (66462.5344)  weight_decay: 0.0500 (0.0500)  time: 0.6468  data: 0.1967  max mem: 15572
Epoch: [3]  [2520/2809]  eta: 0:02:51  lr: 0.000037  min_lr: 0.000000  loss: 4.7367 (4.7937)  class_acc: 0.0417 (0.0715)  loss_scale: 65536.0000 (66458.8592)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0986  max mem: 15572
Epoch: [3]  [2530/2809]  eta: 0:02:45  lr: 0.000037  min_lr: 0.000000  loss: 4.7504 (4.7936)  class_acc: 0.0417 (0.0715)  loss_scale: 65536.0000 (66455.2130)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1139  max mem: 15572
Epoch: [3]  [2540/2809]  eta: 0:02:39  lr: 0.000037  min_lr: 0.000000  loss: 4.8023 (4.7937)  class_acc: 0.0417 (0.0716)  loss_scale: 65536.0000 (66451.5954)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.1959  max mem: 15572
[2025-01-15 16:15:34,169] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:15:34,170] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:15:37,615] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10977
[2025-01-15 16:15:37,616] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:15:37,616] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2550/2809]  eta: 0:02:33  lr: 0.000037  min_lr: 0.000000  loss: 4.7719 (4.7930)  class_acc: 0.0833 (0.0716)  loss_scale: 65536.0000 (66602.1482)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1446  max mem: 15572
Epoch: [3]  [2560/2809]  eta: 0:02:28  lr: 0.000037  min_lr: 0.000000  loss: 4.6493 (4.7925)  class_acc: 0.0833 (0.0718)  loss_scale: 65536.0000 (66597.9852)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0836  max mem: 15572
Epoch: [3]  [2570/2809]  eta: 0:02:22  lr: 0.000037  min_lr: 0.000000  loss: 4.6378 (4.7923)  class_acc: 0.0833 (0.0720)  loss_scale: 65536.0000 (66593.8545)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1175  max mem: 15572
[2025-01-15 16:15:50,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=64, lr=[3.556953454766912e-07, 3.556953454766912e-07, 5.081362078238446e-07, 5.081362078238446e-07, 7.259088683197781e-07, 7.259088683197781e-07, 1.0370126690282546e-06, 1.0370126690282546e-06, 1.4814466700403637e-06, 1.4814466700403637e-06, 2.116352385771948e-06, 2.116352385771948e-06, 3.023360551102783e-06, 3.023360551102783e-06, 4.319086501575405e-06, 4.319086501575405e-06, 6.17012357367915e-06, 6.17012357367915e-06, 8.814462248113072e-06, 8.814462248113072e-06, 1.2592088925875817e-05, 1.2592088925875817e-05, 1.7988698465536884e-05, 1.7988698465536884e-05, 2.5698140665052692e-05, 2.5698140665052692e-05, 3.671162952150385e-05, 3.671162952150385e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:15:50,875] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=27.547094321272898, CurrSamplesPerSec=30.82643624869315, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [2580/2809]  eta: 0:02:16  lr: 0.000037  min_lr: 0.000000  loss: 4.6721 (4.7919)  class_acc: 0.1250 (0.0722)  loss_scale: 65536.0000 (66589.7559)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1237  max mem: 15572
Epoch: [3]  [2590/2809]  eta: 0:02:10  lr: 0.000037  min_lr: 0.000000  loss: 4.7622 (4.7919)  class_acc: 0.0833 (0.0722)  loss_scale: 65536.0000 (66585.6889)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.0959  max mem: 15572
Epoch: [3]  [2600/2809]  eta: 0:02:04  lr: 0.000037  min_lr: 0.000000  loss: 4.8086 (4.7919)  class_acc: 0.0833 (0.0723)  loss_scale: 65536.0000 (66581.6532)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0595  max mem: 15572
Epoch: [3]  [2610/2809]  eta: 0:01:58  lr: 0.000037  min_lr: 0.000000  loss: 4.7135 (4.7915)  class_acc: 0.1250 (0.0724)  loss_scale: 65536.0000 (66577.6484)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0694  max mem: 15572
Epoch: [3]  [2620/2809]  eta: 0:01:52  lr: 0.000037  min_lr: 0.000000  loss: 4.7136 (4.7914)  class_acc: 0.0833 (0.0724)  loss_scale: 65536.0000 (66573.6742)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.1000  max mem: 15572
Epoch: [3]  [2630/2809]  eta: 0:01:46  lr: 0.000037  min_lr: 0.000000  loss: 4.7289 (4.7911)  class_acc: 0.0417 (0.0724)  loss_scale: 65536.0000 (66569.7301)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.1735  max mem: 15572
Epoch: [3]  [2640/2809]  eta: 0:01:40  lr: 0.000037  min_lr: 0.000000  loss: 4.7621 (4.7911)  class_acc: 0.0417 (0.0725)  loss_scale: 65536.0000 (66565.8160)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.1845  max mem: 15572
Epoch: [3]  [2650/2809]  eta: 0:01:34  lr: 0.000037  min_lr: 0.000000  loss: 4.7621 (4.7908)  class_acc: 0.0417 (0.0725)  loss_scale: 65536.0000 (66561.9313)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1151  max mem: 15572
Epoch: [3]  [2660/2809]  eta: 0:01:28  lr: 0.000037  min_lr: 0.000000  loss: 4.6630 (4.7907)  class_acc: 0.0417 (0.0725)  loss_scale: 65536.0000 (66558.0759)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1660  max mem: 15572
Epoch: [3]  [2670/2809]  eta: 0:01:22  lr: 0.000037  min_lr: 0.000000  loss: 4.7666 (4.7906)  class_acc: 0.0417 (0.0725)  loss_scale: 65536.0000 (66554.2493)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1694  max mem: 15572
[2025-01-15 16:16:53,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:16:53,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:16:54,055] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11107
[2025-01-15 16:16:54,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:16:54,056] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2680/2809]  eta: 0:01:16  lr: 0.000037  min_lr: 0.000000  loss: 4.7744 (4.7905)  class_acc: 0.0833 (0.0726)  loss_scale: 65536.0000 (66574.8959)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1066  max mem: 15572
Epoch: [3]  [2690/2809]  eta: 0:01:10  lr: 0.000037  min_lr: 0.000000  loss: 4.7718 (4.7908)  class_acc: 0.0833 (0.0726)  loss_scale: 65536.0000 (66571.0353)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1531  max mem: 15572
Epoch: [3]  [2700/2809]  eta: 0:01:04  lr: 0.000037  min_lr: 0.000000  loss: 4.7718 (4.7909)  class_acc: 0.0417 (0.0726)  loss_scale: 65536.0000 (66567.2033)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.1914  max mem: 15572
Epoch: [3]  [2710/2809]  eta: 0:00:58  lr: 0.000037  min_lr: 0.000000  loss: 4.7951 (4.7909)  class_acc: 0.0417 (0.0726)  loss_scale: 65536.0000 (66563.3995)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1524  max mem: 15572
Epoch: [3]  [2720/2809]  eta: 0:00:52  lr: 0.000037  min_lr: 0.000000  loss: 4.8366 (4.7911)  class_acc: 0.0417 (0.0725)  loss_scale: 65536.0000 (66559.6237)  weight_decay: 0.0500 (0.0500)  time: 0.6679  data: 0.1700  max mem: 15572
Epoch: [3]  [2730/2809]  eta: 0:00:46  lr: 0.000037  min_lr: 0.000000  loss: 4.8241 (4.7910)  class_acc: 0.0417 (0.0726)  loss_scale: 65536.0000 (66555.8755)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1043  max mem: 15572
Epoch: [3]  [2740/2809]  eta: 0:00:40  lr: 0.000037  min_lr: 0.000000  loss: 4.7768 (4.7907)  class_acc: 0.0833 (0.0727)  loss_scale: 65536.0000 (66552.1547)  weight_decay: 0.0500 (0.0500)  time: 0.4335  data: 0.0006  max mem: 15572
Epoch: [3]  [2750/2809]  eta: 0:00:35  lr: 0.000037  min_lr: 0.000000  loss: 4.7089 (4.7902)  class_acc: 0.0833 (0.0727)  loss_scale: 65536.0000 (66548.4609)  weight_decay: 0.0500 (0.0500)  time: 0.4601  data: 0.0008  max mem: 15572
Epoch: [3]  [2760/2809]  eta: 0:00:29  lr: 0.000037  min_lr: 0.000000  loss: 4.6487 (4.7897)  class_acc: 0.0833 (0.0729)  loss_scale: 65536.0000 (66544.7939)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0296  max mem: 15572
Epoch: [3]  [2770/2809]  eta: 0:00:23  lr: 0.000037  min_lr: 0.000000  loss: 4.6487 (4.7893)  class_acc: 0.0833 (0.0730)  loss_scale: 65536.0000 (66541.1534)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1354  max mem: 15572
Epoch: [3]  [2780/2809]  eta: 0:00:17  lr: 0.000037  min_lr: 0.000000  loss: 4.7086 (4.7892)  class_acc: 0.0833 (0.0731)  loss_scale: 65536.0000 (66537.5390)  weight_decay: 0.0500 (0.0500)  time: 0.7105  data: 0.2333  max mem: 15572
Epoch: [3]  [2790/2809]  eta: 0:00:11  lr: 0.000037  min_lr: 0.000000  loss: 4.7462 (4.7890)  class_acc: 0.0417 (0.0730)  loss_scale: 65536.0000 (66533.9506)  weight_decay: 0.0500 (0.0500)  time: 0.7421  data: 0.2554  max mem: 15572
Epoch: [3]  [2800/2809]  eta: 0:00:05  lr: 0.000037  min_lr: 0.000000  loss: 4.7422 (4.7889)  class_acc: 0.0417 (0.0730)  loss_scale: 65536.0000 (66530.3877)  weight_decay: 0.0500 (0.0500)  time: 0.6908  data: 0.2030  max mem: 15572
Epoch: [3]  [2808/2809]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000000  loss: 4.7422 (4.7888)  class_acc: 0.0417 (0.0730)  loss_scale: 65536.0000 (66527.5557)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1011  max mem: 15572
Epoch: [3] Total time: 0:27:51 (0.5949 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000000  loss: 4.7422 (4.7888)  class_acc: 0.0417 (0.0730)  loss_scale: 65536.0000 (66527.5557)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:25:56  loss: 3.1465 (3.1465)  acc1: 72.2222 (72.2222)  acc5: 94.4444 (94.4444)  time: 5.7224  data: 5.4686  max mem: 15572
Val:  [ 10/272]  eta: 0:04:00  loss: 4.8053 (4.5397)  acc1: 0.0000 (16.1616)  acc5: 0.0000 (24.7475)  time: 0.9170  data: 0.6977  max mem: 15572
Val:  [ 20/272]  eta: 0:02:42  loss: 4.4928 (4.4619)  acc1: 0.0000 (12.1693)  acc5: 5.5556 (22.4868)  time: 0.3922  data: 0.1693  max mem: 15572
Val:  [ 30/272]  eta: 0:02:23  loss: 4.4562 (4.4226)  acc1: 0.0000 (12.3656)  acc5: 11.1111 (24.9104)  time: 0.4177  data: 0.2075  max mem: 15572
Val:  [ 40/272]  eta: 0:02:05  loss: 3.9085 (4.2826)  acc1: 0.0000 (9.4851)  acc5: 38.8889 (32.1138)  time: 0.4314  data: 0.2340  max mem: 15572
Val:  [ 50/272]  eta: 0:01:53  loss: 3.8021 (4.2614)  acc1: 0.0000 (11.0022)  acc5: 55.5556 (34.3137)  time: 0.3811  data: 0.1796  max mem: 15572
Val:  [ 60/272]  eta: 0:01:41  loss: 3.8021 (4.1798)  acc1: 11.1111 (13.2969)  acc5: 61.1111 (38.7067)  time: 0.3552  data: 0.1579  max mem: 15572
Val:  [ 70/272]  eta: 0:01:35  loss: 3.8047 (4.1260)  acc1: 16.6667 (14.0845)  acc5: 61.1111 (38.7324)  time: 0.3684  data: 0.1734  max mem: 15572
Val:  [ 80/272]  eta: 0:01:25  loss: 3.8630 (4.1626)  acc1: 5.5556 (13.1001)  acc5: 27.7778 (37.7915)  time: 0.3362  data: 0.1280  max mem: 15572
Val:  [ 90/272]  eta: 0:01:18  loss: 4.6606 (4.2251)  acc1: 0.0000 (11.6606)  acc5: 0.0000 (33.6386)  time: 0.2856  data: 0.0748  max mem: 15572
Val:  [100/272]  eta: 0:01:11  loss: 4.6523 (4.2655)  acc1: 0.0000 (10.8911)  acc5: 0.0000 (31.5732)  time: 0.3058  data: 0.1179  max mem: 15572
Val:  [110/272]  eta: 0:01:04  loss: 4.5731 (4.2979)  acc1: 0.0000 (10.0100)  acc5: 0.0000 (30.3804)  time: 0.2372  data: 0.0617  max mem: 15572
Val:  [120/272]  eta: 0:00:57  loss: 4.5731 (4.3244)  acc1: 0.0000 (9.4582)  acc5: 5.5556 (29.0174)  time: 0.1800  data: 0.0005  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 4.4952 (4.2783)  acc1: 5.5556 (10.5174)  acc5: 27.7778 (31.6370)  time: 0.1921  data: 0.0006  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 4.0804 (4.2658)  acc1: 5.5556 (11.3869)  acc5: 38.8889 (32.1119)  time: 0.2124  data: 0.0010  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 4.2526 (4.2747)  acc1: 0.0000 (10.7800)  acc5: 16.6667 (30.9051)  time: 0.2773  data: 0.0655  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 4.2526 (4.2768)  acc1: 0.0000 (10.8351)  acc5: 22.2222 (31.5735)  time: 0.3302  data: 0.1271  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.3689 (4.3003)  acc1: 0.0000 (10.4938)  acc5: 33.3333 (31.4815)  time: 0.3482  data: 0.1511  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 4.3689 (4.2958)  acc1: 0.0000 (10.4665)  acc5: 22.2222 (30.9392)  time: 0.3303  data: 0.1291  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.4466 (4.3070)  acc1: 0.0000 (10.0349)  acc5: 11.1111 (30.0465)  time: 0.2948  data: 0.0848  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 4.3867 (4.3066)  acc1: 0.0000 (10.3096)  acc5: 11.1111 (30.8734)  time: 0.2866  data: 0.0786  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 4.4797 (4.3226)  acc1: 0.0000 (9.8736)  acc5: 11.1111 (29.8052)  time: 0.2730  data: 0.0749  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.5247 (4.3256)  acc1: 0.0000 (9.7285)  acc5: 11.1111 (29.4620)  time: 0.3351  data: 0.1484  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.3185 (4.3257)  acc1: 5.5556 (9.9327)  acc5: 22.2222 (29.8220)  time: 0.3494  data: 0.1465  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 4.1413 (4.3162)  acc1: 11.1111 (10.0046)  acc5: 50.0000 (30.9129)  time: 0.2977  data: 0.0901  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 4.3132 (4.3353)  acc1: 0.0000 (9.8274)  acc5: 22.2222 (30.3010)  time: 0.3116  data: 0.1143  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.1086 (4.2985)  acc1: 11.1111 (11.0898)  acc5: 38.8889 (31.9285)  time: 0.2874  data: 0.0871  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 4.0168 (4.2943)  acc1: 16.6667 (11.2136)  acc5: 55.5556 (32.4518)  time: 0.2186  data: 0.0359  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 4.0168 (4.2961)  acc1: 16.6667 (11.2021)  acc5: 55.5556 (32.4391)  time: 0.2113  data: 0.0359  max mem: 15572
Val: Total time: 0:01:29 (0.3292 s / it)
* Acc@1 11.202 Acc@5 32.439 loss 4.296
Accuracy of the network on the 4883 val videos: 11.2%
[2025-01-15 16:19:41,078] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 16:19:41,080] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 16:19:41,080] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 16:19:43,922] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 16:19:43,922] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 11.20%
[2025-01-15 16:19:53,272] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:19:53,274] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [   0/2809]  eta: 7:18:50  lr: 0.000038  min_lr: 0.000000  loss: 4.4477 (4.4477)  class_acc: 0.1250 (0.1250)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 9.3735  data: 8.8835  max mem: 15572
[2025-01-15 16:19:58,454] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11246
[2025-01-15 16:19:58,455] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:19:58,455] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [  10/2809]  eta: 1:01:36  lr: 0.000038  min_lr: 0.000000  loss: 4.6331 (4.6086)  class_acc: 0.0417 (0.0568)  loss_scale: 131072.0000 (125114.1818)  weight_decay: 0.0500 (0.0500)  time: 1.3208  data: 0.8458  max mem: 15572
Epoch: [4]  [  20/2809]  eta: 0:44:08  lr: 0.000038  min_lr: 0.000000  loss: 4.6611 (4.6646)  class_acc: 0.0417 (0.0556)  loss_scale: 65536.0000 (96743.6190)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0609  max mem: 15572
Epoch: [4]  [  30/2809]  eta: 0:38:31  lr: 0.000038  min_lr: 0.000000  loss: 4.7432 (4.7092)  class_acc: 0.0833 (0.0685)  loss_scale: 65536.0000 (86676.6452)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1079  max mem: 15572
Epoch: [4]  [  40/2809]  eta: 0:35:21  lr: 0.000038  min_lr: 0.000000  loss: 4.7656 (4.7108)  class_acc: 0.0833 (0.0681)  loss_scale: 65536.0000 (81520.3902)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1153  max mem: 15572
Epoch: [4]  [  50/2809]  eta: 0:33:38  lr: 0.000038  min_lr: 0.000000  loss: 4.7656 (4.7186)  class_acc: 0.0417 (0.0637)  loss_scale: 65536.0000 (78386.1961)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1299  max mem: 15572
Epoch: [4]  [  60/2809]  eta: 0:32:11  lr: 0.000038  min_lr: 0.000000  loss: 4.7786 (4.7351)  class_acc: 0.0417 (0.0683)  loss_scale: 65536.0000 (76279.6066)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1413  max mem: 15572
Epoch: [4]  [  70/2809]  eta: 0:31:22  lr: 0.000038  min_lr: 0.000000  loss: 4.7298 (4.7406)  class_acc: 0.0833 (0.0722)  loss_scale: 65536.0000 (74766.4225)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1358  max mem: 15572
Epoch: [4]  [  80/2809]  eta: 0:30:32  lr: 0.000038  min_lr: 0.000000  loss: 4.6413 (4.7369)  class_acc: 0.0833 (0.0720)  loss_scale: 65536.0000 (73626.8642)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1374  max mem: 15572
Epoch: [4]  [  90/2809]  eta: 0:30:24  lr: 0.000038  min_lr: 0.000000  loss: 4.6970 (4.7315)  class_acc: 0.0833 (0.0774)  loss_scale: 65536.0000 (72737.7582)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1662  max mem: 15572
Epoch: [4]  [ 100/2809]  eta: 0:29:52  lr: 0.000038  min_lr: 0.000000  loss: 4.7168 (4.7355)  class_acc: 0.0833 (0.0813)  loss_scale: 65536.0000 (72024.7129)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1761  max mem: 15572
Epoch: [4]  [ 110/2809]  eta: 0:29:44  lr: 0.000038  min_lr: 0.000000  loss: 4.7441 (4.7314)  class_acc: 0.0833 (0.0837)  loss_scale: 65536.0000 (71440.1441)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1716  max mem: 15572
Epoch: [4]  [ 120/2809]  eta: 0:29:33  lr: 0.000038  min_lr: 0.000000  loss: 4.7098 (4.7284)  class_acc: 0.0833 (0.0830)  loss_scale: 65536.0000 (70952.1983)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.1961  max mem: 15572
Epoch: [4]  [ 130/2809]  eta: 0:28:45  lr: 0.000038  min_lr: 0.000000  loss: 4.7301 (4.7265)  class_acc: 0.0833 (0.0814)  loss_scale: 65536.0000 (70538.7481)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1031  max mem: 15572
[2025-01-15 16:21:13,567] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:21:13,567] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 140/2809]  eta: 0:28:24  lr: 0.000038  min_lr: 0.000000  loss: 4.7251 (4.7240)  class_acc: 0.0833 (0.0836)  loss_scale: 65536.0000 (71113.5319)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0726  max mem: 15572
Epoch: [4]  [ 150/2809]  eta: 0:28:37  lr: 0.000038  min_lr: 0.000000  loss: 4.6878 (4.7173)  class_acc: 0.0833 (0.0817)  loss_scale: 131072.0000 (75084.2914)  weight_decay: 0.0500 (0.0500)  time: 0.6573  data: 0.2096  max mem: 15572
[2025-01-15 16:21:25,582] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11393
[2025-01-15 16:21:25,583] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:21:25,583] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 160/2809]  eta: 0:28:14  lr: 0.000038  min_lr: 0.000000  loss: 4.6793 (4.7150)  class_acc: 0.0417 (0.0797)  loss_scale: 131072.0000 (76933.5652)  weight_decay: 0.0500 (0.0500)  time: 0.6465  data: 0.1920  max mem: 15572
Epoch: [4]  [ 170/2809]  eta: 0:27:44  lr: 0.000038  min_lr: 0.000000  loss: 4.5959 (4.7057)  class_acc: 0.0833 (0.0816)  loss_scale: 65536.0000 (76267.0409)  weight_decay: 0.0500 (0.0500)  time: 0.5161  data: 0.0630  max mem: 15572
Epoch: [4]  [ 180/2809]  eta: 0:27:35  lr: 0.000038  min_lr: 0.000000  loss: 4.6601 (4.7120)  class_acc: 0.0833 (0.0810)  loss_scale: 65536.0000 (75674.1657)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0930  max mem: 15572
Epoch: [4]  [ 190/2809]  eta: 0:27:20  lr: 0.000038  min_lr: 0.000000  loss: 4.8625 (4.7195)  class_acc: 0.0417 (0.0801)  loss_scale: 65536.0000 (75143.3717)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1297  max mem: 15572
Epoch: [4]  [ 200/2809]  eta: 0:27:05  lr: 0.000038  min_lr: 0.000000  loss: 4.8108 (4.7206)  class_acc: 0.0417 (0.0804)  loss_scale: 65536.0000 (74665.3930)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0909  max mem: 15572
Epoch: [4]  [ 210/2809]  eta: 0:27:10  lr: 0.000038  min_lr: 0.000000  loss: 4.7839 (4.7229)  class_acc: 0.0417 (0.0798)  loss_scale: 65536.0000 (74232.7204)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1724  max mem: 15572
Epoch: [4]  [ 220/2809]  eta: 0:26:48  lr: 0.000038  min_lr: 0.000000  loss: 4.7376 (4.7219)  class_acc: 0.0417 (0.0813)  loss_scale: 65536.0000 (73839.2036)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1536  max mem: 15572
Epoch: [4]  [ 230/2809]  eta: 0:26:35  lr: 0.000038  min_lr: 0.000000  loss: 4.7999 (4.7251)  class_acc: 0.0833 (0.0817)  loss_scale: 65536.0000 (73479.7576)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0496  max mem: 15572
Epoch: [4]  [ 240/2809]  eta: 0:26:18  lr: 0.000038  min_lr: 0.000000  loss: 4.7166 (4.7241)  class_acc: 0.0833 (0.0821)  loss_scale: 65536.0000 (73150.1411)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0546  max mem: 15572
Epoch: [4]  [ 250/2809]  eta: 0:26:05  lr: 0.000038  min_lr: 0.000000  loss: 4.7166 (4.7276)  class_acc: 0.0833 (0.0823)  loss_scale: 65536.0000 (72846.7888)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0689  max mem: 15572
Epoch: [4]  [ 260/2809]  eta: 0:25:56  lr: 0.000038  min_lr: 0.000000  loss: 4.8825 (4.7326)  class_acc: 0.0417 (0.0806)  loss_scale: 65536.0000 (72566.6820)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1077  max mem: 15572
Epoch: [4]  [ 270/2809]  eta: 0:25:48  lr: 0.000038  min_lr: 0.000000  loss: 4.7913 (4.7332)  class_acc: 0.0417 (0.0795)  loss_scale: 65536.0000 (72307.2472)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1404  max mem: 15572
Epoch: [4]  [ 280/2809]  eta: 0:25:41  lr: 0.000038  min_lr: 0.000000  loss: 4.7817 (4.7369)  class_acc: 0.0833 (0.0793)  loss_scale: 65536.0000 (72066.2776)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1543  max mem: 15572
[2025-01-15 16:22:39,309] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:22:39,310] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 290/2809]  eta: 0:25:34  lr: 0.000038  min_lr: 0.000000  loss: 4.7817 (4.7340)  class_acc: 0.0833 (0.0795)  loss_scale: 65536.0000 (72967.9175)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1504  max mem: 15572
[2025-01-15 16:22:42,440] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11528
[2025-01-15 16:22:42,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:22:42,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 300/2809]  eta: 0:25:26  lr: 0.000039  min_lr: 0.000000  loss: 4.6181 (4.7352)  class_acc: 0.0417 (0.0785)  loss_scale: 65536.0000 (72938.7375)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1525  max mem: 15572
Epoch: [4]  [ 310/2809]  eta: 0:25:18  lr: 0.000039  min_lr: 0.000000  loss: 4.7074 (4.7355)  class_acc: 0.0833 (0.0784)  loss_scale: 65536.0000 (72700.7074)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1467  max mem: 15572
Epoch: [4]  [ 320/2809]  eta: 0:25:12  lr: 0.000039  min_lr: 0.000000  loss: 4.7756 (4.7365)  class_acc: 0.0833 (0.0790)  loss_scale: 65536.0000 (72477.5078)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1602  max mem: 15572
Epoch: [4]  [ 330/2809]  eta: 0:25:07  lr: 0.000039  min_lr: 0.000000  loss: 4.7308 (4.7361)  class_acc: 0.0833 (0.0798)  loss_scale: 65536.0000 (72267.7946)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1719  max mem: 15572
Epoch: [4]  [ 340/2809]  eta: 0:24:59  lr: 0.000039  min_lr: 0.000000  loss: 4.6901 (4.7360)  class_acc: 0.0833 (0.0805)  loss_scale: 65536.0000 (72070.3812)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1498  max mem: 15572
Epoch: [4]  [ 350/2809]  eta: 0:24:53  lr: 0.000039  min_lr: 0.000000  loss: 4.5815 (4.7324)  class_acc: 0.0833 (0.0806)  loss_scale: 65536.0000 (71884.2165)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1357  max mem: 15572
Epoch: [4]  [ 360/2809]  eta: 0:24:41  lr: 0.000039  min_lr: 0.000000  loss: 4.5448 (4.7313)  class_acc: 0.0833 (0.0802)  loss_scale: 65536.0000 (71708.3657)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0954  max mem: 15572
Epoch: [4]  [ 370/2809]  eta: 0:24:31  lr: 0.000039  min_lr: 0.000000  loss: 4.6858 (4.7310)  class_acc: 0.0417 (0.0800)  loss_scale: 65536.0000 (71541.9946)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0747  max mem: 15572
Epoch: [4]  [ 380/2809]  eta: 0:24:30  lr: 0.000039  min_lr: 0.000000  loss: 4.7979 (4.7323)  class_acc: 0.0833 (0.0802)  loss_scale: 65536.0000 (71384.3570)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1388  max mem: 15572
Epoch: [4]  [ 390/2809]  eta: 0:24:21  lr: 0.000039  min_lr: 0.000000  loss: 4.8334 (4.7352)  class_acc: 0.0833 (0.0803)  loss_scale: 65536.0000 (71234.7826)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1373  max mem: 15572
Epoch: [4]  [ 400/2809]  eta: 0:24:20  lr: 0.000039  min_lr: 0.000000  loss: 4.7670 (4.7342)  class_acc: 0.0833 (0.0806)  loss_scale: 65536.0000 (71092.6683)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1570  max mem: 15572
Epoch: [4]  [ 410/2809]  eta: 0:24:19  lr: 0.000039  min_lr: 0.000000  loss: 4.7670 (4.7357)  class_acc: 0.0833 (0.0810)  loss_scale: 65536.0000 (70957.4696)  weight_decay: 0.0500 (0.0500)  time: 0.6942  data: 0.2351  max mem: 15572
Epoch: [4]  [ 420/2809]  eta: 0:24:16  lr: 0.000039  min_lr: 0.000000  loss: 4.7153 (4.7333)  class_acc: 0.0833 (0.0813)  loss_scale: 65536.0000 (70828.6936)  weight_decay: 0.0500 (0.0500)  time: 0.6827  data: 0.2406  max mem: 15572
[2025-01-15 16:24:01,200] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:24:01,200] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 430/2809]  eta: 0:24:08  lr: 0.000039  min_lr: 0.000000  loss: 4.6034 (4.7305)  class_acc: 0.0833 (0.0817)  loss_scale: 65536.0000 (72226.4501)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1957  max mem: 15572
[2025-01-15 16:24:12,941] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11675
[2025-01-15 16:24:12,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:24:12,941] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 440/2809]  eta: 0:24:06  lr: 0.000039  min_lr: 0.000000  loss: 4.6727 (4.7292)  class_acc: 0.0833 (0.0816)  loss_scale: 131072.0000 (73263.6009)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1787  max mem: 15572
Epoch: [4]  [ 450/2809]  eta: 0:23:58  lr: 0.000039  min_lr: 0.000000  loss: 4.7377 (4.7295)  class_acc: 0.0833 (0.0829)  loss_scale: 65536.0000 (73092.2572)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1546  max mem: 15572
Epoch: [4]  [ 460/2809]  eta: 0:23:49  lr: 0.000039  min_lr: 0.000000  loss: 4.7348 (4.7300)  class_acc: 0.0833 (0.0832)  loss_scale: 65536.0000 (72928.3471)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0932  max mem: 15572
Epoch: [4]  [ 470/2809]  eta: 0:23:43  lr: 0.000039  min_lr: 0.000000  loss: 4.7028 (4.7282)  class_acc: 0.0833 (0.0833)  loss_scale: 65536.0000 (72771.3970)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.0997  max mem: 15572
Epoch: [4]  [ 480/2809]  eta: 0:23:37  lr: 0.000039  min_lr: 0.000000  loss: 4.6650 (4.7272)  class_acc: 0.0417 (0.0826)  loss_scale: 65536.0000 (72620.9730)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1319  max mem: 15572
Epoch: [4]  [ 490/2809]  eta: 0:23:26  lr: 0.000039  min_lr: 0.000000  loss: 4.6721 (4.7278)  class_acc: 0.0833 (0.0827)  loss_scale: 65536.0000 (72476.6762)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0866  max mem: 15572
Epoch: [4]  [ 500/2809]  eta: 0:23:22  lr: 0.000039  min_lr: 0.000000  loss: 4.6966 (4.7282)  class_acc: 0.0833 (0.0824)  loss_scale: 65536.0000 (72338.1397)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0826  max mem: 15572
Epoch: [4]  [ 510/2809]  eta: 0:23:18  lr: 0.000039  min_lr: 0.000000  loss: 4.6716 (4.7250)  class_acc: 0.0833 (0.0830)  loss_scale: 65536.0000 (72205.0254)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.1492  max mem: 15572
Epoch: [4]  [ 520/2809]  eta: 0:23:10  lr: 0.000039  min_lr: 0.000000  loss: 4.6935 (4.7275)  class_acc: 0.0833 (0.0825)  loss_scale: 65536.0000 (72077.0211)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1272  max mem: 15572
Epoch: [4]  [ 530/2809]  eta: 0:23:00  lr: 0.000039  min_lr: 0.000000  loss: 4.8167 (4.7284)  class_acc: 0.0417 (0.0826)  loss_scale: 65536.0000 (71953.8380)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0574  max mem: 15572
Epoch: [4]  [ 540/2809]  eta: 0:22:53  lr: 0.000039  min_lr: 0.000000  loss: 4.8186 (4.7294)  class_acc: 0.0833 (0.0829)  loss_scale: 65536.0000 (71835.2089)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0750  max mem: 15572
Epoch: [4]  [ 550/2809]  eta: 0:22:46  lr: 0.000039  min_lr: 0.000000  loss: 4.8241 (4.7308)  class_acc: 0.0833 (0.0830)  loss_scale: 65536.0000 (71720.8857)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1256  max mem: 15572
Epoch: [4]  [ 560/2809]  eta: 0:22:39  lr: 0.000039  min_lr: 0.000000  loss: 4.7154 (4.7296)  class_acc: 0.0833 (0.0836)  loss_scale: 65536.0000 (71610.6381)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1222  max mem: 15572
[2025-01-15 16:25:27,086] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:25:27,087] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:25:28,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11805
[2025-01-15 16:25:28,463] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:25:28,464] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 570/2809]  eta: 0:22:32  lr: 0.000039  min_lr: 0.000000  loss: 4.6468 (4.7293)  class_acc: 0.0833 (0.0838)  loss_scale: 65536.0000 (71619.0263)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1078  max mem: 15572
Epoch: [4]  [ 580/2809]  eta: 0:22:23  lr: 0.000039  min_lr: 0.000000  loss: 4.6536 (4.7286)  class_acc: 0.0833 (0.0841)  loss_scale: 65536.0000 (71514.3270)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0866  max mem: 15572
Epoch: [4]  [ 590/2809]  eta: 0:22:17  lr: 0.000039  min_lr: 0.000000  loss: 4.6438 (4.7280)  class_acc: 0.0833 (0.0838)  loss_scale: 65536.0000 (71413.1709)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1092  max mem: 15572
Epoch: [4]  [ 600/2809]  eta: 0:22:09  lr: 0.000040  min_lr: 0.000000  loss: 4.6395 (4.7278)  class_acc: 0.0833 (0.0841)  loss_scale: 65536.0000 (71315.3810)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1219  max mem: 15572
Epoch: [4]  [ 610/2809]  eta: 0:22:00  lr: 0.000040  min_lr: 0.000000  loss: 4.7758 (4.7297)  class_acc: 0.1250 (0.0851)  loss_scale: 65536.0000 (71220.7921)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0955  max mem: 15572
Epoch: [4]  [ 620/2809]  eta: 0:21:52  lr: 0.000040  min_lr: 0.000000  loss: 4.8178 (4.7302)  class_acc: 0.0833 (0.0855)  loss_scale: 65536.0000 (71129.2496)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.1023  max mem: 15572
Epoch: [4]  [ 630/2809]  eta: 0:21:47  lr: 0.000040  min_lr: 0.000000  loss: 4.7311 (4.7299)  class_acc: 0.0833 (0.0856)  loss_scale: 65536.0000 (71040.6086)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1567  max mem: 15572
Epoch: [4]  [ 640/2809]  eta: 0:21:43  lr: 0.000040  min_lr: 0.000000  loss: 4.6142 (4.7284)  class_acc: 0.0417 (0.0853)  loss_scale: 65536.0000 (70954.7332)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.2056  max mem: 15572
Epoch: [4]  [ 650/2809]  eta: 0:21:34  lr: 0.000040  min_lr: 0.000000  loss: 4.6125 (4.7271)  class_acc: 0.0833 (0.0861)  loss_scale: 65536.0000 (70871.4962)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1348  max mem: 15572
Epoch: [4]  [ 660/2809]  eta: 0:21:29  lr: 0.000040  min_lr: 0.000000  loss: 4.6986 (4.7274)  class_acc: 0.1250 (0.0864)  loss_scale: 65536.0000 (70790.7776)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1231  max mem: 15572
Epoch: [4]  [ 670/2809]  eta: 0:21:22  lr: 0.000040  min_lr: 0.000000  loss: 4.7380 (4.7280)  class_acc: 0.0833 (0.0865)  loss_scale: 65536.0000 (70712.4650)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1325  max mem: 15572
Epoch: [4]  [ 680/2809]  eta: 0:21:21  lr: 0.000040  min_lr: 0.000000  loss: 4.6961 (4.7269)  class_acc: 0.0833 (0.0868)  loss_scale: 65536.0000 (70636.4523)  weight_decay: 0.0500 (0.0500)  time: 0.6536  data: 0.1667  max mem: 15572
Epoch: [4]  [ 690/2809]  eta: 0:21:10  lr: 0.000040  min_lr: 0.000000  loss: 4.6961 (4.7269)  class_acc: 0.0833 (0.0869)  loss_scale: 65536.0000 (70562.6397)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1312  max mem: 15572
[2025-01-15 16:26:43,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:26:43,361] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:26:43,803] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11935
[2025-01-15 16:26:43,804] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:26:43,805] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 700/2809]  eta: 0:21:03  lr: 0.000040  min_lr: 0.000000  loss: 4.7558 (4.7280)  class_acc: 0.1250 (0.0880)  loss_scale: 65536.0000 (70584.4223)  weight_decay: 0.0500 (0.0500)  time: 0.5141  data: 0.0558  max mem: 15572
Epoch: [4]  [ 710/2809]  eta: 0:20:57  lr: 0.000040  min_lr: 0.000000  loss: 4.7180 (4.7280)  class_acc: 0.1250 (0.0881)  loss_scale: 65536.0000 (70513.4177)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1103  max mem: 15572
Epoch: [4]  [ 720/2809]  eta: 0:20:51  lr: 0.000040  min_lr: 0.000000  loss: 4.7132 (4.7281)  class_acc: 0.0833 (0.0882)  loss_scale: 65536.0000 (70444.3828)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1176  max mem: 15572
Epoch: [4]  [ 730/2809]  eta: 0:20:44  lr: 0.000040  min_lr: 0.000000  loss: 4.7072 (4.7278)  class_acc: 0.0833 (0.0882)  loss_scale: 65536.0000 (70377.2367)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1147  max mem: 15572
Epoch: [4]  [ 740/2809]  eta: 0:20:35  lr: 0.000040  min_lr: 0.000000  loss: 4.7235 (4.7286)  class_acc: 0.0833 (0.0883)  loss_scale: 65536.0000 (70311.9028)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0668  max mem: 15572
Epoch: [4]  [ 750/2809]  eta: 0:20:30  lr: 0.000040  min_lr: 0.000000  loss: 4.7042 (4.7268)  class_acc: 0.0833 (0.0883)  loss_scale: 65536.0000 (70248.3089)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1102  max mem: 15572
Epoch: [4]  [ 760/2809]  eta: 0:20:24  lr: 0.000040  min_lr: 0.000000  loss: 4.6821 (4.7275)  class_acc: 0.0833 (0.0881)  loss_scale: 65536.0000 (70186.3863)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1583  max mem: 15572
[2025-01-15 16:27:20,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=71, lr=[3.8803422587279e-07, 3.8803422587279e-07, 5.543346083897001e-07, 5.543346083897001e-07, 7.919065834138574e-07, 7.919065834138574e-07, 1.1312951191626535e-06, 1.1312951191626535e-06, 1.6161358845180763e-06, 1.6161358845180763e-06, 2.3087655493115375e-06, 2.3087655493115375e-06, 3.298236499016483e-06, 3.298236499016483e-06, 4.7117664271664045e-06, 4.7117664271664045e-06, 6.731094895952006e-06, 6.731094895952006e-06, 9.61584985136001e-06, 9.61584985136001e-06, 1.3736928359085727e-05, 1.3736928359085727e-05, 1.9624183370122472e-05, 1.9624183370122472e-05, 2.8034547671603533e-05, 2.8034547671603533e-05, 4.0049353816576476e-05, 4.0049353816576476e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:27:20,606] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=27.551778293282084, CurrSamplesPerSec=31.013763794141408, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [ 770/2809]  eta: 0:20:17  lr: 0.000040  min_lr: 0.000000  loss: 4.6602 (4.7262)  class_acc: 0.0417 (0.0878)  loss_scale: 65536.0000 (70126.0700)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1170  max mem: 15572
Epoch: [4]  [ 780/2809]  eta: 0:20:11  lr: 0.000040  min_lr: 0.000000  loss: 4.6333 (4.7252)  class_acc: 0.0417 (0.0879)  loss_scale: 65536.0000 (70067.2983)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1213  max mem: 15572
Epoch: [4]  [ 790/2809]  eta: 0:20:06  lr: 0.000040  min_lr: 0.000000  loss: 4.6606 (4.7257)  class_acc: 0.0833 (0.0877)  loss_scale: 65536.0000 (70010.0126)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1638  max mem: 15572
Epoch: [4]  [ 800/2809]  eta: 0:19:59  lr: 0.000040  min_lr: 0.000000  loss: 4.6884 (4.7249)  class_acc: 0.0833 (0.0878)  loss_scale: 65536.0000 (69954.1573)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1236  max mem: 15572
Epoch: [4]  [ 810/2809]  eta: 0:19:51  lr: 0.000040  min_lr: 0.000000  loss: 4.6653 (4.7245)  class_acc: 0.1250 (0.0885)  loss_scale: 65536.0000 (69899.6794)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0462  max mem: 15572
Epoch: [4]  [ 820/2809]  eta: 0:19:48  lr: 0.000040  min_lr: 0.000000  loss: 4.6994 (4.7242)  class_acc: 0.1250 (0.0889)  loss_scale: 65536.0000 (69846.5286)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1494  max mem: 15572
[2025-01-15 16:28:00,087] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:28:00,088] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 830/2809]  eta: 0:19:43  lr: 0.000040  min_lr: 0.000000  loss: 4.6409 (4.7238)  class_acc: 0.0833 (0.0892)  loss_scale: 65536.0000 (70031.2491)  weight_decay: 0.0500 (0.0500)  time: 0.6692  data: 0.2019  max mem: 15572
[2025-01-15 16:28:02,676] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12069
[2025-01-15 16:28:02,676] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:28:02,676] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 840/2809]  eta: 0:19:36  lr: 0.000040  min_lr: 0.000000  loss: 4.6409 (4.7232)  class_acc: 0.0833 (0.0894)  loss_scale: 65536.0000 (70133.6504)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1135  max mem: 15572
Epoch: [4]  [ 850/2809]  eta: 0:19:27  lr: 0.000040  min_lr: 0.000000  loss: 4.7281 (4.7238)  class_acc: 0.0833 (0.0891)  loss_scale: 65536.0000 (70079.6240)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0424  max mem: 15572
Epoch: [4]  [ 860/2809]  eta: 0:19:22  lr: 0.000040  min_lr: 0.000000  loss: 4.7085 (4.7231)  class_acc: 0.0833 (0.0899)  loss_scale: 65536.0000 (70026.8525)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0544  max mem: 15572
Epoch: [4]  [ 870/2809]  eta: 0:19:15  lr: 0.000040  min_lr: 0.000000  loss: 4.6431 (4.7234)  class_acc: 0.0833 (0.0902)  loss_scale: 65536.0000 (69975.2928)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1177  max mem: 15572
Epoch: [4]  [ 880/2809]  eta: 0:19:10  lr: 0.000040  min_lr: 0.000000  loss: 4.6024 (4.7212)  class_acc: 0.1250 (0.0906)  loss_scale: 65536.0000 (69924.9035)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1493  max mem: 15572
Epoch: [4]  [ 890/2809]  eta: 0:19:03  lr: 0.000040  min_lr: 0.000000  loss: 4.5791 (4.7208)  class_acc: 0.1250 (0.0906)  loss_scale: 65536.0000 (69875.6453)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1499  max mem: 15572
Epoch: [4]  [ 900/2809]  eta: 0:18:57  lr: 0.000041  min_lr: 0.000000  loss: 4.6901 (4.7209)  class_acc: 0.0833 (0.0907)  loss_scale: 65536.0000 (69827.4806)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1280  max mem: 15572
Epoch: [4]  [ 910/2809]  eta: 0:18:52  lr: 0.000041  min_lr: 0.000000  loss: 4.7383 (4.7211)  class_acc: 0.1250 (0.0911)  loss_scale: 65536.0000 (69780.3732)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1473  max mem: 15572
Epoch: [4]  [ 920/2809]  eta: 0:18:45  lr: 0.000041  min_lr: 0.000000  loss: 4.7383 (4.7212)  class_acc: 0.1250 (0.0913)  loss_scale: 65536.0000 (69734.2888)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1461  max mem: 15572
Epoch: [4]  [ 930/2809]  eta: 0:18:38  lr: 0.000041  min_lr: 0.000000  loss: 4.7361 (4.7210)  class_acc: 0.0833 (0.0911)  loss_scale: 65536.0000 (69689.1944)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1046  max mem: 15572
Epoch: [4]  [ 940/2809]  eta: 0:18:34  lr: 0.000041  min_lr: 0.000000  loss: 4.7011 (4.7203)  class_acc: 0.0833 (0.0916)  loss_scale: 65536.0000 (69645.0584)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1356  max mem: 15572
Epoch: [4]  [ 950/2809]  eta: 0:18:27  lr: 0.000041  min_lr: 0.000000  loss: 4.6509 (4.7196)  class_acc: 0.0417 (0.0913)  loss_scale: 65536.0000 (69601.8507)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1139  max mem: 15572
Epoch: [4]  [ 960/2809]  eta: 0:18:21  lr: 0.000041  min_lr: 0.000000  loss: 4.6697 (4.7192)  class_acc: 0.0417 (0.0913)  loss_scale: 65536.0000 (69559.5421)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0702  max mem: 15572
[2025-01-15 16:29:17,695] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:29:17,696] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:29:21,551] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12204
[2025-01-15 16:29:21,551] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:29:21,552] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 970/2809]  eta: 0:18:14  lr: 0.000041  min_lr: 0.000000  loss: 4.6697 (4.7189)  class_acc: 0.0833 (0.0912)  loss_scale: 65536.0000 (69923.0649)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1083  max mem: 15572
Epoch: [4]  [ 980/2809]  eta: 0:18:09  lr: 0.000041  min_lr: 0.000000  loss: 4.6427 (4.7187)  class_acc: 0.0417 (0.0909)  loss_scale: 65536.0000 (69878.3445)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1454  max mem: 15572
Epoch: [4]  [ 990/2809]  eta: 0:18:04  lr: 0.000041  min_lr: 0.000000  loss: 4.6901 (4.7183)  class_acc: 0.0417 (0.0914)  loss_scale: 65536.0000 (69834.5267)  weight_decay: 0.0500 (0.0500)  time: 0.6394  data: 0.1925  max mem: 15572
Epoch: [4]  [1000/2809]  eta: 0:17:58  lr: 0.000041  min_lr: 0.000000  loss: 4.6387 (4.7177)  class_acc: 0.0833 (0.0912)  loss_scale: 65536.0000 (69791.5844)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.1983  max mem: 15572
Epoch: [4]  [1010/2809]  eta: 0:17:54  lr: 0.000041  min_lr: 0.000000  loss: 4.6291 (4.7174)  class_acc: 0.0833 (0.0913)  loss_scale: 65536.0000 (69749.4916)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.1912  max mem: 15572
Epoch: [4]  [1020/2809]  eta: 0:17:48  lr: 0.000041  min_lr: 0.000000  loss: 4.6978 (4.7171)  class_acc: 0.0833 (0.0913)  loss_scale: 65536.0000 (69708.2233)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.1785  max mem: 15572
Epoch: [4]  [1030/2809]  eta: 0:17:43  lr: 0.000041  min_lr: 0.000000  loss: 4.7040 (4.7171)  class_acc: 0.0833 (0.0915)  loss_scale: 65536.0000 (69667.7556)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.1537  max mem: 15572
Epoch: [4]  [1040/2809]  eta: 0:17:39  lr: 0.000041  min_lr: 0.000000  loss: 4.7424 (4.7177)  class_acc: 0.1250 (0.0919)  loss_scale: 65536.0000 (69628.0653)  weight_decay: 0.0500 (0.0500)  time: 0.6777  data: 0.1920  max mem: 15572
Epoch: [4]  [1050/2809]  eta: 0:17:31  lr: 0.000041  min_lr: 0.000000  loss: 4.7815 (4.7182)  class_acc: 0.1250 (0.0921)  loss_scale: 65536.0000 (69589.1304)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1426  max mem: 15572
Epoch: [4]  [1060/2809]  eta: 0:17:25  lr: 0.000041  min_lr: 0.000000  loss: 4.7858 (4.7185)  class_acc: 0.0833 (0.0918)  loss_scale: 65536.0000 (69550.9293)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1094  max mem: 15572
Epoch: [4]  [1070/2809]  eta: 0:17:18  lr: 0.000041  min_lr: 0.000000  loss: 4.6780 (4.7182)  class_acc: 0.0417 (0.0916)  loss_scale: 65536.0000 (69513.4416)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1144  max mem: 15572
Epoch: [4]  [1080/2809]  eta: 0:17:11  lr: 0.000041  min_lr: 0.000000  loss: 4.7409 (4.7182)  class_acc: 0.0833 (0.0917)  loss_scale: 65536.0000 (69476.6475)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0683  max mem: 15572
Epoch: [4]  [1090/2809]  eta: 0:17:05  lr: 0.000041  min_lr: 0.000000  loss: 4.8010 (4.7186)  class_acc: 0.0833 (0.0916)  loss_scale: 65536.0000 (69440.5280)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1082  max mem: 15572
[2025-01-15 16:30:39,187] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:30:39,187] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:30:40,622] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12336
[2025-01-15 16:30:40,623] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:30:40,623] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1100/2809]  eta: 0:16:58  lr: 0.000041  min_lr: 0.000000  loss: 4.7387 (4.7179)  class_acc: 0.1250 (0.0922)  loss_scale: 65536.0000 (69583.6367)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1241  max mem: 15572
Epoch: [4]  [1110/2809]  eta: 0:16:53  lr: 0.000041  min_lr: 0.000000  loss: 4.6849 (4.7181)  class_acc: 0.0833 (0.0920)  loss_scale: 65536.0000 (69547.2043)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1512  max mem: 15572
Epoch: [4]  [1120/2809]  eta: 0:16:48  lr: 0.000041  min_lr: 0.000000  loss: 4.6849 (4.7181)  class_acc: 0.0417 (0.0919)  loss_scale: 65536.0000 (69511.4219)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.1932  max mem: 15572
Epoch: [4]  [1130/2809]  eta: 0:16:42  lr: 0.000041  min_lr: 0.000000  loss: 4.6905 (4.7180)  class_acc: 0.0417 (0.0919)  loss_scale: 65536.0000 (69476.2723)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.1556  max mem: 15572
Epoch: [4]  [1140/2809]  eta: 0:16:37  lr: 0.000041  min_lr: 0.000000  loss: 4.6905 (4.7177)  class_acc: 0.0833 (0.0921)  loss_scale: 65536.0000 (69441.7388)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1358  max mem: 15572
Epoch: [4]  [1150/2809]  eta: 0:16:29  lr: 0.000041  min_lr: 0.000000  loss: 4.6706 (4.7173)  class_acc: 0.0833 (0.0920)  loss_scale: 65536.0000 (69407.8054)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0864  max mem: 15572
Epoch: [4]  [1160/2809]  eta: 0:16:22  lr: 0.000041  min_lr: 0.000000  loss: 4.6427 (4.7169)  class_acc: 0.0833 (0.0920)  loss_scale: 65536.0000 (69374.4565)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0283  max mem: 15572
Epoch: [4]  [1170/2809]  eta: 0:16:16  lr: 0.000041  min_lr: 0.000000  loss: 4.7212 (4.7175)  class_acc: 0.0417 (0.0918)  loss_scale: 65536.0000 (69341.6772)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0835  max mem: 15572
Epoch: [4]  [1180/2809]  eta: 0:16:11  lr: 0.000041  min_lr: 0.000000  loss: 4.7212 (4.7172)  class_acc: 0.0417 (0.0918)  loss_scale: 65536.0000 (69309.4530)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1535  max mem: 15572
Epoch: [4]  [1190/2809]  eta: 0:16:04  lr: 0.000041  min_lr: 0.000000  loss: 4.6620 (4.7175)  class_acc: 0.0833 (0.0920)  loss_scale: 65536.0000 (69277.7699)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1443  max mem: 15572
Epoch: [4]  [1200/2809]  eta: 0:15:58  lr: 0.000042  min_lr: 0.000000  loss: 4.7102 (4.7171)  class_acc: 0.0833 (0.0921)  loss_scale: 65536.0000 (69246.6145)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1111  max mem: 15572
Epoch: [4]  [1210/2809]  eta: 0:15:52  lr: 0.000042  min_lr: 0.000000  loss: 4.7642 (4.7175)  class_acc: 0.0833 (0.0923)  loss_scale: 65536.0000 (69215.9736)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1431  max mem: 15572
Epoch: [4]  [1220/2809]  eta: 0:15:46  lr: 0.000042  min_lr: 0.000000  loss: 4.6911 (4.7172)  class_acc: 0.0833 (0.0923)  loss_scale: 65536.0000 (69185.8346)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1526  max mem: 15572
[2025-01-15 16:31:56,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:31:56,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1230/2809]  eta: 0:15:40  lr: 0.000042  min_lr: 0.000000  loss: 4.6633 (4.7159)  class_acc: 0.1250 (0.0925)  loss_scale: 65536.0000 (69262.6613)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1402  max mem: 15572
Epoch: [4]  [1240/2809]  eta: 0:15:33  lr: 0.000042  min_lr: 0.000000  loss: 4.6590 (4.7163)  class_acc: 0.0833 (0.0925)  loss_scale: 131072.0000 (69760.7220)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1263  max mem: 15572
[2025-01-15 16:32:07,387] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12482
[2025-01-15 16:32:07,387] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:32:07,388] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1250/2809]  eta: 0:15:28  lr: 0.000042  min_lr: 0.000000  loss: 4.7295 (4.7167)  class_acc: 0.0833 (0.0923)  loss_scale: 131072.0000 (69988.8857)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1494  max mem: 15572
Epoch: [4]  [1260/2809]  eta: 0:15:22  lr: 0.000042  min_lr: 0.000000  loss: 4.8495 (4.7178)  class_acc: 0.0417 (0.0923)  loss_scale: 65536.0000 (69953.5734)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1623  max mem: 15572
Epoch: [4]  [1270/2809]  eta: 0:15:16  lr: 0.000042  min_lr: 0.000000  loss: 4.8018 (4.7174)  class_acc: 0.0833 (0.0923)  loss_scale: 65536.0000 (69918.8167)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1601  max mem: 15572
Epoch: [4]  [1280/2809]  eta: 0:15:11  lr: 0.000042  min_lr: 0.000000  loss: 4.7563 (4.7176)  class_acc: 0.0833 (0.0925)  loss_scale: 65536.0000 (69884.6027)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1526  max mem: 15572
Epoch: [4]  [1290/2809]  eta: 0:15:05  lr: 0.000042  min_lr: 0.000000  loss: 4.7683 (4.7180)  class_acc: 0.1250 (0.0926)  loss_scale: 65536.0000 (69850.9187)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1419  max mem: 15572
Epoch: [4]  [1300/2809]  eta: 0:14:59  lr: 0.000042  min_lr: 0.000000  loss: 4.6966 (4.7180)  class_acc: 0.0833 (0.0925)  loss_scale: 65536.0000 (69817.7525)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1414  max mem: 15572
Epoch: [4]  [1310/2809]  eta: 0:14:54  lr: 0.000042  min_lr: 0.000000  loss: 4.6527 (4.7180)  class_acc: 0.0833 (0.0927)  loss_scale: 65536.0000 (69785.0923)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1675  max mem: 15572
Epoch: [4]  [1320/2809]  eta: 0:14:47  lr: 0.000042  min_lr: 0.000000  loss: 4.6530 (4.7177)  class_acc: 0.1250 (0.0930)  loss_scale: 65536.0000 (69752.9266)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1401  max mem: 15572
Epoch: [4]  [1330/2809]  eta: 0:14:40  lr: 0.000042  min_lr: 0.000000  loss: 4.6621 (4.7174)  class_acc: 0.1250 (0.0932)  loss_scale: 65536.0000 (69721.2442)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0830  max mem: 15572
Epoch: [4]  [1340/2809]  eta: 0:14:35  lr: 0.000042  min_lr: 0.000000  loss: 4.6715 (4.7175)  class_acc: 0.0833 (0.0931)  loss_scale: 65536.0000 (69690.0343)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1353  max mem: 15572
Epoch: [4]  [1350/2809]  eta: 0:14:29  lr: 0.000042  min_lr: 0.000000  loss: 4.7213 (4.7175)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69659.2865)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1704  max mem: 15572
Epoch: [4]  [1360/2809]  eta: 0:14:22  lr: 0.000042  min_lr: 0.000000  loss: 4.6527 (4.7169)  class_acc: 0.0833 (0.0934)  loss_scale: 65536.0000 (69628.9904)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1275  max mem: 15572
Epoch: [4]  [1370/2809]  eta: 0:14:16  lr: 0.000042  min_lr: 0.000000  loss: 4.6207 (4.7165)  class_acc: 0.0833 (0.0936)  loss_scale: 65536.0000 (69599.1364)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0892  max mem: 15572
[2025-01-15 16:33:22,578] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:33:22,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:33:24,648] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12613
[2025-01-15 16:33:24,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:33:24,649] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1380/2809]  eta: 0:14:10  lr: 0.000042  min_lr: 0.000000  loss: 4.6888 (4.7169)  class_acc: 0.0833 (0.0934)  loss_scale: 65536.0000 (69664.6256)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1012  max mem: 15572
Epoch: [4]  [1390/2809]  eta: 0:14:03  lr: 0.000042  min_lr: 0.000000  loss: 4.7139 (4.7165)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69634.9446)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0511  max mem: 15572
Epoch: [4]  [1400/2809]  eta: 0:13:56  lr: 0.000042  min_lr: 0.000000  loss: 4.7279 (4.7167)  class_acc: 0.0417 (0.0934)  loss_scale: 65536.0000 (69605.6874)  weight_decay: 0.0500 (0.0500)  time: 0.4744  data: 0.0121  max mem: 15572
Epoch: [4]  [1410/2809]  eta: 0:13:50  lr: 0.000042  min_lr: 0.000000  loss: 4.7279 (4.7169)  class_acc: 0.0417 (0.0934)  loss_scale: 65536.0000 (69576.8448)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1036  max mem: 15572
Epoch: [4]  [1420/2809]  eta: 0:13:45  lr: 0.000042  min_lr: 0.000000  loss: 4.7770 (4.7170)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69548.4082)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.1865  max mem: 15572
Epoch: [4]  [1430/2809]  eta: 0:13:39  lr: 0.000042  min_lr: 0.000000  loss: 4.7896 (4.7168)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69520.3690)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.1864  max mem: 15572
Epoch: [4]  [1440/2809]  eta: 0:13:33  lr: 0.000042  min_lr: 0.000000  loss: 4.6710 (4.7165)  class_acc: 0.0833 (0.0932)  loss_scale: 65536.0000 (69492.7189)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1581  max mem: 15572
Epoch: [4]  [1450/2809]  eta: 0:13:27  lr: 0.000042  min_lr: 0.000000  loss: 4.6715 (4.7164)  class_acc: 0.0417 (0.0930)  loss_scale: 65536.0000 (69465.4500)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1177  max mem: 15572
Epoch: [4]  [1460/2809]  eta: 0:13:21  lr: 0.000042  min_lr: 0.000000  loss: 4.6953 (4.7169)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69438.5544)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0996  max mem: 15572
Epoch: [4]  [1470/2809]  eta: 0:13:14  lr: 0.000042  min_lr: 0.000000  loss: 4.7485 (4.7167)  class_acc: 0.0833 (0.0931)  loss_scale: 65536.0000 (69412.0245)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1255  max mem: 15572
Epoch: [4]  [1480/2809]  eta: 0:13:08  lr: 0.000042  min_lr: 0.000000  loss: 4.7485 (4.7170)  class_acc: 0.0833 (0.0932)  loss_scale: 65536.0000 (69385.8528)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1494  max mem: 15572
Epoch: [4]  [1490/2809]  eta: 0:13:03  lr: 0.000042  min_lr: 0.000000  loss: 4.7397 (4.7168)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69360.0322)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1698  max mem: 15572
Epoch: [4]  [1500/2809]  eta: 0:12:57  lr: 0.000043  min_lr: 0.000000  loss: 4.6844 (4.7165)  class_acc: 0.1250 (0.0936)  loss_scale: 65536.0000 (69334.5556)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1517  max mem: 15572
[2025-01-15 16:34:39,678] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:34:39,678] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1510/2809]  eta: 0:12:51  lr: 0.000043  min_lr: 0.000000  loss: 4.7063 (4.7164)  class_acc: 0.0833 (0.0936)  loss_scale: 65536.0000 (69526.2793)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0945  max mem: 15572
[2025-01-15 16:34:44,063] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12751
[2025-01-15 16:34:44,064] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:34:44,064] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1520/2809]  eta: 0:12:44  lr: 0.000043  min_lr: 0.000000  loss: 4.7132 (4.7163)  class_acc: 0.0833 (0.0939)  loss_scale: 65536.0000 (69672.3945)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0573  max mem: 15572
Epoch: [4]  [1530/2809]  eta: 0:12:39  lr: 0.000043  min_lr: 0.000000  loss: 4.7132 (4.7161)  class_acc: 0.0833 (0.0937)  loss_scale: 65536.0000 (69645.3769)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0905  max mem: 15572
Epoch: [4]  [1540/2809]  eta: 0:12:32  lr: 0.000043  min_lr: 0.000000  loss: 4.6672 (4.7166)  class_acc: 0.0833 (0.0936)  loss_scale: 65536.0000 (69618.7099)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1104  max mem: 15572
Epoch: [4]  [1550/2809]  eta: 0:12:27  lr: 0.000043  min_lr: 0.000000  loss: 4.6852 (4.7164)  class_acc: 0.0417 (0.0935)  loss_scale: 65536.0000 (69592.3868)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1233  max mem: 15572
Epoch: [4]  [1560/2809]  eta: 0:12:21  lr: 0.000043  min_lr: 0.000000  loss: 4.7461 (4.7168)  class_acc: 0.0833 (0.0937)  loss_scale: 65536.0000 (69566.4010)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.1720  max mem: 15572
Epoch: [4]  [1570/2809]  eta: 0:12:15  lr: 0.000043  min_lr: 0.000000  loss: 4.7922 (4.7173)  class_acc: 0.0833 (0.0934)  loss_scale: 65536.0000 (69540.7460)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1049  max mem: 15572
Epoch: [4]  [1580/2809]  eta: 0:12:09  lr: 0.000043  min_lr: 0.000000  loss: 4.7926 (4.7178)  class_acc: 0.0417 (0.0933)  loss_scale: 65536.0000 (69515.4156)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0737  max mem: 15572
Epoch: [4]  [1590/2809]  eta: 0:12:02  lr: 0.000043  min_lr: 0.000000  loss: 4.7798 (4.7177)  class_acc: 0.0417 (0.0932)  loss_scale: 65536.0000 (69490.4035)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0995  max mem: 15572
Epoch: [4]  [1600/2809]  eta: 0:11:57  lr: 0.000043  min_lr: 0.000000  loss: 4.7370 (4.7179)  class_acc: 0.0833 (0.0932)  loss_scale: 65536.0000 (69465.7039)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1654  max mem: 15572
Epoch: [4]  [1610/2809]  eta: 0:11:52  lr: 0.000043  min_lr: 0.000000  loss: 4.7832 (4.7179)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69441.3110)  weight_decay: 0.0500 (0.0500)  time: 0.6667  data: 0.2120  max mem: 15572
Epoch: [4]  [1620/2809]  eta: 0:11:45  lr: 0.000043  min_lr: 0.000000  loss: 4.6618 (4.7170)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69417.2190)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1653  max mem: 15572
Epoch: [4]  [1630/2809]  eta: 0:11:39  lr: 0.000043  min_lr: 0.000000  loss: 4.6618 (4.7175)  class_acc: 0.0417 (0.0932)  loss_scale: 65536.0000 (69393.4224)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1414  max mem: 15572
Epoch: [4]  [1640/2809]  eta: 0:11:34  lr: 0.000043  min_lr: 0.000000  loss: 4.7523 (4.7174)  class_acc: 0.0833 (0.0932)  loss_scale: 65536.0000 (69369.9159)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1661  max mem: 15572
[2025-01-15 16:36:01,545] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:36:01,545] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:36:03,725] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12884
[2025-01-15 16:36:03,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:36:03,726] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1650/2809]  eta: 0:11:27  lr: 0.000043  min_lr: 0.000000  loss: 4.7523 (4.7179)  class_acc: 0.0833 (0.0934)  loss_scale: 65536.0000 (69505.4730)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1061  max mem: 15572
Epoch: [4]  [1660/2809]  eta: 0:11:21  lr: 0.000043  min_lr: 0.000000  loss: 4.6788 (4.7175)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69481.5750)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0419  max mem: 15572
Epoch: [4]  [1670/2809]  eta: 0:11:16  lr: 0.000043  min_lr: 0.000000  loss: 4.6729 (4.7178)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69457.9629)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1233  max mem: 15572
Epoch: [4]  [1680/2809]  eta: 0:11:10  lr: 0.000043  min_lr: 0.000000  loss: 4.6729 (4.7171)  class_acc: 0.0833 (0.0934)  loss_scale: 65536.0000 (69434.6318)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1567  max mem: 15572
Epoch: [4]  [1690/2809]  eta: 0:11:03  lr: 0.000043  min_lr: 0.000000  loss: 4.7183 (4.7175)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69411.5766)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0911  max mem: 15572
Epoch: [4]  [1700/2809]  eta: 0:10:58  lr: 0.000043  min_lr: 0.000000  loss: 4.6704 (4.7168)  class_acc: 0.0833 (0.0934)  loss_scale: 65536.0000 (69388.7925)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1100  max mem: 15572
Epoch: [4]  [1710/2809]  eta: 0:10:52  lr: 0.000043  min_lr: 0.000000  loss: 4.6267 (4.7168)  class_acc: 0.0833 (0.0934)  loss_scale: 65536.0000 (69366.2747)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1323  max mem: 15572
Epoch: [4]  [1720/2809]  eta: 0:10:46  lr: 0.000043  min_lr: 0.000000  loss: 4.6516 (4.7167)  class_acc: 0.1250 (0.0936)  loss_scale: 65536.0000 (69344.0186)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1385  max mem: 15572
Epoch: [4]  [1730/2809]  eta: 0:10:40  lr: 0.000043  min_lr: 0.000000  loss: 4.6830 (4.7169)  class_acc: 0.0833 (0.0935)  loss_scale: 65536.0000 (69322.0196)  weight_decay: 0.0500 (0.0500)  time: 0.6523  data: 0.2102  max mem: 15572
Epoch: [4]  [1740/2809]  eta: 0:10:34  lr: 0.000043  min_lr: 0.000000  loss: 4.6933 (4.7167)  class_acc: 0.0833 (0.0934)  loss_scale: 65536.0000 (69300.2734)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1568  max mem: 15572
Epoch: [4]  [1750/2809]  eta: 0:10:28  lr: 0.000043  min_lr: 0.000000  loss: 4.6626 (4.7164)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (69278.7756)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1048  max mem: 15572
Epoch: [4]  [1760/2809]  eta: 0:10:22  lr: 0.000043  min_lr: 0.000000  loss: 4.6258 (4.7156)  class_acc: 0.0833 (0.0934)  loss_scale: 65536.0000 (69257.5219)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1031  max mem: 15572
[2025-01-15 16:37:10,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=78, lr=[4.2037310626888885e-07, 4.2037310626888885e-07, 6.005330089555555e-07, 6.005330089555555e-07, 8.579042985079365e-07, 8.579042985079365e-07, 1.2255775692970524e-06, 1.2255775692970524e-06, 1.750825098995789e-06, 1.750825098995789e-06, 2.5011787128511272e-06, 2.5011787128511272e-06, 3.573112446930182e-06, 3.573112446930182e-06, 5.1044463527574035e-06, 5.1044463527574035e-06, 7.292066218224862e-06, 7.292066218224862e-06, 1.0417237454606947e-05, 1.0417237454606947e-05, 1.4881767792295639e-05, 1.4881767792295639e-05, 2.1259668274708057e-05, 2.1259668274708057e-05, 3.037095467815437e-05, 3.037095467815437e-05, 4.33870781116491e-05, 4.33870781116491e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:37:10,676] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=27.55222949325302, CurrSamplesPerSec=27.11854938560959, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [1770/2809]  eta: 0:10:16  lr: 0.000043  min_lr: 0.000000  loss: 4.6521 (4.7158)  class_acc: 0.0833 (0.0937)  loss_scale: 65536.0000 (69236.5082)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1222  max mem: 15572
[2025-01-15 16:37:19,723] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:37:19,724] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1780/2809]  eta: 0:10:10  lr: 0.000043  min_lr: 0.000000  loss: 4.7222 (4.7153)  class_acc: 0.0833 (0.0936)  loss_scale: 65536.0000 (69362.9197)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1438  max mem: 15572
[2025-01-15 16:37:25,324] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13022
[2025-01-15 16:37:25,324] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:37:25,325] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1790/2809]  eta: 0:10:04  lr: 0.000043  min_lr: 0.000000  loss: 4.6818 (4.7154)  class_acc: 0.0833 (0.0936)  loss_scale: 65536.0000 (69524.5114)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1305  max mem: 15572
Epoch: [4]  [1800/2809]  eta: 0:09:58  lr: 0.000044  min_lr: 0.000000  loss: 4.7247 (4.7157)  class_acc: 0.0833 (0.0936)  loss_scale: 65536.0000 (69502.3654)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1095  max mem: 15572
Epoch: [4]  [1810/2809]  eta: 0:09:52  lr: 0.000044  min_lr: 0.000000  loss: 4.7348 (4.7158)  class_acc: 0.0833 (0.0935)  loss_scale: 65536.0000 (69480.4638)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0821  max mem: 15572
Epoch: [4]  [1820/2809]  eta: 0:09:46  lr: 0.000044  min_lr: 0.000000  loss: 4.7204 (4.7157)  class_acc: 0.0833 (0.0935)  loss_scale: 65536.0000 (69458.8029)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0915  max mem: 15572
Epoch: [4]  [1830/2809]  eta: 0:09:40  lr: 0.000044  min_lr: 0.000000  loss: 4.6496 (4.7151)  class_acc: 0.0833 (0.0938)  loss_scale: 65536.0000 (69437.3785)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0992  max mem: 15572
Epoch: [4]  [1840/2809]  eta: 0:09:34  lr: 0.000044  min_lr: 0.000000  loss: 4.6680 (4.7150)  class_acc: 0.1250 (0.0939)  loss_scale: 65536.0000 (69416.1869)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0724  max mem: 15572
Epoch: [4]  [1850/2809]  eta: 0:09:28  lr: 0.000044  min_lr: 0.000000  loss: 4.6976 (4.7150)  class_acc: 0.0833 (0.0939)  loss_scale: 65536.0000 (69395.2242)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0744  max mem: 15572
Epoch: [4]  [1860/2809]  eta: 0:09:22  lr: 0.000044  min_lr: 0.000000  loss: 4.7080 (4.7150)  class_acc: 0.0417 (0.0939)  loss_scale: 65536.0000 (69374.4868)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1334  max mem: 15572
Epoch: [4]  [1870/2809]  eta: 0:09:16  lr: 0.000044  min_lr: 0.000000  loss: 4.7188 (4.7151)  class_acc: 0.0833 (0.0939)  loss_scale: 65536.0000 (69353.9711)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1082  max mem: 15572
Epoch: [4]  [1880/2809]  eta: 0:09:10  lr: 0.000044  min_lr: 0.000000  loss: 4.7052 (4.7151)  class_acc: 0.0833 (0.0942)  loss_scale: 65536.0000 (69333.6736)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1029  max mem: 15572
Epoch: [4]  [1890/2809]  eta: 0:09:04  lr: 0.000044  min_lr: 0.000000  loss: 4.7052 (4.7151)  class_acc: 0.1250 (0.0944)  loss_scale: 65536.0000 (69313.5907)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.1968  max mem: 15572
Epoch: [4]  [1900/2809]  eta: 0:08:58  lr: 0.000044  min_lr: 0.000000  loss: 4.7194 (4.7153)  class_acc: 0.0833 (0.0943)  loss_scale: 65536.0000 (69293.7191)  weight_decay: 0.0500 (0.0500)  time: 0.6529  data: 0.2200  max mem: 15572
Epoch: [4]  [1910/2809]  eta: 0:08:52  lr: 0.000044  min_lr: 0.000000  loss: 4.7460 (4.7150)  class_acc: 0.0833 (0.0942)  loss_scale: 65536.0000 (69274.0555)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1594  max mem: 15572
[2025-01-15 16:38:39,957] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:38:39,958] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1920/2809]  eta: 0:08:47  lr: 0.000044  min_lr: 0.000000  loss: 4.7460 (4.7150)  class_acc: 0.0833 (0.0943)  loss_scale: 65536.0000 (69459.2900)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.1885  max mem: 15572
[2025-01-15 16:38:47,411] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13161
[2025-01-15 16:38:47,412] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:38:47,412] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1930/2809]  eta: 0:08:41  lr: 0.000044  min_lr: 0.000000  loss: 4.6743 (4.7146)  class_acc: 0.1250 (0.0947)  loss_scale: 65536.0000 (69574.7281)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.1635  max mem: 15572
Epoch: [4]  [1940/2809]  eta: 0:08:35  lr: 0.000044  min_lr: 0.000000  loss: 4.6064 (4.7144)  class_acc: 0.0833 (0.0948)  loss_scale: 65536.0000 (69553.9207)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0849  max mem: 15572
Epoch: [4]  [1950/2809]  eta: 0:08:29  lr: 0.000044  min_lr: 0.000000  loss: 4.6827 (4.7142)  class_acc: 0.0833 (0.0949)  loss_scale: 65536.0000 (69533.3265)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1327  max mem: 15572
Epoch: [4]  [1960/2809]  eta: 0:08:24  lr: 0.000044  min_lr: 0.000000  loss: 4.6827 (4.7139)  class_acc: 0.1250 (0.0950)  loss_scale: 65536.0000 (69512.9424)  weight_decay: 0.0500 (0.0500)  time: 0.6564  data: 0.1734  max mem: 15572
Epoch: [4]  [1970/2809]  eta: 0:08:18  lr: 0.000044  min_lr: 0.000000  loss: 4.6422 (4.7136)  class_acc: 0.1250 (0.0953)  loss_scale: 65536.0000 (69492.7651)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1431  max mem: 15572
Epoch: [4]  [1980/2809]  eta: 0:08:12  lr: 0.000044  min_lr: 0.000000  loss: 4.7402 (4.7138)  class_acc: 0.1250 (0.0952)  loss_scale: 65536.0000 (69472.7915)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1389  max mem: 15572
Epoch: [4]  [1990/2809]  eta: 0:08:06  lr: 0.000044  min_lr: 0.000000  loss: 4.7918 (4.7140)  class_acc: 0.0833 (0.0954)  loss_scale: 65536.0000 (69453.0186)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1559  max mem: 15572
Epoch: [4]  [2000/2809]  eta: 0:08:00  lr: 0.000044  min_lr: 0.000000  loss: 4.6081 (4.7133)  class_acc: 0.1250 (0.0954)  loss_scale: 65536.0000 (69433.4433)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1768  max mem: 15572
Epoch: [4]  [2010/2809]  eta: 0:07:54  lr: 0.000044  min_lr: 0.000000  loss: 4.4928 (4.7123)  class_acc: 0.0833 (0.0954)  loss_scale: 65536.0000 (69414.0627)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.1957  max mem: 15572
Epoch: [4]  [2020/2809]  eta: 0:07:48  lr: 0.000044  min_lr: 0.000000  loss: 4.6059 (4.7126)  class_acc: 0.0833 (0.0954)  loss_scale: 65536.0000 (69394.8738)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1240  max mem: 15572
Epoch: [4]  [2030/2809]  eta: 0:07:42  lr: 0.000044  min_lr: 0.000000  loss: 4.6823 (4.7122)  class_acc: 0.0833 (0.0954)  loss_scale: 65536.0000 (69375.8740)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0336  max mem: 15572
Epoch: [4]  [2040/2809]  eta: 0:07:36  lr: 0.000044  min_lr: 0.000000  loss: 4.6360 (4.7118)  class_acc: 0.1250 (0.0956)  loss_scale: 65536.0000 (69357.0603)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0910  max mem: 15572
Epoch: [4]  [2050/2809]  eta: 0:07:30  lr: 0.000044  min_lr: 0.000000  loss: 4.6515 (4.7116)  class_acc: 0.1250 (0.0956)  loss_scale: 65536.0000 (69338.4300)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1218  max mem: 15572
[2025-01-15 16:40:03,503] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:40:03,503] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2060/2809]  eta: 0:07:24  lr: 0.000044  min_lr: 0.000000  loss: 4.7675 (4.7122)  class_acc: 0.0833 (0.0955)  loss_scale: 65536.0000 (69542.5677)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0753  max mem: 15572
[2025-01-15 16:40:11,527] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13302
[2025-01-15 16:40:11,527] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:40:11,527] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2070/2809]  eta: 0:07:18  lr: 0.000044  min_lr: 0.000000  loss: 4.8040 (4.7124)  class_acc: 0.0417 (0.0953)  loss_scale: 131072.0000 (69681.4447)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0880  max mem: 15572
Epoch: [4]  [2080/2809]  eta: 0:07:12  lr: 0.000044  min_lr: 0.000000  loss: 4.7731 (4.7119)  class_acc: 0.0833 (0.0954)  loss_scale: 65536.0000 (69661.5243)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0641  max mem: 15572
Epoch: [4]  [2090/2809]  eta: 0:07:05  lr: 0.000044  min_lr: 0.000000  loss: 4.7395 (4.7122)  class_acc: 0.1250 (0.0955)  loss_scale: 65536.0000 (69641.7944)  weight_decay: 0.0500 (0.0500)  time: 0.5113  data: 0.0434  max mem: 15572
Epoch: [4]  [2100/2809]  eta: 0:07:00  lr: 0.000045  min_lr: 0.000000  loss: 4.7580 (4.7119)  class_acc: 0.1250 (0.0956)  loss_scale: 65536.0000 (69622.2523)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0823  max mem: 15572
Epoch: [4]  [2110/2809]  eta: 0:06:53  lr: 0.000045  min_lr: 0.000000  loss: 4.7133 (4.7120)  class_acc: 0.1250 (0.0956)  loss_scale: 65536.0000 (69602.8953)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0715  max mem: 15572
Epoch: [4]  [2120/2809]  eta: 0:06:48  lr: 0.000045  min_lr: 0.000000  loss: 4.7001 (4.7119)  class_acc: 0.0833 (0.0956)  loss_scale: 65536.0000 (69583.7209)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1231  max mem: 15572
Epoch: [4]  [2130/2809]  eta: 0:06:42  lr: 0.000045  min_lr: 0.000000  loss: 4.7114 (4.7120)  class_acc: 0.0833 (0.0957)  loss_scale: 65536.0000 (69564.7264)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1416  max mem: 15572
Epoch: [4]  [2140/2809]  eta: 0:06:36  lr: 0.000045  min_lr: 0.000000  loss: 4.7231 (4.7120)  class_acc: 0.0833 (0.0957)  loss_scale: 65536.0000 (69545.9094)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0891  max mem: 15572
Epoch: [4]  [2150/2809]  eta: 0:06:30  lr: 0.000045  min_lr: 0.000000  loss: 4.7139 (4.7120)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (69527.2673)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1403  max mem: 15572
Epoch: [4]  [2160/2809]  eta: 0:06:24  lr: 0.000045  min_lr: 0.000000  loss: 4.7014 (4.7120)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (69508.7978)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1671  max mem: 15572
Epoch: [4]  [2170/2809]  eta: 0:06:18  lr: 0.000045  min_lr: 0.000000  loss: 4.6712 (4.7117)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (69490.4984)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.1707  max mem: 15572
Epoch: [4]  [2180/2809]  eta: 0:06:12  lr: 0.000045  min_lr: 0.000000  loss: 4.6008 (4.7113)  class_acc: 0.0833 (0.0959)  loss_scale: 65536.0000 (69472.3668)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0858  max mem: 15572
Epoch: [4]  [2190/2809]  eta: 0:06:06  lr: 0.000045  min_lr: 0.000000  loss: 4.7302 (4.7119)  class_acc: 0.1250 (0.0961)  loss_scale: 65536.0000 (69454.4007)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0446  max mem: 15572
[2025-01-15 16:41:24,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:41:24,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2200/2809]  eta: 0:06:00  lr: 0.000045  min_lr: 0.000000  loss: 4.7932 (4.7122)  class_acc: 0.1250 (0.0960)  loss_scale: 65536.0000 (69615.2512)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1346  max mem: 15572
[2025-01-15 16:41:29,197] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13437
[2025-01-15 16:41:29,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:41:29,197] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2210/2809]  eta: 0:05:54  lr: 0.000045  min_lr: 0.000000  loss: 4.6908 (4.7122)  class_acc: 0.1250 (0.0961)  loss_scale: 65536.0000 (69596.8014)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1408  max mem: 15572
Epoch: [4]  [2220/2809]  eta: 0:05:48  lr: 0.000045  min_lr: 0.000000  loss: 4.6950 (4.7124)  class_acc: 0.0833 (0.0961)  loss_scale: 65536.0000 (69578.5178)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1113  max mem: 15572
Epoch: [4]  [2230/2809]  eta: 0:05:43  lr: 0.000045  min_lr: 0.000000  loss: 4.7301 (4.7125)  class_acc: 0.0833 (0.0961)  loss_scale: 65536.0000 (69560.3980)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1707  max mem: 15572
Epoch: [4]  [2240/2809]  eta: 0:05:36  lr: 0.000045  min_lr: 0.000000  loss: 4.6423 (4.7119)  class_acc: 0.0417 (0.0960)  loss_scale: 65536.0000 (69542.4400)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1104  max mem: 15572
Epoch: [4]  [2250/2809]  eta: 0:05:30  lr: 0.000045  min_lr: 0.000000  loss: 4.6539 (4.7118)  class_acc: 0.0417 (0.0959)  loss_scale: 65536.0000 (69524.6415)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0670  max mem: 15572
Epoch: [4]  [2260/2809]  eta: 0:05:24  lr: 0.000045  min_lr: 0.000000  loss: 4.7046 (4.7119)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (69507.0004)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1431  max mem: 15572
Epoch: [4]  [2270/2809]  eta: 0:05:19  lr: 0.000045  min_lr: 0.000000  loss: 4.7308 (4.7119)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (69489.5148)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1870  max mem: 15572
Epoch: [4]  [2280/2809]  eta: 0:05:13  lr: 0.000045  min_lr: 0.000000  loss: 4.7396 (4.7118)  class_acc: 0.0417 (0.0956)  loss_scale: 65536.0000 (69472.1824)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1486  max mem: 15572
Epoch: [4]  [2290/2809]  eta: 0:05:07  lr: 0.000045  min_lr: 0.000000  loss: 4.6578 (4.7118)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (69455.0013)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1288  max mem: 15572
Epoch: [4]  [2300/2809]  eta: 0:05:01  lr: 0.000045  min_lr: 0.000000  loss: 4.6932 (4.7118)  class_acc: 0.1250 (0.0958)  loss_scale: 65536.0000 (69437.9696)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.2064  max mem: 15572
Epoch: [4]  [2310/2809]  eta: 0:04:55  lr: 0.000045  min_lr: 0.000000  loss: 4.6984 (4.7116)  class_acc: 0.1250 (0.0960)  loss_scale: 65536.0000 (69421.0852)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.1580  max mem: 15572
Epoch: [4]  [2320/2809]  eta: 0:04:49  lr: 0.000045  min_lr: 0.000000  loss: 4.6446 (4.7107)  class_acc: 0.1250 (0.0964)  loss_scale: 65536.0000 (69404.3464)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1153  max mem: 15572
[2025-01-15 16:42:45,697] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:42:45,698] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2330/2809]  eta: 0:04:43  lr: 0.000045  min_lr: 0.000000  loss: 4.6310 (4.7104)  class_acc: 0.1250 (0.0966)  loss_scale: 65536.0000 (69415.8662)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1053  max mem: 15572
[2025-01-15 16:42:46,206] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13567
[2025-01-15 16:42:46,207] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:42:46,207] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2340/2809]  eta: 0:04:37  lr: 0.000045  min_lr: 0.000000  loss: 4.6400 (4.7100)  class_acc: 0.0833 (0.0967)  loss_scale: 65536.0000 (69399.2926)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1027  max mem: 15572
Epoch: [4]  [2350/2809]  eta: 0:04:32  lr: 0.000045  min_lr: 0.000000  loss: 4.6866 (4.7100)  class_acc: 0.0833 (0.0966)  loss_scale: 65536.0000 (69382.8601)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1326  max mem: 15572
Epoch: [4]  [2360/2809]  eta: 0:04:26  lr: 0.000045  min_lr: 0.000000  loss: 4.6984 (4.7101)  class_acc: 0.0833 (0.0968)  loss_scale: 65536.0000 (69366.5667)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1109  max mem: 15572
Epoch: [4]  [2370/2809]  eta: 0:04:20  lr: 0.000045  min_lr: 0.000000  loss: 4.6298 (4.7101)  class_acc: 0.1250 (0.0969)  loss_scale: 65536.0000 (69350.4108)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0785  max mem: 15572
Epoch: [4]  [2380/2809]  eta: 0:04:14  lr: 0.000045  min_lr: 0.000000  loss: 4.6665 (4.7104)  class_acc: 0.0833 (0.0969)  loss_scale: 65536.0000 (69334.3906)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1435  max mem: 15572
Epoch: [4]  [2390/2809]  eta: 0:04:08  lr: 0.000045  min_lr: 0.000000  loss: 4.6510 (4.7099)  class_acc: 0.0833 (0.0969)  loss_scale: 65536.0000 (69318.5044)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1528  max mem: 15572
Epoch: [4]  [2400/2809]  eta: 0:04:02  lr: 0.000046  min_lr: 0.000000  loss: 4.6781 (4.7101)  class_acc: 0.0417 (0.0969)  loss_scale: 65536.0000 (69302.7505)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1494  max mem: 15572
Epoch: [4]  [2410/2809]  eta: 0:03:56  lr: 0.000046  min_lr: 0.000000  loss: 4.6781 (4.7097)  class_acc: 0.1250 (0.0972)  loss_scale: 65536.0000 (69287.1273)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1107  max mem: 15572
[2025-01-15 16:43:37,664] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13654
[2025-01-15 16:43:37,665] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 16:43:37,665] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [2420/2809]  eta: 0:03:50  lr: 0.000046  min_lr: 0.000000  loss: 4.5530 (4.7094)  class_acc: 0.1250 (0.0973)  loss_scale: 65536.0000 (69231.0285)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0362  max mem: 15572
Epoch: [4]  [2430/2809]  eta: 0:03:44  lr: 0.000046  min_lr: 0.000000  loss: 4.6430 (4.7095)  class_acc: 0.1250 (0.0974)  loss_scale: 32768.0000 (69081.0366)  weight_decay: 0.0500 (0.0500)  time: 0.4885  data: 0.0489  max mem: 15572
Epoch: [4]  [2440/2809]  eta: 0:03:38  lr: 0.000046  min_lr: 0.000000  loss: 4.7246 (4.7095)  class_acc: 0.1250 (0.0973)  loss_scale: 32768.0000 (68932.2737)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0900  max mem: 15572
Epoch: [4]  [2450/2809]  eta: 0:03:32  lr: 0.000046  min_lr: 0.000000  loss: 4.7127 (4.7092)  class_acc: 0.1250 (0.0975)  loss_scale: 32768.0000 (68784.7246)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1824  max mem: 15572
Epoch: [4]  [2460/2809]  eta: 0:03:26  lr: 0.000046  min_lr: 0.000000  loss: 4.5710 (4.7084)  class_acc: 0.1250 (0.0977)  loss_scale: 32768.0000 (68638.3746)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1506  max mem: 15572
Epoch: [4]  [2470/2809]  eta: 0:03:20  lr: 0.000046  min_lr: 0.000000  loss: 4.6163 (4.7083)  class_acc: 0.1250 (0.0978)  loss_scale: 32768.0000 (68493.2092)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1400  max mem: 15572
Epoch: [4]  [2480/2809]  eta: 0:03:14  lr: 0.000046  min_lr: 0.000000  loss: 4.6535 (4.7087)  class_acc: 0.1250 (0.0980)  loss_scale: 32768.0000 (68349.2140)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1426  max mem: 15572
Epoch: [4]  [2490/2809]  eta: 0:03:08  lr: 0.000046  min_lr: 0.000000  loss: 4.8330 (4.7090)  class_acc: 0.1250 (0.0980)  loss_scale: 32768.0000 (68206.3749)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1326  max mem: 15572
Epoch: [4]  [2500/2809]  eta: 0:03:02  lr: 0.000046  min_lr: 0.000000  loss: 4.7227 (4.7088)  class_acc: 0.0833 (0.0981)  loss_scale: 32768.0000 (68064.6781)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1210  max mem: 15572
Epoch: [4]  [2510/2809]  eta: 0:02:56  lr: 0.000046  min_lr: 0.000000  loss: 4.6687 (4.7088)  class_acc: 0.1250 (0.0982)  loss_scale: 32768.0000 (67924.1099)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0791  max mem: 15572
Epoch: [4]  [2520/2809]  eta: 0:02:51  lr: 0.000046  min_lr: 0.000000  loss: 4.6976 (4.7089)  class_acc: 0.0417 (0.0980)  loss_scale: 32768.0000 (67784.6569)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1184  max mem: 15572
Epoch: [4]  [2530/2809]  eta: 0:02:45  lr: 0.000046  min_lr: 0.000000  loss: 4.6836 (4.7089)  class_acc: 0.0833 (0.0980)  loss_scale: 32768.0000 (67646.3058)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1432  max mem: 15572
Epoch: [4]  [2540/2809]  eta: 0:02:39  lr: 0.000046  min_lr: 0.000000  loss: 4.7164 (4.7090)  class_acc: 0.0833 (0.0982)  loss_scale: 32768.0000 (67509.0437)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1295  max mem: 15572
[2025-01-15 16:44:54,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:44:54,662] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [2550/2809]  eta: 0:02:33  lr: 0.000046  min_lr: 0.000000  loss: 4.7216 (4.7091)  class_acc: 0.1250 (0.0982)  loss_scale: 32768.0000 (67424.2383)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.1727  max mem: 15572
Epoch: [4]  [2560/2809]  eta: 0:02:27  lr: 0.000046  min_lr: 0.000000  loss: 4.6852 (4.7090)  class_acc: 0.1250 (0.0983)  loss_scale: 65536.0000 (67416.8653)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1205  max mem: 15572
Epoch: [4]  [2570/2809]  eta: 0:02:21  lr: 0.000046  min_lr: 0.000000  loss: 4.6616 (4.7088)  class_acc: 0.1250 (0.0985)  loss_scale: 65536.0000 (67409.5496)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0937  max mem: 15572
Epoch: [4]  [2580/2809]  eta: 0:02:15  lr: 0.000046  min_lr: 0.000000  loss: 4.6008 (4.7084)  class_acc: 0.1250 (0.0987)  loss_scale: 65536.0000 (67402.2906)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1558  max mem: 15572
Epoch: [4]  [2590/2809]  eta: 0:02:09  lr: 0.000046  min_lr: 0.000000  loss: 4.6920 (4.7084)  class_acc: 0.1250 (0.0987)  loss_scale: 65536.0000 (67395.0876)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1011  max mem: 15572
Epoch: [4]  [2600/2809]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000000  loss: 4.6920 (4.7085)  class_acc: 0.0833 (0.0988)  loss_scale: 65536.0000 (67387.9400)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0850  max mem: 15572
Epoch: [4]  [2610/2809]  eta: 0:01:57  lr: 0.000046  min_lr: 0.000000  loss: 4.6709 (4.7081)  class_acc: 0.0833 (0.0986)  loss_scale: 65536.0000 (67380.8472)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0777  max mem: 15572
Epoch: [4]  [2620/2809]  eta: 0:01:51  lr: 0.000046  min_lr: 0.000000  loss: 4.6570 (4.7081)  class_acc: 0.0417 (0.0985)  loss_scale: 65536.0000 (67373.8085)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0775  max mem: 15572
Epoch: [4]  [2630/2809]  eta: 0:01:45  lr: 0.000046  min_lr: 0.000000  loss: 4.6570 (4.7083)  class_acc: 0.0833 (0.0987)  loss_scale: 65536.0000 (67366.8233)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1213  max mem: 15572
Epoch: [4]  [2640/2809]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000000  loss: 4.6611 (4.7080)  class_acc: 0.1250 (0.0990)  loss_scale: 65536.0000 (67359.8910)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0962  max mem: 15572
Epoch: [4]  [2650/2809]  eta: 0:01:33  lr: 0.000046  min_lr: 0.000000  loss: 4.6611 (4.7079)  class_acc: 0.1250 (0.0990)  loss_scale: 65536.0000 (67353.0109)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0647  max mem: 15572
Epoch: [4]  [2660/2809]  eta: 0:01:28  lr: 0.000046  min_lr: 0.000000  loss: 4.7243 (4.7080)  class_acc: 0.0833 (0.0991)  loss_scale: 65536.0000 (67346.1826)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0646  max mem: 15572
Epoch: [4]  [2670/2809]  eta: 0:01:22  lr: 0.000046  min_lr: 0.000000  loss: 4.7722 (4.7080)  class_acc: 0.0833 (0.0991)  loss_scale: 65536.0000 (67339.4055)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1177  max mem: 15572
[2025-01-15 16:46:05,998] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:46:05,999] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2680/2809]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000000  loss: 4.7872 (4.7085)  class_acc: 0.0833 (0.0992)  loss_scale: 65536.0000 (67479.3465)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1354  max mem: 15572
[2025-01-15 16:46:11,877] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13921
[2025-01-15 16:46:11,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:46:11,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2690/2809]  eta: 0:01:10  lr: 0.000046  min_lr: 0.000000  loss: 4.6796 (4.7080)  class_acc: 0.1250 (0.0993)  loss_scale: 65536.0000 (67569.5399)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1588  max mem: 15572
Epoch: [4]  [2700/2809]  eta: 0:01:04  lr: 0.000047  min_lr: 0.000000  loss: 4.6362 (4.7080)  class_acc: 0.1250 (0.0994)  loss_scale: 65536.0000 (67562.0111)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1857  max mem: 15572
Epoch: [4]  [2710/2809]  eta: 0:00:58  lr: 0.000047  min_lr: 0.000000  loss: 4.6603 (4.7078)  class_acc: 0.0833 (0.0993)  loss_scale: 65536.0000 (67554.5378)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1401  max mem: 15572
Epoch: [4]  [2720/2809]  eta: 0:00:52  lr: 0.000047  min_lr: 0.000000  loss: 4.6603 (4.7075)  class_acc: 0.1250 (0.0995)  loss_scale: 65536.0000 (67547.1194)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0830  max mem: 15572
Epoch: [4]  [2730/2809]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.7012 (4.7073)  class_acc: 0.1250 (0.0995)  loss_scale: 65536.0000 (67539.7554)  weight_decay: 0.0500 (0.0500)  time: 0.4695  data: 0.0182  max mem: 15572
Epoch: [4]  [2740/2809]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000000  loss: 4.6027 (4.7067)  class_acc: 0.0833 (0.0996)  loss_scale: 65536.0000 (67532.4451)  weight_decay: 0.0500 (0.0500)  time: 0.4682  data: 0.0010  max mem: 15572
Epoch: [4]  [2750/2809]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6499 (4.7066)  class_acc: 0.0833 (0.0996)  loss_scale: 65536.0000 (67525.1879)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0013  max mem: 15572
Epoch: [4]  [2760/2809]  eta: 0:00:28  lr: 0.000047  min_lr: 0.000000  loss: 4.6977 (4.7066)  class_acc: 0.0833 (0.0996)  loss_scale: 65536.0000 (67517.9833)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0011  max mem: 15572
[2025-01-15 16:46:55,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=85, lr=[4.527119866649877e-07, 4.527119866649877e-07, 6.467314095214111e-07, 6.467314095214111e-07, 9.239020136020159e-07, 9.239020136020159e-07, 1.3198600194314515e-06, 1.3198600194314515e-06, 1.8855143134735021e-06, 1.8855143134735021e-06, 2.6935918763907173e-06, 2.6935918763907173e-06, 3.847988394843882e-06, 3.847988394843882e-06, 5.497126278348404e-06, 5.497126278348404e-06, 7.85303754049772e-06, 7.85303754049772e-06, 1.1218625057853886e-05, 1.1218625057853886e-05, 1.6026607225505554e-05, 1.6026607225505554e-05, 2.2895153179293648e-05, 2.2895153179293648e-05, 3.270736168470521e-05, 3.270736168470521e-05, 4.672480240672174e-05, 4.672480240672174e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:46:55,240] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=27.5293770285181, CurrSamplesPerSec=28.13049708811661, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [2770/2809]  eta: 0:00:23  lr: 0.000047  min_lr: 0.000000  loss: 4.6081 (4.7066)  class_acc: 0.0833 (0.0996)  loss_scale: 65536.0000 (67510.8307)  weight_decay: 0.0500 (0.0500)  time: 0.6392  data: 0.1431  max mem: 15572
Epoch: [4]  [2780/2809]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000000  loss: 4.5871 (4.7064)  class_acc: 0.0833 (0.0996)  loss_scale: 65536.0000 (67503.7296)  weight_decay: 0.0500 (0.0500)  time: 0.7431  data: 0.2436  max mem: 15572
Epoch: [4]  [2790/2809]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6515 (4.7065)  class_acc: 0.0833 (0.0997)  loss_scale: 65536.0000 (67496.6793)  weight_decay: 0.0500 (0.0500)  time: 0.7294  data: 0.2267  max mem: 15572
Epoch: [4]  [2800/2809]  eta: 0:00:05  lr: 0.000047  min_lr: 0.000000  loss: 4.6515 (4.7065)  class_acc: 0.0833 (0.0996)  loss_scale: 65536.0000 (67489.6794)  weight_decay: 0.0500 (0.0500)  time: 0.6796  data: 0.2000  max mem: 15572
Epoch: [4]  [2808/2809]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6896 (4.7062)  class_acc: 0.0833 (0.0997)  loss_scale: 65536.0000 (67484.1153)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0988  max mem: 15572
Epoch: [4] Total time: 0:27:40 (0.5912 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000000  loss: 4.6896 (4.7062)  class_acc: 0.0833 (0.0997)  loss_scale: 65536.0000 (67484.1153)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:27:57  loss: 2.4217 (2.4217)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 6.1675  data: 5.9792  max mem: 15572
Val:  [ 10/272]  eta: 0:04:53  loss: 4.2637 (4.2045)  acc1: 0.0000 (16.6667)  acc5: 22.2222 (28.7879)  time: 1.1220  data: 0.9170  max mem: 15572
Val:  [ 20/272]  eta: 0:03:04  loss: 4.2336 (4.1884)  acc1: 0.0000 (13.2275)  acc5: 27.7778 (29.8942)  time: 0.4584  data: 0.2558  max mem: 15572
Val:  [ 30/272]  eta: 0:02:18  loss: 4.2090 (4.1962)  acc1: 5.5556 (11.6487)  acc5: 33.3333 (30.8244)  time: 0.2715  data: 0.0509  max mem: 15572
Val:  [ 40/272]  eta: 0:01:57  loss: 3.9491 (4.0804)  acc1: 5.5556 (11.6531)  acc5: 38.8889 (34.8238)  time: 0.2700  data: 0.0384  max mem: 15572
Val:  [ 50/272]  eta: 0:01:54  loss: 3.7317 (4.0390)  acc1: 11.1111 (13.1808)  acc5: 44.4444 (37.2549)  time: 0.4240  data: 0.2024  max mem: 15572
Val:  [ 60/272]  eta: 0:01:46  loss: 3.2182 (3.9399)  acc1: 16.6667 (17.6685)  acc5: 72.2222 (41.9854)  time: 0.4926  data: 0.2542  max mem: 15572
Val:  [ 70/272]  eta: 0:01:36  loss: 3.4888 (3.8932)  acc1: 27.7778 (17.4491)  acc5: 66.6667 (43.0360)  time: 0.3747  data: 0.1285  max mem: 15572
Val:  [ 80/272]  eta: 0:01:26  loss: 3.6545 (3.9151)  acc1: 5.5556 (17.1468)  acc5: 44.4444 (42.1125)  time: 0.2937  data: 0.0715  max mem: 15572
Val:  [ 90/272]  eta: 0:01:17  loss: 4.2843 (3.9581)  acc1: 0.0000 (15.6288)  acc5: 22.2222 (40.1099)  time: 0.2601  data: 0.0638  max mem: 15572
Val:  [100/272]  eta: 0:01:09  loss: 4.2843 (4.0041)  acc1: 0.0000 (15.0165)  acc5: 22.2222 (38.8339)  time: 0.2242  data: 0.0446  max mem: 15572
Val:  [110/272]  eta: 0:01:03  loss: 4.3670 (4.0409)  acc1: 0.0000 (13.9139)  acc5: 16.6667 (37.3874)  time: 0.2137  data: 0.0349  max mem: 15572
Val:  [120/272]  eta: 0:00:56  loss: 4.3670 (4.0697)  acc1: 0.0000 (13.3609)  acc5: 16.6667 (36.3177)  time: 0.2096  data: 0.0377  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 4.2699 (4.0209)  acc1: 5.5556 (15.2672)  acc5: 27.7778 (38.2528)  time: 0.1917  data: 0.0165  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 3.8471 (4.0119)  acc1: 16.6667 (15.2482)  acc5: 44.4444 (38.6131)  time: 0.2188  data: 0.0008  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 4.0150 (4.0140)  acc1: 5.5556 (14.6063)  acc5: 27.7778 (37.8955)  time: 0.2612  data: 0.0290  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.9890 (4.0128)  acc1: 5.5556 (14.6653)  acc5: 33.3333 (38.2678)  time: 0.2789  data: 0.0636  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 4.0964 (4.0412)  acc1: 5.5556 (14.1001)  acc5: 33.3333 (37.4269)  time: 0.2985  data: 0.0936  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 4.1620 (4.0365)  acc1: 0.0000 (14.2419)  acc5: 22.2222 (37.4156)  time: 0.3408  data: 0.1433  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 4.1802 (4.0516)  acc1: 0.0000 (13.6417)  acc5: 16.6667 (36.6492)  time: 0.3258  data: 0.1351  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 4.0980 (4.0527)  acc1: 0.0000 (14.1238)  acc5: 33.3333 (37.5069)  time: 0.2934  data: 0.1053  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.8456 (4.0636)  acc1: 22.2222 (14.2443)  acc5: 55.5556 (37.5461)  time: 0.3017  data: 0.1087  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.2125 (4.0685)  acc1: 5.5556 (14.3037)  acc5: 33.3333 (37.4057)  time: 0.3133  data: 0.1243  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.8987 (4.0555)  acc1: 27.7778 (15.6566)  acc5: 55.5556 (38.7446)  time: 0.3347  data: 0.1569  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.6874 (4.0449)  acc1: 27.7778 (16.0443)  acc5: 77.7778 (39.9954)  time: 0.3341  data: 0.1535  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 4.0713 (4.0692)  acc1: 5.5556 (15.5821)  acc5: 38.8889 (39.0438)  time: 0.3061  data: 0.1206  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.9504 (4.0267)  acc1: 16.6667 (17.4542)  acc5: 61.1111 (40.8046)  time: 0.3260  data: 0.1412  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.5774 (4.0247)  acc1: 33.3333 (17.5072)  acc5: 72.2222 (41.1234)  time: 0.2574  data: 0.0879  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.5774 (4.0266)  acc1: 33.3333 (17.4892)  acc5: 72.2222 (41.1018)  time: 0.2499  data: 0.0878  max mem: 15572
Val: Total time: 0:01:29 (0.3277 s / it)
* Acc@1 17.489 Acc@5 41.102 loss 4.027
Accuracy of the network on the 4883 val videos: 17.5%
[2025-01-15 16:48:53,710] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 16:48:53,713] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 16:48:53,713] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 16:48:56,572] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 16:48:56,574] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 17.49%
Epoch: [5]  [   0/2809]  eta: 7:11:13  lr: 0.000047  min_lr: 0.000000  loss: 4.6465 (4.6465)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.2109  data: 8.7534  max mem: 15572
[2025-01-15 16:49:08,027] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:49:08,028] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:49:09,394] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14053
[2025-01-15 16:49:09,394] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:49:09,395] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [  10/2809]  eta: 1:04:25  lr: 0.000047  min_lr: 0.000000  loss: 4.6465 (4.7014)  class_acc: 0.1250 (0.1439)  loss_scale: 65536.0000 (83409.4545)  weight_decay: 0.0500 (0.0500)  time: 1.3811  data: 0.9287  max mem: 15572
Epoch: [5]  [  20/2809]  eta: 0:47:42  lr: 0.000047  min_lr: 0.000000  loss: 4.6031 (4.6740)  class_acc: 0.1250 (0.1349)  loss_scale: 65536.0000 (74898.2857)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1583  max mem: 15572
Epoch: [5]  [  30/2809]  eta: 0:40:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6745 (4.6630)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (71878.1935)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1582  max mem: 15572
Epoch: [5]  [  40/2809]  eta: 0:36:57  lr: 0.000047  min_lr: 0.000000  loss: 4.6277 (4.6617)  class_acc: 0.0833 (0.1118)  loss_scale: 65536.0000 (70331.3171)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1206  max mem: 15572
Epoch: [5]  [  50/2809]  eta: 0:36:15  lr: 0.000047  min_lr: 0.000000  loss: 4.6256 (4.6400)  class_acc: 0.0833 (0.1111)  loss_scale: 65536.0000 (69391.0588)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1898  max mem: 15572
Epoch: [5]  [  60/2809]  eta: 0:33:37  lr: 0.000047  min_lr: 0.000000  loss: 4.5478 (4.6363)  class_acc: 0.0833 (0.1148)  loss_scale: 65536.0000 (68759.0820)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1426  max mem: 15572
Epoch: [5]  [  70/2809]  eta: 0:32:42  lr: 0.000047  min_lr: 0.000000  loss: 4.6266 (4.6469)  class_acc: 0.0833 (0.1115)  loss_scale: 65536.0000 (68305.1268)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0908  max mem: 15572
Epoch: [5]  [  80/2809]  eta: 0:31:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6796 (4.6520)  class_acc: 0.0833 (0.1116)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1715  max mem: 15572
Epoch: [5]  [  90/2809]  eta: 0:30:40  lr: 0.000047  min_lr: 0.000000  loss: 4.6504 (4.6512)  class_acc: 0.1250 (0.1136)  loss_scale: 65536.0000 (67696.5275)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0976  max mem: 15572
Epoch: [5]  [ 100/2809]  eta: 0:30:24  lr: 0.000047  min_lr: 0.000000  loss: 4.7727 (4.6655)  class_acc: 0.0833 (0.1114)  loss_scale: 65536.0000 (67482.6139)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1064  max mem: 15572
Epoch: [5]  [ 110/2809]  eta: 0:29:53  lr: 0.000047  min_lr: 0.000000  loss: 4.7654 (4.6670)  class_acc: 0.0833 (0.1119)  loss_scale: 65536.0000 (67307.2432)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1609  max mem: 15572
Epoch: [5]  [ 120/2809]  eta: 0:30:09  lr: 0.000047  min_lr: 0.000000  loss: 4.7240 (4.6660)  class_acc: 0.1250 (0.1133)  loss_scale: 65536.0000 (67160.8595)  weight_decay: 0.0500 (0.0500)  time: 0.6709  data: 0.2340  max mem: 15572
Epoch: [5]  [ 130/2809]  eta: 0:29:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5530 (4.6611)  class_acc: 0.0833 (0.1126)  loss_scale: 65536.0000 (67036.8244)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1630  max mem: 15572
[2025-01-15 16:50:25,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:50:25,246] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 140/2809]  eta: 0:28:48  lr: 0.000047  min_lr: 0.000000  loss: 4.5974 (4.6645)  class_acc: 0.0833 (0.1108)  loss_scale: 65536.0000 (68789.5603)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0706  max mem: 15572
[2025-01-15 16:50:29,806] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14189
[2025-01-15 16:50:29,806] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:50:29,807] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 150/2809]  eta: 0:28:37  lr: 0.000047  min_lr: 0.000000  loss: 4.7355 (4.6664)  class_acc: 0.0833 (0.1084)  loss_scale: 65536.0000 (69876.1325)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1461  max mem: 15572
Epoch: [5]  [ 160/2809]  eta: 0:28:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6199 (4.6597)  class_acc: 0.0833 (0.1102)  loss_scale: 65536.0000 (69606.5590)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1164  max mem: 15572
Epoch: [5]  [ 170/2809]  eta: 0:27:51  lr: 0.000047  min_lr: 0.000000  loss: 4.4961 (4.6582)  class_acc: 0.0833 (0.1084)  loss_scale: 65536.0000 (69368.5146)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0829  max mem: 15572
Epoch: [5]  [ 180/2809]  eta: 0:27:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5510 (4.6513)  class_acc: 0.0833 (0.1098)  loss_scale: 65536.0000 (69156.7735)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1401  max mem: 15572
Epoch: [5]  [ 190/2809]  eta: 0:27:26  lr: 0.000047  min_lr: 0.000000  loss: 4.6347 (4.6536)  class_acc: 0.0833 (0.1099)  loss_scale: 65536.0000 (68967.2042)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1519  max mem: 15572
Epoch: [5]  [ 200/2809]  eta: 0:27:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6919 (4.6573)  class_acc: 0.0833 (0.1086)  loss_scale: 65536.0000 (68796.4975)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1062  max mem: 15572
Epoch: [5]  [ 210/2809]  eta: 0:26:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6518 (4.6515)  class_acc: 0.1250 (0.1120)  loss_scale: 65536.0000 (68641.9716)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1147  max mem: 15572
Epoch: [5]  [ 220/2809]  eta: 0:26:55  lr: 0.000047  min_lr: 0.000000  loss: 4.6301 (4.6545)  class_acc: 0.1250 (0.1139)  loss_scale: 65536.0000 (68501.4299)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1492  max mem: 15572
Epoch: [5]  [ 230/2809]  eta: 0:26:37  lr: 0.000047  min_lr: 0.000000  loss: 4.6856 (4.6586)  class_acc: 0.0833 (0.1120)  loss_scale: 65536.0000 (68373.0563)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1227  max mem: 15572
Epoch: [5]  [ 240/2809]  eta: 0:26:25  lr: 0.000047  min_lr: 0.000000  loss: 4.7321 (4.6618)  class_acc: 0.0417 (0.1108)  loss_scale: 65536.0000 (68255.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0548  max mem: 15572
Epoch: [5]  [ 250/2809]  eta: 0:26:16  lr: 0.000047  min_lr: 0.000000  loss: 4.7152 (4.6637)  class_acc: 0.0833 (0.1106)  loss_scale: 65536.0000 (68146.9960)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0635  max mem: 15572
Epoch: [5]  [ 260/2809]  eta: 0:25:59  lr: 0.000047  min_lr: 0.000000  loss: 4.6157 (4.6617)  class_acc: 0.0833 (0.1103)  loss_scale: 65536.0000 (68046.9579)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0710  max mem: 15572
Epoch: [5]  [ 270/2809]  eta: 0:26:08  lr: 0.000047  min_lr: 0.000000  loss: 4.6023 (4.6602)  class_acc: 0.0833 (0.1095)  loss_scale: 65536.0000 (67954.3026)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.1346  max mem: 15572
[2025-01-15 16:51:45,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:51:45,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:51:47,366] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14322
[2025-01-15 16:51:47,367] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:51:47,368] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 280/2809]  eta: 0:25:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6039 (4.6606)  class_acc: 0.0833 (0.1096)  loss_scale: 65536.0000 (68801.1388)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1087  max mem: 15572
Epoch: [5]  [ 290/2809]  eta: 0:25:35  lr: 0.000047  min_lr: 0.000000  loss: 4.6315 (4.6594)  class_acc: 0.0833 (0.1088)  loss_scale: 65536.0000 (68688.9347)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0283  max mem: 15572
Epoch: [5]  [ 300/2809]  eta: 0:25:31  lr: 0.000047  min_lr: 0.000000  loss: 4.6539 (4.6615)  class_acc: 0.1250 (0.1099)  loss_scale: 65536.0000 (68584.1860)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1233  max mem: 15572
Epoch: [5]  [ 310/2809]  eta: 0:25:27  lr: 0.000047  min_lr: 0.000000  loss: 4.7058 (4.6625)  class_acc: 0.1250 (0.1107)  loss_scale: 65536.0000 (68486.1736)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.1927  max mem: 15572
Epoch: [5]  [ 320/2809]  eta: 0:25:23  lr: 0.000047  min_lr: 0.000000  loss: 4.6547 (4.6622)  class_acc: 0.0833 (0.1105)  loss_scale: 65536.0000 (68394.2679)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1768  max mem: 15572
Epoch: [5]  [ 330/2809]  eta: 0:25:13  lr: 0.000047  min_lr: 0.000000  loss: 4.6975 (4.6639)  class_acc: 0.0833 (0.1100)  loss_scale: 65536.0000 (68307.9154)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1309  max mem: 15572
Epoch: [5]  [ 340/2809]  eta: 0:25:06  lr: 0.000047  min_lr: 0.000000  loss: 4.7395 (4.6648)  class_acc: 0.0833 (0.1105)  loss_scale: 65536.0000 (68226.6276)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1222  max mem: 15572
Epoch: [5]  [ 350/2809]  eta: 0:24:50  lr: 0.000047  min_lr: 0.000000  loss: 4.7098 (4.6642)  class_acc: 0.0833 (0.1108)  loss_scale: 65536.0000 (68149.9715)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0965  max mem: 15572
Epoch: [5]  [ 360/2809]  eta: 0:24:42  lr: 0.000047  min_lr: 0.000000  loss: 4.7300 (4.6655)  class_acc: 0.0833 (0.1108)  loss_scale: 65536.0000 (68077.5623)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.0937  max mem: 15572
Epoch: [5]  [ 370/2809]  eta: 0:24:36  lr: 0.000047  min_lr: 0.000000  loss: 4.7029 (4.6635)  class_acc: 0.1250 (0.1114)  loss_scale: 65536.0000 (68009.0566)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1235  max mem: 15572
Epoch: [5]  [ 380/2809]  eta: 0:24:24  lr: 0.000047  min_lr: 0.000000  loss: 4.6534 (4.6638)  class_acc: 0.1250 (0.1108)  loss_scale: 65536.0000 (67944.1470)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0741  max mem: 15572
Epoch: [5]  [ 390/2809]  eta: 0:24:21  lr: 0.000047  min_lr: 0.000000  loss: 4.7192 (4.6654)  class_acc: 0.0833 (0.1101)  loss_scale: 65536.0000 (67882.5575)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1186  max mem: 15572
Epoch: [5]  [ 400/2809]  eta: 0:24:12  lr: 0.000047  min_lr: 0.000000  loss: 4.6003 (4.6633)  class_acc: 0.1250 (0.1106)  loss_scale: 65536.0000 (67824.0399)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1340  max mem: 15572
[2025-01-15 16:53:02,746] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:53:02,746] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:53:03,149] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14452
[2025-01-15 16:53:03,149] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:53:03,150] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 410/2809]  eta: 0:24:09  lr: 0.000047  min_lr: 0.000000  loss: 4.6409 (4.6651)  class_acc: 0.1250 (0.1107)  loss_scale: 65536.0000 (67927.8248)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1426  max mem: 15572
Epoch: [5]  [ 420/2809]  eta: 0:23:58  lr: 0.000047  min_lr: 0.000000  loss: 4.7597 (4.6671)  class_acc: 0.0833 (0.1106)  loss_scale: 65536.0000 (67871.0119)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1437  max mem: 15572
Epoch: [5]  [ 430/2809]  eta: 0:23:54  lr: 0.000047  min_lr: 0.000000  loss: 4.7832 (4.6697)  class_acc: 0.0417 (0.1101)  loss_scale: 65536.0000 (67816.8353)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1113  max mem: 15572
Epoch: [5]  [ 440/2809]  eta: 0:23:47  lr: 0.000047  min_lr: 0.000000  loss: 4.6736 (4.6687)  class_acc: 0.0833 (0.1110)  loss_scale: 65536.0000 (67765.1156)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1443  max mem: 15572
[2025-01-15 16:53:26,762] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14494
[2025-01-15 16:53:26,763] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 16:53:26,763] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [ 450/2809]  eta: 0:23:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6831 (4.6692)  class_acc: 0.1250 (0.1118)  loss_scale: 65536.0000 (67570.3769)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0846  max mem: 15572
Epoch: [5]  [ 460/2809]  eta: 0:23:30  lr: 0.000047  min_lr: 0.000000  loss: 4.6149 (4.6679)  class_acc: 0.1250 (0.1116)  loss_scale: 32768.0000 (66815.4447)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1023  max mem: 15572
Epoch: [5]  [ 470/2809]  eta: 0:23:23  lr: 0.000047  min_lr: 0.000000  loss: 4.6330 (4.6674)  class_acc: 0.0833 (0.1119)  loss_scale: 32768.0000 (66092.5690)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1631  max mem: 15572
Epoch: [5]  [ 480/2809]  eta: 0:23:14  lr: 0.000047  min_lr: 0.000000  loss: 4.6531 (4.6664)  class_acc: 0.0833 (0.1116)  loss_scale: 32768.0000 (65399.7505)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1236  max mem: 15572
Epoch: [5]  [ 490/2809]  eta: 0:23:07  lr: 0.000047  min_lr: 0.000000  loss: 4.6649 (4.6641)  class_acc: 0.0833 (0.1119)  loss_scale: 32768.0000 (64735.1527)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1163  max mem: 15572
Epoch: [5]  [ 500/2809]  eta: 0:22:59  lr: 0.000047  min_lr: 0.000000  loss: 4.6205 (4.6630)  class_acc: 0.1250 (0.1134)  loss_scale: 32768.0000 (64097.0858)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1002  max mem: 15572
Epoch: [5]  [ 510/2809]  eta: 0:22:53  lr: 0.000047  min_lr: 0.000000  loss: 4.6205 (4.6644)  class_acc: 0.1250 (0.1135)  loss_scale: 32768.0000 (63483.9922)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0800  max mem: 15572
Epoch: [5]  [ 520/2809]  eta: 0:22:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5984 (4.6621)  class_acc: 0.1250 (0.1138)  loss_scale: 32768.0000 (62894.4338)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0781  max mem: 15572
Epoch: [5]  [ 530/2809]  eta: 0:22:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5882 (4.6621)  class_acc: 0.0833 (0.1131)  loss_scale: 32768.0000 (62327.0810)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1512  max mem: 15572
Epoch: [5]  [ 540/2809]  eta: 0:22:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6706 (4.6613)  class_acc: 0.0833 (0.1135)  loss_scale: 32768.0000 (61780.7024)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1496  max mem: 15572
Epoch: [5]  [ 550/2809]  eta: 0:22:29  lr: 0.000047  min_lr: 0.000000  loss: 4.7407 (4.6649)  class_acc: 0.0833 (0.1131)  loss_scale: 32768.0000 (61254.1561)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1119  max mem: 15572
Epoch: [5]  [ 560/2809]  eta: 0:22:19  lr: 0.000047  min_lr: 0.000000  loss: 4.8318 (4.6678)  class_acc: 0.0833 (0.1124)  loss_scale: 32768.0000 (60746.3815)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1094  max mem: 15572
Epoch: [5]  [ 570/2809]  eta: 0:22:11  lr: 0.000047  min_lr: 0.000000  loss: 4.7398 (4.6676)  class_acc: 0.0833 (0.1125)  loss_scale: 32768.0000 (60256.3923)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0749  max mem: 15572
[2025-01-15 16:54:40,700] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:54:40,700] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 580/2809]  eta: 0:22:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6660 (4.6682)  class_acc: 0.0833 (0.1126)  loss_scale: 32768.0000 (59952.4682)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0887  max mem: 15572
Epoch: [5]  [ 590/2809]  eta: 0:21:58  lr: 0.000047  min_lr: 0.000000  loss: 4.6563 (4.6679)  class_acc: 0.0833 (0.1122)  loss_scale: 65536.0000 (60046.9442)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1288  max mem: 15572
Epoch: [5]  [ 600/2809]  eta: 0:21:49  lr: 0.000047  min_lr: 0.000000  loss: 4.6571 (4.6676)  class_acc: 0.0833 (0.1124)  loss_scale: 65536.0000 (60138.2762)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1283  max mem: 15572
Epoch: [5]  [ 610/2809]  eta: 0:21:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5826 (4.6656)  class_acc: 0.0833 (0.1123)  loss_scale: 65536.0000 (60226.6187)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0913  max mem: 15572
Epoch: [5]  [ 620/2809]  eta: 0:21:40  lr: 0.000047  min_lr: 0.000000  loss: 4.5826 (4.6663)  class_acc: 0.1250 (0.1125)  loss_scale: 65536.0000 (60312.1159)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1580  max mem: 15572
Epoch: [5]  [ 630/2809]  eta: 0:21:34  lr: 0.000047  min_lr: 0.000000  loss: 4.5637 (4.6636)  class_acc: 0.1250 (0.1130)  loss_scale: 65536.0000 (60394.9033)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1507  max mem: 15572
Epoch: [5]  [ 640/2809]  eta: 0:21:29  lr: 0.000047  min_lr: 0.000000  loss: 4.5198 (4.6621)  class_acc: 0.1667 (0.1132)  loss_scale: 65536.0000 (60475.1076)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1234  max mem: 15572
Epoch: [5]  [ 650/2809]  eta: 0:21:20  lr: 0.000047  min_lr: 0.000000  loss: 4.5999 (4.6614)  class_acc: 0.0833 (0.1130)  loss_scale: 65536.0000 (60552.8479)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1085  max mem: 15572
Epoch: [5]  [ 660/2809]  eta: 0:21:12  lr: 0.000047  min_lr: 0.000000  loss: 4.6582 (4.6611)  class_acc: 0.0833 (0.1131)  loss_scale: 65536.0000 (60628.2360)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0920  max mem: 15572
Epoch: [5]  [ 670/2809]  eta: 0:21:10  lr: 0.000047  min_lr: 0.000000  loss: 4.6311 (4.6606)  class_acc: 0.1250 (0.1131)  loss_scale: 65536.0000 (60701.3770)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1778  max mem: 15572
Epoch: [5]  [ 680/2809]  eta: 0:21:03  lr: 0.000047  min_lr: 0.000000  loss: 4.5532 (4.6594)  class_acc: 0.1250 (0.1137)  loss_scale: 65536.0000 (60772.3700)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.1810  max mem: 15572
Epoch: [5]  [ 690/2809]  eta: 0:20:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5770 (4.6595)  class_acc: 0.1250 (0.1139)  loss_scale: 65536.0000 (60841.3082)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1489  max mem: 15572
Epoch: [5]  [ 700/2809]  eta: 0:20:55  lr: 0.000047  min_lr: 0.000000  loss: 4.6893 (4.6603)  class_acc: 0.1250 (0.1145)  loss_scale: 65536.0000 (60908.2796)  weight_decay: 0.0500 (0.0500)  time: 0.6587  data: 0.1967  max mem: 15572
[2025-01-15 16:55:57,766] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:55:57,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:55:59,253] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14754
[2025-01-15 16:55:59,253] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:55:59,253] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 710/2809]  eta: 0:20:48  lr: 0.000047  min_lr: 0.000000  loss: 4.6592 (4.6602)  class_acc: 0.1250 (0.1144)  loss_scale: 65536.0000 (61249.8903)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1609  max mem: 15572
Epoch: [5]  [ 720/2809]  eta: 0:20:37  lr: 0.000047  min_lr: 0.000000  loss: 4.6342 (4.6591)  class_acc: 0.1250 (0.1141)  loss_scale: 65536.0000 (61309.3370)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0498  max mem: 15572
Epoch: [5]  [ 730/2809]  eta: 0:20:32  lr: 0.000047  min_lr: 0.000000  loss: 4.6286 (4.6592)  class_acc: 0.0833 (0.1141)  loss_scale: 65536.0000 (61367.1573)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0740  max mem: 15572
Epoch: [5]  [ 740/2809]  eta: 0:20:26  lr: 0.000047  min_lr: 0.000000  loss: 4.6534 (4.6587)  class_acc: 0.1250 (0.1148)  loss_scale: 65536.0000 (61423.4170)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1366  max mem: 15572
Epoch: [5]  [ 750/2809]  eta: 0:20:19  lr: 0.000047  min_lr: 0.000000  loss: 4.6138 (4.6592)  class_acc: 0.1250 (0.1147)  loss_scale: 65536.0000 (61478.1784)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0964  max mem: 15572
Epoch: [5]  [ 760/2809]  eta: 0:20:14  lr: 0.000047  min_lr: 0.000000  loss: 4.7132 (4.6603)  class_acc: 0.0833 (0.1148)  loss_scale: 65536.0000 (61531.5007)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1066  max mem: 15572
Epoch: [5]  [ 770/2809]  eta: 0:20:06  lr: 0.000047  min_lr: 0.000000  loss: 4.6293 (4.6589)  class_acc: 0.1250 (0.1148)  loss_scale: 65536.0000 (61583.4397)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1028  max mem: 15572
Epoch: [5]  [ 780/2809]  eta: 0:19:59  lr: 0.000047  min_lr: 0.000000  loss: 4.4840 (4.6574)  class_acc: 0.1250 (0.1155)  loss_scale: 65536.0000 (61634.0487)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0824  max mem: 15572
Epoch: [5]  [ 790/2809]  eta: 0:19:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6312 (4.6582)  class_acc: 0.1250 (0.1154)  loss_scale: 65536.0000 (61683.3780)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0843  max mem: 15572
Epoch: [5]  [ 800/2809]  eta: 0:19:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6894 (4.6586)  class_acc: 0.0833 (0.1155)  loss_scale: 65536.0000 (61731.4757)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1506  max mem: 15572
Epoch: [5]  [ 810/2809]  eta: 0:19:40  lr: 0.000047  min_lr: 0.000000  loss: 4.6487 (4.6578)  class_acc: 0.1250 (0.1155)  loss_scale: 65536.0000 (61778.3872)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1980  max mem: 15572
Epoch: [5]  [ 820/2809]  eta: 0:19:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6502 (4.6587)  class_acc: 0.0833 (0.1153)  loss_scale: 65536.0000 (61824.1559)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1597  max mem: 15572
Epoch: [5]  [ 830/2809]  eta: 0:19:31  lr: 0.000047  min_lr: 0.000000  loss: 4.6667 (4.6591)  class_acc: 0.1250 (0.1154)  loss_scale: 65536.0000 (61868.8231)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.1937  max mem: 15572
[2025-01-15 16:57:12,129] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:57:12,130] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:57:12,599] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14884
[2025-01-15 16:57:12,600] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:57:12,600] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 840/2809]  eta: 0:19:23  lr: 0.000047  min_lr: 0.000000  loss: 4.6501 (4.6589)  class_acc: 0.1250 (0.1154)  loss_scale: 65536.0000 (61990.3543)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1600  max mem: 15572
Epoch: [5]  [ 850/2809]  eta: 0:19:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5577 (4.6583)  class_acc: 0.1250 (0.1152)  loss_scale: 65536.0000 (62032.0188)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0697  max mem: 15572
Epoch: [5]  [ 860/2809]  eta: 0:19:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5089 (4.6582)  class_acc: 0.1250 (0.1155)  loss_scale: 65536.0000 (62072.7154)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0346  max mem: 15572
Epoch: [5]  [ 870/2809]  eta: 0:19:03  lr: 0.000047  min_lr: 0.000000  loss: 4.7377 (4.6592)  class_acc: 0.0833 (0.1151)  loss_scale: 65536.0000 (62112.4776)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0658  max mem: 15572
Epoch: [5]  [ 880/2809]  eta: 0:18:57  lr: 0.000047  min_lr: 0.000000  loss: 4.6458 (4.6577)  class_acc: 0.0833 (0.1152)  loss_scale: 65536.0000 (62151.3371)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1198  max mem: 15572
Epoch: [5]  [ 890/2809]  eta: 0:18:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5335 (4.6574)  class_acc: 0.1250 (0.1155)  loss_scale: 65536.0000 (62189.3244)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1569  max mem: 15572
Epoch: [5]  [ 900/2809]  eta: 0:18:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6309 (4.6573)  class_acc: 0.1250 (0.1154)  loss_scale: 65536.0000 (62226.4684)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1269  max mem: 15572
Epoch: [5]  [ 910/2809]  eta: 0:18:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6383 (4.6577)  class_acc: 0.0833 (0.1157)  loss_scale: 65536.0000 (62262.7969)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0951  max mem: 15572
Epoch: [5]  [ 920/2809]  eta: 0:18:32  lr: 0.000047  min_lr: 0.000000  loss: 4.7046 (4.6580)  class_acc: 0.0833 (0.1156)  loss_scale: 65536.0000 (62298.3366)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.1070  max mem: 15572
Epoch: [5]  [ 930/2809]  eta: 0:18:25  lr: 0.000047  min_lr: 0.000000  loss: 4.6592 (4.6572)  class_acc: 0.0833 (0.1159)  loss_scale: 65536.0000 (62333.1128)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.1003  max mem: 15572
Epoch: [5]  [ 940/2809]  eta: 0:18:18  lr: 0.000047  min_lr: 0.000000  loss: 4.6326 (4.6574)  class_acc: 0.0833 (0.1156)  loss_scale: 65536.0000 (62367.1498)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1133  max mem: 15572
Epoch: [5]  [ 950/2809]  eta: 0:18:12  lr: 0.000047  min_lr: 0.000000  loss: 4.6762 (4.6576)  class_acc: 0.0833 (0.1154)  loss_scale: 65536.0000 (62400.4711)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1270  max mem: 15572
[2025-01-15 16:58:18,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=92, lr=[4.540618353164525e-07, 4.540618353164525e-07, 6.486597647377894e-07, 6.486597647377894e-07, 9.266568067682706e-07, 9.266568067682706e-07, 1.3237954382403866e-06, 1.3237954382403866e-06, 1.8911363403434095e-06, 1.8911363403434095e-06, 2.701623343347728e-06, 2.701623343347728e-06, 3.859461919068183e-06, 3.859461919068183e-06, 5.5135170272402624e-06, 5.5135170272402624e-06, 7.876452896057517e-06, 7.876452896057517e-06, 1.1252075565796455e-05, 1.1252075565796455e-05, 1.6074393665423507e-05, 1.6074393665423507e-05, 2.2963419522033585e-05, 2.2963419522033585e-05, 3.280488503147655e-05, 3.280488503147655e-05, 4.686412147353793e-05, 4.686412147353793e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:58:18,619] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=27.543492364976483, CurrSamplesPerSec=28.032216221394346, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [5]  [ 960/2809]  eta: 0:18:08  lr: 0.000047  min_lr: 0.000000  loss: 4.6568 (4.6574)  class_acc: 0.0833 (0.1153)  loss_scale: 65536.0000 (62433.0989)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1761  max mem: 15572
[2025-01-15 16:58:27,770] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:58:27,770] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 970/2809]  eta: 0:18:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6383 (4.6561)  class_acc: 0.1250 (0.1156)  loss_scale: 65536.0000 (62667.5345)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1692  max mem: 15572
[2025-01-15 16:58:29,726] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15017
[2025-01-15 16:58:29,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:58:29,726] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 980/2809]  eta: 0:17:57  lr: 0.000047  min_lr: 0.000000  loss: 4.6721 (4.6571)  class_acc: 0.0833 (0.1156)  loss_scale: 65536.0000 (62763.5800)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1280  max mem: 15572
Epoch: [5]  [ 990/2809]  eta: 0:17:53  lr: 0.000047  min_lr: 0.000000  loss: 4.7159 (4.6571)  class_acc: 0.0833 (0.1158)  loss_scale: 65536.0000 (62791.5560)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.1812  max mem: 15572
Epoch: [5]  [1000/2809]  eta: 0:17:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6669 (4.6574)  class_acc: 0.1250 (0.1160)  loss_scale: 65536.0000 (62818.9730)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1642  max mem: 15572
Epoch: [5]  [1010/2809]  eta: 0:17:41  lr: 0.000047  min_lr: 0.000000  loss: 4.6612 (4.6572)  class_acc: 0.1250 (0.1161)  loss_scale: 65536.0000 (62845.8477)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1587  max mem: 15572
Epoch: [5]  [1020/2809]  eta: 0:17:35  lr: 0.000047  min_lr: 0.000000  loss: 4.5999 (4.6561)  class_acc: 0.0833 (0.1160)  loss_scale: 65536.0000 (62872.1959)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1810  max mem: 15572
Epoch: [5]  [1030/2809]  eta: 0:17:29  lr: 0.000047  min_lr: 0.000000  loss: 4.5841 (4.6560)  class_acc: 0.0833 (0.1160)  loss_scale: 65536.0000 (62898.0330)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1424  max mem: 15572
[2025-01-15 16:59:07,585] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15080
[2025-01-15 16:59:07,585] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 16:59:07,586] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [1040/2809]  eta: 0:17:24  lr: 0.000047  min_lr: 0.000000  loss: 4.5951 (4.6557)  class_acc: 0.0833 (0.1163)  loss_scale: 65536.0000 (62734.5091)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1451  max mem: 15572
Epoch: [5]  [1050/2809]  eta: 0:17:18  lr: 0.000047  min_lr: 0.000000  loss: 4.6468 (4.6565)  class_acc: 0.0833 (0.1162)  loss_scale: 32768.0000 (62449.3853)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1305  max mem: 15572
Epoch: [5]  [1060/2809]  eta: 0:17:12  lr: 0.000047  min_lr: 0.000000  loss: 4.6468 (4.6555)  class_acc: 0.0833 (0.1163)  loss_scale: 32768.0000 (62169.6362)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0924  max mem: 15572
Epoch: [5]  [1070/2809]  eta: 0:17:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5618 (4.6548)  class_acc: 0.1250 (0.1163)  loss_scale: 32768.0000 (61895.1111)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1079  max mem: 15572
Epoch: [5]  [1080/2809]  eta: 0:16:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5976 (4.6553)  class_acc: 0.0833 (0.1160)  loss_scale: 32768.0000 (61625.6651)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0944  max mem: 15572
Epoch: [5]  [1090/2809]  eta: 0:16:52  lr: 0.000047  min_lr: 0.000000  loss: 4.6882 (4.6554)  class_acc: 0.0833 (0.1156)  loss_scale: 32768.0000 (61361.1586)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0652  max mem: 15572
Epoch: [5]  [1100/2809]  eta: 0:16:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6882 (4.6555)  class_acc: 0.0833 (0.1158)  loss_scale: 32768.0000 (61101.4569)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0693  max mem: 15572
Epoch: [5]  [1110/2809]  eta: 0:16:40  lr: 0.000047  min_lr: 0.000000  loss: 4.6710 (4.6560)  class_acc: 0.1250 (0.1157)  loss_scale: 32768.0000 (60846.4302)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0954  max mem: 15572
Epoch: [5]  [1120/2809]  eta: 0:16:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6710 (4.6562)  class_acc: 0.0833 (0.1159)  loss_scale: 32768.0000 (60595.9536)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1489  max mem: 15572
Epoch: [5]  [1130/2809]  eta: 0:16:29  lr: 0.000047  min_lr: 0.000000  loss: 4.6046 (4.6558)  class_acc: 0.1250 (0.1163)  loss_scale: 32768.0000 (60349.9063)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1980  max mem: 15572
Epoch: [5]  [1140/2809]  eta: 0:16:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5373 (4.6561)  class_acc: 0.0833 (0.1160)  loss_scale: 32768.0000 (60108.1718)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1831  max mem: 15572
Epoch: [5]  [1150/2809]  eta: 0:16:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5980 (4.6556)  class_acc: 0.0833 (0.1164)  loss_scale: 32768.0000 (59870.6377)  weight_decay: 0.0500 (0.0500)  time: 0.6534  data: 0.1880  max mem: 15572
Epoch: [5]  [1160/2809]  eta: 0:16:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6207 (4.6555)  class_acc: 0.1250 (0.1166)  loss_scale: 32768.0000 (59637.1955)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1203  max mem: 15572
[2025-01-15 17:00:24,330] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:00:24,330] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1170/2809]  eta: 0:16:06  lr: 0.000047  min_lr: 0.000000  loss: 4.6304 (4.6553)  class_acc: 0.1250 (0.1167)  loss_scale: 32768.0000 (59603.6208)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0534  max mem: 15572
Epoch: [5]  [1180/2809]  eta: 0:15:59  lr: 0.000047  min_lr: 0.000000  loss: 4.6304 (4.6551)  class_acc: 0.1250 (0.1170)  loss_scale: 65536.0000 (59653.8527)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0976  max mem: 15572
Epoch: [5]  [1190/2809]  eta: 0:15:54  lr: 0.000047  min_lr: 0.000000  loss: 4.5892 (4.6542)  class_acc: 0.1250 (0.1173)  loss_scale: 65536.0000 (59703.2410)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1197  max mem: 15572
Epoch: [5]  [1200/2809]  eta: 0:15:47  lr: 0.000047  min_lr: 0.000000  loss: 4.5890 (4.6550)  class_acc: 0.1250 (0.1174)  loss_scale: 65536.0000 (59751.8068)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1178  max mem: 15572
Epoch: [5]  [1210/2809]  eta: 0:15:41  lr: 0.000047  min_lr: 0.000000  loss: 4.7017 (4.6546)  class_acc: 0.1250 (0.1173)  loss_scale: 65536.0000 (59799.5706)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0926  max mem: 15572
Epoch: [5]  [1220/2809]  eta: 0:15:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6674 (4.6541)  class_acc: 0.0833 (0.1172)  loss_scale: 65536.0000 (59846.5520)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0504  max mem: 15572
Epoch: [5]  [1230/2809]  eta: 0:15:30  lr: 0.000047  min_lr: 0.000000  loss: 4.6259 (4.6540)  class_acc: 0.0833 (0.1170)  loss_scale: 65536.0000 (59892.7701)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1178  max mem: 15572
Epoch: [5]  [1240/2809]  eta: 0:15:23  lr: 0.000047  min_lr: 0.000000  loss: 4.6310 (4.6536)  class_acc: 0.1250 (0.1171)  loss_scale: 65536.0000 (59938.2434)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1589  max mem: 15572
Epoch: [5]  [1250/2809]  eta: 0:15:18  lr: 0.000047  min_lr: 0.000000  loss: 4.6745 (4.6543)  class_acc: 0.1250 (0.1171)  loss_scale: 65536.0000 (59982.9896)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1212  max mem: 15572
Epoch: [5]  [1260/2809]  eta: 0:15:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6389 (4.6541)  class_acc: 0.0833 (0.1171)  loss_scale: 65536.0000 (60027.0262)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1251  max mem: 15572
Epoch: [5]  [1270/2809]  eta: 0:15:06  lr: 0.000047  min_lr: 0.000000  loss: 4.6180 (4.6546)  class_acc: 0.1250 (0.1173)  loss_scale: 65536.0000 (60070.3698)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1082  max mem: 15572
Epoch: [5]  [1280/2809]  eta: 0:15:01  lr: 0.000047  min_lr: 0.000000  loss: 4.5945 (4.6532)  class_acc: 0.1250 (0.1174)  loss_scale: 65536.0000 (60113.0367)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1701  max mem: 15572
Epoch: [5]  [1290/2809]  eta: 0:14:54  lr: 0.000047  min_lr: 0.000000  loss: 4.6803 (4.6536)  class_acc: 0.1250 (0.1175)  loss_scale: 65536.0000 (60155.0426)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1608  max mem: 15572
[2025-01-15 17:01:39,042] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:01:39,042] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:01:42,108] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15344
[2025-01-15 17:01:42,108] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:01:42,108] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1300/2809]  eta: 0:14:47  lr: 0.000047  min_lr: 0.000000  loss: 4.6269 (4.6532)  class_acc: 0.1250 (0.1173)  loss_scale: 65536.0000 (60549.0177)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0810  max mem: 15572
Epoch: [5]  [1310/2809]  eta: 0:14:41  lr: 0.000047  min_lr: 0.000000  loss: 4.6022 (4.6533)  class_acc: 0.0833 (0.1174)  loss_scale: 65536.0000 (60587.0572)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0790  max mem: 15572
Epoch: [5]  [1320/2809]  eta: 0:14:35  lr: 0.000047  min_lr: 0.000000  loss: 4.6841 (4.6534)  class_acc: 0.1250 (0.1176)  loss_scale: 65536.0000 (60624.5208)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1156  max mem: 15572
Epoch: [5]  [1330/2809]  eta: 0:14:30  lr: 0.000047  min_lr: 0.000000  loss: 4.6981 (4.6538)  class_acc: 0.0833 (0.1173)  loss_scale: 65536.0000 (60661.4215)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1279  max mem: 15572
Epoch: [5]  [1340/2809]  eta: 0:14:24  lr: 0.000047  min_lr: 0.000000  loss: 4.6981 (4.6538)  class_acc: 0.1250 (0.1177)  loss_scale: 65536.0000 (60697.7718)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1195  max mem: 15572
Epoch: [5]  [1350/2809]  eta: 0:14:18  lr: 0.000047  min_lr: 0.000000  loss: 4.6184 (4.6532)  class_acc: 0.1250 (0.1177)  loss_scale: 65536.0000 (60733.5840)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1008  max mem: 15572
Epoch: [5]  [1360/2809]  eta: 0:14:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5748 (4.6525)  class_acc: 0.1250 (0.1180)  loss_scale: 65536.0000 (60768.8699)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1391  max mem: 15572
Epoch: [5]  [1370/2809]  eta: 0:14:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5511 (4.6525)  class_acc: 0.1250 (0.1178)  loss_scale: 65536.0000 (60803.6411)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1456  max mem: 15572
Epoch: [5]  [1380/2809]  eta: 0:13:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5511 (4.6518)  class_acc: 0.1250 (0.1182)  loss_scale: 65536.0000 (60837.9088)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0674  max mem: 15572
Epoch: [5]  [1390/2809]  eta: 0:13:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5536 (4.6507)  class_acc: 0.1250 (0.1184)  loss_scale: 65536.0000 (60871.6837)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0791  max mem: 15572
Epoch: [5]  [1400/2809]  eta: 0:13:48  lr: 0.000047  min_lr: 0.000000  loss: 4.6064 (4.6507)  class_acc: 0.1250 (0.1182)  loss_scale: 65536.0000 (60904.9764)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1543  max mem: 15572
Epoch: [5]  [1410/2809]  eta: 0:13:41  lr: 0.000047  min_lr: 0.000000  loss: 4.6064 (4.6506)  class_acc: 0.1250 (0.1184)  loss_scale: 65536.0000 (60937.7973)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1527  max mem: 15572
Epoch: [5]  [1420/2809]  eta: 0:13:36  lr: 0.000047  min_lr: 0.000000  loss: 4.6241 (4.6512)  class_acc: 0.1250 (0.1184)  loss_scale: 65536.0000 (60970.1562)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1312  max mem: 15572
[2025-01-15 17:02:57,368] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:02:57,369] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1430/2809]  eta: 0:13:30  lr: 0.000047  min_lr: 0.000000  loss: 4.6241 (4.6511)  class_acc: 0.1250 (0.1186)  loss_scale: 65536.0000 (61139.4549)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1629  max mem: 15572
[2025-01-15 17:02:59,222] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15477
[2025-01-15 17:02:59,223] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:02:59,223] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1440/2809]  eta: 0:13:24  lr: 0.000047  min_lr: 0.000000  loss: 4.5965 (4.6505)  class_acc: 0.1250 (0.1188)  loss_scale: 65536.0000 (61215.4448)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1246  max mem: 15572
Epoch: [5]  [1450/2809]  eta: 0:13:18  lr: 0.000047  min_lr: 0.000000  loss: 4.6762 (4.6515)  class_acc: 0.1250 (0.1189)  loss_scale: 65536.0000 (61245.2212)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0964  max mem: 15572
Epoch: [5]  [1460/2809]  eta: 0:13:12  lr: 0.000047  min_lr: 0.000000  loss: 4.6349 (4.6509)  class_acc: 0.1250 (0.1190)  loss_scale: 65536.0000 (61274.5900)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1271  max mem: 15572
Epoch: [5]  [1470/2809]  eta: 0:13:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5869 (4.6509)  class_acc: 0.1250 (0.1189)  loss_scale: 65536.0000 (61303.5595)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0925  max mem: 15572
Epoch: [5]  [1480/2809]  eta: 0:13:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6204 (4.6507)  class_acc: 0.0833 (0.1187)  loss_scale: 65536.0000 (61332.1377)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0776  max mem: 15572
Epoch: [5]  [1490/2809]  eta: 0:12:54  lr: 0.000047  min_lr: 0.000000  loss: 4.6179 (4.6507)  class_acc: 0.0833 (0.1188)  loss_scale: 65536.0000 (61360.3327)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1414  max mem: 15572
Epoch: [5]  [1500/2809]  eta: 0:12:48  lr: 0.000047  min_lr: 0.000000  loss: 4.5959 (4.6505)  class_acc: 0.1250 (0.1188)  loss_scale: 65536.0000 (61388.1519)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1710  max mem: 15572
Epoch: [5]  [1510/2809]  eta: 0:12:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5959 (4.6508)  class_acc: 0.0833 (0.1189)  loss_scale: 65536.0000 (61415.6029)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1436  max mem: 15572
Epoch: [5]  [1520/2809]  eta: 0:12:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5750 (4.6498)  class_acc: 0.0833 (0.1189)  loss_scale: 65536.0000 (61442.6930)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0916  max mem: 15572
Epoch: [5]  [1530/2809]  eta: 0:12:30  lr: 0.000047  min_lr: 0.000000  loss: 4.5750 (4.6496)  class_acc: 0.1250 (0.1191)  loss_scale: 65536.0000 (61469.4291)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.0956  max mem: 15572
Epoch: [5]  [1540/2809]  eta: 0:12:24  lr: 0.000047  min_lr: 0.000000  loss: 4.6460 (4.6493)  class_acc: 0.1250 (0.1193)  loss_scale: 65536.0000 (61495.8183)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0976  max mem: 15572
Epoch: [5]  [1550/2809]  eta: 0:12:18  lr: 0.000047  min_lr: 0.000000  loss: 4.5792 (4.6490)  class_acc: 0.1250 (0.1198)  loss_scale: 65536.0000 (61521.8672)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0498  max mem: 15572
Epoch: [5]  [1560/2809]  eta: 0:12:12  lr: 0.000047  min_lr: 0.000000  loss: 4.6354 (4.6490)  class_acc: 0.1250 (0.1198)  loss_scale: 65536.0000 (61547.5823)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1083  max mem: 15572
[2025-01-15 17:04:13,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:04:13,122] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:04:13,971] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15608
[2025-01-15 17:04:13,972] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:04:13,972] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1570/2809]  eta: 0:12:06  lr: 0.000047  min_lr: 0.000000  loss: 4.6755 (4.6493)  class_acc: 0.1250 (0.1200)  loss_scale: 65536.0000 (61656.4023)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1422  max mem: 15572
Epoch: [5]  [1580/2809]  eta: 0:12:01  lr: 0.000047  min_lr: 0.000000  loss: 4.6788 (4.6499)  class_acc: 0.1250 (0.1201)  loss_scale: 65536.0000 (61680.9412)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1744  max mem: 15572
Epoch: [5]  [1590/2809]  eta: 0:11:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6188 (4.6493)  class_acc: 0.1667 (0.1204)  loss_scale: 65536.0000 (61705.1716)  weight_decay: 0.0500 (0.0500)  time: 0.6650  data: 0.1922  max mem: 15572
Epoch: [5]  [1600/2809]  eta: 0:11:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6065 (4.6489)  class_acc: 0.1667 (0.1209)  loss_scale: 65536.0000 (61729.0993)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1323  max mem: 15572
Epoch: [5]  [1610/2809]  eta: 0:11:44  lr: 0.000047  min_lr: 0.000000  loss: 4.6678 (4.6493)  class_acc: 0.1667 (0.1210)  loss_scale: 65536.0000 (61752.7300)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1367  max mem: 15572
Epoch: [5]  [1620/2809]  eta: 0:11:38  lr: 0.000047  min_lr: 0.000000  loss: 4.6678 (4.6491)  class_acc: 0.1667 (0.1212)  loss_scale: 65536.0000 (61776.0691)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1275  max mem: 15572
Epoch: [5]  [1630/2809]  eta: 0:11:32  lr: 0.000047  min_lr: 0.000000  loss: 4.6398 (4.6493)  class_acc: 0.1250 (0.1212)  loss_scale: 65536.0000 (61799.1220)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0909  max mem: 15572
Epoch: [5]  [1640/2809]  eta: 0:11:25  lr: 0.000047  min_lr: 0.000000  loss: 4.5126 (4.6479)  class_acc: 0.1250 (0.1214)  loss_scale: 65536.0000 (61821.8940)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0579  max mem: 15572
Epoch: [5]  [1650/2809]  eta: 0:11:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5443 (4.6473)  class_acc: 0.1250 (0.1216)  loss_scale: 65536.0000 (61844.3901)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0567  max mem: 15572
Epoch: [5]  [1660/2809]  eta: 0:11:13  lr: 0.000047  min_lr: 0.000000  loss: 4.6234 (4.6475)  class_acc: 0.1250 (0.1216)  loss_scale: 65536.0000 (61866.6153)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1152  max mem: 15572
Epoch: [5]  [1670/2809]  eta: 0:11:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5934 (4.6473)  class_acc: 0.0833 (0.1217)  loss_scale: 65536.0000 (61888.5745)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1834  max mem: 15572
Epoch: [5]  [1680/2809]  eta: 0:11:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5877 (4.6462)  class_acc: 0.0833 (0.1219)  loss_scale: 65536.0000 (61910.2725)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1785  max mem: 15572
Epoch: [5]  [1690/2809]  eta: 0:10:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6212 (4.6463)  class_acc: 0.1667 (0.1221)  loss_scale: 65536.0000 (61931.7138)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0956  max mem: 15572
[2025-01-15 17:05:30,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:05:30,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1700/2809]  eta: 0:10:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6507 (4.6461)  class_acc: 0.1250 (0.1221)  loss_scale: 65536.0000 (62299.6543)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0647  max mem: 15572
[2025-01-15 17:05:35,251] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15746
[2025-01-15 17:05:35,252] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:05:35,252] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1710/2809]  eta: 0:10:44  lr: 0.000047  min_lr: 0.000000  loss: 4.5848 (4.6461)  class_acc: 0.0833 (0.1220)  loss_scale: 65536.0000 (62318.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0872  max mem: 15572
Epoch: [5]  [1720/2809]  eta: 0:10:38  lr: 0.000047  min_lr: 0.000000  loss: 4.5964 (4.6457)  class_acc: 0.1250 (0.1223)  loss_scale: 65536.0000 (62337.2644)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1242  max mem: 15572
Epoch: [5]  [1730/2809]  eta: 0:10:34  lr: 0.000047  min_lr: 0.000000  loss: 4.5964 (4.6455)  class_acc: 0.0833 (0.1223)  loss_scale: 65536.0000 (62355.7435)  weight_decay: 0.0500 (0.0500)  time: 0.7045  data: 0.2121  max mem: 15572
Epoch: [5]  [1740/2809]  eta: 0:10:27  lr: 0.000047  min_lr: 0.000000  loss: 4.6608 (4.6453)  class_acc: 0.0833 (0.1225)  loss_scale: 65536.0000 (62374.0103)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1510  max mem: 15572
Epoch: [5]  [1750/2809]  eta: 0:10:21  lr: 0.000047  min_lr: 0.000000  loss: 4.6733 (4.6455)  class_acc: 0.1250 (0.1224)  loss_scale: 65536.0000 (62392.0685)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0529  max mem: 15572
Epoch: [5]  [1760/2809]  eta: 0:10:15  lr: 0.000047  min_lr: 0.000000  loss: 4.6250 (4.6455)  class_acc: 0.0833 (0.1222)  loss_scale: 65536.0000 (62409.9216)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0670  max mem: 15572
Epoch: [5]  [1770/2809]  eta: 0:10:09  lr: 0.000047  min_lr: 0.000000  loss: 4.6390 (4.6458)  class_acc: 0.0833 (0.1222)  loss_scale: 65536.0000 (62427.5731)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0874  max mem: 15572
Epoch: [5]  [1780/2809]  eta: 0:10:03  lr: 0.000047  min_lr: 0.000000  loss: 4.6091 (4.6459)  class_acc: 0.0833 (0.1221)  loss_scale: 65536.0000 (62445.0264)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1265  max mem: 15572
Epoch: [5]  [1790/2809]  eta: 0:09:58  lr: 0.000047  min_lr: 0.000000  loss: 4.6306 (4.6464)  class_acc: 0.0833 (0.1220)  loss_scale: 65536.0000 (62462.2848)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1584  max mem: 15572
Epoch: [5]  [1800/2809]  eta: 0:09:51  lr: 0.000047  min_lr: 0.000000  loss: 4.6848 (4.6466)  class_acc: 0.0417 (0.1219)  loss_scale: 65536.0000 (62479.3515)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1380  max mem: 15572
Epoch: [5]  [1810/2809]  eta: 0:09:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6308 (4.6461)  class_acc: 0.0833 (0.1220)  loss_scale: 65536.0000 (62496.2297)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1386  max mem: 15572
Epoch: [5]  [1820/2809]  eta: 0:09:40  lr: 0.000047  min_lr: 0.000000  loss: 4.5730 (4.6461)  class_acc: 0.1250 (0.1221)  loss_scale: 65536.0000 (62512.9226)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.1852  max mem: 15572
[2025-01-15 17:06:53,653] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:06:53,653] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1830/2809]  eta: 0:09:35  lr: 0.000047  min_lr: 0.000000  loss: 4.6343 (4.6458)  class_acc: 0.1667 (0.1222)  loss_scale: 65536.0000 (62565.2256)  weight_decay: 0.0500 (0.0500)  time: 0.6763  data: 0.2108  max mem: 15572
Epoch: [5]  [1840/2809]  eta: 0:09:29  lr: 0.000047  min_lr: 0.000000  loss: 4.5824 (4.6457)  class_acc: 0.1250 (0.1222)  loss_scale: 131072.0000 (62937.3427)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1830  max mem: 15572
[2025-01-15 17:07:01,119] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15888
[2025-01-15 17:07:01,120] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:07:01,120] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1850/2809]  eta: 0:09:23  lr: 0.000047  min_lr: 0.000000  loss: 4.6550 (4.6458)  class_acc: 0.1250 (0.1222)  loss_scale: 131072.0000 (63022.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1333  max mem: 15572
Epoch: [5]  [1860/2809]  eta: 0:09:18  lr: 0.000047  min_lr: 0.000000  loss: 4.6550 (4.6459)  class_acc: 0.1250 (0.1220)  loss_scale: 65536.0000 (63035.7012)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.1453  max mem: 15572
Epoch: [5]  [1870/2809]  eta: 0:09:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5672 (4.6455)  class_acc: 0.1250 (0.1222)  loss_scale: 65536.0000 (63049.0647)  weight_decay: 0.0500 (0.0500)  time: 0.6468  data: 0.1682  max mem: 15572
Epoch: [5]  [1880/2809]  eta: 0:09:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5448 (4.6454)  class_acc: 0.1667 (0.1223)  loss_scale: 65536.0000 (63062.2860)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1513  max mem: 15572
Epoch: [5]  [1890/2809]  eta: 0:09:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6184 (4.6454)  class_acc: 0.1250 (0.1224)  loss_scale: 65536.0000 (63075.3675)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1380  max mem: 15572
Epoch: [5]  [1900/2809]  eta: 0:08:54  lr: 0.000047  min_lr: 0.000000  loss: 4.6184 (4.6454)  class_acc: 0.1250 (0.1224)  loss_scale: 65536.0000 (63088.3114)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1087  max mem: 15572
Epoch: [5]  [1910/2809]  eta: 0:08:49  lr: 0.000047  min_lr: 0.000000  loss: 4.6268 (4.6452)  class_acc: 0.1250 (0.1226)  loss_scale: 65536.0000 (63101.1198)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1597  max mem: 15572
Epoch: [5]  [1920/2809]  eta: 0:08:43  lr: 0.000047  min_lr: 0.000000  loss: 4.6308 (4.6451)  class_acc: 0.1250 (0.1226)  loss_scale: 65536.0000 (63113.7949)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1659  max mem: 15572
Epoch: [5]  [1930/2809]  eta: 0:08:37  lr: 0.000047  min_lr: 0.000000  loss: 4.6998 (4.6454)  class_acc: 0.1250 (0.1227)  loss_scale: 65536.0000 (63126.3387)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1138  max mem: 15572
Epoch: [5]  [1940/2809]  eta: 0:08:31  lr: 0.000047  min_lr: 0.000000  loss: 4.6936 (4.6449)  class_acc: 0.1250 (0.1229)  loss_scale: 65536.0000 (63138.7532)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0767  max mem: 15572
Epoch: [5]  [1950/2809]  eta: 0:08:25  lr: 0.000047  min_lr: 0.000000  loss: 4.5113 (4.6446)  class_acc: 0.1250 (0.1228)  loss_scale: 65536.0000 (63151.0405)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0133  max mem: 15572
[2025-01-15 17:08:06,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=99, lr=[4.537251678590611e-07, 4.537251678590611e-07, 6.481788112272302e-07, 6.481788112272302e-07, 9.259697303246147e-07, 9.259697303246147e-07, 1.3228139004637354e-06, 1.3228139004637354e-06, 1.889734143519622e-06, 1.889734143519622e-06, 2.6996202050280316e-06, 2.6996202050280316e-06, 3.856600292897189e-06, 3.856600292897189e-06, 5.509428989853127e-06, 5.509428989853127e-06, 7.870612842647324e-06, 7.870612842647324e-06, 1.124373263235332e-05, 1.124373263235332e-05, 1.606247518907617e-05, 1.606247518907617e-05, 2.2946393127251676e-05, 2.2946393127251676e-05, 3.2780561610359544e-05, 3.2780561610359544e-05, 4.682937372908506e-05, 4.682937372908506e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 17:08:06,566] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=27.533815041840764, CurrSamplesPerSec=30.376778006315362, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [5]  [1960/2809]  eta: 0:08:18  lr: 0.000047  min_lr: 0.000000  loss: 4.5955 (4.6447)  class_acc: 0.0833 (0.1228)  loss_scale: 65536.0000 (63163.2024)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0400  max mem: 15572
Epoch: [5]  [1970/2809]  eta: 0:08:13  lr: 0.000047  min_lr: 0.000000  loss: 4.6626 (4.6449)  class_acc: 0.0833 (0.1229)  loss_scale: 65536.0000 (63175.2410)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0909  max mem: 15572
[2025-01-15 17:08:17,200] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:08:17,201] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:08:19,464] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16021
[2025-01-15 17:08:19,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:08:19,465] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1980/2809]  eta: 0:08:07  lr: 0.000047  min_lr: 0.000000  loss: 4.7244 (4.6455)  class_acc: 0.1250 (0.1230)  loss_scale: 65536.0000 (63319.4871)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1099  max mem: 15572
Epoch: [5]  [1990/2809]  eta: 0:08:01  lr: 0.000047  min_lr: 0.000000  loss: 4.7318 (4.6458)  class_acc: 0.1250 (0.1230)  loss_scale: 65536.0000 (63330.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0466  max mem: 15572
Epoch: [5]  [2000/2809]  eta: 0:07:55  lr: 0.000047  min_lr: 0.000000  loss: 4.6702 (4.6460)  class_acc: 0.1250 (0.1230)  loss_scale: 65536.0000 (63341.6412)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0009  max mem: 15572
Epoch: [5]  [2010/2809]  eta: 0:07:48  lr: 0.000047  min_lr: 0.000000  loss: 4.6302 (4.6456)  class_acc: 0.1250 (0.1231)  loss_scale: 65536.0000 (63352.5530)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0177  max mem: 15572
Epoch: [5]  [2020/2809]  eta: 0:07:42  lr: 0.000047  min_lr: 0.000000  loss: 4.6141 (4.6454)  class_acc: 0.1250 (0.1232)  loss_scale: 65536.0000 (63363.3568)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0309  max mem: 15572
Epoch: [5]  [2030/2809]  eta: 0:07:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5735 (4.6450)  class_acc: 0.1250 (0.1232)  loss_scale: 65536.0000 (63374.0542)  weight_decay: 0.0500 (0.0500)  time: 0.5225  data: 0.0693  max mem: 15572
Epoch: [5]  [2040/2809]  eta: 0:07:31  lr: 0.000047  min_lr: 0.000000  loss: 4.5536 (4.6450)  class_acc: 0.0833 (0.1231)  loss_scale: 65536.0000 (63384.6467)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1545  max mem: 15572
Epoch: [5]  [2050/2809]  eta: 0:07:25  lr: 0.000047  min_lr: 0.000000  loss: 4.6014 (4.6448)  class_acc: 0.1250 (0.1231)  loss_scale: 65536.0000 (63395.1360)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1710  max mem: 15572
Epoch: [5]  [2060/2809]  eta: 0:07:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5885 (4.6444)  class_acc: 0.1250 (0.1234)  loss_scale: 65536.0000 (63405.5235)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1951  max mem: 15572
Epoch: [5]  [2070/2809]  eta: 0:07:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5438 (4.6438)  class_acc: 0.1667 (0.1234)  loss_scale: 65536.0000 (63415.8107)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.2012  max mem: 15572
Epoch: [5]  [2080/2809]  eta: 0:07:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5712 (4.6441)  class_acc: 0.1250 (0.1234)  loss_scale: 65536.0000 (63425.9990)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1386  max mem: 15572
Epoch: [5]  [2090/2809]  eta: 0:07:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6496 (4.6441)  class_acc: 0.0833 (0.1233)  loss_scale: 65536.0000 (63436.0899)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1305  max mem: 15572
Epoch: [5]  [2100/2809]  eta: 0:06:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6822 (4.6443)  class_acc: 0.0833 (0.1233)  loss_scale: 65536.0000 (63446.0847)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0893  max mem: 15572
[2025-01-15 17:09:34,419] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:09:34,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [2110/2809]  eta: 0:06:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6889 (4.6445)  class_acc: 0.0833 (0.1232)  loss_scale: 65536.0000 (63642.2549)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0931  max mem: 15572
[2025-01-15 17:09:38,132] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16157
[2025-01-15 17:09:38,132] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:09:38,133] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2120/2809]  eta: 0:06:44  lr: 0.000047  min_lr: 0.000000  loss: 4.6333 (4.6448)  class_acc: 0.1250 (0.1234)  loss_scale: 65536.0000 (63682.0820)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1418  max mem: 15572
Epoch: [5]  [2130/2809]  eta: 0:06:38  lr: 0.000047  min_lr: 0.000000  loss: 4.6154 (4.6441)  class_acc: 0.1250 (0.1234)  loss_scale: 65536.0000 (63690.7818)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0808  max mem: 15572
Epoch: [5]  [2140/2809]  eta: 0:06:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5368 (4.6435)  class_acc: 0.1250 (0.1237)  loss_scale: 65536.0000 (63699.4003)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0264  max mem: 15572
Epoch: [5]  [2150/2809]  eta: 0:06:26  lr: 0.000047  min_lr: 0.000000  loss: 4.5883 (4.6435)  class_acc: 0.1250 (0.1236)  loss_scale: 65536.0000 (63707.9386)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0737  max mem: 15572
Epoch: [5]  [2160/2809]  eta: 0:06:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6960 (4.6437)  class_acc: 0.1250 (0.1236)  loss_scale: 65536.0000 (63716.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0790  max mem: 15572
Epoch: [5]  [2170/2809]  eta: 0:06:14  lr: 0.000047  min_lr: 0.000000  loss: 4.6960 (4.6442)  class_acc: 0.1250 (0.1236)  loss_scale: 65536.0000 (63724.7794)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1236  max mem: 15572
Epoch: [5]  [2180/2809]  eta: 0:06:08  lr: 0.000047  min_lr: 0.000000  loss: 4.7428 (4.6447)  class_acc: 0.1250 (0.1235)  loss_scale: 65536.0000 (63733.0839)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1613  max mem: 15572
Epoch: [5]  [2190/2809]  eta: 0:06:02  lr: 0.000047  min_lr: 0.000000  loss: 4.7160 (4.6451)  class_acc: 0.1250 (0.1235)  loss_scale: 65536.0000 (63741.3126)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1169  max mem: 15572
Epoch: [5]  [2200/2809]  eta: 0:05:57  lr: 0.000047  min_lr: 0.000000  loss: 4.7096 (4.6450)  class_acc: 0.1250 (0.1236)  loss_scale: 65536.0000 (63749.4666)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1212  max mem: 15572
Epoch: [5]  [2210/2809]  eta: 0:05:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6007 (4.6448)  class_acc: 0.1250 (0.1237)  loss_scale: 65536.0000 (63757.5468)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0693  max mem: 15572
Epoch: [5]  [2220/2809]  eta: 0:05:44  lr: 0.000047  min_lr: 0.000000  loss: 4.5955 (4.6450)  class_acc: 0.1250 (0.1238)  loss_scale: 65536.0000 (63765.5543)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0550  max mem: 15572
Epoch: [5]  [2230/2809]  eta: 0:05:39  lr: 0.000047  min_lr: 0.000000  loss: 4.5385 (4.6445)  class_acc: 0.1250 (0.1240)  loss_scale: 65536.0000 (63773.4899)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1171  max mem: 15572
Epoch: [5]  [2240/2809]  eta: 0:05:33  lr: 0.000047  min_lr: 0.000000  loss: 4.5849 (4.6450)  class_acc: 0.1250 (0.1241)  loss_scale: 65536.0000 (63781.3548)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.1545  max mem: 15572
[2025-01-15 17:10:51,344] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:10:51,345] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:10:56,450] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16293
[2025-01-15 17:10:56,450] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:10:56,450] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2250/2809]  eta: 0:05:27  lr: 0.000047  min_lr: 0.000000  loss: 4.6980 (4.6452)  class_acc: 0.1250 (0.1241)  loss_scale: 65536.0000 (63992.9489)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.1683  max mem: 15572
Epoch: [5]  [2260/2809]  eta: 0:05:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5608 (4.6448)  class_acc: 0.1250 (0.1243)  loss_scale: 65536.0000 (63999.7736)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1454  max mem: 15572
Epoch: [5]  [2270/2809]  eta: 0:05:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5647 (4.6449)  class_acc: 0.1250 (0.1242)  loss_scale: 65536.0000 (64006.5381)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1564  max mem: 15572
Epoch: [5]  [2280/2809]  eta: 0:05:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5577 (4.6446)  class_acc: 0.1250 (0.1243)  loss_scale: 65536.0000 (64013.2433)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.1606  max mem: 15572
Epoch: [5]  [2290/2809]  eta: 0:05:04  lr: 0.000047  min_lr: 0.000000  loss: 4.5743 (4.6449)  class_acc: 0.1250 (0.1244)  loss_scale: 65536.0000 (64019.8900)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1556  max mem: 15572
Epoch: [5]  [2300/2809]  eta: 0:04:58  lr: 0.000047  min_lr: 0.000000  loss: 4.6254 (4.6447)  class_acc: 0.0833 (0.1242)  loss_scale: 65536.0000 (64026.4789)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1388  max mem: 15572
Epoch: [5]  [2310/2809]  eta: 0:04:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5777 (4.6444)  class_acc: 0.0833 (0.1242)  loss_scale: 65536.0000 (64033.0108)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0923  max mem: 15572
Epoch: [5]  [2320/2809]  eta: 0:04:47  lr: 0.000047  min_lr: 0.000000  loss: 4.5948 (4.6439)  class_acc: 0.0833 (0.1240)  loss_scale: 65536.0000 (64039.4864)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1680  max mem: 15572
Epoch: [5]  [2330/2809]  eta: 0:04:41  lr: 0.000047  min_lr: 0.000000  loss: 4.6134 (4.6436)  class_acc: 0.0833 (0.1240)  loss_scale: 65536.0000 (64045.9065)  weight_decay: 0.0500 (0.0500)  time: 0.6649  data: 0.1936  max mem: 15572
Epoch: [5]  [2340/2809]  eta: 0:04:35  lr: 0.000047  min_lr: 0.000000  loss: 4.5583 (4.6435)  class_acc: 0.1667 (0.1244)  loss_scale: 65536.0000 (64052.2717)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1168  max mem: 15572
Epoch: [5]  [2350/2809]  eta: 0:04:29  lr: 0.000047  min_lr: 0.000000  loss: 4.5569 (4.6434)  class_acc: 0.1667 (0.1243)  loss_scale: 65536.0000 (64058.5827)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1131  max mem: 15572
Epoch: [5]  [2360/2809]  eta: 0:04:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5569 (4.6434)  class_acc: 0.1250 (0.1245)  loss_scale: 65536.0000 (64064.8403)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1304  max mem: 15572
Epoch: [5]  [2370/2809]  eta: 0:04:17  lr: 0.000047  min_lr: 0.000000  loss: 4.6073 (4.6430)  class_acc: 0.1667 (0.1246)  loss_scale: 65536.0000 (64071.0451)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0841  max mem: 15572
[2025-01-15 17:12:14,999] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:12:14,999] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:12:15,500] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16423
[2025-01-15 17:12:15,501] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:12:15,502] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2380/2809]  eta: 0:04:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5834 (4.6425)  class_acc: 0.1250 (0.1248)  loss_scale: 65536.0000 (64104.7224)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0993  max mem: 15572
Epoch: [5]  [2390/2809]  eta: 0:04:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5860 (4.6425)  class_acc: 0.0833 (0.1247)  loss_scale: 65536.0000 (64110.7085)  weight_decay: 0.0500 (0.0500)  time: 0.6520  data: 0.1808  max mem: 15572
Epoch: [5]  [2400/2809]  eta: 0:04:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6246 (4.6426)  class_acc: 0.0833 (0.1246)  loss_scale: 65536.0000 (64116.6447)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1097  max mem: 15572
Epoch: [5]  [2410/2809]  eta: 0:03:54  lr: 0.000047  min_lr: 0.000000  loss: 4.6456 (4.6424)  class_acc: 0.1250 (0.1247)  loss_scale: 65536.0000 (64122.5317)  weight_decay: 0.0500 (0.0500)  time: 0.5093  data: 0.0363  max mem: 15572
Epoch: [5]  [2420/2809]  eta: 0:03:48  lr: 0.000047  min_lr: 0.000000  loss: 4.5716 (4.6414)  class_acc: 0.1250 (0.1247)  loss_scale: 65536.0000 (64128.3701)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1046  max mem: 15572
Epoch: [5]  [2430/2809]  eta: 0:03:42  lr: 0.000047  min_lr: 0.000000  loss: 4.6462 (4.6419)  class_acc: 0.1250 (0.1247)  loss_scale: 65536.0000 (64134.1604)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1738  max mem: 15572
Epoch: [5]  [2440/2809]  eta: 0:03:36  lr: 0.000047  min_lr: 0.000000  loss: 4.7380 (4.6420)  class_acc: 0.1667 (0.1249)  loss_scale: 65536.0000 (64139.9033)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1361  max mem: 15572
Epoch: [5]  [2450/2809]  eta: 0:03:30  lr: 0.000047  min_lr: 0.000000  loss: 4.5328 (4.6416)  class_acc: 0.1667 (0.1251)  loss_scale: 65536.0000 (64145.5993)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1470  max mem: 15572
Epoch: [5]  [2460/2809]  eta: 0:03:25  lr: 0.000047  min_lr: 0.000000  loss: 4.5328 (4.6421)  class_acc: 0.1250 (0.1249)  loss_scale: 65536.0000 (64151.2491)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1856  max mem: 15572
Epoch: [5]  [2470/2809]  eta: 0:03:19  lr: 0.000047  min_lr: 0.000000  loss: 4.7116 (4.6421)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (64156.8531)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1651  max mem: 15572
Epoch: [5]  [2480/2809]  eta: 0:03:13  lr: 0.000047  min_lr: 0.000000  loss: 4.6091 (4.6422)  class_acc: 0.1250 (0.1252)  loss_scale: 65536.0000 (64162.4119)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1164  max mem: 15572
Epoch: [5]  [2490/2809]  eta: 0:03:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5515 (4.6416)  class_acc: 0.1250 (0.1254)  loss_scale: 65536.0000 (64167.9261)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1672  max mem: 15572
Epoch: [5]  [2500/2809]  eta: 0:03:01  lr: 0.000047  min_lr: 0.000000  loss: 4.5074 (4.6417)  class_acc: 0.1250 (0.1253)  loss_scale: 65536.0000 (64173.3962)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1484  max mem: 15572
[2025-01-15 17:13:31,648] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:13:31,648] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:13:32,678] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16554
[2025-01-15 17:13:32,679] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:13:32,679] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2510/2809]  eta: 0:02:55  lr: 0.000047  min_lr: 0.000000  loss: 4.5943 (4.6415)  class_acc: 0.1250 (0.1255)  loss_scale: 65536.0000 (64231.0219)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0542  max mem: 15572
Epoch: [5]  [2520/2809]  eta: 0:02:49  lr: 0.000047  min_lr: 0.000000  loss: 4.5943 (4.6412)  class_acc: 0.1667 (0.1256)  loss_scale: 65536.0000 (64236.1983)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0740  max mem: 15572
Epoch: [5]  [2530/2809]  eta: 0:02:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5904 (4.6410)  class_acc: 0.1667 (0.1257)  loss_scale: 65536.0000 (64241.3339)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0861  max mem: 15572
Epoch: [5]  [2540/2809]  eta: 0:02:38  lr: 0.000047  min_lr: 0.000000  loss: 4.6597 (4.6412)  class_acc: 0.1250 (0.1258)  loss_scale: 65536.0000 (64246.4290)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1288  max mem: 15572
Epoch: [5]  [2550/2809]  eta: 0:02:32  lr: 0.000047  min_lr: 0.000000  loss: 4.6753 (4.6412)  class_acc: 0.1250 (0.1257)  loss_scale: 65536.0000 (64251.4841)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1330  max mem: 15572
Epoch: [5]  [2560/2809]  eta: 0:02:26  lr: 0.000047  min_lr: 0.000000  loss: 4.5823 (4.6409)  class_acc: 0.1250 (0.1256)  loss_scale: 65536.0000 (64256.4998)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1059  max mem: 15572
Epoch: [5]  [2570/2809]  eta: 0:02:20  lr: 0.000047  min_lr: 0.000000  loss: 4.5836 (4.6409)  class_acc: 0.1250 (0.1256)  loss_scale: 65536.0000 (64261.4765)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1059  max mem: 15572
Epoch: [5]  [2580/2809]  eta: 0:02:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5836 (4.6409)  class_acc: 0.1250 (0.1256)  loss_scale: 65536.0000 (64266.4146)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1417  max mem: 15572
Epoch: [5]  [2590/2809]  eta: 0:02:08  lr: 0.000047  min_lr: 0.000000  loss: 4.6460 (4.6411)  class_acc: 0.1250 (0.1256)  loss_scale: 65536.0000 (64271.3146)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1345  max mem: 15572
Epoch: [5]  [2600/2809]  eta: 0:02:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6431 (4.6412)  class_acc: 0.1250 (0.1256)  loss_scale: 65536.0000 (64276.1769)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1267  max mem: 15572
Epoch: [5]  [2610/2809]  eta: 0:01:56  lr: 0.000047  min_lr: 0.000000  loss: 4.5496 (4.6410)  class_acc: 0.0833 (0.1254)  loss_scale: 65536.0000 (64281.0019)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1849  max mem: 15572
Epoch: [5]  [2620/2809]  eta: 0:01:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5496 (4.6408)  class_acc: 0.0833 (0.1253)  loss_scale: 65536.0000 (64285.7902)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1378  max mem: 15572
Epoch: [5]  [2630/2809]  eta: 0:01:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4846 (4.6407)  class_acc: 0.0833 (0.1254)  loss_scale: 65536.0000 (64290.5420)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0912  max mem: 15572
[2025-01-15 17:14:49,609] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:14:49,610] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:14:50,015] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16684
[2025-01-15 17:14:50,015] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:14:50,016] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2640/2809]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000000  loss: 4.5465 (4.6407)  class_acc: 0.1250 (0.1253)  loss_scale: 65536.0000 (64320.0727)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1473  max mem: 15572
Epoch: [5]  [2650/2809]  eta: 0:01:33  lr: 0.000047  min_lr: 0.000000  loss: 4.4980 (4.6404)  class_acc: 0.0833 (0.1253)  loss_scale: 65536.0000 (64324.6594)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1197  max mem: 15572
Epoch: [5]  [2660/2809]  eta: 0:01:27  lr: 0.000047  min_lr: 0.000000  loss: 4.5769 (4.6403)  class_acc: 0.0833 (0.1253)  loss_scale: 65536.0000 (64329.2116)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0830  max mem: 15572
Epoch: [5]  [2670/2809]  eta: 0:01:21  lr: 0.000047  min_lr: 0.000000  loss: 4.6280 (4.6404)  class_acc: 0.0833 (0.1252)  loss_scale: 65536.0000 (64333.7297)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0464  max mem: 15572
Epoch: [5]  [2680/2809]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5842 (4.6400)  class_acc: 0.1250 (0.1253)  loss_scale: 65536.0000 (64338.2141)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1163  max mem: 15572
Epoch: [5]  [2690/2809]  eta: 0:01:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5021 (4.6392)  class_acc: 0.1250 (0.1255)  loss_scale: 65536.0000 (64342.6652)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1916  max mem: 15572
Epoch: [5]  [2700/2809]  eta: 0:01:04  lr: 0.000047  min_lr: 0.000000  loss: 4.5085 (4.6389)  class_acc: 0.2083 (0.1258)  loss_scale: 65536.0000 (64347.0833)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1523  max mem: 15572
Epoch: [5]  [2710/2809]  eta: 0:00:58  lr: 0.000047  min_lr: 0.000000  loss: 4.6022 (4.6393)  class_acc: 0.2083 (0.1258)  loss_scale: 65536.0000 (64351.4688)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1171  max mem: 15572
Epoch: [5]  [2720/2809]  eta: 0:00:52  lr: 0.000047  min_lr: 0.000000  loss: 4.6959 (4.6395)  class_acc: 0.1250 (0.1258)  loss_scale: 65536.0000 (64355.8221)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1779  max mem: 15572
Epoch: [5]  [2730/2809]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6874 (4.6396)  class_acc: 0.0833 (0.1257)  loss_scale: 65536.0000 (64360.1435)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.2180  max mem: 15572
Epoch: [5]  [2740/2809]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000000  loss: 4.6160 (4.6395)  class_acc: 0.1667 (0.1259)  loss_scale: 65536.0000 (64364.4334)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1403  max mem: 15572
Epoch: [5]  [2750/2809]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4862 (4.6387)  class_acc: 0.1667 (0.1260)  loss_scale: 65536.0000 (64368.6921)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0601  max mem: 15572
Epoch: [5]  [2760/2809]  eta: 0:00:28  lr: 0.000047  min_lr: 0.000000  loss: 4.5154 (4.6385)  class_acc: 0.1667 (0.1261)  loss_scale: 65536.0000 (64372.9200)  weight_decay: 0.0500 (0.0500)  time: 0.4504  data: 0.0008  max mem: 15572
[2025-01-15 17:16:03,012] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:16:03,012] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:16:03,472] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16814
[2025-01-15 17:16:03,472] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:16:03,473] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2770/2809]  eta: 0:00:22  lr: 0.000047  min_lr: 0.000000  loss: 4.5428 (4.6383)  class_acc: 0.1667 (0.1262)  loss_scale: 65536.0000 (64400.7680)  weight_decay: 0.0500 (0.0500)  time: 0.4713  data: 0.0008  max mem: 15572
Epoch: [5]  [2780/2809]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000000  loss: 4.6735 (4.6382)  class_acc: 0.1250 (0.1263)  loss_scale: 65536.0000 (64404.8501)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1007  max mem: 15572
Epoch: [5]  [2790/2809]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6479 (4.6381)  class_acc: 0.1250 (0.1263)  loss_scale: 65536.0000 (64408.9029)  weight_decay: 0.0500 (0.0500)  time: 0.6715  data: 0.1662  max mem: 15572
Epoch: [5]  [2800/2809]  eta: 0:00:05  lr: 0.000047  min_lr: 0.000000  loss: 4.6160 (4.6382)  class_acc: 0.1250 (0.1264)  loss_scale: 65536.0000 (64412.9268)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.1396  max mem: 15572
Epoch: [5]  [2808/2809]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6788 (4.6383)  class_acc: 0.1250 (0.1264)  loss_scale: 65536.0000 (64416.1253)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0969  max mem: 15572
Epoch: [5] Total time: 0:27:31 (0.5879 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000000  loss: 4.6788 (4.6383)  class_acc: 0.1250 (0.1264)  loss_scale: 65536.0000 (64416.1253)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:38:51  loss: 2.3421 (2.3421)  acc1: 72.2222 (72.2222)  acc5: 100.0000 (100.0000)  time: 8.5727  data: 8.3325  max mem: 15572
Val:  [ 10/272]  eta: 0:04:35  loss: 4.0934 (4.0123)  acc1: 5.5556 (20.2020)  acc5: 22.2222 (32.3232)  time: 1.0525  data: 0.8404  max mem: 15572
Val:  [ 20/272]  eta: 0:02:46  loss: 4.0161 (3.9725)  acc1: 5.5556 (16.1376)  acc5: 27.7778 (34.3915)  time: 0.2670  data: 0.0462  max mem: 15572
Val:  [ 30/272]  eta: 0:02:10  loss: 3.9865 (3.9957)  acc1: 5.5556 (12.3656)  acc5: 33.3333 (35.1254)  time: 0.2562  data: 0.0370  max mem: 15572
Val:  [ 40/272]  eta: 0:02:02  loss: 3.5689 (3.8573)  acc1: 5.5556 (13.2791)  acc5: 50.0000 (40.6504)  time: 0.3834  data: 0.1566  max mem: 15572
Val:  [ 50/272]  eta: 0:01:50  loss: 3.4141 (3.8137)  acc1: 11.1111 (13.5076)  acc5: 61.1111 (42.4837)  time: 0.4336  data: 0.2017  max mem: 15572
Val:  [ 60/272]  eta: 0:01:39  loss: 3.0446 (3.7270)  acc1: 16.6667 (18.7614)  acc5: 72.2222 (46.3570)  time: 0.3516  data: 0.1411  max mem: 15572
Val:  [ 70/272]  eta: 0:01:30  loss: 3.1705 (3.6599)  acc1: 38.8889 (20.1878)  acc5: 72.2222 (48.9045)  time: 0.3261  data: 0.1107  max mem: 15572
Val:  [ 80/272]  eta: 0:01:23  loss: 3.3337 (3.6593)  acc1: 16.6667 (20.9877)  acc5: 61.1111 (48.6968)  time: 0.3224  data: 0.0995  max mem: 15572
Val:  [ 90/272]  eta: 0:01:17  loss: 4.3366 (3.7352)  acc1: 0.0000 (18.9866)  acc5: 16.6667 (45.1160)  time: 0.3522  data: 0.1390  max mem: 15572
Val:  [100/272]  eta: 0:01:12  loss: 4.3139 (3.7764)  acc1: 0.0000 (18.2618)  acc5: 16.6667 (43.8394)  time: 0.3732  data: 0.1654  max mem: 15572
Val:  [110/272]  eta: 0:01:06  loss: 4.1241 (3.8184)  acc1: 5.5556 (17.1672)  acc5: 33.3333 (42.7928)  time: 0.3476  data: 0.1432  max mem: 15572
Val:  [120/272]  eta: 0:01:02  loss: 4.1743 (3.8537)  acc1: 5.5556 (16.1616)  acc5: 27.7778 (41.8733)  time: 0.3470  data: 0.1421  max mem: 15572
Val:  [130/272]  eta: 0:00:57  loss: 4.1132 (3.8159)  acc1: 5.5556 (17.9813)  acc5: 33.3333 (43.3842)  time: 0.3479  data: 0.1310  max mem: 15572
Val:  [140/272]  eta: 0:00:52  loss: 3.4488 (3.8052)  acc1: 27.7778 (18.8731)  acc5: 50.0000 (43.6170)  time: 0.3471  data: 0.1481  max mem: 15572
Val:  [150/272]  eta: 0:00:47  loss: 3.8689 (3.8126)  acc1: 5.5556 (18.1383)  acc5: 22.2222 (42.4209)  time: 0.2988  data: 0.1175  max mem: 15572
Val:  [160/272]  eta: 0:00:42  loss: 3.8502 (3.8078)  acc1: 11.1111 (18.7026)  acc5: 33.3333 (43.0987)  time: 0.2127  data: 0.0269  max mem: 15572
Val:  [170/272]  eta: 0:00:37  loss: 3.8027 (3.8342)  acc1: 11.1111 (18.0637)  acc5: 38.8889 (42.4301)  time: 0.1827  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 3.7960 (3.8226)  acc1: 5.5556 (17.6489)  acc5: 38.8889 (43.0325)  time: 0.1966  data: 0.0007  max mem: 15572
Val:  [190/272]  eta: 0:00:29  loss: 3.7960 (3.8313)  acc1: 5.5556 (17.3357)  acc5: 38.8889 (42.3793)  time: 0.2756  data: 0.0727  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.6356 (3.8280)  acc1: 11.1111 (17.7446)  acc5: 44.4444 (43.2836)  time: 0.3362  data: 0.1304  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.8572 (3.8432)  acc1: 16.6667 (17.5619)  acc5: 55.5556 (43.2070)  time: 0.3244  data: 0.1061  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 3.9553 (3.8468)  acc1: 16.6667 (17.7728)  acc5: 38.8889 (43.1624)  time: 0.3158  data: 0.1069  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.6258 (3.8306)  acc1: 38.8889 (19.4084)  acc5: 61.1111 (44.3723)  time: 0.3539  data: 0.1565  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 3.3005 (3.8130)  acc1: 44.4444 (20.2167)  acc5: 83.3333 (45.6201)  time: 0.3750  data: 0.1813  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.6635 (3.8379)  acc1: 11.1111 (19.7432)  acc5: 44.4444 (44.9093)  time: 0.2774  data: 0.0844  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.5988 (3.7967)  acc1: 27.7778 (21.5198)  acc5: 55.5556 (46.3389)  time: 0.2411  data: 0.0521  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2652 (3.7931)  acc1: 44.4444 (21.5662)  acc5: 72.2222 (46.4535)  time: 0.2250  data: 0.0549  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2652 (3.7954)  acc1: 44.4444 (21.5646)  acc5: 72.2222 (46.4264)  time: 0.2166  data: 0.0547  max mem: 15572
Val: Total time: 0:01:32 (0.3383 s / it)
* Acc@1 21.565 Acc@5 46.426 loss 3.795
Accuracy of the network on the 4883 val videos: 21.6%
[2025-01-15 17:17:59,999] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 17:18:00,002] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 17:18:00,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 17:18:02,802] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 17:18:02,803] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 21.56%
Epoch: [6]  [   0/2809]  eta: 7:24:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5380 (4.5380)  class_acc: 0.0833 (0.0833)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.4857  data: 8.9932  max mem: 15572
Epoch: [6]  [  10/2809]  eta: 1:02:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5159 (4.5972)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3316  data: 0.8351  max mem: 15572
Epoch: [6]  [  20/2809]  eta: 0:46:57  lr: 0.000047  min_lr: 0.000000  loss: 4.5159 (4.5844)  class_acc: 0.1250 (0.1111)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1180  max mem: 15572
Epoch: [6]  [  30/2809]  eta: 0:40:03  lr: 0.000047  min_lr: 0.000000  loss: 4.5388 (4.6053)  class_acc: 0.0833 (0.1129)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1601  max mem: 15572
Epoch: [6]  [  40/2809]  eta: 0:37:22  lr: 0.000047  min_lr: 0.000000  loss: 4.5827 (4.5891)  class_acc: 0.0833 (0.1169)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1486  max mem: 15572
Epoch: [6]  [  50/2809]  eta: 0:34:19  lr: 0.000047  min_lr: 0.000000  loss: 4.6170 (4.6122)  class_acc: 0.0833 (0.1242)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1251  max mem: 15572
[2025-01-15 17:18:43,440] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16908
[2025-01-15 17:18:43,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 17:18:43,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [  60/2809]  eta: 0:33:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6091 (4.6166)  class_acc: 0.1250 (0.1264)  loss_scale: 65536.0000 (61775.7377)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0942  max mem: 15572
Epoch: [6]  [  70/2809]  eta: 0:32:39  lr: 0.000047  min_lr: 0.000000  loss: 4.5953 (4.6243)  class_acc: 0.1667 (0.1332)  loss_scale: 32768.0000 (57690.1408)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1894  max mem: 15572
Epoch: [6]  [  80/2809]  eta: 0:32:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6893 (4.6341)  class_acc: 0.1667 (0.1353)  loss_scale: 32768.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.2098  max mem: 15572
Epoch: [6]  [  90/2809]  eta: 0:30:57  lr: 0.000047  min_lr: 0.000000  loss: 4.6861 (4.6315)  class_acc: 0.1250 (0.1305)  loss_scale: 32768.0000 (52212.7473)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1104  max mem: 15572
Epoch: [6]  [ 100/2809]  eta: 0:30:27  lr: 0.000047  min_lr: 0.000000  loss: 4.6184 (4.6269)  class_acc: 0.0833 (0.1308)  loss_scale: 32768.0000 (50287.5248)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.0944  max mem: 15572
Epoch: [6]  [ 110/2809]  eta: 0:29:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5832 (4.6142)  class_acc: 0.1250 (0.1344)  loss_scale: 32768.0000 (48709.1892)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1404  max mem: 15572
Epoch: [6]  [ 120/2809]  eta: 0:29:29  lr: 0.000047  min_lr: 0.000000  loss: 4.5672 (4.6147)  class_acc: 0.1250 (0.1346)  loss_scale: 32768.0000 (47391.7355)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1396  max mem: 15572
Epoch: [6]  [ 130/2809]  eta: 0:29:17  lr: 0.000047  min_lr: 0.000000  loss: 4.5672 (4.6021)  class_acc: 0.1250 (0.1333)  loss_scale: 32768.0000 (46275.4198)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1511  max mem: 15572
Epoch: [6]  [ 140/2809]  eta: 0:28:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4033 (4.6001)  class_acc: 0.1250 (0.1339)  loss_scale: 32768.0000 (45317.4468)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1205  max mem: 15572
[2025-01-15 17:19:36,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=107, lr=[4.531573325722194e-07, 4.531573325722194e-07, 6.473676179603135e-07, 6.473676179603135e-07, 9.248108828004479e-07, 9.248108828004479e-07, 1.32115840400064e-06, 1.32115840400064e-06, 1.887369148572343e-06, 1.887369148572343e-06, 2.696241640817633e-06, 2.696241640817633e-06, 3.851773772596618e-06, 3.851773772596618e-06, 5.502533960852313e-06, 5.502533960852313e-06, 7.86076280121759e-06, 7.86076280121759e-06, 1.1229661144596558e-05, 1.1229661144596558e-05, 1.6042373063709367e-05, 1.6042373063709367e-05, 2.29176758052991e-05, 2.29176758052991e-05, 3.2739536864713004e-05, 3.2739536864713004e-05, 4.6770766949590006e-05, 4.6770766949590006e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 17:19:36,973] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=27.515832167400102, CurrSamplesPerSec=26.439215958194524, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [ 150/2809]  eta: 0:28:17  lr: 0.000047  min_lr: 0.000000  loss: 4.5664 (4.5964)  class_acc: 0.1250 (0.1355)  loss_scale: 32768.0000 (44486.3576)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0690  max mem: 15572
Epoch: [6]  [ 160/2809]  eta: 0:28:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5664 (4.5915)  class_acc: 0.1667 (0.1372)  loss_scale: 32768.0000 (43758.5093)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1469  max mem: 15572
Epoch: [6]  [ 170/2809]  eta: 0:27:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5085 (4.5909)  class_acc: 0.1250 (0.1372)  loss_scale: 32768.0000 (43115.7895)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1698  max mem: 15572
Epoch: [6]  [ 180/2809]  eta: 0:27:44  lr: 0.000047  min_lr: 0.000000  loss: 4.5085 (4.5859)  class_acc: 0.1250 (0.1374)  loss_scale: 32768.0000 (42544.0884)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1326  max mem: 15572
[2025-01-15 17:19:58,863] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:19:58,864] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 190/2809]  eta: 0:27:17  lr: 0.000047  min_lr: 0.000000  loss: 4.5461 (4.5876)  class_acc: 0.1250 (0.1374)  loss_scale: 32768.0000 (43404.7330)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1006  max mem: 15572
Epoch: [6]  [ 200/2809]  eta: 0:27:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5574 (4.5829)  class_acc: 0.1250 (0.1372)  loss_scale: 65536.0000 (44505.7910)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0996  max mem: 15572
Epoch: [6]  [ 210/2809]  eta: 0:26:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5015 (4.5806)  class_acc: 0.1250 (0.1361)  loss_scale: 65536.0000 (45502.4834)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0790  max mem: 15572
Epoch: [6]  [ 220/2809]  eta: 0:26:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4869 (4.5782)  class_acc: 0.0833 (0.1361)  loss_scale: 65536.0000 (46408.9774)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0809  max mem: 15572
Epoch: [6]  [ 230/2809]  eta: 0:26:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5431 (4.5806)  class_acc: 0.0833 (0.1358)  loss_scale: 65536.0000 (47236.9870)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1561  max mem: 15572
Epoch: [6]  [ 240/2809]  eta: 0:26:14  lr: 0.000047  min_lr: 0.000000  loss: 4.6171 (4.5789)  class_acc: 0.0833 (0.1359)  loss_scale: 65536.0000 (47996.2822)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1679  max mem: 15572
[2025-01-15 17:20:33,935] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17097
[2025-01-15 17:20:33,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 17:20:33,936] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 250/2809]  eta: 0:26:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5994 (4.5811)  class_acc: 0.1250 (0.1365)  loss_scale: 65536.0000 (47650.6773)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1944  max mem: 15572
Epoch: [6]  [ 260/2809]  eta: 0:26:01  lr: 0.000047  min_lr: 0.000000  loss: 4.6306 (4.5821)  class_acc: 0.0833 (0.1363)  loss_scale: 32768.0000 (47080.4598)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1614  max mem: 15572
Epoch: [6]  [ 270/2809]  eta: 0:25:55  lr: 0.000047  min_lr: 0.000000  loss: 4.6433 (4.5858)  class_acc: 0.0833 (0.1375)  loss_scale: 32768.0000 (46552.3247)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1455  max mem: 15572
Epoch: [6]  [ 280/2809]  eta: 0:25:53  lr: 0.000047  min_lr: 0.000000  loss: 4.6732 (4.5879)  class_acc: 0.1250 (0.1366)  loss_scale: 32768.0000 (46061.7794)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.1956  max mem: 15572
Epoch: [6]  [ 290/2809]  eta: 0:25:42  lr: 0.000047  min_lr: 0.000000  loss: 4.6732 (4.5867)  class_acc: 0.1250 (0.1369)  loss_scale: 32768.0000 (45604.9485)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1670  max mem: 15572
Epoch: [6]  [ 300/2809]  eta: 0:25:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6186 (4.5896)  class_acc: 0.1250 (0.1368)  loss_scale: 32768.0000 (45178.4718)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1331  max mem: 15572
Epoch: [6]  [ 310/2809]  eta: 0:25:30  lr: 0.000047  min_lr: 0.000000  loss: 4.6186 (4.5874)  class_acc: 0.1667 (0.1379)  loss_scale: 32768.0000 (44779.4212)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1529  max mem: 15572
Epoch: [6]  [ 320/2809]  eta: 0:25:27  lr: 0.000047  min_lr: 0.000000  loss: 4.5313 (4.5876)  class_acc: 0.1667 (0.1401)  loss_scale: 32768.0000 (44405.2336)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.1460  max mem: 15572
Epoch: [6]  [ 330/2809]  eta: 0:25:14  lr: 0.000047  min_lr: 0.000000  loss: 4.6291 (4.5889)  class_acc: 0.1667 (0.1405)  loss_scale: 32768.0000 (44053.6556)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0929  max mem: 15572
Epoch: [6]  [ 340/2809]  eta: 0:25:04  lr: 0.000047  min_lr: 0.000000  loss: 4.5658 (4.5881)  class_acc: 0.1667 (0.1411)  loss_scale: 32768.0000 (43722.6979)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0624  max mem: 15572
Epoch: [6]  [ 350/2809]  eta: 0:24:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5702 (4.5902)  class_acc: 0.1250 (0.1408)  loss_scale: 32768.0000 (43410.5983)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0751  max mem: 15572
Epoch: [6]  [ 360/2809]  eta: 0:24:41  lr: 0.000047  min_lr: 0.000000  loss: 4.6167 (4.5913)  class_acc: 0.1250 (0.1416)  loss_scale: 32768.0000 (43115.7895)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0673  max mem: 15572
Epoch: [6]  [ 370/2809]  eta: 0:24:35  lr: 0.000047  min_lr: 0.000000  loss: 4.6048 (4.5896)  class_acc: 0.1250 (0.1414)  loss_scale: 32768.0000 (42836.8733)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0906  max mem: 15572
[2025-01-15 17:21:49,013] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:21:49,014] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 380/2809]  eta: 0:24:25  lr: 0.000047  min_lr: 0.000000  loss: 4.5456 (4.5902)  class_acc: 0.1250 (0.1414)  loss_scale: 32768.0000 (43346.6457)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0961  max mem: 15572
Epoch: [6]  [ 390/2809]  eta: 0:24:18  lr: 0.000047  min_lr: 0.000000  loss: 4.5305 (4.5885)  class_acc: 0.1250 (0.1412)  loss_scale: 65536.0000 (43914.1483)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0891  max mem: 15572
Epoch: [6]  [ 400/2809]  eta: 0:24:14  lr: 0.000047  min_lr: 0.000000  loss: 4.4488 (4.5871)  class_acc: 0.1250 (0.1415)  loss_scale: 65536.0000 (44453.3466)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1454  max mem: 15572
Epoch: [6]  [ 410/2809]  eta: 0:24:04  lr: 0.000047  min_lr: 0.000000  loss: 4.5655 (4.5870)  class_acc: 0.1250 (0.1415)  loss_scale: 65536.0000 (44966.3066)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1449  max mem: 15572
Epoch: [6]  [ 420/2809]  eta: 0:23:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5543 (4.5861)  class_acc: 0.1667 (0.1428)  loss_scale: 65536.0000 (45454.8979)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1502  max mem: 15572
Epoch: [6]  [ 430/2809]  eta: 0:23:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5812 (4.5877)  class_acc: 0.1667 (0.1437)  loss_scale: 65536.0000 (45920.8167)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1541  max mem: 15572
Epoch: [6]  [ 440/2809]  eta: 0:23:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6101 (4.5876)  class_acc: 0.1667 (0.1444)  loss_scale: 65536.0000 (46365.6054)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1509  max mem: 15572
Epoch: [6]  [ 450/2809]  eta: 0:23:41  lr: 0.000047  min_lr: 0.000000  loss: 4.6620 (4.5895)  class_acc: 0.1250 (0.1449)  loss_scale: 65536.0000 (46790.6696)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1708  max mem: 15572
Epoch: [6]  [ 460/2809]  eta: 0:23:37  lr: 0.000047  min_lr: 0.000000  loss: 4.6919 (4.5916)  class_acc: 0.1250 (0.1450)  loss_scale: 65536.0000 (47197.2928)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1667  max mem: 15572
Epoch: [6]  [ 470/2809]  eta: 0:23:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6814 (4.5917)  class_acc: 0.1250 (0.1455)  loss_scale: 65536.0000 (47586.6497)  weight_decay: 0.0500 (0.0500)  time: 0.6582  data: 0.1921  max mem: 15572
Epoch: [6]  [ 480/2809]  eta: 0:23:30  lr: 0.000047  min_lr: 0.000000  loss: 4.6203 (4.5927)  class_acc: 0.1250 (0.1450)  loss_scale: 65536.0000 (47959.8170)  weight_decay: 0.0500 (0.0500)  time: 0.6564  data: 0.1864  max mem: 15572
Epoch: [6]  [ 490/2809]  eta: 0:23:20  lr: 0.000047  min_lr: 0.000000  loss: 4.5776 (4.5922)  class_acc: 0.1667 (0.1461)  loss_scale: 65536.0000 (48317.7841)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1071  max mem: 15572
[2025-01-15 17:23:06,273] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:23:06,273] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 500/2809]  eta: 0:23:17  lr: 0.000047  min_lr: 0.000000  loss: 4.5776 (4.5918)  class_acc: 0.1667 (0.1462)  loss_scale: 65536.0000 (48792.2715)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1165  max mem: 15572
[2025-01-15 17:23:07,294] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17356
[2025-01-15 17:23:07,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:23:07,295] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 510/2809]  eta: 0:23:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5481 (4.5918)  class_acc: 0.1250 (0.1462)  loss_scale: 65536.0000 (49248.1879)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1288  max mem: 15572
Epoch: [6]  [ 520/2809]  eta: 0:22:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5222 (4.5911)  class_acc: 0.1250 (0.1464)  loss_scale: 65536.0000 (49560.8138)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0565  max mem: 15572
Epoch: [6]  [ 530/2809]  eta: 0:22:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4871 (4.5891)  class_acc: 0.1250 (0.1463)  loss_scale: 65536.0000 (49861.6648)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0740  max mem: 15572
Epoch: [6]  [ 540/2809]  eta: 0:22:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4917 (4.5894)  class_acc: 0.1250 (0.1468)  loss_scale: 65536.0000 (50151.3937)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1188  max mem: 15572
Epoch: [6]  [ 550/2809]  eta: 0:22:37  lr: 0.000047  min_lr: 0.000000  loss: 4.6046 (4.5895)  class_acc: 0.1667 (0.1475)  loss_scale: 65536.0000 (50430.6062)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1260  max mem: 15572
Epoch: [6]  [ 560/2809]  eta: 0:22:28  lr: 0.000047  min_lr: 0.000000  loss: 4.6091 (4.5885)  class_acc: 0.1250 (0.1471)  loss_scale: 65536.0000 (50699.8645)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1104  max mem: 15572
Epoch: [6]  [ 570/2809]  eta: 0:22:26  lr: 0.000047  min_lr: 0.000000  loss: 4.6398 (4.5894)  class_acc: 0.0833 (0.1467)  loss_scale: 65536.0000 (50959.6918)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1764  max mem: 15572
Epoch: [6]  [ 580/2809]  eta: 0:22:18  lr: 0.000047  min_lr: 0.000000  loss: 4.5903 (4.5889)  class_acc: 0.0833 (0.1464)  loss_scale: 65536.0000 (51210.5749)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1771  max mem: 15572
Epoch: [6]  [ 590/2809]  eta: 0:22:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5691 (4.5881)  class_acc: 0.1250 (0.1459)  loss_scale: 65536.0000 (51452.9679)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1204  max mem: 15572
Epoch: [6]  [ 600/2809]  eta: 0:22:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5789 (4.5879)  class_acc: 0.1250 (0.1460)  loss_scale: 65536.0000 (51687.2945)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1196  max mem: 15572
Epoch: [6]  [ 610/2809]  eta: 0:21:57  lr: 0.000047  min_lr: 0.000000  loss: 4.5885 (4.5883)  class_acc: 0.1667 (0.1465)  loss_scale: 65536.0000 (51913.9509)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0957  max mem: 15572
Epoch: [6]  [ 620/2809]  eta: 0:21:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5885 (4.5886)  class_acc: 0.1667 (0.1464)  loss_scale: 65536.0000 (52133.3076)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1203  max mem: 15572
Epoch: [6]  [ 630/2809]  eta: 0:21:45  lr: 0.000047  min_lr: 0.000000  loss: 4.6226 (4.5893)  class_acc: 0.1250 (0.1466)  loss_scale: 65536.0000 (52345.7116)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1554  max mem: 15572
[2025-01-15 17:24:21,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:24:21,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:24:22,487] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17487
[2025-01-15 17:24:22,488] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:24:22,488] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 640/2809]  eta: 0:21:38  lr: 0.000047  min_lr: 0.000000  loss: 4.6226 (4.5894)  class_acc: 0.1667 (0.1475)  loss_scale: 65536.0000 (52755.9688)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1447  max mem: 15572
Epoch: [6]  [ 650/2809]  eta: 0:21:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5872 (4.5889)  class_acc: 0.1667 (0.1473)  loss_scale: 65536.0000 (52952.2826)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1409  max mem: 15572
Epoch: [6]  [ 660/2809]  eta: 0:21:24  lr: 0.000047  min_lr: 0.000000  loss: 4.6430 (4.5905)  class_acc: 0.0833 (0.1466)  loss_scale: 65536.0000 (53142.6566)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1271  max mem: 15572
Epoch: [6]  [ 670/2809]  eta: 0:21:18  lr: 0.000047  min_lr: 0.000000  loss: 4.6503 (4.5902)  class_acc: 0.0833 (0.1459)  loss_scale: 65536.0000 (53327.3562)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1207  max mem: 15572
Epoch: [6]  [ 680/2809]  eta: 0:21:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5805 (4.5896)  class_acc: 0.0833 (0.1460)  loss_scale: 65536.0000 (53506.6314)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1590  max mem: 15572
Epoch: [6]  [ 690/2809]  eta: 0:21:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5714 (4.5891)  class_acc: 0.1250 (0.1465)  loss_scale: 65536.0000 (53680.7178)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.1723  max mem: 15572
Epoch: [6]  [ 700/2809]  eta: 0:21:04  lr: 0.000047  min_lr: 0.000000  loss: 4.5540 (4.5870)  class_acc: 0.1667 (0.1473)  loss_scale: 65536.0000 (53849.8374)  weight_decay: 0.0500 (0.0500)  time: 0.6523  data: 0.1586  max mem: 15572
Epoch: [6]  [ 710/2809]  eta: 0:20:55  lr: 0.000047  min_lr: 0.000000  loss: 4.4636 (4.5857)  class_acc: 0.2083 (0.1479)  loss_scale: 65536.0000 (54014.1997)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0825  max mem: 15572
Epoch: [6]  [ 720/2809]  eta: 0:20:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5275 (4.5860)  class_acc: 0.1667 (0.1479)  loss_scale: 65536.0000 (54174.0028)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0945  max mem: 15572
Epoch: [6]  [ 730/2809]  eta: 0:20:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5891 (4.5861)  class_acc: 0.1667 (0.1483)  loss_scale: 65536.0000 (54329.4337)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1532  max mem: 15572
Epoch: [6]  [ 740/2809]  eta: 0:20:38  lr: 0.000047  min_lr: 0.000000  loss: 4.5440 (4.5861)  class_acc: 0.1250 (0.1480)  loss_scale: 65536.0000 (54480.6694)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1196  max mem: 15572
Epoch: [6]  [ 750/2809]  eta: 0:20:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5360 (4.5861)  class_acc: 0.1250 (0.1479)  loss_scale: 65536.0000 (54627.8775)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1436  max mem: 15572
Epoch: [6]  [ 760/2809]  eta: 0:20:26  lr: 0.000047  min_lr: 0.000000  loss: 4.5679 (4.5867)  class_acc: 0.1667 (0.1484)  loss_scale: 65536.0000 (54771.2168)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1490  max mem: 15572
[2025-01-15 17:25:39,415] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:25:39,415] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:25:40,229] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17618
[2025-01-15 17:25:40,230] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:25:40,230] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 770/2809]  eta: 0:20:19  lr: 0.000047  min_lr: 0.000000  loss: 4.6062 (4.5873)  class_acc: 0.1250 (0.1482)  loss_scale: 65536.0000 (55080.8405)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1205  max mem: 15572
Epoch: [6]  [ 780/2809]  eta: 0:20:13  lr: 0.000047  min_lr: 0.000000  loss: 4.6250 (4.5870)  class_acc: 0.1250 (0.1484)  loss_scale: 65536.0000 (55214.7093)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1304  max mem: 15572
Epoch: [6]  [ 790/2809]  eta: 0:20:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5275 (4.5861)  class_acc: 0.1250 (0.1482)  loss_scale: 65536.0000 (55345.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1093  max mem: 15572
Epoch: [6]  [ 800/2809]  eta: 0:20:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5233 (4.5858)  class_acc: 0.1250 (0.1487)  loss_scale: 65536.0000 (55472.4195)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0874  max mem: 15572
Epoch: [6]  [ 810/2809]  eta: 0:19:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5453 (4.5862)  class_acc: 0.1667 (0.1489)  loss_scale: 65536.0000 (55596.5080)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0609  max mem: 15572
Epoch: [6]  [ 820/2809]  eta: 0:19:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6408 (4.5875)  class_acc: 0.1667 (0.1488)  loss_scale: 65536.0000 (55717.5737)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0692  max mem: 15572
Epoch: [6]  [ 830/2809]  eta: 0:19:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4858 (4.5851)  class_acc: 0.1667 (0.1492)  loss_scale: 65536.0000 (55835.7256)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1479  max mem: 15572
Epoch: [6]  [ 840/2809]  eta: 0:19:36  lr: 0.000047  min_lr: 0.000000  loss: 4.4502 (4.5847)  class_acc: 0.1667 (0.1496)  loss_scale: 65536.0000 (55951.0678)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1380  max mem: 15572
Epoch: [6]  [ 850/2809]  eta: 0:19:28  lr: 0.000047  min_lr: 0.000000  loss: 4.6550 (4.5860)  class_acc: 0.1667 (0.1498)  loss_scale: 65536.0000 (56063.6992)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0648  max mem: 15572
Epoch: [6]  [ 860/2809]  eta: 0:19:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5966 (4.5847)  class_acc: 0.1667 (0.1502)  loss_scale: 65536.0000 (56173.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0424  max mem: 15572
Epoch: [6]  [ 870/2809]  eta: 0:19:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5940 (4.5856)  class_acc: 0.1250 (0.1496)  loss_scale: 65536.0000 (56281.2032)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0769  max mem: 15572
Epoch: [6]  [ 880/2809]  eta: 0:19:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5779 (4.5853)  class_acc: 0.0417 (0.1491)  loss_scale: 65536.0000 (56386.2520)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1302  max mem: 15572
Epoch: [6]  [ 890/2809]  eta: 0:19:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5690 (4.5860)  class_acc: 0.0833 (0.1486)  loss_scale: 65536.0000 (56488.9428)  weight_decay: 0.0500 (0.0500)  time: 0.6985  data: 0.2433  max mem: 15572
[2025-01-15 17:26:58,318] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:26:58,319] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:27:00,129] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17751
[2025-01-15 17:27:00,129] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:27:00,129] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 900/2809]  eta: 0:19:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6816 (4.5866)  class_acc: 0.0833 (0.1484)  loss_scale: 65536.0000 (56880.3019)  weight_decay: 0.0500 (0.0500)  time: 0.6652  data: 0.2271  max mem: 15572
Epoch: [6]  [ 910/2809]  eta: 0:18:55  lr: 0.000047  min_lr: 0.000000  loss: 4.6463 (4.5862)  class_acc: 0.0833 (0.1484)  loss_scale: 65536.0000 (56975.3150)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1532  max mem: 15572
Epoch: [6]  [ 920/2809]  eta: 0:18:48  lr: 0.000047  min_lr: 0.000000  loss: 4.5759 (4.5869)  class_acc: 0.0833 (0.1482)  loss_scale: 65536.0000 (57068.2649)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1162  max mem: 15572
Epoch: [6]  [ 930/2809]  eta: 0:18:41  lr: 0.000047  min_lr: 0.000000  loss: 4.5759 (4.5870)  class_acc: 0.1250 (0.1487)  loss_scale: 65536.0000 (57159.2180)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0823  max mem: 15572
Epoch: [6]  [ 940/2809]  eta: 0:18:33  lr: 0.000047  min_lr: 0.000000  loss: 4.5202 (4.5865)  class_acc: 0.1250 (0.1488)  loss_scale: 65536.0000 (57248.2380)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0705  max mem: 15572
Epoch: [6]  [ 950/2809]  eta: 0:18:28  lr: 0.000047  min_lr: 0.000000  loss: 4.5599 (4.5864)  class_acc: 0.1667 (0.1490)  loss_scale: 65536.0000 (57335.3859)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0988  max mem: 15572
Epoch: [6]  [ 960/2809]  eta: 0:18:22  lr: 0.000047  min_lr: 0.000000  loss: 4.6227 (4.5858)  class_acc: 0.1250 (0.1486)  loss_scale: 65536.0000 (57420.7201)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1444  max mem: 15572
Epoch: [6]  [ 970/2809]  eta: 0:18:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5781 (4.5846)  class_acc: 0.1250 (0.1488)  loss_scale: 65536.0000 (57504.2966)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0704  max mem: 15572
Epoch: [6]  [ 980/2809]  eta: 0:18:06  lr: 0.000047  min_lr: 0.000000  loss: 4.6161 (4.5849)  class_acc: 0.1250 (0.1486)  loss_scale: 65536.0000 (57586.1692)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0137  max mem: 15572
Epoch: [6]  [ 990/2809]  eta: 0:18:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5512 (4.5840)  class_acc: 0.0833 (0.1485)  loss_scale: 65536.0000 (57666.3895)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1099  max mem: 15572
Epoch: [6]  [1000/2809]  eta: 0:17:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5212 (4.5836)  class_acc: 0.1250 (0.1488)  loss_scale: 65536.0000 (57745.0070)  weight_decay: 0.0500 (0.0500)  time: 0.6914  data: 0.2265  max mem: 15572
Epoch: [6]  [1010/2809]  eta: 0:17:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5454 (4.5836)  class_acc: 0.1250 (0.1485)  loss_scale: 65536.0000 (57822.0692)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1709  max mem: 15572
Epoch: [6]  [1020/2809]  eta: 0:17:46  lr: 0.000047  min_lr: 0.000000  loss: 4.5454 (4.5833)  class_acc: 0.0833 (0.1481)  loss_scale: 65536.0000 (57897.6219)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1568  max mem: 15572
[2025-01-15 17:28:15,012] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:28:15,012] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:28:16,303] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17883
[2025-01-15 17:28:16,304] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:28:16,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1030/2809]  eta: 0:17:41  lr: 0.000047  min_lr: 0.000000  loss: 4.5404 (4.5832)  class_acc: 0.1250 (0.1486)  loss_scale: 65536.0000 (58162.4054)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1967  max mem: 15572
Epoch: [6]  [1040/2809]  eta: 0:17:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5889 (4.5823)  class_acc: 0.1667 (0.1486)  loss_scale: 65536.0000 (58233.2373)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.1927  max mem: 15572
Epoch: [6]  [1050/2809]  eta: 0:17:29  lr: 0.000047  min_lr: 0.000000  loss: 4.5583 (4.5821)  class_acc: 0.1667 (0.1485)  loss_scale: 65536.0000 (58302.7212)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1368  max mem: 15572
Epoch: [6]  [1060/2809]  eta: 0:17:24  lr: 0.000047  min_lr: 0.000000  loss: 4.5176 (4.5819)  class_acc: 0.0833 (0.1482)  loss_scale: 65536.0000 (58370.8954)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1071  max mem: 15572
Epoch: [6]  [1070/2809]  eta: 0:17:18  lr: 0.000047  min_lr: 0.000000  loss: 4.5821 (4.5839)  class_acc: 0.1250 (0.1483)  loss_scale: 65536.0000 (58437.7965)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.1514  max mem: 15572
Epoch: [6]  [1080/2809]  eta: 0:17:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5922 (4.5827)  class_acc: 0.0833 (0.1475)  loss_scale: 65536.0000 (58503.4598)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1331  max mem: 15572
Epoch: [6]  [1090/2809]  eta: 0:17:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5922 (4.5835)  class_acc: 0.0417 (0.1470)  loss_scale: 65536.0000 (58567.9193)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1345  max mem: 15572
Epoch: [6]  [1100/2809]  eta: 0:17:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4840 (4.5809)  class_acc: 0.0833 (0.1470)  loss_scale: 65536.0000 (58631.2080)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1280  max mem: 15572
Epoch: [6]  [1110/2809]  eta: 0:16:54  lr: 0.000047  min_lr: 0.000000  loss: 4.4183 (4.5800)  class_acc: 0.0833 (0.1468)  loss_scale: 65536.0000 (58693.3573)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1047  max mem: 15572
Epoch: [6]  [1120/2809]  eta: 0:16:47  lr: 0.000047  min_lr: 0.000000  loss: 4.4982 (4.5803)  class_acc: 0.0833 (0.1464)  loss_scale: 65536.0000 (58754.3979)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1064  max mem: 15572
Epoch: [6]  [1130/2809]  eta: 0:16:40  lr: 0.000047  min_lr: 0.000000  loss: 4.5212 (4.5795)  class_acc: 0.1667 (0.1471)  loss_scale: 65536.0000 (58814.3590)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0724  max mem: 15572
Epoch: [6]  [1140/2809]  eta: 0:16:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4985 (4.5789)  class_acc: 0.1667 (0.1480)  loss_scale: 65536.0000 (58873.2691)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0744  max mem: 15572
[2025-01-15 17:29:25,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=113, lr=[4.523589092123934e-07, 4.523589092123934e-07, 6.462270131605621e-07, 6.462270131605621e-07, 9.231814473722315e-07, 9.231814473722315e-07, 1.318830639103188e-06, 1.318830639103188e-06, 1.8840437701474115e-06, 1.8840437701474115e-06, 2.691491100210588e-06, 2.691491100210588e-06, 3.844987286015126e-06, 3.844987286015126e-06, 5.492838980021609e-06, 5.492838980021609e-06, 7.846912828602299e-06, 7.846912828602299e-06, 1.1209875469431857e-05, 1.1209875469431857e-05, 1.601410781347408e-05, 1.601410781347408e-05, 2.2877296876391547e-05, 2.2877296876391547e-05, 3.268185268055935e-05, 3.268185268055935e-05, 4.668836097222765e-05, 4.668836097222765e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 17:29:25,382] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=27.51500313010121, CurrSamplesPerSec=32.17115799592968, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [1150/2809]  eta: 0:16:26  lr: 0.000047  min_lr: 0.000000  loss: 4.5013 (4.5788)  class_acc: 0.1250 (0.1477)  loss_scale: 65536.0000 (58931.1555)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0632  max mem: 15572
[2025-01-15 17:29:33,576] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:29:33,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:29:34,529] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18014
[2025-01-15 17:29:34,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:29:34,529] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1160/2809]  eta: 0:16:21  lr: 0.000047  min_lr: 0.000000  loss: 4.6191 (4.5795)  class_acc: 0.1250 (0.1478)  loss_scale: 65536.0000 (59100.9406)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1027  max mem: 15572
Epoch: [6]  [1170/2809]  eta: 0:16:15  lr: 0.000047  min_lr: 0.000000  loss: 4.6745 (4.5811)  class_acc: 0.1250 (0.1478)  loss_scale: 65536.0000 (59155.8941)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1288  max mem: 15572
Epoch: [6]  [1180/2809]  eta: 0:16:09  lr: 0.000047  min_lr: 0.000000  loss: 4.6896 (4.5810)  class_acc: 0.1667 (0.1483)  loss_scale: 65536.0000 (59209.9170)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0825  max mem: 15572
Epoch: [6]  [1190/2809]  eta: 0:16:03  lr: 0.000047  min_lr: 0.000000  loss: 4.5383 (4.5804)  class_acc: 0.1667 (0.1482)  loss_scale: 65536.0000 (59263.0327)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0870  max mem: 15572
Epoch: [6]  [1200/2809]  eta: 0:15:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4570 (4.5797)  class_acc: 0.1250 (0.1482)  loss_scale: 65536.0000 (59315.2639)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1263  max mem: 15572
Epoch: [6]  [1210/2809]  eta: 0:15:51  lr: 0.000047  min_lr: 0.000000  loss: 4.4733 (4.5791)  class_acc: 0.1250 (0.1480)  loss_scale: 65536.0000 (59366.6325)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1626  max mem: 15572
Epoch: [6]  [1220/2809]  eta: 0:15:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5359 (4.5789)  class_acc: 0.1250 (0.1479)  loss_scale: 65536.0000 (59417.1597)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1577  max mem: 15572
Epoch: [6]  [1230/2809]  eta: 0:15:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6206 (4.5792)  class_acc: 0.1250 (0.1476)  loss_scale: 65536.0000 (59466.8660)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1413  max mem: 15572
Epoch: [6]  [1240/2809]  eta: 0:15:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6113 (4.5792)  class_acc: 0.0833 (0.1474)  loss_scale: 65536.0000 (59515.7712)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1599  max mem: 15572
Epoch: [6]  [1250/2809]  eta: 0:15:27  lr: 0.000047  min_lr: 0.000000  loss: 4.5621 (4.5796)  class_acc: 0.1250 (0.1473)  loss_scale: 65536.0000 (59563.8945)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1388  max mem: 15572
Epoch: [6]  [1260/2809]  eta: 0:15:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5374 (4.5782)  class_acc: 0.1250 (0.1470)  loss_scale: 65536.0000 (59611.2546)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1232  max mem: 15572
Epoch: [6]  [1270/2809]  eta: 0:15:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5031 (4.5782)  class_acc: 0.1250 (0.1472)  loss_scale: 65536.0000 (59657.8694)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1430  max mem: 15572
Epoch: [6]  [1280/2809]  eta: 0:15:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5557 (4.5782)  class_acc: 0.1667 (0.1471)  loss_scale: 65536.0000 (59703.7564)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1494  max mem: 15572
[2025-01-15 17:30:50,379] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:30:50,380] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1290/2809]  eta: 0:15:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4972 (4.5773)  class_acc: 0.1250 (0.1472)  loss_scale: 65536.0000 (59850.4601)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1103  max mem: 15572
Epoch: [6]  [1300/2809]  eta: 0:14:57  lr: 0.000047  min_lr: 0.000000  loss: 4.5704 (4.5772)  class_acc: 0.1667 (0.1474)  loss_scale: 131072.0000 (60397.8970)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1043  max mem: 15572
Epoch: [6]  [1310/2809]  eta: 0:14:50  lr: 0.000047  min_lr: 0.000000  loss: 4.5718 (4.5766)  class_acc: 0.1667 (0.1475)  loss_scale: 131072.0000 (60936.9825)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1277  max mem: 15572
Epoch: [6]  [1320/2809]  eta: 0:14:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5857 (4.5770)  class_acc: 0.1250 (0.1472)  loss_scale: 131072.0000 (61467.9061)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1277  max mem: 15572
[2025-01-15 17:31:09,348] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18175
[2025-01-15 17:31:09,349] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:31:09,349] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1330/2809]  eta: 0:14:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6248 (4.5778)  class_acc: 0.1250 (0.1475)  loss_scale: 65536.0000 (61498.4703)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.1361  max mem: 15572
Epoch: [6]  [1340/2809]  eta: 0:14:33  lr: 0.000047  min_lr: 0.000000  loss: 4.5646 (4.5773)  class_acc: 0.1667 (0.1476)  loss_scale: 65536.0000 (61528.5787)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1273  max mem: 15572
Epoch: [6]  [1350/2809]  eta: 0:14:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4781 (4.5770)  class_acc: 0.1667 (0.1478)  loss_scale: 65536.0000 (61558.2413)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1147  max mem: 15572
Epoch: [6]  [1360/2809]  eta: 0:14:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5781 (4.5773)  class_acc: 0.1250 (0.1478)  loss_scale: 65536.0000 (61587.4680)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0986  max mem: 15572
Epoch: [6]  [1370/2809]  eta: 0:14:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5599 (4.5776)  class_acc: 0.1250 (0.1478)  loss_scale: 65536.0000 (61616.2684)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1074  max mem: 15572
Epoch: [6]  [1380/2809]  eta: 0:14:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5303 (4.5777)  class_acc: 0.1250 (0.1480)  loss_scale: 65536.0000 (61644.6517)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1068  max mem: 15572
Epoch: [6]  [1390/2809]  eta: 0:14:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5787 (4.5781)  class_acc: 0.1250 (0.1477)  loss_scale: 65536.0000 (61672.6269)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1281  max mem: 15572
Epoch: [6]  [1400/2809]  eta: 0:13:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6580 (4.5789)  class_acc: 0.0833 (0.1473)  loss_scale: 65536.0000 (61700.2027)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1537  max mem: 15572
Epoch: [6]  [1410/2809]  eta: 0:13:51  lr: 0.000047  min_lr: 0.000000  loss: 4.7175 (4.5799)  class_acc: 0.0833 (0.1471)  loss_scale: 65536.0000 (61727.3877)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1542  max mem: 15572
Epoch: [6]  [1420/2809]  eta: 0:13:46  lr: 0.000047  min_lr: 0.000000  loss: 4.7015 (4.5808)  class_acc: 0.0833 (0.1470)  loss_scale: 65536.0000 (61754.1900)  weight_decay: 0.0500 (0.0500)  time: 0.6761  data: 0.1994  max mem: 15572
Epoch: [6]  [1430/2809]  eta: 0:13:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6414 (4.5812)  class_acc: 0.1667 (0.1474)  loss_scale: 65536.0000 (61780.6177)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.1491  max mem: 15572
Epoch: [6]  [1440/2809]  eta: 0:13:33  lr: 0.000047  min_lr: 0.000000  loss: 4.5812 (4.5804)  class_acc: 0.1667 (0.1476)  loss_scale: 65536.0000 (61806.6787)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0294  max mem: 15572
[2025-01-15 17:32:25,995] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:32:25,995] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1450/2809]  eta: 0:13:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4328 (4.5804)  class_acc: 0.1250 (0.1475)  loss_scale: 65536.0000 (61877.5465)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0750  max mem: 15572
Epoch: [6]  [1460/2809]  eta: 0:13:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4862 (4.5798)  class_acc: 0.1667 (0.1476)  loss_scale: 131072.0000 (62351.1567)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1100  max mem: 15572
Epoch: [6]  [1470/2809]  eta: 0:13:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5833 (4.5805)  class_acc: 0.1250 (0.1472)  loss_scale: 131072.0000 (62818.3277)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0918  max mem: 15572
[2025-01-15 17:32:38,298] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18326
[2025-01-15 17:32:38,298] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:32:38,298] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1480/2809]  eta: 0:13:08  lr: 0.000047  min_lr: 0.000000  loss: 4.6628 (4.5803)  class_acc: 0.1250 (0.1473)  loss_scale: 131072.0000 (62880.9291)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0789  max mem: 15572
Epoch: [6]  [1490/2809]  eta: 0:13:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6393 (4.5803)  class_acc: 0.1250 (0.1474)  loss_scale: 65536.0000 (62898.7364)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.1095  max mem: 15572
Epoch: [6]  [1500/2809]  eta: 0:12:56  lr: 0.000047  min_lr: 0.000000  loss: 4.5694 (4.5807)  class_acc: 0.1667 (0.1477)  loss_scale: 65536.0000 (62916.3065)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1786  max mem: 15572
Epoch: [6]  [1510/2809]  eta: 0:12:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5287 (4.5808)  class_acc: 0.2083 (0.1480)  loss_scale: 65536.0000 (62933.6439)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1673  max mem: 15572
Epoch: [6]  [1520/2809]  eta: 0:12:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5396 (4.5807)  class_acc: 0.1667 (0.1480)  loss_scale: 65536.0000 (62950.7535)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.1809  max mem: 15572
Epoch: [6]  [1530/2809]  eta: 0:12:39  lr: 0.000047  min_lr: 0.000000  loss: 4.5627 (4.5811)  class_acc: 0.0833 (0.1477)  loss_scale: 65536.0000 (62967.6395)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1525  max mem: 15572
Epoch: [6]  [1540/2809]  eta: 0:12:33  lr: 0.000047  min_lr: 0.000000  loss: 4.6361 (4.5813)  class_acc: 0.0833 (0.1476)  loss_scale: 65536.0000 (62984.3063)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1100  max mem: 15572
Epoch: [6]  [1550/2809]  eta: 0:12:27  lr: 0.000047  min_lr: 0.000000  loss: 4.6129 (4.5812)  class_acc: 0.0833 (0.1474)  loss_scale: 65536.0000 (63000.7582)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0768  max mem: 15572
Epoch: [6]  [1560/2809]  eta: 0:12:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5546 (4.5811)  class_acc: 0.1250 (0.1474)  loss_scale: 65536.0000 (63016.9994)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0784  max mem: 15572
Epoch: [6]  [1570/2809]  eta: 0:12:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5546 (4.5810)  class_acc: 0.1667 (0.1475)  loss_scale: 65536.0000 (63033.0337)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1330  max mem: 15572
Epoch: [6]  [1580/2809]  eta: 0:12:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5508 (4.5808)  class_acc: 0.1250 (0.1473)  loss_scale: 65536.0000 (63048.8653)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1625  max mem: 15572
Epoch: [6]  [1590/2809]  eta: 0:12:03  lr: 0.000047  min_lr: 0.000000  loss: 4.5105 (4.5806)  class_acc: 0.1250 (0.1475)  loss_scale: 65536.0000 (63064.4978)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1679  max mem: 15572
Epoch: [6]  [1600/2809]  eta: 0:11:57  lr: 0.000047  min_lr: 0.000000  loss: 4.5265 (4.5806)  class_acc: 0.1250 (0.1476)  loss_scale: 65536.0000 (63079.9350)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1225  max mem: 15572
[2025-01-15 17:33:54,588] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:33:54,588] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:33:57,112] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18459
[2025-01-15 17:33:57,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:33:57,113] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1610/2809]  eta: 0:11:51  lr: 0.000047  min_lr: 0.000000  loss: 4.6047 (4.5801)  class_acc: 0.1250 (0.1476)  loss_scale: 65536.0000 (63257.9019)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1167  max mem: 15572
Epoch: [6]  [1620/2809]  eta: 0:11:46  lr: 0.000047  min_lr: 0.000000  loss: 4.5984 (4.5807)  class_acc: 0.1250 (0.1475)  loss_scale: 65536.0000 (63271.9556)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.1851  max mem: 15572
Epoch: [6]  [1630/2809]  eta: 0:11:40  lr: 0.000047  min_lr: 0.000000  loss: 4.5986 (4.5806)  class_acc: 0.1250 (0.1476)  loss_scale: 65536.0000 (63285.8369)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1585  max mem: 15572
Epoch: [6]  [1640/2809]  eta: 0:11:35  lr: 0.000047  min_lr: 0.000000  loss: 4.5986 (4.5808)  class_acc: 0.1250 (0.1476)  loss_scale: 65536.0000 (63299.5491)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1487  max mem: 15572
Epoch: [6]  [1650/2809]  eta: 0:11:29  lr: 0.000047  min_lr: 0.000000  loss: 4.5237 (4.5804)  class_acc: 0.1667 (0.1478)  loss_scale: 65536.0000 (63313.0951)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1608  max mem: 15572
Epoch: [6]  [1660/2809]  eta: 0:11:22  lr: 0.000047  min_lr: 0.000000  loss: 4.4776 (4.5803)  class_acc: 0.1667 (0.1478)  loss_scale: 65536.0000 (63326.4780)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0769  max mem: 15572
Epoch: [6]  [1670/2809]  eta: 0:11:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4580 (4.5798)  class_acc: 0.1250 (0.1478)  loss_scale: 65536.0000 (63339.7008)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0333  max mem: 15572
Epoch: [6]  [1680/2809]  eta: 0:11:10  lr: 0.000047  min_lr: 0.000000  loss: 4.6196 (4.5803)  class_acc: 0.1250 (0.1479)  loss_scale: 65536.0000 (63352.7662)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1103  max mem: 15572
Epoch: [6]  [1690/2809]  eta: 0:11:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5979 (4.5798)  class_acc: 0.1667 (0.1482)  loss_scale: 65536.0000 (63365.6771)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1780  max mem: 15572
Epoch: [6]  [1700/2809]  eta: 0:10:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5650 (4.5798)  class_acc: 0.1667 (0.1481)  loss_scale: 65536.0000 (63378.4362)  weight_decay: 0.0500 (0.0500)  time: 0.6669  data: 0.1808  max mem: 15572
Epoch: [6]  [1710/2809]  eta: 0:10:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5265 (4.5797)  class_acc: 0.1667 (0.1483)  loss_scale: 65536.0000 (63391.0462)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1681  max mem: 15572
Epoch: [6]  [1720/2809]  eta: 0:10:47  lr: 0.000047  min_lr: 0.000000  loss: 4.5587 (4.5800)  class_acc: 0.1667 (0.1483)  loss_scale: 65536.0000 (63403.5096)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1308  max mem: 15572
Epoch: [6]  [1730/2809]  eta: 0:10:40  lr: 0.000047  min_lr: 0.000000  loss: 4.5219 (4.5797)  class_acc: 0.1250 (0.1484)  loss_scale: 65536.0000 (63415.8290)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0893  max mem: 15572
[2025-01-15 17:35:14,557] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:35:14,558] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:35:15,346] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18589
[2025-01-15 17:35:15,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:35:15,347] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1740/2809]  eta: 0:10:34  lr: 0.000047  min_lr: 0.000000  loss: 4.5445 (4.5796)  class_acc: 0.1250 (0.1486)  loss_scale: 65536.0000 (63465.6496)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0842  max mem: 15572
Epoch: [6]  [1750/2809]  eta: 0:10:29  lr: 0.000047  min_lr: 0.000000  loss: 4.6090 (4.5800)  class_acc: 0.1250 (0.1486)  loss_scale: 65536.0000 (63477.4734)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1475  max mem: 15572
Epoch: [6]  [1760/2809]  eta: 0:10:22  lr: 0.000047  min_lr: 0.000000  loss: 4.5996 (4.5802)  class_acc: 0.1250 (0.1487)  loss_scale: 65536.0000 (63489.1630)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1235  max mem: 15572
Epoch: [6]  [1770/2809]  eta: 0:10:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5674 (4.5797)  class_acc: 0.1250 (0.1488)  loss_scale: 65536.0000 (63500.7205)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0978  max mem: 15572
Epoch: [6]  [1780/2809]  eta: 0:10:11  lr: 0.000047  min_lr: 0.000000  loss: 4.5132 (4.5797)  class_acc: 0.1667 (0.1490)  loss_scale: 65536.0000 (63512.1482)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1306  max mem: 15572
Epoch: [6]  [1790/2809]  eta: 0:10:05  lr: 0.000047  min_lr: 0.000000  loss: 4.7044 (4.5801)  class_acc: 0.1667 (0.1491)  loss_scale: 65536.0000 (63523.4484)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1211  max mem: 15572
Epoch: [6]  [1800/2809]  eta: 0:09:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5758 (4.5795)  class_acc: 0.1667 (0.1496)  loss_scale: 65536.0000 (63534.6230)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0884  max mem: 15572
Epoch: [6]  [1810/2809]  eta: 0:09:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5105 (4.5795)  class_acc: 0.2083 (0.1497)  loss_scale: 65536.0000 (63545.6742)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0935  max mem: 15572
Epoch: [6]  [1820/2809]  eta: 0:09:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4935 (4.5784)  class_acc: 0.2083 (0.1500)  loss_scale: 65536.0000 (63556.6041)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1197  max mem: 15572
Epoch: [6]  [1830/2809]  eta: 0:09:41  lr: 0.000047  min_lr: 0.000000  loss: 4.4959 (4.5783)  class_acc: 0.1250 (0.1498)  loss_scale: 65536.0000 (63567.4145)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1635  max mem: 15572
Epoch: [6]  [1840/2809]  eta: 0:09:35  lr: 0.000047  min_lr: 0.000000  loss: 4.5680 (4.5785)  class_acc: 0.1250 (0.1497)  loss_scale: 65536.0000 (63578.1076)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1303  max mem: 15572
Epoch: [6]  [1850/2809]  eta: 0:09:28  lr: 0.000047  min_lr: 0.000000  loss: 4.5932 (4.5789)  class_acc: 0.1250 (0.1496)  loss_scale: 65536.0000 (63588.6850)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0744  max mem: 15572
Epoch: [6]  [1860/2809]  eta: 0:09:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5640 (4.5785)  class_acc: 0.1667 (0.1500)  loss_scale: 65536.0000 (63599.1488)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1396  max mem: 15572
[2025-01-15 17:36:29,888] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:36:29,888] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:36:31,357] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18720
[2025-01-15 17:36:31,358] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:36:31,358] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1870/2809]  eta: 0:09:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5011 (4.5779)  class_acc: 0.1667 (0.1501)  loss_scale: 65536.0000 (63679.5553)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1510  max mem: 15572
Epoch: [6]  [1880/2809]  eta: 0:09:11  lr: 0.000047  min_lr: 0.000000  loss: 4.5078 (4.5779)  class_acc: 0.0833 (0.1499)  loss_scale: 65536.0000 (63689.4248)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1347  max mem: 15572
Epoch: [6]  [1890/2809]  eta: 0:09:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5848 (4.5782)  class_acc: 0.0833 (0.1501)  loss_scale: 65536.0000 (63699.1898)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1473  max mem: 15572
Epoch: [6]  [1900/2809]  eta: 0:08:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5669 (4.5777)  class_acc: 0.1667 (0.1503)  loss_scale: 65536.0000 (63708.8522)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1134  max mem: 15572
Epoch: [6]  [1910/2809]  eta: 0:08:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5136 (4.5780)  class_acc: 0.1667 (0.1503)  loss_scale: 65536.0000 (63718.4134)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1263  max mem: 15572
Epoch: [6]  [1920/2809]  eta: 0:08:47  lr: 0.000047  min_lr: 0.000000  loss: 4.5163 (4.5779)  class_acc: 0.0833 (0.1500)  loss_scale: 65536.0000 (63727.8751)  weight_decay: 0.0500 (0.0500)  time: 0.6352  data: 0.1808  max mem: 15572
[2025-01-15 17:37:04,485] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18776
[2025-01-15 17:37:04,486] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 17:37:04,487] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [1930/2809]  eta: 0:08:41  lr: 0.000047  min_lr: 0.000000  loss: 4.5542 (4.5780)  class_acc: 0.0833 (0.1497)  loss_scale: 65536.0000 (63584.5137)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1630  max mem: 15572
Epoch: [6]  [1940/2809]  eta: 0:08:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6165 (4.5780)  class_acc: 0.1250 (0.1498)  loss_scale: 32768.0000 (63425.7476)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0594  max mem: 15572
Epoch: [6]  [1950/2809]  eta: 0:08:29  lr: 0.000047  min_lr: 0.000000  loss: 4.6120 (4.5782)  class_acc: 0.1667 (0.1496)  loss_scale: 32768.0000 (63268.6089)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0793  max mem: 15572
Epoch: [6]  [1960/2809]  eta: 0:08:23  lr: 0.000047  min_lr: 0.000000  loss: 4.6120 (4.5786)  class_acc: 0.1250 (0.1498)  loss_scale: 32768.0000 (63113.0729)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1416  max mem: 15572
Epoch: [6]  [1970/2809]  eta: 0:08:17  lr: 0.000047  min_lr: 0.000000  loss: 4.6085 (4.5784)  class_acc: 0.1250 (0.1498)  loss_scale: 32768.0000 (62959.1152)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1571  max mem: 15572
Epoch: [6]  [1980/2809]  eta: 0:08:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6085 (4.5785)  class_acc: 0.1667 (0.1499)  loss_scale: 32768.0000 (62806.7118)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1636  max mem: 15572
Epoch: [6]  [1990/2809]  eta: 0:08:05  lr: 0.000047  min_lr: 0.000000  loss: 4.6009 (4.5784)  class_acc: 0.1667 (0.1501)  loss_scale: 32768.0000 (62655.8393)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1497  max mem: 15572
Epoch: [6]  [2000/2809]  eta: 0:07:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5157 (4.5779)  class_acc: 0.1250 (0.1500)  loss_scale: 32768.0000 (62506.4748)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1199  max mem: 15572
Epoch: [6]  [2010/2809]  eta: 0:07:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5213 (4.5784)  class_acc: 0.1250 (0.1501)  loss_scale: 32768.0000 (62358.5957)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1581  max mem: 15572
Epoch: [6]  [2020/2809]  eta: 0:07:48  lr: 0.000047  min_lr: 0.000000  loss: 4.5881 (4.5783)  class_acc: 0.1250 (0.1500)  loss_scale: 32768.0000 (62212.1801)  weight_decay: 0.0500 (0.0500)  time: 0.6564  data: 0.2227  max mem: 15572
Epoch: [6]  [2030/2809]  eta: 0:07:42  lr: 0.000047  min_lr: 0.000000  loss: 4.6179 (4.5790)  class_acc: 0.1250 (0.1500)  loss_scale: 32768.0000 (62067.2063)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1546  max mem: 15572
Epoch: [6]  [2040/2809]  eta: 0:07:36  lr: 0.000047  min_lr: 0.000000  loss: 4.7075 (4.5794)  class_acc: 0.1250 (0.1498)  loss_scale: 32768.0000 (61923.6531)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1031  max mem: 15572
Epoch: [6]  [2050/2809]  eta: 0:07:30  lr: 0.000047  min_lr: 0.000000  loss: 4.6443 (4.5791)  class_acc: 0.1250 (0.1500)  loss_scale: 32768.0000 (61781.4998)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1350  max mem: 15572
[2025-01-15 17:38:21,164] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:38:21,165] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [2060/2809]  eta: 0:07:24  lr: 0.000047  min_lr: 0.000000  loss: 4.4341 (4.5787)  class_acc: 0.1250 (0.1500)  loss_scale: 32768.0000 (61799.7166)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.1600  max mem: 15572
Epoch: [6]  [2070/2809]  eta: 0:07:18  lr: 0.000047  min_lr: 0.000000  loss: 4.4532 (4.5787)  class_acc: 0.1667 (0.1501)  loss_scale: 65536.0000 (61817.7576)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1119  max mem: 15572
Epoch: [6]  [2080/2809]  eta: 0:07:12  lr: 0.000047  min_lr: 0.000000  loss: 4.4670 (4.5784)  class_acc: 0.1667 (0.1503)  loss_scale: 65536.0000 (61835.6252)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0924  max mem: 15572
Epoch: [6]  [2090/2809]  eta: 0:07:06  lr: 0.000047  min_lr: 0.000000  loss: 4.4664 (4.5780)  class_acc: 0.1667 (0.1504)  loss_scale: 65536.0000 (61853.3219)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1127  max mem: 15572
Epoch: [6]  [2100/2809]  eta: 0:07:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5001 (4.5777)  class_acc: 0.1250 (0.1504)  loss_scale: 65536.0000 (61870.8501)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0934  max mem: 15572
Epoch: [6]  [2110/2809]  eta: 0:06:54  lr: 0.000047  min_lr: 0.000000  loss: 4.6260 (4.5780)  class_acc: 0.1250 (0.1504)  loss_scale: 65536.0000 (61888.2122)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0858  max mem: 15572
Epoch: [6]  [2120/2809]  eta: 0:06:48  lr: 0.000047  min_lr: 0.000000  loss: 4.6424 (4.5782)  class_acc: 0.1667 (0.1505)  loss_scale: 65536.0000 (61905.4107)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1095  max mem: 15572
Epoch: [6]  [2130/2809]  eta: 0:06:42  lr: 0.000047  min_lr: 0.000000  loss: 4.5503 (4.5780)  class_acc: 0.2083 (0.1506)  loss_scale: 65536.0000 (61922.4477)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1082  max mem: 15572
Epoch: [6]  [2140/2809]  eta: 0:06:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5093 (4.5775)  class_acc: 0.2083 (0.1508)  loss_scale: 65536.0000 (61939.3255)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0739  max mem: 15572
[2025-01-15 17:39:15,512] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=120, lr=[4.5133071296510653e-07, 4.5133071296510653e-07, 6.447581613787237e-07, 6.447581613787237e-07, 9.21083087683891e-07, 9.21083087683891e-07, 1.3158329824055587e-06, 1.3158329824055587e-06, 1.8797614034365126e-06, 1.8797614034365126e-06, 2.685373433480732e-06, 2.685373433480732e-06, 3.836247762115332e-06, 3.836247762115332e-06, 5.480353945879047e-06, 5.480353945879047e-06, 7.829077065541495e-06, 7.829077065541495e-06, 1.1184395807916422e-05, 1.1184395807916422e-05, 1.597770829702346e-05, 1.597770829702346e-05, 2.2825297567176373e-05, 2.2825297567176373e-05, 3.2607567953109105e-05, 3.2607567953109105e-05, 4.6582239933013015e-05, 4.6582239933013015e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 17:39:15,513] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=27.509018657608124, CurrSamplesPerSec=31.133433912125312, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [2150/2809]  eta: 0:06:30  lr: 0.000047  min_lr: 0.000000  loss: 4.5189 (4.5775)  class_acc: 0.2083 (0.1511)  loss_scale: 65536.0000 (61956.0465)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1122  max mem: 15572
Epoch: [6]  [2160/2809]  eta: 0:06:24  lr: 0.000047  min_lr: 0.000000  loss: 4.5864 (4.5777)  class_acc: 0.1250 (0.1509)  loss_scale: 65536.0000 (61972.6127)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1917  max mem: 15572
Epoch: [6]  [2170/2809]  eta: 0:06:18  lr: 0.000047  min_lr: 0.000000  loss: 4.5864 (4.5778)  class_acc: 0.1250 (0.1510)  loss_scale: 65536.0000 (61989.0263)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1694  max mem: 15572
[2025-01-15 17:39:36,925] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:39:36,926] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [2180/2809]  eta: 0:06:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5838 (4.5777)  class_acc: 0.1667 (0.1513)  loss_scale: 65536.0000 (62065.3865)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1576  max mem: 15572
[2025-01-15 17:39:37,811] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19035
[2025-01-15 17:39:37,811] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:39:37,812] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2190/2809]  eta: 0:06:06  lr: 0.000047  min_lr: 0.000000  loss: 4.6106 (4.5775)  class_acc: 0.1667 (0.1513)  loss_scale: 65536.0000 (62081.2268)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1095  max mem: 15572
Epoch: [6]  [2200/2809]  eta: 0:06:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6068 (4.5775)  class_acc: 0.1250 (0.1511)  loss_scale: 65536.0000 (62096.9232)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0417  max mem: 15572
Epoch: [6]  [2210/2809]  eta: 0:05:55  lr: 0.000047  min_lr: 0.000000  loss: 4.5333 (4.5774)  class_acc: 0.1250 (0.1513)  loss_scale: 65536.0000 (62112.4776)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1137  max mem: 15572
Epoch: [6]  [2220/2809]  eta: 0:05:49  lr: 0.000047  min_lr: 0.000000  loss: 4.5993 (4.5779)  class_acc: 0.1667 (0.1514)  loss_scale: 65536.0000 (62127.8919)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.1858  max mem: 15572
Epoch: [6]  [2230/2809]  eta: 0:05:43  lr: 0.000047  min_lr: 0.000000  loss: 4.6605 (4.5782)  class_acc: 0.1250 (0.1512)  loss_scale: 65536.0000 (62143.1681)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1357  max mem: 15572
Epoch: [6]  [2240/2809]  eta: 0:05:37  lr: 0.000047  min_lr: 0.000000  loss: 4.6073 (4.5777)  class_acc: 0.1250 (0.1514)  loss_scale: 65536.0000 (62158.3079)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0756  max mem: 15572
Epoch: [6]  [2250/2809]  eta: 0:05:31  lr: 0.000047  min_lr: 0.000000  loss: 4.5163 (4.5778)  class_acc: 0.1250 (0.1510)  loss_scale: 65536.0000 (62173.3132)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1441  max mem: 15572
Epoch: [6]  [2260/2809]  eta: 0:05:25  lr: 0.000047  min_lr: 0.000000  loss: 4.5859 (4.5782)  class_acc: 0.0833 (0.1510)  loss_scale: 65536.0000 (62188.1858)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1044  max mem: 15572
Epoch: [6]  [2270/2809]  eta: 0:05:19  lr: 0.000047  min_lr: 0.000000  loss: 4.6025 (4.5778)  class_acc: 0.1250 (0.1511)  loss_scale: 65536.0000 (62202.9273)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0132  max mem: 15572
Epoch: [6]  [2280/2809]  eta: 0:05:13  lr: 0.000047  min_lr: 0.000000  loss: 4.4685 (4.5776)  class_acc: 0.1667 (0.1510)  loss_scale: 65536.0000 (62217.5397)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0706  max mem: 15572
Epoch: [6]  [2290/2809]  eta: 0:05:07  lr: 0.000047  min_lr: 0.000000  loss: 4.4886 (4.5773)  class_acc: 0.1250 (0.1511)  loss_scale: 65536.0000 (62232.0244)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1222  max mem: 15572
Epoch: [6]  [2300/2809]  eta: 0:05:01  lr: 0.000047  min_lr: 0.000000  loss: 4.5756 (4.5774)  class_acc: 0.1667 (0.1511)  loss_scale: 65536.0000 (62246.3833)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1633  max mem: 15572
[2025-01-15 17:40:52,083] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:40:52,084] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [2310/2809]  eta: 0:04:55  lr: 0.000047  min_lr: 0.000000  loss: 4.5692 (4.5769)  class_acc: 0.1667 (0.1512)  loss_scale: 65536.0000 (62288.9762)  weight_decay: 0.0500 (0.0500)  time: 0.6502  data: 0.2212  max mem: 15572
Epoch: [6]  [2320/2809]  eta: 0:04:49  lr: 0.000047  min_lr: 0.000000  loss: 4.5414 (4.5769)  class_acc: 0.1667 (0.1511)  loss_scale: 131072.0000 (62585.3270)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1742  max mem: 15572
[2025-01-15 17:41:00,693] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19180
[2025-01-15 17:41:00,693] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:41:00,693] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2330/2809]  eta: 0:04:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5414 (4.5768)  class_acc: 0.1250 (0.1510)  loss_scale: 131072.0000 (62738.5603)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1305  max mem: 15572
Epoch: [6]  [2340/2809]  eta: 0:04:37  lr: 0.000047  min_lr: 0.000000  loss: 4.5403 (4.5765)  class_acc: 0.1250 (0.1508)  loss_scale: 65536.0000 (62750.5100)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1261  max mem: 15572
Epoch: [6]  [2350/2809]  eta: 0:04:31  lr: 0.000047  min_lr: 0.000000  loss: 4.6528 (4.5767)  class_acc: 0.0833 (0.1506)  loss_scale: 65536.0000 (62762.3581)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0785  max mem: 15572
Epoch: [6]  [2360/2809]  eta: 0:04:25  lr: 0.000047  min_lr: 0.000000  loss: 4.6240 (4.5766)  class_acc: 0.1250 (0.1505)  loss_scale: 65536.0000 (62774.1059)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0564  max mem: 15572
Epoch: [6]  [2370/2809]  eta: 0:04:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5670 (4.5768)  class_acc: 0.1250 (0.1504)  loss_scale: 65536.0000 (62785.7545)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0842  max mem: 15572
Epoch: [6]  [2380/2809]  eta: 0:04:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5520 (4.5768)  class_acc: 0.1250 (0.1506)  loss_scale: 65536.0000 (62797.3053)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1233  max mem: 15572
Epoch: [6]  [2390/2809]  eta: 0:04:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5016 (4.5764)  class_acc: 0.1667 (0.1507)  loss_scale: 65536.0000 (62808.7595)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1897  max mem: 15572
Epoch: [6]  [2400/2809]  eta: 0:04:02  lr: 0.000047  min_lr: 0.000000  loss: 4.4615 (4.5761)  class_acc: 0.1667 (0.1508)  loss_scale: 65536.0000 (62820.1183)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.1883  max mem: 15572
Epoch: [6]  [2410/2809]  eta: 0:03:56  lr: 0.000047  min_lr: 0.000000  loss: 4.4710 (4.5759)  class_acc: 0.1250 (0.1508)  loss_scale: 65536.0000 (62831.3828)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1722  max mem: 15572
Epoch: [6]  [2420/2809]  eta: 0:03:50  lr: 0.000047  min_lr: 0.000000  loss: 4.5650 (4.5758)  class_acc: 0.1667 (0.1509)  loss_scale: 65536.0000 (62842.5543)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1211  max mem: 15572
Epoch: [6]  [2430/2809]  eta: 0:03:44  lr: 0.000047  min_lr: 0.000000  loss: 4.5754 (4.5754)  class_acc: 0.1250 (0.1507)  loss_scale: 65536.0000 (62853.6339)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1037  max mem: 15572
Epoch: [6]  [2440/2809]  eta: 0:03:38  lr: 0.000047  min_lr: 0.000000  loss: 4.5879 (4.5755)  class_acc: 0.1250 (0.1508)  loss_scale: 65536.0000 (62864.6227)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1662  max mem: 15572
Epoch: [6]  [2450/2809]  eta: 0:03:32  lr: 0.000047  min_lr: 0.000000  loss: 4.4870 (4.5750)  class_acc: 0.1667 (0.1510)  loss_scale: 65536.0000 (62875.5218)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1508  max mem: 15572
[2025-01-15 17:42:19,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:42:19,464] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [2460/2809]  eta: 0:03:26  lr: 0.000047  min_lr: 0.000000  loss: 4.5349 (4.5754)  class_acc: 0.1667 (0.1511)  loss_scale: 65536.0000 (63046.1113)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1380  max mem: 15572
[2025-01-15 17:42:22,093] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19315
[2025-01-15 17:42:22,093] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:42:22,094] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2470/2809]  eta: 0:03:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6567 (4.5757)  class_acc: 0.1667 (0.1512)  loss_scale: 65536.0000 (63056.1878)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0619  max mem: 15572
Epoch: [6]  [2480/2809]  eta: 0:03:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5909 (4.5756)  class_acc: 0.1667 (0.1513)  loss_scale: 65536.0000 (63066.1830)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0538  max mem: 15572
Epoch: [6]  [2490/2809]  eta: 0:03:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5735 (4.5759)  class_acc: 0.1250 (0.1512)  loss_scale: 65536.0000 (63076.0980)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1272  max mem: 15572
Epoch: [6]  [2500/2809]  eta: 0:03:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6063 (4.5758)  class_acc: 0.1250 (0.1512)  loss_scale: 65536.0000 (63085.9336)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1093  max mem: 15572
Epoch: [6]  [2510/2809]  eta: 0:02:56  lr: 0.000047  min_lr: 0.000000  loss: 4.7224 (4.5765)  class_acc: 0.1250 (0.1512)  loss_scale: 65536.0000 (63095.6910)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0965  max mem: 15572
Epoch: [6]  [2520/2809]  eta: 0:02:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6969 (4.5763)  class_acc: 0.1250 (0.1512)  loss_scale: 65536.0000 (63105.3709)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0851  max mem: 15572
Epoch: [6]  [2530/2809]  eta: 0:02:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5158 (4.5758)  class_acc: 0.1667 (0.1513)  loss_scale: 65536.0000 (63114.9743)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1152  max mem: 15572
Epoch: [6]  [2540/2809]  eta: 0:02:39  lr: 0.000047  min_lr: 0.000000  loss: 4.5385 (4.5755)  class_acc: 0.1667 (0.1516)  loss_scale: 65536.0000 (63124.5022)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.2070  max mem: 15572
Epoch: [6]  [2550/2809]  eta: 0:02:33  lr: 0.000047  min_lr: 0.000000  loss: 4.5385 (4.5756)  class_acc: 0.1250 (0.1515)  loss_scale: 65536.0000 (63133.9553)  weight_decay: 0.0500 (0.0500)  time: 0.6569  data: 0.2124  max mem: 15572
Epoch: [6]  [2560/2809]  eta: 0:02:27  lr: 0.000047  min_lr: 0.000000  loss: 4.5321 (4.5756)  class_acc: 0.1250 (0.1516)  loss_scale: 65536.0000 (63143.3346)  weight_decay: 0.0500 (0.0500)  time: 0.6681  data: 0.2188  max mem: 15572
Epoch: [6]  [2570/2809]  eta: 0:02:21  lr: 0.000047  min_lr: 0.000000  loss: 4.6983 (4.5760)  class_acc: 0.1250 (0.1515)  loss_scale: 65536.0000 (63152.6410)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1615  max mem: 15572
Epoch: [6]  [2580/2809]  eta: 0:02:15  lr: 0.000047  min_lr: 0.000000  loss: 4.6664 (4.5759)  class_acc: 0.1250 (0.1515)  loss_scale: 65536.0000 (63161.8752)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1262  max mem: 15572
[2025-01-15 17:43:39,282] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:43:39,282] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [2590/2809]  eta: 0:02:09  lr: 0.000047  min_lr: 0.000000  loss: 4.6607 (4.5761)  class_acc: 0.1250 (0.1514)  loss_scale: 65536.0000 (63196.3319)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1914  max mem: 15572
Epoch: [6]  [2600/2809]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000000  loss: 4.6721 (4.5766)  class_acc: 0.1250 (0.1515)  loss_scale: 131072.0000 (63457.2918)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1745  max mem: 15572
[2025-01-15 17:43:46,531] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19457
[2025-01-15 17:43:46,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:43:46,531] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2610/2809]  eta: 0:01:57  lr: 0.000047  min_lr: 0.000000  loss: 4.6345 (4.5767)  class_acc: 0.1250 (0.1514)  loss_scale: 131072.0000 (63515.4531)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0986  max mem: 15572
Epoch: [6]  [2620/2809]  eta: 0:01:51  lr: 0.000047  min_lr: 0.000000  loss: 4.6345 (4.5769)  class_acc: 0.1250 (0.1513)  loss_scale: 65536.0000 (63523.1622)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0545  max mem: 15572
Epoch: [6]  [2630/2809]  eta: 0:01:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5826 (4.5768)  class_acc: 0.1250 (0.1512)  loss_scale: 65536.0000 (63530.8126)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0834  max mem: 15572
Epoch: [6]  [2640/2809]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4814 (4.5763)  class_acc: 0.1250 (0.1511)  loss_scale: 65536.0000 (63538.4051)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.1656  max mem: 15572
Epoch: [6]  [2650/2809]  eta: 0:01:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4810 (4.5760)  class_acc: 0.1667 (0.1512)  loss_scale: 65536.0000 (63545.9404)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1086  max mem: 15572
Epoch: [6]  [2660/2809]  eta: 0:01:28  lr: 0.000047  min_lr: 0.000000  loss: 4.6223 (4.5761)  class_acc: 0.1667 (0.1512)  loss_scale: 65536.0000 (63553.4190)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0893  max mem: 15572
Epoch: [6]  [2670/2809]  eta: 0:01:22  lr: 0.000047  min_lr: 0.000000  loss: 4.5370 (4.5760)  class_acc: 0.1667 (0.1512)  loss_scale: 65536.0000 (63560.8416)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1724  max mem: 15572
Epoch: [6]  [2680/2809]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5244 (4.5760)  class_acc: 0.1667 (0.1512)  loss_scale: 65536.0000 (63568.2089)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.1797  max mem: 15572
Epoch: [6]  [2690/2809]  eta: 0:01:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5244 (4.5758)  class_acc: 0.1667 (0.1513)  loss_scale: 65536.0000 (63575.5214)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0967  max mem: 15572
Epoch: [6]  [2700/2809]  eta: 0:01:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4654 (4.5755)  class_acc: 0.1667 (0.1513)  loss_scale: 65536.0000 (63582.7797)  weight_decay: 0.0500 (0.0500)  time: 0.4747  data: 0.0140  max mem: 15572
Epoch: [6]  [2710/2809]  eta: 0:00:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5086 (4.5754)  class_acc: 0.1250 (0.1511)  loss_scale: 65536.0000 (63589.9845)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0850  max mem: 15572
Epoch: [6]  [2720/2809]  eta: 0:00:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5520 (4.5753)  class_acc: 0.0833 (0.1510)  loss_scale: 65536.0000 (63597.1363)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1162  max mem: 15572
Epoch: [6]  [2730/2809]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.5520 (4.5749)  class_acc: 0.1250 (0.1512)  loss_scale: 65536.0000 (63604.2358)  weight_decay: 0.0500 (0.0500)  time: 0.4948  data: 0.0537  max mem: 15572
[2025-01-15 17:44:59,592] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:44:59,592] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:45:01,148] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19590
[2025-01-15 17:45:01,148] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:45:01,149] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2740/2809]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000000  loss: 4.5012 (4.5746)  class_acc: 0.1250 (0.1513)  loss_scale: 65536.0000 (63706.9216)  weight_decay: 0.0500 (0.0500)  time: 0.4257  data: 0.0090  max mem: 15572
Epoch: [6]  [2750/2809]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.5470 (4.5748)  class_acc: 0.1250 (0.1512)  loss_scale: 65536.0000 (63713.5703)  weight_decay: 0.0500 (0.0500)  time: 0.4586  data: 0.0007  max mem: 15572
Epoch: [6]  [2760/2809]  eta: 0:00:28  lr: 0.000047  min_lr: 0.000000  loss: 4.5803 (4.5749)  class_acc: 0.1667 (0.1513)  loss_scale: 65536.0000 (63720.1710)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1397  max mem: 15572
Epoch: [6]  [2770/2809]  eta: 0:00:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5734 (4.5750)  class_acc: 0.1667 (0.1514)  loss_scale: 65536.0000 (63726.7239)  weight_decay: 0.0500 (0.0500)  time: 0.6973  data: 0.2144  max mem: 15572
Epoch: [6]  [2780/2809]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000000  loss: 4.5931 (4.5751)  class_acc: 0.1250 (0.1514)  loss_scale: 65536.0000 (63733.2298)  weight_decay: 0.0500 (0.0500)  time: 0.6664  data: 0.1608  max mem: 15572
Epoch: [6]  [2790/2809]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.5318 (4.5746)  class_acc: 0.1667 (0.1516)  loss_scale: 65536.0000 (63739.6890)  weight_decay: 0.0500 (0.0500)  time: 0.6871  data: 0.1793  max mem: 15572
Epoch: [6]  [2800/2809]  eta: 0:00:05  lr: 0.000046  min_lr: 0.000000  loss: 4.4868 (4.5742)  class_acc: 0.2083 (0.1517)  loss_scale: 65536.0000 (63746.1021)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.1963  max mem: 15572
Epoch: [6]  [2808/2809]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4480 (4.5742)  class_acc: 0.1667 (0.1518)  loss_scale: 65536.0000 (63751.1997)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.1031  max mem: 15572
Epoch: [6] Total time: 0:27:42 (0.5920 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 4.4480 (4.5742)  class_acc: 0.1667 (0.1518)  loss_scale: 65536.0000 (63751.1997)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:26:03  loss: 1.6835 (1.6835)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 5.7484  data: 5.5154  max mem: 15572
Val:  [ 10/272]  eta: 0:04:05  loss: 4.0189 (3.7282)  acc1: 5.5556 (22.7273)  acc5: 27.7778 (34.3434)  time: 0.9376  data: 0.7176  max mem: 15572
Val:  [ 20/272]  eta: 0:02:48  loss: 3.7912 (3.6851)  acc1: 5.5556 (21.6931)  acc5: 38.8889 (39.4180)  time: 0.4149  data: 0.2090  max mem: 15572
Val:  [ 30/272]  eta: 0:02:18  loss: 3.7695 (3.7636)  acc1: 5.5556 (17.0251)  acc5: 44.4444 (39.4265)  time: 0.3723  data: 0.1715  max mem: 15572
Val:  [ 40/272]  eta: 0:02:01  loss: 3.6165 (3.7105)  acc1: 11.1111 (17.2087)  acc5: 44.4444 (42.1409)  time: 0.3738  data: 0.1697  max mem: 15572
Val:  [ 50/272]  eta: 0:01:46  loss: 3.5973 (3.6429)  acc1: 16.6667 (19.9346)  acc5: 44.4444 (44.6623)  time: 0.3319  data: 0.1350  max mem: 15572
Val:  [ 60/272]  eta: 0:01:36  loss: 2.7753 (3.5505)  acc1: 44.4444 (25.0455)  acc5: 72.2222 (48.4517)  time: 0.3123  data: 0.1141  max mem: 15572
Val:  [ 70/272]  eta: 0:01:28  loss: 2.9047 (3.4698)  acc1: 44.4444 (26.6823)  acc5: 77.7778 (52.5822)  time: 0.3336  data: 0.1342  max mem: 15572
Val:  [ 80/272]  eta: 0:01:22  loss: 3.0050 (3.4679)  acc1: 27.7778 (26.6804)  acc5: 72.2222 (52.6749)  time: 0.3590  data: 0.1633  max mem: 15572
Val:  [ 90/272]  eta: 0:01:17  loss: 4.0713 (3.5374)  acc1: 11.1111 (24.7253)  acc5: 27.7778 (50.0611)  time: 0.3914  data: 0.1863  max mem: 15572
Val:  [100/272]  eta: 0:01:11  loss: 4.0665 (3.5907)  acc1: 11.1111 (23.8724)  acc5: 27.7778 (48.8999)  time: 0.3602  data: 0.1534  max mem: 15572
Val:  [110/272]  eta: 0:01:05  loss: 3.9742 (3.6397)  acc1: 11.1111 (22.4224)  acc5: 38.8889 (47.5976)  time: 0.2869  data: 0.1000  max mem: 15572
Val:  [120/272]  eta: 0:00:59  loss: 4.0913 (3.6698)  acc1: 5.5556 (21.6713)  acc5: 33.3333 (46.9238)  time: 0.2559  data: 0.0558  max mem: 15572
Val:  [130/272]  eta: 0:00:53  loss: 3.8107 (3.6417)  acc1: 16.6667 (22.9432)  acc5: 44.4444 (47.9220)  time: 0.2221  data: 0.0197  max mem: 15572
Val:  [140/272]  eta: 0:00:48  loss: 3.4896 (3.6355)  acc1: 27.7778 (23.4043)  acc5: 44.4444 (48.3058)  time: 0.2070  data: 0.0009  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 3.6059 (3.6245)  acc1: 11.1111 (22.9581)  acc5: 44.4444 (48.5283)  time: 0.2216  data: 0.0011  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.4842 (3.6127)  acc1: 11.1111 (23.1194)  acc5: 50.0000 (49.2754)  time: 0.2355  data: 0.0289  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.5911 (3.6334)  acc1: 11.1111 (22.2872)  acc5: 44.4444 (48.5705)  time: 0.3342  data: 0.1286  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 3.6395 (3.6201)  acc1: 11.1111 (22.4064)  acc5: 44.4444 (49.1099)  time: 0.4126  data: 0.2054  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.6783 (3.6400)  acc1: 5.5556 (21.4951)  acc5: 44.4444 (48.1966)  time: 0.3578  data: 0.1510  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.6198 (3.6418)  acc1: 5.5556 (21.3378)  acc5: 44.4444 (48.5075)  time: 0.3099  data: 0.1070  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.3728 (3.6494)  acc1: 27.7778 (21.6956)  acc5: 66.6667 (48.9995)  time: 0.3103  data: 0.1125  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 3.5541 (3.6479)  acc1: 27.7778 (21.9206)  acc5: 61.1111 (49.0950)  time: 0.3154  data: 0.1092  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.3730 (3.6320)  acc1: 44.4444 (23.3526)  acc5: 66.6667 (50.2646)  time: 0.3271  data: 0.1266  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 3.1157 (3.6133)  acc1: 44.4444 (23.7437)  acc5: 77.7778 (51.5445)  time: 0.3181  data: 0.1220  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.3813 (3.6391)  acc1: 11.1111 (23.2182)  acc5: 55.5556 (50.6419)  time: 0.3178  data: 0.1154  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.3813 (3.5956)  acc1: 27.7778 (25.2448)  acc5: 66.6667 (52.0860)  time: 0.2805  data: 0.0943  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.0034 (3.5934)  acc1: 55.5556 (25.1743)  acc5: 77.7778 (52.0500)  time: 0.1976  data: 0.0322  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.0034 (3.5955)  acc1: 55.5556 (25.1690)  acc5: 77.7778 (52.0377)  time: 0.1917  data: 0.0322  max mem: 15572
Val: Total time: 0:01:30 (0.3339 s / it)
* Acc@1 25.169 Acc@5 52.038 loss 3.596
Accuracy of the network on the 4883 val videos: 25.2%
[2025-01-15 17:47:16,627] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 17:47:16,629] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 17:47:16,629] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 17:47:19,591] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 17:47:19,592] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 25.17%
Epoch: [7]  [   0/2809]  eta: 5:56:41  lr: 0.000046  min_lr: 0.000000  loss: 4.3639 (4.3639)  class_acc: 0.0833 (0.0833)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.6188  data: 7.0978  max mem: 15572
Epoch: [7]  [  10/2809]  eta: 1:01:46  lr: 0.000046  min_lr: 0.000000  loss: 4.3639 (4.3848)  class_acc: 0.1250 (0.1629)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3241  data: 0.8769  max mem: 15572
Epoch: [7]  [  20/2809]  eta: 0:43:04  lr: 0.000046  min_lr: 0.000000  loss: 4.5073 (4.4702)  class_acc: 0.1667 (0.1627)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1280  max mem: 15572
Epoch: [7]  [  30/2809]  eta: 0:38:07  lr: 0.000046  min_lr: 0.000000  loss: 4.5795 (4.4979)  class_acc: 0.1250 (0.1613)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0659  max mem: 15572
Epoch: [7]  [  40/2809]  eta: 0:34:51  lr: 0.000046  min_lr: 0.000000  loss: 4.6154 (4.5174)  class_acc: 0.1250 (0.1555)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1238  max mem: 15572
Epoch: [7]  [  50/2809]  eta: 0:33:01  lr: 0.000046  min_lr: 0.000000  loss: 4.4663 (4.5078)  class_acc: 0.1667 (0.1667)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1141  max mem: 15572
[2025-01-15 17:48:00,131] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:48:00,131] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:48:01,097] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19721
[2025-01-15 17:48:01,098] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:48:01,098] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [  60/2809]  eta: 0:32:20  lr: 0.000046  min_lr: 0.000000  loss: 4.4790 (4.5313)  class_acc: 0.2083 (0.1673)  loss_scale: 65536.0000 (67684.7213)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1524  max mem: 15572
Epoch: [7]  [  70/2809]  eta: 0:31:15  lr: 0.000046  min_lr: 0.000000  loss: 4.6624 (4.5488)  class_acc: 0.1667 (0.1620)  loss_scale: 65536.0000 (67382.0845)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1331  max mem: 15572
Epoch: [7]  [  80/2809]  eta: 0:30:34  lr: 0.000046  min_lr: 0.000000  loss: 4.5166 (4.5389)  class_acc: 0.1667 (0.1646)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1053  max mem: 15572
Epoch: [7]  [  90/2809]  eta: 0:29:56  lr: 0.000046  min_lr: 0.000000  loss: 4.4928 (4.5410)  class_acc: 0.2083 (0.1667)  loss_scale: 65536.0000 (66976.3516)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1208  max mem: 15572
Epoch: [7]  [ 100/2809]  eta: 0:29:14  lr: 0.000046  min_lr: 0.000000  loss: 4.5554 (4.5468)  class_acc: 0.2083 (0.1716)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1001  max mem: 15572
Epoch: [7]  [ 110/2809]  eta: 0:28:59  lr: 0.000046  min_lr: 0.000000  loss: 4.5533 (4.5453)  class_acc: 0.1667 (0.1742)  loss_scale: 65536.0000 (66716.8288)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1263  max mem: 15572
Epoch: [7]  [ 120/2809]  eta: 0:28:38  lr: 0.000046  min_lr: 0.000000  loss: 4.4816 (4.5404)  class_acc: 0.2083 (0.1773)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1395  max mem: 15572
Epoch: [7]  [ 130/2809]  eta: 0:28:09  lr: 0.000046  min_lr: 0.000000  loss: 4.4975 (4.5304)  class_acc: 0.2083 (0.1781)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1003  max mem: 15572
Epoch: [7]  [ 140/2809]  eta: 0:28:03  lr: 0.000046  min_lr: 0.000000  loss: 4.4975 (4.5228)  class_acc: 0.1667 (0.1788)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1120  max mem: 15572
Epoch: [7]  [ 150/2809]  eta: 0:27:55  lr: 0.000046  min_lr: 0.000000  loss: 4.4722 (4.5234)  class_acc: 0.1667 (0.1785)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1693  max mem: 15572
Epoch: [7]  [ 160/2809]  eta: 0:27:35  lr: 0.000046  min_lr: 0.000000  loss: 4.5180 (4.5235)  class_acc: 0.1667 (0.1778)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1464  max mem: 15572
Epoch: [7]  [ 170/2809]  eta: 0:27:31  lr: 0.000046  min_lr: 0.000000  loss: 4.5296 (4.5286)  class_acc: 0.1667 (0.1776)  loss_scale: 65536.0000 (66302.5029)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1441  max mem: 15572
Epoch: [7]  [ 180/2809]  eta: 0:27:27  lr: 0.000046  min_lr: 0.000000  loss: 4.5809 (4.5339)  class_acc: 0.1667 (0.1779)  loss_scale: 65536.0000 (66260.1547)  weight_decay: 0.0500 (0.0500)  time: 0.6392  data: 0.1897  max mem: 15572
[2025-01-15 17:49:16,363] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:49:16,363] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:49:16,773] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19851
[2025-01-15 17:49:16,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:49:16,774] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 190/2809]  eta: 0:27:12  lr: 0.000046  min_lr: 0.000000  loss: 4.5069 (4.5298)  class_acc: 0.1667 (0.1780)  loss_scale: 65536.0000 (66565.3613)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1629  max mem: 15572
Epoch: [7]  [ 200/2809]  eta: 0:27:08  lr: 0.000046  min_lr: 0.000000  loss: 4.4894 (4.5338)  class_acc: 0.1250 (0.1760)  loss_scale: 65536.0000 (66514.1493)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1539  max mem: 15572
Epoch: [7]  [ 210/2809]  eta: 0:26:50  lr: 0.000046  min_lr: 0.000000  loss: 4.5554 (4.5325)  class_acc: 0.1250 (0.1752)  loss_scale: 65536.0000 (66467.7915)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1389  max mem: 15572
Epoch: [7]  [ 220/2809]  eta: 0:26:49  lr: 0.000046  min_lr: 0.000000  loss: 4.6014 (4.5356)  class_acc: 0.1250 (0.1759)  loss_scale: 65536.0000 (66425.6290)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1558  max mem: 15572
Epoch: [7]  [ 230/2809]  eta: 0:26:49  lr: 0.000046  min_lr: 0.000000  loss: 4.6587 (4.5399)  class_acc: 0.1667 (0.1746)  loss_scale: 65536.0000 (66387.1169)  weight_decay: 0.0500 (0.0500)  time: 0.6698  data: 0.2200  max mem: 15572
Epoch: [7]  [ 240/2809]  eta: 0:26:43  lr: 0.000046  min_lr: 0.000000  loss: 4.6221 (4.5393)  class_acc: 0.1667 (0.1753)  loss_scale: 65536.0000 (66351.8008)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.1971  max mem: 15572
Epoch: [7]  [ 250/2809]  eta: 0:26:37  lr: 0.000046  min_lr: 0.000000  loss: 4.5031 (4.5344)  class_acc: 0.1667 (0.1768)  loss_scale: 65536.0000 (66319.2988)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1799  max mem: 15572
Epoch: [7]  [ 260/2809]  eta: 0:26:28  lr: 0.000046  min_lr: 0.000000  loss: 4.4687 (4.5366)  class_acc: 0.1667 (0.1754)  loss_scale: 65536.0000 (66289.2874)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1703  max mem: 15572
Epoch: [7]  [ 270/2809]  eta: 0:26:16  lr: 0.000046  min_lr: 0.000000  loss: 4.6047 (4.5415)  class_acc: 0.1250 (0.1733)  loss_scale: 65536.0000 (66261.4908)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1294  max mem: 15572
Epoch: [7]  [ 280/2809]  eta: 0:26:13  lr: 0.000046  min_lr: 0.000000  loss: 4.6047 (4.5401)  class_acc: 0.1667 (0.1745)  loss_scale: 65536.0000 (66235.6726)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1408  max mem: 15572
Epoch: [7]  [ 290/2809]  eta: 0:26:00  lr: 0.000046  min_lr: 0.000000  loss: 4.5063 (4.5431)  class_acc: 0.1667 (0.1744)  loss_scale: 65536.0000 (66211.6289)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1196  max mem: 15572
Epoch: [7]  [ 300/2809]  eta: 0:25:47  lr: 0.000046  min_lr: 0.000000  loss: 4.4452 (4.5397)  class_acc: 0.1667 (0.1748)  loss_scale: 65536.0000 (66189.1827)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0704  max mem: 15572
Epoch: [7]  [ 310/2809]  eta: 0:25:36  lr: 0.000046  min_lr: 0.000000  loss: 4.4383 (4.5389)  class_acc: 0.1667 (0.1731)  loss_scale: 65536.0000 (66168.1801)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0734  max mem: 15572
[2025-01-15 17:50:34,236] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:50:34,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [ 320/2809]  eta: 0:25:25  lr: 0.000046  min_lr: 0.000000  loss: 4.5421 (4.5408)  class_acc: 0.1250 (0.1720)  loss_scale: 65536.0000 (66965.1340)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0922  max mem: 15572
[2025-01-15 17:50:40,416] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19991
[2025-01-15 17:50:40,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:50:40,417] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 330/2809]  eta: 0:25:13  lr: 0.000046  min_lr: 0.000000  loss: 4.5415 (4.5408)  class_acc: 0.1667 (0.1737)  loss_scale: 131072.0000 (68307.9154)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0783  max mem: 15572
[2025-01-15 17:50:44,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=128, lr=[4.500737936126402e-07, 4.500737936126402e-07, 6.429625623037718e-07, 6.429625623037718e-07, 9.185179461482454e-07, 9.185179461482454e-07, 1.3121684944974935e-06, 1.3121684944974935e-06, 1.8745264207107052e-06, 1.8745264207107052e-06, 2.677894886729579e-06, 2.677894886729579e-06, 3.825564123899399e-06, 3.825564123899399e-06, 5.46509160557057e-06, 5.46509160557057e-06, 7.807273722243671e-06, 7.807273722243671e-06, 1.1153248174633818e-05, 1.1153248174633818e-05, 1.593321167804831e-05, 1.593321167804831e-05, 2.2761730968640446e-05, 2.2761730968640446e-05, 3.2516758526629215e-05, 3.2516758526629215e-05, 4.645251218089888e-05, 4.645251218089888e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 17:50:44,771] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=27.51124044772883, CurrSamplesPerSec=26.549854647865928, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [ 340/2809]  eta: 0:25:11  lr: 0.000046  min_lr: 0.000000  loss: 4.5309 (4.5416)  class_acc: 0.1667 (0.1734)  loss_scale: 65536.0000 (68226.6276)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1380  max mem: 15572
Epoch: [7]  [ 350/2809]  eta: 0:24:58  lr: 0.000046  min_lr: 0.000000  loss: 4.5180 (4.5405)  class_acc: 0.1250 (0.1734)  loss_scale: 65536.0000 (68149.9715)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1432  max mem: 15572
Epoch: [7]  [ 360/2809]  eta: 0:24:52  lr: 0.000046  min_lr: 0.000000  loss: 4.5176 (4.5400)  class_acc: 0.1250 (0.1729)  loss_scale: 65536.0000 (68077.5623)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0970  max mem: 15572
Epoch: [7]  [ 370/2809]  eta: 0:24:45  lr: 0.000046  min_lr: 0.000000  loss: 4.6733 (4.5459)  class_acc: 0.1250 (0.1719)  loss_scale: 65536.0000 (68009.0566)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1462  max mem: 15572
Epoch: [7]  [ 380/2809]  eta: 0:24:44  lr: 0.000046  min_lr: 0.000000  loss: 4.6068 (4.5458)  class_acc: 0.1250 (0.1714)  loss_scale: 65536.0000 (67944.1470)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.2004  max mem: 15572
Epoch: [7]  [ 390/2809]  eta: 0:24:37  lr: 0.000046  min_lr: 0.000000  loss: 4.6170 (4.5487)  class_acc: 0.1667 (0.1710)  loss_scale: 65536.0000 (67882.5575)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.2036  max mem: 15572
Epoch: [7]  [ 400/2809]  eta: 0:24:27  lr: 0.000046  min_lr: 0.000000  loss: 4.6211 (4.5490)  class_acc: 0.1667 (0.1710)  loss_scale: 65536.0000 (67824.0399)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1365  max mem: 15572
Epoch: [7]  [ 410/2809]  eta: 0:24:11  lr: 0.000046  min_lr: 0.000000  loss: 4.6283 (4.5528)  class_acc: 0.1250 (0.1710)  loss_scale: 65536.0000 (67768.3698)  weight_decay: 0.0500 (0.0500)  time: 0.4913  data: 0.0560  max mem: 15572
Epoch: [7]  [ 420/2809]  eta: 0:24:14  lr: 0.000046  min_lr: 0.000000  loss: 4.5010 (4.5503)  class_acc: 0.1250 (0.1704)  loss_scale: 65536.0000 (67715.3444)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1471  max mem: 15572
Epoch: [7]  [ 430/2809]  eta: 0:24:11  lr: 0.000046  min_lr: 0.000000  loss: 4.4513 (4.5531)  class_acc: 0.1250 (0.1697)  loss_scale: 65536.0000 (67664.7796)  weight_decay: 0.0500 (0.0500)  time: 0.7202  data: 0.2492  max mem: 15572
Epoch: [7]  [ 440/2809]  eta: 0:24:07  lr: 0.000046  min_lr: 0.000000  loss: 4.6130 (4.5519)  class_acc: 0.1667 (0.1698)  loss_scale: 65536.0000 (67616.5079)  weight_decay: 0.0500 (0.0500)  time: 0.6535  data: 0.1871  max mem: 15572
Epoch: [7]  [ 450/2809]  eta: 0:23:56  lr: 0.000046  min_lr: 0.000000  loss: 4.6180 (4.5529)  class_acc: 0.2083 (0.1701)  loss_scale: 65536.0000 (67570.3769)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1111  max mem: 15572
[2025-01-15 17:51:57,714] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:51:57,714] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [ 460/2809]  eta: 0:23:46  lr: 0.000046  min_lr: 0.000000  loss: 4.6180 (4.5519)  class_acc: 0.1667 (0.1704)  loss_scale: 65536.0000 (68094.8894)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0599  max mem: 15572
[2025-01-15 17:52:01,663] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20127
[2025-01-15 17:52:01,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:52:01,664] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 470/2809]  eta: 0:23:38  lr: 0.000046  min_lr: 0.000000  loss: 4.5318 (4.5529)  class_acc: 0.1667 (0.1699)  loss_scale: 65536.0000 (68457.9873)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0752  max mem: 15572
Epoch: [7]  [ 480/2809]  eta: 0:23:26  lr: 0.000046  min_lr: 0.000000  loss: 4.5323 (4.5520)  class_acc: 0.1667 (0.1713)  loss_scale: 65536.0000 (68397.2391)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0608  max mem: 15572
Epoch: [7]  [ 490/2809]  eta: 0:23:19  lr: 0.000046  min_lr: 0.000000  loss: 4.4917 (4.5514)  class_acc: 0.1667 (0.1709)  loss_scale: 65536.0000 (68338.9654)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0619  max mem: 15572
Epoch: [7]  [ 500/2809]  eta: 0:23:12  lr: 0.000046  min_lr: 0.000000  loss: 4.5655 (4.5501)  class_acc: 0.1667 (0.1713)  loss_scale: 65536.0000 (68283.0180)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0979  max mem: 15572
Epoch: [7]  [ 510/2809]  eta: 0:23:06  lr: 0.000046  min_lr: 0.000000  loss: 4.5874 (4.5491)  class_acc: 0.1667 (0.1718)  loss_scale: 65536.0000 (68229.2603)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1423  max mem: 15572
Epoch: [7]  [ 520/2809]  eta: 0:22:58  lr: 0.000046  min_lr: 0.000000  loss: 4.3573 (4.5442)  class_acc: 0.1667 (0.1728)  loss_scale: 65536.0000 (68177.5662)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1310  max mem: 15572
Epoch: [7]  [ 530/2809]  eta: 0:22:53  lr: 0.000046  min_lr: 0.000000  loss: 4.3573 (4.5447)  class_acc: 0.2083 (0.1733)  loss_scale: 65536.0000 (68127.8192)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1437  max mem: 15572
Epoch: [7]  [ 540/2809]  eta: 0:22:45  lr: 0.000046  min_lr: 0.000000  loss: 4.5538 (4.5432)  class_acc: 0.1667 (0.1732)  loss_scale: 65536.0000 (68079.9113)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1514  max mem: 15572
Epoch: [7]  [ 550/2809]  eta: 0:22:40  lr: 0.000046  min_lr: 0.000000  loss: 4.4927 (4.5422)  class_acc: 0.1667 (0.1729)  loss_scale: 65536.0000 (68033.7423)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1207  max mem: 15572
Epoch: [7]  [ 560/2809]  eta: 0:22:38  lr: 0.000046  min_lr: 0.000000  loss: 4.5520 (4.5436)  class_acc: 0.1667 (0.1733)  loss_scale: 65536.0000 (67989.2193)  weight_decay: 0.0500 (0.0500)  time: 0.6553  data: 0.1763  max mem: 15572
Epoch: [7]  [ 570/2809]  eta: 0:22:33  lr: 0.000046  min_lr: 0.000000  loss: 4.5337 (4.5437)  class_acc: 0.1667 (0.1737)  loss_scale: 65536.0000 (67946.2557)  weight_decay: 0.0500 (0.0500)  time: 0.6719  data: 0.2078  max mem: 15572
Epoch: [7]  [ 580/2809]  eta: 0:22:23  lr: 0.000046  min_lr: 0.000000  loss: 4.5710 (4.5434)  class_acc: 0.1667 (0.1729)  loss_scale: 65536.0000 (67904.7711)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1217  max mem: 15572
Epoch: [7]  [ 590/2809]  eta: 0:22:16  lr: 0.000046  min_lr: 0.000000  loss: 4.5504 (4.5423)  class_acc: 0.1667 (0.1732)  loss_scale: 65536.0000 (67864.6904)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0802  max mem: 15572
[2025-01-15 17:53:17,440] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:53:17,440] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:53:18,877] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20258
[2025-01-15 17:53:18,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:53:18,878] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 600/2809]  eta: 0:22:08  lr: 0.000046  min_lr: 0.000000  loss: 4.4538 (4.5426)  class_acc: 0.1250 (0.1720)  loss_scale: 65536.0000 (68044.0333)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0928  max mem: 15572
Epoch: [7]  [ 610/2809]  eta: 0:22:00  lr: 0.000046  min_lr: 0.000000  loss: 4.5482 (4.5426)  class_acc: 0.1250 (0.1718)  loss_scale: 65536.0000 (68002.9853)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0814  max mem: 15572
Epoch: [7]  [ 620/2809]  eta: 0:21:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5702 (4.5434)  class_acc: 0.1667 (0.1720)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1359  max mem: 15572
Epoch: [7]  [ 630/2809]  eta: 0:21:48  lr: 0.000046  min_lr: 0.000000  loss: 4.6185 (4.5445)  class_acc: 0.1667 (0.1712)  loss_scale: 65536.0000 (67924.7924)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1259  max mem: 15572
Epoch: [7]  [ 640/2809]  eta: 0:21:46  lr: 0.000046  min_lr: 0.000000  loss: 4.5760 (4.5457)  class_acc: 0.1250 (0.1711)  loss_scale: 65536.0000 (67887.5257)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.1369  max mem: 15572
Epoch: [7]  [ 650/2809]  eta: 0:21:37  lr: 0.000046  min_lr: 0.000000  loss: 4.5862 (4.5461)  class_acc: 0.1667 (0.1711)  loss_scale: 65536.0000 (67851.4040)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1098  max mem: 15572
Epoch: [7]  [ 660/2809]  eta: 0:21:28  lr: 0.000046  min_lr: 0.000000  loss: 4.5862 (4.5455)  class_acc: 0.1667 (0.1714)  loss_scale: 65536.0000 (67816.3752)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0213  max mem: 15572
Epoch: [7]  [ 670/2809]  eta: 0:21:23  lr: 0.000046  min_lr: 0.000000  loss: 4.5104 (4.5453)  class_acc: 0.1667 (0.1716)  loss_scale: 65536.0000 (67782.3905)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0858  max mem: 15572
Epoch: [7]  [ 680/2809]  eta: 0:21:20  lr: 0.000046  min_lr: 0.000000  loss: 4.5104 (4.5458)  class_acc: 0.1667 (0.1713)  loss_scale: 65536.0000 (67749.4038)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.1907  max mem: 15572
Epoch: [7]  [ 690/2809]  eta: 0:21:10  lr: 0.000046  min_lr: 0.000000  loss: 4.4365 (4.5441)  class_acc: 0.1667 (0.1715)  loss_scale: 65536.0000 (67717.3719)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1324  max mem: 15572
Epoch: [7]  [ 700/2809]  eta: 0:21:06  lr: 0.000046  min_lr: 0.000000  loss: 4.4448 (4.5435)  class_acc: 0.1667 (0.1714)  loss_scale: 65536.0000 (67686.2539)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1212  max mem: 15572
Epoch: [7]  [ 710/2809]  eta: 0:21:01  lr: 0.000046  min_lr: 0.000000  loss: 4.4386 (4.5423)  class_acc: 0.1667 (0.1715)  loss_scale: 65536.0000 (67656.0113)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1727  max mem: 15572
Epoch: [7]  [ 720/2809]  eta: 0:20:52  lr: 0.000046  min_lr: 0.000000  loss: 4.3633 (4.5423)  class_acc: 0.1667 (0.1713)  loss_scale: 65536.0000 (67626.6075)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1076  max mem: 15572
[2025-01-15 17:54:33,933] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:54:33,933] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:54:37,123] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20390
[2025-01-15 17:54:37,124] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:54:37,124] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 730/2809]  eta: 0:20:47  lr: 0.000046  min_lr: 0.000000  loss: 4.5569 (4.5432)  class_acc: 0.1667 (0.1709)  loss_scale: 65536.0000 (67866.9658)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1315  max mem: 15572
Epoch: [7]  [ 740/2809]  eta: 0:20:43  lr: 0.000046  min_lr: 0.000000  loss: 4.6204 (4.5444)  class_acc: 0.1250 (0.1705)  loss_scale: 65536.0000 (67835.5088)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.1897  max mem: 15572
Epoch: [7]  [ 750/2809]  eta: 0:20:35  lr: 0.000046  min_lr: 0.000000  loss: 4.4803 (4.5420)  class_acc: 0.2083 (0.1717)  loss_scale: 65536.0000 (67804.8895)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1372  max mem: 15572
Epoch: [7]  [ 760/2809]  eta: 0:20:29  lr: 0.000046  min_lr: 0.000000  loss: 4.4744 (4.5423)  class_acc: 0.1250 (0.1710)  loss_scale: 65536.0000 (67775.0749)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0988  max mem: 15572
Epoch: [7]  [ 770/2809]  eta: 0:20:24  lr: 0.000046  min_lr: 0.000000  loss: 4.5757 (4.5441)  class_acc: 0.1250 (0.1714)  loss_scale: 65536.0000 (67746.0337)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1048  max mem: 15572
Epoch: [7]  [ 780/2809]  eta: 0:20:20  lr: 0.000046  min_lr: 0.000000  loss: 4.5533 (4.5442)  class_acc: 0.2083 (0.1723)  loss_scale: 65536.0000 (67717.7362)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.1472  max mem: 15572
Epoch: [7]  [ 790/2809]  eta: 0:20:13  lr: 0.000046  min_lr: 0.000000  loss: 4.5173 (4.5439)  class_acc: 0.2083 (0.1726)  loss_scale: 65536.0000 (67690.1542)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1380  max mem: 15572
Epoch: [7]  [ 800/2809]  eta: 0:20:07  lr: 0.000046  min_lr: 0.000000  loss: 4.5173 (4.5435)  class_acc: 0.1250 (0.1721)  loss_scale: 65536.0000 (67663.2609)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0840  max mem: 15572
Epoch: [7]  [ 810/2809]  eta: 0:20:01  lr: 0.000046  min_lr: 0.000000  loss: 4.5194 (4.5435)  class_acc: 0.1250 (0.1723)  loss_scale: 65536.0000 (67637.0308)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1065  max mem: 15572
Epoch: [7]  [ 820/2809]  eta: 0:19:54  lr: 0.000046  min_lr: 0.000000  loss: 4.4713 (4.5425)  class_acc: 0.1250 (0.1721)  loss_scale: 65536.0000 (67611.4397)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1153  max mem: 15572
Epoch: [7]  [ 830/2809]  eta: 0:19:49  lr: 0.000046  min_lr: 0.000000  loss: 4.4991 (4.5435)  class_acc: 0.1250 (0.1727)  loss_scale: 65536.0000 (67586.4645)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1522  max mem: 15572
Epoch: [7]  [ 840/2809]  eta: 0:19:42  lr: 0.000046  min_lr: 0.000000  loss: 4.5431 (4.5434)  class_acc: 0.1667 (0.1726)  loss_scale: 65536.0000 (67562.0832)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1472  max mem: 15572
Epoch: [7]  [ 850/2809]  eta: 0:19:36  lr: 0.000046  min_lr: 0.000000  loss: 4.6210 (4.5441)  class_acc: 0.1667 (0.1730)  loss_scale: 65536.0000 (67538.2750)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1073  max mem: 15572
[2025-01-15 17:55:53,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:55:53,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:55:56,168] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20520
[2025-01-15 17:55:56,169] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:55:56,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 860/2809]  eta: 0:19:31  lr: 0.000046  min_lr: 0.000000  loss: 4.5056 (4.5433)  class_acc: 0.1667 (0.1733)  loss_scale: 65536.0000 (67591.1359)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1633  max mem: 15572
[2025-01-15 17:56:04,163] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20533
[2025-01-15 17:56:04,163] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 17:56:04,163] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 870/2809]  eta: 0:19:26  lr: 0.000046  min_lr: 0.000000  loss: 4.4448 (4.5421)  class_acc: 0.1667 (0.1736)  loss_scale: 65536.0000 (67529.9196)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.2055  max mem: 15572
Epoch: [7]  [ 880/2809]  eta: 0:19:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3707 (4.5403)  class_acc: 0.1667 (0.1740)  loss_scale: 32768.0000 (67135.3462)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1563  max mem: 15572
Epoch: [7]  [ 890/2809]  eta: 0:19:14  lr: 0.000046  min_lr: 0.000000  loss: 4.4441 (4.5398)  class_acc: 0.1667 (0.1739)  loss_scale: 32768.0000 (66749.6296)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1124  max mem: 15572
Epoch: [7]  [ 900/2809]  eta: 0:19:06  lr: 0.000046  min_lr: 0.000000  loss: 4.5199 (4.5391)  class_acc: 0.1250 (0.1736)  loss_scale: 32768.0000 (66372.4750)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0732  max mem: 15572
Epoch: [7]  [ 910/2809]  eta: 0:19:01  lr: 0.000046  min_lr: 0.000000  loss: 4.5460 (4.5395)  class_acc: 0.1250 (0.1734)  loss_scale: 32768.0000 (66003.6004)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0825  max mem: 15572
Epoch: [7]  [ 920/2809]  eta: 0:18:54  lr: 0.000046  min_lr: 0.000000  loss: 4.6109 (4.5402)  class_acc: 0.1667 (0.1731)  loss_scale: 32768.0000 (65642.7362)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1067  max mem: 15572
Epoch: [7]  [ 930/2809]  eta: 0:18:49  lr: 0.000046  min_lr: 0.000000  loss: 4.6145 (4.5397)  class_acc: 0.1250 (0.1727)  loss_scale: 32768.0000 (65289.6241)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1373  max mem: 15572
Epoch: [7]  [ 940/2809]  eta: 0:18:41  lr: 0.000046  min_lr: 0.000000  loss: 4.5968 (4.5408)  class_acc: 0.1250 (0.1723)  loss_scale: 32768.0000 (64944.0170)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1273  max mem: 15572
Epoch: [7]  [ 950/2809]  eta: 0:18:33  lr: 0.000046  min_lr: 0.000000  loss: 4.6044 (4.5414)  class_acc: 0.1250 (0.1721)  loss_scale: 32768.0000 (64605.6782)  weight_decay: 0.0500 (0.0500)  time: 0.4884  data: 0.0415  max mem: 15572
Epoch: [7]  [ 960/2809]  eta: 0:18:28  lr: 0.000046  min_lr: 0.000000  loss: 4.5673 (4.5414)  class_acc: 0.1250 (0.1716)  loss_scale: 32768.0000 (64274.3809)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1231  max mem: 15572
Epoch: [7]  [ 970/2809]  eta: 0:18:22  lr: 0.000046  min_lr: 0.000000  loss: 4.5175 (4.5413)  class_acc: 0.1250 (0.1716)  loss_scale: 32768.0000 (63949.9073)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.1961  max mem: 15572
Epoch: [7]  [ 980/2809]  eta: 0:18:16  lr: 0.000046  min_lr: 0.000000  loss: 4.5185 (4.5429)  class_acc: 0.1667 (0.1717)  loss_scale: 32768.0000 (63632.0489)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1569  max mem: 15572
Epoch: [7]  [ 990/2809]  eta: 0:18:12  lr: 0.000046  min_lr: 0.000000  loss: 4.6099 (4.5431)  class_acc: 0.1667 (0.1715)  loss_scale: 32768.0000 (63320.6054)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.2047  max mem: 15572
[2025-01-15 17:57:20,038] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:57:20,038] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [1000/2809]  eta: 0:18:05  lr: 0.000046  min_lr: 0.000000  loss: 4.5350 (4.5420)  class_acc: 0.1667 (0.1717)  loss_scale: 32768.0000 (63080.8551)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1538  max mem: 15572
Epoch: [7]  [1010/2809]  eta: 0:17:59  lr: 0.000046  min_lr: 0.000000  loss: 4.3311 (4.5402)  class_acc: 0.1667 (0.1719)  loss_scale: 65536.0000 (63105.1395)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1064  max mem: 15572
Epoch: [7]  [1020/2809]  eta: 0:17:53  lr: 0.000046  min_lr: 0.000000  loss: 4.4559 (4.5402)  class_acc: 0.1667 (0.1716)  loss_scale: 65536.0000 (63128.9481)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1521  max mem: 15572
Epoch: [7]  [1030/2809]  eta: 0:17:48  lr: 0.000046  min_lr: 0.000000  loss: 4.5635 (4.5404)  class_acc: 0.1250 (0.1714)  loss_scale: 65536.0000 (63152.2949)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1585  max mem: 15572
Epoch: [7]  [1040/2809]  eta: 0:17:40  lr: 0.000046  min_lr: 0.000000  loss: 4.5635 (4.5408)  class_acc: 0.1250 (0.1711)  loss_scale: 65536.0000 (63175.1931)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1011  max mem: 15572
Epoch: [7]  [1050/2809]  eta: 0:17:34  lr: 0.000046  min_lr: 0.000000  loss: 4.5249 (4.5406)  class_acc: 0.1667 (0.1713)  loss_scale: 65536.0000 (63197.6556)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0845  max mem: 15572
Epoch: [7]  [1060/2809]  eta: 0:17:28  lr: 0.000046  min_lr: 0.000000  loss: 4.5855 (4.5414)  class_acc: 0.1667 (0.1717)  loss_scale: 65536.0000 (63219.6946)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1505  max mem: 15572
Epoch: [7]  [1070/2809]  eta: 0:17:23  lr: 0.000046  min_lr: 0.000000  loss: 4.5569 (4.5413)  class_acc: 0.2500 (0.1726)  loss_scale: 65536.0000 (63241.3221)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1728  max mem: 15572
Epoch: [7]  [1080/2809]  eta: 0:17:17  lr: 0.000046  min_lr: 0.000000  loss: 4.5107 (4.5414)  class_acc: 0.2083 (0.1726)  loss_scale: 65536.0000 (63262.5495)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1589  max mem: 15572
Epoch: [7]  [1090/2809]  eta: 0:17:10  lr: 0.000046  min_lr: 0.000000  loss: 4.5107 (4.5407)  class_acc: 0.1667 (0.1726)  loss_scale: 65536.0000 (63283.3877)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1252  max mem: 15572
Epoch: [7]  [1100/2809]  eta: 0:17:04  lr: 0.000046  min_lr: 0.000000  loss: 4.4623 (4.5401)  class_acc: 0.2083 (0.1729)  loss_scale: 65536.0000 (63303.8474)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1165  max mem: 15572
Epoch: [7]  [1110/2809]  eta: 0:16:58  lr: 0.000046  min_lr: 0.000000  loss: 4.5120 (4.5395)  class_acc: 0.1667 (0.1722)  loss_scale: 65536.0000 (63323.9388)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1161  max mem: 15572
Epoch: [7]  [1120/2809]  eta: 0:16:53  lr: 0.000046  min_lr: 0.000000  loss: 4.5453 (4.5399)  class_acc: 0.0833 (0.1716)  loss_scale: 65536.0000 (63343.6717)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1590  max mem: 15572
[2025-01-15 17:58:37,307] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:58:37,307] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1130/2809]  eta: 0:16:47  lr: 0.000046  min_lr: 0.000000  loss: 4.5129 (4.5397)  class_acc: 0.1667 (0.1719)  loss_scale: 65536.0000 (63594.8364)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1422  max mem: 15572
[2025-01-15 17:58:41,444] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20798
[2025-01-15 17:58:41,444] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:58:41,445] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1140/2809]  eta: 0:16:40  lr: 0.000046  min_lr: 0.000000  loss: 4.4524 (4.5386)  class_acc: 0.2083 (0.1721)  loss_scale: 65536.0000 (63841.5986)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0582  max mem: 15572
Epoch: [7]  [1150/2809]  eta: 0:16:34  lr: 0.000046  min_lr: 0.000000  loss: 4.4524 (4.5390)  class_acc: 0.1667 (0.1721)  loss_scale: 65536.0000 (63856.3197)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0267  max mem: 15572
Epoch: [7]  [1160/2809]  eta: 0:16:26  lr: 0.000046  min_lr: 0.000000  loss: 4.5630 (4.5392)  class_acc: 0.1667 (0.1719)  loss_scale: 65536.0000 (63870.7873)  weight_decay: 0.0500 (0.0500)  time: 0.5051  data: 0.0262  max mem: 15572
Epoch: [7]  [1170/2809]  eta: 0:16:20  lr: 0.000046  min_lr: 0.000000  loss: 4.4799 (4.5394)  class_acc: 0.0833 (0.1716)  loss_scale: 65536.0000 (63885.0077)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0757  max mem: 15572
Epoch: [7]  [1180/2809]  eta: 0:16:14  lr: 0.000046  min_lr: 0.000000  loss: 4.6103 (4.5399)  class_acc: 0.1250 (0.1716)  loss_scale: 65536.0000 (63898.9873)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1450  max mem: 15572
Epoch: [7]  [1190/2809]  eta: 0:16:08  lr: 0.000046  min_lr: 0.000000  loss: 4.5610 (4.5397)  class_acc: 0.1667 (0.1717)  loss_scale: 65536.0000 (63912.7322)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1644  max mem: 15572
Epoch: [7]  [1200/2809]  eta: 0:16:02  lr: 0.000046  min_lr: 0.000000  loss: 4.5232 (4.5409)  class_acc: 0.1667 (0.1716)  loss_scale: 65536.0000 (63926.2481)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1754  max mem: 15572
Epoch: [7]  [1210/2809]  eta: 0:15:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5232 (4.5396)  class_acc: 0.1667 (0.1718)  loss_scale: 65536.0000 (63939.5409)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1544  max mem: 15572
Epoch: [7]  [1220/2809]  eta: 0:15:51  lr: 0.000046  min_lr: 0.000000  loss: 4.4332 (4.5392)  class_acc: 0.1667 (0.1719)  loss_scale: 65536.0000 (63952.6159)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1502  max mem: 15572
Epoch: [7]  [1230/2809]  eta: 0:15:45  lr: 0.000046  min_lr: 0.000000  loss: 4.5684 (4.5392)  class_acc: 0.1667 (0.1721)  loss_scale: 65536.0000 (63965.4785)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.1725  max mem: 15572
Epoch: [7]  [1240/2809]  eta: 0:15:39  lr: 0.000046  min_lr: 0.000000  loss: 4.5380 (4.5391)  class_acc: 0.1667 (0.1721)  loss_scale: 65536.0000 (63978.1338)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1225  max mem: 15572
Epoch: [7]  [1250/2809]  eta: 0:15:32  lr: 0.000046  min_lr: 0.000000  loss: 4.5068 (4.5387)  class_acc: 0.1250 (0.1720)  loss_scale: 65536.0000 (63990.5867)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0854  max mem: 15572
Epoch: [7]  [1260/2809]  eta: 0:15:27  lr: 0.000046  min_lr: 0.000000  loss: 4.5876 (4.5400)  class_acc: 0.0833 (0.1718)  loss_scale: 65536.0000 (64002.8422)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1355  max mem: 15572
[2025-01-15 17:59:57,033] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:59:57,034] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:59:59,516] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20931
[2025-01-15 17:59:59,517] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:59:59,517] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1270/2809]  eta: 0:15:20  lr: 0.000046  min_lr: 0.000000  loss: 4.6185 (4.5397)  class_acc: 0.1667 (0.1720)  loss_scale: 65536.0000 (64221.1550)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1136  max mem: 15572
Epoch: [7]  [1280/2809]  eta: 0:15:15  lr: 0.000046  min_lr: 0.000000  loss: 4.4421 (4.5388)  class_acc: 0.2083 (0.1722)  loss_scale: 65536.0000 (64231.4192)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1151  max mem: 15572
Epoch: [7]  [1290/2809]  eta: 0:15:08  lr: 0.000046  min_lr: 0.000000  loss: 4.5675 (4.5395)  class_acc: 0.1667 (0.1721)  loss_scale: 65536.0000 (64241.5244)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1504  max mem: 15572
Epoch: [7]  [1300/2809]  eta: 0:15:02  lr: 0.000046  min_lr: 0.000000  loss: 4.5675 (4.5398)  class_acc: 0.1667 (0.1722)  loss_scale: 65536.0000 (64251.4743)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1203  max mem: 15572
[2025-01-15 18:00:18,631] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20964
[2025-01-15 18:00:18,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:00:18,633] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [1310/2809]  eta: 0:14:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5035 (4.5398)  class_acc: 0.1667 (0.1725)  loss_scale: 32768.0000 (64011.3257)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1415  max mem: 15572
Epoch: [7]  [1320/2809]  eta: 0:14:49  lr: 0.000046  min_lr: 0.000000  loss: 4.4819 (4.5390)  class_acc: 0.1667 (0.1726)  loss_scale: 32768.0000 (63774.8130)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1210  max mem: 15572
Epoch: [7]  [1330/2809]  eta: 0:14:43  lr: 0.000046  min_lr: 0.000000  loss: 4.4920 (4.5398)  class_acc: 0.1667 (0.1726)  loss_scale: 32768.0000 (63541.8542)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1196  max mem: 15572
[2025-01-15 18:00:38,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=136, lr=[4.485894344622119e-07, 4.485894344622119e-07, 6.408420492317313e-07, 6.408420492317313e-07, 9.154886417596163e-07, 9.154886417596163e-07, 1.307840916799452e-06, 1.307840916799452e-06, 1.86834416685636e-06, 1.86834416685636e-06, 2.669063095509086e-06, 2.669063095509086e-06, 3.812947279298694e-06, 3.812947279298694e-06, 5.447067541855278e-06, 5.447067541855278e-06, 7.781525059793253e-06, 7.781525059793253e-06, 1.1116464371133221e-05, 1.1116464371133221e-05, 1.588066338733317e-05, 1.588066338733317e-05, 2.2686661981904537e-05, 2.2686661981904537e-05, 3.2409517117006484e-05, 3.2409517117006484e-05, 4.629931016715212e-05, 4.629931016715212e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 18:00:38,005] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=27.501720410062443, CurrSamplesPerSec=32.18403575744787, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [1340/2809]  eta: 0:14:37  lr: 0.000046  min_lr: 0.000000  loss: 4.5149 (4.5390)  class_acc: 0.1667 (0.1728)  loss_scale: 32768.0000 (63312.3699)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1670  max mem: 15572
Epoch: [7]  [1350/2809]  eta: 0:14:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4823 (4.5385)  class_acc: 0.1667 (0.1730)  loss_scale: 32768.0000 (63086.2828)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.1926  max mem: 15572
Epoch: [7]  [1360/2809]  eta: 0:14:25  lr: 0.000046  min_lr: 0.000000  loss: 4.5560 (4.5384)  class_acc: 0.1667 (0.1732)  loss_scale: 32768.0000 (62863.5180)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1071  max mem: 15572
Epoch: [7]  [1370/2809]  eta: 0:14:18  lr: 0.000046  min_lr: 0.000000  loss: 4.5779 (4.5390)  class_acc: 0.1250 (0.1731)  loss_scale: 32768.0000 (62644.0029)  weight_decay: 0.0500 (0.0500)  time: 0.4891  data: 0.0135  max mem: 15572
Epoch: [7]  [1380/2809]  eta: 0:14:12  lr: 0.000046  min_lr: 0.000000  loss: 4.6040 (4.5396)  class_acc: 0.1250 (0.1732)  loss_scale: 32768.0000 (62427.6669)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0915  max mem: 15572
Epoch: [7]  [1390/2809]  eta: 0:14:05  lr: 0.000046  min_lr: 0.000000  loss: 4.6040 (4.5398)  class_acc: 0.1250 (0.1731)  loss_scale: 32768.0000 (62214.4414)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0788  max mem: 15572
Epoch: [7]  [1400/2809]  eta: 0:13:58  lr: 0.000046  min_lr: 0.000000  loss: 4.6127 (4.5403)  class_acc: 0.1250 (0.1736)  loss_scale: 32768.0000 (62004.2598)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0009  max mem: 15572
Epoch: [7]  [1410/2809]  eta: 0:13:51  lr: 0.000046  min_lr: 0.000000  loss: 4.5893 (4.5401)  class_acc: 0.1667 (0.1735)  loss_scale: 32768.0000 (61797.0574)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0349  max mem: 15572
Epoch: [7]  [1420/2809]  eta: 0:13:45  lr: 0.000046  min_lr: 0.000000  loss: 4.5378 (4.5404)  class_acc: 0.1667 (0.1735)  loss_scale: 32768.0000 (61592.7713)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0579  max mem: 15572
[2025-01-15 18:01:30,211] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:01:30,211] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [1430/2809]  eta: 0:13:39  lr: 0.000046  min_lr: 0.000000  loss: 4.5462 (4.5403)  class_acc: 0.1667 (0.1739)  loss_scale: 32768.0000 (61414.2390)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0937  max mem: 15572
Epoch: [7]  [1440/2809]  eta: 0:13:34  lr: 0.000046  min_lr: 0.000000  loss: 4.5147 (4.5399)  class_acc: 0.1667 (0.1738)  loss_scale: 65536.0000 (61442.8425)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.1984  max mem: 15572
Epoch: [7]  [1450/2809]  eta: 0:13:28  lr: 0.000046  min_lr: 0.000000  loss: 4.4520 (4.5393)  class_acc: 0.1667 (0.1739)  loss_scale: 65536.0000 (61471.0517)  weight_decay: 0.0500 (0.0500)  time: 0.6676  data: 0.2275  max mem: 15572
Epoch: [7]  [1460/2809]  eta: 0:13:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4747 (4.5393)  class_acc: 0.2083 (0.1743)  loss_scale: 65536.0000 (61498.8747)  weight_decay: 0.0500 (0.0500)  time: 0.6901  data: 0.2381  max mem: 15572
Epoch: [7]  [1470/2809]  eta: 0:13:17  lr: 0.000046  min_lr: 0.000000  loss: 4.5856 (4.5392)  class_acc: 0.1667 (0.1740)  loss_scale: 65536.0000 (61526.3195)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.1796  max mem: 15572
Epoch: [7]  [1480/2809]  eta: 0:13:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4905 (4.5387)  class_acc: 0.1667 (0.1740)  loss_scale: 65536.0000 (61553.3937)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1316  max mem: 15572
Epoch: [7]  [1490/2809]  eta: 0:13:06  lr: 0.000046  min_lr: 0.000000  loss: 4.5309 (4.5389)  class_acc: 0.1667 (0.1741)  loss_scale: 65536.0000 (61580.1046)  weight_decay: 0.0500 (0.0500)  time: 0.6492  data: 0.1769  max mem: 15572
Epoch: [7]  [1500/2809]  eta: 0:12:59  lr: 0.000046  min_lr: 0.000000  loss: 4.5390 (4.5384)  class_acc: 0.1667 (0.1741)  loss_scale: 65536.0000 (61606.4597)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0865  max mem: 15572
Epoch: [7]  [1510/2809]  eta: 0:12:53  lr: 0.000046  min_lr: 0.000000  loss: 4.6767 (4.5397)  class_acc: 0.1667 (0.1738)  loss_scale: 65536.0000 (61632.4659)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0773  max mem: 15572
Epoch: [7]  [1520/2809]  eta: 0:12:47  lr: 0.000046  min_lr: 0.000000  loss: 4.6654 (4.5394)  class_acc: 0.1667 (0.1740)  loss_scale: 65536.0000 (61658.1302)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1614  max mem: 15572
Epoch: [7]  [1530/2809]  eta: 0:12:41  lr: 0.000046  min_lr: 0.000000  loss: 4.4963 (4.5397)  class_acc: 0.1667 (0.1739)  loss_scale: 65536.0000 (61683.4592)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1229  max mem: 15572
Epoch: [7]  [1540/2809]  eta: 0:12:35  lr: 0.000046  min_lr: 0.000000  loss: 4.5804 (4.5396)  class_acc: 0.1250 (0.1738)  loss_scale: 65536.0000 (61708.4594)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1226  max mem: 15572
Epoch: [7]  [1550/2809]  eta: 0:12:29  lr: 0.000046  min_lr: 0.000000  loss: 4.5804 (4.5400)  class_acc: 0.0833 (0.1734)  loss_scale: 65536.0000 (61733.1373)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1222  max mem: 15572
[2025-01-15 18:02:47,121] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:02:47,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1560/2809]  eta: 0:12:23  lr: 0.000046  min_lr: 0.000000  loss: 4.5665 (4.5402)  class_acc: 0.1250 (0.1733)  loss_scale: 65536.0000 (61883.4491)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1268  max mem: 15572
[2025-01-15 18:02:51,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21228
[2025-01-15 18:02:51,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:02:51,903] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1570/2809]  eta: 0:12:17  lr: 0.000046  min_lr: 0.000000  loss: 4.5187 (4.5401)  class_acc: 0.1250 (0.1730)  loss_scale: 65536.0000 (62073.5633)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1349  max mem: 15572
Epoch: [7]  [1580/2809]  eta: 0:12:11  lr: 0.000046  min_lr: 0.000000  loss: 4.6616 (4.5410)  class_acc: 0.0833 (0.1728)  loss_scale: 65536.0000 (62095.4636)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1673  max mem: 15572
Epoch: [7]  [1590/2809]  eta: 0:12:05  lr: 0.000046  min_lr: 0.000000  loss: 4.6377 (4.5411)  class_acc: 0.1250 (0.1728)  loss_scale: 65536.0000 (62117.0886)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1577  max mem: 15572
Epoch: [7]  [1600/2809]  eta: 0:11:59  lr: 0.000046  min_lr: 0.000000  loss: 4.5577 (4.5411)  class_acc: 0.1250 (0.1728)  loss_scale: 65536.0000 (62138.4435)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0666  max mem: 15572
Epoch: [7]  [1610/2809]  eta: 0:11:53  lr: 0.000046  min_lr: 0.000000  loss: 4.5477 (4.5413)  class_acc: 0.1667 (0.1729)  loss_scale: 65536.0000 (62159.5332)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0613  max mem: 15572
Epoch: [7]  [1620/2809]  eta: 0:11:47  lr: 0.000046  min_lr: 0.000000  loss: 4.6295 (4.5421)  class_acc: 0.1667 (0.1728)  loss_scale: 65536.0000 (62180.3627)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1086  max mem: 15572
Epoch: [7]  [1630/2809]  eta: 0:11:42  lr: 0.000046  min_lr: 0.000000  loss: 4.6267 (4.5422)  class_acc: 0.1667 (0.1730)  loss_scale: 65536.0000 (62200.9368)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1605  max mem: 15572
Epoch: [7]  [1640/2809]  eta: 0:11:36  lr: 0.000046  min_lr: 0.000000  loss: 4.5925 (4.5424)  class_acc: 0.1667 (0.1729)  loss_scale: 65536.0000 (62221.2602)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1299  max mem: 15572
Epoch: [7]  [1650/2809]  eta: 0:11:30  lr: 0.000046  min_lr: 0.000000  loss: 4.5074 (4.5415)  class_acc: 0.2083 (0.1731)  loss_scale: 65536.0000 (62241.3374)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.1456  max mem: 15572
Epoch: [7]  [1660/2809]  eta: 0:11:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4244 (4.5409)  class_acc: 0.2083 (0.1733)  loss_scale: 65536.0000 (62261.1728)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1105  max mem: 15572
Epoch: [7]  [1670/2809]  eta: 0:11:17  lr: 0.000046  min_lr: 0.000000  loss: 4.4486 (4.5407)  class_acc: 0.2083 (0.1737)  loss_scale: 65536.0000 (62280.7708)  weight_decay: 0.0500 (0.0500)  time: 0.5269  data: 0.0594  max mem: 15572
Epoch: [7]  [1680/2809]  eta: 0:11:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4880 (4.5406)  class_acc: 0.2083 (0.1736)  loss_scale: 65536.0000 (62300.1356)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1634  max mem: 15572
Epoch: [7]  [1690/2809]  eta: 0:11:06  lr: 0.000046  min_lr: 0.000000  loss: 4.4935 (4.5403)  class_acc: 0.1667 (0.1737)  loss_scale: 65536.0000 (62319.2714)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.1692  max mem: 15572
[2025-01-15 18:04:10,227] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:04:10,227] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:04:13,210] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21363
[2025-01-15 18:04:13,210] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:04:13,211] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1700/2809]  eta: 0:11:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4935 (4.5398)  class_acc: 0.2083 (0.1740)  loss_scale: 65536.0000 (62569.3498)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1342  max mem: 15572
Epoch: [7]  [1710/2809]  eta: 0:10:54  lr: 0.000046  min_lr: 0.000000  loss: 4.5034 (4.5399)  class_acc: 0.2083 (0.1744)  loss_scale: 65536.0000 (62586.6885)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1370  max mem: 15572
Epoch: [7]  [1720/2809]  eta: 0:10:47  lr: 0.000046  min_lr: 0.000000  loss: 4.5715 (4.5397)  class_acc: 0.1667 (0.1742)  loss_scale: 65536.0000 (62603.8257)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1002  max mem: 15572
Epoch: [7]  [1730/2809]  eta: 0:10:41  lr: 0.000046  min_lr: 0.000000  loss: 4.6148 (4.5401)  class_acc: 0.1250 (0.1741)  loss_scale: 65536.0000 (62620.7649)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1067  max mem: 15572
Epoch: [7]  [1740/2809]  eta: 0:10:36  lr: 0.000046  min_lr: 0.000000  loss: 4.6439 (4.5404)  class_acc: 0.1250 (0.1739)  loss_scale: 65536.0000 (62637.5095)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1551  max mem: 15572
Epoch: [7]  [1750/2809]  eta: 0:10:29  lr: 0.000046  min_lr: 0.000000  loss: 4.6006 (4.5397)  class_acc: 0.1250 (0.1737)  loss_scale: 65536.0000 (62654.0628)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1259  max mem: 15572
Epoch: [7]  [1760/2809]  eta: 0:10:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4405 (4.5400)  class_acc: 0.1250 (0.1737)  loss_scale: 65536.0000 (62670.4282)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0343  max mem: 15572
Epoch: [7]  [1770/2809]  eta: 0:10:17  lr: 0.000046  min_lr: 0.000000  loss: 4.5477 (4.5398)  class_acc: 0.1250 (0.1736)  loss_scale: 65536.0000 (62686.6087)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0810  max mem: 15572
Epoch: [7]  [1780/2809]  eta: 0:10:12  lr: 0.000046  min_lr: 0.000000  loss: 4.5889 (4.5399)  class_acc: 0.1667 (0.1736)  loss_scale: 65536.0000 (62702.6075)  weight_decay: 0.0500 (0.0500)  time: 0.6961  data: 0.2243  max mem: 15572
Epoch: [7]  [1790/2809]  eta: 0:10:06  lr: 0.000046  min_lr: 0.000000  loss: 4.6416 (4.5404)  class_acc: 0.1250 (0.1733)  loss_scale: 65536.0000 (62718.4277)  weight_decay: 0.0500 (0.0500)  time: 0.6797  data: 0.1783  max mem: 15572
Epoch: [7]  [1800/2809]  eta: 0:10:00  lr: 0.000046  min_lr: 0.000000  loss: 4.6562 (4.5403)  class_acc: 0.1250 (0.1735)  loss_scale: 65536.0000 (62734.0722)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1097  max mem: 15572
Epoch: [7]  [1810/2809]  eta: 0:09:54  lr: 0.000046  min_lr: 0.000000  loss: 4.6232 (4.5409)  class_acc: 0.1667 (0.1736)  loss_scale: 65536.0000 (62749.5439)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1048  max mem: 15572
Epoch: [7]  [1820/2809]  eta: 0:09:47  lr: 0.000046  min_lr: 0.000000  loss: 4.6039 (4.5407)  class_acc: 0.1667 (0.1737)  loss_scale: 65536.0000 (62764.8457)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0871  max mem: 15572
[2025-01-15 18:05:29,550] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:05:29,550] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1830/2809]  eta: 0:09:42  lr: 0.000046  min_lr: 0.000000  loss: 4.5804 (4.5408)  class_acc: 0.1667 (0.1740)  loss_scale: 65536.0000 (62851.5653)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1849  max mem: 15572
[2025-01-15 18:05:31,954] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21496
[2025-01-15 18:05:31,954] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:05:31,954] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1840/2809]  eta: 0:09:36  lr: 0.000046  min_lr: 0.000000  loss: 4.5804 (4.5409)  class_acc: 0.1667 (0.1742)  loss_scale: 65536.0000 (62937.3427)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1566  max mem: 15572
Epoch: [7]  [1850/2809]  eta: 0:09:29  lr: 0.000046  min_lr: 0.000000  loss: 4.4539 (4.5405)  class_acc: 0.1667 (0.1743)  loss_scale: 65536.0000 (62951.3820)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0838  max mem: 15572
Epoch: [7]  [1860/2809]  eta: 0:09:24  lr: 0.000046  min_lr: 0.000000  loss: 4.4549 (4.5404)  class_acc: 0.1667 (0.1743)  loss_scale: 65536.0000 (62965.2703)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1586  max mem: 15572
Epoch: [7]  [1870/2809]  eta: 0:09:18  lr: 0.000046  min_lr: 0.000000  loss: 4.5144 (4.5406)  class_acc: 0.1667 (0.1742)  loss_scale: 65536.0000 (62979.0102)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1345  max mem: 15572
Epoch: [7]  [1880/2809]  eta: 0:09:12  lr: 0.000046  min_lr: 0.000000  loss: 4.5442 (4.5403)  class_acc: 0.1667 (0.1744)  loss_scale: 65536.0000 (62992.6039)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1391  max mem: 15572
Epoch: [7]  [1890/2809]  eta: 0:09:06  lr: 0.000046  min_lr: 0.000000  loss: 4.4978 (4.5402)  class_acc: 0.1667 (0.1742)  loss_scale: 65536.0000 (63006.0539)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1828  max mem: 15572
Epoch: [7]  [1900/2809]  eta: 0:09:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4064 (4.5397)  class_acc: 0.1667 (0.1743)  loss_scale: 65536.0000 (63019.3624)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1099  max mem: 15572
Epoch: [7]  [1910/2809]  eta: 0:08:54  lr: 0.000046  min_lr: 0.000000  loss: 4.4031 (4.5395)  class_acc: 0.2083 (0.1744)  loss_scale: 65536.0000 (63032.5317)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0677  max mem: 15572
Epoch: [7]  [1920/2809]  eta: 0:08:48  lr: 0.000046  min_lr: 0.000000  loss: 4.5363 (4.5398)  class_acc: 0.1667 (0.1743)  loss_scale: 65536.0000 (63045.5638)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0817  max mem: 15572
Epoch: [7]  [1930/2809]  eta: 0:08:41  lr: 0.000046  min_lr: 0.000000  loss: 4.5572 (4.5396)  class_acc: 0.1667 (0.1746)  loss_scale: 65536.0000 (63058.4609)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1009  max mem: 15572
Epoch: [7]  [1940/2809]  eta: 0:08:36  lr: 0.000046  min_lr: 0.000000  loss: 4.5283 (4.5398)  class_acc: 0.2083 (0.1750)  loss_scale: 65536.0000 (63071.2251)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1488  max mem: 15572
Epoch: [7]  [1950/2809]  eta: 0:08:30  lr: 0.000046  min_lr: 0.000000  loss: 4.4290 (4.5392)  class_acc: 0.2083 (0.1750)  loss_scale: 65536.0000 (63083.8585)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1806  max mem: 15572
Epoch: [7]  [1960/2809]  eta: 0:08:24  lr: 0.000046  min_lr: 0.000000  loss: 4.5581 (4.5399)  class_acc: 0.1667 (0.1751)  loss_scale: 65536.0000 (63096.3631)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.1733  max mem: 15572
[2025-01-15 18:06:46,869] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:06:46,869] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:06:47,729] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21627
[2025-01-15 18:06:47,730] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:06:47,730] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1970/2809]  eta: 0:08:18  lr: 0.000046  min_lr: 0.000000  loss: 4.6301 (4.5399)  class_acc: 0.1667 (0.1750)  loss_scale: 65536.0000 (63175.2410)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1485  max mem: 15572
Epoch: [7]  [1980/2809]  eta: 0:08:12  lr: 0.000046  min_lr: 0.000000  loss: 4.5738 (4.5398)  class_acc: 0.1667 (0.1749)  loss_scale: 65536.0000 (63187.1580)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1064  max mem: 15572
Epoch: [7]  [1990/2809]  eta: 0:08:06  lr: 0.000046  min_lr: 0.000000  loss: 4.5302 (4.5396)  class_acc: 0.1667 (0.1749)  loss_scale: 65536.0000 (63198.9553)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1098  max mem: 15572
Epoch: [7]  [2000/2809]  eta: 0:08:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4016 (4.5388)  class_acc: 0.1667 (0.1748)  loss_scale: 65536.0000 (63210.6347)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1288  max mem: 15572
Epoch: [7]  [2010/2809]  eta: 0:07:54  lr: 0.000046  min_lr: 0.000000  loss: 4.4066 (4.5387)  class_acc: 0.1250 (0.1747)  loss_scale: 65536.0000 (63222.1979)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1015  max mem: 15572
Epoch: [7]  [2020/2809]  eta: 0:07:48  lr: 0.000046  min_lr: 0.000000  loss: 4.6360 (4.5397)  class_acc: 0.1250 (0.1745)  loss_scale: 65536.0000 (63233.6467)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0908  max mem: 15572
Epoch: [7]  [2030/2809]  eta: 0:07:42  lr: 0.000046  min_lr: 0.000000  loss: 4.7301 (4.5402)  class_acc: 0.1250 (0.1744)  loss_scale: 65536.0000 (63244.9828)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1216  max mem: 15572
Epoch: [7]  [2040/2809]  eta: 0:07:36  lr: 0.000046  min_lr: 0.000000  loss: 4.5290 (4.5399)  class_acc: 0.1250 (0.1744)  loss_scale: 65536.0000 (63256.2077)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1380  max mem: 15572
Epoch: [7]  [2050/2809]  eta: 0:07:30  lr: 0.000046  min_lr: 0.000000  loss: 4.5222 (4.5400)  class_acc: 0.1250 (0.1742)  loss_scale: 65536.0000 (63267.3233)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1806  max mem: 15572
Epoch: [7]  [2060/2809]  eta: 0:07:24  lr: 0.000046  min_lr: 0.000000  loss: 4.5253 (4.5401)  class_acc: 0.1250 (0.1742)  loss_scale: 65536.0000 (63278.3309)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1736  max mem: 15572
Epoch: [7]  [2070/2809]  eta: 0:07:18  lr: 0.000046  min_lr: 0.000000  loss: 4.5208 (4.5401)  class_acc: 0.1667 (0.1743)  loss_scale: 65536.0000 (63289.2323)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1500  max mem: 15572
Epoch: [7]  [2080/2809]  eta: 0:07:13  lr: 0.000046  min_lr: 0.000000  loss: 4.4407 (4.5396)  class_acc: 0.1667 (0.1740)  loss_scale: 65536.0000 (63300.0288)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1988  max mem: 15572
Epoch: [7]  [2090/2809]  eta: 0:07:07  lr: 0.000046  min_lr: 0.000000  loss: 4.4809 (4.5395)  class_acc: 0.1667 (0.1742)  loss_scale: 65536.0000 (63310.7221)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.2205  max mem: 15572
[2025-01-15 18:08:05,106] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:08:05,106] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [2100/2809]  eta: 0:07:01  lr: 0.000046  min_lr: 0.000000  loss: 4.5410 (4.5395)  class_acc: 0.1667 (0.1741)  loss_scale: 65536.0000 (63570.8558)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1391  max mem: 15572
[2025-01-15 18:08:09,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21764
[2025-01-15 18:08:09,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:08:09,482] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2110/2809]  eta: 0:06:55  lr: 0.000046  min_lr: 0.000000  loss: 4.5304 (4.5397)  class_acc: 0.1250 (0.1742)  loss_scale: 65536.0000 (63580.1649)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0815  max mem: 15572
Epoch: [7]  [2120/2809]  eta: 0:06:49  lr: 0.000046  min_lr: 0.000000  loss: 4.5964 (4.5400)  class_acc: 0.1667 (0.1742)  loss_scale: 65536.0000 (63589.3861)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1150  max mem: 15572
Epoch: [7]  [2130/2809]  eta: 0:06:43  lr: 0.000046  min_lr: 0.000000  loss: 4.5580 (4.5393)  class_acc: 0.2083 (0.1745)  loss_scale: 65536.0000 (63598.5209)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1371  max mem: 15572
Epoch: [7]  [2140/2809]  eta: 0:06:37  lr: 0.000046  min_lr: 0.000000  loss: 4.5451 (4.5400)  class_acc: 0.1667 (0.1743)  loss_scale: 65536.0000 (63607.5703)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1385  max mem: 15572
Epoch: [7]  [2150/2809]  eta: 0:06:32  lr: 0.000046  min_lr: 0.000000  loss: 4.6051 (4.5402)  class_acc: 0.0833 (0.1742)  loss_scale: 65536.0000 (63616.5356)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.1584  max mem: 15572
Epoch: [7]  [2160/2809]  eta: 0:06:25  lr: 0.000046  min_lr: 0.000000  loss: 4.6051 (4.5404)  class_acc: 0.1250 (0.1740)  loss_scale: 65536.0000 (63625.4179)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1160  max mem: 15572
Epoch: [7]  [2170/2809]  eta: 0:06:19  lr: 0.000046  min_lr: 0.000000  loss: 4.6233 (4.5402)  class_acc: 0.1250 (0.1740)  loss_scale: 65536.0000 (63634.2183)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0826  max mem: 15572
Epoch: [7]  [2180/2809]  eta: 0:06:14  lr: 0.000046  min_lr: 0.000000  loss: 4.4962 (4.5398)  class_acc: 0.1667 (0.1740)  loss_scale: 65536.0000 (63642.9381)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.1784  max mem: 15572
Epoch: [7]  [2190/2809]  eta: 0:06:08  lr: 0.000046  min_lr: 0.000000  loss: 4.3847 (4.5393)  class_acc: 0.1667 (0.1740)  loss_scale: 65536.0000 (63651.5783)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.1807  max mem: 15572
Epoch: [7]  [2200/2809]  eta: 0:06:02  lr: 0.000046  min_lr: 0.000000  loss: 4.4147 (4.5391)  class_acc: 0.0833 (0.1739)  loss_scale: 65536.0000 (63660.1399)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1793  max mem: 15572
[2025-01-15 18:09:12,832] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21867
[2025-01-15 18:09:12,832] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:09:12,833] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [2210/2809]  eta: 0:05:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5443 (4.5391)  class_acc: 0.1250 (0.1739)  loss_scale: 65536.0000 (63564.8810)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1487  max mem: 15572
Epoch: [7]  [2220/2809]  eta: 0:05:50  lr: 0.000046  min_lr: 0.000000  loss: 4.4632 (4.5382)  class_acc: 0.2083 (0.1739)  loss_scale: 32768.0000 (63426.2188)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1824  max mem: 15572
Epoch: [7]  [2230/2809]  eta: 0:05:44  lr: 0.000046  min_lr: 0.000000  loss: 4.3186 (4.5378)  class_acc: 0.2083 (0.1742)  loss_scale: 32768.0000 (63288.7996)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1544  max mem: 15572
Epoch: [7]  [2240/2809]  eta: 0:05:38  lr: 0.000046  min_lr: 0.000000  loss: 4.4364 (4.5377)  class_acc: 0.2083 (0.1743)  loss_scale: 32768.0000 (63152.6069)  weight_decay: 0.0500 (0.0500)  time: 0.4527  data: 0.0007  max mem: 15572
Epoch: [7]  [2250/2809]  eta: 0:05:32  lr: 0.000046  min_lr: 0.000000  loss: 4.5124 (4.5374)  class_acc: 0.1250 (0.1744)  loss_scale: 32768.0000 (63017.6242)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0435  max mem: 15572
Epoch: [7]  [2260/2809]  eta: 0:05:25  lr: 0.000046  min_lr: 0.000000  loss: 4.4316 (4.5371)  class_acc: 0.2083 (0.1747)  loss_scale: 32768.0000 (62883.8355)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0435  max mem: 15572
Epoch: [7]  [2270/2809]  eta: 0:05:20  lr: 0.000046  min_lr: 0.000000  loss: 4.5441 (4.5369)  class_acc: 0.2500 (0.1747)  loss_scale: 32768.0000 (62751.2250)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1399  max mem: 15572
Epoch: [7]  [2280/2809]  eta: 0:05:14  lr: 0.000046  min_lr: 0.000000  loss: 4.5827 (4.5376)  class_acc: 0.1667 (0.1746)  loss_scale: 32768.0000 (62619.7773)  weight_decay: 0.0500 (0.0500)  time: 0.7339  data: 0.2595  max mem: 15572
Epoch: [7]  [2290/2809]  eta: 0:05:08  lr: 0.000046  min_lr: 0.000000  loss: 4.5741 (4.5376)  class_acc: 0.1667 (0.1748)  loss_scale: 32768.0000 (62489.4771)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1406  max mem: 15572
Epoch: [7]  [2300/2809]  eta: 0:05:02  lr: 0.000046  min_lr: 0.000000  loss: 4.4714 (4.5374)  class_acc: 0.1667 (0.1748)  loss_scale: 32768.0000 (62360.3094)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0700  max mem: 15572
Epoch: [7]  [2310/2809]  eta: 0:04:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5238 (4.5373)  class_acc: 0.0833 (0.1745)  loss_scale: 32768.0000 (62232.2596)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0981  max mem: 15572
Epoch: [7]  [2320/2809]  eta: 0:04:50  lr: 0.000046  min_lr: 0.000000  loss: 4.5347 (4.5374)  class_acc: 0.0833 (0.1744)  loss_scale: 32768.0000 (62105.3132)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1415  max mem: 15572
Epoch: [7]  [2330/2809]  eta: 0:04:44  lr: 0.000046  min_lr: 0.000000  loss: 4.6234 (4.5375)  class_acc: 0.1667 (0.1745)  loss_scale: 32768.0000 (61979.4560)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1719  max mem: 15572
[2025-01-15 18:10:27,514] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:10:27,515] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 18:10:28,957] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=142, lr=[4.4687915103572624e-07, 4.4687915103572624e-07, 6.383987871938948e-07, 6.383987871938948e-07, 9.119982674198497e-07, 9.119982674198497e-07, 1.3028546677426425e-06, 1.3028546677426425e-06, 1.8612209539180608e-06, 1.8612209539180608e-06, 2.658887077025801e-06, 2.658887077025801e-06, 3.798410110036859e-06, 3.798410110036859e-06, 5.4263001571955135e-06, 5.4263001571955135e-06, 7.751857367422162e-06, 7.751857367422162e-06, 1.1074081953460234e-05, 1.1074081953460234e-05, 1.582011707637176e-05, 1.582011707637176e-05, 2.260016725195966e-05, 2.260016725195966e-05, 3.2285953217085234e-05, 3.2285953217085234e-05, 4.6122790310121765e-05, 4.6122790310121765e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 18:10:28,959] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=27.502364079577646, CurrSamplesPerSec=24.917927993358077, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [2340/2809]  eta: 0:04:38  lr: 0.000046  min_lr: 0.000000  loss: 4.5770 (4.5374)  class_acc: 0.1667 (0.1745)  loss_scale: 32768.0000 (61966.6536)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1513  max mem: 15572
Epoch: [7]  [2350/2809]  eta: 0:04:32  lr: 0.000046  min_lr: 0.000000  loss: 4.5856 (4.5376)  class_acc: 0.1250 (0.1743)  loss_scale: 65536.0000 (61981.8358)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1496  max mem: 15572
Epoch: [7]  [2360/2809]  eta: 0:04:26  lr: 0.000046  min_lr: 0.000000  loss: 4.4110 (4.5371)  class_acc: 0.1667 (0.1744)  loss_scale: 65536.0000 (61996.8895)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1320  max mem: 15572
Epoch: [7]  [2370/2809]  eta: 0:04:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3080 (4.5364)  class_acc: 0.2083 (0.1746)  loss_scale: 65536.0000 (62011.8161)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1346  max mem: 15572
Epoch: [7]  [2380/2809]  eta: 0:04:15  lr: 0.000046  min_lr: 0.000000  loss: 4.4771 (4.5367)  class_acc: 0.1667 (0.1745)  loss_scale: 65536.0000 (62026.6174)  weight_decay: 0.0500 (0.0500)  time: 0.6528  data: 0.1755  max mem: 15572
Epoch: [7]  [2390/2809]  eta: 0:04:09  lr: 0.000046  min_lr: 0.000000  loss: 4.4798 (4.5367)  class_acc: 0.1250 (0.1745)  loss_scale: 65536.0000 (62041.2949)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0962  max mem: 15572
Epoch: [7]  [2400/2809]  eta: 0:04:03  lr: 0.000046  min_lr: 0.000000  loss: 4.5398 (4.5370)  class_acc: 0.1667 (0.1746)  loss_scale: 65536.0000 (62055.8501)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0773  max mem: 15572
Epoch: [7]  [2410/2809]  eta: 0:03:57  lr: 0.000046  min_lr: 0.000000  loss: 4.5505 (4.5372)  class_acc: 0.1667 (0.1746)  loss_scale: 65536.0000 (62070.2845)  weight_decay: 0.0500 (0.0500)  time: 0.6546  data: 0.1836  max mem: 15572
Epoch: [7]  [2420/2809]  eta: 0:03:51  lr: 0.000046  min_lr: 0.000000  loss: 4.5317 (4.5367)  class_acc: 0.2083 (0.1749)  loss_scale: 65536.0000 (62084.5998)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1266  max mem: 15572
Epoch: [7]  [2430/2809]  eta: 0:03:45  lr: 0.000046  min_lr: 0.000000  loss: 4.4570 (4.5366)  class_acc: 0.2083 (0.1748)  loss_scale: 65536.0000 (62098.7972)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1332  max mem: 15572
Epoch: [7]  [2440/2809]  eta: 0:03:39  lr: 0.000046  min_lr: 0.000000  loss: 4.5014 (4.5365)  class_acc: 0.1667 (0.1748)  loss_scale: 65536.0000 (62112.8783)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1893  max mem: 15572
Epoch: [7]  [2450/2809]  eta: 0:03:33  lr: 0.000046  min_lr: 0.000000  loss: 4.4416 (4.5356)  class_acc: 0.1667 (0.1747)  loss_scale: 65536.0000 (62126.8446)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1593  max mem: 15572
Epoch: [7]  [2460/2809]  eta: 0:03:27  lr: 0.000046  min_lr: 0.000000  loss: 4.4572 (4.5356)  class_acc: 0.1667 (0.1750)  loss_scale: 65536.0000 (62140.6973)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1071  max mem: 15572
[2025-01-15 18:11:44,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:11:44,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:11:46,565] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22129
[2025-01-15 18:11:46,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:11:46,566] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2470/2809]  eta: 0:03:21  lr: 0.000046  min_lr: 0.000000  loss: 4.4760 (4.5355)  class_acc: 0.1667 (0.1750)  loss_scale: 65536.0000 (62287.0482)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1319  max mem: 15572
Epoch: [7]  [2480/2809]  eta: 0:03:15  lr: 0.000046  min_lr: 0.000000  loss: 4.4531 (4.5352)  class_acc: 0.1667 (0.1750)  loss_scale: 65536.0000 (62300.1435)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1084  max mem: 15572
Epoch: [7]  [2490/2809]  eta: 0:03:09  lr: 0.000046  min_lr: 0.000000  loss: 4.4966 (4.5352)  class_acc: 0.1667 (0.1749)  loss_scale: 65536.0000 (62313.1337)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0188  max mem: 15572
Epoch: [7]  [2500/2809]  eta: 0:03:03  lr: 0.000046  min_lr: 0.000000  loss: 4.4529 (4.5342)  class_acc: 0.1667 (0.1750)  loss_scale: 65536.0000 (62326.0200)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1150  max mem: 15572
Epoch: [7]  [2510/2809]  eta: 0:02:57  lr: 0.000046  min_lr: 0.000000  loss: 4.4203 (4.5335)  class_acc: 0.1667 (0.1749)  loss_scale: 65536.0000 (62338.8037)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1319  max mem: 15572
Epoch: [7]  [2520/2809]  eta: 0:02:51  lr: 0.000046  min_lr: 0.000000  loss: 4.4441 (4.5336)  class_acc: 0.0833 (0.1747)  loss_scale: 65536.0000 (62351.4859)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0356  max mem: 15572
Epoch: [7]  [2530/2809]  eta: 0:02:45  lr: 0.000046  min_lr: 0.000000  loss: 4.5090 (4.5333)  class_acc: 0.1667 (0.1747)  loss_scale: 65536.0000 (62364.0680)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1345  max mem: 15572
Epoch: [7]  [2540/2809]  eta: 0:02:39  lr: 0.000046  min_lr: 0.000000  loss: 4.4739 (4.5331)  class_acc: 0.1667 (0.1747)  loss_scale: 65536.0000 (62376.5510)  weight_decay: 0.0500 (0.0500)  time: 0.6683  data: 0.2096  max mem: 15572
Epoch: [7]  [2550/2809]  eta: 0:02:33  lr: 0.000046  min_lr: 0.000000  loss: 4.5140 (4.5327)  class_acc: 0.1667 (0.1747)  loss_scale: 65536.0000 (62388.9361)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1097  max mem: 15572
Epoch: [7]  [2560/2809]  eta: 0:02:27  lr: 0.000046  min_lr: 0.000000  loss: 4.5140 (4.5329)  class_acc: 0.1667 (0.1746)  loss_scale: 65536.0000 (62401.2245)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0417  max mem: 15572
Epoch: [7]  [2570/2809]  eta: 0:02:21  lr: 0.000046  min_lr: 0.000000  loss: 4.5694 (4.5328)  class_acc: 0.1667 (0.1748)  loss_scale: 65536.0000 (62413.4173)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0437  max mem: 15572
Epoch: [7]  [2580/2809]  eta: 0:02:15  lr: 0.000046  min_lr: 0.000000  loss: 4.5650 (4.5327)  class_acc: 0.1667 (0.1749)  loss_scale: 65536.0000 (62425.5157)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1040  max mem: 15572
Epoch: [7]  [2590/2809]  eta: 0:02:10  lr: 0.000046  min_lr: 0.000000  loss: 4.5247 (4.5327)  class_acc: 0.1667 (0.1749)  loss_scale: 65536.0000 (62437.5206)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1808  max mem: 15572
[2025-01-15 18:13:01,755] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:13:01,755] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [2600/2809]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000000  loss: 4.5581 (4.5331)  class_acc: 0.1250 (0.1747)  loss_scale: 65536.0000 (62600.6121)  weight_decay: 0.0500 (0.0500)  time: 0.6683  data: 0.2243  max mem: 15572
[2025-01-15 18:13:06,559] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22264
[2025-01-15 18:13:06,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:13:06,559] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2610/2809]  eta: 0:01:58  lr: 0.000046  min_lr: 0.000000  loss: 4.6206 (4.5335)  class_acc: 0.1250 (0.1746)  loss_scale: 65536.0000 (62611.8545)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1870  max mem: 15572
Epoch: [7]  [2620/2809]  eta: 0:01:52  lr: 0.000046  min_lr: 0.000000  loss: 4.5578 (4.5336)  class_acc: 0.1250 (0.1746)  loss_scale: 65536.0000 (62623.0111)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1347  max mem: 15572
Epoch: [7]  [2630/2809]  eta: 0:01:46  lr: 0.000046  min_lr: 0.000000  loss: 4.5015 (4.5332)  class_acc: 0.1667 (0.1745)  loss_scale: 65536.0000 (62634.0829)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1219  max mem: 15572
Epoch: [7]  [2640/2809]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000000  loss: 4.4523 (4.5328)  class_acc: 0.1667 (0.1745)  loss_scale: 65536.0000 (62645.0708)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0804  max mem: 15572
Epoch: [7]  [2650/2809]  eta: 0:01:34  lr: 0.000046  min_lr: 0.000000  loss: 4.4986 (4.5325)  class_acc: 0.1667 (0.1745)  loss_scale: 65536.0000 (62655.9759)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1161  max mem: 15572
Epoch: [7]  [2660/2809]  eta: 0:01:28  lr: 0.000046  min_lr: 0.000000  loss: 4.5074 (4.5323)  class_acc: 0.1667 (0.1745)  loss_scale: 65536.0000 (62666.7989)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1460  max mem: 15572
Epoch: [7]  [2670/2809]  eta: 0:01:22  lr: 0.000046  min_lr: 0.000000  loss: 4.5312 (4.5324)  class_acc: 0.1667 (0.1747)  loss_scale: 65536.0000 (62677.5410)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1303  max mem: 15572
Epoch: [7]  [2680/2809]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000000  loss: 4.5591 (4.5322)  class_acc: 0.2083 (0.1748)  loss_scale: 65536.0000 (62688.2029)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0992  max mem: 15572
Epoch: [7]  [2690/2809]  eta: 0:01:10  lr: 0.000046  min_lr: 0.000000  loss: 4.5328 (4.5322)  class_acc: 0.1667 (0.1748)  loss_scale: 65536.0000 (62698.7856)  weight_decay: 0.0500 (0.0500)  time: 0.4604  data: 0.0167  max mem: 15572
Epoch: [7]  [2700/2809]  eta: 0:01:04  lr: 0.000046  min_lr: 0.000000  loss: 4.5975 (4.5328)  class_acc: 0.1250 (0.1746)  loss_scale: 65536.0000 (62709.2899)  weight_decay: 0.0500 (0.0500)  time: 0.4609  data: 0.0006  max mem: 15572
Epoch: [7]  [2710/2809]  eta: 0:00:58  lr: 0.000046  min_lr: 0.000000  loss: 4.5975 (4.5329)  class_acc: 0.1250 (0.1747)  loss_scale: 65536.0000 (62719.7167)  weight_decay: 0.0500 (0.0500)  time: 0.4942  data: 0.0009  max mem: 15572
Epoch: [7]  [2720/2809]  eta: 0:00:52  lr: 0.000046  min_lr: 0.000000  loss: 4.4793 (4.5330)  class_acc: 0.1667 (0.1745)  loss_scale: 65536.0000 (62730.0669)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0010  max mem: 15572
[2025-01-15 18:14:18,309] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:14:18,310] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [2730/2809]  eta: 0:00:46  lr: 0.000046  min_lr: 0.000000  loss: 4.5325 (4.5330)  class_acc: 0.1667 (0.1747)  loss_scale: 65536.0000 (62764.3383)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1090  max mem: 15572
[2025-01-15 18:14:18,942] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22394
[2025-01-15 18:14:18,942] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:14:18,942] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2740/2809]  eta: 0:00:40  lr: 0.000046  min_lr: 0.000000  loss: 4.5428 (4.5330)  class_acc: 0.1667 (0.1747)  loss_scale: 65536.0000 (62774.4502)  weight_decay: 0.0500 (0.0500)  time: 0.7186  data: 0.2361  max mem: 15572
[2025-01-15 18:14:27,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22407
[2025-01-15 18:14:27,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:14:27,688] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [2750/2809]  eta: 0:00:34  lr: 0.000046  min_lr: 0.000000  loss: 4.5522 (4.5331)  class_acc: 0.1667 (0.1747)  loss_scale: 65536.0000 (62701.1094)  weight_decay: 0.0500 (0.0500)  time: 0.7085  data: 0.2095  max mem: 15572
Epoch: [7]  [2760/2809]  eta: 0:00:29  lr: 0.000046  min_lr: 0.000000  loss: 4.5276 (4.5331)  class_acc: 0.2083 (0.1747)  loss_scale: 32768.0000 (62592.6954)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.1469  max mem: 15572
Epoch: [7]  [2770/2809]  eta: 0:00:23  lr: 0.000046  min_lr: 0.000000  loss: 4.5276 (4.5335)  class_acc: 0.1667 (0.1747)  loss_scale: 32768.0000 (62485.0639)  weight_decay: 0.0500 (0.0500)  time: 0.6877  data: 0.1994  max mem: 15572
Epoch: [7]  [2780/2809]  eta: 0:00:17  lr: 0.000046  min_lr: 0.000000  loss: 4.5732 (4.5341)  class_acc: 0.1667 (0.1747)  loss_scale: 32768.0000 (62378.2064)  weight_decay: 0.0500 (0.0500)  time: 0.7253  data: 0.2322  max mem: 15572
Epoch: [7]  [2790/2809]  eta: 0:00:11  lr: 0.000046  min_lr: 0.000000  loss: 4.5672 (4.5342)  class_acc: 0.1250 (0.1747)  loss_scale: 32768.0000 (62272.1147)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.1749  max mem: 15572
Epoch: [7]  [2800/2809]  eta: 0:00:05  lr: 0.000046  min_lr: 0.000000  loss: 4.5521 (4.5342)  class_acc: 0.1667 (0.1748)  loss_scale: 32768.0000 (62166.7804)  weight_decay: 0.0500 (0.0500)  time: 0.6858  data: 0.2344  max mem: 15572
Epoch: [7]  [2808/2809]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000000  loss: 4.5779 (4.5346)  class_acc: 0.1667 (0.1748)  loss_scale: 32768.0000 (62083.0530)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1870  max mem: 15572
Epoch: [7] Total time: 0:27:51 (0.5951 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 4.5779 (4.5346)  class_acc: 0.1667 (0.1748)  loss_scale: 32768.0000 (62083.0530)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:31:56  loss: 1.7157 (1.7157)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 7.0473  data: 6.8703  max mem: 15572
Val:  [ 10/272]  eta: 0:04:24  loss: 3.7493 (3.5645)  acc1: 5.5556 (22.7273)  acc5: 33.3333 (38.3838)  time: 1.0096  data: 0.8187  max mem: 15572
Val:  [ 20/272]  eta: 0:02:39  loss: 3.6229 (3.5524)  acc1: 16.6667 (22.2222)  acc5: 44.4444 (46.2963)  time: 0.3118  data: 0.1271  max mem: 15572
Val:  [ 30/272]  eta: 0:02:01  loss: 3.5871 (3.6233)  acc1: 11.1111 (17.5627)  acc5: 55.5556 (46.4158)  time: 0.2251  data: 0.0463  max mem: 15572
Val:  [ 40/272]  eta: 0:01:38  loss: 3.4089 (3.5667)  acc1: 11.1111 (18.2927)  acc5: 55.5556 (49.4580)  time: 0.2056  data: 0.0262  max mem: 15572
Val:  [ 50/272]  eta: 0:01:23  loss: 3.3631 (3.5188)  acc1: 16.6667 (20.6972)  acc5: 55.5556 (50.9804)  time: 0.1743  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:01:14  loss: 2.7460 (3.4344)  acc1: 50.0000 (25.5009)  acc5: 72.2222 (53.4608)  time: 0.1978  data: 0.0300  max mem: 15572
Val:  [ 70/272]  eta: 0:01:07  loss: 2.8213 (3.3602)  acc1: 50.0000 (27.9343)  acc5: 77.7778 (57.5900)  time: 0.2383  data: 0.0587  max mem: 15572
Val:  [ 80/272]  eta: 0:01:04  loss: 2.9308 (3.3466)  acc1: 38.8889 (28.7380)  acc5: 77.7778 (57.6818)  time: 0.2837  data: 0.0879  max mem: 15572
Val:  [ 90/272]  eta: 0:01:00  loss: 3.7322 (3.3896)  acc1: 11.1111 (26.8010)  acc5: 44.4444 (55.9219)  time: 0.3205  data: 0.1339  max mem: 15572
Val:  [100/272]  eta: 0:00:56  loss: 3.7322 (3.4389)  acc1: 11.1111 (25.9626)  acc5: 44.4444 (55.1155)  time: 0.3128  data: 0.1400  max mem: 15572
Val:  [110/272]  eta: 0:00:53  loss: 3.6122 (3.4724)  acc1: 16.6667 (25.0250)  acc5: 50.0000 (53.8038)  time: 0.3044  data: 0.1258  max mem: 15572
Val:  [120/272]  eta: 0:00:49  loss: 3.7282 (3.5034)  acc1: 11.1111 (24.2883)  acc5: 38.8889 (52.9844)  time: 0.3253  data: 0.1181  max mem: 15572
Val:  [130/272]  eta: 0:00:46  loss: 3.7282 (3.4832)  acc1: 16.6667 (25.5725)  acc5: 44.4444 (53.3079)  time: 0.3241  data: 0.1124  max mem: 15572
Val:  [140/272]  eta: 0:00:43  loss: 3.3359 (3.4834)  acc1: 27.7778 (25.9653)  acc5: 61.1111 (53.5855)  time: 0.3098  data: 0.1185  max mem: 15572
Val:  [150/272]  eta: 0:00:39  loss: 3.3840 (3.4748)  acc1: 16.6667 (25.5335)  acc5: 61.1111 (53.8999)  time: 0.3061  data: 0.1053  max mem: 15572
Val:  [160/272]  eta: 0:00:35  loss: 3.3215 (3.4574)  acc1: 22.2222 (26.3285)  acc5: 66.6667 (54.9689)  time: 0.2828  data: 0.0821  max mem: 15572
Val:  [170/272]  eta: 0:00:32  loss: 3.4918 (3.4809)  acc1: 16.6667 (25.2762)  acc5: 50.0000 (54.1585)  time: 0.3152  data: 0.1359  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 3.4295 (3.4677)  acc1: 16.6667 (25.2302)  acc5: 55.5556 (55.0338)  time: 0.3362  data: 0.1520  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 3.3098 (3.4932)  acc1: 16.6667 (24.7237)  acc5: 55.5556 (53.9267)  time: 0.3196  data: 0.1323  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.2855 (3.4932)  acc1: 16.6667 (24.6545)  acc5: 44.4444 (54.3118)  time: 0.3251  data: 0.1450  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 3.1771 (3.5039)  acc1: 22.2222 (24.8552)  acc5: 61.1111 (54.3971)  time: 0.3037  data: 0.1135  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 3.5371 (3.4979)  acc1: 22.2222 (25.0628)  acc5: 61.1111 (54.7009)  time: 0.3120  data: 0.1249  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.0907 (3.4808)  acc1: 38.8889 (26.0943)  acc5: 72.2222 (55.6518)  time: 0.3256  data: 0.1419  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.8100 (3.4632)  acc1: 38.8889 (26.6713)  acc5: 83.3333 (56.4776)  time: 0.3170  data: 0.1243  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.3213 (3.4855)  acc1: 27.7778 (26.3391)  acc5: 66.6667 (55.7326)  time: 0.3202  data: 0.1288  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.0019 (3.4456)  acc1: 44.4444 (28.2461)  acc5: 72.2222 (56.9391)  time: 0.3040  data: 0.1206  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.9152 (3.4472)  acc1: 50.0000 (27.9008)  acc5: 77.7778 (56.7856)  time: 0.2209  data: 0.0586  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.9152 (3.4513)  acc1: 44.4444 (27.8722)  acc5: 77.7778 (56.7479)  time: 0.2146  data: 0.0585  max mem: 15572
Val: Total time: 0:01:25 (0.3141 s / it)
* Acc@1 27.872 Acc@5 56.748 loss 3.451
Accuracy of the network on the 4883 val videos: 27.9%
[2025-01-15 18:16:36,607] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 18:16:36,609] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 18:16:36,609] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 18:16:39,706] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 18:16:39,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 27.87%
Epoch: [8]  [   0/2809]  eta: 5:33:09  lr: 0.000046  min_lr: 0.000000  loss: 4.5451 (4.5451)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.1164  data: 6.6928  max mem: 15572
Epoch: [8]  [  10/2809]  eta: 0:57:41  lr: 0.000046  min_lr: 0.000000  loss: 4.5180 (4.4236)  class_acc: 0.2083 (0.1705)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2366  data: 0.7762  max mem: 15572
Epoch: [8]  [  20/2809]  eta: 0:41:53  lr: 0.000046  min_lr: 0.000000  loss: 4.4973 (4.4472)  class_acc: 0.1667 (0.1706)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1493  max mem: 15572
Epoch: [8]  [  30/2809]  eta: 0:37:02  lr: 0.000046  min_lr: 0.000000  loss: 4.5621 (4.5303)  class_acc: 0.1250 (0.1559)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1192  max mem: 15572
Epoch: [8]  [  40/2809]  eta: 0:35:18  lr: 0.000046  min_lr: 0.000000  loss: 4.5822 (4.5228)  class_acc: 0.1250 (0.1514)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1718  max mem: 15572
Epoch: [8]  [  50/2809]  eta: 0:33:45  lr: 0.000046  min_lr: 0.000000  loss: 4.5211 (4.5260)  class_acc: 0.1250 (0.1503)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.1843  max mem: 15572
Epoch: [8]  [  60/2809]  eta: 0:32:09  lr: 0.000046  min_lr: 0.000000  loss: 4.5211 (4.5208)  class_acc: 0.1250 (0.1523)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1295  max mem: 15572
[2025-01-15 18:17:25,347] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:17:25,347] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [  70/2809]  eta: 0:31:07  lr: 0.000046  min_lr: 0.000000  loss: 4.5105 (4.5045)  class_acc: 0.1667 (0.1608)  loss_scale: 32768.0000 (35998.6479)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1072  max mem: 15572
Epoch: [8]  [  80/2809]  eta: 0:30:39  lr: 0.000046  min_lr: 0.000000  loss: 4.4710 (4.5120)  class_acc: 0.1667 (0.1610)  loss_scale: 65536.0000 (39645.2346)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1321  max mem: 15572
Epoch: [8]  [  90/2809]  eta: 0:29:59  lr: 0.000046  min_lr: 0.000000  loss: 4.5568 (4.5254)  class_acc: 0.1667 (0.1625)  loss_scale: 65536.0000 (42490.3736)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1440  max mem: 15572
Epoch: [8]  [ 100/2809]  eta: 0:29:33  lr: 0.000046  min_lr: 0.000000  loss: 4.5386 (4.5271)  class_acc: 0.1667 (0.1646)  loss_scale: 65536.0000 (44772.1188)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1376  max mem: 15572
Epoch: [8]  [ 110/2809]  eta: 0:29:04  lr: 0.000046  min_lr: 0.000000  loss: 4.4964 (4.5142)  class_acc: 0.2083 (0.1704)  loss_scale: 65536.0000 (46642.7387)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1382  max mem: 15572
Epoch: [8]  [ 120/2809]  eta: 0:28:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4465 (4.5070)  class_acc: 0.2500 (0.1798)  loss_scale: 65536.0000 (48204.1653)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0975  max mem: 15572
Epoch: [8]  [ 130/2809]  eta: 0:27:59  lr: 0.000046  min_lr: 0.000000  loss: 4.3954 (4.4986)  class_acc: 0.2500 (0.1826)  loss_scale: 65536.0000 (49527.2061)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0593  max mem: 15572
Epoch: [8]  [ 140/2809]  eta: 0:27:48  lr: 0.000046  min_lr: 0.000000  loss: 4.4370 (4.4992)  class_acc: 0.2083 (0.1841)  loss_scale: 65536.0000 (50662.5816)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1101  max mem: 15572
Epoch: [8]  [ 150/2809]  eta: 0:27:48  lr: 0.000046  min_lr: 0.000000  loss: 4.4949 (4.4973)  class_acc: 0.1667 (0.1824)  loss_scale: 65536.0000 (51647.5762)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1978  max mem: 15572
Epoch: [8]  [ 160/2809]  eta: 0:27:28  lr: 0.000046  min_lr: 0.000000  loss: 4.5543 (4.4995)  class_acc: 0.1667 (0.1801)  loss_scale: 65536.0000 (52510.2112)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1655  max mem: 15572
Epoch: [8]  [ 170/2809]  eta: 0:27:26  lr: 0.000046  min_lr: 0.000000  loss: 4.5638 (4.4998)  class_acc: 0.1667 (0.1784)  loss_scale: 65536.0000 (53271.9532)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1507  max mem: 15572
Epoch: [8]  [ 180/2809]  eta: 0:27:14  lr: 0.000046  min_lr: 0.000000  loss: 4.4178 (4.4924)  class_acc: 0.2083 (0.1798)  loss_scale: 65536.0000 (53949.5249)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1641  max mem: 15572
Epoch: [8]  [ 190/2809]  eta: 0:26:59  lr: 0.000046  min_lr: 0.000000  loss: 4.4222 (4.4955)  class_acc: 0.1667 (0.1782)  loss_scale: 65536.0000 (54556.1466)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1204  max mem: 15572
[2025-01-15 18:18:38,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:18:38,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:18:41,989] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22669
[2025-01-15 18:18:41,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:18:41,990] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 200/2809]  eta: 0:26:52  lr: 0.000046  min_lr: 0.000000  loss: 4.5199 (4.4971)  class_acc: 0.1667 (0.1793)  loss_scale: 65536.0000 (56732.6567)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1484  max mem: 15572
Epoch: [8]  [ 210/2809]  eta: 0:26:50  lr: 0.000046  min_lr: 0.000000  loss: 4.5002 (4.4931)  class_acc: 0.1667 (0.1799)  loss_scale: 65536.0000 (57149.8768)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.2014  max mem: 15572
Epoch: [8]  [ 220/2809]  eta: 0:26:45  lr: 0.000046  min_lr: 0.000000  loss: 4.4557 (4.4932)  class_acc: 0.2083 (0.1810)  loss_scale: 65536.0000 (57529.3394)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.1925  max mem: 15572
Epoch: [8]  [ 230/2809]  eta: 0:26:43  lr: 0.000046  min_lr: 0.000000  loss: 4.4557 (4.4905)  class_acc: 0.2083 (0.1831)  loss_scale: 65536.0000 (57875.9481)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.1844  max mem: 15572
Epoch: [8]  [ 240/2809]  eta: 0:26:27  lr: 0.000046  min_lr: 0.000000  loss: 4.4842 (4.4896)  class_acc: 0.1250 (0.1807)  loss_scale: 65536.0000 (58193.7925)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1456  max mem: 15572
Epoch: [8]  [ 250/2809]  eta: 0:26:25  lr: 0.000046  min_lr: 0.000000  loss: 4.4857 (4.4868)  class_acc: 0.1250 (0.1796)  loss_scale: 65536.0000 (58486.3108)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1474  max mem: 15572
Epoch: [8]  [ 260/2809]  eta: 0:26:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4822 (4.4888)  class_acc: 0.1667 (0.1801)  loss_scale: 65536.0000 (58756.4138)  weight_decay: 0.0500 (0.0500)  time: 0.7030  data: 0.2611  max mem: 15572
Epoch: [8]  [ 270/2809]  eta: 0:26:19  lr: 0.000046  min_lr: 0.000000  loss: 4.4822 (4.4869)  class_acc: 0.1667 (0.1796)  loss_scale: 65536.0000 (59006.5830)  weight_decay: 0.0500 (0.0500)  time: 0.6534  data: 0.2216  max mem: 15572
Epoch: [8]  [ 280/2809]  eta: 0:26:10  lr: 0.000046  min_lr: 0.000000  loss: 4.5414 (4.4894)  class_acc: 0.1667 (0.1790)  loss_scale: 65536.0000 (59238.9466)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1431  max mem: 15572
Epoch: [8]  [ 290/2809]  eta: 0:26:00  lr: 0.000046  min_lr: 0.000000  loss: 4.5571 (4.4902)  class_acc: 0.1667 (0.1784)  loss_scale: 65536.0000 (59455.3402)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1513  max mem: 15572
Epoch: [8]  [ 300/2809]  eta: 0:25:46  lr: 0.000046  min_lr: 0.000000  loss: 4.4472 (4.4883)  class_acc: 0.2083 (0.1808)  loss_scale: 65536.0000 (59657.3555)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1229  max mem: 15572
Epoch: [8]  [ 310/2809]  eta: 0:25:39  lr: 0.000046  min_lr: 0.000000  loss: 4.5332 (4.4911)  class_acc: 0.2083 (0.1809)  loss_scale: 65536.0000 (59846.3794)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1313  max mem: 15572
Epoch: [8]  [ 320/2809]  eta: 0:25:35  lr: 0.000046  min_lr: 0.000000  loss: 4.5293 (4.4905)  class_acc: 0.1250 (0.1806)  loss_scale: 65536.0000 (60023.6262)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1644  max mem: 15572
[2025-01-15 18:20:00,477] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:20:00,478] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 330/2809]  eta: 0:25:24  lr: 0.000046  min_lr: 0.000000  loss: 4.5155 (4.4930)  class_acc: 0.1250 (0.1795)  loss_scale: 65536.0000 (61180.1329)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1449  max mem: 15572
[2025-01-15 18:20:04,645] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22805
[2025-01-15 18:20:04,646] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:20:04,646] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 340/2809]  eta: 0:25:24  lr: 0.000046  min_lr: 0.000000  loss: 4.5566 (4.4940)  class_acc: 0.1667 (0.1808)  loss_scale: 65536.0000 (61692.2463)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1839  max mem: 15572
Epoch: [8]  [ 350/2809]  eta: 0:25:16  lr: 0.000046  min_lr: 0.000000  loss: 4.5171 (4.4933)  class_acc: 0.2083 (0.1808)  loss_scale: 65536.0000 (61801.7550)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.1791  max mem: 15572
Epoch: [8]  [ 360/2809]  eta: 0:25:08  lr: 0.000046  min_lr: 0.000000  loss: 4.3487 (4.4907)  class_acc: 0.1667 (0.1811)  loss_scale: 65536.0000 (61905.1967)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1103  max mem: 15572
Epoch: [8]  [ 370/2809]  eta: 0:25:03  lr: 0.000046  min_lr: 0.000000  loss: 4.4513 (4.4893)  class_acc: 0.1667 (0.1815)  loss_scale: 65536.0000 (62003.0620)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1494  max mem: 15572
Epoch: [8]  [ 380/2809]  eta: 0:24:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5583 (4.4919)  class_acc: 0.1667 (0.1811)  loss_scale: 65536.0000 (62095.7900)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1758  max mem: 15572
Epoch: [8]  [ 390/2809]  eta: 0:24:47  lr: 0.000046  min_lr: 0.000000  loss: 4.5696 (4.4923)  class_acc: 0.1667 (0.1808)  loss_scale: 65536.0000 (62183.7749)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1493  max mem: 15572
Epoch: [8]  [ 400/2809]  eta: 0:24:38  lr: 0.000046  min_lr: 0.000000  loss: 4.4087 (4.4890)  class_acc: 0.1667 (0.1806)  loss_scale: 65536.0000 (62267.3716)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1223  max mem: 15572
Epoch: [8]  [ 410/2809]  eta: 0:24:27  lr: 0.000046  min_lr: 0.000000  loss: 4.3255 (4.4870)  class_acc: 0.2083 (0.1817)  loss_scale: 65536.0000 (62346.9002)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1057  max mem: 15572
Epoch: [8]  [ 420/2809]  eta: 0:24:16  lr: 0.000046  min_lr: 0.000000  loss: 4.4991 (4.4894)  class_acc: 0.2083 (0.1820)  loss_scale: 65536.0000 (62422.6508)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0771  max mem: 15572
Epoch: [8]  [ 430/2809]  eta: 0:24:11  lr: 0.000046  min_lr: 0.000000  loss: 4.4991 (4.4898)  class_acc: 0.1667 (0.1819)  loss_scale: 65536.0000 (62494.8863)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1177  max mem: 15572
Epoch: [8]  [ 440/2809]  eta: 0:24:03  lr: 0.000046  min_lr: 0.000000  loss: 4.5530 (4.4912)  class_acc: 0.1250 (0.1803)  loss_scale: 65536.0000 (62563.8458)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1287  max mem: 15572
[2025-01-15 18:21:10,525] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22916
[2025-01-15 18:21:10,525] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:21:10,526] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 450/2809]  eta: 0:23:52  lr: 0.000046  min_lr: 0.000000  loss: 4.5821 (4.4918)  class_acc: 0.1250 (0.1802)  loss_scale: 65536.0000 (62121.1530)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0426  max mem: 15572
Epoch: [8]  [ 460/2809]  eta: 0:23:42  lr: 0.000046  min_lr: 0.000000  loss: 4.5769 (4.4927)  class_acc: 0.1250 (0.1800)  loss_scale: 32768.0000 (61484.4252)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0146  max mem: 15572
Epoch: [8]  [ 470/2809]  eta: 0:23:32  lr: 0.000046  min_lr: 0.000000  loss: 4.5350 (4.4923)  class_acc: 0.1250 (0.1796)  loss_scale: 32768.0000 (60874.7346)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0261  max mem: 15572
Epoch: [8]  [ 480/2809]  eta: 0:23:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4726 (4.4925)  class_acc: 0.1250 (0.1790)  loss_scale: 32768.0000 (60290.3950)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0640  max mem: 15572
Epoch: [8]  [ 490/2809]  eta: 0:23:14  lr: 0.000046  min_lr: 0.000000  loss: 4.4704 (4.4929)  class_acc: 0.2083 (0.1791)  loss_scale: 32768.0000 (59729.8574)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0689  max mem: 15572
Epoch: [8]  [ 500/2809]  eta: 0:23:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4751 (4.4932)  class_acc: 0.2083 (0.1788)  loss_scale: 32768.0000 (59191.6966)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1139  max mem: 15572
Epoch: [8]  [ 510/2809]  eta: 0:23:08  lr: 0.000046  min_lr: 0.000000  loss: 4.5698 (4.4941)  class_acc: 0.1667 (0.1778)  loss_scale: 32768.0000 (58674.5988)  weight_decay: 0.0500 (0.0500)  time: 0.6721  data: 0.1880  max mem: 15572
Epoch: [8]  [ 520/2809]  eta: 0:22:57  lr: 0.000046  min_lr: 0.000000  loss: 4.5433 (4.4951)  class_acc: 0.1250 (0.1769)  loss_scale: 32768.0000 (58177.3512)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1204  max mem: 15572
[2025-01-15 18:21:56,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=149, lr=[4.449446895224366e-07, 4.449446895224366e-07, 6.356352707463381e-07, 6.356352707463381e-07, 9.080503867804831e-07, 9.080503867804831e-07, 1.297214838257833e-06, 1.297214838257833e-06, 1.8531640546540472e-06, 1.8531640546540472e-06, 2.6473772209343534e-06, 2.6473772209343534e-06, 3.7819674584776477e-06, 3.7819674584776477e-06, 5.402810654968069e-06, 5.402810654968069e-06, 7.718300935668669e-06, 7.718300935668669e-06, 1.1026144193812387e-05, 1.1026144193812387e-05, 1.5751634562589122e-05, 1.5751634562589122e-05, 2.2502335089413037e-05, 2.2502335089413037e-05, 3.214619298487577e-05, 3.214619298487577e-05, 4.5923132835536815e-05, 4.5923132835536815e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 18:21:56,530] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=27.507421677793165, CurrSamplesPerSec=29.057041266806262, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [ 530/2809]  eta: 0:22:55  lr: 0.000046  min_lr: 0.000000  loss: 4.5361 (4.4960)  class_acc: 0.1667 (0.1778)  loss_scale: 32768.0000 (57698.8324)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1519  max mem: 15572
Epoch: [8]  [ 540/2809]  eta: 0:22:51  lr: 0.000046  min_lr: 0.000000  loss: 4.5694 (4.4976)  class_acc: 0.1667 (0.1775)  loss_scale: 32768.0000 (57238.0037)  weight_decay: 0.0500 (0.0500)  time: 0.6762  data: 0.2340  max mem: 15572
Epoch: [8]  [ 550/2809]  eta: 0:22:42  lr: 0.000046  min_lr: 0.000000  loss: 4.5717 (4.4989)  class_acc: 0.1667 (0.1775)  loss_scale: 32768.0000 (56793.9020)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1386  max mem: 15572
Epoch: [8]  [ 560/2809]  eta: 0:22:34  lr: 0.000046  min_lr: 0.000000  loss: 4.5419 (4.4988)  class_acc: 0.1250 (0.1781)  loss_scale: 32768.0000 (56365.6328)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0793  max mem: 15572
Epoch: [8]  [ 570/2809]  eta: 0:22:28  lr: 0.000046  min_lr: 0.000000  loss: 4.4054 (4.4976)  class_acc: 0.1250 (0.1781)  loss_scale: 32768.0000 (55952.3643)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1093  max mem: 15572
[2025-01-15 18:22:25,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:22:25,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 580/2809]  eta: 0:22:20  lr: 0.000046  min_lr: 0.000000  loss: 4.4249 (4.4963)  class_acc: 0.1667 (0.1787)  loss_scale: 32768.0000 (56004.5164)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1182  max mem: 15572
[2025-01-15 18:22:31,558] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23056
[2025-01-15 18:22:31,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:22:31,560] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 590/2809]  eta: 0:22:15  lr: 0.000046  min_lr: 0.000000  loss: 4.4754 (4.4948)  class_acc: 0.2083 (0.1795)  loss_scale: 65536.0000 (55777.6785)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1168  max mem: 15572
Epoch: [8]  [ 600/2809]  eta: 0:22:13  lr: 0.000046  min_lr: 0.000000  loss: 4.5091 (4.4946)  class_acc: 0.1667 (0.1795)  loss_scale: 32768.0000 (55394.8220)  weight_decay: 0.0500 (0.0500)  time: 0.6651  data: 0.1855  max mem: 15572
Epoch: [8]  [ 610/2809]  eta: 0:22:03  lr: 0.000046  min_lr: 0.000000  loss: 4.5190 (4.4953)  class_acc: 0.1250 (0.1789)  loss_scale: 32768.0000 (55024.4975)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1249  max mem: 15572
Epoch: [8]  [ 620/2809]  eta: 0:21:59  lr: 0.000046  min_lr: 0.000000  loss: 4.5256 (4.4961)  class_acc: 0.1250 (0.1782)  loss_scale: 32768.0000 (54666.0998)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0840  max mem: 15572
Epoch: [8]  [ 630/2809]  eta: 0:21:51  lr: 0.000046  min_lr: 0.000000  loss: 4.5669 (4.4967)  class_acc: 0.1250 (0.1777)  loss_scale: 32768.0000 (54319.0618)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1186  max mem: 15572
Epoch: [8]  [ 640/2809]  eta: 0:21:44  lr: 0.000046  min_lr: 0.000000  loss: 4.4671 (4.4946)  class_acc: 0.1667 (0.1784)  loss_scale: 32768.0000 (53982.8518)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1070  max mem: 15572
Epoch: [8]  [ 650/2809]  eta: 0:21:39  lr: 0.000046  min_lr: 0.000000  loss: 4.3959 (4.4935)  class_acc: 0.2083 (0.1788)  loss_scale: 32768.0000 (53656.9708)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1664  max mem: 15572
Epoch: [8]  [ 660/2809]  eta: 0:21:33  lr: 0.000046  min_lr: 0.000000  loss: 4.4028 (4.4930)  class_acc: 0.1667 (0.1785)  loss_scale: 32768.0000 (53340.9501)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1690  max mem: 15572
Epoch: [8]  [ 670/2809]  eta: 0:21:28  lr: 0.000046  min_lr: 0.000000  loss: 4.4366 (4.4938)  class_acc: 0.1250 (0.1786)  loss_scale: 32768.0000 (53034.3487)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1717  max mem: 15572
Epoch: [8]  [ 680/2809]  eta: 0:21:25  lr: 0.000046  min_lr: 0.000000  loss: 4.5809 (4.4960)  class_acc: 0.1667 (0.1784)  loss_scale: 32768.0000 (52736.7518)  weight_decay: 0.0500 (0.0500)  time: 0.6709  data: 0.2186  max mem: 15572
Epoch: [8]  [ 690/2809]  eta: 0:21:17  lr: 0.000046  min_lr: 0.000000  loss: 4.4720 (4.4949)  class_acc: 0.1667 (0.1789)  loss_scale: 32768.0000 (52447.7685)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1597  max mem: 15572
Epoch: [8]  [ 700/2809]  eta: 0:21:11  lr: 0.000046  min_lr: 0.000000  loss: 4.3542 (4.4936)  class_acc: 0.2083 (0.1791)  loss_scale: 32768.0000 (52167.0300)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.0974  max mem: 15572
Epoch: [8]  [ 710/2809]  eta: 0:21:07  lr: 0.000046  min_lr: 0.000000  loss: 4.3554 (4.4938)  class_acc: 0.2083 (0.1797)  loss_scale: 32768.0000 (51894.1885)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1344  max mem: 15572
[2025-01-15 18:23:50,610] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:23:50,610] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 720/2809]  eta: 0:20:58  lr: 0.000046  min_lr: 0.000000  loss: 4.4985 (4.4953)  class_acc: 0.1667 (0.1790)  loss_scale: 32768.0000 (51992.4993)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1015  max mem: 15572
Epoch: [8]  [ 730/2809]  eta: 0:20:53  lr: 0.000046  min_lr: 0.000000  loss: 4.5135 (4.4956)  class_acc: 0.1250 (0.1790)  loss_scale: 65536.0000 (52177.7729)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1119  max mem: 15572
Epoch: [8]  [ 740/2809]  eta: 0:20:46  lr: 0.000046  min_lr: 0.000000  loss: 4.3834 (4.4937)  class_acc: 0.1250 (0.1788)  loss_scale: 65536.0000 (52358.0459)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1478  max mem: 15572
Epoch: [8]  [ 750/2809]  eta: 0:20:39  lr: 0.000046  min_lr: 0.000000  loss: 4.4148 (4.4943)  class_acc: 0.1667 (0.1790)  loss_scale: 65536.0000 (52533.5180)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1280  max mem: 15572
Epoch: [8]  [ 760/2809]  eta: 0:20:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4699 (4.4935)  class_acc: 0.1667 (0.1793)  loss_scale: 65536.0000 (52704.3784)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1247  max mem: 15572
Epoch: [8]  [ 770/2809]  eta: 0:20:29  lr: 0.000046  min_lr: 0.000000  loss: 4.4648 (4.4941)  class_acc: 0.2083 (0.1797)  loss_scale: 65536.0000 (52870.8067)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.2007  max mem: 15572
Epoch: [8]  [ 780/2809]  eta: 0:20:23  lr: 0.000046  min_lr: 0.000000  loss: 4.5584 (4.4948)  class_acc: 0.2083 (0.1795)  loss_scale: 65536.0000 (53032.9731)  weight_decay: 0.0500 (0.0500)  time: 0.6623  data: 0.2211  max mem: 15572
Epoch: [8]  [ 790/2809]  eta: 0:20:17  lr: 0.000046  min_lr: 0.000000  loss: 4.5665 (4.4960)  class_acc: 0.1250 (0.1796)  loss_scale: 65536.0000 (53191.0392)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1627  max mem: 15572
Epoch: [8]  [ 800/2809]  eta: 0:20:11  lr: 0.000046  min_lr: 0.000000  loss: 4.5665 (4.4968)  class_acc: 0.2083 (0.1796)  loss_scale: 65536.0000 (53345.1586)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1137  max mem: 15572
Epoch: [8]  [ 810/2809]  eta: 0:20:03  lr: 0.000046  min_lr: 0.000000  loss: 4.6172 (4.4986)  class_acc: 0.1667 (0.1800)  loss_scale: 65536.0000 (53495.4772)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0676  max mem: 15572
Epoch: [8]  [ 820/2809]  eta: 0:19:57  lr: 0.000046  min_lr: 0.000000  loss: 4.5999 (4.4968)  class_acc: 0.1667 (0.1802)  loss_scale: 65536.0000 (53642.1340)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1048  max mem: 15572
Epoch: [8]  [ 830/2809]  eta: 0:19:51  lr: 0.000046  min_lr: 0.000000  loss: 4.3965 (4.4958)  class_acc: 0.1667 (0.1800)  loss_scale: 65536.0000 (53785.2611)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1393  max mem: 15572
Epoch: [8]  [ 840/2809]  eta: 0:19:43  lr: 0.000046  min_lr: 0.000000  loss: 4.5385 (4.4973)  class_acc: 0.1250 (0.1796)  loss_scale: 65536.0000 (53924.9845)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0962  max mem: 15572
[2025-01-15 18:25:06,156] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:25:06,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:25:06,603] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23314
[2025-01-15 18:25:06,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:25:06,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 850/2809]  eta: 0:19:35  lr: 0.000046  min_lr: 0.000000  loss: 4.5385 (4.4967)  class_acc: 0.1250 (0.1797)  loss_scale: 65536.0000 (54138.4348)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0475  max mem: 15572
Epoch: [8]  [ 860/2809]  eta: 0:19:28  lr: 0.000046  min_lr: 0.000000  loss: 4.4894 (4.4966)  class_acc: 0.1667 (0.1795)  loss_scale: 65536.0000 (54270.8107)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0559  max mem: 15572
Epoch: [8]  [ 870/2809]  eta: 0:19:22  lr: 0.000046  min_lr: 0.000000  loss: 4.4983 (4.4966)  class_acc: 0.1667 (0.1796)  loss_scale: 65536.0000 (54400.1470)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1092  max mem: 15572
Epoch: [8]  [ 880/2809]  eta: 0:19:15  lr: 0.000046  min_lr: 0.000000  loss: 4.4698 (4.4960)  class_acc: 0.2083 (0.1798)  loss_scale: 65536.0000 (54526.5471)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1275  max mem: 15572
Epoch: [8]  [ 890/2809]  eta: 0:19:09  lr: 0.000046  min_lr: 0.000000  loss: 4.4698 (4.4959)  class_acc: 0.1667 (0.1795)  loss_scale: 65536.0000 (54650.1100)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1296  max mem: 15572
Epoch: [8]  [ 900/2809]  eta: 0:19:04  lr: 0.000046  min_lr: 0.000000  loss: 4.4791 (4.4955)  class_acc: 0.1667 (0.1793)  loss_scale: 65536.0000 (54770.9301)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.1953  max mem: 15572
[2025-01-15 18:25:41,203] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23374
[2025-01-15 18:25:41,203] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:25:41,203] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 910/2809]  eta: 0:19:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4707 (4.4954)  class_acc: 0.1667 (0.1796)  loss_scale: 65536.0000 (54565.3743)  weight_decay: 0.0500 (0.0500)  time: 0.6654  data: 0.2398  max mem: 15572
Epoch: [8]  [ 920/2809]  eta: 0:18:54  lr: 0.000046  min_lr: 0.000000  loss: 4.6234 (4.4972)  class_acc: 0.2083 (0.1799)  loss_scale: 32768.0000 (54328.7036)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.2108  max mem: 15572
Epoch: [8]  [ 930/2809]  eta: 0:18:48  lr: 0.000046  min_lr: 0.000000  loss: 4.5010 (4.4942)  class_acc: 0.2083 (0.1806)  loss_scale: 32768.0000 (54097.1171)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1471  max mem: 15572
Epoch: [8]  [ 940/2809]  eta: 0:18:41  lr: 0.000046  min_lr: 0.000000  loss: 4.4152 (4.4941)  class_acc: 0.2083 (0.1807)  loss_scale: 32768.0000 (53870.4527)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1151  max mem: 15572
Epoch: [8]  [ 950/2809]  eta: 0:18:35  lr: 0.000046  min_lr: 0.000000  loss: 4.4572 (4.4929)  class_acc: 0.1667 (0.1810)  loss_scale: 32768.0000 (53648.5552)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1112  max mem: 15572
Epoch: [8]  [ 960/2809]  eta: 0:18:30  lr: 0.000046  min_lr: 0.000000  loss: 4.4542 (4.4924)  class_acc: 0.1667 (0.1810)  loss_scale: 32768.0000 (53431.2758)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1518  max mem: 15572
Epoch: [8]  [ 970/2809]  eta: 0:18:23  lr: 0.000046  min_lr: 0.000000  loss: 4.5640 (4.4927)  class_acc: 0.1667 (0.1814)  loss_scale: 32768.0000 (53218.4717)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1497  max mem: 15572
Epoch: [8]  [ 980/2809]  eta: 0:18:15  lr: 0.000046  min_lr: 0.000000  loss: 4.6008 (4.4933)  class_acc: 0.1250 (0.1812)  loss_scale: 32768.0000 (53010.0061)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0734  max mem: 15572
Epoch: [8]  [ 990/2809]  eta: 0:18:09  lr: 0.000046  min_lr: 0.000000  loss: 4.6179 (4.4946)  class_acc: 0.1250 (0.1807)  loss_scale: 32768.0000 (52805.7477)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1011  max mem: 15572
Epoch: [8]  [1000/2809]  eta: 0:18:01  lr: 0.000046  min_lr: 0.000000  loss: 4.5710 (4.4948)  class_acc: 0.1250 (0.1809)  loss_scale: 32768.0000 (52605.5704)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0904  max mem: 15572
Epoch: [8]  [1010/2809]  eta: 0:17:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5710 (4.4956)  class_acc: 0.1667 (0.1809)  loss_scale: 32768.0000 (52409.3531)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0909  max mem: 15572
Epoch: [8]  [1020/2809]  eta: 0:17:52  lr: 0.000046  min_lr: 0.000000  loss: 4.6366 (4.4968)  class_acc: 0.2083 (0.1814)  loss_scale: 32768.0000 (52216.9794)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.2177  max mem: 15572
Epoch: [8]  [1030/2809]  eta: 0:17:47  lr: 0.000046  min_lr: 0.000000  loss: 4.5574 (4.4970)  class_acc: 0.2083 (0.1817)  loss_scale: 32768.0000 (52028.3375)  weight_decay: 0.0500 (0.0500)  time: 0.6838  data: 0.2284  max mem: 15572
[2025-01-15 18:26:58,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:26:58,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [1040/2809]  eta: 0:17:41  lr: 0.000046  min_lr: 0.000000  loss: 4.5719 (4.4979)  class_acc: 0.1667 (0.1814)  loss_scale: 32768.0000 (52158.0941)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.1670  max mem: 15572
Epoch: [8]  [1050/2809]  eta: 0:17:34  lr: 0.000046  min_lr: 0.000000  loss: 4.5091 (4.4972)  class_acc: 0.1250 (0.1813)  loss_scale: 65536.0000 (52285.3815)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1308  max mem: 15572
Epoch: [8]  [1060/2809]  eta: 0:17:30  lr: 0.000046  min_lr: 0.000000  loss: 4.4243 (4.4966)  class_acc: 0.2083 (0.1815)  loss_scale: 65536.0000 (52410.2696)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.1639  max mem: 15572
Epoch: [8]  [1070/2809]  eta: 0:17:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4243 (4.4965)  class_acc: 0.1667 (0.1813)  loss_scale: 65536.0000 (52532.8254)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1476  max mem: 15572
Epoch: [8]  [1080/2809]  eta: 0:17:16  lr: 0.000046  min_lr: 0.000000  loss: 4.4470 (4.4967)  class_acc: 0.1667 (0.1814)  loss_scale: 65536.0000 (52653.1138)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0702  max mem: 15572
Epoch: [8]  [1090/2809]  eta: 0:17:09  lr: 0.000046  min_lr: 0.000000  loss: 4.5114 (4.4986)  class_acc: 0.1250 (0.1810)  loss_scale: 65536.0000 (52771.1971)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0639  max mem: 15572
Epoch: [8]  [1100/2809]  eta: 0:17:03  lr: 0.000046  min_lr: 0.000000  loss: 4.5114 (4.4975)  class_acc: 0.1667 (0.1814)  loss_scale: 65536.0000 (52887.1353)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1129  max mem: 15572
Epoch: [8]  [1110/2809]  eta: 0:16:59  lr: 0.000046  min_lr: 0.000000  loss: 4.4664 (4.4974)  class_acc: 0.2083 (0.1817)  loss_scale: 65536.0000 (53000.9865)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.2259  max mem: 15572
Epoch: [8]  [1120/2809]  eta: 0:16:52  lr: 0.000046  min_lr: 0.000000  loss: 4.4816 (4.4964)  class_acc: 0.1667 (0.1822)  loss_scale: 65536.0000 (53112.8064)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1678  max mem: 15572
Epoch: [8]  [1130/2809]  eta: 0:16:46  lr: 0.000046  min_lr: 0.000000  loss: 4.4746 (4.4964)  class_acc: 0.2083 (0.1824)  loss_scale: 65536.0000 (53222.6490)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1044  max mem: 15572
Epoch: [8]  [1140/2809]  eta: 0:16:41  lr: 0.000046  min_lr: 0.000000  loss: 4.5369 (4.4970)  class_acc: 0.1667 (0.1819)  loss_scale: 65536.0000 (53330.5662)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.1729  max mem: 15572
Epoch: [8]  [1150/2809]  eta: 0:16:35  lr: 0.000046  min_lr: 0.000000  loss: 4.6025 (4.4982)  class_acc: 0.1250 (0.1817)  loss_scale: 65536.0000 (53436.6082)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1620  max mem: 15572
[2025-01-15 18:28:16,411] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:28:16,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [1160/2809]  eta: 0:16:29  lr: 0.000046  min_lr: 0.000000  loss: 4.3858 (4.4973)  class_acc: 0.0833 (0.1813)  loss_scale: 65536.0000 (53653.7192)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1342  max mem: 15572
[2025-01-15 18:28:18,222] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23635
[2025-01-15 18:28:18,223] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:28:18,223] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1170/2809]  eta: 0:16:25  lr: 0.000046  min_lr: 0.000000  loss: 4.4527 (4.4980)  class_acc: 0.1250 (0.1812)  loss_scale: 65536.0000 (53867.1221)  weight_decay: 0.0500 (0.0500)  time: 0.6575  data: 0.1822  max mem: 15572
Epoch: [8]  [1180/2809]  eta: 0:16:17  lr: 0.000046  min_lr: 0.000000  loss: 4.5434 (4.4982)  class_acc: 0.1667 (0.1813)  loss_scale: 65536.0000 (53965.9272)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1201  max mem: 15572
Epoch: [8]  [1190/2809]  eta: 0:16:10  lr: 0.000046  min_lr: 0.000000  loss: 4.4708 (4.4979)  class_acc: 0.1667 (0.1814)  loss_scale: 65536.0000 (54063.0730)  weight_decay: 0.0500 (0.0500)  time: 0.4894  data: 0.0432  max mem: 15572
Epoch: [8]  [1200/2809]  eta: 0:16:03  lr: 0.000046  min_lr: 0.000000  loss: 4.4707 (4.4980)  class_acc: 0.1667 (0.1813)  loss_scale: 65536.0000 (54158.6012)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0874  max mem: 15572
Epoch: [8]  [1210/2809]  eta: 0:15:57  lr: 0.000046  min_lr: 0.000000  loss: 4.4707 (4.4979)  class_acc: 0.1667 (0.1815)  loss_scale: 65536.0000 (54252.5516)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1313  max mem: 15572
Epoch: [8]  [1220/2809]  eta: 0:15:50  lr: 0.000046  min_lr: 0.000000  loss: 4.5035 (4.4976)  class_acc: 0.1667 (0.1815)  loss_scale: 65536.0000 (54344.9631)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1300  max mem: 15572
Epoch: [8]  [1230/2809]  eta: 0:15:42  lr: 0.000046  min_lr: 0.000000  loss: 4.5323 (4.4985)  class_acc: 0.1250 (0.1813)  loss_scale: 65536.0000 (54435.8733)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0592  max mem: 15572
Epoch: [8]  [1240/2809]  eta: 0:15:36  lr: 0.000046  min_lr: 0.000000  loss: 4.5629 (4.4990)  class_acc: 0.1667 (0.1814)  loss_scale: 65536.0000 (54525.3183)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0909  max mem: 15572
Epoch: [8]  [1250/2809]  eta: 0:15:30  lr: 0.000046  min_lr: 0.000000  loss: 4.5189 (4.4995)  class_acc: 0.1667 (0.1815)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1439  max mem: 15572
Epoch: [8]  [1260/2809]  eta: 0:15:25  lr: 0.000046  min_lr: 0.000000  loss: 4.5189 (4.4995)  class_acc: 0.1667 (0.1815)  loss_scale: 65536.0000 (54699.9524)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1514  max mem: 15572
Epoch: [8]  [1270/2809]  eta: 0:15:20  lr: 0.000046  min_lr: 0.000000  loss: 4.5251 (4.4995)  class_acc: 0.2083 (0.1821)  loss_scale: 65536.0000 (54785.2085)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.1728  max mem: 15572
Epoch: [8]  [1280/2809]  eta: 0:15:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4900 (4.4995)  class_acc: 0.2083 (0.1820)  loss_scale: 65536.0000 (54869.1335)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0908  max mem: 15572
Epoch: [8]  [1290/2809]  eta: 0:15:07  lr: 0.000046  min_lr: 0.000000  loss: 4.4836 (4.4991)  class_acc: 0.1667 (0.1820)  loss_scale: 65536.0000 (54951.7583)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0610  max mem: 15572
[2025-01-15 18:29:32,040] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:29:32,041] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:29:32,509] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23765
[2025-01-15 18:29:32,510] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:29:32,510] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1300/2809]  eta: 0:15:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4973 (4.4994)  class_acc: 0.1667 (0.1819)  loss_scale: 65536.0000 (55083.4865)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1016  max mem: 15572
Epoch: [8]  [1310/2809]  eta: 0:14:53  lr: 0.000046  min_lr: 0.000000  loss: 4.4521 (4.4980)  class_acc: 0.1667 (0.1822)  loss_scale: 65536.0000 (55163.2159)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0485  max mem: 15572
Epoch: [8]  [1320/2809]  eta: 0:14:47  lr: 0.000046  min_lr: 0.000000  loss: 4.3735 (4.4979)  class_acc: 0.1667 (0.1822)  loss_scale: 65536.0000 (55241.7381)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0818  max mem: 15572
Epoch: [8]  [1330/2809]  eta: 0:14:41  lr: 0.000046  min_lr: 0.000000  loss: 4.4690 (4.4972)  class_acc: 0.2083 (0.1823)  loss_scale: 65536.0000 (55319.0804)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1624  max mem: 15572
Epoch: [8]  [1340/2809]  eta: 0:14:37  lr: 0.000046  min_lr: 0.000000  loss: 4.4471 (4.4970)  class_acc: 0.2083 (0.1824)  loss_scale: 65536.0000 (55395.2692)  weight_decay: 0.0500 (0.0500)  time: 0.6592  data: 0.2172  max mem: 15572
Epoch: [8]  [1350/2809]  eta: 0:14:31  lr: 0.000046  min_lr: 0.000000  loss: 4.3872 (4.4962)  class_acc: 0.1250 (0.1823)  loss_scale: 65536.0000 (55470.3301)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.2204  max mem: 15572
Epoch: [8]  [1360/2809]  eta: 0:14:24  lr: 0.000046  min_lr: 0.000000  loss: 4.3488 (4.4962)  class_acc: 0.1667 (0.1821)  loss_scale: 65536.0000 (55544.2880)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1203  max mem: 15572
Epoch: [8]  [1370/2809]  eta: 0:14:18  lr: 0.000046  min_lr: 0.000000  loss: 4.4013 (4.4955)  class_acc: 0.1667 (0.1820)  loss_scale: 65536.0000 (55617.1670)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0769  max mem: 15572
Epoch: [8]  [1380/2809]  eta: 0:14:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4389 (4.4960)  class_acc: 0.1667 (0.1822)  loss_scale: 65536.0000 (55688.9906)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1371  max mem: 15572
Epoch: [8]  [1390/2809]  eta: 0:14:06  lr: 0.000046  min_lr: 0.000000  loss: 4.6385 (4.4976)  class_acc: 0.1667 (0.1819)  loss_scale: 65536.0000 (55759.7815)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1076  max mem: 15572
Epoch: [8]  [1400/2809]  eta: 0:13:59  lr: 0.000046  min_lr: 0.000000  loss: 4.6385 (4.4979)  class_acc: 0.1667 (0.1819)  loss_scale: 65536.0000 (55829.5617)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0263  max mem: 15572
Epoch: [8]  [1410/2809]  eta: 0:13:53  lr: 0.000046  min_lr: 0.000000  loss: 4.4129 (4.4975)  class_acc: 0.1667 (0.1818)  loss_scale: 65536.0000 (55898.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1095  max mem: 15572
Epoch: [8]  [1420/2809]  eta: 0:13:47  lr: 0.000046  min_lr: 0.000000  loss: 4.5035 (4.4980)  class_acc: 0.1250 (0.1816)  loss_scale: 65536.0000 (55966.1759)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.1613  max mem: 15572
[2025-01-15 18:30:48,120] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:30:48,120] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:30:49,280] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23896
[2025-01-15 18:30:49,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:30:49,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1430/2809]  eta: 0:13:42  lr: 0.000046  min_lr: 0.000000  loss: 4.5365 (4.4983)  class_acc: 0.0833 (0.1812)  loss_scale: 65536.0000 (56124.6457)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.1603  max mem: 15572
Epoch: [8]  [1440/2809]  eta: 0:13:37  lr: 0.000046  min_lr: 0.000000  loss: 4.4606 (4.4983)  class_acc: 0.1250 (0.1811)  loss_scale: 65536.0000 (56189.9570)  weight_decay: 0.0500 (0.0500)  time: 0.6903  data: 0.1845  max mem: 15572
Epoch: [8]  [1450/2809]  eta: 0:13:30  lr: 0.000046  min_lr: 0.000000  loss: 4.4284 (4.4982)  class_acc: 0.1250 (0.1808)  loss_scale: 65536.0000 (56254.3680)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0842  max mem: 15572
Epoch: [8]  [1460/2809]  eta: 0:13:24  lr: 0.000046  min_lr: 0.000000  loss: 4.4879 (4.4978)  class_acc: 0.1250 (0.1810)  loss_scale: 65536.0000 (56317.8973)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0843  max mem: 15572
Epoch: [8]  [1470/2809]  eta: 0:13:18  lr: 0.000046  min_lr: 0.000000  loss: 4.5064 (4.4980)  class_acc: 0.2083 (0.1809)  loss_scale: 65536.0000 (56380.5629)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1632  max mem: 15572
Epoch: [8]  [1480/2809]  eta: 0:13:12  lr: 0.000046  min_lr: 0.000000  loss: 4.5172 (4.4981)  class_acc: 0.1250 (0.1806)  loss_scale: 65536.0000 (56442.3822)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1420  max mem: 15572
Epoch: [8]  [1490/2809]  eta: 0:13:06  lr: 0.000046  min_lr: 0.000000  loss: 4.4521 (4.4974)  class_acc: 0.1250 (0.1807)  loss_scale: 65536.0000 (56503.3722)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1266  max mem: 15572
Epoch: [8]  [1500/2809]  eta: 0:12:59  lr: 0.000046  min_lr: 0.000000  loss: 4.5044 (4.4977)  class_acc: 0.1667 (0.1810)  loss_scale: 65536.0000 (56563.5496)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0924  max mem: 15572
Epoch: [8]  [1510/2809]  eta: 0:12:53  lr: 0.000046  min_lr: 0.000000  loss: 4.6178 (4.4982)  class_acc: 0.1250 (0.1806)  loss_scale: 65536.0000 (56622.9305)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0685  max mem: 15572
[2025-01-15 18:31:40,238] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23983
[2025-01-15 18:31:40,239] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:31:40,239] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [1520/2809]  eta: 0:12:47  lr: 0.000046  min_lr: 0.000000  loss: 4.5742 (4.4980)  class_acc: 0.1667 (0.1813)  loss_scale: 32768.0000 (56466.0934)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1034  max mem: 15572
[2025-01-15 18:31:49,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=156, lr=[4.427880249960953e-07, 4.427880249960953e-07, 6.325543214229933e-07, 6.325543214229933e-07, 9.036490306042763e-07, 9.036490306042763e-07, 1.2909271865775375e-06, 1.2909271865775375e-06, 1.844181695110768e-06, 1.844181695110768e-06, 2.6345452787296686e-06, 2.6345452787296686e-06, 3.7636361124709554e-06, 3.7636361124709554e-06, 5.3766230178156515e-06, 5.3766230178156515e-06, 7.68089002545093e-06, 7.68089002545093e-06, 1.0972700036358473e-05, 1.0972700036358473e-05, 1.567528576622639e-05, 1.567528576622639e-05, 2.2393265380323416e-05, 2.2393265380323416e-05, 3.1990379114747744e-05, 3.1990379114747744e-05, 4.5700541592496775e-05, 4.5700541592496775e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 18:31:49,490] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=27.507586292196994, CurrSamplesPerSec=30.917404153349725, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [1530/2809]  eta: 0:12:40  lr: 0.000046  min_lr: 0.000000  loss: 4.5469 (4.4983)  class_acc: 0.1667 (0.1811)  loss_scale: 32768.0000 (56311.3050)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0856  max mem: 15572
Epoch: [8]  [1540/2809]  eta: 0:12:35  lr: 0.000046  min_lr: 0.000000  loss: 4.5797 (4.4988)  class_acc: 0.1667 (0.1813)  loss_scale: 32768.0000 (56158.5256)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1174  max mem: 15572
Epoch: [8]  [1550/2809]  eta: 0:12:29  lr: 0.000046  min_lr: 0.000000  loss: 4.5105 (4.4985)  class_acc: 0.2083 (0.1814)  loss_scale: 32768.0000 (56007.7163)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1433  max mem: 15572
Epoch: [8]  [1560/2809]  eta: 0:12:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4297 (4.4983)  class_acc: 0.2083 (0.1815)  loss_scale: 32768.0000 (55858.8392)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1037  max mem: 15572
Epoch: [8]  [1570/2809]  eta: 0:12:16  lr: 0.000046  min_lr: 0.000000  loss: 4.4793 (4.4987)  class_acc: 0.2083 (0.1817)  loss_scale: 32768.0000 (55711.8574)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1018  max mem: 15572
Epoch: [8]  [1580/2809]  eta: 0:12:11  lr: 0.000046  min_lr: 0.000000  loss: 4.5652 (4.4996)  class_acc: 0.1667 (0.1813)  loss_scale: 32768.0000 (55566.7350)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1299  max mem: 15572
Epoch: [8]  [1590/2809]  eta: 0:12:04  lr: 0.000046  min_lr: 0.000000  loss: 4.5794 (4.5004)  class_acc: 0.1250 (0.1814)  loss_scale: 32768.0000 (55423.4368)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1322  max mem: 15572
Epoch: [8]  [1600/2809]  eta: 0:11:58  lr: 0.000046  min_lr: 0.000000  loss: 4.5751 (4.5008)  class_acc: 0.1667 (0.1814)  loss_scale: 32768.0000 (55281.9288)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0815  max mem: 15572
Epoch: [8]  [1610/2809]  eta: 0:11:52  lr: 0.000046  min_lr: 0.000000  loss: 4.5402 (4.5005)  class_acc: 0.2083 (0.1815)  loss_scale: 32768.0000 (55142.1775)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1432  max mem: 15572
Epoch: [8]  [1620/2809]  eta: 0:11:46  lr: 0.000046  min_lr: 0.000000  loss: 4.4391 (4.5008)  class_acc: 0.1250 (0.1813)  loss_scale: 32768.0000 (55004.1505)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1384  max mem: 15572
Epoch: [8]  [1630/2809]  eta: 0:11:40  lr: 0.000046  min_lr: 0.000000  loss: 4.7033 (4.5024)  class_acc: 0.1250 (0.1812)  loss_scale: 32768.0000 (54867.8161)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0750  max mem: 15572
[2025-01-15 18:32:55,681] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:32:55,682] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [1640/2809]  eta: 0:11:34  lr: 0.000046  min_lr: 0.000000  loss: 4.6577 (4.5017)  class_acc: 0.1667 (0.1814)  loss_scale: 32768.0000 (54753.1115)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1154  max mem: 15572
Epoch: [8]  [1650/2809]  eta: 0:11:28  lr: 0.000046  min_lr: 0.000000  loss: 4.4785 (4.5016)  class_acc: 0.2083 (0.1816)  loss_scale: 65536.0000 (54818.4228)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1160  max mem: 15572
Epoch: [8]  [1660/2809]  eta: 0:11:22  lr: 0.000046  min_lr: 0.000000  loss: 4.3833 (4.5012)  class_acc: 0.2083 (0.1818)  loss_scale: 65536.0000 (54882.9476)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1272  max mem: 15572
Epoch: [8]  [1670/2809]  eta: 0:11:16  lr: 0.000046  min_lr: 0.000000  loss: 4.3833 (4.5012)  class_acc: 0.1667 (0.1819)  loss_scale: 65536.0000 (54946.7002)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1353  max mem: 15572
Epoch: [8]  [1680/2809]  eta: 0:11:11  lr: 0.000046  min_lr: 0.000000  loss: 4.5722 (4.5013)  class_acc: 0.1667 (0.1817)  loss_scale: 65536.0000 (55009.6942)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.1778  max mem: 15572
Epoch: [8]  [1690/2809]  eta: 0:11:05  lr: 0.000046  min_lr: 0.000000  loss: 4.5931 (4.5020)  class_acc: 0.1667 (0.1815)  loss_scale: 65536.0000 (55071.9432)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.1740  max mem: 15572
Epoch: [8]  [1700/2809]  eta: 0:10:59  lr: 0.000046  min_lr: 0.000000  loss: 4.5985 (4.5020)  class_acc: 0.1667 (0.1817)  loss_scale: 65536.0000 (55133.4603)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1232  max mem: 15572
Epoch: [8]  [1710/2809]  eta: 0:10:53  lr: 0.000046  min_lr: 0.000000  loss: 4.5985 (4.5026)  class_acc: 0.1667 (0.1816)  loss_scale: 65536.0000 (55194.2583)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1286  max mem: 15572
Epoch: [8]  [1720/2809]  eta: 0:10:47  lr: 0.000046  min_lr: 0.000000  loss: 4.4726 (4.5016)  class_acc: 0.1667 (0.1817)  loss_scale: 65536.0000 (55254.3498)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1112  max mem: 15572
Epoch: [8]  [1730/2809]  eta: 0:10:41  lr: 0.000046  min_lr: 0.000000  loss: 4.3611 (4.5016)  class_acc: 0.1667 (0.1815)  loss_scale: 65536.0000 (55313.7470)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1055  max mem: 15572
Epoch: [8]  [1740/2809]  eta: 0:10:35  lr: 0.000046  min_lr: 0.000000  loss: 4.5103 (4.5018)  class_acc: 0.1667 (0.1813)  loss_scale: 65536.0000 (55372.4618)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1344  max mem: 15572
Epoch: [8]  [1750/2809]  eta: 0:10:29  lr: 0.000046  min_lr: 0.000000  loss: 4.6276 (4.5014)  class_acc: 0.0833 (0.1809)  loss_scale: 65536.0000 (55430.5060)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1245  max mem: 15572
Epoch: [8]  [1760/2809]  eta: 0:10:22  lr: 0.000046  min_lr: 0.000000  loss: 4.4924 (4.5009)  class_acc: 0.1667 (0.1813)  loss_scale: 65536.0000 (55487.8910)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0418  max mem: 15572
[2025-01-15 18:34:11,514] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:34:11,515] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:34:12,135] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24241
[2025-01-15 18:34:12,136] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:34:12,137] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1770/2809]  eta: 0:10:17  lr: 0.000046  min_lr: 0.000000  loss: 4.3705 (4.5003)  class_acc: 0.1667 (0.1813)  loss_scale: 65536.0000 (55581.6330)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0957  max mem: 15572
Epoch: [8]  [1780/2809]  eta: 0:10:11  lr: 0.000046  min_lr: 0.000000  loss: 4.4761 (4.5014)  class_acc: 0.1667 (0.1814)  loss_scale: 65536.0000 (55637.5250)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1252  max mem: 15572
Epoch: [8]  [1790/2809]  eta: 0:10:04  lr: 0.000046  min_lr: 0.000000  loss: 4.6183 (4.5018)  class_acc: 0.1667 (0.1814)  loss_scale: 65536.0000 (55692.7929)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0780  max mem: 15572
Epoch: [8]  [1800/2809]  eta: 0:09:58  lr: 0.000046  min_lr: 0.000000  loss: 4.5034 (4.5017)  class_acc: 0.1667 (0.1813)  loss_scale: 65536.0000 (55747.4470)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0486  max mem: 15572
Epoch: [8]  [1810/2809]  eta: 0:09:52  lr: 0.000046  min_lr: 0.000000  loss: 4.4729 (4.5013)  class_acc: 0.2083 (0.1815)  loss_scale: 65536.0000 (55801.4975)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.0887  max mem: 15572
Epoch: [8]  [1820/2809]  eta: 0:09:46  lr: 0.000046  min_lr: 0.000000  loss: 4.4492 (4.5005)  class_acc: 0.2083 (0.1820)  loss_scale: 65536.0000 (55854.9544)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.1740  max mem: 15572
Epoch: [8]  [1830/2809]  eta: 0:09:41  lr: 0.000046  min_lr: 0.000000  loss: 4.4704 (4.5014)  class_acc: 0.1667 (0.1817)  loss_scale: 65536.0000 (55907.8274)  weight_decay: 0.0500 (0.0500)  time: 0.6569  data: 0.1811  max mem: 15572
Epoch: [8]  [1840/2809]  eta: 0:09:35  lr: 0.000046  min_lr: 0.000000  loss: 4.5630 (4.5016)  class_acc: 0.1250 (0.1817)  loss_scale: 65536.0000 (55960.1260)  weight_decay: 0.0500 (0.0500)  time: 0.6555  data: 0.1758  max mem: 15572
Epoch: [8]  [1850/2809]  eta: 0:09:29  lr: 0.000046  min_lr: 0.000000  loss: 4.4708 (4.5017)  class_acc: 0.1667 (0.1820)  loss_scale: 65536.0000 (56011.8595)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0968  max mem: 15572
Epoch: [8]  [1860/2809]  eta: 0:09:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4157 (4.5017)  class_acc: 0.1667 (0.1820)  loss_scale: 65536.0000 (56063.0371)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1010  max mem: 15572
Epoch: [8]  [1870/2809]  eta: 0:09:16  lr: 0.000046  min_lr: 0.000000  loss: 4.4254 (4.5014)  class_acc: 0.1667 (0.1820)  loss_scale: 65536.0000 (56113.6676)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0983  max mem: 15572
Epoch: [8]  [1880/2809]  eta: 0:09:10  lr: 0.000046  min_lr: 0.000000  loss: 4.3608 (4.5011)  class_acc: 0.2083 (0.1822)  loss_scale: 65536.0000 (56163.7597)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0826  max mem: 15572
Epoch: [8]  [1890/2809]  eta: 0:09:04  lr: 0.000046  min_lr: 0.000000  loss: 4.3608 (4.5007)  class_acc: 0.2083 (0.1825)  loss_scale: 65536.0000 (56213.3221)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0997  max mem: 15572
[2025-01-15 18:35:23,341] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24365
[2025-01-15 18:35:23,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:35:23,343] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [1900/2809]  eta: 0:08:58  lr: 0.000046  min_lr: 0.000000  loss: 4.4588 (4.5002)  class_acc: 0.2083 (0.1824)  loss_scale: 65536.0000 (56124.4650)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1129  max mem: 15572
Epoch: [8]  [1910/2809]  eta: 0:08:53  lr: 0.000046  min_lr: 0.000000  loss: 4.4588 (4.4995)  class_acc: 0.1667 (0.1825)  loss_scale: 32768.0000 (56002.2439)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1793  max mem: 15572
Epoch: [8]  [1920/2809]  eta: 0:08:47  lr: 0.000046  min_lr: 0.000000  loss: 4.4720 (4.4994)  class_acc: 0.1667 (0.1823)  loss_scale: 32768.0000 (55881.2952)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1607  max mem: 15572
Epoch: [8]  [1930/2809]  eta: 0:08:41  lr: 0.000046  min_lr: 0.000000  loss: 4.5638 (4.4992)  class_acc: 0.1250 (0.1823)  loss_scale: 32768.0000 (55761.5992)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1220  max mem: 15572
Epoch: [8]  [1940/2809]  eta: 0:08:35  lr: 0.000046  min_lr: 0.000000  loss: 4.5818 (4.4996)  class_acc: 0.1667 (0.1822)  loss_scale: 32768.0000 (55643.1365)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1311  max mem: 15572
Epoch: [8]  [1950/2809]  eta: 0:08:29  lr: 0.000046  min_lr: 0.000000  loss: 4.5371 (4.4996)  class_acc: 0.1667 (0.1822)  loss_scale: 32768.0000 (55525.8883)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1199  max mem: 15572
Epoch: [8]  [1960/2809]  eta: 0:08:23  lr: 0.000046  min_lr: 0.000000  loss: 4.5371 (4.5000)  class_acc: 0.1250 (0.1822)  loss_scale: 32768.0000 (55409.8358)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1179  max mem: 15572
Epoch: [8]  [1970/2809]  eta: 0:08:17  lr: 0.000046  min_lr: 0.000000  loss: 4.5391 (4.4997)  class_acc: 0.1250 (0.1822)  loss_scale: 32768.0000 (55294.9609)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1393  max mem: 15572
Epoch: [8]  [1980/2809]  eta: 0:08:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4664 (4.4994)  class_acc: 0.1667 (0.1824)  loss_scale: 32768.0000 (55181.2458)  weight_decay: 0.0500 (0.0500)  time: 0.6578  data: 0.2041  max mem: 15572
Epoch: [8]  [1990/2809]  eta: 0:08:06  lr: 0.000046  min_lr: 0.000000  loss: 4.4903 (4.4994)  class_acc: 0.2083 (0.1824)  loss_scale: 32768.0000 (55068.6730)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1996  max mem: 15572
Epoch: [8]  [2000/2809]  eta: 0:08:00  lr: 0.000046  min_lr: 0.000000  loss: 4.5074 (4.4998)  class_acc: 0.1667 (0.1823)  loss_scale: 32768.0000 (54957.2254)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1327  max mem: 15572
Epoch: [8]  [2010/2809]  eta: 0:07:54  lr: 0.000046  min_lr: 0.000000  loss: 4.6508 (4.4999)  class_acc: 0.1667 (0.1822)  loss_scale: 32768.0000 (54846.8861)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1380  max mem: 15572
Epoch: [8]  [2020/2809]  eta: 0:07:48  lr: 0.000046  min_lr: 0.000000  loss: 4.4921 (4.4997)  class_acc: 0.1667 (0.1822)  loss_scale: 32768.0000 (54737.6388)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.1927  max mem: 15572
[2025-01-15 18:36:42,433] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:36:42,433] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [2030/2809]  eta: 0:07:42  lr: 0.000046  min_lr: 0.000000  loss: 4.5074 (4.4997)  class_acc: 0.1667 (0.1822)  loss_scale: 32768.0000 (54774.6726)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.1648  max mem: 15572
Epoch: [8]  [2040/2809]  eta: 0:07:37  lr: 0.000046  min_lr: 0.000000  loss: 4.5884 (4.5001)  class_acc: 0.1667 (0.1822)  loss_scale: 65536.0000 (54827.3983)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1411  max mem: 15572
Epoch: [8]  [2050/2809]  eta: 0:07:31  lr: 0.000046  min_lr: 0.000000  loss: 4.5238 (4.4997)  class_acc: 0.2083 (0.1824)  loss_scale: 65536.0000 (54879.6099)  weight_decay: 0.0500 (0.0500)  time: 0.6880  data: 0.2108  max mem: 15572
Epoch: [8]  [2060/2809]  eta: 0:07:25  lr: 0.000046  min_lr: 0.000000  loss: 4.4860 (4.4995)  class_acc: 0.2083 (0.1824)  loss_scale: 65536.0000 (54931.3149)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1119  max mem: 15572
Epoch: [8]  [2070/2809]  eta: 0:07:19  lr: 0.000046  min_lr: 0.000000  loss: 4.4874 (4.4995)  class_acc: 0.1667 (0.1823)  loss_scale: 65536.0000 (54982.5205)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1002  max mem: 15572
Epoch: [8]  [2080/2809]  eta: 0:07:13  lr: 0.000046  min_lr: 0.000000  loss: 4.4874 (4.4998)  class_acc: 0.1250 (0.1821)  loss_scale: 65536.0000 (55033.2340)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1413  max mem: 15572
[2025-01-15 18:37:18,919] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24554
[2025-01-15 18:37:18,919] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:37:18,920] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [2090/2809]  eta: 0:07:07  lr: 0.000046  min_lr: 0.000000  loss: 4.4412 (4.4994)  class_acc: 0.1667 (0.1823)  loss_scale: 65536.0000 (54942.4237)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0874  max mem: 15572
Epoch: [8]  [2100/2809]  eta: 0:07:01  lr: 0.000046  min_lr: 0.000000  loss: 4.3697 (4.4989)  class_acc: 0.1667 (0.1823)  loss_scale: 32768.0000 (54836.8815)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1261  max mem: 15572
Epoch: [8]  [2110/2809]  eta: 0:06:55  lr: 0.000046  min_lr: 0.000000  loss: 4.3697 (4.4984)  class_acc: 0.1667 (0.1822)  loss_scale: 32768.0000 (54732.3392)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1010  max mem: 15572
Epoch: [8]  [2120/2809]  eta: 0:06:49  lr: 0.000046  min_lr: 0.000000  loss: 4.4105 (4.4983)  class_acc: 0.2083 (0.1824)  loss_scale: 32768.0000 (54628.7826)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0603  max mem: 15572
Epoch: [8]  [2130/2809]  eta: 0:06:43  lr: 0.000046  min_lr: 0.000000  loss: 4.4592 (4.4977)  class_acc: 0.2083 (0.1825)  loss_scale: 32768.0000 (54526.1980)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0914  max mem: 15572
Epoch: [8]  [2140/2809]  eta: 0:06:37  lr: 0.000046  min_lr: 0.000000  loss: 4.4052 (4.4976)  class_acc: 0.2083 (0.1827)  loss_scale: 32768.0000 (54424.5717)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.1702  max mem: 15572
Epoch: [8]  [2150/2809]  eta: 0:06:31  lr: 0.000046  min_lr: 0.000000  loss: 4.4553 (4.4977)  class_acc: 0.1667 (0.1826)  loss_scale: 32768.0000 (54323.8903)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1916  max mem: 15572
Epoch: [8]  [2160/2809]  eta: 0:06:25  lr: 0.000046  min_lr: 0.000000  loss: 4.5813 (4.4982)  class_acc: 0.1667 (0.1828)  loss_scale: 32768.0000 (54224.1407)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1728  max mem: 15572
Epoch: [8]  [2170/2809]  eta: 0:06:19  lr: 0.000046  min_lr: 0.000000  loss: 4.5686 (4.4979)  class_acc: 0.2500 (0.1830)  loss_scale: 32768.0000 (54125.3100)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1398  max mem: 15572
Epoch: [8]  [2180/2809]  eta: 0:06:13  lr: 0.000046  min_lr: 0.000000  loss: 4.5218 (4.4982)  class_acc: 0.1667 (0.1829)  loss_scale: 32768.0000 (54027.3856)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1047  max mem: 15572
Epoch: [8]  [2190/2809]  eta: 0:06:07  lr: 0.000046  min_lr: 0.000000  loss: 4.5332 (4.4983)  class_acc: 0.1667 (0.1828)  loss_scale: 32768.0000 (53930.3551)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0653  max mem: 15572
Epoch: [8]  [2200/2809]  eta: 0:06:01  lr: 0.000046  min_lr: 0.000000  loss: 4.4182 (4.4980)  class_acc: 0.1250 (0.1828)  loss_scale: 32768.0000 (53834.2063)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0776  max mem: 15572
Epoch: [8]  [2210/2809]  eta: 0:05:55  lr: 0.000046  min_lr: 0.000000  loss: 4.4182 (4.4981)  class_acc: 0.1667 (0.1828)  loss_scale: 32768.0000 (53738.9272)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1281  max mem: 15572
[2025-01-15 18:38:34,306] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:38:34,306] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [2220/2809]  eta: 0:05:49  lr: 0.000046  min_lr: 0.000000  loss: 4.4507 (4.4977)  class_acc: 0.1667 (0.1830)  loss_scale: 32768.0000 (53792.0432)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0721  max mem: 15572
Epoch: [8]  [2230/2809]  eta: 0:05:43  lr: 0.000046  min_lr: 0.000000  loss: 4.4732 (4.4977)  class_acc: 0.1667 (0.1828)  loss_scale: 65536.0000 (53844.6831)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0860  max mem: 15572
Epoch: [8]  [2240/2809]  eta: 0:05:37  lr: 0.000046  min_lr: 0.000000  loss: 4.4428 (4.4974)  class_acc: 0.1250 (0.1829)  loss_scale: 65536.0000 (53896.8532)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1056  max mem: 15572
Epoch: [8]  [2250/2809]  eta: 0:05:31  lr: 0.000046  min_lr: 0.000000  loss: 4.4164 (4.4971)  class_acc: 0.1667 (0.1828)  loss_scale: 65536.0000 (53948.5598)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0895  max mem: 15572
Epoch: [8]  [2260/2809]  eta: 0:05:25  lr: 0.000046  min_lr: 0.000000  loss: 4.6152 (4.4978)  class_acc: 0.2083 (0.1828)  loss_scale: 65536.0000 (53999.8089)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0968  max mem: 15572
Epoch: [8]  [2270/2809]  eta: 0:05:19  lr: 0.000046  min_lr: 0.000000  loss: 4.6152 (4.4983)  class_acc: 0.1667 (0.1828)  loss_scale: 65536.0000 (54050.6068)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1355  max mem: 15572
Epoch: [8]  [2280/2809]  eta: 0:05:13  lr: 0.000046  min_lr: 0.000000  loss: 4.5103 (4.4980)  class_acc: 0.1667 (0.1830)  loss_scale: 65536.0000 (54100.9592)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1518  max mem: 15572
[2025-01-15 18:39:14,708] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24754
[2025-01-15 18:39:14,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:39:14,709] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [2290/2809]  eta: 0:05:08  lr: 0.000046  min_lr: 0.000000  loss: 4.4800 (4.4981)  class_acc: 0.2500 (0.1832)  loss_scale: 65536.0000 (54022.1458)  weight_decay: 0.0500 (0.0500)  time: 0.6351  data: 0.1755  max mem: 15572
Epoch: [8]  [2300/2809]  eta: 0:05:02  lr: 0.000046  min_lr: 0.000000  loss: 4.5200 (4.4985)  class_acc: 0.1667 (0.1833)  loss_scale: 32768.0000 (53929.7766)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1566  max mem: 15572
Epoch: [8]  [2310/2809]  eta: 0:04:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5240 (4.4981)  class_acc: 0.1667 (0.1836)  loss_scale: 32768.0000 (53838.2068)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1774  max mem: 15572
Epoch: [8]  [2320/2809]  eta: 0:04:50  lr: 0.000046  min_lr: 0.000000  loss: 4.4916 (4.4981)  class_acc: 0.1667 (0.1836)  loss_scale: 32768.0000 (53747.4261)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.2051  max mem: 15572
Epoch: [8]  [2330/2809]  eta: 0:04:44  lr: 0.000046  min_lr: 0.000000  loss: 4.4448 (4.4978)  class_acc: 0.1667 (0.1837)  loss_scale: 32768.0000 (53657.4243)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1264  max mem: 15572
Epoch: [8]  [2340/2809]  eta: 0:04:38  lr: 0.000046  min_lr: 0.000000  loss: 4.4123 (4.4976)  class_acc: 0.2500 (0.1841)  loss_scale: 32768.0000 (53568.1914)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1255  max mem: 15572
Epoch: [8]  [2350/2809]  eta: 0:04:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4680 (4.4975)  class_acc: 0.2917 (0.1844)  loss_scale: 32768.0000 (53479.7176)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1688  max mem: 15572
Epoch: [8]  [2360/2809]  eta: 0:04:26  lr: 0.000045  min_lr: 0.000000  loss: 4.4285 (4.4971)  class_acc: 0.2083 (0.1844)  loss_scale: 32768.0000 (53391.9932)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1387  max mem: 15572
Epoch: [8]  [2370/2809]  eta: 0:04:20  lr: 0.000045  min_lr: 0.000000  loss: 4.4178 (4.4969)  class_acc: 0.1667 (0.1843)  loss_scale: 32768.0000 (53305.0089)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1096  max mem: 15572
Epoch: [8]  [2380/2809]  eta: 0:04:14  lr: 0.000045  min_lr: 0.000000  loss: 4.5125 (4.4970)  class_acc: 0.1667 (0.1843)  loss_scale: 32768.0000 (53218.7551)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1373  max mem: 15572
Epoch: [8]  [2390/2809]  eta: 0:04:09  lr: 0.000045  min_lr: 0.000000  loss: 4.4711 (4.4967)  class_acc: 0.1667 (0.1842)  loss_scale: 32768.0000 (53133.2229)  weight_decay: 0.0500 (0.0500)  time: 0.6470  data: 0.1575  max mem: 15572
Epoch: [8]  [2400/2809]  eta: 0:04:03  lr: 0.000045  min_lr: 0.000000  loss: 4.4247 (4.4968)  class_acc: 0.1250 (0.1841)  loss_scale: 32768.0000 (53048.4032)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1783  max mem: 15572
Epoch: [8]  [2410/2809]  eta: 0:03:57  lr: 0.000045  min_lr: 0.000000  loss: 4.4455 (4.4965)  class_acc: 0.1250 (0.1841)  loss_scale: 32768.0000 (52964.2870)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1619  max mem: 15572
[2025-01-15 18:40:34,290] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:40:34,290] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [2420/2809]  eta: 0:03:51  lr: 0.000045  min_lr: 0.000000  loss: 4.4455 (4.4965)  class_acc: 0.1250 (0.1839)  loss_scale: 32768.0000 (53016.2148)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1401  max mem: 15572
Epoch: [8]  [2430/2809]  eta: 0:03:45  lr: 0.000045  min_lr: 0.000000  loss: 4.3737 (4.4955)  class_acc: 0.1250 (0.1839)  loss_scale: 65536.0000 (53067.7153)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1285  max mem: 15572
Epoch: [8]  [2440/2809]  eta: 0:03:39  lr: 0.000045  min_lr: 0.000000  loss: 4.3737 (4.4950)  class_acc: 0.1667 (0.1842)  loss_scale: 65536.0000 (53118.7939)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1530  max mem: 15572
Epoch: [8]  [2450/2809]  eta: 0:03:33  lr: 0.000045  min_lr: 0.000000  loss: 4.4460 (4.4951)  class_acc: 0.2083 (0.1842)  loss_scale: 65536.0000 (53169.4557)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.1578  max mem: 15572
Epoch: [8]  [2460/2809]  eta: 0:03:27  lr: 0.000045  min_lr: 0.000000  loss: 4.6002 (4.4955)  class_acc: 0.1667 (0.1842)  loss_scale: 65536.0000 (53219.7058)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0732  max mem: 15572
Epoch: [8]  [2470/2809]  eta: 0:03:21  lr: 0.000045  min_lr: 0.000000  loss: 4.6125 (4.4959)  class_acc: 0.1667 (0.1841)  loss_scale: 65536.0000 (53269.5492)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0553  max mem: 15572
Epoch: [8]  [2480/2809]  eta: 0:03:15  lr: 0.000045  min_lr: 0.000000  loss: 4.5913 (4.4962)  class_acc: 0.1250 (0.1840)  loss_scale: 65536.0000 (53318.9907)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1032  max mem: 15572
Epoch: [8]  [2490/2809]  eta: 0:03:09  lr: 0.000045  min_lr: 0.000000  loss: 4.5630 (4.4968)  class_acc: 0.1667 (0.1840)  loss_scale: 65536.0000 (53368.0353)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.1596  max mem: 15572
Epoch: [8]  [2500/2809]  eta: 0:03:03  lr: 0.000045  min_lr: 0.000000  loss: 4.5076 (4.4968)  class_acc: 0.1667 (0.1840)  loss_scale: 65536.0000 (53416.6877)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1470  max mem: 15572
Epoch: [8]  [2510/2809]  eta: 0:02:57  lr: 0.000045  min_lr: 0.000000  loss: 4.4852 (4.4968)  class_acc: 0.1667 (0.1838)  loss_scale: 65536.0000 (53464.9526)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0888  max mem: 15572
Epoch: [8]  [2520/2809]  eta: 0:02:51  lr: 0.000045  min_lr: 0.000000  loss: 4.4852 (4.4968)  class_acc: 0.1667 (0.1842)  loss_scale: 65536.0000 (53512.8346)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1307  max mem: 15572
[2025-01-15 18:41:43,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=160, lr=[4.404113593984157e-07, 4.404113593984157e-07, 6.291590848548796e-07, 6.291590848548796e-07, 8.987986926498281e-07, 8.987986926498281e-07, 1.2839981323568972e-06, 1.2839981323568972e-06, 1.8342830462241392e-06, 1.8342830462241392e-06, 2.62040435174877e-06, 2.62040435174877e-06, 3.743434788212529e-06, 3.743434788212529e-06, 5.347763983160757e-06, 5.347763983160757e-06, 7.639662833086796e-06, 7.639662833086796e-06, 1.0913804047266852e-05, 1.0913804047266852e-05, 1.5591148638952645e-05, 1.5591148638952645e-05, 2.2273069484218067e-05, 2.2273069484218067e-05, 3.1818670691740094e-05, 3.1818670691740094e-05, 4.5455243845343e-05, 4.5455243845343e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 18:41:43,441] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=27.49577485143876, CurrSamplesPerSec=28.462814067510024, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [2530/2809]  eta: 0:02:45  lr: 0.000045  min_lr: 0.000000  loss: 4.4850 (4.4970)  class_acc: 0.1250 (0.1839)  loss_scale: 65536.0000 (53560.3382)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1060  max mem: 15572
[2025-01-15 18:41:49,948] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:41:49,948] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [2540/2809]  eta: 0:02:39  lr: 0.000045  min_lr: 0.000000  loss: 4.4658 (4.4972)  class_acc: 0.1250 (0.1838)  loss_scale: 65536.0000 (53659.0508)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0594  max mem: 15572
[2025-01-15 18:41:50,778] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25013
[2025-01-15 18:41:50,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:41:50,779] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2550/2809]  eta: 0:02:33  lr: 0.000045  min_lr: 0.000000  loss: 4.4658 (4.4970)  class_acc: 0.1667 (0.1839)  loss_scale: 65536.0000 (53705.6088)  weight_decay: 0.0500 (0.0500)  time: 0.4941  data: 0.0498  max mem: 15572
Epoch: [8]  [2560/2809]  eta: 0:02:27  lr: 0.000045  min_lr: 0.000000  loss: 4.4699 (4.4971)  class_acc: 0.1667 (0.1838)  loss_scale: 65536.0000 (53751.8032)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1388  max mem: 15572
Epoch: [8]  [2570/2809]  eta: 0:02:21  lr: 0.000045  min_lr: 0.000000  loss: 4.5522 (4.4973)  class_acc: 0.1667 (0.1839)  loss_scale: 65536.0000 (53797.6383)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1284  max mem: 15572
Epoch: [8]  [2580/2809]  eta: 0:02:15  lr: 0.000045  min_lr: 0.000000  loss: 4.5475 (4.4970)  class_acc: 0.2083 (0.1840)  loss_scale: 65536.0000 (53843.1182)  weight_decay: 0.0500 (0.0500)  time: 0.4943  data: 0.0008  max mem: 15572
Epoch: [8]  [2590/2809]  eta: 0:02:09  lr: 0.000045  min_lr: 0.000000  loss: 4.4242 (4.4969)  class_acc: 0.2083 (0.1842)  loss_scale: 65536.0000 (53888.2470)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0621  max mem: 15572
Epoch: [8]  [2600/2809]  eta: 0:02:03  lr: 0.000045  min_lr: 0.000000  loss: 4.4576 (4.4968)  class_acc: 0.1667 (0.1841)  loss_scale: 65536.0000 (53933.0288)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1155  max mem: 15572
Epoch: [8]  [2610/2809]  eta: 0:01:58  lr: 0.000045  min_lr: 0.000000  loss: 4.5636 (4.4972)  class_acc: 0.1667 (0.1841)  loss_scale: 65536.0000 (53977.4676)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1530  max mem: 15572
Epoch: [8]  [2620/2809]  eta: 0:01:52  lr: 0.000045  min_lr: 0.000000  loss: 4.5898 (4.4973)  class_acc: 0.1667 (0.1842)  loss_scale: 65536.0000 (54021.5673)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1447  max mem: 15572
Epoch: [8]  [2630/2809]  eta: 0:01:46  lr: 0.000045  min_lr: 0.000000  loss: 4.5437 (4.4971)  class_acc: 0.1667 (0.1840)  loss_scale: 65536.0000 (54065.3318)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1084  max mem: 15572
Epoch: [8]  [2640/2809]  eta: 0:01:40  lr: 0.000045  min_lr: 0.000000  loss: 4.5485 (4.4973)  class_acc: 0.1667 (0.1842)  loss_scale: 65536.0000 (54108.7649)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0925  max mem: 15572
Epoch: [8]  [2650/2809]  eta: 0:01:34  lr: 0.000045  min_lr: 0.000000  loss: 4.4787 (4.4967)  class_acc: 0.2500 (0.1843)  loss_scale: 65536.0000 (54151.8702)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0762  max mem: 15572
Epoch: [8]  [2660/2809]  eta: 0:01:28  lr: 0.000045  min_lr: 0.000000  loss: 4.4115 (4.4968)  class_acc: 0.2083 (0.1843)  loss_scale: 65536.0000 (54194.6516)  weight_decay: 0.0500 (0.0500)  time: 0.5143  data: 0.0616  max mem: 15572
[2025-01-15 18:43:01,313] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:43:01,314] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [2670/2809]  eta: 0:01:22  lr: 0.000045  min_lr: 0.000000  loss: 4.4633 (4.4967)  class_acc: 0.1667 (0.1842)  loss_scale: 65536.0000 (54261.6488)  weight_decay: 0.0500 (0.0500)  time: 0.4477  data: 0.0151  max mem: 15572
[2025-01-15 18:43:02,508] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25145
[2025-01-15 18:43:02,508] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:43:02,509] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2680/2809]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000000  loss: 4.4688 (4.4964)  class_acc: 0.1667 (0.1843)  loss_scale: 65536.0000 (54352.5908)  weight_decay: 0.0500 (0.0500)  time: 0.4344  data: 0.0007  max mem: 15572
Epoch: [8]  [2690/2809]  eta: 0:01:10  lr: 0.000045  min_lr: 0.000000  loss: 4.5120 (4.4963)  class_acc: 0.1667 (0.1843)  loss_scale: 65536.0000 (54394.1494)  weight_decay: 0.0500 (0.0500)  time: 0.4661  data: 0.0009  max mem: 15572
Epoch: [8]  [2700/2809]  eta: 0:01:04  lr: 0.000045  min_lr: 0.000000  loss: 4.5211 (4.4963)  class_acc: 0.1667 (0.1842)  loss_scale: 65536.0000 (54435.4002)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0514  max mem: 15572
Epoch: [8]  [2710/2809]  eta: 0:00:58  lr: 0.000045  min_lr: 0.000000  loss: 4.5614 (4.4966)  class_acc: 0.1667 (0.1844)  loss_scale: 65536.0000 (54476.3467)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.1417  max mem: 15572
Epoch: [8]  [2720/2809]  eta: 0:00:52  lr: 0.000045  min_lr: 0.000000  loss: 4.4599 (4.4959)  class_acc: 0.2083 (0.1846)  loss_scale: 65536.0000 (54516.9923)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1368  max mem: 15572
Epoch: [8]  [2730/2809]  eta: 0:00:46  lr: 0.000045  min_lr: 0.000000  loss: 4.4759 (4.4962)  class_acc: 0.2083 (0.1846)  loss_scale: 65536.0000 (54557.3402)  weight_decay: 0.0500 (0.0500)  time: 0.7213  data: 0.2406  max mem: 15572
Epoch: [8]  [2740/2809]  eta: 0:00:40  lr: 0.000045  min_lr: 0.000000  loss: 4.6234 (4.4970)  class_acc: 0.2083 (0.1847)  loss_scale: 65536.0000 (54597.3937)  weight_decay: 0.0500 (0.0500)  time: 0.7519  data: 0.2574  max mem: 15572
Epoch: [8]  [2750/2809]  eta: 0:00:34  lr: 0.000045  min_lr: 0.000000  loss: 4.6023 (4.4968)  class_acc: 0.1667 (0.1846)  loss_scale: 65536.0000 (54637.1559)  weight_decay: 0.0500 (0.0500)  time: 0.6817  data: 0.1845  max mem: 15572
Epoch: [8]  [2760/2809]  eta: 0:00:29  lr: 0.000045  min_lr: 0.000000  loss: 4.4303 (4.4967)  class_acc: 0.1667 (0.1846)  loss_scale: 65536.0000 (54676.6302)  weight_decay: 0.0500 (0.0500)  time: 0.6596  data: 0.1636  max mem: 15572
Epoch: [8]  [2770/2809]  eta: 0:00:23  lr: 0.000045  min_lr: 0.000000  loss: 4.4303 (4.4970)  class_acc: 0.1667 (0.1846)  loss_scale: 65536.0000 (54715.8196)  weight_decay: 0.0500 (0.0500)  time: 0.6555  data: 0.1569  max mem: 15572
Epoch: [8]  [2780/2809]  eta: 0:00:17  lr: 0.000045  min_lr: 0.000000  loss: 4.4645 (4.4969)  class_acc: 0.1667 (0.1845)  loss_scale: 65536.0000 (54754.7271)  weight_decay: 0.0500 (0.0500)  time: 0.7186  data: 0.2334  max mem: 15572
Epoch: [8]  [2790/2809]  eta: 0:00:11  lr: 0.000045  min_lr: 0.000000  loss: 4.4645 (4.4968)  class_acc: 0.1667 (0.1844)  loss_scale: 65536.0000 (54793.3558)  weight_decay: 0.0500 (0.0500)  time: 0.6819  data: 0.1977  max mem: 15572
Epoch: [8]  [2800/2809]  eta: 0:00:05  lr: 0.000045  min_lr: 0.000000  loss: 4.4874 (4.4968)  class_acc: 0.1667 (0.1845)  loss_scale: 65536.0000 (54831.7087)  weight_decay: 0.0500 (0.0500)  time: 0.7048  data: 0.2310  max mem: 15572
[2025-01-15 18:44:26,680] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:44:26,680] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [2808/2809]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 4.5586 (4.4970)  class_acc: 0.1667 (0.1844)  loss_scale: 65536.0000 (55025.5094)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1599  max mem: 15572
Epoch: [8] Total time: 0:27:49 (0.5943 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 4.5586 (4.4970)  class_acc: 0.1667 (0.1844)  loss_scale: 65536.0000 (55025.5094)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:09:45  loss: 1.3761 (1.3761)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1539  data: 2.0035  max mem: 15572
Val:  [ 10/272]  eta: 0:01:49  loss: 3.7144 (3.4391)  acc1: 11.1111 (24.2424)  acc5: 38.8889 (43.4343)  time: 0.4185  data: 0.2569  max mem: 15572
Val:  [ 20/272]  eta: 0:01:20  loss: 3.5362 (3.4449)  acc1: 16.6667 (25.1323)  acc5: 44.4444 (51.0582)  time: 0.2261  data: 0.0540  max mem: 15572
Val:  [ 30/272]  eta: 0:01:09  loss: 3.4718 (3.5075)  acc1: 16.6667 (22.2222)  acc5: 55.5556 (51.4337)  time: 0.2119  data: 0.0272  max mem: 15572
Val:  [ 40/272]  eta: 0:01:11  loss: 3.2702 (3.4703)  acc1: 16.6667 (21.1382)  acc5: 55.5556 (53.2520)  time: 0.2998  data: 0.0918  max mem: 15572
Val:  [ 50/272]  eta: 0:01:05  loss: 3.2485 (3.3855)  acc1: 16.6667 (24.7277)  acc5: 61.1111 (56.1002)  time: 0.3100  data: 0.0925  max mem: 15572
Val:  [ 60/272]  eta: 0:01:05  loss: 2.5787 (3.3099)  acc1: 44.4444 (28.5064)  acc5: 72.2222 (58.1056)  time: 0.3061  data: 0.0923  max mem: 15572
Val:  [ 70/272]  eta: 0:01:01  loss: 2.6691 (3.2408)  acc1: 44.4444 (30.6729)  acc5: 77.7778 (60.7199)  time: 0.3235  data: 0.1251  max mem: 15572
Val:  [ 80/272]  eta: 0:00:59  loss: 2.9652 (3.2401)  acc1: 38.8889 (30.7270)  acc5: 77.7778 (60.4252)  time: 0.3081  data: 0.1153  max mem: 15572
Val:  [ 90/272]  eta: 0:00:56  loss: 3.6544 (3.2851)  acc1: 22.2222 (29.5482)  acc5: 44.4444 (58.6691)  time: 0.3396  data: 0.1366  max mem: 15572
Val:  [100/272]  eta: 0:00:53  loss: 3.6723 (3.3340)  acc1: 16.6667 (28.7129)  acc5: 50.0000 (58.1408)  time: 0.3216  data: 0.1341  max mem: 15572
Val:  [110/272]  eta: 0:00:50  loss: 3.6724 (3.3727)  acc1: 11.1111 (27.2272)  acc5: 44.4444 (57.1071)  time: 0.3112  data: 0.1246  max mem: 15572
Val:  [120/272]  eta: 0:00:47  loss: 3.6724 (3.3978)  acc1: 11.1111 (26.5840)  acc5: 44.4444 (56.6116)  time: 0.3244  data: 0.1297  max mem: 15572
Val:  [130/272]  eta: 0:00:44  loss: 3.5111 (3.3746)  acc1: 22.2222 (27.4809)  acc5: 50.0000 (57.2943)  time: 0.3400  data: 0.1418  max mem: 15572
Val:  [140/272]  eta: 0:00:41  loss: 3.1010 (3.3732)  acc1: 27.7778 (28.1324)  acc5: 55.5556 (57.2892)  time: 0.3140  data: 0.1143  max mem: 15572
Val:  [150/272]  eta: 0:00:38  loss: 3.1877 (3.3661)  acc1: 16.6667 (27.6306)  acc5: 61.1111 (57.6527)  time: 0.3153  data: 0.1178  max mem: 15572
Val:  [160/272]  eta: 0:00:35  loss: 3.1452 (3.3523)  acc1: 27.7778 (28.2609)  acc5: 77.7778 (58.6957)  time: 0.3406  data: 0.1421  max mem: 15572
Val:  [170/272]  eta: 0:00:32  loss: 3.4058 (3.3733)  acc1: 27.7778 (27.5504)  acc5: 55.5556 (57.8947)  time: 0.3450  data: 0.1391  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 3.3893 (3.3538)  acc1: 22.2222 (28.2382)  acc5: 55.5556 (58.5942)  time: 0.3613  data: 0.1535  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 3.3039 (3.3797)  acc1: 16.6667 (27.2542)  acc5: 55.5556 (57.4462)  time: 0.3579  data: 0.1581  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.2773 (3.3803)  acc1: 16.6667 (26.9762)  acc5: 50.0000 (57.7667)  time: 0.3255  data: 0.1202  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 3.0713 (3.3841)  acc1: 27.7778 (27.5408)  acc5: 77.7778 (58.1095)  time: 0.2847  data: 0.0698  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 3.2408 (3.3841)  acc1: 38.8889 (27.5013)  acc5: 66.6667 (58.3208)  time: 0.3170  data: 0.1021  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.0599 (3.3657)  acc1: 50.0000 (28.8119)  acc5: 72.2222 (59.1150)  time: 0.3666  data: 0.1448  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.7755 (3.3454)  acc1: 55.5556 (29.5758)  acc5: 83.3333 (60.1429)  time: 0.3340  data: 0.1222  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1612 (3.3643)  acc1: 22.2222 (29.1722)  acc5: 66.6667 (59.4954)  time: 0.2910  data: 0.0852  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.9805 (3.3255)  acc1: 55.5556 (30.9706)  acc5: 72.2222 (60.6428)  time: 0.3326  data: 0.1337  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.8559 (3.3275)  acc1: 55.5556 (30.7298)  acc5: 77.7778 (60.4961)  time: 0.2712  data: 0.1017  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.8559 (3.3311)  acc1: 50.0000 (30.6983)  acc5: 77.7778 (60.4546)  time: 0.2535  data: 0.0917  max mem: 15572
Val: Total time: 0:01:26 (0.3175 s / it)
* Acc@1 30.698 Acc@5 60.455 loss 3.331
Accuracy of the network on the 4883 val videos: 30.7%
[2025-01-15 18:45:55,477] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 18:45:55,482] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 18:45:55,482] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 18:45:58,528] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 18:45:58,529] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 30.70%
Epoch: [9]  [   0/2809]  eta: 5:28:33  lr: 0.000045  min_lr: 0.000000  loss: 4.0905 (4.0905)  class_acc: 0.2500 (0.2500)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 7.0180  data: 6.4665  max mem: 15572
[2025-01-15 18:46:06,029] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25282
[2025-01-15 18:46:06,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:46:06,030] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [  10/2809]  eta: 0:52:00  lr: 0.000045  min_lr: 0.000000  loss: 4.4331 (4.4083)  class_acc: 0.2500 (0.2121)  loss_scale: 65536.0000 (71493.8182)  weight_decay: 0.0500 (0.0500)  time: 1.1148  data: 0.6496  max mem: 15572
Epoch: [9]  [  20/2809]  eta: 0:39:05  lr: 0.000045  min_lr: 0.000000  loss: 4.4926 (4.4735)  class_acc: 0.2083 (0.2222)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0942  max mem: 15572
Epoch: [9]  [  30/2809]  eta: 0:36:32  lr: 0.000045  min_lr: 0.000000  loss: 4.4926 (4.4737)  class_acc: 0.2083 (0.2030)  loss_scale: 65536.0000 (67650.0645)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1829  max mem: 15572
Epoch: [9]  [  40/2809]  eta: 0:33:27  lr: 0.000045  min_lr: 0.000000  loss: 4.4393 (4.4786)  class_acc: 0.1667 (0.1992)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1592  max mem: 15572
Epoch: [9]  [  50/2809]  eta: 0:32:07  lr: 0.000045  min_lr: 0.000000  loss: 4.5181 (4.4768)  class_acc: 0.2083 (0.2092)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1104  max mem: 15572
Epoch: [9]  [  60/2809]  eta: 0:31:34  lr: 0.000045  min_lr: 0.000000  loss: 4.5730 (4.4932)  class_acc: 0.1250 (0.1933)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1789  max mem: 15572
Epoch: [9]  [  70/2809]  eta: 0:30:50  lr: 0.000045  min_lr: 0.000000  loss: 4.6014 (4.5036)  class_acc: 0.1250 (0.1913)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1720  max mem: 15572
Epoch: [9]  [  80/2809]  eta: 0:30:53  lr: 0.000045  min_lr: 0.000000  loss: 4.6014 (4.5120)  class_acc: 0.1250 (0.1924)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6479  data: 0.1900  max mem: 15572
Epoch: [9]  [  90/2809]  eta: 0:30:32  lr: 0.000045  min_lr: 0.000000  loss: 4.4736 (4.5014)  class_acc: 0.2083 (0.1964)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6675  data: 0.2239  max mem: 15572
Epoch: [9]  [ 100/2809]  eta: 0:30:14  lr: 0.000045  min_lr: 0.000000  loss: 4.3962 (4.4964)  class_acc: 0.2500 (0.2005)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.1974  max mem: 15572
Epoch: [9]  [ 110/2809]  eta: 0:29:45  lr: 0.000045  min_lr: 0.000000  loss: 4.5130 (4.4923)  class_acc: 0.2083 (0.1993)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1770  max mem: 15572
Epoch: [9]  [ 120/2809]  eta: 0:29:19  lr: 0.000045  min_lr: 0.000000  loss: 4.5134 (4.4922)  class_acc: 0.1667 (0.1956)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1487  max mem: 15572
[2025-01-15 18:47:23,953] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:47:23,953] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 130/2809]  eta: 0:29:06  lr: 0.000045  min_lr: 0.000000  loss: 4.4176 (4.4823)  class_acc: 0.1667 (0.2004)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1581  max mem: 15572
[2025-01-15 18:47:24,371] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25412
[2025-01-15 18:47:24,371] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:47:24,372] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 140/2809]  eta: 0:28:32  lr: 0.000045  min_lr: 0.000000  loss: 4.4742 (4.4885)  class_acc: 0.2083 (0.2033)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1172  max mem: 15572
Epoch: [9]  [ 150/2809]  eta: 0:28:09  lr: 0.000045  min_lr: 0.000000  loss: 4.5954 (4.4945)  class_acc: 0.1250 (0.1973)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0768  max mem: 15572
Epoch: [9]  [ 160/2809]  eta: 0:27:54  lr: 0.000045  min_lr: 0.000000  loss: 4.5246 (4.4959)  class_acc: 0.1250 (0.1972)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1074  max mem: 15572
Epoch: [9]  [ 170/2809]  eta: 0:27:48  lr: 0.000045  min_lr: 0.000000  loss: 4.4622 (4.4951)  class_acc: 0.1250 (0.1935)  loss_scale: 65536.0000 (66302.5029)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1314  max mem: 15572
Epoch: [9]  [ 180/2809]  eta: 0:27:45  lr: 0.000045  min_lr: 0.000000  loss: 4.4333 (4.4850)  class_acc: 0.1667 (0.1950)  loss_scale: 65536.0000 (66260.1547)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.1632  max mem: 15572
Epoch: [9]  [ 190/2809]  eta: 0:27:30  lr: 0.000045  min_lr: 0.000000  loss: 4.3711 (4.4727)  class_acc: 0.2083 (0.1966)  loss_scale: 65536.0000 (66222.2408)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1410  max mem: 15572
Epoch: [9]  [ 200/2809]  eta: 0:27:18  lr: 0.000045  min_lr: 0.000000  loss: 4.4437 (4.4790)  class_acc: 0.2500 (0.1984)  loss_scale: 65536.0000 (66188.0995)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1266  max mem: 15572
Epoch: [9]  [ 210/2809]  eta: 0:27:20  lr: 0.000045  min_lr: 0.000000  loss: 4.5430 (4.4826)  class_acc: 0.1667 (0.1953)  loss_scale: 65536.0000 (66157.1943)  weight_decay: 0.0500 (0.0500)  time: 0.6438  data: 0.2001  max mem: 15572
Epoch: [9]  [ 220/2809]  eta: 0:27:02  lr: 0.000045  min_lr: 0.000000  loss: 4.5061 (4.4769)  class_acc: 0.1250 (0.1970)  loss_scale: 65536.0000 (66129.0860)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1681  max mem: 15572
Epoch: [9]  [ 230/2809]  eta: 0:26:45  lr: 0.000045  min_lr: 0.000000  loss: 4.3703 (4.4750)  class_acc: 0.2500 (0.1970)  loss_scale: 65536.0000 (66103.4113)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0888  max mem: 15572
Epoch: [9]  [ 240/2809]  eta: 0:26:36  lr: 0.000045  min_lr: 0.000000  loss: 4.3933 (4.4694)  class_acc: 0.2083 (0.1980)  loss_scale: 65536.0000 (66079.8672)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1178  max mem: 15572
Epoch: [9]  [ 250/2809]  eta: 0:26:16  lr: 0.000045  min_lr: 0.000000  loss: 4.3140 (4.4633)  class_acc: 0.2083 (0.1992)  loss_scale: 65536.0000 (66058.1992)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0980  max mem: 15572
[2025-01-15 18:48:39,253] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:48:39,253] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 260/2809]  eta: 0:26:08  lr: 0.000045  min_lr: 0.000000  loss: 4.4181 (4.4629)  class_acc: 0.2083 (0.1988)  loss_scale: 65536.0000 (66289.2874)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.1089  max mem: 15572
Epoch: [9]  [ 270/2809]  eta: 0:26:03  lr: 0.000045  min_lr: 0.000000  loss: 4.4111 (4.4588)  class_acc: 0.1667 (0.1962)  loss_scale: 131072.0000 (68679.7934)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1690  max mem: 15572
[2025-01-15 18:48:45,870] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25552
[2025-01-15 18:48:45,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:48:45,870] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 280/2809]  eta: 0:26:02  lr: 0.000045  min_lr: 0.000000  loss: 4.4349 (4.4608)  class_acc: 0.1250 (0.1966)  loss_scale: 65536.0000 (68567.9146)  weight_decay: 0.0500 (0.0500)  time: 0.6470  data: 0.2022  max mem: 15572
Epoch: [9]  [ 290/2809]  eta: 0:25:57  lr: 0.000045  min_lr: 0.000000  loss: 4.5520 (4.4630)  class_acc: 0.1667 (0.1953)  loss_scale: 65536.0000 (68463.7251)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.2075  max mem: 15572
Epoch: [9]  [ 300/2809]  eta: 0:25:47  lr: 0.000045  min_lr: 0.000000  loss: 4.4782 (4.4630)  class_acc: 0.1250 (0.1949)  loss_scale: 65536.0000 (68366.4585)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1473  max mem: 15572
Epoch: [9]  [ 310/2809]  eta: 0:25:38  lr: 0.000045  min_lr: 0.000000  loss: 4.4192 (4.4624)  class_acc: 0.1667 (0.1943)  loss_scale: 65536.0000 (68275.4469)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1341  max mem: 15572
Epoch: [9]  [ 320/2809]  eta: 0:25:38  lr: 0.000045  min_lr: 0.000000  loss: 4.4291 (4.4609)  class_acc: 0.1667 (0.1934)  loss_scale: 65536.0000 (68190.1059)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.2043  max mem: 15572
Epoch: [9]  [ 330/2809]  eta: 0:25:30  lr: 0.000045  min_lr: 0.000000  loss: 4.4560 (4.4621)  class_acc: 0.2083 (0.1949)  loss_scale: 65536.0000 (68109.9215)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.2061  max mem: 15572
Epoch: [9]  [ 340/2809]  eta: 0:25:25  lr: 0.000045  min_lr: 0.000000  loss: 4.4667 (4.4645)  class_acc: 0.2083 (0.1946)  loss_scale: 65536.0000 (68034.4399)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1396  max mem: 15572
Epoch: [9]  [ 350/2809]  eta: 0:25:17  lr: 0.000045  min_lr: 0.000000  loss: 4.6217 (4.4686)  class_acc: 0.1667 (0.1942)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1101  max mem: 15572
Epoch: [9]  [ 360/2809]  eta: 0:25:07  lr: 0.000045  min_lr: 0.000000  loss: 4.6117 (4.4670)  class_acc: 0.1667 (0.1947)  loss_scale: 65536.0000 (67896.0222)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0906  max mem: 15572
Epoch: [9]  [ 370/2809]  eta: 0:24:58  lr: 0.000045  min_lr: 0.000000  loss: 4.3956 (4.4650)  class_acc: 0.1667 (0.1949)  loss_scale: 65536.0000 (67832.4097)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0936  max mem: 15572
Epoch: [9]  [ 380/2809]  eta: 0:24:55  lr: 0.000045  min_lr: 0.000000  loss: 4.4481 (4.4671)  class_acc: 0.1667 (0.1947)  loss_scale: 65536.0000 (67772.1365)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1380  max mem: 15572
Epoch: [9]  [ 390/2809]  eta: 0:24:45  lr: 0.000045  min_lr: 0.000000  loss: 4.4885 (4.4656)  class_acc: 0.2083 (0.1955)  loss_scale: 65536.0000 (67714.9463)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1257  max mem: 15572
[2025-01-15 18:50:05,045] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:50:05,045] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 400/2809]  eta: 0:24:40  lr: 0.000045  min_lr: 0.000000  loss: 4.3498 (4.4622)  class_acc: 0.2083 (0.1958)  loss_scale: 65536.0000 (67824.0399)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1277  max mem: 15572
[2025-01-15 18:50:05,542] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25682
[2025-01-15 18:50:05,543] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:50:05,543] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 410/2809]  eta: 0:24:33  lr: 0.000045  min_lr: 0.000000  loss: 4.3672 (4.4604)  class_acc: 0.2083 (0.1964)  loss_scale: 65536.0000 (67768.3698)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1668  max mem: 15572
Epoch: [9]  [ 420/2809]  eta: 0:24:29  lr: 0.000045  min_lr: 0.000000  loss: 4.4243 (4.4607)  class_acc: 0.2083 (0.1962)  loss_scale: 65536.0000 (67715.3444)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1662  max mem: 15572
Epoch: [9]  [ 430/2809]  eta: 0:24:16  lr: 0.000045  min_lr: 0.000000  loss: 4.4958 (4.4632)  class_acc: 0.1667 (0.1962)  loss_scale: 65536.0000 (67664.7796)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1267  max mem: 15572
Epoch: [9]  [ 440/2809]  eta: 0:24:04  lr: 0.000045  min_lr: 0.000000  loss: 4.4630 (4.4631)  class_acc: 0.1667 (0.1962)  loss_scale: 65536.0000 (67616.5079)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0648  max mem: 15572
Epoch: [9]  [ 450/2809]  eta: 0:23:57  lr: 0.000045  min_lr: 0.000000  loss: 4.4144 (4.4625)  class_acc: 0.1667 (0.1961)  loss_scale: 65536.0000 (67570.3769)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0997  max mem: 15572
Epoch: [9]  [ 460/2809]  eta: 0:23:51  lr: 0.000045  min_lr: 0.000000  loss: 4.3902 (4.4640)  class_acc: 0.1667 (0.1959)  loss_scale: 65536.0000 (67526.2473)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1406  max mem: 15572
Epoch: [9]  [ 470/2809]  eta: 0:23:45  lr: 0.000045  min_lr: 0.000000  loss: 4.4539 (4.4641)  class_acc: 0.1250 (0.1948)  loss_scale: 65536.0000 (67483.9915)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1509  max mem: 15572
Epoch: [9]  [ 480/2809]  eta: 0:23:43  lr: 0.000045  min_lr: 0.000000  loss: 4.4539 (4.4647)  class_acc: 0.1667 (0.1953)  loss_scale: 65536.0000 (67443.4927)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.1789  max mem: 15572
Epoch: [9]  [ 490/2809]  eta: 0:23:36  lr: 0.000045  min_lr: 0.000000  loss: 4.4407 (4.4613)  class_acc: 0.2500 (0.1968)  loss_scale: 65536.0000 (67404.6436)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1586  max mem: 15572
Epoch: [9]  [ 500/2809]  eta: 0:23:25  lr: 0.000045  min_lr: 0.000000  loss: 4.3275 (4.4579)  class_acc: 0.2917 (0.1977)  loss_scale: 65536.0000 (67367.3453)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0835  max mem: 15572
Epoch: [9]  [ 510/2809]  eta: 0:23:20  lr: 0.000045  min_lr: 0.000000  loss: 4.3333 (4.4570)  class_acc: 0.2500 (0.1986)  loss_scale: 65536.0000 (67331.5068)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0907  max mem: 15572
Epoch: [9]  [ 520/2809]  eta: 0:23:12  lr: 0.000045  min_lr: 0.000000  loss: 4.3675 (4.4550)  class_acc: 0.2500 (0.1994)  loss_scale: 65536.0000 (67297.0441)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1364  max mem: 15572
[2025-01-15 18:51:21,999] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:51:21,999] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 530/2809]  eta: 0:23:07  lr: 0.000045  min_lr: 0.000000  loss: 4.3608 (4.4554)  class_acc: 0.2500 (0.1997)  loss_scale: 65536.0000 (67387.2994)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1527  max mem: 15572
[2025-01-15 18:51:27,488] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25816
[2025-01-15 18:51:27,488] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:51:27,488] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 540/2809]  eta: 0:23:08  lr: 0.000045  min_lr: 0.000000  loss: 4.4106 (4.4556)  class_acc: 0.2083 (0.2005)  loss_scale: 65536.0000 (67837.6340)  weight_decay: 0.0500 (0.0500)  time: 0.6967  data: 0.2445  max mem: 15572
Epoch: [9]  [ 550/2809]  eta: 0:23:00  lr: 0.000045  min_lr: 0.000000  loss: 4.4304 (4.4553)  class_acc: 0.2083 (0.2010)  loss_scale: 65536.0000 (67795.8621)  weight_decay: 0.0500 (0.0500)  time: 0.6666  data: 0.2198  max mem: 15572
Epoch: [9]  [ 560/2809]  eta: 0:22:52  lr: 0.000045  min_lr: 0.000000  loss: 4.3937 (4.4551)  class_acc: 0.2083 (0.2007)  loss_scale: 65536.0000 (67755.5793)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1218  max mem: 15572
Epoch: [9]  [ 570/2809]  eta: 0:22:43  lr: 0.000045  min_lr: 0.000000  loss: 4.4866 (4.4582)  class_acc: 0.2083 (0.2002)  loss_scale: 65536.0000 (67716.7075)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1160  max mem: 15572
Epoch: [9]  [ 580/2809]  eta: 0:22:35  lr: 0.000045  min_lr: 0.000000  loss: 4.4866 (4.4583)  class_acc: 0.2083 (0.2008)  loss_scale: 65536.0000 (67679.1738)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0943  max mem: 15572
Epoch: [9]  [ 590/2809]  eta: 0:22:28  lr: 0.000045  min_lr: 0.000000  loss: 4.4334 (4.4598)  class_acc: 0.1667 (0.1998)  loss_scale: 65536.0000 (67642.9103)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1091  max mem: 15572
Epoch: [9]  [ 600/2809]  eta: 0:22:20  lr: 0.000045  min_lr: 0.000000  loss: 4.4173 (4.4565)  class_acc: 0.1250 (0.1997)  loss_scale: 65536.0000 (67607.8536)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1305  max mem: 15572
Epoch: [9]  [ 610/2809]  eta: 0:22:14  lr: 0.000045  min_lr: 0.000000  loss: 4.3777 (4.4557)  class_acc: 0.1667 (0.1996)  loss_scale: 65536.0000 (67573.9444)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1315  max mem: 15572
Epoch: [9]  [ 620/2809]  eta: 0:22:06  lr: 0.000045  min_lr: 0.000000  loss: 4.3858 (4.4551)  class_acc: 0.1667 (0.2001)  loss_scale: 65536.0000 (67541.1272)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1096  max mem: 15572
Epoch: [9]  [ 630/2809]  eta: 0:21:59  lr: 0.000045  min_lr: 0.000000  loss: 4.3858 (4.4564)  class_acc: 0.2083 (0.2016)  loss_scale: 65536.0000 (67509.3502)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0962  max mem: 15572
Epoch: [9]  [ 640/2809]  eta: 0:21:52  lr: 0.000045  min_lr: 0.000000  loss: 4.3566 (4.4537)  class_acc: 0.2500 (0.2016)  loss_scale: 65536.0000 (67478.5647)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1144  max mem: 15572
Epoch: [9]  [ 650/2809]  eta: 0:21:44  lr: 0.000045  min_lr: 0.000000  loss: 4.3824 (4.4555)  class_acc: 0.2083 (0.2016)  loss_scale: 65536.0000 (67448.7250)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0979  max mem: 15572
Epoch: [9]  [ 660/2809]  eta: 0:21:38  lr: 0.000045  min_lr: 0.000000  loss: 4.5858 (4.4570)  class_acc: 0.1667 (0.2018)  loss_scale: 65536.0000 (67419.7882)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0888  max mem: 15572
[2025-01-15 18:52:42,062] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:52:42,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:52:42,901] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25947
[2025-01-15 18:52:42,901] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:52:42,901] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 670/2809]  eta: 0:21:34  lr: 0.000045  min_lr: 0.000000  loss: 4.5084 (4.4567)  class_acc: 0.1667 (0.2020)  loss_scale: 65536.0000 (67587.0522)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1483  max mem: 15572
Epoch: [9]  [ 680/2809]  eta: 0:21:30  lr: 0.000045  min_lr: 0.000000  loss: 4.4084 (4.4563)  class_acc: 0.2083 (0.2025)  loss_scale: 65536.0000 (67556.9339)  weight_decay: 0.0500 (0.0500)  time: 0.6681  data: 0.2263  max mem: 15572
Epoch: [9]  [ 690/2809]  eta: 0:21:25  lr: 0.000045  min_lr: 0.000000  loss: 4.6156 (4.4589)  class_acc: 0.2083 (0.2023)  loss_scale: 65536.0000 (67527.6874)  weight_decay: 0.0500 (0.0500)  time: 0.6568  data: 0.2188  max mem: 15572
Epoch: [9]  [ 700/2809]  eta: 0:21:20  lr: 0.000045  min_lr: 0.000000  loss: 4.5374 (4.4578)  class_acc: 0.2083 (0.2025)  loss_scale: 65536.0000 (67499.2753)  weight_decay: 0.0500 (0.0500)  time: 0.6477  data: 0.1934  max mem: 15572
Epoch: [9]  [ 710/2809]  eta: 0:21:12  lr: 0.000045  min_lr: 0.000000  loss: 4.2883 (4.4554)  class_acc: 0.2500 (0.2025)  loss_scale: 65536.0000 (67471.6624)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1373  max mem: 15572
[2025-01-15 18:53:15,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=168, lr=[4.3781711929090247e-07, 4.3781711929090247e-07, 6.254530275584321e-07, 6.254530275584321e-07, 8.935043250834747e-07, 8.935043250834747e-07, 1.2764347501192496e-06, 1.2764347501192496e-06, 1.8234782144560709e-06, 1.8234782144560709e-06, 2.604968877794387e-06, 2.604968877794387e-06, 3.721384111134839e-06, 3.721384111134839e-06, 5.3162630159069135e-06, 5.3162630159069135e-06, 7.59466145129559e-06, 7.59466145129559e-06, 1.0849516358993702e-05, 1.0849516358993702e-05, 1.5499309084276716e-05, 1.5499309084276716e-05, 2.2141870120395312e-05, 2.2141870120395312e-05, 3.163124302913616e-05, 3.163124302913616e-05, 4.518749004162309e-05, 4.518749004162309e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 18:53:15,683] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=27.50022386613258, CurrSamplesPerSec=23.265250766970066, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [ 720/2809]  eta: 0:21:09  lr: 0.000045  min_lr: 0.000000  loss: 4.3573 (4.4556)  class_acc: 0.2083 (0.2030)  loss_scale: 65536.0000 (67444.8155)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1395  max mem: 15572
Epoch: [9]  [ 730/2809]  eta: 0:21:02  lr: 0.000045  min_lr: 0.000000  loss: 4.4736 (4.4566)  class_acc: 0.2083 (0.2026)  loss_scale: 65536.0000 (67418.7031)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.1441  max mem: 15572
Epoch: [9]  [ 740/2809]  eta: 0:20:57  lr: 0.000045  min_lr: 0.000000  loss: 4.4421 (4.4556)  class_acc: 0.2083 (0.2029)  loss_scale: 65536.0000 (67393.2955)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1085  max mem: 15572
Epoch: [9]  [ 750/2809]  eta: 0:20:48  lr: 0.000045  min_lr: 0.000000  loss: 4.4594 (4.4575)  class_acc: 0.1667 (0.2020)  loss_scale: 65536.0000 (67368.5646)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0855  max mem: 15572
Epoch: [9]  [ 760/2809]  eta: 0:20:42  lr: 0.000045  min_lr: 0.000000  loss: 4.5082 (4.4567)  class_acc: 0.1667 (0.2026)  loss_scale: 65536.0000 (67344.4836)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0946  max mem: 15572
Epoch: [9]  [ 770/2809]  eta: 0:20:35  lr: 0.000045  min_lr: 0.000000  loss: 4.4502 (4.4562)  class_acc: 0.2083 (0.2022)  loss_scale: 65536.0000 (67321.0272)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1259  max mem: 15572
[2025-01-15 18:53:46,837] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26053
[2025-01-15 18:53:46,838] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:53:46,838] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 780/2809]  eta: 0:20:27  lr: 0.000045  min_lr: 0.000000  loss: 4.3771 (4.4545)  class_acc: 0.2083 (0.2026)  loss_scale: 65536.0000 (66920.5634)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.0975  max mem: 15572
Epoch: [9]  [ 790/2809]  eta: 0:20:21  lr: 0.000045  min_lr: 0.000000  loss: 4.2176 (4.4541)  class_acc: 0.2083 (0.2026)  loss_scale: 32768.0000 (66488.7990)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1254  max mem: 15572
Epoch: [9]  [ 800/2809]  eta: 0:20:14  lr: 0.000045  min_lr: 0.000000  loss: 4.3695 (4.4538)  class_acc: 0.2083 (0.2022)  loss_scale: 32768.0000 (66067.8152)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1396  max mem: 15572
Epoch: [9]  [ 810/2809]  eta: 0:20:09  lr: 0.000045  min_lr: 0.000000  loss: 4.4158 (4.4537)  class_acc: 0.1667 (0.2018)  loss_scale: 32768.0000 (65657.2133)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1601  max mem: 15572
Epoch: [9]  [ 820/2809]  eta: 0:20:00  lr: 0.000045  min_lr: 0.000000  loss: 4.4610 (4.4541)  class_acc: 0.1667 (0.2015)  loss_scale: 32768.0000 (65256.6139)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1239  max mem: 15572
Epoch: [9]  [ 830/2809]  eta: 0:19:55  lr: 0.000045  min_lr: 0.000000  loss: 4.5198 (4.4536)  class_acc: 0.1667 (0.2015)  loss_scale: 32768.0000 (64865.6558)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1126  max mem: 15572
Epoch: [9]  [ 840/2809]  eta: 0:19:49  lr: 0.000045  min_lr: 0.000000  loss: 4.4250 (4.4519)  class_acc: 0.2500 (0.2026)  loss_scale: 32768.0000 (64483.9952)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1373  max mem: 15572
Epoch: [9]  [ 850/2809]  eta: 0:19:40  lr: 0.000045  min_lr: 0.000000  loss: 4.4608 (4.4526)  class_acc: 0.2500 (0.2027)  loss_scale: 32768.0000 (64111.3043)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0480  max mem: 15572
Epoch: [9]  [ 860/2809]  eta: 0:19:33  lr: 0.000045  min_lr: 0.000000  loss: 4.5094 (4.4530)  class_acc: 0.1667 (0.2019)  loss_scale: 32768.0000 (63747.2706)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0375  max mem: 15572
Epoch: [9]  [ 870/2809]  eta: 0:19:26  lr: 0.000045  min_lr: 0.000000  loss: 4.4643 (4.4527)  class_acc: 0.1667 (0.2025)  loss_scale: 32768.0000 (63391.5959)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0849  max mem: 15572
Epoch: [9]  [ 880/2809]  eta: 0:19:19  lr: 0.000045  min_lr: 0.000000  loss: 4.4023 (4.4513)  class_acc: 0.2083 (0.2022)  loss_scale: 32768.0000 (63043.9955)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0901  max mem: 15572
Epoch: [9]  [ 890/2809]  eta: 0:19:13  lr: 0.000045  min_lr: 0.000000  loss: 4.4173 (4.4515)  class_acc: 0.1667 (0.2018)  loss_scale: 32768.0000 (62704.1975)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0823  max mem: 15572
Epoch: [9]  [ 900/2809]  eta: 0:19:06  lr: 0.000045  min_lr: 0.000000  loss: 4.4173 (4.4499)  class_acc: 0.1667 (0.2018)  loss_scale: 32768.0000 (62371.9423)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0946  max mem: 15572
[2025-01-15 18:55:00,701] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:55:00,702] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 910/2809]  eta: 0:19:00  lr: 0.000045  min_lr: 0.000000  loss: 4.3757 (4.4511)  class_acc: 0.1667 (0.2018)  loss_scale: 32768.0000 (62406.6740)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1201  max mem: 15572
Epoch: [9]  [ 920/2809]  eta: 0:18:55  lr: 0.000045  min_lr: 0.000000  loss: 4.5089 (4.4519)  class_acc: 0.2083 (0.2018)  loss_scale: 65536.0000 (62440.6515)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1455  max mem: 15572
Epoch: [9]  [ 930/2809]  eta: 0:18:51  lr: 0.000045  min_lr: 0.000000  loss: 4.4924 (4.4506)  class_acc: 0.2083 (0.2022)  loss_scale: 65536.0000 (62473.8990)  weight_decay: 0.0500 (0.0500)  time: 0.6685  data: 0.2000  max mem: 15572
Epoch: [9]  [ 940/2809]  eta: 0:18:43  lr: 0.000045  min_lr: 0.000000  loss: 4.4456 (4.4506)  class_acc: 0.2083 (0.2024)  loss_scale: 65536.0000 (62506.4400)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1688  max mem: 15572
Epoch: [9]  [ 950/2809]  eta: 0:18:38  lr: 0.000045  min_lr: 0.000000  loss: 4.4367 (4.4503)  class_acc: 0.2083 (0.2031)  loss_scale: 65536.0000 (62538.2965)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1526  max mem: 15572
Epoch: [9]  [ 960/2809]  eta: 0:18:32  lr: 0.000045  min_lr: 0.000000  loss: 4.4326 (4.4505)  class_acc: 0.2083 (0.2033)  loss_scale: 65536.0000 (62569.4901)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1758  max mem: 15572
Epoch: [9]  [ 970/2809]  eta: 0:18:28  lr: 0.000045  min_lr: 0.000000  loss: 4.4226 (4.4512)  class_acc: 0.2083 (0.2029)  loss_scale: 65536.0000 (62600.0412)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1891  max mem: 15572
Epoch: [9]  [ 980/2809]  eta: 0:18:24  lr: 0.000045  min_lr: 0.000000  loss: 4.4704 (4.4520)  class_acc: 0.1667 (0.2027)  loss_scale: 65536.0000 (62629.9694)  weight_decay: 0.0500 (0.0500)  time: 0.7150  data: 0.2428  max mem: 15572
Epoch: [9]  [ 990/2809]  eta: 0:18:15  lr: 0.000045  min_lr: 0.000000  loss: 4.4697 (4.4515)  class_acc: 0.1250 (0.2021)  loss_scale: 65536.0000 (62659.2936)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1352  max mem: 15572
Epoch: [9]  [1000/2809]  eta: 0:18:09  lr: 0.000045  min_lr: 0.000000  loss: 4.3420 (4.4519)  class_acc: 0.1250 (0.2019)  loss_scale: 65536.0000 (62688.0320)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0678  max mem: 15572
Epoch: [9]  [1010/2809]  eta: 0:18:02  lr: 0.000045  min_lr: 0.000000  loss: 4.5414 (4.4529)  class_acc: 0.1667 (0.2020)  loss_scale: 65536.0000 (62716.2018)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0897  max mem: 15572
Epoch: [9]  [1020/2809]  eta: 0:17:55  lr: 0.000045  min_lr: 0.000000  loss: 4.5328 (4.4539)  class_acc: 0.2083 (0.2020)  loss_scale: 65536.0000 (62743.8198)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.0847  max mem: 15572
[2025-01-15 18:56:18,632] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:56:18,633] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [1030/2809]  eta: 0:17:49  lr: 0.000045  min_lr: 0.000000  loss: 4.4984 (4.4542)  class_acc: 0.2083 (0.2019)  loss_scale: 65536.0000 (62898.0330)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1388  max mem: 15572
Epoch: [9]  [1040/2809]  eta: 0:17:44  lr: 0.000045  min_lr: 0.000000  loss: 4.4598 (4.4541)  class_acc: 0.2083 (0.2018)  loss_scale: 131072.0000 (63552.9222)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1584  max mem: 15572
[2025-01-15 18:56:27,008] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26325
[2025-01-15 18:56:27,009] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:56:27,009] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1050/2809]  eta: 0:17:36  lr: 0.000045  min_lr: 0.000000  loss: 4.4472 (4.4539)  class_acc: 0.1667 (0.2017)  loss_scale: 131072.0000 (63758.8582)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0918  max mem: 15572
Epoch: [9]  [1060/2809]  eta: 0:17:28  lr: 0.000045  min_lr: 0.000000  loss: 4.4843 (4.4546)  class_acc: 0.1667 (0.2011)  loss_scale: 65536.0000 (63775.6079)  weight_decay: 0.0500 (0.0500)  time: 0.4935  data: 0.0459  max mem: 15572
Epoch: [9]  [1070/2809]  eta: 0:17:22  lr: 0.000045  min_lr: 0.000000  loss: 4.4480 (4.4546)  class_acc: 0.1667 (0.2012)  loss_scale: 65536.0000 (63792.0448)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1034  max mem: 15572
Epoch: [9]  [1080/2809]  eta: 0:17:17  lr: 0.000045  min_lr: 0.000000  loss: 4.4478 (4.4544)  class_acc: 0.2500 (0.2015)  loss_scale: 65536.0000 (63808.1776)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1863  max mem: 15572
Epoch: [9]  [1090/2809]  eta: 0:17:12  lr: 0.000045  min_lr: 0.000000  loss: 4.4585 (4.4535)  class_acc: 0.2500 (0.2013)  loss_scale: 65536.0000 (63824.0147)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.2086  max mem: 15572
Epoch: [9]  [1100/2809]  eta: 0:17:06  lr: 0.000045  min_lr: 0.000000  loss: 4.4585 (4.4538)  class_acc: 0.1667 (0.2014)  loss_scale: 65536.0000 (63839.5640)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1702  max mem: 15572
Epoch: [9]  [1110/2809]  eta: 0:16:58  lr: 0.000045  min_lr: 0.000000  loss: 4.4798 (4.4533)  class_acc: 0.1667 (0.2013)  loss_scale: 65536.0000 (63854.8335)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0945  max mem: 15572
Epoch: [9]  [1120/2809]  eta: 0:16:52  lr: 0.000045  min_lr: 0.000000  loss: 4.4575 (4.4527)  class_acc: 0.2083 (0.2014)  loss_scale: 65536.0000 (63869.8305)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0643  max mem: 15572
Epoch: [9]  [1130/2809]  eta: 0:16:46  lr: 0.000045  min_lr: 0.000000  loss: 4.4354 (4.4522)  class_acc: 0.1667 (0.2013)  loss_scale: 65536.0000 (63884.5623)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1288  max mem: 15572
Epoch: [9]  [1140/2809]  eta: 0:16:41  lr: 0.000045  min_lr: 0.000000  loss: 4.4375 (4.4523)  class_acc: 0.1667 (0.2010)  loss_scale: 65536.0000 (63899.0359)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.1929  max mem: 15572
Epoch: [9]  [1150/2809]  eta: 0:16:33  lr: 0.000045  min_lr: 0.000000  loss: 4.4495 (4.4513)  class_acc: 0.1667 (0.2008)  loss_scale: 65536.0000 (63913.2580)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1055  max mem: 15572
Epoch: [9]  [1160/2809]  eta: 0:16:27  lr: 0.000045  min_lr: 0.000000  loss: 4.2903 (4.4501)  class_acc: 0.2083 (0.2012)  loss_scale: 65536.0000 (63927.2351)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0801  max mem: 15572
Epoch: [9]  [1170/2809]  eta: 0:16:22  lr: 0.000045  min_lr: 0.000000  loss: 4.3365 (4.4505)  class_acc: 0.2083 (0.2008)  loss_scale: 65536.0000 (63940.9735)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1841  max mem: 15572
[2025-01-15 18:57:43,809] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:57:43,809] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:57:45,032] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26457
[2025-01-15 18:57:45,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:57:45,032] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1180/2809]  eta: 0:16:16  lr: 0.000045  min_lr: 0.000000  loss: 4.5026 (4.4509)  class_acc: 0.1667 (0.2011)  loss_scale: 65536.0000 (64120.9551)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1942  max mem: 15572
Epoch: [9]  [1190/2809]  eta: 0:16:11  lr: 0.000045  min_lr: 0.000000  loss: 4.5030 (4.4504)  class_acc: 0.2083 (0.2015)  loss_scale: 65536.0000 (64132.8363)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.2261  max mem: 15572
Epoch: [9]  [1200/2809]  eta: 0:16:05  lr: 0.000045  min_lr: 0.000000  loss: 4.4099 (4.4498)  class_acc: 0.2083 (0.2015)  loss_scale: 65536.0000 (64144.5196)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1719  max mem: 15572
Epoch: [9]  [1210/2809]  eta: 0:15:59  lr: 0.000045  min_lr: 0.000000  loss: 4.4654 (4.4504)  class_acc: 0.2083 (0.2016)  loss_scale: 65536.0000 (64156.0099)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1089  max mem: 15572
Epoch: [9]  [1220/2809]  eta: 0:15:52  lr: 0.000045  min_lr: 0.000000  loss: 4.5592 (4.4513)  class_acc: 0.1667 (0.2009)  loss_scale: 65536.0000 (64167.3120)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1133  max mem: 15572
Epoch: [9]  [1230/2809]  eta: 0:15:47  lr: 0.000045  min_lr: 0.000000  loss: 4.4720 (4.4516)  class_acc: 0.1250 (0.2007)  loss_scale: 65536.0000 (64178.4305)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1369  max mem: 15572
Epoch: [9]  [1240/2809]  eta: 0:15:40  lr: 0.000045  min_lr: 0.000000  loss: 4.4505 (4.4520)  class_acc: 0.1667 (0.2006)  loss_scale: 65536.0000 (64189.3699)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1392  max mem: 15572
Epoch: [9]  [1250/2809]  eta: 0:15:35  lr: 0.000045  min_lr: 0.000000  loss: 4.4744 (4.4532)  class_acc: 0.1667 (0.2005)  loss_scale: 65536.0000 (64200.1343)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1148  max mem: 15572
Epoch: [9]  [1260/2809]  eta: 0:15:28  lr: 0.000045  min_lr: 0.000000  loss: 4.6012 (4.4543)  class_acc: 0.1667 (0.2000)  loss_scale: 65536.0000 (64210.7280)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.0924  max mem: 15572
[2025-01-15 18:58:39,742] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26546
[2025-01-15 18:58:39,742] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:58:39,742] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [1270/2809]  eta: 0:15:23  lr: 0.000045  min_lr: 0.000000  loss: 4.5717 (4.4549)  class_acc: 0.1667 (0.2000)  loss_scale: 65536.0000 (64066.4673)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1261  max mem: 15572
Epoch: [9]  [1280/2809]  eta: 0:15:16  lr: 0.000045  min_lr: 0.000000  loss: 4.4351 (4.4544)  class_acc: 0.1667 (0.2004)  loss_scale: 32768.0000 (63822.1390)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1069  max mem: 15572
Epoch: [9]  [1290/2809]  eta: 0:15:09  lr: 0.000045  min_lr: 0.000000  loss: 4.4303 (4.4550)  class_acc: 0.1667 (0.2002)  loss_scale: 32768.0000 (63581.5957)  weight_decay: 0.0500 (0.0500)  time: 0.4845  data: 0.0012  max mem: 15572
Epoch: [9]  [1300/2809]  eta: 0:15:02  lr: 0.000045  min_lr: 0.000000  loss: 4.4472 (4.4552)  class_acc: 0.1667 (0.2002)  loss_scale: 32768.0000 (63344.7502)  weight_decay: 0.0500 (0.0500)  time: 0.5166  data: 0.0575  max mem: 15572
Epoch: [9]  [1310/2809]  eta: 0:14:56  lr: 0.000045  min_lr: 0.000000  loss: 4.3715 (4.4543)  class_acc: 0.1667 (0.1999)  loss_scale: 32768.0000 (63111.5179)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1245  max mem: 15572
Epoch: [9]  [1320/2809]  eta: 0:14:51  lr: 0.000045  min_lr: 0.000000  loss: 4.4092 (4.4545)  class_acc: 0.1667 (0.1997)  loss_scale: 32768.0000 (62881.8168)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1591  max mem: 15572
Epoch: [9]  [1330/2809]  eta: 0:14:45  lr: 0.000045  min_lr: 0.000000  loss: 4.5330 (4.4549)  class_acc: 0.1667 (0.1999)  loss_scale: 32768.0000 (62655.5672)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.1850  max mem: 15572
Epoch: [9]  [1340/2809]  eta: 0:14:38  lr: 0.000045  min_lr: 0.000000  loss: 4.4203 (4.4546)  class_acc: 0.2083 (0.2001)  loss_scale: 32768.0000 (62432.6920)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1366  max mem: 15572
Epoch: [9]  [1350/2809]  eta: 0:14:32  lr: 0.000045  min_lr: 0.000000  loss: 4.4118 (4.4544)  class_acc: 0.2083 (0.2000)  loss_scale: 32768.0000 (62213.1162)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1204  max mem: 15572
Epoch: [9]  [1360/2809]  eta: 0:14:27  lr: 0.000045  min_lr: 0.000000  loss: 4.2901 (4.4533)  class_acc: 0.2083 (0.2004)  loss_scale: 32768.0000 (61996.7671)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1474  max mem: 15572
Epoch: [9]  [1370/2809]  eta: 0:14:20  lr: 0.000045  min_lr: 0.000000  loss: 4.4055 (4.4540)  class_acc: 0.1667 (0.1999)  loss_scale: 32768.0000 (61783.5740)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0705  max mem: 15572
Epoch: [9]  [1380/2809]  eta: 0:14:14  lr: 0.000045  min_lr: 0.000000  loss: 4.4715 (4.4541)  class_acc: 0.1250 (0.1996)  loss_scale: 32768.0000 (61573.4685)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0915  max mem: 15572
Epoch: [9]  [1390/2809]  eta: 0:14:08  lr: 0.000045  min_lr: 0.000000  loss: 4.4612 (4.4543)  class_acc: 0.1250 (0.1995)  loss_scale: 32768.0000 (61366.3839)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1453  max mem: 15572
[2025-01-15 18:59:52,516] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:59:52,517] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [1400/2809]  eta: 0:14:02  lr: 0.000045  min_lr: 0.000000  loss: 4.6058 (4.4558)  class_acc: 0.1667 (0.1993)  loss_scale: 32768.0000 (61325.9786)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1157  max mem: 15572
Epoch: [9]  [1410/2809]  eta: 0:13:55  lr: 0.000045  min_lr: 0.000000  loss: 4.5909 (4.4558)  class_acc: 0.1667 (0.1992)  loss_scale: 65536.0000 (61355.8157)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1235  max mem: 15572
Epoch: [9]  [1420/2809]  eta: 0:13:50  lr: 0.000045  min_lr: 0.000000  loss: 4.4162 (4.4564)  class_acc: 0.1667 (0.1993)  loss_scale: 65536.0000 (61385.2329)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1551  max mem: 15572
Epoch: [9]  [1430/2809]  eta: 0:13:44  lr: 0.000045  min_lr: 0.000000  loss: 4.4924 (4.4567)  class_acc: 0.1667 (0.1993)  loss_scale: 65536.0000 (61414.2390)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.1428  max mem: 15572
Epoch: [9]  [1440/2809]  eta: 0:13:38  lr: 0.000045  min_lr: 0.000000  loss: 4.5093 (4.4575)  class_acc: 0.2083 (0.1992)  loss_scale: 65536.0000 (61442.8425)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1184  max mem: 15572
Epoch: [9]  [1450/2809]  eta: 0:13:31  lr: 0.000045  min_lr: 0.000000  loss: 4.5043 (4.4575)  class_acc: 0.2083 (0.1993)  loss_scale: 65536.0000 (61471.0517)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0978  max mem: 15572
Epoch: [9]  [1460/2809]  eta: 0:13:26  lr: 0.000045  min_lr: 0.000000  loss: 4.4247 (4.4569)  class_acc: 0.2083 (0.1995)  loss_scale: 65536.0000 (61498.8747)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1497  max mem: 15572
Epoch: [9]  [1470/2809]  eta: 0:13:20  lr: 0.000045  min_lr: 0.000000  loss: 4.3508 (4.4564)  class_acc: 0.2083 (0.1995)  loss_scale: 65536.0000 (61526.3195)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1794  max mem: 15572
Epoch: [9]  [1480/2809]  eta: 0:13:12  lr: 0.000045  min_lr: 0.000000  loss: 4.3508 (4.4555)  class_acc: 0.2083 (0.1997)  loss_scale: 65536.0000 (61553.3937)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0694  max mem: 15572
Epoch: [9]  [1490/2809]  eta: 0:13:08  lr: 0.000045  min_lr: 0.000000  loss: 4.4270 (4.4559)  class_acc: 0.2083 (0.1996)  loss_scale: 65536.0000 (61580.1046)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1319  max mem: 15572
Epoch: [9]  [1500/2809]  eta: 0:13:01  lr: 0.000045  min_lr: 0.000000  loss: 4.5413 (4.4564)  class_acc: 0.2083 (0.1998)  loss_scale: 65536.0000 (61606.4597)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1218  max mem: 15572
Epoch: [9]  [1510/2809]  eta: 0:12:54  lr: 0.000045  min_lr: 0.000000  loss: 4.4359 (4.4562)  class_acc: 0.2500 (0.2001)  loss_scale: 65536.0000 (61632.4659)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0353  max mem: 15572
Epoch: [9]  [1520/2809]  eta: 0:12:48  lr: 0.000045  min_lr: 0.000000  loss: 4.4060 (4.4559)  class_acc: 0.1667 (0.2000)  loss_scale: 65536.0000 (61658.1302)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0865  max mem: 15572
[2025-01-15 19:01:07,225] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:01:07,225] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:01:09,272] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26805
[2025-01-15 19:01:09,272] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:01:09,272] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1530/2809]  eta: 0:12:42  lr: 0.000045  min_lr: 0.000000  loss: 4.4780 (4.4565)  class_acc: 0.1667 (0.1998)  loss_scale: 65536.0000 (61769.0712)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1158  max mem: 15572
Epoch: [9]  [1540/2809]  eta: 0:12:37  lr: 0.000045  min_lr: 0.000000  loss: 4.5135 (4.4569)  class_acc: 0.1250 (0.1994)  loss_scale: 65536.0000 (61793.5159)  weight_decay: 0.0500 (0.0500)  time: 0.6612  data: 0.1882  max mem: 15572
Epoch: [9]  [1550/2809]  eta: 0:12:32  lr: 0.000045  min_lr: 0.000000  loss: 4.5070 (4.4570)  class_acc: 0.1250 (0.1992)  loss_scale: 65536.0000 (61817.6454)  weight_decay: 0.0500 (0.0500)  time: 0.6800  data: 0.1942  max mem: 15572
Epoch: [9]  [1560/2809]  eta: 0:12:25  lr: 0.000045  min_lr: 0.000000  loss: 4.4520 (4.4564)  class_acc: 0.1667 (0.1993)  loss_scale: 65536.0000 (61841.4657)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1113  max mem: 15572
Epoch: [9]  [1570/2809]  eta: 0:12:19  lr: 0.000045  min_lr: 0.000000  loss: 4.4520 (4.4571)  class_acc: 0.1667 (0.1991)  loss_scale: 65536.0000 (61864.9828)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1091  max mem: 15572
Epoch: [9]  [1580/2809]  eta: 0:12:14  lr: 0.000045  min_lr: 0.000000  loss: 4.5653 (4.4580)  class_acc: 0.1250 (0.1988)  loss_scale: 65536.0000 (61888.2024)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.2183  max mem: 15572
Epoch: [9]  [1590/2809]  eta: 0:12:08  lr: 0.000045  min_lr: 0.000000  loss: 4.5637 (4.4580)  class_acc: 0.1250 (0.1985)  loss_scale: 65536.0000 (61911.1301)  weight_decay: 0.0500 (0.0500)  time: 0.6771  data: 0.2381  max mem: 15572
Epoch: [9]  [1600/2809]  eta: 0:12:02  lr: 0.000045  min_lr: 0.000000  loss: 4.5239 (4.4588)  class_acc: 0.1667 (0.1986)  loss_scale: 65536.0000 (61933.7714)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1709  max mem: 15572
Epoch: [9]  [1610/2809]  eta: 0:11:57  lr: 0.000045  min_lr: 0.000000  loss: 4.5353 (4.4594)  class_acc: 0.2083 (0.1987)  loss_scale: 65536.0000 (61956.1316)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1702  max mem: 15572
Epoch: [9]  [1620/2809]  eta: 0:11:51  lr: 0.000045  min_lr: 0.000000  loss: 4.5780 (4.4601)  class_acc: 0.2083 (0.1986)  loss_scale: 65536.0000 (61978.2159)  weight_decay: 0.0500 (0.0500)  time: 0.6394  data: 0.1588  max mem: 15572
Epoch: [9]  [1630/2809]  eta: 0:11:45  lr: 0.000045  min_lr: 0.000000  loss: 4.5453 (4.4602)  class_acc: 0.1667 (0.1987)  loss_scale: 65536.0000 (62000.0294)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0860  max mem: 15572
Epoch: [9]  [1640/2809]  eta: 0:11:39  lr: 0.000045  min_lr: 0.000000  loss: 4.4940 (4.4610)  class_acc: 0.2083 (0.1990)  loss_scale: 65536.0000 (62021.5771)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0991  max mem: 15572
[2025-01-15 19:02:24,531] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26928
[2025-01-15 19:02:24,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:02:24,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [1650/2809]  eta: 0:11:32  lr: 0.000045  min_lr: 0.000000  loss: 4.4733 (4.4603)  class_acc: 0.2083 (0.1993)  loss_scale: 65536.0000 (61963.4743)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1010  max mem: 15572
Epoch: [9]  [1660/2809]  eta: 0:11:26  lr: 0.000045  min_lr: 0.000000  loss: 4.3267 (4.4600)  class_acc: 0.2500 (0.1995)  loss_scale: 32768.0000 (61787.7038)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0842  max mem: 15572
Epoch: [9]  [1670/2809]  eta: 0:11:21  lr: 0.000045  min_lr: 0.000000  loss: 4.4544 (4.4599)  class_acc: 0.2083 (0.1996)  loss_scale: 32768.0000 (61614.0371)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1700  max mem: 15572
Epoch: [9]  [1680/2809]  eta: 0:11:14  lr: 0.000045  min_lr: 0.000000  loss: 4.5412 (4.4604)  class_acc: 0.2083 (0.1995)  loss_scale: 32768.0000 (61442.4366)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1545  max mem: 15572
Epoch: [9]  [1690/2809]  eta: 0:11:08  lr: 0.000045  min_lr: 0.000000  loss: 4.4540 (4.4598)  class_acc: 0.2083 (0.1997)  loss_scale: 32768.0000 (61272.8658)  weight_decay: 0.0500 (0.0500)  time: 0.5269  data: 0.0768  max mem: 15572
Epoch: [9]  [1700/2809]  eta: 0:11:02  lr: 0.000045  min_lr: 0.000000  loss: 4.3320 (4.4593)  class_acc: 0.2083 (0.2000)  loss_scale: 32768.0000 (61105.2887)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0993  max mem: 15572
Epoch: [9]  [1710/2809]  eta: 0:10:55  lr: 0.000045  min_lr: 0.000000  loss: 4.4034 (4.4593)  class_acc: 0.2083 (0.1999)  loss_scale: 32768.0000 (60939.6704)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1250  max mem: 15572
[2025-01-15 19:03:05,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=174, lr=[4.3500795337734684e-07, 4.3500795337734684e-07, 6.214399333962098e-07, 6.214399333962098e-07, 8.877713334231571e-07, 8.877713334231571e-07, 1.2682447620330817e-06, 1.2682447620330817e-06, 1.811778231475831e-06, 1.811778231475831e-06, 2.5882546163940444e-06, 2.5882546163940444e-06, 3.697506594848635e-06, 3.697506594848635e-06, 5.282152278355193e-06, 5.282152278355193e-06, 7.545931826221704e-06, 7.545931826221704e-06, 1.077990260888815e-05, 1.077990260888815e-05, 1.5399860869840216e-05, 1.5399860869840216e-05, 2.199980124262888e-05, 2.199980124262888e-05, 3.142828748946983e-05, 3.142828748946983e-05, 4.489755355638548e-05, 4.489755355638548e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:03:05,712] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=27.49882716308108, CurrSamplesPerSec=25.866349407837767, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [1720/2809]  eta: 0:10:50  lr: 0.000045  min_lr: 0.000000  loss: 4.4369 (4.4595)  class_acc: 0.2083 (0.2000)  loss_scale: 32768.0000 (60775.9768)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1376  max mem: 15572
Epoch: [9]  [1730/2809]  eta: 0:10:44  lr: 0.000045  min_lr: 0.000000  loss: 4.3847 (4.4591)  class_acc: 0.2083 (0.2002)  loss_scale: 32768.0000 (60614.1745)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1385  max mem: 15572
Epoch: [9]  [1740/2809]  eta: 0:10:37  lr: 0.000045  min_lr: 0.000000  loss: 4.4006 (4.4591)  class_acc: 0.2083 (0.2002)  loss_scale: 32768.0000 (60454.2309)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0957  max mem: 15572
Epoch: [9]  [1750/2809]  eta: 0:10:31  lr: 0.000045  min_lr: 0.000000  loss: 4.4062 (4.4589)  class_acc: 0.1250 (0.1999)  loss_scale: 32768.0000 (60296.1142)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.0474  max mem: 15572
Epoch: [9]  [1760/2809]  eta: 0:10:25  lr: 0.000045  min_lr: 0.000000  loss: 4.5167 (4.4588)  class_acc: 0.1667 (0.2002)  loss_scale: 32768.0000 (60139.7933)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0485  max mem: 15572
Epoch: [9]  [1770/2809]  eta: 0:10:19  lr: 0.000045  min_lr: 0.000000  loss: 4.4934 (4.4590)  class_acc: 0.2083 (0.2002)  loss_scale: 32768.0000 (59985.2377)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1107  max mem: 15572
[2025-01-15 19:03:39,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:03:39,432] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [1780/2809]  eta: 0:10:13  lr: 0.000045  min_lr: 0.000000  loss: 4.4179 (4.4589)  class_acc: 0.1667 (0.1998)  loss_scale: 32768.0000 (59924.4110)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1394  max mem: 15572
Epoch: [9]  [1790/2809]  eta: 0:10:07  lr: 0.000045  min_lr: 0.000000  loss: 4.4314 (4.4589)  class_acc: 0.1667 (0.2001)  loss_scale: 65536.0000 (59955.7432)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1264  max mem: 15572
Epoch: [9]  [1800/2809]  eta: 0:10:00  lr: 0.000045  min_lr: 0.000000  loss: 4.3833 (4.4586)  class_acc: 0.2083 (0.2000)  loss_scale: 65536.0000 (59986.7274)  weight_decay: 0.0500 (0.0500)  time: 0.5142  data: 0.0523  max mem: 15572
Epoch: [9]  [1810/2809]  eta: 0:09:54  lr: 0.000045  min_lr: 0.000000  loss: 4.4996 (4.4591)  class_acc: 0.1667 (0.1997)  loss_scale: 65536.0000 (60017.3694)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0605  max mem: 15572
Epoch: [9]  [1820/2809]  eta: 0:09:48  lr: 0.000045  min_lr: 0.000000  loss: 4.4707 (4.4587)  class_acc: 0.1667 (0.2002)  loss_scale: 65536.0000 (60047.6749)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1083  max mem: 15572
Epoch: [9]  [1830/2809]  eta: 0:09:42  lr: 0.000045  min_lr: 0.000000  loss: 4.3836 (4.4584)  class_acc: 0.1667 (0.2000)  loss_scale: 65536.0000 (60077.6494)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1074  max mem: 15572
Epoch: [9]  [1840/2809]  eta: 0:09:36  lr: 0.000045  min_lr: 0.000000  loss: 4.4810 (4.4586)  class_acc: 0.1667 (0.2000)  loss_scale: 65536.0000 (60107.2982)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1078  max mem: 15572
Epoch: [9]  [1850/2809]  eta: 0:09:30  lr: 0.000045  min_lr: 0.000000  loss: 4.4574 (4.4575)  class_acc: 0.1667 (0.1998)  loss_scale: 65536.0000 (60136.6267)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1324  max mem: 15572
Epoch: [9]  [1860/2809]  eta: 0:09:24  lr: 0.000045  min_lr: 0.000000  loss: 4.4511 (4.4577)  class_acc: 0.1667 (0.2000)  loss_scale: 65536.0000 (60165.6400)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1831  max mem: 15572
Epoch: [9]  [1870/2809]  eta: 0:09:19  lr: 0.000045  min_lr: 0.000000  loss: 4.4727 (4.4581)  class_acc: 0.1667 (0.1997)  loss_scale: 65536.0000 (60194.3431)  weight_decay: 0.0500 (0.0500)  time: 0.6545  data: 0.2148  max mem: 15572
Epoch: [9]  [1880/2809]  eta: 0:09:12  lr: 0.000045  min_lr: 0.000000  loss: 4.4693 (4.4584)  class_acc: 0.1667 (0.1997)  loss_scale: 65536.0000 (60222.7411)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1160  max mem: 15572
Epoch: [9]  [1890/2809]  eta: 0:09:06  lr: 0.000045  min_lr: 0.000000  loss: 4.4237 (4.4585)  class_acc: 0.2083 (0.1997)  loss_scale: 65536.0000 (60250.8387)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0913  max mem: 15572
Epoch: [9]  [1900/2809]  eta: 0:09:01  lr: 0.000045  min_lr: 0.000000  loss: 4.4237 (4.4588)  class_acc: 0.2083 (0.1997)  loss_scale: 65536.0000 (60278.6407)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.2005  max mem: 15572
[2025-01-15 19:04:53,320] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:04:53,321] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:04:53,758] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27186
[2025-01-15 19:04:53,759] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:04:53,759] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1910/2809]  eta: 0:08:55  lr: 0.000045  min_lr: 0.000000  loss: 4.3949 (4.4585)  class_acc: 0.2083 (0.1997)  loss_scale: 65536.0000 (60340.4458)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1418  max mem: 15572
Epoch: [9]  [1920/2809]  eta: 0:08:48  lr: 0.000045  min_lr: 0.000000  loss: 4.4370 (4.4592)  class_acc: 0.1667 (0.1995)  loss_scale: 65536.0000 (60367.4919)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0767  max mem: 15572
Epoch: [9]  [1930/2809]  eta: 0:08:43  lr: 0.000045  min_lr: 0.000000  loss: 4.6186 (4.4598)  class_acc: 0.1250 (0.1994)  loss_scale: 65536.0000 (60394.2579)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1495  max mem: 15572
Epoch: [9]  [1940/2809]  eta: 0:08:37  lr: 0.000045  min_lr: 0.000000  loss: 4.6186 (4.4599)  class_acc: 0.1667 (0.1996)  loss_scale: 65536.0000 (60420.7481)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1343  max mem: 15572
Epoch: [9]  [1950/2809]  eta: 0:08:31  lr: 0.000045  min_lr: 0.000000  loss: 4.4132 (4.4598)  class_acc: 0.2083 (0.1997)  loss_scale: 65536.0000 (60446.9667)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1069  max mem: 15572
Epoch: [9]  [1960/2809]  eta: 0:08:25  lr: 0.000045  min_lr: 0.000000  loss: 4.4551 (4.4598)  class_acc: 0.2083 (0.1996)  loss_scale: 65536.0000 (60472.9179)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1326  max mem: 15572
Epoch: [9]  [1970/2809]  eta: 0:08:19  lr: 0.000045  min_lr: 0.000000  loss: 4.4551 (4.4592)  class_acc: 0.2083 (0.2000)  loss_scale: 65536.0000 (60498.6058)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1363  max mem: 15572
Epoch: [9]  [1980/2809]  eta: 0:08:13  lr: 0.000045  min_lr: 0.000000  loss: 4.4283 (4.4599)  class_acc: 0.1667 (0.1997)  loss_scale: 65536.0000 (60524.0343)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1442  max mem: 15572
Epoch: [9]  [1990/2809]  eta: 0:08:07  lr: 0.000045  min_lr: 0.000000  loss: 4.5765 (4.4602)  class_acc: 0.1250 (0.1996)  loss_scale: 65536.0000 (60549.2074)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1542  max mem: 15572
Epoch: [9]  [2000/2809]  eta: 0:08:01  lr: 0.000045  min_lr: 0.000000  loss: 4.5434 (4.4604)  class_acc: 0.1250 (0.1994)  loss_scale: 65536.0000 (60574.1289)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1855  max mem: 15572
Epoch: [9]  [2010/2809]  eta: 0:07:55  lr: 0.000045  min_lr: 0.000000  loss: 4.5025 (4.4604)  class_acc: 0.1250 (0.1995)  loss_scale: 65536.0000 (60598.8026)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1738  max mem: 15572
Epoch: [9]  [2020/2809]  eta: 0:07:49  lr: 0.000045  min_lr: 0.000000  loss: 4.4780 (4.4606)  class_acc: 0.1667 (0.1993)  loss_scale: 65536.0000 (60623.2321)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0917  max mem: 15572
Epoch: [9]  [2030/2809]  eta: 0:07:43  lr: 0.000045  min_lr: 0.000000  loss: 4.4226 (4.4604)  class_acc: 0.1667 (0.1995)  loss_scale: 65536.0000 (60647.4210)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.0869  max mem: 15572
[2025-01-15 19:06:11,033] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:06:11,034] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [2040/2809]  eta: 0:07:38  lr: 0.000045  min_lr: 0.000000  loss: 4.5135 (4.4613)  class_acc: 0.1667 (0.1994)  loss_scale: 65536.0000 (60896.1411)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.1349  max mem: 15572
[2025-01-15 19:06:17,067] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27325
[2025-01-15 19:06:17,067] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:06:17,068] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [2050/2809]  eta: 0:07:32  lr: 0.000045  min_lr: 0.000000  loss: 4.6109 (4.4619)  class_acc: 0.2083 (0.1994)  loss_scale: 65536.0000 (61014.6231)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1290  max mem: 15572
Epoch: [9]  [2060/2809]  eta: 0:07:25  lr: 0.000045  min_lr: 0.000000  loss: 4.5664 (4.4621)  class_acc: 0.1667 (0.1992)  loss_scale: 65536.0000 (61036.5609)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0693  max mem: 15572
Epoch: [9]  [2070/2809]  eta: 0:07:19  lr: 0.000045  min_lr: 0.000000  loss: 4.5429 (4.4622)  class_acc: 0.1667 (0.1991)  loss_scale: 65536.0000 (61058.2868)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0181  max mem: 15572
Epoch: [9]  [2080/2809]  eta: 0:07:13  lr: 0.000045  min_lr: 0.000000  loss: 4.4012 (4.4618)  class_acc: 0.1250 (0.1987)  loss_scale: 65536.0000 (61079.8039)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0577  max mem: 15572
Epoch: [9]  [2090/2809]  eta: 0:07:07  lr: 0.000045  min_lr: 0.000000  loss: 4.3256 (4.4611)  class_acc: 0.1250 (0.1987)  loss_scale: 65536.0000 (61101.1153)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1431  max mem: 15572
Epoch: [9]  [2100/2809]  eta: 0:07:01  lr: 0.000045  min_lr: 0.000000  loss: 4.3794 (4.4613)  class_acc: 0.2083 (0.1988)  loss_scale: 65536.0000 (61122.2237)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1494  max mem: 15572
Epoch: [9]  [2110/2809]  eta: 0:06:55  lr: 0.000045  min_lr: 0.000000  loss: 4.5107 (4.4619)  class_acc: 0.1667 (0.1986)  loss_scale: 65536.0000 (61143.1322)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1072  max mem: 15572
Epoch: [9]  [2120/2809]  eta: 0:06:49  lr: 0.000045  min_lr: 0.000000  loss: 4.5107 (4.4617)  class_acc: 0.1250 (0.1986)  loss_scale: 65536.0000 (61163.8435)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1479  max mem: 15572
Epoch: [9]  [2130/2809]  eta: 0:06:43  lr: 0.000045  min_lr: 0.000000  loss: 4.5356 (4.4623)  class_acc: 0.1667 (0.1985)  loss_scale: 65536.0000 (61184.3604)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.2074  max mem: 15572
[2025-01-15 19:07:12,125] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27418
[2025-01-15 19:07:12,125] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:07:12,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [2140/2809]  eta: 0:06:38  lr: 0.000045  min_lr: 0.000000  loss: 4.5099 (4.4620)  class_acc: 0.1667 (0.1988)  loss_scale: 65536.0000 (61143.4657)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.1955  max mem: 15572
Epoch: [9]  [2150/2809]  eta: 0:06:32  lr: 0.000045  min_lr: 0.000000  loss: 4.3676 (4.4620)  class_acc: 0.1667 (0.1988)  loss_scale: 32768.0000 (61011.5481)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1206  max mem: 15572
Epoch: [9]  [2160/2809]  eta: 0:06:26  lr: 0.000045  min_lr: 0.000000  loss: 4.4584 (4.4624)  class_acc: 0.2083 (0.1989)  loss_scale: 32768.0000 (60880.8515)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1111  max mem: 15572
Epoch: [9]  [2170/2809]  eta: 0:06:20  lr: 0.000045  min_lr: 0.000000  loss: 4.4728 (4.4624)  class_acc: 0.2083 (0.1987)  loss_scale: 32768.0000 (60751.3588)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1525  max mem: 15572
Epoch: [9]  [2180/2809]  eta: 0:06:14  lr: 0.000045  min_lr: 0.000000  loss: 4.4032 (4.4619)  class_acc: 0.2083 (0.1988)  loss_scale: 32768.0000 (60623.0536)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1046  max mem: 15572
Epoch: [9]  [2190/2809]  eta: 0:06:08  lr: 0.000045  min_lr: 0.000000  loss: 4.4285 (4.4624)  class_acc: 0.2083 (0.1987)  loss_scale: 32768.0000 (60495.9197)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1368  max mem: 15572
Epoch: [9]  [2200/2809]  eta: 0:06:02  lr: 0.000045  min_lr: 0.000000  loss: 4.5475 (4.4631)  class_acc: 0.1667 (0.1985)  loss_scale: 32768.0000 (60369.9409)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.1953  max mem: 15572
Epoch: [9]  [2210/2809]  eta: 0:05:56  lr: 0.000045  min_lr: 0.000000  loss: 4.4356 (4.4627)  class_acc: 0.1667 (0.1985)  loss_scale: 32768.0000 (60245.1018)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0995  max mem: 15572
Epoch: [9]  [2220/2809]  eta: 0:05:50  lr: 0.000045  min_lr: 0.000000  loss: 4.4074 (4.4626)  class_acc: 0.2083 (0.1987)  loss_scale: 32768.0000 (60121.3868)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0238  max mem: 15572
Epoch: [9]  [2230/2809]  eta: 0:05:44  lr: 0.000045  min_lr: 0.000000  loss: 4.4284 (4.4626)  class_acc: 0.2083 (0.1988)  loss_scale: 32768.0000 (59998.7808)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0449  max mem: 15572
Epoch: [9]  [2240/2809]  eta: 0:05:38  lr: 0.000045  min_lr: 0.000000  loss: 4.4386 (4.4623)  class_acc: 0.2500 (0.1991)  loss_scale: 32768.0000 (59877.2691)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1519  max mem: 15572
Epoch: [9]  [2250/2809]  eta: 0:05:32  lr: 0.000045  min_lr: 0.000000  loss: 4.3990 (4.4622)  class_acc: 0.2500 (0.1990)  loss_scale: 32768.0000 (59756.8370)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1795  max mem: 15572
Epoch: [9]  [2260/2809]  eta: 0:05:26  lr: 0.000045  min_lr: 0.000000  loss: 4.5354 (4.4633)  class_acc: 0.1667 (0.1989)  loss_scale: 32768.0000 (59637.4701)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.1008  max mem: 15572
[2025-01-15 19:08:27,328] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:08:27,328] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [2270/2809]  eta: 0:05:20  lr: 0.000045  min_lr: 0.000000  loss: 4.5533 (4.4636)  class_acc: 0.1250 (0.1987)  loss_scale: 32768.0000 (59591.2990)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1208  max mem: 15572
Epoch: [9]  [2280/2809]  eta: 0:05:14  lr: 0.000045  min_lr: 0.000000  loss: 4.5388 (4.4636)  class_acc: 0.1667 (0.1987)  loss_scale: 65536.0000 (59617.3608)  weight_decay: 0.0500 (0.0500)  time: 0.6584  data: 0.1931  max mem: 15572
Epoch: [9]  [2290/2809]  eta: 0:05:08  lr: 0.000045  min_lr: 0.000000  loss: 4.3425 (4.4630)  class_acc: 0.2083 (0.1988)  loss_scale: 65536.0000 (59643.1951)  weight_decay: 0.0500 (0.0500)  time: 0.6626  data: 0.1944  max mem: 15572
Epoch: [9]  [2300/2809]  eta: 0:05:02  lr: 0.000045  min_lr: 0.000000  loss: 4.4539 (4.4629)  class_acc: 0.1667 (0.1987)  loss_scale: 65536.0000 (59668.8049)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1311  max mem: 15572
[2025-01-15 19:08:49,331] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27584
[2025-01-15 19:08:49,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:08:49,332] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [2310/2809]  eta: 0:04:56  lr: 0.000045  min_lr: 0.000000  loss: 4.5334 (4.4630)  class_acc: 0.1667 (0.1986)  loss_scale: 65536.0000 (59580.7598)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0992  max mem: 15572
Epoch: [9]  [2320/2809]  eta: 0:04:50  lr: 0.000045  min_lr: 0.000000  loss: 4.5334 (4.4630)  class_acc: 0.2083 (0.1988)  loss_scale: 32768.0000 (59465.2374)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1092  max mem: 15572
Epoch: [9]  [2330/2809]  eta: 0:04:44  lr: 0.000045  min_lr: 0.000000  loss: 4.4242 (4.4631)  class_acc: 0.1667 (0.1988)  loss_scale: 32768.0000 (59350.7061)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0630  max mem: 15572
Epoch: [9]  [2340/2809]  eta: 0:04:38  lr: 0.000045  min_lr: 0.000000  loss: 4.3529 (4.4626)  class_acc: 0.1667 (0.1988)  loss_scale: 32768.0000 (59237.1534)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1041  max mem: 15572
Epoch: [9]  [2350/2809]  eta: 0:04:32  lr: 0.000045  min_lr: 0.000000  loss: 4.3474 (4.4627)  class_acc: 0.1667 (0.1990)  loss_scale: 32768.0000 (59124.5666)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1571  max mem: 15572
Epoch: [9]  [2360/2809]  eta: 0:04:26  lr: 0.000045  min_lr: 0.000000  loss: 4.5008 (4.4629)  class_acc: 0.2083 (0.1990)  loss_scale: 32768.0000 (59012.9335)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1404  max mem: 15572
Epoch: [9]  [2370/2809]  eta: 0:04:20  lr: 0.000045  min_lr: 0.000000  loss: 4.4429 (4.4627)  class_acc: 0.2083 (0.1991)  loss_scale: 32768.0000 (58902.2421)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1429  max mem: 15572
Epoch: [9]  [2380/2809]  eta: 0:04:14  lr: 0.000045  min_lr: 0.000000  loss: 4.4952 (4.4636)  class_acc: 0.2083 (0.1990)  loss_scale: 32768.0000 (58792.4805)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1655  max mem: 15572
Epoch: [9]  [2390/2809]  eta: 0:04:09  lr: 0.000045  min_lr: 0.000000  loss: 4.5122 (4.4632)  class_acc: 0.1667 (0.1990)  loss_scale: 32768.0000 (58683.6370)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1951  max mem: 15572
Epoch: [9]  [2400/2809]  eta: 0:04:03  lr: 0.000045  min_lr: 0.000000  loss: 4.3685 (4.4627)  class_acc: 0.1667 (0.1990)  loss_scale: 32768.0000 (58575.7001)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1686  max mem: 15572
Epoch: [9]  [2410/2809]  eta: 0:03:57  lr: 0.000045  min_lr: 0.000000  loss: 4.3254 (4.4626)  class_acc: 0.1667 (0.1989)  loss_scale: 32768.0000 (58468.6586)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1221  max mem: 15572
Epoch: [9]  [2420/2809]  eta: 0:03:51  lr: 0.000045  min_lr: 0.000000  loss: 4.4610 (4.4630)  class_acc: 0.1250 (0.1985)  loss_scale: 32768.0000 (58362.5014)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0887  max mem: 15572
Epoch: [9]  [2430/2809]  eta: 0:03:45  lr: 0.000045  min_lr: 0.000000  loss: 4.4624 (4.4628)  class_acc: 0.1250 (0.1983)  loss_scale: 32768.0000 (58257.2176)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0733  max mem: 15572
[2025-01-15 19:10:05,432] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:10:05,432] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [2440/2809]  eta: 0:03:39  lr: 0.000045  min_lr: 0.000000  loss: 4.4599 (4.4627)  class_acc: 0.1667 (0.1984)  loss_scale: 32768.0000 (58273.6125)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0923  max mem: 15572
Epoch: [9]  [2450/2809]  eta: 0:03:33  lr: 0.000045  min_lr: 0.000000  loss: 4.3982 (4.4628)  class_acc: 0.2083 (0.1986)  loss_scale: 65536.0000 (58303.2428)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1239  max mem: 15572
Epoch: [9]  [2460/2809]  eta: 0:03:27  lr: 0.000045  min_lr: 0.000000  loss: 4.3965 (4.4621)  class_acc: 0.2083 (0.1990)  loss_scale: 65536.0000 (58332.6323)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1312  max mem: 15572
[2025-01-15 19:10:26,598] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27748
[2025-01-15 19:10:26,598] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:10:26,598] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [2470/2809]  eta: 0:03:21  lr: 0.000045  min_lr: 0.000000  loss: 4.4596 (4.4629)  class_acc: 0.2083 (0.1990)  loss_scale: 65536.0000 (58308.7398)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1049  max mem: 15572
Epoch: [9]  [2480/2809]  eta: 0:03:15  lr: 0.000045  min_lr: 0.000000  loss: 4.4586 (4.4624)  class_acc: 0.1667 (0.1989)  loss_scale: 32768.0000 (58205.7944)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.1801  max mem: 15572
Epoch: [9]  [2490/2809]  eta: 0:03:09  lr: 0.000045  min_lr: 0.000000  loss: 4.3479 (4.4623)  class_acc: 0.1667 (0.1990)  loss_scale: 32768.0000 (58103.6756)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1765  max mem: 15572
Epoch: [9]  [2500/2809]  eta: 0:03:03  lr: 0.000045  min_lr: 0.000000  loss: 4.3370 (4.4617)  class_acc: 0.2500 (0.1994)  loss_scale: 32768.0000 (58002.3735)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1330  max mem: 15572
Epoch: [9]  [2510/2809]  eta: 0:02:57  lr: 0.000045  min_lr: 0.000000  loss: 4.3471 (4.4618)  class_acc: 0.2500 (0.1996)  loss_scale: 32768.0000 (57901.8781)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.1902  max mem: 15572
Epoch: [9]  [2520/2809]  eta: 0:02:51  lr: 0.000045  min_lr: 0.000000  loss: 4.4530 (4.4621)  class_acc: 0.1667 (0.1997)  loss_scale: 32768.0000 (57802.1801)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1505  max mem: 15572
Epoch: [9]  [2530/2809]  eta: 0:02:45  lr: 0.000045  min_lr: 0.000000  loss: 4.4230 (4.4618)  class_acc: 0.1667 (0.1995)  loss_scale: 32768.0000 (57703.2699)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0897  max mem: 15572
Epoch: [9]  [2540/2809]  eta: 0:02:39  lr: 0.000045  min_lr: 0.000000  loss: 4.6059 (4.4629)  class_acc: 0.1250 (0.1994)  loss_scale: 32768.0000 (57605.1381)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1182  max mem: 15572
Epoch: [9]  [2550/2809]  eta: 0:02:33  lr: 0.000045  min_lr: 0.000000  loss: 4.6147 (4.4629)  class_acc: 0.1667 (0.1993)  loss_scale: 32768.0000 (57507.7758)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1095  max mem: 15572
Epoch: [9]  [2560/2809]  eta: 0:02:28  lr: 0.000045  min_lr: 0.000000  loss: 4.4493 (4.4633)  class_acc: 0.1667 (0.1993)  loss_scale: 32768.0000 (57411.1738)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1533  max mem: 15572
Epoch: [9]  [2570/2809]  eta: 0:02:22  lr: 0.000045  min_lr: 0.000000  loss: 4.5546 (4.4637)  class_acc: 0.1250 (0.1990)  loss_scale: 32768.0000 (57315.3232)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1426  max mem: 15572
Epoch: [9]  [2580/2809]  eta: 0:02:16  lr: 0.000045  min_lr: 0.000000  loss: 4.5563 (4.4638)  class_acc: 0.1667 (0.1991)  loss_scale: 32768.0000 (57220.2154)  weight_decay: 0.0500 (0.0500)  time: 0.5174  data: 0.0603  max mem: 15572
Epoch: [9]  [2590/2809]  eta: 0:02:10  lr: 0.000045  min_lr: 0.000000  loss: 4.5904 (4.4644)  class_acc: 0.1667 (0.1992)  loss_scale: 32768.0000 (57125.8418)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0633  max mem: 15572
[2025-01-15 19:11:41,689] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:11:41,689] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [2600/2809]  eta: 0:02:04  lr: 0.000045  min_lr: 0.000000  loss: 4.5083 (4.4643)  class_acc: 0.2083 (0.1993)  loss_scale: 32768.0000 (57095.1849)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0561  max mem: 15572
Epoch: [9]  [2610/2809]  eta: 0:01:58  lr: 0.000045  min_lr: 0.000000  loss: 4.3769 (4.4643)  class_acc: 0.2083 (0.1992)  loss_scale: 65536.0000 (57127.5128)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0876  max mem: 15572
Epoch: [9]  [2620/2809]  eta: 0:01:52  lr: 0.000045  min_lr: 0.000000  loss: 4.3619 (4.4636)  class_acc: 0.2083 (0.1994)  loss_scale: 65536.0000 (57159.5940)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0846  max mem: 15572
Epoch: [9]  [2630/2809]  eta: 0:01:46  lr: 0.000045  min_lr: 0.000000  loss: 4.3399 (4.4631)  class_acc: 0.2500 (0.1996)  loss_scale: 65536.0000 (57191.4314)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1325  max mem: 15572
Epoch: [9]  [2640/2809]  eta: 0:01:40  lr: 0.000045  min_lr: 0.000000  loss: 4.3413 (4.4627)  class_acc: 0.2500 (0.1996)  loss_scale: 65536.0000 (57223.0276)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0998  max mem: 15572
Epoch: [9]  [2650/2809]  eta: 0:01:34  lr: 0.000045  min_lr: 0.000000  loss: 4.3946 (4.4627)  class_acc: 0.1667 (0.1995)  loss_scale: 65536.0000 (57254.3855)  weight_decay: 0.0500 (0.0500)  time: 0.4346  data: 0.0006  max mem: 15572
Epoch: [9]  [2660/2809]  eta: 0:01:28  lr: 0.000045  min_lr: 0.000000  loss: 4.3946 (4.4626)  class_acc: 0.1667 (0.1996)  loss_scale: 65536.0000 (57285.5077)  weight_decay: 0.0500 (0.0500)  time: 0.4884  data: 0.0010  max mem: 15572
Epoch: [9]  [2670/2809]  eta: 0:01:22  lr: 0.000045  min_lr: 0.000000  loss: 4.4691 (4.4626)  class_acc: 0.1667 (0.1996)  loss_scale: 65536.0000 (57316.3969)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0377  max mem: 15572
Epoch: [9]  [2680/2809]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000000  loss: 4.4440 (4.4623)  class_acc: 0.2083 (0.1998)  loss_scale: 65536.0000 (57347.0556)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1336  max mem: 15572
Epoch: [9]  [2690/2809]  eta: 0:01:10  lr: 0.000045  min_lr: 0.000000  loss: 4.4737 (4.4625)  class_acc: 0.1667 (0.1996)  loss_scale: 65536.0000 (57377.4864)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.1884  max mem: 15572
Epoch: [9]  [2700/2809]  eta: 0:01:04  lr: 0.000045  min_lr: 0.000000  loss: 4.3679 (4.4619)  class_acc: 0.1667 (0.1999)  loss_scale: 65536.0000 (57407.6920)  weight_decay: 0.0500 (0.0500)  time: 0.6724  data: 0.1722  max mem: 15572
Epoch: [9]  [2710/2809]  eta: 0:00:58  lr: 0.000045  min_lr: 0.000000  loss: 4.3476 (4.4613)  class_acc: 0.2083 (0.1998)  loss_scale: 65536.0000 (57437.6747)  weight_decay: 0.0500 (0.0500)  time: 0.6912  data: 0.1949  max mem: 15572
[2025-01-15 19:12:54,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=179, lr=[4.319867297995155e-07, 4.319867297995155e-07, 6.171238997135936e-07, 6.171238997135936e-07, 8.816055710194195e-07, 8.816055710194195e-07, 1.2594365300277421e-06, 1.2594365300277421e-06, 1.7991950428967748e-06, 1.7991950428967748e-06, 2.5702786327096783e-06, 2.5702786327096783e-06, 3.6718266181566832e-06, 3.6718266181566832e-06, 5.2454665973666914e-06, 5.2454665973666914e-06, 7.493523710523844e-06, 7.493523710523844e-06, 1.0705033872176922e-05, 1.0705033872176922e-05, 1.5292905531681318e-05, 1.5292905531681318e-05, 2.1847007902401884e-05, 2.1847007902401884e-05, 3.121001128914555e-05, 3.121001128914555e-05, 4.4585730413065074e-05, 4.4585730413065074e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:12:54,977] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=27.4903185133527, CurrSamplesPerSec=22.662430001139164, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [2720/2809]  eta: 0:00:52  lr: 0.000045  min_lr: 0.000000  loss: 4.3914 (4.4615)  class_acc: 0.1667 (0.1999)  loss_scale: 65536.0000 (57467.4370)  weight_decay: 0.0500 (0.0500)  time: 0.7116  data: 0.2320  max mem: 15572
[2025-01-15 19:12:58,591] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:12:58,592] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:13:00,381] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28007
[2025-01-15 19:13:00,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:13:00,382] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [2730/2809]  eta: 0:00:46  lr: 0.000045  min_lr: 0.000000  loss: 4.4504 (4.4609)  class_acc: 0.2500 (0.2000)  loss_scale: 65536.0000 (57544.9755)  weight_decay: 0.0500 (0.0500)  time: 0.6836  data: 0.1990  max mem: 15572
Epoch: [9]  [2740/2809]  eta: 0:00:40  lr: 0.000045  min_lr: 0.000000  loss: 4.4207 (4.4606)  class_acc: 0.2083 (0.2000)  loss_scale: 65536.0000 (57574.1291)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1040  max mem: 15572
Epoch: [9]  [2750/2809]  eta: 0:00:35  lr: 0.000045  min_lr: 0.000000  loss: 4.4207 (4.4605)  class_acc: 0.2083 (0.2001)  loss_scale: 65536.0000 (57603.0709)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1016  max mem: 15572
Epoch: [9]  [2760/2809]  eta: 0:00:29  lr: 0.000045  min_lr: 0.000000  loss: 4.5108 (4.4608)  class_acc: 0.2083 (0.2001)  loss_scale: 65536.0000 (57631.8030)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.1530  max mem: 15572
Epoch: [9]  [2770/2809]  eta: 0:00:23  lr: 0.000045  min_lr: 0.000000  loss: 4.5583 (4.4609)  class_acc: 0.2083 (0.2003)  loss_scale: 65536.0000 (57660.3277)  weight_decay: 0.0500 (0.0500)  time: 0.7049  data: 0.2203  max mem: 15572
Epoch: [9]  [2780/2809]  eta: 0:00:17  lr: 0.000045  min_lr: 0.000000  loss: 4.5377 (4.4610)  class_acc: 0.1667 (0.2002)  loss_scale: 65536.0000 (57688.6472)  weight_decay: 0.0500 (0.0500)  time: 0.7284  data: 0.2331  max mem: 15572
Epoch: [9]  [2790/2809]  eta: 0:00:11  lr: 0.000045  min_lr: 0.000000  loss: 4.4138 (4.4607)  class_acc: 0.1667 (0.2004)  loss_scale: 65536.0000 (57716.7639)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1073  max mem: 15572
Epoch: [9]  [2800/2809]  eta: 0:00:05  lr: 0.000045  min_lr: 0.000000  loss: 4.4138 (4.4609)  class_acc: 0.1667 (0.2002)  loss_scale: 65536.0000 (57744.6798)  weight_decay: 0.0500 (0.0500)  time: 0.4234  data: 0.0208  max mem: 15572
Epoch: [9]  [2808/2809]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 4.5297 (4.4610)  class_acc: 0.1667 (0.2001)  loss_scale: 65536.0000 (57766.8693)  weight_decay: 0.0500 (0.0500)  time: 0.3994  data: 0.0004  max mem: 15572
Epoch: [9] Total time: 0:27:48 (0.5941 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 4.5297 (4.4610)  class_acc: 0.1667 (0.2001)  loss_scale: 65536.0000 (57766.8693)  weight_decay: 0.0500 (0.0500)
[2025-01-15 19:13:47,454] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-9 is about to be saved!
[2025-01-15 19:13:47,458] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-9/mp_rank_00_model_states.pt
[2025-01-15 19:13:47,458] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-9/mp_rank_00_model_states.pt...
[2025-01-15 19:13:48,030] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-9/mp_rank_00_model_states.pt.
[2025-01-15 19:13:48,031] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
Val:  [  0/272]  eta: 0:20:19  loss: 1.2978 (1.2978)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.4832  data: 4.1797  max mem: 15572
Val:  [ 10/272]  eta: 0:03:28  loss: 3.7293 (3.4272)  acc1: 16.6667 (24.7475)  acc5: 44.4444 (42.9293)  time: 0.7961  data: 0.5959  max mem: 15572
Val:  [ 20/272]  eta: 0:02:19  loss: 3.3687 (3.3528)  acc1: 27.7778 (30.6878)  acc5: 55.5556 (52.9101)  time: 0.3578  data: 0.1722  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 3.2464 (3.4138)  acc1: 22.2222 (25.8065)  acc5: 55.5556 (53.0466)  time: 0.2376  data: 0.0539  max mem: 15572
Val:  [ 40/272]  eta: 0:01:36  loss: 3.1285 (3.3388)  acc1: 22.2222 (25.7453)  acc5: 55.5556 (56.6396)  time: 0.2666  data: 0.0777  max mem: 15572
Val:  [ 50/272]  eta: 0:01:29  loss: 3.0510 (3.2690)  acc1: 27.7778 (28.2135)  acc5: 72.2222 (58.7146)  time: 0.3535  data: 0.1713  max mem: 15572
Val:  [ 60/272]  eta: 0:01:22  loss: 2.3271 (3.1658)  acc1: 55.5556 (32.9690)  acc5: 77.7778 (61.0200)  time: 0.3325  data: 0.1452  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 2.4570 (3.0950)  acc1: 55.5556 (34.4288)  acc5: 77.7778 (63.9280)  time: 0.3317  data: 0.1278  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 2.7354 (3.0952)  acc1: 33.3333 (34.7737)  acc5: 77.7778 (63.9232)  time: 0.3356  data: 0.1360  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 3.5109 (3.1445)  acc1: 22.2222 (33.2112)  acc5: 50.0000 (62.3932)  time: 0.3475  data: 0.1496  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 3.5336 (3.1962)  acc1: 27.7778 (32.6733)  acc5: 50.0000 (61.5512)  time: 0.3504  data: 0.1639  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 3.6240 (3.2485)  acc1: 16.6667 (30.8308)  acc5: 50.0000 (59.9099)  time: 0.3246  data: 0.1390  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 3.6599 (3.2815)  acc1: 5.5556 (29.7521)  acc5: 50.0000 (59.2746)  time: 0.3249  data: 0.1231  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 3.3822 (3.2509)  acc1: 22.2222 (30.8312)  acc5: 61.1111 (60.1357)  time: 0.3343  data: 0.1320  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 2.8900 (3.2518)  acc1: 33.3333 (31.2845)  acc5: 66.6667 (60.0867)  time: 0.3347  data: 0.1340  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 3.2789 (3.2491)  acc1: 27.7778 (30.7579)  acc5: 61.1111 (60.0074)  time: 0.3147  data: 0.1268  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.0795 (3.2308)  acc1: 33.3333 (31.5045)  acc5: 72.2222 (61.1111)  time: 0.3249  data: 0.1281  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.0618 (3.2492)  acc1: 33.3333 (30.8317)  acc5: 66.6667 (60.4289)  time: 0.2906  data: 0.0854  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 3.1618 (3.2357)  acc1: 16.6667 (30.5095)  acc5: 66.6667 (61.0804)  time: 0.3399  data: 0.1407  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.2253 (3.2676)  acc1: 16.6667 (29.7266)  acc5: 55.5556 (59.5986)  time: 0.3601  data: 0.1611  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.3739 (3.2765)  acc1: 16.6667 (29.2703)  acc5: 44.4444 (59.4251)  time: 0.3243  data: 0.1309  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.2487 (3.2898)  acc1: 22.2222 (28.8836)  acc5: 66.6667 (59.3734)  time: 0.3120  data: 0.1155  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 3.3219 (3.2883)  acc1: 22.2222 (28.9090)  acc5: 61.1111 (59.5023)  time: 0.3296  data: 0.1354  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.9693 (3.2718)  acc1: 38.8889 (29.9423)  acc5: 66.6667 (60.1732)  time: 0.3925  data: 0.1817  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 2.6355 (3.2500)  acc1: 61.1111 (30.7515)  acc5: 83.3333 (61.1342)  time: 0.3475  data: 0.1144  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.9993 (3.2685)  acc1: 27.7778 (30.4560)  acc5: 66.6667 (60.5578)  time: 0.2889  data: 0.0650  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 2.6690 (3.2191)  acc1: 72.2222 (32.4819)  acc5: 77.7778 (61.6858)  time: 0.2700  data: 0.0652  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.8051 (3.2245)  acc1: 61.1111 (32.1238)  acc5: 72.2222 (61.4596)  time: 0.2248  data: 0.0458  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.8051 (3.2276)  acc1: 40.0000 (32.1319)  acc5: 72.2222 (61.4376)  time: 0.2163  data: 0.0457  max mem: 15572
Val: Total time: 0:01:31 (0.3361 s / it)
* Acc@1 32.132 Acc@5 61.438 loss 3.228
Accuracy of the network on the 4883 val videos: 32.1%
[2025-01-15 19:15:19,446] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 19:15:19,451] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 19:15:19,451] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 19:15:22,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 19:15:22,629] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 32.13%
Epoch: [10]  [   0/2809]  eta: 6:49:02  lr: 0.000045  min_lr: 0.000000  loss: 4.6529 (4.6529)  class_acc: 0.1667 (0.1667)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.7372  data: 8.2656  max mem: 15572
Epoch: [10]  [  10/2809]  eta: 1:04:17  lr: 0.000045  min_lr: 0.000000  loss: 4.3725 (4.4200)  class_acc: 0.2917 (0.2386)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3783  data: 0.9048  max mem: 15572
Epoch: [10]  [  20/2809]  eta: 0:46:31  lr: 0.000045  min_lr: 0.000000  loss: 4.3546 (4.4329)  class_acc: 0.2500 (0.2381)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1549  max mem: 15572
Epoch: [10]  [  30/2809]  eta: 0:39:44  lr: 0.000045  min_lr: 0.000000  loss: 4.3444 (4.3983)  class_acc: 0.2083 (0.2164)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1319  max mem: 15572
Epoch: [10]  [  40/2809]  eta: 0:37:09  lr: 0.000045  min_lr: 0.000000  loss: 4.4391 (4.4057)  class_acc: 0.2083 (0.2165)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1574  max mem: 15572
[2025-01-15 19:15:58,289] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:15:58,290] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:15:58,710] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28137
[2025-01-15 19:15:58,710] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:15:58,710] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [  50/2809]  eta: 0:34:25  lr: 0.000045  min_lr: 0.000000  loss: 4.4489 (4.3932)  class_acc: 0.2500 (0.2271)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1295  max mem: 15572
Epoch: [10]  [  60/2809]  eta: 0:33:02  lr: 0.000045  min_lr: 0.000000  loss: 4.4489 (4.4217)  class_acc: 0.2083 (0.2206)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1118  max mem: 15572
Epoch: [10]  [  70/2809]  eta: 0:32:14  lr: 0.000045  min_lr: 0.000000  loss: 4.4681 (4.4146)  class_acc: 0.2083 (0.2242)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1658  max mem: 15572
Epoch: [10]  [  80/2809]  eta: 0:31:50  lr: 0.000045  min_lr: 0.000000  loss: 4.4378 (4.4151)  class_acc: 0.2083 (0.2238)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.1918  max mem: 15572
Epoch: [10]  [  90/2809]  eta: 0:30:57  lr: 0.000045  min_lr: 0.000000  loss: 4.3616 (4.4130)  class_acc: 0.1667 (0.2216)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1386  max mem: 15572
Epoch: [10]  [ 100/2809]  eta: 0:30:33  lr: 0.000045  min_lr: 0.000000  loss: 4.3394 (4.4026)  class_acc: 0.2500 (0.2232)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1034  max mem: 15572
Epoch: [10]  [ 110/2809]  eta: 0:30:11  lr: 0.000045  min_lr: 0.000000  loss: 4.3394 (4.3993)  class_acc: 0.2083 (0.2203)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1337  max mem: 15572
[2025-01-15 19:16:39,715] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28206
[2025-01-15 19:16:39,716] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:16:39,716] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 120/2809]  eta: 0:29:58  lr: 0.000045  min_lr: 0.000000  loss: 4.4207 (4.4038)  class_acc: 0.2083 (0.2194)  loss_scale: 65536.0000 (64723.5702)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1629  max mem: 15572
Epoch: [10]  [ 130/2809]  eta: 0:29:25  lr: 0.000045  min_lr: 0.000000  loss: 4.4207 (4.3956)  class_acc: 0.2500 (0.2239)  loss_scale: 32768.0000 (62284.2137)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1490  max mem: 15572
Epoch: [10]  [ 140/2809]  eta: 0:29:06  lr: 0.000045  min_lr: 0.000000  loss: 4.3404 (4.3976)  class_acc: 0.2083 (0.2196)  loss_scale: 32768.0000 (60190.8652)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1217  max mem: 15572
Epoch: [10]  [ 150/2809]  eta: 0:28:35  lr: 0.000045  min_lr: 0.000000  loss: 4.3404 (4.3884)  class_acc: 0.1667 (0.2177)  loss_scale: 32768.0000 (58374.7815)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0995  max mem: 15572
Epoch: [10]  [ 160/2809]  eta: 0:28:20  lr: 0.000045  min_lr: 0.000000  loss: 4.3412 (4.3895)  class_acc: 0.2083 (0.2208)  loss_scale: 32768.0000 (56784.2981)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1023  max mem: 15572
Epoch: [10]  [ 170/2809]  eta: 0:28:02  lr: 0.000045  min_lr: 0.000000  loss: 4.4142 (4.3923)  class_acc: 0.2083 (0.2171)  loss_scale: 32768.0000 (55379.8363)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0941  max mem: 15572
Epoch: [10]  [ 180/2809]  eta: 0:27:50  lr: 0.000044  min_lr: 0.000000  loss: 4.4582 (4.4006)  class_acc: 0.1667 (0.2180)  loss_scale: 32768.0000 (54130.5635)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0883  max mem: 15572
Epoch: [10]  [ 190/2809]  eta: 0:27:31  lr: 0.000044  min_lr: 0.000000  loss: 4.4964 (4.4063)  class_acc: 0.2083 (0.2188)  loss_scale: 32768.0000 (53012.1047)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1104  max mem: 15572
Epoch: [10]  [ 200/2809]  eta: 0:27:13  lr: 0.000044  min_lr: 0.000000  loss: 4.4659 (4.4103)  class_acc: 0.2083 (0.2204)  loss_scale: 32768.0000 (52004.9353)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0866  max mem: 15572
Epoch: [10]  [ 210/2809]  eta: 0:26:58  lr: 0.000044  min_lr: 0.000000  loss: 4.4410 (4.4118)  class_acc: 0.2083 (0.2200)  loss_scale: 32768.0000 (51093.2322)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0935  max mem: 15572
Epoch: [10]  [ 220/2809]  eta: 0:26:47  lr: 0.000044  min_lr: 0.000000  loss: 4.4184 (4.4152)  class_acc: 0.1667 (0.2179)  loss_scale: 32768.0000 (50264.0362)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0910  max mem: 15572
Epoch: [10]  [ 230/2809]  eta: 0:26:40  lr: 0.000044  min_lr: 0.000000  loss: 4.4324 (4.4144)  class_acc: 0.2083 (0.2193)  loss_scale: 32768.0000 (49506.6320)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1015  max mem: 15572
Epoch: [10]  [ 240/2809]  eta: 0:26:43  lr: 0.000044  min_lr: 0.000000  loss: 4.4084 (4.4140)  class_acc: 0.2083 (0.2196)  loss_scale: 32768.0000 (48812.0830)  weight_decay: 0.0500 (0.0500)  time: 0.6605  data: 0.1762  max mem: 15572
[2025-01-15 19:17:55,315] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:17:55,315] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 250/2809]  eta: 0:26:19  lr: 0.000044  min_lr: 0.000000  loss: 4.4434 (4.4173)  class_acc: 0.1667 (0.2191)  loss_scale: 32768.0000 (48956.1753)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1168  max mem: 15572
Epoch: [10]  [ 260/2809]  eta: 0:26:10  lr: 0.000044  min_lr: 0.000000  loss: 4.4708 (4.4175)  class_acc: 0.1667 (0.2190)  loss_scale: 65536.0000 (49591.4176)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0729  max mem: 15572
Epoch: [10]  [ 270/2809]  eta: 0:26:17  lr: 0.000044  min_lr: 0.000000  loss: 4.5269 (4.4214)  class_acc: 0.1667 (0.2169)  loss_scale: 65536.0000 (50179.7786)  weight_decay: 0.0500 (0.0500)  time: 0.6697  data: 0.1898  max mem: 15572
Epoch: [10]  [ 280/2809]  eta: 0:25:57  lr: 0.000044  min_lr: 0.000000  loss: 4.5428 (4.4236)  class_acc: 0.1667 (0.2165)  loss_scale: 65536.0000 (50726.2633)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1178  max mem: 15572
Epoch: [10]  [ 290/2809]  eta: 0:25:46  lr: 0.000044  min_lr: 0.000000  loss: 4.4198 (4.4225)  class_acc: 0.2500 (0.2169)  loss_scale: 65536.0000 (51235.1890)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0420  max mem: 15572
Epoch: [10]  [ 300/2809]  eta: 0:25:34  lr: 0.000044  min_lr: 0.000000  loss: 4.4198 (4.4250)  class_acc: 0.2083 (0.2162)  loss_scale: 65536.0000 (51710.2990)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0948  max mem: 15572
Epoch: [10]  [ 310/2809]  eta: 0:25:26  lr: 0.000044  min_lr: 0.000000  loss: 4.4029 (4.4187)  class_acc: 0.2083 (0.2169)  loss_scale: 65536.0000 (52154.8553)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1099  max mem: 15572
Epoch: [10]  [ 320/2809]  eta: 0:25:19  lr: 0.000044  min_lr: 0.000000  loss: 4.3800 (4.4195)  class_acc: 0.2500 (0.2178)  loss_scale: 65536.0000 (52571.7134)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1245  max mem: 15572
Epoch: [10]  [ 330/2809]  eta: 0:25:11  lr: 0.000044  min_lr: 0.000000  loss: 4.4745 (4.4227)  class_acc: 0.1667 (0.2171)  loss_scale: 65536.0000 (52963.3837)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1303  max mem: 15572
Epoch: [10]  [ 340/2809]  eta: 0:24:59  lr: 0.000044  min_lr: 0.000000  loss: 4.4745 (4.4213)  class_acc: 0.2083 (0.2169)  loss_scale: 65536.0000 (53332.0821)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0977  max mem: 15572
[2025-01-15 19:18:55,482] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28440
[2025-01-15 19:18:55,482] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:18:55,482] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 350/2809]  eta: 0:24:50  lr: 0.000044  min_lr: 0.000000  loss: 4.3537 (4.4206)  class_acc: 0.2500 (0.2188)  loss_scale: 65536.0000 (53586.4160)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1004  max mem: 15572
Epoch: [10]  [ 360/2809]  eta: 0:24:47  lr: 0.000044  min_lr: 0.000000  loss: 4.4157 (4.4242)  class_acc: 0.2500 (0.2188)  loss_scale: 32768.0000 (53009.7285)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1759  max mem: 15572
Epoch: [10]  [ 370/2809]  eta: 0:24:33  lr: 0.000044  min_lr: 0.000000  loss: 4.6054 (4.4282)  class_acc: 0.1667 (0.2172)  loss_scale: 32768.0000 (52464.1294)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1260  max mem: 15572
Epoch: [10]  [ 380/2809]  eta: 0:24:23  lr: 0.000044  min_lr: 0.000000  loss: 4.5293 (4.4283)  class_acc: 0.1667 (0.2165)  loss_scale: 32768.0000 (51947.1706)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0701  max mem: 15572
Epoch: [10]  [ 390/2809]  eta: 0:24:15  lr: 0.000044  min_lr: 0.000000  loss: 4.5230 (4.4314)  class_acc: 0.1667 (0.2153)  loss_scale: 32768.0000 (51456.6547)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1038  max mem: 15572
Epoch: [10]  [ 400/2809]  eta: 0:24:12  lr: 0.000044  min_lr: 0.000000  loss: 4.5128 (4.4314)  class_acc: 0.1250 (0.2151)  loss_scale: 32768.0000 (50990.6035)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1423  max mem: 15572
Epoch: [10]  [ 410/2809]  eta: 0:24:01  lr: 0.000044  min_lr: 0.000000  loss: 4.4596 (4.4319)  class_acc: 0.2083 (0.2151)  loss_scale: 32768.0000 (50547.2311)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1190  max mem: 15572
Epoch: [10]  [ 420/2809]  eta: 0:23:56  lr: 0.000044  min_lr: 0.000000  loss: 4.4395 (4.4337)  class_acc: 0.2083 (0.2145)  loss_scale: 32768.0000 (50124.9216)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1030  max mem: 15572
Epoch: [10]  [ 430/2809]  eta: 0:23:48  lr: 0.000044  min_lr: 0.000000  loss: 4.4181 (4.4336)  class_acc: 0.2083 (0.2138)  loss_scale: 32768.0000 (49722.2088)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1274  max mem: 15572
Epoch: [10]  [ 440/2809]  eta: 0:23:40  lr: 0.000044  min_lr: 0.000000  loss: 4.3980 (4.4318)  class_acc: 0.2083 (0.2150)  loss_scale: 32768.0000 (49337.7596)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0913  max mem: 15572
Epoch: [10]  [ 450/2809]  eta: 0:23:32  lr: 0.000044  min_lr: 0.000000  loss: 4.3931 (4.4330)  class_acc: 0.2083 (0.2148)  loss_scale: 32768.0000 (48970.3592)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0956  max mem: 15572
Epoch: [10]  [ 460/2809]  eta: 0:23:23  lr: 0.000044  min_lr: 0.000000  loss: 4.3931 (4.4316)  class_acc: 0.1667 (0.2145)  loss_scale: 32768.0000 (48618.8980)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1160  max mem: 15572
Epoch: [10]  [ 470/2809]  eta: 0:23:23  lr: 0.000044  min_lr: 0.000000  loss: 4.4662 (4.4325)  class_acc: 0.1667 (0.2143)  loss_scale: 32768.0000 (48282.3609)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.1952  max mem: 15572
[2025-01-15 19:20:10,081] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:20:10,081] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 480/2809]  eta: 0:23:13  lr: 0.000044  min_lr: 0.000000  loss: 4.5222 (4.4338)  class_acc: 0.1667 (0.2134)  loss_scale: 32768.0000 (48096.0665)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1714  max mem: 15572
Epoch: [10]  [ 490/2809]  eta: 0:23:09  lr: 0.000044  min_lr: 0.000000  loss: 4.3444 (4.4309)  class_acc: 0.1667 (0.2138)  loss_scale: 65536.0000 (48451.2587)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1242  max mem: 15572
Epoch: [10]  [ 500/2809]  eta: 0:23:05  lr: 0.000044  min_lr: 0.000000  loss: 4.4067 (4.4328)  class_acc: 0.2083 (0.2134)  loss_scale: 65536.0000 (48792.2715)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.1872  max mem: 15572
Epoch: [10]  [ 510/2809]  eta: 0:22:59  lr: 0.000044  min_lr: 0.000000  loss: 4.4559 (4.4337)  class_acc: 0.2083 (0.2130)  loss_scale: 65536.0000 (49119.9374)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1587  max mem: 15572
Epoch: [10]  [ 520/2809]  eta: 0:22:53  lr: 0.000044  min_lr: 0.000000  loss: 4.4412 (4.4337)  class_acc: 0.2083 (0.2130)  loss_scale: 65536.0000 (49435.0250)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1079  max mem: 15572
Epoch: [10]  [ 530/2809]  eta: 0:22:47  lr: 0.000044  min_lr: 0.000000  loss: 4.4748 (4.4355)  class_acc: 0.1667 (0.2119)  loss_scale: 65536.0000 (49738.2448)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1238  max mem: 15572
[2025-01-15 19:20:42,034] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28621
[2025-01-15 19:20:42,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:20:42,034] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 540/2809]  eta: 0:22:40  lr: 0.000044  min_lr: 0.000000  loss: 4.4697 (4.4352)  class_acc: 0.1667 (0.2119)  loss_scale: 32768.0000 (49424.5619)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1199  max mem: 15572
Epoch: [10]  [ 550/2809]  eta: 0:22:31  lr: 0.000044  min_lr: 0.000000  loss: 4.5272 (4.4373)  class_acc: 0.1667 (0.2121)  loss_scale: 32768.0000 (49122.2650)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0876  max mem: 15572
Epoch: [10]  [ 560/2809]  eta: 0:22:26  lr: 0.000044  min_lr: 0.000000  loss: 4.4005 (4.4356)  class_acc: 0.2083 (0.2119)  loss_scale: 32768.0000 (48830.7451)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1057  max mem: 15572
Epoch: [10]  [ 570/2809]  eta: 0:22:23  lr: 0.000044  min_lr: 0.000000  loss: 4.3398 (4.4350)  class_acc: 0.1667 (0.2108)  loss_scale: 32768.0000 (48549.4361)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.1560  max mem: 15572
Epoch: [10]  [ 580/2809]  eta: 0:22:11  lr: 0.000044  min_lr: 0.000000  loss: 4.3963 (4.4354)  class_acc: 0.2083 (0.2117)  loss_scale: 32768.0000 (48277.8107)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0920  max mem: 15572
Epoch: [10]  [ 590/2809]  eta: 0:22:06  lr: 0.000044  min_lr: 0.000000  loss: 4.4137 (4.4358)  class_acc: 0.1667 (0.2109)  loss_scale: 32768.0000 (48015.3773)  weight_decay: 0.0500 (0.0500)  time: 0.5305  data: 0.0844  max mem: 15572
Epoch: [10]  [ 600/2809]  eta: 0:22:00  lr: 0.000044  min_lr: 0.000000  loss: 4.3857 (4.4359)  class_acc: 0.1667 (0.2111)  loss_scale: 32768.0000 (47761.6772)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1605  max mem: 15572
Epoch: [10]  [ 610/2809]  eta: 0:21:54  lr: 0.000044  min_lr: 0.000000  loss: 4.3164 (4.4359)  class_acc: 0.2083 (0.2104)  loss_scale: 32768.0000 (47516.2815)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1261  max mem: 15572
Epoch: [10]  [ 620/2809]  eta: 0:21:48  lr: 0.000044  min_lr: 0.000000  loss: 4.4024 (4.4361)  class_acc: 0.1667 (0.2100)  loss_scale: 32768.0000 (47278.7890)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1159  max mem: 15572
Epoch: [10]  [ 630/2809]  eta: 0:21:44  lr: 0.000044  min_lr: 0.000000  loss: 4.4024 (4.4360)  class_acc: 0.2083 (0.2106)  loss_scale: 32768.0000 (47048.8241)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1703  max mem: 15572
Epoch: [10]  [ 640/2809]  eta: 0:21:36  lr: 0.000044  min_lr: 0.000000  loss: 4.4042 (4.4351)  class_acc: 0.2083 (0.2101)  loss_scale: 32768.0000 (46826.0343)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1506  max mem: 15572
Epoch: [10]  [ 650/2809]  eta: 0:21:29  lr: 0.000044  min_lr: 0.000000  loss: 4.4067 (4.4346)  class_acc: 0.2083 (0.2108)  loss_scale: 32768.0000 (46610.0891)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1042  max mem: 15572
[2025-01-15 19:21:57,677] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:21:57,677] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 660/2809]  eta: 0:21:23  lr: 0.000044  min_lr: 0.000000  loss: 4.3703 (4.4329)  class_acc: 0.2083 (0.2110)  loss_scale: 32768.0000 (46450.2511)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.0980  max mem: 15572
Epoch: [10]  [ 670/2809]  eta: 0:21:14  lr: 0.000044  min_lr: 0.000000  loss: 4.3902 (4.4325)  class_acc: 0.2500 (0.2117)  loss_scale: 65536.0000 (46734.6885)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0487  max mem: 15572
Epoch: [10]  [ 680/2809]  eta: 0:21:10  lr: 0.000044  min_lr: 0.000000  loss: 4.3181 (4.4296)  class_acc: 0.2083 (0.2118)  loss_scale: 65536.0000 (47010.7724)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1109  max mem: 15572
Epoch: [10]  [ 690/2809]  eta: 0:21:02  lr: 0.000044  min_lr: 0.000000  loss: 4.2939 (4.4294)  class_acc: 0.2083 (0.2123)  loss_scale: 65536.0000 (47278.8654)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1429  max mem: 15572
Epoch: [10]  [ 700/2809]  eta: 0:20:58  lr: 0.000044  min_lr: 0.000000  loss: 4.3608 (4.4298)  class_acc: 0.1667 (0.2120)  loss_scale: 65536.0000 (47539.3096)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1533  max mem: 15572
Epoch: [10]  [ 710/2809]  eta: 0:20:50  lr: 0.000044  min_lr: 0.000000  loss: 4.5343 (4.4305)  class_acc: 0.1667 (0.2118)  loss_scale: 65536.0000 (47792.4276)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1582  max mem: 15572
[2025-01-15 19:22:33,504] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28810
[2025-01-15 19:22:33,504] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:22:33,504] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 720/2809]  eta: 0:20:47  lr: 0.000044  min_lr: 0.000000  loss: 4.3829 (4.4291)  class_acc: 0.2083 (0.2115)  loss_scale: 65536.0000 (47993.0763)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1614  max mem: 15572
Epoch: [10]  [ 730/2809]  eta: 0:20:42  lr: 0.000044  min_lr: 0.000000  loss: 4.3962 (4.4297)  class_acc: 0.2083 (0.2111)  loss_scale: 32768.0000 (47784.7989)  weight_decay: 0.0500 (0.0500)  time: 0.6745  data: 0.2238  max mem: 15572
Epoch: [10]  [ 740/2809]  eta: 0:20:33  lr: 0.000044  min_lr: 0.000000  loss: 4.4243 (4.4291)  class_acc: 0.1667 (0.2110)  loss_scale: 32768.0000 (47582.1430)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1329  max mem: 15572
Epoch: [10]  [ 750/2809]  eta: 0:20:27  lr: 0.000044  min_lr: 0.000000  loss: 4.4544 (4.4300)  class_acc: 0.1667 (0.2112)  loss_scale: 32768.0000 (47384.8842)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0741  max mem: 15572
Epoch: [10]  [ 760/2809]  eta: 0:20:22  lr: 0.000044  min_lr: 0.000000  loss: 4.4544 (4.4309)  class_acc: 0.1667 (0.2110)  loss_scale: 32768.0000 (47192.8095)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1385  max mem: 15572
Epoch: [10]  [ 770/2809]  eta: 0:20:14  lr: 0.000044  min_lr: 0.000000  loss: 4.5252 (4.4316)  class_acc: 0.1667 (0.2108)  loss_scale: 32768.0000 (47005.7173)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1216  max mem: 15572
Epoch: [10]  [ 780/2809]  eta: 0:20:08  lr: 0.000044  min_lr: 0.000000  loss: 4.5252 (4.4325)  class_acc: 0.2083 (0.2101)  loss_scale: 32768.0000 (46823.4161)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1072  max mem: 15572
Epoch: [10]  [ 790/2809]  eta: 0:20:04  lr: 0.000044  min_lr: 0.000000  loss: 4.4822 (4.4320)  class_acc: 0.2083 (0.2112)  loss_scale: 32768.0000 (46645.7244)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.1695  max mem: 15572
Epoch: [10]  [ 800/2809]  eta: 0:19:55  lr: 0.000044  min_lr: 0.000000  loss: 4.4359 (4.4326)  class_acc: 0.2083 (0.2115)  loss_scale: 32768.0000 (46472.4694)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1085  max mem: 15572
Epoch: [10]  [ 810/2809]  eta: 0:19:51  lr: 0.000044  min_lr: 0.000000  loss: 4.4359 (4.4328)  class_acc: 0.1667 (0.2117)  loss_scale: 32768.0000 (46303.4871)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0910  max mem: 15572
Epoch: [10]  [ 820/2809]  eta: 0:19:43  lr: 0.000044  min_lr: 0.000000  loss: 4.3414 (4.4322)  class_acc: 0.2083 (0.2121)  loss_scale: 32768.0000 (46138.6212)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1204  max mem: 15572
Epoch: [10]  [ 830/2809]  eta: 0:19:38  lr: 0.000044  min_lr: 0.000000  loss: 4.3204 (4.4311)  class_acc: 0.2083 (0.2116)  loss_scale: 32768.0000 (45977.7232)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1136  max mem: 15572
Epoch: [10]  [ 840/2809]  eta: 0:19:34  lr: 0.000044  min_lr: 0.000000  loss: 4.3848 (4.4330)  class_acc: 0.1667 (0.2120)  loss_scale: 32768.0000 (45820.6516)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1803  max mem: 15572
[2025-01-15 19:23:49,198] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:23:49,198] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 850/2809]  eta: 0:19:26  lr: 0.000044  min_lr: 0.000000  loss: 4.5522 (4.4331)  class_acc: 0.2083 (0.2116)  loss_scale: 32768.0000 (45744.2820)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1328  max mem: 15572
Epoch: [10]  [ 860/2809]  eta: 0:19:19  lr: 0.000044  min_lr: 0.000000  loss: 4.2564 (4.4310)  class_acc: 0.1667 (0.2114)  loss_scale: 65536.0000 (45974.1510)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0626  max mem: 15572
Epoch: [10]  [ 870/2809]  eta: 0:19:11  lr: 0.000044  min_lr: 0.000000  loss: 4.2539 (4.4311)  class_acc: 0.2083 (0.2113)  loss_scale: 65536.0000 (46198.7417)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0780  max mem: 15572
Epoch: [10]  [ 880/2809]  eta: 0:19:05  lr: 0.000044  min_lr: 0.000000  loss: 4.4257 (4.4314)  class_acc: 0.2083 (0.2110)  loss_scale: 65536.0000 (46418.2338)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1011  max mem: 15572
Epoch: [10]  [ 890/2809]  eta: 0:18:58  lr: 0.000044  min_lr: 0.000000  loss: 4.4786 (4.4328)  class_acc: 0.1667 (0.2107)  loss_scale: 65536.0000 (46632.7991)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0995  max mem: 15572
Epoch: [10]  [ 900/2809]  eta: 0:18:54  lr: 0.000044  min_lr: 0.000000  loss: 4.4409 (4.4325)  class_acc: 0.1667 (0.2107)  loss_scale: 65536.0000 (46842.6016)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1381  max mem: 15572
[2025-01-15 19:24:23,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=185, lr=[4.287565332087948e-07, 4.287565332087948e-07, 6.125093331554213e-07, 6.125093331554213e-07, 8.750133330791733e-07, 8.750133330791733e-07, 1.250019047255962e-06, 1.250019047255962e-06, 1.7857414960799456e-06, 1.7857414960799456e-06, 2.551059280114208e-06, 2.551059280114208e-06, 3.6443704001631546e-06, 3.6443704001631546e-06, 5.2062434288045075e-06, 5.2062434288045075e-06, 7.437490612577868e-06, 7.437490612577868e-06, 1.0624986589396956e-05, 1.0624986589396956e-05, 1.5178552270567078e-05, 1.5178552270567078e-05, 2.1683646100810113e-05, 2.1683646100810113e-05, 3.097663728687159e-05, 3.097663728687159e-05, 4.4252338981245135e-05, 4.4252338981245135e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:24:23,573] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=27.490444453692863, CurrSamplesPerSec=30.559610540139975, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [10]  [ 910/2809]  eta: 0:18:47  lr: 0.000044  min_lr: 0.000000  loss: 4.4090 (4.4334)  class_acc: 0.1667 (0.2103)  loss_scale: 65536.0000 (47047.7980)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1737  max mem: 15572
Epoch: [10]  [ 920/2809]  eta: 0:18:41  lr: 0.000044  min_lr: 0.000000  loss: 4.4725 (4.4336)  class_acc: 0.1667 (0.2102)  loss_scale: 65536.0000 (47248.5385)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1353  max mem: 15572
[2025-01-15 19:24:31,264] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29013
[2025-01-15 19:24:31,264] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:24:31,265] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 930/2809]  eta: 0:18:36  lr: 0.000044  min_lr: 0.000000  loss: 4.4725 (4.4340)  class_acc: 0.2083 (0.2103)  loss_scale: 65536.0000 (47163.3942)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1440  max mem: 15572
Epoch: [10]  [ 940/2809]  eta: 0:18:29  lr: 0.000044  min_lr: 0.000000  loss: 4.4429 (4.4339)  class_acc: 0.2083 (0.2107)  loss_scale: 32768.0000 (47010.4145)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1331  max mem: 15572
Epoch: [10]  [ 950/2809]  eta: 0:18:24  lr: 0.000044  min_lr: 0.000000  loss: 4.4889 (4.4339)  class_acc: 0.2083 (0.2106)  loss_scale: 32768.0000 (46860.6519)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1488  max mem: 15572
Epoch: [10]  [ 960/2809]  eta: 0:18:18  lr: 0.000044  min_lr: 0.000000  loss: 4.3354 (4.4330)  class_acc: 0.2083 (0.2105)  loss_scale: 32768.0000 (46714.0062)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.1818  max mem: 15572
Epoch: [10]  [ 970/2809]  eta: 0:18:13  lr: 0.000044  min_lr: 0.000000  loss: 4.3370 (4.4327)  class_acc: 0.1667 (0.2106)  loss_scale: 32768.0000 (46570.3811)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1517  max mem: 15572
Epoch: [10]  [ 980/2809]  eta: 0:18:07  lr: 0.000044  min_lr: 0.000000  loss: 4.3823 (4.4329)  class_acc: 0.1667 (0.2104)  loss_scale: 32768.0000 (46429.6840)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1068  max mem: 15572
Epoch: [10]  [ 990/2809]  eta: 0:18:00  lr: 0.000044  min_lr: 0.000000  loss: 4.4678 (4.4331)  class_acc: 0.1667 (0.2100)  loss_scale: 32768.0000 (46291.8264)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0562  max mem: 15572
Epoch: [10]  [1000/2809]  eta: 0:17:52  lr: 0.000044  min_lr: 0.000000  loss: 4.4486 (4.4336)  class_acc: 0.1667 (0.2094)  loss_scale: 32768.0000 (46156.7233)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0440  max mem: 15572
Epoch: [10]  [1010/2809]  eta: 0:17:45  lr: 0.000044  min_lr: 0.000000  loss: 4.4283 (4.4330)  class_acc: 0.1250 (0.2092)  loss_scale: 32768.0000 (46024.2928)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0523  max mem: 15572
Epoch: [10]  [1020/2809]  eta: 0:17:39  lr: 0.000044  min_lr: 0.000000  loss: 4.4373 (4.4344)  class_acc: 0.1667 (0.2092)  loss_scale: 32768.0000 (45894.4564)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0881  max mem: 15572
Epoch: [10]  [1030/2809]  eta: 0:17:34  lr: 0.000044  min_lr: 0.000000  loss: 4.4416 (4.4338)  class_acc: 0.2083 (0.2095)  loss_scale: 32768.0000 (45767.1387)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1853  max mem: 15572
Epoch: [10]  [1040/2809]  eta: 0:17:30  lr: 0.000044  min_lr: 0.000000  loss: 4.3433 (4.4327)  class_acc: 0.2500 (0.2101)  loss_scale: 32768.0000 (45642.2671)  weight_decay: 0.0500 (0.0500)  time: 0.6801  data: 0.2438  max mem: 15572
Epoch: [10]  [1050/2809]  eta: 0:17:23  lr: 0.000044  min_lr: 0.000000  loss: 4.3546 (4.4327)  class_acc: 0.2500 (0.2105)  loss_scale: 32768.0000 (45519.7716)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1681  max mem: 15572
[2025-01-15 19:25:47,367] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:25:47,368] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [1060/2809]  eta: 0:17:17  lr: 0.000044  min_lr: 0.000000  loss: 4.4045 (4.4332)  class_acc: 0.2083 (0.2101)  loss_scale: 32768.0000 (45677.5419)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1214  max mem: 15572
Epoch: [10]  [1070/2809]  eta: 0:17:10  lr: 0.000044  min_lr: 0.000000  loss: 4.4184 (4.4336)  class_acc: 0.1667 (0.2101)  loss_scale: 65536.0000 (45862.9617)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0971  max mem: 15572
[2025-01-15 19:25:59,512] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29165
[2025-01-15 19:25:59,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:25:59,512] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [1080/2809]  eta: 0:17:04  lr: 0.000044  min_lr: 0.000000  loss: 4.4538 (4.4336)  class_acc: 0.2083 (0.2100)  loss_scale: 65536.0000 (45863.0749)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.1059  max mem: 15572
Epoch: [10]  [1090/2809]  eta: 0:16:57  lr: 0.000044  min_lr: 0.000000  loss: 4.4538 (4.4337)  class_acc: 0.2083 (0.2104)  loss_scale: 32768.0000 (45743.0467)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1413  max mem: 15572
Epoch: [10]  [1100/2809]  eta: 0:16:52  lr: 0.000044  min_lr: 0.000000  loss: 4.4201 (4.4340)  class_acc: 0.2083 (0.2101)  loss_scale: 32768.0000 (45625.1989)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1423  max mem: 15572
Epoch: [10]  [1110/2809]  eta: 0:16:47  lr: 0.000044  min_lr: 0.000000  loss: 4.3630 (4.4338)  class_acc: 0.2083 (0.2101)  loss_scale: 32768.0000 (45509.4725)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.1384  max mem: 15572
Epoch: [10]  [1120/2809]  eta: 0:16:40  lr: 0.000044  min_lr: 0.000000  loss: 4.4449 (4.4329)  class_acc: 0.2500 (0.2104)  loss_scale: 32768.0000 (45395.8109)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.0885  max mem: 15572
Epoch: [10]  [1130/2809]  eta: 0:16:35  lr: 0.000044  min_lr: 0.000000  loss: 4.4291 (4.4326)  class_acc: 0.1667 (0.2100)  loss_scale: 32768.0000 (45284.1592)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1263  max mem: 15572
Epoch: [10]  [1140/2809]  eta: 0:16:29  lr: 0.000044  min_lr: 0.000000  loss: 4.4291 (4.4331)  class_acc: 0.1667 (0.2105)  loss_scale: 32768.0000 (45174.4645)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1802  max mem: 15572
Epoch: [10]  [1150/2809]  eta: 0:16:23  lr: 0.000044  min_lr: 0.000000  loss: 4.4422 (4.4329)  class_acc: 0.2083 (0.2107)  loss_scale: 32768.0000 (45066.6759)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1865  max mem: 15572
Epoch: [10]  [1160/2809]  eta: 0:16:18  lr: 0.000044  min_lr: 0.000000  loss: 4.4739 (4.4328)  class_acc: 0.1667 (0.2103)  loss_scale: 32768.0000 (44960.7442)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1940  max mem: 15572
Epoch: [10]  [1170/2809]  eta: 0:16:12  lr: 0.000044  min_lr: 0.000000  loss: 4.5088 (4.4336)  class_acc: 0.1667 (0.2101)  loss_scale: 32768.0000 (44856.6217)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1691  max mem: 15572
Epoch: [10]  [1180/2809]  eta: 0:16:05  lr: 0.000044  min_lr: 0.000000  loss: 4.4876 (4.4328)  class_acc: 0.1667 (0.2102)  loss_scale: 32768.0000 (44754.2625)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1096  max mem: 15572
Epoch: [10]  [1190/2809]  eta: 0:15:59  lr: 0.000044  min_lr: 0.000000  loss: 4.3598 (4.4330)  class_acc: 0.1667 (0.2099)  loss_scale: 32768.0000 (44653.6222)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1056  max mem: 15572
Epoch: [10]  [1200/2809]  eta: 0:15:55  lr: 0.000044  min_lr: 0.000000  loss: 4.4569 (4.4339)  class_acc: 0.1250 (0.2094)  loss_scale: 32768.0000 (44554.6578)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1874  max mem: 15572
[2025-01-15 19:27:18,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:27:18,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [1210/2809]  eta: 0:15:48  lr: 0.000044  min_lr: 0.000000  loss: 4.5369 (4.4347)  class_acc: 0.1250 (0.2094)  loss_scale: 32768.0000 (44646.7382)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1678  max mem: 15572
Epoch: [10]  [1220/2809]  eta: 0:15:43  lr: 0.000044  min_lr: 0.000000  loss: 4.5369 (4.4352)  class_acc: 0.1667 (0.2090)  loss_scale: 65536.0000 (44817.8215)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1289  max mem: 15572
Epoch: [10]  [1230/2809]  eta: 0:15:38  lr: 0.000044  min_lr: 0.000000  loss: 4.4419 (4.4358)  class_acc: 0.1667 (0.2088)  loss_scale: 65536.0000 (44986.1251)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1869  max mem: 15572
[2025-01-15 19:27:34,882] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29321
[2025-01-15 19:27:34,883] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:27:34,883] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [1240/2809]  eta: 0:15:31  lr: 0.000044  min_lr: 0.000000  loss: 4.5635 (4.4360)  class_acc: 0.2083 (0.2090)  loss_scale: 32768.0000 (44887.6712)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1477  max mem: 15572
Epoch: [10]  [1250/2809]  eta: 0:15:25  lr: 0.000044  min_lr: 0.000000  loss: 4.6486 (4.4370)  class_acc: 0.1250 (0.2084)  loss_scale: 32768.0000 (44790.7914)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1027  max mem: 15572
Epoch: [10]  [1260/2809]  eta: 0:15:19  lr: 0.000044  min_lr: 0.000000  loss: 4.4574 (4.4367)  class_acc: 0.1667 (0.2087)  loss_scale: 32768.0000 (44695.4481)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1230  max mem: 15572
Epoch: [10]  [1270/2809]  eta: 0:15:14  lr: 0.000044  min_lr: 0.000000  loss: 4.3785 (4.4359)  class_acc: 0.2083 (0.2087)  loss_scale: 32768.0000 (44601.6050)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1517  max mem: 15572
Epoch: [10]  [1280/2809]  eta: 0:15:07  lr: 0.000044  min_lr: 0.000000  loss: 4.3625 (4.4356)  class_acc: 0.2500 (0.2087)  loss_scale: 32768.0000 (44509.2272)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1394  max mem: 15572
Epoch: [10]  [1290/2809]  eta: 0:15:02  lr: 0.000044  min_lr: 0.000000  loss: 4.5074 (4.4368)  class_acc: 0.2500 (0.2090)  loss_scale: 32768.0000 (44418.2804)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1405  max mem: 15572
Epoch: [10]  [1300/2809]  eta: 0:14:56  lr: 0.000044  min_lr: 0.000000  loss: 4.4619 (4.4362)  class_acc: 0.2500 (0.2092)  loss_scale: 32768.0000 (44328.7317)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1860  max mem: 15572
Epoch: [10]  [1310/2809]  eta: 0:14:51  lr: 0.000044  min_lr: 0.000000  loss: 4.3134 (4.4359)  class_acc: 0.2083 (0.2091)  loss_scale: 32768.0000 (44240.5492)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.1895  max mem: 15572
Epoch: [10]  [1320/2809]  eta: 0:14:44  lr: 0.000044  min_lr: 0.000000  loss: 4.4554 (4.4364)  class_acc: 0.2083 (0.2091)  loss_scale: 32768.0000 (44153.7017)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1225  max mem: 15572
Epoch: [10]  [1330/2809]  eta: 0:14:38  lr: 0.000044  min_lr: 0.000000  loss: 4.4855 (4.4361)  class_acc: 0.2083 (0.2092)  loss_scale: 32768.0000 (44068.1593)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1076  max mem: 15572
Epoch: [10]  [1340/2809]  eta: 0:14:32  lr: 0.000044  min_lr: 0.000000  loss: 4.3802 (4.4365)  class_acc: 0.2083 (0.2092)  loss_scale: 32768.0000 (43983.8926)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1083  max mem: 15572
Epoch: [10]  [1350/2809]  eta: 0:14:25  lr: 0.000044  min_lr: 0.000000  loss: 4.3883 (4.4367)  class_acc: 0.1667 (0.2094)  loss_scale: 32768.0000 (43900.8734)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0672  max mem: 15572
[2025-01-15 19:28:50,975] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:28:50,975] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [1360/2809]  eta: 0:14:20  lr: 0.000044  min_lr: 0.000000  loss: 4.3370 (4.4362)  class_acc: 0.2083 (0.2096)  loss_scale: 32768.0000 (43843.1506)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1268  max mem: 15572
Epoch: [10]  [1370/2809]  eta: 0:14:14  lr: 0.000044  min_lr: 0.000000  loss: 4.3370 (4.4363)  class_acc: 0.2083 (0.2098)  loss_scale: 65536.0000 (44001.3771)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1833  max mem: 15572
Epoch: [10]  [1380/2809]  eta: 0:14:08  lr: 0.000044  min_lr: 0.000000  loss: 4.5204 (4.4364)  class_acc: 0.2083 (0.2100)  loss_scale: 65536.0000 (44157.3121)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.1928  max mem: 15572
Epoch: [10]  [1390/2809]  eta: 0:14:01  lr: 0.000044  min_lr: 0.000000  loss: 4.5253 (4.4368)  class_acc: 0.2083 (0.2101)  loss_scale: 65536.0000 (44311.0050)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1204  max mem: 15572
Epoch: [10]  [1400/2809]  eta: 0:13:55  lr: 0.000044  min_lr: 0.000000  loss: 4.5108 (4.4368)  class_acc: 0.2083 (0.2099)  loss_scale: 65536.0000 (44462.5039)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0858  max mem: 15572
Epoch: [10]  [1410/2809]  eta: 0:13:50  lr: 0.000044  min_lr: 0.000000  loss: 4.4769 (4.4374)  class_acc: 0.1667 (0.2096)  loss_scale: 65536.0000 (44611.8554)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1860  max mem: 15572
Epoch: [10]  [1420/2809]  eta: 0:13:44  lr: 0.000044  min_lr: 0.000000  loss: 4.4769 (4.4380)  class_acc: 0.1667 (0.2097)  loss_scale: 65536.0000 (44759.1049)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1577  max mem: 15572
Epoch: [10]  [1430/2809]  eta: 0:13:38  lr: 0.000044  min_lr: 0.000000  loss: 4.3619 (4.4376)  class_acc: 0.2500 (0.2098)  loss_scale: 65536.0000 (44904.2963)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1032  max mem: 15572
Epoch: [10]  [1440/2809]  eta: 0:13:32  lr: 0.000044  min_lr: 0.000000  loss: 4.3597 (4.4373)  class_acc: 0.2500 (0.2100)  loss_scale: 65536.0000 (45047.4726)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1051  max mem: 15572
Epoch: [10]  [1450/2809]  eta: 0:13:25  lr: 0.000044  min_lr: 0.000000  loss: 4.4002 (4.4378)  class_acc: 0.2083 (0.2100)  loss_scale: 65536.0000 (45188.6754)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0727  max mem: 15572
Epoch: [10]  [1460/2809]  eta: 0:13:19  lr: 0.000044  min_lr: 0.000000  loss: 4.4679 (4.4379)  class_acc: 0.2083 (0.2100)  loss_scale: 65536.0000 (45327.9452)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0539  max mem: 15572
Epoch: [10]  [1470/2809]  eta: 0:13:12  lr: 0.000044  min_lr: 0.000000  loss: 4.5315 (4.4383)  class_acc: 0.1667 (0.2099)  loss_scale: 65536.0000 (45465.3215)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0376  max mem: 15572
Epoch: [10]  [1480/2809]  eta: 0:13:06  lr: 0.000044  min_lr: 0.000000  loss: 4.5388 (4.4385)  class_acc: 0.2500 (0.2100)  loss_scale: 65536.0000 (45600.8427)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0852  max mem: 15572
[2025-01-15 19:30:03,312] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:30:03,312] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [1490/2809]  eta: 0:13:00  lr: 0.000044  min_lr: 0.000000  loss: 4.4772 (4.4390)  class_acc: 0.2083 (0.2098)  loss_scale: 65536.0000 (45866.4091)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1498  max mem: 15572
[2025-01-15 19:30:07,284] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29583
[2025-01-15 19:30:07,284] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:30:07,285] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1500/2809]  eta: 0:12:55  lr: 0.000044  min_lr: 0.000000  loss: 4.4098 (4.4391)  class_acc: 0.1667 (0.2100)  loss_scale: 65536.0000 (46084.7755)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1784  max mem: 15572
Epoch: [10]  [1510/2809]  eta: 0:12:49  lr: 0.000044  min_lr: 0.000000  loss: 4.3818 (4.4390)  class_acc: 0.1667 (0.2098)  loss_scale: 65536.0000 (46213.5063)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1558  max mem: 15572
Epoch: [10]  [1520/2809]  eta: 0:12:42  lr: 0.000044  min_lr: 0.000000  loss: 4.5004 (4.4399)  class_acc: 0.1250 (0.2093)  loss_scale: 65536.0000 (46340.5444)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1050  max mem: 15572
Epoch: [10]  [1530/2809]  eta: 0:12:36  lr: 0.000044  min_lr: 0.000000  loss: 4.3208 (4.4382)  class_acc: 0.1667 (0.2094)  loss_scale: 65536.0000 (46465.9229)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1145  max mem: 15572
Epoch: [10]  [1540/2809]  eta: 0:12:31  lr: 0.000044  min_lr: 0.000000  loss: 4.3169 (4.4375)  class_acc: 0.1667 (0.2096)  loss_scale: 65536.0000 (46589.6742)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1603  max mem: 15572
Epoch: [10]  [1550/2809]  eta: 0:12:25  lr: 0.000044  min_lr: 0.000000  loss: 4.5256 (4.4386)  class_acc: 0.2083 (0.2095)  loss_scale: 65536.0000 (46711.8298)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1634  max mem: 15572
Epoch: [10]  [1560/2809]  eta: 0:12:19  lr: 0.000044  min_lr: 0.000000  loss: 4.5256 (4.4385)  class_acc: 0.1667 (0.2091)  loss_scale: 65536.0000 (46832.4202)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1379  max mem: 15572
Epoch: [10]  [1570/2809]  eta: 0:12:13  lr: 0.000044  min_lr: 0.000000  loss: 4.3545 (4.4383)  class_acc: 0.2083 (0.2095)  loss_scale: 65536.0000 (46951.4755)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1079  max mem: 15572
Epoch: [10]  [1580/2809]  eta: 0:12:07  lr: 0.000044  min_lr: 0.000000  loss: 4.4278 (4.4386)  class_acc: 0.1667 (0.2089)  loss_scale: 65536.0000 (47069.0247)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1006  max mem: 15572
Epoch: [10]  [1590/2809]  eta: 0:12:01  lr: 0.000044  min_lr: 0.000000  loss: 4.5607 (4.4392)  class_acc: 0.1250 (0.2087)  loss_scale: 65536.0000 (47185.0962)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1366  max mem: 15572
Epoch: [10]  [1600/2809]  eta: 0:11:55  lr: 0.000044  min_lr: 0.000000  loss: 4.5607 (4.4396)  class_acc: 0.1667 (0.2082)  loss_scale: 65536.0000 (47299.7177)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1027  max mem: 15572
Epoch: [10]  [1610/2809]  eta: 0:11:48  lr: 0.000044  min_lr: 0.000000  loss: 4.5095 (4.4391)  class_acc: 0.1250 (0.2082)  loss_scale: 65536.0000 (47412.9162)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0410  max mem: 15572
Epoch: [10]  [1620/2809]  eta: 0:11:42  lr: 0.000044  min_lr: 0.000000  loss: 4.5470 (4.4399)  class_acc: 0.1667 (0.2080)  loss_scale: 65536.0000 (47524.7181)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0474  max mem: 15572
[2025-01-15 19:31:22,060] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:31:22,060] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:31:22,482] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29713
[2025-01-15 19:31:22,482] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:31:22,482] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1630/2809]  eta: 0:11:37  lr: 0.000044  min_lr: 0.000000  loss: 4.5670 (4.4405)  class_acc: 0.1667 (0.2080)  loss_scale: 65536.0000 (47675.3305)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1368  max mem: 15572
Epoch: [10]  [1640/2809]  eta: 0:11:31  lr: 0.000044  min_lr: 0.000000  loss: 4.3915 (4.4396)  class_acc: 0.2083 (0.2081)  loss_scale: 65536.0000 (47784.1706)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.1937  max mem: 15572
Epoch: [10]  [1650/2809]  eta: 0:11:26  lr: 0.000044  min_lr: 0.000000  loss: 4.2365 (4.4381)  class_acc: 0.2083 (0.2083)  loss_scale: 65536.0000 (47891.6923)  weight_decay: 0.0500 (0.0500)  time: 0.6587  data: 0.2077  max mem: 15572
Epoch: [10]  [1660/2809]  eta: 0:11:20  lr: 0.000044  min_lr: 0.000000  loss: 4.2303 (4.4376)  class_acc: 0.2500 (0.2085)  loss_scale: 65536.0000 (47997.9193)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1683  max mem: 15572
[2025-01-15 19:31:50,683] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29759
[2025-01-15 19:31:50,683] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:31:50,684] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [1670/2809]  eta: 0:11:14  lr: 0.000044  min_lr: 0.000000  loss: 4.3950 (4.4378)  class_acc: 0.2083 (0.2086)  loss_scale: 65536.0000 (48063.6553)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1131  max mem: 15572
Epoch: [10]  [1680/2809]  eta: 0:11:08  lr: 0.000044  min_lr: 0.000000  loss: 4.4021 (4.4378)  class_acc: 0.2083 (0.2089)  loss_scale: 32768.0000 (47972.6639)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1467  max mem: 15572
Epoch: [10]  [1690/2809]  eta: 0:11:02  lr: 0.000044  min_lr: 0.000000  loss: 4.3780 (4.4369)  class_acc: 0.2500 (0.2091)  loss_scale: 32768.0000 (47882.7487)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1245  max mem: 15572
Epoch: [10]  [1700/2809]  eta: 0:10:56  lr: 0.000044  min_lr: 0.000000  loss: 4.3464 (4.4365)  class_acc: 0.2083 (0.2090)  loss_scale: 32768.0000 (47793.8907)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0451  max mem: 15572
Epoch: [10]  [1710/2809]  eta: 0:10:49  lr: 0.000044  min_lr: 0.000000  loss: 4.2932 (4.4356)  class_acc: 0.2083 (0.2092)  loss_scale: 32768.0000 (47706.0713)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0403  max mem: 15572
Epoch: [10]  [1720/2809]  eta: 0:10:43  lr: 0.000044  min_lr: 0.000000  loss: 4.2884 (4.4354)  class_acc: 0.2500 (0.2094)  loss_scale: 32768.0000 (47619.2725)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0927  max mem: 15572
Epoch: [10]  [1730/2809]  eta: 0:10:38  lr: 0.000044  min_lr: 0.000000  loss: 4.3717 (4.4352)  class_acc: 0.1667 (0.2092)  loss_scale: 32768.0000 (47533.4766)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1486  max mem: 15572
Epoch: [10]  [1740/2809]  eta: 0:10:32  lr: 0.000044  min_lr: 0.000000  loss: 4.3476 (4.4342)  class_acc: 0.1667 (0.2091)  loss_scale: 32768.0000 (47448.6663)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1560  max mem: 15572
Epoch: [10]  [1750/2809]  eta: 0:10:26  lr: 0.000044  min_lr: 0.000000  loss: 4.4310 (4.4347)  class_acc: 0.1250 (0.2087)  loss_scale: 32768.0000 (47364.8247)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1604  max mem: 15572
Epoch: [10]  [1760/2809]  eta: 0:10:21  lr: 0.000044  min_lr: 0.000000  loss: 4.4631 (4.4348)  class_acc: 0.2083 (0.2091)  loss_scale: 32768.0000 (47281.9353)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.1969  max mem: 15572
Epoch: [10]  [1770/2809]  eta: 0:10:16  lr: 0.000044  min_lr: 0.000000  loss: 4.4317 (4.4353)  class_acc: 0.2083 (0.2092)  loss_scale: 32768.0000 (47199.9819)  weight_decay: 0.0500 (0.0500)  time: 0.7035  data: 0.2428  max mem: 15572
Epoch: [10]  [1780/2809]  eta: 0:10:09  lr: 0.000044  min_lr: 0.000000  loss: 4.3647 (4.4354)  class_acc: 0.1667 (0.2091)  loss_scale: 32768.0000 (47118.9489)  weight_decay: 0.0500 (0.0500)  time: 0.6394  data: 0.1730  max mem: 15572
Epoch: [10]  [1790/2809]  eta: 0:10:03  lr: 0.000044  min_lr: 0.000000  loss: 4.3403 (4.4348)  class_acc: 0.2500 (0.2097)  loss_scale: 32768.0000 (47038.8208)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0570  max mem: 15572
[2025-01-15 19:33:07,510] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:33:07,511] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [1800/2809]  eta: 0:09:57  lr: 0.000044  min_lr: 0.000000  loss: 4.4108 (4.4349)  class_acc: 0.2500 (0.2096)  loss_scale: 32768.0000 (47014.1655)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0767  max mem: 15572
[2025-01-15 19:33:13,697] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29899
[2025-01-15 19:33:13,698] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:33:13,698] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [1810/2809]  eta: 0:09:50  lr: 0.000044  min_lr: 0.000000  loss: 4.4484 (4.4346)  class_acc: 0.1667 (0.2096)  loss_scale: 65536.0000 (47080.2518)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0470  max mem: 15572
Epoch: [10]  [1820/2809]  eta: 0:09:45  lr: 0.000044  min_lr: 0.000000  loss: 4.4437 (4.4346)  class_acc: 0.2083 (0.2098)  loss_scale: 32768.0000 (47001.6562)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.0881  max mem: 15572
Epoch: [10]  [1830/2809]  eta: 0:09:39  lr: 0.000044  min_lr: 0.000000  loss: 4.4447 (4.4345)  class_acc: 0.2083 (0.2098)  loss_scale: 32768.0000 (46923.9192)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1502  max mem: 15572
Epoch: [10]  [1840/2809]  eta: 0:09:33  lr: 0.000044  min_lr: 0.000000  loss: 4.4447 (4.4346)  class_acc: 0.1667 (0.2095)  loss_scale: 32768.0000 (46847.0266)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1206  max mem: 15572
Epoch: [10]  [1850/2809]  eta: 0:09:27  lr: 0.000044  min_lr: 0.000000  loss: 4.4668 (4.4345)  class_acc: 0.1667 (0.2096)  loss_scale: 32768.0000 (46770.9649)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1259  max mem: 15572
Epoch: [10]  [1860/2809]  eta: 0:09:21  lr: 0.000044  min_lr: 0.000000  loss: 4.4484 (4.4346)  class_acc: 0.1667 (0.2093)  loss_scale: 32768.0000 (46695.7206)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1373  max mem: 15572
Epoch: [10]  [1870/2809]  eta: 0:09:15  lr: 0.000044  min_lr: 0.000000  loss: 4.4657 (4.4347)  class_acc: 0.1250 (0.2092)  loss_scale: 32768.0000 (46621.2806)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1182  max mem: 15572
Epoch: [10]  [1880/2809]  eta: 0:09:09  lr: 0.000044  min_lr: 0.000000  loss: 4.4940 (4.4348)  class_acc: 0.2083 (0.2091)  loss_scale: 32768.0000 (46547.6321)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1417  max mem: 15572
Epoch: [10]  [1890/2809]  eta: 0:09:03  lr: 0.000044  min_lr: 0.000000  loss: 4.4615 (4.4350)  class_acc: 0.2083 (0.2092)  loss_scale: 32768.0000 (46474.7626)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1505  max mem: 15572
Epoch: [10]  [1900/2809]  eta: 0:08:57  lr: 0.000044  min_lr: 0.000000  loss: 4.5182 (4.4354)  class_acc: 0.2083 (0.2092)  loss_scale: 32768.0000 (46402.6597)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1683  max mem: 15572
[2025-01-15 19:34:12,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=192, lr=[4.2532066161678034e-07, 4.2532066161678034e-07, 6.076009451668291e-07, 6.076009451668291e-07, 8.680013502383275e-07, 8.680013502383275e-07, 1.2400019289118964e-06, 1.2400019289118964e-06, 1.771431327016995e-06, 1.771431327016995e-06, 2.53061618145285e-06, 2.53061618145285e-06, 3.6151659735040717e-06, 3.6151659735040717e-06, 5.164522819291532e-06, 5.164522819291532e-06, 7.377889741845045e-06, 7.377889741845045e-06, 1.0539842488350066e-05, 1.0539842488350066e-05, 1.5056917840500093e-05, 1.5056917840500093e-05, 2.150988262928585e-05, 2.150988262928585e-05, 3.0728403756122646e-05, 3.0728403756122646e-05, 4.389771965160378e-05, 4.389771965160378e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:34:12,390] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=27.496533468942367, CurrSamplesPerSec=22.9696288744292, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [10]  [1910/2809]  eta: 0:08:51  lr: 0.000044  min_lr: 0.000000  loss: 4.5182 (4.4354)  class_acc: 0.2083 (0.2094)  loss_scale: 32768.0000 (46331.3114)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1322  max mem: 15572
Epoch: [10]  [1920/2809]  eta: 0:08:45  lr: 0.000044  min_lr: 0.000000  loss: 4.4578 (4.4357)  class_acc: 0.2083 (0.2096)  loss_scale: 32768.0000 (46260.7059)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0909  max mem: 15572
Epoch: [10]  [1930/2809]  eta: 0:08:39  lr: 0.000044  min_lr: 0.000000  loss: 4.4540 (4.4356)  class_acc: 0.2083 (0.2097)  loss_scale: 32768.0000 (46190.8317)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1199  max mem: 15572
[2025-01-15 19:34:29,338] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:34:29,338] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [1940/2809]  eta: 0:08:33  lr: 0.000044  min_lr: 0.000000  loss: 4.4228 (4.4355)  class_acc: 0.1667 (0.2096)  loss_scale: 32768.0000 (46172.3235)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0952  max mem: 15572
Epoch: [10]  [1950/2809]  eta: 0:08:27  lr: 0.000044  min_lr: 0.000000  loss: 4.3406 (4.4348)  class_acc: 0.2083 (0.2098)  loss_scale: 65536.0000 (46271.5736)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0939  max mem: 15572
Epoch: [10]  [1960/2809]  eta: 0:08:21  lr: 0.000044  min_lr: 0.000000  loss: 4.3406 (4.4351)  class_acc: 0.2083 (0.2097)  loss_scale: 65536.0000 (46369.8113)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1045  max mem: 15572
[2025-01-15 19:34:44,649] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30055
[2025-01-15 19:34:44,650] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:34:44,650] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [1970/2809]  eta: 0:08:15  lr: 0.000044  min_lr: 0.000000  loss: 4.5365 (4.4354)  class_acc: 0.1667 (0.2095)  loss_scale: 65536.0000 (46367.3019)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0709  max mem: 15572
Epoch: [10]  [1980/2809]  eta: 0:08:09  lr: 0.000044  min_lr: 0.000000  loss: 4.4440 (4.4353)  class_acc: 0.1667 (0.2095)  loss_scale: 32768.0000 (46298.6532)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0912  max mem: 15572
Epoch: [10]  [1990/2809]  eta: 0:08:03  lr: 0.000044  min_lr: 0.000000  loss: 4.3898 (4.4350)  class_acc: 0.2083 (0.2097)  loss_scale: 32768.0000 (46230.6941)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1580  max mem: 15572
Epoch: [10]  [2000/2809]  eta: 0:07:58  lr: 0.000044  min_lr: 0.000000  loss: 4.3411 (4.4349)  class_acc: 0.2083 (0.2097)  loss_scale: 32768.0000 (46163.4143)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1733  max mem: 15572
Epoch: [10]  [2010/2809]  eta: 0:07:52  lr: 0.000044  min_lr: 0.000000  loss: 4.3601 (4.4348)  class_acc: 0.1667 (0.2096)  loss_scale: 32768.0000 (46096.8036)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1604  max mem: 15572
Epoch: [10]  [2020/2809]  eta: 0:07:46  lr: 0.000044  min_lr: 0.000000  loss: 4.4564 (4.4352)  class_acc: 0.1667 (0.2093)  loss_scale: 32768.0000 (46030.8521)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1298  max mem: 15572
Epoch: [10]  [2030/2809]  eta: 0:07:40  lr: 0.000044  min_lr: 0.000000  loss: 4.4377 (4.4353)  class_acc: 0.2083 (0.2095)  loss_scale: 32768.0000 (45965.5500)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1068  max mem: 15572
Epoch: [10]  [2040/2809]  eta: 0:07:34  lr: 0.000044  min_lr: 0.000000  loss: 4.4377 (4.4357)  class_acc: 0.2083 (0.2093)  loss_scale: 32768.0000 (45900.8878)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1262  max mem: 15572
Epoch: [10]  [2050/2809]  eta: 0:07:28  lr: 0.000044  min_lr: 0.000000  loss: 4.5463 (4.4365)  class_acc: 0.1667 (0.2091)  loss_scale: 32768.0000 (45836.8562)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1081  max mem: 15572
Epoch: [10]  [2060/2809]  eta: 0:07:22  lr: 0.000044  min_lr: 0.000000  loss: 4.4416 (4.4362)  class_acc: 0.1667 (0.2091)  loss_scale: 32768.0000 (45773.4459)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1076  max mem: 15572
Epoch: [10]  [2070/2809]  eta: 0:07:16  lr: 0.000044  min_lr: 0.000000  loss: 4.4092 (4.4364)  class_acc: 0.1667 (0.2091)  loss_scale: 32768.0000 (45710.6480)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1579  max mem: 15572
Epoch: [10]  [2080/2809]  eta: 0:07:11  lr: 0.000044  min_lr: 0.000000  loss: 4.5631 (4.4372)  class_acc: 0.2083 (0.2095)  loss_scale: 32768.0000 (45648.4536)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1671  max mem: 15572
Epoch: [10]  [2090/2809]  eta: 0:07:05  lr: 0.000044  min_lr: 0.000000  loss: 4.5041 (4.4373)  class_acc: 0.2083 (0.2096)  loss_scale: 32768.0000 (45586.8541)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1504  max mem: 15572
[2025-01-15 19:36:01,568] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:36:01,568] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [2100/2809]  eta: 0:06:59  lr: 0.000044  min_lr: 0.000000  loss: 4.5041 (4.4377)  class_acc: 0.1667 (0.2095)  loss_scale: 32768.0000 (45635.0157)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1673  max mem: 15572
Epoch: [10]  [2110/2809]  eta: 0:06:53  lr: 0.000044  min_lr: 0.000000  loss: 4.5659 (4.4383)  class_acc: 0.1667 (0.2095)  loss_scale: 65536.0000 (45729.2885)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1790  max mem: 15572
Epoch: [10]  [2120/2809]  eta: 0:06:47  lr: 0.000044  min_lr: 0.000000  loss: 4.5703 (4.4383)  class_acc: 0.2083 (0.2096)  loss_scale: 65536.0000 (45822.6723)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1015  max mem: 15572
Epoch: [10]  [2130/2809]  eta: 0:06:41  lr: 0.000044  min_lr: 0.000000  loss: 4.4420 (4.4382)  class_acc: 0.2083 (0.2094)  loss_scale: 65536.0000 (45915.1797)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1055  max mem: 15572
Epoch: [10]  [2140/2809]  eta: 0:06:35  lr: 0.000044  min_lr: 0.000000  loss: 4.3948 (4.4374)  class_acc: 0.2083 (0.2095)  loss_scale: 65536.0000 (46006.8230)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1043  max mem: 15572
Epoch: [10]  [2150/2809]  eta: 0:06:29  lr: 0.000044  min_lr: 0.000000  loss: 4.3187 (4.4374)  class_acc: 0.2083 (0.2094)  loss_scale: 65536.0000 (46097.6141)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0961  max mem: 15572
Epoch: [10]  [2160/2809]  eta: 0:06:24  lr: 0.000044  min_lr: 0.000000  loss: 4.4697 (4.4375)  class_acc: 0.2083 (0.2095)  loss_scale: 65536.0000 (46187.5650)  weight_decay: 0.0500 (0.0500)  time: 0.6731  data: 0.1731  max mem: 15572
Epoch: [10]  [2170/2809]  eta: 0:06:18  lr: 0.000044  min_lr: 0.000000  loss: 4.4743 (4.4383)  class_acc: 0.2083 (0.2093)  loss_scale: 65536.0000 (46276.6872)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.1728  max mem: 15572
Epoch: [10]  [2180/2809]  eta: 0:06:12  lr: 0.000044  min_lr: 0.000000  loss: 4.4523 (4.4380)  class_acc: 0.2083 (0.2095)  loss_scale: 65536.0000 (46364.9922)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1070  max mem: 15572
Epoch: [10]  [2190/2809]  eta: 0:06:06  lr: 0.000044  min_lr: 0.000000  loss: 4.4133 (4.4379)  class_acc: 0.2083 (0.2095)  loss_scale: 65536.0000 (46452.4911)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1082  max mem: 15572
Epoch: [10]  [2200/2809]  eta: 0:06:00  lr: 0.000044  min_lr: 0.000000  loss: 4.3793 (4.4375)  class_acc: 0.2500 (0.2096)  loss_scale: 65536.0000 (46539.1949)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1201  max mem: 15572
Epoch: [10]  [2210/2809]  eta: 0:05:54  lr: 0.000044  min_lr: 0.000000  loss: 4.3521 (4.4370)  class_acc: 0.2500 (0.2095)  loss_scale: 65536.0000 (46625.1144)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0999  max mem: 15572
Epoch: [10]  [2220/2809]  eta: 0:05:48  lr: 0.000044  min_lr: 0.000000  loss: 4.3981 (4.4373)  class_acc: 0.2083 (0.2096)  loss_scale: 65536.0000 (46710.2602)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1238  max mem: 15572
[2025-01-15 19:37:17,150] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:37:17,151] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:37:18,339] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30314
[2025-01-15 19:37:18,340] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:37:18,340] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [2230/2809]  eta: 0:05:42  lr: 0.000044  min_lr: 0.000000  loss: 4.4960 (4.4374)  class_acc: 0.1667 (0.2096)  loss_scale: 65536.0000 (46853.3931)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1334  max mem: 15572
Epoch: [10]  [2240/2809]  eta: 0:05:36  lr: 0.000044  min_lr: 0.000000  loss: 4.4264 (4.4371)  class_acc: 0.1667 (0.2094)  loss_scale: 65536.0000 (46936.7604)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1801  max mem: 15572
Epoch: [10]  [2250/2809]  eta: 0:05:30  lr: 0.000044  min_lr: 0.000000  loss: 4.3062 (4.4369)  class_acc: 0.1667 (0.2093)  loss_scale: 65536.0000 (47019.3869)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1338  max mem: 15572
Epoch: [10]  [2260/2809]  eta: 0:05:24  lr: 0.000044  min_lr: 0.000000  loss: 4.3290 (4.4368)  class_acc: 0.1250 (0.2091)  loss_scale: 65536.0000 (47101.2826)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0776  max mem: 15572
Epoch: [10]  [2270/2809]  eta: 0:05:18  lr: 0.000044  min_lr: 0.000000  loss: 4.3290 (4.4367)  class_acc: 0.1667 (0.2093)  loss_scale: 65536.0000 (47182.4571)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1719  max mem: 15572
Epoch: [10]  [2280/2809]  eta: 0:05:12  lr: 0.000044  min_lr: 0.000000  loss: 4.3054 (4.4366)  class_acc: 0.2083 (0.2094)  loss_scale: 65536.0000 (47262.9198)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1735  max mem: 15572
Epoch: [10]  [2290/2809]  eta: 0:05:06  lr: 0.000044  min_lr: 0.000000  loss: 4.3838 (4.4366)  class_acc: 0.2500 (0.2096)  loss_scale: 65536.0000 (47342.6801)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1582  max mem: 15572
Epoch: [10]  [2300/2809]  eta: 0:05:00  lr: 0.000044  min_lr: 0.000000  loss: 4.4702 (4.4364)  class_acc: 0.2500 (0.2097)  loss_scale: 65536.0000 (47421.7471)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1584  max mem: 15572
Epoch: [10]  [2310/2809]  eta: 0:04:55  lr: 0.000044  min_lr: 0.000000  loss: 4.3786 (4.4367)  class_acc: 0.2500 (0.2097)  loss_scale: 65536.0000 (47500.1298)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1487  max mem: 15572
Epoch: [10]  [2320/2809]  eta: 0:04:49  lr: 0.000044  min_lr: 0.000000  loss: 4.4826 (4.4369)  class_acc: 0.2083 (0.2097)  loss_scale: 65536.0000 (47577.8371)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1317  max mem: 15572
Epoch: [10]  [2330/2809]  eta: 0:04:43  lr: 0.000044  min_lr: 0.000000  loss: 4.4826 (4.4368)  class_acc: 0.2083 (0.2098)  loss_scale: 65536.0000 (47654.8777)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1311  max mem: 15572
Epoch: [10]  [2340/2809]  eta: 0:04:37  lr: 0.000044  min_lr: 0.000000  loss: 4.4580 (4.4373)  class_acc: 0.2500 (0.2099)  loss_scale: 65536.0000 (47731.2601)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1618  max mem: 15572
Epoch: [10]  [2350/2809]  eta: 0:04:31  lr: 0.000044  min_lr: 0.000000  loss: 4.4410 (4.4373)  class_acc: 0.2083 (0.2098)  loss_scale: 65536.0000 (47806.9928)  weight_decay: 0.0500 (0.0500)  time: 0.6560  data: 0.2095  max mem: 15572
[2025-01-15 19:38:36,626] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:38:36,626] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:38:37,054] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30444
[2025-01-15 19:38:37,054] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:38:37,054] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [2360/2809]  eta: 0:04:25  lr: 0.000044  min_lr: 0.000000  loss: 4.4410 (4.4372)  class_acc: 0.2083 (0.2100)  loss_scale: 65536.0000 (47909.8416)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1358  max mem: 15572
Epoch: [10]  [2370/2809]  eta: 0:04:19  lr: 0.000044  min_lr: 0.000000  loss: 4.4833 (4.4374)  class_acc: 0.2500 (0.2102)  loss_scale: 65536.0000 (47984.1822)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0510  max mem: 15572
Epoch: [10]  [2380/2809]  eta: 0:04:13  lr: 0.000044  min_lr: 0.000000  loss: 4.4758 (4.4376)  class_acc: 0.2083 (0.2103)  loss_scale: 65536.0000 (48057.8984)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1159  max mem: 15572
Epoch: [10]  [2390/2809]  eta: 0:04:07  lr: 0.000044  min_lr: 0.000000  loss: 4.4618 (4.4378)  class_acc: 0.1667 (0.2102)  loss_scale: 65536.0000 (48130.9979)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1543  max mem: 15572
Epoch: [10]  [2400/2809]  eta: 0:04:01  lr: 0.000044  min_lr: 0.000000  loss: 4.4618 (4.4381)  class_acc: 0.2083 (0.2103)  loss_scale: 65536.0000 (48203.4885)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1638  max mem: 15572
Epoch: [10]  [2410/2809]  eta: 0:03:55  lr: 0.000044  min_lr: 0.000000  loss: 4.4865 (4.4382)  class_acc: 0.2083 (0.2104)  loss_scale: 65536.0000 (48275.3779)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1420  max mem: 15572
Epoch: [10]  [2420/2809]  eta: 0:03:49  lr: 0.000044  min_lr: 0.000000  loss: 4.4865 (4.4382)  class_acc: 0.2083 (0.2106)  loss_scale: 65536.0000 (48346.6733)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0884  max mem: 15572
Epoch: [10]  [2430/2809]  eta: 0:03:44  lr: 0.000044  min_lr: 0.000000  loss: 4.4124 (4.4376)  class_acc: 0.2500 (0.2108)  loss_scale: 65536.0000 (48417.3821)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1179  max mem: 15572
Epoch: [10]  [2440/2809]  eta: 0:03:38  lr: 0.000044  min_lr: 0.000000  loss: 4.2843 (4.4370)  class_acc: 0.2500 (0.2108)  loss_scale: 65536.0000 (48487.5117)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1147  max mem: 15572
Epoch: [10]  [2450/2809]  eta: 0:03:32  lr: 0.000044  min_lr: 0.000000  loss: 4.2843 (4.4369)  class_acc: 0.2083 (0.2108)  loss_scale: 65536.0000 (48557.0690)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1041  max mem: 15572
[2025-01-15 19:39:34,358] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30543
[2025-01-15 19:39:34,358] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:39:34,358] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [2460/2809]  eta: 0:03:26  lr: 0.000044  min_lr: 0.000000  loss: 4.3753 (4.4367)  class_acc: 0.2500 (0.2110)  loss_scale: 65536.0000 (48519.5416)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1302  max mem: 15572
Epoch: [10]  [2470/2809]  eta: 0:03:20  lr: 0.000044  min_lr: 0.000000  loss: 4.4345 (4.4368)  class_acc: 0.2500 (0.2111)  loss_scale: 32768.0000 (48455.7960)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1217  max mem: 15572
Epoch: [10]  [2480/2809]  eta: 0:03:14  lr: 0.000044  min_lr: 0.000000  loss: 4.4486 (4.4370)  class_acc: 0.2500 (0.2111)  loss_scale: 32768.0000 (48392.5643)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1706  max mem: 15572
Epoch: [10]  [2490/2809]  eta: 0:03:08  lr: 0.000044  min_lr: 0.000000  loss: 4.3784 (4.4369)  class_acc: 0.2500 (0.2111)  loss_scale: 32768.0000 (48329.8402)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1441  max mem: 15572
Epoch: [10]  [2500/2809]  eta: 0:03:02  lr: 0.000044  min_lr: 0.000000  loss: 4.4414 (4.4371)  class_acc: 0.2083 (0.2110)  loss_scale: 32768.0000 (48267.6178)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1454  max mem: 15572
Epoch: [10]  [2510/2809]  eta: 0:02:56  lr: 0.000044  min_lr: 0.000000  loss: 4.5269 (4.4375)  class_acc: 0.1667 (0.2108)  loss_scale: 32768.0000 (48205.8909)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1729  max mem: 15572
Epoch: [10]  [2520/2809]  eta: 0:02:50  lr: 0.000044  min_lr: 0.000000  loss: 4.5005 (4.4374)  class_acc: 0.1667 (0.2108)  loss_scale: 32768.0000 (48144.6537)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1300  max mem: 15572
Epoch: [10]  [2530/2809]  eta: 0:02:44  lr: 0.000044  min_lr: 0.000000  loss: 4.4501 (4.4377)  class_acc: 0.2083 (0.2107)  loss_scale: 32768.0000 (48083.9004)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1105  max mem: 15572
Epoch: [10]  [2540/2809]  eta: 0:02:38  lr: 0.000044  min_lr: 0.000000  loss: 4.4348 (4.4377)  class_acc: 0.2083 (0.2109)  loss_scale: 32768.0000 (48023.6253)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0884  max mem: 15572
Epoch: [10]  [2550/2809]  eta: 0:02:33  lr: 0.000044  min_lr: 0.000000  loss: 4.5154 (4.4378)  class_acc: 0.2083 (0.2108)  loss_scale: 32768.0000 (47963.8228)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0857  max mem: 15572
Epoch: [10]  [2560/2809]  eta: 0:02:27  lr: 0.000044  min_lr: 0.000000  loss: 4.3955 (4.4374)  class_acc: 0.1667 (0.2107)  loss_scale: 32768.0000 (47904.4873)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0925  max mem: 15572
Epoch: [10]  [2570/2809]  eta: 0:02:21  lr: 0.000044  min_lr: 0.000000  loss: 4.3955 (4.4377)  class_acc: 0.2083 (0.2108)  loss_scale: 32768.0000 (47845.6134)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1083  max mem: 15572
Epoch: [10]  [2580/2809]  eta: 0:02:15  lr: 0.000044  min_lr: 0.000000  loss: 4.3879 (4.4374)  class_acc: 0.1667 (0.2107)  loss_scale: 32768.0000 (47787.1957)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1069  max mem: 15572
[2025-01-15 19:40:49,466] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:40:49,467] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [2590/2809]  eta: 0:02:09  lr: 0.000044  min_lr: 0.000000  loss: 4.2436 (4.4367)  class_acc: 0.1667 (0.2108)  loss_scale: 32768.0000 (47843.0506)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0825  max mem: 15572
Epoch: [10]  [2600/2809]  eta: 0:02:03  lr: 0.000044  min_lr: 0.000000  loss: 4.2146 (4.4364)  class_acc: 0.2083 (0.2109)  loss_scale: 65536.0000 (47911.0742)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1102  max mem: 15572
Epoch: [10]  [2610/2809]  eta: 0:01:57  lr: 0.000044  min_lr: 0.000000  loss: 4.3749 (4.4361)  class_acc: 0.2500 (0.2110)  loss_scale: 65536.0000 (47978.5768)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1291  max mem: 15572
Epoch: [10]  [2620/2809]  eta: 0:01:51  lr: 0.000044  min_lr: 0.000000  loss: 4.4553 (4.4368)  class_acc: 0.2083 (0.2109)  loss_scale: 65536.0000 (48045.5643)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0828  max mem: 15572
Epoch: [10]  [2630/2809]  eta: 0:01:45  lr: 0.000044  min_lr: 0.000000  loss: 4.5111 (4.4369)  class_acc: 0.1667 (0.2109)  loss_scale: 65536.0000 (48112.0426)  weight_decay: 0.0500 (0.0500)  time: 0.4627  data: 0.0221  max mem: 15572
Epoch: [10]  [2640/2809]  eta: 0:01:39  lr: 0.000044  min_lr: 0.000000  loss: 4.4721 (4.4370)  class_acc: 0.2083 (0.2108)  loss_scale: 65536.0000 (48178.0174)  weight_decay: 0.0500 (0.0500)  time: 0.4238  data: 0.0005  max mem: 15572
Epoch: [10]  [2650/2809]  eta: 0:01:33  lr: 0.000044  min_lr: 0.000000  loss: 4.4029 (4.4367)  class_acc: 0.2083 (0.2108)  loss_scale: 65536.0000 (48243.4945)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0405  max mem: 15572
Epoch: [10]  [2660/2809]  eta: 0:01:27  lr: 0.000044  min_lr: 0.000000  loss: 4.3740 (4.4367)  class_acc: 0.2083 (0.2109)  loss_scale: 65536.0000 (48308.4795)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0943  max mem: 15572
Epoch: [10]  [2670/2809]  eta: 0:01:22  lr: 0.000044  min_lr: 0.000000  loss: 4.5025 (4.4369)  class_acc: 0.2083 (0.2108)  loss_scale: 65536.0000 (48372.9779)  weight_decay: 0.0500 (0.0500)  time: 0.7183  data: 0.2029  max mem: 15572
Epoch: [10]  [2680/2809]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000000  loss: 4.5025 (4.4369)  class_acc: 0.1667 (0.2106)  loss_scale: 65536.0000 (48436.9952)  weight_decay: 0.0500 (0.0500)  time: 0.7179  data: 0.2350  max mem: 15572
[2025-01-15 19:41:51,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30779
[2025-01-15 19:41:51,229] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:41:51,229] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [2690/2809]  eta: 0:01:10  lr: 0.000044  min_lr: 0.000000  loss: 4.3964 (4.4369)  class_acc: 0.2083 (0.2108)  loss_scale: 65536.0000 (48476.1828)  weight_decay: 0.0500 (0.0500)  time: 0.6866  data: 0.1900  max mem: 15572
Epoch: [10]  [2700/2809]  eta: 0:01:04  lr: 0.000044  min_lr: 0.000000  loss: 4.4132 (4.4373)  class_acc: 0.2083 (0.2107)  loss_scale: 32768.0000 (48418.0259)  weight_decay: 0.0500 (0.0500)  time: 0.7268  data: 0.2113  max mem: 15572
Epoch: [10]  [2710/2809]  eta: 0:00:58  lr: 0.000044  min_lr: 0.000000  loss: 4.5577 (4.4379)  class_acc: 0.2083 (0.2106)  loss_scale: 32768.0000 (48360.2980)  weight_decay: 0.0500 (0.0500)  time: 0.6906  data: 0.1812  max mem: 15572
Epoch: [10]  [2720/2809]  eta: 0:00:52  lr: 0.000044  min_lr: 0.000000  loss: 4.4841 (4.4378)  class_acc: 0.2083 (0.2107)  loss_scale: 32768.0000 (48302.9945)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.1593  max mem: 15572
Epoch: [10]  [2730/2809]  eta: 0:00:46  lr: 0.000044  min_lr: 0.000000  loss: 4.4594 (4.4382)  class_acc: 0.2083 (0.2106)  loss_scale: 32768.0000 (48246.1106)  weight_decay: 0.0500 (0.0500)  time: 0.7023  data: 0.2224  max mem: 15572
Epoch: [10]  [2740/2809]  eta: 0:00:40  lr: 0.000044  min_lr: 0.000000  loss: 4.5494 (4.4382)  class_acc: 0.1667 (0.2105)  loss_scale: 32768.0000 (48189.6417)  weight_decay: 0.0500 (0.0500)  time: 0.7095  data: 0.2432  max mem: 15572
Epoch: [10]  [2750/2809]  eta: 0:00:34  lr: 0.000044  min_lr: 0.000000  loss: 4.4037 (4.4378)  class_acc: 0.1667 (0.2104)  loss_scale: 32768.0000 (48133.5834)  weight_decay: 0.0500 (0.0500)  time: 0.6876  data: 0.2189  max mem: 15572
Epoch: [10]  [2760/2809]  eta: 0:00:29  lr: 0.000044  min_lr: 0.000000  loss: 4.4037 (4.4380)  class_acc: 0.1667 (0.2104)  loss_scale: 32768.0000 (48077.9312)  weight_decay: 0.0500 (0.0500)  time: 0.7287  data: 0.2675  max mem: 15572
Epoch: [10]  [2770/2809]  eta: 0:00:23  lr: 0.000044  min_lr: 0.000000  loss: 4.5649 (4.4387)  class_acc: 0.1667 (0.2103)  loss_scale: 32768.0000 (48022.6806)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1552  max mem: 15572
Epoch: [10]  [2780/2809]  eta: 0:00:17  lr: 0.000044  min_lr: 0.000000  loss: 4.5294 (4.4390)  class_acc: 0.2083 (0.2103)  loss_scale: 32768.0000 (47967.8274)  weight_decay: 0.0500 (0.0500)  time: 0.3955  data: 0.0004  max mem: 15572
Epoch: [10]  [2790/2809]  eta: 0:00:11  lr: 0.000044  min_lr: 0.000000  loss: 4.3846 (4.4389)  class_acc: 0.2083 (0.2103)  loss_scale: 32768.0000 (47913.3673)  weight_decay: 0.0500 (0.0500)  time: 0.4334  data: 0.0007  max mem: 15572
Epoch: [10]  [2800/2809]  eta: 0:00:05  lr: 0.000044  min_lr: 0.000000  loss: 4.4136 (4.4391)  class_acc: 0.1667 (0.2102)  loss_scale: 32768.0000 (47859.2960)  weight_decay: 0.0500 (0.0500)  time: 0.4468  data: 0.0007  max mem: 15572
Epoch: [10]  [2808/2809]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000000  loss: 4.4857 (4.4390)  class_acc: 0.1667 (0.2103)  loss_scale: 32768.0000 (47816.3161)  weight_decay: 0.0500 (0.0500)  time: 0.4235  data: 0.0005  max mem: 15572
Epoch: [10] Total time: 0:27:40 (0.5912 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000000  loss: 4.4857 (4.4390)  class_acc: 0.1667 (0.2103)  loss_scale: 32768.0000 (47816.3161)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:22:19  loss: 1.4166 (1.4166)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 4.9241  data: 4.7405  max mem: 15572
Val:  [ 10/272]  eta: 0:03:40  loss: 3.6036 (3.3638)  acc1: 16.6667 (23.7374)  acc5: 50.0000 (47.9798)  time: 0.8408  data: 0.6398  max mem: 15572
Val:  [ 20/272]  eta: 0:02:24  loss: 3.3211 (3.3272)  acc1: 22.2222 (27.7778)  acc5: 55.5556 (55.8201)  time: 0.3556  data: 0.1606  max mem: 15572
Val:  [ 30/272]  eta: 0:01:53  loss: 3.2535 (3.3233)  acc1: 27.7778 (29.3907)  acc5: 61.1111 (57.8853)  time: 0.2659  data: 0.0825  max mem: 15572
Val:  [ 40/272]  eta: 0:01:40  loss: 3.0456 (3.2614)  acc1: 22.2222 (28.0488)  acc5: 66.6667 (60.5691)  time: 0.2846  data: 0.1018  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 3.0226 (3.1933)  acc1: 27.7778 (30.2832)  acc5: 72.2222 (62.6362)  time: 0.3258  data: 0.1343  max mem: 15572
Val:  [ 60/272]  eta: 0:01:26  loss: 2.4311 (3.0912)  acc1: 55.5556 (34.6995)  acc5: 77.7778 (64.5719)  time: 0.3614  data: 0.1709  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 2.5364 (3.0429)  acc1: 50.0000 (35.2113)  acc5: 83.3333 (66.5884)  time: 0.3149  data: 0.1223  max mem: 15572
Val:  [ 80/272]  eta: 0:01:12  loss: 2.8720 (3.0406)  acc1: 33.3333 (35.3224)  acc5: 77.7778 (66.4609)  time: 0.2754  data: 0.0854  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 3.3000 (3.0775)  acc1: 27.7778 (34.1880)  acc5: 61.1111 (65.6288)  time: 0.3190  data: 0.1345  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 3.3000 (3.1100)  acc1: 27.7778 (34.1584)  acc5: 61.1111 (65.2365)  time: 0.3380  data: 0.1508  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 3.3349 (3.1625)  acc1: 11.1111 (32.0821)  acc5: 50.0000 (63.4134)  time: 0.3498  data: 0.1494  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 3.4867 (3.1885)  acc1: 11.1111 (31.4050)  acc5: 50.0000 (62.8558)  time: 0.3562  data: 0.1507  max mem: 15572
Val:  [130/272]  eta: 0:00:52  loss: 3.3247 (3.1668)  acc1: 27.7778 (32.2731)  acc5: 66.6667 (63.3164)  time: 0.3703  data: 0.1792  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 2.9721 (3.1678)  acc1: 38.8889 (32.7029)  acc5: 61.1111 (63.2388)  time: 0.2994  data: 0.1040  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 3.1725 (3.1637)  acc1: 27.7778 (32.0824)  acc5: 61.1111 (63.1714)  time: 0.2922  data: 0.0931  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.0156 (3.1518)  acc1: 27.7778 (32.3326)  acc5: 72.2222 (64.0442)  time: 0.3492  data: 0.1575  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.0323 (3.1658)  acc1: 27.7778 (32.0013)  acc5: 72.2222 (63.4178)  time: 0.3152  data: 0.1141  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 3.0011 (3.1504)  acc1: 27.7778 (32.4432)  acc5: 61.1111 (63.9349)  time: 0.3468  data: 0.1503  max mem: 15572
Val:  [190/272]  eta: 0:00:29  loss: 3.0624 (3.1780)  acc1: 16.6667 (31.2100)  acc5: 55.5556 (62.5364)  time: 0.3665  data: 0.1844  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.1540 (3.1814)  acc1: 16.6667 (30.8181)  acc5: 55.5556 (62.6313)  time: 0.3119  data: 0.1202  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.9543 (3.1863)  acc1: 27.7778 (31.1480)  acc5: 72.2222 (62.7172)  time: 0.3355  data: 0.1403  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 2.9852 (3.1799)  acc1: 38.8889 (31.2971)  acc5: 66.6667 (63.0468)  time: 0.3568  data: 0.1718  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.6955 (3.1629)  acc1: 50.0000 (32.5878)  acc5: 66.6667 (63.7326)  time: 0.3376  data: 0.1512  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 2.6373 (3.1466)  acc1: 55.5556 (33.4255)  acc5: 83.3333 (64.3615)  time: 0.2960  data: 0.1088  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.8253 (3.1597)  acc1: 33.3333 (33.0235)  acc5: 72.2222 (63.9000)  time: 0.3209  data: 0.1317  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 2.5455 (3.1170)  acc1: 66.6667 (34.9298)  acc5: 77.7778 (64.9638)  time: 0.2901  data: 0.1095  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.6103 (3.1186)  acc1: 61.1111 (34.6863)  acc5: 77.7778 (64.8421)  time: 0.1745  data: 0.0148  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.6103 (3.1224)  acc1: 38.8889 (34.6713)  acc5: 77.7778 (64.8167)  time: 0.1654  data: 0.0148  max mem: 15572
Val: Total time: 0:01:31 (0.3360 s / it)
* Acc@1 34.671 Acc@5 64.817 loss 3.122
Accuracy of the network on the 4883 val videos: 34.7%
[2025-01-15 19:44:34,805] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 19:44:34,808] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 19:44:34,808] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 19:44:37,714] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 19:44:37,715] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 34.67%
Epoch: [11]  [   0/2809]  eta: 5:31:39  lr: 0.000044  min_lr: 0.000000  loss: 4.3254 (4.3254)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.0843  data: 6.6596  max mem: 15572
[2025-01-15 19:44:51,048] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:44:51,048] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [  10/2809]  eta: 0:58:33  lr: 0.000044  min_lr: 0.000000  loss: 4.4925 (4.5093)  class_acc: 0.2083 (0.1894)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 1.2552  data: 0.7797  max mem: 15572
Epoch: [11]  [  20/2809]  eta: 0:40:37  lr: 0.000044  min_lr: 0.000000  loss: 4.4820 (4.4863)  class_acc: 0.2083 (0.1925)  loss_scale: 65536.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1005  max mem: 15572
Epoch: [11]  [  30/2809]  eta: 0:35:45  lr: 0.000044  min_lr: 0.000000  loss: 4.3401 (4.4356)  class_acc: 0.2083 (0.2016)  loss_scale: 65536.0000 (56022.7097)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0369  max mem: 15572
Epoch: [11]  [  40/2809]  eta: 0:32:14  lr: 0.000044  min_lr: 0.000000  loss: 4.4687 (4.4252)  class_acc: 0.1667 (0.1961)  loss_scale: 65536.0000 (58343.0244)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0374  max mem: 15572
Epoch: [11]  [  50/2809]  eta: 0:31:41  lr: 0.000044  min_lr: 0.000000  loss: 4.4687 (4.4143)  class_acc: 0.2083 (0.2042)  loss_scale: 65536.0000 (59753.4118)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1004  max mem: 15572
Epoch: [11]  [  60/2809]  eta: 0:30:43  lr: 0.000044  min_lr: 0.000000  loss: 4.3951 (4.4048)  class_acc: 0.2500 (0.2131)  loss_scale: 65536.0000 (60701.3770)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1618  max mem: 15572
Epoch: [11]  [  70/2809]  eta: 0:30:36  lr: 0.000044  min_lr: 0.000000  loss: 4.3891 (4.4035)  class_acc: 0.2083 (0.2107)  loss_scale: 65536.0000 (61382.3099)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1568  max mem: 15572
Epoch: [11]  [  80/2809]  eta: 0:29:39  lr: 0.000044  min_lr: 0.000000  loss: 4.3891 (4.4080)  class_acc: 0.2083 (0.2104)  loss_scale: 65536.0000 (61895.1111)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1310  max mem: 15572
Epoch: [11]  [  90/2809]  eta: 0:29:13  lr: 0.000044  min_lr: 0.000000  loss: 4.4137 (4.3963)  class_acc: 0.2083 (0.2161)  loss_scale: 65536.0000 (62295.2088)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1171  max mem: 15572
[2025-01-15 19:45:41,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=197, lr=[4.216826230280262e-07, 4.216826230280262e-07, 6.024037471828946e-07, 6.024037471828946e-07, 8.605767816898495e-07, 8.605767816898495e-07, 1.2293954024140709e-06, 1.2293954024140709e-06, 1.7562791463058155e-06, 1.7562791463058155e-06, 2.508970209008308e-06, 2.508970209008308e-06, 3.5842431557261544e-06, 3.5842431557261544e-06, 5.120347365323078e-06, 5.120347365323078e-06, 7.31478195046154e-06, 7.31478195046154e-06, 1.0449688500659345e-05, 1.0449688500659345e-05, 1.492812642951335e-05, 1.492812642951335e-05, 2.1325894899304786e-05, 2.1325894899304786e-05, 3.046556414186398e-05, 3.046556414186398e-05, 4.352223448837712e-05, 4.352223448837712e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:45:41,767] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=27.4959622116817, CurrSamplesPerSec=31.574817257240078, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [ 100/2809]  eta: 0:28:36  lr: 0.000044  min_lr: 0.000000  loss: 4.4455 (4.4081)  class_acc: 0.2917 (0.2170)  loss_scale: 65536.0000 (62616.0792)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1169  max mem: 15572
[2025-01-15 19:45:43,543] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31000
[2025-01-15 19:45:43,543] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:45:43,543] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 110/2809]  eta: 0:28:15  lr: 0.000044  min_lr: 0.000000  loss: 4.4002 (4.4018)  class_acc: 0.2500 (0.2226)  loss_scale: 32768.0000 (59927.0631)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1081  max mem: 15572
Epoch: [11]  [ 120/2809]  eta: 0:27:59  lr: 0.000044  min_lr: 0.000000  loss: 4.3008 (4.3976)  class_acc: 0.2500 (0.2259)  loss_scale: 32768.0000 (57682.5124)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1478  max mem: 15572
Epoch: [11]  [ 130/2809]  eta: 0:27:52  lr: 0.000044  min_lr: 0.000000  loss: 4.3748 (4.4101)  class_acc: 0.2083 (0.2239)  loss_scale: 32768.0000 (55780.6412)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1725  max mem: 15572
Epoch: [11]  [ 140/2809]  eta: 0:27:39  lr: 0.000044  min_lr: 0.000000  loss: 4.4550 (4.4136)  class_acc: 0.2083 (0.2267)  loss_scale: 32768.0000 (54148.5390)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1423  max mem: 15572
Epoch: [11]  [ 150/2809]  eta: 0:27:35  lr: 0.000044  min_lr: 0.000000  loss: 4.4298 (4.4154)  class_acc: 0.1667 (0.2235)  loss_scale: 32768.0000 (52732.6093)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1162  max mem: 15572
Epoch: [11]  [ 160/2809]  eta: 0:27:06  lr: 0.000043  min_lr: 0.000000  loss: 4.4904 (4.4198)  class_acc: 0.1667 (0.2197)  loss_scale: 32768.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0666  max mem: 15572
Epoch: [11]  [ 170/2809]  eta: 0:27:08  lr: 0.000043  min_lr: 0.000000  loss: 4.4079 (4.4155)  class_acc: 0.2083 (0.2220)  loss_scale: 32768.0000 (50397.5673)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0946  max mem: 15572
Epoch: [11]  [ 180/2809]  eta: 0:26:59  lr: 0.000043  min_lr: 0.000000  loss: 4.3967 (4.4129)  class_acc: 0.2083 (0.2201)  loss_scale: 32768.0000 (49423.5580)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.1637  max mem: 15572
Epoch: [11]  [ 190/2809]  eta: 0:26:46  lr: 0.000043  min_lr: 0.000000  loss: 4.4117 (4.4117)  class_acc: 0.1667 (0.2184)  loss_scale: 32768.0000 (48551.5393)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1206  max mem: 15572
Epoch: [11]  [ 200/2809]  eta: 0:26:46  lr: 0.000043  min_lr: 0.000000  loss: 4.4367 (4.4190)  class_acc: 0.1667 (0.2162)  loss_scale: 32768.0000 (47766.2886)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1474  max mem: 15572
Epoch: [11]  [ 210/2809]  eta: 0:26:27  lr: 0.000043  min_lr: 0.000000  loss: 4.4447 (4.4213)  class_acc: 0.1667 (0.2160)  loss_scale: 32768.0000 (47055.4692)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1298  max mem: 15572
Epoch: [11]  [ 220/2809]  eta: 0:26:11  lr: 0.000043  min_lr: 0.000000  loss: 4.4357 (4.4243)  class_acc: 0.2083 (0.2153)  loss_scale: 32768.0000 (46408.9774)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0684  max mem: 15572
[2025-01-15 19:46:57,635] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:46:57,636] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 230/2809]  eta: 0:26:01  lr: 0.000043  min_lr: 0.000000  loss: 4.5529 (4.4300)  class_acc: 0.1667 (0.2145)  loss_scale: 32768.0000 (45960.3117)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0858  max mem: 15572
Epoch: [11]  [ 240/2809]  eta: 0:26:01  lr: 0.000043  min_lr: 0.000000  loss: 4.4787 (4.4311)  class_acc: 0.1667 (0.2118)  loss_scale: 65536.0000 (46772.5809)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1318  max mem: 15572
Epoch: [11]  [ 250/2809]  eta: 0:25:55  lr: 0.000043  min_lr: 0.000000  loss: 4.4284 (4.4287)  class_acc: 0.2083 (0.2140)  loss_scale: 65536.0000 (47520.1275)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1423  max mem: 15572
Epoch: [11]  [ 260/2809]  eta: 0:25:39  lr: 0.000043  min_lr: 0.000000  loss: 4.3654 (4.4281)  class_acc: 0.2083 (0.2126)  loss_scale: 65536.0000 (48210.3908)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0850  max mem: 15572
Epoch: [11]  [ 270/2809]  eta: 0:25:33  lr: 0.000043  min_lr: 0.000000  loss: 4.3654 (4.4262)  class_acc: 0.1667 (0.2122)  loss_scale: 65536.0000 (48849.7122)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1049  max mem: 15572
Epoch: [11]  [ 280/2809]  eta: 0:25:28  lr: 0.000043  min_lr: 0.000000  loss: 4.3960 (4.4233)  class_acc: 0.2083 (0.2123)  loss_scale: 65536.0000 (49443.5302)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1765  max mem: 15572
Epoch: [11]  [ 290/2809]  eta: 0:25:22  lr: 0.000043  min_lr: 0.000000  loss: 4.3837 (4.4204)  class_acc: 0.1667 (0.2118)  loss_scale: 65536.0000 (49996.5361)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1710  max mem: 15572
Epoch: [11]  [ 300/2809]  eta: 0:25:23  lr: 0.000043  min_lr: 0.000000  loss: 4.4043 (4.4215)  class_acc: 0.1667 (0.2114)  loss_scale: 65536.0000 (50512.7973)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.2003  max mem: 15572
Epoch: [11]  [ 310/2809]  eta: 0:25:10  lr: 0.000043  min_lr: 0.000000  loss: 4.4839 (4.4197)  class_acc: 0.1667 (0.2122)  loss_scale: 65536.0000 (50995.8585)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1534  max mem: 15572
[2025-01-15 19:47:47,038] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31212
[2025-01-15 19:47:47,038] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:47:47,039] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 320/2809]  eta: 0:25:08  lr: 0.000043  min_lr: 0.000000  loss: 4.3118 (4.4168)  class_acc: 0.2083 (0.2137)  loss_scale: 65536.0000 (50632.1745)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1356  max mem: 15572
Epoch: [11]  [ 330/2809]  eta: 0:25:00  lr: 0.000043  min_lr: 0.000000  loss: 4.3534 (4.4170)  class_acc: 0.2083 (0.2139)  loss_scale: 32768.0000 (50092.4713)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1554  max mem: 15572
Epoch: [11]  [ 340/2809]  eta: 0:25:04  lr: 0.000043  min_lr: 0.000000  loss: 4.3551 (4.4167)  class_acc: 0.2083 (0.2155)  loss_scale: 32768.0000 (49584.4223)  weight_decay: 0.0500 (0.0500)  time: 0.6573  data: 0.1732  max mem: 15572
Epoch: [11]  [ 350/2809]  eta: 0:24:49  lr: 0.000043  min_lr: 0.000000  loss: 4.4819 (4.4189)  class_acc: 0.2500 (0.2162)  loss_scale: 32768.0000 (49105.3219)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1348  max mem: 15572
Epoch: [11]  [ 360/2809]  eta: 0:24:45  lr: 0.000043  min_lr: 0.000000  loss: 4.4819 (4.4210)  class_acc: 0.2083 (0.2158)  loss_scale: 32768.0000 (48652.7645)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0800  max mem: 15572
Epoch: [11]  [ 370/2809]  eta: 0:24:39  lr: 0.000043  min_lr: 0.000000  loss: 4.3928 (4.4174)  class_acc: 0.2083 (0.2169)  loss_scale: 32768.0000 (48224.6038)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1393  max mem: 15572
Epoch: [11]  [ 380/2809]  eta: 0:24:34  lr: 0.000043  min_lr: 0.000000  loss: 4.4050 (4.4218)  class_acc: 0.2083 (0.2165)  loss_scale: 32768.0000 (47818.9186)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1544  max mem: 15572
Epoch: [11]  [ 390/2809]  eta: 0:24:20  lr: 0.000043  min_lr: 0.000000  loss: 4.5460 (4.4211)  class_acc: 0.2083 (0.2168)  loss_scale: 32768.0000 (47433.9847)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1041  max mem: 15572
Epoch: [11]  [ 400/2809]  eta: 0:24:11  lr: 0.000043  min_lr: 0.000000  loss: 4.3748 (4.4196)  class_acc: 0.2500 (0.2175)  loss_scale: 32768.0000 (47068.2494)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0727  max mem: 15572
Epoch: [11]  [ 410/2809]  eta: 0:24:09  lr: 0.000043  min_lr: 0.000000  loss: 4.3748 (4.4199)  class_acc: 0.2500 (0.2171)  loss_scale: 32768.0000 (46720.3114)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1642  max mem: 15572
Epoch: [11]  [ 420/2809]  eta: 0:24:03  lr: 0.000043  min_lr: 0.000000  loss: 4.3535 (4.4169)  class_acc: 0.2083 (0.2162)  loss_scale: 32768.0000 (46388.9026)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.1906  max mem: 15572
Epoch: [11]  [ 430/2809]  eta: 0:23:52  lr: 0.000043  min_lr: 0.000000  loss: 4.4220 (4.4219)  class_acc: 0.1667 (0.2168)  loss_scale: 32768.0000 (46072.8724)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1103  max mem: 15572
Epoch: [11]  [ 440/2809]  eta: 0:23:45  lr: 0.000043  min_lr: 0.000000  loss: 4.4878 (4.4226)  class_acc: 0.1667 (0.2157)  loss_scale: 32768.0000 (45771.1746)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0786  max mem: 15572
[2025-01-15 19:49:04,107] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:49:04,107] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 450/2809]  eta: 0:23:37  lr: 0.000043  min_lr: 0.000000  loss: 4.4571 (4.4208)  class_acc: 0.2083 (0.2164)  loss_scale: 32768.0000 (46136.7627)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0937  max mem: 15572
Epoch: [11]  [ 460/2809]  eta: 0:23:31  lr: 0.000043  min_lr: 0.000000  loss: 4.4355 (4.4246)  class_acc: 0.2500 (0.2171)  loss_scale: 65536.0000 (46557.5705)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1168  max mem: 15572
Epoch: [11]  [ 470/2809]  eta: 0:23:23  lr: 0.000043  min_lr: 0.000000  loss: 4.5717 (4.4251)  class_acc: 0.2083 (0.2174)  loss_scale: 65536.0000 (46960.5096)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1171  max mem: 15572
[2025-01-15 19:49:23,874] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31376
[2025-01-15 19:49:23,874] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:49:23,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 480/2809]  eta: 0:23:11  lr: 0.000043  min_lr: 0.000000  loss: 4.4644 (4.4244)  class_acc: 0.2083 (0.2180)  loss_scale: 65536.0000 (47074.1954)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0463  max mem: 15572
Epoch: [11]  [ 490/2809]  eta: 0:22:59  lr: 0.000043  min_lr: 0.000000  loss: 4.5071 (4.4253)  class_acc: 0.2083 (0.2174)  loss_scale: 32768.0000 (46782.8269)  weight_decay: 0.0500 (0.0500)  time: 0.4661  data: 0.0008  max mem: 15572
Epoch: [11]  [ 500/2809]  eta: 0:22:48  lr: 0.000043  min_lr: 0.000000  loss: 4.4067 (4.4247)  class_acc: 0.2083 (0.2177)  loss_scale: 32768.0000 (46503.0898)  weight_decay: 0.0500 (0.0500)  time: 0.4790  data: 0.0133  max mem: 15572
Epoch: [11]  [ 510/2809]  eta: 0:22:48  lr: 0.000043  min_lr: 0.000000  loss: 4.3989 (4.4243)  class_acc: 0.2083 (0.2180)  loss_scale: 32768.0000 (46234.3014)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1349  max mem: 15572
Epoch: [11]  [ 520/2809]  eta: 0:22:40  lr: 0.000043  min_lr: 0.000000  loss: 4.3788 (4.4231)  class_acc: 0.2083 (0.2183)  loss_scale: 32768.0000 (45975.8311)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.1663  max mem: 15572
Epoch: [11]  [ 530/2809]  eta: 0:22:32  lr: 0.000043  min_lr: 0.000000  loss: 4.4492 (4.4238)  class_acc: 0.2083 (0.2188)  loss_scale: 32768.0000 (45727.0960)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1021  max mem: 15572
Epoch: [11]  [ 540/2809]  eta: 0:22:24  lr: 0.000043  min_lr: 0.000000  loss: 4.5106 (4.4260)  class_acc: 0.2083 (0.2183)  loss_scale: 32768.0000 (45487.5564)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0864  max mem: 15572
Epoch: [11]  [ 550/2809]  eta: 0:22:14  lr: 0.000043  min_lr: 0.000000  loss: 4.4522 (4.4250)  class_acc: 0.2083 (0.2190)  loss_scale: 32768.0000 (45256.7114)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0615  max mem: 15572
Epoch: [11]  [ 560/2809]  eta: 0:22:04  lr: 0.000043  min_lr: 0.000000  loss: 4.4046 (4.4249)  class_acc: 0.2500 (0.2195)  loss_scale: 32768.0000 (45034.0963)  weight_decay: 0.0500 (0.0500)  time: 0.4985  data: 0.0460  max mem: 15572
Epoch: [11]  [ 570/2809]  eta: 0:21:56  lr: 0.000043  min_lr: 0.000000  loss: 4.4364 (4.4253)  class_acc: 0.1667 (0.2181)  loss_scale: 32768.0000 (44819.2785)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0493  max mem: 15572
Epoch: [11]  [ 580/2809]  eta: 0:21:50  lr: 0.000043  min_lr: 0.000000  loss: 4.2371 (4.4230)  class_acc: 0.1667 (0.2187)  loss_scale: 32768.0000 (44611.8554)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1136  max mem: 15572
Epoch: [11]  [ 590/2809]  eta: 0:21:45  lr: 0.000043  min_lr: 0.000000  loss: 4.3331 (4.4223)  class_acc: 0.2083 (0.2179)  loss_scale: 32768.0000 (44411.4518)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1590  max mem: 15572
Epoch: [11]  [ 600/2809]  eta: 0:21:42  lr: 0.000043  min_lr: 0.000000  loss: 4.3545 (4.4220)  class_acc: 0.1667 (0.2185)  loss_scale: 32768.0000 (44217.7171)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.1778  max mem: 15572
[2025-01-15 19:50:35,261] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:50:35,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 610/2809]  eta: 0:21:39  lr: 0.000043  min_lr: 0.000000  loss: 4.4138 (4.4214)  class_acc: 0.1667 (0.2180)  loss_scale: 32768.0000 (44298.4746)  weight_decay: 0.0500 (0.0500)  time: 0.6722  data: 0.2053  max mem: 15572
[2025-01-15 19:50:41,585] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31514
[2025-01-15 19:50:41,586] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:50:41,586] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 620/2809]  eta: 0:21:29  lr: 0.000043  min_lr: 0.000000  loss: 4.4429 (4.4220)  class_acc: 0.1667 (0.2176)  loss_scale: 32768.0000 (44323.8647)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1097  max mem: 15572
Epoch: [11]  [ 630/2809]  eta: 0:21:23  lr: 0.000043  min_lr: 0.000000  loss: 4.5359 (4.4243)  class_acc: 0.1667 (0.2167)  loss_scale: 32768.0000 (44140.7290)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0674  max mem: 15572
Epoch: [11]  [ 640/2809]  eta: 0:21:17  lr: 0.000043  min_lr: 0.000000  loss: 4.5359 (4.4252)  class_acc: 0.1667 (0.2170)  loss_scale: 32768.0000 (43963.3073)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1279  max mem: 15572
Epoch: [11]  [ 650/2809]  eta: 0:21:08  lr: 0.000043  min_lr: 0.000000  loss: 4.3768 (4.4242)  class_acc: 0.2083 (0.2168)  loss_scale: 32768.0000 (43791.3364)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0855  max mem: 15572
Epoch: [11]  [ 660/2809]  eta: 0:21:05  lr: 0.000043  min_lr: 0.000000  loss: 4.3642 (4.4230)  class_acc: 0.2917 (0.2183)  loss_scale: 32768.0000 (43624.5688)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1398  max mem: 15572
Epoch: [11]  [ 670/2809]  eta: 0:20:59  lr: 0.000043  min_lr: 0.000000  loss: 4.4803 (4.4259)  class_acc: 0.2500 (0.2177)  loss_scale: 32768.0000 (43462.7720)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1749  max mem: 15572
Epoch: [11]  [ 680/2809]  eta: 0:20:52  lr: 0.000043  min_lr: 0.000000  loss: 4.6815 (4.4296)  class_acc: 0.1667 (0.2170)  loss_scale: 32768.0000 (43305.7269)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1106  max mem: 15572
Epoch: [11]  [ 690/2809]  eta: 0:20:48  lr: 0.000043  min_lr: 0.000000  loss: 4.5281 (4.4282)  class_acc: 0.1667 (0.2176)  loss_scale: 32768.0000 (43153.2272)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1469  max mem: 15572
Epoch: [11]  [ 700/2809]  eta: 0:20:43  lr: 0.000043  min_lr: 0.000000  loss: 4.3346 (4.4296)  class_acc: 0.2083 (0.2175)  loss_scale: 32768.0000 (43005.0785)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.1731  max mem: 15572
Epoch: [11]  [ 710/2809]  eta: 0:20:37  lr: 0.000043  min_lr: 0.000000  loss: 4.4621 (4.4290)  class_acc: 0.2500 (0.2188)  loss_scale: 32768.0000 (42861.0970)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1316  max mem: 15572
Epoch: [11]  [ 720/2809]  eta: 0:20:33  lr: 0.000043  min_lr: 0.000000  loss: 4.4686 (4.4302)  class_acc: 0.2083 (0.2190)  loss_scale: 32768.0000 (42721.1096)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1470  max mem: 15572
Epoch: [11]  [ 730/2809]  eta: 0:20:29  lr: 0.000043  min_lr: 0.000000  loss: 4.5194 (4.4298)  class_acc: 0.2500 (0.2197)  loss_scale: 32768.0000 (42584.9521)  weight_decay: 0.0500 (0.0500)  time: 0.6702  data: 0.2041  max mem: 15572
Epoch: [11]  [ 740/2809]  eta: 0:20:22  lr: 0.000043  min_lr: 0.000000  loss: 4.4607 (4.4297)  class_acc: 0.2083 (0.2191)  loss_scale: 32768.0000 (42452.4696)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1359  max mem: 15572
[2025-01-15 19:51:57,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:51:57,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 750/2809]  eta: 0:20:15  lr: 0.000043  min_lr: 0.000000  loss: 4.4162 (4.4294)  class_acc: 0.1667 (0.2190)  loss_scale: 32768.0000 (42628.9427)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0767  max mem: 15572
Epoch: [11]  [ 760/2809]  eta: 0:20:07  lr: 0.000043  min_lr: 0.000000  loss: 4.4548 (4.4291)  class_acc: 0.2500 (0.2199)  loss_scale: 65536.0000 (42929.9553)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0782  max mem: 15572
Epoch: [11]  [ 770/2809]  eta: 0:20:01  lr: 0.000043  min_lr: 0.000000  loss: 4.5082 (4.4291)  class_acc: 0.2083 (0.2191)  loss_scale: 65536.0000 (43223.1595)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.1007  max mem: 15572
Epoch: [11]  [ 780/2809]  eta: 0:19:53  lr: 0.000043  min_lr: 0.000000  loss: 4.5082 (4.4295)  class_acc: 0.2083 (0.2193)  loss_scale: 65536.0000 (43508.8553)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1176  max mem: 15572
Epoch: [11]  [ 790/2809]  eta: 0:19:49  lr: 0.000043  min_lr: 0.000000  loss: 4.4902 (4.4299)  class_acc: 0.2083 (0.2197)  loss_scale: 65536.0000 (43787.3274)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1506  max mem: 15572
Epoch: [11]  [ 800/2809]  eta: 0:19:42  lr: 0.000043  min_lr: 0.000000  loss: 4.4721 (4.4295)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (44058.8464)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1696  max mem: 15572
Epoch: [11]  [ 810/2809]  eta: 0:19:39  lr: 0.000043  min_lr: 0.000000  loss: 4.4945 (4.4304)  class_acc: 0.1667 (0.2195)  loss_scale: 65536.0000 (44323.6695)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1977  max mem: 15572
Epoch: [11]  [ 820/2809]  eta: 0:19:32  lr: 0.000043  min_lr: 0.000000  loss: 4.4823 (4.4303)  class_acc: 0.1667 (0.2194)  loss_scale: 65536.0000 (44582.0414)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1661  max mem: 15572
Epoch: [11]  [ 830/2809]  eta: 0:19:26  lr: 0.000043  min_lr: 0.000000  loss: 4.4090 (4.4306)  class_acc: 0.2083 (0.2190)  loss_scale: 65536.0000 (44834.1949)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0760  max mem: 15572
Epoch: [11]  [ 840/2809]  eta: 0:19:20  lr: 0.000043  min_lr: 0.000000  loss: 4.3495 (4.4296)  class_acc: 0.1667 (0.2187)  loss_scale: 65536.0000 (45080.3520)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0961  max mem: 15572
Epoch: [11]  [ 850/2809]  eta: 0:19:14  lr: 0.000043  min_lr: 0.000000  loss: 4.3537 (4.4287)  class_acc: 0.1667 (0.2190)  loss_scale: 65536.0000 (45320.7239)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1164  max mem: 15572
Epoch: [11]  [ 860/2809]  eta: 0:19:09  lr: 0.000043  min_lr: 0.000000  loss: 4.4080 (4.4284)  class_acc: 0.1667 (0.2189)  loss_scale: 65536.0000 (45555.5122)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1344  max mem: 15572
Epoch: [11]  [ 870/2809]  eta: 0:19:04  lr: 0.000043  min_lr: 0.000000  loss: 4.4471 (4.4288)  class_acc: 0.1667 (0.2184)  loss_scale: 65536.0000 (45784.9093)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1584  max mem: 15572
[2025-01-15 19:53:12,903] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:53:12,903] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:53:14,800] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31775
[2025-01-15 19:53:14,801] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:53:14,802] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [ 880/2809]  eta: 0:18:55  lr: 0.000043  min_lr: 0.000000  loss: 4.4471 (4.4284)  class_acc: 0.1667 (0.2179)  loss_scale: 65536.0000 (46306.6515)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1117  max mem: 15572
Epoch: [11]  [ 890/2809]  eta: 0:18:49  lr: 0.000043  min_lr: 0.000000  loss: 4.4703 (4.4294)  class_acc: 0.1667 (0.2173)  loss_scale: 65536.0000 (46522.4691)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0699  max mem: 15572
Epoch: [11]  [ 900/2809]  eta: 0:18:44  lr: 0.000043  min_lr: 0.000000  loss: 4.5126 (4.4310)  class_acc: 0.1667 (0.2170)  loss_scale: 65536.0000 (46733.4961)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1311  max mem: 15572
Epoch: [11]  [ 910/2809]  eta: 0:18:38  lr: 0.000043  min_lr: 0.000000  loss: 4.4458 (4.4297)  class_acc: 0.2083 (0.2174)  loss_scale: 65536.0000 (46939.8902)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1465  max mem: 15572
Epoch: [11]  [ 920/2809]  eta: 0:18:33  lr: 0.000043  min_lr: 0.000000  loss: 4.4967 (4.4304)  class_acc: 0.2083 (0.2176)  loss_scale: 65536.0000 (47141.8024)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1333  max mem: 15572
Epoch: [11]  [ 930/2809]  eta: 0:18:25  lr: 0.000043  min_lr: 0.000000  loss: 4.5446 (4.4317)  class_acc: 0.1667 (0.2174)  loss_scale: 65536.0000 (47339.3770)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0781  max mem: 15572
Epoch: [11]  [ 940/2809]  eta: 0:18:21  lr: 0.000043  min_lr: 0.000000  loss: 4.4478 (4.4310)  class_acc: 0.2083 (0.2174)  loss_scale: 65536.0000 (47532.7524)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1069  max mem: 15572
Epoch: [11]  [ 950/2809]  eta: 0:18:14  lr: 0.000043  min_lr: 0.000000  loss: 4.3487 (4.4314)  class_acc: 0.2083 (0.2170)  loss_scale: 65536.0000 (47722.0610)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1236  max mem: 15572
Epoch: [11]  [ 960/2809]  eta: 0:18:06  lr: 0.000043  min_lr: 0.000000  loss: 4.2955 (4.4294)  class_acc: 0.2083 (0.2175)  loss_scale: 65536.0000 (47907.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5185  data: 0.0574  max mem: 15572
[2025-01-15 19:54:04,885] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31863
[2025-01-15 19:54:04,885] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:54:04,886] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 970/2809]  eta: 0:18:02  lr: 0.000043  min_lr: 0.000000  loss: 4.3820 (4.4292)  class_acc: 0.2083 (0.2176)  loss_scale: 65536.0000 (47852.7539)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1361  max mem: 15572
Epoch: [11]  [ 980/2809]  eta: 0:17:56  lr: 0.000043  min_lr: 0.000000  loss: 4.4897 (4.4307)  class_acc: 0.2083 (0.2173)  loss_scale: 32768.0000 (47698.9847)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1586  max mem: 15572
Epoch: [11]  [ 990/2809]  eta: 0:17:51  lr: 0.000043  min_lr: 0.000000  loss: 4.4547 (4.4306)  class_acc: 0.1667 (0.2173)  loss_scale: 32768.0000 (47548.3189)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1343  max mem: 15572
Epoch: [11]  [1000/2809]  eta: 0:17:45  lr: 0.000043  min_lr: 0.000000  loss: 4.3327 (4.4293)  class_acc: 0.2083 (0.2180)  loss_scale: 32768.0000 (47400.6633)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1582  max mem: 15572
Epoch: [11]  [1010/2809]  eta: 0:17:39  lr: 0.000043  min_lr: 0.000000  loss: 4.4977 (4.4306)  class_acc: 0.2083 (0.2176)  loss_scale: 32768.0000 (47255.9288)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1342  max mem: 15572
Epoch: [11]  [1020/2809]  eta: 0:17:32  lr: 0.000043  min_lr: 0.000000  loss: 4.6012 (4.4314)  class_acc: 0.1250 (0.2174)  loss_scale: 32768.0000 (47114.0294)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1152  max mem: 15572
Epoch: [11]  [1030/2809]  eta: 0:17:26  lr: 0.000043  min_lr: 0.000000  loss: 4.3733 (4.4308)  class_acc: 0.1667 (0.2170)  loss_scale: 32768.0000 (46974.8826)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1322  max mem: 15572
Epoch: [11]  [1040/2809]  eta: 0:17:20  lr: 0.000043  min_lr: 0.000000  loss: 4.3674 (4.4303)  class_acc: 0.1667 (0.2168)  loss_scale: 32768.0000 (46838.4092)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1415  max mem: 15572
Epoch: [11]  [1050/2809]  eta: 0:17:15  lr: 0.000043  min_lr: 0.000000  loss: 4.3674 (4.4289)  class_acc: 0.2083 (0.2165)  loss_scale: 32768.0000 (46704.5328)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1757  max mem: 15572
Epoch: [11]  [1060/2809]  eta: 0:17:09  lr: 0.000043  min_lr: 0.000000  loss: 4.3250 (4.4293)  class_acc: 0.2083 (0.2165)  loss_scale: 32768.0000 (46573.1800)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1496  max mem: 15572
Epoch: [11]  [1070/2809]  eta: 0:17:04  lr: 0.000043  min_lr: 0.000000  loss: 4.4284 (4.4293)  class_acc: 0.2083 (0.2172)  loss_scale: 32768.0000 (46444.2801)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1465  max mem: 15572
Epoch: [11]  [1080/2809]  eta: 0:16:59  lr: 0.000043  min_lr: 0.000000  loss: 4.3973 (4.4282)  class_acc: 0.2083 (0.2173)  loss_scale: 32768.0000 (46317.7650)  weight_decay: 0.0500 (0.0500)  time: 0.6678  data: 0.1918  max mem: 15572
Epoch: [11]  [1090/2809]  eta: 0:16:52  lr: 0.000043  min_lr: 0.000000  loss: 4.3555 (4.4269)  class_acc: 0.2083 (0.2175)  loss_scale: 32768.0000 (46193.5692)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1053  max mem: 15572
[2025-01-15 19:55:22,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:55:22,405] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 19:55:26,800] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31999
[2025-01-15 19:55:26,800] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:55:26,801] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-15 19:55:26,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=204, lr=[4.1784613185839324e-07, 4.1784613185839324e-07, 5.969230455119903e-07, 5.969230455119903e-07, 8.527472078742721e-07, 8.527472078742721e-07, 1.218210296963246e-06, 1.218210296963246e-06, 1.7403004242332084e-06, 1.7403004242332084e-06, 2.486143463190298e-06, 2.486143463190298e-06, 3.551633518843283e-06, 3.551633518843283e-06, 5.073762169776119e-06, 5.073762169776119e-06, 7.248231671108742e-06, 7.248231671108742e-06, 1.035461667301249e-05, 1.035461667301249e-05, 1.4792309532874983e-05, 1.4792309532874983e-05, 2.1131870761249978e-05, 2.1131870761249978e-05, 3.0188386801785686e-05, 3.0188386801785686e-05, 4.312626685969384e-05, 4.312626685969384e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:55:26,802] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=27.49821268846275, CurrSamplesPerSec=30.533063258334742, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [1100/2809]  eta: 0:16:46  lr: 0.000043  min_lr: 0.000000  loss: 4.2982 (4.4260)  class_acc: 0.2083 (0.2177)  loss_scale: 32768.0000 (46279.9637)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0760  max mem: 15572
Epoch: [11]  [1110/2809]  eta: 0:16:41  lr: 0.000043  min_lr: 0.000000  loss: 4.3926 (4.4256)  class_acc: 0.2083 (0.2176)  loss_scale: 32768.0000 (46158.3438)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1384  max mem: 15572
Epoch: [11]  [1120/2809]  eta: 0:16:35  lr: 0.000043  min_lr: 0.000000  loss: 4.4516 (4.4267)  class_acc: 0.1667 (0.2170)  loss_scale: 32768.0000 (46038.8938)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1561  max mem: 15572
Epoch: [11]  [1130/2809]  eta: 0:16:29  lr: 0.000043  min_lr: 0.000000  loss: 4.5084 (4.4262)  class_acc: 0.2083 (0.2171)  loss_scale: 32768.0000 (45921.5561)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1319  max mem: 15572
Epoch: [11]  [1140/2809]  eta: 0:16:22  lr: 0.000043  min_lr: 0.000000  loss: 4.4401 (4.4257)  class_acc: 0.2500 (0.2176)  loss_scale: 32768.0000 (45806.2752)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0936  max mem: 15572
Epoch: [11]  [1150/2809]  eta: 0:16:16  lr: 0.000043  min_lr: 0.000000  loss: 4.3177 (4.4249)  class_acc: 0.2500 (0.2177)  loss_scale: 32768.0000 (45692.9974)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1138  max mem: 15572
Epoch: [11]  [1160/2809]  eta: 0:16:10  lr: 0.000043  min_lr: 0.000000  loss: 4.2626 (4.4247)  class_acc: 0.2500 (0.2179)  loss_scale: 32768.0000 (45581.6710)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1483  max mem: 15572
Epoch: [11]  [1170/2809]  eta: 0:16:05  lr: 0.000043  min_lr: 0.000000  loss: 4.3627 (4.4248)  class_acc: 0.1667 (0.2178)  loss_scale: 32768.0000 (45472.2459)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1715  max mem: 15572
Epoch: [11]  [1180/2809]  eta: 0:15:59  lr: 0.000043  min_lr: 0.000000  loss: 4.4616 (4.4257)  class_acc: 0.1667 (0.2173)  loss_scale: 32768.0000 (45364.6740)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1639  max mem: 15572
Epoch: [11]  [1190/2809]  eta: 0:15:53  lr: 0.000043  min_lr: 0.000000  loss: 4.4421 (4.4251)  class_acc: 0.2083 (0.2174)  loss_scale: 32768.0000 (45258.9085)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1188  max mem: 15572
Epoch: [11]  [1200/2809]  eta: 0:15:47  lr: 0.000043  min_lr: 0.000000  loss: 4.2791 (4.4243)  class_acc: 0.2083 (0.2174)  loss_scale: 32768.0000 (45154.9042)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0907  max mem: 15572
Epoch: [11]  [1210/2809]  eta: 0:15:41  lr: 0.000043  min_lr: 0.000000  loss: 4.3470 (4.4237)  class_acc: 0.2500 (0.2177)  loss_scale: 32768.0000 (45052.6177)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1193  max mem: 15572
Epoch: [11]  [1220/2809]  eta: 0:15:34  lr: 0.000043  min_lr: 0.000000  loss: 4.4454 (4.4247)  class_acc: 0.2083 (0.2174)  loss_scale: 32768.0000 (44952.0066)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.1052  max mem: 15572
[2025-01-15 19:56:42,152] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:56:42,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [1230/2809]  eta: 0:15:29  lr: 0.000043  min_lr: 0.000000  loss: 4.4990 (4.4255)  class_acc: 0.1667 (0.2173)  loss_scale: 32768.0000 (44906.2681)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1260  max mem: 15572
Epoch: [11]  [1240/2809]  eta: 0:15:23  lr: 0.000043  min_lr: 0.000000  loss: 4.4795 (4.4257)  class_acc: 0.2083 (0.2175)  loss_scale: 65536.0000 (45072.5028)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1702  max mem: 15572
Epoch: [11]  [1250/2809]  eta: 0:15:18  lr: 0.000043  min_lr: 0.000000  loss: 4.4889 (4.4265)  class_acc: 0.2500 (0.2175)  loss_scale: 65536.0000 (45236.0799)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1577  max mem: 15572
Epoch: [11]  [1260/2809]  eta: 0:15:12  lr: 0.000043  min_lr: 0.000000  loss: 4.4008 (4.4262)  class_acc: 0.2083 (0.2177)  loss_scale: 65536.0000 (45397.0626)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1577  max mem: 15572
Epoch: [11]  [1270/2809]  eta: 0:15:06  lr: 0.000043  min_lr: 0.000000  loss: 4.4267 (4.4268)  class_acc: 0.1667 (0.2174)  loss_scale: 65536.0000 (45555.5122)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1551  max mem: 15572
Epoch: [11]  [1280/2809]  eta: 0:15:02  lr: 0.000043  min_lr: 0.000000  loss: 4.5360 (4.4279)  class_acc: 0.1667 (0.2174)  loss_scale: 65536.0000 (45711.4879)  weight_decay: 0.0500 (0.0500)  time: 0.6525  data: 0.1927  max mem: 15572
Epoch: [11]  [1290/2809]  eta: 0:14:55  lr: 0.000043  min_lr: 0.000000  loss: 4.5152 (4.4280)  class_acc: 0.2083 (0.2173)  loss_scale: 65536.0000 (45865.0473)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1465  max mem: 15572
Epoch: [11]  [1300/2809]  eta: 0:14:49  lr: 0.000043  min_lr: 0.000000  loss: 4.2945 (4.4266)  class_acc: 0.2083 (0.2173)  loss_scale: 65536.0000 (46016.2460)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0939  max mem: 15572
Epoch: [11]  [1310/2809]  eta: 0:14:43  lr: 0.000043  min_lr: 0.000000  loss: 4.3439 (4.4281)  class_acc: 0.2083 (0.2175)  loss_scale: 65536.0000 (46165.1381)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0935  max mem: 15572
Epoch: [11]  [1320/2809]  eta: 0:14:36  lr: 0.000043  min_lr: 0.000000  loss: 4.4552 (4.4283)  class_acc: 0.1667 (0.2173)  loss_scale: 65536.0000 (46311.7759)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0415  max mem: 15572
Epoch: [11]  [1330/2809]  eta: 0:14:29  lr: 0.000043  min_lr: 0.000000  loss: 4.4129 (4.4284)  class_acc: 0.1667 (0.2172)  loss_scale: 65536.0000 (46456.2104)  weight_decay: 0.0500 (0.0500)  time: 0.4768  data: 0.0357  max mem: 15572
Epoch: [11]  [1340/2809]  eta: 0:14:23  lr: 0.000043  min_lr: 0.000000  loss: 4.4119 (4.4277)  class_acc: 0.2500 (0.2176)  loss_scale: 65536.0000 (46598.4907)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1082  max mem: 15572
Epoch: [11]  [1350/2809]  eta: 0:14:18  lr: 0.000043  min_lr: 0.000000  loss: 4.4006 (4.4269)  class_acc: 0.2500 (0.2177)  loss_scale: 65536.0000 (46738.6647)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1546  max mem: 15572
[2025-01-15 19:57:56,573] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:57:56,573] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:57:57,008] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32257
[2025-01-15 19:57:57,008] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:57:57,008] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1360/2809]  eta: 0:14:11  lr: 0.000043  min_lr: 0.000000  loss: 4.4223 (4.4270)  class_acc: 0.2083 (0.2177)  loss_scale: 65536.0000 (46924.9317)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0950  max mem: 15572
Epoch: [11]  [1370/2809]  eta: 0:14:05  lr: 0.000043  min_lr: 0.000000  loss: 4.5357 (4.4274)  class_acc: 0.1667 (0.2175)  loss_scale: 65536.0000 (47060.6798)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0781  max mem: 15572
Epoch: [11]  [1380/2809]  eta: 0:13:58  lr: 0.000043  min_lr: 0.000000  loss: 4.5332 (4.4279)  class_acc: 0.2083 (0.2173)  loss_scale: 65536.0000 (47194.4620)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0657  max mem: 15572
Epoch: [11]  [1390/2809]  eta: 0:13:52  lr: 0.000043  min_lr: 0.000000  loss: 4.4046 (4.4273)  class_acc: 0.2083 (0.2174)  loss_scale: 65536.0000 (47326.3206)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0377  max mem: 15572
Epoch: [11]  [1400/2809]  eta: 0:13:47  lr: 0.000043  min_lr: 0.000000  loss: 4.4398 (4.4284)  class_acc: 0.2083 (0.2175)  loss_scale: 65536.0000 (47456.2969)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1390  max mem: 15572
Epoch: [11]  [1410/2809]  eta: 0:13:41  lr: 0.000043  min_lr: 0.000000  loss: 4.4356 (4.4285)  class_acc: 0.2083 (0.2175)  loss_scale: 65536.0000 (47584.4309)  weight_decay: 0.0500 (0.0500)  time: 0.6545  data: 0.1691  max mem: 15572
[2025-01-15 19:58:32,669] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32318
[2025-01-15 19:58:32,670] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:58:32,670] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [1420/2809]  eta: 0:13:35  lr: 0.000043  min_lr: 0.000000  loss: 4.4685 (4.4290)  class_acc: 0.2083 (0.2175)  loss_scale: 65536.0000 (47664.6418)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1514  max mem: 15572
Epoch: [11]  [1430/2809]  eta: 0:13:29  lr: 0.000043  min_lr: 0.000000  loss: 4.4685 (4.4286)  class_acc: 0.2083 (0.2174)  loss_scale: 32768.0000 (47560.5423)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1294  max mem: 15572
Epoch: [11]  [1440/2809]  eta: 0:13:23  lr: 0.000043  min_lr: 0.000000  loss: 4.3775 (4.4280)  class_acc: 0.2083 (0.2174)  loss_scale: 32768.0000 (47457.8876)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0803  max mem: 15572
Epoch: [11]  [1450/2809]  eta: 0:13:18  lr: 0.000043  min_lr: 0.000000  loss: 4.2991 (4.4278)  class_acc: 0.1667 (0.2171)  loss_scale: 32768.0000 (47356.6478)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1342  max mem: 15572
Epoch: [11]  [1460/2809]  eta: 0:13:12  lr: 0.000043  min_lr: 0.000000  loss: 4.2991 (4.4268)  class_acc: 0.1250 (0.2165)  loss_scale: 32768.0000 (47256.7940)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1767  max mem: 15572
Epoch: [11]  [1470/2809]  eta: 0:13:08  lr: 0.000043  min_lr: 0.000000  loss: 4.2459 (4.4251)  class_acc: 0.1667 (0.2164)  loss_scale: 32768.0000 (47158.2978)  weight_decay: 0.0500 (0.0500)  time: 0.6802  data: 0.2341  max mem: 15572
Epoch: [11]  [1480/2809]  eta: 0:13:01  lr: 0.000043  min_lr: 0.000000  loss: 4.2633 (4.4253)  class_acc: 0.2500 (0.2165)  loss_scale: 32768.0000 (47061.1317)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1907  max mem: 15572
Epoch: [11]  [1490/2809]  eta: 0:12:56  lr: 0.000043  min_lr: 0.000000  loss: 4.4049 (4.4253)  class_acc: 0.2083 (0.2162)  loss_scale: 32768.0000 (46965.2689)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1323  max mem: 15572
Epoch: [11]  [1500/2809]  eta: 0:12:49  lr: 0.000043  min_lr: 0.000000  loss: 4.4313 (4.4249)  class_acc: 0.2083 (0.2165)  loss_scale: 32768.0000 (46870.6835)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1070  max mem: 15572
Epoch: [11]  [1510/2809]  eta: 0:12:43  lr: 0.000043  min_lr: 0.000000  loss: 4.4516 (4.4253)  class_acc: 0.2083 (0.2162)  loss_scale: 32768.0000 (46777.3501)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0759  max mem: 15572
Epoch: [11]  [1520/2809]  eta: 0:12:38  lr: 0.000043  min_lr: 0.000000  loss: 4.4625 (4.4246)  class_acc: 0.1667 (0.2164)  loss_scale: 32768.0000 (46685.2439)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1483  max mem: 15572
Epoch: [11]  [1530/2809]  eta: 0:12:32  lr: 0.000043  min_lr: 0.000000  loss: 4.4253 (4.4241)  class_acc: 0.2500 (0.2165)  loss_scale: 32768.0000 (46594.3410)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1673  max mem: 15572
Epoch: [11]  [1540/2809]  eta: 0:12:26  lr: 0.000043  min_lr: 0.000000  loss: 4.4026 (4.4241)  class_acc: 0.2500 (0.2166)  loss_scale: 32768.0000 (46504.6178)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1473  max mem: 15572
[2025-01-15 19:59:50,856] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:59:50,856] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [1550/2809]  eta: 0:12:21  lr: 0.000043  min_lr: 0.000000  loss: 4.3647 (4.4237)  class_acc: 0.1667 (0.2165)  loss_scale: 32768.0000 (46479.4326)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1591  max mem: 15572
Epoch: [11]  [1560/2809]  eta: 0:12:15  lr: 0.000043  min_lr: 0.000000  loss: 4.3298 (4.4232)  class_acc: 0.2083 (0.2165)  loss_scale: 65536.0000 (46601.5119)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1224  max mem: 15572
Epoch: [11]  [1570/2809]  eta: 0:12:08  lr: 0.000043  min_lr: 0.000000  loss: 4.4002 (4.4231)  class_acc: 0.2083 (0.2166)  loss_scale: 65536.0000 (46722.0369)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0891  max mem: 15572
Epoch: [11]  [1580/2809]  eta: 0:12:02  lr: 0.000043  min_lr: 0.000000  loss: 4.4849 (4.4233)  class_acc: 0.1667 (0.2164)  loss_scale: 65536.0000 (46841.0373)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0987  max mem: 15572
Epoch: [11]  [1590/2809]  eta: 0:11:56  lr: 0.000043  min_lr: 0.000000  loss: 4.4164 (4.4234)  class_acc: 0.1667 (0.2163)  loss_scale: 65536.0000 (46958.5418)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0898  max mem: 15572
Epoch: [11]  [1600/2809]  eta: 0:11:49  lr: 0.000043  min_lr: 0.000000  loss: 4.4298 (4.4235)  class_acc: 0.1667 (0.2163)  loss_scale: 65536.0000 (47074.5784)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0785  max mem: 15572
Epoch: [11]  [1610/2809]  eta: 0:11:42  lr: 0.000043  min_lr: 0.000000  loss: 4.4884 (4.4240)  class_acc: 0.2083 (0.2162)  loss_scale: 65536.0000 (47189.1744)  weight_decay: 0.0500 (0.0500)  time: 0.4773  data: 0.0287  max mem: 15572
Epoch: [11]  [1620/2809]  eta: 0:11:36  lr: 0.000043  min_lr: 0.000000  loss: 4.4013 (4.4236)  class_acc: 0.2083 (0.2166)  loss_scale: 65536.0000 (47302.3566)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0485  max mem: 15572
[2025-01-15 20:00:28,406] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32520
[2025-01-15 20:00:28,407] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:00:28,407] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [1630/2809]  eta: 0:11:31  lr: 0.000043  min_lr: 0.000000  loss: 4.2843 (4.4229)  class_acc: 0.2500 (0.2173)  loss_scale: 32768.0000 (47213.2434)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1434  max mem: 15572
Epoch: [11]  [1640/2809]  eta: 0:11:25  lr: 0.000043  min_lr: 0.000000  loss: 4.3596 (4.4234)  class_acc: 0.2500 (0.2174)  loss_scale: 32768.0000 (47125.2163)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1627  max mem: 15572
Epoch: [11]  [1650/2809]  eta: 0:11:19  lr: 0.000043  min_lr: 0.000000  loss: 4.4036 (4.4233)  class_acc: 0.2500 (0.2174)  loss_scale: 32768.0000 (47038.2556)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0629  max mem: 15572
Epoch: [11]  [1660/2809]  eta: 0:11:13  lr: 0.000043  min_lr: 0.000000  loss: 4.3178 (4.4223)  class_acc: 0.2500 (0.2173)  loss_scale: 32768.0000 (46952.3420)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0483  max mem: 15572
Epoch: [11]  [1670/2809]  eta: 0:11:07  lr: 0.000043  min_lr: 0.000000  loss: 4.3978 (4.4224)  class_acc: 0.2500 (0.2174)  loss_scale: 32768.0000 (46867.4566)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1218  max mem: 15572
Epoch: [11]  [1680/2809]  eta: 0:11:01  lr: 0.000043  min_lr: 0.000000  loss: 4.4811 (4.4233)  class_acc: 0.1667 (0.2171)  loss_scale: 32768.0000 (46783.5812)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1216  max mem: 15572
Epoch: [11]  [1690/2809]  eta: 0:10:55  lr: 0.000043  min_lr: 0.000000  loss: 4.3584 (4.4222)  class_acc: 0.2083 (0.2172)  loss_scale: 32768.0000 (46700.6978)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1107  max mem: 15572
Epoch: [11]  [1700/2809]  eta: 0:10:49  lr: 0.000043  min_lr: 0.000000  loss: 4.2038 (4.4213)  class_acc: 0.2083 (0.2172)  loss_scale: 32768.0000 (46618.7889)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0937  max mem: 15572
Epoch: [11]  [1710/2809]  eta: 0:10:43  lr: 0.000043  min_lr: 0.000000  loss: 4.3419 (4.4210)  class_acc: 0.1667 (0.2170)  loss_scale: 32768.0000 (46537.8375)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1057  max mem: 15572
Epoch: [11]  [1720/2809]  eta: 0:10:37  lr: 0.000043  min_lr: 0.000000  loss: 4.5026 (4.4208)  class_acc: 0.1667 (0.2173)  loss_scale: 32768.0000 (46457.8268)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1598  max mem: 15572
Epoch: [11]  [1730/2809]  eta: 0:10:31  lr: 0.000043  min_lr: 0.000000  loss: 4.3968 (4.4205)  class_acc: 0.2500 (0.2176)  loss_scale: 32768.0000 (46378.7406)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1546  max mem: 15572
Epoch: [11]  [1740/2809]  eta: 0:10:25  lr: 0.000043  min_lr: 0.000000  loss: 4.4610 (4.4212)  class_acc: 0.2083 (0.2176)  loss_scale: 32768.0000 (46300.5629)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1480  max mem: 15572
[2025-01-15 20:01:44,500] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:01:44,500] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [1750/2809]  eta: 0:10:20  lr: 0.000043  min_lr: 0.000000  loss: 4.5174 (4.4214)  class_acc: 0.1667 (0.2178)  loss_scale: 32768.0000 (46241.9920)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.1502  max mem: 15572
Epoch: [11]  [1760/2809]  eta: 0:10:15  lr: 0.000043  min_lr: 0.000000  loss: 4.4903 (4.4214)  class_acc: 0.2500 (0.2181)  loss_scale: 65536.0000 (46351.5548)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.1645  max mem: 15572
Epoch: [11]  [1770/2809]  eta: 0:10:08  lr: 0.000043  min_lr: 0.000000  loss: 4.4903 (4.4214)  class_acc: 0.2083 (0.2178)  loss_scale: 65536.0000 (46459.8803)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1061  max mem: 15572
Epoch: [11]  [1780/2809]  eta: 0:10:02  lr: 0.000043  min_lr: 0.000000  loss: 4.4342 (4.4212)  class_acc: 0.2083 (0.2177)  loss_scale: 65536.0000 (46566.9893)  weight_decay: 0.0500 (0.0500)  time: 0.4735  data: 0.0142  max mem: 15572
Epoch: [11]  [1790/2809]  eta: 0:09:56  lr: 0.000043  min_lr: 0.000000  loss: 4.3698 (4.4205)  class_acc: 0.2500 (0.2180)  loss_scale: 65536.0000 (46672.9023)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0514  max mem: 15572
Epoch: [11]  [1800/2809]  eta: 0:09:50  lr: 0.000043  min_lr: 0.000000  loss: 4.2935 (4.4190)  class_acc: 0.2500 (0.2184)  loss_scale: 65536.0000 (46777.6391)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0883  max mem: 15572
Epoch: [11]  [1810/2809]  eta: 0:09:44  lr: 0.000043  min_lr: 0.000000  loss: 4.1915 (4.4180)  class_acc: 0.2083 (0.2182)  loss_scale: 65536.0000 (46881.2192)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1484  max mem: 15572
Epoch: [11]  [1820/2809]  eta: 0:09:38  lr: 0.000043  min_lr: 0.000000  loss: 4.3786 (4.4183)  class_acc: 0.2083 (0.2182)  loss_scale: 65536.0000 (46983.6617)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1548  max mem: 15572
Epoch: [11]  [1830/2809]  eta: 0:09:32  lr: 0.000043  min_lr: 0.000000  loss: 4.5029 (4.4186)  class_acc: 0.2083 (0.2181)  loss_scale: 65536.0000 (47084.9853)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0806  max mem: 15572
Epoch: [11]  [1840/2809]  eta: 0:09:26  lr: 0.000043  min_lr: 0.000000  loss: 4.4066 (4.4184)  class_acc: 0.2083 (0.2181)  loss_scale: 65536.0000 (47185.2080)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1155  max mem: 15572
Epoch: [11]  [1850/2809]  eta: 0:09:21  lr: 0.000043  min_lr: 0.000000  loss: 4.4066 (4.4186)  class_acc: 0.2500 (0.2182)  loss_scale: 65536.0000 (47284.3479)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1548  max mem: 15572
Epoch: [11]  [1860/2809]  eta: 0:09:15  lr: 0.000043  min_lr: 0.000000  loss: 4.4936 (4.4193)  class_acc: 0.2500 (0.2183)  loss_scale: 65536.0000 (47382.4224)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1480  max mem: 15572
Epoch: [11]  [1870/2809]  eta: 0:09:09  lr: 0.000043  min_lr: 0.000000  loss: 4.4118 (4.4191)  class_acc: 0.2500 (0.2185)  loss_scale: 65536.0000 (47479.4484)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1286  max mem: 15572
[2025-01-15 20:02:57,678] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:02:57,679] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:02:58,532] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32778
[2025-01-15 20:02:58,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:02:58,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1880/2809]  eta: 0:09:03  lr: 0.000043  min_lr: 0.000000  loss: 4.3938 (4.4187)  class_acc: 0.2500 (0.2185)  loss_scale: 65536.0000 (47610.2839)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1310  max mem: 15572
Epoch: [11]  [1890/2809]  eta: 0:08:57  lr: 0.000043  min_lr: 0.000000  loss: 4.5002 (4.4196)  class_acc: 0.2083 (0.2187)  loss_scale: 65536.0000 (47705.0788)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1757  max mem: 15572
Epoch: [11]  [1900/2809]  eta: 0:08:52  lr: 0.000043  min_lr: 0.000000  loss: 4.5354 (4.4197)  class_acc: 0.2083 (0.2188)  loss_scale: 65536.0000 (47798.8764)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1784  max mem: 15572
[2025-01-15 20:03:13,178] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32803
[2025-01-15 20:03:13,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:03:13,178] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [1910/2809]  eta: 0:08:45  lr: 0.000043  min_lr: 0.000000  loss: 4.3758 (4.4200)  class_acc: 0.2083 (0.2187)  loss_scale: 65536.0000 (47771.6630)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1036  max mem: 15572
Epoch: [11]  [1920/2809]  eta: 0:08:40  lr: 0.000043  min_lr: 0.000000  loss: 4.2796 (4.4195)  class_acc: 0.2083 (0.2187)  loss_scale: 32768.0000 (47693.5596)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1130  max mem: 15572
Epoch: [11]  [1930/2809]  eta: 0:08:34  lr: 0.000043  min_lr: 0.000000  loss: 4.3739 (4.4202)  class_acc: 0.2083 (0.2185)  loss_scale: 32768.0000 (47616.2651)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1388  max mem: 15572
Epoch: [11]  [1940/2809]  eta: 0:08:28  lr: 0.000043  min_lr: 0.000000  loss: 4.5438 (4.4203)  class_acc: 0.2083 (0.2187)  loss_scale: 32768.0000 (47539.7671)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1406  max mem: 15572
Epoch: [11]  [1950/2809]  eta: 0:08:23  lr: 0.000043  min_lr: 0.000000  loss: 4.2866 (4.4201)  class_acc: 0.2500 (0.2190)  loss_scale: 32768.0000 (47464.0533)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1588  max mem: 15572
Epoch: [11]  [1960/2809]  eta: 0:08:17  lr: 0.000043  min_lr: 0.000000  loss: 4.3534 (4.4203)  class_acc: 0.2500 (0.2191)  loss_scale: 32768.0000 (47389.1117)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1536  max mem: 15572
Epoch: [11]  [1970/2809]  eta: 0:08:11  lr: 0.000043  min_lr: 0.000000  loss: 4.4113 (4.4207)  class_acc: 0.2083 (0.2189)  loss_scale: 32768.0000 (47314.9305)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1265  max mem: 15572
Epoch: [11]  [1980/2809]  eta: 0:08:05  lr: 0.000043  min_lr: 0.000000  loss: 4.4096 (4.4206)  class_acc: 0.2500 (0.2191)  loss_scale: 32768.0000 (47241.4982)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1368  max mem: 15572
Epoch: [11]  [1990/2809]  eta: 0:07:59  lr: 0.000043  min_lr: 0.000000  loss: 4.3090 (4.4197)  class_acc: 0.2500 (0.2189)  loss_scale: 32768.0000 (47168.8036)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1375  max mem: 15572
Epoch: [11]  [2000/2809]  eta: 0:07:53  lr: 0.000043  min_lr: 0.000000  loss: 4.4733 (4.4207)  class_acc: 0.1667 (0.2186)  loss_scale: 32768.0000 (47096.8356)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1104  max mem: 15572
Epoch: [11]  [2010/2809]  eta: 0:07:48  lr: 0.000043  min_lr: 0.000000  loss: 4.4733 (4.4203)  class_acc: 0.1667 (0.2186)  loss_scale: 32768.0000 (47025.5833)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1567  max mem: 15572
Epoch: [11]  [2020/2809]  eta: 0:07:42  lr: 0.000043  min_lr: 0.000000  loss: 4.4032 (4.4201)  class_acc: 0.2500 (0.2190)  loss_scale: 32768.0000 (46955.0361)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.1812  max mem: 15572
Epoch: [11]  [2030/2809]  eta: 0:07:36  lr: 0.000043  min_lr: 0.000000  loss: 4.4037 (4.4200)  class_acc: 0.2917 (0.2193)  loss_scale: 32768.0000 (46885.1837)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0973  max mem: 15572
[2025-01-15 20:04:29,755] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:04:29,755] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [2040/2809]  eta: 0:07:30  lr: 0.000043  min_lr: 0.000000  loss: 4.4998 (4.4208)  class_acc: 0.2917 (0.2194)  loss_scale: 32768.0000 (46944.4547)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1079  max mem: 15572
Epoch: [11]  [2050/2809]  eta: 0:07:24  lr: 0.000043  min_lr: 0.000000  loss: 4.4998 (4.4205)  class_acc: 0.1667 (0.2192)  loss_scale: 65536.0000 (47035.1009)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1326  max mem: 15572
Epoch: [11]  [2060/2809]  eta: 0:07:18  lr: 0.000043  min_lr: 0.000000  loss: 4.4312 (4.4202)  class_acc: 0.1667 (0.2191)  loss_scale: 65536.0000 (47124.8675)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0847  max mem: 15572
Epoch: [11]  [2070/2809]  eta: 0:07:13  lr: 0.000043  min_lr: 0.000000  loss: 4.3743 (4.4204)  class_acc: 0.2083 (0.2191)  loss_scale: 65536.0000 (47213.7673)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1204  max mem: 15572
Epoch: [11]  [2080/2809]  eta: 0:07:07  lr: 0.000043  min_lr: 0.000000  loss: 4.4682 (4.4207)  class_acc: 0.2083 (0.2190)  loss_scale: 65536.0000 (47301.8126)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1378  max mem: 15572
Epoch: [11]  [2090/2809]  eta: 0:07:01  lr: 0.000043  min_lr: 0.000000  loss: 4.4504 (4.4203)  class_acc: 0.2500 (0.2193)  loss_scale: 65536.0000 (47389.0158)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1509  max mem: 15572
[2025-01-15 20:05:10,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=33000, skipped=209, lr=[4.1381510514265254e-07, 4.1381510514265254e-07, 5.911644359180752e-07, 5.911644359180752e-07, 8.445206227401074e-07, 8.445206227401074e-07, 1.2064580324858679e-06, 1.2064580324858679e-06, 1.7235114749798113e-06, 1.7235114749798113e-06, 2.462159249971159e-06, 2.462159249971159e-06, 3.517370357101656e-06, 3.517370357101656e-06, 5.024814795859509e-06, 5.024814795859509e-06, 7.17830685122787e-06, 7.17830685122787e-06, 1.0254724073182673e-05, 1.0254724073182673e-05, 1.464960581883239e-05, 1.464960581883239e-05, 2.09280083126177e-05, 2.09280083126177e-05, 2.9897154732311005e-05, 2.9897154732311005e-05, 4.271022104615858e-05, 4.271022104615858e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:05:10,194] [INFO] [timer.py:260:stop] epoch=0/micro_step=33000/global_step=33000, RunningAvgSamplesPerSec=27.503300037815535, CurrSamplesPerSec=30.373368382557942, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [2100/2809]  eta: 0:06:55  lr: 0.000043  min_lr: 0.000000  loss: 4.4231 (4.4204)  class_acc: 0.2500 (0.2195)  loss_scale: 65536.0000 (47475.3889)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1200  max mem: 15572
Epoch: [11]  [2110/2809]  eta: 0:06:50  lr: 0.000043  min_lr: 0.000000  loss: 4.4947 (4.4206)  class_acc: 0.2083 (0.2194)  loss_scale: 65536.0000 (47560.9436)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1498  max mem: 15572
Epoch: [11]  [2120/2809]  eta: 0:06:44  lr: 0.000043  min_lr: 0.000000  loss: 4.4676 (4.4207)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (47645.6917)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1660  max mem: 15572
Epoch: [11]  [2130/2809]  eta: 0:06:38  lr: 0.000043  min_lr: 0.000000  loss: 4.3810 (4.4204)  class_acc: 0.2083 (0.2195)  loss_scale: 65536.0000 (47729.6443)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1305  max mem: 15572
Epoch: [11]  [2140/2809]  eta: 0:06:32  lr: 0.000043  min_lr: 0.000000  loss: 4.4646 (4.4209)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (47812.8127)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1774  max mem: 15572
Epoch: [11]  [2150/2809]  eta: 0:06:27  lr: 0.000043  min_lr: 0.000000  loss: 4.5230 (4.4211)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (47895.2078)  weight_decay: 0.0500 (0.0500)  time: 0.6695  data: 0.1858  max mem: 15572
Epoch: [11]  [2160/2809]  eta: 0:06:21  lr: 0.000043  min_lr: 0.000000  loss: 4.4873 (4.4218)  class_acc: 0.2083 (0.2198)  loss_scale: 65536.0000 (47976.8404)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1182  max mem: 15572
[2025-01-15 20:05:48,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:05:48,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:05:49,148] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33062
[2025-01-15 20:05:49,148] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:05:49,148] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2170/2809]  eta: 0:06:15  lr: 0.000043  min_lr: 0.000000  loss: 4.4068 (4.4208)  class_acc: 0.2083 (0.2199)  loss_scale: 65536.0000 (48118.0949)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1036  max mem: 15572
Epoch: [11]  [2180/2809]  eta: 0:06:09  lr: 0.000043  min_lr: 0.000000  loss: 4.3823 (4.4208)  class_acc: 0.2083 (0.2197)  loss_scale: 65536.0000 (48197.9569)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0964  max mem: 15572
Epoch: [11]  [2190/2809]  eta: 0:06:03  lr: 0.000043  min_lr: 0.000000  loss: 4.3975 (4.4205)  class_acc: 0.1667 (0.2195)  loss_scale: 65536.0000 (48277.0899)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1139  max mem: 15572
Epoch: [11]  [2200/2809]  eta: 0:05:57  lr: 0.000043  min_lr: 0.000000  loss: 4.4597 (4.4211)  class_acc: 0.1667 (0.2198)  loss_scale: 65536.0000 (48355.5039)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1697  max mem: 15572
Epoch: [11]  [2210/2809]  eta: 0:05:51  lr: 0.000043  min_lr: 0.000000  loss: 4.4597 (4.4213)  class_acc: 0.1667 (0.2196)  loss_scale: 65536.0000 (48433.2085)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1170  max mem: 15572
[2025-01-15 20:06:19,308] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33114
[2025-01-15 20:06:19,309] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:06:19,309] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [2220/2809]  eta: 0:05:45  lr: 0.000043  min_lr: 0.000000  loss: 4.4260 (4.4217)  class_acc: 0.1250 (0.2195)  loss_scale: 65536.0000 (48421.6911)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0553  max mem: 15572
Epoch: [11]  [2230/2809]  eta: 0:05:39  lr: 0.000043  min_lr: 0.000000  loss: 4.4115 (4.4216)  class_acc: 0.2083 (0.2196)  loss_scale: 32768.0000 (48351.5267)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0578  max mem: 15572
Epoch: [11]  [2240/2809]  eta: 0:05:34  lr: 0.000043  min_lr: 0.000000  loss: 4.3676 (4.4208)  class_acc: 0.2500 (0.2200)  loss_scale: 32768.0000 (48281.9884)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1304  max mem: 15572
Epoch: [11]  [2250/2809]  eta: 0:05:28  lr: 0.000043  min_lr: 0.000000  loss: 4.3316 (4.4203)  class_acc: 0.2083 (0.2199)  loss_scale: 32768.0000 (48213.0680)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1548  max mem: 15572
Epoch: [11]  [2260/2809]  eta: 0:05:22  lr: 0.000043  min_lr: 0.000000  loss: 4.3665 (4.4205)  class_acc: 0.2500 (0.2203)  loss_scale: 32768.0000 (48144.7572)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1560  max mem: 15572
Epoch: [11]  [2270/2809]  eta: 0:05:16  lr: 0.000043  min_lr: 0.000000  loss: 4.4114 (4.4206)  class_acc: 0.2917 (0.2205)  loss_scale: 32768.0000 (48077.0480)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1303  max mem: 15572
Epoch: [11]  [2280/2809]  eta: 0:05:10  lr: 0.000043  min_lr: 0.000000  loss: 4.4054 (4.4206)  class_acc: 0.2500 (0.2206)  loss_scale: 32768.0000 (48009.9325)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0951  max mem: 15572
Epoch: [11]  [2290/2809]  eta: 0:05:04  lr: 0.000043  min_lr: 0.000000  loss: 4.3991 (4.4204)  class_acc: 0.2083 (0.2206)  loss_scale: 32768.0000 (47943.4029)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1214  max mem: 15572
Epoch: [11]  [2300/2809]  eta: 0:04:58  lr: 0.000043  min_lr: 0.000000  loss: 4.3267 (4.4202)  class_acc: 0.2083 (0.2206)  loss_scale: 32768.0000 (47877.4515)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1408  max mem: 15572
Epoch: [11]  [2310/2809]  eta: 0:04:53  lr: 0.000043  min_lr: 0.000000  loss: 4.3267 (4.4199)  class_acc: 0.2500 (0.2208)  loss_scale: 32768.0000 (47812.0710)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1273  max mem: 15572
Epoch: [11]  [2320/2809]  eta: 0:04:47  lr: 0.000043  min_lr: 0.000000  loss: 4.2150 (4.4193)  class_acc: 0.2500 (0.2206)  loss_scale: 32768.0000 (47747.2538)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0794  max mem: 15572
Epoch: [11]  [2330/2809]  eta: 0:04:41  lr: 0.000043  min_lr: 0.000000  loss: 4.2150 (4.4188)  class_acc: 0.2500 (0.2207)  loss_scale: 32768.0000 (47682.9927)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1087  max mem: 15572
Epoch: [11]  [2340/2809]  eta: 0:04:35  lr: 0.000043  min_lr: 0.000000  loss: 4.3983 (4.4190)  class_acc: 0.2500 (0.2207)  loss_scale: 32768.0000 (47619.2806)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1604  max mem: 15572
[2025-01-15 20:07:36,011] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:07:36,012] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [2350/2809]  eta: 0:04:29  lr: 0.000043  min_lr: 0.000000  loss: 4.3666 (4.4185)  class_acc: 0.1667 (0.2205)  loss_scale: 32768.0000 (47653.6759)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1228  max mem: 15572
Epoch: [11]  [2360/2809]  eta: 0:04:23  lr: 0.000043  min_lr: 0.000000  loss: 4.3955 (4.4186)  class_acc: 0.1667 (0.2204)  loss_scale: 65536.0000 (47729.4163)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0895  max mem: 15572
Epoch: [11]  [2370/2809]  eta: 0:04:17  lr: 0.000043  min_lr: 0.000000  loss: 4.5383 (4.4195)  class_acc: 0.1667 (0.2201)  loss_scale: 65536.0000 (47804.5179)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1133  max mem: 15572
Epoch: [11]  [2380/2809]  eta: 0:04:11  lr: 0.000043  min_lr: 0.000000  loss: 4.5293 (4.4192)  class_acc: 0.1667 (0.2200)  loss_scale: 65536.0000 (47878.9887)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1156  max mem: 15572
Epoch: [11]  [2390/2809]  eta: 0:04:05  lr: 0.000043  min_lr: 0.000000  loss: 4.4610 (4.4195)  class_acc: 0.2083 (0.2200)  loss_scale: 65536.0000 (47952.8365)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0528  max mem: 15572
Epoch: [11]  [2400/2809]  eta: 0:03:59  lr: 0.000043  min_lr: 0.000000  loss: 4.5001 (4.4197)  class_acc: 0.1667 (0.2198)  loss_scale: 65536.0000 (48026.0691)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0435  max mem: 15572
Epoch: [11]  [2410/2809]  eta: 0:03:53  lr: 0.000043  min_lr: 0.000000  loss: 4.4272 (4.4196)  class_acc: 0.1667 (0.2197)  loss_scale: 65536.0000 (48098.6943)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.1116  max mem: 15572
Epoch: [11]  [2420/2809]  eta: 0:03:48  lr: 0.000043  min_lr: 0.000000  loss: 4.4051 (4.4197)  class_acc: 0.1667 (0.2197)  loss_scale: 65536.0000 (48170.7195)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.1189  max mem: 15572
Epoch: [11]  [2430/2809]  eta: 0:03:42  lr: 0.000043  min_lr: 0.000000  loss: 4.5001 (4.4199)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (48242.1522)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1480  max mem: 15572
Epoch: [11]  [2440/2809]  eta: 0:03:36  lr: 0.000043  min_lr: 0.000000  loss: 4.3978 (4.4191)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (48312.9996)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1235  max mem: 15572
Epoch: [11]  [2450/2809]  eta: 0:03:30  lr: 0.000043  min_lr: 0.000000  loss: 4.3733 (4.4193)  class_acc: 0.2500 (0.2198)  loss_scale: 65536.0000 (48383.2689)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1490  max mem: 15572
Epoch: [11]  [2460/2809]  eta: 0:03:24  lr: 0.000043  min_lr: 0.000000  loss: 4.4097 (4.4191)  class_acc: 0.2500 (0.2199)  loss_scale: 65536.0000 (48452.9671)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1580  max mem: 15572
Epoch: [11]  [2470/2809]  eta: 0:03:18  lr: 0.000043  min_lr: 0.000000  loss: 4.3896 (4.4191)  class_acc: 0.2500 (0.2200)  loss_scale: 65536.0000 (48522.1012)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0565  max mem: 15572
[2025-01-15 20:08:47,594] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:08:47,594] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:08:49,372] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33372
[2025-01-15 20:08:49,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:08:49,372] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2480/2809]  eta: 0:03:12  lr: 0.000043  min_lr: 0.000000  loss: 4.4358 (4.4186)  class_acc: 0.1667 (0.2199)  loss_scale: 65536.0000 (48617.0931)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0901  max mem: 15572
Epoch: [11]  [2490/2809]  eta: 0:03:07  lr: 0.000043  min_lr: 0.000000  loss: 4.4704 (4.4188)  class_acc: 0.2083 (0.2200)  loss_scale: 65536.0000 (48685.0132)  weight_decay: 0.0500 (0.0500)  time: 0.6739  data: 0.2054  max mem: 15572
Epoch: [11]  [2500/2809]  eta: 0:03:01  lr: 0.000043  min_lr: 0.000000  loss: 4.4892 (4.4192)  class_acc: 0.2083 (0.2201)  loss_scale: 65536.0000 (48752.3902)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1370  max mem: 15572
Epoch: [11]  [2510/2809]  eta: 0:02:55  lr: 0.000043  min_lr: 0.000000  loss: 4.4732 (4.4191)  class_acc: 0.2083 (0.2201)  loss_scale: 65536.0000 (48819.2306)  weight_decay: 0.0500 (0.0500)  time: 0.4620  data: 0.0009  max mem: 15572
Epoch: [11]  [2520/2809]  eta: 0:02:49  lr: 0.000043  min_lr: 0.000000  loss: 4.4152 (4.4191)  class_acc: 0.1667 (0.2201)  loss_scale: 65536.0000 (48885.5407)  weight_decay: 0.0500 (0.0500)  time: 0.4579  data: 0.0148  max mem: 15572
Epoch: [11]  [2530/2809]  eta: 0:02:43  lr: 0.000043  min_lr: 0.000000  loss: 4.4152 (4.4195)  class_acc: 0.1667 (0.2201)  loss_scale: 65536.0000 (48951.3267)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0896  max mem: 15572
Epoch: [11]  [2540/2809]  eta: 0:02:37  lr: 0.000043  min_lr: 0.000000  loss: 4.3780 (4.4188)  class_acc: 0.2083 (0.2202)  loss_scale: 65536.0000 (49016.5950)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1378  max mem: 15572
Epoch: [11]  [2550/2809]  eta: 0:02:31  lr: 0.000043  min_lr: 0.000000  loss: 4.3772 (4.4189)  class_acc: 0.2083 (0.2203)  loss_scale: 65536.0000 (49081.3516)  weight_decay: 0.0500 (0.0500)  time: 0.6567  data: 0.2166  max mem: 15572
Epoch: [11]  [2560/2809]  eta: 0:02:25  lr: 0.000043  min_lr: 0.000000  loss: 4.3772 (4.4189)  class_acc: 0.2500 (0.2205)  loss_scale: 65536.0000 (49145.6025)  weight_decay: 0.0500 (0.0500)  time: 0.6962  data: 0.2648  max mem: 15572
Epoch: [11]  [2570/2809]  eta: 0:02:20  lr: 0.000043  min_lr: 0.000000  loss: 4.3578 (4.4192)  class_acc: 0.2083 (0.2204)  loss_scale: 65536.0000 (49209.3536)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1594  max mem: 15572
Epoch: [11]  [2580/2809]  eta: 0:02:14  lr: 0.000043  min_lr: 0.000000  loss: 4.3803 (4.4193)  class_acc: 0.1667 (0.2204)  loss_scale: 65536.0000 (49272.6106)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1118  max mem: 15572
Epoch: [11]  [2590/2809]  eta: 0:02:08  lr: 0.000042  min_lr: 0.000000  loss: 4.3803 (4.4193)  class_acc: 0.1667 (0.2202)  loss_scale: 65536.0000 (49335.3794)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1036  max mem: 15572
Epoch: [11]  [2600/2809]  eta: 0:02:02  lr: 0.000042  min_lr: 0.000000  loss: 4.4549 (4.4196)  class_acc: 0.1667 (0.2202)  loss_scale: 65536.0000 (49397.6655)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1424  max mem: 15572
[2025-01-15 20:10:04,834] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:10:04,835] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:10:07,278] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33506
[2025-01-15 20:10:07,278] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:10:07,278] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2610/2809]  eta: 0:01:56  lr: 0.000042  min_lr: 0.000000  loss: 4.5355 (4.4197)  class_acc: 0.2500 (0.2203)  loss_scale: 65536.0000 (49584.9743)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.1623  max mem: 15572
Epoch: [11]  [2620/2809]  eta: 0:01:50  lr: 0.000042  min_lr: 0.000000  loss: 4.5598 (4.4201)  class_acc: 0.1667 (0.2202)  loss_scale: 65536.0000 (49645.8329)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1230  max mem: 15572
Epoch: [11]  [2630/2809]  eta: 0:01:45  lr: 0.000042  min_lr: 0.000000  loss: 4.5387 (4.4205)  class_acc: 0.1667 (0.2200)  loss_scale: 65536.0000 (49706.2288)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1164  max mem: 15572
Epoch: [11]  [2640/2809]  eta: 0:01:39  lr: 0.000042  min_lr: 0.000000  loss: 4.3447 (4.4204)  class_acc: 0.2083 (0.2203)  loss_scale: 65536.0000 (49766.1674)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1089  max mem: 15572
Epoch: [11]  [2650/2809]  eta: 0:01:33  lr: 0.000042  min_lr: 0.000000  loss: 4.3447 (4.4202)  class_acc: 0.2083 (0.2202)  loss_scale: 65536.0000 (49825.6537)  weight_decay: 0.0500 (0.0500)  time: 0.4967  data: 0.0556  max mem: 15572
Epoch: [11]  [2660/2809]  eta: 0:01:27  lr: 0.000042  min_lr: 0.000000  loss: 4.4610 (4.4206)  class_acc: 0.2083 (0.2202)  loss_scale: 65536.0000 (49884.6930)  weight_decay: 0.0500 (0.0500)  time: 0.4180  data: 0.0005  max mem: 15572
[2025-01-15 20:10:37,833] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33562
[2025-01-15 20:10:37,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:10:37,833] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [2670/2809]  eta: 0:01:21  lr: 0.000042  min_lr: 0.000000  loss: 4.4678 (4.4206)  class_acc: 0.2500 (0.2204)  loss_scale: 65536.0000 (49845.1456)  weight_decay: 0.0500 (0.0500)  time: 0.4436  data: 0.0006  max mem: 15572
Epoch: [11]  [2680/2809]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000000  loss: 4.4723 (4.4209)  class_acc: 0.2083 (0.2204)  loss_scale: 32768.0000 (49781.4487)  weight_decay: 0.0500 (0.0500)  time: 0.4748  data: 0.0009  max mem: 15572
Epoch: [11]  [2690/2809]  eta: 0:01:09  lr: 0.000042  min_lr: 0.000000  loss: 4.4267 (4.4207)  class_acc: 0.1667 (0.2203)  loss_scale: 32768.0000 (49718.2252)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0169  max mem: 15572
Epoch: [11]  [2700/2809]  eta: 0:01:03  lr: 0.000042  min_lr: 0.000000  loss: 4.3868 (4.4208)  class_acc: 0.2083 (0.2205)  loss_scale: 32768.0000 (49655.4698)  weight_decay: 0.0500 (0.0500)  time: 0.6663  data: 0.1697  max mem: 15572
Epoch: [11]  [2710/2809]  eta: 0:00:57  lr: 0.000042  min_lr: 0.000000  loss: 4.3954 (4.4207)  class_acc: 0.2083 (0.2205)  loss_scale: 32768.0000 (49593.1774)  weight_decay: 0.0500 (0.0500)  time: 0.7397  data: 0.2521  max mem: 15572
Epoch: [11]  [2720/2809]  eta: 0:00:52  lr: 0.000042  min_lr: 0.000000  loss: 4.3954 (4.4204)  class_acc: 0.2083 (0.2204)  loss_scale: 32768.0000 (49531.3429)  weight_decay: 0.0500 (0.0500)  time: 0.6881  data: 0.2054  max mem: 15572
Epoch: [11]  [2730/2809]  eta: 0:00:46  lr: 0.000042  min_lr: 0.000000  loss: 4.4481 (4.4210)  class_acc: 0.1250 (0.2201)  loss_scale: 32768.0000 (49469.9612)  weight_decay: 0.0500 (0.0500)  time: 0.6672  data: 0.1778  max mem: 15572
Epoch: [11]  [2740/2809]  eta: 0:00:40  lr: 0.000042  min_lr: 0.000000  loss: 4.4446 (4.4205)  class_acc: 0.2083 (0.2202)  loss_scale: 32768.0000 (49409.0274)  weight_decay: 0.0500 (0.0500)  time: 0.6971  data: 0.1788  max mem: 15572
Epoch: [11]  [2750/2809]  eta: 0:00:34  lr: 0.000042  min_lr: 0.000000  loss: 4.3010 (4.4202)  class_acc: 0.2500 (0.2204)  loss_scale: 32768.0000 (49348.5365)  weight_decay: 0.0500 (0.0500)  time: 0.6751  data: 0.1778  max mem: 15572
Epoch: [11]  [2760/2809]  eta: 0:00:28  lr: 0.000042  min_lr: 0.000000  loss: 4.3036 (4.4203)  class_acc: 0.2500 (0.2205)  loss_scale: 32768.0000 (49288.4839)  weight_decay: 0.0500 (0.0500)  time: 0.6790  data: 0.1897  max mem: 15572
Epoch: [11]  [2770/2809]  eta: 0:00:22  lr: 0.000042  min_lr: 0.000000  loss: 4.2869 (4.4198)  class_acc: 0.2500 (0.2208)  loss_scale: 32768.0000 (49228.8647)  weight_decay: 0.0500 (0.0500)  time: 0.7247  data: 0.2302  max mem: 15572
Epoch: [11]  [2780/2809]  eta: 0:00:17  lr: 0.000042  min_lr: 0.000000  loss: 4.3860 (4.4203)  class_acc: 0.2500 (0.2206)  loss_scale: 32768.0000 (49169.6742)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.1645  max mem: 15572
Epoch: [11]  [2790/2809]  eta: 0:00:11  lr: 0.000042  min_lr: 0.000000  loss: 4.3860 (4.4197)  class_acc: 0.2500 (0.2210)  loss_scale: 32768.0000 (49110.9079)  weight_decay: 0.0500 (0.0500)  time: 0.6713  data: 0.1990  max mem: 15572
[2025-01-15 20:12:01,812] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:12:01,812] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [2800/2809]  eta: 0:00:05  lr: 0.000042  min_lr: 0.000000  loss: 4.2804 (4.4201)  class_acc: 0.2500 (0.2209)  loss_scale: 32768.0000 (49157.8493)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1723  max mem: 15572
Epoch: [11]  [2808/2809]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000000  loss: 4.3724 (4.4198)  class_acc: 0.1667 (0.2208)  loss_scale: 65536.0000 (49204.4941)  weight_decay: 0.0500 (0.0500)  time: 0.4328  data: 0.0270  max mem: 15572
Epoch: [11] Total time: 0:27:30 (0.5877 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000000  loss: 4.3724 (4.4198)  class_acc: 0.1667 (0.2208)  loss_scale: 65536.0000 (49204.4941)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:22:26  loss: 1.4330 (1.4330)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 4.9506  data: 4.7750  max mem: 15572
Val:  [ 10/272]  eta: 0:03:40  loss: 3.5508 (3.3371)  acc1: 11.1111 (26.2626)  acc5: 44.4444 (48.9899)  time: 0.8421  data: 0.6418  max mem: 15572
Val:  [ 20/272]  eta: 0:02:17  loss: 3.2525 (3.2662)  acc1: 27.7778 (28.0423)  acc5: 61.1111 (56.8783)  time: 0.3257  data: 0.1147  max mem: 15572
Val:  [ 30/272]  eta: 0:01:52  loss: 3.2176 (3.3230)  acc1: 22.2222 (24.1935)  acc5: 61.1111 (56.0932)  time: 0.2587  data: 0.0413  max mem: 15572
Val:  [ 40/272]  eta: 0:01:43  loss: 3.1302 (3.2723)  acc1: 22.2222 (25.3388)  acc5: 66.6667 (59.0786)  time: 0.3438  data: 0.1292  max mem: 15572
Val:  [ 50/272]  eta: 0:01:37  loss: 3.0650 (3.1761)  acc1: 33.3333 (28.1046)  acc5: 72.2222 (62.5272)  time: 0.3989  data: 0.1842  max mem: 15572
Val:  [ 60/272]  eta: 0:01:31  loss: 2.4334 (3.0905)  acc1: 38.8889 (32.0583)  acc5: 88.8889 (64.6630)  time: 0.3990  data: 0.1909  max mem: 15572
Val:  [ 70/272]  eta: 0:01:21  loss: 2.4334 (3.0191)  acc1: 50.0000 (34.1941)  acc5: 83.3333 (66.8232)  time: 0.3187  data: 0.1078  max mem: 15572
Val:  [ 80/272]  eta: 0:01:12  loss: 2.7996 (3.0265)  acc1: 33.3333 (33.9506)  acc5: 77.7778 (66.1866)  time: 0.2143  data: 0.0130  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 3.2130 (3.0530)  acc1: 33.3333 (34.1270)  acc5: 61.1111 (65.9951)  time: 0.2669  data: 0.0644  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 3.2555 (3.0922)  acc1: 33.3333 (33.4433)  acc5: 66.6667 (65.7316)  time: 0.3435  data: 0.1197  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 3.4047 (3.1387)  acc1: 16.6667 (31.4815)  acc5: 55.5556 (64.3644)  time: 0.3363  data: 0.1208  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 3.4047 (3.1654)  acc1: 16.6667 (30.9917)  acc5: 55.5556 (63.6823)  time: 0.3247  data: 0.1192  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 3.2273 (3.1369)  acc1: 33.3333 (32.1883)  acc5: 61.1111 (64.2070)  time: 0.3410  data: 0.1334  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 2.8455 (3.1332)  acc1: 38.8889 (32.4665)  acc5: 66.6667 (64.1450)  time: 0.3485  data: 0.1338  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 3.1175 (3.1313)  acc1: 22.2222 (31.8249)  acc5: 66.6667 (64.2016)  time: 0.3300  data: 0.1215  max mem: 15572
Val:  [160/272]  eta: 0:00:40  loss: 3.0210 (3.1226)  acc1: 22.2222 (32.4017)  acc5: 72.2222 (64.8723)  time: 0.3445  data: 0.1433  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 3.1826 (3.1450)  acc1: 22.2222 (31.8713)  acc5: 61.1111 (63.8726)  time: 0.3411  data: 0.1377  max mem: 15572
Val:  [180/272]  eta: 0:00:33  loss: 3.1826 (3.1294)  acc1: 22.2222 (31.9214)  acc5: 61.1111 (64.4567)  time: 0.3723  data: 0.1785  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 2.9444 (3.1540)  acc1: 16.6667 (31.0355)  acc5: 61.1111 (63.2054)  time: 0.3155  data: 0.1176  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 2.9767 (3.1547)  acc1: 16.6667 (30.9840)  acc5: 55.5556 (63.1564)  time: 0.2655  data: 0.0476  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.8125 (3.1546)  acc1: 33.3333 (31.6219)  acc5: 77.7778 (63.4018)  time: 0.3379  data: 0.1219  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 3.1294 (3.1494)  acc1: 38.8889 (31.8502)  acc5: 66.6667 (63.7255)  time: 0.3574  data: 0.1477  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.8103 (3.1345)  acc1: 50.0000 (33.0207)  acc5: 72.2222 (64.3098)  time: 0.3605  data: 0.1522  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 2.5950 (3.1128)  acc1: 50.0000 (33.4716)  acc5: 83.3333 (65.2374)  time: 0.3338  data: 0.1238  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.7508 (3.1240)  acc1: 27.7778 (33.0899)  acc5: 77.7778 (64.8960)  time: 0.3019  data: 0.0927  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 2.4453 (3.0759)  acc1: 66.6667 (35.1213)  acc5: 77.7778 (65.9855)  time: 0.2924  data: 0.1006  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.7386 (3.0798)  acc1: 55.5556 (34.9528)  acc5: 77.7778 (65.8262)  time: 0.2392  data: 0.0712  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.7386 (3.0836)  acc1: 55.5556 (34.9171)  acc5: 77.7778 (65.7997)  time: 0.2126  data: 0.0515  max mem: 15572
Val: Total time: 0:01:32 (0.3405 s / it)
* Acc@1 34.917 Acc@5 65.800 loss 3.084
Accuracy of the network on the 4883 val videos: 34.9%
[2025-01-15 20:13:41,334] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 20:13:41,337] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 20:13:41,337] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 20:13:44,567] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 20:13:44,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 34.92%
Epoch: [12]  [   0/2809]  eta: 7:56:33  lr: 0.000042  min_lr: 0.000000  loss: 4.5078 (4.5078)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 10.1792  data: 9.6833  max mem: 15572
Epoch: [12]  [  10/2809]  eta: 1:04:26  lr: 0.000042  min_lr: 0.000000  loss: 4.4313 (4.3551)  class_acc: 0.2500 (0.2500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3813  data: 0.9235  max mem: 15572
Epoch: [12]  [  20/2809]  eta: 0:43:58  lr: 0.000042  min_lr: 0.000000  loss: 4.3981 (4.3626)  class_acc: 0.2500 (0.2520)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0405  max mem: 15572
Epoch: [12]  [  30/2809]  eta: 0:38:04  lr: 0.000042  min_lr: 0.000000  loss: 4.4408 (4.3839)  class_acc: 0.2083 (0.2366)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5143  data: 0.0597  max mem: 15572
Epoch: [12]  [  40/2809]  eta: 0:36:34  lr: 0.000042  min_lr: 0.000000  loss: 4.4043 (4.3895)  class_acc: 0.1667 (0.2297)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1664  max mem: 15572
Epoch: [12]  [  50/2809]  eta: 0:35:29  lr: 0.000042  min_lr: 0.000000  loss: 4.3816 (4.3966)  class_acc: 0.2500 (0.2337)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6942  data: 0.2453  max mem: 15572
Epoch: [12]  [  60/2809]  eta: 0:34:20  lr: 0.000042  min_lr: 0.000000  loss: 4.4539 (4.3964)  class_acc: 0.2500 (0.2384)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6615  data: 0.2088  max mem: 15572
Epoch: [12]  [  70/2809]  eta: 0:32:51  lr: 0.000042  min_lr: 0.000000  loss: 4.3874 (4.3927)  class_acc: 0.2083 (0.2342)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1316  max mem: 15572
Epoch: [12]  [  80/2809]  eta: 0:31:17  lr: 0.000042  min_lr: 0.000000  loss: 4.2144 (4.3723)  class_acc: 0.1667 (0.2310)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5008  data: 0.0450  max mem: 15572
Epoch: [12]  [  90/2809]  eta: 0:30:33  lr: 0.000042  min_lr: 0.000000  loss: 4.2913 (4.3694)  class_acc: 0.2083 (0.2326)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0544  max mem: 15572
Epoch: [12]  [ 100/2809]  eta: 0:30:14  lr: 0.000042  min_lr: 0.000000  loss: 4.4058 (4.3825)  class_acc: 0.1667 (0.2244)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1247  max mem: 15572
Epoch: [12]  [ 110/2809]  eta: 0:29:53  lr: 0.000042  min_lr: 0.000000  loss: 4.4920 (4.3901)  class_acc: 0.1667 (0.2218)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1598  max mem: 15572
[2025-01-15 20:14:58,811] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:14:58,812] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:15:00,245] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33822
[2025-01-15 20:15:00,246] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:15:00,246] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 120/2809]  eta: 0:29:30  lr: 0.000042  min_lr: 0.000000  loss: 4.4752 (4.3854)  class_acc: 0.2083 (0.2242)  loss_scale: 65536.0000 (67160.8595)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1628  max mem: 15572
Epoch: [12]  [ 130/2809]  eta: 0:29:08  lr: 0.000042  min_lr: 0.000000  loss: 4.3980 (4.3901)  class_acc: 0.2500 (0.2230)  loss_scale: 65536.0000 (67036.8244)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1432  max mem: 15572
Epoch: [12]  [ 140/2809]  eta: 0:28:48  lr: 0.000042  min_lr: 0.000000  loss: 4.4080 (4.3927)  class_acc: 0.1667 (0.2193)  loss_scale: 65536.0000 (66930.3830)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1257  max mem: 15572
Epoch: [12]  [ 150/2809]  eta: 0:28:24  lr: 0.000042  min_lr: 0.000000  loss: 4.4445 (4.4058)  class_acc: 0.1667 (0.2194)  loss_scale: 65536.0000 (66838.0397)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1132  max mem: 15572
Epoch: [12]  [ 160/2809]  eta: 0:28:03  lr: 0.000042  min_lr: 0.000000  loss: 4.5136 (4.4059)  class_acc: 0.2083 (0.2208)  loss_scale: 65536.0000 (66757.1677)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1117  max mem: 15572
Epoch: [12]  [ 170/2809]  eta: 0:27:46  lr: 0.000042  min_lr: 0.000000  loss: 4.4216 (4.4078)  class_acc: 0.2500 (0.2215)  loss_scale: 65536.0000 (66685.7544)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1120  max mem: 15572
Epoch: [12]  [ 180/2809]  eta: 0:27:46  lr: 0.000042  min_lr: 0.000000  loss: 4.4097 (4.4043)  class_acc: 0.2083 (0.2180)  loss_scale: 65536.0000 (66622.2320)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1805  max mem: 15572
Epoch: [12]  [ 190/2809]  eta: 0:27:29  lr: 0.000042  min_lr: 0.000000  loss: 4.4384 (4.4062)  class_acc: 0.2083 (0.2190)  loss_scale: 65536.0000 (66565.3613)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1702  max mem: 15572
Epoch: [12]  [ 200/2809]  eta: 0:27:10  lr: 0.000042  min_lr: 0.000000  loss: 4.3935 (4.4039)  class_acc: 0.2083 (0.2201)  loss_scale: 65536.0000 (66514.1493)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0938  max mem: 15572
[2025-01-15 20:15:55,728] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33918
[2025-01-15 20:15:55,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:15:55,730] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 210/2809]  eta: 0:26:54  lr: 0.000042  min_lr: 0.000000  loss: 4.3248 (4.4003)  class_acc: 0.2083 (0.2216)  loss_scale: 65536.0000 (66312.4929)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0964  max mem: 15572
Epoch: [12]  [ 220/2809]  eta: 0:26:32  lr: 0.000042  min_lr: 0.000000  loss: 4.3084 (4.4001)  class_acc: 0.1667 (0.2215)  loss_scale: 32768.0000 (64794.6425)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0634  max mem: 15572
Epoch: [12]  [ 230/2809]  eta: 0:26:18  lr: 0.000042  min_lr: 0.000000  loss: 4.3836 (4.4034)  class_acc: 0.2083 (0.2240)  loss_scale: 32768.0000 (63408.2078)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0638  max mem: 15572
Epoch: [12]  [ 240/2809]  eta: 0:26:09  lr: 0.000042  min_lr: 0.000000  loss: 4.4884 (4.4086)  class_acc: 0.2500 (0.2268)  loss_scale: 32768.0000 (62136.8299)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1152  max mem: 15572
Epoch: [12]  [ 250/2809]  eta: 0:26:02  lr: 0.000042  min_lr: 0.000000  loss: 4.3615 (4.4018)  class_acc: 0.2083 (0.2278)  loss_scale: 32768.0000 (60966.7570)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1433  max mem: 15572
Epoch: [12]  [ 260/2809]  eta: 0:25:50  lr: 0.000042  min_lr: 0.000000  loss: 4.3000 (4.4013)  class_acc: 0.2083 (0.2283)  loss_scale: 32768.0000 (59886.3448)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1304  max mem: 15572
Epoch: [12]  [ 270/2809]  eta: 0:25:37  lr: 0.000042  min_lr: 0.000000  loss: 4.5630 (4.4072)  class_acc: 0.1667 (0.2263)  loss_scale: 32768.0000 (58885.6679)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.1075  max mem: 15572
Epoch: [12]  [ 280/2809]  eta: 0:25:26  lr: 0.000042  min_lr: 0.000000  loss: 4.5845 (4.4082)  class_acc: 0.2083 (0.2266)  loss_scale: 32768.0000 (57956.2135)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0865  max mem: 15572
Epoch: [12]  [ 290/2809]  eta: 0:25:18  lr: 0.000042  min_lr: 0.000000  loss: 4.3900 (4.4091)  class_acc: 0.2083 (0.2258)  loss_scale: 32768.0000 (57090.6392)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0834  max mem: 15572
[2025-01-15 20:16:40,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=34000, skipped=216, lr=[4.095936585352162e-07, 4.095936585352162e-07, 5.851337979074517e-07, 5.851337979074517e-07, 8.359054255820739e-07, 8.359054255820739e-07, 1.1941506079743914e-06, 1.1941506079743914e-06, 1.7059294399634163e-06, 1.7059294399634163e-06, 2.437042057090595e-06, 2.437042057090595e-06, 3.4814886529865642e-06, 3.4814886529865642e-06, 4.973555218552235e-06, 4.973555218552235e-06, 7.10507888364605e-06, 7.10507888364605e-06, 1.0150112690922931e-05, 1.0150112690922931e-05, 1.4500160987032758e-05, 1.4500160987032758e-05, 2.0714515695761083e-05, 2.0714515695761083e-05, 2.9592165279658694e-05, 2.9592165279658694e-05, 4.227452182808385e-05, 4.227452182808385e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:16:40,437] [INFO] [timer.py:260:stop] epoch=0/micro_step=34000/global_step=34000, RunningAvgSamplesPerSec=27.50235547486748, CurrSamplesPerSec=32.833881525300114, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [ 300/2809]  eta: 0:25:09  lr: 0.000042  min_lr: 0.000000  loss: 4.2541 (4.4061)  class_acc: 0.2083 (0.2258)  loss_scale: 32768.0000 (56282.5781)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1322  max mem: 15572
Epoch: [12]  [ 310/2809]  eta: 0:25:03  lr: 0.000042  min_lr: 0.000000  loss: 4.3684 (4.4082)  class_acc: 0.2500 (0.2282)  loss_scale: 32768.0000 (55526.4823)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1614  max mem: 15572
Epoch: [12]  [ 320/2809]  eta: 0:24:54  lr: 0.000042  min_lr: 0.000000  loss: 4.3627 (4.4061)  class_acc: 0.2917 (0.2304)  loss_scale: 32768.0000 (54817.4953)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1331  max mem: 15572
Epoch: [12]  [ 330/2809]  eta: 0:24:44  lr: 0.000042  min_lr: 0.000000  loss: 4.3621 (4.4094)  class_acc: 0.2500 (0.2296)  loss_scale: 32768.0000 (54151.3474)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0960  max mem: 15572
[2025-01-15 20:17:07,118] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:17:07,118] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 340/2809]  eta: 0:24:42  lr: 0.000042  min_lr: 0.000000  loss: 4.3965 (4.4100)  class_acc: 0.2083 (0.2297)  loss_scale: 32768.0000 (53716.4575)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1512  max mem: 15572
Epoch: [12]  [ 350/2809]  eta: 0:24:31  lr: 0.000042  min_lr: 0.000000  loss: 4.3229 (4.4057)  class_acc: 0.2083 (0.2296)  loss_scale: 65536.0000 (54053.1966)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1567  max mem: 15572
Epoch: [12]  [ 360/2809]  eta: 0:24:21  lr: 0.000042  min_lr: 0.000000  loss: 4.3947 (4.4089)  class_acc: 0.2083 (0.2300)  loss_scale: 65536.0000 (54371.2798)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0977  max mem: 15572
Epoch: [12]  [ 370/2809]  eta: 0:24:17  lr: 0.000042  min_lr: 0.000000  loss: 4.4927 (4.4088)  class_acc: 0.2500 (0.2303)  loss_scale: 65536.0000 (54672.2156)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1461  max mem: 15572
Epoch: [12]  [ 380/2809]  eta: 0:24:10  lr: 0.000042  min_lr: 0.000000  loss: 4.3992 (4.4075)  class_acc: 0.2500 (0.2304)  loss_scale: 65536.0000 (54957.3543)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1600  max mem: 15572
Epoch: [12]  [ 390/2809]  eta: 0:24:02  lr: 0.000042  min_lr: 0.000000  loss: 4.3992 (4.4089)  class_acc: 0.2500 (0.2303)  loss_scale: 65536.0000 (55227.9079)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1118  max mem: 15572
Epoch: [12]  [ 400/2809]  eta: 0:23:58  lr: 0.000042  min_lr: 0.000000  loss: 4.2227 (4.4050)  class_acc: 0.2500 (0.2314)  loss_scale: 65536.0000 (55484.9676)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1325  max mem: 15572
Epoch: [12]  [ 410/2809]  eta: 0:23:48  lr: 0.000042  min_lr: 0.000000  loss: 4.3902 (4.4062)  class_acc: 0.2500 (0.2303)  loss_scale: 65536.0000 (55729.5182)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1185  max mem: 15572
Epoch: [12]  [ 420/2809]  eta: 0:23:43  lr: 0.000042  min_lr: 0.000000  loss: 4.5028 (4.4079)  class_acc: 0.2083 (0.2297)  loss_scale: 65536.0000 (55962.4513)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1213  max mem: 15572
Epoch: [12]  [ 430/2809]  eta: 0:23:40  lr: 0.000042  min_lr: 0.000000  loss: 4.4460 (4.4083)  class_acc: 0.2083 (0.2296)  loss_scale: 65536.0000 (56184.5754)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.1841  max mem: 15572
Epoch: [12]  [ 440/2809]  eta: 0:23:38  lr: 0.000042  min_lr: 0.000000  loss: 4.3447 (4.4056)  class_acc: 0.2083 (0.2292)  loss_scale: 65536.0000 (56396.6259)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.2054  max mem: 15572
Epoch: [12]  [ 450/2809]  eta: 0:23:27  lr: 0.000042  min_lr: 0.000000  loss: 4.4308 (4.4096)  class_acc: 0.2083 (0.2289)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1401  max mem: 15572
[2025-01-15 20:18:16,122] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34163
[2025-01-15 20:18:16,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:18:16,123] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 460/2809]  eta: 0:23:14  lr: 0.000042  min_lr: 0.000000  loss: 4.4573 (4.4082)  class_acc: 0.2500 (0.2301)  loss_scale: 65536.0000 (56366.6464)  weight_decay: 0.0500 (0.0500)  time: 0.4789  data: 0.0399  max mem: 15572
Epoch: [12]  [ 470/2809]  eta: 0:23:08  lr: 0.000042  min_lr: 0.000000  loss: 4.2686 (4.4038)  class_acc: 0.2917 (0.2312)  loss_scale: 32768.0000 (55865.6136)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.0818  max mem: 15572
Epoch: [12]  [ 480/2809]  eta: 0:23:02  lr: 0.000042  min_lr: 0.000000  loss: 4.4199 (4.4059)  class_acc: 0.2083 (0.2304)  loss_scale: 32768.0000 (55385.4137)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1469  max mem: 15572
Epoch: [12]  [ 490/2809]  eta: 0:22:51  lr: 0.000042  min_lr: 0.000000  loss: 4.4396 (4.4026)  class_acc: 0.2083 (0.2318)  loss_scale: 32768.0000 (54924.7739)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0881  max mem: 15572
Epoch: [12]  [ 500/2809]  eta: 0:22:42  lr: 0.000042  min_lr: 0.000000  loss: 4.3583 (4.4036)  class_acc: 0.2500 (0.2326)  loss_scale: 32768.0000 (54482.5230)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0445  max mem: 15572
Epoch: [12]  [ 510/2809]  eta: 0:22:36  lr: 0.000042  min_lr: 0.000000  loss: 4.3167 (4.4008)  class_acc: 0.2500 (0.2332)  loss_scale: 32768.0000 (54057.5812)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0962  max mem: 15572
Epoch: [12]  [ 520/2809]  eta: 0:22:28  lr: 0.000042  min_lr: 0.000000  loss: 4.1842 (4.3973)  class_acc: 0.2500 (0.2333)  loss_scale: 32768.0000 (53648.9520)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1093  max mem: 15572
Epoch: [12]  [ 530/2809]  eta: 0:22:22  lr: 0.000042  min_lr: 0.000000  loss: 4.2893 (4.3979)  class_acc: 0.2083 (0.2324)  loss_scale: 32768.0000 (53255.7137)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1123  max mem: 15572
Epoch: [12]  [ 540/2809]  eta: 0:22:16  lr: 0.000042  min_lr: 0.000000  loss: 4.3924 (4.3979)  class_acc: 0.2083 (0.2321)  loss_scale: 32768.0000 (52877.0129)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1363  max mem: 15572
Epoch: [12]  [ 550/2809]  eta: 0:22:10  lr: 0.000042  min_lr: 0.000000  loss: 4.3211 (4.3967)  class_acc: 0.2083 (0.2332)  loss_scale: 32768.0000 (52512.0581)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1306  max mem: 15572
Epoch: [12]  [ 560/2809]  eta: 0:22:04  lr: 0.000042  min_lr: 0.000000  loss: 4.4110 (4.3976)  class_acc: 0.2500 (0.2322)  loss_scale: 32768.0000 (52160.1141)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1331  max mem: 15572
Epoch: [12]  [ 570/2809]  eta: 0:21:55  lr: 0.000042  min_lr: 0.000000  loss: 4.4262 (4.3962)  class_acc: 0.2083 (0.2323)  loss_scale: 32768.0000 (51820.4974)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1092  max mem: 15572
Epoch: [12]  [ 580/2809]  eta: 0:21:47  lr: 0.000042  min_lr: 0.000000  loss: 4.3399 (4.3967)  class_acc: 0.2083 (0.2322)  loss_scale: 32768.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0819  max mem: 15572
[2025-01-15 20:19:27,290] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:19:27,291] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 590/2809]  eta: 0:21:41  lr: 0.000042  min_lr: 0.000000  loss: 4.3543 (4.3953)  class_acc: 0.2500 (0.2327)  loss_scale: 32768.0000 (51563.8579)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.1128  max mem: 15572
Epoch: [12]  [ 600/2809]  eta: 0:21:33  lr: 0.000042  min_lr: 0.000000  loss: 4.3810 (4.3964)  class_acc: 0.1667 (0.2318)  loss_scale: 65536.0000 (51796.3394)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1092  max mem: 15572
Epoch: [12]  [ 610/2809]  eta: 0:21:28  lr: 0.000042  min_lr: 0.000000  loss: 4.4221 (4.3957)  class_acc: 0.1667 (0.2323)  loss_scale: 65536.0000 (52021.2111)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1094  max mem: 15572
Epoch: [12]  [ 620/2809]  eta: 0:21:20  lr: 0.000042  min_lr: 0.000000  loss: 4.4572 (4.3969)  class_acc: 0.2083 (0.2320)  loss_scale: 65536.0000 (52238.8406)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.0919  max mem: 15572
[2025-01-15 20:19:49,900] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34332
[2025-01-15 20:19:49,900] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:19:49,900] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 630/2809]  eta: 0:21:17  lr: 0.000042  min_lr: 0.000000  loss: 4.4526 (4.3968)  class_acc: 0.2083 (0.2318)  loss_scale: 65536.0000 (52086.0602)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1142  max mem: 15572
Epoch: [12]  [ 640/2809]  eta: 0:21:08  lr: 0.000042  min_lr: 0.000000  loss: 4.3487 (4.3956)  class_acc: 0.1667 (0.2315)  loss_scale: 32768.0000 (51784.6864)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1023  max mem: 15572
Epoch: [12]  [ 650/2809]  eta: 0:21:02  lr: 0.000042  min_lr: 0.000000  loss: 4.3234 (4.3961)  class_acc: 0.1667 (0.2309)  loss_scale: 32768.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0593  max mem: 15572
Epoch: [12]  [ 660/2809]  eta: 0:20:55  lr: 0.000042  min_lr: 0.000000  loss: 4.4783 (4.3975)  class_acc: 0.2083 (0.2306)  loss_scale: 32768.0000 (51209.2950)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0856  max mem: 15572
Epoch: [12]  [ 670/2809]  eta: 0:20:50  lr: 0.000042  min_lr: 0.000000  loss: 4.4871 (4.3974)  class_acc: 0.2083 (0.2305)  loss_scale: 32768.0000 (50934.4620)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0985  max mem: 15572
Epoch: [12]  [ 680/2809]  eta: 0:20:40  lr: 0.000042  min_lr: 0.000000  loss: 4.3939 (4.3974)  class_acc: 0.2500 (0.2300)  loss_scale: 32768.0000 (50667.7004)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0652  max mem: 15572
Epoch: [12]  [ 690/2809]  eta: 0:20:31  lr: 0.000042  min_lr: 0.000000  loss: 4.3214 (4.3963)  class_acc: 0.2083 (0.2310)  loss_scale: 32768.0000 (50408.6599)  weight_decay: 0.0500 (0.0500)  time: 0.4759  data: 0.0039  max mem: 15572
Epoch: [12]  [ 700/2809]  eta: 0:20:28  lr: 0.000042  min_lr: 0.000000  loss: 4.4232 (4.3969)  class_acc: 0.2500 (0.2310)  loss_scale: 32768.0000 (50157.0100)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0987  max mem: 15572
Epoch: [12]  [ 710/2809]  eta: 0:20:19  lr: 0.000042  min_lr: 0.000000  loss: 4.4263 (4.3956)  class_acc: 0.2500 (0.2318)  loss_scale: 32768.0000 (49912.4388)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0954  max mem: 15572
Epoch: [12]  [ 720/2809]  eta: 0:20:11  lr: 0.000042  min_lr: 0.000000  loss: 4.2990 (4.3949)  class_acc: 0.2500 (0.2317)  loss_scale: 32768.0000 (49674.6519)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0008  max mem: 15572
Epoch: [12]  [ 730/2809]  eta: 0:20:05  lr: 0.000042  min_lr: 0.000000  loss: 4.3706 (4.3960)  class_acc: 0.2083 (0.2312)  loss_scale: 32768.0000 (49443.3707)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0569  max mem: 15572
Epoch: [12]  [ 740/2809]  eta: 0:20:00  lr: 0.000042  min_lr: 0.000000  loss: 4.4584 (4.3974)  class_acc: 0.2083 (0.2311)  loss_scale: 32768.0000 (49218.3320)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1042  max mem: 15572
Epoch: [12]  [ 750/2809]  eta: 0:19:55  lr: 0.000042  min_lr: 0.000000  loss: 4.5260 (4.3983)  class_acc: 0.2500 (0.2314)  loss_scale: 32768.0000 (48999.2863)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1195  max mem: 15572
[2025-01-15 20:21:03,337] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:21:03,338] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 760/2809]  eta: 0:19:49  lr: 0.000042  min_lr: 0.000000  loss: 4.4199 (4.3979)  class_acc: 0.2500 (0.2324)  loss_scale: 32768.0000 (49130.4704)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1181  max mem: 15572
[2025-01-15 20:21:07,084] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34469
[2025-01-15 20:21:07,084] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:21:07,085] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 770/2809]  eta: 0:19:44  lr: 0.000042  min_lr: 0.000000  loss: 4.3340 (4.3976)  class_acc: 0.2500 (0.2333)  loss_scale: 32768.0000 (48918.2464)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1160  max mem: 15572
Epoch: [12]  [ 780/2809]  eta: 0:19:35  lr: 0.000042  min_lr: 0.000000  loss: 4.4323 (4.3987)  class_acc: 0.2083 (0.2330)  loss_scale: 32768.0000 (48711.4571)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0817  max mem: 15572
Epoch: [12]  [ 790/2809]  eta: 0:19:31  lr: 0.000042  min_lr: 0.000000  loss: 4.4917 (4.4004)  class_acc: 0.2083 (0.2326)  loss_scale: 32768.0000 (48509.8963)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1224  max mem: 15572
Epoch: [12]  [ 800/2809]  eta: 0:19:27  lr: 0.000042  min_lr: 0.000000  loss: 4.4917 (4.4002)  class_acc: 0.2083 (0.2330)  loss_scale: 32768.0000 (48313.3683)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.2041  max mem: 15572
Epoch: [12]  [ 810/2809]  eta: 0:19:22  lr: 0.000042  min_lr: 0.000000  loss: 4.4096 (4.4011)  class_acc: 0.2500 (0.2328)  loss_scale: 32768.0000 (48121.6868)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1791  max mem: 15572
Epoch: [12]  [ 820/2809]  eta: 0:19:15  lr: 0.000042  min_lr: 0.000000  loss: 4.3997 (4.3990)  class_acc: 0.2500 (0.2330)  loss_scale: 32768.0000 (47934.6748)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1171  max mem: 15572
Epoch: [12]  [ 830/2809]  eta: 0:19:08  lr: 0.000042  min_lr: 0.000000  loss: 4.2985 (4.4006)  class_acc: 0.2083 (0.2327)  loss_scale: 32768.0000 (47752.1637)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0898  max mem: 15572
Epoch: [12]  [ 840/2809]  eta: 0:19:02  lr: 0.000042  min_lr: 0.000000  loss: 4.2985 (4.3989)  class_acc: 0.2083 (0.2328)  loss_scale: 32768.0000 (47573.9929)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1028  max mem: 15572
Epoch: [12]  [ 850/2809]  eta: 0:18:57  lr: 0.000042  min_lr: 0.000000  loss: 4.2967 (4.3994)  class_acc: 0.2083 (0.2321)  loss_scale: 32768.0000 (47400.0094)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1463  max mem: 15572
Epoch: [12]  [ 860/2809]  eta: 0:18:52  lr: 0.000042  min_lr: 0.000000  loss: 4.3482 (4.3994)  class_acc: 0.2083 (0.2327)  loss_scale: 32768.0000 (47230.0674)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1702  max mem: 15572
Epoch: [12]  [ 870/2809]  eta: 0:18:47  lr: 0.000042  min_lr: 0.000000  loss: 4.3482 (4.3999)  class_acc: 0.2500 (0.2324)  loss_scale: 32768.0000 (47064.0276)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1691  max mem: 15572
Epoch: [12]  [ 880/2809]  eta: 0:18:40  lr: 0.000042  min_lr: 0.000000  loss: 4.2762 (4.3986)  class_acc: 0.2083 (0.2325)  loss_scale: 32768.0000 (46901.7571)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1337  max mem: 15572
[2025-01-15 20:22:23,383] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:22:23,383] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 890/2809]  eta: 0:18:36  lr: 0.000042  min_lr: 0.000000  loss: 4.3474 (4.3983)  class_acc: 0.2083 (0.2326)  loss_scale: 32768.0000 (46779.9057)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1522  max mem: 15572
Epoch: [12]  [ 900/2809]  eta: 0:18:28  lr: 0.000042  min_lr: 0.000000  loss: 4.3474 (4.3973)  class_acc: 0.2083 (0.2328)  loss_scale: 65536.0000 (46988.0755)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1193  max mem: 15572
Epoch: [12]  [ 910/2809]  eta: 0:18:20  lr: 0.000042  min_lr: 0.000000  loss: 4.2795 (4.3959)  class_acc: 0.2083 (0.2332)  loss_scale: 65536.0000 (47191.6751)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0463  max mem: 15572
Epoch: [12]  [ 920/2809]  eta: 0:18:14  lr: 0.000042  min_lr: 0.000000  loss: 4.3011 (4.3950)  class_acc: 0.2500 (0.2334)  loss_scale: 65536.0000 (47390.8534)  weight_decay: 0.0500 (0.0500)  time: 0.5208  data: 0.0969  max mem: 15572
Epoch: [12]  [ 930/2809]  eta: 0:18:07  lr: 0.000042  min_lr: 0.000000  loss: 4.3542 (4.3947)  class_acc: 0.2083 (0.2333)  loss_scale: 65536.0000 (47585.7530)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.1134  max mem: 15572
Epoch: [12]  [ 940/2809]  eta: 0:18:05  lr: 0.000042  min_lr: 0.000000  loss: 4.3417 (4.3938)  class_acc: 0.2083 (0.2334)  loss_scale: 65536.0000 (47776.5101)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.2025  max mem: 15572
[2025-01-15 20:22:56,104] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34657
[2025-01-15 20:22:56,104] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:22:56,104] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 950/2809]  eta: 0:17:58  lr: 0.000042  min_lr: 0.000000  loss: 4.2692 (4.3927)  class_acc: 0.2500 (0.2341)  loss_scale: 65536.0000 (47894.3428)  weight_decay: 0.0500 (0.0500)  time: 0.6465  data: 0.1745  max mem: 15572
Epoch: [12]  [ 960/2809]  eta: 0:17:50  lr: 0.000042  min_lr: 0.000000  loss: 4.3776 (4.3935)  class_acc: 0.2500 (0.2337)  loss_scale: 32768.0000 (47736.9407)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0500  max mem: 15572
Epoch: [12]  [ 970/2809]  eta: 0:17:46  lr: 0.000042  min_lr: 0.000000  loss: 4.3255 (4.3920)  class_acc: 0.2083 (0.2336)  loss_scale: 32768.0000 (47582.7806)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1216  max mem: 15572
Epoch: [12]  [ 980/2809]  eta: 0:17:39  lr: 0.000042  min_lr: 0.000000  loss: 4.3255 (4.3932)  class_acc: 0.2500 (0.2334)  loss_scale: 32768.0000 (47431.7635)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1633  max mem: 15572
Epoch: [12]  [ 990/2809]  eta: 0:17:32  lr: 0.000042  min_lr: 0.000000  loss: 4.4567 (4.3936)  class_acc: 0.2083 (0.2330)  loss_scale: 32768.0000 (47283.7941)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0732  max mem: 15572
Epoch: [12]  [1000/2809]  eta: 0:17:26  lr: 0.000042  min_lr: 0.000000  loss: 4.4442 (4.3956)  class_acc: 0.1667 (0.2326)  loss_scale: 32768.0000 (47138.7812)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0713  max mem: 15572
Epoch: [12]  [1010/2809]  eta: 0:17:21  lr: 0.000042  min_lr: 0.000000  loss: 4.5074 (4.3964)  class_acc: 0.1667 (0.2322)  loss_scale: 32768.0000 (46996.6370)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1637  max mem: 15572
Epoch: [12]  [1020/2809]  eta: 0:17:15  lr: 0.000042  min_lr: 0.000000  loss: 4.4805 (4.3962)  class_acc: 0.2083 (0.2320)  loss_scale: 32768.0000 (46857.2772)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1463  max mem: 15572
Epoch: [12]  [1030/2809]  eta: 0:17:09  lr: 0.000042  min_lr: 0.000000  loss: 4.3234 (4.3947)  class_acc: 0.2083 (0.2319)  loss_scale: 32768.0000 (46720.6208)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1163  max mem: 15572
Epoch: [12]  [1040/2809]  eta: 0:17:04  lr: 0.000042  min_lr: 0.000000  loss: 4.3377 (4.3955)  class_acc: 0.2083 (0.2316)  loss_scale: 32768.0000 (46586.5898)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1562  max mem: 15572
Epoch: [12]  [1050/2809]  eta: 0:16:56  lr: 0.000042  min_lr: 0.000000  loss: 4.4641 (4.3958)  class_acc: 0.2083 (0.2316)  loss_scale: 32768.0000 (46455.1094)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0971  max mem: 15572
Epoch: [12]  [1060/2809]  eta: 0:16:52  lr: 0.000042  min_lr: 0.000000  loss: 4.4129 (4.3953)  class_acc: 0.2083 (0.2315)  loss_scale: 32768.0000 (46326.1074)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1254  max mem: 15572
Epoch: [12]  [1070/2809]  eta: 0:16:48  lr: 0.000042  min_lr: 0.000000  loss: 4.4129 (4.3954)  class_acc: 0.1667 (0.2308)  loss_scale: 32768.0000 (46199.5145)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.2070  max mem: 15572
[2025-01-15 20:24:09,830] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:24:09,831] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [1080/2809]  eta: 0:16:40  lr: 0.000042  min_lr: 0.000000  loss: 4.4811 (4.3960)  class_acc: 0.1667 (0.2307)  loss_scale: 32768.0000 (46166.2017)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1194  max mem: 15572
Epoch: [12]  [1090/2809]  eta: 0:16:34  lr: 0.000042  min_lr: 0.000000  loss: 4.4531 (4.3960)  class_acc: 0.2500 (0.2312)  loss_scale: 65536.0000 (46343.7434)  weight_decay: 0.0500 (0.0500)  time: 0.5116  data: 0.0615  max mem: 15572
Epoch: [12]  [1100/2809]  eta: 0:16:29  lr: 0.000042  min_lr: 0.000000  loss: 4.3423 (4.3951)  class_acc: 0.2917 (0.2309)  loss_scale: 65536.0000 (46518.0599)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1354  max mem: 15572
Epoch: [12]  [1110/2809]  eta: 0:16:23  lr: 0.000042  min_lr: 0.000000  loss: 4.3423 (4.3954)  class_acc: 0.1667 (0.2306)  loss_scale: 65536.0000 (46689.2385)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1360  max mem: 15572
Epoch: [12]  [1120/2809]  eta: 0:16:17  lr: 0.000042  min_lr: 0.000000  loss: 4.4000 (4.3949)  class_acc: 0.2083 (0.2306)  loss_scale: 65536.0000 (46857.3631)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.1121  max mem: 15572
[2025-01-15 20:24:36,933] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34834
[2025-01-15 20:24:36,933] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:24:36,933] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [1130/2809]  eta: 0:16:10  lr: 0.000042  min_lr: 0.000000  loss: 4.3747 (4.3952)  class_acc: 0.1667 (0.2305)  loss_scale: 65536.0000 (46877.6516)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1057  max mem: 15572
Epoch: [12]  [1140/2809]  eta: 0:16:05  lr: 0.000042  min_lr: 0.000000  loss: 4.4735 (4.3958)  class_acc: 0.2500 (0.2308)  loss_scale: 32768.0000 (46753.9912)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1230  max mem: 15572
Epoch: [12]  [1150/2809]  eta: 0:15:59  lr: 0.000042  min_lr: 0.000000  loss: 4.3636 (4.3961)  class_acc: 0.2500 (0.2306)  loss_scale: 32768.0000 (46632.4796)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1371  max mem: 15572
Epoch: [12]  [1160/2809]  eta: 0:15:53  lr: 0.000042  min_lr: 0.000000  loss: 4.3416 (4.3960)  class_acc: 0.2083 (0.2305)  loss_scale: 32768.0000 (46513.0612)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1292  max mem: 15572
Epoch: [12]  [1170/2809]  eta: 0:15:49  lr: 0.000042  min_lr: 0.000000  loss: 4.3475 (4.3966)  class_acc: 0.2083 (0.2303)  loss_scale: 32768.0000 (46395.6823)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1792  max mem: 15572
Epoch: [12]  [1180/2809]  eta: 0:15:42  lr: 0.000042  min_lr: 0.000000  loss: 4.3681 (4.3954)  class_acc: 0.2083 (0.2303)  loss_scale: 32768.0000 (46280.2913)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1346  max mem: 15572
Epoch: [12]  [1190/2809]  eta: 0:15:37  lr: 0.000042  min_lr: 0.000000  loss: 4.3738 (4.3959)  class_acc: 0.1667 (0.2299)  loss_scale: 32768.0000 (46166.8380)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1125  max mem: 15572
Epoch: [12]  [1200/2809]  eta: 0:15:30  lr: 0.000042  min_lr: 0.000000  loss: 4.3849 (4.3956)  class_acc: 0.1667 (0.2301)  loss_scale: 32768.0000 (46055.2739)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0795  max mem: 15572
Epoch: [12]  [1210/2809]  eta: 0:15:23  lr: 0.000042  min_lr: 0.000000  loss: 4.3163 (4.3954)  class_acc: 0.2083 (0.2299)  loss_scale: 32768.0000 (45945.5524)  weight_decay: 0.0500 (0.0500)  time: 0.4711  data: 0.0166  max mem: 15572
Epoch: [12]  [1220/2809]  eta: 0:15:16  lr: 0.000042  min_lr: 0.000000  loss: 4.4160 (4.3955)  class_acc: 0.2083 (0.2300)  loss_scale: 32768.0000 (45837.6282)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0660  max mem: 15572
Epoch: [12]  [1230/2809]  eta: 0:15:11  lr: 0.000042  min_lr: 0.000000  loss: 4.3901 (4.3952)  class_acc: 0.1667 (0.2298)  loss_scale: 32768.0000 (45731.4574)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1066  max mem: 15572
Epoch: [12]  [1240/2809]  eta: 0:15:04  lr: 0.000042  min_lr: 0.000000  loss: 4.3850 (4.3958)  class_acc: 0.2083 (0.2294)  loss_scale: 32768.0000 (45626.9976)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0846  max mem: 15572
Epoch: [12]  [1250/2809]  eta: 0:14:58  lr: 0.000042  min_lr: 0.000000  loss: 4.3850 (4.3958)  class_acc: 0.1250 (0.2286)  loss_scale: 32768.0000 (45524.2078)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0913  max mem: 15572
[2025-01-15 20:25:49,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:25:49,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [1260/2809]  eta: 0:14:53  lr: 0.000042  min_lr: 0.000000  loss: 4.3541 (4.3951)  class_acc: 0.1250 (0.2286)  loss_scale: 32768.0000 (45578.9627)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1119  max mem: 15572
[2025-01-15 20:25:54,476] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34970
[2025-01-15 20:25:54,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:25:54,476] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [1270/2809]  eta: 0:14:47  lr: 0.000042  min_lr: 0.000000  loss: 4.3541 (4.3946)  class_acc: 0.2500 (0.2290)  loss_scale: 32768.0000 (45503.9496)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1229  max mem: 15572
Epoch: [12]  [1280/2809]  eta: 0:14:42  lr: 0.000042  min_lr: 0.000000  loss: 4.4812 (4.3958)  class_acc: 0.2083 (0.2287)  loss_scale: 32768.0000 (45404.5277)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1570  max mem: 15572
Epoch: [12]  [1290/2809]  eta: 0:14:35  lr: 0.000042  min_lr: 0.000000  loss: 4.5208 (4.3971)  class_acc: 0.1667 (0.2283)  loss_scale: 32768.0000 (45306.6460)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1117  max mem: 15572
[2025-01-15 20:26:11,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=35000, skipped=222, lr=[4.051861021080781e-07, 4.051861021080781e-07, 5.788372887258259e-07, 5.788372887258259e-07, 8.269104124654657e-07, 8.269104124654657e-07, 1.1813005892363797e-06, 1.1813005892363797e-06, 1.6875722703376853e-06, 1.6875722703376853e-06, 2.4108175290538364e-06, 2.4108175290538364e-06, 3.4440250415054805e-06, 3.4440250415054805e-06, 4.920035773579258e-06, 4.920035773579258e-06, 7.028622533684655e-06, 7.028622533684655e-06, 1.0040889333835223e-05, 1.0040889333835223e-05, 1.4344127619764603e-05, 1.4344127619764603e-05, 2.0491610885378006e-05, 2.0491610885378006e-05, 2.9273729836254297e-05, 2.9273729836254297e-05, 4.181961405179186e-05, 4.181961405179186e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:26:11,714] [INFO] [timer.py:260:stop] epoch=0/micro_step=35000/global_step=35000, RunningAvgSamplesPerSec=27.517107089440778, CurrSamplesPerSec=31.362740937785663, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [1300/2809]  eta: 0:14:30  lr: 0.000042  min_lr: 0.000000  loss: 4.4230 (4.3960)  class_acc: 0.2500 (0.2287)  loss_scale: 32768.0000 (45210.2690)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1353  max mem: 15572
Epoch: [12]  [1310/2809]  eta: 0:14:24  lr: 0.000042  min_lr: 0.000000  loss: 4.3126 (4.3953)  class_acc: 0.2500 (0.2289)  loss_scale: 32768.0000 (45115.3623)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1699  max mem: 15572
Epoch: [12]  [1320/2809]  eta: 0:14:18  lr: 0.000042  min_lr: 0.000000  loss: 4.3909 (4.3960)  class_acc: 0.2083 (0.2286)  loss_scale: 32768.0000 (45021.8925)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1247  max mem: 15572
Epoch: [12]  [1330/2809]  eta: 0:14:13  lr: 0.000042  min_lr: 0.000000  loss: 4.4756 (4.3959)  class_acc: 0.1667 (0.2283)  loss_scale: 32768.0000 (44929.8272)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1352  max mem: 15572
Epoch: [12]  [1340/2809]  eta: 0:14:06  lr: 0.000042  min_lr: 0.000000  loss: 4.2359 (4.3947)  class_acc: 0.2083 (0.2285)  loss_scale: 32768.0000 (44839.1350)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0936  max mem: 15572
Epoch: [12]  [1350/2809]  eta: 0:14:00  lr: 0.000042  min_lr: 0.000000  loss: 4.2890 (4.3951)  class_acc: 0.2500 (0.2285)  loss_scale: 32768.0000 (44749.7853)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0712  max mem: 15572
Epoch: [12]  [1360/2809]  eta: 0:13:54  lr: 0.000042  min_lr: 0.000000  loss: 4.4349 (4.3954)  class_acc: 0.2500 (0.2286)  loss_scale: 32768.0000 (44661.7487)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0727  max mem: 15572
Epoch: [12]  [1370/2809]  eta: 0:13:48  lr: 0.000042  min_lr: 0.000000  loss: 4.3715 (4.3949)  class_acc: 0.2083 (0.2285)  loss_scale: 32768.0000 (44574.9964)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0820  max mem: 15572
Epoch: [12]  [1380/2809]  eta: 0:13:43  lr: 0.000042  min_lr: 0.000000  loss: 4.4522 (4.3958)  class_acc: 0.2083 (0.2284)  loss_scale: 32768.0000 (44489.5004)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1564  max mem: 15572
Epoch: [12]  [1390/2809]  eta: 0:13:36  lr: 0.000042  min_lr: 0.000000  loss: 4.4537 (4.3958)  class_acc: 0.2083 (0.2284)  loss_scale: 32768.0000 (44405.2336)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1270  max mem: 15572
[2025-01-15 20:27:07,944] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:27:07,945] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [1400/2809]  eta: 0:13:31  lr: 0.000042  min_lr: 0.000000  loss: 4.3469 (4.3954)  class_acc: 0.1667 (0.2282)  loss_scale: 32768.0000 (44556.0600)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1166  max mem: 15572
Epoch: [12]  [1410/2809]  eta: 0:13:26  lr: 0.000042  min_lr: 0.000000  loss: 4.4037 (4.3960)  class_acc: 0.2083 (0.2280)  loss_scale: 65536.0000 (44704.7484)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1632  max mem: 15572
Epoch: [12]  [1420/2809]  eta: 0:13:19  lr: 0.000042  min_lr: 0.000000  loss: 4.4150 (4.3958)  class_acc: 0.1667 (0.2279)  loss_scale: 65536.0000 (44851.3441)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0789  max mem: 15572
[2025-01-15 20:27:26,262] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35134
[2025-01-15 20:27:26,262] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:27:26,262] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [1430/2809]  eta: 0:13:13  lr: 0.000042  min_lr: 0.000000  loss: 4.4425 (4.3970)  class_acc: 0.1667 (0.2278)  loss_scale: 65536.0000 (44881.3976)  weight_decay: 0.0500 (0.0500)  time: 0.4913  data: 0.0310  max mem: 15572
Epoch: [12]  [1440/2809]  eta: 0:13:07  lr: 0.000042  min_lr: 0.000000  loss: 4.5232 (4.3973)  class_acc: 0.2083 (0.2276)  loss_scale: 32768.0000 (44797.3352)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0935  max mem: 15572
Epoch: [12]  [1450/2809]  eta: 0:13:01  lr: 0.000042  min_lr: 0.000000  loss: 4.4231 (4.3968)  class_acc: 0.2083 (0.2275)  loss_scale: 32768.0000 (44714.4314)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1083  max mem: 15572
Epoch: [12]  [1460/2809]  eta: 0:12:55  lr: 0.000042  min_lr: 0.000000  loss: 4.4235 (4.3970)  class_acc: 0.2083 (0.2274)  loss_scale: 32768.0000 (44632.6626)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1075  max mem: 15572
Epoch: [12]  [1470/2809]  eta: 0:12:49  lr: 0.000042  min_lr: 0.000000  loss: 4.4233 (4.3967)  class_acc: 0.2083 (0.2272)  loss_scale: 32768.0000 (44552.0054)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.0986  max mem: 15572
Epoch: [12]  [1480/2809]  eta: 0:12:44  lr: 0.000042  min_lr: 0.000000  loss: 4.4233 (4.3968)  class_acc: 0.1250 (0.2265)  loss_scale: 32768.0000 (44472.4375)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1339  max mem: 15572
Epoch: [12]  [1490/2809]  eta: 0:12:39  lr: 0.000042  min_lr: 0.000000  loss: 4.4585 (4.3979)  class_acc: 0.1250 (0.2263)  loss_scale: 32768.0000 (44393.9370)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1986  max mem: 15572
Epoch: [12]  [1500/2809]  eta: 0:12:33  lr: 0.000042  min_lr: 0.000000  loss: 4.5218 (4.3983)  class_acc: 0.2083 (0.2263)  loss_scale: 32768.0000 (44316.4823)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1608  max mem: 15572
Epoch: [12]  [1510/2809]  eta: 0:12:27  lr: 0.000042  min_lr: 0.000000  loss: 4.4597 (4.3991)  class_acc: 0.2083 (0.2261)  loss_scale: 32768.0000 (44240.0529)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1091  max mem: 15572
Epoch: [12]  [1520/2809]  eta: 0:12:21  lr: 0.000042  min_lr: 0.000000  loss: 4.4597 (4.3990)  class_acc: 0.2500 (0.2263)  loss_scale: 32768.0000 (44164.6285)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1171  max mem: 15572
Epoch: [12]  [1530/2809]  eta: 0:12:15  lr: 0.000042  min_lr: 0.000000  loss: 4.3273 (4.3985)  class_acc: 0.2500 (0.2265)  loss_scale: 32768.0000 (44090.1894)  weight_decay: 0.0500 (0.0500)  time: 0.5158  data: 0.0730  max mem: 15572
Epoch: [12]  [1540/2809]  eta: 0:12:10  lr: 0.000042  min_lr: 0.000000  loss: 4.3180 (4.3988)  class_acc: 0.2083 (0.2263)  loss_scale: 32768.0000 (44016.7164)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1005  max mem: 15572
Epoch: [12]  [1550/2809]  eta: 0:12:03  lr: 0.000042  min_lr: 0.000000  loss: 4.4364 (4.3994)  class_acc: 0.2083 (0.2265)  loss_scale: 32768.0000 (43944.1908)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1283  max mem: 15572
[2025-01-15 20:28:39,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:28:39,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [1560/2809]  eta: 0:11:57  lr: 0.000042  min_lr: 0.000000  loss: 4.4364 (4.3998)  class_acc: 0.2500 (0.2264)  loss_scale: 32768.0000 (43998.5445)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0882  max mem: 15572
Epoch: [12]  [1570/2809]  eta: 0:11:51  lr: 0.000042  min_lr: 0.000000  loss: 4.3088 (4.3992)  class_acc: 0.2500 (0.2267)  loss_scale: 65536.0000 (44135.6384)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0621  max mem: 15572
Epoch: [12]  [1580/2809]  eta: 0:11:46  lr: 0.000042  min_lr: 0.000000  loss: 4.2954 (4.3990)  class_acc: 0.1667 (0.2263)  loss_scale: 65536.0000 (44270.9981)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1193  max mem: 15572
Epoch: [12]  [1590/2809]  eta: 0:11:40  lr: 0.000042  min_lr: 0.000000  loss: 4.3878 (4.3989)  class_acc: 0.1667 (0.2263)  loss_scale: 65536.0000 (44404.6562)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1754  max mem: 15572
[2025-01-15 20:29:00,107] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35299
[2025-01-15 20:29:00,107] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:29:00,107] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [1600/2809]  eta: 0:11:34  lr: 0.000042  min_lr: 0.000000  loss: 4.3878 (4.3989)  class_acc: 0.1667 (0.2262)  loss_scale: 32768.0000 (44331.9725)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0978  max mem: 15572
Epoch: [12]  [1610/2809]  eta: 0:11:27  lr: 0.000042  min_lr: 0.000000  loss: 4.3435 (4.3986)  class_acc: 0.2500 (0.2266)  loss_scale: 32768.0000 (44260.1912)  weight_decay: 0.0500 (0.0500)  time: 0.4906  data: 0.0349  max mem: 15572
Epoch: [12]  [1620/2809]  eta: 0:11:21  lr: 0.000042  min_lr: 0.000000  loss: 4.4036 (4.3990)  class_acc: 0.2500 (0.2263)  loss_scale: 32768.0000 (44189.2955)  weight_decay: 0.0500 (0.0500)  time: 0.4680  data: 0.0131  max mem: 15572
Epoch: [12]  [1630/2809]  eta: 0:11:16  lr: 0.000042  min_lr: 0.000000  loss: 4.3803 (4.3985)  class_acc: 0.2083 (0.2266)  loss_scale: 32768.0000 (44119.2692)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.1139  max mem: 15572
Epoch: [12]  [1640/2809]  eta: 0:11:09  lr: 0.000042  min_lr: 0.000000  loss: 4.3129 (4.3982)  class_acc: 0.2917 (0.2268)  loss_scale: 32768.0000 (44050.0963)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.1113  max mem: 15572
Epoch: [12]  [1650/2809]  eta: 0:11:04  lr: 0.000042  min_lr: 0.000000  loss: 4.3643 (4.3979)  class_acc: 0.2917 (0.2270)  loss_scale: 32768.0000 (43981.7614)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1237  max mem: 15572
Epoch: [12]  [1660/2809]  eta: 0:10:59  lr: 0.000042  min_lr: 0.000000  loss: 4.4363 (4.3982)  class_acc: 0.2500 (0.2270)  loss_scale: 32768.0000 (43914.2492)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.2030  max mem: 15572
Epoch: [12]  [1670/2809]  eta: 0:10:52  lr: 0.000042  min_lr: 0.000000  loss: 4.4017 (4.3978)  class_acc: 0.2500 (0.2272)  loss_scale: 32768.0000 (43847.5452)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0900  max mem: 15572
Epoch: [12]  [1680/2809]  eta: 0:10:46  lr: 0.000042  min_lr: 0.000000  loss: 4.4017 (4.3980)  class_acc: 0.2083 (0.2274)  loss_scale: 32768.0000 (43781.6347)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0544  max mem: 15572
Epoch: [12]  [1690/2809]  eta: 0:10:40  lr: 0.000042  min_lr: 0.000000  loss: 4.3786 (4.3974)  class_acc: 0.2083 (0.2278)  loss_scale: 32768.0000 (43716.5038)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0766  max mem: 15572
Epoch: [12]  [1700/2809]  eta: 0:10:34  lr: 0.000042  min_lr: 0.000000  loss: 4.2653 (4.3963)  class_acc: 0.2083 (0.2279)  loss_scale: 32768.0000 (43652.1387)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0934  max mem: 15572
Epoch: [12]  [1710/2809]  eta: 0:10:28  lr: 0.000042  min_lr: 0.000000  loss: 4.3788 (4.3966)  class_acc: 0.2083 (0.2280)  loss_scale: 32768.0000 (43588.5260)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1019  max mem: 15572
[2025-01-15 20:30:09,623] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:30:09,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [1720/2809]  eta: 0:10:22  lr: 0.000042  min_lr: 0.000000  loss: 4.4823 (4.3965)  class_acc: 0.2500 (0.2281)  loss_scale: 32768.0000 (43544.6926)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1102  max mem: 15572
Epoch: [12]  [1730/2809]  eta: 0:10:16  lr: 0.000042  min_lr: 0.000000  loss: 4.4980 (4.3967)  class_acc: 0.1667 (0.2280)  loss_scale: 65536.0000 (43671.7366)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0972  max mem: 15572
[2025-01-15 20:30:19,917] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35447
[2025-01-15 20:30:19,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:30:19,917] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [1740/2809]  eta: 0:10:11  lr: 0.000042  min_lr: 0.000000  loss: 4.5103 (4.3963)  class_acc: 0.2083 (0.2281)  loss_scale: 65536.0000 (43759.6783)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0970  max mem: 15572
Epoch: [12]  [1750/2809]  eta: 0:10:05  lr: 0.000042  min_lr: 0.000000  loss: 4.4163 (4.3962)  class_acc: 0.2500 (0.2283)  loss_scale: 32768.0000 (43696.9046)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1695  max mem: 15572
Epoch: [12]  [1760/2809]  eta: 0:09:59  lr: 0.000042  min_lr: 0.000000  loss: 4.3552 (4.3959)  class_acc: 0.2917 (0.2286)  loss_scale: 32768.0000 (43634.8438)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1384  max mem: 15572
Epoch: [12]  [1770/2809]  eta: 0:09:53  lr: 0.000042  min_lr: 0.000000  loss: 4.4681 (4.3966)  class_acc: 0.2500 (0.2287)  loss_scale: 32768.0000 (43573.4839)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.0661  max mem: 15572
Epoch: [12]  [1780/2809]  eta: 0:09:47  lr: 0.000042  min_lr: 0.000000  loss: 4.5185 (4.3977)  class_acc: 0.1667 (0.2283)  loss_scale: 32768.0000 (43512.8130)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0994  max mem: 15572
Epoch: [12]  [1790/2809]  eta: 0:09:42  lr: 0.000042  min_lr: 0.000000  loss: 4.4799 (4.3976)  class_acc: 0.1667 (0.2283)  loss_scale: 32768.0000 (43452.8197)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1438  max mem: 15572
Epoch: [12]  [1800/2809]  eta: 0:09:36  lr: 0.000042  min_lr: 0.000000  loss: 4.2942 (4.3974)  class_acc: 0.2500 (0.2285)  loss_scale: 32768.0000 (43393.4925)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1493  max mem: 15572
Epoch: [12]  [1810/2809]  eta: 0:09:31  lr: 0.000042  min_lr: 0.000000  loss: 4.4648 (4.3981)  class_acc: 0.2500 (0.2285)  loss_scale: 32768.0000 (43334.8205)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1821  max mem: 15572
Epoch: [12]  [1820/2809]  eta: 0:09:25  lr: 0.000042  min_lr: 0.000000  loss: 4.4262 (4.3980)  class_acc: 0.2083 (0.2286)  loss_scale: 32768.0000 (43276.7930)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1645  max mem: 15572
Epoch: [12]  [1830/2809]  eta: 0:09:20  lr: 0.000042  min_lr: 0.000000  loss: 4.2739 (4.3975)  class_acc: 0.2500 (0.2287)  loss_scale: 32768.0000 (43219.3992)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1867  max mem: 15572
Epoch: [12]  [1840/2809]  eta: 0:09:14  lr: 0.000042  min_lr: 0.000000  loss: 4.3971 (4.3974)  class_acc: 0.2083 (0.2287)  loss_scale: 32768.0000 (43162.6290)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1743  max mem: 15572
Epoch: [12]  [1850/2809]  eta: 0:09:09  lr: 0.000042  min_lr: 0.000000  loss: 4.3983 (4.3974)  class_acc: 0.2083 (0.2290)  loss_scale: 32768.0000 (43106.4722)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1805  max mem: 15572
Epoch: [12]  [1860/2809]  eta: 0:09:03  lr: 0.000042  min_lr: 0.000000  loss: 4.4455 (4.3978)  class_acc: 0.2917 (0.2292)  loss_scale: 32768.0000 (43050.9189)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1304  max mem: 15572
[2025-01-15 20:31:34,738] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:31:34,739] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [1870/2809]  eta: 0:08:57  lr: 0.000042  min_lr: 0.000000  loss: 4.4811 (4.3980)  class_acc: 0.1667 (0.2288)  loss_scale: 32768.0000 (43048.5003)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0584  max mem: 15572
Epoch: [12]  [1880/2809]  eta: 0:08:51  lr: 0.000042  min_lr: 0.000000  loss: 4.3705 (4.3976)  class_acc: 0.2083 (0.2288)  loss_scale: 65536.0000 (43168.0510)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0936  max mem: 15572
Epoch: [12]  [1890/2809]  eta: 0:08:45  lr: 0.000042  min_lr: 0.000000  loss: 4.3686 (4.3981)  class_acc: 0.2500 (0.2287)  loss_scale: 65536.0000 (43286.3374)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1375  max mem: 15572
Epoch: [12]  [1900/2809]  eta: 0:08:40  lr: 0.000042  min_lr: 0.000000  loss: 4.4619 (4.3987)  class_acc: 0.1667 (0.2282)  loss_scale: 65536.0000 (43403.3793)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1422  max mem: 15572
Epoch: [12]  [1910/2809]  eta: 0:08:33  lr: 0.000042  min_lr: 0.000000  loss: 4.4611 (4.3988)  class_acc: 0.1250 (0.2278)  loss_scale: 65536.0000 (43519.1962)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0654  max mem: 15572
Epoch: [12]  [1920/2809]  eta: 0:08:28  lr: 0.000042  min_lr: 0.000000  loss: 4.4870 (4.3995)  class_acc: 0.1667 (0.2276)  loss_scale: 65536.0000 (43633.8074)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1022  max mem: 15572
Epoch: [12]  [1930/2809]  eta: 0:08:22  lr: 0.000042  min_lr: 0.000000  loss: 4.4870 (4.3999)  class_acc: 0.2083 (0.2275)  loss_scale: 65536.0000 (43747.2315)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0834  max mem: 15572
Epoch: [12]  [1940/2809]  eta: 0:08:16  lr: 0.000042  min_lr: 0.000000  loss: 4.3305 (4.3994)  class_acc: 0.2500 (0.2278)  loss_scale: 65536.0000 (43859.4869)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0444  max mem: 15572
Epoch: [12]  [1950/2809]  eta: 0:08:10  lr: 0.000042  min_lr: 0.000000  loss: 4.2874 (4.3994)  class_acc: 0.1667 (0.2275)  loss_scale: 65536.0000 (43970.5915)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0759  max mem: 15572
Epoch: [12]  [1960/2809]  eta: 0:08:04  lr: 0.000042  min_lr: 0.000000  loss: 4.2874 (4.3986)  class_acc: 0.2500 (0.2279)  loss_scale: 65536.0000 (44080.5630)  weight_decay: 0.0500 (0.0500)  time: 0.5237  data: 0.0828  max mem: 15572
Epoch: [12]  [1970/2809]  eta: 0:07:58  lr: 0.000042  min_lr: 0.000000  loss: 4.3045 (4.3984)  class_acc: 0.2917 (0.2282)  loss_scale: 65536.0000 (44189.4186)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1240  max mem: 15572
Epoch: [12]  [1980/2809]  eta: 0:07:53  lr: 0.000041  min_lr: 0.000000  loss: 4.3654 (4.3988)  class_acc: 0.2500 (0.2283)  loss_scale: 65536.0000 (44297.1752)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1262  max mem: 15572
Epoch: [12]  [1990/2809]  eta: 0:07:47  lr: 0.000041  min_lr: 0.000000  loss: 4.4499 (4.3990)  class_acc: 0.2083 (0.2281)  loss_scale: 65536.0000 (44403.8493)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1171  max mem: 15572
[2025-01-15 20:32:43,995] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:32:43,995] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:32:45,977] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35705
[2025-01-15 20:32:45,977] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:32:45,978] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2000/2809]  eta: 0:07:41  lr: 0.000041  min_lr: 0.000000  loss: 4.4513 (4.3990)  class_acc: 0.2083 (0.2281)  loss_scale: 65536.0000 (44542.2089)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1587  max mem: 15572
Epoch: [12]  [2010/2809]  eta: 0:07:36  lr: 0.000041  min_lr: 0.000000  loss: 4.2366 (4.3982)  class_acc: 0.2500 (0.2281)  loss_scale: 65536.0000 (44646.6037)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1606  max mem: 15572
Epoch: [12]  [2020/2809]  eta: 0:07:30  lr: 0.000041  min_lr: 0.000000  loss: 4.3016 (4.3986)  class_acc: 0.2500 (0.2282)  loss_scale: 65536.0000 (44749.9654)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1497  max mem: 15572
Epoch: [12]  [2030/2809]  eta: 0:07:25  lr: 0.000041  min_lr: 0.000000  loss: 4.2983 (4.3976)  class_acc: 0.2500 (0.2285)  loss_scale: 65536.0000 (44852.3092)  weight_decay: 0.0500 (0.0500)  time: 0.6733  data: 0.2129  max mem: 15572
Epoch: [12]  [2040/2809]  eta: 0:07:19  lr: 0.000041  min_lr: 0.000000  loss: 4.3218 (4.3979)  class_acc: 0.2500 (0.2285)  loss_scale: 65536.0000 (44953.6502)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1292  max mem: 15572
Epoch: [12]  [2050/2809]  eta: 0:07:13  lr: 0.000041  min_lr: 0.000000  loss: 4.3925 (4.3980)  class_acc: 0.2500 (0.2287)  loss_scale: 65536.0000 (45054.0029)  weight_decay: 0.0500 (0.0500)  time: 0.4951  data: 0.0378  max mem: 15572
Epoch: [12]  [2060/2809]  eta: 0:07:07  lr: 0.000041  min_lr: 0.000000  loss: 4.2822 (4.3968)  class_acc: 0.2500 (0.2289)  loss_scale: 65536.0000 (45153.3819)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0878  max mem: 15572
Epoch: [12]  [2070/2809]  eta: 0:07:01  lr: 0.000041  min_lr: 0.000000  loss: 4.2875 (4.3972)  class_acc: 0.2500 (0.2289)  loss_scale: 65536.0000 (45251.8011)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1039  max mem: 15572
Epoch: [12]  [2080/2809]  eta: 0:06:56  lr: 0.000041  min_lr: 0.000000  loss: 4.4367 (4.3975)  class_acc: 0.2083 (0.2288)  loss_scale: 65536.0000 (45349.2744)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1175  max mem: 15572
Epoch: [12]  [2090/2809]  eta: 0:06:50  lr: 0.000041  min_lr: 0.000000  loss: 4.4411 (4.3980)  class_acc: 0.2083 (0.2287)  loss_scale: 65536.0000 (45445.8154)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.1026  max mem: 15572
Epoch: [12]  [2100/2809]  eta: 0:06:44  lr: 0.000041  min_lr: 0.000000  loss: 4.3963 (4.3972)  class_acc: 0.2083 (0.2289)  loss_scale: 65536.0000 (45541.4374)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0653  max mem: 15572
Epoch: [12]  [2110/2809]  eta: 0:06:38  lr: 0.000041  min_lr: 0.000000  loss: 4.2817 (4.3970)  class_acc: 0.2500 (0.2288)  loss_scale: 65536.0000 (45636.1535)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0916  max mem: 15572
Epoch: [12]  [2120/2809]  eta: 0:06:33  lr: 0.000041  min_lr: 0.000000  loss: 4.4143 (4.3979)  class_acc: 0.2083 (0.2287)  loss_scale: 65536.0000 (45729.9764)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1247  max mem: 15572
[2025-01-15 20:33:58,602] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:33:58,602] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:34:00,797] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35838
[2025-01-15 20:34:00,797] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:34:00,798] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2130/2809]  eta: 0:06:27  lr: 0.000041  min_lr: 0.000000  loss: 4.5399 (4.3982)  class_acc: 0.2083 (0.2288)  loss_scale: 65536.0000 (45945.9334)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0818  max mem: 15572
Epoch: [12]  [2140/2809]  eta: 0:06:21  lr: 0.000041  min_lr: 0.000000  loss: 4.3180 (4.3975)  class_acc: 0.2500 (0.2289)  loss_scale: 65536.0000 (46037.4330)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1318  max mem: 15572
Epoch: [12]  [2150/2809]  eta: 0:06:15  lr: 0.000041  min_lr: 0.000000  loss: 4.3118 (4.3970)  class_acc: 0.2083 (0.2289)  loss_scale: 65536.0000 (46128.0818)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1234  max mem: 15572
Epoch: [12]  [2160/2809]  eta: 0:06:10  lr: 0.000041  min_lr: 0.000000  loss: 4.3161 (4.3968)  class_acc: 0.2083 (0.2290)  loss_scale: 65536.0000 (46217.8917)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0775  max mem: 15572
Epoch: [12]  [2170/2809]  eta: 0:06:04  lr: 0.000041  min_lr: 0.000000  loss: 4.3424 (4.3970)  class_acc: 0.2083 (0.2289)  loss_scale: 65536.0000 (46306.8743)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0755  max mem: 15572
[2025-01-15 20:34:25,541] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35883
[2025-01-15 20:34:25,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:34:25,542] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [2180/2809]  eta: 0:05:58  lr: 0.000041  min_lr: 0.000000  loss: 4.4015 (4.3972)  class_acc: 0.2083 (0.2289)  loss_scale: 65536.0000 (46304.8950)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0604  max mem: 15572
Epoch: [12]  [2190/2809]  eta: 0:05:53  lr: 0.000041  min_lr: 0.000000  loss: 4.3686 (4.3970)  class_acc: 0.2083 (0.2289)  loss_scale: 32768.0000 (46243.1109)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1206  max mem: 15572
Epoch: [12]  [2200/2809]  eta: 0:05:47  lr: 0.000041  min_lr: 0.000000  loss: 4.4074 (4.3974)  class_acc: 0.2500 (0.2289)  loss_scale: 32768.0000 (46181.8882)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1610  max mem: 15572
Epoch: [12]  [2210/2809]  eta: 0:05:41  lr: 0.000041  min_lr: 0.000000  loss: 4.3867 (4.3971)  class_acc: 0.2500 (0.2290)  loss_scale: 32768.0000 (46121.2194)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1384  max mem: 15572
Epoch: [12]  [2220/2809]  eta: 0:05:36  lr: 0.000041  min_lr: 0.000000  loss: 4.2577 (4.3970)  class_acc: 0.2500 (0.2291)  loss_scale: 32768.0000 (46061.0968)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1296  max mem: 15572
Epoch: [12]  [2230/2809]  eta: 0:05:30  lr: 0.000041  min_lr: 0.000000  loss: 4.2434 (4.3973)  class_acc: 0.2083 (0.2292)  loss_scale: 32768.0000 (46001.5132)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0806  max mem: 15572
Epoch: [12]  [2240/2809]  eta: 0:05:24  lr: 0.000041  min_lr: 0.000000  loss: 4.3930 (4.3975)  class_acc: 0.1667 (0.2290)  loss_scale: 32768.0000 (45942.4614)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0196  max mem: 15572
Epoch: [12]  [2250/2809]  eta: 0:05:18  lr: 0.000041  min_lr: 0.000000  loss: 4.3714 (4.3973)  class_acc: 0.1667 (0.2290)  loss_scale: 32768.0000 (45883.9343)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0196  max mem: 15572
Epoch: [12]  [2260/2809]  eta: 0:05:13  lr: 0.000041  min_lr: 0.000000  loss: 4.3386 (4.3966)  class_acc: 0.2083 (0.2290)  loss_scale: 32768.0000 (45825.9248)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0528  max mem: 15572
Epoch: [12]  [2270/2809]  eta: 0:05:07  lr: 0.000041  min_lr: 0.000000  loss: 4.2892 (4.3962)  class_acc: 0.2083 (0.2292)  loss_scale: 32768.0000 (45768.4262)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0724  max mem: 15572
Epoch: [12]  [2280/2809]  eta: 0:05:01  lr: 0.000041  min_lr: 0.000000  loss: 4.2892 (4.3958)  class_acc: 0.2083 (0.2293)  loss_scale: 32768.0000 (45711.4318)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0822  max mem: 15572
Epoch: [12]  [2290/2809]  eta: 0:04:55  lr: 0.000041  min_lr: 0.000000  loss: 4.2341 (4.3950)  class_acc: 0.2083 (0.2291)  loss_scale: 32768.0000 (45654.9350)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1016  max mem: 15572
[2025-01-15 20:35:32,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=36000, skipped=228, lr=[4.0059693595025643e-07, 4.0059693595025643e-07, 5.72281337071795e-07, 5.72281337071795e-07, 8.175447672454215e-07, 8.175447672454215e-07, 1.167921096064888e-06, 1.167921096064888e-06, 1.6684587086641256e-06, 1.6684587086641256e-06, 2.383512440948751e-06, 2.383512440948751e-06, 3.4050177727839304e-06, 3.4050177727839304e-06, 4.8643111039770435e-06, 4.8643111039770435e-06, 6.949015862824348e-06, 6.949015862824348e-06, 9.927165518320499e-06, 9.927165518320499e-06, 1.418166502617214e-05, 1.418166502617214e-05, 2.0259521465960202e-05, 2.0259521465960202e-05, 2.894217352280029e-05, 2.894217352280029e-05, 4.134596217542899e-05, 4.134596217542899e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:35:32,374] [INFO] [timer.py:260:stop] epoch=0/micro_step=36000/global_step=36000, RunningAvgSamplesPerSec=27.531457571072124, CurrSamplesPerSec=30.50717225411103, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [2300/2809]  eta: 0:04:50  lr: 0.000041  min_lr: 0.000000  loss: 4.2682 (4.3945)  class_acc: 0.2083 (0.2295)  loss_scale: 32768.0000 (45598.9292)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0771  max mem: 15572
[2025-01-15 20:35:39,202] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:35:39,202] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [2310/2809]  eta: 0:04:44  lr: 0.000041  min_lr: 0.000000  loss: 4.3416 (4.3944)  class_acc: 0.2917 (0.2296)  loss_scale: 32768.0000 (45642.6621)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1500  max mem: 15572
Epoch: [12]  [2320/2809]  eta: 0:04:38  lr: 0.000041  min_lr: 0.000000  loss: 4.4289 (4.3943)  class_acc: 0.2917 (0.2298)  loss_scale: 65536.0000 (45728.3723)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1126  max mem: 15572
Epoch: [12]  [2330/2809]  eta: 0:04:33  lr: 0.000041  min_lr: 0.000000  loss: 4.3434 (4.3938)  class_acc: 0.2500 (0.2298)  loss_scale: 65536.0000 (45813.3471)  weight_decay: 0.0500 (0.0500)  time: 0.4954  data: 0.0325  max mem: 15572
Epoch: [12]  [2340/2809]  eta: 0:04:27  lr: 0.000041  min_lr: 0.000000  loss: 4.4424 (4.3945)  class_acc: 0.2083 (0.2295)  loss_scale: 65536.0000 (45897.5959)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0565  max mem: 15572
Epoch: [12]  [2350/2809]  eta: 0:04:21  lr: 0.000041  min_lr: 0.000000  loss: 4.4854 (4.3939)  class_acc: 0.1667 (0.2296)  loss_scale: 65536.0000 (45981.1280)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0954  max mem: 15572
Epoch: [12]  [2360/2809]  eta: 0:04:15  lr: 0.000041  min_lr: 0.000000  loss: 4.4050 (4.3940)  class_acc: 0.2083 (0.2297)  loss_scale: 65536.0000 (46063.9526)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1138  max mem: 15572
Epoch: [12]  [2370/2809]  eta: 0:04:10  lr: 0.000041  min_lr: 0.000000  loss: 4.4618 (4.3941)  class_acc: 0.2083 (0.2295)  loss_scale: 65536.0000 (46146.0784)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0999  max mem: 15572
Epoch: [12]  [2380/2809]  eta: 0:04:04  lr: 0.000041  min_lr: 0.000000  loss: 4.4618 (4.3944)  class_acc: 0.2083 (0.2294)  loss_scale: 65536.0000 (46227.5145)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1585  max mem: 15572
[2025-01-15 20:36:25,815] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36094
[2025-01-15 20:36:25,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:36:25,815] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [2390/2809]  eta: 0:03:58  lr: 0.000041  min_lr: 0.000000  loss: 4.2538 (4.3935)  class_acc: 0.2083 (0.2295)  loss_scale: 65536.0000 (46239.7457)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1583  max mem: 15572
Epoch: [12]  [2400/2809]  eta: 0:03:53  lr: 0.000041  min_lr: 0.000000  loss: 4.2538 (4.3935)  class_acc: 0.2083 (0.2295)  loss_scale: 32768.0000 (46183.6368)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1605  max mem: 15572
Epoch: [12]  [2410/2809]  eta: 0:03:47  lr: 0.000041  min_lr: 0.000000  loss: 4.3685 (4.3933)  class_acc: 0.2500 (0.2298)  loss_scale: 32768.0000 (46127.9934)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1340  max mem: 15572
Epoch: [12]  [2420/2809]  eta: 0:03:41  lr: 0.000041  min_lr: 0.000000  loss: 4.2863 (4.3927)  class_acc: 0.2500 (0.2297)  loss_scale: 32768.0000 (46072.8096)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1101  max mem: 15572
Epoch: [12]  [2430/2809]  eta: 0:03:36  lr: 0.000041  min_lr: 0.000000  loss: 4.2863 (4.3929)  class_acc: 0.2083 (0.2298)  loss_scale: 32768.0000 (46018.0798)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1672  max mem: 15572
Epoch: [12]  [2440/2809]  eta: 0:03:30  lr: 0.000041  min_lr: 0.000000  loss: 4.4109 (4.3929)  class_acc: 0.2083 (0.2297)  loss_scale: 32768.0000 (45963.7984)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0878  max mem: 15572
Epoch: [12]  [2450/2809]  eta: 0:03:24  lr: 0.000041  min_lr: 0.000000  loss: 4.4118 (4.3931)  class_acc: 0.2083 (0.2297)  loss_scale: 32768.0000 (45909.9600)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0485  max mem: 15572
Epoch: [12]  [2460/2809]  eta: 0:03:19  lr: 0.000041  min_lr: 0.000000  loss: 4.4218 (4.3932)  class_acc: 0.2500 (0.2299)  loss_scale: 32768.0000 (45856.5591)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1056  max mem: 15572
Epoch: [12]  [2470/2809]  eta: 0:03:13  lr: 0.000041  min_lr: 0.000000  loss: 4.4198 (4.3932)  class_acc: 0.2500 (0.2299)  loss_scale: 32768.0000 (45803.5904)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0894  max mem: 15572
Epoch: [12]  [2480/2809]  eta: 0:03:07  lr: 0.000041  min_lr: 0.000000  loss: 4.4198 (4.3931)  class_acc: 0.2917 (0.2300)  loss_scale: 32768.0000 (45751.0488)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0726  max mem: 15572
Epoch: [12]  [2490/2809]  eta: 0:03:01  lr: 0.000041  min_lr: 0.000000  loss: 4.4828 (4.3934)  class_acc: 0.2083 (0.2300)  loss_scale: 32768.0000 (45698.9289)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0921  max mem: 15572
Epoch: [12]  [2500/2809]  eta: 0:02:56  lr: 0.000041  min_lr: 0.000000  loss: 4.3984 (4.3933)  class_acc: 0.2500 (0.2302)  loss_scale: 32768.0000 (45647.2259)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1995  max mem: 15572
Epoch: [12]  [2510/2809]  eta: 0:02:50  lr: 0.000041  min_lr: 0.000000  loss: 4.3549 (4.3933)  class_acc: 0.2500 (0.2302)  loss_scale: 32768.0000 (45595.9347)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1484  max mem: 15572
[2025-01-15 20:37:40,407] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:37:40,408] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [2520/2809]  eta: 0:02:44  lr: 0.000041  min_lr: 0.000000  loss: 4.3637 (4.3934)  class_acc: 0.2500 (0.2304)  loss_scale: 32768.0000 (45623.0385)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0613  max mem: 15572
Epoch: [12]  [2530/2809]  eta: 0:02:39  lr: 0.000041  min_lr: 0.000000  loss: 4.3478 (4.3930)  class_acc: 0.2917 (0.2305)  loss_scale: 65536.0000 (45701.7147)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0911  max mem: 15572
Epoch: [12]  [2540/2809]  eta: 0:02:33  lr: 0.000041  min_lr: 0.000000  loss: 4.3932 (4.3933)  class_acc: 0.2083 (0.2305)  loss_scale: 65536.0000 (45779.7717)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0412  max mem: 15572
Epoch: [12]  [2550/2809]  eta: 0:02:27  lr: 0.000041  min_lr: 0.000000  loss: 4.4927 (4.3933)  class_acc: 0.2083 (0.2305)  loss_scale: 65536.0000 (45857.2168)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0329  max mem: 15572
Epoch: [12]  [2560/2809]  eta: 0:02:21  lr: 0.000041  min_lr: 0.000000  loss: 4.4476 (4.3940)  class_acc: 0.2500 (0.2304)  loss_scale: 65536.0000 (45934.0570)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0958  max mem: 15572
Epoch: [12]  [2570/2809]  eta: 0:02:16  lr: 0.000041  min_lr: 0.000000  loss: 4.3448 (4.3935)  class_acc: 0.2083 (0.2304)  loss_scale: 65536.0000 (46010.2995)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1205  max mem: 15572
Epoch: [12]  [2580/2809]  eta: 0:02:10  lr: 0.000041  min_lr: 0.000000  loss: 4.3923 (4.3940)  class_acc: 0.2083 (0.2302)  loss_scale: 65536.0000 (46085.9512)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0743  max mem: 15572
Epoch: [12]  [2590/2809]  eta: 0:02:04  lr: 0.000041  min_lr: 0.000000  loss: 4.3908 (4.3935)  class_acc: 0.2500 (0.2303)  loss_scale: 65536.0000 (46161.0189)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0859  max mem: 15572
Epoch: [12]  [2600/2809]  eta: 0:01:59  lr: 0.000041  min_lr: 0.000000  loss: 4.3270 (4.3931)  class_acc: 0.2500 (0.2303)  loss_scale: 65536.0000 (46235.5094)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0587  max mem: 15572
Epoch: [12]  [2610/2809]  eta: 0:01:53  lr: 0.000041  min_lr: 0.000000  loss: 4.3574 (4.3928)  class_acc: 0.1667 (0.2301)  loss_scale: 65536.0000 (46309.4293)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0519  max mem: 15572
Epoch: [12]  [2620/2809]  eta: 0:01:47  lr: 0.000041  min_lr: 0.000000  loss: 4.3482 (4.3928)  class_acc: 0.1667 (0.2303)  loss_scale: 65536.0000 (46382.7852)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0518  max mem: 15572
Epoch: [12]  [2630/2809]  eta: 0:01:41  lr: 0.000041  min_lr: 0.000000  loss: 4.3482 (4.3924)  class_acc: 0.2500 (0.2305)  loss_scale: 65536.0000 (46455.5834)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0599  max mem: 15572
Epoch: [12]  [2640/2809]  eta: 0:01:36  lr: 0.000041  min_lr: 0.000000  loss: 4.3404 (4.3922)  class_acc: 0.2083 (0.2304)  loss_scale: 65536.0000 (46527.8304)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1301  max mem: 15572
[2025-01-15 20:38:51,589] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:38:51,589] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:38:54,086] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36357
[2025-01-15 20:38:54,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:38:54,086] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2650/2809]  eta: 0:01:30  lr: 0.000041  min_lr: 0.000000  loss: 4.3252 (4.3921)  class_acc: 0.2083 (0.2304)  loss_scale: 65536.0000 (46747.8597)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0708  max mem: 15572
Epoch: [12]  [2660/2809]  eta: 0:01:24  lr: 0.000041  min_lr: 0.000000  loss: 4.4238 (4.3924)  class_acc: 0.2083 (0.2305)  loss_scale: 65536.0000 (46818.4652)  weight_decay: 0.0500 (0.0500)  time: 0.4363  data: 0.0007  max mem: 15572
Epoch: [12]  [2670/2809]  eta: 0:01:19  lr: 0.000041  min_lr: 0.000000  loss: 4.3683 (4.3921)  class_acc: 0.2083 (0.2304)  loss_scale: 65536.0000 (46888.5421)  weight_decay: 0.0500 (0.0500)  time: 0.4701  data: 0.0010  max mem: 15572
Epoch: [12]  [2680/2809]  eta: 0:01:13  lr: 0.000041  min_lr: 0.000000  loss: 4.3337 (4.3919)  class_acc: 0.2083 (0.2305)  loss_scale: 65536.0000 (46958.0962)  weight_decay: 0.0500 (0.0500)  time: 0.4756  data: 0.0008  max mem: 15572
Epoch: [12]  [2690/2809]  eta: 0:01:07  lr: 0.000041  min_lr: 0.000000  loss: 4.4031 (4.3923)  class_acc: 0.2083 (0.2305)  loss_scale: 65536.0000 (47027.1334)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0812  max mem: 15572
[2025-01-15 20:39:17,813] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36401
[2025-01-15 20:39:17,813] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:39:17,815] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [2700/2809]  eta: 0:01:02  lr: 0.000041  min_lr: 0.000000  loss: 4.6279 (4.3932)  class_acc: 0.2083 (0.2305)  loss_scale: 65536.0000 (46998.6050)  weight_decay: 0.0500 (0.0500)  time: 0.6685  data: 0.1976  max mem: 15572
Epoch: [12]  [2710/2809]  eta: 0:00:56  lr: 0.000041  min_lr: 0.000000  loss: 4.5053 (4.3929)  class_acc: 0.2500 (0.2306)  loss_scale: 32768.0000 (46946.1129)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.1663  max mem: 15572
Epoch: [12]  [2720/2809]  eta: 0:00:50  lr: 0.000041  min_lr: 0.000000  loss: 4.3700 (4.3928)  class_acc: 0.2083 (0.2305)  loss_scale: 32768.0000 (46894.0066)  weight_decay: 0.0500 (0.0500)  time: 0.6476  data: 0.1877  max mem: 15572
Epoch: [12]  [2730/2809]  eta: 0:00:44  lr: 0.000041  min_lr: 0.000000  loss: 4.4571 (4.3934)  class_acc: 0.2083 (0.2304)  loss_scale: 32768.0000 (46842.2819)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.1891  max mem: 15572
Epoch: [12]  [2740/2809]  eta: 0:00:39  lr: 0.000041  min_lr: 0.000000  loss: 4.3913 (4.3933)  class_acc: 0.2083 (0.2304)  loss_scale: 32768.0000 (46790.9347)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.1472  max mem: 15572
Epoch: [12]  [2750/2809]  eta: 0:00:33  lr: 0.000041  min_lr: 0.000000  loss: 4.3913 (4.3937)  class_acc: 0.2500 (0.2305)  loss_scale: 32768.0000 (46739.9607)  weight_decay: 0.0500 (0.0500)  time: 0.6573  data: 0.1830  max mem: 15572
Epoch: [12]  [2760/2809]  eta: 0:00:27  lr: 0.000041  min_lr: 0.000000  loss: 4.4343 (4.3932)  class_acc: 0.2500 (0.2304)  loss_scale: 32768.0000 (46689.3560)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.1781  max mem: 15572
Epoch: [12]  [2770/2809]  eta: 0:00:22  lr: 0.000041  min_lr: 0.000000  loss: 4.2451 (4.3928)  class_acc: 0.2500 (0.2306)  loss_scale: 32768.0000 (46639.1166)  weight_decay: 0.0500 (0.0500)  time: 0.6785  data: 0.2333  max mem: 15572
Epoch: [12]  [2780/2809]  eta: 0:00:16  lr: 0.000041  min_lr: 0.000000  loss: 4.2451 (4.3926)  class_acc: 0.2917 (0.2309)  loss_scale: 32768.0000 (46589.2384)  weight_decay: 0.0500 (0.0500)  time: 0.7444  data: 0.2854  max mem: 15572
Epoch: [12]  [2790/2809]  eta: 0:00:10  lr: 0.000041  min_lr: 0.000000  loss: 4.3059 (4.3924)  class_acc: 0.2500 (0.2307)  loss_scale: 32768.0000 (46539.7177)  weight_decay: 0.0500 (0.0500)  time: 0.7174  data: 0.2754  max mem: 15572
Epoch: [12]  [2800/2809]  eta: 0:00:05  lr: 0.000041  min_lr: 0.000000  loss: 4.3845 (4.3926)  class_acc: 0.1667 (0.2306)  loss_scale: 32768.0000 (46490.5505)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.1322  max mem: 15572
Epoch: [12]  [2808/2809]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000000  loss: 4.4260 (4.3927)  class_acc: 0.1667 (0.2306)  loss_scale: 32768.0000 (46451.4689)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0002  max mem: 15572
Epoch: [12] Total time: 0:26:44 (0.5712 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000000  loss: 4.4260 (4.3927)  class_acc: 0.1667 (0.2306)  loss_scale: 32768.0000 (46451.4689)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:20:03  loss: 1.3135 (1.3135)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.4246  data: 4.2362  max mem: 15572
Val:  [ 10/272]  eta: 0:03:18  loss: 3.3589 (3.1406)  acc1: 27.7778 (29.2929)  acc5: 50.0000 (52.0202)  time: 0.7590  data: 0.5475  max mem: 15572
Val:  [ 20/272]  eta: 0:02:06  loss: 3.2066 (3.1702)  acc1: 27.7778 (32.0106)  acc5: 55.5556 (56.3492)  time: 0.3049  data: 0.0982  max mem: 15572
Val:  [ 30/272]  eta: 0:01:43  loss: 3.1852 (3.1750)  acc1: 27.7778 (30.6452)  acc5: 66.6667 (59.1398)  time: 0.2434  data: 0.0573  max mem: 15572
Val:  [ 40/272]  eta: 0:01:32  loss: 3.0078 (3.1393)  acc1: 22.2222 (27.7778)  acc5: 66.6667 (60.9756)  time: 0.2881  data: 0.1024  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 2.9786 (3.0590)  acc1: 22.2222 (30.5011)  acc5: 66.6667 (63.3987)  time: 0.3188  data: 0.1222  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 2.3102 (2.9731)  acc1: 55.5556 (34.1530)  acc5: 77.7778 (65.2095)  time: 0.3467  data: 0.1463  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 2.3366 (2.9054)  acc1: 50.0000 (36.5415)  acc5: 77.7778 (67.6839)  time: 0.3145  data: 0.1039  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 2.5761 (2.9039)  acc1: 44.4444 (36.2826)  acc5: 77.7778 (67.6955)  time: 0.2757  data: 0.0732  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 3.1746 (2.9368)  acc1: 27.7778 (35.7143)  acc5: 72.2222 (67.8266)  time: 0.2990  data: 0.1062  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 3.2125 (2.9748)  acc1: 27.7778 (34.8735)  acc5: 66.6667 (67.3267)  time: 0.3404  data: 0.1491  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.3440 (3.0247)  acc1: 11.1111 (32.4825)  acc5: 55.5556 (65.7658)  time: 0.3401  data: 0.1576  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.4893 (3.0497)  acc1: 11.1111 (32.2314)  acc5: 55.5556 (65.2893)  time: 0.3097  data: 0.1106  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.1018 (3.0260)  acc1: 38.8889 (33.4182)  acc5: 66.6667 (65.8609)  time: 0.3597  data: 0.1473  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 2.8644 (3.0309)  acc1: 38.8889 (33.6091)  acc5: 66.6667 (65.6422)  time: 0.3700  data: 0.1703  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 2.8644 (3.0178)  acc1: 27.7778 (33.6645)  acc5: 72.2222 (66.2252)  time: 0.3121  data: 0.1204  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 2.8218 (3.0123)  acc1: 33.3333 (34.1270)  acc5: 77.7778 (66.7357)  time: 0.2965  data: 0.1021  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 2.9837 (3.0329)  acc1: 27.7778 (33.4308)  acc5: 72.2222 (66.4068)  time: 0.3324  data: 0.1373  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.9231 (3.0189)  acc1: 27.7778 (33.7630)  acc5: 66.6667 (66.9122)  time: 0.3109  data: 0.1171  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.9787 (3.0485)  acc1: 27.7778 (32.7225)  acc5: 61.1111 (65.5905)  time: 0.3097  data: 0.1242  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.1483 (3.0597)  acc1: 22.2222 (32.4212)  acc5: 55.5556 (65.3676)  time: 0.2941  data: 0.1120  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.0282 (3.0662)  acc1: 33.3333 (32.6224)  acc5: 66.6667 (65.3502)  time: 0.2479  data: 0.0414  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.0309 (3.0634)  acc1: 38.8889 (33.0820)  acc5: 66.6667 (65.4349)  time: 0.3345  data: 0.1282  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.6745 (3.0469)  acc1: 50.0000 (34.1991)  acc5: 72.2222 (65.9211)  time: 0.3158  data: 0.1279  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.4738 (3.0283)  acc1: 55.5556 (34.9009)  acc5: 83.3333 (66.7128)  time: 0.2838  data: 0.0956  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.6623 (3.0394)  acc1: 33.3333 (34.1744)  acc5: 83.3333 (66.4896)  time: 0.3423  data: 0.1393  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.2760 (2.9950)  acc1: 61.1111 (36.1005)  acc5: 88.8889 (67.4755)  time: 0.3018  data: 0.0939  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.5918 (2.9948)  acc1: 50.0000 (35.8549)  acc5: 77.7778 (67.5687)  time: 0.1995  data: 0.0250  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.6576 (2.9973)  acc1: 50.0000 (35.8591)  acc5: 80.0000 (67.5814)  time: 0.1914  data: 0.0249  max mem: 15572
Val: Total time: 0:01:27 (0.3216 s / it)
* Acc@1 35.859 Acc@5 67.581 loss 2.997
Accuracy of the network on the 4883 val videos: 35.9%
[2025-01-15 20:41:56,613] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 20:41:56,615] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 20:41:56,615] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 20:41:59,664] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 20:41:59,665] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 35.86%
Epoch: [13]  [   0/2809]  eta: 6:11:08  lr: 0.000041  min_lr: 0.000000  loss: 4.5666 (4.5666)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.9277  data: 7.4840  max mem: 15572
Epoch: [13]  [  10/2809]  eta: 0:52:59  lr: 0.000041  min_lr: 0.000000  loss: 4.5303 (4.5015)  class_acc: 0.2083 (0.2235)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1359  data: 0.6808  max mem: 15572
[2025-01-15 20:42:14,340] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:42:14,340] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 20:42:17,072] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36536
[2025-01-15 20:42:17,072] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:42:17,073] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [  20/2809]  eta: 0:39:43  lr: 0.000041  min_lr: 0.000000  loss: 4.4272 (4.4309)  class_acc: 0.2500 (0.2460)  loss_scale: 32768.0000 (42130.2857)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0453  max mem: 15572
Epoch: [13]  [  30/2809]  eta: 0:36:56  lr: 0.000041  min_lr: 0.000000  loss: 4.3544 (4.3871)  class_acc: 0.2083 (0.2433)  loss_scale: 32768.0000 (39110.1935)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1593  max mem: 15572
Epoch: [13]  [  40/2809]  eta: 0:33:56  lr: 0.000041  min_lr: 0.000000  loss: 4.2083 (4.3709)  class_acc: 0.2500 (0.2530)  loss_scale: 32768.0000 (37563.3171)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1622  max mem: 15572
Epoch: [13]  [  50/2809]  eta: 0:31:45  lr: 0.000041  min_lr: 0.000000  loss: 4.2902 (4.3541)  class_acc: 0.2500 (0.2435)  loss_scale: 32768.0000 (36623.0588)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0814  max mem: 15572
Epoch: [13]  [  60/2809]  eta: 0:30:40  lr: 0.000041  min_lr: 0.000000  loss: 4.3976 (4.3588)  class_acc: 0.2083 (0.2459)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.1004  max mem: 15572
Epoch: [13]  [  70/2809]  eta: 0:29:18  lr: 0.000041  min_lr: 0.000000  loss: 4.4159 (4.3803)  class_acc: 0.2083 (0.2406)  loss_scale: 32768.0000 (35537.1268)  weight_decay: 0.0500 (0.0500)  time: 0.5188  data: 0.0826  max mem: 15572
Epoch: [13]  [  80/2809]  eta: 0:29:11  lr: 0.000041  min_lr: 0.000000  loss: 4.4159 (4.3825)  class_acc: 0.2083 (0.2397)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1161  max mem: 15572
Epoch: [13]  [  90/2809]  eta: 0:29:04  lr: 0.000041  min_lr: 0.000000  loss: 4.4424 (4.3863)  class_acc: 0.2500 (0.2386)  loss_scale: 32768.0000 (34928.5275)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.2090  max mem: 15572
Epoch: [13]  [ 100/2809]  eta: 0:28:55  lr: 0.000041  min_lr: 0.000000  loss: 4.3869 (4.3872)  class_acc: 0.2500 (0.2384)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.2015  max mem: 15572
Epoch: [13]  [ 110/2809]  eta: 0:28:30  lr: 0.000041  min_lr: 0.000000  loss: 4.2785 (4.3761)  class_acc: 0.2917 (0.2436)  loss_scale: 32768.0000 (34539.2432)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1532  max mem: 15572
Epoch: [13]  [ 120/2809]  eta: 0:28:01  lr: 0.000041  min_lr: 0.000000  loss: 4.2513 (4.3650)  class_acc: 0.2917 (0.2452)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1071  max mem: 15572
Epoch: [13]  [ 130/2809]  eta: 0:27:42  lr: 0.000041  min_lr: 0.000000  loss: 4.2191 (4.3595)  class_acc: 0.2500 (0.2455)  loss_scale: 32768.0000 (34268.8244)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1053  max mem: 15572
Epoch: [13]  [ 140/2809]  eta: 0:27:28  lr: 0.000041  min_lr: 0.000000  loss: 4.4537 (4.3721)  class_acc: 0.1667 (0.2388)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1192  max mem: 15572
[2025-01-15 20:43:32,000] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:43:32,000] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 150/2809]  eta: 0:27:20  lr: 0.000041  min_lr: 0.000000  loss: 4.3836 (4.3709)  class_acc: 0.2083 (0.2406)  loss_scale: 32768.0000 (34721.0596)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1386  max mem: 15572
Epoch: [13]  [ 160/2809]  eta: 0:27:03  lr: 0.000041  min_lr: 0.000000  loss: 4.3061 (4.3588)  class_acc: 0.2500 (0.2428)  loss_scale: 65536.0000 (36635.0311)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1257  max mem: 15572
Epoch: [13]  [ 170/2809]  eta: 0:27:01  lr: 0.000041  min_lr: 0.000000  loss: 4.2970 (4.3564)  class_acc: 0.2500 (0.2442)  loss_scale: 65536.0000 (38325.1462)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1410  max mem: 15572
Epoch: [13]  [ 180/2809]  eta: 0:26:32  lr: 0.000041  min_lr: 0.000000  loss: 4.2842 (4.3484)  class_acc: 0.2083 (0.2429)  loss_scale: 65536.0000 (39828.5083)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1090  max mem: 15572
Epoch: [13]  [ 190/2809]  eta: 0:26:27  lr: 0.000041  min_lr: 0.000000  loss: 4.2842 (4.3437)  class_acc: 0.2083 (0.2419)  loss_scale: 65536.0000 (41174.4503)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0937  max mem: 15572
Epoch: [13]  [ 200/2809]  eta: 0:26:08  lr: 0.000041  min_lr: 0.000000  loss: 4.1701 (4.3374)  class_acc: 0.2500 (0.2432)  loss_scale: 65536.0000 (42386.4677)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0979  max mem: 15572
Epoch: [13]  [ 210/2809]  eta: 0:25:56  lr: 0.000041  min_lr: 0.000000  loss: 4.3146 (4.3409)  class_acc: 0.2083 (0.2411)  loss_scale: 65536.0000 (43483.6019)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0757  max mem: 15572
Epoch: [13]  [ 220/2809]  eta: 0:25:40  lr: 0.000041  min_lr: 0.000000  loss: 4.4500 (4.3376)  class_acc: 0.2500 (0.2428)  loss_scale: 65536.0000 (44481.4480)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0957  max mem: 15572
Epoch: [13]  [ 230/2809]  eta: 0:25:20  lr: 0.000041  min_lr: 0.000000  loss: 4.2825 (4.3385)  class_acc: 0.2500 (0.2435)  loss_scale: 65536.0000 (45392.9004)  weight_decay: 0.0500 (0.0500)  time: 0.4924  data: 0.0544  max mem: 15572
Epoch: [13]  [ 240/2809]  eta: 0:25:14  lr: 0.000041  min_lr: 0.000000  loss: 4.3572 (4.3413)  class_acc: 0.2500 (0.2410)  loss_scale: 65536.0000 (46228.7137)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0730  max mem: 15572
Epoch: [13]  [ 250/2809]  eta: 0:25:18  lr: 0.000041  min_lr: 0.000000  loss: 4.3742 (4.3487)  class_acc: 0.1667 (0.2397)  loss_scale: 65536.0000 (46997.9283)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1750  max mem: 15572
Epoch: [13]  [ 260/2809]  eta: 0:25:03  lr: 0.000041  min_lr: 0.000000  loss: 4.4833 (4.3488)  class_acc: 0.2500 (0.2399)  loss_scale: 65536.0000 (47708.1992)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1539  max mem: 15572
Epoch: [13]  [ 270/2809]  eta: 0:24:57  lr: 0.000041  min_lr: 0.000000  loss: 4.4379 (4.3523)  class_acc: 0.2500 (0.2411)  loss_scale: 65536.0000 (48366.0517)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.1054  max mem: 15572
[2025-01-15 20:44:42,963] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:44:42,963] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:44:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36794
[2025-01-15 20:44:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:44:43,384] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 280/2809]  eta: 0:24:44  lr: 0.000041  min_lr: 0.000000  loss: 4.3668 (4.3530)  class_acc: 0.2500 (0.2415)  loss_scale: 65536.0000 (49210.3060)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1109  max mem: 15572
Epoch: [13]  [ 290/2809]  eta: 0:24:38  lr: 0.000041  min_lr: 0.000000  loss: 4.3471 (4.3527)  class_acc: 0.2500 (0.2404)  loss_scale: 65536.0000 (49771.3265)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1115  max mem: 15572
Epoch: [13]  [ 300/2809]  eta: 0:24:37  lr: 0.000041  min_lr: 0.000000  loss: 4.3630 (4.3491)  class_acc: 0.2500 (0.2413)  loss_scale: 65536.0000 (50295.0698)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1592  max mem: 15572
Epoch: [13]  [ 310/2809]  eta: 0:24:20  lr: 0.000041  min_lr: 0.000000  loss: 4.3773 (4.3558)  class_acc: 0.2500 (0.2398)  loss_scale: 65536.0000 (50785.1318)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0910  max mem: 15572
[2025-01-15 20:45:07,175] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36836
[2025-01-15 20:45:07,176] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:45:07,176] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 320/2809]  eta: 0:24:16  lr: 0.000041  min_lr: 0.000000  loss: 4.3875 (4.3517)  class_acc: 0.2083 (0.2403)  loss_scale: 65536.0000 (51040.4984)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0766  max mem: 15572
Epoch: [13]  [ 330/2809]  eta: 0:24:15  lr: 0.000041  min_lr: 0.000000  loss: 4.2876 (4.3526)  class_acc: 0.2500 (0.2404)  loss_scale: 32768.0000 (50488.4592)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.1691  max mem: 15572
Epoch: [13]  [ 340/2809]  eta: 0:24:05  lr: 0.000041  min_lr: 0.000000  loss: 4.3969 (4.3539)  class_acc: 0.2500 (0.2400)  loss_scale: 32768.0000 (49968.7977)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1314  max mem: 15572
Epoch: [13]  [ 350/2809]  eta: 0:24:01  lr: 0.000041  min_lr: 0.000000  loss: 4.3210 (4.3520)  class_acc: 0.2500 (0.2405)  loss_scale: 32768.0000 (49478.7464)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1151  max mem: 15572
Epoch: [13]  [ 360/2809]  eta: 0:23:46  lr: 0.000041  min_lr: 0.000000  loss: 4.2847 (4.3506)  class_acc: 0.2500 (0.2412)  loss_scale: 32768.0000 (49015.8449)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0809  max mem: 15572
Epoch: [13]  [ 370/2809]  eta: 0:23:43  lr: 0.000041  min_lr: 0.000000  loss: 4.3875 (4.3545)  class_acc: 0.2083 (0.2405)  loss_scale: 32768.0000 (48577.8976)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.1078  max mem: 15572
Epoch: [13]  [ 380/2809]  eta: 0:23:37  lr: 0.000041  min_lr: 0.000000  loss: 4.4934 (4.3573)  class_acc: 0.2500 (0.2409)  loss_scale: 32768.0000 (48162.9396)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1697  max mem: 15572
Epoch: [13]  [ 390/2809]  eta: 0:23:31  lr: 0.000041  min_lr: 0.000000  loss: 4.4856 (4.3590)  class_acc: 0.2500 (0.2412)  loss_scale: 32768.0000 (47769.2072)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1277  max mem: 15572
Epoch: [13]  [ 400/2809]  eta: 0:23:27  lr: 0.000041  min_lr: 0.000000  loss: 4.3754 (4.3573)  class_acc: 0.2500 (0.2414)  loss_scale: 32768.0000 (47395.1122)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1652  max mem: 15572
Epoch: [13]  [ 410/2809]  eta: 0:23:26  lr: 0.000041  min_lr: 0.000000  loss: 4.3754 (4.3579)  class_acc: 0.2500 (0.2412)  loss_scale: 32768.0000 (47039.2214)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.2089  max mem: 15572
Epoch: [13]  [ 420/2809]  eta: 0:23:13  lr: 0.000041  min_lr: 0.000000  loss: 4.4530 (4.3610)  class_acc: 0.2083 (0.2416)  loss_scale: 32768.0000 (46700.2375)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1220  max mem: 15572
Epoch: [13]  [ 430/2809]  eta: 0:23:08  lr: 0.000041  min_lr: 0.000000  loss: 4.4327 (4.3586)  class_acc: 0.1667 (0.2408)  loss_scale: 32768.0000 (46376.9838)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.0775  max mem: 15572
Epoch: [13]  [ 440/2809]  eta: 0:23:01  lr: 0.000041  min_lr: 0.000000  loss: 4.2008 (4.3561)  class_acc: 0.2083 (0.2409)  loss_scale: 32768.0000 (46068.3900)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1182  max mem: 15572
[2025-01-15 20:46:21,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:46:21,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 450/2809]  eta: 0:22:55  lr: 0.000041  min_lr: 0.000000  loss: 4.2259 (4.3556)  class_acc: 0.2083 (0.2388)  loss_scale: 32768.0000 (45991.4501)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1193  max mem: 15572
[2025-01-15 20:46:28,775] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36977
[2025-01-15 20:46:28,775] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:46:28,775] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 460/2809]  eta: 0:22:50  lr: 0.000041  min_lr: 0.000000  loss: 4.2257 (4.3526)  class_acc: 0.2083 (0.2403)  loss_scale: 65536.0000 (46344.3297)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1380  max mem: 15572
Epoch: [13]  [ 470/2809]  eta: 0:22:40  lr: 0.000041  min_lr: 0.000000  loss: 4.4688 (4.3546)  class_acc: 0.2500 (0.2408)  loss_scale: 32768.0000 (46056.0849)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1120  max mem: 15572
Epoch: [13]  [ 480/2809]  eta: 0:22:35  lr: 0.000041  min_lr: 0.000000  loss: 4.4658 (4.3548)  class_acc: 0.2083 (0.2403)  loss_scale: 32768.0000 (45779.8254)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1068  max mem: 15572
[2025-01-15 20:46:42,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=37000, skipped=235, lr=[3.958308455732292e-07, 3.958308455732292e-07, 5.654726365331847e-07, 5.654726365331847e-07, 8.078180521902639e-07, 8.078180521902639e-07, 1.1540257888432343e-06, 1.1540257888432343e-06, 1.6486082697760488e-06, 1.6486082697760488e-06, 2.355154671108641e-06, 2.355154671108641e-06, 3.3645066730123448e-06, 3.3645066730123448e-06, 4.806438104303351e-06, 4.806438104303351e-06, 6.866340149004786e-06, 6.866340149004786e-06, 9.809057355721125e-06, 9.809057355721125e-06, 1.4012939079601606e-05, 1.4012939079601606e-05, 2.001848439943087e-05, 2.001848439943087e-05, 2.8597834856329814e-05, 2.8597834856329814e-05, 4.085404979475688e-05, 4.085404979475688e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:46:42,399] [INFO] [timer.py:260:stop] epoch=0/micro_step=37000/global_step=37000, RunningAvgSamplesPerSec=27.54193624235982, CurrSamplesPerSec=28.497188026024276, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [ 490/2809]  eta: 0:22:31  lr: 0.000041  min_lr: 0.000000  loss: 4.3379 (4.3545)  class_acc: 0.2083 (0.2402)  loss_scale: 32768.0000 (45514.8187)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1552  max mem: 15572
Epoch: [13]  [ 500/2809]  eta: 0:22:20  lr: 0.000041  min_lr: 0.000000  loss: 4.3379 (4.3533)  class_acc: 0.2083 (0.2404)  loss_scale: 32768.0000 (45260.3912)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0905  max mem: 15572
Epoch: [13]  [ 510/2809]  eta: 0:22:16  lr: 0.000041  min_lr: 0.000000  loss: 4.3004 (4.3518)  class_acc: 0.2500 (0.2406)  loss_scale: 32768.0000 (45015.9217)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0834  max mem: 15572
Epoch: [13]  [ 520/2809]  eta: 0:22:07  lr: 0.000041  min_lr: 0.000000  loss: 4.3038 (4.3538)  class_acc: 0.1667 (0.2398)  loss_scale: 32768.0000 (44780.8369)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1171  max mem: 15572
Epoch: [13]  [ 530/2809]  eta: 0:22:05  lr: 0.000041  min_lr: 0.000000  loss: 4.3045 (4.3517)  class_acc: 0.2500 (0.2407)  loss_scale: 32768.0000 (44554.6064)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1346  max mem: 15572
Epoch: [13]  [ 540/2809]  eta: 0:21:54  lr: 0.000041  min_lr: 0.000000  loss: 4.2859 (4.3536)  class_acc: 0.2500 (0.2404)  loss_scale: 32768.0000 (44336.7394)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0951  max mem: 15572
Epoch: [13]  [ 550/2809]  eta: 0:21:47  lr: 0.000041  min_lr: 0.000000  loss: 4.3731 (4.3539)  class_acc: 0.2083 (0.2403)  loss_scale: 32768.0000 (44126.7804)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0379  max mem: 15572
Epoch: [13]  [ 560/2809]  eta: 0:21:39  lr: 0.000041  min_lr: 0.000000  loss: 4.4319 (4.3550)  class_acc: 0.2083 (0.2398)  loss_scale: 32768.0000 (43924.3066)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.0814  max mem: 15572
Epoch: [13]  [ 570/2809]  eta: 0:21:31  lr: 0.000041  min_lr: 0.000000  loss: 4.2931 (4.3535)  class_acc: 0.2500 (0.2399)  loss_scale: 32768.0000 (43728.9247)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0973  max mem: 15572
Epoch: [13]  [ 580/2809]  eta: 0:21:27  lr: 0.000041  min_lr: 0.000000  loss: 4.2796 (4.3522)  class_acc: 0.2500 (0.2402)  loss_scale: 32768.0000 (43540.2685)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1333  max mem: 15572
[2025-01-15 20:47:41,005] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:47:41,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 590/2809]  eta: 0:21:22  lr: 0.000041  min_lr: 0.000000  loss: 4.4275 (4.3541)  class_acc: 0.2083 (0.2405)  loss_scale: 32768.0000 (43468.8866)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1459  max mem: 15572
Epoch: [13]  [ 600/2809]  eta: 0:21:12  lr: 0.000041  min_lr: 0.000000  loss: 4.4364 (4.3555)  class_acc: 0.1667 (0.2402)  loss_scale: 65536.0000 (43836.0599)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0662  max mem: 15572
Epoch: [13]  [ 610/2809]  eta: 0:21:04  lr: 0.000041  min_lr: 0.000000  loss: 4.3953 (4.3556)  class_acc: 0.2083 (0.2401)  loss_scale: 65536.0000 (44191.2144)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0293  max mem: 15572
Epoch: [13]  [ 620/2809]  eta: 0:21:00  lr: 0.000041  min_lr: 0.000000  loss: 4.3968 (4.3574)  class_acc: 0.2083 (0.2398)  loss_scale: 65536.0000 (44534.9308)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1107  max mem: 15572
Epoch: [13]  [ 630/2809]  eta: 0:20:56  lr: 0.000041  min_lr: 0.000000  loss: 4.4832 (4.3581)  class_acc: 0.1667 (0.2394)  loss_scale: 65536.0000 (44867.7528)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1781  max mem: 15572
[2025-01-15 20:48:08,669] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37156
[2025-01-15 20:48:08,670] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:48:08,671] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 640/2809]  eta: 0:20:49  lr: 0.000041  min_lr: 0.000000  loss: 4.4268 (4.3603)  class_acc: 0.1667 (0.2390)  loss_scale: 65536.0000 (45087.9501)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1440  max mem: 15572
Epoch: [13]  [ 650/2809]  eta: 0:20:45  lr: 0.000041  min_lr: 0.000000  loss: 4.3959 (4.3591)  class_acc: 0.2083 (0.2390)  loss_scale: 32768.0000 (44898.7035)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1328  max mem: 15572
Epoch: [13]  [ 660/2809]  eta: 0:20:37  lr: 0.000041  min_lr: 0.000000  loss: 4.4057 (4.3594)  class_acc: 0.2083 (0.2383)  loss_scale: 32768.0000 (44715.1831)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1248  max mem: 15572
Epoch: [13]  [ 670/2809]  eta: 0:20:33  lr: 0.000041  min_lr: 0.000000  loss: 4.4275 (4.3596)  class_acc: 0.2083 (0.2389)  loss_scale: 32768.0000 (44537.1326)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1174  max mem: 15572
Epoch: [13]  [ 680/2809]  eta: 0:20:25  lr: 0.000041  min_lr: 0.000000  loss: 4.4136 (4.3587)  class_acc: 0.2500 (0.2386)  loss_scale: 32768.0000 (44364.3113)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0955  max mem: 15572
Epoch: [13]  [ 690/2809]  eta: 0:20:19  lr: 0.000041  min_lr: 0.000000  loss: 4.2538 (4.3574)  class_acc: 0.2083 (0.2383)  loss_scale: 32768.0000 (44196.4920)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0772  max mem: 15572
Epoch: [13]  [ 700/2809]  eta: 0:20:11  lr: 0.000041  min_lr: 0.000000  loss: 4.3163 (4.3581)  class_acc: 0.1667 (0.2375)  loss_scale: 32768.0000 (44033.4608)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0863  max mem: 15572
Epoch: [13]  [ 710/2809]  eta: 0:20:03  lr: 0.000041  min_lr: 0.000000  loss: 4.3805 (4.3571)  class_acc: 0.2083 (0.2379)  loss_scale: 32768.0000 (43875.0155)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0554  max mem: 15572
Epoch: [13]  [ 720/2809]  eta: 0:19:57  lr: 0.000041  min_lr: 0.000000  loss: 4.2289 (4.3563)  class_acc: 0.2917 (0.2380)  loss_scale: 32768.0000 (43720.9653)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0808  max mem: 15572
Epoch: [13]  [ 730/2809]  eta: 0:19:52  lr: 0.000041  min_lr: 0.000000  loss: 4.2712 (4.3553)  class_acc: 0.2917 (0.2384)  loss_scale: 32768.0000 (43571.1300)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1261  max mem: 15572
Epoch: [13]  [ 740/2809]  eta: 0:19:45  lr: 0.000041  min_lr: 0.000000  loss: 4.3045 (4.3562)  class_acc: 0.2083 (0.2382)  loss_scale: 32768.0000 (43425.3387)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1271  max mem: 15572
Epoch: [13]  [ 750/2809]  eta: 0:19:40  lr: 0.000041  min_lr: 0.000000  loss: 4.5718 (4.3590)  class_acc: 0.2083 (0.2374)  loss_scale: 32768.0000 (43283.4301)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1215  max mem: 15572
Epoch: [13]  [ 760/2809]  eta: 0:19:32  lr: 0.000041  min_lr: 0.000000  loss: 4.5292 (4.3600)  class_acc: 0.2083 (0.2377)  loss_scale: 32768.0000 (43145.2510)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1075  max mem: 15572
[2025-01-15 20:49:20,395] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:49:20,396] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 770/2809]  eta: 0:19:27  lr: 0.000041  min_lr: 0.000000  loss: 4.3862 (4.3609)  class_acc: 0.2500 (0.2377)  loss_scale: 32768.0000 (43138.1582)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1131  max mem: 15572
Epoch: [13]  [ 780/2809]  eta: 0:19:22  lr: 0.000041  min_lr: 0.000000  loss: 4.4754 (4.3641)  class_acc: 0.2500 (0.2375)  loss_scale: 65536.0000 (43424.9424)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1465  max mem: 15572
Epoch: [13]  [ 790/2809]  eta: 0:19:15  lr: 0.000041  min_lr: 0.000000  loss: 4.4802 (4.3648)  class_acc: 0.2500 (0.2379)  loss_scale: 65536.0000 (43704.4753)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1082  max mem: 15572
Epoch: [13]  [ 800/2809]  eta: 0:19:09  lr: 0.000041  min_lr: 0.000000  loss: 4.4164 (4.3654)  class_acc: 0.2083 (0.2371)  loss_scale: 65536.0000 (43977.0287)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1009  max mem: 15572
Epoch: [13]  [ 810/2809]  eta: 0:19:03  lr: 0.000041  min_lr: 0.000000  loss: 4.4630 (4.3662)  class_acc: 0.1667 (0.2365)  loss_scale: 65536.0000 (44242.8607)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1190  max mem: 15572
Epoch: [13]  [ 820/2809]  eta: 0:18:57  lr: 0.000041  min_lr: 0.000000  loss: 4.4949 (4.3677)  class_acc: 0.2083 (0.2365)  loss_scale: 65536.0000 (44502.2168)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1035  max mem: 15572
Epoch: [13]  [ 830/2809]  eta: 0:18:50  lr: 0.000041  min_lr: 0.000000  loss: 4.5634 (4.3695)  class_acc: 0.2083 (0.2365)  loss_scale: 65536.0000 (44755.3309)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0861  max mem: 15572
Epoch: [13]  [ 840/2809]  eta: 0:18:42  lr: 0.000041  min_lr: 0.000000  loss: 4.5564 (4.3707)  class_acc: 0.1667 (0.2362)  loss_scale: 65536.0000 (45002.4257)  weight_decay: 0.0500 (0.0500)  time: 0.5093  data: 0.0787  max mem: 15572
Epoch: [13]  [ 850/2809]  eta: 0:18:37  lr: 0.000041  min_lr: 0.000000  loss: 4.3242 (4.3705)  class_acc: 0.1667 (0.2365)  loss_scale: 65536.0000 (45243.7133)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0978  max mem: 15572
Epoch: [13]  [ 860/2809]  eta: 0:18:32  lr: 0.000041  min_lr: 0.000000  loss: 4.2606 (4.3692)  class_acc: 0.2500 (0.2368)  loss_scale: 65536.0000 (45479.3961)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1590  max mem: 15572
Epoch: [13]  [ 870/2809]  eta: 0:18:26  lr: 0.000041  min_lr: 0.000000  loss: 4.3525 (4.3697)  class_acc: 0.1667 (0.2366)  loss_scale: 65536.0000 (45709.6670)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1436  max mem: 15572
Epoch: [13]  [ 880/2809]  eta: 0:18:19  lr: 0.000041  min_lr: 0.000000  loss: 4.5010 (4.3708)  class_acc: 0.1667 (0.2364)  loss_scale: 65536.0000 (45934.7106)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0883  max mem: 15572
Epoch: [13]  [ 890/2809]  eta: 0:18:14  lr: 0.000041  min_lr: 0.000000  loss: 4.3908 (4.3702)  class_acc: 0.2500 (0.2364)  loss_scale: 65536.0000 (46154.7026)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1160  max mem: 15572
[2025-01-15 20:50:31,038] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:50:31,038] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:50:31,928] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37415
[2025-01-15 20:50:31,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:50:31,928] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 900/2809]  eta: 0:18:06  lr: 0.000041  min_lr: 0.000000  loss: 4.3098 (4.3702)  class_acc: 0.2083 (0.2365)  loss_scale: 65536.0000 (46515.2852)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0680  max mem: 15572
Epoch: [13]  [ 910/2809]  eta: 0:18:01  lr: 0.000041  min_lr: 0.000000  loss: 4.3880 (4.3709)  class_acc: 0.2083 (0.2363)  loss_scale: 65536.0000 (46724.0746)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0742  max mem: 15572
Epoch: [13]  [ 920/2809]  eta: 0:17:53  lr: 0.000041  min_lr: 0.000000  loss: 4.3616 (4.3701)  class_acc: 0.2500 (0.2367)  loss_scale: 65536.0000 (46928.3301)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0934  max mem: 15572
Epoch: [13]  [ 930/2809]  eta: 0:17:46  lr: 0.000041  min_lr: 0.000000  loss: 4.3616 (4.3701)  class_acc: 0.2083 (0.2364)  loss_scale: 65536.0000 (47128.1976)  weight_decay: 0.0500 (0.0500)  time: 0.4887  data: 0.0598  max mem: 15572
[2025-01-15 20:50:54,410] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37455
[2025-01-15 20:50:54,410] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:50:54,410] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 940/2809]  eta: 0:17:43  lr: 0.000041  min_lr: 0.000000  loss: 4.3437 (4.3691)  class_acc: 0.2083 (0.2369)  loss_scale: 65536.0000 (47219.3496)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1608  max mem: 15572
Epoch: [13]  [ 950/2809]  eta: 0:17:40  lr: 0.000041  min_lr: 0.000000  loss: 4.4502 (4.3711)  class_acc: 0.2917 (0.2376)  loss_scale: 32768.0000 (47067.3901)  weight_decay: 0.0500 (0.0500)  time: 0.6995  data: 0.2537  max mem: 15572
Epoch: [13]  [ 960/2809]  eta: 0:17:35  lr: 0.000041  min_lr: 0.000000  loss: 4.4491 (4.3691)  class_acc: 0.2500 (0.2379)  loss_scale: 32768.0000 (46918.5931)  weight_decay: 0.0500 (0.0500)  time: 0.6576  data: 0.1961  max mem: 15572
Epoch: [13]  [ 970/2809]  eta: 0:17:28  lr: 0.000041  min_lr: 0.000000  loss: 4.1807 (4.3687)  class_acc: 0.2083 (0.2379)  loss_scale: 32768.0000 (46772.8610)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.1013  max mem: 15572
Epoch: [13]  [ 980/2809]  eta: 0:17:23  lr: 0.000041  min_lr: 0.000000  loss: 4.1807 (4.3674)  class_acc: 0.2500 (0.2384)  loss_scale: 32768.0000 (46630.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1199  max mem: 15572
Epoch: [13]  [ 990/2809]  eta: 0:17:18  lr: 0.000041  min_lr: 0.000000  loss: 4.3649 (4.3685)  class_acc: 0.2500 (0.2386)  loss_scale: 32768.0000 (46490.2200)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.1760  max mem: 15572
Epoch: [13]  [1000/2809]  eta: 0:17:11  lr: 0.000041  min_lr: 0.000000  loss: 4.3649 (4.3677)  class_acc: 0.2500 (0.2389)  loss_scale: 32768.0000 (46353.1349)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1040  max mem: 15572
Epoch: [13]  [1010/2809]  eta: 0:17:06  lr: 0.000041  min_lr: 0.000000  loss: 4.3239 (4.3677)  class_acc: 0.2500 (0.2394)  loss_scale: 32768.0000 (46218.7616)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1195  max mem: 15572
Epoch: [13]  [1020/2809]  eta: 0:17:00  lr: 0.000041  min_lr: 0.000000  loss: 4.4018 (4.3679)  class_acc: 0.2500 (0.2396)  loss_scale: 32768.0000 (46087.0206)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1486  max mem: 15572
Epoch: [13]  [1030/2809]  eta: 0:16:54  lr: 0.000041  min_lr: 0.000000  loss: 4.4716 (4.3685)  class_acc: 0.2083 (0.2395)  loss_scale: 32768.0000 (45957.8351)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0933  max mem: 15572
Epoch: [13]  [1040/2809]  eta: 0:16:47  lr: 0.000041  min_lr: 0.000000  loss: 4.3337 (4.3676)  class_acc: 0.2500 (0.2398)  loss_scale: 32768.0000 (45831.1316)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0833  max mem: 15572
Epoch: [13]  [1050/2809]  eta: 0:16:42  lr: 0.000041  min_lr: 0.000000  loss: 4.2956 (4.3677)  class_acc: 0.2500 (0.2397)  loss_scale: 32768.0000 (45706.8392)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1254  max mem: 15572
Epoch: [13]  [1060/2809]  eta: 0:16:36  lr: 0.000041  min_lr: 0.000000  loss: 4.3269 (4.3674)  class_acc: 0.2083 (0.2397)  loss_scale: 32768.0000 (45584.8897)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1533  max mem: 15572
[2025-01-15 20:52:09,616] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:52:09,617] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [1070/2809]  eta: 0:16:31  lr: 0.000041  min_lr: 0.000000  loss: 4.3269 (4.3668)  class_acc: 0.2917 (0.2400)  loss_scale: 32768.0000 (45587.6004)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1507  max mem: 15572
Epoch: [13]  [1080/2809]  eta: 0:16:24  lr: 0.000041  min_lr: 0.000000  loss: 4.1399 (4.3658)  class_acc: 0.2500 (0.2402)  loss_scale: 65536.0000 (45772.1369)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0983  max mem: 15572
Epoch: [13]  [1090/2809]  eta: 0:16:19  lr: 0.000041  min_lr: 0.000000  loss: 4.2554 (4.3667)  class_acc: 0.2500 (0.2403)  loss_scale: 65536.0000 (45953.2906)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0711  max mem: 15572
Epoch: [13]  [1100/2809]  eta: 0:16:13  lr: 0.000041  min_lr: 0.000000  loss: 4.2554 (4.3653)  class_acc: 0.2500 (0.2405)  loss_scale: 65536.0000 (46131.1535)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0989  max mem: 15572
Epoch: [13]  [1110/2809]  eta: 0:16:06  lr: 0.000041  min_lr: 0.000000  loss: 4.2039 (4.3645)  class_acc: 0.2500 (0.2407)  loss_scale: 65536.0000 (46305.8146)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0698  max mem: 15572
Epoch: [13]  [1120/2809]  eta: 0:16:00  lr: 0.000041  min_lr: 0.000000  loss: 4.4109 (4.3655)  class_acc: 0.2083 (0.2404)  loss_scale: 65536.0000 (46477.3595)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0651  max mem: 15572
Epoch: [13]  [1130/2809]  eta: 0:15:53  lr: 0.000041  min_lr: 0.000000  loss: 4.4729 (4.3659)  class_acc: 0.2083 (0.2405)  loss_scale: 65536.0000 (46645.8709)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0668  max mem: 15572
Epoch: [13]  [1140/2809]  eta: 0:15:46  lr: 0.000041  min_lr: 0.000000  loss: 4.4265 (4.3666)  class_acc: 0.2083 (0.2400)  loss_scale: 65536.0000 (46811.4286)  weight_decay: 0.0500 (0.0500)  time: 0.4926  data: 0.0585  max mem: 15572
[2025-01-15 20:52:50,892] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37662
[2025-01-15 20:52:50,893] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:52:50,893] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [1150/2809]  eta: 0:15:41  lr: 0.000041  min_lr: 0.000000  loss: 4.3926 (4.3664)  class_acc: 0.1667 (0.2403)  loss_scale: 65536.0000 (46803.2945)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0901  max mem: 15572
Epoch: [13]  [1160/2809]  eta: 0:15:34  lr: 0.000041  min_lr: 0.000000  loss: 4.3336 (4.3670)  class_acc: 0.2500 (0.2401)  loss_scale: 32768.0000 (46682.4048)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0874  max mem: 15572
Epoch: [13]  [1170/2809]  eta: 0:15:28  lr: 0.000041  min_lr: 0.000000  loss: 4.4039 (4.3666)  class_acc: 0.2500 (0.2398)  loss_scale: 32768.0000 (46563.5798)  weight_decay: 0.0500 (0.0500)  time: 0.4917  data: 0.0501  max mem: 15572
Epoch: [13]  [1180/2809]  eta: 0:15:21  lr: 0.000041  min_lr: 0.000000  loss: 4.4282 (4.3673)  class_acc: 0.2083 (0.2396)  loss_scale: 32768.0000 (46446.7671)  weight_decay: 0.0500 (0.0500)  time: 0.5178  data: 0.0636  max mem: 15572
Epoch: [13]  [1190/2809]  eta: 0:15:16  lr: 0.000040  min_lr: 0.000000  loss: 4.4541 (4.3693)  class_acc: 0.2083 (0.2393)  loss_scale: 32768.0000 (46331.9160)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1167  max mem: 15572
Epoch: [13]  [1200/2809]  eta: 0:15:10  lr: 0.000040  min_lr: 0.000000  loss: 4.4662 (4.3692)  class_acc: 0.2083 (0.2390)  loss_scale: 32768.0000 (46218.9775)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1439  max mem: 15572
Epoch: [13]  [1210/2809]  eta: 0:15:05  lr: 0.000040  min_lr: 0.000000  loss: 4.3283 (4.3693)  class_acc: 0.2083 (0.2389)  loss_scale: 32768.0000 (46107.9042)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.1043  max mem: 15572
Epoch: [13]  [1220/2809]  eta: 0:14:58  lr: 0.000040  min_lr: 0.000000  loss: 4.3361 (4.3686)  class_acc: 0.2500 (0.2395)  loss_scale: 32768.0000 (45998.6503)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0861  max mem: 15572
Epoch: [13]  [1230/2809]  eta: 0:14:55  lr: 0.000040  min_lr: 0.000000  loss: 4.1764 (4.3670)  class_acc: 0.3333 (0.2404)  loss_scale: 32768.0000 (45891.1714)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1799  max mem: 15572
Epoch: [13]  [1240/2809]  eta: 0:14:50  lr: 0.000040  min_lr: 0.000000  loss: 4.2093 (4.3661)  class_acc: 0.2500 (0.2401)  loss_scale: 32768.0000 (45785.4247)  weight_decay: 0.0500 (0.0500)  time: 0.6694  data: 0.2345  max mem: 15572
Epoch: [13]  [1250/2809]  eta: 0:14:44  lr: 0.000040  min_lr: 0.000000  loss: 4.3885 (4.3669)  class_acc: 0.1667 (0.2397)  loss_scale: 32768.0000 (45681.3685)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1713  max mem: 15572
Epoch: [13]  [1260/2809]  eta: 0:14:39  lr: 0.000040  min_lr: 0.000000  loss: 4.4654 (4.3680)  class_acc: 0.2083 (0.2396)  loss_scale: 32768.0000 (45578.9627)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1702  max mem: 15572
Epoch: [13]  [1270/2809]  eta: 0:14:33  lr: 0.000040  min_lr: 0.000000  loss: 4.3821 (4.3672)  class_acc: 0.2500 (0.2400)  loss_scale: 32768.0000 (45478.1684)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1266  max mem: 15572
[2025-01-15 20:54:03,553] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:54:03,553] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [1280/2809]  eta: 0:14:27  lr: 0.000040  min_lr: 0.000000  loss: 4.3318 (4.3685)  class_acc: 0.2083 (0.2392)  loss_scale: 32768.0000 (45558.0078)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0851  max mem: 15572
Epoch: [13]  [1290/2809]  eta: 0:14:21  lr: 0.000040  min_lr: 0.000000  loss: 4.4141 (4.3672)  class_acc: 0.1667 (0.2393)  loss_scale: 65536.0000 (45712.7560)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0764  max mem: 15572
Epoch: [13]  [1300/2809]  eta: 0:14:15  lr: 0.000040  min_lr: 0.000000  loss: 4.2902 (4.3670)  class_acc: 0.2083 (0.2392)  loss_scale: 65536.0000 (45865.1253)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0760  max mem: 15572
Epoch: [13]  [1310/2809]  eta: 0:14:09  lr: 0.000040  min_lr: 0.000000  loss: 4.3317 (4.3663)  class_acc: 0.2500 (0.2398)  loss_scale: 65536.0000 (46015.1701)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1004  max mem: 15572
[2025-01-15 20:54:25,323] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37831
[2025-01-15 20:54:25,324] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:54:25,324] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [1320/2809]  eta: 0:14:04  lr: 0.000040  min_lr: 0.000000  loss: 4.3035 (4.3655)  class_acc: 0.2500 (0.2401)  loss_scale: 65536.0000 (45989.3051)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1072  max mem: 15572
Epoch: [13]  [1330/2809]  eta: 0:13:58  lr: 0.000040  min_lr: 0.000000  loss: 4.3200 (4.3653)  class_acc: 0.2083 (0.2396)  loss_scale: 32768.0000 (45889.9715)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0918  max mem: 15572
Epoch: [13]  [1340/2809]  eta: 0:13:51  lr: 0.000040  min_lr: 0.000000  loss: 4.4219 (4.3663)  class_acc: 0.2083 (0.2395)  loss_scale: 32768.0000 (45792.1193)  weight_decay: 0.0500 (0.0500)  time: 0.4901  data: 0.0347  max mem: 15572
Epoch: [13]  [1350/2809]  eta: 0:13:44  lr: 0.000040  min_lr: 0.000000  loss: 4.4995 (4.3663)  class_acc: 0.2500 (0.2395)  loss_scale: 32768.0000 (45695.7158)  weight_decay: 0.0500 (0.0500)  time: 0.4752  data: 0.0270  max mem: 15572
Epoch: [13]  [1360/2809]  eta: 0:13:39  lr: 0.000040  min_lr: 0.000000  loss: 4.4788 (4.3672)  class_acc: 0.2500 (0.2393)  loss_scale: 32768.0000 (45600.7289)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0962  max mem: 15572
Epoch: [13]  [1370/2809]  eta: 0:13:33  lr: 0.000040  min_lr: 0.000000  loss: 4.4947 (4.3679)  class_acc: 0.1667 (0.2391)  loss_scale: 32768.0000 (45507.1276)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1215  max mem: 15572
Epoch: [13]  [1380/2809]  eta: 0:13:27  lr: 0.000040  min_lr: 0.000000  loss: 4.4812 (4.3679)  class_acc: 0.1667 (0.2390)  loss_scale: 32768.0000 (45414.8820)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0636  max mem: 15572
Epoch: [13]  [1390/2809]  eta: 0:13:20  lr: 0.000040  min_lr: 0.000000  loss: 4.3771 (4.3673)  class_acc: 0.2083 (0.2390)  loss_scale: 32768.0000 (45323.9626)  weight_decay: 0.0500 (0.0500)  time: 0.5015  data: 0.0464  max mem: 15572
Epoch: [13]  [1400/2809]  eta: 0:13:14  lr: 0.000040  min_lr: 0.000000  loss: 4.3348 (4.3662)  class_acc: 0.2083 (0.2393)  loss_scale: 32768.0000 (45234.3412)  weight_decay: 0.0500 (0.0500)  time: 0.5023  data: 0.0509  max mem: 15572
Epoch: [13]  [1410/2809]  eta: 0:13:08  lr: 0.000040  min_lr: 0.000000  loss: 4.2498 (4.3658)  class_acc: 0.2500 (0.2390)  loss_scale: 32768.0000 (45145.9901)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0613  max mem: 15572
Epoch: [13]  [1420/2809]  eta: 0:13:02  lr: 0.000040  min_lr: 0.000000  loss: 4.2498 (4.3654)  class_acc: 0.2083 (0.2387)  loss_scale: 32768.0000 (45058.8825)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0832  max mem: 15572
Epoch: [13]  [1430/2809]  eta: 0:12:57  lr: 0.000040  min_lr: 0.000000  loss: 4.2726 (4.3645)  class_acc: 0.2500 (0.2390)  loss_scale: 32768.0000 (44972.9923)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1100  max mem: 15572
Epoch: [13]  [1440/2809]  eta: 0:12:51  lr: 0.000040  min_lr: 0.000000  loss: 4.2791 (4.3636)  class_acc: 0.2917 (0.2391)  loss_scale: 32768.0000 (44888.2942)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1316  max mem: 15572
[2025-01-15 20:55:34,071] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:55:34,071] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [1450/2809]  eta: 0:12:45  lr: 0.000040  min_lr: 0.000000  loss: 4.3057 (4.3638)  class_acc: 0.2083 (0.2387)  loss_scale: 32768.0000 (44985.4280)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0599  max mem: 15572
Epoch: [13]  [1460/2809]  eta: 0:12:39  lr: 0.000040  min_lr: 0.000000  loss: 4.3434 (4.3637)  class_acc: 0.1667 (0.2382)  loss_scale: 65536.0000 (45126.0890)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0590  max mem: 15572
Epoch: [13]  [1470/2809]  eta: 0:12:33  lr: 0.000040  min_lr: 0.000000  loss: 4.4012 (4.3640)  class_acc: 0.2083 (0.2383)  loss_scale: 65536.0000 (45264.8375)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.1195  max mem: 15572
Epoch: [13]  [1480/2809]  eta: 0:12:27  lr: 0.000040  min_lr: 0.000000  loss: 4.3571 (4.3643)  class_acc: 0.2083 (0.2383)  loss_scale: 65536.0000 (45401.7124)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.1097  max mem: 15572
[2025-01-15 20:55:54,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=38000, skipped=240, lr=[3.9089269712705475e-07, 3.9089269712705475e-07, 5.584181387529355e-07, 5.584181387529355e-07, 7.977401982184794e-07, 7.977401982184794e-07, 1.1396288545978277e-06, 1.1396288545978277e-06, 1.6280412208540395e-06, 1.6280412208540395e-06, 2.325773172648628e-06, 2.325773172648628e-06, 3.3225331037837544e-06, 3.3225331037837544e-06, 4.746475862548221e-06, 4.746475862548221e-06, 6.780679803640316e-06, 6.780679803640316e-06, 9.686685433771882e-06, 9.686685433771882e-06, 1.3838122048245544e-05, 1.3838122048245544e-05, 1.9768745783207923e-05, 1.9768745783207923e-05, 2.8241065404582747e-05, 2.8241065404582747e-05, 4.034437914940393e-05, 4.034437914940393e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:55:54,387] [INFO] [timer.py:260:stop] epoch=0/micro_step=38000/global_step=38000, RunningAvgSamplesPerSec=27.563465185563274, CurrSamplesPerSec=31.111899030883407, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [1490/2809]  eta: 0:12:22  lr: 0.000040  min_lr: 0.000000  loss: 4.2743 (4.3633)  class_acc: 0.2500 (0.2385)  loss_scale: 65536.0000 (45536.7512)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.1141  max mem: 15572
[2025-01-15 20:56:02,578] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38014
[2025-01-15 20:56:02,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:56:02,580] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [1500/2809]  eta: 0:12:16  lr: 0.000040  min_lr: 0.000000  loss: 4.2237 (4.3633)  class_acc: 0.2917 (0.2388)  loss_scale: 65536.0000 (45582.6676)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1348  max mem: 15572
Epoch: [13]  [1510/2809]  eta: 0:12:10  lr: 0.000040  min_lr: 0.000000  loss: 4.3834 (4.3639)  class_acc: 0.2083 (0.2387)  loss_scale: 32768.0000 (45497.8584)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1399  max mem: 15572
Epoch: [13]  [1520/2809]  eta: 0:12:04  lr: 0.000040  min_lr: 0.000000  loss: 4.3834 (4.3639)  class_acc: 0.2083 (0.2389)  loss_scale: 32768.0000 (45414.1644)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0871  max mem: 15572
Epoch: [13]  [1530/2809]  eta: 0:11:58  lr: 0.000040  min_lr: 0.000000  loss: 4.3201 (4.3636)  class_acc: 0.2500 (0.2388)  loss_scale: 32768.0000 (45331.5637)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0697  max mem: 15572
Epoch: [13]  [1540/2809]  eta: 0:11:52  lr: 0.000040  min_lr: 0.000000  loss: 4.3915 (4.3641)  class_acc: 0.2083 (0.2388)  loss_scale: 32768.0000 (45250.0350)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0759  max mem: 15572
Epoch: [13]  [1550/2809]  eta: 0:11:47  lr: 0.000040  min_lr: 0.000000  loss: 4.3721 (4.3638)  class_acc: 0.2083 (0.2389)  loss_scale: 32768.0000 (45169.5577)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0666  max mem: 15572
Epoch: [13]  [1560/2809]  eta: 0:11:41  lr: 0.000040  min_lr: 0.000000  loss: 4.3355 (4.3634)  class_acc: 0.2083 (0.2388)  loss_scale: 32768.0000 (45090.1115)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1327  max mem: 15572
Epoch: [13]  [1570/2809]  eta: 0:11:36  lr: 0.000040  min_lr: 0.000000  loss: 4.4458 (4.3640)  class_acc: 0.2500 (0.2391)  loss_scale: 32768.0000 (45011.6766)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1620  max mem: 15572
Epoch: [13]  [1580/2809]  eta: 0:11:30  lr: 0.000040  min_lr: 0.000000  loss: 4.4877 (4.3650)  class_acc: 0.2500 (0.2389)  loss_scale: 32768.0000 (44934.2340)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1456  max mem: 15572
Epoch: [13]  [1590/2809]  eta: 0:11:25  lr: 0.000040  min_lr: 0.000000  loss: 4.4387 (4.3654)  class_acc: 0.2500 (0.2390)  loss_scale: 32768.0000 (44857.7649)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1689  max mem: 15572
Epoch: [13]  [1600/2809]  eta: 0:11:19  lr: 0.000040  min_lr: 0.000000  loss: 4.4387 (4.3659)  class_acc: 0.2500 (0.2389)  loss_scale: 32768.0000 (44782.2511)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1483  max mem: 15572
Epoch: [13]  [1610/2809]  eta: 0:11:13  lr: 0.000040  min_lr: 0.000000  loss: 4.3812 (4.3659)  class_acc: 0.2083 (0.2385)  loss_scale: 32768.0000 (44707.6747)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0661  max mem: 15572
Epoch: [13]  [1620/2809]  eta: 0:11:07  lr: 0.000040  min_lr: 0.000000  loss: 4.3850 (4.3659)  class_acc: 0.2083 (0.2383)  loss_scale: 32768.0000 (44634.0185)  weight_decay: 0.0500 (0.0500)  time: 0.4999  data: 0.0491  max mem: 15572
[2025-01-15 20:57:15,027] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:57:15,027] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [1630/2809]  eta: 0:11:02  lr: 0.000040  min_lr: 0.000000  loss: 4.4700 (4.3659)  class_acc: 0.2083 (0.2382)  loss_scale: 32768.0000 (44661.7192)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1288  max mem: 15572
Epoch: [13]  [1640/2809]  eta: 0:10:56  lr: 0.000040  min_lr: 0.000000  loss: 4.3924 (4.3658)  class_acc: 0.1667 (0.2379)  loss_scale: 65536.0000 (44788.9238)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1451  max mem: 15572
Epoch: [13]  [1650/2809]  eta: 0:10:51  lr: 0.000040  min_lr: 0.000000  loss: 4.3869 (4.3660)  class_acc: 0.2083 (0.2378)  loss_scale: 65536.0000 (44914.5875)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1296  max mem: 15572
[2025-01-15 20:57:34,138] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38177
[2025-01-15 20:57:34,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:57:34,140] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [1660/2809]  eta: 0:10:46  lr: 0.000040  min_lr: 0.000000  loss: 4.3344 (4.3650)  class_acc: 0.2500 (0.2380)  loss_scale: 65536.0000 (45019.0102)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1435  max mem: 15572
Epoch: [13]  [1670/2809]  eta: 0:10:40  lr: 0.000040  min_lr: 0.000000  loss: 4.2995 (4.3644)  class_acc: 0.2083 (0.2379)  loss_scale: 32768.0000 (44945.6948)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1144  max mem: 15572
Epoch: [13]  [1680/2809]  eta: 0:10:34  lr: 0.000040  min_lr: 0.000000  loss: 4.3371 (4.3648)  class_acc: 0.1667 (0.2378)  loss_scale: 32768.0000 (44873.2516)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1078  max mem: 15572
Epoch: [13]  [1690/2809]  eta: 0:10:28  lr: 0.000040  min_lr: 0.000000  loss: 4.4087 (4.3650)  class_acc: 0.2083 (0.2378)  loss_scale: 32768.0000 (44801.6653)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1228  max mem: 15572
Epoch: [13]  [1700/2809]  eta: 0:10:23  lr: 0.000040  min_lr: 0.000000  loss: 4.4799 (4.3657)  class_acc: 0.2083 (0.2378)  loss_scale: 32768.0000 (44730.9206)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0936  max mem: 15572
Epoch: [13]  [1710/2809]  eta: 0:10:17  lr: 0.000040  min_lr: 0.000000  loss: 4.4609 (4.3659)  class_acc: 0.2083 (0.2378)  loss_scale: 32768.0000 (44661.0029)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1091  max mem: 15572
Epoch: [13]  [1720/2809]  eta: 0:10:11  lr: 0.000040  min_lr: 0.000000  loss: 4.4449 (4.3665)  class_acc: 0.2083 (0.2378)  loss_scale: 32768.0000 (44591.8977)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0801  max mem: 15572
Epoch: [13]  [1730/2809]  eta: 0:10:05  lr: 0.000040  min_lr: 0.000000  loss: 4.4221 (4.3669)  class_acc: 0.2083 (0.2374)  loss_scale: 32768.0000 (44523.5910)  weight_decay: 0.0500 (0.0500)  time: 0.4672  data: 0.0245  max mem: 15572
Epoch: [13]  [1740/2809]  eta: 0:09:59  lr: 0.000040  min_lr: 0.000000  loss: 4.3320 (4.3670)  class_acc: 0.1667 (0.2373)  loss_scale: 32768.0000 (44456.0689)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0806  max mem: 15572
Epoch: [13]  [1750/2809]  eta: 0:09:54  lr: 0.000040  min_lr: 0.000000  loss: 4.3241 (4.3667)  class_acc: 0.2083 (0.2371)  loss_scale: 32768.0000 (44389.3181)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1929  max mem: 15572
Epoch: [13]  [1760/2809]  eta: 0:09:49  lr: 0.000040  min_lr: 0.000000  loss: 4.3790 (4.3676)  class_acc: 0.2083 (0.2371)  loss_scale: 32768.0000 (44323.3254)  weight_decay: 0.0500 (0.0500)  time: 0.6788  data: 0.2296  max mem: 15572
Epoch: [13]  [1770/2809]  eta: 0:09:44  lr: 0.000040  min_lr: 0.000000  loss: 4.5067 (4.3682)  class_acc: 0.2083 (0.2371)  loss_scale: 32768.0000 (44258.0779)  weight_decay: 0.0500 (0.0500)  time: 0.6240  data: 0.1840  max mem: 15572
Epoch: [13]  [1780/2809]  eta: 0:09:38  lr: 0.000040  min_lr: 0.000000  loss: 4.4123 (4.3678)  class_acc: 0.2500 (0.2374)  loss_scale: 32768.0000 (44193.5632)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1480  max mem: 15572
[2025-01-15 20:58:46,989] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:58:46,989] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [1790/2809]  eta: 0:09:33  lr: 0.000040  min_lr: 0.000000  loss: 4.4443 (4.3680)  class_acc: 0.2500 (0.2376)  loss_scale: 32768.0000 (44166.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1021  max mem: 15572
Epoch: [13]  [1800/2809]  eta: 0:09:27  lr: 0.000040  min_lr: 0.000000  loss: 4.4690 (4.3678)  class_acc: 0.2083 (0.2375)  loss_scale: 65536.0000 (44285.0150)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0765  max mem: 15572
Epoch: [13]  [1810/2809]  eta: 0:09:21  lr: 0.000040  min_lr: 0.000000  loss: 4.3117 (4.3671)  class_acc: 0.2500 (0.2377)  loss_scale: 65536.0000 (44402.3589)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1004  max mem: 15572
Epoch: [13]  [1820/2809]  eta: 0:09:16  lr: 0.000040  min_lr: 0.000000  loss: 4.3040 (4.3672)  class_acc: 0.2917 (0.2379)  loss_scale: 65536.0000 (44518.4141)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1294  max mem: 15572
Epoch: [13]  [1830/2809]  eta: 0:09:10  lr: 0.000040  min_lr: 0.000000  loss: 4.3417 (4.3672)  class_acc: 0.2500 (0.2379)  loss_scale: 65536.0000 (44633.2015)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0741  max mem: 15572
[2025-01-15 20:59:12,358] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38351
[2025-01-15 20:59:12,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:59:12,359] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [1840/2809]  eta: 0:09:04  lr: 0.000040  min_lr: 0.000000  loss: 4.3207 (4.3675)  class_acc: 0.2083 (0.2378)  loss_scale: 65536.0000 (44622.1488)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0830  max mem: 15572
Epoch: [13]  [1850/2809]  eta: 0:08:59  lr: 0.000040  min_lr: 0.000000  loss: 4.3207 (4.3669)  class_acc: 0.2083 (0.2378)  loss_scale: 32768.0000 (44558.1070)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1349  max mem: 15572
Epoch: [13]  [1860/2809]  eta: 0:08:53  lr: 0.000040  min_lr: 0.000000  loss: 4.4212 (4.3668)  class_acc: 0.2083 (0.2377)  loss_scale: 32768.0000 (44494.7534)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1042  max mem: 15572
Epoch: [13]  [1870/2809]  eta: 0:08:47  lr: 0.000040  min_lr: 0.000000  loss: 4.4244 (4.3669)  class_acc: 0.2083 (0.2377)  loss_scale: 32768.0000 (44432.0770)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0826  max mem: 15572
Epoch: [13]  [1880/2809]  eta: 0:08:41  lr: 0.000040  min_lr: 0.000000  loss: 4.3958 (4.3671)  class_acc: 0.2500 (0.2376)  loss_scale: 32768.0000 (44370.0670)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0904  max mem: 15572
Epoch: [13]  [1890/2809]  eta: 0:08:36  lr: 0.000040  min_lr: 0.000000  loss: 4.4947 (4.3674)  class_acc: 0.2083 (0.2376)  loss_scale: 32768.0000 (44308.7129)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1166  max mem: 15572
Epoch: [13]  [1900/2809]  eta: 0:08:30  lr: 0.000040  min_lr: 0.000000  loss: 4.4947 (4.3682)  class_acc: 0.2083 (0.2374)  loss_scale: 32768.0000 (44248.0042)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1251  max mem: 15572
Epoch: [13]  [1910/2809]  eta: 0:08:24  lr: 0.000040  min_lr: 0.000000  loss: 4.4362 (4.3684)  class_acc: 0.2083 (0.2373)  loss_scale: 32768.0000 (44187.9309)  weight_decay: 0.0500 (0.0500)  time: 0.5229  data: 0.0738  max mem: 15572
Epoch: [13]  [1920/2809]  eta: 0:08:19  lr: 0.000040  min_lr: 0.000000  loss: 4.4206 (4.3683)  class_acc: 0.2083 (0.2373)  loss_scale: 32768.0000 (44128.4831)  weight_decay: 0.0500 (0.0500)  time: 0.5229  data: 0.0641  max mem: 15572
Epoch: [13]  [1930/2809]  eta: 0:08:13  lr: 0.000040  min_lr: 0.000000  loss: 4.2970 (4.3684)  class_acc: 0.2083 (0.2373)  loss_scale: 32768.0000 (44069.6510)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1391  max mem: 15572
Epoch: [13]  [1940/2809]  eta: 0:08:07  lr: 0.000040  min_lr: 0.000000  loss: 4.2686 (4.3678)  class_acc: 0.2917 (0.2374)  loss_scale: 32768.0000 (44011.4250)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0938  max mem: 15572
Epoch: [13]  [1950/2809]  eta: 0:08:01  lr: 0.000040  min_lr: 0.000000  loss: 4.2686 (4.3676)  class_acc: 0.2500 (0.2375)  loss_scale: 32768.0000 (43953.7960)  weight_decay: 0.0500 (0.0500)  time: 0.4743  data: 0.0322  max mem: 15572
Epoch: [13]  [1960/2809]  eta: 0:07:56  lr: 0.000040  min_lr: 0.000000  loss: 4.2812 (4.3672)  class_acc: 0.2500 (0.2375)  loss_scale: 32768.0000 (43896.7547)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1249  max mem: 15572
[2025-01-15 21:00:23,487] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:00:23,488] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [1970/2809]  eta: 0:07:50  lr: 0.000040  min_lr: 0.000000  loss: 4.2055 (4.3665)  class_acc: 0.2083 (0.2372)  loss_scale: 32768.0000 (43973.2927)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1684  max mem: 15572
[2025-01-15 21:00:31,234] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38495
[2025-01-15 21:00:31,235] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:00:31,235] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [1980/2809]  eta: 0:07:45  lr: 0.000040  min_lr: 0.000000  loss: 4.1624 (4.3660)  class_acc: 0.1667 (0.2374)  loss_scale: 65536.0000 (44032.5169)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1193  max mem: 15572
Epoch: [13]  [1990/2809]  eta: 0:07:39  lr: 0.000040  min_lr: 0.000000  loss: 4.5084 (4.3670)  class_acc: 0.2083 (0.2372)  loss_scale: 32768.0000 (43975.9397)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1430  max mem: 15572
Epoch: [13]  [2000/2809]  eta: 0:07:34  lr: 0.000040  min_lr: 0.000000  loss: 4.5150 (4.3677)  class_acc: 0.1250 (0.2367)  loss_scale: 32768.0000 (43919.9280)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1783  max mem: 15572
Epoch: [13]  [2010/2809]  eta: 0:07:29  lr: 0.000040  min_lr: 0.000000  loss: 4.5082 (4.3675)  class_acc: 0.1250 (0.2366)  loss_scale: 32768.0000 (43864.4734)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1589  max mem: 15572
Epoch: [13]  [2020/2809]  eta: 0:07:23  lr: 0.000040  min_lr: 0.000000  loss: 4.4569 (4.3677)  class_acc: 0.1667 (0.2366)  loss_scale: 32768.0000 (43809.5675)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0803  max mem: 15572
Epoch: [13]  [2030/2809]  eta: 0:07:17  lr: 0.000040  min_lr: 0.000000  loss: 4.4306 (4.3679)  class_acc: 0.1667 (0.2365)  loss_scale: 32768.0000 (43755.2024)  weight_decay: 0.0500 (0.0500)  time: 0.4566  data: 0.0286  max mem: 15572
Epoch: [13]  [2040/2809]  eta: 0:07:11  lr: 0.000040  min_lr: 0.000000  loss: 4.4180 (4.3680)  class_acc: 0.2500 (0.2369)  loss_scale: 32768.0000 (43701.3699)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0984  max mem: 15572
Epoch: [13]  [2050/2809]  eta: 0:07:06  lr: 0.000040  min_lr: 0.000000  loss: 4.4100 (4.3679)  class_acc: 0.3333 (0.2375)  loss_scale: 32768.0000 (43648.0624)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1323  max mem: 15572
Epoch: [13]  [2060/2809]  eta: 0:07:00  lr: 0.000040  min_lr: 0.000000  loss: 4.4899 (4.3685)  class_acc: 0.3333 (0.2375)  loss_scale: 32768.0000 (43595.2722)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.1736  max mem: 15572
Epoch: [13]  [2070/2809]  eta: 0:06:55  lr: 0.000040  min_lr: 0.000000  loss: 4.4104 (4.3681)  class_acc: 0.2083 (0.2377)  loss_scale: 32768.0000 (43542.9918)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1742  max mem: 15572
Epoch: [13]  [2080/2809]  eta: 0:06:49  lr: 0.000040  min_lr: 0.000000  loss: 4.3807 (4.3679)  class_acc: 0.2500 (0.2378)  loss_scale: 32768.0000 (43491.2138)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1267  max mem: 15572
Epoch: [13]  [2090/2809]  eta: 0:06:43  lr: 0.000040  min_lr: 0.000000  loss: 4.4325 (4.3682)  class_acc: 0.2500 (0.2379)  loss_scale: 32768.0000 (43439.9311)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.1176  max mem: 15572
Epoch: [13]  [2100/2809]  eta: 0:06:38  lr: 0.000040  min_lr: 0.000000  loss: 4.4218 (4.3684)  class_acc: 0.2500 (0.2380)  loss_scale: 32768.0000 (43389.1366)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1226  max mem: 15572
[2025-01-15 21:01:45,259] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:01:45,259] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [2110/2809]  eta: 0:06:32  lr: 0.000040  min_lr: 0.000000  loss: 4.4218 (4.3686)  class_acc: 0.2500 (0.2383)  loss_scale: 32768.0000 (43400.9133)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1276  max mem: 15572
[2025-01-15 21:01:49,874] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38630
[2025-01-15 21:01:49,874] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:01:49,874] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [2120/2809]  eta: 0:06:27  lr: 0.000040  min_lr: 0.000000  loss: 4.3726 (4.3685)  class_acc: 0.2500 (0.2384)  loss_scale: 32768.0000 (43381.6803)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1553  max mem: 15572
Epoch: [13]  [2130/2809]  eta: 0:06:21  lr: 0.000040  min_lr: 0.000000  loss: 4.3880 (4.3688)  class_acc: 0.2500 (0.2385)  loss_scale: 32768.0000 (43331.8742)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1624  max mem: 15572
Epoch: [13]  [2140/2809]  eta: 0:06:16  lr: 0.000040  min_lr: 0.000000  loss: 4.3880 (4.3687)  class_acc: 0.2500 (0.2384)  loss_scale: 32768.0000 (43282.5334)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0951  max mem: 15572
Epoch: [13]  [2150/2809]  eta: 0:06:10  lr: 0.000040  min_lr: 0.000000  loss: 4.2876 (4.3684)  class_acc: 0.2500 (0.2388)  loss_scale: 32768.0000 (43233.6513)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1156  max mem: 15572
Epoch: [13]  [2160/2809]  eta: 0:06:04  lr: 0.000040  min_lr: 0.000000  loss: 4.2889 (4.3685)  class_acc: 0.2917 (0.2389)  loss_scale: 32768.0000 (43185.2217)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1137  max mem: 15572
Epoch: [13]  [2170/2809]  eta: 0:05:59  lr: 0.000040  min_lr: 0.000000  loss: 4.4008 (4.3689)  class_acc: 0.2500 (0.2390)  loss_scale: 32768.0000 (43137.2381)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1185  max mem: 15572
Epoch: [13]  [2180/2809]  eta: 0:05:53  lr: 0.000040  min_lr: 0.000000  loss: 4.4156 (4.3692)  class_acc: 0.2917 (0.2393)  loss_scale: 32768.0000 (43089.6946)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0912  max mem: 15572
Epoch: [13]  [2190/2809]  eta: 0:05:47  lr: 0.000040  min_lr: 0.000000  loss: 4.4182 (4.3697)  class_acc: 0.2917 (0.2392)  loss_scale: 32768.0000 (43042.5851)  weight_decay: 0.0500 (0.0500)  time: 0.4681  data: 0.0202  max mem: 15572
Epoch: [13]  [2200/2809]  eta: 0:05:42  lr: 0.000040  min_lr: 0.000000  loss: 4.4060 (4.3695)  class_acc: 0.2500 (0.2396)  loss_scale: 32768.0000 (42995.9037)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1194  max mem: 15572
Epoch: [13]  [2210/2809]  eta: 0:05:36  lr: 0.000040  min_lr: 0.000000  loss: 4.3544 (4.3695)  class_acc: 0.2500 (0.2395)  loss_scale: 32768.0000 (42949.6445)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1409  max mem: 15572
Epoch: [13]  [2220/2809]  eta: 0:05:30  lr: 0.000040  min_lr: 0.000000  loss: 4.3909 (4.3697)  class_acc: 0.1667 (0.2391)  loss_scale: 32768.0000 (42903.8019)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0713  max mem: 15572
Epoch: [13]  [2230/2809]  eta: 0:05:25  lr: 0.000040  min_lr: 0.000000  loss: 4.3909 (4.3694)  class_acc: 0.2083 (0.2393)  loss_scale: 32768.0000 (42858.3702)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0993  max mem: 15572
Epoch: [13]  [2240/2809]  eta: 0:05:19  lr: 0.000040  min_lr: 0.000000  loss: 4.4319 (4.3700)  class_acc: 0.2500 (0.2393)  loss_scale: 32768.0000 (42813.3440)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0991  max mem: 15572
[2025-01-15 21:03:01,715] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:03:01,715] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [2250/2809]  eta: 0:05:14  lr: 0.000040  min_lr: 0.000000  loss: 4.4837 (4.3698)  class_acc: 0.2500 (0.2393)  loss_scale: 32768.0000 (42899.7317)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1120  max mem: 15572
Epoch: [13]  [2260/2809]  eta: 0:05:08  lr: 0.000040  min_lr: 0.000000  loss: 4.3017 (4.3695)  class_acc: 0.2917 (0.2396)  loss_scale: 65536.0000 (42999.8479)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1562  max mem: 15572
Epoch: [13]  [2270/2809]  eta: 0:05:02  lr: 0.000040  min_lr: 0.000000  loss: 4.3082 (4.3693)  class_acc: 0.2917 (0.2396)  loss_scale: 65536.0000 (43099.0823)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1590  max mem: 15572
Epoch: [13]  [2280/2809]  eta: 0:04:57  lr: 0.000040  min_lr: 0.000000  loss: 4.4748 (4.3700)  class_acc: 0.2500 (0.2395)  loss_scale: 65536.0000 (43197.4467)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1400  max mem: 15572
Epoch: [13]  [2290/2809]  eta: 0:04:51  lr: 0.000040  min_lr: 0.000000  loss: 4.4058 (4.3698)  class_acc: 0.2083 (0.2395)  loss_scale: 65536.0000 (43294.9524)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0700  max mem: 15572
Epoch: [13]  [2300/2809]  eta: 0:04:46  lr: 0.000040  min_lr: 0.000000  loss: 4.3141 (4.3694)  class_acc: 0.2083 (0.2397)  loss_scale: 65536.0000 (43391.6106)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.0964  max mem: 15572
Epoch: [13]  [2310/2809]  eta: 0:04:40  lr: 0.000040  min_lr: 0.000000  loss: 4.1552 (4.3681)  class_acc: 0.2917 (0.2397)  loss_scale: 65536.0000 (43487.4323)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0815  max mem: 15572
Epoch: [13]  [2320/2809]  eta: 0:04:34  lr: 0.000040  min_lr: 0.000000  loss: 4.1885 (4.3683)  class_acc: 0.2500 (0.2396)  loss_scale: 65536.0000 (43582.4283)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0456  max mem: 15572
Epoch: [13]  [2330/2809]  eta: 0:04:28  lr: 0.000040  min_lr: 0.000000  loss: 4.3023 (4.3680)  class_acc: 0.2083 (0.2395)  loss_scale: 65536.0000 (43676.6092)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0679  max mem: 15572
Epoch: [13]  [2340/2809]  eta: 0:04:23  lr: 0.000040  min_lr: 0.000000  loss: 4.2056 (4.3675)  class_acc: 0.2083 (0.2396)  loss_scale: 65536.0000 (43769.9855)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1407  max mem: 15572
Epoch: [13]  [2350/2809]  eta: 0:04:17  lr: 0.000040  min_lr: 0.000000  loss: 4.3600 (4.3679)  class_acc: 0.2083 (0.2396)  loss_scale: 65536.0000 (43862.5674)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1763  max mem: 15572
Epoch: [13]  [2360/2809]  eta: 0:04:12  lr: 0.000040  min_lr: 0.000000  loss: 4.4696 (4.3682)  class_acc: 0.2083 (0.2394)  loss_scale: 65536.0000 (43954.3651)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0758  max mem: 15572
[2025-01-15 21:04:11,213] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:04:11,213] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [2370/2809]  eta: 0:04:06  lr: 0.000040  min_lr: 0.000000  loss: 4.4268 (4.3685)  class_acc: 0.1667 (0.2394)  loss_scale: 65536.0000 (44073.0291)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0564  max mem: 15572
[2025-01-15 21:04:13,527] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38888
[2025-01-15 21:04:13,527] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:04:13,527] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2380/2809]  eta: 0:04:00  lr: 0.000040  min_lr: 0.000000  loss: 4.4347 (4.3685)  class_acc: 0.1667 (0.2393)  loss_scale: 65536.0000 (44163.1718)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1281  max mem: 15572
Epoch: [13]  [2390/2809]  eta: 0:03:55  lr: 0.000040  min_lr: 0.000000  loss: 4.4631 (4.3690)  class_acc: 0.2083 (0.2391)  loss_scale: 65536.0000 (44252.5604)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1308  max mem: 15572
Epoch: [13]  [2400/2809]  eta: 0:03:49  lr: 0.000040  min_lr: 0.000000  loss: 4.4015 (4.3691)  class_acc: 0.2500 (0.2392)  loss_scale: 65536.0000 (44341.2045)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0666  max mem: 15572
Epoch: [13]  [2410/2809]  eta: 0:03:43  lr: 0.000040  min_lr: 0.000000  loss: 4.3841 (4.3687)  class_acc: 0.2500 (0.2392)  loss_scale: 65536.0000 (44429.1132)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0881  max mem: 15572
Epoch: [13]  [2420/2809]  eta: 0:03:38  lr: 0.000040  min_lr: 0.000000  loss: 4.4020 (4.3689)  class_acc: 0.2083 (0.2394)  loss_scale: 65536.0000 (44516.2957)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0970  max mem: 15572
Epoch: [13]  [2430/2809]  eta: 0:03:32  lr: 0.000040  min_lr: 0.000000  loss: 4.4173 (4.3687)  class_acc: 0.2083 (0.2393)  loss_scale: 65536.0000 (44602.7610)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.1020  max mem: 15572
[2025-01-15 21:04:46,744] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38949
[2025-01-15 21:04:46,745] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:04:46,745] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [2440/2809]  eta: 0:03:27  lr: 0.000040  min_lr: 0.000000  loss: 4.3665 (4.3688)  class_acc: 0.2083 (0.2391)  loss_scale: 65536.0000 (44567.7018)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1309  max mem: 15572
Epoch: [13]  [2450/2809]  eta: 0:03:21  lr: 0.000040  min_lr: 0.000000  loss: 4.3134 (4.3685)  class_acc: 0.2083 (0.2392)  loss_scale: 32768.0000 (44519.5594)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1352  max mem: 15572
Epoch: [13]  [2460/2809]  eta: 0:03:15  lr: 0.000040  min_lr: 0.000000  loss: 4.2657 (4.3685)  class_acc: 0.2500 (0.2391)  loss_scale: 32768.0000 (44471.8082)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0727  max mem: 15572
Epoch: [13]  [2470/2809]  eta: 0:03:10  lr: 0.000040  min_lr: 0.000000  loss: 4.4167 (4.3685)  class_acc: 0.2500 (0.2392)  loss_scale: 32768.0000 (44424.4435)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0009  max mem: 15572
Epoch: [13]  [2480/2809]  eta: 0:03:04  lr: 0.000040  min_lr: 0.000000  loss: 4.3016 (4.3684)  class_acc: 0.2500 (0.2393)  loss_scale: 32768.0000 (44377.4607)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1092  max mem: 15572
[2025-01-15 21:05:15,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=39000, skipped=247, lr=[3.8578753243206146e-07, 3.8578753243206146e-07, 5.511250463315164e-07, 5.511250463315164e-07, 7.873214947593093e-07, 7.873214947593093e-07, 1.1247449925132992e-06, 1.1247449925132992e-06, 1.6067785607332844e-06, 1.6067785607332844e-06, 2.295397943904692e-06, 2.295397943904692e-06, 3.2791399198638463e-06, 3.2791399198638463e-06, 4.684485599805495e-06, 4.684485599805495e-06, 6.692122285436421e-06, 6.692122285436421e-06, 9.560174693480603e-06, 9.560174693480603e-06, 1.3657392419258004e-05, 1.3657392419258004e-05, 1.951056059894001e-05, 1.951056059894001e-05, 2.7872229427057156e-05, 2.7872229427057156e-05, 3.9817470610081654e-05, 3.9817470610081654e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:05:15,670] [INFO] [timer.py:260:stop] epoch=0/micro_step=39000/global_step=39000, RunningAvgSamplesPerSec=27.583085039179757, CurrSamplesPerSec=24.390640658588733, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [2490/2809]  eta: 0:02:59  lr: 0.000040  min_lr: 0.000000  loss: 4.2968 (4.3685)  class_acc: 0.2500 (0.2394)  loss_scale: 32768.0000 (44330.8551)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1588  max mem: 15572
Epoch: [13]  [2500/2809]  eta: 0:02:53  lr: 0.000040  min_lr: 0.000000  loss: 4.2815 (4.3683)  class_acc: 0.2500 (0.2394)  loss_scale: 32768.0000 (44284.6222)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1038  max mem: 15572
Epoch: [13]  [2510/2809]  eta: 0:02:47  lr: 0.000040  min_lr: 0.000000  loss: 4.4092 (4.3685)  class_acc: 0.2083 (0.2393)  loss_scale: 32768.0000 (44238.7575)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0924  max mem: 15572
Epoch: [13]  [2520/2809]  eta: 0:02:42  lr: 0.000040  min_lr: 0.000000  loss: 4.4819 (4.3687)  class_acc: 0.1667 (0.2391)  loss_scale: 32768.0000 (44193.2566)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0618  max mem: 15572
Epoch: [13]  [2530/2809]  eta: 0:02:36  lr: 0.000040  min_lr: 0.000000  loss: 4.4046 (4.3691)  class_acc: 0.2083 (0.2391)  loss_scale: 32768.0000 (44148.1154)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0627  max mem: 15572
Epoch: [13]  [2540/2809]  eta: 0:02:31  lr: 0.000040  min_lr: 0.000000  loss: 4.4006 (4.3690)  class_acc: 0.2500 (0.2396)  loss_scale: 32768.0000 (44103.3294)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0890  max mem: 15572
Epoch: [13]  [2550/2809]  eta: 0:02:25  lr: 0.000040  min_lr: 0.000000  loss: 4.3304 (4.3688)  class_acc: 0.2917 (0.2394)  loss_scale: 32768.0000 (44058.8946)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0786  max mem: 15572
Epoch: [13]  [2560/2809]  eta: 0:02:19  lr: 0.000040  min_lr: 0.000000  loss: 4.3265 (4.3686)  class_acc: 0.2083 (0.2394)  loss_scale: 32768.0000 (44014.8067)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0298  max mem: 15572
[2025-01-15 21:05:59,120] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:05:59,121] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [2570/2809]  eta: 0:02:14  lr: 0.000040  min_lr: 0.000000  loss: 4.3034 (4.3684)  class_acc: 0.2500 (0.2396)  loss_scale: 32768.0000 (44098.5142)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0128  max mem: 15572
Epoch: [13]  [2580/2809]  eta: 0:02:08  lr: 0.000040  min_lr: 0.000000  loss: 4.3004 (4.3683)  class_acc: 0.2083 (0.2394)  loss_scale: 65536.0000 (44181.5730)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0825  max mem: 15572
Epoch: [13]  [2590/2809]  eta: 0:02:02  lr: 0.000040  min_lr: 0.000000  loss: 4.3916 (4.3688)  class_acc: 0.2083 (0.2393)  loss_scale: 65536.0000 (44263.9907)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1178  max mem: 15572
Epoch: [13]  [2600/2809]  eta: 0:01:57  lr: 0.000040  min_lr: 0.000000  loss: 4.4063 (4.3684)  class_acc: 0.1667 (0.2391)  loss_scale: 65536.0000 (44345.7747)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1296  max mem: 15572
Epoch: [13]  [2610/2809]  eta: 0:01:51  lr: 0.000040  min_lr: 0.000000  loss: 4.3877 (4.3684)  class_acc: 0.1667 (0.2390)  loss_scale: 65536.0000 (44426.9322)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.1855  max mem: 15572
Epoch: [13]  [2620/2809]  eta: 0:01:46  lr: 0.000040  min_lr: 0.000000  loss: 4.4365 (4.3683)  class_acc: 0.2083 (0.2390)  loss_scale: 65536.0000 (44507.4704)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1340  max mem: 15572
Epoch: [13]  [2630/2809]  eta: 0:01:40  lr: 0.000040  min_lr: 0.000000  loss: 4.3086 (4.3681)  class_acc: 0.2083 (0.2390)  loss_scale: 65536.0000 (44587.3964)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0911  max mem: 15572
[2025-01-15 21:06:41,405] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39151
[2025-01-15 21:06:41,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:06:41,405] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [2640/2809]  eta: 0:01:34  lr: 0.000040  min_lr: 0.000000  loss: 4.3086 (4.3678)  class_acc: 0.2500 (0.2392)  loss_scale: 65536.0000 (44579.8652)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0829  max mem: 15572
Epoch: [13]  [2650/2809]  eta: 0:01:29  lr: 0.000040  min_lr: 0.000000  loss: 4.2310 (4.3679)  class_acc: 0.2500 (0.2393)  loss_scale: 32768.0000 (44535.3089)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0380  max mem: 15572
Epoch: [13]  [2660/2809]  eta: 0:01:23  lr: 0.000040  min_lr: 0.000000  loss: 4.3636 (4.3683)  class_acc: 0.2500 (0.2392)  loss_scale: 32768.0000 (44491.0876)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0620  max mem: 15572
Epoch: [13]  [2670/2809]  eta: 0:01:18  lr: 0.000040  min_lr: 0.000000  loss: 4.3452 (4.3677)  class_acc: 0.2083 (0.2392)  loss_scale: 32768.0000 (44447.1973)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1041  max mem: 15572
Epoch: [13]  [2680/2809]  eta: 0:01:12  lr: 0.000040  min_lr: 0.000000  loss: 4.3239 (4.3678)  class_acc: 0.2083 (0.2391)  loss_scale: 32768.0000 (44403.6345)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0786  max mem: 15572
Epoch: [13]  [2690/2809]  eta: 0:01:06  lr: 0.000040  min_lr: 0.000000  loss: 4.3243 (4.3677)  class_acc: 0.2083 (0.2391)  loss_scale: 32768.0000 (44360.3954)  weight_decay: 0.0500 (0.0500)  time: 0.4396  data: 0.0211  max mem: 15572
Epoch: [13]  [2700/2809]  eta: 0:01:01  lr: 0.000040  min_lr: 0.000000  loss: 4.4215 (4.3682)  class_acc: 0.2083 (0.2390)  loss_scale: 32768.0000 (44317.4765)  weight_decay: 0.0500 (0.0500)  time: 0.4362  data: 0.0004  max mem: 15572
Epoch: [13]  [2710/2809]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000000  loss: 4.4215 (4.3681)  class_acc: 0.2500 (0.2392)  loss_scale: 32768.0000 (44274.8742)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0703  max mem: 15572
Epoch: [13]  [2720/2809]  eta: 0:00:49  lr: 0.000040  min_lr: 0.000000  loss: 4.2674 (4.3677)  class_acc: 0.2500 (0.2392)  loss_scale: 32768.0000 (44232.5851)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1627  max mem: 15572
Epoch: [13]  [2730/2809]  eta: 0:00:44  lr: 0.000040  min_lr: 0.000000  loss: 4.2674 (4.3673)  class_acc: 0.2500 (0.2393)  loss_scale: 32768.0000 (44190.6056)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.1912  max mem: 15572
Epoch: [13]  [2740/2809]  eta: 0:00:38  lr: 0.000040  min_lr: 0.000000  loss: 4.3839 (4.3678)  class_acc: 0.2500 (0.2393)  loss_scale: 32768.0000 (44148.9325)  weight_decay: 0.0500 (0.0500)  time: 0.6786  data: 0.2006  max mem: 15572
Epoch: [13]  [2750/2809]  eta: 0:00:33  lr: 0.000040  min_lr: 0.000000  loss: 4.4638 (4.3677)  class_acc: 0.2083 (0.2393)  loss_scale: 32768.0000 (44107.5623)  weight_decay: 0.0500 (0.0500)  time: 0.6775  data: 0.1900  max mem: 15572
Epoch: [13]  [2760/2809]  eta: 0:00:27  lr: 0.000040  min_lr: 0.000000  loss: 4.4292 (4.3679)  class_acc: 0.2083 (0.2392)  loss_scale: 32768.0000 (44066.4919)  weight_decay: 0.0500 (0.0500)  time: 0.7096  data: 0.2353  max mem: 15572
[2025-01-15 21:07:55,586] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:07:55,586] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [2770/2809]  eta: 0:00:21  lr: 0.000040  min_lr: 0.000000  loss: 4.3639 (4.3677)  class_acc: 0.2083 (0.2392)  loss_scale: 32768.0000 (44120.3205)  weight_decay: 0.0500 (0.0500)  time: 0.7055  data: 0.2513  max mem: 15572
[2025-01-15 21:08:02,294] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39290
[2025-01-15 21:08:02,294] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:08:02,295] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [2780/2809]  eta: 0:00:16  lr: 0.000040  min_lr: 0.000000  loss: 4.1694 (4.3666)  class_acc: 0.2500 (0.2395)  loss_scale: 32768.0000 (44103.0651)  weight_decay: 0.0500 (0.0500)  time: 0.6565  data: 0.1880  max mem: 15572
Epoch: [13]  [2790/2809]  eta: 0:00:10  lr: 0.000040  min_lr: 0.000000  loss: 4.2322 (4.3665)  class_acc: 0.2500 (0.2394)  loss_scale: 32768.0000 (44062.4522)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1430  max mem: 15572
Epoch: [13]  [2800/2809]  eta: 0:00:05  lr: 0.000040  min_lr: 0.000000  loss: 4.3893 (4.3663)  class_acc: 0.2083 (0.2395)  loss_scale: 32768.0000 (44022.1292)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1031  max mem: 15572
Epoch: [13]  [2808/2809]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000000  loss: 4.3251 (4.3659)  class_acc: 0.2500 (0.2395)  loss_scale: 32768.0000 (43990.0776)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.1030  max mem: 15572
Epoch: [13] Total time: 0:26:22 (0.5633 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000000  loss: 4.3251 (4.3659)  class_acc: 0.2500 (0.2395)  loss_scale: 32768.0000 (43990.0776)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:22:30  loss: 1.1338 (1.1338)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 4.9655  data: 4.7726  max mem: 15572
Val:  [ 10/272]  eta: 0:03:45  loss: 3.0248 (2.9195)  acc1: 33.3333 (32.8283)  acc5: 61.1111 (63.1313)  time: 0.8604  data: 0.6754  max mem: 15572
Val:  [ 20/272]  eta: 0:02:17  loss: 3.0248 (2.9488)  acc1: 33.3333 (34.9206)  acc5: 61.1111 (66.1376)  time: 0.3241  data: 0.1428  max mem: 15572
Val:  [ 30/272]  eta: 0:01:42  loss: 3.0426 (3.0161)  acc1: 27.7778 (31.3620)  acc5: 61.1111 (64.8746)  time: 0.1853  data: 0.0102  max mem: 15572
Val:  [ 40/272]  eta: 0:01:24  loss: 2.9268 (2.9975)  acc1: 27.7778 (30.7588)  acc5: 66.6667 (66.2602)  time: 0.1708  data: 0.0005  max mem: 15572
Val:  [ 50/272]  eta: 0:01:12  loss: 2.9257 (2.9417)  acc1: 33.3333 (33.2244)  acc5: 66.6667 (68.3007)  time: 0.1717  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:01:04  loss: 2.1968 (2.8658)  acc1: 50.0000 (36.6120)  acc5: 77.7778 (69.4900)  time: 0.1848  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:58  loss: 2.3022 (2.8031)  acc1: 50.0000 (38.7324)  acc5: 83.3333 (71.2050)  time: 0.2061  data: 0.0132  max mem: 15572
Val:  [ 80/272]  eta: 0:00:56  loss: 2.5508 (2.8077)  acc1: 44.4444 (38.8889)  acc5: 77.7778 (70.9191)  time: 0.2555  data: 0.0658  max mem: 15572
Val:  [ 90/272]  eta: 0:00:54  loss: 3.1012 (2.8496)  acc1: 33.3333 (38.6447)  acc5: 61.1111 (69.7802)  time: 0.3309  data: 0.1284  max mem: 15572
Val:  [100/272]  eta: 0:00:51  loss: 3.1012 (2.8885)  acc1: 33.3333 (37.7888)  acc5: 66.6667 (69.1969)  time: 0.3237  data: 0.0984  max mem: 15572
Val:  [110/272]  eta: 0:00:48  loss: 3.1859 (2.9379)  acc1: 22.2222 (35.7858)  acc5: 55.5556 (67.7177)  time: 0.2897  data: 0.0861  max mem: 15572
Val:  [120/272]  eta: 0:00:45  loss: 3.1976 (2.9744)  acc1: 16.6667 (34.9403)  acc5: 55.5556 (66.9421)  time: 0.3105  data: 0.1211  max mem: 15572
Val:  [130/272]  eta: 0:00:43  loss: 3.0284 (2.9472)  acc1: 38.8889 (36.2171)  acc5: 61.1111 (67.3876)  time: 0.3335  data: 0.1311  max mem: 15572
Val:  [140/272]  eta: 0:00:39  loss: 2.3539 (2.9421)  acc1: 38.8889 (36.8006)  acc5: 72.2222 (67.4547)  time: 0.3072  data: 0.1122  max mem: 15572
Val:  [150/272]  eta: 0:00:37  loss: 2.9618 (2.9469)  acc1: 22.2222 (35.9088)  acc5: 66.6667 (67.2185)  time: 0.3092  data: 0.1136  max mem: 15572
Val:  [160/272]  eta: 0:00:34  loss: 2.9367 (2.9399)  acc1: 33.3333 (36.5424)  acc5: 72.2222 (67.7364)  time: 0.3234  data: 0.1207  max mem: 15572
Val:  [170/272]  eta: 0:00:30  loss: 2.8210 (2.9547)  acc1: 38.8889 (36.0624)  acc5: 72.2222 (67.2840)  time: 0.2949  data: 0.0932  max mem: 15572
Val:  [180/272]  eta: 0:00:28  loss: 2.7438 (2.9444)  acc1: 27.7778 (35.8502)  acc5: 66.6667 (67.5875)  time: 0.3214  data: 0.1278  max mem: 15572
Val:  [190/272]  eta: 0:00:24  loss: 2.8944 (2.9758)  acc1: 22.2222 (34.6422)  acc5: 61.1111 (66.2885)  time: 0.3157  data: 0.1327  max mem: 15572
Val:  [200/272]  eta: 0:00:21  loss: 3.1037 (2.9819)  acc1: 16.6667 (34.4113)  acc5: 55.5556 (66.1968)  time: 0.2933  data: 0.1102  max mem: 15572
Val:  [210/272]  eta: 0:00:18  loss: 2.7616 (2.9822)  acc1: 38.8889 (35.0974)  acc5: 77.7778 (66.5087)  time: 0.2887  data: 0.0992  max mem: 15572
Val:  [220/272]  eta: 0:00:15  loss: 2.8378 (2.9771)  acc1: 44.4444 (35.1433)  acc5: 66.6667 (66.6164)  time: 0.3240  data: 0.1275  max mem: 15572
Val:  [230/272]  eta: 0:00:12  loss: 2.5180 (2.9549)  acc1: 44.4444 (36.4117)  acc5: 72.2222 (67.2198)  time: 0.3392  data: 0.1425  max mem: 15572
Val:  [240/272]  eta: 0:00:09  loss: 2.4144 (2.9412)  acc1: 50.0000 (36.5145)  acc5: 83.3333 (67.7732)  time: 0.2860  data: 0.0990  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.6596 (2.9534)  acc1: 27.7778 (36.0115)  acc5: 72.2222 (67.6184)  time: 0.2688  data: 0.0830  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.5878 (2.9129)  acc1: 61.1111 (37.6756)  acc5: 77.7778 (68.4334)  time: 0.2839  data: 0.0986  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.2607 (2.9089)  acc1: 61.1111 (37.6589)  acc5: 83.3333 (68.3887)  time: 0.2492  data: 0.0831  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.2607 (2.9113)  acc1: 61.1111 (37.6613)  acc5: 83.3333 (68.4006)  time: 0.2428  data: 0.0831  max mem: 15572
Val: Total time: 0:01:21 (0.2996 s / it)
* Acc@1 37.661 Acc@5 68.401 loss 2.911
Accuracy of the network on the 4883 val videos: 37.7%
[2025-01-15 21:09:43,576] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 21:09:43,579] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 21:09:43,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 21:09:46,351] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 21:09:46,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 37.66%
Epoch: [14]  [   0/2809]  eta: 5:35:08  lr: 0.000040  min_lr: 0.000000  loss: 4.3685 (4.3685)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.1587  data: 6.7246  max mem: 15572
Epoch: [14]  [  10/2809]  eta: 0:56:06  lr: 0.000040  min_lr: 0.000000  loss: 4.4537 (4.3912)  class_acc: 0.1667 (0.1591)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2027  data: 0.7750  max mem: 15572
Epoch: [14]  [  20/2809]  eta: 0:43:29  lr: 0.000040  min_lr: 0.000000  loss: 4.3060 (4.2678)  class_acc: 0.1667 (0.2143)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1933  max mem: 15572
Epoch: [14]  [  30/2809]  eta: 0:38:40  lr: 0.000040  min_lr: 0.000000  loss: 4.3466 (4.3240)  class_acc: 0.2500 (0.2204)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1899  max mem: 15572
Epoch: [14]  [  40/2809]  eta: 0:35:20  lr: 0.000040  min_lr: 0.000000  loss: 4.4417 (4.3734)  class_acc: 0.2500 (0.2327)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1408  max mem: 15572
Epoch: [14]  [  50/2809]  eta: 0:33:32  lr: 0.000040  min_lr: 0.000000  loss: 4.3948 (4.3284)  class_acc: 0.2917 (0.2525)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1217  max mem: 15572
Epoch: [14]  [  60/2809]  eta: 0:32:03  lr: 0.000040  min_lr: 0.000000  loss: 4.3365 (4.3238)  class_acc: 0.2917 (0.2561)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1205  max mem: 15572
Epoch: [14]  [  70/2809]  eta: 0:30:44  lr: 0.000040  min_lr: 0.000000  loss: 4.4128 (4.3340)  class_acc: 0.2500 (0.2565)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0831  max mem: 15572
Epoch: [14]  [  80/2809]  eta: 0:29:51  lr: 0.000040  min_lr: 0.000000  loss: 4.4971 (4.3565)  class_acc: 0.2083 (0.2438)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.0739  max mem: 15572
Epoch: [14]  [  90/2809]  eta: 0:29:31  lr: 0.000040  min_lr: 0.000000  loss: 4.5059 (4.3634)  class_acc: 0.1667 (0.2454)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1211  max mem: 15572
[2025-01-15 21:10:47,017] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:10:47,017] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 100/2809]  eta: 0:29:03  lr: 0.000040  min_lr: 0.000000  loss: 4.4240 (4.3696)  class_acc: 0.2500 (0.2459)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1381  max mem: 15572
Epoch: [14]  [ 110/2809]  eta: 0:28:44  lr: 0.000040  min_lr: 0.000000  loss: 4.4312 (4.3717)  class_acc: 0.2083 (0.2444)  loss_scale: 65536.0000 (38081.7297)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1340  max mem: 15572
Epoch: [14]  [ 120/2809]  eta: 0:28:27  lr: 0.000040  min_lr: 0.000000  loss: 4.4312 (4.3764)  class_acc: 0.2083 (0.2459)  loss_scale: 65536.0000 (40350.6777)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1552  max mem: 15572
Epoch: [14]  [ 130/2809]  eta: 0:28:02  lr: 0.000040  min_lr: 0.000000  loss: 4.4541 (4.3792)  class_acc: 0.2083 (0.2443)  loss_scale: 65536.0000 (42273.2214)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1280  max mem: 15572
Epoch: [14]  [ 140/2809]  eta: 0:27:39  lr: 0.000040  min_lr: 0.000000  loss: 4.3548 (4.3826)  class_acc: 0.2083 (0.2405)  loss_scale: 65536.0000 (43923.0638)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0828  max mem: 15572
Epoch: [14]  [ 150/2809]  eta: 0:27:34  lr: 0.000040  min_lr: 0.000000  loss: 4.3103 (4.3819)  class_acc: 0.2083 (0.2379)  loss_scale: 65536.0000 (45354.3841)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1245  max mem: 15572
Epoch: [14]  [ 160/2809]  eta: 0:27:36  lr: 0.000040  min_lr: 0.000000  loss: 4.3074 (4.3801)  class_acc: 0.2083 (0.2384)  loss_scale: 65536.0000 (46607.9006)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1965  max mem: 15572
Epoch: [14]  [ 170/2809]  eta: 0:27:04  lr: 0.000040  min_lr: 0.000000  loss: 4.3074 (4.3770)  class_acc: 0.2500 (0.2405)  loss_scale: 65536.0000 (47714.8070)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1090  max mem: 15572
[2025-01-15 21:11:34,847] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39503
[2025-01-15 21:11:34,848] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:11:34,848] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [ 180/2809]  eta: 0:26:35  lr: 0.000040  min_lr: 0.000000  loss: 4.2960 (4.3754)  class_acc: 0.2083 (0.2396)  loss_scale: 65536.0000 (47975.2486)  weight_decay: 0.0500 (0.0500)  time: 0.4594  data: 0.0052  max mem: 15572
Epoch: [14]  [ 190/2809]  eta: 0:26:20  lr: 0.000040  min_lr: 0.000000  loss: 4.4399 (4.3842)  class_acc: 0.2083 (0.2391)  loss_scale: 32768.0000 (47179.0576)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0535  max mem: 15572
Epoch: [14]  [ 200/2809]  eta: 0:26:18  lr: 0.000040  min_lr: 0.000000  loss: 4.4399 (4.3830)  class_acc: 0.2083 (0.2365)  loss_scale: 32768.0000 (46462.0896)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1486  max mem: 15572
Epoch: [14]  [ 210/2809]  eta: 0:26:02  lr: 0.000040  min_lr: 0.000000  loss: 4.3386 (4.3811)  class_acc: 0.2083 (0.2356)  loss_scale: 32768.0000 (45813.0806)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1344  max mem: 15572
Epoch: [14]  [ 220/2809]  eta: 0:25:48  lr: 0.000040  min_lr: 0.000000  loss: 4.2330 (4.3731)  class_acc: 0.2500 (0.2370)  loss_scale: 32768.0000 (45222.8054)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0842  max mem: 15572
Epoch: [14]  [ 230/2809]  eta: 0:25:41  lr: 0.000040  min_lr: 0.000000  loss: 4.2583 (4.3694)  class_acc: 0.2500 (0.2379)  loss_scale: 32768.0000 (44683.6364)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1157  max mem: 15572
Epoch: [14]  [ 240/2809]  eta: 0:25:36  lr: 0.000040  min_lr: 0.000000  loss: 4.3588 (4.3706)  class_acc: 0.2500 (0.2391)  loss_scale: 32768.0000 (44189.2116)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1563  max mem: 15572
Epoch: [14]  [ 250/2809]  eta: 0:25:26  lr: 0.000040  min_lr: 0.000000  loss: 4.4231 (4.3726)  class_acc: 0.2500 (0.2397)  loss_scale: 32768.0000 (43734.1833)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1498  max mem: 15572
Epoch: [14]  [ 260/2809]  eta: 0:25:15  lr: 0.000040  min_lr: 0.000000  loss: 4.4339 (4.3768)  class_acc: 0.2083 (0.2380)  loss_scale: 32768.0000 (43314.0230)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1145  max mem: 15572
Epoch: [14]  [ 270/2809]  eta: 0:25:12  lr: 0.000039  min_lr: 0.000000  loss: 4.4339 (4.3802)  class_acc: 0.2083 (0.2392)  loss_scale: 32768.0000 (42924.8708)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1430  max mem: 15572
Epoch: [14]  [ 280/2809]  eta: 0:25:10  lr: 0.000039  min_lr: 0.000000  loss: 4.4085 (4.3823)  class_acc: 0.2083 (0.2387)  loss_scale: 32768.0000 (42563.4164)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1636  max mem: 15572
Epoch: [14]  [ 290/2809]  eta: 0:24:56  lr: 0.000039  min_lr: 0.000000  loss: 4.4004 (4.3837)  class_acc: 0.2083 (0.2384)  loss_scale: 32768.0000 (42226.8041)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0975  max mem: 15572
Epoch: [14]  [ 300/2809]  eta: 0:24:46  lr: 0.000039  min_lr: 0.000000  loss: 4.3842 (4.3821)  class_acc: 0.2500 (0.2389)  loss_scale: 32768.0000 (41912.5581)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0722  max mem: 15572
[2025-01-15 21:12:47,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:12:47,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 310/2809]  eta: 0:24:36  lr: 0.000039  min_lr: 0.000000  loss: 4.4083 (4.3852)  class_acc: 0.2500 (0.2375)  loss_scale: 32768.0000 (42145.3376)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0951  max mem: 15572
Epoch: [14]  [ 320/2809]  eta: 0:24:27  lr: 0.000039  min_lr: 0.000000  loss: 4.4849 (4.3884)  class_acc: 0.2083 (0.2377)  loss_scale: 65536.0000 (42874.0187)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1047  max mem: 15572
Epoch: [14]  [ 330/2809]  eta: 0:24:14  lr: 0.000039  min_lr: 0.000000  loss: 4.4550 (4.3885)  class_acc: 0.2083 (0.2374)  loss_scale: 65536.0000 (43558.6707)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0882  max mem: 15572
Epoch: [14]  [ 340/2809]  eta: 0:24:08  lr: 0.000039  min_lr: 0.000000  loss: 4.3083 (4.3857)  class_acc: 0.1667 (0.2359)  loss_scale: 65536.0000 (44203.1672)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0858  max mem: 15572
Epoch: [14]  [ 350/2809]  eta: 0:23:57  lr: 0.000039  min_lr: 0.000000  loss: 4.4722 (4.3879)  class_acc: 0.1667 (0.2344)  loss_scale: 65536.0000 (44810.9402)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0812  max mem: 15572
Epoch: [14]  [ 360/2809]  eta: 0:23:51  lr: 0.000039  min_lr: 0.000000  loss: 4.4262 (4.3892)  class_acc: 0.2500 (0.2360)  loss_scale: 65536.0000 (45385.0416)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.1003  max mem: 15572
Epoch: [14]  [ 370/2809]  eta: 0:23:43  lr: 0.000039  min_lr: 0.000000  loss: 4.3911 (4.3889)  class_acc: 0.2083 (0.2352)  loss_scale: 65536.0000 (45928.1941)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1483  max mem: 15572
Epoch: [14]  [ 380/2809]  eta: 0:23:31  lr: 0.000039  min_lr: 0.000000  loss: 4.3554 (4.3831)  class_acc: 0.2083 (0.2356)  loss_scale: 65536.0000 (46442.8346)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0908  max mem: 15572
Epoch: [14]  [ 390/2809]  eta: 0:23:22  lr: 0.000039  min_lr: 0.000000  loss: 4.2946 (4.3829)  class_acc: 0.2500 (0.2358)  loss_scale: 65536.0000 (46931.1509)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0651  max mem: 15572
Epoch: [14]  [ 400/2809]  eta: 0:23:16  lr: 0.000039  min_lr: 0.000000  loss: 4.3234 (4.3822)  class_acc: 0.2500 (0.2369)  loss_scale: 65536.0000 (47395.1122)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1180  max mem: 15572
Epoch: [14]  [ 410/2809]  eta: 0:23:12  lr: 0.000039  min_lr: 0.000000  loss: 4.2692 (4.3781)  class_acc: 0.2500 (0.2376)  loss_scale: 65536.0000 (47836.4964)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1523  max mem: 15572
Epoch: [14]  [ 420/2809]  eta: 0:23:09  lr: 0.000039  min_lr: 0.000000  loss: 4.2955 (4.3791)  class_acc: 0.2500 (0.2380)  loss_scale: 65536.0000 (48256.9121)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1795  max mem: 15572
Epoch: [14]  [ 430/2809]  eta: 0:23:07  lr: 0.000039  min_lr: 0.000000  loss: 4.3576 (4.3785)  class_acc: 0.2500 (0.2381)  loss_scale: 65536.0000 (48657.8190)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.2006  max mem: 15572
[2025-01-15 21:13:59,851] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39760
[2025-01-15 21:13:59,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:13:59,852] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [ 440/2809]  eta: 0:22:59  lr: 0.000039  min_lr: 0.000000  loss: 4.3389 (4.3781)  class_acc: 0.2083 (0.2381)  loss_scale: 65536.0000 (48520.4172)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1429  max mem: 15572
Epoch: [14]  [ 450/2809]  eta: 0:22:55  lr: 0.000039  min_lr: 0.000000  loss: 4.2849 (4.3765)  class_acc: 0.2500 (0.2390)  loss_scale: 32768.0000 (48171.1397)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1220  max mem: 15572
Epoch: [14]  [ 460/2809]  eta: 0:22:48  lr: 0.000039  min_lr: 0.000000  loss: 4.3031 (4.3754)  class_acc: 0.2500 (0.2402)  loss_scale: 32768.0000 (47837.0152)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1265  max mem: 15572
Epoch: [14]  [ 470/2809]  eta: 0:22:39  lr: 0.000039  min_lr: 0.000000  loss: 4.3307 (4.3753)  class_acc: 0.2500 (0.2404)  loss_scale: 32768.0000 (47517.0786)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0775  max mem: 15572
Epoch: [14]  [ 480/2809]  eta: 0:22:35  lr: 0.000039  min_lr: 0.000000  loss: 4.3050 (4.3735)  class_acc: 0.2500 (0.2408)  loss_scale: 32768.0000 (47210.4449)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1262  max mem: 15572
Epoch: [14]  [ 490/2809]  eta: 0:22:26  lr: 0.000039  min_lr: 0.000000  loss: 4.3679 (4.3759)  class_acc: 0.1667 (0.2396)  loss_scale: 32768.0000 (46916.3014)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1317  max mem: 15572
Epoch: [14]  [ 500/2809]  eta: 0:22:24  lr: 0.000039  min_lr: 0.000000  loss: 4.3867 (4.3733)  class_acc: 0.2083 (0.2404)  loss_scale: 32768.0000 (46633.9002)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1312  max mem: 15572
Epoch: [14]  [ 510/2809]  eta: 0:22:12  lr: 0.000039  min_lr: 0.000000  loss: 4.3664 (4.3745)  class_acc: 0.2083 (0.2396)  loss_scale: 32768.0000 (46362.5519)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0948  max mem: 15572
Epoch: [14]  [ 520/2809]  eta: 0:22:03  lr: 0.000039  min_lr: 0.000000  loss: 4.3664 (4.3735)  class_acc: 0.2500 (0.2400)  loss_scale: 32768.0000 (46101.6200)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0342  max mem: 15572
Epoch: [14]  [ 530/2809]  eta: 0:22:00  lr: 0.000039  min_lr: 0.000000  loss: 4.4032 (4.3755)  class_acc: 0.2500 (0.2407)  loss_scale: 32768.0000 (45850.5160)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1251  max mem: 15572
Epoch: [14]  [ 540/2809]  eta: 0:21:57  lr: 0.000039  min_lr: 0.000000  loss: 4.4578 (4.3761)  class_acc: 0.2083 (0.2399)  loss_scale: 32768.0000 (45608.6950)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.1850  max mem: 15572
Epoch: [14]  [ 550/2809]  eta: 0:21:49  lr: 0.000039  min_lr: 0.000000  loss: 4.3265 (4.3769)  class_acc: 0.2500 (0.2405)  loss_scale: 32768.0000 (45375.6515)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1235  max mem: 15572
Epoch: [14]  [ 560/2809]  eta: 0:21:41  lr: 0.000039  min_lr: 0.000000  loss: 4.3150 (4.3734)  class_acc: 0.2917 (0.2420)  loss_scale: 32768.0000 (45150.9162)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0694  max mem: 15572
[2025-01-15 21:15:12,627] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:15:12,627] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 570/2809]  eta: 0:21:38  lr: 0.000039  min_lr: 0.000000  loss: 4.2411 (4.3700)  class_acc: 0.2917 (0.2425)  loss_scale: 32768.0000 (45393.1489)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1278  max mem: 15572
Epoch: [14]  [ 580/2809]  eta: 0:21:31  lr: 0.000039  min_lr: 0.000000  loss: 4.3116 (4.3696)  class_acc: 0.2500 (0.2424)  loss_scale: 65536.0000 (45739.8417)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1422  max mem: 15572
Epoch: [14]  [ 590/2809]  eta: 0:21:27  lr: 0.000039  min_lr: 0.000000  loss: 4.3481 (4.3686)  class_acc: 0.2083 (0.2420)  loss_scale: 65536.0000 (46074.8020)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1273  max mem: 15572
Epoch: [14]  [ 600/2809]  eta: 0:21:18  lr: 0.000039  min_lr: 0.000000  loss: 4.3223 (4.3683)  class_acc: 0.1667 (0.2406)  loss_scale: 65536.0000 (46398.6156)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0977  max mem: 15572
[2025-01-15 21:15:39,649] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39935
[2025-01-15 21:15:39,650] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:15:39,650] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [ 610/2809]  eta: 0:21:12  lr: 0.000039  min_lr: 0.000000  loss: 4.3223 (4.3672)  class_acc: 0.1667 (0.2402)  loss_scale: 65536.0000 (46604.5696)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0824  max mem: 15572
Epoch: [14]  [ 620/2809]  eta: 0:21:06  lr: 0.000039  min_lr: 0.000000  loss: 4.3547 (4.3687)  class_acc: 0.2083 (0.2396)  loss_scale: 32768.0000 (46381.7585)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1151  max mem: 15572
Epoch: [14]  [ 630/2809]  eta: 0:21:00  lr: 0.000039  min_lr: 0.000000  loss: 4.3306 (4.3692)  class_acc: 0.2083 (0.2402)  loss_scale: 32768.0000 (46166.0095)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1183  max mem: 15572
Epoch: [14]  [ 640/2809]  eta: 0:20:53  lr: 0.000039  min_lr: 0.000000  loss: 4.3886 (4.3692)  class_acc: 0.2500 (0.2411)  loss_scale: 32768.0000 (45956.9922)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1104  max mem: 15572
Epoch: [14]  [ 650/2809]  eta: 0:20:50  lr: 0.000039  min_lr: 0.000000  loss: 4.3695 (4.3693)  class_acc: 0.2500 (0.2409)  loss_scale: 32768.0000 (45754.3963)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1532  max mem: 15572
Epoch: [14]  [ 660/2809]  eta: 0:20:42  lr: 0.000039  min_lr: 0.000000  loss: 4.4366 (4.3706)  class_acc: 0.2083 (0.2403)  loss_scale: 32768.0000 (45557.9304)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1266  max mem: 15572
Epoch: [14]  [ 670/2809]  eta: 0:20:37  lr: 0.000039  min_lr: 0.000000  loss: 4.4366 (4.3705)  class_acc: 0.2083 (0.2407)  loss_scale: 32768.0000 (45367.3204)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1123  max mem: 15572
[2025-01-15 21:16:16,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=40000, skipped=252, lr=[3.805205638311788e-07, 3.805205638311788e-07, 5.436008054731127e-07, 5.436008054731127e-07, 7.765725792473039e-07, 7.765725792473039e-07, 1.10938939892472e-06, 1.10938939892472e-06, 1.5848419984638858e-06, 1.5848419984638858e-06, 2.2640599978055513e-06, 2.2640599978055513e-06, 3.234371425436502e-06, 3.234371425436502e-06, 4.620530607766432e-06, 4.620530607766432e-06, 6.600758011094902e-06, 6.600758011094902e-06, 9.429654301564147e-06, 9.429654301564147e-06, 1.347093471652021e-05, 1.347093471652021e-05, 1.924419245217173e-05, 1.924419245217173e-05, 2.7491703503102474e-05, 2.7491703503102474e-05, 3.927386214728925e-05, 3.927386214728925e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:16:16,247] [INFO] [timer.py:260:stop] epoch=0/micro_step=40000/global_step=40000, RunningAvgSamplesPerSec=27.590931500306596, CurrSamplesPerSec=31.27932021753735, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [ 680/2809]  eta: 0:20:29  lr: 0.000039  min_lr: 0.000000  loss: 4.4333 (4.3706)  class_acc: 0.2083 (0.2405)  loss_scale: 32768.0000 (45182.3084)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1198  max mem: 15572
Epoch: [14]  [ 690/2809]  eta: 0:20:24  lr: 0.000039  min_lr: 0.000000  loss: 4.3521 (4.3692)  class_acc: 0.2500 (0.2411)  loss_scale: 32768.0000 (45002.6512)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1041  max mem: 15572
Epoch: [14]  [ 700/2809]  eta: 0:20:19  lr: 0.000039  min_lr: 0.000000  loss: 4.1879 (4.3664)  class_acc: 0.2500 (0.2413)  loss_scale: 32768.0000 (44828.1198)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1487  max mem: 15572
Epoch: [14]  [ 710/2809]  eta: 0:20:14  lr: 0.000039  min_lr: 0.000000  loss: 4.1879 (4.3642)  class_acc: 0.2083 (0.2417)  loss_scale: 32768.0000 (44658.4979)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1596  max mem: 15572
Epoch: [14]  [ 720/2809]  eta: 0:20:08  lr: 0.000039  min_lr: 0.000000  loss: 4.3394 (4.3635)  class_acc: 0.1667 (0.2410)  loss_scale: 32768.0000 (44493.5811)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1497  max mem: 15572
Epoch: [14]  [ 730/2809]  eta: 0:20:00  lr: 0.000039  min_lr: 0.000000  loss: 4.3315 (4.3631)  class_acc: 0.2083 (0.2414)  loss_scale: 32768.0000 (44333.1765)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0893  max mem: 15572
[2025-01-15 21:16:53,943] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:16:53,944] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 740/2809]  eta: 0:19:55  lr: 0.000039  min_lr: 0.000000  loss: 4.3790 (4.3639)  class_acc: 0.2500 (0.2421)  loss_scale: 32768.0000 (44309.7652)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1115  max mem: 15572
Epoch: [14]  [ 750/2809]  eta: 0:19:49  lr: 0.000039  min_lr: 0.000000  loss: 4.4695 (4.3646)  class_acc: 0.2500 (0.2420)  loss_scale: 65536.0000 (44592.4048)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1223  max mem: 15572
[2025-01-15 21:17:04,827] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40086
[2025-01-15 21:17:04,827] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:17:04,828] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [ 760/2809]  eta: 0:19:39  lr: 0.000039  min_lr: 0.000000  loss: 4.4074 (4.3642)  class_acc: 0.2500 (0.2422)  loss_scale: 65536.0000 (44824.5572)  weight_decay: 0.0500 (0.0500)  time: 0.4953  data: 0.0389  max mem: 15572
Epoch: [14]  [ 770/2809]  eta: 0:19:34  lr: 0.000039  min_lr: 0.000000  loss: 4.2896 (4.3642)  class_acc: 0.2500 (0.2424)  loss_scale: 32768.0000 (44668.1816)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0838  max mem: 15572
Epoch: [14]  [ 780/2809]  eta: 0:19:27  lr: 0.000039  min_lr: 0.000000  loss: 4.2608 (4.3642)  class_acc: 0.2500 (0.2428)  loss_scale: 32768.0000 (44515.8105)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1233  max mem: 15572
Epoch: [14]  [ 790/2809]  eta: 0:19:21  lr: 0.000039  min_lr: 0.000000  loss: 4.4359 (4.3651)  class_acc: 0.2500 (0.2428)  loss_scale: 32768.0000 (44367.2920)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.1053  max mem: 15572
Epoch: [14]  [ 800/2809]  eta: 0:19:16  lr: 0.000039  min_lr: 0.000000  loss: 4.4359 (4.3644)  class_acc: 0.2083 (0.2428)  loss_scale: 32768.0000 (44222.4819)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1431  max mem: 15572
Epoch: [14]  [ 810/2809]  eta: 0:19:09  lr: 0.000039  min_lr: 0.000000  loss: 4.4425 (4.3648)  class_acc: 0.2083 (0.2430)  loss_scale: 32768.0000 (44081.2429)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1270  max mem: 15572
Epoch: [14]  [ 820/2809]  eta: 0:19:04  lr: 0.000039  min_lr: 0.000000  loss: 4.4425 (4.3654)  class_acc: 0.2083 (0.2424)  loss_scale: 32768.0000 (43943.4446)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1290  max mem: 15572
Epoch: [14]  [ 830/2809]  eta: 0:18:57  lr: 0.000039  min_lr: 0.000000  loss: 4.3936 (4.3652)  class_acc: 0.2500 (0.2431)  loss_scale: 32768.0000 (43808.9627)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1123  max mem: 15572
Epoch: [14]  [ 840/2809]  eta: 0:18:50  lr: 0.000039  min_lr: 0.000000  loss: 4.2980 (4.3656)  class_acc: 0.2917 (0.2429)  loss_scale: 32768.0000 (43677.6790)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0430  max mem: 15572
Epoch: [14]  [ 850/2809]  eta: 0:18:44  lr: 0.000039  min_lr: 0.000000  loss: 4.2926 (4.3659)  class_acc: 0.2500 (0.2429)  loss_scale: 32768.0000 (43549.4806)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0877  max mem: 15572
Epoch: [14]  [ 860/2809]  eta: 0:18:38  lr: 0.000039  min_lr: 0.000000  loss: 4.3534 (4.3662)  class_acc: 0.2083 (0.2425)  loss_scale: 32768.0000 (43424.2602)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1388  max mem: 15572
Epoch: [14]  [ 870/2809]  eta: 0:18:35  lr: 0.000039  min_lr: 0.000000  loss: 4.2770 (4.3640)  class_acc: 0.2083 (0.2431)  loss_scale: 32768.0000 (43301.9150)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.1647  max mem: 15572
Epoch: [14]  [ 880/2809]  eta: 0:18:28  lr: 0.000039  min_lr: 0.000000  loss: 4.2421 (4.3649)  class_acc: 0.2917 (0.2436)  loss_scale: 32768.0000 (43182.3473)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1589  max mem: 15572
[2025-01-15 21:18:19,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:18:19,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 890/2809]  eta: 0:18:24  lr: 0.000039  min_lr: 0.000000  loss: 4.3809 (4.3643)  class_acc: 0.2500 (0.2433)  loss_scale: 32768.0000 (43139.0168)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1797  max mem: 15572
Epoch: [14]  [ 900/2809]  eta: 0:18:16  lr: 0.000039  min_lr: 0.000000  loss: 4.3809 (4.3646)  class_acc: 0.2500 (0.2444)  loss_scale: 65536.0000 (43387.5960)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1211  max mem: 15572
Epoch: [14]  [ 910/2809]  eta: 0:18:09  lr: 0.000039  min_lr: 0.000000  loss: 4.3780 (4.3642)  class_acc: 0.2917 (0.2445)  loss_scale: 65536.0000 (43630.7179)  weight_decay: 0.0500 (0.0500)  time: 0.4696  data: 0.0264  max mem: 15572
Epoch: [14]  [ 920/2809]  eta: 0:18:02  lr: 0.000039  min_lr: 0.000000  loss: 4.3143 (4.3628)  class_acc: 0.2500 (0.2447)  loss_scale: 65536.0000 (43868.5603)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0816  max mem: 15572
Epoch: [14]  [ 930/2809]  eta: 0:17:56  lr: 0.000039  min_lr: 0.000000  loss: 4.3340 (4.3633)  class_acc: 0.2083 (0.2446)  loss_scale: 65536.0000 (44101.2932)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.1134  max mem: 15572
Epoch: [14]  [ 940/2809]  eta: 0:17:52  lr: 0.000039  min_lr: 0.000000  loss: 4.4274 (4.3637)  class_acc: 0.1667 (0.2439)  loss_scale: 65536.0000 (44329.0797)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1659  max mem: 15572
Epoch: [14]  [ 950/2809]  eta: 0:17:46  lr: 0.000039  min_lr: 0.000000  loss: 4.4274 (4.3643)  class_acc: 0.2083 (0.2437)  loss_scale: 65536.0000 (44552.0757)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1418  max mem: 15572
Epoch: [14]  [ 960/2809]  eta: 0:17:42  lr: 0.000039  min_lr: 0.000000  loss: 4.4253 (4.3646)  class_acc: 0.2083 (0.2434)  loss_scale: 65536.0000 (44770.4308)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1413  max mem: 15572
Epoch: [14]  [ 970/2809]  eta: 0:17:35  lr: 0.000039  min_lr: 0.000000  loss: 4.3824 (4.3648)  class_acc: 0.2083 (0.2432)  loss_scale: 65536.0000 (44984.2884)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1445  max mem: 15572
Epoch: [14]  [ 980/2809]  eta: 0:17:29  lr: 0.000039  min_lr: 0.000000  loss: 4.3750 (4.3644)  class_acc: 0.2083 (0.2430)  loss_scale: 65536.0000 (45193.7859)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1257  max mem: 15572
Epoch: [14]  [ 990/2809]  eta: 0:17:23  lr: 0.000039  min_lr: 0.000000  loss: 4.3750 (4.3648)  class_acc: 0.1667 (0.2427)  loss_scale: 65536.0000 (45399.0555)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1594  max mem: 15572
Epoch: [14]  [1000/2809]  eta: 0:17:17  lr: 0.000039  min_lr: 0.000000  loss: 4.3757 (4.3633)  class_acc: 0.2500 (0.2433)  loss_scale: 65536.0000 (45600.2238)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1308  max mem: 15572
Epoch: [14]  [1010/2809]  eta: 0:17:11  lr: 0.000039  min_lr: 0.000000  loss: 4.4000 (4.3636)  class_acc: 0.2500 (0.2431)  loss_scale: 65536.0000 (45797.4125)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1164  max mem: 15572
[2025-01-15 21:19:28,169] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40340
[2025-01-15 21:19:28,169] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:19:28,169] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1020/2809]  eta: 0:17:05  lr: 0.000039  min_lr: 0.000000  loss: 4.2484 (4.3624)  class_acc: 0.2500 (0.2437)  loss_scale: 65536.0000 (45766.0803)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1235  max mem: 15572
Epoch: [14]  [1030/2809]  eta: 0:16:59  lr: 0.000039  min_lr: 0.000000  loss: 4.2484 (4.3617)  class_acc: 0.2500 (0.2438)  loss_scale: 32768.0000 (45640.0078)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1281  max mem: 15572
Epoch: [14]  [1040/2809]  eta: 0:16:53  lr: 0.000039  min_lr: 0.000000  loss: 4.4142 (4.3621)  class_acc: 0.2500 (0.2438)  loss_scale: 32768.0000 (45516.3573)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1278  max mem: 15572
Epoch: [14]  [1050/2809]  eta: 0:16:49  lr: 0.000039  min_lr: 0.000000  loss: 4.4181 (4.3625)  class_acc: 0.2083 (0.2433)  loss_scale: 32768.0000 (45395.0599)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1508  max mem: 15572
Epoch: [14]  [1060/2809]  eta: 0:16:42  lr: 0.000039  min_lr: 0.000000  loss: 4.4027 (4.3625)  class_acc: 0.1667 (0.2426)  loss_scale: 32768.0000 (45276.0490)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1074  max mem: 15572
Epoch: [14]  [1070/2809]  eta: 0:16:37  lr: 0.000039  min_lr: 0.000000  loss: 4.3981 (4.3616)  class_acc: 0.1667 (0.2425)  loss_scale: 32768.0000 (45159.2605)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1126  max mem: 15572
Epoch: [14]  [1080/2809]  eta: 0:16:30  lr: 0.000039  min_lr: 0.000000  loss: 4.3278 (4.3608)  class_acc: 0.2500 (0.2426)  loss_scale: 32768.0000 (45044.6327)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1167  max mem: 15572
Epoch: [14]  [1090/2809]  eta: 0:16:25  lr: 0.000039  min_lr: 0.000000  loss: 4.3395 (4.3604)  class_acc: 0.2917 (0.2432)  loss_scale: 32768.0000 (44932.1063)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1075  max mem: 15572
Epoch: [14]  [1100/2809]  eta: 0:16:18  lr: 0.000039  min_lr: 0.000000  loss: 4.3898 (4.3611)  class_acc: 0.2917 (0.2433)  loss_scale: 32768.0000 (44821.6240)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1283  max mem: 15572
Epoch: [14]  [1110/2809]  eta: 0:16:14  lr: 0.000039  min_lr: 0.000000  loss: 4.3898 (4.3614)  class_acc: 0.2500 (0.2435)  loss_scale: 32768.0000 (44713.1305)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1611  max mem: 15572
Epoch: [14]  [1120/2809]  eta: 0:16:07  lr: 0.000039  min_lr: 0.000000  loss: 4.3767 (4.3608)  class_acc: 0.2500 (0.2434)  loss_scale: 32768.0000 (44606.5727)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1614  max mem: 15572
Epoch: [14]  [1130/2809]  eta: 0:16:01  lr: 0.000039  min_lr: 0.000000  loss: 4.3103 (4.3612)  class_acc: 0.2083 (0.2428)  loss_scale: 32768.0000 (44501.8992)  weight_decay: 0.0500 (0.0500)  time: 0.5208  data: 0.0905  max mem: 15572
Epoch: [14]  [1140/2809]  eta: 0:15:55  lr: 0.000039  min_lr: 0.000000  loss: 4.4684 (4.3625)  class_acc: 0.2083 (0.2429)  loss_scale: 32768.0000 (44399.0605)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0948  max mem: 15572
[2025-01-15 21:20:41,280] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:20:41,281] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [1150/2809]  eta: 0:15:48  lr: 0.000039  min_lr: 0.000000  loss: 4.4528 (4.3618)  class_acc: 0.2500 (0.2432)  loss_scale: 32768.0000 (44525.7619)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0818  max mem: 15572
Epoch: [14]  [1160/2809]  eta: 0:15:44  lr: 0.000039  min_lr: 0.000000  loss: 4.3236 (4.3625)  class_acc: 0.2083 (0.2425)  loss_scale: 65536.0000 (44706.7287)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1273  max mem: 15572
Epoch: [14]  [1170/2809]  eta: 0:15:38  lr: 0.000039  min_lr: 0.000000  loss: 4.4729 (4.3632)  class_acc: 0.2083 (0.2427)  loss_scale: 65536.0000 (44884.6046)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1493  max mem: 15572
Epoch: [14]  [1180/2809]  eta: 0:15:32  lr: 0.000039  min_lr: 0.000000  loss: 4.3233 (4.3624)  class_acc: 0.2500 (0.2427)  loss_scale: 65536.0000 (45059.4682)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1039  max mem: 15572
Epoch: [14]  [1190/2809]  eta: 0:15:26  lr: 0.000039  min_lr: 0.000000  loss: 4.3233 (4.3630)  class_acc: 0.2500 (0.2427)  loss_scale: 65536.0000 (45231.3955)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1063  max mem: 15572
Epoch: [14]  [1200/2809]  eta: 0:15:20  lr: 0.000039  min_lr: 0.000000  loss: 4.4998 (4.3637)  class_acc: 0.2083 (0.2425)  loss_scale: 65536.0000 (45400.4596)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1131  max mem: 15572
Epoch: [14]  [1210/2809]  eta: 0:15:15  lr: 0.000039  min_lr: 0.000000  loss: 4.4025 (4.3632)  class_acc: 0.2500 (0.2428)  loss_scale: 65536.0000 (45566.7316)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1466  max mem: 15572
Epoch: [14]  [1220/2809]  eta: 0:15:10  lr: 0.000039  min_lr: 0.000000  loss: 4.2604 (4.3618)  class_acc: 0.2500 (0.2427)  loss_scale: 65536.0000 (45730.2801)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1381  max mem: 15572
Epoch: [14]  [1230/2809]  eta: 0:15:04  lr: 0.000039  min_lr: 0.000000  loss: 4.2274 (4.3619)  class_acc: 0.2500 (0.2427)  loss_scale: 65536.0000 (45891.1714)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1078  max mem: 15572
Epoch: [14]  [1240/2809]  eta: 0:14:58  lr: 0.000039  min_lr: 0.000000  loss: 4.3337 (4.3624)  class_acc: 0.2083 (0.2423)  loss_scale: 65536.0000 (46049.4698)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1219  max mem: 15572
Epoch: [14]  [1250/2809]  eta: 0:14:51  lr: 0.000039  min_lr: 0.000000  loss: 4.3449 (4.3631)  class_acc: 0.2083 (0.2424)  loss_scale: 65536.0000 (46205.2374)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0978  max mem: 15572
Epoch: [14]  [1260/2809]  eta: 0:14:45  lr: 0.000039  min_lr: 0.000000  loss: 4.3493 (4.3630)  class_acc: 0.1667 (0.2417)  loss_scale: 65536.0000 (46358.5345)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0788  max mem: 15572
Epoch: [14]  [1270/2809]  eta: 0:14:40  lr: 0.000039  min_lr: 0.000000  loss: 4.4861 (4.3641)  class_acc: 0.1667 (0.2417)  loss_scale: 65536.0000 (46509.4194)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1412  max mem: 15572
[2025-01-15 21:21:54,666] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40597
[2025-01-15 21:21:54,666] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:21:54,666] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1280/2809]  eta: 0:14:34  lr: 0.000039  min_lr: 0.000000  loss: 4.4893 (4.3636)  class_acc: 0.2083 (0.2422)  loss_scale: 32768.0000 (46402.1483)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1244  max mem: 15572
Epoch: [14]  [1290/2809]  eta: 0:14:29  lr: 0.000039  min_lr: 0.000000  loss: 4.3416 (4.3635)  class_acc: 0.2083 (0.2417)  loss_scale: 32768.0000 (46296.5391)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1493  max mem: 15572
Epoch: [14]  [1300/2809]  eta: 0:14:23  lr: 0.000039  min_lr: 0.000000  loss: 4.4219 (4.3643)  class_acc: 0.1667 (0.2415)  loss_scale: 32768.0000 (46192.5534)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1514  max mem: 15572
Epoch: [14]  [1310/2809]  eta: 0:14:17  lr: 0.000039  min_lr: 0.000000  loss: 4.3775 (4.3642)  class_acc: 0.2500 (0.2417)  loss_scale: 32768.0000 (46090.1541)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0913  max mem: 15572
Epoch: [14]  [1320/2809]  eta: 0:14:12  lr: 0.000039  min_lr: 0.000000  loss: 4.3354 (4.3638)  class_acc: 0.2500 (0.2420)  loss_scale: 32768.0000 (45989.3051)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1319  max mem: 15572
Epoch: [14]  [1330/2809]  eta: 0:14:06  lr: 0.000039  min_lr: 0.000000  loss: 4.3178 (4.3640)  class_acc: 0.1667 (0.2415)  loss_scale: 32768.0000 (45889.9715)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1499  max mem: 15572
Epoch: [14]  [1340/2809]  eta: 0:13:59  lr: 0.000039  min_lr: 0.000000  loss: 4.4481 (4.3644)  class_acc: 0.2083 (0.2416)  loss_scale: 32768.0000 (45792.1193)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0881  max mem: 15572
Epoch: [14]  [1350/2809]  eta: 0:13:54  lr: 0.000039  min_lr: 0.000000  loss: 4.4166 (4.3647)  class_acc: 0.2917 (0.2417)  loss_scale: 32768.0000 (45695.7158)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1113  max mem: 15572
Epoch: [14]  [1360/2809]  eta: 0:13:48  lr: 0.000039  min_lr: 0.000000  loss: 4.2682 (4.3639)  class_acc: 0.2917 (0.2422)  loss_scale: 32768.0000 (45600.7289)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1431  max mem: 15572
Epoch: [14]  [1370/2809]  eta: 0:13:44  lr: 0.000039  min_lr: 0.000000  loss: 4.2476 (4.3631)  class_acc: 0.3333 (0.2426)  loss_scale: 32768.0000 (45507.1276)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1813  max mem: 15572
Epoch: [14]  [1380/2809]  eta: 0:13:37  lr: 0.000039  min_lr: 0.000000  loss: 4.1662 (4.3617)  class_acc: 0.2500 (0.2428)  loss_scale: 32768.0000 (45414.8820)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1389  max mem: 15572
Epoch: [14]  [1390/2809]  eta: 0:13:32  lr: 0.000039  min_lr: 0.000000  loss: 4.2066 (4.3618)  class_acc: 0.2500 (0.2429)  loss_scale: 32768.0000 (45323.9626)  weight_decay: 0.0500 (0.0500)  time: 0.5143  data: 0.0599  max mem: 15572
[2025-01-15 21:23:08,028] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:23:08,028] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [1400/2809]  eta: 0:13:25  lr: 0.000039  min_lr: 0.000000  loss: 4.4256 (4.3622)  class_acc: 0.2500 (0.2427)  loss_scale: 32768.0000 (45257.7302)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.1018  max mem: 15572
[2025-01-15 21:23:11,871] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40733
[2025-01-15 21:23:11,871] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:23:11,871] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1410/2809]  eta: 0:13:19  lr: 0.000039  min_lr: 0.000000  loss: 4.4256 (4.3618)  class_acc: 0.2500 (0.2431)  loss_scale: 32768.0000 (45308.5528)  weight_decay: 0.0500 (0.0500)  time: 0.5119  data: 0.0762  max mem: 15572
Epoch: [14]  [1420/2809]  eta: 0:13:13  lr: 0.000039  min_lr: 0.000000  loss: 4.3331 (4.3619)  class_acc: 0.2500 (0.2431)  loss_scale: 32768.0000 (45220.3012)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0869  max mem: 15572
Epoch: [14]  [1430/2809]  eta: 0:13:07  lr: 0.000039  min_lr: 0.000000  loss: 4.2963 (4.3610)  class_acc: 0.2500 (0.2431)  loss_scale: 32768.0000 (45133.2830)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0961  max mem: 15572
Epoch: [14]  [1440/2809]  eta: 0:13:01  lr: 0.000039  min_lr: 0.000000  loss: 4.2640 (4.3605)  class_acc: 0.2500 (0.2432)  loss_scale: 32768.0000 (45047.4726)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0872  max mem: 15572
Epoch: [14]  [1450/2809]  eta: 0:12:55  lr: 0.000039  min_lr: 0.000000  loss: 4.2559 (4.3596)  class_acc: 0.2500 (0.2436)  loss_scale: 32768.0000 (44962.8449)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0827  max mem: 15572
Epoch: [14]  [1460/2809]  eta: 0:12:50  lr: 0.000039  min_lr: 0.000000  loss: 4.2559 (4.3594)  class_acc: 0.2917 (0.2439)  loss_scale: 32768.0000 (44879.3758)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1329  max mem: 15572
Epoch: [14]  [1470/2809]  eta: 0:12:43  lr: 0.000039  min_lr: 0.000000  loss: 4.3597 (4.3595)  class_acc: 0.2500 (0.2438)  loss_scale: 32768.0000 (44797.0415)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1091  max mem: 15572
Epoch: [14]  [1480/2809]  eta: 0:12:37  lr: 0.000039  min_lr: 0.000000  loss: 4.3394 (4.3604)  class_acc: 0.2083 (0.2436)  loss_scale: 32768.0000 (44715.8190)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0500  max mem: 15572
Epoch: [14]  [1490/2809]  eta: 0:12:32  lr: 0.000039  min_lr: 0.000000  loss: 4.3648 (4.3599)  class_acc: 0.2500 (0.2437)  loss_scale: 32768.0000 (44635.6861)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0966  max mem: 15572
Epoch: [14]  [1500/2809]  eta: 0:12:26  lr: 0.000039  min_lr: 0.000000  loss: 4.4302 (4.3600)  class_acc: 0.2500 (0.2437)  loss_scale: 32768.0000 (44556.6209)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1300  max mem: 15572
Epoch: [14]  [1510/2809]  eta: 0:12:20  lr: 0.000039  min_lr: 0.000000  loss: 4.4605 (4.3605)  class_acc: 0.2083 (0.2436)  loss_scale: 32768.0000 (44478.6023)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1525  max mem: 15572
Epoch: [14]  [1520/2809]  eta: 0:12:15  lr: 0.000039  min_lr: 0.000000  loss: 4.3881 (4.3604)  class_acc: 0.2500 (0.2440)  loss_scale: 32768.0000 (44401.6095)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1344  max mem: 15572
Epoch: [14]  [1530/2809]  eta: 0:12:09  lr: 0.000039  min_lr: 0.000000  loss: 4.3714 (4.3607)  class_acc: 0.2083 (0.2438)  loss_scale: 32768.0000 (44325.6225)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0888  max mem: 15572
[2025-01-15 21:24:22,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:24:22,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [1540/2809]  eta: 0:12:02  lr: 0.000039  min_lr: 0.000000  loss: 4.3874 (4.3609)  class_acc: 0.2083 (0.2436)  loss_scale: 32768.0000 (44356.9422)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0678  max mem: 15572
Epoch: [14]  [1550/2809]  eta: 0:11:57  lr: 0.000039  min_lr: 0.000000  loss: 4.4101 (4.3606)  class_acc: 0.2500 (0.2440)  loss_scale: 65536.0000 (44493.4932)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0879  max mem: 15572
[2025-01-15 21:24:33,399] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40880
[2025-01-15 21:24:33,399] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:24:33,399] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1560/2809]  eta: 0:11:51  lr: 0.000039  min_lr: 0.000000  loss: 4.1934 (4.3593)  class_acc: 0.2917 (0.2443)  loss_scale: 65536.0000 (44481.3530)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1270  max mem: 15572
Epoch: [14]  [1570/2809]  eta: 0:11:46  lr: 0.000039  min_lr: 0.000000  loss: 4.1227 (4.3584)  class_acc: 0.2500 (0.2443)  loss_scale: 32768.0000 (44406.7931)  weight_decay: 0.0500 (0.0500)  time: 0.6469  data: 0.1983  max mem: 15572
Epoch: [14]  [1580/2809]  eta: 0:11:40  lr: 0.000039  min_lr: 0.000000  loss: 4.4332 (4.3594)  class_acc: 0.2500 (0.2445)  loss_scale: 32768.0000 (44333.1765)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1302  max mem: 15572
Epoch: [14]  [1590/2809]  eta: 0:11:34  lr: 0.000039  min_lr: 0.000000  loss: 4.4321 (4.3601)  class_acc: 0.2083 (0.2443)  loss_scale: 32768.0000 (44260.4852)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0519  max mem: 15572
Epoch: [14]  [1600/2809]  eta: 0:11:28  lr: 0.000039  min_lr: 0.000000  loss: 4.3764 (4.3600)  class_acc: 0.1667 (0.2445)  loss_scale: 32768.0000 (44188.7021)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0587  max mem: 15572
Epoch: [14]  [1610/2809]  eta: 0:11:22  lr: 0.000039  min_lr: 0.000000  loss: 4.2155 (4.3591)  class_acc: 0.2500 (0.2446)  loss_scale: 32768.0000 (44117.8101)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0674  max mem: 15572
Epoch: [14]  [1620/2809]  eta: 0:11:16  lr: 0.000039  min_lr: 0.000000  loss: 4.1773 (4.3585)  class_acc: 0.2500 (0.2446)  loss_scale: 32768.0000 (44047.7927)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1353  max mem: 15572
Epoch: [14]  [1630/2809]  eta: 0:11:11  lr: 0.000039  min_lr: 0.000000  loss: 4.1773 (4.3579)  class_acc: 0.2500 (0.2446)  loss_scale: 32768.0000 (43978.6340)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1122  max mem: 15572
Epoch: [14]  [1640/2809]  eta: 0:11:05  lr: 0.000039  min_lr: 0.000000  loss: 4.2943 (4.3582)  class_acc: 0.2500 (0.2446)  loss_scale: 32768.0000 (43910.3181)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0934  max mem: 15572
Epoch: [14]  [1650/2809]  eta: 0:11:00  lr: 0.000039  min_lr: 0.000000  loss: 4.3360 (4.3579)  class_acc: 0.2083 (0.2445)  loss_scale: 32768.0000 (43842.8298)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1313  max mem: 15572
Epoch: [14]  [1660/2809]  eta: 0:10:54  lr: 0.000039  min_lr: 0.000000  loss: 4.2826 (4.3578)  class_acc: 0.2500 (0.2448)  loss_scale: 32768.0000 (43776.1541)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1702  max mem: 15572
Epoch: [14]  [1670/2809]  eta: 0:10:48  lr: 0.000039  min_lr: 0.000000  loss: 4.3419 (4.3578)  class_acc: 0.2500 (0.2448)  loss_scale: 32768.0000 (43710.2765)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1269  max mem: 15572
[2025-01-15 21:25:40,149] [INFO] [logging.py:96:log_dist] [Rank 0] step=41000, skipped=257, lr=[3.750971688681664e-07, 3.750971688681664e-07, 5.358530983830948e-07, 5.358530983830948e-07, 7.655044262615642e-07, 7.655044262615642e-07, 1.0935777518022345e-06, 1.0935777518022345e-06, 1.5622539311460496e-06, 1.5622539311460496e-06, 2.231791330208642e-06, 2.231791330208642e-06, 3.188273328869489e-06, 3.188273328869489e-06, 4.55467618409927e-06, 4.55467618409927e-06, 6.506680262998958e-06, 6.506680262998958e-06, 9.295257518569941e-06, 9.295257518569941e-06, 1.3278939312242773e-05, 1.3278939312242773e-05, 1.8969913303203963e-05, 1.8969913303203963e-05, 2.7099876147434235e-05, 2.7099876147434235e-05, 3.871410878204891e-05, 3.871410878204891e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:25:40,150] [INFO] [timer.py:260:stop] epoch=0/micro_step=41000/global_step=41000, RunningAvgSamplesPerSec=27.610683006580647, CurrSamplesPerSec=30.36482935198882, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [1680/2809]  eta: 0:10:42  lr: 0.000039  min_lr: 0.000000  loss: 4.3077 (4.3573)  class_acc: 0.2500 (0.2448)  loss_scale: 32768.0000 (43645.1826)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0893  max mem: 15572
[2025-01-15 21:25:45,387] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:25:45,388] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [1690/2809]  eta: 0:10:36  lr: 0.000039  min_lr: 0.000000  loss: 4.3077 (4.3574)  class_acc: 0.2083 (0.2449)  loss_scale: 32768.0000 (43735.8817)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1016  max mem: 15572
Epoch: [14]  [1700/2809]  eta: 0:10:31  lr: 0.000039  min_lr: 0.000000  loss: 4.3974 (4.3580)  class_acc: 0.2083 (0.2449)  loss_scale: 65536.0000 (43864.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1116  max mem: 15572
[2025-01-15 21:26:01,277] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41036
[2025-01-15 21:26:01,278] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:26:01,278] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1710/2809]  eta: 0:10:25  lr: 0.000039  min_lr: 0.000000  loss: 4.3974 (4.3576)  class_acc: 0.2083 (0.2448)  loss_scale: 65536.0000 (43971.5535)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1421  max mem: 15572
Epoch: [14]  [1720/2809]  eta: 0:10:20  lr: 0.000039  min_lr: 0.000000  loss: 4.3810 (4.3578)  class_acc: 0.2083 (0.2448)  loss_scale: 32768.0000 (43906.4544)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1561  max mem: 15572
Epoch: [14]  [1730/2809]  eta: 0:10:14  lr: 0.000039  min_lr: 0.000000  loss: 4.4427 (4.3574)  class_acc: 0.2500 (0.2449)  loss_scale: 32768.0000 (43842.1075)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1268  max mem: 15572
Epoch: [14]  [1740/2809]  eta: 0:10:08  lr: 0.000039  min_lr: 0.000000  loss: 4.3096 (4.3575)  class_acc: 0.2083 (0.2447)  loss_scale: 32768.0000 (43778.4997)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1319  max mem: 15572
Epoch: [14]  [1750/2809]  eta: 0:10:03  lr: 0.000039  min_lr: 0.000000  loss: 4.2811 (4.3576)  class_acc: 0.1667 (0.2444)  loss_scale: 32768.0000 (43715.6185)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1480  max mem: 15572
Epoch: [14]  [1760/2809]  eta: 0:09:57  lr: 0.000039  min_lr: 0.000000  loss: 4.2792 (4.3572)  class_acc: 0.1667 (0.2442)  loss_scale: 32768.0000 (43653.4514)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1340  max mem: 15572
Epoch: [14]  [1770/2809]  eta: 0:09:51  lr: 0.000039  min_lr: 0.000000  loss: 4.3556 (4.3583)  class_acc: 0.2083 (0.2441)  loss_scale: 32768.0000 (43591.9864)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1577  max mem: 15572
Epoch: [14]  [1780/2809]  eta: 0:09:46  lr: 0.000039  min_lr: 0.000000  loss: 4.4284 (4.3586)  class_acc: 0.2083 (0.2441)  loss_scale: 32768.0000 (43531.2117)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.2032  max mem: 15572
Epoch: [14]  [1790/2809]  eta: 0:09:40  lr: 0.000039  min_lr: 0.000000  loss: 4.3914 (4.3591)  class_acc: 0.2083 (0.2439)  loss_scale: 32768.0000 (43471.1156)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1577  max mem: 15572
Epoch: [14]  [1800/2809]  eta: 0:09:35  lr: 0.000039  min_lr: 0.000000  loss: 4.3977 (4.3594)  class_acc: 0.2083 (0.2439)  loss_scale: 32768.0000 (43411.6868)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1362  max mem: 15572
Epoch: [14]  [1810/2809]  eta: 0:09:29  lr: 0.000039  min_lr: 0.000000  loss: 4.4356 (4.3602)  class_acc: 0.1667 (0.2435)  loss_scale: 32768.0000 (43352.9144)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1397  max mem: 15572
Epoch: [14]  [1820/2809]  eta: 0:09:24  lr: 0.000039  min_lr: 0.000000  loss: 4.3505 (4.3598)  class_acc: 0.2083 (0.2437)  loss_scale: 32768.0000 (43294.7875)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1270  max mem: 15572
Epoch: [14]  [1830/2809]  eta: 0:09:18  lr: 0.000039  min_lr: 0.000000  loss: 4.2640 (4.3591)  class_acc: 0.2500 (0.2438)  loss_scale: 32768.0000 (43237.2955)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1294  max mem: 15572
[2025-01-15 21:27:16,316] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:27:16,316] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [1840/2809]  eta: 0:09:12  lr: 0.000039  min_lr: 0.000000  loss: 4.2041 (4.3584)  class_acc: 0.2500 (0.2440)  loss_scale: 32768.0000 (43216.0261)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1107  max mem: 15572
Epoch: [14]  [1850/2809]  eta: 0:09:07  lr: 0.000039  min_lr: 0.000000  loss: 4.3677 (4.3590)  class_acc: 0.2083 (0.2440)  loss_scale: 65536.0000 (43336.6094)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1445  max mem: 15572
Epoch: [14]  [1860/2809]  eta: 0:09:01  lr: 0.000039  min_lr: 0.000000  loss: 4.3124 (4.3589)  class_acc: 0.2500 (0.2440)  loss_scale: 65536.0000 (43455.8968)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1224  max mem: 15572
Epoch: [14]  [1870/2809]  eta: 0:08:55  lr: 0.000039  min_lr: 0.000000  loss: 4.3182 (4.3591)  class_acc: 0.1667 (0.2437)  loss_scale: 65536.0000 (43573.9091)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1308  max mem: 15572
Epoch: [14]  [1880/2809]  eta: 0:08:50  lr: 0.000039  min_lr: 0.000000  loss: 4.3352 (4.3586)  class_acc: 0.2083 (0.2439)  loss_scale: 65536.0000 (43690.6667)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1484  max mem: 15572
Epoch: [14]  [1890/2809]  eta: 0:08:44  lr: 0.000039  min_lr: 0.000000  loss: 4.3610 (4.3591)  class_acc: 0.2917 (0.2440)  loss_scale: 65536.0000 (43806.1893)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1504  max mem: 15572
Epoch: [14]  [1900/2809]  eta: 0:08:38  lr: 0.000039  min_lr: 0.000000  loss: 4.4462 (4.3599)  class_acc: 0.2917 (0.2443)  loss_scale: 65536.0000 (43920.4966)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1545  max mem: 15572
Epoch: [14]  [1910/2809]  eta: 0:08:32  lr: 0.000039  min_lr: 0.000000  loss: 4.3652 (4.3590)  class_acc: 0.2917 (0.2446)  loss_scale: 65536.0000 (44033.6075)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1223  max mem: 15572
Epoch: [14]  [1920/2809]  eta: 0:08:27  lr: 0.000039  min_lr: 0.000000  loss: 4.3176 (4.3591)  class_acc: 0.2500 (0.2446)  loss_scale: 65536.0000 (44145.5409)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1272  max mem: 15572
Epoch: [14]  [1930/2809]  eta: 0:08:21  lr: 0.000039  min_lr: 0.000000  loss: 4.3176 (4.3586)  class_acc: 0.2083 (0.2447)  loss_scale: 65536.0000 (44256.3149)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1206  max mem: 15572
[2025-01-15 21:28:09,404] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41258
[2025-01-15 21:28:09,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:28:09,405] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1940/2809]  eta: 0:08:15  lr: 0.000039  min_lr: 0.000000  loss: 4.3057 (4.3587)  class_acc: 0.2083 (0.2446)  loss_scale: 65536.0000 (44214.0093)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1051  max mem: 15572
Epoch: [14]  [1950/2809]  eta: 0:08:10  lr: 0.000039  min_lr: 0.000000  loss: 4.4209 (4.3589)  class_acc: 0.1667 (0.2443)  loss_scale: 32768.0000 (44155.3419)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1228  max mem: 15572
Epoch: [14]  [1960/2809]  eta: 0:08:03  lr: 0.000039  min_lr: 0.000000  loss: 4.4389 (4.3594)  class_acc: 0.2083 (0.2446)  loss_scale: 32768.0000 (44097.2728)  weight_decay: 0.0500 (0.0500)  time: 0.5225  data: 0.0761  max mem: 15572
Epoch: [14]  [1970/2809]  eta: 0:07:58  lr: 0.000039  min_lr: 0.000000  loss: 4.4828 (4.3596)  class_acc: 0.2500 (0.2446)  loss_scale: 32768.0000 (44039.7930)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1133  max mem: 15572
Epoch: [14]  [1980/2809]  eta: 0:07:52  lr: 0.000039  min_lr: 0.000000  loss: 4.3216 (4.3591)  class_acc: 0.2500 (0.2448)  loss_scale: 32768.0000 (43982.8935)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1422  max mem: 15572
Epoch: [14]  [1990/2809]  eta: 0:07:47  lr: 0.000039  min_lr: 0.000000  loss: 4.2212 (4.3585)  class_acc: 0.2917 (0.2449)  loss_scale: 32768.0000 (43926.5655)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1104  max mem: 15572
Epoch: [14]  [2000/2809]  eta: 0:07:41  lr: 0.000039  min_lr: 0.000000  loss: 4.3118 (4.3580)  class_acc: 0.2500 (0.2449)  loss_scale: 32768.0000 (43870.8006)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1391  max mem: 15572
Epoch: [14]  [2010/2809]  eta: 0:07:35  lr: 0.000039  min_lr: 0.000000  loss: 4.3118 (4.3574)  class_acc: 0.2500 (0.2449)  loss_scale: 32768.0000 (43815.5903)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1113  max mem: 15572
Epoch: [14]  [2020/2809]  eta: 0:07:30  lr: 0.000039  min_lr: 0.000000  loss: 4.1251 (4.3572)  class_acc: 0.2500 (0.2453)  loss_scale: 32768.0000 (43760.9263)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1036  max mem: 15572
Epoch: [14]  [2030/2809]  eta: 0:07:23  lr: 0.000039  min_lr: 0.000000  loss: 4.2824 (4.3566)  class_acc: 0.2917 (0.2453)  loss_scale: 32768.0000 (43706.8006)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0737  max mem: 15572
Epoch: [14]  [2040/2809]  eta: 0:07:18  lr: 0.000039  min_lr: 0.000000  loss: 4.3915 (4.3571)  class_acc: 0.2083 (0.2452)  loss_scale: 32768.0000 (43653.2053)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1348  max mem: 15572
Epoch: [14]  [2050/2809]  eta: 0:07:12  lr: 0.000038  min_lr: 0.000000  loss: 4.4711 (4.3577)  class_acc: 0.1667 (0.2450)  loss_scale: 32768.0000 (43600.1326)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1486  max mem: 15572
Epoch: [14]  [2060/2809]  eta: 0:07:07  lr: 0.000038  min_lr: 0.000000  loss: 4.4209 (4.3571)  class_acc: 0.2083 (0.2451)  loss_scale: 32768.0000 (43547.5750)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1245  max mem: 15572
[2025-01-15 21:29:23,058] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:29:23,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [2070/2809]  eta: 0:07:01  lr: 0.000038  min_lr: 0.000000  loss: 4.2654 (4.3565)  class_acc: 0.2917 (0.2457)  loss_scale: 32768.0000 (43653.7479)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1778  max mem: 15572
[2025-01-15 21:29:30,297] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41400
[2025-01-15 21:29:30,297] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:29:30,298] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [2080/2809]  eta: 0:06:55  lr: 0.000038  min_lr: 0.000000  loss: 4.2577 (4.3564)  class_acc: 0.3333 (0.2460)  loss_scale: 65536.0000 (43648.6766)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0837  max mem: 15572
Epoch: [14]  [2090/2809]  eta: 0:06:49  lr: 0.000038  min_lr: 0.000000  loss: 4.4323 (4.3565)  class_acc: 0.2917 (0.2462)  loss_scale: 32768.0000 (43596.6408)  weight_decay: 0.0500 (0.0500)  time: 0.4613  data: 0.0160  max mem: 15572
Epoch: [14]  [2100/2809]  eta: 0:06:43  lr: 0.000038  min_lr: 0.000000  loss: 4.3082 (4.3560)  class_acc: 0.2917 (0.2463)  loss_scale: 32768.0000 (43545.1004)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0612  max mem: 15572
Epoch: [14]  [2110/2809]  eta: 0:06:38  lr: 0.000038  min_lr: 0.000000  loss: 4.2830 (4.3563)  class_acc: 0.2500 (0.2464)  loss_scale: 32768.0000 (43494.0483)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1456  max mem: 15572
Epoch: [14]  [2120/2809]  eta: 0:06:32  lr: 0.000038  min_lr: 0.000000  loss: 4.4042 (4.3566)  class_acc: 0.2500 (0.2462)  loss_scale: 32768.0000 (43443.4776)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1569  max mem: 15572
Epoch: [14]  [2130/2809]  eta: 0:06:26  lr: 0.000038  min_lr: 0.000000  loss: 4.4311 (4.3567)  class_acc: 0.2500 (0.2466)  loss_scale: 32768.0000 (43393.3815)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1116  max mem: 15572
Epoch: [14]  [2140/2809]  eta: 0:06:20  lr: 0.000038  min_lr: 0.000000  loss: 4.4192 (4.3569)  class_acc: 0.2917 (0.2467)  loss_scale: 32768.0000 (43343.7534)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0932  max mem: 15572
Epoch: [14]  [2150/2809]  eta: 0:06:15  lr: 0.000038  min_lr: 0.000000  loss: 4.3285 (4.3565)  class_acc: 0.2500 (0.2468)  loss_scale: 32768.0000 (43294.5867)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1065  max mem: 15572
Epoch: [14]  [2160/2809]  eta: 0:06:09  lr: 0.000038  min_lr: 0.000000  loss: 4.3258 (4.3567)  class_acc: 0.2500 (0.2467)  loss_scale: 32768.0000 (43245.8751)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1296  max mem: 15572
Epoch: [14]  [2170/2809]  eta: 0:06:03  lr: 0.000038  min_lr: 0.000000  loss: 4.2914 (4.3567)  class_acc: 0.2500 (0.2469)  loss_scale: 32768.0000 (43197.6122)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1354  max mem: 15572
Epoch: [14]  [2180/2809]  eta: 0:05:58  lr: 0.000038  min_lr: 0.000000  loss: 4.2294 (4.3561)  class_acc: 0.2083 (0.2468)  loss_scale: 32768.0000 (43149.7918)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1062  max mem: 15572
Epoch: [14]  [2190/2809]  eta: 0:05:52  lr: 0.000038  min_lr: 0.000000  loss: 4.3531 (4.3567)  class_acc: 0.2083 (0.2467)  loss_scale: 32768.0000 (43102.4080)  weight_decay: 0.0500 (0.0500)  time: 0.5090  data: 0.0408  max mem: 15572
Epoch: [14]  [2200/2809]  eta: 0:05:46  lr: 0.000038  min_lr: 0.000000  loss: 4.4428 (4.3566)  class_acc: 0.2500 (0.2468)  loss_scale: 32768.0000 (43055.4548)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.0942  max mem: 15572
[2025-01-15 21:30:41,027] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:30:41,027] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [2210/2809]  eta: 0:05:40  lr: 0.000038  min_lr: 0.000000  loss: 4.3108 (4.3568)  class_acc: 0.2500 (0.2469)  loss_scale: 32768.0000 (43127.4898)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1398  max mem: 15572
Epoch: [14]  [2220/2809]  eta: 0:05:35  lr: 0.000038  min_lr: 0.000000  loss: 4.3306 (4.3569)  class_acc: 0.2500 (0.2469)  loss_scale: 65536.0000 (43228.3836)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0890  max mem: 15572
Epoch: [14]  [2230/2809]  eta: 0:05:29  lr: 0.000038  min_lr: 0.000000  loss: 4.3293 (4.3569)  class_acc: 0.2500 (0.2468)  loss_scale: 65536.0000 (43328.3729)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1218  max mem: 15572
Epoch: [14]  [2240/2809]  eta: 0:05:24  lr: 0.000038  min_lr: 0.000000  loss: 4.3496 (4.3572)  class_acc: 0.2083 (0.2468)  loss_scale: 65536.0000 (43427.4699)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.2101  max mem: 15572
[2025-01-15 21:31:07,841] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41573
[2025-01-15 21:31:07,842] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:31:07,842] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [2250/2809]  eta: 0:05:18  lr: 0.000038  min_lr: 0.000000  loss: 4.4599 (4.3579)  class_acc: 0.2500 (0.2469)  loss_scale: 65536.0000 (43467.4580)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1862  max mem: 15572
Epoch: [14]  [2260/2809]  eta: 0:05:12  lr: 0.000038  min_lr: 0.000000  loss: 4.5332 (4.3586)  class_acc: 0.2500 (0.2468)  loss_scale: 32768.0000 (43420.1362)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.1126  max mem: 15572
Epoch: [14]  [2270/2809]  eta: 0:05:07  lr: 0.000038  min_lr: 0.000000  loss: 4.4484 (4.3584)  class_acc: 0.2500 (0.2469)  loss_scale: 32768.0000 (43373.2312)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1449  max mem: 15572
Epoch: [14]  [2280/2809]  eta: 0:05:01  lr: 0.000038  min_lr: 0.000000  loss: 4.2303 (4.3577)  class_acc: 0.2500 (0.2469)  loss_scale: 32768.0000 (43326.7374)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1483  max mem: 15572
Epoch: [14]  [2290/2809]  eta: 0:04:55  lr: 0.000038  min_lr: 0.000000  loss: 4.3962 (4.3584)  class_acc: 0.2083 (0.2469)  loss_scale: 32768.0000 (43280.6495)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0801  max mem: 15572
Epoch: [14]  [2300/2809]  eta: 0:04:49  lr: 0.000038  min_lr: 0.000000  loss: 4.4171 (4.3586)  class_acc: 0.2500 (0.2470)  loss_scale: 32768.0000 (43234.9622)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0950  max mem: 15572
Epoch: [14]  [2310/2809]  eta: 0:04:44  lr: 0.000038  min_lr: 0.000000  loss: 4.2629 (4.3586)  class_acc: 0.2500 (0.2471)  loss_scale: 32768.0000 (43189.6703)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1381  max mem: 15572
Epoch: [14]  [2320/2809]  eta: 0:04:38  lr: 0.000038  min_lr: 0.000000  loss: 4.2835 (4.3584)  class_acc: 0.2917 (0.2473)  loss_scale: 32768.0000 (43144.7686)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1781  max mem: 15572
Epoch: [14]  [2330/2809]  eta: 0:04:32  lr: 0.000038  min_lr: 0.000000  loss: 4.2890 (4.3581)  class_acc: 0.2917 (0.2475)  loss_scale: 32768.0000 (43100.2523)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1080  max mem: 15572
Epoch: [14]  [2340/2809]  eta: 0:04:26  lr: 0.000038  min_lr: 0.000000  loss: 4.2784 (4.3576)  class_acc: 0.2500 (0.2477)  loss_scale: 32768.0000 (43056.1162)  weight_decay: 0.0500 (0.0500)  time: 0.4979  data: 0.0387  max mem: 15572
Epoch: [14]  [2350/2809]  eta: 0:04:21  lr: 0.000038  min_lr: 0.000000  loss: 4.2929 (4.3576)  class_acc: 0.2083 (0.2477)  loss_scale: 32768.0000 (43012.3556)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1222  max mem: 15572
Epoch: [14]  [2360/2809]  eta: 0:04:15  lr: 0.000038  min_lr: 0.000000  loss: 4.4112 (4.3578)  class_acc: 0.2083 (0.2478)  loss_scale: 32768.0000 (42968.9657)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1267  max mem: 15572
Epoch: [14]  [2370/2809]  eta: 0:04:09  lr: 0.000038  min_lr: 0.000000  loss: 4.4112 (4.3581)  class_acc: 0.2500 (0.2478)  loss_scale: 32768.0000 (42925.9418)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0930  max mem: 15572
[2025-01-15 21:32:20,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:32:20,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [2380/2809]  eta: 0:04:04  lr: 0.000038  min_lr: 0.000000  loss: 4.3841 (4.3576)  class_acc: 0.2500 (0.2478)  loss_scale: 32768.0000 (42952.0907)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1042  max mem: 15572
Epoch: [14]  [2390/2809]  eta: 0:03:58  lr: 0.000038  min_lr: 0.000000  loss: 4.1978 (4.3571)  class_acc: 0.2500 (0.2479)  loss_scale: 65536.0000 (43046.5445)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1080  max mem: 15572
Epoch: [14]  [2400/2809]  eta: 0:03:52  lr: 0.000038  min_lr: 0.000000  loss: 4.2308 (4.3567)  class_acc: 0.2500 (0.2478)  loss_scale: 65536.0000 (43140.2116)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0541  max mem: 15572
Epoch: [14]  [2410/2809]  eta: 0:03:46  lr: 0.000038  min_lr: 0.000000  loss: 4.3031 (4.3562)  class_acc: 0.2083 (0.2479)  loss_scale: 65536.0000 (43233.1016)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0782  max mem: 15572
[2025-01-15 21:32:38,782] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41737
[2025-01-15 21:32:38,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:32:38,782] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [2420/2809]  eta: 0:03:41  lr: 0.000038  min_lr: 0.000000  loss: 4.2864 (4.3559)  class_acc: 0.3333 (0.2482)  loss_scale: 32768.0000 (43189.8753)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1132  max mem: 15572
Epoch: [14]  [2430/2809]  eta: 0:03:35  lr: 0.000038  min_lr: 0.000000  loss: 4.2864 (4.3552)  class_acc: 0.3333 (0.2484)  loss_scale: 32768.0000 (43147.0045)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0896  max mem: 15572
Epoch: [14]  [2440/2809]  eta: 0:03:29  lr: 0.000038  min_lr: 0.000000  loss: 4.1325 (4.3547)  class_acc: 0.2083 (0.2485)  loss_scale: 32768.0000 (43104.4850)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0916  max mem: 15572
Epoch: [14]  [2450/2809]  eta: 0:03:24  lr: 0.000038  min_lr: 0.000000  loss: 4.1557 (4.3541)  class_acc: 0.2500 (0.2486)  loss_scale: 32768.0000 (43062.3125)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1403  max mem: 15572
Epoch: [14]  [2460/2809]  eta: 0:03:18  lr: 0.000038  min_lr: 0.000000  loss: 4.3129 (4.3540)  class_acc: 0.2083 (0.2484)  loss_scale: 32768.0000 (43020.4827)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1973  max mem: 15572
Epoch: [14]  [2470/2809]  eta: 0:03:12  lr: 0.000038  min_lr: 0.000000  loss: 4.3021 (4.3534)  class_acc: 0.2500 (0.2485)  loss_scale: 32768.0000 (42978.9915)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1724  max mem: 15572
Epoch: [14]  [2480/2809]  eta: 0:03:07  lr: 0.000038  min_lr: 0.000000  loss: 4.2289 (4.3530)  class_acc: 0.2917 (0.2485)  loss_scale: 32768.0000 (42937.8347)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1414  max mem: 15572
Epoch: [14]  [2490/2809]  eta: 0:03:01  lr: 0.000038  min_lr: 0.000000  loss: 4.2669 (4.3528)  class_acc: 0.1667 (0.2482)  loss_scale: 32768.0000 (42897.0084)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1595  max mem: 15572
Epoch: [14]  [2500/2809]  eta: 0:02:55  lr: 0.000038  min_lr: 0.000000  loss: 4.2588 (4.3522)  class_acc: 0.2083 (0.2485)  loss_scale: 32768.0000 (42856.5086)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1547  max mem: 15572
Epoch: [14]  [2510/2809]  eta: 0:02:50  lr: 0.000038  min_lr: 0.000000  loss: 4.2060 (4.3521)  class_acc: 0.2500 (0.2484)  loss_scale: 32768.0000 (42816.3313)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1038  max mem: 15572
Epoch: [14]  [2520/2809]  eta: 0:02:44  lr: 0.000038  min_lr: 0.000000  loss: 4.3109 (4.3522)  class_acc: 0.2083 (0.2483)  loss_scale: 32768.0000 (42776.4728)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0927  max mem: 15572
Epoch: [14]  [2530/2809]  eta: 0:02:38  lr: 0.000038  min_lr: 0.000000  loss: 4.4249 (4.3524)  class_acc: 0.2500 (0.2483)  loss_scale: 32768.0000 (42736.9293)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1310  max mem: 15572
[2025-01-15 21:33:53,224] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:33:53,224] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [2540/2809]  eta: 0:02:33  lr: 0.000038  min_lr: 0.000000  loss: 4.3168 (4.3521)  class_acc: 0.2500 (0.2484)  loss_scale: 32768.0000 (42710.5927)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1074  max mem: 15572
Epoch: [14]  [2550/2809]  eta: 0:02:27  lr: 0.000038  min_lr: 0.000000  loss: 4.3227 (4.3526)  class_acc: 0.2083 (0.2483)  loss_scale: 65536.0000 (42800.0690)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0521  max mem: 15572
Epoch: [14]  [2560/2809]  eta: 0:02:21  lr: 0.000038  min_lr: 0.000000  loss: 4.4879 (4.3525)  class_acc: 0.2083 (0.2480)  loss_scale: 65536.0000 (42888.8465)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1020  max mem: 15572
Epoch: [14]  [2570/2809]  eta: 0:02:15  lr: 0.000038  min_lr: 0.000000  loss: 4.3035 (4.3525)  class_acc: 0.2500 (0.2482)  loss_scale: 65536.0000 (42976.9335)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1230  max mem: 15572
Epoch: [14]  [2580/2809]  eta: 0:02:10  lr: 0.000038  min_lr: 0.000000  loss: 4.3201 (4.3526)  class_acc: 0.2083 (0.2481)  loss_scale: 65536.0000 (43064.3379)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0945  max mem: 15572
Epoch: [14]  [2590/2809]  eta: 0:02:04  lr: 0.000038  min_lr: 0.000000  loss: 4.3439 (4.3525)  class_acc: 0.2083 (0.2483)  loss_scale: 65536.0000 (43151.0675)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1087  max mem: 15572
Epoch: [14]  [2600/2809]  eta: 0:01:58  lr: 0.000038  min_lr: 0.000000  loss: 4.1693 (4.3520)  class_acc: 0.2500 (0.2484)  loss_scale: 65536.0000 (43237.1303)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1030  max mem: 15572
[2025-01-15 21:34:31,062] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41935
[2025-01-15 21:34:31,062] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:34:31,063] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [2610/2809]  eta: 0:01:53  lr: 0.000038  min_lr: 0.000000  loss: 4.2782 (4.3520)  class_acc: 0.2083 (0.2483)  loss_scale: 65536.0000 (43297.4339)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0700  max mem: 15572
Epoch: [14]  [2620/2809]  eta: 0:01:47  lr: 0.000038  min_lr: 0.000000  loss: 4.2782 (4.3517)  class_acc: 0.1667 (0.2483)  loss_scale: 32768.0000 (43257.2606)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0902  max mem: 15572
Epoch: [14]  [2630/2809]  eta: 0:01:41  lr: 0.000038  min_lr: 0.000000  loss: 4.2162 (4.3515)  class_acc: 0.2083 (0.2483)  loss_scale: 32768.0000 (43217.3926)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1302  max mem: 15572
Epoch: [14]  [2640/2809]  eta: 0:01:36  lr: 0.000038  min_lr: 0.000000  loss: 4.3391 (4.3517)  class_acc: 0.2083 (0.2484)  loss_scale: 32768.0000 (43177.8266)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1403  max mem: 15572
Epoch: [14]  [2650/2809]  eta: 0:01:30  lr: 0.000038  min_lr: 0.000000  loss: 4.3391 (4.3513)  class_acc: 0.2083 (0.2484)  loss_scale: 32768.0000 (43138.5590)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0900  max mem: 15572
Epoch: [14]  [2660/2809]  eta: 0:01:24  lr: 0.000038  min_lr: 0.000000  loss: 4.2904 (4.3514)  class_acc: 0.2083 (0.2482)  loss_scale: 32768.0000 (43099.5866)  weight_decay: 0.0500 (0.0500)  time: 0.4703  data: 0.0590  max mem: 15572
Epoch: [14]  [2670/2809]  eta: 0:01:18  lr: 0.000038  min_lr: 0.000000  loss: 4.3414 (4.3515)  class_acc: 0.2083 (0.2482)  loss_scale: 32768.0000 (43060.9060)  weight_decay: 0.0500 (0.0500)  time: 0.4549  data: 0.0391  max mem: 15572
[2025-01-15 21:35:03,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=42000, skipped=263, lr=[3.695228847971735e-07, 3.695228847971735e-07, 5.278898354245336e-07, 5.278898354245336e-07, 7.541283363207624e-07, 7.541283363207624e-07, 1.0773261947439463e-06, 1.0773261947439463e-06, 1.5390374210627805e-06, 1.5390374210627805e-06, 2.1986248872325438e-06, 2.1986248872325438e-06, 3.140892696046491e-06, 3.140892696046491e-06, 4.4869895657807015e-06, 4.4869895657807015e-06, 6.409985093972431e-06, 6.409985093972431e-06, 9.15712156281776e-06, 9.15712156281776e-06, 1.30816022325968e-05, 1.30816022325968e-05, 1.8688003189424e-05, 1.8688003189424e-05, 2.669714741346286e-05, 2.669714741346286e-05, 3.813878201923266e-05, 3.813878201923266e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:35:03,615] [INFO] [timer.py:260:stop] epoch=0/micro_step=42000/global_step=42000, RunningAvgSamplesPerSec=27.632055655742143, CurrSamplesPerSec=34.091578917422915, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [2680/2809]  eta: 0:01:13  lr: 0.000038  min_lr: 0.000000  loss: 4.3452 (4.3515)  class_acc: 0.2083 (0.2483)  loss_scale: 32768.0000 (43022.5140)  weight_decay: 0.0500 (0.0500)  time: 0.4071  data: 0.0003  max mem: 15572
Epoch: [14]  [2690/2809]  eta: 0:01:07  lr: 0.000038  min_lr: 0.000000  loss: 4.3238 (4.3513)  class_acc: 0.2500 (0.2483)  loss_scale: 32768.0000 (42984.4073)  weight_decay: 0.0500 (0.0500)  time: 0.4146  data: 0.0005  max mem: 15572
Epoch: [14]  [2700/2809]  eta: 0:01:01  lr: 0.000038  min_lr: 0.000000  loss: 4.3238 (4.3515)  class_acc: 0.2500 (0.2483)  loss_scale: 32768.0000 (42946.5827)  weight_decay: 0.0500 (0.0500)  time: 0.4506  data: 0.0006  max mem: 15572
Epoch: [14]  [2710/2809]  eta: 0:00:55  lr: 0.000038  min_lr: 0.000000  loss: 4.3846 (4.3517)  class_acc: 0.1667 (0.2481)  loss_scale: 32768.0000 (42909.0373)  weight_decay: 0.0500 (0.0500)  time: 0.4873  data: 0.0005  max mem: 15572
Epoch: [14]  [2720/2809]  eta: 0:00:50  lr: 0.000038  min_lr: 0.000000  loss: 4.5166 (4.3526)  class_acc: 0.2083 (0.2480)  loss_scale: 32768.0000 (42871.7677)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0645  max mem: 15572
Epoch: [14]  [2730/2809]  eta: 0:00:44  lr: 0.000038  min_lr: 0.000000  loss: 4.5551 (4.3534)  class_acc: 0.2083 (0.2479)  loss_scale: 32768.0000 (42834.7711)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1630  max mem: 15572
[2025-01-15 21:35:39,070] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:35:39,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [2740/2809]  eta: 0:00:39  lr: 0.000038  min_lr: 0.000000  loss: 4.4605 (4.3530)  class_acc: 0.2500 (0.2482)  loss_scale: 32768.0000 (42833.9088)  weight_decay: 0.0500 (0.0500)  time: 0.7092  data: 0.2230  max mem: 15572
Epoch: [14]  [2750/2809]  eta: 0:00:33  lr: 0.000038  min_lr: 0.000000  loss: 4.3596 (4.3531)  class_acc: 0.2917 (0.2482)  loss_scale: 65536.0000 (42916.4318)  weight_decay: 0.0500 (0.0500)  time: 0.7009  data: 0.2195  max mem: 15572
Epoch: [14]  [2760/2809]  eta: 0:00:27  lr: 0.000038  min_lr: 0.000000  loss: 4.3137 (4.3524)  class_acc: 0.2917 (0.2485)  loss_scale: 65536.0000 (42998.3571)  weight_decay: 0.0500 (0.0500)  time: 0.6671  data: 0.2004  max mem: 15572
Epoch: [14]  [2770/2809]  eta: 0:00:22  lr: 0.000038  min_lr: 0.000000  loss: 4.3485 (4.3525)  class_acc: 0.2917 (0.2484)  loss_scale: 65536.0000 (43079.6911)  weight_decay: 0.0500 (0.0500)  time: 0.6963  data: 0.2326  max mem: 15572
Epoch: [14]  [2780/2809]  eta: 0:00:16  lr: 0.000038  min_lr: 0.000000  loss: 4.4664 (4.3530)  class_acc: 0.2917 (0.2486)  loss_scale: 65536.0000 (43160.4401)  weight_decay: 0.0500 (0.0500)  time: 0.6732  data: 0.2169  max mem: 15572
Epoch: [14]  [2790/2809]  eta: 0:00:10  lr: 0.000038  min_lr: 0.000000  loss: 4.4694 (4.3534)  class_acc: 0.2917 (0.2486)  loss_scale: 65536.0000 (43240.6105)  weight_decay: 0.0500 (0.0500)  time: 0.6927  data: 0.2378  max mem: 15572
Epoch: [14]  [2800/2809]  eta: 0:00:05  lr: 0.000038  min_lr: 0.000000  loss: 4.4474 (4.3538)  class_acc: 0.2083 (0.2485)  loss_scale: 65536.0000 (43320.2085)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1913  max mem: 15572
Epoch: [14]  [2808/2809]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000000  loss: 4.5018 (4.3541)  class_acc: 0.2083 (0.2484)  loss_scale: 65536.0000 (43383.4788)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1378  max mem: 15572
Epoch: [14] Total time: 0:26:37 (0.5688 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000000  loss: 4.5018 (4.3541)  class_acc: 0.2083 (0.2484)  loss_scale: 65536.0000 (43383.4788)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:23:18  loss: 1.2138 (1.2138)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 5.1407  data: 4.9461  max mem: 15572
Val:  [ 10/272]  eta: 0:02:41  loss: 3.0044 (2.8288)  acc1: 33.3333 (35.8586)  acc5: 66.6667 (67.6768)  time: 0.6158  data: 0.4500  max mem: 15572
Val:  [ 20/272]  eta: 0:01:41  loss: 3.0044 (2.9354)  acc1: 33.3333 (36.2434)  acc5: 66.6667 (66.6667)  time: 0.1651  data: 0.0004  max mem: 15572
Val:  [ 30/272]  eta: 0:01:18  loss: 2.9696 (2.9630)  acc1: 33.3333 (37.0968)  acc5: 66.6667 (66.6667)  time: 0.1644  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:01:07  loss: 2.9655 (2.9712)  acc1: 27.7778 (33.6043)  acc5: 66.6667 (67.7507)  time: 0.1722  data: 0.0005  max mem: 15572
Val:  [ 50/272]  eta: 0:01:06  loss: 2.9341 (2.8983)  acc1: 33.3333 (35.8388)  acc5: 72.2222 (70.0436)  time: 0.2615  data: 0.0823  max mem: 15572
Val:  [ 60/272]  eta: 0:01:03  loss: 2.1144 (2.8079)  acc1: 55.5556 (38.7067)  acc5: 88.8889 (71.3115)  time: 0.3251  data: 0.1379  max mem: 15572
Val:  [ 70/272]  eta: 0:00:59  loss: 2.2065 (2.7409)  acc1: 61.1111 (41.5493)  acc5: 83.3333 (73.2394)  time: 0.2869  data: 0.0816  max mem: 15572
Val:  [ 80/272]  eta: 0:00:57  loss: 2.4437 (2.7502)  acc1: 50.0000 (41.0151)  acc5: 83.3333 (73.0453)  time: 0.2844  data: 0.0834  max mem: 15572
Val:  [ 90/272]  eta: 0:00:54  loss: 3.1168 (2.8060)  acc1: 27.7778 (39.6825)  acc5: 66.6667 (71.4286)  time: 0.3199  data: 0.1212  max mem: 15572
Val:  [100/272]  eta: 0:00:51  loss: 3.1963 (2.8467)  acc1: 33.3333 (38.8889)  acc5: 61.1111 (70.8471)  time: 0.3202  data: 0.1236  max mem: 15572
Val:  [110/272]  eta: 0:00:49  loss: 3.1963 (2.9014)  acc1: 22.2222 (36.9870)  acc5: 55.5556 (69.3193)  time: 0.3194  data: 0.1402  max mem: 15572
Val:  [120/272]  eta: 0:00:46  loss: 3.2745 (2.9373)  acc1: 16.6667 (36.2718)  acc5: 55.5556 (68.5032)  time: 0.3168  data: 0.1350  max mem: 15572
Val:  [130/272]  eta: 0:00:42  loss: 2.9187 (2.9065)  acc1: 38.8889 (37.0653)  acc5: 72.2222 (69.2536)  time: 0.2925  data: 0.0883  max mem: 15572
Val:  [140/272]  eta: 0:00:40  loss: 2.4731 (2.8989)  acc1: 38.8889 (37.7069)  acc5: 72.2222 (69.3065)  time: 0.3018  data: 0.1006  max mem: 15572
Val:  [150/272]  eta: 0:00:37  loss: 2.8662 (2.8960)  acc1: 33.3333 (37.2701)  acc5: 72.2222 (69.3525)  time: 0.3258  data: 0.1360  max mem: 15572
Val:  [160/272]  eta: 0:00:34  loss: 2.7667 (2.8888)  acc1: 33.3333 (37.4051)  acc5: 72.2222 (69.7378)  time: 0.3504  data: 0.1571  max mem: 15572
Val:  [170/272]  eta: 0:00:30  loss: 2.9012 (2.9085)  acc1: 33.3333 (36.9721)  acc5: 66.6667 (69.2658)  time: 0.2792  data: 0.0886  max mem: 15572
Val:  [180/272]  eta: 0:00:27  loss: 2.8735 (2.8933)  acc1: 27.7778 (37.1393)  acc5: 66.6667 (69.6133)  time: 0.2164  data: 0.0242  max mem: 15572
Val:  [190/272]  eta: 0:00:24  loss: 2.8735 (2.9285)  acc1: 22.2222 (35.7475)  acc5: 61.1111 (68.1792)  time: 0.2859  data: 0.0868  max mem: 15572
Val:  [200/272]  eta: 0:00:21  loss: 3.0243 (2.9348)  acc1: 22.2222 (35.8485)  acc5: 55.5556 (67.9934)  time: 0.3181  data: 0.1263  max mem: 15572
Val:  [210/272]  eta: 0:00:18  loss: 2.6681 (2.9329)  acc1: 38.8889 (36.1506)  acc5: 77.7778 (68.1148)  time: 0.3017  data: 0.1161  max mem: 15572
Val:  [220/272]  eta: 0:00:15  loss: 2.7819 (2.9282)  acc1: 38.8889 (36.4253)  acc5: 77.7778 (68.2252)  time: 0.3217  data: 0.1317  max mem: 15572
Val:  [230/272]  eta: 0:00:12  loss: 2.5620 (2.9107)  acc1: 44.4444 (37.3256)  acc5: 77.7778 (68.8312)  time: 0.3541  data: 0.1683  max mem: 15572
Val:  [240/272]  eta: 0:00:09  loss: 2.3518 (2.8969)  acc1: 61.1111 (37.8976)  acc5: 83.3333 (69.3407)  time: 0.3362  data: 0.1530  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.8376 (2.9059)  acc1: 27.7778 (37.2731)  acc5: 72.2222 (69.1899)  time: 0.2949  data: 0.1097  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.2063 (2.8606)  acc1: 61.1111 (39.1017)  acc5: 83.3333 (70.0298)  time: 0.2765  data: 0.0842  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.2063 (2.8591)  acc1: 55.5556 (38.9709)  acc5: 88.8889 (69.9467)  time: 0.2146  data: 0.0410  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.2063 (2.8618)  acc1: 55.5556 (38.9515)  acc5: 88.8889 (69.9160)  time: 0.2025  data: 0.0348  max mem: 15572
Val: Total time: 0:01:21 (0.2984 s / it)
* Acc@1 38.951 Acc@5 69.916 loss 2.862
Accuracy of the network on the 4883 val videos: 39.0%
[2025-01-15 21:37:45,405] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 21:37:45,407] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 21:37:45,407] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 21:37:48,375] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 21:37:48,375] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 38.95%
Epoch: [15]  [   0/2809]  eta: 6:20:28  lr: 0.000038  min_lr: 0.000000  loss: 4.4078 (4.4078)  class_acc: 0.2500 (0.2500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.1268  data: 7.6759  max mem: 15572
Epoch: [15]  [  10/2809]  eta: 0:57:14  lr: 0.000038  min_lr: 0.000000  loss: 4.4078 (4.3618)  class_acc: 0.2500 (0.2803)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2271  data: 0.7709  max mem: 15572
Epoch: [15]  [  20/2809]  eta: 0:43:17  lr: 0.000038  min_lr: 0.000000  loss: 4.3999 (4.4075)  class_acc: 0.1667 (0.2242)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1104  max mem: 15572
Epoch: [15]  [  30/2809]  eta: 0:38:06  lr: 0.000038  min_lr: 0.000000  loss: 4.2102 (4.3023)  class_acc: 0.1667 (0.2245)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1608  max mem: 15572
Epoch: [15]  [  40/2809]  eta: 0:34:39  lr: 0.000038  min_lr: 0.000000  loss: 4.1513 (4.2829)  class_acc: 0.2083 (0.2226)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1335  max mem: 15572
[2025-01-15 21:38:21,706] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42181
[2025-01-15 21:38:21,707] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:38:21,707] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [  50/2809]  eta: 0:33:08  lr: 0.000038  min_lr: 0.000000  loss: 4.2667 (4.3108)  class_acc: 0.2083 (0.2386)  loss_scale: 65536.0000 (62323.4510)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1285  max mem: 15572
Epoch: [15]  [  60/2809]  eta: 0:31:40  lr: 0.000038  min_lr: 0.000000  loss: 4.4954 (4.3352)  class_acc: 0.2083 (0.2240)  loss_scale: 32768.0000 (57478.2951)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1403  max mem: 15572
Epoch: [15]  [  70/2809]  eta: 0:30:19  lr: 0.000038  min_lr: 0.000000  loss: 4.4357 (4.3581)  class_acc: 0.1667 (0.2254)  loss_scale: 32768.0000 (53997.9718)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0774  max mem: 15572
Epoch: [15]  [  80/2809]  eta: 0:29:28  lr: 0.000038  min_lr: 0.000000  loss: 4.3719 (4.3502)  class_acc: 0.2083 (0.2279)  loss_scale: 32768.0000 (51376.9877)  weight_decay: 0.0500 (0.0500)  time: 0.5158  data: 0.0662  max mem: 15572
Epoch: [15]  [  90/2809]  eta: 0:28:48  lr: 0.000038  min_lr: 0.000000  loss: 4.3939 (4.3594)  class_acc: 0.2083 (0.2289)  loss_scale: 32768.0000 (49332.0440)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0889  max mem: 15572
Epoch: [15]  [ 100/2809]  eta: 0:28:31  lr: 0.000038  min_lr: 0.000000  loss: 4.4966 (4.3713)  class_acc: 0.2083 (0.2269)  loss_scale: 32768.0000 (47692.0396)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1105  max mem: 15572
Epoch: [15]  [ 110/2809]  eta: 0:28:08  lr: 0.000038  min_lr: 0.000000  loss: 4.4364 (4.3697)  class_acc: 0.2500 (0.2331)  loss_scale: 32768.0000 (46347.5315)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1240  max mem: 15572
Epoch: [15]  [ 120/2809]  eta: 0:28:11  lr: 0.000038  min_lr: 0.000000  loss: 4.2766 (4.3595)  class_acc: 0.2500 (0.2345)  loss_scale: 32768.0000 (45225.2562)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1695  max mem: 15572
Epoch: [15]  [ 130/2809]  eta: 0:28:02  lr: 0.000038  min_lr: 0.000000  loss: 4.2439 (4.3567)  class_acc: 0.2083 (0.2363)  loss_scale: 32768.0000 (44274.3206)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.1971  max mem: 15572
Epoch: [15]  [ 140/2809]  eta: 0:27:37  lr: 0.000038  min_lr: 0.000000  loss: 4.3969 (4.3528)  class_acc: 0.2500 (0.2414)  loss_scale: 32768.0000 (43458.2695)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1277  max mem: 15572
Epoch: [15]  [ 150/2809]  eta: 0:27:19  lr: 0.000038  min_lr: 0.000000  loss: 4.4643 (4.3569)  class_acc: 0.3333 (0.2439)  loss_scale: 32768.0000 (42750.3046)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.1018  max mem: 15572
Epoch: [15]  [ 160/2809]  eta: 0:27:15  lr: 0.000038  min_lr: 0.000000  loss: 4.3368 (4.3505)  class_acc: 0.2083 (0.2448)  loss_scale: 32768.0000 (42130.2857)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1235  max mem: 15572
Epoch: [15]  [ 170/2809]  eta: 0:26:49  lr: 0.000038  min_lr: 0.000000  loss: 4.3454 (4.3519)  class_acc: 0.2083 (0.2446)  loss_scale: 32768.0000 (41582.7836)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0857  max mem: 15572
[2025-01-15 21:39:34,956] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:39:34,957] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [ 180/2809]  eta: 0:26:42  lr: 0.000038  min_lr: 0.000000  loss: 4.3728 (4.3502)  class_acc: 0.2500 (0.2461)  loss_scale: 32768.0000 (42182.0110)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0961  max mem: 15572
Epoch: [15]  [ 190/2809]  eta: 0:26:27  lr: 0.000038  min_lr: 0.000000  loss: 4.3037 (4.3506)  class_acc: 0.2500 (0.2456)  loss_scale: 65536.0000 (43404.7330)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1332  max mem: 15572
Epoch: [15]  [ 200/2809]  eta: 0:26:13  lr: 0.000038  min_lr: 0.000000  loss: 4.2689 (4.3461)  class_acc: 0.2500 (0.2467)  loss_scale: 65536.0000 (44505.7910)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0996  max mem: 15572
[2025-01-15 21:39:53,208] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42343
[2025-01-15 21:39:53,209] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:39:53,209] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [ 210/2809]  eta: 0:26:00  lr: 0.000038  min_lr: 0.000000  loss: 4.3047 (4.3449)  class_acc: 0.2083 (0.2464)  loss_scale: 65536.0000 (45036.5877)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0951  max mem: 15572
Epoch: [15]  [ 220/2809]  eta: 0:25:59  lr: 0.000038  min_lr: 0.000000  loss: 4.3181 (4.3422)  class_acc: 0.2500 (0.2483)  loss_scale: 32768.0000 (44481.4480)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1619  max mem: 15572
Epoch: [15]  [ 230/2809]  eta: 0:25:54  lr: 0.000038  min_lr: 0.000000  loss: 4.2965 (4.3350)  class_acc: 0.2917 (0.2509)  loss_scale: 32768.0000 (43974.3723)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1971  max mem: 15572
Epoch: [15]  [ 240/2809]  eta: 0:25:36  lr: 0.000038  min_lr: 0.000000  loss: 4.3159 (4.3332)  class_acc: 0.2500 (0.2502)  loss_scale: 32768.0000 (43509.3776)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1100  max mem: 15572
Epoch: [15]  [ 250/2809]  eta: 0:25:23  lr: 0.000038  min_lr: 0.000000  loss: 4.3392 (4.3334)  class_acc: 0.2500 (0.2498)  loss_scale: 32768.0000 (43081.4343)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0646  max mem: 15572
Epoch: [15]  [ 260/2809]  eta: 0:25:18  lr: 0.000038  min_lr: 0.000000  loss: 4.3256 (4.3330)  class_acc: 0.1667 (0.2471)  loss_scale: 32768.0000 (42686.2835)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1127  max mem: 15572
Epoch: [15]  [ 270/2809]  eta: 0:25:09  lr: 0.000038  min_lr: 0.000000  loss: 4.3183 (4.3306)  class_acc: 0.2083 (0.2472)  loss_scale: 32768.0000 (42320.2952)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1198  max mem: 15572
Epoch: [15]  [ 280/2809]  eta: 0:24:53  lr: 0.000038  min_lr: 0.000000  loss: 4.3059 (4.3293)  class_acc: 0.2500 (0.2481)  loss_scale: 32768.0000 (41980.3559)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0681  max mem: 15572
Epoch: [15]  [ 290/2809]  eta: 0:24:41  lr: 0.000038  min_lr: 0.000000  loss: 4.2563 (4.3294)  class_acc: 0.2083 (0.2483)  loss_scale: 32768.0000 (41663.7801)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0502  max mem: 15572
Epoch: [15]  [ 300/2809]  eta: 0:24:37  lr: 0.000038  min_lr: 0.000000  loss: 4.2607 (4.3294)  class_acc: 0.2500 (0.2488)  loss_scale: 32768.0000 (41368.2392)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1039  max mem: 15572
Epoch: [15]  [ 310/2809]  eta: 0:24:25  lr: 0.000038  min_lr: 0.000000  loss: 4.2976 (4.3276)  class_acc: 0.2500 (0.2495)  loss_scale: 32768.0000 (41091.7042)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1110  max mem: 15572
Epoch: [15]  [ 320/2809]  eta: 0:24:11  lr: 0.000038  min_lr: 0.000000  loss: 4.2959 (4.3276)  class_acc: 0.2500 (0.2491)  loss_scale: 32768.0000 (40832.3988)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0473  max mem: 15572
Epoch: [15]  [ 330/2809]  eta: 0:24:02  lr: 0.000038  min_lr: 0.000000  loss: 4.3965 (4.3299)  class_acc: 0.1667 (0.2463)  loss_scale: 32768.0000 (40588.7613)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0466  max mem: 15572
[2025-01-15 21:41:04,140] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:41:04,140] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [ 340/2809]  eta: 0:24:02  lr: 0.000038  min_lr: 0.000000  loss: 4.3965 (4.3300)  class_acc: 0.2083 (0.2482)  loss_scale: 32768.0000 (40743.7889)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1624  max mem: 15572
Epoch: [15]  [ 350/2809]  eta: 0:23:57  lr: 0.000038  min_lr: 0.000000  loss: 4.3505 (4.3325)  class_acc: 0.2917 (0.2477)  loss_scale: 65536.0000 (41450.1197)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1912  max mem: 15572
Epoch: [15]  [ 360/2809]  eta: 0:23:48  lr: 0.000038  min_lr: 0.000000  loss: 4.4205 (4.3309)  class_acc: 0.2500 (0.2485)  loss_scale: 65536.0000 (42117.3186)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1083  max mem: 15572
Epoch: [15]  [ 370/2809]  eta: 0:23:42  lr: 0.000038  min_lr: 0.000000  loss: 4.4613 (4.3366)  class_acc: 0.2083 (0.2472)  loss_scale: 65536.0000 (42748.5499)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1114  max mem: 15572
Epoch: [15]  [ 380/2809]  eta: 0:23:28  lr: 0.000038  min_lr: 0.000000  loss: 4.4950 (4.3369)  class_acc: 0.2083 (0.2475)  loss_scale: 65536.0000 (43346.6457)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0726  max mem: 15572
Epoch: [15]  [ 390/2809]  eta: 0:23:21  lr: 0.000038  min_lr: 0.000000  loss: 4.4333 (4.3355)  class_acc: 0.2083 (0.2467)  loss_scale: 65536.0000 (43914.1483)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0511  max mem: 15572
Epoch: [15]  [ 400/2809]  eta: 0:23:14  lr: 0.000038  min_lr: 0.000000  loss: 4.3076 (4.3368)  class_acc: 0.2500 (0.2486)  loss_scale: 65536.0000 (44453.3466)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1140  max mem: 15572
Epoch: [15]  [ 410/2809]  eta: 0:23:09  lr: 0.000038  min_lr: 0.000000  loss: 4.3525 (4.3380)  class_acc: 0.2500 (0.2481)  loss_scale: 65536.0000 (44966.3066)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1276  max mem: 15572
[2025-01-15 21:41:49,775] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42552
[2025-01-15 21:41:49,775] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:41:49,776] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [ 420/2809]  eta: 0:23:03  lr: 0.000038  min_lr: 0.000000  loss: 4.3149 (4.3352)  class_acc: 0.2917 (0.2500)  loss_scale: 65536.0000 (45143.5629)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1366  max mem: 15572
Epoch: [15]  [ 430/2809]  eta: 0:22:54  lr: 0.000038  min_lr: 0.000000  loss: 4.2156 (4.3303)  class_acc: 0.3333 (0.2523)  loss_scale: 32768.0000 (44856.4269)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1026  max mem: 15572
Epoch: [15]  [ 440/2809]  eta: 0:22:51  lr: 0.000038  min_lr: 0.000000  loss: 4.2037 (4.3288)  class_acc: 0.2500 (0.2521)  loss_scale: 32768.0000 (44582.3129)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1150  max mem: 15572
Epoch: [15]  [ 450/2809]  eta: 0:22:41  lr: 0.000038  min_lr: 0.000000  loss: 4.3974 (4.3298)  class_acc: 0.2500 (0.2522)  loss_scale: 32768.0000 (44320.3548)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1221  max mem: 15572
Epoch: [15]  [ 460/2809]  eta: 0:22:35  lr: 0.000038  min_lr: 0.000000  loss: 4.3510 (4.3282)  class_acc: 0.2917 (0.2526)  loss_scale: 32768.0000 (44069.7614)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0951  max mem: 15572
Epoch: [15]  [ 470/2809]  eta: 0:22:29  lr: 0.000038  min_lr: 0.000000  loss: 4.3039 (4.3263)  class_acc: 0.2500 (0.2532)  loss_scale: 32768.0000 (43829.8089)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1203  max mem: 15572
Epoch: [15]  [ 480/2809]  eta: 0:22:24  lr: 0.000038  min_lr: 0.000000  loss: 4.3039 (4.3272)  class_acc: 0.2500 (0.2545)  loss_scale: 32768.0000 (43599.8337)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1419  max mem: 15572
Epoch: [15]  [ 490/2809]  eta: 0:22:21  lr: 0.000038  min_lr: 0.000000  loss: 4.3080 (4.3264)  class_acc: 0.2500 (0.2536)  loss_scale: 32768.0000 (43379.2261)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1745  max mem: 15572
Epoch: [15]  [ 500/2809]  eta: 0:22:11  lr: 0.000038  min_lr: 0.000000  loss: 4.3164 (4.3276)  class_acc: 0.1667 (0.2521)  loss_scale: 32768.0000 (43167.4251)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1121  max mem: 15572
Epoch: [15]  [ 510/2809]  eta: 0:22:04  lr: 0.000038  min_lr: 0.000000  loss: 4.3557 (4.3293)  class_acc: 0.2083 (0.2525)  loss_scale: 32768.0000 (42963.9139)  weight_decay: 0.0500 (0.0500)  time: 0.5241  data: 0.0795  max mem: 15572
Epoch: [15]  [ 520/2809]  eta: 0:21:57  lr: 0.000038  min_lr: 0.000000  loss: 4.3769 (4.3321)  class_acc: 0.2083 (0.2518)  loss_scale: 32768.0000 (42768.2150)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.1143  max mem: 15572
Epoch: [15]  [ 530/2809]  eta: 0:21:50  lr: 0.000038  min_lr: 0.000000  loss: 4.4331 (4.3336)  class_acc: 0.2500 (0.2531)  loss_scale: 32768.0000 (42579.8870)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.1032  max mem: 15572
Epoch: [15]  [ 540/2809]  eta: 0:21:47  lr: 0.000038  min_lr: 0.000000  loss: 4.3389 (4.3315)  class_acc: 0.2917 (0.2525)  loss_scale: 32768.0000 (42398.5213)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1595  max mem: 15572
[2025-01-15 21:43:02,957] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:43:02,957] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [ 550/2809]  eta: 0:21:36  lr: 0.000038  min_lr: 0.000000  loss: 4.3628 (4.3329)  class_acc: 0.2083 (0.2520)  loss_scale: 32768.0000 (42521.0889)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.1079  max mem: 15572
Epoch: [15]  [ 560/2809]  eta: 0:21:35  lr: 0.000038  min_lr: 0.000000  loss: 4.3177 (4.3302)  class_acc: 0.2083 (0.2522)  loss_scale: 65536.0000 (42931.3369)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1218  max mem: 15572
Epoch: [15]  [ 570/2809]  eta: 0:21:25  lr: 0.000038  min_lr: 0.000000  loss: 4.2387 (4.3292)  class_acc: 0.2083 (0.2512)  loss_scale: 65536.0000 (43327.2154)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1218  max mem: 15572
Epoch: [15]  [ 580/2809]  eta: 0:21:20  lr: 0.000038  min_lr: 0.000000  loss: 4.3126 (4.3306)  class_acc: 0.1667 (0.2509)  loss_scale: 65536.0000 (43709.4664)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0616  max mem: 15572
Epoch: [15]  [ 590/2809]  eta: 0:21:10  lr: 0.000038  min_lr: 0.000000  loss: 4.4075 (4.3310)  class_acc: 0.2500 (0.2519)  loss_scale: 65536.0000 (44078.7817)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0620  max mem: 15572
Epoch: [15]  [ 600/2809]  eta: 0:21:00  lr: 0.000038  min_lr: 0.000000  loss: 4.4729 (4.3332)  class_acc: 0.2500 (0.2513)  loss_scale: 65536.0000 (44435.8070)  weight_decay: 0.0500 (0.0500)  time: 0.4621  data: 0.0010  max mem: 15572
Epoch: [15]  [ 610/2809]  eta: 0:20:56  lr: 0.000038  min_lr: 0.000000  loss: 4.4088 (4.3340)  class_acc: 0.2083 (0.2510)  loss_scale: 65536.0000 (44781.1457)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0929  max mem: 15572
Epoch: [15]  [ 620/2809]  eta: 0:20:52  lr: 0.000038  min_lr: 0.000000  loss: 4.3356 (4.3358)  class_acc: 0.2500 (0.2512)  loss_scale: 65536.0000 (45115.3623)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1810  max mem: 15572
Epoch: [15]  [ 630/2809]  eta: 0:20:44  lr: 0.000038  min_lr: 0.000000  loss: 4.4003 (4.3360)  class_acc: 0.2083 (0.2507)  loss_scale: 65536.0000 (45438.9857)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1123  max mem: 15572
Epoch: [15]  [ 640/2809]  eta: 0:20:41  lr: 0.000038  min_lr: 0.000000  loss: 4.4378 (4.3365)  class_acc: 0.2083 (0.2503)  loss_scale: 65536.0000 (45752.5117)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1233  max mem: 15572
[2025-01-15 21:43:56,979] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42778
[2025-01-15 21:43:56,979] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:43:56,979] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [ 650/2809]  eta: 0:20:36  lr: 0.000038  min_lr: 0.000000  loss: 4.3839 (4.3338)  class_acc: 0.2500 (0.2508)  loss_scale: 65536.0000 (45653.7266)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1653  max mem: 15572
Epoch: [15]  [ 660/2809]  eta: 0:20:31  lr: 0.000038  min_lr: 0.000000  loss: 4.3504 (4.3348)  class_acc: 0.2917 (0.2523)  loss_scale: 32768.0000 (45458.7837)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1156  max mem: 15572
Epoch: [15]  [ 670/2809]  eta: 0:20:26  lr: 0.000038  min_lr: 0.000000  loss: 4.3637 (4.3356)  class_acc: 0.2500 (0.2519)  loss_scale: 32768.0000 (45269.6513)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0992  max mem: 15572
Epoch: [15]  [ 680/2809]  eta: 0:20:17  lr: 0.000038  min_lr: 0.000000  loss: 4.4180 (4.3356)  class_acc: 0.2500 (0.2517)  loss_scale: 32768.0000 (45086.0734)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0630  max mem: 15572
Epoch: [15]  [ 690/2809]  eta: 0:20:16  lr: 0.000038  min_lr: 0.000000  loss: 4.5134 (4.3386)  class_acc: 0.2083 (0.2513)  loss_scale: 32768.0000 (44907.8090)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1394  max mem: 15572
Epoch: [15]  [ 700/2809]  eta: 0:20:10  lr: 0.000038  min_lr: 0.000000  loss: 4.4652 (4.3392)  class_acc: 0.2500 (0.2513)  loss_scale: 32768.0000 (44734.6305)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.1723  max mem: 15572
Epoch: [15]  [ 710/2809]  eta: 0:20:03  lr: 0.000038  min_lr: 0.000000  loss: 4.2379 (4.3369)  class_acc: 0.2500 (0.2514)  loss_scale: 32768.0000 (44566.3235)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0830  max mem: 15572
Epoch: [15]  [ 720/2809]  eta: 0:19:58  lr: 0.000038  min_lr: 0.000000  loss: 4.1906 (4.3366)  class_acc: 0.2500 (0.2516)  loss_scale: 32768.0000 (44402.6852)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1132  max mem: 15572
Epoch: [15]  [ 730/2809]  eta: 0:19:53  lr: 0.000038  min_lr: 0.000000  loss: 4.2731 (4.3349)  class_acc: 0.2500 (0.2512)  loss_scale: 32768.0000 (44243.5239)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1446  max mem: 15572
Epoch: [15]  [ 740/2809]  eta: 0:19:46  lr: 0.000038  min_lr: 0.000000  loss: 4.2731 (4.3346)  class_acc: 0.1667 (0.2510)  loss_scale: 32768.0000 (44088.6586)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1224  max mem: 15572
Epoch: [15]  [ 750/2809]  eta: 0:19:40  lr: 0.000038  min_lr: 0.000000  loss: 4.3418 (4.3332)  class_acc: 0.2500 (0.2515)  loss_scale: 32768.0000 (43937.9174)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1152  max mem: 15572
Epoch: [15]  [ 760/2809]  eta: 0:19:33  lr: 0.000038  min_lr: 0.000000  loss: 4.1893 (4.3322)  class_acc: 0.2083 (0.2508)  loss_scale: 32768.0000 (43791.1380)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0982  max mem: 15572
Epoch: [15]  [ 770/2809]  eta: 0:19:28  lr: 0.000038  min_lr: 0.000000  loss: 4.2842 (4.3326)  class_acc: 0.1667 (0.2501)  loss_scale: 32768.0000 (43648.1660)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1072  max mem: 15572
[2025-01-15 21:45:11,530] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:45:11,530] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [ 780/2809]  eta: 0:19:20  lr: 0.000038  min_lr: 0.000000  loss: 4.3339 (4.3323)  class_acc: 0.2083 (0.2507)  loss_scale: 32768.0000 (43886.4635)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0872  max mem: 15572
Epoch: [15]  [ 790/2809]  eta: 0:19:13  lr: 0.000038  min_lr: 0.000000  loss: 4.2237 (4.3304)  class_acc: 0.2917 (0.2512)  loss_scale: 65536.0000 (44160.1618)  weight_decay: 0.0500 (0.0500)  time: 0.5002  data: 0.0619  max mem: 15572
Epoch: [15]  [ 800/2809]  eta: 0:19:09  lr: 0.000038  min_lr: 0.000000  loss: 4.1810 (4.3301)  class_acc: 0.2500 (0.2507)  loss_scale: 65536.0000 (44427.0262)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1320  max mem: 15572
Epoch: [15]  [ 810/2809]  eta: 0:19:00  lr: 0.000038  min_lr: 0.000000  loss: 4.1325 (4.3275)  class_acc: 0.2500 (0.2518)  loss_scale: 65536.0000 (44687.3095)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.1012  max mem: 15572
Epoch: [15]  [ 820/2809]  eta: 0:18:56  lr: 0.000038  min_lr: 0.000000  loss: 4.1206 (4.3267)  class_acc: 0.2500 (0.2524)  loss_scale: 65536.0000 (44941.2521)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.1020  max mem: 15572
Epoch: [15]  [ 830/2809]  eta: 0:18:49  lr: 0.000038  min_lr: 0.000000  loss: 4.3110 (4.3263)  class_acc: 0.2500 (0.2524)  loss_scale: 65536.0000 (45189.0830)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1292  max mem: 15572
Epoch: [15]  [ 840/2809]  eta: 0:18:42  lr: 0.000038  min_lr: 0.000000  loss: 4.3515 (4.3268)  class_acc: 0.2500 (0.2527)  loss_scale: 65536.0000 (45431.0202)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0709  max mem: 15572
Epoch: [15]  [ 850/2809]  eta: 0:18:35  lr: 0.000038  min_lr: 0.000000  loss: 4.3652 (4.3272)  class_acc: 0.2500 (0.2531)  loss_scale: 65536.0000 (45667.2714)  weight_decay: 0.0500 (0.0500)  time: 0.5104  data: 0.0605  max mem: 15572
Epoch: [15]  [ 860/2809]  eta: 0:18:28  lr: 0.000038  min_lr: 0.000000  loss: 4.3647 (4.3270)  class_acc: 0.2917 (0.2541)  loss_scale: 65536.0000 (45898.0348)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0631  max mem: 15572
[2025-01-15 21:46:01,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=43000, skipped=267, lr=[3.6380340292923513e-07, 3.6380340292923513e-07, 5.197191470417645e-07, 5.197191470417645e-07, 7.42455924345378e-07, 7.42455924345378e-07, 1.0606513204933972e-06, 1.0606513204933972e-06, 1.5152161721334248e-06, 1.5152161721334248e-06, 2.1645945316191783e-06, 2.1645945316191783e-06, 3.0922779023131118e-06, 3.0922779023131118e-06, 4.417539860447303e-06, 4.417539860447303e-06, 6.310771229210433e-06, 6.310771229210433e-06, 9.01538747030062e-06, 9.01538747030062e-06, 1.2879124957572313e-05, 1.2879124957572313e-05, 1.839874993938902e-05, 1.839874993938902e-05, 2.628392848484146e-05, 2.628392848484146e-05, 3.754846926405923e-05, 3.754846926405923e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:46:01,079] [INFO] [timer.py:260:stop] epoch=0/micro_step=43000/global_step=43000, RunningAvgSamplesPerSec=27.64451098496856, CurrSamplesPerSec=32.43097962450111, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [15]  [ 870/2809]  eta: 0:18:22  lr: 0.000038  min_lr: 0.000000  loss: 4.4152 (4.3296)  class_acc: 0.2917 (0.2540)  loss_scale: 65536.0000 (46123.4994)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0957  max mem: 15572
Epoch: [15]  [ 880/2809]  eta: 0:18:18  lr: 0.000038  min_lr: 0.000000  loss: 4.4571 (4.3303)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (46343.8456)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1600  max mem: 15572
Epoch: [15]  [ 890/2809]  eta: 0:18:13  lr: 0.000038  min_lr: 0.000000  loss: 4.3565 (4.3302)  class_acc: 0.2917 (0.2546)  loss_scale: 65536.0000 (46559.2458)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1580  max mem: 15572
[2025-01-15 21:46:21,625] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43035
[2025-01-15 21:46:21,626] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:46:21,626] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [ 900/2809]  eta: 0:18:06  lr: 0.000038  min_lr: 0.000000  loss: 4.3565 (4.3304)  class_acc: 0.2083 (0.2543)  loss_scale: 65536.0000 (46733.4961)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0952  max mem: 15572
Epoch: [15]  [ 910/2809]  eta: 0:18:00  lr: 0.000038  min_lr: 0.000000  loss: 4.4033 (4.3322)  class_acc: 0.2083 (0.2541)  loss_scale: 32768.0000 (46580.1976)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0854  max mem: 15572
Epoch: [15]  [ 920/2809]  eta: 0:17:54  lr: 0.000038  min_lr: 0.000000  loss: 4.3874 (4.3315)  class_acc: 0.2083 (0.2539)  loss_scale: 32768.0000 (46430.2280)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1274  max mem: 15572
Epoch: [15]  [ 930/2809]  eta: 0:17:47  lr: 0.000038  min_lr: 0.000000  loss: 4.2892 (4.3313)  class_acc: 0.2083 (0.2536)  loss_scale: 32768.0000 (46283.4801)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1012  max mem: 15572
Epoch: [15]  [ 940/2809]  eta: 0:17:42  lr: 0.000038  min_lr: 0.000000  loss: 4.3381 (4.3319)  class_acc: 0.2083 (0.2533)  loss_scale: 32768.0000 (46139.8512)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0910  max mem: 15572
Epoch: [15]  [ 950/2809]  eta: 0:17:37  lr: 0.000037  min_lr: 0.000000  loss: 4.3190 (4.3320)  class_acc: 0.1667 (0.2534)  loss_scale: 32768.0000 (45999.2429)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1289  max mem: 15572
Epoch: [15]  [ 960/2809]  eta: 0:17:30  lr: 0.000037  min_lr: 0.000000  loss: 4.3123 (4.3318)  class_acc: 0.2500 (0.2535)  loss_scale: 32768.0000 (45861.5609)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0950  max mem: 15572
Epoch: [15]  [ 970/2809]  eta: 0:17:25  lr: 0.000037  min_lr: 0.000000  loss: 4.2694 (4.3305)  class_acc: 0.2083 (0.2529)  loss_scale: 32768.0000 (45726.7147)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1227  max mem: 15572
Epoch: [15]  [ 980/2809]  eta: 0:17:19  lr: 0.000037  min_lr: 0.000000  loss: 4.3499 (4.3323)  class_acc: 0.2083 (0.2525)  loss_scale: 32768.0000 (45594.6177)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1545  max mem: 15572
Epoch: [15]  [ 990/2809]  eta: 0:17:14  lr: 0.000037  min_lr: 0.000000  loss: 4.3475 (4.3312)  class_acc: 0.2500 (0.2530)  loss_scale: 32768.0000 (45465.1867)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1324  max mem: 15572
Epoch: [15]  [1000/2809]  eta: 0:17:08  lr: 0.000037  min_lr: 0.000000  loss: 4.1926 (4.3301)  class_acc: 0.2917 (0.2535)  loss_scale: 32768.0000 (45338.3417)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1360  max mem: 15572
Epoch: [15]  [1010/2809]  eta: 0:17:02  lr: 0.000037  min_lr: 0.000000  loss: 4.3131 (4.3303)  class_acc: 0.2083 (0.2533)  loss_scale: 32768.0000 (45214.0059)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1316  max mem: 15572
Epoch: [15]  [1020/2809]  eta: 0:16:56  lr: 0.000037  min_lr: 0.000000  loss: 4.3585 (4.3303)  class_acc: 0.2500 (0.2535)  loss_scale: 32768.0000 (45092.1058)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1181  max mem: 15572
[2025-01-15 21:47:34,401] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:47:34,402] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [1030/2809]  eta: 0:16:51  lr: 0.000037  min_lr: 0.000000  loss: 4.3585 (4.3297)  class_acc: 0.2917 (0.2540)  loss_scale: 32768.0000 (45036.1358)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1192  max mem: 15572
Epoch: [15]  [1040/2809]  eta: 0:16:47  lr: 0.000037  min_lr: 0.000000  loss: 4.4381 (4.3318)  class_acc: 0.1667 (0.2535)  loss_scale: 65536.0000 (45233.0605)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1703  max mem: 15572
Epoch: [15]  [1050/2809]  eta: 0:16:41  lr: 0.000037  min_lr: 0.000000  loss: 4.4763 (4.3325)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (45426.2379)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1503  max mem: 15572
Epoch: [15]  [1060/2809]  eta: 0:16:37  lr: 0.000037  min_lr: 0.000000  loss: 4.2998 (4.3325)  class_acc: 0.2500 (0.2529)  loss_scale: 65536.0000 (45615.7738)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1403  max mem: 15572
Epoch: [15]  [1070/2809]  eta: 0:16:29  lr: 0.000037  min_lr: 0.000000  loss: 4.3244 (4.3335)  class_acc: 0.2500 (0.2529)  loss_scale: 65536.0000 (45801.7703)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0952  max mem: 15572
Epoch: [15]  [1080/2809]  eta: 0:16:22  lr: 0.000037  min_lr: 0.000000  loss: 4.3244 (4.3338)  class_acc: 0.2917 (0.2531)  loss_scale: 65536.0000 (45984.3256)  weight_decay: 0.0500 (0.0500)  time: 0.4820  data: 0.0128  max mem: 15572
Epoch: [15]  [1090/2809]  eta: 0:16:17  lr: 0.000037  min_lr: 0.000000  loss: 4.2646 (4.3336)  class_acc: 0.2500 (0.2529)  loss_scale: 65536.0000 (46163.5344)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0834  max mem: 15572
Epoch: [15]  [1100/2809]  eta: 0:16:11  lr: 0.000037  min_lr: 0.000000  loss: 4.3200 (4.3341)  class_acc: 0.2500 (0.2530)  loss_scale: 65536.0000 (46339.4877)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1067  max mem: 15572
Epoch: [15]  [1110/2809]  eta: 0:16:06  lr: 0.000037  min_lr: 0.000000  loss: 4.4740 (4.3351)  class_acc: 0.2083 (0.2524)  loss_scale: 65536.0000 (46512.2736)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1245  max mem: 15572
Epoch: [15]  [1120/2809]  eta: 0:16:01  lr: 0.000037  min_lr: 0.000000  loss: 4.5374 (4.3371)  class_acc: 0.2083 (0.2522)  loss_scale: 65536.0000 (46681.9768)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1748  max mem: 15572
Epoch: [15]  [1130/2809]  eta: 0:15:54  lr: 0.000037  min_lr: 0.000000  loss: 4.4573 (4.3377)  class_acc: 0.2083 (0.2520)  loss_scale: 65536.0000 (46848.6790)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1099  max mem: 15572
Epoch: [15]  [1140/2809]  eta: 0:15:48  lr: 0.000037  min_lr: 0.000000  loss: 4.4567 (4.3382)  class_acc: 0.2083 (0.2523)  loss_scale: 65536.0000 (47012.4592)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0720  max mem: 15572
Epoch: [15]  [1150/2809]  eta: 0:15:42  lr: 0.000037  min_lr: 0.000000  loss: 4.4274 (4.3389)  class_acc: 0.2500 (0.2519)  loss_scale: 65536.0000 (47173.3936)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0973  max mem: 15572
[2025-01-15 21:48:46,348] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:48:46,348] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:48:46,753] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43293
[2025-01-15 21:48:46,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:48:46,754] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1160/2809]  eta: 0:15:35  lr: 0.000037  min_lr: 0.000000  loss: 4.4746 (4.3401)  class_acc: 0.2083 (0.2512)  loss_scale: 65536.0000 (47388.0034)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0762  max mem: 15572
Epoch: [15]  [1170/2809]  eta: 0:15:28  lr: 0.000037  min_lr: 0.000000  loss: 4.4917 (4.3394)  class_acc: 0.2083 (0.2515)  loss_scale: 65536.0000 (47542.9821)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0568  max mem: 15572
Epoch: [15]  [1180/2809]  eta: 0:15:25  lr: 0.000037  min_lr: 0.000000  loss: 4.5001 (4.3408)  class_acc: 0.2083 (0.2510)  loss_scale: 65536.0000 (47695.3362)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1706  max mem: 15572
[2025-01-15 21:49:03,920] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43323
[2025-01-15 21:49:03,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:49:03,920] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [1190/2809]  eta: 0:15:18  lr: 0.000037  min_lr: 0.000000  loss: 4.4031 (4.3405)  class_acc: 0.2083 (0.2515)  loss_scale: 65536.0000 (47762.5928)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1563  max mem: 15572
Epoch: [15]  [1200/2809]  eta: 0:15:13  lr: 0.000037  min_lr: 0.000000  loss: 4.3384 (4.3409)  class_acc: 0.2917 (0.2515)  loss_scale: 32768.0000 (47637.7419)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0803  max mem: 15572
Epoch: [15]  [1210/2809]  eta: 0:15:06  lr: 0.000037  min_lr: 0.000000  loss: 4.4225 (4.3416)  class_acc: 0.2083 (0.2511)  loss_scale: 32768.0000 (47514.9529)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0798  max mem: 15572
Epoch: [15]  [1220/2809]  eta: 0:15:00  lr: 0.000037  min_lr: 0.000000  loss: 4.3290 (4.3409)  class_acc: 0.2083 (0.2513)  loss_scale: 32768.0000 (47394.1753)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0584  max mem: 15572
Epoch: [15]  [1230/2809]  eta: 0:14:54  lr: 0.000037  min_lr: 0.000000  loss: 4.3108 (4.3400)  class_acc: 0.2083 (0.2510)  loss_scale: 32768.0000 (47275.3599)  weight_decay: 0.0500 (0.0500)  time: 0.5269  data: 0.0747  max mem: 15572
Epoch: [15]  [1240/2809]  eta: 0:14:48  lr: 0.000037  min_lr: 0.000000  loss: 4.3352 (4.3402)  class_acc: 0.2083 (0.2513)  loss_scale: 32768.0000 (47158.4593)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0673  max mem: 15572
Epoch: [15]  [1250/2809]  eta: 0:14:42  lr: 0.000037  min_lr: 0.000000  loss: 4.4184 (4.3408)  class_acc: 0.2917 (0.2516)  loss_scale: 32768.0000 (47043.4277)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1088  max mem: 15572
Epoch: [15]  [1260/2809]  eta: 0:14:37  lr: 0.000037  min_lr: 0.000000  loss: 4.4184 (4.3406)  class_acc: 0.2917 (0.2520)  loss_scale: 32768.0000 (46930.2205)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1212  max mem: 15572
Epoch: [15]  [1270/2809]  eta: 0:14:31  lr: 0.000037  min_lr: 0.000000  loss: 4.2925 (4.3408)  class_acc: 0.1667 (0.2514)  loss_scale: 32768.0000 (46818.7946)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0840  max mem: 15572
Epoch: [15]  [1280/2809]  eta: 0:14:25  lr: 0.000037  min_lr: 0.000000  loss: 4.2906 (4.3404)  class_acc: 0.2083 (0.2515)  loss_scale: 32768.0000 (46709.1085)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1080  max mem: 15572
Epoch: [15]  [1290/2809]  eta: 0:14:20  lr: 0.000037  min_lr: 0.000000  loss: 4.4026 (4.3413)  class_acc: 0.2083 (0.2513)  loss_scale: 32768.0000 (46601.1216)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1084  max mem: 15572
Epoch: [15]  [1300/2809]  eta: 0:14:14  lr: 0.000037  min_lr: 0.000000  loss: 4.4025 (4.3424)  class_acc: 0.2083 (0.2514)  loss_scale: 32768.0000 (46494.7948)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0835  max mem: 15572
Epoch: [15]  [1310/2809]  eta: 0:14:10  lr: 0.000037  min_lr: 0.000000  loss: 4.3813 (4.3429)  class_acc: 0.2500 (0.2516)  loss_scale: 32768.0000 (46390.0900)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1890  max mem: 15572
[2025-01-15 21:50:15,881] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:50:15,882] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [1320/2809]  eta: 0:14:03  lr: 0.000037  min_lr: 0.000000  loss: 4.4698 (4.3436)  class_acc: 0.2500 (0.2513)  loss_scale: 32768.0000 (46386.1923)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1641  max mem: 15572
Epoch: [15]  [1330/2809]  eta: 0:13:57  lr: 0.000037  min_lr: 0.000000  loss: 4.3664 (4.3435)  class_acc: 0.2500 (0.2516)  loss_scale: 65536.0000 (46530.0676)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0777  max mem: 15572
[2025-01-15 21:50:26,533] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43471
[2025-01-15 21:50:26,533] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:50:26,533] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [1340/2809]  eta: 0:13:51  lr: 0.000037  min_lr: 0.000000  loss: 4.3117 (4.3435)  class_acc: 0.2083 (0.2516)  loss_scale: 65536.0000 (46549.6197)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1117  max mem: 15572
Epoch: [15]  [1350/2809]  eta: 0:13:47  lr: 0.000037  min_lr: 0.000000  loss: 4.3404 (4.3427)  class_acc: 0.2083 (0.2520)  loss_scale: 32768.0000 (46447.6092)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1645  max mem: 15572
Epoch: [15]  [1360/2809]  eta: 0:13:42  lr: 0.000037  min_lr: 0.000000  loss: 4.2717 (4.3423)  class_acc: 0.2500 (0.2517)  loss_scale: 32768.0000 (46347.0977)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.1671  max mem: 15572
Epoch: [15]  [1370/2809]  eta: 0:13:35  lr: 0.000037  min_lr: 0.000000  loss: 4.3616 (4.3428)  class_acc: 0.1667 (0.2515)  loss_scale: 32768.0000 (46248.0525)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0660  max mem: 15572
Epoch: [15]  [1380/2809]  eta: 0:13:30  lr: 0.000037  min_lr: 0.000000  loss: 4.4036 (4.3434)  class_acc: 0.2083 (0.2516)  loss_scale: 32768.0000 (46150.4417)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0772  max mem: 15572
Epoch: [15]  [1390/2809]  eta: 0:13:24  lr: 0.000037  min_lr: 0.000000  loss: 4.4036 (4.3439)  class_acc: 0.2083 (0.2513)  loss_scale: 32768.0000 (46054.2344)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1349  max mem: 15572
Epoch: [15]  [1400/2809]  eta: 0:13:18  lr: 0.000037  min_lr: 0.000000  loss: 4.3084 (4.3431)  class_acc: 0.2083 (0.2515)  loss_scale: 32768.0000 (45959.4004)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0997  max mem: 15572
Epoch: [15]  [1410/2809]  eta: 0:13:12  lr: 0.000037  min_lr: 0.000000  loss: 4.2420 (4.3422)  class_acc: 0.2083 (0.2515)  loss_scale: 32768.0000 (45865.9107)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0803  max mem: 15572
Epoch: [15]  [1420/2809]  eta: 0:13:06  lr: 0.000037  min_lr: 0.000000  loss: 4.3199 (4.3436)  class_acc: 0.2083 (0.2510)  loss_scale: 32768.0000 (45773.7368)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0925  max mem: 15572
Epoch: [15]  [1430/2809]  eta: 0:13:00  lr: 0.000037  min_lr: 0.000000  loss: 4.3813 (4.3434)  class_acc: 0.2500 (0.2514)  loss_scale: 32768.0000 (45682.8512)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1209  max mem: 15572
Epoch: [15]  [1440/2809]  eta: 0:12:56  lr: 0.000037  min_lr: 0.000000  loss: 4.3616 (4.3432)  class_acc: 0.2500 (0.2516)  loss_scale: 32768.0000 (45593.2269)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1695  max mem: 15572
Epoch: [15]  [1450/2809]  eta: 0:12:49  lr: 0.000037  min_lr: 0.000000  loss: 4.4087 (4.3430)  class_acc: 0.2500 (0.2517)  loss_scale: 32768.0000 (45504.8380)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1132  max mem: 15572
Epoch: [15]  [1460/2809]  eta: 0:12:43  lr: 0.000037  min_lr: 0.000000  loss: 4.3586 (4.3438)  class_acc: 0.2500 (0.2519)  loss_scale: 32768.0000 (45417.6591)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0788  max mem: 15572
[2025-01-15 21:51:38,299] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:51:38,299] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [1470/2809]  eta: 0:12:36  lr: 0.000037  min_lr: 0.000000  loss: 4.3364 (4.3441)  class_acc: 0.2500 (0.2520)  loss_scale: 32768.0000 (45465.3215)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0709  max mem: 15572
Epoch: [15]  [1480/2809]  eta: 0:12:31  lr: 0.000037  min_lr: 0.000000  loss: 4.3315 (4.3436)  class_acc: 0.2083 (0.2520)  loss_scale: 65536.0000 (45600.8427)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0902  max mem: 15572
Epoch: [15]  [1490/2809]  eta: 0:12:25  lr: 0.000037  min_lr: 0.000000  loss: 4.2393 (4.3422)  class_acc: 0.2500 (0.2525)  loss_scale: 65536.0000 (45734.5459)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1219  max mem: 15572
Epoch: [15]  [1500/2809]  eta: 0:12:20  lr: 0.000037  min_lr: 0.000000  loss: 4.2833 (4.3428)  class_acc: 0.2500 (0.2522)  loss_scale: 65536.0000 (45866.4677)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1124  max mem: 15572
Epoch: [15]  [1510/2809]  eta: 0:12:13  lr: 0.000037  min_lr: 0.000000  loss: 4.4391 (4.3424)  class_acc: 0.2083 (0.2526)  loss_scale: 65536.0000 (45996.6433)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.1005  max mem: 15572
Epoch: [15]  [1520/2809]  eta: 0:12:08  lr: 0.000037  min_lr: 0.000000  loss: 4.4391 (4.3433)  class_acc: 0.2917 (0.2526)  loss_scale: 65536.0000 (46125.1072)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0922  max mem: 15572
Epoch: [15]  [1530/2809]  eta: 0:12:03  lr: 0.000037  min_lr: 0.000000  loss: 4.4104 (4.3434)  class_acc: 0.2500 (0.2526)  loss_scale: 65536.0000 (46251.8929)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1463  max mem: 15572
Epoch: [15]  [1540/2809]  eta: 0:11:57  lr: 0.000037  min_lr: 0.000000  loss: 4.4250 (4.3440)  class_acc: 0.2083 (0.2523)  loss_scale: 65536.0000 (46377.0331)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1261  max mem: 15572
Epoch: [15]  [1550/2809]  eta: 0:11:51  lr: 0.000037  min_lr: 0.000000  loss: 4.4485 (4.3444)  class_acc: 0.1667 (0.2519)  loss_scale: 65536.0000 (46500.5596)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0706  max mem: 15572
Epoch: [15]  [1560/2809]  eta: 0:11:46  lr: 0.000037  min_lr: 0.000000  loss: 4.4207 (4.3453)  class_acc: 0.1667 (0.2516)  loss_scale: 65536.0000 (46622.5035)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1692  max mem: 15572
Epoch: [15]  [1570/2809]  eta: 0:11:40  lr: 0.000037  min_lr: 0.000000  loss: 4.3723 (4.3450)  class_acc: 0.2083 (0.2518)  loss_scale: 65536.0000 (46742.8950)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1723  max mem: 15572
Epoch: [15]  [1580/2809]  eta: 0:11:35  lr: 0.000037  min_lr: 0.000000  loss: 4.3082 (4.3448)  class_acc: 0.2500 (0.2518)  loss_scale: 65536.0000 (46861.7634)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1138  max mem: 15572
Epoch: [15]  [1590/2809]  eta: 0:11:30  lr: 0.000037  min_lr: 0.000000  loss: 4.3679 (4.3453)  class_acc: 0.2500 (0.2522)  loss_scale: 65536.0000 (46979.1376)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1783  max mem: 15572
[2025-01-15 21:52:51,689] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:52:51,689] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:52:52,837] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43729
[2025-01-15 21:52:52,838] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:52:52,838] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1600/2809]  eta: 0:11:24  lr: 0.000037  min_lr: 0.000000  loss: 4.3483 (4.3448)  class_acc: 0.2917 (0.2524)  loss_scale: 65536.0000 (47135.9800)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1285  max mem: 15572
[2025-01-15 21:53:00,898] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43745
[2025-01-15 21:53:00,899] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:53:00,899] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [1610/2809]  eta: 0:11:18  lr: 0.000037  min_lr: 0.000000  loss: 4.3863 (4.3456)  class_acc: 0.2500 (0.2525)  loss_scale: 65536.0000 (47229.8547)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0761  max mem: 15572
Epoch: [15]  [1620/2809]  eta: 0:11:13  lr: 0.000037  min_lr: 0.000000  loss: 4.3987 (4.3457)  class_acc: 0.2500 (0.2525)  loss_scale: 32768.0000 (47140.6391)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1329  max mem: 15572
Epoch: [15]  [1630/2809]  eta: 0:11:07  lr: 0.000037  min_lr: 0.000000  loss: 4.3148 (4.3456)  class_acc: 0.2500 (0.2528)  loss_scale: 32768.0000 (47052.5175)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1176  max mem: 15572
Epoch: [15]  [1640/2809]  eta: 0:11:02  lr: 0.000037  min_lr: 0.000000  loss: 4.3656 (4.3460)  class_acc: 0.2500 (0.2526)  loss_scale: 32768.0000 (46965.4698)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1178  max mem: 15572
Epoch: [15]  [1650/2809]  eta: 0:10:57  lr: 0.000037  min_lr: 0.000000  loss: 4.4107 (4.3460)  class_acc: 0.2083 (0.2525)  loss_scale: 32768.0000 (46879.4767)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.1865  max mem: 15572
Epoch: [15]  [1660/2809]  eta: 0:10:50  lr: 0.000037  min_lr: 0.000000  loss: 4.3107 (4.3455)  class_acc: 0.2500 (0.2527)  loss_scale: 32768.0000 (46794.5190)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0964  max mem: 15572
Epoch: [15]  [1670/2809]  eta: 0:10:44  lr: 0.000037  min_lr: 0.000000  loss: 4.2959 (4.3448)  class_acc: 0.2500 (0.2528)  loss_scale: 32768.0000 (46710.5781)  weight_decay: 0.0500 (0.0500)  time: 0.4765  data: 0.0400  max mem: 15572
Epoch: [15]  [1680/2809]  eta: 0:10:38  lr: 0.000037  min_lr: 0.000000  loss: 4.2959 (4.3451)  class_acc: 0.2500 (0.2527)  loss_scale: 32768.0000 (46627.6359)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0853  max mem: 15572
Epoch: [15]  [1690/2809]  eta: 0:10:32  lr: 0.000037  min_lr: 0.000000  loss: 4.3823 (4.3456)  class_acc: 0.1667 (0.2525)  loss_scale: 32768.0000 (46545.6747)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0705  max mem: 15572
Epoch: [15]  [1700/2809]  eta: 0:10:27  lr: 0.000037  min_lr: 0.000000  loss: 4.3161 (4.3451)  class_acc: 0.2083 (0.2527)  loss_scale: 32768.0000 (46464.6772)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1237  max mem: 15572
Epoch: [15]  [1710/2809]  eta: 0:10:21  lr: 0.000037  min_lr: 0.000000  loss: 4.3389 (4.3451)  class_acc: 0.2083 (0.2524)  loss_scale: 32768.0000 (46384.6265)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1471  max mem: 15572
Epoch: [15]  [1720/2809]  eta: 0:10:16  lr: 0.000037  min_lr: 0.000000  loss: 4.3218 (4.3448)  class_acc: 0.2083 (0.2524)  loss_scale: 32768.0000 (46305.5061)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1666  max mem: 15572
Epoch: [15]  [1730/2809]  eta: 0:10:10  lr: 0.000037  min_lr: 0.000000  loss: 4.1863 (4.3439)  class_acc: 0.2500 (0.2528)  loss_scale: 32768.0000 (46227.2998)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1445  max mem: 15572
[2025-01-15 21:54:14,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:54:14,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [1740/2809]  eta: 0:10:05  lr: 0.000037  min_lr: 0.000000  loss: 4.3152 (4.3449)  class_acc: 0.2917 (0.2530)  loss_scale: 32768.0000 (46187.6347)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0773  max mem: 15572
Epoch: [15]  [1750/2809]  eta: 0:09:59  lr: 0.000037  min_lr: 0.000000  loss: 4.3556 (4.3446)  class_acc: 0.2917 (0.2531)  loss_scale: 65536.0000 (46298.1336)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1367  max mem: 15572
Epoch: [15]  [1760/2809]  eta: 0:09:53  lr: 0.000037  min_lr: 0.000000  loss: 4.1668 (4.3433)  class_acc: 0.2917 (0.2534)  loss_scale: 65536.0000 (46407.3776)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.1015  max mem: 15572
Epoch: [15]  [1770/2809]  eta: 0:09:48  lr: 0.000037  min_lr: 0.000000  loss: 4.2431 (4.3438)  class_acc: 0.2917 (0.2533)  loss_scale: 65536.0000 (46515.3879)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1198  max mem: 15572
Epoch: [15]  [1780/2809]  eta: 0:09:43  lr: 0.000037  min_lr: 0.000000  loss: 4.4931 (4.3445)  class_acc: 0.2083 (0.2530)  loss_scale: 65536.0000 (46622.1853)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.1889  max mem: 15572
Epoch: [15]  [1790/2809]  eta: 0:09:37  lr: 0.000037  min_lr: 0.000000  loss: 4.3071 (4.3441)  class_acc: 0.2500 (0.2529)  loss_scale: 65536.0000 (46727.7901)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1224  max mem: 15572
Epoch: [15]  [1800/2809]  eta: 0:09:31  lr: 0.000037  min_lr: 0.000000  loss: 4.3504 (4.3445)  class_acc: 0.2083 (0.2527)  loss_scale: 65536.0000 (46832.2221)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0660  max mem: 15572
[2025-01-15 21:54:51,566] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43938
[2025-01-15 21:54:51,567] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:54:51,567] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [1810/2809]  eta: 0:09:25  lr: 0.000037  min_lr: 0.000000  loss: 4.3791 (4.3448)  class_acc: 0.2083 (0.2528)  loss_scale: 65536.0000 (46790.7499)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0957  max mem: 15572
Epoch: [15]  [1820/2809]  eta: 0:09:19  lr: 0.000037  min_lr: 0.000000  loss: 4.3507 (4.3445)  class_acc: 0.2500 (0.2528)  loss_scale: 32768.0000 (46713.7441)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1097  max mem: 15572
Epoch: [15]  [1830/2809]  eta: 0:09:14  lr: 0.000037  min_lr: 0.000000  loss: 4.3221 (4.3444)  class_acc: 0.2500 (0.2528)  loss_scale: 32768.0000 (46637.5795)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1024  max mem: 15572
Epoch: [15]  [1840/2809]  eta: 0:09:08  lr: 0.000037  min_lr: 0.000000  loss: 4.4316 (4.3451)  class_acc: 0.2500 (0.2530)  loss_scale: 32768.0000 (46562.2423)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1347  max mem: 15572
Epoch: [15]  [1850/2809]  eta: 0:09:02  lr: 0.000037  min_lr: 0.000000  loss: 4.4590 (4.3452)  class_acc: 0.2917 (0.2530)  loss_scale: 32768.0000 (46487.7191)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1000  max mem: 15572
Epoch: [15]  [1860/2809]  eta: 0:08:57  lr: 0.000037  min_lr: 0.000000  loss: 4.4248 (4.3459)  class_acc: 0.2083 (0.2528)  loss_scale: 32768.0000 (46413.9968)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1292  max mem: 15572
[2025-01-15 21:55:25,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=44000, skipped=274, lr=[3.5794456282147766e-07, 3.5794456282147766e-07, 5.113493754592539e-07, 5.113493754592539e-07, 7.304991077989342e-07, 7.304991077989342e-07, 1.0435701539984775e-06, 1.0435701539984775e-06, 1.4908145057121107e-06, 1.4908145057121107e-06, 2.1297350081601583e-06, 2.1297350081601583e-06, 3.0424785830859404e-06, 3.0424785830859404e-06, 4.346397975837058e-06, 4.346397975837058e-06, 6.2091399654815116e-06, 6.2091399654815116e-06, 8.870199950687875e-06, 8.870199950687875e-06, 1.2671714215268393e-05, 1.2671714215268393e-05, 1.8102448878954847e-05, 1.8102448878954847e-05, 2.5860641255649787e-05, 2.5860641255649787e-05, 3.694377322235684e-05, 3.694377322235684e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:55:25,832] [INFO] [timer.py:260:stop] epoch=0/micro_step=44000/global_step=44000, RunningAvgSamplesPerSec=27.658336623580816, CurrSamplesPerSec=27.681779407696414, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [15]  [1870/2809]  eta: 0:08:51  lr: 0.000037  min_lr: 0.000000  loss: 4.4157 (4.3457)  class_acc: 0.2083 (0.2526)  loss_scale: 32768.0000 (46341.0625)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1477  max mem: 15572
Epoch: [15]  [1880/2809]  eta: 0:08:45  lr: 0.000037  min_lr: 0.000000  loss: 4.3312 (4.3453)  class_acc: 0.2500 (0.2529)  loss_scale: 32768.0000 (46268.9038)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0933  max mem: 15572
Epoch: [15]  [1890/2809]  eta: 0:08:39  lr: 0.000037  min_lr: 0.000000  loss: 4.2063 (4.3449)  class_acc: 0.2500 (0.2529)  loss_scale: 32768.0000 (46197.5082)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0802  max mem: 15572
Epoch: [15]  [1900/2809]  eta: 0:08:34  lr: 0.000037  min_lr: 0.000000  loss: 4.3047 (4.3450)  class_acc: 0.2083 (0.2529)  loss_scale: 32768.0000 (46126.8638)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1084  max mem: 15572
Epoch: [15]  [1910/2809]  eta: 0:08:28  lr: 0.000037  min_lr: 0.000000  loss: 4.4112 (4.3453)  class_acc: 0.2083 (0.2528)  loss_scale: 32768.0000 (46056.9587)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1562  max mem: 15572
Epoch: [15]  [1920/2809]  eta: 0:08:23  lr: 0.000037  min_lr: 0.000000  loss: 4.4395 (4.3459)  class_acc: 0.2083 (0.2526)  loss_scale: 32768.0000 (45987.7814)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1610  max mem: 15572
Epoch: [15]  [1930/2809]  eta: 0:08:17  lr: 0.000037  min_lr: 0.000000  loss: 4.4395 (4.3461)  class_acc: 0.2083 (0.2524)  loss_scale: 32768.0000 (45919.3206)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.0992  max mem: 15572
[2025-01-15 21:56:03,856] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:56:03,857] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [1940/2809]  eta: 0:08:11  lr: 0.000037  min_lr: 0.000000  loss: 4.3612 (4.3465)  class_acc: 0.2083 (0.2523)  loss_scale: 32768.0000 (46003.5033)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0790  max mem: 15572
Epoch: [15]  [1950/2809]  eta: 0:08:05  lr: 0.000037  min_lr: 0.000000  loss: 4.3321 (4.3464)  class_acc: 0.2083 (0.2522)  loss_scale: 65536.0000 (46103.6187)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1037  max mem: 15572
Epoch: [15]  [1960/2809]  eta: 0:08:00  lr: 0.000037  min_lr: 0.000000  loss: 4.2921 (4.3461)  class_acc: 0.2500 (0.2523)  loss_scale: 65536.0000 (46202.7129)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1047  max mem: 15572
Epoch: [15]  [1970/2809]  eta: 0:07:55  lr: 0.000037  min_lr: 0.000000  loss: 4.4134 (4.3467)  class_acc: 0.2500 (0.2523)  loss_scale: 65536.0000 (46300.8016)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.1937  max mem: 15572
Epoch: [15]  [1980/2809]  eta: 0:07:49  lr: 0.000037  min_lr: 0.000000  loss: 4.4573 (4.3473)  class_acc: 0.2500 (0.2522)  loss_scale: 65536.0000 (46397.9001)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.2163  max mem: 15572
Epoch: [15]  [1990/2809]  eta: 0:07:44  lr: 0.000037  min_lr: 0.000000  loss: 4.3407 (4.3476)  class_acc: 0.2500 (0.2523)  loss_scale: 65536.0000 (46494.0231)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1550  max mem: 15572
[2025-01-15 21:56:43,029] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44135
[2025-01-15 21:56:43,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:56:43,029] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [2000/2809]  eta: 0:07:38  lr: 0.000037  min_lr: 0.000000  loss: 4.3514 (4.3472)  class_acc: 0.2500 (0.2525)  loss_scale: 65536.0000 (46572.8096)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.0745  max mem: 15572
Epoch: [15]  [2010/2809]  eta: 0:07:32  lr: 0.000037  min_lr: 0.000000  loss: 4.3967 (4.3473)  class_acc: 0.3333 (0.2529)  loss_scale: 32768.0000 (46504.1631)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0518  max mem: 15572
Epoch: [15]  [2020/2809]  eta: 0:07:26  lr: 0.000037  min_lr: 0.000000  loss: 4.4782 (4.3475)  class_acc: 0.2917 (0.2531)  loss_scale: 32768.0000 (46436.1959)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0995  max mem: 15572
Epoch: [15]  [2030/2809]  eta: 0:07:21  lr: 0.000037  min_lr: 0.000000  loss: 4.4782 (4.3479)  class_acc: 0.2500 (0.2530)  loss_scale: 32768.0000 (46368.8981)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0927  max mem: 15572
Epoch: [15]  [2040/2809]  eta: 0:07:15  lr: 0.000037  min_lr: 0.000000  loss: 4.4216 (4.3481)  class_acc: 0.2083 (0.2528)  loss_scale: 32768.0000 (46302.2597)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0891  max mem: 15572
Epoch: [15]  [2050/2809]  eta: 0:07:09  lr: 0.000037  min_lr: 0.000000  loss: 4.2855 (4.3479)  class_acc: 0.1667 (0.2527)  loss_scale: 32768.0000 (46236.2711)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1292  max mem: 15572
Epoch: [15]  [2060/2809]  eta: 0:07:04  lr: 0.000037  min_lr: 0.000000  loss: 4.2337 (4.3483)  class_acc: 0.1667 (0.2524)  loss_scale: 32768.0000 (46170.9229)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1499  max mem: 15572
Epoch: [15]  [2070/2809]  eta: 0:06:58  lr: 0.000037  min_lr: 0.000000  loss: 4.2966 (4.3480)  class_acc: 0.1667 (0.2524)  loss_scale: 32768.0000 (46106.2057)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1181  max mem: 15572
Epoch: [15]  [2080/2809]  eta: 0:06:52  lr: 0.000037  min_lr: 0.000000  loss: 4.2962 (4.3477)  class_acc: 0.2500 (0.2526)  loss_scale: 32768.0000 (46042.1105)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0852  max mem: 15572
Epoch: [15]  [2090/2809]  eta: 0:06:46  lr: 0.000037  min_lr: 0.000000  loss: 4.4420 (4.3482)  class_acc: 0.2500 (0.2524)  loss_scale: 32768.0000 (45978.6284)  weight_decay: 0.0500 (0.0500)  time: 0.4739  data: 0.0406  max mem: 15572
Epoch: [15]  [2100/2809]  eta: 0:06:41  lr: 0.000037  min_lr: 0.000000  loss: 4.3578 (4.3478)  class_acc: 0.1667 (0.2522)  loss_scale: 32768.0000 (45915.7506)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.1139  max mem: 15572
Epoch: [15]  [2110/2809]  eta: 0:06:35  lr: 0.000037  min_lr: 0.000000  loss: 4.2621 (4.3473)  class_acc: 0.2500 (0.2523)  loss_scale: 32768.0000 (45853.4685)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1697  max mem: 15572
Epoch: [15]  [2120/2809]  eta: 0:06:29  lr: 0.000037  min_lr: 0.000000  loss: 4.2924 (4.3475)  class_acc: 0.2500 (0.2524)  loss_scale: 32768.0000 (45791.7737)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.1085  max mem: 15572
[2025-01-15 21:57:53,150] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:57:53,151] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [2130/2809]  eta: 0:06:23  lr: 0.000037  min_lr: 0.000000  loss: 4.3373 (4.3470)  class_acc: 0.2500 (0.2526)  loss_scale: 32768.0000 (45761.4115)  weight_decay: 0.0500 (0.0500)  time: 0.4923  data: 0.0696  max mem: 15572
Epoch: [15]  [2140/2809]  eta: 0:06:18  lr: 0.000037  min_lr: 0.000000  loss: 4.3073 (4.3469)  class_acc: 0.2500 (0.2526)  loss_scale: 65536.0000 (45853.7730)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0811  max mem: 15572
[2025-01-15 21:58:02,035] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44278
[2025-01-15 21:58:02,036] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:58:02,036] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [2150/2809]  eta: 0:06:12  lr: 0.000037  min_lr: 0.000000  loss: 4.3073 (4.3468)  class_acc: 0.2500 (0.2527)  loss_scale: 65536.0000 (45823.4049)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1324  max mem: 15572
Epoch: [15]  [2160/2809]  eta: 0:06:06  lr: 0.000037  min_lr: 0.000000  loss: 4.3194 (4.3468)  class_acc: 0.2917 (0.2528)  loss_scale: 32768.0000 (45762.9912)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1083  max mem: 15572
Epoch: [15]  [2170/2809]  eta: 0:06:01  lr: 0.000037  min_lr: 0.000000  loss: 4.3958 (4.3470)  class_acc: 0.2083 (0.2526)  loss_scale: 32768.0000 (45703.1340)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0797  max mem: 15572
Epoch: [15]  [2180/2809]  eta: 0:05:55  lr: 0.000037  min_lr: 0.000000  loss: 4.3939 (4.3469)  class_acc: 0.2083 (0.2527)  loss_scale: 32768.0000 (45643.8258)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0974  max mem: 15572
Epoch: [15]  [2190/2809]  eta: 0:05:49  lr: 0.000037  min_lr: 0.000000  loss: 4.3752 (4.3466)  class_acc: 0.2500 (0.2529)  loss_scale: 32768.0000 (45585.0589)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1010  max mem: 15572
Epoch: [15]  [2200/2809]  eta: 0:05:44  lr: 0.000037  min_lr: 0.000000  loss: 4.3063 (4.3466)  class_acc: 0.2917 (0.2528)  loss_scale: 32768.0000 (45526.8260)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1080  max mem: 15572
[2025-01-15 21:58:34,602] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44338
[2025-01-15 21:58:34,602] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-15 21:58:34,602] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [2210/2809]  eta: 0:05:38  lr: 0.000037  min_lr: 0.000000  loss: 4.3514 (4.3462)  class_acc: 0.2500 (0.2529)  loss_scale: 32768.0000 (45409.8381)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0929  max mem: 15572
Epoch: [15]  [2220/2809]  eta: 0:05:32  lr: 0.000037  min_lr: 0.000000  loss: 4.4167 (4.3466)  class_acc: 0.2083 (0.2527)  loss_scale: 16384.0000 (45279.1499)  weight_decay: 0.0500 (0.0500)  time: 0.4761  data: 0.0369  max mem: 15572
Epoch: [15]  [2230/2809]  eta: 0:05:26  lr: 0.000037  min_lr: 0.000000  loss: 4.4645 (4.3470)  class_acc: 0.2083 (0.2526)  loss_scale: 16384.0000 (45149.6333)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1063  max mem: 15572
Epoch: [15]  [2240/2809]  eta: 0:05:21  lr: 0.000037  min_lr: 0.000000  loss: 4.3933 (4.3468)  class_acc: 0.2500 (0.2526)  loss_scale: 16384.0000 (45021.2726)  weight_decay: 0.0500 (0.0500)  time: 0.6520  data: 0.1892  max mem: 15572
Epoch: [15]  [2250/2809]  eta: 0:05:15  lr: 0.000037  min_lr: 0.000000  loss: 4.4133 (4.3472)  class_acc: 0.2500 (0.2525)  loss_scale: 16384.0000 (44894.0524)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1326  max mem: 15572
Epoch: [15]  [2260/2809]  eta: 0:05:10  lr: 0.000037  min_lr: 0.000000  loss: 4.4133 (4.3469)  class_acc: 0.1667 (0.2522)  loss_scale: 16384.0000 (44767.9575)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1458  max mem: 15572
Epoch: [15]  [2270/2809]  eta: 0:05:04  lr: 0.000037  min_lr: 0.000000  loss: 4.3449 (4.3467)  class_acc: 0.1667 (0.2521)  loss_scale: 16384.0000 (44642.9731)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1687  max mem: 15572
Epoch: [15]  [2280/2809]  eta: 0:04:58  lr: 0.000037  min_lr: 0.000000  loss: 4.3291 (4.3464)  class_acc: 0.2500 (0.2522)  loss_scale: 16384.0000 (44519.0846)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0965  max mem: 15572
Epoch: [15]  [2290/2809]  eta: 0:04:53  lr: 0.000037  min_lr: 0.000000  loss: 4.3291 (4.3463)  class_acc: 0.2500 (0.2522)  loss_scale: 16384.0000 (44396.2776)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0870  max mem: 15572
Epoch: [15]  [2300/2809]  eta: 0:04:47  lr: 0.000037  min_lr: 0.000000  loss: 4.2982 (4.3463)  class_acc: 0.2083 (0.2520)  loss_scale: 16384.0000 (44274.5380)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1396  max mem: 15572
Epoch: [15]  [2310/2809]  eta: 0:04:41  lr: 0.000037  min_lr: 0.000000  loss: 4.2982 (4.3461)  class_acc: 0.2500 (0.2522)  loss_scale: 16384.0000 (44153.8520)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1438  max mem: 15572
Epoch: [15]  [2320/2809]  eta: 0:04:36  lr: 0.000037  min_lr: 0.000000  loss: 4.2733 (4.3459)  class_acc: 0.3333 (0.2525)  loss_scale: 16384.0000 (44034.2059)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1534  max mem: 15572
Epoch: [15]  [2330/2809]  eta: 0:04:30  lr: 0.000037  min_lr: 0.000000  loss: 4.3735 (4.3458)  class_acc: 0.2917 (0.2525)  loss_scale: 16384.0000 (43915.5864)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1452  max mem: 15572
[2025-01-15 21:59:49,515] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:59:49,515] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [2340/2809]  eta: 0:04:25  lr: 0.000037  min_lr: 0.000000  loss: 4.3428 (4.3456)  class_acc: 0.2083 (0.2523)  loss_scale: 16384.0000 (43860.9688)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1112  max mem: 15572
Epoch: [15]  [2350/2809]  eta: 0:04:19  lr: 0.000037  min_lr: 0.000000  loss: 4.2782 (4.3450)  class_acc: 0.2083 (0.2522)  loss_scale: 32768.0000 (43813.7848)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0868  max mem: 15572
Epoch: [15]  [2360/2809]  eta: 0:04:13  lr: 0.000037  min_lr: 0.000000  loss: 4.2890 (4.3453)  class_acc: 0.2500 (0.2524)  loss_scale: 32768.0000 (43767.0004)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0822  max mem: 15572
Epoch: [15]  [2370/2809]  eta: 0:04:08  lr: 0.000037  min_lr: 0.000000  loss: 4.3221 (4.3453)  class_acc: 0.2500 (0.2523)  loss_scale: 32768.0000 (43720.6107)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1052  max mem: 15572
Epoch: [15]  [2380/2809]  eta: 0:04:02  lr: 0.000037  min_lr: 0.000000  loss: 4.3158 (4.3449)  class_acc: 0.2083 (0.2524)  loss_scale: 32768.0000 (43674.6107)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0632  max mem: 15572
Epoch: [15]  [2390/2809]  eta: 0:03:56  lr: 0.000037  min_lr: 0.000000  loss: 4.2925 (4.3444)  class_acc: 0.2500 (0.2524)  loss_scale: 32768.0000 (43628.9954)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0564  max mem: 15572
Epoch: [15]  [2400/2809]  eta: 0:03:51  lr: 0.000037  min_lr: 0.000000  loss: 4.2446 (4.3444)  class_acc: 0.2500 (0.2526)  loss_scale: 32768.0000 (43583.7601)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1065  max mem: 15572
Epoch: [15]  [2410/2809]  eta: 0:03:45  lr: 0.000037  min_lr: 0.000000  loss: 4.4499 (4.3448)  class_acc: 0.2083 (0.2524)  loss_scale: 32768.0000 (43538.9000)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1691  max mem: 15572
Epoch: [15]  [2420/2809]  eta: 0:03:40  lr: 0.000037  min_lr: 0.000000  loss: 4.4066 (4.3443)  class_acc: 0.1667 (0.2523)  loss_scale: 32768.0000 (43494.4106)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1603  max mem: 15572
Epoch: [15]  [2430/2809]  eta: 0:03:34  lr: 0.000037  min_lr: 0.000000  loss: 4.3716 (4.3443)  class_acc: 0.2083 (0.2522)  loss_scale: 32768.0000 (43450.2871)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1146  max mem: 15572
Epoch: [15]  [2440/2809]  eta: 0:03:28  lr: 0.000037  min_lr: 0.000000  loss: 4.3279 (4.3442)  class_acc: 0.2500 (0.2523)  loss_scale: 32768.0000 (43406.5252)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0800  max mem: 15572
Epoch: [15]  [2450/2809]  eta: 0:03:23  lr: 0.000037  min_lr: 0.000000  loss: 4.2591 (4.3442)  class_acc: 0.2500 (0.2522)  loss_scale: 32768.0000 (43363.1204)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0711  max mem: 15572
[2025-01-15 22:01:01,868] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:01:01,869] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [2460/2809]  eta: 0:03:17  lr: 0.000037  min_lr: 0.000000  loss: 4.4017 (4.3445)  class_acc: 0.2500 (0.2521)  loss_scale: 32768.0000 (43333.3832)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1046  max mem: 15572
Epoch: [15]  [2470/2809]  eta: 0:03:11  lr: 0.000037  min_lr: 0.000000  loss: 4.3926 (4.3446)  class_acc: 0.2500 (0.2522)  loss_scale: 65536.0000 (43423.2359)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0993  max mem: 15572
[2025-01-15 22:01:13,869] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44615
[2025-01-15 22:01:13,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:01:13,870] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [2480/2809]  eta: 0:03:06  lr: 0.000037  min_lr: 0.000000  loss: 4.3208 (4.3447)  class_acc: 0.2917 (0.2523)  loss_scale: 65536.0000 (43499.1568)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0976  max mem: 15572
Epoch: [15]  [2490/2809]  eta: 0:03:00  lr: 0.000037  min_lr: 0.000000  loss: 4.3242 (4.3445)  class_acc: 0.2917 (0.2524)  loss_scale: 32768.0000 (43456.0771)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1070  max mem: 15572
Epoch: [15]  [2500/2809]  eta: 0:02:54  lr: 0.000037  min_lr: 0.000000  loss: 4.3851 (4.3445)  class_acc: 0.2500 (0.2524)  loss_scale: 32768.0000 (43413.3419)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0578  max mem: 15572
Epoch: [15]  [2510/2809]  eta: 0:02:49  lr: 0.000037  min_lr: 0.000000  loss: 4.3682 (4.3443)  class_acc: 0.2083 (0.2522)  loss_scale: 32768.0000 (43370.9470)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0161  max mem: 15572
Epoch: [15]  [2520/2809]  eta: 0:02:43  lr: 0.000037  min_lr: 0.000000  loss: 4.4640 (4.3449)  class_acc: 0.2083 (0.2522)  loss_scale: 32768.0000 (43328.8885)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0827  max mem: 15572
Epoch: [15]  [2530/2809]  eta: 0:02:37  lr: 0.000037  min_lr: 0.000000  loss: 4.3604 (4.3445)  class_acc: 0.2083 (0.2522)  loss_scale: 32768.0000 (43287.1624)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0673  max mem: 15572
Epoch: [15]  [2540/2809]  eta: 0:02:32  lr: 0.000037  min_lr: 0.000000  loss: 4.1126 (4.3435)  class_acc: 0.2917 (0.2524)  loss_scale: 32768.0000 (43245.7647)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0317  max mem: 15572
Epoch: [15]  [2550/2809]  eta: 0:02:26  lr: 0.000037  min_lr: 0.000000  loss: 4.1850 (4.3436)  class_acc: 0.2917 (0.2523)  loss_scale: 32768.0000 (43204.6915)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0466  max mem: 15572
Epoch: [15]  [2560/2809]  eta: 0:02:20  lr: 0.000037  min_lr: 0.000000  loss: 4.3078 (4.3434)  class_acc: 0.2917 (0.2524)  loss_scale: 32768.0000 (43163.9391)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0970  max mem: 15572
Epoch: [15]  [2570/2809]  eta: 0:02:15  lr: 0.000037  min_lr: 0.000000  loss: 4.2945 (4.3435)  class_acc: 0.2500 (0.2524)  loss_scale: 32768.0000 (43123.5037)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0823  max mem: 15572
Epoch: [15]  [2580/2809]  eta: 0:02:09  lr: 0.000037  min_lr: 0.000000  loss: 4.4038 (4.3441)  class_acc: 0.2500 (0.2526)  loss_scale: 32768.0000 (43083.3816)  weight_decay: 0.0500 (0.0500)  time: 0.5008  data: 0.0009  max mem: 15572
Epoch: [15]  [2590/2809]  eta: 0:02:03  lr: 0.000036  min_lr: 0.000000  loss: 4.4342 (4.3443)  class_acc: 0.2500 (0.2526)  loss_scale: 32768.0000 (43043.5693)  weight_decay: 0.0500 (0.0500)  time: 0.4797  data: 0.0007  max mem: 15572
Epoch: [15]  [2600/2809]  eta: 0:01:57  lr: 0.000036  min_lr: 0.000000  loss: 4.3936 (4.3446)  class_acc: 0.2083 (0.2527)  loss_scale: 32768.0000 (43004.0631)  weight_decay: 0.0500 (0.0500)  time: 0.4849  data: 0.0304  max mem: 15572
[2025-01-15 22:02:23,089] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:02:23,089] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [2610/2809]  eta: 0:01:52  lr: 0.000036  min_lr: 0.000000  loss: 4.3797 (4.3446)  class_acc: 0.2500 (0.2529)  loss_scale: 32768.0000 (42989.9594)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0955  max mem: 15572
Epoch: [15]  [2620/2809]  eta: 0:01:46  lr: 0.000036  min_lr: 0.000000  loss: 4.3514 (4.3444)  class_acc: 0.2500 (0.2530)  loss_scale: 65536.0000 (43075.9802)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1043  max mem: 15572
Epoch: [15]  [2630/2809]  eta: 0:01:41  lr: 0.000036  min_lr: 0.000000  loss: 4.4016 (4.3447)  class_acc: 0.2500 (0.2530)  loss_scale: 65536.0000 (43161.3470)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1540  max mem: 15572
[2025-01-15 22:02:40,370] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44773
[2025-01-15 22:02:40,370] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:02:40,370] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [2640/2809]  eta: 0:01:35  lr: 0.000036  min_lr: 0.000000  loss: 4.3540 (4.3448)  class_acc: 0.2500 (0.2530)  loss_scale: 65536.0000 (43208.8451)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1682  max mem: 15572
Epoch: [15]  [2650/2809]  eta: 0:01:29  lr: 0.000036  min_lr: 0.000000  loss: 4.3035 (4.3447)  class_acc: 0.2500 (0.2530)  loss_scale: 32768.0000 (43169.4606)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1248  max mem: 15572
Epoch: [15]  [2660/2809]  eta: 0:01:24  lr: 0.000036  min_lr: 0.000000  loss: 4.3405 (4.3448)  class_acc: 0.2500 (0.2528)  loss_scale: 32768.0000 (43130.3720)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1789  max mem: 15572
Epoch: [15]  [2670/2809]  eta: 0:01:18  lr: 0.000036  min_lr: 0.000000  loss: 4.3405 (4.3447)  class_acc: 0.2083 (0.2527)  loss_scale: 32768.0000 (43091.5762)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1550  max mem: 15572
Epoch: [15]  [2680/2809]  eta: 0:01:12  lr: 0.000036  min_lr: 0.000000  loss: 4.3899 (4.3449)  class_acc: 0.2500 (0.2530)  loss_scale: 32768.0000 (43053.0698)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1140  max mem: 15572
Epoch: [15]  [2690/2809]  eta: 0:01:07  lr: 0.000036  min_lr: 0.000000  loss: 4.3210 (4.3444)  class_acc: 0.2917 (0.2532)  loss_scale: 32768.0000 (43014.8495)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.1053  max mem: 15572
Epoch: [15]  [2700/2809]  eta: 0:01:01  lr: 0.000036  min_lr: 0.000000  loss: 4.3128 (4.3446)  class_acc: 0.2500 (0.2530)  loss_scale: 32768.0000 (42976.9123)  weight_decay: 0.0500 (0.0500)  time: 0.4653  data: 0.0602  max mem: 15572
Epoch: [15]  [2710/2809]  eta: 0:00:55  lr: 0.000036  min_lr: 0.000000  loss: 4.2478 (4.3441)  class_acc: 0.2500 (0.2532)  loss_scale: 32768.0000 (42939.2549)  weight_decay: 0.0500 (0.0500)  time: 0.4501  data: 0.0215  max mem: 15572
Epoch: [15]  [2720/2809]  eta: 0:00:50  lr: 0.000036  min_lr: 0.000000  loss: 4.2090 (4.3436)  class_acc: 0.2083 (0.2530)  loss_scale: 32768.0000 (42901.8743)  weight_decay: 0.0500 (0.0500)  time: 0.4624  data: 0.0007  max mem: 15572
Epoch: [15]  [2730/2809]  eta: 0:00:44  lr: 0.000036  min_lr: 0.000000  loss: 4.1344 (4.3431)  class_acc: 0.2083 (0.2532)  loss_scale: 32768.0000 (42864.7675)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0440  max mem: 15572
Epoch: [15]  [2740/2809]  eta: 0:00:38  lr: 0.000036  min_lr: 0.000000  loss: 4.1806 (4.3429)  class_acc: 0.2917 (0.2533)  loss_scale: 32768.0000 (42827.9314)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1528  max mem: 15572
Epoch: [15]  [2750/2809]  eta: 0:00:33  lr: 0.000036  min_lr: 0.000000  loss: 4.2801 (4.3430)  class_acc: 0.2500 (0.2534)  loss_scale: 32768.0000 (42791.3631)  weight_decay: 0.0500 (0.0500)  time: 0.6569  data: 0.1935  max mem: 15572
Epoch: [15]  [2760/2809]  eta: 0:00:27  lr: 0.000036  min_lr: 0.000000  loss: 4.2744 (4.3425)  class_acc: 0.2500 (0.2536)  loss_scale: 32768.0000 (42755.0598)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1806  max mem: 15572
[2025-01-15 22:03:52,557] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:03:52,558] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [2770/2809]  eta: 0:00:22  lr: 0.000036  min_lr: 0.000000  loss: 4.2388 (4.3425)  class_acc: 0.2500 (0.2536)  loss_scale: 32768.0000 (42766.3197)  weight_decay: 0.0500 (0.0500)  time: 0.6862  data: 0.2051  max mem: 15572
Epoch: [15]  [2780/2809]  eta: 0:00:16  lr: 0.000036  min_lr: 0.000000  loss: 4.3729 (4.3425)  class_acc: 0.2917 (0.2539)  loss_scale: 65536.0000 (42848.1956)  weight_decay: 0.0500 (0.0500)  time: 0.6755  data: 0.1726  max mem: 15572
Epoch: [15]  [2790/2809]  eta: 0:00:10  lr: 0.000036  min_lr: 0.000000  loss: 4.3591 (4.3424)  class_acc: 0.3333 (0.2540)  loss_scale: 65536.0000 (42929.4848)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1511  max mem: 15572
Epoch: [15]  [2800/2809]  eta: 0:00:05  lr: 0.000036  min_lr: 0.000000  loss: 4.3591 (4.3423)  class_acc: 0.2500 (0.2539)  loss_scale: 65536.0000 (43010.1935)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1572  max mem: 15572
Epoch: [15]  [2808/2809]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000000  loss: 4.3234 (4.3419)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (43074.3467)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1209  max mem: 15572
Epoch: [15] Total time: 0:26:30 (0.5661 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000000  loss: 4.3234 (4.3419)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (43074.3467)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:28:29  loss: 1.0996 (1.0996)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.2836  data: 6.1017  max mem: 15572
Val:  [ 10/272]  eta: 0:03:40  loss: 3.0259 (2.9605)  acc1: 33.3333 (34.8485)  acc5: 61.1111 (64.1414)  time: 0.8426  data: 0.6680  max mem: 15572
Val:  [ 20/272]  eta: 0:02:31  loss: 3.0259 (2.9839)  acc1: 33.3333 (34.1270)  acc5: 61.1111 (64.8148)  time: 0.3181  data: 0.1358  max mem: 15572
Val:  [ 30/272]  eta: 0:02:09  loss: 3.0221 (2.9855)  acc1: 33.3333 (34.0502)  acc5: 61.1111 (66.3082)  time: 0.3665  data: 0.1802  max mem: 15572
Val:  [ 40/272]  eta: 0:01:52  loss: 2.9232 (2.9764)  acc1: 27.7778 (32.5203)  acc5: 72.2222 (67.2087)  time: 0.3598  data: 0.1825  max mem: 15572
Val:  [ 50/272]  eta: 0:01:33  loss: 2.8261 (2.8993)  acc1: 33.3333 (35.0763)  acc5: 72.2222 (69.2810)  time: 0.2489  data: 0.0760  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 2.1685 (2.8089)  acc1: 55.5556 (38.4335)  acc5: 77.7778 (70.8561)  time: 0.1791  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:01:11  loss: 2.2886 (2.7492)  acc1: 55.5556 (40.2191)  acc5: 83.3333 (72.1440)  time: 0.1743  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:01:04  loss: 2.4988 (2.7591)  acc1: 38.8889 (39.9177)  acc5: 77.7778 (71.7421)  time: 0.1830  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:58  loss: 3.2812 (2.8158)  acc1: 27.7778 (38.7668)  acc5: 61.1111 (70.1465)  time: 0.1976  data: 0.0007  max mem: 15572
Val:  [100/272]  eta: 0:00:53  loss: 3.1494 (2.8489)  acc1: 33.3333 (38.6689)  acc5: 61.1111 (69.4169)  time: 0.2124  data: 0.0228  max mem: 15572
Val:  [110/272]  eta: 0:00:50  loss: 3.0907 (2.8968)  acc1: 16.6667 (36.7868)  acc5: 66.6667 (68.4685)  time: 0.2778  data: 0.0875  max mem: 15572
Val:  [120/272]  eta: 0:00:47  loss: 3.1408 (2.9295)  acc1: 16.6667 (35.8127)  acc5: 55.5556 (67.5390)  time: 0.3407  data: 0.1420  max mem: 15572
Val:  [130/272]  eta: 0:00:44  loss: 2.8622 (2.8949)  acc1: 33.3333 (37.0653)  acc5: 66.6667 (68.2782)  time: 0.3218  data: 0.1234  max mem: 15572
Val:  [140/272]  eta: 0:00:40  loss: 2.5114 (2.8897)  acc1: 44.4444 (37.6675)  acc5: 72.2222 (68.2427)  time: 0.2795  data: 0.0915  max mem: 15572
Val:  [150/272]  eta: 0:00:38  loss: 2.7715 (2.8821)  acc1: 27.7778 (37.1597)  acc5: 72.2222 (68.3959)  time: 0.3133  data: 0.1196  max mem: 15572
Val:  [160/272]  eta: 0:00:35  loss: 2.7686 (2.8748)  acc1: 33.3333 (37.7502)  acc5: 77.7778 (68.8751)  time: 0.3382  data: 0.1487  max mem: 15572
Val:  [170/272]  eta: 0:00:32  loss: 2.9095 (2.8950)  acc1: 38.8889 (37.1345)  acc5: 72.2222 (68.4535)  time: 0.3401  data: 0.1568  max mem: 15572
Val:  [180/272]  eta: 0:00:28  loss: 2.9095 (2.8750)  acc1: 33.3333 (37.1393)  acc5: 66.6667 (69.0608)  time: 0.3222  data: 0.1228  max mem: 15572
Val:  [190/272]  eta: 0:00:25  loss: 2.6372 (2.9027)  acc1: 27.7778 (36.1838)  acc5: 66.6667 (68.0047)  time: 0.2923  data: 0.0950  max mem: 15572
Val:  [200/272]  eta: 0:00:22  loss: 2.7966 (2.9024)  acc1: 33.3333 (36.2908)  acc5: 66.6667 (67.9657)  time: 0.2823  data: 0.0973  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.5263 (2.9008)  acc1: 44.4444 (36.7825)  acc5: 77.7778 (68.0885)  time: 0.3217  data: 0.1333  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.8143 (2.8978)  acc1: 38.8889 (36.9030)  acc5: 72.2222 (68.2252)  time: 0.3128  data: 0.1169  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.5283 (2.8843)  acc1: 50.0000 (37.8307)  acc5: 77.7778 (68.6869)  time: 0.3205  data: 0.1310  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.4092 (2.8684)  acc1: 66.6667 (38.3587)  acc5: 83.3333 (69.2946)  time: 0.3686  data: 0.1826  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.6807 (2.8765)  acc1: 27.7778 (37.7158)  acc5: 77.7778 (69.3006)  time: 0.3093  data: 0.1245  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.1808 (2.8298)  acc1: 72.2222 (39.5700)  acc5: 83.3333 (70.1788)  time: 0.2571  data: 0.0770  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.1590 (2.8249)  acc1: 61.1111 (39.6064)  acc5: 88.8889 (70.3157)  time: 0.1973  data: 0.0304  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.1590 (2.8281)  acc1: 61.1111 (39.6068)  acc5: 88.8889 (70.2847)  time: 0.1910  data: 0.0304  max mem: 15572
Val: Total time: 0:01:23 (0.3064 s / it)
* Acc@1 39.607 Acc@5 70.285 loss 2.828
Accuracy of the network on the 4883 val videos: 39.6%
[2025-01-15 22:05:42,020] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 22:05:42,021] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 22:05:42,021] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 22:05:44,955] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 22:05:44,955] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 39.61%
[2025-01-15 22:05:52,148] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44944
[2025-01-15 22:05:52,148] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:05:52,148] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [   0/2809]  eta: 5:36:46  lr: 0.000036  min_lr: 0.000000  loss: 4.1165 (4.1165)  class_acc: 0.5833 (0.5833)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.1936  data: 6.7571  max mem: 15572
Epoch: [16]  [  10/2809]  eta: 0:56:18  lr: 0.000036  min_lr: 0.000000  loss: 4.3625 (4.3535)  class_acc: 0.2083 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2070  data: 0.7764  max mem: 15572
Epoch: [16]  [  20/2809]  eta: 0:39:57  lr: 0.000036  min_lr: 0.000000  loss: 4.3625 (4.3275)  class_acc: 0.2083 (0.2857)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.1147  max mem: 15572
Epoch: [16]  [  30/2809]  eta: 0:34:46  lr: 0.000036  min_lr: 0.000000  loss: 4.3144 (4.3095)  class_acc: 0.2083 (0.2594)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4997  data: 0.0697  max mem: 15572
Epoch: [16]  [  40/2809]  eta: 0:33:42  lr: 0.000036  min_lr: 0.000000  loss: 4.3082 (4.3280)  class_acc: 0.2083 (0.2581)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1586  max mem: 15572
Epoch: [16]  [  50/2809]  eta: 0:32:23  lr: 0.000036  min_lr: 0.000000  loss: 4.3846 (4.3164)  class_acc: 0.2500 (0.2663)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.1849  max mem: 15572
[2025-01-15 22:06:23,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=45000, skipped=280, lr=[3.519523463149651e-07, 3.519523463149651e-07, 5.027890661642359e-07, 5.027890661642359e-07, 7.18270094520337e-07, 7.18270094520337e-07, 1.026100135029053e-06, 1.026100135029053e-06, 1.46585733575579e-06, 1.46585733575579e-06, 2.094081908222557e-06, 2.094081908222557e-06, 2.991545583175082e-06, 2.991545583175082e-06, 4.273636547392975e-06, 4.273636547392975e-06, 6.105195067704249e-06, 6.105195067704249e-06, 8.7217072395775e-06, 8.7217072395775e-06, 1.2459581770825e-05, 1.2459581770825e-05, 1.779940252975e-05, 1.779940252975e-05, 2.5427717899642862e-05, 2.5427717899642862e-05, 3.632531128520409e-05, 3.632531128520409e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:06:23,019] [INFO] [timer.py:260:stop] epoch=0/micro_step=45000/global_step=45000, RunningAvgSamplesPerSec=27.662584553604653, CurrSamplesPerSec=29.985283683189905, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [  60/2809]  eta: 0:31:00  lr: 0.000036  min_lr: 0.000000  loss: 4.3651 (4.3314)  class_acc: 0.2917 (0.2698)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1150  max mem: 15572
Epoch: [16]  [  70/2809]  eta: 0:30:36  lr: 0.000036  min_lr: 0.000000  loss: 4.3839 (4.3421)  class_acc: 0.2500 (0.2647)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1368  max mem: 15572
Epoch: [16]  [  80/2809]  eta: 0:29:54  lr: 0.000036  min_lr: 0.000000  loss: 4.2534 (4.3315)  class_acc: 0.2500 (0.2629)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1554  max mem: 15572
Epoch: [16]  [  90/2809]  eta: 0:29:40  lr: 0.000036  min_lr: 0.000000  loss: 4.2735 (4.3385)  class_acc: 0.2500 (0.2642)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1579  max mem: 15572
Epoch: [16]  [ 100/2809]  eta: 0:29:24  lr: 0.000036  min_lr: 0.000000  loss: 4.3156 (4.3386)  class_acc: 0.2500 (0.2624)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1814  max mem: 15572
Epoch: [16]  [ 110/2809]  eta: 0:28:54  lr: 0.000036  min_lr: 0.000000  loss: 4.2781 (4.3361)  class_acc: 0.2500 (0.2639)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1363  max mem: 15572
Epoch: [16]  [ 120/2809]  eta: 0:28:27  lr: 0.000036  min_lr: 0.000000  loss: 4.3482 (4.3393)  class_acc: 0.2917 (0.2676)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0981  max mem: 15572
[2025-01-15 22:07:06,079] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:07:06,080] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [ 130/2809]  eta: 0:28:11  lr: 0.000036  min_lr: 0.000000  loss: 4.4359 (4.3426)  class_acc: 0.2917 (0.2675)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1059  max mem: 15572
Epoch: [16]  [ 140/2809]  eta: 0:27:55  lr: 0.000036  min_lr: 0.000000  loss: 4.3114 (4.3418)  class_acc: 0.2083 (0.2648)  loss_scale: 65536.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1228  max mem: 15572
Epoch: [16]  [ 150/2809]  eta: 0:27:33  lr: 0.000036  min_lr: 0.000000  loss: 4.3114 (4.3436)  class_acc: 0.2083 (0.2663)  loss_scale: 65536.0000 (37542.1457)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1149  max mem: 15572
Epoch: [16]  [ 160/2809]  eta: 0:27:20  lr: 0.000036  min_lr: 0.000000  loss: 4.3111 (4.3416)  class_acc: 0.2917 (0.2676)  loss_scale: 65536.0000 (39280.8944)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1178  max mem: 15572
Epoch: [16]  [ 170/2809]  eta: 0:27:01  lr: 0.000036  min_lr: 0.000000  loss: 4.3391 (4.3496)  class_acc: 0.2500 (0.2658)  loss_scale: 65536.0000 (40816.2807)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1041  max mem: 15572
Epoch: [16]  [ 180/2809]  eta: 0:26:46  lr: 0.000036  min_lr: 0.000000  loss: 4.4510 (4.3514)  class_acc: 0.1667 (0.2627)  loss_scale: 65536.0000 (42182.0110)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0843  max mem: 15572
Epoch: [16]  [ 190/2809]  eta: 0:26:30  lr: 0.000036  min_lr: 0.000000  loss: 4.3593 (4.3496)  class_acc: 0.1667 (0.2607)  loss_scale: 65536.0000 (43404.7330)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0903  max mem: 15572
[2025-01-15 22:07:43,980] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45141
[2025-01-15 22:07:43,980] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:07:43,981] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [ 200/2809]  eta: 0:26:29  lr: 0.000036  min_lr: 0.000000  loss: 4.3728 (4.3515)  class_acc: 0.2083 (0.2585)  loss_scale: 65536.0000 (43853.6915)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1497  max mem: 15572
Epoch: [16]  [ 210/2809]  eta: 0:26:19  lr: 0.000036  min_lr: 0.000000  loss: 4.3958 (4.3545)  class_acc: 0.2083 (0.2573)  loss_scale: 32768.0000 (43328.3033)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1693  max mem: 15572
Epoch: [16]  [ 220/2809]  eta: 0:26:05  lr: 0.000036  min_lr: 0.000000  loss: 4.3630 (4.3521)  class_acc: 0.2083 (0.2589)  loss_scale: 32768.0000 (42850.4615)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1082  max mem: 15572
Epoch: [16]  [ 230/2809]  eta: 0:25:58  lr: 0.000036  min_lr: 0.000000  loss: 4.3530 (4.3557)  class_acc: 0.1667 (0.2570)  loss_scale: 32768.0000 (42413.9913)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1261  max mem: 15572
Epoch: [16]  [ 240/2809]  eta: 0:25:45  lr: 0.000036  min_lr: 0.000000  loss: 4.4388 (4.3601)  class_acc: 0.1667 (0.2554)  loss_scale: 32768.0000 (42013.7427)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1136  max mem: 15572
Epoch: [16]  [ 250/2809]  eta: 0:25:40  lr: 0.000036  min_lr: 0.000000  loss: 4.3855 (4.3582)  class_acc: 0.2083 (0.2568)  loss_scale: 32768.0000 (41645.3865)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1116  max mem: 15572
Epoch: [16]  [ 260/2809]  eta: 0:25:36  lr: 0.000036  min_lr: 0.000000  loss: 4.2704 (4.3561)  class_acc: 0.2500 (0.2565)  loss_scale: 32768.0000 (41305.2567)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.1697  max mem: 15572
Epoch: [16]  [ 270/2809]  eta: 0:25:25  lr: 0.000036  min_lr: 0.000000  loss: 4.3433 (4.3583)  class_acc: 0.2083 (0.2562)  loss_scale: 32768.0000 (40990.2288)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1415  max mem: 15572
Epoch: [16]  [ 280/2809]  eta: 0:25:12  lr: 0.000036  min_lr: 0.000000  loss: 4.3496 (4.3581)  class_acc: 0.2083 (0.2549)  loss_scale: 32768.0000 (40697.6228)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0811  max mem: 15572
Epoch: [16]  [ 290/2809]  eta: 0:24:55  lr: 0.000036  min_lr: 0.000000  loss: 4.3094 (4.3540)  class_acc: 0.2083 (0.2534)  loss_scale: 32768.0000 (40425.1271)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0368  max mem: 15572
Epoch: [16]  [ 300/2809]  eta: 0:24:48  lr: 0.000036  min_lr: 0.000000  loss: 4.2516 (4.3502)  class_acc: 0.3333 (0.2568)  loss_scale: 32768.0000 (40170.7375)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0763  max mem: 15572
Epoch: [16]  [ 310/2809]  eta: 0:24:50  lr: 0.000036  min_lr: 0.000000  loss: 4.2273 (4.3472)  class_acc: 0.3333 (0.2576)  loss_scale: 32768.0000 (39932.7074)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.1993  max mem: 15572
Epoch: [16]  [ 320/2809]  eta: 0:24:41  lr: 0.000036  min_lr: 0.000000  loss: 4.3163 (4.3497)  class_acc: 0.2917 (0.2575)  loss_scale: 32768.0000 (39709.5078)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1806  max mem: 15572
[2025-01-15 22:08:58,779] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:08:58,779] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [ 330/2809]  eta: 0:24:31  lr: 0.000036  min_lr: 0.000000  loss: 4.3591 (4.3480)  class_acc: 0.2500 (0.2592)  loss_scale: 32768.0000 (39994.7795)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1017  max mem: 15572
[2025-01-15 22:09:04,664] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45281
[2025-01-15 22:09:04,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:09:04,664] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [ 340/2809]  eta: 0:24:24  lr: 0.000036  min_lr: 0.000000  loss: 4.3533 (4.3486)  class_acc: 0.2500 (0.2587)  loss_scale: 65536.0000 (40359.4135)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1113  max mem: 15572
Epoch: [16]  [ 350/2809]  eta: 0:24:12  lr: 0.000036  min_lr: 0.000000  loss: 4.4569 (4.3519)  class_acc: 0.2083 (0.2569)  loss_scale: 32768.0000 (40143.1339)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1013  max mem: 15572
Epoch: [16]  [ 360/2809]  eta: 0:24:10  lr: 0.000036  min_lr: 0.000000  loss: 4.5111 (4.3559)  class_acc: 0.1667 (0.2550)  loss_scale: 32768.0000 (39938.8366)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1274  max mem: 15572
Epoch: [16]  [ 370/2809]  eta: 0:24:05  lr: 0.000036  min_lr: 0.000000  loss: 4.5111 (4.3578)  class_acc: 0.1667 (0.2540)  loss_scale: 32768.0000 (39745.5526)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1720  max mem: 15572
Epoch: [16]  [ 380/2809]  eta: 0:23:55  lr: 0.000036  min_lr: 0.000000  loss: 4.1761 (4.3522)  class_acc: 0.2083 (0.2535)  loss_scale: 32768.0000 (39562.4147)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1278  max mem: 15572
Epoch: [16]  [ 390/2809]  eta: 0:23:53  lr: 0.000036  min_lr: 0.000000  loss: 4.2596 (4.3520)  class_acc: 0.2500 (0.2534)  loss_scale: 32768.0000 (39388.6445)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1474  max mem: 15572
Epoch: [16]  [ 400/2809]  eta: 0:23:44  lr: 0.000036  min_lr: 0.000000  loss: 4.4226 (4.3551)  class_acc: 0.2500 (0.2539)  loss_scale: 32768.0000 (39223.5411)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1505  max mem: 15572
Epoch: [16]  [ 410/2809]  eta: 0:23:40  lr: 0.000036  min_lr: 0.000000  loss: 4.3896 (4.3508)  class_acc: 0.2917 (0.2553)  loss_scale: 32768.0000 (39066.4720)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1402  max mem: 15572
Epoch: [16]  [ 420/2809]  eta: 0:23:31  lr: 0.000036  min_lr: 0.000000  loss: 4.2354 (4.3508)  class_acc: 0.2917 (0.2559)  loss_scale: 32768.0000 (38916.8646)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1447  max mem: 15572
Epoch: [16]  [ 430/2809]  eta: 0:23:25  lr: 0.000036  min_lr: 0.000000  loss: 4.2354 (4.3499)  class_acc: 0.2500 (0.2572)  loss_scale: 32768.0000 (38774.1995)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1166  max mem: 15572
Epoch: [16]  [ 440/2809]  eta: 0:23:21  lr: 0.000036  min_lr: 0.000000  loss: 4.2167 (4.3472)  class_acc: 0.2500 (0.2569)  loss_scale: 32768.0000 (38638.0045)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1587  max mem: 15572
Epoch: [16]  [ 450/2809]  eta: 0:23:13  lr: 0.000036  min_lr: 0.000000  loss: 4.4332 (4.3508)  class_acc: 0.2500 (0.2566)  loss_scale: 32768.0000 (38507.8492)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1519  max mem: 15572
Epoch: [16]  [ 460/2809]  eta: 0:23:02  lr: 0.000036  min_lr: 0.000000  loss: 4.5002 (4.3532)  class_acc: 0.2500 (0.2562)  loss_scale: 32768.0000 (38383.3406)  weight_decay: 0.0500 (0.0500)  time: 0.5223  data: 0.0692  max mem: 15572
[2025-01-15 22:10:18,993] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:10:18,994] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [ 470/2809]  eta: 0:22:57  lr: 0.000036  min_lr: 0.000000  loss: 4.4327 (4.3512)  class_acc: 0.2500 (0.2573)  loss_scale: 32768.0000 (38611.9745)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0964  max mem: 15572
Epoch: [16]  [ 480/2809]  eta: 0:22:48  lr: 0.000036  min_lr: 0.000000  loss: 4.2516 (4.3509)  class_acc: 0.2500 (0.2573)  loss_scale: 65536.0000 (39171.7256)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1307  max mem: 15572
Epoch: [16]  [ 490/2809]  eta: 0:22:41  lr: 0.000036  min_lr: 0.000000  loss: 4.2124 (4.3473)  class_acc: 0.2500 (0.2570)  loss_scale: 65536.0000 (39708.6762)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1077  max mem: 15572
Epoch: [16]  [ 500/2809]  eta: 0:22:31  lr: 0.000036  min_lr: 0.000000  loss: 4.2264 (4.3482)  class_acc: 0.2500 (0.2578)  loss_scale: 65536.0000 (40224.1916)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0940  max mem: 15572
Epoch: [16]  [ 510/2809]  eta: 0:22:28  lr: 0.000036  min_lr: 0.000000  loss: 4.2360 (4.3454)  class_acc: 0.2500 (0.2585)  loss_scale: 65536.0000 (40719.5303)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1403  max mem: 15572
[2025-01-15 22:10:46,174] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45457
[2025-01-15 22:10:46,175] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:10:46,176] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [ 520/2809]  eta: 0:22:21  lr: 0.000036  min_lr: 0.000000  loss: 4.2586 (4.3457)  class_acc: 0.2500 (0.2597)  loss_scale: 65536.0000 (40692.6987)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1571  max mem: 15572
Epoch: [16]  [ 530/2809]  eta: 0:22:13  lr: 0.000036  min_lr: 0.000000  loss: 4.2586 (4.3420)  class_acc: 0.2917 (0.2600)  loss_scale: 32768.0000 (40543.4576)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0967  max mem: 15572
Epoch: [16]  [ 540/2809]  eta: 0:22:06  lr: 0.000036  min_lr: 0.000000  loss: 4.3302 (4.3429)  class_acc: 0.2083 (0.2587)  loss_scale: 32768.0000 (40399.7338)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1043  max mem: 15572
Epoch: [16]  [ 550/2809]  eta: 0:21:59  lr: 0.000036  min_lr: 0.000000  loss: 4.3525 (4.3411)  class_acc: 0.2083 (0.2586)  loss_scale: 32768.0000 (40261.2269)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1196  max mem: 15572
Epoch: [16]  [ 560/2809]  eta: 0:21:51  lr: 0.000036  min_lr: 0.000000  loss: 4.2948 (4.3393)  class_acc: 0.2500 (0.2595)  loss_scale: 32768.0000 (40127.6578)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0836  max mem: 15572
Epoch: [16]  [ 570/2809]  eta: 0:21:44  lr: 0.000036  min_lr: 0.000000  loss: 4.3015 (4.3389)  class_acc: 0.2083 (0.2587)  loss_scale: 32768.0000 (39998.7671)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0814  max mem: 15572
Epoch: [16]  [ 580/2809]  eta: 0:21:39  lr: 0.000036  min_lr: 0.000000  loss: 4.2927 (4.3394)  class_acc: 0.2083 (0.2593)  loss_scale: 32768.0000 (39874.3133)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1399  max mem: 15572
Epoch: [16]  [ 590/2809]  eta: 0:21:37  lr: 0.000036  min_lr: 0.000000  loss: 4.2659 (4.3340)  class_acc: 0.2917 (0.2606)  loss_scale: 32768.0000 (39754.0711)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.2012  max mem: 15572
Epoch: [16]  [ 600/2809]  eta: 0:21:27  lr: 0.000036  min_lr: 0.000000  loss: 4.2298 (4.3332)  class_acc: 0.2500 (0.2602)  loss_scale: 32768.0000 (39637.8303)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1427  max mem: 15572
Epoch: [16]  [ 610/2809]  eta: 0:21:19  lr: 0.000036  min_lr: 0.000000  loss: 4.2356 (4.3334)  class_acc: 0.2083 (0.2599)  loss_scale: 32768.0000 (39525.3944)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0539  max mem: 15572
Epoch: [16]  [ 620/2809]  eta: 0:21:15  lr: 0.000036  min_lr: 0.000000  loss: 4.4115 (4.3340)  class_acc: 0.2083 (0.2596)  loss_scale: 32768.0000 (39416.5797)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1241  max mem: 15572
Epoch: [16]  [ 630/2809]  eta: 0:21:07  lr: 0.000036  min_lr: 0.000000  loss: 4.3860 (4.3327)  class_acc: 0.2500 (0.2600)  loss_scale: 32768.0000 (39311.2139)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1251  max mem: 15572
Epoch: [16]  [ 640/2809]  eta: 0:21:00  lr: 0.000036  min_lr: 0.000000  loss: 4.2963 (4.3327)  class_acc: 0.2917 (0.2608)  loss_scale: 32768.0000 (39209.1357)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0994  max mem: 15572
[2025-01-15 22:11:58,904] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:11:58,905] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [ 650/2809]  eta: 0:20:54  lr: 0.000036  min_lr: 0.000000  loss: 4.3313 (4.3325)  class_acc: 0.2500 (0.2609)  loss_scale: 32768.0000 (39563.2074)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1037  max mem: 15572
Epoch: [16]  [ 660/2809]  eta: 0:20:48  lr: 0.000036  min_lr: 0.000000  loss: 4.3986 (4.3344)  class_acc: 0.2083 (0.2607)  loss_scale: 65536.0000 (39956.1392)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1060  max mem: 15572
Epoch: [16]  [ 670/2809]  eta: 0:20:38  lr: 0.000036  min_lr: 0.000000  loss: 4.4108 (4.3345)  class_acc: 0.2083 (0.2607)  loss_scale: 65536.0000 (40337.3592)  weight_decay: 0.0500 (0.0500)  time: 0.5052  data: 0.0673  max mem: 15572
Epoch: [16]  [ 680/2809]  eta: 0:20:33  lr: 0.000036  min_lr: 0.000000  loss: 4.2947 (4.3337)  class_acc: 0.2500 (0.2611)  loss_scale: 65536.0000 (40707.3833)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0768  max mem: 15572
Epoch: [16]  [ 690/2809]  eta: 0:20:25  lr: 0.000036  min_lr: 0.000000  loss: 4.3238 (4.3334)  class_acc: 0.2083 (0.2601)  loss_scale: 65536.0000 (41066.6975)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1111  max mem: 15572
Epoch: [16]  [ 700/2809]  eta: 0:20:21  lr: 0.000036  min_lr: 0.000000  loss: 4.1957 (4.3312)  class_acc: 0.2083 (0.2608)  loss_scale: 65536.0000 (41415.7603)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1243  max mem: 15572
Epoch: [16]  [ 710/2809]  eta: 0:20:17  lr: 0.000036  min_lr: 0.000000  loss: 4.2965 (4.3314)  class_acc: 0.3333 (0.2608)  loss_scale: 65536.0000 (41755.0042)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1810  max mem: 15572
Epoch: [16]  [ 720/2809]  eta: 0:20:10  lr: 0.000036  min_lr: 0.000000  loss: 4.4667 (4.3335)  class_acc: 0.2500 (0.2602)  loss_scale: 65536.0000 (42084.8377)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1454  max mem: 15572
Epoch: [16]  [ 730/2809]  eta: 0:20:05  lr: 0.000036  min_lr: 0.000000  loss: 4.4667 (4.3347)  class_acc: 0.1667 (0.2590)  loss_scale: 65536.0000 (42405.6471)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1368  max mem: 15572
Epoch: [16]  [ 740/2809]  eta: 0:19:57  lr: 0.000036  min_lr: 0.000000  loss: 4.4113 (4.3347)  class_acc: 0.1667 (0.2584)  loss_scale: 65536.0000 (42717.7976)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1323  max mem: 15572
Epoch: [16]  [ 750/2809]  eta: 0:19:53  lr: 0.000036  min_lr: 0.000000  loss: 4.3321 (4.3345)  class_acc: 0.2083 (0.2583)  loss_scale: 65536.0000 (43021.6352)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1381  max mem: 15572
Epoch: [16]  [ 760/2809]  eta: 0:19:48  lr: 0.000036  min_lr: 0.000000  loss: 4.2988 (4.3354)  class_acc: 0.2083 (0.2582)  loss_scale: 65536.0000 (43317.4875)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1725  max mem: 15572
[2025-01-15 22:13:11,098] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:13:11,099] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [ 770/2809]  eta: 0:19:39  lr: 0.000036  min_lr: 0.000000  loss: 4.2698 (4.3336)  class_acc: 0.2500 (0.2591)  loss_scale: 65536.0000 (43690.6667)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0944  max mem: 15572
[2025-01-15 22:13:11,518] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45715
[2025-01-15 22:13:11,519] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:13:11,519] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 780/2809]  eta: 0:19:32  lr: 0.000036  min_lr: 0.000000  loss: 4.2699 (4.3339)  class_acc: 0.2500 (0.2589)  loss_scale: 65536.0000 (43970.3764)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0702  max mem: 15572
Epoch: [16]  [ 790/2809]  eta: 0:19:25  lr: 0.000036  min_lr: 0.000000  loss: 4.3502 (4.3336)  class_acc: 0.2083 (0.2585)  loss_scale: 65536.0000 (44243.0139)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0974  max mem: 15572
[2025-01-15 22:13:22,513] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45736
[2025-01-15 22:13:22,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:13:22,513] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [ 800/2809]  eta: 0:19:17  lr: 0.000036  min_lr: 0.000000  loss: 4.4274 (4.3343)  class_acc: 0.2083 (0.2581)  loss_scale: 65536.0000 (44140.6642)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0811  max mem: 15572
Epoch: [16]  [ 810/2809]  eta: 0:19:11  lr: 0.000036  min_lr: 0.000000  loss: 4.3562 (4.3326)  class_acc: 0.2500 (0.2579)  loss_scale: 32768.0000 (44000.4340)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.1091  max mem: 15572
Epoch: [16]  [ 820/2809]  eta: 0:19:05  lr: 0.000036  min_lr: 0.000000  loss: 4.3439 (4.3323)  class_acc: 0.2500 (0.2577)  loss_scale: 32768.0000 (43863.6200)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1154  max mem: 15572
Epoch: [16]  [ 830/2809]  eta: 0:19:00  lr: 0.000036  min_lr: 0.000000  loss: 4.2424 (4.3307)  class_acc: 0.2500 (0.2579)  loss_scale: 32768.0000 (43730.0987)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1410  max mem: 15572
Epoch: [16]  [ 840/2809]  eta: 0:18:54  lr: 0.000036  min_lr: 0.000000  loss: 4.2525 (4.3310)  class_acc: 0.2500 (0.2579)  loss_scale: 32768.0000 (43599.7527)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1464  max mem: 15572
Epoch: [16]  [ 850/2809]  eta: 0:18:46  lr: 0.000036  min_lr: 0.000000  loss: 4.3134 (4.3312)  class_acc: 0.2500 (0.2581)  loss_scale: 32768.0000 (43472.4700)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0853  max mem: 15572
Epoch: [16]  [ 860/2809]  eta: 0:18:38  lr: 0.000036  min_lr: 0.000000  loss: 4.3134 (4.3296)  class_acc: 0.2500 (0.2580)  loss_scale: 32768.0000 (43348.1440)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0418  max mem: 15572
Epoch: [16]  [ 870/2809]  eta: 0:18:34  lr: 0.000036  min_lr: 0.000000  loss: 4.2188 (4.3293)  class_acc: 0.2500 (0.2582)  loss_scale: 32768.0000 (43226.6728)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1012  max mem: 15572
Epoch: [16]  [ 880/2809]  eta: 0:18:28  lr: 0.000036  min_lr: 0.000000  loss: 4.1004 (4.3281)  class_acc: 0.2500 (0.2582)  loss_scale: 32768.0000 (43107.9591)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1728  max mem: 15572
Epoch: [16]  [ 890/2809]  eta: 0:18:23  lr: 0.000036  min_lr: 0.000000  loss: 4.2281 (4.3292)  class_acc: 0.2500 (0.2579)  loss_scale: 32768.0000 (42991.9102)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1513  max mem: 15572
Epoch: [16]  [ 900/2809]  eta: 0:18:16  lr: 0.000036  min_lr: 0.000000  loss: 4.3508 (4.3285)  class_acc: 0.2083 (0.2577)  loss_scale: 32768.0000 (42878.4373)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1154  max mem: 15572
Epoch: [16]  [ 910/2809]  eta: 0:18:11  lr: 0.000036  min_lr: 0.000000  loss: 4.3228 (4.3287)  class_acc: 0.2083 (0.2576)  loss_scale: 32768.0000 (42767.4555)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1290  max mem: 15572
Epoch: [16]  [ 920/2809]  eta: 0:18:04  lr: 0.000036  min_lr: 0.000000  loss: 4.2829 (4.3269)  class_acc: 0.2500 (0.2584)  loss_scale: 32768.0000 (42658.8838)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1271  max mem: 15572
[2025-01-15 22:14:34,631] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:14:34,631] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [ 930/2809]  eta: 0:17:58  lr: 0.000036  min_lr: 0.000000  loss: 4.2002 (4.3252)  class_acc: 0.2917 (0.2587)  loss_scale: 32768.0000 (42904.6101)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0878  max mem: 15572
Epoch: [16]  [ 940/2809]  eta: 0:17:51  lr: 0.000036  min_lr: 0.000000  loss: 4.3307 (4.3258)  class_acc: 0.2500 (0.2587)  loss_scale: 65536.0000 (43145.1137)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0787  max mem: 15572
Epoch: [16]  [ 950/2809]  eta: 0:17:47  lr: 0.000036  min_lr: 0.000000  loss: 4.3523 (4.3257)  class_acc: 0.2500 (0.2589)  loss_scale: 65536.0000 (43380.5594)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1397  max mem: 15572
Epoch: [16]  [ 960/2809]  eta: 0:17:42  lr: 0.000036  min_lr: 0.000000  loss: 4.2357 (4.3247)  class_acc: 0.2500 (0.2595)  loss_scale: 65536.0000 (43611.1051)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1770  max mem: 15572
[2025-01-15 22:14:58,385] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45906
[2025-01-15 22:14:58,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:14:58,386] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [ 970/2809]  eta: 0:17:36  lr: 0.000036  min_lr: 0.000000  loss: 4.1792 (4.3223)  class_acc: 0.2500 (0.2595)  loss_scale: 65536.0000 (43533.1823)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1182  max mem: 15572
Epoch: [16]  [ 980/2809]  eta: 0:17:29  lr: 0.000036  min_lr: 0.000000  loss: 4.1810 (4.3231)  class_acc: 0.2083 (0.2586)  loss_scale: 32768.0000 (43423.4455)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0818  max mem: 15572
Epoch: [16]  [ 990/2809]  eta: 0:17:23  lr: 0.000036  min_lr: 0.000000  loss: 4.4828 (4.3249)  class_acc: 0.2083 (0.2588)  loss_scale: 32768.0000 (43315.9233)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0898  max mem: 15572
Epoch: [16]  [1000/2809]  eta: 0:17:18  lr: 0.000036  min_lr: 0.000000  loss: 4.4537 (4.3258)  class_acc: 0.2083 (0.2584)  loss_scale: 32768.0000 (43210.5495)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1183  max mem: 15572
Epoch: [16]  [1010/2809]  eta: 0:17:13  lr: 0.000036  min_lr: 0.000000  loss: 4.3873 (4.3257)  class_acc: 0.2083 (0.2582)  loss_scale: 32768.0000 (43107.2601)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1440  max mem: 15572
Epoch: [16]  [1020/2809]  eta: 0:17:06  lr: 0.000036  min_lr: 0.000000  loss: 4.2570 (4.3251)  class_acc: 0.2500 (0.2584)  loss_scale: 32768.0000 (43005.9941)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1126  max mem: 15572
Epoch: [16]  [1030/2809]  eta: 0:17:00  lr: 0.000036  min_lr: 0.000000  loss: 4.2570 (4.3265)  class_acc: 0.1667 (0.2576)  loss_scale: 32768.0000 (42906.6925)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0825  max mem: 15572
Epoch: [16]  [1040/2809]  eta: 0:16:54  lr: 0.000036  min_lr: 0.000000  loss: 4.4943 (4.3261)  class_acc: 0.1667 (0.2575)  loss_scale: 32768.0000 (42809.2988)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0971  max mem: 15572
Epoch: [16]  [1050/2809]  eta: 0:16:48  lr: 0.000036  min_lr: 0.000000  loss: 4.3542 (4.3266)  class_acc: 0.2500 (0.2572)  loss_scale: 32768.0000 (42713.7583)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1078  max mem: 15572
[2025-01-15 22:15:50,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=46000, skipped=286, lr=[3.458328714272749e-07, 3.458328714272749e-07, 4.940469591818214e-07, 4.940469591818214e-07, 7.057813702597449e-07, 7.057813702597449e-07, 1.0082591003710642e-06, 1.0082591003710642e-06, 1.4403701433872347e-06, 1.4403701433872347e-06, 2.057671633410335e-06, 2.057671633410335e-06, 2.9395309048719076e-06, 2.9395309048719076e-06, 4.199329864102725e-06, 4.199329864102725e-06, 5.999042663003894e-06, 5.999042663003894e-06, 8.570060947148422e-06, 8.570060947148422e-06, 1.2242944210212029e-05, 1.2242944210212029e-05, 1.7489920300302902e-05, 1.7489920300302902e-05, 2.4985600429004146e-05, 2.4985600429004146e-05, 3.5693714898577354e-05, 3.5693714898577354e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:15:50,253] [INFO] [timer.py:260:stop] epoch=0/micro_step=46000/global_step=46000, RunningAvgSamplesPerSec=27.678554946529474, CurrSamplesPerSec=29.383708426011875, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [1060/2809]  eta: 0:16:42  lr: 0.000036  min_lr: 0.000000  loss: 4.3542 (4.3266)  class_acc: 0.2500 (0.2573)  loss_scale: 32768.0000 (42620.0189)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1088  max mem: 15572
Epoch: [16]  [1070/2809]  eta: 0:16:34  lr: 0.000036  min_lr: 0.000000  loss: 4.3458 (4.3263)  class_acc: 0.2917 (0.2576)  loss_scale: 32768.0000 (42528.0299)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0608  max mem: 15572
Epoch: [16]  [1080/2809]  eta: 0:16:28  lr: 0.000036  min_lr: 0.000000  loss: 4.3458 (4.3264)  class_acc: 0.3333 (0.2582)  loss_scale: 32768.0000 (42437.7428)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0646  max mem: 15572
Epoch: [16]  [1090/2809]  eta: 0:16:23  lr: 0.000036  min_lr: 0.000000  loss: 4.3982 (4.3277)  class_acc: 0.2917 (0.2582)  loss_scale: 32768.0000 (42349.1109)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1133  max mem: 15572
[2025-01-15 22:16:09,810] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:16:09,811] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 22:16:14,079] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46041
[2025-01-15 22:16:14,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:16:14,080] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-15 22:16:15,346] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46044
[2025-01-15 22:16:15,346] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-15 22:16:15,346] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [1100/2809]  eta: 0:16:17  lr: 0.000036  min_lr: 0.000000  loss: 4.3855 (4.3267)  class_acc: 0.2500 (0.2584)  loss_scale: 32768.0000 (42425.7802)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1321  max mem: 15572
Epoch: [16]  [1110/2809]  eta: 0:16:11  lr: 0.000036  min_lr: 0.000000  loss: 4.2514 (4.3263)  class_acc: 0.2917 (0.2589)  loss_scale: 16384.0000 (42191.3807)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1144  max mem: 15572
Epoch: [16]  [1120/2809]  eta: 0:16:06  lr: 0.000036  min_lr: 0.000000  loss: 4.3086 (4.3267)  class_acc: 0.2917 (0.2593)  loss_scale: 16384.0000 (41961.1632)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1122  max mem: 15572
Epoch: [16]  [1130/2809]  eta: 0:15:59  lr: 0.000036  min_lr: 0.000000  loss: 4.3007 (4.3259)  class_acc: 0.2917 (0.2594)  loss_scale: 16384.0000 (41735.0168)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0954  max mem: 15572
Epoch: [16]  [1140/2809]  eta: 0:15:52  lr: 0.000036  min_lr: 0.000000  loss: 4.3822 (4.3268)  class_acc: 0.1667 (0.2586)  loss_scale: 16384.0000 (41512.8344)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0531  max mem: 15572
Epoch: [16]  [1150/2809]  eta: 0:15:48  lr: 0.000036  min_lr: 0.000000  loss: 4.3822 (4.3263)  class_acc: 0.1667 (0.2578)  loss_scale: 16384.0000 (41294.5126)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1325  max mem: 15572
Epoch: [16]  [1160/2809]  eta: 0:15:43  lr: 0.000036  min_lr: 0.000000  loss: 4.3239 (4.3267)  class_acc: 0.2083 (0.2580)  loss_scale: 16384.0000 (41079.9518)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.2000  max mem: 15572
Epoch: [16]  [1170/2809]  eta: 0:15:37  lr: 0.000036  min_lr: 0.000000  loss: 4.3087 (4.3270)  class_acc: 0.2917 (0.2581)  loss_scale: 16384.0000 (40869.0555)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1372  max mem: 15572
Epoch: [16]  [1180/2809]  eta: 0:15:30  lr: 0.000036  min_lr: 0.000000  loss: 4.5083 (4.3275)  class_acc: 0.2917 (0.2584)  loss_scale: 16384.0000 (40661.7307)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0935  max mem: 15572
Epoch: [16]  [1190/2809]  eta: 0:15:24  lr: 0.000036  min_lr: 0.000000  loss: 4.3768 (4.3272)  class_acc: 0.2500 (0.2587)  loss_scale: 16384.0000 (40457.8875)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1199  max mem: 15572
Epoch: [16]  [1200/2809]  eta: 0:15:18  lr: 0.000036  min_lr: 0.000000  loss: 4.2878 (4.3280)  class_acc: 0.2500 (0.2585)  loss_scale: 16384.0000 (40257.4388)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1297  max mem: 15572
Epoch: [16]  [1210/2809]  eta: 0:15:12  lr: 0.000036  min_lr: 0.000000  loss: 4.2626 (4.3273)  class_acc: 0.2917 (0.2594)  loss_scale: 16384.0000 (40060.3006)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0960  max mem: 15572
Epoch: [16]  [1220/2809]  eta: 0:15:08  lr: 0.000036  min_lr: 0.000000  loss: 4.1121 (4.3254)  class_acc: 0.2500 (0.2591)  loss_scale: 16384.0000 (39866.3915)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1875  max mem: 15572
[2025-01-15 22:17:27,610] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:17:27,610] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [1230/2809]  eta: 0:15:03  lr: 0.000036  min_lr: 0.000000  loss: 4.2449 (4.3265)  class_acc: 0.2500 (0.2589)  loss_scale: 16384.0000 (39702.2518)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.2200  max mem: 15572
Epoch: [16]  [1240/2809]  eta: 0:14:57  lr: 0.000036  min_lr: 0.000000  loss: 4.3755 (4.3267)  class_acc: 0.2917 (0.2591)  loss_scale: 32768.0000 (39646.3755)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1286  max mem: 15572
Epoch: [16]  [1250/2809]  eta: 0:14:52  lr: 0.000036  min_lr: 0.000000  loss: 4.4713 (4.3286)  class_acc: 0.2083 (0.2588)  loss_scale: 32768.0000 (39591.3925)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1307  max mem: 15572
Epoch: [16]  [1260/2809]  eta: 0:14:46  lr: 0.000036  min_lr: 0.000000  loss: 4.4358 (4.3284)  class_acc: 0.2083 (0.2592)  loss_scale: 32768.0000 (39537.2815)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1252  max mem: 15572
Epoch: [16]  [1270/2809]  eta: 0:14:40  lr: 0.000036  min_lr: 0.000000  loss: 4.2808 (4.3290)  class_acc: 0.2917 (0.2592)  loss_scale: 32768.0000 (39484.0220)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1288  max mem: 15572
Epoch: [16]  [1280/2809]  eta: 0:14:34  lr: 0.000036  min_lr: 0.000000  loss: 4.3786 (4.3287)  class_acc: 0.2917 (0.2592)  loss_scale: 32768.0000 (39431.5941)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1091  max mem: 15572
Epoch: [16]  [1290/2809]  eta: 0:14:27  lr: 0.000036  min_lr: 0.000000  loss: 4.3786 (4.3290)  class_acc: 0.2917 (0.2592)  loss_scale: 32768.0000 (39379.9783)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0798  max mem: 15572
Epoch: [16]  [1300/2809]  eta: 0:14:23  lr: 0.000036  min_lr: 0.000000  loss: 4.3397 (4.3290)  class_acc: 0.2500 (0.2591)  loss_scale: 32768.0000 (39329.1560)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1457  max mem: 15572
Epoch: [16]  [1310/2809]  eta: 0:14:17  lr: 0.000036  min_lr: 0.000000  loss: 4.3397 (4.3294)  class_acc: 0.2500 (0.2593)  loss_scale: 32768.0000 (39279.1091)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1637  max mem: 15572
Epoch: [16]  [1320/2809]  eta: 0:14:10  lr: 0.000036  min_lr: 0.000000  loss: 4.4224 (4.3304)  class_acc: 0.2500 (0.2594)  loss_scale: 32768.0000 (39229.8198)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0867  max mem: 15572
Epoch: [16]  [1330/2809]  eta: 0:14:04  lr: 0.000036  min_lr: 0.000000  loss: 4.3960 (4.3309)  class_acc: 0.2500 (0.2595)  loss_scale: 32768.0000 (39181.2712)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0518  max mem: 15572
Epoch: [16]  [1340/2809]  eta: 0:13:59  lr: 0.000036  min_lr: 0.000000  loss: 4.3733 (4.3314)  class_acc: 0.2500 (0.2594)  loss_scale: 32768.0000 (39133.4467)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0891  max mem: 15572
Epoch: [16]  [1350/2809]  eta: 0:13:53  lr: 0.000036  min_lr: 0.000000  loss: 4.3830 (4.3324)  class_acc: 0.2500 (0.2593)  loss_scale: 32768.0000 (39086.3301)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1146  max mem: 15572
[2025-01-15 22:18:40,890] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:18:40,890] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [1360/2809]  eta: 0:13:47  lr: 0.000035  min_lr: 0.000000  loss: 4.3483 (4.3316)  class_acc: 0.2500 (0.2592)  loss_scale: 32768.0000 (39136.2116)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1198  max mem: 15572
[2025-01-15 22:18:48,489] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46314
[2025-01-15 22:18:48,490] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:18:48,490] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1370/2809]  eta: 0:13:41  lr: 0.000035  min_lr: 0.000000  loss: 4.1813 (4.3304)  class_acc: 0.2083 (0.2590)  loss_scale: 65536.0000 (39304.8694)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1457  max mem: 15572
Epoch: [16]  [1380/2809]  eta: 0:13:36  lr: 0.000035  min_lr: 0.000000  loss: 4.2504 (4.3302)  class_acc: 0.2083 (0.2590)  loss_scale: 32768.0000 (39257.5351)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1372  max mem: 15572
Epoch: [16]  [1390/2809]  eta: 0:13:31  lr: 0.000035  min_lr: 0.000000  loss: 4.3434 (4.3307)  class_acc: 0.2083 (0.2587)  loss_scale: 32768.0000 (39210.8814)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1741  max mem: 15572
Epoch: [16]  [1400/2809]  eta: 0:13:25  lr: 0.000035  min_lr: 0.000000  loss: 4.3026 (4.3306)  class_acc: 0.2083 (0.2586)  loss_scale: 32768.0000 (39164.8936)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1685  max mem: 15572
Epoch: [16]  [1410/2809]  eta: 0:13:20  lr: 0.000035  min_lr: 0.000000  loss: 4.2937 (4.3304)  class_acc: 0.2083 (0.2582)  loss_scale: 32768.0000 (39119.5578)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1275  max mem: 15572
Epoch: [16]  [1420/2809]  eta: 0:13:15  lr: 0.000035  min_lr: 0.000000  loss: 4.2671 (4.3304)  class_acc: 0.2083 (0.2580)  loss_scale: 32768.0000 (39074.8600)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1526  max mem: 15572
Epoch: [16]  [1430/2809]  eta: 0:13:08  lr: 0.000035  min_lr: 0.000000  loss: 4.3307 (4.3312)  class_acc: 0.2083 (0.2576)  loss_scale: 32768.0000 (39030.7869)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0996  max mem: 15572
Epoch: [16]  [1440/2809]  eta: 0:13:02  lr: 0.000035  min_lr: 0.000000  loss: 4.3712 (4.3308)  class_acc: 0.2083 (0.2576)  loss_scale: 32768.0000 (38987.3255)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0510  max mem: 15572
Epoch: [16]  [1450/2809]  eta: 0:12:57  lr: 0.000035  min_lr: 0.000000  loss: 4.3241 (4.3308)  class_acc: 0.2500 (0.2574)  loss_scale: 32768.0000 (38944.4631)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1157  max mem: 15572
Epoch: [16]  [1460/2809]  eta: 0:12:50  lr: 0.000035  min_lr: 0.000000  loss: 4.3706 (4.3305)  class_acc: 0.2500 (0.2577)  loss_scale: 32768.0000 (38902.1875)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1109  max mem: 15572
Epoch: [16]  [1470/2809]  eta: 0:12:44  lr: 0.000035  min_lr: 0.000000  loss: 4.4222 (4.3309)  class_acc: 0.2500 (0.2577)  loss_scale: 32768.0000 (38860.4867)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0858  max mem: 15572
Epoch: [16]  [1480/2809]  eta: 0:12:38  lr: 0.000035  min_lr: 0.000000  loss: 4.3223 (4.3314)  class_acc: 0.2083 (0.2575)  loss_scale: 32768.0000 (38819.3491)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1210  max mem: 15572
Epoch: [16]  [1490/2809]  eta: 0:12:32  lr: 0.000035  min_lr: 0.000000  loss: 4.3370 (4.3313)  class_acc: 0.2083 (0.2578)  loss_scale: 32768.0000 (38778.7632)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0886  max mem: 15572
[2025-01-15 22:20:00,930] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:20:00,931] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [1500/2809]  eta: 0:12:26  lr: 0.000035  min_lr: 0.000000  loss: 4.2954 (4.3314)  class_acc: 0.3333 (0.2584)  loss_scale: 32768.0000 (38782.3797)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0591  max mem: 15572
Epoch: [16]  [1510/2809]  eta: 0:12:20  lr: 0.000035  min_lr: 0.000000  loss: 4.2954 (4.3316)  class_acc: 0.2917 (0.2582)  loss_scale: 65536.0000 (38959.4388)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0545  max mem: 15572
[2025-01-15 22:20:07,620] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46457
[2025-01-15 22:20:07,620] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:20:07,621] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1520/2809]  eta: 0:12:14  lr: 0.000035  min_lr: 0.000000  loss: 4.3109 (4.3314)  class_acc: 0.2083 (0.2579)  loss_scale: 65536.0000 (38961.8199)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1164  max mem: 15572
Epoch: [16]  [1530/2809]  eta: 0:12:09  lr: 0.000035  min_lr: 0.000000  loss: 4.3529 (4.3318)  class_acc: 0.2083 (0.2579)  loss_scale: 32768.0000 (38921.3638)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1580  max mem: 15572
Epoch: [16]  [1540/2809]  eta: 0:12:03  lr: 0.000035  min_lr: 0.000000  loss: 4.2874 (4.3307)  class_acc: 0.2917 (0.2582)  loss_scale: 32768.0000 (38881.4328)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.1066  max mem: 15572
Epoch: [16]  [1550/2809]  eta: 0:11:57  lr: 0.000035  min_lr: 0.000000  loss: 4.1764 (4.3299)  class_acc: 0.2917 (0.2588)  loss_scale: 32768.0000 (38842.0168)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0787  max mem: 15572
Epoch: [16]  [1560/2809]  eta: 0:11:51  lr: 0.000035  min_lr: 0.000000  loss: 4.2811 (4.3308)  class_acc: 0.2917 (0.2587)  loss_scale: 32768.0000 (38803.1057)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1066  max mem: 15572
Epoch: [16]  [1570/2809]  eta: 0:11:46  lr: 0.000035  min_lr: 0.000000  loss: 4.2811 (4.3294)  class_acc: 0.2083 (0.2585)  loss_scale: 32768.0000 (38764.6900)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1618  max mem: 15572
Epoch: [16]  [1580/2809]  eta: 0:11:41  lr: 0.000035  min_lr: 0.000000  loss: 4.2798 (4.3292)  class_acc: 0.2500 (0.2587)  loss_scale: 32768.0000 (38726.7603)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.1848  max mem: 15572
Epoch: [16]  [1590/2809]  eta: 0:11:35  lr: 0.000035  min_lr: 0.000000  loss: 4.4423 (4.3300)  class_acc: 0.2500 (0.2587)  loss_scale: 32768.0000 (38689.3074)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1504  max mem: 15572
Epoch: [16]  [1600/2809]  eta: 0:11:29  lr: 0.000035  min_lr: 0.000000  loss: 4.3808 (4.3310)  class_acc: 0.2500 (0.2589)  loss_scale: 32768.0000 (38652.3223)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1030  max mem: 15572
Epoch: [16]  [1610/2809]  eta: 0:11:23  lr: 0.000035  min_lr: 0.000000  loss: 4.3031 (4.3305)  class_acc: 0.2500 (0.2589)  loss_scale: 32768.0000 (38615.7964)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0878  max mem: 15572
Epoch: [16]  [1620/2809]  eta: 0:11:17  lr: 0.000035  min_lr: 0.000000  loss: 4.3031 (4.3293)  class_acc: 0.2500 (0.2591)  loss_scale: 32768.0000 (38579.7212)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0794  max mem: 15572
Epoch: [16]  [1630/2809]  eta: 0:11:11  lr: 0.000035  min_lr: 0.000000  loss: 4.3377 (4.3305)  class_acc: 0.2083 (0.2589)  loss_scale: 32768.0000 (38544.0883)  weight_decay: 0.0500 (0.0500)  time: 0.5188  data: 0.0671  max mem: 15572
Epoch: [16]  [1640/2809]  eta: 0:11:06  lr: 0.000035  min_lr: 0.000000  loss: 4.5120 (4.3308)  class_acc: 0.2083 (0.2588)  loss_scale: 32768.0000 (38508.8897)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1050  max mem: 15572
[2025-01-15 22:21:21,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:21:21,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [1650/2809]  eta: 0:11:00  lr: 0.000035  min_lr: 0.000000  loss: 4.3603 (4.3306)  class_acc: 0.2917 (0.2590)  loss_scale: 32768.0000 (38652.7438)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1619  max mem: 15572
Epoch: [16]  [1660/2809]  eta: 0:10:54  lr: 0.000035  min_lr: 0.000000  loss: 4.4038 (4.3313)  class_acc: 0.2917 (0.2593)  loss_scale: 65536.0000 (38814.5936)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0976  max mem: 15572
Epoch: [16]  [1670/2809]  eta: 0:10:48  lr: 0.000035  min_lr: 0.000000  loss: 4.4291 (4.3308)  class_acc: 0.2917 (0.2594)  loss_scale: 65536.0000 (38974.5063)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0892  max mem: 15572
[2025-01-15 22:21:39,294] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46618
[2025-01-15 22:21:39,294] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:21:39,294] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1680/2809]  eta: 0:10:43  lr: 0.000035  min_lr: 0.000000  loss: 4.2039 (4.3301)  class_acc: 0.2917 (0.2597)  loss_scale: 65536.0000 (38996.0642)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1303  max mem: 15572
Epoch: [16]  [1690/2809]  eta: 0:10:37  lr: 0.000035  min_lr: 0.000000  loss: 4.2252 (4.3298)  class_acc: 0.2500 (0.2597)  loss_scale: 32768.0000 (38959.2336)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0994  max mem: 15572
Epoch: [16]  [1700/2809]  eta: 0:10:31  lr: 0.000035  min_lr: 0.000000  loss: 4.2544 (4.3301)  class_acc: 0.2500 (0.2594)  loss_scale: 32768.0000 (38922.8360)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1127  max mem: 15572
Epoch: [16]  [1710/2809]  eta: 0:10:25  lr: 0.000035  min_lr: 0.000000  loss: 4.3126 (4.3302)  class_acc: 0.2083 (0.2595)  loss_scale: 32768.0000 (38886.8638)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1219  max mem: 15572
Epoch: [16]  [1720/2809]  eta: 0:10:20  lr: 0.000035  min_lr: 0.000000  loss: 4.3334 (4.3302)  class_acc: 0.2083 (0.2594)  loss_scale: 32768.0000 (38851.3097)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1499  max mem: 15572
Epoch: [16]  [1730/2809]  eta: 0:10:15  lr: 0.000035  min_lr: 0.000000  loss: 4.3370 (4.3301)  class_acc: 0.2083 (0.2591)  loss_scale: 32768.0000 (38816.1664)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1726  max mem: 15572
Epoch: [16]  [1740/2809]  eta: 0:10:09  lr: 0.000035  min_lr: 0.000000  loss: 4.3216 (4.3300)  class_acc: 0.2083 (0.2589)  loss_scale: 32768.0000 (38781.4268)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1389  max mem: 15572
Epoch: [16]  [1750/2809]  eta: 0:10:03  lr: 0.000035  min_lr: 0.000000  loss: 4.3216 (4.3300)  class_acc: 0.2083 (0.2587)  loss_scale: 32768.0000 (38747.0840)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.1098  max mem: 15572
Epoch: [16]  [1760/2809]  eta: 0:09:57  lr: 0.000035  min_lr: 0.000000  loss: 4.1900 (4.3283)  class_acc: 0.2500 (0.2588)  loss_scale: 32768.0000 (38713.1312)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1520  max mem: 15572
Epoch: [16]  [1770/2809]  eta: 0:09:52  lr: 0.000035  min_lr: 0.000000  loss: 4.1984 (4.3284)  class_acc: 0.3333 (0.2590)  loss_scale: 32768.0000 (38679.5618)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1683  max mem: 15572
Epoch: [16]  [1780/2809]  eta: 0:09:46  lr: 0.000035  min_lr: 0.000000  loss: 4.2580 (4.3282)  class_acc: 0.2500 (0.2588)  loss_scale: 32768.0000 (38646.3695)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1530  max mem: 15572
Epoch: [16]  [1790/2809]  eta: 0:09:41  lr: 0.000035  min_lr: 0.000000  loss: 4.3852 (4.3280)  class_acc: 0.2500 (0.2587)  loss_scale: 32768.0000 (38613.5477)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1639  max mem: 15572
Epoch: [16]  [1800/2809]  eta: 0:09:35  lr: 0.000035  min_lr: 0.000000  loss: 4.2307 (4.3274)  class_acc: 0.2083 (0.2587)  loss_scale: 32768.0000 (38581.0905)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1583  max mem: 15572
[2025-01-15 22:22:54,299] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:22:54,299] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 22:22:55,312] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46749
[2025-01-15 22:22:55,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:22:55,313] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1810/2809]  eta: 0:09:29  lr: 0.000035  min_lr: 0.000000  loss: 4.1247 (4.3262)  class_acc: 0.3333 (0.2593)  loss_scale: 32768.0000 (38585.1795)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1110  max mem: 15572
Epoch: [16]  [1820/2809]  eta: 0:09:23  lr: 0.000035  min_lr: 0.000000  loss: 4.1696 (4.3266)  class_acc: 0.2917 (0.2592)  loss_scale: 32768.0000 (38553.2345)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0869  max mem: 15572
Epoch: [16]  [1830/2809]  eta: 0:09:18  lr: 0.000035  min_lr: 0.000000  loss: 4.3620 (4.3264)  class_acc: 0.2500 (0.2589)  loss_scale: 32768.0000 (38521.6384)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1604  max mem: 15572
Epoch: [16]  [1840/2809]  eta: 0:09:12  lr: 0.000035  min_lr: 0.000000  loss: 4.3042 (4.3269)  class_acc: 0.1667 (0.2586)  loss_scale: 32768.0000 (38490.3857)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1797  max mem: 15572
Epoch: [16]  [1850/2809]  eta: 0:09:07  lr: 0.000035  min_lr: 0.000000  loss: 4.4093 (4.3275)  class_acc: 0.2083 (0.2585)  loss_scale: 32768.0000 (38459.4706)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1258  max mem: 15572
Epoch: [16]  [1860/2809]  eta: 0:09:01  lr: 0.000035  min_lr: 0.000000  loss: 4.3124 (4.3271)  class_acc: 0.2083 (0.2584)  loss_scale: 32768.0000 (38428.8877)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1003  max mem: 15572
Epoch: [16]  [1870/2809]  eta: 0:08:55  lr: 0.000035  min_lr: 0.000000  loss: 4.2921 (4.3274)  class_acc: 0.2500 (0.2586)  loss_scale: 32768.0000 (38398.6317)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0759  max mem: 15572
Epoch: [16]  [1880/2809]  eta: 0:08:49  lr: 0.000035  min_lr: 0.000000  loss: 4.4627 (4.3284)  class_acc: 0.2500 (0.2586)  loss_scale: 32768.0000 (38368.6975)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1042  max mem: 15572
Epoch: [16]  [1890/2809]  eta: 0:08:43  lr: 0.000035  min_lr: 0.000000  loss: 4.4027 (4.3287)  class_acc: 0.2083 (0.2584)  loss_scale: 32768.0000 (38339.0799)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1331  max mem: 15572
Epoch: [16]  [1900/2809]  eta: 0:08:38  lr: 0.000035  min_lr: 0.000000  loss: 4.3281 (4.3283)  class_acc: 0.1667 (0.2582)  loss_scale: 32768.0000 (38309.7738)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0919  max mem: 15572
Epoch: [16]  [1910/2809]  eta: 0:08:31  lr: 0.000035  min_lr: 0.000000  loss: 4.2570 (4.3277)  class_acc: 0.2083 (0.2581)  loss_scale: 32768.0000 (38280.7745)  weight_decay: 0.0500 (0.0500)  time: 0.4882  data: 0.0374  max mem: 15572
Epoch: [16]  [1920/2809]  eta: 0:08:26  lr: 0.000035  min_lr: 0.000000  loss: 4.2776 (4.3285)  class_acc: 0.2500 (0.2581)  loss_scale: 32768.0000 (38252.0770)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0547  max mem: 15572
Epoch: [16]  [1930/2809]  eta: 0:08:19  lr: 0.000035  min_lr: 0.000000  loss: 4.3154 (4.3280)  class_acc: 0.2500 (0.2580)  loss_scale: 32768.0000 (38223.6769)  weight_decay: 0.0500 (0.0500)  time: 0.4882  data: 0.0547  max mem: 15572
[2025-01-15 22:24:04,936] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:24:04,937] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [1940/2809]  eta: 0:08:14  lr: 0.000035  min_lr: 0.000000  loss: 4.3154 (4.3280)  class_acc: 0.2083 (0.2577)  loss_scale: 32768.0000 (38313.7434)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0840  max mem: 15572
[2025-01-15 22:24:12,914] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46890
[2025-01-15 22:24:12,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:24:12,914] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1950/2809]  eta: 0:08:08  lr: 0.000035  min_lr: 0.000000  loss: 4.4572 (4.3290)  class_acc: 0.1667 (0.2575)  loss_scale: 65536.0000 (38369.2957)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1476  max mem: 15572
Epoch: [16]  [1960/2809]  eta: 0:08:02  lr: 0.000035  min_lr: 0.000000  loss: 4.4358 (4.3288)  class_acc: 0.2917 (0.2578)  loss_scale: 32768.0000 (38340.7323)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1332  max mem: 15572
Epoch: [16]  [1970/2809]  eta: 0:07:56  lr: 0.000035  min_lr: 0.000000  loss: 4.2237 (4.3282)  class_acc: 0.2917 (0.2579)  loss_scale: 32768.0000 (38312.4587)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1129  max mem: 15572
Epoch: [16]  [1980/2809]  eta: 0:07:51  lr: 0.000035  min_lr: 0.000000  loss: 4.2299 (4.3281)  class_acc: 0.2083 (0.2576)  loss_scale: 32768.0000 (38284.4705)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0839  max mem: 15572
Epoch: [16]  [1990/2809]  eta: 0:07:45  lr: 0.000035  min_lr: 0.000000  loss: 4.2286 (4.3277)  class_acc: 0.1667 (0.2577)  loss_scale: 32768.0000 (38256.7634)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0948  max mem: 15572
Epoch: [16]  [2000/2809]  eta: 0:07:40  lr: 0.000035  min_lr: 0.000000  loss: 4.3274 (4.3284)  class_acc: 0.2917 (0.2576)  loss_scale: 32768.0000 (38229.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1562  max mem: 15572
Epoch: [16]  [2010/2809]  eta: 0:07:34  lr: 0.000035  min_lr: 0.000000  loss: 4.4527 (4.3285)  class_acc: 0.2083 (0.2573)  loss_scale: 32768.0000 (38202.1760)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1546  max mem: 15572
Epoch: [16]  [2020/2809]  eta: 0:07:29  lr: 0.000035  min_lr: 0.000000  loss: 4.4266 (4.3288)  class_acc: 0.1667 (0.2571)  loss_scale: 32768.0000 (38175.2875)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1374  max mem: 15572
Epoch: [16]  [2030/2809]  eta: 0:07:23  lr: 0.000035  min_lr: 0.000000  loss: 4.2837 (4.3277)  class_acc: 0.2500 (0.2574)  loss_scale: 32768.0000 (38148.6637)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1315  max mem: 15572
Epoch: [16]  [2040/2809]  eta: 0:07:17  lr: 0.000035  min_lr: 0.000000  loss: 4.2047 (4.3278)  class_acc: 0.2500 (0.2572)  loss_scale: 32768.0000 (38122.3008)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1149  max mem: 15572
Epoch: [16]  [2050/2809]  eta: 0:07:12  lr: 0.000035  min_lr: 0.000000  loss: 4.3274 (4.3278)  class_acc: 0.2500 (0.2575)  loss_scale: 32768.0000 (38096.1950)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.1811  max mem: 15572
[2025-01-15 22:25:16,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=47000, skipped=293, lr=[3.395923861060386e-07, 3.395923861060386e-07, 4.851319801514837e-07, 4.851319801514837e-07, 6.930456859306911e-07, 6.930456859306911e-07, 9.90065265615273e-07, 9.90065265615273e-07, 1.4143789508789615e-06, 1.4143789508789615e-06, 2.0205413583985166e-06, 2.0205413583985166e-06, 2.886487654855024e-06, 2.886487654855024e-06, 4.123553792650034e-06, 4.123553792650034e-06, 5.890791132357192e-06, 5.890791132357192e-06, 8.415415903367418e-06, 8.415415903367418e-06, 1.2022022719096311e-05, 1.2022022719096311e-05, 1.717431817013759e-05, 1.717431817013759e-05, 2.4534740243053702e-05, 2.4534740243053702e-05, 3.504962891864815e-05, 3.504962891864815e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:25:16,446] [INFO] [timer.py:260:stop] epoch=0/micro_step=47000/global_step=47000, RunningAvgSamplesPerSec=27.693970894805712, CurrSamplesPerSec=32.442853330617496, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [2060/2809]  eta: 0:07:06  lr: 0.000035  min_lr: 0.000000  loss: 4.3362 (4.3285)  class_acc: 0.2500 (0.2574)  loss_scale: 32768.0000 (38070.3426)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1133  max mem: 15572
Epoch: [16]  [2070/2809]  eta: 0:07:00  lr: 0.000035  min_lr: 0.000000  loss: 4.4607 (4.3288)  class_acc: 0.2500 (0.2574)  loss_scale: 32768.0000 (38044.7397)  weight_decay: 0.0500 (0.0500)  time: 0.5028  data: 0.0779  max mem: 15572
[2025-01-15 22:25:27,062] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:25:27,062] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [2080/2809]  eta: 0:06:55  lr: 0.000035  min_lr: 0.000000  loss: 4.4503 (4.3292)  class_acc: 0.2083 (0.2574)  loss_scale: 32768.0000 (38113.8606)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1481  max mem: 15572
Epoch: [16]  [2090/2809]  eta: 0:06:49  lr: 0.000035  min_lr: 0.000000  loss: 4.4222 (4.3296)  class_acc: 0.2083 (0.2575)  loss_scale: 65536.0000 (38245.0043)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1363  max mem: 15572
Epoch: [16]  [2100/2809]  eta: 0:06:43  lr: 0.000035  min_lr: 0.000000  loss: 4.3819 (4.3298)  class_acc: 0.2500 (0.2574)  loss_scale: 65536.0000 (38374.8996)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0935  max mem: 15572
Epoch: [16]  [2110/2809]  eta: 0:06:37  lr: 0.000035  min_lr: 0.000000  loss: 4.3254 (4.3298)  class_acc: 0.2500 (0.2577)  loss_scale: 65536.0000 (38503.5642)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0539  max mem: 15572
Epoch: [16]  [2120/2809]  eta: 0:06:31  lr: 0.000035  min_lr: 0.000000  loss: 4.3254 (4.3299)  class_acc: 0.2917 (0.2578)  loss_scale: 65536.0000 (38631.0156)  weight_decay: 0.0500 (0.0500)  time: 0.5050  data: 0.0581  max mem: 15572
[2025-01-15 22:25:53,381] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47068
[2025-01-15 22:25:53,381] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:25:53,382] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [2130/2809]  eta: 0:06:26  lr: 0.000035  min_lr: 0.000000  loss: 4.2774 (4.3295)  class_acc: 0.2500 (0.2580)  loss_scale: 65536.0000 (38649.6330)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1027  max mem: 15572
Epoch: [16]  [2140/2809]  eta: 0:06:20  lr: 0.000035  min_lr: 0.000000  loss: 4.2902 (4.3293)  class_acc: 0.2500 (0.2581)  loss_scale: 32768.0000 (38622.1616)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1390  max mem: 15572
Epoch: [16]  [2150/2809]  eta: 0:06:15  lr: 0.000035  min_lr: 0.000000  loss: 4.3353 (4.3294)  class_acc: 0.2500 (0.2582)  loss_scale: 32768.0000 (38594.9456)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.1338  max mem: 15572
Epoch: [16]  [2160/2809]  eta: 0:06:09  lr: 0.000035  min_lr: 0.000000  loss: 4.2674 (4.3292)  class_acc: 0.2500 (0.2584)  loss_scale: 32768.0000 (38567.9815)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1218  max mem: 15572
Epoch: [16]  [2170/2809]  eta: 0:06:03  lr: 0.000035  min_lr: 0.000000  loss: 4.3284 (4.3299)  class_acc: 0.2083 (0.2584)  loss_scale: 32768.0000 (38541.2658)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0858  max mem: 15572
Epoch: [16]  [2180/2809]  eta: 0:05:58  lr: 0.000035  min_lr: 0.000000  loss: 4.3412 (4.3295)  class_acc: 0.3333 (0.2588)  loss_scale: 32768.0000 (38514.7950)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1307  max mem: 15572
Epoch: [16]  [2190/2809]  eta: 0:05:52  lr: 0.000035  min_lr: 0.000000  loss: 4.3961 (4.3301)  class_acc: 0.3333 (0.2589)  loss_scale: 32768.0000 (38488.5660)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1699  max mem: 15572
Epoch: [16]  [2200/2809]  eta: 0:05:47  lr: 0.000035  min_lr: 0.000000  loss: 4.5038 (4.3311)  class_acc: 0.2500 (0.2589)  loss_scale: 32768.0000 (38462.5752)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1948  max mem: 15572
Epoch: [16]  [2210/2809]  eta: 0:05:41  lr: 0.000035  min_lr: 0.000000  loss: 4.3646 (4.3307)  class_acc: 0.2917 (0.2593)  loss_scale: 32768.0000 (38436.8195)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1264  max mem: 15572
Epoch: [16]  [2220/2809]  eta: 0:05:35  lr: 0.000035  min_lr: 0.000000  loss: 4.3748 (4.3311)  class_acc: 0.2917 (0.2592)  loss_scale: 32768.0000 (38411.2958)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0344  max mem: 15572
Epoch: [16]  [2230/2809]  eta: 0:05:29  lr: 0.000035  min_lr: 0.000000  loss: 4.3441 (4.3303)  class_acc: 0.2500 (0.2596)  loss_scale: 32768.0000 (38386.0009)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0746  max mem: 15572
Epoch: [16]  [2240/2809]  eta: 0:05:23  lr: 0.000035  min_lr: 0.000000  loss: 4.3166 (4.3305)  class_acc: 0.2500 (0.2593)  loss_scale: 32768.0000 (38360.9317)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0944  max mem: 15572
Epoch: [16]  [2250/2809]  eta: 0:05:17  lr: 0.000035  min_lr: 0.000000  loss: 4.1781 (4.3289)  class_acc: 0.2083 (0.2595)  loss_scale: 32768.0000 (38336.0853)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0827  max mem: 15572
[2025-01-15 22:27:07,251] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:27:07,252] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [2260/2809]  eta: 0:05:12  lr: 0.000035  min_lr: 0.000000  loss: 4.1781 (4.3287)  class_acc: 0.2917 (0.2595)  loss_scale: 32768.0000 (38427.4003)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1165  max mem: 15572
[2025-01-15 22:27:13,477] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47207
[2025-01-15 22:27:13,477] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:27:13,477] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [2270/2809]  eta: 0:05:06  lr: 0.000035  min_lr: 0.000000  loss: 4.4271 (4.3290)  class_acc: 0.2083 (0.2594)  loss_scale: 32768.0000 (38431.3377)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1218  max mem: 15572
Epoch: [16]  [2280/2809]  eta: 0:05:00  lr: 0.000035  min_lr: 0.000000  loss: 4.4552 (4.3293)  class_acc: 0.1667 (0.2590)  loss_scale: 32768.0000 (38406.5094)  weight_decay: 0.0500 (0.0500)  time: 0.5022  data: 0.0555  max mem: 15572
Epoch: [16]  [2290/2809]  eta: 0:04:55  lr: 0.000035  min_lr: 0.000000  loss: 4.4552 (4.3290)  class_acc: 0.1667 (0.2593)  loss_scale: 32768.0000 (38381.8979)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.1132  max mem: 15572
Epoch: [16]  [2300/2809]  eta: 0:04:49  lr: 0.000035  min_lr: 0.000000  loss: 4.4163 (4.3289)  class_acc: 0.2083 (0.2591)  loss_scale: 32768.0000 (38357.5002)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1297  max mem: 15572
Epoch: [16]  [2310/2809]  eta: 0:04:43  lr: 0.000035  min_lr: 0.000000  loss: 4.2813 (4.3285)  class_acc: 0.2083 (0.2592)  loss_scale: 32768.0000 (38333.3137)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1503  max mem: 15572
Epoch: [16]  [2320/2809]  eta: 0:04:38  lr: 0.000035  min_lr: 0.000000  loss: 4.3801 (4.3291)  class_acc: 0.2083 (0.2590)  loss_scale: 32768.0000 (38309.3356)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1649  max mem: 15572
Epoch: [16]  [2330/2809]  eta: 0:04:32  lr: 0.000035  min_lr: 0.000000  loss: 4.3769 (4.3288)  class_acc: 0.2083 (0.2590)  loss_scale: 32768.0000 (38285.5633)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0753  max mem: 15572
Epoch: [16]  [2340/2809]  eta: 0:04:26  lr: 0.000035  min_lr: 0.000000  loss: 4.3310 (4.3288)  class_acc: 0.2083 (0.2588)  loss_scale: 32768.0000 (38261.9940)  weight_decay: 0.0500 (0.0500)  time: 0.5176  data: 0.0700  max mem: 15572
Epoch: [16]  [2350/2809]  eta: 0:04:20  lr: 0.000035  min_lr: 0.000000  loss: 4.3239 (4.3287)  class_acc: 0.2083 (0.2588)  loss_scale: 32768.0000 (38238.6253)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0870  max mem: 15572
Epoch: [16]  [2360/2809]  eta: 0:04:15  lr: 0.000035  min_lr: 0.000000  loss: 4.2754 (4.3281)  class_acc: 0.2500 (0.2589)  loss_scale: 32768.0000 (38215.4545)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.1038  max mem: 15572
Epoch: [16]  [2370/2809]  eta: 0:04:09  lr: 0.000035  min_lr: 0.000000  loss: 4.2781 (4.3286)  class_acc: 0.2083 (0.2588)  loss_scale: 32768.0000 (38192.4791)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1133  max mem: 15572
Epoch: [16]  [2380/2809]  eta: 0:04:03  lr: 0.000035  min_lr: 0.000000  loss: 4.3218 (4.3282)  class_acc: 0.2500 (0.2587)  loss_scale: 32768.0000 (38169.6968)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1392  max mem: 15572
Epoch: [16]  [2390/2809]  eta: 0:03:58  lr: 0.000035  min_lr: 0.000000  loss: 4.3091 (4.3284)  class_acc: 0.2500 (0.2589)  loss_scale: 32768.0000 (38147.1050)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1578  max mem: 15572
[2025-01-15 22:28:25,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:28:25,114] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [2400/2809]  eta: 0:03:52  lr: 0.000035  min_lr: 0.000000  loss: 4.4723 (4.3288)  class_acc: 0.2500 (0.2585)  loss_scale: 32768.0000 (38247.5302)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1152  max mem: 15572
Epoch: [16]  [2410/2809]  eta: 0:03:46  lr: 0.000035  min_lr: 0.000000  loss: 4.4076 (4.3289)  class_acc: 0.2083 (0.2583)  loss_scale: 65536.0000 (38360.7134)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1195  max mem: 15572
Epoch: [16]  [2420/2809]  eta: 0:03:41  lr: 0.000035  min_lr: 0.000000  loss: 4.3224 (4.3290)  class_acc: 0.2500 (0.2583)  loss_scale: 65536.0000 (38472.9616)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1475  max mem: 15572
[2025-01-15 22:28:42,874] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47368
[2025-01-15 22:28:42,874] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:28:42,874] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [2430/2809]  eta: 0:03:35  lr: 0.000035  min_lr: 0.000000  loss: 4.2777 (4.3287)  class_acc: 0.2500 (0.2584)  loss_scale: 65536.0000 (38489.9317)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.1014  max mem: 15572
Epoch: [16]  [2440/2809]  eta: 0:03:29  lr: 0.000035  min_lr: 0.000000  loss: 4.1593 (4.3281)  class_acc: 0.2500 (0.2583)  loss_scale: 32768.0000 (38466.4908)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1009  max mem: 15572
Epoch: [16]  [2450/2809]  eta: 0:03:23  lr: 0.000035  min_lr: 0.000000  loss: 4.1857 (4.3280)  class_acc: 0.1667 (0.2582)  loss_scale: 32768.0000 (38443.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1180  max mem: 15572
Epoch: [16]  [2460/2809]  eta: 0:03:18  lr: 0.000035  min_lr: 0.000000  loss: 4.4041 (4.3278)  class_acc: 0.1667 (0.2581)  loss_scale: 32768.0000 (38420.1804)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0784  max mem: 15572
Epoch: [16]  [2470/2809]  eta: 0:03:12  lr: 0.000035  min_lr: 0.000000  loss: 4.2329 (4.3272)  class_acc: 0.2083 (0.2581)  loss_scale: 32768.0000 (38397.3064)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1057  max mem: 15572
Epoch: [16]  [2480/2809]  eta: 0:03:06  lr: 0.000035  min_lr: 0.000000  loss: 4.2329 (4.3273)  class_acc: 0.2500 (0.2580)  loss_scale: 32768.0000 (38374.6167)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1021  max mem: 15572
Epoch: [16]  [2490/2809]  eta: 0:03:01  lr: 0.000035  min_lr: 0.000000  loss: 4.4269 (4.3275)  class_acc: 0.2083 (0.2577)  loss_scale: 32768.0000 (38352.1092)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1290  max mem: 15572
Epoch: [16]  [2500/2809]  eta: 0:02:55  lr: 0.000035  min_lr: 0.000000  loss: 4.4269 (4.3274)  class_acc: 0.1667 (0.2576)  loss_scale: 32768.0000 (38329.7817)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0980  max mem: 15572
Epoch: [16]  [2510/2809]  eta: 0:02:49  lr: 0.000035  min_lr: 0.000000  loss: 4.3310 (4.3272)  class_acc: 0.2083 (0.2577)  loss_scale: 32768.0000 (38307.6320)  weight_decay: 0.0500 (0.0500)  time: 0.4906  data: 0.0121  max mem: 15572
Epoch: [16]  [2520/2809]  eta: 0:02:44  lr: 0.000035  min_lr: 0.000000  loss: 4.2519 (4.3267)  class_acc: 0.2917 (0.2579)  loss_scale: 32768.0000 (38285.6581)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0783  max mem: 15572
Epoch: [16]  [2530/2809]  eta: 0:02:38  lr: 0.000035  min_lr: 0.000000  loss: 4.2519 (4.3272)  class_acc: 0.2083 (0.2576)  loss_scale: 32768.0000 (38263.8578)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1016  max mem: 15572
Epoch: [16]  [2540/2809]  eta: 0:02:32  lr: 0.000035  min_lr: 0.000000  loss: 4.3393 (4.3270)  class_acc: 0.2500 (0.2577)  loss_scale: 32768.0000 (38242.2290)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1162  max mem: 15572
Epoch: [16]  [2550/2809]  eta: 0:02:27  lr: 0.000035  min_lr: 0.000000  loss: 4.3266 (4.3271)  class_acc: 0.2083 (0.2575)  loss_scale: 32768.0000 (38220.7699)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1339  max mem: 15572
[2025-01-15 22:29:56,315] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:29:56,316] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 22:30:00,314] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47504
[2025-01-15 22:30:00,315] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:30:00,315] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [2560/2809]  eta: 0:02:21  lr: 0.000035  min_lr: 0.000000  loss: 4.3397 (4.3272)  class_acc: 0.2083 (0.2576)  loss_scale: 32768.0000 (38289.0433)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.0880  max mem: 15572
Epoch: [16]  [2570/2809]  eta: 0:02:15  lr: 0.000035  min_lr: 0.000000  loss: 4.3485 (4.3271)  class_acc: 0.2500 (0.2574)  loss_scale: 32768.0000 (38267.5690)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0508  max mem: 15572
Epoch: [16]  [2580/2809]  eta: 0:02:09  lr: 0.000035  min_lr: 0.000000  loss: 4.3514 (4.3272)  class_acc: 0.2083 (0.2573)  loss_scale: 32768.0000 (38246.2611)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0247  max mem: 15572
Epoch: [16]  [2590/2809]  eta: 0:02:04  lr: 0.000035  min_lr: 0.000000  loss: 4.3645 (4.3274)  class_acc: 0.2083 (0.2572)  loss_scale: 32768.0000 (38225.1177)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.0442  max mem: 15572
Epoch: [16]  [2600/2809]  eta: 0:01:58  lr: 0.000035  min_lr: 0.000000  loss: 4.3098 (4.3270)  class_acc: 0.2500 (0.2573)  loss_scale: 32768.0000 (38204.1369)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1376  max mem: 15572
Epoch: [16]  [2610/2809]  eta: 0:01:52  lr: 0.000035  min_lr: 0.000000  loss: 4.2394 (4.3266)  class_acc: 0.2500 (0.2574)  loss_scale: 32768.0000 (38183.3167)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1030  max mem: 15572
Epoch: [16]  [2620/2809]  eta: 0:01:47  lr: 0.000035  min_lr: 0.000000  loss: 4.3720 (4.3267)  class_acc: 0.2083 (0.2572)  loss_scale: 32768.0000 (38162.6555)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0401  max mem: 15572
Epoch: [16]  [2630/2809]  eta: 0:01:41  lr: 0.000035  min_lr: 0.000000  loss: 4.4504 (4.3273)  class_acc: 0.2083 (0.2571)  loss_scale: 32768.0000 (38142.1513)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0734  max mem: 15572
Epoch: [16]  [2640/2809]  eta: 0:01:35  lr: 0.000035  min_lr: 0.000000  loss: 4.3361 (4.3273)  class_acc: 0.2500 (0.2572)  loss_scale: 32768.0000 (38121.8023)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0824  max mem: 15572
Epoch: [16]  [2650/2809]  eta: 0:01:30  lr: 0.000035  min_lr: 0.000000  loss: 4.3361 (4.3274)  class_acc: 0.2917 (0.2573)  loss_scale: 32768.0000 (38101.6069)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0492  max mem: 15572
Epoch: [16]  [2660/2809]  eta: 0:01:24  lr: 0.000035  min_lr: 0.000000  loss: 4.4045 (4.3278)  class_acc: 0.2083 (0.2572)  loss_scale: 32768.0000 (38081.5633)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0595  max mem: 15572
Epoch: [16]  [2670/2809]  eta: 0:01:18  lr: 0.000035  min_lr: 0.000000  loss: 4.3724 (4.3279)  class_acc: 0.2500 (0.2572)  loss_scale: 32768.0000 (38061.6698)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1081  max mem: 15572
Epoch: [16]  [2680/2809]  eta: 0:01:13  lr: 0.000035  min_lr: 0.000000  loss: 4.3997 (4.3281)  class_acc: 0.2500 (0.2570)  loss_scale: 32768.0000 (38041.9247)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1525  max mem: 15572
[2025-01-15 22:31:13,041] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:31:13,041] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [2690/2809]  eta: 0:01:07  lr: 0.000035  min_lr: 0.000000  loss: 4.3509 (4.3278)  class_acc: 0.2500 (0.2569)  loss_scale: 32768.0000 (38046.6800)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.2073  max mem: 15572
Epoch: [16]  [2700/2809]  eta: 0:01:01  lr: 0.000035  min_lr: 0.000000  loss: 4.3094 (4.3279)  class_acc: 0.2500 (0.2567)  loss_scale: 65536.0000 (38148.4546)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.1041  max mem: 15572
[2025-01-15 22:31:22,310] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47654
[2025-01-15 22:31:22,310] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:31:22,310] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [2710/2809]  eta: 0:00:56  lr: 0.000035  min_lr: 0.000000  loss: 4.1702 (4.3272)  class_acc: 0.2500 (0.2569)  loss_scale: 65536.0000 (38237.3914)  weight_decay: 0.0500 (0.0500)  time: 0.4420  data: 0.0007  max mem: 15572
Epoch: [16]  [2720/2809]  eta: 0:00:50  lr: 0.000035  min_lr: 0.000000  loss: 4.2693 (4.3273)  class_acc: 0.2917 (0.2571)  loss_scale: 32768.0000 (38217.2907)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0008  max mem: 15572
Epoch: [16]  [2730/2809]  eta: 0:00:44  lr: 0.000035  min_lr: 0.000000  loss: 4.3308 (4.3275)  class_acc: 0.2917 (0.2572)  loss_scale: 32768.0000 (38197.3372)  weight_decay: 0.0500 (0.0500)  time: 0.4975  data: 0.0009  max mem: 15572
Epoch: [16]  [2740/2809]  eta: 0:00:39  lr: 0.000035  min_lr: 0.000000  loss: 4.3010 (4.3271)  class_acc: 0.2917 (0.2572)  loss_scale: 32768.0000 (38177.5294)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1112  max mem: 15572
Epoch: [16]  [2750/2809]  eta: 0:00:33  lr: 0.000035  min_lr: 0.000000  loss: 4.3548 (4.3274)  class_acc: 0.2083 (0.2570)  loss_scale: 32768.0000 (38157.8655)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1499  max mem: 15572
Epoch: [16]  [2760/2809]  eta: 0:00:27  lr: 0.000035  min_lr: 0.000000  loss: 4.4102 (4.3281)  class_acc: 0.2083 (0.2570)  loss_scale: 32768.0000 (38138.3441)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1151  max mem: 15572
Epoch: [16]  [2770/2809]  eta: 0:00:22  lr: 0.000035  min_lr: 0.000000  loss: 4.4894 (4.3290)  class_acc: 0.2083 (0.2570)  loss_scale: 32768.0000 (38118.9636)  weight_decay: 0.0500 (0.0500)  time: 0.6409  data: 0.1454  max mem: 15572
Epoch: [16]  [2780/2809]  eta: 0:00:16  lr: 0.000035  min_lr: 0.000000  loss: 4.5397 (4.3296)  class_acc: 0.1667 (0.2568)  loss_scale: 32768.0000 (38099.7224)  weight_decay: 0.0500 (0.0500)  time: 0.6705  data: 0.1711  max mem: 15572
Epoch: [16]  [2790/2809]  eta: 0:00:10  lr: 0.000035  min_lr: 0.000000  loss: 4.3537 (4.3294)  class_acc: 0.2917 (0.2571)  loss_scale: 32768.0000 (38080.6191)  weight_decay: 0.0500 (0.0500)  time: 0.7213  data: 0.2276  max mem: 15572
Epoch: [16]  [2800/2809]  eta: 0:00:05  lr: 0.000035  min_lr: 0.000000  loss: 4.2550 (4.3293)  class_acc: 0.2917 (0.2571)  loss_scale: 32768.0000 (38061.6523)  weight_decay: 0.0500 (0.0500)  time: 0.7037  data: 0.2266  max mem: 15572
Epoch: [16]  [2808/2809]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000000  loss: 4.3164 (4.3295)  class_acc: 0.2500 (0.2570)  loss_scale: 32768.0000 (38046.5760)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1039  max mem: 15572
Epoch: [16] Total time: 0:26:37 (0.5687 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000000  loss: 4.3164 (4.3295)  class_acc: 0.2500 (0.2570)  loss_scale: 32768.0000 (38046.5760)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:38:38  loss: 1.0725 (1.0725)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 8.5237  data: 8.3078  max mem: 15572
Val:  [ 10/272]  eta: 0:04:48  loss: 3.0080 (2.8612)  acc1: 33.3333 (37.8788)  acc5: 66.6667 (65.6566)  time: 1.1028  data: 0.8867  max mem: 15572
Val:  [ 20/272]  eta: 0:02:58  loss: 2.9821 (2.8632)  acc1: 38.8889 (39.6825)  acc5: 66.6667 (66.6667)  time: 0.3190  data: 0.1073  max mem: 15572
Val:  [ 30/272]  eta: 0:02:10  loss: 2.9821 (2.9145)  acc1: 38.8889 (36.5591)  acc5: 61.1111 (67.2043)  time: 0.2301  data: 0.0353  max mem: 15572
Val:  [ 40/272]  eta: 0:01:46  loss: 2.7229 (2.8716)  acc1: 27.7778 (34.2818)  acc5: 72.2222 (69.5122)  time: 0.1996  data: 0.0164  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 2.6180 (2.7984)  acc1: 27.7778 (36.4924)  acc5: 77.7778 (71.8954)  time: 0.2133  data: 0.0395  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 2.1957 (2.7346)  acc1: 55.5556 (38.3424)  acc5: 83.3333 (72.4954)  time: 0.1872  data: 0.0251  max mem: 15572
Val:  [ 70/272]  eta: 0:01:09  loss: 2.1973 (2.6801)  acc1: 55.5556 (41.1581)  acc5: 77.7778 (73.7089)  time: 0.1707  data: 0.0033  max mem: 15572
Val:  [ 80/272]  eta: 0:01:02  loss: 2.3947 (2.6820)  acc1: 50.0000 (41.4266)  acc5: 77.7778 (73.1824)  time: 0.1880  data: 0.0021  max mem: 15572
Val:  [ 90/272]  eta: 0:00:56  loss: 3.0110 (2.7212)  acc1: 38.8889 (41.2088)  acc5: 72.2222 (72.9548)  time: 0.2031  data: 0.0009  max mem: 15572
Val:  [100/272]  eta: 0:00:53  loss: 3.0015 (2.7542)  acc1: 38.8889 (40.5941)  acc5: 72.2222 (72.4422)  time: 0.2503  data: 0.0431  max mem: 15572
Val:  [110/272]  eta: 0:00:50  loss: 2.9836 (2.8157)  acc1: 27.7778 (38.7888)  acc5: 66.6667 (71.1211)  time: 0.3200  data: 0.1212  max mem: 15572
Val:  [120/272]  eta: 0:00:48  loss: 3.4052 (2.8559)  acc1: 22.2222 (38.1084)  acc5: 55.5556 (70.0643)  time: 0.3405  data: 0.1526  max mem: 15572
Val:  [130/272]  eta: 0:00:44  loss: 2.9110 (2.8322)  acc1: 38.8889 (38.7617)  acc5: 72.2222 (70.7379)  time: 0.3114  data: 0.1187  max mem: 15572
Val:  [140/272]  eta: 0:00:41  loss: 2.4773 (2.8336)  acc1: 44.4444 (39.0465)  acc5: 77.7778 (70.5280)  time: 0.3004  data: 0.1058  max mem: 15572
Val:  [150/272]  eta: 0:00:38  loss: 2.8406 (2.8265)  acc1: 27.7778 (38.5210)  acc5: 72.2222 (70.8241)  time: 0.3105  data: 0.1157  max mem: 15572
Val:  [160/272]  eta: 0:00:35  loss: 2.7380 (2.8173)  acc1: 38.8889 (39.1649)  acc5: 77.7778 (71.2905)  time: 0.3111  data: 0.1105  max mem: 15572
Val:  [170/272]  eta: 0:00:31  loss: 2.8841 (2.8350)  acc1: 38.8889 (38.6290)  acc5: 72.2222 (70.7602)  time: 0.3077  data: 0.1147  max mem: 15572
Val:  [180/272]  eta: 0:00:28  loss: 2.8030 (2.8218)  acc1: 27.7778 (38.3978)  acc5: 66.6667 (71.1479)  time: 0.3081  data: 0.1283  max mem: 15572
Val:  [190/272]  eta: 0:00:25  loss: 2.7693 (2.8512)  acc1: 27.7778 (37.5800)  acc5: 66.6667 (69.9825)  time: 0.3380  data: 0.1547  max mem: 15572
Val:  [200/272]  eta: 0:00:22  loss: 2.8113 (2.8474)  acc1: 27.7778 (37.6451)  acc5: 66.6667 (70.1493)  time: 0.3051  data: 0.1262  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.4643 (2.8490)  acc1: 44.4444 (37.8884)  acc5: 77.7778 (70.0369)  time: 0.2800  data: 0.1038  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.8752 (2.8476)  acc1: 38.8889 (38.1599)  acc5: 72.2222 (70.0352)  time: 0.3000  data: 0.1139  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.6039 (2.8343)  acc1: 55.5556 (39.0572)  acc5: 77.7778 (70.5147)  time: 0.2856  data: 0.1038  max mem: 15572
Val:  [240/272]  eta: 0:00:09  loss: 2.3866 (2.8194)  acc1: 55.5556 (39.3499)  acc5: 83.3333 (71.0235)  time: 0.3153  data: 0.1241  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.5438 (2.8265)  acc1: 33.3333 (38.7782)  acc5: 77.7778 (70.9385)  time: 0.3075  data: 0.1088  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.2992 (2.7848)  acc1: 61.1111 (40.3576)  acc5: 83.3333 (71.7539)  time: 0.2745  data: 0.0840  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.2992 (2.7864)  acc1: 61.1111 (40.2214)  acc5: 83.3333 (71.6482)  time: 0.2245  data: 0.0587  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.2992 (2.7899)  acc1: 61.1111 (40.2212)  acc5: 83.3333 (71.6158)  time: 0.2186  data: 0.0587  max mem: 15572
Val: Total time: 0:01:22 (0.3033 s / it)
* Acc@1 40.221 Acc@5 71.616 loss 2.790
Accuracy of the network on the 4883 val videos: 40.2%
[2025-01-15 22:33:44,960] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 22:33:44,963] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 22:33:44,963] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 22:33:47,559] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 22:33:47,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 40.22%
Epoch: [17]  [   0/2809]  eta: 6:24:52  lr: 0.000035  min_lr: 0.000000  loss: 4.4896 (4.4896)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.2208  data: 7.7289  max mem: 15572
Epoch: [17]  [  10/2809]  eta: 0:58:24  lr: 0.000035  min_lr: 0.000000  loss: 4.3348 (4.3623)  class_acc: 0.2500 (0.2235)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2520  data: 0.8294  max mem: 15572
Epoch: [17]  [  20/2809]  eta: 0:42:02  lr: 0.000035  min_lr: 0.000000  loss: 4.2383 (4.2724)  class_acc: 0.2083 (0.2302)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0970  max mem: 15572
[2025-01-15 22:34:11,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:34:11,862] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [  30/2809]  eta: 0:36:18  lr: 0.000035  min_lr: 0.000000  loss: 4.2163 (4.2728)  class_acc: 0.2083 (0.2513)  loss_scale: 32768.0000 (33825.0323)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0787  max mem: 15572
[2025-01-15 22:34:13,681] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47786
[2025-01-15 22:34:13,681] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:34:13,681] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [  40/2809]  eta: 0:34:35  lr: 0.000035  min_lr: 0.000000  loss: 4.2856 (4.2884)  class_acc: 0.2083 (0.2480)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1583  max mem: 15572
Epoch: [17]  [  50/2809]  eta: 0:32:52  lr: 0.000035  min_lr: 0.000000  loss: 4.2969 (4.2958)  class_acc: 0.2500 (0.2582)  loss_scale: 32768.0000 (34695.5294)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1830  max mem: 15572
Epoch: [17]  [  60/2809]  eta: 0:30:46  lr: 0.000035  min_lr: 0.000000  loss: 4.2770 (4.3065)  class_acc: 0.2917 (0.2616)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0888  max mem: 15572
Epoch: [17]  [  70/2809]  eta: 0:30:08  lr: 0.000035  min_lr: 0.000000  loss: 4.2337 (4.3008)  class_acc: 0.2500 (0.2535)  loss_scale: 32768.0000 (34152.5634)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0721  max mem: 15572
Epoch: [17]  [  80/2809]  eta: 0:29:42  lr: 0.000035  min_lr: 0.000000  loss: 4.2337 (4.2986)  class_acc: 0.1667 (0.2526)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1322  max mem: 15572
Epoch: [17]  [  90/2809]  eta: 0:28:45  lr: 0.000034  min_lr: 0.000000  loss: 4.3013 (4.2931)  class_acc: 0.2500 (0.2564)  loss_scale: 32768.0000 (33848.2637)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0989  max mem: 15572
Epoch: [17]  [ 100/2809]  eta: 0:28:28  lr: 0.000034  min_lr: 0.000000  loss: 4.4148 (4.3068)  class_acc: 0.2083 (0.2533)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.1043  max mem: 15572
Epoch: [17]  [ 110/2809]  eta: 0:28:08  lr: 0.000034  min_lr: 0.000000  loss: 4.4520 (4.3196)  class_acc: 0.2083 (0.2515)  loss_scale: 32768.0000 (33653.6216)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1478  max mem: 15572
Epoch: [17]  [ 120/2809]  eta: 0:27:44  lr: 0.000034  min_lr: 0.000000  loss: 4.4090 (4.3248)  class_acc: 0.2083 (0.2531)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1243  max mem: 15572
Epoch: [17]  [ 130/2809]  eta: 0:27:33  lr: 0.000034  min_lr: 0.000000  loss: 4.3543 (4.3198)  class_acc: 0.2083 (0.2500)  loss_scale: 32768.0000 (33518.4122)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1350  max mem: 15572
Epoch: [17]  [ 140/2809]  eta: 0:27:12  lr: 0.000034  min_lr: 0.000000  loss: 4.2361 (4.3096)  class_acc: 0.2500 (0.2479)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1113  max mem: 15572
Epoch: [17]  [ 150/2809]  eta: 0:27:05  lr: 0.000034  min_lr: 0.000000  loss: 4.2702 (4.3142)  class_acc: 0.2500 (0.2475)  loss_scale: 32768.0000 (33419.0199)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1217  max mem: 15572
Epoch: [17]  [ 160/2809]  eta: 0:26:58  lr: 0.000034  min_lr: 0.000000  loss: 4.3371 (4.3153)  class_acc: 0.2500 (0.2482)  loss_scale: 32768.0000 (33378.5839)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1591  max mem: 15572
[2025-01-15 22:35:26,943] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:35:26,943] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 22:35:29,567] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47920
[2025-01-15 22:35:29,568] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:35:29,569] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 170/2809]  eta: 0:26:34  lr: 0.000034  min_lr: 0.000000  loss: 4.3054 (4.3085)  class_acc: 0.2917 (0.2510)  loss_scale: 32768.0000 (34301.0058)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.0889  max mem: 15572
Epoch: [17]  [ 180/2809]  eta: 0:26:25  lr: 0.000034  min_lr: 0.000000  loss: 4.2994 (4.3170)  class_acc: 0.2500 (0.2530)  loss_scale: 32768.0000 (34216.3094)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0942  max mem: 15572
Epoch: [17]  [ 190/2809]  eta: 0:26:19  lr: 0.000034  min_lr: 0.000000  loss: 4.4304 (4.3253)  class_acc: 0.2500 (0.2517)  loss_scale: 32768.0000 (34140.4817)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1577  max mem: 15572
Epoch: [17]  [ 200/2809]  eta: 0:25:59  lr: 0.000034  min_lr: 0.000000  loss: 4.3245 (4.3261)  class_acc: 0.2917 (0.2554)  loss_scale: 32768.0000 (34072.1990)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1127  max mem: 15572
Epoch: [17]  [ 210/2809]  eta: 0:25:49  lr: 0.000034  min_lr: 0.000000  loss: 4.3008 (4.3252)  class_acc: 0.2917 (0.2553)  loss_scale: 32768.0000 (34010.3886)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0810  max mem: 15572
Epoch: [17]  [ 220/2809]  eta: 0:25:34  lr: 0.000034  min_lr: 0.000000  loss: 4.2198 (4.3161)  class_acc: 0.2083 (0.2564)  loss_scale: 32768.0000 (33954.1719)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0747  max mem: 15572
Epoch: [17]  [ 230/2809]  eta: 0:25:16  lr: 0.000034  min_lr: 0.000000  loss: 4.3436 (4.3178)  class_acc: 0.2083 (0.2556)  loss_scale: 32768.0000 (33902.8225)  weight_decay: 0.0500 (0.0500)  time: 0.5031  data: 0.0505  max mem: 15572
Epoch: [17]  [ 240/2809]  eta: 0:25:16  lr: 0.000034  min_lr: 0.000000  loss: 4.3833 (4.3235)  class_acc: 0.2083 (0.2547)  loss_scale: 32768.0000 (33855.7344)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1288  max mem: 15572
[2025-01-15 22:36:14,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=48000, skipped=300, lr=[3.332372618498235e-07, 3.332372618498235e-07, 4.7605323121403363e-07, 4.7605323121403363e-07, 6.800760445914767e-07, 6.800760445914767e-07, 9.715372065592525e-07, 9.715372065592525e-07, 1.3879102950846464e-06, 1.3879102950846464e-06, 1.9827289929780666e-06, 1.9827289929780666e-06, 2.8324699899686663e-06, 2.8324699899686663e-06, 4.046385699955239e-06, 4.046385699955239e-06, 5.780550999936054e-06, 5.780550999936054e-06, 8.25792999990865e-06, 8.25792999990865e-06, 1.1797042857012358e-05, 1.1797042857012358e-05, 1.6852918367160514e-05, 1.6852918367160514e-05, 2.4075597667372162e-05, 2.4075597667372162e-05, 3.4393710953388806e-05, 3.4393710953388806e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:36:14,144] [INFO] [timer.py:260:stop] epoch=0/micro_step=48000/global_step=48000, RunningAvgSamplesPerSec=27.696693728207446, CurrSamplesPerSec=28.720044371101267, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [ 250/2809]  eta: 0:25:11  lr: 0.000034  min_lr: 0.000000  loss: 4.3553 (4.3244)  class_acc: 0.2083 (0.2555)  loss_scale: 32768.0000 (33812.3984)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1632  max mem: 15572
Epoch: [17]  [ 260/2809]  eta: 0:25:02  lr: 0.000034  min_lr: 0.000000  loss: 4.3889 (4.3295)  class_acc: 0.2083 (0.2542)  loss_scale: 32768.0000 (33772.3831)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1249  max mem: 15572
Epoch: [17]  [ 270/2809]  eta: 0:24:46  lr: 0.000034  min_lr: 0.000000  loss: 4.4271 (4.3301)  class_acc: 0.2083 (0.2535)  loss_scale: 32768.0000 (33735.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0713  max mem: 15572
[2025-01-15 22:36:29,147] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48027
[2025-01-15 22:36:29,147] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-15 22:36:29,147] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [ 280/2809]  eta: 0:24:36  lr: 0.000034  min_lr: 0.000000  loss: 4.3390 (4.3328)  class_acc: 0.2500 (0.2556)  loss_scale: 32768.0000 (33292.7544)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0606  max mem: 15572
Epoch: [17]  [ 290/2809]  eta: 0:24:26  lr: 0.000034  min_lr: 0.000000  loss: 4.2862 (4.3297)  class_acc: 0.2500 (0.2562)  loss_scale: 16384.0000 (32711.6976)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0872  max mem: 15572
Epoch: [17]  [ 300/2809]  eta: 0:24:25  lr: 0.000034  min_lr: 0.000000  loss: 4.3387 (4.3337)  class_acc: 0.2083 (0.2542)  loss_scale: 16384.0000 (32169.2492)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1332  max mem: 15572
Epoch: [17]  [ 310/2809]  eta: 0:24:18  lr: 0.000034  min_lr: 0.000000  loss: 4.4231 (4.3323)  class_acc: 0.2083 (0.2531)  loss_scale: 16384.0000 (31661.6849)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1616  max mem: 15572
Epoch: [17]  [ 320/2809]  eta: 0:24:06  lr: 0.000034  min_lr: 0.000000  loss: 4.3547 (4.3304)  class_acc: 0.2500 (0.2544)  loss_scale: 16384.0000 (31185.7445)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0877  max mem: 15572
Epoch: [17]  [ 330/2809]  eta: 0:23:55  lr: 0.000034  min_lr: 0.000000  loss: 4.3547 (4.3279)  class_acc: 0.2917 (0.2567)  loss_scale: 16384.0000 (30738.5619)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0545  max mem: 15572
Epoch: [17]  [ 340/2809]  eta: 0:23:46  lr: 0.000034  min_lr: 0.000000  loss: 4.2823 (4.3246)  class_acc: 0.3333 (0.2568)  loss_scale: 16384.0000 (30317.6070)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0534  max mem: 15572
Epoch: [17]  [ 350/2809]  eta: 0:23:36  lr: 0.000034  min_lr: 0.000000  loss: 4.2823 (4.3245)  class_acc: 0.2500 (0.2569)  loss_scale: 16384.0000 (29920.6382)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.0634  max mem: 15572
Epoch: [17]  [ 360/2809]  eta: 0:23:26  lr: 0.000034  min_lr: 0.000000  loss: 4.3693 (4.3275)  class_acc: 0.2500 (0.2563)  loss_scale: 16384.0000 (29545.6620)  weight_decay: 0.0500 (0.0500)  time: 0.5120  data: 0.0717  max mem: 15572
Epoch: [17]  [ 370/2809]  eta: 0:23:17  lr: 0.000034  min_lr: 0.000000  loss: 4.3693 (4.3278)  class_acc: 0.2083 (0.2566)  loss_scale: 16384.0000 (29190.9003)  weight_decay: 0.0500 (0.0500)  time: 0.5241  data: 0.0776  max mem: 15572
Epoch: [17]  [ 380/2809]  eta: 0:23:15  lr: 0.000034  min_lr: 0.000000  loss: 4.2803 (4.3271)  class_acc: 0.2083 (0.2556)  loss_scale: 16384.0000 (28854.7612)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1411  max mem: 15572
Epoch: [17]  [ 390/2809]  eta: 0:23:16  lr: 0.000034  min_lr: 0.000000  loss: 4.2654 (4.3283)  class_acc: 0.2500 (0.2559)  loss_scale: 16384.0000 (28535.8159)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.2082  max mem: 15572
Epoch: [17]  [ 400/2809]  eta: 0:23:08  lr: 0.000034  min_lr: 0.000000  loss: 4.2654 (4.3235)  class_acc: 0.2500 (0.2554)  loss_scale: 16384.0000 (28232.7781)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1781  max mem: 15572
[2025-01-15 22:37:41,236] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:37:41,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 410/2809]  eta: 0:23:01  lr: 0.000034  min_lr: 0.000000  loss: 4.3511 (4.3246)  class_acc: 0.2083 (0.2561)  loss_scale: 16384.0000 (28263.3966)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1156  max mem: 15572
Epoch: [17]  [ 420/2809]  eta: 0:22:58  lr: 0.000034  min_lr: 0.000000  loss: 4.2780 (4.3224)  class_acc: 0.2500 (0.2562)  loss_scale: 32768.0000 (28370.3943)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1438  max mem: 15572
Epoch: [17]  [ 430/2809]  eta: 0:22:51  lr: 0.000034  min_lr: 0.000000  loss: 4.1796 (4.3193)  class_acc: 0.2500 (0.2577)  loss_scale: 32768.0000 (28472.4269)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1507  max mem: 15572
Epoch: [17]  [ 440/2809]  eta: 0:22:37  lr: 0.000034  min_lr: 0.000000  loss: 4.2954 (4.3194)  class_acc: 0.2083 (0.2572)  loss_scale: 32768.0000 (28569.8322)  weight_decay: 0.0500 (0.0500)  time: 0.4935  data: 0.0584  max mem: 15572
Epoch: [17]  [ 450/2809]  eta: 0:22:33  lr: 0.000034  min_lr: 0.000000  loss: 4.2916 (4.3158)  class_acc: 0.2500 (0.2591)  loss_scale: 32768.0000 (28662.9180)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0910  max mem: 15572
Epoch: [17]  [ 460/2809]  eta: 0:22:29  lr: 0.000034  min_lr: 0.000000  loss: 4.1560 (4.3118)  class_acc: 0.2917 (0.2595)  loss_scale: 32768.0000 (28751.9653)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1707  max mem: 15572
Epoch: [17]  [ 470/2809]  eta: 0:22:22  lr: 0.000034  min_lr: 0.000000  loss: 4.2093 (4.3134)  class_acc: 0.2917 (0.2597)  loss_scale: 32768.0000 (28837.2314)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1404  max mem: 15572
Epoch: [17]  [ 480/2809]  eta: 0:22:18  lr: 0.000034  min_lr: 0.000000  loss: 4.3244 (4.3144)  class_acc: 0.2917 (0.2597)  loss_scale: 32768.0000 (28918.9522)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1360  max mem: 15572
Epoch: [17]  [ 490/2809]  eta: 0:22:14  lr: 0.000034  min_lr: 0.000000  loss: 4.3212 (4.3168)  class_acc: 0.2917 (0.2594)  loss_scale: 32768.0000 (28997.3442)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1541  max mem: 15572
Epoch: [17]  [ 500/2809]  eta: 0:22:03  lr: 0.000034  min_lr: 0.000000  loss: 4.4487 (4.3198)  class_acc: 0.2083 (0.2577)  loss_scale: 32768.0000 (29072.6068)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0841  max mem: 15572
Epoch: [17]  [ 510/2809]  eta: 0:21:54  lr: 0.000034  min_lr: 0.000000  loss: 4.3721 (4.3188)  class_acc: 0.2083 (0.2586)  loss_scale: 32768.0000 (29144.9237)  weight_decay: 0.0500 (0.0500)  time: 0.4752  data: 0.0330  max mem: 15572
Epoch: [17]  [ 520/2809]  eta: 0:21:46  lr: 0.000034  min_lr: 0.000000  loss: 4.3721 (4.3197)  class_acc: 0.2083 (0.2571)  loss_scale: 32768.0000 (29214.4645)  weight_decay: 0.0500 (0.0500)  time: 0.5113  data: 0.0779  max mem: 15572
[2025-01-15 22:38:47,641] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48276
[2025-01-15 22:38:47,641] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-15 22:38:47,641] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [ 530/2809]  eta: 0:21:41  lr: 0.000034  min_lr: 0.000000  loss: 4.4495 (4.3221)  class_acc: 0.2083 (0.2572)  loss_scale: 32768.0000 (29034.5461)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1142  max mem: 15572
Epoch: [17]  [ 540/2809]  eta: 0:21:37  lr: 0.000034  min_lr: 0.000000  loss: 4.3878 (4.3239)  class_acc: 0.2083 (0.2562)  loss_scale: 16384.0000 (28800.7098)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1334  max mem: 15572
Epoch: [17]  [ 550/2809]  eta: 0:21:35  lr: 0.000034  min_lr: 0.000000  loss: 4.2238 (4.3197)  class_acc: 0.2500 (0.2570)  loss_scale: 16384.0000 (28575.3612)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.1822  max mem: 15572
Epoch: [17]  [ 560/2809]  eta: 0:21:27  lr: 0.000034  min_lr: 0.000000  loss: 4.1926 (4.3208)  class_acc: 0.2917 (0.2585)  loss_scale: 16384.0000 (28358.0463)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1562  max mem: 15572
Epoch: [17]  [ 570/2809]  eta: 0:21:19  lr: 0.000034  min_lr: 0.000000  loss: 4.2684 (4.3188)  class_acc: 0.3333 (0.2597)  loss_scale: 16384.0000 (28148.3433)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0700  max mem: 15572
Epoch: [17]  [ 580/2809]  eta: 0:21:14  lr: 0.000034  min_lr: 0.000000  loss: 4.3120 (4.3210)  class_acc: 0.2917 (0.2600)  loss_scale: 16384.0000 (27945.8589)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1080  max mem: 15572
Epoch: [17]  [ 590/2809]  eta: 0:21:10  lr: 0.000034  min_lr: 0.000000  loss: 4.4998 (4.3239)  class_acc: 0.2500 (0.2599)  loss_scale: 16384.0000 (27750.2267)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1660  max mem: 15572
Epoch: [17]  [ 600/2809]  eta: 0:21:03  lr: 0.000034  min_lr: 0.000000  loss: 4.4325 (4.3233)  class_acc: 0.2500 (0.2596)  loss_scale: 16384.0000 (27561.1048)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1230  max mem: 15572
Epoch: [17]  [ 610/2809]  eta: 0:20:57  lr: 0.000034  min_lr: 0.000000  loss: 4.2672 (4.3217)  class_acc: 0.2500 (0.2600)  loss_scale: 16384.0000 (27378.1735)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1007  max mem: 15572
Epoch: [17]  [ 620/2809]  eta: 0:20:49  lr: 0.000034  min_lr: 0.000000  loss: 4.2618 (4.3216)  class_acc: 0.2500 (0.2603)  loss_scale: 16384.0000 (27201.1337)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0956  max mem: 15572
Epoch: [17]  [ 630/2809]  eta: 0:20:44  lr: 0.000034  min_lr: 0.000000  loss: 4.4058 (4.3209)  class_acc: 0.2500 (0.2600)  loss_scale: 16384.0000 (27029.7052)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1210  max mem: 15572
Epoch: [17]  [ 640/2809]  eta: 0:20:38  lr: 0.000034  min_lr: 0.000000  loss: 4.4417 (4.3221)  class_acc: 0.2083 (0.2602)  loss_scale: 16384.0000 (26863.6256)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1311  max mem: 15572
Epoch: [17]  [ 650/2809]  eta: 0:20:36  lr: 0.000034  min_lr: 0.000000  loss: 4.3547 (4.3233)  class_acc: 0.2500 (0.2597)  loss_scale: 16384.0000 (26702.6482)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1605  max mem: 15572
[2025-01-15 22:40:01,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:40:01,576] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 660/2809]  eta: 0:20:31  lr: 0.000034  min_lr: 0.000000  loss: 4.3712 (4.3238)  class_acc: 0.2500 (0.2598)  loss_scale: 16384.0000 (26769.6218)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1771  max mem: 15572
Epoch: [17]  [ 670/2809]  eta: 0:20:23  lr: 0.000034  min_lr: 0.000000  loss: 4.3712 (4.3246)  class_acc: 0.2083 (0.2594)  loss_scale: 32768.0000 (26859.0164)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0850  max mem: 15572
Epoch: [17]  [ 680/2809]  eta: 0:20:21  lr: 0.000034  min_lr: 0.000000  loss: 4.3370 (4.3239)  class_acc: 0.2083 (0.2594)  loss_scale: 32768.0000 (26945.7856)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1369  max mem: 15572
Epoch: [17]  [ 690/2809]  eta: 0:20:12  lr: 0.000034  min_lr: 0.000000  loss: 4.5579 (4.3275)  class_acc: 0.2083 (0.2588)  loss_scale: 32768.0000 (27030.0434)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1390  max mem: 15572
Epoch: [17]  [ 700/2809]  eta: 0:20:06  lr: 0.000034  min_lr: 0.000000  loss: 4.4800 (4.3278)  class_acc: 0.2083 (0.2580)  loss_scale: 32768.0000 (27111.8973)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0770  max mem: 15572
Epoch: [17]  [ 710/2809]  eta: 0:19:58  lr: 0.000034  min_lr: 0.000000  loss: 4.3418 (4.3289)  class_acc: 0.2083 (0.2582)  loss_scale: 32768.0000 (27191.4487)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0767  max mem: 15572
Epoch: [17]  [ 720/2809]  eta: 0:19:54  lr: 0.000034  min_lr: 0.000000  loss: 4.3952 (4.3308)  class_acc: 0.2500 (0.2584)  loss_scale: 32768.0000 (27268.7933)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1208  max mem: 15572
Epoch: [17]  [ 730/2809]  eta: 0:19:48  lr: 0.000034  min_lr: 0.000000  loss: 4.3897 (4.3298)  class_acc: 0.2500 (0.2583)  loss_scale: 32768.0000 (27344.0219)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1528  max mem: 15572
Epoch: [17]  [ 740/2809]  eta: 0:19:43  lr: 0.000034  min_lr: 0.000000  loss: 4.2989 (4.3302)  class_acc: 0.2500 (0.2583)  loss_scale: 32768.0000 (27417.2200)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1356  max mem: 15572
Epoch: [17]  [ 750/2809]  eta: 0:19:38  lr: 0.000034  min_lr: 0.000000  loss: 4.3481 (4.3327)  class_acc: 0.2500 (0.2585)  loss_scale: 32768.0000 (27488.4687)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1629  max mem: 15572
Epoch: [17]  [ 760/2809]  eta: 0:19:32  lr: 0.000034  min_lr: 0.000000  loss: 4.3591 (4.3311)  class_acc: 0.2500 (0.2584)  loss_scale: 32768.0000 (27557.8449)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1350  max mem: 15572
Epoch: [17]  [ 770/2809]  eta: 0:19:28  lr: 0.000034  min_lr: 0.000000  loss: 4.3254 (4.3309)  class_acc: 0.2500 (0.2586)  loss_scale: 32768.0000 (27625.4215)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1303  max mem: 15572
[2025-01-15 22:41:14,812] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:41:14,812] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [ 780/2809]  eta: 0:19:21  lr: 0.000034  min_lr: 0.000000  loss: 4.3254 (4.3295)  class_acc: 0.2500 (0.2585)  loss_scale: 32768.0000 (27733.2241)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1184  max mem: 15572
Epoch: [17]  [ 790/2809]  eta: 0:19:14  lr: 0.000034  min_lr: 0.000000  loss: 4.2034 (4.3290)  class_acc: 0.2083 (0.2580)  loss_scale: 65536.0000 (28211.1353)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0775  max mem: 15572
[2025-01-15 22:41:23,113] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48546
[2025-01-15 22:41:23,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:41:23,114] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 800/2809]  eta: 0:19:09  lr: 0.000034  min_lr: 0.000000  loss: 4.2381 (4.3278)  class_acc: 0.2500 (0.2585)  loss_scale: 65536.0000 (28349.8427)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1178  max mem: 15572
Epoch: [17]  [ 810/2809]  eta: 0:19:02  lr: 0.000034  min_lr: 0.000000  loss: 4.3848 (4.3283)  class_acc: 0.2500 (0.2582)  loss_scale: 32768.0000 (28404.3206)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1310  max mem: 15572
Epoch: [17]  [ 820/2809]  eta: 0:18:55  lr: 0.000034  min_lr: 0.000000  loss: 4.3234 (4.3266)  class_acc: 0.2917 (0.2588)  loss_scale: 32768.0000 (28457.4714)  weight_decay: 0.0500 (0.0500)  time: 0.5104  data: 0.0699  max mem: 15572
Epoch: [17]  [ 830/2809]  eta: 0:18:46  lr: 0.000034  min_lr: 0.000000  loss: 4.1632 (4.3267)  class_acc: 0.2500 (0.2584)  loss_scale: 32768.0000 (28509.3430)  weight_decay: 0.0500 (0.0500)  time: 0.4675  data: 0.0169  max mem: 15572
Epoch: [17]  [ 840/2809]  eta: 0:18:38  lr: 0.000034  min_lr: 0.000000  loss: 4.3139 (4.3264)  class_acc: 0.2083 (0.2582)  loss_scale: 32768.0000 (28559.9810)  weight_decay: 0.0500 (0.0500)  time: 0.4607  data: 0.0006  max mem: 15572
Epoch: [17]  [ 850/2809]  eta: 0:18:36  lr: 0.000034  min_lr: 0.000000  loss: 4.3686 (4.3262)  class_acc: 0.2500 (0.2579)  loss_scale: 32768.0000 (28609.4289)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1171  max mem: 15572
Epoch: [17]  [ 860/2809]  eta: 0:18:27  lr: 0.000034  min_lr: 0.000000  loss: 4.3138 (4.3264)  class_acc: 0.2500 (0.2581)  loss_scale: 32768.0000 (28657.7282)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1223  max mem: 15572
Epoch: [17]  [ 870/2809]  eta: 0:18:21  lr: 0.000034  min_lr: 0.000000  loss: 4.4214 (4.3271)  class_acc: 0.2500 (0.2577)  loss_scale: 32768.0000 (28704.9185)  weight_decay: 0.0500 (0.0500)  time: 0.4971  data: 0.0460  max mem: 15572
Epoch: [17]  [ 880/2809]  eta: 0:18:14  lr: 0.000034  min_lr: 0.000000  loss: 4.4483 (4.3282)  class_acc: 0.2083 (0.2574)  loss_scale: 32768.0000 (28751.0375)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0843  max mem: 15572
Epoch: [17]  [ 890/2809]  eta: 0:18:10  lr: 0.000034  min_lr: 0.000000  loss: 4.2051 (4.3262)  class_acc: 0.2500 (0.2573)  loss_scale: 32768.0000 (28796.1212)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1536  max mem: 15572
Epoch: [17]  [ 900/2809]  eta: 0:18:05  lr: 0.000034  min_lr: 0.000000  loss: 4.2592 (4.3274)  class_acc: 0.2917 (0.2574)  loss_scale: 32768.0000 (28840.2042)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1904  max mem: 15572
Epoch: [17]  [ 910/2809]  eta: 0:18:00  lr: 0.000034  min_lr: 0.000000  loss: 4.3801 (4.3267)  class_acc: 0.2917 (0.2579)  loss_scale: 32768.0000 (28883.3194)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1468  max mem: 15572
Epoch: [17]  [ 920/2809]  eta: 0:17:56  lr: 0.000034  min_lr: 0.000000  loss: 4.2019 (4.3258)  class_acc: 0.2500 (0.2584)  loss_scale: 32768.0000 (28925.4984)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1771  max mem: 15572
[2025-01-15 22:42:33,814] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:42:33,814] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [ 930/2809]  eta: 0:17:50  lr: 0.000034  min_lr: 0.000000  loss: 4.2988 (4.3251)  class_acc: 0.2500 (0.2584)  loss_scale: 32768.0000 (29283.5403)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1502  max mem: 15572
Epoch: [17]  [ 940/2809]  eta: 0:17:43  lr: 0.000034  min_lr: 0.000000  loss: 4.2724 (4.3245)  class_acc: 0.2500 (0.2587)  loss_scale: 65536.0000 (29668.7949)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0881  max mem: 15572
[2025-01-15 22:42:48,650] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48700
[2025-01-15 22:42:48,650] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:42:48,650] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 950/2809]  eta: 0:17:39  lr: 0.000034  min_lr: 0.000000  loss: 4.2724 (4.3244)  class_acc: 0.2083 (0.2583)  loss_scale: 65536.0000 (29908.1220)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1616  max mem: 15572
Epoch: [17]  [ 960/2809]  eta: 0:17:32  lr: 0.000034  min_lr: 0.000000  loss: 4.3061 (4.3239)  class_acc: 0.2500 (0.2591)  loss_scale: 32768.0000 (29937.8814)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1250  max mem: 15572
Epoch: [17]  [ 970/2809]  eta: 0:17:26  lr: 0.000034  min_lr: 0.000000  loss: 4.1997 (4.3223)  class_acc: 0.2917 (0.2597)  loss_scale: 32768.0000 (29967.0278)  weight_decay: 0.0500 (0.0500)  time: 0.5176  data: 0.0607  max mem: 15572
Epoch: [17]  [ 980/2809]  eta: 0:17:22  lr: 0.000034  min_lr: 0.000000  loss: 4.1789 (4.3225)  class_acc: 0.2917 (0.2599)  loss_scale: 32768.0000 (29995.5800)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1375  max mem: 15572
Epoch: [17]  [ 990/2809]  eta: 0:17:18  lr: 0.000034  min_lr: 0.000000  loss: 4.2969 (4.3224)  class_acc: 0.2500 (0.2601)  loss_scale: 32768.0000 (30023.5560)  weight_decay: 0.0500 (0.0500)  time: 0.6564  data: 0.2028  max mem: 15572
Epoch: [17]  [1000/2809]  eta: 0:17:12  lr: 0.000034  min_lr: 0.000000  loss: 4.3166 (4.3233)  class_acc: 0.2083 (0.2593)  loss_scale: 32768.0000 (30050.9730)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.1880  max mem: 15572
Epoch: [17]  [1010/2809]  eta: 0:17:05  lr: 0.000034  min_lr: 0.000000  loss: 4.3185 (4.3225)  class_acc: 0.2083 (0.2592)  loss_scale: 32768.0000 (30077.8477)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.1016  max mem: 15572
Epoch: [17]  [1020/2809]  eta: 0:16:59  lr: 0.000034  min_lr: 0.000000  loss: 4.2962 (4.3211)  class_acc: 0.2917 (0.2604)  loss_scale: 32768.0000 (30104.1959)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0649  max mem: 15572
Epoch: [17]  [1030/2809]  eta: 0:16:52  lr: 0.000034  min_lr: 0.000000  loss: 4.2501 (4.3213)  class_acc: 0.2500 (0.2603)  loss_scale: 32768.0000 (30130.0330)  weight_decay: 0.0500 (0.0500)  time: 0.5132  data: 0.0808  max mem: 15572
Epoch: [17]  [1040/2809]  eta: 0:16:47  lr: 0.000034  min_lr: 0.000000  loss: 4.2086 (4.3194)  class_acc: 0.2500 (0.2604)  loss_scale: 32768.0000 (30155.3737)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1182  max mem: 15572
Epoch: [17]  [1050/2809]  eta: 0:16:41  lr: 0.000034  min_lr: 0.000000  loss: 4.2112 (4.3203)  class_acc: 0.2500 (0.2600)  loss_scale: 32768.0000 (30180.2322)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1370  max mem: 15572
Epoch: [17]  [1060/2809]  eta: 0:16:36  lr: 0.000034  min_lr: 0.000000  loss: 4.4017 (4.3206)  class_acc: 0.2500 (0.2598)  loss_scale: 32768.0000 (30204.6221)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1250  max mem: 15572
Epoch: [17]  [1070/2809]  eta: 0:16:29  lr: 0.000034  min_lr: 0.000000  loss: 4.2396 (4.3196)  class_acc: 0.2917 (0.2606)  loss_scale: 32768.0000 (30228.5565)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0840  max mem: 15572
[2025-01-15 22:44:01,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:44:01,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [1080/2809]  eta: 0:16:23  lr: 0.000034  min_lr: 0.000000  loss: 4.2181 (4.3186)  class_acc: 0.3333 (0.2610)  loss_scale: 32768.0000 (30403.6115)  weight_decay: 0.0500 (0.0500)  time: 0.5187  data: 0.0834  max mem: 15572
Epoch: [17]  [1090/2809]  eta: 0:16:17  lr: 0.000034  min_lr: 0.000000  loss: 4.2809 (4.3179)  class_acc: 0.2500 (0.2612)  loss_scale: 65536.0000 (30725.6315)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1215  max mem: 15572
Epoch: [17]  [1100/2809]  eta: 0:16:10  lr: 0.000034  min_lr: 0.000000  loss: 4.2764 (4.3168)  class_acc: 0.2500 (0.2614)  loss_scale: 65536.0000 (31041.8020)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0879  max mem: 15572
Epoch: [17]  [1110/2809]  eta: 0:16:04  lr: 0.000034  min_lr: 0.000000  loss: 4.2743 (4.3160)  class_acc: 0.2083 (0.2611)  loss_scale: 65536.0000 (31352.2808)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0839  max mem: 15572
Epoch: [17]  [1120/2809]  eta: 0:15:59  lr: 0.000034  min_lr: 0.000000  loss: 4.1615 (4.3141)  class_acc: 0.2500 (0.2610)  loss_scale: 65536.0000 (31657.2203)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1235  max mem: 15572
Epoch: [17]  [1130/2809]  eta: 0:15:55  lr: 0.000034  min_lr: 0.000000  loss: 4.3133 (4.3159)  class_acc: 0.2083 (0.2605)  loss_scale: 65536.0000 (31956.7675)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1628  max mem: 15572
Epoch: [17]  [1140/2809]  eta: 0:15:48  lr: 0.000034  min_lr: 0.000000  loss: 4.3891 (4.3155)  class_acc: 0.2083 (0.2603)  loss_scale: 65536.0000 (32251.0640)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1394  max mem: 15572
Epoch: [17]  [1150/2809]  eta: 0:15:43  lr: 0.000034  min_lr: 0.000000  loss: 4.2445 (4.3150)  class_acc: 0.2500 (0.2602)  loss_scale: 65536.0000 (32540.2467)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1024  max mem: 15572
Epoch: [17]  [1160/2809]  eta: 0:15:38  lr: 0.000034  min_lr: 0.000000  loss: 4.2445 (4.3150)  class_acc: 0.2083 (0.2597)  loss_scale: 65536.0000 (32824.4479)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1246  max mem: 15572
Epoch: [17]  [1170/2809]  eta: 0:15:32  lr: 0.000034  min_lr: 0.000000  loss: 4.4157 (4.3164)  class_acc: 0.2083 (0.2596)  loss_scale: 65536.0000 (33103.7950)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1359  max mem: 15572
Epoch: [17]  [1180/2809]  eta: 0:15:26  lr: 0.000034  min_lr: 0.000000  loss: 4.4931 (4.3178)  class_acc: 0.2500 (0.2595)  loss_scale: 65536.0000 (33378.4115)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1197  max mem: 15572
Epoch: [17]  [1190/2809]  eta: 0:15:19  lr: 0.000034  min_lr: 0.000000  loss: 4.4814 (4.3184)  class_acc: 0.2083 (0.2591)  loss_scale: 65536.0000 (33648.4165)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0773  max mem: 15572
Epoch: [17]  [1200/2809]  eta: 0:15:14  lr: 0.000034  min_lr: 0.000000  loss: 4.3308 (4.3181)  class_acc: 0.2083 (0.2595)  loss_scale: 65536.0000 (33913.9251)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.1094  max mem: 15572
[2025-01-15 22:45:12,373] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:45:12,374] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:45:13,299] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48959
[2025-01-15 22:45:13,299] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:45:13,299] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1210/2809]  eta: 0:15:08  lr: 0.000034  min_lr: 0.000000  loss: 4.3357 (4.3180)  class_acc: 0.2917 (0.2598)  loss_scale: 65536.0000 (34283.2832)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1120  max mem: 15572
Epoch: [17]  [1220/2809]  eta: 0:15:02  lr: 0.000034  min_lr: 0.000000  loss: 4.3360 (4.3175)  class_acc: 0.2917 (0.2600)  loss_scale: 65536.0000 (34539.2432)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1030  max mem: 15572
Epoch: [17]  [1230/2809]  eta: 0:14:56  lr: 0.000034  min_lr: 0.000000  loss: 4.4133 (4.3196)  class_acc: 0.2083 (0.2596)  loss_scale: 65536.0000 (34791.0447)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1282  max mem: 15572
Epoch: [17]  [1240/2809]  eta: 0:14:50  lr: 0.000034  min_lr: 0.000000  loss: 4.3549 (4.3186)  class_acc: 0.2083 (0.2600)  loss_scale: 65536.0000 (35038.7881)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0773  max mem: 15572
[2025-01-15 22:45:35,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=49000, skipped=305, lr=[3.267739872028712e-07, 3.267739872028712e-07, 4.6681998171838747e-07, 4.6681998171838747e-07, 6.66885688169125e-07, 6.66885688169125e-07, 9.526938402416073e-07, 9.526938402416073e-07, 1.3609912003451532e-06, 1.3609912003451532e-06, 1.944273143350219e-06, 1.944273143350219e-06, 2.7775330619288843e-06, 2.7775330619288843e-06, 3.967904374184121e-06, 3.967904374184121e-06, 5.66843482026303e-06, 5.66843482026303e-06, 8.097764028947187e-06, 8.097764028947187e-06, 1.1568234327067409e-05, 1.1568234327067409e-05, 1.652604903866773e-05, 1.652604903866773e-05, 2.3608641483811045e-05, 2.3608641483811045e-05, 3.3726630691158636e-05, 3.3726630691158636e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:45:35,057] [INFO] [timer.py:260:stop] epoch=0/micro_step=49000/global_step=49000, RunningAvgSamplesPerSec=27.711107327291188, CurrSamplesPerSec=26.706055703622894, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [1250/2809]  eta: 0:14:45  lr: 0.000034  min_lr: 0.000000  loss: 4.2583 (4.3190)  class_acc: 0.2917 (0.2603)  loss_scale: 65536.0000 (35282.5707)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0864  max mem: 15572
Epoch: [17]  [1260/2809]  eta: 0:14:39  lr: 0.000034  min_lr: 0.000000  loss: 4.3959 (4.3203)  class_acc: 0.2500 (0.2598)  loss_scale: 65536.0000 (35522.4869)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1169  max mem: 15572
[2025-01-15 22:45:44,604] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49015
[2025-01-15 22:45:44,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:45:44,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [1270/2809]  eta: 0:14:33  lr: 0.000034  min_lr: 0.000000  loss: 4.4109 (4.3206)  class_acc: 0.2500 (0.2601)  loss_scale: 65536.0000 (35526.5964)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1140  max mem: 15572
Epoch: [17]  [1280/2809]  eta: 0:14:27  lr: 0.000034  min_lr: 0.000000  loss: 4.3700 (4.3199)  class_acc: 0.2917 (0.2604)  loss_scale: 32768.0000 (35505.0617)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0992  max mem: 15572
Epoch: [17]  [1290/2809]  eta: 0:14:21  lr: 0.000034  min_lr: 0.000000  loss: 4.2409 (4.3194)  class_acc: 0.2917 (0.2607)  loss_scale: 32768.0000 (35483.8606)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0792  max mem: 15572
Epoch: [17]  [1300/2809]  eta: 0:14:16  lr: 0.000034  min_lr: 0.000000  loss: 4.3964 (4.3203)  class_acc: 0.2083 (0.2601)  loss_scale: 32768.0000 (35462.9854)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1197  max mem: 15572
Epoch: [17]  [1310/2809]  eta: 0:14:10  lr: 0.000034  min_lr: 0.000000  loss: 4.4984 (4.3213)  class_acc: 0.2083 (0.2603)  loss_scale: 32768.0000 (35442.4287)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1414  max mem: 15572
Epoch: [17]  [1320/2809]  eta: 0:14:05  lr: 0.000034  min_lr: 0.000000  loss: 4.2813 (4.3206)  class_acc: 0.2500 (0.2602)  loss_scale: 32768.0000 (35422.1832)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1239  max mem: 15572
Epoch: [17]  [1330/2809]  eta: 0:13:59  lr: 0.000034  min_lr: 0.000000  loss: 4.2282 (4.3209)  class_acc: 0.2500 (0.2602)  loss_scale: 32768.0000 (35402.2419)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1510  max mem: 15572
Epoch: [17]  [1340/2809]  eta: 0:13:53  lr: 0.000034  min_lr: 0.000000  loss: 4.3579 (4.3210)  class_acc: 0.2500 (0.2602)  loss_scale: 32768.0000 (35382.5981)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1288  max mem: 15572
Epoch: [17]  [1350/2809]  eta: 0:13:48  lr: 0.000034  min_lr: 0.000000  loss: 4.3579 (4.3210)  class_acc: 0.2500 (0.2602)  loss_scale: 32768.0000 (35363.2450)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1078  max mem: 15572
Epoch: [17]  [1360/2809]  eta: 0:13:42  lr: 0.000034  min_lr: 0.000000  loss: 4.4129 (4.3221)  class_acc: 0.2083 (0.2602)  loss_scale: 32768.0000 (35344.1763)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1397  max mem: 15572
Epoch: [17]  [1370/2809]  eta: 0:13:37  lr: 0.000034  min_lr: 0.000000  loss: 4.3575 (4.3208)  class_acc: 0.2917 (0.2608)  loss_scale: 32768.0000 (35325.3858)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1713  max mem: 15572
Epoch: [17]  [1380/2809]  eta: 0:13:31  lr: 0.000034  min_lr: 0.000000  loss: 4.2224 (4.3214)  class_acc: 0.2917 (0.2609)  loss_scale: 32768.0000 (35306.8675)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1244  max mem: 15572
Epoch: [17]  [1390/2809]  eta: 0:13:25  lr: 0.000034  min_lr: 0.000000  loss: 4.3022 (4.3220)  class_acc: 0.2917 (0.2612)  loss_scale: 32768.0000 (35288.6154)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0590  max mem: 15572
[2025-01-15 22:46:58,044] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:46:58,044] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [1400/2809]  eta: 0:13:20  lr: 0.000034  min_lr: 0.000000  loss: 4.2275 (4.3215)  class_acc: 0.2500 (0.2615)  loss_scale: 32768.0000 (35504.5139)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1141  max mem: 15572
Epoch: [17]  [1410/2809]  eta: 0:13:14  lr: 0.000034  min_lr: 0.000000  loss: 4.3673 (4.3226)  class_acc: 0.2500 (0.2612)  loss_scale: 65536.0000 (35717.3522)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1405  max mem: 15572
Epoch: [17]  [1420/2809]  eta: 0:13:08  lr: 0.000034  min_lr: 0.000000  loss: 4.3660 (4.3228)  class_acc: 0.2500 (0.2613)  loss_scale: 65536.0000 (35927.1949)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1348  max mem: 15572
[2025-01-15 22:47:18,753] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49179
[2025-01-15 22:47:18,753] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:47:18,753] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [1430/2809]  eta: 0:13:03  lr: 0.000034  min_lr: 0.000000  loss: 4.3438 (4.3227)  class_acc: 0.2917 (0.2615)  loss_scale: 65536.0000 (36019.6115)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1215  max mem: 15572
Epoch: [17]  [1440/2809]  eta: 0:12:57  lr: 0.000034  min_lr: 0.000000  loss: 4.4046 (4.3229)  class_acc: 0.2083 (0.2612)  loss_scale: 32768.0000 (35997.0465)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1093  max mem: 15572
Epoch: [17]  [1450/2809]  eta: 0:12:51  lr: 0.000034  min_lr: 0.000000  loss: 4.3223 (4.3222)  class_acc: 0.2083 (0.2612)  loss_scale: 32768.0000 (35974.7926)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1160  max mem: 15572
Epoch: [17]  [1460/2809]  eta: 0:12:46  lr: 0.000034  min_lr: 0.000000  loss: 4.3437 (4.3232)  class_acc: 0.2083 (0.2608)  loss_scale: 32768.0000 (35952.8433)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1333  max mem: 15572
Epoch: [17]  [1470/2809]  eta: 0:12:40  lr: 0.000034  min_lr: 0.000000  loss: 4.3522 (4.3228)  class_acc: 0.2500 (0.2609)  loss_scale: 32768.0000 (35931.1924)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1282  max mem: 15572
Epoch: [17]  [1480/2809]  eta: 0:12:34  lr: 0.000034  min_lr: 0.000000  loss: 4.3029 (4.3222)  class_acc: 0.2500 (0.2610)  loss_scale: 32768.0000 (35909.8339)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.1025  max mem: 15572
Epoch: [17]  [1490/2809]  eta: 0:12:28  lr: 0.000034  min_lr: 0.000000  loss: 4.2784 (4.3223)  class_acc: 0.2917 (0.2612)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1340  max mem: 15572
Epoch: [17]  [1500/2809]  eta: 0:12:23  lr: 0.000034  min_lr: 0.000000  loss: 4.2287 (4.3221)  class_acc: 0.3333 (0.2618)  loss_scale: 32768.0000 (35867.9707)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1406  max mem: 15572
Epoch: [17]  [1510/2809]  eta: 0:12:17  lr: 0.000034  min_lr: 0.000000  loss: 4.3223 (4.3231)  class_acc: 0.2917 (0.2618)  loss_scale: 32768.0000 (35847.4547)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.1067  max mem: 15572
Epoch: [17]  [1520/2809]  eta: 0:12:11  lr: 0.000034  min_lr: 0.000000  loss: 4.4595 (4.3238)  class_acc: 0.2500 (0.2619)  loss_scale: 32768.0000 (35827.2084)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1311  max mem: 15572
Epoch: [17]  [1530/2809]  eta: 0:12:06  lr: 0.000034  min_lr: 0.000000  loss: 4.4595 (4.3239)  class_acc: 0.2083 (0.2617)  loss_scale: 32768.0000 (35807.2266)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1736  max mem: 15572
Epoch: [17]  [1540/2809]  eta: 0:12:01  lr: 0.000034  min_lr: 0.000000  loss: 4.4127 (4.3241)  class_acc: 0.2500 (0.2620)  loss_scale: 32768.0000 (35787.5042)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.1674  max mem: 15572
Epoch: [17]  [1550/2809]  eta: 0:11:54  lr: 0.000034  min_lr: 0.000000  loss: 4.2590 (4.3243)  class_acc: 0.2500 (0.2620)  loss_scale: 32768.0000 (35768.0361)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1030  max mem: 15572
[2025-01-15 22:48:31,255] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:48:31,256] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [1560/2809]  eta: 0:11:48  lr: 0.000034  min_lr: 0.000000  loss: 4.2522 (4.3243)  class_acc: 0.2500 (0.2621)  loss_scale: 32768.0000 (35874.7675)  weight_decay: 0.0500 (0.0500)  time: 0.4643  data: 0.0425  max mem: 15572
[2025-01-15 22:48:33,798] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49314
[2025-01-15 22:48:33,799] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:48:33,799] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [1570/2809]  eta: 0:11:43  lr: 0.000034  min_lr: 0.000000  loss: 4.3743 (4.3239)  class_acc: 0.2917 (0.2624)  loss_scale: 32768.0000 (35854.9917)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1169  max mem: 15572
Epoch: [17]  [1580/2809]  eta: 0:11:37  lr: 0.000034  min_lr: 0.000000  loss: 4.2794 (4.3239)  class_acc: 0.2917 (0.2627)  loss_scale: 32768.0000 (35835.4662)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1484  max mem: 15572
Epoch: [17]  [1590/2809]  eta: 0:11:31  lr: 0.000033  min_lr: 0.000000  loss: 4.2794 (4.3238)  class_acc: 0.2917 (0.2627)  loss_scale: 32768.0000 (35816.1860)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0988  max mem: 15572
Epoch: [17]  [1600/2809]  eta: 0:11:26  lr: 0.000033  min_lr: 0.000000  loss: 4.3311 (4.3242)  class_acc: 0.2917 (0.2629)  loss_scale: 32768.0000 (35797.1468)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1471  max mem: 15572
Epoch: [17]  [1610/2809]  eta: 0:11:20  lr: 0.000033  min_lr: 0.000000  loss: 4.3673 (4.3249)  class_acc: 0.2500 (0.2626)  loss_scale: 32768.0000 (35778.3439)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1369  max mem: 15572
Epoch: [17]  [1620/2809]  eta: 0:11:15  lr: 0.000033  min_lr: 0.000000  loss: 4.4461 (4.3256)  class_acc: 0.1667 (0.2620)  loss_scale: 32768.0000 (35759.7730)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1199  max mem: 15572
Epoch: [17]  [1630/2809]  eta: 0:11:09  lr: 0.000033  min_lr: 0.000000  loss: 4.4398 (4.3258)  class_acc: 0.1667 (0.2617)  loss_scale: 32768.0000 (35741.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1513  max mem: 15572
Epoch: [17]  [1640/2809]  eta: 0:11:03  lr: 0.000033  min_lr: 0.000000  loss: 4.3058 (4.3254)  class_acc: 0.2083 (0.2614)  loss_scale: 32768.0000 (35723.3102)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1122  max mem: 15572
Epoch: [17]  [1650/2809]  eta: 0:10:57  lr: 0.000033  min_lr: 0.000000  loss: 4.3585 (4.3262)  class_acc: 0.2083 (0.2614)  loss_scale: 32768.0000 (35705.4101)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.1015  max mem: 15572
Epoch: [17]  [1660/2809]  eta: 0:10:52  lr: 0.000033  min_lr: 0.000000  loss: 4.3585 (4.3259)  class_acc: 0.2083 (0.2611)  loss_scale: 32768.0000 (35687.7255)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1356  max mem: 15572
Epoch: [17]  [1670/2809]  eta: 0:10:46  lr: 0.000033  min_lr: 0.000000  loss: 4.3000 (4.3259)  class_acc: 0.2500 (0.2612)  loss_scale: 32768.0000 (35670.2525)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1066  max mem: 15572
Epoch: [17]  [1680/2809]  eta: 0:10:41  lr: 0.000033  min_lr: 0.000000  loss: 4.3439 (4.3258)  class_acc: 0.2917 (0.2618)  loss_scale: 32768.0000 (35652.9875)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1005  max mem: 15572
[2025-01-15 22:49:48,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:49:48,073] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [1690/2809]  eta: 0:10:35  lr: 0.000033  min_lr: 0.000000  loss: 4.3955 (4.3260)  class_acc: 0.2917 (0.2616)  loss_scale: 32768.0000 (35655.3046)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1031  max mem: 15572
Epoch: [17]  [1700/2809]  eta: 0:10:29  lr: 0.000033  min_lr: 0.000000  loss: 4.4198 (4.3262)  class_acc: 0.2083 (0.2612)  loss_scale: 65536.0000 (35830.9700)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0851  max mem: 15572
Epoch: [17]  [1710/2809]  eta: 0:10:24  lr: 0.000033  min_lr: 0.000000  loss: 4.4198 (4.3267)  class_acc: 0.2083 (0.2613)  loss_scale: 65536.0000 (36004.5821)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1490  max mem: 15572
Epoch: [17]  [1720/2809]  eta: 0:10:18  lr: 0.000033  min_lr: 0.000000  loss: 4.3563 (4.3264)  class_acc: 0.3333 (0.2616)  loss_scale: 65536.0000 (36176.1766)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1730  max mem: 15572
Epoch: [17]  [1730/2809]  eta: 0:10:13  lr: 0.000033  min_lr: 0.000000  loss: 4.1516 (4.3262)  class_acc: 0.2083 (0.2611)  loss_scale: 65536.0000 (36345.7886)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1671  max mem: 15572
[2025-01-15 22:50:15,492] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49488
[2025-01-15 22:50:15,493] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:50:15,493] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [1740/2809]  eta: 0:10:07  lr: 0.000033  min_lr: 0.000000  loss: 4.2622 (4.3256)  class_acc: 0.2083 (0.2612)  loss_scale: 65536.0000 (36400.5238)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1439  max mem: 15572
Epoch: [17]  [1750/2809]  eta: 0:10:01  lr: 0.000033  min_lr: 0.000000  loss: 4.3445 (4.3257)  class_acc: 0.2500 (0.2614)  loss_scale: 32768.0000 (36379.7784)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0822  max mem: 15572
Epoch: [17]  [1760/2809]  eta: 0:09:56  lr: 0.000033  min_lr: 0.000000  loss: 4.4511 (4.3264)  class_acc: 0.2083 (0.2608)  loss_scale: 32768.0000 (36359.2686)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1240  max mem: 15572
Epoch: [17]  [1770/2809]  eta: 0:09:50  lr: 0.000033  min_lr: 0.000000  loss: 4.3494 (4.3262)  class_acc: 0.1667 (0.2606)  loss_scale: 32768.0000 (36338.9904)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1578  max mem: 15572
Epoch: [17]  [1780/2809]  eta: 0:09:45  lr: 0.000033  min_lr: 0.000000  loss: 4.2408 (4.3256)  class_acc: 0.2500 (0.2610)  loss_scale: 32768.0000 (36318.9399)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1388  max mem: 15572
Epoch: [17]  [1790/2809]  eta: 0:09:38  lr: 0.000033  min_lr: 0.000000  loss: 4.2185 (4.3250)  class_acc: 0.2917 (0.2610)  loss_scale: 32768.0000 (36299.1133)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0755  max mem: 15572
Epoch: [17]  [1800/2809]  eta: 0:09:33  lr: 0.000033  min_lr: 0.000000  loss: 4.3652 (4.3251)  class_acc: 0.2500 (0.2608)  loss_scale: 32768.0000 (36279.5069)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.1090  max mem: 15572
Epoch: [17]  [1810/2809]  eta: 0:09:27  lr: 0.000033  min_lr: 0.000000  loss: 4.3482 (4.3243)  class_acc: 0.2500 (0.2613)  loss_scale: 32768.0000 (36260.1171)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1744  max mem: 15572
Epoch: [17]  [1820/2809]  eta: 0:09:22  lr: 0.000033  min_lr: 0.000000  loss: 4.1044 (4.3239)  class_acc: 0.2917 (0.2614)  loss_scale: 32768.0000 (36240.9401)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1173  max mem: 15572
Epoch: [17]  [1830/2809]  eta: 0:09:15  lr: 0.000033  min_lr: 0.000000  loss: 4.2858 (4.3240)  class_acc: 0.2083 (0.2611)  loss_scale: 32768.0000 (36221.9727)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0592  max mem: 15572
Epoch: [17]  [1840/2809]  eta: 0:09:10  lr: 0.000033  min_lr: 0.000000  loss: 4.3829 (4.3242)  class_acc: 0.2083 (0.2612)  loss_scale: 32768.0000 (36203.2113)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0648  max mem: 15572
Epoch: [17]  [1850/2809]  eta: 0:09:04  lr: 0.000033  min_lr: 0.000000  loss: 4.2932 (4.3238)  class_acc: 0.2500 (0.2611)  loss_scale: 32768.0000 (36184.6526)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1358  max mem: 15572
Epoch: [17]  [1860/2809]  eta: 0:08:58  lr: 0.000033  min_lr: 0.000000  loss: 4.2932 (4.3239)  class_acc: 0.2083 (0.2609)  loss_scale: 32768.0000 (36166.2934)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1312  max mem: 15572
[2025-01-15 22:51:28,859] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:51:28,860] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [1870/2809]  eta: 0:08:53  lr: 0.000033  min_lr: 0.000000  loss: 4.3102 (4.3235)  class_acc: 0.2500 (0.2612)  loss_scale: 32768.0000 (36270.7258)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1559  max mem: 15572
Epoch: [17]  [1880/2809]  eta: 0:08:47  lr: 0.000033  min_lr: 0.000000  loss: 4.2298 (4.3230)  class_acc: 0.2917 (0.2614)  loss_scale: 65536.0000 (36426.3094)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1635  max mem: 15572
Epoch: [17]  [1890/2809]  eta: 0:08:42  lr: 0.000033  min_lr: 0.000000  loss: 4.2091 (4.3226)  class_acc: 0.2917 (0.2614)  loss_scale: 65536.0000 (36580.2475)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1191  max mem: 15572
Epoch: [17]  [1900/2809]  eta: 0:08:36  lr: 0.000033  min_lr: 0.000000  loss: 4.3418 (4.3231)  class_acc: 0.2500 (0.2612)  loss_scale: 65536.0000 (36732.5660)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0728  max mem: 15572
Epoch: [17]  [1910/2809]  eta: 0:08:30  lr: 0.000033  min_lr: 0.000000  loss: 4.3659 (4.3227)  class_acc: 0.2500 (0.2614)  loss_scale: 65536.0000 (36883.2904)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0852  max mem: 15572
Epoch: [17]  [1920/2809]  eta: 0:08:25  lr: 0.000033  min_lr: 0.000000  loss: 4.2254 (4.3230)  class_acc: 0.2917 (0.2615)  loss_scale: 65536.0000 (37032.4456)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1431  max mem: 15572
Epoch: [17]  [1930/2809]  eta: 0:08:19  lr: 0.000033  min_lr: 0.000000  loss: 4.3014 (4.3231)  class_acc: 0.2917 (0.2616)  loss_scale: 65536.0000 (37180.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1705  max mem: 15572
Epoch: [17]  [1940/2809]  eta: 0:08:13  lr: 0.000033  min_lr: 0.000000  loss: 4.2763 (4.3227)  class_acc: 0.2500 (0.2615)  loss_scale: 65536.0000 (37326.1453)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1220  max mem: 15572
[2025-01-15 22:52:14,484] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49700
[2025-01-15 22:52:14,485] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:52:14,485] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [1950/2809]  eta: 0:08:07  lr: 0.000033  min_lr: 0.000000  loss: 4.2617 (4.3224)  class_acc: 0.2500 (0.2617)  loss_scale: 65536.0000 (37403.5551)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0432  max mem: 15572
Epoch: [17]  [1960/2809]  eta: 0:08:02  lr: 0.000033  min_lr: 0.000000  loss: 4.3034 (4.3218)  class_acc: 0.3333 (0.2622)  loss_scale: 32768.0000 (37379.9164)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0843  max mem: 15572
Epoch: [17]  [1970/2809]  eta: 0:07:56  lr: 0.000033  min_lr: 0.000000  loss: 4.2295 (4.3212)  class_acc: 0.2917 (0.2623)  loss_scale: 32768.0000 (37356.5175)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1100  max mem: 15572
Epoch: [17]  [1980/2809]  eta: 0:07:50  lr: 0.000033  min_lr: 0.000000  loss: 4.2472 (4.3213)  class_acc: 0.2500 (0.2621)  loss_scale: 32768.0000 (37333.3549)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1113  max mem: 15572
Epoch: [17]  [1990/2809]  eta: 0:07:45  lr: 0.000033  min_lr: 0.000000  loss: 4.2566 (4.3211)  class_acc: 0.2083 (0.2621)  loss_scale: 32768.0000 (37310.4249)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1289  max mem: 15572
Epoch: [17]  [2000/2809]  eta: 0:07:39  lr: 0.000033  min_lr: 0.000000  loss: 4.2477 (4.3208)  class_acc: 0.2500 (0.2623)  loss_scale: 32768.0000 (37287.7241)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0827  max mem: 15572
Epoch: [17]  [2010/2809]  eta: 0:07:33  lr: 0.000033  min_lr: 0.000000  loss: 4.1025 (4.3199)  class_acc: 0.2500 (0.2624)  loss_scale: 32768.0000 (37265.2491)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0886  max mem: 15572
Epoch: [17]  [2020/2809]  eta: 0:07:27  lr: 0.000033  min_lr: 0.000000  loss: 4.2329 (4.3206)  class_acc: 0.2500 (0.2624)  loss_scale: 32768.0000 (37242.9965)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.0972  max mem: 15572
Epoch: [17]  [2030/2809]  eta: 0:07:21  lr: 0.000033  min_lr: 0.000000  loss: 4.4861 (4.3207)  class_acc: 0.2500 (0.2624)  loss_scale: 32768.0000 (37220.9631)  weight_decay: 0.0500 (0.0500)  time: 0.5280  data: 0.0636  max mem: 15572
Epoch: [17]  [2040/2809]  eta: 0:07:15  lr: 0.000033  min_lr: 0.000000  loss: 4.3210 (4.3210)  class_acc: 0.2500 (0.2624)  loss_scale: 32768.0000 (37199.1455)  weight_decay: 0.0500 (0.0500)  time: 0.4979  data: 0.0359  max mem: 15572
Epoch: [17]  [2050/2809]  eta: 0:07:10  lr: 0.000033  min_lr: 0.000000  loss: 4.1422 (4.3200)  class_acc: 0.2917 (0.2627)  loss_scale: 32768.0000 (37177.5407)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0760  max mem: 15572
Epoch: [17]  [2060/2809]  eta: 0:07:04  lr: 0.000033  min_lr: 0.000000  loss: 4.1743 (4.3197)  class_acc: 0.2500 (0.2626)  loss_scale: 32768.0000 (37156.1456)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1312  max mem: 15572
Epoch: [17]  [2070/2809]  eta: 0:06:58  lr: 0.000033  min_lr: 0.000000  loss: 4.2540 (4.3201)  class_acc: 0.2083 (0.2624)  loss_scale: 32768.0000 (37134.9570)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1057  max mem: 15572
[2025-01-15 22:53:26,038] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:53:26,038] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [2080/2809]  eta: 0:06:53  lr: 0.000033  min_lr: 0.000000  loss: 4.2946 (4.3199)  class_acc: 0.2083 (0.2625)  loss_scale: 32768.0000 (37192.7035)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1066  max mem: 15572
Epoch: [17]  [2090/2809]  eta: 0:06:47  lr: 0.000033  min_lr: 0.000000  loss: 4.2721 (4.3198)  class_acc: 0.2500 (0.2626)  loss_scale: 65536.0000 (37328.2525)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1344  max mem: 15572
Epoch: [17]  [2100/2809]  eta: 0:06:41  lr: 0.000033  min_lr: 0.000000  loss: 4.3043 (4.3198)  class_acc: 0.3333 (0.2629)  loss_scale: 65536.0000 (37462.5112)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1313  max mem: 15572
Epoch: [17]  [2110/2809]  eta: 0:06:36  lr: 0.000033  min_lr: 0.000000  loss: 4.3307 (4.3198)  class_acc: 0.3333 (0.2630)  loss_scale: 65536.0000 (37595.4979)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1075  max mem: 15572
Epoch: [17]  [2120/2809]  eta: 0:06:30  lr: 0.000033  min_lr: 0.000000  loss: 4.3543 (4.3197)  class_acc: 0.2500 (0.2629)  loss_scale: 65536.0000 (37727.2306)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1542  max mem: 15572
Epoch: [17]  [2130/2809]  eta: 0:06:25  lr: 0.000033  min_lr: 0.000000  loss: 4.3210 (4.3199)  class_acc: 0.2500 (0.2629)  loss_scale: 65536.0000 (37857.7269)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.2275  max mem: 15572
Epoch: [17]  [2140/2809]  eta: 0:06:19  lr: 0.000033  min_lr: 0.000000  loss: 4.3210 (4.3203)  class_acc: 0.2083 (0.2626)  loss_scale: 65536.0000 (37987.0042)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1498  max mem: 15572
Epoch: [17]  [2150/2809]  eta: 0:06:13  lr: 0.000033  min_lr: 0.000000  loss: 4.3501 (4.3206)  class_acc: 0.2083 (0.2624)  loss_scale: 65536.0000 (38115.0795)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0928  max mem: 15572
Epoch: [17]  [2160/2809]  eta: 0:06:08  lr: 0.000033  min_lr: 0.000000  loss: 4.3501 (4.3207)  class_acc: 0.2500 (0.2625)  loss_scale: 65536.0000 (38241.9695)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.1046  max mem: 15572
Epoch: [17]  [2170/2809]  eta: 0:06:02  lr: 0.000033  min_lr: 0.000000  loss: 4.4419 (4.3217)  class_acc: 0.2083 (0.2622)  loss_scale: 65536.0000 (38367.6905)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1085  max mem: 15572
Epoch: [17]  [2180/2809]  eta: 0:05:56  lr: 0.000033  min_lr: 0.000000  loss: 4.5477 (4.3229)  class_acc: 0.2083 (0.2619)  loss_scale: 65536.0000 (38492.2586)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1423  max mem: 15572
Epoch: [17]  [2190/2809]  eta: 0:05:51  lr: 0.000033  min_lr: 0.000000  loss: 4.4684 (4.3231)  class_acc: 0.1667 (0.2616)  loss_scale: 65536.0000 (38615.6896)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1680  max mem: 15572
Epoch: [17]  [2200/2809]  eta: 0:05:45  lr: 0.000033  min_lr: 0.000000  loss: 4.3454 (4.3230)  class_acc: 0.2083 (0.2617)  loss_scale: 65536.0000 (38737.9991)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1343  max mem: 15572
[2025-01-15 22:54:39,571] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:54:39,571] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:54:40,031] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49958
[2025-01-15 22:54:40,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:54:40,032] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2210/2809]  eta: 0:05:39  lr: 0.000033  min_lr: 0.000000  loss: 4.2694 (4.3228)  class_acc: 0.2083 (0.2614)  loss_scale: 65536.0000 (38888.8431)  weight_decay: 0.0500 (0.0500)  time: 0.4939  data: 0.0500  max mem: 15572
[2025-01-15 22:54:43,707] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49966
[2025-01-15 22:54:43,708] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:54:43,708] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [2220/2809]  eta: 0:05:33  lr: 0.000033  min_lr: 0.000000  loss: 4.2186 (4.3219)  class_acc: 0.2500 (0.2618)  loss_scale: 65536.0000 (38890.7915)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0727  max mem: 15572
Epoch: [17]  [2230/2809]  eta: 0:05:28  lr: 0.000033  min_lr: 0.000000  loss: 4.3097 (4.3226)  class_acc: 0.2917 (0.2618)  loss_scale: 32768.0000 (38863.3474)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1569  max mem: 15572
Epoch: [17]  [2240/2809]  eta: 0:05:22  lr: 0.000033  min_lr: 0.000000  loss: 4.3822 (4.3226)  class_acc: 0.2500 (0.2619)  loss_scale: 32768.0000 (38836.1481)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1458  max mem: 15572
[2025-01-15 22:55:03,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=50000, skipped=312, lr=[3.202091611303319e-07, 3.202091611303319e-07, 4.5744165875761706e-07, 4.5744165875761706e-07, 6.534880839394531e-07, 6.534880839394531e-07, 9.3355440562779e-07, 9.3355440562779e-07, 1.333649150896843e-06, 1.333649150896843e-06, 1.9052130727097759e-06, 1.9052130727097759e-06, 2.7217329610139656e-06, 2.7217329610139656e-06, 3.888189944305666e-06, 3.888189944305666e-06, 5.554557063293808e-06, 5.554557063293808e-06, 7.935081518991155e-06, 7.935081518991155e-06, 1.1335830741415936e-05, 1.1335830741415936e-05, 1.619404391630848e-05, 1.619404391630848e-05, 2.313434845186926e-05, 2.313434845186926e-05, 3.304906921695609e-05, 3.304906921695609e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:55:03,146] [INFO] [timer.py:260:stop] epoch=0/micro_step=50000/global_step=50000, RunningAvgSamplesPerSec=27.724295672133778, CurrSamplesPerSec=31.22258374084305, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [2250/2809]  eta: 0:05:17  lr: 0.000033  min_lr: 0.000000  loss: 4.3453 (4.3223)  class_acc: 0.3333 (0.2623)  loss_scale: 32768.0000 (38809.1906)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1068  max mem: 15572
Epoch: [17]  [2260/2809]  eta: 0:05:11  lr: 0.000033  min_lr: 0.000000  loss: 4.3453 (4.3223)  class_acc: 0.3333 (0.2623)  loss_scale: 32768.0000 (38782.4715)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0612  max mem: 15572
Epoch: [17]  [2270/2809]  eta: 0:05:05  lr: 0.000033  min_lr: 0.000000  loss: 4.3096 (4.3219)  class_acc: 0.2917 (0.2625)  loss_scale: 32768.0000 (38755.9877)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0509  max mem: 15572
Epoch: [17]  [2280/2809]  eta: 0:04:59  lr: 0.000033  min_lr: 0.000000  loss: 4.3131 (4.3217)  class_acc: 0.3333 (0.2627)  loss_scale: 32768.0000 (38729.7361)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1359  max mem: 15572
Epoch: [17]  [2290/2809]  eta: 0:04:54  lr: 0.000033  min_lr: 0.000000  loss: 4.0690 (4.3207)  class_acc: 0.2917 (0.2627)  loss_scale: 32768.0000 (38703.7137)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1539  max mem: 15572
Epoch: [17]  [2300/2809]  eta: 0:04:48  lr: 0.000033  min_lr: 0.000000  loss: 4.3263 (4.3213)  class_acc: 0.2500 (0.2626)  loss_scale: 32768.0000 (38677.9174)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1305  max mem: 15572
Epoch: [17]  [2310/2809]  eta: 0:04:42  lr: 0.000033  min_lr: 0.000000  loss: 4.3697 (4.3210)  class_acc: 0.2500 (0.2626)  loss_scale: 32768.0000 (38652.3444)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.1041  max mem: 15572
Epoch: [17]  [2320/2809]  eta: 0:04:37  lr: 0.000033  min_lr: 0.000000  loss: 4.3430 (4.3220)  class_acc: 0.2083 (0.2622)  loss_scale: 32768.0000 (38626.9918)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1024  max mem: 15572
Epoch: [17]  [2330/2809]  eta: 0:04:31  lr: 0.000033  min_lr: 0.000000  loss: 4.3430 (4.3218)  class_acc: 0.2083 (0.2624)  loss_scale: 32768.0000 (38601.8567)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1422  max mem: 15572
Epoch: [17]  [2340/2809]  eta: 0:04:26  lr: 0.000033  min_lr: 0.000000  loss: 4.2119 (4.3211)  class_acc: 0.2500 (0.2624)  loss_scale: 32768.0000 (38576.9364)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1625  max mem: 15572
[2025-01-15 22:55:57,179] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:55:57,179] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 22:55:59,589] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50098
[2025-01-15 22:55:59,589] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:55:59,589] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [2350/2809]  eta: 0:04:20  lr: 0.000033  min_lr: 0.000000  loss: 4.2133 (4.3209)  class_acc: 0.2500 (0.2626)  loss_scale: 32768.0000 (38594.0417)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1346  max mem: 15572
Epoch: [17]  [2360/2809]  eta: 0:04:14  lr: 0.000033  min_lr: 0.000000  loss: 4.2558 (4.3210)  class_acc: 0.2500 (0.2626)  loss_scale: 32768.0000 (38569.3655)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1646  max mem: 15572
Epoch: [17]  [2370/2809]  eta: 0:04:09  lr: 0.000033  min_lr: 0.000000  loss: 4.3465 (4.3211)  class_acc: 0.2500 (0.2625)  loss_scale: 32768.0000 (38544.8975)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1667  max mem: 15572
Epoch: [17]  [2380/2809]  eta: 0:04:03  lr: 0.000033  min_lr: 0.000000  loss: 4.3465 (4.3207)  class_acc: 0.2500 (0.2625)  loss_scale: 32768.0000 (38520.6350)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1638  max mem: 15572
Epoch: [17]  [2390/2809]  eta: 0:03:57  lr: 0.000033  min_lr: 0.000000  loss: 4.1545 (4.3202)  class_acc: 0.2083 (0.2624)  loss_scale: 32768.0000 (38496.5755)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1437  max mem: 15572
Epoch: [17]  [2400/2809]  eta: 0:03:52  lr: 0.000033  min_lr: 0.000000  loss: 4.2647 (4.3197)  class_acc: 0.2083 (0.2621)  loss_scale: 32768.0000 (38472.7164)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1180  max mem: 15572
Epoch: [17]  [2410/2809]  eta: 0:03:46  lr: 0.000033  min_lr: 0.000000  loss: 4.1827 (4.3192)  class_acc: 0.2500 (0.2623)  loss_scale: 32768.0000 (38449.0552)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1217  max mem: 15572
Epoch: [17]  [2420/2809]  eta: 0:03:40  lr: 0.000033  min_lr: 0.000000  loss: 4.2605 (4.3194)  class_acc: 0.2083 (0.2622)  loss_scale: 32768.0000 (38425.5894)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1330  max mem: 15572
Epoch: [17]  [2430/2809]  eta: 0:03:35  lr: 0.000033  min_lr: 0.000000  loss: 4.3385 (4.3193)  class_acc: 0.2083 (0.2623)  loss_scale: 32768.0000 (38402.3167)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1252  max mem: 15572
Epoch: [17]  [2440/2809]  eta: 0:03:29  lr: 0.000033  min_lr: 0.000000  loss: 4.3385 (4.3196)  class_acc: 0.2500 (0.2624)  loss_scale: 32768.0000 (38379.2347)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0735  max mem: 15572
Epoch: [17]  [2450/2809]  eta: 0:03:23  lr: 0.000033  min_lr: 0.000000  loss: 4.3203 (4.3196)  class_acc: 0.2500 (0.2625)  loss_scale: 32768.0000 (38356.3411)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1338  max mem: 15572
Epoch: [17]  [2460/2809]  eta: 0:03:18  lr: 0.000033  min_lr: 0.000000  loss: 4.4152 (4.3203)  class_acc: 0.2083 (0.2625)  loss_scale: 32768.0000 (38333.6335)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1444  max mem: 15572
Epoch: [17]  [2470/2809]  eta: 0:03:12  lr: 0.000033  min_lr: 0.000000  loss: 4.3479 (4.3199)  class_acc: 0.2500 (0.2626)  loss_scale: 32768.0000 (38311.1097)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0986  max mem: 15572
[2025-01-15 22:57:13,751] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:57:13,751] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [2480/2809]  eta: 0:03:06  lr: 0.000033  min_lr: 0.000000  loss: 4.2765 (4.3195)  class_acc: 0.2917 (0.2626)  loss_scale: 32768.0000 (38381.2205)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.0945  max mem: 15572
Epoch: [17]  [2490/2809]  eta: 0:03:01  lr: 0.000033  min_lr: 0.000000  loss: 4.2582 (4.3194)  class_acc: 0.2083 (0.2626)  loss_scale: 65536.0000 (38490.2320)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0996  max mem: 15572
Epoch: [17]  [2500/2809]  eta: 0:02:55  lr: 0.000033  min_lr: 0.000000  loss: 4.2382 (4.3193)  class_acc: 0.2500 (0.2627)  loss_scale: 65536.0000 (38598.3719)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0931  max mem: 15572
Epoch: [17]  [2510/2809]  eta: 0:02:49  lr: 0.000033  min_lr: 0.000000  loss: 4.3032 (4.3193)  class_acc: 0.2500 (0.2628)  loss_scale: 65536.0000 (38705.6503)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1248  max mem: 15572
Epoch: [17]  [2520/2809]  eta: 0:02:44  lr: 0.000033  min_lr: 0.000000  loss: 4.3231 (4.3193)  class_acc: 0.2500 (0.2628)  loss_scale: 65536.0000 (38812.0777)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1040  max mem: 15572
Epoch: [17]  [2530/2809]  eta: 0:02:38  lr: 0.000033  min_lr: 0.000000  loss: 4.2640 (4.3189)  class_acc: 0.2500 (0.2629)  loss_scale: 65536.0000 (38917.6642)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1231  max mem: 15572
[2025-01-15 22:57:47,216] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50285
[2025-01-15 22:57:47,216] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:57:47,216] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [2540/2809]  eta: 0:02:32  lr: 0.000033  min_lr: 0.000000  loss: 4.3399 (4.3195)  class_acc: 0.2500 (0.2628)  loss_scale: 65536.0000 (38906.3581)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1165  max mem: 15572
Epoch: [17]  [2550/2809]  eta: 0:02:27  lr: 0.000033  min_lr: 0.000000  loss: 4.3420 (4.3188)  class_acc: 0.2500 (0.2629)  loss_scale: 32768.0000 (38882.2956)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0900  max mem: 15572
Epoch: [17]  [2560/2809]  eta: 0:02:21  lr: 0.000033  min_lr: 0.000000  loss: 4.0638 (4.3182)  class_acc: 0.2500 (0.2628)  loss_scale: 32768.0000 (38858.4209)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1117  max mem: 15572
Epoch: [17]  [2570/2809]  eta: 0:02:15  lr: 0.000033  min_lr: 0.000000  loss: 4.2040 (4.3181)  class_acc: 0.2500 (0.2629)  loss_scale: 32768.0000 (38834.7320)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1344  max mem: 15572
Epoch: [17]  [2580/2809]  eta: 0:02:10  lr: 0.000033  min_lr: 0.000000  loss: 4.3765 (4.3187)  class_acc: 0.2500 (0.2627)  loss_scale: 32768.0000 (38811.2267)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1510  max mem: 15572
Epoch: [17]  [2590/2809]  eta: 0:02:04  lr: 0.000033  min_lr: 0.000000  loss: 4.4342 (4.3190)  class_acc: 0.2500 (0.2626)  loss_scale: 32768.0000 (38787.9027)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0810  max mem: 15572
Epoch: [17]  [2600/2809]  eta: 0:01:58  lr: 0.000033  min_lr: 0.000000  loss: 4.2975 (4.3188)  class_acc: 0.2500 (0.2628)  loss_scale: 32768.0000 (38764.7582)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0576  max mem: 15572
Epoch: [17]  [2610/2809]  eta: 0:01:52  lr: 0.000033  min_lr: 0.000000  loss: 4.2107 (4.3179)  class_acc: 0.2500 (0.2628)  loss_scale: 32768.0000 (38741.7909)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0642  max mem: 15572
Epoch: [17]  [2620/2809]  eta: 0:01:47  lr: 0.000033  min_lr: 0.000000  loss: 4.2107 (4.3176)  class_acc: 0.2083 (0.2627)  loss_scale: 32768.0000 (38718.9989)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1095  max mem: 15572
Epoch: [17]  [2630/2809]  eta: 0:01:41  lr: 0.000033  min_lr: 0.000000  loss: 4.2666 (4.3178)  class_acc: 0.2500 (0.2627)  loss_scale: 32768.0000 (38696.3801)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0942  max mem: 15572
Epoch: [17]  [2640/2809]  eta: 0:01:35  lr: 0.000033  min_lr: 0.000000  loss: 4.4191 (4.3181)  class_acc: 0.2500 (0.2627)  loss_scale: 32768.0000 (38673.9326)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1100  max mem: 15572
Epoch: [17]  [2650/2809]  eta: 0:01:30  lr: 0.000033  min_lr: 0.000000  loss: 4.4150 (4.3185)  class_acc: 0.2083 (0.2624)  loss_scale: 32768.0000 (38651.6545)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1788  max mem: 15572
Epoch: [17]  [2660/2809]  eta: 0:01:24  lr: 0.000033  min_lr: 0.000000  loss: 4.4013 (4.3188)  class_acc: 0.2083 (0.2625)  loss_scale: 32768.0000 (38629.5438)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1499  max mem: 15572
[2025-01-15 22:58:59,733] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:58:59,733] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [2670/2809]  eta: 0:01:18  lr: 0.000033  min_lr: 0.000000  loss: 4.3724 (4.3189)  class_acc: 0.2500 (0.2624)  loss_scale: 32768.0000 (38730.2793)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1205  max mem: 15572
Epoch: [17]  [2680/2809]  eta: 0:01:13  lr: 0.000033  min_lr: 0.000000  loss: 4.2616 (4.3187)  class_acc: 0.2083 (0.2625)  loss_scale: 65536.0000 (38830.2633)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1574  max mem: 15572
Epoch: [17]  [2690/2809]  eta: 0:01:07  lr: 0.000033  min_lr: 0.000000  loss: 4.2616 (4.3188)  class_acc: 0.2500 (0.2624)  loss_scale: 65536.0000 (38929.5043)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1517  max mem: 15572
Epoch: [17]  [2700/2809]  eta: 0:01:01  lr: 0.000033  min_lr: 0.000000  loss: 4.2847 (4.3185)  class_acc: 0.2500 (0.2625)  loss_scale: 65536.0000 (39028.0104)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.0537  max mem: 15572
Epoch: [17]  [2710/2809]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000000  loss: 4.2107 (4.3185)  class_acc: 0.2500 (0.2626)  loss_scale: 65536.0000 (39125.7897)  weight_decay: 0.0500 (0.0500)  time: 0.4151  data: 0.0004  max mem: 15572
Epoch: [17]  [2720/2809]  eta: 0:00:50  lr: 0.000033  min_lr: 0.000000  loss: 4.3613 (4.3189)  class_acc: 0.2500 (0.2625)  loss_scale: 65536.0000 (39222.8504)  weight_decay: 0.0500 (0.0500)  time: 0.4332  data: 0.0006  max mem: 15572
Epoch: [17]  [2730/2809]  eta: 0:00:44  lr: 0.000033  min_lr: 0.000000  loss: 4.3613 (4.3188)  class_acc: 0.2500 (0.2625)  loss_scale: 65536.0000 (39319.2003)  weight_decay: 0.0500 (0.0500)  time: 0.4914  data: 0.0011  max mem: 15572
Epoch: [17]  [2740/2809]  eta: 0:00:39  lr: 0.000033  min_lr: 0.000000  loss: 4.3206 (4.3187)  class_acc: 0.2500 (0.2624)  loss_scale: 65536.0000 (39414.8471)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0546  max mem: 15572
Epoch: [17]  [2750/2809]  eta: 0:00:33  lr: 0.000033  min_lr: 0.000000  loss: 4.3123 (4.3188)  class_acc: 0.2917 (0.2625)  loss_scale: 65536.0000 (39509.7986)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1163  max mem: 15572
[2025-01-15 22:59:53,039] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50510
[2025-01-15 22:59:53,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:59:53,040] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [2760/2809]  eta: 0:00:27  lr: 0.000033  min_lr: 0.000000  loss: 4.3038 (4.3186)  class_acc: 0.2917 (0.2626)  loss_scale: 65536.0000 (39556.5896)  weight_decay: 0.0500 (0.0500)  time: 0.6471  data: 0.1873  max mem: 15572
Epoch: [17]  [2770/2809]  eta: 0:00:22  lr: 0.000033  min_lr: 0.000000  loss: 4.1560 (4.3181)  class_acc: 0.2500 (0.2627)  loss_scale: 32768.0000 (39532.0909)  weight_decay: 0.0500 (0.0500)  time: 0.6696  data: 0.1981  max mem: 15572
Epoch: [17]  [2780/2809]  eta: 0:00:16  lr: 0.000033  min_lr: 0.000000  loss: 4.1613 (4.3182)  class_acc: 0.2500 (0.2627)  loss_scale: 32768.0000 (39507.7684)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1641  max mem: 15572
Epoch: [17]  [2790/2809]  eta: 0:00:10  lr: 0.000033  min_lr: 0.000000  loss: 4.3708 (4.3186)  class_acc: 0.2500 (0.2628)  loss_scale: 32768.0000 (39483.6202)  weight_decay: 0.0500 (0.0500)  time: 0.6879  data: 0.2445  max mem: 15572
Epoch: [17]  [2800/2809]  eta: 0:00:05  lr: 0.000033  min_lr: 0.000000  loss: 4.4059 (4.3184)  class_acc: 0.2500 (0.2629)  loss_scale: 32768.0000 (39459.6444)  weight_decay: 0.0500 (0.0500)  time: 0.6960  data: 0.2542  max mem: 15572
Epoch: [17]  [2808/2809]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000000  loss: 4.3015 (4.3184)  class_acc: 0.2500 (0.2627)  loss_scale: 32768.0000 (39440.5867)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.1054  max mem: 15572
Epoch: [17] Total time: 0:26:37 (0.5686 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000000  loss: 4.3015 (4.3184)  class_acc: 0.2500 (0.2627)  loss_scale: 32768.0000 (39440.5867)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:26:59  loss: 1.1895 (1.1895)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 5.9528  data: 5.7711  max mem: 15572
Val:  [ 10/272]  eta: 0:03:55  loss: 2.6613 (2.7335)  acc1: 44.4444 (40.9091)  acc5: 72.2222 (68.6869)  time: 0.8972  data: 0.7036  max mem: 15572
Val:  [ 20/272]  eta: 0:02:46  loss: 2.8456 (2.8074)  acc1: 38.8889 (40.2116)  acc5: 66.6667 (69.8413)  time: 0.3955  data: 0.2035  max mem: 15572
Val:  [ 30/272]  eta: 0:02:15  loss: 2.9551 (2.8459)  acc1: 38.8889 (37.4552)  acc5: 66.6667 (69.8925)  time: 0.3737  data: 0.1768  max mem: 15572
Val:  [ 40/272]  eta: 0:01:57  loss: 2.6599 (2.8328)  acc1: 27.7778 (35.9079)  acc5: 66.6667 (70.4607)  time: 0.3478  data: 0.1454  max mem: 15572
Val:  [ 50/272]  eta: 0:01:37  loss: 2.6123 (2.7702)  acc1: 38.8889 (38.1264)  acc5: 77.7778 (71.6776)  time: 0.2565  data: 0.0763  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 2.1537 (2.6984)  acc1: 50.0000 (39.8907)  acc5: 83.3333 (72.8597)  time: 0.1741  data: 0.0050  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 2.1699 (2.6394)  acc1: 55.5556 (42.4100)  acc5: 83.3333 (74.4131)  time: 0.1788  data: 0.0044  max mem: 15572
Val:  [ 80/272]  eta: 0:01:05  loss: 2.4038 (2.6431)  acc1: 50.0000 (42.5240)  acc5: 77.7778 (73.7311)  time: 0.1709  data: 0.0023  max mem: 15572
Val:  [ 90/272]  eta: 0:00:59  loss: 2.6220 (2.6600)  acc1: 38.8889 (42.1245)  acc5: 77.7778 (74.1758)  time: 0.1873  data: 0.0151  max mem: 15572
Val:  [100/272]  eta: 0:00:56  loss: 2.7912 (2.6997)  acc1: 38.8889 (41.0891)  acc5: 77.7778 (73.2673)  time: 0.2606  data: 0.0825  max mem: 15572
Val:  [110/272]  eta: 0:00:51  loss: 3.0203 (2.7700)  acc1: 22.2222 (38.9890)  acc5: 55.5556 (71.1712)  time: 0.2905  data: 0.1079  max mem: 15572
Val:  [120/272]  eta: 0:00:48  loss: 3.2787 (2.8126)  acc1: 16.6667 (38.1543)  acc5: 55.5556 (70.2020)  time: 0.2873  data: 0.1055  max mem: 15572
Val:  [130/272]  eta: 0:00:45  loss: 2.9320 (2.7962)  acc1: 33.3333 (38.8465)  acc5: 72.2222 (70.7803)  time: 0.3181  data: 0.1254  max mem: 15572
Val:  [140/272]  eta: 0:00:42  loss: 2.4799 (2.7998)  acc1: 50.0000 (39.3617)  acc5: 72.2222 (70.5674)  time: 0.3212  data: 0.1254  max mem: 15572
Val:  [150/272]  eta: 0:00:38  loss: 2.7826 (2.7919)  acc1: 33.3333 (39.2200)  acc5: 72.2222 (70.8609)  time: 0.2897  data: 0.0903  max mem: 15572
Val:  [160/272]  eta: 0:00:35  loss: 2.6338 (2.7786)  acc1: 38.8889 (39.7861)  acc5: 77.7778 (71.5321)  time: 0.2899  data: 0.0831  max mem: 15572
Val:  [170/272]  eta: 0:00:32  loss: 2.6790 (2.7923)  acc1: 38.8889 (39.2463)  acc5: 77.7778 (71.2151)  time: 0.3061  data: 0.1156  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 2.6403 (2.7788)  acc1: 33.3333 (39.0424)  acc5: 72.2222 (71.6083)  time: 0.3281  data: 0.1438  max mem: 15572
Val:  [190/272]  eta: 0:00:25  loss: 2.6403 (2.8074)  acc1: 33.3333 (38.0454)  acc5: 66.6667 (70.7097)  time: 0.3156  data: 0.1358  max mem: 15572
Val:  [200/272]  eta: 0:00:22  loss: 2.9649 (2.8131)  acc1: 22.2222 (37.2858)  acc5: 66.6667 (70.6191)  time: 0.2991  data: 0.1089  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.6359 (2.8201)  acc1: 27.7778 (37.2301)  acc5: 77.7778 (70.5108)  time: 0.2785  data: 0.0815  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.9374 (2.8195)  acc1: 33.3333 (37.3806)  acc5: 72.2222 (70.5631)  time: 0.2744  data: 0.0825  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.5200 (2.8041)  acc1: 50.0000 (38.2155)  acc5: 72.2222 (70.8754)  time: 0.3417  data: 0.1551  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.2656 (2.7887)  acc1: 55.5556 (38.6353)  acc5: 83.3333 (71.3001)  time: 0.3279  data: 0.1419  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.7043 (2.7960)  acc1: 33.3333 (38.0921)  acc5: 77.7778 (71.1598)  time: 0.3167  data: 0.1292  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.1079 (2.7479)  acc1: 66.6667 (39.8255)  acc5: 88.8889 (72.0094)  time: 0.2785  data: 0.1017  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.1160 (2.7515)  acc1: 50.0000 (39.5244)  acc5: 83.3333 (71.8327)  time: 0.1915  data: 0.0238  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.1160 (2.7561)  acc1: 44.4444 (39.5044)  acc5: 83.3333 (71.8001)  time: 0.1858  data: 0.0237  max mem: 15572
Val: Total time: 0:01:23 (0.3059 s / it)
* Acc@1 39.504 Acc@5 71.800 loss 2.756
Accuracy of the network on the 4883 val videos: 39.5%
Max accuracy: 40.22%
Epoch: [18]  [   0/2809]  eta: 5:19:32  lr: 0.000033  min_lr: 0.000000  loss: 4.6944 (4.6944)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.8255  data: 6.3439  max mem: 15572
Epoch: [18]  [  10/2809]  eta: 0:54:30  lr: 0.000033  min_lr: 0.000000  loss: 4.2637 (4.3877)  class_acc: 0.2500 (0.2197)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1685  data: 0.7099  max mem: 15572
Epoch: [18]  [  20/2809]  eta: 0:39:21  lr: 0.000033  min_lr: 0.000000  loss: 4.3529 (4.4269)  class_acc: 0.2083 (0.2460)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0928  max mem: 15572
Epoch: [18]  [  30/2809]  eta: 0:35:42  lr: 0.000033  min_lr: 0.000000  loss: 4.3773 (4.3877)  class_acc: 0.2083 (0.2433)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1073  max mem: 15572
Epoch: [18]  [  40/2809]  eta: 0:33:06  lr: 0.000033  min_lr: 0.000000  loss: 4.0938 (4.3175)  class_acc: 0.2917 (0.2683)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1438  max mem: 15572
Epoch: [18]  [  50/2809]  eta: 0:32:44  lr: 0.000033  min_lr: 0.000000  loss: 4.1664 (4.3214)  class_acc: 0.2917 (0.2786)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1857  max mem: 15572
Epoch: [18]  [  60/2809]  eta: 0:30:47  lr: 0.000033  min_lr: 0.000000  loss: 4.2947 (4.3055)  class_acc: 0.2917 (0.2780)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1300  max mem: 15572
Epoch: [18]  [  70/2809]  eta: 0:29:51  lr: 0.000033  min_lr: 0.000000  loss: 4.1599 (4.2801)  class_acc: 0.2917 (0.2858)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5064  data: 0.0616  max mem: 15572
[2025-01-15 23:02:37,790] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:02:37,791] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [  80/2809]  eta: 0:28:45  lr: 0.000033  min_lr: 0.000000  loss: 4.2417 (4.2828)  class_acc: 0.2500 (0.2798)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0838  max mem: 15572
[2025-01-15 23:02:43,574] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50650
[2025-01-15 23:02:43,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:02:43,575] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [  90/2809]  eta: 0:28:05  lr: 0.000033  min_lr: 0.000000  loss: 4.3081 (4.2780)  class_acc: 0.2500 (0.2761)  loss_scale: 65536.0000 (36728.9670)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0808  max mem: 15572
Epoch: [18]  [ 100/2809]  eta: 0:27:52  lr: 0.000033  min_lr: 0.000000  loss: 4.2442 (4.2711)  class_acc: 0.2500 (0.2752)  loss_scale: 32768.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1348  max mem: 15572
Epoch: [18]  [ 110/2809]  eta: 0:27:16  lr: 0.000033  min_lr: 0.000000  loss: 4.2473 (4.2750)  class_acc: 0.2083 (0.2706)  loss_scale: 32768.0000 (36015.2793)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.1108  max mem: 15572
Epoch: [18]  [ 120/2809]  eta: 0:27:28  lr: 0.000033  min_lr: 0.000000  loss: 4.3456 (4.2840)  class_acc: 0.2083 (0.2676)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1656  max mem: 15572
Epoch: [18]  [ 130/2809]  eta: 0:27:35  lr: 0.000033  min_lr: 0.000000  loss: 4.4116 (4.2939)  class_acc: 0.2083 (0.2637)  loss_scale: 32768.0000 (35519.5115)  weight_decay: 0.0500 (0.0500)  time: 0.6831  data: 0.2520  max mem: 15572
Epoch: [18]  [ 140/2809]  eta: 0:27:17  lr: 0.000033  min_lr: 0.000000  loss: 4.3605 (4.2897)  class_acc: 0.2917 (0.2660)  loss_scale: 32768.0000 (35324.3688)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1827  max mem: 15572
Epoch: [18]  [ 150/2809]  eta: 0:26:48  lr: 0.000033  min_lr: 0.000000  loss: 4.2400 (4.2890)  class_acc: 0.2917 (0.2655)  loss_scale: 32768.0000 (35155.0728)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0899  max mem: 15572
Epoch: [18]  [ 160/2809]  eta: 0:26:42  lr: 0.000033  min_lr: 0.000000  loss: 4.2636 (4.2933)  class_acc: 0.2500 (0.2642)  loss_scale: 32768.0000 (35006.8075)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.1040  max mem: 15572
Epoch: [18]  [ 170/2809]  eta: 0:26:18  lr: 0.000033  min_lr: 0.000000  loss: 4.4627 (4.3072)  class_acc: 0.2083 (0.2619)  loss_scale: 32768.0000 (34875.8830)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1072  max mem: 15572
Epoch: [18]  [ 180/2809]  eta: 0:26:06  lr: 0.000033  min_lr: 0.000000  loss: 4.3048 (4.3073)  class_acc: 0.2500 (0.2613)  loss_scale: 32768.0000 (34759.4254)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0842  max mem: 15572
Epoch: [18]  [ 190/2809]  eta: 0:25:54  lr: 0.000033  min_lr: 0.000000  loss: 4.2157 (4.3019)  class_acc: 0.2917 (0.2646)  loss_scale: 32768.0000 (34655.1623)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1045  max mem: 15572
Epoch: [18]  [ 200/2809]  eta: 0:25:48  lr: 0.000033  min_lr: 0.000000  loss: 4.1467 (4.2985)  class_acc: 0.2500 (0.2653)  loss_scale: 32768.0000 (34561.2736)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1271  max mem: 15572
Epoch: [18]  [ 210/2809]  eta: 0:25:34  lr: 0.000033  min_lr: 0.000000  loss: 4.3298 (4.3018)  class_acc: 0.2500 (0.2668)  loss_scale: 32768.0000 (34476.2844)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1111  max mem: 15572
[2025-01-15 23:03:57,692] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:03:57,692] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [ 220/2809]  eta: 0:25:33  lr: 0.000033  min_lr: 0.000000  loss: 4.1934 (4.2968)  class_acc: 0.2917 (0.2690)  loss_scale: 32768.0000 (34992.0724)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1471  max mem: 15572
Epoch: [18]  [ 230/2809]  eta: 0:25:12  lr: 0.000033  min_lr: 0.000000  loss: 4.1187 (4.2927)  class_acc: 0.2917 (0.2716)  loss_scale: 65536.0000 (36314.3203)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.1121  max mem: 15572
Epoch: [18]  [ 240/2809]  eta: 0:25:05  lr: 0.000032  min_lr: 0.000000  loss: 4.0656 (4.2815)  class_acc: 0.3333 (0.2730)  loss_scale: 65536.0000 (37526.8382)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0642  max mem: 15572
[2025-01-15 23:04:13,921] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50810
[2025-01-15 23:04:13,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:04:13,922] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [ 250/2809]  eta: 0:24:56  lr: 0.000032  min_lr: 0.000000  loss: 4.2891 (4.2858)  class_acc: 0.2917 (0.2727)  loss_scale: 65536.0000 (38251.0916)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1122  max mem: 15572
Epoch: [18]  [ 260/2809]  eta: 0:24:57  lr: 0.000032  min_lr: 0.000000  loss: 4.2972 (4.2829)  class_acc: 0.2083 (0.2703)  loss_scale: 32768.0000 (38041.0115)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1608  max mem: 15572
Epoch: [18]  [ 270/2809]  eta: 0:24:45  lr: 0.000032  min_lr: 0.000000  loss: 4.2497 (4.2872)  class_acc: 0.2083 (0.2701)  loss_scale: 32768.0000 (37846.4354)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1636  max mem: 15572
Epoch: [18]  [ 280/2809]  eta: 0:24:42  lr: 0.000032  min_lr: 0.000000  loss: 4.2367 (4.2853)  class_acc: 0.2500 (0.2685)  loss_scale: 32768.0000 (37665.7082)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1387  max mem: 15572
Epoch: [18]  [ 290/2809]  eta: 0:24:40  lr: 0.000032  min_lr: 0.000000  loss: 4.2214 (4.2889)  class_acc: 0.2500 (0.2673)  loss_scale: 32768.0000 (37497.4021)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1872  max mem: 15572
Epoch: [18]  [ 300/2809]  eta: 0:24:36  lr: 0.000032  min_lr: 0.000000  loss: 4.4012 (4.2930)  class_acc: 0.2500 (0.2674)  loss_scale: 32768.0000 (37340.2791)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1828  max mem: 15572
Epoch: [18]  [ 310/2809]  eta: 0:24:20  lr: 0.000032  min_lr: 0.000000  loss: 4.4012 (4.2971)  class_acc: 0.2500 (0.2673)  loss_scale: 32768.0000 (37193.2605)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0919  max mem: 15572
Epoch: [18]  [ 320/2809]  eta: 0:24:10  lr: 0.000032  min_lr: 0.000000  loss: 4.3885 (4.3016)  class_acc: 0.2500 (0.2679)  loss_scale: 32768.0000 (37055.4019)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0508  max mem: 15572
Epoch: [18]  [ 330/2809]  eta: 0:24:01  lr: 0.000032  min_lr: 0.000000  loss: 4.3968 (4.3049)  class_acc: 0.2500 (0.2671)  loss_scale: 32768.0000 (36925.8731)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0935  max mem: 15572
Epoch: [18]  [ 340/2809]  eta: 0:23:56  lr: 0.000032  min_lr: 0.000000  loss: 4.3199 (4.3038)  class_acc: 0.2500 (0.2659)  loss_scale: 32768.0000 (36803.9413)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1168  max mem: 15572
Epoch: [18]  [ 350/2809]  eta: 0:23:40  lr: 0.000032  min_lr: 0.000000  loss: 4.2933 (4.3049)  class_acc: 0.2500 (0.2661)  loss_scale: 32768.0000 (36688.9573)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0657  max mem: 15572
Epoch: [18]  [ 360/2809]  eta: 0:23:34  lr: 0.000032  min_lr: 0.000000  loss: 4.3728 (4.3060)  class_acc: 0.3333 (0.2669)  loss_scale: 32768.0000 (36580.3435)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0701  max mem: 15572
Epoch: [18]  [ 370/2809]  eta: 0:23:27  lr: 0.000032  min_lr: 0.000000  loss: 4.3205 (4.3061)  class_acc: 0.2917 (0.2673)  loss_scale: 32768.0000 (36477.5849)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1201  max mem: 15572
[2025-01-15 23:05:26,218] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:05:26,218] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [ 380/2809]  eta: 0:23:19  lr: 0.000032  min_lr: 0.000000  loss: 4.3068 (4.3048)  class_acc: 0.2500 (0.2679)  loss_scale: 32768.0000 (36724.2415)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1017  max mem: 15572
[2025-01-15 23:05:30,430] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50946
[2025-01-15 23:05:30,430] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:05:30,430] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [ 390/2809]  eta: 0:23:12  lr: 0.000032  min_lr: 0.000000  loss: 4.2255 (4.3044)  class_acc: 0.2500 (0.2684)  loss_scale: 32768.0000 (36874.4757)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1092  max mem: 15572
Epoch: [18]  [ 400/2809]  eta: 0:23:03  lr: 0.000032  min_lr: 0.000000  loss: 4.2410 (4.3057)  class_acc: 0.2500 (0.2681)  loss_scale: 32768.0000 (36772.0698)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0882  max mem: 15572
Epoch: [18]  [ 410/2809]  eta: 0:23:00  lr: 0.000032  min_lr: 0.000000  loss: 4.3539 (4.3082)  class_acc: 0.1667 (0.2671)  loss_scale: 32768.0000 (36674.6472)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1181  max mem: 15572
Epoch: [18]  [ 420/2809]  eta: 0:22:54  lr: 0.000032  min_lr: 0.000000  loss: 4.2504 (4.3064)  class_acc: 0.2500 (0.2679)  loss_scale: 32768.0000 (36581.8527)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1459  max mem: 15572
Epoch: [18]  [ 430/2809]  eta: 0:22:45  lr: 0.000032  min_lr: 0.000000  loss: 4.1704 (4.3041)  class_acc: 0.2500 (0.2687)  loss_scale: 32768.0000 (36493.3643)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0977  max mem: 15572
[2025-01-15 23:05:59,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=51000, skipped=318, lr=[3.1354948628075986e-07, 3.1354948628075986e-07, 4.4792783754394275e-07, 4.4792783754394275e-07, 6.398969107770612e-07, 6.398969107770612e-07, 9.141384439672303e-07, 9.141384439672303e-07, 1.3059120628103289e-06, 1.3059120628103289e-06, 1.865588661157613e-06, 1.865588661157613e-06, 2.66512665879659e-06, 2.66512665879659e-06, 3.807323798280843e-06, 3.807323798280843e-06, 5.4390339975440615e-06, 5.4390339975440615e-06, 7.770048567920089e-06, 7.770048567920089e-06, 1.1100069382742983e-05, 1.1100069382742983e-05, 1.5857241975347122e-05, 1.5857241975347122e-05, 2.2653202821924462e-05, 2.2653202821924462e-05, 3.236171831703495e-05, 3.236171831703495e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:05:59,180] [INFO] [timer.py:260:stop] epoch=0/micro_step=51000/global_step=51000, RunningAvgSamplesPerSec=27.735859365637705, CurrSamplesPerSec=25.954931791249354, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [18]  [ 440/2809]  eta: 0:22:36  lr: 0.000032  min_lr: 0.000000  loss: 4.2974 (4.3041)  class_acc: 0.2917 (0.2694)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0696  max mem: 15572
Epoch: [18]  [ 450/2809]  eta: 0:22:30  lr: 0.000032  min_lr: 0.000000  loss: 4.2974 (4.3002)  class_acc: 0.2917 (0.2693)  loss_scale: 32768.0000 (36328.1596)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0957  max mem: 15572
Epoch: [18]  [ 460/2809]  eta: 0:22:24  lr: 0.000032  min_lr: 0.000000  loss: 4.3066 (4.3026)  class_acc: 0.2083 (0.2678)  loss_scale: 32768.0000 (36250.9328)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1184  max mem: 15572
Epoch: [18]  [ 470/2809]  eta: 0:22:17  lr: 0.000032  min_lr: 0.000000  loss: 4.4645 (4.3047)  class_acc: 0.2083 (0.2666)  loss_scale: 32768.0000 (36176.9851)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0929  max mem: 15572
Epoch: [18]  [ 480/2809]  eta: 0:22:12  lr: 0.000032  min_lr: 0.000000  loss: 4.3653 (4.3064)  class_acc: 0.2083 (0.2668)  loss_scale: 32768.0000 (36106.1123)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1150  max mem: 15572
Epoch: [18]  [ 490/2809]  eta: 0:22:05  lr: 0.000032  min_lr: 0.000000  loss: 4.3635 (4.3070)  class_acc: 0.2083 (0.2656)  loss_scale: 32768.0000 (36038.1263)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1227  max mem: 15572
Epoch: [18]  [ 500/2809]  eta: 0:21:55  lr: 0.000032  min_lr: 0.000000  loss: 4.3066 (4.3068)  class_acc: 0.2083 (0.2648)  loss_scale: 32768.0000 (35972.8543)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0746  max mem: 15572
Epoch: [18]  [ 510/2809]  eta: 0:21:50  lr: 0.000032  min_lr: 0.000000  loss: 4.2937 (4.3041)  class_acc: 0.2500 (0.2657)  loss_scale: 32768.0000 (35910.1370)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0772  max mem: 15572
[2025-01-15 23:06:41,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:06:41,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 23:06:41,678] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51076
[2025-01-15 23:06:41,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:06:41,679] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [ 520/2809]  eta: 0:21:41  lr: 0.000032  min_lr: 0.000000  loss: 4.3139 (4.3053)  class_acc: 0.2500 (0.2654)  loss_scale: 32768.0000 (35912.7217)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0770  max mem: 15572
Epoch: [18]  [ 530/2809]  eta: 0:21:36  lr: 0.000032  min_lr: 0.000000  loss: 4.3499 (4.3036)  class_acc: 0.2083 (0.2646)  loss_scale: 32768.0000 (35853.4991)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.1023  max mem: 15572
Epoch: [18]  [ 540/2809]  eta: 0:21:29  lr: 0.000032  min_lr: 0.000000  loss: 4.3650 (4.3040)  class_acc: 0.2500 (0.2647)  loss_scale: 32768.0000 (35796.4658)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1240  max mem: 15572
Epoch: [18]  [ 550/2809]  eta: 0:21:24  lr: 0.000032  min_lr: 0.000000  loss: 4.4490 (4.3059)  class_acc: 0.2500 (0.2642)  loss_scale: 32768.0000 (35741.5027)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1255  max mem: 15572
Epoch: [18]  [ 560/2809]  eta: 0:21:23  lr: 0.000032  min_lr: 0.000000  loss: 4.3838 (4.3051)  class_acc: 0.2083 (0.2641)  loss_scale: 32768.0000 (35688.4991)  weight_decay: 0.0500 (0.0500)  time: 0.6394  data: 0.1973  max mem: 15572
Epoch: [18]  [ 570/2809]  eta: 0:21:18  lr: 0.000032  min_lr: 0.000000  loss: 4.1944 (4.3031)  class_acc: 0.2917 (0.2636)  loss_scale: 32768.0000 (35637.3520)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1752  max mem: 15572
Epoch: [18]  [ 580/2809]  eta: 0:21:09  lr: 0.000032  min_lr: 0.000000  loss: 4.3045 (4.3053)  class_acc: 0.2917 (0.2646)  loss_scale: 32768.0000 (35587.9656)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0797  max mem: 15572
Epoch: [18]  [ 590/2809]  eta: 0:21:03  lr: 0.000032  min_lr: 0.000000  loss: 4.2527 (4.3029)  class_acc: 0.3333 (0.2659)  loss_scale: 32768.0000 (35540.2504)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0844  max mem: 15572
Epoch: [18]  [ 600/2809]  eta: 0:20:59  lr: 0.000032  min_lr: 0.000000  loss: 4.3823 (4.3062)  class_acc: 0.2500 (0.2649)  loss_scale: 32768.0000 (35494.1231)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1418  max mem: 15572
Epoch: [18]  [ 610/2809]  eta: 0:20:54  lr: 0.000032  min_lr: 0.000000  loss: 4.4485 (4.3067)  class_acc: 0.2500 (0.2657)  loss_scale: 32768.0000 (35449.5057)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1497  max mem: 15572
Epoch: [18]  [ 620/2809]  eta: 0:20:46  lr: 0.000032  min_lr: 0.000000  loss: 4.3204 (4.3058)  class_acc: 0.2917 (0.2658)  loss_scale: 32768.0000 (35406.3253)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1066  max mem: 15572
Epoch: [18]  [ 630/2809]  eta: 0:20:44  lr: 0.000032  min_lr: 0.000000  loss: 4.2961 (4.3050)  class_acc: 0.2500 (0.2658)  loss_scale: 32768.0000 (35364.5135)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1614  max mem: 15572
Epoch: [18]  [ 640/2809]  eta: 0:20:38  lr: 0.000032  min_lr: 0.000000  loss: 4.3779 (4.3053)  class_acc: 0.2500 (0.2662)  loss_scale: 32768.0000 (35324.0062)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1722  max mem: 15572
[2025-01-15 23:07:55,440] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:07:55,441] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [ 650/2809]  eta: 0:20:32  lr: 0.000032  min_lr: 0.000000  loss: 4.4015 (4.3062)  class_acc: 0.2500 (0.2664)  loss_scale: 32768.0000 (35687.4224)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1152  max mem: 15572
Epoch: [18]  [ 660/2809]  eta: 0:20:25  lr: 0.000032  min_lr: 0.000000  loss: 4.2429 (4.3058)  class_acc: 0.2500 (0.2666)  loss_scale: 65536.0000 (36138.9894)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0985  max mem: 15572
Epoch: [18]  [ 670/2809]  eta: 0:20:20  lr: 0.000032  min_lr: 0.000000  loss: 4.2936 (4.3057)  class_acc: 0.2500 (0.2664)  loss_scale: 65536.0000 (36577.0969)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1025  max mem: 15572
Epoch: [18]  [ 680/2809]  eta: 0:20:11  lr: 0.000032  min_lr: 0.000000  loss: 4.2773 (4.3036)  class_acc: 0.2083 (0.2661)  loss_scale: 65536.0000 (37002.3377)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0696  max mem: 15572
Epoch: [18]  [ 690/2809]  eta: 0:20:03  lr: 0.000032  min_lr: 0.000000  loss: 4.1828 (4.3014)  class_acc: 0.2500 (0.2673)  loss_scale: 65536.0000 (37415.2706)  weight_decay: 0.0500 (0.0500)  time: 0.4727  data: 0.0299  max mem: 15572
Epoch: [18]  [ 700/2809]  eta: 0:19:57  lr: 0.000032  min_lr: 0.000000  loss: 4.1923 (4.3003)  class_acc: 0.2917 (0.2677)  loss_scale: 65536.0000 (37816.4223)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.1004  max mem: 15572
Epoch: [18]  [ 710/2809]  eta: 0:19:53  lr: 0.000032  min_lr: 0.000000  loss: 4.2786 (4.3014)  class_acc: 0.2500 (0.2671)  loss_scale: 65536.0000 (38206.2897)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1644  max mem: 15572
Epoch: [18]  [ 720/2809]  eta: 0:19:46  lr: 0.000032  min_lr: 0.000000  loss: 4.3155 (4.2997)  class_acc: 0.2500 (0.2677)  loss_scale: 65536.0000 (38585.3426)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1387  max mem: 15572
Epoch: [18]  [ 730/2809]  eta: 0:19:41  lr: 0.000032  min_lr: 0.000000  loss: 4.2642 (4.3000)  class_acc: 0.2500 (0.2672)  loss_scale: 65536.0000 (38954.0246)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1174  max mem: 15572
Epoch: [18]  [ 740/2809]  eta: 0:19:34  lr: 0.000032  min_lr: 0.000000  loss: 4.3768 (4.2998)  class_acc: 0.2083 (0.2672)  loss_scale: 65536.0000 (39312.7557)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.1105  max mem: 15572
[2025-01-15 23:08:50,945] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51307
[2025-01-15 23:08:50,945] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:08:50,945] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [ 750/2809]  eta: 0:19:32  lr: 0.000032  min_lr: 0.000000  loss: 4.3914 (4.3005)  class_acc: 0.2917 (0.2675)  loss_scale: 65536.0000 (39400.1385)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1747  max mem: 15572
Epoch: [18]  [ 760/2809]  eta: 0:19:26  lr: 0.000032  min_lr: 0.000000  loss: 4.3217 (4.2972)  class_acc: 0.2917 (0.2682)  loss_scale: 32768.0000 (39312.9882)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.2048  max mem: 15572
Epoch: [18]  [ 770/2809]  eta: 0:19:21  lr: 0.000032  min_lr: 0.000000  loss: 4.1241 (4.2974)  class_acc: 0.2917 (0.2675)  loss_scale: 32768.0000 (39228.0986)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1416  max mem: 15572
Epoch: [18]  [ 780/2809]  eta: 0:19:13  lr: 0.000032  min_lr: 0.000000  loss: 4.1634 (4.2967)  class_acc: 0.2500 (0.2675)  loss_scale: 32768.0000 (39145.3828)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0767  max mem: 15572
Epoch: [18]  [ 790/2809]  eta: 0:19:08  lr: 0.000032  min_lr: 0.000000  loss: 4.1634 (4.2955)  class_acc: 0.2917 (0.2679)  loss_scale: 32768.0000 (39064.7585)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0655  max mem: 15572
Epoch: [18]  [ 800/2809]  eta: 0:19:03  lr: 0.000032  min_lr: 0.000000  loss: 4.2837 (4.2968)  class_acc: 0.3333 (0.2681)  loss_scale: 32768.0000 (38986.1473)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1503  max mem: 15572
Epoch: [18]  [ 810/2809]  eta: 0:19:00  lr: 0.000032  min_lr: 0.000000  loss: 4.3709 (4.2975)  class_acc: 0.2500 (0.2678)  loss_scale: 32768.0000 (38909.4747)  weight_decay: 0.0500 (0.0500)  time: 0.6409  data: 0.2105  max mem: 15572
Epoch: [18]  [ 820/2809]  eta: 0:18:54  lr: 0.000032  min_lr: 0.000000  loss: 4.2832 (4.2954)  class_acc: 0.2500 (0.2682)  loss_scale: 32768.0000 (38834.6699)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1877  max mem: 15572
Epoch: [18]  [ 830/2809]  eta: 0:18:50  lr: 0.000032  min_lr: 0.000000  loss: 4.2440 (4.2958)  class_acc: 0.2917 (0.2688)  loss_scale: 32768.0000 (38761.6655)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1476  max mem: 15572
Epoch: [18]  [ 840/2809]  eta: 0:18:41  lr: 0.000032  min_lr: 0.000000  loss: 4.2516 (4.2952)  class_acc: 0.2500 (0.2691)  loss_scale: 32768.0000 (38690.3971)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0825  max mem: 15572
Epoch: [18]  [ 850/2809]  eta: 0:18:35  lr: 0.000032  min_lr: 0.000000  loss: 4.4351 (4.2969)  class_acc: 0.2917 (0.2695)  loss_scale: 32768.0000 (38620.8038)  weight_decay: 0.0500 (0.0500)  time: 0.4913  data: 0.0509  max mem: 15572
Epoch: [18]  [ 860/2809]  eta: 0:18:28  lr: 0.000032  min_lr: 0.000000  loss: 4.4351 (4.2971)  class_acc: 0.2500 (0.2689)  loss_scale: 32768.0000 (38552.8269)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.0947  max mem: 15572
Epoch: [18]  [ 870/2809]  eta: 0:18:23  lr: 0.000032  min_lr: 0.000000  loss: 4.3049 (4.2983)  class_acc: 0.2500 (0.2686)  loss_scale: 32768.0000 (38486.4110)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1019  max mem: 15572
[2025-01-15 23:10:05,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:10:05,471] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [ 880/2809]  eta: 0:18:16  lr: 0.000032  min_lr: 0.000000  loss: 4.3835 (4.2989)  class_acc: 0.2083 (0.2680)  loss_scale: 32768.0000 (38681.8615)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1030  max mem: 15572
Epoch: [18]  [ 890/2809]  eta: 0:18:11  lr: 0.000032  min_lr: 0.000000  loss: 4.3767 (4.3000)  class_acc: 0.2500 (0.2680)  loss_scale: 65536.0000 (38983.2548)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1102  max mem: 15572
Epoch: [18]  [ 900/2809]  eta: 0:18:05  lr: 0.000032  min_lr: 0.000000  loss: 4.3387 (4.3007)  class_acc: 0.2500 (0.2681)  loss_scale: 65536.0000 (39277.9578)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1188  max mem: 15572
Epoch: [18]  [ 910/2809]  eta: 0:18:01  lr: 0.000032  min_lr: 0.000000  loss: 4.3874 (4.3022)  class_acc: 0.2917 (0.2682)  loss_scale: 65536.0000 (39566.1910)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1599  max mem: 15572
Epoch: [18]  [ 920/2809]  eta: 0:17:53  lr: 0.000032  min_lr: 0.000000  loss: 4.3070 (4.2997)  class_acc: 0.2917 (0.2688)  loss_scale: 65536.0000 (39848.1650)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1064  max mem: 15572
Epoch: [18]  [ 930/2809]  eta: 0:17:48  lr: 0.000032  min_lr: 0.000000  loss: 4.2571 (4.2990)  class_acc: 0.2917 (0.2686)  loss_scale: 65536.0000 (40124.0816)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0672  max mem: 15572
Epoch: [18]  [ 940/2809]  eta: 0:17:42  lr: 0.000032  min_lr: 0.000000  loss: 4.3838 (4.3010)  class_acc: 0.2083 (0.2686)  loss_scale: 65536.0000 (40394.1339)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1274  max mem: 15572
Epoch: [18]  [ 950/2809]  eta: 0:17:35  lr: 0.000032  min_lr: 0.000000  loss: 4.3598 (4.2997)  class_acc: 0.2083 (0.2684)  loss_scale: 65536.0000 (40658.5068)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0974  max mem: 15572
Epoch: [18]  [ 960/2809]  eta: 0:17:29  lr: 0.000032  min_lr: 0.000000  loss: 4.1910 (4.2989)  class_acc: 0.2500 (0.2687)  loss_scale: 65536.0000 (40917.3777)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0794  max mem: 15572
Epoch: [18]  [ 970/2809]  eta: 0:17:22  lr: 0.000032  min_lr: 0.000000  loss: 4.2056 (4.2989)  class_acc: 0.2917 (0.2690)  loss_scale: 65536.0000 (41170.9166)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0906  max mem: 15572
Epoch: [18]  [ 980/2809]  eta: 0:17:17  lr: 0.000032  min_lr: 0.000000  loss: 4.2613 (4.2992)  class_acc: 0.2500 (0.2693)  loss_scale: 65536.0000 (41419.2864)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1248  max mem: 15572
Epoch: [18]  [ 990/2809]  eta: 0:17:12  lr: 0.000032  min_lr: 0.000000  loss: 4.3050 (4.2981)  class_acc: 0.2083 (0.2688)  loss_scale: 65536.0000 (41662.6438)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1438  max mem: 15572
Epoch: [18]  [1000/2809]  eta: 0:17:05  lr: 0.000032  min_lr: 0.000000  loss: 4.3050 (4.2989)  class_acc: 0.2083 (0.2681)  loss_scale: 65536.0000 (41901.1389)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1129  max mem: 15572
[2025-01-15 23:11:16,762] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:11:16,762] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:11:17,148] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51565
[2025-01-15 23:11:17,148] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:11:17,148] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-15 23:11:17,557] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51566
[2025-01-15 23:11:17,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:11:17,558] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [1010/2809]  eta: 0:17:01  lr: 0.000032  min_lr: 0.000000  loss: 4.2894 (4.2974)  class_acc: 0.2500 (0.2685)  loss_scale: 65536.0000 (41972.8586)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1540  max mem: 15572
Epoch: [18]  [1020/2809]  eta: 0:16:54  lr: 0.000032  min_lr: 0.000000  loss: 4.2894 (4.2975)  class_acc: 0.2500 (0.2688)  loss_scale: 32768.0000 (41882.7032)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1316  max mem: 15572
Epoch: [18]  [1030/2809]  eta: 0:16:48  lr: 0.000032  min_lr: 0.000000  loss: 4.4353 (4.2985)  class_acc: 0.2500 (0.2688)  loss_scale: 32768.0000 (41794.2968)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0771  max mem: 15572
Epoch: [18]  [1040/2809]  eta: 0:16:42  lr: 0.000032  min_lr: 0.000000  loss: 4.4442 (4.2988)  class_acc: 0.2500 (0.2686)  loss_scale: 32768.0000 (41707.5889)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1156  max mem: 15572
Epoch: [18]  [1050/2809]  eta: 0:16:36  lr: 0.000032  min_lr: 0.000000  loss: 4.3212 (4.2994)  class_acc: 0.2500 (0.2685)  loss_scale: 32768.0000 (41622.5309)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1216  max mem: 15572
Epoch: [18]  [1060/2809]  eta: 0:16:31  lr: 0.000032  min_lr: 0.000000  loss: 4.2530 (4.2986)  class_acc: 0.2500 (0.2685)  loss_scale: 32768.0000 (41539.0763)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1236  max mem: 15572
Epoch: [18]  [1070/2809]  eta: 0:16:26  lr: 0.000032  min_lr: 0.000000  loss: 4.2526 (4.2972)  class_acc: 0.2500 (0.2688)  loss_scale: 32768.0000 (41457.1802)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1441  max mem: 15572
Epoch: [18]  [1080/2809]  eta: 0:16:20  lr: 0.000032  min_lr: 0.000000  loss: 4.3496 (4.2979)  class_acc: 0.2083 (0.2683)  loss_scale: 32768.0000 (41376.7993)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1328  max mem: 15572
Epoch: [18]  [1090/2809]  eta: 0:16:13  lr: 0.000032  min_lr: 0.000000  loss: 4.3591 (4.2985)  class_acc: 0.2500 (0.2690)  loss_scale: 32768.0000 (41297.8918)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0883  max mem: 15572
Epoch: [18]  [1100/2809]  eta: 0:16:08  lr: 0.000032  min_lr: 0.000000  loss: 4.3591 (4.2994)  class_acc: 0.3333 (0.2691)  loss_scale: 32768.0000 (41220.4178)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1141  max mem: 15572
Epoch: [18]  [1110/2809]  eta: 0:16:03  lr: 0.000032  min_lr: 0.000000  loss: 4.4098 (4.3002)  class_acc: 0.2500 (0.2692)  loss_scale: 32768.0000 (41144.3384)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1715  max mem: 15572
Epoch: [18]  [1120/2809]  eta: 0:15:57  lr: 0.000032  min_lr: 0.000000  loss: 4.3702 (4.2999)  class_acc: 0.2083 (0.2691)  loss_scale: 32768.0000 (41069.6164)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1149  max mem: 15572
Epoch: [18]  [1130/2809]  eta: 0:15:50  lr: 0.000032  min_lr: 0.000000  loss: 4.2977 (4.2994)  class_acc: 0.2083 (0.2688)  loss_scale: 32768.0000 (40996.2157)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0562  max mem: 15572
[2025-01-15 23:12:30,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:12:30,820] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [1140/2809]  eta: 0:15:44  lr: 0.000032  min_lr: 0.000000  loss: 4.1864 (4.2970)  class_acc: 0.2500 (0.2692)  loss_scale: 32768.0000 (41153.8510)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0536  max mem: 15572
[2025-01-15 23:12:39,358] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51712
[2025-01-15 23:12:39,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:12:39,360] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [1150/2809]  eta: 0:15:38  lr: 0.000032  min_lr: 0.000000  loss: 4.0897 (4.2969)  class_acc: 0.2917 (0.2697)  loss_scale: 65536.0000 (41337.2163)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0754  max mem: 15572
Epoch: [18]  [1160/2809]  eta: 0:15:33  lr: 0.000032  min_lr: 0.000000  loss: 4.2606 (4.2977)  class_acc: 0.2500 (0.2699)  loss_scale: 32768.0000 (41263.4074)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1285  max mem: 15572
Epoch: [18]  [1170/2809]  eta: 0:15:26  lr: 0.000032  min_lr: 0.000000  loss: 4.4888 (4.2987)  class_acc: 0.2500 (0.2701)  loss_scale: 32768.0000 (41190.8591)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1106  max mem: 15572
Epoch: [18]  [1180/2809]  eta: 0:15:20  lr: 0.000032  min_lr: 0.000000  loss: 4.3827 (4.2984)  class_acc: 0.2917 (0.2703)  loss_scale: 32768.0000 (41119.5394)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0808  max mem: 15572
Epoch: [18]  [1190/2809]  eta: 0:15:14  lr: 0.000032  min_lr: 0.000000  loss: 4.2470 (4.2993)  class_acc: 0.2500 (0.2699)  loss_scale: 32768.0000 (41049.4173)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0649  max mem: 15572
Epoch: [18]  [1200/2809]  eta: 0:15:10  lr: 0.000032  min_lr: 0.000000  loss: 4.1871 (4.2980)  class_acc: 0.2083 (0.2698)  loss_scale: 32768.0000 (40980.4629)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1518  max mem: 15572
Epoch: [18]  [1210/2809]  eta: 0:15:05  lr: 0.000032  min_lr: 0.000000  loss: 4.1511 (4.2970)  class_acc: 0.2917 (0.2700)  loss_scale: 32768.0000 (40912.6474)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1890  max mem: 15572
Epoch: [18]  [1220/2809]  eta: 0:14:59  lr: 0.000032  min_lr: 0.000000  loss: 4.2674 (4.2974)  class_acc: 0.2500 (0.2700)  loss_scale: 32768.0000 (40845.9427)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1357  max mem: 15572
Epoch: [18]  [1230/2809]  eta: 0:14:53  lr: 0.000032  min_lr: 0.000000  loss: 4.2332 (4.2972)  class_acc: 0.2500 (0.2701)  loss_scale: 32768.0000 (40780.3217)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1059  max mem: 15572
Epoch: [18]  [1240/2809]  eta: 0:14:48  lr: 0.000032  min_lr: 0.000000  loss: 4.1977 (4.2972)  class_acc: 0.3333 (0.2707)  loss_scale: 32768.0000 (40715.7583)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1276  max mem: 15572
Epoch: [18]  [1250/2809]  eta: 0:14:42  lr: 0.000032  min_lr: 0.000000  loss: 4.2377 (4.2973)  class_acc: 0.3333 (0.2709)  loss_scale: 32768.0000 (40652.2270)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1382  max mem: 15572
Epoch: [18]  [1260/2809]  eta: 0:14:37  lr: 0.000032  min_lr: 0.000000  loss: 4.2377 (4.2972)  class_acc: 0.2500 (0.2708)  loss_scale: 32768.0000 (40589.7034)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1288  max mem: 15572
Epoch: [18]  [1270/2809]  eta: 0:14:31  lr: 0.000032  min_lr: 0.000000  loss: 4.2808 (4.2967)  class_acc: 0.2500 (0.2708)  loss_scale: 32768.0000 (40528.1637)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1190  max mem: 15572
[2025-01-15 23:13:52,678] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:13:52,679] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [1280/2809]  eta: 0:14:24  lr: 0.000032  min_lr: 0.000000  loss: 4.2851 (4.2970)  class_acc: 0.2917 (0.2708)  loss_scale: 32768.0000 (40518.7447)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0450  max mem: 15572
[2025-01-15 23:13:57,577] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51848
[2025-01-15 23:13:57,578] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:13:57,579] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [1290/2809]  eta: 0:14:20  lr: 0.000032  min_lr: 0.000000  loss: 4.3011 (4.2969)  class_acc: 0.2917 (0.2708)  loss_scale: 32768.0000 (40585.6174)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1064  max mem: 15572
Epoch: [18]  [1300/2809]  eta: 0:14:14  lr: 0.000032  min_lr: 0.000000  loss: 4.3011 (4.2972)  class_acc: 0.3333 (0.2712)  loss_scale: 32768.0000 (40525.5281)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1620  max mem: 15572
Epoch: [18]  [1310/2809]  eta: 0:14:08  lr: 0.000032  min_lr: 0.000000  loss: 4.2917 (4.2969)  class_acc: 0.3333 (0.2715)  loss_scale: 32768.0000 (40466.3555)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1164  max mem: 15572
Epoch: [18]  [1320/2809]  eta: 0:14:02  lr: 0.000032  min_lr: 0.000000  loss: 4.2843 (4.2962)  class_acc: 0.2917 (0.2713)  loss_scale: 32768.0000 (40408.0787)  weight_decay: 0.0500 (0.0500)  time: 0.5274  data: 0.0761  max mem: 15572
Epoch: [18]  [1330/2809]  eta: 0:13:56  lr: 0.000032  min_lr: 0.000000  loss: 4.3726 (4.2967)  class_acc: 0.2083 (0.2709)  loss_scale: 32768.0000 (40350.6777)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0893  max mem: 15572
Epoch: [18]  [1340/2809]  eta: 0:13:51  lr: 0.000032  min_lr: 0.000000  loss: 4.3476 (4.2970)  class_acc: 0.2083 (0.2707)  loss_scale: 32768.0000 (40294.1327)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1279  max mem: 15572
Epoch: [18]  [1350/2809]  eta: 0:13:44  lr: 0.000032  min_lr: 0.000000  loss: 4.2743 (4.2966)  class_acc: 0.2500 (0.2704)  loss_scale: 32768.0000 (40238.4249)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0964  max mem: 15572
Epoch: [18]  [1360/2809]  eta: 0:13:38  lr: 0.000032  min_lr: 0.000000  loss: 4.2188 (4.2970)  class_acc: 0.2083 (0.2702)  loss_scale: 32768.0000 (40183.5356)  weight_decay: 0.0500 (0.0500)  time: 0.5124  data: 0.0711  max mem: 15572
Epoch: [18]  [1370/2809]  eta: 0:13:33  lr: 0.000032  min_lr: 0.000000  loss: 4.2583 (4.2977)  class_acc: 0.1667 (0.2698)  loss_scale: 32768.0000 (40129.4471)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1467  max mem: 15572
Epoch: [18]  [1380/2809]  eta: 0:13:27  lr: 0.000032  min_lr: 0.000000  loss: 4.2801 (4.2976)  class_acc: 0.2083 (0.2696)  loss_scale: 32768.0000 (40076.1419)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1448  max mem: 15572
Epoch: [18]  [1390/2809]  eta: 0:13:22  lr: 0.000032  min_lr: 0.000000  loss: 4.2801 (4.2977)  class_acc: 0.2500 (0.2699)  loss_scale: 32768.0000 (40023.6032)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1011  max mem: 15572
Epoch: [18]  [1400/2809]  eta: 0:13:16  lr: 0.000032  min_lr: 0.000000  loss: 4.2607 (4.2979)  class_acc: 0.2500 (0.2699)  loss_scale: 32768.0000 (39971.8144)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1113  max mem: 15572
Epoch: [18]  [1410/2809]  eta: 0:13:10  lr: 0.000032  min_lr: 0.000000  loss: 4.2764 (4.2982)  class_acc: 0.2500 (0.2696)  loss_scale: 32768.0000 (39920.7597)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0841  max mem: 15572
[2025-01-15 23:15:09,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:15:09,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [1420/2809]  eta: 0:13:05  lr: 0.000032  min_lr: 0.000000  loss: 4.2979 (4.2984)  class_acc: 0.2083 (0.2697)  loss_scale: 32768.0000 (40008.7825)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1405  max mem: 15572
Epoch: [18]  [1430/2809]  eta: 0:13:00  lr: 0.000032  min_lr: 0.000000  loss: 4.4831 (4.2984)  class_acc: 0.2500 (0.2697)  loss_scale: 65536.0000 (40187.1698)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1754  max mem: 15572
[2025-01-15 23:15:21,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=52000, skipped=324, lr=[3.068017621427496e-07, 3.068017621427496e-07, 4.382882316324995e-07, 4.382882316324995e-07, 6.26126045189285e-07, 6.26126045189285e-07, 8.944657788418358e-07, 8.944657788418358e-07, 1.277808255488337e-06, 1.277808255488337e-06, 1.8254403649833386e-06, 1.8254403649833386e-06, 2.607771949976198e-06, 2.607771949976198e-06, 3.7253884999659977e-06, 3.7253884999659977e-06, 5.321983571379997e-06, 5.321983571379997e-06, 7.602833673399996e-06, 7.602833673399996e-06, 1.0861190961999994e-05, 1.0861190961999994e-05, 1.5515987088571423e-05, 1.5515987088571423e-05, 2.2165695840816318e-05, 2.2165695840816318e-05, 3.166527977259474e-05, 3.166527977259474e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:15:21,289] [INFO] [timer.py:260:stop] epoch=0/micro_step=52000/global_step=52000, RunningAvgSamplesPerSec=27.74985513406173, CurrSamplesPerSec=31.131604502399277, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [18]  [1440/2809]  eta: 0:12:54  lr: 0.000032  min_lr: 0.000000  loss: 4.4061 (4.2994)  class_acc: 0.2917 (0.2698)  loss_scale: 65536.0000 (40363.0812)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1239  max mem: 15572
[2025-01-15 23:15:25,660] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52006
[2025-01-15 23:15:25,661] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:15:25,662] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [1450/2809]  eta: 0:12:48  lr: 0.000032  min_lr: 0.000000  loss: 4.2901 (4.2987)  class_acc: 0.3333 (0.2701)  loss_scale: 65536.0000 (40378.4866)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1034  max mem: 15572
Epoch: [18]  [1460/2809]  eta: 0:12:43  lr: 0.000032  min_lr: 0.000000  loss: 4.1687 (4.2979)  class_acc: 0.3333 (0.2702)  loss_scale: 32768.0000 (40326.3956)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1415  max mem: 15572
Epoch: [18]  [1470/2809]  eta: 0:12:37  lr: 0.000032  min_lr: 0.000000  loss: 4.1751 (4.2978)  class_acc: 0.2917 (0.2703)  loss_scale: 32768.0000 (40275.0129)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0947  max mem: 15572
Epoch: [18]  [1480/2809]  eta: 0:12:31  lr: 0.000032  min_lr: 0.000000  loss: 4.2925 (4.2984)  class_acc: 0.2500 (0.2700)  loss_scale: 32768.0000 (40224.3241)  weight_decay: 0.0500 (0.0500)  time: 0.4967  data: 0.0397  max mem: 15572
Epoch: [18]  [1490/2809]  eta: 0:12:25  lr: 0.000032  min_lr: 0.000000  loss: 4.1853 (4.2965)  class_acc: 0.2500 (0.2703)  loss_scale: 32768.0000 (40174.3152)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1307  max mem: 15572
Epoch: [18]  [1500/2809]  eta: 0:12:20  lr: 0.000032  min_lr: 0.000000  loss: 4.1853 (4.2970)  class_acc: 0.2500 (0.2700)  loss_scale: 32768.0000 (40124.9727)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1655  max mem: 15572
Epoch: [18]  [1510/2809]  eta: 0:12:15  lr: 0.000032  min_lr: 0.000000  loss: 4.3352 (4.2971)  class_acc: 0.2500 (0.2702)  loss_scale: 32768.0000 (40076.2833)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1347  max mem: 15572
Epoch: [18]  [1520/2809]  eta: 0:12:09  lr: 0.000032  min_lr: 0.000000  loss: 4.3406 (4.2981)  class_acc: 0.2500 (0.2701)  loss_scale: 32768.0000 (40028.2341)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0920  max mem: 15572
Epoch: [18]  [1530/2809]  eta: 0:12:02  lr: 0.000032  min_lr: 0.000000  loss: 4.3250 (4.2979)  class_acc: 0.2917 (0.2704)  loss_scale: 32768.0000 (39980.8125)  weight_decay: 0.0500 (0.0500)  time: 0.4912  data: 0.0317  max mem: 15572
Epoch: [18]  [1540/2809]  eta: 0:11:57  lr: 0.000032  min_lr: 0.000000  loss: 4.2541 (4.2975)  class_acc: 0.2917 (0.2702)  loss_scale: 32768.0000 (39934.0065)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0610  max mem: 15572
Epoch: [18]  [1550/2809]  eta: 0:11:52  lr: 0.000032  min_lr: 0.000000  loss: 4.2641 (4.2977)  class_acc: 0.2917 (0.2706)  loss_scale: 32768.0000 (39887.8040)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1826  max mem: 15572
Epoch: [18]  [1560/2809]  eta: 0:11:46  lr: 0.000032  min_lr: 0.000000  loss: 4.3935 (4.2983)  class_acc: 0.2917 (0.2705)  loss_scale: 32768.0000 (39842.1935)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1955  max mem: 15572
Epoch: [18]  [1570/2809]  eta: 0:11:40  lr: 0.000032  min_lr: 0.000000  loss: 4.4442 (4.2991)  class_acc: 0.1667 (0.2699)  loss_scale: 32768.0000 (39797.1636)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.1036  max mem: 15572
[2025-01-15 23:16:38,271] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:16:38,271] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [1580/2809]  eta: 0:11:34  lr: 0.000032  min_lr: 0.000000  loss: 4.3445 (4.2992)  class_acc: 0.1667 (0.2695)  loss_scale: 32768.0000 (39918.5123)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0766  max mem: 15572
Epoch: [18]  [1590/2809]  eta: 0:11:29  lr: 0.000032  min_lr: 0.000000  loss: 4.3445 (4.2997)  class_acc: 0.2083 (0.2696)  loss_scale: 65536.0000 (40079.5273)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1449  max mem: 15572
Epoch: [18]  [1600/2809]  eta: 0:11:24  lr: 0.000032  min_lr: 0.000000  loss: 4.3557 (4.2996)  class_acc: 0.2500 (0.2695)  loss_scale: 65536.0000 (40238.5309)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1462  max mem: 15572
Epoch: [18]  [1610/2809]  eta: 0:11:18  lr: 0.000032  min_lr: 0.000000  loss: 4.2644 (4.2995)  class_acc: 0.2500 (0.2693)  loss_scale: 65536.0000 (40395.5605)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0812  max mem: 15572
Epoch: [18]  [1620/2809]  eta: 0:11:12  lr: 0.000032  min_lr: 0.000000  loss: 4.2659 (4.2998)  class_acc: 0.2500 (0.2692)  loss_scale: 65536.0000 (40550.6527)  weight_decay: 0.0500 (0.0500)  time: 0.5176  data: 0.0793  max mem: 15572
Epoch: [18]  [1630/2809]  eta: 0:11:06  lr: 0.000032  min_lr: 0.000000  loss: 4.3425 (4.3002)  class_acc: 0.2500 (0.2693)  loss_scale: 65536.0000 (40703.8430)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0967  max mem: 15572
Epoch: [18]  [1640/2809]  eta: 0:11:00  lr: 0.000032  min_lr: 0.000000  loss: 4.2729 (4.2996)  class_acc: 0.2917 (0.2695)  loss_scale: 65536.0000 (40855.1664)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1079  max mem: 15572
[2025-01-15 23:17:17,691] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52206
[2025-01-15 23:17:17,691] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:17:17,692] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [1650/2809]  eta: 0:10:54  lr: 0.000032  min_lr: 0.000000  loss: 4.2739 (4.3003)  class_acc: 0.2500 (0.2692)  loss_scale: 65536.0000 (40865.7250)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1006  max mem: 15572
Epoch: [18]  [1660/2809]  eta: 0:10:48  lr: 0.000032  min_lr: 0.000000  loss: 4.3427 (4.3012)  class_acc: 0.1667 (0.2691)  loss_scale: 32768.0000 (40816.9729)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0645  max mem: 15572
Epoch: [18]  [1670/2809]  eta: 0:10:43  lr: 0.000032  min_lr: 0.000000  loss: 4.3136 (4.3007)  class_acc: 0.2500 (0.2693)  loss_scale: 32768.0000 (40768.8043)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0961  max mem: 15572
Epoch: [18]  [1680/2809]  eta: 0:10:36  lr: 0.000031  min_lr: 0.000000  loss: 4.2036 (4.3005)  class_acc: 0.2917 (0.2697)  loss_scale: 32768.0000 (40721.2088)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0899  max mem: 15572
Epoch: [18]  [1690/2809]  eta: 0:10:31  lr: 0.000031  min_lr: 0.000000  loss: 4.2976 (4.3011)  class_acc: 0.3333 (0.2697)  loss_scale: 32768.0000 (40674.1762)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1234  max mem: 15572
Epoch: [18]  [1700/2809]  eta: 0:10:25  lr: 0.000031  min_lr: 0.000000  loss: 4.2976 (4.3008)  class_acc: 0.3750 (0.2704)  loss_scale: 32768.0000 (40627.6966)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1316  max mem: 15572
Epoch: [18]  [1710/2809]  eta: 0:10:19  lr: 0.000031  min_lr: 0.000000  loss: 4.2581 (4.3006)  class_acc: 0.2917 (0.2702)  loss_scale: 32768.0000 (40581.7604)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0586  max mem: 15572
Epoch: [18]  [1720/2809]  eta: 0:10:13  lr: 0.000031  min_lr: 0.000000  loss: 4.2655 (4.3007)  class_acc: 0.2500 (0.2703)  loss_scale: 32768.0000 (40536.3579)  weight_decay: 0.0500 (0.0500)  time: 0.5045  data: 0.0647  max mem: 15572
Epoch: [18]  [1730/2809]  eta: 0:10:08  lr: 0.000031  min_lr: 0.000000  loss: 4.3044 (4.3000)  class_acc: 0.2917 (0.2704)  loss_scale: 32768.0000 (40491.4801)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1118  max mem: 15572
Epoch: [18]  [1740/2809]  eta: 0:10:02  lr: 0.000031  min_lr: 0.000000  loss: 4.3562 (4.3010)  class_acc: 0.2500 (0.2704)  loss_scale: 32768.0000 (40447.1177)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1405  max mem: 15572
Epoch: [18]  [1750/2809]  eta: 0:09:57  lr: 0.000031  min_lr: 0.000000  loss: 4.4249 (4.3014)  class_acc: 0.2500 (0.2705)  loss_scale: 32768.0000 (40403.2621)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1345  max mem: 15572
Epoch: [18]  [1760/2809]  eta: 0:09:51  lr: 0.000031  min_lr: 0.000000  loss: 4.1699 (4.3009)  class_acc: 0.2917 (0.2706)  loss_scale: 32768.0000 (40359.9046)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1293  max mem: 15572
Epoch: [18]  [1770/2809]  eta: 0:09:45  lr: 0.000031  min_lr: 0.000000  loss: 4.2643 (4.3016)  class_acc: 0.2500 (0.2706)  loss_scale: 32768.0000 (40317.0367)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0891  max mem: 15572
[2025-01-15 23:18:28,744] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:18:28,744] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [1780/2809]  eta: 0:09:40  lr: 0.000031  min_lr: 0.000000  loss: 4.2261 (4.3006)  class_acc: 0.2500 (0.2705)  loss_scale: 32768.0000 (40421.8394)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1250  max mem: 15572
[2025-01-15 23:18:37,928] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52351
[2025-01-15 23:18:37,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:18:37,929] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [1790/2809]  eta: 0:09:34  lr: 0.000031  min_lr: 0.000000  loss: 4.0412 (4.2993)  class_acc: 0.2917 (0.2708)  loss_scale: 65536.0000 (40525.4718)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1298  max mem: 15572
Epoch: [18]  [1800/2809]  eta: 0:09:29  lr: 0.000031  min_lr: 0.000000  loss: 4.0927 (4.2993)  class_acc: 0.2500 (0.2709)  loss_scale: 32768.0000 (40482.3987)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1266  max mem: 15572
Epoch: [18]  [1810/2809]  eta: 0:09:23  lr: 0.000031  min_lr: 0.000000  loss: 4.4060 (4.3002)  class_acc: 0.2083 (0.2704)  loss_scale: 32768.0000 (40439.8012)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1274  max mem: 15572
Epoch: [18]  [1820/2809]  eta: 0:09:17  lr: 0.000031  min_lr: 0.000000  loss: 4.4204 (4.3004)  class_acc: 0.2083 (0.2706)  loss_scale: 32768.0000 (40397.6716)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0724  max mem: 15572
Epoch: [18]  [1830/2809]  eta: 0:09:12  lr: 0.000031  min_lr: 0.000000  loss: 4.3140 (4.3009)  class_acc: 0.2917 (0.2707)  loss_scale: 32768.0000 (40356.0022)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0801  max mem: 15572
Epoch: [18]  [1840/2809]  eta: 0:09:06  lr: 0.000031  min_lr: 0.000000  loss: 4.2422 (4.3003)  class_acc: 0.2500 (0.2706)  loss_scale: 32768.0000 (40314.7854)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0904  max mem: 15572
Epoch: [18]  [1850/2809]  eta: 0:09:00  lr: 0.000031  min_lr: 0.000000  loss: 4.3135 (4.3006)  class_acc: 0.2500 (0.2705)  loss_scale: 32768.0000 (40274.0140)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0536  max mem: 15572
Epoch: [18]  [1860/2809]  eta: 0:08:54  lr: 0.000031  min_lr: 0.000000  loss: 4.3332 (4.3013)  class_acc: 0.2500 (0.2702)  loss_scale: 32768.0000 (40233.6808)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0693  max mem: 15572
Epoch: [18]  [1870/2809]  eta: 0:08:49  lr: 0.000031  min_lr: 0.000000  loss: 4.3332 (4.3011)  class_acc: 0.2500 (0.2700)  loss_scale: 32768.0000 (40193.7787)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1243  max mem: 15572
Epoch: [18]  [1880/2809]  eta: 0:08:43  lr: 0.000031  min_lr: 0.000000  loss: 4.2710 (4.3011)  class_acc: 0.2917 (0.2706)  loss_scale: 32768.0000 (40154.3009)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0866  max mem: 15572
Epoch: [18]  [1890/2809]  eta: 0:08:37  lr: 0.000031  min_lr: 0.000000  loss: 4.2420 (4.3012)  class_acc: 0.2500 (0.2706)  loss_scale: 32768.0000 (40115.2406)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0671  max mem: 15572
Epoch: [18]  [1900/2809]  eta: 0:08:32  lr: 0.000031  min_lr: 0.000000  loss: 4.2407 (4.3011)  class_acc: 0.2083 (0.2701)  loss_scale: 32768.0000 (40076.5913)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1057  max mem: 15572
Epoch: [18]  [1910/2809]  eta: 0:08:26  lr: 0.000031  min_lr: 0.000000  loss: 4.3742 (4.3020)  class_acc: 0.1667 (0.2697)  loss_scale: 32768.0000 (40038.3464)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1184  max mem: 15572
[2025-01-15 23:19:49,978] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:19:49,978] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [1920/2809]  eta: 0:08:20  lr: 0.000031  min_lr: 0.000000  loss: 4.3172 (4.3017)  class_acc: 0.2500 (0.2697)  loss_scale: 32768.0000 (40051.6731)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0794  max mem: 15572
[2025-01-15 23:19:51,740] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52484
[2025-01-15 23:19:51,741] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:19:51,741] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [1930/2809]  eta: 0:08:15  lr: 0.000031  min_lr: 0.000000  loss: 4.2822 (4.3014)  class_acc: 0.2917 (0.2701)  loss_scale: 32768.0000 (40030.9228)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.1061  max mem: 15572
Epoch: [18]  [1940/2809]  eta: 0:08:09  lr: 0.000031  min_lr: 0.000000  loss: 4.2822 (4.3013)  class_acc: 0.2917 (0.2702)  loss_scale: 32768.0000 (39993.5044)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1656  max mem: 15572
Epoch: [18]  [1950/2809]  eta: 0:08:04  lr: 0.000031  min_lr: 0.000000  loss: 4.2903 (4.3017)  class_acc: 0.2917 (0.2703)  loss_scale: 32768.0000 (39956.4695)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1655  max mem: 15572
Epoch: [18]  [1960/2809]  eta: 0:07:59  lr: 0.000031  min_lr: 0.000000  loss: 4.2559 (4.3011)  class_acc: 0.2083 (0.2702)  loss_scale: 32768.0000 (39919.8123)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1433  max mem: 15572
Epoch: [18]  [1970/2809]  eta: 0:07:53  lr: 0.000031  min_lr: 0.000000  loss: 4.3070 (4.3014)  class_acc: 0.2083 (0.2702)  loss_scale: 32768.0000 (39883.5271)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0885  max mem: 15572
Epoch: [18]  [1980/2809]  eta: 0:07:47  lr: 0.000031  min_lr: 0.000000  loss: 4.3663 (4.3017)  class_acc: 0.2083 (0.2702)  loss_scale: 32768.0000 (39847.6083)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0834  max mem: 15572
Epoch: [18]  [1990/2809]  eta: 0:07:42  lr: 0.000031  min_lr: 0.000000  loss: 4.4062 (4.3022)  class_acc: 0.2500 (0.2700)  loss_scale: 32768.0000 (39812.0502)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1078  max mem: 15572
Epoch: [18]  [2000/2809]  eta: 0:07:36  lr: 0.000031  min_lr: 0.000000  loss: 4.3506 (4.3025)  class_acc: 0.2083 (0.2696)  loss_scale: 32768.0000 (39776.8476)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0930  max mem: 15572
Epoch: [18]  [2010/2809]  eta: 0:07:30  lr: 0.000031  min_lr: 0.000000  loss: 4.3406 (4.3030)  class_acc: 0.2083 (0.2696)  loss_scale: 32768.0000 (39741.9950)  weight_decay: 0.0500 (0.0500)  time: 0.4871  data: 0.0357  max mem: 15572
Epoch: [18]  [2020/2809]  eta: 0:07:24  lr: 0.000031  min_lr: 0.000000  loss: 4.3385 (4.3034)  class_acc: 0.2500 (0.2695)  loss_scale: 32768.0000 (39707.4874)  weight_decay: 0.0500 (0.0500)  time: 0.4720  data: 0.0267  max mem: 15572
Epoch: [18]  [2030/2809]  eta: 0:07:18  lr: 0.000031  min_lr: 0.000000  loss: 4.3179 (4.3034)  class_acc: 0.2500 (0.2695)  loss_scale: 32768.0000 (39673.3195)  weight_decay: 0.0500 (0.0500)  time: 0.5045  data: 0.0653  max mem: 15572
Epoch: [18]  [2040/2809]  eta: 0:07:12  lr: 0.000031  min_lr: 0.000000  loss: 4.2993 (4.3031)  class_acc: 0.2500 (0.2694)  loss_scale: 32768.0000 (39639.4865)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.1109  max mem: 15572
Epoch: [18]  [2050/2809]  eta: 0:07:07  lr: 0.000031  min_lr: 0.000000  loss: 4.2875 (4.3028)  class_acc: 0.2500 (0.2696)  loss_scale: 32768.0000 (39605.9834)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1841  max mem: 15572
[2025-01-15 23:21:04,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:21:04,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [2060/2809]  eta: 0:07:01  lr: 0.000031  min_lr: 0.000000  loss: 4.2875 (4.3032)  class_acc: 0.2917 (0.2696)  loss_scale: 32768.0000 (39731.7962)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1641  max mem: 15572
Epoch: [18]  [2070/2809]  eta: 0:06:56  lr: 0.000031  min_lr: 0.000000  loss: 4.2852 (4.3034)  class_acc: 0.3333 (0.2700)  loss_scale: 65536.0000 (39856.3940)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1010  max mem: 15572
Epoch: [18]  [2080/2809]  eta: 0:06:50  lr: 0.000031  min_lr: 0.000000  loss: 4.2852 (4.3033)  class_acc: 0.2917 (0.2699)  loss_scale: 65536.0000 (39979.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1151  max mem: 15572
[2025-01-15 23:21:26,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52652
[2025-01-15 23:21:26,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:21:26,482] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [2090/2809]  eta: 0:06:44  lr: 0.000031  min_lr: 0.000000  loss: 4.3604 (4.3032)  class_acc: 0.2500 (0.2700)  loss_scale: 65536.0000 (40086.3434)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1010  max mem: 15572
Epoch: [18]  [2100/2809]  eta: 0:06:39  lr: 0.000031  min_lr: 0.000000  loss: 4.1941 (4.3020)  class_acc: 0.2500 (0.2701)  loss_scale: 32768.0000 (40051.5107)  weight_decay: 0.0500 (0.0500)  time: 0.5305  data: 0.0760  max mem: 15572
Epoch: [18]  [2110/2809]  eta: 0:06:33  lr: 0.000031  min_lr: 0.000000  loss: 4.1826 (4.3021)  class_acc: 0.2083 (0.2699)  loss_scale: 32768.0000 (40017.0081)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0900  max mem: 15572
Epoch: [18]  [2120/2809]  eta: 0:06:28  lr: 0.000031  min_lr: 0.000000  loss: 4.4079 (4.3026)  class_acc: 0.2500 (0.2699)  loss_scale: 32768.0000 (39982.8307)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1378  max mem: 15572
Epoch: [18]  [2130/2809]  eta: 0:06:22  lr: 0.000031  min_lr: 0.000000  loss: 4.3746 (4.3029)  class_acc: 0.2500 (0.2699)  loss_scale: 32768.0000 (39948.9742)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1558  max mem: 15572
Epoch: [18]  [2140/2809]  eta: 0:06:16  lr: 0.000031  min_lr: 0.000000  loss: 4.3240 (4.3026)  class_acc: 0.2083 (0.2698)  loss_scale: 32768.0000 (39915.4339)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0868  max mem: 15572
Epoch: [18]  [2150/2809]  eta: 0:06:11  lr: 0.000031  min_lr: 0.000000  loss: 4.3837 (4.3032)  class_acc: 0.2083 (0.2698)  loss_scale: 32768.0000 (39882.2055)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0883  max mem: 15572
Epoch: [18]  [2160/2809]  eta: 0:06:05  lr: 0.000031  min_lr: 0.000000  loss: 4.4388 (4.3034)  class_acc: 0.2500 (0.2697)  loss_scale: 32768.0000 (39849.2846)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1010  max mem: 15572
Epoch: [18]  [2170/2809]  eta: 0:05:59  lr: 0.000031  min_lr: 0.000000  loss: 4.3571 (4.3036)  class_acc: 0.2500 (0.2698)  loss_scale: 32768.0000 (39816.6670)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0675  max mem: 15572
Epoch: [18]  [2180/2809]  eta: 0:05:54  lr: 0.000031  min_lr: 0.000000  loss: 4.3367 (4.3038)  class_acc: 0.2917 (0.2701)  loss_scale: 32768.0000 (39784.3485)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1192  max mem: 15572
Epoch: [18]  [2190/2809]  eta: 0:05:48  lr: 0.000031  min_lr: 0.000000  loss: 4.2962 (4.3038)  class_acc: 0.2500 (0.2700)  loss_scale: 32768.0000 (39752.3250)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1685  max mem: 15572
Epoch: [18]  [2200/2809]  eta: 0:05:42  lr: 0.000031  min_lr: 0.000000  loss: 4.2523 (4.3035)  class_acc: 0.2083 (0.2699)  loss_scale: 32768.0000 (39720.5925)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1475  max mem: 15572
Epoch: [18]  [2210/2809]  eta: 0:05:37  lr: 0.000031  min_lr: 0.000000  loss: 4.2289 (4.3030)  class_acc: 0.2500 (0.2701)  loss_scale: 32768.0000 (39689.1470)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1172  max mem: 15572
[2025-01-15 23:22:38,618] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:22:38,618] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [2220/2809]  eta: 0:05:31  lr: 0.000031  min_lr: 0.000000  loss: 4.3102 (4.3032)  class_acc: 0.2917 (0.2701)  loss_scale: 32768.0000 (39687.4921)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0812  max mem: 15572
Epoch: [18]  [2230/2809]  eta: 0:05:25  lr: 0.000031  min_lr: 0.000000  loss: 4.4435 (4.3036)  class_acc: 0.2500 (0.2700)  loss_scale: 65536.0000 (39803.3528)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0674  max mem: 15572
Epoch: [18]  [2240/2809]  eta: 0:05:20  lr: 0.000031  min_lr: 0.000000  loss: 4.2746 (4.3033)  class_acc: 0.2500 (0.2703)  loss_scale: 65536.0000 (39918.1794)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1427  max mem: 15572
Epoch: [18]  [2250/2809]  eta: 0:05:14  lr: 0.000031  min_lr: 0.000000  loss: 4.2322 (4.3036)  class_acc: 0.2917 (0.2701)  loss_scale: 65536.0000 (40031.9858)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1515  max mem: 15572
[2025-01-15 23:23:00,377] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52817
[2025-01-15 23:23:00,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:23:00,377] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [2260/2809]  eta: 0:05:09  lr: 0.000031  min_lr: 0.000000  loss: 4.2216 (4.3029)  class_acc: 0.2083 (0.2701)  loss_scale: 65536.0000 (40057.8293)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1206  max mem: 15572
Epoch: [18]  [2270/2809]  eta: 0:05:03  lr: 0.000031  min_lr: 0.000000  loss: 4.1798 (4.3030)  class_acc: 0.2500 (0.2702)  loss_scale: 32768.0000 (40025.7296)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1500  max mem: 15572
Epoch: [18]  [2280/2809]  eta: 0:04:58  lr: 0.000031  min_lr: 0.000000  loss: 4.2138 (4.3027)  class_acc: 0.2917 (0.2704)  loss_scale: 32768.0000 (39993.9114)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1149  max mem: 15572
Epoch: [18]  [2290/2809]  eta: 0:04:52  lr: 0.000031  min_lr: 0.000000  loss: 4.3044 (4.3026)  class_acc: 0.3333 (0.2706)  loss_scale: 32768.0000 (39962.3710)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0984  max mem: 15572
Epoch: [18]  [2300/2809]  eta: 0:04:46  lr: 0.000031  min_lr: 0.000000  loss: 4.3723 (4.3027)  class_acc: 0.2083 (0.2705)  loss_scale: 32768.0000 (39931.1047)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1176  max mem: 15572
Epoch: [18]  [2310/2809]  eta: 0:04:41  lr: 0.000031  min_lr: 0.000000  loss: 4.3110 (4.3024)  class_acc: 0.2083 (0.2706)  loss_scale: 32768.0000 (39900.1090)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1156  max mem: 15572
Epoch: [18]  [2320/2809]  eta: 0:04:35  lr: 0.000031  min_lr: 0.000000  loss: 4.2029 (4.3025)  class_acc: 0.2083 (0.2705)  loss_scale: 32768.0000 (39869.3804)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1328  max mem: 15572
Epoch: [18]  [2330/2809]  eta: 0:04:29  lr: 0.000031  min_lr: 0.000000  loss: 4.2542 (4.3025)  class_acc: 0.2500 (0.2706)  loss_scale: 32768.0000 (39838.9155)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0931  max mem: 15572
Epoch: [18]  [2340/2809]  eta: 0:04:24  lr: 0.000031  min_lr: 0.000000  loss: 4.2503 (4.3019)  class_acc: 0.2917 (0.2707)  loss_scale: 32768.0000 (39808.7108)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0641  max mem: 15572
Epoch: [18]  [2350/2809]  eta: 0:04:18  lr: 0.000031  min_lr: 0.000000  loss: 4.1974 (4.3016)  class_acc: 0.2917 (0.2709)  loss_scale: 32768.0000 (39778.7631)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1137  max mem: 15572
Epoch: [18]  [2360/2809]  eta: 0:04:12  lr: 0.000031  min_lr: 0.000000  loss: 4.2798 (4.3014)  class_acc: 0.2500 (0.2708)  loss_scale: 32768.0000 (39749.0690)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1086  max mem: 15572
Epoch: [18]  [2370/2809]  eta: 0:04:07  lr: 0.000031  min_lr: 0.000000  loss: 4.3048 (4.3018)  class_acc: 0.2083 (0.2706)  loss_scale: 32768.0000 (39719.6255)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1024  max mem: 15572
Epoch: [18]  [2380/2809]  eta: 0:04:01  lr: 0.000031  min_lr: 0.000000  loss: 4.3751 (4.3027)  class_acc: 0.2500 (0.2708)  loss_scale: 32768.0000 (39690.4292)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1106  max mem: 15572
[2025-01-15 23:24:12,451] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:24:12,451] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [2390/2809]  eta: 0:03:55  lr: 0.000031  min_lr: 0.000000  loss: 4.4503 (4.3028)  class_acc: 0.2917 (0.2708)  loss_scale: 32768.0000 (39757.4103)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0876  max mem: 15572
Epoch: [18]  [2400/2809]  eta: 0:03:50  lr: 0.000031  min_lr: 0.000000  loss: 4.2631 (4.3023)  class_acc: 0.2917 (0.2706)  loss_scale: 65536.0000 (39864.7763)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0904  max mem: 15572
Epoch: [18]  [2410/2809]  eta: 0:03:44  lr: 0.000031  min_lr: 0.000000  loss: 4.2143 (4.3018)  class_acc: 0.2917 (0.2707)  loss_scale: 65536.0000 (39971.2518)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1098  max mem: 15572
Epoch: [18]  [2420/2809]  eta: 0:03:38  lr: 0.000031  min_lr: 0.000000  loss: 4.2792 (4.3022)  class_acc: 0.2917 (0.2705)  loss_scale: 65536.0000 (40076.8476)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0799  max mem: 15572
[2025-01-15 23:24:36,969] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52990
[2025-01-15 23:24:36,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:24:36,970] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [2430/2809]  eta: 0:03:33  lr: 0.000031  min_lr: 0.000000  loss: 4.3708 (4.3025)  class_acc: 0.2083 (0.2704)  loss_scale: 65536.0000 (40141.1370)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1325  max mem: 15572
[2025-01-15 23:24:42,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=53000, skipped=331, lr=[2.9997287810269747e-07, 2.9997287810269747e-07, 4.2853268300385355e-07, 4.2853268300385355e-07, 6.121895471483623e-07, 6.121895471483623e-07, 8.745564959262318e-07, 8.745564959262318e-07, 1.2493664227517598e-06, 1.2493664227517598e-06, 1.784809175359657e-06, 1.784809175359657e-06, 2.5497273933709387e-06, 2.5497273933709387e-06, 3.642467704815627e-06, 3.642467704815627e-06, 5.203525292593753e-06, 5.203525292593753e-06, 7.433607560848219e-06, 7.433607560848219e-06, 1.0619439372640312e-05, 1.0619439372640312e-05, 1.517062767520045e-05, 1.517062767520045e-05, 2.1672325250286358e-05, 2.1672325250286358e-05, 3.096046464326623e-05, 3.096046464326623e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:24:42,625] [INFO] [timer.py:260:stop] epoch=0/micro_step=53000/global_step=53000, RunningAvgSamplesPerSec=27.756296842243003, CurrSamplesPerSec=24.876694616816057, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [18]  [2440/2809]  eta: 0:03:27  lr: 0.000031  min_lr: 0.000000  loss: 4.4451 (4.3033)  class_acc: 0.2083 (0.2704)  loss_scale: 32768.0000 (40110.9316)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.1972  max mem: 15572
Epoch: [18]  [2450/2809]  eta: 0:03:22  lr: 0.000031  min_lr: 0.000000  loss: 4.3691 (4.3028)  class_acc: 0.2917 (0.2706)  loss_scale: 32768.0000 (40080.9727)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0854  max mem: 15572
Epoch: [18]  [2460/2809]  eta: 0:03:16  lr: 0.000031  min_lr: 0.000000  loss: 4.2269 (4.3028)  class_acc: 0.2917 (0.2705)  loss_scale: 32768.0000 (40051.2572)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0713  max mem: 15572
Epoch: [18]  [2470/2809]  eta: 0:03:11  lr: 0.000031  min_lr: 0.000000  loss: 4.3331 (4.3031)  class_acc: 0.2083 (0.2703)  loss_scale: 32768.0000 (40021.7823)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.1333  max mem: 15572
Epoch: [18]  [2480/2809]  eta: 0:03:05  lr: 0.000031  min_lr: 0.000000  loss: 4.2693 (4.3029)  class_acc: 0.2500 (0.2704)  loss_scale: 32768.0000 (39992.5449)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1171  max mem: 15572
Epoch: [18]  [2490/2809]  eta: 0:02:59  lr: 0.000031  min_lr: 0.000000  loss: 4.2463 (4.3026)  class_acc: 0.2917 (0.2705)  loss_scale: 32768.0000 (39963.5424)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0663  max mem: 15572
Epoch: [18]  [2500/2809]  eta: 0:02:54  lr: 0.000031  min_lr: 0.000000  loss: 4.2538 (4.3028)  class_acc: 0.2917 (0.2707)  loss_scale: 32768.0000 (39934.7717)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1302  max mem: 15572
Epoch: [18]  [2510/2809]  eta: 0:02:48  lr: 0.000031  min_lr: 0.000000  loss: 4.2830 (4.3025)  class_acc: 0.2917 (0.2708)  loss_scale: 32768.0000 (39906.2302)  weight_decay: 0.0500 (0.0500)  time: 0.6538  data: 0.1516  max mem: 15572
Epoch: [18]  [2520/2809]  eta: 0:02:42  lr: 0.000031  min_lr: 0.000000  loss: 4.1967 (4.3020)  class_acc: 0.2917 (0.2709)  loss_scale: 32768.0000 (39877.9151)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0334  max mem: 15572
Epoch: [18]  [2530/2809]  eta: 0:02:37  lr: 0.000031  min_lr: 0.000000  loss: 4.1609 (4.3019)  class_acc: 0.2917 (0.2708)  loss_scale: 32768.0000 (39849.8238)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0007  max mem: 15572
Epoch: [18]  [2540/2809]  eta: 0:02:31  lr: 0.000031  min_lr: 0.000000  loss: 4.2337 (4.3024)  class_acc: 0.2917 (0.2709)  loss_scale: 32768.0000 (39821.9536)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0131  max mem: 15572
Epoch: [18]  [2550/2809]  eta: 0:02:26  lr: 0.000031  min_lr: 0.000000  loss: 4.3496 (4.3023)  class_acc: 0.2917 (0.2711)  loss_scale: 32768.0000 (39794.3018)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0932  max mem: 15572
[2025-01-15 23:25:52,387] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:25:52,388] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [2560/2809]  eta: 0:02:20  lr: 0.000031  min_lr: 0.000000  loss: 4.3093 (4.3026)  class_acc: 0.2500 (0.2708)  loss_scale: 32768.0000 (39818.0461)  weight_decay: 0.0500 (0.0500)  time: 0.6531  data: 0.1524  max mem: 15572
[2025-01-15 23:25:58,387] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53132
[2025-01-15 23:25:58,387] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:25:58,388] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [2570/2809]  eta: 0:02:14  lr: 0.000031  min_lr: 0.000000  loss: 4.3093 (4.3025)  class_acc: 0.2083 (0.2708)  loss_scale: 65536.0000 (39905.3318)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0793  max mem: 15572
Epoch: [18]  [2580/2809]  eta: 0:02:09  lr: 0.000031  min_lr: 0.000000  loss: 4.2634 (4.3021)  class_acc: 0.2917 (0.2710)  loss_scale: 32768.0000 (39877.6784)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1146  max mem: 15572
Epoch: [18]  [2590/2809]  eta: 0:02:03  lr: 0.000031  min_lr: 0.000000  loss: 4.3067 (4.3022)  class_acc: 0.2917 (0.2711)  loss_scale: 32768.0000 (39850.2385)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1547  max mem: 15572
Epoch: [18]  [2600/2809]  eta: 0:01:57  lr: 0.000031  min_lr: 0.000000  loss: 4.3594 (4.3025)  class_acc: 0.2500 (0.2710)  loss_scale: 32768.0000 (39823.0096)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0748  max mem: 15572
Epoch: [18]  [2610/2809]  eta: 0:01:52  lr: 0.000031  min_lr: 0.000000  loss: 4.2935 (4.3019)  class_acc: 0.2917 (0.2714)  loss_scale: 32768.0000 (39795.9893)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0620  max mem: 15572
Epoch: [18]  [2620/2809]  eta: 0:01:46  lr: 0.000031  min_lr: 0.000000  loss: 4.2271 (4.3022)  class_acc: 0.2917 (0.2712)  loss_scale: 32768.0000 (39769.1751)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1339  max mem: 15572
Epoch: [18]  [2630/2809]  eta: 0:01:41  lr: 0.000031  min_lr: 0.000000  loss: 4.3831 (4.3026)  class_acc: 0.2083 (0.2712)  loss_scale: 32768.0000 (39742.5648)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.1850  max mem: 15572
Epoch: [18]  [2640/2809]  eta: 0:01:35  lr: 0.000031  min_lr: 0.000000  loss: 4.4085 (4.3024)  class_acc: 0.3333 (0.2716)  loss_scale: 32768.0000 (39716.1560)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1141  max mem: 15572
Epoch: [18]  [2650/2809]  eta: 0:01:29  lr: 0.000031  min_lr: 0.000000  loss: 4.2782 (4.3021)  class_acc: 0.3333 (0.2715)  loss_scale: 32768.0000 (39689.9464)  weight_decay: 0.0500 (0.0500)  time: 0.4893  data: 0.0573  max mem: 15572
Epoch: [18]  [2660/2809]  eta: 0:01:24  lr: 0.000031  min_lr: 0.000000  loss: 4.1611 (4.3022)  class_acc: 0.2500 (0.2715)  loss_scale: 32768.0000 (39663.9339)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1306  max mem: 15572
Epoch: [18]  [2670/2809]  eta: 0:01:18  lr: 0.000031  min_lr: 0.000000  loss: 4.3818 (4.3024)  class_acc: 0.2500 (0.2715)  loss_scale: 32768.0000 (39638.1161)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1616  max mem: 15572
Epoch: [18]  [2680/2809]  eta: 0:01:12  lr: 0.000031  min_lr: 0.000000  loss: 4.2646 (4.3019)  class_acc: 0.2500 (0.2716)  loss_scale: 32768.0000 (39612.4909)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1277  max mem: 15572
Epoch: [18]  [2690/2809]  eta: 0:01:07  lr: 0.000031  min_lr: 0.000000  loss: 4.2450 (4.3018)  class_acc: 0.2917 (0.2717)  loss_scale: 32768.0000 (39587.0561)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1145  max mem: 15572
[2025-01-15 23:27:13,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:27:13,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [2700/2809]  eta: 0:01:01  lr: 0.000031  min_lr: 0.000000  loss: 4.2071 (4.3012)  class_acc: 0.3333 (0.2719)  loss_scale: 32768.0000 (39586.0733)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1018  max mem: 15572
Epoch: [18]  [2710/2809]  eta: 0:00:55  lr: 0.000031  min_lr: 0.000000  loss: 4.1233 (4.3008)  class_acc: 0.2500 (0.2718)  loss_scale: 65536.0000 (39681.7942)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0817  max mem: 15572
[2025-01-15 23:27:22,950] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53276
[2025-01-15 23:27:22,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:27:22,951] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [2720/2809]  eta: 0:00:50  lr: 0.000031  min_lr: 0.000000  loss: 4.2881 (4.3012)  class_acc: 0.2500 (0.2718)  loss_scale: 65536.0000 (39692.5130)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1090  max mem: 15572
Epoch: [18]  [2730/2809]  eta: 0:00:44  lr: 0.000031  min_lr: 0.000000  loss: 4.4182 (4.3012)  class_acc: 0.2500 (0.2717)  loss_scale: 32768.0000 (39667.1578)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1301  max mem: 15572
Epoch: [18]  [2740/2809]  eta: 0:00:38  lr: 0.000031  min_lr: 0.000000  loss: 4.4654 (4.3015)  class_acc: 0.2500 (0.2720)  loss_scale: 32768.0000 (39641.9876)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0678  max mem: 15572
Epoch: [18]  [2750/2809]  eta: 0:00:33  lr: 0.000031  min_lr: 0.000000  loss: 4.4654 (4.3016)  class_acc: 0.2917 (0.2720)  loss_scale: 32768.0000 (39617.0004)  weight_decay: 0.0500 (0.0500)  time: 0.4386  data: 0.0204  max mem: 15572
Epoch: [18]  [2760/2809]  eta: 0:00:27  lr: 0.000031  min_lr: 0.000000  loss: 4.3875 (4.3016)  class_acc: 0.2917 (0.2722)  loss_scale: 32768.0000 (39592.1941)  weight_decay: 0.0500 (0.0500)  time: 0.4448  data: 0.0008  max mem: 15572
Epoch: [18]  [2770/2809]  eta: 0:00:21  lr: 0.000031  min_lr: 0.000000  loss: 4.3795 (4.3018)  class_acc: 0.2500 (0.2722)  loss_scale: 32768.0000 (39567.5669)  weight_decay: 0.0500 (0.0500)  time: 0.4884  data: 0.0010  max mem: 15572
Epoch: [18]  [2780/2809]  eta: 0:00:16  lr: 0.000031  min_lr: 0.000000  loss: 4.3788 (4.3019)  class_acc: 0.2500 (0.2722)  loss_scale: 32768.0000 (39543.1169)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0261  max mem: 15572
Epoch: [18]  [2790/2809]  eta: 0:00:10  lr: 0.000031  min_lr: 0.000000  loss: 4.2342 (4.3019)  class_acc: 0.2500 (0.2721)  loss_scale: 32768.0000 (39518.8420)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0608  max mem: 15572
Epoch: [18]  [2800/2809]  eta: 0:00:05  lr: 0.000031  min_lr: 0.000000  loss: 4.3320 (4.3017)  class_acc: 0.2083 (0.2721)  loss_scale: 32768.0000 (39494.7404)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0720  max mem: 15572
Epoch: [18]  [2808/2809]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000000  loss: 4.2988 (4.3016)  class_acc: 0.2500 (0.2721)  loss_scale: 32768.0000 (39475.5828)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0734  max mem: 15572
Epoch: [18] Total time: 0:26:23 (0.5639 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000000  loss: 4.2988 (4.3016)  class_acc: 0.2500 (0.2721)  loss_scale: 32768.0000 (39475.5828)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:29:10  loss: 1.1204 (1.1204)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.4360  data: 6.2384  max mem: 15572
Val:  [ 10/272]  eta: 0:04:11  loss: 2.8143 (2.7819)  acc1: 38.8889 (38.3838)  acc5: 66.6667 (67.6768)  time: 0.9611  data: 0.7509  max mem: 15572
Val:  [ 20/272]  eta: 0:02:39  loss: 2.8143 (2.8264)  acc1: 38.8889 (38.6243)  acc5: 66.6667 (68.2540)  time: 0.3413  data: 0.1372  max mem: 15572
Val:  [ 30/272]  eta: 0:02:05  loss: 2.8515 (2.8875)  acc1: 33.3333 (34.9462)  acc5: 72.2222 (68.1004)  time: 0.2783  data: 0.0662  max mem: 15572
Val:  [ 40/272]  eta: 0:01:54  loss: 2.9169 (2.8671)  acc1: 27.7778 (33.7398)  acc5: 72.2222 (68.9702)  time: 0.3521  data: 0.1284  max mem: 15572
Val:  [ 50/272]  eta: 0:01:44  loss: 2.6758 (2.8182)  acc1: 33.3333 (35.0763)  acc5: 72.2222 (71.3508)  time: 0.3978  data: 0.1859  max mem: 15572
Val:  [ 60/272]  eta: 0:01:36  loss: 2.2202 (2.7352)  acc1: 50.0000 (37.8871)  acc5: 83.3333 (72.3133)  time: 0.3757  data: 0.1717  max mem: 15572
Val:  [ 70/272]  eta: 0:01:29  loss: 2.1866 (2.6692)  acc1: 61.1111 (40.4538)  acc5: 83.3333 (73.7872)  time: 0.3602  data: 0.1566  max mem: 15572
Val:  [ 80/272]  eta: 0:01:23  loss: 2.3239 (2.6674)  acc1: 50.0000 (40.8779)  acc5: 77.7778 (73.3882)  time: 0.3649  data: 0.1591  max mem: 15572
Val:  [ 90/272]  eta: 0:01:17  loss: 2.8762 (2.6941)  acc1: 38.8889 (40.2320)  acc5: 66.6667 (72.8327)  time: 0.3756  data: 0.1645  max mem: 15572
Val:  [100/272]  eta: 0:01:11  loss: 2.8889 (2.7271)  acc1: 33.3333 (39.1639)  acc5: 72.2222 (72.3872)  time: 0.3318  data: 0.1397  max mem: 15572
Val:  [110/272]  eta: 0:01:06  loss: 2.9988 (2.7737)  acc1: 22.2222 (37.9880)  acc5: 61.1111 (71.4214)  time: 0.3230  data: 0.1393  max mem: 15572
Val:  [120/272]  eta: 0:01:01  loss: 2.9988 (2.7995)  acc1: 22.2222 (37.6492)  acc5: 61.1111 (70.7530)  time: 0.3540  data: 0.1619  max mem: 15572
Val:  [130/272]  eta: 0:00:56  loss: 2.7527 (2.7758)  acc1: 38.8889 (38.2528)  acc5: 77.7778 (71.5861)  time: 0.3488  data: 0.1554  max mem: 15572
Val:  [140/272]  eta: 0:00:51  loss: 2.4619 (2.7788)  acc1: 38.8889 (38.6919)  acc5: 77.7778 (71.1190)  time: 0.3111  data: 0.1207  max mem: 15572
Val:  [150/272]  eta: 0:00:45  loss: 2.7488 (2.7738)  acc1: 33.3333 (38.2266)  acc5: 72.2222 (71.1921)  time: 0.2275  data: 0.0500  max mem: 15572
Val:  [160/272]  eta: 0:00:40  loss: 2.6793 (2.7641)  acc1: 38.8889 (38.5438)  acc5: 77.7778 (71.6011)  time: 0.1698  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 2.8105 (2.7768)  acc1: 38.8889 (38.0767)  acc5: 72.2222 (71.1826)  time: 0.1711  data: 0.0108  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.7323 (2.7625)  acc1: 33.3333 (37.9988)  acc5: 72.2222 (71.7004)  time: 0.1705  data: 0.0108  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.7113 (2.7971)  acc1: 22.2222 (36.8819)  acc5: 66.6667 (70.4770)  time: 0.2074  data: 0.0421  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.9399 (2.7995)  acc1: 22.2222 (36.9265)  acc5: 55.5556 (70.3704)  time: 0.2613  data: 0.0830  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.6807 (2.8067)  acc1: 33.3333 (37.0458)  acc5: 77.7778 (70.3002)  time: 0.2812  data: 0.0954  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.9692 (2.7987)  acc1: 38.8889 (37.1292)  acc5: 72.2222 (70.5128)  time: 0.2919  data: 0.1121  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.3422 (2.7751)  acc1: 50.0000 (38.1914)  acc5: 83.3333 (71.1159)  time: 0.3061  data: 0.1250  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.3155 (2.7594)  acc1: 55.5556 (38.6814)  acc5: 83.3333 (71.5307)  time: 0.3272  data: 0.1420  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.5518 (2.7661)  acc1: 33.3333 (38.3355)  acc5: 77.7778 (71.3590)  time: 0.3325  data: 0.1477  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.0695 (2.7222)  acc1: 66.6667 (40.1022)  acc5: 83.3333 (72.1584)  time: 0.3178  data: 0.1441  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.0695 (2.7230)  acc1: 61.1111 (39.9549)  acc5: 83.3333 (72.1402)  time: 0.2252  data: 0.0712  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.0695 (2.7269)  acc1: 61.1111 (39.9549)  acc5: 83.3333 (72.1073)  time: 0.2180  data: 0.0711  max mem: 15572
Val: Total time: 0:01:27 (0.3223 s / it)
* Acc@1 39.955 Acc@5 72.107 loss 2.727
Accuracy of the network on the 4883 val videos: 40.0%
Max accuracy: 40.22%
Epoch: [19]  [   0/2809]  eta: 6:53:11  lr: 0.000031  min_lr: 0.000000  loss: 4.7556 (4.7556)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.8256  data: 8.3931  max mem: 15572
Epoch: [19]  [  10/2809]  eta: 0:55:45  lr: 0.000031  min_lr: 0.000000  loss: 4.1626 (4.2167)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1951  data: 0.7798  max mem: 15572
Epoch: [19]  [  20/2809]  eta: 0:41:25  lr: 0.000031  min_lr: 0.000000  loss: 4.1063 (4.1654)  class_acc: 0.2500 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4944  data: 0.0668  max mem: 15572
Epoch: [19]  [  30/2809]  eta: 0:36:20  lr: 0.000031  min_lr: 0.000000  loss: 4.1540 (4.1820)  class_acc: 0.2500 (0.2903)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1220  max mem: 15572
[2025-01-15 23:30:06,232] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:30:06,233] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [  40/2809]  eta: 0:33:03  lr: 0.000031  min_lr: 0.000000  loss: 4.1883 (4.1788)  class_acc: 0.2917 (0.3069)  loss_scale: 32768.0000 (38362.5366)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.1003  max mem: 15572
Epoch: [19]  [  50/2809]  eta: 0:31:18  lr: 0.000031  min_lr: 0.000000  loss: 4.2420 (4.2019)  class_acc: 0.2500 (0.2933)  loss_scale: 65536.0000 (43690.6667)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0835  max mem: 15572
[2025-01-15 23:30:18,237] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53427
[2025-01-15 23:30:18,238] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:30:18,238] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [  60/2809]  eta: 0:30:31  lr: 0.000031  min_lr: 0.000000  loss: 4.2395 (4.1906)  class_acc: 0.2500 (0.3012)  loss_scale: 65536.0000 (44585.9672)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1067  max mem: 15572
Epoch: [19]  [  70/2809]  eta: 0:29:18  lr: 0.000031  min_lr: 0.000000  loss: 4.1745 (4.1948)  class_acc: 0.2917 (0.3022)  loss_scale: 32768.0000 (42921.4648)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0809  max mem: 15572
Epoch: [19]  [  80/2809]  eta: 0:28:58  lr: 0.000031  min_lr: 0.000000  loss: 4.3107 (4.2154)  class_acc: 0.2917 (0.2948)  loss_scale: 32768.0000 (41667.9506)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0974  max mem: 15572
Epoch: [19]  [  90/2809]  eta: 0:28:38  lr: 0.000031  min_lr: 0.000000  loss: 4.4367 (4.2163)  class_acc: 0.2500 (0.2944)  loss_scale: 32768.0000 (40689.9341)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1486  max mem: 15572
Epoch: [19]  [ 100/2809]  eta: 0:28:11  lr: 0.000031  min_lr: 0.000000  loss: 4.2772 (4.2183)  class_acc: 0.2500 (0.2925)  loss_scale: 32768.0000 (39905.5842)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1321  max mem: 15572
Epoch: [19]  [ 110/2809]  eta: 0:28:04  lr: 0.000031  min_lr: 0.000000  loss: 4.2628 (4.2201)  class_acc: 0.2500 (0.2913)  loss_scale: 32768.0000 (39262.5586)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1497  max mem: 15572
Epoch: [19]  [ 120/2809]  eta: 0:27:47  lr: 0.000031  min_lr: 0.000000  loss: 4.3523 (4.2363)  class_acc: 0.2500 (0.2903)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1520  max mem: 15572
Epoch: [19]  [ 130/2809]  eta: 0:27:29  lr: 0.000031  min_lr: 0.000000  loss: 4.4348 (4.2465)  class_acc: 0.2917 (0.2901)  loss_scale: 32768.0000 (38271.0229)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1191  max mem: 15572
Epoch: [19]  [ 140/2809]  eta: 0:27:13  lr: 0.000031  min_lr: 0.000000  loss: 4.3704 (4.2574)  class_acc: 0.2917 (0.2887)  loss_scale: 32768.0000 (37880.7376)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1104  max mem: 15572
Epoch: [19]  [ 150/2809]  eta: 0:26:55  lr: 0.000031  min_lr: 0.000000  loss: 4.3293 (4.2578)  class_acc: 0.2500 (0.2900)  loss_scale: 32768.0000 (37542.1457)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0846  max mem: 15572
Epoch: [19]  [ 160/2809]  eta: 0:26:40  lr: 0.000031  min_lr: 0.000000  loss: 4.3059 (4.2633)  class_acc: 0.2500 (0.2873)  loss_scale: 32768.0000 (37245.6149)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0875  max mem: 15572
Epoch: [19]  [ 170/2809]  eta: 0:26:24  lr: 0.000031  min_lr: 0.000000  loss: 4.3059 (4.2694)  class_acc: 0.2500 (0.2865)  loss_scale: 32768.0000 (36983.7661)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.1143  max mem: 15572
Epoch: [19]  [ 180/2809]  eta: 0:26:33  lr: 0.000031  min_lr: 0.000000  loss: 4.3505 (4.2733)  class_acc: 0.2500 (0.2857)  loss_scale: 32768.0000 (36750.8508)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1899  max mem: 15572
[2025-01-15 23:31:32,897] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:31:32,898] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [ 190/2809]  eta: 0:26:25  lr: 0.000031  min_lr: 0.000000  loss: 4.3519 (4.2829)  class_acc: 0.2917 (0.2849)  loss_scale: 32768.0000 (37571.6859)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.2012  max mem: 15572
[2025-01-15 23:31:38,019] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53565
[2025-01-15 23:31:38,020] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:31:38,020] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 200/2809]  eta: 0:26:15  lr: 0.000031  min_lr: 0.000000  loss: 4.3940 (4.2863)  class_acc: 0.2500 (0.2844)  loss_scale: 32768.0000 (37821.7711)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1229  max mem: 15572
Epoch: [19]  [ 210/2809]  eta: 0:26:04  lr: 0.000031  min_lr: 0.000000  loss: 4.4063 (4.2906)  class_acc: 0.2500 (0.2838)  loss_scale: 32768.0000 (37582.2559)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1046  max mem: 15572
Epoch: [19]  [ 220/2809]  eta: 0:25:54  lr: 0.000031  min_lr: 0.000000  loss: 4.4116 (4.2959)  class_acc: 0.2500 (0.2822)  loss_scale: 32768.0000 (37364.4163)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1187  max mem: 15572
Epoch: [19]  [ 230/2809]  eta: 0:25:50  lr: 0.000031  min_lr: 0.000000  loss: 4.3653 (4.2954)  class_acc: 0.2917 (0.2832)  loss_scale: 32768.0000 (37165.4372)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1428  max mem: 15572
Epoch: [19]  [ 240/2809]  eta: 0:25:30  lr: 0.000031  min_lr: 0.000000  loss: 4.3102 (4.2935)  class_acc: 0.2917 (0.2835)  loss_scale: 32768.0000 (36982.9710)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0934  max mem: 15572
Epoch: [19]  [ 250/2809]  eta: 0:25:17  lr: 0.000031  min_lr: 0.000000  loss: 4.2945 (4.2874)  class_acc: 0.2917 (0.2842)  loss_scale: 32768.0000 (36815.0438)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0557  max mem: 15572
Epoch: [19]  [ 260/2809]  eta: 0:25:10  lr: 0.000031  min_lr: 0.000000  loss: 4.2945 (4.2895)  class_acc: 0.2500 (0.2821)  loss_scale: 32768.0000 (36659.9847)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1066  max mem: 15572
Epoch: [19]  [ 270/2809]  eta: 0:25:03  lr: 0.000031  min_lr: 0.000000  loss: 4.2998 (4.2872)  class_acc: 0.2500 (0.2835)  loss_scale: 32768.0000 (36516.3690)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1355  max mem: 15572
Epoch: [19]  [ 280/2809]  eta: 0:25:02  lr: 0.000030  min_lr: 0.000000  loss: 4.2998 (4.2916)  class_acc: 0.2917 (0.2825)  loss_scale: 32768.0000 (36382.9751)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1764  max mem: 15572
Epoch: [19]  [ 290/2809]  eta: 0:24:43  lr: 0.000030  min_lr: 0.000000  loss: 4.2143 (4.2884)  class_acc: 0.2500 (0.2839)  loss_scale: 32768.0000 (36258.7491)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1097  max mem: 15572
Epoch: [19]  [ 300/2809]  eta: 0:24:44  lr: 0.000030  min_lr: 0.000000  loss: 4.2420 (4.2926)  class_acc: 0.3333 (0.2853)  loss_scale: 32768.0000 (36142.7774)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1058  max mem: 15572
Epoch: [19]  [ 310/2809]  eta: 0:24:35  lr: 0.000030  min_lr: 0.000000  loss: 4.2978 (4.2941)  class_acc: 0.3333 (0.2872)  loss_scale: 32768.0000 (36034.2637)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1695  max mem: 15572
Epoch: [19]  [ 320/2809]  eta: 0:24:23  lr: 0.000030  min_lr: 0.000000  loss: 4.4038 (4.2981)  class_acc: 0.2917 (0.2867)  loss_scale: 32768.0000 (35932.5109)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0798  max mem: 15572
[2025-01-15 23:32:50,629] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:32:50,629] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [ 330/2809]  eta: 0:24:14  lr: 0.000030  min_lr: 0.000000  loss: 4.4998 (4.3053)  class_acc: 0.2500 (0.2863)  loss_scale: 32768.0000 (36628.8822)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0566  max mem: 15572
Epoch: [19]  [ 340/2809]  eta: 0:24:00  lr: 0.000030  min_lr: 0.000000  loss: 4.3222 (4.3038)  class_acc: 0.2083 (0.2856)  loss_scale: 65536.0000 (37476.5982)  weight_decay: 0.0500 (0.0500)  time: 0.5064  data: 0.0673  max mem: 15572
Epoch: [19]  [ 350/2809]  eta: 0:23:54  lr: 0.000030  min_lr: 0.000000  loss: 4.3091 (4.3062)  class_acc: 0.2917 (0.2862)  loss_scale: 65536.0000 (38276.0114)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.1081  max mem: 15572
[2025-01-15 23:33:09,771] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53730
[2025-01-15 23:33:09,772] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:33:09,772] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 360/2809]  eta: 0:23:48  lr: 0.000030  min_lr: 0.000000  loss: 4.3435 (4.3038)  class_acc: 0.2917 (0.2860)  loss_scale: 65536.0000 (38849.5956)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1632  max mem: 15572
Epoch: [19]  [ 370/2809]  eta: 0:23:34  lr: 0.000030  min_lr: 0.000000  loss: 4.2456 (4.3025)  class_acc: 0.2083 (0.2838)  loss_scale: 32768.0000 (38685.6712)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0906  max mem: 15572
Epoch: [19]  [ 380/2809]  eta: 0:23:33  lr: 0.000030  min_lr: 0.000000  loss: 4.2456 (4.3034)  class_acc: 0.2083 (0.2823)  loss_scale: 32768.0000 (38530.3517)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0964  max mem: 15572
Epoch: [19]  [ 390/2809]  eta: 0:23:18  lr: 0.000030  min_lr: 0.000000  loss: 4.2912 (4.3043)  class_acc: 0.2083 (0.2809)  loss_scale: 32768.0000 (38382.9770)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0877  max mem: 15572
Epoch: [19]  [ 400/2809]  eta: 0:23:10  lr: 0.000030  min_lr: 0.000000  loss: 4.2912 (4.3015)  class_acc: 0.2083 (0.2815)  loss_scale: 32768.0000 (38242.9526)  weight_decay: 0.0500 (0.0500)  time: 0.4886  data: 0.0513  max mem: 15572
Epoch: [19]  [ 410/2809]  eta: 0:23:03  lr: 0.000030  min_lr: 0.000000  loss: 4.2220 (4.3003)  class_acc: 0.2917 (0.2819)  loss_scale: 32768.0000 (38109.7421)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1120  max mem: 15572
Epoch: [19]  [ 420/2809]  eta: 0:22:59  lr: 0.000030  min_lr: 0.000000  loss: 4.2468 (4.2985)  class_acc: 0.3333 (0.2832)  loss_scale: 32768.0000 (37982.8599)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1431  max mem: 15572
Epoch: [19]  [ 430/2809]  eta: 0:22:53  lr: 0.000030  min_lr: 0.000000  loss: 4.3087 (4.3001)  class_acc: 0.2917 (0.2834)  loss_scale: 32768.0000 (37861.8654)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1574  max mem: 15572
Epoch: [19]  [ 440/2809]  eta: 0:22:56  lr: 0.000030  min_lr: 0.000000  loss: 4.2920 (4.2988)  class_acc: 0.2500 (0.2837)  loss_scale: 32768.0000 (37746.3583)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.2193  max mem: 15572
Epoch: [19]  [ 450/2809]  eta: 0:22:47  lr: 0.000030  min_lr: 0.000000  loss: 4.2920 (4.3008)  class_acc: 0.2083 (0.2821)  loss_scale: 32768.0000 (37635.9734)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.1786  max mem: 15572
Epoch: [19]  [ 460/2809]  eta: 0:22:39  lr: 0.000030  min_lr: 0.000000  loss: 4.4498 (4.3039)  class_acc: 0.2083 (0.2796)  loss_scale: 32768.0000 (37530.3774)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0782  max mem: 15572
Epoch: [19]  [ 470/2809]  eta: 0:22:32  lr: 0.000030  min_lr: 0.000000  loss: 4.3363 (4.3013)  class_acc: 0.2917 (0.2810)  loss_scale: 32768.0000 (37429.2654)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0951  max mem: 15572
Epoch: [19]  [ 480/2809]  eta: 0:22:22  lr: 0.000030  min_lr: 0.000000  loss: 4.2150 (4.3031)  class_acc: 0.2917 (0.2812)  loss_scale: 32768.0000 (37332.3576)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0570  max mem: 15572
[2025-01-15 23:34:21,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:34:21,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [ 490/2809]  eta: 0:22:15  lr: 0.000030  min_lr: 0.000000  loss: 4.3134 (4.3025)  class_acc: 0.2917 (0.2815)  loss_scale: 32768.0000 (37439.6090)  weight_decay: 0.0500 (0.0500)  time: 0.5227  data: 0.0643  max mem: 15572
[2025-01-15 23:34:23,319] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53863
[2025-01-15 23:34:23,319] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:34:23,320] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 500/2809]  eta: 0:22:10  lr: 0.000030  min_lr: 0.000000  loss: 4.3462 (4.3048)  class_acc: 0.2917 (0.2816)  loss_scale: 32768.0000 (37411.7685)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1307  max mem: 15572
Epoch: [19]  [ 510/2809]  eta: 0:22:00  lr: 0.000030  min_lr: 0.000000  loss: 4.3866 (4.3065)  class_acc: 0.2917 (0.2820)  loss_scale: 32768.0000 (37320.8924)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0987  max mem: 15572
Epoch: [19]  [ 520/2809]  eta: 0:21:56  lr: 0.000030  min_lr: 0.000000  loss: 4.3969 (4.3078)  class_acc: 0.2917 (0.2826)  loss_scale: 32768.0000 (37233.5048)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1071  max mem: 15572
Epoch: [19]  [ 530/2809]  eta: 0:21:49  lr: 0.000030  min_lr: 0.000000  loss: 4.3686 (4.3088)  class_acc: 0.2083 (0.2816)  loss_scale: 32768.0000 (37149.4087)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1293  max mem: 15572
Epoch: [19]  [ 540/2809]  eta: 0:21:45  lr: 0.000030  min_lr: 0.000000  loss: 4.3816 (4.3118)  class_acc: 0.2083 (0.2816)  loss_scale: 32768.0000 (37068.4214)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1182  max mem: 15572
Epoch: [19]  [ 550/2809]  eta: 0:21:40  lr: 0.000030  min_lr: 0.000000  loss: 4.2407 (4.3069)  class_acc: 0.2500 (0.2818)  loss_scale: 32768.0000 (36990.3739)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1497  max mem: 15572
Epoch: [19]  [ 560/2809]  eta: 0:21:33  lr: 0.000030  min_lr: 0.000000  loss: 4.1882 (4.3064)  class_acc: 0.2500 (0.2805)  loss_scale: 32768.0000 (36915.1087)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1408  max mem: 15572
Epoch: [19]  [ 570/2809]  eta: 0:21:25  lr: 0.000030  min_lr: 0.000000  loss: 4.2882 (4.3082)  class_acc: 0.2083 (0.2799)  loss_scale: 32768.0000 (36842.4799)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0965  max mem: 15572
Epoch: [19]  [ 580/2809]  eta: 0:21:23  lr: 0.000030  min_lr: 0.000000  loss: 4.4134 (4.3093)  class_acc: 0.2500 (0.2795)  loss_scale: 32768.0000 (36772.3511)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1617  max mem: 15572
Epoch: [19]  [ 590/2809]  eta: 0:21:16  lr: 0.000030  min_lr: 0.000000  loss: 4.3319 (4.3092)  class_acc: 0.2917 (0.2798)  loss_scale: 32768.0000 (36704.5956)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1838  max mem: 15572
Epoch: [19]  [ 600/2809]  eta: 0:21:12  lr: 0.000030  min_lr: 0.000000  loss: 4.2811 (4.3080)  class_acc: 0.2500 (0.2793)  loss_scale: 32768.0000 (36639.0948)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1525  max mem: 15572
Epoch: [19]  [ 610/2809]  eta: 0:21:06  lr: 0.000030  min_lr: 0.000000  loss: 4.2369 (4.3072)  class_acc: 0.2500 (0.2796)  loss_scale: 32768.0000 (36575.7381)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1500  max mem: 15572
Epoch: [19]  [ 620/2809]  eta: 0:20:58  lr: 0.000030  min_lr: 0.000000  loss: 4.3073 (4.3068)  class_acc: 0.2917 (0.2793)  loss_scale: 32768.0000 (36514.4219)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0777  max mem: 15572
[2025-01-15 23:35:37,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:35:37,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 23:35:40,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=54000, skipped=337, lr=[2.930698064107794e-07, 2.930698064107794e-07, 4.186711520153992e-07, 4.186711520153992e-07, 5.981016457362846e-07, 5.981016457362846e-07, 8.544309224804067e-07, 8.544309224804067e-07, 1.220615603543438e-06, 1.220615603543438e-06, 1.7437365764906259e-06, 1.7437365764906259e-06, 2.4910522521294658e-06, 2.4910522521294658e-06, 3.558646074470666e-06, 3.558646074470666e-06, 5.0837801063866654e-06, 5.0837801063866654e-06, 7.262543009123808e-06, 7.262543009123808e-06, 1.037506144160544e-05, 1.037506144160544e-05, 1.4821516345150631e-05, 1.4821516345150631e-05, 2.1173594778786615e-05, 2.1173594778786615e-05, 3.024799254112374e-05, 3.024799254112374e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:35:40,801] [INFO] [timer.py:260:stop] epoch=0/micro_step=54000/global_step=54000, RunningAvgSamplesPerSec=27.757212362457533, CurrSamplesPerSec=27.586860687352388, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [ 630/2809]  eta: 0:20:49  lr: 0.000030  min_lr: 0.000000  loss: 4.3073 (4.3066)  class_acc: 0.2500 (0.2790)  loss_scale: 32768.0000 (36974.3518)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0326  max mem: 15572
[2025-01-15 23:35:44,077] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54004
[2025-01-15 23:35:44,077] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:35:44,077] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 640/2809]  eta: 0:20:43  lr: 0.000030  min_lr: 0.000000  loss: 4.2391 (4.3049)  class_acc: 0.2500 (0.2797)  loss_scale: 65536.0000 (37010.9704)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0663  max mem: 15572
Epoch: [19]  [ 650/2809]  eta: 0:20:37  lr: 0.000030  min_lr: 0.000000  loss: 4.2326 (4.3052)  class_acc: 0.2500 (0.2800)  loss_scale: 32768.0000 (36945.7942)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1205  max mem: 15572
Epoch: [19]  [ 660/2809]  eta: 0:20:31  lr: 0.000030  min_lr: 0.000000  loss: 4.2343 (4.3044)  class_acc: 0.2083 (0.2794)  loss_scale: 32768.0000 (36882.5900)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1195  max mem: 15572
Epoch: [19]  [ 670/2809]  eta: 0:20:26  lr: 0.000030  min_lr: 0.000000  loss: 4.3804 (4.3066)  class_acc: 0.2083 (0.2791)  loss_scale: 32768.0000 (36821.2697)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1273  max mem: 15572
Epoch: [19]  [ 680/2809]  eta: 0:20:20  lr: 0.000030  min_lr: 0.000000  loss: 4.3804 (4.3064)  class_acc: 0.2500 (0.2791)  loss_scale: 32768.0000 (36761.7504)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1215  max mem: 15572
Epoch: [19]  [ 690/2809]  eta: 0:20:14  lr: 0.000030  min_lr: 0.000000  loss: 4.4832 (4.3079)  class_acc: 0.2500 (0.2786)  loss_scale: 32768.0000 (36703.9537)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1058  max mem: 15572
Epoch: [19]  [ 700/2809]  eta: 0:20:10  lr: 0.000030  min_lr: 0.000000  loss: 4.4020 (4.3074)  class_acc: 0.2500 (0.2784)  loss_scale: 32768.0000 (36647.8060)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1411  max mem: 15572
Epoch: [19]  [ 710/2809]  eta: 0:20:03  lr: 0.000030  min_lr: 0.000000  loss: 4.3932 (4.3094)  class_acc: 0.2500 (0.2782)  loss_scale: 32768.0000 (36593.2377)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1340  max mem: 15572
Epoch: [19]  [ 720/2809]  eta: 0:19:58  lr: 0.000030  min_lr: 0.000000  loss: 4.4252 (4.3109)  class_acc: 0.2083 (0.2766)  loss_scale: 32768.0000 (36540.1831)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1225  max mem: 15572
Epoch: [19]  [ 730/2809]  eta: 0:19:52  lr: 0.000030  min_lr: 0.000000  loss: 4.2542 (4.3074)  class_acc: 0.1667 (0.2768)  loss_scale: 32768.0000 (36488.5800)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1242  max mem: 15572
Epoch: [19]  [ 740/2809]  eta: 0:19:46  lr: 0.000030  min_lr: 0.000000  loss: 4.0396 (4.3073)  class_acc: 0.2500 (0.2771)  loss_scale: 32768.0000 (36438.3698)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0995  max mem: 15572
Epoch: [19]  [ 750/2809]  eta: 0:19:41  lr: 0.000030  min_lr: 0.000000  loss: 4.2682 (4.3069)  class_acc: 0.2917 (0.2773)  loss_scale: 32768.0000 (36389.4967)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1077  max mem: 15572
Epoch: [19]  [ 760/2809]  eta: 0:19:34  lr: 0.000030  min_lr: 0.000000  loss: 4.2019 (4.3051)  class_acc: 0.2500 (0.2777)  loss_scale: 32768.0000 (36341.9080)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1140  max mem: 15572
[2025-01-15 23:36:57,143] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:36:57,143] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [ 770/2809]  eta: 0:19:29  lr: 0.000030  min_lr: 0.000000  loss: 4.2261 (4.3048)  class_acc: 0.2500 (0.2774)  loss_scale: 32768.0000 (36678.0597)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1149  max mem: 15572
[2025-01-15 23:37:03,465] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54144
[2025-01-15 23:37:03,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:37:03,467] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 780/2809]  eta: 0:19:24  lr: 0.000030  min_lr: 0.000000  loss: 4.2261 (4.3025)  class_acc: 0.2917 (0.2777)  loss_scale: 65536.0000 (36711.9078)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1306  max mem: 15572
Epoch: [19]  [ 790/2809]  eta: 0:19:19  lr: 0.000030  min_lr: 0.000000  loss: 4.2413 (4.3034)  class_acc: 0.2917 (0.2780)  loss_scale: 32768.0000 (36662.0480)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1442  max mem: 15572
Epoch: [19]  [ 800/2809]  eta: 0:19:12  lr: 0.000030  min_lr: 0.000000  loss: 4.3167 (4.3033)  class_acc: 0.2917 (0.2786)  loss_scale: 32768.0000 (36613.4332)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1133  max mem: 15572
Epoch: [19]  [ 810/2809]  eta: 0:19:06  lr: 0.000030  min_lr: 0.000000  loss: 4.2297 (4.3021)  class_acc: 0.2917 (0.2783)  loss_scale: 32768.0000 (36566.0173)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1060  max mem: 15572
Epoch: [19]  [ 820/2809]  eta: 0:19:00  lr: 0.000030  min_lr: 0.000000  loss: 4.3514 (4.3032)  class_acc: 0.2083 (0.2776)  loss_scale: 32768.0000 (36519.7564)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1108  max mem: 15572
Epoch: [19]  [ 830/2809]  eta: 0:18:54  lr: 0.000030  min_lr: 0.000000  loss: 4.4256 (4.3043)  class_acc: 0.2083 (0.2778)  loss_scale: 32768.0000 (36474.6089)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1098  max mem: 15572
Epoch: [19]  [ 840/2809]  eta: 0:18:50  lr: 0.000030  min_lr: 0.000000  loss: 4.3378 (4.3037)  class_acc: 0.2500 (0.2778)  loss_scale: 32768.0000 (36430.5351)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1682  max mem: 15572
Epoch: [19]  [ 850/2809]  eta: 0:18:43  lr: 0.000030  min_lr: 0.000000  loss: 4.3175 (4.3031)  class_acc: 0.2083 (0.2773)  loss_scale: 32768.0000 (36387.4971)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1540  max mem: 15572
Epoch: [19]  [ 860/2809]  eta: 0:18:38  lr: 0.000030  min_lr: 0.000000  loss: 4.1774 (4.3015)  class_acc: 0.2500 (0.2771)  loss_scale: 32768.0000 (36345.4588)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1231  max mem: 15572
Epoch: [19]  [ 870/2809]  eta: 0:18:32  lr: 0.000030  min_lr: 0.000000  loss: 4.1774 (4.3006)  class_acc: 0.2917 (0.2774)  loss_scale: 32768.0000 (36304.3858)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1213  max mem: 15572
Epoch: [19]  [ 880/2809]  eta: 0:18:25  lr: 0.000030  min_lr: 0.000000  loss: 4.3694 (4.3015)  class_acc: 0.2500 (0.2767)  loss_scale: 32768.0000 (36264.2452)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0999  max mem: 15572
Epoch: [19]  [ 890/2809]  eta: 0:18:21  lr: 0.000030  min_lr: 0.000000  loss: 4.3694 (4.3022)  class_acc: 0.2500 (0.2770)  loss_scale: 32768.0000 (36225.0056)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1322  max mem: 15572
Epoch: [19]  [ 900/2809]  eta: 0:18:15  lr: 0.000030  min_lr: 0.000000  loss: 4.3794 (4.3041)  class_acc: 0.2500 (0.2764)  loss_scale: 32768.0000 (36186.6371)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1349  max mem: 15572
[2025-01-15 23:38:17,592] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:38:17,593] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [ 910/2809]  eta: 0:18:08  lr: 0.000030  min_lr: 0.000000  loss: 4.5051 (4.3063)  class_acc: 0.2083 (0.2761)  loss_scale: 32768.0000 (36472.8342)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.1008  max mem: 15572
Epoch: [19]  [ 920/2809]  eta: 0:18:01  lr: 0.000030  min_lr: 0.000000  loss: 4.3256 (4.3062)  class_acc: 0.2917 (0.2761)  loss_scale: 65536.0000 (36788.3952)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0609  max mem: 15572
Epoch: [19]  [ 930/2809]  eta: 0:17:54  lr: 0.000030  min_lr: 0.000000  loss: 4.2939 (4.3066)  class_acc: 0.2917 (0.2763)  loss_scale: 65536.0000 (37097.1772)  weight_decay: 0.0500 (0.0500)  time: 0.4943  data: 0.0225  max mem: 15572
Epoch: [19]  [ 940/2809]  eta: 0:17:47  lr: 0.000030  min_lr: 0.000000  loss: 4.2939 (4.3066)  class_acc: 0.2917 (0.2765)  loss_scale: 65536.0000 (37399.3964)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0570  max mem: 15572
Epoch: [19]  [ 950/2809]  eta: 0:17:44  lr: 0.000030  min_lr: 0.000000  loss: 4.2670 (4.3059)  class_acc: 0.2917 (0.2764)  loss_scale: 65536.0000 (37695.2597)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1640  max mem: 15572
Epoch: [19]  [ 960/2809]  eta: 0:17:40  lr: 0.000030  min_lr: 0.000000  loss: 4.2231 (4.3049)  class_acc: 0.2500 (0.2766)  loss_scale: 65536.0000 (37984.9657)  weight_decay: 0.0500 (0.0500)  time: 0.6620  data: 0.2226  max mem: 15572
Epoch: [19]  [ 970/2809]  eta: 0:17:33  lr: 0.000030  min_lr: 0.000000  loss: 4.2060 (4.3058)  class_acc: 0.2500 (0.2765)  loss_scale: 65536.0000 (38268.7044)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1498  max mem: 15572
Epoch: [19]  [ 980/2809]  eta: 0:17:26  lr: 0.000030  min_lr: 0.000000  loss: 4.3529 (4.3063)  class_acc: 0.2500 (0.2766)  loss_scale: 65536.0000 (38546.6585)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0474  max mem: 15572
[2025-01-15 23:39:01,771] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54353
[2025-01-15 23:39:01,771] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:39:01,772] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 990/2809]  eta: 0:17:19  lr: 0.000030  min_lr: 0.000000  loss: 4.2205 (4.3058)  class_acc: 0.2083 (0.2762)  loss_scale: 65536.0000 (38521.4127)  weight_decay: 0.0500 (0.0500)  time: 0.4956  data: 0.0484  max mem: 15572
Epoch: [19]  [1000/2809]  eta: 0:17:13  lr: 0.000030  min_lr: 0.000000  loss: 4.1648 (4.3039)  class_acc: 0.2083 (0.2769)  loss_scale: 32768.0000 (38463.9361)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0942  max mem: 15572
Epoch: [19]  [1010/2809]  eta: 0:17:07  lr: 0.000030  min_lr: 0.000000  loss: 4.1268 (4.3044)  class_acc: 0.2917 (0.2766)  loss_scale: 32768.0000 (38407.5964)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0916  max mem: 15572
Epoch: [19]  [1020/2809]  eta: 0:17:01  lr: 0.000030  min_lr: 0.000000  loss: 4.3915 (4.3051)  class_acc: 0.2500 (0.2764)  loss_scale: 32768.0000 (38352.3604)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0840  max mem: 15572
Epoch: [19]  [1030/2809]  eta: 0:16:57  lr: 0.000030  min_lr: 0.000000  loss: 4.2961 (4.3050)  class_acc: 0.2500 (0.2762)  loss_scale: 32768.0000 (38298.1959)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1617  max mem: 15572
Epoch: [19]  [1040/2809]  eta: 0:16:51  lr: 0.000030  min_lr: 0.000000  loss: 4.2561 (4.3042)  class_acc: 0.2500 (0.2763)  loss_scale: 32768.0000 (38245.0720)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1893  max mem: 15572
Epoch: [19]  [1050/2809]  eta: 0:16:44  lr: 0.000030  min_lr: 0.000000  loss: 4.2623 (4.3037)  class_acc: 0.2917 (0.2767)  loss_scale: 32768.0000 (38192.9591)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0943  max mem: 15572
Epoch: [19]  [1060/2809]  eta: 0:16:38  lr: 0.000030  min_lr: 0.000000  loss: 4.2892 (4.3036)  class_acc: 0.2500 (0.2763)  loss_scale: 32768.0000 (38141.8285)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0794  max mem: 15572
Epoch: [19]  [1070/2809]  eta: 0:16:33  lr: 0.000030  min_lr: 0.000000  loss: 4.3311 (4.3031)  class_acc: 0.2500 (0.2766)  loss_scale: 32768.0000 (38091.6527)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1351  max mem: 15572
Epoch: [19]  [1080/2809]  eta: 0:16:28  lr: 0.000030  min_lr: 0.000000  loss: 4.1022 (4.3011)  class_acc: 0.3333 (0.2770)  loss_scale: 32768.0000 (38042.4052)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1775  max mem: 15572
Epoch: [19]  [1090/2809]  eta: 0:16:22  lr: 0.000030  min_lr: 0.000000  loss: 4.2474 (4.3016)  class_acc: 0.2917 (0.2767)  loss_scale: 32768.0000 (37994.0605)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1344  max mem: 15572
Epoch: [19]  [1100/2809]  eta: 0:16:17  lr: 0.000030  min_lr: 0.000000  loss: 4.3390 (4.3023)  class_acc: 0.2917 (0.2770)  loss_scale: 32768.0000 (37946.5940)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1014  max mem: 15572
Epoch: [19]  [1110/2809]  eta: 0:16:09  lr: 0.000030  min_lr: 0.000000  loss: 4.3034 (4.3027)  class_acc: 0.2500 (0.2769)  loss_scale: 32768.0000 (37899.9820)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0637  max mem: 15572
[2025-01-15 23:40:14,159] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:40:14,159] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [1120/2809]  eta: 0:16:04  lr: 0.000030  min_lr: 0.000000  loss: 4.2736 (4.3013)  class_acc: 0.2917 (0.2771)  loss_scale: 32768.0000 (38146.5120)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.1015  max mem: 15572
[2025-01-15 23:40:22,497] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54496
[2025-01-15 23:40:22,497] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:40:22,497] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [1130/2809]  eta: 0:15:59  lr: 0.000030  min_lr: 0.000000  loss: 4.2502 (4.3019)  class_acc: 0.2917 (0.2767)  loss_scale: 65536.0000 (38214.8470)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1795  max mem: 15572
Epoch: [19]  [1140/2809]  eta: 0:15:53  lr: 0.000030  min_lr: 0.000000  loss: 4.2432 (4.3017)  class_acc: 0.2083 (0.2769)  loss_scale: 32768.0000 (38167.1096)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1349  max mem: 15572
Epoch: [19]  [1150/2809]  eta: 0:15:47  lr: 0.000030  min_lr: 0.000000  loss: 4.2760 (4.3012)  class_acc: 0.2917 (0.2771)  loss_scale: 32768.0000 (38120.2016)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1115  max mem: 15572
Epoch: [19]  [1160/2809]  eta: 0:15:41  lr: 0.000030  min_lr: 0.000000  loss: 4.2885 (4.3005)  class_acc: 0.2917 (0.2770)  loss_scale: 32768.0000 (38074.1016)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0987  max mem: 15572
Epoch: [19]  [1170/2809]  eta: 0:15:37  lr: 0.000030  min_lr: 0.000000  loss: 4.3287 (4.3002)  class_acc: 0.2917 (0.2775)  loss_scale: 32768.0000 (38028.7891)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1587  max mem: 15572
Epoch: [19]  [1180/2809]  eta: 0:15:31  lr: 0.000030  min_lr: 0.000000  loss: 4.3482 (4.3009)  class_acc: 0.2500 (0.2768)  loss_scale: 32768.0000 (37984.2439)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1715  max mem: 15572
Epoch: [19]  [1190/2809]  eta: 0:15:25  lr: 0.000030  min_lr: 0.000000  loss: 4.3557 (4.3015)  class_acc: 0.2083 (0.2768)  loss_scale: 32768.0000 (37940.4467)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1312  max mem: 15572
Epoch: [19]  [1200/2809]  eta: 0:15:20  lr: 0.000030  min_lr: 0.000000  loss: 4.2788 (4.3014)  class_acc: 0.2500 (0.2768)  loss_scale: 32768.0000 (37897.3789)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1375  max mem: 15572
Epoch: [19]  [1210/2809]  eta: 0:15:13  lr: 0.000030  min_lr: 0.000000  loss: 4.1689 (4.2998)  class_acc: 0.2500 (0.2766)  loss_scale: 32768.0000 (37855.0223)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.1013  max mem: 15572
Epoch: [19]  [1220/2809]  eta: 0:15:09  lr: 0.000030  min_lr: 0.000000  loss: 4.1992 (4.3010)  class_acc: 0.2500 (0.2766)  loss_scale: 32768.0000 (37813.3595)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1552  max mem: 15572
Epoch: [19]  [1230/2809]  eta: 0:15:02  lr: 0.000030  min_lr: 0.000000  loss: 4.3452 (4.3004)  class_acc: 0.2917 (0.2765)  loss_scale: 32768.0000 (37772.3737)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1419  max mem: 15572
Epoch: [19]  [1240/2809]  eta: 0:14:56  lr: 0.000030  min_lr: 0.000000  loss: 4.2174 (4.2994)  class_acc: 0.2500 (0.2767)  loss_scale: 32768.0000 (37732.0483)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0594  max mem: 15572
Epoch: [19]  [1250/2809]  eta: 0:14:51  lr: 0.000030  min_lr: 0.000000  loss: 4.2568 (4.2995)  class_acc: 0.2500 (0.2766)  loss_scale: 32768.0000 (37692.3677)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0979  max mem: 15572
[2025-01-15 23:41:36,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:41:36,840] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [1260/2809]  eta: 0:14:45  lr: 0.000030  min_lr: 0.000000  loss: 4.3177 (4.2997)  class_acc: 0.2083 (0.2765)  loss_scale: 32768.0000 (37835.2165)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1211  max mem: 15572
Epoch: [19]  [1270/2809]  eta: 0:14:40  lr: 0.000030  min_lr: 0.000000  loss: 4.3780 (4.3005)  class_acc: 0.2500 (0.2766)  loss_scale: 65536.0000 (38053.1613)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1524  max mem: 15572
Epoch: [19]  [1280/2809]  eta: 0:14:34  lr: 0.000030  min_lr: 0.000000  loss: 4.2244 (4.2994)  class_acc: 0.2917 (0.2768)  loss_scale: 65536.0000 (38267.7034)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1215  max mem: 15572
Epoch: [19]  [1290/2809]  eta: 0:14:28  lr: 0.000030  min_lr: 0.000000  loss: 4.1221 (4.2984)  class_acc: 0.3333 (0.2775)  loss_scale: 65536.0000 (38478.9218)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0803  max mem: 15572
Epoch: [19]  [1300/2809]  eta: 0:14:22  lr: 0.000030  min_lr: 0.000000  loss: 4.1831 (4.2979)  class_acc: 0.3750 (0.2778)  loss_scale: 65536.0000 (38686.8932)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1195  max mem: 15572
Epoch: [19]  [1310/2809]  eta: 0:14:16  lr: 0.000030  min_lr: 0.000000  loss: 4.2859 (4.2993)  class_acc: 0.2917 (0.2778)  loss_scale: 65536.0000 (38891.6918)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0975  max mem: 15572
Epoch: [19]  [1320/2809]  eta: 0:14:09  lr: 0.000030  min_lr: 0.000000  loss: 4.3025 (4.2983)  class_acc: 0.2500 (0.2778)  loss_scale: 65536.0000 (39093.3899)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0789  max mem: 15572
Epoch: [19]  [1330/2809]  eta: 0:14:04  lr: 0.000030  min_lr: 0.000000  loss: 4.1548 (4.2971)  class_acc: 0.2500 (0.2780)  loss_scale: 65536.0000 (39292.0571)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1109  max mem: 15572
Epoch: [19]  [1340/2809]  eta: 0:13:59  lr: 0.000030  min_lr: 0.000000  loss: 4.0895 (4.2962)  class_acc: 0.2083 (0.2773)  loss_scale: 65536.0000 (39487.7614)  weight_decay: 0.0500 (0.0500)  time: 0.6376  data: 0.1672  max mem: 15572
[2025-01-15 23:42:29,074] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54717
[2025-01-15 23:42:29,075] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:42:29,077] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [1350/2809]  eta: 0:13:52  lr: 0.000030  min_lr: 0.000000  loss: 4.0895 (4.2954)  class_acc: 0.2083 (0.2775)  loss_scale: 65536.0000 (39559.2953)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1038  max mem: 15572
Epoch: [19]  [1360/2809]  eta: 0:13:47  lr: 0.000030  min_lr: 0.000000  loss: 4.1727 (4.2944)  class_acc: 0.2917 (0.2777)  loss_scale: 32768.0000 (39509.3960)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0722  max mem: 15572
Epoch: [19]  [1370/2809]  eta: 0:13:40  lr: 0.000030  min_lr: 0.000000  loss: 4.2966 (4.2941)  class_acc: 0.2917 (0.2777)  loss_scale: 32768.0000 (39460.2247)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1023  max mem: 15572
Epoch: [19]  [1380/2809]  eta: 0:13:35  lr: 0.000030  min_lr: 0.000000  loss: 4.3474 (4.2942)  class_acc: 0.2500 (0.2777)  loss_scale: 32768.0000 (39411.7654)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0898  max mem: 15572
Epoch: [19]  [1390/2809]  eta: 0:13:29  lr: 0.000030  min_lr: 0.000000  loss: 4.2164 (4.2932)  class_acc: 0.2917 (0.2781)  loss_scale: 32768.0000 (39364.0029)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1212  max mem: 15572
Epoch: [19]  [1400/2809]  eta: 0:13:24  lr: 0.000030  min_lr: 0.000000  loss: 4.2255 (4.2923)  class_acc: 0.3333 (0.2786)  loss_scale: 32768.0000 (39316.9222)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1295  max mem: 15572
Epoch: [19]  [1410/2809]  eta: 0:13:18  lr: 0.000030  min_lr: 0.000000  loss: 4.2993 (4.2927)  class_acc: 0.2917 (0.2785)  loss_scale: 32768.0000 (39270.5089)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1247  max mem: 15572
Epoch: [19]  [1420/2809]  eta: 0:13:13  lr: 0.000030  min_lr: 0.000000  loss: 4.3789 (4.2928)  class_acc: 0.2083 (0.2784)  loss_scale: 32768.0000 (39224.7488)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1549  max mem: 15572
Epoch: [19]  [1430/2809]  eta: 0:13:07  lr: 0.000030  min_lr: 0.000000  loss: 4.3696 (4.2935)  class_acc: 0.2500 (0.2786)  loss_scale: 32768.0000 (39179.6282)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.1787  max mem: 15572
Epoch: [19]  [1440/2809]  eta: 0:13:01  lr: 0.000030  min_lr: 0.000000  loss: 4.3607 (4.2932)  class_acc: 0.2500 (0.2785)  loss_scale: 32768.0000 (39135.1339)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.1095  max mem: 15572
Epoch: [19]  [1450/2809]  eta: 0:12:56  lr: 0.000030  min_lr: 0.000000  loss: 4.2296 (4.2932)  class_acc: 0.2500 (0.2785)  loss_scale: 32768.0000 (39091.2529)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1331  max mem: 15572
Epoch: [19]  [1460/2809]  eta: 0:12:49  lr: 0.000030  min_lr: 0.000000  loss: 4.2296 (4.2931)  class_acc: 0.2500 (0.2779)  loss_scale: 32768.0000 (39047.9726)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1079  max mem: 15572
Epoch: [19]  [1470/2809]  eta: 0:12:44  lr: 0.000030  min_lr: 0.000000  loss: 4.0896 (4.2919)  class_acc: 0.2500 (0.2783)  loss_scale: 32768.0000 (39005.2808)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0806  max mem: 15572
[2025-01-15 23:43:41,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:43:41,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 23:43:43,599] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54848
[2025-01-15 23:43:43,599] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:43:43,599] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [1480/2809]  eta: 0:12:38  lr: 0.000030  min_lr: 0.000000  loss: 4.1946 (4.2922)  class_acc: 0.2917 (0.2784)  loss_scale: 32768.0000 (39007.4166)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1283  max mem: 15572
Epoch: [19]  [1490/2809]  eta: 0:12:32  lr: 0.000030  min_lr: 0.000000  loss: 4.3635 (4.2919)  class_acc: 0.2500 (0.2783)  loss_scale: 32768.0000 (38965.5694)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1102  max mem: 15572
Epoch: [19]  [1500/2809]  eta: 0:12:26  lr: 0.000030  min_lr: 0.000000  loss: 4.2569 (4.2914)  class_acc: 0.2083 (0.2781)  loss_scale: 32768.0000 (38924.2798)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1080  max mem: 15572
Epoch: [19]  [1510/2809]  eta: 0:12:20  lr: 0.000030  min_lr: 0.000000  loss: 4.3124 (4.2923)  class_acc: 0.2083 (0.2777)  loss_scale: 32768.0000 (38883.5367)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0908  max mem: 15572
Epoch: [19]  [1520/2809]  eta: 0:12:15  lr: 0.000030  min_lr: 0.000000  loss: 4.3124 (4.2918)  class_acc: 0.2500 (0.2778)  loss_scale: 32768.0000 (38843.3294)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1229  max mem: 15572
Epoch: [19]  [1530/2809]  eta: 0:12:09  lr: 0.000030  min_lr: 0.000000  loss: 4.3050 (4.2927)  class_acc: 0.2500 (0.2774)  loss_scale: 32768.0000 (38803.6473)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1249  max mem: 15572
Epoch: [19]  [1540/2809]  eta: 0:12:02  lr: 0.000030  min_lr: 0.000000  loss: 4.4328 (4.2930)  class_acc: 0.2500 (0.2775)  loss_scale: 32768.0000 (38764.4802)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0443  max mem: 15572
Epoch: [19]  [1550/2809]  eta: 0:11:57  lr: 0.000030  min_lr: 0.000000  loss: 4.3401 (4.2930)  class_acc: 0.2917 (0.2774)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0774  max mem: 15572
Epoch: [19]  [1560/2809]  eta: 0:11:50  lr: 0.000030  min_lr: 0.000000  loss: 4.3146 (4.2930)  class_acc: 0.2083 (0.2771)  loss_scale: 32768.0000 (38687.6515)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0964  max mem: 15572
Epoch: [19]  [1570/2809]  eta: 0:11:44  lr: 0.000030  min_lr: 0.000000  loss: 4.4511 (4.2940)  class_acc: 0.2083 (0.2767)  loss_scale: 32768.0000 (38649.9707)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.0600  max mem: 15572
Epoch: [19]  [1580/2809]  eta: 0:11:39  lr: 0.000030  min_lr: 0.000000  loss: 4.4216 (4.2932)  class_acc: 0.2500 (0.2771)  loss_scale: 32768.0000 (38612.7666)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1179  max mem: 15572
Epoch: [19]  [1590/2809]  eta: 0:11:33  lr: 0.000030  min_lr: 0.000000  loss: 4.1904 (4.2929)  class_acc: 0.2500 (0.2769)  loss_scale: 32768.0000 (38576.0302)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1244  max mem: 15572
Epoch: [19]  [1600/2809]  eta: 0:11:28  lr: 0.000030  min_lr: 0.000000  loss: 4.1904 (4.2916)  class_acc: 0.2083 (0.2768)  loss_scale: 32768.0000 (38539.7527)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1250  max mem: 15572
[2025-01-15 23:44:54,316] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:44:54,316] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [1610/2809]  eta: 0:11:22  lr: 0.000030  min_lr: 0.000000  loss: 4.2800 (4.2918)  class_acc: 0.2083 (0.2766)  loss_scale: 32768.0000 (38605.6263)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1230  max mem: 15572
[2025-01-15 23:44:58,201] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54983
[2025-01-15 23:44:58,201] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:44:58,201] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [1620/2809]  eta: 0:11:16  lr: 0.000030  min_lr: 0.000000  loss: 4.4011 (4.2928)  class_acc: 0.2500 (0.2766)  loss_scale: 32768.0000 (38589.8285)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0658  max mem: 15572
[2025-01-15 23:45:07,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=55000, skipped=344, lr=[2.860995950623248e-07, 2.860995950623248e-07, 4.087137072318926e-07, 4.087137072318926e-07, 5.838767246169895e-07, 5.838767246169895e-07, 8.341096065956993e-07, 8.341096065956993e-07, 1.1915851522795705e-06, 1.1915851522795705e-06, 1.7022645032565293e-06, 1.7022645032565293e-06, 2.4318064332236134e-06, 2.4318064332236134e-06, 3.474009190319448e-06, 3.474009190319448e-06, 4.962870271884926e-06, 4.962870271884926e-06, 7.089814674121324e-06, 7.089814674121324e-06, 1.0128306677316177e-05, 1.0128306677316177e-05, 1.446900953902311e-05, 1.446900953902311e-05, 2.0670013627175875e-05, 2.0670013627175875e-05, 2.9528590895965536e-05, 2.9528590895965536e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:45:07,463] [INFO] [timer.py:260:stop] epoch=0/micro_step=55000/global_step=55000, RunningAvgSamplesPerSec=27.765103664579197, CurrSamplesPerSec=29.09266458810922, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [1630/2809]  eta: 0:11:11  lr: 0.000030  min_lr: 0.000000  loss: 4.3409 (4.2928)  class_acc: 0.2917 (0.2770)  loss_scale: 32768.0000 (38554.1337)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1011  max mem: 15572
Epoch: [19]  [1640/2809]  eta: 0:11:05  lr: 0.000030  min_lr: 0.000000  loss: 4.2713 (4.2931)  class_acc: 0.3333 (0.2771)  loss_scale: 32768.0000 (38518.8739)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1283  max mem: 15572
Epoch: [19]  [1650/2809]  eta: 0:10:59  lr: 0.000030  min_lr: 0.000000  loss: 4.2327 (4.2925)  class_acc: 0.2500 (0.2769)  loss_scale: 32768.0000 (38484.0412)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1245  max mem: 15572
Epoch: [19]  [1660/2809]  eta: 0:10:53  lr: 0.000030  min_lr: 0.000000  loss: 4.2653 (4.2924)  class_acc: 0.2083 (0.2765)  loss_scale: 32768.0000 (38449.6279)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1113  max mem: 15572
Epoch: [19]  [1670/2809]  eta: 0:10:48  lr: 0.000029  min_lr: 0.000000  loss: 4.2653 (4.2911)  class_acc: 0.2500 (0.2765)  loss_scale: 32768.0000 (38415.6266)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1273  max mem: 15572
Epoch: [19]  [1680/2809]  eta: 0:10:42  lr: 0.000029  min_lr: 0.000000  loss: 4.1905 (4.2912)  class_acc: 0.2500 (0.2763)  loss_scale: 32768.0000 (38382.0297)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1225  max mem: 15572
Epoch: [19]  [1690/2809]  eta: 0:10:36  lr: 0.000029  min_lr: 0.000000  loss: 4.3021 (4.2913)  class_acc: 0.2917 (0.2765)  loss_scale: 32768.0000 (38348.8303)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0576  max mem: 15572
Epoch: [19]  [1700/2809]  eta: 0:10:31  lr: 0.000029  min_lr: 0.000000  loss: 4.3385 (4.2915)  class_acc: 0.2917 (0.2765)  loss_scale: 32768.0000 (38316.0212)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1211  max mem: 15572
Epoch: [19]  [1710/2809]  eta: 0:10:25  lr: 0.000029  min_lr: 0.000000  loss: 4.2940 (4.2907)  class_acc: 0.2500 (0.2764)  loss_scale: 32768.0000 (38283.5956)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1647  max mem: 15572
Epoch: [19]  [1720/2809]  eta: 0:10:20  lr: 0.000029  min_lr: 0.000000  loss: 4.1596 (4.2902)  class_acc: 0.2917 (0.2770)  loss_scale: 32768.0000 (38251.5468)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1352  max mem: 15572
Epoch: [19]  [1730/2809]  eta: 0:10:14  lr: 0.000029  min_lr: 0.000000  loss: 4.2172 (4.2902)  class_acc: 0.2917 (0.2768)  loss_scale: 32768.0000 (38219.8683)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1156  max mem: 15572
Epoch: [19]  [1740/2809]  eta: 0:10:07  lr: 0.000029  min_lr: 0.000000  loss: 4.4684 (4.2913)  class_acc: 0.2083 (0.2766)  loss_scale: 32768.0000 (38188.5537)  weight_decay: 0.0500 (0.0500)  time: 0.4920  data: 0.0520  max mem: 15572
[2025-01-15 23:46:10,495] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:46:10,495] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [1750/2809]  eta: 0:10:02  lr: 0.000029  min_lr: 0.000000  loss: 4.2717 (4.2909)  class_acc: 0.2500 (0.2769)  loss_scale: 32768.0000 (38344.7356)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0711  max mem: 15572
Epoch: [19]  [1760/2809]  eta: 0:09:56  lr: 0.000029  min_lr: 0.000000  loss: 4.2353 (4.2914)  class_acc: 0.2917 (0.2771)  loss_scale: 65536.0000 (38499.1437)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0984  max mem: 15572
Epoch: [19]  [1770/2809]  eta: 0:09:50  lr: 0.000029  min_lr: 0.000000  loss: 4.3951 (4.2918)  class_acc: 0.2500 (0.2768)  loss_scale: 65536.0000 (38651.8080)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0842  max mem: 15572
Epoch: [19]  [1780/2809]  eta: 0:09:44  lr: 0.000029  min_lr: 0.000000  loss: 4.3482 (4.2919)  class_acc: 0.2500 (0.2768)  loss_scale: 65536.0000 (38802.7580)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1287  max mem: 15572
Epoch: [19]  [1790/2809]  eta: 0:09:39  lr: 0.000029  min_lr: 0.000000  loss: 4.3482 (4.2927)  class_acc: 0.2500 (0.2768)  loss_scale: 65536.0000 (38952.0223)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1450  max mem: 15572
Epoch: [19]  [1800/2809]  eta: 0:09:33  lr: 0.000029  min_lr: 0.000000  loss: 4.2836 (4.2922)  class_acc: 0.2500 (0.2769)  loss_scale: 65536.0000 (39099.6291)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1219  max mem: 15572
Epoch: [19]  [1810/2809]  eta: 0:09:27  lr: 0.000029  min_lr: 0.000000  loss: 4.2431 (4.2924)  class_acc: 0.2917 (0.2771)  loss_scale: 65536.0000 (39245.6057)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1296  max mem: 15572
Epoch: [19]  [1820/2809]  eta: 0:09:22  lr: 0.000029  min_lr: 0.000000  loss: 4.2623 (4.2927)  class_acc: 0.2500 (0.2768)  loss_scale: 65536.0000 (39389.9791)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1492  max mem: 15572
Epoch: [19]  [1830/2809]  eta: 0:09:16  lr: 0.000029  min_lr: 0.000000  loss: 4.2623 (4.2926)  class_acc: 0.2083 (0.2767)  loss_scale: 65536.0000 (39532.7755)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1332  max mem: 15572
Epoch: [19]  [1840/2809]  eta: 0:09:11  lr: 0.000029  min_lr: 0.000000  loss: 4.3312 (4.2931)  class_acc: 0.2917 (0.2767)  loss_scale: 65536.0000 (39674.0206)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1594  max mem: 15572
Epoch: [19]  [1850/2809]  eta: 0:09:05  lr: 0.000029  min_lr: 0.000000  loss: 4.3042 (4.2934)  class_acc: 0.2917 (0.2768)  loss_scale: 65536.0000 (39813.7396)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1163  max mem: 15572
Epoch: [19]  [1860/2809]  eta: 0:08:59  lr: 0.000029  min_lr: 0.000000  loss: 4.3015 (4.2935)  class_acc: 0.2500 (0.2768)  loss_scale: 65536.0000 (39951.9570)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0482  max mem: 15572
[2025-01-15 23:47:23,505] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:47:23,506] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:47:23,984] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55241
[2025-01-15 23:47:23,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:47:23,984] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1870/2809]  eta: 0:08:53  lr: 0.000029  min_lr: 0.000000  loss: 4.1803 (4.2927)  class_acc: 0.2500 (0.2767)  loss_scale: 65536.0000 (40123.7242)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1078  max mem: 15572
Epoch: [19]  [1880/2809]  eta: 0:08:48  lr: 0.000029  min_lr: 0.000000  loss: 4.1803 (4.2926)  class_acc: 0.2500 (0.2769)  loss_scale: 65536.0000 (40258.8240)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1591  max mem: 15572
Epoch: [19]  [1890/2809]  eta: 0:08:43  lr: 0.000029  min_lr: 0.000000  loss: 4.3347 (4.2927)  class_acc: 0.2500 (0.2769)  loss_scale: 65536.0000 (40392.4950)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1611  max mem: 15572
Epoch: [19]  [1900/2809]  eta: 0:08:37  lr: 0.000029  min_lr: 0.000000  loss: 4.2758 (4.2926)  class_acc: 0.2500 (0.2769)  loss_scale: 65536.0000 (40524.7596)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1538  max mem: 15572
[2025-01-15 23:47:45,565] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55276
[2025-01-15 23:47:45,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:47:45,565] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [1910/2809]  eta: 0:08:31  lr: 0.000029  min_lr: 0.000000  loss: 4.2645 (4.2929)  class_acc: 0.2500 (0.2771)  loss_scale: 65536.0000 (40552.7577)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1230  max mem: 15572
Epoch: [19]  [1920/2809]  eta: 0:08:25  lr: 0.000029  min_lr: 0.000000  loss: 4.3983 (4.2928)  class_acc: 0.2917 (0.2772)  loss_scale: 32768.0000 (40512.2332)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0994  max mem: 15572
Epoch: [19]  [1930/2809]  eta: 0:08:20  lr: 0.000029  min_lr: 0.000000  loss: 4.2983 (4.2925)  class_acc: 0.3333 (0.2774)  loss_scale: 32768.0000 (40472.1284)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1623  max mem: 15572
Epoch: [19]  [1940/2809]  eta: 0:08:14  lr: 0.000029  min_lr: 0.000000  loss: 4.3643 (4.2929)  class_acc: 0.2917 (0.2775)  loss_scale: 32768.0000 (40432.4369)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1154  max mem: 15572
Epoch: [19]  [1950/2809]  eta: 0:08:09  lr: 0.000029  min_lr: 0.000000  loss: 4.3461 (4.2925)  class_acc: 0.2500 (0.2775)  loss_scale: 32768.0000 (40393.1522)  weight_decay: 0.0500 (0.0500)  time: 0.5241  data: 0.0781  max mem: 15572
Epoch: [19]  [1960/2809]  eta: 0:08:03  lr: 0.000029  min_lr: 0.000000  loss: 4.3508 (4.2935)  class_acc: 0.2500 (0.2776)  loss_scale: 32768.0000 (40354.2682)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1559  max mem: 15572
Epoch: [19]  [1970/2809]  eta: 0:07:57  lr: 0.000029  min_lr: 0.000000  loss: 4.4373 (4.2939)  class_acc: 0.2500 (0.2776)  loss_scale: 32768.0000 (40315.7788)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1416  max mem: 15572
Epoch: [19]  [1980/2809]  eta: 0:07:52  lr: 0.000029  min_lr: 0.000000  loss: 4.2918 (4.2940)  class_acc: 0.2500 (0.2776)  loss_scale: 32768.0000 (40277.6779)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1458  max mem: 15572
Epoch: [19]  [1990/2809]  eta: 0:07:46  lr: 0.000029  min_lr: 0.000000  loss: 4.3128 (4.2940)  class_acc: 0.2917 (0.2779)  loss_scale: 32768.0000 (40239.9598)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1522  max mem: 15572
Epoch: [19]  [2000/2809]  eta: 0:07:40  lr: 0.000029  min_lr: 0.000000  loss: 4.3388 (4.2946)  class_acc: 0.3333 (0.2780)  loss_scale: 32768.0000 (40202.6187)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1322  max mem: 15572
Epoch: [19]  [2010/2809]  eta: 0:07:35  lr: 0.000029  min_lr: 0.000000  loss: 4.4600 (4.2950)  class_acc: 0.2917 (0.2779)  loss_scale: 32768.0000 (40165.6489)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1284  max mem: 15572
Epoch: [19]  [2020/2809]  eta: 0:07:29  lr: 0.000029  min_lr: 0.000000  loss: 4.4662 (4.2959)  class_acc: 0.2917 (0.2783)  loss_scale: 32768.0000 (40129.0450)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.1049  max mem: 15572
Epoch: [19]  [2030/2809]  eta: 0:07:23  lr: 0.000029  min_lr: 0.000000  loss: 4.4809 (4.2963)  class_acc: 0.2917 (0.2783)  loss_scale: 32768.0000 (40092.8016)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1224  max mem: 15572
[2025-01-15 23:48:58,887] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:48:58,888] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [2040/2809]  eta: 0:07:17  lr: 0.000029  min_lr: 0.000000  loss: 4.3738 (4.2964)  class_acc: 0.2500 (0.2781)  loss_scale: 32768.0000 (40169.2974)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0840  max mem: 15572
[2025-01-15 23:49:06,519] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55419
[2025-01-15 23:49:06,519] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:49:06,519] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [2050/2809]  eta: 0:07:11  lr: 0.000029  min_lr: 0.000000  loss: 4.3738 (4.2970)  class_acc: 0.2500 (0.2779)  loss_scale: 65536.0000 (40245.0473)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0723  max mem: 15572
Epoch: [19]  [2060/2809]  eta: 0:07:05  lr: 0.000029  min_lr: 0.000000  loss: 4.3710 (4.2969)  class_acc: 0.2083 (0.2777)  loss_scale: 32768.0000 (40208.7686)  weight_decay: 0.0500 (0.0500)  time: 0.5111  data: 0.0723  max mem: 15572
Epoch: [19]  [2070/2809]  eta: 0:06:59  lr: 0.000029  min_lr: 0.000000  loss: 4.2518 (4.2970)  class_acc: 0.2083 (0.2776)  loss_scale: 32768.0000 (40172.8402)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0365  max mem: 15572
Epoch: [19]  [2080/2809]  eta: 0:06:54  lr: 0.000029  min_lr: 0.000000  loss: 4.2518 (4.2968)  class_acc: 0.2500 (0.2778)  loss_scale: 32768.0000 (40137.2571)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1137  max mem: 15572
Epoch: [19]  [2090/2809]  eta: 0:06:49  lr: 0.000029  min_lr: 0.000000  loss: 4.1999 (4.2963)  class_acc: 0.2917 (0.2779)  loss_scale: 32768.0000 (40102.0143)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.2011  max mem: 15572
Epoch: [19]  [2100/2809]  eta: 0:06:43  lr: 0.000029  min_lr: 0.000000  loss: 4.1999 (4.2961)  class_acc: 0.2917 (0.2783)  loss_scale: 32768.0000 (40067.1071)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1576  max mem: 15572
Epoch: [19]  [2110/2809]  eta: 0:06:37  lr: 0.000029  min_lr: 0.000000  loss: 4.2406 (4.2965)  class_acc: 0.2917 (0.2780)  loss_scale: 32768.0000 (40032.5306)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1359  max mem: 15572
Epoch: [19]  [2120/2809]  eta: 0:06:31  lr: 0.000029  min_lr: 0.000000  loss: 4.2904 (4.2964)  class_acc: 0.2500 (0.2779)  loss_scale: 32768.0000 (39998.2801)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1124  max mem: 15572
Epoch: [19]  [2130/2809]  eta: 0:06:26  lr: 0.000029  min_lr: 0.000000  loss: 4.2333 (4.2960)  class_acc: 0.2917 (0.2781)  loss_scale: 32768.0000 (39964.3510)  weight_decay: 0.0500 (0.0500)  time: 0.5223  data: 0.0826  max mem: 15572
Epoch: [19]  [2140/2809]  eta: 0:06:20  lr: 0.000029  min_lr: 0.000000  loss: 4.1350 (4.2952)  class_acc: 0.3333 (0.2785)  loss_scale: 32768.0000 (39930.7389)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1189  max mem: 15572
Epoch: [19]  [2150/2809]  eta: 0:06:14  lr: 0.000029  min_lr: 0.000000  loss: 4.3102 (4.2955)  class_acc: 0.3333 (0.2787)  loss_scale: 32768.0000 (39897.4393)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0807  max mem: 15572
Epoch: [19]  [2160/2809]  eta: 0:06:08  lr: 0.000029  min_lr: 0.000000  loss: 4.4551 (4.2958)  class_acc: 0.2500 (0.2784)  loss_scale: 32768.0000 (39864.4479)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1151  max mem: 15572
Epoch: [19]  [2170/2809]  eta: 0:06:03  lr: 0.000029  min_lr: 0.000000  loss: 4.2796 (4.2952)  class_acc: 0.2500 (0.2783)  loss_scale: 32768.0000 (39831.7605)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1786  max mem: 15572
[2025-01-15 23:50:18,550] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:50:18,551] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [2180/2809]  eta: 0:05:58  lr: 0.000029  min_lr: 0.000000  loss: 4.2190 (4.2950)  class_acc: 0.2500 (0.2784)  loss_scale: 32768.0000 (39859.4700)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.2055  max mem: 15572
Epoch: [19]  [2190/2809]  eta: 0:05:52  lr: 0.000029  min_lr: 0.000000  loss: 4.3651 (4.2958)  class_acc: 0.2500 (0.2780)  loss_scale: 65536.0000 (39976.6609)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1599  max mem: 15572
Epoch: [19]  [2200/2809]  eta: 0:05:46  lr: 0.000029  min_lr: 0.000000  loss: 4.3073 (4.2951)  class_acc: 0.2083 (0.2779)  loss_scale: 65536.0000 (40092.7869)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0855  max mem: 15572
[2025-01-15 23:50:35,352] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55577
[2025-01-15 23:50:35,352] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:50:35,353] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [2210/2809]  eta: 0:05:40  lr: 0.000029  min_lr: 0.000000  loss: 4.2501 (4.2952)  class_acc: 0.2500 (0.2782)  loss_scale: 65536.0000 (40133.7603)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0625  max mem: 15572
Epoch: [19]  [2220/2809]  eta: 0:05:35  lr: 0.000029  min_lr: 0.000000  loss: 4.3439 (4.2953)  class_acc: 0.3333 (0.2782)  loss_scale: 32768.0000 (40100.5961)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1299  max mem: 15572
Epoch: [19]  [2230/2809]  eta: 0:05:29  lr: 0.000029  min_lr: 0.000000  loss: 4.2905 (4.2953)  class_acc: 0.2917 (0.2783)  loss_scale: 32768.0000 (40067.7293)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1415  max mem: 15572
Epoch: [19]  [2240/2809]  eta: 0:05:23  lr: 0.000029  min_lr: 0.000000  loss: 4.2659 (4.2953)  class_acc: 0.2917 (0.2784)  loss_scale: 32768.0000 (40035.1557)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1302  max mem: 15572
Epoch: [19]  [2250/2809]  eta: 0:05:18  lr: 0.000029  min_lr: 0.000000  loss: 4.3533 (4.2955)  class_acc: 0.2917 (0.2786)  loss_scale: 32768.0000 (40002.8716)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1171  max mem: 15572
Epoch: [19]  [2260/2809]  eta: 0:05:12  lr: 0.000029  min_lr: 0.000000  loss: 4.2714 (4.2952)  class_acc: 0.2917 (0.2784)  loss_scale: 32768.0000 (39970.8731)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0765  max mem: 15572
Epoch: [19]  [2270/2809]  eta: 0:05:06  lr: 0.000029  min_lr: 0.000000  loss: 4.2853 (4.2951)  class_acc: 0.2083 (0.2785)  loss_scale: 32768.0000 (39939.1563)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0824  max mem: 15572
Epoch: [19]  [2280/2809]  eta: 0:05:00  lr: 0.000029  min_lr: 0.000000  loss: 4.2853 (4.2950)  class_acc: 0.2500 (0.2785)  loss_scale: 32768.0000 (39907.7177)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0761  max mem: 15572
Epoch: [19]  [2290/2809]  eta: 0:04:55  lr: 0.000029  min_lr: 0.000000  loss: 4.0785 (4.2937)  class_acc: 0.3333 (0.2790)  loss_scale: 32768.0000 (39876.5535)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1171  max mem: 15572
Epoch: [19]  [2300/2809]  eta: 0:04:49  lr: 0.000029  min_lr: 0.000000  loss: 4.0785 (4.2938)  class_acc: 0.3333 (0.2791)  loss_scale: 32768.0000 (39845.6601)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1289  max mem: 15572
Epoch: [19]  [2310/2809]  eta: 0:04:43  lr: 0.000029  min_lr: 0.000000  loss: 4.1466 (4.2930)  class_acc: 0.2500 (0.2790)  loss_scale: 32768.0000 (39815.0342)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1256  max mem: 15572
Epoch: [19]  [2320/2809]  eta: 0:04:38  lr: 0.000029  min_lr: 0.000000  loss: 4.2446 (4.2932)  class_acc: 0.2500 (0.2788)  loss_scale: 32768.0000 (39784.6721)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1224  max mem: 15572
Epoch: [19]  [2330/2809]  eta: 0:04:32  lr: 0.000029  min_lr: 0.000000  loss: 4.3352 (4.2934)  class_acc: 0.2500 (0.2787)  loss_scale: 32768.0000 (39754.5706)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1292  max mem: 15572
[2025-01-15 23:51:48,501] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:51:48,502] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [2340/2809]  eta: 0:04:26  lr: 0.000029  min_lr: 0.000000  loss: 4.3323 (4.2933)  class_acc: 0.2500 (0.2787)  loss_scale: 32768.0000 (39808.7108)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1069  max mem: 15572
Epoch: [19]  [2350/2809]  eta: 0:04:20  lr: 0.000029  min_lr: 0.000000  loss: 4.1907 (4.2927)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (39918.1421)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0509  max mem: 15572
Epoch: [19]  [2360/2809]  eta: 0:04:15  lr: 0.000029  min_lr: 0.000000  loss: 4.1669 (4.2924)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (40026.6463)  weight_decay: 0.0500 (0.0500)  time: 0.4947  data: 0.0484  max mem: 15572
Epoch: [19]  [2370/2809]  eta: 0:04:09  lr: 0.000029  min_lr: 0.000000  loss: 4.3476 (4.2926)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (40134.2353)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1433  max mem: 15572
Epoch: [19]  [2380/2809]  eta: 0:04:03  lr: 0.000029  min_lr: 0.000000  loss: 4.3476 (4.2926)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (40240.9206)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1546  max mem: 15572
Epoch: [19]  [2390/2809]  eta: 0:03:58  lr: 0.000029  min_lr: 0.000000  loss: 4.2674 (4.2920)  class_acc: 0.2917 (0.2788)  loss_scale: 65536.0000 (40346.7135)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1300  max mem: 15572
Epoch: [19]  [2400/2809]  eta: 0:03:52  lr: 0.000029  min_lr: 0.000000  loss: 4.0937 (4.2912)  class_acc: 0.2917 (0.2789)  loss_scale: 65536.0000 (40451.6252)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1566  max mem: 15572
Epoch: [19]  [2410/2809]  eta: 0:03:46  lr: 0.000029  min_lr: 0.000000  loss: 4.1432 (4.2911)  class_acc: 0.2500 (0.2786)  loss_scale: 65536.0000 (40555.6665)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1173  max mem: 15572
Epoch: [19]  [2420/2809]  eta: 0:03:41  lr: 0.000029  min_lr: 0.000000  loss: 4.2910 (4.2912)  class_acc: 0.2083 (0.2784)  loss_scale: 65536.0000 (40658.8484)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0961  max mem: 15572
Epoch: [19]  [2430/2809]  eta: 0:03:35  lr: 0.000029  min_lr: 0.000000  loss: 4.2244 (4.2908)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (40761.1814)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1258  max mem: 15572
Epoch: [19]  [2440/2809]  eta: 0:03:29  lr: 0.000029  min_lr: 0.000000  loss: 4.1946 (4.2907)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (40862.6760)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1625  max mem: 15572
[2025-01-15 23:52:51,941] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55817
[2025-01-15 23:52:51,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:52:51,942] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [2450/2809]  eta: 0:03:24  lr: 0.000029  min_lr: 0.000000  loss: 4.2947 (4.2909)  class_acc: 0.2500 (0.2787)  loss_scale: 65536.0000 (40896.4961)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1553  max mem: 15572
Epoch: [19]  [2460/2809]  eta: 0:03:18  lr: 0.000029  min_lr: 0.000000  loss: 4.2947 (4.2906)  class_acc: 0.2500 (0.2788)  loss_scale: 32768.0000 (40863.4669)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1204  max mem: 15572
Epoch: [19]  [2470/2809]  eta: 0:03:12  lr: 0.000029  min_lr: 0.000000  loss: 4.2544 (4.2906)  class_acc: 0.2500 (0.2789)  loss_scale: 32768.0000 (40830.7050)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1217  max mem: 15572
Epoch: [19]  [2480/2809]  eta: 0:03:07  lr: 0.000029  min_lr: 0.000000  loss: 4.2246 (4.2904)  class_acc: 0.2917 (0.2789)  loss_scale: 32768.0000 (40798.2072)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1331  max mem: 15572
Epoch: [19]  [2490/2809]  eta: 0:03:01  lr: 0.000029  min_lr: 0.000000  loss: 4.3467 (4.2914)  class_acc: 0.2500 (0.2785)  loss_scale: 32768.0000 (40765.9703)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1481  max mem: 15572
Epoch: [19]  [2500/2809]  eta: 0:02:55  lr: 0.000029  min_lr: 0.000000  loss: 4.3147 (4.2912)  class_acc: 0.2500 (0.2787)  loss_scale: 32768.0000 (40733.9912)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0922  max mem: 15572
Epoch: [19]  [2510/2809]  eta: 0:02:50  lr: 0.000029  min_lr: 0.000000  loss: 4.2286 (4.2910)  class_acc: 0.2917 (0.2786)  loss_scale: 32768.0000 (40702.2668)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0358  max mem: 15572
Epoch: [19]  [2520/2809]  eta: 0:02:44  lr: 0.000029  min_lr: 0.000000  loss: 4.2785 (4.2908)  class_acc: 0.2917 (0.2787)  loss_scale: 32768.0000 (40670.7941)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1379  max mem: 15572
Epoch: [19]  [2530/2809]  eta: 0:02:38  lr: 0.000029  min_lr: 0.000000  loss: 4.1945 (4.2907)  class_acc: 0.3333 (0.2788)  loss_scale: 32768.0000 (40639.5701)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1273  max mem: 15572
Epoch: [19]  [2540/2809]  eta: 0:02:33  lr: 0.000029  min_lr: 0.000000  loss: 4.1663 (4.2903)  class_acc: 0.2083 (0.2786)  loss_scale: 32768.0000 (40608.5919)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0337  max mem: 15572
Epoch: [19]  [2550/2809]  eta: 0:02:27  lr: 0.000029  min_lr: 0.000000  loss: 4.3100 (4.2905)  class_acc: 0.2083 (0.2783)  loss_scale: 32768.0000 (40577.8565)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0451  max mem: 15572
Epoch: [19]  [2560/2809]  eta: 0:02:21  lr: 0.000029  min_lr: 0.000000  loss: 4.3100 (4.2906)  class_acc: 0.2083 (0.2782)  loss_scale: 32768.0000 (40547.3612)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0856  max mem: 15572
Epoch: [19]  [2570/2809]  eta: 0:02:15  lr: 0.000029  min_lr: 0.000000  loss: 4.2180 (4.2901)  class_acc: 0.2500 (0.2785)  loss_scale: 32768.0000 (40517.1031)  weight_decay: 0.0500 (0.0500)  time: 0.5151  data: 0.0499  max mem: 15572
[2025-01-15 23:54:04,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:54:04,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [2580/2809]  eta: 0:02:10  lr: 0.000029  min_lr: 0.000000  loss: 4.2180 (4.2902)  class_acc: 0.2917 (0.2784)  loss_scale: 32768.0000 (40563.2546)  weight_decay: 0.0500 (0.0500)  time: 0.5224  data: 0.0371  max mem: 15572
Epoch: [19]  [2590/2809]  eta: 0:02:04  lr: 0.000029  min_lr: 0.000000  loss: 4.3666 (4.2909)  class_acc: 0.2500 (0.2785)  loss_scale: 65536.0000 (40659.6372)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0728  max mem: 15572
Epoch: [19]  [2600/2809]  eta: 0:01:58  lr: 0.000029  min_lr: 0.000000  loss: 4.3579 (4.2907)  class_acc: 0.2500 (0.2787)  loss_scale: 65536.0000 (40755.2787)  weight_decay: 0.0500 (0.0500)  time: 0.6575  data: 0.1543  max mem: 15572
Epoch: [19]  [2610/2809]  eta: 0:01:53  lr: 0.000029  min_lr: 0.000000  loss: 4.3510 (4.2910)  class_acc: 0.2500 (0.2784)  loss_scale: 65536.0000 (40850.1877)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.1188  max mem: 15572
[2025-01-15 23:54:27,373] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55983
[2025-01-15 23:54:27,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:54:27,374] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [2620/2809]  eta: 0:01:47  lr: 0.000029  min_lr: 0.000000  loss: 4.3238 (4.2907)  class_acc: 0.2500 (0.2785)  loss_scale: 65536.0000 (40831.8535)  weight_decay: 0.0500 (0.0500)  time: 0.5161  data: 0.0009  max mem: 15572
[2025-01-15 23:54:35,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=56000, skipped=350, lr=[2.790693606018548e-07, 2.790693606018548e-07, 3.9867051514550694e-07, 3.9867051514550694e-07, 5.695293073507243e-07, 5.695293073507243e-07, 8.136132962153204e-07, 8.136132962153204e-07, 1.1623047088790292e-06, 1.1623047088790292e-06, 1.6604352983986132e-06, 1.6604352983986132e-06, 2.3720504262837332e-06, 2.3720504262837332e-06, 3.3886434661196192e-06, 3.3886434661196192e-06, 4.840919237313742e-06, 4.840919237313742e-06, 6.915598910448203e-06, 6.915598910448203e-06, 9.879427014926004e-06, 9.879427014926004e-06, 1.4113467164180008e-05, 1.4113467164180008e-05, 2.0162095948828585e-05, 2.0162095948828585e-05, 2.8802994212612265e-05, 2.8802994212612265e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:54:35,114] [INFO] [timer.py:260:stop] epoch=0/micro_step=56000/global_step=56000, RunningAvgSamplesPerSec=27.767717834381386, CurrSamplesPerSec=25.869194462847595, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [2630/2809]  eta: 0:01:41  lr: 0.000029  min_lr: 0.000000  loss: 4.2642 (4.2908)  class_acc: 0.2500 (0.2783)  loss_scale: 32768.0000 (40801.2041)  weight_decay: 0.0500 (0.0500)  time: 0.4790  data: 0.0130  max mem: 15572
Epoch: [19]  [2640/2809]  eta: 0:01:36  lr: 0.000029  min_lr: 0.000000  loss: 4.3469 (4.2907)  class_acc: 0.2500 (0.2784)  loss_scale: 32768.0000 (40770.7868)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1270  max mem: 15572
Epoch: [19]  [2650/2809]  eta: 0:01:30  lr: 0.000029  min_lr: 0.000000  loss: 4.3140 (4.2911)  class_acc: 0.2500 (0.2782)  loss_scale: 32768.0000 (40740.5990)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.1740  max mem: 15572
Epoch: [19]  [2660/2809]  eta: 0:01:24  lr: 0.000029  min_lr: 0.000000  loss: 4.2780 (4.2912)  class_acc: 0.2500 (0.2782)  loss_scale: 32768.0000 (40710.6381)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1265  max mem: 15572
Epoch: [19]  [2670/2809]  eta: 0:01:19  lr: 0.000029  min_lr: 0.000000  loss: 4.2089 (4.2910)  class_acc: 0.2917 (0.2783)  loss_scale: 32768.0000 (40680.9015)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1191  max mem: 15572
Epoch: [19]  [2680/2809]  eta: 0:01:13  lr: 0.000029  min_lr: 0.000000  loss: 4.1974 (4.2909)  class_acc: 0.2500 (0.2782)  loss_scale: 32768.0000 (40651.3868)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.1022  max mem: 15572
Epoch: [19]  [2690/2809]  eta: 0:01:07  lr: 0.000029  min_lr: 0.000000  loss: 4.3623 (4.2913)  class_acc: 0.2917 (0.2784)  loss_scale: 32768.0000 (40622.0914)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1074  max mem: 15572
Epoch: [19]  [2700/2809]  eta: 0:01:01  lr: 0.000029  min_lr: 0.000000  loss: 4.3623 (4.2911)  class_acc: 0.2917 (0.2785)  loss_scale: 32768.0000 (40593.0130)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0982  max mem: 15572
Epoch: [19]  [2710/2809]  eta: 0:00:56  lr: 0.000029  min_lr: 0.000000  loss: 4.2456 (4.2911)  class_acc: 0.2500 (0.2786)  loss_scale: 32768.0000 (40564.1490)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0904  max mem: 15572
Epoch: [19]  [2720/2809]  eta: 0:00:50  lr: 0.000029  min_lr: 0.000000  loss: 4.3246 (4.2909)  class_acc: 0.2500 (0.2785)  loss_scale: 32768.0000 (40535.4972)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0938  max mem: 15572
Epoch: [19]  [2730/2809]  eta: 0:00:44  lr: 0.000029  min_lr: 0.000000  loss: 4.3177 (4.2908)  class_acc: 0.2500 (0.2787)  loss_scale: 32768.0000 (40507.0553)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0955  max mem: 15572
Epoch: [19]  [2740/2809]  eta: 0:00:39  lr: 0.000029  min_lr: 0.000000  loss: 4.3177 (4.2910)  class_acc: 0.2500 (0.2786)  loss_scale: 32768.0000 (40478.8209)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0514  max mem: 15572
[2025-01-15 23:55:37,459] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:55:37,459] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [2750/2809]  eta: 0:00:33  lr: 0.000029  min_lr: 0.000000  loss: 4.3438 (4.2911)  class_acc: 0.2917 (0.2787)  loss_scale: 32768.0000 (40569.9048)  weight_decay: 0.0500 (0.0500)  time: 0.3998  data: 0.0003  max mem: 15572
Epoch: [19]  [2760/2809]  eta: 0:00:27  lr: 0.000029  min_lr: 0.000000  loss: 4.2951 (4.2911)  class_acc: 0.2917 (0.2789)  loss_scale: 65536.0000 (40660.3289)  weight_decay: 0.0500 (0.0500)  time: 0.4382  data: 0.0005  max mem: 15572
Epoch: [19]  [2770/2809]  eta: 0:00:22  lr: 0.000029  min_lr: 0.000000  loss: 4.2040 (4.2909)  class_acc: 0.3333 (0.2792)  loss_scale: 65536.0000 (40750.1003)  weight_decay: 0.0500 (0.0500)  time: 0.4673  data: 0.0006  max mem: 15572
Epoch: [19]  [2780/2809]  eta: 0:00:16  lr: 0.000029  min_lr: 0.000000  loss: 4.2040 (4.2908)  class_acc: 0.3333 (0.2793)  loss_scale: 65536.0000 (40839.2262)  weight_decay: 0.0500 (0.0500)  time: 0.4662  data: 0.0063  max mem: 15572
[2025-01-15 23:55:59,374] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56156
[2025-01-15 23:55:59,375] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:55:59,375] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [2790/2809]  eta: 0:00:10  lr: 0.000029  min_lr: 0.000000  loss: 4.3491 (4.2907)  class_acc: 0.2917 (0.2794)  loss_scale: 65536.0000 (40857.2698)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.1861  max mem: 15572
Epoch: [19]  [2800/2809]  eta: 0:00:05  lr: 0.000029  min_lr: 0.000000  loss: 4.3234 (4.2908)  class_acc: 0.2917 (0.2794)  loss_scale: 32768.0000 (40828.3899)  weight_decay: 0.0500 (0.0500)  time: 0.7173  data: 0.2611  max mem: 15572
Epoch: [19]  [2808/2809]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000000  loss: 4.3201 (4.2907)  class_acc: 0.2500 (0.2794)  loss_scale: 32768.0000 (40805.4340)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.1024  max mem: 15572
Epoch: [19] Total time: 0:26:34 (0.5675 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000000  loss: 4.3201 (4.2907)  class_acc: 0.2500 (0.2794)  loss_scale: 32768.0000 (40805.4340)  weight_decay: 0.0500 (0.0500)
[2025-01-15 23:56:13,539] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2025-01-15 23:56:13,544] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-19/mp_rank_00_model_states.pt
[2025-01-15 23:56:13,544] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-19/mp_rank_00_model_states.pt...
[2025-01-15 23:56:14,569] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-19/mp_rank_00_model_states.pt.
[2025-01-15 23:56:14,569] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [  0/272]  eta: 0:26:46  loss: 1.3616 (1.3616)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 5.9080  data: 5.6668  max mem: 15572
Val:  [ 10/272]  eta: 0:03:54  loss: 2.7609 (2.6602)  acc1: 38.8889 (38.3838)  acc5: 77.7778 (73.7374)  time: 0.8966  data: 0.6977  max mem: 15572
Val:  [ 20/272]  eta: 0:02:36  loss: 2.7663 (2.7350)  acc1: 38.8889 (39.1534)  acc5: 72.2222 (71.6931)  time: 0.3554  data: 0.1628  max mem: 15572
Val:  [ 30/272]  eta: 0:02:01  loss: 2.8490 (2.7997)  acc1: 44.4444 (37.0968)  acc5: 66.6667 (70.2509)  time: 0.2846  data: 0.0874  max mem: 15572
Val:  [ 40/272]  eta: 0:01:49  loss: 2.8490 (2.8132)  acc1: 27.7778 (35.9079)  acc5: 72.2222 (70.5962)  time: 0.3158  data: 0.1106  max mem: 15572
Val:  [ 50/272]  eta: 0:01:40  loss: 2.6828 (2.7367)  acc1: 38.8889 (38.6710)  acc5: 77.7778 (72.9847)  time: 0.3727  data: 0.1572  max mem: 15572
Val:  [ 60/272]  eta: 0:01:31  loss: 2.0695 (2.6621)  acc1: 55.5556 (40.7104)  acc5: 83.3333 (73.5883)  time: 0.3467  data: 0.1396  max mem: 15572
Val:  [ 70/272]  eta: 0:01:25  loss: 2.1047 (2.6000)  acc1: 55.5556 (43.4272)  acc5: 83.3333 (75.3521)  time: 0.3589  data: 0.1520  max mem: 15572
Val:  [ 80/272]  eta: 0:01:19  loss: 2.3776 (2.6110)  acc1: 50.0000 (43.4156)  acc5: 83.3333 (74.9657)  time: 0.3561  data: 0.1445  max mem: 15572
Val:  [ 90/272]  eta: 0:01:15  loss: 2.7303 (2.6270)  acc1: 38.8889 (43.1013)  acc5: 77.7778 (75.1526)  time: 0.3684  data: 0.1706  max mem: 15572
Val:  [100/272]  eta: 0:01:10  loss: 2.7533 (2.6575)  acc1: 33.3333 (41.9692)  acc5: 72.2222 (74.5325)  time: 0.4042  data: 0.2038  max mem: 15572
Val:  [110/272]  eta: 0:01:05  loss: 2.9096 (2.7191)  acc1: 27.7778 (40.0400)  acc5: 72.2222 (72.6727)  time: 0.3750  data: 0.1682  max mem: 15572
Val:  [120/272]  eta: 0:01:00  loss: 3.0985 (2.7453)  acc1: 27.7778 (40.0826)  acc5: 61.1111 (71.9467)  time: 0.3405  data: 0.1406  max mem: 15572
Val:  [130/272]  eta: 0:00:55  loss: 2.6548 (2.7230)  acc1: 50.0000 (41.0093)  acc5: 72.2222 (72.6039)  time: 0.3051  data: 0.1143  max mem: 15572
Val:  [140/272]  eta: 0:00:50  loss: 2.3181 (2.7225)  acc1: 50.0000 (41.5288)  acc5: 77.7778 (72.4980)  time: 0.2671  data: 0.0867  max mem: 15572
Val:  [150/272]  eta: 0:00:44  loss: 2.6427 (2.7163)  acc1: 33.3333 (40.9124)  acc5: 72.2222 (72.6269)  time: 0.2049  data: 0.0389  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 2.5867 (2.7062)  acc1: 38.8889 (41.5804)  acc5: 77.7778 (73.1539)  time: 0.1797  data: 0.0124  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 2.6704 (2.7176)  acc1: 33.3333 (41.0981)  acc5: 77.7778 (72.7096)  time: 0.1890  data: 0.0125  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.6704 (2.7076)  acc1: 33.3333 (40.7612)  acc5: 66.6667 (73.0510)  time: 0.2079  data: 0.0213  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.7464 (2.7443)  acc1: 27.7778 (39.5870)  acc5: 66.6667 (71.8150)  time: 0.2779  data: 0.0937  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.9421 (2.7488)  acc1: 22.2222 (39.4417)  acc5: 66.6667 (71.8076)  time: 0.3422  data: 0.1636  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.7014 (2.7540)  acc1: 33.3333 (39.6524)  acc5: 77.7778 (71.7220)  time: 0.3469  data: 0.1682  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.7268 (2.7473)  acc1: 44.4444 (39.7687)  acc5: 77.7778 (71.8703)  time: 0.2911  data: 0.1135  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.3471 (2.7321)  acc1: 50.0000 (40.4521)  acc5: 77.7778 (72.2222)  time: 0.2701  data: 0.0841  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.2872 (2.7197)  acc1: 55.5556 (40.6639)  acc5: 83.3333 (72.6372)  time: 0.2919  data: 0.0930  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4767 (2.7268)  acc1: 27.7778 (40.0177)  acc5: 77.7778 (72.5764)  time: 0.3117  data: 0.1108  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.1432 (2.6844)  acc1: 61.1111 (41.5922)  acc5: 83.3333 (73.3291)  time: 0.2706  data: 0.0720  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.1432 (2.6843)  acc1: 61.1111 (41.4309)  acc5: 83.3333 (73.2882)  time: 0.2047  data: 0.0269  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.1432 (2.6897)  acc1: 55.5556 (41.4090)  acc5: 83.3333 (73.2337)  time: 0.1984  data: 0.0268  max mem: 15572
Val: Total time: 0:01:27 (0.3222 s / it)
* Acc@1 41.409 Acc@5 73.234 loss 2.690
Accuracy of the network on the 4883 val videos: 41.4%
[2025-01-15 23:57:42,225] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 23:57:42,229] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 23:57:42,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 23:57:45,208] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 23:57:45,209] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.41%
Epoch: [20]  [   0/2809]  eta: 5:44:01  lr: 0.000029  min_lr: 0.000000  loss: 4.0667 (4.0667)  class_acc: 0.1667 (0.1667)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.3483  data: 6.5922  max mem: 15572
Epoch: [20]  [  10/2809]  eta: 1:01:37  lr: 0.000029  min_lr: 0.000000  loss: 4.3752 (4.3645)  class_acc: 0.2917 (0.2689)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3209  data: 0.8474  max mem: 15572
Epoch: [20]  [  20/2809]  eta: 0:46:05  lr: 0.000029  min_lr: 0.000000  loss: 4.3377 (4.3117)  class_acc: 0.2917 (0.2639)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6739  data: 0.2288  max mem: 15572
Epoch: [20]  [  30/2809]  eta: 0:38:01  lr: 0.000029  min_lr: 0.000000  loss: 4.2012 (4.2715)  class_acc: 0.2500 (0.2755)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.1032  max mem: 15572
Epoch: [20]  [  40/2809]  eta: 0:34:19  lr: 0.000029  min_lr: 0.000000  loss: 4.2268 (4.2691)  class_acc: 0.2500 (0.2673)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0534  max mem: 15572
Epoch: [20]  [  50/2809]  eta: 0:33:23  lr: 0.000029  min_lr: 0.000000  loss: 4.2567 (4.2562)  class_acc: 0.2917 (0.2770)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1459  max mem: 15572
Epoch: [20]  [  60/2809]  eta: 0:31:24  lr: 0.000029  min_lr: 0.000000  loss: 4.2611 (4.2823)  class_acc: 0.2917 (0.2739)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1187  max mem: 15572
Epoch: [20]  [  70/2809]  eta: 0:30:31  lr: 0.000029  min_lr: 0.000000  loss: 4.3201 (4.2867)  class_acc: 0.2500 (0.2717)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0889  max mem: 15572
Epoch: [20]  [  80/2809]  eta: 0:29:50  lr: 0.000029  min_lr: 0.000000  loss: 4.2490 (4.2683)  class_acc: 0.2917 (0.2798)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1453  max mem: 15572
Epoch: [20]  [  90/2809]  eta: 0:29:11  lr: 0.000029  min_lr: 0.000000  loss: 4.2490 (4.2768)  class_acc: 0.2917 (0.2807)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1189  max mem: 15572
Epoch: [20]  [ 100/2809]  eta: 0:28:11  lr: 0.000029  min_lr: 0.000000  loss: 4.4223 (4.2828)  class_acc: 0.2500 (0.2801)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4973  data: 0.0474  max mem: 15572
[2025-01-15 23:58:51,343] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:58:51,344] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-15 23:58:53,839] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56290
[2025-01-15 23:58:53,839] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:58:53,839] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 110/2809]  eta: 0:27:47  lr: 0.000029  min_lr: 0.000000  loss: 4.4223 (4.2844)  class_acc: 0.2500 (0.2763)  loss_scale: 32768.0000 (34244.0360)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0461  max mem: 15572
Epoch: [20]  [ 120/2809]  eta: 0:27:30  lr: 0.000029  min_lr: 0.000000  loss: 4.2188 (4.2846)  class_acc: 0.2500 (0.2769)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1111  max mem: 15572
Epoch: [20]  [ 130/2809]  eta: 0:27:24  lr: 0.000029  min_lr: 0.000000  loss: 4.2311 (4.2895)  class_acc: 0.2917 (0.2767)  loss_scale: 32768.0000 (34018.6870)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1354  max mem: 15572
Epoch: [20]  [ 140/2809]  eta: 0:27:11  lr: 0.000029  min_lr: 0.000000  loss: 4.3998 (4.3013)  class_acc: 0.2083 (0.2728)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1401  max mem: 15572
Epoch: [20]  [ 150/2809]  eta: 0:26:55  lr: 0.000029  min_lr: 0.000000  loss: 4.4501 (4.3075)  class_acc: 0.2083 (0.2707)  loss_scale: 32768.0000 (33853.0331)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1251  max mem: 15572
Epoch: [20]  [ 160/2809]  eta: 0:26:43  lr: 0.000029  min_lr: 0.000000  loss: 4.4501 (4.3047)  class_acc: 0.2083 (0.2686)  loss_scale: 32768.0000 (33785.6398)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1213  max mem: 15572
Epoch: [20]  [ 170/2809]  eta: 0:26:51  lr: 0.000029  min_lr: 0.000000  loss: 4.4135 (4.3061)  class_acc: 0.2083 (0.2692)  loss_scale: 32768.0000 (33726.1287)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1841  max mem: 15572
Epoch: [20]  [ 180/2809]  eta: 0:26:23  lr: 0.000029  min_lr: 0.000000  loss: 4.3790 (4.3131)  class_acc: 0.2083 (0.2684)  loss_scale: 32768.0000 (33673.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1255  max mem: 15572
Epoch: [20]  [ 190/2809]  eta: 0:26:10  lr: 0.000029  min_lr: 0.000000  loss: 4.3598 (4.3170)  class_acc: 0.2500 (0.2690)  loss_scale: 32768.0000 (33625.8010)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0645  max mem: 15572
Epoch: [20]  [ 200/2809]  eta: 0:25:59  lr: 0.000029  min_lr: 0.000000  loss: 4.3170 (4.3168)  class_acc: 0.2500 (0.2676)  loss_scale: 32768.0000 (33583.1244)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1084  max mem: 15572
Epoch: [20]  [ 210/2809]  eta: 0:25:54  lr: 0.000029  min_lr: 0.000000  loss: 4.3170 (4.3152)  class_acc: 0.2500 (0.2690)  loss_scale: 32768.0000 (33544.4929)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1271  max mem: 15572
Epoch: [20]  [ 220/2809]  eta: 0:25:42  lr: 0.000029  min_lr: 0.000000  loss: 4.2533 (4.3114)  class_acc: 0.2917 (0.2698)  loss_scale: 32768.0000 (33509.3575)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1253  max mem: 15572
Epoch: [20]  [ 230/2809]  eta: 0:25:38  lr: 0.000029  min_lr: 0.000000  loss: 4.2746 (4.3135)  class_acc: 0.2500 (0.2689)  loss_scale: 32768.0000 (33477.2641)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1462  max mem: 15572
[2025-01-16 00:00:07,484] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:00:07,485] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 240/2809]  eta: 0:25:19  lr: 0.000028  min_lr: 0.000000  loss: 4.3587 (4.3088)  class_acc: 0.2500 (0.2695)  loss_scale: 32768.0000 (33719.7676)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.1177  max mem: 15572
Epoch: [20]  [ 250/2809]  eta: 0:25:07  lr: 0.000028  min_lr: 0.000000  loss: 4.2798 (4.3037)  class_acc: 0.2917 (0.2724)  loss_scale: 65536.0000 (34987.3466)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0754  max mem: 15572
Epoch: [20]  [ 260/2809]  eta: 0:25:02  lr: 0.000028  min_lr: 0.000000  loss: 4.1008 (4.2982)  class_acc: 0.3333 (0.2760)  loss_scale: 65536.0000 (36157.7931)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1312  max mem: 15572
Epoch: [20]  [ 270/2809]  eta: 0:24:56  lr: 0.000028  min_lr: 0.000000  loss: 4.2477 (4.3001)  class_acc: 0.3333 (0.2766)  loss_scale: 65536.0000 (37241.8598)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1514  max mem: 15572
[2025-01-16 00:00:30,609] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56458
[2025-01-16 00:00:30,610] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:00:30,610] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 280/2809]  eta: 0:24:54  lr: 0.000028  min_lr: 0.000000  loss: 4.2477 (4.2978)  class_acc: 0.3333 (0.2805)  loss_scale: 65536.0000 (37898.9324)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1700  max mem: 15572
Epoch: [20]  [ 290/2809]  eta: 0:24:43  lr: 0.000028  min_lr: 0.000000  loss: 4.2177 (4.2964)  class_acc: 0.3333 (0.2798)  loss_scale: 32768.0000 (37722.6117)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1480  max mem: 15572
Epoch: [20]  [ 300/2809]  eta: 0:24:34  lr: 0.000028  min_lr: 0.000000  loss: 4.2319 (4.2891)  class_acc: 0.2917 (0.2818)  loss_scale: 32768.0000 (37558.0066)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.1113  max mem: 15572
Epoch: [20]  [ 310/2809]  eta: 0:24:22  lr: 0.000028  min_lr: 0.000000  loss: 4.3748 (4.2977)  class_acc: 0.2917 (0.2831)  loss_scale: 32768.0000 (37403.9871)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.1009  max mem: 15572
Epoch: [20]  [ 320/2809]  eta: 0:24:10  lr: 0.000028  min_lr: 0.000000  loss: 4.3686 (4.2952)  class_acc: 0.3333 (0.2840)  loss_scale: 32768.0000 (37259.5639)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0847  max mem: 15572
Epoch: [20]  [ 330/2809]  eta: 0:23:59  lr: 0.000028  min_lr: 0.000000  loss: 4.2548 (4.2942)  class_acc: 0.3333 (0.2859)  loss_scale: 32768.0000 (37123.8671)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0865  max mem: 15572
Epoch: [20]  [ 340/2809]  eta: 0:23:56  lr: 0.000028  min_lr: 0.000000  loss: 4.3558 (4.2942)  class_acc: 0.2917 (0.2864)  loss_scale: 32768.0000 (36996.1290)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1361  max mem: 15572
Epoch: [20]  [ 350/2809]  eta: 0:23:49  lr: 0.000028  min_lr: 0.000000  loss: 4.2974 (4.2918)  class_acc: 0.2500 (0.2862)  loss_scale: 32768.0000 (36875.6695)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1510  max mem: 15572
Epoch: [20]  [ 360/2809]  eta: 0:23:39  lr: 0.000028  min_lr: 0.000000  loss: 4.2561 (4.2888)  class_acc: 0.2500 (0.2860)  loss_scale: 32768.0000 (36761.8837)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1025  max mem: 15572
Epoch: [20]  [ 370/2809]  eta: 0:23:33  lr: 0.000028  min_lr: 0.000000  loss: 4.2763 (4.2874)  class_acc: 0.3333 (0.2883)  loss_scale: 32768.0000 (36654.2318)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.1161  max mem: 15572
Epoch: [20]  [ 380/2809]  eta: 0:23:36  lr: 0.000028  min_lr: 0.000000  loss: 4.3103 (4.2875)  class_acc: 0.2917 (0.2878)  loss_scale: 32768.0000 (36552.2310)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.2216  max mem: 15572
Epoch: [20]  [ 390/2809]  eta: 0:23:25  lr: 0.000028  min_lr: 0.000000  loss: 4.2047 (4.2829)  class_acc: 0.2917 (0.2888)  loss_scale: 32768.0000 (36455.4476)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1496  max mem: 15572
Epoch: [20]  [ 400/2809]  eta: 0:23:15  lr: 0.000028  min_lr: 0.000000  loss: 4.2047 (4.2848)  class_acc: 0.2917 (0.2889)  loss_scale: 32768.0000 (36363.4913)  weight_decay: 0.0500 (0.0500)  time: 0.5055  data: 0.0361  max mem: 15572
[2025-01-16 00:01:41,937] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:01:41,938] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 410/2809]  eta: 0:23:11  lr: 0.000028  min_lr: 0.000000  loss: 4.4571 (4.2886)  class_acc: 0.2500 (0.2881)  loss_scale: 32768.0000 (36594.9197)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1268  max mem: 15572
Epoch: [20]  [ 420/2809]  eta: 0:23:03  lr: 0.000028  min_lr: 0.000000  loss: 4.3518 (4.2880)  class_acc: 0.2083 (0.2868)  loss_scale: 65536.0000 (37282.3563)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1267  max mem: 15572
[2025-01-16 00:01:50,133] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56602
[2025-01-16 00:01:50,133] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:01:50,134] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 430/2809]  eta: 0:22:58  lr: 0.000028  min_lr: 0.000000  loss: 4.1376 (4.2859)  class_acc: 0.2500 (0.2877)  loss_scale: 65536.0000 (37253.6427)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1024  max mem: 15572
Epoch: [20]  [ 440/2809]  eta: 0:22:53  lr: 0.000028  min_lr: 0.000000  loss: 4.3308 (4.2889)  class_acc: 0.2500 (0.2861)  loss_scale: 32768.0000 (37151.9274)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1416  max mem: 15572
Epoch: [20]  [ 450/2809]  eta: 0:22:44  lr: 0.000028  min_lr: 0.000000  loss: 4.3308 (4.2884)  class_acc: 0.2500 (0.2865)  loss_scale: 32768.0000 (37054.7228)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1227  max mem: 15572
Epoch: [20]  [ 460/2809]  eta: 0:22:35  lr: 0.000028  min_lr: 0.000000  loss: 4.2780 (4.2894)  class_acc: 0.2500 (0.2862)  loss_scale: 32768.0000 (36961.7354)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0850  max mem: 15572
Epoch: [20]  [ 470/2809]  eta: 0:22:24  lr: 0.000028  min_lr: 0.000000  loss: 4.3040 (4.2892)  class_acc: 0.2500 (0.2863)  loss_scale: 32768.0000 (36872.6964)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0658  max mem: 15572
Epoch: [20]  [ 480/2809]  eta: 0:22:25  lr: 0.000028  min_lr: 0.000000  loss: 4.2049 (4.2863)  class_acc: 0.2500 (0.2853)  loss_scale: 32768.0000 (36787.3597)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1467  max mem: 15572
Epoch: [20]  [ 490/2809]  eta: 0:22:13  lr: 0.000028  min_lr: 0.000000  loss: 4.2513 (4.2871)  class_acc: 0.2083 (0.2845)  loss_scale: 32768.0000 (36705.4990)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1242  max mem: 15572
Epoch: [20]  [ 500/2809]  eta: 0:22:10  lr: 0.000028  min_lr: 0.000000  loss: 4.2513 (4.2864)  class_acc: 0.2083 (0.2845)  loss_scale: 32768.0000 (36626.9062)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1006  max mem: 15572
Epoch: [20]  [ 510/2809]  eta: 0:22:06  lr: 0.000028  min_lr: 0.000000  loss: 4.2440 (4.2863)  class_acc: 0.2500 (0.2848)  loss_scale: 32768.0000 (36551.3894)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1710  max mem: 15572
Epoch: [20]  [ 520/2809]  eta: 0:21:58  lr: 0.000028  min_lr: 0.000000  loss: 4.3594 (4.2866)  class_acc: 0.2500 (0.2846)  loss_scale: 32768.0000 (36478.7716)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1135  max mem: 15572
Epoch: [20]  [ 530/2809]  eta: 0:21:53  lr: 0.000028  min_lr: 0.000000  loss: 4.2402 (4.2856)  class_acc: 0.2917 (0.2857)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1022  max mem: 15572
Epoch: [20]  [ 540/2809]  eta: 0:21:45  lr: 0.000028  min_lr: 0.000000  loss: 4.3038 (4.2866)  class_acc: 0.2917 (0.2857)  loss_scale: 32768.0000 (36341.5896)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1133  max mem: 15572
Epoch: [20]  [ 550/2809]  eta: 0:21:41  lr: 0.000028  min_lr: 0.000000  loss: 4.2267 (4.2839)  class_acc: 0.2917 (0.2855)  loss_scale: 32768.0000 (36276.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1226  max mem: 15572
[2025-01-16 00:03:03,284] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:03:03,284] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 560/2809]  eta: 0:21:35  lr: 0.000028  min_lr: 0.000000  loss: 4.2068 (4.2838)  class_acc: 0.2917 (0.2857)  loss_scale: 32768.0000 (36798.2888)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1441  max mem: 15572
Epoch: [20]  [ 570/2809]  eta: 0:21:28  lr: 0.000028  min_lr: 0.000000  loss: 4.2005 (4.2821)  class_acc: 0.2500 (0.2852)  loss_scale: 65536.0000 (37301.5762)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1069  max mem: 15572
Epoch: [20]  [ 580/2809]  eta: 0:21:21  lr: 0.000028  min_lr: 0.000000  loss: 4.2359 (4.2828)  class_acc: 0.2500 (0.2848)  loss_scale: 65536.0000 (37787.5387)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0855  max mem: 15572
Epoch: [20]  [ 590/2809]  eta: 0:21:15  lr: 0.000028  min_lr: 0.000000  loss: 4.3788 (4.2834)  class_acc: 0.2917 (0.2850)  loss_scale: 65536.0000 (38257.0558)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1130  max mem: 15572
Epoch: [20]  [ 600/2809]  eta: 0:21:07  lr: 0.000028  min_lr: 0.000000  loss: 4.2585 (4.2829)  class_acc: 0.2917 (0.2840)  loss_scale: 65536.0000 (38710.9484)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.1043  max mem: 15572
Epoch: [20]  [ 610/2809]  eta: 0:20:58  lr: 0.000028  min_lr: 0.000000  loss: 4.2557 (4.2829)  class_acc: 0.2500 (0.2838)  loss_scale: 65536.0000 (39149.9836)  weight_decay: 0.0500 (0.0500)  time: 0.5015  data: 0.0622  max mem: 15572
Epoch: [20]  [ 620/2809]  eta: 0:20:55  lr: 0.000028  min_lr: 0.000000  loss: 4.2064 (4.2815)  class_acc: 0.2500 (0.2844)  loss_scale: 65536.0000 (39574.8792)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1329  max mem: 15572
Epoch: [20]  [ 630/2809]  eta: 0:20:50  lr: 0.000028  min_lr: 0.000000  loss: 4.2688 (4.2840)  class_acc: 0.2500 (0.2841)  loss_scale: 65536.0000 (39986.3074)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1944  max mem: 15572
Epoch: [20]  [ 640/2809]  eta: 0:20:44  lr: 0.000028  min_lr: 0.000000  loss: 4.2279 (4.2816)  class_acc: 0.2917 (0.2848)  loss_scale: 65536.0000 (40384.8986)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1348  max mem: 15572
Epoch: [20]  [ 650/2809]  eta: 0:20:37  lr: 0.000028  min_lr: 0.000000  loss: 4.2163 (4.2826)  class_acc: 0.2917 (0.2847)  loss_scale: 65536.0000 (40771.2442)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1003  max mem: 15572
Epoch: [20]  [ 660/2809]  eta: 0:20:30  lr: 0.000028  min_lr: 0.000000  loss: 4.5003 (4.2858)  class_acc: 0.2917 (0.2837)  loss_scale: 65536.0000 (41145.9002)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.1023  max mem: 15572
Epoch: [20]  [ 670/2809]  eta: 0:20:23  lr: 0.000028  min_lr: 0.000000  loss: 4.4701 (4.2869)  class_acc: 0.2083 (0.2827)  loss_scale: 65536.0000 (41509.3890)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0933  max mem: 15572
[2025-01-16 00:04:14,362] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:04:14,362] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:04:14,769] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56860
[2025-01-16 00:04:14,770] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:04:14,770] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [ 680/2809]  eta: 0:20:17  lr: 0.000028  min_lr: 0.000000  loss: 4.4041 (4.2866)  class_acc: 0.2500 (0.2830)  loss_scale: 65536.0000 (41958.4376)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0920  max mem: 15572
Epoch: [20]  [ 690/2809]  eta: 0:20:11  lr: 0.000028  min_lr: 0.000000  loss: 4.2773 (4.2861)  class_acc: 0.2500 (0.2827)  loss_scale: 65536.0000 (42299.6469)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1064  max mem: 15572
Epoch: [20]  [ 700/2809]  eta: 0:20:06  lr: 0.000028  min_lr: 0.000000  loss: 4.2348 (4.2860)  class_acc: 0.2083 (0.2818)  loss_scale: 65536.0000 (42631.1213)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1473  max mem: 15572
Epoch: [20]  [ 710/2809]  eta: 0:20:00  lr: 0.000028  min_lr: 0.000000  loss: 4.2708 (4.2867)  class_acc: 0.2500 (0.2826)  loss_scale: 65536.0000 (42953.2714)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1578  max mem: 15572
Epoch: [20]  [ 720/2809]  eta: 0:19:51  lr: 0.000028  min_lr: 0.000000  loss: 4.2837 (4.2856)  class_acc: 0.2500 (0.2821)  loss_scale: 65536.0000 (43266.4854)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0802  max mem: 15572
Epoch: [20]  [ 730/2809]  eta: 0:19:47  lr: 0.000028  min_lr: 0.000000  loss: 4.2664 (4.2861)  class_acc: 0.2083 (0.2817)  loss_scale: 65536.0000 (43571.1300)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1216  max mem: 15572
[2025-01-16 00:04:47,010] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56919
[2025-01-16 00:04:47,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:04:47,010] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 740/2809]  eta: 0:19:38  lr: 0.000028  min_lr: 0.000000  loss: 4.2054 (4.2849)  class_acc: 0.2083 (0.2804)  loss_scale: 65536.0000 (43779.1093)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.1071  max mem: 15572
Epoch: [20]  [ 750/2809]  eta: 0:19:31  lr: 0.000028  min_lr: 0.000000  loss: 4.1619 (4.2844)  class_acc: 0.1667 (0.2798)  loss_scale: 32768.0000 (43632.4900)  weight_decay: 0.0500 (0.0500)  time: 0.4893  data: 0.0534  max mem: 15572
Epoch: [20]  [ 760/2809]  eta: 0:19:26  lr: 0.000028  min_lr: 0.000000  loss: 4.1991 (4.2831)  class_acc: 0.2083 (0.2796)  loss_scale: 32768.0000 (43489.7240)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1297  max mem: 15572
Epoch: [20]  [ 770/2809]  eta: 0:19:18  lr: 0.000028  min_lr: 0.000000  loss: 4.2598 (4.2826)  class_acc: 0.2500 (0.2803)  loss_scale: 32768.0000 (43350.6615)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0887  max mem: 15572
Epoch: [20]  [ 780/2809]  eta: 0:19:12  lr: 0.000028  min_lr: 0.000000  loss: 4.3012 (4.2834)  class_acc: 0.2917 (0.2803)  loss_scale: 32768.0000 (43215.1601)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0525  max mem: 15572
Epoch: [20]  [ 790/2809]  eta: 0:19:07  lr: 0.000028  min_lr: 0.000000  loss: 4.3610 (4.2835)  class_acc: 0.2500 (0.2792)  loss_scale: 32768.0000 (43083.0847)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1231  max mem: 15572
Epoch: [20]  [ 800/2809]  eta: 0:19:00  lr: 0.000028  min_lr: 0.000000  loss: 4.4162 (4.2856)  class_acc: 0.2083 (0.2785)  loss_scale: 32768.0000 (42954.3071)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1309  max mem: 15572
Epoch: [20]  [ 810/2809]  eta: 0:18:54  lr: 0.000028  min_lr: 0.000000  loss: 4.4691 (4.2864)  class_acc: 0.2500 (0.2787)  loss_scale: 32768.0000 (42828.7053)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0961  max mem: 15572
[2025-01-16 00:05:31,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=57000, skipped=356, lr=[2.7198628085713264e-07, 2.7198628085713264e-07, 3.885518297959038e-07, 3.885518297959038e-07, 5.550740425655769e-07, 5.550740425655769e-07, 7.929629179508242e-07, 7.929629179508242e-07, 1.1328041685011774e-06, 1.1328041685011774e-06, 1.6182916692873964e-06, 1.6182916692873964e-06, 2.311845241839138e-06, 2.311845241839138e-06, 3.3026360597701973e-06, 3.3026360597701973e-06, 4.718051513957425e-06, 4.718051513957425e-06, 6.740073591367751e-06, 6.740073591367751e-06, 9.628676559096786e-06, 9.628676559096786e-06, 1.3755252227281124e-05, 1.3755252227281124e-05, 1.9650360324687324e-05, 1.9650360324687324e-05, 2.807194332098189e-05, 2.807194332098189e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:05:31,435] [INFO] [timer.py:260:stop] epoch=0/micro_step=57000/global_step=57000, RunningAvgSamplesPerSec=27.782634744862804, CurrSamplesPerSec=27.536571341347656, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [20]  [ 820/2809]  eta: 0:18:49  lr: 0.000028  min_lr: 0.000000  loss: 4.3027 (4.2850)  class_acc: 0.2917 (0.2789)  loss_scale: 32768.0000 (42706.1632)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1115  max mem: 15572
Epoch: [20]  [ 830/2809]  eta: 0:18:44  lr: 0.000028  min_lr: 0.000000  loss: 4.3027 (4.2846)  class_acc: 0.2083 (0.2780)  loss_scale: 32768.0000 (42586.5704)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1290  max mem: 15572
Epoch: [20]  [ 840/2809]  eta: 0:18:38  lr: 0.000028  min_lr: 0.000000  loss: 4.2479 (4.2843)  class_acc: 0.2917 (0.2785)  loss_scale: 32768.0000 (42469.8216)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1068  max mem: 15572
Epoch: [20]  [ 850/2809]  eta: 0:18:32  lr: 0.000028  min_lr: 0.000000  loss: 4.1427 (4.2828)  class_acc: 0.3333 (0.2797)  loss_scale: 32768.0000 (42355.8167)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0987  max mem: 15572
Epoch: [20]  [ 860/2809]  eta: 0:18:26  lr: 0.000028  min_lr: 0.000000  loss: 4.1427 (4.2823)  class_acc: 0.3750 (0.2803)  loss_scale: 32768.0000 (42244.4599)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1436  max mem: 15572
[2025-01-16 00:05:58,858] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:05:58,858] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 870/2809]  eta: 0:18:20  lr: 0.000028  min_lr: 0.000000  loss: 4.2979 (4.2820)  class_acc: 0.3333 (0.2806)  loss_scale: 32768.0000 (42248.5235)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.1230  max mem: 15572
Epoch: [20]  [ 880/2809]  eta: 0:18:15  lr: 0.000028  min_lr: 0.000000  loss: 4.3366 (4.2830)  class_acc: 0.2500 (0.2806)  loss_scale: 65536.0000 (42512.8536)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1244  max mem: 15572
[2025-01-16 00:06:11,195] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57070
[2025-01-16 00:06:11,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:06:11,196] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 890/2809]  eta: 0:18:09  lr: 0.000028  min_lr: 0.000000  loss: 4.3990 (4.2829)  class_acc: 0.2917 (0.2812)  loss_scale: 65536.0000 (42734.4736)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1376  max mem: 15572
Epoch: [20]  [ 900/2809]  eta: 0:18:02  lr: 0.000028  min_lr: 0.000000  loss: 4.2140 (4.2820)  class_acc: 0.2917 (0.2816)  loss_scale: 32768.0000 (42623.8579)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0894  max mem: 15572
Epoch: [20]  [ 910/2809]  eta: 0:17:58  lr: 0.000028  min_lr: 0.000000  loss: 4.3323 (4.2824)  class_acc: 0.2917 (0.2813)  loss_scale: 32768.0000 (42515.6707)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1311  max mem: 15572
Epoch: [20]  [ 920/2809]  eta: 0:17:53  lr: 0.000028  min_lr: 0.000000  loss: 4.3654 (4.2841)  class_acc: 0.2500 (0.2809)  loss_scale: 32768.0000 (42409.8328)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1875  max mem: 15572
Epoch: [20]  [ 930/2809]  eta: 0:17:47  lr: 0.000028  min_lr: 0.000000  loss: 4.2981 (4.2839)  class_acc: 0.2500 (0.2807)  loss_scale: 32768.0000 (42306.2685)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1269  max mem: 15572
Epoch: [20]  [ 940/2809]  eta: 0:17:41  lr: 0.000028  min_lr: 0.000000  loss: 4.2371 (4.2843)  class_acc: 0.2500 (0.2811)  loss_scale: 32768.0000 (42204.9054)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0745  max mem: 15572
Epoch: [20]  [ 950/2809]  eta: 0:17:35  lr: 0.000028  min_lr: 0.000000  loss: 4.2371 (4.2840)  class_acc: 0.2500 (0.2808)  loss_scale: 32768.0000 (42105.6740)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0938  max mem: 15572
Epoch: [20]  [ 960/2809]  eta: 0:17:29  lr: 0.000028  min_lr: 0.000000  loss: 4.2419 (4.2844)  class_acc: 0.2500 (0.2807)  loss_scale: 32768.0000 (42008.5078)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0964  max mem: 15572
Epoch: [20]  [ 970/2809]  eta: 0:17:23  lr: 0.000028  min_lr: 0.000000  loss: 4.2660 (4.2844)  class_acc: 0.2500 (0.2808)  loss_scale: 32768.0000 (41913.3429)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1257  max mem: 15572
Epoch: [20]  [ 980/2809]  eta: 0:17:17  lr: 0.000028  min_lr: 0.000000  loss: 4.2475 (4.2835)  class_acc: 0.2917 (0.2813)  loss_scale: 32768.0000 (41820.1182)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1395  max mem: 15572
Epoch: [20]  [ 990/2809]  eta: 0:17:10  lr: 0.000028  min_lr: 0.000000  loss: 4.2981 (4.2842)  class_acc: 0.3333 (0.2819)  loss_scale: 32768.0000 (41728.7750)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0704  max mem: 15572
Epoch: [20]  [1000/2809]  eta: 0:17:05  lr: 0.000028  min_lr: 0.000000  loss: 4.3026 (4.2838)  class_acc: 0.2917 (0.2816)  loss_scale: 32768.0000 (41639.2567)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0810  max mem: 15572
Epoch: [20]  [1010/2809]  eta: 0:16:57  lr: 0.000028  min_lr: 0.000000  loss: 4.2645 (4.2843)  class_acc: 0.2500 (0.2818)  loss_scale: 32768.0000 (41551.5094)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0858  max mem: 15572
[2025-01-16 00:07:22,574] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:07:22,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1020/2809]  eta: 0:16:51  lr: 0.000028  min_lr: 0.000000  loss: 4.4301 (4.2852)  class_acc: 0.2500 (0.2818)  loss_scale: 32768.0000 (41529.6690)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0663  max mem: 15572
Epoch: [20]  [1030/2809]  eta: 0:16:47  lr: 0.000028  min_lr: 0.000000  loss: 4.2511 (4.2836)  class_acc: 0.2083 (0.2813)  loss_scale: 65536.0000 (41762.5141)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1537  max mem: 15572
Epoch: [20]  [1040/2809]  eta: 0:16:40  lr: 0.000028  min_lr: 0.000000  loss: 4.1256 (4.2827)  class_acc: 0.2083 (0.2811)  loss_scale: 65536.0000 (41990.8857)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1321  max mem: 15572
[2025-01-16 00:07:37,295] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57226
[2025-01-16 00:07:37,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:07:37,295] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1050/2809]  eta: 0:16:34  lr: 0.000028  min_lr: 0.000000  loss: 4.2562 (4.2824)  class_acc: 0.2500 (0.2814)  loss_scale: 65536.0000 (42059.0219)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0542  max mem: 15572
Epoch: [20]  [1060/2809]  eta: 0:16:29  lr: 0.000028  min_lr: 0.000000  loss: 4.0820 (4.2800)  class_acc: 0.2917 (0.2819)  loss_scale: 32768.0000 (41971.4533)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1135  max mem: 15572
Epoch: [20]  [1070/2809]  eta: 0:16:24  lr: 0.000028  min_lr: 0.000000  loss: 4.2646 (4.2795)  class_acc: 0.2500 (0.2818)  loss_scale: 32768.0000 (41885.5201)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1691  max mem: 15572
Epoch: [20]  [1080/2809]  eta: 0:16:18  lr: 0.000028  min_lr: 0.000000  loss: 4.3934 (4.2810)  class_acc: 0.2500 (0.2815)  loss_scale: 32768.0000 (41801.1767)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1432  max mem: 15572
Epoch: [20]  [1090/2809]  eta: 0:16:14  lr: 0.000028  min_lr: 0.000000  loss: 4.3476 (4.2806)  class_acc: 0.2917 (0.2816)  loss_scale: 32768.0000 (41718.3795)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1792  max mem: 15572
Epoch: [20]  [1100/2809]  eta: 0:16:09  lr: 0.000028  min_lr: 0.000000  loss: 4.1865 (4.2800)  class_acc: 0.2917 (0.2820)  loss_scale: 32768.0000 (41637.0863)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.1914  max mem: 15572
Epoch: [20]  [1110/2809]  eta: 0:16:03  lr: 0.000028  min_lr: 0.000000  loss: 4.1724 (4.2798)  class_acc: 0.2917 (0.2820)  loss_scale: 32768.0000 (41557.2565)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0975  max mem: 15572
Epoch: [20]  [1120/2809]  eta: 0:15:57  lr: 0.000028  min_lr: 0.000000  loss: 4.2806 (4.2803)  class_acc: 0.2917 (0.2820)  loss_scale: 32768.0000 (41478.8510)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0822  max mem: 15572
Epoch: [20]  [1130/2809]  eta: 0:15:50  lr: 0.000028  min_lr: 0.000000  loss: 4.2004 (4.2798)  class_acc: 0.2917 (0.2820)  loss_scale: 32768.0000 (41401.8320)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0804  max mem: 15572
Epoch: [20]  [1140/2809]  eta: 0:15:44  lr: 0.000028  min_lr: 0.000000  loss: 4.2004 (4.2809)  class_acc: 0.2917 (0.2818)  loss_scale: 32768.0000 (41326.1630)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0631  max mem: 15572
Epoch: [20]  [1150/2809]  eta: 0:15:39  lr: 0.000028  min_lr: 0.000000  loss: 4.2975 (4.2800)  class_acc: 0.2917 (0.2818)  loss_scale: 32768.0000 (41251.8089)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1269  max mem: 15572
Epoch: [20]  [1160/2809]  eta: 0:15:33  lr: 0.000028  min_lr: 0.000000  loss: 4.3102 (4.2814)  class_acc: 0.2500 (0.2814)  loss_scale: 32768.0000 (41178.7356)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1289  max mem: 15572
Epoch: [20]  [1170/2809]  eta: 0:15:28  lr: 0.000028  min_lr: 0.000000  loss: 4.3634 (4.2809)  class_acc: 0.2500 (0.2812)  loss_scale: 32768.0000 (41106.9103)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1145  max mem: 15572
[2025-01-16 00:08:51,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:08:51,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1180/2809]  eta: 0:15:22  lr: 0.000028  min_lr: 0.000000  loss: 4.1844 (4.2806)  class_acc: 0.2917 (0.2817)  loss_scale: 32768.0000 (41202.7773)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1208  max mem: 15572
Epoch: [20]  [1190/2809]  eta: 0:15:17  lr: 0.000028  min_lr: 0.000000  loss: 4.2124 (4.2804)  class_acc: 0.3333 (0.2820)  loss_scale: 65536.0000 (41407.0865)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1154  max mem: 15572
Epoch: [20]  [1200/2809]  eta: 0:15:11  lr: 0.000028  min_lr: 0.000000  loss: 4.2397 (4.2808)  class_acc: 0.2917 (0.2817)  loss_scale: 65536.0000 (41607.9933)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1115  max mem: 15572
[2025-01-16 00:09:10,678] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57390
[2025-01-16 00:09:10,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:09:10,678] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1210/2809]  eta: 0:15:04  lr: 0.000028  min_lr: 0.000000  loss: 4.2610 (4.2814)  class_acc: 0.2500 (0.2821)  loss_scale: 65536.0000 (41778.5235)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0616  max mem: 15572
Epoch: [20]  [1220/2809]  eta: 0:14:58  lr: 0.000028  min_lr: 0.000000  loss: 4.2632 (4.2806)  class_acc: 0.3750 (0.2828)  loss_scale: 32768.0000 (41704.7273)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0625  max mem: 15572
Epoch: [20]  [1230/2809]  eta: 0:14:53  lr: 0.000028  min_lr: 0.000000  loss: 4.2107 (4.2804)  class_acc: 0.3750 (0.2824)  loss_scale: 32768.0000 (41632.1300)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1313  max mem: 15572
Epoch: [20]  [1240/2809]  eta: 0:14:47  lr: 0.000028  min_lr: 0.000000  loss: 4.1983 (4.2805)  class_acc: 0.2500 (0.2825)  loss_scale: 32768.0000 (41560.7027)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1281  max mem: 15572
Epoch: [20]  [1250/2809]  eta: 0:14:41  lr: 0.000028  min_lr: 0.000000  loss: 4.3016 (4.2806)  class_acc: 0.2500 (0.2823)  loss_scale: 32768.0000 (41490.4173)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0905  max mem: 15572
Epoch: [20]  [1260/2809]  eta: 0:14:35  lr: 0.000028  min_lr: 0.000000  loss: 4.3241 (4.2807)  class_acc: 0.2917 (0.2826)  loss_scale: 32768.0000 (41421.2466)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0903  max mem: 15572
Epoch: [20]  [1270/2809]  eta: 0:14:30  lr: 0.000028  min_lr: 0.000000  loss: 4.4049 (4.2811)  class_acc: 0.2917 (0.2824)  loss_scale: 32768.0000 (41353.1644)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1378  max mem: 15572
Epoch: [20]  [1280/2809]  eta: 0:14:23  lr: 0.000028  min_lr: 0.000000  loss: 4.2761 (4.2805)  class_acc: 0.2917 (0.2826)  loss_scale: 32768.0000 (41286.1452)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1180  max mem: 15572
Epoch: [20]  [1290/2809]  eta: 0:14:18  lr: 0.000028  min_lr: 0.000000  loss: 4.1472 (4.2796)  class_acc: 0.3333 (0.2830)  loss_scale: 32768.0000 (41220.1642)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0872  max mem: 15572
Epoch: [20]  [1300/2809]  eta: 0:14:12  lr: 0.000028  min_lr: 0.000000  loss: 4.1764 (4.2796)  class_acc: 0.3333 (0.2831)  loss_scale: 32768.0000 (41155.1975)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1087  max mem: 15572
Epoch: [20]  [1310/2809]  eta: 0:14:07  lr: 0.000028  min_lr: 0.000000  loss: 4.2962 (4.2794)  class_acc: 0.2917 (0.2832)  loss_scale: 32768.0000 (41091.2220)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1174  max mem: 15572
Epoch: [20]  [1320/2809]  eta: 0:14:03  lr: 0.000028  min_lr: 0.000000  loss: 4.0573 (4.2785)  class_acc: 0.2917 (0.2832)  loss_scale: 32768.0000 (41028.2150)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.1961  max mem: 15572
Epoch: [20]  [1330/2809]  eta: 0:13:57  lr: 0.000028  min_lr: 0.000000  loss: 4.0573 (4.2787)  class_acc: 0.2083 (0.2829)  loss_scale: 32768.0000 (40966.1548)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1704  max mem: 15572
[2025-01-16 00:10:24,608] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:10:24,608] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1340/2809]  eta: 0:13:51  lr: 0.000028  min_lr: 0.000000  loss: 4.4075 (4.2796)  class_acc: 0.2500 (0.2827)  loss_scale: 32768.0000 (40953.8911)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1231  max mem: 15572
Epoch: [20]  [1350/2809]  eta: 0:13:46  lr: 0.000028  min_lr: 0.000000  loss: 4.4075 (4.2793)  class_acc: 0.2500 (0.2827)  loss_scale: 65536.0000 (41135.8460)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1601  max mem: 15572
Epoch: [20]  [1360/2809]  eta: 0:13:41  lr: 0.000028  min_lr: 0.000000  loss: 4.2538 (4.2789)  class_acc: 0.2500 (0.2824)  loss_scale: 65536.0000 (41315.1271)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1761  max mem: 15572
Epoch: [20]  [1370/2809]  eta: 0:13:35  lr: 0.000028  min_lr: 0.000000  loss: 4.3030 (4.2785)  class_acc: 0.2500 (0.2824)  loss_scale: 65536.0000 (41491.7929)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1074  max mem: 15572
[2025-01-16 00:10:44,192] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57552
[2025-01-16 00:10:44,192] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:10:44,192] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1380/2809]  eta: 0:13:29  lr: 0.000028  min_lr: 0.000000  loss: 4.3064 (4.2789)  class_acc: 0.2500 (0.2821)  loss_scale: 65536.0000 (41452.3505)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0617  max mem: 15572
Epoch: [20]  [1390/2809]  eta: 0:13:23  lr: 0.000028  min_lr: 0.000000  loss: 4.3075 (4.2794)  class_acc: 0.2083 (0.2822)  loss_scale: 32768.0000 (41389.9180)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.1039  max mem: 15572
Epoch: [20]  [1400/2809]  eta: 0:13:17  lr: 0.000028  min_lr: 0.000000  loss: 4.3689 (4.2795)  class_acc: 0.2917 (0.2823)  loss_scale: 32768.0000 (41328.3769)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1116  max mem: 15572
Epoch: [20]  [1410/2809]  eta: 0:13:12  lr: 0.000028  min_lr: 0.000000  loss: 4.4010 (4.2807)  class_acc: 0.2917 (0.2822)  loss_scale: 32768.0000 (41267.7080)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1207  max mem: 15572
Epoch: [20]  [1420/2809]  eta: 0:13:06  lr: 0.000028  min_lr: 0.000000  loss: 4.3899 (4.2815)  class_acc: 0.2500 (0.2819)  loss_scale: 32768.0000 (41207.8930)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1143  max mem: 15572
Epoch: [20]  [1430/2809]  eta: 0:13:01  lr: 0.000028  min_lr: 0.000000  loss: 4.3232 (4.2815)  class_acc: 0.2917 (0.2823)  loss_scale: 32768.0000 (41148.9140)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1683  max mem: 15572
Epoch: [20]  [1440/2809]  eta: 0:12:55  lr: 0.000028  min_lr: 0.000000  loss: 4.2875 (4.2816)  class_acc: 0.2917 (0.2823)  loss_scale: 32768.0000 (41090.7536)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1616  max mem: 15572
Epoch: [20]  [1450/2809]  eta: 0:12:50  lr: 0.000028  min_lr: 0.000000  loss: 4.3009 (4.2816)  class_acc: 0.2917 (0.2824)  loss_scale: 32768.0000 (41033.3949)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1234  max mem: 15572
Epoch: [20]  [1460/2809]  eta: 0:12:44  lr: 0.000028  min_lr: 0.000000  loss: 4.2767 (4.2819)  class_acc: 0.2917 (0.2824)  loss_scale: 32768.0000 (40976.8214)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1181  max mem: 15572
Epoch: [20]  [1470/2809]  eta: 0:12:38  lr: 0.000028  min_lr: 0.000000  loss: 4.3080 (4.2825)  class_acc: 0.2917 (0.2824)  loss_scale: 32768.0000 (40921.0170)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0727  max mem: 15572
Epoch: [20]  [1480/2809]  eta: 0:12:33  lr: 0.000028  min_lr: 0.000000  loss: 4.2239 (4.2815)  class_acc: 0.2917 (0.2820)  loss_scale: 32768.0000 (40865.9662)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1073  max mem: 15572
Epoch: [20]  [1490/2809]  eta: 0:12:28  lr: 0.000028  min_lr: 0.000000  loss: 4.2239 (4.2817)  class_acc: 0.2083 (0.2819)  loss_scale: 32768.0000 (40811.6539)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.1864  max mem: 15572
Epoch: [20]  [1500/2809]  eta: 0:12:22  lr: 0.000028  min_lr: 0.000000  loss: 4.3597 (4.2818)  class_acc: 0.2917 (0.2818)  loss_scale: 32768.0000 (40758.0653)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1592  max mem: 15572
[2025-01-16 00:11:58,702] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:11:58,703] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1510/2809]  eta: 0:12:16  lr: 0.000028  min_lr: 0.000000  loss: 4.3776 (4.2824)  class_acc: 0.2917 (0.2815)  loss_scale: 32768.0000 (40922.0490)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1161  max mem: 15572
Epoch: [20]  [1520/2809]  eta: 0:12:11  lr: 0.000028  min_lr: 0.000000  loss: 4.3995 (4.2832)  class_acc: 0.2917 (0.2818)  loss_scale: 65536.0000 (41083.8764)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1168  max mem: 15572
Epoch: [20]  [1530/2809]  eta: 0:12:05  lr: 0.000028  min_lr: 0.000000  loss: 4.1476 (4.2828)  class_acc: 0.3333 (0.2824)  loss_scale: 65536.0000 (41243.5898)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0897  max mem: 15572
[2025-01-16 00:12:18,436] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57717
[2025-01-16 00:12:18,437] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:12:18,437] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1540/2809]  eta: 0:11:59  lr: 0.000028  min_lr: 0.000000  loss: 4.1377 (4.2822)  class_acc: 0.3333 (0.2827)  loss_scale: 65536.0000 (41316.1739)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1236  max mem: 15572
Epoch: [20]  [1550/2809]  eta: 0:11:53  lr: 0.000028  min_lr: 0.000000  loss: 4.2439 (4.2822)  class_acc: 0.2917 (0.2828)  loss_scale: 32768.0000 (41261.0600)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1339  max mem: 15572
Epoch: [20]  [1560/2809]  eta: 0:11:47  lr: 0.000028  min_lr: 0.000000  loss: 4.2439 (4.2813)  class_acc: 0.2917 (0.2833)  loss_scale: 32768.0000 (41206.6521)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.0908  max mem: 15572
Epoch: [20]  [1570/2809]  eta: 0:11:42  lr: 0.000028  min_lr: 0.000000  loss: 4.2955 (4.2815)  class_acc: 0.2917 (0.2833)  loss_scale: 32768.0000 (41152.9370)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1175  max mem: 15572
Epoch: [20]  [1580/2809]  eta: 0:11:36  lr: 0.000028  min_lr: 0.000000  loss: 4.3397 (4.2816)  class_acc: 0.3333 (0.2837)  loss_scale: 32768.0000 (41099.9013)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1312  max mem: 15572
Epoch: [20]  [1590/2809]  eta: 0:11:31  lr: 0.000028  min_lr: 0.000000  loss: 4.3030 (4.2820)  class_acc: 0.3333 (0.2835)  loss_scale: 32768.0000 (41047.5324)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1301  max mem: 15572
Epoch: [20]  [1600/2809]  eta: 0:11:25  lr: 0.000027  min_lr: 0.000000  loss: 4.2810 (4.2821)  class_acc: 0.2917 (0.2840)  loss_scale: 32768.0000 (40995.8176)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1266  max mem: 15572
Epoch: [20]  [1610/2809]  eta: 0:11:19  lr: 0.000027  min_lr: 0.000000  loss: 4.3416 (4.2828)  class_acc: 0.2500 (0.2837)  loss_scale: 32768.0000 (40944.7449)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0520  max mem: 15572
Epoch: [20]  [1620/2809]  eta: 0:11:13  lr: 0.000027  min_lr: 0.000000  loss: 4.3813 (4.2830)  class_acc: 0.2500 (0.2836)  loss_scale: 32768.0000 (40894.3023)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0624  max mem: 15572
Epoch: [20]  [1630/2809]  eta: 0:11:08  lr: 0.000027  min_lr: 0.000000  loss: 4.2539 (4.2832)  class_acc: 0.2917 (0.2837)  loss_scale: 32768.0000 (40844.4782)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1576  max mem: 15572
Epoch: [20]  [1640/2809]  eta: 0:11:02  lr: 0.000027  min_lr: 0.000000  loss: 4.1709 (4.2828)  class_acc: 0.2917 (0.2837)  loss_scale: 32768.0000 (40795.2614)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1556  max mem: 15572
Epoch: [20]  [1650/2809]  eta: 0:10:57  lr: 0.000027  min_lr: 0.000000  loss: 4.1357 (4.2819)  class_acc: 0.2500 (0.2834)  loss_scale: 32768.0000 (40746.6408)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1194  max mem: 15572
Epoch: [20]  [1660/2809]  eta: 0:10:51  lr: 0.000027  min_lr: 0.000000  loss: 4.2498 (4.2818)  class_acc: 0.2917 (0.2837)  loss_scale: 32768.0000 (40698.6057)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0823  max mem: 15572
[2025-01-16 00:13:30,645] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:13:30,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1670/2809]  eta: 0:10:45  lr: 0.000027  min_lr: 0.000000  loss: 4.2774 (4.2817)  class_acc: 0.2917 (0.2834)  loss_scale: 32768.0000 (40749.1945)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0992  max mem: 15572
Epoch: [20]  [1680/2809]  eta: 0:10:40  lr: 0.000027  min_lr: 0.000000  loss: 4.2234 (4.2808)  class_acc: 0.2917 (0.2837)  loss_scale: 65536.0000 (40896.6472)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1824  max mem: 15572
[2025-01-16 00:13:41,460] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57864
[2025-01-16 00:13:41,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:13:41,460] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1690/2809]  eta: 0:10:35  lr: 0.000027  min_lr: 0.000000  loss: 4.2323 (4.2800)  class_acc: 0.2917 (0.2839)  loss_scale: 65536.0000 (40906.7108)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1632  max mem: 15572
Epoch: [20]  [1700/2809]  eta: 0:10:28  lr: 0.000027  min_lr: 0.000000  loss: 4.2322 (4.2782)  class_acc: 0.3333 (0.2842)  loss_scale: 32768.0000 (40858.8642)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0891  max mem: 15572
Epoch: [20]  [1710/2809]  eta: 0:10:23  lr: 0.000027  min_lr: 0.000000  loss: 4.1296 (4.2779)  class_acc: 0.3333 (0.2844)  loss_scale: 32768.0000 (40811.5769)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.1303  max mem: 15572
Epoch: [20]  [1720/2809]  eta: 0:10:17  lr: 0.000027  min_lr: 0.000000  loss: 4.1633 (4.2774)  class_acc: 0.2917 (0.2842)  loss_scale: 32768.0000 (40764.8390)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1369  max mem: 15572
Epoch: [20]  [1730/2809]  eta: 0:10:11  lr: 0.000027  min_lr: 0.000000  loss: 4.2350 (4.2773)  class_acc: 0.2917 (0.2842)  loss_scale: 32768.0000 (40718.6412)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1168  max mem: 15572
Epoch: [20]  [1740/2809]  eta: 0:10:05  lr: 0.000027  min_lr: 0.000000  loss: 4.2372 (4.2776)  class_acc: 0.3333 (0.2844)  loss_scale: 32768.0000 (40672.9742)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0843  max mem: 15572
Epoch: [20]  [1750/2809]  eta: 0:09:59  lr: 0.000027  min_lr: 0.000000  loss: 4.2372 (4.2773)  class_acc: 0.3333 (0.2847)  loss_scale: 32768.0000 (40627.8287)  weight_decay: 0.0500 (0.0500)  time: 0.4883  data: 0.0317  max mem: 15572
Epoch: [20]  [1760/2809]  eta: 0:09:54  lr: 0.000027  min_lr: 0.000000  loss: 4.2423 (4.2770)  class_acc: 0.2917 (0.2848)  loss_scale: 32768.0000 (40583.1959)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.1048  max mem: 15572
Epoch: [20]  [1770/2809]  eta: 0:09:48  lr: 0.000027  min_lr: 0.000000  loss: 4.3533 (4.2777)  class_acc: 0.2500 (0.2846)  loss_scale: 32768.0000 (40539.0672)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1369  max mem: 15572
Epoch: [20]  [1780/2809]  eta: 0:09:42  lr: 0.000027  min_lr: 0.000000  loss: 4.3683 (4.2780)  class_acc: 0.2500 (0.2843)  loss_scale: 32768.0000 (40495.4340)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0982  max mem: 15572
Epoch: [20]  [1790/2809]  eta: 0:09:36  lr: 0.000027  min_lr: 0.000000  loss: 4.3346 (4.2786)  class_acc: 0.2500 (0.2839)  loss_scale: 32768.0000 (40452.2881)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0724  max mem: 15572
Epoch: [20]  [1800/2809]  eta: 0:09:31  lr: 0.000027  min_lr: 0.000000  loss: 4.4541 (4.2790)  class_acc: 0.2500 (0.2838)  loss_scale: 32768.0000 (40409.6213)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0859  max mem: 15572
Epoch: [20]  [1810/2809]  eta: 0:09:25  lr: 0.000027  min_lr: 0.000000  loss: 4.2148 (4.2784)  class_acc: 0.2500 (0.2837)  loss_scale: 32768.0000 (40367.4257)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1191  max mem: 15572
[2025-01-16 00:14:52,253] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:14:52,253] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 00:14:55,370] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57999
[2025-01-16 00:14:55,370] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:14:55,370] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 00:14:55,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=58000, skipped=363, lr=[2.648575876106438e-07, 2.648575876106438e-07, 3.783679823009198e-07, 3.783679823009198e-07, 5.40525689001314e-07, 5.40525689001314e-07, 7.721795557161629e-07, 7.721795557161629e-07, 1.1031136510230898e-06, 1.1031136510230898e-06, 1.5758766443187e-06, 1.5758766443187e-06, 2.2512523490267144e-06, 2.2512523490267144e-06, 3.216074784323878e-06, 3.216074784323878e-06, 4.594392549034111e-06, 4.594392549034111e-06, 6.563417927191589e-06, 6.563417927191589e-06, 9.376311324559412e-06, 9.376311324559412e-06, 1.3394730463656304e-05, 1.3394730463656304e-05, 1.9135329233794723e-05, 1.9135329233794723e-05, 2.7336184619706747e-05, 2.7336184619706747e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:14:55,371] [INFO] [timer.py:260:stop] epoch=0/micro_step=58000/global_step=58000, RunningAvgSamplesPerSec=27.792867045985844, CurrSamplesPerSec=31.72921923964579, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [20]  [1820/2809]  eta: 0:09:19  lr: 0.000027  min_lr: 0.000000  loss: 4.1879 (4.2776)  class_acc: 0.2917 (0.2840)  loss_scale: 32768.0000 (40433.6606)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0976  max mem: 15572
Epoch: [20]  [1830/2809]  eta: 0:09:13  lr: 0.000027  min_lr: 0.000000  loss: 4.2234 (4.2780)  class_acc: 0.2917 (0.2839)  loss_scale: 32768.0000 (40391.7946)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0622  max mem: 15572
Epoch: [20]  [1840/2809]  eta: 0:09:07  lr: 0.000027  min_lr: 0.000000  loss: 4.3378 (4.2784)  class_acc: 0.2500 (0.2839)  loss_scale: 32768.0000 (40350.3835)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0932  max mem: 15572
Epoch: [20]  [1850/2809]  eta: 0:09:02  lr: 0.000027  min_lr: 0.000000  loss: 4.3368 (4.2782)  class_acc: 0.3333 (0.2842)  loss_scale: 32768.0000 (40309.4198)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0998  max mem: 15572
Epoch: [20]  [1860/2809]  eta: 0:08:57  lr: 0.000027  min_lr: 0.000000  loss: 4.2886 (4.2784)  class_acc: 0.2917 (0.2841)  loss_scale: 32768.0000 (40268.8963)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.1845  max mem: 15572
Epoch: [20]  [1870/2809]  eta: 0:08:51  lr: 0.000027  min_lr: 0.000000  loss: 4.3277 (4.2786)  class_acc: 0.2500 (0.2837)  loss_scale: 32768.0000 (40228.8060)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.1883  max mem: 15572
Epoch: [20]  [1880/2809]  eta: 0:08:45  lr: 0.000027  min_lr: 0.000000  loss: 4.2902 (4.2786)  class_acc: 0.2500 (0.2838)  loss_scale: 32768.0000 (40189.1419)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0949  max mem: 15572
Epoch: [20]  [1890/2809]  eta: 0:08:39  lr: 0.000027  min_lr: 0.000000  loss: 4.2421 (4.2791)  class_acc: 0.2917 (0.2836)  loss_scale: 32768.0000 (40149.8974)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.1047  max mem: 15572
Epoch: [20]  [1900/2809]  eta: 0:08:34  lr: 0.000027  min_lr: 0.000000  loss: 4.3394 (4.2790)  class_acc: 0.2917 (0.2838)  loss_scale: 32768.0000 (40111.0658)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0972  max mem: 15572
Epoch: [20]  [1910/2809]  eta: 0:08:28  lr: 0.000027  min_lr: 0.000000  loss: 4.1846 (4.2778)  class_acc: 0.2917 (0.2836)  loss_scale: 32768.0000 (40072.6405)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.1076  max mem: 15572
Epoch: [20]  [1920/2809]  eta: 0:08:23  lr: 0.000027  min_lr: 0.000000  loss: 4.3254 (4.2789)  class_acc: 0.1667 (0.2833)  loss_scale: 32768.0000 (40034.6153)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1663  max mem: 15572
Epoch: [20]  [1930/2809]  eta: 0:08:17  lr: 0.000027  min_lr: 0.000000  loss: 4.4399 (4.2799)  class_acc: 0.1667 (0.2830)  loss_scale: 32768.0000 (39996.9839)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1526  max mem: 15572
Epoch: [20]  [1940/2809]  eta: 0:08:11  lr: 0.000027  min_lr: 0.000000  loss: 4.3698 (4.2796)  class_acc: 0.2500 (0.2829)  loss_scale: 32768.0000 (39959.7403)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1174  max mem: 15572
[2025-01-16 00:16:08,270] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:16:08,270] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1950/2809]  eta: 0:08:06  lr: 0.000027  min_lr: 0.000000  loss: 4.2976 (4.2800)  class_acc: 0.2083 (0.2826)  loss_scale: 32768.0000 (39973.2650)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1284  max mem: 15572
Epoch: [20]  [1960/2809]  eta: 0:08:00  lr: 0.000027  min_lr: 0.000000  loss: 4.2158 (4.2797)  class_acc: 0.2500 (0.2828)  loss_scale: 65536.0000 (40103.6206)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1426  max mem: 15572
Epoch: [20]  [1970/2809]  eta: 0:07:54  lr: 0.000027  min_lr: 0.000000  loss: 4.2141 (4.2792)  class_acc: 0.2917 (0.2829)  loss_scale: 65536.0000 (40232.6535)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1120  max mem: 15572
[2025-01-16 00:16:24,183] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58156
[2025-01-16 00:16:24,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:16:24,184] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1980/2809]  eta: 0:07:48  lr: 0.000027  min_lr: 0.000000  loss: 4.2212 (4.2789)  class_acc: 0.2083 (0.2826)  loss_scale: 65536.0000 (40277.6779)  weight_decay: 0.0500 (0.0500)  time: 0.4921  data: 0.0431  max mem: 15572
Epoch: [20]  [1990/2809]  eta: 0:07:43  lr: 0.000027  min_lr: 0.000000  loss: 4.1822 (4.2788)  class_acc: 0.2083 (0.2826)  loss_scale: 32768.0000 (40239.9598)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1019  max mem: 15572
Epoch: [20]  [2000/2809]  eta: 0:07:37  lr: 0.000027  min_lr: 0.000000  loss: 4.1808 (4.2780)  class_acc: 0.2500 (0.2827)  loss_scale: 32768.0000 (40202.6187)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1591  max mem: 15572
Epoch: [20]  [2010/2809]  eta: 0:07:31  lr: 0.000027  min_lr: 0.000000  loss: 4.2295 (4.2785)  class_acc: 0.2083 (0.2826)  loss_scale: 32768.0000 (40165.6489)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1149  max mem: 15572
Epoch: [20]  [2020/2809]  eta: 0:07:26  lr: 0.000027  min_lr: 0.000000  loss: 4.3430 (4.2787)  class_acc: 0.2083 (0.2825)  loss_scale: 32768.0000 (40129.0450)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1135  max mem: 15572
Epoch: [20]  [2030/2809]  eta: 0:07:20  lr: 0.000027  min_lr: 0.000000  loss: 4.2290 (4.2781)  class_acc: 0.2500 (0.2825)  loss_scale: 32768.0000 (40092.8016)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1325  max mem: 15572
Epoch: [20]  [2040/2809]  eta: 0:07:14  lr: 0.000027  min_lr: 0.000000  loss: 4.0813 (4.2776)  class_acc: 0.2917 (0.2824)  loss_scale: 32768.0000 (40056.9133)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0956  max mem: 15572
Epoch: [20]  [2050/2809]  eta: 0:07:09  lr: 0.000027  min_lr: 0.000000  loss: 4.1926 (4.2775)  class_acc: 0.2917 (0.2824)  loss_scale: 32768.0000 (40021.3749)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1193  max mem: 15572
Epoch: [20]  [2060/2809]  eta: 0:07:03  lr: 0.000027  min_lr: 0.000000  loss: 4.2705 (4.2776)  class_acc: 0.2917 (0.2824)  loss_scale: 32768.0000 (39986.1815)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1619  max mem: 15572
Epoch: [20]  [2070/2809]  eta: 0:06:58  lr: 0.000027  min_lr: 0.000000  loss: 4.1586 (4.2768)  class_acc: 0.2917 (0.2827)  loss_scale: 32768.0000 (39951.3279)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1797  max mem: 15572
Epoch: [20]  [2080/2809]  eta: 0:06:53  lr: 0.000027  min_lr: 0.000000  loss: 4.1656 (4.2767)  class_acc: 0.2917 (0.2826)  loss_scale: 32768.0000 (39916.8092)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.2187  max mem: 15572
Epoch: [20]  [2090/2809]  eta: 0:06:47  lr: 0.000027  min_lr: 0.000000  loss: 4.2243 (4.2765)  class_acc: 0.2500 (0.2827)  loss_scale: 32768.0000 (39882.6208)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1657  max mem: 15572
Epoch: [20]  [2100/2809]  eta: 0:06:41  lr: 0.000027  min_lr: 0.000000  loss: 4.3316 (4.2770)  class_acc: 0.2083 (0.2826)  loss_scale: 32768.0000 (39848.7577)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1120  max mem: 15572
[2025-01-16 00:17:38,659] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:17:38,659] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [2110/2809]  eta: 0:06:36  lr: 0.000027  min_lr: 0.000000  loss: 4.1369 (4.2762)  class_acc: 0.3750 (0.2831)  loss_scale: 32768.0000 (39908.3505)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1192  max mem: 15572
Epoch: [20]  [2120/2809]  eta: 0:06:30  lr: 0.000027  min_lr: 0.000000  loss: 4.2585 (4.2766)  class_acc: 0.2917 (0.2829)  loss_scale: 65536.0000 (40029.1787)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1182  max mem: 15572
[2025-01-16 00:17:51,924] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58308
[2025-01-16 00:17:51,924] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:17:51,924] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [2130/2809]  eta: 0:06:24  lr: 0.000027  min_lr: 0.000000  loss: 4.3045 (4.2762)  class_acc: 0.2500 (0.2828)  loss_scale: 65536.0000 (40102.7424)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0855  max mem: 15572
Epoch: [20]  [2140/2809]  eta: 0:06:18  lr: 0.000027  min_lr: 0.000000  loss: 4.2735 (4.2768)  class_acc: 0.2500 (0.2826)  loss_scale: 32768.0000 (40068.4839)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0748  max mem: 15572
Epoch: [20]  [2150/2809]  eta: 0:06:13  lr: 0.000027  min_lr: 0.000000  loss: 4.3494 (4.2765)  class_acc: 0.2500 (0.2827)  loss_scale: 32768.0000 (40034.5439)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1077  max mem: 15572
Epoch: [20]  [2160/2809]  eta: 0:06:07  lr: 0.000027  min_lr: 0.000000  loss: 4.2348 (4.2763)  class_acc: 0.3333 (0.2829)  loss_scale: 32768.0000 (40000.9181)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1423  max mem: 15572
Epoch: [20]  [2170/2809]  eta: 0:06:02  lr: 0.000027  min_lr: 0.000000  loss: 4.2647 (4.2764)  class_acc: 0.3333 (0.2829)  loss_scale: 32768.0000 (39967.6020)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1554  max mem: 15572
Epoch: [20]  [2180/2809]  eta: 0:05:56  lr: 0.000027  min_lr: 0.000000  loss: 4.2647 (4.2761)  class_acc: 0.2500 (0.2832)  loss_scale: 32768.0000 (39934.5915)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1505  max mem: 15572
Epoch: [20]  [2190/2809]  eta: 0:05:51  lr: 0.000027  min_lr: 0.000000  loss: 4.2153 (4.2764)  class_acc: 0.2917 (0.2832)  loss_scale: 32768.0000 (39901.8822)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1581  max mem: 15572
Epoch: [20]  [2200/2809]  eta: 0:05:45  lr: 0.000027  min_lr: 0.000000  loss: 4.2153 (4.2765)  class_acc: 0.2917 (0.2833)  loss_scale: 32768.0000 (39869.4702)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1404  max mem: 15572
Epoch: [20]  [2210/2809]  eta: 0:05:39  lr: 0.000027  min_lr: 0.000000  loss: 4.2092 (4.2760)  class_acc: 0.2917 (0.2832)  loss_scale: 32768.0000 (39837.3514)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0945  max mem: 15572
Epoch: [20]  [2220/2809]  eta: 0:05:33  lr: 0.000027  min_lr: 0.000000  loss: 4.2078 (4.2757)  class_acc: 0.2917 (0.2834)  loss_scale: 32768.0000 (39805.5218)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0876  max mem: 15572
Epoch: [20]  [2230/2809]  eta: 0:05:27  lr: 0.000027  min_lr: 0.000000  loss: 4.1492 (4.2760)  class_acc: 0.3333 (0.2837)  loss_scale: 32768.0000 (39773.9776)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0565  max mem: 15572
Epoch: [20]  [2240/2809]  eta: 0:05:22  lr: 0.000027  min_lr: 0.000000  loss: 4.2943 (4.2760)  class_acc: 0.3333 (0.2839)  loss_scale: 32768.0000 (39742.7149)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0814  max mem: 15572
Epoch: [20]  [2250/2809]  eta: 0:05:16  lr: 0.000027  min_lr: 0.000000  loss: 4.3354 (4.2765)  class_acc: 0.3333 (0.2841)  loss_scale: 32768.0000 (39711.7299)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1369  max mem: 15572
[2025-01-16 00:19:04,442] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:19:04,442] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [2260/2809]  eta: 0:05:10  lr: 0.000027  min_lr: 0.000000  loss: 4.3354 (4.2760)  class_acc: 0.2917 (0.2839)  loss_scale: 32768.0000 (39738.9898)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0877  max mem: 15572
Epoch: [20]  [2270/2809]  eta: 0:05:05  lr: 0.000027  min_lr: 0.000000  loss: 4.2385 (4.2759)  class_acc: 0.2917 (0.2842)  loss_scale: 65536.0000 (39852.5830)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0838  max mem: 15572
Epoch: [20]  [2280/2809]  eta: 0:04:59  lr: 0.000027  min_lr: 0.000000  loss: 4.3076 (4.2759)  class_acc: 0.2917 (0.2843)  loss_scale: 65536.0000 (39965.1802)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1110  max mem: 15572
Epoch: [20]  [2290/2809]  eta: 0:04:53  lr: 0.000027  min_lr: 0.000000  loss: 4.2698 (4.2759)  class_acc: 0.2083 (0.2841)  loss_scale: 65536.0000 (40076.7944)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1214  max mem: 15572
[2025-01-16 00:19:24,263] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58472
[2025-01-16 00:19:24,263] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:19:24,263] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [2300/2809]  eta: 0:04:48  lr: 0.000027  min_lr: 0.000000  loss: 4.2456 (4.2759)  class_acc: 0.2917 (0.2843)  loss_scale: 65536.0000 (40059.2716)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1496  max mem: 15572
Epoch: [20]  [2310/2809]  eta: 0:04:42  lr: 0.000027  min_lr: 0.000000  loss: 4.3052 (4.2765)  class_acc: 0.2500 (0.2841)  loss_scale: 32768.0000 (40027.7213)  weight_decay: 0.0500 (0.0500)  time: 0.5241  data: 0.0819  max mem: 15572
Epoch: [20]  [2320/2809]  eta: 0:04:36  lr: 0.000027  min_lr: 0.000000  loss: 4.3052 (4.2760)  class_acc: 0.2500 (0.2842)  loss_scale: 32768.0000 (39996.4429)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0817  max mem: 15572
Epoch: [20]  [2330/2809]  eta: 0:04:31  lr: 0.000027  min_lr: 0.000000  loss: 4.2191 (4.2759)  class_acc: 0.2917 (0.2842)  loss_scale: 32768.0000 (39965.4329)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1332  max mem: 15572
Epoch: [20]  [2340/2809]  eta: 0:04:25  lr: 0.000027  min_lr: 0.000000  loss: 4.1281 (4.2753)  class_acc: 0.2917 (0.2845)  loss_scale: 32768.0000 (39934.6877)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0878  max mem: 15572
Epoch: [20]  [2350/2809]  eta: 0:04:19  lr: 0.000027  min_lr: 0.000000  loss: 4.2185 (4.2753)  class_acc: 0.3333 (0.2846)  loss_scale: 32768.0000 (39904.2042)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0660  max mem: 15572
Epoch: [20]  [2360/2809]  eta: 0:04:13  lr: 0.000027  min_lr: 0.000000  loss: 4.1845 (4.2747)  class_acc: 0.2500 (0.2845)  loss_scale: 32768.0000 (39873.9788)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.0799  max mem: 15572
Epoch: [20]  [2370/2809]  eta: 0:04:08  lr: 0.000027  min_lr: 0.000000  loss: 4.1430 (4.2744)  class_acc: 0.2500 (0.2843)  loss_scale: 32768.0000 (39844.0084)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1386  max mem: 15572
Epoch: [20]  [2380/2809]  eta: 0:04:02  lr: 0.000027  min_lr: 0.000000  loss: 4.2684 (4.2745)  class_acc: 0.2500 (0.2844)  loss_scale: 32768.0000 (39814.2898)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1302  max mem: 15572
Epoch: [20]  [2390/2809]  eta: 0:03:56  lr: 0.000027  min_lr: 0.000000  loss: 4.3616 (4.2747)  class_acc: 0.2917 (0.2845)  loss_scale: 32768.0000 (39784.8197)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1020  max mem: 15572
Epoch: [20]  [2400/2809]  eta: 0:03:51  lr: 0.000027  min_lr: 0.000000  loss: 4.2985 (4.2748)  class_acc: 0.2500 (0.2844)  loss_scale: 32768.0000 (39755.5952)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1176  max mem: 15572
Epoch: [20]  [2410/2809]  eta: 0:03:45  lr: 0.000027  min_lr: 0.000000  loss: 4.2798 (4.2743)  class_acc: 0.2917 (0.2844)  loss_scale: 32768.0000 (39726.6130)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1377  max mem: 15572
Epoch: [20]  [2420/2809]  eta: 0:03:40  lr: 0.000027  min_lr: 0.000000  loss: 4.2506 (4.2737)  class_acc: 0.2917 (0.2845)  loss_scale: 32768.0000 (39697.8703)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1408  max mem: 15572
[2025-01-16 00:20:36,576] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:20:36,576] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [2430/2809]  eta: 0:03:34  lr: 0.000027  min_lr: 0.000000  loss: 4.1809 (4.2736)  class_acc: 0.2917 (0.2844)  loss_scale: 32768.0000 (39804.1563)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.1004  max mem: 15572
Epoch: [20]  [2440/2809]  eta: 0:03:28  lr: 0.000027  min_lr: 0.000000  loss: 4.2509 (4.2737)  class_acc: 0.2500 (0.2842)  loss_scale: 65536.0000 (39909.5715)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1754  max mem: 15572
[2025-01-16 00:20:49,931] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58623
[2025-01-16 00:20:49,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:20:49,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [2450/2809]  eta: 0:03:23  lr: 0.000027  min_lr: 0.000000  loss: 4.3377 (4.2735)  class_acc: 0.2500 (0.2844)  loss_scale: 65536.0000 (39907.1726)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.1925  max mem: 15572
Epoch: [20]  [2460/2809]  eta: 0:03:17  lr: 0.000027  min_lr: 0.000000  loss: 4.2503 (4.2733)  class_acc: 0.3333 (0.2845)  loss_scale: 32768.0000 (39878.1633)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0910  max mem: 15572
Epoch: [20]  [2470/2809]  eta: 0:03:11  lr: 0.000027  min_lr: 0.000000  loss: 4.1807 (4.2731)  class_acc: 0.2917 (0.2845)  loss_scale: 32768.0000 (39849.3889)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0804  max mem: 15572
Epoch: [20]  [2480/2809]  eta: 0:03:06  lr: 0.000027  min_lr: 0.000000  loss: 4.1807 (4.2733)  class_acc: 0.2500 (0.2845)  loss_scale: 32768.0000 (39820.8464)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0700  max mem: 15572
Epoch: [20]  [2490/2809]  eta: 0:03:00  lr: 0.000027  min_lr: 0.000000  loss: 4.3789 (4.2735)  class_acc: 0.2500 (0.2846)  loss_scale: 32768.0000 (39792.5331)  weight_decay: 0.0500 (0.0500)  time: 0.4771  data: 0.0359  max mem: 15572
Epoch: [20]  [2500/2809]  eta: 0:02:54  lr: 0.000027  min_lr: 0.000000  loss: 4.2326 (4.2735)  class_acc: 0.2500 (0.2843)  loss_scale: 32768.0000 (39764.4462)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0726  max mem: 15572
Epoch: [20]  [2510/2809]  eta: 0:02:48  lr: 0.000027  min_lr: 0.000000  loss: 4.3587 (4.2739)  class_acc: 0.2083 (0.2839)  loss_scale: 32768.0000 (39736.5830)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1070  max mem: 15572
Epoch: [20]  [2520/2809]  eta: 0:02:43  lr: 0.000027  min_lr: 0.000000  loss: 4.3587 (4.2737)  class_acc: 0.2500 (0.2840)  loss_scale: 32768.0000 (39708.9409)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1237  max mem: 15572
Epoch: [20]  [2530/2809]  eta: 0:02:37  lr: 0.000027  min_lr: 0.000000  loss: 4.2892 (4.2737)  class_acc: 0.2917 (0.2840)  loss_scale: 32768.0000 (39681.5172)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1332  max mem: 15572
Epoch: [20]  [2540/2809]  eta: 0:02:32  lr: 0.000027  min_lr: 0.000000  loss: 4.2188 (4.2732)  class_acc: 0.2500 (0.2840)  loss_scale: 32768.0000 (39654.3093)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0937  max mem: 15572
Epoch: [20]  [2550/2809]  eta: 0:02:26  lr: 0.000027  min_lr: 0.000000  loss: 4.2188 (4.2731)  class_acc: 0.2917 (0.2840)  loss_scale: 32768.0000 (39627.3148)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0530  max mem: 15572
Epoch: [20]  [2560/2809]  eta: 0:02:20  lr: 0.000027  min_lr: 0.000000  loss: 4.2743 (4.2735)  class_acc: 0.2917 (0.2841)  loss_scale: 32768.0000 (39600.5310)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1078  max mem: 15572
Epoch: [20]  [2570/2809]  eta: 0:02:15  lr: 0.000027  min_lr: 0.000000  loss: 4.3955 (4.2737)  class_acc: 0.2917 (0.2843)  loss_scale: 32768.0000 (39573.9557)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0925  max mem: 15572
[2025-01-16 00:21:59,833] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:21:59,833] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [2580/2809]  eta: 0:02:09  lr: 0.000027  min_lr: 0.000000  loss: 4.3955 (4.2742)  class_acc: 0.2917 (0.2844)  loss_scale: 32768.0000 (39661.8489)  weight_decay: 0.0500 (0.0500)  time: 0.4724  data: 0.0081  max mem: 15572
Epoch: [20]  [2590/2809]  eta: 0:02:03  lr: 0.000027  min_lr: 0.000000  loss: 4.3101 (4.2746)  class_acc: 0.2917 (0.2843)  loss_scale: 65536.0000 (39761.7105)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0894  max mem: 15572
Epoch: [20]  [2600/2809]  eta: 0:01:58  lr: 0.000027  min_lr: 0.000000  loss: 4.3225 (4.2750)  class_acc: 0.2083 (0.2842)  loss_scale: 65536.0000 (39860.8043)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1336  max mem: 15572
Epoch: [20]  [2610/2809]  eta: 0:01:52  lr: 0.000027  min_lr: 0.000000  loss: 4.2304 (4.2746)  class_acc: 0.2500 (0.2843)  loss_scale: 65536.0000 (39959.1390)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1384  max mem: 15572
[2025-01-16 00:22:26,724] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58799
[2025-01-16 00:22:26,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:22:26,725] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [2620/2809]  eta: 0:01:46  lr: 0.000027  min_lr: 0.000000  loss: 4.2387 (4.2752)  class_acc: 0.2500 (0.2843)  loss_scale: 65536.0000 (40031.7192)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1166  max mem: 15572
Epoch: [20]  [2630/2809]  eta: 0:01:41  lr: 0.000027  min_lr: 0.000000  loss: 4.1628 (4.2746)  class_acc: 0.2083 (0.2841)  loss_scale: 32768.0000 (40004.1110)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1170  max mem: 15572
Epoch: [20]  [2640/2809]  eta: 0:01:35  lr: 0.000027  min_lr: 0.000000  loss: 4.0714 (4.2744)  class_acc: 0.2500 (0.2841)  loss_scale: 32768.0000 (39976.7119)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0874  max mem: 15572
Epoch: [20]  [2650/2809]  eta: 0:01:29  lr: 0.000027  min_lr: 0.000000  loss: 4.2411 (4.2748)  class_acc: 0.2500 (0.2839)  loss_scale: 32768.0000 (39949.5194)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0425  max mem: 15572
Epoch: [20]  [2660/2809]  eta: 0:01:24  lr: 0.000027  min_lr: 0.000000  loss: 4.2227 (4.2747)  class_acc: 0.2083 (0.2840)  loss_scale: 32768.0000 (39922.5314)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0760  max mem: 15572
Epoch: [20]  [2670/2809]  eta: 0:01:18  lr: 0.000027  min_lr: 0.000000  loss: 4.1751 (4.2740)  class_acc: 0.2917 (0.2843)  loss_scale: 32768.0000 (39895.7454)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1099  max mem: 15572
Epoch: [20]  [2680/2809]  eta: 0:01:12  lr: 0.000027  min_lr: 0.000000  loss: 4.1930 (4.2743)  class_acc: 0.2917 (0.2842)  loss_scale: 32768.0000 (39869.1593)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1174  max mem: 15572
Epoch: [20]  [2690/2809]  eta: 0:01:07  lr: 0.000027  min_lr: 0.000000  loss: 4.3702 (4.2747)  class_acc: 0.2083 (0.2840)  loss_scale: 32768.0000 (39842.7707)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1393  max mem: 15572
Epoch: [20]  [2700/2809]  eta: 0:01:01  lr: 0.000027  min_lr: 0.000000  loss: 4.4020 (4.2751)  class_acc: 0.2083 (0.2840)  loss_scale: 32768.0000 (39816.5776)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1073  max mem: 15572
Epoch: [20]  [2710/2809]  eta: 0:00:55  lr: 0.000027  min_lr: 0.000000  loss: 4.3651 (4.2753)  class_acc: 0.2500 (0.2837)  loss_scale: 32768.0000 (39790.5776)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1031  max mem: 15572
Epoch: [20]  [2720/2809]  eta: 0:00:50  lr: 0.000027  min_lr: 0.000000  loss: 4.1333 (4.2745)  class_acc: 0.2917 (0.2841)  loss_scale: 32768.0000 (39764.7688)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1355  max mem: 15572
Epoch: [20]  [2730/2809]  eta: 0:00:44  lr: 0.000027  min_lr: 0.000000  loss: 4.1049 (4.2745)  class_acc: 0.3333 (0.2841)  loss_scale: 32768.0000 (39739.1490)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0715  max mem: 15572
Epoch: [20]  [2740/2809]  eta: 0:00:39  lr: 0.000027  min_lr: 0.000000  loss: 4.2335 (4.2743)  class_acc: 0.2917 (0.2844)  loss_scale: 32768.0000 (39713.7162)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1168  max mem: 15572
[2025-01-16 00:23:40,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:23:40,349] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [2750/2809]  eta: 0:00:33  lr: 0.000027  min_lr: 0.000000  loss: 4.3046 (4.2748)  class_acc: 0.2917 (0.2844)  loss_scale: 32768.0000 (39724.2021)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.1022  max mem: 15572
[2025-01-16 00:23:43,461] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58936
[2025-01-16 00:23:43,461] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:23:43,461] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [2760/2809]  eta: 0:00:27  lr: 0.000027  min_lr: 0.000000  loss: 4.2589 (4.2743)  class_acc: 0.2917 (0.2845)  loss_scale: 32768.0000 (39758.3484)  weight_decay: 0.0500 (0.0500)  time: 0.4207  data: 0.0153  max mem: 15572
Epoch: [20]  [2770/2809]  eta: 0:00:21  lr: 0.000027  min_lr: 0.000000  loss: 4.1730 (4.2739)  class_acc: 0.3333 (0.2846)  loss_scale: 32768.0000 (39733.1216)  weight_decay: 0.0500 (0.0500)  time: 0.4121  data: 0.0006  max mem: 15572
Epoch: [20]  [2780/2809]  eta: 0:00:16  lr: 0.000027  min_lr: 0.000000  loss: 4.3685 (4.2743)  class_acc: 0.3333 (0.2845)  loss_scale: 32768.0000 (39708.0762)  weight_decay: 0.0500 (0.0500)  time: 0.4717  data: 0.0245  max mem: 15572
Epoch: [20]  [2790/2809]  eta: 0:00:10  lr: 0.000027  min_lr: 0.000000  loss: 4.3870 (4.2748)  class_acc: 0.2500 (0.2843)  loss_scale: 32768.0000 (39683.2103)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0898  max mem: 15572
Epoch: [20]  [2800/2809]  eta: 0:00:05  lr: 0.000027  min_lr: 0.000000  loss: 4.3238 (4.2747)  class_acc: 0.2083 (0.2840)  loss_scale: 32768.0000 (39658.5220)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1546  max mem: 15572
Epoch: [20]  [2808/2809]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000000  loss: 4.3102 (4.2748)  class_acc: 0.1667 (0.2838)  loss_scale: 32768.0000 (39638.8978)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0988  max mem: 15572
Epoch: [20] Total time: 0:26:25 (0.5643 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000000  loss: 4.3102 (4.2748)  class_acc: 0.1667 (0.2838)  loss_scale: 32768.0000 (39638.8978)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:23:51  loss: 1.1730 (1.1730)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 5.2641  data: 5.0821  max mem: 15572
Val:  [ 10/272]  eta: 0:03:51  loss: 2.6615 (2.6477)  acc1: 50.0000 (42.9293)  acc5: 72.2222 (71.2121)  time: 0.8828  data: 0.6853  max mem: 15572
Val:  [ 20/272]  eta: 0:02:35  loss: 2.8337 (2.7252)  acc1: 44.4444 (42.8571)  acc5: 66.6667 (69.8413)  time: 0.3857  data: 0.1953  max mem: 15572
Val:  [ 30/272]  eta: 0:02:05  loss: 2.8337 (2.7466)  acc1: 44.4444 (40.3226)  acc5: 66.6667 (70.2509)  time: 0.3202  data: 0.1216  max mem: 15572
Val:  [ 40/272]  eta: 0:01:49  loss: 2.7583 (2.7500)  acc1: 27.7778 (38.8889)  acc5: 72.2222 (71.0027)  time: 0.3189  data: 0.1104  max mem: 15572
Val:  [ 50/272]  eta: 0:01:40  loss: 2.6118 (2.7079)  acc1: 38.8889 (39.6514)  acc5: 72.2222 (72.1133)  time: 0.3488  data: 0.1407  max mem: 15572
Val:  [ 60/272]  eta: 0:01:32  loss: 2.0560 (2.6238)  acc1: 55.5556 (41.8033)  acc5: 77.7778 (73.4062)  time: 0.3631  data: 0.1609  max mem: 15572
Val:  [ 70/272]  eta: 0:01:28  loss: 1.9632 (2.5493)  acc1: 61.1111 (45.1487)  acc5: 83.3333 (74.8044)  time: 0.3938  data: 0.1965  max mem: 15572
Val:  [ 80/272]  eta: 0:01:23  loss: 2.1365 (2.5509)  acc1: 50.0000 (44.6502)  acc5: 83.3333 (74.6228)  time: 0.4389  data: 0.2319  max mem: 15572
Val:  [ 90/272]  eta: 0:01:16  loss: 2.6737 (2.5697)  acc1: 44.4444 (44.1392)  acc5: 72.2222 (74.5421)  time: 0.3724  data: 0.1541  max mem: 15572
Val:  [100/272]  eta: 0:01:11  loss: 2.7199 (2.6026)  acc1: 33.3333 (42.9043)  acc5: 72.2222 (74.0924)  time: 0.3381  data: 0.1149  max mem: 15572
Val:  [110/272]  eta: 0:01:06  loss: 2.9113 (2.6505)  acc1: 27.7778 (41.6416)  acc5: 72.2222 (73.1231)  time: 0.3685  data: 0.1516  max mem: 15572
Val:  [120/272]  eta: 0:01:00  loss: 2.9113 (2.6886)  acc1: 27.7778 (40.9091)  acc5: 61.1111 (72.1304)  time: 0.3159  data: 0.0982  max mem: 15572
Val:  [130/272]  eta: 0:00:56  loss: 2.6838 (2.6696)  acc1: 44.4444 (41.4758)  acc5: 77.7778 (72.9432)  time: 0.2975  data: 0.0902  max mem: 15572
Val:  [140/272]  eta: 0:00:51  loss: 2.3214 (2.6721)  acc1: 50.0000 (41.8834)  acc5: 83.3333 (72.7344)  time: 0.3365  data: 0.1436  max mem: 15572
Val:  [150/272]  eta: 0:00:47  loss: 2.6206 (2.6676)  acc1: 38.8889 (41.6851)  acc5: 72.2222 (72.7741)  time: 0.3416  data: 0.1475  max mem: 15572
Val:  [160/272]  eta: 0:00:42  loss: 2.5896 (2.6672)  acc1: 44.4444 (41.9945)  acc5: 77.7778 (73.0504)  time: 0.3296  data: 0.1394  max mem: 15572
Val:  [170/272]  eta: 0:00:38  loss: 2.7149 (2.6821)  acc1: 38.8889 (41.5205)  acc5: 72.2222 (72.7096)  time: 0.2794  data: 0.0992  max mem: 15572
Val:  [180/272]  eta: 0:00:33  loss: 2.7129 (2.6714)  acc1: 33.3333 (41.5285)  acc5: 72.2222 (73.0203)  time: 0.2193  data: 0.0298  max mem: 15572
Val:  [190/272]  eta: 0:00:29  loss: 2.7129 (2.7097)  acc1: 27.7778 (40.3432)  acc5: 66.6667 (71.6405)  time: 0.2055  data: 0.0059  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 2.9199 (2.7170)  acc1: 22.2222 (39.8563)  acc5: 55.5556 (71.4483)  time: 0.2441  data: 0.0442  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.6470 (2.7185)  acc1: 44.4444 (40.1001)  acc5: 77.7778 (71.5113)  time: 0.2801  data: 0.0841  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 2.6470 (2.7196)  acc1: 44.4444 (40.0452)  acc5: 77.7778 (71.3675)  time: 0.2775  data: 0.0905  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.3209 (2.7069)  acc1: 38.8889 (40.5243)  acc5: 77.7778 (71.6691)  time: 0.3079  data: 0.1134  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 2.2496 (2.6948)  acc1: 61.1111 (41.1019)  acc5: 83.3333 (72.1070)  time: 0.3174  data: 0.1130  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.5816 (2.6989)  acc1: 44.4444 (40.9252)  acc5: 77.7778 (72.1558)  time: 0.2966  data: 0.1070  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 2.1018 (2.6588)  acc1: 55.5556 (42.4649)  acc5: 83.3333 (72.9246)  time: 0.3121  data: 0.1307  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.1255 (2.6610)  acc1: 55.5556 (42.1484)  acc5: 83.3333 (72.8372)  time: 0.2461  data: 0.0767  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.1255 (2.6664)  acc1: 44.4444 (42.1257)  acc5: 77.7778 (72.8036)  time: 0.2387  data: 0.0767  max mem: 15572
Val: Total time: 0:01:31 (0.3356 s / it)
* Acc@1 42.126 Acc@5 72.804 loss 2.666
Accuracy of the network on the 4883 val videos: 42.1%
[2025-01-16 00:25:41,717] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 00:25:41,725] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 00:25:41,725] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 00:25:44,326] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 00:25:44,327] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 42.13%
Epoch: [21]  [   0/2809]  eta: 6:41:05  lr: 0.000027  min_lr: 0.000000  loss: 4.4466 (4.4466)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.5673  data: 7.9992  max mem: 15572
[2025-01-16 00:25:59,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=59000, skipped=369, lr=[2.57690559215989e-07, 2.57690559215989e-07, 3.681293703085558e-07, 3.681293703085558e-07, 5.25899100440794e-07, 5.25899100440794e-07, 7.512844292011343e-07, 7.512844292011343e-07, 1.0732634702873348e-06, 1.0732634702873348e-06, 1.533233528981907e-06, 1.533233528981907e-06, 2.1903336128312955e-06, 2.1903336128312955e-06, 3.129048018330423e-06, 3.129048018330423e-06, 4.470068597614889e-06, 4.470068597614889e-06, 6.385812282306986e-06, 6.385812282306986e-06, 9.122588974724265e-06, 9.122588974724265e-06, 1.3032269963891809e-05, 1.3032269963891809e-05, 1.8617528519845444e-05, 1.8617528519845444e-05, 2.659646931406492e-05, 2.659646931406492e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:25:59,002] [INFO] [timer.py:260:stop] epoch=0/micro_step=59000/global_step=59000, RunningAvgSamplesPerSec=27.79905329869388, CurrSamplesPerSec=24.33803863011261, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [21]  [  10/2809]  eta: 1:02:13  lr: 0.000027  min_lr: 0.000000  loss: 4.1039 (4.0975)  class_acc: 0.2500 (0.2576)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3337  data: 0.8789  max mem: 15572
Epoch: [21]  [  20/2809]  eta: 0:44:23  lr: 0.000027  min_lr: 0.000000  loss: 4.1534 (4.2545)  class_acc: 0.2500 (0.2579)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1317  max mem: 15572
Epoch: [21]  [  30/2809]  eta: 0:38:13  lr: 0.000027  min_lr: 0.000000  loss: 4.3160 (4.2406)  class_acc: 0.2917 (0.2836)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1075  max mem: 15572
Epoch: [21]  [  40/2809]  eta: 0:34:58  lr: 0.000027  min_lr: 0.000000  loss: 4.2345 (4.2578)  class_acc: 0.3333 (0.2876)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1080  max mem: 15572
Epoch: [21]  [  50/2809]  eta: 0:32:23  lr: 0.000027  min_lr: 0.000000  loss: 4.3536 (4.2856)  class_acc: 0.3333 (0.2827)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0762  max mem: 15572
Epoch: [21]  [  60/2809]  eta: 0:31:31  lr: 0.000027  min_lr: 0.000000  loss: 4.2947 (4.2660)  class_acc: 0.2917 (0.2835)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1133  max mem: 15572
Epoch: [21]  [  70/2809]  eta: 0:30:32  lr: 0.000027  min_lr: 0.000000  loss: 4.2558 (4.2664)  class_acc: 0.2500 (0.2805)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1359  max mem: 15572
[2025-01-16 00:26:35,602] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:26:35,602] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [  80/2809]  eta: 0:29:47  lr: 0.000027  min_lr: 0.000000  loss: 4.2803 (4.2669)  class_acc: 0.2500 (0.2803)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1109  max mem: 15572
Epoch: [21]  [  90/2809]  eta: 0:29:43  lr: 0.000027  min_lr: 0.000000  loss: 4.2258 (4.2649)  class_acc: 0.2500 (0.2784)  loss_scale: 65536.0000 (38169.3187)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1728  max mem: 15572
Epoch: [21]  [ 100/2809]  eta: 0:29:06  lr: 0.000027  min_lr: 0.000000  loss: 4.2258 (4.2562)  class_acc: 0.2083 (0.2781)  loss_scale: 65536.0000 (40878.8911)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1522  max mem: 15572
Epoch: [21]  [ 110/2809]  eta: 0:28:43  lr: 0.000027  min_lr: 0.000000  loss: 4.2250 (4.2526)  class_acc: 0.2917 (0.2827)  loss_scale: 65536.0000 (43100.2523)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0974  max mem: 15572
[2025-01-16 00:26:58,926] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59103
[2025-01-16 00:26:58,926] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:26:58,926] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 120/2809]  eta: 0:28:38  lr: 0.000027  min_lr: 0.000000  loss: 4.2634 (4.2487)  class_acc: 0.3333 (0.2882)  loss_scale: 65536.0000 (43058.7769)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1517  max mem: 15572
Epoch: [21]  [ 130/2809]  eta: 0:28:01  lr: 0.000027  min_lr: 0.000000  loss: 4.2568 (4.2492)  class_acc: 0.2917 (0.2875)  loss_scale: 32768.0000 (42273.2214)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1160  max mem: 15572
Epoch: [21]  [ 140/2809]  eta: 0:27:37  lr: 0.000027  min_lr: 0.000000  loss: 4.2568 (4.2551)  class_acc: 0.2500 (0.2869)  loss_scale: 32768.0000 (41599.0922)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0497  max mem: 15572
Epoch: [21]  [ 150/2809]  eta: 0:27:28  lr: 0.000026  min_lr: 0.000000  loss: 4.2702 (4.2588)  class_acc: 0.2500 (0.2873)  loss_scale: 32768.0000 (41014.2517)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1057  max mem: 15572
Epoch: [21]  [ 160/2809]  eta: 0:27:19  lr: 0.000026  min_lr: 0.000000  loss: 4.2664 (4.2553)  class_acc: 0.2917 (0.2888)  loss_scale: 32768.0000 (40502.0621)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1532  max mem: 15572
Epoch: [21]  [ 170/2809]  eta: 0:27:10  lr: 0.000026  min_lr: 0.000000  loss: 4.2568 (4.2625)  class_acc: 0.2500 (0.2875)  loss_scale: 32768.0000 (40049.7778)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1580  max mem: 15572
Epoch: [21]  [ 180/2809]  eta: 0:26:45  lr: 0.000026  min_lr: 0.000000  loss: 4.2980 (4.2652)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (39647.4696)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0970  max mem: 15572
Epoch: [21]  [ 190/2809]  eta: 0:26:27  lr: 0.000026  min_lr: 0.000000  loss: 4.2681 (4.2631)  class_acc: 0.2500 (0.2901)  loss_scale: 32768.0000 (39287.2880)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0627  max mem: 15572
Epoch: [21]  [ 200/2809]  eta: 0:26:17  lr: 0.000026  min_lr: 0.000000  loss: 4.2426 (4.2612)  class_acc: 0.2500 (0.2900)  loss_scale: 32768.0000 (38962.9453)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.0931  max mem: 15572
Epoch: [21]  [ 210/2809]  eta: 0:26:02  lr: 0.000026  min_lr: 0.000000  loss: 4.2748 (4.2678)  class_acc: 0.2500 (0.2879)  loss_scale: 32768.0000 (38669.3460)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0887  max mem: 15572
Epoch: [21]  [ 220/2809]  eta: 0:25:48  lr: 0.000026  min_lr: 0.000000  loss: 4.3383 (4.2702)  class_acc: 0.2083 (0.2864)  loss_scale: 32768.0000 (38402.3167)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0816  max mem: 15572
Epoch: [21]  [ 230/2809]  eta: 0:25:33  lr: 0.000026  min_lr: 0.000000  loss: 4.2702 (4.2722)  class_acc: 0.2500 (0.2863)  loss_scale: 32768.0000 (38158.4069)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0794  max mem: 15572
Epoch: [21]  [ 240/2809]  eta: 0:25:22  lr: 0.000026  min_lr: 0.000000  loss: 4.2698 (4.2680)  class_acc: 0.2500 (0.2868)  loss_scale: 32768.0000 (37934.7386)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0967  max mem: 15572
[2025-01-16 00:28:08,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:28:08,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [ 250/2809]  eta: 0:25:14  lr: 0.000026  min_lr: 0.000000  loss: 4.2357 (4.2678)  class_acc: 0.3333 (0.2887)  loss_scale: 32768.0000 (38773.2908)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1185  max mem: 15572
[2025-01-16 00:28:14,581] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59243
[2025-01-16 00:28:14,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:28:14,581] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 260/2809]  eta: 0:25:04  lr: 0.000026  min_lr: 0.000000  loss: 4.1254 (4.2605)  class_acc: 0.3333 (0.2904)  loss_scale: 65536.0000 (38919.8467)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1250  max mem: 15572
Epoch: [21]  [ 270/2809]  eta: 0:25:00  lr: 0.000026  min_lr: 0.000000  loss: 4.0901 (4.2600)  class_acc: 0.2917 (0.2909)  loss_scale: 32768.0000 (38692.8413)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1423  max mem: 15572
Epoch: [21]  [ 280/2809]  eta: 0:24:53  lr: 0.000026  min_lr: 0.000000  loss: 4.2409 (4.2624)  class_acc: 0.2917 (0.2900)  loss_scale: 32768.0000 (38481.9929)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1494  max mem: 15572
Epoch: [21]  [ 290/2809]  eta: 0:24:49  lr: 0.000026  min_lr: 0.000000  loss: 4.2355 (4.2603)  class_acc: 0.2500 (0.2892)  loss_scale: 32768.0000 (38285.6357)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1491  max mem: 15572
Epoch: [21]  [ 300/2809]  eta: 0:24:46  lr: 0.000026  min_lr: 0.000000  loss: 4.1223 (4.2631)  class_acc: 0.2500 (0.2903)  loss_scale: 32768.0000 (38102.3256)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1636  max mem: 15572
Epoch: [21]  [ 310/2809]  eta: 0:24:36  lr: 0.000026  min_lr: 0.000000  loss: 4.0913 (4.2578)  class_acc: 0.2917 (0.2910)  loss_scale: 32768.0000 (37930.8039)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1312  max mem: 15572
Epoch: [21]  [ 320/2809]  eta: 0:24:25  lr: 0.000026  min_lr: 0.000000  loss: 4.2928 (4.2615)  class_acc: 0.2500 (0.2887)  loss_scale: 32768.0000 (37769.9688)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0946  max mem: 15572
Epoch: [21]  [ 330/2809]  eta: 0:24:23  lr: 0.000026  min_lr: 0.000000  loss: 4.4419 (4.2640)  class_acc: 0.2083 (0.2873)  loss_scale: 32768.0000 (37618.8520)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1359  max mem: 15572
Epoch: [21]  [ 340/2809]  eta: 0:24:16  lr: 0.000026  min_lr: 0.000000  loss: 4.4419 (4.2648)  class_acc: 0.2917 (0.2880)  loss_scale: 32768.0000 (37476.5982)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1515  max mem: 15572
Epoch: [21]  [ 350/2809]  eta: 0:24:13  lr: 0.000026  min_lr: 0.000000  loss: 4.3513 (4.2671)  class_acc: 0.2917 (0.2860)  loss_scale: 32768.0000 (37342.4501)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1531  max mem: 15572
Epoch: [21]  [ 360/2809]  eta: 0:24:02  lr: 0.000026  min_lr: 0.000000  loss: 4.3883 (4.2739)  class_acc: 0.1667 (0.2834)  loss_scale: 32768.0000 (37215.7341)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1313  max mem: 15572
Epoch: [21]  [ 370/2809]  eta: 0:23:55  lr: 0.000026  min_lr: 0.000000  loss: 4.4292 (4.2774)  class_acc: 0.2083 (0.2826)  loss_scale: 32768.0000 (37095.8491)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0970  max mem: 15572
Epoch: [21]  [ 380/2809]  eta: 0:23:46  lr: 0.000026  min_lr: 0.000000  loss: 4.3601 (4.2762)  class_acc: 0.2500 (0.2824)  loss_scale: 32768.0000 (36982.2572)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1030  max mem: 15572
[2025-01-16 00:29:29,663] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:29:29,663] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [ 390/2809]  eta: 0:23:38  lr: 0.000026  min_lr: 0.000000  loss: 4.1022 (4.2712)  class_acc: 0.3333 (0.2843)  loss_scale: 32768.0000 (37544.9207)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.0913  max mem: 15572
Epoch: [21]  [ 400/2809]  eta: 0:23:30  lr: 0.000026  min_lr: 0.000000  loss: 4.1861 (4.2744)  class_acc: 0.2917 (0.2826)  loss_scale: 65536.0000 (38242.9526)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0962  max mem: 15572
Epoch: [21]  [ 410/2809]  eta: 0:23:24  lr: 0.000026  min_lr: 0.000000  loss: 4.2759 (4.2749)  class_acc: 0.2500 (0.2822)  loss_scale: 65536.0000 (38907.0170)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1183  max mem: 15572
Epoch: [21]  [ 420/2809]  eta: 0:23:19  lr: 0.000026  min_lr: 0.000000  loss: 4.2896 (4.2746)  class_acc: 0.2083 (0.2806)  loss_scale: 65536.0000 (39539.5344)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1334  max mem: 15572
[2025-01-16 00:29:53,888] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59416
[2025-01-16 00:29:53,889] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:29:53,889] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 430/2809]  eta: 0:23:11  lr: 0.000026  min_lr: 0.000000  loss: 4.2896 (4.2728)  class_acc: 0.2500 (0.2815)  loss_scale: 65536.0000 (39838.5893)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1381  max mem: 15572
Epoch: [21]  [ 440/2809]  eta: 0:22:59  lr: 0.000026  min_lr: 0.000000  loss: 4.2281 (4.2739)  class_acc: 0.2500 (0.2817)  loss_scale: 32768.0000 (39678.2585)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0903  max mem: 15572
Epoch: [21]  [ 450/2809]  eta: 0:22:53  lr: 0.000026  min_lr: 0.000000  loss: 4.2946 (4.2750)  class_acc: 0.2500 (0.2822)  loss_scale: 32768.0000 (39525.0377)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0789  max mem: 15572
Epoch: [21]  [ 460/2809]  eta: 0:22:45  lr: 0.000026  min_lr: 0.000000  loss: 4.3149 (4.2743)  class_acc: 0.2500 (0.2804)  loss_scale: 32768.0000 (39378.4642)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1031  max mem: 15572
Epoch: [21]  [ 470/2809]  eta: 0:22:36  lr: 0.000026  min_lr: 0.000000  loss: 4.1518 (4.2687)  class_acc: 0.2500 (0.2818)  loss_scale: 32768.0000 (39238.1146)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0786  max mem: 15572
Epoch: [21]  [ 480/2809]  eta: 0:22:25  lr: 0.000026  min_lr: 0.000000  loss: 4.2274 (4.2690)  class_acc: 0.3333 (0.2826)  loss_scale: 32768.0000 (39103.6008)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0451  max mem: 15572
Epoch: [21]  [ 490/2809]  eta: 0:22:21  lr: 0.000026  min_lr: 0.000000  loss: 4.2645 (4.2668)  class_acc: 0.2917 (0.2831)  loss_scale: 32768.0000 (38974.5662)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1010  max mem: 15572
Epoch: [21]  [ 500/2809]  eta: 0:22:15  lr: 0.000026  min_lr: 0.000000  loss: 4.2603 (4.2670)  class_acc: 0.2500 (0.2819)  loss_scale: 32768.0000 (38850.6826)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1589  max mem: 15572
Epoch: [21]  [ 510/2809]  eta: 0:22:06  lr: 0.000026  min_lr: 0.000000  loss: 4.2603 (4.2664)  class_acc: 0.2500 (0.2821)  loss_scale: 32768.0000 (38731.6477)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.1087  max mem: 15572
Epoch: [21]  [ 520/2809]  eta: 0:21:58  lr: 0.000026  min_lr: 0.000000  loss: 4.2576 (4.2671)  class_acc: 0.2500 (0.2815)  loss_scale: 32768.0000 (38617.1823)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0900  max mem: 15572
Epoch: [21]  [ 530/2809]  eta: 0:21:54  lr: 0.000026  min_lr: 0.000000  loss: 4.3034 (4.2690)  class_acc: 0.2500 (0.2819)  loss_scale: 32768.0000 (38507.0282)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1367  max mem: 15572
Epoch: [21]  [ 540/2809]  eta: 0:21:46  lr: 0.000026  min_lr: 0.000000  loss: 4.3623 (4.2710)  class_acc: 0.3750 (0.2831)  loss_scale: 32768.0000 (38400.9464)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1157  max mem: 15572
Epoch: [21]  [ 550/2809]  eta: 0:21:44  lr: 0.000026  min_lr: 0.000000  loss: 4.3134 (4.2724)  class_acc: 0.3750 (0.2843)  loss_scale: 32768.0000 (38298.7151)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1481  max mem: 15572
[2025-01-16 00:31:05,682] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:31:05,682] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [ 560/2809]  eta: 0:21:41  lr: 0.000026  min_lr: 0.000000  loss: 4.3134 (4.2706)  class_acc: 0.3333 (0.2842)  loss_scale: 32768.0000 (38492.1783)  weight_decay: 0.0500 (0.0500)  time: 0.6537  data: 0.2167  max mem: 15572
Epoch: [21]  [ 570/2809]  eta: 0:21:32  lr: 0.000026  min_lr: 0.000000  loss: 4.3574 (4.2737)  class_acc: 0.2500 (0.2832)  loss_scale: 65536.0000 (38965.8004)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1226  max mem: 15572
Epoch: [21]  [ 580/2809]  eta: 0:21:25  lr: 0.000026  min_lr: 0.000000  loss: 4.3955 (4.2732)  class_acc: 0.2917 (0.2838)  loss_scale: 65536.0000 (39423.1188)  weight_decay: 0.0500 (0.0500)  time: 0.5229  data: 0.0706  max mem: 15572
[2025-01-16 00:31:20,869] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59572
[2025-01-16 00:31:20,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:31:20,871] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 590/2809]  eta: 0:21:18  lr: 0.000026  min_lr: 0.000000  loss: 4.3388 (4.2732)  class_acc: 0.2917 (0.2838)  loss_scale: 65536.0000 (39421.4010)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.1008  max mem: 15572
Epoch: [21]  [ 600/2809]  eta: 0:21:11  lr: 0.000026  min_lr: 0.000000  loss: 4.3388 (4.2734)  class_acc: 0.2500 (0.2841)  loss_scale: 32768.0000 (39310.6955)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.1052  max mem: 15572
Epoch: [21]  [ 610/2809]  eta: 0:21:04  lr: 0.000026  min_lr: 0.000000  loss: 4.2924 (4.2743)  class_acc: 0.2083 (0.2832)  loss_scale: 32768.0000 (39203.6137)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0981  max mem: 15572
Epoch: [21]  [ 620/2809]  eta: 0:20:58  lr: 0.000026  min_lr: 0.000000  loss: 4.2618 (4.2745)  class_acc: 0.2917 (0.2844)  loss_scale: 32768.0000 (39099.9807)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1081  max mem: 15572
Epoch: [21]  [ 630/2809]  eta: 0:20:51  lr: 0.000026  min_lr: 0.000000  loss: 4.1276 (4.2719)  class_acc: 0.3333 (0.2845)  loss_scale: 32768.0000 (38999.6323)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.0941  max mem: 15572
Epoch: [21]  [ 640/2809]  eta: 0:20:45  lr: 0.000026  min_lr: 0.000000  loss: 4.2491 (4.2739)  class_acc: 0.2917 (0.2841)  loss_scale: 32768.0000 (38902.4150)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0892  max mem: 15572
[2025-01-16 00:31:52,941] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59630
[2025-01-16 00:31:52,942] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 00:31:52,942] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [ 650/2809]  eta: 0:20:39  lr: 0.000026  min_lr: 0.000000  loss: 4.2593 (4.2715)  class_acc: 0.2917 (0.2853)  loss_scale: 16384.0000 (38556.5100)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1184  max mem: 15572
Epoch: [21]  [ 660/2809]  eta: 0:20:35  lr: 0.000026  min_lr: 0.000000  loss: 4.1816 (4.2702)  class_acc: 0.2917 (0.2852)  loss_scale: 16384.0000 (38221.0711)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1534  max mem: 15572
Epoch: [21]  [ 670/2809]  eta: 0:20:28  lr: 0.000026  min_lr: 0.000000  loss: 4.2151 (4.2700)  class_acc: 0.2917 (0.2854)  loss_scale: 16384.0000 (37895.6304)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1319  max mem: 15572
Epoch: [21]  [ 680/2809]  eta: 0:20:21  lr: 0.000026  min_lr: 0.000000  loss: 4.2881 (4.2719)  class_acc: 0.2500 (0.2850)  loss_scale: 16384.0000 (37579.7474)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0919  max mem: 15572
Epoch: [21]  [ 690/2809]  eta: 0:20:18  lr: 0.000026  min_lr: 0.000000  loss: 4.2881 (4.2717)  class_acc: 0.2083 (0.2849)  loss_scale: 16384.0000 (37273.0072)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1624  max mem: 15572
Epoch: [21]  [ 700/2809]  eta: 0:20:13  lr: 0.000026  min_lr: 0.000000  loss: 4.2394 (4.2704)  class_acc: 0.2917 (0.2855)  loss_scale: 16384.0000 (36975.0185)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1931  max mem: 15572
Epoch: [21]  [ 710/2809]  eta: 0:20:03  lr: 0.000026  min_lr: 0.000000  loss: 4.1719 (4.2695)  class_acc: 0.2917 (0.2848)  loss_scale: 16384.0000 (36685.4121)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0858  max mem: 15572
Epoch: [21]  [ 720/2809]  eta: 0:19:57  lr: 0.000026  min_lr: 0.000000  loss: 4.1719 (4.2692)  class_acc: 0.2083 (0.2848)  loss_scale: 16384.0000 (36403.8391)  weight_decay: 0.0500 (0.0500)  time: 0.4905  data: 0.0622  max mem: 15572
Epoch: [21]  [ 730/2809]  eta: 0:19:50  lr: 0.000026  min_lr: 0.000000  loss: 4.1942 (4.2686)  class_acc: 0.2500 (0.2848)  loss_scale: 16384.0000 (36129.9699)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.1032  max mem: 15572
Epoch: [21]  [ 740/2809]  eta: 0:19:45  lr: 0.000026  min_lr: 0.000000  loss: 4.2609 (4.2693)  class_acc: 0.2917 (0.2850)  loss_scale: 16384.0000 (35863.4926)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1322  max mem: 15572
Epoch: [21]  [ 750/2809]  eta: 0:19:40  lr: 0.000026  min_lr: 0.000000  loss: 4.2381 (4.2671)  class_acc: 0.2917 (0.2857)  loss_scale: 16384.0000 (35604.1119)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1651  max mem: 15572
Epoch: [21]  [ 760/2809]  eta: 0:19:30  lr: 0.000026  min_lr: 0.000000  loss: 4.1718 (4.2675)  class_acc: 0.2917 (0.2855)  loss_scale: 16384.0000 (35351.5480)  weight_decay: 0.0500 (0.0500)  time: 0.5090  data: 0.0804  max mem: 15572
[2025-01-16 00:33:03,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:33:03,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [ 770/2809]  eta: 0:19:21  lr: 0.000026  min_lr: 0.000000  loss: 4.2755 (4.2669)  class_acc: 0.2500 (0.2855)  loss_scale: 16384.0000 (35126.7860)  weight_decay: 0.0500 (0.0500)  time: 0.4330  data: 0.0064  max mem: 15572
Epoch: [21]  [ 780/2809]  eta: 0:19:17  lr: 0.000026  min_lr: 0.000000  loss: 4.2145 (4.2656)  class_acc: 0.2500 (0.2858)  loss_scale: 32768.0000 (35096.5839)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0847  max mem: 15572
Epoch: [21]  [ 790/2809]  eta: 0:19:12  lr: 0.000026  min_lr: 0.000000  loss: 4.1179 (4.2652)  class_acc: 0.2500 (0.2861)  loss_scale: 32768.0000 (35067.1454)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1659  max mem: 15572
Epoch: [21]  [ 800/2809]  eta: 0:19:09  lr: 0.000026  min_lr: 0.000000  loss: 4.1376 (4.2638)  class_acc: 0.2500 (0.2859)  loss_scale: 32768.0000 (35038.4419)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.2030  max mem: 15572
Epoch: [21]  [ 810/2809]  eta: 0:19:02  lr: 0.000026  min_lr: 0.000000  loss: 4.1597 (4.2639)  class_acc: 0.2917 (0.2861)  loss_scale: 32768.0000 (35010.4464)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1706  max mem: 15572
Epoch: [21]  [ 820/2809]  eta: 0:18:56  lr: 0.000026  min_lr: 0.000000  loss: 4.3419 (4.2659)  class_acc: 0.2083 (0.2846)  loss_scale: 32768.0000 (34983.1328)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1105  max mem: 15572
Epoch: [21]  [ 830/2809]  eta: 0:18:49  lr: 0.000026  min_lr: 0.000000  loss: 4.2308 (4.2657)  class_acc: 0.2500 (0.2845)  loss_scale: 32768.0000 (34956.4765)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0928  max mem: 15572
Epoch: [21]  [ 840/2809]  eta: 0:18:43  lr: 0.000026  min_lr: 0.000000  loss: 4.2229 (4.2655)  class_acc: 0.2917 (0.2843)  loss_scale: 32768.0000 (34930.4542)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0914  max mem: 15572
Epoch: [21]  [ 850/2809]  eta: 0:18:38  lr: 0.000026  min_lr: 0.000000  loss: 4.3030 (4.2665)  class_acc: 0.2917 (0.2851)  loss_scale: 32768.0000 (34905.0435)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1314  max mem: 15572
Epoch: [21]  [ 860/2809]  eta: 0:18:33  lr: 0.000026  min_lr: 0.000000  loss: 4.2699 (4.2660)  class_acc: 0.2917 (0.2850)  loss_scale: 32768.0000 (34880.2230)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1582  max mem: 15572
Epoch: [21]  [ 870/2809]  eta: 0:18:28  lr: 0.000026  min_lr: 0.000000  loss: 4.2349 (4.2659)  class_acc: 0.2917 (0.2852)  loss_scale: 32768.0000 (34855.9724)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1650  max mem: 15572
Epoch: [21]  [ 880/2809]  eta: 0:18:21  lr: 0.000026  min_lr: 0.000000  loss: 4.4057 (4.2674)  class_acc: 0.2500 (0.2847)  loss_scale: 32768.0000 (34832.2724)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1154  max mem: 15572
Epoch: [21]  [ 890/2809]  eta: 0:18:15  lr: 0.000026  min_lr: 0.000000  loss: 4.2919 (4.2663)  class_acc: 0.2917 (0.2852)  loss_scale: 32768.0000 (34809.1044)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0915  max mem: 15572
[2025-01-16 00:34:17,667] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:34:17,667] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [ 900/2809]  eta: 0:18:08  lr: 0.000026  min_lr: 0.000000  loss: 4.2152 (4.2652)  class_acc: 0.2917 (0.2857)  loss_scale: 32768.0000 (34895.5560)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.1107  max mem: 15572
Epoch: [21]  [ 910/2809]  eta: 0:18:03  lr: 0.000026  min_lr: 0.000000  loss: 4.2516 (4.2652)  class_acc: 0.2917 (0.2859)  loss_scale: 65536.0000 (35231.8946)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1332  max mem: 15572
Epoch: [21]  [ 920/2809]  eta: 0:17:58  lr: 0.000026  min_lr: 0.000000  loss: 4.2001 (4.2649)  class_acc: 0.3333 (0.2864)  loss_scale: 65536.0000 (35560.9294)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1430  max mem: 15572
[2025-01-16 00:34:31,428] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59912
[2025-01-16 00:34:31,429] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:34:31,429] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 930/2809]  eta: 0:17:50  lr: 0.000026  min_lr: 0.000000  loss: 4.2001 (4.2638)  class_acc: 0.3333 (0.2868)  loss_scale: 65536.0000 (35601.3233)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0844  max mem: 15572
Epoch: [21]  [ 940/2809]  eta: 0:17:45  lr: 0.000026  min_lr: 0.000000  loss: 4.2244 (4.2633)  class_acc: 0.3333 (0.2870)  loss_scale: 32768.0000 (35571.2136)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0785  max mem: 15572
Epoch: [21]  [ 950/2809]  eta: 0:17:39  lr: 0.000026  min_lr: 0.000000  loss: 4.1850 (4.2613)  class_acc: 0.2917 (0.2870)  loss_scale: 32768.0000 (35541.7371)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1075  max mem: 15572
Epoch: [21]  [ 960/2809]  eta: 0:17:34  lr: 0.000026  min_lr: 0.000000  loss: 4.2162 (4.2606)  class_acc: 0.2917 (0.2869)  loss_scale: 32768.0000 (35512.8741)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1506  max mem: 15572
Epoch: [21]  [ 970/2809]  eta: 0:17:30  lr: 0.000026  min_lr: 0.000000  loss: 4.1894 (4.2594)  class_acc: 0.2917 (0.2869)  loss_scale: 32768.0000 (35484.6056)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1811  max mem: 15572
Epoch: [21]  [ 980/2809]  eta: 0:17:24  lr: 0.000026  min_lr: 0.000000  loss: 4.1894 (4.2604)  class_acc: 0.2500 (0.2864)  loss_scale: 32768.0000 (35456.9134)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1437  max mem: 15572
Epoch: [21]  [ 990/2809]  eta: 0:17:18  lr: 0.000026  min_lr: 0.000000  loss: 4.3359 (4.2614)  class_acc: 0.2083 (0.2854)  loss_scale: 32768.0000 (35429.7800)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1273  max mem: 15572
Epoch: [21]  [1000/2809]  eta: 0:17:12  lr: 0.000026  min_lr: 0.000000  loss: 4.3359 (4.2617)  class_acc: 0.2083 (0.2855)  loss_scale: 32768.0000 (35403.1888)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1305  max mem: 15572
[2025-01-16 00:35:22,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=60000, skipped=375, lr=[2.5049251316672856e-07, 2.5049251316672856e-07, 3.5784644738104086e-07, 3.5784644738104086e-07, 5.112092105443442e-07, 5.112092105443442e-07, 7.30298872206206e-07, 7.30298872206206e-07, 1.0432841031517227e-06, 1.0432841031517227e-06, 1.4904058616453183e-06, 1.4904058616453183e-06, 2.1291512309218833e-06, 2.1291512309218833e-06, 3.041644615602691e-06, 3.041644615602691e-06, 4.34520659371813e-06, 4.34520659371813e-06, 6.207437991025901e-06, 6.207437991025901e-06, 8.86776855860843e-06, 8.86776855860843e-06, 1.2668240798012043e-05, 1.2668240798012043e-05, 1.8097486854302922e-05, 1.8097486854302922e-05, 2.5853552649004174e-05, 2.5853552649004174e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:35:22,500] [INFO] [timer.py:260:stop] epoch=0/micro_step=60000/global_step=60000, RunningAvgSamplesPerSec=27.81104874949741, CurrSamplesPerSec=31.521090507596316, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [21]  [1010/2809]  eta: 0:17:08  lr: 0.000026  min_lr: 0.000000  loss: 4.4250 (4.2632)  class_acc: 0.2500 (0.2853)  loss_scale: 32768.0000 (35377.1236)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1398  max mem: 15572
Epoch: [21]  [1020/2809]  eta: 0:17:01  lr: 0.000026  min_lr: 0.000000  loss: 4.4088 (4.2630)  class_acc: 0.2083 (0.2849)  loss_scale: 32768.0000 (35351.5690)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1124  max mem: 15572
Epoch: [21]  [1030/2809]  eta: 0:16:56  lr: 0.000026  min_lr: 0.000000  loss: 4.2662 (4.2641)  class_acc: 0.2917 (0.2850)  loss_scale: 32768.0000 (35326.5102)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1136  max mem: 15572
Epoch: [21]  [1040/2809]  eta: 0:16:49  lr: 0.000026  min_lr: 0.000000  loss: 4.3456 (4.2657)  class_acc: 0.2917 (0.2853)  loss_scale: 32768.0000 (35301.9328)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1004  max mem: 15572
Epoch: [21]  [1050/2809]  eta: 0:16:45  lr: 0.000026  min_lr: 0.000000  loss: 4.4404 (4.2674)  class_acc: 0.2917 (0.2853)  loss_scale: 32768.0000 (35277.8230)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1306  max mem: 15572
[2025-01-16 00:35:47,073] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:35:47,074] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [1060/2809]  eta: 0:16:39  lr: 0.000026  min_lr: 0.000000  loss: 4.2733 (4.2673)  class_acc: 0.2917 (0.2855)  loss_scale: 32768.0000 (35532.1244)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1481  max mem: 15572
Epoch: [21]  [1070/2809]  eta: 0:16:33  lr: 0.000026  min_lr: 0.000000  loss: 4.2936 (4.2679)  class_acc: 0.2917 (0.2857)  loss_scale: 65536.0000 (35812.2726)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1008  max mem: 15572
Epoch: [21]  [1080/2809]  eta: 0:16:27  lr: 0.000026  min_lr: 0.000000  loss: 4.4127 (4.2670)  class_acc: 0.2917 (0.2859)  loss_scale: 65536.0000 (36087.2377)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1271  max mem: 15572
Epoch: [21]  [1090/2809]  eta: 0:16:22  lr: 0.000026  min_lr: 0.000000  loss: 4.1644 (4.2672)  class_acc: 0.2917 (0.2859)  loss_scale: 65536.0000 (36357.1622)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1415  max mem: 15572
Epoch: [21]  [1100/2809]  eta: 0:16:16  lr: 0.000026  min_lr: 0.000000  loss: 4.1887 (4.2672)  class_acc: 0.2500 (0.2860)  loss_scale: 65536.0000 (36622.1835)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1446  max mem: 15572
Epoch: [21]  [1110/2809]  eta: 0:16:10  lr: 0.000026  min_lr: 0.000000  loss: 4.1903 (4.2669)  class_acc: 0.2500 (0.2860)  loss_scale: 65536.0000 (36882.4338)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1149  max mem: 15572
Epoch: [21]  [1120/2809]  eta: 0:16:05  lr: 0.000026  min_lr: 0.000000  loss: 4.3081 (4.2673)  class_acc: 0.2917 (0.2858)  loss_scale: 65536.0000 (37138.0410)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1130  max mem: 15572
Epoch: [21]  [1130/2809]  eta: 0:15:57  lr: 0.000026  min_lr: 0.000000  loss: 4.2944 (4.2671)  class_acc: 0.2500 (0.2856)  loss_scale: 65536.0000 (37389.1282)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0747  max mem: 15572
Epoch: [21]  [1140/2809]  eta: 0:15:53  lr: 0.000026  min_lr: 0.000000  loss: 4.1980 (4.2666)  class_acc: 0.2500 (0.2852)  loss_scale: 65536.0000 (37635.8142)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1214  max mem: 15572
[2025-01-16 00:36:39,875] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60133
[2025-01-16 00:36:39,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:36:39,876] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1150/2809]  eta: 0:15:48  lr: 0.000026  min_lr: 0.000000  loss: 4.2904 (4.2674)  class_acc: 0.2083 (0.2853)  loss_scale: 65536.0000 (37678.9296)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.1798  max mem: 15572
Epoch: [21]  [1160/2809]  eta: 0:15:42  lr: 0.000026  min_lr: 0.000000  loss: 4.2864 (4.2668)  class_acc: 0.2500 (0.2852)  loss_scale: 32768.0000 (37636.6305)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1037  max mem: 15572
Epoch: [21]  [1170/2809]  eta: 0:15:35  lr: 0.000026  min_lr: 0.000000  loss: 4.2436 (4.2671)  class_acc: 0.2500 (0.2854)  loss_scale: 32768.0000 (37595.0538)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0519  max mem: 15572
Epoch: [21]  [1180/2809]  eta: 0:15:30  lr: 0.000026  min_lr: 0.000000  loss: 4.2223 (4.2668)  class_acc: 0.2500 (0.2851)  loss_scale: 32768.0000 (37554.1812)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0859  max mem: 15572
Epoch: [21]  [1190/2809]  eta: 0:15:23  lr: 0.000026  min_lr: 0.000000  loss: 4.2182 (4.2671)  class_acc: 0.2083 (0.2848)  loss_scale: 32768.0000 (37513.9950)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1235  max mem: 15572
Epoch: [21]  [1200/2809]  eta: 0:15:18  lr: 0.000026  min_lr: 0.000000  loss: 4.2591 (4.2670)  class_acc: 0.2500 (0.2846)  loss_scale: 32768.0000 (37474.4779)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1259  max mem: 15572
Epoch: [21]  [1210/2809]  eta: 0:15:13  lr: 0.000026  min_lr: 0.000000  loss: 4.1284 (4.2649)  class_acc: 0.2917 (0.2850)  loss_scale: 32768.0000 (37435.6135)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1389  max mem: 15572
Epoch: [21]  [1220/2809]  eta: 0:15:07  lr: 0.000026  min_lr: 0.000000  loss: 4.1598 (4.2660)  class_acc: 0.2500 (0.2847)  loss_scale: 32768.0000 (37397.3857)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1366  max mem: 15572
Epoch: [21]  [1230/2809]  eta: 0:15:01  lr: 0.000026  min_lr: 0.000000  loss: 4.4103 (4.2671)  class_acc: 0.2500 (0.2845)  loss_scale: 32768.0000 (37359.7790)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1266  max mem: 15572
Epoch: [21]  [1240/2809]  eta: 0:14:55  lr: 0.000026  min_lr: 0.000000  loss: 4.3527 (4.2674)  class_acc: 0.2500 (0.2842)  loss_scale: 32768.0000 (37322.7784)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1077  max mem: 15572
Epoch: [21]  [1250/2809]  eta: 0:14:50  lr: 0.000026  min_lr: 0.000000  loss: 4.2895 (4.2670)  class_acc: 0.2917 (0.2844)  loss_scale: 32768.0000 (37286.3693)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1126  max mem: 15572
Epoch: [21]  [1260/2809]  eta: 0:14:45  lr: 0.000026  min_lr: 0.000000  loss: 4.2751 (4.2672)  class_acc: 0.2917 (0.2843)  loss_scale: 32768.0000 (37250.5377)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1919  max mem: 15572
Epoch: [21]  [1270/2809]  eta: 0:14:39  lr: 0.000026  min_lr: 0.000000  loss: 4.1775 (4.2661)  class_acc: 0.2917 (0.2844)  loss_scale: 32768.0000 (37215.2699)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1559  max mem: 15572
[2025-01-16 00:37:53,121] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:37:53,121] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 00:37:55,731] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60268
[2025-01-16 00:37:55,732] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:37:55,732] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1280/2809]  eta: 0:14:33  lr: 0.000026  min_lr: 0.000000  loss: 4.2013 (4.2666)  class_acc: 0.2917 (0.2843)  loss_scale: 32768.0000 (37334.0328)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0761  max mem: 15572
Epoch: [21]  [1290/2809]  eta: 0:14:27  lr: 0.000026  min_lr: 0.000000  loss: 4.3733 (4.2669)  class_acc: 0.2917 (0.2843)  loss_scale: 32768.0000 (37298.6646)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1203  max mem: 15572
Epoch: [21]  [1300/2809]  eta: 0:14:22  lr: 0.000026  min_lr: 0.000000  loss: 4.3546 (4.2675)  class_acc: 0.2500 (0.2841)  loss_scale: 32768.0000 (37263.8401)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1247  max mem: 15572
Epoch: [21]  [1310/2809]  eta: 0:14:15  lr: 0.000026  min_lr: 0.000000  loss: 4.2671 (4.2675)  class_acc: 0.2500 (0.2838)  loss_scale: 32768.0000 (37229.5469)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0749  max mem: 15572
Epoch: [21]  [1320/2809]  eta: 0:14:09  lr: 0.000026  min_lr: 0.000000  loss: 4.2582 (4.2665)  class_acc: 0.2500 (0.2838)  loss_scale: 32768.0000 (37195.7729)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0924  max mem: 15572
Epoch: [21]  [1330/2809]  eta: 0:14:04  lr: 0.000026  min_lr: 0.000000  loss: 4.2297 (4.2664)  class_acc: 0.2917 (0.2840)  loss_scale: 32768.0000 (37162.5064)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1529  max mem: 15572
Epoch: [21]  [1340/2809]  eta: 0:14:00  lr: 0.000026  min_lr: 0.000000  loss: 4.2471 (4.2661)  class_acc: 0.2917 (0.2841)  loss_scale: 32768.0000 (37129.7360)  weight_decay: 0.0500 (0.0500)  time: 0.6656  data: 0.2112  max mem: 15572
Epoch: [21]  [1350/2809]  eta: 0:13:53  lr: 0.000026  min_lr: 0.000000  loss: 4.1770 (4.2649)  class_acc: 0.2917 (0.2840)  loss_scale: 32768.0000 (37097.4508)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1339  max mem: 15572
Epoch: [21]  [1360/2809]  eta: 0:13:48  lr: 0.000026  min_lr: 0.000000  loss: 4.1792 (4.2647)  class_acc: 0.2083 (0.2836)  loss_scale: 32768.0000 (37065.6400)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0838  max mem: 15572
Epoch: [21]  [1370/2809]  eta: 0:13:41  lr: 0.000026  min_lr: 0.000000  loss: 4.2318 (4.2646)  class_acc: 0.2500 (0.2836)  loss_scale: 32768.0000 (37034.2932)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1229  max mem: 15572
Epoch: [21]  [1380/2809]  eta: 0:13:36  lr: 0.000026  min_lr: 0.000000  loss: 4.3188 (4.2656)  class_acc: 0.2500 (0.2834)  loss_scale: 32768.0000 (37003.4004)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0951  max mem: 15572
Epoch: [21]  [1390/2809]  eta: 0:13:30  lr: 0.000026  min_lr: 0.000000  loss: 4.2972 (4.2655)  class_acc: 0.2500 (0.2832)  loss_scale: 32768.0000 (36972.9518)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1170  max mem: 15572
Epoch: [21]  [1400/2809]  eta: 0:13:24  lr: 0.000026  min_lr: 0.000000  loss: 4.2147 (4.2656)  class_acc: 0.2917 (0.2834)  loss_scale: 32768.0000 (36942.9379)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1204  max mem: 15572
[2025-01-16 00:39:09,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:39:09,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [1410/2809]  eta: 0:13:18  lr: 0.000026  min_lr: 0.000000  loss: 4.2105 (4.2653)  class_acc: 0.2917 (0.2833)  loss_scale: 32768.0000 (36983.0191)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.0974  max mem: 15572
Epoch: [21]  [1420/2809]  eta: 0:13:11  lr: 0.000026  min_lr: 0.000000  loss: 4.2102 (4.2648)  class_acc: 0.2917 (0.2832)  loss_scale: 65536.0000 (37183.9550)  weight_decay: 0.0500 (0.0500)  time: 0.4961  data: 0.0387  max mem: 15572
[2025-01-16 00:39:17,436] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60411
[2025-01-16 00:39:17,437] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:39:17,437] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1430/2809]  eta: 0:13:06  lr: 0.000026  min_lr: 0.000000  loss: 4.1784 (4.2647)  class_acc: 0.2917 (0.2836)  loss_scale: 65536.0000 (37175.9944)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0879  max mem: 15572
Epoch: [21]  [1440/2809]  eta: 0:13:02  lr: 0.000026  min_lr: 0.000000  loss: 4.3088 (4.2648)  class_acc: 0.2500 (0.2836)  loss_scale: 32768.0000 (37145.4046)  weight_decay: 0.0500 (0.0500)  time: 0.6688  data: 0.2223  max mem: 15572
Epoch: [21]  [1450/2809]  eta: 0:12:55  lr: 0.000026  min_lr: 0.000000  loss: 4.3088 (4.2651)  class_acc: 0.2500 (0.2835)  loss_scale: 32768.0000 (37115.2364)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1633  max mem: 15572
Epoch: [21]  [1460/2809]  eta: 0:12:49  lr: 0.000026  min_lr: 0.000000  loss: 4.2815 (4.2650)  class_acc: 0.2917 (0.2837)  loss_scale: 32768.0000 (37085.4812)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0534  max mem: 15572
Epoch: [21]  [1470/2809]  eta: 0:12:44  lr: 0.000026  min_lr: 0.000000  loss: 4.3619 (4.2648)  class_acc: 0.2500 (0.2837)  loss_scale: 32768.0000 (37056.1305)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0946  max mem: 15572
Epoch: [21]  [1480/2809]  eta: 0:12:37  lr: 0.000026  min_lr: 0.000000  loss: 4.3807 (4.2645)  class_acc: 0.2083 (0.2836)  loss_scale: 32768.0000 (37027.1762)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1056  max mem: 15572
Epoch: [21]  [1490/2809]  eta: 0:12:32  lr: 0.000025  min_lr: 0.000000  loss: 4.3038 (4.2654)  class_acc: 0.2917 (0.2837)  loss_scale: 32768.0000 (36998.6103)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1078  max mem: 15572
Epoch: [21]  [1500/2809]  eta: 0:12:26  lr: 0.000025  min_lr: 0.000000  loss: 4.1989 (4.2649)  class_acc: 0.2917 (0.2841)  loss_scale: 32768.0000 (36970.4250)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1390  max mem: 15572
Epoch: [21]  [1510/2809]  eta: 0:12:20  lr: 0.000025  min_lr: 0.000000  loss: 4.2566 (4.2649)  class_acc: 0.3333 (0.2843)  loss_scale: 32768.0000 (36942.6128)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1183  max mem: 15572
Epoch: [21]  [1520/2809]  eta: 0:12:15  lr: 0.000025  min_lr: 0.000000  loss: 4.2566 (4.2650)  class_acc: 0.2500 (0.2841)  loss_scale: 32768.0000 (36915.1663)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1425  max mem: 15572
Epoch: [21]  [1530/2809]  eta: 0:12:09  lr: 0.000025  min_lr: 0.000000  loss: 4.1966 (4.2646)  class_acc: 0.2500 (0.2841)  loss_scale: 32768.0000 (36888.0784)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1127  max mem: 15572
Epoch: [21]  [1540/2809]  eta: 0:12:02  lr: 0.000025  min_lr: 0.000000  loss: 4.3056 (4.2658)  class_acc: 0.2500 (0.2836)  loss_scale: 32768.0000 (36861.3420)  weight_decay: 0.0500 (0.0500)  time: 0.4838  data: 0.0218  max mem: 15572
Epoch: [21]  [1550/2809]  eta: 0:11:58  lr: 0.000025  min_lr: 0.000000  loss: 4.4468 (4.2663)  class_acc: 0.2083 (0.2833)  loss_scale: 32768.0000 (36834.9504)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1042  max mem: 15572
[2025-01-16 00:40:29,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:40:29,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [1560/2809]  eta: 0:11:52  lr: 0.000025  min_lr: 0.000000  loss: 4.2445 (4.2664)  class_acc: 0.2083 (0.2829)  loss_scale: 32768.0000 (37018.8136)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1763  max mem: 15572
Epoch: [21]  [1570/2809]  eta: 0:11:47  lr: 0.000025  min_lr: 0.000000  loss: 4.2445 (4.2662)  class_acc: 0.2500 (0.2829)  loss_scale: 65536.0000 (37200.3361)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1644  max mem: 15572
[2025-01-16 00:40:44,934] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60566
[2025-01-16 00:40:44,934] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:40:44,934] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1580/2809]  eta: 0:11:40  lr: 0.000025  min_lr: 0.000000  loss: 4.1916 (4.2649)  class_acc: 0.2500 (0.2826)  loss_scale: 65536.0000 (37296.6578)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1258  max mem: 15572
Epoch: [21]  [1590/2809]  eta: 0:11:35  lr: 0.000025  min_lr: 0.000000  loss: 4.2121 (4.2652)  class_acc: 0.2500 (0.2831)  loss_scale: 32768.0000 (37268.1936)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1281  max mem: 15572
Epoch: [21]  [1600/2809]  eta: 0:11:30  lr: 0.000025  min_lr: 0.000000  loss: 4.2414 (4.2651)  class_acc: 0.2500 (0.2828)  loss_scale: 32768.0000 (37240.0849)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1650  max mem: 15572
Epoch: [21]  [1610/2809]  eta: 0:11:24  lr: 0.000025  min_lr: 0.000000  loss: 4.2944 (4.2651)  class_acc: 0.2500 (0.2829)  loss_scale: 32768.0000 (37212.3253)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1308  max mem: 15572
Epoch: [21]  [1620/2809]  eta: 0:11:19  lr: 0.000025  min_lr: 0.000000  loss: 4.2944 (4.2648)  class_acc: 0.2500 (0.2830)  loss_scale: 32768.0000 (37184.9081)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1191  max mem: 15572
Epoch: [21]  [1630/2809]  eta: 0:11:13  lr: 0.000025  min_lr: 0.000000  loss: 4.2616 (4.2652)  class_acc: 0.2917 (0.2832)  loss_scale: 32768.0000 (37157.8271)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1047  max mem: 15572
Epoch: [21]  [1640/2809]  eta: 0:11:07  lr: 0.000025  min_lr: 0.000000  loss: 4.2389 (4.2645)  class_acc: 0.2917 (0.2834)  loss_scale: 32768.0000 (37131.0762)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1060  max mem: 15572
Epoch: [21]  [1650/2809]  eta: 0:11:01  lr: 0.000025  min_lr: 0.000000  loss: 4.2031 (4.2654)  class_acc: 0.2917 (0.2834)  loss_scale: 32768.0000 (37104.6493)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0873  max mem: 15572
Epoch: [21]  [1660/2809]  eta: 0:10:55  lr: 0.000025  min_lr: 0.000000  loss: 4.3028 (4.2656)  class_acc: 0.2083 (0.2832)  loss_scale: 32768.0000 (37078.5406)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0930  max mem: 15572
Epoch: [21]  [1670/2809]  eta: 0:10:49  lr: 0.000025  min_lr: 0.000000  loss: 4.2858 (4.2660)  class_acc: 0.2083 (0.2826)  loss_scale: 32768.0000 (37052.7445)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1308  max mem: 15572
Epoch: [21]  [1680/2809]  eta: 0:10:43  lr: 0.000025  min_lr: 0.000000  loss: 4.2719 (4.2657)  class_acc: 0.2083 (0.2821)  loss_scale: 32768.0000 (37027.2552)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.1122  max mem: 15572
Epoch: [21]  [1690/2809]  eta: 0:10:37  lr: 0.000025  min_lr: 0.000000  loss: 4.2951 (4.2661)  class_acc: 0.2083 (0.2821)  loss_scale: 32768.0000 (37002.0674)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0959  max mem: 15572
Epoch: [21]  [1700/2809]  eta: 0:10:32  lr: 0.000025  min_lr: 0.000000  loss: 4.3707 (4.2668)  class_acc: 0.2500 (0.2819)  loss_scale: 32768.0000 (36977.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1311  max mem: 15572
[2025-01-16 00:41:56,871] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60691
[2025-01-16 00:41:56,871] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 00:41:56,871] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [1710/2809]  eta: 0:10:26  lr: 0.000025  min_lr: 0.000000  loss: 4.3259 (4.2664)  class_acc: 0.2500 (0.2820)  loss_scale: 32768.0000 (36866.3939)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1276  max mem: 15572
Epoch: [21]  [1720/2809]  eta: 0:10:21  lr: 0.000025  min_lr: 0.000000  loss: 4.2233 (4.2670)  class_acc: 0.2500 (0.2819)  loss_scale: 16384.0000 (36747.3794)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1261  max mem: 15572
Epoch: [21]  [1730/2809]  eta: 0:10:15  lr: 0.000025  min_lr: 0.000000  loss: 4.2969 (4.2669)  class_acc: 0.2500 (0.2819)  loss_scale: 16384.0000 (36629.7400)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1122  max mem: 15572
Epoch: [21]  [1740/2809]  eta: 0:10:09  lr: 0.000025  min_lr: 0.000000  loss: 4.2816 (4.2667)  class_acc: 0.2500 (0.2821)  loss_scale: 16384.0000 (36513.4520)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1121  max mem: 15572
Epoch: [21]  [1750/2809]  eta: 0:10:03  lr: 0.000025  min_lr: 0.000000  loss: 4.3479 (4.2672)  class_acc: 0.2500 (0.2818)  loss_scale: 16384.0000 (36398.4923)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1109  max mem: 15572
Epoch: [21]  [1760/2809]  eta: 0:09:58  lr: 0.000025  min_lr: 0.000000  loss: 4.3547 (4.2673)  class_acc: 0.2083 (0.2817)  loss_scale: 16384.0000 (36284.8382)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1017  max mem: 15572
Epoch: [21]  [1770/2809]  eta: 0:09:52  lr: 0.000025  min_lr: 0.000000  loss: 4.2684 (4.2672)  class_acc: 0.2917 (0.2816)  loss_scale: 16384.0000 (36172.4675)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1187  max mem: 15572
Epoch: [21]  [1780/2809]  eta: 0:09:46  lr: 0.000025  min_lr: 0.000000  loss: 4.0862 (4.2664)  class_acc: 0.2917 (0.2818)  loss_scale: 16384.0000 (36061.3588)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0820  max mem: 15572
Epoch: [21]  [1790/2809]  eta: 0:09:40  lr: 0.000025  min_lr: 0.000000  loss: 4.1648 (4.2661)  class_acc: 0.3333 (0.2820)  loss_scale: 16384.0000 (35951.4908)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1086  max mem: 15572
Epoch: [21]  [1800/2809]  eta: 0:09:35  lr: 0.000025  min_lr: 0.000000  loss: 4.1469 (4.2653)  class_acc: 0.2917 (0.2820)  loss_scale: 16384.0000 (35842.8429)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1258  max mem: 15572
Epoch: [21]  [1810/2809]  eta: 0:09:29  lr: 0.000025  min_lr: 0.000000  loss: 4.0852 (4.2656)  class_acc: 0.2083 (0.2818)  loss_scale: 16384.0000 (35735.3948)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0884  max mem: 15572
Epoch: [21]  [1820/2809]  eta: 0:09:23  lr: 0.000025  min_lr: 0.000000  loss: 4.2427 (4.2659)  class_acc: 0.2083 (0.2819)  loss_scale: 16384.0000 (35629.1269)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1276  max mem: 15572
Epoch: [21]  [1830/2809]  eta: 0:09:18  lr: 0.000025  min_lr: 0.000000  loss: 4.3654 (4.2661)  class_acc: 0.2500 (0.2817)  loss_scale: 16384.0000 (35524.0197)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1411  max mem: 15572
[2025-01-16 00:43:09,087] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:43:09,088] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [1840/2809]  eta: 0:09:12  lr: 0.000025  min_lr: 0.000000  loss: 4.3607 (4.2662)  class_acc: 0.2500 (0.2818)  loss_scale: 16384.0000 (35509.0494)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1293  max mem: 15572
Epoch: [21]  [1850/2809]  eta: 0:09:07  lr: 0.000025  min_lr: 0.000000  loss: 4.3707 (4.2673)  class_acc: 0.2500 (0.2816)  loss_scale: 32768.0000 (35494.2410)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1732  max mem: 15572
Epoch: [21]  [1860/2809]  eta: 0:09:01  lr: 0.000025  min_lr: 0.000000  loss: 4.3367 (4.2664)  class_acc: 0.2500 (0.2816)  loss_scale: 32768.0000 (35479.5916)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1676  max mem: 15572
Epoch: [21]  [1870/2809]  eta: 0:08:55  lr: 0.000025  min_lr: 0.000000  loss: 4.0284 (4.2649)  class_acc: 0.2917 (0.2818)  loss_scale: 32768.0000 (35465.0989)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1379  max mem: 15572
Epoch: [21]  [1880/2809]  eta: 0:08:50  lr: 0.000025  min_lr: 0.000000  loss: 4.0321 (4.2647)  class_acc: 0.2917 (0.2818)  loss_scale: 32768.0000 (35450.7602)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1585  max mem: 15572
Epoch: [21]  [1890/2809]  eta: 0:08:44  lr: 0.000025  min_lr: 0.000000  loss: 4.2282 (4.2645)  class_acc: 0.2500 (0.2821)  loss_scale: 32768.0000 (35436.5732)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1478  max mem: 15572
Epoch: [21]  [1900/2809]  eta: 0:08:38  lr: 0.000025  min_lr: 0.000000  loss: 4.3144 (4.2652)  class_acc: 0.3333 (0.2823)  loss_scale: 32768.0000 (35422.5355)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0950  max mem: 15572
Epoch: [21]  [1910/2809]  eta: 0:08:32  lr: 0.000025  min_lr: 0.000000  loss: 4.3144 (4.2652)  class_acc: 0.2500 (0.2822)  loss_scale: 32768.0000 (35408.6447)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1199  max mem: 15572
Epoch: [21]  [1920/2809]  eta: 0:08:27  lr: 0.000025  min_lr: 0.000000  loss: 4.2011 (4.2655)  class_acc: 0.2500 (0.2819)  loss_scale: 32768.0000 (35394.8985)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1407  max mem: 15572
Epoch: [21]  [1930/2809]  eta: 0:08:20  lr: 0.000025  min_lr: 0.000000  loss: 4.1913 (4.2654)  class_acc: 0.2500 (0.2819)  loss_scale: 32768.0000 (35381.2947)  weight_decay: 0.0500 (0.0500)  time: 0.5141  data: 0.0738  max mem: 15572
Epoch: [21]  [1940/2809]  eta: 0:08:15  lr: 0.000025  min_lr: 0.000000  loss: 4.2282 (4.2657)  class_acc: 0.2917 (0.2822)  loss_scale: 32768.0000 (35367.8310)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0453  max mem: 15572
Epoch: [21]  [1950/2809]  eta: 0:08:09  lr: 0.000025  min_lr: 0.000000  loss: 4.1027 (4.2651)  class_acc: 0.2917 (0.2823)  loss_scale: 32768.0000 (35354.5054)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1003  max mem: 15572
[2025-01-16 00:44:22,084] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:44:22,085] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [1960/2809]  eta: 0:08:03  lr: 0.000025  min_lr: 0.000000  loss: 4.0354 (4.2644)  class_acc: 0.2917 (0.2821)  loss_scale: 32768.0000 (35374.7353)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1526  max mem: 15572
Epoch: [21]  [1970/2809]  eta: 0:07:58  lr: 0.000025  min_lr: 0.000000  loss: 4.1271 (4.2644)  class_acc: 0.2500 (0.2821)  loss_scale: 65536.0000 (35527.7605)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1605  max mem: 15572
Epoch: [21]  [1980/2809]  eta: 0:07:52  lr: 0.000025  min_lr: 0.000000  loss: 4.2880 (4.2646)  class_acc: 0.2083 (0.2819)  loss_scale: 65536.0000 (35679.2408)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1314  max mem: 15572
[2025-01-16 00:44:39,692] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60978
[2025-01-16 00:44:39,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:44:39,692] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1990/2809]  eta: 0:07:46  lr: 0.000025  min_lr: 0.000000  loss: 4.2880 (4.2651)  class_acc: 0.2083 (0.2817)  loss_scale: 65536.0000 (35796.2833)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1265  max mem: 15572
Epoch: [21]  [2000/2809]  eta: 0:07:41  lr: 0.000025  min_lr: 0.000000  loss: 4.2810 (4.2649)  class_acc: 0.2917 (0.2821)  loss_scale: 32768.0000 (35781.1494)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1297  max mem: 15572
[2025-01-16 00:44:50,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=61000, skipped=381, lr=[2.432707986252642e-07, 2.432707986252642e-07, 3.4752971232180603e-07, 3.4752971232180603e-07, 4.964710176025801e-07, 4.964710176025801e-07, 7.092443108608287e-07, 7.092443108608287e-07, 1.0132061583726125e-06, 1.0132061583726125e-06, 1.4474373691037321e-06, 1.4474373691037321e-06, 2.067767670148189e-06, 2.067767670148189e-06, 2.9539538144974133e-06, 2.9539538144974133e-06, 4.2199340207105905e-06, 4.2199340207105905e-06, 6.028477172443701e-06, 6.028477172443701e-06, 8.612110246348144e-06, 8.612110246348144e-06, 1.2303014637640208e-05, 1.2303014637640208e-05, 1.7575735196628867e-05, 1.7575735196628867e-05, 2.5108193138041243e-05, 2.5108193138041243e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:44:50,740] [INFO] [timer.py:260:stop] epoch=0/micro_step=61000/global_step=61000, RunningAvgSamplesPerSec=27.818441725379902, CurrSamplesPerSec=23.91431609576056, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [21]  [2010/2809]  eta: 0:07:35  lr: 0.000025  min_lr: 0.000000  loss: 4.2689 (4.2650)  class_acc: 0.3333 (0.2824)  loss_scale: 32768.0000 (35766.1661)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0679  max mem: 15572
Epoch: [21]  [2020/2809]  eta: 0:07:29  lr: 0.000025  min_lr: 0.000000  loss: 4.4035 (4.2655)  class_acc: 0.2917 (0.2823)  loss_scale: 32768.0000 (35751.3310)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0558  max mem: 15572
Epoch: [21]  [2030/2809]  eta: 0:07:24  lr: 0.000025  min_lr: 0.000000  loss: 4.2692 (4.2654)  class_acc: 0.2500 (0.2823)  loss_scale: 32768.0000 (35736.6420)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1621  max mem: 15572
Epoch: [21]  [2040/2809]  eta: 0:07:18  lr: 0.000025  min_lr: 0.000000  loss: 4.2126 (4.2652)  class_acc: 0.2500 (0.2823)  loss_scale: 32768.0000 (35722.0970)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1675  max mem: 15572
Epoch: [21]  [2050/2809]  eta: 0:07:12  lr: 0.000025  min_lr: 0.000000  loss: 4.3486 (4.2662)  class_acc: 0.2083 (0.2821)  loss_scale: 32768.0000 (35707.6938)  weight_decay: 0.0500 (0.0500)  time: 0.5120  data: 0.0544  max mem: 15572
Epoch: [21]  [2060/2809]  eta: 0:07:06  lr: 0.000025  min_lr: 0.000000  loss: 4.3869 (4.2659)  class_acc: 0.2917 (0.2824)  loss_scale: 32768.0000 (35693.4304)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0769  max mem: 15572
Epoch: [21]  [2070/2809]  eta: 0:07:01  lr: 0.000025  min_lr: 0.000000  loss: 4.1749 (4.2662)  class_acc: 0.3333 (0.2823)  loss_scale: 32768.0000 (35679.3047)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1410  max mem: 15572
Epoch: [21]  [2080/2809]  eta: 0:06:55  lr: 0.000025  min_lr: 0.000000  loss: 4.2778 (4.2658)  class_acc: 0.3333 (0.2825)  loss_scale: 32768.0000 (35665.3148)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0961  max mem: 15572
Epoch: [21]  [2090/2809]  eta: 0:06:49  lr: 0.000025  min_lr: 0.000000  loss: 4.2167 (4.2652)  class_acc: 0.3333 (0.2826)  loss_scale: 32768.0000 (35651.4586)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1189  max mem: 15572
Epoch: [21]  [2100/2809]  eta: 0:06:43  lr: 0.000025  min_lr: 0.000000  loss: 4.1036 (4.2652)  class_acc: 0.2500 (0.2824)  loss_scale: 32768.0000 (35637.7344)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1160  max mem: 15572
Epoch: [21]  [2110/2809]  eta: 0:06:38  lr: 0.000025  min_lr: 0.000000  loss: 4.2437 (4.2652)  class_acc: 0.2083 (0.2824)  loss_scale: 32768.0000 (35624.1402)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1275  max mem: 15572
[2025-01-16 00:45:52,364] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:45:52,365] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [2120/2809]  eta: 0:06:32  lr: 0.000025  min_lr: 0.000000  loss: 4.3185 (4.2652)  class_acc: 0.2500 (0.2822)  loss_scale: 32768.0000 (35657.0222)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1215  max mem: 15572
Epoch: [21]  [2130/2809]  eta: 0:06:26  lr: 0.000025  min_lr: 0.000000  loss: 4.3224 (4.2652)  class_acc: 0.2500 (0.2821)  loss_scale: 65536.0000 (35797.2332)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0504  max mem: 15572
Epoch: [21]  [2140/2809]  eta: 0:06:21  lr: 0.000025  min_lr: 0.000000  loss: 4.2795 (4.2648)  class_acc: 0.2500 (0.2821)  loss_scale: 65536.0000 (35936.1345)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0979  max mem: 15572
[2025-01-16 00:46:08,956] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61138
[2025-01-16 00:46:08,956] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:46:08,956] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [2150/2809]  eta: 0:06:15  lr: 0.000025  min_lr: 0.000000  loss: 4.1506 (4.2644)  class_acc: 0.2500 (0.2823)  loss_scale: 65536.0000 (36043.2766)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0989  max mem: 15572
Epoch: [21]  [2160/2809]  eta: 0:06:09  lr: 0.000025  min_lr: 0.000000  loss: 4.2657 (4.2641)  class_acc: 0.2500 (0.2823)  loss_scale: 32768.0000 (36028.1203)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0958  max mem: 15572
Epoch: [21]  [2170/2809]  eta: 0:06:03  lr: 0.000025  min_lr: 0.000000  loss: 4.3390 (4.2647)  class_acc: 0.2500 (0.2822)  loss_scale: 32768.0000 (36013.1036)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0829  max mem: 15572
Epoch: [21]  [2180/2809]  eta: 0:05:57  lr: 0.000025  min_lr: 0.000000  loss: 4.2737 (4.2637)  class_acc: 0.2917 (0.2826)  loss_scale: 32768.0000 (35998.2247)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0572  max mem: 15572
Epoch: [21]  [2190/2809]  eta: 0:05:52  lr: 0.000025  min_lr: 0.000000  loss: 4.2243 (4.2641)  class_acc: 0.2917 (0.2823)  loss_scale: 32768.0000 (35983.4815)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1134  max mem: 15572
Epoch: [21]  [2200/2809]  eta: 0:05:46  lr: 0.000025  min_lr: 0.000000  loss: 4.2741 (4.2641)  class_acc: 0.2500 (0.2822)  loss_scale: 32768.0000 (35968.8723)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.1095  max mem: 15572
Epoch: [21]  [2210/2809]  eta: 0:05:40  lr: 0.000025  min_lr: 0.000000  loss: 4.2028 (4.2635)  class_acc: 0.2500 (0.2823)  loss_scale: 32768.0000 (35954.3953)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1475  max mem: 15572
Epoch: [21]  [2220/2809]  eta: 0:05:34  lr: 0.000025  min_lr: 0.000000  loss: 4.1920 (4.2635)  class_acc: 0.2500 (0.2824)  loss_scale: 32768.0000 (35940.0486)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1181  max mem: 15572
Epoch: [21]  [2230/2809]  eta: 0:05:29  lr: 0.000025  min_lr: 0.000000  loss: 4.2677 (4.2638)  class_acc: 0.2500 (0.2823)  loss_scale: 32768.0000 (35925.8306)  weight_decay: 0.0500 (0.0500)  time: 0.5224  data: 0.0812  max mem: 15572
Epoch: [21]  [2240/2809]  eta: 0:05:23  lr: 0.000025  min_lr: 0.000000  loss: 4.2171 (4.2636)  class_acc: 0.2500 (0.2823)  loss_scale: 32768.0000 (35911.7394)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1306  max mem: 15572
Epoch: [21]  [2250/2809]  eta: 0:05:17  lr: 0.000025  min_lr: 0.000000  loss: 4.2523 (4.2643)  class_acc: 0.2917 (0.2823)  loss_scale: 32768.0000 (35897.7734)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1449  max mem: 15572
Epoch: [21]  [2260/2809]  eta: 0:05:12  lr: 0.000025  min_lr: 0.000000  loss: 4.3368 (4.2644)  class_acc: 0.2500 (0.2823)  loss_scale: 32768.0000 (35883.9310)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1600  max mem: 15572
Epoch: [21]  [2270/2809]  eta: 0:05:06  lr: 0.000025  min_lr: 0.000000  loss: 4.3066 (4.2649)  class_acc: 0.2500 (0.2822)  loss_scale: 32768.0000 (35870.2105)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1210  max mem: 15572
[2025-01-16 00:47:20,869] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:47:20,869] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [2280/2809]  eta: 0:05:00  lr: 0.000025  min_lr: 0.000000  loss: 4.2460 (4.2646)  class_acc: 0.2500 (0.2822)  loss_scale: 32768.0000 (35899.7071)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0985  max mem: 15572
Epoch: [21]  [2290/2809]  eta: 0:04:55  lr: 0.000025  min_lr: 0.000000  loss: 4.2688 (4.2653)  class_acc: 0.2500 (0.2820)  loss_scale: 65536.0000 (36029.0668)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1021  max mem: 15572
Epoch: [21]  [2300/2809]  eta: 0:04:49  lr: 0.000025  min_lr: 0.000000  loss: 4.1994 (4.2644)  class_acc: 0.2500 (0.2821)  loss_scale: 65536.0000 (36157.3020)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0999  max mem: 15572
Epoch: [21]  [2310/2809]  eta: 0:04:43  lr: 0.000025  min_lr: 0.000000  loss: 4.1598 (4.2645)  class_acc: 0.2917 (0.2822)  loss_scale: 65536.0000 (36284.4275)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1138  max mem: 15572
Epoch: [21]  [2320/2809]  eta: 0:04:37  lr: 0.000025  min_lr: 0.000000  loss: 4.2483 (4.2646)  class_acc: 0.2917 (0.2824)  loss_scale: 65536.0000 (36410.4576)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1116  max mem: 15572
[2025-01-16 00:47:48,737] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61315
[2025-01-16 00:47:48,737] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:47:48,737] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [2330/2809]  eta: 0:04:32  lr: 0.000025  min_lr: 0.000000  loss: 4.2994 (4.2650)  class_acc: 0.2917 (0.2825)  loss_scale: 65536.0000 (36465.1188)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1191  max mem: 15572
Epoch: [21]  [2340/2809]  eta: 0:04:26  lr: 0.000025  min_lr: 0.000000  loss: 4.3899 (4.2650)  class_acc: 0.2500 (0.2826)  loss_scale: 32768.0000 (36449.3259)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1521  max mem: 15572
[2025-01-16 00:47:59,497] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61332
[2025-01-16 00:47:59,498] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 00:47:59,498] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [2350/2809]  eta: 0:04:21  lr: 0.000025  min_lr: 0.000000  loss: 4.2864 (4.2648)  class_acc: 0.2500 (0.2827)  loss_scale: 32768.0000 (36377.9158)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1479  max mem: 15572
Epoch: [21]  [2360/2809]  eta: 0:04:15  lr: 0.000025  min_lr: 0.000000  loss: 4.2136 (4.2648)  class_acc: 0.2083 (0.2825)  loss_scale: 16384.0000 (36293.2317)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0694  max mem: 15572
Epoch: [21]  [2370/2809]  eta: 0:04:09  lr: 0.000025  min_lr: 0.000000  loss: 4.1097 (4.2643)  class_acc: 0.2083 (0.2824)  loss_scale: 16384.0000 (36209.2619)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0489  max mem: 15572
Epoch: [21]  [2380/2809]  eta: 0:04:03  lr: 0.000025  min_lr: 0.000000  loss: 4.2983 (4.2646)  class_acc: 0.2917 (0.2826)  loss_scale: 16384.0000 (36125.9975)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1266  max mem: 15572
Epoch: [21]  [2390/2809]  eta: 0:03:58  lr: 0.000025  min_lr: 0.000000  loss: 4.2399 (4.2640)  class_acc: 0.2917 (0.2828)  loss_scale: 16384.0000 (36043.4295)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1299  max mem: 15572
Epoch: [21]  [2400/2809]  eta: 0:03:52  lr: 0.000025  min_lr: 0.000000  loss: 4.1906 (4.2644)  class_acc: 0.2917 (0.2829)  loss_scale: 16384.0000 (35961.5494)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0579  max mem: 15572
Epoch: [21]  [2410/2809]  eta: 0:03:46  lr: 0.000025  min_lr: 0.000000  loss: 4.3340 (4.2643)  class_acc: 0.2500 (0.2828)  loss_scale: 16384.0000 (35880.3484)  weight_decay: 0.0500 (0.0500)  time: 0.4909  data: 0.0465  max mem: 15572
Epoch: [21]  [2420/2809]  eta: 0:03:40  lr: 0.000025  min_lr: 0.000000  loss: 4.2777 (4.2644)  class_acc: 0.2500 (0.2828)  loss_scale: 16384.0000 (35799.8183)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.1078  max mem: 15572
Epoch: [21]  [2430/2809]  eta: 0:03:35  lr: 0.000025  min_lr: 0.000000  loss: 4.2475 (4.2641)  class_acc: 0.2917 (0.2832)  loss_scale: 16384.0000 (35719.9506)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0998  max mem: 15572
Epoch: [21]  [2440/2809]  eta: 0:03:29  lr: 0.000025  min_lr: 0.000000  loss: 4.2475 (4.2645)  class_acc: 0.2917 (0.2830)  loss_scale: 16384.0000 (35640.7374)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0688  max mem: 15572
Epoch: [21]  [2450/2809]  eta: 0:03:23  lr: 0.000025  min_lr: 0.000000  loss: 4.3014 (4.2649)  class_acc: 0.2083 (0.2828)  loss_scale: 16384.0000 (35562.1705)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.1014  max mem: 15572
Epoch: [21]  [2460/2809]  eta: 0:03:18  lr: 0.000025  min_lr: 0.000000  loss: 4.3600 (4.2650)  class_acc: 0.2500 (0.2830)  loss_scale: 16384.0000 (35484.2422)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1758  max mem: 15572
Epoch: [21]  [2470/2809]  eta: 0:03:12  lr: 0.000025  min_lr: 0.000000  loss: 4.3566 (4.2652)  class_acc: 0.3333 (0.2832)  loss_scale: 16384.0000 (35406.9446)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1767  max mem: 15572
[2025-01-16 00:49:08,321] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:49:08,322] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [2480/2809]  eta: 0:03:06  lr: 0.000025  min_lr: 0.000000  loss: 4.2005 (4.2649)  class_acc: 0.2917 (0.2834)  loss_scale: 16384.0000 (35389.7042)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1576  max mem: 15572
Epoch: [21]  [2490/2809]  eta: 0:03:01  lr: 0.000025  min_lr: 0.000000  loss: 4.2005 (4.2648)  class_acc: 0.2500 (0.2833)  loss_scale: 32768.0000 (35379.1794)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1131  max mem: 15572
Epoch: [21]  [2500/2809]  eta: 0:02:55  lr: 0.000025  min_lr: 0.000000  loss: 4.1953 (4.2642)  class_acc: 0.2500 (0.2833)  loss_scale: 32768.0000 (35368.7389)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1067  max mem: 15572
Epoch: [21]  [2510/2809]  eta: 0:02:49  lr: 0.000025  min_lr: 0.000000  loss: 4.1713 (4.2640)  class_acc: 0.2917 (0.2835)  loss_scale: 32768.0000 (35358.3815)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1385  max mem: 15572
Epoch: [21]  [2520/2809]  eta: 0:02:44  lr: 0.000025  min_lr: 0.000000  loss: 4.1711 (4.2636)  class_acc: 0.3333 (0.2837)  loss_scale: 32768.0000 (35348.1063)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0893  max mem: 15572
Epoch: [21]  [2530/2809]  eta: 0:02:38  lr: 0.000025  min_lr: 0.000000  loss: 4.2096 (4.2639)  class_acc: 0.2500 (0.2836)  loss_scale: 32768.0000 (35337.9123)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0736  max mem: 15572
Epoch: [21]  [2540/2809]  eta: 0:02:32  lr: 0.000025  min_lr: 0.000000  loss: 4.2901 (4.2641)  class_acc: 0.2500 (0.2837)  loss_scale: 32768.0000 (35327.7985)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.0491  max mem: 15572
Epoch: [21]  [2550/2809]  eta: 0:02:27  lr: 0.000025  min_lr: 0.000000  loss: 4.3237 (4.2644)  class_acc: 0.3333 (0.2839)  loss_scale: 32768.0000 (35317.7640)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0725  max mem: 15572
Epoch: [21]  [2560/2809]  eta: 0:02:21  lr: 0.000025  min_lr: 0.000000  loss: 4.3267 (4.2648)  class_acc: 0.2917 (0.2836)  loss_scale: 32768.0000 (35307.8079)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0963  max mem: 15572
Epoch: [21]  [2570/2809]  eta: 0:02:15  lr: 0.000025  min_lr: 0.000000  loss: 4.3378 (4.2647)  class_acc: 0.2083 (0.2836)  loss_scale: 32768.0000 (35297.9292)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0877  max mem: 15572
Epoch: [21]  [2580/2809]  eta: 0:02:09  lr: 0.000025  min_lr: 0.000000  loss: 4.3766 (4.2654)  class_acc: 0.2500 (0.2835)  loss_scale: 32768.0000 (35288.1271)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0932  max mem: 15572
Epoch: [21]  [2590/2809]  eta: 0:02:04  lr: 0.000025  min_lr: 0.000000  loss: 4.3866 (4.2654)  class_acc: 0.2500 (0.2836)  loss_scale: 32768.0000 (35278.4006)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0975  max mem: 15572
[2025-01-16 00:50:21,693] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:50:21,693] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [2600/2809]  eta: 0:01:58  lr: 0.000025  min_lr: 0.000000  loss: 4.2580 (4.2654)  class_acc: 0.2500 (0.2835)  loss_scale: 32768.0000 (35281.3472)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0571  max mem: 15572
Epoch: [21]  [2610/2809]  eta: 0:01:52  lr: 0.000025  min_lr: 0.000000  loss: 4.1727 (4.2650)  class_acc: 0.2500 (0.2835)  loss_scale: 65536.0000 (35397.2210)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0009  max mem: 15572
Epoch: [21]  [2620/2809]  eta: 0:01:47  lr: 0.000025  min_lr: 0.000000  loss: 4.2811 (4.2653)  class_acc: 0.3333 (0.2837)  loss_scale: 65536.0000 (35512.2106)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0458  max mem: 15572
Epoch: [21]  [2630/2809]  eta: 0:01:41  lr: 0.000025  min_lr: 0.000000  loss: 4.3348 (4.2657)  class_acc: 0.3333 (0.2838)  loss_scale: 65536.0000 (35626.3261)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0996  max mem: 15572
Epoch: [21]  [2640/2809]  eta: 0:01:36  lr: 0.000025  min_lr: 0.000000  loss: 4.2915 (4.2656)  class_acc: 0.2917 (0.2837)  loss_scale: 65536.0000 (35739.5774)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1516  max mem: 15572
[2025-01-16 00:50:50,437] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61637
[2025-01-16 00:50:50,437] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:50:50,437] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [2650/2809]  eta: 0:01:30  lr: 0.000025  min_lr: 0.000000  loss: 4.2915 (4.2659)  class_acc: 0.2500 (0.2837)  loss_scale: 65536.0000 (35814.8925)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1698  max mem: 15572
Epoch: [21]  [2660/2809]  eta: 0:01:24  lr: 0.000025  min_lr: 0.000000  loss: 4.4240 (4.2664)  class_acc: 0.2500 (0.2839)  loss_scale: 32768.0000 (35803.4423)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1615  max mem: 15572
Epoch: [21]  [2670/2809]  eta: 0:01:18  lr: 0.000025  min_lr: 0.000000  loss: 4.3855 (4.2667)  class_acc: 0.2500 (0.2838)  loss_scale: 32768.0000 (35792.0779)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1349  max mem: 15572
Epoch: [21]  [2680/2809]  eta: 0:01:13  lr: 0.000025  min_lr: 0.000000  loss: 4.3380 (4.2666)  class_acc: 0.2500 (0.2836)  loss_scale: 32768.0000 (35780.7982)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1389  max mem: 15572
Epoch: [21]  [2690/2809]  eta: 0:01:07  lr: 0.000025  min_lr: 0.000000  loss: 4.1924 (4.2662)  class_acc: 0.1667 (0.2834)  loss_scale: 32768.0000 (35769.6024)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1468  max mem: 15572
Epoch: [21]  [2700/2809]  eta: 0:01:01  lr: 0.000025  min_lr: 0.000000  loss: 4.1992 (4.2658)  class_acc: 0.3333 (0.2838)  loss_scale: 32768.0000 (35758.4894)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1073  max mem: 15572
Epoch: [21]  [2710/2809]  eta: 0:00:56  lr: 0.000025  min_lr: 0.000000  loss: 4.2262 (4.2664)  class_acc: 0.3750 (0.2839)  loss_scale: 32768.0000 (35747.4585)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1157  max mem: 15572
Epoch: [21]  [2720/2809]  eta: 0:00:50  lr: 0.000025  min_lr: 0.000000  loss: 4.3243 (4.2662)  class_acc: 0.2500 (0.2839)  loss_scale: 32768.0000 (35736.5086)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1202  max mem: 15572
Epoch: [21]  [2730/2809]  eta: 0:00:44  lr: 0.000025  min_lr: 0.000000  loss: 4.2201 (4.2658)  class_acc: 0.2500 (0.2840)  loss_scale: 32768.0000 (35725.6390)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1081  max mem: 15572
Epoch: [21]  [2740/2809]  eta: 0:00:39  lr: 0.000025  min_lr: 0.000000  loss: 4.2323 (4.2661)  class_acc: 0.2500 (0.2838)  loss_scale: 32768.0000 (35714.8486)  weight_decay: 0.0500 (0.0500)  time: 0.4786  data: 0.0502  max mem: 15572
Epoch: [21]  [2750/2809]  eta: 0:00:33  lr: 0.000025  min_lr: 0.000000  loss: 4.2533 (4.2659)  class_acc: 0.2500 (0.2839)  loss_scale: 32768.0000 (35704.1367)  weight_decay: 0.0500 (0.0500)  time: 0.4123  data: 0.0005  max mem: 15572
Epoch: [21]  [2760/2809]  eta: 0:00:27  lr: 0.000025  min_lr: 0.000000  loss: 4.2533 (4.2662)  class_acc: 0.2917 (0.2838)  loss_scale: 32768.0000 (35693.5024)  weight_decay: 0.0500 (0.0500)  time: 0.4352  data: 0.0008  max mem: 15572
Epoch: [21]  [2770/2809]  eta: 0:00:22  lr: 0.000025  min_lr: 0.000000  loss: 4.2646 (4.2660)  class_acc: 0.2917 (0.2837)  loss_scale: 32768.0000 (35682.9448)  weight_decay: 0.0500 (0.0500)  time: 0.4552  data: 0.0007  max mem: 15572
[2025-01-16 00:51:58,908] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:51:58,908] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [2780/2809]  eta: 0:00:16  lr: 0.000025  min_lr: 0.000000  loss: 4.1319 (4.2653)  class_acc: 0.3333 (0.2840)  loss_scale: 32768.0000 (35719.5944)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0675  max mem: 15572
Epoch: [21]  [2790/2809]  eta: 0:00:10  lr: 0.000025  min_lr: 0.000000  loss: 4.1089 (4.2651)  class_acc: 0.3333 (0.2841)  loss_scale: 65536.0000 (35826.4249)  weight_decay: 0.0500 (0.0500)  time: 0.6645  data: 0.1894  max mem: 15572
Epoch: [21]  [2800/2809]  eta: 0:00:05  lr: 0.000025  min_lr: 0.000000  loss: 4.1670 (4.2653)  class_acc: 0.2500 (0.2839)  loss_scale: 65536.0000 (35932.4927)  weight_decay: 0.0500 (0.0500)  time: 0.7060  data: 0.2326  max mem: 15572
Epoch: [21]  [2808/2809]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000000  loss: 4.3072 (4.2653)  class_acc: 0.2917 (0.2839)  loss_scale: 65536.0000 (36016.8031)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.2005  max mem: 15572
Epoch: [21] Total time: 0:26:33 (0.5674 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000000  loss: 4.3072 (4.2653)  class_acc: 0.2917 (0.2839)  loss_scale: 65536.0000 (36016.8031)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:24:05  loss: 1.0147 (1.0147)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.3159  data: 5.0818  max mem: 15572
Val:  [ 10/272]  eta: 0:03:51  loss: 2.7059 (2.6626)  acc1: 38.8889 (40.9091)  acc5: 66.6667 (69.6970)  time: 0.8824  data: 0.6765  max mem: 15572
Val:  [ 20/272]  eta: 0:02:30  loss: 2.7192 (2.7141)  acc1: 44.4444 (43.1217)  acc5: 66.6667 (69.3122)  time: 0.3601  data: 0.1654  max mem: 15572
Val:  [ 30/272]  eta: 0:02:00  loss: 2.7567 (2.7282)  acc1: 44.4444 (41.5771)  acc5: 66.6667 (69.8925)  time: 0.2868  data: 0.0995  max mem: 15572
Val:  [ 40/272]  eta: 0:01:48  loss: 2.6517 (2.7358)  acc1: 27.7778 (39.2954)  acc5: 77.7778 (70.4607)  time: 0.3313  data: 0.1381  max mem: 15572
Val:  [ 50/272]  eta: 0:01:39  loss: 2.5500 (2.6777)  acc1: 38.8889 (40.5229)  acc5: 77.7778 (72.4401)  time: 0.3677  data: 0.1568  max mem: 15572
Val:  [ 60/272]  eta: 0:01:33  loss: 2.0974 (2.6076)  acc1: 55.5556 (42.8051)  acc5: 83.3333 (73.6794)  time: 0.3852  data: 0.1586  max mem: 15572
Val:  [ 70/272]  eta: 0:01:25  loss: 2.1363 (2.5533)  acc1: 61.1111 (45.2269)  acc5: 83.3333 (74.8044)  time: 0.3702  data: 0.1542  max mem: 15572
Val:  [ 80/272]  eta: 0:01:19  loss: 2.3322 (2.5689)  acc1: 50.0000 (44.8560)  acc5: 77.7778 (74.2798)  time: 0.3289  data: 0.1179  max mem: 15572
Val:  [ 90/272]  eta: 0:01:13  loss: 2.7356 (2.5931)  acc1: 44.4444 (44.4444)  acc5: 72.2222 (73.9316)  time: 0.3259  data: 0.1253  max mem: 15572
Val:  [100/272]  eta: 0:01:09  loss: 2.7356 (2.6187)  acc1: 38.8889 (43.6744)  acc5: 72.2222 (73.7624)  time: 0.3770  data: 0.1888  max mem: 15572
Val:  [110/272]  eta: 0:01:04  loss: 2.7446 (2.6679)  acc1: 33.3333 (42.4424)  acc5: 66.6667 (72.5726)  time: 0.3819  data: 0.1968  max mem: 15572
Val:  [120/272]  eta: 0:00:59  loss: 3.0838 (2.7011)  acc1: 22.2222 (41.6437)  acc5: 61.1111 (71.8090)  time: 0.3361  data: 0.1485  max mem: 15572
Val:  [130/272]  eta: 0:00:54  loss: 2.5938 (2.6676)  acc1: 44.4444 (42.6633)  acc5: 77.7778 (72.8159)  time: 0.3152  data: 0.1339  max mem: 15572
Val:  [140/272]  eta: 0:00:49  loss: 2.2600 (2.6688)  acc1: 44.4444 (42.5532)  acc5: 83.3333 (72.8526)  time: 0.2703  data: 0.1089  max mem: 15572
Val:  [150/272]  eta: 0:00:44  loss: 2.5604 (2.6631)  acc1: 33.3333 (42.2001)  acc5: 77.7778 (73.2156)  time: 0.2094  data: 0.0480  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 2.5390 (2.6576)  acc1: 38.8889 (42.2360)  acc5: 77.7778 (73.7060)  time: 0.1777  data: 0.0065  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.6974 (2.6731)  acc1: 33.3333 (41.6179)  acc5: 72.2222 (73.2294)  time: 0.1896  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.6478 (2.6592)  acc1: 33.3333 (41.7434)  acc5: 72.2222 (73.5727)  time: 0.2485  data: 0.0533  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.6321 (2.6965)  acc1: 27.7778 (40.4305)  acc5: 66.6667 (72.4258)  time: 0.3120  data: 0.1035  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.7478 (2.6983)  acc1: 27.7778 (40.0498)  acc5: 61.1111 (72.3881)  time: 0.3020  data: 0.0806  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.4302 (2.7019)  acc1: 38.8889 (40.2317)  acc5: 77.7778 (72.3539)  time: 0.3111  data: 0.1105  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.7121 (2.6955)  acc1: 44.4444 (40.4475)  acc5: 77.7778 (72.5993)  time: 0.3570  data: 0.1665  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.3374 (2.6823)  acc1: 50.0000 (41.1736)  acc5: 77.7778 (72.9437)  time: 0.3386  data: 0.1390  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.2433 (2.6696)  acc1: 50.0000 (41.3555)  acc5: 88.8889 (73.4440)  time: 0.3153  data: 0.1181  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.5525 (2.6767)  acc1: 27.7778 (40.7481)  acc5: 77.7778 (73.3510)  time: 0.3343  data: 0.1371  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 2.1637 (2.6346)  acc1: 72.2222 (42.3585)  acc5: 83.3333 (74.0528)  time: 0.2957  data: 0.1103  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.0394 (2.6309)  acc1: 61.1111 (42.3739)  acc5: 83.3333 (74.2517)  time: 0.2007  data: 0.0437  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.0394 (2.6341)  acc1: 61.1111 (42.3510)  acc5: 83.3333 (74.2167)  time: 0.1924  data: 0.0436  max mem: 15572
Val: Total time: 0:01:29 (0.3273 s / it)
* Acc@1 42.351 Acc@5 74.217 loss 2.634
Accuracy of the network on the 4883 val videos: 42.4%
[2025-01-16 00:53:47,237] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 00:53:47,240] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 00:53:47,240] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 00:53:49,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 00:53:49,198] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 42.35%
Epoch: [22]  [   0/2809]  eta: 5:11:23  lr: 0.000025  min_lr: 0.000000  loss: 3.9527 (3.9527)  class_acc: 0.2500 (0.2500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.6513  data: 6.1690  max mem: 15572
Epoch: [22]  [  10/2809]  eta: 0:54:10  lr: 0.000025  min_lr: 0.000000  loss: 4.1621 (4.1815)  class_acc: 0.2500 (0.2652)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1613  data: 0.7088  max mem: 15572
Epoch: [22]  [  20/2809]  eta: 0:42:54  lr: 0.000024  min_lr: 0.000000  loss: 4.1621 (4.1951)  class_acc: 0.2917 (0.2956)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1985  max mem: 15572
[2025-01-16 00:54:11,721] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61824
[2025-01-16 00:54:11,723] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:54:11,724] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [  30/2809]  eta: 0:36:42  lr: 0.000024  min_lr: 0.000000  loss: 4.1964 (4.2155)  class_acc: 0.2917 (0.3132)  loss_scale: 65536.0000 (60250.8387)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1488  max mem: 15572
Epoch: [22]  [  40/2809]  eta: 0:33:14  lr: 0.000024  min_lr: 0.000000  loss: 4.2121 (4.2219)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (53547.7073)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0724  max mem: 15572
Epoch: [22]  [  50/2809]  eta: 0:32:16  lr: 0.000024  min_lr: 0.000000  loss: 4.3091 (4.2245)  class_acc: 0.3750 (0.3325)  loss_scale: 32768.0000 (49473.2549)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1348  max mem: 15572
Epoch: [22]  [  60/2809]  eta: 0:31:13  lr: 0.000024  min_lr: 0.000000  loss: 4.3663 (4.2343)  class_acc: 0.3333 (0.3306)  loss_scale: 32768.0000 (46734.6885)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1589  max mem: 15572
Epoch: [22]  [  70/2809]  eta: 0:30:19  lr: 0.000024  min_lr: 0.000000  loss: 4.2857 (4.2549)  class_acc: 0.2500 (0.3134)  loss_scale: 32768.0000 (44767.5493)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1223  max mem: 15572
Epoch: [22]  [  80/2809]  eta: 0:29:51  lr: 0.000024  min_lr: 0.000000  loss: 4.2741 (4.2532)  class_acc: 0.2083 (0.3076)  loss_scale: 32768.0000 (43286.1235)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1419  max mem: 15572
Epoch: [22]  [  90/2809]  eta: 0:29:09  lr: 0.000024  min_lr: 0.000000  loss: 4.1316 (4.2580)  class_acc: 0.2917 (0.3059)  loss_scale: 32768.0000 (42130.2857)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1313  max mem: 15572
Epoch: [22]  [ 100/2809]  eta: 0:28:33  lr: 0.000024  min_lr: 0.000000  loss: 4.3450 (4.2690)  class_acc: 0.2917 (0.3016)  loss_scale: 32768.0000 (41203.3267)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0963  max mem: 15572
Epoch: [22]  [ 110/2809]  eta: 0:28:15  lr: 0.000024  min_lr: 0.000000  loss: 4.3798 (4.2761)  class_acc: 0.2500 (0.3011)  loss_scale: 32768.0000 (40443.3874)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1243  max mem: 15572
Epoch: [22]  [ 120/2809]  eta: 0:27:32  lr: 0.000024  min_lr: 0.000000  loss: 4.2839 (4.2829)  class_acc: 0.2500 (0.2992)  loss_scale: 32768.0000 (39809.0579)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0990  max mem: 15572
Epoch: [22]  [ 130/2809]  eta: 0:27:02  lr: 0.000024  min_lr: 0.000000  loss: 4.2792 (4.2833)  class_acc: 0.2500 (0.2945)  loss_scale: 32768.0000 (39271.5725)  weight_decay: 0.0500 (0.0500)  time: 0.4796  data: 0.0479  max mem: 15572
Epoch: [22]  [ 140/2809]  eta: 0:26:42  lr: 0.000024  min_lr: 0.000000  loss: 4.2998 (4.2893)  class_acc: 0.2500 (0.2928)  loss_scale: 32768.0000 (38810.3262)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0460  max mem: 15572
Epoch: [22]  [ 150/2809]  eta: 0:26:39  lr: 0.000024  min_lr: 0.000000  loss: 4.3428 (4.3019)  class_acc: 0.2500 (0.2906)  loss_scale: 32768.0000 (38410.1722)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1065  max mem: 15572
[2025-01-16 00:55:23,874] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:55:23,875] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 160/2809]  eta: 0:26:35  lr: 0.000024  min_lr: 0.000000  loss: 4.3382 (4.2968)  class_acc: 0.2500 (0.2886)  loss_scale: 32768.0000 (39280.8944)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1586  max mem: 15572
Epoch: [22]  [ 170/2809]  eta: 0:26:27  lr: 0.000024  min_lr: 0.000000  loss: 4.1733 (4.2983)  class_acc: 0.2500 (0.2890)  loss_scale: 65536.0000 (40816.2807)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1574  max mem: 15572
[2025-01-16 00:55:34,743] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61974
[2025-01-16 00:55:34,743] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:55:34,744] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 180/2809]  eta: 0:26:22  lr: 0.000024  min_lr: 0.000000  loss: 4.3404 (4.2974)  class_acc: 0.2917 (0.2926)  loss_scale: 65536.0000 (41276.8177)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1709  max mem: 15572
Epoch: [22]  [ 190/2809]  eta: 0:26:07  lr: 0.000024  min_lr: 0.000000  loss: 4.2044 (4.2939)  class_acc: 0.3750 (0.2965)  loss_scale: 32768.0000 (40831.3298)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1428  max mem: 15572
Epoch: [22]  [ 200/2809]  eta: 0:26:04  lr: 0.000024  min_lr: 0.000000  loss: 4.3166 (4.2944)  class_acc: 0.3333 (0.2971)  loss_scale: 32768.0000 (40430.1692)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1522  max mem: 15572
[2025-01-16 00:55:50,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=62000, skipped=387, lr=[2.360327889193882e-07, 2.360327889193882e-07, 3.371896984562689e-07, 3.371896984562689e-07, 4.816995692232414e-07, 4.816995692232414e-07, 6.881422417474877e-07, 6.881422417474877e-07, 9.830603453535539e-07, 9.830603453535539e-07, 1.4043719219336484e-06, 1.4043719219336484e-06, 2.006245602762355e-06, 2.006245602762355e-06, 2.8660651468033645e-06, 2.8660651468033645e-06, 4.094378781147663e-06, 4.094378781147663e-06, 5.849112544496663e-06, 5.849112544496663e-06, 8.35587506356666e-06, 8.35587506356666e-06, 1.1936964376523803e-05, 1.1936964376523803e-05, 1.705280625217686e-05, 1.705280625217686e-05, 2.436115178882409e-05, 2.436115178882409e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:55:50,568] [INFO] [timer.py:260:stop] epoch=0/micro_step=62000/global_step=62000, RunningAvgSamplesPerSec=27.823014581377606, CurrSamplesPerSec=31.083596883713607, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [ 210/2809]  eta: 0:26:00  lr: 0.000024  min_lr: 0.000000  loss: 4.3166 (4.2943)  class_acc: 0.2917 (0.2974)  loss_scale: 32768.0000 (40067.0332)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1751  max mem: 15572
Epoch: [22]  [ 220/2809]  eta: 0:25:39  lr: 0.000024  min_lr: 0.000000  loss: 4.2684 (4.2954)  class_acc: 0.2917 (0.2998)  loss_scale: 32768.0000 (39736.7602)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0869  max mem: 15572
Epoch: [22]  [ 230/2809]  eta: 0:25:44  lr: 0.000024  min_lr: 0.000000  loss: 4.2279 (4.2880)  class_acc: 0.3333 (0.2994)  loss_scale: 32768.0000 (39435.0823)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1284  max mem: 15572
Epoch: [22]  [ 240/2809]  eta: 0:25:29  lr: 0.000024  min_lr: 0.000000  loss: 4.2688 (4.2900)  class_acc: 0.3333 (0.2994)  loss_scale: 32768.0000 (39158.4398)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1480  max mem: 15572
Epoch: [22]  [ 250/2809]  eta: 0:25:22  lr: 0.000024  min_lr: 0.000000  loss: 4.2881 (4.2875)  class_acc: 0.3333 (0.2986)  loss_scale: 32768.0000 (38903.8406)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0947  max mem: 15572
Epoch: [22]  [ 260/2809]  eta: 0:25:12  lr: 0.000024  min_lr: 0.000000  loss: 4.1897 (4.2921)  class_acc: 0.2917 (0.2965)  loss_scale: 32768.0000 (38668.7510)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1083  max mem: 15572
Epoch: [22]  [ 270/2809]  eta: 0:25:00  lr: 0.000024  min_lr: 0.000000  loss: 4.2457 (4.2920)  class_acc: 0.2917 (0.2967)  loss_scale: 32768.0000 (38451.0111)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0941  max mem: 15572
Epoch: [22]  [ 280/2809]  eta: 0:24:45  lr: 0.000024  min_lr: 0.000000  loss: 4.2457 (4.2900)  class_acc: 0.2500 (0.2961)  loss_scale: 32768.0000 (38248.7687)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0726  max mem: 15572
Epoch: [22]  [ 290/2809]  eta: 0:24:37  lr: 0.000024  min_lr: 0.000000  loss: 4.2875 (4.2916)  class_acc: 0.2500 (0.2951)  loss_scale: 32768.0000 (38060.4261)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0706  max mem: 15572
Epoch: [22]  [ 300/2809]  eta: 0:24:24  lr: 0.000024  min_lr: 0.000000  loss: 4.2688 (4.2947)  class_acc: 0.2917 (0.2967)  loss_scale: 32768.0000 (37884.5980)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0910  max mem: 15572
[2025-01-16 00:56:47,097] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:56:47,098] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 310/2809]  eta: 0:24:14  lr: 0.000024  min_lr: 0.000000  loss: 4.2149 (4.2882)  class_acc: 0.3750 (0.2982)  loss_scale: 32768.0000 (38352.2572)  weight_decay: 0.0500 (0.0500)  time: 0.5178  data: 0.0865  max mem: 15572
Epoch: [22]  [ 320/2809]  eta: 0:24:04  lr: 0.000024  min_lr: 0.000000  loss: 4.2149 (4.2902)  class_acc: 0.3333 (0.2969)  loss_scale: 65536.0000 (39199.1028)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0885  max mem: 15572
Epoch: [22]  [ 330/2809]  eta: 0:23:56  lr: 0.000024  min_lr: 0.000000  loss: 4.2388 (4.2905)  class_acc: 0.2917 (0.2966)  loss_scale: 65536.0000 (39994.7795)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.1071  max mem: 15572
Epoch: [22]  [ 340/2809]  eta: 0:23:49  lr: 0.000024  min_lr: 0.000000  loss: 4.2321 (4.2901)  class_acc: 0.2917 (0.2968)  loss_scale: 65536.0000 (40743.7889)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1157  max mem: 15572
[2025-01-16 00:57:10,707] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62146
[2025-01-16 00:57:10,708] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:57:10,708] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 350/2809]  eta: 0:23:39  lr: 0.000024  min_lr: 0.000000  loss: 4.1644 (4.2898)  class_acc: 0.2917 (0.2967)  loss_scale: 65536.0000 (41170.0513)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0912  max mem: 15572
Epoch: [22]  [ 360/2809]  eta: 0:23:36  lr: 0.000024  min_lr: 0.000000  loss: 4.1705 (4.2872)  class_acc: 0.2917 (0.2962)  loss_scale: 32768.0000 (40937.3075)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1286  max mem: 15572
Epoch: [22]  [ 370/2809]  eta: 0:23:24  lr: 0.000024  min_lr: 0.000000  loss: 4.2464 (4.2857)  class_acc: 0.2917 (0.2967)  loss_scale: 32768.0000 (40717.1105)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1210  max mem: 15572
Epoch: [22]  [ 380/2809]  eta: 0:23:15  lr: 0.000024  min_lr: 0.000000  loss: 4.2448 (4.2834)  class_acc: 0.2917 (0.2975)  loss_scale: 32768.0000 (40508.4724)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0795  max mem: 15572
Epoch: [22]  [ 390/2809]  eta: 0:23:08  lr: 0.000024  min_lr: 0.000000  loss: 4.2448 (4.2834)  class_acc: 0.2917 (0.2972)  loss_scale: 32768.0000 (40310.5064)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.1074  max mem: 15572
Epoch: [22]  [ 400/2809]  eta: 0:23:09  lr: 0.000024  min_lr: 0.000000  loss: 4.2584 (4.2799)  class_acc: 0.2917 (0.2988)  loss_scale: 32768.0000 (40122.4140)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1800  max mem: 15572
Epoch: [22]  [ 410/2809]  eta: 0:23:07  lr: 0.000024  min_lr: 0.000000  loss: 4.2327 (4.2785)  class_acc: 0.3333 (0.2992)  loss_scale: 32768.0000 (39943.4745)  weight_decay: 0.0500 (0.0500)  time: 0.6589  data: 0.2036  max mem: 15572
Epoch: [22]  [ 420/2809]  eta: 0:23:00  lr: 0.000024  min_lr: 0.000000  loss: 4.1607 (4.2735)  class_acc: 0.2917 (0.2987)  loss_scale: 32768.0000 (39773.0356)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1371  max mem: 15572
Epoch: [22]  [ 430/2809]  eta: 0:22:57  lr: 0.000024  min_lr: 0.000000  loss: 4.1513 (4.2728)  class_acc: 0.2500 (0.2980)  loss_scale: 32768.0000 (39610.5058)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1465  max mem: 15572
Epoch: [22]  [ 440/2809]  eta: 0:22:48  lr: 0.000024  min_lr: 0.000000  loss: 4.2505 (4.2726)  class_acc: 0.2500 (0.2982)  loss_scale: 32768.0000 (39455.3469)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1416  max mem: 15572
Epoch: [22]  [ 450/2809]  eta: 0:22:41  lr: 0.000024  min_lr: 0.000000  loss: 4.2706 (4.2721)  class_acc: 0.2500 (0.2968)  loss_scale: 32768.0000 (39307.0687)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0777  max mem: 15572
Epoch: [22]  [ 460/2809]  eta: 0:22:35  lr: 0.000024  min_lr: 0.000000  loss: 4.1664 (4.2701)  class_acc: 0.2917 (0.2979)  loss_scale: 32768.0000 (39165.2234)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0904  max mem: 15572
Epoch: [22]  [ 470/2809]  eta: 0:22:28  lr: 0.000024  min_lr: 0.000000  loss: 4.1611 (4.2712)  class_acc: 0.2500 (0.2965)  loss_scale: 32768.0000 (39029.4013)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1157  max mem: 15572
[2025-01-16 00:58:24,131] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:58:24,131] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 480/2809]  eta: 0:22:21  lr: 0.000024  min_lr: 0.000000  loss: 4.1174 (4.2671)  class_acc: 0.2917 (0.2984)  loss_scale: 32768.0000 (39171.7256)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1062  max mem: 15572
Epoch: [22]  [ 490/2809]  eta: 0:22:16  lr: 0.000024  min_lr: 0.000000  loss: 4.2264 (4.2681)  class_acc: 0.3333 (0.2980)  loss_scale: 65536.0000 (39708.6762)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1276  max mem: 15572
Epoch: [22]  [ 500/2809]  eta: 0:22:09  lr: 0.000024  min_lr: 0.000000  loss: 4.3423 (4.2687)  class_acc: 0.2917 (0.2985)  loss_scale: 65536.0000 (40224.1916)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1281  max mem: 15572
Epoch: [22]  [ 510/2809]  eta: 0:22:08  lr: 0.000024  min_lr: 0.000000  loss: 4.2356 (4.2637)  class_acc: 0.3333 (0.2998)  loss_scale: 65536.0000 (40719.5303)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1668  max mem: 15572
Epoch: [22]  [ 520/2809]  eta: 0:21:58  lr: 0.000024  min_lr: 0.000000  loss: 4.0830 (4.2599)  class_acc: 0.2917 (0.2989)  loss_scale: 65536.0000 (41195.8541)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1310  max mem: 15572
Epoch: [22]  [ 530/2809]  eta: 0:21:57  lr: 0.000024  min_lr: 0.000000  loss: 4.1651 (4.2589)  class_acc: 0.2500 (0.2979)  loss_scale: 65536.0000 (41654.2373)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1496  max mem: 15572
[2025-01-16 00:58:58,620] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62333
[2025-01-16 00:58:58,621] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:58:58,621] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 540/2809]  eta: 0:21:50  lr: 0.000024  min_lr: 0.000000  loss: 4.3806 (4.2603)  class_acc: 0.2500 (0.2972)  loss_scale: 65536.0000 (41732.2588)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1845  max mem: 15572
Epoch: [22]  [ 550/2809]  eta: 0:21:45  lr: 0.000024  min_lr: 0.000000  loss: 4.3016 (4.2575)  class_acc: 0.2500 (0.2967)  loss_scale: 32768.0000 (41569.5681)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1294  max mem: 15572
Epoch: [22]  [ 560/2809]  eta: 0:21:36  lr: 0.000024  min_lr: 0.000000  loss: 4.1174 (4.2550)  class_acc: 0.2500 (0.2964)  loss_scale: 32768.0000 (41412.6774)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1225  max mem: 15572
Epoch: [22]  [ 570/2809]  eta: 0:21:32  lr: 0.000024  min_lr: 0.000000  loss: 4.2856 (4.2574)  class_acc: 0.2500 (0.2959)  loss_scale: 32768.0000 (41261.2820)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1325  max mem: 15572
Epoch: [22]  [ 580/2809]  eta: 0:21:26  lr: 0.000024  min_lr: 0.000000  loss: 4.2229 (4.2545)  class_acc: 0.3333 (0.2981)  loss_scale: 32768.0000 (41115.0981)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1484  max mem: 15572
Epoch: [22]  [ 590/2809]  eta: 0:21:20  lr: 0.000024  min_lr: 0.000000  loss: 4.1871 (4.2543)  class_acc: 0.3333 (0.2989)  loss_scale: 32768.0000 (40973.8613)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1186  max mem: 15572
Epoch: [22]  [ 600/2809]  eta: 0:21:12  lr: 0.000024  min_lr: 0.000000  loss: 4.3224 (4.2557)  class_acc: 0.2917 (0.2985)  loss_scale: 32768.0000 (40837.3245)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0985  max mem: 15572
Epoch: [22]  [ 610/2809]  eta: 0:21:02  lr: 0.000024  min_lr: 0.000000  loss: 4.3224 (4.2549)  class_acc: 0.2500 (0.2984)  loss_scale: 32768.0000 (40705.2570)  weight_decay: 0.0500 (0.0500)  time: 0.4914  data: 0.0471  max mem: 15572
Epoch: [22]  [ 620/2809]  eta: 0:20:55  lr: 0.000024  min_lr: 0.000000  loss: 4.1758 (4.2549)  class_acc: 0.2500 (0.2989)  loss_scale: 32768.0000 (40577.4428)  weight_decay: 0.0500 (0.0500)  time: 0.4941  data: 0.0478  max mem: 15572
Epoch: [22]  [ 630/2809]  eta: 0:20:53  lr: 0.000024  min_lr: 0.000000  loss: 4.3360 (4.2567)  class_acc: 0.2500 (0.2995)  loss_scale: 32768.0000 (40453.6799)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1727  max mem: 15572
Epoch: [22]  [ 640/2809]  eta: 0:20:45  lr: 0.000024  min_lr: 0.000000  loss: 4.3777 (4.2588)  class_acc: 0.2500 (0.2991)  loss_scale: 32768.0000 (40333.7785)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1523  max mem: 15572
Epoch: [22]  [ 650/2809]  eta: 0:20:38  lr: 0.000024  min_lr: 0.000000  loss: 4.3777 (4.2586)  class_acc: 0.3333 (0.2990)  loss_scale: 32768.0000 (40217.5607)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0666  max mem: 15572
Epoch: [22]  [ 660/2809]  eta: 0:20:32  lr: 0.000024  min_lr: 0.000000  loss: 4.3777 (4.2611)  class_acc: 0.2500 (0.2980)  loss_scale: 32768.0000 (40104.8593)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1176  max mem: 15572
[2025-01-16 01:00:11,163] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:00:11,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 670/2809]  eta: 0:20:25  lr: 0.000024  min_lr: 0.000000  loss: 4.3088 (4.2589)  class_acc: 0.2083 (0.2981)  loss_scale: 32768.0000 (40337.3592)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1107  max mem: 15572
Epoch: [22]  [ 680/2809]  eta: 0:20:16  lr: 0.000024  min_lr: 0.000000  loss: 4.2632 (4.2598)  class_acc: 0.2917 (0.2985)  loss_scale: 65536.0000 (40707.3833)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0538  max mem: 15572
Epoch: [22]  [ 690/2809]  eta: 0:20:10  lr: 0.000024  min_lr: 0.000000  loss: 4.2632 (4.2596)  class_acc: 0.3333 (0.2992)  loss_scale: 65536.0000 (41066.6975)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0776  max mem: 15572
[2025-01-16 01:00:26,705] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62490
[2025-01-16 01:00:26,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:00:26,705] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 700/2809]  eta: 0:20:05  lr: 0.000024  min_lr: 0.000000  loss: 4.2888 (4.2610)  class_acc: 0.3333 (0.2999)  loss_scale: 65536.0000 (40995.0585)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1391  max mem: 15572
Epoch: [22]  [ 710/2809]  eta: 0:20:04  lr: 0.000024  min_lr: 0.000000  loss: 4.2888 (4.2607)  class_acc: 0.2917 (0.2996)  loss_scale: 32768.0000 (40879.3474)  weight_decay: 0.0500 (0.0500)  time: 0.6499  data: 0.2013  max mem: 15572
Epoch: [22]  [ 720/2809]  eta: 0:19:58  lr: 0.000024  min_lr: 0.000000  loss: 4.2075 (4.2611)  class_acc: 0.2917 (0.2993)  loss_scale: 32768.0000 (40766.8460)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.1795  max mem: 15572
Epoch: [22]  [ 730/2809]  eta: 0:19:51  lr: 0.000024  min_lr: 0.000000  loss: 4.2605 (4.2628)  class_acc: 0.2500 (0.2987)  loss_scale: 32768.0000 (40657.4227)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1110  max mem: 15572
Epoch: [22]  [ 740/2809]  eta: 0:19:45  lr: 0.000024  min_lr: 0.000000  loss: 4.4054 (4.2641)  class_acc: 0.2500 (0.2984)  loss_scale: 32768.0000 (40550.9528)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1079  max mem: 15572
Epoch: [22]  [ 750/2809]  eta: 0:19:38  lr: 0.000024  min_lr: 0.000000  loss: 4.2758 (4.2641)  class_acc: 0.2500 (0.2977)  loss_scale: 32768.0000 (40447.3182)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0858  max mem: 15572
Epoch: [22]  [ 760/2809]  eta: 0:19:33  lr: 0.000024  min_lr: 0.000000  loss: 4.1683 (4.2623)  class_acc: 0.2917 (0.2983)  loss_scale: 32768.0000 (40346.4074)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0900  max mem: 15572
Epoch: [22]  [ 770/2809]  eta: 0:19:27  lr: 0.000024  min_lr: 0.000000  loss: 4.1758 (4.2626)  class_acc: 0.3333 (0.2986)  loss_scale: 32768.0000 (40248.1141)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1113  max mem: 15572
Epoch: [22]  [ 780/2809]  eta: 0:19:21  lr: 0.000024  min_lr: 0.000000  loss: 4.1766 (4.2616)  class_acc: 0.2917 (0.2984)  loss_scale: 32768.0000 (40152.3380)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1230  max mem: 15572
Epoch: [22]  [ 790/2809]  eta: 0:19:16  lr: 0.000024  min_lr: 0.000000  loss: 4.1834 (4.2602)  class_acc: 0.2917 (0.2984)  loss_scale: 32768.0000 (40058.9836)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1360  max mem: 15572
Epoch: [22]  [ 800/2809]  eta: 0:19:11  lr: 0.000024  min_lr: 0.000000  loss: 4.2076 (4.2604)  class_acc: 0.3333 (0.2991)  loss_scale: 32768.0000 (39967.9600)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1477  max mem: 15572
Epoch: [22]  [ 810/2809]  eta: 0:19:05  lr: 0.000024  min_lr: 0.000000  loss: 4.3129 (4.2604)  class_acc: 0.3333 (0.2994)  loss_scale: 32768.0000 (39879.1813)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1429  max mem: 15572
Epoch: [22]  [ 820/2809]  eta: 0:19:00  lr: 0.000024  min_lr: 0.000000  loss: 4.2403 (4.2598)  class_acc: 0.3333 (0.2996)  loss_scale: 32768.0000 (39792.5652)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1318  max mem: 15572
[2025-01-16 01:01:40,764] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:01:40,765] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 830/2809]  eta: 0:18:52  lr: 0.000024  min_lr: 0.000000  loss: 4.1967 (4.2580)  class_acc: 0.3333 (0.2999)  loss_scale: 32768.0000 (40102.3538)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0869  max mem: 15572
Epoch: [22]  [ 840/2809]  eta: 0:18:45  lr: 0.000024  min_lr: 0.000000  loss: 4.1099 (4.2572)  class_acc: 0.2917 (0.2992)  loss_scale: 65536.0000 (40404.7753)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0549  max mem: 15572
[2025-01-16 01:01:53,488] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62643
[2025-01-16 01:01:53,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:01:53,489] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 850/2809]  eta: 0:18:38  lr: 0.000024  min_lr: 0.000000  loss: 4.2750 (4.2561)  class_acc: 0.2917 (0.2995)  loss_scale: 65536.0000 (40469.0576)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0842  max mem: 15572
Epoch: [22]  [ 860/2809]  eta: 0:18:33  lr: 0.000024  min_lr: 0.000000  loss: 4.3521 (4.2581)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (40379.6144)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1267  max mem: 15572
Epoch: [22]  [ 870/2809]  eta: 0:18:28  lr: 0.000024  min_lr: 0.000000  loss: 4.4284 (4.2589)  class_acc: 0.2500 (0.2988)  loss_scale: 32768.0000 (40292.2250)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1394  max mem: 15572
Epoch: [22]  [ 880/2809]  eta: 0:18:23  lr: 0.000024  min_lr: 0.000000  loss: 4.3920 (4.2600)  class_acc: 0.2500 (0.2988)  loss_scale: 32768.0000 (40206.8195)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1291  max mem: 15572
Epoch: [22]  [ 890/2809]  eta: 0:18:15  lr: 0.000024  min_lr: 0.000000  loss: 4.2512 (4.2594)  class_acc: 0.2917 (0.2986)  loss_scale: 32768.0000 (40123.3311)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0681  max mem: 15572
Epoch: [22]  [ 900/2809]  eta: 0:18:09  lr: 0.000024  min_lr: 0.000000  loss: 4.1759 (4.2583)  class_acc: 0.2917 (0.2984)  loss_scale: 32768.0000 (40041.6959)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0631  max mem: 15572
Epoch: [22]  [ 910/2809]  eta: 0:18:02  lr: 0.000024  min_lr: 0.000000  loss: 4.1730 (4.2578)  class_acc: 0.2917 (0.2987)  loss_scale: 32768.0000 (39961.8529)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0922  max mem: 15572
Epoch: [22]  [ 920/2809]  eta: 0:17:55  lr: 0.000024  min_lr: 0.000000  loss: 4.2470 (4.2591)  class_acc: 0.2500 (0.2986)  loss_scale: 32768.0000 (39883.7438)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0631  max mem: 15572
Epoch: [22]  [ 930/2809]  eta: 0:17:51  lr: 0.000024  min_lr: 0.000000  loss: 4.2470 (4.2584)  class_acc: 0.2500 (0.2981)  loss_scale: 32768.0000 (39807.3126)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1209  max mem: 15572
Epoch: [22]  [ 940/2809]  eta: 0:17:45  lr: 0.000024  min_lr: 0.000000  loss: 4.2428 (4.2577)  class_acc: 0.2917 (0.2981)  loss_scale: 32768.0000 (39732.5058)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1671  max mem: 15572
Epoch: [22]  [ 950/2809]  eta: 0:17:42  lr: 0.000024  min_lr: 0.000000  loss: 4.2171 (4.2578)  class_acc: 0.2917 (0.2986)  loss_scale: 32768.0000 (39659.2723)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.2139  max mem: 15572
Epoch: [22]  [ 960/2809]  eta: 0:17:35  lr: 0.000024  min_lr: 0.000000  loss: 4.1461 (4.2564)  class_acc: 0.3333 (0.2986)  loss_scale: 32768.0000 (39587.5630)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1346  max mem: 15572
Epoch: [22]  [ 970/2809]  eta: 0:17:28  lr: 0.000024  min_lr: 0.000000  loss: 4.2058 (4.2558)  class_acc: 0.2083 (0.2978)  loss_scale: 32768.0000 (39517.3306)  weight_decay: 0.0500 (0.0500)  time: 0.4973  data: 0.0400  max mem: 15572
[2025-01-16 01:03:06,236] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:03:06,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 980/2809]  eta: 0:17:22  lr: 0.000024  min_lr: 0.000000  loss: 4.3640 (4.2565)  class_acc: 0.2083 (0.2974)  loss_scale: 32768.0000 (39682.3486)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0979  max mem: 15572
Epoch: [22]  [ 990/2809]  eta: 0:17:17  lr: 0.000024  min_lr: 0.000000  loss: 4.3878 (4.2582)  class_acc: 0.2083 (0.2966)  loss_scale: 65536.0000 (39943.2331)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1254  max mem: 15572
[2025-01-16 01:03:20,559] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62798
[2025-01-16 01:03:20,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:03:20,560] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1000/2809]  eta: 0:17:11  lr: 0.000024  min_lr: 0.000000  loss: 4.3134 (4.2580)  class_acc: 0.2500 (0.2966)  loss_scale: 65536.0000 (40166.1698)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1266  max mem: 15572
Epoch: [22]  [1010/2809]  eta: 0:17:04  lr: 0.000024  min_lr: 0.000000  loss: 4.1571 (4.2571)  class_acc: 0.2917 (0.2967)  loss_scale: 32768.0000 (40092.9931)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0946  max mem: 15572
Epoch: [22]  [1020/2809]  eta: 0:16:57  lr: 0.000024  min_lr: 0.000000  loss: 4.1450 (4.2569)  class_acc: 0.2500 (0.2963)  loss_scale: 32768.0000 (40021.2498)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0526  max mem: 15572
Epoch: [22]  [1030/2809]  eta: 0:16:51  lr: 0.000024  min_lr: 0.000000  loss: 4.3117 (4.2583)  class_acc: 0.2083 (0.2960)  loss_scale: 32768.0000 (39950.8982)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0813  max mem: 15572
Epoch: [22]  [1040/2809]  eta: 0:16:47  lr: 0.000024  min_lr: 0.000000  loss: 4.3133 (4.2594)  class_acc: 0.2917 (0.2959)  loss_scale: 32768.0000 (39881.8982)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1874  max mem: 15572
Epoch: [22]  [1050/2809]  eta: 0:16:41  lr: 0.000024  min_lr: 0.000000  loss: 4.1799 (4.2586)  class_acc: 0.2917 (0.2961)  loss_scale: 32768.0000 (39814.2112)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1726  max mem: 15572
Epoch: [22]  [1060/2809]  eta: 0:16:36  lr: 0.000024  min_lr: 0.000000  loss: 4.2596 (4.2588)  class_acc: 0.3333 (0.2961)  loss_scale: 32768.0000 (39747.8002)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1301  max mem: 15572
Epoch: [22]  [1070/2809]  eta: 0:16:30  lr: 0.000024  min_lr: 0.000000  loss: 4.2879 (4.2590)  class_acc: 0.2500 (0.2963)  loss_scale: 32768.0000 (39682.6293)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1351  max mem: 15572
Epoch: [22]  [1080/2809]  eta: 0:16:23  lr: 0.000024  min_lr: 0.000000  loss: 4.2350 (4.2595)  class_acc: 0.2500 (0.2959)  loss_scale: 32768.0000 (39618.6642)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0830  max mem: 15572
Epoch: [22]  [1090/2809]  eta: 0:16:18  lr: 0.000024  min_lr: 0.000000  loss: 4.1514 (4.2588)  class_acc: 0.2917 (0.2964)  loss_scale: 32768.0000 (39555.8717)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1164  max mem: 15572
Epoch: [22]  [1100/2809]  eta: 0:16:13  lr: 0.000024  min_lr: 0.000000  loss: 4.1645 (4.2596)  class_acc: 0.2917 (0.2959)  loss_scale: 32768.0000 (39494.2198)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1662  max mem: 15572
Epoch: [22]  [1110/2809]  eta: 0:16:06  lr: 0.000024  min_lr: 0.000000  loss: 4.2995 (4.2598)  class_acc: 0.2500 (0.2956)  loss_scale: 32768.0000 (39433.6778)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1418  max mem: 15572
Epoch: [22]  [1120/2809]  eta: 0:16:00  lr: 0.000024  min_lr: 0.000000  loss: 4.2017 (4.2588)  class_acc: 0.2500 (0.2955)  loss_scale: 32768.0000 (39374.2159)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1149  max mem: 15572
[2025-01-16 01:04:31,638] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:04:31,638] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [1130/2809]  eta: 0:15:56  lr: 0.000024  min_lr: 0.000000  loss: 4.2529 (4.2603)  class_acc: 0.2500 (0.2945)  loss_scale: 32768.0000 (39373.7507)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1624  max mem: 15572
Epoch: [22]  [1140/2809]  eta: 0:15:50  lr: 0.000024  min_lr: 0.000000  loss: 4.2848 (4.2602)  class_acc: 0.2083 (0.2940)  loss_scale: 65536.0000 (39603.0429)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1489  max mem: 15572
[2025-01-16 01:04:46,262] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62948
[2025-01-16 01:04:46,262] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:04:46,262] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1150/2809]  eta: 0:15:46  lr: 0.000024  min_lr: 0.000000  loss: 4.2216 (4.2598)  class_acc: 0.2083 (0.2939)  loss_scale: 65536.0000 (39799.8818)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1476  max mem: 15572
Epoch: [22]  [1160/2809]  eta: 0:15:39  lr: 0.000024  min_lr: 0.000000  loss: 4.2545 (4.2603)  class_acc: 0.2500 (0.2937)  loss_scale: 32768.0000 (39739.3144)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1313  max mem: 15572
Epoch: [22]  [1170/2809]  eta: 0:15:36  lr: 0.000024  min_lr: 0.000000  loss: 4.2545 (4.2597)  class_acc: 0.2917 (0.2940)  loss_scale: 32768.0000 (39679.7814)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1664  max mem: 15572
Epoch: [22]  [1180/2809]  eta: 0:15:29  lr: 0.000024  min_lr: 0.000000  loss: 4.1330 (4.2590)  class_acc: 0.3333 (0.2942)  loss_scale: 32768.0000 (39621.2566)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1662  max mem: 15572
Epoch: [22]  [1190/2809]  eta: 0:15:25  lr: 0.000024  min_lr: 0.000000  loss: 4.2403 (4.2585)  class_acc: 0.3333 (0.2942)  loss_scale: 32768.0000 (39563.7145)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1484  max mem: 15572
Epoch: [22]  [1200/2809]  eta: 0:15:19  lr: 0.000024  min_lr: 0.000000  loss: 4.2792 (4.2585)  class_acc: 0.2917 (0.2944)  loss_scale: 32768.0000 (39507.1307)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1878  max mem: 15572
[2025-01-16 01:05:16,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=63000, skipped=393, lr=[2.2878587401415915e-07, 2.2878587401415915e-07, 3.2683696287737025e-07, 3.2683696287737025e-07, 4.6690994696767183e-07, 4.6690994696767183e-07, 6.670142099538169e-07, 6.670142099538169e-07, 9.528774427911671e-07, 9.528774427911671e-07, 1.3612534897016674e-06, 1.3612534897016674e-06, 1.9446478424309535e-06, 1.9446478424309535e-06, 2.778068346329934e-06, 2.778068346329934e-06, 3.96866906618562e-06, 3.96866906618562e-06, 5.6695272374080294e-06, 5.6695272374080294e-06, 8.099324624868612e-06, 8.099324624868612e-06, 1.1570463749812306e-05, 1.1570463749812306e-05, 1.6529233928303293e-05, 1.6529233928303293e-05, 2.3613191326147565e-05, 2.3613191326147565e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:05:16,209] [INFO] [timer.py:260:stop] epoch=0/micro_step=63000/global_step=63000, RunningAvgSamplesPerSec=27.833713060213277, CurrSamplesPerSec=31.325712009559847, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [1210/2809]  eta: 0:15:13  lr: 0.000024  min_lr: 0.000000  loss: 4.3775 (4.2581)  class_acc: 0.3333 (0.2947)  loss_scale: 32768.0000 (39451.4814)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1234  max mem: 15572
Epoch: [22]  [1220/2809]  eta: 0:15:07  lr: 0.000024  min_lr: 0.000000  loss: 4.3775 (4.2582)  class_acc: 0.3333 (0.2944)  loss_scale: 32768.0000 (39396.7437)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0931  max mem: 15572
Epoch: [22]  [1230/2809]  eta: 0:15:00  lr: 0.000024  min_lr: 0.000000  loss: 4.3273 (4.2585)  class_acc: 0.2083 (0.2941)  loss_scale: 32768.0000 (39342.8952)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0679  max mem: 15572
Epoch: [22]  [1240/2809]  eta: 0:14:55  lr: 0.000024  min_lr: 0.000000  loss: 4.3650 (4.2590)  class_acc: 0.2500 (0.2944)  loss_scale: 32768.0000 (39289.9146)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1145  max mem: 15572
Epoch: [22]  [1250/2809]  eta: 0:14:49  lr: 0.000024  min_lr: 0.000000  loss: 4.3780 (4.2599)  class_acc: 0.2917 (0.2944)  loss_scale: 32768.0000 (39237.7810)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1459  max mem: 15572
Epoch: [22]  [1260/2809]  eta: 0:14:43  lr: 0.000024  min_lr: 0.000000  loss: 4.3251 (4.2604)  class_acc: 0.2917 (0.2944)  loss_scale: 32768.0000 (39186.4742)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1150  max mem: 15572
Epoch: [22]  [1270/2809]  eta: 0:14:37  lr: 0.000024  min_lr: 0.000000  loss: 4.1787 (4.2587)  class_acc: 0.2917 (0.2946)  loss_scale: 32768.0000 (39135.9748)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0916  max mem: 15572
[2025-01-16 01:05:59,653] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:05:59,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [1280/2809]  eta: 0:14:31  lr: 0.000024  min_lr: 0.000000  loss: 4.2470 (4.2597)  class_acc: 0.2917 (0.2943)  loss_scale: 32768.0000 (39137.4239)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1348  max mem: 15572
[2025-01-16 01:06:03,546] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63081
[2025-01-16 01:06:03,546] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:06:03,546] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1290/2809]  eta: 0:14:27  lr: 0.000024  min_lr: 0.000000  loss: 4.4612 (4.2617)  class_acc: 0.2917 (0.2945)  loss_scale: 32768.0000 (39138.8505)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1881  max mem: 15572
Epoch: [22]  [1300/2809]  eta: 0:14:20  lr: 0.000024  min_lr: 0.000000  loss: 4.4396 (4.2621)  class_acc: 0.2500 (0.2940)  loss_scale: 32768.0000 (39089.8816)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1113  max mem: 15572
Epoch: [22]  [1310/2809]  eta: 0:14:14  lr: 0.000024  min_lr: 0.000000  loss: 4.1223 (4.2610)  class_acc: 0.2917 (0.2941)  loss_scale: 32768.0000 (39041.6598)  weight_decay: 0.0500 (0.0500)  time: 0.4944  data: 0.0526  max mem: 15572
Epoch: [22]  [1320/2809]  eta: 0:14:08  lr: 0.000024  min_lr: 0.000000  loss: 4.1070 (4.2608)  class_acc: 0.2500 (0.2935)  loss_scale: 32768.0000 (38994.1681)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0796  max mem: 15572
Epoch: [22]  [1330/2809]  eta: 0:14:01  lr: 0.000024  min_lr: 0.000000  loss: 4.3200 (4.2611)  class_acc: 0.2083 (0.2934)  loss_scale: 32768.0000 (38947.3899)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0576  max mem: 15572
Epoch: [22]  [1340/2809]  eta: 0:13:55  lr: 0.000024  min_lr: 0.000000  loss: 4.2896 (4.2612)  class_acc: 0.2500 (0.2936)  loss_scale: 32768.0000 (38901.3095)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0567  max mem: 15572
Epoch: [22]  [1350/2809]  eta: 0:13:49  lr: 0.000024  min_lr: 0.000000  loss: 4.2692 (4.2605)  class_acc: 0.3333 (0.2945)  loss_scale: 32768.0000 (38855.9112)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0753  max mem: 15572
Epoch: [22]  [1360/2809]  eta: 0:13:43  lr: 0.000023  min_lr: 0.000000  loss: 4.2321 (4.2596)  class_acc: 0.3333 (0.2945)  loss_scale: 32768.0000 (38811.1800)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1080  max mem: 15572
Epoch: [22]  [1370/2809]  eta: 0:13:38  lr: 0.000023  min_lr: 0.000000  loss: 4.2254 (4.2597)  class_acc: 0.2500 (0.2942)  loss_scale: 32768.0000 (38767.1014)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1729  max mem: 15572
Epoch: [22]  [1380/2809]  eta: 0:13:32  lr: 0.000023  min_lr: 0.000000  loss: 4.2254 (4.2597)  class_acc: 0.2500 (0.2941)  loss_scale: 32768.0000 (38723.6611)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1698  max mem: 15572
Epoch: [22]  [1390/2809]  eta: 0:13:27  lr: 0.000023  min_lr: 0.000000  loss: 4.2943 (4.2598)  class_acc: 0.2500 (0.2938)  loss_scale: 32768.0000 (38680.8454)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1699  max mem: 15572
Epoch: [22]  [1400/2809]  eta: 0:13:21  lr: 0.000023  min_lr: 0.000000  loss: 4.2980 (4.2605)  class_acc: 0.2500 (0.2938)  loss_scale: 32768.0000 (38638.6410)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1219  max mem: 15572
Epoch: [22]  [1410/2809]  eta: 0:13:16  lr: 0.000023  min_lr: 0.000000  loss: 4.3451 (4.2611)  class_acc: 0.3333 (0.2941)  loss_scale: 32768.0000 (38597.0347)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1209  max mem: 15572
[2025-01-16 01:07:14,711] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:07:14,711] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [1420/2809]  eta: 0:13:10  lr: 0.000023  min_lr: 0.000000  loss: 4.3825 (4.2620)  class_acc: 0.3333 (0.2943)  loss_scale: 32768.0000 (38763.5524)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1515  max mem: 15572
Epoch: [22]  [1430/2809]  eta: 0:13:05  lr: 0.000023  min_lr: 0.000000  loss: 4.3337 (4.2620)  class_acc: 0.2917 (0.2941)  loss_scale: 65536.0000 (38950.6415)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1254  max mem: 15572
Epoch: [22]  [1440/2809]  eta: 0:12:59  lr: 0.000023  min_lr: 0.000000  loss: 4.3322 (4.2628)  class_acc: 0.2917 (0.2943)  loss_scale: 65536.0000 (39135.1339)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1466  max mem: 15572
[2025-01-16 01:07:34,328] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63245
[2025-01-16 01:07:34,328] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:07:34,328] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1450/2809]  eta: 0:12:53  lr: 0.000023  min_lr: 0.000000  loss: 4.3720 (4.2632)  class_acc: 0.2917 (0.2938)  loss_scale: 65536.0000 (39226.7512)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1203  max mem: 15572
Epoch: [22]  [1460/2809]  eta: 0:12:48  lr: 0.000023  min_lr: 0.000000  loss: 4.2872 (4.2630)  class_acc: 0.2500 (0.2937)  loss_scale: 32768.0000 (39182.5435)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1181  max mem: 15572
Epoch: [22]  [1470/2809]  eta: 0:12:43  lr: 0.000023  min_lr: 0.000000  loss: 4.2872 (4.2636)  class_acc: 0.2083 (0.2933)  loss_scale: 32768.0000 (39138.9368)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1571  max mem: 15572
Epoch: [22]  [1480/2809]  eta: 0:12:37  lr: 0.000023  min_lr: 0.000000  loss: 4.2099 (4.2622)  class_acc: 0.2500 (0.2931)  loss_scale: 32768.0000 (39095.9190)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1428  max mem: 15572
Epoch: [22]  [1490/2809]  eta: 0:12:31  lr: 0.000023  min_lr: 0.000000  loss: 4.1581 (4.2617)  class_acc: 0.2500 (0.2930)  loss_scale: 32768.0000 (39053.4782)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1068  max mem: 15572
Epoch: [22]  [1500/2809]  eta: 0:12:25  lr: 0.000023  min_lr: 0.000000  loss: 4.2874 (4.2624)  class_acc: 0.2917 (0.2933)  loss_scale: 32768.0000 (39011.6029)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1148  max mem: 15572
Epoch: [22]  [1510/2809]  eta: 0:12:20  lr: 0.000023  min_lr: 0.000000  loss: 4.4028 (4.2636)  class_acc: 0.2917 (0.2930)  loss_scale: 32768.0000 (38970.2819)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1262  max mem: 15572
Epoch: [22]  [1520/2809]  eta: 0:12:14  lr: 0.000023  min_lr: 0.000000  loss: 4.3360 (4.2629)  class_acc: 0.2917 (0.2930)  loss_scale: 32768.0000 (38929.5043)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0938  max mem: 15572
Epoch: [22]  [1530/2809]  eta: 0:12:08  lr: 0.000023  min_lr: 0.000000  loss: 4.2335 (4.2634)  class_acc: 0.2917 (0.2930)  loss_scale: 32768.0000 (38889.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1123  max mem: 15572
Epoch: [22]  [1540/2809]  eta: 0:12:02  lr: 0.000023  min_lr: 0.000000  loss: 4.3966 (4.2640)  class_acc: 0.2500 (0.2929)  loss_scale: 32768.0000 (38849.5367)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1546  max mem: 15572
Epoch: [22]  [1550/2809]  eta: 0:11:56  lr: 0.000023  min_lr: 0.000000  loss: 4.2823 (4.2640)  class_acc: 0.2500 (0.2927)  loss_scale: 32768.0000 (38810.3262)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1042  max mem: 15572
Epoch: [22]  [1560/2809]  eta: 0:11:51  lr: 0.000023  min_lr: 0.000000  loss: 4.1228 (4.2629)  class_acc: 0.2917 (0.2932)  loss_scale: 32768.0000 (38771.6182)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0951  max mem: 15572
Epoch: [22]  [1570/2809]  eta: 0:11:44  lr: 0.000023  min_lr: 0.000000  loss: 4.0879 (4.2628)  class_acc: 0.3333 (0.2933)  loss_scale: 32768.0000 (38733.4029)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0857  max mem: 15572
[2025-01-16 01:08:46,578] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:08:46,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [1580/2809]  eta: 0:11:39  lr: 0.000023  min_lr: 0.000000  loss: 4.2413 (4.2628)  class_acc: 0.3333 (0.2930)  loss_scale: 32768.0000 (38799.3017)  weight_decay: 0.0500 (0.0500)  time: 0.5185  data: 0.0618  max mem: 15572
Epoch: [22]  [1590/2809]  eta: 0:11:32  lr: 0.000023  min_lr: 0.000000  loss: 4.2192 (4.2628)  class_acc: 0.2917 (0.2928)  loss_scale: 65536.0000 (38967.3514)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0765  max mem: 15572
[2025-01-16 01:08:55,051] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63390
[2025-01-16 01:08:55,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:08:55,052] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1600/2809]  eta: 0:11:27  lr: 0.000023  min_lr: 0.000000  loss: 4.2019 (4.2621)  class_acc: 0.2917 (0.2927)  loss_scale: 65536.0000 (38949.0968)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0911  max mem: 15572
Epoch: [22]  [1610/2809]  eta: 0:11:21  lr: 0.000023  min_lr: 0.000000  loss: 4.2803 (4.2628)  class_acc: 0.2083 (0.2926)  loss_scale: 32768.0000 (38910.7287)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1082  max mem: 15572
Epoch: [22]  [1620/2809]  eta: 0:11:15  lr: 0.000023  min_lr: 0.000000  loss: 4.2539 (4.2625)  class_acc: 0.2917 (0.2932)  loss_scale: 32768.0000 (38872.8341)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1185  max mem: 15572
Epoch: [22]  [1630/2809]  eta: 0:11:09  lr: 0.000023  min_lr: 0.000000  loss: 4.2869 (4.2628)  class_acc: 0.2917 (0.2928)  loss_scale: 32768.0000 (38835.4040)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0829  max mem: 15572
Epoch: [22]  [1640/2809]  eta: 0:11:03  lr: 0.000023  min_lr: 0.000000  loss: 4.3012 (4.2624)  class_acc: 0.2500 (0.2930)  loss_scale: 32768.0000 (38798.4302)  weight_decay: 0.0500 (0.0500)  time: 0.5274  data: 0.0674  max mem: 15572
Epoch: [22]  [1650/2809]  eta: 0:10:58  lr: 0.000023  min_lr: 0.000000  loss: 4.3308 (4.2633)  class_acc: 0.3333 (0.2930)  loss_scale: 32768.0000 (38761.9043)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1427  max mem: 15572
Epoch: [22]  [1660/2809]  eta: 0:10:52  lr: 0.000023  min_lr: 0.000000  loss: 4.3769 (4.2640)  class_acc: 0.2917 (0.2930)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1303  max mem: 15572
Epoch: [22]  [1670/2809]  eta: 0:10:47  lr: 0.000023  min_lr: 0.000000  loss: 4.2306 (4.2633)  class_acc: 0.2500 (0.2932)  loss_scale: 32768.0000 (38690.1640)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1254  max mem: 15572
Epoch: [22]  [1680/2809]  eta: 0:10:41  lr: 0.000023  min_lr: 0.000000  loss: 4.2363 (4.2639)  class_acc: 0.2917 (0.2932)  loss_scale: 32768.0000 (38654.9340)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1544  max mem: 15572
Epoch: [22]  [1690/2809]  eta: 0:10:36  lr: 0.000023  min_lr: 0.000000  loss: 4.2567 (4.2638)  class_acc: 0.2917 (0.2932)  loss_scale: 32768.0000 (38620.1206)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1880  max mem: 15572
Epoch: [22]  [1700/2809]  eta: 0:10:31  lr: 0.000023  min_lr: 0.000000  loss: 4.2853 (4.2641)  class_acc: 0.3333 (0.2935)  loss_scale: 32768.0000 (38585.7166)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.1895  max mem: 15572
Epoch: [22]  [1710/2809]  eta: 0:10:25  lr: 0.000023  min_lr: 0.000000  loss: 4.3752 (4.2647)  class_acc: 0.3333 (0.2934)  loss_scale: 32768.0000 (38551.7148)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1461  max mem: 15572
Epoch: [22]  [1720/2809]  eta: 0:10:19  lr: 0.000023  min_lr: 0.000000  loss: 4.2442 (4.2644)  class_acc: 0.2917 (0.2935)  loss_scale: 32768.0000 (38518.1081)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0990  max mem: 15572
[2025-01-16 01:10:09,374] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:10:09,375] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [1730/2809]  eta: 0:10:13  lr: 0.000023  min_lr: 0.000000  loss: 4.1781 (4.2647)  class_acc: 0.2917 (0.2937)  loss_scale: 32768.0000 (38674.1906)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0983  max mem: 15572
Epoch: [22]  [1740/2809]  eta: 0:10:08  lr: 0.000023  min_lr: 0.000000  loss: 4.2595 (4.2645)  class_acc: 0.2917 (0.2936)  loss_scale: 65536.0000 (38828.4802)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1510  max mem: 15572
Epoch: [22]  [1750/2809]  eta: 0:10:02  lr: 0.000023  min_lr: 0.000000  loss: 4.2568 (4.2640)  class_acc: 0.2500 (0.2934)  loss_scale: 65536.0000 (38981.0074)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1107  max mem: 15572
Epoch: [22]  [1760/2809]  eta: 0:09:56  lr: 0.000023  min_lr: 0.000000  loss: 4.2568 (4.2641)  class_acc: 0.2500 (0.2931)  loss_scale: 65536.0000 (39131.8024)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1172  max mem: 15572
Epoch: [22]  [1770/2809]  eta: 0:09:51  lr: 0.000023  min_lr: 0.000000  loss: 4.2565 (4.2645)  class_acc: 0.2500 (0.2929)  loss_scale: 65536.0000 (39280.8944)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1490  max mem: 15572
[2025-01-16 01:10:40,966] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63575
[2025-01-16 01:10:40,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:10:40,966] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1780/2809]  eta: 0:09:45  lr: 0.000023  min_lr: 0.000000  loss: 4.1946 (4.2646)  class_acc: 0.2500 (0.2927)  loss_scale: 65536.0000 (39354.7176)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1250  max mem: 15572
Epoch: [22]  [1790/2809]  eta: 0:09:40  lr: 0.000023  min_lr: 0.000000  loss: 4.1887 (4.2645)  class_acc: 0.2500 (0.2926)  loss_scale: 32768.0000 (39317.9408)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1557  max mem: 15572
Epoch: [22]  [1800/2809]  eta: 0:09:34  lr: 0.000023  min_lr: 0.000000  loss: 4.2673 (4.2647)  class_acc: 0.2917 (0.2930)  loss_scale: 32768.0000 (39281.5725)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1533  max mem: 15572
Epoch: [22]  [1810/2809]  eta: 0:09:28  lr: 0.000023  min_lr: 0.000000  loss: 4.2339 (4.2643)  class_acc: 0.2917 (0.2931)  loss_scale: 32768.0000 (39245.6057)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1366  max mem: 15572
Epoch: [22]  [1820/2809]  eta: 0:09:23  lr: 0.000023  min_lr: 0.000000  loss: 4.2339 (4.2649)  class_acc: 0.2917 (0.2928)  loss_scale: 32768.0000 (39210.0340)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1485  max mem: 15572
Epoch: [22]  [1830/2809]  eta: 0:09:17  lr: 0.000023  min_lr: 0.000000  loss: 4.1925 (4.2647)  class_acc: 0.2500 (0.2927)  loss_scale: 32768.0000 (39174.8509)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1600  max mem: 15572
Epoch: [22]  [1840/2809]  eta: 0:09:11  lr: 0.000023  min_lr: 0.000000  loss: 4.2017 (4.2651)  class_acc: 0.2500 (0.2929)  loss_scale: 32768.0000 (39140.0500)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1117  max mem: 15572
Epoch: [22]  [1850/2809]  eta: 0:09:06  lr: 0.000023  min_lr: 0.000000  loss: 4.2785 (4.2649)  class_acc: 0.2917 (0.2928)  loss_scale: 32768.0000 (39105.6251)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0750  max mem: 15572
Epoch: [22]  [1860/2809]  eta: 0:09:00  lr: 0.000023  min_lr: 0.000000  loss: 4.2794 (4.2653)  class_acc: 0.2917 (0.2929)  loss_scale: 32768.0000 (39071.5701)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1414  max mem: 15572
Epoch: [22]  [1870/2809]  eta: 0:08:55  lr: 0.000023  min_lr: 0.000000  loss: 4.3356 (4.2659)  class_acc: 0.2500 (0.2924)  loss_scale: 32768.0000 (39037.8792)  weight_decay: 0.0500 (0.0500)  time: 0.6641  data: 0.1981  max mem: 15572
Epoch: [22]  [1880/2809]  eta: 0:08:49  lr: 0.000023  min_lr: 0.000000  loss: 4.3155 (4.2660)  class_acc: 0.2500 (0.2925)  loss_scale: 32768.0000 (39004.5465)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1174  max mem: 15572
Epoch: [22]  [1890/2809]  eta: 0:08:44  lr: 0.000023  min_lr: 0.000000  loss: 4.2971 (4.2658)  class_acc: 0.2917 (0.2926)  loss_scale: 32768.0000 (38971.5664)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1197  max mem: 15572
Epoch: [22]  [1900/2809]  eta: 0:08:38  lr: 0.000023  min_lr: 0.000000  loss: 4.2540 (4.2659)  class_acc: 0.3333 (0.2928)  loss_scale: 32768.0000 (38938.9332)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1988  max mem: 15572
[2025-01-16 01:11:57,136] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:11:57,136] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [1910/2809]  eta: 0:08:32  lr: 0.000023  min_lr: 0.000000  loss: 4.3058 (4.2661)  class_acc: 0.2917 (0.2926)  loss_scale: 32768.0000 (38992.3768)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1472  max mem: 15572
[2025-01-16 01:12:03,767] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63716
[2025-01-16 01:12:03,768] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:12:03,768] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1920/2809]  eta: 0:08:26  lr: 0.000023  min_lr: 0.000000  loss: 4.3351 (4.2667)  class_acc: 0.2500 (0.2925)  loss_scale: 65536.0000 (39079.3795)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.1080  max mem: 15572
Epoch: [22]  [1930/2809]  eta: 0:08:21  lr: 0.000023  min_lr: 0.000000  loss: 4.4123 (4.2669)  class_acc: 0.2500 (0.2925)  loss_scale: 32768.0000 (39046.6950)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0797  max mem: 15572
Epoch: [22]  [1940/2809]  eta: 0:08:15  lr: 0.000023  min_lr: 0.000000  loss: 4.4114 (4.2675)  class_acc: 0.2917 (0.2925)  loss_scale: 32768.0000 (39014.3472)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0910  max mem: 15572
Epoch: [22]  [1950/2809]  eta: 0:08:09  lr: 0.000023  min_lr: 0.000000  loss: 4.2849 (4.2673)  class_acc: 0.2500 (0.2922)  loss_scale: 32768.0000 (38982.3311)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1256  max mem: 15572
Epoch: [22]  [1960/2809]  eta: 0:08:04  lr: 0.000023  min_lr: 0.000000  loss: 4.2607 (4.2674)  class_acc: 0.2500 (0.2920)  loss_scale: 32768.0000 (38950.6415)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1906  max mem: 15572
Epoch: [22]  [1970/2809]  eta: 0:07:58  lr: 0.000023  min_lr: 0.000000  loss: 4.3498 (4.2676)  class_acc: 0.2917 (0.2921)  loss_scale: 32768.0000 (38919.2735)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1205  max mem: 15572
Epoch: [22]  [1980/2809]  eta: 0:07:53  lr: 0.000023  min_lr: 0.000000  loss: 4.2821 (4.2679)  class_acc: 0.3333 (0.2921)  loss_scale: 32768.0000 (38888.2221)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1075  max mem: 15572
Epoch: [22]  [1990/2809]  eta: 0:07:47  lr: 0.000023  min_lr: 0.000000  loss: 4.2696 (4.2681)  class_acc: 0.2500 (0.2921)  loss_scale: 32768.0000 (38857.4827)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1692  max mem: 15572
Epoch: [22]  [2000/2809]  eta: 0:07:41  lr: 0.000023  min_lr: 0.000000  loss: 4.2117 (4.2674)  class_acc: 0.2500 (0.2919)  loss_scale: 32768.0000 (38827.0505)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1491  max mem: 15572
Epoch: [22]  [2010/2809]  eta: 0:07:35  lr: 0.000023  min_lr: 0.000000  loss: 4.2313 (4.2675)  class_acc: 0.2500 (0.2920)  loss_scale: 32768.0000 (38796.9209)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1258  max mem: 15572
Epoch: [22]  [2020/2809]  eta: 0:07:30  lr: 0.000023  min_lr: 0.000000  loss: 4.2812 (4.2675)  class_acc: 0.2500 (0.2917)  loss_scale: 32768.0000 (38767.0896)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1250  max mem: 15572
Epoch: [22]  [2030/2809]  eta: 0:07:24  lr: 0.000023  min_lr: 0.000000  loss: 4.1849 (4.2673)  class_acc: 0.2500 (0.2917)  loss_scale: 32768.0000 (38737.5519)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1188  max mem: 15572
Epoch: [22]  [2040/2809]  eta: 0:07:18  lr: 0.000023  min_lr: 0.000000  loss: 4.0945 (4.2663)  class_acc: 0.2500 (0.2918)  loss_scale: 32768.0000 (38708.3038)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0944  max mem: 15572
[2025-01-16 01:13:17,890] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:13:17,890] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [2050/2809]  eta: 0:07:13  lr: 0.000023  min_lr: 0.000000  loss: 4.1969 (4.2670)  class_acc: 0.2500 (0.2917)  loss_scale: 32768.0000 (38743.2472)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1545  max mem: 15572
Epoch: [22]  [2060/2809]  eta: 0:07:07  lr: 0.000023  min_lr: 0.000000  loss: 4.3221 (4.2664)  class_acc: 0.2500 (0.2921)  loss_scale: 65536.0000 (38873.2460)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1202  max mem: 15572
Epoch: [22]  [2070/2809]  eta: 0:07:01  lr: 0.000023  min_lr: 0.000000  loss: 4.2403 (4.2671)  class_acc: 0.2083 (0.2914)  loss_scale: 65536.0000 (39001.9894)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0806  max mem: 15572
Epoch: [22]  [2080/2809]  eta: 0:06:56  lr: 0.000023  min_lr: 0.000000  loss: 4.4022 (4.2676)  class_acc: 0.1667 (0.2914)  loss_scale: 65536.0000 (39129.4954)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1091  max mem: 15572
Epoch: [22]  [2090/2809]  eta: 0:06:50  lr: 0.000023  min_lr: 0.000000  loss: 4.3324 (4.2681)  class_acc: 0.2917 (0.2915)  loss_scale: 65536.0000 (39255.7819)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0917  max mem: 15572
Epoch: [22]  [2100/2809]  eta: 0:06:44  lr: 0.000023  min_lr: 0.000000  loss: 4.3324 (4.2689)  class_acc: 0.2500 (0.2914)  loss_scale: 65536.0000 (39380.8663)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0568  max mem: 15572
Epoch: [22]  [2110/2809]  eta: 0:06:38  lr: 0.000023  min_lr: 0.000000  loss: 4.3209 (4.2690)  class_acc: 0.3333 (0.2915)  loss_scale: 65536.0000 (39504.7655)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1104  max mem: 15572
Epoch: [22]  [2120/2809]  eta: 0:06:32  lr: 0.000023  min_lr: 0.000000  loss: 4.3054 (4.2691)  class_acc: 0.2917 (0.2915)  loss_scale: 65536.0000 (39627.4965)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1176  max mem: 15572
[2025-01-16 01:14:00,580] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63922
[2025-01-16 01:14:00,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:14:00,581] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [2130/2809]  eta: 0:06:27  lr: 0.000023  min_lr: 0.000000  loss: 4.3255 (4.2692)  class_acc: 0.2917 (0.2915)  loss_scale: 65536.0000 (39641.4378)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1260  max mem: 15572
Epoch: [22]  [2140/2809]  eta: 0:06:21  lr: 0.000023  min_lr: 0.000000  loss: 4.3334 (4.2695)  class_acc: 0.2083 (0.2914)  loss_scale: 32768.0000 (39609.3340)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1300  max mem: 15572
Epoch: [22]  [2150/2809]  eta: 0:06:15  lr: 0.000023  min_lr: 0.000000  loss: 4.3323 (4.2691)  class_acc: 0.2500 (0.2915)  loss_scale: 32768.0000 (39577.5286)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0970  max mem: 15572
Epoch: [22]  [2160/2809]  eta: 0:06:10  lr: 0.000023  min_lr: 0.000000  loss: 4.4310 (4.2694)  class_acc: 0.2500 (0.2914)  loss_scale: 32768.0000 (39546.0176)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1450  max mem: 15572
Epoch: [22]  [2170/2809]  eta: 0:06:04  lr: 0.000023  min_lr: 0.000000  loss: 4.3404 (4.2697)  class_acc: 0.2500 (0.2915)  loss_scale: 32768.0000 (39514.7969)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1342  max mem: 15572
Epoch: [22]  [2180/2809]  eta: 0:05:58  lr: 0.000023  min_lr: 0.000000  loss: 4.3103 (4.2691)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (39483.8624)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0714  max mem: 15572
Epoch: [22]  [2190/2809]  eta: 0:05:52  lr: 0.000023  min_lr: 0.000000  loss: 4.2426 (4.2688)  class_acc: 0.2917 (0.2918)  loss_scale: 32768.0000 (39453.2104)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1235  max mem: 15572
Epoch: [22]  [2200/2809]  eta: 0:05:47  lr: 0.000023  min_lr: 0.000000  loss: 4.3062 (4.2685)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (39422.8369)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.1977  max mem: 15572
[2025-01-16 01:14:45,646] [INFO] [logging.py:96:log_dist] [Rank 0] step=64000, skipped=399, lr=[2.2153745296679163e-07, 2.2153745296679163e-07, 3.164820756668452e-07, 3.164820756668452e-07, 4.5211725095263607e-07, 4.5211725095263607e-07, 6.458817870751944e-07, 6.458817870751944e-07, 9.226882672502778e-07, 9.226882672502778e-07, 1.3181260960718254e-06, 1.3181260960718254e-06, 1.8830372801026078e-06, 1.8830372801026078e-06, 2.69005325728944e-06, 2.69005325728944e-06, 3.8429332246992e-06, 3.8429332246992e-06, 5.489904606713144e-06, 5.489904606713144e-06, 7.842720866733062e-06, 7.842720866733062e-06, 1.1203886952475804e-05, 1.1203886952475804e-05, 1.600555278925115e-05, 1.600555278925115e-05, 2.286507541321593e-05, 2.286507541321593e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:14:45,647] [INFO] [timer.py:260:stop] epoch=0/micro_step=64000/global_step=64000, RunningAvgSamplesPerSec=27.842713970071692, CurrSamplesPerSec=29.113346294026492, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [2210/2809]  eta: 0:05:41  lr: 0.000023  min_lr: 0.000000  loss: 4.3238 (4.2686)  class_acc: 0.2083 (0.2914)  loss_scale: 32768.0000 (39392.7381)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1446  max mem: 15572
Epoch: [22]  [2220/2809]  eta: 0:05:35  lr: 0.000023  min_lr: 0.000000  loss: 4.2998 (4.2691)  class_acc: 0.2500 (0.2912)  loss_scale: 32768.0000 (39362.9104)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1040  max mem: 15572
Epoch: [22]  [2230/2809]  eta: 0:05:29  lr: 0.000023  min_lr: 0.000000  loss: 4.2988 (4.2693)  class_acc: 0.2500 (0.2910)  loss_scale: 32768.0000 (39333.3501)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0841  max mem: 15572
Epoch: [22]  [2240/2809]  eta: 0:05:24  lr: 0.000023  min_lr: 0.000000  loss: 4.1732 (4.2676)  class_acc: 0.2917 (0.2914)  loss_scale: 32768.0000 (39304.0535)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0799  max mem: 15572
Epoch: [22]  [2250/2809]  eta: 0:05:18  lr: 0.000023  min_lr: 0.000000  loss: 4.1180 (4.2679)  class_acc: 0.3333 (0.2914)  loss_scale: 32768.0000 (39275.0173)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1022  max mem: 15572
[2025-01-16 01:15:14,311] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:15:14,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [2260/2809]  eta: 0:05:12  lr: 0.000023  min_lr: 0.000000  loss: 4.3255 (4.2678)  class_acc: 0.2500 (0.2911)  loss_scale: 32768.0000 (39362.1796)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1068  max mem: 15572
Epoch: [22]  [2270/2809]  eta: 0:05:07  lr: 0.000023  min_lr: 0.000000  loss: 4.2076 (4.2679)  class_acc: 0.2500 (0.2911)  loss_scale: 65536.0000 (39477.4320)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.1015  max mem: 15572
Epoch: [22]  [2280/2809]  eta: 0:05:01  lr: 0.000023  min_lr: 0.000000  loss: 4.3908 (4.2684)  class_acc: 0.2917 (0.2910)  loss_scale: 65536.0000 (39591.6738)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1509  max mem: 15572
Epoch: [22]  [2290/2809]  eta: 0:04:55  lr: 0.000023  min_lr: 0.000000  loss: 4.3501 (4.2683)  class_acc: 0.2917 (0.2910)  loss_scale: 65536.0000 (39704.9184)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.1041  max mem: 15572
Epoch: [22]  [2300/2809]  eta: 0:04:50  lr: 0.000023  min_lr: 0.000000  loss: 4.2823 (4.2682)  class_acc: 0.2917 (0.2910)  loss_scale: 65536.0000 (39817.1786)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.1012  max mem: 15572
Epoch: [22]  [2310/2809]  eta: 0:04:44  lr: 0.000023  min_lr: 0.000000  loss: 4.2505 (4.2682)  class_acc: 0.2917 (0.2914)  loss_scale: 65536.0000 (39928.4673)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.1961  max mem: 15572
Epoch: [22]  [2320/2809]  eta: 0:04:38  lr: 0.000023  min_lr: 0.000000  loss: 4.2046 (4.2679)  class_acc: 0.2917 (0.2913)  loss_scale: 65536.0000 (40038.7971)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1319  max mem: 15572
Epoch: [22]  [2330/2809]  eta: 0:04:33  lr: 0.000023  min_lr: 0.000000  loss: 4.1762 (4.2677)  class_acc: 0.2917 (0.2915)  loss_scale: 65536.0000 (40148.1802)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0947  max mem: 15572
Epoch: [22]  [2340/2809]  eta: 0:04:27  lr: 0.000023  min_lr: 0.000000  loss: 4.1999 (4.2672)  class_acc: 0.2917 (0.2914)  loss_scale: 65536.0000 (40256.6288)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.1903  max mem: 15572
Epoch: [22]  [2350/2809]  eta: 0:04:21  lr: 0.000023  min_lr: 0.000000  loss: 4.1857 (4.2669)  class_acc: 0.2917 (0.2915)  loss_scale: 65536.0000 (40364.1548)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1297  max mem: 15572
Epoch: [22]  [2360/2809]  eta: 0:04:15  lr: 0.000023  min_lr: 0.000000  loss: 4.1621 (4.2661)  class_acc: 0.2917 (0.2916)  loss_scale: 65536.0000 (40470.7700)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0616  max mem: 15572
Epoch: [22]  [2370/2809]  eta: 0:04:10  lr: 0.000023  min_lr: 0.000000  loss: 4.1621 (4.2660)  class_acc: 0.2917 (0.2916)  loss_scale: 65536.0000 (40576.4859)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.1092  max mem: 15572
Epoch: [22]  [2380/2809]  eta: 0:04:04  lr: 0.000023  min_lr: 0.000000  loss: 4.3472 (4.2665)  class_acc: 0.2083 (0.2911)  loss_scale: 65536.0000 (40681.3137)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1239  max mem: 15572
[2025-01-16 01:16:27,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:16:27,841] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:16:28,362] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64180
[2025-01-16 01:16:28,363] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:16:28,363] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [2390/2809]  eta: 0:03:58  lr: 0.000023  min_lr: 0.000000  loss: 4.3277 (4.2664)  class_acc: 0.2083 (0.2910)  loss_scale: 65536.0000 (40812.6742)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1165  max mem: 15572
Epoch: [22]  [2400/2809]  eta: 0:03:53  lr: 0.000023  min_lr: 0.000000  loss: 4.1688 (4.2655)  class_acc: 0.2917 (0.2912)  loss_scale: 65536.0000 (40915.6451)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0981  max mem: 15572
Epoch: [22]  [2410/2809]  eta: 0:03:47  lr: 0.000023  min_lr: 0.000000  loss: 4.1664 (4.2655)  class_acc: 0.2917 (0.2912)  loss_scale: 65536.0000 (41017.7619)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.1012  max mem: 15572
[2025-01-16 01:16:43,813] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64209
[2025-01-16 01:16:43,813] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:16:43,814] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [2420/2809]  eta: 0:03:41  lr: 0.000023  min_lr: 0.000000  loss: 4.3327 (4.2659)  class_acc: 0.2917 (0.2913)  loss_scale: 32768.0000 (40983.6861)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0847  max mem: 15572
Epoch: [22]  [2430/2809]  eta: 0:03:35  lr: 0.000023  min_lr: 0.000000  loss: 4.2548 (4.2657)  class_acc: 0.2917 (0.2915)  loss_scale: 32768.0000 (40949.8906)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1056  max mem: 15572
Epoch: [22]  [2440/2809]  eta: 0:03:30  lr: 0.000023  min_lr: 0.000000  loss: 4.1448 (4.2658)  class_acc: 0.2917 (0.2915)  loss_scale: 32768.0000 (40916.3720)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1337  max mem: 15572
Epoch: [22]  [2450/2809]  eta: 0:03:24  lr: 0.000023  min_lr: 0.000000  loss: 4.1985 (4.2658)  class_acc: 0.2500 (0.2915)  loss_scale: 32768.0000 (40883.1269)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0843  max mem: 15572
Epoch: [22]  [2460/2809]  eta: 0:03:18  lr: 0.000023  min_lr: 0.000000  loss: 4.1979 (4.2653)  class_acc: 0.2917 (0.2918)  loss_scale: 32768.0000 (40850.1520)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0864  max mem: 15572
Epoch: [22]  [2470/2809]  eta: 0:03:13  lr: 0.000023  min_lr: 0.000000  loss: 4.1719 (4.2652)  class_acc: 0.2917 (0.2916)  loss_scale: 32768.0000 (40817.4439)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1614  max mem: 15572
Epoch: [22]  [2480/2809]  eta: 0:03:07  lr: 0.000023  min_lr: 0.000000  loss: 4.2984 (4.2651)  class_acc: 0.2083 (0.2916)  loss_scale: 32768.0000 (40784.9996)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1337  max mem: 15572
Epoch: [22]  [2490/2809]  eta: 0:03:01  lr: 0.000023  min_lr: 0.000000  loss: 4.1494 (4.2648)  class_acc: 0.2500 (0.2916)  loss_scale: 32768.0000 (40752.8157)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0706  max mem: 15572
Epoch: [22]  [2500/2809]  eta: 0:02:55  lr: 0.000023  min_lr: 0.000000  loss: 4.1902 (4.2649)  class_acc: 0.2917 (0.2916)  loss_scale: 32768.0000 (40720.8892)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1103  max mem: 15572
Epoch: [22]  [2510/2809]  eta: 0:02:50  lr: 0.000023  min_lr: 0.000000  loss: 4.3016 (4.2654)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (40689.2170)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1543  max mem: 15572
Epoch: [22]  [2520/2809]  eta: 0:02:44  lr: 0.000023  min_lr: 0.000000  loss: 4.3102 (4.2659)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (40657.7961)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1112  max mem: 15572
[2025-01-16 01:17:46,750] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64320
[2025-01-16 01:17:46,751] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 01:17:46,751] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [22]  [2530/2809]  eta: 0:02:38  lr: 0.000023  min_lr: 0.000000  loss: 4.3102 (4.2659)  class_acc: 0.2917 (0.2918)  loss_scale: 32768.0000 (40568.3635)  weight_decay: 0.0500 (0.0500)  time: 0.4918  data: 0.0455  max mem: 15572
Epoch: [22]  [2540/2809]  eta: 0:02:33  lr: 0.000023  min_lr: 0.000000  loss: 4.3339 (4.2661)  class_acc: 0.2917 (0.2920)  loss_scale: 16384.0000 (40473.1869)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0500  max mem: 15572
Epoch: [22]  [2550/2809]  eta: 0:02:27  lr: 0.000023  min_lr: 0.000000  loss: 4.1987 (4.2654)  class_acc: 0.3333 (0.2922)  loss_scale: 16384.0000 (40378.7566)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0868  max mem: 15572
Epoch: [22]  [2560/2809]  eta: 0:02:21  lr: 0.000023  min_lr: 0.000000  loss: 4.1987 (4.2657)  class_acc: 0.2917 (0.2921)  loss_scale: 16384.0000 (40285.0636)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1284  max mem: 15572
Epoch: [22]  [2570/2809]  eta: 0:02:16  lr: 0.000023  min_lr: 0.000000  loss: 4.3198 (4.2657)  class_acc: 0.2917 (0.2922)  loss_scale: 16384.0000 (40192.0996)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1217  max mem: 15572
Epoch: [22]  [2580/2809]  eta: 0:02:10  lr: 0.000023  min_lr: 0.000000  loss: 4.1598 (4.2653)  class_acc: 0.2500 (0.2920)  loss_scale: 16384.0000 (40099.8559)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0820  max mem: 15572
Epoch: [22]  [2590/2809]  eta: 0:02:04  lr: 0.000023  min_lr: 0.000000  loss: 4.1514 (4.2651)  class_acc: 0.2500 (0.2921)  loss_scale: 16384.0000 (40008.3242)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0921  max mem: 15572
Epoch: [22]  [2600/2809]  eta: 0:01:59  lr: 0.000023  min_lr: 0.000000  loss: 4.1851 (4.2652)  class_acc: 0.2917 (0.2920)  loss_scale: 16384.0000 (39917.4963)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1203  max mem: 15572
Epoch: [22]  [2610/2809]  eta: 0:01:53  lr: 0.000023  min_lr: 0.000000  loss: 4.2538 (4.2652)  class_acc: 0.2500 (0.2920)  loss_scale: 16384.0000 (39827.3642)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1365  max mem: 15572
Epoch: [22]  [2620/2809]  eta: 0:01:47  lr: 0.000023  min_lr: 0.000000  loss: 4.3741 (4.2660)  class_acc: 0.2917 (0.2918)  loss_scale: 16384.0000 (39737.9199)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0623  max mem: 15572
Epoch: [22]  [2630/2809]  eta: 0:01:42  lr: 0.000023  min_lr: 0.000000  loss: 4.3680 (4.2656)  class_acc: 0.2917 (0.2919)  loss_scale: 16384.0000 (39649.1555)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.1542  max mem: 15572
Epoch: [22]  [2640/2809]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000000  loss: 4.0273 (4.2650)  class_acc: 0.2917 (0.2920)  loss_scale: 16384.0000 (39561.0632)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1541  max mem: 15572
Epoch: [22]  [2650/2809]  eta: 0:01:30  lr: 0.000023  min_lr: 0.000000  loss: 4.1890 (4.2648)  class_acc: 0.3333 (0.2921)  loss_scale: 16384.0000 (39473.6356)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0728  max mem: 15572
[2025-01-16 01:19:01,683] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:19:01,684] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [2660/2809]  eta: 0:01:24  lr: 0.000023  min_lr: 0.000000  loss: 4.2774 (4.2649)  class_acc: 0.2917 (0.2921)  loss_scale: 16384.0000 (39448.4359)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1145  max mem: 15572
Epoch: [22]  [2670/2809]  eta: 0:01:19  lr: 0.000023  min_lr: 0.000000  loss: 4.2942 (4.2651)  class_acc: 0.2917 (0.2923)  loss_scale: 32768.0000 (39423.4249)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0771  max mem: 15572
Epoch: [22]  [2680/2809]  eta: 0:01:13  lr: 0.000023  min_lr: 0.000000  loss: 4.2658 (4.2651)  class_acc: 0.3333 (0.2924)  loss_scale: 32768.0000 (39398.6005)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0708  max mem: 15572
Epoch: [22]  [2690/2809]  eta: 0:01:07  lr: 0.000022  min_lr: 0.000000  loss: 4.3207 (4.2655)  class_acc: 0.2917 (0.2923)  loss_scale: 32768.0000 (39373.9606)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0616  max mem: 15572
Epoch: [22]  [2700/2809]  eta: 0:01:02  lr: 0.000022  min_lr: 0.000000  loss: 4.2418 (4.2648)  class_acc: 0.2917 (0.2923)  loss_scale: 32768.0000 (39349.5031)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0262  max mem: 15572
Epoch: [22]  [2710/2809]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000000  loss: 4.0698 (4.2641)  class_acc: 0.2917 (0.2923)  loss_scale: 32768.0000 (39325.2261)  weight_decay: 0.0500 (0.0500)  time: 0.4167  data: 0.0005  max mem: 15572
Epoch: [22]  [2720/2809]  eta: 0:00:50  lr: 0.000022  min_lr: 0.000000  loss: 4.0795 (4.2640)  class_acc: 0.2500 (0.2922)  loss_scale: 32768.0000 (39301.1275)  weight_decay: 0.0500 (0.0500)  time: 0.4145  data: 0.0004  max mem: 15572
Epoch: [22]  [2730/2809]  eta: 0:00:44  lr: 0.000022  min_lr: 0.000000  loss: 4.1326 (4.2637)  class_acc: 0.2917 (0.2925)  loss_scale: 32768.0000 (39277.2054)  weight_decay: 0.0500 (0.0500)  time: 0.4553  data: 0.0006  max mem: 15572
Epoch: [22]  [2740/2809]  eta: 0:00:39  lr: 0.000022  min_lr: 0.000000  loss: 4.1471 (4.2636)  class_acc: 0.2917 (0.2925)  loss_scale: 32768.0000 (39253.4579)  weight_decay: 0.0500 (0.0500)  time: 0.4700  data: 0.0091  max mem: 15572
Epoch: [22]  [2750/2809]  eta: 0:00:33  lr: 0.000022  min_lr: 0.000000  loss: 4.2310 (4.2639)  class_acc: 0.2917 (0.2925)  loss_scale: 32768.0000 (39229.8830)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1403  max mem: 15572
Epoch: [22]  [2760/2809]  eta: 0:00:27  lr: 0.000022  min_lr: 0.000000  loss: 4.2498 (4.2639)  class_acc: 0.2917 (0.2925)  loss_scale: 32768.0000 (39206.4788)  weight_decay: 0.0500 (0.0500)  time: 0.6781  data: 0.2091  max mem: 15572
Epoch: [22]  [2770/2809]  eta: 0:00:22  lr: 0.000022  min_lr: 0.000000  loss: 4.3132 (4.2640)  class_acc: 0.2917 (0.2926)  loss_scale: 32768.0000 (39183.2436)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1821  max mem: 15572
[2025-01-16 01:20:10,376] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:20:10,376] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [2780/2809]  eta: 0:00:16  lr: 0.000022  min_lr: 0.000000  loss: 4.3952 (4.2645)  class_acc: 0.2917 (0.2924)  loss_scale: 32768.0000 (39183.7411)  weight_decay: 0.0500 (0.0500)  time: 0.6841  data: 0.2121  max mem: 15572
Epoch: [22]  [2790/2809]  eta: 0:00:10  lr: 0.000022  min_lr: 0.000000  loss: 4.5316 (4.2651)  class_acc: 0.2917 (0.2925)  loss_scale: 65536.0000 (39278.1598)  weight_decay: 0.0500 (0.0500)  time: 0.6907  data: 0.2286  max mem: 15572
Epoch: [22]  [2800/2809]  eta: 0:00:05  lr: 0.000022  min_lr: 0.000000  loss: 4.3405 (4.2651)  class_acc: 0.2917 (0.2923)  loss_scale: 65536.0000 (39371.9043)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.2301  max mem: 15572
Epoch: [22]  [2808/2809]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000000  loss: 4.2992 (4.2653)  class_acc: 0.2083 (0.2922)  loss_scale: 65536.0000 (39446.4194)  weight_decay: 0.0500 (0.0500)  time: 0.6588  data: 0.2298  max mem: 15572
Epoch: [22] Total time: 0:26:40 (0.5699 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000000  loss: 4.2992 (4.2653)  class_acc: 0.2083 (0.2922)  loss_scale: 65536.0000 (39446.4194)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:27:35  loss: 1.1228 (1.1228)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 6.0878  data: 5.8524  max mem: 15572
Val:  [ 10/272]  eta: 0:04:24  loss: 2.7193 (2.6735)  acc1: 44.4444 (43.4343)  acc5: 66.6667 (68.1818)  time: 1.0077  data: 0.7953  max mem: 15572
Val:  [ 20/272]  eta: 0:02:45  loss: 2.7527 (2.7010)  acc1: 44.4444 (43.6508)  acc5: 66.6667 (69.5767)  time: 0.3838  data: 0.1719  max mem: 15572
Val:  [ 30/272]  eta: 0:02:11  loss: 2.7712 (2.7209)  acc1: 44.4444 (41.0394)  acc5: 66.6667 (70.7885)  time: 0.2900  data: 0.0660  max mem: 15572
Val:  [ 40/272]  eta: 0:01:53  loss: 2.7756 (2.7472)  acc1: 27.7778 (38.0759)  acc5: 66.6667 (70.8672)  time: 0.3114  data: 0.0859  max mem: 15572
Val:  [ 50/272]  eta: 0:01:43  loss: 2.6155 (2.6721)  acc1: 33.3333 (39.6514)  acc5: 77.7778 (73.0937)  time: 0.3414  data: 0.1424  max mem: 15572
Val:  [ 60/272]  eta: 0:01:29  loss: 2.0220 (2.6027)  acc1: 50.0000 (41.1658)  acc5: 83.3333 (74.0437)  time: 0.2899  data: 0.1195  max mem: 15572
Val:  [ 70/272]  eta: 0:01:18  loss: 2.0460 (2.5395)  acc1: 55.5556 (44.0532)  acc5: 83.3333 (75.1956)  time: 0.2001  data: 0.0350  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 2.2702 (2.5523)  acc1: 55.5556 (44.1701)  acc5: 77.7778 (74.8285)  time: 0.1838  data: 0.0111  max mem: 15572
Val:  [ 90/272]  eta: 0:01:02  loss: 2.6373 (2.5671)  acc1: 44.4444 (44.0781)  acc5: 77.7778 (75.2137)  time: 0.1803  data: 0.0006  max mem: 15572
Val:  [100/272]  eta: 0:00:56  loss: 2.6465 (2.5944)  acc1: 38.8889 (42.9043)  acc5: 77.7778 (74.9175)  time: 0.1948  data: 0.0008  max mem: 15572
Val:  [110/272]  eta: 0:00:52  loss: 2.8025 (2.6470)  acc1: 22.2222 (41.1411)  acc5: 66.6667 (73.8739)  time: 0.2343  data: 0.0400  max mem: 15572
Val:  [120/272]  eta: 0:00:49  loss: 3.0483 (2.6808)  acc1: 22.2222 (40.4959)  acc5: 61.1111 (73.0487)  time: 0.3032  data: 0.1149  max mem: 15572
Val:  [130/272]  eta: 0:00:45  loss: 2.6992 (2.6510)  acc1: 44.4444 (41.5182)  acc5: 77.7778 (73.9186)  time: 0.3141  data: 0.1229  max mem: 15572
Val:  [140/272]  eta: 0:00:42  loss: 2.3513 (2.6544)  acc1: 55.5556 (41.7652)  acc5: 83.3333 (73.7983)  time: 0.3156  data: 0.1140  max mem: 15572
Val:  [150/272]  eta: 0:00:39  loss: 2.5930 (2.6477)  acc1: 33.3333 (41.5379)  acc5: 77.7778 (74.0250)  time: 0.3405  data: 0.1366  max mem: 15572
Val:  [160/272]  eta: 0:00:36  loss: 2.5798 (2.6449)  acc1: 38.8889 (41.9255)  acc5: 77.7778 (74.3616)  time: 0.3133  data: 0.1238  max mem: 15572
Val:  [170/272]  eta: 0:00:32  loss: 2.6688 (2.6606)  acc1: 33.3333 (41.2930)  acc5: 72.2222 (73.8791)  time: 0.3097  data: 0.1266  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 2.6688 (2.6491)  acc1: 33.3333 (41.1602)  acc5: 66.6667 (74.0638)  time: 0.3230  data: 0.1322  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.6655 (2.6843)  acc1: 27.7778 (39.9360)  acc5: 66.6667 (72.9785)  time: 0.3284  data: 0.1348  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.6722 (2.6844)  acc1: 22.2222 (39.6075)  acc5: 66.6667 (72.9409)  time: 0.3058  data: 0.1134  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.4697 (2.6849)  acc1: 44.4444 (39.8368)  acc5: 77.7778 (72.8805)  time: 0.2839  data: 0.0845  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.7032 (2.6835)  acc1: 38.8889 (39.9698)  acc5: 77.7778 (72.8507)  time: 0.3197  data: 0.1247  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.3174 (2.6673)  acc1: 50.0000 (40.7888)  acc5: 77.7778 (73.2323)  time: 0.3556  data: 0.1688  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.2221 (2.6559)  acc1: 55.5556 (41.0327)  acc5: 83.3333 (73.5823)  time: 0.3379  data: 0.1532  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.5521 (2.6574)  acc1: 33.3333 (40.7924)  acc5: 77.7778 (73.6166)  time: 0.3067  data: 0.1140  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9437 (2.6128)  acc1: 72.2222 (42.5287)  acc5: 88.8889 (74.3721)  time: 0.2795  data: 0.0944  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.9570 (2.6113)  acc1: 55.5556 (42.5174)  acc5: 88.8889 (74.3952)  time: 0.2023  data: 0.0431  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.9570 (2.6153)  acc1: 50.0000 (42.4944)  acc5: 88.8889 (74.3600)  time: 0.1961  data: 0.0431  max mem: 15572
Val: Total time: 0:01:25 (0.3132 s / it)
* Acc@1 42.494 Acc@5 74.360 loss 2.615
Accuracy of the network on the 4883 val videos: 42.5%
[2025-01-16 01:21:55,322] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 01:21:55,324] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 01:21:55,324] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 01:21:57,231] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 01:21:57,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 42.49%
Epoch: [23]  [   0/2809]  eta: 6:30:01  lr: 0.000022  min_lr: 0.000000  loss: 4.3399 (4.3399)  class_acc: 0.2500 (0.2500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.3310  data: 7.9122  max mem: 15572
Epoch: [23]  [  10/2809]  eta: 0:59:30  lr: 0.000022  min_lr: 0.000000  loss: 4.2897 (4.2720)  class_acc: 0.2500 (0.2727)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2755  data: 0.8194  max mem: 15572
Epoch: [23]  [  20/2809]  eta: 0:43:23  lr: 0.000022  min_lr: 0.000000  loss: 4.2897 (4.2704)  class_acc: 0.2500 (0.2619)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1218  max mem: 15572
Epoch: [23]  [  30/2809]  eta: 0:36:30  lr: 0.000022  min_lr: 0.000000  loss: 4.1221 (4.1925)  class_acc: 0.2917 (0.2876)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0906  max mem: 15572
Epoch: [23]  [  40/2809]  eta: 0:32:36  lr: 0.000022  min_lr: 0.000000  loss: 4.0384 (4.1986)  class_acc: 0.3333 (0.2907)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4682  data: 0.0401  max mem: 15572
Epoch: [23]  [  50/2809]  eta: 0:31:34  lr: 0.000022  min_lr: 0.000000  loss: 4.2754 (4.2160)  class_acc: 0.3333 (0.3031)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0970  max mem: 15572
Epoch: [23]  [  60/2809]  eta: 0:30:22  lr: 0.000022  min_lr: 0.000000  loss: 4.3240 (4.2141)  class_acc: 0.3750 (0.3142)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1263  max mem: 15572
Epoch: [23]  [  70/2809]  eta: 0:29:25  lr: 0.000022  min_lr: 0.000000  loss: 4.1654 (4.2061)  class_acc: 0.2917 (0.3093)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0966  max mem: 15572
Epoch: [23]  [  80/2809]  eta: 0:28:21  lr: 0.000022  min_lr: 0.000000  loss: 4.1634 (4.2199)  class_acc: 0.2500 (0.3056)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0726  max mem: 15572
Epoch: [23]  [  90/2809]  eta: 0:27:55  lr: 0.000022  min_lr: 0.000000  loss: 4.1634 (4.2111)  class_acc: 0.2917 (0.3072)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0700  max mem: 15572
[2025-01-16 01:22:57,773] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:22:57,774] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:22:58,206] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64706
[2025-01-16 01:22:58,206] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:22:58,208] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 100/2809]  eta: 0:27:29  lr: 0.000022  min_lr: 0.000000  loss: 4.2377 (4.2175)  class_acc: 0.2917 (0.3020)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1022  max mem: 15572
[2025-01-16 01:23:04,953] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64717
[2025-01-16 01:23:04,954] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:23:04,954] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 110/2809]  eta: 0:27:25  lr: 0.000022  min_lr: 0.000000  loss: 4.2517 (4.2124)  class_acc: 0.2500 (0.2988)  loss_scale: 65536.0000 (65831.2072)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1449  max mem: 15572
Epoch: [23]  [ 120/2809]  eta: 0:27:14  lr: 0.000022  min_lr: 0.000000  loss: 4.2554 (4.2111)  class_acc: 0.2917 (0.2996)  loss_scale: 32768.0000 (63098.7107)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1683  max mem: 15572
Epoch: [23]  [ 130/2809]  eta: 0:26:55  lr: 0.000022  min_lr: 0.000000  loss: 4.1817 (4.2111)  class_acc: 0.2917 (0.2999)  loss_scale: 32768.0000 (60783.3893)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1404  max mem: 15572
Epoch: [23]  [ 140/2809]  eta: 0:26:45  lr: 0.000022  min_lr: 0.000000  loss: 4.1624 (4.2153)  class_acc: 0.3333 (0.3011)  loss_scale: 32768.0000 (58796.4823)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1354  max mem: 15572
Epoch: [23]  [ 150/2809]  eta: 0:26:37  lr: 0.000022  min_lr: 0.000000  loss: 4.2239 (4.2181)  class_acc: 0.3333 (0.3046)  loss_scale: 32768.0000 (57072.7417)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1342  max mem: 15572
Epoch: [23]  [ 160/2809]  eta: 0:26:27  lr: 0.000022  min_lr: 0.000000  loss: 4.2623 (4.2232)  class_acc: 0.2917 (0.3020)  loss_scale: 32768.0000 (55563.1304)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1268  max mem: 15572
Epoch: [23]  [ 170/2809]  eta: 0:26:05  lr: 0.000022  min_lr: 0.000000  loss: 4.2005 (4.2188)  class_acc: 0.2917 (0.3031)  loss_scale: 32768.0000 (54230.0819)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0983  max mem: 15572
Epoch: [23]  [ 180/2809]  eta: 0:25:59  lr: 0.000022  min_lr: 0.000000  loss: 4.1522 (4.2227)  class_acc: 0.3333 (0.3011)  loss_scale: 32768.0000 (53044.3315)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.1018  max mem: 15572
Epoch: [23]  [ 190/2809]  eta: 0:25:45  lr: 0.000022  min_lr: 0.000000  loss: 4.1522 (4.2144)  class_acc: 0.3333 (0.3028)  loss_scale: 32768.0000 (51982.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1158  max mem: 15572
Epoch: [23]  [ 200/2809]  eta: 0:25:43  lr: 0.000022  min_lr: 0.000000  loss: 4.1872 (4.2110)  class_acc: 0.2917 (0.3031)  loss_scale: 32768.0000 (51026.7861)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1172  max mem: 15572
Epoch: [23]  [ 210/2809]  eta: 0:25:38  lr: 0.000022  min_lr: 0.000000  loss: 4.2414 (4.2061)  class_acc: 0.2917 (0.3033)  loss_scale: 32768.0000 (50161.4408)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1307  max mem: 15572
Epoch: [23]  [ 220/2809]  eta: 0:25:30  lr: 0.000022  min_lr: 0.000000  loss: 4.0345 (4.1990)  class_acc: 0.3333 (0.3039)  loss_scale: 32768.0000 (49374.4072)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1227  max mem: 15572
Epoch: [23]  [ 230/2809]  eta: 0:25:17  lr: 0.000022  min_lr: 0.000000  loss: 4.1320 (4.2001)  class_acc: 0.3333 (0.3043)  loss_scale: 32768.0000 (48655.5152)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0936  max mem: 15572
[2025-01-16 01:24:18,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:24:18,432] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [ 240/2809]  eta: 0:25:10  lr: 0.000022  min_lr: 0.000000  loss: 4.2823 (4.2032)  class_acc: 0.2917 (0.3041)  loss_scale: 32768.0000 (48268.2158)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0915  max mem: 15572
Epoch: [23]  [ 250/2809]  eta: 0:24:58  lr: 0.000022  min_lr: 0.000000  loss: 4.3658 (4.2112)  class_acc: 0.2917 (0.3036)  loss_scale: 65536.0000 (48956.1753)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0953  max mem: 15572
Epoch: [23]  [ 260/2809]  eta: 0:24:47  lr: 0.000022  min_lr: 0.000000  loss: 4.3658 (4.2134)  class_acc: 0.2917 (0.3035)  loss_scale: 65536.0000 (49591.4176)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0914  max mem: 15572
Epoch: [23]  [ 270/2809]  eta: 0:24:40  lr: 0.000022  min_lr: 0.000000  loss: 4.2752 (4.2150)  class_acc: 0.2917 (0.3037)  loss_scale: 65536.0000 (50179.7786)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1227  max mem: 15572
Epoch: [23]  [ 280/2809]  eta: 0:24:29  lr: 0.000022  min_lr: 0.000000  loss: 4.2328 (4.2155)  class_acc: 0.2917 (0.3031)  loss_scale: 65536.0000 (50726.2633)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1230  max mem: 15572
Epoch: [23]  [ 290/2809]  eta: 0:24:29  lr: 0.000022  min_lr: 0.000000  loss: 4.2938 (4.2209)  class_acc: 0.2917 (0.3025)  loss_scale: 65536.0000 (51235.1890)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1571  max mem: 15572
Epoch: [23]  [ 300/2809]  eta: 0:24:15  lr: 0.000022  min_lr: 0.000000  loss: 4.3083 (4.2210)  class_acc: 0.2500 (0.3045)  loss_scale: 65536.0000 (51710.2990)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1232  max mem: 15572
Epoch: [23]  [ 310/2809]  eta: 0:24:08  lr: 0.000022  min_lr: 0.000000  loss: 4.2292 (4.2172)  class_acc: 0.2917 (0.3041)  loss_scale: 65536.0000 (52154.8553)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0819  max mem: 15572
Epoch: [23]  [ 320/2809]  eta: 0:24:03  lr: 0.000022  min_lr: 0.000000  loss: 4.2384 (4.2177)  class_acc: 0.2917 (0.3037)  loss_scale: 65536.0000 (52571.7134)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1319  max mem: 15572
Epoch: [23]  [ 330/2809]  eta: 0:23:47  lr: 0.000022  min_lr: 0.000000  loss: 4.3424 (4.2217)  class_acc: 0.2083 (0.3019)  loss_scale: 65536.0000 (52963.3837)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0725  max mem: 15572
Epoch: [23]  [ 340/2809]  eta: 0:23:43  lr: 0.000022  min_lr: 0.000000  loss: 4.2161 (4.2210)  class_acc: 0.2083 (0.3008)  loss_scale: 65536.0000 (53332.0821)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0891  max mem: 15572
Epoch: [23]  [ 350/2809]  eta: 0:23:40  lr: 0.000022  min_lr: 0.000000  loss: 4.1486 (4.2214)  class_acc: 0.2917 (0.3006)  loss_scale: 65536.0000 (53679.7721)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1709  max mem: 15572
Epoch: [23]  [ 360/2809]  eta: 0:23:41  lr: 0.000022  min_lr: 0.000000  loss: 4.3287 (4.2267)  class_acc: 0.2917 (0.3007)  loss_scale: 65536.0000 (54008.1994)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1841  max mem: 15572
[2025-01-16 01:25:31,068] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:25:31,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:25:31,524] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64975
[2025-01-16 01:25:31,525] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:25:31,525] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 370/2809]  eta: 0:23:33  lr: 0.000022  min_lr: 0.000000  loss: 4.3372 (4.2293)  class_acc: 0.3333 (0.3021)  loss_scale: 65536.0000 (54495.5687)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1650  max mem: 15572
Epoch: [23]  [ 380/2809]  eta: 0:23:27  lr: 0.000022  min_lr: 0.000000  loss: 4.3272 (4.2285)  class_acc: 0.3750 (0.3034)  loss_scale: 65536.0000 (54785.3438)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1293  max mem: 15572
Epoch: [23]  [ 390/2809]  eta: 0:23:18  lr: 0.000022  min_lr: 0.000000  loss: 4.2573 (4.2282)  class_acc: 0.3333 (0.3026)  loss_scale: 65536.0000 (55060.2967)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0994  max mem: 15572
[2025-01-16 01:25:44,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=65000, skipped=405, lr=[2.1429492637226224e-07, 2.1429492637226224e-07, 3.061356091032318e-07, 3.061356091032318e-07, 4.3733658443318834e-07, 4.3733658443318834e-07, 6.24766549190269e-07, 6.24766549190269e-07, 8.925236417003845e-07, 8.925236417003845e-07, 1.275033773857692e-06, 1.275033773857692e-06, 1.821476819796703e-06, 1.821476819796703e-06, 2.602109742566719e-06, 2.602109742566719e-06, 3.71729963223817e-06, 3.71729963223817e-06, 5.310428046054529e-06, 5.310428046054529e-06, 7.586325780077899e-06, 7.586325780077899e-06, 1.0837608257254143e-05, 1.0837608257254143e-05, 1.548229751036306e-05, 1.548229751036306e-05, 2.211756787194723e-05, 2.211756787194723e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:25:44,210] [INFO] [timer.py:260:stop] epoch=0/micro_step=65000/global_step=65000, RunningAvgSamplesPerSec=27.84556930284848, CurrSamplesPerSec=29.86107490416944, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [23]  [ 400/2809]  eta: 0:23:08  lr: 0.000022  min_lr: 0.000000  loss: 4.3213 (4.2295)  class_acc: 0.2500 (0.3023)  loss_scale: 65536.0000 (55321.5362)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0612  max mem: 15572
Epoch: [23]  [ 410/2809]  eta: 0:23:02  lr: 0.000022  min_lr: 0.000000  loss: 4.3456 (4.2304)  class_acc: 0.2917 (0.3032)  loss_scale: 65536.0000 (55570.0633)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0825  max mem: 15572
Epoch: [23]  [ 420/2809]  eta: 0:22:54  lr: 0.000022  min_lr: 0.000000  loss: 4.3547 (4.2339)  class_acc: 0.2500 (0.3011)  loss_scale: 65536.0000 (55806.7838)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1045  max mem: 15572
Epoch: [23]  [ 430/2809]  eta: 0:22:44  lr: 0.000022  min_lr: 0.000000  loss: 4.3398 (4.2336)  class_acc: 0.2500 (0.3020)  loss_scale: 65536.0000 (56032.5197)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0750  max mem: 15572
Epoch: [23]  [ 440/2809]  eta: 0:22:42  lr: 0.000022  min_lr: 0.000000  loss: 4.2410 (4.2344)  class_acc: 0.2917 (0.3011)  loss_scale: 65536.0000 (56248.0181)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1368  max mem: 15572
[2025-01-16 01:26:13,891] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65052
[2025-01-16 01:26:13,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:26:13,891] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 450/2809]  eta: 0:22:34  lr: 0.000022  min_lr: 0.000000  loss: 4.2698 (4.2350)  class_acc: 0.2500 (0.3005)  loss_scale: 65536.0000 (56018.0222)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1658  max mem: 15572
Epoch: [23]  [ 460/2809]  eta: 0:22:33  lr: 0.000022  min_lr: 0.000000  loss: 4.2162 (4.2361)  class_acc: 0.2917 (0.3009)  loss_scale: 32768.0000 (55513.6833)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1692  max mem: 15572
Epoch: [23]  [ 470/2809]  eta: 0:22:23  lr: 0.000022  min_lr: 0.000000  loss: 4.2003 (4.2354)  class_acc: 0.2917 (0.3013)  loss_scale: 32768.0000 (55030.7601)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1352  max mem: 15572
Epoch: [23]  [ 480/2809]  eta: 0:22:19  lr: 0.000022  min_lr: 0.000000  loss: 4.2106 (4.2371)  class_acc: 0.2917 (0.3008)  loss_scale: 32768.0000 (54567.9168)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1069  max mem: 15572
Epoch: [23]  [ 490/2809]  eta: 0:22:12  lr: 0.000022  min_lr: 0.000000  loss: 4.2173 (4.2359)  class_acc: 0.2917 (0.3002)  loss_scale: 32768.0000 (54123.9267)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1346  max mem: 15572
Epoch: [23]  [ 500/2809]  eta: 0:22:07  lr: 0.000022  min_lr: 0.000000  loss: 4.2725 (4.2372)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (53697.6607)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1043  max mem: 15572
Epoch: [23]  [ 510/2809]  eta: 0:22:02  lr: 0.000022  min_lr: 0.000000  loss: 4.2911 (4.2381)  class_acc: 0.2083 (0.2984)  loss_scale: 32768.0000 (53288.0783)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1238  max mem: 15572
Epoch: [23]  [ 520/2809]  eta: 0:21:57  lr: 0.000022  min_lr: 0.000000  loss: 4.1916 (4.2376)  class_acc: 0.2083 (0.2981)  loss_scale: 32768.0000 (52894.2188)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1416  max mem: 15572
Epoch: [23]  [ 530/2809]  eta: 0:21:50  lr: 0.000022  min_lr: 0.000000  loss: 4.0506 (4.2343)  class_acc: 0.2917 (0.2997)  loss_scale: 32768.0000 (52515.1940)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1286  max mem: 15572
Epoch: [23]  [ 540/2809]  eta: 0:21:49  lr: 0.000022  min_lr: 0.000000  loss: 4.1197 (4.2334)  class_acc: 0.3333 (0.2998)  loss_scale: 32768.0000 (52150.1811)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1673  max mem: 15572
Epoch: [23]  [ 550/2809]  eta: 0:21:38  lr: 0.000022  min_lr: 0.000000  loss: 4.1885 (4.2327)  class_acc: 0.2917 (0.3001)  loss_scale: 32768.0000 (51798.4174)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1133  max mem: 15572
Epoch: [23]  [ 560/2809]  eta: 0:21:30  lr: 0.000022  min_lr: 0.000000  loss: 4.2932 (4.2323)  class_acc: 0.3333 (0.3006)  loss_scale: 32768.0000 (51459.1943)  weight_decay: 0.0500 (0.0500)  time: 0.4900  data: 0.0423  max mem: 15572
Epoch: [23]  [ 570/2809]  eta: 0:21:20  lr: 0.000022  min_lr: 0.000000  loss: 4.3570 (4.2355)  class_acc: 0.2083 (0.3002)  loss_scale: 32768.0000 (51131.8529)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0572  max mem: 15572
[2025-01-16 01:27:27,386] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:27:27,387] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [ 580/2809]  eta: 0:21:15  lr: 0.000022  min_lr: 0.000000  loss: 4.3692 (4.2369)  class_acc: 0.2083 (0.2996)  loss_scale: 32768.0000 (51210.5749)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0922  max mem: 15572
Epoch: [23]  [ 590/2809]  eta: 0:21:12  lr: 0.000022  min_lr: 0.000000  loss: 4.4105 (4.2419)  class_acc: 0.2083 (0.2989)  loss_scale: 65536.0000 (51452.9679)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1757  max mem: 15572
Epoch: [23]  [ 600/2809]  eta: 0:21:08  lr: 0.000022  min_lr: 0.000000  loss: 4.4239 (4.2444)  class_acc: 0.2083 (0.2975)  loss_scale: 65536.0000 (51687.2945)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1864  max mem: 15572
Epoch: [23]  [ 610/2809]  eta: 0:21:02  lr: 0.000022  min_lr: 0.000000  loss: 4.3329 (4.2438)  class_acc: 0.2500 (0.2969)  loss_scale: 65536.0000 (51913.9509)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1384  max mem: 15572
Epoch: [23]  [ 620/2809]  eta: 0:20:58  lr: 0.000022  min_lr: 0.000000  loss: 4.3531 (4.2477)  class_acc: 0.2500 (0.2957)  loss_scale: 65536.0000 (52133.3076)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1355  max mem: 15572
Epoch: [23]  [ 630/2809]  eta: 0:20:52  lr: 0.000022  min_lr: 0.000000  loss: 4.2855 (4.2450)  class_acc: 0.2917 (0.2972)  loss_scale: 65536.0000 (52345.7116)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1490  max mem: 15572
Epoch: [23]  [ 640/2809]  eta: 0:20:44  lr: 0.000022  min_lr: 0.000000  loss: 4.1178 (4.2452)  class_acc: 0.2917 (0.2966)  loss_scale: 65536.0000 (52551.4883)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0840  max mem: 15572
[2025-01-16 01:28:06,888] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65249
[2025-01-16 01:28:06,889] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:28:06,889] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 650/2809]  eta: 0:20:37  lr: 0.000022  min_lr: 0.000000  loss: 4.2139 (4.2443)  class_acc: 0.2917 (0.2969)  loss_scale: 65536.0000 (52297.9293)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0589  max mem: 15572
Epoch: [23]  [ 660/2809]  eta: 0:20:30  lr: 0.000022  min_lr: 0.000000  loss: 4.1747 (4.2430)  class_acc: 0.2917 (0.2967)  loss_scale: 32768.0000 (52002.4690)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0771  max mem: 15572
Epoch: [23]  [ 670/2809]  eta: 0:20:25  lr: 0.000022  min_lr: 0.000000  loss: 4.1747 (4.2434)  class_acc: 0.2500 (0.2965)  loss_scale: 32768.0000 (51715.8152)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1077  max mem: 15572
Epoch: [23]  [ 680/2809]  eta: 0:20:20  lr: 0.000022  min_lr: 0.000000  loss: 4.3464 (4.2442)  class_acc: 0.2500 (0.2966)  loss_scale: 32768.0000 (51437.5800)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1524  max mem: 15572
Epoch: [23]  [ 690/2809]  eta: 0:20:14  lr: 0.000022  min_lr: 0.000000  loss: 4.3464 (4.2453)  class_acc: 0.2500 (0.2969)  loss_scale: 32768.0000 (51167.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1374  max mem: 15572
Epoch: [23]  [ 700/2809]  eta: 0:20:08  lr: 0.000022  min_lr: 0.000000  loss: 4.2176 (4.2428)  class_acc: 0.3333 (0.2974)  loss_scale: 32768.0000 (50904.9244)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0932  max mem: 15572
Epoch: [23]  [ 710/2809]  eta: 0:20:03  lr: 0.000022  min_lr: 0.000000  loss: 4.2931 (4.2439)  class_acc: 0.2917 (0.2971)  loss_scale: 32768.0000 (50649.8340)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1159  max mem: 15572
Epoch: [23]  [ 720/2809]  eta: 0:19:56  lr: 0.000022  min_lr: 0.000000  loss: 4.3209 (4.2443)  class_acc: 0.2917 (0.2971)  loss_scale: 32768.0000 (50401.8197)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1233  max mem: 15572
Epoch: [23]  [ 730/2809]  eta: 0:19:51  lr: 0.000022  min_lr: 0.000000  loss: 4.2961 (4.2437)  class_acc: 0.2917 (0.2971)  loss_scale: 32768.0000 (50160.5910)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1281  max mem: 15572
Epoch: [23]  [ 740/2809]  eta: 0:19:45  lr: 0.000022  min_lr: 0.000000  loss: 4.1562 (4.2435)  class_acc: 0.2500 (0.2970)  loss_scale: 32768.0000 (49925.8731)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1393  max mem: 15572
Epoch: [23]  [ 750/2809]  eta: 0:19:38  lr: 0.000022  min_lr: 0.000000  loss: 4.3018 (4.2447)  class_acc: 0.2500 (0.2968)  loss_scale: 32768.0000 (49697.4061)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1095  max mem: 15572
Epoch: [23]  [ 760/2809]  eta: 0:19:31  lr: 0.000022  min_lr: 0.000000  loss: 4.2137 (4.2410)  class_acc: 0.3750 (0.2980)  loss_scale: 32768.0000 (49474.9435)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0855  max mem: 15572
Epoch: [23]  [ 770/2809]  eta: 0:19:28  lr: 0.000022  min_lr: 0.000000  loss: 4.1219 (4.2414)  class_acc: 0.2917 (0.2971)  loss_scale: 32768.0000 (49258.2516)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1552  max mem: 15572
[2025-01-16 01:29:19,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:29:19,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 01:29:23,985] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65384
[2025-01-16 01:29:23,985] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:29:23,985] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 780/2809]  eta: 0:19:23  lr: 0.000022  min_lr: 0.000000  loss: 4.4003 (4.2428)  class_acc: 0.2500 (0.2974)  loss_scale: 32768.0000 (49298.8476)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.2016  max mem: 15572
Epoch: [23]  [ 790/2809]  eta: 0:19:21  lr: 0.000022  min_lr: 0.000000  loss: 4.3437 (4.2434)  class_acc: 0.2917 (0.2976)  loss_scale: 32768.0000 (49089.8609)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.2292  max mem: 15572
Epoch: [23]  [ 800/2809]  eta: 0:19:14  lr: 0.000022  min_lr: 0.000000  loss: 4.2870 (4.2442)  class_acc: 0.3333 (0.2983)  loss_scale: 32768.0000 (48886.0924)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.2114  max mem: 15572
Epoch: [23]  [ 810/2809]  eta: 0:19:08  lr: 0.000022  min_lr: 0.000000  loss: 4.1366 (4.2431)  class_acc: 0.2917 (0.2982)  loss_scale: 32768.0000 (48687.3490)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.1143  max mem: 15572
Epoch: [23]  [ 820/2809]  eta: 0:19:03  lr: 0.000022  min_lr: 0.000000  loss: 4.2539 (4.2445)  class_acc: 0.2500 (0.2972)  loss_scale: 32768.0000 (48493.4470)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1241  max mem: 15572
Epoch: [23]  [ 830/2809]  eta: 0:18:56  lr: 0.000022  min_lr: 0.000000  loss: 4.3670 (4.2463)  class_acc: 0.2500 (0.2972)  loss_scale: 32768.0000 (48304.2118)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1067  max mem: 15572
Epoch: [23]  [ 840/2809]  eta: 0:18:48  lr: 0.000022  min_lr: 0.000000  loss: 4.3163 (4.2469)  class_acc: 0.2500 (0.2972)  loss_scale: 32768.0000 (48119.4768)  weight_decay: 0.0500 (0.0500)  time: 0.4867  data: 0.0477  max mem: 15572
Epoch: [23]  [ 850/2809]  eta: 0:18:40  lr: 0.000022  min_lr: 0.000000  loss: 4.2561 (4.2477)  class_acc: 0.2500 (0.2973)  loss_scale: 32768.0000 (47939.0834)  weight_decay: 0.0500 (0.0500)  time: 0.4756  data: 0.0377  max mem: 15572
Epoch: [23]  [ 860/2809]  eta: 0:18:35  lr: 0.000022  min_lr: 0.000000  loss: 4.2561 (4.2482)  class_acc: 0.2500 (0.2971)  loss_scale: 32768.0000 (47762.8804)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.0942  max mem: 15572
Epoch: [23]  [ 870/2809]  eta: 0:18:30  lr: 0.000022  min_lr: 0.000000  loss: 4.2350 (4.2482)  class_acc: 0.2500 (0.2969)  loss_scale: 32768.0000 (47590.7233)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1425  max mem: 15572
Epoch: [23]  [ 880/2809]  eta: 0:18:23  lr: 0.000022  min_lr: 0.000000  loss: 4.2779 (4.2488)  class_acc: 0.2917 (0.2974)  loss_scale: 32768.0000 (47422.4745)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1245  max mem: 15572
Epoch: [23]  [ 890/2809]  eta: 0:18:17  lr: 0.000022  min_lr: 0.000000  loss: 4.3286 (4.2503)  class_acc: 0.2917 (0.2978)  loss_scale: 32768.0000 (47258.0022)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0948  max mem: 15572
Epoch: [23]  [ 900/2809]  eta: 0:18:10  lr: 0.000022  min_lr: 0.000000  loss: 4.3286 (4.2510)  class_acc: 0.2917 (0.2974)  loss_scale: 32768.0000 (47097.1809)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0701  max mem: 15572
[2025-01-16 01:30:37,208] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:30:37,209] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [ 910/2809]  eta: 0:18:07  lr: 0.000022  min_lr: 0.000000  loss: 4.3517 (4.2525)  class_acc: 0.2083 (0.2964)  loss_scale: 32768.0000 (47119.7366)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1245  max mem: 15572
Epoch: [23]  [ 920/2809]  eta: 0:17:58  lr: 0.000022  min_lr: 0.000000  loss: 4.2320 (4.2517)  class_acc: 0.2083 (0.2965)  loss_scale: 65536.0000 (47319.6960)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.0983  max mem: 15572
Epoch: [23]  [ 930/2809]  eta: 0:17:52  lr: 0.000022  min_lr: 0.000000  loss: 4.3595 (4.2539)  class_acc: 0.2083 (0.2954)  loss_scale: 65536.0000 (47515.3598)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0509  max mem: 15572
Epoch: [23]  [ 940/2809]  eta: 0:17:47  lr: 0.000022  min_lr: 0.000000  loss: 4.3266 (4.2530)  class_acc: 0.2083 (0.2956)  loss_scale: 65536.0000 (47706.8650)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1212  max mem: 15572
Epoch: [23]  [ 950/2809]  eta: 0:17:42  lr: 0.000022  min_lr: 0.000000  loss: 4.2530 (4.2539)  class_acc: 0.2500 (0.2952)  loss_scale: 65536.0000 (47894.3428)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1401  max mem: 15572
Epoch: [23]  [ 960/2809]  eta: 0:17:36  lr: 0.000022  min_lr: 0.000000  loss: 4.3316 (4.2540)  class_acc: 0.2500 (0.2946)  loss_scale: 65536.0000 (48077.9188)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1282  max mem: 15572
Epoch: [23]  [ 970/2809]  eta: 0:17:30  lr: 0.000022  min_lr: 0.000000  loss: 4.3316 (4.2539)  class_acc: 0.2500 (0.2945)  loss_scale: 65536.0000 (48257.7137)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1059  max mem: 15572
[2025-01-16 01:31:17,121] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65586
[2025-01-16 01:31:17,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:31:17,121] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 980/2809]  eta: 0:17:24  lr: 0.000022  min_lr: 0.000000  loss: 4.2979 (4.2535)  class_acc: 0.2500 (0.2948)  loss_scale: 65536.0000 (48367.0377)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.1082  max mem: 15572
Epoch: [23]  [ 990/2809]  eta: 0:17:18  lr: 0.000022  min_lr: 0.000000  loss: 4.2287 (4.2525)  class_acc: 0.2917 (0.2950)  loss_scale: 32768.0000 (48209.6307)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1311  max mem: 15572
Epoch: [23]  [1000/2809]  eta: 0:17:14  lr: 0.000022  min_lr: 0.000000  loss: 4.2287 (4.2539)  class_acc: 0.2917 (0.2948)  loss_scale: 32768.0000 (48055.3686)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.2001  max mem: 15572
Epoch: [23]  [1010/2809]  eta: 0:17:08  lr: 0.000022  min_lr: 0.000000  loss: 4.2189 (4.2524)  class_acc: 0.2917 (0.2957)  loss_scale: 32768.0000 (47904.1583)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1850  max mem: 15572
Epoch: [23]  [1020/2809]  eta: 0:17:02  lr: 0.000022  min_lr: 0.000000  loss: 4.1901 (4.2524)  class_acc: 0.2917 (0.2951)  loss_scale: 32768.0000 (47755.9099)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0955  max mem: 15572
Epoch: [23]  [1030/2809]  eta: 0:16:56  lr: 0.000022  min_lr: 0.000000  loss: 4.3157 (4.2525)  class_acc: 0.2083 (0.2943)  loss_scale: 32768.0000 (47610.5373)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0961  max mem: 15572
Epoch: [23]  [1040/2809]  eta: 0:16:51  lr: 0.000022  min_lr: 0.000000  loss: 4.2904 (4.2527)  class_acc: 0.2500 (0.2945)  loss_scale: 32768.0000 (47467.9577)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1439  max mem: 15572
Epoch: [23]  [1050/2809]  eta: 0:16:46  lr: 0.000022  min_lr: 0.000000  loss: 4.1893 (4.2514)  class_acc: 0.2917 (0.2943)  loss_scale: 32768.0000 (47328.0913)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1774  max mem: 15572
Epoch: [23]  [1060/2809]  eta: 0:16:42  lr: 0.000022  min_lr: 0.000000  loss: 4.2352 (4.2528)  class_acc: 0.2083 (0.2939)  loss_scale: 32768.0000 (47190.8615)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1816  max mem: 15572
Epoch: [23]  [1070/2809]  eta: 0:16:36  lr: 0.000022  min_lr: 0.000000  loss: 4.3634 (4.2536)  class_acc: 0.2500 (0.2942)  loss_scale: 32768.0000 (47056.1942)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1671  max mem: 15572
Epoch: [23]  [1080/2809]  eta: 0:16:30  lr: 0.000022  min_lr: 0.000000  loss: 4.3538 (4.2533)  class_acc: 0.3333 (0.2946)  loss_scale: 32768.0000 (46924.0185)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1014  max mem: 15572
Epoch: [23]  [1090/2809]  eta: 0:16:26  lr: 0.000022  min_lr: 0.000000  loss: 4.3479 (4.2534)  class_acc: 0.2083 (0.2939)  loss_scale: 32768.0000 (46794.2658)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1618  max mem: 15572
Epoch: [23]  [1100/2809]  eta: 0:16:19  lr: 0.000022  min_lr: 0.000000  loss: 4.3083 (4.2530)  class_acc: 0.2083 (0.2943)  loss_scale: 32768.0000 (46666.8701)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1563  max mem: 15572
[2025-01-16 01:32:32,769] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:32:32,770] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [1110/2809]  eta: 0:16:12  lr: 0.000022  min_lr: 0.000000  loss: 4.2020 (4.2524)  class_acc: 0.2917 (0.2945)  loss_scale: 32768.0000 (46630.2502)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0641  max mem: 15572
Epoch: [23]  [1120/2809]  eta: 0:16:07  lr: 0.000022  min_lr: 0.000000  loss: 4.2281 (4.2530)  class_acc: 0.2500 (0.2943)  loss_scale: 65536.0000 (46798.9010)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1184  max mem: 15572
[2025-01-16 01:32:40,444] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65729
[2025-01-16 01:32:40,445] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:32:40,445] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [1130/2809]  eta: 0:16:00  lr: 0.000022  min_lr: 0.000000  loss: 4.2360 (4.2526)  class_acc: 0.3333 (0.2952)  loss_scale: 65536.0000 (46703.8161)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1129  max mem: 15572
Epoch: [23]  [1140/2809]  eta: 0:15:54  lr: 0.000022  min_lr: 0.000000  loss: 4.2977 (4.2528)  class_acc: 0.2917 (0.2950)  loss_scale: 32768.0000 (46581.6792)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1049  max mem: 15572
Epoch: [23]  [1150/2809]  eta: 0:15:48  lr: 0.000022  min_lr: 0.000000  loss: 4.3147 (4.2534)  class_acc: 0.2500 (0.2948)  loss_scale: 32768.0000 (46461.6646)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0947  max mem: 15572
Epoch: [23]  [1160/2809]  eta: 0:15:41  lr: 0.000022  min_lr: 0.000000  loss: 4.3549 (4.2542)  class_acc: 0.2083 (0.2942)  loss_scale: 32768.0000 (46343.7175)  weight_decay: 0.0500 (0.0500)  time: 0.5034  data: 0.0547  max mem: 15572
Epoch: [23]  [1170/2809]  eta: 0:15:36  lr: 0.000022  min_lr: 0.000000  loss: 4.2180 (4.2525)  class_acc: 0.2500 (0.2948)  loss_scale: 32768.0000 (46227.7848)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0901  max mem: 15572
Epoch: [23]  [1180/2809]  eta: 0:15:28  lr: 0.000022  min_lr: 0.000000  loss: 4.2180 (4.2535)  class_acc: 0.3333 (0.2949)  loss_scale: 32768.0000 (46113.8154)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0805  max mem: 15572
Epoch: [23]  [1190/2809]  eta: 0:15:23  lr: 0.000022  min_lr: 0.000000  loss: 4.3380 (4.2540)  class_acc: 0.2500 (0.2945)  loss_scale: 32768.0000 (46001.7599)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0973  max mem: 15572
Epoch: [23]  [1200/2809]  eta: 0:15:18  lr: 0.000022  min_lr: 0.000000  loss: 4.2935 (4.2542)  class_acc: 0.2083 (0.2941)  loss_scale: 32768.0000 (45891.5704)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1537  max mem: 15572
Epoch: [23]  [1210/2809]  eta: 0:15:12  lr: 0.000022  min_lr: 0.000000  loss: 4.2282 (4.2535)  class_acc: 0.2917 (0.2946)  loss_scale: 32768.0000 (45783.2007)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1425  max mem: 15572
Epoch: [23]  [1220/2809]  eta: 0:15:07  lr: 0.000021  min_lr: 0.000000  loss: 4.0932 (4.2522)  class_acc: 0.3333 (0.2949)  loss_scale: 32768.0000 (45676.6061)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1413  max mem: 15572
Epoch: [23]  [1230/2809]  eta: 0:15:01  lr: 0.000021  min_lr: 0.000000  loss: 4.0360 (4.2508)  class_acc: 0.2917 (0.2955)  loss_scale: 32768.0000 (45571.7433)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1403  max mem: 15572
Epoch: [23]  [1240/2809]  eta: 0:14:56  lr: 0.000021  min_lr: 0.000000  loss: 4.1356 (4.2509)  class_acc: 0.2917 (0.2952)  loss_scale: 32768.0000 (45468.5705)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1574  max mem: 15572
Epoch: [23]  [1250/2809]  eta: 0:14:50  lr: 0.000021  min_lr: 0.000000  loss: 4.2729 (4.2519)  class_acc: 0.2500 (0.2952)  loss_scale: 32768.0000 (45367.0472)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1450  max mem: 15572
[2025-01-16 01:33:53,959] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:33:53,959] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 01:33:56,683] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65864
[2025-01-16 01:33:56,684] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:33:56,684] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [1260/2809]  eta: 0:14:44  lr: 0.000021  min_lr: 0.000000  loss: 4.3038 (4.2528)  class_acc: 0.2500 (0.2950)  loss_scale: 32768.0000 (45423.0484)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.1015  max mem: 15572
Epoch: [23]  [1270/2809]  eta: 0:14:39  lr: 0.000021  min_lr: 0.000000  loss: 4.3045 (4.2538)  class_acc: 0.2500 (0.2950)  loss_scale: 32768.0000 (45323.4807)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1284  max mem: 15572
Epoch: [23]  [1280/2809]  eta: 0:14:33  lr: 0.000021  min_lr: 0.000000  loss: 4.3203 (4.2542)  class_acc: 0.2917 (0.2949)  loss_scale: 32768.0000 (45225.4676)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1076  max mem: 15572
Epoch: [23]  [1290/2809]  eta: 0:14:27  lr: 0.000021  min_lr: 0.000000  loss: 4.2088 (4.2529)  class_acc: 0.2917 (0.2953)  loss_scale: 32768.0000 (45128.9729)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0780  max mem: 15572
Epoch: [23]  [1300/2809]  eta: 0:14:21  lr: 0.000021  min_lr: 0.000000  loss: 4.2582 (4.2539)  class_acc: 0.2917 (0.2952)  loss_scale: 32768.0000 (45033.9616)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.1073  max mem: 15572
Epoch: [23]  [1310/2809]  eta: 0:14:15  lr: 0.000021  min_lr: 0.000000  loss: 4.3634 (4.2543)  class_acc: 0.2500 (0.2949)  loss_scale: 32768.0000 (44940.3997)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0924  max mem: 15572
Epoch: [23]  [1320/2809]  eta: 0:14:08  lr: 0.000021  min_lr: 0.000000  loss: 4.4027 (4.2546)  class_acc: 0.2500 (0.2948)  loss_scale: 32768.0000 (44848.2544)  weight_decay: 0.0500 (0.0500)  time: 0.5099  data: 0.0541  max mem: 15572
Epoch: [23]  [1330/2809]  eta: 0:14:02  lr: 0.000021  min_lr: 0.000000  loss: 4.3186 (4.2548)  class_acc: 0.2500 (0.2945)  loss_scale: 32768.0000 (44757.4936)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0728  max mem: 15572
Epoch: [23]  [1340/2809]  eta: 0:13:56  lr: 0.000021  min_lr: 0.000000  loss: 4.3029 (4.2547)  class_acc: 0.2917 (0.2946)  loss_scale: 32768.0000 (44668.0865)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1179  max mem: 15572
Epoch: [23]  [1350/2809]  eta: 0:13:51  lr: 0.000021  min_lr: 0.000000  loss: 4.1919 (4.2539)  class_acc: 0.2917 (0.2944)  loss_scale: 32768.0000 (44580.0030)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1329  max mem: 15572
Epoch: [23]  [1360/2809]  eta: 0:13:45  lr: 0.000021  min_lr: 0.000000  loss: 4.1502 (4.2533)  class_acc: 0.2500 (0.2947)  loss_scale: 32768.0000 (44493.2138)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1290  max mem: 15572
[2025-01-16 01:34:56,165] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65971
[2025-01-16 01:34:56,165] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 01:34:56,165] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [1370/2809]  eta: 0:13:39  lr: 0.000021  min_lr: 0.000000  loss: 4.0991 (4.2517)  class_acc: 0.3333 (0.2957)  loss_scale: 32768.0000 (44324.0379)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1152  max mem: 15572
Epoch: [23]  [1380/2809]  eta: 0:13:34  lr: 0.000021  min_lr: 0.000000  loss: 4.0680 (4.2514)  class_acc: 0.2917 (0.2954)  loss_scale: 16384.0000 (44121.7205)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1234  max mem: 15572
Epoch: [23]  [1390/2809]  eta: 0:13:28  lr: 0.000021  min_lr: 0.000000  loss: 4.1433 (4.2510)  class_acc: 0.2500 (0.2947)  loss_scale: 16384.0000 (43922.3120)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1057  max mem: 15572
[2025-01-16 01:35:11,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=66000, skipped=412, lr=[2.0706568880734621e-07, 2.0706568880734621e-07, 2.958081268676375e-07, 2.958081268676375e-07, 4.2258303838233926e-07, 4.2258303838233926e-07, 6.036900548319133e-07, 6.036900548319133e-07, 8.624143640455905e-07, 8.624143640455905e-07, 1.2320205200651293e-06, 1.2320205200651293e-06, 1.7600293143787562e-06, 1.7600293143787562e-06, 2.514327591969652e-06, 2.514327591969652e-06, 3.5918965599566456e-06, 3.5918965599566456e-06, 5.131280799938066e-06, 5.131280799938066e-06, 7.330401142768665e-06, 7.330401142768665e-06, 1.0472001632526666e-05, 1.0472001632526666e-05, 1.4960002332180953e-05, 1.4960002332180953e-05, 2.1371431903115647e-05, 2.1371431903115647e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:35:11,261] [INFO] [timer.py:260:stop] epoch=0/micro_step=66000/global_step=66000, RunningAvgSamplesPerSec=27.85433313567242, CurrSamplesPerSec=31.50112562923519, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [23]  [1400/2809]  eta: 0:13:22  lr: 0.000021  min_lr: 0.000000  loss: 4.2210 (4.2508)  class_acc: 0.2500 (0.2948)  loss_scale: 16384.0000 (43725.7502)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1117  max mem: 15572
Epoch: [23]  [1410/2809]  eta: 0:13:17  lr: 0.000021  min_lr: 0.000000  loss: 4.1919 (4.2502)  class_acc: 0.2500 (0.2946)  loss_scale: 16384.0000 (43531.9745)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1531  max mem: 15572
Epoch: [23]  [1420/2809]  eta: 0:13:11  lr: 0.000021  min_lr: 0.000000  loss: 4.1834 (4.2495)  class_acc: 0.2500 (0.2945)  loss_scale: 16384.0000 (43340.9261)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1272  max mem: 15572
Epoch: [23]  [1430/2809]  eta: 0:13:05  lr: 0.000021  min_lr: 0.000000  loss: 4.2485 (4.2487)  class_acc: 0.3333 (0.2948)  loss_scale: 16384.0000 (43152.5479)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0687  max mem: 15572
Epoch: [23]  [1440/2809]  eta: 0:12:59  lr: 0.000021  min_lr: 0.000000  loss: 4.2508 (4.2487)  class_acc: 0.3333 (0.2949)  loss_scale: 16384.0000 (42966.7842)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0719  max mem: 15572
Epoch: [23]  [1450/2809]  eta: 0:12:53  lr: 0.000021  min_lr: 0.000000  loss: 4.3125 (4.2488)  class_acc: 0.3333 (0.2949)  loss_scale: 16384.0000 (42783.5810)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1339  max mem: 15572
Epoch: [23]  [1460/2809]  eta: 0:12:47  lr: 0.000021  min_lr: 0.000000  loss: 4.3698 (4.2494)  class_acc: 0.2500 (0.2946)  loss_scale: 16384.0000 (42602.8857)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1227  max mem: 15572
Epoch: [23]  [1470/2809]  eta: 0:12:41  lr: 0.000021  min_lr: 0.000000  loss: 4.4480 (4.2506)  class_acc: 0.2500 (0.2947)  loss_scale: 16384.0000 (42424.6472)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0790  max mem: 15572
Epoch: [23]  [1480/2809]  eta: 0:12:36  lr: 0.000021  min_lr: 0.000000  loss: 4.5005 (4.2517)  class_acc: 0.2500 (0.2945)  loss_scale: 16384.0000 (42248.8157)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.1072  max mem: 15572
Epoch: [23]  [1490/2809]  eta: 0:12:30  lr: 0.000021  min_lr: 0.000000  loss: 4.3337 (4.2513)  class_acc: 0.2500 (0.2943)  loss_scale: 16384.0000 (42075.3427)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1333  max mem: 15572
[2025-01-16 01:36:08,988] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:36:08,988] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [1500/2809]  eta: 0:12:25  lr: 0.000021  min_lr: 0.000000  loss: 4.3530 (4.2523)  class_acc: 0.2500 (0.2939)  loss_scale: 16384.0000 (41991.5043)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1498  max mem: 15572
Epoch: [23]  [1510/2809]  eta: 0:12:19  lr: 0.000021  min_lr: 0.000000  loss: 4.3725 (4.2523)  class_acc: 0.2500 (0.2939)  loss_scale: 32768.0000 (41930.4619)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1655  max mem: 15572
Epoch: [23]  [1520/2809]  eta: 0:12:14  lr: 0.000021  min_lr: 0.000000  loss: 4.2341 (4.2526)  class_acc: 0.2917 (0.2938)  loss_scale: 32768.0000 (41870.2222)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1637  max mem: 15572
Epoch: [23]  [1530/2809]  eta: 0:12:09  lr: 0.000021  min_lr: 0.000000  loss: 4.3171 (4.2534)  class_acc: 0.2917 (0.2940)  loss_scale: 32768.0000 (41810.7694)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1783  max mem: 15572
Epoch: [23]  [1540/2809]  eta: 0:12:03  lr: 0.000021  min_lr: 0.000000  loss: 4.1806 (4.2532)  class_acc: 0.3333 (0.2945)  loss_scale: 32768.0000 (41752.0883)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1470  max mem: 15572
Epoch: [23]  [1550/2809]  eta: 0:11:57  lr: 0.000021  min_lr: 0.000000  loss: 4.2082 (4.2531)  class_acc: 0.2917 (0.2945)  loss_scale: 32768.0000 (41694.1638)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1223  max mem: 15572
Epoch: [23]  [1560/2809]  eta: 0:11:51  lr: 0.000021  min_lr: 0.000000  loss: 4.2732 (4.2526)  class_acc: 0.3333 (0.2948)  loss_scale: 32768.0000 (41636.9814)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0968  max mem: 15572
Epoch: [23]  [1570/2809]  eta: 0:11:45  lr: 0.000021  min_lr: 0.000000  loss: 4.2614 (4.2528)  class_acc: 0.3750 (0.2953)  loss_scale: 32768.0000 (41580.5271)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.0827  max mem: 15572
Epoch: [23]  [1580/2809]  eta: 0:11:40  lr: 0.000021  min_lr: 0.000000  loss: 4.2033 (4.2523)  class_acc: 0.3750 (0.2956)  loss_scale: 32768.0000 (41524.7868)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1303  max mem: 15572
Epoch: [23]  [1590/2809]  eta: 0:11:34  lr: 0.000021  min_lr: 0.000000  loss: 4.1467 (4.2518)  class_acc: 0.3333 (0.2960)  loss_scale: 32768.0000 (41469.7473)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1441  max mem: 15572
Epoch: [23]  [1600/2809]  eta: 0:11:29  lr: 0.000021  min_lr: 0.000000  loss: 4.3531 (4.2523)  class_acc: 0.3333 (0.2961)  loss_scale: 32768.0000 (41415.3954)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1482  max mem: 15572
Epoch: [23]  [1610/2809]  eta: 0:11:23  lr: 0.000021  min_lr: 0.000000  loss: 4.3810 (4.2518)  class_acc: 0.3333 (0.2963)  loss_scale: 32768.0000 (41361.7182)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1575  max mem: 15572
Epoch: [23]  [1620/2809]  eta: 0:11:17  lr: 0.000021  min_lr: 0.000000  loss: 4.0785 (4.2508)  class_acc: 0.2500 (0.2961)  loss_scale: 32768.0000 (41308.7033)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1255  max mem: 15572
[2025-01-16 01:37:23,430] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:37:23,431] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [1630/2809]  eta: 0:11:12  lr: 0.000021  min_lr: 0.000000  loss: 4.1565 (4.2509)  class_acc: 0.2500 (0.2964)  loss_scale: 32768.0000 (41457.2459)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1004  max mem: 15572
Epoch: [23]  [1640/2809]  eta: 0:11:06  lr: 0.000021  min_lr: 0.000000  loss: 4.2222 (4.2502)  class_acc: 0.2917 (0.2964)  loss_scale: 65536.0000 (41603.9781)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1394  max mem: 15572
Epoch: [23]  [1650/2809]  eta: 0:11:01  lr: 0.000021  min_lr: 0.000000  loss: 4.2249 (4.2506)  class_acc: 0.2917 (0.2965)  loss_scale: 65536.0000 (41748.9328)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1413  max mem: 15572
Epoch: [23]  [1660/2809]  eta: 0:10:54  lr: 0.000021  min_lr: 0.000000  loss: 4.2249 (4.2501)  class_acc: 0.2500 (0.2963)  loss_scale: 65536.0000 (41892.1421)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0645  max mem: 15572
Epoch: [23]  [1670/2809]  eta: 0:10:48  lr: 0.000021  min_lr: 0.000000  loss: 4.1379 (4.2498)  class_acc: 0.2500 (0.2964)  loss_scale: 65536.0000 (42033.6373)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0827  max mem: 15572
Epoch: [23]  [1680/2809]  eta: 0:10:43  lr: 0.000021  min_lr: 0.000000  loss: 4.1945 (4.2504)  class_acc: 0.2917 (0.2963)  loss_scale: 65536.0000 (42173.4491)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1593  max mem: 15572
Epoch: [23]  [1690/2809]  eta: 0:10:38  lr: 0.000021  min_lr: 0.000000  loss: 4.3792 (4.2505)  class_acc: 0.2917 (0.2966)  loss_scale: 65536.0000 (42311.6073)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1576  max mem: 15572
Epoch: [23]  [1700/2809]  eta: 0:10:32  lr: 0.000021  min_lr: 0.000000  loss: 4.2565 (4.2506)  class_acc: 0.2917 (0.2965)  loss_scale: 65536.0000 (42448.1411)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1171  max mem: 15572
Epoch: [23]  [1710/2809]  eta: 0:10:25  lr: 0.000021  min_lr: 0.000000  loss: 4.0918 (4.2489)  class_acc: 0.2917 (0.2966)  loss_scale: 65536.0000 (42583.0789)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0687  max mem: 15572
Epoch: [23]  [1720/2809]  eta: 0:10:20  lr: 0.000021  min_lr: 0.000000  loss: 4.1056 (4.2487)  class_acc: 0.2917 (0.2968)  loss_scale: 65536.0000 (42716.4486)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1109  max mem: 15572
Epoch: [23]  [1730/2809]  eta: 0:10:14  lr: 0.000021  min_lr: 0.000000  loss: 4.2763 (4.2487)  class_acc: 0.2917 (0.2968)  loss_scale: 65536.0000 (42848.2773)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0907  max mem: 15572
Epoch: [23]  [1740/2809]  eta: 0:10:08  lr: 0.000021  min_lr: 0.000000  loss: 4.2670 (4.2483)  class_acc: 0.2917 (0.2967)  loss_scale: 65536.0000 (42978.5916)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0466  max mem: 15572
[2025-01-16 01:38:33,403] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:38:33,403] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [1750/2809]  eta: 0:10:02  lr: 0.000021  min_lr: 0.000000  loss: 4.2463 (4.2490)  class_acc: 0.2917 (0.2965)  loss_scale: 65536.0000 (43182.2730)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0986  max mem: 15572
[2025-01-16 01:38:35,015] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66358
[2025-01-16 01:38:35,015] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:38:35,016] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [1760/2809]  eta: 0:09:56  lr: 0.000021  min_lr: 0.000000  loss: 4.2901 (4.2498)  class_acc: 0.2917 (0.2963)  loss_scale: 65536.0000 (43309.2107)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.1015  max mem: 15572
Epoch: [23]  [1770/2809]  eta: 0:09:51  lr: 0.000021  min_lr: 0.000000  loss: 4.3567 (4.2503)  class_acc: 0.2500 (0.2963)  loss_scale: 65536.0000 (43434.7149)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1655  max mem: 15572
Epoch: [23]  [1780/2809]  eta: 0:09:45  lr: 0.000021  min_lr: 0.000000  loss: 4.3567 (4.2508)  class_acc: 0.2500 (0.2960)  loss_scale: 65536.0000 (43558.8097)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1788  max mem: 15572
Epoch: [23]  [1790/2809]  eta: 0:09:39  lr: 0.000021  min_lr: 0.000000  loss: 4.4146 (4.2513)  class_acc: 0.2500 (0.2958)  loss_scale: 65536.0000 (43681.5187)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1196  max mem: 15572
[2025-01-16 01:39:00,393] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66398
[2025-01-16 01:39:00,394] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:39:00,394] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [1800/2809]  eta: 0:09:35  lr: 0.000021  min_lr: 0.000000  loss: 4.3298 (4.2513)  class_acc: 0.2917 (0.2957)  loss_scale: 32768.0000 (43620.9217)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.2004  max mem: 15572
Epoch: [23]  [1810/2809]  eta: 0:09:29  lr: 0.000021  min_lr: 0.000000  loss: 4.2215 (4.2509)  class_acc: 0.2917 (0.2957)  loss_scale: 32768.0000 (43560.9939)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1749  max mem: 15572
Epoch: [23]  [1820/2809]  eta: 0:09:22  lr: 0.000021  min_lr: 0.000000  loss: 4.2327 (4.2509)  class_acc: 0.3333 (0.2959)  loss_scale: 32768.0000 (43501.7243)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0373  max mem: 15572
Epoch: [23]  [1830/2809]  eta: 0:09:17  lr: 0.000021  min_lr: 0.000000  loss: 4.3034 (4.2506)  class_acc: 0.2917 (0.2956)  loss_scale: 32768.0000 (43443.1021)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1026  max mem: 15572
Epoch: [23]  [1840/2809]  eta: 0:09:11  lr: 0.000021  min_lr: 0.000000  loss: 4.1749 (4.2500)  class_acc: 0.2917 (0.2959)  loss_scale: 32768.0000 (43385.1168)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1026  max mem: 15572
Epoch: [23]  [1850/2809]  eta: 0:09:05  lr: 0.000021  min_lr: 0.000000  loss: 4.1749 (4.2505)  class_acc: 0.2917 (0.2959)  loss_scale: 32768.0000 (43327.7580)  weight_decay: 0.0500 (0.0500)  time: 0.5210  data: 0.0719  max mem: 15572
Epoch: [23]  [1860/2809]  eta: 0:08:59  lr: 0.000021  min_lr: 0.000000  loss: 4.3573 (4.2515)  class_acc: 0.2917 (0.2963)  loss_scale: 32768.0000 (43271.0156)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0718  max mem: 15572
Epoch: [23]  [1870/2809]  eta: 0:08:54  lr: 0.000021  min_lr: 0.000000  loss: 4.1467 (4.2511)  class_acc: 0.3750 (0.2965)  loss_scale: 32768.0000 (43214.8797)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.1050  max mem: 15572
Epoch: [23]  [1880/2809]  eta: 0:08:49  lr: 0.000021  min_lr: 0.000000  loss: 4.3271 (4.2520)  class_acc: 0.2917 (0.2963)  loss_scale: 32768.0000 (43159.3408)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.2202  max mem: 15572
Epoch: [23]  [1890/2809]  eta: 0:08:43  lr: 0.000021  min_lr: 0.000000  loss: 4.2281 (4.2510)  class_acc: 0.2917 (0.2967)  loss_scale: 32768.0000 (43104.3892)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1630  max mem: 15572
Epoch: [23]  [1900/2809]  eta: 0:08:37  lr: 0.000021  min_lr: 0.000000  loss: 4.1136 (4.2511)  class_acc: 0.3333 (0.2967)  loss_scale: 32768.0000 (43050.0158)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1252  max mem: 15572
Epoch: [23]  [1910/2809]  eta: 0:08:32  lr: 0.000021  min_lr: 0.000000  loss: 4.2419 (4.2510)  class_acc: 0.2917 (0.2969)  loss_scale: 32768.0000 (42996.2114)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1559  max mem: 15572
[2025-01-16 01:40:12,653] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:40:12,653] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [1920/2809]  eta: 0:08:26  lr: 0.000021  min_lr: 0.000000  loss: 4.2337 (4.2509)  class_acc: 0.2500 (0.2968)  loss_scale: 32768.0000 (42960.0250)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1622  max mem: 15572
[2025-01-16 01:40:15,552] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66532
[2025-01-16 01:40:15,552] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:40:15,552] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [1930/2809]  eta: 0:08:20  lr: 0.000021  min_lr: 0.000000  loss: 4.3151 (4.2512)  class_acc: 0.2500 (0.2969)  loss_scale: 32768.0000 (42975.1217)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1261  max mem: 15572
Epoch: [23]  [1940/2809]  eta: 0:08:14  lr: 0.000021  min_lr: 0.000000  loss: 4.3569 (4.2509)  class_acc: 0.3333 (0.2972)  loss_scale: 32768.0000 (42922.5348)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0958  max mem: 15572
Epoch: [23]  [1950/2809]  eta: 0:08:09  lr: 0.000021  min_lr: 0.000000  loss: 4.3272 (4.2512)  class_acc: 0.2500 (0.2968)  loss_scale: 32768.0000 (42870.4869)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1547  max mem: 15572
Epoch: [23]  [1960/2809]  eta: 0:08:03  lr: 0.000021  min_lr: 0.000000  loss: 4.3417 (4.2518)  class_acc: 0.2083 (0.2966)  loss_scale: 32768.0000 (42818.9699)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1493  max mem: 15572
Epoch: [23]  [1970/2809]  eta: 0:07:57  lr: 0.000021  min_lr: 0.000000  loss: 4.4235 (4.2522)  class_acc: 0.2083 (0.2961)  loss_scale: 32768.0000 (42767.9756)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0722  max mem: 15572
Epoch: [23]  [1980/2809]  eta: 0:07:51  lr: 0.000021  min_lr: 0.000000  loss: 4.2441 (4.2517)  class_acc: 0.2500 (0.2963)  loss_scale: 32768.0000 (42717.4962)  weight_decay: 0.0500 (0.0500)  time: 0.4904  data: 0.0508  max mem: 15572
Epoch: [23]  [1990/2809]  eta: 0:07:46  lr: 0.000021  min_lr: 0.000000  loss: 4.2505 (4.2518)  class_acc: 0.2917 (0.2962)  loss_scale: 32768.0000 (42667.5239)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1073  max mem: 15572
Epoch: [23]  [2000/2809]  eta: 0:07:40  lr: 0.000021  min_lr: 0.000000  loss: 4.2804 (4.2518)  class_acc: 0.2917 (0.2963)  loss_scale: 32768.0000 (42618.0510)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1651  max mem: 15572
Epoch: [23]  [2010/2809]  eta: 0:07:34  lr: 0.000021  min_lr: 0.000000  loss: 4.2804 (4.2516)  class_acc: 0.2917 (0.2962)  loss_scale: 32768.0000 (42569.0701)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1417  max mem: 15572
Epoch: [23]  [2020/2809]  eta: 0:07:29  lr: 0.000021  min_lr: 0.000000  loss: 4.2147 (4.2510)  class_acc: 0.2917 (0.2964)  loss_scale: 32768.0000 (42520.5740)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1309  max mem: 15572
Epoch: [23]  [2030/2809]  eta: 0:07:23  lr: 0.000021  min_lr: 0.000000  loss: 4.2204 (4.2512)  class_acc: 0.2500 (0.2964)  loss_scale: 32768.0000 (42472.5554)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1209  max mem: 15572
Epoch: [23]  [2040/2809]  eta: 0:07:17  lr: 0.000021  min_lr: 0.000000  loss: 4.2204 (4.2513)  class_acc: 0.2500 (0.2964)  loss_scale: 32768.0000 (42425.0073)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0826  max mem: 15572
Epoch: [23]  [2050/2809]  eta: 0:07:11  lr: 0.000021  min_lr: 0.000000  loss: 4.1796 (4.2515)  class_acc: 0.2917 (0.2963)  loss_scale: 32768.0000 (42377.9230)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1299  max mem: 15572
[2025-01-16 01:41:28,194] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:41:28,195] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [2060/2809]  eta: 0:07:06  lr: 0.000021  min_lr: 0.000000  loss: 4.2745 (4.2514)  class_acc: 0.3333 (0.2965)  loss_scale: 32768.0000 (42442.5890)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1471  max mem: 15572
[2025-01-16 01:41:36,190] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66677
[2025-01-16 01:41:36,190] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:41:36,191] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [2070/2809]  eta: 0:07:00  lr: 0.000021  min_lr: 0.000000  loss: 4.2745 (4.2516)  class_acc: 0.3333 (0.2966)  loss_scale: 65536.0000 (42538.2752)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0901  max mem: 15572
Epoch: [23]  [2080/2809]  eta: 0:06:54  lr: 0.000021  min_lr: 0.000000  loss: 4.3179 (4.2521)  class_acc: 0.2917 (0.2965)  loss_scale: 32768.0000 (42491.3253)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1253  max mem: 15572
Epoch: [23]  [2090/2809]  eta: 0:06:49  lr: 0.000021  min_lr: 0.000000  loss: 4.1928 (4.2510)  class_acc: 0.3333 (0.2967)  loss_scale: 32768.0000 (42444.8245)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1395  max mem: 15572
Epoch: [23]  [2100/2809]  eta: 0:06:43  lr: 0.000021  min_lr: 0.000000  loss: 4.1191 (4.2510)  class_acc: 0.2500 (0.2965)  loss_scale: 32768.0000 (42398.7663)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1099  max mem: 15572
Epoch: [23]  [2110/2809]  eta: 0:06:37  lr: 0.000021  min_lr: 0.000000  loss: 4.1754 (4.2508)  class_acc: 0.2500 (0.2964)  loss_scale: 32768.0000 (42353.1445)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0958  max mem: 15572
Epoch: [23]  [2120/2809]  eta: 0:06:31  lr: 0.000021  min_lr: 0.000000  loss: 4.2020 (4.2507)  class_acc: 0.2500 (0.2964)  loss_scale: 32768.0000 (42307.9529)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0610  max mem: 15572
Epoch: [23]  [2130/2809]  eta: 0:06:25  lr: 0.000021  min_lr: 0.000000  loss: 4.1805 (4.2502)  class_acc: 0.2500 (0.2964)  loss_scale: 32768.0000 (42263.1854)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0540  max mem: 15572
Epoch: [23]  [2140/2809]  eta: 0:06:19  lr: 0.000021  min_lr: 0.000000  loss: 4.2187 (4.2506)  class_acc: 0.2917 (0.2964)  loss_scale: 32768.0000 (42218.8361)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0876  max mem: 15572
Epoch: [23]  [2150/2809]  eta: 0:06:14  lr: 0.000021  min_lr: 0.000000  loss: 4.2448 (4.2502)  class_acc: 0.2917 (0.2963)  loss_scale: 32768.0000 (42174.8991)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.2089  max mem: 15572
Epoch: [23]  [2160/2809]  eta: 0:06:08  lr: 0.000021  min_lr: 0.000000  loss: 4.3251 (4.2510)  class_acc: 0.2500 (0.2962)  loss_scale: 32768.0000 (42131.3688)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1773  max mem: 15572
Epoch: [23]  [2170/2809]  eta: 0:06:03  lr: 0.000021  min_lr: 0.000000  loss: 4.4008 (4.2514)  class_acc: 0.2917 (0.2963)  loss_scale: 32768.0000 (42088.2395)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0872  max mem: 15572
Epoch: [23]  [2180/2809]  eta: 0:05:57  lr: 0.000021  min_lr: 0.000000  loss: 4.3483 (4.2519)  class_acc: 0.2917 (0.2963)  loss_scale: 32768.0000 (42045.5057)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1208  max mem: 15572
Epoch: [23]  [2190/2809]  eta: 0:05:51  lr: 0.000021  min_lr: 0.000000  loss: 4.3452 (4.2519)  class_acc: 0.2500 (0.2962)  loss_scale: 32768.0000 (42003.1620)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0812  max mem: 15572
[2025-01-16 01:42:48,252] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:42:48,252] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [2200/2809]  eta: 0:05:46  lr: 0.000021  min_lr: 0.000000  loss: 4.4060 (4.2526)  class_acc: 0.2917 (0.2966)  loss_scale: 32768.0000 (41990.9786)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.0823  max mem: 15572
Epoch: [23]  [2210/2809]  eta: 0:05:40  lr: 0.000021  min_lr: 0.000000  loss: 4.4341 (4.2531)  class_acc: 0.3750 (0.2967)  loss_scale: 65536.0000 (42097.4690)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1125  max mem: 15572
Epoch: [23]  [2220/2809]  eta: 0:05:34  lr: 0.000021  min_lr: 0.000000  loss: 4.3132 (4.2534)  class_acc: 0.2917 (0.2968)  loss_scale: 65536.0000 (42203.0005)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0901  max mem: 15572
Epoch: [23]  [2230/2809]  eta: 0:05:28  lr: 0.000021  min_lr: 0.000000  loss: 4.3132 (4.2537)  class_acc: 0.2500 (0.2965)  loss_scale: 65536.0000 (42307.5858)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1019  max mem: 15572
Epoch: [23]  [2240/2809]  eta: 0:05:23  lr: 0.000021  min_lr: 0.000000  loss: 4.2978 (4.2537)  class_acc: 0.2083 (0.2964)  loss_scale: 65536.0000 (42411.2378)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1364  max mem: 15572
Epoch: [23]  [2250/2809]  eta: 0:05:17  lr: 0.000021  min_lr: 0.000000  loss: 4.2978 (4.2538)  class_acc: 0.2083 (0.2963)  loss_scale: 65536.0000 (42513.9689)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1350  max mem: 15572
[2025-01-16 01:43:18,583] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66859
[2025-01-16 01:43:18,583] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:43:18,583] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [2260/2809]  eta: 0:05:12  lr: 0.000021  min_lr: 0.000000  loss: 4.3119 (4.2540)  class_acc: 0.2500 (0.2963)  loss_scale: 65536.0000 (42485.3569)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1561  max mem: 15572
Epoch: [23]  [2270/2809]  eta: 0:05:06  lr: 0.000021  min_lr: 0.000000  loss: 4.3012 (4.2537)  class_acc: 0.2917 (0.2964)  loss_scale: 32768.0000 (42442.5680)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1377  max mem: 15572
Epoch: [23]  [2280/2809]  eta: 0:05:00  lr: 0.000021  min_lr: 0.000000  loss: 4.2513 (4.2538)  class_acc: 0.3333 (0.2965)  loss_scale: 32768.0000 (42400.1543)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0571  max mem: 15572
Epoch: [23]  [2290/2809]  eta: 0:04:54  lr: 0.000021  min_lr: 0.000000  loss: 4.3413 (4.2540)  class_acc: 0.2917 (0.2964)  loss_scale: 32768.0000 (42358.1109)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0425  max mem: 15572
Epoch: [23]  [2300/2809]  eta: 0:04:49  lr: 0.000021  min_lr: 0.000000  loss: 4.3413 (4.2536)  class_acc: 0.3333 (0.2967)  loss_scale: 32768.0000 (42316.4329)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1039  max mem: 15572
Epoch: [23]  [2310/2809]  eta: 0:04:43  lr: 0.000021  min_lr: 0.000000  loss: 4.2377 (4.2534)  class_acc: 0.3333 (0.2966)  loss_scale: 32768.0000 (42275.1155)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1112  max mem: 15572
Epoch: [23]  [2320/2809]  eta: 0:04:38  lr: 0.000021  min_lr: 0.000000  loss: 4.3070 (4.2537)  class_acc: 0.2500 (0.2964)  loss_scale: 32768.0000 (42234.1542)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1489  max mem: 15572
Epoch: [23]  [2330/2809]  eta: 0:04:32  lr: 0.000021  min_lr: 0.000000  loss: 4.2830 (4.2532)  class_acc: 0.2917 (0.2965)  loss_scale: 32768.0000 (42193.5444)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1944  max mem: 15572
Epoch: [23]  [2340/2809]  eta: 0:04:26  lr: 0.000021  min_lr: 0.000000  loss: 4.2429 (4.2533)  class_acc: 0.2917 (0.2964)  loss_scale: 32768.0000 (42153.2815)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1755  max mem: 15572
Epoch: [23]  [2350/2809]  eta: 0:04:21  lr: 0.000021  min_lr: 0.000000  loss: 4.2429 (4.2532)  class_acc: 0.2917 (0.2964)  loss_scale: 32768.0000 (42113.3611)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1445  max mem: 15572
Epoch: [23]  [2360/2809]  eta: 0:04:15  lr: 0.000021  min_lr: 0.000000  loss: 4.3798 (4.2538)  class_acc: 0.2917 (0.2965)  loss_scale: 32768.0000 (42073.7789)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0885  max mem: 15572
Epoch: [23]  [2370/2809]  eta: 0:04:09  lr: 0.000021  min_lr: 0.000000  loss: 4.2666 (4.2538)  class_acc: 0.2917 (0.2964)  loss_scale: 32768.0000 (42034.5306)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0751  max mem: 15572
Epoch: [23]  [2380/2809]  eta: 0:04:03  lr: 0.000021  min_lr: 0.000000  loss: 4.2324 (4.2541)  class_acc: 0.2917 (0.2963)  loss_scale: 32768.0000 (41995.6119)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0795  max mem: 15572
[2025-01-16 01:44:31,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:44:31,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [2390/2809]  eta: 0:03:58  lr: 0.000021  min_lr: 0.000000  loss: 4.2223 (4.2534)  class_acc: 0.2917 (0.2965)  loss_scale: 32768.0000 (42094.0661)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1288  max mem: 15572
[2025-01-16 01:44:38,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=67000, skipped=417, lr=[1.998571212807982e-07, 1.998571212807982e-07, 2.855101732582832e-07, 2.855101732582832e-07, 4.0787167608326175e-07, 4.0787167608326175e-07, 5.826738229760882e-07, 5.826738229760882e-07, 8.323911756801261e-07, 8.323911756801261e-07, 1.1891302509716086e-06, 1.1891302509716086e-06, 1.6987575013880125e-06, 1.6987575013880125e-06, 2.426796430554304e-06, 2.426796430554304e-06, 3.4668520436490055e-06, 3.4668520436490055e-06, 4.952645776641437e-06, 4.952645776641437e-06, 7.07520825234491e-06, 7.07520825234491e-06, 1.010744036049273e-05, 1.010744036049273e-05, 1.4439200514989614e-05, 1.4439200514989614e-05, 2.0627429307128022e-05, 2.0627429307128022e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:44:38,303] [INFO] [timer.py:260:stop] epoch=0/micro_step=67000/global_step=67000, RunningAvgSamplesPerSec=27.86247000687514, CurrSamplesPerSec=30.784089678890805, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [23]  [2400/2809]  eta: 0:03:52  lr: 0.000021  min_lr: 0.000000  loss: 4.0644 (4.2528)  class_acc: 0.2917 (0.2966)  loss_scale: 65536.0000 (42191.7001)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1105  max mem: 15572
Epoch: [23]  [2410/2809]  eta: 0:03:46  lr: 0.000021  min_lr: 0.000000  loss: 4.1358 (4.2529)  class_acc: 0.2500 (0.2965)  loss_scale: 65536.0000 (42288.5243)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0769  max mem: 15572
[2025-01-16 01:44:50,774] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67023
[2025-01-16 01:44:50,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:44:50,774] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [2420/2809]  eta: 0:03:41  lr: 0.000021  min_lr: 0.000000  loss: 4.2028 (4.2522)  class_acc: 0.2500 (0.2967)  loss_scale: 65536.0000 (42316.8740)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1416  max mem: 15572
Epoch: [23]  [2430/2809]  eta: 0:03:35  lr: 0.000021  min_lr: 0.000000  loss: 4.1942 (4.2520)  class_acc: 0.2500 (0.2965)  loss_scale: 32768.0000 (42277.5944)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1232  max mem: 15572
Epoch: [23]  [2440/2809]  eta: 0:03:29  lr: 0.000021  min_lr: 0.000000  loss: 4.2005 (4.2517)  class_acc: 0.2083 (0.2962)  loss_scale: 32768.0000 (42238.6366)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0686  max mem: 15572
Epoch: [23]  [2450/2809]  eta: 0:03:24  lr: 0.000021  min_lr: 0.000000  loss: 4.2087 (4.2514)  class_acc: 0.2917 (0.2963)  loss_scale: 32768.0000 (42199.9967)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1587  max mem: 15572
Epoch: [23]  [2460/2809]  eta: 0:03:18  lr: 0.000021  min_lr: 0.000000  loss: 4.2716 (4.2517)  class_acc: 0.2917 (0.2962)  loss_scale: 32768.0000 (42161.6709)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1262  max mem: 15572
Epoch: [23]  [2470/2809]  eta: 0:03:12  lr: 0.000021  min_lr: 0.000000  loss: 4.4115 (4.2522)  class_acc: 0.2500 (0.2961)  loss_scale: 32768.0000 (42123.6552)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1038  max mem: 15572
Epoch: [23]  [2480/2809]  eta: 0:03:07  lr: 0.000021  min_lr: 0.000000  loss: 4.2125 (4.2511)  class_acc: 0.2917 (0.2964)  loss_scale: 32768.0000 (42085.9460)  weight_decay: 0.0500 (0.0500)  time: 0.6939  data: 0.2501  max mem: 15572
Epoch: [23]  [2490/2809]  eta: 0:03:01  lr: 0.000021  min_lr: 0.000000  loss: 4.1897 (4.2512)  class_acc: 0.2917 (0.2965)  loss_scale: 32768.0000 (42048.5395)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1471  max mem: 15572
Epoch: [23]  [2500/2809]  eta: 0:02:55  lr: 0.000021  min_lr: 0.000000  loss: 4.2832 (4.2509)  class_acc: 0.2500 (0.2963)  loss_scale: 32768.0000 (42011.4322)  weight_decay: 0.0500 (0.0500)  time: 0.4919  data: 0.0367  max mem: 15572
Epoch: [23]  [2510/2809]  eta: 0:02:49  lr: 0.000021  min_lr: 0.000000  loss: 4.1101 (4.2505)  class_acc: 0.2500 (0.2965)  loss_scale: 32768.0000 (41974.6205)  weight_decay: 0.0500 (0.0500)  time: 0.5223  data: 0.0458  max mem: 15572
Epoch: [23]  [2520/2809]  eta: 0:02:44  lr: 0.000021  min_lr: 0.000000  loss: 4.1897 (4.2507)  class_acc: 0.2917 (0.2964)  loss_scale: 32768.0000 (41938.1008)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0781  max mem: 15572
Epoch: [23]  [2530/2809]  eta: 0:02:38  lr: 0.000021  min_lr: 0.000000  loss: 4.2988 (4.2510)  class_acc: 0.2500 (0.2964)  loss_scale: 32768.0000 (41901.8696)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1305  max mem: 15572
Epoch: [23]  [2540/2809]  eta: 0:02:32  lr: 0.000021  min_lr: 0.000000  loss: 4.3377 (4.2511)  class_acc: 0.2500 (0.2964)  loss_scale: 32768.0000 (41865.9237)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1447  max mem: 15572
[2025-01-16 01:46:05,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:46:05,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [2550/2809]  eta: 0:02:27  lr: 0.000021  min_lr: 0.000000  loss: 4.1342 (4.2506)  class_acc: 0.3333 (0.2965)  loss_scale: 32768.0000 (41907.3305)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0963  max mem: 15572
[2025-01-16 01:46:11,136] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67163
[2025-01-16 01:46:11,137] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:46:11,137] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [2560/2809]  eta: 0:02:21  lr: 0.000021  min_lr: 0.000000  loss: 4.0884 (4.2505)  class_acc: 0.2917 (0.2965)  loss_scale: 65536.0000 (41935.6189)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0851  max mem: 15572
Epoch: [23]  [2570/2809]  eta: 0:02:15  lr: 0.000020  min_lr: 0.000000  loss: 4.2300 (4.2509)  class_acc: 0.2917 (0.2965)  loss_scale: 32768.0000 (41899.9611)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1677  max mem: 15572
Epoch: [23]  [2580/2809]  eta: 0:02:10  lr: 0.000020  min_lr: 0.000000  loss: 4.3489 (4.2513)  class_acc: 0.2500 (0.2963)  loss_scale: 32768.0000 (41864.5796)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0967  max mem: 15572
Epoch: [23]  [2590/2809]  eta: 0:02:04  lr: 0.000020  min_lr: 0.000000  loss: 4.1682 (4.2506)  class_acc: 0.2083 (0.2961)  loss_scale: 32768.0000 (41829.4712)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0501  max mem: 15572
Epoch: [23]  [2600/2809]  eta: 0:01:58  lr: 0.000020  min_lr: 0.000000  loss: 4.1511 (4.2504)  class_acc: 0.2500 (0.2962)  loss_scale: 32768.0000 (41794.6328)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1052  max mem: 15572
Epoch: [23]  [2610/2809]  eta: 0:01:53  lr: 0.000020  min_lr: 0.000000  loss: 4.1981 (4.2508)  class_acc: 0.2500 (0.2960)  loss_scale: 32768.0000 (41760.0613)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1193  max mem: 15572
Epoch: [23]  [2620/2809]  eta: 0:01:47  lr: 0.000020  min_lr: 0.000000  loss: 4.0946 (4.2500)  class_acc: 0.2500 (0.2961)  loss_scale: 32768.0000 (41725.7535)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0643  max mem: 15572
Epoch: [23]  [2630/2809]  eta: 0:01:41  lr: 0.000020  min_lr: 0.000000  loss: 4.2163 (4.2506)  class_acc: 0.2500 (0.2958)  loss_scale: 32768.0000 (41691.7066)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0074  max mem: 15572
Epoch: [23]  [2640/2809]  eta: 0:01:36  lr: 0.000020  min_lr: 0.000000  loss: 4.2550 (4.2501)  class_acc: 0.2083 (0.2957)  loss_scale: 32768.0000 (41657.9175)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0963  max mem: 15572
Epoch: [23]  [2650/2809]  eta: 0:01:30  lr: 0.000020  min_lr: 0.000000  loss: 3.9965 (4.2493)  class_acc: 0.2500 (0.2955)  loss_scale: 32768.0000 (41624.3833)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1357  max mem: 15572
Epoch: [23]  [2660/2809]  eta: 0:01:24  lr: 0.000020  min_lr: 0.000000  loss: 3.9965 (4.2484)  class_acc: 0.2500 (0.2956)  loss_scale: 32768.0000 (41591.1011)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0761  max mem: 15572
Epoch: [23]  [2670/2809]  eta: 0:01:18  lr: 0.000020  min_lr: 0.000000  loss: 4.0409 (4.2479)  class_acc: 0.3333 (0.2958)  loss_scale: 32768.0000 (41558.0681)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0888  max mem: 15572
Epoch: [23]  [2680/2809]  eta: 0:01:13  lr: 0.000020  min_lr: 0.000000  loss: 4.1887 (4.2480)  class_acc: 0.2500 (0.2955)  loss_scale: 32768.0000 (41525.2816)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0926  max mem: 15572
[2025-01-16 01:47:23,497] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:47:23,497] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [2690/2809]  eta: 0:01:07  lr: 0.000020  min_lr: 0.000000  loss: 4.1990 (4.2483)  class_acc: 0.2500 (0.2955)  loss_scale: 32768.0000 (41565.8001)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0837  max mem: 15572
Epoch: [23]  [2700/2809]  eta: 0:01:01  lr: 0.000020  min_lr: 0.000000  loss: 4.2252 (4.2484)  class_acc: 0.2500 (0.2954)  loss_scale: 65536.0000 (41654.5457)  weight_decay: 0.0500 (0.0500)  time: 0.4699  data: 0.0502  max mem: 15572
Epoch: [23]  [2710/2809]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000000  loss: 4.1612 (4.2480)  class_acc: 0.2500 (0.2954)  loss_scale: 65536.0000 (41742.6367)  weight_decay: 0.0500 (0.0500)  time: 0.4097  data: 0.0003  max mem: 15572
[2025-01-16 01:47:39,108] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67326
[2025-01-16 01:47:39,108] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:47:39,108] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [2720/2809]  eta: 0:00:50  lr: 0.000020  min_lr: 0.000000  loss: 4.0863 (4.2476)  class_acc: 0.3333 (0.2957)  loss_scale: 65536.0000 (41805.9949)  weight_decay: 0.0500 (0.0500)  time: 0.4433  data: 0.0004  max mem: 15572
Epoch: [23]  [2730/2809]  eta: 0:00:44  lr: 0.000020  min_lr: 0.000000  loss: 4.2224 (4.2475)  class_acc: 0.3333 (0.2957)  loss_scale: 32768.0000 (41772.9008)  weight_decay: 0.0500 (0.0500)  time: 0.4649  data: 0.0006  max mem: 15572
Epoch: [23]  [2740/2809]  eta: 0:00:39  lr: 0.000020  min_lr: 0.000000  loss: 4.3687 (4.2481)  class_acc: 0.2500 (0.2956)  loss_scale: 32768.0000 (41740.0482)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0572  max mem: 15572
Epoch: [23]  [2750/2809]  eta: 0:00:33  lr: 0.000020  min_lr: 0.000000  loss: 4.3829 (4.2484)  class_acc: 0.2500 (0.2954)  loss_scale: 32768.0000 (41707.4344)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.1621  max mem: 15572
Epoch: [23]  [2760/2809]  eta: 0:00:27  lr: 0.000020  min_lr: 0.000000  loss: 4.4347 (4.2492)  class_acc: 0.2917 (0.2953)  loss_scale: 32768.0000 (41675.0569)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.1830  max mem: 15572
Epoch: [23]  [2770/2809]  eta: 0:00:22  lr: 0.000020  min_lr: 0.000000  loss: 4.2973 (4.2489)  class_acc: 0.2917 (0.2952)  loss_scale: 32768.0000 (41642.9130)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.2207  max mem: 15572
Epoch: [23]  [2780/2809]  eta: 0:00:16  lr: 0.000020  min_lr: 0.000000  loss: 4.2266 (4.2488)  class_acc: 0.2917 (0.2954)  loss_scale: 32768.0000 (41611.0004)  weight_decay: 0.0500 (0.0500)  time: 0.7831  data: 0.3370  max mem: 15572
Epoch: [23]  [2790/2809]  eta: 0:00:10  lr: 0.000020  min_lr: 0.000000  loss: 4.2473 (4.2490)  class_acc: 0.2917 (0.2953)  loss_scale: 32768.0000 (41579.3164)  weight_decay: 0.0500 (0.0500)  time: 0.7448  data: 0.2749  max mem: 15572
Epoch: [23]  [2800/2809]  eta: 0:00:05  lr: 0.000020  min_lr: 0.000000  loss: 4.2020 (4.2487)  class_acc: 0.2500 (0.2953)  loss_scale: 32768.0000 (41547.8586)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1580  max mem: 15572
Epoch: [23]  [2808/2809]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 4.2661 (4.2487)  class_acc: 0.2500 (0.2953)  loss_scale: 32768.0000 (41522.8537)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0920  max mem: 15572
Epoch: [23] Total time: 0:26:37 (0.5687 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 4.2661 (4.2487)  class_acc: 0.2500 (0.2953)  loss_scale: 32768.0000 (41522.8537)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:22:23  loss: 1.2603 (1.2603)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 4.9393  data: 4.7581  max mem: 15572
Val:  [ 10/272]  eta: 0:03:33  loss: 2.6577 (2.6395)  acc1: 38.8889 (40.4040)  acc5: 72.2222 (71.2121)  time: 0.8131  data: 0.6256  max mem: 15572
Val:  [ 20/272]  eta: 0:02:28  loss: 2.7568 (2.7049)  acc1: 38.8889 (41.2698)  acc5: 66.6667 (70.3704)  time: 0.3727  data: 0.1889  max mem: 15572
Val:  [ 30/272]  eta: 0:02:04  loss: 2.7677 (2.7267)  acc1: 38.8889 (40.1434)  acc5: 66.6667 (70.6093)  time: 0.3472  data: 0.1584  max mem: 15572
Val:  [ 40/272]  eta: 0:01:49  loss: 2.7019 (2.7421)  acc1: 22.2222 (36.4499)  acc5: 72.2222 (70.7317)  time: 0.3455  data: 0.1457  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 2.5784 (2.6708)  acc1: 33.3333 (38.2353)  acc5: 77.7778 (72.9847)  time: 0.2661  data: 0.0826  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 2.0883 (2.5978)  acc1: 55.5556 (40.3461)  acc5: 83.3333 (74.0437)  time: 0.2029  data: 0.0404  max mem: 15572
Val:  [ 70/272]  eta: 0:01:10  loss: 2.0588 (2.5406)  acc1: 55.5556 (43.0360)  acc5: 83.3333 (74.8826)  time: 0.1841  data: 0.0279  max mem: 15572
Val:  [ 80/272]  eta: 0:01:03  loss: 2.2706 (2.5582)  acc1: 50.0000 (42.7984)  acc5: 77.7778 (74.6228)  time: 0.1620  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:58  loss: 2.6713 (2.5731)  acc1: 38.8889 (42.9792)  acc5: 72.2222 (74.6032)  time: 0.2225  data: 0.0305  max mem: 15572
Val:  [100/272]  eta: 0:00:54  loss: 2.6803 (2.5983)  acc1: 44.4444 (42.4642)  acc5: 77.7778 (74.4224)  time: 0.2698  data: 0.0730  max mem: 15572
Val:  [110/272]  eta: 0:00:51  loss: 2.8606 (2.6556)  acc1: 27.7778 (40.8909)  acc5: 72.2222 (73.2232)  time: 0.2943  data: 0.1147  max mem: 15572
Val:  [120/272]  eta: 0:00:48  loss: 3.1879 (2.6897)  acc1: 27.7778 (40.0367)  acc5: 61.1111 (72.4977)  time: 0.3171  data: 0.1334  max mem: 15572
Val:  [130/272]  eta: 0:00:45  loss: 2.5933 (2.6616)  acc1: 44.4444 (40.9245)  acc5: 77.7778 (73.3673)  time: 0.3334  data: 0.1391  max mem: 15572
Val:  [140/272]  eta: 0:00:42  loss: 2.3047 (2.6601)  acc1: 50.0000 (41.4106)  acc5: 77.7778 (73.2467)  time: 0.3264  data: 0.1267  max mem: 15572
Val:  [150/272]  eta: 0:00:38  loss: 2.5113 (2.6514)  acc1: 38.8889 (41.1700)  acc5: 77.7778 (73.6203)  time: 0.2944  data: 0.1026  max mem: 15572
Val:  [160/272]  eta: 0:00:35  loss: 2.4909 (2.6427)  acc1: 44.4444 (41.7184)  acc5: 77.7778 (74.0166)  time: 0.2822  data: 0.0947  max mem: 15572
Val:  [170/272]  eta: 0:00:32  loss: 2.5403 (2.6543)  acc1: 33.3333 (41.2281)  acc5: 77.7778 (73.5218)  time: 0.2994  data: 0.0971  max mem: 15572
Val:  [180/272]  eta: 0:00:28  loss: 2.5044 (2.6375)  acc1: 33.3333 (41.3137)  acc5: 72.2222 (73.9411)  time: 0.3243  data: 0.1104  max mem: 15572
Val:  [190/272]  eta: 0:00:25  loss: 2.5234 (2.6664)  acc1: 33.3333 (40.2269)  acc5: 72.2222 (73.0657)  time: 0.3357  data: 0.1323  max mem: 15572
Val:  [200/272]  eta: 0:00:22  loss: 2.8153 (2.6685)  acc1: 27.7778 (39.9668)  acc5: 72.2222 (72.9409)  time: 0.3377  data: 0.1503  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.5111 (2.6714)  acc1: 33.3333 (39.9684)  acc5: 77.7778 (73.0384)  time: 0.3118  data: 0.1261  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.7278 (2.6683)  acc1: 44.4444 (40.2212)  acc5: 77.7778 (73.1021)  time: 0.3048  data: 0.1142  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.4427 (2.6597)  acc1: 50.0000 (40.8850)  acc5: 77.7778 (73.2804)  time: 0.3039  data: 0.1100  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.2617 (2.6475)  acc1: 55.5556 (41.1941)  acc5: 83.3333 (73.7437)  time: 0.3396  data: 0.1412  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.4948 (2.6522)  acc1: 33.3333 (40.7703)  acc5: 83.3333 (73.7716)  time: 0.3173  data: 0.1200  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9867 (2.6092)  acc1: 66.6667 (42.4010)  acc5: 83.3333 (74.5211)  time: 0.2596  data: 0.0609  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.1165 (2.6070)  acc1: 61.1111 (42.3124)  acc5: 88.8889 (74.7027)  time: 0.2087  data: 0.0327  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.1165 (2.6116)  acc1: 55.5556 (42.3101)  acc5: 88.8889 (74.6672)  time: 0.1960  data: 0.0327  max mem: 15572
Val: Total time: 0:01:23 (0.3076 s / it)
* Acc@1 42.310 Acc@5 74.667 loss 2.612
Accuracy of the network on the 4883 val videos: 42.3%
Max accuracy: 42.49%
Epoch: [24]  [   0/2809]  eta: 4:59:45  lr: 0.000020  min_lr: 0.000000  loss: 3.8368 (3.8368)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.4028  data: 5.9180  max mem: 15572
Epoch: [24]  [  10/2809]  eta: 0:52:13  lr: 0.000020  min_lr: 0.000000  loss: 4.2526 (4.2487)  class_acc: 0.2917 (0.3295)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1196  data: 0.6943  max mem: 15572
Epoch: [24]  [  20/2809]  eta: 0:41:24  lr: 0.000020  min_lr: 0.000000  loss: 4.1612 (4.2059)  class_acc: 0.2917 (0.3254)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1890  max mem: 15572
Epoch: [24]  [  30/2809]  eta: 0:35:48  lr: 0.000020  min_lr: 0.000000  loss: 4.2336 (4.2803)  class_acc: 0.2917 (0.2930)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1385  max mem: 15572
[2025-01-16 01:50:26,765] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:50:26,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [  40/2809]  eta: 0:33:47  lr: 0.000020  min_lr: 0.000000  loss: 4.3798 (4.2565)  class_acc: 0.2083 (0.2978)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1243  max mem: 15572
[2025-01-16 01:50:31,767] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67463
[2025-01-16 01:50:31,767] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:50:31,768] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [  50/2809]  eta: 0:31:56  lr: 0.000020  min_lr: 0.000000  loss: 4.2366 (4.2480)  class_acc: 0.2500 (0.2941)  loss_scale: 32768.0000 (37908.0784)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1439  max mem: 15572
Epoch: [24]  [  60/2809]  eta: 0:31:10  lr: 0.000020  min_lr: 0.000000  loss: 4.1893 (4.2467)  class_acc: 0.2917 (0.3060)  loss_scale: 32768.0000 (37065.4426)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1409  max mem: 15572
Epoch: [24]  [  70/2809]  eta: 0:30:06  lr: 0.000020  min_lr: 0.000000  loss: 4.1873 (4.2397)  class_acc: 0.3333 (0.3046)  loss_scale: 32768.0000 (36460.1690)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1295  max mem: 15572
Epoch: [24]  [  80/2809]  eta: 0:29:45  lr: 0.000020  min_lr: 0.000000  loss: 4.1094 (4.2308)  class_acc: 0.3333 (0.3092)  loss_scale: 32768.0000 (36004.3457)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1279  max mem: 15572
Epoch: [24]  [  90/2809]  eta: 0:29:28  lr: 0.000020  min_lr: 0.000000  loss: 4.1339 (4.2337)  class_acc: 0.2917 (0.3072)  loss_scale: 32768.0000 (35648.7033)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1712  max mem: 15572
Epoch: [24]  [ 100/2809]  eta: 0:29:03  lr: 0.000020  min_lr: 0.000000  loss: 4.2083 (4.2269)  class_acc: 0.2500 (0.3078)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1471  max mem: 15572
Epoch: [24]  [ 110/2809]  eta: 0:28:26  lr: 0.000020  min_lr: 0.000000  loss: 4.2083 (4.2291)  class_acc: 0.3333 (0.3112)  loss_scale: 32768.0000 (35129.6577)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0936  max mem: 15572
Epoch: [24]  [ 120/2809]  eta: 0:27:57  lr: 0.000020  min_lr: 0.000000  loss: 4.2662 (4.2209)  class_acc: 0.3333 (0.3103)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0795  max mem: 15572
Epoch: [24]  [ 130/2809]  eta: 0:27:40  lr: 0.000020  min_lr: 0.000000  loss: 4.2662 (4.2237)  class_acc: 0.3333 (0.3133)  loss_scale: 32768.0000 (34769.0992)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1173  max mem: 15572
Epoch: [24]  [ 140/2809]  eta: 0:27:17  lr: 0.000020  min_lr: 0.000000  loss: 4.0966 (4.2122)  class_acc: 0.3333 (0.3153)  loss_scale: 32768.0000 (34627.1773)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1074  max mem: 15572
Epoch: [24]  [ 150/2809]  eta: 0:27:09  lr: 0.000020  min_lr: 0.000000  loss: 4.1863 (4.2154)  class_acc: 0.2917 (0.3113)  loss_scale: 32768.0000 (34504.0530)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1207  max mem: 15572
Epoch: [24]  [ 160/2809]  eta: 0:26:55  lr: 0.000020  min_lr: 0.000000  loss: 4.1863 (4.2058)  class_acc: 0.2500 (0.3098)  loss_scale: 32768.0000 (34396.2236)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1400  max mem: 15572
Epoch: [24]  [ 170/2809]  eta: 0:26:40  lr: 0.000020  min_lr: 0.000000  loss: 4.2168 (4.2170)  class_acc: 0.3333 (0.3116)  loss_scale: 32768.0000 (34301.0058)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1157  max mem: 15572
[2025-01-16 01:51:45,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:51:45,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 180/2809]  eta: 0:26:14  lr: 0.000020  min_lr: 0.000000  loss: 4.2795 (4.2152)  class_acc: 0.3333 (0.3087)  loss_scale: 32768.0000 (35121.5028)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0793  max mem: 15572
Epoch: [24]  [ 190/2809]  eta: 0:25:59  lr: 0.000020  min_lr: 0.000000  loss: 4.1554 (4.2086)  class_acc: 0.2083 (0.3050)  loss_scale: 65536.0000 (36713.8848)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0639  max mem: 15572
Epoch: [24]  [ 200/2809]  eta: 0:25:57  lr: 0.000020  min_lr: 0.000000  loss: 4.1083 (4.2041)  class_acc: 0.2500 (0.3043)  loss_scale: 65536.0000 (38147.8209)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1295  max mem: 15572
Epoch: [24]  [ 210/2809]  eta: 0:25:35  lr: 0.000020  min_lr: 0.000000  loss: 4.1422 (4.1967)  class_acc: 0.2917 (0.3051)  loss_scale: 65536.0000 (39445.8389)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0995  max mem: 15572
Epoch: [24]  [ 220/2809]  eta: 0:25:17  lr: 0.000020  min_lr: 0.000000  loss: 4.2085 (4.1965)  class_acc: 0.3333 (0.3052)  loss_scale: 65536.0000 (40626.3891)  weight_decay: 0.0500 (0.0500)  time: 0.4744  data: 0.0538  max mem: 15572
Epoch: [24]  [ 230/2809]  eta: 0:25:21  lr: 0.000020  min_lr: 0.000000  loss: 4.2227 (4.1971)  class_acc: 0.2917 (0.3038)  loss_scale: 65536.0000 (41704.7273)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1588  max mem: 15572
Epoch: [24]  [ 240/2809]  eta: 0:25:13  lr: 0.000020  min_lr: 0.000000  loss: 4.2327 (4.1996)  class_acc: 0.2917 (0.3050)  loss_scale: 65536.0000 (42693.5768)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1943  max mem: 15572
Epoch: [24]  [ 250/2809]  eta: 0:24:59  lr: 0.000020  min_lr: 0.000000  loss: 4.3948 (4.2063)  class_acc: 0.2917 (0.3040)  loss_scale: 65536.0000 (43603.6335)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.1048  max mem: 15572
Epoch: [24]  [ 260/2809]  eta: 0:24:47  lr: 0.000020  min_lr: 0.000000  loss: 4.1988 (4.2050)  class_acc: 0.2083 (0.3016)  loss_scale: 65536.0000 (44443.9540)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0772  max mem: 15572
Epoch: [24]  [ 270/2809]  eta: 0:24:46  lr: 0.000020  min_lr: 0.000000  loss: 4.2299 (4.2113)  class_acc: 0.2083 (0.3001)  loss_scale: 65536.0000 (45222.2583)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1453  max mem: 15572
[2025-01-16 01:52:39,404] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67691
[2025-01-16 01:52:39,404] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:52:39,404] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 280/2809]  eta: 0:24:45  lr: 0.000020  min_lr: 0.000000  loss: 4.3466 (4.2128)  class_acc: 0.2500 (0.2997)  loss_scale: 65536.0000 (45245.4947)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.2038  max mem: 15572
Epoch: [24]  [ 290/2809]  eta: 0:24:38  lr: 0.000020  min_lr: 0.000000  loss: 4.3466 (4.2136)  class_acc: 0.2500 (0.2991)  loss_scale: 32768.0000 (44816.7148)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1777  max mem: 15572
Epoch: [24]  [ 300/2809]  eta: 0:24:27  lr: 0.000020  min_lr: 0.000000  loss: 4.1682 (4.2091)  class_acc: 0.2500 (0.2979)  loss_scale: 32768.0000 (44416.4252)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0913  max mem: 15572
Epoch: [24]  [ 310/2809]  eta: 0:24:15  lr: 0.000020  min_lr: 0.000000  loss: 4.1978 (4.2094)  class_acc: 0.2500 (0.2978)  loss_scale: 32768.0000 (44041.8778)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0511  max mem: 15572
Epoch: [24]  [ 320/2809]  eta: 0:24:12  lr: 0.000020  min_lr: 0.000000  loss: 4.2625 (4.2100)  class_acc: 0.2917 (0.2983)  loss_scale: 32768.0000 (43690.6667)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1196  max mem: 15572
Epoch: [24]  [ 330/2809]  eta: 0:24:00  lr: 0.000020  min_lr: 0.000000  loss: 4.2363 (4.2110)  class_acc: 0.2917 (0.2980)  loss_scale: 32768.0000 (43360.6767)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1081  max mem: 15572
Epoch: [24]  [ 340/2809]  eta: 0:23:52  lr: 0.000020  min_lr: 0.000000  loss: 4.1634 (4.2075)  class_acc: 0.2917 (0.2997)  loss_scale: 32768.0000 (43050.0411)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0752  max mem: 15572
Epoch: [24]  [ 350/2809]  eta: 0:23:47  lr: 0.000020  min_lr: 0.000000  loss: 4.1653 (4.2086)  class_acc: 0.3333 (0.2996)  loss_scale: 32768.0000 (42757.1054)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1255  max mem: 15572
Epoch: [24]  [ 360/2809]  eta: 0:23:41  lr: 0.000020  min_lr: 0.000000  loss: 4.2583 (4.2091)  class_acc: 0.2917 (0.3001)  loss_scale: 32768.0000 (42480.3989)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1314  max mem: 15572
Epoch: [24]  [ 370/2809]  eta: 0:23:40  lr: 0.000020  min_lr: 0.000000  loss: 4.2568 (4.2105)  class_acc: 0.2917 (0.2990)  loss_scale: 32768.0000 (42218.6092)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1834  max mem: 15572
Epoch: [24]  [ 380/2809]  eta: 0:23:31  lr: 0.000020  min_lr: 0.000000  loss: 4.2568 (4.2124)  class_acc: 0.2500 (0.2975)  loss_scale: 32768.0000 (41970.5617)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1624  max mem: 15572
Epoch: [24]  [ 390/2809]  eta: 0:23:26  lr: 0.000020  min_lr: 0.000000  loss: 4.2335 (4.2133)  class_acc: 0.2500 (0.2971)  loss_scale: 32768.0000 (41735.2020)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1143  max mem: 15572
Epoch: [24]  [ 400/2809]  eta: 0:23:15  lr: 0.000020  min_lr: 0.000000  loss: 4.2011 (4.2120)  class_acc: 0.2500 (0.2971)  loss_scale: 32768.0000 (41511.5810)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0889  max mem: 15572
[2025-01-16 01:53:52,591] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:53:52,592] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 410/2809]  eta: 0:23:04  lr: 0.000020  min_lr: 0.000000  loss: 4.2104 (4.2134)  class_acc: 0.3750 (0.2988)  loss_scale: 32768.0000 (41856.9343)  weight_decay: 0.0500 (0.0500)  time: 0.4923  data: 0.0480  max mem: 15572
Epoch: [24]  [ 420/2809]  eta: 0:22:54  lr: 0.000020  min_lr: 0.000000  loss: 4.2105 (4.2136)  class_acc: 0.3750 (0.2998)  loss_scale: 65536.0000 (42419.3824)  weight_decay: 0.0500 (0.0500)  time: 0.4917  data: 0.0613  max mem: 15572
Epoch: [24]  [ 430/2809]  eta: 0:22:44  lr: 0.000020  min_lr: 0.000000  loss: 4.1464 (4.2107)  class_acc: 0.3333 (0.2996)  loss_scale: 65536.0000 (42955.7309)  weight_decay: 0.0500 (0.0500)  time: 0.5034  data: 0.0648  max mem: 15572
Epoch: [24]  [ 440/2809]  eta: 0:22:41  lr: 0.000020  min_lr: 0.000000  loss: 4.1464 (4.2079)  class_acc: 0.3333 (0.3015)  loss_scale: 65536.0000 (43467.7551)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1079  max mem: 15572
[2025-01-16 01:54:15,178] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67863
[2025-01-16 01:54:15,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:54:15,178] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 450/2809]  eta: 0:22:39  lr: 0.000020  min_lr: 0.000000  loss: 4.1933 (4.2083)  class_acc: 0.3333 (0.3017)  loss_scale: 65536.0000 (43666.4479)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.1681  max mem: 15572
Epoch: [24]  [ 460/2809]  eta: 0:22:34  lr: 0.000020  min_lr: 0.000000  loss: 4.2470 (4.2087)  class_acc: 0.3333 (0.3010)  loss_scale: 32768.0000 (43430.0390)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1644  max mem: 15572
Epoch: [24]  [ 470/2809]  eta: 0:22:30  lr: 0.000020  min_lr: 0.000000  loss: 4.2470 (4.2098)  class_acc: 0.2500 (0.3008)  loss_scale: 32768.0000 (43203.6688)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1412  max mem: 15572
Epoch: [24]  [ 480/2809]  eta: 0:22:22  lr: 0.000020  min_lr: 0.000000  loss: 4.2004 (4.2096)  class_acc: 0.2500 (0.3008)  loss_scale: 32768.0000 (42986.7110)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1231  max mem: 15572
Epoch: [24]  [ 490/2809]  eta: 0:22:17  lr: 0.000020  min_lr: 0.000000  loss: 4.2893 (4.2119)  class_acc: 0.2917 (0.2998)  loss_scale: 32768.0000 (42778.5906)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1155  max mem: 15572
Epoch: [24]  [ 500/2809]  eta: 0:22:12  lr: 0.000020  min_lr: 0.000000  loss: 4.4172 (4.2154)  class_acc: 0.2917 (0.2990)  loss_scale: 32768.0000 (42578.7784)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1567  max mem: 15572
Epoch: [24]  [ 510/2809]  eta: 0:22:02  lr: 0.000020  min_lr: 0.000000  loss: 4.4172 (4.2180)  class_acc: 0.2917 (0.2987)  loss_scale: 32768.0000 (42386.7867)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.1118  max mem: 15572
Epoch: [24]  [ 520/2809]  eta: 0:21:55  lr: 0.000020  min_lr: 0.000000  loss: 4.2498 (4.2183)  class_acc: 0.2500 (0.2979)  loss_scale: 32768.0000 (42202.1651)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0641  max mem: 15572
Epoch: [24]  [ 530/2809]  eta: 0:21:48  lr: 0.000020  min_lr: 0.000000  loss: 4.2061 (4.2178)  class_acc: 0.2500 (0.2988)  loss_scale: 32768.0000 (42024.4972)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0693  max mem: 15572
Epoch: [24]  [ 540/2809]  eta: 0:21:42  lr: 0.000020  min_lr: 0.000000  loss: 4.2490 (4.2183)  class_acc: 0.2917 (0.2979)  loss_scale: 32768.0000 (41853.3974)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0831  max mem: 15572
Epoch: [24]  [ 550/2809]  eta: 0:21:38  lr: 0.000020  min_lr: 0.000000  loss: 4.2782 (4.2168)  class_acc: 0.2917 (0.2981)  loss_scale: 32768.0000 (41688.5082)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1373  max mem: 15572
Epoch: [24]  [ 560/2809]  eta: 0:21:30  lr: 0.000020  min_lr: 0.000000  loss: 4.1666 (4.2140)  class_acc: 0.2917 (0.2972)  loss_scale: 32768.0000 (41529.4973)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1241  max mem: 15572
Epoch: [24]  [ 570/2809]  eta: 0:21:24  lr: 0.000020  min_lr: 0.000000  loss: 4.1666 (4.2140)  class_acc: 0.2917 (0.2968)  loss_scale: 32768.0000 (41376.0560)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0993  max mem: 15572
[2025-01-16 01:55:29,178] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:55:29,178] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 580/2809]  eta: 0:21:19  lr: 0.000020  min_lr: 0.000000  loss: 4.1727 (4.2135)  class_acc: 0.2917 (0.2976)  loss_scale: 32768.0000 (41509.8933)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1095  max mem: 15572
[2025-01-16 01:55:33,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=68000, skipped=423, lr=[1.926765836973862e-07, 1.926765836973862e-07, 2.7525226242483745e-07, 2.7525226242483745e-07, 3.932175177497678e-07, 3.932175177497678e-07, 5.617393110710969e-07, 5.617393110710969e-07, 8.024847301015671e-07, 8.024847301015671e-07, 1.146406757287953e-06, 1.146406757287953e-06, 1.6377239389827901e-06, 1.6377239389827901e-06, 2.339605627118272e-06, 2.339605627118272e-06, 3.3422937530261024e-06, 3.3422937530261024e-06, 4.774705361465862e-06, 4.774705361465862e-06, 6.821007659236944e-06, 6.821007659236944e-06, 9.74429665605278e-06, 9.74429665605278e-06, 1.3920423794361114e-05, 1.3920423794361114e-05, 1.9886319706230164e-05, 1.9886319706230164e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:55:33,392] [INFO] [timer.py:260:stop] epoch=0/micro_step=68000/global_step=68000, RunningAvgSamplesPerSec=27.867715608018024, CurrSamplesPerSec=27.873929563728858, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [ 590/2809]  eta: 0:21:18  lr: 0.000020  min_lr: 0.000000  loss: 4.1813 (4.2143)  class_acc: 0.2917 (0.2976)  loss_scale: 65536.0000 (41916.4264)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.1878  max mem: 15572
[2025-01-16 01:55:39,677] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68007
[2025-01-16 01:55:39,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:55:39,678] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 600/2809]  eta: 0:21:08  lr: 0.000020  min_lr: 0.000000  loss: 4.2310 (4.2141)  class_acc: 0.3333 (0.2988)  loss_scale: 32768.0000 (41764.2063)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1349  max mem: 15572
Epoch: [24]  [ 610/2809]  eta: 0:21:02  lr: 0.000020  min_lr: 0.000000  loss: 4.2622 (4.2162)  class_acc: 0.3333 (0.2984)  loss_scale: 32768.0000 (41616.9689)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0565  max mem: 15572
Epoch: [24]  [ 620/2809]  eta: 0:20:56  lr: 0.000020  min_lr: 0.000000  loss: 4.2824 (4.2180)  class_acc: 0.2500 (0.2976)  loss_scale: 32768.0000 (41474.4734)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1053  max mem: 15572
Epoch: [24]  [ 630/2809]  eta: 0:20:48  lr: 0.000020  min_lr: 0.000000  loss: 4.2390 (4.2164)  class_acc: 0.3333 (0.2987)  loss_scale: 32768.0000 (41336.4945)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0793  max mem: 15572
Epoch: [24]  [ 640/2809]  eta: 0:20:45  lr: 0.000020  min_lr: 0.000000  loss: 4.1194 (4.2162)  class_acc: 0.3333 (0.2992)  loss_scale: 32768.0000 (41202.8206)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1246  max mem: 15572
Epoch: [24]  [ 650/2809]  eta: 0:20:40  lr: 0.000020  min_lr: 0.000000  loss: 4.1591 (4.2158)  class_acc: 0.2917 (0.2993)  loss_scale: 32768.0000 (41073.2535)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1565  max mem: 15572
Epoch: [24]  [ 660/2809]  eta: 0:20:34  lr: 0.000020  min_lr: 0.000000  loss: 4.1136 (4.2152)  class_acc: 0.2917 (0.2996)  loss_scale: 32768.0000 (40947.6067)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1268  max mem: 15572
Epoch: [24]  [ 670/2809]  eta: 0:20:26  lr: 0.000020  min_lr: 0.000000  loss: 4.1136 (4.2157)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (40825.7049)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.1018  max mem: 15572
Epoch: [24]  [ 680/2809]  eta: 0:20:17  lr: 0.000020  min_lr: 0.000000  loss: 4.1811 (4.2132)  class_acc: 0.2917 (0.3009)  loss_scale: 32768.0000 (40707.3833)  weight_decay: 0.0500 (0.0500)  time: 0.4869  data: 0.0539  max mem: 15572
Epoch: [24]  [ 690/2809]  eta: 0:20:10  lr: 0.000020  min_lr: 0.000000  loss: 4.1625 (4.2137)  class_acc: 0.3333 (0.3011)  loss_scale: 32768.0000 (40592.4863)  weight_decay: 0.0500 (0.0500)  time: 0.4918  data: 0.0604  max mem: 15572
Epoch: [24]  [ 700/2809]  eta: 0:20:04  lr: 0.000020  min_lr: 0.000000  loss: 4.1693 (4.2142)  class_acc: 0.3333 (0.3014)  loss_scale: 32768.0000 (40480.8673)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1151  max mem: 15572
Epoch: [24]  [ 710/2809]  eta: 0:19:59  lr: 0.000020  min_lr: 0.000000  loss: 4.1693 (4.2128)  class_acc: 0.3333 (0.3022)  loss_scale: 32768.0000 (40372.3882)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1408  max mem: 15572
[2025-01-16 01:56:50,196] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:56:50,196] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 720/2809]  eta: 0:19:52  lr: 0.000020  min_lr: 0.000000  loss: 4.1069 (4.2136)  class_acc: 0.2917 (0.3015)  loss_scale: 32768.0000 (40312.3662)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1041  max mem: 15572
[2025-01-16 01:56:51,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68138
[2025-01-16 01:56:51,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:56:51,171] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 730/2809]  eta: 0:19:48  lr: 0.000020  min_lr: 0.000000  loss: 4.2627 (4.2135)  class_acc: 0.2917 (0.3027)  loss_scale: 32768.0000 (40253.9863)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1147  max mem: 15572
Epoch: [24]  [ 740/2809]  eta: 0:19:41  lr: 0.000020  min_lr: 0.000000  loss: 4.2896 (4.2154)  class_acc: 0.3750 (0.3034)  loss_scale: 32768.0000 (40152.9609)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1161  max mem: 15572
Epoch: [24]  [ 750/2809]  eta: 0:19:34  lr: 0.000020  min_lr: 0.000000  loss: 4.3333 (4.2147)  class_acc: 0.3750 (0.3038)  loss_scale: 32768.0000 (40054.6258)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0759  max mem: 15572
Epoch: [24]  [ 760/2809]  eta: 0:19:28  lr: 0.000020  min_lr: 0.000000  loss: 4.3133 (4.2149)  class_acc: 0.2917 (0.3039)  loss_scale: 32768.0000 (39958.8752)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.1034  max mem: 15572
Epoch: [24]  [ 770/2809]  eta: 0:19:21  lr: 0.000020  min_lr: 0.000000  loss: 4.3862 (4.2170)  class_acc: 0.2500 (0.3032)  loss_scale: 32768.0000 (39865.6083)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1017  max mem: 15572
Epoch: [24]  [ 780/2809]  eta: 0:19:15  lr: 0.000020  min_lr: 0.000000  loss: 4.4436 (4.2197)  class_acc: 0.2500 (0.3029)  loss_scale: 32768.0000 (39774.7298)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0982  max mem: 15572
Epoch: [24]  [ 790/2809]  eta: 0:19:10  lr: 0.000020  min_lr: 0.000000  loss: 4.3211 (4.2204)  class_acc: 0.2917 (0.3035)  loss_scale: 32768.0000 (39686.1492)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1362  max mem: 15572
Epoch: [24]  [ 800/2809]  eta: 0:19:03  lr: 0.000020  min_lr: 0.000000  loss: 4.2238 (4.2207)  class_acc: 0.2917 (0.3039)  loss_scale: 32768.0000 (39599.7803)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1118  max mem: 15572
Epoch: [24]  [ 810/2809]  eta: 0:18:58  lr: 0.000020  min_lr: 0.000000  loss: 4.2238 (4.2209)  class_acc: 0.3333 (0.3037)  loss_scale: 32768.0000 (39515.5413)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1149  max mem: 15572
Epoch: [24]  [ 820/2809]  eta: 0:18:52  lr: 0.000020  min_lr: 0.000000  loss: 4.3215 (4.2235)  class_acc: 0.2917 (0.3030)  loss_scale: 32768.0000 (39433.3544)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1253  max mem: 15572
Epoch: [24]  [ 830/2809]  eta: 0:18:46  lr: 0.000020  min_lr: 0.000000  loss: 4.3215 (4.2247)  class_acc: 0.2083 (0.3025)  loss_scale: 32768.0000 (39353.1456)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1113  max mem: 15572
Epoch: [24]  [ 840/2809]  eta: 0:18:40  lr: 0.000020  min_lr: 0.000000  loss: 4.3031 (4.2266)  class_acc: 0.2500 (0.3024)  loss_scale: 32768.0000 (39274.8442)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1177  max mem: 15572
Epoch: [24]  [ 850/2809]  eta: 0:18:33  lr: 0.000020  min_lr: 0.000000  loss: 4.3031 (4.2270)  class_acc: 0.2917 (0.3021)  loss_scale: 32768.0000 (39198.3831)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0753  max mem: 15572
[2025-01-16 01:58:03,086] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:58:03,087] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 860/2809]  eta: 0:18:26  lr: 0.000020  min_lr: 0.000000  loss: 4.2916 (4.2277)  class_acc: 0.2500 (0.3019)  loss_scale: 32768.0000 (39504.2787)  weight_decay: 0.0500 (0.0500)  time: 0.5113  data: 0.0593  max mem: 15572
Epoch: [24]  [ 870/2809]  eta: 0:18:20  lr: 0.000020  min_lr: 0.000000  loss: 4.2326 (4.2269)  class_acc: 0.2917 (0.3024)  loss_scale: 65536.0000 (39803.1504)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0872  max mem: 15572
Epoch: [24]  [ 880/2809]  eta: 0:18:16  lr: 0.000020  min_lr: 0.000000  loss: 4.2713 (4.2271)  class_acc: 0.3333 (0.3024)  loss_scale: 65536.0000 (40095.2372)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1283  max mem: 15572
Epoch: [24]  [ 890/2809]  eta: 0:18:09  lr: 0.000020  min_lr: 0.000000  loss: 4.2573 (4.2271)  class_acc: 0.2500 (0.3021)  loss_scale: 65536.0000 (40380.7677)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1144  max mem: 15572
Epoch: [24]  [ 900/2809]  eta: 0:18:04  lr: 0.000020  min_lr: 0.000000  loss: 4.2489 (4.2274)  class_acc: 0.2500 (0.3018)  loss_scale: 65536.0000 (40659.9600)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1163  max mem: 15572
[2025-01-16 01:58:31,573] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68318
[2025-01-16 01:58:31,573] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:58:31,574] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 910/2809]  eta: 0:17:59  lr: 0.000020  min_lr: 0.000000  loss: 4.1997 (4.2264)  class_acc: 0.2083 (0.3014)  loss_scale: 65536.0000 (40609.2997)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1614  max mem: 15572
Epoch: [24]  [ 920/2809]  eta: 0:17:53  lr: 0.000020  min_lr: 0.000000  loss: 4.3039 (4.2273)  class_acc: 0.2500 (0.3013)  loss_scale: 32768.0000 (40524.1607)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1415  max mem: 15572
Epoch: [24]  [ 930/2809]  eta: 0:17:49  lr: 0.000020  min_lr: 0.000000  loss: 4.3524 (4.2291)  class_acc: 0.2917 (0.3009)  loss_scale: 32768.0000 (40440.8507)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1265  max mem: 15572
Epoch: [24]  [ 940/2809]  eta: 0:17:44  lr: 0.000020  min_lr: 0.000000  loss: 4.3599 (4.2302)  class_acc: 0.2917 (0.3007)  loss_scale: 32768.0000 (40359.3114)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.1595  max mem: 15572
Epoch: [24]  [ 950/2809]  eta: 0:17:38  lr: 0.000020  min_lr: 0.000000  loss: 4.4057 (4.2318)  class_acc: 0.2500 (0.2999)  loss_scale: 32768.0000 (40279.4869)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1363  max mem: 15572
Epoch: [24]  [ 960/2809]  eta: 0:17:32  lr: 0.000020  min_lr: 0.000000  loss: 4.2879 (4.2321)  class_acc: 0.1667 (0.2987)  loss_scale: 32768.0000 (40201.3236)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0836  max mem: 15572
Epoch: [24]  [ 970/2809]  eta: 0:17:24  lr: 0.000020  min_lr: 0.000000  loss: 4.2232 (4.2323)  class_acc: 0.2083 (0.2985)  loss_scale: 32768.0000 (40124.7703)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0578  max mem: 15572
Epoch: [24]  [ 980/2809]  eta: 0:17:18  lr: 0.000020  min_lr: 0.000000  loss: 4.2060 (4.2321)  class_acc: 0.2500 (0.2982)  loss_scale: 32768.0000 (40049.7778)  weight_decay: 0.0500 (0.0500)  time: 0.5113  data: 0.0583  max mem: 15572
Epoch: [24]  [ 990/2809]  eta: 0:17:13  lr: 0.000020  min_lr: 0.000000  loss: 4.2324 (4.2332)  class_acc: 0.2917 (0.2981)  loss_scale: 32768.0000 (39976.2987)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1165  max mem: 15572
Epoch: [24]  [1000/2809]  eta: 0:17:08  lr: 0.000020  min_lr: 0.000000  loss: 4.3319 (4.2336)  class_acc: 0.2917 (0.2981)  loss_scale: 32768.0000 (39904.2877)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1556  max mem: 15572
Epoch: [24]  [1010/2809]  eta: 0:17:02  lr: 0.000020  min_lr: 0.000000  loss: 4.3226 (4.2339)  class_acc: 0.2917 (0.2979)  loss_scale: 32768.0000 (39833.7013)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1274  max mem: 15572
Epoch: [24]  [1020/2809]  eta: 0:16:56  lr: 0.000020  min_lr: 0.000000  loss: 4.3314 (4.2356)  class_acc: 0.2500 (0.2972)  loss_scale: 32768.0000 (39764.4976)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0903  max mem: 15572
Epoch: [24]  [1030/2809]  eta: 0:16:50  lr: 0.000020  min_lr: 0.000000  loss: 4.2871 (4.2355)  class_acc: 0.1667 (0.2970)  loss_scale: 32768.0000 (39696.6363)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.1033  max mem: 15572
[2025-01-16 01:59:44,646] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:59:44,646] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 01:59:48,612] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68452
[2025-01-16 01:59:48,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:59:48,613] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [1040/2809]  eta: 0:16:45  lr: 0.000020  min_lr: 0.000000  loss: 4.2645 (4.2361)  class_acc: 0.2917 (0.2974)  loss_scale: 32768.0000 (39787.4659)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1458  max mem: 15572
Epoch: [24]  [1050/2809]  eta: 0:16:38  lr: 0.000020  min_lr: 0.000000  loss: 4.2645 (4.2369)  class_acc: 0.2500 (0.2964)  loss_scale: 32768.0000 (39720.6775)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1372  max mem: 15572
Epoch: [24]  [1060/2809]  eta: 0:16:31  lr: 0.000020  min_lr: 0.000000  loss: 4.2981 (4.2367)  class_acc: 0.2917 (0.2965)  loss_scale: 32768.0000 (39655.1480)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0526  max mem: 15572
Epoch: [24]  [1070/2809]  eta: 0:16:24  lr: 0.000020  min_lr: 0.000000  loss: 4.2506 (4.2356)  class_acc: 0.2917 (0.2971)  loss_scale: 32768.0000 (39590.8422)  weight_decay: 0.0500 (0.0500)  time: 0.4712  data: 0.0109  max mem: 15572
Epoch: [24]  [1080/2809]  eta: 0:16:18  lr: 0.000020  min_lr: 0.000000  loss: 4.0723 (4.2348)  class_acc: 0.3333 (0.2975)  loss_scale: 32768.0000 (39527.7262)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0623  max mem: 15572
Epoch: [24]  [1090/2809]  eta: 0:16:15  lr: 0.000020  min_lr: 0.000000  loss: 4.0899 (4.2336)  class_acc: 0.2917 (0.2981)  loss_scale: 32768.0000 (39465.7672)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1653  max mem: 15572
Epoch: [24]  [1100/2809]  eta: 0:16:11  lr: 0.000020  min_lr: 0.000000  loss: 4.0834 (4.2327)  class_acc: 0.2917 (0.2983)  loss_scale: 32768.0000 (39404.9337)  weight_decay: 0.0500 (0.0500)  time: 0.6930  data: 0.2402  max mem: 15572
Epoch: [24]  [1110/2809]  eta: 0:16:05  lr: 0.000019  min_lr: 0.000000  loss: 4.3513 (4.2342)  class_acc: 0.2917 (0.2982)  loss_scale: 32768.0000 (39345.1953)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.1964  max mem: 15572
Epoch: [24]  [1120/2809]  eta: 0:16:00  lr: 0.000019  min_lr: 0.000000  loss: 4.3229 (4.2338)  class_acc: 0.3333 (0.2988)  loss_scale: 32768.0000 (39286.5227)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1331  max mem: 15572
Epoch: [24]  [1130/2809]  eta: 0:15:53  lr: 0.000019  min_lr: 0.000000  loss: 4.1733 (4.2333)  class_acc: 0.3333 (0.2989)  loss_scale: 32768.0000 (39228.8877)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0957  max mem: 15572
Epoch: [24]  [1140/2809]  eta: 0:15:47  lr: 0.000019  min_lr: 0.000000  loss: 4.2885 (4.2336)  class_acc: 0.2500 (0.2986)  loss_scale: 32768.0000 (39172.2629)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.1021  max mem: 15572
Epoch: [24]  [1150/2809]  eta: 0:15:41  lr: 0.000019  min_lr: 0.000000  loss: 4.4161 (4.2354)  class_acc: 0.2083 (0.2979)  loss_scale: 32768.0000 (39116.6221)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0968  max mem: 15572
Epoch: [24]  [1160/2809]  eta: 0:15:35  lr: 0.000019  min_lr: 0.000000  loss: 4.3872 (4.2342)  class_acc: 0.2917 (0.2983)  loss_scale: 32768.0000 (39061.9397)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0869  max mem: 15572
[2025-01-16 02:00:59,847] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:00:59,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [1170/2809]  eta: 0:15:30  lr: 0.000019  min_lr: 0.000000  loss: 4.2041 (4.2340)  class_acc: 0.3333 (0.2986)  loss_scale: 32768.0000 (39176.0888)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1271  max mem: 15572
Epoch: [24]  [1180/2809]  eta: 0:15:24  lr: 0.000019  min_lr: 0.000000  loss: 4.2298 (4.2348)  class_acc: 0.2917 (0.2984)  loss_scale: 65536.0000 (39399.2887)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1290  max mem: 15572
[2025-01-16 02:01:11,586] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68601
[2025-01-16 02:01:11,587] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:01:11,587] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [1190/2809]  eta: 0:15:18  lr: 0.000019  min_lr: 0.000000  loss: 4.2000 (4.2340)  class_acc: 0.2917 (0.2989)  loss_scale: 65536.0000 (39453.6625)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1253  max mem: 15572
Epoch: [24]  [1200/2809]  eta: 0:15:13  lr: 0.000019  min_lr: 0.000000  loss: 4.1565 (4.2337)  class_acc: 0.3333 (0.2990)  loss_scale: 32768.0000 (39397.9950)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1305  max mem: 15572
Epoch: [24]  [1210/2809]  eta: 0:15:08  lr: 0.000019  min_lr: 0.000000  loss: 4.1831 (4.2340)  class_acc: 0.2917 (0.2991)  loss_scale: 32768.0000 (39343.2469)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1530  max mem: 15572
Epoch: [24]  [1220/2809]  eta: 0:15:01  lr: 0.000019  min_lr: 0.000000  loss: 4.1831 (4.2333)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (39289.3956)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1387  max mem: 15572
Epoch: [24]  [1230/2809]  eta: 0:14:56  lr: 0.000019  min_lr: 0.000000  loss: 4.2253 (4.2340)  class_acc: 0.2500 (0.2986)  loss_scale: 32768.0000 (39236.4192)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0931  max mem: 15572
Epoch: [24]  [1240/2809]  eta: 0:14:51  lr: 0.000019  min_lr: 0.000000  loss: 4.3468 (4.2342)  class_acc: 0.1667 (0.2980)  loss_scale: 32768.0000 (39184.2965)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1401  max mem: 15572
Epoch: [24]  [1250/2809]  eta: 0:14:45  lr: 0.000019  min_lr: 0.000000  loss: 4.3883 (4.2356)  class_acc: 0.2083 (0.2976)  loss_scale: 32768.0000 (39133.0072)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1320  max mem: 15572
Epoch: [24]  [1260/2809]  eta: 0:14:39  lr: 0.000019  min_lr: 0.000000  loss: 4.3177 (4.2355)  class_acc: 0.2917 (0.2979)  loss_scale: 32768.0000 (39082.5313)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0761  max mem: 15572
Epoch: [24]  [1270/2809]  eta: 0:14:33  lr: 0.000019  min_lr: 0.000000  loss: 4.1949 (4.2335)  class_acc: 0.3333 (0.2981)  loss_scale: 32768.0000 (39032.8497)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0973  max mem: 15572
Epoch: [24]  [1280/2809]  eta: 0:14:29  lr: 0.000019  min_lr: 0.000000  loss: 4.0919 (4.2329)  class_acc: 0.2500 (0.2976)  loss_scale: 32768.0000 (38983.9438)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1663  max mem: 15572
Epoch: [24]  [1290/2809]  eta: 0:14:22  lr: 0.000019  min_lr: 0.000000  loss: 4.1949 (4.2331)  class_acc: 0.2917 (0.2979)  loss_scale: 32768.0000 (38935.7955)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1371  max mem: 15572
Epoch: [24]  [1300/2809]  eta: 0:14:15  lr: 0.000019  min_lr: 0.000000  loss: 4.1877 (4.2331)  class_acc: 0.3333 (0.2979)  loss_scale: 32768.0000 (38888.3874)  weight_decay: 0.0500 (0.0500)  time: 0.4758  data: 0.0410  max mem: 15572
Epoch: [24]  [1310/2809]  eta: 0:14:09  lr: 0.000019  min_lr: 0.000000  loss: 4.3512 (4.2347)  class_acc: 0.2917 (0.2978)  loss_scale: 32768.0000 (38841.7025)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0392  max mem: 15572
[2025-01-16 02:02:24,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:02:24,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [1320/2809]  eta: 0:14:03  lr: 0.000019  min_lr: 0.000000  loss: 4.3517 (4.2339)  class_acc: 0.2917 (0.2981)  loss_scale: 32768.0000 (38969.3626)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0826  max mem: 15572
Epoch: [24]  [1330/2809]  eta: 0:13:58  lr: 0.000019  min_lr: 0.000000  loss: 4.1103 (4.2333)  class_acc: 0.2500 (0.2982)  loss_scale: 65536.0000 (39168.9617)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1305  max mem: 15572
[2025-01-16 02:02:35,121] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68747
[2025-01-16 02:02:35,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:02:35,122] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [1340/2809]  eta: 0:13:52  lr: 0.000019  min_lr: 0.000000  loss: 4.2390 (4.2338)  class_acc: 0.2917 (0.2985)  loss_scale: 32768.0000 (39121.2289)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1274  max mem: 15572
Epoch: [24]  [1350/2809]  eta: 0:13:48  lr: 0.000019  min_lr: 0.000000  loss: 4.2674 (4.2339)  class_acc: 0.3333 (0.2987)  loss_scale: 32768.0000 (39074.2028)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1800  max mem: 15572
[2025-01-16 02:02:49,255] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68772
[2025-01-16 02:02:49,255] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 02:02:49,255] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [1360/2809]  eta: 0:13:42  lr: 0.000019  min_lr: 0.000000  loss: 4.1605 (4.2335)  class_acc: 0.3333 (0.2989)  loss_scale: 32768.0000 (38967.6767)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1551  max mem: 15572
Epoch: [24]  [1370/2809]  eta: 0:13:36  lr: 0.000019  min_lr: 0.000000  loss: 4.3111 (4.2341)  class_acc: 0.2500 (0.2988)  loss_scale: 16384.0000 (38802.9526)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0868  max mem: 15572
Epoch: [24]  [1380/2809]  eta: 0:13:30  lr: 0.000019  min_lr: 0.000000  loss: 4.3111 (4.2332)  class_acc: 0.2500 (0.2985)  loss_scale: 16384.0000 (38640.6140)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1048  max mem: 15572
Epoch: [24]  [1390/2809]  eta: 0:13:25  lr: 0.000019  min_lr: 0.000000  loss: 4.2467 (4.2339)  class_acc: 0.2917 (0.2987)  loss_scale: 16384.0000 (38480.6096)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1377  max mem: 15572
Epoch: [24]  [1400/2809]  eta: 0:13:20  lr: 0.000019  min_lr: 0.000000  loss: 4.2986 (4.2345)  class_acc: 0.3333 (0.2988)  loss_scale: 16384.0000 (38322.8894)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1746  max mem: 15572
Epoch: [24]  [1410/2809]  eta: 0:13:14  lr: 0.000019  min_lr: 0.000000  loss: 4.2174 (4.2340)  class_acc: 0.2917 (0.2988)  loss_scale: 16384.0000 (38167.4047)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1342  max mem: 15572
Epoch: [24]  [1420/2809]  eta: 0:13:08  lr: 0.000019  min_lr: 0.000000  loss: 4.2066 (4.2343)  class_acc: 0.2500 (0.2985)  loss_scale: 16384.0000 (38014.1084)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0999  max mem: 15572
Epoch: [24]  [1430/2809]  eta: 0:13:02  lr: 0.000019  min_lr: 0.000000  loss: 4.2066 (4.2341)  class_acc: 0.2500 (0.2984)  loss_scale: 16384.0000 (37862.9546)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1169  max mem: 15572
Epoch: [24]  [1440/2809]  eta: 0:12:56  lr: 0.000019  min_lr: 0.000000  loss: 4.1857 (4.2339)  class_acc: 0.2917 (0.2985)  loss_scale: 16384.0000 (37713.8987)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0999  max mem: 15572
Epoch: [24]  [1450/2809]  eta: 0:12:51  lr: 0.000019  min_lr: 0.000000  loss: 4.0839 (4.2334)  class_acc: 0.3333 (0.2989)  loss_scale: 16384.0000 (37566.8973)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1205  max mem: 15572
Epoch: [24]  [1460/2809]  eta: 0:12:45  lr: 0.000019  min_lr: 0.000000  loss: 4.2944 (4.2338)  class_acc: 0.3333 (0.2990)  loss_scale: 16384.0000 (37421.9083)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1478  max mem: 15572
Epoch: [24]  [1470/2809]  eta: 0:12:40  lr: 0.000019  min_lr: 0.000000  loss: 4.3586 (4.2343)  class_acc: 0.2500 (0.2987)  loss_scale: 16384.0000 (37278.8906)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1308  max mem: 15572
Epoch: [24]  [1480/2809]  eta: 0:12:33  lr: 0.000019  min_lr: 0.000000  loss: 4.2274 (4.2339)  class_acc: 0.2500 (0.2983)  loss_scale: 16384.0000 (37137.8042)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0678  max mem: 15572
[2025-01-16 02:04:03,124] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:04:03,125] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [1490/2809]  eta: 0:12:28  lr: 0.000019  min_lr: 0.000000  loss: 4.1584 (4.2335)  class_acc: 0.2500 (0.2985)  loss_scale: 16384.0000 (37064.5419)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0960  max mem: 15572
Epoch: [24]  [1500/2809]  eta: 0:12:23  lr: 0.000019  min_lr: 0.000000  loss: 4.2301 (4.2337)  class_acc: 0.2500 (0.2982)  loss_scale: 32768.0000 (37035.9174)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1710  max mem: 15572
Epoch: [24]  [1510/2809]  eta: 0:12:17  lr: 0.000019  min_lr: 0.000000  loss: 4.3175 (4.2345)  class_acc: 0.2500 (0.2982)  loss_scale: 32768.0000 (37007.6717)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1098  max mem: 15572
Epoch: [24]  [1520/2809]  eta: 0:12:11  lr: 0.000019  min_lr: 0.000000  loss: 4.3462 (4.2351)  class_acc: 0.2917 (0.2990)  loss_scale: 32768.0000 (36979.7975)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0631  max mem: 15572
Epoch: [24]  [1530/2809]  eta: 0:12:05  lr: 0.000019  min_lr: 0.000000  loss: 4.2505 (4.2354)  class_acc: 0.3333 (0.2990)  loss_scale: 32768.0000 (36952.2874)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0847  max mem: 15572
Epoch: [24]  [1540/2809]  eta: 0:12:00  lr: 0.000019  min_lr: 0.000000  loss: 4.2265 (4.2358)  class_acc: 0.2917 (0.2992)  loss_scale: 32768.0000 (36925.1343)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1583  max mem: 15572
Epoch: [24]  [1550/2809]  eta: 0:11:54  lr: 0.000019  min_lr: 0.000000  loss: 4.3321 (4.2375)  class_acc: 0.2917 (0.2989)  loss_scale: 32768.0000 (36898.3314)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1409  max mem: 15572
Epoch: [24]  [1560/2809]  eta: 0:11:48  lr: 0.000019  min_lr: 0.000000  loss: 4.3959 (4.2374)  class_acc: 0.2500 (0.2985)  loss_scale: 32768.0000 (36871.8719)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1097  max mem: 15572
Epoch: [24]  [1570/2809]  eta: 0:11:43  lr: 0.000019  min_lr: 0.000000  loss: 4.1930 (4.2363)  class_acc: 0.2500 (0.2987)  loss_scale: 32768.0000 (36845.7492)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1230  max mem: 15572
Epoch: [24]  [1580/2809]  eta: 0:11:37  lr: 0.000019  min_lr: 0.000000  loss: 4.2381 (4.2366)  class_acc: 0.2917 (0.2985)  loss_scale: 32768.0000 (36819.9570)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0885  max mem: 15572
[2025-01-16 02:04:58,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=69000, skipped=430, lr=[1.8553140734347188e-07, 1.8553140734347188e-07, 2.650448676335313e-07, 2.650448676335313e-07, 3.7863552519075905e-07, 3.7863552519075905e-07, 5.409078931296558e-07, 5.409078931296558e-07, 7.72725561613794e-07, 7.72725561613794e-07, 1.1038936594482773e-06, 1.1038936594482773e-06, 1.5769909420689674e-06, 1.5769909420689674e-06, 2.252844202955668e-06, 2.252844202955668e-06, 3.21834886136524e-06, 3.21834886136524e-06, 4.597641230521772e-06, 4.597641230521772e-06, 6.568058900745388e-06, 6.568058900745388e-06, 9.382941286779127e-06, 9.382941286779127e-06, 1.3404201838255898e-05, 1.3404201838255898e-05, 1.9148859768936997e-05, 1.9148859768936997e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:04:58,239] [INFO] [timer.py:260:stop] epoch=0/micro_step=69000/global_step=69000, RunningAvgSamplesPerSec=27.873587258863015, CurrSamplesPerSec=31.37282826790633, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [1590/2809]  eta: 0:11:31  lr: 0.000019  min_lr: 0.000000  loss: 4.4005 (4.2384)  class_acc: 0.2083 (0.2979)  loss_scale: 32768.0000 (36794.4890)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0882  max mem: 15572
Epoch: [24]  [1600/2809]  eta: 0:11:25  lr: 0.000019  min_lr: 0.000000  loss: 4.2771 (4.2379)  class_acc: 0.2083 (0.2979)  loss_scale: 32768.0000 (36769.3392)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.1058  max mem: 15572
Epoch: [24]  [1610/2809]  eta: 0:11:19  lr: 0.000019  min_lr: 0.000000  loss: 4.0975 (4.2387)  class_acc: 0.2500 (0.2979)  loss_scale: 32768.0000 (36744.5016)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0838  max mem: 15572
[2025-01-16 02:05:14,601] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:05:14,601] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [1620/2809]  eta: 0:11:14  lr: 0.000019  min_lr: 0.000000  loss: 4.1396 (4.2382)  class_acc: 0.2500 (0.2976)  loss_scale: 32768.0000 (36881.6878)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1120  max mem: 15572
Epoch: [24]  [1630/2809]  eta: 0:11:08  lr: 0.000019  min_lr: 0.000000  loss: 4.1177 (4.2373)  class_acc: 0.3333 (0.2981)  loss_scale: 65536.0000 (37057.3734)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1365  max mem: 15572
Epoch: [24]  [1640/2809]  eta: 0:11:03  lr: 0.000019  min_lr: 0.000000  loss: 4.1443 (4.2374)  class_acc: 0.2917 (0.2978)  loss_scale: 65536.0000 (37230.9177)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1770  max mem: 15572
Epoch: [24]  [1650/2809]  eta: 0:10:57  lr: 0.000019  min_lr: 0.000000  loss: 4.2318 (4.2377)  class_acc: 0.2500 (0.2975)  loss_scale: 65536.0000 (37402.3598)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1619  max mem: 15572
[2025-01-16 02:05:39,456] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69069
[2025-01-16 02:05:39,457] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:05:39,457] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [1660/2809]  eta: 0:10:52  lr: 0.000019  min_lr: 0.000000  loss: 4.1024 (4.2370)  class_acc: 0.2917 (0.2976)  loss_scale: 65536.0000 (37413.9145)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1712  max mem: 15572
Epoch: [24]  [1670/2809]  eta: 0:10:47  lr: 0.000019  min_lr: 0.000000  loss: 4.1012 (4.2362)  class_acc: 0.3333 (0.2980)  loss_scale: 32768.0000 (37386.1113)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.2308  max mem: 15572
Epoch: [24]  [1680/2809]  eta: 0:10:41  lr: 0.000019  min_lr: 0.000000  loss: 4.2008 (4.2357)  class_acc: 0.3750 (0.2984)  loss_scale: 32768.0000 (37358.6389)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1238  max mem: 15572
Epoch: [24]  [1690/2809]  eta: 0:10:35  lr: 0.000019  min_lr: 0.000000  loss: 4.2407 (4.2362)  class_acc: 0.3333 (0.2982)  loss_scale: 32768.0000 (37331.4914)  weight_decay: 0.0500 (0.0500)  time: 0.4887  data: 0.0251  max mem: 15572
Epoch: [24]  [1700/2809]  eta: 0:10:29  lr: 0.000019  min_lr: 0.000000  loss: 4.3312 (4.2367)  class_acc: 0.3333 (0.2982)  loss_scale: 32768.0000 (37304.6631)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0713  max mem: 15572
Epoch: [24]  [1710/2809]  eta: 0:10:23  lr: 0.000019  min_lr: 0.000000  loss: 4.1478 (4.2352)  class_acc: 0.2917 (0.2982)  loss_scale: 32768.0000 (37278.1485)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1101  max mem: 15572
Epoch: [24]  [1720/2809]  eta: 0:10:17  lr: 0.000019  min_lr: 0.000000  loss: 4.0126 (4.2342)  class_acc: 0.3333 (0.2986)  loss_scale: 32768.0000 (37251.9419)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1082  max mem: 15572
Epoch: [24]  [1730/2809]  eta: 0:10:12  lr: 0.000019  min_lr: 0.000000  loss: 4.1064 (4.2342)  class_acc: 0.3750 (0.2989)  loss_scale: 32768.0000 (37226.0381)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1363  max mem: 15572
Epoch: [24]  [1740/2809]  eta: 0:10:06  lr: 0.000019  min_lr: 0.000000  loss: 4.1064 (4.2338)  class_acc: 0.2917 (0.2986)  loss_scale: 32768.0000 (37200.4319)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.1088  max mem: 15572
Epoch: [24]  [1750/2809]  eta: 0:10:00  lr: 0.000019  min_lr: 0.000000  loss: 4.2172 (4.2338)  class_acc: 0.2500 (0.2988)  loss_scale: 32768.0000 (37175.1182)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1064  max mem: 15572
Epoch: [24]  [1760/2809]  eta: 0:09:54  lr: 0.000019  min_lr: 0.000000  loss: 4.2603 (4.2337)  class_acc: 0.2500 (0.2985)  loss_scale: 32768.0000 (37150.0920)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0750  max mem: 15572
Epoch: [24]  [1770/2809]  eta: 0:09:49  lr: 0.000019  min_lr: 0.000000  loss: 4.2202 (4.2333)  class_acc: 0.2500 (0.2987)  loss_scale: 32768.0000 (37125.3484)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1087  max mem: 15572
Epoch: [24]  [1780/2809]  eta: 0:09:43  lr: 0.000019  min_lr: 0.000000  loss: 4.2733 (4.2336)  class_acc: 0.3333 (0.2990)  loss_scale: 32768.0000 (37100.8827)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1321  max mem: 15572
[2025-01-16 02:06:50,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:06:50,134] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 02:06:53,640] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69205
[2025-01-16 02:06:53,641] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:06:53,641] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [1790/2809]  eta: 0:09:37  lr: 0.000019  min_lr: 0.000000  loss: 4.2733 (4.2332)  class_acc: 0.3333 (0.2991)  loss_scale: 32768.0000 (37204.7616)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0830  max mem: 15572
Epoch: [24]  [1800/2809]  eta: 0:09:32  lr: 0.000019  min_lr: 0.000000  loss: 4.2488 (4.2336)  class_acc: 0.2917 (0.2993)  loss_scale: 32768.0000 (37180.1266)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1639  max mem: 15572
Epoch: [24]  [1810/2809]  eta: 0:09:26  lr: 0.000019  min_lr: 0.000000  loss: 4.2881 (4.2340)  class_acc: 0.2917 (0.2992)  loss_scale: 32768.0000 (37155.7637)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1148  max mem: 15572
Epoch: [24]  [1820/2809]  eta: 0:09:20  lr: 0.000019  min_lr: 0.000000  loss: 4.2382 (4.2336)  class_acc: 0.2917 (0.2993)  loss_scale: 32768.0000 (37131.6683)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0311  max mem: 15572
Epoch: [24]  [1830/2809]  eta: 0:09:14  lr: 0.000019  min_lr: 0.000000  loss: 4.2584 (4.2337)  class_acc: 0.2500 (0.2992)  loss_scale: 32768.0000 (37107.8362)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1203  max mem: 15572
Epoch: [24]  [1840/2809]  eta: 0:09:09  lr: 0.000019  min_lr: 0.000000  loss: 4.2171 (4.2333)  class_acc: 0.2500 (0.2992)  loss_scale: 32768.0000 (37084.2629)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.1601  max mem: 15572
Epoch: [24]  [1850/2809]  eta: 0:09:03  lr: 0.000019  min_lr: 0.000000  loss: 4.1676 (4.2335)  class_acc: 0.2917 (0.2994)  loss_scale: 32768.0000 (37060.9444)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0945  max mem: 15572
Epoch: [24]  [1860/2809]  eta: 0:08:57  lr: 0.000019  min_lr: 0.000000  loss: 4.3128 (4.2343)  class_acc: 0.2917 (0.2991)  loss_scale: 32768.0000 (37037.8764)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0865  max mem: 15572
Epoch: [24]  [1870/2809]  eta: 0:08:52  lr: 0.000019  min_lr: 0.000000  loss: 4.2395 (4.2341)  class_acc: 0.2083 (0.2990)  loss_scale: 32768.0000 (37015.0551)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1289  max mem: 15572
Epoch: [24]  [1880/2809]  eta: 0:08:46  lr: 0.000019  min_lr: 0.000000  loss: 4.2195 (4.2346)  class_acc: 0.2083 (0.2992)  loss_scale: 32768.0000 (36992.4763)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1141  max mem: 15572
Epoch: [24]  [1890/2809]  eta: 0:08:41  lr: 0.000019  min_lr: 0.000000  loss: 4.0881 (4.2340)  class_acc: 0.3333 (0.2994)  loss_scale: 32768.0000 (36970.1364)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1446  max mem: 15572
Epoch: [24]  [1900/2809]  eta: 0:08:35  lr: 0.000019  min_lr: 0.000000  loss: 4.0531 (4.2338)  class_acc: 0.3333 (0.2993)  loss_scale: 32768.0000 (36948.0316)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1433  max mem: 15572
Epoch: [24]  [1910/2809]  eta: 0:08:29  lr: 0.000019  min_lr: 0.000000  loss: 4.2276 (4.2341)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (36926.1580)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1204  max mem: 15572
[2025-01-16 02:08:06,967] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:08:06,967] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [1920/2809]  eta: 0:08:23  lr: 0.000019  min_lr: 0.000000  loss: 4.2998 (4.2339)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (36955.6856)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0743  max mem: 15572
[2025-01-16 02:08:08,952] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69338
[2025-01-16 02:08:08,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:08:08,952] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [1930/2809]  eta: 0:08:18  lr: 0.000019  min_lr: 0.000000  loss: 4.2416 (4.2336)  class_acc: 0.3333 (0.2995)  loss_scale: 32768.0000 (36950.9684)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0683  max mem: 15572
Epoch: [24]  [1940/2809]  eta: 0:08:12  lr: 0.000019  min_lr: 0.000000  loss: 4.2489 (4.2346)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (36929.4178)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1253  max mem: 15572
Epoch: [24]  [1950/2809]  eta: 0:08:07  lr: 0.000019  min_lr: 0.000000  loss: 4.3251 (4.2346)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (36908.0882)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1621  max mem: 15572
Epoch: [24]  [1960/2809]  eta: 0:08:01  lr: 0.000019  min_lr: 0.000000  loss: 4.2049 (4.2348)  class_acc: 0.2500 (0.2990)  loss_scale: 32768.0000 (36886.9760)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1300  max mem: 15572
Epoch: [24]  [1970/2809]  eta: 0:07:55  lr: 0.000019  min_lr: 0.000000  loss: 4.2632 (4.2348)  class_acc: 0.2083 (0.2991)  loss_scale: 32768.0000 (36866.0781)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0795  max mem: 15572
Epoch: [24]  [1980/2809]  eta: 0:07:49  lr: 0.000019  min_lr: 0.000000  loss: 4.2838 (4.2357)  class_acc: 0.2917 (0.2993)  loss_scale: 32768.0000 (36845.3912)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1207  max mem: 15572
Epoch: [24]  [1990/2809]  eta: 0:07:44  lr: 0.000019  min_lr: 0.000000  loss: 4.2612 (4.2353)  class_acc: 0.2917 (0.2993)  loss_scale: 32768.0000 (36824.9121)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1671  max mem: 15572
Epoch: [24]  [2000/2809]  eta: 0:07:38  lr: 0.000019  min_lr: 0.000000  loss: 4.1580 (4.2350)  class_acc: 0.2917 (0.2994)  loss_scale: 32768.0000 (36804.6377)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1489  max mem: 15572
Epoch: [24]  [2010/2809]  eta: 0:07:33  lr: 0.000019  min_lr: 0.000000  loss: 4.2408 (4.2351)  class_acc: 0.2917 (0.2991)  loss_scale: 32768.0000 (36784.5649)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1230  max mem: 15572
Epoch: [24]  [2020/2809]  eta: 0:07:27  lr: 0.000019  min_lr: 0.000000  loss: 4.2889 (4.2352)  class_acc: 0.2083 (0.2988)  loss_scale: 32768.0000 (36764.6907)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1420  max mem: 15572
Epoch: [24]  [2030/2809]  eta: 0:07:21  lr: 0.000019  min_lr: 0.000000  loss: 4.3095 (4.2351)  class_acc: 0.2917 (0.2987)  loss_scale: 32768.0000 (36745.0123)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1369  max mem: 15572
Epoch: [24]  [2040/2809]  eta: 0:07:16  lr: 0.000019  min_lr: 0.000000  loss: 4.3095 (4.2352)  class_acc: 0.2917 (0.2986)  loss_scale: 32768.0000 (36725.5267)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1690  max mem: 15572
Epoch: [24]  [2050/2809]  eta: 0:07:10  lr: 0.000019  min_lr: 0.000000  loss: 4.3049 (4.2357)  class_acc: 0.2917 (0.2987)  loss_scale: 32768.0000 (36706.2311)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1089  max mem: 15572
[2025-01-16 02:09:22,770] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:09:22,771] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [2060/2809]  eta: 0:07:04  lr: 0.000019  min_lr: 0.000000  loss: 4.3049 (4.2354)  class_acc: 0.2500 (0.2984)  loss_scale: 32768.0000 (36846.1135)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0735  max mem: 15572
[2025-01-16 02:09:32,887] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69484
[2025-01-16 02:09:32,887] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:09:32,888] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [2070/2809]  eta: 0:06:59  lr: 0.000019  min_lr: 0.000000  loss: 4.3750 (4.2362)  class_acc: 0.2500 (0.2984)  loss_scale: 65536.0000 (36937.1782)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1321  max mem: 15572
Epoch: [24]  [2080/2809]  eta: 0:06:53  lr: 0.000019  min_lr: 0.000000  loss: 4.3094 (4.2362)  class_acc: 0.2917 (0.2985)  loss_scale: 32768.0000 (36917.1437)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0830  max mem: 15572
Epoch: [24]  [2090/2809]  eta: 0:06:47  lr: 0.000019  min_lr: 0.000000  loss: 4.2276 (4.2361)  class_acc: 0.2917 (0.2985)  loss_scale: 32768.0000 (36897.3008)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.1011  max mem: 15572
Epoch: [24]  [2100/2809]  eta: 0:06:41  lr: 0.000019  min_lr: 0.000000  loss: 4.2308 (4.2362)  class_acc: 0.2917 (0.2985)  loss_scale: 32768.0000 (36877.6468)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1571  max mem: 15572
Epoch: [24]  [2110/2809]  eta: 0:06:36  lr: 0.000019  min_lr: 0.000000  loss: 4.2082 (4.2358)  class_acc: 0.2917 (0.2987)  loss_scale: 32768.0000 (36858.1791)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.1740  max mem: 15572
Epoch: [24]  [2120/2809]  eta: 0:06:31  lr: 0.000019  min_lr: 0.000000  loss: 4.2350 (4.2365)  class_acc: 0.2917 (0.2986)  loss_scale: 32768.0000 (36838.8949)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1620  max mem: 15572
Epoch: [24]  [2130/2809]  eta: 0:06:25  lr: 0.000019  min_lr: 0.000000  loss: 4.3865 (4.2365)  class_acc: 0.2500 (0.2983)  loss_scale: 32768.0000 (36819.7916)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1403  max mem: 15572
Epoch: [24]  [2140/2809]  eta: 0:06:19  lr: 0.000019  min_lr: 0.000000  loss: 4.2576 (4.2366)  class_acc: 0.2917 (0.2984)  loss_scale: 32768.0000 (36800.8669)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1284  max mem: 15572
Epoch: [24]  [2150/2809]  eta: 0:06:14  lr: 0.000019  min_lr: 0.000000  loss: 4.2358 (4.2366)  class_acc: 0.3333 (0.2987)  loss_scale: 32768.0000 (36782.1181)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1026  max mem: 15572
Epoch: [24]  [2160/2809]  eta: 0:06:08  lr: 0.000019  min_lr: 0.000000  loss: 4.1959 (4.2360)  class_acc: 0.3333 (0.2990)  loss_scale: 32768.0000 (36763.5428)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0819  max mem: 15572
Epoch: [24]  [2170/2809]  eta: 0:06:02  lr: 0.000019  min_lr: 0.000000  loss: 4.1779 (4.2359)  class_acc: 0.2917 (0.2990)  loss_scale: 32768.0000 (36745.1386)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0906  max mem: 15572
Epoch: [24]  [2180/2809]  eta: 0:05:57  lr: 0.000019  min_lr: 0.000000  loss: 4.1953 (4.2356)  class_acc: 0.2500 (0.2990)  loss_scale: 32768.0000 (36726.9033)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1270  max mem: 15572
Epoch: [24]  [2190/2809]  eta: 0:05:51  lr: 0.000019  min_lr: 0.000000  loss: 4.2069 (4.2352)  class_acc: 0.2917 (0.2991)  loss_scale: 32768.0000 (36708.8343)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0787  max mem: 15572
[2025-01-16 02:10:47,609] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:10:47,610] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [2200/2809]  eta: 0:05:45  lr: 0.000019  min_lr: 0.000000  loss: 4.2740 (4.2354)  class_acc: 0.2917 (0.2990)  loss_scale: 32768.0000 (36750.4807)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0907  max mem: 15572
Epoch: [24]  [2210/2809]  eta: 0:05:39  lr: 0.000019  min_lr: 0.000000  loss: 4.1796 (4.2350)  class_acc: 0.3333 (0.2994)  loss_scale: 65536.0000 (36880.6730)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0809  max mem: 15572
Epoch: [24]  [2220/2809]  eta: 0:05:34  lr: 0.000019  min_lr: 0.000000  loss: 4.1469 (4.2343)  class_acc: 0.4167 (0.2998)  loss_scale: 65536.0000 (37009.6929)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0564  max mem: 15572
[2025-01-16 02:11:03,175] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69642
[2025-01-16 02:11:03,176] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:11:03,176] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [2230/2809]  eta: 0:05:28  lr: 0.000019  min_lr: 0.000000  loss: 4.1501 (4.2339)  class_acc: 0.3750 (0.3000)  loss_scale: 65536.0000 (37064.1183)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1199  max mem: 15572
Epoch: [24]  [2240/2809]  eta: 0:05:22  lr: 0.000019  min_lr: 0.000000  loss: 4.2426 (4.2336)  class_acc: 0.2917 (0.2999)  loss_scale: 32768.0000 (37044.9478)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.1005  max mem: 15572
Epoch: [24]  [2250/2809]  eta: 0:05:17  lr: 0.000019  min_lr: 0.000000  loss: 4.3013 (4.2339)  class_acc: 0.2917 (0.2998)  loss_scale: 32768.0000 (37025.9476)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1228  max mem: 15572
Epoch: [24]  [2260/2809]  eta: 0:05:11  lr: 0.000019  min_lr: 0.000000  loss: 4.2451 (4.2337)  class_acc: 0.2500 (0.2998)  loss_scale: 32768.0000 (37007.1154)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.2122  max mem: 15572
Epoch: [24]  [2270/2809]  eta: 0:05:06  lr: 0.000019  min_lr: 0.000000  loss: 4.2451 (4.2343)  class_acc: 0.2500 (0.2996)  loss_scale: 32768.0000 (36988.4491)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1622  max mem: 15572
Epoch: [24]  [2280/2809]  eta: 0:05:00  lr: 0.000019  min_lr: 0.000000  loss: 4.2820 (4.2345)  class_acc: 0.2500 (0.2995)  loss_scale: 32768.0000 (36969.9465)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.1059  max mem: 15572
Epoch: [24]  [2290/2809]  eta: 0:04:54  lr: 0.000019  min_lr: 0.000000  loss: 4.2729 (4.2350)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (36951.6054)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1837  max mem: 15572
Epoch: [24]  [2300/2809]  eta: 0:04:49  lr: 0.000019  min_lr: 0.000000  loss: 4.2729 (4.2356)  class_acc: 0.2917 (0.2996)  loss_scale: 32768.0000 (36933.4237)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1575  max mem: 15572
Epoch: [24]  [2310/2809]  eta: 0:04:43  lr: 0.000019  min_lr: 0.000000  loss: 4.2647 (4.2356)  class_acc: 0.3333 (0.2996)  loss_scale: 32768.0000 (36915.3994)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.1089  max mem: 15572
Epoch: [24]  [2320/2809]  eta: 0:04:37  lr: 0.000019  min_lr: 0.000000  loss: 4.2863 (4.2359)  class_acc: 0.3333 (0.2995)  loss_scale: 32768.0000 (36897.5304)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1381  max mem: 15572
Epoch: [24]  [2330/2809]  eta: 0:04:31  lr: 0.000019  min_lr: 0.000000  loss: 4.2919 (4.2365)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (36879.8147)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0672  max mem: 15572
Epoch: [24]  [2340/2809]  eta: 0:04:26  lr: 0.000019  min_lr: 0.000000  loss: 4.2542 (4.2360)  class_acc: 0.2917 (0.2994)  loss_scale: 32768.0000 (36862.2503)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0355  max mem: 15572
Epoch: [24]  [2350/2809]  eta: 0:04:20  lr: 0.000019  min_lr: 0.000000  loss: 4.1301 (4.2360)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (36844.8354)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0930  max mem: 15572
[2025-01-16 02:12:15,455] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:12:15,455] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [2360/2809]  eta: 0:04:14  lr: 0.000019  min_lr: 0.000000  loss: 4.2882 (4.2363)  class_acc: 0.2500 (0.2992)  loss_scale: 32768.0000 (36910.8412)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0736  max mem: 15572
Epoch: [24]  [2370/2809]  eta: 0:04:09  lr: 0.000019  min_lr: 0.000000  loss: 4.2300 (4.2359)  class_acc: 0.3333 (0.2996)  loss_scale: 65536.0000 (37031.5715)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1221  max mem: 15572
Epoch: [24]  [2380/2809]  eta: 0:04:03  lr: 0.000019  min_lr: 0.000000  loss: 4.1232 (4.2356)  class_acc: 0.3333 (0.2995)  loss_scale: 65536.0000 (37151.2877)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1517  max mem: 15572
Epoch: [24]  [2390/2809]  eta: 0:03:57  lr: 0.000019  min_lr: 0.000000  loss: 4.2374 (4.2360)  class_acc: 0.2917 (0.2994)  loss_scale: 65536.0000 (37270.0025)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0705  max mem: 15572
Epoch: [24]  [2400/2809]  eta: 0:03:51  lr: 0.000019  min_lr: 0.000000  loss: 4.2010 (4.2354)  class_acc: 0.2917 (0.2994)  loss_scale: 65536.0000 (37387.7284)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0821  max mem: 15572
[2025-01-16 02:12:46,001] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69825
[2025-01-16 02:12:46,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:12:46,001] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [2410/2809]  eta: 0:03:46  lr: 0.000019  min_lr: 0.000000  loss: 4.2141 (4.2360)  class_acc: 0.2500 (0.2993)  loss_scale: 65536.0000 (37477.2957)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1059  max mem: 15572
Epoch: [24]  [2420/2809]  eta: 0:03:40  lr: 0.000019  min_lr: 0.000000  loss: 4.2922 (4.2362)  class_acc: 0.2083 (0.2992)  loss_scale: 32768.0000 (37457.8439)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1418  max mem: 15572
Epoch: [24]  [2430/2809]  eta: 0:03:35  lr: 0.000019  min_lr: 0.000000  loss: 4.3510 (4.2366)  class_acc: 0.2083 (0.2990)  loss_scale: 32768.0000 (37438.5520)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1911  max mem: 15572
Epoch: [24]  [2440/2809]  eta: 0:03:29  lr: 0.000019  min_lr: 0.000000  loss: 4.3603 (4.2364)  class_acc: 0.2917 (0.2993)  loss_scale: 32768.0000 (37419.4183)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0986  max mem: 15572
Epoch: [24]  [2450/2809]  eta: 0:03:23  lr: 0.000019  min_lr: 0.000000  loss: 4.3840 (4.2371)  class_acc: 0.2917 (0.2990)  loss_scale: 32768.0000 (37400.4406)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0560  max mem: 15572
Epoch: [24]  [2460/2809]  eta: 0:03:17  lr: 0.000019  min_lr: 0.000000  loss: 4.0990 (4.2360)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (37381.6172)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.1056  max mem: 15572
Epoch: [24]  [2470/2809]  eta: 0:03:12  lr: 0.000018  min_lr: 0.000000  loss: 4.0207 (4.2356)  class_acc: 0.3333 (0.2994)  loss_scale: 32768.0000 (37362.9462)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1180  max mem: 15572
Epoch: [24]  [2480/2809]  eta: 0:03:06  lr: 0.000018  min_lr: 0.000000  loss: 4.2966 (4.2359)  class_acc: 0.2917 (0.2994)  loss_scale: 32768.0000 (37344.4256)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1546  max mem: 15572
Epoch: [24]  [2490/2809]  eta: 0:03:00  lr: 0.000018  min_lr: 0.000000  loss: 4.2083 (4.2355)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (37326.0538)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1378  max mem: 15572
Epoch: [24]  [2500/2809]  eta: 0:02:55  lr: 0.000018  min_lr: 0.000000  loss: 4.1216 (4.2354)  class_acc: 0.2917 (0.2996)  loss_scale: 32768.0000 (37307.8289)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1057  max mem: 15572
Epoch: [24]  [2510/2809]  eta: 0:02:49  lr: 0.000018  min_lr: 0.000000  loss: 4.2426 (4.2358)  class_acc: 0.2500 (0.2995)  loss_scale: 32768.0000 (37289.7491)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1394  max mem: 15572
Epoch: [24]  [2520/2809]  eta: 0:02:43  lr: 0.000018  min_lr: 0.000000  loss: 4.2426 (4.2360)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (37271.8128)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1097  max mem: 15572
Epoch: [24]  [2530/2809]  eta: 0:02:38  lr: 0.000018  min_lr: 0.000000  loss: 4.2381 (4.2365)  class_acc: 0.2917 (0.2993)  loss_scale: 32768.0000 (37254.0182)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1001  max mem: 15572
[2025-01-16 02:14:00,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:14:00,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [2540/2809]  eta: 0:02:32  lr: 0.000018  min_lr: 0.000000  loss: 4.2924 (4.2368)  class_acc: 0.2917 (0.2994)  loss_scale: 32768.0000 (37275.0508)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0878  max mem: 15572
Epoch: [24]  [2550/2809]  eta: 0:02:26  lr: 0.000018  min_lr: 0.000000  loss: 4.2621 (4.2361)  class_acc: 0.2500 (0.2992)  loss_scale: 65536.0000 (37385.8346)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0994  max mem: 15572
Epoch: [24]  [2560/2809]  eta: 0:02:21  lr: 0.000018  min_lr: 0.000000  loss: 4.1877 (4.2365)  class_acc: 0.3750 (0.2996)  loss_scale: 65536.0000 (37495.7532)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1617  max mem: 15572
Epoch: [24]  [2570/2809]  eta: 0:02:15  lr: 0.000018  min_lr: 0.000000  loss: 4.1857 (4.2354)  class_acc: 0.4167 (0.2999)  loss_scale: 65536.0000 (37604.8168)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1276  max mem: 15572
[2025-01-16 02:14:21,254] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69991
[2025-01-16 02:14:21,254] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:14:21,254] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [2580/2809]  eta: 0:02:09  lr: 0.000018  min_lr: 0.000000  loss: 4.0833 (4.2351)  class_acc: 0.3333 (0.2999)  loss_scale: 65536.0000 (37636.8601)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0742  max mem: 15572
[2025-01-16 02:14:25,514] [INFO] [logging.py:96:log_dist] [Rank 0] step=70000, skipped=437, lr=[1.7842888740181126e-07, 1.7842888740181126e-07, 2.548984105740161e-07, 2.548984105740161e-07, 3.6414058653430874e-07, 3.6414058653430874e-07, 5.202008379061554e-07, 5.202008379061554e-07, 7.431440541516506e-07, 7.431440541516506e-07, 1.0616343630737865e-06, 1.0616343630737865e-06, 1.516620518676838e-06, 1.516620518676838e-06, 2.1666007409669117e-06, 2.1666007409669117e-06, 3.095143915667017e-06, 3.095143915667017e-06, 4.421634165238596e-06, 4.421634165238596e-06, 6.316620236055137e-06, 6.316620236055137e-06, 9.023743194364482e-06, 9.023743194364482e-06, 1.2891061706234976e-05, 1.2891061706234976e-05, 1.8415802437478537e-05, 1.8415802437478537e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:14:25,515] [INFO] [timer.py:260:stop] epoch=0/micro_step=70000/global_step=70000, RunningAvgSamplesPerSec=27.877634245069803, CurrSamplesPerSec=24.962937723651027, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [2590/2809]  eta: 0:02:04  lr: 0.000018  min_lr: 0.000000  loss: 4.0935 (4.2346)  class_acc: 0.3333 (0.3000)  loss_scale: 32768.0000 (37618.0687)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0379  max mem: 15572
Epoch: [24]  [2600/2809]  eta: 0:01:58  lr: 0.000018  min_lr: 0.000000  loss: 4.1684 (4.2343)  class_acc: 0.2500 (0.2999)  loss_scale: 32768.0000 (37599.4218)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0916  max mem: 15572
Epoch: [24]  [2610/2809]  eta: 0:01:52  lr: 0.000018  min_lr: 0.000000  loss: 4.1735 (4.2344)  class_acc: 0.2917 (0.3000)  loss_scale: 32768.0000 (37580.9177)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0971  max mem: 15572
Epoch: [24]  [2620/2809]  eta: 0:01:47  lr: 0.000018  min_lr: 0.000000  loss: 4.1190 (4.2339)  class_acc: 0.3333 (0.3001)  loss_scale: 32768.0000 (37562.5548)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0323  max mem: 15572
Epoch: [24]  [2630/2809]  eta: 0:01:41  lr: 0.000018  min_lr: 0.000000  loss: 4.2155 (4.2340)  class_acc: 0.2500 (0.2998)  loss_scale: 32768.0000 (37544.3314)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1199  max mem: 15572
Epoch: [24]  [2640/2809]  eta: 0:01:35  lr: 0.000018  min_lr: 0.000000  loss: 4.2870 (4.2340)  class_acc: 0.2500 (0.2998)  loss_scale: 32768.0000 (37526.2461)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1346  max mem: 15572
Epoch: [24]  [2650/2809]  eta: 0:01:30  lr: 0.000018  min_lr: 0.000000  loss: 4.1899 (4.2337)  class_acc: 0.2917 (0.2998)  loss_scale: 32768.0000 (37508.2972)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0638  max mem: 15572
Epoch: [24]  [2660/2809]  eta: 0:01:24  lr: 0.000018  min_lr: 0.000000  loss: 4.2577 (4.2344)  class_acc: 0.3333 (0.2997)  loss_scale: 32768.0000 (37490.4833)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1093  max mem: 15572
Epoch: [24]  [2670/2809]  eta: 0:01:18  lr: 0.000018  min_lr: 0.000000  loss: 4.2577 (4.2338)  class_acc: 0.3333 (0.2999)  loss_scale: 32768.0000 (37472.8027)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1056  max mem: 15572
Epoch: [24]  [2680/2809]  eta: 0:01:13  lr: 0.000018  min_lr: 0.000000  loss: 4.1779 (4.2340)  class_acc: 0.3333 (0.3000)  loss_scale: 32768.0000 (37455.2540)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0998  max mem: 15572
Epoch: [24]  [2690/2809]  eta: 0:01:07  lr: 0.000018  min_lr: 0.000000  loss: 4.1705 (4.2338)  class_acc: 0.3750 (0.3003)  loss_scale: 32768.0000 (37437.8357)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1318  max mem: 15572
Epoch: [24]  [2700/2809]  eta: 0:01:01  lr: 0.000018  min_lr: 0.000000  loss: 4.1278 (4.2339)  class_acc: 0.3333 (0.3001)  loss_scale: 32768.0000 (37420.5465)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1165  max mem: 15572
[2025-01-16 02:15:33,965] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:15:33,965] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [2710/2809]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000000  loss: 4.1689 (4.2336)  class_acc: 0.2917 (0.3004)  loss_scale: 32768.0000 (37487.9941)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0729  max mem: 15572
Epoch: [24]  [2720/2809]  eta: 0:00:50  lr: 0.000018  min_lr: 0.000000  loss: 4.1166 (4.2333)  class_acc: 0.3333 (0.3005)  loss_scale: 65536.0000 (37591.0739)  weight_decay: 0.0500 (0.0500)  time: 0.4267  data: 0.0238  max mem: 15572
[2025-01-16 02:15:43,441] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70142
[2025-01-16 02:15:43,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:15:43,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [2730/2809]  eta: 0:00:44  lr: 0.000018  min_lr: 0.000000  loss: 4.2593 (4.2334)  class_acc: 0.2500 (0.3003)  loss_scale: 65536.0000 (37633.4061)  weight_decay: 0.0500 (0.0500)  time: 0.4145  data: 0.0004  max mem: 15572
Epoch: [24]  [2740/2809]  eta: 0:00:39  lr: 0.000018  min_lr: 0.000000  loss: 4.3154 (4.2335)  class_acc: 0.2500 (0.3002)  loss_scale: 32768.0000 (37615.6556)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0582  max mem: 15572
Epoch: [24]  [2750/2809]  eta: 0:00:33  lr: 0.000018  min_lr: 0.000000  loss: 4.1616 (4.2336)  class_acc: 0.2500 (0.3002)  loss_scale: 32768.0000 (37598.0342)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1301  max mem: 15572
Epoch: [24]  [2760/2809]  eta: 0:00:27  lr: 0.000018  min_lr: 0.000000  loss: 4.1616 (4.2335)  class_acc: 0.3750 (0.3004)  loss_scale: 32768.0000 (37580.5404)  weight_decay: 0.0500 (0.0500)  time: 0.6771  data: 0.2082  max mem: 15572
Epoch: [24]  [2770/2809]  eta: 0:00:22  lr: 0.000018  min_lr: 0.000000  loss: 4.2520 (4.2336)  class_acc: 0.3750 (0.3004)  loss_scale: 32768.0000 (37563.1729)  weight_decay: 0.0500 (0.0500)  time: 0.6997  data: 0.2358  max mem: 15572
Epoch: [24]  [2780/2809]  eta: 0:00:16  lr: 0.000018  min_lr: 0.000000  loss: 4.3449 (4.2341)  class_acc: 0.2500 (0.3004)  loss_scale: 32768.0000 (37545.9302)  weight_decay: 0.0500 (0.0500)  time: 0.7073  data: 0.2456  max mem: 15572
Epoch: [24]  [2790/2809]  eta: 0:00:10  lr: 0.000018  min_lr: 0.000000  loss: 4.3213 (4.2341)  class_acc: 0.2500 (0.3002)  loss_scale: 32768.0000 (37528.8112)  weight_decay: 0.0500 (0.0500)  time: 0.7227  data: 0.2600  max mem: 15572
Epoch: [24]  [2800/2809]  eta: 0:00:05  lr: 0.000018  min_lr: 0.000000  loss: 4.2464 (4.2338)  class_acc: 0.2917 (0.3006)  loss_scale: 32768.0000 (37511.8144)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.1754  max mem: 15572
Epoch: [24]  [2808/2809]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 4.2432 (4.2340)  class_acc: 0.4167 (0.3007)  loss_scale: 32768.0000 (37498.3040)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0737  max mem: 15572
Epoch: [24] Total time: 0:26:36 (0.5684 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 4.2432 (4.2340)  class_acc: 0.4167 (0.3007)  loss_scale: 32768.0000 (37498.3040)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:23:59  loss: 1.0865 (1.0865)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.2913  data: 5.0279  max mem: 15572
Val:  [ 10/272]  eta: 0:03:27  loss: 2.7136 (2.6266)  acc1: 38.8889 (40.4040)  acc5: 72.2222 (71.7172)  time: 0.7902  data: 0.5906  max mem: 15572
Val:  [ 20/272]  eta: 0:02:27  loss: 2.7229 (2.6962)  acc1: 38.8889 (41.5344)  acc5: 72.2222 (70.8995)  time: 0.3483  data: 0.1530  max mem: 15572
Val:  [ 30/272]  eta: 0:02:05  loss: 2.7229 (2.7199)  acc1: 38.8889 (39.2473)  acc5: 72.2222 (71.5054)  time: 0.3664  data: 0.1638  max mem: 15572
Val:  [ 40/272]  eta: 0:01:54  loss: 2.6761 (2.7052)  acc1: 33.3333 (37.2629)  acc5: 77.7778 (72.4932)  time: 0.3993  data: 0.1972  max mem: 15572
Val:  [ 50/272]  eta: 0:01:44  loss: 2.5243 (2.6087)  acc1: 38.8889 (40.6318)  acc5: 77.7778 (75.0545)  time: 0.4000  data: 0.1953  max mem: 15572
Val:  [ 60/272]  eta: 0:01:32  loss: 1.9923 (2.5469)  acc1: 61.1111 (42.7140)  acc5: 83.3333 (75.7741)  time: 0.3183  data: 0.1268  max mem: 15572
Val:  [ 70/272]  eta: 0:01:20  loss: 2.0982 (2.4959)  acc1: 61.1111 (45.3834)  acc5: 83.3333 (76.5258)  time: 0.2091  data: 0.0447  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 2.2917 (2.5107)  acc1: 50.0000 (44.9246)  acc5: 77.7778 (76.0631)  time: 0.1662  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 2.4579 (2.5090)  acc1: 44.4444 (45.1770)  acc5: 77.7778 (76.4347)  time: 0.1779  data: 0.0006  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 2.5163 (2.5376)  acc1: 38.8889 (44.3344)  acc5: 77.7778 (76.2376)  time: 0.2409  data: 0.0563  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.6577 (2.5937)  acc1: 33.3333 (43.2432)  acc5: 72.2222 (74.6747)  time: 0.3213  data: 0.1165  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.0637 (2.6285)  acc1: 33.3333 (42.5161)  acc5: 61.1111 (73.8751)  time: 0.3326  data: 0.1223  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.5961 (2.6060)  acc1: 44.4444 (43.0025)  acc5: 77.7778 (74.6819)  time: 0.3466  data: 0.1457  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 2.3442 (2.6057)  acc1: 44.4444 (43.4988)  acc5: 83.3333 (74.5075)  time: 0.3072  data: 0.1096  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 2.5604 (2.6012)  acc1: 44.4444 (43.3775)  acc5: 77.7778 (74.7609)  time: 0.2813  data: 0.0882  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.5433 (2.5986)  acc1: 38.8889 (43.5473)  acc5: 77.7778 (75.0173)  time: 0.3275  data: 0.1368  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.6618 (2.6150)  acc1: 38.8889 (42.8850)  acc5: 77.7778 (74.4639)  time: 0.3245  data: 0.1339  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.5760 (2.5998)  acc1: 33.3333 (42.6949)  acc5: 72.2222 (74.8312)  time: 0.3022  data: 0.0926  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.5058 (2.6317)  acc1: 27.7778 (41.5940)  acc5: 72.2222 (73.7929)  time: 0.3115  data: 0.0863  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.8193 (2.6372)  acc1: 27.7778 (41.3212)  acc5: 72.2222 (73.5489)  time: 0.2965  data: 0.0820  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.5696 (2.6452)  acc1: 38.8889 (41.1532)  acc5: 77.7778 (73.4597)  time: 0.2656  data: 0.0756  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.6954 (2.6432)  acc1: 38.8889 (41.2519)  acc5: 72.2222 (73.4540)  time: 0.3205  data: 0.1400  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.3720 (2.6294)  acc1: 50.0000 (41.9913)  acc5: 77.7778 (73.8095)  time: 0.3356  data: 0.1505  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1859 (2.6162)  acc1: 61.1111 (42.3928)  acc5: 83.3333 (74.2508)  time: 0.3273  data: 0.1445  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.3821 (2.6225)  acc1: 33.3333 (41.8991)  acc5: 83.3333 (74.3471)  time: 0.3179  data: 0.1376  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.0642 (2.5831)  acc1: 66.6667 (43.4014)  acc5: 88.8889 (74.9894)  time: 0.2949  data: 0.1216  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.0629 (2.5823)  acc1: 61.1111 (43.3579)  acc5: 83.3333 (75.0513)  time: 0.2296  data: 0.0714  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.0629 (2.5853)  acc1: 55.5556 (43.3545)  acc5: 83.3333 (75.0358)  time: 0.2235  data: 0.0714  max mem: 15572
Val: Total time: 0:01:26 (0.3193 s / it)
* Acc@1 43.354 Acc@5 75.036 loss 2.585
Accuracy of the network on the 4883 val videos: 43.4%
[2025-01-16 02:18:01,975] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 02:18:01,978] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 02:18:01,978] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 02:18:03,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 02:18:03,975] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 43.35%
Epoch: [25]  [   0/2809]  eta: 5:52:16  lr: 0.000018  min_lr: 0.000000  loss: 3.9862 (3.9862)  class_acc: 0.3750 (0.3750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.5247  data: 7.0343  max mem: 15572
Epoch: [25]  [  10/2809]  eta: 0:55:41  lr: 0.000018  min_lr: 0.000000  loss: 4.2396 (4.2027)  class_acc: 0.2083 (0.2652)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1937  data: 0.7550  max mem: 15572
Epoch: [25]  [  20/2809]  eta: 0:42:08  lr: 0.000018  min_lr: 0.000000  loss: 4.2251 (4.2129)  class_acc: 0.2500 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1338  max mem: 15572
Epoch: [25]  [  30/2809]  eta: 0:35:01  lr: 0.000018  min_lr: 0.000000  loss: 4.2173 (4.2185)  class_acc: 0.2917 (0.2984)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5158  data: 0.0706  max mem: 15572
Epoch: [25]  [  40/2809]  eta: 0:32:44  lr: 0.000018  min_lr: 0.000000  loss: 4.2518 (4.2450)  class_acc: 0.3333 (0.3089)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0628  max mem: 15572
[2025-01-16 02:18:36,078] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:18:36,078] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [  50/2809]  eta: 0:31:34  lr: 0.000018  min_lr: 0.000000  loss: 4.2499 (4.2275)  class_acc: 0.3333 (0.3064)  loss_scale: 32768.0000 (35980.5490)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1390  max mem: 15572
[2025-01-16 02:18:41,703] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70281
[2025-01-16 02:18:41,703] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:18:41,704] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [  60/2809]  eta: 0:30:58  lr: 0.000018  min_lr: 0.000000  loss: 4.1912 (4.2278)  class_acc: 0.2917 (0.3094)  loss_scale: 32768.0000 (38139.8033)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1631  max mem: 15572
Epoch: [25]  [  70/2809]  eta: 0:29:31  lr: 0.000018  min_lr: 0.000000  loss: 4.2265 (4.2410)  class_acc: 0.2917 (0.3022)  loss_scale: 32768.0000 (37383.2113)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0956  max mem: 15572
Epoch: [25]  [  80/2809]  eta: 0:28:43  lr: 0.000018  min_lr: 0.000000  loss: 4.2257 (4.2463)  class_acc: 0.2917 (0.3061)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.0500 (0.0500)  time: 0.4961  data: 0.0581  max mem: 15572
Epoch: [25]  [  90/2809]  eta: 0:28:26  lr: 0.000018  min_lr: 0.000000  loss: 4.2976 (4.2539)  class_acc: 0.2917 (0.3109)  loss_scale: 32768.0000 (36368.8791)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1191  max mem: 15572
Epoch: [25]  [ 100/2809]  eta: 0:28:00  lr: 0.000018  min_lr: 0.000000  loss: 4.2976 (4.2444)  class_acc: 0.3333 (0.3135)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1271  max mem: 15572
Epoch: [25]  [ 110/2809]  eta: 0:27:56  lr: 0.000018  min_lr: 0.000000  loss: 4.1482 (4.2425)  class_acc: 0.3750 (0.3157)  loss_scale: 32768.0000 (35720.0721)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1510  max mem: 15572
Epoch: [25]  [ 120/2809]  eta: 0:27:40  lr: 0.000018  min_lr: 0.000000  loss: 4.2539 (4.2433)  class_acc: 0.3333 (0.3168)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1578  max mem: 15572
Epoch: [25]  [ 130/2809]  eta: 0:27:07  lr: 0.000018  min_lr: 0.000000  loss: 4.3604 (4.2515)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (35269.3740)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0720  max mem: 15572
Epoch: [25]  [ 140/2809]  eta: 0:27:00  lr: 0.000018  min_lr: 0.000000  loss: 4.3914 (4.2574)  class_acc: 0.2917 (0.3147)  loss_scale: 32768.0000 (35091.9716)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0959  max mem: 15572
Epoch: [25]  [ 150/2809]  eta: 0:26:51  lr: 0.000018  min_lr: 0.000000  loss: 4.2287 (4.2499)  class_acc: 0.2917 (0.3148)  loss_scale: 32768.0000 (34938.0662)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1546  max mem: 15572
Epoch: [25]  [ 160/2809]  eta: 0:26:32  lr: 0.000018  min_lr: 0.000000  loss: 4.2301 (4.2605)  class_acc: 0.2917 (0.3121)  loss_scale: 32768.0000 (34803.2795)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1051  max mem: 15572
Epoch: [25]  [ 170/2809]  eta: 0:26:28  lr: 0.000018  min_lr: 0.000000  loss: 4.4167 (4.2706)  class_acc: 0.2500 (0.3131)  loss_scale: 32768.0000 (34684.2573)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1297  max mem: 15572
Epoch: [25]  [ 180/2809]  eta: 0:26:23  lr: 0.000018  min_lr: 0.000000  loss: 4.4420 (4.2697)  class_acc: 0.2917 (0.3140)  loss_scale: 32768.0000 (34578.3867)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1581  max mem: 15572
[2025-01-16 02:19:56,584] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:19:56,585] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 190/2809]  eta: 0:26:13  lr: 0.000018  min_lr: 0.000000  loss: 4.2270 (4.2645)  class_acc: 0.2917 (0.3146)  loss_scale: 32768.0000 (35512.9634)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1325  max mem: 15572
Epoch: [25]  [ 200/2809]  eta: 0:26:04  lr: 0.000018  min_lr: 0.000000  loss: 4.1498 (4.2587)  class_acc: 0.3333 (0.3147)  loss_scale: 65536.0000 (37006.6468)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1390  max mem: 15572
Epoch: [25]  [ 210/2809]  eta: 0:25:59  lr: 0.000018  min_lr: 0.000000  loss: 4.2813 (4.2639)  class_acc: 0.2917 (0.3118)  loss_scale: 65536.0000 (38358.7488)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1509  max mem: 15572
Epoch: [25]  [ 220/2809]  eta: 0:25:55  lr: 0.000018  min_lr: 0.000000  loss: 4.4347 (4.2714)  class_acc: 0.2500 (0.3113)  loss_scale: 65536.0000 (39588.4887)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1663  max mem: 15572
Epoch: [25]  [ 230/2809]  eta: 0:25:42  lr: 0.000018  min_lr: 0.000000  loss: 4.2853 (4.2695)  class_acc: 0.2917 (0.3104)  loss_scale: 65536.0000 (40711.7576)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1268  max mem: 15572
[2025-01-16 02:20:27,861] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70465
[2025-01-16 02:20:27,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:20:27,862] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 240/2809]  eta: 0:25:32  lr: 0.000018  min_lr: 0.000000  loss: 4.2042 (4.2666)  class_acc: 0.2917 (0.3103)  loss_scale: 65536.0000 (41605.8423)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0869  max mem: 15572
Epoch: [25]  [ 250/2809]  eta: 0:25:17  lr: 0.000018  min_lr: 0.000000  loss: 4.1325 (4.2598)  class_acc: 0.2500 (0.3099)  loss_scale: 32768.0000 (41253.7371)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0884  max mem: 15572
Epoch: [25]  [ 260/2809]  eta: 0:25:12  lr: 0.000018  min_lr: 0.000000  loss: 4.2843 (4.2632)  class_acc: 0.1667 (0.3067)  loss_scale: 32768.0000 (40928.6130)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1152  max mem: 15572
Epoch: [25]  [ 270/2809]  eta: 0:25:02  lr: 0.000018  min_lr: 0.000000  loss: 4.3068 (4.2607)  class_acc: 0.2917 (0.3106)  loss_scale: 32768.0000 (40627.4834)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1323  max mem: 15572
Epoch: [25]  [ 280/2809]  eta: 0:24:53  lr: 0.000018  min_lr: 0.000000  loss: 4.2702 (4.2617)  class_acc: 0.3750 (0.3103)  loss_scale: 32768.0000 (40347.7865)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1091  max mem: 15572
Epoch: [25]  [ 290/2809]  eta: 0:24:45  lr: 0.000018  min_lr: 0.000000  loss: 4.2702 (4.2607)  class_acc: 0.3333 (0.3093)  loss_scale: 32768.0000 (40087.3127)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0904  max mem: 15572
Epoch: [25]  [ 300/2809]  eta: 0:24:35  lr: 0.000018  min_lr: 0.000000  loss: 4.2575 (4.2595)  class_acc: 0.2917 (0.3088)  loss_scale: 32768.0000 (39844.1462)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.0806  max mem: 15572
Epoch: [25]  [ 310/2809]  eta: 0:24:25  lr: 0.000018  min_lr: 0.000000  loss: 4.2808 (4.2616)  class_acc: 0.2500 (0.3076)  loss_scale: 32768.0000 (39616.6174)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0991  max mem: 15572
Epoch: [25]  [ 320/2809]  eta: 0:24:23  lr: 0.000018  min_lr: 0.000000  loss: 4.2256 (4.2547)  class_acc: 0.2917 (0.3083)  loss_scale: 32768.0000 (39403.2648)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1561  max mem: 15572
Epoch: [25]  [ 330/2809]  eta: 0:24:18  lr: 0.000018  min_lr: 0.000000  loss: 4.0583 (4.2542)  class_acc: 0.3333 (0.3090)  loss_scale: 32768.0000 (39202.8036)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1734  max mem: 15572
Epoch: [25]  [ 340/2809]  eta: 0:24:01  lr: 0.000018  min_lr: 0.000000  loss: 4.2243 (4.2503)  class_acc: 0.3333 (0.3095)  loss_scale: 32768.0000 (39014.0997)  weight_decay: 0.0500 (0.0500)  time: 0.5165  data: 0.0800  max mem: 15572
Epoch: [25]  [ 350/2809]  eta: 0:23:53  lr: 0.000018  min_lr: 0.000000  loss: 4.2294 (4.2517)  class_acc: 0.2500 (0.3084)  loss_scale: 32768.0000 (38836.1481)  weight_decay: 0.0500 (0.0500)  time: 0.4879  data: 0.0562  max mem: 15572
Epoch: [25]  [ 360/2809]  eta: 0:23:51  lr: 0.000018  min_lr: 0.000000  loss: 4.2094 (4.2493)  class_acc: 0.2500 (0.3066)  loss_scale: 32768.0000 (38668.0554)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1370  max mem: 15572
[2025-01-16 02:21:40,088] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:21:40,089] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 370/2809]  eta: 0:23:42  lr: 0.000018  min_lr: 0.000000  loss: 4.2042 (4.2489)  class_acc: 0.2500 (0.3054)  loss_scale: 32768.0000 (38685.6712)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1253  max mem: 15572
Epoch: [25]  [ 380/2809]  eta: 0:23:36  lr: 0.000018  min_lr: 0.000000  loss: 4.3034 (4.2506)  class_acc: 0.2500 (0.3038)  loss_scale: 65536.0000 (39390.4042)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1087  max mem: 15572
Epoch: [25]  [ 390/2809]  eta: 0:23:31  lr: 0.000018  min_lr: 0.000000  loss: 4.1712 (4.2492)  class_acc: 0.2500 (0.3039)  loss_scale: 65536.0000 (40059.0895)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1510  max mem: 15572
Epoch: [25]  [ 400/2809]  eta: 0:23:22  lr: 0.000018  min_lr: 0.000000  loss: 4.1780 (4.2489)  class_acc: 0.3333 (0.3041)  loss_scale: 65536.0000 (40694.4239)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1244  max mem: 15572
Epoch: [25]  [ 410/2809]  eta: 0:23:15  lr: 0.000018  min_lr: 0.000000  loss: 4.2996 (4.2494)  class_acc: 0.2500 (0.3039)  loss_scale: 65536.0000 (41298.8418)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1087  max mem: 15572
Epoch: [25]  [ 420/2809]  eta: 0:23:09  lr: 0.000018  min_lr: 0.000000  loss: 4.2712 (4.2471)  class_acc: 0.2500 (0.3045)  loss_scale: 65536.0000 (41874.5463)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1424  max mem: 15572
Epoch: [25]  [ 430/2809]  eta: 0:23:03  lr: 0.000018  min_lr: 0.000000  loss: 4.2232 (4.2466)  class_acc: 0.3333 (0.3047)  loss_scale: 65536.0000 (42423.5360)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1352  max mem: 15572
Epoch: [25]  [ 440/2809]  eta: 0:22:57  lr: 0.000018  min_lr: 0.000000  loss: 4.3660 (4.2508)  class_acc: 0.3333 (0.3054)  loss_scale: 65536.0000 (42947.6281)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1260  max mem: 15572
Epoch: [25]  [ 450/2809]  eta: 0:22:46  lr: 0.000018  min_lr: 0.000000  loss: 4.3710 (4.2527)  class_acc: 0.3333 (0.3055)  loss_scale: 65536.0000 (43448.4789)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0892  max mem: 15572
Epoch: [25]  [ 460/2809]  eta: 0:22:39  lr: 0.000018  min_lr: 0.000000  loss: 4.2505 (4.2519)  class_acc: 0.2500 (0.3043)  loss_scale: 65536.0000 (43927.6009)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0742  max mem: 15572
Epoch: [25]  [ 470/2809]  eta: 0:22:31  lr: 0.000018  min_lr: 0.000000  loss: 4.2273 (4.2498)  class_acc: 0.2083 (0.3036)  loss_scale: 65536.0000 (44386.3779)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1004  max mem: 15572
Epoch: [25]  [ 480/2809]  eta: 0:22:26  lr: 0.000018  min_lr: 0.000000  loss: 4.2555 (4.2521)  class_acc: 0.2500 (0.3033)  loss_scale: 65536.0000 (44826.0790)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1102  max mem: 15572
Epoch: [25]  [ 490/2809]  eta: 0:22:21  lr: 0.000018  min_lr: 0.000000  loss: 4.3157 (4.2525)  class_acc: 0.2500 (0.3024)  loss_scale: 65536.0000 (45247.8697)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1427  max mem: 15572
[2025-01-16 02:22:52,252] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:22:52,253] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:22:52,659] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70723
[2025-01-16 02:22:52,659] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:22:52,659] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 500/2809]  eta: 0:22:13  lr: 0.000018  min_lr: 0.000000  loss: 4.5093 (4.2559)  class_acc: 0.2917 (0.3027)  loss_scale: 65536.0000 (45783.6327)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1214  max mem: 15572
Epoch: [25]  [ 510/2809]  eta: 0:22:07  lr: 0.000018  min_lr: 0.000000  loss: 4.4653 (4.2567)  class_acc: 0.2917 (0.3024)  loss_scale: 65536.0000 (46170.1761)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1115  max mem: 15572
Epoch: [25]  [ 520/2809]  eta: 0:22:03  lr: 0.000018  min_lr: 0.000000  loss: 4.1971 (4.2519)  class_acc: 0.3333 (0.3033)  loss_scale: 65536.0000 (46541.8810)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1257  max mem: 15572
Epoch: [25]  [ 530/2809]  eta: 0:21:54  lr: 0.000018  min_lr: 0.000000  loss: 4.1398 (4.2473)  class_acc: 0.3333 (0.3048)  loss_scale: 65536.0000 (46899.5857)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0894  max mem: 15572
Epoch: [25]  [ 540/2809]  eta: 0:21:49  lr: 0.000018  min_lr: 0.000000  loss: 4.2628 (4.2458)  class_acc: 0.2917 (0.3039)  loss_scale: 65536.0000 (47244.0665)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.1105  max mem: 15572
Epoch: [25]  [ 550/2809]  eta: 0:21:40  lr: 0.000018  min_lr: 0.000000  loss: 4.2744 (4.2440)  class_acc: 0.2500 (0.3035)  loss_scale: 65536.0000 (47576.0436)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1095  max mem: 15572
Epoch: [25]  [ 560/2809]  eta: 0:21:34  lr: 0.000018  min_lr: 0.000000  loss: 4.2744 (4.2464)  class_acc: 0.2500 (0.3033)  loss_scale: 65536.0000 (47896.1854)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.1046  max mem: 15572
Epoch: [25]  [ 570/2809]  eta: 0:21:30  lr: 0.000018  min_lr: 0.000000  loss: 4.1636 (4.2409)  class_acc: 0.3333 (0.3049)  loss_scale: 65536.0000 (48205.1138)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1632  max mem: 15572
Epoch: [25]  [ 580/2809]  eta: 0:21:26  lr: 0.000018  min_lr: 0.000000  loss: 4.1197 (4.2408)  class_acc: 0.3750 (0.3049)  loss_scale: 65536.0000 (48503.4079)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1704  max mem: 15572
Epoch: [25]  [ 590/2809]  eta: 0:21:17  lr: 0.000018  min_lr: 0.000000  loss: 4.2158 (4.2408)  class_acc: 0.2500 (0.3039)  loss_scale: 65536.0000 (48791.6074)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1009  max mem: 15572
Epoch: [25]  [ 600/2809]  eta: 0:21:12  lr: 0.000018  min_lr: 0.000000  loss: 4.2919 (4.2408)  class_acc: 0.2500 (0.3033)  loss_scale: 65536.0000 (49070.2163)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.1065  max mem: 15572
Epoch: [25]  [ 610/2809]  eta: 0:21:05  lr: 0.000018  min_lr: 0.000000  loss: 4.2667 (4.2413)  class_acc: 0.2917 (0.3026)  loss_scale: 65536.0000 (49339.7054)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1443  max mem: 15572
Epoch: [25]  [ 620/2809]  eta: 0:20:59  lr: 0.000018  min_lr: 0.000000  loss: 4.2667 (4.2423)  class_acc: 0.2500 (0.3020)  loss_scale: 65536.0000 (49600.5153)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1256  max mem: 15572
[2025-01-16 02:24:05,963] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70850
[2025-01-16 02:24:05,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:24:05,963] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 630/2809]  eta: 0:20:57  lr: 0.000018  min_lr: 0.000000  loss: 4.2492 (4.2414)  class_acc: 0.2500 (0.3014)  loss_scale: 65536.0000 (49541.4770)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1756  max mem: 15572
Epoch: [25]  [ 640/2809]  eta: 0:20:49  lr: 0.000018  min_lr: 0.000000  loss: 4.2466 (4.2441)  class_acc: 0.2083 (0.3000)  loss_scale: 32768.0000 (49279.8003)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1450  max mem: 15572
Epoch: [25]  [ 650/2809]  eta: 0:20:41  lr: 0.000018  min_lr: 0.000000  loss: 4.2231 (4.2436)  class_acc: 0.2500 (0.3008)  loss_scale: 32768.0000 (49026.1628)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0617  max mem: 15572
Epoch: [25]  [ 660/2809]  eta: 0:20:36  lr: 0.000018  min_lr: 0.000000  loss: 4.1518 (4.2441)  class_acc: 0.3333 (0.3008)  loss_scale: 32768.0000 (48780.1997)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1186  max mem: 15572
Epoch: [25]  [ 670/2809]  eta: 0:20:32  lr: 0.000018  min_lr: 0.000000  loss: 4.3197 (4.2443)  class_acc: 0.2917 (0.3011)  loss_scale: 32768.0000 (48541.5678)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1752  max mem: 15572
Epoch: [25]  [ 680/2809]  eta: 0:20:27  lr: 0.000018  min_lr: 0.000000  loss: 4.2987 (4.2443)  class_acc: 0.2917 (0.3011)  loss_scale: 32768.0000 (48309.9442)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1560  max mem: 15572
Epoch: [25]  [ 690/2809]  eta: 0:20:21  lr: 0.000018  min_lr: 0.000000  loss: 4.2987 (4.2455)  class_acc: 0.2500 (0.3008)  loss_scale: 32768.0000 (48085.0246)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1402  max mem: 15572
Epoch: [25]  [ 700/2809]  eta: 0:20:16  lr: 0.000018  min_lr: 0.000000  loss: 4.2089 (4.2421)  class_acc: 0.2917 (0.3017)  loss_scale: 32768.0000 (47866.5221)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1455  max mem: 15572
Epoch: [25]  [ 710/2809]  eta: 0:20:10  lr: 0.000018  min_lr: 0.000000  loss: 3.9647 (4.2414)  class_acc: 0.3333 (0.3026)  loss_scale: 32768.0000 (47654.1660)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1386  max mem: 15572
Epoch: [25]  [ 720/2809]  eta: 0:20:06  lr: 0.000018  min_lr: 0.000000  loss: 4.3145 (4.2438)  class_acc: 0.2917 (0.3018)  loss_scale: 32768.0000 (47447.7004)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1449  max mem: 15572
Epoch: [25]  [ 730/2809]  eta: 0:19:59  lr: 0.000018  min_lr: 0.000000  loss: 4.3863 (4.2467)  class_acc: 0.2500 (0.3012)  loss_scale: 32768.0000 (47246.8837)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1412  max mem: 15572
Epoch: [25]  [ 740/2809]  eta: 0:19:54  lr: 0.000018  min_lr: 0.000000  loss: 4.4366 (4.2492)  class_acc: 0.2500 (0.3003)  loss_scale: 32768.0000 (47051.4872)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1359  max mem: 15572
Epoch: [25]  [ 750/2809]  eta: 0:19:49  lr: 0.000018  min_lr: 0.000000  loss: 4.3597 (4.2496)  class_acc: 0.2917 (0.3004)  loss_scale: 32768.0000 (46861.2943)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1511  max mem: 15572
[2025-01-16 02:25:19,803] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:25:19,803] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 02:25:21,323] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70980
[2025-01-16 02:25:21,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:25:21,323] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 760/2809]  eta: 0:19:42  lr: 0.000018  min_lr: 0.000000  loss: 4.2059 (4.2485)  class_acc: 0.2917 (0.3007)  loss_scale: 32768.0000 (46719.1590)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1272  max mem: 15572
Epoch: [25]  [ 770/2809]  eta: 0:19:36  lr: 0.000018  min_lr: 0.000000  loss: 4.2132 (4.2497)  class_acc: 0.3333 (0.3010)  loss_scale: 32768.0000 (46538.2101)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1323  max mem: 15572
[2025-01-16 02:25:31,107] [INFO] [logging.py:96:log_dist] [Rank 0] step=71000, skipped=443, lr=[1.7137627550321582e-07, 1.7137627550321582e-07, 2.4482325071887975e-07, 2.4482325071887975e-07, 3.497475010269711e-07, 3.497475010269711e-07, 4.996392871813873e-07, 4.996392871813873e-07, 7.137704102591248e-07, 7.137704102591248e-07, 1.0196720146558926e-06, 1.0196720146558926e-06, 1.4566743066512753e-06, 1.4566743066512753e-06, 2.0809632952161078e-06, 2.0809632952161078e-06, 2.9728047074515825e-06, 2.9728047074515825e-06, 4.2468638677879755e-06, 4.2468638677879755e-06, 6.06694838255425e-06, 6.06694838255425e-06, 8.667069117934644e-06, 8.667069117934644e-06, 1.2381527311335207e-05, 1.2381527311335207e-05, 1.7687896159050297e-05, 1.7687896159050297e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:25:31,108] [INFO] [timer.py:260:stop] epoch=0/micro_step=71000/global_step=71000, RunningAvgSamplesPerSec=27.883840609010853, CurrSamplesPerSec=29.975426077230797, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [25]  [ 780/2809]  eta: 0:19:29  lr: 0.000018  min_lr: 0.000000  loss: 4.2409 (4.2502)  class_acc: 0.3333 (0.3008)  loss_scale: 32768.0000 (46361.8950)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1282  max mem: 15572
Epoch: [25]  [ 790/2809]  eta: 0:19:24  lr: 0.000018  min_lr: 0.000000  loss: 4.2404 (4.2497)  class_acc: 0.2917 (0.3006)  loss_scale: 32768.0000 (46190.0379)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1091  max mem: 15572
Epoch: [25]  [ 800/2809]  eta: 0:19:17  lr: 0.000018  min_lr: 0.000000  loss: 4.2404 (4.2503)  class_acc: 0.2500 (0.3003)  loss_scale: 32768.0000 (46022.4719)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1002  max mem: 15572
Epoch: [25]  [ 810/2809]  eta: 0:19:10  lr: 0.000018  min_lr: 0.000000  loss: 4.2480 (4.2500)  class_acc: 0.2500 (0.2999)  loss_scale: 32768.0000 (45859.0382)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0977  max mem: 15572
Epoch: [25]  [ 820/2809]  eta: 0:19:08  lr: 0.000018  min_lr: 0.000000  loss: 4.2989 (4.2514)  class_acc: 0.2500 (0.3001)  loss_scale: 32768.0000 (45699.5859)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1993  max mem: 15572
Epoch: [25]  [ 830/2809]  eta: 0:19:05  lr: 0.000018  min_lr: 0.000000  loss: 4.4238 (4.2550)  class_acc: 0.2917 (0.2993)  loss_scale: 32768.0000 (45543.9711)  weight_decay: 0.0500 (0.0500)  time: 0.7124  data: 0.2839  max mem: 15572
Epoch: [25]  [ 840/2809]  eta: 0:18:56  lr: 0.000018  min_lr: 0.000000  loss: 4.3371 (4.2546)  class_acc: 0.2917 (0.2999)  loss_scale: 32768.0000 (45392.0571)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1399  max mem: 15572
Epoch: [25]  [ 850/2809]  eta: 0:18:49  lr: 0.000018  min_lr: 0.000000  loss: 4.1962 (4.2532)  class_acc: 0.2917 (0.2996)  loss_scale: 32768.0000 (45243.7133)  weight_decay: 0.0500 (0.0500)  time: 0.4797  data: 0.0421  max mem: 15572
Epoch: [25]  [ 860/2809]  eta: 0:18:44  lr: 0.000018  min_lr: 0.000000  loss: 4.2296 (4.2528)  class_acc: 0.3333 (0.3007)  loss_scale: 32768.0000 (45098.8153)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1214  max mem: 15572
Epoch: [25]  [ 870/2809]  eta: 0:18:38  lr: 0.000018  min_lr: 0.000000  loss: 4.2296 (4.2532)  class_acc: 0.2917 (0.3003)  loss_scale: 32768.0000 (44957.2445)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1382  max mem: 15572
Epoch: [25]  [ 880/2809]  eta: 0:18:30  lr: 0.000018  min_lr: 0.000000  loss: 4.2411 (4.2531)  class_acc: 0.2500 (0.3000)  loss_scale: 32768.0000 (44818.8876)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0714  max mem: 15572
[2025-01-16 02:26:33,284] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:26:33,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 890/2809]  eta: 0:18:25  lr: 0.000018  min_lr: 0.000000  loss: 4.2047 (4.2523)  class_acc: 0.2500 (0.2999)  loss_scale: 32768.0000 (44941.0730)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.1009  max mem: 15572
Epoch: [25]  [ 900/2809]  eta: 0:18:18  lr: 0.000018  min_lr: 0.000000  loss: 4.1551 (4.2527)  class_acc: 0.2500 (0.2993)  loss_scale: 65536.0000 (45169.6515)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1017  max mem: 15572
Epoch: [25]  [ 910/2809]  eta: 0:18:12  lr: 0.000018  min_lr: 0.000000  loss: 4.3026 (4.2526)  class_acc: 0.2500 (0.2995)  loss_scale: 65536.0000 (45393.2119)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0803  max mem: 15572
[2025-01-16 02:26:49,654] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71138
[2025-01-16 02:26:49,654] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:26:49,654] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 920/2809]  eta: 0:18:07  lr: 0.000018  min_lr: 0.000000  loss: 4.3026 (4.2528)  class_acc: 0.2917 (0.2990)  loss_scale: 65536.0000 (45327.2877)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1545  max mem: 15572
Epoch: [25]  [ 930/2809]  eta: 0:18:04  lr: 0.000018  min_lr: 0.000000  loss: 4.2487 (4.2530)  class_acc: 0.2917 (0.2996)  loss_scale: 32768.0000 (45192.3867)  weight_decay: 0.0500 (0.0500)  time: 0.6550  data: 0.2038  max mem: 15572
Epoch: [25]  [ 940/2809]  eta: 0:17:56  lr: 0.000018  min_lr: 0.000000  loss: 4.2487 (4.2524)  class_acc: 0.3333 (0.2999)  loss_scale: 32768.0000 (45060.3528)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1317  max mem: 15572
Epoch: [25]  [ 950/2809]  eta: 0:17:50  lr: 0.000018  min_lr: 0.000000  loss: 4.2006 (4.2517)  class_acc: 0.2917 (0.2998)  loss_scale: 32768.0000 (44931.0957)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0824  max mem: 15572
Epoch: [25]  [ 960/2809]  eta: 0:17:45  lr: 0.000018  min_lr: 0.000000  loss: 4.2407 (4.2518)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (44804.5286)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1413  max mem: 15572
Epoch: [25]  [ 970/2809]  eta: 0:17:38  lr: 0.000018  min_lr: 0.000000  loss: 4.2842 (4.2525)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (44680.5685)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1273  max mem: 15572
Epoch: [25]  [ 980/2809]  eta: 0:17:33  lr: 0.000018  min_lr: 0.000000  loss: 4.2749 (4.2519)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (44559.1356)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1374  max mem: 15572
Epoch: [25]  [ 990/2809]  eta: 0:17:27  lr: 0.000018  min_lr: 0.000000  loss: 4.1811 (4.2505)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (44440.1534)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1240  max mem: 15572
Epoch: [25]  [1000/2809]  eta: 0:17:20  lr: 0.000018  min_lr: 0.000000  loss: 4.2000 (4.2516)  class_acc: 0.2500 (0.2997)  loss_scale: 32768.0000 (44323.5485)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0864  max mem: 15572
Epoch: [25]  [1010/2809]  eta: 0:17:16  lr: 0.000018  min_lr: 0.000000  loss: 4.3857 (4.2519)  class_acc: 0.2500 (0.2994)  loss_scale: 32768.0000 (44209.2502)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1529  max mem: 15572
Epoch: [25]  [1020/2809]  eta: 0:17:10  lr: 0.000018  min_lr: 0.000000  loss: 4.2884 (4.2518)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (44097.1910)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1518  max mem: 15572
Epoch: [25]  [1030/2809]  eta: 0:17:04  lr: 0.000018  min_lr: 0.000000  loss: 4.1921 (4.2503)  class_acc: 0.2500 (0.2997)  loss_scale: 32768.0000 (43987.3055)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1086  max mem: 15572
Epoch: [25]  [1040/2809]  eta: 0:16:58  lr: 0.000017  min_lr: 0.000000  loss: 4.1852 (4.2501)  class_acc: 0.2500 (0.2994)  loss_scale: 32768.0000 (43879.5312)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1113  max mem: 15572
[2025-01-16 02:28:04,527] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:28:04,528] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [1050/2809]  eta: 0:16:52  lr: 0.000017  min_lr: 0.000000  loss: 4.2373 (4.2512)  class_acc: 0.2500 (0.2988)  loss_scale: 32768.0000 (44054.4091)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1115  max mem: 15572
[2025-01-16 02:28:12,331] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71280
[2025-01-16 02:28:12,331] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:28:12,332] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [1060/2809]  eta: 0:16:45  lr: 0.000017  min_lr: 0.000000  loss: 4.2190 (4.2500)  class_acc: 0.2500 (0.2990)  loss_scale: 65536.0000 (44071.5702)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1133  max mem: 15572
Epoch: [25]  [1070/2809]  eta: 0:16:40  lr: 0.000017  min_lr: 0.000000  loss: 4.1997 (4.2507)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (43966.0280)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1191  max mem: 15572
Epoch: [25]  [1080/2809]  eta: 0:16:34  lr: 0.000017  min_lr: 0.000000  loss: 4.2511 (4.2507)  class_acc: 0.2917 (0.2989)  loss_scale: 32768.0000 (43862.4385)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1122  max mem: 15572
Epoch: [25]  [1090/2809]  eta: 0:16:27  lr: 0.000017  min_lr: 0.000000  loss: 4.2269 (4.2511)  class_acc: 0.2083 (0.2982)  loss_scale: 32768.0000 (43760.7479)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0873  max mem: 15572
Epoch: [25]  [1100/2809]  eta: 0:16:22  lr: 0.000017  min_lr: 0.000000  loss: 4.1609 (4.2496)  class_acc: 0.2917 (0.2990)  loss_scale: 32768.0000 (43660.9046)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1211  max mem: 15572
Epoch: [25]  [1110/2809]  eta: 0:16:16  lr: 0.000017  min_lr: 0.000000  loss: 3.9929 (4.2479)  class_acc: 0.3333 (0.2993)  loss_scale: 32768.0000 (43562.8587)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1421  max mem: 15572
Epoch: [25]  [1120/2809]  eta: 0:16:11  lr: 0.000017  min_lr: 0.000000  loss: 4.1051 (4.2480)  class_acc: 0.3333 (0.2994)  loss_scale: 32768.0000 (43466.5620)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1573  max mem: 15572
Epoch: [25]  [1130/2809]  eta: 0:16:06  lr: 0.000017  min_lr: 0.000000  loss: 4.1719 (4.2462)  class_acc: 0.3333 (0.2999)  loss_scale: 32768.0000 (43371.9682)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1842  max mem: 15572
Epoch: [25]  [1140/2809]  eta: 0:16:00  lr: 0.000017  min_lr: 0.000000  loss: 4.1868 (4.2472)  class_acc: 0.2917 (0.3001)  loss_scale: 32768.0000 (43279.0324)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1587  max mem: 15572
Epoch: [25]  [1150/2809]  eta: 0:15:54  lr: 0.000017  min_lr: 0.000000  loss: 4.2703 (4.2456)  class_acc: 0.2917 (0.2998)  loss_scale: 32768.0000 (43187.7116)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1071  max mem: 15572
Epoch: [25]  [1160/2809]  eta: 0:15:48  lr: 0.000017  min_lr: 0.000000  loss: 4.2642 (4.2468)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (43097.9638)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0963  max mem: 15572
Epoch: [25]  [1170/2809]  eta: 0:15:41  lr: 0.000017  min_lr: 0.000000  loss: 4.4072 (4.2468)  class_acc: 0.2500 (0.2997)  loss_scale: 32768.0000 (43009.7489)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0827  max mem: 15572
Epoch: [25]  [1180/2809]  eta: 0:15:36  lr: 0.000017  min_lr: 0.000000  loss: 4.3685 (4.2468)  class_acc: 0.2500 (0.2995)  loss_scale: 32768.0000 (42923.0279)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1345  max mem: 15572
[2025-01-16 02:29:25,507] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:29:25,507] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [1190/2809]  eta: 0:15:30  lr: 0.000017  min_lr: 0.000000  loss: 4.3049 (4.2473)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (43030.3543)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1549  max mem: 15572
[2025-01-16 02:29:30,092] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71417
[2025-01-16 02:29:30,093] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:29:30,093] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [1200/2809]  eta: 0:15:24  lr: 0.000017  min_lr: 0.000000  loss: 4.2059 (4.2452)  class_acc: 0.2500 (0.2998)  loss_scale: 32768.0000 (42972.1898)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0841  max mem: 15572
Epoch: [25]  [1210/2809]  eta: 0:15:18  lr: 0.000017  min_lr: 0.000000  loss: 4.2059 (4.2451)  class_acc: 0.2500 (0.2998)  loss_scale: 32768.0000 (42887.9273)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1053  max mem: 15572
Epoch: [25]  [1220/2809]  eta: 0:15:13  lr: 0.000017  min_lr: 0.000000  loss: 4.2335 (4.2449)  class_acc: 0.2500 (0.2995)  loss_scale: 32768.0000 (42805.0450)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1367  max mem: 15572
Epoch: [25]  [1230/2809]  eta: 0:15:06  lr: 0.000017  min_lr: 0.000000  loss: 4.3281 (4.2453)  class_acc: 0.2500 (0.2995)  loss_scale: 32768.0000 (42723.5093)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1245  max mem: 15572
Epoch: [25]  [1240/2809]  eta: 0:15:01  lr: 0.000017  min_lr: 0.000000  loss: 4.1762 (4.2442)  class_acc: 0.2500 (0.2998)  loss_scale: 32768.0000 (42643.2877)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1542  max mem: 15572
Epoch: [25]  [1250/2809]  eta: 0:14:56  lr: 0.000017  min_lr: 0.000000  loss: 4.0512 (4.2424)  class_acc: 0.3333 (0.3001)  loss_scale: 32768.0000 (42564.3485)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1910  max mem: 15572
Epoch: [25]  [1260/2809]  eta: 0:14:50  lr: 0.000017  min_lr: 0.000000  loss: 4.1269 (4.2434)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (42486.6614)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1466  max mem: 15572
Epoch: [25]  [1270/2809]  eta: 0:14:44  lr: 0.000017  min_lr: 0.000000  loss: 4.2298 (4.2433)  class_acc: 0.2083 (0.2994)  loss_scale: 32768.0000 (42410.1967)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1250  max mem: 15572
Epoch: [25]  [1280/2809]  eta: 0:14:38  lr: 0.000017  min_lr: 0.000000  loss: 4.2896 (4.2449)  class_acc: 0.2917 (0.2996)  loss_scale: 32768.0000 (42334.9258)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1348  max mem: 15572
Epoch: [25]  [1290/2809]  eta: 0:14:32  lr: 0.000017  min_lr: 0.000000  loss: 4.2011 (4.2432)  class_acc: 0.2917 (0.2994)  loss_scale: 32768.0000 (42260.8211)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1221  max mem: 15572
Epoch: [25]  [1300/2809]  eta: 0:14:26  lr: 0.000017  min_lr: 0.000000  loss: 4.1211 (4.2432)  class_acc: 0.2083 (0.2991)  loss_scale: 32768.0000 (42187.8555)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.1147  max mem: 15572
Epoch: [25]  [1310/2809]  eta: 0:14:21  lr: 0.000017  min_lr: 0.000000  loss: 4.2477 (4.2434)  class_acc: 0.2500 (0.2992)  loss_scale: 32768.0000 (42116.0031)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1561  max mem: 15572
Epoch: [25]  [1320/2809]  eta: 0:14:15  lr: 0.000017  min_lr: 0.000000  loss: 4.2253 (4.2437)  class_acc: 0.2917 (0.2993)  loss_scale: 32768.0000 (42045.2385)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1391  max mem: 15572
[2025-01-16 02:30:44,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:30:44,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [1330/2809]  eta: 0:14:10  lr: 0.000017  min_lr: 0.000000  loss: 4.2579 (4.2431)  class_acc: 0.3333 (0.2999)  loss_scale: 32768.0000 (42221.7280)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1078  max mem: 15572
Epoch: [25]  [1340/2809]  eta: 0:14:03  lr: 0.000017  min_lr: 0.000000  loss: 4.2205 (4.2427)  class_acc: 0.3333 (0.2997)  loss_scale: 65536.0000 (42395.5854)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0821  max mem: 15572
Epoch: [25]  [1350/2809]  eta: 0:13:58  lr: 0.000017  min_lr: 0.000000  loss: 4.2205 (4.2434)  class_acc: 0.2917 (0.2999)  loss_scale: 65536.0000 (42566.8690)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1116  max mem: 15572
Epoch: [25]  [1360/2809]  eta: 0:13:52  lr: 0.000017  min_lr: 0.000000  loss: 4.1800 (4.2429)  class_acc: 0.2917 (0.2999)  loss_scale: 65536.0000 (42735.6356)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1503  max mem: 15572
Epoch: [25]  [1370/2809]  eta: 0:13:46  lr: 0.000017  min_lr: 0.000000  loss: 4.2290 (4.2444)  class_acc: 0.2500 (0.2995)  loss_scale: 65536.0000 (42901.9402)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0871  max mem: 15572
Epoch: [25]  [1380/2809]  eta: 0:13:40  lr: 0.000017  min_lr: 0.000000  loss: 4.3058 (4.2441)  class_acc: 0.2917 (0.2999)  loss_scale: 65536.0000 (43065.8364)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0913  max mem: 15572
Epoch: [25]  [1390/2809]  eta: 0:13:35  lr: 0.000017  min_lr: 0.000000  loss: 4.1100 (4.2433)  class_acc: 0.2917 (0.2998)  loss_scale: 65536.0000 (43227.3760)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1123  max mem: 15572
[2025-01-16 02:31:27,700] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71622
[2025-01-16 02:31:27,701] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:31:27,701] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [1400/2809]  eta: 0:13:29  lr: 0.000017  min_lr: 0.000000  loss: 4.2036 (4.2430)  class_acc: 0.2917 (0.2999)  loss_scale: 65536.0000 (43293.0535)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1098  max mem: 15572
Epoch: [25]  [1410/2809]  eta: 0:13:23  lr: 0.000017  min_lr: 0.000000  loss: 4.1331 (4.2429)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (43218.4607)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1139  max mem: 15572
Epoch: [25]  [1420/2809]  eta: 0:13:17  lr: 0.000017  min_lr: 0.000000  loss: 4.1283 (4.2416)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (43144.9177)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1183  max mem: 15572
Epoch: [25]  [1430/2809]  eta: 0:13:11  lr: 0.000017  min_lr: 0.000000  loss: 4.2720 (4.2416)  class_acc: 0.3333 (0.2997)  loss_scale: 32768.0000 (43072.4025)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1259  max mem: 15572
Epoch: [25]  [1440/2809]  eta: 0:13:06  lr: 0.000017  min_lr: 0.000000  loss: 4.1811 (4.2416)  class_acc: 0.3750 (0.3001)  loss_scale: 32768.0000 (43000.8938)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1565  max mem: 15572
Epoch: [25]  [1450/2809]  eta: 0:12:59  lr: 0.000017  min_lr: 0.000000  loss: 4.2798 (4.2416)  class_acc: 0.2500 (0.2995)  loss_scale: 32768.0000 (42930.3708)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.1084  max mem: 15572
Epoch: [25]  [1460/2809]  eta: 0:12:54  lr: 0.000017  min_lr: 0.000000  loss: 4.2913 (4.2410)  class_acc: 0.2083 (0.2993)  loss_scale: 32768.0000 (42860.8131)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0908  max mem: 15572
Epoch: [25]  [1470/2809]  eta: 0:12:47  lr: 0.000017  min_lr: 0.000000  loss: 4.1731 (4.2410)  class_acc: 0.2500 (0.2988)  loss_scale: 32768.0000 (42792.2012)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0850  max mem: 15572
Epoch: [25]  [1480/2809]  eta: 0:12:41  lr: 0.000017  min_lr: 0.000000  loss: 4.2398 (4.2410)  class_acc: 0.1667 (0.2985)  loss_scale: 32768.0000 (42724.5159)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0403  max mem: 15572
Epoch: [25]  [1490/2809]  eta: 0:12:35  lr: 0.000017  min_lr: 0.000000  loss: 4.2685 (4.2406)  class_acc: 0.3333 (0.2989)  loss_scale: 32768.0000 (42657.7384)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.1003  max mem: 15572
Epoch: [25]  [1500/2809]  eta: 0:12:29  lr: 0.000017  min_lr: 0.000000  loss: 4.2261 (4.2408)  class_acc: 0.3333 (0.2987)  loss_scale: 32768.0000 (42591.8508)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1049  max mem: 15572
Epoch: [25]  [1510/2809]  eta: 0:12:24  lr: 0.000017  min_lr: 0.000000  loss: 4.2962 (4.2415)  class_acc: 0.2917 (0.2990)  loss_scale: 32768.0000 (42526.8352)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1418  max mem: 15572
Epoch: [25]  [1520/2809]  eta: 0:12:19  lr: 0.000017  min_lr: 0.000000  loss: 4.2402 (4.2406)  class_acc: 0.2500 (0.2989)  loss_scale: 32768.0000 (42462.6746)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.1788  max mem: 15572
[2025-01-16 02:32:40,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:32:40,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [1530/2809]  eta: 0:12:13  lr: 0.000017  min_lr: 0.000000  loss: 4.1543 (4.2404)  class_acc: 0.2083 (0.2985)  loss_scale: 32768.0000 (42506.3671)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1251  max mem: 15572
Epoch: [25]  [1540/2809]  eta: 0:12:08  lr: 0.000017  min_lr: 0.000000  loss: 4.3430 (4.2414)  class_acc: 0.2917 (0.2986)  loss_scale: 65536.0000 (42655.8131)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1660  max mem: 15572
Epoch: [25]  [1550/2809]  eta: 0:12:02  lr: 0.000017  min_lr: 0.000000  loss: 4.3430 (4.2418)  class_acc: 0.2917 (0.2985)  loss_scale: 65536.0000 (42803.3320)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1534  max mem: 15572
Epoch: [25]  [1560/2809]  eta: 0:11:56  lr: 0.000017  min_lr: 0.000000  loss: 4.2558 (4.2420)  class_acc: 0.2917 (0.2985)  loss_scale: 65536.0000 (42948.9609)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0940  max mem: 15572
Epoch: [25]  [1570/2809]  eta: 0:11:49  lr: 0.000017  min_lr: 0.000000  loss: 4.1986 (4.2411)  class_acc: 0.2917 (0.2985)  loss_scale: 65536.0000 (43092.7358)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0582  max mem: 15572
Epoch: [25]  [1580/2809]  eta: 0:11:45  lr: 0.000017  min_lr: 0.000000  loss: 4.1986 (4.2407)  class_acc: 0.2917 (0.2984)  loss_scale: 65536.0000 (43234.6920)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1154  max mem: 15572
Epoch: [25]  [1590/2809]  eta: 0:11:38  lr: 0.000017  min_lr: 0.000000  loss: 4.2340 (4.2409)  class_acc: 0.2500 (0.2978)  loss_scale: 65536.0000 (43374.8636)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1543  max mem: 15572
[2025-01-16 02:33:23,070] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71824
[2025-01-16 02:33:23,070] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:33:23,070] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [1600/2809]  eta: 0:11:34  lr: 0.000017  min_lr: 0.000000  loss: 4.3256 (4.2409)  class_acc: 0.2500 (0.2979)  loss_scale: 65536.0000 (43472.3498)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1646  max mem: 15572
Epoch: [25]  [1610/2809]  eta: 0:11:28  lr: 0.000017  min_lr: 0.000000  loss: 4.1953 (4.2411)  class_acc: 0.3333 (0.2978)  loss_scale: 32768.0000 (43405.9044)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.2137  max mem: 15572
Epoch: [25]  [1620/2809]  eta: 0:11:23  lr: 0.000017  min_lr: 0.000000  loss: 4.2075 (4.2414)  class_acc: 0.3333 (0.2984)  loss_scale: 32768.0000 (43340.2788)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1869  max mem: 15572
Epoch: [25]  [1630/2809]  eta: 0:11:17  lr: 0.000017  min_lr: 0.000000  loss: 4.2075 (4.2417)  class_acc: 0.3750 (0.2983)  loss_scale: 32768.0000 (43275.4580)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1398  max mem: 15572
Epoch: [25]  [1640/2809]  eta: 0:11:11  lr: 0.000017  min_lr: 0.000000  loss: 4.1524 (4.2416)  class_acc: 0.2500 (0.2982)  loss_scale: 32768.0000 (43211.4272)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0822  max mem: 15572
Epoch: [25]  [1650/2809]  eta: 0:11:06  lr: 0.000017  min_lr: 0.000000  loss: 4.3116 (4.2416)  class_acc: 0.2917 (0.2987)  loss_scale: 32768.0000 (43148.1720)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1182  max mem: 15572
Epoch: [25]  [1660/2809]  eta: 0:10:59  lr: 0.000017  min_lr: 0.000000  loss: 4.2757 (4.2414)  class_acc: 0.2917 (0.2986)  loss_scale: 32768.0000 (43085.6785)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1008  max mem: 15572
Epoch: [25]  [1670/2809]  eta: 0:10:54  lr: 0.000017  min_lr: 0.000000  loss: 4.2757 (4.2414)  class_acc: 0.2917 (0.2989)  loss_scale: 32768.0000 (43023.9330)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0903  max mem: 15572
Epoch: [25]  [1680/2809]  eta: 0:10:48  lr: 0.000017  min_lr: 0.000000  loss: 4.1012 (4.2402)  class_acc: 0.2917 (0.2991)  loss_scale: 32768.0000 (42962.9221)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1291  max mem: 15572
Epoch: [25]  [1690/2809]  eta: 0:10:43  lr: 0.000017  min_lr: 0.000000  loss: 4.1577 (4.2398)  class_acc: 0.2917 (0.2990)  loss_scale: 32768.0000 (42902.6328)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1615  max mem: 15572
Epoch: [25]  [1700/2809]  eta: 0:10:37  lr: 0.000017  min_lr: 0.000000  loss: 4.1659 (4.2395)  class_acc: 0.2917 (0.2991)  loss_scale: 32768.0000 (42843.0523)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1595  max mem: 15572
Epoch: [25]  [1710/2809]  eta: 0:10:31  lr: 0.000017  min_lr: 0.000000  loss: 4.2471 (4.2397)  class_acc: 0.2917 (0.2991)  loss_scale: 32768.0000 (42784.1683)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1025  max mem: 15572
Epoch: [25]  [1720/2809]  eta: 0:10:25  lr: 0.000017  min_lr: 0.000000  loss: 4.1782 (4.2394)  class_acc: 0.2917 (0.2992)  loss_scale: 32768.0000 (42725.9686)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0909  max mem: 15572
[2025-01-16 02:34:37,965] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:34:37,965] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [1730/2809]  eta: 0:10:19  lr: 0.000017  min_lr: 0.000000  loss: 4.1782 (4.2396)  class_acc: 0.2917 (0.2989)  loss_scale: 32768.0000 (42725.2317)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0863  max mem: 15572
[2025-01-16 02:34:43,845] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71965
[2025-01-16 02:34:43,846] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:34:43,846] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [1740/2809]  eta: 0:10:13  lr: 0.000017  min_lr: 0.000000  loss: 4.2368 (4.2392)  class_acc: 0.2917 (0.2992)  loss_scale: 65536.0000 (42837.4314)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0720  max mem: 15572
Epoch: [25]  [1750/2809]  eta: 0:10:08  lr: 0.000017  min_lr: 0.000000  loss: 4.2368 (4.2393)  class_acc: 0.3333 (0.2990)  loss_scale: 32768.0000 (42779.9246)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1462  max mem: 15572
Epoch: [25]  [1760/2809]  eta: 0:10:02  lr: 0.000017  min_lr: 0.000000  loss: 4.2756 (4.2394)  class_acc: 0.2083 (0.2988)  loss_scale: 32768.0000 (42723.0710)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1696  max mem: 15572
Epoch: [25]  [1770/2809]  eta: 0:09:57  lr: 0.000017  min_lr: 0.000000  loss: 4.4218 (4.2400)  class_acc: 0.2500 (0.2992)  loss_scale: 32768.0000 (42666.8594)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1511  max mem: 15572
[2025-01-16 02:35:04,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=72000, skipped=449, lr=[1.6438077232268055e-07, 1.6438077232268055e-07, 2.3482967474668654e-07, 2.3482967474668654e-07, 3.3547096392383795e-07, 3.3547096392383795e-07, 4.792442341769114e-07, 4.792442341769114e-07, 6.846346202527306e-07, 6.846346202527306e-07, 9.78049457503901e-07, 9.78049457503901e-07, 1.3972135107198584e-06, 1.3972135107198584e-06, 1.9960193010283693e-06, 1.9960193010283693e-06, 2.851456144326242e-06, 2.851456144326242e-06, 4.073508777608918e-06, 4.073508777608918e-06, 5.819298253727025e-06, 5.819298253727025e-06, 8.313283219610036e-06, 8.313283219610036e-06, 1.1876118885157196e-05, 1.1876118885157196e-05, 1.6965884121653138e-05, 1.6965884121653138e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:35:04,230] [INFO] [timer.py:260:stop] epoch=0/micro_step=72000/global_step=72000, RunningAvgSamplesPerSec=27.890409646718226, CurrSamplesPerSec=28.806439427759045, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [25]  [1780/2809]  eta: 0:09:50  lr: 0.000017  min_lr: 0.000000  loss: 4.3782 (4.2404)  class_acc: 0.3333 (0.2993)  loss_scale: 32768.0000 (42611.2791)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1156  max mem: 15572
Epoch: [25]  [1790/2809]  eta: 0:09:44  lr: 0.000017  min_lr: 0.000000  loss: 4.2747 (4.2403)  class_acc: 0.2917 (0.2990)  loss_scale: 32768.0000 (42556.3194)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0351  max mem: 15572
Epoch: [25]  [1800/2809]  eta: 0:09:38  lr: 0.000017  min_lr: 0.000000  loss: 4.1517 (4.2403)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (42501.9700)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0212  max mem: 15572
Epoch: [25]  [1810/2809]  eta: 0:09:33  lr: 0.000017  min_lr: 0.000000  loss: 4.1517 (4.2401)  class_acc: 0.3333 (0.2997)  loss_scale: 32768.0000 (42448.2209)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1188  max mem: 15572
Epoch: [25]  [1820/2809]  eta: 0:09:27  lr: 0.000017  min_lr: 0.000000  loss: 4.1473 (4.2396)  class_acc: 0.3333 (0.3000)  loss_scale: 32768.0000 (42395.0621)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.1853  max mem: 15572
Epoch: [25]  [1830/2809]  eta: 0:09:22  lr: 0.000017  min_lr: 0.000000  loss: 4.2469 (4.2397)  class_acc: 0.2917 (0.2997)  loss_scale: 32768.0000 (42342.4839)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1552  max mem: 15572
Epoch: [25]  [1840/2809]  eta: 0:09:16  lr: 0.000017  min_lr: 0.000000  loss: 4.1353 (4.2388)  class_acc: 0.2917 (0.2998)  loss_scale: 32768.0000 (42290.4769)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1171  max mem: 15572
Epoch: [25]  [1850/2809]  eta: 0:09:10  lr: 0.000017  min_lr: 0.000000  loss: 4.1353 (4.2389)  class_acc: 0.3333 (0.2998)  loss_scale: 32768.0000 (42239.0319)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0876  max mem: 15572
Epoch: [25]  [1860/2809]  eta: 0:09:05  lr: 0.000017  min_lr: 0.000000  loss: 4.3257 (4.2389)  class_acc: 0.2500 (0.2996)  loss_scale: 32768.0000 (42188.1397)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1579  max mem: 15572
[2025-01-16 02:35:58,708] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:35:58,709] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [1870/2809]  eta: 0:08:59  lr: 0.000017  min_lr: 0.000000  loss: 4.2457 (4.2392)  class_acc: 0.2500 (0.2995)  loss_scale: 32768.0000 (42172.8188)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1541  max mem: 15572
Epoch: [25]  [1880/2809]  eta: 0:08:53  lr: 0.000017  min_lr: 0.000000  loss: 4.1505 (4.2386)  class_acc: 0.2917 (0.2994)  loss_scale: 65536.0000 (42297.0250)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1486  max mem: 15572
Epoch: [25]  [1890/2809]  eta: 0:08:47  lr: 0.000017  min_lr: 0.000000  loss: 4.2109 (4.2388)  class_acc: 0.2500 (0.2993)  loss_scale: 65536.0000 (42419.9175)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1404  max mem: 15572
Epoch: [25]  [1900/2809]  eta: 0:08:41  lr: 0.000017  min_lr: 0.000000  loss: 4.2918 (4.2389)  class_acc: 0.2917 (0.2994)  loss_scale: 65536.0000 (42541.5171)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0665  max mem: 15572
Epoch: [25]  [1910/2809]  eta: 0:08:36  lr: 0.000017  min_lr: 0.000000  loss: 4.2644 (4.2389)  class_acc: 0.3333 (0.2995)  loss_scale: 65536.0000 (42661.8441)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.1073  max mem: 15572
Epoch: [25]  [1920/2809]  eta: 0:08:30  lr: 0.000017  min_lr: 0.000000  loss: 4.2144 (4.2390)  class_acc: 0.2917 (0.2992)  loss_scale: 65536.0000 (42780.9183)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1651  max mem: 15572
Epoch: [25]  [1930/2809]  eta: 0:08:24  lr: 0.000017  min_lr: 0.000000  loss: 4.2103 (4.2384)  class_acc: 0.2500 (0.2992)  loss_scale: 65536.0000 (42898.7592)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1609  max mem: 15572
Epoch: [25]  [1940/2809]  eta: 0:08:19  lr: 0.000017  min_lr: 0.000000  loss: 4.2291 (4.2383)  class_acc: 0.3333 (0.2995)  loss_scale: 65536.0000 (43015.3859)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1493  max mem: 15572
Epoch: [25]  [1950/2809]  eta: 0:08:13  lr: 0.000017  min_lr: 0.000000  loss: 4.2637 (4.2382)  class_acc: 0.3333 (0.2995)  loss_scale: 65536.0000 (43130.8170)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1315  max mem: 15572
Epoch: [25]  [1960/2809]  eta: 0:08:07  lr: 0.000017  min_lr: 0.000000  loss: 4.2371 (4.2380)  class_acc: 0.3333 (0.2998)  loss_scale: 65536.0000 (43245.0709)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1196  max mem: 15572
[2025-01-16 02:36:53,716] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72190
[2025-01-16 02:36:53,716] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:36:53,716] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [1970/2809]  eta: 0:08:01  lr: 0.000017  min_lr: 0.000000  loss: 4.2371 (4.2385)  class_acc: 0.2917 (0.3000)  loss_scale: 65536.0000 (43258.4150)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0676  max mem: 15572
Epoch: [25]  [1980/2809]  eta: 0:07:56  lr: 0.000017  min_lr: 0.000000  loss: 4.2231 (4.2382)  class_acc: 0.3333 (0.3002)  loss_scale: 32768.0000 (43205.4599)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1123  max mem: 15572
Epoch: [25]  [1990/2809]  eta: 0:07:50  lr: 0.000017  min_lr: 0.000000  loss: 4.1960 (4.2382)  class_acc: 0.3333 (0.3001)  loss_scale: 32768.0000 (43153.0367)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1583  max mem: 15572
Epoch: [25]  [2000/2809]  eta: 0:07:44  lr: 0.000017  min_lr: 0.000000  loss: 4.2217 (4.2382)  class_acc: 0.2917 (0.3005)  loss_scale: 32768.0000 (43101.1374)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1302  max mem: 15572
Epoch: [25]  [2010/2809]  eta: 0:07:38  lr: 0.000017  min_lr: 0.000000  loss: 4.2215 (4.2381)  class_acc: 0.2917 (0.3005)  loss_scale: 32768.0000 (43049.7544)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1602  max mem: 15572
Epoch: [25]  [2020/2809]  eta: 0:07:32  lr: 0.000017  min_lr: 0.000000  loss: 4.1970 (4.2378)  class_acc: 0.2083 (0.3003)  loss_scale: 32768.0000 (42998.8798)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1223  max mem: 15572
Epoch: [25]  [2030/2809]  eta: 0:07:27  lr: 0.000017  min_lr: 0.000000  loss: 4.2473 (4.2374)  class_acc: 0.3333 (0.3008)  loss_scale: 32768.0000 (42948.5062)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0785  max mem: 15572
Epoch: [25]  [2040/2809]  eta: 0:07:21  lr: 0.000017  min_lr: 0.000000  loss: 4.3351 (4.2376)  class_acc: 0.2083 (0.3003)  loss_scale: 32768.0000 (42898.6262)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1491  max mem: 15572
Epoch: [25]  [2050/2809]  eta: 0:07:15  lr: 0.000017  min_lr: 0.000000  loss: 4.3124 (4.2377)  class_acc: 0.2083 (0.3000)  loss_scale: 32768.0000 (42849.2326)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1753  max mem: 15572
Epoch: [25]  [2060/2809]  eta: 0:07:10  lr: 0.000017  min_lr: 0.000000  loss: 4.3124 (4.2383)  class_acc: 0.2083 (0.2997)  loss_scale: 32768.0000 (42800.3183)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1962  max mem: 15572
Epoch: [25]  [2070/2809]  eta: 0:07:04  lr: 0.000017  min_lr: 0.000000  loss: 4.3736 (4.2386)  class_acc: 0.3333 (0.3000)  loss_scale: 32768.0000 (42751.8764)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1243  max mem: 15572
Epoch: [25]  [2080/2809]  eta: 0:06:59  lr: 0.000017  min_lr: 0.000000  loss: 4.3159 (4.2385)  class_acc: 0.3333 (0.2999)  loss_scale: 32768.0000 (42703.9000)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1120  max mem: 15572
Epoch: [25]  [2090/2809]  eta: 0:06:53  lr: 0.000017  min_lr: 0.000000  loss: 4.1770 (4.2375)  class_acc: 0.3333 (0.3001)  loss_scale: 32768.0000 (42656.3826)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1451  max mem: 15572
[2025-01-16 02:38:08,505] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:38:08,506] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 02:38:11,217] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72323
[2025-01-16 02:38:11,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:38:11,218] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [2100/2809]  eta: 0:06:47  lr: 0.000017  min_lr: 0.000000  loss: 4.1236 (4.2372)  class_acc: 0.2917 (0.3002)  loss_scale: 32768.0000 (42671.7030)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0814  max mem: 15572
Epoch: [25]  [2110/2809]  eta: 0:06:41  lr: 0.000017  min_lr: 0.000000  loss: 4.2345 (4.2365)  class_acc: 0.3333 (0.3005)  loss_scale: 32768.0000 (42624.7883)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0860  max mem: 15572
Epoch: [25]  [2120/2809]  eta: 0:06:35  lr: 0.000017  min_lr: 0.000000  loss: 4.2345 (4.2363)  class_acc: 0.3750 (0.3008)  loss_scale: 32768.0000 (42578.3159)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0785  max mem: 15572
Epoch: [25]  [2130/2809]  eta: 0:06:29  lr: 0.000017  min_lr: 0.000000  loss: 4.2379 (4.2359)  class_acc: 0.3333 (0.3009)  loss_scale: 32768.0000 (42532.2797)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1031  max mem: 15572
Epoch: [25]  [2140/2809]  eta: 0:06:24  lr: 0.000017  min_lr: 0.000000  loss: 4.2738 (4.2367)  class_acc: 0.3333 (0.3010)  loss_scale: 32768.0000 (42486.6735)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1062  max mem: 15572
Epoch: [25]  [2150/2809]  eta: 0:06:18  lr: 0.000017  min_lr: 0.000000  loss: 4.4309 (4.2370)  class_acc: 0.2917 (0.3012)  loss_scale: 32768.0000 (42441.4914)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0876  max mem: 15572
Epoch: [25]  [2160/2809]  eta: 0:06:12  lr: 0.000017  min_lr: 0.000000  loss: 4.3236 (4.2371)  class_acc: 0.2917 (0.3014)  loss_scale: 32768.0000 (42396.7274)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0877  max mem: 15572
Epoch: [25]  [2170/2809]  eta: 0:06:06  lr: 0.000017  min_lr: 0.000000  loss: 4.1625 (4.2370)  class_acc: 0.2917 (0.3013)  loss_scale: 32768.0000 (42352.3759)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1059  max mem: 15572
Epoch: [25]  [2180/2809]  eta: 0:06:01  lr: 0.000017  min_lr: 0.000000  loss: 4.1815 (4.2366)  class_acc: 0.3333 (0.3016)  loss_scale: 32768.0000 (42308.4310)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1461  max mem: 15572
Epoch: [25]  [2190/2809]  eta: 0:05:55  lr: 0.000017  min_lr: 0.000000  loss: 4.1637 (4.2360)  class_acc: 0.2917 (0.3016)  loss_scale: 32768.0000 (42264.8873)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1567  max mem: 15572
Epoch: [25]  [2200/2809]  eta: 0:05:49  lr: 0.000017  min_lr: 0.000000  loss: 4.0560 (4.2354)  class_acc: 0.2500 (0.3016)  loss_scale: 32768.0000 (42221.7392)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1148  max mem: 15572
Epoch: [25]  [2210/2809]  eta: 0:05:43  lr: 0.000017  min_lr: 0.000000  loss: 4.1754 (4.2354)  class_acc: 0.2500 (0.3015)  loss_scale: 32768.0000 (42178.9815)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0712  max mem: 15572
Epoch: [25]  [2220/2809]  eta: 0:05:38  lr: 0.000017  min_lr: 0.000000  loss: 4.1772 (4.2351)  class_acc: 0.3333 (0.3016)  loss_scale: 32768.0000 (42136.6087)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1110  max mem: 15572
[2025-01-16 02:39:25,387] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:39:25,387] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 02:39:25,835] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72453
[2025-01-16 02:39:25,835] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:39:25,835] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [2230/2809]  eta: 0:05:32  lr: 0.000017  min_lr: 0.000000  loss: 4.2074 (4.2351)  class_acc: 0.2917 (0.3016)  loss_scale: 32768.0000 (42109.3035)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1742  max mem: 15572
Epoch: [25]  [2240/2809]  eta: 0:05:26  lr: 0.000017  min_lr: 0.000000  loss: 4.3419 (4.2359)  class_acc: 0.2917 (0.3015)  loss_scale: 32768.0000 (42067.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1215  max mem: 15572
Epoch: [25]  [2250/2809]  eta: 0:05:21  lr: 0.000017  min_lr: 0.000000  loss: 4.3419 (4.2356)  class_acc: 0.2500 (0.3016)  loss_scale: 32768.0000 (42026.3065)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0989  max mem: 15572
Epoch: [25]  [2260/2809]  eta: 0:05:15  lr: 0.000017  min_lr: 0.000000  loss: 4.2415 (4.2355)  class_acc: 0.2917 (0.3015)  loss_scale: 32768.0000 (41985.3587)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.0784  max mem: 15572
Epoch: [25]  [2270/2809]  eta: 0:05:09  lr: 0.000017  min_lr: 0.000000  loss: 4.2415 (4.2358)  class_acc: 0.2917 (0.3016)  loss_scale: 32768.0000 (41944.7715)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0665  max mem: 15572
Epoch: [25]  [2280/2809]  eta: 0:05:03  lr: 0.000017  min_lr: 0.000000  loss: 4.1623 (4.2355)  class_acc: 0.3333 (0.3016)  loss_scale: 32768.0000 (41904.5401)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1234  max mem: 15572
Epoch: [25]  [2290/2809]  eta: 0:04:57  lr: 0.000017  min_lr: 0.000000  loss: 4.0597 (4.2349)  class_acc: 0.3750 (0.3017)  loss_scale: 32768.0000 (41864.6600)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1242  max mem: 15572
Epoch: [25]  [2300/2809]  eta: 0:04:52  lr: 0.000017  min_lr: 0.000000  loss: 4.0463 (4.2344)  class_acc: 0.3333 (0.3019)  loss_scale: 32768.0000 (41825.1265)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1410  max mem: 15572
Epoch: [25]  [2310/2809]  eta: 0:04:46  lr: 0.000017  min_lr: 0.000000  loss: 4.1152 (4.2341)  class_acc: 0.3333 (0.3019)  loss_scale: 32768.0000 (41785.9351)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1528  max mem: 15572
Epoch: [25]  [2320/2809]  eta: 0:04:40  lr: 0.000017  min_lr: 0.000000  loss: 4.2049 (4.2340)  class_acc: 0.2917 (0.3019)  loss_scale: 32768.0000 (41747.0814)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1133  max mem: 15572
Epoch: [25]  [2330/2809]  eta: 0:04:34  lr: 0.000017  min_lr: 0.000000  loss: 4.2727 (4.2342)  class_acc: 0.3333 (0.3022)  loss_scale: 32768.0000 (41708.5611)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1459  max mem: 15572
Epoch: [25]  [2340/2809]  eta: 0:04:29  lr: 0.000017  min_lr: 0.000000  loss: 4.1644 (4.2334)  class_acc: 0.3333 (0.3023)  loss_scale: 32768.0000 (41670.3699)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1317  max mem: 15572
Epoch: [25]  [2350/2809]  eta: 0:04:23  lr: 0.000017  min_lr: 0.000000  loss: 4.0413 (4.2324)  class_acc: 0.3333 (0.3026)  loss_scale: 32768.0000 (41632.5036)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1262  max mem: 15572
[2025-01-16 02:40:38,029] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:40:38,030] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [2360/2809]  eta: 0:04:17  lr: 0.000017  min_lr: 0.000000  loss: 4.1283 (4.2328)  class_acc: 0.3750 (0.3028)  loss_scale: 32768.0000 (41650.4735)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1266  max mem: 15572
Epoch: [25]  [2370/2809]  eta: 0:04:11  lr: 0.000017  min_lr: 0.000000  loss: 4.2463 (4.2327)  class_acc: 0.2917 (0.3027)  loss_scale: 65536.0000 (41751.2138)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0992  max mem: 15572
[2025-01-16 02:40:46,471] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72598
[2025-01-16 02:40:46,472] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:40:46,472] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [2380/2809]  eta: 0:04:06  lr: 0.000017  min_lr: 0.000000  loss: 4.2086 (4.2317)  class_acc: 0.3333 (0.3031)  loss_scale: 65536.0000 (41741.0097)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1825  max mem: 15572
Epoch: [25]  [2390/2809]  eta: 0:04:00  lr: 0.000017  min_lr: 0.000000  loss: 4.0791 (4.2314)  class_acc: 0.3333 (0.3032)  loss_scale: 32768.0000 (41703.4814)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.1859  max mem: 15572
Epoch: [25]  [2400/2809]  eta: 0:03:54  lr: 0.000017  min_lr: 0.000000  loss: 4.2691 (4.2317)  class_acc: 0.2917 (0.3033)  loss_scale: 32768.0000 (41666.2657)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1291  max mem: 15572
Epoch: [25]  [2410/2809]  eta: 0:03:49  lr: 0.000017  min_lr: 0.000000  loss: 4.3221 (4.2318)  class_acc: 0.2500 (0.3030)  loss_scale: 32768.0000 (41629.3588)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1052  max mem: 15572
Epoch: [25]  [2420/2809]  eta: 0:03:43  lr: 0.000017  min_lr: 0.000000  loss: 4.3265 (4.2320)  class_acc: 0.2500 (0.3030)  loss_scale: 32768.0000 (41592.7567)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1355  max mem: 15572
Epoch: [25]  [2430/2809]  eta: 0:03:37  lr: 0.000016  min_lr: 0.000000  loss: 4.3356 (4.2320)  class_acc: 0.2500 (0.3027)  loss_scale: 32768.0000 (41556.4558)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1433  max mem: 15572
Epoch: [25]  [2440/2809]  eta: 0:03:31  lr: 0.000016  min_lr: 0.000000  loss: 4.3189 (4.2323)  class_acc: 0.2917 (0.3027)  loss_scale: 32768.0000 (41520.4523)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1208  max mem: 15572
Epoch: [25]  [2450/2809]  eta: 0:03:26  lr: 0.000016  min_lr: 0.000000  loss: 4.2972 (4.2319)  class_acc: 0.2917 (0.3029)  loss_scale: 32768.0000 (41484.7426)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1086  max mem: 15572
Epoch: [25]  [2460/2809]  eta: 0:03:20  lr: 0.000016  min_lr: 0.000000  loss: 4.2127 (4.2323)  class_acc: 0.3333 (0.3029)  loss_scale: 32768.0000 (41449.3230)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1286  max mem: 15572
Epoch: [25]  [2470/2809]  eta: 0:03:14  lr: 0.000016  min_lr: 0.000000  loss: 4.2160 (4.2319)  class_acc: 0.3333 (0.3030)  loss_scale: 32768.0000 (41414.1902)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1653  max mem: 15572
Epoch: [25]  [2480/2809]  eta: 0:03:08  lr: 0.000016  min_lr: 0.000000  loss: 4.2056 (4.2323)  class_acc: 0.2917 (0.3029)  loss_scale: 32768.0000 (41379.3406)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1370  max mem: 15572
Epoch: [25]  [2490/2809]  eta: 0:03:03  lr: 0.000016  min_lr: 0.000000  loss: 4.1495 (4.2317)  class_acc: 0.2917 (0.3029)  loss_scale: 32768.0000 (41344.7708)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1007  max mem: 15572
Epoch: [25]  [2500/2809]  eta: 0:02:57  lr: 0.000016  min_lr: 0.000000  loss: 4.1379 (4.2322)  class_acc: 0.2917 (0.3030)  loss_scale: 32768.0000 (41310.4774)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0691  max mem: 15572
[2025-01-16 02:42:01,468] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:42:01,468] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [2510/2809]  eta: 0:02:51  lr: 0.000016  min_lr: 0.000000  loss: 4.3081 (4.2326)  class_acc: 0.3333 (0.3032)  loss_scale: 32768.0000 (41393.9052)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0716  max mem: 15572
Epoch: [25]  [2520/2809]  eta: 0:02:45  lr: 0.000016  min_lr: 0.000000  loss: 4.3064 (4.2326)  class_acc: 0.2917 (0.3031)  loss_scale: 65536.0000 (41489.6692)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0668  max mem: 15572
[2025-01-16 02:42:17,248] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72754
[2025-01-16 02:42:17,249] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:42:17,249] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [2530/2809]  eta: 0:02:40  lr: 0.000016  min_lr: 0.000000  loss: 4.2734 (4.2324)  class_acc: 0.3750 (0.3034)  loss_scale: 65536.0000 (41558.7831)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0660  max mem: 15572
Epoch: [25]  [2540/2809]  eta: 0:02:34  lr: 0.000016  min_lr: 0.000000  loss: 4.2751 (4.2326)  class_acc: 0.4167 (0.3032)  loss_scale: 32768.0000 (41524.1873)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0476  max mem: 15572
Epoch: [25]  [2550/2809]  eta: 0:02:28  lr: 0.000016  min_lr: 0.000000  loss: 4.2518 (4.2325)  class_acc: 0.2500 (0.3033)  loss_scale: 32768.0000 (41489.8628)  weight_decay: 0.0500 (0.0500)  time: 0.4678  data: 0.0007  max mem: 15572
Epoch: [25]  [2560/2809]  eta: 0:02:22  lr: 0.000016  min_lr: 0.000000  loss: 4.2623 (4.2331)  class_acc: 0.2917 (0.3033)  loss_scale: 32768.0000 (41455.8063)  weight_decay: 0.0500 (0.0500)  time: 0.4882  data: 0.0345  max mem: 15572
Epoch: [25]  [2570/2809]  eta: 0:02:16  lr: 0.000016  min_lr: 0.000000  loss: 4.3304 (4.2334)  class_acc: 0.2500 (0.3031)  loss_scale: 32768.0000 (41422.0148)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0529  max mem: 15572
Epoch: [25]  [2580/2809]  eta: 0:02:11  lr: 0.000016  min_lr: 0.000000  loss: 4.2579 (4.2336)  class_acc: 0.2500 (0.3030)  loss_scale: 32768.0000 (41388.4851)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0491  max mem: 15572
Epoch: [25]  [2590/2809]  eta: 0:02:05  lr: 0.000016  min_lr: 0.000000  loss: 4.3073 (4.2336)  class_acc: 0.2083 (0.3030)  loss_scale: 32768.0000 (41355.2142)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.1612  max mem: 15572
Epoch: [25]  [2600/2809]  eta: 0:01:59  lr: 0.000016  min_lr: 0.000000  loss: 4.2615 (4.2334)  class_acc: 0.2500 (0.3030)  loss_scale: 32768.0000 (41322.1992)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.1819  max mem: 15572
Epoch: [25]  [2610/2809]  eta: 0:01:54  lr: 0.000016  min_lr: 0.000000  loss: 4.1201 (4.2334)  class_acc: 0.2917 (0.3029)  loss_scale: 32768.0000 (41289.4370)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1070  max mem: 15572
Epoch: [25]  [2620/2809]  eta: 0:01:48  lr: 0.000016  min_lr: 0.000000  loss: 4.2599 (4.2332)  class_acc: 0.2500 (0.3028)  loss_scale: 32768.0000 (41256.9248)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1119  max mem: 15572
Epoch: [25]  [2630/2809]  eta: 0:01:42  lr: 0.000016  min_lr: 0.000000  loss: 4.2599 (4.2333)  class_acc: 0.2917 (0.3030)  loss_scale: 32768.0000 (41224.6598)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1444  max mem: 15572
Epoch: [25]  [2640/2809]  eta: 0:01:36  lr: 0.000016  min_lr: 0.000000  loss: 4.2168 (4.2334)  class_acc: 0.2500 (0.3027)  loss_scale: 32768.0000 (41192.6392)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1004  max mem: 15572
Epoch: [25]  [2650/2809]  eta: 0:01:31  lr: 0.000016  min_lr: 0.000000  loss: 4.1375 (4.2331)  class_acc: 0.2500 (0.3029)  loss_scale: 32768.0000 (41160.8601)  weight_decay: 0.0500 (0.0500)  time: 0.4704  data: 0.0458  max mem: 15572
[2025-01-16 02:43:26,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:43:26,673] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [2660/2809]  eta: 0:01:25  lr: 0.000016  min_lr: 0.000000  loss: 4.2439 (4.2340)  class_acc: 0.2500 (0.3025)  loss_scale: 32768.0000 (41166.2623)  weight_decay: 0.0500 (0.0500)  time: 0.4507  data: 0.0341  max mem: 15572
Epoch: [25]  [2670/2809]  eta: 0:01:19  lr: 0.000016  min_lr: 0.000000  loss: 4.4698 (4.2344)  class_acc: 0.2083 (0.3023)  loss_scale: 65536.0000 (41257.5006)  weight_decay: 0.0500 (0.0500)  time: 0.4423  data: 0.0006  max mem: 15572
Epoch: [25]  [2680/2809]  eta: 0:01:13  lr: 0.000016  min_lr: 0.000000  loss: 4.2143 (4.2340)  class_acc: 0.2500 (0.3024)  loss_scale: 65536.0000 (41348.0582)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1132  max mem: 15572
Epoch: [25]  [2690/2809]  eta: 0:01:08  lr: 0.000016  min_lr: 0.000000  loss: 4.1004 (4.2337)  class_acc: 0.2917 (0.3025)  loss_scale: 65536.0000 (41437.9428)  weight_decay: 0.0500 (0.0500)  time: 0.7223  data: 0.2576  max mem: 15572
Epoch: [25]  [2700/2809]  eta: 0:01:02  lr: 0.000016  min_lr: 0.000000  loss: 4.2153 (4.2334)  class_acc: 0.2917 (0.3024)  loss_scale: 65536.0000 (41527.1618)  weight_decay: 0.0500 (0.0500)  time: 0.7358  data: 0.2503  max mem: 15572
Epoch: [25]  [2710/2809]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 4.3100 (4.2335)  class_acc: 0.2500 (0.3024)  loss_scale: 65536.0000 (41615.7226)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.1982  max mem: 15572
Epoch: [25]  [2720/2809]  eta: 0:00:51  lr: 0.000016  min_lr: 0.000000  loss: 4.3100 (4.2340)  class_acc: 0.2917 (0.3023)  loss_scale: 65536.0000 (41703.6325)  weight_decay: 0.0500 (0.0500)  time: 0.6776  data: 0.1930  max mem: 15572
Epoch: [25]  [2730/2809]  eta: 0:00:45  lr: 0.000016  min_lr: 0.000000  loss: 4.2831 (4.2338)  class_acc: 0.2917 (0.3024)  loss_scale: 65536.0000 (41790.8986)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.1687  max mem: 15572
Epoch: [25]  [2740/2809]  eta: 0:00:39  lr: 0.000016  min_lr: 0.000000  loss: 4.2329 (4.2336)  class_acc: 0.3333 (0.3024)  loss_scale: 65536.0000 (41877.5279)  weight_decay: 0.0500 (0.0500)  time: 0.6712  data: 0.2128  max mem: 15572
Epoch: [25]  [2750/2809]  eta: 0:00:33  lr: 0.000016  min_lr: 0.000000  loss: 4.1714 (4.2335)  class_acc: 0.2917 (0.3024)  loss_scale: 65536.0000 (41963.5274)  weight_decay: 0.0500 (0.0500)  time: 0.6796  data: 0.2411  max mem: 15572
Epoch: [25]  [2760/2809]  eta: 0:00:28  lr: 0.000016  min_lr: 0.000000  loss: 4.2458 (4.2334)  class_acc: 0.2917 (0.3024)  loss_scale: 65536.0000 (42048.9040)  weight_decay: 0.0500 (0.0500)  time: 0.6564  data: 0.1947  max mem: 15572
[2025-01-16 02:44:39,625] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72994
[2025-01-16 02:44:39,625] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:44:39,626] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [2770/2809]  eta: 0:00:22  lr: 0.000016  min_lr: 0.000000  loss: 4.2858 (4.2337)  class_acc: 0.2500 (0.3023)  loss_scale: 65536.0000 (42110.0137)  weight_decay: 0.0500 (0.0500)  time: 0.6591  data: 0.1960  max mem: 15572
[2025-01-16 02:44:42,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=73000, skipped=455, lr=[1.5744952022753748e-07, 1.5744952022753748e-07, 2.2492788603933928e-07, 2.2492788603933928e-07, 3.2132555148477044e-07, 3.2132555148477044e-07, 4.5903650212110065e-07, 4.5903650212110065e-07, 6.557664316015724e-07, 6.557664316015724e-07, 9.368091880022463e-07, 9.368091880022463e-07, 1.338298840003209e-06, 1.338298840003209e-06, 1.91185548571887e-06, 1.91185548571887e-06, 2.731222122455529e-06, 2.731222122455529e-06, 3.9017458892221845e-06, 3.9017458892221845e-06, 5.573922698888835e-06, 5.573922698888835e-06, 7.962746712698336e-06, 7.962746712698336e-06, 1.137535244671191e-05, 1.137535244671191e-05, 1.625050349530273e-05, 1.625050349530273e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:44:42,045] [INFO] [timer.py:260:stop] epoch=0/micro_step=73000/global_step=73000, RunningAvgSamplesPerSec=27.89060289333422, CurrSamplesPerSec=26.272815964245503, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [25]  [2780/2809]  eta: 0:00:16  lr: 0.000016  min_lr: 0.000000  loss: 4.2858 (4.2340)  class_acc: 0.2917 (0.3024)  loss_scale: 32768.0000 (42076.4214)  weight_decay: 0.0500 (0.0500)  time: 0.6580  data: 0.2066  max mem: 15572
Epoch: [25]  [2790/2809]  eta: 0:00:10  lr: 0.000016  min_lr: 0.000000  loss: 4.2646 (4.2339)  class_acc: 0.2917 (0.3024)  loss_scale: 32768.0000 (42043.0699)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1396  max mem: 15572
Epoch: [25]  [2800/2809]  eta: 0:00:05  lr: 0.000016  min_lr: 0.000000  loss: 4.2646 (4.2342)  class_acc: 0.2917 (0.3025)  loss_scale: 32768.0000 (42009.9564)  weight_decay: 0.0500 (0.0500)  time: 0.4264  data: 0.0310  max mem: 15572
Epoch: [25]  [2808/2809]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 4.3190 (4.2348)  class_acc: 0.2917 (0.3024)  loss_scale: 32768.0000 (41983.6355)  weight_decay: 0.0500 (0.0500)  time: 0.3858  data: 0.0003  max mem: 15572
Epoch: [25] Total time: 0:26:54 (0.5748 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 4.3190 (4.2348)  class_acc: 0.2917 (0.3024)  loss_scale: 32768.0000 (41983.6355)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:15:54  loss: 1.0432 (1.0432)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.5085  data: 3.3300  max mem: 15572
Val:  [ 10/272]  eta: 0:02:53  loss: 2.6867 (2.6762)  acc1: 33.3333 (38.8889)  acc5: 66.6667 (68.6869)  time: 0.6641  data: 0.4530  max mem: 15572
Val:  [ 20/272]  eta: 0:02:06  loss: 2.7437 (2.7031)  acc1: 38.8889 (41.0053)  acc5: 66.6667 (68.5185)  time: 0.3506  data: 0.1558  max mem: 15572
Val:  [ 30/272]  eta: 0:01:46  loss: 2.7437 (2.7363)  acc1: 44.4444 (38.8889)  acc5: 66.6667 (69.1756)  time: 0.3147  data: 0.1313  max mem: 15572
Val:  [ 40/272]  eta: 0:01:32  loss: 2.6839 (2.7230)  acc1: 27.7778 (37.2629)  acc5: 77.7778 (70.0542)  time: 0.2957  data: 0.1076  max mem: 15572
Val:  [ 50/272]  eta: 0:01:26  loss: 2.5664 (2.6442)  acc1: 38.8889 (39.5425)  acc5: 77.7778 (72.5490)  time: 0.3131  data: 0.1284  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 1.9970 (2.5793)  acc1: 50.0000 (41.0747)  acc5: 83.3333 (73.4973)  time: 0.3201  data: 0.1398  max mem: 15572
Val:  [ 70/272]  eta: 0:01:12  loss: 2.1203 (2.5194)  acc1: 55.5556 (43.6620)  acc5: 83.3333 (74.6479)  time: 0.2783  data: 0.0984  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 2.2578 (2.5240)  acc1: 50.0000 (43.4842)  acc5: 77.7778 (74.5542)  time: 0.2959  data: 0.1132  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 2.5151 (2.5261)  acc1: 50.0000 (44.0781)  acc5: 77.7778 (75.1526)  time: 0.3607  data: 0.1757  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.5151 (2.5470)  acc1: 50.0000 (43.5094)  acc5: 77.7778 (74.9175)  time: 0.3256  data: 0.1346  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.6196 (2.5958)  acc1: 38.8889 (42.3423)  acc5: 77.7778 (74.0741)  time: 0.2813  data: 0.0920  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 2.8640 (2.6237)  acc1: 33.3333 (41.8733)  acc5: 61.1111 (73.5078)  time: 0.3181  data: 0.1289  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.4775 (2.5967)  acc1: 50.0000 (42.8329)  acc5: 77.7778 (74.3003)  time: 0.3408  data: 0.1516  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 2.2862 (2.5979)  acc1: 55.5556 (42.9866)  acc5: 83.3333 (74.2711)  time: 0.2928  data: 0.1160  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.5808 (2.5976)  acc1: 38.8889 (42.7888)  acc5: 77.7778 (74.2458)  time: 0.2939  data: 0.1148  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.5808 (2.6000)  acc1: 44.4444 (42.9607)  acc5: 77.7778 (74.4306)  time: 0.3385  data: 0.1483  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.6254 (2.6125)  acc1: 38.8889 (42.5601)  acc5: 72.2222 (74.0091)  time: 0.3161  data: 0.1183  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.5565 (2.5971)  acc1: 38.8889 (42.7870)  acc5: 72.2222 (74.4322)  time: 0.2974  data: 0.0855  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.5565 (2.6369)  acc1: 33.3333 (41.4194)  acc5: 72.2222 (73.2112)  time: 0.2698  data: 0.0719  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.7852 (2.6421)  acc1: 27.7778 (40.9066)  acc5: 72.2222 (73.0514)  time: 0.2711  data: 0.0916  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.4947 (2.6461)  acc1: 33.3333 (40.8373)  acc5: 77.7778 (72.9595)  time: 0.3113  data: 0.1196  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.7689 (2.6426)  acc1: 38.8889 (41.0508)  acc5: 72.2222 (73.0518)  time: 0.3563  data: 0.1568  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.2399 (2.6267)  acc1: 55.5556 (41.8470)  acc5: 83.3333 (73.4488)  time: 0.3196  data: 0.1239  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1341 (2.6107)  acc1: 55.5556 (42.2314)  acc5: 88.8889 (73.9281)  time: 0.2944  data: 0.0981  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.5518 (2.6152)  acc1: 33.3333 (41.7220)  acc5: 77.7778 (73.9708)  time: 0.3099  data: 0.1153  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9553 (2.5726)  acc1: 66.6667 (43.2312)  acc5: 88.8889 (74.6701)  time: 0.2879  data: 0.1057  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.9553 (2.5694)  acc1: 61.1111 (43.2349)  acc5: 88.8889 (74.8462)  time: 0.2356  data: 0.0729  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.9553 (2.5735)  acc1: 61.1111 (43.2111)  acc5: 88.8889 (74.8106)  time: 0.2290  data: 0.0729  max mem: 15572
Val: Total time: 0:01:26 (0.3181 s / it)
* Acc@1 43.211 Acc@5 74.811 loss 2.574
Accuracy of the network on the 4883 val videos: 43.2%
Max accuracy: 43.35%
Epoch: [26]  [   0/2809]  eta: 7:14:35  lr: 0.000016  min_lr: 0.000000  loss: 4.2629 (4.2629)  class_acc: 0.1667 (0.1667)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.2829  data: 8.8350  max mem: 15572
Epoch: [26]  [  10/2809]  eta: 0:58:27  lr: 0.000016  min_lr: 0.000000  loss: 4.3267 (4.2631)  class_acc: 0.2083 (0.2348)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2531  data: 0.8044  max mem: 15572
Epoch: [26]  [  20/2809]  eta: 0:43:14  lr: 0.000016  min_lr: 0.000000  loss: 4.1835 (4.1440)  class_acc: 0.2500 (0.2540)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0680  max mem: 15572
Epoch: [26]  [  30/2809]  eta: 0:36:02  lr: 0.000016  min_lr: 0.000000  loss: 4.1765 (4.2110)  class_acc: 0.2917 (0.2782)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0815  max mem: 15572
Epoch: [26]  [  40/2809]  eta: 0:33:51  lr: 0.000016  min_lr: 0.000000  loss: 4.3384 (4.2440)  class_acc: 0.3750 (0.3008)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.1066  max mem: 15572
Epoch: [26]  [  50/2809]  eta: 0:31:57  lr: 0.000016  min_lr: 0.000000  loss: 4.2771 (4.2355)  class_acc: 0.2917 (0.2900)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1504  max mem: 15572
Epoch: [26]  [  60/2809]  eta: 0:30:36  lr: 0.000016  min_lr: 0.000000  loss: 4.2737 (4.2512)  class_acc: 0.2917 (0.2964)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.1063  max mem: 15572
Epoch: [26]  [  70/2809]  eta: 0:29:39  lr: 0.000016  min_lr: 0.000000  loss: 4.2641 (4.2537)  class_acc: 0.2917 (0.3028)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.1103  max mem: 15572
Epoch: [26]  [  80/2809]  eta: 0:29:05  lr: 0.000016  min_lr: 0.000000  loss: 4.2473 (4.2534)  class_acc: 0.2917 (0.2953)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1114  max mem: 15572
[2025-01-16 02:47:22,639] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:47:22,639] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [  90/2809]  eta: 0:28:48  lr: 0.000016  min_lr: 0.000000  loss: 4.2523 (4.2480)  class_acc: 0.2500 (0.2967)  loss_scale: 32768.0000 (33488.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1420  max mem: 15572
Epoch: [26]  [ 100/2809]  eta: 0:28:09  lr: 0.000016  min_lr: 0.000000  loss: 4.1954 (4.2334)  class_acc: 0.2917 (0.2979)  loss_scale: 65536.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1447  max mem: 15572
Epoch: [26]  [ 110/2809]  eta: 0:28:03  lr: 0.000016  min_lr: 0.000000  loss: 4.1844 (4.2361)  class_acc: 0.2500 (0.2950)  loss_scale: 65536.0000 (39262.5586)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1415  max mem: 15572
Epoch: [26]  [ 120/2809]  eta: 0:27:48  lr: 0.000016  min_lr: 0.000000  loss: 4.1967 (4.2359)  class_acc: 0.2917 (0.2975)  loss_scale: 65536.0000 (41433.9174)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1429  max mem: 15572
Epoch: [26]  [ 130/2809]  eta: 0:27:16  lr: 0.000016  min_lr: 0.000000  loss: 4.2686 (4.2337)  class_acc: 0.2917 (0.2939)  loss_scale: 65536.0000 (43273.7710)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0887  max mem: 15572
Epoch: [26]  [ 140/2809]  eta: 0:26:49  lr: 0.000016  min_lr: 0.000000  loss: 4.2194 (4.2256)  class_acc: 0.2500 (0.2973)  loss_scale: 65536.0000 (44852.6525)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0608  max mem: 15572
Epoch: [26]  [ 150/2809]  eta: 0:26:37  lr: 0.000016  min_lr: 0.000000  loss: 4.1141 (4.2257)  class_acc: 0.2917 (0.2942)  loss_scale: 65536.0000 (46222.4106)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0895  max mem: 15572
Epoch: [26]  [ 160/2809]  eta: 0:26:33  lr: 0.000016  min_lr: 0.000000  loss: 4.2125 (4.2268)  class_acc: 0.2083 (0.2917)  loss_scale: 65536.0000 (47422.0124)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1446  max mem: 15572
Epoch: [26]  [ 170/2809]  eta: 0:26:18  lr: 0.000016  min_lr: 0.000000  loss: 4.2250 (4.2303)  class_acc: 0.2917 (0.2912)  loss_scale: 65536.0000 (48481.3099)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1369  max mem: 15572
Epoch: [26]  [ 180/2809]  eta: 0:26:05  lr: 0.000016  min_lr: 0.000000  loss: 4.2412 (4.2322)  class_acc: 0.2917 (0.2919)  loss_scale: 65536.0000 (49423.5580)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0955  max mem: 15572
Epoch: [26]  [ 190/2809]  eta: 0:25:55  lr: 0.000016  min_lr: 0.000000  loss: 4.2217 (4.2237)  class_acc: 0.2917 (0.2943)  loss_scale: 65536.0000 (50267.1414)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1146  max mem: 15572
Epoch: [26]  [ 200/2809]  eta: 0:25:51  lr: 0.000016  min_lr: 0.000000  loss: 4.2059 (4.2199)  class_acc: 0.3333 (0.2942)  loss_scale: 65536.0000 (51026.7861)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1545  max mem: 15572
[2025-01-16 02:48:26,931] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73239
[2025-01-16 02:48:26,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:48:26,932] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 210/2809]  eta: 0:25:26  lr: 0.000016  min_lr: 0.000000  loss: 4.2453 (4.2159)  class_acc: 0.3333 (0.2970)  loss_scale: 65536.0000 (50782.6351)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0797  max mem: 15572
Epoch: [26]  [ 220/2809]  eta: 0:25:18  lr: 0.000016  min_lr: 0.000000  loss: 4.1932 (4.2158)  class_acc: 0.2917 (0.2958)  loss_scale: 32768.0000 (49967.4932)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0532  max mem: 15572
Epoch: [26]  [ 230/2809]  eta: 0:25:06  lr: 0.000016  min_lr: 0.000000  loss: 4.2070 (4.2185)  class_acc: 0.2917 (0.2992)  loss_scale: 32768.0000 (49222.9264)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1011  max mem: 15572
Epoch: [26]  [ 240/2809]  eta: 0:25:04  lr: 0.000016  min_lr: 0.000000  loss: 4.3471 (4.2250)  class_acc: 0.3333 (0.3010)  loss_scale: 32768.0000 (48540.1494)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1385  max mem: 15572
Epoch: [26]  [ 250/2809]  eta: 0:24:56  lr: 0.000016  min_lr: 0.000000  loss: 4.2592 (4.2227)  class_acc: 0.2917 (0.3010)  loss_scale: 32768.0000 (47911.7769)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1462  max mem: 15572
Epoch: [26]  [ 260/2809]  eta: 0:24:55  lr: 0.000016  min_lr: 0.000000  loss: 4.2459 (4.2252)  class_acc: 0.3333 (0.3025)  loss_scale: 32768.0000 (47331.5556)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1403  max mem: 15572
Epoch: [26]  [ 270/2809]  eta: 0:24:39  lr: 0.000016  min_lr: 0.000000  loss: 4.2750 (4.2295)  class_acc: 0.2917 (0.3015)  loss_scale: 32768.0000 (46794.1550)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1055  max mem: 15572
Epoch: [26]  [ 280/2809]  eta: 0:24:30  lr: 0.000016  min_lr: 0.000000  loss: 4.2750 (4.2304)  class_acc: 0.2917 (0.3029)  loss_scale: 32768.0000 (46295.0036)  weight_decay: 0.0500 (0.0500)  time: 0.5141  data: 0.0742  max mem: 15572
Epoch: [26]  [ 290/2809]  eta: 0:24:25  lr: 0.000016  min_lr: 0.000000  loss: 4.2652 (4.2325)  class_acc: 0.3333 (0.3024)  loss_scale: 32768.0000 (45830.1581)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1227  max mem: 15572
Epoch: [26]  [ 300/2809]  eta: 0:24:18  lr: 0.000016  min_lr: 0.000000  loss: 4.2004 (4.2294)  class_acc: 0.2917 (0.3026)  loss_scale: 32768.0000 (45396.1993)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1509  max mem: 15572
Epoch: [26]  [ 310/2809]  eta: 0:24:12  lr: 0.000016  min_lr: 0.000000  loss: 4.2004 (4.2290)  class_acc: 0.2917 (0.3025)  loss_scale: 32768.0000 (44990.1479)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1493  max mem: 15572
Epoch: [26]  [ 320/2809]  eta: 0:24:11  lr: 0.000016  min_lr: 0.000000  loss: 4.1979 (4.2291)  class_acc: 0.2917 (0.3032)  loss_scale: 32768.0000 (44609.3956)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1675  max mem: 15572
Epoch: [26]  [ 330/2809]  eta: 0:23:58  lr: 0.000016  min_lr: 0.000000  loss: 4.1964 (4.2283)  class_acc: 0.2917 (0.3046)  loss_scale: 32768.0000 (44251.6495)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1260  max mem: 15572
[2025-01-16 02:49:39,231] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:49:39,232] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 340/2809]  eta: 0:23:56  lr: 0.000016  min_lr: 0.000000  loss: 4.2225 (4.2288)  class_acc: 0.2917 (0.3043)  loss_scale: 32768.0000 (44587.5425)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1221  max mem: 15572
[2025-01-16 02:49:49,302] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73383
[2025-01-16 02:49:49,303] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:49:49,303] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 350/2809]  eta: 0:23:52  lr: 0.000016  min_lr: 0.000000  loss: 4.2150 (4.2259)  class_acc: 0.2917 (0.3058)  loss_scale: 65536.0000 (44997.6524)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1761  max mem: 15572
Epoch: [26]  [ 360/2809]  eta: 0:23:42  lr: 0.000016  min_lr: 0.000000  loss: 4.2779 (4.2281)  class_acc: 0.2917 (0.3066)  loss_scale: 32768.0000 (44658.8809)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1143  max mem: 15572
Epoch: [26]  [ 370/2809]  eta: 0:23:38  lr: 0.000016  min_lr: 0.000000  loss: 4.1875 (4.2259)  class_acc: 0.3750 (0.3078)  loss_scale: 32768.0000 (44338.3720)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1151  max mem: 15572
Epoch: [26]  [ 380/2809]  eta: 0:23:31  lr: 0.000016  min_lr: 0.000000  loss: 4.1715 (4.2243)  class_acc: 0.3333 (0.3084)  loss_scale: 32768.0000 (44034.6877)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1331  max mem: 15572
Epoch: [26]  [ 390/2809]  eta: 0:23:20  lr: 0.000016  min_lr: 0.000000  loss: 4.0891 (4.2228)  class_acc: 0.2917 (0.3090)  loss_scale: 32768.0000 (43746.5371)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0951  max mem: 15572
Epoch: [26]  [ 400/2809]  eta: 0:23:11  lr: 0.000016  min_lr: 0.000000  loss: 4.1932 (4.2254)  class_acc: 0.2917 (0.3082)  loss_scale: 32768.0000 (43472.7581)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0673  max mem: 15572
Epoch: [26]  [ 410/2809]  eta: 0:23:03  lr: 0.000016  min_lr: 0.000000  loss: 4.2705 (4.2231)  class_acc: 0.2917 (0.3091)  loss_scale: 32768.0000 (43212.3017)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0768  max mem: 15572
Epoch: [26]  [ 420/2809]  eta: 0:22:56  lr: 0.000016  min_lr: 0.000000  loss: 4.1456 (4.2229)  class_acc: 0.3333 (0.3098)  loss_scale: 32768.0000 (42964.2185)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1135  max mem: 15572
Epoch: [26]  [ 430/2809]  eta: 0:22:51  lr: 0.000016  min_lr: 0.000000  loss: 4.2962 (4.2232)  class_acc: 0.2917 (0.3098)  loss_scale: 32768.0000 (42727.6473)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1397  max mem: 15572
Epoch: [26]  [ 440/2809]  eta: 0:22:41  lr: 0.000016  min_lr: 0.000000  loss: 4.2962 (4.2226)  class_acc: 0.2917 (0.3110)  loss_scale: 32768.0000 (42501.8050)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.1029  max mem: 15572
Epoch: [26]  [ 450/2809]  eta: 0:22:30  lr: 0.000016  min_lr: 0.000000  loss: 4.2217 (4.2225)  class_acc: 0.3333 (0.3112)  loss_scale: 32768.0000 (42285.9778)  weight_decay: 0.0500 (0.0500)  time: 0.4791  data: 0.0242  max mem: 15572
Epoch: [26]  [ 460/2809]  eta: 0:22:23  lr: 0.000016  min_lr: 0.000000  loss: 4.3442 (4.2230)  class_acc: 0.3333 (0.3122)  loss_scale: 32768.0000 (42079.5141)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0507  max mem: 15572
Epoch: [26]  [ 470/2809]  eta: 0:22:15  lr: 0.000016  min_lr: 0.000000  loss: 4.3215 (4.2235)  class_acc: 0.2917 (0.3119)  loss_scale: 32768.0000 (41881.8174)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0985  max mem: 15572
[2025-01-16 02:50:58,500] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:50:58,501] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 480/2809]  eta: 0:22:07  lr: 0.000016  min_lr: 0.000000  loss: 4.2800 (4.2225)  class_acc: 0.2917 (0.3112)  loss_scale: 32768.0000 (41896.7152)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0680  max mem: 15572
Epoch: [26]  [ 490/2809]  eta: 0:22:03  lr: 0.000016  min_lr: 0.000000  loss: 4.2255 (4.2217)  class_acc: 0.2917 (0.3116)  loss_scale: 65536.0000 (42378.1670)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1050  max mem: 15572
Epoch: [26]  [ 500/2809]  eta: 0:21:58  lr: 0.000016  min_lr: 0.000000  loss: 4.1192 (4.2207)  class_acc: 0.2917 (0.3107)  loss_scale: 65536.0000 (42840.3992)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1516  max mem: 15572
Epoch: [26]  [ 510/2809]  eta: 0:21:50  lr: 0.000016  min_lr: 0.000000  loss: 4.1153 (4.2192)  class_acc: 0.2500 (0.3104)  loss_scale: 65536.0000 (43284.5401)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1132  max mem: 15572
Epoch: [26]  [ 520/2809]  eta: 0:21:44  lr: 0.000016  min_lr: 0.000000  loss: 4.2438 (4.2211)  class_acc: 0.2500 (0.3097)  loss_scale: 65536.0000 (43711.6315)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.1064  max mem: 15572
Epoch: [26]  [ 530/2809]  eta: 0:21:37  lr: 0.000016  min_lr: 0.000000  loss: 4.2440 (4.2220)  class_acc: 0.2500 (0.3099)  loss_scale: 65536.0000 (44122.6365)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1110  max mem: 15572
Epoch: [26]  [ 540/2809]  eta: 0:21:38  lr: 0.000016  min_lr: 0.000000  loss: 4.1816 (4.2230)  class_acc: 0.3333 (0.3107)  loss_scale: 65536.0000 (44518.4473)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.1948  max mem: 15572
Epoch: [26]  [ 550/2809]  eta: 0:21:28  lr: 0.000016  min_lr: 0.000000  loss: 4.1816 (4.2225)  class_acc: 0.2917 (0.3107)  loss_scale: 65536.0000 (44899.8911)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1764  max mem: 15572
Epoch: [26]  [ 560/2809]  eta: 0:21:24  lr: 0.000016  min_lr: 0.000000  loss: 4.2107 (4.2221)  class_acc: 0.2917 (0.3110)  loss_scale: 65536.0000 (45267.7362)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1242  max mem: 15572
[2025-01-16 02:51:49,582] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73600
[2025-01-16 02:51:49,582] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:51:49,582] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 570/2809]  eta: 0:21:18  lr: 0.000016  min_lr: 0.000000  loss: 4.3268 (4.2225)  class_acc: 0.3333 (0.3109)  loss_scale: 65536.0000 (45335.7618)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1452  max mem: 15572
Epoch: [26]  [ 580/2809]  eta: 0:21:08  lr: 0.000016  min_lr: 0.000000  loss: 4.2834 (4.2220)  class_acc: 0.3333 (0.3110)  loss_scale: 32768.0000 (45119.4492)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0542  max mem: 15572
Epoch: [26]  [ 590/2809]  eta: 0:21:02  lr: 0.000016  min_lr: 0.000000  loss: 4.2206 (4.2228)  class_acc: 0.2917 (0.3104)  loss_scale: 32768.0000 (44910.4569)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0579  max mem: 15572
Epoch: [26]  [ 600/2809]  eta: 0:20:56  lr: 0.000016  min_lr: 0.000000  loss: 4.2609 (4.2249)  class_acc: 0.2500 (0.3108)  loss_scale: 32768.0000 (44708.4193)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0979  max mem: 15572
Epoch: [26]  [ 610/2809]  eta: 0:20:52  lr: 0.000016  min_lr: 0.000000  loss: 4.2099 (4.2223)  class_acc: 0.3333 (0.3109)  loss_scale: 32768.0000 (44512.9951)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1298  max mem: 15572
Epoch: [26]  [ 620/2809]  eta: 0:20:45  lr: 0.000016  min_lr: 0.000000  loss: 4.1497 (4.2229)  class_acc: 0.3333 (0.3107)  loss_scale: 32768.0000 (44323.8647)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1414  max mem: 15572
Epoch: [26]  [ 630/2809]  eta: 0:20:39  lr: 0.000016  min_lr: 0.000000  loss: 4.0459 (4.2188)  class_acc: 0.3333 (0.3117)  loss_scale: 32768.0000 (44140.7290)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1133  max mem: 15572
Epoch: [26]  [ 640/2809]  eta: 0:20:34  lr: 0.000016  min_lr: 0.000000  loss: 4.1472 (4.2207)  class_acc: 0.3333 (0.3118)  loss_scale: 32768.0000 (43963.3073)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1416  max mem: 15572
Epoch: [26]  [ 650/2809]  eta: 0:20:27  lr: 0.000016  min_lr: 0.000000  loss: 4.2698 (4.2186)  class_acc: 0.2500 (0.3119)  loss_scale: 32768.0000 (43791.3364)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1208  max mem: 15572
Epoch: [26]  [ 660/2809]  eta: 0:20:22  lr: 0.000016  min_lr: 0.000000  loss: 4.1899 (4.2194)  class_acc: 0.2500 (0.3118)  loss_scale: 32768.0000 (43624.5688)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1115  max mem: 15572
Epoch: [26]  [ 670/2809]  eta: 0:20:18  lr: 0.000016  min_lr: 0.000000  loss: 4.1937 (4.2178)  class_acc: 0.2917 (0.3118)  loss_scale: 32768.0000 (43462.7720)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1501  max mem: 15572
Epoch: [26]  [ 680/2809]  eta: 0:20:10  lr: 0.000016  min_lr: 0.000000  loss: 4.0981 (4.2159)  class_acc: 0.2500 (0.3114)  loss_scale: 32768.0000 (43305.7269)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1037  max mem: 15572
Epoch: [26]  [ 690/2809]  eta: 0:20:07  lr: 0.000016  min_lr: 0.000000  loss: 4.0898 (4.2130)  class_acc: 0.2500 (0.3112)  loss_scale: 32768.0000 (43153.2272)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1283  max mem: 15572
[2025-01-16 02:53:02,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:53:02,405] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 700/2809]  eta: 0:20:03  lr: 0.000016  min_lr: 0.000000  loss: 4.0578 (4.2125)  class_acc: 0.2917 (0.3111)  loss_scale: 32768.0000 (43285.5464)  weight_decay: 0.0500 (0.0500)  time: 0.6381  data: 0.1997  max mem: 15572
Epoch: [26]  [ 710/2809]  eta: 0:19:57  lr: 0.000016  min_lr: 0.000000  loss: 4.0801 (4.2122)  class_acc: 0.2917 (0.3109)  loss_scale: 65536.0000 (43598.4923)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1582  max mem: 15572
Epoch: [26]  [ 720/2809]  eta: 0:19:52  lr: 0.000016  min_lr: 0.000000  loss: 4.1465 (4.2122)  class_acc: 0.3333 (0.3115)  loss_scale: 65536.0000 (43902.7573)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1462  max mem: 15572
Epoch: [26]  [ 730/2809]  eta: 0:19:45  lr: 0.000016  min_lr: 0.000000  loss: 4.2853 (4.2137)  class_acc: 0.2500 (0.3110)  loss_scale: 65536.0000 (44198.6977)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1151  max mem: 15572
Epoch: [26]  [ 740/2809]  eta: 0:19:38  lr: 0.000016  min_lr: 0.000000  loss: 4.2723 (4.2131)  class_acc: 0.2500 (0.3112)  loss_scale: 65536.0000 (44486.6505)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0815  max mem: 15572
[2025-01-16 02:53:30,678] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73778
[2025-01-16 02:53:30,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:53:30,678] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 750/2809]  eta: 0:19:32  lr: 0.000016  min_lr: 0.000000  loss: 4.1205 (4.2124)  class_acc: 0.3750 (0.3119)  loss_scale: 65536.0000 (44461.5073)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.1138  max mem: 15572
Epoch: [26]  [ 760/2809]  eta: 0:19:27  lr: 0.000016  min_lr: 0.000000  loss: 4.2549 (4.2128)  class_acc: 0.2083 (0.3110)  loss_scale: 32768.0000 (44307.8476)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1257  max mem: 15572
Epoch: [26]  [ 770/2809]  eta: 0:19:20  lr: 0.000016  min_lr: 0.000000  loss: 4.3098 (4.2143)  class_acc: 0.2083 (0.3109)  loss_scale: 32768.0000 (44158.1738)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0909  max mem: 15572
Epoch: [26]  [ 780/2809]  eta: 0:19:13  lr: 0.000016  min_lr: 0.000000  loss: 4.2283 (4.2128)  class_acc: 0.3333 (0.3116)  loss_scale: 32768.0000 (44012.3329)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0774  max mem: 15572
Epoch: [26]  [ 790/2809]  eta: 0:19:07  lr: 0.000016  min_lr: 0.000000  loss: 4.2427 (4.2144)  class_acc: 0.3750 (0.3113)  loss_scale: 32768.0000 (43870.1795)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0950  max mem: 15572
Epoch: [26]  [ 800/2809]  eta: 0:19:03  lr: 0.000016  min_lr: 0.000000  loss: 4.2532 (4.2130)  class_acc: 0.2917 (0.3114)  loss_scale: 32768.0000 (43731.5755)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1535  max mem: 15572
Epoch: [26]  [ 810/2809]  eta: 0:18:59  lr: 0.000016  min_lr: 0.000000  loss: 4.1754 (4.2132)  class_acc: 0.2917 (0.3107)  loss_scale: 32768.0000 (43596.3896)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1955  max mem: 15572
Epoch: [26]  [ 820/2809]  eta: 0:18:52  lr: 0.000016  min_lr: 0.000000  loss: 4.1754 (4.2131)  class_acc: 0.2917 (0.3111)  loss_scale: 32768.0000 (43464.4970)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1206  max mem: 15572
Epoch: [26]  [ 830/2809]  eta: 0:18:47  lr: 0.000016  min_lr: 0.000000  loss: 4.2284 (4.2130)  class_acc: 0.3333 (0.3106)  loss_scale: 32768.0000 (43335.7786)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1059  max mem: 15572
Epoch: [26]  [ 840/2809]  eta: 0:18:40  lr: 0.000016  min_lr: 0.000000  loss: 4.2715 (4.2133)  class_acc: 0.2917 (0.3108)  loss_scale: 32768.0000 (43210.1213)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1198  max mem: 15572
Epoch: [26]  [ 850/2809]  eta: 0:18:35  lr: 0.000016  min_lr: 0.000000  loss: 4.1285 (4.2119)  class_acc: 0.4167 (0.3119)  loss_scale: 32768.0000 (43087.4172)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1098  max mem: 15572
Epoch: [26]  [ 860/2809]  eta: 0:18:29  lr: 0.000016  min_lr: 0.000000  loss: 4.1548 (4.2127)  class_acc: 0.4167 (0.3120)  loss_scale: 32768.0000 (42967.5633)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1380  max mem: 15572
Epoch: [26]  [ 870/2809]  eta: 0:18:23  lr: 0.000016  min_lr: 0.000000  loss: 4.3749 (4.2133)  class_acc: 0.2500 (0.3121)  loss_scale: 32768.0000 (42850.4615)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1245  max mem: 15572
[2025-01-16 02:54:43,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:54:43,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 880/2809]  eta: 0:18:16  lr: 0.000016  min_lr: 0.000000  loss: 4.3549 (4.2140)  class_acc: 0.2500 (0.3117)  loss_scale: 32768.0000 (43033.5709)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0900  max mem: 15572
Epoch: [26]  [ 890/2809]  eta: 0:18:11  lr: 0.000016  min_lr: 0.000000  loss: 4.2173 (4.2134)  class_acc: 0.2917 (0.3116)  loss_scale: 65536.0000 (43286.1235)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1133  max mem: 15572
[2025-01-16 02:54:55,116] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73927
[2025-01-16 02:54:55,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:54:55,116] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 900/2809]  eta: 0:18:06  lr: 0.000016  min_lr: 0.000000  loss: 4.2085 (4.2132)  class_acc: 0.3333 (0.3123)  loss_scale: 65536.0000 (43242.1221)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1491  max mem: 15572
Epoch: [26]  [ 910/2809]  eta: 0:18:01  lr: 0.000016  min_lr: 0.000000  loss: 4.2532 (4.2141)  class_acc: 0.3333 (0.3122)  loss_scale: 32768.0000 (43127.1482)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1635  max mem: 15572
Epoch: [26]  [ 920/2809]  eta: 0:17:54  lr: 0.000016  min_lr: 0.000000  loss: 4.2532 (4.2149)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (43014.6710)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.1022  max mem: 15572
Epoch: [26]  [ 930/2809]  eta: 0:17:48  lr: 0.000016  min_lr: 0.000000  loss: 4.2397 (4.2153)  class_acc: 0.2917 (0.3119)  loss_scale: 32768.0000 (42904.6101)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0894  max mem: 15572
Epoch: [26]  [ 940/2809]  eta: 0:17:42  lr: 0.000016  min_lr: 0.000000  loss: 4.1651 (4.2154)  class_acc: 0.2500 (0.3116)  loss_scale: 32768.0000 (42796.8884)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1532  max mem: 15572
Epoch: [26]  [ 950/2809]  eta: 0:17:34  lr: 0.000016  min_lr: 0.000000  loss: 4.2171 (4.2147)  class_acc: 0.2917 (0.3120)  loss_scale: 32768.0000 (42691.4322)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0783  max mem: 15572
Epoch: [26]  [ 960/2809]  eta: 0:17:27  lr: 0.000016  min_lr: 0.000000  loss: 4.2575 (4.2155)  class_acc: 0.2917 (0.3112)  loss_scale: 32768.0000 (42588.1707)  weight_decay: 0.0500 (0.0500)  time: 0.4697  data: 0.0306  max mem: 15572
[2025-01-16 02:55:32,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=74000, skipped=460, lr=[1.505895959851408e-07, 1.505895959851408e-07, 2.151279942644869e-07, 2.151279942644869e-07, 3.073257060921242e-07, 3.073257060921242e-07, 4.390367229887489e-07, 4.390367229887489e-07, 6.271953185553555e-07, 6.271953185553555e-07, 8.959933122219366e-07, 8.959933122219366e-07, 1.279990446031338e-06, 1.279990446031338e-06, 1.8285577800447688e-06, 1.8285577800447688e-06, 2.612225400063955e-06, 2.612225400063955e-06, 3.7317505715199367e-06, 3.7317505715199367e-06, 5.331072245028481e-06, 5.331072245028481e-06, 7.615817492897831e-06, 7.615817492897831e-06, 1.087973927556833e-05, 1.087973927556833e-05, 1.554248467938333e-05, 1.554248467938333e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:55:32,935] [INFO] [timer.py:260:stop] epoch=0/micro_step=74000/global_step=74000, RunningAvgSamplesPerSec=27.90309576807866, CurrSamplesPerSec=31.074730644064907, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [26]  [ 970/2809]  eta: 0:17:20  lr: 0.000016  min_lr: 0.000000  loss: 4.2575 (4.2161)  class_acc: 0.2500 (0.3114)  loss_scale: 32768.0000 (42487.0360)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0710  max mem: 15572
Epoch: [26]  [ 980/2809]  eta: 0:17:15  lr: 0.000016  min_lr: 0.000000  loss: 4.2575 (4.2161)  class_acc: 0.2917 (0.3111)  loss_scale: 32768.0000 (42387.9633)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.1005  max mem: 15572
Epoch: [26]  [ 990/2809]  eta: 0:17:09  lr: 0.000016  min_lr: 0.000000  loss: 4.3291 (4.2174)  class_acc: 0.2917 (0.3108)  loss_scale: 32768.0000 (42290.8900)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1009  max mem: 15572
Epoch: [26]  [1000/2809]  eta: 0:17:04  lr: 0.000016  min_lr: 0.000000  loss: 4.2310 (4.2170)  class_acc: 0.2917 (0.3106)  loss_scale: 32768.0000 (42195.7562)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1150  max mem: 15572
Epoch: [26]  [1010/2809]  eta: 0:16:59  lr: 0.000016  min_lr: 0.000000  loss: 4.1487 (4.2163)  class_acc: 0.2917 (0.3104)  loss_scale: 32768.0000 (42102.5045)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1472  max mem: 15572
Epoch: [26]  [1020/2809]  eta: 0:16:54  lr: 0.000016  min_lr: 0.000000  loss: 4.1271 (4.2161)  class_acc: 0.2917 (0.3099)  loss_scale: 32768.0000 (42011.0793)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1602  max mem: 15572
[2025-01-16 02:56:06,825] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:56:06,826] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1030/2809]  eta: 0:16:49  lr: 0.000015  min_lr: 0.000000  loss: 4.2591 (4.2162)  class_acc: 0.2917 (0.3098)  loss_scale: 32768.0000 (42207.4724)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1754  max mem: 15572
Epoch: [26]  [1040/2809]  eta: 0:16:44  lr: 0.000015  min_lr: 0.000000  loss: 4.2472 (4.2167)  class_acc: 0.3333 (0.3096)  loss_scale: 65536.0000 (42431.5696)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1603  max mem: 15572
Epoch: [26]  [1050/2809]  eta: 0:16:38  lr: 0.000015  min_lr: 0.000000  loss: 4.2457 (4.2165)  class_acc: 0.3333 (0.3097)  loss_scale: 65536.0000 (42651.4025)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1162  max mem: 15572
Epoch: [26]  [1060/2809]  eta: 0:16:33  lr: 0.000015  min_lr: 0.000000  loss: 4.2786 (4.2181)  class_acc: 0.2500 (0.3091)  loss_scale: 65536.0000 (42867.0914)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1265  max mem: 15572
[2025-01-16 02:56:32,629] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74101
[2025-01-16 02:56:32,629] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:56:32,629] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1070/2809]  eta: 0:16:27  lr: 0.000015  min_lr: 0.000000  loss: 4.3813 (4.2184)  class_acc: 0.2083 (0.3086)  loss_scale: 65536.0000 (42956.3697)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1462  max mem: 15572
Epoch: [26]  [1080/2809]  eta: 0:16:22  lr: 0.000015  min_lr: 0.000000  loss: 4.2053 (4.2190)  class_acc: 0.2500 (0.3079)  loss_scale: 32768.0000 (42862.1203)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1205  max mem: 15572
Epoch: [26]  [1090/2809]  eta: 0:16:17  lr: 0.000015  min_lr: 0.000000  loss: 4.2497 (4.2199)  class_acc: 0.2917 (0.3076)  loss_scale: 32768.0000 (42769.5985)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1325  max mem: 15572
Epoch: [26]  [1100/2809]  eta: 0:16:10  lr: 0.000015  min_lr: 0.000000  loss: 4.2767 (4.2201)  class_acc: 0.2917 (0.3078)  loss_scale: 32768.0000 (42678.7575)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1341  max mem: 15572
Epoch: [26]  [1110/2809]  eta: 0:16:06  lr: 0.000015  min_lr: 0.000000  loss: 4.3086 (4.2207)  class_acc: 0.2917 (0.3076)  loss_scale: 32768.0000 (42589.5518)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1725  max mem: 15572
Epoch: [26]  [1120/2809]  eta: 0:16:00  lr: 0.000015  min_lr: 0.000000  loss: 4.2746 (4.2210)  class_acc: 0.2917 (0.3076)  loss_scale: 32768.0000 (42501.9376)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1408  max mem: 15572
Epoch: [26]  [1130/2809]  eta: 0:15:54  lr: 0.000015  min_lr: 0.000000  loss: 4.2419 (4.2214)  class_acc: 0.2917 (0.3074)  loss_scale: 32768.0000 (42415.8727)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0902  max mem: 15572
Epoch: [26]  [1140/2809]  eta: 0:15:48  lr: 0.000015  min_lr: 0.000000  loss: 4.1373 (4.2205)  class_acc: 0.2917 (0.3078)  loss_scale: 32768.0000 (42331.3164)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1170  max mem: 15572
Epoch: [26]  [1150/2809]  eta: 0:15:42  lr: 0.000015  min_lr: 0.000000  loss: 4.1564 (4.2210)  class_acc: 0.3333 (0.3078)  loss_scale: 32768.0000 (42248.2294)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0863  max mem: 15572
Epoch: [26]  [1160/2809]  eta: 0:15:36  lr: 0.000015  min_lr: 0.000000  loss: 4.2083 (4.2206)  class_acc: 0.3333 (0.3076)  loss_scale: 32768.0000 (42166.5736)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0845  max mem: 15572
Epoch: [26]  [1170/2809]  eta: 0:15:30  lr: 0.000015  min_lr: 0.000000  loss: 4.3162 (4.2216)  class_acc: 0.2917 (0.3075)  loss_scale: 32768.0000 (42086.3126)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0952  max mem: 15572
Epoch: [26]  [1180/2809]  eta: 0:15:24  lr: 0.000015  min_lr: 0.000000  loss: 4.2797 (4.2210)  class_acc: 0.2917 (0.3076)  loss_scale: 32768.0000 (42007.4107)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1116  max mem: 15572
Epoch: [26]  [1190/2809]  eta: 0:15:19  lr: 0.000015  min_lr: 0.000000  loss: 4.1224 (4.2197)  class_acc: 0.2917 (0.3077)  loss_scale: 32768.0000 (41929.8338)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1168  max mem: 15572
[2025-01-16 02:57:45,227] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:57:45,227] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1200/2809]  eta: 0:15:12  lr: 0.000015  min_lr: 0.000000  loss: 4.1335 (4.2185)  class_acc: 0.2917 (0.3081)  loss_scale: 32768.0000 (41989.9684)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0996  max mem: 15572
[2025-01-16 02:57:52,018] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74242
[2025-01-16 02:57:52,019] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:57:52,021] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1210/2809]  eta: 0:15:07  lr: 0.000015  min_lr: 0.000000  loss: 4.1335 (4.2170)  class_acc: 0.3750 (0.3088)  loss_scale: 65536.0000 (42103.2271)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1266  max mem: 15572
Epoch: [26]  [1220/2809]  eta: 0:15:01  lr: 0.000015  min_lr: 0.000000  loss: 4.2034 (4.2176)  class_acc: 0.3333 (0.3088)  loss_scale: 32768.0000 (42026.7715)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0952  max mem: 15572
Epoch: [26]  [1230/2809]  eta: 0:14:56  lr: 0.000015  min_lr: 0.000000  loss: 4.2238 (4.2179)  class_acc: 0.2917 (0.3084)  loss_scale: 32768.0000 (41951.5581)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1132  max mem: 15572
Epoch: [26]  [1240/2809]  eta: 0:14:50  lr: 0.000015  min_lr: 0.000000  loss: 4.3395 (4.2186)  class_acc: 0.2500 (0.3080)  loss_scale: 32768.0000 (41877.5568)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1564  max mem: 15572
Epoch: [26]  [1250/2809]  eta: 0:14:44  lr: 0.000015  min_lr: 0.000000  loss: 4.3588 (4.2190)  class_acc: 0.2500 (0.3073)  loss_scale: 32768.0000 (41804.7386)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1167  max mem: 15572
Epoch: [26]  [1260/2809]  eta: 0:14:39  lr: 0.000015  min_lr: 0.000000  loss: 4.4164 (4.2204)  class_acc: 0.2083 (0.3067)  loss_scale: 32768.0000 (41733.0753)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1366  max mem: 15572
Epoch: [26]  [1270/2809]  eta: 0:14:33  lr: 0.000015  min_lr: 0.000000  loss: 4.3858 (4.2212)  class_acc: 0.2500 (0.3067)  loss_scale: 32768.0000 (41662.5397)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1204  max mem: 15572
Epoch: [26]  [1280/2809]  eta: 0:14:26  lr: 0.000015  min_lr: 0.000000  loss: 4.3331 (4.2225)  class_acc: 0.2500 (0.3061)  loss_scale: 32768.0000 (41593.1054)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0638  max mem: 15572
Epoch: [26]  [1290/2809]  eta: 0:14:21  lr: 0.000015  min_lr: 0.000000  loss: 4.4730 (4.2237)  class_acc: 0.2083 (0.3057)  loss_scale: 32768.0000 (41524.7467)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1207  max mem: 15572
Epoch: [26]  [1300/2809]  eta: 0:14:14  lr: 0.000015  min_lr: 0.000000  loss: 4.3937 (4.2241)  class_acc: 0.2500 (0.3050)  loss_scale: 32768.0000 (41457.4389)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0984  max mem: 15572
Epoch: [26]  [1310/2809]  eta: 0:14:09  lr: 0.000015  min_lr: 0.000000  loss: 4.2597 (4.2236)  class_acc: 0.2500 (0.3053)  loss_scale: 32768.0000 (41391.1579)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.1172  max mem: 15572
Epoch: [26]  [1320/2809]  eta: 0:14:04  lr: 0.000015  min_lr: 0.000000  loss: 4.2667 (4.2249)  class_acc: 0.2500 (0.3047)  loss_scale: 32768.0000 (41325.8804)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1778  max mem: 15572
Epoch: [26]  [1330/2809]  eta: 0:13:58  lr: 0.000015  min_lr: 0.000000  loss: 4.2667 (4.2252)  class_acc: 0.2500 (0.3045)  loss_scale: 32768.0000 (41261.5838)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.0983  max mem: 15572
[2025-01-16 02:59:05,446] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:59:05,446] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1340/2809]  eta: 0:13:53  lr: 0.000015  min_lr: 0.000000  loss: 4.2600 (4.2259)  class_acc: 0.2917 (0.3050)  loss_scale: 32768.0000 (41295.9881)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1435  max mem: 15572
[2025-01-16 02:59:08,216] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74376
[2025-01-16 02:59:08,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:59:08,217] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1350/2809]  eta: 0:13:47  lr: 0.000015  min_lr: 0.000000  loss: 4.3524 (4.2269)  class_acc: 0.4167 (0.3051)  loss_scale: 32768.0000 (41257.1192)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1345  max mem: 15572
Epoch: [26]  [1360/2809]  eta: 0:13:41  lr: 0.000015  min_lr: 0.000000  loss: 4.4379 (4.2277)  class_acc: 0.2083 (0.3046)  loss_scale: 32768.0000 (41194.7450)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.1040  max mem: 15572
Epoch: [26]  [1370/2809]  eta: 0:13:35  lr: 0.000015  min_lr: 0.000000  loss: 4.2969 (4.2279)  class_acc: 0.2917 (0.3053)  loss_scale: 32768.0000 (41133.2808)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1253  max mem: 15572
Epoch: [26]  [1380/2809]  eta: 0:13:29  lr: 0.000015  min_lr: 0.000000  loss: 4.1993 (4.2276)  class_acc: 0.2917 (0.3051)  loss_scale: 32768.0000 (41072.7067)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0827  max mem: 15572
Epoch: [26]  [1390/2809]  eta: 0:13:24  lr: 0.000015  min_lr: 0.000000  loss: 4.1719 (4.2274)  class_acc: 0.2917 (0.3053)  loss_scale: 32768.0000 (41013.0036)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1117  max mem: 15572
Epoch: [26]  [1400/2809]  eta: 0:13:18  lr: 0.000015  min_lr: 0.000000  loss: 4.1719 (4.2268)  class_acc: 0.3333 (0.3052)  loss_scale: 32768.0000 (40954.1527)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1378  max mem: 15572
Epoch: [26]  [1410/2809]  eta: 0:13:12  lr: 0.000015  min_lr: 0.000000  loss: 4.2643 (4.2271)  class_acc: 0.2500 (0.3053)  loss_scale: 32768.0000 (40896.1361)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1126  max mem: 15572
Epoch: [26]  [1420/2809]  eta: 0:13:07  lr: 0.000015  min_lr: 0.000000  loss: 4.2720 (4.2272)  class_acc: 0.2500 (0.3050)  loss_scale: 32768.0000 (40838.9360)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1415  max mem: 15572
Epoch: [26]  [1430/2809]  eta: 0:13:01  lr: 0.000015  min_lr: 0.000000  loss: 4.2467 (4.2272)  class_acc: 0.2917 (0.3049)  loss_scale: 32768.0000 (40782.5353)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1351  max mem: 15572
Epoch: [26]  [1440/2809]  eta: 0:12:56  lr: 0.000015  min_lr: 0.000000  loss: 4.2393 (4.2278)  class_acc: 0.2917 (0.3048)  loss_scale: 32768.0000 (40726.9174)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1082  max mem: 15572
Epoch: [26]  [1450/2809]  eta: 0:12:50  lr: 0.000015  min_lr: 0.000000  loss: 4.2393 (4.2279)  class_acc: 0.2500 (0.3044)  loss_scale: 32768.0000 (40672.0662)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1164  max mem: 15572
Epoch: [26]  [1460/2809]  eta: 0:12:44  lr: 0.000015  min_lr: 0.000000  loss: 4.1840 (4.2275)  class_acc: 0.2917 (0.3046)  loss_scale: 32768.0000 (40617.9658)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1320  max mem: 15572
Epoch: [26]  [1470/2809]  eta: 0:12:38  lr: 0.000015  min_lr: 0.000000  loss: 4.1840 (4.2276)  class_acc: 0.3333 (0.3048)  loss_scale: 32768.0000 (40564.6010)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1271  max mem: 15572
[2025-01-16 03:00:20,068] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:00:20,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 03:00:23,693] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74512
[2025-01-16 03:00:23,693] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:00:23,693] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1480/2809]  eta: 0:12:32  lr: 0.000015  min_lr: 0.000000  loss: 4.2571 (4.2277)  class_acc: 0.3333 (0.3048)  loss_scale: 32768.0000 (40666.8359)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0829  max mem: 15572
Epoch: [26]  [1490/2809]  eta: 0:12:26  lr: 0.000015  min_lr: 0.000000  loss: 4.2571 (4.2283)  class_acc: 0.2917 (0.3049)  loss_scale: 32768.0000 (40613.8592)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0961  max mem: 15572
Epoch: [26]  [1500/2809]  eta: 0:12:22  lr: 0.000015  min_lr: 0.000000  loss: 4.2515 (4.2286)  class_acc: 0.2917 (0.3049)  loss_scale: 32768.0000 (40561.5883)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1386  max mem: 15572
Epoch: [26]  [1510/2809]  eta: 0:12:15  lr: 0.000015  min_lr: 0.000000  loss: 4.1639 (4.2274)  class_acc: 0.2917 (0.3052)  loss_scale: 32768.0000 (40510.0093)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1137  max mem: 15572
Epoch: [26]  [1520/2809]  eta: 0:12:10  lr: 0.000015  min_lr: 0.000000  loss: 4.0429 (4.2266)  class_acc: 0.2917 (0.3054)  loss_scale: 32768.0000 (40459.1085)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0931  max mem: 15572
Epoch: [26]  [1530/2809]  eta: 0:12:04  lr: 0.000015  min_lr: 0.000000  loss: 4.0791 (4.2262)  class_acc: 0.2917 (0.3054)  loss_scale: 32768.0000 (40408.8726)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1389  max mem: 15572
Epoch: [26]  [1540/2809]  eta: 0:11:59  lr: 0.000015  min_lr: 0.000000  loss: 4.2849 (4.2263)  class_acc: 0.2083 (0.3051)  loss_scale: 32768.0000 (40359.2888)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1646  max mem: 15572
Epoch: [26]  [1550/2809]  eta: 0:11:53  lr: 0.000015  min_lr: 0.000000  loss: 4.3193 (4.2259)  class_acc: 0.2500 (0.3050)  loss_scale: 32768.0000 (40310.3443)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1643  max mem: 15572
Epoch: [26]  [1560/2809]  eta: 0:11:48  lr: 0.000015  min_lr: 0.000000  loss: 4.3355 (4.2264)  class_acc: 0.2917 (0.3049)  loss_scale: 32768.0000 (40262.0269)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1554  max mem: 15572
Epoch: [26]  [1570/2809]  eta: 0:11:42  lr: 0.000015  min_lr: 0.000000  loss: 4.2544 (4.2260)  class_acc: 0.2500 (0.3048)  loss_scale: 32768.0000 (40214.3246)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1428  max mem: 15572
Epoch: [26]  [1580/2809]  eta: 0:11:37  lr: 0.000015  min_lr: 0.000000  loss: 4.2544 (4.2257)  class_acc: 0.2917 (0.3049)  loss_scale: 32768.0000 (40167.2258)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1369  max mem: 15572
Epoch: [26]  [1590/2809]  eta: 0:11:31  lr: 0.000015  min_lr: 0.000000  loss: 4.2405 (4.2255)  class_acc: 0.3333 (0.3053)  loss_scale: 32768.0000 (40120.7190)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1172  max mem: 15572
Epoch: [26]  [1600/2809]  eta: 0:11:26  lr: 0.000015  min_lr: 0.000000  loss: 4.2523 (4.2258)  class_acc: 0.3333 (0.3054)  loss_scale: 32768.0000 (40074.7933)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1147  max mem: 15572
[2025-01-16 03:01:39,975] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:01:39,976] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1610/2809]  eta: 0:11:21  lr: 0.000015  min_lr: 0.000000  loss: 4.3214 (4.2264)  class_acc: 0.2500 (0.3051)  loss_scale: 32768.0000 (40110.7983)  weight_decay: 0.0500 (0.0500)  time: 0.6513  data: 0.1850  max mem: 15572
Epoch: [26]  [1620/2809]  eta: 0:11:14  lr: 0.000015  min_lr: 0.000000  loss: 4.2553 (4.2269)  class_acc: 0.2500 (0.3045)  loss_scale: 65536.0000 (40267.6471)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1264  max mem: 15572
Epoch: [26]  [1630/2809]  eta: 0:11:09  lr: 0.000015  min_lr: 0.000000  loss: 4.2553 (4.2273)  class_acc: 0.2083 (0.3043)  loss_scale: 65536.0000 (40422.5727)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0871  max mem: 15572
Epoch: [26]  [1640/2809]  eta: 0:11:04  lr: 0.000015  min_lr: 0.000000  loss: 4.2369 (4.2272)  class_acc: 0.2917 (0.3044)  loss_scale: 65536.0000 (40575.6100)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1753  max mem: 15572
Epoch: [26]  [1650/2809]  eta: 0:10:58  lr: 0.000015  min_lr: 0.000000  loss: 4.2451 (4.2269)  class_acc: 0.2917 (0.3043)  loss_scale: 65536.0000 (40726.7935)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0961  max mem: 15572
[2025-01-16 03:02:07,218] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74690
[2025-01-16 03:02:07,218] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:02:07,219] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1660/2809]  eta: 0:10:52  lr: 0.000015  min_lr: 0.000000  loss: 4.2672 (4.2277)  class_acc: 0.2917 (0.3045)  loss_scale: 65536.0000 (40777.5172)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0558  max mem: 15572
Epoch: [26]  [1670/2809]  eta: 0:10:47  lr: 0.000015  min_lr: 0.000000  loss: 4.2615 (4.2273)  class_acc: 0.3333 (0.3047)  loss_scale: 32768.0000 (40729.5847)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1609  max mem: 15572
Epoch: [26]  [1680/2809]  eta: 0:10:41  lr: 0.000015  min_lr: 0.000000  loss: 4.2694 (4.2276)  class_acc: 0.2917 (0.3045)  loss_scale: 32768.0000 (40682.2225)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1571  max mem: 15572
Epoch: [26]  [1690/2809]  eta: 0:10:35  lr: 0.000015  min_lr: 0.000000  loss: 4.1919 (4.2276)  class_acc: 0.2500 (0.3043)  loss_scale: 32768.0000 (40635.4205)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0900  max mem: 15572
Epoch: [26]  [1700/2809]  eta: 0:10:30  lr: 0.000015  min_lr: 0.000000  loss: 4.2250 (4.2280)  class_acc: 0.2917 (0.3044)  loss_scale: 32768.0000 (40589.1687)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0978  max mem: 15572
Epoch: [26]  [1710/2809]  eta: 0:10:24  lr: 0.000015  min_lr: 0.000000  loss: 4.2676 (4.2280)  class_acc: 0.2917 (0.3043)  loss_scale: 32768.0000 (40543.4576)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1074  max mem: 15572
Epoch: [26]  [1720/2809]  eta: 0:10:18  lr: 0.000015  min_lr: 0.000000  loss: 4.2342 (4.2281)  class_acc: 0.2500 (0.3041)  loss_scale: 32768.0000 (40498.2777)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0948  max mem: 15572
Epoch: [26]  [1730/2809]  eta: 0:10:12  lr: 0.000015  min_lr: 0.000000  loss: 4.2771 (4.2289)  class_acc: 0.2500 (0.3037)  loss_scale: 32768.0000 (40453.6199)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.1163  max mem: 15572
Epoch: [26]  [1740/2809]  eta: 0:10:06  lr: 0.000015  min_lr: 0.000000  loss: 4.2303 (4.2279)  class_acc: 0.2500 (0.3040)  loss_scale: 32768.0000 (40409.4750)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0701  max mem: 15572
Epoch: [26]  [1750/2809]  eta: 0:10:00  lr: 0.000015  min_lr: 0.000000  loss: 4.1971 (4.2282)  class_acc: 0.2500 (0.3040)  loss_scale: 32768.0000 (40365.8344)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0240  max mem: 15572
Epoch: [26]  [1760/2809]  eta: 0:09:55  lr: 0.000015  min_lr: 0.000000  loss: 4.3136 (4.2289)  class_acc: 0.2917 (0.3040)  loss_scale: 32768.0000 (40322.6894)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1172  max mem: 15572
Epoch: [26]  [1770/2809]  eta: 0:09:49  lr: 0.000015  min_lr: 0.000000  loss: 4.3326 (4.2299)  class_acc: 0.2917 (0.3037)  loss_scale: 32768.0000 (40280.0316)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1492  max mem: 15572
Epoch: [26]  [1780/2809]  eta: 0:09:43  lr: 0.000015  min_lr: 0.000000  loss: 4.3689 (4.2303)  class_acc: 0.2917 (0.3038)  loss_scale: 32768.0000 (40237.8529)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1092  max mem: 15572
[2025-01-16 03:03:19,058] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:03:19,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 03:03:21,225] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74822
[2025-01-16 03:03:21,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:03:21,226] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1790/2809]  eta: 0:09:38  lr: 0.000015  min_lr: 0.000000  loss: 4.1968 (4.2297)  class_acc: 0.2917 (0.3039)  loss_scale: 32768.0000 (40251.0329)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1296  max mem: 15572
Epoch: [26]  [1800/2809]  eta: 0:09:32  lr: 0.000015  min_lr: 0.000000  loss: 4.1820 (4.2292)  class_acc: 0.2917 (0.3038)  loss_scale: 32768.0000 (40209.4836)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1308  max mem: 15572
Epoch: [26]  [1810/2809]  eta: 0:09:26  lr: 0.000015  min_lr: 0.000000  loss: 4.2180 (4.2295)  class_acc: 0.2917 (0.3037)  loss_scale: 32768.0000 (40168.3932)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1269  max mem: 15572
Epoch: [26]  [1820/2809]  eta: 0:09:21  lr: 0.000015  min_lr: 0.000000  loss: 4.2211 (4.2298)  class_acc: 0.2500 (0.3035)  loss_scale: 32768.0000 (40127.7540)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1633  max mem: 15572
Epoch: [26]  [1830/2809]  eta: 0:09:15  lr: 0.000015  min_lr: 0.000000  loss: 4.2464 (4.2300)  class_acc: 0.2500 (0.3034)  loss_scale: 32768.0000 (40087.5587)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1524  max mem: 15572
Epoch: [26]  [1840/2809]  eta: 0:09:09  lr: 0.000015  min_lr: 0.000000  loss: 4.2027 (4.2292)  class_acc: 0.2500 (0.3033)  loss_scale: 32768.0000 (40047.8001)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0886  max mem: 15572
Epoch: [26]  [1850/2809]  eta: 0:09:03  lr: 0.000015  min_lr: 0.000000  loss: 4.1964 (4.2295)  class_acc: 0.3333 (0.3037)  loss_scale: 32768.0000 (40008.4711)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0277  max mem: 15572
Epoch: [26]  [1860/2809]  eta: 0:08:58  lr: 0.000015  min_lr: 0.000000  loss: 4.4187 (4.2311)  class_acc: 0.2917 (0.3035)  loss_scale: 32768.0000 (39969.5648)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0697  max mem: 15572
Epoch: [26]  [1870/2809]  eta: 0:08:52  lr: 0.000015  min_lr: 0.000000  loss: 4.3656 (4.2308)  class_acc: 0.2500 (0.3033)  loss_scale: 32768.0000 (39931.0743)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0972  max mem: 15572
Epoch: [26]  [1880/2809]  eta: 0:08:46  lr: 0.000015  min_lr: 0.000000  loss: 4.3654 (4.2317)  class_acc: 0.2500 (0.3029)  loss_scale: 32768.0000 (39892.9931)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1013  max mem: 15572
Epoch: [26]  [1890/2809]  eta: 0:08:41  lr: 0.000015  min_lr: 0.000000  loss: 4.2954 (4.2313)  class_acc: 0.2083 (0.3026)  loss_scale: 32768.0000 (39855.3146)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1427  max mem: 15572
Epoch: [26]  [1900/2809]  eta: 0:08:35  lr: 0.000015  min_lr: 0.000000  loss: 4.2151 (4.2316)  class_acc: 0.2500 (0.3025)  loss_scale: 32768.0000 (39818.0326)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1431  max mem: 15572
Epoch: [26]  [1910/2809]  eta: 0:08:29  lr: 0.000015  min_lr: 0.000000  loss: 4.2759 (4.2318)  class_acc: 0.2917 (0.3025)  loss_scale: 32768.0000 (39781.1408)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0970  max mem: 15572
[2025-01-16 03:04:33,595] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:04:33,595] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1920/2809]  eta: 0:08:24  lr: 0.000015  min_lr: 0.000000  loss: 4.2867 (4.2322)  class_acc: 0.2917 (0.3023)  loss_scale: 32768.0000 (39812.8641)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1036  max mem: 15572
Epoch: [26]  [1930/2809]  eta: 0:08:18  lr: 0.000015  min_lr: 0.000000  loss: 4.1326 (4.2315)  class_acc: 0.2917 (0.3022)  loss_scale: 65536.0000 (39946.0756)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0925  max mem: 15572
Epoch: [26]  [1940/2809]  eta: 0:08:12  lr: 0.000015  min_lr: 0.000000  loss: 4.1326 (4.2316)  class_acc: 0.3333 (0.3025)  loss_scale: 65536.0000 (40077.9145)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0698  max mem: 15572
Epoch: [26]  [1950/2809]  eta: 0:08:06  lr: 0.000015  min_lr: 0.000000  loss: 4.1959 (4.2316)  class_acc: 0.3333 (0.3022)  loss_scale: 65536.0000 (40208.4018)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0613  max mem: 15572
Epoch: [26]  [1960/2809]  eta: 0:08:00  lr: 0.000015  min_lr: 0.000000  loss: 4.2938 (4.2315)  class_acc: 0.2500 (0.3020)  loss_scale: 65536.0000 (40337.5584)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.0930  max mem: 15572
[2025-01-16 03:04:58,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=75000, skipped=466, lr=[1.4380800353752938e-07, 1.4380800353752938e-07, 2.0544000505361344e-07, 2.0544000505361344e-07, 2.934857215051621e-07, 2.934857215051621e-07, 4.1926531643594586e-07, 4.1926531643594586e-07, 5.989504520513512e-07, 5.989504520513512e-07, 8.556435029305019e-07, 8.556435029305019e-07, 1.2223478613292883e-06, 1.2223478613292883e-06, 1.7462112304704122e-06, 1.7462112304704122e-06, 2.4945874721005886e-06, 2.4945874721005886e-06, 3.5636963887151274e-06, 3.5636963887151274e-06, 5.090994841021611e-06, 5.090994841021611e-06, 7.2728497728880156e-06, 7.2728497728880156e-06, 1.0389785389840023e-05, 1.0389785389840023e-05, 1.484255055691432e-05, 1.484255055691432e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:04:58,747] [INFO] [timer.py:260:stop] epoch=0/micro_step=75000/global_step=75000, RunningAvgSamplesPerSec=27.909209695715163, CurrSamplesPerSec=29.26707820270088, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [26]  [1970/2809]  eta: 0:07:55  lr: 0.000015  min_lr: 0.000000  loss: 4.1939 (4.2309)  class_acc: 0.3333 (0.3024)  loss_scale: 65536.0000 (40465.4044)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1611  max mem: 15572
[2025-01-16 03:05:03,569] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75008
[2025-01-16 03:05:03,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:05:03,570] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1980/2809]  eta: 0:07:49  lr: 0.000015  min_lr: 0.000000  loss: 4.1166 (4.2304)  class_acc: 0.3750 (0.3025)  loss_scale: 65536.0000 (40476.1716)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1173  max mem: 15572
Epoch: [26]  [1990/2809]  eta: 0:07:43  lr: 0.000015  min_lr: 0.000000  loss: 4.1477 (4.2307)  class_acc: 0.2917 (0.3024)  loss_scale: 32768.0000 (40437.4566)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0778  max mem: 15572
Epoch: [26]  [2000/2809]  eta: 0:07:38  lr: 0.000015  min_lr: 0.000000  loss: 4.3152 (4.2308)  class_acc: 0.2917 (0.3023)  loss_scale: 32768.0000 (40399.1284)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1177  max mem: 15572
Epoch: [26]  [2010/2809]  eta: 0:07:32  lr: 0.000015  min_lr: 0.000000  loss: 4.3152 (4.2308)  class_acc: 0.2917 (0.3020)  loss_scale: 32768.0000 (40361.1815)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1347  max mem: 15572
Epoch: [26]  [2020/2809]  eta: 0:07:26  lr: 0.000015  min_lr: 0.000000  loss: 4.1786 (4.2303)  class_acc: 0.2917 (0.3018)  loss_scale: 32768.0000 (40323.6101)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1301  max mem: 15572
Epoch: [26]  [2030/2809]  eta: 0:07:20  lr: 0.000015  min_lr: 0.000000  loss: 4.1744 (4.2304)  class_acc: 0.2917 (0.3017)  loss_scale: 32768.0000 (40286.4087)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.1053  max mem: 15572
Epoch: [26]  [2040/2809]  eta: 0:07:15  lr: 0.000015  min_lr: 0.000000  loss: 4.2630 (4.2302)  class_acc: 0.2917 (0.3018)  loss_scale: 32768.0000 (40249.5718)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0751  max mem: 15572
Epoch: [26]  [2050/2809]  eta: 0:07:09  lr: 0.000015  min_lr: 0.000000  loss: 4.1479 (4.2298)  class_acc: 0.2917 (0.3017)  loss_scale: 32768.0000 (40213.0941)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0971  max mem: 15572
Epoch: [26]  [2060/2809]  eta: 0:07:03  lr: 0.000015  min_lr: 0.000000  loss: 4.1763 (4.2300)  class_acc: 0.2917 (0.3016)  loss_scale: 32768.0000 (40176.9704)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0828  max mem: 15572
Epoch: [26]  [2070/2809]  eta: 0:06:57  lr: 0.000015  min_lr: 0.000000  loss: 4.2813 (4.2303)  class_acc: 0.3333 (0.3017)  loss_scale: 32768.0000 (40141.1956)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0992  max mem: 15572
Epoch: [26]  [2080/2809]  eta: 0:06:52  lr: 0.000015  min_lr: 0.000000  loss: 4.2813 (4.2306)  class_acc: 0.3333 (0.3019)  loss_scale: 32768.0000 (40105.7645)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1321  max mem: 15572
Epoch: [26]  [2090/2809]  eta: 0:06:46  lr: 0.000015  min_lr: 0.000000  loss: 4.2171 (4.2306)  class_acc: 0.2917 (0.3016)  loss_scale: 32768.0000 (40070.6724)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1142  max mem: 15572
Epoch: [26]  [2100/2809]  eta: 0:06:41  lr: 0.000015  min_lr: 0.000000  loss: 4.2372 (4.2305)  class_acc: 0.2917 (0.3019)  loss_scale: 32768.0000 (40035.9143)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1407  max mem: 15572
[2025-01-16 03:06:17,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:06:17,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [2110/2809]  eta: 0:06:35  lr: 0.000015  min_lr: 0.000000  loss: 4.2828 (4.2299)  class_acc: 0.3333 (0.3023)  loss_scale: 32768.0000 (40125.6656)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1653  max mem: 15572
Epoch: [26]  [2120/2809]  eta: 0:06:30  lr: 0.000015  min_lr: 0.000000  loss: 4.2542 (4.2298)  class_acc: 0.3333 (0.3023)  loss_scale: 65536.0000 (40245.4691)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1574  max mem: 15572
Epoch: [26]  [2130/2809]  eta: 0:06:24  lr: 0.000015  min_lr: 0.000000  loss: 4.1721 (4.2301)  class_acc: 0.2917 (0.3022)  loss_scale: 65536.0000 (40364.1483)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1614  max mem: 15572
[2025-01-16 03:06:36,987] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75171
[2025-01-16 03:06:36,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:06:36,987] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [2140/2809]  eta: 0:06:18  lr: 0.000015  min_lr: 0.000000  loss: 4.1721 (4.2301)  class_acc: 0.2917 (0.3020)  loss_scale: 65536.0000 (40420.4988)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1282  max mem: 15572
Epoch: [26]  [2150/2809]  eta: 0:06:13  lr: 0.000015  min_lr: 0.000000  loss: 4.2459 (4.2303)  class_acc: 0.2917 (0.3020)  loss_scale: 32768.0000 (40384.9224)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1332  max mem: 15572
Epoch: [26]  [2160/2809]  eta: 0:06:07  lr: 0.000015  min_lr: 0.000000  loss: 4.2325 (4.2302)  class_acc: 0.2917 (0.3020)  loss_scale: 32768.0000 (40349.6752)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1177  max mem: 15572
Epoch: [26]  [2170/2809]  eta: 0:06:01  lr: 0.000015  min_lr: 0.000000  loss: 4.2252 (4.2297)  class_acc: 0.2917 (0.3020)  loss_scale: 32768.0000 (40314.7526)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0842  max mem: 15572
Epoch: [26]  [2180/2809]  eta: 0:05:56  lr: 0.000015  min_lr: 0.000000  loss: 4.2713 (4.2301)  class_acc: 0.3333 (0.3021)  loss_scale: 32768.0000 (40280.1504)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1180  max mem: 15572
Epoch: [26]  [2190/2809]  eta: 0:05:50  lr: 0.000015  min_lr: 0.000000  loss: 4.3965 (4.2308)  class_acc: 0.2917 (0.3017)  loss_scale: 32768.0000 (40245.8640)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1170  max mem: 15572
Epoch: [26]  [2200/2809]  eta: 0:05:45  lr: 0.000015  min_lr: 0.000000  loss: 4.3967 (4.2316)  class_acc: 0.2083 (0.3015)  loss_scale: 32768.0000 (40211.8891)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1130  max mem: 15572
Epoch: [26]  [2210/2809]  eta: 0:05:39  lr: 0.000015  min_lr: 0.000000  loss: 4.3236 (4.2317)  class_acc: 0.2083 (0.3015)  loss_scale: 32768.0000 (40178.2216)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1318  max mem: 15572
Epoch: [26]  [2220/2809]  eta: 0:05:33  lr: 0.000015  min_lr: 0.000000  loss: 4.2081 (4.2316)  class_acc: 0.2917 (0.3017)  loss_scale: 32768.0000 (40144.8573)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0754  max mem: 15572
Epoch: [26]  [2230/2809]  eta: 0:05:27  lr: 0.000015  min_lr: 0.000000  loss: 4.2000 (4.2311)  class_acc: 0.3333 (0.3020)  loss_scale: 32768.0000 (40111.7920)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0706  max mem: 15572
Epoch: [26]  [2240/2809]  eta: 0:05:22  lr: 0.000015  min_lr: 0.000000  loss: 4.1719 (4.2315)  class_acc: 0.3333 (0.3019)  loss_scale: 32768.0000 (40079.0219)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1518  max mem: 15572
Epoch: [26]  [2250/2809]  eta: 0:05:16  lr: 0.000015  min_lr: 0.000000  loss: 4.2793 (4.2316)  class_acc: 0.3333 (0.3018)  loss_scale: 32768.0000 (40046.5429)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1442  max mem: 15572
Epoch: [26]  [2260/2809]  eta: 0:05:11  lr: 0.000015  min_lr: 0.000000  loss: 4.2966 (4.2315)  class_acc: 0.3333 (0.3019)  loss_scale: 32768.0000 (40014.3512)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1379  max mem: 15572
[2025-01-16 03:07:51,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:07:51,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [2270/2809]  eta: 0:05:05  lr: 0.000015  min_lr: 0.000000  loss: 4.2594 (4.2315)  class_acc: 0.2083 (0.3015)  loss_scale: 32768.0000 (40054.5874)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1536  max mem: 15572
[2025-01-16 03:07:56,831] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75312
[2025-01-16 03:07:56,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:07:56,831] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [2280/2809]  eta: 0:04:59  lr: 0.000015  min_lr: 0.000000  loss: 4.1900 (4.2315)  class_acc: 0.2500 (0.3018)  loss_scale: 65536.0000 (40123.2021)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0664  max mem: 15572
Epoch: [26]  [2290/2809]  eta: 0:04:53  lr: 0.000015  min_lr: 0.000000  loss: 4.3234 (4.2319)  class_acc: 0.2500 (0.3015)  loss_scale: 32768.0000 (40091.0973)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0540  max mem: 15572
Epoch: [26]  [2300/2809]  eta: 0:04:48  lr: 0.000015  min_lr: 0.000000  loss: 4.3067 (4.2317)  class_acc: 0.2917 (0.3018)  loss_scale: 32768.0000 (40059.2716)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1576  max mem: 15572
Epoch: [26]  [2310/2809]  eta: 0:04:42  lr: 0.000015  min_lr: 0.000000  loss: 4.2394 (4.2313)  class_acc: 0.3333 (0.3018)  loss_scale: 32768.0000 (40027.7213)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1327  max mem: 15572
Epoch: [26]  [2320/2809]  eta: 0:04:36  lr: 0.000015  min_lr: 0.000000  loss: 4.0697 (4.2311)  class_acc: 0.2917 (0.3022)  loss_scale: 32768.0000 (39996.4429)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0869  max mem: 15572
Epoch: [26]  [2330/2809]  eta: 0:04:31  lr: 0.000015  min_lr: 0.000000  loss: 4.0697 (4.2307)  class_acc: 0.2917 (0.3021)  loss_scale: 32768.0000 (39965.4329)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1615  max mem: 15572
Epoch: [26]  [2340/2809]  eta: 0:04:26  lr: 0.000015  min_lr: 0.000000  loss: 4.2125 (4.2300)  class_acc: 0.2917 (0.3022)  loss_scale: 32768.0000 (39934.6877)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.2164  max mem: 15572
Epoch: [26]  [2350/2809]  eta: 0:04:20  lr: 0.000015  min_lr: 0.000000  loss: 4.2222 (4.2301)  class_acc: 0.3333 (0.3024)  loss_scale: 32768.0000 (39904.2042)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1181  max mem: 15572
Epoch: [26]  [2360/2809]  eta: 0:04:14  lr: 0.000015  min_lr: 0.000000  loss: 4.2269 (4.2297)  class_acc: 0.2917 (0.3023)  loss_scale: 32768.0000 (39873.9788)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0435  max mem: 15572
Epoch: [26]  [2370/2809]  eta: 0:04:08  lr: 0.000015  min_lr: 0.000000  loss: 4.1782 (4.2297)  class_acc: 0.2917 (0.3022)  loss_scale: 32768.0000 (39844.0084)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0781  max mem: 15572
Epoch: [26]  [2380/2809]  eta: 0:04:03  lr: 0.000015  min_lr: 0.000000  loss: 4.1137 (4.2295)  class_acc: 0.2500 (0.3022)  loss_scale: 32768.0000 (39814.2898)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.0926  max mem: 15572
Epoch: [26]  [2390/2809]  eta: 0:03:57  lr: 0.000015  min_lr: 0.000000  loss: 4.1137 (4.2294)  class_acc: 0.2500 (0.3021)  loss_scale: 32768.0000 (39784.8197)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0811  max mem: 15572
Epoch: [26]  [2400/2809]  eta: 0:03:51  lr: 0.000015  min_lr: 0.000000  loss: 4.1002 (4.2289)  class_acc: 0.2917 (0.3023)  loss_scale: 32768.0000 (39755.5952)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1259  max mem: 15572
[2025-01-16 03:09:11,631] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:09:11,631] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [2410/2809]  eta: 0:03:46  lr: 0.000015  min_lr: 0.000000  loss: 4.1452 (4.2291)  class_acc: 0.3333 (0.3024)  loss_scale: 32768.0000 (39780.9772)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1187  max mem: 15572
[2025-01-16 03:09:14,609] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75447
[2025-01-16 03:09:14,610] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:09:14,610] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [2420/2809]  eta: 0:03:40  lr: 0.000015  min_lr: 0.000000  loss: 4.2840 (4.2291)  class_acc: 0.3333 (0.3025)  loss_scale: 32768.0000 (39779.0797)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1153  max mem: 15572
Epoch: [26]  [2430/2809]  eta: 0:03:34  lr: 0.000015  min_lr: 0.000000  loss: 4.3041 (4.2294)  class_acc: 0.2500 (0.3022)  loss_scale: 32768.0000 (39750.2394)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0949  max mem: 15572
Epoch: [26]  [2440/2809]  eta: 0:03:29  lr: 0.000015  min_lr: 0.000000  loss: 4.3012 (4.2295)  class_acc: 0.2500 (0.3021)  loss_scale: 32768.0000 (39721.6354)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0915  max mem: 15572
Epoch: [26]  [2450/2809]  eta: 0:03:23  lr: 0.000015  min_lr: 0.000000  loss: 4.1880 (4.2292)  class_acc: 0.2917 (0.3020)  loss_scale: 32768.0000 (39693.2648)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0915  max mem: 15572
Epoch: [26]  [2460/2809]  eta: 0:03:17  lr: 0.000014  min_lr: 0.000000  loss: 4.4152 (4.2296)  class_acc: 0.2917 (0.3018)  loss_scale: 32768.0000 (39665.1247)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0013  max mem: 15572
Epoch: [26]  [2470/2809]  eta: 0:03:12  lr: 0.000014  min_lr: 0.000000  loss: 4.3591 (4.2291)  class_acc: 0.2500 (0.3018)  loss_scale: 32768.0000 (39637.2125)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0592  max mem: 15572
Epoch: [26]  [2480/2809]  eta: 0:03:06  lr: 0.000014  min_lr: 0.000000  loss: 4.1911 (4.2292)  class_acc: 0.2500 (0.3017)  loss_scale: 32768.0000 (39609.5252)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1162  max mem: 15572
Epoch: [26]  [2490/2809]  eta: 0:03:00  lr: 0.000014  min_lr: 0.000000  loss: 4.2929 (4.2301)  class_acc: 0.2500 (0.3016)  loss_scale: 32768.0000 (39582.0602)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0986  max mem: 15572
Epoch: [26]  [2500/2809]  eta: 0:02:55  lr: 0.000014  min_lr: 0.000000  loss: 4.2929 (4.2300)  class_acc: 0.2500 (0.3014)  loss_scale: 32768.0000 (39554.8149)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1120  max mem: 15572
Epoch: [26]  [2510/2809]  eta: 0:02:49  lr: 0.000014  min_lr: 0.000000  loss: 4.1668 (4.2293)  class_acc: 0.2500 (0.3015)  loss_scale: 32768.0000 (39527.7865)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1395  max mem: 15572
Epoch: [26]  [2520/2809]  eta: 0:02:44  lr: 0.000014  min_lr: 0.000000  loss: 3.9717 (4.2285)  class_acc: 0.2917 (0.3014)  loss_scale: 32768.0000 (39500.9726)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1438  max mem: 15572
[2025-01-16 03:10:17,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75555
[2025-01-16 03:10:17,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 03:10:17,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [26]  [2530/2809]  eta: 0:02:38  lr: 0.000014  min_lr: 0.000000  loss: 4.2583 (4.2287)  class_acc: 0.2500 (0.3011)  loss_scale: 16384.0000 (39409.6373)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1390  max mem: 15572
Epoch: [26]  [2540/2809]  eta: 0:02:32  lr: 0.000014  min_lr: 0.000000  loss: 4.2932 (4.2292)  class_acc: 0.2500 (0.3012)  loss_scale: 16384.0000 (39319.0209)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1518  max mem: 15572
Epoch: [26]  [2550/2809]  eta: 0:02:27  lr: 0.000014  min_lr: 0.000000  loss: 4.3352 (4.2298)  class_acc: 0.3333 (0.3011)  loss_scale: 16384.0000 (39229.1149)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1340  max mem: 15572
Epoch: [26]  [2560/2809]  eta: 0:02:21  lr: 0.000014  min_lr: 0.000000  loss: 4.4532 (4.2305)  class_acc: 0.2500 (0.3011)  loss_scale: 16384.0000 (39139.9110)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1159  max mem: 15572
Epoch: [26]  [2570/2809]  eta: 0:02:15  lr: 0.000014  min_lr: 0.000000  loss: 4.3032 (4.2303)  class_acc: 0.2917 (0.3009)  loss_scale: 16384.0000 (39051.4010)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1354  max mem: 15572
Epoch: [26]  [2580/2809]  eta: 0:02:10  lr: 0.000014  min_lr: 0.000000  loss: 4.2022 (4.2307)  class_acc: 0.2500 (0.3009)  loss_scale: 16384.0000 (38963.5769)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1266  max mem: 15572
Epoch: [26]  [2590/2809]  eta: 0:02:04  lr: 0.000014  min_lr: 0.000000  loss: 4.3957 (4.2315)  class_acc: 0.2500 (0.3010)  loss_scale: 16384.0000 (38876.4307)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1730  max mem: 15572
Epoch: [26]  [2600/2809]  eta: 0:01:58  lr: 0.000014  min_lr: 0.000000  loss: 4.3288 (4.2313)  class_acc: 0.3333 (0.3012)  loss_scale: 16384.0000 (38789.9546)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1247  max mem: 15572
Epoch: [26]  [2610/2809]  eta: 0:01:53  lr: 0.000014  min_lr: 0.000000  loss: 4.2166 (4.2314)  class_acc: 0.3333 (0.3013)  loss_scale: 16384.0000 (38704.1409)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0403  max mem: 15572
Epoch: [26]  [2620/2809]  eta: 0:01:47  lr: 0.000014  min_lr: 0.000000  loss: 4.2468 (4.2316)  class_acc: 0.3333 (0.3013)  loss_scale: 16384.0000 (38618.9821)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0275  max mem: 15572
Epoch: [26]  [2630/2809]  eta: 0:01:41  lr: 0.000014  min_lr: 0.000000  loss: 4.2410 (4.2315)  class_acc: 0.2917 (0.3013)  loss_scale: 16384.0000 (38534.4705)  weight_decay: 0.0500 (0.0500)  time: 0.4752  data: 0.0008  max mem: 15572
Epoch: [26]  [2640/2809]  eta: 0:01:35  lr: 0.000014  min_lr: 0.000000  loss: 4.2502 (4.2322)  class_acc: 0.2500 (0.3013)  loss_scale: 16384.0000 (38450.5990)  weight_decay: 0.0500 (0.0500)  time: 0.4525  data: 0.0007  max mem: 15572
[2025-01-16 03:11:27,990] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:11:27,990] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [2650/2809]  eta: 0:01:30  lr: 0.000014  min_lr: 0.000000  loss: 4.2578 (4.2320)  class_acc: 0.2917 (0.3014)  loss_scale: 16384.0000 (38373.5406)  weight_decay: 0.0500 (0.0500)  time: 0.4556  data: 0.0005  max mem: 15572
Epoch: [26]  [2660/2809]  eta: 0:01:24  lr: 0.000014  min_lr: 0.000000  loss: 4.0883 (4.2316)  class_acc: 0.3333 (0.3015)  loss_scale: 32768.0000 (38352.4750)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0008  max mem: 15572
Epoch: [26]  [2670/2809]  eta: 0:01:18  lr: 0.000014  min_lr: 0.000000  loss: 4.1064 (4.2319)  class_acc: 0.2917 (0.3013)  loss_scale: 32768.0000 (38331.5672)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0385  max mem: 15572
Epoch: [26]  [2680/2809]  eta: 0:01:13  lr: 0.000014  min_lr: 0.000000  loss: 4.2704 (4.2322)  class_acc: 0.2917 (0.3013)  loss_scale: 32768.0000 (38310.8154)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1240  max mem: 15572
Epoch: [26]  [2690/2809]  eta: 0:01:07  lr: 0.000014  min_lr: 0.000000  loss: 4.2879 (4.2325)  class_acc: 0.2917 (0.3013)  loss_scale: 32768.0000 (38290.2178)  weight_decay: 0.0500 (0.0500)  time: 0.6784  data: 0.1789  max mem: 15572
Epoch: [26]  [2700/2809]  eta: 0:01:01  lr: 0.000014  min_lr: 0.000000  loss: 4.2464 (4.2323)  class_acc: 0.2917 (0.3014)  loss_scale: 32768.0000 (38269.7727)  weight_decay: 0.0500 (0.0500)  time: 0.6965  data: 0.2026  max mem: 15572
Epoch: [26]  [2710/2809]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000000  loss: 4.2787 (4.2333)  class_acc: 0.3333 (0.3013)  loss_scale: 32768.0000 (38249.4784)  weight_decay: 0.0500 (0.0500)  time: 0.7027  data: 0.2201  max mem: 15572
Epoch: [26]  [2720/2809]  eta: 0:00:50  lr: 0.000014  min_lr: 0.000000  loss: 4.2570 (4.2329)  class_acc: 0.3333 (0.3014)  loss_scale: 32768.0000 (38229.3333)  weight_decay: 0.0500 (0.0500)  time: 0.7098  data: 0.2102  max mem: 15572
Epoch: [26]  [2730/2809]  eta: 0:00:44  lr: 0.000014  min_lr: 0.000000  loss: 4.1656 (4.2330)  class_acc: 0.2917 (0.3014)  loss_scale: 32768.0000 (38209.3358)  weight_decay: 0.0500 (0.0500)  time: 0.7407  data: 0.2418  max mem: 15572
Epoch: [26]  [2740/2809]  eta: 0:00:39  lr: 0.000014  min_lr: 0.000000  loss: 4.2751 (4.2332)  class_acc: 0.3333 (0.3015)  loss_scale: 32768.0000 (38189.4841)  weight_decay: 0.0500 (0.0500)  time: 0.6928  data: 0.1916  max mem: 15572
Epoch: [26]  [2750/2809]  eta: 0:00:33  lr: 0.000014  min_lr: 0.000000  loss: 4.2902 (4.2334)  class_acc: 0.3333 (0.3014)  loss_scale: 32768.0000 (38169.7768)  weight_decay: 0.0500 (0.0500)  time: 0.6741  data: 0.1753  max mem: 15572
Epoch: [26]  [2760/2809]  eta: 0:00:27  lr: 0.000014  min_lr: 0.000000  loss: 4.2764 (4.2330)  class_acc: 0.2500 (0.3014)  loss_scale: 32768.0000 (38150.2122)  weight_decay: 0.0500 (0.0500)  time: 0.7009  data: 0.2068  max mem: 15572
Epoch: [26]  [2770/2809]  eta: 0:00:22  lr: 0.000014  min_lr: 0.000000  loss: 4.3054 (4.2331)  class_acc: 0.2500 (0.3012)  loss_scale: 32768.0000 (38130.7889)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1339  max mem: 15572
[2025-01-16 03:12:50,458] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:12:50,458] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [2780/2809]  eta: 0:00:16  lr: 0.000014  min_lr: 0.000000  loss: 4.2613 (4.2325)  class_acc: 0.2500 (0.3011)  loss_scale: 32768.0000 (38146.8536)  weight_decay: 0.0500 (0.0500)  time: 0.4691  data: 0.0529  max mem: 15572
[2025-01-16 03:12:55,247] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75824
[2025-01-16 03:12:55,247] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:12:55,247] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [2790/2809]  eta: 0:00:10  lr: 0.000014  min_lr: 0.000000  loss: 4.0654 (4.2321)  class_acc: 0.2917 (0.3013)  loss_scale: 65536.0000 (38233.2469)  weight_decay: 0.0500 (0.0500)  time: 0.3946  data: 0.0005  max mem: 15572
Epoch: [26]  [2800/2809]  eta: 0:00:05  lr: 0.000014  min_lr: 0.000000  loss: 4.1316 (4.2322)  class_acc: 0.3750 (0.3015)  loss_scale: 32768.0000 (38213.7351)  weight_decay: 0.0500 (0.0500)  time: 0.4007  data: 0.0005  max mem: 15572
Epoch: [26]  [2808/2809]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 4.2186 (4.2321)  class_acc: 0.2917 (0.3016)  loss_scale: 32768.0000 (38198.2257)  weight_decay: 0.0500 (0.0500)  time: 0.3906  data: 0.0004  max mem: 15572
Epoch: [26] Total time: 0:26:37 (0.5687 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 4.2186 (4.2321)  class_acc: 0.2917 (0.3016)  loss_scale: 32768.0000 (38198.2257)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:16:03  loss: 1.2410 (1.2410)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 3.5431  data: 3.3495  max mem: 15572
Val:  [ 10/272]  eta: 0:03:05  loss: 2.6944 (2.6297)  acc1: 44.4444 (40.9091)  acc5: 72.2222 (71.2121)  time: 0.7066  data: 0.5040  max mem: 15572
Val:  [ 20/272]  eta: 0:02:02  loss: 2.6941 (2.6563)  acc1: 44.4444 (42.5926)  acc5: 72.2222 (72.2222)  time: 0.3333  data: 0.1353  max mem: 15572
Val:  [ 30/272]  eta: 0:01:40  loss: 2.6941 (2.7062)  acc1: 38.8889 (39.0681)  acc5: 72.2222 (72.0430)  time: 0.2578  data: 0.0719  max mem: 15572
Val:  [ 40/272]  eta: 0:01:32  loss: 2.6890 (2.7015)  acc1: 27.7778 (36.4499)  acc5: 72.2222 (72.3577)  time: 0.3055  data: 0.1246  max mem: 15572
Val:  [ 50/272]  eta: 0:01:27  loss: 2.5379 (2.6237)  acc1: 33.3333 (38.8889)  acc5: 77.7778 (74.2919)  time: 0.3631  data: 0.1769  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 1.9107 (2.5496)  acc1: 55.5556 (41.1658)  acc5: 88.8889 (75.6831)  time: 0.3374  data: 0.1438  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 1.9107 (2.4787)  acc1: 61.1111 (44.2097)  acc5: 88.8889 (76.9171)  time: 0.2845  data: 0.0911  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 2.1126 (2.4912)  acc1: 50.0000 (44.5816)  acc5: 77.7778 (76.4060)  time: 0.2979  data: 0.1049  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 2.4455 (2.4930)  acc1: 44.4444 (44.3834)  acc5: 77.7778 (76.8010)  time: 0.2917  data: 0.0942  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 2.4907 (2.5201)  acc1: 38.8889 (43.6194)  acc5: 77.7778 (76.2376)  time: 0.3090  data: 0.1209  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.7132 (2.5744)  acc1: 33.3333 (42.4925)  acc5: 66.6667 (74.9249)  time: 0.3132  data: 0.1264  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 2.9856 (2.6029)  acc1: 27.7778 (41.7815)  acc5: 61.1111 (74.4720)  time: 0.2857  data: 0.0903  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 2.4349 (2.5732)  acc1: 50.0000 (42.9177)  acc5: 83.3333 (75.3605)  time: 0.3037  data: 0.1052  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 2.1560 (2.5718)  acc1: 55.5556 (43.4594)  acc5: 83.3333 (75.3743)  time: 0.3101  data: 0.1141  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 2.5132 (2.5707)  acc1: 38.8889 (43.0464)  acc5: 72.2222 (75.4967)  time: 0.3103  data: 0.1116  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.5081 (2.5657)  acc1: 44.4444 (43.6163)  acc5: 77.7778 (75.7764)  time: 0.3165  data: 0.1144  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.5774 (2.5788)  acc1: 44.4444 (43.1449)  acc5: 77.7778 (75.3411)  time: 0.3239  data: 0.1394  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.5774 (2.5675)  acc1: 33.3333 (43.1553)  acc5: 72.2222 (75.6599)  time: 0.3193  data: 0.1302  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.6443 (2.6086)  acc1: 33.3333 (42.0593)  acc5: 77.7778 (74.4619)  time: 0.3071  data: 0.1041  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.6872 (2.6167)  acc1: 22.2222 (41.3212)  acc5: 72.2222 (74.2123)  time: 0.3112  data: 0.1097  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.6493 (2.6253)  acc1: 22.2222 (41.0216)  acc5: 77.7778 (74.0653)  time: 0.3077  data: 0.1075  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.7657 (2.6222)  acc1: 38.8889 (41.2519)  acc5: 72.2222 (74.0573)  time: 0.3086  data: 0.1054  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.2507 (2.6120)  acc1: 55.5556 (42.0154)  acc5: 77.7778 (74.1462)  time: 0.3320  data: 0.1274  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.2808 (2.6020)  acc1: 50.0000 (42.1162)  acc5: 83.3333 (74.4122)  time: 0.3359  data: 0.1471  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4689 (2.6089)  acc1: 33.3333 (41.5007)  acc5: 77.7778 (74.4356)  time: 0.3286  data: 0.1508  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.0038 (2.5667)  acc1: 66.6667 (43.0183)  acc5: 83.3333 (75.0958)  time: 0.2967  data: 0.1118  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.0038 (2.5654)  acc1: 61.1111 (43.0914)  acc5: 83.3333 (75.2563)  time: 0.2286  data: 0.0589  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.0038 (2.5692)  acc1: 55.5556 (43.0883)  acc5: 83.3333 (75.2202)  time: 0.2196  data: 0.0587  max mem: 15572
Val: Total time: 0:01:27 (0.3205 s / it)
* Acc@1 43.088 Acc@5 75.220 loss 2.569
Accuracy of the network on the 4883 val videos: 43.1%
Max accuracy: 43.35%
Epoch: [27]  [   0/2809]  eta: 5:22:10  lr: 0.000014  min_lr: 0.000000  loss: 4.0379 (4.0379)  class_acc: 0.5000 (0.5000)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.8815  data: 6.4236  max mem: 15572
Epoch: [27]  [  10/2809]  eta: 0:55:15  lr: 0.000014  min_lr: 0.000000  loss: 4.2443 (4.2501)  class_acc: 0.3750 (0.3561)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1846  data: 0.7284  max mem: 15572
Epoch: [27]  [  20/2809]  eta: 0:41:12  lr: 0.000014  min_lr: 0.000000  loss: 4.2450 (4.2627)  class_acc: 0.3333 (0.3333)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1508  max mem: 15572
Epoch: [27]  [  30/2809]  eta: 0:37:18  lr: 0.000014  min_lr: 0.000000  loss: 4.2785 (4.2484)  class_acc: 0.3333 (0.3387)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1624  max mem: 15572
Epoch: [27]  [  40/2809]  eta: 0:33:55  lr: 0.000014  min_lr: 0.000000  loss: 4.3404 (4.2527)  class_acc: 0.3333 (0.3343)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1286  max mem: 15572
Epoch: [27]  [  50/2809]  eta: 0:32:42  lr: 0.000014  min_lr: 0.000000  loss: 4.2363 (4.2404)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1221  max mem: 15572
Epoch: [27]  [  60/2809]  eta: 0:31:33  lr: 0.000014  min_lr: 0.000000  loss: 4.1336 (4.2296)  class_acc: 0.2500 (0.3169)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1541  max mem: 15572
Epoch: [27]  [  70/2809]  eta: 0:31:07  lr: 0.000014  min_lr: 0.000000  loss: 4.1249 (4.2252)  class_acc: 0.2500 (0.3198)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1619  max mem: 15572
Epoch: [27]  [  80/2809]  eta: 0:30:26  lr: 0.000014  min_lr: 0.000000  loss: 4.1968 (4.2308)  class_acc: 0.2917 (0.3148)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1474  max mem: 15572
Epoch: [27]  [  90/2809]  eta: 0:29:21  lr: 0.000014  min_lr: 0.000000  loss: 4.1824 (4.2161)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0599  max mem: 15572
Epoch: [27]  [ 100/2809]  eta: 0:28:49  lr: 0.000014  min_lr: 0.000000  loss: 4.2483 (4.2228)  class_acc: 0.2917 (0.3164)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0477  max mem: 15572
[2025-01-16 03:15:40,102] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:15:40,103] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [ 110/2809]  eta: 0:28:31  lr: 0.000014  min_lr: 0.000000  loss: 4.2483 (4.2091)  class_acc: 0.3333 (0.3187)  loss_scale: 32768.0000 (33063.2072)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1135  max mem: 15572
Epoch: [27]  [ 120/2809]  eta: 0:28:15  lr: 0.000014  min_lr: 0.000000  loss: 4.0593 (4.1984)  class_acc: 0.3750 (0.3240)  loss_scale: 65536.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1387  max mem: 15572
Epoch: [27]  [ 130/2809]  eta: 0:27:59  lr: 0.000014  min_lr: 0.000000  loss: 4.2184 (4.2081)  class_acc: 0.3750 (0.3232)  loss_scale: 65536.0000 (38020.8855)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1382  max mem: 15572
Epoch: [27]  [ 140/2809]  eta: 0:27:47  lr: 0.000014  min_lr: 0.000000  loss: 4.1510 (4.2002)  class_acc: 0.3333 (0.3224)  loss_scale: 65536.0000 (39972.3121)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1462  max mem: 15572
Epoch: [27]  [ 150/2809]  eta: 0:27:29  lr: 0.000014  min_lr: 0.000000  loss: 4.0100 (4.1829)  class_acc: 0.3333 (0.3242)  loss_scale: 65536.0000 (41665.2715)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1242  max mem: 15572
[2025-01-16 03:16:07,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=76000, skipped=472, lr=[1.3711166685044318e-07, 1.3711166685044318e-07, 1.9587380978634742e-07, 1.9587380978634742e-07, 2.7981972826621064e-07, 2.7981972826621064e-07, 3.997424689517295e-07, 3.997424689517295e-07, 5.710606699310422e-07, 5.710606699310422e-07, 8.15800957044346e-07, 8.15800957044346e-07, 1.1654299386347801e-06, 1.1654299386347801e-06, 1.6648999123354003e-06, 1.6648999123354003e-06, 2.378428446193429e-06, 2.378428446193429e-06, 3.3977549231334703e-06, 3.3977549231334703e-06, 4.853935604476386e-06, 4.853935604476386e-06, 6.9341937206805526e-06, 6.9341937206805526e-06, 9.905991029543647e-06, 9.905991029543647e-06, 1.4151415756490925e-05, 1.4151415756490925e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:16:07,065] [INFO] [timer.py:260:stop] epoch=0/micro_step=76000/global_step=76000, RunningAvgSamplesPerSec=27.904429138406755, CurrSamplesPerSec=31.1495419935673, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [27]  [ 160/2809]  eta: 0:27:09  lr: 0.000014  min_lr: 0.000000  loss: 4.2031 (4.1903)  class_acc: 0.2917 (0.3199)  loss_scale: 65536.0000 (43147.9255)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1004  max mem: 15572
[2025-01-16 03:16:10,401] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76004
[2025-01-16 03:16:10,402] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:16:10,402] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [ 170/2809]  eta: 0:26:53  lr: 0.000014  min_lr: 0.000000  loss: 4.2312 (4.1852)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (42540.9123)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.1143  max mem: 15572
Epoch: [27]  [ 180/2809]  eta: 0:26:42  lr: 0.000014  min_lr: 0.000000  loss: 4.2234 (4.1855)  class_acc: 0.2917 (0.3218)  loss_scale: 32768.0000 (42000.9724)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1342  max mem: 15572
Epoch: [27]  [ 190/2809]  eta: 0:26:17  lr: 0.000014  min_lr: 0.000000  loss: 4.1681 (4.1848)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (41517.5707)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0847  max mem: 15572
Epoch: [27]  [ 200/2809]  eta: 0:26:04  lr: 0.000014  min_lr: 0.000000  loss: 4.1681 (4.1889)  class_acc: 0.2917 (0.3230)  loss_scale: 32768.0000 (41082.2687)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0656  max mem: 15572
Epoch: [27]  [ 210/2809]  eta: 0:25:54  lr: 0.000014  min_lr: 0.000000  loss: 4.1745 (4.1882)  class_acc: 0.3333 (0.3280)  loss_scale: 32768.0000 (40688.2275)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1171  max mem: 15572
Epoch: [27]  [ 220/2809]  eta: 0:25:35  lr: 0.000014  min_lr: 0.000000  loss: 4.1469 (4.1889)  class_acc: 0.3333 (0.3273)  loss_scale: 32768.0000 (40329.8462)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0911  max mem: 15572
Epoch: [27]  [ 230/2809]  eta: 0:25:22  lr: 0.000014  min_lr: 0.000000  loss: 4.1632 (4.1897)  class_acc: 0.3333 (0.3252)  loss_scale: 32768.0000 (40002.4935)  weight_decay: 0.0500 (0.0500)  time: 0.5083  data: 0.0629  max mem: 15572
Epoch: [27]  [ 240/2809]  eta: 0:25:15  lr: 0.000014  min_lr: 0.000000  loss: 4.2355 (4.1951)  class_acc: 0.3333 (0.3242)  loss_scale: 32768.0000 (39702.3071)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0937  max mem: 15572
Epoch: [27]  [ 250/2809]  eta: 0:25:05  lr: 0.000014  min_lr: 0.000000  loss: 4.3743 (4.2017)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (39426.0398)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1160  max mem: 15572
Epoch: [27]  [ 260/2809]  eta: 0:25:10  lr: 0.000014  min_lr: 0.000000  loss: 4.3479 (4.2029)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (39170.9425)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1758  max mem: 15572
Epoch: [27]  [ 270/2809]  eta: 0:24:52  lr: 0.000014  min_lr: 0.000000  loss: 4.2770 (4.2068)  class_acc: 0.2917 (0.3203)  loss_scale: 32768.0000 (38934.6716)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1300  max mem: 15572
Epoch: [27]  [ 280/2809]  eta: 0:24:50  lr: 0.000014  min_lr: 0.000000  loss: 4.1984 (4.2073)  class_acc: 0.2917 (0.3195)  loss_scale: 32768.0000 (38715.2171)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1048  max mem: 15572
[2025-01-16 03:17:20,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:17:20,591] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [ 290/2809]  eta: 0:24:38  lr: 0.000014  min_lr: 0.000000  loss: 4.1941 (4.2063)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (38623.4502)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1251  max mem: 15572
Epoch: [27]  [ 300/2809]  eta: 0:24:32  lr: 0.000014  min_lr: 0.000000  loss: 4.2222 (4.2049)  class_acc: 0.3333 (0.3210)  loss_scale: 65536.0000 (39517.5548)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0968  max mem: 15572
Epoch: [27]  [ 310/2809]  eta: 0:24:32  lr: 0.000014  min_lr: 0.000000  loss: 4.2215 (4.2049)  class_acc: 0.3333 (0.3213)  loss_scale: 65536.0000 (40354.1608)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1713  max mem: 15572
[2025-01-16 03:17:34,731] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76156
[2025-01-16 03:17:34,732] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:17:34,732] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [ 320/2809]  eta: 0:24:18  lr: 0.000014  min_lr: 0.000000  loss: 4.1457 (4.2001)  class_acc: 0.3333 (0.3217)  loss_scale: 65536.0000 (40321.9938)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1315  max mem: 15572
Epoch: [27]  [ 330/2809]  eta: 0:24:09  lr: 0.000014  min_lr: 0.000000  loss: 4.1263 (4.2012)  class_acc: 0.3333 (0.3204)  loss_scale: 32768.0000 (40093.7764)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0818  max mem: 15572
Epoch: [27]  [ 340/2809]  eta: 0:24:00  lr: 0.000014  min_lr: 0.000000  loss: 4.3138 (4.2046)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (39878.9443)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.1072  max mem: 15572
Epoch: [27]  [ 350/2809]  eta: 0:24:00  lr: 0.000014  min_lr: 0.000000  loss: 4.2836 (4.2059)  class_acc: 0.3333 (0.3193)  loss_scale: 32768.0000 (39676.3533)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1463  max mem: 15572
Epoch: [27]  [ 360/2809]  eta: 0:23:55  lr: 0.000014  min_lr: 0.000000  loss: 4.2758 (4.2069)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (39484.9861)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1682  max mem: 15572
Epoch: [27]  [ 370/2809]  eta: 0:23:48  lr: 0.000014  min_lr: 0.000000  loss: 4.2297 (4.2059)  class_acc: 0.3333 (0.3209)  loss_scale: 32768.0000 (39303.9353)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1322  max mem: 15572
Epoch: [27]  [ 380/2809]  eta: 0:23:48  lr: 0.000014  min_lr: 0.000000  loss: 4.3416 (4.2105)  class_acc: 0.2917 (0.3187)  loss_scale: 32768.0000 (39132.3885)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1654  max mem: 15572
Epoch: [27]  [ 390/2809]  eta: 0:23:41  lr: 0.000014  min_lr: 0.000000  loss: 4.3579 (4.2079)  class_acc: 0.2500 (0.3188)  loss_scale: 32768.0000 (38969.6164)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1677  max mem: 15572
Epoch: [27]  [ 400/2809]  eta: 0:23:31  lr: 0.000014  min_lr: 0.000000  loss: 4.1392 (4.2055)  class_acc: 0.3750 (0.3196)  loss_scale: 32768.0000 (38814.9626)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0944  max mem: 15572
Epoch: [27]  [ 410/2809]  eta: 0:23:26  lr: 0.000014  min_lr: 0.000000  loss: 4.1459 (4.2054)  class_acc: 0.3333 (0.3180)  loss_scale: 32768.0000 (38667.8345)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0925  max mem: 15572
Epoch: [27]  [ 420/2809]  eta: 0:23:21  lr: 0.000014  min_lr: 0.000000  loss: 4.1083 (4.2038)  class_acc: 0.2500 (0.3177)  loss_scale: 32768.0000 (38527.6960)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1273  max mem: 15572
Epoch: [27]  [ 430/2809]  eta: 0:23:10  lr: 0.000014  min_lr: 0.000000  loss: 4.0164 (4.2006)  class_acc: 0.2917 (0.3160)  loss_scale: 32768.0000 (38394.0603)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0916  max mem: 15572
Epoch: [27]  [ 440/2809]  eta: 0:23:01  lr: 0.000014  min_lr: 0.000000  loss: 4.2272 (4.2036)  class_acc: 0.2083 (0.3144)  loss_scale: 32768.0000 (38266.4853)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0616  max mem: 15572
[2025-01-16 03:18:50,002] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:18:50,002] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 03:18:50,819] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76287
[2025-01-16 03:18:50,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:18:50,819] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [ 450/2809]  eta: 0:22:59  lr: 0.000014  min_lr: 0.000000  loss: 4.2672 (4.2030)  class_acc: 0.2500 (0.3136)  loss_scale: 32768.0000 (38289.8803)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1398  max mem: 15572
Epoch: [27]  [ 460/2809]  eta: 0:22:52  lr: 0.000014  min_lr: 0.000000  loss: 4.2466 (4.2032)  class_acc: 0.2917 (0.3130)  loss_scale: 32768.0000 (38170.0998)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1366  max mem: 15572
Epoch: [27]  [ 470/2809]  eta: 0:22:41  lr: 0.000014  min_lr: 0.000000  loss: 4.1324 (4.2023)  class_acc: 0.2917 (0.3139)  loss_scale: 32768.0000 (38055.4055)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0485  max mem: 15572
Epoch: [27]  [ 480/2809]  eta: 0:22:35  lr: 0.000014  min_lr: 0.000000  loss: 4.2190 (4.2019)  class_acc: 0.3750 (0.3144)  loss_scale: 32768.0000 (37945.4802)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0797  max mem: 15572
Epoch: [27]  [ 490/2809]  eta: 0:22:27  lr: 0.000014  min_lr: 0.000000  loss: 4.2190 (4.2004)  class_acc: 0.2500 (0.3137)  loss_scale: 32768.0000 (37840.0326)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0982  max mem: 15572
Epoch: [27]  [ 500/2809]  eta: 0:22:20  lr: 0.000014  min_lr: 0.000000  loss: 4.2734 (4.2034)  class_acc: 0.2917 (0.3160)  loss_scale: 32768.0000 (37738.7944)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0983  max mem: 15572
Epoch: [27]  [ 510/2809]  eta: 0:22:14  lr: 0.000014  min_lr: 0.000000  loss: 4.2734 (4.2039)  class_acc: 0.3750 (0.3161)  loss_scale: 32768.0000 (37641.5186)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1334  max mem: 15572
Epoch: [27]  [ 520/2809]  eta: 0:22:03  lr: 0.000014  min_lr: 0.000000  loss: 4.2411 (4.2028)  class_acc: 0.2917 (0.3161)  loss_scale: 32768.0000 (37547.9770)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0790  max mem: 15572
Epoch: [27]  [ 530/2809]  eta: 0:21:58  lr: 0.000014  min_lr: 0.000000  loss: 4.0747 (4.2005)  class_acc: 0.3750 (0.3172)  loss_scale: 32768.0000 (37457.9586)  weight_decay: 0.0500 (0.0500)  time: 0.5274  data: 0.0788  max mem: 15572
Epoch: [27]  [ 540/2809]  eta: 0:21:53  lr: 0.000014  min_lr: 0.000000  loss: 4.1114 (4.2033)  class_acc: 0.3750 (0.3172)  loss_scale: 32768.0000 (37371.2680)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1451  max mem: 15572
Epoch: [27]  [ 550/2809]  eta: 0:21:46  lr: 0.000014  min_lr: 0.000000  loss: 4.2707 (4.2042)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (37287.7241)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1280  max mem: 15572
Epoch: [27]  [ 560/2809]  eta: 0:21:37  lr: 0.000014  min_lr: 0.000000  loss: 4.2685 (4.2042)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (37207.1586)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0859  max mem: 15572
Epoch: [27]  [ 570/2809]  eta: 0:21:30  lr: 0.000014  min_lr: 0.000000  loss: 4.2754 (4.2062)  class_acc: 0.2917 (0.3166)  loss_scale: 32768.0000 (37129.4151)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0777  max mem: 15572
[2025-01-16 03:20:00,972] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:20:00,973] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [ 580/2809]  eta: 0:21:21  lr: 0.000014  min_lr: 0.000000  loss: 4.2253 (4.2043)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (37505.5422)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0779  max mem: 15572
Epoch: [27]  [ 590/2809]  eta: 0:21:15  lr: 0.000014  min_lr: 0.000000  loss: 4.1200 (4.2046)  class_acc: 0.2917 (0.3166)  loss_scale: 65536.0000 (37979.8308)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.1008  max mem: 15572
Epoch: [27]  [ 600/2809]  eta: 0:21:12  lr: 0.000014  min_lr: 0.000000  loss: 4.3031 (4.2058)  class_acc: 0.2917 (0.3168)  loss_scale: 65536.0000 (38438.3361)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1620  max mem: 15572
Epoch: [27]  [ 610/2809]  eta: 0:21:05  lr: 0.000014  min_lr: 0.000000  loss: 4.2222 (4.2052)  class_acc: 0.2917 (0.3174)  loss_scale: 65536.0000 (38881.8331)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1526  max mem: 15572
Epoch: [27]  [ 620/2809]  eta: 0:20:59  lr: 0.000014  min_lr: 0.000000  loss: 4.1677 (4.2056)  class_acc: 0.3750 (0.3183)  loss_scale: 65536.0000 (39311.0467)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1133  max mem: 15572
Epoch: [27]  [ 630/2809]  eta: 0:20:52  lr: 0.000014  min_lr: 0.000000  loss: 4.2702 (4.2043)  class_acc: 0.3333 (0.3179)  loss_scale: 65536.0000 (39726.6561)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1016  max mem: 15572
Epoch: [27]  [ 640/2809]  eta: 0:20:45  lr: 0.000014  min_lr: 0.000000  loss: 4.2357 (4.2030)  class_acc: 0.2917 (0.3192)  loss_scale: 65536.0000 (40129.2980)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0821  max mem: 15572
Epoch: [27]  [ 650/2809]  eta: 0:20:39  lr: 0.000014  min_lr: 0.000000  loss: 4.1691 (4.2005)  class_acc: 0.3333 (0.3189)  loss_scale: 65536.0000 (40519.5699)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1118  max mem: 15572
Epoch: [27]  [ 660/2809]  eta: 0:20:34  lr: 0.000014  min_lr: 0.000000  loss: 4.2501 (4.2009)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (40898.0333)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1279  max mem: 15572
Epoch: [27]  [ 670/2809]  eta: 0:20:28  lr: 0.000014  min_lr: 0.000000  loss: 4.2501 (4.2017)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (41265.2161)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1247  max mem: 15572
[2025-01-16 03:21:02,045] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76523
[2025-01-16 03:21:02,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:21:02,046] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [ 680/2809]  eta: 0:20:25  lr: 0.000014  min_lr: 0.000000  loss: 4.1385 (4.1990)  class_acc: 0.3333 (0.3190)  loss_scale: 65536.0000 (41573.4978)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1851  max mem: 15572
Epoch: [27]  [ 690/2809]  eta: 0:20:18  lr: 0.000014  min_lr: 0.000000  loss: 4.1199 (4.1981)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (41446.0666)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1514  max mem: 15572
Epoch: [27]  [ 700/2809]  eta: 0:20:11  lr: 0.000014  min_lr: 0.000000  loss: 4.2520 (4.2007)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (41322.2710)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0847  max mem: 15572
Epoch: [27]  [ 710/2809]  eta: 0:20:06  lr: 0.000014  min_lr: 0.000000  loss: 4.3294 (4.2012)  class_acc: 0.2500 (0.3179)  loss_scale: 32768.0000 (41201.9578)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1302  max mem: 15572
Epoch: [27]  [ 720/2809]  eta: 0:20:00  lr: 0.000014  min_lr: 0.000000  loss: 4.1687 (4.1999)  class_acc: 0.3333 (0.3186)  loss_scale: 32768.0000 (41084.9820)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1539  max mem: 15572
Epoch: [27]  [ 730/2809]  eta: 0:19:55  lr: 0.000014  min_lr: 0.000000  loss: 4.1600 (4.1996)  class_acc: 0.3333 (0.3188)  loss_scale: 32768.0000 (40971.2066)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1344  max mem: 15572
Epoch: [27]  [ 740/2809]  eta: 0:19:49  lr: 0.000014  min_lr: 0.000000  loss: 4.1782 (4.2004)  class_acc: 0.2500 (0.3180)  loss_scale: 32768.0000 (40860.5020)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1239  max mem: 15572
Epoch: [27]  [ 750/2809]  eta: 0:19:42  lr: 0.000014  min_lr: 0.000000  loss: 4.2682 (4.2009)  class_acc: 0.2500 (0.3174)  loss_scale: 32768.0000 (40752.7457)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1134  max mem: 15572
Epoch: [27]  [ 760/2809]  eta: 0:19:38  lr: 0.000014  min_lr: 0.000000  loss: 4.2227 (4.2011)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (40647.8213)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1413  max mem: 15572
Epoch: [27]  [ 770/2809]  eta: 0:19:29  lr: 0.000014  min_lr: 0.000000  loss: 4.1926 (4.2014)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (40545.6187)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0926  max mem: 15572
Epoch: [27]  [ 780/2809]  eta: 0:19:23  lr: 0.000014  min_lr: 0.000000  loss: 4.2427 (4.2037)  class_acc: 0.2917 (0.3167)  loss_scale: 32768.0000 (40446.0333)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0492  max mem: 15572
Epoch: [27]  [ 790/2809]  eta: 0:19:18  lr: 0.000014  min_lr: 0.000000  loss: 4.3925 (4.2052)  class_acc: 0.2500 (0.3158)  loss_scale: 32768.0000 (40348.9659)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1102  max mem: 15572
Epoch: [27]  [ 800/2809]  eta: 0:19:12  lr: 0.000014  min_lr: 0.000000  loss: 4.3034 (4.2044)  class_acc: 0.2500 (0.3160)  loss_scale: 32768.0000 (40254.3221)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1265  max mem: 15572
[2025-01-16 03:22:14,826] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:22:14,826] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [ 810/2809]  eta: 0:19:06  lr: 0.000014  min_lr: 0.000000  loss: 4.1634 (4.2035)  class_acc: 0.2917 (0.3168)  loss_scale: 32768.0000 (40242.8212)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1323  max mem: 15572
[2025-01-16 03:22:17,006] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76654
[2025-01-16 03:22:17,006] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:22:17,007] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [ 820/2809]  eta: 0:19:01  lr: 0.000014  min_lr: 0.000000  loss: 4.2175 (4.2041)  class_acc: 0.3750 (0.3168)  loss_scale: 32768.0000 (40151.7759)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1536  max mem: 15572
Epoch: [27]  [ 830/2809]  eta: 0:18:55  lr: 0.000014  min_lr: 0.000000  loss: 4.2391 (4.2029)  class_acc: 0.3333 (0.3166)  loss_scale: 32768.0000 (40062.9218)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1295  max mem: 15572
Epoch: [27]  [ 840/2809]  eta: 0:18:48  lr: 0.000014  min_lr: 0.000000  loss: 4.2391 (4.2043)  class_acc: 0.2500 (0.3158)  loss_scale: 32768.0000 (39976.1807)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0843  max mem: 15572
Epoch: [27]  [ 850/2809]  eta: 0:18:41  lr: 0.000014  min_lr: 0.000000  loss: 4.3108 (4.2041)  class_acc: 0.2917 (0.3160)  loss_scale: 32768.0000 (39891.4783)  weight_decay: 0.0500 (0.0500)  time: 0.5223  data: 0.0717  max mem: 15572
Epoch: [27]  [ 860/2809]  eta: 0:18:34  lr: 0.000014  min_lr: 0.000000  loss: 4.2458 (4.2039)  class_acc: 0.2500 (0.3150)  loss_scale: 32768.0000 (39808.7433)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0715  max mem: 15572
Epoch: [27]  [ 870/2809]  eta: 0:18:29  lr: 0.000014  min_lr: 0.000000  loss: 4.2458 (4.2031)  class_acc: 0.2083 (0.3147)  loss_scale: 32768.0000 (39727.9082)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0941  max mem: 15572
Epoch: [27]  [ 880/2809]  eta: 0:18:26  lr: 0.000014  min_lr: 0.000000  loss: 4.2853 (4.2042)  class_acc: 0.2500 (0.3147)  loss_scale: 32768.0000 (39648.9081)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.1806  max mem: 15572
Epoch: [27]  [ 890/2809]  eta: 0:18:20  lr: 0.000014  min_lr: 0.000000  loss: 4.2148 (4.2038)  class_acc: 0.2917 (0.3143)  loss_scale: 32768.0000 (39571.6813)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1784  max mem: 15572
Epoch: [27]  [ 900/2809]  eta: 0:18:11  lr: 0.000014  min_lr: 0.000000  loss: 4.2148 (4.2040)  class_acc: 0.3333 (0.3150)  loss_scale: 32768.0000 (39496.1687)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0598  max mem: 15572
Epoch: [27]  [ 910/2809]  eta: 0:18:05  lr: 0.000014  min_lr: 0.000000  loss: 4.2735 (4.2041)  class_acc: 0.3750 (0.3149)  loss_scale: 32768.0000 (39422.3139)  weight_decay: 0.0500 (0.0500)  time: 0.4956  data: 0.0574  max mem: 15572
Epoch: [27]  [ 920/2809]  eta: 0:18:01  lr: 0.000014  min_lr: 0.000000  loss: 4.3998 (4.2038)  class_acc: 0.3333 (0.3155)  loss_scale: 32768.0000 (39350.0630)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1327  max mem: 15572
Epoch: [27]  [ 930/2809]  eta: 0:17:56  lr: 0.000014  min_lr: 0.000000  loss: 4.2198 (4.2035)  class_acc: 0.3333 (0.3152)  loss_scale: 32768.0000 (39279.3641)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1613  max mem: 15572
[2025-01-16 03:23:29,219] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:23:29,219] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [ 940/2809]  eta: 0:17:50  lr: 0.000014  min_lr: 0.000000  loss: 4.0816 (4.2038)  class_acc: 0.2917 (0.3150)  loss_scale: 32768.0000 (39244.9904)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1540  max mem: 15572
Epoch: [27]  [ 950/2809]  eta: 0:17:46  lr: 0.000014  min_lr: 0.000000  loss: 4.2366 (4.2049)  class_acc: 0.3333 (0.3152)  loss_scale: 65536.0000 (39521.4469)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1767  max mem: 15572
Epoch: [27]  [ 960/2809]  eta: 0:17:40  lr: 0.000014  min_lr: 0.000000  loss: 4.0874 (4.2038)  class_acc: 0.3333 (0.3147)  loss_scale: 65536.0000 (39792.1498)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1429  max mem: 15572
Epoch: [27]  [ 970/2809]  eta: 0:17:33  lr: 0.000014  min_lr: 0.000000  loss: 4.2550 (4.2044)  class_acc: 0.2083 (0.3139)  loss_scale: 65536.0000 (40057.2770)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0887  max mem: 15572
Epoch: [27]  [ 980/2809]  eta: 0:17:26  lr: 0.000014  min_lr: 0.000000  loss: 4.3194 (4.2054)  class_acc: 0.2500 (0.3138)  loss_scale: 65536.0000 (40316.9990)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0876  max mem: 15572
Epoch: [27]  [ 990/2809]  eta: 0:17:19  lr: 0.000014  min_lr: 0.000000  loss: 4.3194 (4.2074)  class_acc: 0.2917 (0.3140)  loss_scale: 65536.0000 (40571.4793)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0718  max mem: 15572
Epoch: [27]  [1000/2809]  eta: 0:17:11  lr: 0.000014  min_lr: 0.000000  loss: 4.2121 (4.2062)  class_acc: 0.2917 (0.3143)  loss_scale: 65536.0000 (40820.8751)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0440  max mem: 15572
[2025-01-16 03:24:05,245] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76849
[2025-01-16 03:24:05,245] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:24:05,247] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1010/2809]  eta: 0:17:06  lr: 0.000014  min_lr: 0.000000  loss: 4.1180 (4.2055)  class_acc: 0.3750 (0.3153)  loss_scale: 65536.0000 (40903.2799)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0913  max mem: 15572
Epoch: [27]  [1020/2809]  eta: 0:17:02  lr: 0.000014  min_lr: 0.000000  loss: 4.2053 (4.2059)  class_acc: 0.2917 (0.3151)  loss_scale: 32768.0000 (40823.6004)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.2023  max mem: 15572
Epoch: [27]  [1030/2809]  eta: 0:16:57  lr: 0.000014  min_lr: 0.000000  loss: 4.2405 (4.2068)  class_acc: 0.2917 (0.3157)  loss_scale: 32768.0000 (40745.4665)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.1860  max mem: 15572
Epoch: [27]  [1040/2809]  eta: 0:16:51  lr: 0.000014  min_lr: 0.000000  loss: 4.2405 (4.2060)  class_acc: 0.2917 (0.3158)  loss_scale: 32768.0000 (40668.8338)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1152  max mem: 15572
Epoch: [27]  [1050/2809]  eta: 0:16:44  lr: 0.000014  min_lr: 0.000000  loss: 4.2237 (4.2062)  class_acc: 0.2917 (0.3163)  loss_scale: 32768.0000 (40593.6594)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0758  max mem: 15572
Epoch: [27]  [1060/2809]  eta: 0:16:38  lr: 0.000014  min_lr: 0.000000  loss: 4.2373 (4.2073)  class_acc: 0.3333 (0.3168)  loss_scale: 32768.0000 (40519.9020)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.1138  max mem: 15572
Epoch: [27]  [1070/2809]  eta: 0:16:34  lr: 0.000014  min_lr: 0.000000  loss: 4.1992 (4.2068)  class_acc: 0.4167 (0.3173)  loss_scale: 32768.0000 (40447.5219)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1861  max mem: 15572
Epoch: [27]  [1080/2809]  eta: 0:16:28  lr: 0.000014  min_lr: 0.000000  loss: 4.1616 (4.2076)  class_acc: 0.3333 (0.3171)  loss_scale: 32768.0000 (40376.4810)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1633  max mem: 15572
Epoch: [27]  [1090/2809]  eta: 0:16:21  lr: 0.000014  min_lr: 0.000000  loss: 4.2420 (4.2072)  class_acc: 0.2500 (0.3168)  loss_scale: 32768.0000 (40306.7424)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0729  max mem: 15572
Epoch: [27]  [1100/2809]  eta: 0:16:14  lr: 0.000014  min_lr: 0.000000  loss: 4.2250 (4.2067)  class_acc: 0.2500 (0.3165)  loss_scale: 32768.0000 (40238.2707)  weight_decay: 0.0500 (0.0500)  time: 0.4689  data: 0.0082  max mem: 15572
Epoch: [27]  [1110/2809]  eta: 0:16:08  lr: 0.000014  min_lr: 0.000000  loss: 4.2517 (4.2072)  class_acc: 0.2500 (0.3165)  loss_scale: 32768.0000 (40171.0315)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0548  max mem: 15572
Epoch: [27]  [1120/2809]  eta: 0:16:01  lr: 0.000013  min_lr: 0.000000  loss: 4.2732 (4.2068)  class_acc: 0.2917 (0.3170)  loss_scale: 32768.0000 (40104.9920)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0895  max mem: 15572
Epoch: [27]  [1130/2809]  eta: 0:15:54  lr: 0.000013  min_lr: 0.000000  loss: 4.2972 (4.2067)  class_acc: 0.2917 (0.3166)  loss_scale: 32768.0000 (40040.1202)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0674  max mem: 15572
[2025-01-16 03:25:15,644] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:25:15,644] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1140/2809]  eta: 0:15:49  lr: 0.000013  min_lr: 0.000000  loss: 4.1672 (4.2061)  class_acc: 0.2917 (0.3167)  loss_scale: 32768.0000 (40148.6976)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1107  max mem: 15572
[2025-01-16 03:25:23,091] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76990
[2025-01-16 03:25:23,092] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:25:23,092] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1150/2809]  eta: 0:15:43  lr: 0.000013  min_lr: 0.000000  loss: 4.1672 (4.2061)  class_acc: 0.2917 (0.3169)  loss_scale: 65536.0000 (40255.3884)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1194  max mem: 15572
[2025-01-16 03:25:28,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=77000, skipped=479, lr=[1.3050742284399558e-07, 1.3050742284399558e-07, 1.8643917549142228e-07, 1.8643917549142228e-07, 2.663416792734604e-07, 2.663416792734604e-07, 3.804881132478006e-07, 3.804881132478006e-07, 5.435544474968581e-07, 5.435544474968581e-07, 7.765063535669401e-07, 7.765063535669401e-07, 1.1092947908099145e-06, 1.1092947908099145e-06, 1.5847068440141638e-06, 1.5847068440141638e-06, 2.263866920020234e-06, 2.263866920020234e-06, 3.234095600028906e-06, 3.234095600028906e-06, 4.620136571469866e-06, 4.620136571469866e-06, 6.600195102099809e-06, 6.600195102099809e-06, 9.42885014585687e-06, 9.42885014585687e-06, 1.3469785922652673e-05, 1.3469785922652673e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:25:28,715] [INFO] [timer.py:260:stop] epoch=0/micro_step=77000/global_step=77000, RunningAvgSamplesPerSec=27.910745403847265, CurrSamplesPerSec=30.71497582182602, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [27]  [1160/2809]  eta: 0:15:37  lr: 0.000013  min_lr: 0.000000  loss: 4.1362 (4.2057)  class_acc: 0.2917 (0.3167)  loss_scale: 32768.0000 (40190.8975)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1086  max mem: 15572
Epoch: [27]  [1170/2809]  eta: 0:15:33  lr: 0.000013  min_lr: 0.000000  loss: 4.1362 (4.2055)  class_acc: 0.2500 (0.3159)  loss_scale: 32768.0000 (40127.5081)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1577  max mem: 15572
Epoch: [27]  [1180/2809]  eta: 0:15:27  lr: 0.000013  min_lr: 0.000000  loss: 4.1811 (4.2058)  class_acc: 0.2500 (0.3159)  loss_scale: 32768.0000 (40065.1922)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1467  max mem: 15572
Epoch: [27]  [1190/2809]  eta: 0:15:21  lr: 0.000013  min_lr: 0.000000  loss: 4.3417 (4.2074)  class_acc: 0.3333 (0.3157)  loss_scale: 32768.0000 (40003.9228)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1208  max mem: 15572
Epoch: [27]  [1200/2809]  eta: 0:15:16  lr: 0.000013  min_lr: 0.000000  loss: 4.3076 (4.2067)  class_acc: 0.3750 (0.3163)  loss_scale: 32768.0000 (39943.6736)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1221  max mem: 15572
Epoch: [27]  [1210/2809]  eta: 0:15:10  lr: 0.000013  min_lr: 0.000000  loss: 4.1341 (4.2062)  class_acc: 0.3333 (0.3159)  loss_scale: 32768.0000 (39884.4195)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1366  max mem: 15572
Epoch: [27]  [1220/2809]  eta: 0:15:04  lr: 0.000013  min_lr: 0.000000  loss: 4.3086 (4.2072)  class_acc: 0.2917 (0.3158)  loss_scale: 32768.0000 (39826.1360)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1028  max mem: 15572
Epoch: [27]  [1230/2809]  eta: 0:14:59  lr: 0.000013  min_lr: 0.000000  loss: 4.3516 (4.2082)  class_acc: 0.2917 (0.3157)  loss_scale: 32768.0000 (39768.7994)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1002  max mem: 15572
Epoch: [27]  [1240/2809]  eta: 0:14:54  lr: 0.000013  min_lr: 0.000000  loss: 4.3846 (4.2100)  class_acc: 0.2083 (0.3148)  loss_scale: 32768.0000 (39712.3868)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1517  max mem: 15572
Epoch: [27]  [1250/2809]  eta: 0:14:49  lr: 0.000013  min_lr: 0.000000  loss: 4.3865 (4.2102)  class_acc: 0.2500 (0.3144)  loss_scale: 32768.0000 (39656.8761)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1752  max mem: 15572
Epoch: [27]  [1260/2809]  eta: 0:14:43  lr: 0.000013  min_lr: 0.000000  loss: 4.2648 (4.2107)  class_acc: 0.2917 (0.3143)  loss_scale: 32768.0000 (39602.2458)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1547  max mem: 15572
Epoch: [27]  [1270/2809]  eta: 0:14:38  lr: 0.000013  min_lr: 0.000000  loss: 4.2892 (4.2121)  class_acc: 0.2917 (0.3143)  loss_scale: 32768.0000 (39548.4752)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1277  max mem: 15572
[2025-01-16 03:26:38,485] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:26:38,486] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1280/2809]  eta: 0:14:33  lr: 0.000013  min_lr: 0.000000  loss: 4.3504 (4.2125)  class_acc: 0.2917 (0.3141)  loss_scale: 32768.0000 (39623.4442)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1559  max mem: 15572
Epoch: [27]  [1290/2809]  eta: 0:14:27  lr: 0.000013  min_lr: 0.000000  loss: 4.1762 (4.2108)  class_acc: 0.2917 (0.3144)  loss_scale: 65536.0000 (39824.1611)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1022  max mem: 15572
Epoch: [27]  [1300/2809]  eta: 0:14:20  lr: 0.000013  min_lr: 0.000000  loss: 4.1598 (4.2106)  class_acc: 0.2917 (0.3142)  loss_scale: 65536.0000 (40021.7925)  weight_decay: 0.0500 (0.0500)  time: 0.4900  data: 0.0354  max mem: 15572
Epoch: [27]  [1310/2809]  eta: 0:14:13  lr: 0.000013  min_lr: 0.000000  loss: 4.3630 (4.2125)  class_acc: 0.2500 (0.3139)  loss_scale: 65536.0000 (40216.4088)  weight_decay: 0.0500 (0.0500)  time: 0.4801  data: 0.0237  max mem: 15572
[2025-01-16 03:27:00,465] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77161
[2025-01-16 03:27:00,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:27:00,465] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1320/2809]  eta: 0:14:08  lr: 0.000013  min_lr: 0.000000  loss: 4.3983 (4.2127)  class_acc: 0.2500 (0.3139)  loss_scale: 65536.0000 (40333.6624)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0718  max mem: 15572
Epoch: [27]  [1330/2809]  eta: 0:14:01  lr: 0.000013  min_lr: 0.000000  loss: 4.2308 (4.2134)  class_acc: 0.2917 (0.3134)  loss_scale: 32768.0000 (40276.8204)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0787  max mem: 15572
Epoch: [27]  [1340/2809]  eta: 0:13:57  lr: 0.000013  min_lr: 0.000000  loss: 4.2085 (4.2136)  class_acc: 0.2500 (0.3129)  loss_scale: 32768.0000 (40220.8262)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1244  max mem: 15572
Epoch: [27]  [1350/2809]  eta: 0:13:51  lr: 0.000013  min_lr: 0.000000  loss: 4.3098 (4.2135)  class_acc: 0.2500 (0.3130)  loss_scale: 32768.0000 (40165.6610)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.1906  max mem: 15572
Epoch: [27]  [1360/2809]  eta: 0:13:46  lr: 0.000013  min_lr: 0.000000  loss: 4.2776 (4.2135)  class_acc: 0.2917 (0.3126)  loss_scale: 32768.0000 (40111.3064)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1710  max mem: 15572
Epoch: [27]  [1370/2809]  eta: 0:13:40  lr: 0.000013  min_lr: 0.000000  loss: 4.2776 (4.2140)  class_acc: 0.2917 (0.3125)  loss_scale: 32768.0000 (40057.7447)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1520  max mem: 15572
Epoch: [27]  [1380/2809]  eta: 0:13:34  lr: 0.000013  min_lr: 0.000000  loss: 4.3450 (4.2141)  class_acc: 0.2917 (0.3127)  loss_scale: 32768.0000 (40004.9587)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1211  max mem: 15572
Epoch: [27]  [1390/2809]  eta: 0:13:29  lr: 0.000013  min_lr: 0.000000  loss: 4.1383 (4.2143)  class_acc: 0.3750 (0.3129)  loss_scale: 32768.0000 (39952.9317)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1431  max mem: 15572
Epoch: [27]  [1400/2809]  eta: 0:13:24  lr: 0.000013  min_lr: 0.000000  loss: 4.2488 (4.2144)  class_acc: 0.2917 (0.3126)  loss_scale: 32768.0000 (39901.6474)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1622  max mem: 15572
Epoch: [27]  [1410/2809]  eta: 0:13:18  lr: 0.000013  min_lr: 0.000000  loss: 4.2987 (4.2148)  class_acc: 0.2500 (0.3126)  loss_scale: 32768.0000 (39851.0900)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1296  max mem: 15572
Epoch: [27]  [1420/2809]  eta: 0:13:12  lr: 0.000013  min_lr: 0.000000  loss: 4.2987 (4.2148)  class_acc: 0.3333 (0.3123)  loss_scale: 32768.0000 (39801.2442)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1134  max mem: 15572
Epoch: [27]  [1430/2809]  eta: 0:13:07  lr: 0.000013  min_lr: 0.000000  loss: 4.1276 (4.2137)  class_acc: 0.3333 (0.3127)  loss_scale: 32768.0000 (39752.0950)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1464  max mem: 15572
Epoch: [27]  [1440/2809]  eta: 0:13:01  lr: 0.000013  min_lr: 0.000000  loss: 4.1276 (4.2136)  class_acc: 0.2917 (0.3126)  loss_scale: 32768.0000 (39703.6280)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1113  max mem: 15572
[2025-01-16 03:28:17,196] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:28:17,196] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1450/2809]  eta: 0:12:55  lr: 0.000013  min_lr: 0.000000  loss: 4.1770 (4.2133)  class_acc: 0.2083 (0.3122)  loss_scale: 32768.0000 (39746.1613)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1045  max mem: 15572
Epoch: [27]  [1460/2809]  eta: 0:12:50  lr: 0.000013  min_lr: 0.000000  loss: 4.1654 (4.2136)  class_acc: 0.2083 (0.3120)  loss_scale: 65536.0000 (39922.6831)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1341  max mem: 15572
Epoch: [27]  [1470/2809]  eta: 0:12:44  lr: 0.000013  min_lr: 0.000000  loss: 4.2221 (4.2136)  class_acc: 0.2500 (0.3115)  loss_scale: 65536.0000 (40096.8049)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1192  max mem: 15572
Epoch: [27]  [1480/2809]  eta: 0:12:39  lr: 0.000013  min_lr: 0.000000  loss: 4.2221 (4.2137)  class_acc: 0.2500 (0.3114)  loss_scale: 65536.0000 (40268.5753)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1454  max mem: 15572
[2025-01-16 03:28:38,344] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77328
[2025-01-16 03:28:38,344] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:28:38,344] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1490/2809]  eta: 0:12:33  lr: 0.000013  min_lr: 0.000000  loss: 4.2246 (4.2138)  class_acc: 0.2917 (0.3117)  loss_scale: 65536.0000 (40306.1784)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1374  max mem: 15572
Epoch: [27]  [1500/2809]  eta: 0:12:27  lr: 0.000013  min_lr: 0.000000  loss: 4.2246 (4.2142)  class_acc: 0.2917 (0.3118)  loss_scale: 32768.0000 (40255.9574)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1437  max mem: 15572
Epoch: [27]  [1510/2809]  eta: 0:12:21  lr: 0.000013  min_lr: 0.000000  loss: 4.2334 (4.2153)  class_acc: 0.2917 (0.3113)  loss_scale: 32768.0000 (40206.4011)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1206  max mem: 15572
Epoch: [27]  [1520/2809]  eta: 0:12:15  lr: 0.000013  min_lr: 0.000000  loss: 4.3224 (4.2153)  class_acc: 0.2917 (0.3115)  loss_scale: 32768.0000 (40157.4964)  weight_decay: 0.0500 (0.0500)  time: 0.5055  data: 0.0632  max mem: 15572
Epoch: [27]  [1530/2809]  eta: 0:12:09  lr: 0.000013  min_lr: 0.000000  loss: 4.2327 (4.2150)  class_acc: 0.3333 (0.3115)  loss_scale: 32768.0000 (40109.2306)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0862  max mem: 15572
Epoch: [27]  [1540/2809]  eta: 0:12:03  lr: 0.000013  min_lr: 0.000000  loss: 4.0817 (4.2142)  class_acc: 0.2917 (0.3115)  loss_scale: 32768.0000 (40061.5912)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1146  max mem: 15572
Epoch: [27]  [1550/2809]  eta: 0:11:57  lr: 0.000013  min_lr: 0.000000  loss: 4.0962 (4.2141)  class_acc: 0.2917 (0.3117)  loss_scale: 32768.0000 (40014.5661)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1019  max mem: 15572
Epoch: [27]  [1560/2809]  eta: 0:11:52  lr: 0.000013  min_lr: 0.000000  loss: 4.3161 (4.2152)  class_acc: 0.2917 (0.3113)  loss_scale: 32768.0000 (39968.1435)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1015  max mem: 15572
Epoch: [27]  [1570/2809]  eta: 0:11:46  lr: 0.000013  min_lr: 0.000000  loss: 4.3321 (4.2153)  class_acc: 0.2917 (0.3111)  loss_scale: 32768.0000 (39922.3119)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0883  max mem: 15572
Epoch: [27]  [1580/2809]  eta: 0:11:40  lr: 0.000013  min_lr: 0.000000  loss: 4.3615 (4.2169)  class_acc: 0.2500 (0.3113)  loss_scale: 32768.0000 (39877.0601)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1236  max mem: 15572
Epoch: [27]  [1590/2809]  eta: 0:11:34  lr: 0.000013  min_lr: 0.000000  loss: 4.4048 (4.2171)  class_acc: 0.3333 (0.3117)  loss_scale: 32768.0000 (39832.3771)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1189  max mem: 15572
Epoch: [27]  [1600/2809]  eta: 0:11:28  lr: 0.000013  min_lr: 0.000000  loss: 4.2246 (4.2163)  class_acc: 0.4167 (0.3122)  loss_scale: 32768.0000 (39788.2523)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0465  max mem: 15572
Epoch: [27]  [1610/2809]  eta: 0:11:22  lr: 0.000013  min_lr: 0.000000  loss: 4.1400 (4.2162)  class_acc: 0.4167 (0.3126)  loss_scale: 32768.0000 (39744.6754)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0755  max mem: 15572
[2025-01-16 03:29:49,229] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:29:49,229] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1620/2809]  eta: 0:11:16  lr: 0.000013  min_lr: 0.000000  loss: 4.1400 (4.2154)  class_acc: 0.3750 (0.3127)  loss_scale: 32768.0000 (39843.1388)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1126  max mem: 15572
Epoch: [27]  [1630/2809]  eta: 0:11:11  lr: 0.000013  min_lr: 0.000000  loss: 4.1455 (4.2158)  class_acc: 0.3750 (0.3128)  loss_scale: 65536.0000 (40000.6671)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1573  max mem: 15572
Epoch: [27]  [1640/2809]  eta: 0:11:06  lr: 0.000013  min_lr: 0.000000  loss: 4.3067 (4.2159)  class_acc: 0.2500 (0.3127)  loss_scale: 65536.0000 (40156.2754)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1565  max mem: 15572
Epoch: [27]  [1650/2809]  eta: 0:11:00  lr: 0.000013  min_lr: 0.000000  loss: 4.1087 (4.2153)  class_acc: 0.3333 (0.3130)  loss_scale: 65536.0000 (40309.9988)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1462  max mem: 15572
[2025-01-16 03:30:11,655] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77494
[2025-01-16 03:30:11,655] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:30:11,655] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1660/2809]  eta: 0:10:54  lr: 0.000013  min_lr: 0.000000  loss: 4.1507 (4.2150)  class_acc: 0.3333 (0.3131)  loss_scale: 32768.0000 (40264.5924)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1379  max mem: 15572
Epoch: [27]  [1670/2809]  eta: 0:10:48  lr: 0.000013  min_lr: 0.000000  loss: 4.2319 (4.2148)  class_acc: 0.3333 (0.3132)  loss_scale: 32768.0000 (40219.7295)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0651  max mem: 15572
Epoch: [27]  [1680/2809]  eta: 0:10:42  lr: 0.000013  min_lr: 0.000000  loss: 4.2471 (4.2151)  class_acc: 0.3333 (0.3132)  loss_scale: 32768.0000 (40175.4004)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0757  max mem: 15572
Epoch: [27]  [1690/2809]  eta: 0:10:37  lr: 0.000013  min_lr: 0.000000  loss: 4.2023 (4.2144)  class_acc: 0.3333 (0.3136)  loss_scale: 32768.0000 (40131.5955)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1163  max mem: 15572
Epoch: [27]  [1700/2809]  eta: 0:10:30  lr: 0.000013  min_lr: 0.000000  loss: 4.1122 (4.2135)  class_acc: 0.3333 (0.3138)  loss_scale: 32768.0000 (40088.3057)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0750  max mem: 15572
Epoch: [27]  [1710/2809]  eta: 0:10:24  lr: 0.000013  min_lr: 0.000000  loss: 4.0993 (4.2138)  class_acc: 0.3333 (0.3138)  loss_scale: 32768.0000 (40045.5219)  weight_decay: 0.0500 (0.0500)  time: 0.4861  data: 0.0427  max mem: 15572
Epoch: [27]  [1720/2809]  eta: 0:10:19  lr: 0.000013  min_lr: 0.000000  loss: 4.1812 (4.2137)  class_acc: 0.2917 (0.3137)  loss_scale: 32768.0000 (40003.2353)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0915  max mem: 15572
Epoch: [27]  [1730/2809]  eta: 0:10:13  lr: 0.000013  min_lr: 0.000000  loss: 4.2008 (4.2137)  class_acc: 0.3333 (0.3140)  loss_scale: 32768.0000 (39961.4373)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1297  max mem: 15572
Epoch: [27]  [1740/2809]  eta: 0:10:07  lr: 0.000013  min_lr: 0.000000  loss: 4.2108 (4.2137)  class_acc: 0.3333 (0.3140)  loss_scale: 32768.0000 (39920.1195)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1137  max mem: 15572
Epoch: [27]  [1750/2809]  eta: 0:10:01  lr: 0.000013  min_lr: 0.000000  loss: 4.2511 (4.2146)  class_acc: 0.2917 (0.3138)  loss_scale: 32768.0000 (39879.2736)  weight_decay: 0.0500 (0.0500)  time: 0.4953  data: 0.0653  max mem: 15572
Epoch: [27]  [1760/2809]  eta: 0:09:56  lr: 0.000013  min_lr: 0.000000  loss: 4.2197 (4.2147)  class_acc: 0.2500 (0.3136)  loss_scale: 32768.0000 (39838.8915)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1304  max mem: 15572
Epoch: [27]  [1770/2809]  eta: 0:09:50  lr: 0.000013  min_lr: 0.000000  loss: 4.1743 (4.2147)  class_acc: 0.2917 (0.3137)  loss_scale: 32768.0000 (39798.9656)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1327  max mem: 15572
[2025-01-16 03:31:22,260] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:31:22,261] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1780/2809]  eta: 0:09:44  lr: 0.000013  min_lr: 0.000000  loss: 4.2342 (4.2143)  class_acc: 0.3333 (0.3141)  loss_scale: 32768.0000 (39777.8866)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0964  max mem: 15572
Epoch: [27]  [1790/2809]  eta: 0:09:38  lr: 0.000013  min_lr: 0.000000  loss: 4.2342 (4.2145)  class_acc: 0.3333 (0.3138)  loss_scale: 65536.0000 (39921.7063)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1159  max mem: 15572
Epoch: [27]  [1800/2809]  eta: 0:09:33  lr: 0.000013  min_lr: 0.000000  loss: 4.2301 (4.2140)  class_acc: 0.2917 (0.3140)  loss_scale: 65536.0000 (40063.9289)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0866  max mem: 15572
[2025-01-16 03:31:33,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77644
[2025-01-16 03:31:33,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:31:33,903] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1810/2809]  eta: 0:09:27  lr: 0.000013  min_lr: 0.000000  loss: 4.2301 (4.2142)  class_acc: 0.2917 (0.3139)  loss_scale: 32768.0000 (40023.6422)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1110  max mem: 15572
Epoch: [27]  [1820/2809]  eta: 0:09:21  lr: 0.000013  min_lr: 0.000000  loss: 4.2673 (4.2146)  class_acc: 0.3333 (0.3143)  loss_scale: 32768.0000 (39983.7979)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1198  max mem: 15572
Epoch: [27]  [1830/2809]  eta: 0:09:16  lr: 0.000013  min_lr: 0.000000  loss: 4.1454 (4.2145)  class_acc: 0.3750 (0.3146)  loss_scale: 32768.0000 (39944.3889)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1500  max mem: 15572
Epoch: [27]  [1840/2809]  eta: 0:09:10  lr: 0.000013  min_lr: 0.000000  loss: 4.1706 (4.2145)  class_acc: 0.2917 (0.3146)  loss_scale: 32768.0000 (39905.4079)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1935  max mem: 15572
Epoch: [27]  [1850/2809]  eta: 0:09:05  lr: 0.000013  min_lr: 0.000000  loss: 4.1813 (4.2135)  class_acc: 0.3333 (0.3149)  loss_scale: 32768.0000 (39866.8482)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1657  max mem: 15572
Epoch: [27]  [1860/2809]  eta: 0:08:59  lr: 0.000013  min_lr: 0.000000  loss: 4.1813 (4.2136)  class_acc: 0.3333 (0.3150)  loss_scale: 32768.0000 (39828.7028)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1194  max mem: 15572
Epoch: [27]  [1870/2809]  eta: 0:08:53  lr: 0.000013  min_lr: 0.000000  loss: 4.1604 (4.2130)  class_acc: 0.3333 (0.3152)  loss_scale: 32768.0000 (39790.9653)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1115  max mem: 15572
Epoch: [27]  [1880/2809]  eta: 0:08:48  lr: 0.000013  min_lr: 0.000000  loss: 4.1098 (4.2126)  class_acc: 0.2917 (0.3151)  loss_scale: 32768.0000 (39753.6289)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1500  max mem: 15572
Epoch: [27]  [1890/2809]  eta: 0:08:42  lr: 0.000013  min_lr: 0.000000  loss: 4.1098 (4.2117)  class_acc: 0.2917 (0.3152)  loss_scale: 32768.0000 (39716.6875)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1531  max mem: 15572
Epoch: [27]  [1900/2809]  eta: 0:08:37  lr: 0.000013  min_lr: 0.000000  loss: 4.1364 (4.2118)  class_acc: 0.2500 (0.3146)  loss_scale: 32768.0000 (39680.1347)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1472  max mem: 15572
Epoch: [27]  [1910/2809]  eta: 0:08:31  lr: 0.000013  min_lr: 0.000000  loss: 4.1364 (4.2110)  class_acc: 0.2083 (0.3145)  loss_scale: 32768.0000 (39643.9644)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1190  max mem: 15572
Epoch: [27]  [1920/2809]  eta: 0:08:26  lr: 0.000013  min_lr: 0.000000  loss: 4.2499 (4.2115)  class_acc: 0.2500 (0.3144)  loss_scale: 32768.0000 (39608.1707)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1326  max mem: 15572
[2025-01-16 03:32:50,314] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:32:50,314] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1930/2809]  eta: 0:08:20  lr: 0.000013  min_lr: 0.000000  loss: 4.3353 (4.2125)  class_acc: 0.2500 (0.3138)  loss_scale: 32768.0000 (39589.7172)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1750  max mem: 15572
Epoch: [27]  [1940/2809]  eta: 0:08:14  lr: 0.000013  min_lr: 0.000000  loss: 4.2784 (4.2122)  class_acc: 0.2917 (0.3142)  loss_scale: 65536.0000 (39723.3921)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0773  max mem: 15572
[2025-01-16 03:32:56,198] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77786
[2025-01-16 03:32:56,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:32:56,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1950/2809]  eta: 0:08:08  lr: 0.000013  min_lr: 0.000000  loss: 4.2756 (4.2123)  class_acc: 0.2917 (0.3142)  loss_scale: 65536.0000 (39721.3326)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0696  max mem: 15572
Epoch: [27]  [1960/2809]  eta: 0:08:02  lr: 0.000013  min_lr: 0.000000  loss: 4.3018 (4.2128)  class_acc: 0.2500 (0.3140)  loss_scale: 32768.0000 (39685.8746)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0906  max mem: 15572
Epoch: [27]  [1970/2809]  eta: 0:07:57  lr: 0.000013  min_lr: 0.000000  loss: 4.2054 (4.2121)  class_acc: 0.2917 (0.3143)  loss_scale: 32768.0000 (39650.7763)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1725  max mem: 15572
Epoch: [27]  [1980/2809]  eta: 0:07:51  lr: 0.000013  min_lr: 0.000000  loss: 4.0888 (4.2123)  class_acc: 0.2917 (0.3138)  loss_scale: 32768.0000 (39616.0323)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1750  max mem: 15572
Epoch: [27]  [1990/2809]  eta: 0:07:45  lr: 0.000013  min_lr: 0.000000  loss: 4.2588 (4.2125)  class_acc: 0.2083 (0.3136)  loss_scale: 32768.0000 (39581.6374)  weight_decay: 0.0500 (0.0500)  time: 0.4712  data: 0.0309  max mem: 15572
Epoch: [27]  [2000/2809]  eta: 0:07:39  lr: 0.000013  min_lr: 0.000000  loss: 4.2126 (4.2118)  class_acc: 0.2500 (0.3135)  loss_scale: 32768.0000 (39547.5862)  weight_decay: 0.0500 (0.0500)  time: 0.4912  data: 0.0307  max mem: 15572
Epoch: [27]  [2010/2809]  eta: 0:07:34  lr: 0.000013  min_lr: 0.000000  loss: 4.1964 (4.2118)  class_acc: 0.3333 (0.3134)  loss_scale: 32768.0000 (39513.8737)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0961  max mem: 15572
Epoch: [27]  [2020/2809]  eta: 0:07:28  lr: 0.000013  min_lr: 0.000000  loss: 4.3585 (4.2127)  class_acc: 0.2917 (0.3134)  loss_scale: 32768.0000 (39480.4948)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0956  max mem: 15572
Epoch: [27]  [2030/2809]  eta: 0:07:22  lr: 0.000013  min_lr: 0.000000  loss: 4.4369 (4.2134)  class_acc: 0.2500 (0.3132)  loss_scale: 32768.0000 (39447.4446)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0973  max mem: 15572
Epoch: [27]  [2040/2809]  eta: 0:07:16  lr: 0.000013  min_lr: 0.000000  loss: 4.3780 (4.2137)  class_acc: 0.2917 (0.3134)  loss_scale: 32768.0000 (39414.7183)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0909  max mem: 15572
Epoch: [27]  [2050/2809]  eta: 0:07:11  lr: 0.000013  min_lr: 0.000000  loss: 4.2338 (4.2137)  class_acc: 0.3333 (0.3136)  loss_scale: 32768.0000 (39382.3111)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1091  max mem: 15572
Epoch: [27]  [2060/2809]  eta: 0:07:05  lr: 0.000013  min_lr: 0.000000  loss: 4.3181 (4.2144)  class_acc: 0.2917 (0.3135)  loss_scale: 32768.0000 (39350.2183)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1161  max mem: 15572
Epoch: [27]  [2070/2809]  eta: 0:06:59  lr: 0.000013  min_lr: 0.000000  loss: 4.3263 (4.2141)  class_acc: 0.2500 (0.3134)  loss_scale: 32768.0000 (39318.4355)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0853  max mem: 15572
[2025-01-16 03:34:07,386] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:34:07,386] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [2080/2809]  eta: 0:06:54  lr: 0.000013  min_lr: 0.000000  loss: 4.0846 (4.2138)  class_acc: 0.2917 (0.3132)  loss_scale: 32768.0000 (39428.6747)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1170  max mem: 15572
Epoch: [27]  [2090/2809]  eta: 0:06:48  lr: 0.000013  min_lr: 0.000000  loss: 4.3363 (4.2149)  class_acc: 0.2500 (0.3131)  loss_scale: 65536.0000 (39553.5304)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0924  max mem: 15572
Epoch: [27]  [2100/2809]  eta: 0:06:42  lr: 0.000013  min_lr: 0.000000  loss: 4.3859 (4.2155)  class_acc: 0.2917 (0.3131)  loss_scale: 65536.0000 (39677.1975)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0893  max mem: 15572
Epoch: [27]  [2110/2809]  eta: 0:06:36  lr: 0.000013  min_lr: 0.000000  loss: 4.2307 (4.2155)  class_acc: 0.2917 (0.3131)  loss_scale: 65536.0000 (39799.6930)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0932  max mem: 15572
Epoch: [27]  [2120/2809]  eta: 0:06:30  lr: 0.000013  min_lr: 0.000000  loss: 4.3636 (4.2162)  class_acc: 0.2917 (0.3132)  loss_scale: 65536.0000 (39921.0335)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0619  max mem: 15572
Epoch: [27]  [2130/2809]  eta: 0:06:25  lr: 0.000013  min_lr: 0.000000  loss: 4.2027 (4.2160)  class_acc: 0.3333 (0.3134)  loss_scale: 65536.0000 (40041.2351)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0691  max mem: 15572
Epoch: [27]  [2140/2809]  eta: 0:06:19  lr: 0.000013  min_lr: 0.000000  loss: 4.2027 (4.2166)  class_acc: 0.3333 (0.3135)  loss_scale: 65536.0000 (40160.3139)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0842  max mem: 15572
[2025-01-16 03:34:46,328] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77985
[2025-01-16 03:34:46,329] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:34:46,329] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [2150/2809]  eta: 0:06:13  lr: 0.000013  min_lr: 0.000000  loss: 4.2090 (4.2165)  class_acc: 0.2917 (0.3135)  loss_scale: 65536.0000 (40141.1808)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0995  max mem: 15572
[2025-01-16 03:34:53,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=78000, skipped=485, lr=[1.2400201441221816e-07, 1.2400201441221816e-07, 1.7714573487459742e-07, 1.7714573487459742e-07, 2.5306533553513916e-07, 2.5306533553513916e-07, 3.615219079073417e-07, 3.615219079073417e-07, 5.164598684390596e-07, 5.164598684390596e-07, 7.377998120557995e-07, 7.377998120557995e-07, 1.053999731508285e-06, 1.053999731508285e-06, 1.505713902154693e-06, 1.505713902154693e-06, 2.15101986022099e-06, 2.15101986022099e-06, 3.0728855146014146e-06, 3.0728855146014146e-06, 4.389836449430592e-06, 4.389836449430592e-06, 6.2711949277579896e-06, 6.2711949277579896e-06, 8.95884989679713e-06, 8.95884989679713e-06, 1.279835699542447e-05, 1.279835699542447e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:34:53,679] [INFO] [timer.py:260:stop] epoch=0/micro_step=78000/global_step=78000, RunningAvgSamplesPerSec=27.913910744086742, CurrSamplesPerSec=31.030341291752105, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [27]  [2160/2809]  eta: 0:06:07  lr: 0.000013  min_lr: 0.000000  loss: 4.3414 (4.2169)  class_acc: 0.2917 (0.3136)  loss_scale: 32768.0000 (40107.0615)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1241  max mem: 15572
Epoch: [27]  [2170/2809]  eta: 0:06:02  lr: 0.000013  min_lr: 0.000000  loss: 4.1575 (4.2164)  class_acc: 0.3333 (0.3135)  loss_scale: 32768.0000 (40073.2566)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1604  max mem: 15572
[2025-01-16 03:35:06,527] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78020
[2025-01-16 03:35:06,528] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 03:35:06,528] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [27]  [2180/2809]  eta: 0:05:56  lr: 0.000013  min_lr: 0.000000  loss: 4.1120 (4.2165)  class_acc: 0.2500 (0.3134)  loss_scale: 32768.0000 (40009.7130)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1594  max mem: 15572
Epoch: [27]  [2190/2809]  eta: 0:05:51  lr: 0.000013  min_lr: 0.000000  loss: 4.2551 (4.2164)  class_acc: 0.2500 (0.3132)  loss_scale: 16384.0000 (39901.8822)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1311  max mem: 15572
Epoch: [27]  [2200/2809]  eta: 0:05:45  lr: 0.000013  min_lr: 0.000000  loss: 4.2064 (4.2160)  class_acc: 0.2500 (0.3131)  loss_scale: 16384.0000 (39795.0313)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1438  max mem: 15572
Epoch: [27]  [2210/2809]  eta: 0:05:39  lr: 0.000013  min_lr: 0.000000  loss: 4.2149 (4.2161)  class_acc: 0.3333 (0.3133)  loss_scale: 16384.0000 (39689.1470)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1429  max mem: 15572
Epoch: [27]  [2220/2809]  eta: 0:05:34  lr: 0.000013  min_lr: 0.000000  loss: 4.2857 (4.2166)  class_acc: 0.3333 (0.3132)  loss_scale: 16384.0000 (39584.2161)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1151  max mem: 15572
Epoch: [27]  [2230/2809]  eta: 0:05:28  lr: 0.000013  min_lr: 0.000000  loss: 4.1951 (4.2162)  class_acc: 0.3333 (0.3133)  loss_scale: 16384.0000 (39480.2259)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0936  max mem: 15572
Epoch: [27]  [2240/2809]  eta: 0:05:22  lr: 0.000013  min_lr: 0.000000  loss: 4.1951 (4.2164)  class_acc: 0.2917 (0.3131)  loss_scale: 16384.0000 (39377.1638)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0658  max mem: 15572
Epoch: [27]  [2250/2809]  eta: 0:05:16  lr: 0.000013  min_lr: 0.000000  loss: 4.3219 (4.2169)  class_acc: 0.2500 (0.3129)  loss_scale: 16384.0000 (39275.0173)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0498  max mem: 15572
Epoch: [27]  [2260/2809]  eta: 0:05:11  lr: 0.000013  min_lr: 0.000000  loss: 4.3219 (4.2172)  class_acc: 0.2083 (0.3125)  loss_scale: 16384.0000 (39173.7744)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1007  max mem: 15572
Epoch: [27]  [2270/2809]  eta: 0:05:05  lr: 0.000013  min_lr: 0.000000  loss: 4.3219 (4.2180)  class_acc: 0.2500 (0.3127)  loss_scale: 16384.0000 (39073.4232)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1573  max mem: 15572
Epoch: [27]  [2280/2809]  eta: 0:04:59  lr: 0.000013  min_lr: 0.000000  loss: 4.2691 (4.2181)  class_acc: 0.2917 (0.3127)  loss_scale: 16384.0000 (38973.9518)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1062  max mem: 15572
Epoch: [27]  [2290/2809]  eta: 0:04:54  lr: 0.000013  min_lr: 0.000000  loss: 4.2691 (4.2183)  class_acc: 0.2917 (0.3128)  loss_scale: 16384.0000 (38875.3488)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0773  max mem: 15572
Epoch: [27]  [2300/2809]  eta: 0:04:48  lr: 0.000013  min_lr: 0.000000  loss: 4.2281 (4.2185)  class_acc: 0.2500 (0.3126)  loss_scale: 16384.0000 (38777.6028)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0995  max mem: 15572
[2025-01-16 03:36:17,966] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:36:17,966] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [2310/2809]  eta: 0:04:42  lr: 0.000013  min_lr: 0.000000  loss: 4.2168 (4.2187)  class_acc: 0.2083 (0.3125)  loss_scale: 16384.0000 (38716.1506)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0459  max mem: 15572
Epoch: [27]  [2320/2809]  eta: 0:04:37  lr: 0.000013  min_lr: 0.000000  loss: 4.2168 (4.2188)  class_acc: 0.2500 (0.3125)  loss_scale: 32768.0000 (38690.5231)  weight_decay: 0.0500 (0.0500)  time: 0.5224  data: 0.0730  max mem: 15572
Epoch: [27]  [2330/2809]  eta: 0:04:31  lr: 0.000013  min_lr: 0.000000  loss: 4.2492 (4.2192)  class_acc: 0.3333 (0.3127)  loss_scale: 32768.0000 (38665.1154)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1592  max mem: 15572
Epoch: [27]  [2340/2809]  eta: 0:04:25  lr: 0.000013  min_lr: 0.000000  loss: 4.2492 (4.2189)  class_acc: 0.3333 (0.3125)  loss_scale: 32768.0000 (38639.9248)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1444  max mem: 15572
Epoch: [27]  [2350/2809]  eta: 0:04:20  lr: 0.000013  min_lr: 0.000000  loss: 4.2387 (4.2189)  class_acc: 0.2917 (0.3124)  loss_scale: 32768.0000 (38614.9485)  weight_decay: 0.0500 (0.0500)  time: 0.7006  data: 0.2452  max mem: 15572
Epoch: [27]  [2360/2809]  eta: 0:04:14  lr: 0.000013  min_lr: 0.000000  loss: 4.2705 (4.2188)  class_acc: 0.2917 (0.3124)  loss_scale: 32768.0000 (38590.1838)  weight_decay: 0.0500 (0.0500)  time: 0.6557  data: 0.1875  max mem: 15572
Epoch: [27]  [2370/2809]  eta: 0:04:08  lr: 0.000013  min_lr: 0.000000  loss: 4.3132 (4.2193)  class_acc: 0.3333 (0.3122)  loss_scale: 32768.0000 (38565.6280)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0007  max mem: 15572
Epoch: [27]  [2380/2809]  eta: 0:04:03  lr: 0.000013  min_lr: 0.000000  loss: 4.3015 (4.2194)  class_acc: 0.3333 (0.3123)  loss_scale: 32768.0000 (38541.2785)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0508  max mem: 15572
Epoch: [27]  [2390/2809]  eta: 0:03:57  lr: 0.000013  min_lr: 0.000000  loss: 4.1960 (4.2191)  class_acc: 0.2917 (0.3123)  loss_scale: 32768.0000 (38517.1326)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0729  max mem: 15572
Epoch: [27]  [2400/2809]  eta: 0:03:51  lr: 0.000013  min_lr: 0.000000  loss: 4.2312 (4.2194)  class_acc: 0.2917 (0.3124)  loss_scale: 32768.0000 (38493.1878)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1085  max mem: 15572
Epoch: [27]  [2410/2809]  eta: 0:03:46  lr: 0.000013  min_lr: 0.000000  loss: 4.0767 (4.2180)  class_acc: 0.3750 (0.3127)  loss_scale: 32768.0000 (38469.4417)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0863  max mem: 15572
Epoch: [27]  [2420/2809]  eta: 0:03:40  lr: 0.000013  min_lr: 0.000000  loss: 4.0955 (4.2177)  class_acc: 0.3750 (0.3127)  loss_scale: 32768.0000 (38445.8918)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0276  max mem: 15572
Epoch: [27]  [2430/2809]  eta: 0:03:35  lr: 0.000013  min_lr: 0.000000  loss: 4.1191 (4.2173)  class_acc: 0.3333 (0.3128)  loss_scale: 32768.0000 (38422.5356)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1149  max mem: 15572
[2025-01-16 03:37:31,690] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:37:31,691] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 03:37:32,200] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78278
[2025-01-16 03:37:32,201] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:37:32,202] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [2440/2809]  eta: 0:03:29  lr: 0.000013  min_lr: 0.000000  loss: 4.1560 (4.2171)  class_acc: 0.3333 (0.3129)  loss_scale: 32768.0000 (38412.7948)  weight_decay: 0.0500 (0.0500)  time: 0.6557  data: 0.1716  max mem: 15572
Epoch: [27]  [2450/2809]  eta: 0:03:23  lr: 0.000013  min_lr: 0.000000  loss: 4.2264 (4.2171)  class_acc: 0.2917 (0.3129)  loss_scale: 32768.0000 (38389.7642)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1391  max mem: 15572
Epoch: [27]  [2460/2809]  eta: 0:03:18  lr: 0.000013  min_lr: 0.000000  loss: 4.2290 (4.2174)  class_acc: 0.2917 (0.3128)  loss_scale: 32768.0000 (38366.9208)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0955  max mem: 15572
Epoch: [27]  [2470/2809]  eta: 0:03:12  lr: 0.000013  min_lr: 0.000000  loss: 4.1670 (4.2172)  class_acc: 0.2917 (0.3129)  loss_scale: 32768.0000 (38344.2622)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0406  max mem: 15572
Epoch: [27]  [2480/2809]  eta: 0:03:06  lr: 0.000013  min_lr: 0.000000  loss: 4.1670 (4.2172)  class_acc: 0.2500 (0.3127)  loss_scale: 32768.0000 (38321.7864)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0472  max mem: 15572
Epoch: [27]  [2490/2809]  eta: 0:03:00  lr: 0.000013  min_lr: 0.000000  loss: 4.3118 (4.2174)  class_acc: 0.2917 (0.3128)  loss_scale: 32768.0000 (38299.4910)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0657  max mem: 15572
Epoch: [27]  [2500/2809]  eta: 0:02:55  lr: 0.000013  min_lr: 0.000000  loss: 4.3149 (4.2176)  class_acc: 0.3750 (0.3133)  loss_scale: 32768.0000 (38277.3739)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0724  max mem: 15572
Epoch: [27]  [2510/2809]  eta: 0:02:49  lr: 0.000013  min_lr: 0.000000  loss: 4.1884 (4.2173)  class_acc: 0.3750 (0.3134)  loss_scale: 32768.0000 (38255.4329)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0839  max mem: 15572
Epoch: [27]  [2520/2809]  eta: 0:02:43  lr: 0.000013  min_lr: 0.000000  loss: 4.1748 (4.2177)  class_acc: 0.2500 (0.3131)  loss_scale: 32768.0000 (38233.6660)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0478  max mem: 15572
Epoch: [27]  [2530/2809]  eta: 0:02:38  lr: 0.000013  min_lr: 0.000000  loss: 4.2905 (4.2176)  class_acc: 0.2083 (0.3129)  loss_scale: 32768.0000 (38212.0711)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0907  max mem: 15572
Epoch: [27]  [2540/2809]  eta: 0:02:32  lr: 0.000013  min_lr: 0.000000  loss: 4.2801 (4.2179)  class_acc: 0.3333 (0.3129)  loss_scale: 32768.0000 (38190.6462)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1103  max mem: 15572
Epoch: [27]  [2550/2809]  eta: 0:02:26  lr: 0.000013  min_lr: 0.000000  loss: 4.2467 (4.2184)  class_acc: 0.2917 (0.3129)  loss_scale: 32768.0000 (38169.3893)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1099  max mem: 15572
Epoch: [27]  [2560/2809]  eta: 0:02:21  lr: 0.000013  min_lr: 0.000000  loss: 4.2021 (4.2176)  class_acc: 0.3333 (0.3132)  loss_scale: 32768.0000 (38148.2983)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1424  max mem: 15572
[2025-01-16 03:38:44,758] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:38:44,759] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [2570/2809]  eta: 0:02:15  lr: 0.000013  min_lr: 0.000000  loss: 4.2232 (4.2177)  class_acc: 0.2917 (0.3132)  loss_scale: 32768.0000 (38216.5881)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1569  max mem: 15572
[2025-01-16 03:38:49,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78414
[2025-01-16 03:38:49,462] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:38:49,463] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [2580/2809]  eta: 0:02:09  lr: 0.000013  min_lr: 0.000000  loss: 4.2232 (4.2175)  class_acc: 0.2917 (0.3131)  loss_scale: 32768.0000 (38195.4777)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1206  max mem: 15572
Epoch: [27]  [2590/2809]  eta: 0:02:04  lr: 0.000013  min_lr: 0.000000  loss: 4.1061 (4.2171)  class_acc: 0.3333 (0.3133)  loss_scale: 32768.0000 (38174.5303)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1134  max mem: 15572
Epoch: [27]  [2600/2809]  eta: 0:01:58  lr: 0.000013  min_lr: 0.000000  loss: 4.1132 (4.2170)  class_acc: 0.2917 (0.3131)  loss_scale: 32768.0000 (38153.7439)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1311  max mem: 15572
Epoch: [27]  [2610/2809]  eta: 0:01:52  lr: 0.000012  min_lr: 0.000000  loss: 4.2637 (4.2171)  class_acc: 0.2500 (0.3130)  loss_scale: 32768.0000 (38133.1168)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0713  max mem: 15572
Epoch: [27]  [2620/2809]  eta: 0:01:47  lr: 0.000012  min_lr: 0.000000  loss: 4.2437 (4.2170)  class_acc: 0.3333 (0.3132)  loss_scale: 32768.0000 (38112.6471)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0637  max mem: 15572
Epoch: [27]  [2630/2809]  eta: 0:01:41  lr: 0.000012  min_lr: 0.000000  loss: 4.1153 (4.2167)  class_acc: 0.3333 (0.3134)  loss_scale: 32768.0000 (38092.3330)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1020  max mem: 15572
Epoch: [27]  [2640/2809]  eta: 0:01:35  lr: 0.000012  min_lr: 0.000000  loss: 4.2357 (4.2167)  class_acc: 0.2917 (0.3134)  loss_scale: 32768.0000 (38072.1727)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0895  max mem: 15572
Epoch: [27]  [2650/2809]  eta: 0:01:29  lr: 0.000012  min_lr: 0.000000  loss: 4.2629 (4.2169)  class_acc: 0.2917 (0.3132)  loss_scale: 32768.0000 (38052.1645)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0312  max mem: 15572
Epoch: [27]  [2660/2809]  eta: 0:01:24  lr: 0.000012  min_lr: 0.000000  loss: 4.3824 (4.2177)  class_acc: 0.2500 (0.3131)  loss_scale: 32768.0000 (38032.3067)  weight_decay: 0.0500 (0.0500)  time: 0.4884  data: 0.0008  max mem: 15572
Epoch: [27]  [2670/2809]  eta: 0:01:18  lr: 0.000012  min_lr: 0.000000  loss: 4.2900 (4.2176)  class_acc: 0.2917 (0.3131)  loss_scale: 32768.0000 (38012.5975)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0008  max mem: 15572
Epoch: [27]  [2680/2809]  eta: 0:01:12  lr: 0.000012  min_lr: 0.000000  loss: 4.1357 (4.2173)  class_acc: 0.2917 (0.3131)  loss_scale: 32768.0000 (37993.0354)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0008  max mem: 15572
Epoch: [27]  [2690/2809]  eta: 0:01:07  lr: 0.000012  min_lr: 0.000000  loss: 4.1847 (4.2176)  class_acc: 0.2917 (0.3131)  loss_scale: 32768.0000 (37973.6187)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.0727  max mem: 15572
[2025-01-16 03:39:58,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:39:58,840] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [2700/2809]  eta: 0:01:01  lr: 0.000012  min_lr: 0.000000  loss: 4.4077 (4.2184)  class_acc: 0.2917 (0.3129)  loss_scale: 32768.0000 (37966.4776)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1362  max mem: 15572
Epoch: [27]  [2710/2809]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 4.3339 (4.2185)  class_acc: 0.2917 (0.3128)  loss_scale: 65536.0000 (38068.1726)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.1514  max mem: 15572
Epoch: [27]  [2720/2809]  eta: 0:00:50  lr: 0.000012  min_lr: 0.000000  loss: 4.2338 (4.2190)  class_acc: 0.2917 (0.3126)  loss_scale: 65536.0000 (38169.1202)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.1546  max mem: 15572
Epoch: [27]  [2730/2809]  eta: 0:00:44  lr: 0.000012  min_lr: 0.000000  loss: 4.2557 (4.2191)  class_acc: 0.2917 (0.3125)  loss_scale: 65536.0000 (38269.3285)  weight_decay: 0.0500 (0.0500)  time: 0.6862  data: 0.1645  max mem: 15572
Epoch: [27]  [2740/2809]  eta: 0:00:39  lr: 0.000012  min_lr: 0.000000  loss: 4.4392 (4.2199)  class_acc: 0.2500 (0.3122)  loss_scale: 65536.0000 (38368.8055)  weight_decay: 0.0500 (0.0500)  time: 0.7108  data: 0.1791  max mem: 15572
Epoch: [27]  [2750/2809]  eta: 0:00:33  lr: 0.000012  min_lr: 0.000000  loss: 4.4392 (4.2201)  class_acc: 0.2917 (0.3123)  loss_scale: 65536.0000 (38467.5594)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.1381  max mem: 15572
Epoch: [27]  [2760/2809]  eta: 0:00:27  lr: 0.000012  min_lr: 0.000000  loss: 4.3029 (4.2204)  class_acc: 0.2917 (0.3122)  loss_scale: 65536.0000 (38565.5980)  weight_decay: 0.0500 (0.0500)  time: 0.6792  data: 0.1859  max mem: 15572
Epoch: [27]  [2770/2809]  eta: 0:00:22  lr: 0.000012  min_lr: 0.000000  loss: 4.2764 (4.2206)  class_acc: 0.2917 (0.3121)  loss_scale: 65536.0000 (38662.9289)  weight_decay: 0.0500 (0.0500)  time: 0.7037  data: 0.2024  max mem: 15572
Epoch: [27]  [2780/2809]  eta: 0:00:16  lr: 0.000012  min_lr: 0.000000  loss: 4.1981 (4.2207)  class_acc: 0.2917 (0.3120)  loss_scale: 65536.0000 (38759.5599)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.1482  max mem: 15572
Epoch: [27]  [2790/2809]  eta: 0:00:10  lr: 0.000012  min_lr: 0.000000  loss: 4.1704 (4.2207)  class_acc: 0.2917 (0.3119)  loss_scale: 65536.0000 (38855.4984)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0755  max mem: 15572
Epoch: [27]  [2800/2809]  eta: 0:00:05  lr: 0.000012  min_lr: 0.000000  loss: 4.3464 (4.2213)  class_acc: 0.2917 (0.3118)  loss_scale: 65536.0000 (38950.7519)  weight_decay: 0.0500 (0.0500)  time: 0.4007  data: 0.0003  max mem: 15572
Epoch: [27]  [2808/2809]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 4.4043 (4.2217)  class_acc: 0.2917 (0.3119)  loss_scale: 65536.0000 (39026.4664)  weight_decay: 0.0500 (0.0500)  time: 0.3912  data: 0.0003  max mem: 15572
Epoch: [27] Total time: 0:26:34 (0.5675 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 4.4043 (4.2217)  class_acc: 0.2917 (0.3119)  loss_scale: 65536.0000 (39026.4664)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:28:07  loss: 1.0118 (1.0118)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.2024  data: 5.9659  max mem: 15572
Val:  [ 10/272]  eta: 0:03:46  loss: 2.4326 (2.4777)  acc1: 50.0000 (47.4747)  acc5: 77.7778 (76.2626)  time: 0.8655  data: 0.6578  max mem: 15572
Val:  [ 20/272]  eta: 0:02:26  loss: 2.4962 (2.5720)  acc1: 50.0000 (46.5608)  acc5: 77.7778 (75.1323)  time: 0.3019  data: 0.1127  max mem: 15572
Val:  [ 30/272]  eta: 0:01:57  loss: 2.7081 (2.6190)  acc1: 44.4444 (42.2939)  acc5: 72.2222 (74.3728)  time: 0.2773  data: 0.0873  max mem: 15572
Val:  [ 40/272]  eta: 0:01:44  loss: 2.7115 (2.6327)  acc1: 27.7778 (39.9729)  acc5: 72.2222 (74.2547)  time: 0.3076  data: 0.1127  max mem: 15572
Val:  [ 50/272]  eta: 0:01:34  loss: 2.4513 (2.5620)  acc1: 38.8889 (41.5033)  acc5: 77.7778 (75.9259)  time: 0.3359  data: 0.1484  max mem: 15572
Val:  [ 60/272]  eta: 0:01:25  loss: 2.0377 (2.5098)  acc1: 55.5556 (42.8962)  acc5: 88.8889 (76.6849)  time: 0.3060  data: 0.1130  max mem: 15572
Val:  [ 70/272]  eta: 0:01:19  loss: 2.1485 (2.4674)  acc1: 55.5556 (45.1487)  acc5: 83.3333 (76.9953)  time: 0.3044  data: 0.1057  max mem: 15572
Val:  [ 80/272]  eta: 0:01:13  loss: 2.3244 (2.4716)  acc1: 50.0000 (44.9246)  acc5: 77.7778 (76.8176)  time: 0.3335  data: 0.1387  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 2.4620 (2.4720)  acc1: 44.4444 (45.4212)  acc5: 77.7778 (77.0452)  time: 0.3109  data: 0.1198  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 2.4620 (2.4964)  acc1: 44.4444 (44.8295)  acc5: 77.7778 (76.7877)  time: 0.3107  data: 0.1125  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 2.6322 (2.5478)  acc1: 33.3333 (43.3433)  acc5: 72.2222 (75.7257)  time: 0.3073  data: 0.1075  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 2.8735 (2.5823)  acc1: 27.7778 (42.5161)  acc5: 72.2222 (75.2066)  time: 0.2999  data: 0.1105  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 2.5195 (2.5573)  acc1: 44.4444 (43.3842)  acc5: 77.7778 (75.9118)  time: 0.3354  data: 0.1543  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 2.2358 (2.5594)  acc1: 55.5556 (43.7746)  acc5: 83.3333 (75.8077)  time: 0.3122  data: 0.1259  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 2.5444 (2.5600)  acc1: 38.8889 (43.4143)  acc5: 72.2222 (75.7910)  time: 0.2964  data: 0.1065  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 2.5444 (2.5606)  acc1: 38.8889 (43.6163)  acc5: 77.7778 (76.0179)  time: 0.3029  data: 0.1095  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.6260 (2.5747)  acc1: 38.8889 (43.1124)  acc5: 72.2222 (75.5036)  time: 0.2869  data: 0.0837  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.6260 (2.5631)  acc1: 33.3333 (43.0018)  acc5: 72.2222 (75.7520)  time: 0.3179  data: 0.1180  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.6389 (2.6029)  acc1: 27.7778 (41.8848)  acc5: 66.6667 (74.4619)  time: 0.3019  data: 0.1104  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.7375 (2.6037)  acc1: 33.3333 (41.8187)  acc5: 72.2222 (74.3781)  time: 0.2914  data: 0.1010  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.4322 (2.6096)  acc1: 38.8889 (41.4955)  acc5: 77.7778 (74.3286)  time: 0.2715  data: 0.0814  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.6657 (2.6028)  acc1: 38.8889 (41.6290)  acc5: 77.7778 (74.4093)  time: 0.2703  data: 0.0788  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.2410 (2.5905)  acc1: 50.0000 (42.4242)  acc5: 77.7778 (74.5551)  time: 0.2872  data: 0.0909  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1910 (2.5773)  acc1: 50.0000 (42.5772)  acc5: 83.3333 (74.9654)  time: 0.2794  data: 0.0915  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4802 (2.5834)  acc1: 38.8889 (42.2089)  acc5: 83.3333 (74.9225)  time: 0.3069  data: 0.1261  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9486 (2.5428)  acc1: 72.2222 (43.8484)  acc5: 88.8889 (75.5854)  time: 0.3257  data: 0.1345  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.0018 (2.5403)  acc1: 61.1111 (43.7679)  acc5: 88.8889 (75.7483)  time: 0.2498  data: 0.0736  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.0018 (2.5440)  acc1: 55.5556 (43.7641)  acc5: 88.8889 (75.7321)  time: 0.2417  data: 0.0736  max mem: 15572
Val: Total time: 0:01:27 (0.3210 s / it)
* Acc@1 43.764 Acc@5 75.732 loss 2.544
Accuracy of the network on the 4883 val videos: 43.8%
[2025-01-16 03:42:31,149] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 03:42:31,153] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 03:42:31,153] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 03:42:33,946] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 03:42:33,946] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 43.76%
Epoch: [28]  [   0/2809]  eta: 5:48:58  lr: 0.000012  min_lr: 0.000000  loss: 4.3195 (4.3195)  class_acc: 0.2083 (0.2083)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.4541  data: 6.8282  max mem: 15572
Epoch: [28]  [  10/2809]  eta: 1:02:24  lr: 0.000012  min_lr: 0.000000  loss: 4.1347 (4.1232)  class_acc: 0.2917 (0.3295)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3377  data: 0.8880  max mem: 15572
[2025-01-16 03:42:52,691] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:42:52,691] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:42:54,287] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78672
[2025-01-16 03:42:54,287] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:42:54,288] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [  20/2809]  eta: 0:45:00  lr: 0.000012  min_lr: 0.000000  loss: 4.1347 (4.1428)  class_acc: 0.2917 (0.3214)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.2038  max mem: 15572
[2025-01-16 03:42:55,107] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78674
[2025-01-16 03:42:55,107] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:42:55,108] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [  30/2809]  eta: 0:38:58  lr: 0.000012  min_lr: 0.000000  loss: 4.1192 (4.1222)  class_acc: 0.3333 (0.3280)  loss_scale: 65536.0000 (58136.7742)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1259  max mem: 15572
Epoch: [28]  [  40/2809]  eta: 0:35:25  lr: 0.000012  min_lr: 0.000000  loss: 4.0817 (4.1225)  class_acc: 0.3333 (0.3303)  loss_scale: 32768.0000 (51949.2683)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1208  max mem: 15572
Epoch: [28]  [  50/2809]  eta: 0:33:58  lr: 0.000012  min_lr: 0.000000  loss: 4.1001 (4.1248)  class_acc: 0.2917 (0.3145)  loss_scale: 32768.0000 (48188.2353)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1526  max mem: 15572
Epoch: [28]  [  60/2809]  eta: 0:32:21  lr: 0.000012  min_lr: 0.000000  loss: 4.1133 (4.1392)  class_acc: 0.2500 (0.3176)  loss_scale: 32768.0000 (45660.3279)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1483  max mem: 15572
Epoch: [28]  [  70/2809]  eta: 0:31:11  lr: 0.000012  min_lr: 0.000000  loss: 4.0568 (4.1439)  class_acc: 0.2500 (0.3099)  loss_scale: 32768.0000 (43844.5070)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.1061  max mem: 15572
Epoch: [28]  [  80/2809]  eta: 0:30:22  lr: 0.000012  min_lr: 0.000000  loss: 4.2822 (4.1642)  class_acc: 0.2917 (0.3056)  loss_scale: 32768.0000 (42477.0370)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.1174  max mem: 15572
Epoch: [28]  [  90/2809]  eta: 0:29:39  lr: 0.000012  min_lr: 0.000000  loss: 4.1985 (4.1511)  class_acc: 0.3333 (0.3100)  loss_scale: 32768.0000 (41410.1099)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1039  max mem: 15572
Epoch: [28]  [ 100/2809]  eta: 0:28:33  lr: 0.000012  min_lr: 0.000000  loss: 4.1984 (4.1617)  class_acc: 0.3333 (0.3086)  loss_scale: 32768.0000 (40554.4554)  weight_decay: 0.0500 (0.0500)  time: 0.4895  data: 0.0559  max mem: 15572
Epoch: [28]  [ 110/2809]  eta: 0:28:07  lr: 0.000012  min_lr: 0.000000  loss: 4.2692 (4.1660)  class_acc: 0.2500 (0.3093)  loss_scale: 32768.0000 (39852.9730)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0592  max mem: 15572
Epoch: [28]  [ 120/2809]  eta: 0:27:47  lr: 0.000012  min_lr: 0.000000  loss: 4.3291 (4.1810)  class_acc: 0.2500 (0.3041)  loss_scale: 32768.0000 (39267.4380)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1078  max mem: 15572
Epoch: [28]  [ 130/2809]  eta: 0:27:36  lr: 0.000012  min_lr: 0.000000  loss: 4.3279 (4.1803)  class_acc: 0.2917 (0.3066)  loss_scale: 32768.0000 (38771.2977)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1268  max mem: 15572
Epoch: [28]  [ 140/2809]  eta: 0:27:16  lr: 0.000012  min_lr: 0.000000  loss: 4.1619 (4.1726)  class_acc: 0.3750 (0.3121)  loss_scale: 32768.0000 (38345.5319)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1266  max mem: 15572
Epoch: [28]  [ 150/2809]  eta: 0:27:02  lr: 0.000012  min_lr: 0.000000  loss: 4.2172 (4.1755)  class_acc: 0.5000 (0.3206)  loss_scale: 32768.0000 (37976.1589)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1055  max mem: 15572
[2025-01-16 03:44:06,501] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:44:06,501] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [ 160/2809]  eta: 0:26:42  lr: 0.000012  min_lr: 0.000000  loss: 4.2172 (4.1793)  class_acc: 0.3750 (0.3188)  loss_scale: 32768.0000 (39687.9503)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0928  max mem: 15572
Epoch: [28]  [ 170/2809]  eta: 0:26:33  lr: 0.000012  min_lr: 0.000000  loss: 4.2088 (4.1827)  class_acc: 0.2917 (0.3224)  loss_scale: 65536.0000 (41199.5322)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1165  max mem: 15572
Epoch: [28]  [ 180/2809]  eta: 0:26:27  lr: 0.000012  min_lr: 0.000000  loss: 4.1194 (4.1809)  class_acc: 0.4167 (0.3269)  loss_scale: 65536.0000 (42544.0884)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1641  max mem: 15572
Epoch: [28]  [ 190/2809]  eta: 0:26:29  lr: 0.000012  min_lr: 0.000000  loss: 4.1830 (4.1901)  class_acc: 0.2500 (0.3235)  loss_scale: 65536.0000 (43747.8534)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.2023  max mem: 15572
Epoch: [28]  [ 200/2809]  eta: 0:26:18  lr: 0.000012  min_lr: 0.000000  loss: 4.3519 (4.1933)  class_acc: 0.2500 (0.3213)  loss_scale: 65536.0000 (44831.8408)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1679  max mem: 15572
Epoch: [28]  [ 210/2809]  eta: 0:26:04  lr: 0.000012  min_lr: 0.000000  loss: 4.3978 (4.1994)  class_acc: 0.2917 (0.3225)  loss_scale: 65536.0000 (45813.0806)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1075  max mem: 15572
[2025-01-16 03:44:41,918] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78864
[2025-01-16 03:44:41,918] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:44:41,918] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 220/2809]  eta: 0:25:39  lr: 0.000012  min_lr: 0.000000  loss: 4.2333 (4.2028)  class_acc: 0.3333 (0.3228)  loss_scale: 65536.0000 (45371.0769)  weight_decay: 0.0500 (0.0500)  time: 0.4868  data: 0.0483  max mem: 15572
Epoch: [28]  [ 230/2809]  eta: 0:25:29  lr: 0.000012  min_lr: 0.000000  loss: 4.1793 (4.2056)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (44825.4892)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0517  max mem: 15572
Epoch: [28]  [ 240/2809]  eta: 0:25:23  lr: 0.000012  min_lr: 0.000000  loss: 4.0482 (4.1994)  class_acc: 0.2917 (0.3204)  loss_scale: 32768.0000 (44325.1784)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1285  max mem: 15572
Epoch: [28]  [ 250/2809]  eta: 0:25:11  lr: 0.000012  min_lr: 0.000000  loss: 4.1442 (4.1974)  class_acc: 0.3333 (0.3204)  loss_scale: 32768.0000 (43864.7331)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1101  max mem: 15572
Epoch: [28]  [ 260/2809]  eta: 0:24:56  lr: 0.000012  min_lr: 0.000000  loss: 4.2202 (4.2017)  class_acc: 0.2917 (0.3196)  loss_scale: 32768.0000 (43439.5709)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0334  max mem: 15572
Epoch: [28]  [ 270/2809]  eta: 0:24:46  lr: 0.000012  min_lr: 0.000000  loss: 4.2775 (4.2046)  class_acc: 0.2500 (0.3183)  loss_scale: 32768.0000 (43045.7860)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0437  max mem: 15572
Epoch: [28]  [ 280/2809]  eta: 0:24:41  lr: 0.000012  min_lr: 0.000000  loss: 4.2793 (4.2113)  class_acc: 0.2500 (0.3164)  loss_scale: 32768.0000 (42680.0285)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1204  max mem: 15572
Epoch: [28]  [ 290/2809]  eta: 0:24:30  lr: 0.000012  min_lr: 0.000000  loss: 4.3254 (4.2097)  class_acc: 0.2917 (0.3184)  loss_scale: 32768.0000 (42339.4089)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1250  max mem: 15572
Epoch: [28]  [ 300/2809]  eta: 0:24:25  lr: 0.000012  min_lr: 0.000000  loss: 4.3404 (4.2126)  class_acc: 0.3333 (0.3180)  loss_scale: 32768.0000 (42021.4219)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1340  max mem: 15572
Epoch: [28]  [ 310/2809]  eta: 0:24:27  lr: 0.000012  min_lr: 0.000000  loss: 4.3613 (4.2132)  class_acc: 0.3333 (0.3181)  loss_scale: 32768.0000 (41723.8842)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1936  max mem: 15572
Epoch: [28]  [ 320/2809]  eta: 0:24:15  lr: 0.000012  min_lr: 0.000000  loss: 4.2096 (4.2076)  class_acc: 0.3750 (0.3207)  loss_scale: 32768.0000 (41444.8847)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1423  max mem: 15572
Epoch: [28]  [ 330/2809]  eta: 0:24:12  lr: 0.000012  min_lr: 0.000000  loss: 4.0729 (4.2046)  class_acc: 0.3750 (0.3221)  loss_scale: 32768.0000 (41182.7432)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1357  max mem: 15572
Epoch: [28]  [ 340/2809]  eta: 0:24:09  lr: 0.000012  min_lr: 0.000000  loss: 4.0729 (4.2043)  class_acc: 0.3333 (0.3221)  loss_scale: 32768.0000 (40935.9765)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1797  max mem: 15572
[2025-01-16 03:45:54,629] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:45:54,629] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 03:45:57,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=79000, skipped=491, lr=[1.1760208353860594e-07, 1.1760208353860594e-07, 1.680029764837228e-07, 1.680029764837228e-07, 2.40004252119604e-07, 2.40004252119604e-07, 3.4286321731372005e-07, 3.4286321731372005e-07, 4.898045961624572e-07, 4.898045961624572e-07, 6.997208516606532e-07, 6.997208516606532e-07, 9.99601216658076e-07, 9.99601216658076e-07, 1.428001738082966e-06, 1.428001738082966e-06, 2.0400024829756658e-06, 2.0400024829756658e-06, 2.9142892613938086e-06, 2.9142892613938086e-06, 4.1632703734197264e-06, 4.1632703734197264e-06, 5.947529104885324e-06, 5.947529104885324e-06, 8.496470149836178e-06, 8.496470149836178e-06, 1.2137814499765969e-05, 1.2137814499765969e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:45:57,356] [INFO] [timer.py:260:stop] epoch=0/micro_step=79000/global_step=79000, RunningAvgSamplesPerSec=27.911061917598957, CurrSamplesPerSec=31.813411255624683, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [28]  [ 350/2809]  eta: 0:23:52  lr: 0.000012  min_lr: 0.000000  loss: 4.2761 (4.2077)  class_acc: 0.3333 (0.3242)  loss_scale: 32768.0000 (41636.8319)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0889  max mem: 15572
Epoch: [28]  [ 360/2809]  eta: 0:23:43  lr: 0.000012  min_lr: 0.000000  loss: 4.2772 (4.2088)  class_acc: 0.3333 (0.3241)  loss_scale: 65536.0000 (42298.8587)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0394  max mem: 15572
Epoch: [28]  [ 370/2809]  eta: 0:23:38  lr: 0.000012  min_lr: 0.000000  loss: 4.1998 (4.2076)  class_acc: 0.3333 (0.3246)  loss_scale: 65536.0000 (42925.1968)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1101  max mem: 15572
[2025-01-16 03:46:10,256] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79023
[2025-01-16 03:46:10,256] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:46:10,257] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 380/2809]  eta: 0:23:31  lr: 0.000012  min_lr: 0.000000  loss: 4.1426 (4.2071)  class_acc: 0.2917 (0.3237)  loss_scale: 32768.0000 (42658.6037)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1485  max mem: 15572
Epoch: [28]  [ 390/2809]  eta: 0:23:33  lr: 0.000012  min_lr: 0.000000  loss: 4.1426 (4.2092)  class_acc: 0.2500 (0.3221)  loss_scale: 32768.0000 (42405.6471)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.1905  max mem: 15572
Epoch: [28]  [ 400/2809]  eta: 0:23:23  lr: 0.000012  min_lr: 0.000000  loss: 4.2076 (4.2095)  class_acc: 0.2917 (0.3235)  loss_scale: 32768.0000 (42165.3067)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1576  max mem: 15572
Epoch: [28]  [ 410/2809]  eta: 0:23:17  lr: 0.000012  min_lr: 0.000000  loss: 4.1610 (4.2095)  class_acc: 0.3750 (0.3234)  loss_scale: 32768.0000 (41936.6618)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1013  max mem: 15572
Epoch: [28]  [ 420/2809]  eta: 0:23:09  lr: 0.000012  min_lr: 0.000000  loss: 4.3084 (4.2111)  class_acc: 0.3333 (0.3242)  loss_scale: 32768.0000 (41718.8789)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1177  max mem: 15572
Epoch: [28]  [ 430/2809]  eta: 0:23:03  lr: 0.000012  min_lr: 0.000000  loss: 4.2942 (4.2073)  class_acc: 0.3333 (0.3251)  loss_scale: 32768.0000 (41511.2019)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1281  max mem: 15572
Epoch: [28]  [ 440/2809]  eta: 0:22:55  lr: 0.000012  min_lr: 0.000000  loss: 4.1184 (4.2051)  class_acc: 0.2917 (0.3251)  loss_scale: 32768.0000 (41312.9433)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1240  max mem: 15572
Epoch: [28]  [ 450/2809]  eta: 0:22:48  lr: 0.000012  min_lr: 0.000000  loss: 4.0036 (4.2010)  class_acc: 0.3333 (0.3254)  loss_scale: 32768.0000 (41123.4767)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0962  max mem: 15572
Epoch: [28]  [ 460/2809]  eta: 0:22:42  lr: 0.000012  min_lr: 0.000000  loss: 4.0957 (4.2010)  class_acc: 0.2917 (0.3240)  loss_scale: 32768.0000 (40942.2299)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1272  max mem: 15572
Epoch: [28]  [ 470/2809]  eta: 0:22:36  lr: 0.000012  min_lr: 0.000000  loss: 4.1899 (4.1992)  class_acc: 0.2500 (0.3231)  loss_scale: 32768.0000 (40768.6794)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1450  max mem: 15572
Epoch: [28]  [ 480/2809]  eta: 0:22:30  lr: 0.000012  min_lr: 0.000000  loss: 4.2795 (4.2022)  class_acc: 0.2917 (0.3229)  loss_scale: 32768.0000 (40602.3451)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1454  max mem: 15572
Epoch: [28]  [ 490/2809]  eta: 0:22:25  lr: 0.000012  min_lr: 0.000000  loss: 4.2539 (4.2023)  class_acc: 0.3333 (0.3221)  loss_scale: 32768.0000 (40442.7862)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1463  max mem: 15572
[2025-01-16 03:47:23,939] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:47:23,940] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [ 500/2809]  eta: 0:22:15  lr: 0.000012  min_lr: 0.000000  loss: 4.1678 (4.2012)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (40355.0020)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.1000  max mem: 15572
Epoch: [28]  [ 510/2809]  eta: 0:22:08  lr: 0.000012  min_lr: 0.000000  loss: 4.1654 (4.2009)  class_acc: 0.3333 (0.3221)  loss_scale: 65536.0000 (40847.7808)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0718  max mem: 15572
Epoch: [28]  [ 520/2809]  eta: 0:22:01  lr: 0.000012  min_lr: 0.000000  loss: 4.2743 (4.2021)  class_acc: 0.3333 (0.3226)  loss_scale: 65536.0000 (41321.6430)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0938  max mem: 15572
Epoch: [28]  [ 530/2809]  eta: 0:21:59  lr: 0.000012  min_lr: 0.000000  loss: 4.2129 (4.2019)  class_acc: 0.2917 (0.3225)  loss_scale: 65536.0000 (41777.6573)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1650  max mem: 15572
[2025-01-16 03:47:42,594] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79184
[2025-01-16 03:47:42,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:47:42,594] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 540/2809]  eta: 0:21:49  lr: 0.000012  min_lr: 0.000000  loss: 4.2129 (4.2021)  class_acc: 0.2917 (0.3219)  loss_scale: 65536.0000 (41671.6895)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1362  max mem: 15572
Epoch: [28]  [ 550/2809]  eta: 0:21:42  lr: 0.000012  min_lr: 0.000000  loss: 4.2894 (4.2043)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (41510.0980)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0814  max mem: 15572
Epoch: [28]  [ 560/2809]  eta: 0:21:39  lr: 0.000012  min_lr: 0.000000  loss: 4.2740 (4.2046)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (41354.2674)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1606  max mem: 15572
Epoch: [28]  [ 570/2809]  eta: 0:21:32  lr: 0.000012  min_lr: 0.000000  loss: 4.2022 (4.2045)  class_acc: 0.2500 (0.3215)  loss_scale: 32768.0000 (41203.8949)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1473  max mem: 15572
Epoch: [28]  [ 580/2809]  eta: 0:21:25  lr: 0.000012  min_lr: 0.000000  loss: 4.3285 (4.2076)  class_acc: 0.2500 (0.3201)  loss_scale: 32768.0000 (41058.6988)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0963  max mem: 15572
Epoch: [28]  [ 590/2809]  eta: 0:21:18  lr: 0.000012  min_lr: 0.000000  loss: 4.2381 (4.2057)  class_acc: 0.2500 (0.3192)  loss_scale: 32768.0000 (40918.4162)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1021  max mem: 15572
Epoch: [28]  [ 600/2809]  eta: 0:21:14  lr: 0.000012  min_lr: 0.000000  loss: 4.2139 (4.2084)  class_acc: 0.2500 (0.3177)  loss_scale: 32768.0000 (40782.8020)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1483  max mem: 15572
Epoch: [28]  [ 610/2809]  eta: 0:21:06  lr: 0.000012  min_lr: 0.000000  loss: 4.3461 (4.2104)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (40651.6268)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1474  max mem: 15572
Epoch: [28]  [ 620/2809]  eta: 0:21:01  lr: 0.000012  min_lr: 0.000000  loss: 4.3461 (4.2129)  class_acc: 0.3333 (0.3194)  loss_scale: 32768.0000 (40524.6763)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1128  max mem: 15572
Epoch: [28]  [ 630/2809]  eta: 0:20:56  lr: 0.000012  min_lr: 0.000000  loss: 4.1606 (4.2101)  class_acc: 0.3333 (0.3199)  loss_scale: 32768.0000 (40401.7496)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1334  max mem: 15572
Epoch: [28]  [ 640/2809]  eta: 0:20:49  lr: 0.000012  min_lr: 0.000000  loss: 4.2108 (4.2105)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (40282.6583)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1114  max mem: 15572
Epoch: [28]  [ 650/2809]  eta: 0:20:43  lr: 0.000012  min_lr: 0.000000  loss: 4.2220 (4.2103)  class_acc: 0.3333 (0.3201)  loss_scale: 32768.0000 (40167.2258)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1174  max mem: 15572
Epoch: [28]  [ 660/2809]  eta: 0:20:37  lr: 0.000012  min_lr: 0.000000  loss: 4.2007 (4.2114)  class_acc: 0.3333 (0.3196)  loss_scale: 32768.0000 (40055.2859)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1313  max mem: 15572
[2025-01-16 03:48:55,080] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:48:55,080] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [ 670/2809]  eta: 0:20:32  lr: 0.000012  min_lr: 0.000000  loss: 4.2007 (4.2110)  class_acc: 0.2500 (0.3187)  loss_scale: 32768.0000 (40435.0283)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1422  max mem: 15572
[2025-01-16 03:49:06,180] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79331
[2025-01-16 03:49:06,180] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:49:06,181] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 680/2809]  eta: 0:20:26  lr: 0.000012  min_lr: 0.000000  loss: 4.1290 (4.2102)  class_acc: 0.2500 (0.3185)  loss_scale: 65536.0000 (40707.3833)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1474  max mem: 15572
Epoch: [28]  [ 690/2809]  eta: 0:20:20  lr: 0.000012  min_lr: 0.000000  loss: 4.1290 (4.2094)  class_acc: 0.2500 (0.3177)  loss_scale: 32768.0000 (40592.4863)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1025  max mem: 15572
Epoch: [28]  [ 700/2809]  eta: 0:20:15  lr: 0.000012  min_lr: 0.000000  loss: 4.1846 (4.2089)  class_acc: 0.2500 (0.3175)  loss_scale: 32768.0000 (40480.8673)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1260  max mem: 15572
Epoch: [28]  [ 710/2809]  eta: 0:20:13  lr: 0.000012  min_lr: 0.000000  loss: 4.2867 (4.2098)  class_acc: 0.2500 (0.3172)  loss_scale: 32768.0000 (40372.3882)  weight_decay: 0.0500 (0.0500)  time: 0.6536  data: 0.2131  max mem: 15572
Epoch: [28]  [ 720/2809]  eta: 0:20:06  lr: 0.000012  min_lr: 0.000000  loss: 4.2708 (4.2106)  class_acc: 0.2500 (0.3163)  loss_scale: 32768.0000 (40266.9182)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1747  max mem: 15572
Epoch: [28]  [ 730/2809]  eta: 0:19:59  lr: 0.000012  min_lr: 0.000000  loss: 4.1546 (4.2083)  class_acc: 0.2500 (0.3163)  loss_scale: 32768.0000 (40164.3338)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0900  max mem: 15572
Epoch: [28]  [ 740/2809]  eta: 0:19:53  lr: 0.000012  min_lr: 0.000000  loss: 4.0704 (4.2083)  class_acc: 0.2917 (0.3164)  loss_scale: 32768.0000 (40064.5182)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1235  max mem: 15572
Epoch: [28]  [ 750/2809]  eta: 0:19:44  lr: 0.000012  min_lr: 0.000000  loss: 4.0704 (4.2054)  class_acc: 0.2917 (0.3166)  loss_scale: 32768.0000 (39967.3609)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.1011  max mem: 15572
Epoch: [28]  [ 760/2809]  eta: 0:19:39  lr: 0.000012  min_lr: 0.000000  loss: 4.1204 (4.2050)  class_acc: 0.3333 (0.3167)  loss_scale: 32768.0000 (39872.7569)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0964  max mem: 15572
Epoch: [28]  [ 770/2809]  eta: 0:19:32  lr: 0.000012  min_lr: 0.000000  loss: 4.1306 (4.2054)  class_acc: 0.3333 (0.3170)  loss_scale: 32768.0000 (39780.6070)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1035  max mem: 15572
Epoch: [28]  [ 780/2809]  eta: 0:19:24  lr: 0.000012  min_lr: 0.000000  loss: 4.3807 (4.2087)  class_acc: 0.2917 (0.3162)  loss_scale: 32768.0000 (39690.8169)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0554  max mem: 15572
Epoch: [28]  [ 790/2809]  eta: 0:19:20  lr: 0.000012  min_lr: 0.000000  loss: 4.3807 (4.2094)  class_acc: 0.2500 (0.3163)  loss_scale: 32768.0000 (39603.2971)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1103  max mem: 15572
Epoch: [28]  [ 800/2809]  eta: 0:19:13  lr: 0.000012  min_lr: 0.000000  loss: 4.1788 (4.2092)  class_acc: 0.2917 (0.3161)  loss_scale: 32768.0000 (39517.9625)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1230  max mem: 15572
[2025-01-16 03:50:18,135] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:50:18,135] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [ 810/2809]  eta: 0:19:05  lr: 0.000012  min_lr: 0.000000  loss: 4.3536 (4.2121)  class_acc: 0.2083 (0.3150)  loss_scale: 32768.0000 (39555.9457)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0683  max mem: 15572
Epoch: [28]  [ 820/2809]  eta: 0:18:59  lr: 0.000012  min_lr: 0.000000  loss: 4.3480 (4.2119)  class_acc: 0.2917 (0.3153)  loss_scale: 65536.0000 (39872.3898)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0696  max mem: 15572
Epoch: [28]  [ 830/2809]  eta: 0:18:51  lr: 0.000012  min_lr: 0.000000  loss: 4.3318 (4.2131)  class_acc: 0.2917 (0.3147)  loss_scale: 65536.0000 (40181.2178)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0774  max mem: 15572
[2025-01-16 03:50:29,880] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79483
[2025-01-16 03:50:29,880] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:50:29,880] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 840/2809]  eta: 0:18:43  lr: 0.000012  min_lr: 0.000000  loss: 4.2957 (4.2138)  class_acc: 0.2917 (0.3146)  loss_scale: 32768.0000 (40093.0702)  weight_decay: 0.0500 (0.0500)  time: 0.4703  data: 0.0356  max mem: 15572
Epoch: [28]  [ 850/2809]  eta: 0:18:37  lr: 0.000012  min_lr: 0.000000  loss: 4.2647 (4.2146)  class_acc: 0.2917 (0.3148)  loss_scale: 32768.0000 (40006.9941)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0557  max mem: 15572
Epoch: [28]  [ 860/2809]  eta: 0:18:30  lr: 0.000012  min_lr: 0.000000  loss: 4.2647 (4.2151)  class_acc: 0.2500 (0.3147)  loss_scale: 32768.0000 (39922.9175)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0957  max mem: 15572
Epoch: [28]  [ 870/2809]  eta: 0:18:25  lr: 0.000012  min_lr: 0.000000  loss: 4.2444 (4.2149)  class_acc: 0.2917 (0.3144)  loss_scale: 32768.0000 (39840.7715)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1316  max mem: 15572
Epoch: [28]  [ 880/2809]  eta: 0:18:19  lr: 0.000012  min_lr: 0.000000  loss: 4.1030 (4.2133)  class_acc: 0.3333 (0.3152)  loss_scale: 32768.0000 (39760.4904)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1295  max mem: 15572
Epoch: [28]  [ 890/2809]  eta: 0:18:15  lr: 0.000012  min_lr: 0.000000  loss: 4.1545 (4.2132)  class_acc: 0.3333 (0.3154)  loss_scale: 32768.0000 (39682.0112)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1363  max mem: 15572
Epoch: [28]  [ 900/2809]  eta: 0:18:10  lr: 0.000012  min_lr: 0.000000  loss: 4.2032 (4.2131)  class_acc: 0.3333 (0.3155)  loss_scale: 32768.0000 (39605.2741)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1588  max mem: 15572
Epoch: [28]  [ 910/2809]  eta: 0:18:05  lr: 0.000012  min_lr: 0.000000  loss: 4.1878 (4.2116)  class_acc: 0.3333 (0.3158)  loss_scale: 32768.0000 (39530.2217)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1409  max mem: 15572
Epoch: [28]  [ 920/2809]  eta: 0:17:58  lr: 0.000012  min_lr: 0.000000  loss: 4.1580 (4.2126)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (39456.7991)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1264  max mem: 15572
Epoch: [28]  [ 930/2809]  eta: 0:17:53  lr: 0.000012  min_lr: 0.000000  loss: 4.1835 (4.2121)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (39384.9538)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1155  max mem: 15572
Epoch: [28]  [ 940/2809]  eta: 0:17:47  lr: 0.000012  min_lr: 0.000000  loss: 4.1692 (4.2103)  class_acc: 0.2500 (0.3150)  loss_scale: 32768.0000 (39314.6355)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1143  max mem: 15572
Epoch: [28]  [ 950/2809]  eta: 0:17:43  lr: 0.000012  min_lr: 0.000000  loss: 4.1583 (4.2103)  class_acc: 0.3333 (0.3157)  loss_scale: 32768.0000 (39245.7960)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1543  max mem: 15572
[2025-01-16 03:51:43,307] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:51:43,307] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [ 960/2809]  eta: 0:17:36  lr: 0.000012  min_lr: 0.000000  loss: 4.2086 (4.2100)  class_acc: 0.3750 (0.3165)  loss_scale: 32768.0000 (39212.4870)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1374  max mem: 15572
Epoch: [28]  [ 970/2809]  eta: 0:17:31  lr: 0.000012  min_lr: 0.000000  loss: 4.2976 (4.2117)  class_acc: 0.3333 (0.3159)  loss_scale: 65536.0000 (39483.5839)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1065  max mem: 15572
Epoch: [28]  [ 980/2809]  eta: 0:17:24  lr: 0.000012  min_lr: 0.000000  loss: 4.2223 (4.2115)  class_acc: 0.2917 (0.3163)  loss_scale: 65536.0000 (39749.1539)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1167  max mem: 15572
[2025-01-16 03:51:58,453] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79637
[2025-01-16 03:51:58,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:51:58,453] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 990/2809]  eta: 0:17:19  lr: 0.000012  min_lr: 0.000000  loss: 4.1879 (4.2106)  class_acc: 0.3750 (0.3169)  loss_scale: 65536.0000 (39810.9707)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1270  max mem: 15572
Epoch: [28]  [1000/2809]  eta: 0:17:14  lr: 0.000012  min_lr: 0.000000  loss: 4.2684 (4.2124)  class_acc: 0.3333 (0.3166)  loss_scale: 32768.0000 (39740.6114)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1505  max mem: 15572
Epoch: [28]  [1010/2809]  eta: 0:17:07  lr: 0.000012  min_lr: 0.000000  loss: 4.2077 (4.2108)  class_acc: 0.2917 (0.3168)  loss_scale: 32768.0000 (39671.6439)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0910  max mem: 15572
Epoch: [28]  [1020/2809]  eta: 0:17:03  lr: 0.000012  min_lr: 0.000000  loss: 4.1595 (4.2096)  class_acc: 0.3333 (0.3172)  loss_scale: 32768.0000 (39604.0274)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1467  max mem: 15572
Epoch: [28]  [1030/2809]  eta: 0:16:56  lr: 0.000012  min_lr: 0.000000  loss: 4.1139 (4.2091)  class_acc: 0.3333 (0.3178)  loss_scale: 32768.0000 (39537.7226)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1554  max mem: 15572
Epoch: [28]  [1040/2809]  eta: 0:16:51  lr: 0.000012  min_lr: 0.000000  loss: 4.3070 (4.2112)  class_acc: 0.3333 (0.3173)  loss_scale: 32768.0000 (39472.6916)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1150  max mem: 15572
Epoch: [28]  [1050/2809]  eta: 0:16:44  lr: 0.000012  min_lr: 0.000000  loss: 4.1687 (4.2103)  class_acc: 0.3750 (0.3183)  loss_scale: 32768.0000 (39408.8982)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1150  max mem: 15572
Epoch: [28]  [1060/2809]  eta: 0:16:38  lr: 0.000012  min_lr: 0.000000  loss: 4.0540 (4.2096)  class_acc: 0.3750 (0.3180)  loss_scale: 32768.0000 (39346.3073)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0862  max mem: 15572
Epoch: [28]  [1070/2809]  eta: 0:16:33  lr: 0.000012  min_lr: 0.000000  loss: 4.1565 (4.2084)  class_acc: 0.3750 (0.3188)  loss_scale: 32768.0000 (39284.8852)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1391  max mem: 15572
Epoch: [28]  [1080/2809]  eta: 0:16:26  lr: 0.000012  min_lr: 0.000000  loss: 4.1565 (4.2091)  class_acc: 0.3750 (0.3192)  loss_scale: 32768.0000 (39224.5994)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1026  max mem: 15572
Epoch: [28]  [1090/2809]  eta: 0:16:22  lr: 0.000012  min_lr: 0.000000  loss: 4.3420 (4.2111)  class_acc: 0.2917 (0.3187)  loss_scale: 32768.0000 (39165.4189)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1301  max mem: 15572
Epoch: [28]  [1100/2809]  eta: 0:16:15  lr: 0.000012  min_lr: 0.000000  loss: 4.2490 (4.2106)  class_acc: 0.2500 (0.3186)  loss_scale: 32768.0000 (39107.3134)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1430  max mem: 15572
Epoch: [28]  [1110/2809]  eta: 0:16:11  lr: 0.000012  min_lr: 0.000000  loss: 4.1956 (4.2112)  class_acc: 0.2917 (0.3186)  loss_scale: 32768.0000 (39050.2538)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1523  max mem: 15572
[2025-01-16 03:53:12,364] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:53:12,365] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1120/2809]  eta: 0:16:05  lr: 0.000012  min_lr: 0.000000  loss: 3.9865 (4.2089)  class_acc: 0.3333 (0.3189)  loss_scale: 32768.0000 (39198.8296)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1635  max mem: 15572
Epoch: [28]  [1130/2809]  eta: 0:15:59  lr: 0.000012  min_lr: 0.000000  loss: 4.0966 (4.2081)  class_acc: 0.2917 (0.3186)  loss_scale: 65536.0000 (39431.6958)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.1132  max mem: 15572
[2025-01-16 03:53:24,062] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79789
[2025-01-16 03:53:24,063] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:53:24,063] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1140/2809]  eta: 0:15:52  lr: 0.000012  min_lr: 0.000000  loss: 4.1699 (4.2076)  class_acc: 0.2917 (0.3192)  loss_scale: 65536.0000 (39545.6056)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.1021  max mem: 15572
Epoch: [28]  [1150/2809]  eta: 0:15:47  lr: 0.000012  min_lr: 0.000000  loss: 4.1927 (4.2079)  class_acc: 0.3750 (0.3195)  loss_scale: 32768.0000 (39486.7211)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0997  max mem: 15572
Epoch: [28]  [1160/2809]  eta: 0:15:41  lr: 0.000012  min_lr: 0.000000  loss: 4.2742 (4.2091)  class_acc: 0.2917 (0.3188)  loss_scale: 32768.0000 (39428.8510)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1192  max mem: 15572
Epoch: [28]  [1170/2809]  eta: 0:15:34  lr: 0.000012  min_lr: 0.000000  loss: 4.2866 (4.2092)  class_acc: 0.2500 (0.3182)  loss_scale: 32768.0000 (39371.9693)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0758  max mem: 15572
Epoch: [28]  [1180/2809]  eta: 0:15:28  lr: 0.000012  min_lr: 0.000000  loss: 4.2420 (4.2084)  class_acc: 0.2500 (0.3180)  loss_scale: 32768.0000 (39316.0508)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0852  max mem: 15572
Epoch: [28]  [1190/2809]  eta: 0:15:23  lr: 0.000012  min_lr: 0.000000  loss: 4.1316 (4.2082)  class_acc: 0.2917 (0.3181)  loss_scale: 32768.0000 (39261.0714)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1286  max mem: 15572
Epoch: [28]  [1200/2809]  eta: 0:15:16  lr: 0.000012  min_lr: 0.000000  loss: 4.2332 (4.2090)  class_acc: 0.2500 (0.3175)  loss_scale: 32768.0000 (39207.0075)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1126  max mem: 15572
Epoch: [28]  [1210/2809]  eta: 0:15:10  lr: 0.000012  min_lr: 0.000000  loss: 4.2194 (4.2074)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (39153.8365)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0959  max mem: 15572
Epoch: [28]  [1220/2809]  eta: 0:15:05  lr: 0.000012  min_lr: 0.000000  loss: 4.1852 (4.2079)  class_acc: 0.3750 (0.3176)  loss_scale: 32768.0000 (39101.5364)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1251  max mem: 15572
Epoch: [28]  [1230/2809]  eta: 0:14:59  lr: 0.000012  min_lr: 0.000000  loss: 4.1827 (4.2066)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (39050.0861)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1264  max mem: 15572
Epoch: [28]  [1240/2809]  eta: 0:14:51  lr: 0.000012  min_lr: 0.000000  loss: 4.0323 (4.2048)  class_acc: 0.2083 (0.3172)  loss_scale: 32768.0000 (38999.4649)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0597  max mem: 15572
Epoch: [28]  [1250/2809]  eta: 0:14:45  lr: 0.000012  min_lr: 0.000000  loss: 4.1025 (4.2052)  class_acc: 0.2500 (0.3168)  loss_scale: 32768.0000 (38949.6531)  weight_decay: 0.0500 (0.0500)  time: 0.4736  data: 0.0421  max mem: 15572
Epoch: [28]  [1260/2809]  eta: 0:14:39  lr: 0.000012  min_lr: 0.000000  loss: 4.2705 (4.2059)  class_acc: 0.2917 (0.3168)  loss_scale: 32768.0000 (38900.6312)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.1078  max mem: 15572
[2025-01-16 03:54:34,571] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:54:34,572] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 03:54:36,470] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79922
[2025-01-16 03:54:36,470] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:54:36,471] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1270/2809]  eta: 0:14:34  lr: 0.000012  min_lr: 0.000000  loss: 4.3222 (4.2066)  class_acc: 0.2917 (0.3166)  loss_scale: 32768.0000 (38955.5059)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1314  max mem: 15572
Epoch: [28]  [1280/2809]  eta: 0:14:29  lr: 0.000012  min_lr: 0.000000  loss: 4.3428 (4.2077)  class_acc: 0.2500 (0.3163)  loss_scale: 32768.0000 (38907.2037)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1271  max mem: 15572
Epoch: [28]  [1290/2809]  eta: 0:14:22  lr: 0.000012  min_lr: 0.000000  loss: 4.2736 (4.2075)  class_acc: 0.2500 (0.3160)  loss_scale: 32768.0000 (38859.6499)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1192  max mem: 15572
Epoch: [28]  [1300/2809]  eta: 0:14:16  lr: 0.000012  min_lr: 0.000000  loss: 4.2519 (4.2076)  class_acc: 0.2500 (0.3155)  loss_scale: 32768.0000 (38812.8271)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0936  max mem: 15572
Epoch: [28]  [1310/2809]  eta: 0:14:10  lr: 0.000012  min_lr: 0.000000  loss: 4.1829 (4.2073)  class_acc: 0.2500 (0.3154)  loss_scale: 32768.0000 (38766.7185)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0931  max mem: 15572
Epoch: [28]  [1320/2809]  eta: 0:14:05  lr: 0.000012  min_lr: 0.000000  loss: 4.2475 (4.2081)  class_acc: 0.2083 (0.3145)  loss_scale: 32768.0000 (38721.3081)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1402  max mem: 15572
Epoch: [28]  [1330/2809]  eta: 0:13:59  lr: 0.000011  min_lr: 0.000000  loss: 4.2953 (4.2080)  class_acc: 0.2500 (0.3143)  loss_scale: 32768.0000 (38676.5800)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1609  max mem: 15572
Epoch: [28]  [1340/2809]  eta: 0:13:54  lr: 0.000011  min_lr: 0.000000  loss: 4.2573 (4.2081)  class_acc: 0.2500 (0.3142)  loss_scale: 32768.0000 (38632.5190)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1533  max mem: 15572
[2025-01-16 03:55:21,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=80000, skipped=498, lr=[1.1131416451469152e-07, 1.1131416451469152e-07, 1.590202350209879e-07, 1.590202350209879e-07, 2.2717176431569702e-07, 2.2717176431569702e-07, 3.245310918795672e-07, 3.245310918795672e-07, 4.636158455422389e-07, 4.636158455422389e-07, 6.62308350774627e-07, 6.62308350774627e-07, 9.461547868208958e-07, 9.461547868208958e-07, 1.3516496954584228e-06, 1.3516496954584228e-06, 1.930928136369175e-06, 1.930928136369175e-06, 2.758468766241679e-06, 2.758468766241679e-06, 3.9406696660595414e-06, 3.9406696660595414e-06, 5.629528094370775e-06, 5.629528094370775e-06, 8.04218299195825e-06, 8.04218299195825e-06, 1.1488832845654643e-05, 1.1488832845654643e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:55:21,047] [INFO] [timer.py:260:stop] epoch=0/micro_step=80000/global_step=80000, RunningAvgSamplesPerSec=27.920204746614026, CurrSamplesPerSec=30.945594593398162, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [28]  [1350/2809]  eta: 0:13:49  lr: 0.000011  min_lr: 0.000000  loss: 4.2731 (4.2075)  class_acc: 0.2917 (0.3143)  loss_scale: 32768.0000 (38589.1103)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1716  max mem: 15572
Epoch: [28]  [1360/2809]  eta: 0:13:43  lr: 0.000011  min_lr: 0.000000  loss: 4.2265 (4.2063)  class_acc: 0.2500 (0.3138)  loss_scale: 32768.0000 (38546.3395)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1268  max mem: 15572
Epoch: [28]  [1370/2809]  eta: 0:13:37  lr: 0.000011  min_lr: 0.000000  loss: 4.2246 (4.2070)  class_acc: 0.2500 (0.3138)  loss_scale: 32768.0000 (38504.1926)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0646  max mem: 15572
Epoch: [28]  [1380/2809]  eta: 0:13:32  lr: 0.000011  min_lr: 0.000000  loss: 4.2594 (4.2067)  class_acc: 0.3333 (0.3139)  loss_scale: 32768.0000 (38462.6560)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1418  max mem: 15572
Epoch: [28]  [1390/2809]  eta: 0:13:25  lr: 0.000011  min_lr: 0.000000  loss: 4.1745 (4.2071)  class_acc: 0.2500 (0.3134)  loss_scale: 32768.0000 (38421.7168)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1313  max mem: 15572
[2025-01-16 03:55:50,777] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:55:50,778] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1400/2809]  eta: 0:13:21  lr: 0.000011  min_lr: 0.000000  loss: 4.1125 (4.2062)  class_acc: 0.2500 (0.3135)  loss_scale: 32768.0000 (38428.1399)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1359  max mem: 15572
Epoch: [28]  [1410/2809]  eta: 0:13:15  lr: 0.000011  min_lr: 0.000000  loss: 4.0438 (4.2054)  class_acc: 0.3333 (0.3141)  loss_scale: 65536.0000 (38620.2580)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1571  max mem: 15572
Epoch: [28]  [1420/2809]  eta: 0:13:09  lr: 0.000011  min_lr: 0.000000  loss: 4.1283 (4.2059)  class_acc: 0.3333 (0.3142)  loss_scale: 65536.0000 (38809.6721)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0913  max mem: 15572
[2025-01-16 03:56:07,301] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80081
[2025-01-16 03:56:07,302] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:56:07,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1430/2809]  eta: 0:13:03  lr: 0.000011  min_lr: 0.000000  loss: 4.2221 (4.2062)  class_acc: 0.3333 (0.3143)  loss_scale: 65536.0000 (38950.6415)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1205  max mem: 15572
Epoch: [28]  [1440/2809]  eta: 0:12:57  lr: 0.000011  min_lr: 0.000000  loss: 4.2065 (4.2072)  class_acc: 0.2500 (0.3140)  loss_scale: 32768.0000 (38907.7363)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1378  max mem: 15572
Epoch: [28]  [1450/2809]  eta: 0:12:51  lr: 0.000011  min_lr: 0.000000  loss: 4.2021 (4.2057)  class_acc: 0.2917 (0.3143)  loss_scale: 32768.0000 (38865.4225)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0859  max mem: 15572
Epoch: [28]  [1460/2809]  eta: 0:12:46  lr: 0.000011  min_lr: 0.000000  loss: 4.2260 (4.2063)  class_acc: 0.2917 (0.3139)  loss_scale: 32768.0000 (38823.6879)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1242  max mem: 15572
Epoch: [28]  [1470/2809]  eta: 0:12:41  lr: 0.000011  min_lr: 0.000000  loss: 4.2736 (4.2066)  class_acc: 0.2917 (0.3138)  loss_scale: 32768.0000 (38782.5207)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.2072  max mem: 15572
Epoch: [28]  [1480/2809]  eta: 0:12:34  lr: 0.000011  min_lr: 0.000000  loss: 4.1049 (4.2057)  class_acc: 0.2917 (0.3136)  loss_scale: 32768.0000 (38741.9095)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1051  max mem: 15572
Epoch: [28]  [1490/2809]  eta: 0:12:28  lr: 0.000011  min_lr: 0.000000  loss: 4.0792 (4.2050)  class_acc: 0.2500 (0.3136)  loss_scale: 32768.0000 (38701.8431)  weight_decay: 0.0500 (0.0500)  time: 0.4689  data: 0.0060  max mem: 15572
Epoch: [28]  [1500/2809]  eta: 0:12:22  lr: 0.000011  min_lr: 0.000000  loss: 4.1277 (4.2046)  class_acc: 0.2917 (0.3138)  loss_scale: 32768.0000 (38662.3105)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0470  max mem: 15572
Epoch: [28]  [1510/2809]  eta: 0:12:16  lr: 0.000011  min_lr: 0.000000  loss: 4.1277 (4.2042)  class_acc: 0.3333 (0.3135)  loss_scale: 32768.0000 (38623.3011)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1219  max mem: 15572
Epoch: [28]  [1520/2809]  eta: 0:12:11  lr: 0.000011  min_lr: 0.000000  loss: 4.1305 (4.2039)  class_acc: 0.2500 (0.3138)  loss_scale: 32768.0000 (38584.8047)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1536  max mem: 15572
Epoch: [28]  [1530/2809]  eta: 0:12:06  lr: 0.000011  min_lr: 0.000000  loss: 3.9846 (4.2029)  class_acc: 0.3333 (0.3143)  loss_scale: 32768.0000 (38546.8112)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1508  max mem: 15572
Epoch: [28]  [1540/2809]  eta: 0:12:00  lr: 0.000011  min_lr: 0.000000  loss: 4.0947 (4.2028)  class_acc: 0.2917 (0.3139)  loss_scale: 32768.0000 (38509.3108)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1281  max mem: 15572
Epoch: [28]  [1550/2809]  eta: 0:11:53  lr: 0.000011  min_lr: 0.000000  loss: 4.1826 (4.2027)  class_acc: 0.2917 (0.3143)  loss_scale: 32768.0000 (38472.2940)  weight_decay: 0.0500 (0.0500)  time: 0.5028  data: 0.0507  max mem: 15572
[2025-01-16 03:57:18,620] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:57:18,620] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1560/2809]  eta: 0:11:48  lr: 0.000011  min_lr: 0.000000  loss: 4.2953 (4.2036)  class_acc: 0.3333 (0.3139)  loss_scale: 32768.0000 (38498.7265)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0736  max mem: 15572
[2025-01-16 03:57:21,913] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80217
[2025-01-16 03:57:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:57:21,915] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1570/2809]  eta: 0:11:41  lr: 0.000011  min_lr: 0.000000  loss: 4.3578 (4.2041)  class_acc: 0.2500 (0.3138)  loss_scale: 32768.0000 (38545.6805)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0920  max mem: 15572
Epoch: [28]  [1580/2809]  eta: 0:11:36  lr: 0.000011  min_lr: 0.000000  loss: 4.3278 (4.2043)  class_acc: 0.3333 (0.3139)  loss_scale: 32768.0000 (38509.1360)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0820  max mem: 15572
Epoch: [28]  [1590/2809]  eta: 0:11:31  lr: 0.000011  min_lr: 0.000000  loss: 4.1833 (4.2044)  class_acc: 0.3333 (0.3138)  loss_scale: 32768.0000 (38473.0509)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1667  max mem: 15572
Epoch: [28]  [1600/2809]  eta: 0:11:25  lr: 0.000011  min_lr: 0.000000  loss: 4.1683 (4.2041)  class_acc: 0.2917 (0.3137)  loss_scale: 32768.0000 (38437.4166)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1751  max mem: 15572
Epoch: [28]  [1610/2809]  eta: 0:11:20  lr: 0.000011  min_lr: 0.000000  loss: 4.1553 (4.2043)  class_acc: 0.2917 (0.3134)  loss_scale: 32768.0000 (38402.2247)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1340  max mem: 15572
Epoch: [28]  [1620/2809]  eta: 0:11:14  lr: 0.000011  min_lr: 0.000000  loss: 4.2132 (4.2048)  class_acc: 0.2917 (0.3136)  loss_scale: 32768.0000 (38367.4670)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0903  max mem: 15572
Epoch: [28]  [1630/2809]  eta: 0:11:08  lr: 0.000011  min_lr: 0.000000  loss: 4.1640 (4.2043)  class_acc: 0.3333 (0.3134)  loss_scale: 32768.0000 (38333.1355)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0757  max mem: 15572
Epoch: [28]  [1640/2809]  eta: 0:11:02  lr: 0.000011  min_lr: 0.000000  loss: 4.1061 (4.2039)  class_acc: 0.3333 (0.3133)  loss_scale: 32768.0000 (38299.2224)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.1020  max mem: 15572
Epoch: [28]  [1650/2809]  eta: 0:10:55  lr: 0.000011  min_lr: 0.000000  loss: 4.1199 (4.2044)  class_acc: 0.2917 (0.3130)  loss_scale: 32768.0000 (38265.7202)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0876  max mem: 15572
Epoch: [28]  [1660/2809]  eta: 0:10:50  lr: 0.000011  min_lr: 0.000000  loss: 4.1537 (4.2037)  class_acc: 0.3333 (0.3132)  loss_scale: 32768.0000 (38232.6213)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1167  max mem: 15572
Epoch: [28]  [1670/2809]  eta: 0:10:45  lr: 0.000011  min_lr: 0.000000  loss: 4.1793 (4.2038)  class_acc: 0.3333 (0.3131)  loss_scale: 32768.0000 (38199.9186)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1512  max mem: 15572
Epoch: [28]  [1680/2809]  eta: 0:10:38  lr: 0.000011  min_lr: 0.000000  loss: 4.2083 (4.2038)  class_acc: 0.2500 (0.3130)  loss_scale: 32768.0000 (38167.6050)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0853  max mem: 15572
Epoch: [28]  [1690/2809]  eta: 0:10:33  lr: 0.000011  min_lr: 0.000000  loss: 4.2141 (4.2041)  class_acc: 0.2917 (0.3129)  loss_scale: 32768.0000 (38135.6736)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1126  max mem: 15572
[2025-01-16 03:58:33,916] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:58:33,916] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1700/2809]  eta: 0:10:27  lr: 0.000011  min_lr: 0.000000  loss: 4.1874 (4.2037)  class_acc: 0.2917 (0.3129)  loss_scale: 32768.0000 (38238.9653)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1342  max mem: 15572
[2025-01-16 03:58:42,659] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80361
[2025-01-16 03:58:42,659] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:58:42,659] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1710/2809]  eta: 0:10:23  lr: 0.000011  min_lr: 0.000000  loss: 4.1114 (4.2033)  class_acc: 0.3333 (0.3130)  loss_scale: 65536.0000 (38360.2011)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1734  max mem: 15572
Epoch: [28]  [1720/2809]  eta: 0:10:16  lr: 0.000011  min_lr: 0.000000  loss: 4.1141 (4.2033)  class_acc: 0.2917 (0.3130)  loss_scale: 32768.0000 (38327.7071)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1700  max mem: 15572
Epoch: [28]  [1730/2809]  eta: 0:10:11  lr: 0.000011  min_lr: 0.000000  loss: 4.1753 (4.2031)  class_acc: 0.2917 (0.3131)  loss_scale: 32768.0000 (38295.5887)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1025  max mem: 15572
Epoch: [28]  [1740/2809]  eta: 0:10:05  lr: 0.000011  min_lr: 0.000000  loss: 4.1813 (4.2033)  class_acc: 0.2917 (0.3129)  loss_scale: 32768.0000 (38263.8392)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0974  max mem: 15572
Epoch: [28]  [1750/2809]  eta: 0:10:00  lr: 0.000011  min_lr: 0.000000  loss: 4.2055 (4.2034)  class_acc: 0.2917 (0.3130)  loss_scale: 32768.0000 (38232.4523)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1400  max mem: 15572
Epoch: [28]  [1760/2809]  eta: 0:09:54  lr: 0.000011  min_lr: 0.000000  loss: 4.2794 (4.2037)  class_acc: 0.2500 (0.3126)  loss_scale: 32768.0000 (38201.4219)  weight_decay: 0.0500 (0.0500)  time: 0.6479  data: 0.2031  max mem: 15572
Epoch: [28]  [1770/2809]  eta: 0:09:49  lr: 0.000011  min_lr: 0.000000  loss: 4.1721 (4.2035)  class_acc: 0.2917 (0.3129)  loss_scale: 32768.0000 (38170.7420)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1647  max mem: 15572
Epoch: [28]  [1780/2809]  eta: 0:09:43  lr: 0.000011  min_lr: 0.000000  loss: 4.1492 (4.2033)  class_acc: 0.3750 (0.3132)  loss_scale: 32768.0000 (38140.4065)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1186  max mem: 15572
Epoch: [28]  [1790/2809]  eta: 0:09:38  lr: 0.000011  min_lr: 0.000000  loss: 4.3335 (4.2039)  class_acc: 0.2917 (0.3130)  loss_scale: 32768.0000 (38110.4098)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1302  max mem: 15572
Epoch: [28]  [1800/2809]  eta: 0:09:32  lr: 0.000011  min_lr: 0.000000  loss: 4.3335 (4.2043)  class_acc: 0.2917 (0.3132)  loss_scale: 32768.0000 (38080.7463)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1291  max mem: 15572
Epoch: [28]  [1810/2809]  eta: 0:09:26  lr: 0.000011  min_lr: 0.000000  loss: 4.0649 (4.2038)  class_acc: 0.3333 (0.3132)  loss_scale: 32768.0000 (38051.4103)  weight_decay: 0.0500 (0.0500)  time: 0.5158  data: 0.0684  max mem: 15572
Epoch: [28]  [1820/2809]  eta: 0:09:20  lr: 0.000011  min_lr: 0.000000  loss: 4.2539 (4.2047)  class_acc: 0.2917 (0.3129)  loss_scale: 32768.0000 (38022.3965)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0655  max mem: 15572
Epoch: [28]  [1830/2809]  eta: 0:09:14  lr: 0.000011  min_lr: 0.000000  loss: 4.1101 (4.2038)  class_acc: 0.2917 (0.3128)  loss_scale: 32768.0000 (37993.6996)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0741  max mem: 15572
[2025-01-16 03:59:55,679] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:59:55,679] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1840/2809]  eta: 0:09:08  lr: 0.000011  min_lr: 0.000000  loss: 4.1291 (4.2038)  class_acc: 0.3333 (0.3129)  loss_scale: 32768.0000 (38018.7116)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0876  max mem: 15572
Epoch: [28]  [1850/2809]  eta: 0:09:03  lr: 0.000011  min_lr: 0.000000  loss: 4.1724 (4.2037)  class_acc: 0.3333 (0.3129)  loss_scale: 65536.0000 (38167.3733)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1067  max mem: 15572
Epoch: [28]  [1860/2809]  eta: 0:08:57  lr: 0.000011  min_lr: 0.000000  loss: 4.1814 (4.2037)  class_acc: 0.2917 (0.3126)  loss_scale: 65536.0000 (38314.4374)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1050  max mem: 15572
Epoch: [28]  [1870/2809]  eta: 0:08:51  lr: 0.000011  min_lr: 0.000000  loss: 4.1814 (4.2031)  class_acc: 0.2500 (0.3126)  loss_scale: 65536.0000 (38459.9294)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1321  max mem: 15572
[2025-01-16 04:00:19,217] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80529
[2025-01-16 04:00:19,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:00:19,217] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1880/2809]  eta: 0:08:46  lr: 0.000011  min_lr: 0.000000  loss: 4.1911 (4.2031)  class_acc: 0.2917 (0.3125)  loss_scale: 65536.0000 (38534.1925)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1578  max mem: 15572
Epoch: [28]  [1890/2809]  eta: 0:08:40  lr: 0.000011  min_lr: 0.000000  loss: 4.2987 (4.2038)  class_acc: 0.3333 (0.3126)  loss_scale: 32768.0000 (38503.6996)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1021  max mem: 15572
Epoch: [28]  [1900/2809]  eta: 0:08:35  lr: 0.000011  min_lr: 0.000000  loss: 4.1083 (4.2036)  class_acc: 0.2500 (0.3123)  loss_scale: 32768.0000 (38473.5276)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1216  max mem: 15572
Epoch: [28]  [1910/2809]  eta: 0:08:29  lr: 0.000011  min_lr: 0.000000  loss: 4.1658 (4.2043)  class_acc: 0.2500 (0.3122)  loss_scale: 32768.0000 (38443.6714)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1288  max mem: 15572
Epoch: [28]  [1920/2809]  eta: 0:08:23  lr: 0.000011  min_lr: 0.000000  loss: 4.3234 (4.2049)  class_acc: 0.2917 (0.3123)  loss_scale: 32768.0000 (38414.1260)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.1085  max mem: 15572
Epoch: [28]  [1930/2809]  eta: 0:08:18  lr: 0.000011  min_lr: 0.000000  loss: 4.3781 (4.2056)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (38384.8866)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1597  max mem: 15572
Epoch: [28]  [1940/2809]  eta: 0:08:12  lr: 0.000011  min_lr: 0.000000  loss: 4.2823 (4.2058)  class_acc: 0.2500 (0.3120)  loss_scale: 32768.0000 (38355.9485)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1762  max mem: 15572
Epoch: [28]  [1950/2809]  eta: 0:08:07  lr: 0.000011  min_lr: 0.000000  loss: 4.2642 (4.2059)  class_acc: 0.2500 (0.3116)  loss_scale: 32768.0000 (38327.3070)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1657  max mem: 15572
Epoch: [28]  [1960/2809]  eta: 0:08:01  lr: 0.000011  min_lr: 0.000000  loss: 4.0850 (4.2054)  class_acc: 0.2917 (0.3115)  loss_scale: 32768.0000 (38298.9577)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1268  max mem: 15572
Epoch: [28]  [1970/2809]  eta: 0:07:55  lr: 0.000011  min_lr: 0.000000  loss: 4.0245 (4.2051)  class_acc: 0.3333 (0.3117)  loss_scale: 32768.0000 (38270.8960)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1042  max mem: 15572
Epoch: [28]  [1980/2809]  eta: 0:07:50  lr: 0.000011  min_lr: 0.000000  loss: 4.1825 (4.2045)  class_acc: 0.2917 (0.3118)  loss_scale: 32768.0000 (38243.1176)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1292  max mem: 15572
Epoch: [28]  [1990/2809]  eta: 0:07:44  lr: 0.000011  min_lr: 0.000000  loss: 4.0954 (4.2040)  class_acc: 0.2917 (0.3120)  loss_scale: 32768.0000 (38215.6183)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1320  max mem: 15572
Epoch: [28]  [2000/2809]  eta: 0:07:38  lr: 0.000011  min_lr: 0.000000  loss: 4.1764 (4.2044)  class_acc: 0.2917 (0.3115)  loss_scale: 32768.0000 (38188.3938)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0824  max mem: 15572
[2025-01-16 04:01:31,721] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:01:31,722] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [2010/2809]  eta: 0:07:33  lr: 0.000011  min_lr: 0.000000  loss: 4.3017 (4.2044)  class_acc: 0.2500 (0.3115)  loss_scale: 32768.0000 (38242.9120)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1071  max mem: 15572
Epoch: [28]  [2020/2809]  eta: 0:07:27  lr: 0.000011  min_lr: 0.000000  loss: 4.2913 (4.2047)  class_acc: 0.2917 (0.3114)  loss_scale: 65536.0000 (38377.9594)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1434  max mem: 15572
Epoch: [28]  [2030/2809]  eta: 0:07:22  lr: 0.000011  min_lr: 0.000000  loss: 4.2250 (4.2045)  class_acc: 0.3333 (0.3114)  loss_scale: 65536.0000 (38511.6770)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.1978  max mem: 15572
Epoch: [28]  [2040/2809]  eta: 0:07:16  lr: 0.000011  min_lr: 0.000000  loss: 4.2154 (4.2044)  class_acc: 0.2917 (0.3112)  loss_scale: 65536.0000 (38644.0843)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.1993  max mem: 15572
Epoch: [28]  [2050/2809]  eta: 0:07:10  lr: 0.000011  min_lr: 0.000000  loss: 4.1448 (4.2038)  class_acc: 0.2500 (0.3113)  loss_scale: 65536.0000 (38775.2004)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1284  max mem: 15572
Epoch: [28]  [2060/2809]  eta: 0:07:05  lr: 0.000011  min_lr: 0.000000  loss: 4.0643 (4.2033)  class_acc: 0.2917 (0.3112)  loss_scale: 65536.0000 (38905.0442)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1254  max mem: 15572
[2025-01-16 04:02:09,793] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80722
[2025-01-16 04:02:09,794] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:02:09,796] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [2070/2809]  eta: 0:06:59  lr: 0.000011  min_lr: 0.000000  loss: 4.1451 (4.2032)  class_acc: 0.2500 (0.3112)  loss_scale: 65536.0000 (39017.8117)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0682  max mem: 15572
Epoch: [28]  [2080/2809]  eta: 0:06:53  lr: 0.000011  min_lr: 0.000000  loss: 4.2135 (4.2033)  class_acc: 0.2500 (0.3108)  loss_scale: 32768.0000 (38987.7790)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0654  max mem: 15572
Epoch: [28]  [2090/2809]  eta: 0:06:48  lr: 0.000011  min_lr: 0.000000  loss: 4.2300 (4.2038)  class_acc: 0.2083 (0.3106)  loss_scale: 32768.0000 (38958.0335)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1271  max mem: 15572
Epoch: [28]  [2100/2809]  eta: 0:06:42  lr: 0.000011  min_lr: 0.000000  loss: 4.1975 (4.2039)  class_acc: 0.2917 (0.3105)  loss_scale: 32768.0000 (38928.5712)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1083  max mem: 15572
Epoch: [28]  [2110/2809]  eta: 0:06:36  lr: 0.000011  min_lr: 0.000000  loss: 4.1476 (4.2035)  class_acc: 0.2917 (0.3104)  loss_scale: 32768.0000 (38899.3880)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.0969  max mem: 15572
Epoch: [28]  [2120/2809]  eta: 0:06:30  lr: 0.000011  min_lr: 0.000000  loss: 4.1669 (4.2036)  class_acc: 0.3333 (0.3105)  loss_scale: 32768.0000 (38870.4800)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1056  max mem: 15572
Epoch: [28]  [2130/2809]  eta: 0:06:25  lr: 0.000011  min_lr: 0.000000  loss: 4.1669 (4.2040)  class_acc: 0.3333 (0.3106)  loss_scale: 32768.0000 (38841.8433)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0957  max mem: 15572
Epoch: [28]  [2140/2809]  eta: 0:06:19  lr: 0.000011  min_lr: 0.000000  loss: 4.2548 (4.2043)  class_acc: 0.2917 (0.3106)  loss_scale: 32768.0000 (38813.4741)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.1011  max mem: 15572
Epoch: [28]  [2150/2809]  eta: 0:06:13  lr: 0.000011  min_lr: 0.000000  loss: 4.3541 (4.2052)  class_acc: 0.2917 (0.3105)  loss_scale: 32768.0000 (38785.3687)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0943  max mem: 15572
Epoch: [28]  [2160/2809]  eta: 0:06:08  lr: 0.000011  min_lr: 0.000000  loss: 4.4313 (4.2059)  class_acc: 0.2500 (0.3105)  loss_scale: 32768.0000 (38757.5234)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1508  max mem: 15572
Epoch: [28]  [2170/2809]  eta: 0:06:02  lr: 0.000011  min_lr: 0.000000  loss: 4.3550 (4.2061)  class_acc: 0.2917 (0.3106)  loss_scale: 32768.0000 (38729.9346)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1170  max mem: 15572
Epoch: [28]  [2180/2809]  eta: 0:05:56  lr: 0.000011  min_lr: 0.000000  loss: 4.2236 (4.2061)  class_acc: 0.2917 (0.3105)  loss_scale: 32768.0000 (38702.5988)  weight_decay: 0.0500 (0.0500)  time: 0.4877  data: 0.0355  max mem: 15572
Epoch: [28]  [2190/2809]  eta: 0:05:51  lr: 0.000011  min_lr: 0.000000  loss: 4.1498 (4.2058)  class_acc: 0.2917 (0.3106)  loss_scale: 32768.0000 (38675.5126)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1365  max mem: 15572
[2025-01-16 04:03:22,528] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:03:22,528] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [2200/2809]  eta: 0:05:45  lr: 0.000011  min_lr: 0.000000  loss: 4.2617 (4.2068)  class_acc: 0.3750 (0.3108)  loss_scale: 32768.0000 (38678.4480)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1565  max mem: 15572
[2025-01-16 04:03:27,785] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80862
[2025-01-16 04:03:27,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:03:27,785] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [2210/2809]  eta: 0:05:39  lr: 0.000011  min_lr: 0.000000  loss: 4.3021 (4.2069)  class_acc: 0.3333 (0.3109)  loss_scale: 65536.0000 (38785.1000)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0555  max mem: 15572
Epoch: [28]  [2220/2809]  eta: 0:05:34  lr: 0.000011  min_lr: 0.000000  loss: 4.2420 (4.2068)  class_acc: 0.3333 (0.3111)  loss_scale: 32768.0000 (38758.0081)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1115  max mem: 15572
Epoch: [28]  [2230/2809]  eta: 0:05:28  lr: 0.000011  min_lr: 0.000000  loss: 4.1984 (4.2068)  class_acc: 0.2500 (0.3109)  loss_scale: 32768.0000 (38731.1591)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1446  max mem: 15572
Epoch: [28]  [2240/2809]  eta: 0:05:22  lr: 0.000011  min_lr: 0.000000  loss: 4.2111 (4.2071)  class_acc: 0.2500 (0.3109)  loss_scale: 32768.0000 (38704.5498)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1184  max mem: 15572
Epoch: [28]  [2250/2809]  eta: 0:05:16  lr: 0.000011  min_lr: 0.000000  loss: 4.2894 (4.2074)  class_acc: 0.2917 (0.3109)  loss_scale: 32768.0000 (38678.1768)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1222  max mem: 15572
Epoch: [28]  [2260/2809]  eta: 0:05:11  lr: 0.000011  min_lr: 0.000000  loss: 4.0843 (4.2069)  class_acc: 0.3333 (0.3109)  loss_scale: 32768.0000 (38652.0372)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1425  max mem: 15572
Epoch: [28]  [2270/2809]  eta: 0:05:05  lr: 0.000011  min_lr: 0.000000  loss: 4.0271 (4.2065)  class_acc: 0.2917 (0.3109)  loss_scale: 32768.0000 (38626.1277)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1349  max mem: 15572
Epoch: [28]  [2280/2809]  eta: 0:04:59  lr: 0.000011  min_lr: 0.000000  loss: 4.1225 (4.2068)  class_acc: 0.2500 (0.3108)  loss_scale: 32768.0000 (38600.4454)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0720  max mem: 15572
Epoch: [28]  [2290/2809]  eta: 0:04:54  lr: 0.000011  min_lr: 0.000000  loss: 4.1971 (4.2064)  class_acc: 0.2500 (0.3107)  loss_scale: 32768.0000 (38574.9873)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1109  max mem: 15572
Epoch: [28]  [2300/2809]  eta: 0:04:48  lr: 0.000011  min_lr: 0.000000  loss: 4.1658 (4.2067)  class_acc: 0.2500 (0.3105)  loss_scale: 32768.0000 (38549.7505)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1428  max mem: 15572
Epoch: [28]  [2310/2809]  eta: 0:04:42  lr: 0.000011  min_lr: 0.000000  loss: 4.1658 (4.2060)  class_acc: 0.2917 (0.3105)  loss_scale: 32768.0000 (38524.7322)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1225  max mem: 15572
Epoch: [28]  [2320/2809]  eta: 0:04:37  lr: 0.000011  min_lr: 0.000000  loss: 4.1933 (4.2064)  class_acc: 0.2500 (0.3103)  loss_scale: 32768.0000 (38499.9293)  weight_decay: 0.0500 (0.0500)  time: 0.5104  data: 0.0488  max mem: 15572
Epoch: [28]  [2330/2809]  eta: 0:04:31  lr: 0.000011  min_lr: 0.000000  loss: 4.2474 (4.2066)  class_acc: 0.2500 (0.3103)  loss_scale: 32768.0000 (38475.3393)  weight_decay: 0.0500 (0.0500)  time: 0.4958  data: 0.0331  max mem: 15572
[2025-01-16 04:04:40,571] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:04:40,572] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [2340/2809]  eta: 0:04:25  lr: 0.000011  min_lr: 0.000000  loss: 4.2007 (4.2064)  class_acc: 0.3333 (0.3103)  loss_scale: 32768.0000 (38478.9543)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1161  max mem: 15572
[2025-01-16 04:04:44,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=81000, skipped=504, lr=[1.0514467726857202e-07, 1.0514467726857202e-07, 1.5020668181224575e-07, 1.5020668181224575e-07, 2.1458097401749398e-07, 2.1458097401749398e-07, 3.0654424859642e-07, 3.0654424859642e-07, 4.379203551377428e-07, 4.379203551377428e-07, 6.256005073396326e-07, 6.256005073396326e-07, 8.937150104851895e-07, 8.937150104851895e-07, 1.2767357292645567e-06, 1.2767357292645567e-06, 1.8239081846636523e-06, 1.8239081846636523e-06, 2.605583120948075e-06, 2.605583120948075e-06, 3.722261601354393e-06, 3.722261601354393e-06, 5.317516573363419e-06, 5.317516573363419e-06, 7.5964522476620275e-06, 7.5964522476620275e-06, 1.0852074639517183e-05, 1.0852074639517183e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:04:44,566] [INFO] [timer.py:260:stop] epoch=0/micro_step=81000/global_step=81000, RunningAvgSamplesPerSec=27.925667956824867, CurrSamplesPerSec=23.441895298780064, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [28]  [2350/2809]  eta: 0:04:19  lr: 0.000011  min_lr: 0.000000  loss: 4.2008 (4.2066)  class_acc: 0.3333 (0.3103)  loss_scale: 65536.0000 (38594.0417)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0837  max mem: 15572
Epoch: [28]  [2360/2809]  eta: 0:04:14  lr: 0.000011  min_lr: 0.000000  loss: 4.2008 (4.2070)  class_acc: 0.2500 (0.3101)  loss_scale: 65536.0000 (38708.1542)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0790  max mem: 15572
[2025-01-16 04:04:58,134] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81022
[2025-01-16 04:04:58,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:04:58,135] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [2370/2809]  eta: 0:04:08  lr: 0.000011  min_lr: 0.000000  loss: 4.1233 (4.2062)  class_acc: 0.2917 (0.3104)  loss_scale: 65536.0000 (38807.4838)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1585  max mem: 15572
Epoch: [28]  [2380/2809]  eta: 0:04:03  lr: 0.000011  min_lr: 0.000000  loss: 4.1509 (4.2064)  class_acc: 0.3333 (0.3105)  loss_scale: 32768.0000 (38782.1184)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1450  max mem: 15572
Epoch: [28]  [2390/2809]  eta: 0:03:57  lr: 0.000011  min_lr: 0.000000  loss: 4.2572 (4.2067)  class_acc: 0.2500 (0.3102)  loss_scale: 32768.0000 (38756.9653)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1222  max mem: 15572
Epoch: [28]  [2400/2809]  eta: 0:03:51  lr: 0.000011  min_lr: 0.000000  loss: 4.1063 (4.2067)  class_acc: 0.2500 (0.3102)  loss_scale: 32768.0000 (38732.0217)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1096  max mem: 15572
Epoch: [28]  [2410/2809]  eta: 0:03:46  lr: 0.000011  min_lr: 0.000000  loss: 4.0495 (4.2060)  class_acc: 0.3333 (0.3102)  loss_scale: 32768.0000 (38707.2849)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1204  max mem: 15572
Epoch: [28]  [2420/2809]  eta: 0:03:40  lr: 0.000011  min_lr: 0.000000  loss: 4.0715 (4.2062)  class_acc: 0.2917 (0.3101)  loss_scale: 32768.0000 (38682.7526)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1474  max mem: 15572
Epoch: [28]  [2430/2809]  eta: 0:03:34  lr: 0.000011  min_lr: 0.000000  loss: 4.2516 (4.2063)  class_acc: 0.2917 (0.3102)  loss_scale: 32768.0000 (38658.4220)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1380  max mem: 15572
Epoch: [28]  [2440/2809]  eta: 0:03:29  lr: 0.000011  min_lr: 0.000000  loss: 4.2516 (4.2062)  class_acc: 0.3333 (0.3102)  loss_scale: 32768.0000 (38634.2909)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1314  max mem: 15572
Epoch: [28]  [2450/2809]  eta: 0:03:23  lr: 0.000011  min_lr: 0.000000  loss: 4.1611 (4.2063)  class_acc: 0.3333 (0.3104)  loss_scale: 32768.0000 (38610.3566)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1014  max mem: 15572
Epoch: [28]  [2460/2809]  eta: 0:03:17  lr: 0.000011  min_lr: 0.000000  loss: 4.1791 (4.2061)  class_acc: 0.3750 (0.3106)  loss_scale: 32768.0000 (38586.6168)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1086  max mem: 15572
Epoch: [28]  [2470/2809]  eta: 0:03:12  lr: 0.000011  min_lr: 0.000000  loss: 4.1984 (4.2064)  class_acc: 0.3333 (0.3109)  loss_scale: 32768.0000 (38563.0692)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1171  max mem: 15572
Epoch: [28]  [2480/2809]  eta: 0:03:06  lr: 0.000011  min_lr: 0.000000  loss: 4.1408 (4.2060)  class_acc: 0.3333 (0.3111)  loss_scale: 32768.0000 (38539.7114)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0663  max mem: 15572
Epoch: [28]  [2490/2809]  eta: 0:03:00  lr: 0.000011  min_lr: 0.000000  loss: 4.1408 (4.2064)  class_acc: 0.2917 (0.3111)  loss_scale: 32768.0000 (38516.5411)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0590  max mem: 15572
[2025-01-16 04:06:12,724] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:06:12,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [2500/2809]  eta: 0:02:55  lr: 0.000011  min_lr: 0.000000  loss: 4.2985 (4.2068)  class_acc: 0.2500 (0.3108)  loss_scale: 32768.0000 (38519.7601)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1237  max mem: 15572
Epoch: [28]  [2510/2809]  eta: 0:02:49  lr: 0.000011  min_lr: 0.000000  loss: 4.1993 (4.2064)  class_acc: 0.2500 (0.3105)  loss_scale: 65536.0000 (38627.3517)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0945  max mem: 15572
Epoch: [28]  [2520/2809]  eta: 0:02:43  lr: 0.000011  min_lr: 0.000000  loss: 4.2626 (4.2070)  class_acc: 0.2917 (0.3106)  loss_scale: 65536.0000 (38734.0896)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0010  max mem: 15572
[2025-01-16 04:06:23,812] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81173
[2025-01-16 04:06:23,813] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:06:23,813] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [2530/2809]  eta: 0:02:38  lr: 0.000011  min_lr: 0.000000  loss: 4.3131 (4.2073)  class_acc: 0.2917 (0.3105)  loss_scale: 32768.0000 (38710.5176)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0008  max mem: 15572
Epoch: [28]  [2540/2809]  eta: 0:02:32  lr: 0.000011  min_lr: 0.000000  loss: 4.2357 (4.2075)  class_acc: 0.2917 (0.3105)  loss_scale: 32768.0000 (38687.1311)  weight_decay: 0.0500 (0.0500)  time: 0.5097  data: 0.0100  max mem: 15572
Epoch: [28]  [2550/2809]  eta: 0:02:26  lr: 0.000011  min_lr: 0.000000  loss: 4.2357 (4.2077)  class_acc: 0.2917 (0.3105)  loss_scale: 32768.0000 (38663.9279)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.0276  max mem: 15572
Epoch: [28]  [2560/2809]  eta: 0:02:20  lr: 0.000011  min_lr: 0.000000  loss: 4.4042 (4.2082)  class_acc: 0.2917 (0.3104)  loss_scale: 32768.0000 (38640.9059)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0939  max mem: 15572
Epoch: [28]  [2570/2809]  eta: 0:02:15  lr: 0.000011  min_lr: 0.000000  loss: 4.2290 (4.2079)  class_acc: 0.2917 (0.3104)  loss_scale: 32768.0000 (38618.0630)  weight_decay: 0.0500 (0.0500)  time: 0.6477  data: 0.1801  max mem: 15572
Epoch: [28]  [2580/2809]  eta: 0:02:09  lr: 0.000011  min_lr: 0.000000  loss: 4.2409 (4.2086)  class_acc: 0.2917 (0.3104)  loss_scale: 32768.0000 (38595.3971)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1046  max mem: 15572
Epoch: [28]  [2590/2809]  eta: 0:02:03  lr: 0.000011  min_lr: 0.000000  loss: 4.3770 (4.2088)  class_acc: 0.3333 (0.3105)  loss_scale: 32768.0000 (38572.9062)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0327  max mem: 15572
Epoch: [28]  [2600/2809]  eta: 0:01:58  lr: 0.000011  min_lr: 0.000000  loss: 4.0948 (4.2083)  class_acc: 0.3333 (0.3106)  loss_scale: 32768.0000 (38550.5882)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1146  max mem: 15572
Epoch: [28]  [2610/2809]  eta: 0:01:52  lr: 0.000011  min_lr: 0.000000  loss: 4.3875 (4.2093)  class_acc: 0.2917 (0.3106)  loss_scale: 32768.0000 (38528.4412)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1739  max mem: 15572
Epoch: [28]  [2620/2809]  eta: 0:01:47  lr: 0.000011  min_lr: 0.000000  loss: 4.3914 (4.2090)  class_acc: 0.2917 (0.3108)  loss_scale: 32768.0000 (38506.4632)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1636  max mem: 15572
Epoch: [28]  [2630/2809]  eta: 0:01:41  lr: 0.000011  min_lr: 0.000000  loss: 4.0908 (4.2089)  class_acc: 0.2917 (0.3107)  loss_scale: 32768.0000 (38484.6522)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.1122  max mem: 15572
Epoch: [28]  [2640/2809]  eta: 0:01:35  lr: 0.000011  min_lr: 0.000000  loss: 4.1910 (4.2091)  class_acc: 0.2917 (0.3105)  loss_scale: 32768.0000 (38463.0064)  weight_decay: 0.0500 (0.0500)  time: 0.4471  data: 0.0403  max mem: 15572
[2025-01-16 04:07:33,898] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:07:33,899] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [2650/2809]  eta: 0:01:29  lr: 0.000011  min_lr: 0.000000  loss: 4.1269 (4.2085)  class_acc: 0.3333 (0.3108)  loss_scale: 32768.0000 (38453.8846)  weight_decay: 0.0500 (0.0500)  time: 0.4458  data: 0.0008  max mem: 15572
[2025-01-16 04:07:39,047] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81312
[2025-01-16 04:07:39,047] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:07:39,047] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [2660/2809]  eta: 0:01:24  lr: 0.000011  min_lr: 0.000000  loss: 4.0147 (4.2078)  class_acc: 0.3333 (0.3109)  loss_scale: 32768.0000 (38543.3446)  weight_decay: 0.0500 (0.0500)  time: 0.4989  data: 0.0012  max mem: 15572
Epoch: [28]  [2670/2809]  eta: 0:01:18  lr: 0.000011  min_lr: 0.000000  loss: 4.0106 (4.2072)  class_acc: 0.3333 (0.3111)  loss_scale: 32768.0000 (38521.7222)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0600  max mem: 15572
Epoch: [28]  [2680/2809]  eta: 0:01:12  lr: 0.000011  min_lr: 0.000000  loss: 3.9848 (4.2067)  class_acc: 0.3333 (0.3111)  loss_scale: 32768.0000 (38500.2611)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.1669  max mem: 15572
Epoch: [28]  [2690/2809]  eta: 0:01:07  lr: 0.000011  min_lr: 0.000000  loss: 4.0374 (4.2065)  class_acc: 0.3333 (0.3111)  loss_scale: 32768.0000 (38478.9595)  weight_decay: 0.0500 (0.0500)  time: 0.6680  data: 0.1975  max mem: 15572
Epoch: [28]  [2700/2809]  eta: 0:01:01  lr: 0.000011  min_lr: 0.000000  loss: 4.1855 (4.2066)  class_acc: 0.2917 (0.3111)  loss_scale: 32768.0000 (38457.8156)  weight_decay: 0.0500 (0.0500)  time: 0.6488  data: 0.1735  max mem: 15572
Epoch: [28]  [2710/2809]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 4.3323 (4.2073)  class_acc: 0.2500 (0.3108)  loss_scale: 32768.0000 (38436.8277)  weight_decay: 0.0500 (0.0500)  time: 0.6620  data: 0.1908  max mem: 15572
Epoch: [28]  [2720/2809]  eta: 0:00:50  lr: 0.000011  min_lr: 0.000000  loss: 4.2353 (4.2068)  class_acc: 0.2083 (0.3106)  loss_scale: 32768.0000 (38415.9941)  weight_decay: 0.0500 (0.0500)  time: 0.6744  data: 0.2012  max mem: 15572
Epoch: [28]  [2730/2809]  eta: 0:00:44  lr: 0.000011  min_lr: 0.000000  loss: 4.1302 (4.2066)  class_acc: 0.2500 (0.3107)  loss_scale: 32768.0000 (38395.3131)  weight_decay: 0.0500 (0.0500)  time: 0.6860  data: 0.2103  max mem: 15572
Epoch: [28]  [2740/2809]  eta: 0:00:39  lr: 0.000011  min_lr: 0.000000  loss: 4.1882 (4.2064)  class_acc: 0.3333 (0.3107)  loss_scale: 32768.0000 (38374.7829)  weight_decay: 0.0500 (0.0500)  time: 0.6929  data: 0.2179  max mem: 15572
Epoch: [28]  [2750/2809]  eta: 0:00:33  lr: 0.000011  min_lr: 0.000000  loss: 4.1175 (4.2060)  class_acc: 0.3333 (0.3108)  loss_scale: 32768.0000 (38354.4020)  weight_decay: 0.0500 (0.0500)  time: 0.7081  data: 0.2377  max mem: 15572
Epoch: [28]  [2760/2809]  eta: 0:00:27  lr: 0.000011  min_lr: 0.000000  loss: 4.0844 (4.2053)  class_acc: 0.2917 (0.3108)  loss_scale: 32768.0000 (38334.1688)  weight_decay: 0.0500 (0.0500)  time: 0.7036  data: 0.2383  max mem: 15572
Epoch: [28]  [2770/2809]  eta: 0:00:22  lr: 0.000011  min_lr: 0.000000  loss: 4.0844 (4.2049)  class_acc: 0.3750 (0.3111)  loss_scale: 32768.0000 (38314.0816)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.1860  max mem: 15572
Epoch: [28]  [2780/2809]  eta: 0:00:16  lr: 0.000011  min_lr: 0.000000  loss: 4.2195 (4.2050)  class_acc: 0.3750 (0.3110)  loss_scale: 32768.0000 (38294.1388)  weight_decay: 0.0500 (0.0500)  time: 0.5008  data: 0.0841  max mem: 15572
[2025-01-16 04:08:59,867] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:08:59,867] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [2790/2809]  eta: 0:00:10  lr: 0.000011  min_lr: 0.000000  loss: 4.2195 (4.2049)  class_acc: 0.2917 (0.3112)  loss_scale: 32768.0000 (38297.8201)  weight_decay: 0.0500 (0.0500)  time: 0.3967  data: 0.0005  max mem: 15572
Epoch: [28]  [2800/2809]  eta: 0:00:05  lr: 0.000011  min_lr: 0.000000  loss: 4.2868 (4.2059)  class_acc: 0.2917 (0.3110)  loss_scale: 65536.0000 (38395.0646)  weight_decay: 0.0500 (0.0500)  time: 0.4156  data: 0.0005  max mem: 15572
Epoch: [28]  [2808/2809]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 4.4143 (4.2062)  class_acc: 0.2917 (0.3110)  loss_scale: 65536.0000 (38472.3617)  weight_decay: 0.0500 (0.0500)  time: 0.4062  data: 0.0003  max mem: 15572
Epoch: [28] Total time: 0:26:33 (0.5674 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 4.4143 (4.2062)  class_acc: 0.2917 (0.3110)  loss_scale: 65536.0000 (38472.3617)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:13:22  loss: 0.9708 (0.9708)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.9521  data: 2.7845  max mem: 15572
Val:  [ 10/272]  eta: 0:02:39  loss: 2.3492 (2.4897)  acc1: 50.0000 (47.4747)  acc5: 77.7778 (75.7576)  time: 0.6099  data: 0.4249  max mem: 15572
Val:  [ 20/272]  eta: 0:01:59  loss: 2.5653 (2.5604)  acc1: 44.4444 (46.2963)  acc5: 72.2222 (74.6032)  time: 0.3488  data: 0.1569  max mem: 15572
Val:  [ 30/272]  eta: 0:01:40  loss: 2.5881 (2.5957)  acc1: 44.4444 (43.0108)  acc5: 72.2222 (74.0143)  time: 0.3068  data: 0.1176  max mem: 15572
Val:  [ 40/272]  eta: 0:01:29  loss: 2.5881 (2.6203)  acc1: 33.3333 (41.4634)  acc5: 72.2222 (73.7127)  time: 0.2921  data: 0.0979  max mem: 15572
Val:  [ 50/272]  eta: 0:01:24  loss: 2.4666 (2.5473)  acc1: 38.8889 (43.1373)  acc5: 77.7778 (75.7081)  time: 0.3341  data: 0.1326  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 2.0191 (2.4952)  acc1: 61.1111 (44.9909)  acc5: 83.3333 (76.1384)  time: 0.3897  data: 0.1781  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 2.0461 (2.4472)  acc1: 61.1111 (47.0266)  acc5: 83.3333 (76.9953)  time: 0.3591  data: 0.1532  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 2.1714 (2.4483)  acc1: 55.5556 (47.0508)  acc5: 77.7778 (76.8861)  time: 0.3113  data: 0.1114  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 2.3721 (2.4417)  acc1: 50.0000 (47.8022)  acc5: 77.7778 (77.5336)  time: 0.3112  data: 0.1110  max mem: 15572
Val:  [100/272]  eta: 0:01:02  loss: 2.4249 (2.4629)  acc1: 50.0000 (46.8647)  acc5: 83.3333 (77.3377)  time: 0.3349  data: 0.1396  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 2.5914 (2.5181)  acc1: 27.7778 (45.1952)  acc5: 66.6667 (76.1261)  time: 0.3147  data: 0.1207  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.0436 (2.5554)  acc1: 22.2222 (44.2608)  acc5: 61.1111 (75.3444)  time: 0.2785  data: 0.0868  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.5516 (2.5305)  acc1: 44.4444 (44.9958)  acc5: 77.7778 (76.1238)  time: 0.2801  data: 0.0777  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 2.1738 (2.5342)  acc1: 50.0000 (45.4689)  acc5: 83.3333 (75.7683)  time: 0.2942  data: 0.0884  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.4858 (2.5350)  acc1: 38.8889 (44.9595)  acc5: 77.7778 (75.8278)  time: 0.3022  data: 0.1058  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.4956 (2.5353)  acc1: 44.4444 (45.2726)  acc5: 77.7778 (75.9834)  time: 0.3236  data: 0.1249  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.6420 (2.5552)  acc1: 44.4444 (44.7693)  acc5: 72.2222 (75.4061)  time: 0.3526  data: 0.1469  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.5699 (2.5405)  acc1: 38.8889 (44.6900)  acc5: 72.2222 (75.8134)  time: 0.3173  data: 0.1126  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.4736 (2.5761)  acc1: 33.3333 (43.5718)  acc5: 72.2222 (74.8109)  time: 0.3151  data: 0.1127  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.7644 (2.5818)  acc1: 33.3333 (42.9243)  acc5: 72.2222 (74.5992)  time: 0.3228  data: 0.1277  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.3859 (2.5912)  acc1: 33.3333 (42.6540)  acc5: 72.2222 (74.4339)  time: 0.2702  data: 0.0794  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.6603 (2.5841)  acc1: 44.4444 (42.9864)  acc5: 72.2222 (74.5349)  time: 0.2902  data: 0.1067  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.0801 (2.5662)  acc1: 55.5556 (43.7710)  acc5: 77.7778 (74.8196)  time: 0.3429  data: 0.1547  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.0360 (2.5509)  acc1: 61.1111 (44.0987)  acc5: 83.3333 (75.2190)  time: 0.3089  data: 0.1114  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4465 (2.5589)  acc1: 38.8889 (43.5370)  acc5: 83.3333 (75.0996)  time: 0.2704  data: 0.0646  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.0774 (2.5200)  acc1: 61.1111 (44.9127)  acc5: 83.3333 (75.7344)  time: 0.2870  data: 0.0895  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.0509 (2.5163)  acc1: 61.1111 (44.8544)  acc5: 83.3333 (75.9123)  time: 0.2300  data: 0.0605  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.0509 (2.5205)  acc1: 55.5556 (44.8495)  acc5: 83.3333 (75.8755)  time: 0.2155  data: 0.0527  max mem: 15572
Val: Total time: 0:01:26 (0.3192 s / it)
* Acc@1 44.849 Acc@5 75.875 loss 2.521
Accuracy of the network on the 4883 val videos: 44.8%
[2025-01-16 04:10:34,691] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 04:10:34,693] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 04:10:34,693] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 04:10:37,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 04:10:37,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.85%
Epoch: [29]  [   0/2809]  eta: 6:01:42  lr: 0.000011  min_lr: 0.000000  loss: 4.2412 (4.2412)  class_acc: 0.2500 (0.2500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.7260  data: 7.2943  max mem: 15572
Epoch: [29]  [  10/2809]  eta: 0:59:17  lr: 0.000011  min_lr: 0.000000  loss: 4.3122 (4.2907)  class_acc: 0.2917 (0.2803)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2711  data: 0.8431  max mem: 15572
Epoch: [29]  [  20/2809]  eta: 0:40:54  lr: 0.000011  min_lr: 0.000000  loss: 4.2656 (4.2365)  class_acc: 0.2917 (0.3155)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0993  max mem: 15572
[2025-01-16 04:10:58,190] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81486
[2025-01-16 04:10:58,190] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:10:58,190] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [  30/2809]  eta: 0:37:13  lr: 0.000011  min_lr: 0.000000  loss: 4.1416 (4.1764)  class_acc: 0.3333 (0.3374)  loss_scale: 65536.0000 (59193.8065)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.1046  max mem: 15572
Epoch: [29]  [  40/2809]  eta: 0:33:06  lr: 0.000011  min_lr: 0.000000  loss: 4.0672 (4.1687)  class_acc: 0.3333 (0.3435)  loss_scale: 32768.0000 (52748.4878)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1123  max mem: 15572
Epoch: [29]  [  50/2809]  eta: 0:32:11  lr: 0.000011  min_lr: 0.000000  loss: 4.0577 (4.1503)  class_acc: 0.3750 (0.3472)  loss_scale: 32768.0000 (48830.7451)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.1066  max mem: 15572
Epoch: [29]  [  60/2809]  eta: 0:31:10  lr: 0.000011  min_lr: 0.000000  loss: 4.1801 (4.1530)  class_acc: 0.3750 (0.3402)  loss_scale: 32768.0000 (46197.5082)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1625  max mem: 15572
Epoch: [29]  [  70/2809]  eta: 0:29:38  lr: 0.000011  min_lr: 0.000000  loss: 4.1055 (4.1455)  class_acc: 0.2917 (0.3363)  loss_scale: 32768.0000 (44306.0282)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0642  max mem: 15572
Epoch: [29]  [  80/2809]  eta: 0:29:15  lr: 0.000011  min_lr: 0.000000  loss: 4.1843 (4.1473)  class_acc: 0.3333 (0.3426)  loss_scale: 32768.0000 (42881.5802)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0724  max mem: 15572
Epoch: [29]  [  90/2809]  eta: 0:28:52  lr: 0.000011  min_lr: 0.000000  loss: 4.2431 (4.1448)  class_acc: 0.2917 (0.3370)  loss_scale: 32768.0000 (41770.1978)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1474  max mem: 15572
Epoch: [29]  [ 100/2809]  eta: 0:28:31  lr: 0.000010  min_lr: 0.000000  loss: 4.2743 (4.1613)  class_acc: 0.2500 (0.3263)  loss_scale: 32768.0000 (40878.8911)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1409  max mem: 15572
Epoch: [29]  [ 110/2809]  eta: 0:27:59  lr: 0.000010  min_lr: 0.000000  loss: 4.2951 (4.1689)  class_acc: 0.2500 (0.3281)  loss_scale: 32768.0000 (40148.1802)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1060  max mem: 15572
Epoch: [29]  [ 120/2809]  eta: 0:27:39  lr: 0.000010  min_lr: 0.000000  loss: 4.2382 (4.1674)  class_acc: 0.2917 (0.3261)  loss_scale: 32768.0000 (39538.2479)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0910  max mem: 15572
Epoch: [29]  [ 130/2809]  eta: 0:27:36  lr: 0.000010  min_lr: 0.000000  loss: 4.2189 (4.1697)  class_acc: 0.2917 (0.3244)  loss_scale: 32768.0000 (39021.4351)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1515  max mem: 15572
Epoch: [29]  [ 140/2809]  eta: 0:27:26  lr: 0.000010  min_lr: 0.000000  loss: 4.2668 (4.1800)  class_acc: 0.3333 (0.3254)  loss_scale: 32768.0000 (38577.9291)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1739  max mem: 15572
Epoch: [29]  [ 150/2809]  eta: 0:27:12  lr: 0.000010  min_lr: 0.000000  loss: 4.1680 (4.1739)  class_acc: 0.3333 (0.3270)  loss_scale: 32768.0000 (38193.1656)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1396  max mem: 15572
[2025-01-16 04:12:11,953] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:12:11,953] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 160/2809]  eta: 0:26:54  lr: 0.000010  min_lr: 0.000000  loss: 4.1087 (4.1720)  class_acc: 0.3333 (0.3282)  loss_scale: 32768.0000 (39280.8944)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1244  max mem: 15572
Epoch: [29]  [ 170/2809]  eta: 0:26:28  lr: 0.000010  min_lr: 0.000000  loss: 4.0734 (4.1731)  class_acc: 0.2917 (0.3248)  loss_scale: 65536.0000 (40816.2807)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0806  max mem: 15572
Epoch: [29]  [ 180/2809]  eta: 0:26:12  lr: 0.000010  min_lr: 0.000000  loss: 4.1136 (4.1678)  class_acc: 0.2917 (0.3234)  loss_scale: 65536.0000 (42182.0110)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0587  max mem: 15572
Epoch: [29]  [ 190/2809]  eta: 0:26:09  lr: 0.000010  min_lr: 0.000000  loss: 4.1238 (4.1727)  class_acc: 0.2917 (0.3229)  loss_scale: 65536.0000 (43404.7330)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1310  max mem: 15572
[2025-01-16 04:12:32,397] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81652
[2025-01-16 04:12:32,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:12:32,397] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [ 200/2809]  eta: 0:25:56  lr: 0.000010  min_lr: 0.000000  loss: 4.3106 (4.1854)  class_acc: 0.2500 (0.3211)  loss_scale: 32768.0000 (42875.5423)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1564  max mem: 15572
Epoch: [29]  [ 210/2809]  eta: 0:25:53  lr: 0.000010  min_lr: 0.000000  loss: 4.3106 (4.1925)  class_acc: 0.2500 (0.3185)  loss_scale: 32768.0000 (42396.5118)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1510  max mem: 15572
Epoch: [29]  [ 220/2809]  eta: 0:25:39  lr: 0.000010  min_lr: 0.000000  loss: 4.2431 (4.2009)  class_acc: 0.2500 (0.3167)  loss_scale: 32768.0000 (41960.8326)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1376  max mem: 15572
Epoch: [29]  [ 230/2809]  eta: 0:25:29  lr: 0.000010  min_lr: 0.000000  loss: 4.3202 (4.2020)  class_acc: 0.2917 (0.3182)  loss_scale: 32768.0000 (41562.8745)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.1034  max mem: 15572
Epoch: [29]  [ 240/2809]  eta: 0:25:20  lr: 0.000010  min_lr: 0.000000  loss: 4.3012 (4.2069)  class_acc: 0.2917 (0.3193)  loss_scale: 32768.0000 (41197.9419)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1177  max mem: 15572
Epoch: [29]  [ 250/2809]  eta: 0:25:09  lr: 0.000010  min_lr: 0.000000  loss: 4.1857 (4.2090)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (40862.0876)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1128  max mem: 15572
Epoch: [29]  [ 260/2809]  eta: 0:24:58  lr: 0.000010  min_lr: 0.000000  loss: 4.2794 (4.2142)  class_acc: 0.2500 (0.3180)  loss_scale: 32768.0000 (40551.9693)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.1012  max mem: 15572
Epoch: [29]  [ 270/2809]  eta: 0:24:53  lr: 0.000010  min_lr: 0.000000  loss: 4.2794 (4.2124)  class_acc: 0.2500 (0.3192)  loss_scale: 32768.0000 (40264.7380)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1177  max mem: 15572
Epoch: [29]  [ 280/2809]  eta: 0:24:41  lr: 0.000010  min_lr: 0.000000  loss: 4.2181 (4.2088)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (39997.9502)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1005  max mem: 15572
Epoch: [29]  [ 290/2809]  eta: 0:24:36  lr: 0.000010  min_lr: 0.000000  loss: 4.2338 (4.2104)  class_acc: 0.2500 (0.3182)  loss_scale: 32768.0000 (39749.4983)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1110  max mem: 15572
Epoch: [29]  [ 300/2809]  eta: 0:24:29  lr: 0.000010  min_lr: 0.000000  loss: 4.2526 (4.2088)  class_acc: 0.3333 (0.3205)  loss_scale: 32768.0000 (39517.5548)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1375  max mem: 15572
Epoch: [29]  [ 310/2809]  eta: 0:24:14  lr: 0.000010  min_lr: 0.000000  loss: 4.1252 (4.2090)  class_acc: 0.2917 (0.3199)  loss_scale: 32768.0000 (39300.5273)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0699  max mem: 15572
[2025-01-16 04:13:44,756] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:13:44,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 320/2809]  eta: 0:24:11  lr: 0.000010  min_lr: 0.000000  loss: 4.0925 (4.2046)  class_acc: 0.2917 (0.3189)  loss_scale: 32768.0000 (39199.1028)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.1002  max mem: 15572
Epoch: [29]  [ 330/2809]  eta: 0:23:59  lr: 0.000010  min_lr: 0.000000  loss: 4.2145 (4.2085)  class_acc: 0.2917 (0.3190)  loss_scale: 65536.0000 (39994.7795)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1197  max mem: 15572
[2025-01-16 04:13:50,212] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81792
[2025-01-16 04:13:50,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:13:50,212] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [ 340/2809]  eta: 0:23:49  lr: 0.000010  min_lr: 0.000000  loss: 4.2335 (4.2078)  class_acc: 0.2917 (0.3195)  loss_scale: 32768.0000 (39782.8504)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0592  max mem: 15572
Epoch: [29]  [ 350/2809]  eta: 0:23:40  lr: 0.000010  min_lr: 0.000000  loss: 4.2759 (4.2092)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (39582.9972)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0757  max mem: 15572
Epoch: [29]  [ 360/2809]  eta: 0:23:32  lr: 0.000010  min_lr: 0.000000  loss: 4.2291 (4.2088)  class_acc: 0.2917 (0.3195)  loss_scale: 32768.0000 (39394.2161)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0827  max mem: 15572
Epoch: [29]  [ 370/2809]  eta: 0:23:26  lr: 0.000010  min_lr: 0.000000  loss: 4.1645 (4.2107)  class_acc: 0.3333 (0.3201)  loss_scale: 32768.0000 (39215.6119)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0930  max mem: 15572
Epoch: [29]  [ 380/2809]  eta: 0:23:23  lr: 0.000010  min_lr: 0.000000  loss: 4.1810 (4.2106)  class_acc: 0.3750 (0.3223)  loss_scale: 32768.0000 (39046.3832)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1590  max mem: 15572
Epoch: [29]  [ 390/2809]  eta: 0:23:17  lr: 0.000010  min_lr: 0.000000  loss: 4.1802 (4.2096)  class_acc: 0.3750 (0.3231)  loss_scale: 32768.0000 (38885.8107)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1728  max mem: 15572
Epoch: [29]  [ 400/2809]  eta: 0:23:14  lr: 0.000010  min_lr: 0.000000  loss: 4.2121 (4.2112)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (38733.2469)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1588  max mem: 15572
Epoch: [29]  [ 410/2809]  eta: 0:23:11  lr: 0.000010  min_lr: 0.000000  loss: 4.1390 (4.2085)  class_acc: 0.2917 (0.3217)  loss_scale: 32768.0000 (38588.1071)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1891  max mem: 15572
Epoch: [29]  [ 420/2809]  eta: 0:23:05  lr: 0.000010  min_lr: 0.000000  loss: 4.1390 (4.2095)  class_acc: 0.3333 (0.3231)  loss_scale: 32768.0000 (38449.8622)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1627  max mem: 15572
Epoch: [29]  [ 430/2809]  eta: 0:23:02  lr: 0.000010  min_lr: 0.000000  loss: 4.1914 (4.2074)  class_acc: 0.3333 (0.3239)  loss_scale: 32768.0000 (38318.0325)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1401  max mem: 15572
Epoch: [29]  [ 440/2809]  eta: 0:22:48  lr: 0.000010  min_lr: 0.000000  loss: 4.0868 (4.2060)  class_acc: 0.2500 (0.3228)  loss_scale: 32768.0000 (38192.1814)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0780  max mem: 15572
Epoch: [29]  [ 450/2809]  eta: 0:22:38  lr: 0.000010  min_lr: 0.000000  loss: 4.0868 (4.2030)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (38071.9113)  weight_decay: 0.0500 (0.0500)  time: 0.4704  data: 0.0132  max mem: 15572
[2025-01-16 04:15:02,627] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:15:02,627] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 460/2809]  eta: 0:22:30  lr: 0.000010  min_lr: 0.000000  loss: 4.1954 (4.2041)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (38027.9393)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0460  max mem: 15572
Epoch: [29]  [ 470/2809]  eta: 0:22:21  lr: 0.000010  min_lr: 0.000000  loss: 4.1904 (4.2030)  class_acc: 0.2917 (0.3230)  loss_scale: 65536.0000 (38611.9745)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0336  max mem: 15572
Epoch: [29]  [ 480/2809]  eta: 0:22:19  lr: 0.000010  min_lr: 0.000000  loss: 4.1654 (4.2084)  class_acc: 0.2500 (0.3209)  loss_scale: 65536.0000 (39171.7256)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1071  max mem: 15572
Epoch: [29]  [ 490/2809]  eta: 0:22:11  lr: 0.000010  min_lr: 0.000000  loss: 4.2754 (4.2091)  class_acc: 0.2500 (0.3210)  loss_scale: 65536.0000 (39708.6762)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1402  max mem: 15572
Epoch: [29]  [ 500/2809]  eta: 0:22:08  lr: 0.000010  min_lr: 0.000000  loss: 4.1853 (4.2092)  class_acc: 0.2917 (0.3200)  loss_scale: 65536.0000 (40224.1916)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1381  max mem: 15572
Epoch: [29]  [ 510/2809]  eta: 0:21:58  lr: 0.000010  min_lr: 0.000000  loss: 4.2972 (4.2127)  class_acc: 0.2917 (0.3194)  loss_scale: 65536.0000 (40719.5303)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1101  max mem: 15572
Epoch: [29]  [ 520/2809]  eta: 0:21:54  lr: 0.000010  min_lr: 0.000000  loss: 4.2952 (4.2119)  class_acc: 0.2917 (0.3192)  loss_scale: 65536.0000 (41195.8541)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0723  max mem: 15572
[2025-01-16 04:15:37,664] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81983
[2025-01-16 04:15:37,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:15:37,664] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [ 530/2809]  eta: 0:21:49  lr: 0.000010  min_lr: 0.000000  loss: 4.1105 (4.2113)  class_acc: 0.2917 (0.3180)  loss_scale: 65536.0000 (41098.8475)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1439  max mem: 15572
[2025-01-16 04:15:46,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=82000, skipped=511, lr=[9.909992081020063e-08, 9.909992081020063e-08, 1.4157131544314378e-07, 1.4157131544314378e-07, 2.0224473634734827e-07, 2.0224473634734827e-07, 2.889210519247833e-07, 2.889210519247833e-07, 4.1274435989254753e-07, 4.1274435989254753e-07, 5.896347998464965e-07, 5.896347998464965e-07, 8.423354283521379e-07, 8.423354283521379e-07, 1.20333632621734e-06, 1.20333632621734e-06, 1.7190518945962e-06, 1.7190518945962e-06, 2.4557884208517147e-06, 2.4557884208517147e-06, 3.5082691726453065e-06, 3.5082691726453065e-06, 5.01181310377901e-06, 5.01181310377901e-06, 7.159733005398586e-06, 7.159733005398586e-06, 1.0228190007712266e-05, 1.0228190007712266e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:15:46,554] [INFO] [timer.py:260:stop] epoch=0/micro_step=82000/global_step=82000, RunningAvgSamplesPerSec=27.926686632630094, CurrSamplesPerSec=30.325385004699587, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [ 540/2809]  eta: 0:21:45  lr: 0.000010  min_lr: 0.000000  loss: 4.3288 (4.2148)  class_acc: 0.2083 (0.3171)  loss_scale: 32768.0000 (40944.8577)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1443  max mem: 15572
Epoch: [29]  [ 550/2809]  eta: 0:21:40  lr: 0.000010  min_lr: 0.000000  loss: 4.3433 (4.2161)  class_acc: 0.2917 (0.3168)  loss_scale: 32768.0000 (40796.4574)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1452  max mem: 15572
Epoch: [29]  [ 560/2809]  eta: 0:21:36  lr: 0.000010  min_lr: 0.000000  loss: 4.2118 (4.2147)  class_acc: 0.2917 (0.3157)  loss_scale: 32768.0000 (40653.3476)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1653  max mem: 15572
Epoch: [29]  [ 570/2809]  eta: 0:21:33  lr: 0.000010  min_lr: 0.000000  loss: 4.0833 (4.2125)  class_acc: 0.2917 (0.3157)  loss_scale: 32768.0000 (40515.2504)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1775  max mem: 15572
Epoch: [29]  [ 580/2809]  eta: 0:21:24  lr: 0.000010  min_lr: 0.000000  loss: 4.0995 (4.2132)  class_acc: 0.3333 (0.3153)  loss_scale: 32768.0000 (40381.9071)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1223  max mem: 15572
Epoch: [29]  [ 590/2809]  eta: 0:21:17  lr: 0.000010  min_lr: 0.000000  loss: 4.3597 (4.2173)  class_acc: 0.2917 (0.3151)  loss_scale: 32768.0000 (40253.0761)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0922  max mem: 15572
Epoch: [29]  [ 600/2809]  eta: 0:21:09  lr: 0.000010  min_lr: 0.000000  loss: 4.3196 (4.2178)  class_acc: 0.2917 (0.3148)  loss_scale: 32768.0000 (40128.5324)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0685  max mem: 15572
Epoch: [29]  [ 610/2809]  eta: 0:21:03  lr: 0.000010  min_lr: 0.000000  loss: 4.2022 (4.2174)  class_acc: 0.2917 (0.3150)  loss_scale: 32768.0000 (40008.0655)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0670  max mem: 15572
Epoch: [29]  [ 620/2809]  eta: 0:20:56  lr: 0.000010  min_lr: 0.000000  loss: 4.2498 (4.2184)  class_acc: 0.2917 (0.3138)  loss_scale: 32768.0000 (39891.4783)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1250  max mem: 15572
Epoch: [29]  [ 630/2809]  eta: 0:20:50  lr: 0.000010  min_lr: 0.000000  loss: 4.2945 (4.2201)  class_acc: 0.2500 (0.3133)  loss_scale: 32768.0000 (39778.5864)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1099  max mem: 15572
Epoch: [29]  [ 640/2809]  eta: 0:20:46  lr: 0.000010  min_lr: 0.000000  loss: 4.2533 (4.2207)  class_acc: 0.2917 (0.3137)  loss_scale: 32768.0000 (39669.2168)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1264  max mem: 15572
Epoch: [29]  [ 650/2809]  eta: 0:20:39  lr: 0.000010  min_lr: 0.000000  loss: 4.3073 (4.2213)  class_acc: 0.2917 (0.3137)  loss_scale: 32768.0000 (39563.2074)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1417  max mem: 15572
[2025-01-16 04:16:52,044] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:16:52,045] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 660/2809]  eta: 0:20:31  lr: 0.000010  min_lr: 0.000000  loss: 4.3077 (4.2215)  class_acc: 0.2500 (0.3140)  loss_scale: 32768.0000 (39956.1392)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0679  max mem: 15572
Epoch: [29]  [ 670/2809]  eta: 0:20:26  lr: 0.000010  min_lr: 0.000000  loss: 4.1750 (4.2212)  class_acc: 0.2500 (0.3132)  loss_scale: 65536.0000 (40337.3592)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0955  max mem: 15572
[2025-01-16 04:17:04,241] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82135
[2025-01-16 04:17:04,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:17:04,241] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [ 680/2809]  eta: 0:20:17  lr: 0.000010  min_lr: 0.000000  loss: 4.1118 (4.2196)  class_acc: 0.2500 (0.3125)  loss_scale: 65536.0000 (40370.5609)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0960  max mem: 15572
Epoch: [29]  [ 690/2809]  eta: 0:20:12  lr: 0.000010  min_lr: 0.000000  loss: 4.2432 (4.2208)  class_acc: 0.2500 (0.3129)  loss_scale: 32768.0000 (40260.5384)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0778  max mem: 15572
Epoch: [29]  [ 700/2809]  eta: 0:20:05  lr: 0.000010  min_lr: 0.000000  loss: 4.2432 (4.2231)  class_acc: 0.3333 (0.3137)  loss_scale: 32768.0000 (40153.6548)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1080  max mem: 15572
Epoch: [29]  [ 710/2809]  eta: 0:19:59  lr: 0.000010  min_lr: 0.000000  loss: 4.1988 (4.2226)  class_acc: 0.3333 (0.3134)  loss_scale: 32768.0000 (40049.7778)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0813  max mem: 15572
Epoch: [29]  [ 720/2809]  eta: 0:19:52  lr: 0.000010  min_lr: 0.000000  loss: 4.1455 (4.2206)  class_acc: 0.2917 (0.3137)  loss_scale: 32768.0000 (39948.7822)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0828  max mem: 15572
Epoch: [29]  [ 730/2809]  eta: 0:19:47  lr: 0.000010  min_lr: 0.000000  loss: 4.0848 (4.2193)  class_acc: 0.2917 (0.3141)  loss_scale: 32768.0000 (39850.5499)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1027  max mem: 15572
Epoch: [29]  [ 740/2809]  eta: 0:19:39  lr: 0.000010  min_lr: 0.000000  loss: 4.0848 (4.2193)  class_acc: 0.3333 (0.3149)  loss_scale: 32768.0000 (39754.9690)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0816  max mem: 15572
Epoch: [29]  [ 750/2809]  eta: 0:19:35  lr: 0.000010  min_lr: 0.000000  loss: 4.2599 (4.2180)  class_acc: 0.3333 (0.3164)  loss_scale: 32768.0000 (39661.9334)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0960  max mem: 15572
Epoch: [29]  [ 760/2809]  eta: 0:19:28  lr: 0.000010  min_lr: 0.000000  loss: 4.2599 (4.2180)  class_acc: 0.3333 (0.3165)  loss_scale: 32768.0000 (39571.3430)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1236  max mem: 15572
Epoch: [29]  [ 770/2809]  eta: 0:19:25  lr: 0.000010  min_lr: 0.000000  loss: 4.3027 (4.2174)  class_acc: 0.3333 (0.3166)  loss_scale: 32768.0000 (39483.1025)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1570  max mem: 15572
Epoch: [29]  [ 780/2809]  eta: 0:19:20  lr: 0.000010  min_lr: 0.000000  loss: 4.1848 (4.2161)  class_acc: 0.3750 (0.3171)  loss_scale: 32768.0000 (39397.1216)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1758  max mem: 15572
Epoch: [29]  [ 790/2809]  eta: 0:19:14  lr: 0.000010  min_lr: 0.000000  loss: 4.1701 (4.2156)  class_acc: 0.3750 (0.3176)  loss_scale: 32768.0000 (39313.3148)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1245  max mem: 15572
Epoch: [29]  [ 800/2809]  eta: 0:19:07  lr: 0.000010  min_lr: 0.000000  loss: 4.1701 (4.2161)  class_acc: 0.2917 (0.3169)  loss_scale: 32768.0000 (39231.6005)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.0930  max mem: 15572
[2025-01-16 04:18:17,140] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:18:17,140] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 810/2809]  eta: 0:19:02  lr: 0.000010  min_lr: 0.000000  loss: 4.2308 (4.2154)  class_acc: 0.2500 (0.3165)  loss_scale: 32768.0000 (39475.1369)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1026  max mem: 15572
Epoch: [29]  [ 820/2809]  eta: 0:18:56  lr: 0.000010  min_lr: 0.000000  loss: 4.1325 (4.2136)  class_acc: 0.2917 (0.3171)  loss_scale: 65536.0000 (39792.5652)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1352  max mem: 15572
Epoch: [29]  [ 830/2809]  eta: 0:18:50  lr: 0.000010  min_lr: 0.000000  loss: 4.1882 (4.2131)  class_acc: 0.2917 (0.3177)  loss_scale: 65536.0000 (40102.3538)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1237  max mem: 15572
Epoch: [29]  [ 840/2809]  eta: 0:18:45  lr: 0.000010  min_lr: 0.000000  loss: 4.2447 (4.2142)  class_acc: 0.2917 (0.3173)  loss_scale: 65536.0000 (40404.7753)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1326  max mem: 15572
[2025-01-16 04:18:40,784] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82305
[2025-01-16 04:18:40,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:18:40,785] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [ 850/2809]  eta: 0:18:40  lr: 0.000010  min_lr: 0.000000  loss: 4.2389 (4.2144)  class_acc: 0.2500 (0.3174)  loss_scale: 65536.0000 (40430.5523)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1382  max mem: 15572
Epoch: [29]  [ 860/2809]  eta: 0:18:32  lr: 0.000010  min_lr: 0.000000  loss: 4.1908 (4.2136)  class_acc: 0.2500 (0.3165)  loss_scale: 32768.0000 (40341.5563)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0804  max mem: 15572
Epoch: [29]  [ 870/2809]  eta: 0:18:25  lr: 0.000010  min_lr: 0.000000  loss: 4.3095 (4.2150)  class_acc: 0.2083 (0.3165)  loss_scale: 32768.0000 (40254.6039)  weight_decay: 0.0500 (0.0500)  time: 0.4919  data: 0.0443  max mem: 15572
Epoch: [29]  [ 880/2809]  eta: 0:18:18  lr: 0.000010  min_lr: 0.000000  loss: 4.2179 (4.2140)  class_acc: 0.2917 (0.3166)  loss_scale: 32768.0000 (40169.6254)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0833  max mem: 15572
Epoch: [29]  [ 890/2809]  eta: 0:18:14  lr: 0.000010  min_lr: 0.000000  loss: 4.1937 (4.2149)  class_acc: 0.3750 (0.3174)  loss_scale: 32768.0000 (40086.5544)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1369  max mem: 15572
Epoch: [29]  [ 900/2809]  eta: 0:18:07  lr: 0.000010  min_lr: 0.000000  loss: 4.2740 (4.2146)  class_acc: 0.2917 (0.3168)  loss_scale: 32768.0000 (40005.3274)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1228  max mem: 15572
Epoch: [29]  [ 910/2809]  eta: 0:18:02  lr: 0.000010  min_lr: 0.000000  loss: 4.2468 (4.2149)  class_acc: 0.2500 (0.3163)  loss_scale: 32768.0000 (39925.8836)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0985  max mem: 15572
Epoch: [29]  [ 920/2809]  eta: 0:17:55  lr: 0.000010  min_lr: 0.000000  loss: 4.0905 (4.2138)  class_acc: 0.2500 (0.3161)  loss_scale: 32768.0000 (39848.1650)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0894  max mem: 15572
Epoch: [29]  [ 930/2809]  eta: 0:17:50  lr: 0.000010  min_lr: 0.000000  loss: 4.1176 (4.2145)  class_acc: 0.2917 (0.3159)  loss_scale: 32768.0000 (39772.1160)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1013  max mem: 15572
Epoch: [29]  [ 940/2809]  eta: 0:17:46  lr: 0.000010  min_lr: 0.000000  loss: 4.2056 (4.2139)  class_acc: 0.2917 (0.3159)  loss_scale: 32768.0000 (39697.6833)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1731  max mem: 15572
Epoch: [29]  [ 950/2809]  eta: 0:17:41  lr: 0.000010  min_lr: 0.000000  loss: 4.2299 (4.2143)  class_acc: 0.2500 (0.3151)  loss_scale: 32768.0000 (39624.8160)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1636  max mem: 15572
Epoch: [29]  [ 960/2809]  eta: 0:17:35  lr: 0.000010  min_lr: 0.000000  loss: 4.3168 (4.2156)  class_acc: 0.2500 (0.3146)  loss_scale: 32768.0000 (39553.4651)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1253  max mem: 15572
Epoch: [29]  [ 970/2809]  eta: 0:17:31  lr: 0.000010  min_lr: 0.000000  loss: 4.2326 (4.2149)  class_acc: 0.2917 (0.3144)  loss_scale: 32768.0000 (39483.5839)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1804  max mem: 15572
[2025-01-16 04:19:56,209] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:19:56,209] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 980/2809]  eta: 0:17:26  lr: 0.000010  min_lr: 0.000000  loss: 4.1658 (4.2150)  class_acc: 0.2917 (0.3143)  loss_scale: 32768.0000 (39682.3486)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.2088  max mem: 15572
Epoch: [29]  [ 990/2809]  eta: 0:17:20  lr: 0.000010  min_lr: 0.000000  loss: 4.2465 (4.2165)  class_acc: 0.2083 (0.3133)  loss_scale: 65536.0000 (39943.2331)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1518  max mem: 15572
Epoch: [29]  [1000/2809]  eta: 0:17:15  lr: 0.000010  min_lr: 0.000000  loss: 4.2465 (4.2165)  class_acc: 0.2083 (0.3124)  loss_scale: 65536.0000 (40198.9051)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1510  max mem: 15572
[2025-01-16 04:20:13,976] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82465
[2025-01-16 04:20:13,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:20:13,976] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1010/2809]  eta: 0:17:10  lr: 0.000010  min_lr: 0.000000  loss: 4.1446 (4.2152)  class_acc: 0.2083 (0.3126)  loss_scale: 65536.0000 (40222.6390)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1551  max mem: 15572
Epoch: [29]  [1020/2809]  eta: 0:17:03  lr: 0.000010  min_lr: 0.000000  loss: 4.1200 (4.2146)  class_acc: 0.2917 (0.3128)  loss_scale: 32768.0000 (40149.6259)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0869  max mem: 15572
Epoch: [29]  [1030/2809]  eta: 0:16:58  lr: 0.000010  min_lr: 0.000000  loss: 4.0933 (4.2127)  class_acc: 0.2917 (0.3126)  loss_scale: 32768.0000 (40078.0291)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1252  max mem: 15572
Epoch: [29]  [1040/2809]  eta: 0:16:53  lr: 0.000010  min_lr: 0.000000  loss: 3.9906 (4.2130)  class_acc: 0.2917 (0.3127)  loss_scale: 32768.0000 (40007.8079)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.1833  max mem: 15572
Epoch: [29]  [1050/2809]  eta: 0:16:47  lr: 0.000010  min_lr: 0.000000  loss: 4.2517 (4.2148)  class_acc: 0.3333 (0.3124)  loss_scale: 32768.0000 (39938.9229)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1308  max mem: 15572
Epoch: [29]  [1060/2809]  eta: 0:16:41  lr: 0.000010  min_lr: 0.000000  loss: 4.2419 (4.2148)  class_acc: 0.2500 (0.3119)  loss_scale: 32768.0000 (39871.3365)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0953  max mem: 15572
Epoch: [29]  [1070/2809]  eta: 0:16:35  lr: 0.000010  min_lr: 0.000000  loss: 4.2454 (4.2156)  class_acc: 0.2500 (0.3120)  loss_scale: 32768.0000 (39805.0121)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0889  max mem: 15572
Epoch: [29]  [1080/2809]  eta: 0:16:28  lr: 0.000010  min_lr: 0.000000  loss: 4.3048 (4.2163)  class_acc: 0.2917 (0.3116)  loss_scale: 32768.0000 (39739.9149)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0650  max mem: 15572
Epoch: [29]  [1090/2809]  eta: 0:16:22  lr: 0.000010  min_lr: 0.000000  loss: 4.2459 (4.2170)  class_acc: 0.2083 (0.3107)  loss_scale: 32768.0000 (39676.0110)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0538  max mem: 15572
Epoch: [29]  [1100/2809]  eta: 0:16:16  lr: 0.000010  min_lr: 0.000000  loss: 4.1816 (4.2154)  class_acc: 0.2917 (0.3109)  loss_scale: 32768.0000 (39613.2679)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0781  max mem: 15572
Epoch: [29]  [1110/2809]  eta: 0:16:10  lr: 0.000010  min_lr: 0.000000  loss: 4.1377 (4.2153)  class_acc: 0.3750 (0.3115)  loss_scale: 32768.0000 (39551.6544)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0935  max mem: 15572
Epoch: [29]  [1120/2809]  eta: 0:16:03  lr: 0.000010  min_lr: 0.000000  loss: 4.1640 (4.2148)  class_acc: 0.3333 (0.3120)  loss_scale: 32768.0000 (39491.1401)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0929  max mem: 15572
Epoch: [29]  [1130/2809]  eta: 0:15:58  lr: 0.000010  min_lr: 0.000000  loss: 4.2420 (4.2173)  class_acc: 0.3333 (0.3119)  loss_scale: 32768.0000 (39431.6958)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.1237  max mem: 15572
[2025-01-16 04:21:25,713] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:21:25,713] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [1140/2809]  eta: 0:15:52  lr: 0.000010  min_lr: 0.000000  loss: 4.4194 (4.2179)  class_acc: 0.2917 (0.3119)  loss_scale: 32768.0000 (39603.0429)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1333  max mem: 15572
Epoch: [29]  [1150/2809]  eta: 0:15:47  lr: 0.000010  min_lr: 0.000000  loss: 4.2323 (4.2179)  class_acc: 0.2917 (0.3118)  loss_scale: 65536.0000 (39828.3510)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1611  max mem: 15572
Epoch: [29]  [1160/2809]  eta: 0:15:41  lr: 0.000010  min_lr: 0.000000  loss: 4.2866 (4.2183)  class_acc: 0.2500 (0.3117)  loss_scale: 65536.0000 (40049.7778)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1506  max mem: 15572
Epoch: [29]  [1170/2809]  eta: 0:15:36  lr: 0.000010  min_lr: 0.000000  loss: 4.3347 (4.2193)  class_acc: 0.2500 (0.3113)  loss_scale: 65536.0000 (40267.4227)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1428  max mem: 15572
Epoch: [29]  [1180/2809]  eta: 0:15:31  lr: 0.000010  min_lr: 0.000000  loss: 4.2654 (4.2184)  class_acc: 0.2917 (0.3112)  loss_scale: 65536.0000 (40481.3819)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1755  max mem: 15572
Epoch: [29]  [1190/2809]  eta: 0:15:24  lr: 0.000010  min_lr: 0.000000  loss: 4.1156 (4.2191)  class_acc: 0.2917 (0.3113)  loss_scale: 65536.0000 (40691.7481)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1269  max mem: 15572
Epoch: [29]  [1200/2809]  eta: 0:15:20  lr: 0.000010  min_lr: 0.000000  loss: 4.1058 (4.2176)  class_acc: 0.3333 (0.3114)  loss_scale: 65536.0000 (40898.6112)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1393  max mem: 15572
Epoch: [29]  [1210/2809]  eta: 0:15:12  lr: 0.000010  min_lr: 0.000000  loss: 4.1292 (4.2170)  class_acc: 0.3333 (0.3117)  loss_scale: 65536.0000 (41102.0578)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.1084  max mem: 15572
Epoch: [29]  [1220/2809]  eta: 0:15:06  lr: 0.000010  min_lr: 0.000000  loss: 4.2134 (4.2172)  class_acc: 0.2917 (0.3114)  loss_scale: 65536.0000 (41302.1720)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0571  max mem: 15572
Epoch: [29]  [1230/2809]  eta: 0:15:00  lr: 0.000010  min_lr: 0.000000  loss: 4.3548 (4.2185)  class_acc: 0.2917 (0.3113)  loss_scale: 65536.0000 (41499.0349)  weight_decay: 0.0500 (0.0500)  time: 0.5229  data: 0.0684  max mem: 15572
[2025-01-16 04:22:21,414] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82693
[2025-01-16 04:22:21,415] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:22:21,415] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1240/2809]  eta: 0:14:53  lr: 0.000010  min_lr: 0.000000  loss: 4.4030 (4.2192)  class_acc: 0.2917 (0.3110)  loss_scale: 65536.0000 (41455.0846)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0689  max mem: 15572
Epoch: [29]  [1250/2809]  eta: 0:14:49  lr: 0.000010  min_lr: 0.000000  loss: 4.2138 (4.2181)  class_acc: 0.2917 (0.3111)  loss_scale: 32768.0000 (41385.6435)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1343  max mem: 15572
Epoch: [29]  [1260/2809]  eta: 0:14:43  lr: 0.000010  min_lr: 0.000000  loss: 4.2648 (4.2187)  class_acc: 0.3333 (0.3111)  loss_scale: 32768.0000 (41317.3037)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1599  max mem: 15572
Epoch: [29]  [1270/2809]  eta: 0:14:38  lr: 0.000010  min_lr: 0.000000  loss: 4.2249 (4.2179)  class_acc: 0.3750 (0.3115)  loss_scale: 32768.0000 (41250.0393)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1666  max mem: 15572
Epoch: [29]  [1280/2809]  eta: 0:14:32  lr: 0.000010  min_lr: 0.000000  loss: 4.1390 (4.2176)  class_acc: 0.3750 (0.3118)  loss_scale: 32768.0000 (41183.8251)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1534  max mem: 15572
Epoch: [29]  [1290/2809]  eta: 0:14:27  lr: 0.000010  min_lr: 0.000000  loss: 4.1397 (4.2181)  class_acc: 0.2917 (0.3117)  loss_scale: 32768.0000 (41118.6367)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1497  max mem: 15572
Epoch: [29]  [1300/2809]  eta: 0:14:21  lr: 0.000010  min_lr: 0.000000  loss: 4.3022 (4.2183)  class_acc: 0.2917 (0.3118)  loss_scale: 32768.0000 (41054.4504)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1603  max mem: 15572
Epoch: [29]  [1310/2809]  eta: 0:14:15  lr: 0.000010  min_lr: 0.000000  loss: 4.1761 (4.2177)  class_acc: 0.2500 (0.3114)  loss_scale: 32768.0000 (40991.2433)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.1061  max mem: 15572
Epoch: [29]  [1320/2809]  eta: 0:14:09  lr: 0.000010  min_lr: 0.000000  loss: 4.1761 (4.2173)  class_acc: 0.2500 (0.3115)  loss_scale: 32768.0000 (40928.9932)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0864  max mem: 15572
Epoch: [29]  [1330/2809]  eta: 0:14:02  lr: 0.000010  min_lr: 0.000000  loss: 4.2068 (4.2170)  class_acc: 0.3333 (0.3117)  loss_scale: 32768.0000 (40867.6784)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0591  max mem: 15572
Epoch: [29]  [1340/2809]  eta: 0:13:56  lr: 0.000010  min_lr: 0.000000  loss: 4.2923 (4.2181)  class_acc: 0.3750 (0.3116)  loss_scale: 32768.0000 (40807.2782)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0436  max mem: 15572
Epoch: [29]  [1350/2809]  eta: 0:13:50  lr: 0.000010  min_lr: 0.000000  loss: 4.3215 (4.2184)  class_acc: 0.3333 (0.3118)  loss_scale: 32768.0000 (40747.7720)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0926  max mem: 15572
Epoch: [29]  [1360/2809]  eta: 0:13:45  lr: 0.000010  min_lr: 0.000000  loss: 4.2084 (4.2183)  class_acc: 0.3333 (0.3120)  loss_scale: 32768.0000 (40689.1403)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1427  max mem: 15572
[2025-01-16 04:23:33,989] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:23:33,989] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [1370/2809]  eta: 0:13:38  lr: 0.000010  min_lr: 0.000000  loss: 4.0864 (4.2176)  class_acc: 0.2917 (0.3118)  loss_scale: 32768.0000 (40870.3720)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0955  max mem: 15572
Epoch: [29]  [1380/2809]  eta: 0:13:33  lr: 0.000010  min_lr: 0.000000  loss: 4.0880 (4.2174)  class_acc: 0.2917 (0.3114)  loss_scale: 65536.0000 (41048.9790)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1174  max mem: 15572
Epoch: [29]  [1390/2809]  eta: 0:13:27  lr: 0.000010  min_lr: 0.000000  loss: 4.3009 (4.2186)  class_acc: 0.2917 (0.3110)  loss_scale: 65536.0000 (41225.0180)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1501  max mem: 15572
Epoch: [29]  [1400/2809]  eta: 0:13:23  lr: 0.000010  min_lr: 0.000000  loss: 4.3359 (4.2191)  class_acc: 0.3333 (0.3115)  loss_scale: 65536.0000 (41398.5439)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1553  max mem: 15572
Epoch: [29]  [1410/2809]  eta: 0:13:16  lr: 0.000010  min_lr: 0.000000  loss: 4.1428 (4.2180)  class_acc: 0.3333 (0.3115)  loss_scale: 65536.0000 (41569.6102)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1588  max mem: 15572
Epoch: [29]  [1420/2809]  eta: 0:13:11  lr: 0.000010  min_lr: 0.000000  loss: 4.0892 (4.2182)  class_acc: 0.2917 (0.3112)  loss_scale: 65536.0000 (41738.2688)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.1036  max mem: 15572
Epoch: [29]  [1430/2809]  eta: 0:13:05  lr: 0.000010  min_lr: 0.000000  loss: 4.2135 (4.2182)  class_acc: 0.2917 (0.3111)  loss_scale: 65536.0000 (41904.5702)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.1013  max mem: 15572
Epoch: [29]  [1440/2809]  eta: 0:13:00  lr: 0.000010  min_lr: 0.000000  loss: 4.2144 (4.2180)  class_acc: 0.2917 (0.3113)  loss_scale: 65536.0000 (42068.5635)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1413  max mem: 15572
Epoch: [29]  [1450/2809]  eta: 0:12:54  lr: 0.000010  min_lr: 0.000000  loss: 4.1752 (4.2174)  class_acc: 0.2917 (0.3112)  loss_scale: 65536.0000 (42230.2963)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1711  max mem: 15572
Epoch: [29]  [1460/2809]  eta: 0:12:48  lr: 0.000010  min_lr: 0.000000  loss: 4.1752 (4.2170)  class_acc: 0.3333 (0.3115)  loss_scale: 65536.0000 (42389.8152)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1330  max mem: 15572
[2025-01-16 04:24:34,867] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82928
[2025-01-16 04:24:34,867] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:24:34,868] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1470/2809]  eta: 0:12:42  lr: 0.000010  min_lr: 0.000000  loss: 4.2842 (4.2181)  class_acc: 0.3333 (0.3114)  loss_scale: 65536.0000 (42458.0612)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1147  max mem: 15572
Epoch: [29]  [1480/2809]  eta: 0:12:37  lr: 0.000010  min_lr: 0.000000  loss: 4.3165 (4.2184)  class_acc: 0.2917 (0.3111)  loss_scale: 32768.0000 (42392.6320)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1447  max mem: 15572
Epoch: [29]  [1490/2809]  eta: 0:12:31  lr: 0.000010  min_lr: 0.000000  loss: 4.3165 (4.2187)  class_acc: 0.2917 (0.3111)  loss_scale: 32768.0000 (42328.0805)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1401  max mem: 15572
Epoch: [29]  [1500/2809]  eta: 0:12:26  lr: 0.000010  min_lr: 0.000000  loss: 4.1784 (4.2174)  class_acc: 0.3333 (0.3117)  loss_scale: 32768.0000 (42264.3891)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1095  max mem: 15572
Epoch: [29]  [1510/2809]  eta: 0:12:19  lr: 0.000010  min_lr: 0.000000  loss: 4.1784 (4.2181)  class_acc: 0.3333 (0.3119)  loss_scale: 32768.0000 (42201.5407)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0590  max mem: 15572
Epoch: [29]  [1520/2809]  eta: 0:12:13  lr: 0.000010  min_lr: 0.000000  loss: 4.2263 (4.2180)  class_acc: 0.3750 (0.3122)  loss_scale: 32768.0000 (42139.5187)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0584  max mem: 15572
Epoch: [29]  [1530/2809]  eta: 0:12:08  lr: 0.000010  min_lr: 0.000000  loss: 4.1047 (4.2178)  class_acc: 0.3750 (0.3123)  loss_scale: 32768.0000 (42078.3070)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1380  max mem: 15572
[2025-01-16 04:25:14,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=83000, skipped=516, lr=[9.318606680013476e-08, 9.318606680013476e-08, 1.331229525716211e-07, 1.331229525716211e-07, 1.901756465308873e-07, 1.901756465308873e-07, 2.7167949504412473e-07, 2.7167949504412473e-07, 3.8811356434874963e-07, 3.8811356434874963e-07, 5.544479490696423e-07, 5.544479490696423e-07, 7.920684986709176e-07, 7.920684986709176e-07, 1.1315264266727396e-06, 1.1315264266727396e-06, 1.6164663238181993e-06, 1.6164663238181993e-06, 2.309237605454571e-06, 2.309237605454571e-06, 3.2989108649351014e-06, 3.2989108649351014e-06, 4.712729807050145e-06, 4.712729807050145e-06, 6.732471152928779e-06, 6.732471152928779e-06, 9.6178159327554e-06, 9.6178159327554e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:25:14,657] [INFO] [timer.py:260:stop] epoch=0/micro_step=83000/global_step=83000, RunningAvgSamplesPerSec=27.931228710971187, CurrSamplesPerSec=30.9303809603406, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [1540/2809]  eta: 0:12:02  lr: 0.000010  min_lr: 0.000000  loss: 4.2367 (4.2186)  class_acc: 0.2500 (0.3120)  loss_scale: 32768.0000 (42017.8897)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1340  max mem: 15572
Epoch: [29]  [1550/2809]  eta: 0:11:56  lr: 0.000010  min_lr: 0.000000  loss: 4.2674 (4.2183)  class_acc: 0.2917 (0.3125)  loss_scale: 32768.0000 (41958.2515)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0808  max mem: 15572
Epoch: [29]  [1560/2809]  eta: 0:11:51  lr: 0.000010  min_lr: 0.000000  loss: 4.1267 (4.2178)  class_acc: 0.2917 (0.3123)  loss_scale: 32768.0000 (41899.3773)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1293  max mem: 15572
Epoch: [29]  [1570/2809]  eta: 0:11:45  lr: 0.000010  min_lr: 0.000000  loss: 4.1263 (4.2171)  class_acc: 0.2500 (0.3124)  loss_scale: 32768.0000 (41841.2527)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1554  max mem: 15572
Epoch: [29]  [1580/2809]  eta: 0:11:39  lr: 0.000010  min_lr: 0.000000  loss: 4.1572 (4.2172)  class_acc: 0.3333 (0.3125)  loss_scale: 32768.0000 (41783.8634)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1312  max mem: 15572
Epoch: [29]  [1590/2809]  eta: 0:11:34  lr: 0.000010  min_lr: 0.000000  loss: 4.1572 (4.2164)  class_acc: 0.3333 (0.3125)  loss_scale: 32768.0000 (41727.1955)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1420  max mem: 15572
[2025-01-16 04:25:48,834] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:25:48,835] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [1600/2809]  eta: 0:11:29  lr: 0.000010  min_lr: 0.000000  loss: 4.1900 (4.2161)  class_acc: 0.3333 (0.3125)  loss_scale: 32768.0000 (41773.5715)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1789  max mem: 15572
Epoch: [29]  [1610/2809]  eta: 0:11:22  lr: 0.000010  min_lr: 0.000000  loss: 4.1900 (4.2158)  class_acc: 0.3333 (0.3124)  loss_scale: 65536.0000 (41921.0726)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1203  max mem: 15572
Epoch: [29]  [1620/2809]  eta: 0:11:17  lr: 0.000010  min_lr: 0.000000  loss: 4.2191 (4.2157)  class_acc: 0.2917 (0.3123)  loss_scale: 65536.0000 (42066.7539)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0776  max mem: 15572
Epoch: [29]  [1630/2809]  eta: 0:11:12  lr: 0.000010  min_lr: 0.000000  loss: 4.2911 (4.2162)  class_acc: 0.2917 (0.3123)  loss_scale: 65536.0000 (42210.6487)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1792  max mem: 15572
Epoch: [29]  [1640/2809]  eta: 0:11:06  lr: 0.000010  min_lr: 0.000000  loss: 4.2031 (4.2152)  class_acc: 0.3333 (0.3127)  loss_scale: 65536.0000 (42352.7898)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1306  max mem: 15572
Epoch: [29]  [1650/2809]  eta: 0:11:00  lr: 0.000010  min_lr: 0.000000  loss: 4.1519 (4.2158)  class_acc: 0.3333 (0.3126)  loss_scale: 65536.0000 (42493.2090)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1237  max mem: 15572
Epoch: [29]  [1660/2809]  eta: 0:10:54  lr: 0.000010  min_lr: 0.000000  loss: 4.2168 (4.2157)  class_acc: 0.2500 (0.3124)  loss_scale: 65536.0000 (42631.9374)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1242  max mem: 15572
Epoch: [29]  [1670/2809]  eta: 0:10:49  lr: 0.000010  min_lr: 0.000000  loss: 4.2227 (4.2161)  class_acc: 0.2500 (0.3121)  loss_scale: 65536.0000 (42769.0054)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1320  max mem: 15572
[2025-01-16 04:26:34,988] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83136
[2025-01-16 04:26:34,988] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:26:34,989] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1680/2809]  eta: 0:10:44  lr: 0.000010  min_lr: 0.000000  loss: 4.2195 (4.2156)  class_acc: 0.2917 (0.3123)  loss_scale: 65536.0000 (42787.4836)  weight_decay: 0.0500 (0.0500)  time: 0.6549  data: 0.2037  max mem: 15572
Epoch: [29]  [1690/2809]  eta: 0:10:38  lr: 0.000010  min_lr: 0.000000  loss: 4.1760 (4.2148)  class_acc: 0.3750 (0.3126)  loss_scale: 32768.0000 (42728.2318)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1537  max mem: 15572
Epoch: [29]  [1700/2809]  eta: 0:10:32  lr: 0.000010  min_lr: 0.000000  loss: 4.1765 (4.2146)  class_acc: 0.3750 (0.3127)  loss_scale: 32768.0000 (42669.6767)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1297  max mem: 15572
Epoch: [29]  [1710/2809]  eta: 0:10:27  lr: 0.000010  min_lr: 0.000000  loss: 4.2064 (4.2148)  class_acc: 0.2917 (0.3127)  loss_scale: 32768.0000 (42611.8060)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1487  max mem: 15572
Epoch: [29]  [1720/2809]  eta: 0:10:21  lr: 0.000010  min_lr: 0.000000  loss: 4.1824 (4.2143)  class_acc: 0.2917 (0.3127)  loss_scale: 32768.0000 (42554.6078)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1258  max mem: 15572
Epoch: [29]  [1730/2809]  eta: 0:10:15  lr: 0.000010  min_lr: 0.000000  loss: 4.2074 (4.2147)  class_acc: 0.2917 (0.3126)  loss_scale: 32768.0000 (42498.0705)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0908  max mem: 15572
Epoch: [29]  [1740/2809]  eta: 0:10:10  lr: 0.000009  min_lr: 0.000000  loss: 4.2141 (4.2144)  class_acc: 0.2917 (0.3127)  loss_scale: 32768.0000 (42442.1827)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1039  max mem: 15572
Epoch: [29]  [1750/2809]  eta: 0:10:04  lr: 0.000009  min_lr: 0.000000  loss: 4.2292 (4.2152)  class_acc: 0.3333 (0.3126)  loss_scale: 32768.0000 (42386.9332)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1229  max mem: 15572
Epoch: [29]  [1760/2809]  eta: 0:09:59  lr: 0.000009  min_lr: 0.000000  loss: 4.2907 (4.2155)  class_acc: 0.2500 (0.3122)  loss_scale: 32768.0000 (42332.3112)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1671  max mem: 15572
Epoch: [29]  [1770/2809]  eta: 0:09:52  lr: 0.000009  min_lr: 0.000000  loss: 4.2480 (4.2155)  class_acc: 0.2500 (0.3121)  loss_scale: 32768.0000 (42278.3060)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1021  max mem: 15572
Epoch: [29]  [1780/2809]  eta: 0:09:46  lr: 0.000009  min_lr: 0.000000  loss: 4.2304 (4.2160)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (42224.9074)  weight_decay: 0.0500 (0.0500)  time: 0.4794  data: 0.0208  max mem: 15572
Epoch: [29]  [1790/2809]  eta: 0:09:41  lr: 0.000009  min_lr: 0.000000  loss: 4.2786 (4.2155)  class_acc: 0.3333 (0.3126)  loss_scale: 32768.0000 (42172.1050)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0946  max mem: 15572
Epoch: [29]  [1800/2809]  eta: 0:09:35  lr: 0.000009  min_lr: 0.000000  loss: 4.3434 (4.2162)  class_acc: 0.2500 (0.3122)  loss_scale: 32768.0000 (42119.8890)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1481  max mem: 15572
[2025-01-16 04:27:47,295] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:27:47,296] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [1810/2809]  eta: 0:09:29  lr: 0.000009  min_lr: 0.000000  loss: 4.2846 (4.2160)  class_acc: 0.2500 (0.3120)  loss_scale: 32768.0000 (42194.9067)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0848  max mem: 15572
Epoch: [29]  [1820/2809]  eta: 0:09:23  lr: 0.000009  min_lr: 0.000000  loss: 4.2151 (4.2161)  class_acc: 0.2500 (0.3118)  loss_scale: 65536.0000 (42323.0840)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0672  max mem: 15572
Epoch: [29]  [1830/2809]  eta: 0:09:17  lr: 0.000009  min_lr: 0.000000  loss: 4.2943 (4.2168)  class_acc: 0.2500 (0.3116)  loss_scale: 65536.0000 (42449.8613)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0954  max mem: 15572
Epoch: [29]  [1840/2809]  eta: 0:09:12  lr: 0.000009  min_lr: 0.000000  loss: 4.2775 (4.2168)  class_acc: 0.2917 (0.3116)  loss_scale: 65536.0000 (42575.2613)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1434  max mem: 15572
Epoch: [29]  [1850/2809]  eta: 0:09:06  lr: 0.000009  min_lr: 0.000000  loss: 4.2524 (4.2166)  class_acc: 0.3333 (0.3117)  loss_scale: 65536.0000 (42699.3063)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1505  max mem: 15572
Epoch: [29]  [1860/2809]  eta: 0:09:00  lr: 0.000009  min_lr: 0.000000  loss: 4.1105 (4.2159)  class_acc: 0.3333 (0.3117)  loss_scale: 65536.0000 (42822.0183)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0671  max mem: 15572
Epoch: [29]  [1870/2809]  eta: 0:08:55  lr: 0.000009  min_lr: 0.000000  loss: 4.1594 (4.2163)  class_acc: 0.3333 (0.3117)  loss_scale: 65536.0000 (42943.4185)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1111  max mem: 15572
Epoch: [29]  [1880/2809]  eta: 0:08:49  lr: 0.000009  min_lr: 0.000000  loss: 4.1938 (4.2155)  class_acc: 0.3333 (0.3119)  loss_scale: 65536.0000 (43063.5279)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1711  max mem: 15572
[2025-01-16 04:28:36,169] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83350
[2025-01-16 04:28:36,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:28:36,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1890/2809]  eta: 0:08:44  lr: 0.000009  min_lr: 0.000000  loss: 4.1776 (4.2153)  class_acc: 0.3333 (0.3120)  loss_scale: 65536.0000 (43147.7102)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1533  max mem: 15572
Epoch: [29]  [1900/2809]  eta: 0:08:38  lr: 0.000009  min_lr: 0.000000  loss: 4.1745 (4.2154)  class_acc: 0.3750 (0.3123)  loss_scale: 32768.0000 (43093.1089)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1687  max mem: 15572
Epoch: [29]  [1910/2809]  eta: 0:08:32  lr: 0.000009  min_lr: 0.000000  loss: 4.2208 (4.2154)  class_acc: 0.3333 (0.3123)  loss_scale: 32768.0000 (43039.0790)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1357  max mem: 15572
Epoch: [29]  [1920/2809]  eta: 0:08:27  lr: 0.000009  min_lr: 0.000000  loss: 4.2208 (4.2155)  class_acc: 0.3333 (0.3127)  loss_scale: 32768.0000 (42985.6117)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.1042  max mem: 15572
Epoch: [29]  [1930/2809]  eta: 0:08:21  lr: 0.000009  min_lr: 0.000000  loss: 4.1384 (4.2149)  class_acc: 0.3750 (0.3127)  loss_scale: 32768.0000 (42932.6981)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1630  max mem: 15572
Epoch: [29]  [1940/2809]  eta: 0:08:15  lr: 0.000009  min_lr: 0.000000  loss: 4.1384 (4.2146)  class_acc: 0.3333 (0.3127)  loss_scale: 32768.0000 (42880.3297)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1299  max mem: 15572
Epoch: [29]  [1950/2809]  eta: 0:08:09  lr: 0.000009  min_lr: 0.000000  loss: 4.1223 (4.2145)  class_acc: 0.2917 (0.3125)  loss_scale: 32768.0000 (42828.4982)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0847  max mem: 15572
Epoch: [29]  [1960/2809]  eta: 0:08:04  lr: 0.000009  min_lr: 0.000000  loss: 4.1746 (4.2147)  class_acc: 0.2917 (0.3124)  loss_scale: 32768.0000 (42777.1953)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0948  max mem: 15572
Epoch: [29]  [1970/2809]  eta: 0:07:58  lr: 0.000009  min_lr: 0.000000  loss: 4.1558 (4.2141)  class_acc: 0.3333 (0.3124)  loss_scale: 32768.0000 (42726.4130)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0978  max mem: 15572
Epoch: [29]  [1980/2809]  eta: 0:07:52  lr: 0.000009  min_lr: 0.000000  loss: 4.0519 (4.2138)  class_acc: 0.2917 (0.3123)  loss_scale: 32768.0000 (42676.1434)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1531  max mem: 15572
Epoch: [29]  [1990/2809]  eta: 0:07:46  lr: 0.000009  min_lr: 0.000000  loss: 4.1720 (4.2135)  class_acc: 0.2917 (0.3123)  loss_scale: 32768.0000 (42626.3787)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0978  max mem: 15572
Epoch: [29]  [2000/2809]  eta: 0:07:41  lr: 0.000009  min_lr: 0.000000  loss: 4.1720 (4.2133)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (42577.1114)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1529  max mem: 15572
Epoch: [29]  [2010/2809]  eta: 0:07:35  lr: 0.000009  min_lr: 0.000000  loss: 4.2363 (4.2142)  class_acc: 0.2500 (0.3122)  loss_scale: 32768.0000 (42528.3342)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.2073  max mem: 15572
[2025-01-16 04:29:50,388] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:29:50,388] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [2020/2809]  eta: 0:07:30  lr: 0.000009  min_lr: 0.000000  loss: 4.2363 (4.2140)  class_acc: 0.3333 (0.3124)  loss_scale: 32768.0000 (42528.6809)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1078  max mem: 15572
[2025-01-16 04:29:52,084] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83483
[2025-01-16 04:29:52,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:29:52,085] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [2030/2809]  eta: 0:07:24  lr: 0.000009  min_lr: 0.000000  loss: 4.1722 (4.2141)  class_acc: 0.3333 (0.3122)  loss_scale: 32768.0000 (42496.7563)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0952  max mem: 15572
Epoch: [29]  [2040/2809]  eta: 0:07:18  lr: 0.000009  min_lr: 0.000000  loss: 4.2976 (4.2146)  class_acc: 0.2917 (0.3121)  loss_scale: 32768.0000 (42449.0897)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0543  max mem: 15572
Epoch: [29]  [2050/2809]  eta: 0:07:12  lr: 0.000009  min_lr: 0.000000  loss: 4.2317 (4.2147)  class_acc: 0.2917 (0.3121)  loss_scale: 32768.0000 (42401.8879)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1341  max mem: 15572
Epoch: [29]  [2060/2809]  eta: 0:07:07  lr: 0.000009  min_lr: 0.000000  loss: 4.1751 (4.2148)  class_acc: 0.2917 (0.3118)  loss_scale: 32768.0000 (42355.1441)  weight_decay: 0.0500 (0.0500)  time: 0.6525  data: 0.2124  max mem: 15572
Epoch: [29]  [2070/2809]  eta: 0:07:01  lr: 0.000009  min_lr: 0.000000  loss: 4.1110 (4.2149)  class_acc: 0.2500 (0.3119)  loss_scale: 32768.0000 (42308.8518)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1071  max mem: 15572
Epoch: [29]  [2080/2809]  eta: 0:06:55  lr: 0.000009  min_lr: 0.000000  loss: 4.3120 (4.2154)  class_acc: 0.2917 (0.3120)  loss_scale: 32768.0000 (42263.0043)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0711  max mem: 15572
Epoch: [29]  [2090/2809]  eta: 0:06:50  lr: 0.000009  min_lr: 0.000000  loss: 4.3205 (4.2151)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (42217.5954)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1284  max mem: 15572
Epoch: [29]  [2100/2809]  eta: 0:06:44  lr: 0.000009  min_lr: 0.000000  loss: 4.2833 (4.2155)  class_acc: 0.2500 (0.3119)  loss_scale: 32768.0000 (42172.6188)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1406  max mem: 15572
Epoch: [29]  [2110/2809]  eta: 0:06:38  lr: 0.000009  min_lr: 0.000000  loss: 4.2585 (4.2153)  class_acc: 0.2500 (0.3118)  loss_scale: 32768.0000 (42128.0682)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0671  max mem: 15572
Epoch: [29]  [2120/2809]  eta: 0:06:32  lr: 0.000009  min_lr: 0.000000  loss: 4.1676 (4.2148)  class_acc: 0.2500 (0.3116)  loss_scale: 32768.0000 (42083.9378)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0491  max mem: 15572
Epoch: [29]  [2130/2809]  eta: 0:06:26  lr: 0.000009  min_lr: 0.000000  loss: 4.1877 (4.2150)  class_acc: 0.2500 (0.3118)  loss_scale: 32768.0000 (42040.2215)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1000  max mem: 15572
Epoch: [29]  [2140/2809]  eta: 0:06:21  lr: 0.000009  min_lr: 0.000000  loss: 4.2692 (4.2147)  class_acc: 0.2500 (0.3118)  loss_scale: 32768.0000 (41996.9136)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1017  max mem: 15572
Epoch: [29]  [2150/2809]  eta: 0:06:15  lr: 0.000009  min_lr: 0.000000  loss: 4.2838 (4.2155)  class_acc: 0.2500 (0.3116)  loss_scale: 32768.0000 (41954.0084)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.0978  max mem: 15572
[2025-01-16 04:31:04,169] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:31:04,170] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [2160/2809]  eta: 0:06:09  lr: 0.000009  min_lr: 0.000000  loss: 4.2643 (4.2156)  class_acc: 0.2917 (0.3116)  loss_scale: 32768.0000 (42063.1337)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1321  max mem: 15572
Epoch: [29]  [2170/2809]  eta: 0:06:03  lr: 0.000009  min_lr: 0.000000  loss: 4.1742 (4.2157)  class_acc: 0.2917 (0.3118)  loss_scale: 65536.0000 (42171.2538)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0961  max mem: 15572
[2025-01-16 04:31:15,949] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83634
[2025-01-16 04:31:15,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:31:15,949] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [2180/2809]  eta: 0:05:58  lr: 0.000009  min_lr: 0.000000  loss: 4.1740 (4.2159)  class_acc: 0.3333 (0.3119)  loss_scale: 65536.0000 (42158.1880)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0521  max mem: 15572
Epoch: [29]  [2190/2809]  eta: 0:05:52  lr: 0.000009  min_lr: 0.000000  loss: 4.1740 (4.2154)  class_acc: 0.3333 (0.3121)  loss_scale: 32768.0000 (42115.3300)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0876  max mem: 15572
Epoch: [29]  [2200/2809]  eta: 0:05:46  lr: 0.000009  min_lr: 0.000000  loss: 4.2751 (4.2159)  class_acc: 0.3750 (0.3122)  loss_scale: 32768.0000 (42072.8614)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1028  max mem: 15572
Epoch: [29]  [2210/2809]  eta: 0:05:40  lr: 0.000009  min_lr: 0.000000  loss: 4.2496 (4.2156)  class_acc: 0.3333 (0.3124)  loss_scale: 32768.0000 (42030.7770)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0717  max mem: 15572
Epoch: [29]  [2220/2809]  eta: 0:05:35  lr: 0.000009  min_lr: 0.000000  loss: 4.2496 (4.2160)  class_acc: 0.3333 (0.3124)  loss_scale: 32768.0000 (41989.0716)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1269  max mem: 15572
Epoch: [29]  [2230/2809]  eta: 0:05:29  lr: 0.000009  min_lr: 0.000000  loss: 4.2874 (4.2162)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (41947.7400)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1758  max mem: 15572
Epoch: [29]  [2240/2809]  eta: 0:05:23  lr: 0.000009  min_lr: 0.000000  loss: 4.2718 (4.2165)  class_acc: 0.2917 (0.3123)  loss_scale: 32768.0000 (41906.7773)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1371  max mem: 15572
Epoch: [29]  [2250/2809]  eta: 0:05:18  lr: 0.000009  min_lr: 0.000000  loss: 4.1923 (4.2167)  class_acc: 0.2917 (0.3121)  loss_scale: 32768.0000 (41866.1786)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1174  max mem: 15572
Epoch: [29]  [2260/2809]  eta: 0:05:12  lr: 0.000009  min_lr: 0.000000  loss: 4.1288 (4.2164)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (41825.9390)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0724  max mem: 15572
Epoch: [29]  [2270/2809]  eta: 0:05:06  lr: 0.000009  min_lr: 0.000000  loss: 4.1337 (4.2164)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (41786.0537)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0477  max mem: 15572
Epoch: [29]  [2280/2809]  eta: 0:05:00  lr: 0.000009  min_lr: 0.000000  loss: 4.1651 (4.2162)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (41746.5182)  weight_decay: 0.0500 (0.0500)  time: 0.5124  data: 0.0633  max mem: 15572
Epoch: [29]  [2290/2809]  eta: 0:04:55  lr: 0.000009  min_lr: 0.000000  loss: 4.1866 (4.2163)  class_acc: 0.2500 (0.3119)  loss_scale: 32768.0000 (41707.3278)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.1750  max mem: 15572
Epoch: [29]  [2300/2809]  eta: 0:04:49  lr: 0.000009  min_lr: 0.000000  loss: 4.3441 (4.2170)  class_acc: 0.2500 (0.3119)  loss_scale: 32768.0000 (41668.4781)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.1843  max mem: 15572
[2025-01-16 04:32:29,086] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:32:29,086] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [2310/2809]  eta: 0:04:43  lr: 0.000009  min_lr: 0.000000  loss: 4.1975 (4.2161)  class_acc: 0.3333 (0.3122)  loss_scale: 32768.0000 (41757.5768)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0975  max mem: 15572
[2025-01-16 04:32:36,141] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83776
[2025-01-16 04:32:36,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:32:36,141] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [2320/2809]  eta: 0:04:38  lr: 0.000009  min_lr: 0.000000  loss: 4.1975 (4.2161)  class_acc: 0.3333 (0.3123)  loss_scale: 65536.0000 (41775.3175)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0453  max mem: 15572
Epoch: [29]  [2330/2809]  eta: 0:04:32  lr: 0.000009  min_lr: 0.000000  loss: 4.2845 (4.2161)  class_acc: 0.3333 (0.3124)  loss_scale: 32768.0000 (41736.6761)  weight_decay: 0.0500 (0.0500)  time: 0.5142  data: 0.0293  max mem: 15572
Epoch: [29]  [2340/2809]  eta: 0:04:26  lr: 0.000009  min_lr: 0.000000  loss: 4.2845 (4.2165)  class_acc: 0.2917 (0.3123)  loss_scale: 32768.0000 (41698.3648)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0601  max mem: 15572
Epoch: [29]  [2350/2809]  eta: 0:04:20  lr: 0.000009  min_lr: 0.000000  loss: 4.2765 (4.2165)  class_acc: 0.2917 (0.3123)  loss_scale: 32768.0000 (41660.3794)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0490  max mem: 15572
Epoch: [29]  [2360/2809]  eta: 0:04:15  lr: 0.000009  min_lr: 0.000000  loss: 4.2762 (4.2166)  class_acc: 0.2917 (0.3121)  loss_scale: 32768.0000 (41622.7158)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0888  max mem: 15572
Epoch: [29]  [2370/2809]  eta: 0:04:09  lr: 0.000009  min_lr: 0.000000  loss: 4.2762 (4.2166)  class_acc: 0.2917 (0.3121)  loss_scale: 32768.0000 (41585.3699)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1421  max mem: 15572
Epoch: [29]  [2380/2809]  eta: 0:04:04  lr: 0.000009  min_lr: 0.000000  loss: 4.2170 (4.2165)  class_acc: 0.2917 (0.3120)  loss_scale: 32768.0000 (41548.3377)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.1609  max mem: 15572
Epoch: [29]  [2390/2809]  eta: 0:03:58  lr: 0.000009  min_lr: 0.000000  loss: 4.2339 (4.2166)  class_acc: 0.2500 (0.3119)  loss_scale: 32768.0000 (41511.6152)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1224  max mem: 15572
Epoch: [29]  [2400/2809]  eta: 0:03:52  lr: 0.000009  min_lr: 0.000000  loss: 4.1762 (4.2158)  class_acc: 0.3333 (0.3120)  loss_scale: 32768.0000 (41475.1987)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0836  max mem: 15572
Epoch: [29]  [2410/2809]  eta: 0:03:47  lr: 0.000009  min_lr: 0.000000  loss: 4.2481 (4.2159)  class_acc: 0.3333 (0.3119)  loss_scale: 32768.0000 (41439.0842)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1205  max mem: 15572
Epoch: [29]  [2420/2809]  eta: 0:03:41  lr: 0.000009  min_lr: 0.000000  loss: 4.2742 (4.2161)  class_acc: 0.2083 (0.3116)  loss_scale: 32768.0000 (41403.2681)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0998  max mem: 15572
Epoch: [29]  [2430/2809]  eta: 0:03:35  lr: 0.000009  min_lr: 0.000000  loss: 4.2568 (4.2159)  class_acc: 0.2500 (0.3117)  loss_scale: 32768.0000 (41367.7466)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0685  max mem: 15572
Epoch: [29]  [2440/2809]  eta: 0:03:30  lr: 0.000009  min_lr: 0.000000  loss: 4.2484 (4.2158)  class_acc: 0.3333 (0.3120)  loss_scale: 32768.0000 (41332.5162)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0990  max mem: 15572
[2025-01-16 04:33:49,891] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:33:49,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [2450/2809]  eta: 0:03:24  lr: 0.000009  min_lr: 0.000000  loss: 4.3033 (4.2160)  class_acc: 0.3333 (0.3118)  loss_scale: 32768.0000 (41391.1579)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0612  max mem: 15572
[2025-01-16 04:33:54,142] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83914
[2025-01-16 04:33:54,143] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:33:54,143] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [2460/2809]  eta: 0:03:18  lr: 0.000009  min_lr: 0.000000  loss: 4.3044 (4.2160)  class_acc: 0.2917 (0.3121)  loss_scale: 32768.0000 (41382.7485)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0582  max mem: 15572
Epoch: [29]  [2470/2809]  eta: 0:03:12  lr: 0.000009  min_lr: 0.000000  loss: 4.1819 (4.2155)  class_acc: 0.3333 (0.3122)  loss_scale: 32768.0000 (41347.8851)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0908  max mem: 15572
Epoch: [29]  [2480/2809]  eta: 0:03:06  lr: 0.000009  min_lr: 0.000000  loss: 4.1161 (4.2155)  class_acc: 0.2917 (0.3120)  loss_scale: 32768.0000 (41313.3027)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0519  max mem: 15572
Epoch: [29]  [2490/2809]  eta: 0:03:01  lr: 0.000009  min_lr: 0.000000  loss: 4.1778 (4.2155)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (41278.9980)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0936  max mem: 15572
Epoch: [29]  [2500/2809]  eta: 0:02:55  lr: 0.000009  min_lr: 0.000000  loss: 4.1778 (4.2160)  class_acc: 0.2917 (0.3123)  loss_scale: 32768.0000 (41244.9676)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1579  max mem: 15572
Epoch: [29]  [2510/2809]  eta: 0:02:50  lr: 0.000009  min_lr: 0.000000  loss: 4.1697 (4.2158)  class_acc: 0.3333 (0.3123)  loss_scale: 32768.0000 (41211.2083)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1712  max mem: 15572
Epoch: [29]  [2520/2809]  eta: 0:02:44  lr: 0.000009  min_lr: 0.000000  loss: 4.1697 (4.2157)  class_acc: 0.3333 (0.3124)  loss_scale: 32768.0000 (41177.7168)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1770  max mem: 15572
Epoch: [29]  [2530/2809]  eta: 0:02:38  lr: 0.000009  min_lr: 0.000000  loss: 4.2826 (4.2162)  class_acc: 0.3333 (0.3124)  loss_scale: 32768.0000 (41144.4899)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1507  max mem: 15572
[2025-01-16 04:34:43,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=84000, skipped=522, lr=[8.740915324830693e-08, 8.740915324830693e-08, 1.2487021892615276e-07, 1.2487021892615276e-07, 1.7838602703736113e-07, 1.7838602703736113e-07, 2.5483718148194445e-07, 2.5483718148194445e-07, 3.6405311640277783e-07, 3.6405311640277783e-07, 5.20075880575397e-07, 5.20075880575397e-07, 7.429655436791385e-07, 7.429655436791385e-07, 1.0613793481130551e-06, 1.0613793481130551e-06, 1.5162562115900786e-06, 1.5162562115900786e-06, 2.166080302271541e-06, 2.166080302271541e-06, 3.0944004318164876e-06, 3.0944004318164876e-06, 4.420572045452126e-06, 4.420572045452126e-06, 6.315102922074465e-06, 6.315102922074465e-06, 9.021575602963522e-06, 9.021575602963522e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:34:43,041] [INFO] [timer.py:260:stop] epoch=0/micro_step=84000/global_step=84000, RunningAvgSamplesPerSec=27.93167769151996, CurrSamplesPerSec=26.065460330579796, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [2540/2809]  eta: 0:02:33  lr: 0.000009  min_lr: 0.000000  loss: 4.2900 (4.2164)  class_acc: 0.3333 (0.3125)  loss_scale: 32768.0000 (41111.5246)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.1173  max mem: 15572
Epoch: [29]  [2550/2809]  eta: 0:02:27  lr: 0.000009  min_lr: 0.000000  loss: 4.1682 (4.2162)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (41078.8177)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1446  max mem: 15572
Epoch: [29]  [2560/2809]  eta: 0:02:21  lr: 0.000009  min_lr: 0.000000  loss: 4.1543 (4.2164)  class_acc: 0.2917 (0.3121)  loss_scale: 32768.0000 (41046.3663)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1257  max mem: 15572
Epoch: [29]  [2570/2809]  eta: 0:02:16  lr: 0.000009  min_lr: 0.000000  loss: 4.1543 (4.2159)  class_acc: 0.2917 (0.3124)  loss_scale: 32768.0000 (41014.1673)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1079  max mem: 15572
Epoch: [29]  [2580/2809]  eta: 0:02:10  lr: 0.000009  min_lr: 0.000000  loss: 4.2764 (4.2171)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (40982.2177)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1460  max mem: 15572
[2025-01-16 04:35:08,756] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:35:08,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [2590/2809]  eta: 0:02:04  lr: 0.000009  min_lr: 0.000000  loss: 4.2878 (4.2167)  class_acc: 0.2917 (0.3123)  loss_scale: 32768.0000 (41064.3365)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0809  max mem: 15572
[2025-01-16 04:35:15,125] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84055
[2025-01-16 04:35:15,125] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:35:15,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [2600/2809]  eta: 0:01:58  lr: 0.000009  min_lr: 0.000000  loss: 4.2516 (4.2169)  class_acc: 0.2917 (0.3121)  loss_scale: 65536.0000 (41070.2345)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0461  max mem: 15572
Epoch: [29]  [2610/2809]  eta: 0:01:53  lr: 0.000009  min_lr: 0.000000  loss: 4.3031 (4.2168)  class_acc: 0.2500 (0.3121)  loss_scale: 32768.0000 (41038.4374)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.0954  max mem: 15572
Epoch: [29]  [2620/2809]  eta: 0:01:47  lr: 0.000009  min_lr: 0.000000  loss: 4.1976 (4.2169)  class_acc: 0.2500 (0.3120)  loss_scale: 32768.0000 (41006.8829)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0557  max mem: 15572
Epoch: [29]  [2630/2809]  eta: 0:01:41  lr: 0.000009  min_lr: 0.000000  loss: 4.2857 (4.2176)  class_acc: 0.2500 (0.3118)  loss_scale: 32768.0000 (40975.5682)  weight_decay: 0.0500 (0.0500)  time: 0.4406  data: 0.0007  max mem: 15572
Epoch: [29]  [2640/2809]  eta: 0:01:35  lr: 0.000009  min_lr: 0.000000  loss: 4.2990 (4.2173)  class_acc: 0.2083 (0.3116)  loss_scale: 32768.0000 (40944.4907)  weight_decay: 0.0500 (0.0500)  time: 0.4668  data: 0.0007  max mem: 15572
Epoch: [29]  [2650/2809]  eta: 0:01:30  lr: 0.000009  min_lr: 0.000000  loss: 4.1468 (4.2173)  class_acc: 0.3333 (0.3119)  loss_scale: 32768.0000 (40913.6477)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0010  max mem: 15572
Epoch: [29]  [2660/2809]  eta: 0:01:24  lr: 0.000009  min_lr: 0.000000  loss: 4.2380 (4.2176)  class_acc: 0.3750 (0.3118)  loss_scale: 32768.0000 (40883.0365)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0010  max mem: 15572
Epoch: [29]  [2670/2809]  eta: 0:01:18  lr: 0.000009  min_lr: 0.000000  loss: 4.2874 (4.2177)  class_acc: 0.2500 (0.3117)  loss_scale: 32768.0000 (40852.6544)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0273  max mem: 15572
[2025-01-16 04:35:57,879] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84136
[2025-01-16 04:35:57,879] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 04:35:57,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [2680/2809]  eta: 0:01:13  lr: 0.000009  min_lr: 0.000000  loss: 4.3190 (4.2179)  class_acc: 0.3333 (0.3119)  loss_scale: 32768.0000 (40785.8322)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.0772  max mem: 15572
Epoch: [29]  [2690/2809]  eta: 0:01:07  lr: 0.000009  min_lr: 0.000000  loss: 4.2546 (4.2176)  class_acc: 0.3333 (0.3119)  loss_scale: 16384.0000 (40695.1527)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1019  max mem: 15572
Epoch: [29]  [2700/2809]  eta: 0:01:01  lr: 0.000009  min_lr: 0.000000  loss: 4.1940 (4.2178)  class_acc: 0.2917 (0.3118)  loss_scale: 16384.0000 (40605.1448)  weight_decay: 0.0500 (0.0500)  time: 0.6438  data: 0.1010  max mem: 15572
Epoch: [29]  [2710/2809]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000000  loss: 4.1815 (4.2176)  class_acc: 0.2917 (0.3120)  loss_scale: 16384.0000 (40515.8008)  weight_decay: 0.0500 (0.0500)  time: 0.6822  data: 0.1625  max mem: 15572
Epoch: [29]  [2720/2809]  eta: 0:00:50  lr: 0.000009  min_lr: 0.000000  loss: 4.2179 (4.2180)  class_acc: 0.3333 (0.3120)  loss_scale: 16384.0000 (40427.1136)  weight_decay: 0.0500 (0.0500)  time: 0.7406  data: 0.2190  max mem: 15572
Epoch: [29]  [2730/2809]  eta: 0:00:45  lr: 0.000009  min_lr: 0.000000  loss: 4.4301 (4.2186)  class_acc: 0.3333 (0.3122)  loss_scale: 16384.0000 (40339.0758)  weight_decay: 0.0500 (0.0500)  time: 0.7175  data: 0.1837  max mem: 15572
Epoch: [29]  [2740/2809]  eta: 0:00:39  lr: 0.000009  min_lr: 0.000000  loss: 4.2214 (4.2183)  class_acc: 0.2917 (0.3122)  loss_scale: 16384.0000 (40251.6804)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1281  max mem: 15572
Epoch: [29]  [2750/2809]  eta: 0:00:33  lr: 0.000009  min_lr: 0.000000  loss: 4.1497 (4.2181)  class_acc: 0.2917 (0.3122)  loss_scale: 16384.0000 (40164.9204)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1073  max mem: 15572
Epoch: [29]  [2760/2809]  eta: 0:00:27  lr: 0.000009  min_lr: 0.000000  loss: 4.1851 (4.2184)  class_acc: 0.3333 (0.3126)  loss_scale: 16384.0000 (40078.7888)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1504  max mem: 15572
Epoch: [29]  [2770/2809]  eta: 0:00:22  lr: 0.000009  min_lr: 0.000000  loss: 4.3162 (4.2184)  class_acc: 0.3333 (0.3124)  loss_scale: 16384.0000 (39993.2790)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1392  max mem: 15572
Epoch: [29]  [2780/2809]  eta: 0:00:16  lr: 0.000009  min_lr: 0.000000  loss: 4.2556 (4.2186)  class_acc: 0.2500 (0.3122)  loss_scale: 16384.0000 (39908.3840)  weight_decay: 0.0500 (0.0500)  time: 0.4685  data: 0.0461  max mem: 15572
Epoch: [29]  [2790/2809]  eta: 0:00:10  lr: 0.000009  min_lr: 0.000000  loss: 4.2850 (4.2184)  class_acc: 0.2500 (0.3122)  loss_scale: 16384.0000 (39824.0975)  weight_decay: 0.0500 (0.0500)  time: 0.4075  data: 0.0005  max mem: 15572
Epoch: [29]  [2800/2809]  eta: 0:00:05  lr: 0.000009  min_lr: 0.000000  loss: 4.2461 (4.2183)  class_acc: 0.2500 (0.3123)  loss_scale: 16384.0000 (39740.4127)  weight_decay: 0.0500 (0.0500)  time: 0.4079  data: 0.0004  max mem: 15572
[2025-01-16 04:37:13,411] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:37:13,411] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [2808/2809]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 4.1921 (4.2183)  class_acc: 0.2083 (0.3121)  loss_scale: 16384.0000 (39703.0573)  weight_decay: 0.0500 (0.0500)  time: 0.3951  data: 0.0003  max mem: 15572
Epoch: [29] Total time: 0:26:37 (0.5687 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 4.1921 (4.2183)  class_acc: 0.2083 (0.3121)  loss_scale: 16384.0000 (39703.0573)  weight_decay: 0.0500 (0.0500)
[2025-01-16 04:37:15,104] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-29 is about to be saved!
[2025-01-16 04:37:15,108] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-29/mp_rank_00_model_states.pt
[2025-01-16 04:37:15,108] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-29/mp_rank_00_model_states.pt...
[2025-01-16 04:37:15,595] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-29/mp_rank_00_model_states.pt.
[2025-01-16 04:37:15,596] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-29 is ready now!
Val:  [  0/272]  eta: 0:16:07  loss: 1.1966 (1.1966)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.5566  data: 3.3891  max mem: 15572
Val:  [ 10/272]  eta: 0:03:02  loss: 2.5442 (2.4516)  acc1: 44.4444 (48.4848)  acc5: 77.7778 (75.7576)  time: 0.6952  data: 0.5137  max mem: 15572
Val:  [ 20/272]  eta: 0:02:06  loss: 2.6067 (2.5253)  acc1: 44.4444 (48.1481)  acc5: 72.2222 (74.3386)  time: 0.3494  data: 0.1672  max mem: 15572
Val:  [ 30/272]  eta: 0:01:46  loss: 2.6322 (2.5781)  acc1: 44.4444 (44.8029)  acc5: 72.2222 (74.7312)  time: 0.2998  data: 0.1137  max mem: 15572
Val:  [ 40/272]  eta: 0:01:32  loss: 2.6322 (2.6023)  acc1: 27.7778 (41.8699)  acc5: 77.7778 (74.3902)  time: 0.2903  data: 0.1021  max mem: 15572
Val:  [ 50/272]  eta: 0:01:23  loss: 2.5511 (2.5312)  acc1: 33.3333 (43.7908)  acc5: 77.7778 (75.8170)  time: 0.2800  data: 0.0961  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 1.9647 (2.4740)  acc1: 61.1111 (45.7195)  acc5: 83.3333 (76.4117)  time: 0.3131  data: 0.1205  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 1.9859 (2.4196)  acc1: 61.1111 (47.7308)  acc5: 83.3333 (77.2300)  time: 0.3370  data: 0.1372  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 2.1528 (2.4319)  acc1: 50.0000 (47.8052)  acc5: 77.7778 (77.0919)  time: 0.3117  data: 0.1088  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 2.5376 (2.4465)  acc1: 44.4444 (47.1306)  acc5: 77.7778 (77.2894)  time: 0.2957  data: 0.0981  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.5612 (2.4772)  acc1: 44.4444 (46.1496)  acc5: 77.7778 (77.0627)  time: 0.3324  data: 0.1373  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 2.6593 (2.5375)  acc1: 27.7778 (44.1942)  acc5: 72.2222 (75.7257)  time: 0.4159  data: 0.2094  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.0674 (2.5744)  acc1: 27.7778 (43.6180)  acc5: 61.1111 (74.9311)  time: 0.3457  data: 0.1520  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.5036 (2.5498)  acc1: 38.8889 (43.9355)  acc5: 77.7778 (75.6573)  time: 0.2306  data: 0.0422  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 2.2801 (2.5545)  acc1: 50.0000 (44.1292)  acc5: 83.3333 (75.4531)  time: 0.2926  data: 0.0851  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.6163 (2.5536)  acc1: 38.8889 (43.6718)  acc5: 77.7778 (75.5703)  time: 0.3257  data: 0.1295  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.4632 (2.5497)  acc1: 38.8889 (44.1684)  acc5: 77.7778 (75.9489)  time: 0.3087  data: 0.1277  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.5062 (2.5634)  acc1: 38.8889 (43.5673)  acc5: 72.2222 (75.6010)  time: 0.3164  data: 0.1222  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.5062 (2.5518)  acc1: 38.8889 (43.6771)  acc5: 72.2222 (75.9055)  time: 0.3154  data: 0.1233  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.5023 (2.5883)  acc1: 44.4444 (42.6120)  acc5: 72.2222 (74.8109)  time: 0.3288  data: 0.1453  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.7357 (2.5900)  acc1: 38.8889 (42.3991)  acc5: 72.2222 (74.6545)  time: 0.3572  data: 0.1792  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.3747 (2.5979)  acc1: 38.8889 (42.1274)  acc5: 77.7778 (74.5129)  time: 0.3189  data: 0.1420  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.6319 (2.5906)  acc1: 38.8889 (42.3831)  acc5: 77.7778 (74.6355)  time: 0.3047  data: 0.1205  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.0971 (2.5747)  acc1: 55.5556 (43.0976)  acc5: 83.3333 (74.9399)  time: 0.3086  data: 0.1132  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1075 (2.5623)  acc1: 55.5556 (43.2918)  acc5: 83.3333 (75.2882)  time: 0.2769  data: 0.0835  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4986 (2.5682)  acc1: 33.3333 (42.8508)  acc5: 83.3333 (75.3209)  time: 0.3294  data: 0.1445  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.0277 (2.5282)  acc1: 66.6667 (44.2954)  acc5: 88.8889 (75.8834)  time: 0.2839  data: 0.0994  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.9366 (2.5226)  acc1: 66.6667 (44.3214)  acc5: 83.3333 (76.0353)  time: 0.1691  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.9366 (2.5261)  acc1: 55.5556 (44.3170)  acc5: 83.3333 (75.9984)  time: 0.1593  data: 0.0002  max mem: 15572
Val: Total time: 0:01:27 (0.3203 s / it)
* Acc@1 44.317 Acc@5 75.998 loss 2.526
Accuracy of the network on the 4883 val videos: 44.3%
Max accuracy: 44.85%
Epoch: [30]  [   0/2809]  eta: 7:10:14  lr: 0.000009  min_lr: 0.000000  loss: 4.0340 (4.0340)  class_acc: 0.5833 (0.5833)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.1899  data: 8.7444  max mem: 15572
Epoch: [30]  [  10/2809]  eta: 0:59:10  lr: 0.000009  min_lr: 0.000000  loss: 4.1765 (4.2452)  class_acc: 0.2500 (0.2879)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2686  data: 0.7956  max mem: 15572
Epoch: [30]  [  20/2809]  eta: 0:41:48  lr: 0.000009  min_lr: 0.000000  loss: 4.2783 (4.2321)  class_acc: 0.2500 (0.2937)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4849  data: 0.0310  max mem: 15572
Epoch: [30]  [  30/2809]  eta: 0:36:43  lr: 0.000009  min_lr: 0.000000  loss: 4.1973 (4.1780)  class_acc: 0.2500 (0.3024)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.0899  max mem: 15572
Epoch: [30]  [  40/2809]  eta: 0:34:37  lr: 0.000009  min_lr: 0.000000  loss: 4.2338 (4.1977)  class_acc: 0.2500 (0.2876)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1434  max mem: 15572
Epoch: [30]  [  50/2809]  eta: 0:32:32  lr: 0.000009  min_lr: 0.000000  loss: 4.2488 (4.2064)  class_acc: 0.2500 (0.2851)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1294  max mem: 15572
Epoch: [30]  [  60/2809]  eta: 0:30:53  lr: 0.000009  min_lr: 0.000000  loss: 4.2469 (4.2041)  class_acc: 0.2917 (0.2944)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0885  max mem: 15572
Epoch: [30]  [  70/2809]  eta: 0:29:24  lr: 0.000009  min_lr: 0.000000  loss: 4.2337 (4.1918)  class_acc: 0.2917 (0.2981)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4820  data: 0.0569  max mem: 15572
Epoch: [30]  [  80/2809]  eta: 0:28:40  lr: 0.000009  min_lr: 0.000000  loss: 4.1464 (4.1914)  class_acc: 0.2500 (0.2973)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0649  max mem: 15572
Epoch: [30]  [  90/2809]  eta: 0:28:06  lr: 0.000009  min_lr: 0.000000  loss: 4.2247 (4.1821)  class_acc: 0.3333 (0.3045)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0850  max mem: 15572
Epoch: [30]  [ 100/2809]  eta: 0:27:36  lr: 0.000009  min_lr: 0.000000  loss: 4.1855 (4.1719)  class_acc: 0.3333 (0.3106)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0679  max mem: 15572
Epoch: [30]  [ 110/2809]  eta: 0:27:14  lr: 0.000009  min_lr: 0.000000  loss: 4.0921 (4.1692)  class_acc: 0.3333 (0.3149)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0703  max mem: 15572
Epoch: [30]  [ 120/2809]  eta: 0:26:52  lr: 0.000009  min_lr: 0.000000  loss: 4.2856 (4.1853)  class_acc: 0.2917 (0.3120)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0793  max mem: 15572
[2025-01-16 04:39:56,601] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:39:56,602] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [ 130/2809]  eta: 0:26:42  lr: 0.000009  min_lr: 0.000000  loss: 4.2857 (4.1879)  class_acc: 0.2917 (0.3133)  loss_scale: 32768.0000 (34769.0992)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1126  max mem: 15572
Epoch: [30]  [ 140/2809]  eta: 0:26:41  lr: 0.000009  min_lr: 0.000000  loss: 4.3586 (4.1997)  class_acc: 0.2917 (0.3126)  loss_scale: 65536.0000 (36951.1489)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1618  max mem: 15572
Epoch: [30]  [ 150/2809]  eta: 0:26:38  lr: 0.000009  min_lr: 0.000000  loss: 4.3957 (4.2093)  class_acc: 0.2500 (0.3093)  loss_scale: 65536.0000 (38844.1854)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1800  max mem: 15572
Epoch: [30]  [ 160/2809]  eta: 0:26:18  lr: 0.000009  min_lr: 0.000000  loss: 4.3271 (4.2068)  class_acc: 0.2500 (0.3106)  loss_scale: 65536.0000 (40502.0621)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1120  max mem: 15572
Epoch: [30]  [ 170/2809]  eta: 0:25:58  lr: 0.000009  min_lr: 0.000000  loss: 4.1356 (4.2022)  class_acc: 0.3750 (0.3146)  loss_scale: 65536.0000 (41966.0351)  weight_decay: 0.0500 (0.0500)  time: 0.5119  data: 0.0686  max mem: 15572
[2025-01-16 04:40:25,115] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84443
[2025-01-16 04:40:25,115] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:40:25,115] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 180/2809]  eta: 0:25:34  lr: 0.000009  min_lr: 0.000000  loss: 4.2719 (4.2081)  class_acc: 0.3750 (0.3151)  loss_scale: 65536.0000 (41819.9337)  weight_decay: 0.0500 (0.0500)  time: 0.4859  data: 0.0654  max mem: 15572
Epoch: [30]  [ 190/2809]  eta: 0:25:40  lr: 0.000009  min_lr: 0.000000  loss: 4.3072 (4.2181)  class_acc: 0.3333 (0.3163)  loss_scale: 32768.0000 (41346.0105)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1361  max mem: 15572
Epoch: [30]  [ 200/2809]  eta: 0:25:22  lr: 0.000009  min_lr: 0.000000  loss: 4.3479 (4.2302)  class_acc: 0.2083 (0.3118)  loss_scale: 32768.0000 (40919.2438)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1420  max mem: 15572
Epoch: [30]  [ 210/2809]  eta: 0:25:25  lr: 0.000009  min_lr: 0.000000  loss: 4.3106 (4.2267)  class_acc: 0.2500 (0.3112)  loss_scale: 32768.0000 (40532.9289)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1419  max mem: 15572
Epoch: [30]  [ 220/2809]  eta: 0:25:20  lr: 0.000009  min_lr: 0.000000  loss: 4.2039 (4.2247)  class_acc: 0.2500 (0.3111)  loss_scale: 32768.0000 (40181.5747)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1811  max mem: 15572
Epoch: [30]  [ 230/2809]  eta: 0:25:08  lr: 0.000009  min_lr: 0.000000  loss: 4.3088 (4.2312)  class_acc: 0.2083 (0.3086)  loss_scale: 32768.0000 (39860.6407)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1196  max mem: 15572
Epoch: [30]  [ 240/2809]  eta: 0:24:58  lr: 0.000009  min_lr: 0.000000  loss: 4.3474 (4.2296)  class_acc: 0.2500 (0.3084)  loss_scale: 32768.0000 (39566.3402)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.1065  max mem: 15572
Epoch: [30]  [ 250/2809]  eta: 0:25:00  lr: 0.000009  min_lr: 0.000000  loss: 4.2478 (4.2280)  class_acc: 0.3333 (0.3109)  loss_scale: 32768.0000 (39295.4900)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1538  max mem: 15572
Epoch: [30]  [ 260/2809]  eta: 0:24:47  lr: 0.000009  min_lr: 0.000000  loss: 4.1965 (4.2305)  class_acc: 0.2917 (0.3116)  loss_scale: 32768.0000 (39045.3946)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1379  max mem: 15572
Epoch: [30]  [ 270/2809]  eta: 0:24:34  lr: 0.000009  min_lr: 0.000000  loss: 4.1840 (4.2257)  class_acc: 0.3333 (0.3150)  loss_scale: 32768.0000 (38813.7565)  weight_decay: 0.0500 (0.0500)  time: 0.5113  data: 0.0768  max mem: 15572
Epoch: [30]  [ 280/2809]  eta: 0:24:33  lr: 0.000009  min_lr: 0.000000  loss: 4.1512 (4.2238)  class_acc: 0.3333 (0.3158)  loss_scale: 32768.0000 (38598.6050)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1241  max mem: 15572
Epoch: [30]  [ 290/2809]  eta: 0:24:26  lr: 0.000009  min_lr: 0.000000  loss: 4.1317 (4.2210)  class_acc: 0.3333 (0.3180)  loss_scale: 32768.0000 (38398.2405)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1539  max mem: 15572
Epoch: [30]  [ 300/2809]  eta: 0:24:16  lr: 0.000009  min_lr: 0.000000  loss: 4.2864 (4.2220)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (38211.1894)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1192  max mem: 15572
[2025-01-16 04:41:39,724] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:41:39,724] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [ 310/2809]  eta: 0:24:08  lr: 0.000009  min_lr: 0.000000  loss: 4.3205 (4.2202)  class_acc: 0.2083 (0.3140)  loss_scale: 32768.0000 (38984.4373)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.1089  max mem: 15572
Epoch: [30]  [ 320/2809]  eta: 0:24:01  lr: 0.000009  min_lr: 0.000000  loss: 4.1897 (4.2186)  class_acc: 0.3333 (0.3143)  loss_scale: 65536.0000 (39811.5888)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1304  max mem: 15572
Epoch: [30]  [ 330/2809]  eta: 0:23:56  lr: 0.000009  min_lr: 0.000000  loss: 4.2011 (4.2214)  class_acc: 0.2917 (0.3133)  loss_scale: 65536.0000 (40588.7613)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1217  max mem: 15572
[2025-01-16 04:41:58,748] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84609
[2025-01-16 04:41:58,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:41:58,748] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 340/2809]  eta: 0:23:41  lr: 0.000009  min_lr: 0.000000  loss: 4.3273 (4.2211)  class_acc: 0.2500 (0.3126)  loss_scale: 65536.0000 (41128.1642)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0663  max mem: 15572
Epoch: [30]  [ 350/2809]  eta: 0:23:33  lr: 0.000009  min_lr: 0.000000  loss: 4.2930 (4.2216)  class_acc: 0.3333 (0.3130)  loss_scale: 32768.0000 (40889.9829)  weight_decay: 0.0500 (0.0500)  time: 0.4975  data: 0.0665  max mem: 15572
Epoch: [30]  [ 360/2809]  eta: 0:23:25  lr: 0.000009  min_lr: 0.000000  loss: 4.1796 (4.2202)  class_acc: 0.3333 (0.3143)  loss_scale: 32768.0000 (40664.9972)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0836  max mem: 15572
Epoch: [30]  [ 370/2809]  eta: 0:23:16  lr: 0.000009  min_lr: 0.000000  loss: 4.1234 (4.2193)  class_acc: 0.3333 (0.3159)  loss_scale: 32768.0000 (40452.1402)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0662  max mem: 15572
Epoch: [30]  [ 380/2809]  eta: 0:23:06  lr: 0.000009  min_lr: 0.000000  loss: 4.1611 (4.2193)  class_acc: 0.3333 (0.3163)  loss_scale: 32768.0000 (40250.4567)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0678  max mem: 15572
Epoch: [30]  [ 390/2809]  eta: 0:23:02  lr: 0.000009  min_lr: 0.000000  loss: 4.2779 (4.2196)  class_acc: 0.3333 (0.3170)  loss_scale: 32768.0000 (40059.0895)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1087  max mem: 15572
Epoch: [30]  [ 400/2809]  eta: 0:22:56  lr: 0.000009  min_lr: 0.000000  loss: 4.2900 (4.2207)  class_acc: 0.2917 (0.3156)  loss_scale: 32768.0000 (39877.2668)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1408  max mem: 15572
Epoch: [30]  [ 410/2809]  eta: 0:22:50  lr: 0.000009  min_lr: 0.000000  loss: 4.3362 (4.2221)  class_acc: 0.2500 (0.3143)  loss_scale: 32768.0000 (39704.2920)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1213  max mem: 15572
Epoch: [30]  [ 420/2809]  eta: 0:22:45  lr: 0.000009  min_lr: 0.000000  loss: 4.3290 (4.2252)  class_acc: 0.2083 (0.3116)  loss_scale: 32768.0000 (39539.5344)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1048  max mem: 15572
Epoch: [30]  [ 430/2809]  eta: 0:22:41  lr: 0.000009  min_lr: 0.000000  loss: 4.1777 (4.2201)  class_acc: 0.2917 (0.3143)  loss_scale: 32768.0000 (39382.4223)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1310  max mem: 15572
Epoch: [30]  [ 440/2809]  eta: 0:22:34  lr: 0.000009  min_lr: 0.000000  loss: 4.1296 (4.2207)  class_acc: 0.2917 (0.3131)  loss_scale: 32768.0000 (39232.4354)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1376  max mem: 15572
Epoch: [30]  [ 450/2809]  eta: 0:22:26  lr: 0.000009  min_lr: 0.000000  loss: 4.2739 (4.2217)  class_acc: 0.2917 (0.3129)  loss_scale: 32768.0000 (39089.0998)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0983  max mem: 15572
Epoch: [30]  [ 460/2809]  eta: 0:22:23  lr: 0.000009  min_lr: 0.000000  loss: 4.2732 (4.2187)  class_acc: 0.2917 (0.3128)  loss_scale: 32768.0000 (38951.9826)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1383  max mem: 15572
[2025-01-16 04:43:11,600] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:43:11,601] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [ 470/2809]  eta: 0:22:18  lr: 0.000009  min_lr: 0.000000  loss: 4.0581 (4.2162)  class_acc: 0.2917 (0.3140)  loss_scale: 32768.0000 (39029.4013)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1735  max mem: 15572
Epoch: [30]  [ 480/2809]  eta: 0:22:12  lr: 0.000009  min_lr: 0.000000  loss: 4.0802 (4.2138)  class_acc: 0.3750 (0.3150)  loss_scale: 65536.0000 (39580.4740)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1406  max mem: 15572
[2025-01-16 04:43:22,218] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84758
[2025-01-16 04:43:22,218] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:43:22,218] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 490/2809]  eta: 0:22:03  lr: 0.000009  min_lr: 0.000000  loss: 4.1624 (4.2130)  class_acc: 0.3333 (0.3140)  loss_scale: 65536.0000 (39908.8880)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0959  max mem: 15572
Epoch: [30]  [ 500/2809]  eta: 0:22:02  lr: 0.000009  min_lr: 0.000000  loss: 4.1875 (4.2114)  class_acc: 0.2917 (0.3148)  loss_scale: 32768.0000 (39766.3553)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1488  max mem: 15572
Epoch: [30]  [ 510/2809]  eta: 0:21:57  lr: 0.000009  min_lr: 0.000000  loss: 4.1789 (4.2101)  class_acc: 0.3333 (0.3160)  loss_scale: 32768.0000 (39629.4012)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1592  max mem: 15572
Epoch: [30]  [ 520/2809]  eta: 0:21:56  lr: 0.000009  min_lr: 0.000000  loss: 4.1789 (4.2103)  class_acc: 0.3333 (0.3163)  loss_scale: 32768.0000 (39497.7044)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.1450  max mem: 15572
Epoch: [30]  [ 530/2809]  eta: 0:21:47  lr: 0.000009  min_lr: 0.000000  loss: 4.3225 (4.2118)  class_acc: 0.3333 (0.3160)  loss_scale: 32768.0000 (39370.9680)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1210  max mem: 15572
Epoch: [30]  [ 540/2809]  eta: 0:21:44  lr: 0.000009  min_lr: 0.000000  loss: 4.2875 (4.2127)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (39248.9168)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1158  max mem: 15572
Epoch: [30]  [ 550/2809]  eta: 0:21:35  lr: 0.000009  min_lr: 0.000000  loss: 4.1937 (4.2117)  class_acc: 0.2917 (0.3159)  loss_scale: 32768.0000 (39131.2958)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1305  max mem: 15572
Epoch: [30]  [ 560/2809]  eta: 0:21:29  lr: 0.000009  min_lr: 0.000000  loss: 4.2011 (4.2119)  class_acc: 0.2917 (0.3161)  loss_scale: 32768.0000 (39017.8681)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0857  max mem: 15572
Epoch: [30]  [ 570/2809]  eta: 0:21:19  lr: 0.000009  min_lr: 0.000000  loss: 4.2100 (4.2133)  class_acc: 0.2917 (0.3167)  loss_scale: 32768.0000 (38908.4133)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0730  max mem: 15572
Epoch: [30]  [ 580/2809]  eta: 0:21:12  lr: 0.000009  min_lr: 0.000000  loss: 4.2862 (4.2146)  class_acc: 0.3333 (0.3178)  loss_scale: 32768.0000 (38802.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0683  max mem: 15572
Epoch: [30]  [ 590/2809]  eta: 0:21:04  lr: 0.000009  min_lr: 0.000000  loss: 4.2760 (4.2157)  class_acc: 0.3333 (0.3180)  loss_scale: 32768.0000 (38700.6159)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0829  max mem: 15572
Epoch: [30]  [ 600/2809]  eta: 0:21:03  lr: 0.000009  min_lr: 0.000000  loss: 4.1973 (4.2153)  class_acc: 0.3333 (0.3184)  loss_scale: 32768.0000 (38601.9035)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1442  max mem: 15572
Epoch: [30]  [ 610/2809]  eta: 0:20:57  lr: 0.000009  min_lr: 0.000000  loss: 4.2074 (4.2172)  class_acc: 0.3333 (0.3182)  loss_scale: 32768.0000 (38506.4223)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.1644  max mem: 15572
[2025-01-16 04:44:36,310] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:44:36,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [ 620/2809]  eta: 0:20:50  lr: 0.000009  min_lr: 0.000000  loss: 4.2808 (4.2157)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (38625.0821)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0975  max mem: 15572
Epoch: [30]  [ 630/2809]  eta: 0:20:45  lr: 0.000008  min_lr: 0.000000  loss: 4.2069 (4.2171)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (39051.5626)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1082  max mem: 15572
[2025-01-16 04:44:46,220] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84904
[2025-01-16 04:44:46,222] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:44:46,222] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 640/2809]  eta: 0:20:38  lr: 0.000008  min_lr: 0.000000  loss: 4.2694 (4.2177)  class_acc: 0.2500 (0.3176)  loss_scale: 65536.0000 (39106.8955)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1093  max mem: 15572
Epoch: [30]  [ 650/2809]  eta: 0:20:34  lr: 0.000008  min_lr: 0.000000  loss: 4.2750 (4.2183)  class_acc: 0.3333 (0.3175)  loss_scale: 32768.0000 (39009.5238)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1376  max mem: 15572
Epoch: [30]  [ 660/2809]  eta: 0:20:28  lr: 0.000008  min_lr: 0.000000  loss: 4.2022 (4.2162)  class_acc: 0.3333 (0.3174)  loss_scale: 32768.0000 (38915.0983)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1450  max mem: 15572
Epoch: [30]  [ 670/2809]  eta: 0:20:21  lr: 0.000008  min_lr: 0.000000  loss: 4.1455 (4.2168)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (38823.4873)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0790  max mem: 15572
Epoch: [30]  [ 680/2809]  eta: 0:20:16  lr: 0.000008  min_lr: 0.000000  loss: 4.1923 (4.2165)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (38734.5668)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.0878  max mem: 15572
Epoch: [30]  [ 690/2809]  eta: 0:20:06  lr: 0.000008  min_lr: 0.000000  loss: 4.1923 (4.2164)  class_acc: 0.2500 (0.3168)  loss_scale: 32768.0000 (38648.2200)  weight_decay: 0.0500 (0.0500)  time: 0.5116  data: 0.0599  max mem: 15572
Epoch: [30]  [ 700/2809]  eta: 0:19:58  lr: 0.000008  min_lr: 0.000000  loss: 4.3660 (4.2179)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (38564.3367)  weight_decay: 0.0500 (0.0500)  time: 0.4609  data: 0.0283  max mem: 15572
Epoch: [30]  [ 710/2809]  eta: 0:19:50  lr: 0.000008  min_lr: 0.000000  loss: 4.2792 (4.2190)  class_acc: 0.2917 (0.3174)  loss_scale: 32768.0000 (38482.8129)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0661  max mem: 15572
Epoch: [30]  [ 720/2809]  eta: 0:19:47  lr: 0.000008  min_lr: 0.000000  loss: 4.2249 (4.2183)  class_acc: 0.3333 (0.3179)  loss_scale: 32768.0000 (38403.5506)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1435  max mem: 15572
[2025-01-16 04:45:37,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=85000, skipped=528, lr=[8.177507834925266e-08, 8.177507834925266e-08, 1.1682154049893238e-07, 1.1682154049893238e-07, 1.6688791499847485e-07, 1.6688791499847485e-07, 2.3841130714067836e-07, 2.3841130714067836e-07, 3.4058758162954053e-07, 3.4058758162954053e-07, 4.865536880422007e-07, 4.865536880422007e-07, 6.95076697203144e-07, 6.95076697203144e-07, 9.929667102902058e-07, 9.929667102902058e-07, 1.4185238718431511e-06, 1.4185238718431511e-06, 2.026462674061645e-06, 2.026462674061645e-06, 2.894946677230921e-06, 2.894946677230921e-06, 4.135638110329887e-06, 4.135638110329887e-06, 5.908054443328411e-06, 5.908054443328411e-06, 8.440077776183445e-06, 8.440077776183445e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:45:37,897] [INFO] [timer.py:260:stop] epoch=0/micro_step=85000/global_step=85000, RunningAvgSamplesPerSec=27.93041878345339, CurrSamplesPerSec=20.917378851073803, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [30]  [ 730/2809]  eta: 0:19:41  lr: 0.000008  min_lr: 0.000000  loss: 4.1110 (4.2170)  class_acc: 0.3333 (0.3178)  loss_scale: 32768.0000 (38326.4569)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1513  max mem: 15572
Epoch: [30]  [ 740/2809]  eta: 0:19:36  lr: 0.000008  min_lr: 0.000000  loss: 4.1491 (4.2169)  class_acc: 0.3333 (0.3180)  loss_scale: 32768.0000 (38251.4440)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1214  max mem: 15572
Epoch: [30]  [ 750/2809]  eta: 0:19:32  lr: 0.000008  min_lr: 0.000000  loss: 4.1690 (4.2165)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (38178.4288)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1759  max mem: 15572
Epoch: [30]  [ 760/2809]  eta: 0:19:25  lr: 0.000008  min_lr: 0.000000  loss: 4.3489 (4.2187)  class_acc: 0.2917 (0.3181)  loss_scale: 32768.0000 (38107.3325)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1402  max mem: 15572
[2025-01-16 04:45:57,158] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:45:57,159] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [ 770/2809]  eta: 0:19:21  lr: 0.000008  min_lr: 0.000000  loss: 4.3343 (4.2183)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (38378.0856)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1559  max mem: 15572
[2025-01-16 04:46:06,207] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85048
[2025-01-16 04:46:06,208] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:46:06,208] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 780/2809]  eta: 0:19:13  lr: 0.000008  min_lr: 0.000000  loss: 4.2000 (4.2179)  class_acc: 0.3333 (0.3176)  loss_scale: 65536.0000 (38599.9488)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1289  max mem: 15572
Epoch: [30]  [ 790/2809]  eta: 0:19:07  lr: 0.000008  min_lr: 0.000000  loss: 4.0800 (4.2154)  class_acc: 0.3333 (0.3180)  loss_scale: 32768.0000 (38526.2200)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0690  max mem: 15572
Epoch: [30]  [ 800/2809]  eta: 0:19:03  lr: 0.000008  min_lr: 0.000000  loss: 4.1849 (4.2155)  class_acc: 0.3333 (0.3186)  loss_scale: 32768.0000 (38454.3321)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1366  max mem: 15572
Epoch: [30]  [ 810/2809]  eta: 0:18:56  lr: 0.000008  min_lr: 0.000000  loss: 4.2854 (4.2153)  class_acc: 0.3750 (0.3194)  loss_scale: 32768.0000 (38384.2170)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1220  max mem: 15572
Epoch: [30]  [ 820/2809]  eta: 0:18:50  lr: 0.000008  min_lr: 0.000000  loss: 4.1886 (4.2130)  class_acc: 0.3750 (0.3196)  loss_scale: 32768.0000 (38315.8100)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0791  max mem: 15572
Epoch: [30]  [ 830/2809]  eta: 0:18:44  lr: 0.000008  min_lr: 0.000000  loss: 4.2894 (4.2149)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (38249.0493)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0932  max mem: 15572
Epoch: [30]  [ 840/2809]  eta: 0:18:38  lr: 0.000008  min_lr: 0.000000  loss: 4.3572 (4.2154)  class_acc: 0.2917 (0.3189)  loss_scale: 32768.0000 (38183.8763)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1184  max mem: 15572
Epoch: [30]  [ 850/2809]  eta: 0:18:34  lr: 0.000008  min_lr: 0.000000  loss: 4.3206 (4.2155)  class_acc: 0.2917 (0.3197)  loss_scale: 32768.0000 (38120.2350)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1719  max mem: 15572
Epoch: [30]  [ 860/2809]  eta: 0:18:29  lr: 0.000008  min_lr: 0.000000  loss: 4.0604 (4.2122)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (38058.0720)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1670  max mem: 15572
Epoch: [30]  [ 870/2809]  eta: 0:18:22  lr: 0.000008  min_lr: 0.000000  loss: 4.0604 (4.2126)  class_acc: 0.2917 (0.3193)  loss_scale: 32768.0000 (37997.3364)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0792  max mem: 15572
Epoch: [30]  [ 880/2809]  eta: 0:18:18  lr: 0.000008  min_lr: 0.000000  loss: 4.2256 (4.2122)  class_acc: 0.2500 (0.3190)  loss_scale: 32768.0000 (37937.9796)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1288  max mem: 15572
Epoch: [30]  [ 890/2809]  eta: 0:18:11  lr: 0.000008  min_lr: 0.000000  loss: 4.2148 (4.2120)  class_acc: 0.2917 (0.3194)  loss_scale: 32768.0000 (37879.9551)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1478  max mem: 15572
Epoch: [30]  [ 900/2809]  eta: 0:18:04  lr: 0.000008  min_lr: 0.000000  loss: 4.2247 (4.2134)  class_acc: 0.2917 (0.3189)  loss_scale: 32768.0000 (37823.2186)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0621  max mem: 15572
[2025-01-16 04:47:18,418] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:47:18,418] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [ 910/2809]  eta: 0:17:59  lr: 0.000008  min_lr: 0.000000  loss: 4.2247 (4.2137)  class_acc: 0.2917 (0.3188)  loss_scale: 32768.0000 (37911.6048)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.1081  max mem: 15572
[2025-01-16 04:47:23,167] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85185
[2025-01-16 04:47:23,168] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:47:23,168] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 920/2809]  eta: 0:17:52  lr: 0.000008  min_lr: 0.000000  loss: 4.1368 (4.2140)  class_acc: 0.3750 (0.3193)  loss_scale: 32768.0000 (37998.0717)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1299  max mem: 15572
Epoch: [30]  [ 930/2809]  eta: 0:17:47  lr: 0.000008  min_lr: 0.000000  loss: 4.1368 (4.2139)  class_acc: 0.3750 (0.3192)  loss_scale: 32768.0000 (37941.8947)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1174  max mem: 15572
Epoch: [30]  [ 940/2809]  eta: 0:17:42  lr: 0.000008  min_lr: 0.000000  loss: 4.1360 (4.2128)  class_acc: 0.3750 (0.3195)  loss_scale: 32768.0000 (37886.9118)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1456  max mem: 15572
Epoch: [30]  [ 950/2809]  eta: 0:17:35  lr: 0.000008  min_lr: 0.000000  loss: 4.2160 (4.2133)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (37833.0852)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1110  max mem: 15572
Epoch: [30]  [ 960/2809]  eta: 0:17:29  lr: 0.000008  min_lr: 0.000000  loss: 4.2220 (4.2142)  class_acc: 0.2917 (0.3189)  loss_scale: 32768.0000 (37780.3788)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0813  max mem: 15572
Epoch: [30]  [ 970/2809]  eta: 0:17:23  lr: 0.000008  min_lr: 0.000000  loss: 4.1771 (4.2147)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (37728.7580)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1168  max mem: 15572
Epoch: [30]  [ 980/2809]  eta: 0:17:18  lr: 0.000008  min_lr: 0.000000  loss: 4.0905 (4.2126)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (37678.1896)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1447  max mem: 15572
Epoch: [30]  [ 990/2809]  eta: 0:17:13  lr: 0.000008  min_lr: 0.000000  loss: 4.1024 (4.2111)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (37628.6418)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1539  max mem: 15572
Epoch: [30]  [1000/2809]  eta: 0:17:08  lr: 0.000008  min_lr: 0.000000  loss: 4.1290 (4.2101)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (37580.0839)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1631  max mem: 15572
Epoch: [30]  [1010/2809]  eta: 0:17:02  lr: 0.000008  min_lr: 0.000000  loss: 4.1118 (4.2084)  class_acc: 0.2917 (0.3187)  loss_scale: 32768.0000 (37532.4866)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1418  max mem: 15572
Epoch: [30]  [1020/2809]  eta: 0:16:56  lr: 0.000008  min_lr: 0.000000  loss: 4.1775 (4.2086)  class_acc: 0.2917 (0.3184)  loss_scale: 32768.0000 (37485.8217)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0955  max mem: 15572
Epoch: [30]  [1030/2809]  eta: 0:16:51  lr: 0.000008  min_lr: 0.000000  loss: 4.2560 (4.2086)  class_acc: 0.2917 (0.3188)  loss_scale: 32768.0000 (37440.0621)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1343  max mem: 15572
Epoch: [30]  [1040/2809]  eta: 0:16:46  lr: 0.000008  min_lr: 0.000000  loss: 4.1368 (4.2081)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (37395.1816)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1613  max mem: 15572
[2025-01-16 04:48:37,226] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:48:37,226] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1050/2809]  eta: 0:16:41  lr: 0.000008  min_lr: 0.000000  loss: 4.2787 (4.2096)  class_acc: 0.3333 (0.3195)  loss_scale: 32768.0000 (37569.4006)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1246  max mem: 15572
Epoch: [30]  [1060/2809]  eta: 0:16:35  lr: 0.000008  min_lr: 0.000000  loss: 4.3452 (4.2106)  class_acc: 0.3333 (0.3199)  loss_scale: 65536.0000 (37832.9877)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1288  max mem: 15572
Epoch: [30]  [1070/2809]  eta: 0:16:29  lr: 0.000008  min_lr: 0.000000  loss: 4.3287 (4.2111)  class_acc: 0.2917 (0.3194)  loss_scale: 65536.0000 (38091.6527)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1090  max mem: 15572
[2025-01-16 04:48:58,366] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85350
[2025-01-16 04:48:58,366] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:48:58,366] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1080/2809]  eta: 0:16:24  lr: 0.000008  min_lr: 0.000000  loss: 4.1827 (4.2101)  class_acc: 0.3333 (0.3202)  loss_scale: 65536.0000 (38315.2192)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1323  max mem: 15572
Epoch: [30]  [1090/2809]  eta: 0:16:17  lr: 0.000008  min_lr: 0.000000  loss: 4.1827 (4.2104)  class_acc: 0.2917 (0.3193)  loss_scale: 32768.0000 (38264.3740)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1306  max mem: 15572
Epoch: [30]  [1100/2809]  eta: 0:16:13  lr: 0.000008  min_lr: 0.000000  loss: 4.1881 (4.2100)  class_acc: 0.2917 (0.3196)  loss_scale: 32768.0000 (38214.4523)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1528  max mem: 15572
Epoch: [30]  [1110/2809]  eta: 0:16:06  lr: 0.000008  min_lr: 0.000000  loss: 4.2358 (4.2107)  class_acc: 0.2917 (0.3193)  loss_scale: 32768.0000 (38165.4293)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1298  max mem: 15572
Epoch: [30]  [1120/2809]  eta: 0:16:01  lr: 0.000008  min_lr: 0.000000  loss: 4.2358 (4.2110)  class_acc: 0.2500 (0.3186)  loss_scale: 32768.0000 (38117.2810)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0708  max mem: 15572
Epoch: [30]  [1130/2809]  eta: 0:15:55  lr: 0.000008  min_lr: 0.000000  loss: 4.1055 (4.2102)  class_acc: 0.2917 (0.3194)  loss_scale: 32768.0000 (38069.9841)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1007  max mem: 15572
Epoch: [30]  [1140/2809]  eta: 0:15:48  lr: 0.000008  min_lr: 0.000000  loss: 4.1177 (4.2105)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (38023.5162)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0807  max mem: 15572
Epoch: [30]  [1150/2809]  eta: 0:15:42  lr: 0.000008  min_lr: 0.000000  loss: 4.1215 (4.2099)  class_acc: 0.3333 (0.3193)  loss_scale: 32768.0000 (37977.8558)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0553  max mem: 15572
Epoch: [30]  [1160/2809]  eta: 0:15:36  lr: 0.000008  min_lr: 0.000000  loss: 4.2030 (4.2097)  class_acc: 0.3333 (0.3187)  loss_scale: 32768.0000 (37932.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0872  max mem: 15572
Epoch: [30]  [1170/2809]  eta: 0:15:29  lr: 0.000008  min_lr: 0.000000  loss: 4.2606 (4.2112)  class_acc: 0.2083 (0.3182)  loss_scale: 32768.0000 (37888.8745)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0756  max mem: 15572
Epoch: [30]  [1180/2809]  eta: 0:15:23  lr: 0.000008  min_lr: 0.000000  loss: 4.3206 (4.2118)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (37845.5140)  weight_decay: 0.0500 (0.0500)  time: 0.4961  data: 0.0684  max mem: 15572
Epoch: [30]  [1190/2809]  eta: 0:15:16  lr: 0.000008  min_lr: 0.000000  loss: 4.3206 (4.2117)  class_acc: 0.2917 (0.3180)  loss_scale: 32768.0000 (37802.8816)  weight_decay: 0.0500 (0.0500)  time: 0.5144  data: 0.0789  max mem: 15572
Epoch: [30]  [1200/2809]  eta: 0:15:10  lr: 0.000008  min_lr: 0.000000  loss: 4.2583 (4.2123)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (37760.9592)  weight_decay: 0.0500 (0.0500)  time: 0.5093  data: 0.0510  max mem: 15572
[2025-01-16 04:50:07,250] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:50:07,250] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1210/2809]  eta: 0:15:03  lr: 0.000008  min_lr: 0.000000  loss: 4.1692 (4.2106)  class_acc: 0.3333 (0.3189)  loss_scale: 32768.0000 (37773.8464)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0618  max mem: 15572
Epoch: [30]  [1220/2809]  eta: 0:14:58  lr: 0.000008  min_lr: 0.000000  loss: 4.1883 (4.2118)  class_acc: 0.3333 (0.3183)  loss_scale: 65536.0000 (38001.2187)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1055  max mem: 15572
[2025-01-16 04:50:15,850] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85495
[2025-01-16 04:50:15,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:50:15,850] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1230/2809]  eta: 0:14:52  lr: 0.000008  min_lr: 0.000000  loss: 4.3096 (4.2116)  class_acc: 0.2917 (0.3188)  loss_scale: 65536.0000 (38065.1828)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1294  max mem: 15572
Epoch: [30]  [1240/2809]  eta: 0:14:46  lr: 0.000008  min_lr: 0.000000  loss: 4.2185 (4.2108)  class_acc: 0.3750 (0.3189)  loss_scale: 32768.0000 (38022.4980)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0927  max mem: 15572
Epoch: [30]  [1250/2809]  eta: 0:14:43  lr: 0.000008  min_lr: 0.000000  loss: 4.1902 (4.2106)  class_acc: 0.3333 (0.3188)  loss_scale: 32768.0000 (37980.4956)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.1571  max mem: 15572
Epoch: [30]  [1260/2809]  eta: 0:14:37  lr: 0.000008  min_lr: 0.000000  loss: 4.1517 (4.2098)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (37939.1594)  weight_decay: 0.0500 (0.0500)  time: 0.6352  data: 0.1748  max mem: 15572
Epoch: [30]  [1270/2809]  eta: 0:14:31  lr: 0.000008  min_lr: 0.000000  loss: 4.1596 (4.2088)  class_acc: 0.2917 (0.3186)  loss_scale: 32768.0000 (37898.4736)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1084  max mem: 15572
Epoch: [30]  [1280/2809]  eta: 0:14:25  lr: 0.000008  min_lr: 0.000000  loss: 4.1645 (4.2092)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (37858.4231)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0984  max mem: 15572
Epoch: [30]  [1290/2809]  eta: 0:14:19  lr: 0.000008  min_lr: 0.000000  loss: 4.4033 (4.2113)  class_acc: 0.2500 (0.3174)  loss_scale: 32768.0000 (37818.9930)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1067  max mem: 15572
Epoch: [30]  [1300/2809]  eta: 0:14:14  lr: 0.000008  min_lr: 0.000000  loss: 4.3956 (4.2117)  class_acc: 0.2083 (0.3174)  loss_scale: 32768.0000 (37780.1691)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1450  max mem: 15572
Epoch: [30]  [1310/2809]  eta: 0:14:07  lr: 0.000008  min_lr: 0.000000  loss: 4.2142 (4.2116)  class_acc: 0.2500 (0.3170)  loss_scale: 32768.0000 (37741.9375)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0805  max mem: 15572
Epoch: [30]  [1320/2809]  eta: 0:14:02  lr: 0.000008  min_lr: 0.000000  loss: 4.1923 (4.2111)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (37704.2846)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0832  max mem: 15572
Epoch: [30]  [1330/2809]  eta: 0:13:56  lr: 0.000008  min_lr: 0.000000  loss: 4.1466 (4.2113)  class_acc: 0.3750 (0.3174)  loss_scale: 32768.0000 (37667.1976)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1254  max mem: 15572
Epoch: [30]  [1340/2809]  eta: 0:13:51  lr: 0.000008  min_lr: 0.000000  loss: 4.2277 (4.2109)  class_acc: 0.3333 (0.3177)  loss_scale: 32768.0000 (37630.6637)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1427  max mem: 15572
Epoch: [30]  [1350/2809]  eta: 0:13:46  lr: 0.000008  min_lr: 0.000000  loss: 4.2161 (4.2108)  class_acc: 0.3333 (0.3175)  loss_scale: 32768.0000 (37594.6706)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1884  max mem: 15572
[2025-01-16 04:51:30,484] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:51:30,484] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1360/2809]  eta: 0:13:39  lr: 0.000008  min_lr: 0.000000  loss: 4.1969 (4.2107)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (37727.7414)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0884  max mem: 15572
Epoch: [30]  [1370/2809]  eta: 0:13:34  lr: 0.000008  min_lr: 0.000000  loss: 4.1366 (4.2104)  class_acc: 0.3333 (0.3178)  loss_scale: 65536.0000 (37930.5733)  weight_decay: 0.0500 (0.0500)  time: 0.5133  data: 0.0687  max mem: 15572
Epoch: [30]  [1380/2809]  eta: 0:13:28  lr: 0.000008  min_lr: 0.000000  loss: 4.1265 (4.2104)  class_acc: 0.3333 (0.3177)  loss_scale: 65536.0000 (38130.4678)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1363  max mem: 15572
Epoch: [30]  [1390/2809]  eta: 0:13:24  lr: 0.000008  min_lr: 0.000000  loss: 4.2797 (4.2114)  class_acc: 0.2500 (0.3169)  loss_scale: 65536.0000 (38327.4881)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1790  max mem: 15572
Epoch: [30]  [1400/2809]  eta: 0:13:18  lr: 0.000008  min_lr: 0.000000  loss: 4.2434 (4.2103)  class_acc: 0.2500 (0.3169)  loss_scale: 65536.0000 (38521.6959)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1643  max mem: 15572
Epoch: [30]  [1410/2809]  eta: 0:13:12  lr: 0.000008  min_lr: 0.000000  loss: 4.0303 (4.2095)  class_acc: 0.2917 (0.3168)  loss_scale: 65536.0000 (38713.1510)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0929  max mem: 15572
Epoch: [30]  [1420/2809]  eta: 0:13:06  lr: 0.000008  min_lr: 0.000000  loss: 4.0303 (4.2087)  class_acc: 0.2917 (0.3171)  loss_scale: 65536.0000 (38901.9113)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0750  max mem: 15572
Epoch: [30]  [1430/2809]  eta: 0:13:00  lr: 0.000008  min_lr: 0.000000  loss: 4.1718 (4.2087)  class_acc: 0.2917 (0.3169)  loss_scale: 65536.0000 (39088.0335)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0761  max mem: 15572
Epoch: [30]  [1440/2809]  eta: 0:12:54  lr: 0.000008  min_lr: 0.000000  loss: 4.1718 (4.2076)  class_acc: 0.2917 (0.3168)  loss_scale: 65536.0000 (39271.5725)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0983  max mem: 15572
Epoch: [30]  [1450/2809]  eta: 0:12:48  lr: 0.000008  min_lr: 0.000000  loss: 4.0835 (4.2078)  class_acc: 0.2917 (0.3164)  loss_scale: 65536.0000 (39452.5817)  weight_decay: 0.0500 (0.0500)  time: 0.5166  data: 0.0931  max mem: 15572
Epoch: [30]  [1460/2809]  eta: 0:12:43  lr: 0.000008  min_lr: 0.000000  loss: 4.1857 (4.2085)  class_acc: 0.2500 (0.3159)  loss_scale: 65536.0000 (39631.1129)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1568  max mem: 15572
[2025-01-16 04:52:30,762] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85731
[2025-01-16 04:52:30,762] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:52:30,762] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1470/2809]  eta: 0:12:37  lr: 0.000008  min_lr: 0.000000  loss: 4.1857 (4.2085)  class_acc: 0.2500 (0.3162)  loss_scale: 32768.0000 (39584.4568)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1466  max mem: 15572
Epoch: [30]  [1480/2809]  eta: 0:12:31  lr: 0.000008  min_lr: 0.000000  loss: 4.1361 (4.2087)  class_acc: 0.3333 (0.3163)  loss_scale: 32768.0000 (39538.4308)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0838  max mem: 15572
Epoch: [30]  [1490/2809]  eta: 0:12:25  lr: 0.000008  min_lr: 0.000000  loss: 4.2846 (4.2098)  class_acc: 0.3333 (0.3166)  loss_scale: 32768.0000 (39493.0221)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1303  max mem: 15572
Epoch: [30]  [1500/2809]  eta: 0:12:20  lr: 0.000008  min_lr: 0.000000  loss: 4.3788 (4.2106)  class_acc: 0.2917 (0.3165)  loss_scale: 32768.0000 (39448.2185)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1283  max mem: 15572
Epoch: [30]  [1510/2809]  eta: 0:12:14  lr: 0.000008  min_lr: 0.000000  loss: 4.2434 (4.2104)  class_acc: 0.2917 (0.3165)  loss_scale: 32768.0000 (39404.0079)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1104  max mem: 15572
Epoch: [30]  [1520/2809]  eta: 0:12:08  lr: 0.000008  min_lr: 0.000000  loss: 4.1371 (4.2097)  class_acc: 0.3333 (0.3166)  loss_scale: 32768.0000 (39360.3787)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0951  max mem: 15572
Epoch: [30]  [1530/2809]  eta: 0:12:03  lr: 0.000008  min_lr: 0.000000  loss: 4.1371 (4.2096)  class_acc: 0.2917 (0.3165)  loss_scale: 32768.0000 (39317.3194)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1252  max mem: 15572
Epoch: [30]  [1540/2809]  eta: 0:11:57  lr: 0.000008  min_lr: 0.000000  loss: 4.1888 (4.2099)  class_acc: 0.3333 (0.3166)  loss_scale: 32768.0000 (39274.8189)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1225  max mem: 15572
Epoch: [30]  [1550/2809]  eta: 0:11:52  lr: 0.000008  min_lr: 0.000000  loss: 4.1888 (4.2102)  class_acc: 0.2917 (0.3164)  loss_scale: 32768.0000 (39232.8665)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1122  max mem: 15572
Epoch: [30]  [1560/2809]  eta: 0:11:46  lr: 0.000008  min_lr: 0.000000  loss: 4.2614 (4.2099)  class_acc: 0.2500 (0.3161)  loss_scale: 32768.0000 (39191.4516)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1562  max mem: 15572
Epoch: [30]  [1570/2809]  eta: 0:11:41  lr: 0.000008  min_lr: 0.000000  loss: 4.2775 (4.2103)  class_acc: 0.2500 (0.3159)  loss_scale: 32768.0000 (39150.5640)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.1625  max mem: 15572
Epoch: [30]  [1580/2809]  eta: 0:11:35  lr: 0.000008  min_lr: 0.000000  loss: 4.2496 (4.2108)  class_acc: 0.2500 (0.3155)  loss_scale: 32768.0000 (39110.1935)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1044  max mem: 15572
[2025-01-16 04:53:43,454] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:53:43,455] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1590/2809]  eta: 0:11:29  lr: 0.000008  min_lr: 0.000000  loss: 4.2857 (4.2114)  class_acc: 0.2500 (0.3154)  loss_scale: 32768.0000 (39090.9265)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0606  max mem: 15572
[2025-01-16 04:53:45,935] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85863
[2025-01-16 04:53:45,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:53:45,936] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1600/2809]  eta: 0:11:24  lr: 0.000008  min_lr: 0.000000  loss: 4.3009 (4.2117)  class_acc: 0.2500 (0.3152)  loss_scale: 32768.0000 (39092.3673)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0943  max mem: 15572
Epoch: [30]  [1610/2809]  eta: 0:11:18  lr: 0.000008  min_lr: 0.000000  loss: 4.2632 (4.2116)  class_acc: 0.2917 (0.3154)  loss_scale: 32768.0000 (39053.1099)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1445  max mem: 15572
Epoch: [30]  [1620/2809]  eta: 0:11:13  lr: 0.000008  min_lr: 0.000000  loss: 4.1949 (4.2113)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (39014.3368)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1528  max mem: 15572
Epoch: [30]  [1630/2809]  eta: 0:11:07  lr: 0.000008  min_lr: 0.000000  loss: 4.0125 (4.2110)  class_acc: 0.3333 (0.3157)  loss_scale: 32768.0000 (38976.0392)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0884  max mem: 15572
Epoch: [30]  [1640/2809]  eta: 0:11:01  lr: 0.000008  min_lr: 0.000000  loss: 4.2027 (4.2113)  class_acc: 0.3333 (0.3157)  loss_scale: 32768.0000 (38938.2084)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0491  max mem: 15572
Epoch: [30]  [1650/2809]  eta: 0:10:55  lr: 0.000008  min_lr: 0.000000  loss: 4.2529 (4.2112)  class_acc: 0.2917 (0.3158)  loss_scale: 32768.0000 (38900.8359)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0747  max mem: 15572
Epoch: [30]  [1660/2809]  eta: 0:10:49  lr: 0.000008  min_lr: 0.000000  loss: 4.1779 (4.2106)  class_acc: 0.2917 (0.3158)  loss_scale: 32768.0000 (38863.9133)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1245  max mem: 15572
Epoch: [30]  [1670/2809]  eta: 0:10:43  lr: 0.000008  min_lr: 0.000000  loss: 4.1020 (4.2101)  class_acc: 0.2917 (0.3161)  loss_scale: 32768.0000 (38827.4327)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1219  max mem: 15572
Epoch: [30]  [1680/2809]  eta: 0:10:38  lr: 0.000008  min_lr: 0.000000  loss: 4.1020 (4.2097)  class_acc: 0.3333 (0.3164)  loss_scale: 32768.0000 (38791.3861)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1051  max mem: 15572
Epoch: [30]  [1690/2809]  eta: 0:10:32  lr: 0.000008  min_lr: 0.000000  loss: 4.2694 (4.2107)  class_acc: 0.2917 (0.3164)  loss_scale: 32768.0000 (38755.7658)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1153  max mem: 15572
Epoch: [30]  [1700/2809]  eta: 0:10:27  lr: 0.000008  min_lr: 0.000000  loss: 4.2116 (4.2104)  class_acc: 0.3333 (0.3166)  loss_scale: 32768.0000 (38720.5644)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1159  max mem: 15572
Epoch: [30]  [1710/2809]  eta: 0:10:21  lr: 0.000008  min_lr: 0.000000  loss: 4.1725 (4.2107)  class_acc: 0.3333 (0.3164)  loss_scale: 32768.0000 (38685.7744)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1453  max mem: 15572
Epoch: [30]  [1720/2809]  eta: 0:10:16  lr: 0.000008  min_lr: 0.000000  loss: 4.1488 (4.2108)  class_acc: 0.2083 (0.3161)  loss_scale: 32768.0000 (38651.3887)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1496  max mem: 15572
[2025-01-16 04:54:58,228] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:54:58,228] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 04:55:01,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=86000, skipped=534, lr=[7.628959446008848e-08, 7.628959446008848e-08, 1.0898513494298357e-07, 1.0898513494298357e-07, 1.5569304991854797e-07, 1.5569304991854797e-07, 2.2241864274078283e-07, 2.2241864274078283e-07, 3.177409182011183e-07, 3.177409182011183e-07, 4.5391559743016906e-07, 4.5391559743016906e-07, 6.484508534716701e-07, 6.484508534716701e-07, 9.26358362102386e-07, 9.26358362102386e-07, 1.3233690887176942e-06, 1.3233690887176942e-06, 1.8905272695967064e-06, 1.8905272695967064e-06, 2.700753242281009e-06, 2.700753242281009e-06, 3.858218917544299e-06, 3.858218917544299e-06, 5.5117413107775706e-06, 5.5117413107775706e-06, 7.873916158253672e-06, 7.873916158253672e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:55:01,374] [INFO] [timer.py:260:stop] epoch=0/micro_step=86000/global_step=86000, RunningAvgSamplesPerSec=27.935772226697473, CurrSamplesPerSec=28.151407440386468, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [30]  [1730/2809]  eta: 0:10:09  lr: 0.000008  min_lr: 0.000000  loss: 4.1340 (4.2108)  class_acc: 0.2500 (0.3161)  loss_scale: 32768.0000 (38787.7712)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0819  max mem: 15572
Epoch: [30]  [1740/2809]  eta: 0:10:04  lr: 0.000008  min_lr: 0.000000  loss: 4.1456 (4.2107)  class_acc: 0.2500 (0.3159)  loss_scale: 65536.0000 (38941.4084)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0660  max mem: 15572
Epoch: [30]  [1750/2809]  eta: 0:09:58  lr: 0.000008  min_lr: 0.000000  loss: 4.2380 (4.2111)  class_acc: 0.2917 (0.3162)  loss_scale: 65536.0000 (39093.2907)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1242  max mem: 15572
[2025-01-16 04:55:13,616] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86021
[2025-01-16 04:55:13,616] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:55:13,616] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1760/2809]  eta: 0:09:52  lr: 0.000008  min_lr: 0.000000  loss: 4.1847 (4.2105)  class_acc: 0.3333 (0.3161)  loss_scale: 32768.0000 (39057.3719)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1304  max mem: 15572
Epoch: [30]  [1770/2809]  eta: 0:09:47  lr: 0.000008  min_lr: 0.000000  loss: 4.1397 (4.2104)  class_acc: 0.2917 (0.3162)  loss_scale: 32768.0000 (39021.8588)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1294  max mem: 15572
Epoch: [30]  [1780/2809]  eta: 0:09:41  lr: 0.000008  min_lr: 0.000000  loss: 4.1148 (4.2097)  class_acc: 0.3333 (0.3163)  loss_scale: 32768.0000 (38986.7445)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1199  max mem: 15572
Epoch: [30]  [1790/2809]  eta: 0:09:36  lr: 0.000008  min_lr: 0.000000  loss: 4.2002 (4.2107)  class_acc: 0.3750 (0.3165)  loss_scale: 32768.0000 (38952.0223)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1016  max mem: 15572
Epoch: [30]  [1800/2809]  eta: 0:09:30  lr: 0.000008  min_lr: 0.000000  loss: 4.3162 (4.2106)  class_acc: 0.3333 (0.3166)  loss_scale: 32768.0000 (38917.6857)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1644  max mem: 15572
Epoch: [30]  [1810/2809]  eta: 0:09:25  lr: 0.000008  min_lr: 0.000000  loss: 4.2036 (4.2100)  class_acc: 0.2917 (0.3162)  loss_scale: 32768.0000 (38883.7283)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1808  max mem: 15572
Epoch: [30]  [1820/2809]  eta: 0:09:20  lr: 0.000008  min_lr: 0.000000  loss: 4.2126 (4.2102)  class_acc: 0.2500 (0.3160)  loss_scale: 32768.0000 (38850.1439)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1776  max mem: 15572
Epoch: [30]  [1830/2809]  eta: 0:09:14  lr: 0.000008  min_lr: 0.000000  loss: 4.3000 (4.2100)  class_acc: 0.2917 (0.3163)  loss_scale: 32768.0000 (38816.9263)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.1918  max mem: 15572
Epoch: [30]  [1840/2809]  eta: 0:09:09  lr: 0.000008  min_lr: 0.000000  loss: 4.0651 (4.2092)  class_acc: 0.3333 (0.3165)  loss_scale: 32768.0000 (38784.0695)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1650  max mem: 15572
Epoch: [30]  [1850/2809]  eta: 0:09:03  lr: 0.000008  min_lr: 0.000000  loss: 4.1187 (4.2091)  class_acc: 0.3333 (0.3167)  loss_scale: 32768.0000 (38751.5678)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1221  max mem: 15572
Epoch: [30]  [1860/2809]  eta: 0:08:57  lr: 0.000008  min_lr: 0.000000  loss: 4.2288 (4.2097)  class_acc: 0.3333 (0.3167)  loss_scale: 32768.0000 (38719.4154)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1175  max mem: 15572
Epoch: [30]  [1870/2809]  eta: 0:08:52  lr: 0.000008  min_lr: 0.000000  loss: 4.2288 (4.2093)  class_acc: 0.3750 (0.3169)  loss_scale: 32768.0000 (38687.6066)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1351  max mem: 15572
[2025-01-16 04:56:30,973] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:56:30,974] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1880/2809]  eta: 0:08:47  lr: 0.000008  min_lr: 0.000000  loss: 4.0246 (4.2085)  class_acc: 0.3750 (0.3170)  loss_scale: 32768.0000 (38673.5566)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.1954  max mem: 15572
[2025-01-16 04:56:33,153] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86155
[2025-01-16 04:56:33,153] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:56:33,153] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1890/2809]  eta: 0:08:40  lr: 0.000008  min_lr: 0.000000  loss: 4.1596 (4.2089)  class_acc: 0.2500 (0.3169)  loss_scale: 32768.0000 (38711.6404)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1350  max mem: 15572
Epoch: [30]  [1900/2809]  eta: 0:08:34  lr: 0.000008  min_lr: 0.000000  loss: 4.2038 (4.2088)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (38680.3745)  weight_decay: 0.0500 (0.0500)  time: 0.4613  data: 0.0293  max mem: 15572
Epoch: [30]  [1910/2809]  eta: 0:08:28  lr: 0.000008  min_lr: 0.000000  loss: 4.3333 (4.2098)  class_acc: 0.2500 (0.3165)  loss_scale: 32768.0000 (38649.4359)  weight_decay: 0.0500 (0.0500)  time: 0.4964  data: 0.0416  max mem: 15572
Epoch: [30]  [1920/2809]  eta: 0:08:23  lr: 0.000008  min_lr: 0.000000  loss: 4.0890 (4.2091)  class_acc: 0.2083 (0.3166)  loss_scale: 32768.0000 (38618.8194)  weight_decay: 0.0500 (0.0500)  time: 0.5269  data: 0.0745  max mem: 15572
Epoch: [30]  [1930/2809]  eta: 0:08:17  lr: 0.000008  min_lr: 0.000000  loss: 4.0361 (4.2090)  class_acc: 0.3333 (0.3167)  loss_scale: 32768.0000 (38588.5199)  weight_decay: 0.0500 (0.0500)  time: 0.5120  data: 0.0617  max mem: 15572
Epoch: [30]  [1940/2809]  eta: 0:08:11  lr: 0.000008  min_lr: 0.000000  loss: 4.2248 (4.2093)  class_acc: 0.3333 (0.3169)  loss_scale: 32768.0000 (38558.5327)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0751  max mem: 15572
Epoch: [30]  [1950/2809]  eta: 0:08:06  lr: 0.000008  min_lr: 0.000000  loss: 4.2585 (4.2095)  class_acc: 0.2917 (0.3169)  loss_scale: 32768.0000 (38528.8529)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1633  max mem: 15572
Epoch: [30]  [1960/2809]  eta: 0:08:00  lr: 0.000008  min_lr: 0.000000  loss: 4.1366 (4.2092)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (38499.4758)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1628  max mem: 15572
Epoch: [30]  [1970/2809]  eta: 0:07:54  lr: 0.000008  min_lr: 0.000000  loss: 4.1065 (4.2089)  class_acc: 0.3333 (0.3172)  loss_scale: 32768.0000 (38470.3968)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1151  max mem: 15572
Epoch: [30]  [1980/2809]  eta: 0:07:49  lr: 0.000008  min_lr: 0.000000  loss: 4.2478 (4.2083)  class_acc: 0.3333 (0.3174)  loss_scale: 32768.0000 (38441.6113)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1075  max mem: 15572
Epoch: [30]  [1990/2809]  eta: 0:07:43  lr: 0.000008  min_lr: 0.000000  loss: 4.2925 (4.2092)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (38413.1150)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.1004  max mem: 15572
Epoch: [30]  [2000/2809]  eta: 0:07:37  lr: 0.000008  min_lr: 0.000000  loss: 4.2309 (4.2082)  class_acc: 0.3333 (0.3174)  loss_scale: 32768.0000 (38384.9035)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0696  max mem: 15572
Epoch: [30]  [2010/2809]  eta: 0:07:32  lr: 0.000008  min_lr: 0.000000  loss: 3.9916 (4.2080)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (38356.9727)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1297  max mem: 15572
[2025-01-16 04:57:43,281] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:57:43,282] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [2020/2809]  eta: 0:07:26  lr: 0.000008  min_lr: 0.000000  loss: 4.0302 (4.2074)  class_acc: 0.2917 (0.3176)  loss_scale: 32768.0000 (38442.8144)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1841  max mem: 15572
Epoch: [30]  [2030/2809]  eta: 0:07:20  lr: 0.000008  min_lr: 0.000000  loss: 4.0767 (4.2075)  class_acc: 0.3333 (0.3172)  loss_scale: 65536.0000 (38576.2127)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1335  max mem: 15572
Epoch: [30]  [2040/2809]  eta: 0:07:15  lr: 0.000008  min_lr: 0.000000  loss: 4.0767 (4.2070)  class_acc: 0.3333 (0.3172)  loss_scale: 65536.0000 (38708.3038)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0743  max mem: 15572
Epoch: [30]  [2050/2809]  eta: 0:07:09  lr: 0.000008  min_lr: 0.000000  loss: 4.1067 (4.2070)  class_acc: 0.2917 (0.3172)  loss_scale: 65536.0000 (38839.1068)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0678  max mem: 15572
[2025-01-16 04:58:08,951] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86329
[2025-01-16 04:58:08,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:58:08,951] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [2060/2809]  eta: 0:07:03  lr: 0.000008  min_lr: 0.000000  loss: 4.1649 (4.2069)  class_acc: 0.2917 (0.3172)  loss_scale: 65536.0000 (38936.8423)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1157  max mem: 15572
Epoch: [30]  [2070/2809]  eta: 0:06:58  lr: 0.000008  min_lr: 0.000000  loss: 4.1998 (4.2069)  class_acc: 0.3750 (0.3175)  loss_scale: 32768.0000 (38907.0555)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1562  max mem: 15572
Epoch: [30]  [2080/2809]  eta: 0:06:52  lr: 0.000008  min_lr: 0.000000  loss: 4.2397 (4.2067)  class_acc: 0.2917 (0.3177)  loss_scale: 32768.0000 (38877.5550)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1288  max mem: 15572
Epoch: [30]  [2090/2809]  eta: 0:06:46  lr: 0.000008  min_lr: 0.000000  loss: 4.2510 (4.2061)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (38848.3367)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0955  max mem: 15572
Epoch: [30]  [2100/2809]  eta: 0:06:40  lr: 0.000008  min_lr: 0.000000  loss: 4.1115 (4.2059)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (38819.3965)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0571  max mem: 15572
Epoch: [30]  [2110/2809]  eta: 0:06:35  lr: 0.000008  min_lr: 0.000000  loss: 4.1509 (4.2059)  class_acc: 0.2917 (0.3177)  loss_scale: 32768.0000 (38790.7305)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1192  max mem: 15572
Epoch: [30]  [2120/2809]  eta: 0:06:29  lr: 0.000008  min_lr: 0.000000  loss: 4.1509 (4.2059)  class_acc: 0.3333 (0.3179)  loss_scale: 32768.0000 (38762.3347)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1784  max mem: 15572
Epoch: [30]  [2130/2809]  eta: 0:06:24  lr: 0.000008  min_lr: 0.000000  loss: 4.1302 (4.2057)  class_acc: 0.3333 (0.3178)  loss_scale: 32768.0000 (38734.2055)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1542  max mem: 15572
Epoch: [30]  [2140/2809]  eta: 0:06:18  lr: 0.000008  min_lr: 0.000000  loss: 4.2197 (4.2060)  class_acc: 0.2500 (0.3175)  loss_scale: 32768.0000 (38706.3391)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1560  max mem: 15572
Epoch: [30]  [2150/2809]  eta: 0:06:12  lr: 0.000008  min_lr: 0.000000  loss: 4.2338 (4.2061)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (38678.7318)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.1038  max mem: 15572
Epoch: [30]  [2160/2809]  eta: 0:06:07  lr: 0.000008  min_lr: 0.000000  loss: 4.2624 (4.2062)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (38651.3799)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1052  max mem: 15572
Epoch: [30]  [2170/2809]  eta: 0:06:01  lr: 0.000008  min_lr: 0.000000  loss: 4.2069 (4.2061)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (38624.2801)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1300  max mem: 15572
Epoch: [30]  [2180/2809]  eta: 0:05:56  lr: 0.000008  min_lr: 0.000000  loss: 4.1044 (4.2056)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (38597.4287)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1246  max mem: 15572
[2025-01-16 04:59:22,155] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:59:22,155] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [2190/2809]  eta: 0:05:50  lr: 0.000008  min_lr: 0.000000  loss: 4.2034 (4.2060)  class_acc: 0.2917 (0.3170)  loss_scale: 32768.0000 (38615.6896)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1361  max mem: 15572
[2025-01-16 04:59:27,521] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86467
[2025-01-16 04:59:27,521] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:59:27,522] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [2200/2809]  eta: 0:05:44  lr: 0.000008  min_lr: 0.000000  loss: 4.3175 (4.2069)  class_acc: 0.3333 (0.3171)  loss_scale: 32768.0000 (38678.4480)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1211  max mem: 15572
Epoch: [30]  [2210/2809]  eta: 0:05:39  lr: 0.000008  min_lr: 0.000000  loss: 4.2339 (4.2063)  class_acc: 0.3333 (0.3174)  loss_scale: 32768.0000 (38651.7160)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1125  max mem: 15572
Epoch: [30]  [2220/2809]  eta: 0:05:33  lr: 0.000008  min_lr: 0.000000  loss: 4.2281 (4.2061)  class_acc: 0.3750 (0.3175)  loss_scale: 32768.0000 (38625.2247)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0577  max mem: 15572
Epoch: [30]  [2230/2809]  eta: 0:05:27  lr: 0.000008  min_lr: 0.000000  loss: 4.0044 (4.2042)  class_acc: 0.3750 (0.3180)  loss_scale: 32768.0000 (38598.9709)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0979  max mem: 15572
Epoch: [30]  [2240/2809]  eta: 0:05:22  lr: 0.000008  min_lr: 0.000000  loss: 4.0237 (4.2049)  class_acc: 0.3333 (0.3178)  loss_scale: 32768.0000 (38572.9514)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1495  max mem: 15572
Epoch: [30]  [2250/2809]  eta: 0:05:16  lr: 0.000008  min_lr: 0.000000  loss: 4.2051 (4.2049)  class_acc: 0.2500 (0.3175)  loss_scale: 32768.0000 (38547.1630)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0635  max mem: 15572
Epoch: [30]  [2260/2809]  eta: 0:05:10  lr: 0.000008  min_lr: 0.000000  loss: 4.1021 (4.2048)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (38521.6028)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0915  max mem: 15572
Epoch: [30]  [2270/2809]  eta: 0:05:04  lr: 0.000008  min_lr: 0.000000  loss: 4.0670 (4.2042)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (38496.2677)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1336  max mem: 15572
Epoch: [30]  [2280/2809]  eta: 0:04:59  lr: 0.000008  min_lr: 0.000000  loss: 4.1925 (4.2050)  class_acc: 0.2500 (0.3169)  loss_scale: 32768.0000 (38471.1548)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1111  max mem: 15572
Epoch: [30]  [2290/2809]  eta: 0:04:53  lr: 0.000008  min_lr: 0.000000  loss: 4.2027 (4.2044)  class_acc: 0.2083 (0.3169)  loss_scale: 32768.0000 (38446.2610)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1234  max mem: 15572
Epoch: [30]  [2300/2809]  eta: 0:04:47  lr: 0.000008  min_lr: 0.000000  loss: 4.1525 (4.2042)  class_acc: 0.2917 (0.3169)  loss_scale: 32768.0000 (38421.5837)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0958  max mem: 15572
Epoch: [30]  [2310/2809]  eta: 0:04:42  lr: 0.000008  min_lr: 0.000000  loss: 4.3462 (4.2051)  class_acc: 0.2917 (0.3168)  loss_scale: 32768.0000 (38397.1199)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0649  max mem: 15572
Epoch: [30]  [2320/2809]  eta: 0:04:36  lr: 0.000008  min_lr: 0.000000  loss: 4.3462 (4.2042)  class_acc: 0.2917 (0.3170)  loss_scale: 32768.0000 (38372.8669)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0876  max mem: 15572
[2025-01-16 05:00:38,176] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:00:38,176] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [2330/2809]  eta: 0:04:30  lr: 0.000008  min_lr: 0.000000  loss: 4.3184 (4.2045)  class_acc: 0.2917 (0.3168)  loss_scale: 32768.0000 (38419.1094)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1289  max mem: 15572
Epoch: [30]  [2340/2809]  eta: 0:04:25  lr: 0.000008  min_lr: 0.000000  loss: 4.3630 (4.2052)  class_acc: 0.2500 (0.3167)  loss_scale: 65536.0000 (38534.9440)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1042  max mem: 15572
Epoch: [30]  [2350/2809]  eta: 0:04:19  lr: 0.000008  min_lr: 0.000000  loss: 4.3395 (4.2051)  class_acc: 0.2917 (0.3167)  loss_scale: 65536.0000 (38649.7933)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0686  max mem: 15572
Epoch: [30]  [2360/2809]  eta: 0:04:13  lr: 0.000008  min_lr: 0.000000  loss: 4.2409 (4.2046)  class_acc: 0.2917 (0.3166)  loss_scale: 65536.0000 (38763.6696)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1454  max mem: 15572
Epoch: [30]  [2370/2809]  eta: 0:04:08  lr: 0.000008  min_lr: 0.000000  loss: 4.2409 (4.2047)  class_acc: 0.2917 (0.3168)  loss_scale: 65536.0000 (38876.5854)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1380  max mem: 15572
Epoch: [30]  [2380/2809]  eta: 0:04:02  lr: 0.000008  min_lr: 0.000000  loss: 4.2420 (4.2045)  class_acc: 0.2917 (0.3168)  loss_scale: 65536.0000 (38988.5527)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.0793  max mem: 15572
[2025-01-16 05:01:10,082] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86651
[2025-01-16 05:01:10,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:01:10,083] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [2390/2809]  eta: 0:03:57  lr: 0.000008  min_lr: 0.000000  loss: 4.1547 (4.2044)  class_acc: 0.2917 (0.3167)  loss_scale: 32768.0000 (38962.5362)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1345  max mem: 15572
Epoch: [30]  [2400/2809]  eta: 0:03:51  lr: 0.000008  min_lr: 0.000000  loss: 4.1381 (4.2043)  class_acc: 0.3750 (0.3172)  loss_scale: 32768.0000 (38936.7364)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1135  max mem: 15572
Epoch: [30]  [2410/2809]  eta: 0:03:45  lr: 0.000007  min_lr: 0.000000  loss: 4.0869 (4.2044)  class_acc: 0.3750 (0.3174)  loss_scale: 32768.0000 (38911.1506)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0868  max mem: 15572
Epoch: [30]  [2420/2809]  eta: 0:03:39  lr: 0.000007  min_lr: 0.000000  loss: 4.2745 (4.2043)  class_acc: 0.2917 (0.3174)  loss_scale: 32768.0000 (38885.7761)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1011  max mem: 15572
Epoch: [30]  [2430/2809]  eta: 0:03:34  lr: 0.000007  min_lr: 0.000000  loss: 4.0999 (4.2040)  class_acc: 0.3750 (0.3177)  loss_scale: 32768.0000 (38860.6104)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1004  max mem: 15572
Epoch: [30]  [2440/2809]  eta: 0:03:28  lr: 0.000007  min_lr: 0.000000  loss: 4.0757 (4.2037)  class_acc: 0.3750 (0.3177)  loss_scale: 32768.0000 (38835.6510)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0723  max mem: 15572
Epoch: [30]  [2450/2809]  eta: 0:03:23  lr: 0.000007  min_lr: 0.000000  loss: 4.1625 (4.2042)  class_acc: 0.3333 (0.3180)  loss_scale: 32768.0000 (38810.8951)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0739  max mem: 15572
Epoch: [30]  [2460/2809]  eta: 0:03:17  lr: 0.000007  min_lr: 0.000000  loss: 4.1790 (4.2036)  class_acc: 0.3750 (0.3182)  loss_scale: 32768.0000 (38786.3405)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0631  max mem: 15572
Epoch: [30]  [2470/2809]  eta: 0:03:11  lr: 0.000007  min_lr: 0.000000  loss: 4.0922 (4.2033)  class_acc: 0.3750 (0.3183)  loss_scale: 32768.0000 (38761.9846)  weight_decay: 0.0500 (0.0500)  time: 0.5224  data: 0.0569  max mem: 15572
Epoch: [30]  [2480/2809]  eta: 0:03:05  lr: 0.000007  min_lr: 0.000000  loss: 4.0643 (4.2030)  class_acc: 0.3333 (0.3182)  loss_scale: 32768.0000 (38737.8251)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0918  max mem: 15572
Epoch: [30]  [2490/2809]  eta: 0:03:00  lr: 0.000007  min_lr: 0.000000  loss: 4.1456 (4.2028)  class_acc: 0.2917 (0.3182)  loss_scale: 32768.0000 (38713.8595)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1050  max mem: 15572
Epoch: [30]  [2500/2809]  eta: 0:02:54  lr: 0.000007  min_lr: 0.000000  loss: 4.1496 (4.2027)  class_acc: 0.3333 (0.3183)  loss_scale: 32768.0000 (38690.0856)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0981  max mem: 15572
[2025-01-16 05:02:23,197] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:02:23,198] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [2510/2809]  eta: 0:02:49  lr: 0.000007  min_lr: 0.000000  loss: 4.0948 (4.2021)  class_acc: 0.3333 (0.3182)  loss_scale: 32768.0000 (38679.5508)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0594  max mem: 15572
Epoch: [30]  [2520/2809]  eta: 0:02:43  lr: 0.000007  min_lr: 0.000000  loss: 4.1701 (4.2023)  class_acc: 0.2917 (0.3183)  loss_scale: 65536.0000 (38786.0817)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1040  max mem: 15572
Epoch: [30]  [2530/2809]  eta: 0:02:37  lr: 0.000007  min_lr: 0.000000  loss: 4.1487 (4.2019)  class_acc: 0.2917 (0.3182)  loss_scale: 65536.0000 (38891.7708)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0733  max mem: 15572
Epoch: [30]  [2540/2809]  eta: 0:02:31  lr: 0.000007  min_lr: 0.000000  loss: 4.1394 (4.2017)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (38996.6281)  weight_decay: 0.0500 (0.0500)  time: 0.4878  data: 0.0136  max mem: 15572
Epoch: [30]  [2550/2809]  eta: 0:02:26  lr: 0.000007  min_lr: 0.000000  loss: 4.2028 (4.2018)  class_acc: 0.2917 (0.3182)  loss_scale: 65536.0000 (39100.6633)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0801  max mem: 15572
Epoch: [30]  [2560/2809]  eta: 0:02:20  lr: 0.000007  min_lr: 0.000000  loss: 4.2028 (4.2019)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (39203.8860)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1170  max mem: 15572
[2025-01-16 05:02:53,010] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86835
[2025-01-16 05:02:53,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:02:53,010] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [2570/2809]  eta: 0:02:14  lr: 0.000007  min_lr: 0.000000  loss: 4.1724 (4.2016)  class_acc: 0.2917 (0.3180)  loss_scale: 65536.0000 (39229.8343)  weight_decay: 0.0500 (0.0500)  time: 0.5187  data: 0.0670  max mem: 15572
Epoch: [30]  [2580/2809]  eta: 0:02:09  lr: 0.000007  min_lr: 0.000000  loss: 4.0352 (4.2009)  class_acc: 0.3333 (0.3183)  loss_scale: 32768.0000 (39204.7981)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0581  max mem: 15572
Epoch: [30]  [2590/2809]  eta: 0:02:03  lr: 0.000007  min_lr: 0.000000  loss: 4.1354 (4.2009)  class_acc: 0.3750 (0.3185)  loss_scale: 32768.0000 (39179.9552)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0858  max mem: 15572
Epoch: [30]  [2600/2809]  eta: 0:01:58  lr: 0.000007  min_lr: 0.000000  loss: 4.2323 (4.2013)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (39155.3033)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1317  max mem: 15572
Epoch: [30]  [2610/2809]  eta: 0:01:52  lr: 0.000007  min_lr: 0.000000  loss: 4.2388 (4.2015)  class_acc: 0.2917 (0.3184)  loss_scale: 32768.0000 (39130.8403)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1200  max mem: 15572
Epoch: [30]  [2620/2809]  eta: 0:01:46  lr: 0.000007  min_lr: 0.000000  loss: 4.3632 (4.2023)  class_acc: 0.3333 (0.3184)  loss_scale: 32768.0000 (39106.5639)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0624  max mem: 15572
Epoch: [30]  [2630/2809]  eta: 0:01:41  lr: 0.000007  min_lr: 0.000000  loss: 4.3745 (4.2027)  class_acc: 0.2917 (0.3184)  loss_scale: 32768.0000 (39082.4721)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0810  max mem: 15572
Epoch: [30]  [2640/2809]  eta: 0:01:35  lr: 0.000007  min_lr: 0.000000  loss: 4.3142 (4.2031)  class_acc: 0.2083 (0.3180)  loss_scale: 32768.0000 (39058.5627)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1125  max mem: 15572
Epoch: [30]  [2650/2809]  eta: 0:01:29  lr: 0.000007  min_lr: 0.000000  loss: 4.2973 (4.2034)  class_acc: 0.2083 (0.3180)  loss_scale: 32768.0000 (39034.8336)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.1031  max mem: 15572
Epoch: [30]  [2660/2809]  eta: 0:01:24  lr: 0.000007  min_lr: 0.000000  loss: 4.2807 (4.2036)  class_acc: 0.2917 (0.3181)  loss_scale: 32768.0000 (39011.2830)  weight_decay: 0.0500 (0.0500)  time: 0.4610  data: 0.0423  max mem: 15572
Epoch: [30]  [2670/2809]  eta: 0:01:18  lr: 0.000007  min_lr: 0.000000  loss: 4.2378 (4.2037)  class_acc: 0.2917 (0.3179)  loss_scale: 32768.0000 (38987.9086)  weight_decay: 0.0500 (0.0500)  time: 0.4672  data: 0.0006  max mem: 15572
Epoch: [30]  [2680/2809]  eta: 0:01:12  lr: 0.000007  min_lr: 0.000000  loss: 4.1354 (4.2036)  class_acc: 0.2917 (0.3179)  loss_scale: 32768.0000 (38964.7087)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0793  max mem: 15572
Epoch: [30]  [2690/2809]  eta: 0:01:07  lr: 0.000007  min_lr: 0.000000  loss: 4.1034 (4.2032)  class_acc: 0.2917 (0.3179)  loss_scale: 32768.0000 (38941.6812)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.1492  max mem: 15572
[2025-01-16 05:04:05,079] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:04:05,079] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [2700/2809]  eta: 0:01:01  lr: 0.000007  min_lr: 0.000000  loss: 4.1056 (4.2031)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (39003.7468)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.1417  max mem: 15572
Epoch: [30]  [2710/2809]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 4.1859 (4.2029)  class_acc: 0.2917 (0.3178)  loss_scale: 65536.0000 (39101.6156)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0996  max mem: 15572
Epoch: [30]  [2720/2809]  eta: 0:00:50  lr: 0.000007  min_lr: 0.000000  loss: 4.1736 (4.2029)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (39198.7652)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1520  max mem: 15572
[2025-01-16 05:04:27,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=87000, skipped=540, lr=[7.09583022273898e-08, 7.09583022273898e-08, 1.0136900318198544e-07, 1.0136900318198544e-07, 1.4481286168855063e-07, 1.4481286168855063e-07, 2.068755166979295e-07, 2.068755166979295e-07, 2.955364524256136e-07, 2.955364524256136e-07, 4.2219493203659086e-07, 4.2219493203659086e-07, 6.031356171951298e-07, 6.031356171951298e-07, 8.61622310278757e-07, 8.61622310278757e-07, 1.2308890146839386e-06, 1.2308890146839386e-06, 1.7584128781199124e-06, 1.7584128781199124e-06, 2.5120183973141607e-06, 2.5120183973141607e-06, 3.588597710448801e-06, 3.588597710448801e-06, 5.1265681577840024e-06, 5.1265681577840024e-06, 7.323668796834289e-06, 7.323668796834289e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:04:27,609] [INFO] [timer.py:260:stop] epoch=0/micro_step=87000/global_step=87000, RunningAvgSamplesPerSec=27.93473015701704, CurrSamplesPerSec=25.86265442962459, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [30]  [2730/2809]  eta: 0:00:44  lr: 0.000007  min_lr: 0.000000  loss: 4.2467 (4.2033)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (39295.2032)  weight_decay: 0.0500 (0.0500)  time: 0.7171  data: 0.2617  max mem: 15572
Epoch: [30]  [2740/2809]  eta: 0:00:39  lr: 0.000007  min_lr: 0.000000  loss: 4.2261 (4.2026)  class_acc: 0.3333 (0.3182)  loss_scale: 65536.0000 (39390.9376)  weight_decay: 0.0500 (0.0500)  time: 0.7073  data: 0.2101  max mem: 15572
Epoch: [30]  [2750/2809]  eta: 0:00:33  lr: 0.000007  min_lr: 0.000000  loss: 4.0751 (4.2019)  class_acc: 0.3750 (0.3183)  loss_scale: 65536.0000 (39485.9760)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.1637  max mem: 15572
Epoch: [30]  [2760/2809]  eta: 0:00:27  lr: 0.000007  min_lr: 0.000000  loss: 4.2693 (4.2026)  class_acc: 0.3333 (0.3184)  loss_scale: 65536.0000 (39580.3260)  weight_decay: 0.0500 (0.0500)  time: 0.6480  data: 0.1853  max mem: 15572
Epoch: [30]  [2770/2809]  eta: 0:00:22  lr: 0.000007  min_lr: 0.000000  loss: 4.3063 (4.2026)  class_acc: 0.3333 (0.3185)  loss_scale: 65536.0000 (39673.9949)  weight_decay: 0.0500 (0.0500)  time: 0.6631  data: 0.1970  max mem: 15572
Epoch: [30]  [2780/2809]  eta: 0:00:16  lr: 0.000007  min_lr: 0.000000  loss: 4.2123 (4.2025)  class_acc: 0.2917 (0.3184)  loss_scale: 65536.0000 (39766.9903)  weight_decay: 0.0500 (0.0500)  time: 0.7005  data: 0.2107  max mem: 15572
Epoch: [30]  [2790/2809]  eta: 0:00:10  lr: 0.000007  min_lr: 0.000000  loss: 4.2396 (4.2028)  class_acc: 0.3333 (0.3186)  loss_scale: 65536.0000 (39859.3192)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1489  max mem: 15572
Epoch: [30]  [2800/2809]  eta: 0:00:05  lr: 0.000007  min_lr: 0.000000  loss: 4.1925 (4.2027)  class_acc: 0.3333 (0.3185)  loss_scale: 65536.0000 (39950.9889)  weight_decay: 0.0500 (0.0500)  time: 0.4577  data: 0.0415  max mem: 15572
Epoch: [30]  [2808/2809]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 4.1925 (4.2030)  class_acc: 0.2917 (0.3186)  loss_scale: 65536.0000 (40023.8548)  weight_decay: 0.0500 (0.0500)  time: 0.4309  data: 0.0414  max mem: 15572
Epoch: [30] Total time: 0:26:31 (0.5665 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 4.1925 (4.2030)  class_acc: 0.2917 (0.3186)  loss_scale: 65536.0000 (40023.8548)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:17:05  loss: 1.1631 (1.1631)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 3.7691  data: 3.5754  max mem: 15572
Val:  [ 10/272]  eta: 0:02:52  loss: 2.5135 (2.5172)  acc1: 50.0000 (45.9596)  acc5: 72.2222 (74.7475)  time: 0.6572  data: 0.4806  max mem: 15572
Val:  [ 20/272]  eta: 0:02:24  loss: 2.5135 (2.5310)  acc1: 50.0000 (48.6772)  acc5: 72.2222 (74.3386)  time: 0.4140  data: 0.2299  max mem: 15572
Val:  [ 30/272]  eta: 0:01:55  loss: 2.5902 (2.5779)  acc1: 50.0000 (45.1613)  acc5: 72.2222 (74.3728)  time: 0.3767  data: 0.1753  max mem: 15572
Val:  [ 40/272]  eta: 0:01:38  loss: 2.6215 (2.6243)  acc1: 27.7778 (41.7344)  acc5: 72.2222 (73.8482)  time: 0.2659  data: 0.0644  max mem: 15572
Val:  [ 50/272]  eta: 0:01:27  loss: 2.6017 (2.5624)  acc1: 33.3333 (42.5926)  acc5: 72.2222 (75.0545)  time: 0.2637  data: 0.0657  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 1.9399 (2.5013)  acc1: 61.1111 (44.6266)  acc5: 83.3333 (75.7741)  time: 0.2655  data: 0.0606  max mem: 15572
Val:  [ 70/272]  eta: 0:01:16  loss: 1.9687 (2.4443)  acc1: 61.1111 (46.9484)  acc5: 83.3333 (76.7606)  time: 0.3431  data: 0.1406  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 2.1876 (2.4546)  acc1: 50.0000 (46.7764)  acc5: 77.7778 (76.4060)  time: 0.3288  data: 0.1355  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 2.5115 (2.4666)  acc1: 44.4444 (46.7643)  acc5: 77.7778 (76.6178)  time: 0.2530  data: 0.0623  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.5649 (2.4900)  acc1: 38.8889 (45.7096)  acc5: 77.7778 (76.5127)  time: 0.3081  data: 0.0995  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.6520 (2.5415)  acc1: 27.7778 (44.3944)  acc5: 77.7778 (75.6256)  time: 0.2998  data: 0.1003  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 2.8908 (2.5749)  acc1: 27.7778 (43.5721)  acc5: 72.2222 (75.0689)  time: 0.2813  data: 0.1038  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 2.4523 (2.5444)  acc1: 44.4444 (44.5293)  acc5: 77.7778 (75.8694)  time: 0.3106  data: 0.1293  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 2.0909 (2.5421)  acc1: 55.5556 (44.9567)  acc5: 83.3333 (75.8471)  time: 0.3136  data: 0.1277  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 2.4963 (2.5363)  acc1: 44.4444 (44.5916)  acc5: 77.7778 (76.0854)  time: 0.2684  data: 0.0754  max mem: 15572
Val:  [160/272]  eta: 0:00:36  loss: 2.4963 (2.5349)  acc1: 44.4444 (44.6860)  acc5: 77.7778 (76.1905)  time: 0.2924  data: 0.0938  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.6313 (2.5525)  acc1: 38.8889 (44.0546)  acc5: 72.2222 (75.6335)  time: 0.3119  data: 0.1205  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.5202 (2.5398)  acc1: 38.8889 (44.1682)  acc5: 72.2222 (75.9362)  time: 0.3056  data: 0.1187  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.5107 (2.5786)  acc1: 33.3333 (42.9029)  acc5: 77.7778 (74.8109)  time: 0.3598  data: 0.1728  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.6134 (2.5819)  acc1: 33.3333 (42.3991)  acc5: 72.2222 (74.7098)  time: 0.3288  data: 0.1397  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.3944 (2.5866)  acc1: 33.3333 (42.3644)  acc5: 77.7778 (74.6446)  time: 0.2930  data: 0.1007  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.6389 (2.5788)  acc1: 44.4444 (42.6094)  acc5: 72.2222 (74.7109)  time: 0.3155  data: 0.1186  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.1916 (2.5628)  acc1: 50.0000 (43.4584)  acc5: 77.7778 (74.9880)  time: 0.3026  data: 0.1041  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1737 (2.5518)  acc1: 55.5556 (43.6146)  acc5: 77.7778 (75.2190)  time: 0.3090  data: 0.1212  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.5192 (2.5579)  acc1: 33.3333 (43.1164)  acc5: 77.7778 (75.1439)  time: 0.3214  data: 0.1367  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9869 (2.5172)  acc1: 55.5556 (44.5722)  acc5: 88.8889 (75.8195)  time: 0.2960  data: 0.1144  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.9427 (2.5120)  acc1: 61.1111 (44.6904)  acc5: 88.8889 (76.0353)  time: 0.2555  data: 0.0925  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.9427 (2.5156)  acc1: 61.1111 (44.6856)  acc5: 88.8889 (75.9984)  time: 0.2488  data: 0.0924  max mem: 15572
Val: Total time: 0:01:26 (0.3191 s / it)
* Acc@1 44.686 Acc@5 75.998 loss 2.516
Accuracy of the network on the 4883 val videos: 44.7%
Max accuracy: 44.85%
Epoch: [31]  [   0/2809]  eta: 5:31:32  lr: 0.000007  min_lr: 0.000000  loss: 4.6169 (4.6169)  class_acc: 0.1667 (0.1667)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.0818  data: 6.6434  max mem: 15572
[2025-01-16 05:06:50,383] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87081
[2025-01-16 05:06:50,384] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:06:50,384] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [  10/2809]  eta: 0:55:57  lr: 0.000007  min_lr: 0.000000  loss: 4.0227 (4.1714)  class_acc: 0.4167 (0.3485)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 1.1997  data: 0.7676  max mem: 15572
Epoch: [31]  [  20/2809]  eta: 0:42:47  lr: 0.000007  min_lr: 0.000000  loss: 4.0227 (4.2243)  class_acc: 0.3750 (0.3532)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1824  max mem: 15572
Epoch: [31]  [  30/2809]  eta: 0:36:22  lr: 0.000007  min_lr: 0.000000  loss: 4.1306 (4.2136)  class_acc: 0.3333 (0.3387)  loss_scale: 32768.0000 (34882.0645)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1227  max mem: 15572
Epoch: [31]  [  40/2809]  eta: 0:32:37  lr: 0.000007  min_lr: 0.000000  loss: 4.3829 (4.2685)  class_acc: 0.2500 (0.3272)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0306  max mem: 15572
Epoch: [31]  [  50/2809]  eta: 0:31:32  lr: 0.000007  min_lr: 0.000000  loss: 4.3829 (4.2526)  class_acc: 0.2500 (0.3235)  loss_scale: 32768.0000 (34053.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0772  max mem: 15572
Epoch: [31]  [  60/2809]  eta: 0:30:12  lr: 0.000007  min_lr: 0.000000  loss: 4.2103 (4.2315)  class_acc: 0.2917 (0.3251)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1172  max mem: 15572
Epoch: [31]  [  70/2809]  eta: 0:29:42  lr: 0.000007  min_lr: 0.000000  loss: 4.1070 (4.2198)  class_acc: 0.3750 (0.3322)  loss_scale: 32768.0000 (33691.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1102  max mem: 15572
Epoch: [31]  [  80/2809]  eta: 0:29:03  lr: 0.000007  min_lr: 0.000000  loss: 4.0765 (4.2117)  class_acc: 0.3750 (0.3385)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1176  max mem: 15572
Epoch: [31]  [  90/2809]  eta: 0:29:00  lr: 0.000007  min_lr: 0.000000  loss: 4.1488 (4.2127)  class_acc: 0.3750 (0.3388)  loss_scale: 32768.0000 (33488.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1521  max mem: 15572
Epoch: [31]  [ 100/2809]  eta: 0:28:21  lr: 0.000007  min_lr: 0.000000  loss: 4.2821 (4.2265)  class_acc: 0.2917 (0.3346)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1509  max mem: 15572
Epoch: [31]  [ 110/2809]  eta: 0:28:26  lr: 0.000007  min_lr: 0.000000  loss: 4.3879 (4.2346)  class_acc: 0.2500 (0.3247)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1648  max mem: 15572
Epoch: [31]  [ 120/2809]  eta: 0:27:59  lr: 0.000007  min_lr: 0.000000  loss: 4.2131 (4.2296)  class_acc: 0.2083 (0.3213)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1611  max mem: 15572
Epoch: [31]  [ 130/2809]  eta: 0:28:00  lr: 0.000007  min_lr: 0.000000  loss: 4.1755 (4.2222)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1534  max mem: 15572
[2025-01-16 05:08:03,578] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:08:03,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 140/2809]  eta: 0:27:38  lr: 0.000007  min_lr: 0.000000  loss: 4.2658 (4.2248)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1545  max mem: 15572
Epoch: [31]  [ 150/2809]  eta: 0:27:25  lr: 0.000007  min_lr: 0.000000  loss: 4.2822 (4.2279)  class_acc: 0.3333 (0.3253)  loss_scale: 65536.0000 (37542.1457)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1102  max mem: 15572
Epoch: [31]  [ 160/2809]  eta: 0:27:05  lr: 0.000007  min_lr: 0.000000  loss: 4.2536 (4.2308)  class_acc: 0.3333 (0.3274)  loss_scale: 65536.0000 (39280.8944)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1189  max mem: 15572
[2025-01-16 05:08:22,933] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87244
[2025-01-16 05:08:22,933] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:08:22,933] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [ 170/2809]  eta: 0:26:48  lr: 0.000007  min_lr: 0.000000  loss: 4.2381 (4.2297)  class_acc: 0.3333 (0.3246)  loss_scale: 65536.0000 (39666.5263)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.1058  max mem: 15572
Epoch: [31]  [ 180/2809]  eta: 0:26:21  lr: 0.000007  min_lr: 0.000000  loss: 4.1414 (4.2260)  class_acc: 0.2500 (0.3209)  loss_scale: 32768.0000 (39285.3923)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0534  max mem: 15572
Epoch: [31]  [ 190/2809]  eta: 0:26:10  lr: 0.000007  min_lr: 0.000000  loss: 4.2104 (4.2340)  class_acc: 0.2500 (0.3183)  loss_scale: 32768.0000 (38944.1675)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0482  max mem: 15572
Epoch: [31]  [ 200/2809]  eta: 0:26:00  lr: 0.000007  min_lr: 0.000000  loss: 4.2132 (4.2303)  class_acc: 0.3333 (0.3219)  loss_scale: 32768.0000 (38636.8955)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1165  max mem: 15572
Epoch: [31]  [ 210/2809]  eta: 0:25:45  lr: 0.000007  min_lr: 0.000000  loss: 4.1329 (4.2286)  class_acc: 0.3750 (0.3197)  loss_scale: 32768.0000 (38358.7488)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1199  max mem: 15572
Epoch: [31]  [ 220/2809]  eta: 0:25:37  lr: 0.000007  min_lr: 0.000000  loss: 4.1329 (4.2209)  class_acc: 0.3333 (0.3220)  loss_scale: 32768.0000 (38105.7738)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1164  max mem: 15572
Epoch: [31]  [ 230/2809]  eta: 0:25:27  lr: 0.000007  min_lr: 0.000000  loss: 4.2036 (4.2221)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (37874.7013)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1223  max mem: 15572
Epoch: [31]  [ 240/2809]  eta: 0:25:12  lr: 0.000007  min_lr: 0.000000  loss: 4.3120 (4.2246)  class_acc: 0.2917 (0.3207)  loss_scale: 32768.0000 (37662.8050)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0846  max mem: 15572
Epoch: [31]  [ 250/2809]  eta: 0:25:01  lr: 0.000007  min_lr: 0.000000  loss: 4.3484 (4.2287)  class_acc: 0.2917 (0.3206)  loss_scale: 32768.0000 (37467.7928)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0721  max mem: 15572
Epoch: [31]  [ 260/2809]  eta: 0:24:53  lr: 0.000007  min_lr: 0.000000  loss: 4.2962 (4.2314)  class_acc: 0.2500 (0.3201)  loss_scale: 32768.0000 (37287.7241)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1138  max mem: 15572
Epoch: [31]  [ 270/2809]  eta: 0:24:44  lr: 0.000007  min_lr: 0.000000  loss: 4.2023 (4.2241)  class_acc: 0.2917 (0.3212)  loss_scale: 32768.0000 (37120.9446)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1256  max mem: 15572
Epoch: [31]  [ 280/2809]  eta: 0:24:31  lr: 0.000007  min_lr: 0.000000  loss: 4.0369 (4.2179)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (36966.0356)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.1000  max mem: 15572
Epoch: [31]  [ 290/2809]  eta: 0:24:25  lr: 0.000007  min_lr: 0.000000  loss: 4.1855 (4.2217)  class_acc: 0.2917 (0.3213)  loss_scale: 32768.0000 (36821.7732)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.1113  max mem: 15572
[2025-01-16 05:09:32,168] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:09:32,169] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 300/2809]  eta: 0:24:23  lr: 0.000007  min_lr: 0.000000  loss: 4.1919 (4.2193)  class_acc: 0.2917 (0.3195)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1594  max mem: 15572
Epoch: [31]  [ 310/2809]  eta: 0:24:22  lr: 0.000007  min_lr: 0.000000  loss: 4.2452 (4.2210)  class_acc: 0.2917 (0.3210)  loss_scale: 65536.0000 (38352.2572)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1839  max mem: 15572
[2025-01-16 05:09:46,273] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87395
[2025-01-16 05:09:46,274] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:09:46,274] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [ 320/2809]  eta: 0:24:13  lr: 0.000007  min_lr: 0.000000  loss: 4.2305 (4.2180)  class_acc: 0.3333 (0.3214)  loss_scale: 65536.0000 (38688.6978)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1460  max mem: 15572
Epoch: [31]  [ 330/2809]  eta: 0:24:10  lr: 0.000007  min_lr: 0.000000  loss: 4.1528 (4.2133)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (38509.8248)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1455  max mem: 15572
Epoch: [31]  [ 340/2809]  eta: 0:24:02  lr: 0.000007  min_lr: 0.000000  loss: 4.1528 (4.2103)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (38341.4428)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1413  max mem: 15572
Epoch: [31]  [ 350/2809]  eta: 0:24:04  lr: 0.000007  min_lr: 0.000000  loss: 4.1730 (4.2134)  class_acc: 0.2500 (0.3199)  loss_scale: 32768.0000 (38182.6553)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1789  max mem: 15572
Epoch: [31]  [ 360/2809]  eta: 0:23:50  lr: 0.000007  min_lr: 0.000000  loss: 4.2462 (4.2098)  class_acc: 0.2500 (0.3209)  loss_scale: 32768.0000 (38032.6648)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1447  max mem: 15572
Epoch: [31]  [ 370/2809]  eta: 0:23:43  lr: 0.000007  min_lr: 0.000000  loss: 4.1639 (4.2086)  class_acc: 0.3333 (0.3203)  loss_scale: 32768.0000 (37890.7601)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0722  max mem: 15572
Epoch: [31]  [ 380/2809]  eta: 0:23:32  lr: 0.000007  min_lr: 0.000000  loss: 4.0983 (4.2039)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37756.3045)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0826  max mem: 15572
Epoch: [31]  [ 390/2809]  eta: 0:23:27  lr: 0.000007  min_lr: 0.000000  loss: 4.0213 (4.2014)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (37628.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1023  max mem: 15572
Epoch: [31]  [ 400/2809]  eta: 0:23:27  lr: 0.000007  min_lr: 0.000000  loss: 4.1044 (4.1979)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (37507.5112)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1890  max mem: 15572
Epoch: [31]  [ 410/2809]  eta: 0:23:21  lr: 0.000007  min_lr: 0.000000  loss: 4.2090 (4.1972)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (37392.1946)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1775  max mem: 15572
Epoch: [31]  [ 420/2809]  eta: 0:23:16  lr: 0.000007  min_lr: 0.000000  loss: 4.2292 (4.1988)  class_acc: 0.2500 (0.3213)  loss_scale: 32768.0000 (37282.3563)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1501  max mem: 15572
Epoch: [31]  [ 430/2809]  eta: 0:23:11  lr: 0.000007  min_lr: 0.000000  loss: 4.1077 (4.1930)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (37177.6148)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1680  max mem: 15572
Epoch: [31]  [ 440/2809]  eta: 0:23:03  lr: 0.000007  min_lr: 0.000000  loss: 4.0682 (4.1909)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (37077.6236)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1380  max mem: 15572
[2025-01-16 05:11:00,973] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:11:00,974] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 05:11:03,280] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87528
[2025-01-16 05:11:03,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:11:03,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [ 450/2809]  eta: 0:22:53  lr: 0.000007  min_lr: 0.000000  loss: 4.2907 (4.1925)  class_acc: 0.2500 (0.3216)  loss_scale: 32768.0000 (37272.6918)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0866  max mem: 15572
Epoch: [31]  [ 460/2809]  eta: 0:22:49  lr: 0.000007  min_lr: 0.000000  loss: 4.1333 (4.1906)  class_acc: 0.2917 (0.3210)  loss_scale: 32768.0000 (37174.9761)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1135  max mem: 15572
Epoch: [31]  [ 470/2809]  eta: 0:22:41  lr: 0.000007  min_lr: 0.000000  loss: 4.1210 (4.1904)  class_acc: 0.3333 (0.3217)  loss_scale: 32768.0000 (37081.4098)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1424  max mem: 15572
Epoch: [31]  [ 480/2809]  eta: 0:22:40  lr: 0.000007  min_lr: 0.000000  loss: 4.1776 (4.1904)  class_acc: 0.3333 (0.3217)  loss_scale: 32768.0000 (36991.7339)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1637  max mem: 15572
Epoch: [31]  [ 490/2809]  eta: 0:22:28  lr: 0.000007  min_lr: 0.000000  loss: 4.3491 (4.1935)  class_acc: 0.3333 (0.3220)  loss_scale: 32768.0000 (36905.7108)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1243  max mem: 15572
Epoch: [31]  [ 500/2809]  eta: 0:22:23  lr: 0.000007  min_lr: 0.000000  loss: 4.3791 (4.1956)  class_acc: 0.2500 (0.3201)  loss_scale: 32768.0000 (36823.1218)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0854  max mem: 15572
Epoch: [31]  [ 510/2809]  eta: 0:22:13  lr: 0.000007  min_lr: 0.000000  loss: 4.2774 (4.1948)  class_acc: 0.2917 (0.3207)  loss_scale: 32768.0000 (36743.7652)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0683  max mem: 15572
Epoch: [31]  [ 520/2809]  eta: 0:22:08  lr: 0.000007  min_lr: 0.000000  loss: 4.2447 (4.1940)  class_acc: 0.3750 (0.3219)  loss_scale: 32768.0000 (36667.4549)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0766  max mem: 15572
Epoch: [31]  [ 530/2809]  eta: 0:22:00  lr: 0.000007  min_lr: 0.000000  loss: 4.3152 (4.1954)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (36594.0188)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1216  max mem: 15572
Epoch: [31]  [ 540/2809]  eta: 0:21:52  lr: 0.000007  min_lr: 0.000000  loss: 4.2603 (4.1957)  class_acc: 0.2500 (0.3213)  loss_scale: 32768.0000 (36523.2976)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0987  max mem: 15572
Epoch: [31]  [ 550/2809]  eta: 0:21:45  lr: 0.000007  min_lr: 0.000000  loss: 4.2240 (4.1969)  class_acc: 0.2083 (0.3212)  loss_scale: 32768.0000 (36455.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.1027  max mem: 15572
Epoch: [31]  [ 560/2809]  eta: 0:21:39  lr: 0.000007  min_lr: 0.000000  loss: 4.2521 (4.1968)  class_acc: 0.3333 (0.3217)  loss_scale: 32768.0000 (36389.4189)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1204  max mem: 15572
Epoch: [31]  [ 570/2809]  eta: 0:21:34  lr: 0.000007  min_lr: 0.000000  loss: 4.1866 (4.1978)  class_acc: 0.2917 (0.3218)  loss_scale: 32768.0000 (36325.9965)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1402  max mem: 15572
[2025-01-16 05:12:14,870] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:12:14,871] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 580/2809]  eta: 0:21:23  lr: 0.000007  min_lr: 0.000000  loss: 4.0821 (4.1939)  class_acc: 0.4167 (0.3238)  loss_scale: 32768.0000 (36433.9552)  weight_decay: 0.0500 (0.0500)  time: 0.5208  data: 0.0731  max mem: 15572
Epoch: [31]  [ 590/2809]  eta: 0:21:16  lr: 0.000007  min_lr: 0.000000  loss: 4.0821 (4.1949)  class_acc: 0.3750 (0.3235)  loss_scale: 65536.0000 (36926.3756)  weight_decay: 0.0500 (0.0500)  time: 0.4912  data: 0.0473  max mem: 15572
Epoch: [31]  [ 600/2809]  eta: 0:21:12  lr: 0.000007  min_lr: 0.000000  loss: 4.1843 (4.1947)  class_acc: 0.3750 (0.3238)  loss_scale: 65536.0000 (37402.4093)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1372  max mem: 15572
[2025-01-16 05:12:28,535] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87682
[2025-01-16 05:12:28,536] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:12:28,536] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [ 610/2809]  eta: 0:21:05  lr: 0.000007  min_lr: 0.000000  loss: 4.1843 (4.1915)  class_acc: 0.2500 (0.3232)  loss_scale: 65536.0000 (37433.8200)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1466  max mem: 15572
Epoch: [31]  [ 620/2809]  eta: 0:20:59  lr: 0.000007  min_lr: 0.000000  loss: 3.9920 (4.1905)  class_acc: 0.2500 (0.3229)  loss_scale: 32768.0000 (37358.6860)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1235  max mem: 15572
Epoch: [31]  [ 630/2809]  eta: 0:20:56  lr: 0.000007  min_lr: 0.000000  loss: 4.3424 (4.1927)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (37285.9334)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1771  max mem: 15572
Epoch: [31]  [ 640/2809]  eta: 0:20:49  lr: 0.000007  min_lr: 0.000000  loss: 4.2663 (4.1931)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (37215.4509)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1579  max mem: 15572
Epoch: [31]  [ 650/2809]  eta: 0:20:43  lr: 0.000007  min_lr: 0.000000  loss: 4.2012 (4.1930)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (37147.1336)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1186  max mem: 15572
Epoch: [31]  [ 660/2809]  eta: 0:20:38  lr: 0.000007  min_lr: 0.000000  loss: 4.2469 (4.1954)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (37080.8835)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1504  max mem: 15572
Epoch: [31]  [ 670/2809]  eta: 0:20:32  lr: 0.000007  min_lr: 0.000000  loss: 4.2487 (4.1957)  class_acc: 0.2917 (0.3230)  loss_scale: 32768.0000 (37016.6080)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1472  max mem: 15572
Epoch: [31]  [ 680/2809]  eta: 0:20:28  lr: 0.000007  min_lr: 0.000000  loss: 4.2204 (4.1966)  class_acc: 0.2917 (0.3224)  loss_scale: 32768.0000 (36954.2203)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1624  max mem: 15572
Epoch: [31]  [ 690/2809]  eta: 0:20:24  lr: 0.000007  min_lr: 0.000000  loss: 4.2917 (4.1965)  class_acc: 0.2500 (0.3215)  loss_scale: 32768.0000 (36893.6382)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1885  max mem: 15572
Epoch: [31]  [ 700/2809]  eta: 0:20:16  lr: 0.000007  min_lr: 0.000000  loss: 4.2568 (4.1958)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (36834.7846)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1258  max mem: 15572
Epoch: [31]  [ 710/2809]  eta: 0:20:11  lr: 0.000007  min_lr: 0.000000  loss: 4.1547 (4.1959)  class_acc: 0.2917 (0.3217)  loss_scale: 32768.0000 (36777.5865)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0871  max mem: 15572
Epoch: [31]  [ 720/2809]  eta: 0:20:06  lr: 0.000007  min_lr: 0.000000  loss: 4.2031 (4.1972)  class_acc: 0.2500 (0.3210)  loss_scale: 32768.0000 (36721.9750)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1235  max mem: 15572
Epoch: [31]  [ 730/2809]  eta: 0:19:59  lr: 0.000007  min_lr: 0.000000  loss: 4.2031 (4.1966)  class_acc: 0.2917 (0.3209)  loss_scale: 32768.0000 (36667.8851)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1130  max mem: 15572
[2025-01-16 05:13:43,732] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:13:43,732] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 740/2809]  eta: 0:19:52  lr: 0.000007  min_lr: 0.000000  loss: 4.1624 (4.1950)  class_acc: 0.3333 (0.3209)  loss_scale: 32768.0000 (37013.2470)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0959  max mem: 15572
Epoch: [31]  [ 750/2809]  eta: 0:19:48  lr: 0.000007  min_lr: 0.000000  loss: 4.1836 (4.1966)  class_acc: 0.2917 (0.3201)  loss_scale: 65536.0000 (37393.0439)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1399  max mem: 15572
Epoch: [31]  [ 760/2809]  eta: 0:19:43  lr: 0.000007  min_lr: 0.000000  loss: 4.2011 (4.1963)  class_acc: 0.2917 (0.3201)  loss_scale: 65536.0000 (37762.8594)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1395  max mem: 15572
[2025-01-16 05:14:01,020] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87840
[2025-01-16 05:14:01,020] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:14:01,020] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [ 770/2809]  eta: 0:19:36  lr: 0.000007  min_lr: 0.000000  loss: 4.2004 (4.1964)  class_acc: 0.2917 (0.3204)  loss_scale: 32768.0000 (37698.0752)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1236  max mem: 15572
Epoch: [31]  [ 780/2809]  eta: 0:19:30  lr: 0.000007  min_lr: 0.000000  loss: 4.1951 (4.1976)  class_acc: 0.2917 (0.3208)  loss_scale: 32768.0000 (37634.9501)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1014  max mem: 15572
Epoch: [31]  [ 790/2809]  eta: 0:19:24  lr: 0.000007  min_lr: 0.000000  loss: 4.2716 (4.1981)  class_acc: 0.2917 (0.3208)  loss_scale: 32768.0000 (37573.4210)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0734  max mem: 15572
Epoch: [31]  [ 800/2809]  eta: 0:19:16  lr: 0.000007  min_lr: 0.000000  loss: 4.2716 (4.1996)  class_acc: 0.2500 (0.3200)  loss_scale: 32768.0000 (37513.4282)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0574  max mem: 15572
Epoch: [31]  [ 810/2809]  eta: 0:19:11  lr: 0.000007  min_lr: 0.000000  loss: 4.2381 (4.2007)  class_acc: 0.2500 (0.3198)  loss_scale: 32768.0000 (37454.9149)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1020  max mem: 15572
Epoch: [31]  [ 820/2809]  eta: 0:19:05  lr: 0.000007  min_lr: 0.000000  loss: 4.1888 (4.2003)  class_acc: 0.2917 (0.3200)  loss_scale: 32768.0000 (37397.8270)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1465  max mem: 15572
Epoch: [31]  [ 830/2809]  eta: 0:19:00  lr: 0.000007  min_lr: 0.000000  loss: 4.1742 (4.2003)  class_acc: 0.2500 (0.3196)  loss_scale: 32768.0000 (37342.1131)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1210  max mem: 15572
Epoch: [31]  [ 840/2809]  eta: 0:18:52  lr: 0.000007  min_lr: 0.000000  loss: 4.0743 (4.1980)  class_acc: 0.3750 (0.3208)  loss_scale: 32768.0000 (37287.7241)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.1012  max mem: 15572
Epoch: [31]  [ 850/2809]  eta: 0:18:46  lr: 0.000007  min_lr: 0.000000  loss: 4.1328 (4.1990)  class_acc: 0.2500 (0.3194)  loss_scale: 32768.0000 (37234.6134)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0852  max mem: 15572
Epoch: [31]  [ 860/2809]  eta: 0:18:41  lr: 0.000007  min_lr: 0.000000  loss: 4.2157 (4.1992)  class_acc: 0.2083 (0.3192)  loss_scale: 32768.0000 (37182.7364)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1315  max mem: 15572
Epoch: [31]  [ 870/2809]  eta: 0:18:37  lr: 0.000007  min_lr: 0.000000  loss: 4.2308 (4.1991)  class_acc: 0.3333 (0.3191)  loss_scale: 32768.0000 (37132.0505)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.1944  max mem: 15572
Epoch: [31]  [ 880/2809]  eta: 0:18:31  lr: 0.000007  min_lr: 0.000000  loss: 4.2393 (4.2008)  class_acc: 0.2917 (0.3188)  loss_scale: 32768.0000 (37082.5153)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1735  max mem: 15572
[2025-01-16 05:15:14,836] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:15:14,836] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 890/2809]  eta: 0:18:26  lr: 0.000007  min_lr: 0.000000  loss: 4.1975 (4.1989)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (37070.8687)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1091  max mem: 15572
Epoch: [31]  [ 900/2809]  eta: 0:18:17  lr: 0.000007  min_lr: 0.000000  loss: 4.1975 (4.2014)  class_acc: 0.2917 (0.3184)  loss_scale: 65536.0000 (37386.7969)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0606  max mem: 15572
Epoch: [31]  [ 910/2809]  eta: 0:18:10  lr: 0.000007  min_lr: 0.000000  loss: 4.3542 (4.2017)  class_acc: 0.2500 (0.3181)  loss_scale: 65536.0000 (37695.7892)  weight_decay: 0.0500 (0.0500)  time: 0.4897  data: 0.0503  max mem: 15572
[2025-01-16 05:15:25,470] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87991
[2025-01-16 05:15:25,470] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:15:25,470] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 05:15:29,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=88000, skipped=547, lr=[6.578664486896344e-08, 6.578664486896344e-08, 9.398092124137635e-08, 9.398092124137635e-08, 1.3425845891625195e-07, 1.3425845891625195e-07, 1.9179779845178852e-07, 1.9179779845178852e-07, 2.7399685493112645e-07, 2.7399685493112645e-07, 3.914240784730378e-07, 3.914240784730378e-07, 5.591772549614826e-07, 5.591772549614826e-07, 7.988246499449752e-07, 7.988246499449752e-07, 1.1411780713499645e-06, 1.1411780713499645e-06, 1.6302543876428069e-06, 1.6302543876428069e-06, 2.328934839489724e-06, 2.328934839489724e-06, 3.327049770699606e-06, 3.327049770699606e-06, 4.752928243856581e-06, 4.752928243856581e-06, 6.7898974912236865e-06, 6.7898974912236865e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:15:29,389] [INFO] [timer.py:260:stop] epoch=0/micro_step=88000/global_step=88000, RunningAvgSamplesPerSec=27.94008289802385, CurrSamplesPerSec=22.681701382848996, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [31]  [ 920/2809]  eta: 0:18:03  lr: 0.000007  min_lr: 0.000000  loss: 4.1869 (4.2016)  class_acc: 0.2083 (0.3175)  loss_scale: 65536.0000 (37677.8632)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0494  max mem: 15572
Epoch: [31]  [ 930/2809]  eta: 0:17:57  lr: 0.000007  min_lr: 0.000000  loss: 4.1562 (4.1999)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (37625.1257)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0666  max mem: 15572
Epoch: [31]  [ 940/2809]  eta: 0:17:51  lr: 0.000007  min_lr: 0.000000  loss: 4.1039 (4.1987)  class_acc: 0.3750 (0.3184)  loss_scale: 32768.0000 (37573.5090)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1060  max mem: 15572
Epoch: [31]  [ 950/2809]  eta: 0:17:43  lr: 0.000007  min_lr: 0.000000  loss: 4.2881 (4.1997)  class_acc: 0.3333 (0.3177)  loss_scale: 32768.0000 (37522.9779)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0668  max mem: 15572
Epoch: [31]  [ 960/2809]  eta: 0:17:38  lr: 0.000007  min_lr: 0.000000  loss: 4.2881 (4.1995)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (37473.4984)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0655  max mem: 15572
Epoch: [31]  [ 970/2809]  eta: 0:17:31  lr: 0.000007  min_lr: 0.000000  loss: 4.1273 (4.1984)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (37425.0381)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0982  max mem: 15572
Epoch: [31]  [ 980/2809]  eta: 0:17:24  lr: 0.000007  min_lr: 0.000000  loss: 4.0455 (4.1973)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (37377.5657)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0593  max mem: 15572
Epoch: [31]  [ 990/2809]  eta: 0:17:19  lr: 0.000007  min_lr: 0.000000  loss: 4.1418 (4.1976)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (37331.0515)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0868  max mem: 15572
Epoch: [31]  [1000/2809]  eta: 0:17:14  lr: 0.000007  min_lr: 0.000000  loss: 4.1541 (4.1965)  class_acc: 0.3333 (0.3188)  loss_scale: 32768.0000 (37285.4665)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1891  max mem: 15572
Epoch: [31]  [1010/2809]  eta: 0:17:12  lr: 0.000007  min_lr: 0.000000  loss: 4.1869 (4.1972)  class_acc: 0.3750 (0.3194)  loss_scale: 32768.0000 (37240.7834)  weight_decay: 0.0500 (0.0500)  time: 0.6963  data: 0.2681  max mem: 15572
Epoch: [31]  [1020/2809]  eta: 0:17:04  lr: 0.000007  min_lr: 0.000000  loss: 4.2994 (4.1989)  class_acc: 0.3750 (0.3195)  loss_scale: 32768.0000 (37196.9755)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1532  max mem: 15572
Epoch: [31]  [1030/2809]  eta: 0:16:59  lr: 0.000007  min_lr: 0.000000  loss: 4.2026 (4.1977)  class_acc: 0.3333 (0.3195)  loss_scale: 32768.0000 (37154.0175)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0810  max mem: 15572
Epoch: [31]  [1040/2809]  eta: 0:16:52  lr: 0.000007  min_lr: 0.000000  loss: 4.1214 (4.1976)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (37111.8847)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1222  max mem: 15572
[2025-01-16 05:16:37,878] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:16:37,879] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1050/2809]  eta: 0:16:47  lr: 0.000007  min_lr: 0.000000  loss: 4.1214 (4.1964)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (37382.3330)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1180  max mem: 15572
[2025-01-16 05:16:45,243] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88133
[2025-01-16 05:16:45,244] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:16:45,244] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1060/2809]  eta: 0:16:40  lr: 0.000007  min_lr: 0.000000  loss: 3.9403 (4.1955)  class_acc: 0.3333 (0.3194)  loss_scale: 65536.0000 (37431.4948)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1064  max mem: 15572
Epoch: [31]  [1070/2809]  eta: 0:16:36  lr: 0.000007  min_lr: 0.000000  loss: 4.1502 (4.1960)  class_acc: 0.2917 (0.3195)  loss_scale: 32768.0000 (37387.9514)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1203  max mem: 15572
Epoch: [31]  [1080/2809]  eta: 0:16:29  lr: 0.000007  min_lr: 0.000000  loss: 4.1830 (4.1960)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (37345.2137)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1354  max mem: 15572
Epoch: [31]  [1090/2809]  eta: 0:16:24  lr: 0.000007  min_lr: 0.000000  loss: 4.2443 (4.1962)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (37303.2594)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1114  max mem: 15572
Epoch: [31]  [1100/2809]  eta: 0:16:18  lr: 0.000007  min_lr: 0.000000  loss: 4.2443 (4.1957)  class_acc: 0.3333 (0.3204)  loss_scale: 32768.0000 (37262.0672)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1235  max mem: 15572
Epoch: [31]  [1110/2809]  eta: 0:16:12  lr: 0.000007  min_lr: 0.000000  loss: 4.3824 (4.1980)  class_acc: 0.3333 (0.3201)  loss_scale: 32768.0000 (37221.6166)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1319  max mem: 15572
Epoch: [31]  [1120/2809]  eta: 0:16:06  lr: 0.000007  min_lr: 0.000000  loss: 4.3824 (4.1986)  class_acc: 0.2917 (0.3193)  loss_scale: 32768.0000 (37181.8876)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1442  max mem: 15572
Epoch: [31]  [1130/2809]  eta: 0:16:00  lr: 0.000007  min_lr: 0.000000  loss: 4.1421 (4.1973)  class_acc: 0.2917 (0.3199)  loss_scale: 32768.0000 (37142.8612)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1222  max mem: 15572
Epoch: [31]  [1140/2809]  eta: 0:15:53  lr: 0.000007  min_lr: 0.000000  loss: 4.1144 (4.1965)  class_acc: 0.3333 (0.3201)  loss_scale: 32768.0000 (37104.5188)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0619  max mem: 15572
Epoch: [31]  [1150/2809]  eta: 0:15:47  lr: 0.000007  min_lr: 0.000000  loss: 4.0511 (4.1938)  class_acc: 0.3333 (0.3203)  loss_scale: 32768.0000 (37066.8427)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0627  max mem: 15572
Epoch: [31]  [1160/2809]  eta: 0:15:41  lr: 0.000007  min_lr: 0.000000  loss: 4.1827 (4.1950)  class_acc: 0.3333 (0.3206)  loss_scale: 32768.0000 (37029.8157)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1070  max mem: 15572
Epoch: [31]  [1170/2809]  eta: 0:15:36  lr: 0.000007  min_lr: 0.000000  loss: 4.2381 (4.1946)  class_acc: 0.3333 (0.3208)  loss_scale: 32768.0000 (36993.4210)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1400  max mem: 15572
Epoch: [31]  [1180/2809]  eta: 0:15:31  lr: 0.000007  min_lr: 0.000000  loss: 4.1479 (4.1946)  class_acc: 0.3750 (0.3214)  loss_scale: 32768.0000 (36957.6427)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1778  max mem: 15572
[2025-01-16 05:17:58,448] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:17:58,448] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1190/2809]  eta: 0:15:25  lr: 0.000007  min_lr: 0.000000  loss: 4.1927 (4.1958)  class_acc: 0.3333 (0.3212)  loss_scale: 32768.0000 (37142.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1229  max mem: 15572
Epoch: [31]  [1200/2809]  eta: 0:15:18  lr: 0.000007  min_lr: 0.000000  loss: 4.1954 (4.1948)  class_acc: 0.3333 (0.3213)  loss_scale: 65536.0000 (37378.9842)  weight_decay: 0.0500 (0.0500)  time: 0.5113  data: 0.0486  max mem: 15572
Epoch: [31]  [1210/2809]  eta: 0:15:13  lr: 0.000007  min_lr: 0.000000  loss: 4.1954 (4.1960)  class_acc: 0.3333 (0.3212)  loss_scale: 65536.0000 (37611.4946)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0984  max mem: 15572
[2025-01-16 05:18:15,837] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88295
[2025-01-16 05:18:15,838] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:18:15,838] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1220/2809]  eta: 0:15:08  lr: 0.000007  min_lr: 0.000000  loss: 4.1897 (4.1955)  class_acc: 0.3333 (0.3217)  loss_scale: 65536.0000 (37706.0115)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1684  max mem: 15572
Epoch: [31]  [1230/2809]  eta: 0:15:02  lr: 0.000007  min_lr: 0.000000  loss: 4.2643 (4.1960)  class_acc: 0.3333 (0.3220)  loss_scale: 32768.0000 (37665.8976)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1549  max mem: 15572
Epoch: [31]  [1240/2809]  eta: 0:14:57  lr: 0.000007  min_lr: 0.000000  loss: 4.1638 (4.1953)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (37626.4303)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1624  max mem: 15572
Epoch: [31]  [1250/2809]  eta: 0:14:51  lr: 0.000007  min_lr: 0.000000  loss: 4.0770 (4.1942)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (37587.5939)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1624  max mem: 15572
Epoch: [31]  [1260/2809]  eta: 0:14:45  lr: 0.000007  min_lr: 0.000000  loss: 4.0116 (4.1934)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (37549.3735)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1110  max mem: 15572
Epoch: [31]  [1270/2809]  eta: 0:14:38  lr: 0.000007  min_lr: 0.000000  loss: 4.1607 (4.1934)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (37511.7545)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0737  max mem: 15572
Epoch: [31]  [1280/2809]  eta: 0:14:32  lr: 0.000007  min_lr: 0.000000  loss: 4.1796 (4.1934)  class_acc: 0.2500 (0.3223)  loss_scale: 32768.0000 (37474.7229)  weight_decay: 0.0500 (0.0500)  time: 0.5040  data: 0.0515  max mem: 15572
Epoch: [31]  [1290/2809]  eta: 0:14:27  lr: 0.000007  min_lr: 0.000000  loss: 4.2090 (4.1933)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (37438.2649)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0880  max mem: 15572
Epoch: [31]  [1300/2809]  eta: 0:14:20  lr: 0.000007  min_lr: 0.000000  loss: 4.1998 (4.1937)  class_acc: 0.2917 (0.3218)  loss_scale: 32768.0000 (37402.3674)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1039  max mem: 15572
Epoch: [31]  [1310/2809]  eta: 0:14:15  lr: 0.000007  min_lr: 0.000000  loss: 4.1616 (4.1944)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (37367.0175)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0966  max mem: 15572
Epoch: [31]  [1320/2809]  eta: 0:14:09  lr: 0.000007  min_lr: 0.000000  loss: 4.2066 (4.1954)  class_acc: 0.2500 (0.3213)  loss_scale: 32768.0000 (37332.2029)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1132  max mem: 15572
Epoch: [31]  [1330/2809]  eta: 0:14:04  lr: 0.000007  min_lr: 0.000000  loss: 4.2066 (4.1956)  class_acc: 0.2500 (0.3209)  loss_scale: 32768.0000 (37297.9113)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1231  max mem: 15572
Epoch: [31]  [1340/2809]  eta: 0:13:59  lr: 0.000007  min_lr: 0.000000  loss: 4.1694 (4.1963)  class_acc: 0.2500 (0.3212)  loss_scale: 32768.0000 (37264.1312)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1635  max mem: 15572
[2025-01-16 05:19:32,364] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:19:32,364] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1350/2809]  eta: 0:13:54  lr: 0.000007  min_lr: 0.000000  loss: 4.1694 (4.1966)  class_acc: 0.2500 (0.3210)  loss_scale: 32768.0000 (37376.3790)  weight_decay: 0.0500 (0.0500)  time: 0.6502  data: 0.2233  max mem: 15572
Epoch: [31]  [1360/2809]  eta: 0:13:47  lr: 0.000007  min_lr: 0.000000  loss: 4.2866 (4.1973)  class_acc: 0.2500 (0.3208)  loss_scale: 65536.0000 (37583.2829)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1406  max mem: 15572
[2025-01-16 05:19:39,386] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88440
[2025-01-16 05:19:39,387] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:19:39,387] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1370/2809]  eta: 0:13:42  lr: 0.000007  min_lr: 0.000000  loss: 4.2772 (4.1979)  class_acc: 0.2917 (0.3212)  loss_scale: 32768.0000 (37548.1605)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0980  max mem: 15572
Epoch: [31]  [1380/2809]  eta: 0:13:36  lr: 0.000007  min_lr: 0.000000  loss: 4.2972 (4.1979)  class_acc: 0.3750 (0.3216)  loss_scale: 32768.0000 (37513.5467)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1210  max mem: 15572
Epoch: [31]  [1390/2809]  eta: 0:13:30  lr: 0.000007  min_lr: 0.000000  loss: 4.2171 (4.1981)  class_acc: 0.4167 (0.3221)  loss_scale: 32768.0000 (37479.4306)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0904  max mem: 15572
Epoch: [31]  [1400/2809]  eta: 0:13:24  lr: 0.000007  min_lr: 0.000000  loss: 4.2045 (4.1979)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (37445.8016)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.1029  max mem: 15572
Epoch: [31]  [1410/2809]  eta: 0:13:18  lr: 0.000007  min_lr: 0.000000  loss: 4.0407 (4.1966)  class_acc: 0.2500 (0.3213)  loss_scale: 32768.0000 (37412.6492)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1141  max mem: 15572
Epoch: [31]  [1420/2809]  eta: 0:13:11  lr: 0.000007  min_lr: 0.000000  loss: 4.1349 (4.1967)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (37379.9634)  weight_decay: 0.0500 (0.0500)  time: 0.5231  data: 0.0678  max mem: 15572
Epoch: [31]  [1430/2809]  eta: 0:13:06  lr: 0.000007  min_lr: 0.000000  loss: 4.2180 (4.1971)  class_acc: 0.2500 (0.3209)  loss_scale: 32768.0000 (37347.7345)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0628  max mem: 15572
Epoch: [31]  [1440/2809]  eta: 0:13:00  lr: 0.000007  min_lr: 0.000000  loss: 4.2043 (4.1960)  class_acc: 0.2500 (0.3206)  loss_scale: 32768.0000 (37315.9528)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1348  max mem: 15572
Epoch: [31]  [1450/2809]  eta: 0:12:54  lr: 0.000007  min_lr: 0.000000  loss: 4.2646 (4.1971)  class_acc: 0.2917 (0.3204)  loss_scale: 32768.0000 (37284.6092)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0917  max mem: 15572
Epoch: [31]  [1460/2809]  eta: 0:12:49  lr: 0.000007  min_lr: 0.000000  loss: 4.2833 (4.1975)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (37253.6947)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1074  max mem: 15572
Epoch: [31]  [1470/2809]  eta: 0:12:42  lr: 0.000007  min_lr: 0.000000  loss: 4.0863 (4.1965)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (37223.2005)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1166  max mem: 15572
Epoch: [31]  [1480/2809]  eta: 0:12:36  lr: 0.000006  min_lr: 0.000000  loss: 4.1764 (4.1974)  class_acc: 0.2917 (0.3198)  loss_scale: 32768.0000 (37193.1182)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0642  max mem: 15572
[2025-01-16 05:20:49,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:20:49,763] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1490/2809]  eta: 0:12:30  lr: 0.000006  min_lr: 0.000000  loss: 4.2394 (4.1976)  class_acc: 0.2917 (0.3200)  loss_scale: 32768.0000 (37185.4165)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0842  max mem: 15572
[2025-01-16 05:20:54,232] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88577
[2025-01-16 05:20:54,232] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:20:54,232] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1500/2809]  eta: 0:12:24  lr: 0.000006  min_lr: 0.000000  loss: 4.1866 (4.1974)  class_acc: 0.2917 (0.3200)  loss_scale: 32768.0000 (37308.8021)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0882  max mem: 15572
Epoch: [31]  [1510/2809]  eta: 0:12:18  lr: 0.000006  min_lr: 0.000000  loss: 4.1396 (4.1974)  class_acc: 0.2917 (0.3199)  loss_scale: 32768.0000 (37278.7505)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0904  max mem: 15572
Epoch: [31]  [1520/2809]  eta: 0:12:13  lr: 0.000006  min_lr: 0.000000  loss: 4.2022 (4.1975)  class_acc: 0.2917 (0.3198)  loss_scale: 32768.0000 (37249.0940)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1702  max mem: 15572
Epoch: [31]  [1530/2809]  eta: 0:12:07  lr: 0.000006  min_lr: 0.000000  loss: 4.2022 (4.1971)  class_acc: 0.2917 (0.3197)  loss_scale: 32768.0000 (37219.8250)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1874  max mem: 15572
Epoch: [31]  [1540/2809]  eta: 0:12:02  lr: 0.000006  min_lr: 0.000000  loss: 4.2280 (4.1981)  class_acc: 0.2500 (0.3191)  loss_scale: 32768.0000 (37190.9358)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1355  max mem: 15572
Epoch: [31]  [1550/2809]  eta: 0:11:57  lr: 0.000006  min_lr: 0.000000  loss: 4.2160 (4.1977)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (37162.4191)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.1683  max mem: 15572
Epoch: [31]  [1560/2809]  eta: 0:11:51  lr: 0.000006  min_lr: 0.000000  loss: 4.1430 (4.1978)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (37134.2678)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1399  max mem: 15572
Epoch: [31]  [1570/2809]  eta: 0:11:45  lr: 0.000006  min_lr: 0.000000  loss: 4.2145 (4.1979)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (37106.4749)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0935  max mem: 15572
Epoch: [31]  [1580/2809]  eta: 0:11:40  lr: 0.000006  min_lr: 0.000000  loss: 4.2145 (4.1983)  class_acc: 0.2917 (0.3189)  loss_scale: 32768.0000 (37079.0335)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1531  max mem: 15572
Epoch: [31]  [1590/2809]  eta: 0:11:35  lr: 0.000006  min_lr: 0.000000  loss: 4.2132 (4.1981)  class_acc: 0.2917 (0.3187)  loss_scale: 32768.0000 (37051.9371)  weight_decay: 0.0500 (0.0500)  time: 0.6594  data: 0.1979  max mem: 15572
Epoch: [31]  [1600/2809]  eta: 0:11:29  lr: 0.000006  min_lr: 0.000000  loss: 4.1903 (4.1983)  class_acc: 0.2917 (0.3187)  loss_scale: 32768.0000 (37025.1793)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1449  max mem: 15572
Epoch: [31]  [1610/2809]  eta: 0:11:23  lr: 0.000006  min_lr: 0.000000  loss: 4.2217 (4.1987)  class_acc: 0.2917 (0.3186)  loss_scale: 32768.0000 (36998.7536)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.1003  max mem: 15572
Epoch: [31]  [1620/2809]  eta: 0:11:17  lr: 0.000006  min_lr: 0.000000  loss: 4.1960 (4.1983)  class_acc: 0.3333 (0.3187)  loss_scale: 32768.0000 (36972.6539)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0690  max mem: 15572
[2025-01-16 05:22:10,067] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:22:10,068] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1630/2809]  eta: 0:11:12  lr: 0.000006  min_lr: 0.000000  loss: 4.0716 (4.1980)  class_acc: 0.3333 (0.3187)  loss_scale: 32768.0000 (37027.2373)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0989  max mem: 15572
[2025-01-16 05:22:12,728] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88712
[2025-01-16 05:22:12,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:22:12,728] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1640/2809]  eta: 0:11:05  lr: 0.000006  min_lr: 0.000000  loss: 4.2358 (4.1979)  class_acc: 0.2917 (0.3188)  loss_scale: 32768.0000 (37041.2188)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1080  max mem: 15572
Epoch: [31]  [1650/2809]  eta: 0:11:00  lr: 0.000006  min_lr: 0.000000  loss: 4.2657 (4.1985)  class_acc: 0.2500 (0.3184)  loss_scale: 32768.0000 (37015.3362)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0868  max mem: 15572
Epoch: [31]  [1660/2809]  eta: 0:10:54  lr: 0.000006  min_lr: 0.000000  loss: 4.2604 (4.1983)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (36989.7652)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0902  max mem: 15572
Epoch: [31]  [1670/2809]  eta: 0:10:48  lr: 0.000006  min_lr: 0.000000  loss: 4.1411 (4.1978)  class_acc: 0.3750 (0.3189)  loss_scale: 32768.0000 (36964.5003)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0589  max mem: 15572
Epoch: [31]  [1680/2809]  eta: 0:10:42  lr: 0.000006  min_lr: 0.000000  loss: 4.2266 (4.1984)  class_acc: 0.3750 (0.3193)  loss_scale: 32768.0000 (36939.5360)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1215  max mem: 15572
Epoch: [31]  [1690/2809]  eta: 0:10:37  lr: 0.000006  min_lr: 0.000000  loss: 4.2602 (4.1991)  class_acc: 0.4167 (0.3195)  loss_scale: 32768.0000 (36914.8669)  weight_decay: 0.0500 (0.0500)  time: 0.6454  data: 0.1909  max mem: 15572
Epoch: [31]  [1700/2809]  eta: 0:10:31  lr: 0.000006  min_lr: 0.000000  loss: 4.2524 (4.1992)  class_acc: 0.3750 (0.3193)  loss_scale: 32768.0000 (36890.4879)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1265  max mem: 15572
Epoch: [31]  [1710/2809]  eta: 0:10:25  lr: 0.000006  min_lr: 0.000000  loss: 4.1031 (4.1983)  class_acc: 0.3333 (0.3196)  loss_scale: 32768.0000 (36866.3939)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0488  max mem: 15572
Epoch: [31]  [1720/2809]  eta: 0:10:19  lr: 0.000006  min_lr: 0.000000  loss: 4.0807 (4.1982)  class_acc: 0.3333 (0.3195)  loss_scale: 32768.0000 (36842.5799)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0488  max mem: 15572
Epoch: [31]  [1730/2809]  eta: 0:10:13  lr: 0.000006  min_lr: 0.000000  loss: 4.0874 (4.1983)  class_acc: 0.3333 (0.3197)  loss_scale: 32768.0000 (36819.0410)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0712  max mem: 15572
Epoch: [31]  [1740/2809]  eta: 0:10:08  lr: 0.000006  min_lr: 0.000000  loss: 4.0874 (4.1973)  class_acc: 0.3750 (0.3197)  loss_scale: 32768.0000 (36795.7725)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1662  max mem: 15572
Epoch: [31]  [1750/2809]  eta: 0:10:02  lr: 0.000006  min_lr: 0.000000  loss: 4.2251 (4.1976)  class_acc: 0.3750 (0.3200)  loss_scale: 32768.0000 (36772.7698)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.1924  max mem: 15572
Epoch: [31]  [1760/2809]  eta: 0:09:57  lr: 0.000006  min_lr: 0.000000  loss: 4.3146 (4.1989)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (36750.0284)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1483  max mem: 15572
[2025-01-16 05:23:25,263] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:23:25,263] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1770/2809]  eta: 0:09:51  lr: 0.000006  min_lr: 0.000000  loss: 4.1704 (4.1980)  class_acc: 0.3333 (0.3201)  loss_scale: 32768.0000 (36894.0666)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1326  max mem: 15572
Epoch: [31]  [1780/2809]  eta: 0:09:45  lr: 0.000006  min_lr: 0.000000  loss: 4.1717 (4.1982)  class_acc: 0.3333 (0.3199)  loss_scale: 65536.0000 (37054.8860)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0934  max mem: 15572
Epoch: [31]  [1790/2809]  eta: 0:09:40  lr: 0.000006  min_lr: 0.000000  loss: 4.3425 (4.1988)  class_acc: 0.2917 (0.3196)  loss_scale: 65536.0000 (37213.9095)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1055  max mem: 15572
Epoch: [31]  [1800/2809]  eta: 0:09:33  lr: 0.000006  min_lr: 0.000000  loss: 4.3553 (4.1998)  class_acc: 0.2917 (0.3195)  loss_scale: 65536.0000 (37371.1671)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0694  max mem: 15572
Epoch: [31]  [1810/2809]  eta: 0:09:28  lr: 0.000006  min_lr: 0.000000  loss: 4.3577 (4.1999)  class_acc: 0.2917 (0.3196)  loss_scale: 65536.0000 (37526.6880)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0356  max mem: 15572
Epoch: [31]  [1820/2809]  eta: 0:09:22  lr: 0.000006  min_lr: 0.000000  loss: 4.1129 (4.1994)  class_acc: 0.3333 (0.3198)  loss_scale: 65536.0000 (37680.5008)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1402  max mem: 15572
[2025-01-16 05:24:03,681] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88909
[2025-01-16 05:24:03,682] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:24:03,682] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1830/2809]  eta: 0:09:17  lr: 0.000006  min_lr: 0.000000  loss: 4.1117 (4.1989)  class_acc: 0.3333 (0.3200)  loss_scale: 65536.0000 (37814.7373)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1647  max mem: 15572
Epoch: [31]  [1840/2809]  eta: 0:09:11  lr: 0.000006  min_lr: 0.000000  loss: 4.1656 (4.1986)  class_acc: 0.3333 (0.3201)  loss_scale: 32768.0000 (37787.3243)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1201  max mem: 15572
Epoch: [31]  [1850/2809]  eta: 0:09:05  lr: 0.000006  min_lr: 0.000000  loss: 4.2900 (4.1997)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (37760.2075)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0927  max mem: 15572
Epoch: [31]  [1860/2809]  eta: 0:08:59  lr: 0.000006  min_lr: 0.000000  loss: 4.3156 (4.1994)  class_acc: 0.2917 (0.3201)  loss_scale: 32768.0000 (37733.3821)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0971  max mem: 15572
Epoch: [31]  [1870/2809]  eta: 0:08:54  lr: 0.000006  min_lr: 0.000000  loss: 4.1865 (4.1991)  class_acc: 0.2917 (0.3205)  loss_scale: 32768.0000 (37706.8434)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1366  max mem: 15572
Epoch: [31]  [1880/2809]  eta: 0:08:48  lr: 0.000006  min_lr: 0.000000  loss: 4.1713 (4.1987)  class_acc: 0.2917 (0.3206)  loss_scale: 32768.0000 (37680.5869)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1839  max mem: 15572
Epoch: [31]  [1890/2809]  eta: 0:08:43  lr: 0.000006  min_lr: 0.000000  loss: 4.1713 (4.1985)  class_acc: 0.2917 (0.3205)  loss_scale: 32768.0000 (37654.6081)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1808  max mem: 15572
Epoch: [31]  [1900/2809]  eta: 0:08:37  lr: 0.000006  min_lr: 0.000000  loss: 4.1864 (4.1984)  class_acc: 0.2500 (0.3205)  loss_scale: 32768.0000 (37628.9027)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0880  max mem: 15572
Epoch: [31]  [1910/2809]  eta: 0:08:31  lr: 0.000006  min_lr: 0.000000  loss: 4.2138 (4.1986)  class_acc: 0.2500 (0.3206)  loss_scale: 32768.0000 (37603.4662)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0399  max mem: 15572
[2025-01-16 05:24:53,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=89000, skipped=553, lr=[6.077990261635452e-08, 6.077990261635452e-08, 8.68284323090779e-08, 8.68284323090779e-08, 1.24040617584397e-07, 1.24040617584397e-07, 1.772008822634243e-07, 1.772008822634243e-07, 2.531441175191776e-07, 2.531441175191776e-07, 3.6163445359882513e-07, 3.6163445359882513e-07, 5.166206479983216e-07, 5.166206479983216e-07, 7.380294971404596e-07, 7.380294971404596e-07, 1.0543278530577993e-06, 1.0543278530577993e-06, 1.506182647225428e-06, 1.506182647225428e-06, 2.1516894960363255e-06, 2.1516894960363255e-06, 3.073842137194751e-06, 3.073842137194751e-06, 4.391203053135359e-06, 4.391203053135359e-06, 6.273147218764799e-06, 6.273147218764799e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:24:53,279] [INFO] [timer.py:260:stop] epoch=0/micro_step=89000/global_step=89000, RunningAvgSamplesPerSec=27.94394167783316, CurrSamplesPerSec=31.74548936154109, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [31]  [1920/2809]  eta: 0:08:25  lr: 0.000006  min_lr: 0.000000  loss: 4.2443 (4.1993)  class_acc: 0.2917 (0.3203)  loss_scale: 32768.0000 (37578.2946)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0208  max mem: 15572
[2025-01-16 05:24:57,384] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89008
[2025-01-16 05:24:57,385] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 05:24:57,386] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [1930/2809]  eta: 0:08:19  lr: 0.000006  min_lr: 0.000000  loss: 4.2992 (4.1995)  class_acc: 0.2083 (0.3203)  loss_scale: 32768.0000 (37536.4143)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0414  max mem: 15572
Epoch: [31]  [1940/2809]  eta: 0:08:13  lr: 0.000006  min_lr: 0.000000  loss: 4.2081 (4.1986)  class_acc: 0.2917 (0.3202)  loss_scale: 16384.0000 (37427.4374)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0909  max mem: 15572
Epoch: [31]  [1950/2809]  eta: 0:08:08  lr: 0.000006  min_lr: 0.000000  loss: 4.1671 (4.1990)  class_acc: 0.3333 (0.3202)  loss_scale: 16384.0000 (37319.5777)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.1916  max mem: 15572
Epoch: [31]  [1960/2809]  eta: 0:08:02  lr: 0.000006  min_lr: 0.000000  loss: 4.2008 (4.1992)  class_acc: 0.2917 (0.3201)  loss_scale: 16384.0000 (37212.8180)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1426  max mem: 15572
Epoch: [31]  [1970/2809]  eta: 0:07:56  lr: 0.000006  min_lr: 0.000000  loss: 4.2312 (4.1985)  class_acc: 0.2917 (0.3202)  loss_scale: 16384.0000 (37107.1416)  weight_decay: 0.0500 (0.0500)  time: 0.4670  data: 0.0011  max mem: 15572
Epoch: [31]  [1980/2809]  eta: 0:07:50  lr: 0.000006  min_lr: 0.000000  loss: 4.4191 (4.1994)  class_acc: 0.3333 (0.3203)  loss_scale: 16384.0000 (37002.5321)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0777  max mem: 15572
Epoch: [31]  [1990/2809]  eta: 0:07:44  lr: 0.000006  min_lr: 0.000000  loss: 4.3515 (4.1991)  class_acc: 0.3333 (0.3203)  loss_scale: 16384.0000 (36898.9734)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1025  max mem: 15572
Epoch: [31]  [2000/2809]  eta: 0:07:39  lr: 0.000006  min_lr: 0.000000  loss: 4.2601 (4.1991)  class_acc: 0.2917 (0.3204)  loss_scale: 16384.0000 (36796.4498)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0833  max mem: 15572
Epoch: [31]  [2010/2809]  eta: 0:07:33  lr: 0.000006  min_lr: 0.000000  loss: 4.1005 (4.1981)  class_acc: 0.2917 (0.3204)  loss_scale: 16384.0000 (36694.9458)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1188  max mem: 15572
Epoch: [31]  [2020/2809]  eta: 0:07:27  lr: 0.000006  min_lr: 0.000000  loss: 4.0445 (4.1975)  class_acc: 0.2917 (0.3205)  loss_scale: 16384.0000 (36594.4463)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1196  max mem: 15572
Epoch: [31]  [2030/2809]  eta: 0:07:22  lr: 0.000006  min_lr: 0.000000  loss: 4.1284 (4.1974)  class_acc: 0.2917 (0.3204)  loss_scale: 16384.0000 (36494.9365)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1117  max mem: 15572
Epoch: [31]  [2040/2809]  eta: 0:07:16  lr: 0.000006  min_lr: 0.000000  loss: 4.1481 (4.1972)  class_acc: 0.3333 (0.3203)  loss_scale: 16384.0000 (36396.4018)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1487  max mem: 15572
Epoch: [31]  [2050/2809]  eta: 0:07:10  lr: 0.000006  min_lr: 0.000000  loss: 4.0863 (4.1972)  class_acc: 0.2917 (0.3200)  loss_scale: 16384.0000 (36298.8279)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1485  max mem: 15572
[2025-01-16 05:26:10,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:26:10,758] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [2060/2809]  eta: 0:07:05  lr: 0.000006  min_lr: 0.000000  loss: 4.1477 (4.1968)  class_acc: 0.2500 (0.3199)  loss_scale: 16384.0000 (36226.0495)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1173  max mem: 15572
Epoch: [31]  [2070/2809]  eta: 0:06:59  lr: 0.000006  min_lr: 0.000000  loss: 4.1477 (4.1970)  class_acc: 0.2500 (0.3195)  loss_scale: 32768.0000 (36209.3520)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0716  max mem: 15572
Epoch: [31]  [2080/2809]  eta: 0:06:53  lr: 0.000006  min_lr: 0.000000  loss: 4.1461 (4.1972)  class_acc: 0.2917 (0.3196)  loss_scale: 32768.0000 (36192.8150)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0755  max mem: 15572
Epoch: [31]  [2090/2809]  eta: 0:06:47  lr: 0.000006  min_lr: 0.000000  loss: 4.0909 (4.1963)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (36176.4362)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1343  max mem: 15572
Epoch: [31]  [2100/2809]  eta: 0:06:42  lr: 0.000006  min_lr: 0.000000  loss: 4.0974 (4.1961)  class_acc: 0.3750 (0.3200)  loss_scale: 32768.0000 (36160.2132)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1330  max mem: 15572
Epoch: [31]  [2110/2809]  eta: 0:06:36  lr: 0.000006  min_lr: 0.000000  loss: 4.2506 (4.1964)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (36144.1440)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1809  max mem: 15572
Epoch: [31]  [2120/2809]  eta: 0:06:30  lr: 0.000006  min_lr: 0.000000  loss: 4.2104 (4.1962)  class_acc: 0.2917 (0.3201)  loss_scale: 32768.0000 (36128.2263)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1257  max mem: 15572
Epoch: [31]  [2130/2809]  eta: 0:06:25  lr: 0.000006  min_lr: 0.000000  loss: 4.1927 (4.1962)  class_acc: 0.2917 (0.3200)  loss_scale: 32768.0000 (36112.4580)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0914  max mem: 15572
Epoch: [31]  [2140/2809]  eta: 0:06:19  lr: 0.000006  min_lr: 0.000000  loss: 4.1995 (4.1959)  class_acc: 0.2917 (0.3201)  loss_scale: 32768.0000 (36096.8370)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1620  max mem: 15572
Epoch: [31]  [2150/2809]  eta: 0:06:14  lr: 0.000006  min_lr: 0.000000  loss: 4.1273 (4.1956)  class_acc: 0.2500 (0.3201)  loss_scale: 32768.0000 (36081.3612)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1875  max mem: 15572
Epoch: [31]  [2160/2809]  eta: 0:06:08  lr: 0.000006  min_lr: 0.000000  loss: 4.0796 (4.1954)  class_acc: 0.2500 (0.3202)  loss_scale: 32768.0000 (36066.0287)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1049  max mem: 15572
[2025-01-16 05:27:12,348] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89248
[2025-01-16 05:27:12,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 05:27:12,348] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [2170/2809]  eta: 0:06:02  lr: 0.000006  min_lr: 0.000000  loss: 4.1314 (4.1950)  class_acc: 0.3750 (0.3205)  loss_scale: 32768.0000 (36035.7439)  weight_decay: 0.0500 (0.0500)  time: 0.4584  data: 0.0088  max mem: 15572
Epoch: [31]  [2180/2809]  eta: 0:05:56  lr: 0.000006  min_lr: 0.000000  loss: 4.1544 (4.1949)  class_acc: 0.2917 (0.3201)  loss_scale: 16384.0000 (35945.6396)  weight_decay: 0.0500 (0.0500)  time: 0.4886  data: 0.0261  max mem: 15572
Epoch: [31]  [2190/2809]  eta: 0:05:50  lr: 0.000006  min_lr: 0.000000  loss: 4.1982 (4.1950)  class_acc: 0.2083 (0.3198)  loss_scale: 16384.0000 (35856.3578)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0654  max mem: 15572
Epoch: [31]  [2200/2809]  eta: 0:05:44  lr: 0.000006  min_lr: 0.000000  loss: 4.4001 (4.1958)  class_acc: 0.2500 (0.3197)  loss_scale: 16384.0000 (35767.8873)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0866  max mem: 15572
Epoch: [31]  [2210/2809]  eta: 0:05:39  lr: 0.000006  min_lr: 0.000000  loss: 4.3700 (4.1959)  class_acc: 0.2917 (0.3197)  loss_scale: 16384.0000 (35680.2171)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0664  max mem: 15572
Epoch: [31]  [2220/2809]  eta: 0:05:33  lr: 0.000006  min_lr: 0.000000  loss: 4.2531 (4.1965)  class_acc: 0.2917 (0.3194)  loss_scale: 16384.0000 (35593.3363)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1244  max mem: 15572
Epoch: [31]  [2230/2809]  eta: 0:05:27  lr: 0.000006  min_lr: 0.000000  loss: 4.2121 (4.1963)  class_acc: 0.2917 (0.3194)  loss_scale: 16384.0000 (35507.2344)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1556  max mem: 15572
Epoch: [31]  [2240/2809]  eta: 0:05:22  lr: 0.000006  min_lr: 0.000000  loss: 4.1966 (4.1967)  class_acc: 0.3333 (0.3195)  loss_scale: 16384.0000 (35421.9009)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1169  max mem: 15572
Epoch: [31]  [2250/2809]  eta: 0:05:16  lr: 0.000006  min_lr: 0.000000  loss: 4.1957 (4.1968)  class_acc: 0.2917 (0.3193)  loss_scale: 16384.0000 (35337.3256)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1253  max mem: 15572
Epoch: [31]  [2260/2809]  eta: 0:05:11  lr: 0.000006  min_lr: 0.000000  loss: 4.1809 (4.1968)  class_acc: 0.2917 (0.3194)  loss_scale: 16384.0000 (35253.4985)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1599  max mem: 15572
Epoch: [31]  [2270/2809]  eta: 0:05:05  lr: 0.000006  min_lr: 0.000000  loss: 4.1809 (4.1967)  class_acc: 0.2917 (0.3193)  loss_scale: 16384.0000 (35170.4095)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1140  max mem: 15572
Epoch: [31]  [2280/2809]  eta: 0:04:59  lr: 0.000006  min_lr: 0.000000  loss: 4.1602 (4.1966)  class_acc: 0.2500 (0.3190)  loss_scale: 16384.0000 (35088.0491)  weight_decay: 0.0500 (0.0500)  time: 0.5023  data: 0.0407  max mem: 15572
Epoch: [31]  [2290/2809]  eta: 0:04:54  lr: 0.000006  min_lr: 0.000000  loss: 4.1602 (4.1964)  class_acc: 0.2500 (0.3191)  loss_scale: 16384.0000 (35006.4077)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1109  max mem: 15572
[2025-01-16 05:28:24,696] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:28:24,696] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [2300/2809]  eta: 0:04:48  lr: 0.000006  min_lr: 0.000000  loss: 4.0914 (4.1960)  class_acc: 0.2917 (0.3191)  loss_scale: 16384.0000 (34946.8370)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1286  max mem: 15572
Epoch: [31]  [2310/2809]  eta: 0:04:42  lr: 0.000006  min_lr: 0.000000  loss: 4.1882 (4.1961)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (34937.4089)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0761  max mem: 15572
Epoch: [31]  [2320/2809]  eta: 0:04:37  lr: 0.000006  min_lr: 0.000000  loss: 4.1692 (4.1957)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (34928.0620)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1141  max mem: 15572
Epoch: [31]  [2330/2809]  eta: 0:04:31  lr: 0.000006  min_lr: 0.000000  loss: 4.0959 (4.1959)  class_acc: 0.3333 (0.3192)  loss_scale: 32768.0000 (34918.7954)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1397  max mem: 15572
Epoch: [31]  [2340/2809]  eta: 0:04:25  lr: 0.000006  min_lr: 0.000000  loss: 4.2331 (4.1957)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (34909.6079)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1223  max mem: 15572
Epoch: [31]  [2350/2809]  eta: 0:04:20  lr: 0.000006  min_lr: 0.000000  loss: 4.2385 (4.1961)  class_acc: 0.2917 (0.3189)  loss_scale: 32768.0000 (34900.4985)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1230  max mem: 15572
Epoch: [31]  [2360/2809]  eta: 0:04:14  lr: 0.000006  min_lr: 0.000000  loss: 4.2198 (4.1961)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (34891.4663)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1536  max mem: 15572
Epoch: [31]  [2370/2809]  eta: 0:04:09  lr: 0.000006  min_lr: 0.000000  loss: 4.2023 (4.1965)  class_acc: 0.2917 (0.3188)  loss_scale: 32768.0000 (34882.5103)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.2208  max mem: 15572
Epoch: [31]  [2380/2809]  eta: 0:04:03  lr: 0.000006  min_lr: 0.000000  loss: 4.3004 (4.1969)  class_acc: 0.2500 (0.3187)  loss_scale: 32768.0000 (34873.6296)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1456  max mem: 15572
Epoch: [31]  [2390/2809]  eta: 0:03:57  lr: 0.000006  min_lr: 0.000000  loss: 4.2443 (4.1966)  class_acc: 0.3333 (0.3188)  loss_scale: 32768.0000 (34864.8231)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0612  max mem: 15572
Epoch: [31]  [2400/2809]  eta: 0:03:51  lr: 0.000006  min_lr: 0.000000  loss: 4.3394 (4.1973)  class_acc: 0.2917 (0.3187)  loss_scale: 32768.0000 (34856.0900)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1083  max mem: 15572
Epoch: [31]  [2410/2809]  eta: 0:03:46  lr: 0.000006  min_lr: 0.000000  loss: 4.3512 (4.1973)  class_acc: 0.2500 (0.3188)  loss_scale: 32768.0000 (34847.4293)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1187  max mem: 15572
Epoch: [31]  [2420/2809]  eta: 0:03:40  lr: 0.000006  min_lr: 0.000000  loss: 4.2266 (4.1974)  class_acc: 0.2500 (0.3186)  loss_scale: 32768.0000 (34838.8401)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0711  max mem: 15572
[2025-01-16 05:29:36,853] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:29:36,853] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [2430/2809]  eta: 0:03:34  lr: 0.000006  min_lr: 0.000000  loss: 4.2336 (4.1978)  class_acc: 0.2500 (0.3185)  loss_scale: 32768.0000 (34897.7178)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0803  max mem: 15572
Epoch: [31]  [2440/2809]  eta: 0:03:29  lr: 0.000006  min_lr: 0.000000  loss: 4.2299 (4.1979)  class_acc: 0.3750 (0.3188)  loss_scale: 65536.0000 (35023.2331)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1654  max mem: 15572
Epoch: [31]  [2450/2809]  eta: 0:03:23  lr: 0.000006  min_lr: 0.000000  loss: 4.1464 (4.1981)  class_acc: 0.3750 (0.3188)  loss_scale: 65536.0000 (35147.7242)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1277  max mem: 15572
[2025-01-16 05:29:52,584] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89532
[2025-01-16 05:29:52,584] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:29:52,585] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [2460/2809]  eta: 0:03:17  lr: 0.000006  min_lr: 0.000000  loss: 4.1263 (4.1975)  class_acc: 0.3333 (0.3188)  loss_scale: 65536.0000 (35164.6843)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0978  max mem: 15572
Epoch: [31]  [2470/2809]  eta: 0:03:12  lr: 0.000006  min_lr: 0.000000  loss: 4.0678 (4.1974)  class_acc: 0.2917 (0.3188)  loss_scale: 32768.0000 (35154.9850)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1491  max mem: 15572
Epoch: [31]  [2480/2809]  eta: 0:03:06  lr: 0.000006  min_lr: 0.000000  loss: 4.1516 (4.1978)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (35145.3640)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1113  max mem: 15572
Epoch: [31]  [2490/2809]  eta: 0:03:00  lr: 0.000006  min_lr: 0.000000  loss: 4.1516 (4.1973)  class_acc: 0.2917 (0.3187)  loss_scale: 32768.0000 (35135.8202)  weight_decay: 0.0500 (0.0500)  time: 0.4894  data: 0.0397  max mem: 15572
Epoch: [31]  [2500/2809]  eta: 0:02:55  lr: 0.000006  min_lr: 0.000000  loss: 4.1414 (4.1974)  class_acc: 0.3333 (0.3185)  loss_scale: 32768.0000 (35126.3527)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0757  max mem: 15572
Epoch: [31]  [2510/2809]  eta: 0:02:49  lr: 0.000006  min_lr: 0.000000  loss: 4.2196 (4.1975)  class_acc: 0.2500 (0.3183)  loss_scale: 32768.0000 (35116.9606)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1053  max mem: 15572
Epoch: [31]  [2520/2809]  eta: 0:02:43  lr: 0.000006  min_lr: 0.000000  loss: 4.2196 (4.1978)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (35107.6430)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0649  max mem: 15572
Epoch: [31]  [2530/2809]  eta: 0:02:38  lr: 0.000006  min_lr: 0.000000  loss: 4.2947 (4.1983)  class_acc: 0.2917 (0.3181)  loss_scale: 32768.0000 (35098.3991)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1328  max mem: 15572
Epoch: [31]  [2540/2809]  eta: 0:02:32  lr: 0.000006  min_lr: 0.000000  loss: 4.3171 (4.1988)  class_acc: 0.2500 (0.3181)  loss_scale: 32768.0000 (35089.2279)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1526  max mem: 15572
Epoch: [31]  [2550/2809]  eta: 0:02:26  lr: 0.000006  min_lr: 0.000000  loss: 4.2330 (4.1986)  class_acc: 0.3750 (0.3183)  loss_scale: 32768.0000 (35080.1286)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0457  max mem: 15572
Epoch: [31]  [2560/2809]  eta: 0:02:20  lr: 0.000006  min_lr: 0.000000  loss: 4.2122 (4.1989)  class_acc: 0.3750 (0.3184)  loss_scale: 32768.0000 (35071.1004)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0488  max mem: 15572
Epoch: [31]  [2570/2809]  eta: 0:02:15  lr: 0.000006  min_lr: 0.000000  loss: 4.3475 (4.1996)  class_acc: 0.2500 (0.3181)  loss_scale: 32768.0000 (35062.1424)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1160  max mem: 15572
Epoch: [31]  [2580/2809]  eta: 0:02:09  lr: 0.000006  min_lr: 0.000000  loss: 4.3285 (4.1995)  class_acc: 0.2917 (0.3179)  loss_scale: 32768.0000 (35053.2538)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0905  max mem: 15572
[2025-01-16 05:31:04,104] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:31:04,104] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [2590/2809]  eta: 0:02:03  lr: 0.000006  min_lr: 0.000000  loss: 4.2876 (4.1995)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (35158.2555)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0234  max mem: 15572
Epoch: [31]  [2600/2809]  eta: 0:01:58  lr: 0.000006  min_lr: 0.000000  loss: 4.2672 (4.1994)  class_acc: 0.2917 (0.3178)  loss_scale: 65536.0000 (35275.0481)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0489  max mem: 15572
Epoch: [31]  [2610/2809]  eta: 0:01:52  lr: 0.000006  min_lr: 0.000000  loss: 4.1017 (4.1990)  class_acc: 0.2917 (0.3177)  loss_scale: 65536.0000 (35390.9460)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0774  max mem: 15572
Epoch: [31]  [2620/2809]  eta: 0:01:46  lr: 0.000006  min_lr: 0.000000  loss: 4.1017 (4.1990)  class_acc: 0.2083 (0.3173)  loss_scale: 65536.0000 (35505.9596)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1038  max mem: 15572
Epoch: [31]  [2630/2809]  eta: 0:01:41  lr: 0.000006  min_lr: 0.000000  loss: 4.0735 (4.1981)  class_acc: 0.2500 (0.3176)  loss_scale: 65536.0000 (35620.0988)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1308  max mem: 15572
Epoch: [31]  [2640/2809]  eta: 0:01:35  lr: 0.000006  min_lr: 0.000000  loss: 4.0735 (4.1980)  class_acc: 0.3333 (0.3176)  loss_scale: 65536.0000 (35733.3737)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1358  max mem: 15572
[2025-01-16 05:31:40,498] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89726
[2025-01-16 05:31:40,498] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:31:40,498] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [2650/2809]  eta: 0:01:29  lr: 0.000006  min_lr: 0.000000  loss: 4.0919 (4.1979)  class_acc: 0.2917 (0.3176)  loss_scale: 65536.0000 (35796.3516)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.0800  max mem: 15572
Epoch: [31]  [2660/2809]  eta: 0:01:24  lr: 0.000006  min_lr: 0.000000  loss: 4.0919 (4.1974)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (35784.9711)  weight_decay: 0.0500 (0.0500)  time: 0.4389  data: 0.0006  max mem: 15572
Epoch: [31]  [2670/2809]  eta: 0:01:18  lr: 0.000006  min_lr: 0.000000  loss: 4.1072 (4.1977)  class_acc: 0.3333 (0.3176)  loss_scale: 32768.0000 (35773.6758)  weight_decay: 0.0500 (0.0500)  time: 0.4899  data: 0.0008  max mem: 15572
Epoch: [31]  [2680/2809]  eta: 0:01:12  lr: 0.000006  min_lr: 0.000000  loss: 4.2483 (4.1980)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (35762.4648)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0007  max mem: 15572
Epoch: [31]  [2690/2809]  eta: 0:01:07  lr: 0.000006  min_lr: 0.000000  loss: 4.4313 (4.1991)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (35751.3370)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0007  max mem: 15572
Epoch: [31]  [2700/2809]  eta: 0:01:01  lr: 0.000006  min_lr: 0.000000  loss: 4.4183 (4.1988)  class_acc: 0.3333 (0.3173)  loss_scale: 32768.0000 (35740.2917)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0598  max mem: 15572
Epoch: [31]  [2710/2809]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 4.0261 (4.1979)  class_acc: 0.3333 (0.3174)  loss_scale: 32768.0000 (35729.3279)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1108  max mem: 15572
Epoch: [31]  [2720/2809]  eta: 0:00:50  lr: 0.000006  min_lr: 0.000000  loss: 4.0173 (4.1979)  class_acc: 0.2500 (0.3171)  loss_scale: 32768.0000 (35718.4447)  weight_decay: 0.0500 (0.0500)  time: 0.6998  data: 0.2216  max mem: 15572
Epoch: [31]  [2730/2809]  eta: 0:00:44  lr: 0.000006  min_lr: 0.000000  loss: 4.2404 (4.1983)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (35707.6412)  weight_decay: 0.0500 (0.0500)  time: 0.7446  data: 0.2833  max mem: 15572
Epoch: [31]  [2740/2809]  eta: 0:00:39  lr: 0.000006  min_lr: 0.000000  loss: 4.3415 (4.1985)  class_acc: 0.3333 (0.3172)  loss_scale: 32768.0000 (35696.9165)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1690  max mem: 15572
Epoch: [31]  [2750/2809]  eta: 0:00:33  lr: 0.000006  min_lr: 0.000000  loss: 4.2512 (4.1984)  class_acc: 0.3750 (0.3172)  loss_scale: 32768.0000 (35686.2697)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1210  max mem: 15572
Epoch: [31]  [2760/2809]  eta: 0:00:27  lr: 0.000006  min_lr: 0.000000  loss: 4.3178 (4.1986)  class_acc: 0.3333 (0.3171)  loss_scale: 32768.0000 (35675.7001)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1531  max mem: 15572
Epoch: [31]  [2770/2809]  eta: 0:00:22  lr: 0.000006  min_lr: 0.000000  loss: 4.1501 (4.1981)  class_acc: 0.3333 (0.3176)  loss_scale: 32768.0000 (35665.2068)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.1782  max mem: 15572
[2025-01-16 05:32:57,791] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:32:57,791] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [2780/2809]  eta: 0:00:16  lr: 0.000006  min_lr: 0.000000  loss: 4.1399 (4.1981)  class_acc: 0.3750 (0.3177)  loss_scale: 32768.0000 (35713.7030)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1547  max mem: 15572
Epoch: [31]  [2790/2809]  eta: 0:00:10  lr: 0.000006  min_lr: 0.000000  loss: 4.0522 (4.1970)  class_acc: 0.3333 (0.3177)  loss_scale: 65536.0000 (35820.5546)  weight_decay: 0.0500 (0.0500)  time: 0.6889  data: 0.1979  max mem: 15572
Epoch: [31]  [2800/2809]  eta: 0:00:05  lr: 0.000006  min_lr: 0.000000  loss: 4.1099 (4.1974)  class_acc: 0.2500 (0.3175)  loss_scale: 65536.0000 (35926.6433)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1335  max mem: 15572
Epoch: [31]  [2808/2809]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 4.1901 (4.1971)  class_acc: 0.2917 (0.3176)  loss_scale: 65536.0000 (36010.9705)  weight_decay: 0.0500 (0.0500)  time: 0.4031  data: 0.0006  max mem: 15572
Epoch: [31] Total time: 0:26:34 (0.5675 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 4.1901 (4.1971)  class_acc: 0.2917 (0.3176)  loss_scale: 65536.0000 (36010.9705)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:30:45  loss: 1.1506 (1.1506)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 6.7847  data: 6.5763  max mem: 15572
Val:  [ 10/272]  eta: 0:03:26  loss: 2.4915 (2.5175)  acc1: 44.4444 (44.9495)  acc5: 77.7778 (75.2525)  time: 0.7882  data: 0.5983  max mem: 15572
Val:  [ 20/272]  eta: 0:02:07  loss: 2.6114 (2.6049)  acc1: 44.4444 (44.9735)  acc5: 72.2222 (73.2804)  time: 0.1903  data: 0.0006  max mem: 15572
Val:  [ 30/272]  eta: 0:01:43  loss: 2.6133 (2.6441)  acc1: 38.8889 (41.7563)  acc5: 66.6667 (72.7599)  time: 0.2277  data: 0.0450  max mem: 15572
Val:  [ 40/272]  eta: 0:01:32  loss: 2.6133 (2.6530)  acc1: 33.3333 (40.1084)  acc5: 72.2222 (72.7642)  time: 0.2921  data: 0.1107  max mem: 15572
Val:  [ 50/272]  eta: 0:01:26  loss: 2.5213 (2.5799)  acc1: 38.8889 (41.3943)  acc5: 77.7778 (74.6187)  time: 0.3353  data: 0.1425  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 1.9647 (2.5143)  acc1: 55.5556 (43.5337)  acc5: 83.3333 (75.5920)  time: 0.3375  data: 0.1386  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 2.0390 (2.4472)  acc1: 61.1111 (46.3224)  acc5: 83.3333 (76.6823)  time: 0.3097  data: 0.1054  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 2.1298 (2.4553)  acc1: 50.0000 (46.0905)  acc5: 77.7778 (76.6118)  time: 0.3000  data: 0.1010  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 2.3947 (2.4520)  acc1: 44.4444 (46.1538)  acc5: 77.7778 (77.1673)  time: 0.2941  data: 0.1134  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.3947 (2.4766)  acc1: 38.8889 (45.2695)  acc5: 77.7778 (76.8977)  time: 0.3126  data: 0.1311  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.6289 (2.5292)  acc1: 33.3333 (43.7437)  acc5: 72.2222 (75.8759)  time: 0.3222  data: 0.1301  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 2.9691 (2.5630)  acc1: 27.7778 (42.9293)  acc5: 66.6667 (75.3903)  time: 0.3069  data: 0.1145  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.5335 (2.5332)  acc1: 38.8889 (43.8083)  acc5: 83.3333 (76.0814)  time: 0.3103  data: 0.1242  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 2.1219 (2.5351)  acc1: 55.5556 (44.1292)  acc5: 83.3333 (75.7683)  time: 0.3114  data: 0.1265  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.5005 (2.5310)  acc1: 33.3333 (43.8926)  acc5: 77.7778 (76.0486)  time: 0.3145  data: 0.1260  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.4621 (2.5299)  acc1: 44.4444 (44.3754)  acc5: 77.7778 (76.3285)  time: 0.2956  data: 0.1020  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.5572 (2.5422)  acc1: 44.4444 (44.0221)  acc5: 77.7778 (75.9259)  time: 0.3187  data: 0.1134  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.5413 (2.5308)  acc1: 38.8889 (44.0454)  acc5: 72.2222 (76.2431)  time: 0.3225  data: 0.1250  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.4983 (2.5697)  acc1: 33.3333 (42.8447)  acc5: 72.2222 (75.1891)  time: 0.3116  data: 0.1186  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.6013 (2.5750)  acc1: 33.3333 (42.4544)  acc5: 72.2222 (75.0415)  time: 0.2925  data: 0.0973  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.4862 (2.5806)  acc1: 38.8889 (42.3907)  acc5: 77.7778 (74.9605)  time: 0.2896  data: 0.0983  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.6778 (2.5759)  acc1: 44.4444 (42.3580)  acc5: 72.2222 (74.9874)  time: 0.3238  data: 0.1364  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.1317 (2.5604)  acc1: 50.0000 (43.0976)  acc5: 83.3333 (75.3247)  time: 0.3137  data: 0.1229  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1489 (2.5476)  acc1: 61.1111 (43.3379)  acc5: 83.3333 (75.6570)  time: 0.3108  data: 0.1149  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.5295 (2.5556)  acc1: 38.8889 (42.8951)  acc5: 83.3333 (75.5865)  time: 0.3124  data: 0.1175  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9073 (2.5159)  acc1: 61.1111 (44.4019)  acc5: 88.8889 (76.2452)  time: 0.2954  data: 0.1061  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.9528 (2.5115)  acc1: 61.1111 (44.4649)  acc5: 83.3333 (76.4658)  time: 0.2097  data: 0.0472  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.9528 (2.5146)  acc1: 61.1111 (44.4604)  acc5: 83.3333 (76.4489)  time: 0.2010  data: 0.0472  max mem: 15572
Val: Total time: 0:01:26 (0.3180 s / it)
* Acc@1 44.460 Acc@5 76.449 loss 2.515
Accuracy of the network on the 4883 val videos: 44.5%
Max accuracy: 44.85%
Epoch: [32]  [   0/2809]  eta: 5:45:26  lr: 0.000006  min_lr: 0.000000  loss: 4.0687 (4.0687)  class_acc: 0.2500 (0.2500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.3785  data: 6.8905  max mem: 15572
Epoch: [32]  [  10/2809]  eta: 0:56:47  lr: 0.000006  min_lr: 0.000000  loss: 4.0889 (4.1381)  class_acc: 0.3333 (0.3485)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2173  data: 0.7754  max mem: 15572
Epoch: [32]  [  20/2809]  eta: 0:42:30  lr: 0.000006  min_lr: 0.000000  loss: 4.1789 (4.1560)  class_acc: 0.2917 (0.3194)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1403  max mem: 15572
Epoch: [32]  [  30/2809]  eta: 0:36:42  lr: 0.000006  min_lr: 0.000000  loss: 4.2066 (4.1492)  class_acc: 0.2917 (0.3199)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1046  max mem: 15572
Epoch: [32]  [  40/2809]  eta: 0:33:37  lr: 0.000006  min_lr: 0.000000  loss: 4.1342 (4.1310)  class_acc: 0.3333 (0.3252)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0763  max mem: 15572
Epoch: [32]  [  50/2809]  eta: 0:31:55  lr: 0.000006  min_lr: 0.000000  loss: 4.2497 (4.1698)  class_acc: 0.3333 (0.3211)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0817  max mem: 15572
Epoch: [32]  [  60/2809]  eta: 0:30:50  lr: 0.000006  min_lr: 0.000000  loss: 4.3188 (4.1790)  class_acc: 0.3333 (0.3135)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1149  max mem: 15572
[2025-01-16 05:35:27,526] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89956
[2025-01-16 05:35:27,526] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:35:27,529] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [  70/2809]  eta: 0:30:09  lr: 0.000006  min_lr: 0.000000  loss: 4.2373 (4.1737)  class_acc: 0.3333 (0.3151)  loss_scale: 65536.0000 (64151.4366)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1316  max mem: 15572
Epoch: [32]  [  80/2809]  eta: 0:29:43  lr: 0.000006  min_lr: 0.000000  loss: 4.2621 (4.2024)  class_acc: 0.2917 (0.3071)  loss_scale: 32768.0000 (60276.9383)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1498  max mem: 15572
Epoch: [32]  [  90/2809]  eta: 0:29:06  lr: 0.000006  min_lr: 0.000000  loss: 4.2697 (4.2051)  class_acc: 0.2917 (0.3091)  loss_scale: 32768.0000 (57253.9780)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1353  max mem: 15572
Epoch: [32]  [ 100/2809]  eta: 0:28:59  lr: 0.000006  min_lr: 0.000000  loss: 4.1366 (4.1994)  class_acc: 0.2917 (0.3119)  loss_scale: 32768.0000 (54829.6238)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1573  max mem: 15572
Epoch: [32]  [ 110/2809]  eta: 0:28:56  lr: 0.000006  min_lr: 0.000000  loss: 3.9451 (4.1855)  class_acc: 0.3333 (0.3153)  loss_scale: 32768.0000 (52842.0901)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.1971  max mem: 15572
[2025-01-16 05:35:53,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=90000, skipped=558, lr=[5.594318732376175e-08, 5.594318732376175e-08, 7.991883903394536e-08, 7.991883903394536e-08, 1.141697700484934e-07, 1.141697700484934e-07, 1.6309967149784771e-07, 1.6309967149784771e-07, 2.3299953071121103e-07, 2.3299953071121103e-07, 3.328564724445872e-07, 3.328564724445872e-07, 4.755092463494103e-07, 4.755092463494103e-07, 6.792989233563005e-07, 6.792989233563005e-07, 9.704270333661435e-07, 9.704270333661435e-07, 1.3863243333802054e-06, 1.3863243333802054e-06, 1.980463333400293e-06, 1.980463333400293e-06, 2.8292333334289905e-06, 2.8292333334289905e-06, 4.041761904898559e-06, 4.041761904898559e-06, 5.773945578426512e-06, 5.773945578426512e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:35:53,401] [INFO] [timer.py:260:stop] epoch=0/micro_step=90000/global_step=90000, RunningAvgSamplesPerSec=27.942882517060376, CurrSamplesPerSec=31.680389140522127, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [32]  [ 120/2809]  eta: 0:28:15  lr: 0.000006  min_lr: 0.000000  loss: 4.1241 (4.1795)  class_acc: 0.3333 (0.3137)  loss_scale: 32768.0000 (51183.0744)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1151  max mem: 15572
Epoch: [32]  [ 130/2809]  eta: 0:28:16  lr: 0.000006  min_lr: 0.000000  loss: 4.1241 (4.1749)  class_acc: 0.2917 (0.3174)  loss_scale: 32768.0000 (49777.3435)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1250  max mem: 15572
Epoch: [32]  [ 140/2809]  eta: 0:27:52  lr: 0.000006  min_lr: 0.000000  loss: 4.0756 (4.1628)  class_acc: 0.3750 (0.3206)  loss_scale: 32768.0000 (48571.0071)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1387  max mem: 15572
Epoch: [32]  [ 150/2809]  eta: 0:27:25  lr: 0.000006  min_lr: 0.000000  loss: 4.1877 (4.1668)  class_acc: 0.3750 (0.3228)  loss_scale: 32768.0000 (47524.4503)  weight_decay: 0.0500 (0.0500)  time: 0.5225  data: 0.0674  max mem: 15572
Epoch: [32]  [ 160/2809]  eta: 0:27:11  lr: 0.000006  min_lr: 0.000000  loss: 4.1523 (4.1570)  class_acc: 0.3750 (0.3263)  loss_scale: 32768.0000 (46607.9006)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.1049  max mem: 15572
Epoch: [32]  [ 170/2809]  eta: 0:26:58  lr: 0.000006  min_lr: 0.000000  loss: 4.0461 (4.1547)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (45798.5497)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1285  max mem: 15572
Epoch: [32]  [ 180/2809]  eta: 0:26:41  lr: 0.000006  min_lr: 0.000000  loss: 4.2612 (4.1621)  class_acc: 0.2500 (0.3207)  loss_scale: 32768.0000 (45078.6298)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1092  max mem: 15572
Epoch: [32]  [ 190/2809]  eta: 0:26:27  lr: 0.000006  min_lr: 0.000000  loss: 4.2612 (4.1698)  class_acc: 0.2500 (0.3185)  loss_scale: 32768.0000 (44434.0942)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1079  max mem: 15572
[2025-01-16 05:36:41,695] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:36:41,695] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 200/2809]  eta: 0:26:15  lr: 0.000006  min_lr: 0.000000  loss: 4.1868 (4.1691)  class_acc: 0.3333 (0.3203)  loss_scale: 32768.0000 (44505.7910)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1011  max mem: 15572
Epoch: [32]  [ 210/2809]  eta: 0:25:56  lr: 0.000006  min_lr: 0.000000  loss: 4.0638 (4.1640)  class_acc: 0.3750 (0.3227)  loss_scale: 65536.0000 (45502.4834)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0630  max mem: 15572
[2025-01-16 05:36:53,176] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90106
[2025-01-16 05:36:53,177] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:36:53,177] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 220/2809]  eta: 0:25:52  lr: 0.000006  min_lr: 0.000000  loss: 4.1128 (4.1628)  class_acc: 0.3333 (0.3226)  loss_scale: 65536.0000 (45964.1629)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.0964  max mem: 15572
Epoch: [32]  [ 230/2809]  eta: 0:25:37  lr: 0.000006  min_lr: 0.000000  loss: 4.0254 (4.1573)  class_acc: 0.2917 (0.3203)  loss_scale: 32768.0000 (45392.9004)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1202  max mem: 15572
Epoch: [32]  [ 240/2809]  eta: 0:25:28  lr: 0.000006  min_lr: 0.000000  loss: 4.0633 (4.1607)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (44869.0456)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.1113  max mem: 15572
Epoch: [32]  [ 250/2809]  eta: 0:25:15  lr: 0.000006  min_lr: 0.000000  loss: 4.1617 (4.1603)  class_acc: 0.3333 (0.3217)  loss_scale: 32768.0000 (44386.9323)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1056  max mem: 15572
Epoch: [32]  [ 260/2809]  eta: 0:25:05  lr: 0.000006  min_lr: 0.000000  loss: 3.9880 (4.1543)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (43941.7625)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0890  max mem: 15572
Epoch: [32]  [ 270/2809]  eta: 0:25:04  lr: 0.000006  min_lr: 0.000000  loss: 3.9670 (4.1499)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (43529.4465)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1535  max mem: 15572
Epoch: [32]  [ 280/2809]  eta: 0:24:54  lr: 0.000006  min_lr: 0.000000  loss: 4.1516 (4.1546)  class_acc: 0.3750 (0.3246)  loss_scale: 32768.0000 (43146.4769)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1519  max mem: 15572
Epoch: [32]  [ 290/2809]  eta: 0:24:38  lr: 0.000006  min_lr: 0.000000  loss: 4.3002 (4.1574)  class_acc: 0.2917 (0.3240)  loss_scale: 32768.0000 (42789.8282)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0724  max mem: 15572
Epoch: [32]  [ 300/2809]  eta: 0:24:38  lr: 0.000006  min_lr: 0.000000  loss: 4.1975 (4.1578)  class_acc: 0.3333 (0.3253)  loss_scale: 32768.0000 (42456.8771)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1195  max mem: 15572
Epoch: [32]  [ 310/2809]  eta: 0:24:28  lr: 0.000006  min_lr: 0.000000  loss: 4.2049 (4.1630)  class_acc: 0.2500 (0.3237)  loss_scale: 32768.0000 (42145.3376)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1310  max mem: 15572
Epoch: [32]  [ 320/2809]  eta: 0:24:24  lr: 0.000006  min_lr: 0.000000  loss: 4.1907 (4.1636)  class_acc: 0.2500 (0.3227)  loss_scale: 32768.0000 (41853.2087)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1175  max mem: 15572
Epoch: [32]  [ 330/2809]  eta: 0:24:16  lr: 0.000006  min_lr: 0.000000  loss: 4.1357 (4.1596)  class_acc: 0.3333 (0.3249)  loss_scale: 32768.0000 (41578.7311)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1301  max mem: 15572
Epoch: [32]  [ 340/2809]  eta: 0:24:12  lr: 0.000006  min_lr: 0.000000  loss: 4.0721 (4.1607)  class_acc: 0.3333 (0.3237)  loss_scale: 32768.0000 (41320.3519)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1140  max mem: 15572
[2025-01-16 05:38:06,196] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:38:06,196] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 350/2809]  eta: 0:24:02  lr: 0.000006  min_lr: 0.000000  loss: 4.2866 (4.1649)  class_acc: 0.2917 (0.3242)  loss_scale: 32768.0000 (41450.1197)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1093  max mem: 15572
Epoch: [32]  [ 360/2809]  eta: 0:23:57  lr: 0.000006  min_lr: 0.000000  loss: 4.3359 (4.1654)  class_acc: 0.3333 (0.3248)  loss_scale: 65536.0000 (42117.3186)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1180  max mem: 15572
[2025-01-16 05:38:17,930] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90255
[2025-01-16 05:38:17,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:38:17,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 370/2809]  eta: 0:23:50  lr: 0.000006  min_lr: 0.000000  loss: 4.2187 (4.1676)  class_acc: 0.3750 (0.3264)  loss_scale: 65536.0000 (42395.2561)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1490  max mem: 15572
Epoch: [32]  [ 380/2809]  eta: 0:23:36  lr: 0.000006  min_lr: 0.000000  loss: 4.2254 (4.1704)  class_acc: 0.3750 (0.3278)  loss_scale: 32768.0000 (42142.5722)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0786  max mem: 15572
Epoch: [32]  [ 390/2809]  eta: 0:23:27  lr: 0.000006  min_lr: 0.000000  loss: 4.2200 (4.1706)  class_acc: 0.3750 (0.3285)  loss_scale: 32768.0000 (41902.8133)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0402  max mem: 15572
Epoch: [32]  [ 400/2809]  eta: 0:23:22  lr: 0.000006  min_lr: 0.000000  loss: 4.1863 (4.1739)  class_acc: 0.3333 (0.3284)  loss_scale: 32768.0000 (41675.0125)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1140  max mem: 15572
Epoch: [32]  [ 410/2809]  eta: 0:23:16  lr: 0.000006  min_lr: 0.000000  loss: 4.1401 (4.1732)  class_acc: 0.3333 (0.3287)  loss_scale: 32768.0000 (41458.2968)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1492  max mem: 15572
Epoch: [32]  [ 420/2809]  eta: 0:23:09  lr: 0.000006  min_lr: 0.000000  loss: 4.0695 (4.1681)  class_acc: 0.3333 (0.3292)  loss_scale: 32768.0000 (41251.8765)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1431  max mem: 15572
Epoch: [32]  [ 430/2809]  eta: 0:23:00  lr: 0.000006  min_lr: 0.000000  loss: 4.1266 (4.1673)  class_acc: 0.3333 (0.3282)  loss_scale: 32768.0000 (41055.0348)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.1062  max mem: 15572
Epoch: [32]  [ 440/2809]  eta: 0:22:52  lr: 0.000006  min_lr: 0.000000  loss: 4.2040 (4.1663)  class_acc: 0.2917 (0.3278)  loss_scale: 32768.0000 (40867.1202)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0928  max mem: 15572
Epoch: [32]  [ 450/2809]  eta: 0:22:48  lr: 0.000006  min_lr: 0.000000  loss: 4.2466 (4.1684)  class_acc: 0.2917 (0.3274)  loss_scale: 32768.0000 (40687.5388)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1523  max mem: 15572
Epoch: [32]  [ 460/2809]  eta: 0:22:42  lr: 0.000006  min_lr: 0.000000  loss: 4.3421 (4.1729)  class_acc: 0.2917 (0.3268)  loss_scale: 32768.0000 (40515.7484)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1470  max mem: 15572
Epoch: [32]  [ 470/2809]  eta: 0:22:36  lr: 0.000006  min_lr: 0.000000  loss: 4.2051 (4.1700)  class_acc: 0.2500 (0.3259)  loss_scale: 32768.0000 (40351.2527)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1112  max mem: 15572
Epoch: [32]  [ 480/2809]  eta: 0:22:24  lr: 0.000006  min_lr: 0.000000  loss: 4.0824 (4.1685)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (40193.5967)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0540  max mem: 15572
Epoch: [32]  [ 490/2809]  eta: 0:22:22  lr: 0.000006  min_lr: 0.000000  loss: 4.1608 (4.1694)  class_acc: 0.2917 (0.3256)  loss_scale: 32768.0000 (40042.3625)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1112  max mem: 15572
[2025-01-16 05:39:28,581] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:39:28,582] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 500/2809]  eta: 0:22:14  lr: 0.000006  min_lr: 0.000000  loss: 4.2464 (4.1715)  class_acc: 0.2917 (0.3263)  loss_scale: 32768.0000 (40224.1916)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1532  max mem: 15572
Epoch: [32]  [ 510/2809]  eta: 0:22:11  lr: 0.000006  min_lr: 0.000000  loss: 4.2660 (4.1706)  class_acc: 0.3333 (0.3265)  loss_scale: 65536.0000 (40719.5303)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1286  max mem: 15572
Epoch: [32]  [ 520/2809]  eta: 0:22:01  lr: 0.000006  min_lr: 0.000000  loss: 4.2738 (4.1745)  class_acc: 0.2917 (0.3265)  loss_scale: 65536.0000 (41195.8541)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1057  max mem: 15572
Epoch: [32]  [ 530/2809]  eta: 0:22:02  lr: 0.000006  min_lr: 0.000000  loss: 4.3466 (4.1789)  class_acc: 0.2917 (0.3261)  loss_scale: 65536.0000 (41654.2373)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1620  max mem: 15572
[2025-01-16 05:39:54,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90428
[2025-01-16 05:39:54,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:39:54,481] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 540/2809]  eta: 0:21:51  lr: 0.000006  min_lr: 0.000000  loss: 4.3466 (4.1833)  class_acc: 0.2917 (0.3252)  loss_scale: 65536.0000 (42035.1054)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1457  max mem: 15572
Epoch: [32]  [ 550/2809]  eta: 0:21:48  lr: 0.000006  min_lr: 0.000000  loss: 4.3036 (4.1844)  class_acc: 0.2500 (0.3242)  loss_scale: 32768.0000 (41866.9183)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1024  max mem: 15572
Epoch: [32]  [ 560/2809]  eta: 0:21:43  lr: 0.000006  min_lr: 0.000000  loss: 4.2025 (4.1823)  class_acc: 0.2500 (0.3233)  loss_scale: 32768.0000 (41704.7273)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1647  max mem: 15572
Epoch: [32]  [ 570/2809]  eta: 0:21:37  lr: 0.000006  min_lr: 0.000000  loss: 4.0400 (4.1788)  class_acc: 0.2500 (0.3230)  loss_scale: 32768.0000 (41548.2172)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1157  max mem: 15572
Epoch: [32]  [ 580/2809]  eta: 0:21:33  lr: 0.000006  min_lr: 0.000000  loss: 4.0349 (4.1787)  class_acc: 0.2917 (0.3231)  loss_scale: 32768.0000 (41397.0947)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1556  max mem: 15572
Epoch: [32]  [ 590/2809]  eta: 0:21:22  lr: 0.000006  min_lr: 0.000000  loss: 4.3282 (4.1813)  class_acc: 0.2917 (0.3232)  loss_scale: 32768.0000 (41251.0863)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.1105  max mem: 15572
Epoch: [32]  [ 600/2809]  eta: 0:21:16  lr: 0.000006  min_lr: 0.000000  loss: 4.2515 (4.1817)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (41109.9368)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0615  max mem: 15572
Epoch: [32]  [ 610/2809]  eta: 0:21:08  lr: 0.000006  min_lr: 0.000000  loss: 4.1143 (4.1827)  class_acc: 0.2917 (0.3230)  loss_scale: 32768.0000 (40973.4075)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0852  max mem: 15572
Epoch: [32]  [ 620/2809]  eta: 0:21:03  lr: 0.000006  min_lr: 0.000000  loss: 4.1909 (4.1815)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (40841.2754)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0988  max mem: 15572
Epoch: [32]  [ 630/2809]  eta: 0:20:55  lr: 0.000006  min_lr: 0.000000  loss: 4.1909 (4.1811)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (40713.3312)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1097  max mem: 15572
Epoch: [32]  [ 640/2809]  eta: 0:20:47  lr: 0.000006  min_lr: 0.000000  loss: 4.1810 (4.1809)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (40589.3791)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0706  max mem: 15572
Epoch: [32]  [ 650/2809]  eta: 0:20:40  lr: 0.000006  min_lr: 0.000000  loss: 4.2453 (4.1817)  class_acc: 0.2917 (0.3225)  loss_scale: 32768.0000 (40469.2350)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0723  max mem: 15572
Epoch: [32]  [ 660/2809]  eta: 0:20:34  lr: 0.000006  min_lr: 0.000000  loss: 4.3437 (4.1851)  class_acc: 0.3333 (0.3233)  loss_scale: 32768.0000 (40352.7262)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.1142  max mem: 15572
[2025-01-16 05:41:06,956] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:41:06,956] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 670/2809]  eta: 0:20:29  lr: 0.000006  min_lr: 0.000000  loss: 4.3717 (4.1852)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (40337.3592)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1452  max mem: 15572
Epoch: [32]  [ 680/2809]  eta: 0:20:26  lr: 0.000005  min_lr: 0.000000  loss: 4.1678 (4.1836)  class_acc: 0.3333 (0.3238)  loss_scale: 65536.0000 (40707.3833)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1781  max mem: 15572
[2025-01-16 05:41:14,489] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90569
[2025-01-16 05:41:14,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:41:14,489] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 690/2809]  eta: 0:20:21  lr: 0.000005  min_lr: 0.000000  loss: 4.0948 (4.1831)  class_acc: 0.3333 (0.3236)  loss_scale: 32768.0000 (40592.4863)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.1897  max mem: 15572
Epoch: [32]  [ 700/2809]  eta: 0:20:17  lr: 0.000005  min_lr: 0.000000  loss: 4.1204 (4.1829)  class_acc: 0.3750 (0.3248)  loss_scale: 32768.0000 (40480.8673)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1705  max mem: 15572
Epoch: [32]  [ 710/2809]  eta: 0:20:08  lr: 0.000005  min_lr: 0.000000  loss: 4.1783 (4.1845)  class_acc: 0.4167 (0.3261)  loss_scale: 32768.0000 (40372.3882)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1126  max mem: 15572
Epoch: [32]  [ 720/2809]  eta: 0:20:03  lr: 0.000005  min_lr: 0.000000  loss: 4.0769 (4.1822)  class_acc: 0.4167 (0.3265)  loss_scale: 32768.0000 (40266.9182)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1055  max mem: 15572
Epoch: [32]  [ 730/2809]  eta: 0:19:56  lr: 0.000005  min_lr: 0.000000  loss: 3.9921 (4.1804)  class_acc: 0.2917 (0.3264)  loss_scale: 32768.0000 (40164.3338)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1184  max mem: 15572
Epoch: [32]  [ 740/2809]  eta: 0:19:49  lr: 0.000005  min_lr: 0.000000  loss: 4.1965 (4.1810)  class_acc: 0.2917 (0.3257)  loss_scale: 32768.0000 (40064.5182)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0737  max mem: 15572
Epoch: [32]  [ 750/2809]  eta: 0:19:42  lr: 0.000005  min_lr: 0.000000  loss: 4.2434 (4.1809)  class_acc: 0.2917 (0.3262)  loss_scale: 32768.0000 (39967.3609)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0602  max mem: 15572
Epoch: [32]  [ 760/2809]  eta: 0:19:37  lr: 0.000005  min_lr: 0.000000  loss: 4.1549 (4.1817)  class_acc: 0.3333 (0.3269)  loss_scale: 32768.0000 (39872.7569)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0886  max mem: 15572
Epoch: [32]  [ 770/2809]  eta: 0:19:30  lr: 0.000005  min_lr: 0.000000  loss: 4.1628 (4.1817)  class_acc: 0.3333 (0.3268)  loss_scale: 32768.0000 (39780.6070)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1094  max mem: 15572
Epoch: [32]  [ 780/2809]  eta: 0:19:24  lr: 0.000005  min_lr: 0.000000  loss: 4.1245 (4.1798)  class_acc: 0.3333 (0.3274)  loss_scale: 32768.0000 (39690.8169)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0997  max mem: 15572
Epoch: [32]  [ 790/2809]  eta: 0:19:17  lr: 0.000005  min_lr: 0.000000  loss: 4.0903 (4.1787)  class_acc: 0.3750 (0.3275)  loss_scale: 32768.0000 (39603.2971)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1172  max mem: 15572
Epoch: [32]  [ 800/2809]  eta: 0:19:14  lr: 0.000005  min_lr: 0.000000  loss: 4.1444 (4.1798)  class_acc: 0.3750 (0.3281)  loss_scale: 32768.0000 (39517.9625)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1685  max mem: 15572
[2025-01-16 05:42:26,936] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:42:26,937] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 810/2809]  eta: 0:19:06  lr: 0.000005  min_lr: 0.000000  loss: 4.2121 (4.1809)  class_acc: 0.3750 (0.3283)  loss_scale: 32768.0000 (39475.1369)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1384  max mem: 15572
[2025-01-16 05:42:30,370] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90702
[2025-01-16 05:42:30,370] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:42:30,371] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 820/2809]  eta: 0:19:01  lr: 0.000005  min_lr: 0.000000  loss: 4.1885 (4.1819)  class_acc: 0.2500 (0.3275)  loss_scale: 32768.0000 (39513.1790)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1060  max mem: 15572
Epoch: [32]  [ 830/2809]  eta: 0:18:55  lr: 0.000005  min_lr: 0.000000  loss: 4.2747 (4.1840)  class_acc: 0.2500 (0.3269)  loss_scale: 32768.0000 (39432.0096)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1173  max mem: 15572
Epoch: [32]  [ 840/2809]  eta: 0:18:48  lr: 0.000005  min_lr: 0.000000  loss: 4.2747 (4.1835)  class_acc: 0.3333 (0.3275)  loss_scale: 32768.0000 (39352.7705)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0756  max mem: 15572
Epoch: [32]  [ 850/2809]  eta: 0:18:39  lr: 0.000005  min_lr: 0.000000  loss: 4.1732 (4.1819)  class_acc: 0.3750 (0.3278)  loss_scale: 32768.0000 (39275.3937)  weight_decay: 0.0500 (0.0500)  time: 0.4769  data: 0.0308  max mem: 15572
Epoch: [32]  [ 860/2809]  eta: 0:18:32  lr: 0.000005  min_lr: 0.000000  loss: 4.1967 (4.1828)  class_acc: 0.2917 (0.3271)  loss_scale: 32768.0000 (39199.8142)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0273  max mem: 15572
Epoch: [32]  [ 870/2809]  eta: 0:18:25  lr: 0.000005  min_lr: 0.000000  loss: 4.1967 (4.1833)  class_acc: 0.2917 (0.3274)  loss_scale: 32768.0000 (39125.9701)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0532  max mem: 15572
Epoch: [32]  [ 880/2809]  eta: 0:18:20  lr: 0.000005  min_lr: 0.000000  loss: 4.1448 (4.1829)  class_acc: 0.2917 (0.3270)  loss_scale: 32768.0000 (39053.8025)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1166  max mem: 15572
Epoch: [32]  [ 890/2809]  eta: 0:18:14  lr: 0.000005  min_lr: 0.000000  loss: 4.1916 (4.1824)  class_acc: 0.2917 (0.3266)  loss_scale: 32768.0000 (38983.2548)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1451  max mem: 15572
Epoch: [32]  [ 900/2809]  eta: 0:18:10  lr: 0.000005  min_lr: 0.000000  loss: 4.2577 (4.1849)  class_acc: 0.2917 (0.3258)  loss_scale: 32768.0000 (38914.2730)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1492  max mem: 15572
Epoch: [32]  [ 910/2809]  eta: 0:18:04  lr: 0.000005  min_lr: 0.000000  loss: 4.2577 (4.1819)  class_acc: 0.3333 (0.3270)  loss_scale: 32768.0000 (38846.8057)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1490  max mem: 15572
Epoch: [32]  [ 920/2809]  eta: 0:17:56  lr: 0.000005  min_lr: 0.000000  loss: 4.3399 (4.1839)  class_acc: 0.4167 (0.3272)  loss_scale: 32768.0000 (38780.8035)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0703  max mem: 15572
Epoch: [32]  [ 930/2809]  eta: 0:17:51  lr: 0.000005  min_lr: 0.000000  loss: 4.3399 (4.1832)  class_acc: 0.3750 (0.3277)  loss_scale: 32768.0000 (38716.2191)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0950  max mem: 15572
Epoch: [32]  [ 940/2809]  eta: 0:17:46  lr: 0.000005  min_lr: 0.000000  loss: 4.2748 (4.1849)  class_acc: 0.2917 (0.3270)  loss_scale: 32768.0000 (38653.0074)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1490  max mem: 15572
[2025-01-16 05:43:39,903] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:43:39,903] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 05:43:42,402] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90835
[2025-01-16 05:43:42,402] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:43:42,402] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 950/2809]  eta: 0:17:39  lr: 0.000005  min_lr: 0.000000  loss: 4.2748 (4.1846)  class_acc: 0.2917 (0.3275)  loss_scale: 32768.0000 (38728.9506)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1247  max mem: 15572
Epoch: [32]  [ 960/2809]  eta: 0:17:34  lr: 0.000005  min_lr: 0.000000  loss: 4.0401 (4.1838)  class_acc: 0.3333 (0.3276)  loss_scale: 32768.0000 (38666.9220)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1237  max mem: 15572
Epoch: [32]  [ 970/2809]  eta: 0:17:27  lr: 0.000005  min_lr: 0.000000  loss: 4.2423 (4.1855)  class_acc: 0.2500 (0.3265)  loss_scale: 32768.0000 (38606.1710)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.0959  max mem: 15572
Epoch: [32]  [ 980/2809]  eta: 0:17:20  lr: 0.000005  min_lr: 0.000000  loss: 4.1499 (4.1853)  class_acc: 0.2500 (0.3266)  loss_scale: 32768.0000 (38546.6585)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0579  max mem: 15572
Epoch: [32]  [ 990/2809]  eta: 0:17:14  lr: 0.000005  min_lr: 0.000000  loss: 4.0939 (4.1855)  class_acc: 0.3333 (0.3264)  loss_scale: 32768.0000 (38488.3471)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0830  max mem: 15572
Epoch: [32]  [1000/2809]  eta: 0:17:08  lr: 0.000005  min_lr: 0.000000  loss: 4.1873 (4.1861)  class_acc: 0.2500 (0.3254)  loss_scale: 32768.0000 (38431.2008)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1115  max mem: 15572
Epoch: [32]  [1010/2809]  eta: 0:17:04  lr: 0.000005  min_lr: 0.000000  loss: 4.1873 (4.1853)  class_acc: 0.3333 (0.3259)  loss_scale: 32768.0000 (38375.1850)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1429  max mem: 15572
Epoch: [32]  [1020/2809]  eta: 0:16:58  lr: 0.000005  min_lr: 0.000000  loss: 4.1642 (4.1851)  class_acc: 0.3750 (0.3263)  loss_scale: 32768.0000 (38320.2664)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1371  max mem: 15572
Epoch: [32]  [1030/2809]  eta: 0:16:52  lr: 0.000005  min_lr: 0.000000  loss: 4.2030 (4.1862)  class_acc: 0.3333 (0.3259)  loss_scale: 32768.0000 (38266.4132)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0871  max mem: 15572
Epoch: [32]  [1040/2809]  eta: 0:16:47  lr: 0.000005  min_lr: 0.000000  loss: 4.1020 (4.1849)  class_acc: 0.2917 (0.3258)  loss_scale: 32768.0000 (38213.5946)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1263  max mem: 15572
Epoch: [32]  [1050/2809]  eta: 0:16:40  lr: 0.000005  min_lr: 0.000000  loss: 4.0978 (4.1868)  class_acc: 0.2500 (0.3254)  loss_scale: 32768.0000 (38161.7812)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1256  max mem: 15572
Epoch: [32]  [1060/2809]  eta: 0:16:35  lr: 0.000005  min_lr: 0.000000  loss: 4.2861 (4.1869)  class_acc: 0.2500 (0.3248)  loss_scale: 32768.0000 (38110.9444)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1106  max mem: 15572
Epoch: [32]  [1070/2809]  eta: 0:16:30  lr: 0.000005  min_lr: 0.000000  loss: 4.2793 (4.1881)  class_acc: 0.2500 (0.3247)  loss_scale: 32768.0000 (38061.0570)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1523  max mem: 15572
[2025-01-16 05:44:56,081] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:44:56,082] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1080/2809]  eta: 0:16:25  lr: 0.000005  min_lr: 0.000000  loss: 4.1892 (4.1868)  class_acc: 0.2500 (0.3245)  loss_scale: 32768.0000 (38163.6559)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1551  max mem: 15572
Epoch: [32]  [1090/2809]  eta: 0:16:20  lr: 0.000005  min_lr: 0.000000  loss: 4.1103 (4.1863)  class_acc: 0.2917 (0.3243)  loss_scale: 65536.0000 (38414.5481)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1856  max mem: 15572
Epoch: [32]  [1100/2809]  eta: 0:16:14  lr: 0.000005  min_lr: 0.000000  loss: 4.1952 (4.1876)  class_acc: 0.2917 (0.3243)  loss_scale: 65536.0000 (38660.8828)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1703  max mem: 15572
Epoch: [32]  [1110/2809]  eta: 0:16:08  lr: 0.000005  min_lr: 0.000000  loss: 4.2204 (4.1873)  class_acc: 0.3333 (0.3243)  loss_scale: 65536.0000 (38902.7831)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1121  max mem: 15572
[2025-01-16 05:45:15,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=91000, skipped=564, lr=[5.128143724886411e-08, 5.128143724886411e-08, 7.325919606980588e-08, 7.325919606980588e-08, 1.0465599438543698e-07, 1.0465599438543698e-07, 1.4950856340776714e-07, 1.4950856340776714e-07, 2.135836620110959e-07, 2.135836620110959e-07, 3.051195171587084e-07, 3.051195171587084e-07, 4.3588502451244066e-07, 4.3588502451244066e-07, 6.226928921606296e-07, 6.226928921606296e-07, 8.895612745151851e-07, 8.895612745151851e-07, 1.2708018207359789e-06, 1.2708018207359789e-06, 1.8154311724799698e-06, 1.8154311724799698e-06, 2.593473103542814e-06, 2.593473103542814e-06, 3.704961576489735e-06, 3.704961576489735e-06, 5.292802252128193e-06, 5.292802252128193e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:45:15,603] [INFO] [timer.py:260:stop] epoch=0/micro_step=91000/global_step=91000, RunningAvgSamplesPerSec=27.947340012441053, CurrSamplesPerSec=31.261330075421, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-16 05:45:21,588] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91008
[2025-01-16 05:45:21,588] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:45:21,588] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [1120/2809]  eta: 0:16:03  lr: 0.000005  min_lr: 0.000000  loss: 4.2217 (4.1880)  class_acc: 0.3750 (0.3248)  loss_scale: 65536.0000 (39111.1365)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1430  max mem: 15572
Epoch: [32]  [1130/2809]  eta: 0:15:56  lr: 0.000005  min_lr: 0.000000  loss: 4.1430 (4.1873)  class_acc: 0.3750 (0.3245)  loss_scale: 32768.0000 (39055.0522)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1060  max mem: 15572
Epoch: [32]  [1140/2809]  eta: 0:15:52  lr: 0.000005  min_lr: 0.000000  loss: 4.0884 (4.1872)  class_acc: 0.3333 (0.3248)  loss_scale: 32768.0000 (38999.9509)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1070  max mem: 15572
Epoch: [32]  [1150/2809]  eta: 0:15:45  lr: 0.000005  min_lr: 0.000000  loss: 4.1363 (4.1864)  class_acc: 0.2917 (0.3246)  loss_scale: 32768.0000 (38945.8071)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1112  max mem: 15572
Epoch: [32]  [1160/2809]  eta: 0:15:39  lr: 0.000005  min_lr: 0.000000  loss: 4.0864 (4.1861)  class_acc: 0.2500 (0.3242)  loss_scale: 32768.0000 (38892.5960)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0764  max mem: 15572
Epoch: [32]  [1170/2809]  eta: 0:15:34  lr: 0.000005  min_lr: 0.000000  loss: 4.2401 (4.1872)  class_acc: 0.2500 (0.3239)  loss_scale: 32768.0000 (38840.2938)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1277  max mem: 15572
Epoch: [32]  [1180/2809]  eta: 0:15:27  lr: 0.000005  min_lr: 0.000000  loss: 4.2421 (4.1879)  class_acc: 0.3333 (0.3242)  loss_scale: 32768.0000 (38788.8772)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0944  max mem: 15572
Epoch: [32]  [1190/2809]  eta: 0:15:21  lr: 0.000005  min_lr: 0.000000  loss: 4.2421 (4.1886)  class_acc: 0.3333 (0.3235)  loss_scale: 32768.0000 (38738.3241)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0928  max mem: 15572
Epoch: [32]  [1200/2809]  eta: 0:15:16  lr: 0.000005  min_lr: 0.000000  loss: 4.1952 (4.1879)  class_acc: 0.2500 (0.3232)  loss_scale: 32768.0000 (38688.6128)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1332  max mem: 15572
Epoch: [32]  [1210/2809]  eta: 0:15:12  lr: 0.000005  min_lr: 0.000000  loss: 4.1727 (4.1889)  class_acc: 0.3333 (0.3240)  loss_scale: 32768.0000 (38639.7225)  weight_decay: 0.0500 (0.0500)  time: 0.6568  data: 0.2025  max mem: 15572
Epoch: [32]  [1220/2809]  eta: 0:15:06  lr: 0.000005  min_lr: 0.000000  loss: 4.2221 (4.1884)  class_acc: 0.3750 (0.3240)  loss_scale: 32768.0000 (38591.6331)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1758  max mem: 15572
Epoch: [32]  [1230/2809]  eta: 0:15:00  lr: 0.000005  min_lr: 0.000000  loss: 4.2086 (4.1875)  class_acc: 0.3750 (0.3241)  loss_scale: 32768.0000 (38544.3249)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.1017  max mem: 15572
Epoch: [32]  [1240/2809]  eta: 0:14:53  lr: 0.000005  min_lr: 0.000000  loss: 4.0926 (4.1873)  class_acc: 0.3333 (0.3241)  loss_scale: 32768.0000 (38497.7792)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0711  max mem: 15572
[2025-01-16 05:46:33,829] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:46:33,830] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1250/2809]  eta: 0:14:47  lr: 0.000005  min_lr: 0.000000  loss: 4.3205 (4.1886)  class_acc: 0.2500 (0.3235)  loss_scale: 32768.0000 (38504.3645)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0667  max mem: 15572
[2025-01-16 05:46:39,018] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91146
[2025-01-16 05:46:39,018] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:46:39,020] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [1260/2809]  eta: 0:14:41  lr: 0.000005  min_lr: 0.000000  loss: 4.3414 (4.1897)  class_acc: 0.2500 (0.3234)  loss_scale: 32768.0000 (38640.7740)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0964  max mem: 15572
Epoch: [32]  [1270/2809]  eta: 0:14:35  lr: 0.000005  min_lr: 0.000000  loss: 4.1924 (4.1900)  class_acc: 0.3333 (0.3236)  loss_scale: 32768.0000 (38594.5681)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0707  max mem: 15572
Epoch: [32]  [1280/2809]  eta: 0:14:29  lr: 0.000005  min_lr: 0.000000  loss: 4.2462 (4.1905)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (38549.0835)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0926  max mem: 15572
Epoch: [32]  [1290/2809]  eta: 0:14:23  lr: 0.000005  min_lr: 0.000000  loss: 4.1800 (4.1901)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (38504.3036)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1050  max mem: 15572
Epoch: [32]  [1300/2809]  eta: 0:14:17  lr: 0.000005  min_lr: 0.000000  loss: 4.0530 (4.1893)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (38460.2121)  weight_decay: 0.0500 (0.0500)  time: 0.5176  data: 0.0487  max mem: 15572
Epoch: [32]  [1310/2809]  eta: 0:14:11  lr: 0.000005  min_lr: 0.000000  loss: 4.1679 (4.1904)  class_acc: 0.2917 (0.3224)  loss_scale: 32768.0000 (38416.7933)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0689  max mem: 15572
Epoch: [32]  [1320/2809]  eta: 0:14:07  lr: 0.000005  min_lr: 0.000000  loss: 4.1679 (4.1898)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (38374.0318)  weight_decay: 0.0500 (0.0500)  time: 0.6572  data: 0.2011  max mem: 15572
Epoch: [32]  [1330/2809]  eta: 0:14:00  lr: 0.000005  min_lr: 0.000000  loss: 4.0940 (4.1897)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (38331.9128)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1422  max mem: 15572
Epoch: [32]  [1340/2809]  eta: 0:13:54  lr: 0.000005  min_lr: 0.000000  loss: 4.3153 (4.1914)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (38290.4221)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0551  max mem: 15572
Epoch: [32]  [1350/2809]  eta: 0:13:48  lr: 0.000005  min_lr: 0.000000  loss: 4.2352 (4.1904)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (38249.5455)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0838  max mem: 15572
Epoch: [32]  [1360/2809]  eta: 0:13:43  lr: 0.000005  min_lr: 0.000000  loss: 4.0915 (4.1911)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (38209.2697)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1250  max mem: 15572
Epoch: [32]  [1370/2809]  eta: 0:13:37  lr: 0.000005  min_lr: 0.000000  loss: 4.2736 (4.1924)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (38169.5813)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1731  max mem: 15572
Epoch: [32]  [1380/2809]  eta: 0:13:32  lr: 0.000005  min_lr: 0.000000  loss: 4.2974 (4.1930)  class_acc: 0.2500 (0.3216)  loss_scale: 32768.0000 (38130.4678)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1280  max mem: 15572
[2025-01-16 05:47:52,261] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:47:52,261] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1390/2809]  eta: 0:13:27  lr: 0.000005  min_lr: 0.000000  loss: 4.2429 (4.1928)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (38186.1452)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1588  max mem: 15572
Epoch: [32]  [1400/2809]  eta: 0:13:22  lr: 0.000005  min_lr: 0.000000  loss: 4.1762 (4.1923)  class_acc: 0.3750 (0.3225)  loss_scale: 65536.0000 (38381.3619)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.2178  max mem: 15572
Epoch: [32]  [1410/2809]  eta: 0:13:16  lr: 0.000005  min_lr: 0.000000  loss: 4.2148 (4.1934)  class_acc: 0.3750 (0.3225)  loss_scale: 65536.0000 (38573.8115)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1613  max mem: 15572
Epoch: [32]  [1420/2809]  eta: 0:13:10  lr: 0.000005  min_lr: 0.000000  loss: 4.2532 (4.1934)  class_acc: 0.3333 (0.3225)  loss_scale: 65536.0000 (38763.5524)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0985  max mem: 15572
Epoch: [32]  [1430/2809]  eta: 0:13:05  lr: 0.000005  min_lr: 0.000000  loss: 4.1205 (4.1933)  class_acc: 0.3333 (0.3226)  loss_scale: 65536.0000 (38950.6415)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0985  max mem: 15572
Epoch: [32]  [1440/2809]  eta: 0:13:00  lr: 0.000005  min_lr: 0.000000  loss: 4.1875 (4.1941)  class_acc: 0.3333 (0.3227)  loss_scale: 65536.0000 (39135.1339)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1552  max mem: 15572
[2025-01-16 05:48:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91334
[2025-01-16 05:48:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:48:26,580] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [1450/2809]  eta: 0:12:53  lr: 0.000005  min_lr: 0.000000  loss: 4.0834 (4.1932)  class_acc: 0.3750 (0.3232)  loss_scale: 65536.0000 (39204.1682)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1249  max mem: 15572
Epoch: [32]  [1460/2809]  eta: 0:12:48  lr: 0.000005  min_lr: 0.000000  loss: 4.0799 (4.1929)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (39160.1150)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1065  max mem: 15572
Epoch: [32]  [1470/2809]  eta: 0:12:41  lr: 0.000005  min_lr: 0.000000  loss: 4.2146 (4.1929)  class_acc: 0.2917 (0.3229)  loss_scale: 32768.0000 (39116.6608)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0892  max mem: 15572
Epoch: [32]  [1480/2809]  eta: 0:12:36  lr: 0.000005  min_lr: 0.000000  loss: 4.1225 (4.1922)  class_acc: 0.2917 (0.3230)  loss_scale: 32768.0000 (39073.7934)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0677  max mem: 15572
Epoch: [32]  [1490/2809]  eta: 0:12:29  lr: 0.000005  min_lr: 0.000000  loss: 4.2418 (4.1935)  class_acc: 0.2917 (0.3229)  loss_scale: 32768.0000 (39031.5010)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0857  max mem: 15572
Epoch: [32]  [1500/2809]  eta: 0:12:24  lr: 0.000005  min_lr: 0.000000  loss: 4.2615 (4.1929)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (38989.7722)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0907  max mem: 15572
Epoch: [32]  [1510/2809]  eta: 0:12:18  lr: 0.000005  min_lr: 0.000000  loss: 4.2401 (4.1934)  class_acc: 0.3750 (0.3231)  loss_scale: 32768.0000 (38948.5956)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1140  max mem: 15572
Epoch: [32]  [1520/2809]  eta: 0:12:13  lr: 0.000005  min_lr: 0.000000  loss: 4.2745 (4.1929)  class_acc: 0.2917 (0.3229)  loss_scale: 32768.0000 (38907.9606)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1106  max mem: 15572
Epoch: [32]  [1530/2809]  eta: 0:12:06  lr: 0.000005  min_lr: 0.000000  loss: 4.3313 (4.1940)  class_acc: 0.3333 (0.3231)  loss_scale: 32768.0000 (38867.8563)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0889  max mem: 15572
Epoch: [32]  [1540/2809]  eta: 0:12:01  lr: 0.000005  min_lr: 0.000000  loss: 4.3714 (4.1946)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (38828.2726)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0935  max mem: 15572
Epoch: [32]  [1550/2809]  eta: 0:11:55  lr: 0.000005  min_lr: 0.000000  loss: 4.3831 (4.1953)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (38789.1992)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1025  max mem: 15572
Epoch: [32]  [1560/2809]  eta: 0:11:49  lr: 0.000005  min_lr: 0.000000  loss: 4.2086 (4.1950)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (38750.6265)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1025  max mem: 15572
Epoch: [32]  [1570/2809]  eta: 0:11:43  lr: 0.000005  min_lr: 0.000000  loss: 4.2086 (4.1956)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (38712.5449)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0785  max mem: 15572
[2025-01-16 05:49:36,120] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:49:36,121] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1580/2809]  eta: 0:11:36  lr: 0.000005  min_lr: 0.000000  loss: 4.0736 (4.1943)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (38799.3017)  weight_decay: 0.0500 (0.0500)  time: 0.4907  data: 0.0275  max mem: 15572
Epoch: [32]  [1590/2809]  eta: 0:11:32  lr: 0.000005  min_lr: 0.000000  loss: 4.0736 (4.1938)  class_acc: 0.2917 (0.3227)  loss_scale: 65536.0000 (38967.3514)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1225  max mem: 15572
Epoch: [32]  [1600/2809]  eta: 0:11:26  lr: 0.000005  min_lr: 0.000000  loss: 4.1578 (4.1933)  class_acc: 0.3333 (0.3230)  loss_scale: 65536.0000 (39133.3017)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1559  max mem: 15572
Epoch: [32]  [1610/2809]  eta: 0:11:19  lr: 0.000005  min_lr: 0.000000  loss: 4.1635 (4.1918)  class_acc: 0.3333 (0.3231)  loss_scale: 65536.0000 (39297.1918)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0605  max mem: 15572
Epoch: [32]  [1620/2809]  eta: 0:11:14  lr: 0.000005  min_lr: 0.000000  loss: 4.1417 (4.1919)  class_acc: 0.2917 (0.3230)  loss_scale: 65536.0000 (39459.0598)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0970  max mem: 15572
[2025-01-16 05:50:02,066] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91509
[2025-01-16 05:50:02,066] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:50:02,066] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [1630/2809]  eta: 0:11:08  lr: 0.000005  min_lr: 0.000000  loss: 4.1876 (4.1917)  class_acc: 0.2500 (0.3225)  loss_scale: 32768.0000 (39418.0356)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1106  max mem: 15572
Epoch: [32]  [1640/2809]  eta: 0:11:02  lr: 0.000005  min_lr: 0.000000  loss: 4.3220 (4.1930)  class_acc: 0.2083 (0.3220)  loss_scale: 32768.0000 (39377.5113)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0964  max mem: 15572
Epoch: [32]  [1650/2809]  eta: 0:10:57  lr: 0.000005  min_lr: 0.000000  loss: 4.3321 (4.1931)  class_acc: 0.2500 (0.3219)  loss_scale: 32768.0000 (39337.4779)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1376  max mem: 15572
Epoch: [32]  [1660/2809]  eta: 0:10:51  lr: 0.000005  min_lr: 0.000000  loss: 4.2541 (4.1937)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (39297.9266)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1165  max mem: 15572
Epoch: [32]  [1670/2809]  eta: 0:10:45  lr: 0.000005  min_lr: 0.000000  loss: 4.1909 (4.1930)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (39258.8486)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0825  max mem: 15572
Epoch: [32]  [1680/2809]  eta: 0:10:40  lr: 0.000005  min_lr: 0.000000  loss: 4.0884 (4.1930)  class_acc: 0.2917 (0.3218)  loss_scale: 32768.0000 (39220.2356)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1028  max mem: 15572
Epoch: [32]  [1690/2809]  eta: 0:10:34  lr: 0.000005  min_lr: 0.000000  loss: 4.2194 (4.1931)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (39182.0792)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1193  max mem: 15572
Epoch: [32]  [1700/2809]  eta: 0:10:28  lr: 0.000005  min_lr: 0.000000  loss: 4.2194 (4.1924)  class_acc: 0.2500 (0.3214)  loss_scale: 32768.0000 (39144.3715)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1186  max mem: 15572
Epoch: [32]  [1710/2809]  eta: 0:10:23  lr: 0.000005  min_lr: 0.000000  loss: 4.2665 (4.1930)  class_acc: 0.2500 (0.3212)  loss_scale: 32768.0000 (39107.1046)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1614  max mem: 15572
Epoch: [32]  [1720/2809]  eta: 0:10:17  lr: 0.000005  min_lr: 0.000000  loss: 4.2832 (4.1930)  class_acc: 0.2917 (0.3212)  loss_scale: 32768.0000 (39070.2708)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1006  max mem: 15572
Epoch: [32]  [1730/2809]  eta: 0:10:12  lr: 0.000005  min_lr: 0.000000  loss: 4.2861 (4.1942)  class_acc: 0.2917 (0.3210)  loss_scale: 32768.0000 (39033.8625)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0827  max mem: 15572
Epoch: [32]  [1740/2809]  eta: 0:10:06  lr: 0.000005  min_lr: 0.000000  loss: 4.3187 (4.1950)  class_acc: 0.2500 (0.3206)  loss_scale: 32768.0000 (38997.8725)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1717  max mem: 15572
[2025-01-16 05:51:15,678] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:51:15,678] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1750/2809]  eta: 0:10:00  lr: 0.000005  min_lr: 0.000000  loss: 4.2859 (4.1949)  class_acc: 0.2500 (0.3207)  loss_scale: 32768.0000 (38981.0074)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1579  max mem: 15572
Epoch: [32]  [1760/2809]  eta: 0:09:55  lr: 0.000005  min_lr: 0.000000  loss: 4.1902 (4.1946)  class_acc: 0.2917 (0.3208)  loss_scale: 65536.0000 (39131.8024)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1057  max mem: 15572
[2025-01-16 05:51:22,869] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91652
[2025-01-16 05:51:22,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:51:22,869] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [1770/2809]  eta: 0:09:49  lr: 0.000005  min_lr: 0.000000  loss: 4.1995 (4.1946)  class_acc: 0.2917 (0.3205)  loss_scale: 65536.0000 (39151.3766)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1227  max mem: 15572
Epoch: [32]  [1780/2809]  eta: 0:09:43  lr: 0.000005  min_lr: 0.000000  loss: 4.1676 (4.1939)  class_acc: 0.2917 (0.3205)  loss_scale: 32768.0000 (39115.5351)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1072  max mem: 15572
Epoch: [32]  [1790/2809]  eta: 0:09:37  lr: 0.000005  min_lr: 0.000000  loss: 4.1164 (4.1941)  class_acc: 0.3333 (0.3208)  loss_scale: 32768.0000 (39080.0938)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0767  max mem: 15572
Epoch: [32]  [1800/2809]  eta: 0:09:31  lr: 0.000005  min_lr: 0.000000  loss: 4.2355 (4.1945)  class_acc: 0.2917 (0.3205)  loss_scale: 32768.0000 (39045.0461)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0926  max mem: 15572
Epoch: [32]  [1810/2809]  eta: 0:09:26  lr: 0.000005  min_lr: 0.000000  loss: 4.2355 (4.1942)  class_acc: 0.3333 (0.3206)  loss_scale: 32768.0000 (39010.3854)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1130  max mem: 15572
Epoch: [32]  [1820/2809]  eta: 0:09:20  lr: 0.000005  min_lr: 0.000000  loss: 4.1976 (4.1941)  class_acc: 0.3750 (0.3208)  loss_scale: 32768.0000 (38976.1054)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1269  max mem: 15572
Epoch: [32]  [1830/2809]  eta: 0:09:15  lr: 0.000005  min_lr: 0.000000  loss: 4.2710 (4.1950)  class_acc: 0.3333 (0.3206)  loss_scale: 32768.0000 (38942.1999)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1312  max mem: 15572
Epoch: [32]  [1840/2809]  eta: 0:09:09  lr: 0.000005  min_lr: 0.000000  loss: 4.2861 (4.1958)  class_acc: 0.3333 (0.3206)  loss_scale: 32768.0000 (38908.6627)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1470  max mem: 15572
Epoch: [32]  [1850/2809]  eta: 0:09:04  lr: 0.000005  min_lr: 0.000000  loss: 4.3134 (4.1962)  class_acc: 0.3333 (0.3206)  loss_scale: 32768.0000 (38875.4878)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1740  max mem: 15572
Epoch: [32]  [1860/2809]  eta: 0:08:58  lr: 0.000005  min_lr: 0.000000  loss: 4.0925 (4.1959)  class_acc: 0.3333 (0.3209)  loss_scale: 32768.0000 (38842.6695)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1130  max mem: 15572
Epoch: [32]  [1870/2809]  eta: 0:08:52  lr: 0.000005  min_lr: 0.000000  loss: 4.2689 (4.1968)  class_acc: 0.3333 (0.3206)  loss_scale: 32768.0000 (38810.2020)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0961  max mem: 15572
Epoch: [32]  [1880/2809]  eta: 0:08:46  lr: 0.000005  min_lr: 0.000000  loss: 4.2689 (4.1963)  class_acc: 0.2500 (0.3208)  loss_scale: 32768.0000 (38778.0797)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1069  max mem: 15572
Epoch: [32]  [1890/2809]  eta: 0:08:40  lr: 0.000005  min_lr: 0.000000  loss: 4.1192 (4.1959)  class_acc: 0.3333 (0.3211)  loss_scale: 32768.0000 (38746.2972)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0661  max mem: 15572
[2025-01-16 05:52:35,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:52:35,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1900/2809]  eta: 0:08:35  lr: 0.000005  min_lr: 0.000000  loss: 4.2233 (4.1964)  class_acc: 0.3333 (0.3209)  loss_scale: 32768.0000 (38852.7470)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1400  max mem: 15572
[2025-01-16 05:52:43,087] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91793
[2025-01-16 05:52:43,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:52:43,088] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [1910/2809]  eta: 0:08:30  lr: 0.000005  min_lr: 0.000000  loss: 4.2296 (4.1960)  class_acc: 0.2917 (0.3214)  loss_scale: 65536.0000 (38889.4945)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1609  max mem: 15572
[2025-01-16 05:52:48,614] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91803
[2025-01-16 05:52:48,614] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 05:52:48,614] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [1920/2809]  eta: 0:08:24  lr: 0.000005  min_lr: 0.000000  loss: 4.1817 (4.1966)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (38806.4550)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0838  max mem: 15572
Epoch: [32]  [1930/2809]  eta: 0:08:18  lr: 0.000005  min_lr: 0.000000  loss: 4.1887 (4.1959)  class_acc: 0.3333 (0.3216)  loss_scale: 16384.0000 (38690.3366)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0869  max mem: 15572
Epoch: [32]  [1940/2809]  eta: 0:08:13  lr: 0.000005  min_lr: 0.000000  loss: 4.1746 (4.1962)  class_acc: 0.2917 (0.3213)  loss_scale: 16384.0000 (38575.4147)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1513  max mem: 15572
Epoch: [32]  [1950/2809]  eta: 0:08:07  lr: 0.000005  min_lr: 0.000000  loss: 4.2850 (4.1963)  class_acc: 0.2917 (0.3215)  loss_scale: 16384.0000 (38461.6709)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.1556  max mem: 15572
Epoch: [32]  [1960/2809]  eta: 0:08:01  lr: 0.000005  min_lr: 0.000000  loss: 4.1779 (4.1963)  class_acc: 0.2917 (0.3215)  loss_scale: 16384.0000 (38349.0872)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0932  max mem: 15572
Epoch: [32]  [1970/2809]  eta: 0:07:56  lr: 0.000005  min_lr: 0.000000  loss: 4.1641 (4.1955)  class_acc: 0.3750 (0.3218)  loss_scale: 16384.0000 (38237.6459)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0852  max mem: 15572
Epoch: [32]  [1980/2809]  eta: 0:07:50  lr: 0.000005  min_lr: 0.000000  loss: 4.1046 (4.1947)  class_acc: 0.3750 (0.3220)  loss_scale: 16384.0000 (38127.3296)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1429  max mem: 15572
Epoch: [32]  [1990/2809]  eta: 0:07:45  lr: 0.000005  min_lr: 0.000000  loss: 4.2049 (4.1947)  class_acc: 0.3333 (0.3221)  loss_scale: 16384.0000 (38018.1215)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1496  max mem: 15572
Epoch: [32]  [2000/2809]  eta: 0:07:39  lr: 0.000005  min_lr: 0.000000  loss: 4.1855 (4.1950)  class_acc: 0.2917 (0.3219)  loss_scale: 16384.0000 (37910.0050)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1005  max mem: 15572
Epoch: [32]  [2010/2809]  eta: 0:07:33  lr: 0.000005  min_lr: 0.000000  loss: 4.1326 (4.1950)  class_acc: 0.2500 (0.3215)  loss_scale: 16384.0000 (37802.9637)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0840  max mem: 15572
Epoch: [32]  [2020/2809]  eta: 0:07:28  lr: 0.000005  min_lr: 0.000000  loss: 4.2238 (4.1956)  class_acc: 0.2917 (0.3215)  loss_scale: 16384.0000 (37696.9817)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1568  max mem: 15572
Epoch: [32]  [2030/2809]  eta: 0:07:22  lr: 0.000005  min_lr: 0.000000  loss: 4.3665 (4.1965)  class_acc: 0.3333 (0.3217)  loss_scale: 16384.0000 (37592.0433)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1071  max mem: 15572
Epoch: [32]  [2040/2809]  eta: 0:07:16  lr: 0.000005  min_lr: 0.000000  loss: 4.2058 (4.1963)  class_acc: 0.3750 (0.3220)  loss_scale: 16384.0000 (37488.1333)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0268  max mem: 15572
[2025-01-16 05:54:01,847] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:54:01,848] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [2050/2809]  eta: 0:07:10  lr: 0.000005  min_lr: 0.000000  loss: 4.2059 (4.1971)  class_acc: 0.3333 (0.3218)  loss_scale: 16384.0000 (37441.1546)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1013  max mem: 15572
Epoch: [32]  [2060/2809]  eta: 0:07:04  lr: 0.000005  min_lr: 0.000000  loss: 4.3290 (4.1970)  class_acc: 0.3333 (0.3220)  loss_scale: 32768.0000 (37418.4803)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1037  max mem: 15572
Epoch: [32]  [2070/2809]  eta: 0:06:58  lr: 0.000005  min_lr: 0.000000  loss: 4.2928 (4.1968)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (37396.0251)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0293  max mem: 15572
Epoch: [32]  [2080/2809]  eta: 0:06:53  lr: 0.000005  min_lr: 0.000000  loss: 4.0077 (4.1962)  class_acc: 0.3333 (0.3221)  loss_scale: 32768.0000 (37373.7857)  weight_decay: 0.0500 (0.0500)  time: 0.4855  data: 0.0136  max mem: 15572
Epoch: [32]  [2090/2809]  eta: 0:06:47  lr: 0.000005  min_lr: 0.000000  loss: 3.9692 (4.1961)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (37351.7590)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0614  max mem: 15572
Epoch: [32]  [2100/2809]  eta: 0:06:41  lr: 0.000005  min_lr: 0.000000  loss: 4.2262 (4.1968)  class_acc: 0.3333 (0.3221)  loss_scale: 32768.0000 (37329.9419)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0893  max mem: 15572
Epoch: [32]  [2110/2809]  eta: 0:06:35  lr: 0.000005  min_lr: 0.000000  loss: 4.2262 (4.1963)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (37308.3316)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.1050  max mem: 15572
[2025-01-16 05:54:38,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=92000, skipped=571, lr=[4.679941201088924e-08, 4.679941201088924e-08, 6.685630287269892e-08, 6.685630287269892e-08, 9.550900410385561e-08, 9.550900410385561e-08, 1.3644143443407946e-07, 1.3644143443407946e-07, 1.949163349058278e-07, 1.949163349058278e-07, 2.784519070083254e-07, 2.784519070083254e-07, 3.9778843858332207e-07, 3.9778843858332207e-07, 5.682691979761744e-07, 5.682691979761744e-07, 8.118131399659635e-07, 8.118131399659635e-07, 1.1597330570942337e-06, 1.1597330570942337e-06, 1.6567615101346195e-06, 1.6567615101346195e-06, 2.366802157335171e-06, 2.366802157335171e-06, 3.3811459390502447e-06, 3.3811459390502447e-06, 4.830208484357493e-06, 4.830208484357493e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:54:38,612] [INFO] [timer.py:260:stop] epoch=0/micro_step=92000/global_step=92000, RunningAvgSamplesPerSec=27.94852483167663, CurrSamplesPerSec=28.220938095392956, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [32]  [2120/2809]  eta: 0:06:30  lr: 0.000005  min_lr: 0.000000  loss: 4.1869 (4.1963)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (37286.9250)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1162  max mem: 15572
Epoch: [32]  [2130/2809]  eta: 0:06:24  lr: 0.000005  min_lr: 0.000000  loss: 4.2477 (4.1966)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37265.7194)  weight_decay: 0.0500 (0.0500)  time: 0.5151  data: 0.0675  max mem: 15572
Epoch: [32]  [2140/2809]  eta: 0:06:18  lr: 0.000005  min_lr: 0.000000  loss: 4.2070 (4.1960)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (37244.7118)  weight_decay: 0.0500 (0.0500)  time: 0.4940  data: 0.0397  max mem: 15572
Epoch: [32]  [2150/2809]  eta: 0:06:12  lr: 0.000005  min_lr: 0.000000  loss: 4.1054 (4.1958)  class_acc: 0.3750 (0.3222)  loss_scale: 32768.0000 (37223.8996)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0847  max mem: 15572
Epoch: [32]  [2160/2809]  eta: 0:06:06  lr: 0.000005  min_lr: 0.000000  loss: 4.1725 (4.1958)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (37203.2800)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0896  max mem: 15572
Epoch: [32]  [2170/2809]  eta: 0:06:01  lr: 0.000005  min_lr: 0.000000  loss: 4.2532 (4.1958)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (37182.8503)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0517  max mem: 15572
[2025-01-16 05:55:10,819] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:55:10,819] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 05:55:14,106] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92066
[2025-01-16 05:55:14,106] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:55:14,106] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [2180/2809]  eta: 0:05:55  lr: 0.000005  min_lr: 0.000000  loss: 4.2641 (4.1959)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (37252.7538)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0692  max mem: 15572
Epoch: [32]  [2190/2809]  eta: 0:05:49  lr: 0.000005  min_lr: 0.000000  loss: 4.2664 (4.1961)  class_acc: 0.2500 (0.3220)  loss_scale: 32768.0000 (37232.2848)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0897  max mem: 15572
Epoch: [32]  [2200/2809]  eta: 0:05:44  lr: 0.000005  min_lr: 0.000000  loss: 4.2997 (4.1965)  class_acc: 0.3333 (0.3219)  loss_scale: 32768.0000 (37212.0018)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1217  max mem: 15572
Epoch: [32]  [2210/2809]  eta: 0:05:38  lr: 0.000005  min_lr: 0.000000  loss: 4.1954 (4.1956)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (37191.9023)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1199  max mem: 15572
Epoch: [32]  [2220/2809]  eta: 0:05:32  lr: 0.000005  min_lr: 0.000000  loss: 4.1392 (4.1954)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (37171.9838)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0867  max mem: 15572
Epoch: [32]  [2230/2809]  eta: 0:05:27  lr: 0.000005  min_lr: 0.000000  loss: 4.2009 (4.1957)  class_acc: 0.2917 (0.3224)  loss_scale: 32768.0000 (37152.2438)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1028  max mem: 15572
Epoch: [32]  [2240/2809]  eta: 0:05:21  lr: 0.000005  min_lr: 0.000000  loss: 4.1791 (4.1955)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (37132.6801)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1531  max mem: 15572
Epoch: [32]  [2250/2809]  eta: 0:05:15  lr: 0.000005  min_lr: 0.000000  loss: 4.1276 (4.1954)  class_acc: 0.2500 (0.3223)  loss_scale: 32768.0000 (37113.2901)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1199  max mem: 15572
Epoch: [32]  [2260/2809]  eta: 0:05:10  lr: 0.000005  min_lr: 0.000000  loss: 4.0156 (4.1946)  class_acc: 0.2917 (0.3224)  loss_scale: 32768.0000 (37094.0716)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1161  max mem: 15572
Epoch: [32]  [2270/2809]  eta: 0:05:04  lr: 0.000005  min_lr: 0.000000  loss: 4.1860 (4.1953)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (37075.0225)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1372  max mem: 15572
Epoch: [32]  [2280/2809]  eta: 0:04:59  lr: 0.000005  min_lr: 0.000000  loss: 4.2931 (4.1951)  class_acc: 0.2917 (0.3225)  loss_scale: 32768.0000 (37056.1403)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1027  max mem: 15572
Epoch: [32]  [2290/2809]  eta: 0:04:53  lr: 0.000005  min_lr: 0.000000  loss: 4.2289 (4.1955)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (37037.4230)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1428  max mem: 15572
Epoch: [32]  [2300/2809]  eta: 0:04:47  lr: 0.000005  min_lr: 0.000000  loss: 4.2659 (4.1955)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (37018.8683)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1171  max mem: 15572
[2025-01-16 05:56:27,434] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:56:27,434] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [2310/2809]  eta: 0:04:42  lr: 0.000005  min_lr: 0.000000  loss: 4.2883 (4.1956)  class_acc: 0.2917 (0.3224)  loss_scale: 32768.0000 (37057.1908)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0979  max mem: 15572
Epoch: [32]  [2320/2809]  eta: 0:04:36  lr: 0.000005  min_lr: 0.000000  loss: 4.2351 (4.1956)  class_acc: 0.2500 (0.3223)  loss_scale: 65536.0000 (37179.8914)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1060  max mem: 15572
Epoch: [32]  [2330/2809]  eta: 0:04:30  lr: 0.000005  min_lr: 0.000000  loss: 4.2097 (4.1959)  class_acc: 0.2500 (0.3220)  loss_scale: 65536.0000 (37301.5393)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0850  max mem: 15572
Epoch: [32]  [2340/2809]  eta: 0:04:25  lr: 0.000005  min_lr: 0.000000  loss: 4.2097 (4.1960)  class_acc: 0.2500 (0.3220)  loss_scale: 65536.0000 (37422.1478)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1333  max mem: 15572
Epoch: [32]  [2350/2809]  eta: 0:04:19  lr: 0.000005  min_lr: 0.000000  loss: 4.1341 (4.1958)  class_acc: 0.2917 (0.3218)  loss_scale: 65536.0000 (37541.7303)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1486  max mem: 15572
Epoch: [32]  [2360/2809]  eta: 0:04:13  lr: 0.000005  min_lr: 0.000000  loss: 4.1226 (4.1962)  class_acc: 0.2500 (0.3217)  loss_scale: 65536.0000 (37660.2999)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1196  max mem: 15572
[2025-01-16 05:56:58,098] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92250
[2025-01-16 05:56:58,098] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:56:58,098] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [2370/2809]  eta: 0:04:08  lr: 0.000005  min_lr: 0.000000  loss: 4.1648 (4.1961)  class_acc: 0.2500 (0.3216)  loss_scale: 65536.0000 (37653.4863)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.1131  max mem: 15572
Epoch: [32]  [2380/2809]  eta: 0:04:02  lr: 0.000005  min_lr: 0.000000  loss: 4.1888 (4.1962)  class_acc: 0.2500 (0.3215)  loss_scale: 32768.0000 (37632.9677)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.1055  max mem: 15572
Epoch: [32]  [2390/2809]  eta: 0:03:56  lr: 0.000005  min_lr: 0.000000  loss: 4.2453 (4.1961)  class_acc: 0.2500 (0.3214)  loss_scale: 32768.0000 (37612.6207)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1117  max mem: 15572
Epoch: [32]  [2400/2809]  eta: 0:03:51  lr: 0.000005  min_lr: 0.000000  loss: 4.2258 (4.1963)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (37592.4431)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1056  max mem: 15572
Epoch: [32]  [2410/2809]  eta: 0:03:45  lr: 0.000005  min_lr: 0.000000  loss: 4.2402 (4.1966)  class_acc: 0.3333 (0.3217)  loss_scale: 32768.0000 (37572.4330)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1142  max mem: 15572
Epoch: [32]  [2420/2809]  eta: 0:03:40  lr: 0.000005  min_lr: 0.000000  loss: 4.2686 (4.1971)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (37552.5882)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.1894  max mem: 15572
Epoch: [32]  [2430/2809]  eta: 0:03:34  lr: 0.000005  min_lr: 0.000000  loss: 4.3926 (4.1973)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (37532.9066)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.1951  max mem: 15572
Epoch: [32]  [2440/2809]  eta: 0:03:28  lr: 0.000005  min_lr: 0.000000  loss: 4.3067 (4.1972)  class_acc: 0.2500 (0.3213)  loss_scale: 32768.0000 (37513.3863)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1297  max mem: 15572
Epoch: [32]  [2450/2809]  eta: 0:03:23  lr: 0.000005  min_lr: 0.000000  loss: 4.2766 (4.1976)  class_acc: 0.2500 (0.3212)  loss_scale: 32768.0000 (37494.0253)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1331  max mem: 15572
Epoch: [32]  [2460/2809]  eta: 0:03:17  lr: 0.000005  min_lr: 0.000000  loss: 4.2708 (4.1978)  class_acc: 0.2500 (0.3213)  loss_scale: 32768.0000 (37474.8216)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0847  max mem: 15572
Epoch: [32]  [2470/2809]  eta: 0:03:11  lr: 0.000005  min_lr: 0.000000  loss: 4.1916 (4.1978)  class_acc: 0.3333 (0.3214)  loss_scale: 32768.0000 (37455.7734)  weight_decay: 0.0500 (0.0500)  time: 0.4698  data: 0.0367  max mem: 15572
Epoch: [32]  [2480/2809]  eta: 0:03:06  lr: 0.000005  min_lr: 0.000000  loss: 4.1620 (4.1974)  class_acc: 0.2917 (0.3212)  loss_scale: 32768.0000 (37436.8787)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1135  max mem: 15572
Epoch: [32]  [2490/2809]  eta: 0:03:00  lr: 0.000005  min_lr: 0.000000  loss: 4.0891 (4.1974)  class_acc: 0.2917 (0.3212)  loss_scale: 32768.0000 (37418.1357)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1397  max mem: 15572
[2025-01-16 05:58:12,017] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:58:12,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [2500/2809]  eta: 0:02:54  lr: 0.000005  min_lr: 0.000000  loss: 4.0311 (4.1968)  class_acc: 0.3750 (0.3215)  loss_scale: 32768.0000 (37530.5622)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0569  max mem: 15572
Epoch: [32]  [2510/2809]  eta: 0:02:49  lr: 0.000005  min_lr: 0.000000  loss: 4.0562 (4.1968)  class_acc: 0.3333 (0.3215)  loss_scale: 65536.0000 (37642.0932)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0334  max mem: 15572
Epoch: [32]  [2520/2809]  eta: 0:02:43  lr: 0.000005  min_lr: 0.000000  loss: 4.2211 (4.1973)  class_acc: 0.2917 (0.3214)  loss_scale: 65536.0000 (37752.7394)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0330  max mem: 15572
[2025-01-16 05:58:31,111] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92415
[2025-01-16 05:58:31,111] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:58:31,111] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [2530/2809]  eta: 0:02:37  lr: 0.000005  min_lr: 0.000000  loss: 4.1733 (4.1969)  class_acc: 0.3333 (0.3219)  loss_scale: 65536.0000 (37810.7246)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0398  max mem: 15572
Epoch: [32]  [2540/2809]  eta: 0:02:31  lr: 0.000005  min_lr: 0.000000  loss: 4.1722 (4.1974)  class_acc: 0.4167 (0.3220)  loss_scale: 32768.0000 (37790.8792)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0683  max mem: 15572
Epoch: [32]  [2550/2809]  eta: 0:02:26  lr: 0.000005  min_lr: 0.000000  loss: 4.2298 (4.1973)  class_acc: 0.3750 (0.3221)  loss_scale: 32768.0000 (37771.1893)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1075  max mem: 15572
Epoch: [32]  [2560/2809]  eta: 0:02:20  lr: 0.000005  min_lr: 0.000000  loss: 4.1953 (4.1973)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (37751.6533)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0888  max mem: 15572
Epoch: [32]  [2570/2809]  eta: 0:02:14  lr: 0.000005  min_lr: 0.000000  loss: 4.1921 (4.1973)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (37732.2692)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0230  max mem: 15572
Epoch: [32]  [2580/2809]  eta: 0:02:09  lr: 0.000005  min_lr: 0.000000  loss: 4.2147 (4.1975)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (37713.0353)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0538  max mem: 15572
Epoch: [32]  [2590/2809]  eta: 0:02:03  lr: 0.000005  min_lr: 0.000000  loss: 4.3822 (4.1981)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (37693.9498)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1335  max mem: 15572
Epoch: [32]  [2600/2809]  eta: 0:01:58  lr: 0.000005  min_lr: 0.000000  loss: 4.2566 (4.1980)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (37675.0111)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0927  max mem: 15572
Epoch: [32]  [2610/2809]  eta: 0:01:52  lr: 0.000005  min_lr: 0.000000  loss: 4.2097 (4.1978)  class_acc: 0.3750 (0.3226)  loss_scale: 32768.0000 (37656.2175)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0057  max mem: 15572
Epoch: [32]  [2620/2809]  eta: 0:01:46  lr: 0.000005  min_lr: 0.000000  loss: 4.2302 (4.1979)  class_acc: 0.3750 (0.3228)  loss_scale: 32768.0000 (37637.5673)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0694  max mem: 15572
Epoch: [32]  [2630/2809]  eta: 0:01:41  lr: 0.000005  min_lr: 0.000000  loss: 4.2758 (4.1984)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (37619.0589)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.0997  max mem: 15572
Epoch: [32]  [2640/2809]  eta: 0:01:35  lr: 0.000005  min_lr: 0.000000  loss: 4.2933 (4.1984)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (37600.6906)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1346  max mem: 15572
Epoch: [32]  [2650/2809]  eta: 0:01:29  lr: 0.000005  min_lr: 0.000000  loss: 4.2933 (4.1987)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (37582.4610)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1526  max mem: 15572
[2025-01-16 05:59:45,549] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:59:45,549] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [2660/2809]  eta: 0:01:24  lr: 0.000005  min_lr: 0.000000  loss: 4.1755 (4.1989)  class_acc: 0.2917 (0.3228)  loss_scale: 32768.0000 (37625.9391)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1256  max mem: 15572
Epoch: [32]  [2670/2809]  eta: 0:01:18  lr: 0.000005  min_lr: 0.000000  loss: 4.1755 (4.1988)  class_acc: 0.3333 (0.3229)  loss_scale: 65536.0000 (37730.4320)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0846  max mem: 15572
Epoch: [32]  [2680/2809]  eta: 0:01:12  lr: 0.000005  min_lr: 0.000000  loss: 4.2299 (4.1998)  class_acc: 0.2917 (0.3227)  loss_scale: 65536.0000 (37834.1455)  weight_decay: 0.0500 (0.0500)  time: 0.4213  data: 0.0130  max mem: 15572
[2025-01-16 05:59:56,108] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92569
[2025-01-16 05:59:56,108] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:59:56,109] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [2690/2809]  eta: 0:01:07  lr: 0.000005  min_lr: 0.000000  loss: 4.1384 (4.1993)  class_acc: 0.2917 (0.3228)  loss_scale: 32768.0000 (37815.3192)  weight_decay: 0.0500 (0.0500)  time: 0.4444  data: 0.0007  max mem: 15572
Epoch: [32]  [2700/2809]  eta: 0:01:01  lr: 0.000005  min_lr: 0.000000  loss: 4.0817 (4.1992)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (37796.6324)  weight_decay: 0.0500 (0.0500)  time: 0.4979  data: 0.0008  max mem: 15572
Epoch: [32]  [2710/2809]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 4.0394 (4.1989)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (37778.0834)  weight_decay: 0.0500 (0.0500)  time: 0.4940  data: 0.0009  max mem: 15572
Epoch: [32]  [2720/2809]  eta: 0:00:50  lr: 0.000005  min_lr: 0.000000  loss: 4.0394 (4.1989)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (37759.6707)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0993  max mem: 15572
Epoch: [32]  [2730/2809]  eta: 0:00:44  lr: 0.000005  min_lr: 0.000000  loss: 4.1955 (4.1988)  class_acc: 0.2500 (0.3226)  loss_scale: 32768.0000 (37741.3929)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1229  max mem: 15572
Epoch: [32]  [2740/2809]  eta: 0:00:38  lr: 0.000005  min_lr: 0.000000  loss: 4.2802 (4.1990)  class_acc: 0.2500 (0.3225)  loss_scale: 32768.0000 (37723.2484)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1510  max mem: 15572
Epoch: [32]  [2750/2809]  eta: 0:00:33  lr: 0.000005  min_lr: 0.000000  loss: 4.2834 (4.1992)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (37705.2359)  weight_decay: 0.0500 (0.0500)  time: 0.7010  data: 0.2322  max mem: 15572
Epoch: [32]  [2760/2809]  eta: 0:00:27  lr: 0.000005  min_lr: 0.000000  loss: 4.2891 (4.1996)  class_acc: 0.2917 (0.3225)  loss_scale: 32768.0000 (37687.3539)  weight_decay: 0.0500 (0.0500)  time: 0.6619  data: 0.1932  max mem: 15572
Epoch: [32]  [2770/2809]  eta: 0:00:22  lr: 0.000005  min_lr: 0.000000  loss: 4.3210 (4.1996)  class_acc: 0.2500 (0.3223)  loss_scale: 32768.0000 (37669.6009)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.2011  max mem: 15572
Epoch: [32]  [2780/2809]  eta: 0:00:16  lr: 0.000005  min_lr: 0.000000  loss: 4.3217 (4.2000)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (37651.9755)  weight_decay: 0.0500 (0.0500)  time: 0.7173  data: 0.2695  max mem: 15572
Epoch: [32]  [2790/2809]  eta: 0:00:10  lr: 0.000005  min_lr: 0.000000  loss: 4.2169 (4.1995)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (37634.4765)  weight_decay: 0.0500 (0.0500)  time: 0.7471  data: 0.2987  max mem: 15572
Epoch: [32]  [2800/2809]  eta: 0:00:05  lr: 0.000005  min_lr: 0.000000  loss: 4.0733 (4.1994)  class_acc: 0.3333 (0.3221)  loss_scale: 32768.0000 (37617.1025)  weight_decay: 0.0500 (0.0500)  time: 0.7201  data: 0.2648  max mem: 15572
Epoch: [32]  [2808/2809]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 4.2203 (4.1998)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (37603.2923)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1363  max mem: 15572
Epoch: [32] Total time: 0:26:33 (0.5674 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 4.2203 (4.1998)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (37603.2923)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:13:43  loss: 1.1727 (1.1727)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 3.0283  data: 2.7924  max mem: 15572
Val:  [ 10/272]  eta: 0:02:31  loss: 2.5325 (2.5098)  acc1: 50.0000 (50.0000)  acc5: 72.2222 (74.7475)  time: 0.5771  data: 0.3826  max mem: 15572
Val:  [ 20/272]  eta: 0:01:43  loss: 2.5325 (2.5420)  acc1: 50.0000 (50.2646)  acc5: 72.2222 (74.8677)  time: 0.2784  data: 0.0877  max mem: 15572
Val:  [ 30/272]  eta: 0:01:26  loss: 2.5272 (2.5824)  acc1: 44.4444 (45.5197)  acc5: 72.2222 (74.7312)  time: 0.2387  data: 0.0427  max mem: 15572
Val:  [ 40/272]  eta: 0:01:18  loss: 2.5762 (2.6155)  acc1: 27.7778 (42.2764)  acc5: 72.2222 (73.8482)  time: 0.2626  data: 0.0509  max mem: 15572
Val:  [ 50/272]  eta: 0:01:15  loss: 2.5687 (2.5535)  acc1: 38.8889 (43.5730)  acc5: 77.7778 (75.2723)  time: 0.3113  data: 0.1100  max mem: 15572
Val:  [ 60/272]  eta: 0:01:11  loss: 1.9978 (2.4946)  acc1: 55.5556 (44.9909)  acc5: 83.3333 (75.6831)  time: 0.3351  data: 0.1523  max mem: 15572
Val:  [ 70/272]  eta: 0:01:06  loss: 2.0000 (2.4375)  acc1: 61.1111 (47.1831)  acc5: 83.3333 (76.6823)  time: 0.2935  data: 0.1027  max mem: 15572
Val:  [ 80/272]  eta: 0:01:03  loss: 2.1242 (2.4416)  acc1: 55.5556 (47.2565)  acc5: 77.7778 (76.6118)  time: 0.3118  data: 0.1220  max mem: 15572
Val:  [ 90/272]  eta: 0:00:59  loss: 2.4659 (2.4532)  acc1: 50.0000 (47.0085)  acc5: 77.7778 (76.8010)  time: 0.3314  data: 0.1515  max mem: 15572
Val:  [100/272]  eta: 0:00:56  loss: 2.4659 (2.4747)  acc1: 38.8889 (46.3696)  acc5: 77.7778 (76.8427)  time: 0.3106  data: 0.1309  max mem: 15572
Val:  [110/272]  eta: 0:00:52  loss: 2.6533 (2.5248)  acc1: 38.8889 (44.8448)  acc5: 77.7778 (76.0260)  time: 0.3149  data: 0.1367  max mem: 15572
Val:  [120/272]  eta: 0:00:49  loss: 2.8930 (2.5578)  acc1: 27.7778 (44.0312)  acc5: 66.6667 (75.5739)  time: 0.3255  data: 0.1480  max mem: 15572
Val:  [130/272]  eta: 0:00:45  loss: 2.5268 (2.5303)  acc1: 44.4444 (44.6141)  acc5: 83.3333 (76.3359)  time: 0.2996  data: 0.1217  max mem: 15572
Val:  [140/272]  eta: 0:00:42  loss: 2.1583 (2.5347)  acc1: 50.0000 (44.9567)  acc5: 83.3333 (76.1229)  time: 0.2959  data: 0.1097  max mem: 15572
Val:  [150/272]  eta: 0:00:38  loss: 2.4775 (2.5323)  acc1: 38.8889 (44.5180)  acc5: 77.7778 (76.3429)  time: 0.2996  data: 0.0957  max mem: 15572
Val:  [160/272]  eta: 0:00:35  loss: 2.4340 (2.5281)  acc1: 44.4444 (44.9275)  acc5: 77.7778 (76.7081)  time: 0.2926  data: 0.0904  max mem: 15572
Val:  [170/272]  eta: 0:00:32  loss: 2.5930 (2.5428)  acc1: 38.8889 (44.2170)  acc5: 72.2222 (76.1858)  time: 0.3080  data: 0.1205  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 2.5202 (2.5305)  acc1: 33.3333 (44.2603)  acc5: 72.2222 (76.4273)  time: 0.2930  data: 0.1079  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.4397 (2.5652)  acc1: 38.8889 (43.1937)  acc5: 72.2222 (75.3345)  time: 0.3381  data: 0.1460  max mem: 15572
Val:  [200/272]  eta: 0:00:22  loss: 2.7257 (2.5687)  acc1: 33.3333 (42.9243)  acc5: 72.2222 (75.1797)  time: 0.3291  data: 0.1429  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.4302 (2.5767)  acc1: 33.3333 (42.6804)  acc5: 77.7778 (75.0395)  time: 0.2907  data: 0.0886  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.6651 (2.5719)  acc1: 33.3333 (42.6848)  acc5: 77.7778 (75.1131)  time: 0.3265  data: 0.1187  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.2208 (2.5568)  acc1: 55.5556 (43.5065)  acc5: 83.3333 (75.3968)  time: 0.3418  data: 0.1557  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1047 (2.5443)  acc1: 61.1111 (43.6837)  acc5: 83.3333 (75.6339)  time: 0.3011  data: 0.1168  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.4406 (2.5499)  acc1: 33.3333 (43.1386)  acc5: 77.7778 (75.6087)  time: 0.2749  data: 0.0987  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9067 (2.5103)  acc1: 61.1111 (44.5296)  acc5: 88.8889 (76.2239)  time: 0.3138  data: 0.1421  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.8832 (2.5071)  acc1: 61.1111 (44.5059)  acc5: 83.3333 (76.3018)  time: 0.2661  data: 0.1018  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8832 (2.5114)  acc1: 61.1111 (44.4809)  acc5: 83.3333 (76.2646)  time: 0.2059  data: 0.0485  max mem: 15572
Val: Total time: 0:01:24 (0.3122 s / it)
* Acc@1 44.481 Acc@5 76.265 loss 2.511
Accuracy of the network on the 4883 val videos: 44.5%
Max accuracy: 44.85%
Epoch: [33]  [   0/2809]  eta: 6:03:36  lr: 0.000005  min_lr: 0.000000  loss: 3.8311 (3.8311)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.7668  data: 7.2066  max mem: 15572
[2025-01-16 06:02:49,570] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:02:49,571] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 06:02:50,437] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92700
[2025-01-16 06:02:50,437] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:02:50,438] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [  10/2809]  eta: 1:02:23  lr: 0.000005  min_lr: 0.000000  loss: 4.1271 (4.1564)  class_acc: 0.3750 (0.3258)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 1.3374  data: 0.8673  max mem: 15572
Epoch: [33]  [  20/2809]  eta: 0:47:23  lr: 0.000005  min_lr: 0.000000  loss: 4.2637 (4.2289)  class_acc: 0.3333 (0.3075)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  time: 0.6823  data: 0.2254  max mem: 15572
Epoch: [33]  [  30/2809]  eta: 0:40:33  lr: 0.000005  min_lr: 0.000000  loss: 4.2911 (4.2624)  class_acc: 0.2917 (0.2970)  loss_scale: 32768.0000 (34882.0645)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1693  max mem: 15572
Epoch: [33]  [  40/2809]  eta: 0:37:29  lr: 0.000005  min_lr: 0.000000  loss: 4.1338 (4.2021)  class_acc: 0.2917 (0.3049)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1445  max mem: 15572
Epoch: [33]  [  50/2809]  eta: 0:35:41  lr: 0.000004  min_lr: 0.000000  loss: 4.0397 (4.2061)  class_acc: 0.3333 (0.3162)  loss_scale: 32768.0000 (34053.0196)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1765  max mem: 15572
Epoch: [33]  [  60/2809]  eta: 0:33:38  lr: 0.000004  min_lr: 0.000000  loss: 4.2639 (4.2192)  class_acc: 0.2917 (0.3115)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1374  max mem: 15572
Epoch: [33]  [  70/2809]  eta: 0:32:39  lr: 0.000004  min_lr: 0.000000  loss: 4.2639 (4.2160)  class_acc: 0.3333 (0.3245)  loss_scale: 32768.0000 (33691.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1198  max mem: 15572
Epoch: [33]  [  80/2809]  eta: 0:31:38  lr: 0.000004  min_lr: 0.000000  loss: 4.1008 (4.2028)  class_acc: 0.3750 (0.3225)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1310  max mem: 15572
Epoch: [33]  [  90/2809]  eta: 0:30:56  lr: 0.000004  min_lr: 0.000000  loss: 4.0687 (4.1973)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (33488.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1239  max mem: 15572
Epoch: [33]  [ 100/2809]  eta: 0:30:29  lr: 0.000004  min_lr: 0.000000  loss: 4.1117 (4.1825)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1484  max mem: 15572
Epoch: [33]  [ 110/2809]  eta: 0:29:35  lr: 0.000004  min_lr: 0.000000  loss: 4.0934 (4.1844)  class_acc: 0.2917 (0.3217)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0809  max mem: 15572
Epoch: [33]  [ 120/2809]  eta: 0:29:05  lr: 0.000004  min_lr: 0.000000  loss: 4.0224 (4.1768)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0552  max mem: 15572
Epoch: [33]  [ 130/2809]  eta: 0:28:36  lr: 0.000004  min_lr: 0.000000  loss: 4.1186 (4.1807)  class_acc: 0.2917 (0.3193)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0957  max mem: 15572
[2025-01-16 06:04:05,261] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:04:05,261] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 140/2809]  eta: 0:28:20  lr: 0.000004  min_lr: 0.000000  loss: 4.1445 (4.1804)  class_acc: 0.3333 (0.3203)  loss_scale: 32768.0000 (35324.3688)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.0919  max mem: 15572
Epoch: [33]  [ 150/2809]  eta: 0:27:56  lr: 0.000004  min_lr: 0.000000  loss: 4.0855 (4.1814)  class_acc: 0.2917 (0.3176)  loss_scale: 65536.0000 (37325.1391)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0950  max mem: 15572
Epoch: [33]  [ 160/2809]  eta: 0:27:34  lr: 0.000004  min_lr: 0.000000  loss: 4.1207 (4.1797)  class_acc: 0.2917 (0.3186)  loss_scale: 65536.0000 (39077.3665)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0871  max mem: 15572
[2025-01-16 06:04:22,722] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92861
[2025-01-16 06:04:22,722] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:04:22,723] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 170/2809]  eta: 0:27:08  lr: 0.000004  min_lr: 0.000000  loss: 4.2151 (4.1839)  class_acc: 0.2917 (0.3187)  loss_scale: 65536.0000 (39283.2749)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0742  max mem: 15572
Epoch: [33]  [ 180/2809]  eta: 0:27:03  lr: 0.000004  min_lr: 0.000000  loss: 4.2820 (4.1858)  class_acc: 0.3333 (0.3204)  loss_scale: 32768.0000 (38923.3149)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1217  max mem: 15572
Epoch: [33]  [ 190/2809]  eta: 0:26:49  lr: 0.000004  min_lr: 0.000000  loss: 4.2681 (4.1879)  class_acc: 0.3333 (0.3216)  loss_scale: 32768.0000 (38601.0471)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1448  max mem: 15572
Epoch: [33]  [ 200/2809]  eta: 0:26:49  lr: 0.000004  min_lr: 0.000000  loss: 4.2614 (4.1840)  class_acc: 0.2917 (0.3201)  loss_scale: 32768.0000 (38310.8458)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1505  max mem: 15572
Epoch: [33]  [ 210/2809]  eta: 0:26:42  lr: 0.000004  min_lr: 0.000000  loss: 4.2366 (4.1819)  class_acc: 0.3333 (0.3235)  loss_scale: 32768.0000 (38048.1517)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.1781  max mem: 15572
Epoch: [33]  [ 220/2809]  eta: 0:26:23  lr: 0.000004  min_lr: 0.000000  loss: 4.2181 (4.1823)  class_acc: 0.3333 (0.3247)  loss_scale: 32768.0000 (37809.2308)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1008  max mem: 15572
Epoch: [33]  [ 230/2809]  eta: 0:26:11  lr: 0.000004  min_lr: 0.000000  loss: 4.1847 (4.1868)  class_acc: 0.3750 (0.3263)  loss_scale: 32768.0000 (37590.9957)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0838  max mem: 15572
Epoch: [33]  [ 240/2809]  eta: 0:25:48  lr: 0.000004  min_lr: 0.000000  loss: 4.2187 (4.1874)  class_acc: 0.3750 (0.3261)  loss_scale: 32768.0000 (37390.8714)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0653  max mem: 15572
Epoch: [33]  [ 250/2809]  eta: 0:25:38  lr: 0.000004  min_lr: 0.000000  loss: 4.3011 (4.1939)  class_acc: 0.2083 (0.3220)  loss_scale: 32768.0000 (37206.6932)  weight_decay: 0.0500 (0.0500)  time: 0.5104  data: 0.0554  max mem: 15572
Epoch: [33]  [ 260/2809]  eta: 0:25:24  lr: 0.000004  min_lr: 0.000000  loss: 4.3011 (4.1955)  class_acc: 0.2500 (0.3218)  loss_scale: 32768.0000 (37036.6284)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0689  max mem: 15572
Epoch: [33]  [ 270/2809]  eta: 0:25:10  lr: 0.000004  min_lr: 0.000000  loss: 4.3021 (4.1965)  class_acc: 0.2500 (0.3200)  loss_scale: 32768.0000 (36879.1144)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0522  max mem: 15572
Epoch: [33]  [ 280/2809]  eta: 0:25:02  lr: 0.000004  min_lr: 0.000000  loss: 4.2852 (4.1942)  class_acc: 0.2917 (0.3212)  loss_scale: 32768.0000 (36732.8114)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0956  max mem: 15572
Epoch: [33]  [ 290/2809]  eta: 0:24:52  lr: 0.000004  min_lr: 0.000000  loss: 4.1796 (4.1942)  class_acc: 0.3333 (0.3207)  loss_scale: 32768.0000 (36596.5636)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1105  max mem: 15572
[2025-01-16 06:05:34,879] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:05:34,879] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 06:05:35,273] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92991
[2025-01-16 06:05:35,273] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:05:35,273] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 300/2809]  eta: 0:24:44  lr: 0.000004  min_lr: 0.000000  loss: 4.2227 (4.1936)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (36578.2326)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1147  max mem: 15572
[2025-01-16 06:05:41,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=93000, skipped=578, lr=[4.25016877310702e-08, 4.25016877310702e-08, 6.071669675867172e-08, 6.071669675867172e-08, 8.67381382266739e-08, 8.67381382266739e-08, 1.2391162603810558e-07, 1.2391162603810558e-07, 1.770166086258651e-07, 1.770166086258651e-07, 2.528808694655216e-07, 2.528808694655216e-07, 3.6125838495074515e-07, 3.6125838495074515e-07, 5.160834070724932e-07, 5.160834070724932e-07, 7.372620101035616e-07, 7.372620101035616e-07, 1.053231443005088e-06, 1.053231443005088e-06, 1.5046163471501259e-06, 1.5046163471501259e-06, 2.14945192450018e-06, 2.14945192450018e-06, 3.070645606428829e-06, 3.070645606428829e-06, 4.386636580612613e-06, 4.386636580612613e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:05:41,581] [INFO] [timer.py:260:stop] epoch=0/micro_step=93000/global_step=93000, RunningAvgSamplesPerSec=27.945809707770742, CurrSamplesPerSec=29.82815915969484, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [33]  [ 310/2809]  eta: 0:24:44  lr: 0.000004  min_lr: 0.000000  loss: 4.2227 (4.1964)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (36455.7170)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1697  max mem: 15572
Epoch: [33]  [ 320/2809]  eta: 0:24:33  lr: 0.000004  min_lr: 0.000000  loss: 4.1990 (4.1994)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (36340.8349)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1480  max mem: 15572
Epoch: [33]  [ 330/2809]  eta: 0:24:29  lr: 0.000004  min_lr: 0.000000  loss: 4.1830 (4.1991)  class_acc: 0.2500 (0.3166)  loss_scale: 32768.0000 (36232.8943)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1250  max mem: 15572
Epoch: [33]  [ 340/2809]  eta: 0:24:18  lr: 0.000004  min_lr: 0.000000  loss: 4.0792 (4.1989)  class_acc: 0.2917 (0.3165)  loss_scale: 32768.0000 (36131.2845)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1312  max mem: 15572
Epoch: [33]  [ 350/2809]  eta: 0:24:13  lr: 0.000004  min_lr: 0.000000  loss: 4.0791 (4.1996)  class_acc: 0.3333 (0.3171)  loss_scale: 32768.0000 (36035.4644)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1255  max mem: 15572
Epoch: [33]  [ 360/2809]  eta: 0:24:03  lr: 0.000004  min_lr: 0.000000  loss: 4.1927 (4.2000)  class_acc: 0.3333 (0.3159)  loss_scale: 32768.0000 (35944.9529)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1290  max mem: 15572
Epoch: [33]  [ 370/2809]  eta: 0:23:59  lr: 0.000004  min_lr: 0.000000  loss: 4.1318 (4.1970)  class_acc: 0.3750 (0.3168)  loss_scale: 32768.0000 (35859.3208)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1441  max mem: 15572
Epoch: [33]  [ 380/2809]  eta: 0:23:48  lr: 0.000004  min_lr: 0.000000  loss: 4.1318 (4.1963)  class_acc: 0.3750 (0.3166)  loss_scale: 32768.0000 (35778.1837)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1318  max mem: 15572
Epoch: [33]  [ 390/2809]  eta: 0:23:49  lr: 0.000004  min_lr: 0.000000  loss: 4.1569 (4.1940)  class_acc: 0.2917 (0.3162)  loss_scale: 32768.0000 (35701.1969)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1653  max mem: 15572
Epoch: [33]  [ 400/2809]  eta: 0:23:38  lr: 0.000004  min_lr: 0.000000  loss: 4.1062 (4.1926)  class_acc: 0.2917 (0.3154)  loss_scale: 32768.0000 (35628.0499)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1558  max mem: 15572
Epoch: [33]  [ 410/2809]  eta: 0:23:30  lr: 0.000004  min_lr: 0.000000  loss: 4.1751 (4.1926)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (35558.4623)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0926  max mem: 15572
Epoch: [33]  [ 420/2809]  eta: 0:23:23  lr: 0.000004  min_lr: 0.000000  loss: 4.1797 (4.1927)  class_acc: 0.2917 (0.3149)  loss_scale: 32768.0000 (35492.1805)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1378  max mem: 15572
[2025-01-16 06:06:49,212] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:06:49,213] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 430/2809]  eta: 0:23:12  lr: 0.000004  min_lr: 0.000000  loss: 4.2371 (4.1929)  class_acc: 0.2917 (0.3153)  loss_scale: 32768.0000 (36037.1972)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.1009  max mem: 15572
Epoch: [33]  [ 440/2809]  eta: 0:23:04  lr: 0.000004  min_lr: 0.000000  loss: 4.2233 (4.1956)  class_acc: 0.2917 (0.3146)  loss_scale: 65536.0000 (36706.1043)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0890  max mem: 15572
[2025-01-16 06:07:01,413] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93142
[2025-01-16 06:07:01,413] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:07:01,413] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 450/2809]  eta: 0:22:58  lr: 0.000004  min_lr: 0.000000  loss: 4.2471 (4.1972)  class_acc: 0.2917 (0.3145)  loss_scale: 65536.0000 (36909.4102)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1361  max mem: 15572
Epoch: [33]  [ 460/2809]  eta: 0:22:55  lr: 0.000004  min_lr: 0.000000  loss: 4.2471 (4.1974)  class_acc: 0.3333 (0.3154)  loss_scale: 32768.0000 (36819.5748)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1828  max mem: 15572
Epoch: [33]  [ 470/2809]  eta: 0:22:52  lr: 0.000004  min_lr: 0.000000  loss: 4.1034 (4.1956)  class_acc: 0.3750 (0.3169)  loss_scale: 32768.0000 (36733.5541)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.2188  max mem: 15572
Epoch: [33]  [ 480/2809]  eta: 0:22:46  lr: 0.000004  min_lr: 0.000000  loss: 4.0870 (4.1924)  class_acc: 0.3333 (0.3179)  loss_scale: 32768.0000 (36651.1102)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1612  max mem: 15572
Epoch: [33]  [ 490/2809]  eta: 0:22:35  lr: 0.000004  min_lr: 0.000000  loss: 4.1428 (4.1942)  class_acc: 0.3333 (0.3179)  loss_scale: 32768.0000 (36572.0244)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.0770  max mem: 15572
Epoch: [33]  [ 500/2809]  eta: 0:22:28  lr: 0.000004  min_lr: 0.000000  loss: 4.1958 (4.1915)  class_acc: 0.3333 (0.3188)  loss_scale: 32768.0000 (36496.0958)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0886  max mem: 15572
Epoch: [33]  [ 510/2809]  eta: 0:22:23  lr: 0.000004  min_lr: 0.000000  loss: 4.0905 (4.1919)  class_acc: 0.3333 (0.3184)  loss_scale: 32768.0000 (36423.1389)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1364  max mem: 15572
Epoch: [33]  [ 520/2809]  eta: 0:22:14  lr: 0.000004  min_lr: 0.000000  loss: 4.2359 (4.1927)  class_acc: 0.2917 (0.3182)  loss_scale: 32768.0000 (36352.9827)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0881  max mem: 15572
Epoch: [33]  [ 530/2809]  eta: 0:22:07  lr: 0.000004  min_lr: 0.000000  loss: 4.1789 (4.1902)  class_acc: 0.3333 (0.3185)  loss_scale: 32768.0000 (36285.4689)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0519  max mem: 15572
Epoch: [33]  [ 540/2809]  eta: 0:22:03  lr: 0.000004  min_lr: 0.000000  loss: 4.1219 (4.1894)  class_acc: 0.4167 (0.3195)  loss_scale: 32768.0000 (36220.4510)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1463  max mem: 15572
Epoch: [33]  [ 550/2809]  eta: 0:21:56  lr: 0.000004  min_lr: 0.000000  loss: 4.0704 (4.1884)  class_acc: 0.3750 (0.3198)  loss_scale: 32768.0000 (36157.7931)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1604  max mem: 15572
Epoch: [33]  [ 560/2809]  eta: 0:21:46  lr: 0.000004  min_lr: 0.000000  loss: 4.0704 (4.1875)  class_acc: 0.3333 (0.3191)  loss_scale: 32768.0000 (36097.3690)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0623  max mem: 15572
Epoch: [33]  [ 570/2809]  eta: 0:21:40  lr: 0.000004  min_lr: 0.000000  loss: 4.1027 (4.1863)  class_acc: 0.2500 (0.3182)  loss_scale: 32768.0000 (36039.0613)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0650  max mem: 15572
[2025-01-16 06:08:14,130] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:08:14,130] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 580/2809]  eta: 0:21:31  lr: 0.000004  min_lr: 0.000000  loss: 4.1228 (4.1869)  class_acc: 0.3750 (0.3192)  loss_scale: 32768.0000 (36377.5559)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0937  max mem: 15572
Epoch: [33]  [ 590/2809]  eta: 0:21:26  lr: 0.000004  min_lr: 0.000000  loss: 4.1157 (4.1869)  class_acc: 0.3750 (0.3200)  loss_scale: 65536.0000 (36870.9306)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1260  max mem: 15572
[2025-01-16 06:08:25,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93292
[2025-01-16 06:08:25,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:08:25,591] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 600/2809]  eta: 0:21:22  lr: 0.000004  min_lr: 0.000000  loss: 4.0962 (4.1851)  class_acc: 0.3333 (0.3203)  loss_scale: 65536.0000 (37020.7521)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1834  max mem: 15572
Epoch: [33]  [ 610/2809]  eta: 0:21:14  lr: 0.000004  min_lr: 0.000000  loss: 4.0972 (4.1845)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (36951.1489)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1406  max mem: 15572
Epoch: [33]  [ 620/2809]  eta: 0:21:09  lr: 0.000004  min_lr: 0.000000  loss: 4.0972 (4.1826)  class_acc: 0.2917 (0.3211)  loss_scale: 32768.0000 (36883.7874)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1251  max mem: 15572
Epoch: [33]  [ 630/2809]  eta: 0:21:03  lr: 0.000004  min_lr: 0.000000  loss: 4.1139 (4.1819)  class_acc: 0.3333 (0.3220)  loss_scale: 32768.0000 (36818.5610)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1468  max mem: 15572
Epoch: [33]  [ 640/2809]  eta: 0:20:58  lr: 0.000004  min_lr: 0.000000  loss: 4.1823 (4.1825)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (36755.3697)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1671  max mem: 15572
Epoch: [33]  [ 650/2809]  eta: 0:20:53  lr: 0.000004  min_lr: 0.000000  loss: 4.2922 (4.1839)  class_acc: 0.2917 (0.3217)  loss_scale: 32768.0000 (36694.1198)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1828  max mem: 15572
Epoch: [33]  [ 660/2809]  eta: 0:20:47  lr: 0.000004  min_lr: 0.000000  loss: 4.1642 (4.1819)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (36634.7231)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1601  max mem: 15572
Epoch: [33]  [ 670/2809]  eta: 0:20:39  lr: 0.000004  min_lr: 0.000000  loss: 4.1826 (4.1827)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (36577.0969)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.1160  max mem: 15572
Epoch: [33]  [ 680/2809]  eta: 0:20:33  lr: 0.000004  min_lr: 0.000000  loss: 4.3847 (4.1848)  class_acc: 0.2917 (0.3218)  loss_scale: 32768.0000 (36521.1630)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1162  max mem: 15572
Epoch: [33]  [ 690/2809]  eta: 0:20:28  lr: 0.000004  min_lr: 0.000000  loss: 4.1490 (4.1833)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (36466.8480)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1553  max mem: 15572
Epoch: [33]  [ 700/2809]  eta: 0:20:22  lr: 0.000004  min_lr: 0.000000  loss: 4.0718 (4.1832)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (36414.0827)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1394  max mem: 15572
Epoch: [33]  [ 710/2809]  eta: 0:20:17  lr: 0.000004  min_lr: 0.000000  loss: 4.0823 (4.1826)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (36362.8017)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1564  max mem: 15572
Epoch: [33]  [ 720/2809]  eta: 0:20:12  lr: 0.000004  min_lr: 0.000000  loss: 4.1341 (4.1825)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (36312.9431)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1813  max mem: 15572
[2025-01-16 06:09:40,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:09:40,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 730/2809]  eta: 0:20:05  lr: 0.000004  min_lr: 0.000000  loss: 4.1776 (4.1813)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (36578.2326)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1422  max mem: 15572
[2025-01-16 06:09:48,473] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93436
[2025-01-16 06:09:48,474] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:09:48,474] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 740/2809]  eta: 0:19:56  lr: 0.000004  min_lr: 0.000000  loss: 4.2237 (4.1817)  class_acc: 0.3333 (0.3222)  loss_scale: 65536.0000 (36880.5830)  weight_decay: 0.0500 (0.0500)  time: 0.5036  data: 0.0643  max mem: 15572
Epoch: [33]  [ 750/2809]  eta: 0:19:52  lr: 0.000004  min_lr: 0.000000  loss: 4.2900 (4.1834)  class_acc: 0.2500 (0.3213)  loss_scale: 32768.0000 (36825.8216)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1064  max mem: 15572
Epoch: [33]  [ 760/2809]  eta: 0:19:44  lr: 0.000004  min_lr: 0.000000  loss: 4.1715 (4.1829)  class_acc: 0.2500 (0.3209)  loss_scale: 32768.0000 (36772.4993)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1300  max mem: 15572
Epoch: [33]  [ 770/2809]  eta: 0:19:39  lr: 0.000004  min_lr: 0.000000  loss: 4.1247 (4.1823)  class_acc: 0.3333 (0.3214)  loss_scale: 32768.0000 (36720.5603)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.1109  max mem: 15572
Epoch: [33]  [ 780/2809]  eta: 0:19:34  lr: 0.000004  min_lr: 0.000000  loss: 4.1820 (4.1841)  class_acc: 0.3333 (0.3211)  loss_scale: 32768.0000 (36669.9513)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1732  max mem: 15572
Epoch: [33]  [ 790/2809]  eta: 0:19:31  lr: 0.000004  min_lr: 0.000000  loss: 4.2640 (4.1839)  class_acc: 0.2500 (0.3215)  loss_scale: 32768.0000 (36620.6220)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.2026  max mem: 15572
Epoch: [33]  [ 800/2809]  eta: 0:19:23  lr: 0.000004  min_lr: 0.000000  loss: 4.1235 (4.1840)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (36572.5243)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1519  max mem: 15572
Epoch: [33]  [ 810/2809]  eta: 0:19:17  lr: 0.000004  min_lr: 0.000000  loss: 4.1519 (4.1833)  class_acc: 0.2917 (0.3212)  loss_scale: 32768.0000 (36525.6128)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.1025  max mem: 15572
Epoch: [33]  [ 820/2809]  eta: 0:19:13  lr: 0.000004  min_lr: 0.000000  loss: 4.1730 (4.1828)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (36479.8441)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1599  max mem: 15572
Epoch: [33]  [ 830/2809]  eta: 0:19:06  lr: 0.000004  min_lr: 0.000000  loss: 4.2468 (4.1846)  class_acc: 0.2500 (0.3212)  loss_scale: 32768.0000 (36435.1769)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1573  max mem: 15572
Epoch: [33]  [ 840/2809]  eta: 0:18:59  lr: 0.000004  min_lr: 0.000000  loss: 4.3149 (4.1846)  class_acc: 0.2500 (0.3208)  loss_scale: 32768.0000 (36391.5719)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0953  max mem: 15572
Epoch: [33]  [ 850/2809]  eta: 0:18:55  lr: 0.000004  min_lr: 0.000000  loss: 4.1809 (4.1834)  class_acc: 0.3333 (0.3209)  loss_scale: 32768.0000 (36348.9918)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1348  max mem: 15572
Epoch: [33]  [ 860/2809]  eta: 0:18:49  lr: 0.000004  min_lr: 0.000000  loss: 4.1624 (4.1817)  class_acc: 0.3333 (0.3210)  loss_scale: 32768.0000 (36307.4007)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1791  max mem: 15572
[2025-01-16 06:11:03,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:11:03,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 870/2809]  eta: 0:18:43  lr: 0.000004  min_lr: 0.000000  loss: 4.1624 (4.1824)  class_acc: 0.3333 (0.3214)  loss_scale: 32768.0000 (36379.6280)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1422  max mem: 15572
Epoch: [33]  [ 880/2809]  eta: 0:18:38  lr: 0.000004  min_lr: 0.000000  loss: 4.2206 (4.1831)  class_acc: 0.2917 (0.3216)  loss_scale: 65536.0000 (36710.5743)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1546  max mem: 15572
[2025-01-16 06:11:13,114] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93581
[2025-01-16 06:11:13,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:11:13,115] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 890/2809]  eta: 0:18:31  lr: 0.000004  min_lr: 0.000000  loss: 4.2206 (4.1830)  class_acc: 0.2500 (0.3215)  loss_scale: 65536.0000 (36776.6554)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1452  max mem: 15572
Epoch: [33]  [ 900/2809]  eta: 0:18:25  lr: 0.000004  min_lr: 0.000000  loss: 4.0587 (4.1820)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (36732.1643)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1069  max mem: 15572
Epoch: [33]  [ 910/2809]  eta: 0:18:17  lr: 0.000004  min_lr: 0.000000  loss: 4.1137 (4.1804)  class_acc: 0.3750 (0.3225)  loss_scale: 32768.0000 (36688.6498)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0590  max mem: 15572
Epoch: [33]  [ 920/2809]  eta: 0:18:11  lr: 0.000004  min_lr: 0.000000  loss: 4.1137 (4.1794)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (36646.0803)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0377  max mem: 15572
Epoch: [33]  [ 930/2809]  eta: 0:18:04  lr: 0.000004  min_lr: 0.000000  loss: 4.1241 (4.1786)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (36604.4253)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0677  max mem: 15572
Epoch: [33]  [ 940/2809]  eta: 0:18:00  lr: 0.000004  min_lr: 0.000000  loss: 4.1713 (4.1794)  class_acc: 0.2917 (0.3212)  loss_scale: 32768.0000 (36563.6557)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1236  max mem: 15572
Epoch: [33]  [ 950/2809]  eta: 0:17:53  lr: 0.000004  min_lr: 0.000000  loss: 4.1778 (4.1791)  class_acc: 0.3333 (0.3210)  loss_scale: 32768.0000 (36523.7434)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1152  max mem: 15572
Epoch: [33]  [ 960/2809]  eta: 0:17:46  lr: 0.000004  min_lr: 0.000000  loss: 4.2103 (4.1803)  class_acc: 0.3333 (0.3211)  loss_scale: 32768.0000 (36484.6618)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0605  max mem: 15572
Epoch: [33]  [ 970/2809]  eta: 0:17:41  lr: 0.000004  min_lr: 0.000000  loss: 4.2103 (4.1810)  class_acc: 0.2917 (0.3206)  loss_scale: 32768.0000 (36446.3852)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1038  max mem: 15572
Epoch: [33]  [ 980/2809]  eta: 0:17:36  lr: 0.000004  min_lr: 0.000000  loss: 4.1673 (4.1812)  class_acc: 0.2500 (0.3207)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1738  max mem: 15572
Epoch: [33]  [ 990/2809]  eta: 0:17:29  lr: 0.000004  min_lr: 0.000000  loss: 4.0731 (4.1811)  class_acc: 0.3333 (0.3208)  loss_scale: 32768.0000 (36372.1493)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1273  max mem: 15572
Epoch: [33]  [1000/2809]  eta: 0:17:24  lr: 0.000004  min_lr: 0.000000  loss: 4.1022 (4.1811)  class_acc: 0.2917 (0.3206)  loss_scale: 32768.0000 (36336.1439)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1102  max mem: 15572
Epoch: [33]  [1010/2809]  eta: 0:17:18  lr: 0.000004  min_lr: 0.000000  loss: 4.1990 (4.1814)  class_acc: 0.3333 (0.3206)  loss_scale: 32768.0000 (36300.8506)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1392  max mem: 15572
[2025-01-16 06:12:25,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:12:25,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [1020/2809]  eta: 0:17:12  lr: 0.000004  min_lr: 0.000000  loss: 4.3412 (4.1819)  class_acc: 0.3333 (0.3205)  loss_scale: 32768.0000 (36523.0010)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0948  max mem: 15572
Epoch: [33]  [1030/2809]  eta: 0:17:07  lr: 0.000004  min_lr: 0.000000  loss: 4.2099 (4.1807)  class_acc: 0.2917 (0.3206)  loss_scale: 65536.0000 (36804.4074)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1227  max mem: 15572
Epoch: [33]  [1040/2809]  eta: 0:17:04  lr: 0.000004  min_lr: 0.000000  loss: 4.1345 (4.1806)  class_acc: 0.3333 (0.3207)  loss_scale: 65536.0000 (37080.4073)  weight_decay: 0.0500 (0.0500)  time: 0.6836  data: 0.2246  max mem: 15572
[2025-01-16 06:12:44,311] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93739
[2025-01-16 06:12:44,312] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:12:44,312] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1050/2809]  eta: 0:16:57  lr: 0.000004  min_lr: 0.000000  loss: 4.1710 (4.1813)  class_acc: 0.3333 (0.3209)  loss_scale: 65536.0000 (37070.5538)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.1915  max mem: 15572
Epoch: [33]  [1060/2809]  eta: 0:16:52  lr: 0.000004  min_lr: 0.000000  loss: 4.3114 (4.1818)  class_acc: 0.2917 (0.3210)  loss_scale: 32768.0000 (37030.0019)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1161  max mem: 15572
Epoch: [33]  [1070/2809]  eta: 0:16:46  lr: 0.000004  min_lr: 0.000000  loss: 4.1854 (4.1806)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (36990.2073)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1323  max mem: 15572
Epoch: [33]  [1080/2809]  eta: 0:16:38  lr: 0.000004  min_lr: 0.000000  loss: 4.1854 (4.1808)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (36951.1489)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0593  max mem: 15572
Epoch: [33]  [1090/2809]  eta: 0:16:33  lr: 0.000004  min_lr: 0.000000  loss: 4.3858 (4.1832)  class_acc: 0.2500 (0.3213)  loss_scale: 32768.0000 (36912.8066)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0716  max mem: 15572
Epoch: [33]  [1100/2809]  eta: 0:16:27  lr: 0.000004  min_lr: 0.000000  loss: 4.2077 (4.1828)  class_acc: 0.2500 (0.3213)  loss_scale: 32768.0000 (36875.1608)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1517  max mem: 15572
Epoch: [33]  [1110/2809]  eta: 0:16:21  lr: 0.000004  min_lr: 0.000000  loss: 4.1064 (4.1825)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (36838.1926)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1420  max mem: 15572
Epoch: [33]  [1120/2809]  eta: 0:16:15  lr: 0.000004  min_lr: 0.000000  loss: 4.2049 (4.1834)  class_acc: 0.3333 (0.3217)  loss_scale: 32768.0000 (36801.8840)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1404  max mem: 15572
Epoch: [33]  [1130/2809]  eta: 0:16:10  lr: 0.000004  min_lr: 0.000000  loss: 4.2477 (4.1841)  class_acc: 0.2917 (0.3217)  loss_scale: 32768.0000 (36766.2175)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1542  max mem: 15572
Epoch: [33]  [1140/2809]  eta: 0:16:04  lr: 0.000004  min_lr: 0.000000  loss: 4.2229 (4.1837)  class_acc: 0.2500 (0.3210)  loss_scale: 32768.0000 (36731.1762)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1575  max mem: 15572
[2025-01-16 06:13:42,863] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93843
[2025-01-16 06:13:42,863] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 06:13:42,863] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [33]  [1150/2809]  eta: 0:15:58  lr: 0.000004  min_lr: 0.000000  loss: 4.0760 (4.1829)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (36625.5708)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1424  max mem: 15572
Epoch: [33]  [1160/2809]  eta: 0:15:54  lr: 0.000004  min_lr: 0.000000  loss: 4.0760 (4.1822)  class_acc: 0.3750 (0.3220)  loss_scale: 16384.0000 (36451.2248)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.2117  max mem: 15572
Epoch: [33]  [1170/2809]  eta: 0:15:47  lr: 0.000004  min_lr: 0.000000  loss: 4.2058 (4.1827)  class_acc: 0.2500 (0.3216)  loss_scale: 16384.0000 (36279.8565)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1619  max mem: 15572
Epoch: [33]  [1180/2809]  eta: 0:15:40  lr: 0.000004  min_lr: 0.000000  loss: 4.2311 (4.1829)  class_acc: 0.2500 (0.3211)  loss_scale: 16384.0000 (36111.3903)  weight_decay: 0.0500 (0.0500)  time: 0.4880  data: 0.0482  max mem: 15572
Epoch: [33]  [1190/2809]  eta: 0:15:34  lr: 0.000004  min_lr: 0.000000  loss: 4.2278 (4.1827)  class_acc: 0.2500 (0.3208)  loss_scale: 16384.0000 (35945.7531)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0979  max mem: 15572
Epoch: [33]  [1200/2809]  eta: 0:15:27  lr: 0.000004  min_lr: 0.000000  loss: 4.0262 (4.1804)  class_acc: 0.3333 (0.3210)  loss_scale: 16384.0000 (35782.8743)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0749  max mem: 15572
Epoch: [33]  [1210/2809]  eta: 0:15:22  lr: 0.000004  min_lr: 0.000000  loss: 4.0368 (4.1804)  class_acc: 0.3750 (0.3215)  loss_scale: 16384.0000 (35622.6854)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0781  max mem: 15572
Epoch: [33]  [1220/2809]  eta: 0:15:15  lr: 0.000004  min_lr: 0.000000  loss: 4.1846 (4.1804)  class_acc: 0.3750 (0.3218)  loss_scale: 16384.0000 (35465.1204)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.0886  max mem: 15572
Epoch: [33]  [1230/2809]  eta: 0:15:09  lr: 0.000004  min_lr: 0.000000  loss: 4.2712 (4.1814)  class_acc: 0.3333 (0.3220)  loss_scale: 16384.0000 (35310.1154)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0989  max mem: 15572
Epoch: [33]  [1240/2809]  eta: 0:15:03  lr: 0.000004  min_lr: 0.000000  loss: 4.1908 (4.1807)  class_acc: 0.2500 (0.3215)  loss_scale: 16384.0000 (35157.6084)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1375  max mem: 15572
Epoch: [33]  [1250/2809]  eta: 0:14:58  lr: 0.000004  min_lr: 0.000000  loss: 4.1428 (4.1809)  class_acc: 0.2917 (0.3217)  loss_scale: 16384.0000 (35007.5396)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1246  max mem: 15572
Epoch: [33]  [1260/2809]  eta: 0:14:51  lr: 0.000004  min_lr: 0.000000  loss: 4.2513 (4.1803)  class_acc: 0.3750 (0.3221)  loss_scale: 16384.0000 (34859.8509)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0986  max mem: 15572
Epoch: [33]  [1270/2809]  eta: 0:14:45  lr: 0.000004  min_lr: 0.000000  loss: 4.2513 (4.1812)  class_acc: 0.2917 (0.3214)  loss_scale: 16384.0000 (34714.4862)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0936  max mem: 15572
[2025-01-16 06:14:54,777] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:14:54,777] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [1280/2809]  eta: 0:14:40  lr: 0.000004  min_lr: 0.000000  loss: 4.2501 (4.1815)  class_acc: 0.2917 (0.3215)  loss_scale: 16384.0000 (34648.1311)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1410  max mem: 15572
Epoch: [33]  [1290/2809]  eta: 0:14:35  lr: 0.000004  min_lr: 0.000000  loss: 4.3015 (4.1828)  class_acc: 0.3333 (0.3213)  loss_scale: 32768.0000 (34633.5678)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.1988  max mem: 15572
Epoch: [33]  [1300/2809]  eta: 0:14:28  lr: 0.000004  min_lr: 0.000000  loss: 4.2486 (4.1825)  class_acc: 0.2500 (0.3212)  loss_scale: 32768.0000 (34619.2283)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1139  max mem: 15572
[2025-01-16 06:15:10,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=94000, skipped=584, lr=[3.8392652360453044e-08, 3.8392652360453044e-08, 5.4846646229218644e-08, 5.4846646229218644e-08, 7.835235175602664e-08, 7.835235175602664e-08, 1.1193193108003806e-07, 1.1193193108003806e-07, 1.5990275868576867e-07, 1.5990275868576867e-07, 2.2843251240824097e-07, 2.2843251240824097e-07, 3.263321605832014e-07, 3.263321605832014e-07, 4.661888008331449e-07, 4.661888008331449e-07, 6.65984001190207e-07, 6.65984001190207e-07, 9.514057159860102e-07, 9.514057159860102e-07, 1.3591510228371572e-06, 1.3591510228371572e-06, 1.9416443183387965e-06, 1.9416443183387965e-06, 2.773777597626852e-06, 2.773777597626852e-06, 3.9625394251812176e-06, 3.9625394251812176e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:15:10,449] [INFO] [timer.py:260:stop] epoch=0/micro_step=94000/global_step=94000, RunningAvgSamplesPerSec=27.95372505529022, CurrSamplesPerSec=22.582580081856367, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [33]  [1310/2809]  eta: 0:14:22  lr: 0.000004  min_lr: 0.000000  loss: 4.1996 (4.1837)  class_acc: 0.3333 (0.3212)  loss_scale: 32768.0000 (34605.1076)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0642  max mem: 15572
Epoch: [33]  [1320/2809]  eta: 0:14:16  lr: 0.000004  min_lr: 0.000000  loss: 4.2982 (4.1839)  class_acc: 0.3333 (0.3208)  loss_scale: 32768.0000 (34591.2006)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1063  max mem: 15572
Epoch: [33]  [1330/2809]  eta: 0:14:10  lr: 0.000004  min_lr: 0.000000  loss: 4.2142 (4.1840)  class_acc: 0.2917 (0.3209)  loss_scale: 32768.0000 (34577.5026)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0794  max mem: 15572
Epoch: [33]  [1340/2809]  eta: 0:14:04  lr: 0.000004  min_lr: 0.000000  loss: 4.2041 (4.1839)  class_acc: 0.3333 (0.3212)  loss_scale: 32768.0000 (34564.0089)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0888  max mem: 15572
Epoch: [33]  [1350/2809]  eta: 0:13:59  lr: 0.000004  min_lr: 0.000000  loss: 4.1706 (4.1838)  class_acc: 0.3333 (0.3216)  loss_scale: 32768.0000 (34550.7150)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1695  max mem: 15572
Epoch: [33]  [1360/2809]  eta: 0:13:53  lr: 0.000004  min_lr: 0.000000  loss: 4.2808 (4.1845)  class_acc: 0.3333 (0.3219)  loss_scale: 32768.0000 (34537.6165)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1745  max mem: 15572
Epoch: [33]  [1370/2809]  eta: 0:13:47  lr: 0.000004  min_lr: 0.000000  loss: 4.2640 (4.1850)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (34524.7090)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0879  max mem: 15572
Epoch: [33]  [1380/2809]  eta: 0:13:41  lr: 0.000004  min_lr: 0.000000  loss: 4.2300 (4.1855)  class_acc: 0.3333 (0.3221)  loss_scale: 32768.0000 (34511.9884)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0649  max mem: 15572
Epoch: [33]  [1390/2809]  eta: 0:13:35  lr: 0.000004  min_lr: 0.000000  loss: 4.3116 (4.1858)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (34499.4508)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1172  max mem: 15572
Epoch: [33]  [1400/2809]  eta: 0:13:29  lr: 0.000004  min_lr: 0.000000  loss: 4.2693 (4.1855)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (34487.0921)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1589  max mem: 15572
[2025-01-16 06:16:07,326] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:16:07,326] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [1410/2809]  eta: 0:13:24  lr: 0.000004  min_lr: 0.000000  loss: 4.2693 (4.1863)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (34660.6945)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1633  max mem: 15572
Epoch: [33]  [1420/2809]  eta: 0:13:18  lr: 0.000004  min_lr: 0.000000  loss: 4.2572 (4.1870)  class_acc: 0.2917 (0.3223)  loss_scale: 65536.0000 (34877.9733)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1536  max mem: 15572
Epoch: [33]  [1430/2809]  eta: 0:13:13  lr: 0.000004  min_lr: 0.000000  loss: 4.1624 (4.1869)  class_acc: 0.2917 (0.3223)  loss_scale: 65536.0000 (35092.2152)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1482  max mem: 15572
Epoch: [33]  [1440/2809]  eta: 0:13:07  lr: 0.000004  min_lr: 0.000000  loss: 4.1485 (4.1860)  class_acc: 0.3333 (0.3225)  loss_scale: 65536.0000 (35303.4837)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1500  max mem: 15572
Epoch: [33]  [1450/2809]  eta: 0:13:02  lr: 0.000004  min_lr: 0.000000  loss: 4.0627 (4.1852)  class_acc: 0.3333 (0.3225)  loss_scale: 65536.0000 (35511.8401)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1604  max mem: 15572
Epoch: [33]  [1460/2809]  eta: 0:12:55  lr: 0.000004  min_lr: 0.000000  loss: 4.0642 (4.1842)  class_acc: 0.2500 (0.3222)  loss_scale: 65536.0000 (35717.3443)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1198  max mem: 15572
Epoch: [33]  [1470/2809]  eta: 0:12:49  lr: 0.000004  min_lr: 0.000000  loss: 4.2494 (4.1849)  class_acc: 0.2917 (0.3223)  loss_scale: 65536.0000 (35920.0544)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0731  max mem: 15572
[2025-01-16 06:16:47,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94170
[2025-01-16 06:16:47,462] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:16:47,462] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1480/2809]  eta: 0:12:44  lr: 0.000004  min_lr: 0.000000  loss: 4.3421 (4.1851)  class_acc: 0.3333 (0.3227)  loss_scale: 65536.0000 (35943.0223)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1421  max mem: 15572
Epoch: [33]  [1490/2809]  eta: 0:12:38  lr: 0.000004  min_lr: 0.000000  loss: 4.0859 (4.1843)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (35921.7277)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1724  max mem: 15572
Epoch: [33]  [1500/2809]  eta: 0:12:32  lr: 0.000004  min_lr: 0.000000  loss: 4.1841 (4.1846)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (35900.7169)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1214  max mem: 15572
Epoch: [33]  [1510/2809]  eta: 0:12:26  lr: 0.000004  min_lr: 0.000000  loss: 4.0960 (4.1842)  class_acc: 0.2917 (0.3230)  loss_scale: 32768.0000 (35879.9841)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.1086  max mem: 15572
Epoch: [33]  [1520/2809]  eta: 0:12:20  lr: 0.000004  min_lr: 0.000000  loss: 4.2540 (4.1849)  class_acc: 0.2917 (0.3228)  loss_scale: 32768.0000 (35859.5240)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1042  max mem: 15572
Epoch: [33]  [1530/2809]  eta: 0:12:15  lr: 0.000004  min_lr: 0.000000  loss: 4.2540 (4.1848)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (35839.3312)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1442  max mem: 15572
Epoch: [33]  [1540/2809]  eta: 0:12:09  lr: 0.000004  min_lr: 0.000000  loss: 4.0948 (4.1842)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (35819.4004)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1363  max mem: 15572
Epoch: [33]  [1550/2809]  eta: 0:12:03  lr: 0.000004  min_lr: 0.000000  loss: 4.1990 (4.1846)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (35799.7266)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0606  max mem: 15572
Epoch: [33]  [1560/2809]  eta: 0:11:57  lr: 0.000004  min_lr: 0.000000  loss: 4.2784 (4.1850)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (35780.3049)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1230  max mem: 15572
Epoch: [33]  [1570/2809]  eta: 0:11:52  lr: 0.000004  min_lr: 0.000000  loss: 4.0803 (4.1842)  class_acc: 0.3750 (0.3231)  loss_scale: 32768.0000 (35761.1305)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1919  max mem: 15572
Epoch: [33]  [1580/2809]  eta: 0:11:46  lr: 0.000004  min_lr: 0.000000  loss: 4.1119 (4.1846)  class_acc: 0.2917 (0.3229)  loss_scale: 32768.0000 (35742.1986)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1798  max mem: 15572
Epoch: [33]  [1590/2809]  eta: 0:11:42  lr: 0.000004  min_lr: 0.000000  loss: 4.2215 (4.1847)  class_acc: 0.2500 (0.3230)  loss_scale: 32768.0000 (35723.5047)  weight_decay: 0.0500 (0.0500)  time: 0.6547  data: 0.2062  max mem: 15572
Epoch: [33]  [1600/2809]  eta: 0:11:35  lr: 0.000004  min_lr: 0.000000  loss: 4.2273 (4.1855)  class_acc: 0.2500 (0.3228)  loss_scale: 32768.0000 (35705.0443)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1238  max mem: 15572
[2025-01-16 06:18:02,427] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:18:02,428] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 06:18:03,244] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94301
[2025-01-16 06:18:03,245] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:18:03,245] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1610/2809]  eta: 0:11:28  lr: 0.000004  min_lr: 0.000000  loss: 4.2160 (4.1855)  class_acc: 0.3750 (0.3232)  loss_scale: 32768.0000 (35727.4935)  weight_decay: 0.0500 (0.0500)  time: 0.4336  data: 0.0110  max mem: 15572
Epoch: [33]  [1620/2809]  eta: 0:11:23  lr: 0.000004  min_lr: 0.000000  loss: 4.2600 (4.1862)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (35709.2363)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.1004  max mem: 15572
Epoch: [33]  [1630/2809]  eta: 0:11:17  lr: 0.000004  min_lr: 0.000000  loss: 4.2463 (4.1856)  class_acc: 0.2917 (0.3231)  loss_scale: 32768.0000 (35691.2029)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1730  max mem: 15572
Epoch: [33]  [1640/2809]  eta: 0:11:11  lr: 0.000004  min_lr: 0.000000  loss: 4.1951 (4.1855)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (35673.3894)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1401  max mem: 15572
Epoch: [33]  [1650/2809]  eta: 0:11:05  lr: 0.000004  min_lr: 0.000000  loss: 4.2152 (4.1854)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (35655.7916)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1027  max mem: 15572
Epoch: [33]  [1660/2809]  eta: 0:11:00  lr: 0.000004  min_lr: 0.000000  loss: 4.2907 (4.1873)  class_acc: 0.2500 (0.3223)  loss_scale: 32768.0000 (35638.4058)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1039  max mem: 15572
Epoch: [33]  [1670/2809]  eta: 0:10:53  lr: 0.000004  min_lr: 0.000000  loss: 4.4160 (4.1876)  class_acc: 0.2500 (0.3223)  loss_scale: 32768.0000 (35621.2280)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0883  max mem: 15572
Epoch: [33]  [1680/2809]  eta: 0:10:48  lr: 0.000004  min_lr: 0.000000  loss: 4.1186 (4.1874)  class_acc: 0.2917 (0.3224)  loss_scale: 32768.0000 (35604.2546)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1234  max mem: 15572
Epoch: [33]  [1690/2809]  eta: 0:10:42  lr: 0.000004  min_lr: 0.000000  loss: 4.0653 (4.1868)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (35587.4820)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1321  max mem: 15572
Epoch: [33]  [1700/2809]  eta: 0:10:36  lr: 0.000004  min_lr: 0.000000  loss: 4.2429 (4.1877)  class_acc: 0.2083 (0.3213)  loss_scale: 32768.0000 (35570.9065)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0736  max mem: 15572
Epoch: [33]  [1710/2809]  eta: 0:10:30  lr: 0.000004  min_lr: 0.000000  loss: 4.3068 (4.1876)  class_acc: 0.2083 (0.3210)  loss_scale: 32768.0000 (35554.5248)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0992  max mem: 15572
Epoch: [33]  [1720/2809]  eta: 0:10:24  lr: 0.000004  min_lr: 0.000000  loss: 4.1270 (4.1875)  class_acc: 0.2500 (0.3207)  loss_scale: 32768.0000 (35538.3335)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1273  max mem: 15572
Epoch: [33]  [1730/2809]  eta: 0:10:19  lr: 0.000004  min_lr: 0.000000  loss: 4.1270 (4.1872)  class_acc: 0.2500 (0.3205)  loss_scale: 32768.0000 (35522.3293)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1349  max mem: 15572
[2025-01-16 06:19:15,845] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:19:15,845] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [1740/2809]  eta: 0:10:13  lr: 0.000004  min_lr: 0.000000  loss: 4.2828 (4.1875)  class_acc: 0.2500 (0.3201)  loss_scale: 32768.0000 (35657.0798)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1484  max mem: 15572
Epoch: [33]  [1750/2809]  eta: 0:10:07  lr: 0.000004  min_lr: 0.000000  loss: 4.2654 (4.1875)  class_acc: 0.2500 (0.3199)  loss_scale: 65536.0000 (35827.7190)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1188  max mem: 15572
[2025-01-16 06:19:28,174] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94452
[2025-01-16 06:19:28,176] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:19:28,177] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1760/2809]  eta: 0:10:01  lr: 0.000004  min_lr: 0.000000  loss: 4.2302 (4.1883)  class_acc: 0.2500 (0.3201)  loss_scale: 65536.0000 (35884.7746)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0736  max mem: 15572
Epoch: [33]  [1770/2809]  eta: 0:09:55  lr: 0.000004  min_lr: 0.000000  loss: 4.3479 (4.1894)  class_acc: 0.2917 (0.3201)  loss_scale: 32768.0000 (35867.1756)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.1031  max mem: 15572
Epoch: [33]  [1780/2809]  eta: 0:09:50  lr: 0.000004  min_lr: 0.000000  loss: 4.3625 (4.1899)  class_acc: 0.2917 (0.3200)  loss_scale: 32768.0000 (35849.7743)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1465  max mem: 15572
Epoch: [33]  [1790/2809]  eta: 0:09:45  lr: 0.000004  min_lr: 0.000000  loss: 4.1914 (4.1898)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (35832.5673)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1825  max mem: 15572
Epoch: [33]  [1800/2809]  eta: 0:09:39  lr: 0.000004  min_lr: 0.000000  loss: 4.1914 (4.1898)  class_acc: 0.2917 (0.3201)  loss_scale: 32768.0000 (35815.5514)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1600  max mem: 15572
Epoch: [33]  [1810/2809]  eta: 0:09:33  lr: 0.000004  min_lr: 0.000000  loss: 4.0685 (4.1891)  class_acc: 0.3333 (0.3205)  loss_scale: 32768.0000 (35798.7234)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1022  max mem: 15572
Epoch: [33]  [1820/2809]  eta: 0:09:27  lr: 0.000004  min_lr: 0.000000  loss: 4.2326 (4.1900)  class_acc: 0.3333 (0.3204)  loss_scale: 32768.0000 (35782.0802)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1315  max mem: 15572
Epoch: [33]  [1830/2809]  eta: 0:09:22  lr: 0.000004  min_lr: 0.000000  loss: 4.1977 (4.1895)  class_acc: 0.3333 (0.3205)  loss_scale: 32768.0000 (35765.6188)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1492  max mem: 15572
Epoch: [33]  [1840/2809]  eta: 0:09:16  lr: 0.000004  min_lr: 0.000000  loss: 4.1150 (4.1903)  class_acc: 0.3333 (0.3207)  loss_scale: 32768.0000 (35749.3362)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1092  max mem: 15572
Epoch: [33]  [1850/2809]  eta: 0:09:10  lr: 0.000004  min_lr: 0.000000  loss: 4.2812 (4.1906)  class_acc: 0.2917 (0.3206)  loss_scale: 32768.0000 (35733.2296)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0599  max mem: 15572
Epoch: [33]  [1860/2809]  eta: 0:09:04  lr: 0.000004  min_lr: 0.000000  loss: 4.3195 (4.1914)  class_acc: 0.2500 (0.3204)  loss_scale: 32768.0000 (35717.2961)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0824  max mem: 15572
Epoch: [33]  [1870/2809]  eta: 0:08:58  lr: 0.000004  min_lr: 0.000000  loss: 3.9829 (4.1900)  class_acc: 0.3333 (0.3208)  loss_scale: 32768.0000 (35701.5329)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1333  max mem: 15572
Epoch: [33]  [1880/2809]  eta: 0:08:52  lr: 0.000004  min_lr: 0.000000  loss: 3.9381 (4.1893)  class_acc: 0.3750 (0.3209)  loss_scale: 32768.0000 (35685.9373)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1188  max mem: 15572
[2025-01-16 06:20:41,929] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:20:41,930] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [1890/2809]  eta: 0:08:47  lr: 0.000004  min_lr: 0.000000  loss: 4.2064 (4.1894)  class_acc: 0.3750 (0.3209)  loss_scale: 32768.0000 (35791.8054)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1089  max mem: 15572
Epoch: [33]  [1900/2809]  eta: 0:08:41  lr: 0.000004  min_lr: 0.000000  loss: 4.2366 (4.1898)  class_acc: 0.2917 (0.3208)  loss_scale: 65536.0000 (35948.2714)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1283  max mem: 15572
Epoch: [33]  [1910/2809]  eta: 0:08:35  lr: 0.000004  min_lr: 0.000000  loss: 4.3249 (4.1904)  class_acc: 0.2500 (0.3207)  loss_scale: 65536.0000 (36103.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1087  max mem: 15572
Epoch: [33]  [1920/2809]  eta: 0:08:29  lr: 0.000004  min_lr: 0.000000  loss: 4.3249 (4.1911)  class_acc: 0.2500 (0.3201)  loss_scale: 65536.0000 (36256.3165)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0746  max mem: 15572
Epoch: [33]  [1930/2809]  eta: 0:08:24  lr: 0.000004  min_lr: 0.000000  loss: 4.1841 (4.1908)  class_acc: 0.2500 (0.3201)  loss_scale: 65536.0000 (36407.9461)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1417  max mem: 15572
[2025-01-16 06:21:11,246] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94632
[2025-01-16 06:21:11,246] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:21:11,246] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1940/2809]  eta: 0:08:18  lr: 0.000004  min_lr: 0.000000  loss: 4.1980 (4.1904)  class_acc: 0.2917 (0.3201)  loss_scale: 65536.0000 (36456.7213)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1596  max mem: 15572
Epoch: [33]  [1950/2809]  eta: 0:08:13  lr: 0.000004  min_lr: 0.000000  loss: 4.1980 (4.1901)  class_acc: 0.2500 (0.3199)  loss_scale: 32768.0000 (36437.8145)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1466  max mem: 15572
Epoch: [33]  [1960/2809]  eta: 0:08:06  lr: 0.000004  min_lr: 0.000000  loss: 4.1283 (4.1901)  class_acc: 0.2500 (0.3197)  loss_scale: 32768.0000 (36419.1005)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1188  max mem: 15572
Epoch: [33]  [1970/2809]  eta: 0:08:01  lr: 0.000004  min_lr: 0.000000  loss: 4.1283 (4.1897)  class_acc: 0.2917 (0.3198)  loss_scale: 32768.0000 (36400.5764)  weight_decay: 0.0500 (0.0500)  time: 0.5208  data: 0.0772  max mem: 15572
Epoch: [33]  [1980/2809]  eta: 0:07:55  lr: 0.000004  min_lr: 0.000000  loss: 4.2578 (4.1903)  class_acc: 0.2917 (0.3197)  loss_scale: 32768.0000 (36382.2393)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1510  max mem: 15572
Epoch: [33]  [1990/2809]  eta: 0:07:50  lr: 0.000004  min_lr: 0.000000  loss: 4.1858 (4.1900)  class_acc: 0.2917 (0.3195)  loss_scale: 32768.0000 (36364.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1881  max mem: 15572
Epoch: [33]  [2000/2809]  eta: 0:07:44  lr: 0.000004  min_lr: 0.000000  loss: 4.1650 (4.1902)  class_acc: 0.2500 (0.3193)  loss_scale: 32768.0000 (36346.1149)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1823  max mem: 15572
Epoch: [33]  [2010/2809]  eta: 0:07:38  lr: 0.000004  min_lr: 0.000000  loss: 4.2990 (4.1910)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (36328.3222)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1338  max mem: 15572
Epoch: [33]  [2020/2809]  eta: 0:07:32  lr: 0.000004  min_lr: 0.000000  loss: 4.3646 (4.1920)  class_acc: 0.2917 (0.3189)  loss_scale: 32768.0000 (36310.7056)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0859  max mem: 15572
Epoch: [33]  [2030/2809]  eta: 0:07:27  lr: 0.000004  min_lr: 0.000000  loss: 4.2677 (4.1911)  class_acc: 0.3333 (0.3192)  loss_scale: 32768.0000 (36293.2624)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1485  max mem: 15572
Epoch: [33]  [2040/2809]  eta: 0:07:21  lr: 0.000004  min_lr: 0.000000  loss: 4.0523 (4.1906)  class_acc: 0.3750 (0.3193)  loss_scale: 32768.0000 (36275.9902)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1557  max mem: 15572
Epoch: [33]  [2050/2809]  eta: 0:07:15  lr: 0.000004  min_lr: 0.000000  loss: 4.0326 (4.1897)  class_acc: 0.3333 (0.3196)  loss_scale: 32768.0000 (36258.8864)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0580  max mem: 15572
Epoch: [33]  [2060/2809]  eta: 0:07:09  lr: 0.000004  min_lr: 0.000000  loss: 4.1607 (4.1902)  class_acc: 0.2917 (0.3193)  loss_scale: 32768.0000 (36241.9486)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0694  max mem: 15572
[2025-01-16 06:22:25,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:22:25,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 06:22:25,899] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94762
[2025-01-16 06:22:25,900] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:22:25,900] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2070/2809]  eta: 0:07:04  lr: 0.000004  min_lr: 0.000000  loss: 4.1607 (4.1892)  class_acc: 0.2500 (0.3194)  loss_scale: 32768.0000 (36240.9966)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1356  max mem: 15572
Epoch: [33]  [2080/2809]  eta: 0:06:58  lr: 0.000004  min_lr: 0.000000  loss: 4.2680 (4.1901)  class_acc: 0.2083 (0.3191)  loss_scale: 32768.0000 (36224.3075)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0929  max mem: 15572
Epoch: [33]  [2090/2809]  eta: 0:06:52  lr: 0.000004  min_lr: 0.000000  loss: 4.3356 (4.1906)  class_acc: 0.2083 (0.3188)  loss_scale: 32768.0000 (36207.7781)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0959  max mem: 15572
Epoch: [33]  [2100/2809]  eta: 0:06:46  lr: 0.000004  min_lr: 0.000000  loss: 4.2525 (4.1908)  class_acc: 0.2917 (0.3189)  loss_scale: 32768.0000 (36191.4060)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1235  max mem: 15572
Epoch: [33]  [2110/2809]  eta: 0:06:40  lr: 0.000004  min_lr: 0.000000  loss: 4.1222 (4.1904)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (36175.1890)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0995  max mem: 15572
Epoch: [33]  [2120/2809]  eta: 0:06:35  lr: 0.000004  min_lr: 0.000000  loss: 4.1415 (4.1901)  class_acc: 0.3333 (0.3191)  loss_scale: 32768.0000 (36159.1249)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1069  max mem: 15572
Epoch: [33]  [2130/2809]  eta: 0:06:29  lr: 0.000004  min_lr: 0.000000  loss: 4.1293 (4.1894)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (36143.2116)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1140  max mem: 15572
Epoch: [33]  [2140/2809]  eta: 0:06:23  lr: 0.000004  min_lr: 0.000000  loss: 4.1935 (4.1898)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (36127.4470)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.1643  max mem: 15572
Epoch: [33]  [2150/2809]  eta: 0:06:17  lr: 0.000004  min_lr: 0.000000  loss: 4.3031 (4.1902)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (36111.8289)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1061  max mem: 15572
Epoch: [33]  [2160/2809]  eta: 0:06:11  lr: 0.000004  min_lr: 0.000000  loss: 4.2521 (4.1902)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (36096.3554)  weight_decay: 0.0500 (0.0500)  time: 0.4902  data: 0.0262  max mem: 15572
Epoch: [33]  [2170/2809]  eta: 0:06:06  lr: 0.000004  min_lr: 0.000000  loss: 4.1637 (4.1901)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (36081.0244)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1127  max mem: 15572
Epoch: [33]  [2180/2809]  eta: 0:06:00  lr: 0.000004  min_lr: 0.000000  loss: 4.2822 (4.1903)  class_acc: 0.2500 (0.3187)  loss_scale: 32768.0000 (36065.8340)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1796  max mem: 15572
Epoch: [33]  [2190/2809]  eta: 0:05:55  lr: 0.000004  min_lr: 0.000000  loss: 4.1139 (4.1902)  class_acc: 0.2500 (0.3188)  loss_scale: 32768.0000 (36050.7823)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.2034  max mem: 15572
[2025-01-16 06:23:40,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:23:40,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [2200/2809]  eta: 0:05:49  lr: 0.000004  min_lr: 0.000000  loss: 4.1443 (4.1903)  class_acc: 0.2500 (0.3183)  loss_scale: 32768.0000 (36140.0818)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1692  max mem: 15572
[2025-01-16 06:23:45,275] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94900
[2025-01-16 06:23:45,275] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:23:45,330] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2210/2809]  eta: 0:05:43  lr: 0.000004  min_lr: 0.000000  loss: 4.1837 (4.1902)  class_acc: 0.2500 (0.3183)  loss_scale: 32768.0000 (36154.4713)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1437  max mem: 15572
Epoch: [33]  [2220/2809]  eta: 0:05:38  lr: 0.000004  min_lr: 0.000000  loss: 4.0605 (4.1891)  class_acc: 0.3333 (0.3184)  loss_scale: 32768.0000 (36139.2238)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1314  max mem: 15572
Epoch: [33]  [2230/2809]  eta: 0:05:32  lr: 0.000004  min_lr: 0.000000  loss: 4.0605 (4.1892)  class_acc: 0.2917 (0.3184)  loss_scale: 32768.0000 (36124.1130)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1035  max mem: 15572
Epoch: [33]  [2240/2809]  eta: 0:05:26  lr: 0.000004  min_lr: 0.000000  loss: 4.1184 (4.1888)  class_acc: 0.2500 (0.3182)  loss_scale: 32768.0000 (36109.1370)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1215  max mem: 15572
Epoch: [33]  [2250/2809]  eta: 0:05:20  lr: 0.000004  min_lr: 0.000000  loss: 4.1184 (4.1888)  class_acc: 0.2500 (0.3181)  loss_scale: 32768.0000 (36094.2941)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1508  max mem: 15572
Epoch: [33]  [2260/2809]  eta: 0:05:15  lr: 0.000004  min_lr: 0.000000  loss: 4.3343 (4.1890)  class_acc: 0.2500 (0.3180)  loss_scale: 32768.0000 (36079.5825)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1362  max mem: 15572
Epoch: [33]  [2270/2809]  eta: 0:05:09  lr: 0.000004  min_lr: 0.000000  loss: 4.2695 (4.1885)  class_acc: 0.2500 (0.3181)  loss_scale: 32768.0000 (36065.0004)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1013  max mem: 15572
Epoch: [33]  [2280/2809]  eta: 0:05:03  lr: 0.000004  min_lr: 0.000000  loss: 4.1918 (4.1887)  class_acc: 0.3333 (0.3182)  loss_scale: 32768.0000 (36050.5463)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1114  max mem: 15572
Epoch: [33]  [2290/2809]  eta: 0:04:57  lr: 0.000004  min_lr: 0.000000  loss: 4.1379 (4.1884)  class_acc: 0.3333 (0.3180)  loss_scale: 32768.0000 (36036.2182)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1334  max mem: 15572
Epoch: [33]  [2300/2809]  eta: 0:04:52  lr: 0.000004  min_lr: 0.000000  loss: 4.2299 (4.1890)  class_acc: 0.2917 (0.3179)  loss_scale: 32768.0000 (36022.0148)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1217  max mem: 15572
[2025-01-16 06:24:42,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=95000, skipped=590, lr=[3.4476501199824503e-08, 3.4476501199824503e-08, 4.925214457117787e-08, 4.925214457117787e-08, 7.036020653025411e-08, 7.036020653025411e-08, 1.0051458075750587e-07, 1.0051458075750587e-07, 1.435922582250084e-07, 1.435922582250084e-07, 2.0513179746429771e-07, 2.0513179746429771e-07, 2.930454249489968e-07, 2.930454249489968e-07, 4.186363213557097e-07, 4.186363213557097e-07, 5.980518876510139e-07, 5.980518876510139e-07, 8.543598395014485e-07, 8.543598395014485e-07, 1.2205140564306406e-06, 1.2205140564306406e-06, 1.7435915091866297e-06, 1.7435915091866297e-06, 2.4908450131237567e-06, 2.4908450131237567e-06, 3.5583500187482243e-06, 3.5583500187482243e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:24:42,220] [INFO] [timer.py:260:stop] epoch=0/micro_step=95000/global_step=95000, RunningAvgSamplesPerSec=27.958243660405795, CurrSamplesPerSec=23.019339636879863, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [33]  [2310/2809]  eta: 0:04:46  lr: 0.000004  min_lr: 0.000000  loss: 4.1388 (4.1889)  class_acc: 0.3333 (0.3181)  loss_scale: 32768.0000 (36007.9342)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0972  max mem: 15572
Epoch: [33]  [2320/2809]  eta: 0:04:40  lr: 0.000004  min_lr: 0.000000  loss: 4.0675 (4.1885)  class_acc: 0.3333 (0.3180)  loss_scale: 32768.0000 (35993.9750)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1149  max mem: 15572
Epoch: [33]  [2330/2809]  eta: 0:04:34  lr: 0.000004  min_lr: 0.000000  loss: 4.0007 (4.1883)  class_acc: 0.3333 (0.3182)  loss_scale: 32768.0000 (35980.1356)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0766  max mem: 15572
[2025-01-16 06:24:58,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:24:58,431] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [2340/2809]  eta: 0:04:28  lr: 0.000004  min_lr: 0.000000  loss: 4.1948 (4.1884)  class_acc: 0.3333 (0.3182)  loss_scale: 32768.0000 (36092.3913)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0932  max mem: 15572
Epoch: [33]  [2350/2809]  eta: 0:04:23  lr: 0.000004  min_lr: 0.000000  loss: 4.1948 (4.1882)  class_acc: 0.2917 (0.3182)  loss_scale: 65536.0000 (36217.6299)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1174  max mem: 15572
Epoch: [33]  [2360/2809]  eta: 0:04:17  lr: 0.000004  min_lr: 0.000000  loss: 4.2334 (4.1888)  class_acc: 0.2500 (0.3179)  loss_scale: 65536.0000 (36341.8077)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0627  max mem: 15572
Epoch: [33]  [2370/2809]  eta: 0:04:11  lr: 0.000004  min_lr: 0.000000  loss: 4.2210 (4.1888)  class_acc: 0.2500 (0.3179)  loss_scale: 65536.0000 (36464.9380)  weight_decay: 0.0500 (0.0500)  time: 0.4871  data: 0.0357  max mem: 15572
Epoch: [33]  [2380/2809]  eta: 0:04:05  lr: 0.000004  min_lr: 0.000000  loss: 4.1672 (4.1881)  class_acc: 0.3333 (0.3180)  loss_scale: 65536.0000 (36587.0340)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0869  max mem: 15572
Epoch: [33]  [2390/2809]  eta: 0:04:00  lr: 0.000004  min_lr: 0.000000  loss: 4.2052 (4.1884)  class_acc: 0.3333 (0.3181)  loss_scale: 65536.0000 (36708.1087)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1613  max mem: 15572
[2025-01-16 06:25:32,191] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95090
[2025-01-16 06:25:32,191] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:25:32,191] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2400/2809]  eta: 0:03:54  lr: 0.000004  min_lr: 0.000000  loss: 4.1460 (4.1881)  class_acc: 0.3333 (0.3183)  loss_scale: 65536.0000 (36718.9938)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1385  max mem: 15572
Epoch: [33]  [2410/2809]  eta: 0:03:48  lr: 0.000004  min_lr: 0.000000  loss: 4.1902 (4.1886)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (36702.6064)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1118  max mem: 15572
Epoch: [33]  [2420/2809]  eta: 0:03:43  lr: 0.000004  min_lr: 0.000000  loss: 4.2436 (4.1889)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (36686.3544)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1698  max mem: 15572
Epoch: [33]  [2430/2809]  eta: 0:03:37  lr: 0.000004  min_lr: 0.000000  loss: 4.1824 (4.1881)  class_acc: 0.4167 (0.3189)  loss_scale: 32768.0000 (36670.2361)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1727  max mem: 15572
Epoch: [33]  [2440/2809]  eta: 0:03:31  lr: 0.000004  min_lr: 0.000000  loss: 3.9473 (4.1882)  class_acc: 0.3750 (0.3188)  loss_scale: 32768.0000 (36654.2499)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1253  max mem: 15572
Epoch: [33]  [2450/2809]  eta: 0:03:25  lr: 0.000004  min_lr: 0.000000  loss: 4.2548 (4.1886)  class_acc: 0.3750 (0.3190)  loss_scale: 32768.0000 (36638.3941)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1632  max mem: 15572
Epoch: [33]  [2460/2809]  eta: 0:03:20  lr: 0.000003  min_lr: 0.000000  loss: 4.2346 (4.1884)  class_acc: 0.3750 (0.3192)  loss_scale: 32768.0000 (36622.6672)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1454  max mem: 15572
Epoch: [33]  [2470/2809]  eta: 0:03:14  lr: 0.000003  min_lr: 0.000000  loss: 4.2346 (4.1889)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (36607.0676)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0996  max mem: 15572
Epoch: [33]  [2480/2809]  eta: 0:03:08  lr: 0.000003  min_lr: 0.000000  loss: 4.2438 (4.1890)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (36591.5937)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0595  max mem: 15572
Epoch: [33]  [2490/2809]  eta: 0:03:02  lr: 0.000003  min_lr: 0.000000  loss: 4.3142 (4.1894)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (36576.2441)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0573  max mem: 15572
Epoch: [33]  [2500/2809]  eta: 0:02:56  lr: 0.000003  min_lr: 0.000000  loss: 4.3430 (4.1900)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (36561.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0979  max mem: 15572
Epoch: [33]  [2510/2809]  eta: 0:02:51  lr: 0.000003  min_lr: 0.000000  loss: 4.3271 (4.1903)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (36545.9116)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1487  max mem: 15572
Epoch: [33]  [2520/2809]  eta: 0:02:45  lr: 0.000003  min_lr: 0.000000  loss: 4.1975 (4.1905)  class_acc: 0.3333 (0.3192)  loss_scale: 32768.0000 (36530.9258)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1304  max mem: 15572
[2025-01-16 06:26:47,068] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:26:47,068] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [2530/2809]  eta: 0:02:39  lr: 0.000003  min_lr: 0.000000  loss: 4.1975 (4.1906)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (36632.5784)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1039  max mem: 15572
Epoch: [33]  [2540/2809]  eta: 0:02:34  lr: 0.000003  min_lr: 0.000000  loss: 4.3042 (4.1909)  class_acc: 0.3333 (0.3193)  loss_scale: 65536.0000 (36746.3266)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1191  max mem: 15572
[2025-01-16 06:27:00,396] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95244
[2025-01-16 06:27:00,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:27:00,397] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2550/2809]  eta: 0:02:28  lr: 0.000003  min_lr: 0.000000  loss: 4.3186 (4.1916)  class_acc: 0.3333 (0.3194)  loss_scale: 65536.0000 (36807.8024)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0816  max mem: 15572
Epoch: [33]  [2560/2809]  eta: 0:02:22  lr: 0.000003  min_lr: 0.000000  loss: 4.2719 (4.1919)  class_acc: 0.3333 (0.3194)  loss_scale: 32768.0000 (36792.0281)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1319  max mem: 15572
Epoch: [33]  [2570/2809]  eta: 0:02:16  lr: 0.000003  min_lr: 0.000000  loss: 4.2719 (4.1925)  class_acc: 0.3333 (0.3192)  loss_scale: 32768.0000 (36776.3765)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1478  max mem: 15572
Epoch: [33]  [2580/2809]  eta: 0:02:11  lr: 0.000003  min_lr: 0.000000  loss: 4.2229 (4.1925)  class_acc: 0.3750 (0.3194)  loss_scale: 32768.0000 (36760.8462)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1336  max mem: 15572
Epoch: [33]  [2590/2809]  eta: 0:02:05  lr: 0.000003  min_lr: 0.000000  loss: 4.1985 (4.1926)  class_acc: 0.3750 (0.3197)  loss_scale: 32768.0000 (36745.4357)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1436  max mem: 15572
Epoch: [33]  [2600/2809]  eta: 0:01:59  lr: 0.000003  min_lr: 0.000000  loss: 4.2111 (4.1925)  class_acc: 0.3750 (0.3195)  loss_scale: 32768.0000 (36730.1438)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1306  max mem: 15572
Epoch: [33]  [2610/2809]  eta: 0:01:54  lr: 0.000003  min_lr: 0.000000  loss: 4.2173 (4.1925)  class_acc: 0.2917 (0.3197)  loss_scale: 32768.0000 (36714.9690)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1018  max mem: 15572
Epoch: [33]  [2620/2809]  eta: 0:01:48  lr: 0.000003  min_lr: 0.000000  loss: 4.2355 (4.1928)  class_acc: 0.2917 (0.3195)  loss_scale: 32768.0000 (36699.9100)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0757  max mem: 15572
Epoch: [33]  [2630/2809]  eta: 0:01:42  lr: 0.000003  min_lr: 0.000000  loss: 4.1777 (4.1925)  class_acc: 0.2917 (0.3196)  loss_scale: 32768.0000 (36684.9654)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0532  max mem: 15572
Epoch: [33]  [2640/2809]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000000  loss: 4.1068 (4.1922)  class_acc: 0.2917 (0.3196)  loss_scale: 32768.0000 (36670.1340)  weight_decay: 0.0500 (0.0500)  time: 0.4320  data: 0.0102  max mem: 15572
Epoch: [33]  [2650/2809]  eta: 0:01:30  lr: 0.000003  min_lr: 0.000000  loss: 4.2038 (4.1924)  class_acc: 0.2917 (0.3196)  loss_scale: 32768.0000 (36655.4146)  weight_decay: 0.0500 (0.0500)  time: 0.4448  data: 0.0007  max mem: 15572
Epoch: [33]  [2660/2809]  eta: 0:01:25  lr: 0.000003  min_lr: 0.000000  loss: 4.1292 (4.1919)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (36640.8057)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0008  max mem: 15572
Epoch: [33]  [2670/2809]  eta: 0:01:19  lr: 0.000003  min_lr: 0.000000  loss: 4.0686 (4.1912)  class_acc: 0.3750 (0.3199)  loss_scale: 32768.0000 (36626.3063)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0009  max mem: 15572
[2025-01-16 06:28:10,815] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:28:10,815] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 06:28:12,329] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95376
[2025-01-16 06:28:12,330] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:28:12,330] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2680/2809]  eta: 0:01:13  lr: 0.000003  min_lr: 0.000000  loss: 4.1777 (4.1913)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (36648.5819)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0775  max mem: 15572
Epoch: [33]  [2690/2809]  eta: 0:01:08  lr: 0.000003  min_lr: 0.000000  loss: 4.1491 (4.1910)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (36634.1613)  weight_decay: 0.0500 (0.0500)  time: 0.6704  data: 0.1517  max mem: 15572
Epoch: [33]  [2700/2809]  eta: 0:01:02  lr: 0.000003  min_lr: 0.000000  loss: 4.1491 (4.1911)  class_acc: 0.2917 (0.3200)  loss_scale: 32768.0000 (36619.8475)  weight_decay: 0.0500 (0.0500)  time: 0.6713  data: 0.1707  max mem: 15572
Epoch: [33]  [2710/2809]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 4.3317 (4.1917)  class_acc: 0.2500 (0.3199)  loss_scale: 32768.0000 (36605.6392)  weight_decay: 0.0500 (0.0500)  time: 0.6738  data: 0.1852  max mem: 15572
Epoch: [33]  [2720/2809]  eta: 0:00:50  lr: 0.000003  min_lr: 0.000000  loss: 4.2012 (4.1914)  class_acc: 0.2500 (0.3198)  loss_scale: 32768.0000 (36591.5355)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.1817  max mem: 15572
Epoch: [33]  [2730/2809]  eta: 0:00:45  lr: 0.000003  min_lr: 0.000000  loss: 4.2538 (4.1919)  class_acc: 0.2500 (0.3196)  loss_scale: 32768.0000 (36577.5350)  weight_decay: 0.0500 (0.0500)  time: 0.6978  data: 0.2186  max mem: 15572
Epoch: [33]  [2740/2809]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000000  loss: 4.2361 (4.1918)  class_acc: 0.2500 (0.3195)  loss_scale: 32768.0000 (36563.6366)  weight_decay: 0.0500 (0.0500)  time: 0.7008  data: 0.2251  max mem: 15572
Epoch: [33]  [2750/2809]  eta: 0:00:33  lr: 0.000003  min_lr: 0.000000  loss: 3.9884 (4.1907)  class_acc: 0.3333 (0.3197)  loss_scale: 32768.0000 (36549.8393)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1285  max mem: 15572
Epoch: [33]  [2760/2809]  eta: 0:00:28  lr: 0.000003  min_lr: 0.000000  loss: 4.1621 (4.1913)  class_acc: 0.2917 (0.3196)  loss_scale: 32768.0000 (36536.1420)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1484  max mem: 15572
Epoch: [33]  [2770/2809]  eta: 0:00:22  lr: 0.000003  min_lr: 0.000000  loss: 4.2907 (4.1911)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (36522.5435)  weight_decay: 0.0500 (0.0500)  time: 0.6734  data: 0.2283  max mem: 15572
Epoch: [33]  [2780/2809]  eta: 0:00:16  lr: 0.000003  min_lr: 0.000000  loss: 4.1270 (4.1911)  class_acc: 0.3750 (0.3201)  loss_scale: 32768.0000 (36509.0428)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1492  max mem: 15572
Epoch: [33]  [2790/2809]  eta: 0:00:10  lr: 0.000003  min_lr: 0.000000  loss: 4.2088 (4.1917)  class_acc: 0.3333 (0.3201)  loss_scale: 32768.0000 (36495.6388)  weight_decay: 0.0500 (0.0500)  time: 0.4659  data: 0.0400  max mem: 15572
Epoch: [33]  [2800/2809]  eta: 0:00:05  lr: 0.000003  min_lr: 0.000000  loss: 4.3099 (4.1918)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (36482.3306)  weight_decay: 0.0500 (0.0500)  time: 0.3825  data: 0.0003  max mem: 15572
[2025-01-16 06:29:29,115] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:29:29,115] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [2808/2809]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 4.2652 (4.1918)  class_acc: 0.2917 (0.3200)  loss_scale: 32768.0000 (36483.4176)  weight_decay: 0.0500 (0.0500)  time: 0.3793  data: 0.0003  max mem: 15572
Epoch: [33] Total time: 0:26:49 (0.5728 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 4.2652 (4.1918)  class_acc: 0.2917 (0.3200)  loss_scale: 32768.0000 (36483.4176)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:26:47  loss: 1.0958 (1.0958)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 5.9097  data: 5.7084  max mem: 15572
Val:  [ 10/272]  eta: 0:03:45  loss: 2.4112 (2.4861)  acc1: 44.4444 (47.9798)  acc5: 77.7778 (76.7677)  time: 0.8589  data: 0.6795  max mem: 15572
Val:  [ 20/272]  eta: 0:02:20  loss: 2.5782 (2.5640)  acc1: 50.0000 (48.9418)  acc5: 72.2222 (74.8677)  time: 0.2879  data: 0.1094  max mem: 15572
Val:  [ 30/272]  eta: 0:01:46  loss: 2.5782 (2.5999)  acc1: 50.0000 (45.3405)  acc5: 72.2222 (74.1935)  time: 0.2096  data: 0.0215  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 2.6123 (2.6265)  acc1: 27.7778 (41.5989)  acc5: 72.2222 (73.5772)  time: 0.2387  data: 0.0510  max mem: 15572
Val:  [ 50/272]  eta: 0:01:29  loss: 2.6058 (2.5633)  acc1: 38.8889 (42.8105)  acc5: 77.7778 (75.2723)  time: 0.3455  data: 0.1637  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 1.9220 (2.4959)  acc1: 55.5556 (44.5355)  acc5: 88.8889 (76.4117)  time: 0.3576  data: 0.1548  max mem: 15572
Val:  [ 70/272]  eta: 0:01:16  loss: 1.9802 (2.4357)  acc1: 61.1111 (46.8701)  acc5: 83.3333 (77.3865)  time: 0.3177  data: 0.1181  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 2.1599 (2.4427)  acc1: 50.0000 (46.7764)  acc5: 83.3333 (77.2291)  time: 0.2788  data: 0.0924  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 2.4250 (2.4418)  acc1: 44.4444 (46.7643)  acc5: 77.7778 (77.6557)  time: 0.2764  data: 0.0681  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.4404 (2.4673)  acc1: 44.4444 (46.0946)  acc5: 77.7778 (77.4477)  time: 0.3118  data: 0.1055  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.6394 (2.5211)  acc1: 33.3333 (44.3944)  acc5: 77.7778 (76.6266)  time: 0.2819  data: 0.0938  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 2.9272 (2.5534)  acc1: 27.7778 (43.7098)  acc5: 66.6667 (75.8953)  time: 0.2756  data: 0.0915  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.5006 (2.5282)  acc1: 44.4444 (44.2748)  acc5: 77.7778 (76.5479)  time: 0.3127  data: 0.1223  max mem: 15572
Val:  [140/272]  eta: 0:00:43  loss: 2.1699 (2.5305)  acc1: 50.0000 (44.6809)  acc5: 83.3333 (76.4381)  time: 0.3031  data: 0.1066  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 2.4811 (2.5268)  acc1: 38.8889 (44.3341)  acc5: 77.7778 (76.6004)  time: 0.2952  data: 0.0949  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.4813 (2.5247)  acc1: 44.4444 (44.7895)  acc5: 77.7778 (76.8116)  time: 0.3309  data: 0.1333  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.5669 (2.5400)  acc1: 44.4444 (44.3795)  acc5: 77.7778 (76.2183)  time: 0.3285  data: 0.1310  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.5315 (2.5278)  acc1: 33.3333 (44.3217)  acc5: 72.2222 (76.4273)  time: 0.3007  data: 0.0970  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.4664 (2.5650)  acc1: 38.8889 (43.3682)  acc5: 72.2222 (75.2763)  time: 0.3046  data: 0.1057  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.6932 (2.5681)  acc1: 38.8889 (42.8966)  acc5: 72.2222 (75.2073)  time: 0.3324  data: 0.1359  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.4333 (2.5772)  acc1: 38.8889 (42.5750)  acc5: 77.7778 (74.9868)  time: 0.3330  data: 0.1465  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.6466 (2.5729)  acc1: 38.8889 (42.6345)  acc5: 72.2222 (74.9623)  time: 0.3106  data: 0.1379  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.1069 (2.5563)  acc1: 50.0000 (43.5065)  acc5: 77.7778 (75.2766)  time: 0.2980  data: 0.1114  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1007 (2.5434)  acc1: 55.5556 (43.7529)  acc5: 83.3333 (75.6570)  time: 0.2841  data: 0.0813  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4186 (2.5491)  acc1: 38.8889 (43.3156)  acc5: 83.3333 (75.6308)  time: 0.3280  data: 0.1349  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9374 (2.5090)  acc1: 55.5556 (44.6573)  acc5: 88.8889 (76.2878)  time: 0.3319  data: 0.1485  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.8837 (2.5042)  acc1: 66.6667 (44.8339)  acc5: 88.8889 (76.4453)  time: 0.2184  data: 0.0498  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8837 (2.5071)  acc1: 66.6667 (44.8290)  acc5: 88.8889 (76.4284)  time: 0.2121  data: 0.0498  max mem: 15572
Val: Total time: 0:01:26 (0.3192 s / it)
* Acc@1 44.829 Acc@5 76.428 loss 2.507
Accuracy of the network on the 4883 val videos: 44.8%
Max accuracy: 44.85%
Epoch: [34]  [   0/2809]  eta: 6:40:56  lr: 0.000003  min_lr: 0.000000  loss: 4.0357 (4.0357)  class_acc: 0.5000 (0.5000)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.5642  data: 8.1415  max mem: 15572
Epoch: [34]  [  10/2809]  eta: 0:55:10  lr: 0.000003  min_lr: 0.000000  loss: 4.3508 (4.3094)  class_acc: 0.4167 (0.4091)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1826  data: 0.7406  max mem: 15572
Epoch: [34]  [  20/2809]  eta: 0:39:43  lr: 0.000003  min_lr: 0.000000  loss: 4.2688 (4.2132)  class_acc: 0.3333 (0.3730)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4689  data: 0.0274  max mem: 15572
Epoch: [34]  [  30/2809]  eta: 0:34:35  lr: 0.000003  min_lr: 0.000000  loss: 4.1841 (4.2232)  class_acc: 0.2917 (0.3575)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0490  max mem: 15572
Epoch: [34]  [  40/2809]  eta: 0:33:00  lr: 0.000003  min_lr: 0.000000  loss: 4.1174 (4.1803)  class_acc: 0.2917 (0.3486)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1016  max mem: 15572
Epoch: [34]  [  50/2809]  eta: 0:31:44  lr: 0.000003  min_lr: 0.000000  loss: 3.9183 (4.1674)  class_acc: 0.2917 (0.3391)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1445  max mem: 15572
Epoch: [34]  [  60/2809]  eta: 0:31:06  lr: 0.000003  min_lr: 0.000000  loss: 4.2267 (4.1707)  class_acc: 0.3333 (0.3347)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1553  max mem: 15572
[2025-01-16 06:31:42,870] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95572
[2025-01-16 06:31:42,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:31:42,871] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [  70/2809]  eta: 0:31:05  lr: 0.000003  min_lr: 0.000000  loss: 4.1115 (4.1605)  class_acc: 0.3333 (0.3363)  loss_scale: 65536.0000 (63228.3944)  weight_decay: 0.0500 (0.0500)  time: 0.6576  data: 0.2226  max mem: 15572
Epoch: [34]  [  80/2809]  eta: 0:29:59  lr: 0.000003  min_lr: 0.000000  loss: 4.2177 (4.1736)  class_acc: 0.2917 (0.3333)  loss_scale: 32768.0000 (59467.8519)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1630  max mem: 15572
Epoch: [34]  [  90/2809]  eta: 0:29:06  lr: 0.000003  min_lr: 0.000000  loss: 4.2410 (4.1555)  class_acc: 0.3333 (0.3411)  loss_scale: 32768.0000 (56533.8022)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0457  max mem: 15572
Epoch: [34]  [ 100/2809]  eta: 0:28:53  lr: 0.000003  min_lr: 0.000000  loss: 4.2192 (4.1633)  class_acc: 0.3333 (0.3391)  loss_scale: 32768.0000 (54180.7525)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0931  max mem: 15572
Epoch: [34]  [ 110/2809]  eta: 0:28:28  lr: 0.000003  min_lr: 0.000000  loss: 4.0853 (4.1561)  class_acc: 0.3333 (0.3375)  loss_scale: 32768.0000 (52251.6757)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1329  max mem: 15572
Epoch: [34]  [ 120/2809]  eta: 0:27:42  lr: 0.000003  min_lr: 0.000000  loss: 4.0853 (4.1515)  class_acc: 0.3333 (0.3395)  loss_scale: 32768.0000 (50641.4545)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0578  max mem: 15572
Epoch: [34]  [ 130/2809]  eta: 0:27:25  lr: 0.000003  min_lr: 0.000000  loss: 4.1627 (4.1492)  class_acc: 0.4167 (0.3432)  loss_scale: 32768.0000 (49277.0687)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0547  max mem: 15572
Epoch: [34]  [ 140/2809]  eta: 0:27:20  lr: 0.000003  min_lr: 0.000000  loss: 4.1373 (4.1463)  class_acc: 0.3750 (0.3422)  loss_scale: 32768.0000 (48106.2128)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1293  max mem: 15572
Epoch: [34]  [ 150/2809]  eta: 0:26:55  lr: 0.000003  min_lr: 0.000000  loss: 3.9215 (4.1455)  class_acc: 0.2500 (0.3372)  loss_scale: 32768.0000 (47090.4371)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1077  max mem: 15572
Epoch: [34]  [ 160/2809]  eta: 0:26:44  lr: 0.000003  min_lr: 0.000000  loss: 4.1251 (4.1469)  class_acc: 0.2500 (0.3385)  loss_scale: 32768.0000 (46200.8447)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0932  max mem: 15572
Epoch: [34]  [ 170/2809]  eta: 0:26:30  lr: 0.000003  min_lr: 0.000000  loss: 4.2214 (4.1576)  class_acc: 0.2917 (0.3360)  loss_scale: 32768.0000 (45415.2982)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1216  max mem: 15572
Epoch: [34]  [ 180/2809]  eta: 0:26:10  lr: 0.000003  min_lr: 0.000000  loss: 4.2932 (4.1621)  class_acc: 0.2917 (0.3336)  loss_scale: 32768.0000 (44716.5525)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0763  max mem: 15572
Epoch: [34]  [ 190/2809]  eta: 0:26:12  lr: 0.000003  min_lr: 0.000000  loss: 4.2132 (4.1576)  class_acc: 0.2500 (0.3314)  loss_scale: 32768.0000 (44090.9738)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1235  max mem: 15572
[2025-01-16 06:32:53,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:32:53,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 200/2809]  eta: 0:25:44  lr: 0.000003  min_lr: 0.000000  loss: 4.2142 (4.1628)  class_acc: 0.2500 (0.3277)  loss_scale: 32768.0000 (44505.7910)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1056  max mem: 15572
[2025-01-16 06:32:57,932] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95710
[2025-01-16 06:32:57,932] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:32:57,933] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 210/2809]  eta: 0:25:35  lr: 0.000003  min_lr: 0.000000  loss: 4.2142 (4.1618)  class_acc: 0.2917 (0.3310)  loss_scale: 32768.0000 (44415.3934)  weight_decay: 0.0500 (0.0500)  time: 0.4973  data: 0.0684  max mem: 15572
Epoch: [34]  [ 220/2809]  eta: 0:25:26  lr: 0.000003  min_lr: 0.000000  loss: 4.2140 (4.1658)  class_acc: 0.3333 (0.3279)  loss_scale: 32768.0000 (43888.3620)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1256  max mem: 15572
Epoch: [34]  [ 230/2809]  eta: 0:25:13  lr: 0.000003  min_lr: 0.000000  loss: 4.2658 (4.1683)  class_acc: 0.2917 (0.3272)  loss_scale: 32768.0000 (43406.9610)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0996  max mem: 15572
Epoch: [34]  [ 240/2809]  eta: 0:25:04  lr: 0.000003  min_lr: 0.000000  loss: 4.2658 (4.1683)  class_acc: 0.3333 (0.3268)  loss_scale: 32768.0000 (42965.5104)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0917  max mem: 15572
Epoch: [34]  [ 250/2809]  eta: 0:25:06  lr: 0.000003  min_lr: 0.000000  loss: 4.3682 (4.1767)  class_acc: 0.2917 (0.3240)  loss_scale: 32768.0000 (42559.2351)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1651  max mem: 15572
Epoch: [34]  [ 260/2809]  eta: 0:24:48  lr: 0.000003  min_lr: 0.000000  loss: 4.2764 (4.1746)  class_acc: 0.2917 (0.3265)  loss_scale: 32768.0000 (42184.0920)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1330  max mem: 15572
Epoch: [34]  [ 270/2809]  eta: 0:24:47  lr: 0.000003  min_lr: 0.000000  loss: 4.2079 (4.1741)  class_acc: 0.3333 (0.3243)  loss_scale: 32768.0000 (41836.6347)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1173  max mem: 15572
Epoch: [34]  [ 280/2809]  eta: 0:24:36  lr: 0.000003  min_lr: 0.000000  loss: 4.2760 (4.1781)  class_acc: 0.2500 (0.3244)  loss_scale: 32768.0000 (41513.9075)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1353  max mem: 15572
Epoch: [34]  [ 290/2809]  eta: 0:24:29  lr: 0.000003  min_lr: 0.000000  loss: 4.3318 (4.1854)  class_acc: 0.3333 (0.3252)  loss_scale: 32768.0000 (41213.3608)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0879  max mem: 15572
Epoch: [34]  [ 300/2809]  eta: 0:24:17  lr: 0.000003  min_lr: 0.000000  loss: 4.2882 (4.1863)  class_acc: 0.2917 (0.3245)  loss_scale: 32768.0000 (40932.7841)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0866  max mem: 15572
Epoch: [34]  [ 310/2809]  eta: 0:24:09  lr: 0.000003  min_lr: 0.000000  loss: 4.1007 (4.1875)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (40670.2508)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.1016  max mem: 15572
Epoch: [34]  [ 320/2809]  eta: 0:24:07  lr: 0.000003  min_lr: 0.000000  loss: 4.1007 (4.1877)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (40424.0748)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1477  max mem: 15572
Epoch: [34]  [ 330/2809]  eta: 0:23:52  lr: 0.000003  min_lr: 0.000000  loss: 4.2877 (4.1930)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (40192.7734)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0989  max mem: 15572
[2025-01-16 06:34:09,133] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:34:09,133] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 06:34:13,291] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95846
[2025-01-16 06:34:13,292] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:34:13,294] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 340/2809]  eta: 0:23:45  lr: 0.000003  min_lr: 0.000000  loss: 4.2877 (4.1938)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (40647.6950)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0747  max mem: 15572
Epoch: [34]  [ 350/2809]  eta: 0:23:35  lr: 0.000003  min_lr: 0.000000  loss: 4.2758 (4.1968)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (40423.2023)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0968  max mem: 15572
Epoch: [34]  [ 360/2809]  eta: 0:23:24  lr: 0.000003  min_lr: 0.000000  loss: 4.2008 (4.1922)  class_acc: 0.3333 (0.3241)  loss_scale: 32768.0000 (40211.1468)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0656  max mem: 15572
Epoch: [34]  [ 370/2809]  eta: 0:23:23  lr: 0.000003  min_lr: 0.000000  loss: 4.1803 (4.1931)  class_acc: 0.2917 (0.3237)  loss_scale: 32768.0000 (40010.5229)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1296  max mem: 15572
Epoch: [34]  [ 380/2809]  eta: 0:23:23  lr: 0.000003  min_lr: 0.000000  loss: 4.2773 (4.1929)  class_acc: 0.2917 (0.3234)  loss_scale: 32768.0000 (39820.4304)  weight_decay: 0.0500 (0.0500)  time: 0.6620  data: 0.2103  max mem: 15572
Epoch: [34]  [ 390/2809]  eta: 0:23:14  lr: 0.000003  min_lr: 0.000000  loss: 4.2506 (4.1916)  class_acc: 0.2500 (0.3238)  loss_scale: 32768.0000 (39640.0614)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1383  max mem: 15572
Epoch: [34]  [ 400/2809]  eta: 0:23:09  lr: 0.000003  min_lr: 0.000000  loss: 4.2467 (4.1925)  class_acc: 0.2500 (0.3223)  loss_scale: 32768.0000 (39468.6883)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0944  max mem: 15572
Epoch: [34]  [ 410/2809]  eta: 0:23:04  lr: 0.000003  min_lr: 0.000000  loss: 4.2422 (4.1912)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (39305.6545)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1280  max mem: 15572
Epoch: [34]  [ 420/2809]  eta: 0:22:58  lr: 0.000003  min_lr: 0.000000  loss: 4.1678 (4.1913)  class_acc: 0.2917 (0.3208)  loss_scale: 32768.0000 (39150.3658)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1353  max mem: 15572
Epoch: [34]  [ 430/2809]  eta: 0:22:56  lr: 0.000003  min_lr: 0.000000  loss: 4.1317 (4.1884)  class_acc: 0.2500 (0.3198)  loss_scale: 32768.0000 (39002.2831)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1648  max mem: 15572
Epoch: [34]  [ 440/2809]  eta: 0:22:52  lr: 0.000003  min_lr: 0.000000  loss: 4.1317 (4.1891)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (38860.9161)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1792  max mem: 15572
Epoch: [34]  [ 450/2809]  eta: 0:22:40  lr: 0.000003  min_lr: 0.000000  loss: 4.2081 (4.1877)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0865  max mem: 15572
Epoch: [34]  [ 460/2809]  eta: 0:22:30  lr: 0.000003  min_lr: 0.000000  loss: 4.2855 (4.1889)  class_acc: 0.2917 (0.3180)  loss_scale: 32768.0000 (38596.5813)  weight_decay: 0.0500 (0.0500)  time: 0.4790  data: 0.0354  max mem: 15572
[2025-01-16 06:35:26,552] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:35:26,553] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 470/2809]  eta: 0:22:23  lr: 0.000003  min_lr: 0.000000  loss: 4.2923 (4.1902)  class_acc: 0.1667 (0.3168)  loss_scale: 32768.0000 (38611.9745)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0772  max mem: 15572
Epoch: [34]  [ 480/2809]  eta: 0:22:16  lr: 0.000003  min_lr: 0.000000  loss: 4.1623 (4.1891)  class_acc: 0.3333 (0.3171)  loss_scale: 65536.0000 (39171.7256)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1079  max mem: 15572
Epoch: [34]  [ 490/2809]  eta: 0:22:10  lr: 0.000003  min_lr: 0.000000  loss: 4.0257 (4.1864)  class_acc: 0.3333 (0.3175)  loss_scale: 65536.0000 (39708.6762)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1264  max mem: 15572
[2025-01-16 06:35:40,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=96000, skipped=596, lr=[3.075723261633489e-08, 3.075723261633489e-08, 4.393890373762128e-08, 4.393890373762128e-08, 6.276986248231613e-08, 6.276986248231613e-08, 8.967123211759446e-08, 8.967123211759446e-08, 1.281017601679921e-07, 1.281017601679921e-07, 1.83002514525703e-07, 1.83002514525703e-07, 2.6143216360814715e-07, 2.6143216360814715e-07, 3.734745194402103e-07, 3.734745194402103e-07, 5.335350277717289e-07, 5.335350277717289e-07, 7.621928968167557e-07, 7.621928968167557e-07, 1.0888469954525083e-06, 1.0888469954525083e-06, 1.5554957077892975e-06, 1.5554957077892975e-06, 2.222136725413282e-06, 2.222136725413282e-06, 3.1744810363046893e-06, 3.1744810363046893e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:35:40,584] [INFO] [timer.py:260:stop] epoch=0/micro_step=96000/global_step=96000, RunningAvgSamplesPerSec=27.95943533350827, CurrSamplesPerSec=29.16872042583894, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [ 500/2809]  eta: 0:22:03  lr: 0.000003  min_lr: 0.000000  loss: 4.1837 (4.1874)  class_acc: 0.3333 (0.3172)  loss_scale: 65536.0000 (40224.1916)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1217  max mem: 15572
[2025-01-16 06:35:47,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96012
[2025-01-16 06:35:47,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:35:47,591] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 510/2809]  eta: 0:21:58  lr: 0.000003  min_lr: 0.000000  loss: 4.2128 (4.1868)  class_acc: 0.3333 (0.3178)  loss_scale: 65536.0000 (40398.9041)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1393  max mem: 15572
Epoch: [34]  [ 520/2809]  eta: 0:21:53  lr: 0.000003  min_lr: 0.000000  loss: 4.1288 (4.1856)  class_acc: 0.3333 (0.3185)  loss_scale: 32768.0000 (40252.4376)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1489  max mem: 15572
Epoch: [34]  [ 530/2809]  eta: 0:21:46  lr: 0.000003  min_lr: 0.000000  loss: 4.1514 (4.1865)  class_acc: 0.3333 (0.3172)  loss_scale: 32768.0000 (40111.4878)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1221  max mem: 15572
Epoch: [34]  [ 540/2809]  eta: 0:21:43  lr: 0.000003  min_lr: 0.000000  loss: 4.4001 (4.1883)  class_acc: 0.2917 (0.3167)  loss_scale: 32768.0000 (39975.7486)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1642  max mem: 15572
Epoch: [34]  [ 550/2809]  eta: 0:21:38  lr: 0.000003  min_lr: 0.000000  loss: 4.3408 (4.1905)  class_acc: 0.2917 (0.3162)  loss_scale: 32768.0000 (39844.9365)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1835  max mem: 15572
Epoch: [34]  [ 560/2809]  eta: 0:21:31  lr: 0.000003  min_lr: 0.000000  loss: 4.2541 (4.1921)  class_acc: 0.2917 (0.3160)  loss_scale: 32768.0000 (39718.7879)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1406  max mem: 15572
Epoch: [34]  [ 570/2809]  eta: 0:21:26  lr: 0.000003  min_lr: 0.000000  loss: 4.1903 (4.1907)  class_acc: 0.2917 (0.3163)  loss_scale: 32768.0000 (39597.0578)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1405  max mem: 15572
Epoch: [34]  [ 580/2809]  eta: 0:21:18  lr: 0.000003  min_lr: 0.000000  loss: 4.1479 (4.1915)  class_acc: 0.2917 (0.3160)  loss_scale: 32768.0000 (39479.5181)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1123  max mem: 15572
Epoch: [34]  [ 590/2809]  eta: 0:21:14  lr: 0.000003  min_lr: 0.000000  loss: 4.1725 (4.1905)  class_acc: 0.2917 (0.3157)  loss_scale: 32768.0000 (39365.9560)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1205  max mem: 15572
Epoch: [34]  [ 600/2809]  eta: 0:21:06  lr: 0.000003  min_lr: 0.000000  loss: 4.1725 (4.1914)  class_acc: 0.2500 (0.3162)  loss_scale: 32768.0000 (39256.1730)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1256  max mem: 15572
Epoch: [34]  [ 610/2809]  eta: 0:21:00  lr: 0.000003  min_lr: 0.000000  loss: 4.2215 (4.1909)  class_acc: 0.2500 (0.3161)  loss_scale: 32768.0000 (39149.9836)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0899  max mem: 15572
Epoch: [34]  [ 620/2809]  eta: 0:20:55  lr: 0.000003  min_lr: 0.000000  loss: 4.2688 (4.1931)  class_acc: 0.2500 (0.3150)  loss_scale: 32768.0000 (39047.2142)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1121  max mem: 15572
Epoch: [34]  [ 630/2809]  eta: 0:20:48  lr: 0.000003  min_lr: 0.000000  loss: 4.2215 (4.1912)  class_acc: 0.2500 (0.3154)  loss_scale: 32768.0000 (38947.7021)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1224  max mem: 15572
[2025-01-16 06:37:00,423] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:37:00,423] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 640/2809]  eta: 0:20:43  lr: 0.000003  min_lr: 0.000000  loss: 4.2255 (4.1927)  class_acc: 0.2917 (0.3151)  loss_scale: 32768.0000 (39158.0156)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1300  max mem: 15572
Epoch: [34]  [ 650/2809]  eta: 0:20:39  lr: 0.000003  min_lr: 0.000000  loss: 4.3463 (4.1941)  class_acc: 0.2917 (0.3149)  loss_scale: 65536.0000 (39563.2074)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1525  max mem: 15572
Epoch: [34]  [ 660/2809]  eta: 0:20:31  lr: 0.000003  min_lr: 0.000000  loss: 4.3055 (4.1950)  class_acc: 0.2917 (0.3143)  loss_scale: 65536.0000 (39956.1392)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1130  max mem: 15572
[2025-01-16 06:37:18,050] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96172
[2025-01-16 06:37:18,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:37:18,050] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 670/2809]  eta: 0:20:25  lr: 0.000003  min_lr: 0.000000  loss: 4.3055 (4.1964)  class_acc: 0.2917 (0.3140)  loss_scale: 65536.0000 (40093.1863)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0942  max mem: 15572
Epoch: [34]  [ 680/2809]  eta: 0:20:19  lr: 0.000003  min_lr: 0.000000  loss: 4.2410 (4.1969)  class_acc: 0.3333 (0.3141)  loss_scale: 32768.0000 (39985.6211)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1019  max mem: 15572
Epoch: [34]  [ 690/2809]  eta: 0:20:13  lr: 0.000003  min_lr: 0.000000  loss: 4.1784 (4.1963)  class_acc: 0.2917 (0.3133)  loss_scale: 32768.0000 (39881.1693)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1004  max mem: 15572
Epoch: [34]  [ 700/2809]  eta: 0:20:08  lr: 0.000003  min_lr: 0.000000  loss: 4.0676 (4.1946)  class_acc: 0.2917 (0.3144)  loss_scale: 32768.0000 (39779.6976)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1458  max mem: 15572
Epoch: [34]  [ 710/2809]  eta: 0:20:02  lr: 0.000003  min_lr: 0.000000  loss: 4.1943 (4.1953)  class_acc: 0.3750 (0.3156)  loss_scale: 32768.0000 (39681.0802)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1494  max mem: 15572
Epoch: [34]  [ 720/2809]  eta: 0:19:56  lr: 0.000003  min_lr: 0.000000  loss: 4.2179 (4.1952)  class_acc: 0.3750 (0.3159)  loss_scale: 32768.0000 (39585.1983)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1114  max mem: 15572
Epoch: [34]  [ 730/2809]  eta: 0:19:51  lr: 0.000003  min_lr: 0.000000  loss: 4.0609 (4.1956)  class_acc: 0.3333 (0.3160)  loss_scale: 32768.0000 (39491.9398)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1159  max mem: 15572
Epoch: [34]  [ 740/2809]  eta: 0:19:42  lr: 0.000003  min_lr: 0.000000  loss: 4.1707 (4.1954)  class_acc: 0.3750 (0.3167)  loss_scale: 32768.0000 (39401.1984)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.0668  max mem: 15572
Epoch: [34]  [ 750/2809]  eta: 0:19:37  lr: 0.000003  min_lr: 0.000000  loss: 4.2160 (4.1953)  class_acc: 0.3750 (0.3175)  loss_scale: 32768.0000 (39312.8735)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0754  max mem: 15572
Epoch: [34]  [ 760/2809]  eta: 0:19:33  lr: 0.000003  min_lr: 0.000000  loss: 4.2332 (4.1946)  class_acc: 0.3333 (0.3172)  loss_scale: 32768.0000 (39226.8699)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1531  max mem: 15572
Epoch: [34]  [ 770/2809]  eta: 0:19:26  lr: 0.000003  min_lr: 0.000000  loss: 4.1153 (4.1932)  class_acc: 0.2500 (0.3169)  loss_scale: 32768.0000 (39143.0973)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1156  max mem: 15572
Epoch: [34]  [ 780/2809]  eta: 0:19:20  lr: 0.000003  min_lr: 0.000000  loss: 4.1245 (4.1923)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (39061.4699)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0839  max mem: 15572
Epoch: [34]  [ 790/2809]  eta: 0:19:17  lr: 0.000003  min_lr: 0.000000  loss: 4.1454 (4.1911)  class_acc: 0.3333 (0.3176)  loss_scale: 32768.0000 (38981.9064)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1418  max mem: 15572
[2025-01-16 06:38:32,340] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:38:32,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 800/2809]  eta: 0:19:10  lr: 0.000003  min_lr: 0.000000  loss: 4.1113 (4.1911)  class_acc: 0.3333 (0.3171)  loss_scale: 32768.0000 (39149.7828)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1244  max mem: 15572
Epoch: [34]  [ 810/2809]  eta: 0:19:04  lr: 0.000003  min_lr: 0.000000  loss: 4.2161 (4.1909)  class_acc: 0.3333 (0.3175)  loss_scale: 65536.0000 (39475.1369)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.1002  max mem: 15572
Epoch: [34]  [ 820/2809]  eta: 0:18:57  lr: 0.000003  min_lr: 0.000000  loss: 4.0297 (4.1891)  class_acc: 0.3333 (0.3177)  loss_scale: 65536.0000 (39792.5652)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.1082  max mem: 15572
Epoch: [34]  [ 830/2809]  eta: 0:18:52  lr: 0.000003  min_lr: 0.000000  loss: 4.1000 (4.1902)  class_acc: 0.3333 (0.3179)  loss_scale: 65536.0000 (40102.3538)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1130  max mem: 15572
[2025-01-16 06:38:55,042] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96343
[2025-01-16 06:38:55,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:38:55,043] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 840/2809]  eta: 0:18:46  lr: 0.000003  min_lr: 0.000000  loss: 4.1738 (4.1911)  class_acc: 0.3333 (0.3184)  loss_scale: 65536.0000 (40248.9227)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1456  max mem: 15572
Epoch: [34]  [ 850/2809]  eta: 0:18:39  lr: 0.000003  min_lr: 0.000000  loss: 4.1686 (4.1886)  class_acc: 0.3750 (0.3196)  loss_scale: 32768.0000 (40161.0153)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1171  max mem: 15572
Epoch: [34]  [ 860/2809]  eta: 0:18:34  lr: 0.000003  min_lr: 0.000000  loss: 4.1326 (4.1889)  class_acc: 0.3750 (0.3194)  loss_scale: 32768.0000 (40075.1498)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1208  max mem: 15572
Epoch: [34]  [ 870/2809]  eta: 0:18:29  lr: 0.000003  min_lr: 0.000000  loss: 4.1964 (4.1898)  class_acc: 0.2500 (0.3182)  loss_scale: 32768.0000 (39991.2560)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1451  max mem: 15572
Epoch: [34]  [ 880/2809]  eta: 0:18:22  lr: 0.000003  min_lr: 0.000000  loss: 4.1985 (4.1891)  class_acc: 0.2917 (0.3188)  loss_scale: 32768.0000 (39909.2667)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1274  max mem: 15572
Epoch: [34]  [ 890/2809]  eta: 0:18:15  lr: 0.000003  min_lr: 0.000000  loss: 4.1684 (4.1895)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (39829.1178)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0894  max mem: 15572
Epoch: [34]  [ 900/2809]  eta: 0:18:09  lr: 0.000003  min_lr: 0.000000  loss: 4.2214 (4.1911)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (39750.7481)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0731  max mem: 15572
Epoch: [34]  [ 910/2809]  eta: 0:18:04  lr: 0.000003  min_lr: 0.000000  loss: 4.2144 (4.1917)  class_acc: 0.2500 (0.3177)  loss_scale: 32768.0000 (39674.0988)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1327  max mem: 15572
Epoch: [34]  [ 920/2809]  eta: 0:17:56  lr: 0.000003  min_lr: 0.000000  loss: 4.2144 (4.1922)  class_acc: 0.2917 (0.3180)  loss_scale: 32768.0000 (39599.1140)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0940  max mem: 15572
Epoch: [34]  [ 930/2809]  eta: 0:17:49  lr: 0.000003  min_lr: 0.000000  loss: 4.1664 (4.1914)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (39525.7401)  weight_decay: 0.0500 (0.0500)  time: 0.4743  data: 0.0162  max mem: 15572
Epoch: [34]  [ 940/2809]  eta: 0:17:44  lr: 0.000003  min_lr: 0.000000  loss: 4.1664 (4.1921)  class_acc: 0.2917 (0.3177)  loss_scale: 32768.0000 (39453.9256)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0917  max mem: 15572
Epoch: [34]  [ 950/2809]  eta: 0:17:40  lr: 0.000003  min_lr: 0.000000  loss: 4.2890 (4.1949)  class_acc: 0.2917 (0.3180)  loss_scale: 32768.0000 (39383.6215)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1780  max mem: 15572
Epoch: [34]  [ 960/2809]  eta: 0:17:33  lr: 0.000003  min_lr: 0.000000  loss: 4.3707 (4.1961)  class_acc: 0.2500 (0.3178)  loss_scale: 32768.0000 (39314.7804)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1239  max mem: 15572
[2025-01-16 06:40:06,697] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:40:06,698] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 970/2809]  eta: 0:17:25  lr: 0.000003  min_lr: 0.000000  loss: 4.3099 (4.1971)  class_acc: 0.2500 (0.3177)  loss_scale: 32768.0000 (39416.0906)  weight_decay: 0.0500 (0.0500)  time: 0.4877  data: 0.0433  max mem: 15572
Epoch: [34]  [ 980/2809]  eta: 0:17:20  lr: 0.000003  min_lr: 0.000000  loss: 4.2854 (4.1978)  class_acc: 0.3333 (0.3178)  loss_scale: 65536.0000 (39682.3486)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0849  max mem: 15572
[2025-01-16 06:40:17,483] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96489
[2025-01-16 06:40:17,483] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:40:17,483] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 990/2809]  eta: 0:17:15  lr: 0.000003  min_lr: 0.000000  loss: 4.1967 (4.1983)  class_acc: 0.3333 (0.3180)  loss_scale: 65536.0000 (39678.7084)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1506  max mem: 15572
Epoch: [34]  [1000/2809]  eta: 0:17:09  lr: 0.000003  min_lr: 0.000000  loss: 4.2410 (4.1980)  class_acc: 0.2917 (0.3180)  loss_scale: 32768.0000 (39609.6703)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1232  max mem: 15572
Epoch: [34]  [1010/2809]  eta: 0:17:03  lr: 0.000003  min_lr: 0.000000  loss: 4.2632 (4.1972)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (39541.9980)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1036  max mem: 15572
Epoch: [34]  [1020/2809]  eta: 0:16:57  lr: 0.000003  min_lr: 0.000000  loss: 4.2044 (4.1973)  class_acc: 0.2917 (0.3176)  loss_scale: 32768.0000 (39475.6513)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1082  max mem: 15572
Epoch: [34]  [1030/2809]  eta: 0:16:50  lr: 0.000003  min_lr: 0.000000  loss: 4.2297 (4.1984)  class_acc: 0.2500 (0.3173)  loss_scale: 32768.0000 (39410.5917)  weight_decay: 0.0500 (0.0500)  time: 0.5188  data: 0.0678  max mem: 15572
Epoch: [34]  [1040/2809]  eta: 0:16:46  lr: 0.000003  min_lr: 0.000000  loss: 4.2964 (4.1981)  class_acc: 0.2917 (0.3176)  loss_scale: 32768.0000 (39346.7819)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1409  max mem: 15572
Epoch: [34]  [1050/2809]  eta: 0:16:40  lr: 0.000003  min_lr: 0.000000  loss: 4.0403 (4.1987)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (39284.1865)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1648  max mem: 15572
Epoch: [34]  [1060/2809]  eta: 0:16:35  lr: 0.000003  min_lr: 0.000000  loss: 4.4034 (4.2010)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (39222.7710)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1222  max mem: 15572
Epoch: [34]  [1070/2809]  eta: 0:16:28  lr: 0.000003  min_lr: 0.000000  loss: 4.2368 (4.1996)  class_acc: 0.3333 (0.3176)  loss_scale: 32768.0000 (39162.5023)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1145  max mem: 15572
Epoch: [34]  [1080/2809]  eta: 0:16:24  lr: 0.000003  min_lr: 0.000000  loss: 4.1212 (4.2000)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (39103.3488)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1526  max mem: 15572
Epoch: [34]  [1090/2809]  eta: 0:16:18  lr: 0.000003  min_lr: 0.000000  loss: 4.2795 (4.2012)  class_acc: 0.2500 (0.3164)  loss_scale: 32768.0000 (39045.2796)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1598  max mem: 15572
Epoch: [34]  [1100/2809]  eta: 0:16:12  lr: 0.000003  min_lr: 0.000000  loss: 4.2252 (4.2013)  class_acc: 0.2500 (0.3162)  loss_scale: 32768.0000 (38988.2652)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1158  max mem: 15572
Epoch: [34]  [1110/2809]  eta: 0:16:06  lr: 0.000003  min_lr: 0.000000  loss: 4.1293 (4.1994)  class_acc: 0.3333 (0.3170)  loss_scale: 32768.0000 (38932.2772)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1174  max mem: 15572
[2025-01-16 06:41:29,802] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:41:29,803] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 06:41:31,639] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96622
[2025-01-16 06:41:31,640] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:41:31,640] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1120/2809]  eta: 0:16:00  lr: 0.000003  min_lr: 0.000000  loss: 4.1486 (4.2003)  class_acc: 0.3750 (0.3172)  loss_scale: 32768.0000 (38994.2123)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1118  max mem: 15572
Epoch: [34]  [1130/2809]  eta: 0:15:53  lr: 0.000003  min_lr: 0.000000  loss: 4.2754 (4.2003)  class_acc: 0.2917 (0.3169)  loss_scale: 32768.0000 (38939.1618)  weight_decay: 0.0500 (0.0500)  time: 0.4934  data: 0.0597  max mem: 15572
Epoch: [34]  [1140/2809]  eta: 0:15:49  lr: 0.000003  min_lr: 0.000000  loss: 4.2652 (4.2005)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (38885.0762)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1155  max mem: 15572
Epoch: [34]  [1150/2809]  eta: 0:15:43  lr: 0.000003  min_lr: 0.000000  loss: 4.2456 (4.2016)  class_acc: 0.2917 (0.3168)  loss_scale: 32768.0000 (38831.9305)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.1841  max mem: 15572
Epoch: [34]  [1160/2809]  eta: 0:15:38  lr: 0.000003  min_lr: 0.000000  loss: 4.2436 (4.2023)  class_acc: 0.2917 (0.3166)  loss_scale: 32768.0000 (38779.7003)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1324  max mem: 15572
Epoch: [34]  [1170/2809]  eta: 0:15:32  lr: 0.000003  min_lr: 0.000000  loss: 4.1691 (4.2008)  class_acc: 0.3333 (0.3167)  loss_scale: 32768.0000 (38728.3621)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1068  max mem: 15572
Epoch: [34]  [1180/2809]  eta: 0:15:27  lr: 0.000003  min_lr: 0.000000  loss: 4.1377 (4.2010)  class_acc: 0.3333 (0.3172)  loss_scale: 32768.0000 (38677.8933)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1663  max mem: 15572
Epoch: [34]  [1190/2809]  eta: 0:15:22  lr: 0.000003  min_lr: 0.000000  loss: 4.2218 (4.2012)  class_acc: 0.3750 (0.3176)  loss_scale: 32768.0000 (38628.2720)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.1926  max mem: 15572
Epoch: [34]  [1200/2809]  eta: 0:15:16  lr: 0.000003  min_lr: 0.000000  loss: 4.2218 (4.2013)  class_acc: 0.2917 (0.3174)  loss_scale: 32768.0000 (38579.4771)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1263  max mem: 15572
Epoch: [34]  [1210/2809]  eta: 0:15:09  lr: 0.000003  min_lr: 0.000000  loss: 4.1699 (4.2011)  class_acc: 0.3333 (0.3176)  loss_scale: 32768.0000 (38531.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0974  max mem: 15572
Epoch: [34]  [1220/2809]  eta: 0:15:03  lr: 0.000003  min_lr: 0.000000  loss: 4.1699 (4.2011)  class_acc: 0.3333 (0.3173)  loss_scale: 32768.0000 (38484.2850)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0726  max mem: 15572
Epoch: [34]  [1230/2809]  eta: 0:14:58  lr: 0.000003  min_lr: 0.000000  loss: 4.2378 (4.2014)  class_acc: 0.3333 (0.3173)  loss_scale: 32768.0000 (38437.8489)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.1059  max mem: 15572
Epoch: [34]  [1240/2809]  eta: 0:14:52  lr: 0.000003  min_lr: 0.000000  loss: 4.2378 (4.2020)  class_acc: 0.3333 (0.3175)  loss_scale: 32768.0000 (38392.1612)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1298  max mem: 15572
[2025-01-16 06:42:44,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:42:44,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [1250/2809]  eta: 0:14:46  lr: 0.000003  min_lr: 0.000000  loss: 4.2477 (4.2031)  class_acc: 0.3333 (0.3174)  loss_scale: 32768.0000 (38504.3645)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1201  max mem: 15572
[2025-01-16 06:42:49,182] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96758
[2025-01-16 06:42:49,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:42:49,183] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1260/2809]  eta: 0:14:39  lr: 0.000003  min_lr: 0.000000  loss: 4.3447 (4.2041)  class_acc: 0.3333 (0.3175)  loss_scale: 32768.0000 (38484.8596)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0744  max mem: 15572
Epoch: [34]  [1270/2809]  eta: 0:14:34  lr: 0.000003  min_lr: 0.000000  loss: 4.1091 (4.2032)  class_acc: 0.3333 (0.3177)  loss_scale: 32768.0000 (38439.8804)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0791  max mem: 15572
Epoch: [34]  [1280/2809]  eta: 0:14:29  lr: 0.000003  min_lr: 0.000000  loss: 4.1037 (4.2032)  class_acc: 0.3333 (0.3177)  loss_scale: 32768.0000 (38395.6034)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1609  max mem: 15572
Epoch: [34]  [1290/2809]  eta: 0:14:22  lr: 0.000003  min_lr: 0.000000  loss: 4.2899 (4.2042)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (38352.0124)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1276  max mem: 15572
Epoch: [34]  [1300/2809]  eta: 0:14:17  lr: 0.000003  min_lr: 0.000000  loss: 4.2286 (4.2040)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (38309.0915)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1146  max mem: 15572
Epoch: [34]  [1310/2809]  eta: 0:14:11  lr: 0.000003  min_lr: 0.000000  loss: 4.2229 (4.2036)  class_acc: 0.2500 (0.3173)  loss_scale: 32768.0000 (38266.8253)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1144  max mem: 15572
Epoch: [34]  [1320/2809]  eta: 0:14:06  lr: 0.000003  min_lr: 0.000000  loss: 4.3102 (4.2039)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (38225.1991)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1161  max mem: 15572
Epoch: [34]  [1330/2809]  eta: 0:14:00  lr: 0.000003  min_lr: 0.000000  loss: 4.2777 (4.2036)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (38184.1983)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1586  max mem: 15572
Epoch: [34]  [1340/2809]  eta: 0:13:54  lr: 0.000003  min_lr: 0.000000  loss: 4.0982 (4.2026)  class_acc: 0.2917 (0.3170)  loss_scale: 32768.0000 (38143.8091)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1083  max mem: 15572
Epoch: [34]  [1350/2809]  eta: 0:13:48  lr: 0.000003  min_lr: 0.000000  loss: 4.0324 (4.2017)  class_acc: 0.2917 (0.3169)  loss_scale: 32768.0000 (38104.0178)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0879  max mem: 15572
Epoch: [34]  [1360/2809]  eta: 0:13:43  lr: 0.000003  min_lr: 0.000000  loss: 4.0057 (4.2008)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (38064.8112)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1586  max mem: 15572
Epoch: [34]  [1370/2809]  eta: 0:13:37  lr: 0.000003  min_lr: 0.000000  loss: 4.0877 (4.2012)  class_acc: 0.2917 (0.3170)  loss_scale: 32768.0000 (38026.1765)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1700  max mem: 15572
Epoch: [34]  [1380/2809]  eta: 0:13:32  lr: 0.000003  min_lr: 0.000000  loss: 4.2380 (4.2016)  class_acc: 0.2917 (0.3167)  loss_scale: 32768.0000 (37988.1014)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1333  max mem: 15572
[2025-01-16 06:44:02,081] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:44:02,082] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [1390/2809]  eta: 0:13:26  lr: 0.000003  min_lr: 0.000000  loss: 4.2032 (4.2006)  class_acc: 0.2917 (0.3167)  loss_scale: 32768.0000 (38186.1452)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1182  max mem: 15572
Epoch: [34]  [1400/2809]  eta: 0:13:20  lr: 0.000003  min_lr: 0.000000  loss: 4.0182 (4.1998)  class_acc: 0.2917 (0.3171)  loss_scale: 65536.0000 (38381.3619)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0802  max mem: 15572
[2025-01-16 06:44:16,866] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96912
[2025-01-16 06:44:16,866] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:44:16,866] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1410/2809]  eta: 0:13:15  lr: 0.000003  min_lr: 0.000000  loss: 4.0590 (4.1993)  class_acc: 0.3333 (0.3173)  loss_scale: 65536.0000 (38457.6953)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1047  max mem: 15572
Epoch: [34]  [1420/2809]  eta: 0:13:09  lr: 0.000003  min_lr: 0.000000  loss: 4.1328 (4.2003)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (38417.6552)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1524  max mem: 15572
Epoch: [34]  [1430/2809]  eta: 0:13:04  lr: 0.000003  min_lr: 0.000000  loss: 4.2406 (4.1997)  class_acc: 0.2917 (0.3174)  loss_scale: 32768.0000 (38378.1747)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1395  max mem: 15572
Epoch: [34]  [1440/2809]  eta: 0:12:57  lr: 0.000003  min_lr: 0.000000  loss: 4.1506 (4.1998)  class_acc: 0.3750 (0.3174)  loss_scale: 32768.0000 (38339.2422)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0832  max mem: 15572
Epoch: [34]  [1450/2809]  eta: 0:12:52  lr: 0.000003  min_lr: 0.000000  loss: 4.2571 (4.1999)  class_acc: 0.3750 (0.3175)  loss_scale: 32768.0000 (38300.8463)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.1010  max mem: 15572
Epoch: [34]  [1460/2809]  eta: 0:12:45  lr: 0.000003  min_lr: 0.000000  loss: 4.1598 (4.1982)  class_acc: 0.3750 (0.3177)  loss_scale: 32768.0000 (38262.9760)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0834  max mem: 15572
Epoch: [34]  [1470/2809]  eta: 0:12:39  lr: 0.000003  min_lr: 0.000000  loss: 4.0554 (4.1979)  class_acc: 0.3750 (0.3179)  loss_scale: 32768.0000 (38225.6207)  weight_decay: 0.0500 (0.0500)  time: 0.4614  data: 0.0082  max mem: 15572
Epoch: [34]  [1480/2809]  eta: 0:12:34  lr: 0.000003  min_lr: 0.000000  loss: 4.1003 (4.1972)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (38188.7698)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0975  max mem: 15572
Epoch: [34]  [1490/2809]  eta: 0:12:28  lr: 0.000003  min_lr: 0.000000  loss: 4.1333 (4.1972)  class_acc: 0.2500 (0.3174)  loss_scale: 32768.0000 (38152.4131)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1385  max mem: 15572
[2025-01-16 06:45:04,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=97000, skipped=603, lr=[2.723864396118874e-08, 2.723864396118874e-08, 3.891234851598392e-08, 3.891234851598392e-08, 5.558906930854846e-08, 5.558906930854846e-08, 7.941295615506924e-08, 7.941295615506924e-08, 1.1344708022152749e-07, 1.1344708022152749e-07, 1.62067257459325e-07, 1.62067257459325e-07, 2.3152465351332144e-07, 2.3152465351332144e-07, 3.3074950501903063e-07, 3.3074950501903063e-07, 4.724992928843295e-07, 4.724992928843295e-07, 6.749989898347565e-07, 6.749989898347565e-07, 9.642842711925092e-07, 9.642842711925092e-07, 1.377548958846442e-06, 1.377548958846442e-06, 1.967927084066346e-06, 1.967927084066346e-06, 2.8113244058090655e-06, 2.8113244058090655e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:45:04,500] [INFO] [timer.py:260:stop] epoch=0/micro_step=97000/global_step=97000, RunningAvgSamplesPerSec=27.964621801201027, CurrSamplesPerSec=28.23620017032048, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [1500/2809]  eta: 0:12:22  lr: 0.000003  min_lr: 0.000000  loss: 4.1521 (4.1975)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (38116.5410)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1243  max mem: 15572
Epoch: [34]  [1510/2809]  eta: 0:12:17  lr: 0.000003  min_lr: 0.000000  loss: 4.3053 (4.1990)  class_acc: 0.2917 (0.3170)  loss_scale: 32768.0000 (38081.1436)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.2024  max mem: 15572
Epoch: [34]  [1520/2809]  eta: 0:12:11  lr: 0.000003  min_lr: 0.000000  loss: 4.3589 (4.1993)  class_acc: 0.2500 (0.3167)  loss_scale: 32768.0000 (38046.2117)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1588  max mem: 15572
Epoch: [34]  [1530/2809]  eta: 0:12:05  lr: 0.000003  min_lr: 0.000000  loss: 4.2708 (4.1998)  class_acc: 0.2917 (0.3169)  loss_scale: 32768.0000 (38011.7361)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0771  max mem: 15572
[2025-01-16 06:45:28,969] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:45:28,969] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 06:45:30,895] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97045
[2025-01-16 06:45:30,895] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:45:30,895] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1540/2809]  eta: 0:12:00  lr: 0.000003  min_lr: 0.000000  loss: 4.1836 (4.1999)  class_acc: 0.3333 (0.3174)  loss_scale: 32768.0000 (38062.7644)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1112  max mem: 15572
Epoch: [34]  [1550/2809]  eta: 0:11:54  lr: 0.000003  min_lr: 0.000000  loss: 4.1779 (4.2000)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (38028.6267)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1092  max mem: 15572
Epoch: [34]  [1560/2809]  eta: 0:11:48  lr: 0.000003  min_lr: 0.000000  loss: 4.2248 (4.2001)  class_acc: 0.2083 (0.3168)  loss_scale: 32768.0000 (37994.9263)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1233  max mem: 15572
Epoch: [34]  [1570/2809]  eta: 0:11:43  lr: 0.000003  min_lr: 0.000000  loss: 4.2248 (4.2004)  class_acc: 0.2500 (0.3168)  loss_scale: 32768.0000 (37961.6550)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1608  max mem: 15572
Epoch: [34]  [1580/2809]  eta: 0:11:37  lr: 0.000003  min_lr: 0.000000  loss: 4.3004 (4.2009)  class_acc: 0.3333 (0.3169)  loss_scale: 32768.0000 (37928.8046)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1271  max mem: 15572
Epoch: [34]  [1590/2809]  eta: 0:11:32  lr: 0.000003  min_lr: 0.000000  loss: 4.2720 (4.2007)  class_acc: 0.2500 (0.3167)  loss_scale: 32768.0000 (37896.3671)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1584  max mem: 15572
Epoch: [34]  [1600/2809]  eta: 0:11:27  lr: 0.000003  min_lr: 0.000000  loss: 4.2566 (4.2014)  class_acc: 0.2917 (0.3164)  loss_scale: 32768.0000 (37864.3348)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.2301  max mem: 15572
Epoch: [34]  [1610/2809]  eta: 0:11:21  lr: 0.000003  min_lr: 0.000000  loss: 4.2117 (4.2001)  class_acc: 0.2917 (0.3163)  loss_scale: 32768.0000 (37832.7002)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1908  max mem: 15572
Epoch: [34]  [1620/2809]  eta: 0:11:16  lr: 0.000003  min_lr: 0.000000  loss: 4.0873 (4.2000)  class_acc: 0.2917 (0.3167)  loss_scale: 32768.0000 (37801.4559)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1805  max mem: 15572
Epoch: [34]  [1630/2809]  eta: 0:11:10  lr: 0.000003  min_lr: 0.000000  loss: 4.2220 (4.1999)  class_acc: 0.4167 (0.3170)  loss_scale: 32768.0000 (37770.5947)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1435  max mem: 15572
Epoch: [34]  [1640/2809]  eta: 0:11:04  lr: 0.000003  min_lr: 0.000000  loss: 4.2370 (4.2005)  class_acc: 0.2500 (0.3168)  loss_scale: 32768.0000 (37740.1097)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0798  max mem: 15572
Epoch: [34]  [1650/2809]  eta: 0:10:59  lr: 0.000003  min_lr: 0.000000  loss: 4.2041 (4.2004)  class_acc: 0.3750 (0.3175)  loss_scale: 32768.0000 (37709.9939)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0812  max mem: 15572
Epoch: [34]  [1660/2809]  eta: 0:10:52  lr: 0.000003  min_lr: 0.000000  loss: 4.1158 (4.2001)  class_acc: 0.3750 (0.3175)  loss_scale: 32768.0000 (37680.2408)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0599  max mem: 15572
[2025-01-16 06:46:45,697] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:46:45,697] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [1670/2809]  eta: 0:10:47  lr: 0.000003  min_lr: 0.000000  loss: 4.1158 (4.1995)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (37709.6732)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0877  max mem: 15572
Epoch: [34]  [1680/2809]  eta: 0:10:41  lr: 0.000003  min_lr: 0.000000  loss: 4.1972 (4.1998)  class_acc: 0.2500 (0.3168)  loss_scale: 65536.0000 (37875.2076)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0800  max mem: 15572
Epoch: [34]  [1690/2809]  eta: 0:10:35  lr: 0.000003  min_lr: 0.000000  loss: 4.2724 (4.1996)  class_acc: 0.3333 (0.3169)  loss_scale: 65536.0000 (38038.7842)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1175  max mem: 15572
Epoch: [34]  [1700/2809]  eta: 0:10:29  lr: 0.000003  min_lr: 0.000000  loss: 4.0531 (4.1989)  class_acc: 0.3333 (0.3169)  loss_scale: 65536.0000 (38200.4374)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1225  max mem: 15572
Epoch: [34]  [1710/2809]  eta: 0:10:24  lr: 0.000003  min_lr: 0.000000  loss: 4.2714 (4.1997)  class_acc: 0.3333 (0.3169)  loss_scale: 65536.0000 (38360.2011)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0998  max mem: 15572
Epoch: [34]  [1720/2809]  eta: 0:10:18  lr: 0.000003  min_lr: 0.000000  loss: 4.3260 (4.2004)  class_acc: 0.2917 (0.3170)  loss_scale: 65536.0000 (38518.1081)  weight_decay: 0.0500 (0.0500)  time: 0.6240  data: 0.1939  max mem: 15572
[2025-01-16 06:47:15,542] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97227
[2025-01-16 06:47:15,543] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:47:15,545] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1730/2809]  eta: 0:10:13  lr: 0.000003  min_lr: 0.000000  loss: 4.2344 (4.2002)  class_acc: 0.3333 (0.3171)  loss_scale: 32768.0000 (38484.8897)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1965  max mem: 15572
Epoch: [34]  [1740/2809]  eta: 0:10:07  lr: 0.000003  min_lr: 0.000000  loss: 4.1992 (4.2009)  class_acc: 0.3333 (0.3172)  loss_scale: 32768.0000 (38452.0528)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1442  max mem: 15572
Epoch: [34]  [1750/2809]  eta: 0:10:02  lr: 0.000003  min_lr: 0.000000  loss: 4.2271 (4.2006)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (38419.5911)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1303  max mem: 15572
Epoch: [34]  [1760/2809]  eta: 0:09:57  lr: 0.000003  min_lr: 0.000000  loss: 4.0453 (4.2000)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (38387.4980)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1289  max mem: 15572
Epoch: [34]  [1770/2809]  eta: 0:09:50  lr: 0.000003  min_lr: 0.000000  loss: 4.0898 (4.2001)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (38355.7674)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0828  max mem: 15572
Epoch: [34]  [1780/2809]  eta: 0:09:45  lr: 0.000003  min_lr: 0.000000  loss: 4.2420 (4.2001)  class_acc: 0.3333 (0.3173)  loss_scale: 32768.0000 (38324.3930)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0746  max mem: 15572
Epoch: [34]  [1790/2809]  eta: 0:09:39  lr: 0.000003  min_lr: 0.000000  loss: 4.2941 (4.2002)  class_acc: 0.3333 (0.3174)  loss_scale: 32768.0000 (38293.3691)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0864  max mem: 15572
Epoch: [34]  [1800/2809]  eta: 0:09:33  lr: 0.000003  min_lr: 0.000000  loss: 4.2970 (4.2006)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (38262.6896)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1150  max mem: 15572
Epoch: [34]  [1810/2809]  eta: 0:09:28  lr: 0.000003  min_lr: 0.000000  loss: 4.3147 (4.2012)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (38232.3490)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1150  max mem: 15572
Epoch: [34]  [1820/2809]  eta: 0:09:22  lr: 0.000003  min_lr: 0.000000  loss: 4.2231 (4.2007)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (38202.3416)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1142  max mem: 15572
Epoch: [34]  [1830/2809]  eta: 0:09:16  lr: 0.000003  min_lr: 0.000000  loss: 4.2021 (4.2007)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (38172.6619)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1589  max mem: 15572
Epoch: [34]  [1840/2809]  eta: 0:09:11  lr: 0.000003  min_lr: 0.000000  loss: 4.2478 (4.2013)  class_acc: 0.2500 (0.3173)  loss_scale: 32768.0000 (38143.3047)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1434  max mem: 15572
[2025-01-16 06:48:29,615] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:48:29,616] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [1850/2809]  eta: 0:09:05  lr: 0.000003  min_lr: 0.000000  loss: 4.2739 (4.2021)  class_acc: 0.2500 (0.3171)  loss_scale: 32768.0000 (38131.9676)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1143  max mem: 15572
[2025-01-16 06:48:31,058] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97358
[2025-01-16 06:48:31,059] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:48:31,060] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1860/2809]  eta: 0:08:59  lr: 0.000003  min_lr: 0.000000  loss: 4.2627 (4.2019)  class_acc: 0.2083 (0.3168)  loss_scale: 32768.0000 (38120.7523)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0954  max mem: 15572
Epoch: [34]  [1870/2809]  eta: 0:08:54  lr: 0.000003  min_lr: 0.000000  loss: 4.2108 (4.2021)  class_acc: 0.2917 (0.3168)  loss_scale: 32768.0000 (38092.1432)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1421  max mem: 15572
Epoch: [34]  [1880/2809]  eta: 0:08:48  lr: 0.000003  min_lr: 0.000000  loss: 4.2108 (4.2018)  class_acc: 0.2917 (0.3165)  loss_scale: 32768.0000 (38063.8384)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1718  max mem: 15572
Epoch: [34]  [1890/2809]  eta: 0:08:43  lr: 0.000003  min_lr: 0.000000  loss: 4.2486 (4.2031)  class_acc: 0.2917 (0.3165)  loss_scale: 32768.0000 (38035.8329)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1394  max mem: 15572
Epoch: [34]  [1900/2809]  eta: 0:08:37  lr: 0.000003  min_lr: 0.000000  loss: 4.2523 (4.2024)  class_acc: 0.3333 (0.3168)  loss_scale: 32768.0000 (38008.1220)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0940  max mem: 15572
Epoch: [34]  [1910/2809]  eta: 0:08:31  lr: 0.000003  min_lr: 0.000000  loss: 4.1890 (4.2026)  class_acc: 0.3333 (0.3167)  loss_scale: 32768.0000 (37980.7012)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0742  max mem: 15572
Epoch: [34]  [1920/2809]  eta: 0:08:25  lr: 0.000003  min_lr: 0.000000  loss: 4.1145 (4.2019)  class_acc: 0.3750 (0.3173)  loss_scale: 32768.0000 (37953.5659)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0745  max mem: 15572
Epoch: [34]  [1930/2809]  eta: 0:08:19  lr: 0.000003  min_lr: 0.000000  loss: 4.1048 (4.2018)  class_acc: 0.3750 (0.3174)  loss_scale: 32768.0000 (37926.7115)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0867  max mem: 15572
Epoch: [34]  [1940/2809]  eta: 0:08:14  lr: 0.000003  min_lr: 0.000000  loss: 4.1206 (4.2016)  class_acc: 0.3333 (0.3174)  loss_scale: 32768.0000 (37900.1340)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1240  max mem: 15572
Epoch: [34]  [1950/2809]  eta: 0:08:08  lr: 0.000003  min_lr: 0.000000  loss: 4.1206 (4.2010)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (37873.8288)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1021  max mem: 15572
Epoch: [34]  [1960/2809]  eta: 0:08:02  lr: 0.000003  min_lr: 0.000000  loss: 4.0631 (4.2004)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (37847.7919)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0960  max mem: 15572
Epoch: [34]  [1970/2809]  eta: 0:07:56  lr: 0.000003  min_lr: 0.000000  loss: 4.0345 (4.1998)  class_acc: 0.3333 (0.3179)  loss_scale: 32768.0000 (37822.0193)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1233  max mem: 15572
Epoch: [34]  [1980/2809]  eta: 0:07:50  lr: 0.000003  min_lr: 0.000000  loss: 3.9890 (4.1994)  class_acc: 0.4167 (0.3183)  loss_scale: 32768.0000 (37796.5068)  weight_decay: 0.0500 (0.0500)  time: 0.5186  data: 0.0786  max mem: 15572
[2025-01-16 06:49:42,411] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:49:42,411] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 06:49:44,746] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97492
[2025-01-16 06:49:44,746] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:49:44,746] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1990/2809]  eta: 0:07:44  lr: 0.000003  min_lr: 0.000000  loss: 4.1991 (4.1985)  class_acc: 0.4167 (0.3188)  loss_scale: 32768.0000 (37853.5409)  weight_decay: 0.0500 (0.0500)  time: 0.4667  data: 0.0286  max mem: 15572
Epoch: [34]  [2000/2809]  eta: 0:07:38  lr: 0.000003  min_lr: 0.000000  loss: 4.1991 (4.1987)  class_acc: 0.3750 (0.3188)  loss_scale: 32768.0000 (37828.1259)  weight_decay: 0.0500 (0.0500)  time: 0.5185  data: 0.0792  max mem: 15572
Epoch: [34]  [2010/2809]  eta: 0:07:32  lr: 0.000003  min_lr: 0.000000  loss: 4.1980 (4.1983)  class_acc: 0.2917 (0.3189)  loss_scale: 32768.0000 (37802.9637)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0775  max mem: 15572
[2025-01-16 06:49:59,085] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97519
[2025-01-16 06:49:59,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 06:49:59,086] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [2020/2809]  eta: 0:07:27  lr: 0.000003  min_lr: 0.000000  loss: 4.1512 (4.1983)  class_acc: 0.3333 (0.3194)  loss_scale: 32768.0000 (37713.1954)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1279  max mem: 15572
Epoch: [34]  [2030/2809]  eta: 0:07:21  lr: 0.000003  min_lr: 0.000000  loss: 4.2047 (4.1981)  class_acc: 0.3333 (0.3191)  loss_scale: 16384.0000 (37608.1773)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1486  max mem: 15572
Epoch: [34]  [2040/2809]  eta: 0:07:16  lr: 0.000003  min_lr: 0.000000  loss: 4.2807 (4.1978)  class_acc: 0.3333 (0.3193)  loss_scale: 16384.0000 (37504.1881)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1474  max mem: 15572
Epoch: [34]  [2050/2809]  eta: 0:07:10  lr: 0.000003  min_lr: 0.000000  loss: 4.2969 (4.1983)  class_acc: 0.3333 (0.3192)  loss_scale: 16384.0000 (37401.2131)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1317  max mem: 15572
Epoch: [34]  [2060/2809]  eta: 0:07:04  lr: 0.000003  min_lr: 0.000000  loss: 4.2660 (4.1977)  class_acc: 0.3333 (0.3195)  loss_scale: 16384.0000 (37299.2373)  weight_decay: 0.0500 (0.0500)  time: 0.5124  data: 0.0662  max mem: 15572
Epoch: [34]  [2070/2809]  eta: 0:06:58  lr: 0.000003  min_lr: 0.000000  loss: 4.1048 (4.1980)  class_acc: 0.3333 (0.3194)  loss_scale: 16384.0000 (37198.2463)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0564  max mem: 15572
Epoch: [34]  [2080/2809]  eta: 0:06:53  lr: 0.000003  min_lr: 0.000000  loss: 4.1048 (4.1977)  class_acc: 0.2917 (0.3192)  loss_scale: 16384.0000 (37098.2259)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0578  max mem: 15572
Epoch: [34]  [2090/2809]  eta: 0:06:47  lr: 0.000003  min_lr: 0.000000  loss: 4.2584 (4.1982)  class_acc: 0.2083 (0.3189)  loss_scale: 16384.0000 (36999.1621)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1051  max mem: 15572
Epoch: [34]  [2100/2809]  eta: 0:06:41  lr: 0.000003  min_lr: 0.000000  loss: 4.3284 (4.1987)  class_acc: 0.2917 (0.3190)  loss_scale: 16384.0000 (36901.0414)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1146  max mem: 15572
Epoch: [34]  [2110/2809]  eta: 0:06:36  lr: 0.000003  min_lr: 0.000000  loss: 4.2328 (4.1986)  class_acc: 0.3750 (0.3192)  loss_scale: 16384.0000 (36803.8503)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1599  max mem: 15572
Epoch: [34]  [2120/2809]  eta: 0:06:30  lr: 0.000003  min_lr: 0.000000  loss: 4.1603 (4.1985)  class_acc: 0.3333 (0.3192)  loss_scale: 16384.0000 (36707.5757)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1566  max mem: 15572
Epoch: [34]  [2130/2809]  eta: 0:06:24  lr: 0.000003  min_lr: 0.000000  loss: 4.0845 (4.1973)  class_acc: 0.3333 (0.3193)  loss_scale: 16384.0000 (36612.2046)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1025  max mem: 15572
Epoch: [34]  [2140/2809]  eta: 0:06:19  lr: 0.000003  min_lr: 0.000000  loss: 3.9240 (4.1958)  class_acc: 0.3333 (0.3193)  loss_scale: 16384.0000 (36517.7244)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0997  max mem: 15572
[2025-01-16 06:51:11,198] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:51:11,198] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [2150/2809]  eta: 0:06:13  lr: 0.000003  min_lr: 0.000000  loss: 4.0984 (4.1960)  class_acc: 0.3333 (0.3193)  loss_scale: 16384.0000 (36492.6750)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1592  max mem: 15572
Epoch: [34]  [2160/2809]  eta: 0:06:07  lr: 0.000003  min_lr: 0.000000  loss: 4.1640 (4.1961)  class_acc: 0.2917 (0.3193)  loss_scale: 32768.0000 (36475.4391)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1434  max mem: 15572
Epoch: [34]  [2170/2809]  eta: 0:06:02  lr: 0.000003  min_lr: 0.000000  loss: 4.2136 (4.1965)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (36458.3620)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0960  max mem: 15572
Epoch: [34]  [2180/2809]  eta: 0:05:56  lr: 0.000003  min_lr: 0.000000  loss: 4.2792 (4.1966)  class_acc: 0.2500 (0.3190)  loss_scale: 32768.0000 (36441.4415)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1392  max mem: 15572
Epoch: [34]  [2190/2809]  eta: 0:05:50  lr: 0.000003  min_lr: 0.000000  loss: 4.1516 (4.1962)  class_acc: 0.3333 (0.3192)  loss_scale: 32768.0000 (36424.6755)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1462  max mem: 15572
Epoch: [34]  [2200/2809]  eta: 0:05:45  lr: 0.000003  min_lr: 0.000000  loss: 4.1516 (4.1966)  class_acc: 0.3333 (0.3193)  loss_scale: 32768.0000 (36408.0618)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1309  max mem: 15572
Epoch: [34]  [2210/2809]  eta: 0:05:39  lr: 0.000003  min_lr: 0.000000  loss: 4.2911 (4.1965)  class_acc: 0.3333 (0.3194)  loss_scale: 32768.0000 (36391.5984)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1522  max mem: 15572
Epoch: [34]  [2220/2809]  eta: 0:05:34  lr: 0.000003  min_lr: 0.000000  loss: 4.2113 (4.1965)  class_acc: 0.3333 (0.3195)  loss_scale: 32768.0000 (36375.2832)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1767  max mem: 15572
Epoch: [34]  [2230/2809]  eta: 0:05:28  lr: 0.000003  min_lr: 0.000000  loss: 4.2541 (4.1968)  class_acc: 0.3333 (0.3194)  loss_scale: 32768.0000 (36359.1143)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1517  max mem: 15572
Epoch: [34]  [2240/2809]  eta: 0:05:22  lr: 0.000003  min_lr: 0.000000  loss: 4.2238 (4.1967)  class_acc: 0.3333 (0.3195)  loss_scale: 32768.0000 (36343.0897)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0980  max mem: 15572
Epoch: [34]  [2250/2809]  eta: 0:05:17  lr: 0.000003  min_lr: 0.000000  loss: 4.1480 (4.1968)  class_acc: 0.3333 (0.3197)  loss_scale: 32768.0000 (36327.2075)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1165  max mem: 15572
Epoch: [34]  [2260/2809]  eta: 0:05:11  lr: 0.000003  min_lr: 0.000000  loss: 4.1050 (4.1960)  class_acc: 0.3750 (0.3200)  loss_scale: 32768.0000 (36311.4657)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0868  max mem: 15572
[2025-01-16 06:52:24,865] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:52:24,865] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [2270/2809]  eta: 0:05:05  lr: 0.000003  min_lr: 0.000000  loss: 4.1989 (4.1967)  class_acc: 0.3333 (0.3197)  loss_scale: 32768.0000 (36310.2915)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0598  max mem: 15572
Epoch: [34]  [2280/2809]  eta: 0:04:59  lr: 0.000003  min_lr: 0.000000  loss: 4.2755 (4.1964)  class_acc: 0.2500 (0.3196)  loss_scale: 65536.0000 (36438.4182)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1248  max mem: 15572
Epoch: [34]  [2290/2809]  eta: 0:04:54  lr: 0.000003  min_lr: 0.000000  loss: 4.2237 (4.1964)  class_acc: 0.2917 (0.3197)  loss_scale: 65536.0000 (36565.4265)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1576  max mem: 15572
Epoch: [34]  [2300/2809]  eta: 0:04:48  lr: 0.000003  min_lr: 0.000000  loss: 4.2517 (4.1965)  class_acc: 0.3333 (0.3197)  loss_scale: 65536.0000 (36691.3307)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.2044  max mem: 15572
Epoch: [34]  [2310/2809]  eta: 0:04:43  lr: 0.000003  min_lr: 0.000000  loss: 4.2008 (4.1962)  class_acc: 0.3333 (0.3199)  loss_scale: 65536.0000 (36816.1454)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.1875  max mem: 15572
Epoch: [34]  [2320/2809]  eta: 0:04:37  lr: 0.000003  min_lr: 0.000000  loss: 4.0841 (4.1962)  class_acc: 0.3750 (0.3201)  loss_scale: 65536.0000 (36939.8845)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0922  max mem: 15572
[2025-01-16 06:52:54,753] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97827
[2025-01-16 06:52:54,753] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:52:54,754] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [2330/2809]  eta: 0:04:31  lr: 0.000003  min_lr: 0.000000  loss: 4.0612 (4.1955)  class_acc: 0.3750 (0.3205)  loss_scale: 32768.0000 (36921.9871)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0935  max mem: 15572
Epoch: [34]  [2340/2809]  eta: 0:04:26  lr: 0.000003  min_lr: 0.000000  loss: 4.1144 (4.1955)  class_acc: 0.3333 (0.3204)  loss_scale: 32768.0000 (36904.2426)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1407  max mem: 15572
Epoch: [34]  [2350/2809]  eta: 0:04:20  lr: 0.000003  min_lr: 0.000000  loss: 4.2295 (4.1954)  class_acc: 0.2500 (0.3203)  loss_scale: 32768.0000 (36886.6491)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1406  max mem: 15572
Epoch: [34]  [2360/2809]  eta: 0:04:14  lr: 0.000003  min_lr: 0.000000  loss: 4.2007 (4.1954)  class_acc: 0.2917 (0.3203)  loss_scale: 32768.0000 (36869.2046)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1403  max mem: 15572
Epoch: [34]  [2370/2809]  eta: 0:04:09  lr: 0.000003  min_lr: 0.000000  loss: 4.1891 (4.1958)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (36851.9072)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1116  max mem: 15572
Epoch: [34]  [2380/2809]  eta: 0:04:03  lr: 0.000003  min_lr: 0.000000  loss: 4.1985 (4.1958)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (36834.7551)  weight_decay: 0.0500 (0.0500)  time: 0.4905  data: 0.0636  max mem: 15572
Epoch: [34]  [2390/2809]  eta: 0:03:57  lr: 0.000003  min_lr: 0.000000  loss: 4.2871 (4.1961)  class_acc: 0.2917 (0.3200)  loss_scale: 32768.0000 (36817.7465)  weight_decay: 0.0500 (0.0500)  time: 0.5208  data: 0.0676  max mem: 15572
Epoch: [34]  [2400/2809]  eta: 0:03:52  lr: 0.000003  min_lr: 0.000000  loss: 4.2498 (4.1955)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (36800.8796)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0925  max mem: 15572
Epoch: [34]  [2410/2809]  eta: 0:03:46  lr: 0.000002  min_lr: 0.000000  loss: 4.1447 (4.1952)  class_acc: 0.3750 (0.3204)  loss_scale: 32768.0000 (36784.1526)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0584  max mem: 15572
Epoch: [34]  [2420/2809]  eta: 0:03:40  lr: 0.000002  min_lr: 0.000000  loss: 4.1969 (4.1958)  class_acc: 0.3333 (0.3205)  loss_scale: 32768.0000 (36767.5638)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0531  max mem: 15572
Epoch: [34]  [2430/2809]  eta: 0:03:34  lr: 0.000002  min_lr: 0.000000  loss: 4.1424 (4.1956)  class_acc: 0.3750 (0.3208)  loss_scale: 32768.0000 (36751.1115)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.0530  max mem: 15572
Epoch: [34]  [2440/2809]  eta: 0:03:29  lr: 0.000002  min_lr: 0.000000  loss: 4.1360 (4.1956)  class_acc: 0.3750 (0.3206)  loss_scale: 32768.0000 (36734.7939)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1203  max mem: 15572
[2025-01-16 06:54:07,995] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:54:07,995] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [2450/2809]  eta: 0:03:23  lr: 0.000002  min_lr: 0.000000  loss: 4.1303 (4.1956)  class_acc: 0.3750 (0.3208)  loss_scale: 32768.0000 (36731.9788)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1430  max mem: 15572
Epoch: [34]  [2460/2809]  eta: 0:03:18  lr: 0.000002  min_lr: 0.000000  loss: 4.1303 (4.1957)  class_acc: 0.3750 (0.3209)  loss_scale: 65536.0000 (36849.0207)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1256  max mem: 15572
Epoch: [34]  [2470/2809]  eta: 0:03:12  lr: 0.000002  min_lr: 0.000000  loss: 4.1863 (4.1951)  class_acc: 0.3333 (0.3211)  loss_scale: 65536.0000 (36965.1153)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1507  max mem: 15572
[2025-01-16 06:54:24,217] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97983
[2025-01-16 06:54:24,218] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:54:24,218] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [2480/2809]  eta: 0:03:06  lr: 0.000002  min_lr: 0.000000  loss: 4.1628 (4.1954)  class_acc: 0.3333 (0.3212)  loss_scale: 65536.0000 (37027.4438)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0686  max mem: 15572
Epoch: [34]  [2490/2809]  eta: 0:03:00  lr: 0.000002  min_lr: 0.000000  loss: 4.2839 (4.1960)  class_acc: 0.2917 (0.3210)  loss_scale: 32768.0000 (37010.3444)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0123  max mem: 15572
[2025-01-16 06:54:32,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=98000, skipped=610, lr=[2.3924327692572e-08, 2.3924327692572e-08, 3.417761098938857e-08, 3.417761098938857e-08, 4.88251585562694e-08, 4.88251585562694e-08, 6.975022650895629e-08, 6.975022650895629e-08, 9.964318072708041e-08, 9.964318072708041e-08, 1.4234740103868632e-07, 1.4234740103868632e-07, 2.0335343005526615e-07, 2.0335343005526615e-07, 2.905049000789517e-07, 2.905049000789517e-07, 4.1500700011278813e-07, 4.1500700011278813e-07, 5.928671430182688e-07, 5.928671430182688e-07, 8.469530614546697e-07, 8.469530614546697e-07, 1.2099329449352427e-06, 1.2099329449352427e-06, 1.7284756356217752e-06, 1.7284756356217752e-06, 2.4692509080311077e-06, 2.4692509080311077e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:54:32,141] [INFO] [timer.py:260:stop] epoch=0/micro_step=98000/global_step=98000, RunningAvgSamplesPerSec=27.966478079410802, CurrSamplesPerSec=29.76469217645337, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [2500/2809]  eta: 0:02:55  lr: 0.000002  min_lr: 0.000000  loss: 4.2626 (4.1958)  class_acc: 0.2917 (0.3210)  loss_scale: 32768.0000 (36993.3818)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0600  max mem: 15572
Epoch: [34]  [2510/2809]  eta: 0:02:49  lr: 0.000002  min_lr: 0.000000  loss: 4.1319 (4.1960)  class_acc: 0.2917 (0.3209)  loss_scale: 32768.0000 (36976.5544)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0669  max mem: 15572
Epoch: [34]  [2520/2809]  eta: 0:02:43  lr: 0.000002  min_lr: 0.000000  loss: 4.1969 (4.1960)  class_acc: 0.2917 (0.3208)  loss_scale: 32768.0000 (36959.8604)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0485  max mem: 15572
Epoch: [34]  [2530/2809]  eta: 0:02:38  lr: 0.000002  min_lr: 0.000000  loss: 4.1041 (4.1956)  class_acc: 0.2917 (0.3208)  loss_scale: 32768.0000 (36943.2983)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0816  max mem: 15572
Epoch: [34]  [2540/2809]  eta: 0:02:32  lr: 0.000002  min_lr: 0.000000  loss: 3.9516 (4.1951)  class_acc: 0.3333 (0.3211)  loss_scale: 32768.0000 (36926.8666)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0624  max mem: 15572
Epoch: [34]  [2550/2809]  eta: 0:02:26  lr: 0.000002  min_lr: 0.000000  loss: 3.9482 (4.1946)  class_acc: 0.3333 (0.3211)  loss_scale: 32768.0000 (36910.5637)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1015  max mem: 15572
Epoch: [34]  [2560/2809]  eta: 0:02:21  lr: 0.000002  min_lr: 0.000000  loss: 4.2687 (4.1949)  class_acc: 0.2500 (0.3209)  loss_scale: 32768.0000 (36894.3881)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1125  max mem: 15572
Epoch: [34]  [2570/2809]  eta: 0:02:15  lr: 0.000002  min_lr: 0.000000  loss: 4.2862 (4.1953)  class_acc: 0.2500 (0.3207)  loss_scale: 32768.0000 (36878.3384)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1147  max mem: 15572
Epoch: [34]  [2580/2809]  eta: 0:02:09  lr: 0.000002  min_lr: 0.000000  loss: 4.2862 (4.1955)  class_acc: 0.2917 (0.3207)  loss_scale: 32768.0000 (36862.4130)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0957  max mem: 15572
Epoch: [34]  [2590/2809]  eta: 0:02:04  lr: 0.000002  min_lr: 0.000000  loss: 4.2400 (4.1952)  class_acc: 0.3333 (0.3208)  loss_scale: 32768.0000 (36846.6106)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0812  max mem: 15572
Epoch: [34]  [2600/2809]  eta: 0:01:58  lr: 0.000002  min_lr: 0.000000  loss: 4.0252 (4.1944)  class_acc: 0.3333 (0.3208)  loss_scale: 32768.0000 (36830.9296)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1144  max mem: 15572
[2025-01-16 06:55:34,602] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:55:34,602] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [2610/2809]  eta: 0:01:52  lr: 0.000002  min_lr: 0.000000  loss: 4.0186 (4.1944)  class_acc: 0.2917 (0.3208)  loss_scale: 32768.0000 (36878.1187)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0994  max mem: 15572
Epoch: [34]  [2620/2809]  eta: 0:01:47  lr: 0.000002  min_lr: 0.000000  loss: 4.1434 (4.1940)  class_acc: 0.3333 (0.3210)  loss_scale: 65536.0000 (36987.4582)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1012  max mem: 15572
Epoch: [34]  [2630/2809]  eta: 0:01:41  lr: 0.000002  min_lr: 0.000000  loss: 4.1294 (4.1935)  class_acc: 0.3750 (0.3212)  loss_scale: 65536.0000 (37095.9666)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1283  max mem: 15572
[2025-01-16 06:55:52,290] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98143
[2025-01-16 06:55:52,290] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:55:52,290] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [2640/2809]  eta: 0:01:35  lr: 0.000002  min_lr: 0.000000  loss: 4.0956 (4.1933)  class_acc: 0.3750 (0.3215)  loss_scale: 65536.0000 (37154.0235)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0797  max mem: 15572
Epoch: [34]  [2650/2809]  eta: 0:01:29  lr: 0.000002  min_lr: 0.000000  loss: 4.2600 (4.1941)  class_acc: 0.4167 (0.3217)  loss_scale: 32768.0000 (37137.4787)  weight_decay: 0.0500 (0.0500)  time: 0.4124  data: 0.0004  max mem: 15572
Epoch: [34]  [2660/2809]  eta: 0:01:24  lr: 0.000002  min_lr: 0.000000  loss: 4.3108 (4.1939)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (37121.0582)  weight_decay: 0.0500 (0.0500)  time: 0.4364  data: 0.0006  max mem: 15572
Epoch: [34]  [2670/2809]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 4.1679 (4.1932)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (37104.7608)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0665  max mem: 15572
Epoch: [34]  [2680/2809]  eta: 0:01:13  lr: 0.000002  min_lr: 0.000000  loss: 4.2449 (4.1936)  class_acc: 0.3750 (0.3219)  loss_scale: 32768.0000 (37088.5849)  weight_decay: 0.0500 (0.0500)  time: 0.6764  data: 0.2187  max mem: 15572
Epoch: [34]  [2690/2809]  eta: 0:01:07  lr: 0.000002  min_lr: 0.000000  loss: 4.2338 (4.1933)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (37072.5292)  weight_decay: 0.0500 (0.0500)  time: 0.7686  data: 0.2964  max mem: 15572
Epoch: [34]  [2700/2809]  eta: 0:01:01  lr: 0.000002  min_lr: 0.000000  loss: 4.0342 (4.1926)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37056.5924)  weight_decay: 0.0500 (0.0500)  time: 0.7209  data: 0.2317  max mem: 15572
Epoch: [34]  [2710/2809]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 4.1045 (4.1926)  class_acc: 0.3750 (0.3219)  loss_scale: 32768.0000 (37040.7731)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1310  max mem: 15572
Epoch: [34]  [2720/2809]  eta: 0:00:50  lr: 0.000002  min_lr: 0.000000  loss: 4.1784 (4.1923)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (37025.0702)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1312  max mem: 15572
Epoch: [34]  [2730/2809]  eta: 0:00:44  lr: 0.000002  min_lr: 0.000000  loss: 4.0629 (4.1917)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (37009.4822)  weight_decay: 0.0500 (0.0500)  time: 0.6934  data: 0.2131  max mem: 15572
[2025-01-16 06:56:52,348] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98241
[2025-01-16 06:56:52,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 06:56:52,350] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [2740/2809]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000000  loss: 4.1535 (4.1917)  class_acc: 0.3750 (0.3226)  loss_scale: 32768.0000 (36958.1437)  weight_decay: 0.0500 (0.0500)  time: 0.7805  data: 0.3044  max mem: 15572
Epoch: [34]  [2750/2809]  eta: 0:00:33  lr: 0.000002  min_lr: 0.000000  loss: 4.3167 (4.1919)  class_acc: 0.2500 (0.3225)  loss_scale: 16384.0000 (36883.3559)  weight_decay: 0.0500 (0.0500)  time: 0.6735  data: 0.2156  max mem: 15572
Epoch: [34]  [2760/2809]  eta: 0:00:27  lr: 0.000002  min_lr: 0.000000  loss: 4.2597 (4.1922)  class_acc: 0.2917 (0.3226)  loss_scale: 16384.0000 (36809.1097)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1169  max mem: 15572
Epoch: [34]  [2770/2809]  eta: 0:00:22  lr: 0.000002  min_lr: 0.000000  loss: 4.1784 (4.1922)  class_acc: 0.2917 (0.3223)  loss_scale: 16384.0000 (36735.3995)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.1751  max mem: 15572
Epoch: [34]  [2780/2809]  eta: 0:00:16  lr: 0.000002  min_lr: 0.000000  loss: 4.1703 (4.1919)  class_acc: 0.2500 (0.3223)  loss_scale: 16384.0000 (36662.2193)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1381  max mem: 15572
Epoch: [34]  [2790/2809]  eta: 0:00:10  lr: 0.000002  min_lr: 0.000000  loss: 4.1993 (4.1921)  class_acc: 0.2500 (0.3222)  loss_scale: 16384.0000 (36589.5636)  weight_decay: 0.0500 (0.0500)  time: 0.4562  data: 0.0439  max mem: 15572
Epoch: [34]  [2800/2809]  eta: 0:00:05  lr: 0.000002  min_lr: 0.000000  loss: 4.2283 (4.1925)  class_acc: 0.2917 (0.3223)  loss_scale: 16384.0000 (36517.4266)  weight_decay: 0.0500 (0.0500)  time: 0.3822  data: 0.0003  max mem: 15572
Epoch: [34]  [2808/2809]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 4.2229 (4.1924)  class_acc: 0.2917 (0.3221)  loss_scale: 16384.0000 (36460.0869)  weight_decay: 0.0500 (0.0500)  time: 0.3779  data: 0.0003  max mem: 15572
Epoch: [34] Total time: 0:26:36 (0.5683 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 4.2229 (4.1924)  class_acc: 0.2917 (0.3221)  loss_scale: 16384.0000 (36460.0869)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:28:16  loss: 1.0867 (1.0867)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 6.2367  data: 6.0602  max mem: 15572
Val:  [ 10/272]  eta: 0:03:41  loss: 2.4383 (2.4490)  acc1: 50.0000 (48.4848)  acc5: 77.7778 (77.7778)  time: 0.8473  data: 0.6738  max mem: 15572
Val:  [ 20/272]  eta: 0:02:18  loss: 2.4695 (2.5267)  acc1: 50.0000 (48.1481)  acc5: 77.7778 (76.1905)  time: 0.2634  data: 0.0765  max mem: 15572
Val:  [ 30/272]  eta: 0:01:49  loss: 2.6358 (2.5663)  acc1: 44.4444 (44.0860)  acc5: 72.2222 (75.2688)  time: 0.2364  data: 0.0471  max mem: 15572
Val:  [ 40/272]  eta: 0:01:31  loss: 2.6358 (2.5961)  acc1: 27.7778 (41.4634)  acc5: 72.2222 (74.2547)  time: 0.2340  data: 0.0560  max mem: 15572
Val:  [ 50/272]  eta: 0:01:24  loss: 2.4876 (2.5348)  acc1: 44.4444 (43.3551)  acc5: 77.7778 (76.2527)  time: 0.2636  data: 0.0825  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 1.9361 (2.4704)  acc1: 55.5556 (45.0820)  acc5: 83.3333 (76.9581)  time: 0.3232  data: 0.1407  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 1.9392 (2.4055)  acc1: 61.1111 (47.4961)  acc5: 83.3333 (78.0125)  time: 0.3217  data: 0.1322  max mem: 15572
Val:  [ 80/272]  eta: 0:01:07  loss: 2.1129 (2.4158)  acc1: 50.0000 (47.2565)  acc5: 83.3333 (77.6406)  time: 0.2893  data: 0.0955  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 2.4252 (2.4173)  acc1: 50.0000 (47.5580)  acc5: 83.3333 (78.2662)  time: 0.2935  data: 0.1074  max mem: 15572
Val:  [100/272]  eta: 0:00:58  loss: 2.4631 (2.4437)  acc1: 44.4444 (46.5897)  acc5: 83.3333 (77.9428)  time: 0.3103  data: 0.1197  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.5792 (2.4988)  acc1: 33.3333 (44.8949)  acc5: 77.7778 (76.8769)  time: 0.3196  data: 0.1198  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 2.9453 (2.5350)  acc1: 27.7778 (44.2608)  acc5: 66.6667 (76.1249)  time: 0.3185  data: 0.1276  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 2.4964 (2.5116)  acc1: 38.8889 (44.7837)  acc5: 77.7778 (76.8024)  time: 0.3019  data: 0.1133  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 2.1389 (2.5141)  acc1: 55.5556 (45.0749)  acc5: 83.3333 (76.5957)  time: 0.3330  data: 0.1332  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 2.4858 (2.5124)  acc1: 38.8889 (44.7020)  acc5: 77.7778 (76.7476)  time: 0.3317  data: 0.1307  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.4416 (2.5110)  acc1: 38.8889 (44.9620)  acc5: 77.7778 (76.9496)  time: 0.2987  data: 0.1013  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.6383 (2.5268)  acc1: 38.8889 (44.4769)  acc5: 72.2222 (76.3483)  time: 0.3018  data: 0.1085  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.5244 (2.5168)  acc1: 38.8889 (44.2910)  acc5: 72.2222 (76.5193)  time: 0.3706  data: 0.1734  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.4886 (2.5510)  acc1: 33.3333 (43.1355)  acc5: 72.2222 (75.3927)  time: 0.3837  data: 0.1788  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.7664 (2.5547)  acc1: 33.3333 (42.9519)  acc5: 66.6667 (75.3179)  time: 0.3072  data: 0.1081  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.3712 (2.5607)  acc1: 44.4444 (42.9437)  acc5: 77.7778 (75.2765)  time: 0.2835  data: 0.0896  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.6663 (2.5576)  acc1: 44.4444 (42.9613)  acc5: 77.7778 (75.3645)  time: 0.3221  data: 0.1276  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.2140 (2.5403)  acc1: 50.0000 (43.7229)  acc5: 77.7778 (75.6614)  time: 0.3300  data: 0.1365  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.0641 (2.5276)  acc1: 50.0000 (43.8451)  acc5: 83.3333 (75.9567)  time: 0.2926  data: 0.1025  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4116 (2.5337)  acc1: 38.8889 (43.4484)  acc5: 77.7778 (75.9849)  time: 0.3178  data: 0.1252  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9594 (2.4939)  acc1: 61.1111 (44.8914)  acc5: 88.8889 (76.6284)  time: 0.3132  data: 0.1254  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.8878 (2.4884)  acc1: 61.1111 (45.0595)  acc5: 88.8889 (76.7733)  time: 0.2099  data: 0.0483  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8878 (2.4917)  acc1: 61.1111 (45.0543)  acc5: 88.8889 (76.7561)  time: 0.2032  data: 0.0483  max mem: 15572
Val: Total time: 0:01:27 (0.3223 s / it)
* Acc@1 45.054 Acc@5 76.756 loss 2.492
Accuracy of the network on the 4883 val videos: 45.1%
[2025-01-16 06:59:00,181] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 06:59:00,185] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 06:59:00,185] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 06:59:02,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 06:59:02,989] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.05%
Epoch: [35]  [   0/2809]  eta: 6:29:13  lr: 0.000002  min_lr: 0.000000  loss: 3.9821 (3.9821)  class_acc: 0.3333 (0.3333)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 8.3138  data: 7.8114  max mem: 15572
Epoch: [35]  [  10/2809]  eta: 0:57:21  lr: 0.000002  min_lr: 0.000000  loss: 4.2039 (4.1244)  class_acc: 0.2500 (0.2538)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2297  data: 0.7810  max mem: 15572
Epoch: [35]  [  20/2809]  eta: 0:43:38  lr: 0.000002  min_lr: 0.000000  loss: 4.2499 (4.1878)  class_acc: 0.3333 (0.3036)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1302  max mem: 15572
Epoch: [35]  [  30/2809]  eta: 0:38:01  lr: 0.000002  min_lr: 0.000000  loss: 4.2574 (4.1659)  class_acc: 0.3333 (0.3199)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1549  max mem: 15572
Epoch: [35]  [  40/2809]  eta: 0:33:43  lr: 0.000002  min_lr: 0.000000  loss: 4.1352 (4.1552)  class_acc: 0.2917 (0.3089)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0639  max mem: 15572
Epoch: [35]  [  50/2809]  eta: 0:32:11  lr: 0.000002  min_lr: 0.000000  loss: 4.2313 (4.1851)  class_acc: 0.2500 (0.3031)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0565  max mem: 15572
[2025-01-16 06:59:41,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:59:41,466] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [  60/2809]  eta: 0:30:31  lr: 0.000002  min_lr: 0.000000  loss: 4.2816 (4.1993)  class_acc: 0.2500 (0.2917)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0887  max mem: 15572
Epoch: [35]  [  70/2809]  eta: 0:30:32  lr: 0.000002  min_lr: 0.000000  loss: 4.1463 (4.1737)  class_acc: 0.2917 (0.3046)  loss_scale: 32768.0000 (20076.1690)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1459  max mem: 15572
Epoch: [35]  [  80/2809]  eta: 0:29:51  lr: 0.000002  min_lr: 0.000000  loss: 4.0660 (4.1658)  class_acc: 0.3750 (0.3035)  loss_scale: 32768.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1689  max mem: 15572
Epoch: [35]  [  90/2809]  eta: 0:29:36  lr: 0.000002  min_lr: 0.000000  loss: 4.1820 (4.1576)  class_acc: 0.2917 (0.3136)  loss_scale: 32768.0000 (22865.5824)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1494  max mem: 15572
Epoch: [35]  [ 100/2809]  eta: 0:28:38  lr: 0.000002  min_lr: 0.000000  loss: 4.1384 (4.1678)  class_acc: 0.3333 (0.3123)  loss_scale: 32768.0000 (23846.0198)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1028  max mem: 15572
Epoch: [35]  [ 110/2809]  eta: 0:28:00  lr: 0.000002  min_lr: 0.000000  loss: 4.0291 (4.1469)  class_acc: 0.3333 (0.3183)  loss_scale: 32768.0000 (24649.8018)  weight_decay: 0.0500 (0.0500)  time: 0.4814  data: 0.0404  max mem: 15572
Epoch: [35]  [ 120/2809]  eta: 0:27:25  lr: 0.000002  min_lr: 0.000000  loss: 4.0074 (4.1437)  class_acc: 0.3750 (0.3206)  loss_scale: 32768.0000 (25320.7273)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0551  max mem: 15572
Epoch: [35]  [ 130/2809]  eta: 0:27:11  lr: 0.000002  min_lr: 0.000000  loss: 4.1721 (4.1546)  class_acc: 0.2917 (0.3203)  loss_scale: 32768.0000 (25889.2214)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0782  max mem: 15572
Epoch: [35]  [ 140/2809]  eta: 0:27:13  lr: 0.000002  min_lr: 0.000000  loss: 4.1797 (4.1528)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (26377.0780)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1658  max mem: 15572
Epoch: [35]  [ 150/2809]  eta: 0:27:00  lr: 0.000002  min_lr: 0.000000  loss: 4.0477 (4.1518)  class_acc: 0.2917 (0.3193)  loss_scale: 32768.0000 (26800.3179)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1635  max mem: 15572
Epoch: [35]  [ 160/2809]  eta: 0:26:41  lr: 0.000002  min_lr: 0.000000  loss: 4.1104 (4.1529)  class_acc: 0.3333 (0.3201)  loss_scale: 32768.0000 (27170.9814)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0956  max mem: 15572
Epoch: [35]  [ 170/2809]  eta: 0:26:32  lr: 0.000002  min_lr: 0.000000  loss: 4.2266 (4.1591)  class_acc: 0.3333 (0.3173)  loss_scale: 32768.0000 (27498.2924)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1124  max mem: 15572
Epoch: [35]  [ 180/2809]  eta: 0:26:18  lr: 0.000002  min_lr: 0.000000  loss: 4.2266 (4.1601)  class_acc: 0.2500 (0.3161)  loss_scale: 32768.0000 (27789.4365)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1277  max mem: 15572
[2025-01-16 07:00:54,922] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:00:54,923] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [ 190/2809]  eta: 0:26:17  lr: 0.000002  min_lr: 0.000000  loss: 4.2464 (4.1630)  class_acc: 0.3333 (0.3183)  loss_scale: 32768.0000 (29422.5759)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1433  max mem: 15572
Epoch: [35]  [ 200/2809]  eta: 0:26:02  lr: 0.000002  min_lr: 0.000000  loss: 4.2576 (4.1648)  class_acc: 0.3333 (0.3186)  loss_scale: 65536.0000 (31219.2637)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1358  max mem: 15572
Epoch: [35]  [ 210/2809]  eta: 0:26:00  lr: 0.000002  min_lr: 0.000000  loss: 4.2259 (4.1675)  class_acc: 0.2917 (0.3173)  loss_scale: 65536.0000 (32845.6493)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1382  max mem: 15572
Epoch: [35]  [ 220/2809]  eta: 0:25:49  lr: 0.000002  min_lr: 0.000000  loss: 4.1397 (4.1619)  class_acc: 0.2917 (0.3194)  loss_scale: 65536.0000 (34324.8507)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1486  max mem: 15572
Epoch: [35]  [ 230/2809]  eta: 0:25:50  lr: 0.000002  min_lr: 0.000000  loss: 4.2625 (4.1726)  class_acc: 0.3333 (0.3173)  loss_scale: 65536.0000 (35675.9827)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1613  max mem: 15572
Epoch: [35]  [ 240/2809]  eta: 0:25:35  lr: 0.000002  min_lr: 0.000000  loss: 4.2786 (4.1704)  class_acc: 0.3333 (0.3186)  loss_scale: 65536.0000 (36914.9876)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1441  max mem: 15572
Epoch: [35]  [ 250/2809]  eta: 0:25:28  lr: 0.000002  min_lr: 0.000000  loss: 4.1389 (4.1735)  class_acc: 0.2917 (0.3164)  loss_scale: 65536.0000 (38055.2669)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1177  max mem: 15572
[2025-01-16 07:01:36,497] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98569
[2025-01-16 07:01:36,497] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:01:36,497] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 260/2809]  eta: 0:25:24  lr: 0.000002  min_lr: 0.000000  loss: 4.1389 (4.1715)  class_acc: 0.3333 (0.3206)  loss_scale: 65536.0000 (38229.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1592  max mem: 15572
Epoch: [35]  [ 270/2809]  eta: 0:25:15  lr: 0.000002  min_lr: 0.000000  loss: 4.1428 (4.1681)  class_acc: 0.3750 (0.3227)  loss_scale: 32768.0000 (38027.8081)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1506  max mem: 15572
Epoch: [35]  [ 280/2809]  eta: 0:25:06  lr: 0.000002  min_lr: 0.000000  loss: 4.2413 (4.1736)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37840.6263)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1316  max mem: 15572
Epoch: [35]  [ 290/2809]  eta: 0:24:49  lr: 0.000002  min_lr: 0.000000  loss: 4.2577 (4.1750)  class_acc: 0.2917 (0.3235)  loss_scale: 32768.0000 (37666.3093)  weight_decay: 0.0500 (0.0500)  time: 0.5174  data: 0.0795  max mem: 15572
Epoch: [35]  [ 300/2809]  eta: 0:24:47  lr: 0.000002  min_lr: 0.000000  loss: 4.2573 (4.1768)  class_acc: 0.2917 (0.3217)  loss_scale: 32768.0000 (37503.5748)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1113  max mem: 15572
Epoch: [35]  [ 310/2809]  eta: 0:24:39  lr: 0.000002  min_lr: 0.000000  loss: 4.2112 (4.1791)  class_acc: 0.2500 (0.3199)  loss_scale: 32768.0000 (37351.3055)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1623  max mem: 15572
Epoch: [35]  [ 320/2809]  eta: 0:24:31  lr: 0.000002  min_lr: 0.000000  loss: 4.2957 (4.1821)  class_acc: 0.2917 (0.3198)  loss_scale: 32768.0000 (37208.5234)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1286  max mem: 15572
Epoch: [35]  [ 330/2809]  eta: 0:24:24  lr: 0.000002  min_lr: 0.000000  loss: 4.3487 (4.1858)  class_acc: 0.2917 (0.3197)  loss_scale: 32768.0000 (37074.3686)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1331  max mem: 15572
Epoch: [35]  [ 340/2809]  eta: 0:24:08  lr: 0.000002  min_lr: 0.000000  loss: 4.1132 (4.1801)  class_acc: 0.2917 (0.3199)  loss_scale: 32768.0000 (36948.0821)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0726  max mem: 15572
Epoch: [35]  [ 350/2809]  eta: 0:24:02  lr: 0.000002  min_lr: 0.000000  loss: 4.1058 (4.1836)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (36828.9915)  weight_decay: 0.0500 (0.0500)  time: 0.5227  data: 0.0645  max mem: 15572
Epoch: [35]  [ 360/2809]  eta: 0:23:51  lr: 0.000002  min_lr: 0.000000  loss: 4.2719 (4.1856)  class_acc: 0.2500 (0.3175)  loss_scale: 32768.0000 (36716.4986)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0787  max mem: 15572
Epoch: [35]  [ 370/2809]  eta: 0:23:45  lr: 0.000002  min_lr: 0.000000  loss: 4.1815 (4.1879)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (36610.0701)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0868  max mem: 15572
Epoch: [35]  [ 380/2809]  eta: 0:23:31  lr: 0.000002  min_lr: 0.000000  loss: 4.2147 (4.1907)  class_acc: 0.2500 (0.3174)  loss_scale: 32768.0000 (36509.2283)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0789  max mem: 15572
[2025-01-16 07:02:47,937] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:02:47,938] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 07:02:49,227] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98701
[2025-01-16 07:02:49,227] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:02:49,227] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 390/2809]  eta: 0:23:31  lr: 0.000002  min_lr: 0.000000  loss: 4.2147 (4.1916)  class_acc: 0.2500 (0.3167)  loss_scale: 32768.0000 (36664.9616)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1101  max mem: 15572
Epoch: [35]  [ 400/2809]  eta: 0:23:24  lr: 0.000002  min_lr: 0.000000  loss: 4.0884 (4.1883)  class_acc: 0.2917 (0.3169)  loss_scale: 32768.0000 (36567.7805)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1594  max mem: 15572
Epoch: [35]  [ 410/2809]  eta: 0:23:23  lr: 0.000002  min_lr: 0.000000  loss: 4.0580 (4.1875)  class_acc: 0.3333 (0.3176)  loss_scale: 32768.0000 (36475.3285)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1703  max mem: 15572
Epoch: [35]  [ 420/2809]  eta: 0:23:11  lr: 0.000002  min_lr: 0.000000  loss: 4.1244 (4.1876)  class_acc: 0.3333 (0.3182)  loss_scale: 32768.0000 (36387.2684)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1374  max mem: 15572
Epoch: [35]  [ 430/2809]  eta: 0:23:06  lr: 0.000002  min_lr: 0.000000  loss: 4.2483 (4.1892)  class_acc: 0.3333 (0.3177)  loss_scale: 32768.0000 (36303.2947)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.1029  max mem: 15572
Epoch: [35]  [ 440/2809]  eta: 0:22:57  lr: 0.000002  min_lr: 0.000000  loss: 4.2124 (4.1873)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (36223.1293)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1226  max mem: 15572
Epoch: [35]  [ 450/2809]  eta: 0:22:50  lr: 0.000002  min_lr: 0.000000  loss: 4.2124 (4.1896)  class_acc: 0.3750 (0.3184)  loss_scale: 32768.0000 (36146.5188)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1177  max mem: 15572
Epoch: [35]  [ 460/2809]  eta: 0:22:46  lr: 0.000002  min_lr: 0.000000  loss: 4.1238 (4.1883)  class_acc: 0.3333 (0.3184)  loss_scale: 32768.0000 (36073.2321)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1548  max mem: 15572
Epoch: [35]  [ 470/2809]  eta: 0:22:40  lr: 0.000002  min_lr: 0.000000  loss: 4.1238 (4.1892)  class_acc: 0.2500 (0.3182)  loss_scale: 32768.0000 (36003.0573)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1489  max mem: 15572
Epoch: [35]  [ 480/2809]  eta: 0:22:31  lr: 0.000002  min_lr: 0.000000  loss: 4.2206 (4.1891)  class_acc: 0.2917 (0.3189)  loss_scale: 32768.0000 (35935.8004)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0999  max mem: 15572
Epoch: [35]  [ 490/2809]  eta: 0:22:26  lr: 0.000002  min_lr: 0.000000  loss: 4.1606 (4.1909)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (35871.2831)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1121  max mem: 15572
Epoch: [35]  [ 500/2809]  eta: 0:22:24  lr: 0.000002  min_lr: 0.000000  loss: 4.1702 (4.1877)  class_acc: 0.3333 (0.3189)  loss_scale: 32768.0000 (35809.3413)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1860  max mem: 15572
Epoch: [35]  [ 510/2809]  eta: 0:22:16  lr: 0.000002  min_lr: 0.000000  loss: 4.1702 (4.1871)  class_acc: 0.2500 (0.3186)  loss_scale: 32768.0000 (35749.8239)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1684  max mem: 15572
[2025-01-16 07:04:04,971] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:04:04,972] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [ 520/2809]  eta: 0:22:15  lr: 0.000002  min_lr: 0.000000  loss: 4.2107 (4.1895)  class_acc: 0.3333 (0.3188)  loss_scale: 32768.0000 (36069.9578)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1858  max mem: 15572
Epoch: [35]  [ 530/2809]  eta: 0:22:08  lr: 0.000002  min_lr: 0.000000  loss: 4.2489 (4.1904)  class_acc: 0.3750 (0.3195)  loss_scale: 65536.0000 (36624.8738)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1798  max mem: 15572
Epoch: [35]  [ 540/2809]  eta: 0:21:57  lr: 0.000002  min_lr: 0.000000  loss: 4.2223 (4.1897)  class_acc: 0.2917 (0.3197)  loss_scale: 65536.0000 (37159.2754)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0695  max mem: 15572
Epoch: [35]  [ 550/2809]  eta: 0:21:54  lr: 0.000002  min_lr: 0.000000  loss: 4.2563 (4.1911)  class_acc: 0.2917 (0.3192)  loss_scale: 65536.0000 (37674.2795)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1175  max mem: 15572
[2025-01-16 07:04:26,985] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98868
[2025-01-16 07:04:26,985] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:04:26,985] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 560/2809]  eta: 0:21:50  lr: 0.000002  min_lr: 0.000000  loss: 4.1554 (4.1897)  class_acc: 0.2917 (0.3192)  loss_scale: 65536.0000 (37703.6435)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.1941  max mem: 15572
Epoch: [35]  [ 570/2809]  eta: 0:21:44  lr: 0.000002  min_lr: 0.000000  loss: 4.0462 (4.1894)  class_acc: 0.2917 (0.3187)  loss_scale: 32768.0000 (37617.2049)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1495  max mem: 15572
Epoch: [35]  [ 580/2809]  eta: 0:21:38  lr: 0.000002  min_lr: 0.000000  loss: 4.1282 (4.1890)  class_acc: 0.3333 (0.3193)  loss_scale: 32768.0000 (37533.7418)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1281  max mem: 15572
Epoch: [35]  [ 590/2809]  eta: 0:21:31  lr: 0.000002  min_lr: 0.000000  loss: 4.1282 (4.1912)  class_acc: 0.3333 (0.3191)  loss_scale: 32768.0000 (37453.1032)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1191  max mem: 15572
Epoch: [35]  [ 600/2809]  eta: 0:21:23  lr: 0.000002  min_lr: 0.000000  loss: 4.1553 (4.1912)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (37375.1481)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1030  max mem: 15572
Epoch: [35]  [ 610/2809]  eta: 0:21:13  lr: 0.000002  min_lr: 0.000000  loss: 4.1553 (4.1914)  class_acc: 0.3750 (0.3211)  loss_scale: 32768.0000 (37299.7447)  weight_decay: 0.0500 (0.0500)  time: 0.4978  data: 0.0699  max mem: 15572
Epoch: [35]  [ 620/2809]  eta: 0:21:08  lr: 0.000002  min_lr: 0.000000  loss: 4.1439 (4.1924)  class_acc: 0.3750 (0.3216)  loss_scale: 32768.0000 (37226.7697)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.1023  max mem: 15572
Epoch: [35]  [ 630/2809]  eta: 0:21:01  lr: 0.000002  min_lr: 0.000000  loss: 4.2487 (4.1938)  class_acc: 0.3750 (0.3212)  loss_scale: 32768.0000 (37156.1078)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1236  max mem: 15572
Epoch: [35]  [ 640/2809]  eta: 0:20:55  lr: 0.000002  min_lr: 0.000000  loss: 4.2922 (4.1952)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (37087.6505)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.0970  max mem: 15572
Epoch: [35]  [ 650/2809]  eta: 0:20:49  lr: 0.000002  min_lr: 0.000000  loss: 4.0962 (4.1924)  class_acc: 0.3750 (0.3226)  loss_scale: 32768.0000 (37021.2965)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1196  max mem: 15572
Epoch: [35]  [ 660/2809]  eta: 0:20:44  lr: 0.000002  min_lr: 0.000000  loss: 4.0205 (4.1931)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (36956.9501)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1410  max mem: 15572
Epoch: [35]  [ 670/2809]  eta: 0:20:36  lr: 0.000002  min_lr: 0.000000  loss: 4.2562 (4.1938)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (36894.5216)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1069  max mem: 15572
Epoch: [35]  [ 680/2809]  eta: 0:20:29  lr: 0.000002  min_lr: 0.000000  loss: 4.2562 (4.1956)  class_acc: 0.2917 (0.3231)  loss_scale: 32768.0000 (36833.9266)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0568  max mem: 15572
[2025-01-16 07:05:38,534] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:05:38,534] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 07:05:39,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=99000, skipped=615, lr=[2.0817667707773205e-08, 2.0817667707773205e-08, 2.9739525296818868e-08, 2.9739525296818868e-08, 4.248503613831267e-08, 4.248503613831267e-08, 6.06929087690181e-08, 6.06929087690181e-08, 8.670415538431158e-08, 8.670415538431158e-08, 1.2386307912044512e-07, 1.2386307912044512e-07, 1.7694725588635018e-07, 1.7694725588635018e-07, 2.5278179412335745e-07, 2.5278179412335745e-07, 3.611168487476535e-07, 3.611168487476535e-07, 5.158812124966479e-07, 5.158812124966479e-07, 7.36973160709497e-07, 7.36973160709497e-07, 1.0528188010135672e-06, 1.0528188010135672e-06, 1.5040268585908103e-06, 1.5040268585908103e-06, 2.148609797986872e-06, 2.148609797986872e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:05:39,464] [INFO] [timer.py:260:stop] epoch=0/micro_step=99000/global_step=99000, RunningAvgSamplesPerSec=27.970183966288648, CurrSamplesPerSec=25.77381541882231, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [35]  [ 690/2809]  eta: 0:20:23  lr: 0.000002  min_lr: 0.000000  loss: 4.2445 (4.1976)  class_acc: 0.2500 (0.3223)  loss_scale: 32768.0000 (37201.8755)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0938  max mem: 15572
Epoch: [35]  [ 700/2809]  eta: 0:20:18  lr: 0.000002  min_lr: 0.000000  loss: 4.2559 (4.1976)  class_acc: 0.2917 (0.3226)  loss_scale: 65536.0000 (37606.0713)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1363  max mem: 15572
Epoch: [35]  [ 710/2809]  eta: 0:20:09  lr: 0.000002  min_lr: 0.000000  loss: 4.2462 (4.1993)  class_acc: 0.2917 (0.3217)  loss_scale: 65536.0000 (37998.8973)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0855  max mem: 15572
Epoch: [35]  [ 720/2809]  eta: 0:20:01  lr: 0.000002  min_lr: 0.000000  loss: 4.2462 (4.1993)  class_acc: 0.3333 (0.3233)  loss_scale: 65536.0000 (38380.8266)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0447  max mem: 15572
[2025-01-16 07:06:04,380] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99045
[2025-01-16 07:06:04,380] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:06:04,381] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 730/2809]  eta: 0:19:57  lr: 0.000002  min_lr: 0.000000  loss: 4.0506 (4.1973)  class_acc: 0.3750 (0.3228)  loss_scale: 65536.0000 (38707.4802)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1264  max mem: 15572
Epoch: [35]  [ 740/2809]  eta: 0:19:52  lr: 0.000002  min_lr: 0.000000  loss: 4.1611 (4.1979)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (38627.3252)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1554  max mem: 15572
Epoch: [35]  [ 750/2809]  eta: 0:19:44  lr: 0.000002  min_lr: 0.000000  loss: 4.2688 (4.1996)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (38549.3049)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0913  max mem: 15572
Epoch: [35]  [ 760/2809]  eta: 0:19:38  lr: 0.000002  min_lr: 0.000000  loss: 4.1983 (4.1983)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (38473.3351)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0866  max mem: 15572
Epoch: [35]  [ 770/2809]  eta: 0:19:30  lr: 0.000002  min_lr: 0.000000  loss: 3.9937 (4.1957)  class_acc: 0.2917 (0.3245)  loss_scale: 32768.0000 (38399.3359)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0583  max mem: 15572
Epoch: [35]  [ 780/2809]  eta: 0:19:25  lr: 0.000002  min_lr: 0.000000  loss: 4.0147 (4.1947)  class_acc: 0.3333 (0.3239)  loss_scale: 32768.0000 (38327.2318)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0993  max mem: 15572
Epoch: [35]  [ 790/2809]  eta: 0:19:18  lr: 0.000002  min_lr: 0.000000  loss: 4.0563 (4.1925)  class_acc: 0.3333 (0.3249)  loss_scale: 32768.0000 (38256.9507)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1435  max mem: 15572
Epoch: [35]  [ 800/2809]  eta: 0:19:13  lr: 0.000002  min_lr: 0.000000  loss: 4.0413 (4.1910)  class_acc: 0.3750 (0.3250)  loss_scale: 32768.0000 (38188.4245)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1079  max mem: 15572
Epoch: [35]  [ 810/2809]  eta: 0:19:07  lr: 0.000002  min_lr: 0.000000  loss: 4.1111 (4.1922)  class_acc: 0.2917 (0.3245)  loss_scale: 32768.0000 (38121.5882)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1182  max mem: 15572
Epoch: [35]  [ 820/2809]  eta: 0:19:02  lr: 0.000002  min_lr: 0.000000  loss: 4.1111 (4.1904)  class_acc: 0.3333 (0.3253)  loss_scale: 32768.0000 (38056.3800)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1552  max mem: 15572
Epoch: [35]  [ 830/2809]  eta: 0:18:55  lr: 0.000002  min_lr: 0.000000  loss: 4.0900 (4.1894)  class_acc: 0.3750 (0.3258)  loss_scale: 32768.0000 (37992.7413)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1330  max mem: 15572
Epoch: [35]  [ 840/2809]  eta: 0:18:50  lr: 0.000002  min_lr: 0.000000  loss: 4.1900 (4.1899)  class_acc: 0.2917 (0.3258)  loss_scale: 32768.0000 (37930.6159)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0983  max mem: 15572
Epoch: [35]  [ 850/2809]  eta: 0:18:43  lr: 0.000002  min_lr: 0.000000  loss: 4.1900 (4.1895)  class_acc: 0.2500 (0.3254)  loss_scale: 32768.0000 (37869.9506)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1163  max mem: 15572
[2025-01-16 07:07:16,780] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:07:16,780] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [ 860/2809]  eta: 0:18:38  lr: 0.000002  min_lr: 0.000000  loss: 4.1708 (4.1895)  class_acc: 0.2500 (0.3250)  loss_scale: 32768.0000 (37886.8107)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1227  max mem: 15572
[2025-01-16 07:07:20,294] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99180
[2025-01-16 07:07:20,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:07:20,295] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 870/2809]  eta: 0:18:30  lr: 0.000002  min_lr: 0.000000  loss: 4.1048 (4.1882)  class_acc: 0.2917 (0.3251)  loss_scale: 32768.0000 (37978.5258)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1165  max mem: 15572
Epoch: [35]  [ 880/2809]  eta: 0:18:25  lr: 0.000002  min_lr: 0.000000  loss: 4.1260 (4.1888)  class_acc: 0.2917 (0.3243)  loss_scale: 32768.0000 (37919.3825)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.1108  max mem: 15572
Epoch: [35]  [ 890/2809]  eta: 0:18:19  lr: 0.000002  min_lr: 0.000000  loss: 4.2807 (4.1880)  class_acc: 0.2917 (0.3242)  loss_scale: 32768.0000 (37861.5668)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1419  max mem: 15572
Epoch: [35]  [ 900/2809]  eta: 0:18:14  lr: 0.000002  min_lr: 0.000000  loss: 4.1797 (4.1892)  class_acc: 0.2917 (0.3239)  loss_scale: 32768.0000 (37805.0344)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1473  max mem: 15572
Epoch: [35]  [ 910/2809]  eta: 0:18:08  lr: 0.000002  min_lr: 0.000000  loss: 4.2441 (4.1888)  class_acc: 0.2917 (0.3236)  loss_scale: 32768.0000 (37749.7431)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1223  max mem: 15572
Epoch: [35]  [ 920/2809]  eta: 0:18:04  lr: 0.000002  min_lr: 0.000000  loss: 4.2358 (4.1883)  class_acc: 0.2917 (0.3235)  loss_scale: 32768.0000 (37695.6526)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1742  max mem: 15572
Epoch: [35]  [ 930/2809]  eta: 0:17:57  lr: 0.000002  min_lr: 0.000000  loss: 4.2439 (4.1894)  class_acc: 0.2083 (0.3221)  loss_scale: 32768.0000 (37642.7240)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1444  max mem: 15572
Epoch: [35]  [ 940/2809]  eta: 0:17:51  lr: 0.000002  min_lr: 0.000000  loss: 4.2439 (4.1896)  class_acc: 0.2500 (0.3219)  loss_scale: 32768.0000 (37590.9203)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0957  max mem: 15572
Epoch: [35]  [ 950/2809]  eta: 0:17:46  lr: 0.000002  min_lr: 0.000000  loss: 4.2476 (4.1909)  class_acc: 0.2917 (0.3213)  loss_scale: 32768.0000 (37540.2061)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1348  max mem: 15572
Epoch: [35]  [ 960/2809]  eta: 0:17:40  lr: 0.000002  min_lr: 0.000000  loss: 4.2371 (4.1900)  class_acc: 0.2500 (0.3212)  loss_scale: 32768.0000 (37490.5473)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1399  max mem: 15572
Epoch: [35]  [ 970/2809]  eta: 0:17:34  lr: 0.000002  min_lr: 0.000000  loss: 4.1634 (4.1885)  class_acc: 0.2917 (0.3218)  loss_scale: 32768.0000 (37441.9114)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1340  max mem: 15572
Epoch: [35]  [ 980/2809]  eta: 0:17:28  lr: 0.000002  min_lr: 0.000000  loss: 4.1448 (4.1888)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37394.2671)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1163  max mem: 15572
Epoch: [35]  [ 990/2809]  eta: 0:17:23  lr: 0.000002  min_lr: 0.000000  loss: 4.2691 (4.1897)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (37347.5843)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1320  max mem: 15572
[2025-01-16 07:08:34,515] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:08:34,515] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 07:08:35,411] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99311
[2025-01-16 07:08:35,411] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:08:35,411] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1000/2809]  eta: 0:17:16  lr: 0.000002  min_lr: 0.000000  loss: 4.2691 (4.1904)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (37367.3047)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1209  max mem: 15572
Epoch: [35]  [1010/2809]  eta: 0:17:11  lr: 0.000002  min_lr: 0.000000  loss: 4.2114 (4.1906)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (37321.8121)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1134  max mem: 15572
Epoch: [35]  [1020/2809]  eta: 0:17:06  lr: 0.000002  min_lr: 0.000000  loss: 4.2114 (4.1900)  class_acc: 0.3750 (0.3226)  loss_scale: 32768.0000 (37277.2106)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1766  max mem: 15572
Epoch: [35]  [1030/2809]  eta: 0:16:59  lr: 0.000002  min_lr: 0.000000  loss: 4.1923 (4.1900)  class_acc: 0.3750 (0.3230)  loss_scale: 32768.0000 (37233.4743)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1362  max mem: 15572
Epoch: [35]  [1040/2809]  eta: 0:16:53  lr: 0.000002  min_lr: 0.000000  loss: 4.2626 (4.1918)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (37190.5783)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0589  max mem: 15572
Epoch: [35]  [1050/2809]  eta: 0:16:46  lr: 0.000002  min_lr: 0.000000  loss: 4.2420 (4.1917)  class_acc: 0.3750 (0.3245)  loss_scale: 32768.0000 (37148.4986)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0669  max mem: 15572
Epoch: [35]  [1060/2809]  eta: 0:16:39  lr: 0.000002  min_lr: 0.000000  loss: 4.2565 (4.1937)  class_acc: 0.2917 (0.3240)  loss_scale: 32768.0000 (37107.2121)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0608  max mem: 15572
Epoch: [35]  [1070/2809]  eta: 0:16:34  lr: 0.000002  min_lr: 0.000000  loss: 4.3176 (4.1943)  class_acc: 0.2917 (0.3239)  loss_scale: 32768.0000 (37066.6965)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0986  max mem: 15572
Epoch: [35]  [1080/2809]  eta: 0:16:27  lr: 0.000002  min_lr: 0.000000  loss: 4.1520 (4.1943)  class_acc: 0.3750 (0.3242)  loss_scale: 32768.0000 (37026.9306)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0965  max mem: 15572
Epoch: [35]  [1090/2809]  eta: 0:16:20  lr: 0.000002  min_lr: 0.000000  loss: 4.1799 (4.1945)  class_acc: 0.3333 (0.3242)  loss_scale: 32768.0000 (36987.8937)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0707  max mem: 15572
Epoch: [35]  [1100/2809]  eta: 0:16:15  lr: 0.000002  min_lr: 0.000000  loss: 4.2742 (4.1952)  class_acc: 0.3333 (0.3241)  loss_scale: 32768.0000 (36949.5658)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.0916  max mem: 15572
Epoch: [35]  [1110/2809]  eta: 0:16:08  lr: 0.000002  min_lr: 0.000000  loss: 4.2742 (4.1968)  class_acc: 0.2083 (0.3231)  loss_scale: 32768.0000 (36911.9280)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0995  max mem: 15572
Epoch: [35]  [1120/2809]  eta: 0:16:03  lr: 0.000002  min_lr: 0.000000  loss: 4.2274 (4.1962)  class_acc: 0.2083 (0.3236)  loss_scale: 32768.0000 (36874.9616)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1263  max mem: 15572
[2025-01-16 07:09:45,387] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:09:45,387] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1130/2809]  eta: 0:15:56  lr: 0.000002  min_lr: 0.000000  loss: 4.1580 (4.1957)  class_acc: 0.3333 (0.3235)  loss_scale: 32768.0000 (37012.4845)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0960  max mem: 15572
Epoch: [35]  [1140/2809]  eta: 0:15:51  lr: 0.000002  min_lr: 0.000000  loss: 4.2423 (4.1971)  class_acc: 0.2500 (0.3229)  loss_scale: 65536.0000 (37262.4715)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0936  max mem: 15572
Epoch: [35]  [1150/2809]  eta: 0:15:45  lr: 0.000002  min_lr: 0.000000  loss: 4.2981 (4.1965)  class_acc: 0.2500 (0.3224)  loss_scale: 65536.0000 (37508.1147)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1431  max mem: 15572
[2025-01-16 07:10:02,630] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99471
[2025-01-16 07:10:02,630] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:10:02,630] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1160/2809]  eta: 0:15:39  lr: 0.000002  min_lr: 0.000000  loss: 4.1372 (4.1953)  class_acc: 0.2917 (0.3226)  loss_scale: 65536.0000 (37608.4065)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1153  max mem: 15572
Epoch: [35]  [1170/2809]  eta: 0:15:34  lr: 0.000002  min_lr: 0.000000  loss: 4.1372 (4.1944)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (37567.0709)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1211  max mem: 15572
Epoch: [35]  [1180/2809]  eta: 0:15:28  lr: 0.000002  min_lr: 0.000000  loss: 4.2512 (4.1949)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (37526.4352)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1282  max mem: 15572
Epoch: [35]  [1190/2809]  eta: 0:15:22  lr: 0.000002  min_lr: 0.000000  loss: 4.3239 (4.1958)  class_acc: 0.2500 (0.3221)  loss_scale: 32768.0000 (37486.4819)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1122  max mem: 15572
Epoch: [35]  [1200/2809]  eta: 0:15:16  lr: 0.000002  min_lr: 0.000000  loss: 4.1981 (4.1948)  class_acc: 0.3333 (0.3221)  loss_scale: 32768.0000 (37447.1940)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0990  max mem: 15572
Epoch: [35]  [1210/2809]  eta: 0:15:10  lr: 0.000002  min_lr: 0.000000  loss: 4.1633 (4.1944)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (37408.5549)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0736  max mem: 15572
Epoch: [35]  [1220/2809]  eta: 0:15:05  lr: 0.000002  min_lr: 0.000000  loss: 4.1062 (4.1930)  class_acc: 0.3333 (0.3220)  loss_scale: 32768.0000 (37370.5487)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1351  max mem: 15572
Epoch: [35]  [1230/2809]  eta: 0:14:58  lr: 0.000002  min_lr: 0.000000  loss: 4.1496 (4.1934)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (37333.1600)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1023  max mem: 15572
Epoch: [35]  [1240/2809]  eta: 0:14:53  lr: 0.000002  min_lr: 0.000000  loss: 4.1642 (4.1932)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (37296.3739)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0703  max mem: 15572
Epoch: [35]  [1250/2809]  eta: 0:14:48  lr: 0.000002  min_lr: 0.000000  loss: 4.1242 (4.1924)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (37260.1759)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1547  max mem: 15572
Epoch: [35]  [1260/2809]  eta: 0:14:44  lr: 0.000002  min_lr: 0.000000  loss: 4.2002 (4.1923)  class_acc: 0.2917 (0.3225)  loss_scale: 32768.0000 (37224.5519)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.1814  max mem: 15572
Epoch: [35]  [1270/2809]  eta: 0:14:37  lr: 0.000002  min_lr: 0.000000  loss: 4.1797 (4.1924)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (37189.4886)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1142  max mem: 15572
Epoch: [35]  [1280/2809]  eta: 0:14:31  lr: 0.000002  min_lr: 0.000000  loss: 4.1808 (4.1929)  class_acc: 0.3333 (0.3233)  loss_scale: 32768.0000 (37154.9727)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0523  max mem: 15572
[2025-01-16 07:11:17,097] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:11:17,097] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 07:11:18,518] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99603
[2025-01-16 07:11:18,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:11:18,519] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1290/2809]  eta: 0:14:25  lr: 0.000002  min_lr: 0.000000  loss: 4.1935 (4.1933)  class_acc: 0.3750 (0.3237)  loss_scale: 32768.0000 (37197.1371)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0989  max mem: 15572
Epoch: [35]  [1300/2809]  eta: 0:14:20  lr: 0.000002  min_lr: 0.000000  loss: 4.3092 (4.1946)  class_acc: 0.2917 (0.3235)  loss_scale: 32768.0000 (37163.0930)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1365  max mem: 15572
Epoch: [35]  [1310/2809]  eta: 0:14:14  lr: 0.000002  min_lr: 0.000000  loss: 4.2131 (4.1936)  class_acc: 0.2917 (0.3237)  loss_scale: 32768.0000 (37129.5683)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1212  max mem: 15572
Epoch: [35]  [1320/2809]  eta: 0:14:09  lr: 0.000002  min_lr: 0.000000  loss: 4.3147 (4.1947)  class_acc: 0.2917 (0.3233)  loss_scale: 32768.0000 (37096.5511)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1183  max mem: 15572
Epoch: [35]  [1330/2809]  eta: 0:14:03  lr: 0.000002  min_lr: 0.000000  loss: 4.3322 (4.1939)  class_acc: 0.2500 (0.3230)  loss_scale: 32768.0000 (37064.0301)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1347  max mem: 15572
Epoch: [35]  [1340/2809]  eta: 0:13:56  lr: 0.000002  min_lr: 0.000000  loss: 4.0921 (4.1934)  class_acc: 0.2917 (0.3231)  loss_scale: 32768.0000 (37031.9940)  weight_decay: 0.0500 (0.0500)  time: 0.5227  data: 0.0976  max mem: 15572
Epoch: [35]  [1350/2809]  eta: 0:13:51  lr: 0.000002  min_lr: 0.000000  loss: 4.1140 (4.1936)  class_acc: 0.2917 (0.3232)  loss_scale: 32768.0000 (37000.4323)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.1059  max mem: 15572
Epoch: [35]  [1360/2809]  eta: 0:13:45  lr: 0.000002  min_lr: 0.000000  loss: 4.2671 (4.1942)  class_acc: 0.2500 (0.3225)  loss_scale: 32768.0000 (36969.3343)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1415  max mem: 15572
Epoch: [35]  [1370/2809]  eta: 0:13:39  lr: 0.000002  min_lr: 0.000000  loss: 4.1322 (4.1932)  class_acc: 0.2500 (0.3226)  loss_scale: 32768.0000 (36938.6900)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1277  max mem: 15572
Epoch: [35]  [1380/2809]  eta: 0:13:34  lr: 0.000002  min_lr: 0.000000  loss: 4.0393 (4.1933)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (36908.4895)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1348  max mem: 15572
Epoch: [35]  [1390/2809]  eta: 0:13:27  lr: 0.000002  min_lr: 0.000000  loss: 4.2310 (4.1943)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (36878.7232)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1009  max mem: 15572
Epoch: [35]  [1400/2809]  eta: 0:13:22  lr: 0.000002  min_lr: 0.000000  loss: 4.1680 (4.1934)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (36849.3819)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1041  max mem: 15572
Epoch: [35]  [1410/2809]  eta: 0:13:17  lr: 0.000002  min_lr: 0.000000  loss: 4.1126 (4.1929)  class_acc: 0.3750 (0.3229)  loss_scale: 32768.0000 (36820.4564)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.1525  max mem: 15572
[2025-01-16 07:12:32,779] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:12:32,779] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1420/2809]  eta: 0:13:12  lr: 0.000002  min_lr: 0.000000  loss: 4.2743 (4.1935)  class_acc: 0.2500 (0.3222)  loss_scale: 32768.0000 (36884.1773)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1459  max mem: 15572
[2025-01-16 07:12:39,124] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99743
[2025-01-16 07:12:39,125] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:12:39,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1430/2809]  eta: 0:13:06  lr: 0.000002  min_lr: 0.000000  loss: 4.3616 (4.1942)  class_acc: 0.2083 (0.3219)  loss_scale: 65536.0000 (37015.7037)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1293  max mem: 15572
Epoch: [35]  [1440/2809]  eta: 0:13:01  lr: 0.000002  min_lr: 0.000000  loss: 4.2445 (4.1942)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (36986.2262)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1222  max mem: 15572
Epoch: [35]  [1450/2809]  eta: 0:12:54  lr: 0.000002  min_lr: 0.000000  loss: 4.2309 (4.1946)  class_acc: 0.2917 (0.3217)  loss_scale: 32768.0000 (36957.1551)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0979  max mem: 15572
Epoch: [35]  [1460/2809]  eta: 0:12:48  lr: 0.000002  min_lr: 0.000000  loss: 4.2116 (4.1932)  class_acc: 0.3333 (0.3220)  loss_scale: 32768.0000 (36928.4819)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0914  max mem: 15572
Epoch: [35]  [1470/2809]  eta: 0:12:43  lr: 0.000002  min_lr: 0.000000  loss: 4.0053 (4.1927)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (36900.1985)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1192  max mem: 15572
Epoch: [35]  [1480/2809]  eta: 0:12:36  lr: 0.000002  min_lr: 0.000000  loss: 4.0754 (4.1925)  class_acc: 0.2500 (0.3217)  loss_scale: 32768.0000 (36872.2971)  weight_decay: 0.0500 (0.0500)  time: 0.4951  data: 0.0640  max mem: 15572
Epoch: [35]  [1490/2809]  eta: 0:12:29  lr: 0.000002  min_lr: 0.000000  loss: 4.2427 (4.1932)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (36844.7700)  weight_decay: 0.0500 (0.0500)  time: 0.4656  data: 0.0229  max mem: 15572
Epoch: [35]  [1500/2809]  eta: 0:12:24  lr: 0.000002  min_lr: 0.000000  loss: 4.2758 (4.1934)  class_acc: 0.2917 (0.3210)  loss_scale: 32768.0000 (36817.6096)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1163  max mem: 15572
Epoch: [35]  [1510/2809]  eta: 0:12:19  lr: 0.000002  min_lr: 0.000000  loss: 4.2897 (4.1938)  class_acc: 0.2917 (0.3210)  loss_scale: 32768.0000 (36790.8087)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1856  max mem: 15572
Epoch: [35]  [1520/2809]  eta: 0:12:13  lr: 0.000002  min_lr: 0.000000  loss: 4.2810 (4.1943)  class_acc: 0.3333 (0.3211)  loss_scale: 32768.0000 (36764.3603)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1524  max mem: 15572
Epoch: [35]  [1530/2809]  eta: 0:12:07  lr: 0.000002  min_lr: 0.000000  loss: 4.2455 (4.1944)  class_acc: 0.2500 (0.3205)  loss_scale: 32768.0000 (36738.2573)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0893  max mem: 15572
Epoch: [35]  [1540/2809]  eta: 0:12:01  lr: 0.000002  min_lr: 0.000000  loss: 4.0729 (4.1935)  class_acc: 0.2917 (0.3211)  loss_scale: 32768.0000 (36712.4932)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0790  max mem: 15572
Epoch: [35]  [1550/2809]  eta: 0:11:55  lr: 0.000002  min_lr: 0.000000  loss: 4.0559 (4.1933)  class_acc: 0.3750 (0.3212)  loss_scale: 32768.0000 (36687.0613)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0785  max mem: 15572
[2025-01-16 07:13:50,591] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:13:50,591] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1560/2809]  eta: 0:11:50  lr: 0.000002  min_lr: 0.000000  loss: 4.2800 (4.1943)  class_acc: 0.3333 (0.3210)  loss_scale: 32768.0000 (36745.9218)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1464  max mem: 15572
Epoch: [35]  [1570/2809]  eta: 0:11:45  lr: 0.000002  min_lr: 0.000000  loss: 4.3378 (4.1947)  class_acc: 0.2917 (0.3213)  loss_scale: 65536.0000 (36929.1814)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1901  max mem: 15572
Epoch: [35]  [1580/2809]  eta: 0:11:39  lr: 0.000002  min_lr: 0.000000  loss: 4.2763 (4.1948)  class_acc: 0.2917 (0.3209)  loss_scale: 65536.0000 (37110.1227)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1571  max mem: 15572
Epoch: [35]  [1590/2809]  eta: 0:11:34  lr: 0.000002  min_lr: 0.000000  loss: 4.2195 (4.1945)  class_acc: 0.2917 (0.3211)  loss_scale: 65536.0000 (37288.7894)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1674  max mem: 15572
Epoch: [35]  [1600/2809]  eta: 0:11:28  lr: 0.000002  min_lr: 0.000000  loss: 4.2033 (4.1949)  class_acc: 0.3333 (0.3211)  loss_scale: 65536.0000 (37465.2242)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1363  max mem: 15572
[2025-01-16 07:14:19,559] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99922
[2025-01-16 07:14:19,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:14:19,560] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1610/2809]  eta: 0:11:22  lr: 0.000002  min_lr: 0.000000  loss: 4.1583 (4.1949)  class_acc: 0.3333 (0.3216)  loss_scale: 65536.0000 (37558.1080)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1309  max mem: 15572
Epoch: [35]  [1620/2809]  eta: 0:11:16  lr: 0.000002  min_lr: 0.000000  loss: 4.0361 (4.1933)  class_acc: 0.3333 (0.3215)  loss_scale: 32768.0000 (37528.5577)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1290  max mem: 15572
Epoch: [35]  [1630/2809]  eta: 0:11:11  lr: 0.000002  min_lr: 0.000000  loss: 4.0361 (4.1938)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (37499.3697)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1360  max mem: 15572
Epoch: [35]  [1640/2809]  eta: 0:11:05  lr: 0.000002  min_lr: 0.000000  loss: 4.2010 (4.1935)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (37470.5375)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1244  max mem: 15572
Epoch: [35]  [1650/2809]  eta: 0:11:00  lr: 0.000002  min_lr: 0.000000  loss: 4.1836 (4.1936)  class_acc: 0.3333 (0.3214)  loss_scale: 32768.0000 (37442.0545)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1127  max mem: 15572
Epoch: [35]  [1660/2809]  eta: 0:10:54  lr: 0.000002  min_lr: 0.000000  loss: 4.1836 (4.1938)  class_acc: 0.3333 (0.3214)  loss_scale: 32768.0000 (37413.9145)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1284  max mem: 15572
Epoch: [35]  [1670/2809]  eta: 0:10:48  lr: 0.000002  min_lr: 0.000000  loss: 4.2527 (4.1943)  class_acc: 0.2917 (0.3211)  loss_scale: 32768.0000 (37386.1113)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0890  max mem: 15572
Epoch: [35]  [1680/2809]  eta: 0:10:42  lr: 0.000002  min_lr: 0.000000  loss: 4.2929 (4.1951)  class_acc: 0.2500 (0.3208)  loss_scale: 32768.0000 (37358.6389)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0867  max mem: 15572
[2025-01-16 07:15:02,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=100000, skipped=622, lr=[1.7921835888244555e-08, 1.7921835888244555e-08, 2.5602622697492225e-08, 2.5602622697492225e-08, 3.6575175282131754e-08, 3.6575175282131754e-08, 5.225025040304537e-08, 5.225025040304537e-08, 7.464321486149339e-08, 7.464321486149339e-08, 1.0663316408784769e-07, 1.0663316408784769e-07, 1.5233309155406816e-07, 1.5233309155406816e-07, 2.1761870222009737e-07, 2.1761870222009737e-07, 3.1088386031442484e-07, 3.1088386031442484e-07, 4.441198004491784e-07, 4.441198004491784e-07, 6.344568577845405e-07, 6.344568577845405e-07, 9.063669396922008e-07, 9.063669396922008e-07, 1.2948099138460012e-06, 1.2948099138460012e-06, 1.8497284483514305e-06, 1.8497284483514305e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:15:02,446] [INFO] [timer.py:260:stop] epoch=0/micro_step=100000/global_step=100000, RunningAvgSamplesPerSec=27.974199163479437, CurrSamplesPerSec=26.58123488581745, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [35]  [1690/2809]  eta: 0:10:37  lr: 0.000002  min_lr: 0.000000  loss: 4.2433 (4.1951)  class_acc: 0.3333 (0.3212)  loss_scale: 32768.0000 (37331.4914)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1324  max mem: 15572
Epoch: [35]  [1700/2809]  eta: 0:10:32  lr: 0.000002  min_lr: 0.000000  loss: 4.0795 (4.1943)  class_acc: 0.3750 (0.3217)  loss_scale: 32768.0000 (37304.6631)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1806  max mem: 15572
Epoch: [35]  [1710/2809]  eta: 0:10:25  lr: 0.000002  min_lr: 0.000000  loss: 4.2047 (4.1945)  class_acc: 0.3750 (0.3221)  loss_scale: 32768.0000 (37278.1485)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1056  max mem: 15572
Epoch: [35]  [1720/2809]  eta: 0:10:19  lr: 0.000002  min_lr: 0.000000  loss: 4.2278 (4.1952)  class_acc: 0.3750 (0.3220)  loss_scale: 32768.0000 (37251.9419)  weight_decay: 0.0500 (0.0500)  time: 0.4954  data: 0.0478  max mem: 15572
Epoch: [35]  [1730/2809]  eta: 0:10:13  lr: 0.000002  min_lr: 0.000000  loss: 4.1449 (4.1945)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (37226.0381)  weight_decay: 0.0500 (0.0500)  time: 0.5174  data: 0.0704  max mem: 15572
[2025-01-16 07:15:30,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:15:30,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1740/2809]  eta: 0:10:07  lr: 0.000002  min_lr: 0.000000  loss: 4.0972 (4.1939)  class_acc: 0.3750 (0.3223)  loss_scale: 32768.0000 (37294.5388)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0716  max mem: 15572
Epoch: [35]  [1750/2809]  eta: 0:10:02  lr: 0.000002  min_lr: 0.000000  loss: 4.1361 (4.1931)  class_acc: 0.3333 (0.3226)  loss_scale: 65536.0000 (37455.8264)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1236  max mem: 15572
Epoch: [35]  [1760/2809]  eta: 0:09:56  lr: 0.000002  min_lr: 0.000000  loss: 4.1296 (4.1923)  class_acc: 0.3333 (0.3223)  loss_scale: 65536.0000 (37615.2822)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1202  max mem: 15572
[2025-01-16 07:15:44,981] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100076
[2025-01-16 07:15:44,981] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:15:44,982] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1770/2809]  eta: 0:09:50  lr: 0.000002  min_lr: 0.000000  loss: 4.1296 (4.1923)  class_acc: 0.2917 (0.3225)  loss_scale: 32768.0000 (37587.9119)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1271  max mem: 15572
Epoch: [35]  [1780/2809]  eta: 0:09:45  lr: 0.000002  min_lr: 0.000000  loss: 4.0410 (4.1916)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (37560.8490)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1789  max mem: 15572
Epoch: [35]  [1790/2809]  eta: 0:09:39  lr: 0.000002  min_lr: 0.000000  loss: 4.1532 (4.1916)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (37534.0882)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1353  max mem: 15572
Epoch: [35]  [1800/2809]  eta: 0:09:34  lr: 0.000002  min_lr: 0.000000  loss: 4.1534 (4.1913)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (37507.6247)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1149  max mem: 15572
Epoch: [35]  [1810/2809]  eta: 0:09:28  lr: 0.000002  min_lr: 0.000000  loss: 4.1858 (4.1909)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (37481.4533)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1103  max mem: 15572
Epoch: [35]  [1820/2809]  eta: 0:09:22  lr: 0.000002  min_lr: 0.000000  loss: 4.2607 (4.1915)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (37455.5695)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1107  max mem: 15572
Epoch: [35]  [1830/2809]  eta: 0:09:16  lr: 0.000002  min_lr: 0.000000  loss: 4.2607 (4.1911)  class_acc: 0.3750 (0.3233)  loss_scale: 32768.0000 (37429.9683)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.1014  max mem: 15572
Epoch: [35]  [1840/2809]  eta: 0:09:11  lr: 0.000002  min_lr: 0.000000  loss: 4.2088 (4.1909)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (37404.6453)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1218  max mem: 15572
Epoch: [35]  [1850/2809]  eta: 0:09:05  lr: 0.000002  min_lr: 0.000000  loss: 4.2127 (4.1909)  class_acc: 0.2917 (0.3229)  loss_scale: 32768.0000 (37379.5959)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1441  max mem: 15572
Epoch: [35]  [1860/2809]  eta: 0:09:00  lr: 0.000002  min_lr: 0.000000  loss: 4.2774 (4.1912)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (37354.8157)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1585  max mem: 15572
Epoch: [35]  [1870/2809]  eta: 0:08:54  lr: 0.000002  min_lr: 0.000000  loss: 4.0665 (4.1902)  class_acc: 0.2917 (0.3229)  loss_scale: 32768.0000 (37330.3004)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.2028  max mem: 15572
Epoch: [35]  [1880/2809]  eta: 0:08:48  lr: 0.000002  min_lr: 0.000000  loss: 4.1567 (4.1904)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (37306.0457)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0911  max mem: 15572
[2025-01-16 07:17:00,446] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:17:00,446] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1890/2809]  eta: 0:08:43  lr: 0.000002  min_lr: 0.000000  loss: 4.2292 (4.1907)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (37299.3760)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.1132  max mem: 15572
[2025-01-16 07:17:02,598] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100210
[2025-01-16 07:17:02,599] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:17:02,599] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1900/2809]  eta: 0:08:37  lr: 0.000002  min_lr: 0.000000  loss: 4.3653 (4.1915)  class_acc: 0.2500 (0.3225)  loss_scale: 32768.0000 (37344.4882)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1873  max mem: 15572
Epoch: [35]  [1910/2809]  eta: 0:08:31  lr: 0.000002  min_lr: 0.000000  loss: 4.1800 (4.1913)  class_acc: 0.2917 (0.3225)  loss_scale: 32768.0000 (37320.5400)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1256  max mem: 15572
Epoch: [35]  [1920/2809]  eta: 0:08:25  lr: 0.000002  min_lr: 0.000000  loss: 4.1289 (4.1909)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (37296.8412)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0759  max mem: 15572
Epoch: [35]  [1930/2809]  eta: 0:08:19  lr: 0.000002  min_lr: 0.000000  loss: 4.2107 (4.1911)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (37273.3879)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0601  max mem: 15572
Epoch: [35]  [1940/2809]  eta: 0:08:14  lr: 0.000002  min_lr: 0.000000  loss: 4.2107 (4.1910)  class_acc: 0.2500 (0.3226)  loss_scale: 32768.0000 (37250.1762)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0938  max mem: 15572
Epoch: [35]  [1950/2809]  eta: 0:08:07  lr: 0.000002  min_lr: 0.000000  loss: 4.2591 (4.1913)  class_acc: 0.2500 (0.3226)  loss_scale: 32768.0000 (37227.2025)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0664  max mem: 15572
Epoch: [35]  [1960/2809]  eta: 0:08:02  lr: 0.000002  min_lr: 0.000000  loss: 4.2698 (4.1918)  class_acc: 0.2500 (0.3222)  loss_scale: 32768.0000 (37204.4630)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1182  max mem: 15572
Epoch: [35]  [1970/2809]  eta: 0:07:56  lr: 0.000002  min_lr: 0.000000  loss: 4.3661 (4.1925)  class_acc: 0.2500 (0.3220)  loss_scale: 32768.0000 (37181.9543)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1696  max mem: 15572
Epoch: [35]  [1980/2809]  eta: 0:07:51  lr: 0.000002  min_lr: 0.000000  loss: 4.2183 (4.1916)  class_acc: 0.3333 (0.3221)  loss_scale: 32768.0000 (37159.6729)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1103  max mem: 15572
Epoch: [35]  [1990/2809]  eta: 0:07:45  lr: 0.000002  min_lr: 0.000000  loss: 4.1817 (4.1919)  class_acc: 0.3750 (0.3223)  loss_scale: 32768.0000 (37137.6153)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.1010  max mem: 15572
Epoch: [35]  [2000/2809]  eta: 0:07:39  lr: 0.000002  min_lr: 0.000000  loss: 4.2551 (4.1918)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (37115.7781)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1307  max mem: 15572
Epoch: [35]  [2010/2809]  eta: 0:07:34  lr: 0.000002  min_lr: 0.000000  loss: 4.2234 (4.1920)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37094.1581)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1977  max mem: 15572
Epoch: [35]  [2020/2809]  eta: 0:07:28  lr: 0.000002  min_lr: 0.000000  loss: 4.1313 (4.1916)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (37072.7521)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.2152  max mem: 15572
[2025-01-16 07:18:15,587] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:18:15,588] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [2030/2809]  eta: 0:07:23  lr: 0.000002  min_lr: 0.000000  loss: 4.2288 (4.1923)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (37164.4943)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1408  max mem: 15572
[2025-01-16 07:18:21,065] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100349
[2025-01-16 07:18:21,065] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:18:21,065] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [2040/2809]  eta: 0:07:17  lr: 0.000002  min_lr: 0.000000  loss: 4.2552 (4.1919)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (37191.1181)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1265  max mem: 15572
Epoch: [35]  [2050/2809]  eta: 0:07:11  lr: 0.000002  min_lr: 0.000000  loss: 4.2521 (4.1924)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (37169.5524)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1099  max mem: 15572
Epoch: [35]  [2060/2809]  eta: 0:07:06  lr: 0.000002  min_lr: 0.000000  loss: 4.2010 (4.1925)  class_acc: 0.2500 (0.3218)  loss_scale: 32768.0000 (37148.1960)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1296  max mem: 15572
Epoch: [35]  [2070/2809]  eta: 0:07:00  lr: 0.000002  min_lr: 0.000000  loss: 4.2031 (4.1928)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37127.0459)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1654  max mem: 15572
Epoch: [35]  [2080/2809]  eta: 0:06:54  lr: 0.000002  min_lr: 0.000000  loss: 4.1982 (4.1921)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (37106.0990)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1293  max mem: 15572
Epoch: [35]  [2090/2809]  eta: 0:06:48  lr: 0.000002  min_lr: 0.000000  loss: 4.1462 (4.1915)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (37085.3525)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0679  max mem: 15572
Epoch: [35]  [2100/2809]  eta: 0:06:43  lr: 0.000002  min_lr: 0.000000  loss: 4.1760 (4.1913)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (37064.8034)  weight_decay: 0.0500 (0.0500)  time: 0.5093  data: 0.0542  max mem: 15572
Epoch: [35]  [2110/2809]  eta: 0:06:37  lr: 0.000002  min_lr: 0.000000  loss: 4.1084 (4.1908)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (37044.4491)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0665  max mem: 15572
Epoch: [35]  [2120/2809]  eta: 0:06:31  lr: 0.000002  min_lr: 0.000000  loss: 4.1084 (4.1908)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (37024.2867)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0951  max mem: 15572
Epoch: [35]  [2130/2809]  eta: 0:06:25  lr: 0.000002  min_lr: 0.000000  loss: 4.1018 (4.1902)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (37004.3135)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1390  max mem: 15572
Epoch: [35]  [2140/2809]  eta: 0:06:20  lr: 0.000002  min_lr: 0.000000  loss: 4.0819 (4.1896)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (36984.5269)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0955  max mem: 15572
Epoch: [35]  [2150/2809]  eta: 0:06:14  lr: 0.000002  min_lr: 0.000000  loss: 4.2170 (4.1896)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (36964.9242)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1033  max mem: 15572
Epoch: [35]  [2160/2809]  eta: 0:06:08  lr: 0.000002  min_lr: 0.000000  loss: 4.2181 (4.1895)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (36945.5030)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1015  max mem: 15572
[2025-01-16 07:19:32,769] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:19:32,769] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [2170/2809]  eta: 0:06:02  lr: 0.000002  min_lr: 0.000000  loss: 4.2172 (4.1894)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (37047.0088)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0733  max mem: 15572
Epoch: [35]  [2180/2809]  eta: 0:05:57  lr: 0.000002  min_lr: 0.000000  loss: 4.2904 (4.1904)  class_acc: 0.2917 (0.3227)  loss_scale: 65536.0000 (37177.6323)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.1087  max mem: 15572
Epoch: [35]  [2190/2809]  eta: 0:05:51  lr: 0.000002  min_lr: 0.000000  loss: 4.3552 (4.1905)  class_acc: 0.2917 (0.3229)  loss_scale: 65536.0000 (37307.0634)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1719  max mem: 15572
[2025-01-16 07:19:52,598] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100513
[2025-01-16 07:19:52,598] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:19:52,598] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [2200/2809]  eta: 0:05:46  lr: 0.000002  min_lr: 0.000000  loss: 4.2060 (4.1904)  class_acc: 0.3750 (0.3231)  loss_scale: 65536.0000 (37390.6552)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1778  max mem: 15572
Epoch: [35]  [2210/2809]  eta: 0:05:40  lr: 0.000002  min_lr: 0.000000  loss: 4.2243 (4.1912)  class_acc: 0.3750 (0.3231)  loss_scale: 32768.0000 (37369.7476)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.1061  max mem: 15572
Epoch: [35]  [2220/2809]  eta: 0:05:34  lr: 0.000002  min_lr: 0.000000  loss: 4.2907 (4.1915)  class_acc: 0.3333 (0.3231)  loss_scale: 32768.0000 (37349.0284)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0471  max mem: 15572
Epoch: [35]  [2230/2809]  eta: 0:05:28  lr: 0.000002  min_lr: 0.000000  loss: 4.2421 (4.1912)  class_acc: 0.3750 (0.3233)  loss_scale: 32768.0000 (37328.4948)  weight_decay: 0.0500 (0.0500)  time: 0.5269  data: 0.0888  max mem: 15572
Epoch: [35]  [2240/2809]  eta: 0:05:23  lr: 0.000002  min_lr: 0.000000  loss: 4.1377 (4.1914)  class_acc: 0.3333 (0.3231)  loss_scale: 32768.0000 (37308.1446)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1434  max mem: 15572
Epoch: [35]  [2250/2809]  eta: 0:05:17  lr: 0.000002  min_lr: 0.000000  loss: 4.1678 (4.1913)  class_acc: 0.2500 (0.3230)  loss_scale: 32768.0000 (37287.9751)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1473  max mem: 15572
Epoch: [35]  [2260/2809]  eta: 0:05:11  lr: 0.000002  min_lr: 0.000000  loss: 4.1465 (4.1913)  class_acc: 0.2500 (0.3228)  loss_scale: 32768.0000 (37267.9841)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1338  max mem: 15572
Epoch: [35]  [2270/2809]  eta: 0:05:06  lr: 0.000002  min_lr: 0.000000  loss: 4.1907 (4.1912)  class_acc: 0.2500 (0.3230)  loss_scale: 32768.0000 (37248.1691)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1067  max mem: 15572
Epoch: [35]  [2280/2809]  eta: 0:05:00  lr: 0.000002  min_lr: 0.000000  loss: 4.1907 (4.1912)  class_acc: 0.3750 (0.3232)  loss_scale: 32768.0000 (37228.5278)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0913  max mem: 15572
Epoch: [35]  [2290/2809]  eta: 0:04:54  lr: 0.000002  min_lr: 0.000000  loss: 4.1770 (4.1911)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (37209.0581)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1549  max mem: 15572
Epoch: [35]  [2300/2809]  eta: 0:04:49  lr: 0.000002  min_lr: 0.000000  loss: 4.1444 (4.1915)  class_acc: 0.2917 (0.3230)  loss_scale: 32768.0000 (37189.7575)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1777  max mem: 15572
Epoch: [35]  [2310/2809]  eta: 0:04:43  lr: 0.000002  min_lr: 0.000000  loss: 4.3266 (4.1910)  class_acc: 0.2917 (0.3232)  loss_scale: 32768.0000 (37170.6240)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1397  max mem: 15572
Epoch: [35]  [2320/2809]  eta: 0:04:37  lr: 0.000002  min_lr: 0.000000  loss: 4.1928 (4.1910)  class_acc: 0.3333 (0.3233)  loss_scale: 32768.0000 (37151.6553)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0720  max mem: 15572
[2025-01-16 07:21:05,244] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:21:05,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [2330/2809]  eta: 0:04:31  lr: 0.000002  min_lr: 0.000000  loss: 4.1928 (4.1912)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (37189.0794)  weight_decay: 0.0500 (0.0500)  time: 0.4796  data: 0.0300  max mem: 15572
[2025-01-16 07:21:08,400] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100649
[2025-01-16 07:21:08,400] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:21:08,400] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [2340/2809]  eta: 0:04:26  lr: 0.000002  min_lr: 0.000000  loss: 4.2791 (4.1917)  class_acc: 0.2500 (0.3232)  loss_scale: 32768.0000 (37212.1862)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1308  max mem: 15572
Epoch: [35]  [2350/2809]  eta: 0:04:20  lr: 0.000002  min_lr: 0.000000  loss: 4.3492 (4.1918)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (37193.2829)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1245  max mem: 15572
Epoch: [35]  [2360/2809]  eta: 0:04:15  lr: 0.000002  min_lr: 0.000000  loss: 4.2431 (4.1917)  class_acc: 0.3333 (0.3233)  loss_scale: 32768.0000 (37174.5396)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1652  max mem: 15572
Epoch: [35]  [2370/2809]  eta: 0:04:09  lr: 0.000002  min_lr: 0.000000  loss: 4.2577 (4.1920)  class_acc: 0.3333 (0.3237)  loss_scale: 32768.0000 (37155.9544)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1746  max mem: 15572
Epoch: [35]  [2380/2809]  eta: 0:04:03  lr: 0.000002  min_lr: 0.000000  loss: 4.2577 (4.1918)  class_acc: 0.4167 (0.3239)  loss_scale: 32768.0000 (37137.5254)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0997  max mem: 15572
Epoch: [35]  [2390/2809]  eta: 0:03:57  lr: 0.000002  min_lr: 0.000000  loss: 4.2173 (4.1918)  class_acc: 0.3333 (0.3239)  loss_scale: 32768.0000 (37119.2505)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1240  max mem: 15572
Epoch: [35]  [2400/2809]  eta: 0:03:52  lr: 0.000002  min_lr: 0.000000  loss: 4.2650 (4.1917)  class_acc: 0.2917 (0.3239)  loss_scale: 32768.0000 (37101.1279)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1600  max mem: 15572
Epoch: [35]  [2410/2809]  eta: 0:03:46  lr: 0.000002  min_lr: 0.000000  loss: 4.0563 (4.1908)  class_acc: 0.3333 (0.3241)  loss_scale: 32768.0000 (37083.1555)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1412  max mem: 15572
Epoch: [35]  [2420/2809]  eta: 0:03:40  lr: 0.000002  min_lr: 0.000000  loss: 4.0563 (4.1909)  class_acc: 0.4167 (0.3244)  loss_scale: 32768.0000 (37065.3317)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1163  max mem: 15572
Epoch: [35]  [2430/2809]  eta: 0:03:35  lr: 0.000002  min_lr: 0.000000  loss: 4.2399 (4.1914)  class_acc: 0.4167 (0.3246)  loss_scale: 32768.0000 (37047.6545)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1082  max mem: 15572
Epoch: [35]  [2440/2809]  eta: 0:03:29  lr: 0.000002  min_lr: 0.000000  loss: 4.2399 (4.1915)  class_acc: 0.3750 (0.3247)  loss_scale: 32768.0000 (37030.1221)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.1173  max mem: 15572
Epoch: [35]  [2450/2809]  eta: 0:03:23  lr: 0.000002  min_lr: 0.000000  loss: 4.1584 (4.1915)  class_acc: 0.3750 (0.3247)  loss_scale: 32768.0000 (37012.7328)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1222  max mem: 15572
Epoch: [35]  [2460/2809]  eta: 0:03:18  lr: 0.000002  min_lr: 0.000000  loss: 4.1164 (4.1916)  class_acc: 0.2917 (0.3247)  loss_scale: 32768.0000 (36995.4848)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1167  max mem: 15572
[2025-01-16 07:22:22,847] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:22:22,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [2470/2809]  eta: 0:03:12  lr: 0.000002  min_lr: 0.000000  loss: 4.0553 (4.1912)  class_acc: 0.3750 (0.3252)  loss_scale: 32768.0000 (37084.4646)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1384  max mem: 15572
Epoch: [35]  [2480/2809]  eta: 0:03:06  lr: 0.000002  min_lr: 0.000000  loss: 4.0733 (4.1911)  class_acc: 0.3750 (0.3252)  loss_scale: 65536.0000 (37199.1423)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0977  max mem: 15572
Epoch: [35]  [2490/2809]  eta: 0:03:01  lr: 0.000002  min_lr: 0.000000  loss: 4.1958 (4.1915)  class_acc: 0.2917 (0.3253)  loss_scale: 65536.0000 (37312.8992)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1220  max mem: 15572
Epoch: [35]  [2500/2809]  eta: 0:02:55  lr: 0.000002  min_lr: 0.000000  loss: 4.2053 (4.1914)  class_acc: 0.2917 (0.3252)  loss_scale: 65536.0000 (37425.7465)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1310  max mem: 15572
Epoch: [35]  [2510/2809]  eta: 0:02:49  lr: 0.000002  min_lr: 0.000000  loss: 4.1440 (4.1913)  class_acc: 0.3333 (0.3252)  loss_scale: 65536.0000 (37537.6949)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.1037  max mem: 15572
[2025-01-16 07:22:52,035] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100831
[2025-01-16 07:22:52,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:22:52,035] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [2520/2809]  eta: 0:02:44  lr: 0.000002  min_lr: 0.000000  loss: 4.1988 (4.1919)  class_acc: 0.3333 (0.3253)  loss_scale: 65536.0000 (37583.7652)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0990  max mem: 15572
Epoch: [35]  [2530/2809]  eta: 0:02:38  lr: 0.000002  min_lr: 0.000000  loss: 4.2958 (4.1923)  class_acc: 0.2917 (0.3250)  loss_scale: 32768.0000 (37564.7380)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.0978  max mem: 15572
Epoch: [35]  [2540/2809]  eta: 0:02:32  lr: 0.000002  min_lr: 0.000000  loss: 4.2335 (4.1922)  class_acc: 0.2500 (0.3248)  loss_scale: 32768.0000 (37545.8607)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0963  max mem: 15572
Epoch: [35]  [2550/2809]  eta: 0:02:26  lr: 0.000002  min_lr: 0.000000  loss: 4.2335 (4.1926)  class_acc: 0.2500 (0.3247)  loss_scale: 32768.0000 (37527.1313)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0905  max mem: 15572
Epoch: [35]  [2560/2809]  eta: 0:02:21  lr: 0.000002  min_lr: 0.000000  loss: 4.2972 (4.1928)  class_acc: 0.2917 (0.3247)  loss_scale: 32768.0000 (37508.5482)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1101  max mem: 15572
Epoch: [35]  [2570/2809]  eta: 0:02:15  lr: 0.000002  min_lr: 0.000000  loss: 4.2112 (4.1928)  class_acc: 0.3333 (0.3248)  loss_scale: 32768.0000 (37490.1097)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1559  max mem: 15572
Epoch: [35]  [2580/2809]  eta: 0:02:10  lr: 0.000002  min_lr: 0.000000  loss: 4.1268 (4.1926)  class_acc: 0.3750 (0.3248)  loss_scale: 32768.0000 (37471.8140)  weight_decay: 0.0500 (0.0500)  time: 0.6710  data: 0.1994  max mem: 15572
Epoch: [35]  [2590/2809]  eta: 0:02:04  lr: 0.000002  min_lr: 0.000000  loss: 4.1268 (4.1920)  class_acc: 0.2917 (0.3249)  loss_scale: 32768.0000 (37453.6596)  weight_decay: 0.0500 (0.0500)  time: 0.6480  data: 0.1751  max mem: 15572
Epoch: [35]  [2600/2809]  eta: 0:01:58  lr: 0.000002  min_lr: 0.000000  loss: 4.1174 (4.1917)  class_acc: 0.3333 (0.3252)  loss_scale: 32768.0000 (37435.6448)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0684  max mem: 15572
Epoch: [35]  [2610/2809]  eta: 0:01:53  lr: 0.000002  min_lr: 0.000000  loss: 3.9529 (4.1910)  class_acc: 0.4167 (0.3255)  loss_scale: 32768.0000 (37417.7679)  weight_decay: 0.0500 (0.0500)  time: 0.5142  data: 0.0536  max mem: 15572
Epoch: [35]  [2620/2809]  eta: 0:01:47  lr: 0.000002  min_lr: 0.000000  loss: 4.0719 (4.1908)  class_acc: 0.3333 (0.3253)  loss_scale: 32768.0000 (37400.0275)  weight_decay: 0.0500 (0.0500)  time: 0.4922  data: 0.0535  max mem: 15572
Epoch: [35]  [2630/2809]  eta: 0:01:41  lr: 0.000002  min_lr: 0.000000  loss: 4.2501 (4.1912)  class_acc: 0.2917 (0.3254)  loss_scale: 32768.0000 (37382.4219)  weight_decay: 0.0500 (0.0500)  time: 0.4309  data: 0.0007  max mem: 15572
Epoch: [35]  [2640/2809]  eta: 0:01:35  lr: 0.000002  min_lr: 0.000000  loss: 4.2501 (4.1907)  class_acc: 0.3333 (0.3255)  loss_scale: 32768.0000 (37364.9496)  weight_decay: 0.0500 (0.0500)  time: 0.4571  data: 0.0008  max mem: 15572
[2025-01-16 07:24:03,269] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:24:03,270] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [2650/2809]  eta: 0:01:30  lr: 0.000002  min_lr: 0.000000  loss: 4.1994 (4.1912)  class_acc: 0.3333 (0.3255)  loss_scale: 32768.0000 (37421.7729)  weight_decay: 0.0500 (0.0500)  time: 0.4845  data: 0.0008  max mem: 15572
Epoch: [35]  [2660/2809]  eta: 0:01:24  lr: 0.000002  min_lr: 0.000000  loss: 4.1156 (4.1904)  class_acc: 0.2917 (0.3254)  loss_scale: 65536.0000 (37527.4258)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0134  max mem: 15572
Epoch: [35]  [2670/2809]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 4.1156 (4.1907)  class_acc: 0.2917 (0.3253)  loss_scale: 65536.0000 (37632.2875)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0827  max mem: 15572
Epoch: [35]  [2680/2809]  eta: 0:01:13  lr: 0.000002  min_lr: 0.000000  loss: 4.2154 (4.1903)  class_acc: 0.3333 (0.3254)  loss_scale: 65536.0000 (37736.3670)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.2049  max mem: 15572
[2025-01-16 07:24:25,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=101000, skipped=628, lr=[1.523978886112964e-08, 1.523978886112964e-08, 2.1771126944470916e-08, 2.1771126944470916e-08, 3.110160992067274e-08, 3.110160992067274e-08, 4.4430871315246775e-08, 4.4430871315246775e-08, 6.34726733074954e-08, 6.34726733074954e-08, 9.067524758213629e-08, 9.067524758213629e-08, 1.2953606797448042e-07, 1.2953606797448042e-07, 1.8505152567782917e-07, 1.8505152567782917e-07, 2.6435932239689883e-07, 2.6435932239689883e-07, 3.7765617485271267e-07, 3.7765617485271267e-07, 5.39508821218161e-07, 5.39508821218161e-07, 7.707268874545158e-07, 7.707268874545158e-07, 1.1010384106493083e-06, 1.1010384106493083e-06, 1.5729120152132976e-06, 1.5729120152132976e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:24:25,860] [INFO] [timer.py:260:stop] epoch=0/micro_step=101000/global_step=101000, RunningAvgSamplesPerSec=27.978397208291902, CurrSamplesPerSec=32.44645060474903, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-16 07:24:26,243] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101000
[2025-01-16 07:24:26,243] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:24:26,243] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [2690/2809]  eta: 0:01:07  lr: 0.000002  min_lr: 0.000000  loss: 4.1782 (4.1901)  class_acc: 0.3750 (0.3256)  loss_scale: 65536.0000 (37766.6117)  weight_decay: 0.0500 (0.0500)  time: 0.6806  data: 0.2526  max mem: 15572
Epoch: [35]  [2700/2809]  eta: 0:01:01  lr: 0.000002  min_lr: 0.000000  loss: 4.2245 (4.1908)  class_acc: 0.2917 (0.3253)  loss_scale: 32768.0000 (37748.1051)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1757  max mem: 15572
Epoch: [35]  [2710/2809]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 4.3209 (4.1907)  class_acc: 0.2917 (0.3255)  loss_scale: 32768.0000 (37729.7352)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1485  max mem: 15572
Epoch: [35]  [2720/2809]  eta: 0:00:50  lr: 0.000002  min_lr: 0.000000  loss: 4.1746 (4.1907)  class_acc: 0.3333 (0.3254)  loss_scale: 32768.0000 (37711.5002)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1247  max mem: 15572
Epoch: [35]  [2730/2809]  eta: 0:00:44  lr: 0.000002  min_lr: 0.000000  loss: 4.1272 (4.1903)  class_acc: 0.3333 (0.3256)  loss_scale: 32768.0000 (37693.3988)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1526  max mem: 15572
Epoch: [35]  [2740/2809]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000000  loss: 4.2335 (4.1905)  class_acc: 0.3333 (0.3257)  loss_scale: 32768.0000 (37675.4294)  weight_decay: 0.0500 (0.0500)  time: 0.7110  data: 0.2447  max mem: 15572
Epoch: [35]  [2750/2809]  eta: 0:00:33  lr: 0.000002  min_lr: 0.000000  loss: 4.3393 (4.1913)  class_acc: 0.2917 (0.3257)  loss_scale: 32768.0000 (37657.5907)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.1948  max mem: 15572
Epoch: [35]  [2760/2809]  eta: 0:00:27  lr: 0.000002  min_lr: 0.000000  loss: 4.1678 (4.1911)  class_acc: 0.2500 (0.3255)  loss_scale: 32768.0000 (37639.8812)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.1861  max mem: 15572
Epoch: [35]  [2770/2809]  eta: 0:00:22  lr: 0.000002  min_lr: 0.000000  loss: 4.1065 (4.1909)  class_acc: 0.2917 (0.3255)  loss_scale: 32768.0000 (37622.2995)  weight_decay: 0.0500 (0.0500)  time: 0.7050  data: 0.2420  max mem: 15572
Epoch: [35]  [2780/2809]  eta: 0:00:16  lr: 0.000002  min_lr: 0.000000  loss: 4.1576 (4.1908)  class_acc: 0.3333 (0.3255)  loss_scale: 32768.0000 (37604.8443)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1244  max mem: 15572
Epoch: [35]  [2790/2809]  eta: 0:00:10  lr: 0.000002  min_lr: 0.000000  loss: 4.1743 (4.1911)  class_acc: 0.3333 (0.3255)  loss_scale: 32768.0000 (37587.5142)  weight_decay: 0.0500 (0.0500)  time: 0.4229  data: 0.0006  max mem: 15572
Epoch: [35]  [2800/2809]  eta: 0:00:05  lr: 0.000002  min_lr: 0.000000  loss: 4.2033 (4.1913)  class_acc: 0.2917 (0.3255)  loss_scale: 32768.0000 (37570.3077)  weight_decay: 0.0500 (0.0500)  time: 0.4650  data: 0.0342  max mem: 15572
Epoch: [35]  [2808/2809]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 4.1942 (4.1908)  class_acc: 0.3750 (0.3258)  loss_scale: 32768.0000 (37556.6308)  weight_decay: 0.0500 (0.0500)  time: 0.4467  data: 0.0342  max mem: 15572
Epoch: [35] Total time: 0:26:35 (0.5681 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 4.1942 (4.1908)  class_acc: 0.3750 (0.3258)  loss_scale: 32768.0000 (37556.6308)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:18:54  loss: 1.1064 (1.1064)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 4.1719  data: 3.9972  max mem: 15572
Val:  [ 10/272]  eta: 0:03:15  loss: 2.5046 (2.4666)  acc1: 44.4444 (44.9495)  acc5: 77.7778 (76.7677)  time: 0.7473  data: 0.5699  max mem: 15572
Val:  [ 20/272]  eta: 0:02:17  loss: 2.5046 (2.5289)  acc1: 44.4444 (46.0317)  acc5: 72.2222 (75.6614)  time: 0.3639  data: 0.1736  max mem: 15572
Val:  [ 30/272]  eta: 0:01:47  loss: 2.5009 (2.5628)  acc1: 44.4444 (43.0108)  acc5: 72.2222 (74.7312)  time: 0.2789  data: 0.0855  max mem: 15572
Val:  [ 40/272]  eta: 0:01:40  loss: 2.6335 (2.6037)  acc1: 27.7778 (40.2439)  acc5: 72.2222 (73.8482)  time: 0.3168  data: 0.1293  max mem: 15572
Val:  [ 50/272]  eta: 0:01:29  loss: 2.6098 (2.5407)  acc1: 33.3333 (41.9390)  acc5: 72.2222 (75.4902)  time: 0.3431  data: 0.1560  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 1.9540 (2.4806)  acc1: 61.1111 (43.9891)  acc5: 83.3333 (76.4117)  time: 0.2596  data: 0.0634  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 1.9628 (2.4239)  acc1: 61.1111 (46.4006)  acc5: 83.3333 (77.3083)  time: 0.2553  data: 0.0599  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 2.1243 (2.4320)  acc1: 50.0000 (46.2963)  acc5: 77.7778 (77.1605)  time: 0.3546  data: 0.1697  max mem: 15572
Val:  [ 90/272]  eta: 0:01:06  loss: 2.4657 (2.4343)  acc1: 50.0000 (46.3980)  acc5: 77.7778 (77.5336)  time: 0.3826  data: 0.1833  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.4657 (2.4584)  acc1: 44.4444 (45.5996)  acc5: 77.7778 (77.2827)  time: 0.2846  data: 0.0628  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.5881 (2.5109)  acc1: 27.7778 (43.8939)  acc5: 77.7778 (76.2763)  time: 0.2428  data: 0.0346  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 2.9736 (2.5438)  acc1: 27.7778 (43.2048)  acc5: 72.2222 (75.8035)  time: 0.3125  data: 0.1256  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 2.4745 (2.5192)  acc1: 44.4444 (43.8083)  acc5: 77.7778 (76.5055)  time: 0.3478  data: 0.1472  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 2.1796 (2.5226)  acc1: 50.0000 (43.9716)  acc5: 83.3333 (76.2805)  time: 0.2950  data: 0.1027  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.5430 (2.5194)  acc1: 38.8889 (43.7822)  acc5: 77.7778 (76.4165)  time: 0.3040  data: 0.1274  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.4651 (2.5169)  acc1: 44.4444 (44.2374)  acc5: 77.7778 (76.6046)  time: 0.2925  data: 0.1114  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.5752 (2.5322)  acc1: 38.8889 (43.7947)  acc5: 72.2222 (76.0234)  time: 0.2893  data: 0.1038  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.4987 (2.5202)  acc1: 38.8889 (43.8306)  acc5: 72.2222 (76.3659)  time: 0.3480  data: 0.1572  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.4387 (2.5551)  acc1: 38.8889 (42.8738)  acc5: 72.2222 (75.3345)  time: 0.3406  data: 0.1480  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.6518 (2.5595)  acc1: 38.8889 (42.4544)  acc5: 72.2222 (75.2073)  time: 0.3224  data: 0.1280  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.4364 (2.5663)  acc1: 38.8889 (42.2064)  acc5: 83.3333 (75.0922)  time: 0.3012  data: 0.1074  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.7018 (2.5609)  acc1: 38.8889 (42.2071)  acc5: 77.7778 (75.1885)  time: 0.3054  data: 0.1161  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.1490 (2.5469)  acc1: 50.0000 (43.0255)  acc5: 83.3333 (75.5171)  time: 0.3205  data: 0.1288  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1026 (2.5340)  acc1: 55.5556 (43.2227)  acc5: 83.3333 (75.9336)  time: 0.3251  data: 0.1311  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.3805 (2.5387)  acc1: 38.8889 (42.8951)  acc5: 83.3333 (75.9628)  time: 0.2935  data: 0.1020  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9448 (2.4989)  acc1: 61.1111 (44.4232)  acc5: 88.8889 (76.5645)  time: 0.2588  data: 0.0742  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.8606 (2.4934)  acc1: 61.1111 (44.6084)  acc5: 83.3333 (76.6913)  time: 0.2129  data: 0.0478  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8606 (2.4965)  acc1: 61.1111 (44.6037)  acc5: 83.3333 (76.6742)  time: 0.2067  data: 0.0477  max mem: 15572
Val: Total time: 0:01:26 (0.3196 s / it)
* Acc@1 44.604 Acc@5 76.674 loss 2.497
Accuracy of the network on the 4883 val videos: 44.6%
Max accuracy: 45.05%
Epoch: [36]  [   0/2809]  eta: 4:45:33  lr: 0.000002  min_lr: 0.000000  loss: 4.4246 (4.4246)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.0997  data: 5.6609  max mem: 15572
[2025-01-16 07:27:15,680] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:27:15,681] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [  10/2809]  eta: 0:50:59  lr: 0.000002  min_lr: 0.000000  loss: 4.1742 (4.1868)  class_acc: 0.3333 (0.3144)  loss_scale: 65536.0000 (50641.4545)  weight_decay: 0.0500 (0.0500)  time: 1.0930  data: 0.6536  max mem: 15572
Epoch: [36]  [  20/2809]  eta: 0:42:07  lr: 0.000002  min_lr: 0.000000  loss: 4.2798 (4.2820)  class_acc: 0.3333 (0.2976)  loss_scale: 65536.0000 (57734.0952)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.2150  max mem: 15572
Epoch: [36]  [  30/2809]  eta: 0:36:44  lr: 0.000002  min_lr: 0.000000  loss: 4.3932 (4.2877)  class_acc: 0.2500 (0.3065)  loss_scale: 65536.0000 (60250.8387)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1985  max mem: 15572
[2025-01-16 07:27:34,762] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101164
[2025-01-16 07:27:34,762] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:27:34,763] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [  40/2809]  eta: 0:32:34  lr: 0.000002  min_lr: 0.000000  loss: 4.3119 (4.2674)  class_acc: 0.2500 (0.3150)  loss_scale: 65536.0000 (60740.6829)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0602  max mem: 15572
Epoch: [36]  [  50/2809]  eta: 0:30:52  lr: 0.000002  min_lr: 0.000000  loss: 4.1937 (4.2723)  class_acc: 0.2917 (0.3145)  loss_scale: 32768.0000 (55255.8431)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0475  max mem: 15572
Epoch: [36]  [  60/2809]  eta: 0:30:56  lr: 0.000002  min_lr: 0.000000  loss: 4.2236 (4.2679)  class_acc: 0.2500 (0.3108)  loss_scale: 32768.0000 (51569.3115)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1747  max mem: 15572
Epoch: [36]  [  70/2809]  eta: 0:30:06  lr: 0.000002  min_lr: 0.000000  loss: 4.2525 (4.2660)  class_acc: 0.2917 (0.3187)  loss_scale: 32768.0000 (48921.2394)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1985  max mem: 15572
Epoch: [36]  [  80/2809]  eta: 0:29:13  lr: 0.000002  min_lr: 0.000000  loss: 4.2306 (4.2504)  class_acc: 0.3750 (0.3272)  loss_scale: 32768.0000 (46927.0123)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.1093  max mem: 15572
Epoch: [36]  [  90/2809]  eta: 0:28:26  lr: 0.000002  min_lr: 0.000000  loss: 4.1580 (4.2453)  class_acc: 0.3333 (0.3246)  loss_scale: 32768.0000 (45371.0769)  weight_decay: 0.0500 (0.0500)  time: 0.5132  data: 0.0768  max mem: 15572
Epoch: [36]  [ 100/2809]  eta: 0:27:44  lr: 0.000002  min_lr: 0.000000  loss: 4.1293 (4.2305)  class_acc: 0.2917 (0.3284)  loss_scale: 32768.0000 (44123.2475)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0681  max mem: 15572
Epoch: [36]  [ 110/2809]  eta: 0:27:31  lr: 0.000002  min_lr: 0.000000  loss: 4.1195 (4.2176)  class_acc: 0.3750 (0.3285)  loss_scale: 32768.0000 (43100.2523)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.1047  max mem: 15572
Epoch: [36]  [ 120/2809]  eta: 0:27:22  lr: 0.000002  min_lr: 0.000000  loss: 4.1928 (4.2167)  class_acc: 0.2917 (0.3268)  loss_scale: 32768.0000 (42246.3471)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1613  max mem: 15572
Epoch: [36]  [ 130/2809]  eta: 0:27:07  lr: 0.000002  min_lr: 0.000000  loss: 4.1928 (4.2194)  class_acc: 0.2917 (0.3238)  loss_scale: 32768.0000 (41522.8092)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1429  max mem: 15572
Epoch: [36]  [ 140/2809]  eta: 0:27:04  lr: 0.000002  min_lr: 0.000000  loss: 4.2264 (4.2134)  class_acc: 0.2917 (0.3230)  loss_scale: 32768.0000 (40901.9007)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1355  max mem: 15572
Epoch: [36]  [ 150/2809]  eta: 0:26:51  lr: 0.000002  min_lr: 0.000000  loss: 4.1736 (4.2050)  class_acc: 0.2500 (0.3195)  loss_scale: 32768.0000 (40363.2318)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1541  max mem: 15572
Epoch: [36]  [ 160/2809]  eta: 0:26:46  lr: 0.000001  min_lr: 0.000000  loss: 4.1357 (4.2003)  class_acc: 0.2917 (0.3253)  loss_scale: 32768.0000 (39891.4783)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1607  max mem: 15572
[2025-01-16 07:28:49,056] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:28:49,056] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 170/2809]  eta: 0:26:40  lr: 0.000001  min_lr: 0.000000  loss: 4.2981 (4.2073)  class_acc: 0.2917 (0.3199)  loss_scale: 32768.0000 (39858.1520)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1619  max mem: 15572
Epoch: [36]  [ 180/2809]  eta: 0:26:24  lr: 0.000001  min_lr: 0.000000  loss: 4.2944 (4.2041)  class_acc: 0.2500 (0.3211)  loss_scale: 65536.0000 (41276.8177)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1307  max mem: 15572
Epoch: [36]  [ 190/2809]  eta: 0:26:13  lr: 0.000001  min_lr: 0.000000  loss: 4.2258 (4.2113)  class_acc: 0.2500 (0.3135)  loss_scale: 65536.0000 (42546.9319)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1268  max mem: 15572
[2025-01-16 07:29:02,413] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101318
[2025-01-16 07:29:02,414] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:29:02,414] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 200/2809]  eta: 0:26:05  lr: 0.000001  min_lr: 0.000000  loss: 4.2705 (4.2093)  class_acc: 0.2917 (0.3141)  loss_scale: 65536.0000 (42549.4925)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1421  max mem: 15572
Epoch: [36]  [ 210/2809]  eta: 0:25:49  lr: 0.000001  min_lr: 0.000000  loss: 4.2705 (4.2125)  class_acc: 0.3333 (0.3146)  loss_scale: 32768.0000 (42085.9147)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0986  max mem: 15572
Epoch: [36]  [ 220/2809]  eta: 0:25:43  lr: 0.000001  min_lr: 0.000000  loss: 4.1910 (4.2078)  class_acc: 0.2917 (0.3156)  loss_scale: 32768.0000 (41664.2896)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0891  max mem: 15572
Epoch: [36]  [ 230/2809]  eta: 0:25:35  lr: 0.000001  min_lr: 0.000000  loss: 4.1849 (4.2115)  class_acc: 0.3750 (0.3167)  loss_scale: 32768.0000 (41279.1688)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1275  max mem: 15572
Epoch: [36]  [ 240/2809]  eta: 0:25:28  lr: 0.000001  min_lr: 0.000000  loss: 4.2309 (4.2113)  class_acc: 0.3333 (0.3157)  loss_scale: 32768.0000 (40926.0083)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1448  max mem: 15572
Epoch: [36]  [ 250/2809]  eta: 0:25:08  lr: 0.000001  min_lr: 0.000000  loss: 4.1067 (4.2025)  class_acc: 0.3333 (0.3149)  loss_scale: 32768.0000 (40600.9880)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0820  max mem: 15572
Epoch: [36]  [ 260/2809]  eta: 0:24:59  lr: 0.000001  min_lr: 0.000000  loss: 4.1184 (4.2078)  class_acc: 0.3333 (0.3145)  loss_scale: 32768.0000 (40300.8736)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0636  max mem: 15572
Epoch: [36]  [ 270/2809]  eta: 0:24:56  lr: 0.000001  min_lr: 0.000000  loss: 4.3638 (4.2128)  class_acc: 0.3333 (0.3157)  loss_scale: 32768.0000 (40022.9077)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1481  max mem: 15572
Epoch: [36]  [ 280/2809]  eta: 0:24:51  lr: 0.000001  min_lr: 0.000000  loss: 4.2799 (4.2153)  class_acc: 0.3333 (0.3160)  loss_scale: 32768.0000 (39764.7260)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1632  max mem: 15572
Epoch: [36]  [ 290/2809]  eta: 0:24:45  lr: 0.000001  min_lr: 0.000000  loss: 4.2799 (4.2179)  class_acc: 0.3333 (0.3150)  loss_scale: 32768.0000 (39524.2887)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1505  max mem: 15572
Epoch: [36]  [ 300/2809]  eta: 0:24:34  lr: 0.000001  min_lr: 0.000000  loss: 4.2603 (4.2165)  class_acc: 0.2917 (0.3142)  loss_scale: 32768.0000 (39299.8272)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1130  max mem: 15572
Epoch: [36]  [ 310/2809]  eta: 0:24:26  lr: 0.000001  min_lr: 0.000000  loss: 4.0644 (4.2114)  class_acc: 0.3750 (0.3169)  loss_scale: 32768.0000 (39089.8006)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1049  max mem: 15572
Epoch: [36]  [ 320/2809]  eta: 0:24:21  lr: 0.000001  min_lr: 0.000000  loss: 4.0500 (4.2075)  class_acc: 0.3750 (0.3158)  loss_scale: 32768.0000 (38892.8598)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1414  max mem: 15572
[2025-01-16 07:30:15,633] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:30:15,634] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 07:30:16,979] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101449
[2025-01-16 07:30:16,979] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:30:16,979] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 330/2809]  eta: 0:24:12  lr: 0.000001  min_lr: 0.000000  loss: 4.1744 (4.2142)  class_acc: 0.2500 (0.3148)  loss_scale: 32768.0000 (38905.8127)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1196  max mem: 15572
Epoch: [36]  [ 340/2809]  eta: 0:24:10  lr: 0.000001  min_lr: 0.000000  loss: 4.3515 (4.2138)  class_acc: 0.2500 (0.3130)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1471  max mem: 15572
Epoch: [36]  [ 350/2809]  eta: 0:24:03  lr: 0.000001  min_lr: 0.000000  loss: 4.1130 (4.2118)  class_acc: 0.2917 (0.3137)  loss_scale: 32768.0000 (38556.0798)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1614  max mem: 15572
Epoch: [36]  [ 360/2809]  eta: 0:23:53  lr: 0.000001  min_lr: 0.000000  loss: 4.2164 (4.2178)  class_acc: 0.2917 (0.3127)  loss_scale: 32768.0000 (38395.7452)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1063  max mem: 15572
Epoch: [36]  [ 370/2809]  eta: 0:23:45  lr: 0.000001  min_lr: 0.000000  loss: 4.3344 (4.2190)  class_acc: 0.2500 (0.3133)  loss_scale: 32768.0000 (38244.0539)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0879  max mem: 15572
Epoch: [36]  [ 380/2809]  eta: 0:23:37  lr: 0.000001  min_lr: 0.000000  loss: 4.3871 (4.2247)  class_acc: 0.2500 (0.3117)  loss_scale: 32768.0000 (38100.3255)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1052  max mem: 15572
Epoch: [36]  [ 390/2809]  eta: 0:23:33  lr: 0.000001  min_lr: 0.000000  loss: 4.4408 (4.2264)  class_acc: 0.2917 (0.3111)  loss_scale: 32768.0000 (37963.9488)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1421  max mem: 15572
Epoch: [36]  [ 400/2809]  eta: 0:23:25  lr: 0.000001  min_lr: 0.000000  loss: 4.2992 (4.2281)  class_acc: 0.3333 (0.3123)  loss_scale: 32768.0000 (37834.3741)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1198  max mem: 15572
Epoch: [36]  [ 410/2809]  eta: 0:23:19  lr: 0.000001  min_lr: 0.000000  loss: 4.1494 (4.2262)  class_acc: 0.3333 (0.3114)  loss_scale: 32768.0000 (37711.1046)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1038  max mem: 15572
Epoch: [36]  [ 420/2809]  eta: 0:23:10  lr: 0.000001  min_lr: 0.000000  loss: 4.2727 (4.2265)  class_acc: 0.2500 (0.3104)  loss_scale: 32768.0000 (37593.6912)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0871  max mem: 15572
Epoch: [36]  [ 430/2809]  eta: 0:23:00  lr: 0.000001  min_lr: 0.000000  loss: 4.2009 (4.2266)  class_acc: 0.2917 (0.3110)  loss_scale: 32768.0000 (37481.7262)  weight_decay: 0.0500 (0.0500)  time: 0.5116  data: 0.0515  max mem: 15572
Epoch: [36]  [ 440/2809]  eta: 0:22:56  lr: 0.000001  min_lr: 0.000000  loss: 4.2009 (4.2252)  class_acc: 0.3333 (0.3126)  loss_scale: 32768.0000 (37374.8390)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1092  max mem: 15572
Epoch: [36]  [ 450/2809]  eta: 0:22:47  lr: 0.000001  min_lr: 0.000000  loss: 4.1958 (4.2209)  class_acc: 0.3333 (0.3136)  loss_scale: 32768.0000 (37272.6918)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1187  max mem: 15572
[2025-01-16 07:31:29,222] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:31:29,222] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 07:31:33,088] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101584
[2025-01-16 07:31:33,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:31:33,088] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 460/2809]  eta: 0:22:41  lr: 0.000001  min_lr: 0.000000  loss: 4.1958 (4.2245)  class_acc: 0.3333 (0.3145)  loss_scale: 32768.0000 (37601.4577)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0999  max mem: 15572
Epoch: [36]  [ 470/2809]  eta: 0:22:39  lr: 0.000001  min_lr: 0.000000  loss: 4.3002 (4.2261)  class_acc: 0.2917 (0.3142)  loss_scale: 32768.0000 (37498.8365)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1794  max mem: 15572
Epoch: [36]  [ 480/2809]  eta: 0:22:28  lr: 0.000001  min_lr: 0.000000  loss: 4.1829 (4.2246)  class_acc: 0.3333 (0.3147)  loss_scale: 32768.0000 (37400.4823)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1170  max mem: 15572
Epoch: [36]  [ 490/2809]  eta: 0:22:16  lr: 0.000001  min_lr: 0.000000  loss: 4.1096 (4.2240)  class_acc: 0.3333 (0.3143)  loss_scale: 32768.0000 (37306.1344)  weight_decay: 0.0500 (0.0500)  time: 0.4527  data: 0.0068  max mem: 15572
Epoch: [36]  [ 500/2809]  eta: 0:22:13  lr: 0.000001  min_lr: 0.000000  loss: 4.2114 (4.2222)  class_acc: 0.3333 (0.3170)  loss_scale: 32768.0000 (37215.5529)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1052  max mem: 15572
Epoch: [36]  [ 510/2809]  eta: 0:22:08  lr: 0.000001  min_lr: 0.000000  loss: 4.2114 (4.2241)  class_acc: 0.3333 (0.3161)  loss_scale: 32768.0000 (37128.5166)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1866  max mem: 15572
Epoch: [36]  [ 520/2809]  eta: 0:21:59  lr: 0.000001  min_lr: 0.000000  loss: 4.2332 (4.2241)  class_acc: 0.3333 (0.3167)  loss_scale: 32768.0000 (37044.8215)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1286  max mem: 15572
Epoch: [36]  [ 530/2809]  eta: 0:21:56  lr: 0.000001  min_lr: 0.000000  loss: 4.3002 (4.2265)  class_acc: 0.3333 (0.3158)  loss_scale: 32768.0000 (36964.2787)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1285  max mem: 15572
Epoch: [36]  [ 540/2809]  eta: 0:21:46  lr: 0.000001  min_lr: 0.000000  loss: 4.2960 (4.2226)  class_acc: 0.2917 (0.3162)  loss_scale: 32768.0000 (36886.7135)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1083  max mem: 15572
Epoch: [36]  [ 550/2809]  eta: 0:21:41  lr: 0.000001  min_lr: 0.000000  loss: 4.0636 (4.2201)  class_acc: 0.3333 (0.3168)  loss_scale: 32768.0000 (36811.9637)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0740  max mem: 15572
Epoch: [36]  [ 560/2809]  eta: 0:21:31  lr: 0.000001  min_lr: 0.000000  loss: 4.2961 (4.2196)  class_acc: 0.2917 (0.3168)  loss_scale: 32768.0000 (36739.8788)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0685  max mem: 15572
Epoch: [36]  [ 570/2809]  eta: 0:21:26  lr: 0.000001  min_lr: 0.000000  loss: 4.2961 (4.2214)  class_acc: 0.2917 (0.3165)  loss_scale: 32768.0000 (36670.3187)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0799  max mem: 15572
Epoch: [36]  [ 580/2809]  eta: 0:21:18  lr: 0.000001  min_lr: 0.000000  loss: 4.1978 (4.2207)  class_acc: 0.2917 (0.3162)  loss_scale: 32768.0000 (36603.1532)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0959  max mem: 15572
[2025-01-16 07:32:44,380] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:32:44,381] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 590/2809]  eta: 0:21:12  lr: 0.000001  min_lr: 0.000000  loss: 4.1454 (4.2191)  class_acc: 0.2917 (0.3158)  loss_scale: 32768.0000 (36649.1506)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0746  max mem: 15572
Epoch: [36]  [ 600/2809]  eta: 0:21:05  lr: 0.000001  min_lr: 0.000000  loss: 4.2105 (4.2197)  class_acc: 0.2500 (0.3151)  loss_scale: 65536.0000 (37129.7970)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0755  max mem: 15572
Epoch: [36]  [ 610/2809]  eta: 0:21:02  lr: 0.000001  min_lr: 0.000000  loss: 4.2449 (4.2195)  class_acc: 0.2500 (0.3153)  loss_scale: 65536.0000 (37594.7103)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1130  max mem: 15572
Epoch: [36]  [ 620/2809]  eta: 0:20:55  lr: 0.000001  min_lr: 0.000000  loss: 4.2380 (4.2177)  class_acc: 0.2917 (0.3152)  loss_scale: 65536.0000 (38044.6506)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1293  max mem: 15572
Epoch: [36]  [ 630/2809]  eta: 0:20:49  lr: 0.000001  min_lr: 0.000000  loss: 4.2570 (4.2177)  class_acc: 0.2917 (0.3160)  loss_scale: 65536.0000 (38480.3296)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1016  max mem: 15572
Epoch: [36]  [ 640/2809]  eta: 0:20:44  lr: 0.000001  min_lr: 0.000000  loss: 4.1138 (4.2149)  class_acc: 0.3333 (0.3160)  loss_scale: 65536.0000 (38902.4150)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1121  max mem: 15572
[2025-01-16 07:33:16,318] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101768
[2025-01-16 07:33:16,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:33:16,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 650/2809]  eta: 0:20:38  lr: 0.000001  min_lr: 0.000000  loss: 4.1249 (4.2156)  class_acc: 0.2500 (0.3150)  loss_scale: 65536.0000 (38959.1889)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1236  max mem: 15572
Epoch: [36]  [ 660/2809]  eta: 0:20:33  lr: 0.000001  min_lr: 0.000000  loss: 4.2556 (4.2147)  class_acc: 0.2917 (0.3149)  loss_scale: 32768.0000 (38865.5250)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1371  max mem: 15572
Epoch: [36]  [ 670/2809]  eta: 0:20:26  lr: 0.000001  min_lr: 0.000000  loss: 4.1750 (4.2139)  class_acc: 0.2917 (0.3143)  loss_scale: 32768.0000 (38774.6528)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1109  max mem: 15572
Epoch: [36]  [ 680/2809]  eta: 0:20:20  lr: 0.000001  min_lr: 0.000000  loss: 4.1140 (4.2132)  class_acc: 0.2917 (0.3153)  loss_scale: 32768.0000 (38686.4493)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0950  max mem: 15572
Epoch: [36]  [ 690/2809]  eta: 0:20:15  lr: 0.000001  min_lr: 0.000000  loss: 4.1521 (4.2135)  class_acc: 0.3333 (0.3158)  loss_scale: 32768.0000 (38600.7988)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1357  max mem: 15572
Epoch: [36]  [ 700/2809]  eta: 0:20:09  lr: 0.000001  min_lr: 0.000000  loss: 4.2623 (4.2116)  class_acc: 0.3333 (0.3164)  loss_scale: 32768.0000 (38517.5920)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1208  max mem: 15572
Epoch: [36]  [ 710/2809]  eta: 0:20:03  lr: 0.000001  min_lr: 0.000000  loss: 3.9907 (4.2073)  class_acc: 0.3333 (0.3166)  loss_scale: 32768.0000 (38436.7257)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1036  max mem: 15572
Epoch: [36]  [ 720/2809]  eta: 0:19:58  lr: 0.000001  min_lr: 0.000000  loss: 3.9907 (4.2059)  class_acc: 0.3750 (0.3176)  loss_scale: 32768.0000 (38358.1026)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1236  max mem: 15572
Epoch: [36]  [ 730/2809]  eta: 0:19:53  lr: 0.000001  min_lr: 0.000000  loss: 4.1322 (4.2045)  class_acc: 0.3750 (0.3171)  loss_scale: 32768.0000 (38281.6306)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1356  max mem: 15572
Epoch: [36]  [ 740/2809]  eta: 0:19:48  lr: 0.000001  min_lr: 0.000000  loss: 4.0053 (4.2020)  class_acc: 0.3750 (0.3178)  loss_scale: 32768.0000 (38207.2227)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1448  max mem: 15572
Epoch: [36]  [ 750/2809]  eta: 0:19:41  lr: 0.000001  min_lr: 0.000000  loss: 4.0887 (4.2027)  class_acc: 0.3333 (0.3174)  loss_scale: 32768.0000 (38134.7963)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1177  max mem: 15572
Epoch: [36]  [ 760/2809]  eta: 0:19:35  lr: 0.000001  min_lr: 0.000000  loss: 4.2583 (4.2029)  class_acc: 0.2917 (0.3169)  loss_scale: 32768.0000 (38064.2733)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0990  max mem: 15572
Epoch: [36]  [ 770/2809]  eta: 0:19:27  lr: 0.000001  min_lr: 0.000000  loss: 4.2926 (4.2040)  class_acc: 0.3333 (0.3174)  loss_scale: 32768.0000 (37995.5798)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0824  max mem: 15572
[2025-01-16 07:34:29,788] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:34:29,789] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 07:34:31,929] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101900
[2025-01-16 07:34:31,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:34:31,929] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 780/2809]  eta: 0:19:23  lr: 0.000001  min_lr: 0.000000  loss: 4.1824 (4.2036)  class_acc: 0.3333 (0.3172)  loss_scale: 32768.0000 (38054.5147)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1055  max mem: 15572
Epoch: [36]  [ 790/2809]  eta: 0:19:19  lr: 0.000001  min_lr: 0.000000  loss: 4.2042 (4.2068)  class_acc: 0.2917 (0.3168)  loss_scale: 32768.0000 (37987.6814)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1778  max mem: 15572
Epoch: [36]  [ 800/2809]  eta: 0:19:15  lr: 0.000001  min_lr: 0.000000  loss: 4.1858 (4.2056)  class_acc: 0.2917 (0.3174)  loss_scale: 32768.0000 (37922.5169)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1844  max mem: 15572
Epoch: [36]  [ 810/2809]  eta: 0:19:06  lr: 0.000001  min_lr: 0.000000  loss: 4.1709 (4.2057)  class_acc: 0.2917 (0.3166)  loss_scale: 32768.0000 (37858.9593)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.0957  max mem: 15572
Epoch: [36]  [ 820/2809]  eta: 0:19:00  lr: 0.000001  min_lr: 0.000000  loss: 4.2118 (4.2059)  class_acc: 0.2500 (0.3168)  loss_scale: 32768.0000 (37796.9501)  weight_decay: 0.0500 (0.0500)  time: 0.5141  data: 0.0752  max mem: 15572
Epoch: [36]  [ 830/2809]  eta: 0:18:54  lr: 0.000001  min_lr: 0.000000  loss: 4.1554 (4.2037)  class_acc: 0.2917 (0.3169)  loss_scale: 32768.0000 (37736.4332)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1044  max mem: 15572
Epoch: [36]  [ 840/2809]  eta: 0:18:46  lr: 0.000001  min_lr: 0.000000  loss: 4.1173 (4.2031)  class_acc: 0.2917 (0.3163)  loss_scale: 32768.0000 (37677.3555)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.0499  max mem: 15572
Epoch: [36]  [ 850/2809]  eta: 0:18:40  lr: 0.000001  min_lr: 0.000000  loss: 4.1424 (4.2040)  class_acc: 0.2917 (0.3162)  loss_scale: 32768.0000 (37619.6663)  weight_decay: 0.0500 (0.0500)  time: 0.5186  data: 0.0582  max mem: 15572
Epoch: [36]  [ 860/2809]  eta: 0:18:34  lr: 0.000001  min_lr: 0.000000  loss: 4.2699 (4.2055)  class_acc: 0.2500 (0.3154)  loss_scale: 32768.0000 (37563.3171)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1006  max mem: 15572
Epoch: [36]  [ 870/2809]  eta: 0:18:29  lr: 0.000001  min_lr: 0.000000  loss: 4.2106 (4.2054)  class_acc: 0.2500 (0.3152)  loss_scale: 32768.0000 (37508.2618)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1188  max mem: 15572
[2025-01-16 07:35:26,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=102000, skipped=635, lr=[1.2774264980564707e-08, 1.2774264980564707e-08, 1.82489499722353e-08, 1.82489499722353e-08, 2.6069928531764718e-08, 2.6069928531764718e-08, 3.724275504537817e-08, 3.724275504537817e-08, 5.320393577911167e-08, 5.320393577911167e-08, 7.60056225415881e-08, 7.60056225415881e-08, 1.085794607736973e-07, 1.085794607736973e-07, 1.5511351539099616e-07, 1.5511351539099616e-07, 2.2159073627285164e-07, 2.2159073627285164e-07, 3.165581946755024e-07, 3.165581946755024e-07, 4.5222599239357485e-07, 4.5222599239357485e-07, 6.460371319908213e-07, 6.460371319908213e-07, 9.229101885583161e-07, 9.229101885583161e-07, 1.3184431265118803e-06, 1.3184431265118803e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:35:26,598] [INFO] [timer.py:260:stop] epoch=0/micro_step=102000/global_step=102000, RunningAvgSamplesPerSec=27.98169915315894, CurrSamplesPerSec=31.35474996760593, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [36]  [ 880/2809]  eta: 0:18:22  lr: 0.000001  min_lr: 0.000000  loss: 4.1998 (4.2048)  class_acc: 0.2500 (0.3146)  loss_scale: 32768.0000 (37454.4563)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1003  max mem: 15572
Epoch: [36]  [ 890/2809]  eta: 0:18:14  lr: 0.000001  min_lr: 0.000000  loss: 4.1810 (4.2049)  class_acc: 0.2500 (0.3145)  loss_scale: 32768.0000 (37401.8586)  weight_decay: 0.0500 (0.0500)  time: 0.4868  data: 0.0335  max mem: 15572
Epoch: [36]  [ 900/2809]  eta: 0:18:07  lr: 0.000001  min_lr: 0.000000  loss: 4.0760 (4.2041)  class_acc: 0.2917 (0.3145)  loss_scale: 32768.0000 (37350.4284)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0465  max mem: 15572
[2025-01-16 07:35:42,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:35:42,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 910/2809]  eta: 0:18:02  lr: 0.000001  min_lr: 0.000000  loss: 4.0969 (4.2034)  class_acc: 0.2500 (0.3142)  loss_scale: 32768.0000 (37515.9429)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1294  max mem: 15572
Epoch: [36]  [ 920/2809]  eta: 0:17:56  lr: 0.000001  min_lr: 0.000000  loss: 4.0969 (4.2022)  class_acc: 0.2500 (0.3146)  loss_scale: 65536.0000 (37820.1781)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1606  max mem: 15572
Epoch: [36]  [ 930/2809]  eta: 0:17:50  lr: 0.000001  min_lr: 0.000000  loss: 4.2638 (4.2029)  class_acc: 0.2500 (0.3140)  loss_scale: 65536.0000 (38117.8776)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1206  max mem: 15572
Epoch: [36]  [ 940/2809]  eta: 0:17:46  lr: 0.000001  min_lr: 0.000000  loss: 4.2162 (4.2023)  class_acc: 0.2500 (0.3138)  loss_scale: 65536.0000 (38409.2497)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1491  max mem: 15572
[2025-01-16 07:36:03,286] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102065
[2025-01-16 07:36:03,287] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:36:03,287] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 950/2809]  eta: 0:17:40  lr: 0.000001  min_lr: 0.000000  loss: 4.1136 (4.2003)  class_acc: 0.3333 (0.3148)  loss_scale: 32768.0000 (38349.9306)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1676  max mem: 15572
Epoch: [36]  [ 960/2809]  eta: 0:17:36  lr: 0.000001  min_lr: 0.000000  loss: 4.2157 (4.2011)  class_acc: 0.3333 (0.3148)  loss_scale: 32768.0000 (38291.8460)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1664  max mem: 15572
Epoch: [36]  [ 970/2809]  eta: 0:17:31  lr: 0.000001  min_lr: 0.000000  loss: 4.2983 (4.2026)  class_acc: 0.2917 (0.3148)  loss_scale: 32768.0000 (38234.9578)  weight_decay: 0.0500 (0.0500)  time: 0.6392  data: 0.1922  max mem: 15572
Epoch: [36]  [ 980/2809]  eta: 0:17:26  lr: 0.000001  min_lr: 0.000000  loss: 4.1363 (4.2003)  class_acc: 0.3333 (0.3159)  loss_scale: 32768.0000 (38179.2294)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1672  max mem: 15572
Epoch: [36]  [ 990/2809]  eta: 0:17:21  lr: 0.000001  min_lr: 0.000000  loss: 3.9874 (4.1984)  class_acc: 0.3333 (0.3161)  loss_scale: 32768.0000 (38124.6256)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1478  max mem: 15572
Epoch: [36]  [1000/2809]  eta: 0:17:15  lr: 0.000001  min_lr: 0.000000  loss: 4.1520 (4.1994)  class_acc: 0.2917 (0.3158)  loss_scale: 32768.0000 (38071.1129)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1209  max mem: 15572
Epoch: [36]  [1010/2809]  eta: 0:17:08  lr: 0.000001  min_lr: 0.000000  loss: 4.2852 (4.1996)  class_acc: 0.2917 (0.3162)  loss_scale: 32768.0000 (38018.6588)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0781  max mem: 15572
Epoch: [36]  [1020/2809]  eta: 0:17:03  lr: 0.000001  min_lr: 0.000000  loss: 4.3034 (4.2009)  class_acc: 0.2500 (0.3153)  loss_scale: 32768.0000 (37967.2321)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1172  max mem: 15572
Epoch: [36]  [1030/2809]  eta: 0:16:57  lr: 0.000001  min_lr: 0.000000  loss: 4.2398 (4.2000)  class_acc: 0.2500 (0.3154)  loss_scale: 32768.0000 (37916.8031)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1498  max mem: 15572
Epoch: [36]  [1040/2809]  eta: 0:16:50  lr: 0.000001  min_lr: 0.000000  loss: 4.2219 (4.1992)  class_acc: 0.3333 (0.3156)  loss_scale: 32768.0000 (37867.3429)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0737  max mem: 15572
Epoch: [36]  [1050/2809]  eta: 0:16:45  lr: 0.000001  min_lr: 0.000000  loss: 4.2577 (4.1997)  class_acc: 0.2917 (0.3150)  loss_scale: 32768.0000 (37818.8240)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0916  max mem: 15572
Epoch: [36]  [1060/2809]  eta: 0:16:39  lr: 0.000001  min_lr: 0.000000  loss: 4.2024 (4.1989)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (37771.2196)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1242  max mem: 15572
[2025-01-16 07:37:18,281] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:37:18,281] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1070/2809]  eta: 0:16:33  lr: 0.000001  min_lr: 0.000000  loss: 4.1167 (4.1984)  class_acc: 0.3333 (0.3155)  loss_scale: 32768.0000 (37755.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1099  max mem: 15572
Epoch: [36]  [1080/2809]  eta: 0:16:29  lr: 0.000001  min_lr: 0.000000  loss: 4.1311 (4.1984)  class_acc: 0.3333 (0.3159)  loss_scale: 65536.0000 (38012.0925)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1597  max mem: 15572
[2025-01-16 07:37:27,713] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102211
[2025-01-16 07:37:27,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:37:27,714] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [1090/2809]  eta: 0:16:22  lr: 0.000001  min_lr: 0.000000  loss: 4.1311 (4.1971)  class_acc: 0.3750 (0.3164)  loss_scale: 65536.0000 (38144.2346)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1241  max mem: 15572
Epoch: [36]  [1100/2809]  eta: 0:16:17  lr: 0.000001  min_lr: 0.000000  loss: 4.1485 (4.1972)  class_acc: 0.2917 (0.3160)  loss_scale: 32768.0000 (38095.4042)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0902  max mem: 15572
Epoch: [36]  [1110/2809]  eta: 0:16:09  lr: 0.000001  min_lr: 0.000000  loss: 4.1203 (4.1956)  class_acc: 0.3333 (0.3167)  loss_scale: 32768.0000 (38047.4527)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0745  max mem: 15572
Epoch: [36]  [1120/2809]  eta: 0:16:03  lr: 0.000001  min_lr: 0.000000  loss: 4.0901 (4.1963)  class_acc: 0.3333 (0.3164)  loss_scale: 32768.0000 (38000.3568)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0589  max mem: 15572
Epoch: [36]  [1130/2809]  eta: 0:15:58  lr: 0.000001  min_lr: 0.000000  loss: 4.1309 (4.1953)  class_acc: 0.3333 (0.3169)  loss_scale: 32768.0000 (37954.0937)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0933  max mem: 15572
Epoch: [36]  [1140/2809]  eta: 0:15:52  lr: 0.000001  min_lr: 0.000000  loss: 4.1673 (4.1963)  class_acc: 0.3333 (0.3172)  loss_scale: 32768.0000 (37908.6415)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1118  max mem: 15572
Epoch: [36]  [1150/2809]  eta: 0:15:46  lr: 0.000001  min_lr: 0.000000  loss: 4.2529 (4.1955)  class_acc: 0.2500 (0.3165)  loss_scale: 32768.0000 (37863.9791)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1000  max mem: 15572
Epoch: [36]  [1160/2809]  eta: 0:15:39  lr: 0.000001  min_lr: 0.000000  loss: 4.2485 (4.1963)  class_acc: 0.2500 (0.3165)  loss_scale: 32768.0000 (37820.0861)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0566  max mem: 15572
Epoch: [36]  [1170/2809]  eta: 0:15:34  lr: 0.000001  min_lr: 0.000000  loss: 4.2576 (4.1965)  class_acc: 0.2500 (0.3166)  loss_scale: 32768.0000 (37776.9428)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1124  max mem: 15572
Epoch: [36]  [1180/2809]  eta: 0:15:28  lr: 0.000001  min_lr: 0.000000  loss: 4.1576 (4.1961)  class_acc: 0.2500 (0.3162)  loss_scale: 32768.0000 (37734.5301)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1422  max mem: 15572
Epoch: [36]  [1190/2809]  eta: 0:15:24  lr: 0.000001  min_lr: 0.000000  loss: 4.1576 (4.1961)  class_acc: 0.2917 (0.3169)  loss_scale: 32768.0000 (37692.8296)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1561  max mem: 15572
Epoch: [36]  [1200/2809]  eta: 0:15:17  lr: 0.000001  min_lr: 0.000000  loss: 4.2598 (4.1963)  class_acc: 0.3750 (0.3167)  loss_scale: 32768.0000 (37651.8235)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1250  max mem: 15572
Epoch: [36]  [1210/2809]  eta: 0:15:12  lr: 0.000001  min_lr: 0.000000  loss: 4.1528 (4.1950)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (37611.4946)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1085  max mem: 15572
[2025-01-16 07:38:40,224] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:38:40,224] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1220/2809]  eta: 0:15:07  lr: 0.000001  min_lr: 0.000000  loss: 4.1528 (4.1959)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (37706.0115)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1664  max mem: 15572
[2025-01-16 07:38:46,013] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102350
[2025-01-16 07:38:46,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:38:46,013] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [1230/2809]  eta: 0:15:02  lr: 0.000001  min_lr: 0.000000  loss: 4.2861 (4.1966)  class_acc: 0.2500 (0.3168)  loss_scale: 32768.0000 (37798.9927)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1832  max mem: 15572
Epoch: [36]  [1240/2809]  eta: 0:14:55  lr: 0.000001  min_lr: 0.000000  loss: 4.1269 (4.1955)  class_acc: 0.3333 (0.3176)  loss_scale: 32768.0000 (37758.4529)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1445  max mem: 15572
Epoch: [36]  [1250/2809]  eta: 0:14:50  lr: 0.000001  min_lr: 0.000000  loss: 4.0779 (4.1947)  class_acc: 0.3750 (0.3176)  loss_scale: 32768.0000 (37718.5612)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.1094  max mem: 15572
Epoch: [36]  [1260/2809]  eta: 0:14:43  lr: 0.000001  min_lr: 0.000000  loss: 4.1998 (4.1961)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (37679.3021)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0996  max mem: 15572
Epoch: [36]  [1270/2809]  eta: 0:14:38  lr: 0.000001  min_lr: 0.000000  loss: 4.2448 (4.1964)  class_acc: 0.3333 (0.3184)  loss_scale: 32768.0000 (37640.6609)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1109  max mem: 15572
Epoch: [36]  [1280/2809]  eta: 0:14:32  lr: 0.000001  min_lr: 0.000000  loss: 4.0511 (4.1944)  class_acc: 0.3750 (0.3189)  loss_scale: 32768.0000 (37602.6230)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1215  max mem: 15572
Epoch: [36]  [1290/2809]  eta: 0:14:26  lr: 0.000001  min_lr: 0.000000  loss: 4.0821 (4.1956)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (37565.1743)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0910  max mem: 15572
Epoch: [36]  [1300/2809]  eta: 0:14:21  lr: 0.000001  min_lr: 0.000000  loss: 4.2637 (4.1956)  class_acc: 0.2917 (0.3188)  loss_scale: 32768.0000 (37528.3013)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1289  max mem: 15572
Epoch: [36]  [1310/2809]  eta: 0:14:15  lr: 0.000001  min_lr: 0.000000  loss: 4.1912 (4.1945)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (37491.9908)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1413  max mem: 15572
Epoch: [36]  [1320/2809]  eta: 0:14:09  lr: 0.000001  min_lr: 0.000000  loss: 4.1578 (4.1941)  class_acc: 0.2917 (0.3194)  loss_scale: 32768.0000 (37456.2301)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1258  max mem: 15572
Epoch: [36]  [1330/2809]  eta: 0:14:04  lr: 0.000001  min_lr: 0.000000  loss: 4.1578 (4.1938)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (37421.0068)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1416  max mem: 15572
Epoch: [36]  [1340/2809]  eta: 0:13:58  lr: 0.000001  min_lr: 0.000000  loss: 4.1654 (4.1936)  class_acc: 0.3333 (0.3199)  loss_scale: 32768.0000 (37386.3087)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1658  max mem: 15572
Epoch: [36]  [1350/2809]  eta: 0:13:52  lr: 0.000001  min_lr: 0.000000  loss: 4.3150 (4.1946)  class_acc: 0.2917 (0.3198)  loss_scale: 32768.0000 (37352.1244)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1384  max mem: 15572
[2025-01-16 07:39:59,882] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:39:59,882] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1360/2809]  eta: 0:13:46  lr: 0.000001  min_lr: 0.000000  loss: 4.3445 (4.1946)  class_acc: 0.2500 (0.3194)  loss_scale: 32768.0000 (37462.9008)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0963  max mem: 15572
[2025-01-16 07:40:06,138] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102491
[2025-01-16 07:40:06,138] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:40:06,139] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [1370/2809]  eta: 0:13:40  lr: 0.000001  min_lr: 0.000000  loss: 4.2667 (4.1952)  class_acc: 0.2500 (0.3194)  loss_scale: 65536.0000 (37572.0613)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0928  max mem: 15572
Epoch: [36]  [1380/2809]  eta: 0:13:34  lr: 0.000001  min_lr: 0.000000  loss: 4.1575 (4.1953)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (37537.2744)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0958  max mem: 15572
Epoch: [36]  [1390/2809]  eta: 0:13:28  lr: 0.000001  min_lr: 0.000000  loss: 4.0955 (4.1944)  class_acc: 0.3333 (0.3194)  loss_scale: 32768.0000 (37502.9878)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1176  max mem: 15572
Epoch: [36]  [1400/2809]  eta: 0:13:23  lr: 0.000001  min_lr: 0.000000  loss: 4.1232 (4.1945)  class_acc: 0.3333 (0.3194)  loss_scale: 32768.0000 (37469.1906)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1509  max mem: 15572
Epoch: [36]  [1410/2809]  eta: 0:13:18  lr: 0.000001  min_lr: 0.000000  loss: 4.2321 (4.1949)  class_acc: 0.2917 (0.3190)  loss_scale: 32768.0000 (37435.8724)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1849  max mem: 15572
Epoch: [36]  [1420/2809]  eta: 0:13:11  lr: 0.000001  min_lr: 0.000000  loss: 4.2464 (4.1953)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (37403.0232)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.1296  max mem: 15572
Epoch: [36]  [1430/2809]  eta: 0:13:06  lr: 0.000001  min_lr: 0.000000  loss: 4.2364 (4.1955)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (37370.6331)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0749  max mem: 15572
Epoch: [36]  [1440/2809]  eta: 0:12:59  lr: 0.000001  min_lr: 0.000000  loss: 4.3026 (4.1964)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (37338.6926)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0803  max mem: 15572
Epoch: [36]  [1450/2809]  eta: 0:12:54  lr: 0.000001  min_lr: 0.000000  loss: 4.3123 (4.1967)  class_acc: 0.2917 (0.3193)  loss_scale: 32768.0000 (37307.1923)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0567  max mem: 15572
Epoch: [36]  [1460/2809]  eta: 0:12:47  lr: 0.000001  min_lr: 0.000000  loss: 4.3024 (4.1969)  class_acc: 0.3333 (0.3193)  loss_scale: 32768.0000 (37276.1232)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0462  max mem: 15572
Epoch: [36]  [1470/2809]  eta: 0:12:42  lr: 0.000001  min_lr: 0.000000  loss: 4.2271 (4.1971)  class_acc: 0.2917 (0.3194)  loss_scale: 32768.0000 (37245.4765)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1247  max mem: 15572
Epoch: [36]  [1480/2809]  eta: 0:12:36  lr: 0.000001  min_lr: 0.000000  loss: 4.2079 (4.1970)  class_acc: 0.2917 (0.3195)  loss_scale: 32768.0000 (37215.2438)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1618  max mem: 15572
Epoch: [36]  [1490/2809]  eta: 0:12:31  lr: 0.000001  min_lr: 0.000000  loss: 4.0825 (4.1965)  class_acc: 0.3333 (0.3201)  loss_scale: 32768.0000 (37185.4165)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1492  max mem: 15572
[2025-01-16 07:41:18,345] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:41:18,345] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1500/2809]  eta: 0:12:25  lr: 0.000001  min_lr: 0.000000  loss: 4.2458 (4.1978)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (37265.1406)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1715  max mem: 15572
Epoch: [36]  [1510/2809]  eta: 0:12:19  lr: 0.000001  min_lr: 0.000000  loss: 4.2622 (4.1974)  class_acc: 0.3333 (0.3204)  loss_scale: 65536.0000 (37452.2409)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1076  max mem: 15572
Epoch: [36]  [1520/2809]  eta: 0:12:13  lr: 0.000001  min_lr: 0.000000  loss: 4.2622 (4.1979)  class_acc: 0.2917 (0.3202)  loss_scale: 65536.0000 (37636.8810)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0881  max mem: 15572
Epoch: [36]  [1530/2809]  eta: 0:12:07  lr: 0.000001  min_lr: 0.000000  loss: 4.1916 (4.1973)  class_acc: 0.2917 (0.3204)  loss_scale: 65536.0000 (37819.1091)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0796  max mem: 15572
[2025-01-16 07:41:41,290] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102662
[2025-01-16 07:41:41,291] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:41:41,291] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [1540/2809]  eta: 0:12:01  lr: 0.000001  min_lr: 0.000000  loss: 4.1193 (4.1968)  class_acc: 0.3333 (0.3207)  loss_scale: 65536.0000 (37935.1798)  weight_decay: 0.0500 (0.0500)  time: 0.4940  data: 0.0487  max mem: 15572
Epoch: [36]  [1550/2809]  eta: 0:11:55  lr: 0.000001  min_lr: 0.000000  loss: 4.1435 (4.1973)  class_acc: 0.3333 (0.3209)  loss_scale: 32768.0000 (37901.8646)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0975  max mem: 15572
Epoch: [36]  [1560/2809]  eta: 0:11:49  lr: 0.000001  min_lr: 0.000000  loss: 4.2498 (4.1981)  class_acc: 0.2917 (0.3207)  loss_scale: 32768.0000 (37868.9763)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0798  max mem: 15572
Epoch: [36]  [1570/2809]  eta: 0:11:43  lr: 0.000001  min_lr: 0.000000  loss: 4.2323 (4.1975)  class_acc: 0.2917 (0.3207)  loss_scale: 32768.0000 (37836.5067)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0801  max mem: 15572
Epoch: [36]  [1580/2809]  eta: 0:11:37  lr: 0.000001  min_lr: 0.000000  loss: 4.1265 (4.1973)  class_acc: 0.3333 (0.3205)  loss_scale: 32768.0000 (37804.4478)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0892  max mem: 15572
Epoch: [36]  [1590/2809]  eta: 0:11:31  lr: 0.000001  min_lr: 0.000000  loss: 4.1717 (4.1974)  class_acc: 0.2500 (0.3200)  loss_scale: 32768.0000 (37772.7920)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1078  max mem: 15572
Epoch: [36]  [1600/2809]  eta: 0:11:26  lr: 0.000001  min_lr: 0.000000  loss: 4.3345 (4.1978)  class_acc: 0.2500 (0.3197)  loss_scale: 32768.0000 (37741.5315)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1446  max mem: 15572
Epoch: [36]  [1610/2809]  eta: 0:11:20  lr: 0.000001  min_lr: 0.000000  loss: 4.3538 (4.1982)  class_acc: 0.3333 (0.3199)  loss_scale: 32768.0000 (37710.6592)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.1154  max mem: 15572
Epoch: [36]  [1620/2809]  eta: 0:11:14  lr: 0.000001  min_lr: 0.000000  loss: 4.3195 (4.1988)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (37680.1678)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.1041  max mem: 15572
Epoch: [36]  [1630/2809]  eta: 0:11:09  lr: 0.000001  min_lr: 0.000000  loss: 4.2666 (4.1989)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (37650.0503)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1544  max mem: 15572
Epoch: [36]  [1640/2809]  eta: 0:11:03  lr: 0.000001  min_lr: 0.000000  loss: 4.2463 (4.1992)  class_acc: 0.3750 (0.3202)  loss_scale: 32768.0000 (37620.2998)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1655  max mem: 15572
Epoch: [36]  [1650/2809]  eta: 0:10:58  lr: 0.000001  min_lr: 0.000000  loss: 4.1731 (4.1987)  class_acc: 0.3333 (0.3205)  loss_scale: 32768.0000 (37590.9098)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1225  max mem: 15572
Epoch: [36]  [1660/2809]  eta: 0:10:52  lr: 0.000001  min_lr: 0.000000  loss: 4.1603 (4.1985)  class_acc: 0.3333 (0.3204)  loss_scale: 32768.0000 (37561.8736)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1109  max mem: 15572
[2025-01-16 07:42:53,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:42:53,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1670/2809]  eta: 0:10:46  lr: 0.000001  min_lr: 0.000000  loss: 4.1847 (4.1984)  class_acc: 0.2500 (0.3203)  loss_scale: 32768.0000 (37611.6242)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0933  max mem: 15572
Epoch: [36]  [1680/2809]  eta: 0:10:41  lr: 0.000001  min_lr: 0.000000  loss: 4.2341 (4.1988)  class_acc: 0.3333 (0.3206)  loss_scale: 65536.0000 (37777.7418)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1255  max mem: 15572
Epoch: [36]  [1690/2809]  eta: 0:10:35  lr: 0.000001  min_lr: 0.000000  loss: 4.2550 (4.1989)  class_acc: 0.3333 (0.3204)  loss_scale: 65536.0000 (37941.8947)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1222  max mem: 15572
Epoch: [36]  [1700/2809]  eta: 0:10:29  lr: 0.000001  min_lr: 0.000000  loss: 4.2550 (4.1995)  class_acc: 0.3333 (0.3207)  loss_scale: 65536.0000 (38104.1176)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1133  max mem: 15572
Epoch: [36]  [1710/2809]  eta: 0:10:24  lr: 0.000001  min_lr: 0.000000  loss: 4.3443 (4.1999)  class_acc: 0.2917 (0.3205)  loss_scale: 65536.0000 (38264.4442)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1511  max mem: 15572
[2025-01-16 07:43:23,259] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102841
[2025-01-16 07:43:23,260] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:43:23,260] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [1720/2809]  eta: 0:10:18  lr: 0.000001  min_lr: 0.000000  loss: 4.2028 (4.1998)  class_acc: 0.2500 (0.3201)  loss_scale: 65536.0000 (38346.7472)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1598  max mem: 15572
Epoch: [36]  [1730/2809]  eta: 0:10:13  lr: 0.000001  min_lr: 0.000000  loss: 4.1723 (4.1996)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (38314.5188)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1280  max mem: 15572
Epoch: [36]  [1740/2809]  eta: 0:10:07  lr: 0.000001  min_lr: 0.000000  loss: 4.2699 (4.1997)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (38282.6605)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1198  max mem: 15572
Epoch: [36]  [1750/2809]  eta: 0:10:01  lr: 0.000001  min_lr: 0.000000  loss: 4.1906 (4.2000)  class_acc: 0.3750 (0.3203)  loss_scale: 32768.0000 (38251.1662)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1090  max mem: 15572
Epoch: [36]  [1760/2809]  eta: 0:09:55  lr: 0.000001  min_lr: 0.000000  loss: 4.1949 (4.2002)  class_acc: 0.3750 (0.3207)  loss_scale: 32768.0000 (38220.0295)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0804  max mem: 15572
Epoch: [36]  [1770/2809]  eta: 0:09:50  lr: 0.000001  min_lr: 0.000000  loss: 4.2049 (4.2003)  class_acc: 0.3333 (0.3206)  loss_scale: 32768.0000 (38189.2445)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1186  max mem: 15572
Epoch: [36]  [1780/2809]  eta: 0:09:44  lr: 0.000001  min_lr: 0.000000  loss: 4.1645 (4.1994)  class_acc: 0.2917 (0.3207)  loss_scale: 32768.0000 (38158.8052)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1308  max mem: 15572
Epoch: [36]  [1790/2809]  eta: 0:09:38  lr: 0.000001  min_lr: 0.000000  loss: 4.1989 (4.1998)  class_acc: 0.2917 (0.3208)  loss_scale: 32768.0000 (38128.7058)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0899  max mem: 15572
Epoch: [36]  [1800/2809]  eta: 0:09:31  lr: 0.000001  min_lr: 0.000000  loss: 4.2634 (4.2000)  class_acc: 0.3750 (0.3211)  loss_scale: 32768.0000 (38098.9406)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0371  max mem: 15572
Epoch: [36]  [1810/2809]  eta: 0:09:26  lr: 0.000001  min_lr: 0.000000  loss: 4.2518 (4.2002)  class_acc: 0.3333 (0.3211)  loss_scale: 32768.0000 (38069.5041)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0650  max mem: 15572
Epoch: [36]  [1820/2809]  eta: 0:09:20  lr: 0.000001  min_lr: 0.000000  loss: 4.2045 (4.2001)  class_acc: 0.3333 (0.3212)  loss_scale: 32768.0000 (38040.3910)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1184  max mem: 15572
Epoch: [36]  [1830/2809]  eta: 0:09:14  lr: 0.000001  min_lr: 0.000000  loss: 4.2045 (4.2004)  class_acc: 0.3333 (0.3210)  loss_scale: 32768.0000 (38011.5958)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1218  max mem: 15572
Epoch: [36]  [1840/2809]  eta: 0:09:09  lr: 0.000001  min_lr: 0.000000  loss: 4.0744 (4.1992)  class_acc: 0.3750 (0.3214)  loss_scale: 32768.0000 (37983.1135)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1274  max mem: 15572
[2025-01-16 07:44:32,948] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:44:32,948] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1850/2809]  eta: 0:09:03  lr: 0.000001  min_lr: 0.000000  loss: 4.3041 (4.2006)  class_acc: 0.3333 (0.3210)  loss_scale: 32768.0000 (38043.4533)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0941  max mem: 15572
Epoch: [36]  [1860/2809]  eta: 0:08:57  lr: 0.000001  min_lr: 0.000000  loss: 4.3473 (4.2003)  class_acc: 0.2500 (0.3211)  loss_scale: 65536.0000 (38191.1832)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0858  max mem: 15572
[2025-01-16 07:44:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102994
[2025-01-16 07:44:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:44:46,316] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [1870/2809]  eta: 0:08:51  lr: 0.000001  min_lr: 0.000000  loss: 4.2072 (4.2006)  class_acc: 0.2917 (0.3210)  loss_scale: 65536.0000 (38319.8204)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1118  max mem: 15572
[2025-01-16 07:44:49,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=103000, skipped=642, lr=[1.0527781531835162e-08, 1.0527781531835162e-08, 1.5039687902621662e-08, 1.5039687902621662e-08, 2.1485268432316662e-08, 2.1485268432316662e-08, 3.069324061759523e-08, 3.069324061759523e-08, 4.384748659656462e-08, 4.384748659656462e-08, 6.26392665665209e-08, 6.26392665665209e-08, 8.948466652360128e-08, 8.948466652360128e-08, 1.2783523789085898e-07, 1.2783523789085898e-07, 1.8262176841551284e-07, 1.8262176841551284e-07, 2.608882405935898e-07, 2.608882405935898e-07, 3.7269748656227115e-07, 3.7269748656227115e-07, 5.324249808032445e-07, 5.324249808032445e-07, 7.606071154332065e-07, 7.606071154332065e-07, 1.0865815934760094e-06, 1.0865815934760094e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:44:49,271] [INFO] [timer.py:260:stop] epoch=0/micro_step=103000/global_step=103000, RunningAvgSamplesPerSec=27.98632303699016, CurrSamplesPerSec=31.480457486824676, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [36]  [1880/2809]  eta: 0:08:46  lr: 0.000001  min_lr: 0.000000  loss: 4.2139 (4.2005)  class_acc: 0.2500 (0.3208)  loss_scale: 32768.0000 (38290.3052)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1491  max mem: 15572
Epoch: [36]  [1890/2809]  eta: 0:08:41  lr: 0.000001  min_lr: 0.000000  loss: 4.0556 (4.2000)  class_acc: 0.2917 (0.3209)  loss_scale: 32768.0000 (38261.1021)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1895  max mem: 15572
Epoch: [36]  [1900/2809]  eta: 0:08:35  lr: 0.000001  min_lr: 0.000000  loss: 4.0805 (4.1992)  class_acc: 0.3333 (0.3213)  loss_scale: 32768.0000 (38232.2062)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1384  max mem: 15572
Epoch: [36]  [1910/2809]  eta: 0:08:29  lr: 0.000001  min_lr: 0.000000  loss: 4.1697 (4.1994)  class_acc: 0.3750 (0.3215)  loss_scale: 32768.0000 (38203.6128)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0570  max mem: 15572
Epoch: [36]  [1920/2809]  eta: 0:08:23  lr: 0.000001  min_lr: 0.000000  loss: 4.1676 (4.1993)  class_acc: 0.3333 (0.3214)  loss_scale: 32768.0000 (38175.3170)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0984  max mem: 15572
Epoch: [36]  [1930/2809]  eta: 0:08:18  lr: 0.000001  min_lr: 0.000000  loss: 4.1669 (4.1987)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (38147.3143)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1231  max mem: 15572
Epoch: [36]  [1940/2809]  eta: 0:08:12  lr: 0.000001  min_lr: 0.000000  loss: 4.1802 (4.1993)  class_acc: 0.3333 (0.3219)  loss_scale: 32768.0000 (38119.6002)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0571  max mem: 15572
Epoch: [36]  [1950/2809]  eta: 0:08:06  lr: 0.000001  min_lr: 0.000000  loss: 4.1121 (4.1985)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (38092.1702)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0653  max mem: 15572
Epoch: [36]  [1960/2809]  eta: 0:08:01  lr: 0.000001  min_lr: 0.000000  loss: 4.0925 (4.1982)  class_acc: 0.2917 (0.3218)  loss_scale: 32768.0000 (38065.0199)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1660  max mem: 15572
Epoch: [36]  [1970/2809]  eta: 0:07:55  lr: 0.000001  min_lr: 0.000000  loss: 4.0925 (4.1980)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (38038.1451)  weight_decay: 0.0500 (0.0500)  time: 0.6756  data: 0.2470  max mem: 15572
Epoch: [36]  [1980/2809]  eta: 0:07:50  lr: 0.000001  min_lr: 0.000000  loss: 4.0613 (4.1974)  class_acc: 0.3750 (0.3219)  loss_scale: 32768.0000 (38011.5416)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1819  max mem: 15572
Epoch: [36]  [1990/2809]  eta: 0:07:43  lr: 0.000001  min_lr: 0.000000  loss: 4.1876 (4.1972)  class_acc: 0.2500 (0.3216)  loss_scale: 32768.0000 (37985.2054)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0486  max mem: 15572
[2025-01-16 07:46:00,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:46:00,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [2000/2809]  eta: 0:07:38  lr: 0.000001  min_lr: 0.000000  loss: 4.1876 (4.1967)  class_acc: 0.2500 (0.3215)  loss_scale: 32768.0000 (37991.8841)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.1083  max mem: 15572
Epoch: [36]  [2010/2809]  eta: 0:07:33  lr: 0.000001  min_lr: 0.000000  loss: 4.0426 (4.1963)  class_acc: 0.2917 (0.3216)  loss_scale: 65536.0000 (38128.8513)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.2010  max mem: 15572
[2025-01-16 07:46:07,696] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103136
[2025-01-16 07:46:07,696] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:46:07,696] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [2020/2809]  eta: 0:07:27  lr: 0.000001  min_lr: 0.000000  loss: 4.1325 (4.1962)  class_acc: 0.3333 (0.3217)  loss_scale: 65536.0000 (38118.5393)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0933  max mem: 15572
Epoch: [36]  [2030/2809]  eta: 0:07:21  lr: 0.000001  min_lr: 0.000000  loss: 4.0906 (4.1952)  class_acc: 0.4167 (0.3222)  loss_scale: 32768.0000 (38092.1950)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0655  max mem: 15572
Epoch: [36]  [2040/2809]  eta: 0:07:15  lr: 0.000001  min_lr: 0.000000  loss: 4.0265 (4.1946)  class_acc: 0.4167 (0.3223)  loss_scale: 32768.0000 (38066.1088)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1367  max mem: 15572
Epoch: [36]  [2050/2809]  eta: 0:07:10  lr: 0.000001  min_lr: 0.000000  loss: 4.1253 (4.1956)  class_acc: 0.3333 (0.3221)  loss_scale: 32768.0000 (38040.2769)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1240  max mem: 15572
Epoch: [36]  [2060/2809]  eta: 0:07:04  lr: 0.000001  min_lr: 0.000000  loss: 4.2544 (4.1951)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (38014.6958)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1377  max mem: 15572
Epoch: [36]  [2070/2809]  eta: 0:06:59  lr: 0.000001  min_lr: 0.000000  loss: 4.2346 (4.1958)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37989.3617)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1488  max mem: 15572
Epoch: [36]  [2080/2809]  eta: 0:06:53  lr: 0.000001  min_lr: 0.000000  loss: 4.1913 (4.1952)  class_acc: 0.3333 (0.3221)  loss_scale: 32768.0000 (37964.2710)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.1798  max mem: 15572
Epoch: [36]  [2090/2809]  eta: 0:06:47  lr: 0.000001  min_lr: 0.000000  loss: 4.1223 (4.1950)  class_acc: 0.3333 (0.3219)  loss_scale: 32768.0000 (37939.4204)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1582  max mem: 15572
Epoch: [36]  [2100/2809]  eta: 0:06:42  lr: 0.000001  min_lr: 0.000000  loss: 4.2194 (4.1953)  class_acc: 0.2500 (0.3217)  loss_scale: 32768.0000 (37914.8063)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1745  max mem: 15572
Epoch: [36]  [2110/2809]  eta: 0:06:37  lr: 0.000001  min_lr: 0.000000  loss: 4.1301 (4.1948)  class_acc: 0.2500 (0.3216)  loss_scale: 32768.0000 (37890.4254)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1809  max mem: 15572
Epoch: [36]  [2120/2809]  eta: 0:06:31  lr: 0.000001  min_lr: 0.000000  loss: 4.0432 (4.1942)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (37866.2744)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0933  max mem: 15572
Epoch: [36]  [2130/2809]  eta: 0:06:25  lr: 0.000001  min_lr: 0.000000  loss: 4.1509 (4.1942)  class_acc: 0.2917 (0.3218)  loss_scale: 32768.0000 (37842.3501)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0987  max mem: 15572
Epoch: [36]  [2140/2809]  eta: 0:06:20  lr: 0.000001  min_lr: 0.000000  loss: 4.1509 (4.1935)  class_acc: 0.3333 (0.3219)  loss_scale: 32768.0000 (37818.6492)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1011  max mem: 15572
[2025-01-16 07:47:23,284] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:47:23,284] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [2150/2809]  eta: 0:06:14  lr: 0.000001  min_lr: 0.000000  loss: 4.1880 (4.1935)  class_acc: 0.2917 (0.3218)  loss_scale: 32768.0000 (37947.5072)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0658  max mem: 15572
[2025-01-16 07:47:28,265] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103275
[2025-01-16 07:47:28,265] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:47:28,265] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [2160/2809]  eta: 0:06:08  lr: 0.000001  min_lr: 0.000000  loss: 4.1880 (4.1931)  class_acc: 0.2917 (0.3218)  loss_scale: 32768.0000 (37923.5391)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0186  max mem: 15572
Epoch: [36]  [2170/2809]  eta: 0:06:02  lr: 0.000001  min_lr: 0.000000  loss: 4.1170 (4.1929)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (37899.7918)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0603  max mem: 15572
Epoch: [36]  [2180/2809]  eta: 0:05:56  lr: 0.000001  min_lr: 0.000000  loss: 4.0481 (4.1923)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37876.2623)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.1058  max mem: 15572
Epoch: [36]  [2190/2809]  eta: 0:05:51  lr: 0.000001  min_lr: 0.000000  loss: 4.0325 (4.1919)  class_acc: 0.2917 (0.3218)  loss_scale: 32768.0000 (37852.9475)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1169  max mem: 15572
Epoch: [36]  [2200/2809]  eta: 0:05:45  lr: 0.000001  min_lr: 0.000000  loss: 4.1999 (4.1922)  class_acc: 0.2500 (0.3216)  loss_scale: 32768.0000 (37829.8446)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1085  max mem: 15572
Epoch: [36]  [2210/2809]  eta: 0:05:39  lr: 0.000001  min_lr: 0.000000  loss: 4.2121 (4.1921)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (37806.9507)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1102  max mem: 15572
Epoch: [36]  [2220/2809]  eta: 0:05:34  lr: 0.000001  min_lr: 0.000000  loss: 4.2200 (4.1922)  class_acc: 0.2917 (0.3216)  loss_scale: 32768.0000 (37784.2629)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1397  max mem: 15572
Epoch: [36]  [2230/2809]  eta: 0:05:28  lr: 0.000001  min_lr: 0.000000  loss: 4.1508 (4.1913)  class_acc: 0.3333 (0.3219)  loss_scale: 32768.0000 (37761.7786)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.1067  max mem: 15572
Epoch: [36]  [2240/2809]  eta: 0:05:22  lr: 0.000001  min_lr: 0.000000  loss: 4.1473 (4.1914)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (37739.4949)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1176  max mem: 15572
Epoch: [36]  [2250/2809]  eta: 0:05:16  lr: 0.000001  min_lr: 0.000000  loss: 4.1867 (4.1915)  class_acc: 0.2917 (0.3217)  loss_scale: 32768.0000 (37717.4092)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1093  max mem: 15572
Epoch: [36]  [2260/2809]  eta: 0:05:11  lr: 0.000001  min_lr: 0.000000  loss: 4.2436 (4.1915)  class_acc: 0.2500 (0.3217)  loss_scale: 32768.0000 (37695.5188)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1384  max mem: 15572
Epoch: [36]  [2270/2809]  eta: 0:05:05  lr: 0.000001  min_lr: 0.000000  loss: 4.2436 (4.1921)  class_acc: 0.2500 (0.3216)  loss_scale: 32768.0000 (37673.8212)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.1880  max mem: 15572
[2025-01-16 07:48:41,194] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:48:41,195] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [2280/2809]  eta: 0:05:00  lr: 0.000001  min_lr: 0.000000  loss: 4.2604 (4.1917)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (37666.6795)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1197  max mem: 15572
Epoch: [36]  [2290/2809]  eta: 0:04:54  lr: 0.000001  min_lr: 0.000000  loss: 4.2790 (4.1921)  class_acc: 0.3750 (0.3218)  loss_scale: 65536.0000 (37788.3265)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.0788  max mem: 15572
Epoch: [36]  [2300/2809]  eta: 0:04:48  lr: 0.000001  min_lr: 0.000000  loss: 4.3375 (4.1929)  class_acc: 0.3333 (0.3219)  loss_scale: 65536.0000 (37908.9161)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1207  max mem: 15572
Epoch: [36]  [2310/2809]  eta: 0:04:43  lr: 0.000001  min_lr: 0.000000  loss: 4.2723 (4.1924)  class_acc: 0.2917 (0.3218)  loss_scale: 65536.0000 (38028.4621)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1211  max mem: 15572
Epoch: [36]  [2320/2809]  eta: 0:04:37  lr: 0.000001  min_lr: 0.000000  loss: 4.2053 (4.1923)  class_acc: 0.2500 (0.3219)  loss_scale: 65536.0000 (38146.9780)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1184  max mem: 15572
[2025-01-16 07:49:06,315] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103448
[2025-01-16 07:49:06,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:49:06,317] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [2330/2809]  eta: 0:04:32  lr: 0.000001  min_lr: 0.000000  loss: 4.2053 (4.1918)  class_acc: 0.3750 (0.3221)  loss_scale: 65536.0000 (38166.0746)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1461  max mem: 15572
Epoch: [36]  [2340/2809]  eta: 0:04:26  lr: 0.000001  min_lr: 0.000000  loss: 4.1480 (4.1913)  class_acc: 0.3750 (0.3223)  loss_scale: 32768.0000 (38143.0158)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1666  max mem: 15572
Epoch: [36]  [2350/2809]  eta: 0:04:20  lr: 0.000001  min_lr: 0.000000  loss: 4.1480 (4.1912)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (38120.1531)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1134  max mem: 15572
Epoch: [36]  [2360/2809]  eta: 0:04:14  lr: 0.000001  min_lr: 0.000000  loss: 4.2071 (4.1913)  class_acc: 0.2500 (0.3221)  loss_scale: 32768.0000 (38097.4841)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0505  max mem: 15572
Epoch: [36]  [2370/2809]  eta: 0:04:09  lr: 0.000001  min_lr: 0.000000  loss: 4.2071 (4.1914)  class_acc: 0.2500 (0.3218)  loss_scale: 32768.0000 (38075.0063)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1372  max mem: 15572
Epoch: [36]  [2380/2809]  eta: 0:04:03  lr: 0.000001  min_lr: 0.000000  loss: 4.2789 (4.1916)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (38052.7173)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1370  max mem: 15572
Epoch: [36]  [2390/2809]  eta: 0:03:57  lr: 0.000001  min_lr: 0.000000  loss: 4.2305 (4.1919)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (38030.6148)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0868  max mem: 15572
Epoch: [36]  [2400/2809]  eta: 0:03:52  lr: 0.000001  min_lr: 0.000000  loss: 4.1849 (4.1916)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (38008.6964)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1035  max mem: 15572
Epoch: [36]  [2410/2809]  eta: 0:03:46  lr: 0.000001  min_lr: 0.000000  loss: 4.1687 (4.1917)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37986.9598)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1175  max mem: 15572
Epoch: [36]  [2420/2809]  eta: 0:03:40  lr: 0.000001  min_lr: 0.000000  loss: 4.2433 (4.1919)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37965.4027)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1117  max mem: 15572
Epoch: [36]  [2430/2809]  eta: 0:03:35  lr: 0.000001  min_lr: 0.000000  loss: 4.0823 (4.1911)  class_acc: 0.3750 (0.3223)  loss_scale: 32768.0000 (37944.0230)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1231  max mem: 15572
Epoch: [36]  [2440/2809]  eta: 0:03:29  lr: 0.000001  min_lr: 0.000000  loss: 4.0823 (4.1912)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (37922.8185)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1190  max mem: 15572
Epoch: [36]  [2450/2809]  eta: 0:03:23  lr: 0.000001  min_lr: 0.000000  loss: 4.2195 (4.1917)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (37901.7870)  weight_decay: 0.0500 (0.0500)  time: 0.5287  data: 0.0537  max mem: 15572
[2025-01-16 07:50:19,492] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:50:19,492] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [2460/2809]  eta: 0:03:18  lr: 0.000001  min_lr: 0.000000  loss: 4.1843 (4.1912)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (37987.4458)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0744  max mem: 15572
[2025-01-16 07:50:25,057] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103586
[2025-01-16 07:50:25,057] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:50:25,058] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [2470/2809]  eta: 0:03:12  lr: 0.000001  min_lr: 0.000000  loss: 4.1091 (4.1914)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37979.5840)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1508  max mem: 15572
Epoch: [36]  [2480/2809]  eta: 0:03:06  lr: 0.000001  min_lr: 0.000000  loss: 4.1635 (4.1910)  class_acc: 0.3333 (0.3221)  loss_scale: 32768.0000 (37958.5780)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1558  max mem: 15572
Epoch: [36]  [2490/2809]  eta: 0:03:01  lr: 0.000001  min_lr: 0.000000  loss: 4.1635 (4.1906)  class_acc: 0.4167 (0.3225)  loss_scale: 32768.0000 (37937.7407)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1447  max mem: 15572
Epoch: [36]  [2500/2809]  eta: 0:02:55  lr: 0.000001  min_lr: 0.000000  loss: 4.3192 (4.1912)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (37917.0700)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1530  max mem: 15572
Epoch: [36]  [2510/2809]  eta: 0:02:50  lr: 0.000001  min_lr: 0.000000  loss: 4.3228 (4.1909)  class_acc: 0.2917 (0.3225)  loss_scale: 32768.0000 (37896.5639)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1312  max mem: 15572
Epoch: [36]  [2520/2809]  eta: 0:02:44  lr: 0.000001  min_lr: 0.000000  loss: 4.1606 (4.1910)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (37876.2205)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1036  max mem: 15572
Epoch: [36]  [2530/2809]  eta: 0:02:38  lr: 0.000001  min_lr: 0.000000  loss: 4.2767 (4.1915)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (37856.0379)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1163  max mem: 15572
Epoch: [36]  [2540/2809]  eta: 0:02:32  lr: 0.000001  min_lr: 0.000000  loss: 4.2077 (4.1912)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (37836.0142)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0992  max mem: 15572
Epoch: [36]  [2550/2809]  eta: 0:02:27  lr: 0.000001  min_lr: 0.000000  loss: 4.0845 (4.1912)  class_acc: 0.2917 (0.3224)  loss_scale: 32768.0000 (37816.1474)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0386  max mem: 15572
Epoch: [36]  [2560/2809]  eta: 0:02:21  lr: 0.000001  min_lr: 0.000000  loss: 4.0845 (4.1911)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (37796.4358)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0394  max mem: 15572
Epoch: [36]  [2570/2809]  eta: 0:02:15  lr: 0.000001  min_lr: 0.000000  loss: 4.1837 (4.1912)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (37776.8775)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0420  max mem: 15572
Epoch: [36]  [2580/2809]  eta: 0:02:10  lr: 0.000001  min_lr: 0.000000  loss: 4.1803 (4.1911)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (37757.4707)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0609  max mem: 15572
Epoch: [36]  [2590/2809]  eta: 0:02:04  lr: 0.000001  min_lr: 0.000000  loss: 4.3181 (4.1919)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (37738.2138)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1377  max mem: 15572
[2025-01-16 07:51:40,933] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:51:40,934] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [2600/2809]  eta: 0:01:58  lr: 0.000001  min_lr: 0.000000  loss: 4.3659 (4.1923)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (37845.0873)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.1625  max mem: 15572
[2025-01-16 07:51:50,046] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103732
[2025-01-16 07:51:50,047] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:51:50,047] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [2610/2809]  eta: 0:01:53  lr: 0.000001  min_lr: 0.000000  loss: 4.2335 (4.1923)  class_acc: 0.3333 (0.3224)  loss_scale: 65536.0000 (37913.4921)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0849  max mem: 15572
Epoch: [36]  [2620/2809]  eta: 0:01:47  lr: 0.000001  min_lr: 0.000000  loss: 4.1354 (4.1918)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (37893.8604)  weight_decay: 0.0500 (0.0500)  time: 0.4192  data: 0.0092  max mem: 15572
Epoch: [36]  [2630/2809]  eta: 0:01:41  lr: 0.000001  min_lr: 0.000000  loss: 4.1132 (4.1919)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (37874.3778)  weight_decay: 0.0500 (0.0500)  time: 0.4130  data: 0.0006  max mem: 15572
Epoch: [36]  [2640/2809]  eta: 0:01:35  lr: 0.000001  min_lr: 0.000000  loss: 4.1132 (4.1911)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (37855.0428)  weight_decay: 0.0500 (0.0500)  time: 0.4301  data: 0.0007  max mem: 15572
Epoch: [36]  [2650/2809]  eta: 0:01:30  lr: 0.000001  min_lr: 0.000000  loss: 4.0488 (4.1911)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (37835.8536)  weight_decay: 0.0500 (0.0500)  time: 0.4583  data: 0.0006  max mem: 15572
Epoch: [36]  [2660/2809]  eta: 0:01:24  lr: 0.000001  min_lr: 0.000000  loss: 4.1618 (4.1905)  class_acc: 0.3333 (0.3231)  loss_scale: 32768.0000 (37816.8087)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0401  max mem: 15572
Epoch: [36]  [2670/2809]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 4.1618 (4.1908)  class_acc: 0.3333 (0.3231)  loss_scale: 32768.0000 (37797.9064)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1330  max mem: 15572
Epoch: [36]  [2680/2809]  eta: 0:01:13  lr: 0.000001  min_lr: 0.000000  loss: 4.2311 (4.1910)  class_acc: 0.2917 (0.3231)  loss_scale: 32768.0000 (37779.1451)  weight_decay: 0.0500 (0.0500)  time: 0.6812  data: 0.2093  max mem: 15572
Epoch: [36]  [2690/2809]  eta: 0:01:07  lr: 0.000001  min_lr: 0.000000  loss: 4.2915 (4.1912)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (37760.5232)  weight_decay: 0.0500 (0.0500)  time: 0.7447  data: 0.2754  max mem: 15572
Epoch: [36]  [2700/2809]  eta: 0:01:01  lr: 0.000001  min_lr: 0.000000  loss: 4.2915 (4.1911)  class_acc: 0.2917 (0.3229)  loss_scale: 32768.0000 (37742.0392)  weight_decay: 0.0500 (0.0500)  time: 0.7906  data: 0.3280  max mem: 15572
Epoch: [36]  [2710/2809]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 4.0791 (4.1912)  class_acc: 0.2917 (0.3228)  loss_scale: 32768.0000 (37723.6916)  weight_decay: 0.0500 (0.0500)  time: 0.7191  data: 0.2661  max mem: 15572
Epoch: [36]  [2720/2809]  eta: 0:00:50  lr: 0.000001  min_lr: 0.000000  loss: 4.0660 (4.1906)  class_acc: 0.2917 (0.3229)  loss_scale: 32768.0000 (37705.4789)  weight_decay: 0.0500 (0.0500)  time: 0.6873  data: 0.2313  max mem: 15572
Epoch: [36]  [2730/2809]  eta: 0:00:45  lr: 0.000001  min_lr: 0.000000  loss: 4.1905 (4.1904)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (37687.3995)  weight_decay: 0.0500 (0.0500)  time: 0.7325  data: 0.2677  max mem: 15572
[2025-01-16 07:53:07,907] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:53:07,908] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [2740/2809]  eta: 0:00:39  lr: 0.000001  min_lr: 0.000000  loss: 4.1682 (4.1905)  class_acc: 0.2500 (0.3227)  loss_scale: 32768.0000 (37717.2711)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.1851  max mem: 15572
Epoch: [36]  [2750/2809]  eta: 0:00:33  lr: 0.000001  min_lr: 0.000000  loss: 4.1878 (4.1907)  class_acc: 0.2917 (0.3226)  loss_scale: 65536.0000 (37818.3933)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1503  max mem: 15572
Epoch: [36]  [2760/2809]  eta: 0:00:27  lr: 0.000001  min_lr: 0.000000  loss: 4.2210 (4.1907)  class_acc: 0.2917 (0.3225)  loss_scale: 65536.0000 (37918.7830)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1389  max mem: 15572
[2025-01-16 07:53:24,146] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103889
[2025-01-16 07:53:24,146] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:53:24,147] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [2770/2809]  eta: 0:00:22  lr: 0.000001  min_lr: 0.000000  loss: 4.1947 (4.1912)  class_acc: 0.2500 (0.3223)  loss_scale: 65536.0000 (37947.4962)  weight_decay: 0.0500 (0.0500)  time: 0.4792  data: 0.0401  max mem: 15572
Epoch: [36]  [2780/2809]  eta: 0:00:16  lr: 0.000001  min_lr: 0.000000  loss: 4.1367 (4.1906)  class_acc: 0.2500 (0.3224)  loss_scale: 32768.0000 (37928.8716)  weight_decay: 0.0500 (0.0500)  time: 0.4670  data: 0.0340  max mem: 15572
Epoch: [36]  [2790/2809]  eta: 0:00:10  lr: 0.000001  min_lr: 0.000000  loss: 4.0293 (4.1904)  class_acc: 0.2917 (0.3224)  loss_scale: 32768.0000 (37910.3805)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0729  max mem: 15572
Epoch: [36]  [2800/2809]  eta: 0:00:05  lr: 0.000001  min_lr: 0.000000  loss: 4.1127 (4.1904)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (37892.0214)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0729  max mem: 15572
Epoch: [36]  [2808/2809]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 4.2441 (4.1910)  class_acc: 0.2500 (0.3219)  loss_scale: 32768.0000 (37877.4283)  weight_decay: 0.0500 (0.0500)  time: 0.4465  data: 0.0339  max mem: 15572
Epoch: [36] Total time: 0:26:39 (0.5693 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 4.2441 (4.1910)  class_acc: 0.2500 (0.3219)  loss_scale: 32768.0000 (37877.4283)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:18:32  loss: 1.1190 (1.1190)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.0911  data: 3.8928  max mem: 15572
Val:  [ 10/272]  eta: 0:02:55  loss: 2.5057 (2.4644)  acc1: 50.0000 (49.4950)  acc5: 77.7778 (75.7576)  time: 0.6709  data: 0.4943  max mem: 15572
Val:  [ 20/272]  eta: 0:02:06  loss: 2.5137 (2.5167)  acc1: 50.0000 (48.1481)  acc5: 77.7778 (76.1905)  time: 0.3224  data: 0.1384  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 2.5739 (2.5646)  acc1: 44.4444 (45.1613)  acc5: 72.2222 (74.5520)  time: 0.3037  data: 0.1085  max mem: 15572
Val:  [ 40/272]  eta: 0:01:31  loss: 2.6131 (2.6003)  acc1: 33.3333 (42.5474)  acc5: 72.2222 (73.7127)  time: 0.2826  data: 0.0939  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 2.4723 (2.5363)  acc1: 44.4444 (43.8998)  acc5: 77.7778 (75.7081)  time: 0.3078  data: 0.1246  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 1.9339 (2.4758)  acc1: 55.5556 (45.8106)  acc5: 88.8889 (76.6849)  time: 0.3249  data: 0.1311  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 1.9376 (2.4153)  acc1: 61.1111 (48.2003)  acc5: 83.3333 (77.6213)  time: 0.3254  data: 0.1305  max mem: 15572
Val:  [ 80/272]  eta: 0:01:07  loss: 2.1603 (2.4195)  acc1: 55.5556 (48.0110)  acc5: 83.3333 (77.7092)  time: 0.2956  data: 0.1094  max mem: 15572
Val:  [ 90/272]  eta: 0:01:02  loss: 2.4055 (2.4231)  acc1: 50.0000 (47.8632)  acc5: 77.7778 (78.0830)  time: 0.2650  data: 0.0829  max mem: 15572
Val:  [100/272]  eta: 0:00:58  loss: 2.4703 (2.4480)  acc1: 44.4444 (47.0297)  acc5: 77.7778 (77.6678)  time: 0.2839  data: 0.0955  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.5927 (2.5005)  acc1: 27.7778 (45.2452)  acc5: 72.2222 (76.5265)  time: 0.3296  data: 0.1455  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 2.9629 (2.5355)  acc1: 27.7778 (44.4904)  acc5: 66.6667 (75.6657)  time: 0.3325  data: 0.1563  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 2.5011 (2.5092)  acc1: 44.4444 (45.0382)  acc5: 77.7778 (76.5055)  time: 0.2880  data: 0.0976  max mem: 15572
Val:  [140/272]  eta: 0:00:43  loss: 2.1501 (2.5138)  acc1: 50.0000 (45.2325)  acc5: 88.8889 (76.2411)  time: 0.2990  data: 0.1018  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 2.5457 (2.5126)  acc1: 38.8889 (44.7756)  acc5: 77.7778 (76.4533)  time: 0.3258  data: 0.1365  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.4387 (2.5090)  acc1: 44.4444 (45.1346)  acc5: 77.7778 (76.7771)  time: 0.3307  data: 0.1438  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.5509 (2.5256)  acc1: 38.8889 (44.5744)  acc5: 72.2222 (76.2183)  time: 0.3093  data: 0.1115  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.5509 (2.5153)  acc1: 33.3333 (44.5058)  acc5: 72.2222 (76.4886)  time: 0.2989  data: 0.1043  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.4784 (2.5529)  acc1: 33.3333 (43.3101)  acc5: 72.2222 (75.4218)  time: 0.2798  data: 0.0957  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.6672 (2.5566)  acc1: 33.3333 (42.9243)  acc5: 72.2222 (75.3455)  time: 0.2892  data: 0.1036  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.3857 (2.5649)  acc1: 38.8889 (42.6277)  acc5: 77.7778 (75.1711)  time: 0.3256  data: 0.1279  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.6795 (2.5598)  acc1: 38.8889 (42.7099)  acc5: 77.7778 (75.1885)  time: 0.3279  data: 0.1273  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.1763 (2.5451)  acc1: 55.5556 (43.4103)  acc5: 77.7778 (75.4690)  time: 0.3123  data: 0.1181  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1624 (2.5335)  acc1: 61.1111 (43.5915)  acc5: 83.3333 (75.7031)  time: 0.3017  data: 0.1120  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4641 (2.5386)  acc1: 38.8889 (43.1828)  acc5: 77.7778 (75.6751)  time: 0.2980  data: 0.1114  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9535 (2.4989)  acc1: 66.6667 (44.6999)  acc5: 88.8889 (76.3304)  time: 0.3251  data: 0.1353  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.9535 (2.4952)  acc1: 61.1111 (44.6699)  acc5: 88.8889 (76.4863)  time: 0.2619  data: 0.0937  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.9535 (2.4993)  acc1: 55.5556 (44.6652)  acc5: 88.8889 (76.4694)  time: 0.2019  data: 0.0405  max mem: 15572
Val: Total time: 0:01:26 (0.3177 s / it)
* Acc@1 44.665 Acc@5 76.469 loss 2.499
Accuracy of the network on the 4883 val videos: 44.7%
Max accuracy: 45.05%
Epoch: [37]  [   0/2809]  eta: 6:17:00  lr: 0.000001  min_lr: 0.000000  loss: 4.2640 (4.2640)  class_acc: 0.5000 (0.5000)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.0530  data: 7.5955  max mem: 15572
Epoch: [37]  [  10/2809]  eta: 0:56:31  lr: 0.000001  min_lr: 0.000000  loss: 4.1322 (4.1755)  class_acc: 0.3333 (0.3523)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2117  data: 0.7452  max mem: 15572
Epoch: [37]  [  20/2809]  eta: 0:40:20  lr: 0.000001  min_lr: 0.000000  loss: 4.1322 (4.2131)  class_acc: 0.2917 (0.3075)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0623  max mem: 15572
Epoch: [37]  [  30/2809]  eta: 0:36:12  lr: 0.000001  min_lr: 0.000000  loss: 4.2161 (4.1874)  class_acc: 0.3333 (0.3266)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.1173  max mem: 15572
Epoch: [37]  [  40/2809]  eta: 0:34:05  lr: 0.000001  min_lr: 0.000000  loss: 4.2161 (4.2192)  class_acc: 0.2917 (0.3140)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1603  max mem: 15572
Epoch: [37]  [  50/2809]  eta: 0:32:39  lr: 0.000001  min_lr: 0.000000  loss: 4.2122 (4.2040)  class_acc: 0.2500 (0.3186)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1419  max mem: 15572
Epoch: [37]  [  60/2809]  eta: 0:31:26  lr: 0.000001  min_lr: 0.000000  loss: 4.1505 (4.1991)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1347  max mem: 15572
[2025-01-16 07:55:56,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=104000, skipped=648, lr=[8.502632161242406e-09, 8.502632161242406e-09, 1.214661737320344e-08, 1.214661737320344e-08, 1.735231053314777e-08, 1.735231053314777e-08, 2.4789015047353962e-08, 2.4789015047353962e-08, 3.541287863907709e-08, 3.541287863907709e-08, 5.0589826627252987e-08, 5.0589826627252987e-08, 7.22711808960757e-08, 7.22711808960757e-08, 1.03244544137251e-07, 1.03244544137251e-07, 1.4749220591035857e-07, 1.4749220591035857e-07, 2.107031513005123e-07, 2.107031513005123e-07, 3.010045018578747e-07, 3.010045018578747e-07, 4.3000643122553533e-07, 4.3000643122553533e-07, 6.142949017507648e-07, 6.142949017507648e-07, 8.775641453582355e-07, 8.775641453582355e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:55:56,446] [INFO] [timer.py:260:stop] epoch=0/micro_step=104000/global_step=104000, RunningAvgSamplesPerSec=27.985365246077755, CurrSamplesPerSec=31.983488341945865, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [37]  [  70/2809]  eta: 0:30:31  lr: 0.000001  min_lr: 0.000000  loss: 4.1678 (4.1989)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1318  max mem: 15572
Epoch: [37]  [  80/2809]  eta: 0:29:48  lr: 0.000001  min_lr: 0.000000  loss: 4.2680 (4.1959)  class_acc: 0.3333 (0.3215)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1282  max mem: 15572
[2025-01-16 07:56:07,873] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:56:07,873] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [  90/2809]  eta: 0:29:16  lr: 0.000001  min_lr: 0.000000  loss: 4.2680 (4.1992)  class_acc: 0.3333 (0.3265)  loss_scale: 32768.0000 (34928.5275)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1301  max mem: 15572
Epoch: [37]  [ 100/2809]  eta: 0:29:01  lr: 0.000001  min_lr: 0.000000  loss: 4.2276 (4.2001)  class_acc: 0.3750 (0.3296)  loss_scale: 65536.0000 (37958.9703)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1539  max mem: 15572
Epoch: [37]  [ 110/2809]  eta: 0:28:17  lr: 0.000001  min_lr: 0.000000  loss: 4.1973 (4.1900)  class_acc: 0.3333 (0.3311)  loss_scale: 65536.0000 (40443.3874)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1145  max mem: 15572
Epoch: [37]  [ 120/2809]  eta: 0:28:23  lr: 0.000001  min_lr: 0.000000  loss: 4.2479 (4.2030)  class_acc: 0.3333 (0.3282)  loss_scale: 65536.0000 (42517.1570)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1397  max mem: 15572
Epoch: [37]  [ 130/2809]  eta: 0:27:49  lr: 0.000001  min_lr: 0.000000  loss: 4.2526 (4.2097)  class_acc: 0.2917 (0.3251)  loss_scale: 65536.0000 (44274.3206)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1314  max mem: 15572
Epoch: [37]  [ 140/2809]  eta: 0:27:27  lr: 0.000001  min_lr: 0.000000  loss: 4.1367 (4.2074)  class_acc: 0.2917 (0.3277)  loss_scale: 65536.0000 (45782.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0799  max mem: 15572
Epoch: [37]  [ 150/2809]  eta: 0:27:08  lr: 0.000001  min_lr: 0.000000  loss: 4.2227 (4.2141)  class_acc: 0.3750 (0.3292)  loss_scale: 65536.0000 (47090.4371)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1221  max mem: 15572
Epoch: [37]  [ 160/2809]  eta: 0:26:51  lr: 0.000001  min_lr: 0.000000  loss: 4.2116 (4.2087)  class_acc: 0.3750 (0.3333)  loss_scale: 65536.0000 (48236.1242)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1149  max mem: 15572
Epoch: [37]  [ 170/2809]  eta: 0:26:28  lr: 0.000001  min_lr: 0.000000  loss: 4.0988 (4.2023)  class_acc: 0.3750 (0.3353)  loss_scale: 65536.0000 (49247.8129)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0833  max mem: 15572
Epoch: [37]  [ 180/2809]  eta: 0:26:21  lr: 0.000001  min_lr: 0.000000  loss: 4.1328 (4.2035)  class_acc: 0.3333 (0.3331)  loss_scale: 65536.0000 (50147.7127)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.1004  max mem: 15572
Epoch: [37]  [ 190/2809]  eta: 0:26:27  lr: 0.000001  min_lr: 0.000000  loss: 4.2313 (4.2075)  class_acc: 0.2917 (0.3333)  loss_scale: 65536.0000 (50953.3822)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.2000  max mem: 15572
Epoch: [37]  [ 200/2809]  eta: 0:26:16  lr: 0.000001  min_lr: 0.000000  loss: 4.1517 (4.2041)  class_acc: 0.2917 (0.3306)  loss_scale: 65536.0000 (51678.8856)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1699  max mem: 15572
Epoch: [37]  [ 210/2809]  eta: 0:26:04  lr: 0.000001  min_lr: 0.000000  loss: 4.0914 (4.1980)  class_acc: 0.3333 (0.3321)  loss_scale: 65536.0000 (52335.6209)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1001  max mem: 15572
[2025-01-16 07:57:20,659] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:57:20,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:57:21,129] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104147
[2025-01-16 07:57:21,129] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:57:21,129] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [ 220/2809]  eta: 0:25:50  lr: 0.000001  min_lr: 0.000000  loss: 4.1275 (4.1947)  class_acc: 0.3333 (0.3307)  loss_scale: 65536.0000 (53229.4661)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.1023  max mem: 15572
Epoch: [37]  [ 230/2809]  eta: 0:25:35  lr: 0.000001  min_lr: 0.000000  loss: 4.2216 (4.2002)  class_acc: 0.2917 (0.3308)  loss_scale: 65536.0000 (53762.2165)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0880  max mem: 15572
Epoch: [37]  [ 240/2809]  eta: 0:25:28  lr: 0.000001  min_lr: 0.000000  loss: 4.2611 (4.1995)  class_acc: 0.2917 (0.3304)  loss_scale: 65536.0000 (54250.7552)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1008  max mem: 15572
Epoch: [37]  [ 250/2809]  eta: 0:25:09  lr: 0.000001  min_lr: 0.000000  loss: 4.2278 (4.2013)  class_acc: 0.2917 (0.3287)  loss_scale: 65536.0000 (54700.3665)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0739  max mem: 15572
Epoch: [37]  [ 260/2809]  eta: 0:25:01  lr: 0.000001  min_lr: 0.000000  loss: 4.1651 (4.1972)  class_acc: 0.3333 (0.3293)  loss_scale: 65536.0000 (55115.5249)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0866  max mem: 15572
[2025-01-16 07:57:48,905] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104198
[2025-01-16 07:57:48,906] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:57:48,906] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 270/2809]  eta: 0:24:54  lr: 0.000001  min_lr: 0.000000  loss: 4.1629 (4.1970)  class_acc: 0.3750 (0.3289)  loss_scale: 65536.0000 (54774.5535)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1388  max mem: 15572
Epoch: [37]  [ 280/2809]  eta: 0:24:46  lr: 0.000001  min_lr: 0.000000  loss: 4.1231 (4.1938)  class_acc: 0.3750 (0.3307)  loss_scale: 32768.0000 (53991.4021)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1370  max mem: 15572
Epoch: [37]  [ 290/2809]  eta: 0:24:42  lr: 0.000001  min_lr: 0.000000  loss: 4.1214 (4.1927)  class_acc: 0.3750 (0.3320)  loss_scale: 32768.0000 (53262.0756)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1497  max mem: 15572
Epoch: [37]  [ 300/2809]  eta: 0:24:43  lr: 0.000001  min_lr: 0.000000  loss: 4.2569 (4.1967)  class_acc: 0.3333 (0.3322)  loss_scale: 32768.0000 (52581.2093)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.1796  max mem: 15572
Epoch: [37]  [ 310/2809]  eta: 0:24:32  lr: 0.000001  min_lr: 0.000000  loss: 4.2911 (4.1964)  class_acc: 0.2500 (0.3298)  loss_scale: 32768.0000 (51944.1286)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1538  max mem: 15572
Epoch: [37]  [ 320/2809]  eta: 0:24:23  lr: 0.000001  min_lr: 0.000000  loss: 4.1670 (4.1962)  class_acc: 0.3333 (0.3315)  loss_scale: 32768.0000 (51346.7414)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.1025  max mem: 15572
Epoch: [37]  [ 330/2809]  eta: 0:24:14  lr: 0.000001  min_lr: 0.000000  loss: 4.1839 (4.1987)  class_acc: 0.3750 (0.3317)  loss_scale: 32768.0000 (50785.4502)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0982  max mem: 15572
Epoch: [37]  [ 340/2809]  eta: 0:24:09  lr: 0.000001  min_lr: 0.000000  loss: 4.1912 (4.2002)  class_acc: 0.2917 (0.3303)  loss_scale: 32768.0000 (50257.0792)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1238  max mem: 15572
Epoch: [37]  [ 350/2809]  eta: 0:24:03  lr: 0.000001  min_lr: 0.000000  loss: 4.0455 (4.1960)  class_acc: 0.2917 (0.3317)  loss_scale: 32768.0000 (49758.8148)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1337  max mem: 15572
Epoch: [37]  [ 360/2809]  eta: 0:23:54  lr: 0.000001  min_lr: 0.000000  loss: 4.0912 (4.1965)  class_acc: 0.3750 (0.3329)  loss_scale: 32768.0000 (49288.1551)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1056  max mem: 15572
Epoch: [37]  [ 370/2809]  eta: 0:23:52  lr: 0.000001  min_lr: 0.000000  loss: 4.1429 (4.1927)  class_acc: 0.3750 (0.3324)  loss_scale: 32768.0000 (48842.8679)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1304  max mem: 15572
Epoch: [37]  [ 380/2809]  eta: 0:23:45  lr: 0.000001  min_lr: 0.000000  loss: 4.0307 (4.1905)  class_acc: 0.3750 (0.3344)  loss_scale: 32768.0000 (48420.9554)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1440  max mem: 15572
Epoch: [37]  [ 390/2809]  eta: 0:23:43  lr: 0.000001  min_lr: 0.000000  loss: 4.1705 (4.1925)  class_acc: 0.3750 (0.3341)  loss_scale: 32768.0000 (48020.6240)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1607  max mem: 15572
[2025-01-16 07:59:03,704] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:59:03,705] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 07:59:05,493] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104331
[2025-01-16 07:59:05,493] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:59:05,493] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 400/2809]  eta: 0:23:31  lr: 0.000001  min_lr: 0.000000  loss: 4.2879 (4.1960)  class_acc: 0.3333 (0.3347)  loss_scale: 32768.0000 (47967.1222)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1252  max mem: 15572
Epoch: [37]  [ 410/2809]  eta: 0:23:24  lr: 0.000001  min_lr: 0.000000  loss: 4.3588 (4.2002)  class_acc: 0.2917 (0.3341)  loss_scale: 32768.0000 (47597.3139)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0796  max mem: 15572
Epoch: [37]  [ 420/2809]  eta: 0:23:17  lr: 0.000001  min_lr: 0.000000  loss: 4.2846 (4.2006)  class_acc: 0.3333 (0.3354)  loss_scale: 32768.0000 (47245.0736)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1167  max mem: 15572
Epoch: [37]  [ 430/2809]  eta: 0:23:13  lr: 0.000001  min_lr: 0.000000  loss: 3.9381 (4.1943)  class_acc: 0.3333 (0.3358)  loss_scale: 32768.0000 (46909.1787)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1315  max mem: 15572
Epoch: [37]  [ 440/2809]  eta: 0:23:02  lr: 0.000001  min_lr: 0.000000  loss: 4.0244 (4.1931)  class_acc: 0.2917 (0.3343)  loss_scale: 32768.0000 (46588.5170)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1008  max mem: 15572
Epoch: [37]  [ 450/2809]  eta: 0:22:55  lr: 0.000001  min_lr: 0.000000  loss: 4.1969 (4.1938)  class_acc: 0.2083 (0.3339)  loss_scale: 32768.0000 (46282.0754)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0865  max mem: 15572
Epoch: [37]  [ 460/2809]  eta: 0:22:53  lr: 0.000001  min_lr: 0.000000  loss: 4.2861 (4.1961)  class_acc: 0.2917 (0.3328)  loss_scale: 32768.0000 (45988.9284)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1613  max mem: 15572
Epoch: [37]  [ 470/2809]  eta: 0:22:45  lr: 0.000001  min_lr: 0.000000  loss: 4.3216 (4.1992)  class_acc: 0.2500 (0.3312)  loss_scale: 32768.0000 (45708.2293)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1387  max mem: 15572
Epoch: [37]  [ 480/2809]  eta: 0:22:38  lr: 0.000001  min_lr: 0.000000  loss: 4.2379 (4.1966)  class_acc: 0.2917 (0.3308)  loss_scale: 32768.0000 (45439.2017)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0970  max mem: 15572
Epoch: [37]  [ 490/2809]  eta: 0:22:29  lr: 0.000001  min_lr: 0.000000  loss: 4.0520 (4.1942)  class_acc: 0.2917 (0.3300)  loss_scale: 32768.0000 (45181.1324)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.1006  max mem: 15572
Epoch: [37]  [ 500/2809]  eta: 0:22:21  lr: 0.000001  min_lr: 0.000000  loss: 4.1784 (4.1950)  class_acc: 0.2917 (0.3290)  loss_scale: 32768.0000 (44933.3653)  weight_decay: 0.0500 (0.0500)  time: 0.5188  data: 0.0868  max mem: 15572
Epoch: [37]  [ 510/2809]  eta: 0:22:12  lr: 0.000001  min_lr: 0.000000  loss: 4.3045 (4.1957)  class_acc: 0.2500 (0.3283)  loss_scale: 32768.0000 (44695.2955)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0713  max mem: 15572
Epoch: [37]  [ 520/2809]  eta: 0:22:10  lr: 0.000001  min_lr: 0.000000  loss: 4.2424 (4.1962)  class_acc: 0.2500 (0.3277)  loss_scale: 32768.0000 (44466.3647)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1175  max mem: 15572
[2025-01-16 08:00:17,450] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:00:17,451] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 530/2809]  eta: 0:21:58  lr: 0.000001  min_lr: 0.000000  loss: 4.1644 (4.1950)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (44492.8964)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.0992  max mem: 15572
Epoch: [37]  [ 540/2809]  eta: 0:21:55  lr: 0.000001  min_lr: 0.000000  loss: 4.1644 (4.1949)  class_acc: 0.2917 (0.3259)  loss_scale: 65536.0000 (44881.8632)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0940  max mem: 15572
Epoch: [37]  [ 550/2809]  eta: 0:21:44  lr: 0.000001  min_lr: 0.000000  loss: 4.2955 (4.1963)  class_acc: 0.2917 (0.3259)  loss_scale: 65536.0000 (45256.7114)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.1167  max mem: 15572
Epoch: [37]  [ 560/2809]  eta: 0:21:39  lr: 0.000001  min_lr: 0.000000  loss: 4.2442 (4.1958)  class_acc: 0.2917 (0.3252)  loss_scale: 65536.0000 (45618.1961)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0962  max mem: 15572
Epoch: [37]  [ 570/2809]  eta: 0:21:28  lr: 0.000001  min_lr: 0.000000  loss: 4.1961 (4.1963)  class_acc: 0.2500 (0.3249)  loss_scale: 65536.0000 (45967.0193)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0682  max mem: 15572
Epoch: [37]  [ 580/2809]  eta: 0:21:18  lr: 0.000001  min_lr: 0.000000  loss: 4.2573 (4.1961)  class_acc: 0.2917 (0.3252)  loss_scale: 65536.0000 (46303.8348)  weight_decay: 0.0500 (0.0500)  time: 0.4602  data: 0.0006  max mem: 15572
[2025-01-16 08:00:50,314] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104522
[2025-01-16 08:00:50,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:00:50,314] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 590/2809]  eta: 0:21:13  lr: 0.000001  min_lr: 0.000000  loss: 4.1195 (4.1942)  class_acc: 0.2500 (0.3246)  loss_scale: 65536.0000 (46518.3621)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0648  max mem: 15572
Epoch: [37]  [ 600/2809]  eta: 0:21:07  lr: 0.000001  min_lr: 0.000000  loss: 4.0815 (4.1931)  class_acc: 0.2500 (0.3240)  loss_scale: 32768.0000 (46289.5707)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1196  max mem: 15572
Epoch: [37]  [ 610/2809]  eta: 0:21:04  lr: 0.000001  min_lr: 0.000000  loss: 4.2222 (4.1942)  class_acc: 0.3333 (0.3243)  loss_scale: 32768.0000 (46068.2684)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1503  max mem: 15572
Epoch: [37]  [ 620/2809]  eta: 0:20:55  lr: 0.000001  min_lr: 0.000000  loss: 4.2222 (4.1928)  class_acc: 0.3333 (0.3241)  loss_scale: 32768.0000 (45854.0934)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1265  max mem: 15572
Epoch: [37]  [ 630/2809]  eta: 0:20:50  lr: 0.000001  min_lr: 0.000000  loss: 4.2523 (4.1921)  class_acc: 0.2500 (0.3236)  loss_scale: 32768.0000 (45646.7068)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0856  max mem: 15572
Epoch: [37]  [ 640/2809]  eta: 0:20:45  lr: 0.000001  min_lr: 0.000000  loss: 4.2820 (4.1925)  class_acc: 0.3333 (0.3240)  loss_scale: 32768.0000 (45445.7910)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1227  max mem: 15572
Epoch: [37]  [ 650/2809]  eta: 0:20:37  lr: 0.000001  min_lr: 0.000000  loss: 4.2820 (4.1940)  class_acc: 0.3333 (0.3248)  loss_scale: 32768.0000 (45251.0476)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1032  max mem: 15572
Epoch: [37]  [ 660/2809]  eta: 0:20:35  lr: 0.000001  min_lr: 0.000000  loss: 4.2469 (4.1934)  class_acc: 0.3333 (0.3255)  loss_scale: 32768.0000 (45062.1967)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1576  max mem: 15572
Epoch: [37]  [ 670/2809]  eta: 0:20:31  lr: 0.000001  min_lr: 0.000000  loss: 4.1589 (4.1909)  class_acc: 0.3333 (0.3253)  loss_scale: 32768.0000 (44878.9747)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.1955  max mem: 15572
Epoch: [37]  [ 680/2809]  eta: 0:20:22  lr: 0.000001  min_lr: 0.000000  loss: 4.1149 (4.1897)  class_acc: 0.2500 (0.3242)  loss_scale: 32768.0000 (44701.1336)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0829  max mem: 15572
Epoch: [37]  [ 690/2809]  eta: 0:20:15  lr: 0.000001  min_lr: 0.000000  loss: 4.2255 (4.1913)  class_acc: 0.3333 (0.3247)  loss_scale: 32768.0000 (44528.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5051  data: 0.0558  max mem: 15572
Epoch: [37]  [ 700/2809]  eta: 0:20:08  lr: 0.000001  min_lr: 0.000000  loss: 4.2015 (4.1904)  class_acc: 0.3333 (0.3244)  loss_scale: 32768.0000 (44360.6733)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0898  max mem: 15572
Epoch: [37]  [ 710/2809]  eta: 0:20:00  lr: 0.000001  min_lr: 0.000000  loss: 4.0901 (4.1884)  class_acc: 0.2917 (0.3256)  loss_scale: 32768.0000 (44197.6259)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0773  max mem: 15572
[2025-01-16 08:02:02,758] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:02:02,758] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 720/2809]  eta: 0:19:53  lr: 0.000001  min_lr: 0.000000  loss: 4.1171 (4.1892)  class_acc: 0.3333 (0.3251)  loss_scale: 32768.0000 (44175.4452)  weight_decay: 0.0500 (0.0500)  time: 0.5227  data: 0.0730  max mem: 15572
Epoch: [37]  [ 730/2809]  eta: 0:19:47  lr: 0.000001  min_lr: 0.000000  loss: 4.2085 (4.1899)  class_acc: 0.3333 (0.3258)  loss_scale: 65536.0000 (44467.6553)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0835  max mem: 15572
[2025-01-16 08:02:12,128] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104667
[2025-01-16 08:02:12,128] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:02:12,128] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 740/2809]  eta: 0:19:41  lr: 0.000001  min_lr: 0.000000  loss: 4.1285 (4.1887)  class_acc: 0.4167 (0.3265)  loss_scale: 65536.0000 (44442.4291)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1064  max mem: 15572
Epoch: [37]  [ 750/2809]  eta: 0:19:34  lr: 0.000001  min_lr: 0.000000  loss: 4.1629 (4.1904)  class_acc: 0.3333 (0.3260)  loss_scale: 32768.0000 (44286.9774)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0952  max mem: 15572
Epoch: [37]  [ 760/2809]  eta: 0:19:29  lr: 0.000001  min_lr: 0.000000  loss: 4.2103 (4.1896)  class_acc: 0.2917 (0.3265)  loss_scale: 32768.0000 (44135.6110)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1080  max mem: 15572
Epoch: [37]  [ 770/2809]  eta: 0:19:23  lr: 0.000001  min_lr: 0.000000  loss: 4.2103 (4.1895)  class_acc: 0.3333 (0.3262)  loss_scale: 32768.0000 (43988.1712)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1317  max mem: 15572
Epoch: [37]  [ 780/2809]  eta: 0:19:17  lr: 0.000001  min_lr: 0.000000  loss: 4.2585 (4.1898)  class_acc: 0.2917 (0.3257)  loss_scale: 32768.0000 (43844.5070)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1167  max mem: 15572
Epoch: [37]  [ 790/2809]  eta: 0:19:10  lr: 0.000001  min_lr: 0.000000  loss: 4.3331 (4.1926)  class_acc: 0.2500 (0.3250)  loss_scale: 32768.0000 (43704.4753)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0956  max mem: 15572
Epoch: [37]  [ 800/2809]  eta: 0:19:05  lr: 0.000001  min_lr: 0.000000  loss: 4.2108 (4.1918)  class_acc: 0.2917 (0.3255)  loss_scale: 32768.0000 (43567.9401)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1080  max mem: 15572
Epoch: [37]  [ 810/2809]  eta: 0:18:57  lr: 0.000001  min_lr: 0.000000  loss: 4.1545 (4.1907)  class_acc: 0.3750 (0.3255)  loss_scale: 32768.0000 (43434.7719)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0838  max mem: 15572
Epoch: [37]  [ 820/2809]  eta: 0:18:50  lr: 0.000001  min_lr: 0.000000  loss: 4.2502 (4.1908)  class_acc: 0.2917 (0.3253)  loss_scale: 32768.0000 (43304.8477)  weight_decay: 0.0500 (0.0500)  time: 0.4967  data: 0.0645  max mem: 15572
Epoch: [37]  [ 830/2809]  eta: 0:18:47  lr: 0.000001  min_lr: 0.000000  loss: 4.3195 (4.1923)  class_acc: 0.2917 (0.3249)  loss_scale: 32768.0000 (43178.0505)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1542  max mem: 15572
Epoch: [37]  [ 840/2809]  eta: 0:18:40  lr: 0.000001  min_lr: 0.000000  loss: 4.2516 (4.1926)  class_acc: 0.2500 (0.3248)  loss_scale: 32768.0000 (43054.2687)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1345  max mem: 15572
Epoch: [37]  [ 850/2809]  eta: 0:18:33  lr: 0.000001  min_lr: 0.000000  loss: 4.2516 (4.1940)  class_acc: 0.2500 (0.3239)  loss_scale: 32768.0000 (42933.3960)  weight_decay: 0.0500 (0.0500)  time: 0.5184  data: 0.0561  max mem: 15572
Epoch: [37]  [ 860/2809]  eta: 0:18:26  lr: 0.000001  min_lr: 0.000000  loss: 4.1356 (4.1932)  class_acc: 0.2917 (0.3240)  loss_scale: 32768.0000 (42815.3310)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0623  max mem: 15572
[2025-01-16 08:03:22,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:03:22,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 870/2809]  eta: 0:18:20  lr: 0.000001  min_lr: 0.000000  loss: 4.1108 (4.1934)  class_acc: 0.3333 (0.3246)  loss_scale: 32768.0000 (43000.9460)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0929  max mem: 15572
Epoch: [37]  [ 880/2809]  eta: 0:18:14  lr: 0.000001  min_lr: 0.000000  loss: 4.1945 (4.1925)  class_acc: 0.3750 (0.3253)  loss_scale: 65536.0000 (43256.7355)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1211  max mem: 15572
[2025-01-16 08:03:37,278] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104822
[2025-01-16 08:03:37,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:03:37,280] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 890/2809]  eta: 0:18:09  lr: 0.000001  min_lr: 0.000000  loss: 4.1396 (4.1926)  class_acc: 0.3333 (0.3262)  loss_scale: 65536.0000 (43433.2301)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1415  max mem: 15572
Epoch: [37]  [ 900/2809]  eta: 0:18:04  lr: 0.000001  min_lr: 0.000000  loss: 4.1749 (4.1925)  class_acc: 0.2917 (0.3260)  loss_scale: 32768.0000 (43314.8590)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1396  max mem: 15572
Epoch: [37]  [ 910/2809]  eta: 0:17:58  lr: 0.000001  min_lr: 0.000000  loss: 4.1431 (4.1910)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (43199.0867)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1224  max mem: 15572
Epoch: [37]  [ 920/2809]  eta: 0:17:56  lr: 0.000001  min_lr: 0.000000  loss: 4.1162 (4.1924)  class_acc: 0.2917 (0.3262)  loss_scale: 32768.0000 (43085.8284)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.2110  max mem: 15572
Epoch: [37]  [ 930/2809]  eta: 0:17:49  lr: 0.000001  min_lr: 0.000000  loss: 4.2470 (4.1918)  class_acc: 0.2917 (0.3261)  loss_scale: 32768.0000 (42975.0032)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.2017  max mem: 15572
Epoch: [37]  [ 940/2809]  eta: 0:17:45  lr: 0.000001  min_lr: 0.000000  loss: 4.0605 (4.1907)  class_acc: 0.2917 (0.3262)  loss_scale: 32768.0000 (42866.5335)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1617  max mem: 15572
Epoch: [37]  [ 950/2809]  eta: 0:17:39  lr: 0.000001  min_lr: 0.000000  loss: 3.9675 (4.1889)  class_acc: 0.2917 (0.3258)  loss_scale: 32768.0000 (42760.3449)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1588  max mem: 15572
Epoch: [37]  [ 960/2809]  eta: 0:17:33  lr: 0.000001  min_lr: 0.000000  loss: 4.2007 (4.1896)  class_acc: 0.2917 (0.3251)  loss_scale: 32768.0000 (42656.3663)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0997  max mem: 15572
Epoch: [37]  [ 970/2809]  eta: 0:17:25  lr: 0.000001  min_lr: 0.000000  loss: 4.2096 (4.1896)  class_acc: 0.2917 (0.3252)  loss_scale: 32768.0000 (42554.5294)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0564  max mem: 15572
Epoch: [37]  [ 980/2809]  eta: 0:17:21  lr: 0.000001  min_lr: 0.000000  loss: 4.0561 (4.1882)  class_acc: 0.3333 (0.3252)  loss_scale: 32768.0000 (42454.7686)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0875  max mem: 15572
Epoch: [37]  [ 990/2809]  eta: 0:17:14  lr: 0.000001  min_lr: 0.000000  loss: 4.1140 (4.1876)  class_acc: 0.3333 (0.3260)  loss_scale: 32768.0000 (42357.0212)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1329  max mem: 15572
Epoch: [37]  [1000/2809]  eta: 0:17:06  lr: 0.000001  min_lr: 0.000000  loss: 4.1466 (4.1880)  class_acc: 0.3333 (0.3256)  loss_scale: 32768.0000 (42261.2268)  weight_decay: 0.0500 (0.0500)  time: 0.4872  data: 0.0459  max mem: 15572
Epoch: [37]  [1010/2809]  eta: 0:17:01  lr: 0.000001  min_lr: 0.000000  loss: 4.1783 (4.1882)  class_acc: 0.2917 (0.3256)  loss_scale: 32768.0000 (42167.3274)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0793  max mem: 15572
[2025-01-16 08:04:51,713] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:04:51,714] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [1020/2809]  eta: 0:16:58  lr: 0.000001  min_lr: 0.000000  loss: 4.2103 (4.1874)  class_acc: 0.3333 (0.3255)  loss_scale: 32768.0000 (42171.5495)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.1999  max mem: 15572
Epoch: [37]  [1030/2809]  eta: 0:16:51  lr: 0.000001  min_lr: 0.000000  loss: 4.1969 (4.1874)  class_acc: 0.2917 (0.3256)  loss_scale: 65536.0000 (42398.1688)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1571  max mem: 15572
[2025-01-16 08:04:58,515] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104964
[2025-01-16 08:04:58,516] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:04:58,516] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [1040/2809]  eta: 0:16:46  lr: 0.000001  min_lr: 0.000000  loss: 4.2085 (4.1869)  class_acc: 0.2917 (0.3253)  loss_scale: 32768.0000 (42305.6599)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1050  max mem: 15572
Epoch: [37]  [1050/2809]  eta: 0:16:42  lr: 0.000001  min_lr: 0.000000  loss: 4.2766 (4.1886)  class_acc: 0.2500 (0.3250)  loss_scale: 32768.0000 (42214.9115)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1537  max mem: 15572
Epoch: [37]  [1060/2809]  eta: 0:16:34  lr: 0.000001  min_lr: 0.000000  loss: 4.1789 (4.1888)  class_acc: 0.2917 (0.3251)  loss_scale: 32768.0000 (42125.8737)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1010  max mem: 15572
[2025-01-16 08:05:19,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=105000, skipped=655, lr=[6.70088453430434e-09, 6.70088453430434e-09, 9.572692191863344e-09, 9.572692191863344e-09, 1.367527455980478e-08, 1.367527455980478e-08, 1.9536106514006828e-08, 1.9536106514006828e-08, 2.7908723591438326e-08, 2.7908723591438326e-08, 3.986960513062619e-08, 3.986960513062619e-08, 5.695657875803741e-08, 5.695657875803741e-08, 8.13665410829106e-08, 8.13665410829106e-08, 1.1623791583272941e-07, 1.1623791583272941e-07, 1.6605416547532776e-07, 1.6605416547532776e-07, 2.3722023639332537e-07, 2.3722023639332537e-07, 3.3888605199046486e-07, 3.3888605199046486e-07, 4.841229314149498e-07, 4.841229314149498e-07, 6.916041877356426e-07, 6.916041877356426e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:05:19,765] [INFO] [timer.py:260:stop] epoch=0/micro_step=105000/global_step=105000, RunningAvgSamplesPerSec=27.98905226423606, CurrSamplesPerSec=27.28779083479039, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [37]  [1070/2809]  eta: 0:16:29  lr: 0.000001  min_lr: 0.000000  loss: 4.1789 (4.1902)  class_acc: 0.2500 (0.3240)  loss_scale: 32768.0000 (42038.4986)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.1067  max mem: 15572
Epoch: [37]  [1080/2809]  eta: 0:16:23  lr: 0.000001  min_lr: 0.000000  loss: 4.1584 (4.1896)  class_acc: 0.2500 (0.3240)  loss_scale: 32768.0000 (41952.7401)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1331  max mem: 15572
Epoch: [37]  [1090/2809]  eta: 0:16:17  lr: 0.000001  min_lr: 0.000000  loss: 4.1593 (4.1901)  class_acc: 0.2917 (0.3237)  loss_scale: 32768.0000 (41868.5536)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.1028  max mem: 15572
Epoch: [37]  [1100/2809]  eta: 0:16:10  lr: 0.000001  min_lr: 0.000000  loss: 4.1593 (4.1890)  class_acc: 0.3750 (0.3246)  loss_scale: 32768.0000 (41785.8965)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.1034  max mem: 15572
Epoch: [37]  [1110/2809]  eta: 0:16:04  lr: 0.000001  min_lr: 0.000000  loss: 4.1431 (4.1895)  class_acc: 0.3333 (0.3245)  loss_scale: 32768.0000 (41704.7273)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0798  max mem: 15572
Epoch: [37]  [1120/2809]  eta: 0:15:58  lr: 0.000001  min_lr: 0.000000  loss: 4.1322 (4.1887)  class_acc: 0.3333 (0.3252)  loss_scale: 32768.0000 (41625.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0985  max mem: 15572
Epoch: [37]  [1130/2809]  eta: 0:15:53  lr: 0.000001  min_lr: 0.000000  loss: 4.1092 (4.1889)  class_acc: 0.3750 (0.3256)  loss_scale: 32768.0000 (41546.6950)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1296  max mem: 15572
Epoch: [37]  [1140/2809]  eta: 0:15:48  lr: 0.000001  min_lr: 0.000000  loss: 4.1265 (4.1891)  class_acc: 0.3333 (0.3258)  loss_scale: 32768.0000 (41469.7564)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1532  max mem: 15572
Epoch: [37]  [1150/2809]  eta: 0:15:42  lr: 0.000001  min_lr: 0.000000  loss: 4.3388 (4.1893)  class_acc: 0.3333 (0.3260)  loss_scale: 32768.0000 (41394.1546)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1135  max mem: 15572
[2025-01-16 08:06:10,293] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:06:10,293] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [1160/2809]  eta: 0:15:35  lr: 0.000001  min_lr: 0.000000  loss: 4.3037 (4.1894)  class_acc: 0.3333 (0.3263)  loss_scale: 32768.0000 (41348.0792)  weight_decay: 0.0500 (0.0500)  time: 0.4974  data: 0.0285  max mem: 15572
Epoch: [37]  [1170/2809]  eta: 0:15:30  lr: 0.000001  min_lr: 0.000000  loss: 4.1959 (4.1892)  class_acc: 0.2917 (0.3257)  loss_scale: 65536.0000 (41554.6371)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0943  max mem: 15572
Epoch: [37]  [1180/2809]  eta: 0:15:23  lr: 0.000001  min_lr: 0.000000  loss: 4.1495 (4.1888)  class_acc: 0.2917 (0.3256)  loss_scale: 65536.0000 (41757.6969)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1105  max mem: 15572
Epoch: [37]  [1190/2809]  eta: 0:15:19  lr: 0.000001  min_lr: 0.000000  loss: 4.1622 (4.1888)  class_acc: 0.2917 (0.3254)  loss_scale: 65536.0000 (41957.3468)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1370  max mem: 15572
Epoch: [37]  [1200/2809]  eta: 0:15:12  lr: 0.000001  min_lr: 0.000000  loss: 4.2941 (4.1901)  class_acc: 0.2500 (0.3250)  loss_scale: 65536.0000 (42153.6719)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1244  max mem: 15572
Epoch: [37]  [1210/2809]  eta: 0:15:07  lr: 0.000001  min_lr: 0.000000  loss: 4.3395 (4.1905)  class_acc: 0.2500 (0.3245)  loss_scale: 65536.0000 (42346.7547)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0662  max mem: 15572
Epoch: [37]  [1220/2809]  eta: 0:15:00  lr: 0.000001  min_lr: 0.000000  loss: 4.1701 (4.1902)  class_acc: 0.2500 (0.3241)  loss_scale: 65536.0000 (42536.6749)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0894  max mem: 15572
Epoch: [37]  [1230/2809]  eta: 0:14:54  lr: 0.000001  min_lr: 0.000000  loss: 4.1918 (4.1907)  class_acc: 0.3333 (0.3246)  loss_scale: 65536.0000 (42723.5093)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0718  max mem: 15572
Epoch: [37]  [1240/2809]  eta: 0:14:49  lr: 0.000001  min_lr: 0.000000  loss: 4.1636 (4.1904)  class_acc: 0.3333 (0.3248)  loss_scale: 65536.0000 (42907.3328)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1309  max mem: 15572
Epoch: [37]  [1250/2809]  eta: 0:14:43  lr: 0.000001  min_lr: 0.000000  loss: 4.0930 (4.1895)  class_acc: 0.4167 (0.3251)  loss_scale: 65536.0000 (43088.2174)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1242  max mem: 15572
Epoch: [37]  [1260/2809]  eta: 0:14:38  lr: 0.000001  min_lr: 0.000000  loss: 4.0930 (4.1895)  class_acc: 0.3750 (0.3253)  loss_scale: 65536.0000 (43266.2331)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1391  max mem: 15572
Epoch: [37]  [1270/2809]  eta: 0:14:31  lr: 0.000001  min_lr: 0.000000  loss: 4.1719 (4.1892)  class_acc: 0.2917 (0.3246)  loss_scale: 65536.0000 (43441.4477)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1128  max mem: 15572
Epoch: [37]  [1280/2809]  eta: 0:14:26  lr: 0.000001  min_lr: 0.000000  loss: 4.2419 (4.1894)  class_acc: 0.2500 (0.3241)  loss_scale: 65536.0000 (43613.9266)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.1129  max mem: 15572
[2025-01-16 08:07:21,962] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:07:21,962] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:07:22,467] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105222
[2025-01-16 08:07:22,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:07:22,468] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [1290/2809]  eta: 0:14:20  lr: 0.000001  min_lr: 0.000000  loss: 4.2519 (4.1894)  class_acc: 0.2500 (0.3239)  loss_scale: 65536.0000 (43834.4973)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1182  max mem: 15572
Epoch: [37]  [1300/2809]  eta: 0:14:14  lr: 0.000001  min_lr: 0.000000  loss: 4.2781 (4.1905)  class_acc: 0.3333 (0.3241)  loss_scale: 65536.0000 (44001.3036)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0788  max mem: 15572
Epoch: [37]  [1310/2809]  eta: 0:14:08  lr: 0.000001  min_lr: 0.000000  loss: 4.2364 (4.1892)  class_acc: 0.3333 (0.3241)  loss_scale: 65536.0000 (44165.5652)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.1183  max mem: 15572
Epoch: [37]  [1320/2809]  eta: 0:14:03  lr: 0.000001  min_lr: 0.000000  loss: 4.1421 (4.1893)  class_acc: 0.2917 (0.3241)  loss_scale: 65536.0000 (44327.3399)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1511  max mem: 15572
[2025-01-16 08:07:42,065] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105257
[2025-01-16 08:07:42,065] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:07:42,065] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [1330/2809]  eta: 0:13:56  lr: 0.000001  min_lr: 0.000000  loss: 4.2052 (4.1898)  class_acc: 0.2917 (0.3240)  loss_scale: 65536.0000 (44314.3501)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1206  max mem: 15572
Epoch: [37]  [1340/2809]  eta: 0:13:51  lr: 0.000001  min_lr: 0.000000  loss: 4.1471 (4.1884)  class_acc: 0.3333 (0.3242)  loss_scale: 32768.0000 (44228.2476)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1167  max mem: 15572
Epoch: [37]  [1350/2809]  eta: 0:13:46  lr: 0.000001  min_lr: 0.000000  loss: 4.2003 (4.1892)  class_acc: 0.3333 (0.3240)  loss_scale: 32768.0000 (44143.4197)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1861  max mem: 15572
Epoch: [37]  [1360/2809]  eta: 0:13:41  lr: 0.000001  min_lr: 0.000000  loss: 4.1990 (4.1880)  class_acc: 0.3333 (0.3241)  loss_scale: 32768.0000 (44059.8384)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.1712  max mem: 15572
Epoch: [37]  [1370/2809]  eta: 0:13:36  lr: 0.000001  min_lr: 0.000000  loss: 4.0075 (4.1879)  class_acc: 0.2917 (0.3240)  loss_scale: 32768.0000 (43977.4763)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1607  max mem: 15572
Epoch: [37]  [1380/2809]  eta: 0:13:32  lr: 0.000001  min_lr: 0.000000  loss: 4.0885 (4.1870)  class_acc: 0.2917 (0.3242)  loss_scale: 32768.0000 (43896.3070)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.2056  max mem: 15572
Epoch: [37]  [1390/2809]  eta: 0:13:26  lr: 0.000001  min_lr: 0.000000  loss: 4.1594 (4.1864)  class_acc: 0.2917 (0.3239)  loss_scale: 32768.0000 (43816.3048)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1893  max mem: 15572
Epoch: [37]  [1400/2809]  eta: 0:13:20  lr: 0.000001  min_lr: 0.000000  loss: 4.2482 (4.1863)  class_acc: 0.2917 (0.3238)  loss_scale: 32768.0000 (43737.4447)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.1242  max mem: 15572
Epoch: [37]  [1410/2809]  eta: 0:13:14  lr: 0.000001  min_lr: 0.000000  loss: 4.2652 (4.1861)  class_acc: 0.2917 (0.3236)  loss_scale: 32768.0000 (43659.7023)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0913  max mem: 15572
Epoch: [37]  [1420/2809]  eta: 0:13:08  lr: 0.000001  min_lr: 0.000000  loss: 4.1503 (4.1858)  class_acc: 0.2917 (0.3239)  loss_scale: 32768.0000 (43583.0542)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0791  max mem: 15572
Epoch: [37]  [1430/2809]  eta: 0:13:02  lr: 0.000001  min_lr: 0.000000  loss: 4.2432 (4.1862)  class_acc: 0.3333 (0.3240)  loss_scale: 32768.0000 (43507.4773)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1037  max mem: 15572
Epoch: [37]  [1440/2809]  eta: 0:12:56  lr: 0.000001  min_lr: 0.000000  loss: 4.1801 (4.1862)  class_acc: 0.2917 (0.3238)  loss_scale: 32768.0000 (43432.9493)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0660  max mem: 15572
Epoch: [37]  [1450/2809]  eta: 0:12:50  lr: 0.000001  min_lr: 0.000000  loss: 4.1317 (4.1858)  class_acc: 0.2917 (0.3239)  loss_scale: 32768.0000 (43359.4487)  weight_decay: 0.0500 (0.0500)  time: 0.5132  data: 0.0580  max mem: 15572
[2025-01-16 08:08:55,688] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:08:55,688] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [1460/2809]  eta: 0:12:43  lr: 0.000001  min_lr: 0.000000  loss: 4.1394 (4.1857)  class_acc: 0.2917 (0.3235)  loss_scale: 32768.0000 (43466.3819)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0712  max mem: 15572
Epoch: [37]  [1470/2809]  eta: 0:12:38  lr: 0.000001  min_lr: 0.000000  loss: 4.2186 (4.1865)  class_acc: 0.2500 (0.3231)  loss_scale: 65536.0000 (43616.4133)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0720  max mem: 15572
[2025-01-16 08:09:08,108] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105409
[2025-01-16 08:09:08,109] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:09:08,109] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [1480/2809]  eta: 0:12:32  lr: 0.000001  min_lr: 0.000000  loss: 4.2419 (4.1872)  class_acc: 0.2083 (0.3226)  loss_scale: 65536.0000 (43653.7907)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0971  max mem: 15572
Epoch: [37]  [1490/2809]  eta: 0:12:27  lr: 0.000001  min_lr: 0.000000  loss: 4.2227 (4.1870)  class_acc: 0.2083 (0.3223)  loss_scale: 32768.0000 (43580.7807)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1383  max mem: 15572
Epoch: [37]  [1500/2809]  eta: 0:12:21  lr: 0.000001  min_lr: 0.000000  loss: 4.2370 (4.1884)  class_acc: 0.2917 (0.3225)  loss_scale: 32768.0000 (43508.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1027  max mem: 15572
Epoch: [37]  [1510/2809]  eta: 0:12:15  lr: 0.000001  min_lr: 0.000000  loss: 4.2628 (4.1885)  class_acc: 0.3333 (0.3225)  loss_scale: 32768.0000 (43437.6598)  weight_decay: 0.0500 (0.0500)  time: 0.5227  data: 0.0461  max mem: 15572
Epoch: [37]  [1520/2809]  eta: 0:12:09  lr: 0.000001  min_lr: 0.000000  loss: 4.2176 (4.1886)  class_acc: 0.3750 (0.3226)  loss_scale: 32768.0000 (43367.5108)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.1031  max mem: 15572
Epoch: [37]  [1530/2809]  eta: 0:12:04  lr: 0.000001  min_lr: 0.000000  loss: 4.2176 (4.1882)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (43298.2782)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1755  max mem: 15572
Epoch: [37]  [1540/2809]  eta: 0:11:59  lr: 0.000001  min_lr: 0.000000  loss: 4.1475 (4.1876)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (43229.9442)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1780  max mem: 15572
Epoch: [37]  [1550/2809]  eta: 0:11:53  lr: 0.000001  min_lr: 0.000000  loss: 4.2370 (4.1886)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (43162.4913)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1240  max mem: 15572
Epoch: [37]  [1560/2809]  eta: 0:11:47  lr: 0.000001  min_lr: 0.000000  loss: 4.3270 (4.1883)  class_acc: 0.3333 (0.3228)  loss_scale: 32768.0000 (43095.9026)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1050  max mem: 15572
Epoch: [37]  [1570/2809]  eta: 0:11:42  lr: 0.000001  min_lr: 0.000000  loss: 4.2169 (4.1879)  class_acc: 0.3333 (0.3231)  loss_scale: 32768.0000 (43030.1617)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1084  max mem: 15572
Epoch: [37]  [1580/2809]  eta: 0:11:37  lr: 0.000001  min_lr: 0.000000  loss: 4.1084 (4.1872)  class_acc: 0.3333 (0.3233)  loss_scale: 32768.0000 (42965.2524)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1642  max mem: 15572
Epoch: [37]  [1590/2809]  eta: 0:11:30  lr: 0.000001  min_lr: 0.000000  loss: 4.2674 (4.1877)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (42901.1590)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1439  max mem: 15572
Epoch: [37]  [1600/2809]  eta: 0:11:25  lr: 0.000001  min_lr: 0.000000  loss: 4.2816 (4.1873)  class_acc: 0.2917 (0.3231)  loss_scale: 32768.0000 (42837.8663)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0998  max mem: 15572
[2025-01-16 08:10:21,800] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:10:21,801] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [1610/2809]  eta: 0:11:19  lr: 0.000001  min_lr: 0.000000  loss: 4.1991 (4.1873)  class_acc: 0.2917 (0.3230)  loss_scale: 32768.0000 (42897.4004)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1166  max mem: 15572
Epoch: [37]  [1620/2809]  eta: 0:11:14  lr: 0.000001  min_lr: 0.000000  loss: 4.3063 (4.1876)  class_acc: 0.2917 (0.3228)  loss_scale: 65536.0000 (43037.0586)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1377  max mem: 15572
Epoch: [37]  [1630/2809]  eta: 0:11:09  lr: 0.000001  min_lr: 0.000000  loss: 4.2698 (4.1879)  class_acc: 0.2917 (0.3228)  loss_scale: 65536.0000 (43175.0043)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1664  max mem: 15572
Epoch: [37]  [1640/2809]  eta: 0:11:03  lr: 0.000001  min_lr: 0.000000  loss: 4.1931 (4.1875)  class_acc: 0.2917 (0.3228)  loss_scale: 65536.0000 (43311.2687)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1451  max mem: 15572
Epoch: [37]  [1650/2809]  eta: 0:10:57  lr: 0.000001  min_lr: 0.000000  loss: 4.1291 (4.1872)  class_acc: 0.3333 (0.3230)  loss_scale: 65536.0000 (43445.8825)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1136  max mem: 15572
Epoch: [37]  [1660/2809]  eta: 0:10:51  lr: 0.000001  min_lr: 0.000000  loss: 4.1291 (4.1868)  class_acc: 0.3333 (0.3229)  loss_scale: 65536.0000 (43578.8754)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0630  max mem: 15572
Epoch: [37]  [1670/2809]  eta: 0:10:44  lr: 0.000001  min_lr: 0.000000  loss: 4.2071 (4.1867)  class_acc: 0.3750 (0.3230)  loss_scale: 65536.0000 (43710.2765)  weight_decay: 0.0500 (0.0500)  time: 0.4622  data: 0.0304  max mem: 15572
[2025-01-16 08:10:59,982] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105607
[2025-01-16 08:10:59,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:10:59,983] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [1680/2809]  eta: 0:10:39  lr: 0.000001  min_lr: 0.000000  loss: 4.1845 (4.1870)  class_acc: 0.3750 (0.3233)  loss_scale: 65536.0000 (43703.6621)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0696  max mem: 15572
Epoch: [37]  [1690/2809]  eta: 0:10:33  lr: 0.000001  min_lr: 0.000000  loss: 4.2505 (4.1872)  class_acc: 0.3333 (0.3233)  loss_scale: 32768.0000 (43638.9923)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0763  max mem: 15572
Epoch: [37]  [1700/2809]  eta: 0:10:27  lr: 0.000001  min_lr: 0.000000  loss: 4.2465 (4.1869)  class_acc: 0.2917 (0.3232)  loss_scale: 32768.0000 (43575.0829)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0494  max mem: 15572
Epoch: [37]  [1710/2809]  eta: 0:10:22  lr: 0.000001  min_lr: 0.000000  loss: 4.1324 (4.1862)  class_acc: 0.2500 (0.3234)  loss_scale: 32768.0000 (43511.9205)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1150  max mem: 15572
Epoch: [37]  [1720/2809]  eta: 0:10:16  lr: 0.000001  min_lr: 0.000000  loss: 4.1324 (4.1858)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (43449.4922)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.1720  max mem: 15572
Epoch: [37]  [1730/2809]  eta: 0:10:11  lr: 0.000001  min_lr: 0.000000  loss: 4.2417 (4.1860)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (43387.7851)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1721  max mem: 15572
Epoch: [37]  [1740/2809]  eta: 0:10:06  lr: 0.000001  min_lr: 0.000000  loss: 4.1583 (4.1858)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (43326.7869)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1723  max mem: 15572
Epoch: [37]  [1750/2809]  eta: 0:09:59  lr: 0.000001  min_lr: 0.000000  loss: 4.0912 (4.1861)  class_acc: 0.3333 (0.3236)  loss_scale: 32768.0000 (43266.4854)  weight_decay: 0.0500 (0.0500)  time: 0.5287  data: 0.0796  max mem: 15572
Epoch: [37]  [1760/2809]  eta: 0:09:54  lr: 0.000001  min_lr: 0.000000  loss: 4.0622 (4.1859)  class_acc: 0.2917 (0.3235)  loss_scale: 32768.0000 (43206.8688)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0859  max mem: 15572
Epoch: [37]  [1770/2809]  eta: 0:09:48  lr: 0.000001  min_lr: 0.000000  loss: 4.1899 (4.1862)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (43147.9255)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1491  max mem: 15572
Epoch: [37]  [1780/2809]  eta: 0:09:42  lr: 0.000001  min_lr: 0.000000  loss: 4.1899 (4.1862)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (43089.6440)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1112  max mem: 15572
Epoch: [37]  [1790/2809]  eta: 0:09:36  lr: 0.000001  min_lr: 0.000000  loss: 4.1678 (4.1864)  class_acc: 0.2917 (0.3234)  loss_scale: 32768.0000 (43032.0134)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0762  max mem: 15572
Epoch: [37]  [1800/2809]  eta: 0:09:31  lr: 0.000001  min_lr: 0.000000  loss: 4.1932 (4.1864)  class_acc: 0.2917 (0.3236)  loss_scale: 32768.0000 (42975.0228)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1003  max mem: 15572
[2025-01-16 08:12:13,217] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:12:13,217] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [1810/2809]  eta: 0:09:26  lr: 0.000001  min_lr: 0.000000  loss: 4.2366 (4.1862)  class_acc: 0.3333 (0.3237)  loss_scale: 32768.0000 (43063.4125)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1680  max mem: 15572
Epoch: [37]  [1820/2809]  eta: 0:09:20  lr: 0.000001  min_lr: 0.000000  loss: 4.1531 (4.1863)  class_acc: 0.2917 (0.3234)  loss_scale: 65536.0000 (43186.8204)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1542  max mem: 15572
Epoch: [37]  [1830/2809]  eta: 0:09:14  lr: 0.000001  min_lr: 0.000000  loss: 4.2125 (4.1868)  class_acc: 0.2917 (0.3234)  loss_scale: 65536.0000 (43308.8804)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1406  max mem: 15572
[2025-01-16 08:12:35,681] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105773
[2025-01-16 08:12:35,681] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:12:35,681] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [1840/2809]  eta: 0:09:09  lr: 0.000001  min_lr: 0.000000  loss: 4.2901 (4.1875)  class_acc: 0.2917 (0.3233)  loss_scale: 65536.0000 (43411.8153)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1675  max mem: 15572
Epoch: [37]  [1850/2809]  eta: 0:09:03  lr: 0.000001  min_lr: 0.000000  loss: 4.2302 (4.1873)  class_acc: 0.3333 (0.3237)  loss_scale: 32768.0000 (43354.3123)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1282  max mem: 15572
Epoch: [37]  [1860/2809]  eta: 0:08:57  lr: 0.000001  min_lr: 0.000000  loss: 4.0737 (4.1873)  class_acc: 0.3333 (0.3239)  loss_scale: 32768.0000 (43297.4272)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0749  max mem: 15572
Epoch: [37]  [1870/2809]  eta: 0:08:51  lr: 0.000001  min_lr: 0.000000  loss: 4.0901 (4.1865)  class_acc: 0.2917 (0.3239)  loss_scale: 32768.0000 (43241.1502)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1063  max mem: 15572
Epoch: [37]  [1880/2809]  eta: 0:08:45  lr: 0.000001  min_lr: 0.000000  loss: 4.1135 (4.1870)  class_acc: 0.3333 (0.3237)  loss_scale: 32768.0000 (43185.4716)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1029  max mem: 15572
Epoch: [37]  [1890/2809]  eta: 0:08:39  lr: 0.000001  min_lr: 0.000000  loss: 4.3204 (4.1878)  class_acc: 0.2500 (0.3237)  loss_scale: 32768.0000 (43130.3818)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0468  max mem: 15572
Epoch: [37]  [1900/2809]  eta: 0:08:34  lr: 0.000001  min_lr: 0.000000  loss: 4.2889 (4.1881)  class_acc: 0.2500 (0.3238)  loss_scale: 32768.0000 (43075.8716)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0930  max mem: 15572
Epoch: [37]  [1910/2809]  eta: 0:08:28  lr: 0.000001  min_lr: 0.000000  loss: 4.2403 (4.1883)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (43021.9320)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0748  max mem: 15572
Epoch: [37]  [1920/2809]  eta: 0:08:22  lr: 0.000001  min_lr: 0.000000  loss: 4.1937 (4.1883)  class_acc: 0.3333 (0.3239)  loss_scale: 32768.0000 (42968.5539)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0341  max mem: 15572
Epoch: [37]  [1930/2809]  eta: 0:08:17  lr: 0.000001  min_lr: 0.000000  loss: 4.1013 (4.1876)  class_acc: 0.3333 (0.3235)  loss_scale: 32768.0000 (42915.7286)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1348  max mem: 15572
Epoch: [37]  [1940/2809]  eta: 0:08:11  lr: 0.000001  min_lr: 0.000000  loss: 4.0743 (4.1875)  class_acc: 0.2500 (0.3233)  loss_scale: 32768.0000 (42863.4477)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1859  max mem: 15572
Epoch: [37]  [1950/2809]  eta: 0:08:05  lr: 0.000001  min_lr: 0.000000  loss: 4.1148 (4.1873)  class_acc: 0.2917 (0.3233)  loss_scale: 32768.0000 (42811.7027)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1239  max mem: 15572
Epoch: [37]  [1960/2809]  eta: 0:08:00  lr: 0.000001  min_lr: 0.000000  loss: 4.2772 (4.1881)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (42760.4855)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1222  max mem: 15572
[2025-01-16 08:13:46,677] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:13:46,678] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [1970/2809]  eta: 0:07:54  lr: 0.000001  min_lr: 0.000000  loss: 4.2523 (4.1879)  class_acc: 0.3333 (0.3233)  loss_scale: 32768.0000 (42743.0381)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1374  max mem: 15572
[2025-01-16 08:13:51,224] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105910
[2025-01-16 08:13:51,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:13:51,224] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [1980/2809]  eta: 0:07:49  lr: 0.000001  min_lr: 0.000000  loss: 4.1620 (4.1883)  class_acc: 0.2500 (0.3231)  loss_scale: 32768.0000 (42791.9313)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1326  max mem: 15572
Epoch: [37]  [1990/2809]  eta: 0:07:43  lr: 0.000001  min_lr: 0.000000  loss: 4.1689 (4.1881)  class_acc: 0.2500 (0.3232)  loss_scale: 32768.0000 (42741.5851)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.1031  max mem: 15572
Epoch: [37]  [2000/2809]  eta: 0:07:37  lr: 0.000001  min_lr: 0.000000  loss: 4.1020 (4.1872)  class_acc: 0.2500 (0.3232)  loss_scale: 32768.0000 (42691.7421)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1368  max mem: 15572
Epoch: [37]  [2010/2809]  eta: 0:07:32  lr: 0.000001  min_lr: 0.000000  loss: 4.0850 (4.1870)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (42642.3948)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1401  max mem: 15572
Epoch: [37]  [2020/2809]  eta: 0:07:26  lr: 0.000001  min_lr: 0.000000  loss: 4.2656 (4.1875)  class_acc: 0.3750 (0.3236)  loss_scale: 32768.0000 (42593.5359)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0717  max mem: 15572
Epoch: [37]  [2030/2809]  eta: 0:07:20  lr: 0.000001  min_lr: 0.000000  loss: 4.2590 (4.1875)  class_acc: 0.3750 (0.3238)  loss_scale: 32768.0000 (42545.1581)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1206  max mem: 15572
Epoch: [37]  [2040/2809]  eta: 0:07:15  lr: 0.000001  min_lr: 0.000000  loss: 4.2152 (4.1876)  class_acc: 0.3333 (0.3237)  loss_scale: 32768.0000 (42497.2543)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1131  max mem: 15572
Epoch: [37]  [2050/2809]  eta: 0:07:09  lr: 0.000001  min_lr: 0.000000  loss: 4.2243 (4.1876)  class_acc: 0.2917 (0.3236)  loss_scale: 32768.0000 (42449.8176)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1123  max mem: 15572
Epoch: [37]  [2060/2809]  eta: 0:07:03  lr: 0.000001  min_lr: 0.000000  loss: 4.2243 (4.1872)  class_acc: 0.2917 (0.3235)  loss_scale: 32768.0000 (42402.8413)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1255  max mem: 15572
[2025-01-16 08:14:41,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=106000, skipped=661, lr=[5.12437822468119e-09, 5.12437822468119e-09, 7.320540320973129e-09, 7.320540320973129e-09, 1.045791474424733e-08, 1.045791474424733e-08, 1.4939878206067616e-08, 1.4939878206067616e-08, 2.1342683151525164e-08, 2.1342683151525164e-08, 3.048954735932166e-08, 3.048954735932166e-08, 4.355649622760238e-08, 4.355649622760238e-08, 6.222356603943198e-08, 6.222356603943198e-08, 8.889080862775997e-08, 8.889080862775997e-08, 1.2698686946822854e-07, 1.2698686946822854e-07, 1.8140981352604078e-07, 1.8140981352604078e-07, 2.5915687646577256e-07, 2.5915687646577256e-07, 3.7022410923681794e-07, 3.7022410923681794e-07, 5.288915846240257e-07, 5.288915846240257e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:14:41,236] [INFO] [timer.py:260:stop] epoch=0/micro_step=106000/global_step=106000, RunningAvgSamplesPerSec=27.992527621207433, CurrSamplesPerSec=22.008034221744346, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [37]  [2070/2809]  eta: 0:06:58  lr: 0.000001  min_lr: 0.000000  loss: 4.1762 (4.1870)  class_acc: 0.3333 (0.3237)  loss_scale: 32768.0000 (42356.3187)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0881  max mem: 15572
Epoch: [37]  [2080/2809]  eta: 0:06:52  lr: 0.000001  min_lr: 0.000000  loss: 4.1340 (4.1870)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (42310.2432)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0966  max mem: 15572
Epoch: [37]  [2090/2809]  eta: 0:06:46  lr: 0.000001  min_lr: 0.000000  loss: 4.2449 (4.1869)  class_acc: 0.3333 (0.3241)  loss_scale: 32768.0000 (42264.6083)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1091  max mem: 15572
Epoch: [37]  [2100/2809]  eta: 0:06:40  lr: 0.000001  min_lr: 0.000000  loss: 4.1390 (4.1865)  class_acc: 0.3750 (0.3245)  loss_scale: 32768.0000 (42219.4079)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0941  max mem: 15572
[2025-01-16 08:15:02,654] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:15:02,654] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [2110/2809]  eta: 0:06:35  lr: 0.000001  min_lr: 0.000000  loss: 4.1367 (4.1867)  class_acc: 0.3750 (0.3247)  loss_scale: 32768.0000 (42252.2482)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1133  max mem: 15572
Epoch: [37]  [2120/2809]  eta: 0:06:29  lr: 0.000001  min_lr: 0.000000  loss: 4.1379 (4.1866)  class_acc: 0.3750 (0.3249)  loss_scale: 65536.0000 (42362.0255)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1262  max mem: 15572
Epoch: [37]  [2130/2809]  eta: 0:06:23  lr: 0.000001  min_lr: 0.000000  loss: 4.1379 (4.1872)  class_acc: 0.3333 (0.3248)  loss_scale: 65536.0000 (42470.7724)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0935  max mem: 15572
Epoch: [37]  [2140/2809]  eta: 0:06:18  lr: 0.000001  min_lr: 0.000000  loss: 4.2023 (4.1871)  class_acc: 0.2917 (0.3247)  loss_scale: 65536.0000 (42578.5035)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1117  max mem: 15572
Epoch: [37]  [2150/2809]  eta: 0:06:12  lr: 0.000001  min_lr: 0.000000  loss: 4.1673 (4.1870)  class_acc: 0.2917 (0.3248)  loss_scale: 65536.0000 (42685.2329)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0882  max mem: 15572
[2025-01-16 08:15:30,997] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106090
[2025-01-16 08:15:30,997] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:15:30,998] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [2160/2809]  eta: 0:06:06  lr: 0.000001  min_lr: 0.000000  loss: 4.1510 (4.1861)  class_acc: 0.2917 (0.3248)  loss_scale: 65536.0000 (42730.3211)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0725  max mem: 15572
Epoch: [37]  [2170/2809]  eta: 0:06:00  lr: 0.000001  min_lr: 0.000000  loss: 4.1587 (4.1863)  class_acc: 0.3333 (0.3248)  loss_scale: 32768.0000 (42684.4330)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0766  max mem: 15572
Epoch: [37]  [2180/2809]  eta: 0:05:55  lr: 0.000001  min_lr: 0.000000  loss: 4.2695 (4.1874)  class_acc: 0.3333 (0.3248)  loss_scale: 32768.0000 (42638.9656)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0796  max mem: 15572
Epoch: [37]  [2190/2809]  eta: 0:05:50  lr: 0.000001  min_lr: 0.000000  loss: 4.2695 (4.1872)  class_acc: 0.2917 (0.3246)  loss_scale: 32768.0000 (42593.9133)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.2023  max mem: 15572
Epoch: [37]  [2200/2809]  eta: 0:05:44  lr: 0.000001  min_lr: 0.000000  loss: 4.1690 (4.1872)  class_acc: 0.2917 (0.3245)  loss_scale: 32768.0000 (42549.2703)  weight_decay: 0.0500 (0.0500)  time: 0.6593  data: 0.2319  max mem: 15572
Epoch: [37]  [2210/2809]  eta: 0:05:38  lr: 0.000001  min_lr: 0.000000  loss: 4.1690 (4.1868)  class_acc: 0.3333 (0.3249)  loss_scale: 32768.0000 (42505.0312)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1431  max mem: 15572
Epoch: [37]  [2220/2809]  eta: 0:05:33  lr: 0.000001  min_lr: 0.000000  loss: 4.2337 (4.1875)  class_acc: 0.3333 (0.3249)  loss_scale: 32768.0000 (42461.1905)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.0889  max mem: 15572
Epoch: [37]  [2230/2809]  eta: 0:05:27  lr: 0.000001  min_lr: 0.000000  loss: 4.2337 (4.1870)  class_acc: 0.2917 (0.3248)  loss_scale: 32768.0000 (42417.7427)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0819  max mem: 15572
Epoch: [37]  [2240/2809]  eta: 0:05:21  lr: 0.000001  min_lr: 0.000000  loss: 4.1888 (4.1876)  class_acc: 0.2500 (0.3246)  loss_scale: 32768.0000 (42374.6827)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0455  max mem: 15572
Epoch: [37]  [2250/2809]  eta: 0:05:15  lr: 0.000001  min_lr: 0.000000  loss: 4.3274 (4.1880)  class_acc: 0.2500 (0.3244)  loss_scale: 32768.0000 (42332.0053)  weight_decay: 0.0500 (0.0500)  time: 0.4926  data: 0.0372  max mem: 15572
Epoch: [37]  [2260/2809]  eta: 0:05:10  lr: 0.000001  min_lr: 0.000000  loss: 4.1278 (4.1874)  class_acc: 0.2917 (0.3245)  loss_scale: 32768.0000 (42289.7054)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1252  max mem: 15572
Epoch: [37]  [2270/2809]  eta: 0:05:04  lr: 0.000000  min_lr: 0.000000  loss: 4.1762 (4.1881)  class_acc: 0.3333 (0.3245)  loss_scale: 32768.0000 (42247.7781)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1592  max mem: 15572
Epoch: [37]  [2280/2809]  eta: 0:04:59  lr: 0.000000  min_lr: 0.000000  loss: 4.2462 (4.1878)  class_acc: 0.2917 (0.3244)  loss_scale: 32768.0000 (42206.2183)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1411  max mem: 15572
[2025-01-16 08:16:44,806] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:16:44,806] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [2290/2809]  eta: 0:04:53  lr: 0.000000  min_lr: 0.000000  loss: 4.0865 (4.1878)  class_acc: 0.2500 (0.3242)  loss_scale: 32768.0000 (42236.5360)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1238  max mem: 15572
Epoch: [37]  [2300/2809]  eta: 0:04:47  lr: 0.000000  min_lr: 0.000000  loss: 4.0865 (4.1876)  class_acc: 0.2917 (0.3241)  loss_scale: 65536.0000 (42337.7940)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0929  max mem: 15572
Epoch: [37]  [2310/2809]  eta: 0:04:42  lr: 0.000000  min_lr: 0.000000  loss: 4.1474 (4.1877)  class_acc: 0.2917 (0.3239)  loss_scale: 65536.0000 (42438.1757)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1392  max mem: 15572
Epoch: [37]  [2320/2809]  eta: 0:04:36  lr: 0.000000  min_lr: 0.000000  loss: 4.2951 (4.1884)  class_acc: 0.2500 (0.3235)  loss_scale: 65536.0000 (42537.6924)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1219  max mem: 15572
[2025-01-16 08:17:05,477] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106256
[2025-01-16 08:17:05,478] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:17:05,478] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [2330/2809]  eta: 0:04:30  lr: 0.000000  min_lr: 0.000000  loss: 4.3185 (4.1888)  class_acc: 0.2500 (0.3234)  loss_scale: 65536.0000 (42523.8953)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0676  max mem: 15572
Epoch: [37]  [2340/2809]  eta: 0:04:24  lr: 0.000000  min_lr: 0.000000  loss: 4.2032 (4.1883)  class_acc: 0.2917 (0.3237)  loss_scale: 32768.0000 (42482.2213)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0940  max mem: 15572
Epoch: [37]  [2350/2809]  eta: 0:04:19  lr: 0.000000  min_lr: 0.000000  loss: 4.1163 (4.1881)  class_acc: 0.2917 (0.3237)  loss_scale: 32768.0000 (42440.9017)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1217  max mem: 15572
Epoch: [37]  [2360/2809]  eta: 0:04:13  lr: 0.000000  min_lr: 0.000000  loss: 4.1831 (4.1875)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (42399.9322)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1029  max mem: 15572
Epoch: [37]  [2370/2809]  eta: 0:04:07  lr: 0.000000  min_lr: 0.000000  loss: 4.1831 (4.1873)  class_acc: 0.3750 (0.3240)  loss_scale: 32768.0000 (42359.3083)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0891  max mem: 15572
Epoch: [37]  [2380/2809]  eta: 0:04:02  lr: 0.000000  min_lr: 0.000000  loss: 4.1899 (4.1874)  class_acc: 0.3333 (0.3238)  loss_scale: 32768.0000 (42319.0256)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1076  max mem: 15572
Epoch: [37]  [2390/2809]  eta: 0:03:56  lr: 0.000000  min_lr: 0.000000  loss: 4.2863 (4.1879)  class_acc: 0.2917 (0.3235)  loss_scale: 32768.0000 (42279.0799)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1268  max mem: 15572
Epoch: [37]  [2400/2809]  eta: 0:03:51  lr: 0.000000  min_lr: 0.000000  loss: 4.1983 (4.1880)  class_acc: 0.3333 (0.3237)  loss_scale: 32768.0000 (42239.4669)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1716  max mem: 15572
Epoch: [37]  [2410/2809]  eta: 0:03:45  lr: 0.000000  min_lr: 0.000000  loss: 4.1850 (4.1873)  class_acc: 0.2917 (0.3236)  loss_scale: 32768.0000 (42200.1825)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1698  max mem: 15572
Epoch: [37]  [2420/2809]  eta: 0:03:39  lr: 0.000000  min_lr: 0.000000  loss: 4.2255 (4.1879)  class_acc: 0.2500 (0.3233)  loss_scale: 32768.0000 (42161.2226)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0773  max mem: 15572
Epoch: [37]  [2430/2809]  eta: 0:03:33  lr: 0.000000  min_lr: 0.000000  loss: 4.3007 (4.1882)  class_acc: 0.2500 (0.3233)  loss_scale: 32768.0000 (42122.5833)  weight_decay: 0.0500 (0.0500)  time: 0.4546  data: 0.0161  max mem: 15572
Epoch: [37]  [2440/2809]  eta: 0:03:28  lr: 0.000000  min_lr: 0.000000  loss: 4.1324 (4.1876)  class_acc: 0.3333 (0.3236)  loss_scale: 32768.0000 (42084.2605)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1094  max mem: 15572
Epoch: [37]  [2450/2809]  eta: 0:03:22  lr: 0.000000  min_lr: 0.000000  loss: 4.1598 (4.1880)  class_acc: 0.3750 (0.3236)  loss_scale: 32768.0000 (42046.2505)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1484  max mem: 15572
[2025-01-16 08:18:17,629] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:18:17,630] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 08:18:22,252] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106393
[2025-01-16 08:18:22,253] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:18:22,255] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [2460/2809]  eta: 0:03:17  lr: 0.000000  min_lr: 0.000000  loss: 4.1929 (4.1882)  class_acc: 0.3333 (0.3239)  loss_scale: 32768.0000 (42115.0687)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1059  max mem: 15572
Epoch: [37]  [2470/2809]  eta: 0:03:11  lr: 0.000000  min_lr: 0.000000  loss: 4.1555 (4.1883)  class_acc: 0.3333 (0.3240)  loss_scale: 32768.0000 (42077.2416)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1323  max mem: 15572
Epoch: [37]  [2480/2809]  eta: 0:03:05  lr: 0.000000  min_lr: 0.000000  loss: 4.2718 (4.1885)  class_acc: 0.3333 (0.3239)  loss_scale: 32768.0000 (42039.7195)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1219  max mem: 15572
Epoch: [37]  [2490/2809]  eta: 0:03:00  lr: 0.000000  min_lr: 0.000000  loss: 4.2856 (4.1889)  class_acc: 0.3333 (0.3240)  loss_scale: 32768.0000 (42002.4986)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1148  max mem: 15572
Epoch: [37]  [2500/2809]  eta: 0:02:54  lr: 0.000000  min_lr: 0.000000  loss: 4.1858 (4.1888)  class_acc: 0.3750 (0.3241)  loss_scale: 32768.0000 (41965.5754)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1102  max mem: 15572
Epoch: [37]  [2510/2809]  eta: 0:02:48  lr: 0.000000  min_lr: 0.000000  loss: 4.1330 (4.1885)  class_acc: 0.2917 (0.3241)  loss_scale: 32768.0000 (41928.9462)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1174  max mem: 15572
Epoch: [37]  [2520/2809]  eta: 0:02:43  lr: 0.000000  min_lr: 0.000000  loss: 4.2291 (4.1889)  class_acc: 0.2917 (0.3244)  loss_scale: 32768.0000 (41892.6077)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1484  max mem: 15572
Epoch: [37]  [2530/2809]  eta: 0:02:37  lr: 0.000000  min_lr: 0.000000  loss: 4.3187 (4.1888)  class_acc: 0.3750 (0.3246)  loss_scale: 32768.0000 (41856.5563)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1610  max mem: 15572
Epoch: [37]  [2540/2809]  eta: 0:02:31  lr: 0.000000  min_lr: 0.000000  loss: 4.1436 (4.1885)  class_acc: 0.3333 (0.3246)  loss_scale: 32768.0000 (41820.7887)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0978  max mem: 15572
Epoch: [37]  [2550/2809]  eta: 0:02:26  lr: 0.000000  min_lr: 0.000000  loss: 4.1056 (4.1884)  class_acc: 0.3333 (0.3245)  loss_scale: 32768.0000 (41785.3015)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0962  max mem: 15572
Epoch: [37]  [2560/2809]  eta: 0:02:20  lr: 0.000000  min_lr: 0.000000  loss: 4.1173 (4.1881)  class_acc: 0.3333 (0.3246)  loss_scale: 32768.0000 (41750.0914)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1701  max mem: 15572
Epoch: [37]  [2570/2809]  eta: 0:02:15  lr: 0.000000  min_lr: 0.000000  loss: 4.2471 (4.1884)  class_acc: 0.3333 (0.3244)  loss_scale: 32768.0000 (41715.1552)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1441  max mem: 15572
Epoch: [37]  [2580/2809]  eta: 0:02:09  lr: 0.000000  min_lr: 0.000000  loss: 4.2960 (4.1880)  class_acc: 0.2500 (0.3243)  loss_scale: 32768.0000 (41680.4897)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1512  max mem: 15572
[2025-01-16 08:19:37,186] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:19:37,186] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [2590/2809]  eta: 0:02:03  lr: 0.000000  min_lr: 0.000000  loss: 4.0463 (4.1877)  class_acc: 0.3333 (0.3247)  loss_scale: 32768.0000 (41671.3856)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1541  max mem: 15572
Epoch: [37]  [2600/2809]  eta: 0:01:58  lr: 0.000000  min_lr: 0.000000  loss: 4.1345 (4.1875)  class_acc: 0.3750 (0.3248)  loss_scale: 65536.0000 (41763.1373)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1312  max mem: 15572
Epoch: [37]  [2610/2809]  eta: 0:01:52  lr: 0.000000  min_lr: 0.000000  loss: 4.0869 (4.1863)  class_acc: 0.3333 (0.3246)  loss_scale: 65536.0000 (41854.1861)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1151  max mem: 15572
Epoch: [37]  [2620/2809]  eta: 0:01:46  lr: 0.000000  min_lr: 0.000000  loss: 4.1756 (4.1866)  class_acc: 0.3333 (0.3246)  loss_scale: 65536.0000 (41944.5403)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0593  max mem: 15572
Epoch: [37]  [2630/2809]  eta: 0:01:41  lr: 0.000000  min_lr: 0.000000  loss: 4.1573 (4.1861)  class_acc: 0.3333 (0.3247)  loss_scale: 65536.0000 (42034.2075)  weight_decay: 0.0500 (0.0500)  time: 0.4673  data: 0.0063  max mem: 15572
[2025-01-16 08:20:00,911] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106570
[2025-01-16 08:20:00,911] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:20:00,912] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [2640/2809]  eta: 0:01:35  lr: 0.000000  min_lr: 0.000000  loss: 4.1387 (4.1863)  class_acc: 0.3333 (0.3248)  loss_scale: 65536.0000 (42073.5661)  weight_decay: 0.0500 (0.0500)  time: 0.4436  data: 0.0007  max mem: 15572
Epoch: [37]  [2650/2809]  eta: 0:01:29  lr: 0.000000  min_lr: 0.000000  loss: 4.1569 (4.1860)  class_acc: 0.3333 (0.3249)  loss_scale: 32768.0000 (42038.4640)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.0007  max mem: 15572
Epoch: [37]  [2660/2809]  eta: 0:01:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1707 (4.1862)  class_acc: 0.3333 (0.3249)  loss_scale: 32768.0000 (42003.6257)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0009  max mem: 15572
Epoch: [37]  [2670/2809]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 4.1717 (4.1858)  class_acc: 0.3333 (0.3251)  loss_scale: 32768.0000 (41969.0483)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0010  max mem: 15572
Epoch: [37]  [2680/2809]  eta: 0:01:12  lr: 0.000000  min_lr: 0.000000  loss: 4.2758 (4.1863)  class_acc: 0.3333 (0.3253)  loss_scale: 32768.0000 (41934.7288)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0070  max mem: 15572
Epoch: [37]  [2690/2809]  eta: 0:01:07  lr: 0.000000  min_lr: 0.000000  loss: 4.2758 (4.1865)  class_acc: 0.2917 (0.3251)  loss_scale: 32768.0000 (41900.6644)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1124  max mem: 15572
Epoch: [37]  [2700/2809]  eta: 0:01:01  lr: 0.000000  min_lr: 0.000000  loss: 4.2587 (4.1868)  class_acc: 0.2917 (0.3251)  loss_scale: 32768.0000 (41866.8523)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.1557  max mem: 15572
Epoch: [37]  [2710/2809]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 4.2620 (4.1869)  class_acc: 0.3333 (0.3251)  loss_scale: 32768.0000 (41833.2896)  weight_decay: 0.0500 (0.0500)  time: 0.6919  data: 0.1902  max mem: 15572
Epoch: [37]  [2720/2809]  eta: 0:00:50  lr: 0.000000  min_lr: 0.000000  loss: 4.3466 (4.1875)  class_acc: 0.2917 (0.3248)  loss_scale: 32768.0000 (41799.9735)  weight_decay: 0.0500 (0.0500)  time: 0.7110  data: 0.2108  max mem: 15572
Epoch: [37]  [2730/2809]  eta: 0:00:44  lr: 0.000000  min_lr: 0.000000  loss: 4.3466 (4.1878)  class_acc: 0.2083 (0.3245)  loss_scale: 32768.0000 (41766.9015)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.1562  max mem: 15572
Epoch: [37]  [2740/2809]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 4.1557 (4.1878)  class_acc: 0.2500 (0.3245)  loss_scale: 32768.0000 (41734.0708)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.1403  max mem: 15572
Epoch: [37]  [2750/2809]  eta: 0:00:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1823 (4.1876)  class_acc: 0.2917 (0.3246)  loss_scale: 32768.0000 (41701.4787)  weight_decay: 0.0500 (0.0500)  time: 0.6798  data: 0.1786  max mem: 15572
Epoch: [37]  [2760/2809]  eta: 0:00:27  lr: 0.000000  min_lr: 0.000000  loss: 4.1823 (4.1878)  class_acc: 0.2500 (0.3246)  loss_scale: 32768.0000 (41669.1228)  weight_decay: 0.0500 (0.0500)  time: 0.7556  data: 0.2342  max mem: 15572
[2025-01-16 08:21:21,448] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:21:21,448] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 08:21:23,342] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106700
[2025-01-16 08:21:23,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:21:23,343] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [2770/2809]  eta: 0:00:22  lr: 0.000000  min_lr: 0.000000  loss: 4.1176 (4.1874)  class_acc: 0.3333 (0.3248)  loss_scale: 32768.0000 (41648.8257)  weight_decay: 0.0500 (0.0500)  time: 0.7015  data: 0.1780  max mem: 15572
Epoch: [37]  [2780/2809]  eta: 0:00:16  lr: 0.000000  min_lr: 0.000000  loss: 4.1863 (4.1878)  class_acc: 0.2917 (0.3246)  loss_scale: 32768.0000 (41616.8918)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0679  max mem: 15572
Epoch: [37]  [2790/2809]  eta: 0:00:10  lr: 0.000000  min_lr: 0.000000  loss: 4.2044 (4.1881)  class_acc: 0.2500 (0.3244)  loss_scale: 32768.0000 (41585.1867)  weight_decay: 0.0500 (0.0500)  time: 0.4581  data: 0.0008  max mem: 15572
Epoch: [37]  [2800/2809]  eta: 0:00:05  lr: 0.000000  min_lr: 0.000000  loss: 4.1324 (4.1874)  class_acc: 0.2500 (0.3243)  loss_scale: 32768.0000 (41553.7080)  weight_decay: 0.0500 (0.0500)  time: 0.4638  data: 0.0008  max mem: 15572
Epoch: [37]  [2808/2809]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 4.1377 (4.1877)  class_acc: 0.3333 (0.3245)  loss_scale: 32768.0000 (41528.6864)  weight_decay: 0.0500 (0.0500)  time: 0.4167  data: 0.0005  max mem: 15572
Epoch: [37] Total time: 0:26:30 (0.5661 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 4.1377 (4.1877)  class_acc: 0.3333 (0.3245)  loss_scale: 32768.0000 (41528.6864)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:15:00  loss: 1.1069 (1.1069)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 3.3108  data: 3.1469  max mem: 15572
Val:  [ 10/272]  eta: 0:03:00  loss: 2.4246 (2.4795)  acc1: 44.4444 (45.9596)  acc5: 77.7778 (76.2626)  time: 0.6874  data: 0.5030  max mem: 15572
Val:  [ 20/272]  eta: 0:02:02  loss: 2.5740 (2.5362)  acc1: 44.4444 (47.3545)  acc5: 77.7778 (75.9259)  time: 0.3446  data: 0.1523  max mem: 15572
Val:  [ 30/272]  eta: 0:01:43  loss: 2.6124 (2.5774)  acc1: 44.4444 (43.9068)  acc5: 72.2222 (75.2688)  time: 0.2880  data: 0.0875  max mem: 15572
Val:  [ 40/272]  eta: 0:01:29  loss: 2.6637 (2.6064)  acc1: 27.7778 (41.5989)  acc5: 72.2222 (74.5257)  time: 0.2814  data: 0.0779  max mem: 15572
Val:  [ 50/272]  eta: 0:01:21  loss: 2.5037 (2.5428)  acc1: 38.8889 (42.8105)  acc5: 77.7778 (75.9259)  time: 0.2660  data: 0.0745  max mem: 15572
Val:  [ 60/272]  eta: 0:01:16  loss: 1.9457 (2.4809)  acc1: 55.5556 (44.5355)  acc5: 83.3333 (76.5938)  time: 0.3137  data: 0.1295  max mem: 15572
Val:  [ 70/272]  eta: 0:01:11  loss: 1.9515 (2.4188)  acc1: 61.1111 (46.9484)  acc5: 83.3333 (77.3865)  time: 0.3298  data: 0.1475  max mem: 15572
Val:  [ 80/272]  eta: 0:01:06  loss: 2.1523 (2.4247)  acc1: 55.5556 (47.0508)  acc5: 83.3333 (77.1605)  time: 0.2942  data: 0.1181  max mem: 15572
Val:  [ 90/272]  eta: 0:01:02  loss: 2.4405 (2.4302)  acc1: 50.0000 (47.2527)  acc5: 83.3333 (77.5946)  time: 0.3109  data: 0.1206  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 2.5108 (2.4530)  acc1: 44.4444 (46.5897)  acc5: 83.3333 (77.3377)  time: 0.3620  data: 0.1675  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.5970 (2.5054)  acc1: 33.3333 (45.0450)  acc5: 77.7778 (76.3764)  time: 0.3363  data: 0.1478  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 2.8521 (2.5393)  acc1: 27.7778 (44.3985)  acc5: 66.6667 (75.8494)  time: 0.3104  data: 0.1315  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.4945 (2.5148)  acc1: 44.4444 (45.0382)  acc5: 77.7778 (76.5479)  time: 0.3184  data: 0.1420  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 2.1402 (2.5171)  acc1: 55.5556 (45.4689)  acc5: 83.3333 (76.2805)  time: 0.2844  data: 0.1006  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.5324 (2.5150)  acc1: 44.4444 (45.1435)  acc5: 77.7778 (76.4165)  time: 0.3147  data: 0.1192  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.4553 (2.5125)  acc1: 44.4444 (45.4451)  acc5: 77.7778 (76.7771)  time: 0.3070  data: 0.1081  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.5731 (2.5292)  acc1: 38.8889 (44.8018)  acc5: 72.2222 (76.2833)  time: 0.2703  data: 0.0819  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 2.5729 (2.5187)  acc1: 38.8889 (44.6593)  acc5: 72.2222 (76.5500)  time: 0.2809  data: 0.0937  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.4734 (2.5548)  acc1: 38.8889 (43.6300)  acc5: 72.2222 (75.4508)  time: 0.3054  data: 0.1193  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.6447 (2.5593)  acc1: 38.8889 (43.2007)  acc5: 72.2222 (75.2902)  time: 0.3094  data: 0.1252  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.3247 (2.5669)  acc1: 38.8889 (42.9963)  acc5: 77.7778 (75.1711)  time: 0.2838  data: 0.1046  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.7051 (2.5622)  acc1: 38.8889 (43.0870)  acc5: 72.2222 (75.2891)  time: 0.3556  data: 0.1704  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.1403 (2.5478)  acc1: 50.0000 (43.8191)  acc5: 83.3333 (75.5411)  time: 0.3520  data: 0.1567  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.1166 (2.5348)  acc1: 55.5556 (44.0987)  acc5: 83.3333 (75.9336)  time: 0.2966  data: 0.1139  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4227 (2.5392)  acc1: 38.8889 (43.6476)  acc5: 83.3333 (75.9628)  time: 0.3001  data: 0.1122  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.9098 (2.4999)  acc1: 66.6667 (45.0830)  acc5: 88.8889 (76.6071)  time: 0.2881  data: 0.0957  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.9000 (2.4953)  acc1: 66.6667 (45.1825)  acc5: 88.8889 (76.7938)  time: 0.2399  data: 0.0763  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.9000 (2.4990)  acc1: 61.1111 (45.1771)  acc5: 88.8889 (76.7766)  time: 0.2340  data: 0.0763  max mem: 15572
Val: Total time: 0:01:26 (0.3171 s / it)
* Acc@1 45.177 Acc@5 76.777 loss 2.499
Accuracy of the network on the 4883 val videos: 45.2%
[2025-01-16 08:23:07,857] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 08:23:07,860] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 08:23:07,860] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 08:23:10,664] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 08:23:10,664] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.18%
Epoch: [38]  [   0/2809]  eta: 7:13:38  lr: 0.000000  min_lr: 0.000000  loss: 3.8697 (3.8697)  class_acc: 0.4583 (0.4583)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.2624  data: 8.7695  max mem: 15572
Epoch: [38]  [  10/2809]  eta: 0:58:12  lr: 0.000000  min_lr: 0.000000  loss: 4.0957 (4.1335)  class_acc: 0.2917 (0.3447)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2476  data: 0.8126  max mem: 15572
Epoch: [38]  [  20/2809]  eta: 0:43:42  lr: 0.000000  min_lr: 0.000000  loss: 4.0624 (4.0769)  class_acc: 0.3750 (0.3810)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0737  max mem: 15572
Epoch: [38]  [  30/2809]  eta: 0:37:36  lr: 0.000000  min_lr: 0.000000  loss: 3.9660 (4.0148)  class_acc: 0.4167 (0.3737)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1103  max mem: 15572
Epoch: [38]  [  40/2809]  eta: 0:34:35  lr: 0.000000  min_lr: 0.000000  loss: 4.0243 (4.0501)  class_acc: 0.3750 (0.3577)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1069  max mem: 15572
Epoch: [38]  [  50/2809]  eta: 0:33:59  lr: 0.000000  min_lr: 0.000000  loss: 4.1769 (4.0546)  class_acc: 0.2917 (0.3505)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1962  max mem: 15572
Epoch: [38]  [  60/2809]  eta: 0:32:23  lr: 0.000000  min_lr: 0.000000  loss: 4.2265 (4.0793)  class_acc: 0.2917 (0.3477)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1873  max mem: 15572
Epoch: [38]  [  70/2809]  eta: 0:31:23  lr: 0.000000  min_lr: 0.000000  loss: 4.2881 (4.0910)  class_acc: 0.2917 (0.3539)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1221  max mem: 15572
Epoch: [38]  [  80/2809]  eta: 0:30:35  lr: 0.000000  min_lr: 0.000000  loss: 4.0808 (4.0890)  class_acc: 0.3333 (0.3493)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1283  max mem: 15572
[2025-01-16 08:24:08,293] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:24:08,294] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [  90/2809]  eta: 0:29:26  lr: 0.000000  min_lr: 0.000000  loss: 4.1910 (4.0977)  class_acc: 0.2917 (0.3434)  loss_scale: 32768.0000 (34208.3516)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0664  max mem: 15572
Epoch: [38]  [ 100/2809]  eta: 0:28:51  lr: 0.000000  min_lr: 0.000000  loss: 4.1113 (4.0974)  class_acc: 0.2917 (0.3387)  loss_scale: 65536.0000 (37310.0990)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0479  max mem: 15572
Epoch: [38]  [ 110/2809]  eta: 0:28:14  lr: 0.000000  min_lr: 0.000000  loss: 4.0962 (4.1014)  class_acc: 0.2917 (0.3375)  loss_scale: 65536.0000 (39852.9730)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0776  max mem: 15572
[2025-01-16 08:24:23,632] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106859
[2025-01-16 08:24:23,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:24:23,633] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 120/2809]  eta: 0:28:00  lr: 0.000000  min_lr: 0.000000  loss: 4.1616 (4.1156)  class_acc: 0.2917 (0.3361)  loss_scale: 65536.0000 (40892.2975)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1076  max mem: 15572
Epoch: [38]  [ 130/2809]  eta: 0:27:40  lr: 0.000000  min_lr: 0.000000  loss: 4.2569 (4.1276)  class_acc: 0.2917 (0.3321)  loss_scale: 32768.0000 (40272.1221)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1251  max mem: 15572
Epoch: [38]  [ 140/2809]  eta: 0:27:25  lr: 0.000000  min_lr: 0.000000  loss: 4.0838 (4.1130)  class_acc: 0.2917 (0.3298)  loss_scale: 32768.0000 (39739.9149)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1170  max mem: 15572
Epoch: [38]  [ 150/2809]  eta: 0:27:01  lr: 0.000000  min_lr: 0.000000  loss: 4.0808 (4.1177)  class_acc: 0.2917 (0.3270)  loss_scale: 32768.0000 (39278.1987)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1041  max mem: 15572
Epoch: [38]  [ 160/2809]  eta: 0:27:01  lr: 0.000000  min_lr: 0.000000  loss: 4.2792 (4.1315)  class_acc: 0.2083 (0.3219)  loss_scale: 32768.0000 (38873.8385)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1493  max mem: 15572
Epoch: [38]  [ 170/2809]  eta: 0:26:39  lr: 0.000000  min_lr: 0.000000  loss: 4.3266 (4.1432)  class_acc: 0.2083 (0.3207)  loss_scale: 32768.0000 (38516.7719)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1370  max mem: 15572
Epoch: [38]  [ 180/2809]  eta: 0:26:23  lr: 0.000000  min_lr: 0.000000  loss: 4.2698 (4.1421)  class_acc: 0.2917 (0.3209)  loss_scale: 32768.0000 (38199.1602)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0574  max mem: 15572
Epoch: [38]  [ 190/2809]  eta: 0:26:18  lr: 0.000000  min_lr: 0.000000  loss: 4.1406 (4.1362)  class_acc: 0.3750 (0.3222)  loss_scale: 32768.0000 (37914.8063)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1175  max mem: 15572
Epoch: [38]  [ 200/2809]  eta: 0:26:03  lr: 0.000000  min_lr: 0.000000  loss: 4.1818 (4.1440)  class_acc: 0.3750 (0.3252)  loss_scale: 32768.0000 (37658.7463)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1249  max mem: 15572
Epoch: [38]  [ 210/2809]  eta: 0:25:48  lr: 0.000000  min_lr: 0.000000  loss: 4.2806 (4.1517)  class_acc: 0.3333 (0.3235)  loss_scale: 32768.0000 (37426.9573)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0817  max mem: 15572
Epoch: [38]  [ 220/2809]  eta: 0:25:40  lr: 0.000000  min_lr: 0.000000  loss: 4.2806 (4.1536)  class_acc: 0.2917 (0.3232)  loss_scale: 32768.0000 (37216.1448)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1096  max mem: 15572
Epoch: [38]  [ 230/2809]  eta: 0:25:33  lr: 0.000000  min_lr: 0.000000  loss: 4.3084 (4.1597)  class_acc: 0.2917 (0.3211)  loss_scale: 32768.0000 (37023.5844)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1366  max mem: 15572
Epoch: [38]  [ 240/2809]  eta: 0:25:15  lr: 0.000000  min_lr: 0.000000  loss: 4.3026 (4.1580)  class_acc: 0.3333 (0.3245)  loss_scale: 32768.0000 (36847.0041)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0912  max mem: 15572
[2025-01-16 08:25:36,963] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:25:36,963] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 08:25:37,807] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106990
[2025-01-16 08:25:37,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:25:37,807] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 250/2809]  eta: 0:25:10  lr: 0.000000  min_lr: 0.000000  loss: 4.3026 (4.1628)  class_acc: 0.4167 (0.3275)  loss_scale: 32768.0000 (36945.5936)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1060  max mem: 15572
[2025-01-16 08:25:43,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=107000, skipped=668, lr=[3.774722835981766e-09, 3.774722835981766e-09, 5.392461194259667e-09, 5.392461194259667e-09, 7.703515991799525e-09, 7.703515991799525e-09, 1.1005022845427893e-08, 1.1005022845427893e-08, 1.5721461207754134e-08, 1.5721461207754134e-08, 2.245923029679162e-08, 2.245923029679162e-08, 3.208461470970231e-08, 3.208461470970231e-08, 4.5835163871003313e-08, 4.5835163871003313e-08, 6.547880553000474e-08, 6.547880553000474e-08, 9.354115075714963e-08, 9.354115075714963e-08, 1.336302153673566e-07, 1.336302153673566e-07, 1.9090030766765233e-07, 1.9090030766765233e-07, 2.7271472523950333e-07, 2.7271472523950333e-07, 3.8959246462786193e-07, 3.8959246462786193e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:25:43,380] [INFO] [timer.py:260:stop] epoch=0/micro_step=107000/global_step=107000, RunningAvgSamplesPerSec=27.9920593055922, CurrSamplesPerSec=24.709804556902366, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [38]  [ 260/2809]  eta: 0:25:05  lr: 0.000000  min_lr: 0.000000  loss: 4.2578 (4.1639)  class_acc: 0.3333 (0.3284)  loss_scale: 32768.0000 (36785.5326)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1521  max mem: 15572
Epoch: [38]  [ 270/2809]  eta: 0:24:54  lr: 0.000000  min_lr: 0.000000  loss: 4.2207 (4.1662)  class_acc: 0.2917 (0.3284)  loss_scale: 32768.0000 (36637.2841)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1181  max mem: 15572
Epoch: [38]  [ 280/2809]  eta: 0:24:43  lr: 0.000000  min_lr: 0.000000  loss: 4.2186 (4.1687)  class_acc: 0.2917 (0.3265)  loss_scale: 32768.0000 (36499.5872)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0890  max mem: 15572
Epoch: [38]  [ 290/2809]  eta: 0:24:29  lr: 0.000000  min_lr: 0.000000  loss: 4.2121 (4.1692)  class_acc: 0.3333 (0.3272)  loss_scale: 32768.0000 (36371.3540)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0588  max mem: 15572
Epoch: [38]  [ 300/2809]  eta: 0:24:21  lr: 0.000000  min_lr: 0.000000  loss: 4.1836 (4.1716)  class_acc: 0.3333 (0.3281)  loss_scale: 32768.0000 (36251.6412)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0622  max mem: 15572
Epoch: [38]  [ 310/2809]  eta: 0:24:10  lr: 0.000000  min_lr: 0.000000  loss: 4.1734 (4.1718)  class_acc: 0.3750 (0.3303)  loss_scale: 32768.0000 (36139.6270)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0812  max mem: 15572
Epoch: [38]  [ 320/2809]  eta: 0:24:06  lr: 0.000000  min_lr: 0.000000  loss: 4.2652 (4.1733)  class_acc: 0.3750 (0.3302)  loss_scale: 32768.0000 (36034.5919)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1251  max mem: 15572
Epoch: [38]  [ 330/2809]  eta: 0:24:02  lr: 0.000000  min_lr: 0.000000  loss: 4.3596 (4.1737)  class_acc: 0.3333 (0.3313)  loss_scale: 32768.0000 (35935.9033)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1509  max mem: 15572
Epoch: [38]  [ 340/2809]  eta: 0:23:56  lr: 0.000000  min_lr: 0.000000  loss: 4.2275 (4.1745)  class_acc: 0.3333 (0.3306)  loss_scale: 32768.0000 (35843.0029)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1266  max mem: 15572
Epoch: [38]  [ 350/2809]  eta: 0:23:48  lr: 0.000000  min_lr: 0.000000  loss: 4.3090 (4.1769)  class_acc: 0.2917 (0.3304)  loss_scale: 32768.0000 (35755.3960)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1147  max mem: 15572
Epoch: [38]  [ 360/2809]  eta: 0:23:45  lr: 0.000000  min_lr: 0.000000  loss: 4.2345 (4.1803)  class_acc: 0.2917 (0.3294)  loss_scale: 32768.0000 (35672.6427)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1482  max mem: 15572
Epoch: [38]  [ 370/2809]  eta: 0:23:38  lr: 0.000000  min_lr: 0.000000  loss: 4.2329 (4.1812)  class_acc: 0.2917 (0.3295)  loss_scale: 32768.0000 (35594.3504)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1407  max mem: 15572
[2025-01-16 08:26:51,442] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:26:51,443] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 380/2809]  eta: 0:23:35  lr: 0.000000  min_lr: 0.000000  loss: 4.1261 (4.1780)  class_acc: 0.2917 (0.3287)  loss_scale: 32768.0000 (35864.1890)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1303  max mem: 15572
Epoch: [38]  [ 390/2809]  eta: 0:23:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1493 (4.1795)  class_acc: 0.3333 (0.3298)  loss_scale: 65536.0000 (36623.0588)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.1783  max mem: 15572
[2025-01-16 08:27:02,512] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107135
[2025-01-16 08:27:02,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:27:02,513] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 400/2809]  eta: 0:23:30  lr: 0.000000  min_lr: 0.000000  loss: 4.2207 (4.1780)  class_acc: 0.3333 (0.3298)  loss_scale: 65536.0000 (36690.3541)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.1905  max mem: 15572
Epoch: [38]  [ 410/2809]  eta: 0:23:17  lr: 0.000000  min_lr: 0.000000  loss: 4.1472 (4.1776)  class_acc: 0.3333 (0.3287)  loss_scale: 32768.0000 (36594.9197)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0938  max mem: 15572
Epoch: [38]  [ 420/2809]  eta: 0:23:09  lr: 0.000000  min_lr: 0.000000  loss: 4.2993 (4.1814)  class_acc: 0.2500 (0.3273)  loss_scale: 32768.0000 (36504.0190)  weight_decay: 0.0500 (0.0500)  time: 0.4973  data: 0.0334  max mem: 15572
Epoch: [38]  [ 430/2809]  eta: 0:23:10  lr: 0.000000  min_lr: 0.000000  loss: 4.3369 (4.1844)  class_acc: 0.2917 (0.3276)  loss_scale: 32768.0000 (36417.3364)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1770  max mem: 15572
Epoch: [38]  [ 440/2809]  eta: 0:23:02  lr: 0.000000  min_lr: 0.000000  loss: 4.1327 (4.1809)  class_acc: 0.3333 (0.3283)  loss_scale: 32768.0000 (36334.5850)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.2111  max mem: 15572
Epoch: [38]  [ 450/2809]  eta: 0:22:55  lr: 0.000000  min_lr: 0.000000  loss: 4.1327 (4.1773)  class_acc: 0.3333 (0.3278)  loss_scale: 32768.0000 (36255.5033)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1186  max mem: 15572
Epoch: [38]  [ 460/2809]  eta: 0:22:45  lr: 0.000000  min_lr: 0.000000  loss: 4.1926 (4.1770)  class_acc: 0.2500 (0.3272)  loss_scale: 32768.0000 (36179.8525)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0914  max mem: 15572
Epoch: [38]  [ 470/2809]  eta: 0:22:37  lr: 0.000000  min_lr: 0.000000  loss: 4.2035 (4.1777)  class_acc: 0.2917 (0.3286)  loss_scale: 32768.0000 (36107.4140)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0861  max mem: 15572
Epoch: [38]  [ 480/2809]  eta: 0:22:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1721 (4.1780)  class_acc: 0.2917 (0.3280)  loss_scale: 32768.0000 (36037.9875)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1271  max mem: 15572
Epoch: [38]  [ 490/2809]  eta: 0:22:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1911 (4.1792)  class_acc: 0.2917 (0.3273)  loss_scale: 32768.0000 (35971.3890)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1039  max mem: 15572
Epoch: [38]  [ 500/2809]  eta: 0:22:14  lr: 0.000000  min_lr: 0.000000  loss: 4.2084 (4.1798)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (35907.4491)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0580  max mem: 15572
Epoch: [38]  [ 510/2809]  eta: 0:22:09  lr: 0.000000  min_lr: 0.000000  loss: 4.2666 (4.1826)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (35846.0117)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1110  max mem: 15572
Epoch: [38]  [ 520/2809]  eta: 0:22:05  lr: 0.000000  min_lr: 0.000000  loss: 4.2901 (4.1833)  class_acc: 0.2500 (0.3255)  loss_scale: 32768.0000 (35786.9328)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1636  max mem: 15572
[2025-01-16 08:28:14,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:28:14,673] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 08:28:15,109] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107265
[2025-01-16 08:28:15,109] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:28:15,109] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 530/2809]  eta: 0:21:59  lr: 0.000000  min_lr: 0.000000  loss: 4.1606 (4.1831)  class_acc: 0.2500 (0.3257)  loss_scale: 32768.0000 (35791.7891)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1490  max mem: 15572
Epoch: [38]  [ 540/2809]  eta: 0:21:57  lr: 0.000000  min_lr: 0.000000  loss: 4.1606 (4.1823)  class_acc: 0.2917 (0.3261)  loss_scale: 32768.0000 (35735.8965)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1714  max mem: 15572
Epoch: [38]  [ 550/2809]  eta: 0:21:48  lr: 0.000000  min_lr: 0.000000  loss: 4.0078 (4.1796)  class_acc: 0.3333 (0.3277)  loss_scale: 32768.0000 (35682.0327)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1452  max mem: 15572
Epoch: [38]  [ 560/2809]  eta: 0:21:41  lr: 0.000000  min_lr: 0.000000  loss: 4.0078 (4.1782)  class_acc: 0.4167 (0.3287)  loss_scale: 32768.0000 (35630.0891)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0992  max mem: 15572
Epoch: [38]  [ 570/2809]  eta: 0:21:39  lr: 0.000000  min_lr: 0.000000  loss: 4.2660 (4.1815)  class_acc: 0.3750 (0.3284)  loss_scale: 32768.0000 (35579.9650)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1776  max mem: 15572
Epoch: [38]  [ 580/2809]  eta: 0:21:34  lr: 0.000000  min_lr: 0.000000  loss: 4.2717 (4.1828)  class_acc: 0.2917 (0.3287)  loss_scale: 32768.0000 (35531.5663)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.1978  max mem: 15572
Epoch: [38]  [ 590/2809]  eta: 0:21:26  lr: 0.000000  min_lr: 0.000000  loss: 4.1807 (4.1820)  class_acc: 0.2917 (0.3282)  loss_scale: 32768.0000 (35484.8054)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1174  max mem: 15572
Epoch: [38]  [ 600/2809]  eta: 0:21:18  lr: 0.000000  min_lr: 0.000000  loss: 4.0781 (4.1817)  class_acc: 0.2917 (0.3287)  loss_scale: 32768.0000 (35439.6007)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0815  max mem: 15572
Epoch: [38]  [ 610/2809]  eta: 0:21:12  lr: 0.000000  min_lr: 0.000000  loss: 4.1345 (4.1821)  class_acc: 0.2917 (0.3283)  loss_scale: 32768.0000 (35395.8756)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0954  max mem: 15572
Epoch: [38]  [ 620/2809]  eta: 0:21:04  lr: 0.000000  min_lr: 0.000000  loss: 4.2279 (4.1821)  class_acc: 0.2917 (0.3291)  loss_scale: 32768.0000 (35353.5588)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0709  max mem: 15572
Epoch: [38]  [ 630/2809]  eta: 0:20:56  lr: 0.000000  min_lr: 0.000000  loss: 4.2279 (4.1812)  class_acc: 0.2917 (0.3291)  loss_scale: 32768.0000 (35312.5832)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0605  max mem: 15572
Epoch: [38]  [ 640/2809]  eta: 0:20:51  lr: 0.000000  min_lr: 0.000000  loss: 4.0920 (4.1811)  class_acc: 0.3333 (0.3295)  loss_scale: 32768.0000 (35272.8861)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0905  max mem: 15572
Epoch: [38]  [ 650/2809]  eta: 0:20:47  lr: 0.000000  min_lr: 0.000000  loss: 4.1007 (4.1826)  class_acc: 0.3333 (0.3291)  loss_scale: 32768.0000 (35234.4086)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1463  max mem: 15572
[2025-01-16 08:29:27,778] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:29:27,778] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 08:29:31,173] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107400
[2025-01-16 08:29:31,173] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:29:31,173] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 660/2809]  eta: 0:20:40  lr: 0.000000  min_lr: 0.000000  loss: 4.1297 (4.1804)  class_acc: 0.2917 (0.3290)  loss_scale: 32768.0000 (35494.5356)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1476  max mem: 15572
Epoch: [38]  [ 670/2809]  eta: 0:20:33  lr: 0.000000  min_lr: 0.000000  loss: 4.0345 (4.1796)  class_acc: 0.2917 (0.3284)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0928  max mem: 15572
Epoch: [38]  [ 680/2809]  eta: 0:20:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1198 (4.1793)  class_acc: 0.3333 (0.3290)  loss_scale: 32768.0000 (35414.4611)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0636  max mem: 15572
Epoch: [38]  [ 690/2809]  eta: 0:20:19  lr: 0.000000  min_lr: 0.000000  loss: 4.0587 (4.1777)  class_acc: 0.3333 (0.3288)  loss_scale: 32768.0000 (35376.1621)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.1147  max mem: 15572
Epoch: [38]  [ 700/2809]  eta: 0:20:14  lr: 0.000000  min_lr: 0.000000  loss: 4.0843 (4.1784)  class_acc: 0.2917 (0.3287)  loss_scale: 32768.0000 (35338.9558)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1623  max mem: 15572
Epoch: [38]  [ 710/2809]  eta: 0:20:11  lr: 0.000000  min_lr: 0.000000  loss: 4.1667 (4.1784)  class_acc: 0.2500 (0.3274)  loss_scale: 32768.0000 (35302.7961)  weight_decay: 0.0500 (0.0500)  time: 0.6392  data: 0.1746  max mem: 15572
Epoch: [38]  [ 720/2809]  eta: 0:20:05  lr: 0.000000  min_lr: 0.000000  loss: 4.1819 (4.1790)  class_acc: 0.2083 (0.3267)  loss_scale: 32768.0000 (35267.6394)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1493  max mem: 15572
Epoch: [38]  [ 730/2809]  eta: 0:20:00  lr: 0.000000  min_lr: 0.000000  loss: 4.2725 (4.1805)  class_acc: 0.3333 (0.3271)  loss_scale: 32768.0000 (35233.4446)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1373  max mem: 15572
Epoch: [38]  [ 740/2809]  eta: 0:19:53  lr: 0.000000  min_lr: 0.000000  loss: 4.3257 (4.1822)  class_acc: 0.2917 (0.3269)  loss_scale: 32768.0000 (35200.1727)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1342  max mem: 15572
Epoch: [38]  [ 750/2809]  eta: 0:19:47  lr: 0.000000  min_lr: 0.000000  loss: 4.1888 (4.1810)  class_acc: 0.3750 (0.3278)  loss_scale: 32768.0000 (35167.7870)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1254  max mem: 15572
Epoch: [38]  [ 760/2809]  eta: 0:19:40  lr: 0.000000  min_lr: 0.000000  loss: 4.1810 (4.1814)  class_acc: 0.4167 (0.3280)  loss_scale: 32768.0000 (35136.2523)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1162  max mem: 15572
Epoch: [38]  [ 770/2809]  eta: 0:19:35  lr: 0.000000  min_lr: 0.000000  loss: 4.2116 (4.1820)  class_acc: 0.3333 (0.3283)  loss_scale: 32768.0000 (35105.5357)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1080  max mem: 15572
Epoch: [38]  [ 780/2809]  eta: 0:19:32  lr: 0.000000  min_lr: 0.000000  loss: 4.3053 (4.1819)  class_acc: 0.3333 (0.3283)  loss_scale: 32768.0000 (35075.6056)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1685  max mem: 15572
[2025-01-16 08:30:45,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:30:45,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 790/2809]  eta: 0:19:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1927 (4.1820)  class_acc: 0.2917 (0.3282)  loss_scale: 32768.0000 (35212.1365)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1370  max mem: 15572
Epoch: [38]  [ 800/2809]  eta: 0:19:18  lr: 0.000000  min_lr: 0.000000  loss: 4.1537 (4.1814)  class_acc: 0.2917 (0.3280)  loss_scale: 65536.0000 (35590.7116)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0951  max mem: 15572
[2025-01-16 08:30:55,735] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107549
[2025-01-16 08:30:55,736] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:30:55,736] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 810/2809]  eta: 0:19:12  lr: 0.000000  min_lr: 0.000000  loss: 4.0993 (4.1810)  class_acc: 0.2917 (0.3278)  loss_scale: 65536.0000 (35798.3329)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1314  max mem: 15572
Epoch: [38]  [ 820/2809]  eta: 0:19:08  lr: 0.000000  min_lr: 0.000000  loss: 4.1269 (4.1785)  class_acc: 0.2917 (0.3276)  loss_scale: 32768.0000 (35761.4227)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1557  max mem: 15572
Epoch: [38]  [ 830/2809]  eta: 0:19:01  lr: 0.000000  min_lr: 0.000000  loss: 4.2505 (4.1801)  class_acc: 0.2500 (0.3268)  loss_scale: 32768.0000 (35725.4007)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1288  max mem: 15572
Epoch: [38]  [ 840/2809]  eta: 0:18:54  lr: 0.000000  min_lr: 0.000000  loss: 4.2814 (4.1799)  class_acc: 0.2917 (0.3273)  loss_scale: 32768.0000 (35690.2354)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.1008  max mem: 15572
Epoch: [38]  [ 850/2809]  eta: 0:18:48  lr: 0.000000  min_lr: 0.000000  loss: 4.1771 (4.1800)  class_acc: 0.3333 (0.3268)  loss_scale: 32768.0000 (35655.8966)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1070  max mem: 15572
Epoch: [38]  [ 860/2809]  eta: 0:18:42  lr: 0.000000  min_lr: 0.000000  loss: 4.1730 (4.1796)  class_acc: 0.2917 (0.3268)  loss_scale: 32768.0000 (35622.3554)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1136  max mem: 15572
Epoch: [38]  [ 870/2809]  eta: 0:18:36  lr: 0.000000  min_lr: 0.000000  loss: 4.2170 (4.1812)  class_acc: 0.3333 (0.3266)  loss_scale: 32768.0000 (35589.5844)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1231  max mem: 15572
Epoch: [38]  [ 880/2809]  eta: 0:18:30  lr: 0.000000  min_lr: 0.000000  loss: 4.2415 (4.1811)  class_acc: 0.3333 (0.3269)  loss_scale: 32768.0000 (35557.5573)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1116  max mem: 15572
Epoch: [38]  [ 890/2809]  eta: 0:18:26  lr: 0.000000  min_lr: 0.000000  loss: 4.2411 (4.1831)  class_acc: 0.2500 (0.3259)  loss_scale: 32768.0000 (35526.2492)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1677  max mem: 15572
Epoch: [38]  [ 900/2809]  eta: 0:18:19  lr: 0.000000  min_lr: 0.000000  loss: 4.4081 (4.1855)  class_acc: 0.2500 (0.3258)  loss_scale: 32768.0000 (35495.6360)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1317  max mem: 15572
Epoch: [38]  [ 910/2809]  eta: 0:18:12  lr: 0.000000  min_lr: 0.000000  loss: 4.3840 (4.1873)  class_acc: 0.2917 (0.3252)  loss_scale: 32768.0000 (35465.6948)  weight_decay: 0.0500 (0.0500)  time: 0.5184  data: 0.0557  max mem: 15572
Epoch: [38]  [ 920/2809]  eta: 0:18:07  lr: 0.000000  min_lr: 0.000000  loss: 4.2830 (4.1864)  class_acc: 0.2917 (0.3249)  loss_scale: 32768.0000 (35436.4039)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1256  max mem: 15572
Epoch: [38]  [ 930/2809]  eta: 0:17:58  lr: 0.000000  min_lr: 0.000000  loss: 4.1943 (4.1867)  class_acc: 0.2917 (0.3246)  loss_scale: 32768.0000 (35407.7422)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0802  max mem: 15572
[2025-01-16 08:32:08,058] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:32:08,058] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 940/2809]  eta: 0:17:55  lr: 0.000000  min_lr: 0.000000  loss: 4.1943 (4.1867)  class_acc: 0.2917 (0.3246)  loss_scale: 32768.0000 (35553.8023)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1398  max mem: 15572
Epoch: [38]  [ 950/2809]  eta: 0:17:48  lr: 0.000000  min_lr: 0.000000  loss: 4.1080 (4.1860)  class_acc: 0.2917 (0.3239)  loss_scale: 65536.0000 (35869.0726)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1666  max mem: 15572
[2025-01-16 08:32:24,275] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107701
[2025-01-16 08:32:24,275] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:32:24,275] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 960/2809]  eta: 0:17:45  lr: 0.000000  min_lr: 0.000000  loss: 4.2296 (4.1875)  class_acc: 0.3333 (0.3243)  loss_scale: 65536.0000 (36109.5858)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1502  max mem: 15572
Epoch: [38]  [ 970/2809]  eta: 0:17:38  lr: 0.000000  min_lr: 0.000000  loss: 4.2888 (4.1877)  class_acc: 0.3333 (0.3243)  loss_scale: 32768.0000 (36075.1720)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1685  max mem: 15572
Epoch: [38]  [ 980/2809]  eta: 0:17:32  lr: 0.000000  min_lr: 0.000000  loss: 4.2280 (4.1881)  class_acc: 0.2917 (0.3236)  loss_scale: 32768.0000 (36041.4597)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.1088  max mem: 15572
Epoch: [38]  [ 990/2809]  eta: 0:17:26  lr: 0.000000  min_lr: 0.000000  loss: 4.1732 (4.1864)  class_acc: 0.2917 (0.3241)  loss_scale: 32768.0000 (36008.4279)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1253  max mem: 15572
Epoch: [38]  [1000/2809]  eta: 0:17:22  lr: 0.000000  min_lr: 0.000000  loss: 4.0001 (4.1843)  class_acc: 0.3333 (0.3243)  loss_scale: 32768.0000 (35976.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1661  max mem: 15572
Epoch: [38]  [1010/2809]  eta: 0:17:17  lr: 0.000000  min_lr: 0.000000  loss: 4.1035 (4.1845)  class_acc: 0.3333 (0.3243)  loss_scale: 32768.0000 (35944.3244)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.1853  max mem: 15572
Epoch: [38]  [1020/2809]  eta: 0:17:10  lr: 0.000000  min_lr: 0.000000  loss: 4.1592 (4.1842)  class_acc: 0.3333 (0.3244)  loss_scale: 32768.0000 (35913.2145)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0985  max mem: 15572
Epoch: [38]  [1030/2809]  eta: 0:17:03  lr: 0.000000  min_lr: 0.000000  loss: 4.3326 (4.1859)  class_acc: 0.2917 (0.3244)  loss_scale: 32768.0000 (35882.7081)  weight_decay: 0.0500 (0.0500)  time: 0.5287  data: 0.0634  max mem: 15572
Epoch: [38]  [1040/2809]  eta: 0:16:59  lr: 0.000000  min_lr: 0.000000  loss: 4.3326 (4.1864)  class_acc: 0.3333 (0.3245)  loss_scale: 32768.0000 (35852.7877)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1617  max mem: 15572
Epoch: [38]  [1050/2809]  eta: 0:16:52  lr: 0.000000  min_lr: 0.000000  loss: 4.0915 (4.1857)  class_acc: 0.2500 (0.3237)  loss_scale: 32768.0000 (35823.4367)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1506  max mem: 15572
Epoch: [38]  [1060/2809]  eta: 0:16:47  lr: 0.000000  min_lr: 0.000000  loss: 4.0637 (4.1866)  class_acc: 0.2500 (0.3232)  loss_scale: 32768.0000 (35794.6390)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1087  max mem: 15572
Epoch: [38]  [1070/2809]  eta: 0:16:41  lr: 0.000000  min_lr: 0.000000  loss: 4.2757 (4.1891)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (35766.3791)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1411  max mem: 15572
Epoch: [38]  [1080/2809]  eta: 0:16:36  lr: 0.000000  min_lr: 0.000000  loss: 4.3602 (4.1903)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (35738.6420)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1443  max mem: 15572
[2025-01-16 08:33:38,940] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:33:38,941] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [1090/2809]  eta: 0:16:30  lr: 0.000000  min_lr: 0.000000  loss: 4.2087 (4.1906)  class_acc: 0.2500 (0.3224)  loss_scale: 32768.0000 (35801.5179)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1329  max mem: 15572
Epoch: [38]  [1100/2809]  eta: 0:16:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1602 (4.1899)  class_acc: 0.2500 (0.3222)  loss_scale: 65536.0000 (36071.5858)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1092  max mem: 15572
Epoch: [38]  [1110/2809]  eta: 0:16:18  lr: 0.000000  min_lr: 0.000000  loss: 4.1934 (4.1919)  class_acc: 0.2500 (0.3221)  loss_scale: 65536.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0966  max mem: 15572
Epoch: [38]  [1120/2809]  eta: 0:16:12  lr: 0.000000  min_lr: 0.000000  loss: 4.2597 (4.1926)  class_acc: 0.2917 (0.3219)  loss_scale: 65536.0000 (36597.2667)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0836  max mem: 15572
Epoch: [38]  [1130/2809]  eta: 0:16:06  lr: 0.000000  min_lr: 0.000000  loss: 4.1812 (4.1925)  class_acc: 0.2917 (0.3217)  loss_scale: 65536.0000 (36853.1353)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1139  max mem: 15572
Epoch: [38]  [1140/2809]  eta: 0:16:01  lr: 0.000000  min_lr: 0.000000  loss: 4.1778 (4.1931)  class_acc: 0.2917 (0.3212)  loss_scale: 65536.0000 (37104.5188)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1413  max mem: 15572
Epoch: [38]  [1150/2809]  eta: 0:15:54  lr: 0.000000  min_lr: 0.000000  loss: 4.1828 (4.1929)  class_acc: 0.3333 (0.3211)  loss_scale: 65536.0000 (37351.5343)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0992  max mem: 15572
[2025-01-16 08:34:15,700] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107898
[2025-01-16 08:34:15,700] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:34:15,700] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [1160/2809]  eta: 0:15:48  lr: 0.000000  min_lr: 0.000000  loss: 4.1964 (4.1922)  class_acc: 0.3333 (0.3212)  loss_scale: 65536.0000 (37453.1748)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.1012  max mem: 15572
Epoch: [38]  [1170/2809]  eta: 0:15:41  lr: 0.000000  min_lr: 0.000000  loss: 4.1627 (4.1921)  class_acc: 0.2500 (0.3207)  loss_scale: 32768.0000 (37413.1648)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0704  max mem: 15572
Epoch: [38]  [1180/2809]  eta: 0:15:35  lr: 0.000000  min_lr: 0.000000  loss: 4.2147 (4.1927)  class_acc: 0.2917 (0.3206)  loss_scale: 32768.0000 (37373.8323)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0447  max mem: 15572
Epoch: [38]  [1190/2809]  eta: 0:15:28  lr: 0.000000  min_lr: 0.000000  loss: 4.2147 (4.1922)  class_acc: 0.2917 (0.3208)  loss_scale: 32768.0000 (37335.1604)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0680  max mem: 15572
Epoch: [38]  [1200/2809]  eta: 0:15:22  lr: 0.000000  min_lr: 0.000000  loss: 4.1119 (4.1919)  class_acc: 0.3333 (0.3208)  loss_scale: 32768.0000 (37297.1324)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0692  max mem: 15572
Epoch: [38]  [1210/2809]  eta: 0:15:17  lr: 0.000000  min_lr: 0.000000  loss: 4.1975 (4.1925)  class_acc: 0.3750 (0.3211)  loss_scale: 32768.0000 (37259.7325)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1274  max mem: 15572
Epoch: [38]  [1220/2809]  eta: 0:15:11  lr: 0.000000  min_lr: 0.000000  loss: 4.2353 (4.1932)  class_acc: 0.3333 (0.3211)  loss_scale: 32768.0000 (37222.9451)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1465  max mem: 15572
Epoch: [38]  [1230/2809]  eta: 0:15:05  lr: 0.000000  min_lr: 0.000000  loss: 4.2171 (4.1938)  class_acc: 0.3333 (0.3208)  loss_scale: 32768.0000 (37186.7555)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0883  max mem: 15572
Epoch: [38]  [1240/2809]  eta: 0:14:59  lr: 0.000000  min_lr: 0.000000  loss: 4.1943 (4.1938)  class_acc: 0.2500 (0.3203)  loss_scale: 32768.0000 (37151.1491)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0888  max mem: 15572
Epoch: [38]  [1250/2809]  eta: 0:14:53  lr: 0.000000  min_lr: 0.000000  loss: 4.1989 (4.1931)  class_acc: 0.2917 (0.3203)  loss_scale: 32768.0000 (37116.1119)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1328  max mem: 15572
[2025-01-16 08:35:12,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=108000, skipped=674, lr=[2.6532963583675663e-09, 2.6532963583675663e-09, 3.790423369096524e-09, 3.790423369096524e-09, 5.414890527280749e-09, 5.414890527280749e-09, 7.735557896115355e-09, 7.735557896115355e-09, 1.1050796994450509e-08, 1.1050796994450509e-08, 1.5786852849215013e-08, 1.5786852849215013e-08, 2.255264692745002e-08, 2.255264692745002e-08, 3.221806703921432e-08, 3.221806703921432e-08, 4.602581005602045e-08, 4.602581005602045e-08, 6.575115722288638e-08, 6.575115722288638e-08, 9.393022460412338e-08, 9.393022460412338e-08, 1.341860351487477e-07, 1.341860351487477e-07, 1.9169433592678244e-07, 1.9169433592678244e-07, 2.7384905132397494e-07, 2.7384905132397494e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:35:12,029] [INFO] [timer.py:260:stop] epoch=0/micro_step=108000/global_step=108000, RunningAvgSamplesPerSec=27.99527212995731, CurrSamplesPerSec=23.533977967718407, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [38]  [1260/2809]  eta: 0:14:47  lr: 0.000000  min_lr: 0.000000  loss: 4.2037 (4.1936)  class_acc: 0.3333 (0.3207)  loss_scale: 32768.0000 (37081.6305)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1065  max mem: 15572
Epoch: [38]  [1270/2809]  eta: 0:14:41  lr: 0.000000  min_lr: 0.000000  loss: 4.2531 (4.1943)  class_acc: 0.3750 (0.3212)  loss_scale: 32768.0000 (37047.6916)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0953  max mem: 15572
Epoch: [38]  [1280/2809]  eta: 0:14:37  lr: 0.000000  min_lr: 0.000000  loss: 4.3296 (4.1946)  class_acc: 0.3750 (0.3216)  loss_scale: 32768.0000 (37014.2826)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.1735  max mem: 15572
[2025-01-16 08:35:28,558] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:35:28,559] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [1290/2809]  eta: 0:14:31  lr: 0.000000  min_lr: 0.000000  loss: 4.2152 (4.1941)  class_acc: 0.2917 (0.3210)  loss_scale: 32768.0000 (37133.6824)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1705  max mem: 15572
[2025-01-16 08:35:36,097] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108039
[2025-01-16 08:35:36,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:35:36,098] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [1300/2809]  eta: 0:14:25  lr: 0.000000  min_lr: 0.000000  loss: 4.1395 (4.1924)  class_acc: 0.2917 (0.3216)  loss_scale: 65536.0000 (37251.2467)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1205  max mem: 15572
Epoch: [38]  [1310/2809]  eta: 0:14:18  lr: 0.000000  min_lr: 0.000000  loss: 3.9200 (4.1910)  class_acc: 0.3750 (0.3220)  loss_scale: 32768.0000 (37217.0496)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0735  max mem: 15572
Epoch: [38]  [1320/2809]  eta: 0:14:14  lr: 0.000000  min_lr: 0.000000  loss: 4.1248 (4.1903)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (37183.3702)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1360  max mem: 15572
Epoch: [38]  [1330/2809]  eta: 0:14:08  lr: 0.000000  min_lr: 0.000000  loss: 4.1462 (4.1898)  class_acc: 0.2500 (0.3216)  loss_scale: 32768.0000 (37150.1968)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1689  max mem: 15572
Epoch: [38]  [1340/2809]  eta: 0:14:01  lr: 0.000000  min_lr: 0.000000  loss: 4.1032 (4.1889)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (37117.5183)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0398  max mem: 15572
Epoch: [38]  [1350/2809]  eta: 0:13:55  lr: 0.000000  min_lr: 0.000000  loss: 4.1481 (4.1887)  class_acc: 0.2917 (0.3213)  loss_scale: 32768.0000 (37085.3235)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0430  max mem: 15572
Epoch: [38]  [1360/2809]  eta: 0:13:48  lr: 0.000000  min_lr: 0.000000  loss: 4.2736 (4.1896)  class_acc: 0.2917 (0.3211)  loss_scale: 32768.0000 (37053.6018)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0734  max mem: 15572
Epoch: [38]  [1370/2809]  eta: 0:13:42  lr: 0.000000  min_lr: 0.000000  loss: 4.3383 (4.1904)  class_acc: 0.2917 (0.3212)  loss_scale: 32768.0000 (37022.3428)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0926  max mem: 15572
Epoch: [38]  [1380/2809]  eta: 0:13:37  lr: 0.000000  min_lr: 0.000000  loss: 4.1383 (4.1890)  class_acc: 0.3750 (0.3217)  loss_scale: 32768.0000 (36991.5366)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1668  max mem: 15572
Epoch: [38]  [1390/2809]  eta: 0:13:31  lr: 0.000000  min_lr: 0.000000  loss: 4.0741 (4.1889)  class_acc: 0.3750 (0.3215)  loss_scale: 32768.0000 (36961.1733)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1469  max mem: 15572
Epoch: [38]  [1400/2809]  eta: 0:13:26  lr: 0.000000  min_lr: 0.000000  loss: 4.1809 (4.1890)  class_acc: 0.3333 (0.3220)  loss_scale: 32768.0000 (36931.2434)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1331  max mem: 15572
Epoch: [38]  [1410/2809]  eta: 0:13:20  lr: 0.000000  min_lr: 0.000000  loss: 4.1905 (4.1883)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (36901.7378)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1048  max mem: 15572
Epoch: [38]  [1420/2809]  eta: 0:13:14  lr: 0.000000  min_lr: 0.000000  loss: 4.1676 (4.1887)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (36872.6474)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1111  max mem: 15572
[2025-01-16 08:36:47,985] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:36:47,985] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [1430/2809]  eta: 0:13:09  lr: 0.000000  min_lr: 0.000000  loss: 4.1676 (4.1889)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (36958.4570)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1679  max mem: 15572
[2025-01-16 08:36:54,508] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108180
[2025-01-16 08:36:54,508] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:36:54,508] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [1440/2809]  eta: 0:13:03  lr: 0.000000  min_lr: 0.000000  loss: 4.2031 (4.1891)  class_acc: 0.2917 (0.3222)  loss_scale: 65536.0000 (37088.5552)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1081  max mem: 15572
Epoch: [38]  [1450/2809]  eta: 0:12:57  lr: 0.000000  min_lr: 0.000000  loss: 4.2964 (4.1894)  class_acc: 0.2500 (0.3221)  loss_scale: 32768.0000 (37058.7788)  weight_decay: 0.0500 (0.0500)  time: 0.5305  data: 0.0754  max mem: 15572
Epoch: [38]  [1460/2809]  eta: 0:12:51  lr: 0.000000  min_lr: 0.000000  loss: 4.1499 (4.1895)  class_acc: 0.2500 (0.3216)  loss_scale: 32768.0000 (37029.4100)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1079  max mem: 15572
Epoch: [38]  [1470/2809]  eta: 0:12:46  lr: 0.000000  min_lr: 0.000000  loss: 4.1717 (4.1900)  class_acc: 0.2917 (0.3217)  loss_scale: 32768.0000 (37000.4405)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1496  max mem: 15572
Epoch: [38]  [1480/2809]  eta: 0:12:40  lr: 0.000000  min_lr: 0.000000  loss: 4.2742 (4.1899)  class_acc: 0.3333 (0.3216)  loss_scale: 32768.0000 (36971.8623)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1709  max mem: 15572
Epoch: [38]  [1490/2809]  eta: 0:12:35  lr: 0.000000  min_lr: 0.000000  loss: 4.3046 (4.1906)  class_acc: 0.3333 (0.3219)  loss_scale: 32768.0000 (36943.6673)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.1915  max mem: 15572
Epoch: [38]  [1500/2809]  eta: 0:12:30  lr: 0.000000  min_lr: 0.000000  loss: 4.2642 (4.1899)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (36915.8481)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1564  max mem: 15572
Epoch: [38]  [1510/2809]  eta: 0:12:23  lr: 0.000000  min_lr: 0.000000  loss: 4.2200 (4.1898)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (36888.3971)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0983  max mem: 15572
Epoch: [38]  [1520/2809]  eta: 0:12:17  lr: 0.000000  min_lr: 0.000000  loss: 4.1776 (4.1899)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (36861.3070)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0824  max mem: 15572
Epoch: [38]  [1530/2809]  eta: 0:12:12  lr: 0.000000  min_lr: 0.000000  loss: 4.2011 (4.1900)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (36834.5709)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1097  max mem: 15572
Epoch: [38]  [1540/2809]  eta: 0:12:06  lr: 0.000000  min_lr: 0.000000  loss: 4.2105 (4.1890)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (36808.1817)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1138  max mem: 15572
Epoch: [38]  [1550/2809]  eta: 0:12:00  lr: 0.000000  min_lr: 0.000000  loss: 4.2105 (4.1887)  class_acc: 0.3750 (0.3231)  loss_scale: 32768.0000 (36782.1328)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0763  max mem: 15572
Epoch: [38]  [1560/2809]  eta: 0:11:54  lr: 0.000000  min_lr: 0.000000  loss: 4.3313 (4.1891)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (36756.4177)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0711  max mem: 15572
[2025-01-16 08:38:08,283] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:38:08,384] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [1570/2809]  eta: 0:11:48  lr: 0.000000  min_lr: 0.000000  loss: 4.2848 (4.1886)  class_acc: 0.2917 (0.3228)  loss_scale: 32768.0000 (36814.4621)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1085  max mem: 15572
Epoch: [38]  [1580/2809]  eta: 0:11:42  lr: 0.000000  min_lr: 0.000000  loss: 4.1426 (4.1884)  class_acc: 0.3750 (0.3230)  loss_scale: 65536.0000 (36996.1290)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1122  max mem: 15572
Epoch: [38]  [1590/2809]  eta: 0:11:37  lr: 0.000000  min_lr: 0.000000  loss: 4.1984 (4.1893)  class_acc: 0.2500 (0.3228)  loss_scale: 65536.0000 (37175.5123)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1364  max mem: 15572
Epoch: [38]  [1600/2809]  eta: 0:11:32  lr: 0.000000  min_lr: 0.000000  loss: 4.3154 (4.1896)  class_acc: 0.2500 (0.3227)  loss_scale: 65536.0000 (37352.6546)  weight_decay: 0.0500 (0.0500)  time: 0.6476  data: 0.1906  max mem: 15572
Epoch: [38]  [1610/2809]  eta: 0:11:25  lr: 0.000000  min_lr: 0.000000  loss: 4.1869 (4.1891)  class_acc: 0.3333 (0.3228)  loss_scale: 65536.0000 (37527.5978)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0977  max mem: 15572
Epoch: [38]  [1620/2809]  eta: 0:11:20  lr: 0.000000  min_lr: 0.000000  loss: 4.1400 (4.1886)  class_acc: 0.3333 (0.3228)  loss_scale: 65536.0000 (37700.3825)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0781  max mem: 15572
[2025-01-16 08:38:42,832] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108367
[2025-01-16 08:38:42,832] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:38:42,832] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [1630/2809]  eta: 0:11:14  lr: 0.000000  min_lr: 0.000000  loss: 4.1109 (4.1883)  class_acc: 0.3333 (0.3229)  loss_scale: 65536.0000 (37750.5040)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1745  max mem: 15572
Epoch: [38]  [1640/2809]  eta: 0:11:09  lr: 0.000000  min_lr: 0.000000  loss: 4.1752 (4.1885)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (37720.1414)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1717  max mem: 15572
Epoch: [38]  [1650/2809]  eta: 0:11:03  lr: 0.000000  min_lr: 0.000000  loss: 4.2483 (4.1892)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (37690.1466)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1239  max mem: 15572
Epoch: [38]  [1660/2809]  eta: 0:10:57  lr: 0.000000  min_lr: 0.000000  loss: 4.1519 (4.1889)  class_acc: 0.2500 (0.3227)  loss_scale: 32768.0000 (37660.5129)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1072  max mem: 15572
Epoch: [38]  [1670/2809]  eta: 0:10:52  lr: 0.000000  min_lr: 0.000000  loss: 4.1095 (4.1898)  class_acc: 0.2500 (0.3225)  loss_scale: 32768.0000 (37631.2340)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1179  max mem: 15572
Epoch: [38]  [1680/2809]  eta: 0:10:46  lr: 0.000000  min_lr: 0.000000  loss: 4.1376 (4.1897)  class_acc: 0.2917 (0.3224)  loss_scale: 32768.0000 (37602.3034)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1155  max mem: 15572
Epoch: [38]  [1690/2809]  eta: 0:10:40  lr: 0.000000  min_lr: 0.000000  loss: 4.2759 (4.1901)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (37573.7150)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0954  max mem: 15572
Epoch: [38]  [1700/2809]  eta: 0:10:34  lr: 0.000000  min_lr: 0.000000  loss: 4.3348 (4.1904)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (37545.4627)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0929  max mem: 15572
Epoch: [38]  [1710/2809]  eta: 0:10:28  lr: 0.000000  min_lr: 0.000000  loss: 4.1768 (4.1901)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (37517.5406)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1112  max mem: 15572
Epoch: [38]  [1720/2809]  eta: 0:10:23  lr: 0.000000  min_lr: 0.000000  loss: 4.1187 (4.1902)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (37489.9431)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1634  max mem: 15572
Epoch: [38]  [1730/2809]  eta: 0:10:17  lr: 0.000000  min_lr: 0.000000  loss: 4.1313 (4.1900)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (37462.6644)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1310  max mem: 15572
Epoch: [38]  [1740/2809]  eta: 0:10:11  lr: 0.000000  min_lr: 0.000000  loss: 4.1313 (4.1896)  class_acc: 0.3333 (0.3227)  loss_scale: 32768.0000 (37435.6990)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0540  max mem: 15572
Epoch: [38]  [1750/2809]  eta: 0:10:05  lr: 0.000000  min_lr: 0.000000  loss: 4.1860 (4.1899)  class_acc: 0.2083 (0.3224)  loss_scale: 32768.0000 (37409.0417)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0436  max mem: 15572
[2025-01-16 08:39:55,506] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:39:55,506] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 08:39:56,806] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108499
[2025-01-16 08:39:56,806] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:39:56,807] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [1760/2809]  eta: 0:09:59  lr: 0.000000  min_lr: 0.000000  loss: 4.2735 (4.1902)  class_acc: 0.2083 (0.3223)  loss_scale: 32768.0000 (37438.5099)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0847  max mem: 15572
Epoch: [38]  [1770/2809]  eta: 0:09:53  lr: 0.000000  min_lr: 0.000000  loss: 4.2735 (4.1907)  class_acc: 0.3750 (0.3227)  loss_scale: 32768.0000 (37412.1378)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0948  max mem: 15572
Epoch: [38]  [1780/2809]  eta: 0:09:48  lr: 0.000000  min_lr: 0.000000  loss: 4.1334 (4.1906)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (37386.0618)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1166  max mem: 15572
Epoch: [38]  [1790/2809]  eta: 0:09:42  lr: 0.000000  min_lr: 0.000000  loss: 4.1561 (4.1904)  class_acc: 0.2500 (0.3224)  loss_scale: 32768.0000 (37360.2769)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1095  max mem: 15572
Epoch: [38]  [1800/2809]  eta: 0:09:36  lr: 0.000000  min_lr: 0.000000  loss: 4.2735 (4.1907)  class_acc: 0.2500 (0.3222)  loss_scale: 32768.0000 (37334.7785)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0799  max mem: 15572
Epoch: [38]  [1810/2809]  eta: 0:09:30  lr: 0.000000  min_lr: 0.000000  loss: 4.2315 (4.1910)  class_acc: 0.2500 (0.3219)  loss_scale: 32768.0000 (37309.5616)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1270  max mem: 15572
Epoch: [38]  [1820/2809]  eta: 0:09:24  lr: 0.000000  min_lr: 0.000000  loss: 4.2315 (4.1904)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (37284.6216)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1169  max mem: 15572
Epoch: [38]  [1830/2809]  eta: 0:09:18  lr: 0.000000  min_lr: 0.000000  loss: 4.2286 (4.1908)  class_acc: 0.3750 (0.3221)  loss_scale: 32768.0000 (37259.9541)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0931  max mem: 15572
Epoch: [38]  [1840/2809]  eta: 0:09:13  lr: 0.000000  min_lr: 0.000000  loss: 4.0891 (4.1899)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (37235.5546)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1731  max mem: 15572
Epoch: [38]  [1850/2809]  eta: 0:09:07  lr: 0.000000  min_lr: 0.000000  loss: 3.9835 (4.1893)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (37211.4187)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1732  max mem: 15572
Epoch: [38]  [1860/2809]  eta: 0:09:02  lr: 0.000000  min_lr: 0.000000  loss: 4.1666 (4.1893)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (37187.5422)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.0981  max mem: 15572
Epoch: [38]  [1870/2809]  eta: 0:08:56  lr: 0.000000  min_lr: 0.000000  loss: 4.1995 (4.1896)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (37163.9209)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1486  max mem: 15572
Epoch: [38]  [1880/2809]  eta: 0:08:50  lr: 0.000000  min_lr: 0.000000  loss: 4.3014 (4.1900)  class_acc: 0.2500 (0.3219)  loss_scale: 32768.0000 (37140.5508)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1327  max mem: 15572
[2025-01-16 08:41:09,616] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:41:09,619] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [1890/2809]  eta: 0:08:45  lr: 0.000000  min_lr: 0.000000  loss: 4.3106 (4.1905)  class_acc: 0.2500 (0.3219)  loss_scale: 32768.0000 (37204.0698)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0873  max mem: 15572
Epoch: [38]  [1900/2809]  eta: 0:08:38  lr: 0.000000  min_lr: 0.000000  loss: 4.1964 (4.1910)  class_acc: 0.2917 (0.3221)  loss_scale: 65536.0000 (37353.1068)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0468  max mem: 15572
Epoch: [38]  [1910/2809]  eta: 0:08:32  lr: 0.000000  min_lr: 0.000000  loss: 4.1710 (4.1914)  class_acc: 0.3333 (0.3221)  loss_scale: 65536.0000 (37500.5840)  weight_decay: 0.0500 (0.0500)  time: 0.4735  data: 0.0275  max mem: 15572
Epoch: [38]  [1920/2809]  eta: 0:08:26  lr: 0.000000  min_lr: 0.000000  loss: 4.1568 (4.1905)  class_acc: 0.3333 (0.3222)  loss_scale: 65536.0000 (37646.5258)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0849  max mem: 15572
Epoch: [38]  [1930/2809]  eta: 0:08:21  lr: 0.000000  min_lr: 0.000000  loss: 4.0639 (4.1904)  class_acc: 0.2917 (0.3222)  loss_scale: 65536.0000 (37790.9560)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0937  max mem: 15572
Epoch: [38]  [1940/2809]  eta: 0:08:15  lr: 0.000000  min_lr: 0.000000  loss: 4.0639 (4.1897)  class_acc: 0.3333 (0.3221)  loss_scale: 65536.0000 (37933.8980)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0811  max mem: 15572
[2025-01-16 08:41:43,074] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108690
[2025-01-16 08:41:43,074] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:41:43,075] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [1950/2809]  eta: 0:08:09  lr: 0.000000  min_lr: 0.000000  loss: 4.1765 (4.1903)  class_acc: 0.3333 (0.3221)  loss_scale: 65536.0000 (38024.9882)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1228  max mem: 15572
Epoch: [38]  [1960/2809]  eta: 0:08:03  lr: 0.000000  min_lr: 0.000000  loss: 4.1886 (4.1903)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (37998.1805)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0922  max mem: 15572
Epoch: [38]  [1970/2809]  eta: 0:07:58  lr: 0.000000  min_lr: 0.000000  loss: 4.1850 (4.1901)  class_acc: 0.3333 (0.3226)  loss_scale: 32768.0000 (37971.6449)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1401  max mem: 15572
Epoch: [38]  [1980/2809]  eta: 0:07:53  lr: 0.000000  min_lr: 0.000000  loss: 4.2118 (4.1907)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (37945.3771)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.1972  max mem: 15572
Epoch: [38]  [1990/2809]  eta: 0:07:47  lr: 0.000000  min_lr: 0.000000  loss: 4.3332 (4.1913)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (37919.3732)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1287  max mem: 15572
Epoch: [38]  [2000/2809]  eta: 0:07:41  lr: 0.000000  min_lr: 0.000000  loss: 4.3212 (4.1914)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (37893.6292)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0848  max mem: 15572
Epoch: [38]  [2010/2809]  eta: 0:07:35  lr: 0.000000  min_lr: 0.000000  loss: 4.2087 (4.1909)  class_acc: 0.3750 (0.3228)  loss_scale: 32768.0000 (37868.1412)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0377  max mem: 15572
Epoch: [38]  [2020/2809]  eta: 0:07:29  lr: 0.000000  min_lr: 0.000000  loss: 4.1753 (4.1912)  class_acc: 0.4167 (0.3231)  loss_scale: 32768.0000 (37842.9055)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0605  max mem: 15572
Epoch: [38]  [2030/2809]  eta: 0:07:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1476 (4.1910)  class_acc: 0.3333 (0.3233)  loss_scale: 32768.0000 (37817.9183)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1345  max mem: 15572
Epoch: [38]  [2040/2809]  eta: 0:07:18  lr: 0.000000  min_lr: 0.000000  loss: 4.0963 (4.1910)  class_acc: 0.3333 (0.3235)  loss_scale: 32768.0000 (37793.1759)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1601  max mem: 15572
Epoch: [38]  [2050/2809]  eta: 0:07:12  lr: 0.000000  min_lr: 0.000000  loss: 4.0963 (4.1903)  class_acc: 0.3333 (0.3233)  loss_scale: 32768.0000 (37768.6748)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1614  max mem: 15572
Epoch: [38]  [2060/2809]  eta: 0:07:06  lr: 0.000000  min_lr: 0.000000  loss: 4.1344 (4.1907)  class_acc: 0.2500 (0.3231)  loss_scale: 32768.0000 (37744.4115)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1197  max mem: 15572
Epoch: [38]  [2070/2809]  eta: 0:07:01  lr: 0.000000  min_lr: 0.000000  loss: 4.2725 (4.1912)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (37720.3824)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1174  max mem: 15572
[2025-01-16 08:42:55,754] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:42:55,755] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [2080/2809]  eta: 0:06:55  lr: 0.000000  min_lr: 0.000000  loss: 4.3512 (4.1915)  class_acc: 0.3750 (0.3233)  loss_scale: 32768.0000 (37759.5694)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1349  max mem: 15572
Epoch: [38]  [2090/2809]  eta: 0:06:49  lr: 0.000000  min_lr: 0.000000  loss: 4.2424 (4.1916)  class_acc: 0.3333 (0.3233)  loss_scale: 65536.0000 (37892.4075)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0785  max mem: 15572
[2025-01-16 08:43:07,723] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108840
[2025-01-16 08:43:07,723] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:43:07,723] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [2100/2809]  eta: 0:06:43  lr: 0.000000  min_lr: 0.000000  loss: 4.1937 (4.1916)  class_acc: 0.2917 (0.3232)  loss_scale: 65536.0000 (37977.1918)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0948  max mem: 15572
Epoch: [38]  [2110/2809]  eta: 0:06:38  lr: 0.000000  min_lr: 0.000000  loss: 4.2206 (4.1911)  class_acc: 0.2917 (0.3232)  loss_scale: 32768.0000 (37952.5154)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1506  max mem: 15572
Epoch: [38]  [2120/2809]  eta: 0:06:32  lr: 0.000000  min_lr: 0.000000  loss: 4.2206 (4.1912)  class_acc: 0.2917 (0.3234)  loss_scale: 32768.0000 (37928.0717)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1442  max mem: 15572
Epoch: [38]  [2130/2809]  eta: 0:06:27  lr: 0.000000  min_lr: 0.000000  loss: 4.1292 (4.1907)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (37903.8573)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1597  max mem: 15572
Epoch: [38]  [2140/2809]  eta: 0:06:21  lr: 0.000000  min_lr: 0.000000  loss: 4.0718 (4.1900)  class_acc: 0.3333 (0.3235)  loss_scale: 32768.0000 (37879.8692)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.1741  max mem: 15572
Epoch: [38]  [2150/2809]  eta: 0:06:15  lr: 0.000000  min_lr: 0.000000  loss: 4.0718 (4.1905)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (37856.1041)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1553  max mem: 15572
Epoch: [38]  [2160/2809]  eta: 0:06:10  lr: 0.000000  min_lr: 0.000000  loss: 4.1435 (4.1904)  class_acc: 0.2500 (0.3232)  loss_scale: 32768.0000 (37832.5590)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1022  max mem: 15572
Epoch: [38]  [2170/2809]  eta: 0:06:04  lr: 0.000000  min_lr: 0.000000  loss: 4.1008 (4.1906)  class_acc: 0.2917 (0.3232)  loss_scale: 32768.0000 (37809.2308)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1271  max mem: 15572
Epoch: [38]  [2180/2809]  eta: 0:05:58  lr: 0.000000  min_lr: 0.000000  loss: 4.0898 (4.1903)  class_acc: 0.2917 (0.3230)  loss_scale: 32768.0000 (37786.1165)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1530  max mem: 15572
Epoch: [38]  [2190/2809]  eta: 0:05:53  lr: 0.000000  min_lr: 0.000000  loss: 4.2008 (4.1905)  class_acc: 0.2500 (0.3227)  loss_scale: 32768.0000 (37763.2131)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0776  max mem: 15572
Epoch: [38]  [2200/2809]  eta: 0:05:47  lr: 0.000000  min_lr: 0.000000  loss: 4.2887 (4.1905)  class_acc: 0.2500 (0.3229)  loss_scale: 32768.0000 (37740.5179)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0802  max mem: 15572
Epoch: [38]  [2210/2809]  eta: 0:05:41  lr: 0.000000  min_lr: 0.000000  loss: 4.2004 (4.1905)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (37718.0280)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0923  max mem: 15572
Epoch: [38]  [2220/2809]  eta: 0:05:35  lr: 0.000000  min_lr: 0.000000  loss: 4.1277 (4.1897)  class_acc: 0.3333 (0.3233)  loss_scale: 32768.0000 (37695.7407)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0988  max mem: 15572
[2025-01-16 08:44:22,559] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:44:22,559] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [2230/2809]  eta: 0:05:30  lr: 0.000000  min_lr: 0.000000  loss: 4.0393 (4.1885)  class_acc: 0.3333 (0.3235)  loss_scale: 32768.0000 (37732.4034)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1315  max mem: 15572
Epoch: [38]  [2240/2809]  eta: 0:05:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1530 (4.1894)  class_acc: 0.3333 (0.3237)  loss_scale: 65536.0000 (37856.4712)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1318  max mem: 15572
Epoch: [38]  [2250/2809]  eta: 0:05:19  lr: 0.000000  min_lr: 0.000000  loss: 4.3124 (4.1901)  class_acc: 0.2917 (0.3238)  loss_scale: 65536.0000 (37979.4367)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1580  max mem: 15572
[2025-01-16 08:44:39,070] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108997
[2025-01-16 08:44:39,070] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:44:39,070] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 08:44:39,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=109000, skipped=681, lr=[1.7612437616330731e-09, 1.7612437616330731e-09, 2.516062516618676e-09, 2.516062516618676e-09, 3.5943750237409664e-09, 3.5943750237409664e-09, 5.134821462487095e-09, 5.134821462487095e-09, 7.335459232124422e-09, 7.335459232124422e-09, 1.047922747446346e-08, 1.047922747446346e-08, 1.497032496351923e-08, 1.497032496351923e-08, 2.1386178519313188e-08, 2.1386178519313188e-08, 3.055168359901884e-08, 3.055168359901884e-08, 4.364526228431263e-08, 4.364526228431263e-08, 6.235037469187519e-08, 6.235037469187519e-08, 8.907196384553599e-08, 8.907196384553599e-08, 1.2724566263648e-07, 1.2724566263648e-07, 1.817795180521143e-07, 1.817795180521143e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:44:39,880] [INFO] [timer.py:260:stop] epoch=0/micro_step=109000/global_step=109000, RunningAvgSamplesPerSec=27.996890968967644, CurrSamplesPerSec=31.519175781440804, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [38]  [2260/2809]  eta: 0:05:13  lr: 0.000000  min_lr: 0.000000  loss: 4.3124 (4.1902)  class_acc: 0.2917 (0.3237)  loss_scale: 65536.0000 (38014.3582)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1371  max mem: 15572
Epoch: [38]  [2270/2809]  eta: 0:05:07  lr: 0.000000  min_lr: 0.000000  loss: 4.2705 (4.1897)  class_acc: 0.2917 (0.3238)  loss_scale: 32768.0000 (37991.2567)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1320  max mem: 15572
Epoch: [38]  [2280/2809]  eta: 0:05:01  lr: 0.000000  min_lr: 0.000000  loss: 4.1854 (4.1897)  class_acc: 0.3333 (0.3239)  loss_scale: 32768.0000 (37968.3577)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1316  max mem: 15572
Epoch: [38]  [2290/2809]  eta: 0:04:56  lr: 0.000000  min_lr: 0.000000  loss: 4.1854 (4.1895)  class_acc: 0.3333 (0.3239)  loss_scale: 32768.0000 (37945.6587)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0940  max mem: 15572
Epoch: [38]  [2300/2809]  eta: 0:04:50  lr: 0.000000  min_lr: 0.000000  loss: 4.1027 (4.1891)  class_acc: 0.3333 (0.3241)  loss_scale: 32768.0000 (37923.1569)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1418  max mem: 15572
Epoch: [38]  [2310/2809]  eta: 0:04:44  lr: 0.000000  min_lr: 0.000000  loss: 4.0765 (4.1888)  class_acc: 0.2500 (0.3240)  loss_scale: 32768.0000 (37900.8498)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1318  max mem: 15572
Epoch: [38]  [2320/2809]  eta: 0:04:38  lr: 0.000000  min_lr: 0.000000  loss: 4.1340 (4.1886)  class_acc: 0.2500 (0.3240)  loss_scale: 32768.0000 (37878.7350)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.1020  max mem: 15572
Epoch: [38]  [2330/2809]  eta: 0:04:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1305 (4.1881)  class_acc: 0.3333 (0.3243)  loss_scale: 32768.0000 (37856.8100)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0955  max mem: 15572
Epoch: [38]  [2340/2809]  eta: 0:04:27  lr: 0.000000  min_lr: 0.000000  loss: 4.1863 (4.1885)  class_acc: 0.3333 (0.3242)  loss_scale: 32768.0000 (37835.0722)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1405  max mem: 15572
Epoch: [38]  [2350/2809]  eta: 0:04:22  lr: 0.000000  min_lr: 0.000000  loss: 4.1909 (4.1880)  class_acc: 0.3333 (0.3245)  loss_scale: 32768.0000 (37813.5194)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1785  max mem: 15572
Epoch: [38]  [2360/2809]  eta: 0:04:16  lr: 0.000000  min_lr: 0.000000  loss: 3.9435 (4.1871)  class_acc: 0.3750 (0.3248)  loss_scale: 32768.0000 (37792.1491)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1279  max mem: 15572
Epoch: [38]  [2370/2809]  eta: 0:04:10  lr: 0.000000  min_lr: 0.000000  loss: 4.0799 (4.1874)  class_acc: 0.3750 (0.3248)  loss_scale: 32768.0000 (37770.9591)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0962  max mem: 15572
Epoch: [38]  [2380/2809]  eta: 0:04:04  lr: 0.000000  min_lr: 0.000000  loss: 4.2785 (4.1882)  class_acc: 0.2917 (0.3247)  loss_scale: 32768.0000 (37749.9471)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1482  max mem: 15572
[2025-01-16 08:45:53,850] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:45:53,850] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 08:45:55,212] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109129
[2025-01-16 08:45:55,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:45:55,213] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [2390/2809]  eta: 0:03:59  lr: 0.000000  min_lr: 0.000000  loss: 4.2146 (4.1880)  class_acc: 0.3333 (0.3249)  loss_scale: 32768.0000 (37770.2250)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1716  max mem: 15572
Epoch: [38]  [2400/2809]  eta: 0:03:53  lr: 0.000000  min_lr: 0.000000  loss: 4.1166 (4.1878)  class_acc: 0.3333 (0.3249)  loss_scale: 32768.0000 (37749.3911)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1516  max mem: 15572
Epoch: [38]  [2410/2809]  eta: 0:03:47  lr: 0.000000  min_lr: 0.000000  loss: 4.1166 (4.1872)  class_acc: 0.4167 (0.3254)  loss_scale: 32768.0000 (37728.7300)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1375  max mem: 15572
Epoch: [38]  [2420/2809]  eta: 0:03:42  lr: 0.000000  min_lr: 0.000000  loss: 4.1947 (4.1870)  class_acc: 0.3750 (0.3254)  loss_scale: 32768.0000 (37708.2396)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1109  max mem: 15572
Epoch: [38]  [2430/2809]  eta: 0:03:36  lr: 0.000000  min_lr: 0.000000  loss: 4.1375 (4.1862)  class_acc: 0.3333 (0.3255)  loss_scale: 32768.0000 (37687.9177)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0549  max mem: 15572
Epoch: [38]  [2440/2809]  eta: 0:03:30  lr: 0.000000  min_lr: 0.000000  loss: 4.0752 (4.1861)  class_acc: 0.2917 (0.3254)  loss_scale: 32768.0000 (37667.7624)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0758  max mem: 15572
Epoch: [38]  [2450/2809]  eta: 0:03:24  lr: 0.000000  min_lr: 0.000000  loss: 4.0752 (4.1857)  class_acc: 0.3333 (0.3255)  loss_scale: 32768.0000 (37647.7715)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1274  max mem: 15572
Epoch: [38]  [2460/2809]  eta: 0:03:19  lr: 0.000000  min_lr: 0.000000  loss: 4.1871 (4.1862)  class_acc: 0.3333 (0.3254)  loss_scale: 32768.0000 (37627.9431)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0902  max mem: 15572
Epoch: [38]  [2470/2809]  eta: 0:03:13  lr: 0.000000  min_lr: 0.000000  loss: 4.2012 (4.1860)  class_acc: 0.2917 (0.3253)  loss_scale: 32768.0000 (37608.2752)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1232  max mem: 15572
Epoch: [38]  [2480/2809]  eta: 0:03:07  lr: 0.000000  min_lr: 0.000000  loss: 4.1493 (4.1858)  class_acc: 0.2917 (0.3252)  loss_scale: 32768.0000 (37588.7658)  weight_decay: 0.0500 (0.0500)  time: 0.6573  data: 0.2072  max mem: 15572
Epoch: [38]  [2490/2809]  eta: 0:03:02  lr: 0.000000  min_lr: 0.000000  loss: 4.1399 (4.1856)  class_acc: 0.3333 (0.3254)  loss_scale: 32768.0000 (37569.4131)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.2012  max mem: 15572
Epoch: [38]  [2500/2809]  eta: 0:02:56  lr: 0.000000  min_lr: 0.000000  loss: 4.0494 (4.1850)  class_acc: 0.2917 (0.3253)  loss_scale: 32768.0000 (37550.2151)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0930  max mem: 15572
Epoch: [38]  [2510/2809]  eta: 0:02:50  lr: 0.000000  min_lr: 0.000000  loss: 4.0508 (4.1850)  class_acc: 0.2917 (0.3256)  loss_scale: 32768.0000 (37531.1701)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0673  max mem: 15572
[2025-01-16 08:47:09,733] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:47:09,734] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [2520/2809]  eta: 0:02:45  lr: 0.000000  min_lr: 0.000000  loss: 4.3312 (4.1850)  class_acc: 0.2917 (0.3254)  loss_scale: 32768.0000 (37577.2662)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1720  max mem: 15572
Epoch: [38]  [2530/2809]  eta: 0:02:39  lr: 0.000000  min_lr: 0.000000  loss: 4.1796 (4.1847)  class_acc: 0.3333 (0.3257)  loss_scale: 65536.0000 (37687.7313)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1375  max mem: 15572
Epoch: [38]  [2540/2809]  eta: 0:02:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1858 (4.1852)  class_acc: 0.3750 (0.3258)  loss_scale: 65536.0000 (37797.3270)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1099  max mem: 15572
Epoch: [38]  [2550/2809]  eta: 0:02:27  lr: 0.000000  min_lr: 0.000000  loss: 4.2334 (4.1851)  class_acc: 0.2917 (0.3258)  loss_scale: 65536.0000 (37906.0635)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1515  max mem: 15572
Epoch: [38]  [2560/2809]  eta: 0:02:22  lr: 0.000000  min_lr: 0.000000  loss: 4.1283 (4.1847)  class_acc: 0.3333 (0.3259)  loss_scale: 65536.0000 (38013.9508)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1204  max mem: 15572
Epoch: [38]  [2570/2809]  eta: 0:02:16  lr: 0.000000  min_lr: 0.000000  loss: 4.2018 (4.1852)  class_acc: 0.3333 (0.3257)  loss_scale: 65536.0000 (38120.9988)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1263  max mem: 15572
Epoch: [38]  [2580/2809]  eta: 0:02:10  lr: 0.000000  min_lr: 0.000000  loss: 4.2136 (4.1850)  class_acc: 0.2917 (0.3257)  loss_scale: 65536.0000 (38227.2174)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1582  max mem: 15572
Epoch: [38]  [2590/2809]  eta: 0:02:05  lr: 0.000000  min_lr: 0.000000  loss: 4.3107 (4.1857)  class_acc: 0.2917 (0.3257)  loss_scale: 65536.0000 (38332.6160)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1588  max mem: 15572
Epoch: [38]  [2600/2809]  eta: 0:01:59  lr: 0.000000  min_lr: 0.000000  loss: 4.3811 (4.1856)  class_acc: 0.2917 (0.3256)  loss_scale: 65536.0000 (38437.2042)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1762  max mem: 15572
Epoch: [38]  [2610/2809]  eta: 0:01:53  lr: 0.000000  min_lr: 0.000000  loss: 4.2439 (4.1855)  class_acc: 0.2917 (0.3256)  loss_scale: 65536.0000 (38540.9912)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.1100  max mem: 15572
Epoch: [38]  [2620/2809]  eta: 0:01:47  lr: 0.000000  min_lr: 0.000000  loss: 4.2439 (4.1857)  class_acc: 0.2917 (0.3256)  loss_scale: 65536.0000 (38643.9863)  weight_decay: 0.0500 (0.0500)  time: 0.4302  data: 0.0006  max mem: 15572
Epoch: [38]  [2630/2809]  eta: 0:01:42  lr: 0.000000  min_lr: 0.000000  loss: 4.0912 (4.1853)  class_acc: 0.2917 (0.3255)  loss_scale: 65536.0000 (38746.1984)  weight_decay: 0.0500 (0.0500)  time: 0.4762  data: 0.0232  max mem: 15572
Epoch: [38]  [2640/2809]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 4.1365 (4.1857)  class_acc: 0.2917 (0.3254)  loss_scale: 65536.0000 (38847.6365)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.1863  max mem: 15572
[2025-01-16 08:48:24,292] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:48:24,292] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:48:24,772] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109387
[2025-01-16 08:48:24,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:48:24,773] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [2650/2809]  eta: 0:01:30  lr: 0.000000  min_lr: 0.000000  loss: 4.1694 (4.1859)  class_acc: 0.3333 (0.3253)  loss_scale: 65536.0000 (38973.0306)  weight_decay: 0.0500 (0.0500)  time: 0.7190  data: 0.2628  max mem: 15572
[2025-01-16 08:48:32,969] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109400
[2025-01-16 08:48:32,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:48:32,969] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [2660/2809]  eta: 0:01:25  lr: 0.000000  min_lr: 0.000000  loss: 4.1772 (4.1862)  class_acc: 0.2500 (0.3252)  loss_scale: 65536.0000 (39035.9113)  weight_decay: 0.0500 (0.0500)  time: 0.6709  data: 0.1856  max mem: 15572
Epoch: [38]  [2670/2809]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 4.1947 (4.1862)  class_acc: 0.2917 (0.3251)  loss_scale: 32768.0000 (39012.4448)  weight_decay: 0.0500 (0.0500)  time: 0.7121  data: 0.2216  max mem: 15572
Epoch: [38]  [2680/2809]  eta: 0:01:13  lr: 0.000000  min_lr: 0.000000  loss: 4.1547 (4.1858)  class_acc: 0.3333 (0.3251)  loss_scale: 32768.0000 (38989.1533)  weight_decay: 0.0500 (0.0500)  time: 0.7328  data: 0.2691  max mem: 15572
Epoch: [38]  [2690/2809]  eta: 0:01:08  lr: 0.000000  min_lr: 0.000000  loss: 4.1287 (4.1855)  class_acc: 0.3333 (0.3251)  loss_scale: 32768.0000 (38966.0349)  weight_decay: 0.0500 (0.0500)  time: 0.7590  data: 0.3011  max mem: 15572
Epoch: [38]  [2700/2809]  eta: 0:01:02  lr: 0.000000  min_lr: 0.000000  loss: 4.1687 (4.1855)  class_acc: 0.2917 (0.3250)  loss_scale: 32768.0000 (38943.0877)  weight_decay: 0.0500 (0.0500)  time: 0.7599  data: 0.2904  max mem: 15572
Epoch: [38]  [2710/2809]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 4.1181 (4.1850)  class_acc: 0.2917 (0.3250)  loss_scale: 32768.0000 (38920.3098)  weight_decay: 0.0500 (0.0500)  time: 0.7363  data: 0.2471  max mem: 15572
Epoch: [38]  [2720/2809]  eta: 0:00:51  lr: 0.000000  min_lr: 0.000000  loss: 4.1181 (4.1844)  class_acc: 0.2500 (0.3249)  loss_scale: 32768.0000 (38897.6994)  weight_decay: 0.0500 (0.0500)  time: 0.7726  data: 0.2902  max mem: 15572
Epoch: [38]  [2730/2809]  eta: 0:00:45  lr: 0.000000  min_lr: 0.000000  loss: 4.1978 (4.1849)  class_acc: 0.2500 (0.3247)  loss_scale: 32768.0000 (38875.2545)  weight_decay: 0.0500 (0.0500)  time: 0.7885  data: 0.3094  max mem: 15572
Epoch: [38]  [2740/2809]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 4.2858 (4.1854)  class_acc: 0.2500 (0.3246)  loss_scale: 32768.0000 (38852.9734)  weight_decay: 0.0500 (0.0500)  time: 0.7708  data: 0.2963  max mem: 15572
Epoch: [38]  [2750/2809]  eta: 0:00:34  lr: 0.000000  min_lr: 0.000000  loss: 4.2055 (4.1852)  class_acc: 0.2917 (0.3245)  loss_scale: 32768.0000 (38830.8542)  weight_decay: 0.0500 (0.0500)  time: 0.7099  data: 0.2633  max mem: 15572
Epoch: [38]  [2760/2809]  eta: 0:00:28  lr: 0.000000  min_lr: 0.000000  loss: 4.1030 (4.1850)  class_acc: 0.3333 (0.3248)  loss_scale: 32768.0000 (38808.8953)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.1107  max mem: 15572
Epoch: [38]  [2770/2809]  eta: 0:00:22  lr: 0.000000  min_lr: 0.000000  loss: 4.1440 (4.1847)  class_acc: 0.3333 (0.3249)  loss_scale: 32768.0000 (38787.0949)  weight_decay: 0.0500 (0.0500)  time: 0.4615  data: 0.0256  max mem: 15572
Epoch: [38]  [2780/2809]  eta: 0:00:16  lr: 0.000000  min_lr: 0.000000  loss: 4.2105 (4.1853)  class_acc: 0.2917 (0.3248)  loss_scale: 32768.0000 (38765.4513)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0837  max mem: 15572
[2025-01-16 08:50:00,958] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:50:00,959] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [2790/2809]  eta: 0:00:10  lr: 0.000000  min_lr: 0.000000  loss: 4.2985 (4.1851)  class_acc: 0.2500 (0.3247)  loss_scale: 32768.0000 (38790.9251)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1462  max mem: 15572
[2025-01-16 08:50:06,338] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109540
[2025-01-16 08:50:06,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:50:06,338] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [2800/2809]  eta: 0:00:05  lr: 0.000000  min_lr: 0.000000  loss: 4.1166 (4.1850)  class_acc: 0.2500 (0.3248)  loss_scale: 65536.0000 (38851.3131)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1114  max mem: 15572
Epoch: [38]  [2808/2809]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 4.1795 (4.1850)  class_acc: 0.2917 (0.3246)  loss_scale: 32768.0000 (38833.9879)  weight_decay: 0.0500 (0.0500)  time: 0.4517  data: 0.0237  max mem: 15572
Epoch: [38] Total time: 0:26:59 (0.5767 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 4.1795 (4.1850)  class_acc: 0.2917 (0.3246)  loss_scale: 32768.0000 (38833.9879)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:22:18  loss: 1.1105 (1.1105)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 4.9194  data: 4.5732  max mem: 15572
Val:  [ 10/272]  eta: 0:03:22  loss: 2.4521 (2.4606)  acc1: 50.0000 (47.4747)  acc5: 77.7778 (77.2727)  time: 0.7711  data: 0.5476  max mem: 15572
Val:  [ 20/272]  eta: 0:02:09  loss: 2.5055 (2.5218)  acc1: 50.0000 (47.8836)  acc5: 77.7778 (76.4550)  time: 0.2952  data: 0.1034  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 2.6168 (2.5601)  acc1: 44.4444 (45.3405)  acc5: 72.2222 (75.0896)  time: 0.2540  data: 0.0734  max mem: 15572
Val:  [ 40/272]  eta: 0:01:35  loss: 2.6168 (2.5911)  acc1: 27.7778 (42.1409)  acc5: 72.2222 (74.6612)  time: 0.3060  data: 0.1221  max mem: 15572
Val:  [ 50/272]  eta: 0:01:30  loss: 2.5018 (2.5282)  acc1: 33.3333 (43.7908)  acc5: 77.7778 (76.2527)  time: 0.3630  data: 0.1773  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 1.8890 (2.4655)  acc1: 61.1111 (45.4463)  acc5: 88.8889 (77.0492)  time: 0.3646  data: 0.1712  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 1.9539 (2.4070)  acc1: 61.1111 (47.8091)  acc5: 83.3333 (77.7778)  time: 0.2892  data: 0.0973  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 2.1367 (2.4172)  acc1: 50.0000 (47.5309)  acc5: 77.7778 (77.4348)  time: 0.2779  data: 0.0897  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 2.4720 (2.4208)  acc1: 50.0000 (47.4359)  acc5: 77.7778 (77.7167)  time: 0.3566  data: 0.1642  max mem: 15572
Val:  [100/272]  eta: 0:01:02  loss: 2.4792 (2.4462)  acc1: 44.4444 (46.4796)  acc5: 77.7778 (77.3377)  time: 0.3503  data: 0.1620  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 2.5971 (2.5003)  acc1: 33.3333 (44.8448)  acc5: 77.7778 (76.4264)  time: 0.3129  data: 0.1258  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 2.9124 (2.5368)  acc1: 27.7778 (43.8476)  acc5: 72.2222 (75.7117)  time: 0.3328  data: 0.1501  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 2.4906 (2.5102)  acc1: 38.8889 (44.4869)  acc5: 83.3333 (76.4207)  time: 0.3400  data: 0.1596  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 2.1421 (2.5134)  acc1: 50.0000 (44.7203)  acc5: 83.3333 (76.2805)  time: 0.3137  data: 0.1241  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 2.5496 (2.5116)  acc1: 44.4444 (44.4812)  acc5: 77.7778 (76.3797)  time: 0.2811  data: 0.0939  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 2.4050 (2.5092)  acc1: 44.4444 (44.8585)  acc5: 77.7778 (76.7771)  time: 0.3182  data: 0.1234  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 2.5645 (2.5259)  acc1: 38.8889 (44.4120)  acc5: 72.2222 (76.1533)  time: 0.3222  data: 0.1267  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.5549 (2.5144)  acc1: 38.8889 (44.4444)  acc5: 72.2222 (76.3966)  time: 0.3252  data: 0.1357  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 2.4724 (2.5510)  acc1: 38.8889 (43.3392)  acc5: 77.7778 (75.2472)  time: 0.3755  data: 0.1791  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.7025 (2.5548)  acc1: 33.3333 (42.8966)  acc5: 77.7778 (75.1244)  time: 0.3368  data: 0.1321  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.3250 (2.5613)  acc1: 33.3333 (42.6540)  acc5: 77.7778 (75.0132)  time: 0.3226  data: 0.1156  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 2.6157 (2.5583)  acc1: 33.3333 (42.6596)  acc5: 72.2222 (74.9120)  time: 0.3770  data: 0.1808  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.2221 (2.5437)  acc1: 50.0000 (43.4584)  acc5: 83.3333 (75.1804)  time: 0.3423  data: 0.1535  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 2.1034 (2.5304)  acc1: 61.1111 (43.7529)  acc5: 83.3333 (75.4956)  time: 0.3289  data: 0.1265  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4286 (2.5359)  acc1: 38.8889 (43.3378)  acc5: 77.7778 (75.5423)  time: 0.3538  data: 0.1419  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.9462 (2.4965)  acc1: 61.1111 (44.7637)  acc5: 83.3333 (76.1601)  time: 0.2830  data: 0.0801  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.9462 (2.4911)  acc1: 66.6667 (44.8954)  acc5: 88.8889 (76.4043)  time: 0.2010  data: 0.0287  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.9462 (2.4945)  acc1: 66.6667 (44.8904)  acc5: 88.8889 (76.3875)  time: 0.1950  data: 0.0287  max mem: 15572
Val: Total time: 0:01:31 (0.3353 s / it)
* Acc@1 44.890 Acc@5 76.387 loss 2.494
Accuracy of the network on the 4883 val videos: 44.9%
Max accuracy: 45.18%
Epoch: [39]  [   0/2809]  eta: 7:43:49  lr: 0.000000  min_lr: 0.000000  loss: 4.1549 (4.1549)  class_acc: 0.3750 (0.3750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.9071  data: 9.4887  max mem: 15572
Epoch: [39]  [  10/2809]  eta: 1:07:17  lr: 0.000000  min_lr: 0.000000  loss: 4.3549 (4.2815)  class_acc: 0.3333 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4426  data: 1.0080  max mem: 15572
Epoch: [39]  [  20/2809]  eta: 0:48:18  lr: 0.000000  min_lr: 0.000000  loss: 4.3549 (4.2778)  class_acc: 0.3333 (0.3056)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1408  max mem: 15572
Epoch: [39]  [  30/2809]  eta: 0:41:19  lr: 0.000000  min_lr: 0.000000  loss: 4.0960 (4.2354)  class_acc: 0.3750 (0.3266)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1259  max mem: 15572
Epoch: [39]  [  40/2809]  eta: 0:36:46  lr: 0.000000  min_lr: 0.000000  loss: 4.0644 (4.1774)  class_acc: 0.2917 (0.3232)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0894  max mem: 15572
Epoch: [39]  [  50/2809]  eta: 0:35:13  lr: 0.000000  min_lr: 0.000000  loss: 4.1701 (4.1849)  class_acc: 0.2500 (0.3088)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1248  max mem: 15572
Epoch: [39]  [  60/2809]  eta: 0:33:17  lr: 0.000000  min_lr: 0.000000  loss: 4.2296 (4.1963)  class_acc: 0.2917 (0.3163)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1496  max mem: 15572
Epoch: [39]  [  70/2809]  eta: 0:32:10  lr: 0.000000  min_lr: 0.000000  loss: 4.1947 (4.1979)  class_acc: 0.3750 (0.3222)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1174  max mem: 15572
Epoch: [39]  [  80/2809]  eta: 0:31:34  lr: 0.000000  min_lr: 0.000000  loss: 4.1595 (4.1856)  class_acc: 0.4167 (0.3256)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1564  max mem: 15572
Epoch: [39]  [  90/2809]  eta: 0:30:53  lr: 0.000000  min_lr: 0.000000  loss: 4.0912 (4.1780)  class_acc: 0.3333 (0.3283)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1615  max mem: 15572
Epoch: [39]  [ 100/2809]  eta: 0:30:46  lr: 0.000000  min_lr: 0.000000  loss: 4.1394 (4.1795)  class_acc: 0.3333 (0.3300)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1982  max mem: 15572
Epoch: [39]  [ 110/2809]  eta: 0:30:11  lr: 0.000000  min_lr: 0.000000  loss: 4.2254 (4.1780)  class_acc: 0.3333 (0.3360)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1796  max mem: 15572
[2025-01-16 08:53:00,170] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:53:00,170] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 120/2809]  eta: 0:29:42  lr: 0.000000  min_lr: 0.000000  loss: 4.1530 (4.1732)  class_acc: 0.3333 (0.3351)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1063  max mem: 15572
[2025-01-16 08:53:03,855] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109675
[2025-01-16 08:53:03,856] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:53:03,856] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 130/2809]  eta: 0:29:27  lr: 0.000000  min_lr: 0.000000  loss: 4.0501 (4.1753)  class_acc: 0.2917 (0.3346)  loss_scale: 32768.0000 (34268.8244)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1393  max mem: 15572
Epoch: [39]  [ 140/2809]  eta: 0:28:56  lr: 0.000000  min_lr: 0.000000  loss: 4.2444 (4.1905)  class_acc: 0.2917 (0.3295)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1240  max mem: 15572
Epoch: [39]  [ 150/2809]  eta: 0:28:31  lr: 0.000000  min_lr: 0.000000  loss: 4.1718 (4.1809)  class_acc: 0.3750 (0.3350)  loss_scale: 32768.0000 (34070.0397)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0752  max mem: 15572
Epoch: [39]  [ 160/2809]  eta: 0:28:18  lr: 0.000000  min_lr: 0.000000  loss: 4.1334 (4.1831)  class_acc: 0.2917 (0.3326)  loss_scale: 32768.0000 (33989.1677)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1241  max mem: 15572
Epoch: [39]  [ 170/2809]  eta: 0:27:59  lr: 0.000000  min_lr: 0.000000  loss: 4.0779 (4.1719)  class_acc: 0.2917 (0.3350)  loss_scale: 32768.0000 (33917.7544)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1451  max mem: 15572
Epoch: [39]  [ 180/2809]  eta: 0:27:30  lr: 0.000000  min_lr: 0.000000  loss: 4.1227 (4.1774)  class_acc: 0.3333 (0.3340)  loss_scale: 32768.0000 (33854.2320)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0748  max mem: 15572
Epoch: [39]  [ 190/2809]  eta: 0:27:12  lr: 0.000000  min_lr: 0.000000  loss: 4.2198 (4.1777)  class_acc: 0.3333 (0.3344)  loss_scale: 32768.0000 (33797.3613)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0664  max mem: 15572
Epoch: [39]  [ 200/2809]  eta: 0:27:04  lr: 0.000000  min_lr: 0.000000  loss: 4.1794 (4.1729)  class_acc: 0.3333 (0.3327)  loss_scale: 32768.0000 (33746.1493)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1378  max mem: 15572
Epoch: [39]  [ 210/2809]  eta: 0:26:57  lr: 0.000000  min_lr: 0.000000  loss: 4.0185 (4.1708)  class_acc: 0.3333 (0.3341)  loss_scale: 32768.0000 (33699.7915)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1601  max mem: 15572
Epoch: [39]  [ 220/2809]  eta: 0:26:39  lr: 0.000000  min_lr: 0.000000  loss: 4.0724 (4.1711)  class_acc: 0.3333 (0.3352)  loss_scale: 32768.0000 (33657.6290)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1194  max mem: 15572
Epoch: [39]  [ 230/2809]  eta: 0:26:15  lr: 0.000000  min_lr: 0.000000  loss: 4.1461 (4.1697)  class_acc: 0.2917 (0.3317)  loss_scale: 32768.0000 (33619.1169)  weight_decay: 0.0500 (0.0500)  time: 0.4905  data: 0.0461  max mem: 15572
Epoch: [39]  [ 240/2809]  eta: 0:26:09  lr: 0.000000  min_lr: 0.000000  loss: 4.3242 (4.1791)  class_acc: 0.2500 (0.3281)  loss_scale: 32768.0000 (33583.8008)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0798  max mem: 15572
Epoch: [39]  [ 250/2809]  eta: 0:26:06  lr: 0.000000  min_lr: 0.000000  loss: 4.3242 (4.1802)  class_acc: 0.2917 (0.3270)  loss_scale: 32768.0000 (33551.2988)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1667  max mem: 15572
[2025-01-16 08:54:16,951] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:54:16,951] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 08:54:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109807
[2025-01-16 08:54:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:54:18,313] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 260/2809]  eta: 0:25:57  lr: 0.000000  min_lr: 0.000000  loss: 4.1719 (4.1816)  class_acc: 0.2917 (0.3268)  loss_scale: 32768.0000 (33897.9310)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1566  max mem: 15572
Epoch: [39]  [ 270/2809]  eta: 0:25:43  lr: 0.000000  min_lr: 0.000000  loss: 4.2031 (4.1831)  class_acc: 0.2917 (0.3247)  loss_scale: 32768.0000 (33856.2362)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1098  max mem: 15572
Epoch: [39]  [ 280/2809]  eta: 0:25:40  lr: 0.000000  min_lr: 0.000000  loss: 4.2031 (4.1798)  class_acc: 0.2917 (0.3276)  loss_scale: 32768.0000 (33817.5089)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1340  max mem: 15572
Epoch: [39]  [ 290/2809]  eta: 0:25:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1437 (4.1785)  class_acc: 0.3333 (0.3267)  loss_scale: 32768.0000 (33781.4433)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.1729  max mem: 15572
Epoch: [39]  [ 300/2809]  eta: 0:25:28  lr: 0.000000  min_lr: 0.000000  loss: 4.1437 (4.1781)  class_acc: 0.2917 (0.3259)  loss_scale: 32768.0000 (33747.7741)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1728  max mem: 15572
Epoch: [39]  [ 310/2809]  eta: 0:25:22  lr: 0.000000  min_lr: 0.000000  loss: 4.1502 (4.1789)  class_acc: 0.2917 (0.3256)  loss_scale: 32768.0000 (33716.2701)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1790  max mem: 15572
Epoch: [39]  [ 320/2809]  eta: 0:25:20  lr: 0.000000  min_lr: 0.000000  loss: 4.1502 (4.1802)  class_acc: 0.3333 (0.3290)  loss_scale: 32768.0000 (33686.7290)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1980  max mem: 15572
Epoch: [39]  [ 330/2809]  eta: 0:25:11  lr: 0.000000  min_lr: 0.000000  loss: 4.2605 (4.1833)  class_acc: 0.2917 (0.3269)  loss_scale: 32768.0000 (33658.9728)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1756  max mem: 15572
Epoch: [39]  [ 340/2809]  eta: 0:25:01  lr: 0.000000  min_lr: 0.000000  loss: 4.2605 (4.1839)  class_acc: 0.2500 (0.3267)  loss_scale: 32768.0000 (33632.8446)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1158  max mem: 15572
Epoch: [39]  [ 350/2809]  eta: 0:24:55  lr: 0.000000  min_lr: 0.000000  loss: 4.2301 (4.1854)  class_acc: 0.2917 (0.3268)  loss_scale: 32768.0000 (33608.2051)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1301  max mem: 15572
Epoch: [39]  [ 360/2809]  eta: 0:24:48  lr: 0.000000  min_lr: 0.000000  loss: 4.1331 (4.1834)  class_acc: 0.2917 (0.3262)  loss_scale: 32768.0000 (33584.9307)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1530  max mem: 15572
Epoch: [39]  [ 370/2809]  eta: 0:24:40  lr: 0.000000  min_lr: 0.000000  loss: 4.0746 (4.1791)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (33562.9111)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1418  max mem: 15572
Epoch: [39]  [ 380/2809]  eta: 0:24:31  lr: 0.000000  min_lr: 0.000000  loss: 4.1722 (4.1820)  class_acc: 0.3333 (0.3261)  loss_scale: 32768.0000 (33542.0472)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1176  max mem: 15572
[2025-01-16 08:55:35,186] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:55:35,186] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 390/2809]  eta: 0:24:25  lr: 0.000000  min_lr: 0.000000  loss: 4.3773 (4.1870)  class_acc: 0.3333 (0.3263)  loss_scale: 32768.0000 (34025.0844)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1371  max mem: 15572
Epoch: [39]  [ 400/2809]  eta: 0:24:21  lr: 0.000000  min_lr: 0.000000  loss: 4.2671 (4.1897)  class_acc: 0.2917 (0.3240)  loss_scale: 65536.0000 (34810.8928)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.1858  max mem: 15572
Epoch: [39]  [ 410/2809]  eta: 0:24:11  lr: 0.000000  min_lr: 0.000000  loss: 4.2440 (4.1910)  class_acc: 0.2917 (0.3244)  loss_scale: 65536.0000 (35558.4623)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1478  max mem: 15572
[2025-01-16 08:55:57,752] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109971
[2025-01-16 08:55:57,752] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:55:57,752] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 420/2809]  eta: 0:24:11  lr: 0.000000  min_lr: 0.000000  loss: 4.2895 (4.1929)  class_acc: 0.3333 (0.3243)  loss_scale: 65536.0000 (36192.6841)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1795  max mem: 15572
Epoch: [39]  [ 430/2809]  eta: 0:24:06  lr: 0.000000  min_lr: 0.000000  loss: 4.2131 (4.1935)  class_acc: 0.2917 (0.3237)  loss_scale: 32768.0000 (36113.2251)  weight_decay: 0.0500 (0.0500)  time: 0.6661  data: 0.2186  max mem: 15572
Epoch: [39]  [ 440/2809]  eta: 0:23:59  lr: 0.000000  min_lr: 0.000000  loss: 4.0939 (4.1925)  class_acc: 0.2917 (0.3237)  loss_scale: 32768.0000 (36037.3696)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1548  max mem: 15572
[2025-01-16 08:56:14,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=110000, skipped=688, lr=[1.0994758261983567e-09, 1.0994758261983567e-09, 1.5706797517119385e-09, 1.5706797517119385e-09, 2.243828216731341e-09, 2.243828216731341e-09, 3.205468881044773e-09, 3.205468881044773e-09, 4.57924125863539e-09, 4.57924125863539e-09, 6.541773226621986e-09, 6.541773226621986e-09, 9.345390323745694e-09, 9.345390323745694e-09, 1.3350557605350994e-08, 1.3350557605350994e-08, 1.907222515050142e-08, 1.907222515050142e-08, 2.7246035929287747e-08, 2.7246035929287747e-08, 3.8922908470411064e-08, 3.8922908470411064e-08, 5.56041549577301e-08, 5.56041549577301e-08, 7.943450708247158e-08, 7.943450708247158e-08, 1.1347786726067369e-07, 1.1347786726067369e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:56:14,892] [INFO] [timer.py:260:stop] epoch=0/micro_step=110000/global_step=110000, RunningAvgSamplesPerSec=27.99942876375291, CurrSamplesPerSec=30.27330977549885, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [39]  [ 450/2809]  eta: 0:23:52  lr: 0.000000  min_lr: 0.000000  loss: 4.2981 (4.1973)  class_acc: 0.2917 (0.3237)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1228  max mem: 15572
Epoch: [39]  [ 460/2809]  eta: 0:23:45  lr: 0.000000  min_lr: 0.000000  loss: 4.2062 (4.1936)  class_acc: 0.3333 (0.3247)  loss_scale: 32768.0000 (35895.5315)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1236  max mem: 15572
Epoch: [39]  [ 470/2809]  eta: 0:23:40  lr: 0.000000  min_lr: 0.000000  loss: 4.0554 (4.1930)  class_acc: 0.3333 (0.3252)  loss_scale: 32768.0000 (35829.1295)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1620  max mem: 15572
Epoch: [39]  [ 480/2809]  eta: 0:23:32  lr: 0.000000  min_lr: 0.000000  loss: 4.1313 (4.1929)  class_acc: 0.3750 (0.3272)  loss_scale: 32768.0000 (35765.4886)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1673  max mem: 15572
Epoch: [39]  [ 490/2809]  eta: 0:23:24  lr: 0.000000  min_lr: 0.000000  loss: 4.2115 (4.1941)  class_acc: 0.3333 (0.3261)  loss_scale: 32768.0000 (35704.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1258  max mem: 15572
Epoch: [39]  [ 500/2809]  eta: 0:23:19  lr: 0.000000  min_lr: 0.000000  loss: 4.3928 (4.1971)  class_acc: 0.2917 (0.3256)  loss_scale: 32768.0000 (35645.8283)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1299  max mem: 15572
Epoch: [39]  [ 510/2809]  eta: 0:23:13  lr: 0.000000  min_lr: 0.000000  loss: 4.2141 (4.1966)  class_acc: 0.2917 (0.3259)  loss_scale: 32768.0000 (35589.5108)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1467  max mem: 15572
Epoch: [39]  [ 520/2809]  eta: 0:23:05  lr: 0.000000  min_lr: 0.000000  loss: 4.3753 (4.1993)  class_acc: 0.2917 (0.3254)  loss_scale: 32768.0000 (35535.3551)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1245  max mem: 15572
Epoch: [39]  [ 530/2809]  eta: 0:22:54  lr: 0.000000  min_lr: 0.000000  loss: 4.3484 (4.1996)  class_acc: 0.2500 (0.3234)  loss_scale: 32768.0000 (35483.2392)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0614  max mem: 15572
Epoch: [39]  [ 540/2809]  eta: 0:22:47  lr: 0.000000  min_lr: 0.000000  loss: 4.2211 (4.1995)  class_acc: 0.2500 (0.3233)  loss_scale: 32768.0000 (35433.0499)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0867  max mem: 15572
[2025-01-16 08:57:13,191] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:57:13,191] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 550/2809]  eta: 0:22:39  lr: 0.000000  min_lr: 0.000000  loss: 4.1724 (4.1974)  class_acc: 0.3333 (0.3237)  loss_scale: 32768.0000 (35503.6225)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1106  max mem: 15572
Epoch: [39]  [ 560/2809]  eta: 0:22:31  lr: 0.000000  min_lr: 0.000000  loss: 4.1268 (4.1975)  class_acc: 0.3333 (0.3238)  loss_scale: 65536.0000 (36038.9590)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0933  max mem: 15572
[2025-01-16 08:57:25,596] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110121
[2025-01-16 08:57:25,596] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:57:25,596] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 570/2809]  eta: 0:22:27  lr: 0.000000  min_lr: 0.000000  loss: 4.1855 (4.1961)  class_acc: 0.3750 (0.3246)  loss_scale: 65536.0000 (36498.1576)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1551  max mem: 15572
Epoch: [39]  [ 580/2809]  eta: 0:22:23  lr: 0.000000  min_lr: 0.000000  loss: 4.3332 (4.1994)  class_acc: 0.3333 (0.3236)  loss_scale: 32768.0000 (36433.9552)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.2050  max mem: 15572
Epoch: [39]  [ 590/2809]  eta: 0:22:15  lr: 0.000000  min_lr: 0.000000  loss: 4.3909 (4.1997)  class_acc: 0.2917 (0.3245)  loss_scale: 32768.0000 (36371.9255)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1653  max mem: 15572
Epoch: [39]  [ 600/2809]  eta: 0:22:08  lr: 0.000000  min_lr: 0.000000  loss: 4.3089 (4.2002)  class_acc: 0.2917 (0.3241)  loss_scale: 32768.0000 (36311.9601)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1183  max mem: 15572
Epoch: [39]  [ 610/2809]  eta: 0:21:58  lr: 0.000000  min_lr: 0.000000  loss: 4.1603 (4.1986)  class_acc: 0.3750 (0.3259)  loss_scale: 32768.0000 (36253.9574)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0848  max mem: 15572
Epoch: [39]  [ 620/2809]  eta: 0:21:51  lr: 0.000000  min_lr: 0.000000  loss: 4.1036 (4.1978)  class_acc: 0.3750 (0.3258)  loss_scale: 32768.0000 (36197.8229)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0656  max mem: 15572
Epoch: [39]  [ 630/2809]  eta: 0:21:45  lr: 0.000000  min_lr: 0.000000  loss: 4.2003 (4.1985)  class_acc: 0.2500 (0.3253)  loss_scale: 32768.0000 (36143.4675)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1289  max mem: 15572
Epoch: [39]  [ 640/2809]  eta: 0:21:39  lr: 0.000000  min_lr: 0.000000  loss: 4.1614 (4.1955)  class_acc: 0.2500 (0.3251)  loss_scale: 32768.0000 (36090.8081)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1688  max mem: 15572
Epoch: [39]  [ 650/2809]  eta: 0:21:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1614 (4.1958)  class_acc: 0.3333 (0.3252)  loss_scale: 32768.0000 (36039.7665)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1627  max mem: 15572
Epoch: [39]  [ 660/2809]  eta: 0:21:28  lr: 0.000000  min_lr: 0.000000  loss: 4.2168 (4.1974)  class_acc: 0.3333 (0.3245)  loss_scale: 32768.0000 (35990.2693)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1687  max mem: 15572
Epoch: [39]  [ 670/2809]  eta: 0:21:25  lr: 0.000000  min_lr: 0.000000  loss: 4.2238 (4.1965)  class_acc: 0.2917 (0.3245)  loss_scale: 32768.0000 (35942.2474)  weight_decay: 0.0500 (0.0500)  time: 0.6580  data: 0.1926  max mem: 15572
Epoch: [39]  [ 680/2809]  eta: 0:21:16  lr: 0.000000  min_lr: 0.000000  loss: 4.1837 (4.1967)  class_acc: 0.2917 (0.3238)  loss_scale: 32768.0000 (35895.6358)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1370  max mem: 15572
Epoch: [39]  [ 690/2809]  eta: 0:21:09  lr: 0.000000  min_lr: 0.000000  loss: 4.1837 (4.1965)  class_acc: 0.2917 (0.3242)  loss_scale: 32768.0000 (35850.3734)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.0897  max mem: 15572
[2025-01-16 08:58:41,111] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:58:41,111] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 700/2809]  eta: 0:21:01  lr: 0.000000  min_lr: 0.000000  loss: 4.1319 (4.1962)  class_acc: 0.2917 (0.3231)  loss_scale: 32768.0000 (35899.8916)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.1175  max mem: 15572
Epoch: [39]  [ 710/2809]  eta: 0:20:55  lr: 0.000000  min_lr: 0.000000  loss: 4.0953 (4.1942)  class_acc: 0.2500 (0.3233)  loss_scale: 65536.0000 (36316.7145)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1195  max mem: 15572
[2025-01-16 08:58:48,802] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110264
[2025-01-16 08:58:48,803] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:58:48,803] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 720/2809]  eta: 0:20:45  lr: 0.000000  min_lr: 0.000000  loss: 4.1480 (4.1936)  class_acc: 0.3333 (0.3239)  loss_scale: 65536.0000 (36358.3911)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0660  max mem: 15572
Epoch: [39]  [ 730/2809]  eta: 0:20:37  lr: 0.000000  min_lr: 0.000000  loss: 4.2553 (4.1941)  class_acc: 0.2917 (0.3232)  loss_scale: 32768.0000 (36309.2750)  weight_decay: 0.0500 (0.0500)  time: 0.4894  data: 0.0343  max mem: 15572
Epoch: [39]  [ 740/2809]  eta: 0:20:31  lr: 0.000000  min_lr: 0.000000  loss: 4.2140 (4.1935)  class_acc: 0.3333 (0.3244)  loss_scale: 32768.0000 (36261.4845)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.1014  max mem: 15572
Epoch: [39]  [ 750/2809]  eta: 0:20:25  lr: 0.000000  min_lr: 0.000000  loss: 4.2140 (4.1943)  class_acc: 0.3333 (0.3235)  loss_scale: 32768.0000 (36214.9667)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1539  max mem: 15572
Epoch: [39]  [ 760/2809]  eta: 0:20:19  lr: 0.000000  min_lr: 0.000000  loss: 4.2781 (4.1942)  class_acc: 0.2500 (0.3239)  loss_scale: 32768.0000 (36169.6715)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1576  max mem: 15572
Epoch: [39]  [ 770/2809]  eta: 0:20:14  lr: 0.000000  min_lr: 0.000000  loss: 4.1992 (4.1962)  class_acc: 0.2917 (0.3238)  loss_scale: 32768.0000 (36125.5512)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1515  max mem: 15572
Epoch: [39]  [ 780/2809]  eta: 0:20:07  lr: 0.000000  min_lr: 0.000000  loss: 4.1211 (4.1947)  class_acc: 0.3750 (0.3255)  loss_scale: 32768.0000 (36082.5608)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1198  max mem: 15572
Epoch: [39]  [ 790/2809]  eta: 0:19:58  lr: 0.000000  min_lr: 0.000000  loss: 4.1255 (4.1959)  class_acc: 0.3333 (0.3251)  loss_scale: 32768.0000 (36040.6574)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0822  max mem: 15572
Epoch: [39]  [ 800/2809]  eta: 0:19:52  lr: 0.000000  min_lr: 0.000000  loss: 4.1629 (4.1960)  class_acc: 0.2500 (0.3250)  loss_scale: 32768.0000 (35999.8002)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1113  max mem: 15572
Epoch: [39]  [ 810/2809]  eta: 0:19:47  lr: 0.000000  min_lr: 0.000000  loss: 4.2588 (4.1982)  class_acc: 0.2500 (0.3239)  loss_scale: 32768.0000 (35959.9507)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1780  max mem: 15572
Epoch: [39]  [ 820/2809]  eta: 0:19:41  lr: 0.000000  min_lr: 0.000000  loss: 4.3071 (4.1978)  class_acc: 0.2500 (0.3240)  loss_scale: 32768.0000 (35921.0719)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1855  max mem: 15572
Epoch: [39]  [ 830/2809]  eta: 0:19:36  lr: 0.000000  min_lr: 0.000000  loss: 4.2104 (4.1974)  class_acc: 0.2917 (0.3238)  loss_scale: 32768.0000 (35883.1288)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1799  max mem: 15572
Epoch: [39]  [ 840/2809]  eta: 0:19:30  lr: 0.000000  min_lr: 0.000000  loss: 4.2104 (4.1972)  class_acc: 0.2917 (0.3236)  loss_scale: 32768.0000 (35846.0880)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1533  max mem: 15572
[2025-01-16 09:00:02,976] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:00:02,976] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 850/2809]  eta: 0:19:24  lr: 0.000000  min_lr: 0.000000  loss: 4.2281 (4.1970)  class_acc: 0.3750 (0.3244)  loss_scale: 32768.0000 (36156.4653)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1319  max mem: 15572
Epoch: [39]  [ 860/2809]  eta: 0:19:20  lr: 0.000000  min_lr: 0.000000  loss: 4.0510 (4.1960)  class_acc: 0.4167 (0.3255)  loss_scale: 65536.0000 (36497.6911)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.2023  max mem: 15572
[2025-01-16 09:00:19,997] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110419
[2025-01-16 09:00:19,997] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:00:19,998] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 870/2809]  eta: 0:19:14  lr: 0.000000  min_lr: 0.000000  loss: 4.1428 (4.1962)  class_acc: 0.3750 (0.3258)  loss_scale: 65536.0000 (36718.2181)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.2019  max mem: 15572
Epoch: [39]  [ 880/2809]  eta: 0:19:06  lr: 0.000000  min_lr: 0.000000  loss: 4.1428 (4.1961)  class_acc: 0.3333 (0.3263)  loss_scale: 32768.0000 (36673.3802)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1071  max mem: 15572
Epoch: [39]  [ 890/2809]  eta: 0:19:00  lr: 0.000000  min_lr: 0.000000  loss: 4.2201 (4.1969)  class_acc: 0.3333 (0.3263)  loss_scale: 32768.0000 (36629.5488)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0861  max mem: 15572
Epoch: [39]  [ 900/2809]  eta: 0:18:54  lr: 0.000000  min_lr: 0.000000  loss: 4.3021 (4.1970)  class_acc: 0.3333 (0.3262)  loss_scale: 32768.0000 (36586.6903)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1401  max mem: 15572
Epoch: [39]  [ 910/2809]  eta: 0:18:48  lr: 0.000000  min_lr: 0.000000  loss: 4.2410 (4.1984)  class_acc: 0.2917 (0.3251)  loss_scale: 32768.0000 (36544.7728)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1642  max mem: 15572
Epoch: [39]  [ 920/2809]  eta: 0:18:42  lr: 0.000000  min_lr: 0.000000  loss: 4.2398 (4.1976)  class_acc: 0.2917 (0.3260)  loss_scale: 32768.0000 (36503.7655)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1274  max mem: 15572
Epoch: [39]  [ 930/2809]  eta: 0:18:36  lr: 0.000000  min_lr: 0.000000  loss: 4.1918 (4.1983)  class_acc: 0.3750 (0.3264)  loss_scale: 32768.0000 (36463.6391)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1195  max mem: 15572
Epoch: [39]  [ 940/2809]  eta: 0:18:30  lr: 0.000000  min_lr: 0.000000  loss: 4.2330 (4.1984)  class_acc: 0.3750 (0.3268)  loss_scale: 32768.0000 (36424.3656)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1269  max mem: 15572
Epoch: [39]  [ 950/2809]  eta: 0:18:24  lr: 0.000000  min_lr: 0.000000  loss: 4.2330 (4.1996)  class_acc: 0.2917 (0.3265)  loss_scale: 32768.0000 (36385.9180)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1189  max mem: 15572
Epoch: [39]  [ 960/2809]  eta: 0:18:18  lr: 0.000000  min_lr: 0.000000  loss: 4.3214 (4.2009)  class_acc: 0.2500 (0.3258)  loss_scale: 32768.0000 (36348.2706)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1478  max mem: 15572
Epoch: [39]  [ 970/2809]  eta: 0:18:12  lr: 0.000000  min_lr: 0.000000  loss: 4.2858 (4.2018)  class_acc: 0.3333 (0.3257)  loss_scale: 32768.0000 (36311.3986)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1563  max mem: 15572
Epoch: [39]  [ 980/2809]  eta: 0:18:05  lr: 0.000000  min_lr: 0.000000  loss: 4.3029 (4.2036)  class_acc: 0.2917 (0.3254)  loss_scale: 32768.0000 (36275.2783)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1229  max mem: 15572
Epoch: [39]  [ 990/2809]  eta: 0:17:59  lr: 0.000000  min_lr: 0.000000  loss: 4.3413 (4.2041)  class_acc: 0.2917 (0.3253)  loss_scale: 32768.0000 (36239.8870)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1280  max mem: 15572
[2025-01-16 09:01:35,579] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:01:35,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [1000/2809]  eta: 0:17:54  lr: 0.000000  min_lr: 0.000000  loss: 4.2713 (4.2042)  class_acc: 0.2917 (0.3252)  loss_scale: 32768.0000 (36336.1439)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1575  max mem: 15572
[2025-01-16 09:01:41,303] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110559
[2025-01-16 09:01:41,304] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:01:41,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [1010/2809]  eta: 0:17:47  lr: 0.000000  min_lr: 0.000000  loss: 4.2218 (4.2058)  class_acc: 0.2917 (0.3249)  loss_scale: 65536.0000 (36527.7310)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1409  max mem: 15572
Epoch: [39]  [1020/2809]  eta: 0:17:41  lr: 0.000000  min_lr: 0.000000  loss: 4.1539 (4.2034)  class_acc: 0.3333 (0.3256)  loss_scale: 32768.0000 (36490.9070)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1293  max mem: 15572
Epoch: [39]  [1030/2809]  eta: 0:17:36  lr: 0.000000  min_lr: 0.000000  loss: 4.1745 (4.2046)  class_acc: 0.3333 (0.3255)  loss_scale: 32768.0000 (36454.7973)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1626  max mem: 15572
Epoch: [39]  [1040/2809]  eta: 0:17:30  lr: 0.000000  min_lr: 0.000000  loss: 4.1291 (4.2035)  class_acc: 0.3333 (0.3259)  loss_scale: 32768.0000 (36419.3814)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1477  max mem: 15572
Epoch: [39]  [1050/2809]  eta: 0:17:25  lr: 0.000000  min_lr: 0.000000  loss: 4.0141 (4.2026)  class_acc: 0.3333 (0.3260)  loss_scale: 32768.0000 (36384.6394)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1359  max mem: 15572
Epoch: [39]  [1060/2809]  eta: 0:17:19  lr: 0.000000  min_lr: 0.000000  loss: 4.0013 (4.2018)  class_acc: 0.3333 (0.3264)  loss_scale: 32768.0000 (36350.5523)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1514  max mem: 15572
Epoch: [39]  [1070/2809]  eta: 0:17:12  lr: 0.000000  min_lr: 0.000000  loss: 4.1825 (4.2014)  class_acc: 0.3333 (0.3263)  loss_scale: 32768.0000 (36317.1018)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1311  max mem: 15572
Epoch: [39]  [1080/2809]  eta: 0:17:05  lr: 0.000000  min_lr: 0.000000  loss: 4.2005 (4.2019)  class_acc: 0.2917 (0.3261)  loss_scale: 32768.0000 (36284.2701)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0909  max mem: 15572
Epoch: [39]  [1090/2809]  eta: 0:17:00  lr: 0.000000  min_lr: 0.000000  loss: 4.2419 (4.2021)  class_acc: 0.3333 (0.3265)  loss_scale: 32768.0000 (36252.0403)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1277  max mem: 15572
Epoch: [39]  [1100/2809]  eta: 0:16:52  lr: 0.000000  min_lr: 0.000000  loss: 4.1167 (4.2003)  class_acc: 0.3750 (0.3270)  loss_scale: 32768.0000 (36220.3960)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1242  max mem: 15572
Epoch: [39]  [1110/2809]  eta: 0:16:48  lr: 0.000000  min_lr: 0.000000  loss: 4.1069 (4.2004)  class_acc: 0.3333 (0.3266)  loss_scale: 32768.0000 (36189.3213)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1493  max mem: 15572
Epoch: [39]  [1120/2809]  eta: 0:16:41  lr: 0.000000  min_lr: 0.000000  loss: 4.2740 (4.2017)  class_acc: 0.2500 (0.3262)  loss_scale: 32768.0000 (36158.8011)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1516  max mem: 15572
Epoch: [39]  [1130/2809]  eta: 0:16:35  lr: 0.000000  min_lr: 0.000000  loss: 4.3552 (4.2027)  class_acc: 0.2500 (0.3257)  loss_scale: 32768.0000 (36128.8205)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0930  max mem: 15572
[2025-01-16 09:02:56,395] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:02:56,395] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [1140/2809]  eta: 0:16:27  lr: 0.000000  min_lr: 0.000000  loss: 4.2169 (4.2023)  class_acc: 0.2500 (0.3256)  loss_scale: 32768.0000 (36214.2401)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.0899  max mem: 15572
Epoch: [39]  [1150/2809]  eta: 0:16:21  lr: 0.000000  min_lr: 0.000000  loss: 4.2169 (4.2020)  class_acc: 0.3750 (0.3265)  loss_scale: 65536.0000 (36468.9904)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0917  max mem: 15572
[2025-01-16 09:03:05,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110706
[2025-01-16 09:03:05,689] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:03:05,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [1160/2809]  eta: 0:16:14  lr: 0.000000  min_lr: 0.000000  loss: 4.0610 (4.2010)  class_acc: 0.3750 (0.3266)  loss_scale: 65536.0000 (36550.0086)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1063  max mem: 15572
Epoch: [39]  [1170/2809]  eta: 0:16:10  lr: 0.000000  min_lr: 0.000000  loss: 3.9962 (4.2008)  class_acc: 0.2917 (0.3266)  loss_scale: 32768.0000 (36517.7114)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1584  max mem: 15572
Epoch: [39]  [1180/2809]  eta: 0:16:04  lr: 0.000000  min_lr: 0.000000  loss: 4.2384 (4.2001)  class_acc: 0.3333 (0.3269)  loss_scale: 32768.0000 (36485.9610)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1791  max mem: 15572
Epoch: [39]  [1190/2809]  eta: 0:15:58  lr: 0.000000  min_lr: 0.000000  loss: 4.1661 (4.2000)  class_acc: 0.3333 (0.3269)  loss_scale: 32768.0000 (36454.7439)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1656  max mem: 15572
Epoch: [39]  [1200/2809]  eta: 0:15:53  lr: 0.000000  min_lr: 0.000000  loss: 4.1086 (4.1994)  class_acc: 0.2917 (0.3270)  loss_scale: 32768.0000 (36424.0466)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.2096  max mem: 15572
Epoch: [39]  [1210/2809]  eta: 0:15:47  lr: 0.000000  min_lr: 0.000000  loss: 4.3179 (4.2004)  class_acc: 0.2500 (0.3268)  loss_scale: 32768.0000 (36393.8563)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1777  max mem: 15572
Epoch: [39]  [1220/2809]  eta: 0:15:42  lr: 0.000000  min_lr: 0.000000  loss: 4.3322 (4.2012)  class_acc: 0.2917 (0.3272)  loss_scale: 32768.0000 (36364.1605)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1703  max mem: 15572
Epoch: [39]  [1230/2809]  eta: 0:15:35  lr: 0.000000  min_lr: 0.000000  loss: 4.2320 (4.2007)  class_acc: 0.3333 (0.3273)  loss_scale: 32768.0000 (36334.9472)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1582  max mem: 15572
Epoch: [39]  [1240/2809]  eta: 0:15:29  lr: 0.000000  min_lr: 0.000000  loss: 4.1899 (4.2005)  class_acc: 0.3333 (0.3272)  loss_scale: 32768.0000 (36306.2047)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1086  max mem: 15572
Epoch: [39]  [1250/2809]  eta: 0:15:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1899 (4.2000)  class_acc: 0.3750 (0.3279)  loss_scale: 32768.0000 (36277.9217)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1698  max mem: 15572
Epoch: [39]  [1260/2809]  eta: 0:15:19  lr: 0.000000  min_lr: 0.000000  loss: 4.1518 (4.1998)  class_acc: 0.3333 (0.3274)  loss_scale: 32768.0000 (36250.0872)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.2066  max mem: 15572
Epoch: [39]  [1270/2809]  eta: 0:15:14  lr: 0.000000  min_lr: 0.000000  loss: 4.1252 (4.1996)  class_acc: 0.2500 (0.3276)  loss_scale: 32768.0000 (36222.6908)  weight_decay: 0.0500 (0.0500)  time: 0.6509  data: 0.1928  max mem: 15572
Epoch: [39]  [1280/2809]  eta: 0:15:06  lr: 0.000000  min_lr: 0.000000  loss: 4.2205 (4.1995)  class_acc: 0.3750 (0.3277)  loss_scale: 32768.0000 (36195.7221)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1195  max mem: 15572
[2025-01-16 09:04:24,038] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:04:24,039] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [1290/2809]  eta: 0:15:01  lr: 0.000000  min_lr: 0.000000  loss: 4.1416 (4.1988)  class_acc: 0.3333 (0.3275)  loss_scale: 32768.0000 (36346.8443)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0914  max mem: 15572
Epoch: [39]  [1300/2809]  eta: 0:14:55  lr: 0.000000  min_lr: 0.000000  loss: 4.1836 (4.1996)  class_acc: 0.2917 (0.3272)  loss_scale: 65536.0000 (36571.2037)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1655  max mem: 15572
Epoch: [39]  [1310/2809]  eta: 0:14:50  lr: 0.000000  min_lr: 0.000000  loss: 4.1836 (4.1993)  class_acc: 0.2500 (0.3268)  loss_scale: 65536.0000 (36792.1404)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.2006  max mem: 15572
Epoch: [39]  [1320/2809]  eta: 0:14:45  lr: 0.000000  min_lr: 0.000000  loss: 4.2141 (4.1998)  class_acc: 0.2500 (0.3266)  loss_scale: 65536.0000 (37009.7320)  weight_decay: 0.0500 (0.0500)  time: 0.6718  data: 0.2281  max mem: 15572
[2025-01-16 09:04:52,317] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110877
[2025-01-16 09:04:52,317] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:04:52,317] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [1330/2809]  eta: 0:14:39  lr: 0.000000  min_lr: 0.000000  loss: 4.2230 (4.2005)  class_acc: 0.2917 (0.3269)  loss_scale: 65536.0000 (37100.9587)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.1997  max mem: 15572
Epoch: [39]  [1340/2809]  eta: 0:14:32  lr: 0.000000  min_lr: 0.000000  loss: 4.1986 (4.1998)  class_acc: 0.3750 (0.3273)  loss_scale: 32768.0000 (37068.6473)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1110  max mem: 15572
Epoch: [39]  [1350/2809]  eta: 0:14:27  lr: 0.000000  min_lr: 0.000000  loss: 4.1303 (4.1995)  class_acc: 0.3333 (0.3274)  loss_scale: 32768.0000 (37036.8142)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1223  max mem: 15572
Epoch: [39]  [1360/2809]  eta: 0:14:21  lr: 0.000000  min_lr: 0.000000  loss: 4.1440 (4.1994)  class_acc: 0.3333 (0.3273)  loss_scale: 32768.0000 (37005.4489)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1701  max mem: 15572
Epoch: [39]  [1370/2809]  eta: 0:14:14  lr: 0.000000  min_lr: 0.000000  loss: 4.1440 (4.1985)  class_acc: 0.2917 (0.3273)  loss_scale: 32768.0000 (36974.5412)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0824  max mem: 15572
Epoch: [39]  [1380/2809]  eta: 0:14:09  lr: 0.000000  min_lr: 0.000000  loss: 4.1416 (4.1980)  class_acc: 0.2917 (0.3272)  loss_scale: 32768.0000 (36944.0811)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0870  max mem: 15572
Epoch: [39]  [1390/2809]  eta: 0:14:03  lr: 0.000000  min_lr: 0.000000  loss: 4.1817 (4.1979)  class_acc: 0.3333 (0.3273)  loss_scale: 32768.0000 (36914.0590)  weight_decay: 0.0500 (0.0500)  time: 0.6567  data: 0.1750  max mem: 15572
Epoch: [39]  [1400/2809]  eta: 0:13:57  lr: 0.000000  min_lr: 0.000000  loss: 4.1682 (4.1974)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (36884.4654)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1239  max mem: 15572
Epoch: [39]  [1410/2809]  eta: 0:13:50  lr: 0.000000  min_lr: 0.000000  loss: 4.1351 (4.1969)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (36855.2913)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0740  max mem: 15572
Epoch: [39]  [1420/2809]  eta: 0:13:45  lr: 0.000000  min_lr: 0.000000  loss: 4.1713 (4.1969)  class_acc: 0.3333 (0.3271)  loss_scale: 32768.0000 (36826.5278)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1622  max mem: 15572
Epoch: [39]  [1430/2809]  eta: 0:13:38  lr: 0.000000  min_lr: 0.000000  loss: 4.2400 (4.1973)  class_acc: 0.2917 (0.3268)  loss_scale: 32768.0000 (36798.1663)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1679  max mem: 15572
Epoch: [39]  [1440/2809]  eta: 0:13:33  lr: 0.000000  min_lr: 0.000000  loss: 4.2614 (4.1976)  class_acc: 0.2500 (0.3268)  loss_scale: 32768.0000 (36770.1985)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1227  max mem: 15572
[2025-01-16 09:06:03,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=111000, skipped=694, lr=[6.686682132076383e-10, 6.686682132076383e-10, 9.552403045823405e-10, 9.552403045823405e-10, 1.3646290065462008e-09, 1.3646290065462008e-09, 1.9494700093517155e-09, 1.9494700093517155e-09, 2.784957156216737e-09, 2.784957156216737e-09, 3.978510223166767e-09, 3.978510223166767e-09, 5.6835860330953815e-09, 5.6835860330953815e-09, 8.11940861870769e-09, 8.11940861870769e-09, 1.1599155169582413e-08, 1.1599155169582413e-08, 1.6570221670832022e-08, 1.6570221670832022e-08, 2.3671745244045743e-08, 2.3671745244045743e-08, 3.381677892006535e-08, 3.381677892006535e-08, 4.8309684171521934e-08, 4.8309684171521934e-08, 6.901383453074562e-08, 6.901383453074562e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 09:06:03,828] [INFO] [timer.py:260:stop] epoch=0/micro_step=111000/global_step=111000, RunningAvgSamplesPerSec=28.001480427777594, CurrSamplesPerSec=30.115322411276992, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [39]  [1450/2809]  eta: 0:13:27  lr: 0.000000  min_lr: 0.000000  loss: 4.2865 (4.1974)  class_acc: 0.2917 (0.3270)  loss_scale: 32768.0000 (36742.6161)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1577  max mem: 15572
[2025-01-16 09:06:07,233] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:06:07,234] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [1460/2809]  eta: 0:13:20  lr: 0.000000  min_lr: 0.000000  loss: 4.1931 (4.1973)  class_acc: 0.3333 (0.3270)  loss_scale: 32768.0000 (36849.9822)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0964  max mem: 15572
Epoch: [39]  [1470/2809]  eta: 0:13:14  lr: 0.000000  min_lr: 0.000000  loss: 4.1931 (4.1973)  class_acc: 0.2917 (0.3266)  loss_scale: 65536.0000 (37044.9925)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0664  max mem: 15572
[2025-01-16 09:06:16,974] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111026
[2025-01-16 09:06:16,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:06:16,975] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [1480/2809]  eta: 0:13:08  lr: 0.000000  min_lr: 0.000000  loss: 4.1826 (4.1964)  class_acc: 0.2917 (0.3266)  loss_scale: 65536.0000 (37104.6158)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1305  max mem: 15572
Epoch: [39]  [1490/2809]  eta: 0:13:02  lr: 0.000000  min_lr: 0.000000  loss: 4.2284 (4.1969)  class_acc: 0.2917 (0.3266)  loss_scale: 32768.0000 (37075.5305)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1458  max mem: 15572
Epoch: [39]  [1500/2809]  eta: 0:12:56  lr: 0.000000  min_lr: 0.000000  loss: 4.2194 (4.1973)  class_acc: 0.2500 (0.3263)  loss_scale: 32768.0000 (37046.8328)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1394  max mem: 15572
Epoch: [39]  [1510/2809]  eta: 0:12:49  lr: 0.000000  min_lr: 0.000000  loss: 4.2021 (4.1967)  class_acc: 0.2500 (0.3261)  loss_scale: 32768.0000 (37018.5149)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1156  max mem: 15572
Epoch: [39]  [1520/2809]  eta: 0:12:44  lr: 0.000000  min_lr: 0.000000  loss: 4.2676 (4.1970)  class_acc: 0.2083 (0.3256)  loss_scale: 32768.0000 (36990.5694)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1313  max mem: 15572
Epoch: [39]  [1530/2809]  eta: 0:12:38  lr: 0.000000  min_lr: 0.000000  loss: 4.2029 (4.1963)  class_acc: 0.2500 (0.3252)  loss_scale: 32768.0000 (36962.9889)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1612  max mem: 15572
Epoch: [39]  [1540/2809]  eta: 0:12:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1653 (4.1973)  class_acc: 0.2500 (0.3249)  loss_scale: 32768.0000 (36935.7664)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1704  max mem: 15572
Epoch: [39]  [1550/2809]  eta: 0:12:27  lr: 0.000000  min_lr: 0.000000  loss: 4.2757 (4.1970)  class_acc: 0.2917 (0.3251)  loss_scale: 32768.0000 (36908.8949)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.2004  max mem: 15572
Epoch: [39]  [1560/2809]  eta: 0:12:21  lr: 0.000000  min_lr: 0.000000  loss: 4.0799 (4.1966)  class_acc: 0.2917 (0.3247)  loss_scale: 32768.0000 (36882.3677)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1712  max mem: 15572
Epoch: [39]  [1570/2809]  eta: 0:12:15  lr: 0.000000  min_lr: 0.000000  loss: 4.1930 (4.1970)  class_acc: 0.2917 (0.3249)  loss_scale: 32768.0000 (36856.1782)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1183  max mem: 15572
Epoch: [39]  [1580/2809]  eta: 0:12:09  lr: 0.000000  min_lr: 0.000000  loss: 4.2781 (4.1981)  class_acc: 0.3333 (0.3253)  loss_scale: 32768.0000 (36830.3201)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1500  max mem: 15572
Epoch: [39]  [1590/2809]  eta: 0:12:03  lr: 0.000000  min_lr: 0.000000  loss: 4.2895 (4.1986)  class_acc: 0.3333 (0.3251)  loss_scale: 32768.0000 (36804.7869)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1570  max mem: 15572
Epoch: [39]  [1600/2809]  eta: 0:11:57  lr: 0.000000  min_lr: 0.000000  loss: 4.2468 (4.1992)  class_acc: 0.2917 (0.3250)  loss_scale: 32768.0000 (36779.5728)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1036  max mem: 15572
[2025-01-16 09:07:34,071] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:07:34,071] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 09:07:34,486] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111156
[2025-01-16 09:07:34,486] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:07:34,486] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [1610/2809]  eta: 0:11:51  lr: 0.000000  min_lr: 0.000000  loss: 4.2161 (4.1987)  class_acc: 0.4167 (0.3256)  loss_scale: 32768.0000 (36775.0118)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1003  max mem: 15572
Epoch: [39]  [1620/2809]  eta: 0:11:45  lr: 0.000000  min_lr: 0.000000  loss: 4.1916 (4.1991)  class_acc: 0.3750 (0.3254)  loss_scale: 32768.0000 (36750.2924)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1521  max mem: 15572
Epoch: [39]  [1630/2809]  eta: 0:11:39  lr: 0.000000  min_lr: 0.000000  loss: 4.1660 (4.1986)  class_acc: 0.2917 (0.3253)  loss_scale: 32768.0000 (36725.8761)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1508  max mem: 15572
Epoch: [39]  [1640/2809]  eta: 0:11:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1035 (4.1976)  class_acc: 0.3750 (0.3259)  loss_scale: 32768.0000 (36701.7575)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1317  max mem: 15572
Epoch: [39]  [1650/2809]  eta: 0:11:27  lr: 0.000000  min_lr: 0.000000  loss: 4.0779 (4.1972)  class_acc: 0.3750 (0.3259)  loss_scale: 32768.0000 (36677.9310)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1343  max mem: 15572
Epoch: [39]  [1660/2809]  eta: 0:11:21  lr: 0.000000  min_lr: 0.000000  loss: 4.2230 (4.1974)  class_acc: 0.3333 (0.3259)  loss_scale: 32768.0000 (36654.3913)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1080  max mem: 15572
Epoch: [39]  [1670/2809]  eta: 0:11:15  lr: 0.000000  min_lr: 0.000000  loss: 4.1835 (4.1978)  class_acc: 0.3750 (0.3262)  loss_scale: 32768.0000 (36631.1335)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1360  max mem: 15572
Epoch: [39]  [1680/2809]  eta: 0:11:09  lr: 0.000000  min_lr: 0.000000  loss: 4.1283 (4.1980)  class_acc: 0.3750 (0.3263)  loss_scale: 32768.0000 (36608.1523)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1722  max mem: 15572
Epoch: [39]  [1690/2809]  eta: 0:11:02  lr: 0.000000  min_lr: 0.000000  loss: 4.1672 (4.1976)  class_acc: 0.3333 (0.3263)  loss_scale: 32768.0000 (36585.4429)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1128  max mem: 15572
Epoch: [39]  [1700/2809]  eta: 0:10:56  lr: 0.000000  min_lr: 0.000000  loss: 4.1801 (4.1974)  class_acc: 0.2917 (0.3263)  loss_scale: 32768.0000 (36563.0006)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0907  max mem: 15572
Epoch: [39]  [1710/2809]  eta: 0:10:50  lr: 0.000000  min_lr: 0.000000  loss: 4.2389 (4.1975)  class_acc: 0.2917 (0.3263)  loss_scale: 32768.0000 (36540.8206)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1279  max mem: 15572
Epoch: [39]  [1720/2809]  eta: 0:10:45  lr: 0.000000  min_lr: 0.000000  loss: 4.2389 (4.1972)  class_acc: 0.3750 (0.3265)  loss_scale: 32768.0000 (36518.8983)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1602  max mem: 15572
Epoch: [39]  [1730/2809]  eta: 0:10:39  lr: 0.000000  min_lr: 0.000000  loss: 4.2216 (4.1978)  class_acc: 0.3750 (0.3268)  loss_scale: 32768.0000 (36497.2293)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1384  max mem: 15572
[2025-01-16 09:08:49,744] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:08:49,744] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [1740/2809]  eta: 0:10:33  lr: 0.000000  min_lr: 0.000000  loss: 4.2216 (4.1974)  class_acc: 0.3750 (0.3271)  loss_scale: 32768.0000 (36607.5589)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.1661  max mem: 15572
[2025-01-16 09:08:57,283] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111296
[2025-01-16 09:08:57,284] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:08:57,285] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [1750/2809]  eta: 0:10:27  lr: 0.000000  min_lr: 0.000000  loss: 4.1450 (4.1970)  class_acc: 0.2917 (0.3271)  loss_scale: 65536.0000 (36660.4866)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1545  max mem: 15572
Epoch: [39]  [1760/2809]  eta: 0:10:21  lr: 0.000000  min_lr: 0.000000  loss: 4.1957 (4.1982)  class_acc: 0.2500 (0.3268)  loss_scale: 32768.0000 (36638.3827)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0754  max mem: 15572
Epoch: [39]  [1770/2809]  eta: 0:10:15  lr: 0.000000  min_lr: 0.000000  loss: 4.2619 (4.1976)  class_acc: 0.2917 (0.3270)  loss_scale: 32768.0000 (36616.5285)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1324  max mem: 15572
Epoch: [39]  [1780/2809]  eta: 0:10:09  lr: 0.000000  min_lr: 0.000000  loss: 4.1066 (4.1973)  class_acc: 0.3333 (0.3270)  loss_scale: 32768.0000 (36594.9197)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1502  max mem: 15572
Epoch: [39]  [1790/2809]  eta: 0:10:03  lr: 0.000000  min_lr: 0.000000  loss: 4.1820 (4.1968)  class_acc: 0.2917 (0.3268)  loss_scale: 32768.0000 (36573.5522)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1236  max mem: 15572
Epoch: [39]  [1800/2809]  eta: 0:09:57  lr: 0.000000  min_lr: 0.000000  loss: 4.1058 (4.1963)  class_acc: 0.3333 (0.3269)  loss_scale: 32768.0000 (36552.4220)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1217  max mem: 15572
Epoch: [39]  [1810/2809]  eta: 0:09:51  lr: 0.000000  min_lr: 0.000000  loss: 4.2662 (4.1972)  class_acc: 0.2500 (0.3263)  loss_scale: 32768.0000 (36531.5251)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1482  max mem: 15572
Epoch: [39]  [1820/2809]  eta: 0:09:45  lr: 0.000000  min_lr: 0.000000  loss: 4.2696 (4.1970)  class_acc: 0.2500 (0.3264)  loss_scale: 32768.0000 (36510.8578)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1648  max mem: 15572
Epoch: [39]  [1830/2809]  eta: 0:09:39  lr: 0.000000  min_lr: 0.000000  loss: 4.1244 (4.1961)  class_acc: 0.3333 (0.3261)  loss_scale: 32768.0000 (36490.4162)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1464  max mem: 15572
Epoch: [39]  [1840/2809]  eta: 0:09:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1244 (4.1957)  class_acc: 0.3333 (0.3264)  loss_scale: 32768.0000 (36470.1966)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1244  max mem: 15572
Epoch: [39]  [1850/2809]  eta: 0:09:28  lr: 0.000000  min_lr: 0.000000  loss: 4.2013 (4.1961)  class_acc: 0.3750 (0.3266)  loss_scale: 32768.0000 (36450.1956)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1706  max mem: 15572
Epoch: [39]  [1860/2809]  eta: 0:09:22  lr: 0.000000  min_lr: 0.000000  loss: 4.2311 (4.1957)  class_acc: 0.4167 (0.3271)  loss_scale: 32768.0000 (36430.4095)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1836  max mem: 15572
Epoch: [39]  [1870/2809]  eta: 0:09:16  lr: 0.000000  min_lr: 0.000000  loss: 4.0335 (4.1946)  class_acc: 0.3333 (0.3271)  loss_scale: 32768.0000 (36410.8348)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1471  max mem: 15572
[2025-01-16 09:10:12,396] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:10:12,396] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [1880/2809]  eta: 0:09:10  lr: 0.000000  min_lr: 0.000000  loss: 3.9942 (4.1944)  class_acc: 0.2917 (0.3269)  loss_scale: 32768.0000 (36513.4120)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1395  max mem: 15572
Epoch: [39]  [1890/2809]  eta: 0:09:03  lr: 0.000000  min_lr: 0.000000  loss: 4.0981 (4.1936)  class_acc: 0.2917 (0.3269)  loss_scale: 65536.0000 (36666.8895)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0690  max mem: 15572
Epoch: [39]  [1900/2809]  eta: 0:08:57  lr: 0.000000  min_lr: 0.000000  loss: 4.1633 (4.1932)  class_acc: 0.2917 (0.3267)  loss_scale: 65536.0000 (36818.7522)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1105  max mem: 15572
Epoch: [39]  [1910/2809]  eta: 0:08:51  lr: 0.000000  min_lr: 0.000000  loss: 4.1747 (4.1933)  class_acc: 0.3333 (0.3270)  loss_scale: 65536.0000 (36969.0256)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1569  max mem: 15572
[2025-01-16 09:10:33,854] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111463
[2025-01-16 09:10:33,854] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:10:33,854] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [1920/2809]  eta: 0:08:45  lr: 0.000000  min_lr: 0.000000  loss: 4.3029 (4.1940)  class_acc: 0.3750 (0.3271)  loss_scale: 65536.0000 (36964.2145)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1318  max mem: 15572
Epoch: [39]  [1930/2809]  eta: 0:08:40  lr: 0.000000  min_lr: 0.000000  loss: 4.1161 (4.1928)  class_acc: 0.3333 (0.3268)  loss_scale: 32768.0000 (36942.4837)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1845  max mem: 15572
Epoch: [39]  [1940/2809]  eta: 0:08:34  lr: 0.000000  min_lr: 0.000000  loss: 4.0478 (4.1926)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (36920.9768)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.1909  max mem: 15572
Epoch: [39]  [1950/2809]  eta: 0:08:28  lr: 0.000000  min_lr: 0.000000  loss: 4.1824 (4.1928)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (36899.6904)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1455  max mem: 15572
Epoch: [39]  [1960/2809]  eta: 0:08:22  lr: 0.000000  min_lr: 0.000000  loss: 4.1225 (4.1924)  class_acc: 0.3333 (0.3268)  loss_scale: 32768.0000 (36878.6211)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1352  max mem: 15572
Epoch: [39]  [1970/2809]  eta: 0:08:16  lr: 0.000000  min_lr: 0.000000  loss: 4.1036 (4.1930)  class_acc: 0.3333 (0.3264)  loss_scale: 32768.0000 (36857.7656)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1603  max mem: 15572
Epoch: [39]  [1980/2809]  eta: 0:08:10  lr: 0.000000  min_lr: 0.000000  loss: 4.2109 (4.1929)  class_acc: 0.2083 (0.3265)  loss_scale: 32768.0000 (36837.1206)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1335  max mem: 15572
Epoch: [39]  [1990/2809]  eta: 0:08:04  lr: 0.000000  min_lr: 0.000000  loss: 4.1213 (4.1927)  class_acc: 0.2500 (0.3263)  loss_scale: 32768.0000 (36816.6831)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0921  max mem: 15572
Epoch: [39]  [2000/2809]  eta: 0:07:58  lr: 0.000000  min_lr: 0.000000  loss: 4.1213 (4.1932)  class_acc: 0.2917 (0.3265)  loss_scale: 32768.0000 (36796.4498)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0772  max mem: 15572
Epoch: [39]  [2010/2809]  eta: 0:07:52  lr: 0.000000  min_lr: 0.000000  loss: 4.3067 (4.1938)  class_acc: 0.2917 (0.3265)  loss_scale: 32768.0000 (36776.4177)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1038  max mem: 15572
Epoch: [39]  [2020/2809]  eta: 0:07:46  lr: 0.000000  min_lr: 0.000000  loss: 4.2944 (4.1939)  class_acc: 0.2917 (0.3263)  loss_scale: 32768.0000 (36756.5839)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1562  max mem: 15572
Epoch: [39]  [2030/2809]  eta: 0:07:41  lr: 0.000000  min_lr: 0.000000  loss: 4.0777 (4.1935)  class_acc: 0.2917 (0.3264)  loss_scale: 32768.0000 (36736.9453)  weight_decay: 0.0500 (0.0500)  time: 0.6394  data: 0.1977  max mem: 15572
Epoch: [39]  [2040/2809]  eta: 0:07:34  lr: 0.000000  min_lr: 0.000000  loss: 4.1012 (4.1935)  class_acc: 0.3333 (0.3264)  loss_scale: 32768.0000 (36717.4993)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1590  max mem: 15572
[2025-01-16 09:11:50,605] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:11:50,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [2050/2809]  eta: 0:07:28  lr: 0.000000  min_lr: 0.000000  loss: 4.3020 (4.1938)  class_acc: 0.2500 (0.3262)  loss_scale: 32768.0000 (36858.0088)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0976  max mem: 15572
[2025-01-16 09:11:58,764] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111604
[2025-01-16 09:11:58,764] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:11:58,764] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [2060/2809]  eta: 0:07:23  lr: 0.000000  min_lr: 0.000000  loss: 4.1989 (4.1931)  class_acc: 0.2917 (0.3266)  loss_scale: 65536.0000 (36869.9622)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1359  max mem: 15572
Epoch: [39]  [2070/2809]  eta: 0:07:17  lr: 0.000000  min_lr: 0.000000  loss: 4.1311 (4.1926)  class_acc: 0.3333 (0.3268)  loss_scale: 32768.0000 (36850.1555)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1587  max mem: 15572
Epoch: [39]  [2080/2809]  eta: 0:07:11  lr: 0.000000  min_lr: 0.000000  loss: 4.1768 (4.1927)  class_acc: 0.3333 (0.3268)  loss_scale: 32768.0000 (36830.5392)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1054  max mem: 15572
Epoch: [39]  [2090/2809]  eta: 0:07:05  lr: 0.000000  min_lr: 0.000000  loss: 4.2131 (4.1933)  class_acc: 0.3333 (0.3267)  loss_scale: 32768.0000 (36811.1105)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1275  max mem: 15572
Epoch: [39]  [2100/2809]  eta: 0:06:59  lr: 0.000000  min_lr: 0.000000  loss: 4.1435 (4.1927)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (36791.8667)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1162  max mem: 15572
Epoch: [39]  [2110/2809]  eta: 0:06:53  lr: 0.000000  min_lr: 0.000000  loss: 4.1271 (4.1923)  class_acc: 0.2917 (0.3268)  loss_scale: 32768.0000 (36772.8053)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0975  max mem: 15572
Epoch: [39]  [2120/2809]  eta: 0:06:47  lr: 0.000000  min_lr: 0.000000  loss: 4.1531 (4.1921)  class_acc: 0.3333 (0.3270)  loss_scale: 32768.0000 (36753.9236)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0827  max mem: 15572
Epoch: [39]  [2130/2809]  eta: 0:06:41  lr: 0.000000  min_lr: 0.000000  loss: 4.1666 (4.1920)  class_acc: 0.3750 (0.3272)  loss_scale: 32768.0000 (36735.2191)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0831  max mem: 15572
Epoch: [39]  [2140/2809]  eta: 0:06:35  lr: 0.000000  min_lr: 0.000000  loss: 4.2243 (4.1926)  class_acc: 0.4167 (0.3273)  loss_scale: 32768.0000 (36716.6894)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1369  max mem: 15572
Epoch: [39]  [2150/2809]  eta: 0:06:29  lr: 0.000000  min_lr: 0.000000  loss: 4.2832 (4.1929)  class_acc: 0.4167 (0.3274)  loss_scale: 32768.0000 (36698.3319)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1069  max mem: 15572
Epoch: [39]  [2160/2809]  eta: 0:06:23  lr: 0.000000  min_lr: 0.000000  loss: 4.1780 (4.1918)  class_acc: 0.4167 (0.3277)  loss_scale: 32768.0000 (36680.1444)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1318  max mem: 15572
Epoch: [39]  [2170/2809]  eta: 0:06:17  lr: 0.000000  min_lr: 0.000000  loss: 4.0257 (4.1918)  class_acc: 0.3333 (0.3277)  loss_scale: 32768.0000 (36662.1244)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.2180  max mem: 15572
Epoch: [39]  [2180/2809]  eta: 0:06:12  lr: 0.000000  min_lr: 0.000000  loss: 4.1379 (4.1916)  class_acc: 0.3333 (0.3277)  loss_scale: 32768.0000 (36644.2696)  weight_decay: 0.0500 (0.0500)  time: 0.6570  data: 0.2082  max mem: 15572
[2025-01-16 09:13:13,561] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:13:13,561] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [2190/2809]  eta: 0:06:06  lr: 0.000000  min_lr: 0.000000  loss: 4.1379 (4.1916)  class_acc: 0.3333 (0.3275)  loss_scale: 32768.0000 (36761.1794)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1537  max mem: 15572
Epoch: [39]  [2200/2809]  eta: 0:05:59  lr: 0.000000  min_lr: 0.000000  loss: 4.1605 (4.1913)  class_acc: 0.2917 (0.3277)  loss_scale: 65536.0000 (36891.9146)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.1050  max mem: 15572
Epoch: [39]  [2210/2809]  eta: 0:05:53  lr: 0.000000  min_lr: 0.000000  loss: 4.1671 (4.1912)  class_acc: 0.3750 (0.3279)  loss_scale: 65536.0000 (37021.4672)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0790  max mem: 15572
Epoch: [39]  [2220/2809]  eta: 0:05:47  lr: 0.000000  min_lr: 0.000000  loss: 4.2168 (4.1915)  class_acc: 0.2917 (0.3281)  loss_scale: 65536.0000 (37149.8532)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0738  max mem: 15572
Epoch: [39]  [2230/2809]  eta: 0:05:41  lr: 0.000000  min_lr: 0.000000  loss: 4.2450 (4.1914)  class_acc: 0.2917 (0.3282)  loss_scale: 65536.0000 (37277.0883)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0999  max mem: 15572
[2025-01-16 09:13:42,545] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111786
[2025-01-16 09:13:42,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:13:42,545] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [2240/2809]  eta: 0:05:35  lr: 0.000000  min_lr: 0.000000  loss: 4.1610 (4.1909)  class_acc: 0.3333 (0.3285)  loss_scale: 65536.0000 (37315.4556)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1275  max mem: 15572
Epoch: [39]  [2250/2809]  eta: 0:05:29  lr: 0.000000  min_lr: 0.000000  loss: 4.1610 (4.1907)  class_acc: 0.2917 (0.3282)  loss_scale: 32768.0000 (37295.2537)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1218  max mem: 15572
Epoch: [39]  [2260/2809]  eta: 0:05:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1968 (4.1908)  class_acc: 0.2917 (0.3282)  loss_scale: 32768.0000 (37275.2304)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1235  max mem: 15572
Epoch: [39]  [2270/2809]  eta: 0:05:18  lr: 0.000000  min_lr: 0.000000  loss: 4.1311 (4.1905)  class_acc: 0.3333 (0.3280)  loss_scale: 32768.0000 (37255.3835)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1062  max mem: 15572
Epoch: [39]  [2280/2809]  eta: 0:05:12  lr: 0.000000  min_lr: 0.000000  loss: 4.0110 (4.1903)  class_acc: 0.3333 (0.3279)  loss_scale: 32768.0000 (37235.7107)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1505  max mem: 15572
Epoch: [39]  [2290/2809]  eta: 0:05:06  lr: 0.000000  min_lr: 0.000000  loss: 4.1274 (4.1902)  class_acc: 0.2917 (0.3276)  loss_scale: 32768.0000 (37216.2095)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1228  max mem: 15572
Epoch: [39]  [2300/2809]  eta: 0:05:00  lr: 0.000000  min_lr: 0.000000  loss: 4.2221 (4.1901)  class_acc: 0.2917 (0.3278)  loss_scale: 32768.0000 (37196.8779)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0789  max mem: 15572
Epoch: [39]  [2310/2809]  eta: 0:04:54  lr: 0.000000  min_lr: 0.000000  loss: 4.2221 (4.1900)  class_acc: 0.2917 (0.3278)  loss_scale: 32768.0000 (37177.7135)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1118  max mem: 15572
Epoch: [39]  [2320/2809]  eta: 0:04:48  lr: 0.000000  min_lr: 0.000000  loss: 4.1895 (4.1899)  class_acc: 0.2917 (0.3276)  loss_scale: 32768.0000 (37158.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0918  max mem: 15572
Epoch: [39]  [2330/2809]  eta: 0:04:42  lr: 0.000000  min_lr: 0.000000  loss: 4.1251 (4.1894)  class_acc: 0.3333 (0.3276)  loss_scale: 32768.0000 (37139.8782)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1035  max mem: 15572
Epoch: [39]  [2340/2809]  eta: 0:04:36  lr: 0.000000  min_lr: 0.000000  loss: 4.0968 (4.1888)  class_acc: 0.3333 (0.3277)  loss_scale: 32768.0000 (37121.2029)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.1021  max mem: 15572
Epoch: [39]  [2350/2809]  eta: 0:04:30  lr: 0.000000  min_lr: 0.000000  loss: 4.1270 (4.1887)  class_acc: 0.3333 (0.3279)  loss_scale: 32768.0000 (37102.6865)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0897  max mem: 15572
Epoch: [39]  [2360/2809]  eta: 0:04:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1593 (4.1886)  class_acc: 0.3333 (0.3281)  loss_scale: 32768.0000 (37084.3270)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1298  max mem: 15572
[2025-01-16 09:14:57,266] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:14:57,266] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [2370/2809]  eta: 0:04:18  lr: 0.000000  min_lr: 0.000000  loss: 4.0756 (4.1879)  class_acc: 0.3750 (0.3283)  loss_scale: 32768.0000 (37162.8646)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1788  max mem: 15572
[2025-01-16 09:15:02,076] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111924
[2025-01-16 09:15:02,077] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:15:02,077] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [2380/2809]  eta: 0:04:12  lr: 0.000000  min_lr: 0.000000  loss: 4.1163 (4.1880)  class_acc: 0.3750 (0.3284)  loss_scale: 32768.0000 (37171.9311)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1852  max mem: 15572
Epoch: [39]  [2390/2809]  eta: 0:04:06  lr: 0.000000  min_lr: 0.000000  loss: 4.1773 (4.1877)  class_acc: 0.3333 (0.3283)  loss_scale: 32768.0000 (37153.5123)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1096  max mem: 15572
Epoch: [39]  [2400/2809]  eta: 0:04:00  lr: 0.000000  min_lr: 0.000000  loss: 4.1950 (4.1877)  class_acc: 0.3333 (0.3282)  loss_scale: 32768.0000 (37135.2470)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1358  max mem: 15572
Epoch: [39]  [2410/2809]  eta: 0:03:55  lr: 0.000000  min_lr: 0.000000  loss: 4.1919 (4.1878)  class_acc: 0.3333 (0.3281)  loss_scale: 32768.0000 (37117.1331)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1639  max mem: 15572
Epoch: [39]  [2420/2809]  eta: 0:03:49  lr: 0.000000  min_lr: 0.000000  loss: 4.1629 (4.1878)  class_acc: 0.3750 (0.3284)  loss_scale: 32768.0000 (37099.1689)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1350  max mem: 15572
Epoch: [39]  [2430/2809]  eta: 0:03:43  lr: 0.000000  min_lr: 0.000000  loss: 4.1469 (4.1874)  class_acc: 0.4167 (0.3286)  loss_scale: 32768.0000 (37081.3525)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1499  max mem: 15572
Epoch: [39]  [2440/2809]  eta: 0:03:37  lr: 0.000000  min_lr: 0.000000  loss: 4.0622 (4.1872)  class_acc: 0.3333 (0.3286)  loss_scale: 32768.0000 (37063.6821)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1289  max mem: 15572
[2025-01-16 09:15:44,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=112000, skipped=701, lr=[4.692607746832985e-10, 4.692607746832985e-10, 6.703725352618551e-10, 6.703725352618551e-10, 9.576750503740788e-10, 9.576750503740788e-10, 1.3681072148201127e-09, 1.3681072148201127e-09, 1.9544388783144467e-09, 1.9544388783144467e-09, 2.79205554044921e-09, 2.79205554044921e-09, 3.9886507720703e-09, 3.9886507720703e-09, 5.698072531529001e-09, 5.698072531529001e-09, 8.14010361647e-09, 8.14010361647e-09, 1.1628719452100003e-08, 1.1628719452100003e-08, 1.661245636014286e-08, 1.661245636014286e-08, 2.3732080514489803e-08, 2.3732080514489803e-08, 3.390297216355687e-08, 3.390297216355687e-08, 4.843281737650981e-08, 4.843281737650981e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 09:15:44,782] [INFO] [timer.py:260:stop] epoch=0/micro_step=112000/global_step=112000, RunningAvgSamplesPerSec=28.003847799421493, CurrSamplesPerSec=23.90952207223641, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [39]  [2450/2809]  eta: 0:03:31  lr: 0.000000  min_lr: 0.000000  loss: 4.3001 (4.1872)  class_acc: 0.3333 (0.3286)  loss_scale: 32768.0000 (37046.1559)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0824  max mem: 15572
Epoch: [39]  [2460/2809]  eta: 0:03:25  lr: 0.000000  min_lr: 0.000000  loss: 4.3045 (4.1875)  class_acc: 0.2917 (0.3284)  loss_scale: 32768.0000 (37028.7720)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.0990  max mem: 15572
Epoch: [39]  [2470/2809]  eta: 0:03:19  lr: 0.000000  min_lr: 0.000000  loss: 4.2573 (4.1877)  class_acc: 0.2917 (0.3284)  loss_scale: 32768.0000 (37011.5289)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1199  max mem: 15572
Epoch: [39]  [2480/2809]  eta: 0:03:13  lr: 0.000000  min_lr: 0.000000  loss: 4.2573 (4.1878)  class_acc: 0.2500 (0.3281)  loss_scale: 32768.0000 (36994.4248)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1450  max mem: 15572
Epoch: [39]  [2490/2809]  eta: 0:03:07  lr: 0.000000  min_lr: 0.000000  loss: 4.2342 (4.1881)  class_acc: 0.2917 (0.3279)  loss_scale: 32768.0000 (36977.4580)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.1830  max mem: 15572
Epoch: [39]  [2500/2809]  eta: 0:03:01  lr: 0.000000  min_lr: 0.000000  loss: 4.2791 (4.1887)  class_acc: 0.2917 (0.3278)  loss_scale: 32768.0000 (36960.6269)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1405  max mem: 15572
[2025-01-16 09:16:16,525] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:16:16,525] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [2510/2809]  eta: 0:02:55  lr: 0.000000  min_lr: 0.000000  loss: 4.2705 (4.1887)  class_acc: 0.2917 (0.3277)  loss_scale: 32768.0000 (37061.3779)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1398  max mem: 15572
Epoch: [39]  [2520/2809]  eta: 0:02:50  lr: 0.000000  min_lr: 0.000000  loss: 4.2373 (4.1888)  class_acc: 0.2917 (0.3275)  loss_scale: 65536.0000 (37174.3276)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1412  max mem: 15572
Epoch: [39]  [2530/2809]  eta: 0:02:44  lr: 0.000000  min_lr: 0.000000  loss: 4.1550 (4.1883)  class_acc: 0.2917 (0.3277)  loss_scale: 65536.0000 (37286.3848)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.1003  max mem: 15572
[2025-01-16 09:16:33,042] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 112083
[2025-01-16 09:16:33,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:16:33,042] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [2540/2809]  eta: 0:02:38  lr: 0.000000  min_lr: 0.000000  loss: 4.1550 (4.1881)  class_acc: 0.3750 (0.3280)  loss_scale: 65536.0000 (37281.4986)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1563  max mem: 15572
Epoch: [39]  [2550/2809]  eta: 0:02:32  lr: 0.000000  min_lr: 0.000000  loss: 4.1875 (4.1884)  class_acc: 0.2917 (0.3278)  loss_scale: 32768.0000 (37263.8056)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1541  max mem: 15572
Epoch: [39]  [2560/2809]  eta: 0:02:26  lr: 0.000000  min_lr: 0.000000  loss: 4.1788 (4.1884)  class_acc: 0.2917 (0.3278)  loss_scale: 32768.0000 (37246.2507)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0865  max mem: 15572
Epoch: [39]  [2570/2809]  eta: 0:02:20  lr: 0.000000  min_lr: 0.000000  loss: 4.1773 (4.1877)  class_acc: 0.2917 (0.3280)  loss_scale: 32768.0000 (37228.8324)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1727  max mem: 15572
Epoch: [39]  [2580/2809]  eta: 0:02:14  lr: 0.000000  min_lr: 0.000000  loss: 4.1508 (4.1878)  class_acc: 0.3333 (0.3281)  loss_scale: 32768.0000 (37211.5490)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1487  max mem: 15572
Epoch: [39]  [2590/2809]  eta: 0:02:08  lr: 0.000000  min_lr: 0.000000  loss: 4.1151 (4.1877)  class_acc: 0.3333 (0.3281)  loss_scale: 32768.0000 (37194.3991)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1214  max mem: 15572
Epoch: [39]  [2600/2809]  eta: 0:02:02  lr: 0.000000  min_lr: 0.000000  loss: 4.0925 (4.1876)  class_acc: 0.2917 (0.3280)  loss_scale: 32768.0000 (37177.3810)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.2020  max mem: 15572
Epoch: [39]  [2610/2809]  eta: 0:01:57  lr: 0.000000  min_lr: 0.000000  loss: 4.1445 (4.1877)  class_acc: 0.2917 (0.3280)  loss_scale: 32768.0000 (37160.4933)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1785  max mem: 15572
Epoch: [39]  [2620/2809]  eta: 0:01:51  lr: 0.000000  min_lr: 0.000000  loss: 4.2489 (4.1882)  class_acc: 0.3333 (0.3282)  loss_scale: 32768.0000 (37143.7345)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1319  max mem: 15572
Epoch: [39]  [2630/2809]  eta: 0:01:45  lr: 0.000000  min_lr: 0.000000  loss: 4.2962 (4.1882)  class_acc: 0.3333 (0.3282)  loss_scale: 32768.0000 (37127.1030)  weight_decay: 0.0500 (0.0500)  time: 0.4579  data: 0.0504  max mem: 15572
Epoch: [39]  [2640/2809]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 4.3574 (4.1885)  class_acc: 0.2500 (0.3278)  loss_scale: 32768.0000 (37110.5975)  weight_decay: 0.0500 (0.0500)  time: 0.4320  data: 0.0006  max mem: 15572
Epoch: [39]  [2650/2809]  eta: 0:01:33  lr: 0.000000  min_lr: 0.000000  loss: 4.2150 (4.1884)  class_acc: 0.2500 (0.3278)  loss_scale: 32768.0000 (37094.2165)  weight_decay: 0.0500 (0.0500)  time: 0.4643  data: 0.0009  max mem: 15572
Epoch: [39]  [2660/2809]  eta: 0:01:27  lr: 0.000000  min_lr: 0.000000  loss: 4.1442 (4.1882)  class_acc: 0.2917 (0.3277)  loss_scale: 32768.0000 (37077.9587)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1105  max mem: 15572
[2025-01-16 09:17:44,887] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:17:44,887] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 09:17:50,987] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 112220
[2025-01-16 09:17:50,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:17:50,987] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [2670/2809]  eta: 0:01:21  lr: 0.000000  min_lr: 0.000000  loss: 4.2573 (4.1886)  class_acc: 0.3333 (0.3277)  loss_scale: 32768.0000 (37159.9671)  weight_decay: 0.0500 (0.0500)  time: 0.6809  data: 0.2354  max mem: 15572
Epoch: [39]  [2680/2809]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 4.2541 (4.1886)  class_acc: 0.2917 (0.3275)  loss_scale: 32768.0000 (37143.5852)  weight_decay: 0.0500 (0.0500)  time: 0.6943  data: 0.2428  max mem: 15572
Epoch: [39]  [2690/2809]  eta: 0:01:09  lr: 0.000000  min_lr: 0.000000  loss: 4.2541 (4.1893)  class_acc: 0.2083 (0.3274)  loss_scale: 32768.0000 (37127.3252)  weight_decay: 0.0500 (0.0500)  time: 0.6979  data: 0.2472  max mem: 15572
Epoch: [39]  [2700/2809]  eta: 0:01:04  lr: 0.000000  min_lr: 0.000000  loss: 4.1859 (4.1891)  class_acc: 0.3333 (0.3274)  loss_scale: 32768.0000 (37111.1855)  weight_decay: 0.0500 (0.0500)  time: 0.6601  data: 0.2031  max mem: 15572
Epoch: [39]  [2710/2809]  eta: 0:00:58  lr: 0.000000  min_lr: 0.000000  loss: 4.1535 (4.1890)  class_acc: 0.3333 (0.3274)  loss_scale: 32768.0000 (37095.1649)  weight_decay: 0.0500 (0.0500)  time: 0.6521  data: 0.1746  max mem: 15572
Epoch: [39]  [2720/2809]  eta: 0:00:52  lr: 0.000000  min_lr: 0.000000  loss: 4.3491 (4.1897)  class_acc: 0.2917 (0.3274)  loss_scale: 32768.0000 (37079.2620)  weight_decay: 0.0500 (0.0500)  time: 0.6591  data: 0.1735  max mem: 15572
Epoch: [39]  [2730/2809]  eta: 0:00:46  lr: 0.000000  min_lr: 0.000000  loss: 4.3018 (4.1897)  class_acc: 0.2917 (0.3274)  loss_scale: 32768.0000 (37063.4756)  weight_decay: 0.0500 (0.0500)  time: 0.7173  data: 0.2283  max mem: 15572
Epoch: [39]  [2740/2809]  eta: 0:00:40  lr: 0.000000  min_lr: 0.000000  loss: 4.1056 (4.1888)  class_acc: 0.2917 (0.3275)  loss_scale: 32768.0000 (37047.8045)  weight_decay: 0.0500 (0.0500)  time: 0.7420  data: 0.2601  max mem: 15572
Epoch: [39]  [2750/2809]  eta: 0:00:34  lr: 0.000000  min_lr: 0.000000  loss: 4.0855 (4.1886)  class_acc: 0.3333 (0.3278)  loss_scale: 32768.0000 (37032.2472)  weight_decay: 0.0500 (0.0500)  time: 0.6973  data: 0.2249  max mem: 15572
Epoch: [39]  [2760/2809]  eta: 0:00:28  lr: 0.000000  min_lr: 0.000000  loss: 4.1835 (4.1888)  class_acc: 0.3333 (0.3278)  loss_scale: 32768.0000 (37016.8026)  weight_decay: 0.0500 (0.0500)  time: 0.7029  data: 0.2211  max mem: 15572
Epoch: [39]  [2770/2809]  eta: 0:00:23  lr: 0.000000  min_lr: 0.000000  loss: 4.2322 (4.1891)  class_acc: 0.2917 (0.3276)  loss_scale: 32768.0000 (37001.4695)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1003  max mem: 15572
Epoch: [39]  [2780/2809]  eta: 0:00:17  lr: 0.000000  min_lr: 0.000000  loss: 4.1941 (4.1888)  class_acc: 0.3333 (0.3276)  loss_scale: 32768.0000 (36986.2467)  weight_decay: 0.0500 (0.0500)  time: 0.4062  data: 0.0004  max mem: 15572
Epoch: [39]  [2790/2809]  eta: 0:00:11  lr: 0.000000  min_lr: 0.000000  loss: 4.1131 (4.1888)  class_acc: 0.2917 (0.3273)  loss_scale: 32768.0000 (36971.1329)  weight_decay: 0.0500 (0.0500)  time: 0.4305  data: 0.0006  max mem: 15572
[2025-01-16 09:19:11,656] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:19:11,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [2800/2809]  eta: 0:00:05  lr: 0.000000  min_lr: 0.000000  loss: 4.1717 (4.1890)  class_acc: 0.2500 (0.3272)  loss_scale: 32768.0000 (36991.2231)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0832  max mem: 15572
[2025-01-16 09:19:14,595] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 112356
[2025-01-16 09:19:14,595] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:19:14,595] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [2808/2809]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 4.1862 (4.1890)  class_acc: 0.2917 (0.3271)  loss_scale: 32768.0000 (37025.8569)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1413  max mem: 15572
Epoch: [39] Total time: 0:27:35 (0.5893 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 4.1862 (4.1890)  class_acc: 0.2917 (0.3271)  loss_scale: 32768.0000 (37025.8569)  weight_decay: 0.0500 (0.0500)
[2025-01-16 09:19:17,337] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2025-01-16 09:19:17,342] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-39/mp_rank_00_model_states.pt
[2025-01-16 09:19:17,342] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-39/mp_rank_00_model_states.pt...
[2025-01-16 09:19:18,362] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-39/mp_rank_00_model_states.pt.
[2025-01-16 09:19:18,362] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [  0/272]  eta: 0:24:40  loss: 1.1146 (1.1146)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 5.4421  data: 5.2643  max mem: 15572
Val:  [ 10/272]  eta: 0:04:21  loss: 2.4303 (2.4560)  acc1: 50.0000 (46.9697)  acc5: 83.3333 (78.7879)  time: 0.9977  data: 0.8021  max mem: 15572
Val:  [ 20/272]  eta: 0:02:38  loss: 2.4910 (2.5083)  acc1: 44.4444 (47.6190)  acc5: 77.7778 (77.5132)  time: 0.3871  data: 0.1917  max mem: 15572
Val:  [ 30/272]  eta: 0:02:09  loss: 2.4910 (2.5432)  acc1: 44.4444 (44.4444)  acc5: 77.7778 (76.3441)  time: 0.2782  data: 0.0800  max mem: 15572
Val:  [ 40/272]  eta: 0:01:55  loss: 2.5376 (2.5760)  acc1: 27.7778 (41.8699)  acc5: 77.7778 (75.2033)  time: 0.3573  data: 0.1587  max mem: 15572
Val:  [ 50/272]  eta: 0:01:44  loss: 2.5376 (2.5196)  acc1: 44.4444 (43.2462)  acc5: 77.7778 (76.7974)  time: 0.3765  data: 0.1865  max mem: 15572
Val:  [ 60/272]  eta: 0:01:35  loss: 1.9577 (2.4603)  acc1: 55.5556 (44.8087)  acc5: 83.3333 (77.5046)  time: 0.3594  data: 0.1774  max mem: 15572
Val:  [ 70/272]  eta: 0:01:30  loss: 1.9933 (2.4029)  acc1: 61.1111 (46.9484)  acc5: 83.3333 (78.5603)  time: 0.3887  data: 0.1945  max mem: 15572
Val:  [ 80/272]  eta: 0:01:24  loss: 2.1500 (2.4139)  acc1: 50.0000 (47.1879)  acc5: 83.3333 (78.3951)  time: 0.3982  data: 0.1896  max mem: 15572
Val:  [ 90/272]  eta: 0:01:18  loss: 2.4393 (2.4189)  acc1: 50.0000 (47.6190)  acc5: 77.7778 (78.4493)  time: 0.3801  data: 0.1822  max mem: 15572
Val:  [100/272]  eta: 0:01:12  loss: 2.4606 (2.4451)  acc1: 50.0000 (46.8097)  acc5: 77.7778 (78.0528)  time: 0.3590  data: 0.1670  max mem: 15572
Val:  [110/272]  eta: 0:01:07  loss: 2.5913 (2.4998)  acc1: 33.3333 (45.2452)  acc5: 72.2222 (77.1271)  time: 0.3327  data: 0.1343  max mem: 15572
Val:  [120/272]  eta: 0:01:02  loss: 3.0135 (2.5338)  acc1: 27.7778 (44.3526)  acc5: 66.6667 (76.4463)  time: 0.3358  data: 0.1424  max mem: 15572
Val:  [130/272]  eta: 0:00:58  loss: 2.4862 (2.5073)  acc1: 44.4444 (45.0382)  acc5: 77.7778 (77.0992)  time: 0.3929  data: 0.2103  max mem: 15572
Val:  [140/272]  eta: 0:00:54  loss: 2.1418 (2.5119)  acc1: 55.5556 (45.3113)  acc5: 83.3333 (77.0292)  time: 0.4166  data: 0.2242  max mem: 15572
Val:  [150/272]  eta: 0:00:49  loss: 2.5680 (2.5088)  acc1: 38.8889 (44.9227)  acc5: 77.7778 (77.1891)  time: 0.3522  data: 0.1466  max mem: 15572
Val:  [160/272]  eta: 0:00:45  loss: 2.4839 (2.5078)  acc1: 44.4444 (45.2381)  acc5: 77.7778 (77.3637)  time: 0.3659  data: 0.1627  max mem: 15572
Val:  [170/272]  eta: 0:00:40  loss: 2.6217 (2.5234)  acc1: 38.8889 (44.6394)  acc5: 72.2222 (76.9331)  time: 0.3648  data: 0.1718  max mem: 15572
Val:  [180/272]  eta: 0:00:36  loss: 2.5390 (2.5128)  acc1: 38.8889 (44.6286)  acc5: 72.2222 (77.1946)  time: 0.3523  data: 0.1582  max mem: 15572
Val:  [190/272]  eta: 0:00:32  loss: 2.5113 (2.5492)  acc1: 38.8889 (43.5718)  acc5: 72.2222 (76.1198)  time: 0.3903  data: 0.1904  max mem: 15572
Val:  [200/272]  eta: 0:00:28  loss: 2.7071 (2.5539)  acc1: 38.8889 (43.0901)  acc5: 72.2222 (75.9812)  time: 0.3733  data: 0.1785  max mem: 15572
Val:  [210/272]  eta: 0:00:24  loss: 2.4045 (2.5604)  acc1: 38.8889 (42.9700)  acc5: 77.7778 (75.8557)  time: 0.3405  data: 0.1424  max mem: 15572
Val:  [220/272]  eta: 0:00:20  loss: 2.6184 (2.5550)  acc1: 44.4444 (43.1624)  acc5: 72.2222 (75.8421)  time: 0.3786  data: 0.1698  max mem: 15572
Val:  [230/272]  eta: 0:00:16  loss: 2.1813 (2.5402)  acc1: 55.5556 (44.0596)  acc5: 77.7778 (76.0462)  time: 0.3692  data: 0.1705  max mem: 15572
Val:  [240/272]  eta: 0:00:12  loss: 2.1125 (2.5280)  acc1: 61.1111 (44.3292)  acc5: 83.3333 (76.3485)  time: 0.3424  data: 0.1474  max mem: 15572
Val:  [250/272]  eta: 0:00:08  loss: 2.4297 (2.5326)  acc1: 38.8889 (43.8690)  acc5: 77.7778 (76.2948)  time: 0.3755  data: 0.1733  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.9486 (2.4936)  acc1: 61.1111 (45.2746)  acc5: 83.3333 (76.8838)  time: 0.3577  data: 0.1527  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.9486 (2.4889)  acc1: 61.1111 (45.3055)  acc5: 83.3333 (77.0808)  time: 0.2655  data: 0.0793  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.9486 (2.4926)  acc1: 61.1111 (45.3000)  acc5: 83.3333 (77.0633)  time: 0.2555  data: 0.0792  max mem: 15572
Val: Total time: 0:01:43 (0.3812 s / it)
* Acc@1 45.300 Acc@5 77.063 loss 2.493
Accuracy of the network on the 4883 val videos: 45.3%
[2025-01-16 09:21:02,041] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 09:21:02,048] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 09:21:02,048] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 09:21:05,719] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 09:21:05,720] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.30%
Test:  [   0/2442]  eta: 4:33:56  loss: 0.9565 (0.9565)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 6.7306  data: 6.5165  max mem: 15572
Test:  [  10/2442]  eta: 0:34:08  loss: 2.7817 (2.6694)  acc1: 33.3333 (40.9091)  acc5: 75.0000 (72.7273)  time: 0.8425  data: 0.6929  max mem: 15572
Test:  [  20/2442]  eta: 0:23:28  loss: 2.6899 (2.5451)  acc1: 33.3333 (45.2381)  acc5: 75.0000 (74.2064)  time: 0.2740  data: 0.1352  max mem: 15572
Test:  [  30/2442]  eta: 0:19:36  loss: 2.5520 (2.5857)  acc1: 50.0000 (44.3548)  acc5: 75.0000 (74.4624)  time: 0.2927  data: 0.1577  max mem: 15572
Test:  [  40/2442]  eta: 0:17:20  loss: 2.5680 (2.5491)  acc1: 41.6667 (43.6992)  acc5: 75.0000 (76.2195)  time: 0.2774  data: 0.1414  max mem: 15572
Test:  [  50/2442]  eta: 0:15:46  loss: 2.8021 (2.6754)  acc1: 25.0000 (39.0523)  acc5: 66.6667 (72.3856)  time: 0.2526  data: 0.1173  max mem: 15572
Test:  [  60/2442]  eta: 0:14:49  loss: 2.8275 (2.6872)  acc1: 25.0000 (37.9781)  acc5: 66.6667 (71.3115)  time: 0.2510  data: 0.1081  max mem: 15572
Test:  [  70/2442]  eta: 0:14:24  loss: 2.7249 (2.6725)  acc1: 33.3333 (38.0282)  acc5: 66.6667 (71.7136)  time: 0.2844  data: 0.1346  max mem: 15572
Test:  [  80/2442]  eta: 0:13:47  loss: 2.1051 (2.5809)  acc1: 58.3333 (41.5638)  acc5: 83.3333 (73.5597)  time: 0.2803  data: 0.1337  max mem: 15572
Test:  [  90/2442]  eta: 0:13:44  loss: 2.0378 (2.5764)  acc1: 58.3333 (41.8498)  acc5: 83.3333 (73.7180)  time: 0.3023  data: 0.1556  max mem: 15572
Test:  [ 100/2442]  eta: 0:13:18  loss: 2.2419 (2.5537)  acc1: 50.0000 (42.3267)  acc5: 83.3333 (73.8449)  time: 0.3023  data: 0.1657  max mem: 15572
Test:  [ 110/2442]  eta: 0:12:49  loss: 2.1904 (2.5163)  acc1: 50.0000 (43.9189)  acc5: 75.0000 (74.1742)  time: 0.2353  data: 0.0999  max mem: 15572
Test:  [ 120/2442]  eta: 0:12:42  loss: 2.4460 (2.5344)  acc1: 50.0000 (43.4573)  acc5: 75.0000 (73.8981)  time: 0.2653  data: 0.1210  max mem: 15572
Test:  [ 130/2442]  eta: 0:12:31  loss: 2.4805 (2.5296)  acc1: 41.6667 (43.8295)  acc5: 75.0000 (74.3639)  time: 0.2993  data: 0.1587  max mem: 15572
Test:  [ 140/2442]  eta: 0:12:28  loss: 2.5603 (2.5498)  acc1: 41.6667 (43.2033)  acc5: 75.0000 (73.9362)  time: 0.3051  data: 0.1665  max mem: 15572
Test:  [ 150/2442]  eta: 0:12:28  loss: 2.6336 (2.5631)  acc1: 41.6667 (43.1015)  acc5: 75.0000 (74.0066)  time: 0.3365  data: 0.1908  max mem: 15572
Test:  [ 160/2442]  eta: 0:12:27  loss: 2.9077 (2.6195)  acc1: 25.0000 (41.6667)  acc5: 66.6667 (72.8261)  time: 0.3435  data: 0.1966  max mem: 15572
Test:  [ 170/2442]  eta: 0:12:10  loss: 3.1326 (2.6348)  acc1: 25.0000 (41.3743)  acc5: 66.6667 (72.6608)  time: 0.2816  data: 0.1329  max mem: 15572
Test:  [ 180/2442]  eta: 0:11:59  loss: 2.7597 (2.6511)  acc1: 33.3333 (41.2063)  acc5: 66.6667 (72.1455)  time: 0.2420  data: 0.0874  max mem: 15572
Test:  [ 190/2442]  eta: 0:11:49  loss: 2.1905 (2.6373)  acc1: 41.6667 (41.3613)  acc5: 75.0000 (72.5567)  time: 0.2625  data: 0.1167  max mem: 15572
Test:  [ 200/2442]  eta: 0:11:43  loss: 2.0370 (2.6072)  acc1: 50.0000 (42.3300)  acc5: 91.6667 (73.4245)  time: 0.2771  data: 0.1464  max mem: 15572
Test:  [ 210/2442]  eta: 0:11:39  loss: 2.3411 (2.6311)  acc1: 41.6667 (42.0616)  acc5: 83.3333 (72.7488)  time: 0.2972  data: 0.1601  max mem: 15572
Test:  [ 220/2442]  eta: 0:11:39  loss: 2.7277 (2.6329)  acc1: 33.3333 (41.6667)  acc5: 66.6667 (72.6621)  time: 0.3215  data: 0.1618  max mem: 15572
Test:  [ 230/2442]  eta: 0:11:33  loss: 2.6720 (2.6277)  acc1: 33.3333 (41.8470)  acc5: 75.0000 (73.0520)  time: 0.3110  data: 0.1490  max mem: 15572
Test:  [ 240/2442]  eta: 0:11:28  loss: 2.6210 (2.6277)  acc1: 41.6667 (42.0124)  acc5: 75.0000 (73.2019)  time: 0.2912  data: 0.1502  max mem: 15572
Test:  [ 250/2442]  eta: 0:11:23  loss: 2.6210 (2.6313)  acc1: 41.6667 (41.9655)  acc5: 75.0000 (73.1740)  time: 0.2972  data: 0.1619  max mem: 15572
Test:  [ 260/2442]  eta: 0:11:21  loss: 2.5649 (2.6290)  acc1: 41.6667 (41.8582)  acc5: 66.6667 (73.1801)  time: 0.3092  data: 0.1668  max mem: 15572
Test:  [ 270/2442]  eta: 0:11:14  loss: 2.5571 (2.6304)  acc1: 33.3333 (41.4822)  acc5: 75.0000 (73.3087)  time: 0.2931  data: 0.1477  max mem: 15572
Test:  [ 280/2442]  eta: 0:11:10  loss: 2.6672 (2.6522)  acc1: 25.0000 (40.7177)  acc5: 75.0000 (72.5979)  time: 0.2793  data: 0.1259  max mem: 15572
Test:  [ 290/2442]  eta: 0:11:08  loss: 2.8691 (2.6577)  acc1: 25.0000 (40.4353)  acc5: 66.6667 (72.5086)  time: 0.3109  data: 0.1562  max mem: 15572
Test:  [ 300/2442]  eta: 0:11:00  loss: 2.5620 (2.6585)  acc1: 41.6667 (40.3654)  acc5: 75.0000 (72.4253)  time: 0.2854  data: 0.1323  max mem: 15572
Test:  [ 310/2442]  eta: 0:10:54  loss: 2.2654 (2.6609)  acc1: 41.6667 (40.4073)  acc5: 83.3333 (72.5348)  time: 0.2532  data: 0.0913  max mem: 15572
Test:  [ 320/2442]  eta: 0:10:50  loss: 2.5710 (2.6688)  acc1: 41.6667 (40.2388)  acc5: 75.0000 (72.3001)  time: 0.2794  data: 0.1228  max mem: 15572
Test:  [ 330/2442]  eta: 0:10:48  loss: 2.7117 (2.6636)  acc1: 41.6667 (40.4079)  acc5: 75.0000 (72.4824)  time: 0.3058  data: 0.1712  max mem: 15572
Test:  [ 340/2442]  eta: 0:10:43  loss: 2.5002 (2.6603)  acc1: 50.0000 (40.6892)  acc5: 75.0000 (72.5562)  time: 0.2987  data: 0.1678  max mem: 15572
Test:  [ 350/2442]  eta: 0:10:39  loss: 2.0091 (2.6413)  acc1: 66.6667 (41.4055)  acc5: 91.6667 (73.1007)  time: 0.2862  data: 0.1533  max mem: 15572
Test:  [ 360/2442]  eta: 0:10:37  loss: 2.0507 (2.6320)  acc1: 50.0000 (41.4127)  acc5: 91.6667 (73.4072)  time: 0.3111  data: 0.1761  max mem: 15572
Test:  [ 370/2442]  eta: 0:10:31  loss: 2.7013 (2.6377)  acc1: 33.3333 (41.0827)  acc5: 83.3333 (73.3603)  time: 0.2917  data: 0.1503  max mem: 15572
Test:  [ 380/2442]  eta: 0:10:26  loss: 2.7259 (2.6376)  acc1: 33.3333 (41.1636)  acc5: 75.0000 (73.3596)  time: 0.2597  data: 0.1174  max mem: 15572
Test:  [ 390/2442]  eta: 0:10:20  loss: 1.6197 (2.6044)  acc1: 75.0000 (42.2634)  acc5: 91.6667 (73.9130)  time: 0.2549  data: 0.1185  max mem: 15572
Test:  [ 400/2442]  eta: 0:10:17  loss: 1.8444 (2.5992)  acc1: 66.6667 (42.4771)  acc5: 83.3333 (74.0233)  time: 0.2738  data: 0.1407  max mem: 15572
Test:  [ 410/2442]  eta: 0:10:14  loss: 2.1836 (2.5929)  acc1: 50.0000 (42.7616)  acc5: 83.3333 (74.2092)  time: 0.3019  data: 0.1623  max mem: 15572
Test:  [ 420/2442]  eta: 0:10:09  loss: 2.5848 (2.5946)  acc1: 50.0000 (42.6960)  acc5: 75.0000 (74.2676)  time: 0.2863  data: 0.1444  max mem: 15572
Test:  [ 430/2442]  eta: 0:10:07  loss: 2.5512 (2.5874)  acc1: 50.0000 (43.0201)  acc5: 75.0000 (74.4006)  time: 0.2896  data: 0.1504  max mem: 15572
Test:  [ 440/2442]  eta: 0:10:03  loss: 2.3870 (2.5846)  acc1: 50.0000 (42.9327)  acc5: 83.3333 (74.6599)  time: 0.2912  data: 0.1559  max mem: 15572
Test:  [ 450/2442]  eta: 0:09:59  loss: 2.5015 (2.5831)  acc1: 33.3333 (42.9416)  acc5: 83.3333 (74.7228)  time: 0.2887  data: 0.1423  max mem: 15572
Test:  [ 460/2442]  eta: 0:09:55  loss: 2.8528 (2.5912)  acc1: 33.3333 (42.5524)  acc5: 75.0000 (74.4939)  time: 0.2878  data: 0.1407  max mem: 15572
Test:  [ 470/2442]  eta: 0:09:50  loss: 2.6563 (2.5893)  acc1: 25.0000 (42.4275)  acc5: 75.0000 (74.5931)  time: 0.2613  data: 0.1264  max mem: 15572
Test:  [ 480/2442]  eta: 0:09:46  loss: 2.3434 (2.5809)  acc1: 41.6667 (42.5676)  acc5: 75.0000 (74.7401)  time: 0.2632  data: 0.1244  max mem: 15572
Test:  [ 490/2442]  eta: 0:09:46  loss: 2.0215 (2.5762)  acc1: 50.0000 (42.7189)  acc5: 75.0000 (74.7963)  time: 0.3268  data: 0.1784  max mem: 15572
Test:  [ 500/2442]  eta: 0:09:41  loss: 1.9192 (2.5648)  acc1: 58.3333 (43.0472)  acc5: 91.6667 (75.0166)  time: 0.3068  data: 0.1630  max mem: 15572
Test:  [ 510/2442]  eta: 0:09:38  loss: 1.8381 (2.5524)  acc1: 66.6667 (43.4932)  acc5: 83.3333 (75.1631)  time: 0.2708  data: 0.1364  max mem: 15572
Test:  [ 520/2442]  eta: 0:09:35  loss: 1.9225 (2.5434)  acc1: 66.6667 (43.7620)  acc5: 83.3333 (75.2559)  time: 0.3061  data: 0.1589  max mem: 15572
Test:  [ 530/2442]  eta: 0:09:30  loss: 2.2782 (2.5461)  acc1: 41.6667 (43.6598)  acc5: 83.3333 (75.2354)  time: 0.2708  data: 0.1161  max mem: 15572
Test:  [ 540/2442]  eta: 0:09:26  loss: 2.4100 (2.5420)  acc1: 41.6667 (43.9464)  acc5: 83.3333 (75.4005)  time: 0.2501  data: 0.1047  max mem: 15572
Test:  [ 550/2442]  eta: 0:09:22  loss: 2.3595 (2.5431)  acc1: 58.3333 (43.9050)  acc5: 83.3333 (75.4084)  time: 0.2729  data: 0.1389  max mem: 15572
Test:  [ 560/2442]  eta: 0:09:20  loss: 2.5545 (2.5491)  acc1: 33.3333 (43.7166)  acc5: 75.0000 (75.3268)  time: 0.2934  data: 0.1621  max mem: 15572
Test:  [ 570/2442]  eta: 0:09:16  loss: 2.8934 (2.5595)  acc1: 25.0000 (43.3888)  acc5: 66.6667 (75.1605)  time: 0.2958  data: 0.1527  max mem: 15572
Test:  [ 580/2442]  eta: 0:09:14  loss: 2.7821 (2.5584)  acc1: 33.3333 (43.5600)  acc5: 75.0000 (75.1578)  time: 0.2984  data: 0.1492  max mem: 15572
Test:  [ 590/2442]  eta: 0:09:10  loss: 2.5336 (2.5622)  acc1: 41.6667 (43.4292)  acc5: 75.0000 (75.1269)  time: 0.2960  data: 0.1486  max mem: 15572
Test:  [ 600/2442]  eta: 0:09:08  loss: 2.2736 (2.5582)  acc1: 41.6667 (43.4692)  acc5: 83.3333 (75.2773)  time: 0.2938  data: 0.1460  max mem: 15572
Test:  [ 610/2442]  eta: 0:09:05  loss: 1.9622 (2.5571)  acc1: 50.0000 (43.6307)  acc5: 91.6667 (75.2728)  time: 0.3071  data: 0.1419  max mem: 15572
Test:  [ 620/2442]  eta: 0:09:03  loss: 2.4242 (2.5572)  acc1: 50.0000 (43.5990)  acc5: 75.0000 (75.2147)  time: 0.3162  data: 0.1506  max mem: 15572
Test:  [ 630/2442]  eta: 0:09:00  loss: 2.4407 (2.5538)  acc1: 41.6667 (43.6080)  acc5: 75.0000 (75.3302)  time: 0.3108  data: 0.1590  max mem: 15572
Test:  [ 640/2442]  eta: 0:08:57  loss: 2.4071 (2.5520)  acc1: 41.6667 (43.7598)  acc5: 83.3333 (75.4550)  time: 0.2990  data: 0.1452  max mem: 15572
Test:  [ 650/2442]  eta: 0:08:55  loss: 2.4260 (2.5510)  acc1: 50.0000 (43.8044)  acc5: 75.0000 (75.4608)  time: 0.3137  data: 0.1484  max mem: 15572
Test:  [ 660/2442]  eta: 0:08:51  loss: 2.5193 (2.5514)  acc1: 50.0000 (43.7847)  acc5: 75.0000 (75.4160)  time: 0.3092  data: 0.1428  max mem: 15572
Test:  [ 670/2442]  eta: 0:08:49  loss: 2.3226 (2.5476)  acc1: 50.0000 (43.9146)  acc5: 75.0000 (75.4223)  time: 0.3157  data: 0.1542  max mem: 15572
Test:  [ 680/2442]  eta: 0:08:45  loss: 2.2742 (2.5485)  acc1: 33.3333 (43.6858)  acc5: 83.3333 (75.3671)  time: 0.2917  data: 0.1307  max mem: 15572
Test:  [ 690/2442]  eta: 0:08:42  loss: 2.6571 (2.5603)  acc1: 33.3333 (43.3671)  acc5: 66.6667 (75.0121)  time: 0.2758  data: 0.1201  max mem: 15572
Test:  [ 700/2442]  eta: 0:08:40  loss: 2.6499 (2.5581)  acc1: 25.0000 (43.3072)  acc5: 75.0000 (75.1070)  time: 0.3221  data: 0.1607  max mem: 15572
Test:  [ 710/2442]  eta: 0:08:36  loss: 2.2511 (2.5622)  acc1: 41.6667 (43.1903)  acc5: 83.3333 (74.9883)  time: 0.2867  data: 0.1282  max mem: 15572
Test:  [ 720/2442]  eta: 0:08:33  loss: 2.3780 (2.5633)  acc1: 41.6667 (43.1923)  acc5: 83.3333 (75.0231)  time: 0.2765  data: 0.1083  max mem: 15572
Test:  [ 730/2442]  eta: 0:08:30  loss: 2.7249 (2.5648)  acc1: 41.6667 (43.0689)  acc5: 75.0000 (74.9886)  time: 0.3131  data: 0.1442  max mem: 15572
Test:  [ 740/2442]  eta: 0:08:27  loss: 2.6364 (2.5632)  acc1: 41.6667 (43.1287)  acc5: 75.0000 (75.0225)  time: 0.2990  data: 0.1492  max mem: 15572
Test:  [ 750/2442]  eta: 0:08:24  loss: 2.0388 (2.5558)  acc1: 58.3333 (43.4421)  acc5: 83.3333 (75.1997)  time: 0.2865  data: 0.1394  max mem: 15572
Test:  [ 760/2442]  eta: 0:08:22  loss: 1.9017 (2.5488)  acc1: 58.3333 (43.6049)  acc5: 91.6667 (75.3833)  time: 0.3102  data: 0.1563  max mem: 15572
Test:  [ 770/2442]  eta: 0:08:18  loss: 2.1651 (2.5464)  acc1: 50.0000 (43.5798)  acc5: 83.3333 (75.4648)  time: 0.3011  data: 0.1375  max mem: 15572
Test:  [ 780/2442]  eta: 0:08:15  loss: 2.5378 (2.5497)  acc1: 41.6667 (43.4272)  acc5: 75.0000 (75.4055)  time: 0.2708  data: 0.1114  max mem: 15572
Test:  [ 790/2442]  eta: 0:08:12  loss: 2.4152 (2.5424)  acc1: 41.6667 (43.7105)  acc5: 83.3333 (75.5900)  time: 0.2981  data: 0.1449  max mem: 15572
Test:  [ 800/2442]  eta: 0:08:09  loss: 1.6011 (2.5305)  acc1: 75.0000 (44.0179)  acc5: 91.6667 (75.7595)  time: 0.2998  data: 0.1428  max mem: 15572
Test:  [ 810/2442]  eta: 0:08:06  loss: 1.8317 (2.5250)  acc1: 66.6667 (44.1944)  acc5: 91.6667 (75.8631)  time: 0.3009  data: 0.1378  max mem: 15572
Test:  [ 820/2442]  eta: 0:08:02  loss: 2.0722 (2.5263)  acc1: 58.3333 (44.1636)  acc5: 75.0000 (75.7917)  time: 0.2863  data: 0.1326  max mem: 15572
Test:  [ 830/2442]  eta: 0:08:00  loss: 2.7033 (2.5257)  acc1: 41.6667 (44.2238)  acc5: 75.0000 (75.8022)  time: 0.2991  data: 0.1570  max mem: 15572
Test:  [ 840/2442]  eta: 0:07:57  loss: 2.4103 (2.5247)  acc1: 50.0000 (44.3123)  acc5: 75.0000 (75.8224)  time: 0.3201  data: 0.1683  max mem: 15572
Test:  [ 850/2442]  eta: 0:07:54  loss: 2.3822 (2.5255)  acc1: 50.0000 (44.2225)  acc5: 75.0000 (75.8421)  time: 0.2858  data: 0.1272  max mem: 15572
Test:  [ 860/2442]  eta: 0:07:50  loss: 2.7861 (2.5298)  acc1: 25.0000 (44.0767)  acc5: 66.6667 (75.7259)  time: 0.2750  data: 0.1084  max mem: 15572
Test:  [ 870/2442]  eta: 0:07:48  loss: 3.0190 (2.5361)  acc1: 16.6667 (43.8194)  acc5: 58.3333 (75.4879)  time: 0.2970  data: 0.1217  max mem: 15572
Test:  [ 880/2442]  eta: 0:07:44  loss: 2.7074 (2.5368)  acc1: 33.3333 (43.7760)  acc5: 66.6667 (75.5202)  time: 0.2911  data: 0.1357  max mem: 15572
Test:  [ 890/2442]  eta: 0:07:41  loss: 2.3535 (2.5318)  acc1: 41.6667 (43.8459)  acc5: 83.3333 (75.6547)  time: 0.2622  data: 0.1277  max mem: 15572
Test:  [ 900/2442]  eta: 0:07:39  loss: 2.0719 (2.5310)  acc1: 50.0000 (43.9142)  acc5: 83.3333 (75.6289)  time: 0.3031  data: 0.1566  max mem: 15572
Test:  [ 910/2442]  eta: 0:07:36  loss: 2.2745 (2.5279)  acc1: 50.0000 (43.9901)  acc5: 83.3333 (75.6769)  time: 0.3207  data: 0.1583  max mem: 15572
Test:  [ 920/2442]  eta: 0:07:32  loss: 2.3176 (2.5249)  acc1: 58.3333 (44.1278)  acc5: 75.0000 (75.7058)  time: 0.2875  data: 0.1343  max mem: 15572
Test:  [ 930/2442]  eta: 0:07:30  loss: 2.3282 (2.5236)  acc1: 58.3333 (44.1729)  acc5: 75.0000 (75.7608)  time: 0.3017  data: 0.1461  max mem: 15572
Test:  [ 940/2442]  eta: 0:07:26  loss: 2.3988 (2.5248)  acc1: 50.0000 (44.1729)  acc5: 83.3333 (75.7439)  time: 0.2822  data: 0.1181  max mem: 15572
Test:  [ 950/2442]  eta: 0:07:24  loss: 2.5283 (2.5270)  acc1: 41.6667 (44.2517)  acc5: 75.0000 (75.6923)  time: 0.3125  data: 0.1491  max mem: 15572
Test:  [ 960/2442]  eta: 0:07:21  loss: 2.5465 (2.5293)  acc1: 41.6667 (44.1641)  acc5: 75.0000 (75.6590)  time: 0.3339  data: 0.1588  max mem: 15572
Test:  [ 970/2442]  eta: 0:07:18  loss: 2.9736 (2.5362)  acc1: 25.0000 (43.9924)  acc5: 66.6667 (75.5064)  time: 0.2774  data: 0.0933  max mem: 15572
Test:  [ 980/2442]  eta: 0:07:15  loss: 3.1171 (2.5412)  acc1: 16.6667 (43.8583)  acc5: 66.6667 (75.3993)  time: 0.2935  data: 0.1166  max mem: 15572
Test:  [ 990/2442]  eta: 0:07:12  loss: 2.8737 (2.5459)  acc1: 25.0000 (43.7773)  acc5: 66.6667 (75.2186)  time: 0.2978  data: 0.1257  max mem: 15572
Test:  [1000/2442]  eta: 0:07:08  loss: 2.6995 (2.5457)  acc1: 41.6667 (43.7229)  acc5: 66.6667 (75.2498)  time: 0.2689  data: 0.0999  max mem: 15572
Test:  [1010/2442]  eta: 0:07:07  loss: 2.2904 (2.5454)  acc1: 50.0000 (43.7521)  acc5: 83.3333 (75.2885)  time: 0.3288  data: 0.1637  max mem: 15572
Test:  [1020/2442]  eta: 0:07:02  loss: 2.5581 (2.5490)  acc1: 33.3333 (43.6908)  acc5: 75.0000 (75.1959)  time: 0.2985  data: 0.1355  max mem: 15572
Test:  [1030/2442]  eta: 0:06:58  loss: 2.6687 (2.5495)  acc1: 33.3333 (43.6631)  acc5: 66.6667 (75.1697)  time: 0.1967  data: 0.0460  max mem: 15572
Test:  [1040/2442]  eta: 0:06:56  loss: 2.5358 (2.5488)  acc1: 41.6667 (43.6519)  acc5: 75.0000 (75.2241)  time: 0.2753  data: 0.1205  max mem: 15572
Test:  [1050/2442]  eta: 0:06:52  loss: 2.5713 (2.5493)  acc1: 33.3333 (43.6568)  acc5: 75.0000 (75.2062)  time: 0.2843  data: 0.1411  max mem: 15572
Test:  [1060/2442]  eta: 0:06:49  loss: 2.6425 (2.5506)  acc1: 33.3333 (43.6145)  acc5: 75.0000 (75.2121)  time: 0.2794  data: 0.1400  max mem: 15572
Test:  [1070/2442]  eta: 0:06:46  loss: 2.7844 (2.5542)  acc1: 33.3333 (43.5185)  acc5: 66.6667 (75.0622)  time: 0.3028  data: 0.1517  max mem: 15572
Test:  [1080/2442]  eta: 0:06:43  loss: 2.8380 (2.5525)  acc1: 33.3333 (43.5014)  acc5: 66.6667 (75.1002)  time: 0.2853  data: 0.1328  max mem: 15572
Test:  [1090/2442]  eta: 0:06:41  loss: 2.4326 (2.5585)  acc1: 33.3333 (43.2783)  acc5: 75.0000 (74.9389)  time: 0.3220  data: 0.1668  max mem: 15572
Test:  [1100/2442]  eta: 0:06:38  loss: 2.9043 (2.5626)  acc1: 16.6667 (43.0820)  acc5: 66.6667 (74.8486)  time: 0.3173  data: 0.1622  max mem: 15572
Test:  [1110/2442]  eta: 0:06:35  loss: 2.7247 (2.5602)  acc1: 33.3333 (43.0918)  acc5: 75.0000 (74.9625)  time: 0.3002  data: 0.1431  max mem: 15572
Test:  [1120/2442]  eta: 0:06:32  loss: 2.3183 (2.5632)  acc1: 41.6667 (43.0122)  acc5: 83.3333 (74.8365)  time: 0.3098  data: 0.1591  max mem: 15572
Test:  [1130/2442]  eta: 0:06:29  loss: 2.7558 (2.5667)  acc1: 33.3333 (42.9782)  acc5: 75.0000 (74.7790)  time: 0.3122  data: 0.1626  max mem: 15572
Test:  [1140/2442]  eta: 0:06:26  loss: 2.8335 (2.5676)  acc1: 33.3333 (42.9375)  acc5: 66.6667 (74.7371)  time: 0.2825  data: 0.1371  max mem: 15572
Test:  [1150/2442]  eta: 0:06:23  loss: 2.5979 (2.5698)  acc1: 41.6667 (42.9047)  acc5: 66.6667 (74.6525)  time: 0.2969  data: 0.1509  max mem: 15572
Test:  [1160/2442]  eta: 0:06:20  loss: 2.3507 (2.5640)  acc1: 58.3333 (43.1812)  acc5: 83.3333 (74.7918)  time: 0.3148  data: 0.1542  max mem: 15572
Test:  [1170/2442]  eta: 0:06:17  loss: 2.1388 (2.5619)  acc1: 58.3333 (43.1327)  acc5: 83.3333 (74.8577)  time: 0.2973  data: 0.1367  max mem: 15572
Test:  [1180/2442]  eta: 0:06:15  loss: 2.3831 (2.5612)  acc1: 41.6667 (43.1555)  acc5: 75.0000 (74.8589)  time: 0.3157  data: 0.1631  max mem: 15572
Test:  [1190/2442]  eta: 0:06:11  loss: 2.6697 (2.5647)  acc1: 33.3333 (43.0171)  acc5: 75.0000 (74.7901)  time: 0.2953  data: 0.1500  max mem: 15572
Test:  [1200/2442]  eta: 0:06:09  loss: 2.2142 (2.5569)  acc1: 50.0000 (43.2903)  acc5: 83.3333 (74.9237)  time: 0.2948  data: 0.1565  max mem: 15572
Test:  [1210/2442]  eta: 0:06:06  loss: 1.8574 (2.5528)  acc1: 66.6667 (43.4077)  acc5: 91.6667 (74.9931)  time: 0.3311  data: 0.1925  max mem: 15572
Test:  [1220/2442]  eta: 0:06:03  loss: 2.0819 (2.5503)  acc1: 66.6667 (43.5504)  acc5: 91.6667 (75.0819)  time: 0.3116  data: 0.1725  max mem: 15572
Test:  [1230/2442]  eta: 0:06:00  loss: 2.6487 (2.5517)  acc1: 50.0000 (43.5080)  acc5: 75.0000 (75.0339)  time: 0.3046  data: 0.1669  max mem: 15572
Test:  [1240/2442]  eta: 0:05:57  loss: 2.6324 (2.5504)  acc1: 41.6667 (43.5603)  acc5: 75.0000 (75.0470)  time: 0.3106  data: 0.1755  max mem: 15572
Test:  [1250/2442]  eta: 0:05:54  loss: 2.4669 (2.5515)  acc1: 41.6667 (43.5851)  acc5: 75.0000 (75.0466)  time: 0.2826  data: 0.1463  max mem: 15572
Test:  [1260/2442]  eta: 0:05:51  loss: 2.6663 (2.5516)  acc1: 41.6667 (43.5501)  acc5: 83.3333 (75.0661)  time: 0.2974  data: 0.1579  max mem: 15572
Test:  [1270/2442]  eta: 0:05:48  loss: 2.9681 (2.5568)  acc1: 25.0000 (43.3714)  acc5: 66.6667 (74.9279)  time: 0.3104  data: 0.1764  max mem: 15572
Test:  [1280/2442]  eta: 0:05:45  loss: 2.8850 (2.5580)  acc1: 33.3333 (43.2995)  acc5: 66.6667 (74.8959)  time: 0.2963  data: 0.1659  max mem: 15572
Test:  [1290/2442]  eta: 0:05:43  loss: 2.6245 (2.5585)  acc1: 41.6667 (43.2739)  acc5: 75.0000 (74.9032)  time: 0.3241  data: 0.1952  max mem: 15572
Test:  [1300/2442]  eta: 0:05:40  loss: 2.3718 (2.5540)  acc1: 50.0000 (43.4025)  acc5: 75.0000 (75.0064)  time: 0.3130  data: 0.1781  max mem: 15572
Test:  [1310/2442]  eta: 0:05:37  loss: 2.0645 (2.5541)  acc1: 50.0000 (43.4083)  acc5: 83.3333 (74.9682)  time: 0.2941  data: 0.1545  max mem: 15572
Test:  [1320/2442]  eta: 0:05:34  loss: 2.2601 (2.5523)  acc1: 50.0000 (43.4456)  acc5: 75.0000 (74.9874)  time: 0.3365  data: 0.1910  max mem: 15572
Test:  [1330/2442]  eta: 0:05:31  loss: 2.2208 (2.5498)  acc1: 50.0000 (43.5262)  acc5: 75.0000 (74.9875)  time: 0.3284  data: 0.1687  max mem: 15572
Test:  [1340/2442]  eta: 0:05:28  loss: 2.4834 (2.5510)  acc1: 41.6667 (43.4999)  acc5: 75.0000 (74.9689)  time: 0.2737  data: 0.1167  max mem: 15572
Test:  [1350/2442]  eta: 0:05:25  loss: 2.5653 (2.5502)  acc1: 41.6667 (43.5542)  acc5: 75.0000 (75.0062)  time: 0.2678  data: 0.1173  max mem: 15572
Test:  [1360/2442]  eta: 0:05:22  loss: 2.6001 (2.5526)  acc1: 41.6667 (43.5342)  acc5: 75.0000 (74.9694)  time: 0.3139  data: 0.1430  max mem: 15572
Test:  [1370/2442]  eta: 0:05:19  loss: 2.6001 (2.5533)  acc1: 41.6667 (43.5145)  acc5: 75.0000 (74.9818)  time: 0.2920  data: 0.1194  max mem: 15572
Test:  [1380/2442]  eta: 0:05:16  loss: 2.9296 (2.5596)  acc1: 16.6667 (43.3080)  acc5: 75.0000 (74.8431)  time: 0.2665  data: 0.1063  max mem: 15572
Test:  [1390/2442]  eta: 0:05:12  loss: 3.1479 (2.5623)  acc1: 16.6667 (43.2602)  acc5: 58.3333 (74.7903)  time: 0.2585  data: 0.0856  max mem: 15572
Test:  [1400/2442]  eta: 0:05:10  loss: 2.7944 (2.5648)  acc1: 33.3333 (43.2310)  acc5: 66.6667 (74.7264)  time: 0.3049  data: 0.1375  max mem: 15572
Test:  [1410/2442]  eta: 0:05:07  loss: 2.5627 (2.5636)  acc1: 41.6667 (43.2495)  acc5: 75.0000 (74.7579)  time: 0.3511  data: 0.1924  max mem: 15572
Test:  [1420/2442]  eta: 0:05:04  loss: 2.1379 (2.5598)  acc1: 58.3333 (43.4025)  acc5: 91.6667 (74.8827)  time: 0.2945  data: 0.1387  max mem: 15572
Test:  [1430/2442]  eta: 0:05:01  loss: 2.5505 (2.5643)  acc1: 41.6667 (43.3031)  acc5: 83.3333 (74.7496)  time: 0.2821  data: 0.1293  max mem: 15572
Test:  [1440/2442]  eta: 0:04:58  loss: 2.7424 (2.5643)  acc1: 33.3333 (43.2628)  acc5: 66.6667 (74.7571)  time: 0.2779  data: 0.1298  max mem: 15572
Test:  [1450/2442]  eta: 0:04:55  loss: 2.6390 (2.5644)  acc1: 33.3333 (43.2346)  acc5: 75.0000 (74.7875)  time: 0.2938  data: 0.1449  max mem: 15572
Test:  [1460/2442]  eta: 0:04:52  loss: 2.6138 (2.5652)  acc1: 41.6667 (43.2352)  acc5: 75.0000 (74.7718)  time: 0.2908  data: 0.1220  max mem: 15572
Test:  [1470/2442]  eta: 0:04:49  loss: 2.6652 (2.5661)  acc1: 41.6667 (43.2076)  acc5: 75.0000 (74.7791)  time: 0.3069  data: 0.1424  max mem: 15572
Test:  [1480/2442]  eta: 0:04:46  loss: 2.6877 (2.5655)  acc1: 33.3333 (43.2028)  acc5: 75.0000 (74.7974)  time: 0.3245  data: 0.1676  max mem: 15572
Test:  [1490/2442]  eta: 0:04:43  loss: 2.5699 (2.5667)  acc1: 33.3333 (43.1254)  acc5: 75.0000 (74.7653)  time: 0.3027  data: 0.1343  max mem: 15572
Test:  [1500/2442]  eta: 0:04:40  loss: 2.7455 (2.5712)  acc1: 25.0000 (42.9658)  acc5: 66.6667 (74.6169)  time: 0.2997  data: 0.1294  max mem: 15572
Test:  [1510/2442]  eta: 0:04:37  loss: 2.6767 (2.5730)  acc1: 25.0000 (42.9021)  acc5: 75.0000 (74.5864)  time: 0.2683  data: 0.0981  max mem: 15572
Test:  [1520/2442]  eta: 0:04:34  loss: 2.5868 (2.5727)  acc1: 41.6667 (42.8885)  acc5: 75.0000 (74.6165)  time: 0.2539  data: 0.0951  max mem: 15572
Test:  [1530/2442]  eta: 0:04:31  loss: 2.2963 (2.5737)  acc1: 41.6667 (42.9022)  acc5: 83.3333 (74.5918)  time: 0.2782  data: 0.1330  max mem: 15572
Test:  [1540/2442]  eta: 0:04:28  loss: 2.3387 (2.5764)  acc1: 41.6667 (42.8401)  acc5: 75.0000 (74.5241)  time: 0.3019  data: 0.1500  max mem: 15572
Test:  [1550/2442]  eta: 0:04:25  loss: 2.8708 (2.5770)  acc1: 33.3333 (42.8111)  acc5: 66.6667 (74.5272)  time: 0.3248  data: 0.1720  max mem: 15572
Test:  [1560/2442]  eta: 0:04:22  loss: 2.7108 (2.5772)  acc1: 41.6667 (42.8198)  acc5: 66.6667 (74.5035)  time: 0.3126  data: 0.1662  max mem: 15572
Test:  [1570/2442]  eta: 0:04:19  loss: 2.2393 (2.5737)  acc1: 58.3333 (42.9557)  acc5: 83.3333 (74.6181)  time: 0.2913  data: 0.1304  max mem: 15572
Test:  [1580/2442]  eta: 0:04:16  loss: 2.2227 (2.5716)  acc1: 58.3333 (42.9633)  acc5: 91.6667 (74.6890)  time: 0.2840  data: 0.1236  max mem: 15572
Test:  [1590/2442]  eta: 0:04:13  loss: 2.2899 (2.5729)  acc1: 41.6667 (42.9290)  acc5: 83.3333 (74.6648)  time: 0.2958  data: 0.1255  max mem: 15572
Test:  [1600/2442]  eta: 0:04:10  loss: 2.8921 (2.5743)  acc1: 33.3333 (42.9159)  acc5: 66.6667 (74.6356)  time: 0.2994  data: 0.1089  max mem: 15572
Test:  [1610/2442]  eta: 0:04:07  loss: 1.8350 (2.5670)  acc1: 66.6667 (43.1461)  acc5: 91.6667 (74.7672)  time: 0.2529  data: 0.0782  max mem: 15572
Test:  [1620/2442]  eta: 0:04:04  loss: 1.8459 (2.5661)  acc1: 58.3333 (43.1832)  acc5: 83.3333 (74.7635)  time: 0.2603  data: 0.0955  max mem: 15572
Test:  [1630/2442]  eta: 0:04:01  loss: 2.1877 (2.5643)  acc1: 50.0000 (43.2710)  acc5: 83.3333 (74.8161)  time: 0.2930  data: 0.1286  max mem: 15572
Test:  [1640/2442]  eta: 0:03:58  loss: 2.6139 (2.5657)  acc1: 33.3333 (43.1952)  acc5: 75.0000 (74.7867)  time: 0.3060  data: 0.1457  max mem: 15572
Test:  [1650/2442]  eta: 0:03:55  loss: 2.6441 (2.5644)  acc1: 41.6667 (43.2718)  acc5: 75.0000 (74.8132)  time: 0.2806  data: 0.1196  max mem: 15572
Test:  [1660/2442]  eta: 0:03:52  loss: 2.3990 (2.5643)  acc1: 50.0000 (43.2470)  acc5: 75.0000 (74.8395)  time: 0.2769  data: 0.1107  max mem: 15572
Test:  [1670/2442]  eta: 0:03:49  loss: 2.3990 (2.5639)  acc1: 41.6667 (43.2825)  acc5: 75.0000 (74.8554)  time: 0.2866  data: 0.1146  max mem: 15572
Test:  [1680/2442]  eta: 0:03:46  loss: 2.8647 (2.5666)  acc1: 33.3333 (43.1489)  acc5: 66.6667 (74.7819)  time: 0.2959  data: 0.1285  max mem: 15572
Test:  [1690/2442]  eta: 0:03:43  loss: 2.6918 (2.5661)  acc1: 33.3333 (43.1303)  acc5: 75.0000 (74.8177)  time: 0.2965  data: 0.1416  max mem: 15572
Test:  [1700/2442]  eta: 0:03:40  loss: 2.3604 (2.5642)  acc1: 41.6667 (43.1854)  acc5: 83.3333 (74.8432)  time: 0.2760  data: 0.1201  max mem: 15572
Test:  [1710/2442]  eta: 0:03:37  loss: 2.3123 (2.5629)  acc1: 58.3333 (43.2350)  acc5: 83.3333 (74.8685)  time: 0.2937  data: 0.1417  max mem: 15572
Test:  [1720/2442]  eta: 0:03:34  loss: 1.9293 (2.5599)  acc1: 58.3333 (43.3130)  acc5: 83.3333 (74.9274)  time: 0.2966  data: 0.1260  max mem: 15572
Test:  [1730/2442]  eta: 0:03:31  loss: 1.9137 (2.5566)  acc1: 58.3333 (43.4287)  acc5: 83.3333 (74.9711)  time: 0.2848  data: 0.1069  max mem: 15572
Test:  [1740/2442]  eta: 0:03:28  loss: 2.0027 (2.5535)  acc1: 58.3333 (43.5238)  acc5: 83.3333 (75.0144)  time: 0.2815  data: 0.1211  max mem: 15572
Test:  [1750/2442]  eta: 0:03:25  loss: 2.3324 (2.5545)  acc1: 50.0000 (43.4942)  acc5: 75.0000 (74.9952)  time: 0.2759  data: 0.1096  max mem: 15572
Test:  [1760/2442]  eta: 0:03:22  loss: 2.4772 (2.5531)  acc1: 50.0000 (43.5832)  acc5: 83.3333 (75.0568)  time: 0.2980  data: 0.1393  max mem: 15572
Test:  [1770/2442]  eta: 0:03:18  loss: 2.3444 (2.5537)  acc1: 50.0000 (43.5677)  acc5: 83.3333 (75.0518)  time: 0.2750  data: 0.1249  max mem: 15572
Test:  [1780/2442]  eta: 0:03:16  loss: 2.5650 (2.5548)  acc1: 41.6667 (43.5289)  acc5: 75.0000 (75.0515)  time: 0.2810  data: 0.1357  max mem: 15572
Test:  [1790/2442]  eta: 0:03:13  loss: 2.9161 (2.5585)  acc1: 16.6667 (43.3929)  acc5: 66.6667 (74.9721)  time: 0.3207  data: 0.1803  max mem: 15572
Test:  [1800/2442]  eta: 0:03:10  loss: 2.9161 (2.5582)  acc1: 25.0000 (43.4296)  acc5: 66.6667 (74.9815)  time: 0.3006  data: 0.1563  max mem: 15572
Test:  [1810/2442]  eta: 0:03:07  loss: 2.4978 (2.5599)  acc1: 41.6667 (43.4060)  acc5: 75.0000 (74.9586)  time: 0.2945  data: 0.1502  max mem: 15572
Test:  [1820/2442]  eta: 0:03:04  loss: 2.3985 (2.5585)  acc1: 41.6667 (43.4148)  acc5: 83.3333 (75.0046)  time: 0.2933  data: 0.1512  max mem: 15572
Test:  [1830/2442]  eta: 0:03:01  loss: 2.0034 (2.5576)  acc1: 50.0000 (43.4963)  acc5: 91.6667 (75.0091)  time: 0.2706  data: 0.1333  max mem: 15572
Test:  [1840/2442]  eta: 0:02:58  loss: 2.5659 (2.5583)  acc1: 41.6667 (43.4682)  acc5: 66.6667 (74.9683)  time: 0.2837  data: 0.1388  max mem: 15572
Test:  [1850/2442]  eta: 0:02:55  loss: 2.6250 (2.5575)  acc1: 33.3333 (43.4450)  acc5: 75.0000 (75.0090)  time: 0.2812  data: 0.1311  max mem: 15572
Test:  [1860/2442]  eta: 0:02:52  loss: 2.5196 (2.5572)  acc1: 41.6667 (43.4623)  acc5: 75.0000 (75.0313)  time: 0.2489  data: 0.1074  max mem: 15572
Test:  [1870/2442]  eta: 0:02:49  loss: 2.4359 (2.5563)  acc1: 50.0000 (43.5061)  acc5: 75.0000 (75.0668)  time: 0.2907  data: 0.1514  max mem: 15572
Test:  [1880/2442]  eta: 0:02:46  loss: 2.4359 (2.5567)  acc1: 41.6667 (43.4698)  acc5: 75.0000 (75.0532)  time: 0.3013  data: 0.1622  max mem: 15572
Test:  [1890/2442]  eta: 0:02:43  loss: 2.5108 (2.5554)  acc1: 33.3333 (43.4867)  acc5: 75.0000 (75.0749)  time: 0.2928  data: 0.1512  max mem: 15572
Test:  [1900/2442]  eta: 0:02:40  loss: 2.3503 (2.5551)  acc1: 33.3333 (43.4508)  acc5: 75.0000 (75.0701)  time: 0.2875  data: 0.1435  max mem: 15572
Test:  [1910/2442]  eta: 0:02:37  loss: 2.5437 (2.5590)  acc1: 33.3333 (43.3673)  acc5: 75.0000 (74.9608)  time: 0.2779  data: 0.1396  max mem: 15572
Test:  [1920/2442]  eta: 0:02:34  loss: 2.6140 (2.5595)  acc1: 33.3333 (43.2891)  acc5: 75.0000 (74.9566)  time: 0.2870  data: 0.1505  max mem: 15572
Test:  [1930/2442]  eta: 0:02:31  loss: 2.2868 (2.5608)  acc1: 33.3333 (43.2591)  acc5: 83.3333 (74.9051)  time: 0.2844  data: 0.1289  max mem: 15572
Test:  [1940/2442]  eta: 0:02:28  loss: 2.2476 (2.5602)  acc1: 41.6667 (43.2810)  acc5: 83.3333 (74.9270)  time: 0.2323  data: 0.0844  max mem: 15572
Test:  [1950/2442]  eta: 0:02:24  loss: 2.4886 (2.5611)  acc1: 33.3333 (43.2257)  acc5: 75.0000 (74.9060)  time: 0.1829  data: 0.0642  max mem: 15572
Test:  [1960/2442]  eta: 0:02:21  loss: 2.5668 (2.5610)  acc1: 41.6667 (43.2390)  acc5: 66.6667 (74.9065)  time: 0.1897  data: 0.0778  max mem: 15572
Test:  [1970/2442]  eta: 0:02:18  loss: 2.2065 (2.5589)  acc1: 58.3333 (43.3536)  acc5: 83.3333 (74.9535)  time: 0.1652  data: 0.0456  max mem: 15572
Test:  [1980/2442]  eta: 0:02:14  loss: 1.9148 (2.5561)  acc1: 66.6667 (43.4503)  acc5: 91.6667 (75.0294)  time: 0.1238  data: 0.0004  max mem: 15572
Test:  [1990/2442]  eta: 0:02:11  loss: 2.1985 (2.5548)  acc1: 50.0000 (43.4455)  acc5: 83.3333 (75.0502)  time: 0.1274  data: 0.0005  max mem: 15572
Test:  [2000/2442]  eta: 0:02:08  loss: 2.5924 (2.5557)  acc1: 33.3333 (43.4116)  acc5: 75.0000 (75.0458)  time: 0.1299  data: 0.0004  max mem: 15572
Test:  [2010/2442]  eta: 0:02:05  loss: 2.3328 (2.5541)  acc1: 50.0000 (43.5024)  acc5: 83.3333 (75.0912)  time: 0.1180  data: 0.0003  max mem: 15572
Test:  [2020/2442]  eta: 0:02:01  loss: 1.6582 (2.5491)  acc1: 75.0000 (43.6335)  acc5: 91.6667 (75.1732)  time: 0.1474  data: 0.0337  max mem: 15572
Test:  [2030/2442]  eta: 0:01:58  loss: 1.7355 (2.5472)  acc1: 66.6667 (43.7059)  acc5: 91.6667 (75.2216)  time: 0.1608  data: 0.0420  max mem: 15572
Test:  [2040/2442]  eta: 0:01:55  loss: 2.1337 (2.5474)  acc1: 50.0000 (43.7122)  acc5: 83.3333 (75.2082)  time: 0.1225  data: 0.0087  max mem: 15572
Test:  [2050/2442]  eta: 0:01:52  loss: 2.6508 (2.5477)  acc1: 50.0000 (43.7185)  acc5: 75.0000 (75.1950)  time: 0.1438  data: 0.0289  max mem: 15572
Test:  [2060/2442]  eta: 0:01:49  loss: 2.4303 (2.5474)  acc1: 50.0000 (43.7530)  acc5: 75.0000 (75.2062)  time: 0.1631  data: 0.0341  max mem: 15572
Test:  [2070/2442]  eta: 0:01:46  loss: 2.5513 (2.5482)  acc1: 41.6667 (43.7148)  acc5: 66.6667 (75.1891)  time: 0.1428  data: 0.0116  max mem: 15572
Test:  [2080/2442]  eta: 0:01:43  loss: 2.8089 (2.5495)  acc1: 25.0000 (43.6649)  acc5: 66.6667 (75.1442)  time: 0.1536  data: 0.0211  max mem: 15572
Test:  [2090/2442]  eta: 0:01:40  loss: 2.9174 (2.5522)  acc1: 25.0000 (43.5597)  acc5: 58.3333 (75.0678)  time: 0.1502  data: 0.0210  max mem: 15572
Test:  [2100/2442]  eta: 0:01:36  loss: 2.7997 (2.5521)  acc1: 33.3333 (43.5507)  acc5: 66.6667 (75.0912)  time: 0.1401  data: 0.0065  max mem: 15572
Test:  [2110/2442]  eta: 0:01:33  loss: 2.3324 (2.5504)  acc1: 41.6667 (43.5694)  acc5: 83.3333 (75.1382)  time: 0.1395  data: 0.0006  max mem: 15572
Test:  [2120/2442]  eta: 0:01:30  loss: 2.1509 (2.5503)  acc1: 50.0000 (43.5919)  acc5: 83.3333 (75.1179)  time: 0.1344  data: 0.0088  max mem: 15572
Test:  [2130/2442]  eta: 0:01:27  loss: 2.2156 (2.5489)  acc1: 58.3333 (43.6258)  acc5: 75.0000 (75.1369)  time: 0.1438  data: 0.0151  max mem: 15572
Test:  [2140/2442]  eta: 0:01:24  loss: 2.3747 (2.5479)  acc1: 50.0000 (43.6673)  acc5: 75.0000 (75.1401)  time: 0.1398  data: 0.0069  max mem: 15572
Test:  [2150/2442]  eta: 0:01:21  loss: 2.3457 (2.5470)  acc1: 50.0000 (43.6967)  acc5: 75.0000 (75.1627)  time: 0.1353  data: 0.0006  max mem: 15572
Test:  [2160/2442]  eta: 0:01:18  loss: 2.5958 (2.5477)  acc1: 41.6667 (43.6719)  acc5: 83.3333 (75.1427)  time: 0.1420  data: 0.0007  max mem: 15572
Test:  [2170/2442]  eta: 0:01:15  loss: 2.6063 (2.5475)  acc1: 41.6667 (43.7126)  acc5: 75.0000 (75.1497)  time: 0.1420  data: 0.0007  max mem: 15572
Test:  [2180/2442]  eta: 0:01:12  loss: 2.5930 (2.5493)  acc1: 41.6667 (43.6497)  acc5: 75.0000 (75.1261)  time: 0.1442  data: 0.0006  max mem: 15572
Test:  [2190/2442]  eta: 0:01:09  loss: 2.7991 (2.5518)  acc1: 25.0000 (43.5684)  acc5: 66.6667 (75.0761)  time: 0.1348  data: 0.0005  max mem: 15572
Test:  [2200/2442]  eta: 0:01:07  loss: 3.1657 (2.5541)  acc1: 25.0000 (43.5030)  acc5: 58.3333 (75.0265)  time: 0.1192  data: 0.0004  max mem: 15572
Test:  [2210/2442]  eta: 0:01:04  loss: 3.0411 (2.5564)  acc1: 33.3333 (43.4570)  acc5: 58.3333 (74.9623)  time: 0.1398  data: 0.0201  max mem: 15572
Test:  [2220/2442]  eta: 0:01:01  loss: 2.8831 (2.5567)  acc1: 33.3333 (43.4414)  acc5: 66.6667 (74.9587)  time: 0.1513  data: 0.0304  max mem: 15572
Test:  [2230/2442]  eta: 0:00:58  loss: 2.3985 (2.5567)  acc1: 50.0000 (43.4559)  acc5: 83.3333 (74.9813)  time: 0.1278  data: 0.0107  max mem: 15572
Test:  [2240/2442]  eta: 0:00:55  loss: 2.3985 (2.5578)  acc1: 41.6667 (43.4553)  acc5: 83.3333 (74.9479)  time: 0.1284  data: 0.0077  max mem: 15572
Test:  [2250/2442]  eta: 0:00:52  loss: 2.8197 (2.5582)  acc1: 33.3333 (43.4437)  acc5: 66.6667 (74.9260)  time: 0.1432  data: 0.0212  max mem: 15572
Test:  [2260/2442]  eta: 0:00:49  loss: 2.5332 (2.5582)  acc1: 33.3333 (43.4211)  acc5: 75.0000 (74.9410)  time: 0.1452  data: 0.0162  max mem: 15572
Test:  [2270/2442]  eta: 0:00:46  loss: 2.6947 (2.5585)  acc1: 41.6667 (43.4464)  acc5: 66.6667 (74.9303)  time: 0.1352  data: 0.0027  max mem: 15572
Test:  [2280/2442]  eta: 0:00:44  loss: 2.6218 (2.5590)  acc1: 41.6667 (43.4203)  acc5: 66.6667 (74.9415)  time: 0.1407  data: 0.0137  max mem: 15572
Test:  [2290/2442]  eta: 0:00:41  loss: 2.8722 (2.5608)  acc1: 33.3333 (43.3799)  acc5: 66.6667 (74.8763)  time: 0.1413  data: 0.0173  max mem: 15572
Test:  [2300/2442]  eta: 0:00:38  loss: 2.8910 (2.5601)  acc1: 41.6667 (43.3833)  acc5: 66.6667 (74.9058)  time: 0.1420  data: 0.0041  max mem: 15572
Test:  [2310/2442]  eta: 0:00:35  loss: 2.4126 (2.5617)  acc1: 33.3333 (43.3002)  acc5: 75.0000 (74.8486)  time: 0.1419  data: 0.0005  max mem: 15572
Test:  [2320/2442]  eta: 0:00:32  loss: 2.5994 (2.5641)  acc1: 16.6667 (43.2321)  acc5: 66.6667 (74.7846)  time: 0.1275  data: 0.0004  max mem: 15572
Test:  [2330/2442]  eta: 0:00:30  loss: 2.5337 (2.5635)  acc1: 41.6667 (43.2254)  acc5: 75.0000 (74.8105)  time: 0.1384  data: 0.0066  max mem: 15572
Test:  [2340/2442]  eta: 0:00:27  loss: 2.3050 (2.5649)  acc1: 41.6667 (43.1902)  acc5: 75.0000 (74.7544)  time: 0.1463  data: 0.0067  max mem: 15572
Test:  [2350/2442]  eta: 0:00:24  loss: 2.5406 (2.5661)  acc1: 41.6667 (43.1873)  acc5: 75.0000 (74.7448)  time: 0.1428  data: 0.0006  max mem: 15572
Test:  [2360/2442]  eta: 0:00:21  loss: 2.8264 (2.5673)  acc1: 25.0000 (43.1279)  acc5: 66.6667 (74.7106)  time: 0.1463  data: 0.0007  max mem: 15572
Test:  [2370/2442]  eta: 0:00:19  loss: 2.7574 (2.5685)  acc1: 33.3333 (43.0866)  acc5: 58.3333 (74.6696)  time: 0.1578  data: 0.0008  max mem: 15572
Test:  [2380/2442]  eta: 0:00:16  loss: 2.2479 (2.5659)  acc1: 50.0000 (43.2136)  acc5: 83.3333 (74.7410)  time: 0.1558  data: 0.0009  max mem: 15572
Test:  [2390/2442]  eta: 0:00:13  loss: 2.1217 (2.5650)  acc1: 50.0000 (43.2037)  acc5: 91.6667 (74.7804)  time: 0.1579  data: 0.0160  max mem: 15572
Test:  [2400/2442]  eta: 0:00:11  loss: 2.3663 (2.5643)  acc1: 41.6667 (43.2250)  acc5: 83.3333 (74.7987)  time: 0.1628  data: 0.0382  max mem: 15572
Test:  [2410/2442]  eta: 0:00:08  loss: 2.5500 (2.5665)  acc1: 25.0000 (43.1356)  acc5: 83.3333 (74.7684)  time: 0.1544  data: 0.0280  max mem: 15572
Test:  [2420/2442]  eta: 0:00:05  loss: 2.3396 (2.5634)  acc1: 58.3333 (43.2672)  acc5: 83.3333 (74.8210)  time: 0.1504  data: 0.0070  max mem: 15572
Test:  [2430/2442]  eta: 0:00:03  loss: 1.8563 (2.5613)  acc1: 66.6667 (43.3189)  acc5: 83.3333 (74.8629)  time: 0.1273  data: 0.0017  max mem: 15572
Test:  [2440/2442]  eta: 0:00:00  loss: 2.1103 (2.5598)  acc1: 58.3333 (43.3839)  acc5: 83.3333 (74.9112)  time: 0.1025  data: 0.0002  max mem: 15572
Test:  [2441/2442]  eta: 0:00:00  loss: 2.1322 (2.5603)  acc1: 58.3333 (43.3784)  acc5: 83.3333 (74.9096)  time: 0.1001  data: 0.0001  max mem: 15572
Test: Total time: 0:10:44 (0.2639 s / it)
* Acc@1 43.378 Acc@5 74.910 loss 2.560
Start merging results...
Reading individual output files
Computing final results
4883
Accuracy of the network on the 29298 test videos: Top-1: 46.57%, Top-5: 77.72%
Training time 19:09:18
