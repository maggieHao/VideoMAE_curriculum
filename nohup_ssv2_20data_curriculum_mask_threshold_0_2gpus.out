/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-16 20:13:49,212] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-16 20:13:49,214] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 1): env://, gpu 1| distributed init (rank 0): env://, gpu 0

Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=2, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, mask_curriculum_threshold=0, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fd3e8a503d0>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00009375
Batch size = 24
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 1404
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
[2025-01-16 20:13:52,398] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-16 20:13:52,398] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-16 20:13:52,398] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-16 20:13:52,398] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-16 20:13:52,432] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.10169625282287598 seconds
[2025-01-16 20:13:52,721] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-16 20:13:52,721] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-16 20:13:52,723] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-16 20:13:52,723] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-16 20:13:52,729] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-16 20:13:52,729] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-16 20:13:52,729] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-16 20:13:52,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 20:13:52,730] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd3e80bae60>
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-16 20:13:52,730] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   train_batch_size ............. 24
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   world_size ................... 2
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-16 20:13:52,731] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-16 20:13:52,731] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 24, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 7020
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train curriculum learning mask.
Mask curriculum threshold = 0
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2311239 closing signal SIGHUP
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2311240 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2311233 got signal: 1
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-16 20:13:58,207] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-16 20:13:58,208] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=2, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, mask_curriculum_threshold=0, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f06f16083d0>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
[2025-01-16 20:14:00,927] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-16 20:14:00,927] [INFO] [comm.py:637:init_distributed] cdb=None
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00009375
Batch size = 24
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 1404
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-16 20:14:00,949] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-16 20:14:00,949] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-16 20:14:00,991] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.10134339332580566 seconds
[2025-01-16 20:14:01,254] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-16 20:14:01,255] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-16 20:14:01,257] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-16 20:14:01,257] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-16 20:14:01,262] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-16 20:14:01,263] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-16 20:14:01,263] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-16 20:14:01,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 20:14:01,263] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f06f0c9ae60>
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-16 20:14:01,263] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   train_batch_size ............. 24
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   world_size ................... 2
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-16 20:14:01,264] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-16 20:14:01,264] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 24, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 7020
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train curriculum learning mask.
Mask curriculum threshold = 0
Epoch: [0]  [   0/1404]  eta: 5:26:55  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  class_acc: 0.0000 (0.0000)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 13.9714  data: 5.2221  max mem: 15572
Epoch: [0]  [  10/1404]  eta: 0:38:20  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0038)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6501  data: 0.4752  max mem: 15572
Epoch: [0]  [  20/1404]  eta: 0:25:07  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  class_acc: 0.0000 (0.0020)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4454  data: 0.0005  max mem: 15572
Epoch: [0]  [  30/1404]  eta: 0:20:38  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  class_acc: 0.0000 (0.0054)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4898  data: 0.0007  max mem: 15572
Epoch: [0]  [  40/1404]  eta: 0:18:04  lr: 0.000001  min_lr: 0.000000  loss: 5.1600 (5.1600)  class_acc: 0.0000 (0.0071)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4858  data: 0.0009  max mem: 15572
Epoch: [0]  [  50/1404]  eta: 0:16:32  lr: 0.000001  min_lr: 0.000000  loss: 5.1593 (5.1598)  class_acc: 0.0000 (0.0106)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4712  data: 0.0007  max mem: 15572
Epoch: [0]  [  60/1404]  eta: 0:16:00  lr: 0.000001  min_lr: 0.000000  loss: 5.1589 (5.1597)  class_acc: 0.0000 (0.0123)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.0646  max mem: 15572
Epoch: [0]  [  70/1404]  eta: 0:15:48  lr: 0.000001  min_lr: 0.000000  loss: 5.1586 (5.1595)  class_acc: 0.0000 (0.0135)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6553  data: 0.0963  max mem: 15572
Epoch: [0]  [  80/1404]  eta: 0:15:23  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1593)  class_acc: 0.0000 (0.0134)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.0323  max mem: 15572
Epoch: [0]  [  90/1404]  eta: 0:14:59  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1592)  class_acc: 0.0000 (0.0133)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0317  max mem: 15572
Epoch: [0]  [ 100/1404]  eta: 0:14:52  lr: 0.000001  min_lr: 0.000000  loss: 5.1583 (5.1591)  class_acc: 0.0000 (0.0153)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.0791  max mem: 15572
Epoch: [0]  [ 110/1404]  eta: 0:14:37  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1590)  class_acc: 0.0000 (0.0146)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.0481  max mem: 15572
Epoch: [0]  [ 120/1404]  eta: 0:14:25  lr: 0.000002  min_lr: 0.000000  loss: 5.1573 (5.1588)  class_acc: 0.0000 (0.0152)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.0045  max mem: 15572
[2025-01-16 20:15:27,927] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:15:27,928] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
[2025-01-16 20:15:27,983] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:15:27,984] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/1404]  eta: 0:14:10  lr: 0.000002  min_lr: 0.000000  loss: 5.1569 (5.1587)  class_acc: 0.0000 (0.0162)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.0553  max mem: 15572
Epoch: [0]  [ 140/1404]  eta: 0:13:57  lr: 0.000002  min_lr: 0.000000  loss: 5.1563 (5.1585)  class_acc: 0.0000 (0.0160)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.0920  max mem: 15572
Epoch: [0]  [ 150/1404]  eta: 0:13:48  lr: 0.000002  min_lr: 0.000000  loss: 5.1553 (5.1582)  class_acc: 0.0000 (0.0166)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0978  max mem: 15572
Epoch: [0]  [ 160/1404]  eta: 0:13:44  lr: 0.000002  min_lr: 0.000000  loss: 5.1552 (5.1580)  class_acc: 0.0000 (0.0158)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.0500 (0.0500)  time: 0.6655  data: 0.1612  max mem: 15572
Epoch: [0]  [ 170/1404]  eta: 0:13:26  lr: 0.000002  min_lr: 0.000000  loss: 5.1546 (5.1578)  class_acc: 0.0000 (0.0154)  loss_scale: 256.0000 (160.1871)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1143  max mem: 15572
Epoch: [0]  [ 180/1404]  eta: 0:13:19  lr: 0.000002  min_lr: 0.000000  loss: 5.1540 (5.1576)  class_acc: 0.0000 (0.0154)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0627  max mem: 15572
Epoch: [0]  [ 190/1404]  eta: 0:13:09  lr: 0.000003  min_lr: 0.000000  loss: 5.1534 (5.1574)  class_acc: 0.0000 (0.0155)  loss_scale: 256.0000 (170.2199)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.0528  max mem: 15572
Epoch: [0]  [ 200/1404]  eta: 0:12:54  lr: 0.000003  min_lr: 0.000000  loss: 5.1518 (5.1570)  class_acc: 0.0000 (0.0170)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0006  max mem: 15572
Epoch: [0]  [ 210/1404]  eta: 0:12:49  lr: 0.000003  min_lr: 0.000000  loss: 5.1512 (5.1567)  class_acc: 0.0000 (0.0174)  loss_scale: 256.0000 (178.3507)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0414  max mem: 15572
Epoch: [0]  [ 220/1404]  eta: 0:12:40  lr: 0.000003  min_lr: 0.000000  loss: 5.1488 (5.1564)  class_acc: 0.0000 (0.0168)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.0550  max mem: 15572
Epoch: [0]  [ 230/1404]  eta: 0:12:32  lr: 0.000003  min_lr: 0.000000  loss: 5.1480 (5.1560)  class_acc: 0.0000 (0.0171)  loss_scale: 256.0000 (185.0736)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0698  max mem: 15572
Epoch: [0]  [ 240/1404]  eta: 0:12:29  lr: 0.000003  min_lr: 0.000000  loss: 5.1440 (5.1555)  class_acc: 0.0000 (0.0175)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.1667  max mem: 15572
Epoch: [0]  [ 250/1404]  eta: 0:12:22  lr: 0.000003  min_lr: 0.000000  loss: 5.1430 (5.1550)  class_acc: 0.0000 (0.0174)  loss_scale: 256.0000 (190.7251)  weight_decay: 0.0500 (0.0500)  time: 0.6695  data: 0.1812  max mem: 15572
[2025-01-16 20:16:47,129] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:16:47,129] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
[2025-01-16 20:16:47,131] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:16:47,131] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [ 260/1404]  eta: 0:12:15  lr: 0.000003  min_lr: 0.000000  loss: 5.1401 (5.1544)  class_acc: 0.0000 (0.0172)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1391  max mem: 15572
Epoch: [0]  [ 270/1404]  eta: 0:12:08  lr: 0.000004  min_lr: 0.000000  loss: 5.1393 (5.1538)  class_acc: 0.0000 (0.0172)  loss_scale: 512.0000 (209.7122)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1158  max mem: 15572
Epoch: [0]  [ 280/1404]  eta: 0:12:01  lr: 0.000004  min_lr: 0.000000  loss: 5.1372 (5.1534)  class_acc: 0.0000 (0.0166)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1119  max mem: 15572
Epoch: [0]  [ 290/1404]  eta: 0:11:53  lr: 0.000004  min_lr: 0.000000  loss: 5.1423 (5.1530)  class_acc: 0.0000 (0.0169)  loss_scale: 512.0000 (230.4880)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1157  max mem: 15572
Epoch: [0]  [ 300/1404]  eta: 0:11:45  lr: 0.000004  min_lr: 0.000000  loss: 5.1327 (5.1522)  class_acc: 0.0000 (0.0177)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0925  max mem: 15572
Epoch: [0]  [ 310/1404]  eta: 0:11:35  lr: 0.000004  min_lr: 0.000000  loss: 5.1288 (5.1516)  class_acc: 0.0000 (0.0177)  loss_scale: 512.0000 (248.5916)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0806  max mem: 15572
Epoch: [0]  [ 320/1404]  eta: 0:11:29  lr: 0.000004  min_lr: 0.000000  loss: 5.1310 (5.1509)  class_acc: 0.0000 (0.0173)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1031  max mem: 15572
Epoch: [0]  [ 330/1404]  eta: 0:11:21  lr: 0.000004  min_lr: 0.000000  loss: 5.1309 (5.1502)  class_acc: 0.0000 (0.0175)  loss_scale: 512.0000 (264.5076)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1122  max mem: 15572
Epoch: [0]  [ 340/1404]  eta: 0:11:16  lr: 0.000005  min_lr: 0.000000  loss: 5.1157 (5.1491)  class_acc: 0.0000 (0.0180)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1065  max mem: 15572
Epoch: [0]  [ 350/1404]  eta: 0:11:09  lr: 0.000005  min_lr: 0.000000  loss: 5.1157 (5.1484)  class_acc: 0.0000 (0.0175)  loss_scale: 512.0000 (278.6097)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1240  max mem: 15572
Epoch: [0]  [ 360/1404]  eta: 0:11:00  lr: 0.000005  min_lr: 0.000000  loss: 5.1194 (5.1475)  class_acc: 0.0000 (0.0179)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0796  max mem: 15572
Epoch: [0]  [ 370/1404]  eta: 0:10:52  lr: 0.000005  min_lr: 0.000000  loss: 5.1133 (5.1466)  class_acc: 0.0000 (0.0179)  loss_scale: 512.0000 (291.1914)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0474  max mem: 15572
Epoch: [0]  [ 380/1404]  eta: 0:10:45  lr: 0.000005  min_lr: 0.000000  loss: 5.1026 (5.1454)  class_acc: 0.0000 (0.0176)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.0894  max mem: 15572
[2025-01-16 20:18:04,941] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:18:04,941] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2025-01-16 20:18:04,986] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:18:04,987] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [ 390/1404]  eta: 0:10:38  lr: 0.000005  min_lr: 0.000000  loss: 5.1035 (5.1445)  class_acc: 0.0000 (0.0180)  loss_scale: 512.0000 (311.6522)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.0955  max mem: 15572
Epoch: [0]  [ 400/1404]  eta: 0:10:32  lr: 0.000005  min_lr: 0.000000  loss: 5.1051 (5.1436)  class_acc: 0.0000 (0.0178)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.0975  max mem: 15572
Epoch: [0]  [ 410/1404]  eta: 0:10:26  lr: 0.000005  min_lr: 0.000000  loss: 5.1072 (5.1428)  class_acc: 0.0000 (0.0177)  loss_scale: 1024.0000 (346.3163)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1255  max mem: 15572
Epoch: [0]  [ 420/1404]  eta: 0:10:17  lr: 0.000006  min_lr: 0.000000  loss: 5.1072 (5.1417)  class_acc: 0.0000 (0.0177)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0953  max mem: 15572
Epoch: [0]  [ 430/1404]  eta: 0:10:11  lr: 0.000006  min_lr: 0.000000  loss: 5.0894 (5.1406)  class_acc: 0.0000 (0.0178)  loss_scale: 1024.0000 (377.7633)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0831  max mem: 15572
Epoch: [0]  [ 440/1404]  eta: 0:10:05  lr: 0.000006  min_lr: 0.000000  loss: 5.0894 (5.1397)  class_acc: 0.0000 (0.0183)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1256  max mem: 15572
Epoch: [0]  [ 450/1404]  eta: 0:10:00  lr: 0.000006  min_lr: 0.000000  loss: 5.0972 (5.1389)  class_acc: 0.0000 (0.0187)  loss_scale: 1024.0000 (406.4213)  weight_decay: 0.0500 (0.0500)  time: 0.6729  data: 0.1908  max mem: 15572
Epoch: [0]  [ 460/1404]  eta: 0:09:51  lr: 0.000006  min_lr: 0.000000  loss: 5.1002 (5.1382)  class_acc: 0.0000 (0.0185)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1342  max mem: 15572
Epoch: [0]  [ 470/1404]  eta: 0:09:46  lr: 0.000006  min_lr: 0.000000  loss: 5.1002 (5.1374)  class_acc: 0.0000 (0.0187)  loss_scale: 1024.0000 (432.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1172  max mem: 15572
Epoch: [0]  [ 480/1404]  eta: 0:09:38  lr: 0.000006  min_lr: 0.000000  loss: 5.0968 (5.1365)  class_acc: 0.0000 (0.0185)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1162  max mem: 15572
Epoch: [0]  [ 490/1404]  eta: 0:09:30  lr: 0.000007  min_lr: 0.000000  loss: 5.0845 (5.1355)  class_acc: 0.0000 (0.0182)  loss_scale: 1024.0000 (456.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0224  max mem: 15572
Epoch: [0]  [ 500/1404]  eta: 0:09:23  lr: 0.000007  min_lr: 0.000000  loss: 5.0730 (5.1343)  class_acc: 0.0000 (0.0185)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0566  max mem: 15572
Epoch: [0]  [ 510/1404]  eta: 0:09:17  lr: 0.000007  min_lr: 0.000000  loss: 5.0711 (5.1333)  class_acc: 0.0000 (0.0187)  loss_scale: 1024.0000 (478.9354)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1118  max mem: 15572
[2025-01-16 20:19:21,236] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:19:21,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2025-01-16 20:19:21,236] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:19:21,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [0]  [ 520/1404]  eta: 0:09:10  lr: 0.000007  min_lr: 0.000000  loss: 5.0656 (5.1322)  class_acc: 0.0000 (0.0188)  loss_scale: 1024.0000 (507.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1119  max mem: 15572
Epoch: [0]  [ 530/1404]  eta: 0:09:02  lr: 0.000007  min_lr: 0.000000  loss: 5.0866 (5.1316)  class_acc: 0.0000 (0.0186)  loss_scale: 2048.0000 (536.1055)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0699  max mem: 15572
Epoch: [0]  [ 540/1404]  eta: 0:08:56  lr: 0.000007  min_lr: 0.000000  loss: 5.0656 (5.1302)  class_acc: 0.0000 (0.0185)  loss_scale: 2048.0000 (564.0518)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0762  max mem: 15572
Epoch: [0]  [ 550/1404]  eta: 0:08:49  lr: 0.000007  min_lr: 0.000000  loss: 5.0614 (5.1294)  class_acc: 0.0000 (0.0185)  loss_scale: 2048.0000 (590.9837)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.0539  max mem: 15572
Epoch: [0]  [ 560/1404]  eta: 0:08:42  lr: 0.000007  min_lr: 0.000000  loss: 5.0848 (5.1283)  class_acc: 0.0000 (0.0183)  loss_scale: 2048.0000 (616.9554)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0351  max mem: 15572
Epoch: [0]  [ 570/1404]  eta: 0:08:36  lr: 0.000008  min_lr: 0.000000  loss: 5.0664 (5.1274)  class_acc: 0.0000 (0.0186)  loss_scale: 2048.0000 (642.0175)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.0575  max mem: 15572
Epoch: [0]  [ 580/1404]  eta: 0:08:29  lr: 0.000008  min_lr: 0.000000  loss: 5.0578 (5.1264)  class_acc: 0.0000 (0.0184)  loss_scale: 2048.0000 (666.2169)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0233  max mem: 15572
Epoch: [0]  [ 590/1404]  eta: 0:08:23  lr: 0.000008  min_lr: 0.000000  loss: 5.0578 (5.1253)  class_acc: 0.0000 (0.0188)  loss_scale: 2048.0000 (689.5973)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0222  max mem: 15572
Epoch: [0]  [ 600/1404]  eta: 0:08:18  lr: 0.000008  min_lr: 0.000000  loss: 5.0571 (5.1241)  class_acc: 0.0000 (0.0186)  loss_scale: 2048.0000 (712.1997)  weight_decay: 0.0500 (0.0500)  time: 0.6827  data: 0.0814  max mem: 15572
Epoch: [0]  [ 610/1404]  eta: 0:08:12  lr: 0.000008  min_lr: 0.000000  loss: 5.0753 (5.1234)  class_acc: 0.0000 (0.0188)  loss_scale: 2048.0000 (734.0622)  weight_decay: 0.0500 (0.0500)  time: 0.6643  data: 0.0601  max mem: 15572
Epoch: [0]  [ 620/1404]  eta: 0:08:06  lr: 0.000008  min_lr: 0.000000  loss: 5.0757 (5.1227)  class_acc: 0.0000 (0.0188)  loss_scale: 2048.0000 (755.2206)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.0009  max mem: 15572
Epoch: [0]  [ 630/1404]  eta: 0:08:00  lr: 0.000008  min_lr: 0.000000  loss: 5.0667 (5.1216)  class_acc: 0.0000 (0.0188)  loss_scale: 2048.0000 (775.7084)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.0008  max mem: 15572
[2025-01-16 20:20:39,173] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:20:39,173] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 640/1404]  eta: 0:07:53  lr: 0.000009  min_lr: 0.000000  loss: 5.0783 (5.1210)  class_acc: 0.0000 (0.0185)  loss_scale: 2048.0000 (798.7520)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.0009  max mem: 15572
[2025-01-16 20:20:39,204] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:20:39,204] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 650/1404]  eta: 0:07:47  lr: 0.000009  min_lr: 0.000000  loss: 5.0813 (5.1203)  class_acc: 0.0000 (0.0186)  loss_scale: 4096.0000 (849.4009)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0013  max mem: 15572
Epoch: [0]  [ 660/1404]  eta: 0:07:41  lr: 0.000009  min_lr: 0.000000  loss: 5.0448 (5.1188)  class_acc: 0.0000 (0.0188)  loss_scale: 4096.0000 (898.5174)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0015  max mem: 15572
Epoch: [0]  [ 670/1404]  eta: 0:07:34  lr: 0.000009  min_lr: 0.000000  loss: 5.0334 (5.1180)  class_acc: 0.0000 (0.0189)  loss_scale: 4096.0000 (946.1699)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0008  max mem: 15572
Epoch: [0]  [ 680/1404]  eta: 0:07:28  lr: 0.000009  min_lr: 0.000000  loss: 5.0254 (5.1168)  class_acc: 0.0000 (0.0189)  loss_scale: 4096.0000 (992.4229)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.0010  max mem: 15572
Epoch: [0]  [ 690/1404]  eta: 0:07:22  lr: 0.000009  min_lr: 0.000000  loss: 5.0254 (5.1159)  class_acc: 0.0000 (0.0188)  loss_scale: 4096.0000 (1037.3372)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.0012  max mem: 15572
Epoch: [0]  [ 700/1404]  eta: 0:07:16  lr: 0.000009  min_lr: 0.000000  loss: 5.0369 (5.1150)  class_acc: 0.0000 (0.0187)  loss_scale: 4096.0000 (1080.9700)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.0009  max mem: 15572
Epoch: [0]  [ 710/1404]  eta: 0:07:10  lr: 0.000009  min_lr: 0.000000  loss: 5.0509 (5.1140)  class_acc: 0.0000 (0.0190)  loss_scale: 4096.0000 (1123.3755)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.0007  max mem: 15572
Epoch: [0]  [ 720/1404]  eta: 0:07:03  lr: 0.000010  min_lr: 0.000000  loss: 5.0343 (5.1130)  class_acc: 0.0000 (0.0190)  loss_scale: 4096.0000 (1164.6047)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0006  max mem: 15572
Epoch: [0]  [ 730/1404]  eta: 0:06:58  lr: 0.000010  min_lr: 0.000000  loss: 5.0464 (5.1125)  class_acc: 0.0000 (0.0190)  loss_scale: 4096.0000 (1204.7059)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.0008  max mem: 15572
Epoch: [0]  [ 740/1404]  eta: 0:06:51  lr: 0.000010  min_lr: 0.000000  loss: 5.0641 (5.1119)  class_acc: 0.0000 (0.0189)  loss_scale: 4096.0000 (1243.7247)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.0007  max mem: 15572
Epoch: [0]  [ 750/1404]  eta: 0:06:44  lr: 0.000010  min_lr: 0.000000  loss: 5.0406 (5.1106)  class_acc: 0.0000 (0.0192)  loss_scale: 4096.0000 (1281.7044)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0005  max mem: 15572
Epoch: [0]  [ 760/1404]  eta: 0:06:38  lr: 0.000010  min_lr: 0.000000  loss: 5.0423 (5.1101)  class_acc: 0.0000 (0.0192)  loss_scale: 4096.0000 (1318.6859)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0006  max mem: 15572
[2025-01-16 20:21:57,904] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:21:57,905] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2025-01-16 20:21:57,984] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:21:57,985] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [0]  [ 770/1404]  eta: 0:06:32  lr: 0.000010  min_lr: 0.000000  loss: 5.0504 (5.1094)  class_acc: 0.0000 (0.0196)  loss_scale: 4096.0000 (1370.6459)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.0008  max mem: 15572
Epoch: [0]  [ 780/1404]  eta: 0:06:26  lr: 0.000010  min_lr: 0.000000  loss: 5.0397 (5.1083)  class_acc: 0.0000 (0.0200)  loss_scale: 8192.0000 (1457.9872)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.0008  max mem: 15572
Epoch: [0]  [ 790/1404]  eta: 0:06:20  lr: 0.000011  min_lr: 0.000000  loss: 5.0143 (5.1073)  class_acc: 0.0000 (0.0200)  loss_scale: 8192.0000 (1543.1201)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.0055  max mem: 15572
Epoch: [0]  [ 800/1404]  eta: 0:06:14  lr: 0.000011  min_lr: 0.000000  loss: 5.0111 (5.1061)  class_acc: 0.0000 (0.0201)  loss_scale: 8192.0000 (1626.1273)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.0056  max mem: 15572
Epoch: [0]  [ 810/1404]  eta: 0:06:08  lr: 0.000011  min_lr: 0.000000  loss: 5.0486 (5.1056)  class_acc: 0.0000 (0.0201)  loss_scale: 8192.0000 (1707.0875)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.0008  max mem: 15572
Epoch: [0]  [ 820/1404]  eta: 0:06:01  lr: 0.000011  min_lr: 0.000000  loss: 5.0402 (5.1045)  class_acc: 0.0000 (0.0200)  loss_scale: 8192.0000 (1786.0755)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0007  max mem: 15572
Epoch: [0]  [ 830/1404]  eta: 0:05:55  lr: 0.000011  min_lr: 0.000000  loss: 4.9970 (5.1035)  class_acc: 0.0000 (0.0205)  loss_scale: 8192.0000 (1863.1625)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0009  max mem: 15572
Epoch: [0]  [ 840/1404]  eta: 0:05:48  lr: 0.000011  min_lr: 0.000000  loss: 5.0185 (5.1028)  class_acc: 0.0000 (0.0204)  loss_scale: 8192.0000 (1938.4162)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0009  max mem: 15572
Epoch: [0]  [ 850/1404]  eta: 0:05:42  lr: 0.000011  min_lr: 0.000000  loss: 5.0299 (5.1021)  class_acc: 0.0000 (0.0204)  loss_scale: 8192.0000 (2011.9013)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0007  max mem: 15572
Epoch: [0]  [ 860/1404]  eta: 0:05:36  lr: 0.000011  min_lr: 0.000000  loss: 5.0165 (5.1010)  class_acc: 0.0000 (0.0205)  loss_scale: 8192.0000 (2083.6794)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0008  max mem: 15572
Epoch: [0]  [ 870/1404]  eta: 0:05:29  lr: 0.000012  min_lr: 0.000000  loss: 5.0078 (5.0998)  class_acc: 0.0000 (0.0204)  loss_scale: 8192.0000 (2153.8094)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0009  max mem: 15572
Epoch: [0]  [ 880/1404]  eta: 0:05:23  lr: 0.000012  min_lr: 0.000000  loss: 5.0263 (5.0992)  class_acc: 0.0000 (0.0205)  loss_scale: 8192.0000 (2222.3473)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0008  max mem: 15572
Epoch: [0]  [ 890/1404]  eta: 0:05:17  lr: 0.000012  min_lr: 0.000000  loss: 5.0030 (5.0982)  class_acc: 0.0000 (0.0205)  loss_scale: 8192.0000 (2289.3468)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.0008  max mem: 15572
[2025-01-16 20:23:16,450] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:23:16,450] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2025-01-16 20:23:16,562] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:23:16,562] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [0]  [ 900/1404]  eta: 0:05:11  lr: 0.000012  min_lr: 0.000000  loss: 5.0145 (5.0978)  class_acc: 0.0000 (0.0203)  loss_scale: 8192.0000 (2400.3196)  weight_decay: 0.0500 (0.0500)  time: 0.6540  data: 0.0008  max mem: 15572
Epoch: [0]  [ 910/1404]  eta: 0:05:05  lr: 0.000012  min_lr: 0.000000  loss: 5.0414 (5.0973)  class_acc: 0.0000 (0.0204)  loss_scale: 16384.0000 (2553.8178)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.0010  max mem: 15572
Epoch: [0]  [ 920/1404]  eta: 0:04:58  lr: 0.000012  min_lr: 0.000000  loss: 5.0414 (5.0971)  class_acc: 0.0000 (0.0203)  loss_scale: 16384.0000 (2703.9826)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0010  max mem: 15572
Epoch: [0]  [ 930/1404]  eta: 0:04:52  lr: 0.000012  min_lr: 0.000000  loss: 5.0276 (5.0966)  class_acc: 0.0000 (0.0201)  loss_scale: 16384.0000 (2850.9216)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0011  max mem: 15572
Epoch: [0]  [ 940/1404]  eta: 0:04:46  lr: 0.000013  min_lr: 0.000000  loss: 5.0038 (5.0957)  class_acc: 0.0000 (0.0202)  loss_scale: 16384.0000 (2994.7375)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.0475  max mem: 15572
Epoch: [0]  [ 950/1404]  eta: 0:04:40  lr: 0.000013  min_lr: 0.000000  loss: 5.0038 (5.0952)  class_acc: 0.0000 (0.0202)  loss_scale: 16384.0000 (3135.5289)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.0991  max mem: 15572
Epoch: [0]  [ 960/1404]  eta: 0:04:34  lr: 0.000013  min_lr: 0.000000  loss: 5.0519 (5.0947)  class_acc: 0.0000 (0.0202)  loss_scale: 16384.0000 (3273.3902)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1060  max mem: 15572
Epoch: [0]  [ 970/1404]  eta: 0:04:28  lr: 0.000013  min_lr: 0.000000  loss: 5.0398 (5.0939)  class_acc: 0.0000 (0.0200)  loss_scale: 16384.0000 (3408.4119)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.1564  max mem: 15572
Epoch: [0]  [ 980/1404]  eta: 0:04:21  lr: 0.000013  min_lr: 0.000000  loss: 5.0187 (5.0932)  class_acc: 0.0000 (0.0201)  loss_scale: 16384.0000 (3540.6809)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1314  max mem: 15572
Epoch: [0]  [ 990/1404]  eta: 0:04:15  lr: 0.000013  min_lr: 0.000000  loss: 5.0187 (5.0927)  class_acc: 0.0000 (0.0199)  loss_scale: 16384.0000 (3670.2805)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1102  max mem: 15572
[2025-01-16 20:24:19,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.2928139878801235e-07, 1.2928139878801235e-07, 1.8468771255430337e-07, 1.8468771255430337e-07, 2.638395893632906e-07, 2.638395893632906e-07, 3.7691369909041513e-07, 3.7691369909041513e-07, 5.384481415577359e-07, 5.384481415577359e-07, 7.692116307967656e-07, 7.692116307967656e-07, 1.0988737582810937e-06, 1.0988737582810937e-06, 1.569819654687277e-06, 1.569819654687277e-06, 2.24259950669611e-06, 2.24259950669611e-06, 3.2037135809944434e-06, 3.2037135809944434e-06, 4.5767336871349184e-06, 4.5767336871349184e-06, 6.538190981621313e-06, 6.538190981621313e-06, 9.34027283088759e-06, 9.34027283088759e-06, 1.3343246901267988e-05, 1.3343246901267988e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 20:24:19,696] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=46.34338174914406, CurrSamplesPerSec=50.40546680613759, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [1000/1404]  eta: 0:04:09  lr: 0.000013  min_lr: 0.000000  loss: 5.0627 (5.0923)  class_acc: 0.0000 (0.0199)  loss_scale: 16384.0000 (3797.2907)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1485  max mem: 15572
Epoch: [0]  [1010/1404]  eta: 0:04:03  lr: 0.000013  min_lr: 0.000000  loss: 5.0627 (5.0921)  class_acc: 0.0000 (0.0199)  loss_scale: 16384.0000 (3921.7883)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1358  max mem: 15572
Epoch: [0]  [1020/1404]  eta: 0:03:56  lr: 0.000014  min_lr: 0.000000  loss: 5.0517 (5.0914)  class_acc: 0.0000 (0.0201)  loss_scale: 16384.0000 (4043.8472)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0693  max mem: 15572
[2025-01-16 20:24:34,026] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:24:34,027] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2025-01-16 20:24:34,061] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:24:34,062] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [0]  [1030/1404]  eta: 0:03:50  lr: 0.000014  min_lr: 0.000000  loss: 4.9611 (5.0902)  class_acc: 0.0000 (0.0199)  loss_scale: 16384.0000 (4274.7779)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0010  max mem: 15572
Epoch: [0]  [1040/1404]  eta: 0:03:44  lr: 0.000014  min_lr: 0.000000  loss: 4.9735 (5.0899)  class_acc: 0.0000 (0.0199)  loss_scale: 32768.0000 (4548.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0220  max mem: 15572
Epoch: [0]  [1050/1404]  eta: 0:03:38  lr: 0.000014  min_lr: 0.000000  loss: 5.0654 (5.0894)  class_acc: 0.0000 (0.0201)  loss_scale: 32768.0000 (4816.9895)  weight_decay: 0.0500 (0.0500)  time: 0.6640  data: 0.1293  max mem: 15572
Epoch: [0]  [1060/1404]  eta: 0:03:32  lr: 0.000014  min_lr: 0.000000  loss: 5.0589 (5.0886)  class_acc: 0.0000 (0.0200)  loss_scale: 32768.0000 (5080.4298)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.1252  max mem: 15572
Epoch: [0]  [1070/1404]  eta: 0:03:26  lr: 0.000014  min_lr: 0.000000  loss: 5.0091 (5.0879)  class_acc: 0.0000 (0.0201)  loss_scale: 32768.0000 (5338.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.0964  max mem: 15572
Epoch: [0]  [1080/1404]  eta: 0:03:20  lr: 0.000014  min_lr: 0.000000  loss: 5.0577 (5.0873)  class_acc: 0.0000 (0.0201)  loss_scale: 32768.0000 (5592.6883)  weight_decay: 0.0500 (0.0500)  time: 0.6760  data: 0.1650  max mem: 15572
Epoch: [0]  [1090/1404]  eta: 0:03:13  lr: 0.000015  min_lr: 0.000000  loss: 5.0977 (5.0877)  class_acc: 0.0000 (0.0199)  loss_scale: 32768.0000 (5841.7745)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0864  max mem: 15572
Epoch: [0]  [1100/1404]  eta: 0:03:07  lr: 0.000015  min_lr: 0.000000  loss: 5.0390 (5.0866)  class_acc: 0.0000 (0.0198)  loss_scale: 32768.0000 (6086.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0570  max mem: 15572
Epoch: [0]  [1110/1404]  eta: 0:03:01  lr: 0.000015  min_lr: 0.000000  loss: 4.9965 (5.0856)  class_acc: 0.0000 (0.0200)  loss_scale: 32768.0000 (6326.4950)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0912  max mem: 15572
Epoch: [0]  [1120/1404]  eta: 0:02:55  lr: 0.000015  min_lr: 0.000000  loss: 4.9873 (5.0847)  class_acc: 0.0000 (0.0200)  loss_scale: 32768.0000 (6562.3693)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1042  max mem: 15572
Epoch: [0]  [1130/1404]  eta: 0:02:49  lr: 0.000015  min_lr: 0.000000  loss: 4.9914 (5.0840)  class_acc: 0.0000 (0.0200)  loss_scale: 32768.0000 (6794.0725)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.1081  max mem: 15572
Epoch: [0]  [1140/1404]  eta: 0:02:42  lr: 0.000015  min_lr: 0.000000  loss: 5.0033 (5.0832)  class_acc: 0.0000 (0.0199)  loss_scale: 32768.0000 (7021.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0388  max mem: 15572
Epoch: [0]  [1150/1404]  eta: 0:02:36  lr: 0.000015  min_lr: 0.000000  loss: 4.9716 (5.0823)  class_acc: 0.0000 (0.0199)  loss_scale: 32768.0000 (7245.4005)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0692  max mem: 15572
[2025-01-16 20:25:53,383] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:25:53,383] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2025-01-16 20:25:53,385] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:25:53,386] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [0]  [1160/1404]  eta: 0:02:30  lr: 0.000015  min_lr: 0.000000  loss: 4.9809 (5.0815)  class_acc: 0.0000 (0.0199)  loss_scale: 32768.0000 (7719.2489)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0691  max mem: 15572
Epoch: [0]  [1170/1404]  eta: 0:02:24  lr: 0.000016  min_lr: 0.000000  loss: 5.0352 (5.0813)  class_acc: 0.0000 (0.0198)  loss_scale: 65536.0000 (8212.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0007  max mem: 15572
Epoch: [0]  [1180/1404]  eta: 0:02:17  lr: 0.000016  min_lr: 0.000000  loss: 5.0211 (5.0806)  class_acc: 0.0000 (0.0196)  loss_scale: 65536.0000 (8698.3641)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0390  max mem: 15572
Epoch: [0]  [1190/1404]  eta: 0:02:11  lr: 0.000016  min_lr: 0.000000  loss: 5.0069 (5.0800)  class_acc: 0.0000 (0.0195)  loss_scale: 65536.0000 (9175.5903)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0861  max mem: 15572
Epoch: [0]  [1200/1404]  eta: 0:02:05  lr: 0.000016  min_lr: 0.000000  loss: 5.0249 (5.0797)  class_acc: 0.0000 (0.0198)  loss_scale: 65536.0000 (9644.8693)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0479  max mem: 15572
Epoch: [0]  [1210/1404]  eta: 0:01:59  lr: 0.000016  min_lr: 0.000000  loss: 5.0054 (5.0788)  class_acc: 0.0000 (0.0197)  loss_scale: 65536.0000 (10106.3980)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0244  max mem: 15572
Epoch: [0]  [1220/1404]  eta: 0:01:53  lr: 0.000016  min_lr: 0.000000  loss: 4.9944 (5.0782)  class_acc: 0.0000 (0.0197)  loss_scale: 65536.0000 (10560.3669)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0320  max mem: 15572
Epoch: [0]  [1230/1404]  eta: 0:01:47  lr: 0.000016  min_lr: 0.000000  loss: 4.9944 (5.0776)  class_acc: 0.0000 (0.0195)  loss_scale: 65536.0000 (11006.9602)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0153  max mem: 15572
Epoch: [0]  [1240/1404]  eta: 0:01:40  lr: 0.000017  min_lr: 0.000000  loss: 4.9469 (5.0769)  class_acc: 0.0000 (0.0195)  loss_scale: 65536.0000 (11446.3562)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0294  max mem: 15572
Epoch: [0]  [1250/1404]  eta: 0:01:34  lr: 0.000017  min_lr: 0.000000  loss: 5.0289 (5.0762)  class_acc: 0.0000 (0.0196)  loss_scale: 65536.0000 (11878.7274)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0225  max mem: 15572
Epoch: [0]  [1260/1404]  eta: 0:01:28  lr: 0.000017  min_lr: 0.000000  loss: 5.0282 (5.0755)  class_acc: 0.0000 (0.0195)  loss_scale: 65536.0000 (12304.2411)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0005  max mem: 15572
Epoch: [0]  [1270/1404]  eta: 0:01:22  lr: 0.000017  min_lr: 0.000000  loss: 4.9884 (5.0748)  class_acc: 0.0000 (0.0194)  loss_scale: 65536.0000 (12723.0590)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.0359  max mem: 15572
[2025-01-16 20:27:10,448] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:27:10,448] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
[2025-01-16 20:27:10,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:27:10,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [0]  [1280/1404]  eta: 0:01:16  lr: 0.000017  min_lr: 0.000000  loss: 5.0087 (5.0749)  class_acc: 0.0000 (0.0195)  loss_scale: 65536.0000 (13186.4980)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.0745  max mem: 15572
Epoch: [0]  [1290/1404]  eta: 0:01:10  lr: 0.000017  min_lr: 0.000000  loss: 5.0332 (5.0743)  class_acc: 0.0000 (0.0196)  loss_scale: 131072.0000 (14099.6313)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0395  max mem: 15572
Epoch: [0]  [1300/1404]  eta: 0:01:03  lr: 0.000017  min_lr: 0.000000  loss: 5.0340 (5.0740)  class_acc: 0.0000 (0.0196)  loss_scale: 131072.0000 (14998.7271)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0208  max mem: 15572
Epoch: [0]  [1310/1404]  eta: 0:00:57  lr: 0.000017  min_lr: 0.000000  loss: 5.0432 (5.0734)  class_acc: 0.0000 (0.0195)  loss_scale: 131072.0000 (15884.1068)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0206  max mem: 15572
Epoch: [0]  [1320/1404]  eta: 0:00:51  lr: 0.000018  min_lr: 0.000000  loss: 5.0478 (5.0733)  class_acc: 0.0000 (0.0197)  loss_scale: 131072.0000 (16756.0818)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.0008  max mem: 15572
Epoch: [0]  [1330/1404]  eta: 0:00:45  lr: 0.000018  min_lr: 0.000000  loss: 5.0606 (5.0732)  class_acc: 0.0000 (0.0196)  loss_scale: 131072.0000 (17614.9542)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0109  max mem: 15572
Epoch: [0]  [1340/1404]  eta: 0:00:39  lr: 0.000018  min_lr: 0.000000  loss: 5.0050 (5.0728)  class_acc: 0.0000 (0.0195)  loss_scale: 131072.0000 (18461.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0110  max mem: 15572
Epoch: [0]  [1350/1404]  eta: 0:00:33  lr: 0.000018  min_lr: 0.000000  loss: 4.9995 (5.0725)  class_acc: 0.0000 (0.0196)  loss_scale: 131072.0000 (19294.5551)  weight_decay: 0.0500 (0.0500)  time: 0.6470  data: 0.0873  max mem: 15572
Epoch: [0]  [1360/1404]  eta: 0:00:26  lr: 0.000018  min_lr: 0.000000  loss: 4.9995 (5.0722)  class_acc: 0.0000 (0.0195)  loss_scale: 131072.0000 (20115.8442)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.0871  max mem: 15572
Epoch: [0]  [1370/1404]  eta: 0:00:20  lr: 0.000018  min_lr: 0.000000  loss: 4.9992 (5.0716)  class_acc: 0.0000 (0.0196)  loss_scale: 131072.0000 (20925.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0482  max mem: 15572
Epoch: [0]  [1380/1404]  eta: 0:00:14  lr: 0.000018  min_lr: 0.000000  loss: 5.0015 (5.0712)  class_acc: 0.0000 (0.0195)  loss_scale: 131072.0000 (21722.7400)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.1086  max mem: 15572
Epoch: [0]  [1390/1404]  eta: 0:00:08  lr: 0.000019  min_lr: 0.000000  loss: 5.0118 (5.0707)  class_acc: 0.0000 (0.0194)  loss_scale: 131072.0000 (22508.8598)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.0662  max mem: 15572
Epoch: [0]  [1400/1404]  eta: 0:00:02  lr: 0.000019  min_lr: 0.000000  loss: 4.9796 (5.0700)  class_acc: 0.0000 (0.0193)  loss_scale: 131072.0000 (23283.7573)  weight_decay: 0.0500 (0.0500)  time: 0.4521  data: 0.0056  max mem: 15572
Epoch: [0]  [1403/1404]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 5.0019 (5.0700)  class_acc: 0.0000 (0.0192)  loss_scale: 131072.0000 (23514.0741)  weight_decay: 0.0500 (0.0500)  time: 0.4221  data: 0.0054  max mem: 15572
Epoch: [0] Total time: 0:14:17 (0.6109 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 5.0019 (5.0692)  class_acc: 0.0000 (0.0186)  loss_scale: 131072.0000 (23514.0741)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:01  loss: 5.0901 (5.0901)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.9790  data: 3.7070  max mem: 15572
Val:  [ 10/136]  eta: 0:01:34  loss: 5.1391 (5.0702)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.7494  data: 0.5573  max mem: 15572
Val:  [ 20/136]  eta: 0:01:01  loss: 5.0150 (5.0018)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3567  data: 0.1771  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 4.8294 (5.0063)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3192  data: 0.1300  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 4.9371 (4.9571)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (4.7425)  time: 0.3956  data: 0.1861  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 5.0421 (5.0228)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (3.8126)  time: 0.3587  data: 0.1541  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 5.2201 (5.0660)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (3.1876)  time: 0.3113  data: 0.1229  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 5.0395 (5.0427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (6.5728)  time: 0.3119  data: 0.1372  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 4.7083 (5.0042)  acc1: 0.0000 (2.0576)  acc5: 0.0000 (7.8189)  time: 0.3371  data: 0.1679  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 4.9809 (5.0156)  acc1: 0.0000 (1.8315)  acc5: 0.0000 (6.9597)  time: 0.4403  data: 0.2655  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 5.0820 (5.0352)  acc1: 0.0000 (1.6502)  acc5: 0.0000 (6.2706)  time: 0.4145  data: 0.2224  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 5.0195 (5.0206)  acc1: 0.0000 (1.5015)  acc5: 0.0000 (5.7057)  time: 0.3491  data: 0.1367  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.7526 (5.0084)  acc1: 0.0000 (1.3774)  acc5: 0.0000 (5.2342)  time: 0.3537  data: 0.1454  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 4.9102 (5.0237)  acc1: 0.0000 (1.2723)  acc5: 0.0000 (4.8346)  time: 0.2778  data: 0.1080  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 5.2016 (5.0243)  acc1: 0.0000 (1.2285)  acc5: 0.0000 (5.9378)  time: 0.1850  data: 0.0312  max mem: 15572
Val: Total time: 0:00:50 (0.3692 s / it)
* Acc@1 1.229 Acc@5 5.938 loss 5.023
Accuracy of the network on the 4883 val videos: 1.2%
[2025-01-16 20:29:09,312] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2025-01-16 20:29:09,314] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2025-01-16 20:29:09,316] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 20:29:09,316] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 20:29:09,538] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 20:29:09,538] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 1.23%
Epoch: [1]  [   0/1404]  eta: 3:05:31  lr: 0.000019  min_lr: 0.000000  loss: 5.0448 (5.0448)  class_acc: 0.0833 (0.0833)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 7.9282  data: 7.2765  max mem: 15572
[2025-01-16 20:29:19,318] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:29:19,319] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
[2025-01-16 20:29:19,319] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:29:19,320] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
Epoch: [1]  [  10/1404]  eta: 0:28:42  lr: 0.000019  min_lr: 0.000000  loss: 5.0154 (5.0169)  class_acc: 0.0000 (0.0227)  loss_scale: 262144.0000 (214481.4545)  weight_decay: 0.0500 (0.0500)  time: 1.2358  data: 0.7435  max mem: 15572
[2025-01-16 20:29:27,213] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1422
[2025-01-16 20:29:27,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
[2025-01-16 20:29:27,214] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144, reducing to 131072.0
[2025-01-16 20:29:27,215] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1422
[2025-01-16 20:29:27,215] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
Epoch: [1]  [  20/1404]  eta: 0:21:01  lr: 0.000019  min_lr: 0.000000  loss: 4.9868 (5.0013)  class_acc: 0.0000 (0.0159)  loss_scale: 262144.0000 (218453.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0623  max mem: 15572
Epoch: [1]  [  30/1404]  eta: 0:18:46  lr: 0.000019  min_lr: 0.000000  loss: 4.9902 (5.0066)  class_acc: 0.0000 (0.0121)  loss_scale: 131072.0000 (190265.8065)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.0896  max mem: 15572
Epoch: [1]  [  40/1404]  eta: 0:16:53  lr: 0.000019  min_lr: 0.000000  loss: 4.9918 (5.0101)  class_acc: 0.0000 (0.0122)  loss_scale: 131072.0000 (175828.2927)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0817  max mem: 15572
Epoch: [1]  [  50/1404]  eta: 0:16:08  lr: 0.000019  min_lr: 0.000000  loss: 4.9918 (4.9985)  class_acc: 0.0000 (0.0147)  loss_scale: 131072.0000 (167052.5490)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0583  max mem: 15572
Epoch: [1]  [  60/1404]  eta: 0:16:19  lr: 0.000020  min_lr: 0.000000  loss: 4.9640 (4.9922)  class_acc: 0.0000 (0.0137)  loss_scale: 131072.0000 (161154.0984)  weight_decay: 0.0500 (0.0500)  time: 0.7003  data: 0.2029  max mem: 15572
Epoch: [1]  [  70/1404]  eta: 0:15:27  lr: 0.000020  min_lr: 0.000000  loss: 4.9977 (5.0029)  class_acc: 0.0000 (0.0129)  loss_scale: 131072.0000 (156917.1831)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.1620  max mem: 15572
Epoch: [1]  [  80/1404]  eta: 0:15:07  lr: 0.000020  min_lr: 0.000000  loss: 5.0064 (5.0050)  class_acc: 0.0000 (0.0123)  loss_scale: 131072.0000 (153726.4198)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0715  max mem: 15572
Epoch: [1]  [  90/1404]  eta: 0:14:48  lr: 0.000020  min_lr: 0.000000  loss: 4.9944 (5.0087)  class_acc: 0.0000 (0.0119)  loss_scale: 131072.0000 (151236.9231)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1302  max mem: 15572
Epoch: [1]  [ 100/1404]  eta: 0:14:17  lr: 0.000020  min_lr: 0.000000  loss: 5.0084 (5.0080)  class_acc: 0.0000 (0.0124)  loss_scale: 131072.0000 (149240.3960)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0748  max mem: 15572
Epoch: [1]  [ 110/1404]  eta: 0:14:04  lr: 0.000020  min_lr: 0.000000  loss: 5.0039 (5.0044)  class_acc: 0.0000 (0.0139)  loss_scale: 131072.0000 (147603.6036)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0690  max mem: 15572
Epoch: [1]  [ 120/1404]  eta: 0:13:52  lr: 0.000020  min_lr: 0.000000  loss: 5.0313 (5.0117)  class_acc: 0.0000 (0.0138)  loss_scale: 131072.0000 (146237.3554)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1091  max mem: 15572
Epoch: [1]  [ 130/1404]  eta: 0:13:38  lr: 0.000020  min_lr: 0.000000  loss: 5.0707 (5.0148)  class_acc: 0.0000 (0.0140)  loss_scale: 131072.0000 (145079.6947)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0839  max mem: 15572
Epoch: [1]  [ 140/1404]  eta: 0:13:23  lr: 0.000021  min_lr: 0.000000  loss: 5.0773 (5.0185)  class_acc: 0.0000 (0.0145)  loss_scale: 131072.0000 (144086.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0613  max mem: 15572
[2025-01-16 20:30:42,727] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:30:42,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-16 20:30:42,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:30:42,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-16 20:30:43,266] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1552
[2025-01-16 20:30:43,266] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-16 20:30:43,266] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-01-16 20:30:43,309] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1552
[2025-01-16 20:30:43,309] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Epoch: [1]  [ 150/1404]  eta: 0:13:14  lr: 0.000021  min_lr: 0.000000  loss: 5.0701 (5.0227)  class_acc: 0.0000 (0.0157)  loss_scale: 131072.0000 (144092.3974)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0759  max mem: 15572
Epoch: [1]  [ 160/1404]  eta: 0:13:09  lr: 0.000021  min_lr: 0.000000  loss: 5.0050 (5.0189)  class_acc: 0.0000 (0.0148)  loss_scale: 131072.0000 (143283.6770)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1289  max mem: 15572
Epoch: [1]  [ 170/1404]  eta: 0:13:00  lr: 0.000021  min_lr: 0.000000  loss: 4.9227 (5.0132)  class_acc: 0.0000 (0.0154)  loss_scale: 131072.0000 (142569.5439)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1466  max mem: 15572
Epoch: [1]  [ 180/1404]  eta: 0:12:44  lr: 0.000021  min_lr: 0.000000  loss: 4.9415 (5.0135)  class_acc: 0.0000 (0.0163)  loss_scale: 131072.0000 (141934.3204)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0684  max mem: 15572
Epoch: [1]  [ 190/1404]  eta: 0:12:34  lr: 0.000021  min_lr: 0.000000  loss: 5.0368 (5.0141)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (141365.6126)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0165  max mem: 15572
Epoch: [1]  [ 200/1404]  eta: 0:12:21  lr: 0.000021  min_lr: 0.000000  loss: 5.0144 (5.0128)  class_acc: 0.0000 (0.0160)  loss_scale: 131072.0000 (140853.4925)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0165  max mem: 15572
Epoch: [1]  [ 210/1404]  eta: 0:12:12  lr: 0.000022  min_lr: 0.000000  loss: 5.0046 (5.0130)  class_acc: 0.0000 (0.0168)  loss_scale: 131072.0000 (140389.9147)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0007  max mem: 15572
Epoch: [1]  [ 220/1404]  eta: 0:12:08  lr: 0.000022  min_lr: 0.000000  loss: 5.0405 (5.0133)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (139968.2896)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0007  max mem: 15572
Epoch: [1]  [ 230/1404]  eta: 0:11:59  lr: 0.000022  min_lr: 0.000000  loss: 5.0244 (5.0125)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (139583.1688)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.0005  max mem: 15572
Epoch: [1]  [ 240/1404]  eta: 0:11:51  lr: 0.000022  min_lr: 0.000000  loss: 5.0244 (5.0144)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (139230.0083)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0005  max mem: 15572
Epoch: [1]  [ 250/1404]  eta: 0:11:42  lr: 0.000022  min_lr: 0.000000  loss: 5.0065 (5.0119)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (138904.9880)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0005  max mem: 15572
Epoch: [1]  [ 260/1404]  eta: 0:11:32  lr: 0.000022  min_lr: 0.000000  loss: 4.9546 (5.0110)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (138604.8736)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0006  max mem: 15572
Epoch: [1]  [ 270/1404]  eta: 0:11:27  lr: 0.000022  min_lr: 0.000000  loss: 5.0240 (5.0118)  class_acc: 0.0000 (0.0168)  loss_scale: 131072.0000 (138326.9077)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.0006  max mem: 15572
[2025-01-16 20:31:57,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:31:57,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-16 20:31:57,276] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:31:57,276] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-16 20:31:57,734] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1682
[2025-01-16 20:31:57,734] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-16 20:31:57,786] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1682
[2025-01-16 20:31:57,787] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-16 20:31:57,787] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 280/1404]  eta: 0:11:17  lr: 0.000022  min_lr: 0.000000  loss: 5.0479 (5.0154)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (138535.1744)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.0146  max mem: 15572
Epoch: [1]  [ 290/1404]  eta: 0:11:11  lr: 0.000023  min_lr: 0.000000  loss: 5.0581 (5.0140)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (138278.7079)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0767  max mem: 15572
[2025-01-16 20:32:10,136] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1703
[2025-01-16 20:32:10,137] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 20:32:10,137] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 20:32:10,181] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1703
[2025-01-16 20:32:10,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [1]  [ 300/1404]  eta: 0:11:04  lr: 0.000023  min_lr: 0.000000  loss: 5.0014 (5.0138)  class_acc: 0.0000 (0.0179)  loss_scale: 131072.0000 (137603.8272)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0908  max mem: 15572
Epoch: [1]  [ 310/1404]  eta: 0:10:59  lr: 0.000023  min_lr: 0.000000  loss: 4.9880 (5.0133)  class_acc: 0.0000 (0.0185)  loss_scale: 65536.0000 (135286.5338)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0892  max mem: 15572
Epoch: [1]  [ 320/1404]  eta: 0:10:57  lr: 0.000023  min_lr: 0.000000  loss: 5.0618 (5.0164)  class_acc: 0.0000 (0.0187)  loss_scale: 65536.0000 (133113.6199)  weight_decay: 0.0500 (0.0500)  time: 0.6746  data: 0.1256  max mem: 15572
Epoch: [1]  [ 330/1404]  eta: 0:10:49  lr: 0.000023  min_lr: 0.000000  loss: 5.0557 (5.0165)  class_acc: 0.0000 (0.0188)  loss_scale: 65536.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.0651  max mem: 15572
Epoch: [1]  [ 340/1404]  eta: 0:10:43  lr: 0.000023  min_lr: 0.000000  loss: 5.0232 (5.0181)  class_acc: 0.0000 (0.0185)  loss_scale: 65536.0000 (129150.1232)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0094  max mem: 15572
Epoch: [1]  [ 350/1404]  eta: 0:10:38  lr: 0.000023  min_lr: 0.000000  loss: 5.0418 (5.0194)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (127337.7550)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.0094  max mem: 15572
Epoch: [1]  [ 360/1404]  eta: 0:10:31  lr: 0.000024  min_lr: 0.000000  loss: 5.0030 (5.0180)  class_acc: 0.0000 (0.0184)  loss_scale: 65536.0000 (125625.7950)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.0005  max mem: 15572
Epoch: [1]  [ 370/1404]  eta: 0:10:24  lr: 0.000024  min_lr: 0.000000  loss: 4.9728 (5.0163)  class_acc: 0.0000 (0.0186)  loss_scale: 65536.0000 (124006.1240)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0005  max mem: 15572
Epoch: [1]  [ 380/1404]  eta: 0:10:16  lr: 0.000024  min_lr: 0.000000  loss: 4.9982 (5.0164)  class_acc: 0.0000 (0.0188)  loss_scale: 65536.0000 (122471.4751)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0007  max mem: 15572
Epoch: [1]  [ 390/1404]  eta: 0:10:09  lr: 0.000024  min_lr: 0.000000  loss: 5.0119 (5.0150)  class_acc: 0.0000 (0.0188)  loss_scale: 65536.0000 (121015.3248)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0007  max mem: 15572
Epoch: [1]  [ 400/1404]  eta: 0:10:04  lr: 0.000024  min_lr: 0.000000  loss: 4.9810 (5.0140)  class_acc: 0.0000 (0.0189)  loss_scale: 65536.0000 (119631.8005)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.0006  max mem: 15572
Epoch: [1]  [ 410/1404]  eta: 0:09:57  lr: 0.000024  min_lr: 0.000000  loss: 4.9910 (5.0146)  class_acc: 0.0000 (0.0187)  loss_scale: 65536.0000 (118315.6010)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0005  max mem: 15572
Epoch: [1]  [ 420/1404]  eta: 0:09:51  lr: 0.000024  min_lr: 0.000000  loss: 5.0429 (5.0156)  class_acc: 0.0000 (0.0189)  loss_scale: 65536.0000 (117061.9287)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0007  max mem: 15572
[2025-01-16 20:33:27,138] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:33:27,139] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 20:33:27,142] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:33:27,142] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 20:33:27,700] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1833
[2025-01-16 20:33:27,700] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 20:33:27,701] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 20:33:27,742] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1833
[2025-01-16 20:33:27,742] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [1]  [ 430/1404]  eta: 0:09:45  lr: 0.000024  min_lr: 0.000000  loss: 5.0169 (5.0150)  class_acc: 0.0000 (0.0188)  loss_scale: 65536.0000 (116018.4872)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0183  max mem: 15572
Epoch: [1]  [ 440/1404]  eta: 0:09:39  lr: 0.000025  min_lr: 0.000000  loss: 5.0163 (5.0148)  class_acc: 0.0000 (0.0186)  loss_scale: 65536.0000 (114873.7596)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.0426  max mem: 15572
Epoch: [1]  [ 450/1404]  eta: 0:09:32  lr: 0.000025  min_lr: 0.000000  loss: 5.0198 (5.0149)  class_acc: 0.0000 (0.0185)  loss_scale: 65536.0000 (113779.7960)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0536  max mem: 15572
Epoch: [1]  [ 460/1404]  eta: 0:09:24  lr: 0.000025  min_lr: 0.000000  loss: 5.0046 (5.0145)  class_acc: 0.0000 (0.0187)  loss_scale: 65536.0000 (112733.2928)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0293  max mem: 15572
Epoch: [1]  [ 470/1404]  eta: 0:09:18  lr: 0.000025  min_lr: 0.000000  loss: 5.0046 (5.0145)  class_acc: 0.0417 (0.0188)  loss_scale: 65536.0000 (111731.2272)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0594  max mem: 15572
Epoch: [1]  [ 480/1404]  eta: 0:09:13  lr: 0.000025  min_lr: 0.000000  loss: 4.9617 (5.0131)  class_acc: 0.0000 (0.0189)  loss_scale: 65536.0000 (110770.8274)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1108  max mem: 15572
Epoch: [1]  [ 490/1404]  eta: 0:09:05  lr: 0.000025  min_lr: 0.000000  loss: 4.9175 (5.0127)  class_acc: 0.0000 (0.0190)  loss_scale: 65536.0000 (109849.5479)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0602  max mem: 15572
Epoch: [1]  [ 500/1404]  eta: 0:09:01  lr: 0.000025  min_lr: 0.000000  loss: 5.0518 (5.0125)  class_acc: 0.0000 (0.0198)  loss_scale: 65536.0000 (108965.0459)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0422  max mem: 15572
Epoch: [1]  [ 510/1404]  eta: 0:08:54  lr: 0.000026  min_lr: 0.000000  loss: 5.0518 (5.0131)  class_acc: 0.0000 (0.0196)  loss_scale: 65536.0000 (108115.1624)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.0341  max mem: 15572
Epoch: [1]  [ 520/1404]  eta: 0:08:49  lr: 0.000026  min_lr: 0.000000  loss: 5.0084 (5.0122)  class_acc: 0.0000 (0.0200)  loss_scale: 65536.0000 (107297.9040)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0004  max mem: 15572
Epoch: [1]  [ 530/1404]  eta: 0:08:41  lr: 0.000026  min_lr: 0.000000  loss: 4.9475 (5.0111)  class_acc: 0.0000 (0.0199)  loss_scale: 65536.0000 (106511.4275)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0004  max mem: 15572
Epoch: [1]  [ 540/1404]  eta: 0:08:35  lr: 0.000026  min_lr: 0.000000  loss: 4.9650 (5.0109)  class_acc: 0.0000 (0.0207)  loss_scale: 65536.0000 (105754.0259)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0005  max mem: 15572
Epoch: [1]  [ 550/1404]  eta: 0:08:28  lr: 0.000026  min_lr: 0.000000  loss: 4.9754 (5.0105)  class_acc: 0.0417 (0.0213)  loss_scale: 65536.0000 (105024.1162)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0007  max mem: 15572
[2025-01-16 20:34:43,156] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1962
[2025-01-16 20:34:43,157] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:34:43,159] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1962
[2025-01-16 20:34:43,160] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:34:43,160] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [ 560/1404]  eta: 0:08:23  lr: 0.000026  min_lr: 0.000000  loss: 5.0087 (5.0114)  class_acc: 0.0000 (0.0214)  loss_scale: 65536.0000 (104144.9982)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.0006  max mem: 15572
Epoch: [1]  [ 570/1404]  eta: 0:08:16  lr: 0.000026  min_lr: 0.000000  loss: 4.9981 (5.0104)  class_acc: 0.0000 (0.0218)  loss_scale: 32768.0000 (102894.9632)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0006  max mem: 15572
Epoch: [1]  [ 580/1404]  eta: 0:08:10  lr: 0.000026  min_lr: 0.000000  loss: 4.9754 (5.0110)  class_acc: 0.0417 (0.0225)  loss_scale: 32768.0000 (101687.9587)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0006  max mem: 15572
Epoch: [1]  [ 590/1404]  eta: 0:08:04  lr: 0.000027  min_lr: 0.000000  loss: 4.9940 (5.0107)  class_acc: 0.0000 (0.0228)  loss_scale: 32768.0000 (100521.8003)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0006  max mem: 15572
[2025-01-16 20:35:04,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=6, lr=[2.586922083856223e-07, 2.586922083856223e-07, 3.695602976937462e-07, 3.695602976937462e-07, 5.279432824196375e-07, 5.279432824196375e-07, 7.542046891709107e-07, 7.542046891709107e-07, 1.0774352702441582e-06, 1.0774352702441582e-06, 1.5391932432059404e-06, 1.5391932432059404e-06, 2.1988474902942005e-06, 2.1988474902942005e-06, 3.141210700420287e-06, 3.141210700420287e-06, 4.487443857743267e-06, 4.487443857743267e-06, 6.410634082490383e-06, 6.410634082490383e-06, 9.158048689271974e-06, 9.158048689271974e-06, 1.3082926698959966e-05, 1.3082926698959966e-05, 1.8689895284228524e-05, 1.8689895284228524e-05, 2.669985040604075e-05, 2.669985040604075e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 20:35:04,213] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=46.80541617956605, CurrSamplesPerSec=51.11049528236025, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [ 600/1404]  eta: 0:07:57  lr: 0.000027  min_lr: 0.000000  loss: 4.9508 (5.0102)  class_acc: 0.0000 (0.0234)  loss_scale: 32768.0000 (99394.4493)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0006  max mem: 15572
Epoch: [1]  [ 610/1404]  eta: 0:07:51  lr: 0.000027  min_lr: 0.000000  loss: 4.9715 (5.0095)  class_acc: 0.0000 (0.0234)  loss_scale: 32768.0000 (98304.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0006  max mem: 15572
Epoch: [1]  [ 620/1404]  eta: 0:07:45  lr: 0.000027  min_lr: 0.000000  loss: 5.0009 (5.0100)  class_acc: 0.0000 (0.0233)  loss_scale: 32768.0000 (97248.6699)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0006  max mem: 15572
Epoch: [1]  [ 630/1404]  eta: 0:07:40  lr: 0.000027  min_lr: 0.000000  loss: 4.9828 (5.0094)  class_acc: 0.0000 (0.0234)  loss_scale: 32768.0000 (96226.7892)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0005  max mem: 15572
Epoch: [1]  [ 640/1404]  eta: 0:07:34  lr: 0.000027  min_lr: 0.000000  loss: 4.9828 (5.0087)  class_acc: 0.0000 (0.0234)  loss_scale: 32768.0000 (95236.7925)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0039  max mem: 15572
Epoch: [1]  [ 650/1404]  eta: 0:07:28  lr: 0.000027  min_lr: 0.000000  loss: 4.9883 (5.0096)  class_acc: 0.0000 (0.0230)  loss_scale: 32768.0000 (94277.2104)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0040  max mem: 15572
Epoch: [1]  [ 660/1404]  eta: 0:07:21  lr: 0.000028  min_lr: 0.000000  loss: 4.9702 (5.0088)  class_acc: 0.0000 (0.0233)  loss_scale: 32768.0000 (93346.6626)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0006  max mem: 15572
Epoch: [1]  [ 670/1404]  eta: 0:07:15  lr: 0.000028  min_lr: 0.000000  loss: 4.9840 (5.0086)  class_acc: 0.0417 (0.0237)  loss_scale: 32768.0000 (92443.8510)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0005  max mem: 15572
Epoch: [1]  [ 680/1404]  eta: 0:07:08  lr: 0.000028  min_lr: 0.000000  loss: 5.0154 (5.0084)  class_acc: 0.0417 (0.0242)  loss_scale: 32768.0000 (91567.5536)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0006  max mem: 15572
[2025-01-16 20:35:56,602] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:35:56,603] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 20:35:56,663] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:35:56,664] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [ 690/1404]  eta: 0:07:03  lr: 0.000028  min_lr: 0.000000  loss: 4.9791 (5.0072)  class_acc: 0.0417 (0.0244)  loss_scale: 32768.0000 (90906.3039)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0007  max mem: 15572
Epoch: [1]  [ 700/1404]  eta: 0:06:58  lr: 0.000028  min_lr: 0.000000  loss: 4.9254 (5.0065)  class_acc: 0.0000 (0.0244)  loss_scale: 65536.0000 (90544.3880)  weight_decay: 0.0500 (0.0500)  time: 0.6818  data: 0.0006  max mem: 15572
Epoch: [1]  [ 710/1404]  eta: 0:06:52  lr: 0.000028  min_lr: 0.000000  loss: 4.9599 (5.0056)  class_acc: 0.0000 (0.0244)  loss_scale: 65536.0000 (90192.6526)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.0006  max mem: 15572
Epoch: [1]  [ 720/1404]  eta: 0:06:46  lr: 0.000028  min_lr: 0.000000  loss: 5.0353 (5.0070)  class_acc: 0.0000 (0.0244)  loss_scale: 65536.0000 (89850.6741)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0006  max mem: 15572
Epoch: [1]  [ 730/1404]  eta: 0:06:40  lr: 0.000029  min_lr: 0.000000  loss: 5.0353 (5.0061)  class_acc: 0.0000 (0.0245)  loss_scale: 65536.0000 (89518.0520)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0006  max mem: 15572
Epoch: [1]  [ 740/1404]  eta: 0:06:34  lr: 0.000029  min_lr: 0.000000  loss: 4.8995 (5.0047)  class_acc: 0.0000 (0.0243)  loss_scale: 65536.0000 (89194.4076)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0006  max mem: 15572
Epoch: [1]  [ 750/1404]  eta: 0:06:27  lr: 0.000029  min_lr: 0.000000  loss: 4.9482 (5.0042)  class_acc: 0.0000 (0.0245)  loss_scale: 65536.0000 (88879.3822)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0005  max mem: 15572
Epoch: [1]  [ 760/1404]  eta: 0:06:22  lr: 0.000029  min_lr: 0.000000  loss: 4.9482 (5.0033)  class_acc: 0.0417 (0.0249)  loss_scale: 65536.0000 (88572.6360)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0005  max mem: 15572
Epoch: [1]  [ 770/1404]  eta: 0:06:15  lr: 0.000029  min_lr: 0.000000  loss: 4.9181 (5.0026)  class_acc: 0.0417 (0.0251)  loss_scale: 65536.0000 (88273.8470)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0005  max mem: 15572
Epoch: [1]  [ 780/1404]  eta: 0:06:10  lr: 0.000029  min_lr: 0.000000  loss: 4.9661 (5.0029)  class_acc: 0.0417 (0.0253)  loss_scale: 65536.0000 (87982.7093)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0006  max mem: 15572
Epoch: [1]  [ 790/1404]  eta: 0:06:04  lr: 0.000029  min_lr: 0.000000  loss: 5.0019 (5.0026)  class_acc: 0.0417 (0.0256)  loss_scale: 65536.0000 (87698.9330)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.0006  max mem: 15572
Epoch: [1]  [ 800/1404]  eta: 0:05:58  lr: 0.000029  min_lr: 0.000000  loss: 5.0099 (5.0027)  class_acc: 0.0000 (0.0255)  loss_scale: 65536.0000 (87422.2422)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0104  max mem: 15572
Epoch: [1]  [ 810/1404]  eta: 0:05:52  lr: 0.000030  min_lr: 0.000000  loss: 4.9922 (5.0021)  class_acc: 0.0417 (0.0260)  loss_scale: 65536.0000 (87152.3748)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.0104  max mem: 15572
[2025-01-16 20:37:13,905] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:37:13,905] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 20:37:13,927] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:37:13,927] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 20:37:14,370] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2220
[2025-01-16 20:37:14,371] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 20:37:14,373] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2220
[2025-01-16 20:37:14,373] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 20:37:14,374] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [ 820/1404]  eta: 0:05:47  lr: 0.000030  min_lr: 0.000000  loss: 4.9185 (5.0012)  class_acc: 0.0417 (0.0261)  loss_scale: 65536.0000 (86968.9062)  weight_decay: 0.0500 (0.0500)  time: 0.6769  data: 0.0006  max mem: 15572
[2025-01-16 20:37:21,684] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2231
[2025-01-16 20:37:21,684] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:37:21,684] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 20:37:21,685] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2231
[2025-01-16 20:37:21,685] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [1]  [ 830/1404]  eta: 0:05:40  lr: 0.000030  min_lr: 0.000000  loss: 4.9279 (5.0010)  class_acc: 0.0000 (0.0261)  loss_scale: 65536.0000 (86553.2611)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.0006  max mem: 15572
Epoch: [1]  [ 840/1404]  eta: 0:05:34  lr: 0.000030  min_lr: 0.000000  loss: 4.9440 (5.0001)  class_acc: 0.0000 (0.0264)  loss_scale: 32768.0000 (85913.7218)  weight_decay: 0.0500 (0.0500)  time: 0.4862  data: 0.0006  max mem: 15572
Epoch: [1]  [ 850/1404]  eta: 0:05:27  lr: 0.000030  min_lr: 0.000000  loss: 4.9316 (4.9994)  class_acc: 0.0417 (0.0267)  loss_scale: 32768.0000 (85289.2127)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.0007  max mem: 15572
Epoch: [1]  [ 860/1404]  eta: 0:05:22  lr: 0.000030  min_lr: 0.000000  loss: 4.9316 (4.9987)  class_acc: 0.0417 (0.0270)  loss_scale: 32768.0000 (84679.2102)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0006  max mem: 15572
Epoch: [1]  [ 870/1404]  eta: 0:05:15  lr: 0.000030  min_lr: 0.000000  loss: 4.9119 (4.9975)  class_acc: 0.0417 (0.0269)  loss_scale: 32768.0000 (84083.2147)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0006  max mem: 15572
Epoch: [1]  [ 880/1404]  eta: 0:05:10  lr: 0.000031  min_lr: 0.000000  loss: 4.9504 (4.9976)  class_acc: 0.0000 (0.0269)  loss_scale: 32768.0000 (83500.7491)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0006  max mem: 15572
Epoch: [1]  [ 890/1404]  eta: 0:05:04  lr: 0.000031  min_lr: 0.000000  loss: 4.9972 (4.9976)  class_acc: 0.0000 (0.0268)  loss_scale: 32768.0000 (82931.3580)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0006  max mem: 15572
Epoch: [1]  [ 900/1404]  eta: 0:04:58  lr: 0.000031  min_lr: 0.000000  loss: 4.9972 (4.9972)  class_acc: 0.0000 (0.0269)  loss_scale: 32768.0000 (82374.6060)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.0007  max mem: 15572
Epoch: [1]  [ 910/1404]  eta: 0:04:52  lr: 0.000031  min_lr: 0.000000  loss: 5.0031 (4.9976)  class_acc: 0.0000 (0.0268)  loss_scale: 32768.0000 (81830.0768)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0010  max mem: 15572
Epoch: [1]  [ 920/1404]  eta: 0:04:45  lr: 0.000031  min_lr: 0.000000  loss: 5.0031 (4.9974)  class_acc: 0.0000 (0.0267)  loss_scale: 32768.0000 (81297.3724)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0010  max mem: 15572
Epoch: [1]  [ 930/1404]  eta: 0:04:39  lr: 0.000031  min_lr: 0.000000  loss: 4.9321 (4.9971)  class_acc: 0.0000 (0.0268)  loss_scale: 32768.0000 (80776.1117)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.0006  max mem: 15572
Epoch: [1]  [ 940/1404]  eta: 0:04:33  lr: 0.000031  min_lr: 0.000000  loss: 4.9272 (4.9970)  class_acc: 0.0417 (0.0272)  loss_scale: 32768.0000 (80265.9299)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0006  max mem: 15572
Epoch: [1]  [ 950/1404]  eta: 0:04:28  lr: 0.000031  min_lr: 0.000000  loss: 4.9583 (4.9966)  class_acc: 0.0417 (0.0275)  loss_scale: 32768.0000 (79766.4774)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0615  max mem: 15572
[2025-01-16 20:38:34,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:38:34,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 20:38:34,572] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:38:34,573] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [ 960/1404]  eta: 0:04:22  lr: 0.000032  min_lr: 0.000000  loss: 4.9583 (4.9966)  class_acc: 0.0417 (0.0277)  loss_scale: 32768.0000 (79447.9084)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.1332  max mem: 15572
Epoch: [1]  [ 970/1404]  eta: 0:04:16  lr: 0.000032  min_lr: 0.000000  loss: 4.9726 (4.9961)  class_acc: 0.0000 (0.0276)  loss_scale: 65536.0000 (79304.6344)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1178  max mem: 15572
Epoch: [1]  [ 980/1404]  eta: 0:04:11  lr: 0.000032  min_lr: 0.000000  loss: 4.9554 (4.9955)  class_acc: 0.0000 (0.0275)  loss_scale: 65536.0000 (79164.2813)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.1198  max mem: 15572
Epoch: [1]  [ 990/1404]  eta: 0:04:04  lr: 0.000032  min_lr: 0.000000  loss: 4.9159 (4.9950)  class_acc: 0.0000 (0.0278)  loss_scale: 65536.0000 (79026.7608)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0744  max mem: 15572
Epoch: [1]  [1000/1404]  eta: 0:03:59  lr: 0.000032  min_lr: 0.000000  loss: 4.9784 (4.9950)  class_acc: 0.0000 (0.0278)  loss_scale: 65536.0000 (78891.9880)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0766  max mem: 15572
Epoch: [1]  [1010/1404]  eta: 0:03:53  lr: 0.000032  min_lr: 0.000000  loss: 5.0004 (4.9946)  class_acc: 0.0000 (0.0278)  loss_scale: 65536.0000 (78759.8813)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1123  max mem: 15572
[2025-01-16 20:39:10,510] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2419
[2025-01-16 20:39:10,510] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2419
[2025-01-16 20:39:10,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:39:10,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:39:10,511] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [1020/1404]  eta: 0:03:47  lr: 0.000032  min_lr: 0.000000  loss: 4.9248 (4.9940)  class_acc: 0.0000 (0.0279)  loss_scale: 65536.0000 (78437.7982)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0622  max mem: 15572
Epoch: [1]  [1030/1404]  eta: 0:03:41  lr: 0.000033  min_lr: 0.000000  loss: 4.9290 (4.9939)  class_acc: 0.0417 (0.0283)  loss_scale: 32768.0000 (77994.8322)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1272  max mem: 15572
Epoch: [1]  [1040/1404]  eta: 0:03:35  lr: 0.000033  min_lr: 0.000000  loss: 5.0200 (4.9940)  class_acc: 0.0417 (0.0283)  loss_scale: 32768.0000 (77560.3766)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1013  max mem: 15572
Epoch: [1]  [1050/1404]  eta: 0:03:29  lr: 0.000033  min_lr: 0.000000  loss: 4.9642 (4.9938)  class_acc: 0.0000 (0.0283)  loss_scale: 32768.0000 (77134.1884)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0293  max mem: 15572
Epoch: [1]  [1060/1404]  eta: 0:03:23  lr: 0.000033  min_lr: 0.000000  loss: 4.9339 (4.9932)  class_acc: 0.0000 (0.0283)  loss_scale: 32768.0000 (76716.0339)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.0781  max mem: 15572
Epoch: [1]  [1070/1404]  eta: 0:03:17  lr: 0.000033  min_lr: 0.000000  loss: 4.9141 (4.9929)  class_acc: 0.0000 (0.0286)  loss_scale: 32768.0000 (76305.6881)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.0493  max mem: 15572
Epoch: [1]  [1080/1404]  eta: 0:03:11  lr: 0.000033  min_lr: 0.000000  loss: 4.9257 (4.9928)  class_acc: 0.0000 (0.0286)  loss_scale: 32768.0000 (75902.9343)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0008  max mem: 15572
Epoch: [1]  [1090/1404]  eta: 0:03:05  lr: 0.000033  min_lr: 0.000000  loss: 4.9552 (4.9924)  class_acc: 0.0000 (0.0284)  loss_scale: 32768.0000 (75507.5637)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0161  max mem: 15572
Epoch: [1]  [1100/1404]  eta: 0:02:59  lr: 0.000033  min_lr: 0.000000  loss: 4.9586 (4.9919)  class_acc: 0.0000 (0.0284)  loss_scale: 32768.0000 (75119.3751)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0159  max mem: 15572
Epoch: [1]  [1110/1404]  eta: 0:02:53  lr: 0.000034  min_lr: 0.000000  loss: 4.9561 (4.9914)  class_acc: 0.0000 (0.0287)  loss_scale: 32768.0000 (74738.1746)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0084  max mem: 15572
Epoch: [1]  [1120/1404]  eta: 0:02:47  lr: 0.000034  min_lr: 0.000000  loss: 4.9491 (4.9913)  class_acc: 0.0000 (0.0287)  loss_scale: 32768.0000 (74363.7752)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0512  max mem: 15572
Epoch: [1]  [1130/1404]  eta: 0:02:41  lr: 0.000034  min_lr: 0.000000  loss: 4.9630 (4.9909)  class_acc: 0.0000 (0.0287)  loss_scale: 32768.0000 (73995.9965)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.0958  max mem: 15572
Epoch: [1]  [1140/1404]  eta: 0:02:35  lr: 0.000034  min_lr: 0.000000  loss: 4.9241 (4.9897)  class_acc: 0.0417 (0.0291)  loss_scale: 32768.0000 (73634.6643)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.0532  max mem: 15572
[2025-01-16 20:40:25,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:40:25,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 20:40:25,126] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:40:25,127] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [1150/1404]  eta: 0:02:29  lr: 0.000034  min_lr: 0.000000  loss: 4.8836 (4.9892)  class_acc: 0.0417 (0.0291)  loss_scale: 32768.0000 (73478.8949)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0009  max mem: 15572
Epoch: [1]  [1160/1404]  eta: 0:02:23  lr: 0.000034  min_lr: 0.000000  loss: 4.9547 (4.9888)  class_acc: 0.0000 (0.0291)  loss_scale: 65536.0000 (73410.4806)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0249  max mem: 15572
Epoch: [1]  [1170/1404]  eta: 0:02:18  lr: 0.000034  min_lr: 0.000000  loss: 4.9987 (4.9886)  class_acc: 0.0417 (0.0292)  loss_scale: 65536.0000 (73343.2348)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1091  max mem: 15572
Epoch: [1]  [1180/1404]  eta: 0:02:12  lr: 0.000035  min_lr: 0.000000  loss: 4.9485 (4.9886)  class_acc: 0.0417 (0.0293)  loss_scale: 65536.0000 (73277.1279)  weight_decay: 0.0500 (0.0500)  time: 0.6492  data: 0.1694  max mem: 15572
Epoch: [1]  [1190/1404]  eta: 0:02:06  lr: 0.000035  min_lr: 0.000000  loss: 4.9485 (4.9884)  class_acc: 0.0417 (0.0294)  loss_scale: 65536.0000 (73212.1310)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1054  max mem: 15572
Epoch: [1]  [1200/1404]  eta: 0:02:00  lr: 0.000035  min_lr: 0.000000  loss: 4.8869 (4.9871)  class_acc: 0.0417 (0.0295)  loss_scale: 65536.0000 (73148.2165)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0390  max mem: 15572
Epoch: [1]  [1210/1404]  eta: 0:01:54  lr: 0.000035  min_lr: 0.000000  loss: 4.9122 (4.9868)  class_acc: 0.0000 (0.0295)  loss_scale: 65536.0000 (73085.3576)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0747  max mem: 15572
Epoch: [1]  [1220/1404]  eta: 0:01:48  lr: 0.000035  min_lr: 0.000000  loss: 4.9678 (4.9867)  class_acc: 0.0417 (0.0299)  loss_scale: 65536.0000 (73023.5283)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0933  max mem: 15572
[2025-01-16 20:41:14,509] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2632
[2025-01-16 20:41:14,510] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:41:14,510] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 20:41:14,510] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2632
[2025-01-16 20:41:14,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [1]  [1230/1404]  eta: 0:01:42  lr: 0.000035  min_lr: 0.000000  loss: 4.9883 (4.9868)  class_acc: 0.0417 (0.0301)  loss_scale: 65536.0000 (72882.8465)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.1060  max mem: 15572
Epoch: [1]  [1240/1404]  eta: 0:01:36  lr: 0.000035  min_lr: 0.000000  loss: 4.9500 (4.9861)  class_acc: 0.0000 (0.0302)  loss_scale: 32768.0000 (72559.6003)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1220  max mem: 15572
Epoch: [1]  [1250/1404]  eta: 0:01:30  lr: 0.000035  min_lr: 0.000000  loss: 4.8952 (4.9855)  class_acc: 0.0000 (0.0306)  loss_scale: 32768.0000 (72241.5220)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1198  max mem: 15572
Epoch: [1]  [1260/1404]  eta: 0:01:25  lr: 0.000036  min_lr: 0.000000  loss: 4.8958 (4.9847)  class_acc: 0.0000 (0.0306)  loss_scale: 32768.0000 (71928.4885)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1091  max mem: 15572
Epoch: [1]  [1270/1404]  eta: 0:01:19  lr: 0.000036  min_lr: 0.000000  loss: 4.9033 (4.9844)  class_acc: 0.0000 (0.0306)  loss_scale: 32768.0000 (71620.3808)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0601  max mem: 15572
Epoch: [1]  [1280/1404]  eta: 0:01:13  lr: 0.000036  min_lr: 0.000000  loss: 4.9088 (4.9841)  class_acc: 0.0000 (0.0306)  loss_scale: 32768.0000 (71317.0835)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0432  max mem: 15572
Epoch: [1]  [1290/1404]  eta: 0:01:07  lr: 0.000036  min_lr: 0.000000  loss: 4.8789 (4.9832)  class_acc: 0.0000 (0.0307)  loss_scale: 32768.0000 (71018.4849)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.1119  max mem: 15572
Epoch: [1]  [1300/1404]  eta: 0:01:01  lr: 0.000036  min_lr: 0.000000  loss: 4.9065 (4.9836)  class_acc: 0.0000 (0.0310)  loss_scale: 32768.0000 (70724.4766)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0868  max mem: 15572
Epoch: [1]  [1310/1404]  eta: 0:00:55  lr: 0.000036  min_lr: 0.000000  loss: 4.9870 (4.9833)  class_acc: 0.0417 (0.0312)  loss_scale: 32768.0000 (70434.9535)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0544  max mem: 15572
Epoch: [1]  [1320/1404]  eta: 0:00:49  lr: 0.000036  min_lr: 0.000000  loss: 4.8735 (4.9822)  class_acc: 0.0417 (0.0314)  loss_scale: 32768.0000 (70149.8138)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0734  max mem: 15572
Epoch: [1]  [1330/1404]  eta: 0:00:43  lr: 0.000037  min_lr: 0.000000  loss: 4.8487 (4.9814)  class_acc: 0.0417 (0.0316)  loss_scale: 32768.0000 (69868.9587)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0589  max mem: 15572
Epoch: [1]  [1340/1404]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000000  loss: 4.9085 (4.9812)  class_acc: 0.0000 (0.0317)  loss_scale: 32768.0000 (69592.2923)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0473  max mem: 15572
Epoch: [1]  [1350/1404]  eta: 0:00:31  lr: 0.000037  min_lr: 0.000000  loss: 4.9400 (4.9802)  class_acc: 0.0000 (0.0320)  loss_scale: 32768.0000 (69319.7217)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0738  max mem: 15572
[2025-01-16 20:42:30,442] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:42:30,443] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 20:42:30,496] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:42:30,497] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [1360/1404]  eta: 0:00:25  lr: 0.000037  min_lr: 0.000000  loss: 4.9400 (4.9801)  class_acc: 0.0417 (0.0321)  loss_scale: 32768.0000 (69147.4622)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1085  max mem: 15572
Epoch: [1]  [1370/1404]  eta: 0:00:20  lr: 0.000037  min_lr: 0.000000  loss: 4.9470 (4.9802)  class_acc: 0.0417 (0.0322)  loss_scale: 65536.0000 (69121.1204)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1133  max mem: 15572
Epoch: [1]  [1380/1404]  eta: 0:00:14  lr: 0.000037  min_lr: 0.000000  loss: 4.9116 (4.9793)  class_acc: 0.0417 (0.0323)  loss_scale: 65536.0000 (69095.1600)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0713  max mem: 15572
[2025-01-16 20:42:49,466] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2794
[2025-01-16 20:42:49,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:42:49,467] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [1390/1404]  eta: 0:00:08  lr: 0.000037  min_lr: 0.000000  loss: 4.8674 (4.9784)  class_acc: 0.0417 (0.0327)  loss_scale: 65536.0000 (69046.0158)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0143  max mem: 15572
[2025-01-16 20:42:49,477] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2794
[2025-01-16 20:42:49,477] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [1]  [1400/1404]  eta: 0:00:02  lr: 0.000037  min_lr: 0.000000  loss: 4.9040 (4.9779)  class_acc: 0.0417 (0.0328)  loss_scale: 32768.0000 (68787.0721)  weight_decay: 0.0500 (0.0500)  time: 0.4499  data: 0.0140  max mem: 15572
Epoch: [1]  [1403/1404]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000000  loss: 4.9054 (4.9779)  class_acc: 0.0417 (0.0328)  loss_scale: 32768.0000 (68710.1083)  weight_decay: 0.0500 (0.0500)  time: 0.4305  data: 0.0140  max mem: 15572
Epoch: [1] Total time: 0:13:45 (0.5876 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000000  loss: 4.9054 (4.9765)  class_acc: 0.0417 (0.0331)  loss_scale: 32768.0000 (68710.1083)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:08:39  loss: 4.9813 (4.9813)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.8165  data: 3.6303  max mem: 15572
Val:  [ 10/136]  eta: 0:01:35  loss: 4.9373 (4.8575)  acc1: 0.0000 (7.5758)  acc5: 0.0000 (16.1616)  time: 0.7541  data: 0.5553  max mem: 15572
Val:  [ 20/136]  eta: 0:01:01  loss: 4.6847 (4.6929)  acc1: 0.0000 (8.9947)  acc5: 5.5556 (28.3069)  time: 0.3658  data: 0.1616  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 4.2569 (4.6536)  acc1: 5.5556 (10.2151)  acc5: 44.4444 (29.9283)  time: 0.3151  data: 0.1055  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 4.4320 (4.6397)  acc1: 0.0000 (11.5176)  acc5: 11.1111 (28.7263)  time: 0.3473  data: 0.1383  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 4.8047 (4.7133)  acc1: 0.0000 (9.2593)  acc5: 0.0000 (23.7473)  time: 0.3682  data: 0.1698  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 4.9223 (4.7644)  acc1: 0.0000 (7.7413)  acc5: 0.0000 (20.4007)  time: 0.3599  data: 0.1596  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 4.8509 (4.7318)  acc1: 0.0000 (8.1377)  acc5: 0.0000 (23.3959)  time: 0.3541  data: 0.1511  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 4.5820 (4.7237)  acc1: 0.0000 (7.9561)  acc5: 5.5556 (22.7023)  time: 0.3482  data: 0.1530  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 4.6938 (4.7376)  acc1: 0.0000 (7.1429)  acc5: 5.5556 (21.0623)  time: 0.3296  data: 0.1390  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 4.8937 (4.7664)  acc1: 0.0000 (6.4356)  acc5: 0.0000 (18.9769)  time: 0.3628  data: 0.1635  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 4.8587 (4.7707)  acc1: 0.0000 (5.8559)  acc5: 0.0000 (18.0180)  time: 0.3550  data: 0.1516  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.7546 (4.7664)  acc1: 0.0000 (5.3719)  acc5: 5.5556 (17.9982)  time: 0.3563  data: 0.1577  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 4.6890 (4.7687)  acc1: 0.0000 (5.9796)  acc5: 11.1111 (18.4902)  time: 0.3097  data: 0.1257  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 4.8913 (4.7678)  acc1: 0.0000 (5.8149)  acc5: 8.3333 (18.8370)  time: 0.2322  data: 0.0637  max mem: 15572
Val: Total time: 0:00:49 (0.3658 s / it)
* Acc@1 5.856 Acc@5 18.346 loss 4.776
Accuracy of the network on the 4883 val videos: 5.9%
[2025-01-16 20:43:44,668] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 20:43:44,670] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 20:43:44,670] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 20:43:44,670] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 20:43:47,183] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 20:43:47,183] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 5.86%
Epoch: [2]  [   0/1404]  eta: 2:32:05  lr: 0.000038  min_lr: 0.000000  loss: 4.9442 (4.9442)  class_acc: 0.1250 (0.1250)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.4998  data: 4.9514  max mem: 15572
Epoch: [2]  [  10/1404]  eta: 0:27:28  lr: 0.000038  min_lr: 0.000000  loss: 4.9442 (4.9352)  class_acc: 0.0417 (0.0530)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1826  data: 0.4927  max mem: 15572
Epoch: [2]  [  20/1404]  eta: 0:20:50  lr: 0.000038  min_lr: 0.000000  loss: 4.8941 (4.9355)  class_acc: 0.0000 (0.0377)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.0236  max mem: 15572
Epoch: [2]  [  30/1404]  eta: 0:18:57  lr: 0.000038  min_lr: 0.000000  loss: 4.9273 (4.9320)  class_acc: 0.0000 (0.0376)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.0005  max mem: 15572
Epoch: [2]  [  40/1404]  eta: 0:17:03  lr: 0.000038  min_lr: 0.000000  loss: 4.9363 (4.9342)  class_acc: 0.0000 (0.0346)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0006  max mem: 15572
Epoch: [2]  [  50/1404]  eta: 0:16:15  lr: 0.000038  min_lr: 0.000000  loss: 4.8679 (4.9133)  class_acc: 0.0417 (0.0408)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0006  max mem: 15572
Epoch: [2]  [  60/1404]  eta: 0:15:49  lr: 0.000038  min_lr: 0.000000  loss: 4.8629 (4.9116)  class_acc: 0.0417 (0.0437)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.0005  max mem: 15572
Epoch: [2]  [  70/1404]  eta: 0:15:21  lr: 0.000038  min_lr: 0.000000  loss: 4.9393 (4.9152)  class_acc: 0.0417 (0.0452)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0006  max mem: 15572
Epoch: [2]  [  80/1404]  eta: 0:14:59  lr: 0.000039  min_lr: 0.000000  loss: 4.8647 (4.9056)  class_acc: 0.0417 (0.0478)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.0007  max mem: 15572
Epoch: [2]  [  90/1404]  eta: 0:14:32  lr: 0.000039  min_lr: 0.000000  loss: 4.8277 (4.8998)  class_acc: 0.0417 (0.0472)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0005  max mem: 15572
Epoch: [2]  [ 100/1404]  eta: 0:14:22  lr: 0.000039  min_lr: 0.000000  loss: 4.8113 (4.8916)  class_acc: 0.0417 (0.0474)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0004  max mem: 15572
Epoch: [2]  [ 110/1404]  eta: 0:14:03  lr: 0.000039  min_lr: 0.000000  loss: 4.8778 (4.8938)  class_acc: 0.0000 (0.0462)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0006  max mem: 15572
[2025-01-16 20:45:02,136] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:45:02,137] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 20:45:02,169] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:45:02,170] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 120/1404]  eta: 0:13:57  lr: 0.000039  min_lr: 0.000000  loss: 4.8627 (4.8896)  class_acc: 0.0000 (0.0479)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0007  max mem: 15572
Epoch: [2]  [ 130/1404]  eta: 0:13:37  lr: 0.000039  min_lr: 0.000000  loss: 4.9042 (4.8961)  class_acc: 0.0000 (0.0464)  loss_scale: 65536.0000 (36770.1985)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0006  max mem: 15572
Epoch: [2]  [ 140/1404]  eta: 0:13:18  lr: 0.000039  min_lr: 0.000000  loss: 4.9219 (4.8942)  class_acc: 0.0000 (0.0461)  loss_scale: 65536.0000 (38810.3262)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0005  max mem: 15572
Epoch: [2]  [ 150/1404]  eta: 0:13:09  lr: 0.000040  min_lr: 0.000000  loss: 4.8422 (4.8919)  class_acc: 0.0417 (0.0461)  loss_scale: 65536.0000 (40580.2384)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0505  max mem: 15572
Epoch: [2]  [ 160/1404]  eta: 0:12:58  lr: 0.000040  min_lr: 0.000000  loss: 4.9205 (4.8927)  class_acc: 0.0000 (0.0450)  loss_scale: 65536.0000 (42130.2857)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.0826  max mem: 15572
Epoch: [2]  [ 170/1404]  eta: 0:12:43  lr: 0.000040  min_lr: 0.000000  loss: 4.8775 (4.8896)  class_acc: 0.0000 (0.0436)  loss_scale: 65536.0000 (43499.0409)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0421  max mem: 15572
Epoch: [2]  [ 180/1404]  eta: 0:12:36  lr: 0.000040  min_lr: 0.000000  loss: 4.8775 (4.8906)  class_acc: 0.0417 (0.0451)  loss_scale: 65536.0000 (44716.5525)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0495  max mem: 15572
Epoch: [2]  [ 190/1404]  eta: 0:12:34  lr: 0.000040  min_lr: 0.000000  loss: 4.8629 (4.8880)  class_acc: 0.0417 (0.0452)  loss_scale: 65536.0000 (45806.5759)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.0741  max mem: 15572
[2025-01-16 20:45:46,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=11, lr=[3.8810301798323227e-07, 3.8810301798323227e-07, 5.54432882833189e-07, 5.54432882833189e-07, 7.920469754759844e-07, 7.920469754759844e-07, 1.1314956792514063e-06, 1.1314956792514063e-06, 1.6164223989305805e-06, 1.6164223989305805e-06, 2.309174855615115e-06, 2.309174855615115e-06, 3.2988212223073076e-06, 3.2988212223073076e-06, 4.712601746153297e-06, 4.712601746153297e-06, 6.732288208790424e-06, 6.732288208790424e-06, 9.617554583986322e-06, 9.617554583986322e-06, 1.373936369140903e-05, 1.373936369140903e-05, 1.9627662416298616e-05, 1.9627662416298616e-05, 2.8039517737569454e-05, 2.8039517737569454e-05, 4.005645391081351e-05, 4.005645391081351e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 20:45:46,616] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=46.695049441327384, CurrSamplesPerSec=62.871807171123635, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [ 200/1404]  eta: 0:12:23  lr: 0.000040  min_lr: 0.000000  loss: 4.8756 (4.8918)  class_acc: 0.0417 (0.0454)  loss_scale: 65536.0000 (46788.1393)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0486  max mem: 15572
[2025-01-16 20:45:55,030] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3013
[2025-01-16 20:45:55,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:45:55,030] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 20:45:55,039] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3013
[2025-01-16 20:45:55,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [ 210/1404]  eta: 0:12:17  lr: 0.000040  min_lr: 0.000000  loss: 4.8937 (4.8908)  class_acc: 0.0417 (0.0450)  loss_scale: 65536.0000 (46744.8720)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0852  max mem: 15572
Epoch: [2]  [ 220/1404]  eta: 0:12:07  lr: 0.000040  min_lr: 0.000000  loss: 4.8003 (4.8862)  class_acc: 0.0417 (0.0445)  loss_scale: 32768.0000 (46112.4344)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0787  max mem: 15572
Epoch: [2]  [ 230/1404]  eta: 0:11:56  lr: 0.000041  min_lr: 0.000000  loss: 4.8385 (4.8870)  class_acc: 0.0417 (0.0444)  loss_scale: 32768.0000 (45534.7532)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0082  max mem: 15572
Epoch: [2]  [ 240/1404]  eta: 0:11:47  lr: 0.000041  min_lr: 0.000000  loss: 4.8774 (4.8869)  class_acc: 0.0417 (0.0446)  loss_scale: 32768.0000 (45005.0124)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0200  max mem: 15572
Epoch: [2]  [ 250/1404]  eta: 0:11:45  lr: 0.000041  min_lr: 0.000000  loss: 4.9048 (4.8887)  class_acc: 0.0417 (0.0440)  loss_scale: 32768.0000 (44517.4821)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.0271  max mem: 15572
Epoch: [2]  [ 260/1404]  eta: 0:11:36  lr: 0.000041  min_lr: 0.000000  loss: 4.8820 (4.8872)  class_acc: 0.0417 (0.0455)  loss_scale: 32768.0000 (44067.3103)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.0078  max mem: 15572
Epoch: [2]  [ 270/1404]  eta: 0:11:29  lr: 0.000041  min_lr: 0.000000  loss: 4.8788 (4.8889)  class_acc: 0.0833 (0.0458)  loss_scale: 32768.0000 (43650.3616)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0005  max mem: 15572
Epoch: [2]  [ 280/1404]  eta: 0:11:20  lr: 0.000041  min_lr: 0.000000  loss: 4.9139 (4.8886)  class_acc: 0.0417 (0.0463)  loss_scale: 32768.0000 (43263.0890)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0006  max mem: 15572
Epoch: [2]  [ 290/1404]  eta: 0:11:14  lr: 0.000041  min_lr: 0.000000  loss: 4.8964 (4.8875)  class_acc: 0.0417 (0.0468)  loss_scale: 32768.0000 (42902.4330)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0007  max mem: 15572
Epoch: [2]  [ 300/1404]  eta: 0:11:04  lr: 0.000042  min_lr: 0.000000  loss: 4.8873 (4.8883)  class_acc: 0.0417 (0.0461)  loss_scale: 32768.0000 (42565.7409)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0006  max mem: 15572
Epoch: [2]  [ 310/1404]  eta: 0:10:59  lr: 0.000042  min_lr: 0.000000  loss: 4.8892 (4.8905)  class_acc: 0.0417 (0.0465)  loss_scale: 32768.0000 (42250.7010)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.0077  max mem: 15572
Epoch: [2]  [ 320/1404]  eta: 0:10:51  lr: 0.000042  min_lr: 0.000000  loss: 4.8621 (4.8902)  class_acc: 0.0417 (0.0466)  loss_scale: 32768.0000 (41955.2897)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0231  max mem: 15572
Epoch: [2]  [ 330/1404]  eta: 0:10:44  lr: 0.000042  min_lr: 0.000000  loss: 4.8621 (4.8912)  class_acc: 0.0000 (0.0461)  loss_scale: 32768.0000 (41677.7281)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0364  max mem: 15572
[2025-01-16 20:47:09,108] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:47:09,108] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 20:47:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:47:09,154] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 340/1404]  eta: 0:10:38  lr: 0.000042  min_lr: 0.000000  loss: 4.8948 (4.8915)  class_acc: 0.0000 (0.0455)  loss_scale: 32768.0000 (42089.1026)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.0210  max mem: 15572
Epoch: [2]  [ 350/1404]  eta: 0:10:31  lr: 0.000042  min_lr: 0.000000  loss: 4.7943 (4.8886)  class_acc: 0.0000 (0.0452)  loss_scale: 65536.0000 (42757.1054)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0231  max mem: 15572
Epoch: [2]  [ 360/1404]  eta: 0:10:25  lr: 0.000042  min_lr: 0.000000  loss: 4.8833 (4.8889)  class_acc: 0.0417 (0.0461)  loss_scale: 65536.0000 (43388.0997)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0516  max mem: 15572
[2025-01-16 20:47:26,390] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3171
[2025-01-16 20:47:26,390] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:47:26,390] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 20:47:26,395] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3171
[2025-01-16 20:47:26,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [ 370/1404]  eta: 0:10:19  lr: 0.000042  min_lr: 0.000000  loss: 4.9599 (4.8914)  class_acc: 0.0417 (0.0468)  loss_scale: 65536.0000 (43278.4906)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0897  max mem: 15572
Epoch: [2]  [ 380/1404]  eta: 0:10:14  lr: 0.000043  min_lr: 0.000000  loss: 4.9311 (4.8894)  class_acc: 0.0417 (0.0475)  loss_scale: 32768.0000 (43002.6247)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1106  max mem: 15572
Epoch: [2]  [ 390/1404]  eta: 0:10:06  lr: 0.000043  min_lr: 0.000000  loss: 4.7758 (4.8878)  class_acc: 0.0833 (0.0480)  loss_scale: 32768.0000 (42740.8696)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0670  max mem: 15572
Epoch: [2]  [ 400/1404]  eta: 0:10:01  lr: 0.000043  min_lr: 0.000000  loss: 4.7772 (4.8873)  class_acc: 0.0833 (0.0486)  loss_scale: 32768.0000 (42492.1696)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.0978  max mem: 15572
Epoch: [2]  [ 410/1404]  eta: 0:09:56  lr: 0.000043  min_lr: 0.000000  loss: 4.7772 (4.8854)  class_acc: 0.0417 (0.0482)  loss_scale: 32768.0000 (42255.5718)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.1475  max mem: 15572
Epoch: [2]  [ 420/1404]  eta: 0:09:50  lr: 0.000043  min_lr: 0.000000  loss: 4.8352 (4.8851)  class_acc: 0.0417 (0.0488)  loss_scale: 32768.0000 (42030.2138)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1346  max mem: 15572
Epoch: [2]  [ 430/1404]  eta: 0:09:43  lr: 0.000043  min_lr: 0.000000  loss: 4.8738 (4.8842)  class_acc: 0.0417 (0.0486)  loss_scale: 32768.0000 (41815.3132)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0897  max mem: 15572
Epoch: [2]  [ 440/1404]  eta: 0:09:36  lr: 0.000043  min_lr: 0.000000  loss: 4.8790 (4.8842)  class_acc: 0.0417 (0.0488)  loss_scale: 32768.0000 (41610.1587)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0605  max mem: 15572
Epoch: [2]  [ 450/1404]  eta: 0:09:29  lr: 0.000044  min_lr: 0.000000  loss: 4.8737 (4.8842)  class_acc: 0.0417 (0.0489)  loss_scale: 32768.0000 (41414.1020)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0388  max mem: 15572
Epoch: [2]  [ 460/1404]  eta: 0:09:23  lr: 0.000044  min_lr: 0.000000  loss: 4.9052 (4.8856)  class_acc: 0.0417 (0.0489)  loss_scale: 32768.0000 (41226.5510)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0418  max mem: 15572
Epoch: [2]  [ 470/1404]  eta: 0:09:15  lr: 0.000044  min_lr: 0.000000  loss: 4.8677 (4.8852)  class_acc: 0.0833 (0.0498)  loss_scale: 32768.0000 (41046.9639)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0525  max mem: 15572
Epoch: [2]  [ 480/1404]  eta: 0:09:09  lr: 0.000044  min_lr: 0.000000  loss: 4.8482 (4.8836)  class_acc: 0.0833 (0.0500)  loss_scale: 32768.0000 (40874.8441)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0455  max mem: 15572
Epoch: [2]  [ 490/1404]  eta: 0:09:02  lr: 0.000044  min_lr: 0.000000  loss: 4.8641 (4.8833)  class_acc: 0.0833 (0.0504)  loss_scale: 32768.0000 (40709.7352)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0596  max mem: 15572
[2025-01-16 20:48:40,680] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:48:40,681] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 20:48:40,694] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:48:40,694] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 500/1404]  eta: 0:08:58  lr: 0.000044  min_lr: 0.000000  loss: 4.8844 (4.8839)  class_acc: 0.0833 (0.0509)  loss_scale: 32768.0000 (41139.8643)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1079  max mem: 15572
[2025-01-16 20:48:46,756] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3310
[2025-01-16 20:48:46,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:48:46,800] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3310
[2025-01-16 20:48:46,800] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:48:46,801] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 510/1404]  eta: 0:08:51  lr: 0.000044  min_lr: 0.000000  loss: 4.8537 (4.8825)  class_acc: 0.0417 (0.0509)  loss_scale: 32768.0000 (41040.1566)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1155  max mem: 15572
Epoch: [2]  [ 520/1404]  eta: 0:08:46  lr: 0.000044  min_lr: 0.000000  loss: 4.7607 (4.8813)  class_acc: 0.0417 (0.0507)  loss_scale: 32768.0000 (40881.3820)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0801  max mem: 15572
Epoch: [2]  [ 530/1404]  eta: 0:08:40  lr: 0.000045  min_lr: 0.000000  loss: 4.8013 (4.8806)  class_acc: 0.0417 (0.0508)  loss_scale: 32768.0000 (40728.5876)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1201  max mem: 15572
Epoch: [2]  [ 540/1404]  eta: 0:08:34  lr: 0.000045  min_lr: 0.000000  loss: 4.8275 (4.8805)  class_acc: 0.0417 (0.0509)  loss_scale: 32768.0000 (40581.4418)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1030  max mem: 15572
Epoch: [2]  [ 550/1404]  eta: 0:08:28  lr: 0.000045  min_lr: 0.000000  loss: 4.8408 (4.8798)  class_acc: 0.0417 (0.0514)  loss_scale: 32768.0000 (40439.6370)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0788  max mem: 15572
Epoch: [2]  [ 560/1404]  eta: 0:08:21  lr: 0.000045  min_lr: 0.000000  loss: 4.7971 (4.8789)  class_acc: 0.0417 (0.0515)  loss_scale: 32768.0000 (40302.8877)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0953  max mem: 15572
Epoch: [2]  [ 570/1404]  eta: 0:08:16  lr: 0.000045  min_lr: 0.000000  loss: 4.8011 (4.8788)  class_acc: 0.0417 (0.0514)  loss_scale: 32768.0000 (40170.9282)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0912  max mem: 15572
Epoch: [2]  [ 580/1404]  eta: 0:08:09  lr: 0.000045  min_lr: 0.000000  loss: 4.9068 (4.8792)  class_acc: 0.0417 (0.0515)  loss_scale: 32768.0000 (40043.5112)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0513  max mem: 15572
Epoch: [2]  [ 590/1404]  eta: 0:08:03  lr: 0.000045  min_lr: 0.000000  loss: 4.8579 (4.8783)  class_acc: 0.0417 (0.0520)  loss_scale: 32768.0000 (39920.4061)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0759  max mem: 15572
Epoch: [2]  [ 600/1404]  eta: 0:07:57  lr: 0.000046  min_lr: 0.000000  loss: 4.8297 (4.8771)  class_acc: 0.0417 (0.0518)  loss_scale: 32768.0000 (39801.3977)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1089  max mem: 15572
Epoch: [2]  [ 610/1404]  eta: 0:07:52  lr: 0.000046  min_lr: 0.000000  loss: 4.8649 (4.8774)  class_acc: 0.0417 (0.0519)  loss_scale: 32768.0000 (39686.2848)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0969  max mem: 15572
Epoch: [2]  [ 620/1404]  eta: 0:07:46  lr: 0.000046  min_lr: 0.000000  loss: 4.8339 (4.8754)  class_acc: 0.0417 (0.0519)  loss_scale: 32768.0000 (39574.8792)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1162  max mem: 15572
Epoch: [2]  [ 630/1404]  eta: 0:07:39  lr: 0.000046  min_lr: 0.000000  loss: 4.8620 (4.8761)  class_acc: 0.0833 (0.0524)  loss_scale: 32768.0000 (39467.0048)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0596  max mem: 15572
[2025-01-16 20:50:02,298] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:50:02,299] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 20:50:02,375] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:50:02,376] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 640/1404]  eta: 0:07:34  lr: 0.000046  min_lr: 0.000000  loss: 4.9264 (4.8756)  class_acc: 0.0833 (0.0526)  loss_scale: 32768.0000 (39873.6973)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0829  max mem: 15572
[2025-01-16 20:50:11,772] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3454
[2025-01-16 20:50:11,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:50:11,773] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 20:50:11,798] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3454
[2025-01-16 20:50:11,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [ 650/1404]  eta: 0:07:28  lr: 0.000046  min_lr: 0.000000  loss: 4.8295 (4.8734)  class_acc: 0.0833 (0.0530)  loss_scale: 65536.0000 (40016.2212)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.1269  max mem: 15572
Epoch: [2]  [ 660/1404]  eta: 0:07:21  lr: 0.000046  min_lr: 0.000000  loss: 4.7451 (4.8720)  class_acc: 0.0833 (0.0535)  loss_scale: 32768.0000 (39906.5658)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0834  max mem: 15572
Epoch: [2]  [ 670/1404]  eta: 0:07:15  lr: 0.000046  min_lr: 0.000000  loss: 4.7733 (4.8708)  class_acc: 0.0417 (0.0533)  loss_scale: 32768.0000 (39800.1788)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0840  max mem: 15572
Epoch: [2]  [ 680/1404]  eta: 0:07:10  lr: 0.000047  min_lr: 0.000000  loss: 4.7733 (4.8696)  class_acc: 0.0417 (0.0535)  loss_scale: 32768.0000 (39696.9163)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1317  max mem: 15572
Epoch: [2]  [ 690/1404]  eta: 0:07:03  lr: 0.000047  min_lr: 0.000000  loss: 4.7576 (4.8684)  class_acc: 0.0417 (0.0537)  loss_scale: 32768.0000 (39596.6425)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0960  max mem: 15572
Epoch: [2]  [ 700/1404]  eta: 0:06:57  lr: 0.000047  min_lr: 0.000000  loss: 4.8529 (4.8691)  class_acc: 0.0417 (0.0536)  loss_scale: 32768.0000 (39499.2297)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0484  max mem: 15572
Epoch: [2]  [ 710/1404]  eta: 0:06:51  lr: 0.000047  min_lr: 0.000000  loss: 4.8374 (4.8676)  class_acc: 0.0833 (0.0539)  loss_scale: 32768.0000 (39404.5570)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0791  max mem: 15572
Epoch: [2]  [ 720/1404]  eta: 0:06:45  lr: 0.000047  min_lr: 0.000000  loss: 4.8100 (4.8664)  class_acc: 0.0833 (0.0546)  loss_scale: 32768.0000 (39312.5104)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0930  max mem: 15572
Epoch: [2]  [ 730/1404]  eta: 0:06:40  lr: 0.000047  min_lr: 0.000000  loss: 4.7872 (4.8646)  class_acc: 0.0833 (0.0545)  loss_scale: 32768.0000 (39222.9822)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.0945  max mem: 15572
Epoch: [2]  [ 740/1404]  eta: 0:06:33  lr: 0.000047  min_lr: 0.000000  loss: 4.8583 (4.8653)  class_acc: 0.0417 (0.0548)  loss_scale: 32768.0000 (39135.8704)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0560  max mem: 15572
Epoch: [2]  [ 750/1404]  eta: 0:06:27  lr: 0.000048  min_lr: 0.000000  loss: 4.8583 (4.8630)  class_acc: 0.0417 (0.0549)  loss_scale: 32768.0000 (39051.0786)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0273  max mem: 15572
Epoch: [2]  [ 760/1404]  eta: 0:06:21  lr: 0.000048  min_lr: 0.000000  loss: 4.7840 (4.8639)  class_acc: 0.0417 (0.0549)  loss_scale: 32768.0000 (38968.5151)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1075  max mem: 15572
Epoch: [2]  [ 770/1404]  eta: 0:06:15  lr: 0.000048  min_lr: 0.000000  loss: 4.8562 (4.8637)  class_acc: 0.0417 (0.0551)  loss_scale: 32768.0000 (38888.0934)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1244  max mem: 15572
[2025-01-16 20:51:26,627] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:51:26,628] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 20:51:26,631] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:51:26,631] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 780/1404]  eta: 0:06:09  lr: 0.000048  min_lr: 0.000000  loss: 4.8584 (4.8643)  class_acc: 0.0833 (0.0554)  loss_scale: 32768.0000 (39061.4699)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0773  max mem: 15572
Epoch: [2]  [ 790/1404]  eta: 0:06:03  lr: 0.000048  min_lr: 0.000000  loss: 4.8925 (4.8643)  class_acc: 0.0833 (0.0557)  loss_scale: 65536.0000 (39396.1669)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0839  max mem: 15572
Epoch: [2]  [ 800/1404]  eta: 0:05:56  lr: 0.000048  min_lr: 0.000000  loss: 4.8953 (4.8647)  class_acc: 0.0417 (0.0556)  loss_scale: 65536.0000 (39722.5069)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0464  max mem: 15572
Epoch: [2]  [ 810/1404]  eta: 0:05:51  lr: 0.000048  min_lr: 0.000000  loss: 4.8933 (4.8639)  class_acc: 0.0417 (0.0556)  loss_scale: 65536.0000 (40040.7990)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0621  max mem: 15572
Epoch: [2]  [ 820/1404]  eta: 0:05:45  lr: 0.000048  min_lr: 0.000000  loss: 4.7721 (4.8632)  class_acc: 0.0417 (0.0557)  loss_scale: 65536.0000 (40351.3374)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1106  max mem: 15572
Epoch: [2]  [ 830/1404]  eta: 0:05:38  lr: 0.000049  min_lr: 0.000000  loss: 4.7584 (4.8622)  class_acc: 0.0417 (0.0556)  loss_scale: 65536.0000 (40654.4019)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0589  max mem: 15572
Epoch: [2]  [ 840/1404]  eta: 0:05:33  lr: 0.000049  min_lr: 0.000000  loss: 4.7667 (4.8614)  class_acc: 0.0417 (0.0557)  loss_scale: 65536.0000 (40950.2592)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0803  max mem: 15572
Epoch: [2]  [ 850/1404]  eta: 0:05:27  lr: 0.000049  min_lr: 0.000000  loss: 4.8091 (4.8615)  class_acc: 0.0417 (0.0558)  loss_scale: 65536.0000 (41239.1633)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.1373  max mem: 15572
Epoch: [2]  [ 860/1404]  eta: 0:05:21  lr: 0.000049  min_lr: 0.000000  loss: 4.8734 (4.8615)  class_acc: 0.0417 (0.0558)  loss_scale: 65536.0000 (41521.3566)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1039  max mem: 15572
Epoch: [2]  [ 870/1404]  eta: 0:05:15  lr: 0.000049  min_lr: 0.000000  loss: 4.8775 (4.8612)  class_acc: 0.0417 (0.0559)  loss_scale: 65536.0000 (41797.0700)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0750  max mem: 15572
Epoch: [2]  [ 880/1404]  eta: 0:05:09  lr: 0.000049  min_lr: 0.000000  loss: 4.8813 (4.8615)  class_acc: 0.0417 (0.0560)  loss_scale: 65536.0000 (42066.5244)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0681  max mem: 15572
Epoch: [2]  [ 890/1404]  eta: 0:05:03  lr: 0.000049  min_lr: 0.000000  loss: 4.8883 (4.8615)  class_acc: 0.0417 (0.0561)  loss_scale: 65536.0000 (42329.9304)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0685  max mem: 15572
Epoch: [2]  [ 900/1404]  eta: 0:04:57  lr: 0.000050  min_lr: 0.000000  loss: 4.8674 (4.8612)  class_acc: 0.0417 (0.0561)  loss_scale: 65536.0000 (42587.4895)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0793  max mem: 15572
[2025-01-16 20:52:41,342] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:52:41,343] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 20:52:41,343] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:52:41,344] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 20:52:41,878] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3712
[2025-01-16 20:52:41,879] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 20:52:41,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 20:52:41,920] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3712
[2025-01-16 20:52:41,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [2]  [ 910/1404]  eta: 0:04:51  lr: 0.000050  min_lr: 0.000000  loss: 4.8848 (4.8617)  class_acc: 0.0417 (0.0562)  loss_scale: 65536.0000 (42911.3326)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0712  max mem: 15572
[2025-01-16 20:52:47,028] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3721
[2025-01-16 20:52:47,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:52:47,030] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3721
[2025-01-16 20:52:47,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:52:47,030] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 920/1404]  eta: 0:04:46  lr: 0.000050  min_lr: 0.000000  loss: 4.8053 (4.8604)  class_acc: 0.0417 (0.0564)  loss_scale: 65536.0000 (42872.3561)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1128  max mem: 15572
Epoch: [2]  [ 930/1404]  eta: 0:04:40  lr: 0.000050  min_lr: 0.000000  loss: 4.7331 (4.8592)  class_acc: 0.0417 (0.0566)  loss_scale: 32768.0000 (42763.8238)  weight_decay: 0.0500 (0.0500)  time: 0.6716  data: 0.1841  max mem: 15572
Epoch: [2]  [ 940/1404]  eta: 0:04:34  lr: 0.000050  min_lr: 0.000000  loss: 4.7623 (4.8590)  class_acc: 0.0833 (0.0569)  loss_scale: 32768.0000 (42657.5983)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0921  max mem: 15572
Epoch: [2]  [ 950/1404]  eta: 0:04:28  lr: 0.000050  min_lr: 0.000000  loss: 4.8052 (4.8584)  class_acc: 0.0833 (0.0572)  loss_scale: 32768.0000 (42553.6067)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0804  max mem: 15572
Epoch: [2]  [ 960/1404]  eta: 0:04:22  lr: 0.000050  min_lr: 0.000000  loss: 4.8213 (4.8587)  class_acc: 0.0417 (0.0571)  loss_scale: 32768.0000 (42451.7794)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1030  max mem: 15572
Epoch: [2]  [ 970/1404]  eta: 0:04:16  lr: 0.000050  min_lr: 0.000000  loss: 4.9052 (4.8583)  class_acc: 0.0417 (0.0573)  loss_scale: 32768.0000 (42352.0494)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0770  max mem: 15572
Epoch: [2]  [ 980/1404]  eta: 0:04:10  lr: 0.000051  min_lr: 0.000000  loss: 4.8430 (4.8578)  class_acc: 0.0417 (0.0573)  loss_scale: 32768.0000 (42254.3527)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.0984  max mem: 15572
Epoch: [2]  [ 990/1404]  eta: 0:04:04  lr: 0.000051  min_lr: 0.000000  loss: 4.7814 (4.8571)  class_acc: 0.0417 (0.0572)  loss_scale: 32768.0000 (42158.6276)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0917  max mem: 15572
Epoch: [2]  [1000/1404]  eta: 0:03:58  lr: 0.000051  min_lr: 0.000000  loss: 4.7772 (4.8569)  class_acc: 0.0417 (0.0574)  loss_scale: 32768.0000 (42064.8152)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0977  max mem: 15572
Epoch: [2]  [1010/1404]  eta: 0:03:53  lr: 0.000051  min_lr: 0.000000  loss: 4.7863 (4.8563)  class_acc: 0.0417 (0.0572)  loss_scale: 32768.0000 (41972.8586)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0906  max mem: 15572
Epoch: [2]  [1020/1404]  eta: 0:03:46  lr: 0.000051  min_lr: 0.000000  loss: 4.8222 (4.8558)  class_acc: 0.0417 (0.0572)  loss_scale: 32768.0000 (41882.7032)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0404  max mem: 15572
Epoch: [2]  [1030/1404]  eta: 0:03:40  lr: 0.000051  min_lr: 0.000000  loss: 4.7801 (4.8548)  class_acc: 0.0417 (0.0574)  loss_scale: 32768.0000 (41794.2968)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0193  max mem: 15572
Epoch: [2]  [1040/1404]  eta: 0:03:34  lr: 0.000051  min_lr: 0.000000  loss: 4.7636 (4.8538)  class_acc: 0.0833 (0.0579)  loss_scale: 32768.0000 (41707.5889)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0384  max mem: 15572
[2025-01-16 20:54:02,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:54:02,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:54:02,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 20:54:02,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1050/1404]  eta: 0:03:29  lr: 0.000052  min_lr: 0.000000  loss: 4.7946 (4.8540)  class_acc: 0.1250 (0.0583)  loss_scale: 32768.0000 (41903.1323)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0972  max mem: 15572
[2025-01-16 20:54:10,395] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3862
[2025-01-16 20:54:10,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:54:10,439] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3862
[2025-01-16 20:54:10,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:54:10,439] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [1060/1404]  eta: 0:03:22  lr: 0.000052  min_lr: 0.000001  loss: 4.7819 (4.8526)  class_acc: 0.0833 (0.0586)  loss_scale: 65536.0000 (41909.6852)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0782  max mem: 15572
Epoch: [2]  [1070/1404]  eta: 0:03:16  lr: 0.000052  min_lr: 0.000001  loss: 4.7281 (4.8524)  class_acc: 0.0833 (0.0589)  loss_scale: 32768.0000 (41824.3287)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0007  max mem: 15572
Epoch: [2]  [1080/1404]  eta: 0:03:10  lr: 0.000052  min_lr: 0.000001  loss: 4.7933 (4.8523)  class_acc: 0.0833 (0.0591)  loss_scale: 32768.0000 (41740.5513)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0323  max mem: 15572
Epoch: [2]  [1090/1404]  eta: 0:03:05  lr: 0.000052  min_lr: 0.000001  loss: 4.7862 (4.8515)  class_acc: 0.0833 (0.0593)  loss_scale: 32768.0000 (41658.3098)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0915  max mem: 15572
Epoch: [2]  [1100/1404]  eta: 0:02:59  lr: 0.000052  min_lr: 0.000001  loss: 4.7032 (4.8508)  class_acc: 0.0833 (0.0594)  loss_scale: 32768.0000 (41577.5622)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.1519  max mem: 15572
Epoch: [2]  [1110/1404]  eta: 0:02:53  lr: 0.000052  min_lr: 0.000001  loss: 4.7426 (4.8500)  class_acc: 0.0417 (0.0595)  loss_scale: 32768.0000 (41498.2682)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1225  max mem: 15572
Epoch: [2]  [1120/1404]  eta: 0:02:47  lr: 0.000052  min_lr: 0.000001  loss: 4.8015 (4.8495)  class_acc: 0.0833 (0.0600)  loss_scale: 32768.0000 (41420.3889)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0758  max mem: 15572
Epoch: [2]  [1130/1404]  eta: 0:02:41  lr: 0.000053  min_lr: 0.000001  loss: 4.8234 (4.8490)  class_acc: 0.0833 (0.0600)  loss_scale: 32768.0000 (41343.8868)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0459  max mem: 15572
Epoch: [2]  [1140/1404]  eta: 0:02:35  lr: 0.000053  min_lr: 0.000001  loss: 4.7123 (4.8478)  class_acc: 0.0417 (0.0601)  loss_scale: 32768.0000 (41268.7257)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0006  max mem: 15572
Epoch: [2]  [1150/1404]  eta: 0:02:29  lr: 0.000053  min_lr: 0.000001  loss: 4.7605 (4.8478)  class_acc: 0.0417 (0.0601)  loss_scale: 32768.0000 (41194.8705)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1025  max mem: 15572
Epoch: [2]  [1160/1404]  eta: 0:02:23  lr: 0.000053  min_lr: 0.000001  loss: 4.7843 (4.8469)  class_acc: 0.0417 (0.0599)  loss_scale: 32768.0000 (41122.2877)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.1508  max mem: 15572
Epoch: [2]  [1170/1404]  eta: 0:02:17  lr: 0.000053  min_lr: 0.000001  loss: 4.7760 (4.8473)  class_acc: 0.0417 (0.0598)  loss_scale: 32768.0000 (41050.9445)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0655  max mem: 15572
Epoch: [2]  [1180/1404]  eta: 0:02:12  lr: 0.000053  min_lr: 0.000001  loss: 4.8286 (4.8475)  class_acc: 0.0417 (0.0600)  loss_scale: 32768.0000 (40980.8095)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0926  max mem: 15572
[2025-01-16 20:55:25,492] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:55:25,492] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:55:25,492] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 20:55:25,492] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1190/1404]  eta: 0:02:06  lr: 0.000053  min_lr: 0.000001  loss: 4.8286 (4.8474)  class_acc: 0.0417 (0.0601)  loss_scale: 32768.0000 (41131.9563)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0761  max mem: 15572
[2025-01-16 20:55:29,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=18, lr=[5.175138275808422e-07, 5.175138275808422e-07, 7.393054679726318e-07, 7.393054679726318e-07, 1.0561506685323313e-06, 1.0561506685323313e-06, 1.508786669331902e-06, 1.508786669331902e-06, 2.1554095276170026e-06, 2.1554095276170026e-06, 3.0791564680242897e-06, 3.0791564680242897e-06, 4.398794954320414e-06, 4.398794954320414e-06, 6.283992791886307e-06, 6.283992791886307e-06, 8.97713255983758e-06, 8.97713255983758e-06, 1.282447508548226e-05, 1.282447508548226e-05, 1.8320678693546087e-05, 1.8320678693546087e-05, 2.6172398133637267e-05, 2.6172398133637267e-05, 3.7389140190910385e-05, 3.7389140190910385e-05, 5.341305741558627e-05, 5.341305741558627e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 20:55:29,545] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=47.52032586810163, CurrSamplesPerSec=59.19796383549786, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [1200/1404]  eta: 0:02:00  lr: 0.000054  min_lr: 0.000001  loss: 4.8451 (4.8469)  class_acc: 0.0833 (0.0605)  loss_scale: 65536.0000 (41335.1540)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0432  max mem: 15572
Epoch: [2]  [1210/1404]  eta: 0:01:54  lr: 0.000054  min_lr: 0.000001  loss: 4.6790 (4.8460)  class_acc: 0.0833 (0.0607)  loss_scale: 65536.0000 (41534.9959)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0813  max mem: 15572
Epoch: [2]  [1220/1404]  eta: 0:01:48  lr: 0.000054  min_lr: 0.000001  loss: 4.7292 (4.8454)  class_acc: 0.0833 (0.0607)  loss_scale: 65536.0000 (41731.5643)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0965  max mem: 15572
[2025-01-16 20:55:47,376] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4029
[2025-01-16 20:55:47,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:55:47,404] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4029
[2025-01-16 20:55:47,404] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:55:47,405] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [1230/1404]  eta: 0:01:42  lr: 0.000054  min_lr: 0.000001  loss: 4.7930 (4.8451)  class_acc: 0.0417 (0.0608)  loss_scale: 32768.0000 (41658.7490)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0928  max mem: 15572
Epoch: [2]  [1240/1404]  eta: 0:01:36  lr: 0.000054  min_lr: 0.000001  loss: 4.7930 (4.8447)  class_acc: 0.0417 (0.0608)  loss_scale: 32768.0000 (41587.1072)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0864  max mem: 15572
Epoch: [2]  [1250/1404]  eta: 0:01:30  lr: 0.000054  min_lr: 0.000001  loss: 4.8229 (4.8448)  class_acc: 0.0833 (0.0611)  loss_scale: 32768.0000 (41516.6107)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0757  max mem: 15572
Epoch: [2]  [1260/1404]  eta: 0:01:24  lr: 0.000054  min_lr: 0.000001  loss: 4.8229 (4.8442)  class_acc: 0.0833 (0.0615)  loss_scale: 32768.0000 (41447.2324)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0243  max mem: 15572
Epoch: [2]  [1270/1404]  eta: 0:01:18  lr: 0.000054  min_lr: 0.000001  loss: 4.7876 (4.8437)  class_acc: 0.1250 (0.0619)  loss_scale: 32768.0000 (41378.9457)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0491  max mem: 15572
Epoch: [2]  [1280/1404]  eta: 0:01:12  lr: 0.000055  min_lr: 0.000001  loss: 4.7614 (4.8425)  class_acc: 0.0833 (0.0619)  loss_scale: 32768.0000 (41311.7252)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0980  max mem: 15572
Epoch: [2]  [1290/1404]  eta: 0:01:07  lr: 0.000055  min_lr: 0.000001  loss: 4.7614 (4.8424)  class_acc: 0.0833 (0.0621)  loss_scale: 32768.0000 (41245.5461)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0698  max mem: 15572
Epoch: [2]  [1300/1404]  eta: 0:01:01  lr: 0.000055  min_lr: 0.000001  loss: 4.7961 (4.8413)  class_acc: 0.0833 (0.0623)  loss_scale: 32768.0000 (41180.3843)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0949  max mem: 15572
Epoch: [2]  [1310/1404]  eta: 0:00:55  lr: 0.000055  min_lr: 0.000001  loss: 4.7686 (4.8410)  class_acc: 0.0417 (0.0623)  loss_scale: 32768.0000 (41116.2166)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0821  max mem: 15572
Epoch: [2]  [1320/1404]  eta: 0:00:49  lr: 0.000055  min_lr: 0.000001  loss: 4.7521 (4.8402)  class_acc: 0.0417 (0.0623)  loss_scale: 32768.0000 (41053.0204)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0286  max mem: 15572
Epoch: [2]  [1330/1404]  eta: 0:00:43  lr: 0.000055  min_lr: 0.000001  loss: 4.7418 (4.8399)  class_acc: 0.0417 (0.0621)  loss_scale: 32768.0000 (40990.7739)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0924  max mem: 15572
Epoch: [2]  [1340/1404]  eta: 0:00:37  lr: 0.000055  min_lr: 0.000001  loss: 4.7802 (4.8398)  class_acc: 0.0417 (0.0622)  loss_scale: 32768.0000 (40929.4556)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1124  max mem: 15572
[2025-01-16 20:57:02,452] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:57:02,452] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1350/1404]  eta: 0:00:31  lr: 0.000056  min_lr: 0.000001  loss: 4.6992 (4.8385)  class_acc: 0.0833 (0.0626)  loss_scale: 32768.0000 (40893.2998)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0929  max mem: 15572
[2025-01-16 20:57:02,494] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:57:02,495] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1360/1404]  eta: 0:00:25  lr: 0.000056  min_lr: 0.000001  loss: 4.6992 (4.8376)  class_acc: 0.0417 (0.0625)  loss_scale: 65536.0000 (41074.3630)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0953  max mem: 15572
Epoch: [2]  [1370/1404]  eta: 0:00:20  lr: 0.000056  min_lr: 0.000001  loss: 4.7999 (4.8379)  class_acc: 0.0417 (0.0625)  loss_scale: 65536.0000 (41252.7848)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0571  max mem: 15572
Epoch: [2]  [1380/1404]  eta: 0:00:14  lr: 0.000056  min_lr: 0.000001  loss: 4.8708 (4.8381)  class_acc: 0.0417 (0.0624)  loss_scale: 65536.0000 (41428.6227)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0143  max mem: 15572
Epoch: [2]  [1390/1404]  eta: 0:00:08  lr: 0.000056  min_lr: 0.000001  loss: 4.6895 (4.8373)  class_acc: 0.0417 (0.0626)  loss_scale: 65536.0000 (41601.9324)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0008  max mem: 15572
Epoch: [2]  [1400/1404]  eta: 0:00:02  lr: 0.000056  min_lr: 0.000001  loss: 4.6895 (4.8372)  class_acc: 0.0417 (0.0626)  loss_scale: 65536.0000 (41772.7680)  weight_decay: 0.0500 (0.0500)  time: 0.4640  data: 0.0004  max mem: 15572
Epoch: [2]  [1403/1404]  eta: 0:00:00  lr: 0.000056  min_lr: 0.000001  loss: 4.6895 (4.8368)  class_acc: 0.0417 (0.0626)  loss_scale: 65536.0000 (41823.5442)  weight_decay: 0.0500 (0.0500)  time: 0.4415  data: 0.0003  max mem: 15572
Epoch: [2] Total time: 0:13:44 (0.5872 s / it)
Averaged stats: lr: 0.000056  min_lr: 0.000001  loss: 4.6895 (4.8407)  class_acc: 0.0417 (0.0639)  loss_scale: 65536.0000 (41823.5442)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:12:41  loss: 3.9401 (3.9401)  acc1: 33.3333 (33.3333)  acc5: 55.5556 (55.5556)  time: 5.6003  data: 5.3891  max mem: 15572
Val:  [ 10/136]  eta: 0:01:33  loss: 4.5595 (4.5385)  acc1: 0.0000 (7.0707)  acc5: 5.5556 (21.2121)  time: 0.7418  data: 0.5497  max mem: 15572
Val:  [ 20/136]  eta: 0:01:07  loss: 4.4057 (4.3989)  acc1: 0.0000 (7.4074)  acc5: 16.6667 (28.3069)  time: 0.3291  data: 0.1420  max mem: 15572
Val:  [ 30/136]  eta: 0:00:52  loss: 3.8077 (4.2565)  acc1: 5.5556 (11.8280)  acc5: 50.0000 (38.1720)  time: 0.3570  data: 0.1678  max mem: 15572
Val:  [ 40/136]  eta: 0:00:45  loss: 3.7848 (4.2286)  acc1: 5.5556 (13.1436)  acc5: 55.5556 (36.3144)  time: 0.3586  data: 0.1573  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 4.4915 (4.3373)  acc1: 0.0000 (11.2200)  acc5: 0.0000 (30.7190)  time: 0.3806  data: 0.1770  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 4.7179 (4.4103)  acc1: 0.0000 (9.7450)  acc5: 0.0000 (27.3224)  time: 0.3759  data: 0.1703  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 4.5885 (4.3467)  acc1: 0.0000 (11.5023)  acc5: 11.1111 (30.2034)  time: 0.3503  data: 0.1425  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 4.1092 (4.3500)  acc1: 5.5556 (11.7284)  acc5: 38.8889 (30.5898)  time: 0.3907  data: 0.1873  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 4.3902 (4.3636)  acc1: 0.0000 (11.0501)  acc5: 16.6667 (29.7314)  time: 0.3576  data: 0.1370  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 4.5551 (4.4089)  acc1: 0.0000 (10.0660)  acc5: 5.5556 (27.7228)  time: 0.2519  data: 0.0403  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 4.5004 (4.4173)  acc1: 0.0000 (9.6597)  acc5: 16.6667 (27.6276)  time: 0.3186  data: 0.1189  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.3226 (4.4092)  acc1: 5.5556 (9.9174)  acc5: 27.7778 (28.4206)  time: 0.3530  data: 0.1501  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 4.2931 (4.3990)  acc1: 11.1111 (10.8567)  acc5: 33.3333 (29.3045)  time: 0.2761  data: 0.1027  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 4.4640 (4.4026)  acc1: 11.1111 (10.8927)  acc5: 27.7778 (29.3612)  time: 0.2487  data: 0.0949  max mem: 15572
Val: Total time: 0:00:49 (0.3661 s / it)
* Acc@1 10.708 Acc@5 28.296 loss 4.414
Accuracy of the network on the 4883 val videos: 10.7%
[2025-01-16 20:58:21,474] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 20:58:21,476] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 20:58:21,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 20:58:21,476] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 20:58:23,924] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 20:58:23,924] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 10.71%
Epoch: [3]  [   0/1404]  eta: 3:17:49  lr: 0.000056  min_lr: 0.000001  loss: 4.5095 (4.5095)  class_acc: 0.0833 (0.0833)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.4543  data: 7.8617  max mem: 15572
Epoch: [3]  [  10/1404]  eta: 0:29:33  lr: 0.000056  min_lr: 0.000001  loss: 4.7574 (4.7503)  class_acc: 0.0417 (0.0492)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2719  data: 0.7885  max mem: 15572
Epoch: [3]  [  20/1404]  eta: 0:21:44  lr: 0.000057  min_lr: 0.000001  loss: 4.7200 (4.6951)  class_acc: 0.0417 (0.0754)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.0826  max mem: 15572
Epoch: [3]  [  30/1404]  eta: 0:18:26  lr: 0.000057  min_lr: 0.000001  loss: 4.6430 (4.6935)  class_acc: 0.0833 (0.0793)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0423  max mem: 15572
Epoch: [3]  [  40/1404]  eta: 0:16:43  lr: 0.000057  min_lr: 0.000001  loss: 4.7083 (4.7236)  class_acc: 0.0833 (0.0823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5185  data: 0.0005  max mem: 15572
Epoch: [3]  [  50/1404]  eta: 0:15:36  lr: 0.000057  min_lr: 0.000001  loss: 4.7832 (4.7366)  class_acc: 0.0833 (0.0899)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0113  max mem: 15572
Epoch: [3]  [  60/1404]  eta: 0:15:10  lr: 0.000057  min_lr: 0.000001  loss: 4.7542 (4.7450)  class_acc: 0.0833 (0.0874)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0115  max mem: 15572
Epoch: [3]  [  70/1404]  eta: 0:14:50  lr: 0.000057  min_lr: 0.000001  loss: 4.7781 (4.7513)  class_acc: 0.0417 (0.0804)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0119  max mem: 15572
[2025-01-16 20:59:14,954] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:59:14,955] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 20:59:14,964] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 20:59:14,966] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 20:59:16,551] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4289
[2025-01-16 20:59:16,552] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 20:59:16,601] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4289
[2025-01-16 20:59:16,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 20:59:16,602] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 20:59:17,058] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4290
[2025-01-16 20:59:17,058] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:59:17,075] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4290
[2025-01-16 20:59:17,076] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 20:59:17,076] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [  80/1404]  eta: 0:14:44  lr: 0.000057  min_lr: 0.000001  loss: 4.7826 (4.7477)  class_acc: 0.0417 (0.0782)  loss_scale: 65536.0000 (66749.6296)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.0123  max mem: 15572
Epoch: [3]  [  90/1404]  eta: 0:14:24  lr: 0.000057  min_lr: 0.000001  loss: 4.7949 (4.7529)  class_acc: 0.0417 (0.0760)  loss_scale: 32768.0000 (63015.3846)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.0011  max mem: 15572
Epoch: [3]  [ 100/1404]  eta: 0:14:01  lr: 0.000058  min_lr: 0.000001  loss: 4.8123 (4.7591)  class_acc: 0.0417 (0.0755)  loss_scale: 32768.0000 (60020.5941)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0140  max mem: 15572
Epoch: [3]  [ 110/1404]  eta: 0:13:50  lr: 0.000058  min_lr: 0.000001  loss: 4.8065 (4.7615)  class_acc: 0.0417 (0.0739)  loss_scale: 32768.0000 (57565.4054)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0682  max mem: 15572
Epoch: [3]  [ 120/1404]  eta: 0:13:48  lr: 0.000058  min_lr: 0.000001  loss: 4.8041 (4.7625)  class_acc: 0.0417 (0.0716)  loss_scale: 32768.0000 (55516.0331)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.1468  max mem: 15572
Epoch: [3]  [ 130/1404]  eta: 0:13:39  lr: 0.000058  min_lr: 0.000001  loss: 4.7896 (4.7632)  class_acc: 0.0417 (0.0735)  loss_scale: 32768.0000 (53779.5420)  weight_decay: 0.0500 (0.0500)  time: 0.6528  data: 0.1498  max mem: 15572
Epoch: [3]  [ 140/1404]  eta: 0:13:29  lr: 0.000058  min_lr: 0.000001  loss: 4.7802 (4.7654)  class_acc: 0.0833 (0.0736)  loss_scale: 32768.0000 (52289.3617)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1153  max mem: 15572
Epoch: [3]  [ 150/1404]  eta: 0:13:14  lr: 0.000058  min_lr: 0.000001  loss: 4.8030 (4.7710)  class_acc: 0.0833 (0.0751)  loss_scale: 32768.0000 (50996.5563)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0846  max mem: 15572
Epoch: [3]  [ 160/1404]  eta: 0:13:06  lr: 0.000058  min_lr: 0.000001  loss: 4.8030 (4.7733)  class_acc: 0.0833 (0.0761)  loss_scale: 32768.0000 (49864.3478)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0850  max mem: 15572
Epoch: [3]  [ 170/1404]  eta: 0:12:52  lr: 0.000059  min_lr: 0.000001  loss: 4.7085 (4.7690)  class_acc: 0.0833 (0.0770)  loss_scale: 32768.0000 (48864.5614)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.0765  max mem: 15572
Epoch: [3]  [ 180/1404]  eta: 0:12:39  lr: 0.000059  min_lr: 0.000001  loss: 4.7322 (4.7699)  class_acc: 0.0833 (0.0780)  loss_scale: 32768.0000 (47975.2486)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0245  max mem: 15572
Epoch: [3]  [ 190/1404]  eta: 0:12:29  lr: 0.000059  min_lr: 0.000001  loss: 4.7357 (4.7682)  class_acc: 0.0417 (0.0768)  loss_scale: 32768.0000 (47179.0576)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0064  max mem: 15572
Epoch: [3]  [ 200/1404]  eta: 0:12:25  lr: 0.000059  min_lr: 0.000001  loss: 4.7317 (4.7657)  class_acc: 0.0417 (0.0763)  loss_scale: 32768.0000 (46462.0896)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0469  max mem: 15572
[2025-01-16 21:00:31,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:00:31,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:00:31,790] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:00:31,791] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 210/1404]  eta: 0:12:17  lr: 0.000059  min_lr: 0.000001  loss: 4.8260 (4.7705)  class_acc: 0.0417 (0.0752)  loss_scale: 32768.0000 (46434.2749)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.0926  max mem: 15572
Epoch: [3]  [ 220/1404]  eta: 0:12:08  lr: 0.000059  min_lr: 0.000001  loss: 4.8814 (4.7701)  class_acc: 0.0417 (0.0748)  loss_scale: 65536.0000 (47298.6063)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0775  max mem: 15572
Epoch: [3]  [ 230/1404]  eta: 0:12:00  lr: 0.000059  min_lr: 0.000001  loss: 4.7163 (4.7682)  class_acc: 0.0417 (0.0732)  loss_scale: 65536.0000 (48088.1039)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.0889  max mem: 15572
Epoch: [3]  [ 240/1404]  eta: 0:11:54  lr: 0.000059  min_lr: 0.000001  loss: 4.7329 (4.7676)  class_acc: 0.0417 (0.0737)  loss_scale: 65536.0000 (48812.0830)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0577  max mem: 15572
Epoch: [3]  [ 250/1404]  eta: 0:11:47  lr: 0.000060  min_lr: 0.000001  loss: 4.6675 (4.7668)  class_acc: 0.0417 (0.0732)  loss_scale: 65536.0000 (49478.3745)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0140  max mem: 15572
Epoch: [3]  [ 260/1404]  eta: 0:11:43  lr: 0.000060  min_lr: 0.000001  loss: 4.7012 (4.7685)  class_acc: 0.0833 (0.0761)  loss_scale: 65536.0000 (50093.6092)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.0411  max mem: 15572
Epoch: [3]  [ 270/1404]  eta: 0:11:32  lr: 0.000060  min_lr: 0.000001  loss: 4.7880 (4.7679)  class_acc: 0.0833 (0.0772)  loss_scale: 65536.0000 (50663.4391)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0426  max mem: 15572
Epoch: [3]  [ 280/1404]  eta: 0:11:25  lr: 0.000060  min_lr: 0.000001  loss: 4.7416 (4.7651)  class_acc: 0.0833 (0.0781)  loss_scale: 65536.0000 (51192.7117)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0827  max mem: 15572
Epoch: [3]  [ 290/1404]  eta: 0:11:15  lr: 0.000060  min_lr: 0.000001  loss: 4.7416 (4.7662)  class_acc: 0.0833 (0.0785)  loss_scale: 65536.0000 (51685.6082)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0678  max mem: 15572
Epoch: [3]  [ 300/1404]  eta: 0:11:11  lr: 0.000060  min_lr: 0.000001  loss: 4.8142 (4.7670)  class_acc: 0.0417 (0.0779)  loss_scale: 65536.0000 (52145.7542)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0663  max mem: 15572
Epoch: [3]  [ 310/1404]  eta: 0:11:02  lr: 0.000060  min_lr: 0.000001  loss: 4.7795 (4.7670)  class_acc: 0.0417 (0.0778)  loss_scale: 65536.0000 (52576.3087)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0866  max mem: 15572
Epoch: [3]  [ 320/1404]  eta: 0:10:55  lr: 0.000061  min_lr: 0.000001  loss: 4.7292 (4.7658)  class_acc: 0.0833 (0.0774)  loss_scale: 65536.0000 (52980.0374)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0520  max mem: 15572
Epoch: [3]  [ 330/1404]  eta: 0:10:49  lr: 0.000061  min_lr: 0.000001  loss: 4.7260 (4.7661)  class_acc: 0.0833 (0.0777)  loss_scale: 65536.0000 (53359.3716)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0579  max mem: 15572
[2025-01-16 21:01:46,473] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:01:46,473] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 21:01:46,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:01:46,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 21:01:47,052] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4548
[2025-01-16 21:01:47,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 21:01:47,055] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4548
[2025-01-16 21:01:47,055] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 21:01:47,055] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 340/1404]  eta: 0:10:42  lr: 0.000061  min_lr: 0.000001  loss: 4.7122 (4.7646)  class_acc: 0.0833 (0.0781)  loss_scale: 65536.0000 (53908.6452)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.0414  max mem: 15572
[2025-01-16 21:01:54,193] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4560
[2025-01-16 21:01:54,193] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:01:54,193] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:01:54,239] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4560
[2025-01-16 21:01:54,240] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [ 350/1404]  eta: 0:10:35  lr: 0.000061  min_lr: 0.000001  loss: 4.6514 (4.7632)  class_acc: 0.0833 (0.0778)  loss_scale: 65536.0000 (53959.8405)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0151  max mem: 15572
Epoch: [3]  [ 360/1404]  eta: 0:10:28  lr: 0.000061  min_lr: 0.000001  loss: 4.7101 (4.7637)  class_acc: 0.0417 (0.0769)  loss_scale: 32768.0000 (53372.8089)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.0347  max mem: 15572
Epoch: [3]  [ 370/1404]  eta: 0:10:23  lr: 0.000061  min_lr: 0.000001  loss: 4.7803 (4.7659)  class_acc: 0.0417 (0.0765)  loss_scale: 32768.0000 (52817.4232)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0975  max mem: 15572
Epoch: [3]  [ 380/1404]  eta: 0:10:16  lr: 0.000061  min_lr: 0.000001  loss: 4.7362 (4.7642)  class_acc: 0.0417 (0.0763)  loss_scale: 32768.0000 (52291.1916)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0785  max mem: 15572
Epoch: [3]  [ 390/1404]  eta: 0:10:11  lr: 0.000061  min_lr: 0.000001  loss: 4.6640 (4.7628)  class_acc: 0.0833 (0.0772)  loss_scale: 32768.0000 (51791.8772)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.0416  max mem: 15572
Epoch: [3]  [ 400/1404]  eta: 0:10:02  lr: 0.000062  min_lr: 0.000001  loss: 4.6743 (4.7626)  class_acc: 0.0833 (0.0773)  loss_scale: 32768.0000 (51317.4663)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0346  max mem: 15572
Epoch: [3]  [ 410/1404]  eta: 0:09:56  lr: 0.000062  min_lr: 0.000001  loss: 4.7708 (4.7625)  class_acc: 0.0417 (0.0769)  loss_scale: 32768.0000 (50866.1411)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0499  max mem: 15572
Epoch: [3]  [ 420/1404]  eta: 0:09:50  lr: 0.000062  min_lr: 0.000001  loss: 4.7490 (4.7611)  class_acc: 0.0417 (0.0762)  loss_scale: 32768.0000 (50436.2565)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0960  max mem: 15572
Epoch: [3]  [ 430/1404]  eta: 0:09:45  lr: 0.000062  min_lr: 0.000001  loss: 4.6820 (4.7618)  class_acc: 0.0833 (0.0765)  loss_scale: 32768.0000 (50026.3202)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1072  max mem: 15572
Epoch: [3]  [ 440/1404]  eta: 0:09:37  lr: 0.000062  min_lr: 0.000001  loss: 4.7420 (4.7625)  class_acc: 0.0833 (0.0772)  loss_scale: 32768.0000 (49634.9751)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0715  max mem: 15572
Epoch: [3]  [ 450/1404]  eta: 0:09:32  lr: 0.000062  min_lr: 0.000001  loss: 4.7636 (4.7631)  class_acc: 0.0833 (0.0768)  loss_scale: 32768.0000 (49260.9845)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0957  max mem: 15572
Epoch: [3]  [ 460/1404]  eta: 0:09:27  lr: 0.000062  min_lr: 0.000001  loss: 4.7706 (4.7624)  class_acc: 0.0833 (0.0775)  loss_scale: 32768.0000 (48903.2191)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1384  max mem: 15572
Epoch: [3]  [ 470/1404]  eta: 0:09:20  lr: 0.000063  min_lr: 0.000001  loss: 4.7659 (4.7629)  class_acc: 0.0833 (0.0775)  loss_scale: 32768.0000 (48560.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0815  max mem: 15572
[2025-01-16 21:03:11,608] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:03:11,608] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:03:11,616] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:03:11,616] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 480/1404]  eta: 0:09:15  lr: 0.000063  min_lr: 0.000001  loss: 4.7980 (4.7627)  class_acc: 0.0417 (0.0773)  loss_scale: 32768.0000 (48504.8150)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.0882  max mem: 15572
Epoch: [3]  [ 490/1404]  eta: 0:09:08  lr: 0.000063  min_lr: 0.000001  loss: 4.7875 (4.7638)  class_acc: 0.0417 (0.0769)  loss_scale: 65536.0000 (48851.6823)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1258  max mem: 15572
Epoch: [3]  [ 500/1404]  eta: 0:09:00  lr: 0.000063  min_lr: 0.000001  loss: 4.7603 (4.7623)  class_acc: 0.0833 (0.0774)  loss_scale: 65536.0000 (49184.7026)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0579  max mem: 15572
Epoch: [3]  [ 510/1404]  eta: 0:08:55  lr: 0.000063  min_lr: 0.000001  loss: 4.7666 (4.7625)  class_acc: 0.0833 (0.0777)  loss_scale: 65536.0000 (49504.6888)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0691  max mem: 15572
Epoch: [3]  [ 520/1404]  eta: 0:08:48  lr: 0.000063  min_lr: 0.000001  loss: 4.7666 (4.7624)  class_acc: 0.0833 (0.0781)  loss_scale: 65536.0000 (49812.3916)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.0690  max mem: 15572
Epoch: [3]  [ 530/1404]  eta: 0:08:41  lr: 0.000063  min_lr: 0.000001  loss: 4.7178 (4.7617)  class_acc: 0.0833 (0.0788)  loss_scale: 65536.0000 (50108.5047)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0356  max mem: 15572
Epoch: [3]  [ 540/1404]  eta: 0:08:35  lr: 0.000063  min_lr: 0.000001  loss: 4.7532 (4.7619)  class_acc: 0.0833 (0.0789)  loss_scale: 65536.0000 (50393.6710)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0735  max mem: 15572
Epoch: [3]  [ 550/1404]  eta: 0:08:30  lr: 0.000064  min_lr: 0.000001  loss: 4.7558 (4.7633)  class_acc: 0.0417 (0.0787)  loss_scale: 65536.0000 (50668.4864)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1163  max mem: 15572
Epoch: [3]  [ 560/1404]  eta: 0:08:23  lr: 0.000064  min_lr: 0.000001  loss: 4.8014 (4.7640)  class_acc: 0.0833 (0.0793)  loss_scale: 65536.0000 (50933.5045)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1029  max mem: 15572
Epoch: [3]  [ 570/1404]  eta: 0:08:16  lr: 0.000064  min_lr: 0.000001  loss: 4.7593 (4.7633)  class_acc: 0.0833 (0.0797)  loss_scale: 65536.0000 (51189.2399)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0467  max mem: 15572
Epoch: [3]  [ 580/1404]  eta: 0:08:12  lr: 0.000064  min_lr: 0.000001  loss: 4.7347 (4.7623)  class_acc: 0.0833 (0.0802)  loss_scale: 65536.0000 (51436.1721)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1476  max mem: 15572
[2025-01-16 21:04:12,433] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4794
[2025-01-16 21:04:12,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:04:12,433] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:04:12,433] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4794
[2025-01-16 21:04:12,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [ 590/1404]  eta: 0:08:05  lr: 0.000064  min_lr: 0.000001  loss: 4.7323 (4.7628)  class_acc: 0.0833 (0.0809)  loss_scale: 65536.0000 (51175.7428)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1261  max mem: 15572
Epoch: [3]  [ 600/1404]  eta: 0:07:58  lr: 0.000064  min_lr: 0.000001  loss: 4.7325 (4.7623)  class_acc: 0.0833 (0.0810)  loss_scale: 32768.0000 (50869.4576)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0289  max mem: 15572
Epoch: [3]  [ 610/1404]  eta: 0:07:51  lr: 0.000064  min_lr: 0.000001  loss: 4.7335 (4.7620)  class_acc: 0.0833 (0.0816)  loss_scale: 32768.0000 (50573.1980)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0290  max mem: 15572
Epoch: [3]  [ 620/1404]  eta: 0:07:46  lr: 0.000065  min_lr: 0.000001  loss: 4.7776 (4.7630)  class_acc: 0.0833 (0.0811)  loss_scale: 32768.0000 (50286.4799)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0587  max mem: 15572
Epoch: [3]  [ 630/1404]  eta: 0:07:39  lr: 0.000065  min_lr: 0.000001  loss: 4.7833 (4.7631)  class_acc: 0.0417 (0.0810)  loss_scale: 32768.0000 (50008.8494)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0658  max mem: 15572
Epoch: [3]  [ 640/1404]  eta: 0:07:33  lr: 0.000065  min_lr: 0.000001  loss: 4.7145 (4.7622)  class_acc: 0.0833 (0.0807)  loss_scale: 32768.0000 (49739.8814)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0514  max mem: 15572
Epoch: [3]  [ 650/1404]  eta: 0:07:27  lr: 0.000065  min_lr: 0.000001  loss: 4.7404 (4.7620)  class_acc: 0.0833 (0.0810)  loss_scale: 32768.0000 (49479.1767)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0665  max mem: 15572
Epoch: [3]  [ 660/1404]  eta: 0:07:21  lr: 0.000065  min_lr: 0.000001  loss: 4.7890 (4.7607)  class_acc: 0.0833 (0.0813)  loss_scale: 32768.0000 (49226.3601)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0701  max mem: 15572
Epoch: [3]  [ 670/1404]  eta: 0:07:16  lr: 0.000065  min_lr: 0.000001  loss: 4.7693 (4.7619)  class_acc: 0.0833 (0.0812)  loss_scale: 32768.0000 (48981.0790)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1243  max mem: 15572
Epoch: [3]  [ 680/1404]  eta: 0:07:10  lr: 0.000065  min_lr: 0.000001  loss: 4.7685 (4.7614)  class_acc: 0.0833 (0.0816)  loss_scale: 32768.0000 (48743.0015)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.1291  max mem: 15572
Epoch: [3]  [ 690/1404]  eta: 0:07:04  lr: 0.000065  min_lr: 0.000001  loss: 4.7443 (4.7621)  class_acc: 0.0833 (0.0815)  loss_scale: 32768.0000 (48511.8148)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0938  max mem: 15572
Epoch: [3]  [ 700/1404]  eta: 0:06:58  lr: 0.000066  min_lr: 0.000001  loss: 4.7502 (4.7613)  class_acc: 0.0833 (0.0820)  loss_scale: 32768.0000 (48287.2240)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0938  max mem: 15572
Epoch: [3]  [ 710/1404]  eta: 0:06:52  lr: 0.000066  min_lr: 0.000001  loss: 4.7558 (4.7613)  class_acc: 0.0833 (0.0820)  loss_scale: 32768.0000 (48068.9508)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0964  max mem: 15572
[2025-01-16 21:05:26,996] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:05:26,996] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:05:26,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:05:26,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 720/1404]  eta: 0:06:46  lr: 0.000066  min_lr: 0.000001  loss: 4.7703 (4.7623)  class_acc: 0.0833 (0.0818)  loss_scale: 32768.0000 (48311.2122)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1031  max mem: 15572
Epoch: [3]  [ 730/1404]  eta: 0:06:39  lr: 0.000066  min_lr: 0.000001  loss: 4.7703 (4.7626)  class_acc: 0.0833 (0.0819)  loss_scale: 65536.0000 (48546.8454)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0674  max mem: 15572
Epoch: [3]  [ 740/1404]  eta: 0:06:34  lr: 0.000066  min_lr: 0.000001  loss: 4.6566 (4.7611)  class_acc: 0.0833 (0.0820)  loss_scale: 65536.0000 (48776.1188)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.0970  max mem: 15572
[2025-01-16 21:05:48,465] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4959
[2025-01-16 21:05:48,465] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4959
[2025-01-16 21:05:48,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:05:48,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:05:48,466] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [ 750/1404]  eta: 0:06:28  lr: 0.000066  min_lr: 0.000001  loss: 4.5827 (4.7598)  class_acc: 0.0833 (0.0825)  loss_scale: 65536.0000 (48824.7563)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1270  max mem: 15572
Epoch: [3]  [ 760/1404]  eta: 0:06:23  lr: 0.000066  min_lr: 0.000001  loss: 4.6212 (4.7595)  class_acc: 0.0833 (0.0827)  loss_scale: 32768.0000 (48613.7608)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.1645  max mem: 15572
Epoch: [3]  [ 770/1404]  eta: 0:06:16  lr: 0.000067  min_lr: 0.000001  loss: 4.7035 (4.7591)  class_acc: 0.0833 (0.0826)  loss_scale: 32768.0000 (48408.2387)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1262  max mem: 15572
Epoch: [3]  [ 780/1404]  eta: 0:06:09  lr: 0.000067  min_lr: 0.000001  loss: 4.7798 (4.7593)  class_acc: 0.0833 (0.0827)  loss_scale: 32768.0000 (48207.9795)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0008  max mem: 15572
[2025-01-16 21:06:10,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=25, lr=[6.469246371784522e-07, 6.469246371784522e-07, 9.241780531120747e-07, 9.241780531120747e-07, 1.3202543615886782e-06, 1.3202543615886782e-06, 1.8860776594123977e-06, 1.8860776594123977e-06, 2.694396656303425e-06, 2.694396656303425e-06, 3.849138080433465e-06, 3.849138080433465e-06, 5.498768686333522e-06, 5.498768686333522e-06, 7.855383837619317e-06, 7.855383837619317e-06, 1.1221976910884738e-05, 1.1221976910884738e-05, 1.60313955869782e-05, 1.60313955869782e-05, 2.290199369568314e-05, 2.290199369568314e-05, 3.271713385097592e-05, 3.271713385097592e-05, 4.673876264425132e-05, 4.673876264425132e-05, 6.676966092035903e-05, 6.676966092035903e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 21:06:10,829] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=47.95506481890018, CurrSamplesPerSec=55.567771306307726, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [ 790/1404]  eta: 0:06:03  lr: 0.000067  min_lr: 0.000001  loss: 4.7901 (4.7593)  class_acc: 0.0833 (0.0830)  loss_scale: 32768.0000 (48012.7838)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0093  max mem: 15572
Epoch: [3]  [ 800/1404]  eta: 0:05:57  lr: 0.000067  min_lr: 0.000001  loss: 4.7259 (4.7586)  class_acc: 0.0833 (0.0830)  loss_scale: 32768.0000 (47822.4619)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0628  max mem: 15572
Epoch: [3]  [ 810/1404]  eta: 0:05:51  lr: 0.000067  min_lr: 0.000001  loss: 4.7175 (4.7575)  class_acc: 0.0833 (0.0829)  loss_scale: 32768.0000 (47636.8335)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0545  max mem: 15572
Epoch: [3]  [ 820/1404]  eta: 0:05:45  lr: 0.000067  min_lr: 0.000001  loss: 4.6983 (4.7561)  class_acc: 0.0833 (0.0830)  loss_scale: 32768.0000 (47455.7272)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0443  max mem: 15572
Epoch: [3]  [ 830/1404]  eta: 0:05:39  lr: 0.000067  min_lr: 0.000001  loss: 4.7498 (4.7568)  class_acc: 0.0833 (0.0832)  loss_scale: 32768.0000 (47278.9795)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0816  max mem: 15572
Epoch: [3]  [ 840/1404]  eta: 0:05:33  lr: 0.000067  min_lr: 0.000001  loss: 4.7632 (4.7568)  class_acc: 0.0833 (0.0833)  loss_scale: 32768.0000 (47106.4352)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0685  max mem: 15572
Epoch: [3]  [ 850/1404]  eta: 0:05:26  lr: 0.000068  min_lr: 0.000001  loss: 4.8406 (4.7578)  class_acc: 0.0833 (0.0835)  loss_scale: 32768.0000 (46937.9459)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0311  max mem: 15572
Epoch: [3]  [ 860/1404]  eta: 0:05:21  lr: 0.000068  min_lr: 0.000001  loss: 4.7408 (4.7570)  class_acc: 0.0833 (0.0836)  loss_scale: 32768.0000 (46773.3705)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0512  max mem: 15572
Epoch: [3]  [ 870/1404]  eta: 0:05:14  lr: 0.000068  min_lr: 0.000001  loss: 4.6768 (4.7573)  class_acc: 0.0833 (0.0835)  loss_scale: 32768.0000 (46612.5741)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0672  max mem: 15572
[2025-01-16 21:07:02,158] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:07:02,159] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:07:02,179] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:07:02,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 880/1404]  eta: 0:05:09  lr: 0.000068  min_lr: 0.000001  loss: 4.6707 (4.7556)  class_acc: 0.1250 (0.0840)  loss_scale: 32768.0000 (46641.3984)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0879  max mem: 15572
Epoch: [3]  [ 890/1404]  eta: 0:05:03  lr: 0.000068  min_lr: 0.000001  loss: 4.6791 (4.7558)  class_acc: 0.1250 (0.0841)  loss_scale: 65536.0000 (46853.4590)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1021  max mem: 15572
Epoch: [3]  [ 900/1404]  eta: 0:04:57  lr: 0.000068  min_lr: 0.000001  loss: 4.7881 (4.7566)  class_acc: 0.0417 (0.0838)  loss_scale: 65536.0000 (47060.8124)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0553  max mem: 15572
Epoch: [3]  [ 910/1404]  eta: 0:04:51  lr: 0.000068  min_lr: 0.000001  loss: 4.7355 (4.7558)  class_acc: 0.0417 (0.0839)  loss_scale: 65536.0000 (47263.6136)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.0532  max mem: 15572
Epoch: [3]  [ 920/1404]  eta: 0:04:45  lr: 0.000069  min_lr: 0.000001  loss: 4.7261 (4.7563)  class_acc: 0.0417 (0.0840)  loss_scale: 65536.0000 (47462.0109)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0286  max mem: 15572
Epoch: [3]  [ 930/1404]  eta: 0:04:39  lr: 0.000069  min_lr: 0.000001  loss: 4.6941 (4.7549)  class_acc: 0.0833 (0.0839)  loss_scale: 65536.0000 (47656.1461)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0281  max mem: 15572
Epoch: [3]  [ 940/1404]  eta: 0:04:33  lr: 0.000069  min_lr: 0.000001  loss: 4.6712 (4.7553)  class_acc: 0.0833 (0.0841)  loss_scale: 65536.0000 (47846.1552)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0367  max mem: 15572
Epoch: [3]  [ 950/1404]  eta: 0:04:27  lr: 0.000069  min_lr: 0.000001  loss: 4.7305 (4.7550)  class_acc: 0.0833 (0.0843)  loss_scale: 65536.0000 (48032.1682)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0766  max mem: 15572
[2025-01-16 21:07:46,494] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5164
[2025-01-16 21:07:46,494] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:07:46,513] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5164
[2025-01-16 21:07:46,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:07:46,514] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [ 960/1404]  eta: 0:04:22  lr: 0.000069  min_lr: 0.000001  loss: 4.7305 (4.7546)  class_acc: 0.0833 (0.0847)  loss_scale: 65536.0000 (47907.4298)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0682  max mem: 15572
Epoch: [3]  [ 970/1404]  eta: 0:04:16  lr: 0.000069  min_lr: 0.000001  loss: 4.8146 (4.7556)  class_acc: 0.0417 (0.0844)  loss_scale: 32768.0000 (47751.5139)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.0007  max mem: 15572
Epoch: [3]  [ 980/1404]  eta: 0:04:10  lr: 0.000069  min_lr: 0.000001  loss: 4.7724 (4.7553)  class_acc: 0.0833 (0.0844)  loss_scale: 32768.0000 (47598.7768)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.0006  max mem: 15572
Epoch: [3]  [ 990/1404]  eta: 0:04:04  lr: 0.000069  min_lr: 0.000001  loss: 4.7724 (4.7555)  class_acc: 0.0833 (0.0843)  loss_scale: 32768.0000 (47449.1221)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0006  max mem: 15572
Epoch: [3]  [1000/1404]  eta: 0:03:58  lr: 0.000070  min_lr: 0.000001  loss: 4.7247 (4.7547)  class_acc: 0.0833 (0.0846)  loss_scale: 32768.0000 (47302.4575)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0008  max mem: 15572
Epoch: [3]  [1010/1404]  eta: 0:03:52  lr: 0.000070  min_lr: 0.000001  loss: 4.7125 (4.7543)  class_acc: 0.1250 (0.0849)  loss_scale: 32768.0000 (47158.6944)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0009  max mem: 15572
Epoch: [3]  [1020/1404]  eta: 0:03:46  lr: 0.000070  min_lr: 0.000001  loss: 4.6902 (4.7529)  class_acc: 0.0833 (0.0848)  loss_scale: 32768.0000 (47017.7473)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0007  max mem: 15572
Epoch: [3]  [1030/1404]  eta: 0:03:40  lr: 0.000070  min_lr: 0.000001  loss: 4.6364 (4.7526)  class_acc: 0.0417 (0.0847)  loss_scale: 32768.0000 (46879.5344)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0007  max mem: 15572
Epoch: [3]  [1040/1404]  eta: 0:03:34  lr: 0.000070  min_lr: 0.000001  loss: 4.6521 (4.7516)  class_acc: 0.0833 (0.0847)  loss_scale: 32768.0000 (46743.9769)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0007  max mem: 15572
Epoch: [3]  [1050/1404]  eta: 0:03:28  lr: 0.000070  min_lr: 0.000001  loss: 4.6521 (4.7507)  class_acc: 0.0417 (0.0843)  loss_scale: 32768.0000 (46610.9990)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0157  max mem: 15572
Epoch: [3]  [1060/1404]  eta: 0:03:22  lr: 0.000070  min_lr: 0.000001  loss: 4.6103 (4.7500)  class_acc: 0.0833 (0.0846)  loss_scale: 32768.0000 (46480.5278)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.0494  max mem: 15572
Epoch: [3]  [1070/1404]  eta: 0:03:17  lr: 0.000071  min_lr: 0.000001  loss: 4.6103 (4.7494)  class_acc: 0.1250 (0.0849)  loss_scale: 32768.0000 (46352.4930)  weight_decay: 0.0500 (0.0500)  time: 0.6765  data: 0.1303  max mem: 15572
Epoch: [3]  [1080/1404]  eta: 0:03:11  lr: 0.000071  min_lr: 0.000001  loss: 4.7248 (4.7500)  class_acc: 0.0833 (0.0847)  loss_scale: 32768.0000 (46226.8270)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1242  max mem: 15572
[2025-01-16 21:09:03,124] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:09:03,125] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:09:03,136] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:09:03,137] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [1090/1404]  eta: 0:03:05  lr: 0.000071  min_lr: 0.000001  loss: 4.8289 (4.7507)  class_acc: 0.0417 (0.0845)  loss_scale: 32768.0000 (46403.8130)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0823  max mem: 15572
Epoch: [3]  [1100/1404]  eta: 0:02:59  lr: 0.000071  min_lr: 0.000001  loss: 4.7772 (4.7498)  class_acc: 0.0417 (0.0847)  loss_scale: 65536.0000 (46577.5840)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1067  max mem: 15572
Epoch: [3]  [1110/1404]  eta: 0:02:53  lr: 0.000071  min_lr: 0.000001  loss: 4.6329 (4.7490)  class_acc: 0.1250 (0.0854)  loss_scale: 65536.0000 (46748.2268)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1081  max mem: 15572
Epoch: [3]  [1120/1404]  eta: 0:02:47  lr: 0.000071  min_lr: 0.000001  loss: 4.6329 (4.7491)  class_acc: 0.1250 (0.0853)  loss_scale: 65536.0000 (46915.8252)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1021  max mem: 15572
Epoch: [3]  [1130/1404]  eta: 0:02:41  lr: 0.000071  min_lr: 0.000001  loss: 4.7439 (4.7490)  class_acc: 0.0417 (0.0851)  loss_scale: 65536.0000 (47080.4598)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0765  max mem: 15572
[2025-01-16 21:09:37,618] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5352
[2025-01-16 21:09:37,618] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:09:37,618] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [1140/1404]  eta: 0:02:35  lr: 0.000071  min_lr: 0.000001  loss: 4.7458 (4.7494)  class_acc: 0.0833 (0.0852)  loss_scale: 65536.0000 (47213.4899)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0305  max mem: 15572
[2025-01-16 21:09:37,622] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5352
[2025-01-16 21:09:37,623] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [1150/1404]  eta: 0:02:29  lr: 0.000072  min_lr: 0.000001  loss: 4.7458 (4.7492)  class_acc: 0.0833 (0.0856)  loss_scale: 32768.0000 (47087.9861)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0039  max mem: 15572
Epoch: [3]  [1160/1404]  eta: 0:02:24  lr: 0.000072  min_lr: 0.000001  loss: 4.6831 (4.7470)  class_acc: 0.0833 (0.0860)  loss_scale: 32768.0000 (46964.6443)  weight_decay: 0.0500 (0.0500)  time: 0.6546  data: 0.1389  max mem: 15572
Epoch: [3]  [1170/1404]  eta: 0:02:18  lr: 0.000072  min_lr: 0.000001  loss: 4.5918 (4.7458)  class_acc: 0.0833 (0.0861)  loss_scale: 32768.0000 (46843.4091)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.1357  max mem: 15572
Epoch: [3]  [1180/1404]  eta: 0:02:12  lr: 0.000072  min_lr: 0.000001  loss: 4.5874 (4.7452)  class_acc: 0.0833 (0.0863)  loss_scale: 32768.0000 (46724.2269)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0008  max mem: 15572
Epoch: [3]  [1190/1404]  eta: 0:02:06  lr: 0.000072  min_lr: 0.000001  loss: 4.7345 (4.7465)  class_acc: 0.0833 (0.0860)  loss_scale: 32768.0000 (46607.0462)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0117  max mem: 15572
Epoch: [3]  [1200/1404]  eta: 0:02:00  lr: 0.000072  min_lr: 0.000001  loss: 4.7931 (4.7464)  class_acc: 0.0833 (0.0863)  loss_scale: 32768.0000 (46491.8168)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0653  max mem: 15572
Epoch: [3]  [1210/1404]  eta: 0:01:54  lr: 0.000072  min_lr: 0.000001  loss: 4.7004 (4.7454)  class_acc: 0.1250 (0.0866)  loss_scale: 32768.0000 (46378.4905)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0543  max mem: 15572
Epoch: [3]  [1220/1404]  eta: 0:01:48  lr: 0.000073  min_lr: 0.000001  loss: 4.6283 (4.7454)  class_acc: 0.1250 (0.0866)  loss_scale: 32768.0000 (46267.0205)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.0442  max mem: 15572
Epoch: [3]  [1230/1404]  eta: 0:01:42  lr: 0.000073  min_lr: 0.000001  loss: 4.6370 (4.7450)  class_acc: 0.0833 (0.0869)  loss_scale: 32768.0000 (46157.3615)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0588  max mem: 15572
Epoch: [3]  [1240/1404]  eta: 0:01:36  lr: 0.000073  min_lr: 0.000001  loss: 4.6914 (4.7446)  class_acc: 0.0833 (0.0867)  loss_scale: 32768.0000 (46049.4698)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.0789  max mem: 15572
Epoch: [3]  [1250/1404]  eta: 0:01:30  lr: 0.000073  min_lr: 0.000001  loss: 4.7185 (4.7443)  class_acc: 0.0833 (0.0869)  loss_scale: 32768.0000 (45943.3030)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1118  max mem: 15572
Epoch: [3]  [1260/1404]  eta: 0:01:24  lr: 0.000073  min_lr: 0.000001  loss: 4.7316 (4.7439)  class_acc: 0.0833 (0.0868)  loss_scale: 32768.0000 (45838.8200)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1061  max mem: 15572
[2025-01-16 21:10:53,896] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:10:53,897] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:10:53,898] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:10:53,898] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [1270/1404]  eta: 0:01:19  lr: 0.000073  min_lr: 0.000001  loss: 4.7414 (4.7443)  class_acc: 0.0417 (0.0868)  loss_scale: 32768.0000 (45787.5437)  weight_decay: 0.0500 (0.0500)  time: 0.6499  data: 0.1538  max mem: 15572
Epoch: [3]  [1280/1404]  eta: 0:01:13  lr: 0.000073  min_lr: 0.000001  loss: 4.6271 (4.7427)  class_acc: 0.0833 (0.0868)  loss_scale: 65536.0000 (45941.7080)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1207  max mem: 15572
Epoch: [3]  [1290/1404]  eta: 0:01:07  lr: 0.000073  min_lr: 0.000001  loss: 4.5350 (4.7419)  class_acc: 0.1250 (0.0872)  loss_scale: 65536.0000 (46093.4841)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.0791  max mem: 15572
Epoch: [3]  [1300/1404]  eta: 0:01:01  lr: 0.000074  min_lr: 0.000001  loss: 4.6632 (4.7410)  class_acc: 0.1250 (0.0872)  loss_scale: 65536.0000 (46242.9270)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0827  max mem: 15572
Epoch: [3]  [1310/1404]  eta: 0:00:55  lr: 0.000074  min_lr: 0.000001  loss: 4.6602 (4.7410)  class_acc: 0.0833 (0.0871)  loss_scale: 65536.0000 (46390.0900)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0892  max mem: 15572
Epoch: [3]  [1320/1404]  eta: 0:00:49  lr: 0.000074  min_lr: 0.000001  loss: 4.6602 (4.7405)  class_acc: 0.0833 (0.0874)  loss_scale: 65536.0000 (46535.0250)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0609  max mem: 15572
Epoch: [3]  [1330/1404]  eta: 0:00:43  lr: 0.000074  min_lr: 0.000001  loss: 4.6277 (4.7396)  class_acc: 0.1250 (0.0877)  loss_scale: 65536.0000 (46677.7821)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0099  max mem: 15572
Epoch: [3]  [1340/1404]  eta: 0:00:37  lr: 0.000074  min_lr: 0.000001  loss: 4.7243 (4.7398)  class_acc: 0.0833 (0.0876)  loss_scale: 65536.0000 (46818.4101)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0640  max mem: 15572
Epoch: [3]  [1350/1404]  eta: 0:00:31  lr: 0.000074  min_lr: 0.000001  loss: 4.7374 (4.7399)  class_acc: 0.0833 (0.0874)  loss_scale: 65536.0000 (46956.9563)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1084  max mem: 15572
Epoch: [3]  [1360/1404]  eta: 0:00:25  lr: 0.000074  min_lr: 0.000001  loss: 4.6888 (4.7391)  class_acc: 0.0417 (0.0873)  loss_scale: 65536.0000 (47093.4666)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0748  max mem: 15572
[2025-01-16 21:11:48,495] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5578
[2025-01-16 21:11:48,495] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:11:48,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:11:48,496] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5578
[2025-01-16 21:11:48,496] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [1370/1404]  eta: 0:00:20  lr: 0.000075  min_lr: 0.000001  loss: 4.5906 (4.7383)  class_acc: 0.0833 (0.0874)  loss_scale: 65536.0000 (47108.4814)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0638  max mem: 15572
Epoch: [3]  [1380/1404]  eta: 0:00:14  lr: 0.000075  min_lr: 0.000001  loss: 4.6763 (4.7380)  class_acc: 0.0833 (0.0874)  loss_scale: 32768.0000 (47004.6401)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1006  max mem: 15572
Epoch: [3]  [1390/1404]  eta: 0:00:08  lr: 0.000075  min_lr: 0.000001  loss: 4.6870 (4.7373)  class_acc: 0.0833 (0.0874)  loss_scale: 32768.0000 (46902.2919)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1079  max mem: 15572
Epoch: [3]  [1400/1404]  eta: 0:00:02  lr: 0.000075  min_lr: 0.000001  loss: 4.6422 (4.7373)  class_acc: 0.1250 (0.0877)  loss_scale: 32768.0000 (46801.4047)  weight_decay: 0.0500 (0.0500)  time: 0.4890  data: 0.0504  max mem: 15572
Epoch: [3]  [1403/1404]  eta: 0:00:00  lr: 0.000075  min_lr: 0.000001  loss: 4.6422 (4.7370)  class_acc: 0.1250 (0.0877)  loss_scale: 32768.0000 (46771.4188)  weight_decay: 0.0500 (0.0500)  time: 0.4658  data: 0.0503  max mem: 15572
Epoch: [3] Total time: 0:13:44 (0.5873 s / it)
Averaged stats: lr: 0.000075  min_lr: 0.000001  loss: 4.6422 (4.7317)  class_acc: 0.1250 (0.0893)  loss_scale: 32768.0000 (46771.4188)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:08:43  loss: 3.3612 (3.3612)  acc1: 50.0000 (50.0000)  acc5: 66.6667 (66.6667)  time: 3.8529  data: 3.6563  max mem: 15572
Val:  [ 10/136]  eta: 0:01:35  loss: 4.1240 (4.1801)  acc1: 0.0000 (11.6162)  acc5: 11.1111 (26.2626)  time: 0.7560  data: 0.5603  max mem: 15572
Val:  [ 20/136]  eta: 0:01:02  loss: 4.1240 (4.0895)  acc1: 0.0000 (10.3175)  acc5: 22.2222 (33.3333)  time: 0.3772  data: 0.1899  max mem: 15572
Val:  [ 30/136]  eta: 0:00:52  loss: 3.5624 (3.9450)  acc1: 5.5556 (17.9211)  acc5: 55.5556 (41.3978)  time: 0.3510  data: 0.1654  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 3.4678 (3.9186)  acc1: 16.6667 (18.2927)  acc5: 66.6667 (41.4634)  time: 0.3635  data: 0.1679  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 4.3362 (4.0360)  acc1: 0.0000 (15.4684)  acc5: 0.0000 (35.5120)  time: 0.3363  data: 0.1436  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 4.4693 (4.1148)  acc1: 0.0000 (13.2969)  acc5: 5.5556 (33.1512)  time: 0.3509  data: 0.1596  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 4.2742 (4.0692)  acc1: 5.5556 (14.9452)  acc5: 27.7778 (35.5243)  time: 0.3459  data: 0.1445  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 4.1219 (4.0843)  acc1: 11.1111 (14.4033)  acc5: 33.3333 (35.1166)  time: 0.3549  data: 0.1465  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 4.1695 (4.0872)  acc1: 5.5556 (14.4078)  acc5: 33.3333 (35.4701)  time: 0.3645  data: 0.1529  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 4.1695 (4.0981)  acc1: 0.0000 (14.3564)  acc5: 33.3333 (35.5886)  time: 0.3408  data: 0.1321  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 4.3235 (4.1216)  acc1: 5.5556 (13.6637)  acc5: 16.6667 (34.1341)  time: 0.3456  data: 0.1408  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.0863 (4.0975)  acc1: 5.5556 (14.4628)  acc5: 27.7778 (36.5014)  time: 0.3440  data: 0.1464  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 3.8321 (4.0836)  acc1: 22.2222 (15.6913)  acc5: 44.4444 (36.7260)  time: 0.2826  data: 0.1110  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 3.8515 (4.0855)  acc1: 22.2222 (15.8067)  acc5: 44.4444 (37.2645)  time: 0.2261  data: 0.0701  max mem: 15572
Val: Total time: 0:00:49 (0.3650 s / it)
* Acc@1 15.520 Acc@5 36.916 loss 4.111
Accuracy of the network on the 4883 val videos: 15.5%
[2025-01-16 21:12:58,336] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 21:12:58,338] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 21:12:58,338] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 21:12:58,338] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 21:13:00,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 21:13:00,726] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 15.52%
Epoch: [4]  [   0/1404]  eta: 2:37:38  lr: 0.000075  min_lr: 0.000001  loss: 4.2637 (4.2637)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.7365  data: 5.3830  max mem: 15572
Epoch: [4]  [  10/1404]  eta: 0:25:39  lr: 0.000075  min_lr: 0.000001  loss: 4.7811 (4.6647)  class_acc: 0.0833 (0.0985)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1045  data: 0.5152  max mem: 15572
Epoch: [4]  [  20/1404]  eta: 0:20:52  lr: 0.000075  min_lr: 0.000001  loss: 4.7618 (4.6940)  class_acc: 0.0833 (0.1091)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0465  max mem: 15572
Epoch: [4]  [  30/1404]  eta: 0:18:32  lr: 0.000075  min_lr: 0.000001  loss: 4.7375 (4.7087)  class_acc: 0.1250 (0.1048)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.0327  max mem: 15572
Epoch: [4]  [  40/1404]  eta: 0:17:04  lr: 0.000076  min_lr: 0.000001  loss: 4.6789 (4.7145)  class_acc: 0.1250 (0.1108)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0007  max mem: 15572
Epoch: [4]  [  50/1404]  eta: 0:16:13  lr: 0.000076  min_lr: 0.000001  loss: 4.6691 (4.6960)  class_acc: 0.0833 (0.1062)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0006  max mem: 15572
Epoch: [4]  [  60/1404]  eta: 0:15:43  lr: 0.000076  min_lr: 0.000001  loss: 4.6395 (4.6811)  class_acc: 0.0833 (0.1100)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0006  max mem: 15572
Epoch: [4]  [  70/1404]  eta: 0:14:56  lr: 0.000076  min_lr: 0.000001  loss: 4.6395 (4.6818)  class_acc: 0.0833 (0.1074)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0007  max mem: 15572
Epoch: [4]  [  80/1404]  eta: 0:14:28  lr: 0.000076  min_lr: 0.000001  loss: 4.6700 (4.6755)  class_acc: 0.0833 (0.1029)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0009  max mem: 15572
Epoch: [4]  [  90/1404]  eta: 0:14:22  lr: 0.000076  min_lr: 0.000001  loss: 4.6700 (4.6801)  class_acc: 0.0833 (0.1026)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0553  max mem: 15572
[2025-01-16 21:14:00,994] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:14:00,995] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:14:00,996] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:14:00,996] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [ 100/1404]  eta: 0:13:57  lr: 0.000076  min_lr: 0.000001  loss: 4.7559 (4.6804)  class_acc: 0.0833 (0.1015)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0552  max mem: 15572
Epoch: [4]  [ 110/1404]  eta: 0:13:42  lr: 0.000076  min_lr: 0.000001  loss: 4.7559 (4.6835)  class_acc: 0.0833 (0.1036)  loss_scale: 65536.0000 (38672.1441)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0010  max mem: 15572
Epoch: [4]  [ 120/1404]  eta: 0:13:24  lr: 0.000077  min_lr: 0.000001  loss: 4.7156 (4.6825)  class_acc: 0.0833 (0.1019)  loss_scale: 65536.0000 (40892.2975)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.0010  max mem: 15572
Epoch: [4]  [ 130/1404]  eta: 0:13:19  lr: 0.000077  min_lr: 0.000001  loss: 4.7156 (4.6883)  class_acc: 0.0833 (0.1024)  loss_scale: 65536.0000 (42773.4962)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0007  max mem: 15572
Epoch: [4]  [ 140/1404]  eta: 0:13:05  lr: 0.000077  min_lr: 0.000001  loss: 4.7648 (4.6930)  class_acc: 0.0833 (0.1022)  loss_scale: 65536.0000 (44387.8582)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0006  max mem: 15572
Epoch: [4]  [ 150/1404]  eta: 0:12:54  lr: 0.000077  min_lr: 0.000001  loss: 4.6557 (4.6906)  class_acc: 0.0833 (0.1029)  loss_scale: 65536.0000 (45788.3974)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0007  max mem: 15572
Epoch: [4]  [ 160/1404]  eta: 0:12:52  lr: 0.000077  min_lr: 0.000001  loss: 4.6432 (4.6914)  class_acc: 0.1250 (0.1079)  loss_scale: 65536.0000 (47014.9565)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.0011  max mem: 15572
Epoch: [4]  [ 170/1404]  eta: 0:12:47  lr: 0.000077  min_lr: 0.000001  loss: 4.7161 (4.6907)  class_acc: 0.1250 (0.1065)  loss_scale: 65536.0000 (48098.0585)  weight_decay: 0.0500 (0.0500)  time: 0.6560  data: 0.0011  max mem: 15572
Epoch: [4]  [ 180/1404]  eta: 0:12:42  lr: 0.000077  min_lr: 0.000001  loss: 4.6283 (4.6890)  class_acc: 0.0833 (0.1059)  loss_scale: 65536.0000 (49061.4807)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.0009  max mem: 15572
Epoch: [4]  [ 190/1404]  eta: 0:12:28  lr: 0.000078  min_lr: 0.000001  loss: 4.7295 (4.6895)  class_acc: 0.0833 (0.1051)  loss_scale: 65536.0000 (49924.0209)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0008  max mem: 15572
Epoch: [4]  [ 200/1404]  eta: 0:12:19  lr: 0.000078  min_lr: 0.000001  loss: 4.7392 (4.6882)  class_acc: 0.1250 (0.1080)  loss_scale: 65536.0000 (50700.7363)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0008  max mem: 15572
Epoch: [4]  [ 210/1404]  eta: 0:12:12  lr: 0.000078  min_lr: 0.000001  loss: 4.7392 (4.6916)  class_acc: 0.0833 (0.1062)  loss_scale: 65536.0000 (51403.8294)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0007  max mem: 15572
[2025-01-16 21:15:15,717] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:15:15,718] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 21:15:15,738] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:15:15,739] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 21:15:16,164] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5836
[2025-01-16 21:15:16,165] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 21:15:16,165] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 21:15:16,165] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5836
[2025-01-16 21:15:16,165] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [4]  [ 220/1404]  eta: 0:12:05  lr: 0.000078  min_lr: 0.000001  loss: 4.7012 (4.6912)  class_acc: 0.0833 (0.1061)  loss_scale: 65536.0000 (52339.8371)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0046  max mem: 15572
Epoch: [4]  [ 230/1404]  eta: 0:12:01  lr: 0.000078  min_lr: 0.000001  loss: 4.7049 (4.6951)  class_acc: 0.0833 (0.1066)  loss_scale: 65536.0000 (52911.0996)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.0074  max mem: 15572
Epoch: [4]  [ 240/1404]  eta: 0:11:54  lr: 0.000078  min_lr: 0.000001  loss: 4.6834 (4.6917)  class_acc: 0.1250 (0.1079)  loss_scale: 65536.0000 (53434.9544)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.0033  max mem: 15572
Epoch: [4]  [ 250/1404]  eta: 0:11:44  lr: 0.000078  min_lr: 0.000001  loss: 4.6800 (4.6960)  class_acc: 0.0833 (0.1071)  loss_scale: 65536.0000 (53917.0677)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.0006  max mem: 15572
Epoch: [4]  [ 260/1404]  eta: 0:11:36  lr: 0.000078  min_lr: 0.000001  loss: 4.7548 (4.6989)  class_acc: 0.0833 (0.1066)  loss_scale: 65536.0000 (54362.2375)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0009  max mem: 15572
Epoch: [4]  [ 270/1404]  eta: 0:11:29  lr: 0.000079  min_lr: 0.000001  loss: 4.7653 (4.6984)  class_acc: 0.0833 (0.1061)  loss_scale: 65536.0000 (54774.5535)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0010  max mem: 15572
Epoch: [4]  [ 280/1404]  eta: 0:11:21  lr: 0.000079  min_lr: 0.000001  loss: 4.6054 (4.6925)  class_acc: 0.1250 (0.1074)  loss_scale: 65536.0000 (55157.5231)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0008  max mem: 15572
Epoch: [4]  [ 290/1404]  eta: 0:11:17  lr: 0.000079  min_lr: 0.000001  loss: 4.5417 (4.6885)  class_acc: 0.1250 (0.1091)  loss_scale: 65536.0000 (55514.1718)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0006  max mem: 15572
Epoch: [4]  [ 300/1404]  eta: 0:11:10  lr: 0.000079  min_lr: 0.000001  loss: 4.6266 (4.6878)  class_acc: 0.1250 (0.1087)  loss_scale: 65536.0000 (55847.1229)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.0006  max mem: 15572
Epoch: [4]  [ 310/1404]  eta: 0:11:03  lr: 0.000079  min_lr: 0.000001  loss: 4.6266 (4.6860)  class_acc: 0.0833 (0.1091)  loss_scale: 65536.0000 (56158.6624)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0007  max mem: 15572
Epoch: [4]  [ 320/1404]  eta: 0:10:55  lr: 0.000079  min_lr: 0.000001  loss: 4.6264 (4.6869)  class_acc: 0.0833 (0.1081)  loss_scale: 65536.0000 (56450.7913)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0007  max mem: 15572
Epoch: [4]  [ 330/1404]  eta: 0:10:48  lr: 0.000079  min_lr: 0.000001  loss: 4.6076 (4.6843)  class_acc: 0.0833 (0.1081)  loss_scale: 65536.0000 (56725.2689)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0008  max mem: 15572
Epoch: [4]  [ 340/1404]  eta: 0:10:42  lr: 0.000080  min_lr: 0.000001  loss: 4.5657 (4.6846)  class_acc: 0.0833 (0.1076)  loss_scale: 65536.0000 (56983.6481)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0006  max mem: 15572
[2025-01-16 21:16:31,158] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:16:31,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 21:16:31,206] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:16:31,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 350/1404]  eta: 0:10:33  lr: 0.000080  min_lr: 0.000001  loss: 4.6187 (4.6841)  class_acc: 0.0833 (0.1073)  loss_scale: 65536.0000 (57600.7293)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0004  max mem: 15572
[2025-01-16 21:16:33,176] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5968
[2025-01-16 21:16:33,177] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 21:16:33,182] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5968
[2025-01-16 21:16:33,183] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 21:16:33,183] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 360/1404]  eta: 0:10:25  lr: 0.000080  min_lr: 0.000001  loss: 4.5998 (4.6808)  class_acc: 0.0833 (0.1070)  loss_scale: 65536.0000 (58002.0831)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0005  max mem: 15572
Epoch: [4]  [ 370/1404]  eta: 0:10:19  lr: 0.000080  min_lr: 0.000001  loss: 4.5998 (4.6798)  class_acc: 0.0833 (0.1059)  loss_scale: 65536.0000 (58205.1536)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0006  max mem: 15572
Epoch: [4]  [ 380/1404]  eta: 0:10:12  lr: 0.000080  min_lr: 0.000001  loss: 4.6138 (4.6814)  class_acc: 0.0833 (0.1068)  loss_scale: 65536.0000 (58397.5643)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0006  max mem: 15572
[2025-01-16 21:16:52,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=30, lr=[7.763354467760621e-07, 7.763354467760621e-07, 1.1090506382515173e-06, 1.1090506382515173e-06, 1.584358054645025e-06, 1.584358054645025e-06, 2.263368649492893e-06, 2.263368649492893e-06, 3.2333837849898472e-06, 3.2333837849898472e-06, 4.619119692842639e-06, 4.619119692842639e-06, 6.5987424183466274e-06, 6.5987424183466274e-06, 9.426774883352326e-06, 9.426774883352326e-06, 1.3466821261931895e-05, 1.3466821261931895e-05, 1.923831608847414e-05, 1.923831608847414e-05, 2.7483308697820196e-05, 2.7483308697820196e-05, 3.926186956831457e-05, 3.926186956831457e-05, 5.6088385097592245e-05, 5.6088385097592245e-05, 8.012626442513179e-05, 8.012626442513179e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 21:16:52,416] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=47.74591001735828, CurrSamplesPerSec=55.25467365899552, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [ 390/1404]  eta: 0:10:09  lr: 0.000080  min_lr: 0.000001  loss: 4.5855 (4.6782)  class_acc: 0.0833 (0.1065)  loss_scale: 65536.0000 (58580.1330)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.0005  max mem: 15572
Epoch: [4]  [ 400/1404]  eta: 0:09:59  lr: 0.000080  min_lr: 0.000001  loss: 4.6246 (4.6790)  class_acc: 0.1250 (0.1072)  loss_scale: 65536.0000 (58753.5960)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0006  max mem: 15572
Epoch: [4]  [ 410/1404]  eta: 0:09:54  lr: 0.000080  min_lr: 0.000001  loss: 4.7055 (4.6781)  class_acc: 0.1250 (0.1084)  loss_scale: 65536.0000 (58918.6180)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.0006  max mem: 15572
Epoch: [4]  [ 420/1404]  eta: 0:09:49  lr: 0.000081  min_lr: 0.000001  loss: 4.5234 (4.6747)  class_acc: 0.0833 (0.1086)  loss_scale: 65536.0000 (59075.8005)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.0382  max mem: 15572
Epoch: [4]  [ 430/1404]  eta: 0:09:43  lr: 0.000081  min_lr: 0.000001  loss: 4.5875 (4.6747)  class_acc: 0.0833 (0.1088)  loss_scale: 65536.0000 (59225.6891)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.0384  max mem: 15572
Epoch: [4]  [ 440/1404]  eta: 0:09:35  lr: 0.000081  min_lr: 0.000001  loss: 4.6827 (4.6728)  class_acc: 0.0833 (0.1094)  loss_scale: 65536.0000 (59368.7800)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0007  max mem: 15572
[2025-01-16 21:17:29,483] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6064
[2025-01-16 21:17:29,483] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:17:29,483] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:17:29,545] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6064
[2025-01-16 21:17:29,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [4]  [ 450/1404]  eta: 0:09:30  lr: 0.000081  min_lr: 0.000001  loss: 4.6319 (4.6740)  class_acc: 0.0833 (0.1088)  loss_scale: 65536.0000 (59287.5565)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0012  max mem: 15572
Epoch: [4]  [ 460/1404]  eta: 0:09:25  lr: 0.000081  min_lr: 0.000001  loss: 4.6971 (4.6756)  class_acc: 0.1250 (0.1097)  loss_scale: 32768.0000 (58712.2950)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.0010  max mem: 15572
Epoch: [4]  [ 470/1404]  eta: 0:09:18  lr: 0.000081  min_lr: 0.000001  loss: 4.6971 (4.6752)  class_acc: 0.1250 (0.1094)  loss_scale: 32768.0000 (58161.4607)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0006  max mem: 15572
Epoch: [4]  [ 480/1404]  eta: 0:09:14  lr: 0.000081  min_lr: 0.000001  loss: 4.6132 (4.6740)  class_acc: 0.0417 (0.1089)  loss_scale: 32768.0000 (57633.5301)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0007  max mem: 15572
Epoch: [4]  [ 490/1404]  eta: 0:09:07  lr: 0.000082  min_lr: 0.000001  loss: 4.6329 (4.6732)  class_acc: 0.0833 (0.1095)  loss_scale: 32768.0000 (57127.1039)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.0011  max mem: 15572
Epoch: [4]  [ 500/1404]  eta: 0:09:01  lr: 0.000082  min_lr: 0.000001  loss: 4.6455 (4.6718)  class_acc: 0.1250 (0.1099)  loss_scale: 32768.0000 (56640.8942)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0011  max mem: 15572
Epoch: [4]  [ 510/1404]  eta: 0:08:54  lr: 0.000082  min_lr: 0.000001  loss: 4.5974 (4.6697)  class_acc: 0.1250 (0.1098)  loss_scale: 32768.0000 (56173.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0007  max mem: 15572
Epoch: [4]  [ 520/1404]  eta: 0:08:49  lr: 0.000082  min_lr: 0.000001  loss: 4.6151 (4.6705)  class_acc: 0.1250 (0.1111)  loss_scale: 32768.0000 (55724.4683)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0005  max mem: 15572
Epoch: [4]  [ 530/1404]  eta: 0:08:42  lr: 0.000082  min_lr: 0.000001  loss: 4.6859 (4.6716)  class_acc: 0.1250 (0.1112)  loss_scale: 32768.0000 (55292.1431)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0004  max mem: 15572
Epoch: [4]  [ 540/1404]  eta: 0:08:35  lr: 0.000082  min_lr: 0.000001  loss: 4.6489 (4.6724)  class_acc: 0.0833 (0.1104)  loss_scale: 32768.0000 (54875.8004)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0005  max mem: 15572
Epoch: [4]  [ 550/1404]  eta: 0:08:30  lr: 0.000082  min_lr: 0.000001  loss: 4.6489 (4.6722)  class_acc: 0.0833 (0.1109)  loss_scale: 32768.0000 (54474.5699)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0007  max mem: 15572
Epoch: [4]  [ 560/1404]  eta: 0:08:23  lr: 0.000082  min_lr: 0.000001  loss: 4.7145 (4.6746)  class_acc: 0.0833 (0.1100)  loss_scale: 32768.0000 (54087.6435)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0007  max mem: 15572
Epoch: [4]  [ 570/1404]  eta: 0:08:17  lr: 0.000083  min_lr: 0.000001  loss: 4.7145 (4.6753)  class_acc: 0.0833 (0.1103)  loss_scale: 32768.0000 (53714.2697)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0005  max mem: 15572
[2025-01-16 21:18:46,284] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:18:46,284] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:18:46,292] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:18:46,293] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [ 580/1404]  eta: 0:08:11  lr: 0.000083  min_lr: 0.000001  loss: 4.7668 (4.6774)  class_acc: 0.0833 (0.1100)  loss_scale: 32768.0000 (53579.3460)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.0004  max mem: 15572
Epoch: [4]  [ 590/1404]  eta: 0:08:04  lr: 0.000083  min_lr: 0.000001  loss: 4.8347 (4.6793)  class_acc: 0.0833 (0.1108)  loss_scale: 65536.0000 (53781.6582)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0006  max mem: 15572
Epoch: [4]  [ 600/1404]  eta: 0:07:57  lr: 0.000083  min_lr: 0.000001  loss: 4.6112 (4.6764)  class_acc: 0.1250 (0.1115)  loss_scale: 65536.0000 (53977.2379)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0007  max mem: 15572
Epoch: [4]  [ 610/1404]  eta: 0:07:52  lr: 0.000083  min_lr: 0.000001  loss: 4.5455 (4.6767)  class_acc: 0.1250 (0.1118)  loss_scale: 65536.0000 (54166.4157)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0007  max mem: 15572
Epoch: [4]  [ 620/1404]  eta: 0:07:46  lr: 0.000083  min_lr: 0.000001  loss: 4.5523 (4.6738)  class_acc: 0.1250 (0.1120)  loss_scale: 65536.0000 (54349.5008)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0006  max mem: 15572
Epoch: [4]  [ 630/1404]  eta: 0:07:41  lr: 0.000083  min_lr: 0.000001  loss: 4.5523 (4.6744)  class_acc: 0.0833 (0.1117)  loss_scale: 65536.0000 (54526.7829)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.0005  max mem: 15572
[2025-01-16 21:19:21,814] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6254
[2025-01-16 21:19:21,814] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:19:21,815] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6254
[2025-01-16 21:19:21,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:19:21,815] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [ 640/1404]  eta: 0:07:35  lr: 0.000084  min_lr: 0.000001  loss: 4.6643 (4.6745)  class_acc: 0.0833 (0.1115)  loss_scale: 65536.0000 (54545.1732)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.0006  max mem: 15572
Epoch: [4]  [ 650/1404]  eta: 0:07:28  lr: 0.000084  min_lr: 0.000001  loss: 4.6751 (4.6750)  class_acc: 0.0833 (0.1115)  loss_scale: 32768.0000 (54210.6544)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0007  max mem: 15572
Epoch: [4]  [ 660/1404]  eta: 0:07:23  lr: 0.000084  min_lr: 0.000001  loss: 4.6358 (4.6738)  class_acc: 0.0833 (0.1118)  loss_scale: 32768.0000 (53886.2572)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0008  max mem: 15572
Epoch: [4]  [ 670/1404]  eta: 0:07:17  lr: 0.000084  min_lr: 0.000001  loss: 4.5794 (4.6730)  class_acc: 0.1250 (0.1120)  loss_scale: 32768.0000 (53571.5291)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.0006  max mem: 15572
Epoch: [4]  [ 680/1404]  eta: 0:07:11  lr: 0.000084  min_lr: 0.000001  loss: 4.6003 (4.6722)  class_acc: 0.1250 (0.1123)  loss_scale: 32768.0000 (53266.0441)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0006  max mem: 15572
Epoch: [4]  [ 690/1404]  eta: 0:07:04  lr: 0.000084  min_lr: 0.000001  loss: 4.6220 (4.6743)  class_acc: 0.1250 (0.1123)  loss_scale: 32768.0000 (52969.4009)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0006  max mem: 15572
Epoch: [4]  [ 700/1404]  eta: 0:06:58  lr: 0.000084  min_lr: 0.000001  loss: 4.6510 (4.6745)  class_acc: 0.0833 (0.1128)  loss_scale: 32768.0000 (52681.2211)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0007  max mem: 15572
Epoch: [4]  [ 710/1404]  eta: 0:06:52  lr: 0.000084  min_lr: 0.000001  loss: 4.7796 (4.6761)  class_acc: 0.0833 (0.1123)  loss_scale: 32768.0000 (52401.1477)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0008  max mem: 15572
Epoch: [4]  [ 720/1404]  eta: 0:06:46  lr: 0.000085  min_lr: 0.000001  loss: 4.7029 (4.6757)  class_acc: 0.0833 (0.1123)  loss_scale: 32768.0000 (52128.8433)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0007  max mem: 15572
Epoch: [4]  [ 730/1404]  eta: 0:06:39  lr: 0.000085  min_lr: 0.000001  loss: 4.6410 (4.6762)  class_acc: 0.0833 (0.1116)  loss_scale: 32768.0000 (51863.9891)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0006  max mem: 15572
Epoch: [4]  [ 740/1404]  eta: 0:06:33  lr: 0.000085  min_lr: 0.000001  loss: 4.6410 (4.6748)  class_acc: 0.0833 (0.1120)  loss_scale: 32768.0000 (51606.2834)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0007  max mem: 15572
Epoch: [4]  [ 750/1404]  eta: 0:06:26  lr: 0.000085  min_lr: 0.000001  loss: 4.6347 (4.6743)  class_acc: 0.1250 (0.1127)  loss_scale: 32768.0000 (51355.4407)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0007  max mem: 15572
Epoch: [4]  [ 760/1404]  eta: 0:06:21  lr: 0.000085  min_lr: 0.000001  loss: 4.6624 (4.6743)  class_acc: 0.1250 (0.1126)  loss_scale: 32768.0000 (51111.1905)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0007  max mem: 15572
[2025-01-16 21:20:36,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:20:36,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:20:36,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:20:36,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [ 770/1404]  eta: 0:06:16  lr: 0.000085  min_lr: 0.000001  loss: 4.6084 (4.6742)  class_acc: 0.0833 (0.1131)  loss_scale: 32768.0000 (51043.2789)  weight_decay: 0.0500 (0.0500)  time: 0.6694  data: 0.0007  max mem: 15572
Epoch: [4]  [ 780/1404]  eta: 0:06:09  lr: 0.000085  min_lr: 0.000001  loss: 4.7114 (4.6745)  class_acc: 0.0833 (0.1128)  loss_scale: 65536.0000 (51228.8451)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.0008  max mem: 15572
Epoch: [4]  [ 790/1404]  eta: 0:06:03  lr: 0.000086  min_lr: 0.000001  loss: 4.7114 (4.6743)  class_acc: 0.0833 (0.1129)  loss_scale: 65536.0000 (51409.7193)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0008  max mem: 15572
Epoch: [4]  [ 800/1404]  eta: 0:05:57  lr: 0.000086  min_lr: 0.000001  loss: 4.7397 (4.6755)  class_acc: 0.0833 (0.1132)  loss_scale: 65536.0000 (51586.0774)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0007  max mem: 15572
Epoch: [4]  [ 810/1404]  eta: 0:05:51  lr: 0.000086  min_lr: 0.000001  loss: 4.7036 (4.6750)  class_acc: 0.1250 (0.1133)  loss_scale: 65536.0000 (51758.0863)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0006  max mem: 15572
Epoch: [4]  [ 820/1404]  eta: 0:05:45  lr: 0.000086  min_lr: 0.000001  loss: 4.6179 (4.6750)  class_acc: 0.1250 (0.1131)  loss_scale: 65536.0000 (51925.9050)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.0005  max mem: 15572
Epoch: [4]  [ 830/1404]  eta: 0:05:39  lr: 0.000086  min_lr: 0.000001  loss: 4.6196 (4.6748)  class_acc: 0.1250 (0.1136)  loss_scale: 65536.0000 (52089.6847)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0005  max mem: 15572
Epoch: [4]  [ 840/1404]  eta: 0:05:32  lr: 0.000086  min_lr: 0.000001  loss: 4.6321 (4.6743)  class_acc: 0.1250 (0.1141)  loss_scale: 65536.0000 (52249.5696)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0009  max mem: 15572
Epoch: [4]  [ 850/1404]  eta: 0:05:26  lr: 0.000086  min_lr: 0.000001  loss: 4.5716 (4.6729)  class_acc: 0.1250 (0.1140)  loss_scale: 65536.0000 (52405.6968)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0009  max mem: 15572
[2025-01-16 21:21:27,474] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6473
[2025-01-16 21:21:27,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:21:27,507] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6473
[2025-01-16 21:21:27,508] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:21:27,508] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [ 860/1404]  eta: 0:05:20  lr: 0.000086  min_lr: 0.000001  loss: 4.4883 (4.6715)  class_acc: 0.1250 (0.1145)  loss_scale: 65536.0000 (52405.9652)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0006  max mem: 15572
Epoch: [4]  [ 870/1404]  eta: 0:05:14  lr: 0.000087  min_lr: 0.000001  loss: 4.5763 (4.6716)  class_acc: 0.1250 (0.1142)  loss_scale: 32768.0000 (52180.5006)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0006  max mem: 15572
Epoch: [4]  [ 880/1404]  eta: 0:05:08  lr: 0.000087  min_lr: 0.000001  loss: 4.5930 (4.6713)  class_acc: 0.1250 (0.1144)  loss_scale: 32768.0000 (51960.1544)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.0420  max mem: 15572
Epoch: [4]  [ 890/1404]  eta: 0:05:03  lr: 0.000087  min_lr: 0.000001  loss: 4.6736 (4.6716)  class_acc: 0.1667 (0.1149)  loss_scale: 32768.0000 (51744.7542)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1198  max mem: 15572
Epoch: [4]  [ 900/1404]  eta: 0:04:57  lr: 0.000087  min_lr: 0.000001  loss: 4.6736 (4.6717)  class_acc: 0.1250 (0.1150)  loss_scale: 32768.0000 (51534.1354)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.1515  max mem: 15572
Epoch: [4]  [ 910/1404]  eta: 0:04:51  lr: 0.000087  min_lr: 0.000001  loss: 4.6215 (4.6710)  class_acc: 0.1250 (0.1154)  loss_scale: 32768.0000 (51328.1405)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0736  max mem: 15572
Epoch: [4]  [ 920/1404]  eta: 0:04:45  lr: 0.000087  min_lr: 0.000001  loss: 4.6192 (4.6710)  class_acc: 0.1250 (0.1155)  loss_scale: 32768.0000 (51126.6189)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0005  max mem: 15572
Epoch: [4]  [ 930/1404]  eta: 0:04:39  lr: 0.000087  min_lr: 0.000001  loss: 4.6359 (4.6704)  class_acc: 0.1250 (0.1157)  loss_scale: 32768.0000 (50929.4264)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0007  max mem: 15572
Epoch: [4]  [ 940/1404]  eta: 0:04:33  lr: 0.000088  min_lr: 0.000001  loss: 4.5332 (4.6697)  class_acc: 0.1250 (0.1155)  loss_scale: 32768.0000 (50736.4251)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0008  max mem: 15572
Epoch: [4]  [ 950/1404]  eta: 0:04:27  lr: 0.000088  min_lr: 0.000001  loss: 4.6502 (4.6704)  class_acc: 0.1250 (0.1158)  loss_scale: 32768.0000 (50547.4826)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0008  max mem: 15572
Epoch: [4]  [ 960/1404]  eta: 0:04:21  lr: 0.000088  min_lr: 0.000001  loss: 4.6532 (4.6697)  class_acc: 0.1250 (0.1158)  loss_scale: 32768.0000 (50362.4724)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0006  max mem: 15572
Epoch: [4]  [ 970/1404]  eta: 0:04:15  lr: 0.000088  min_lr: 0.000001  loss: 4.5422 (4.6690)  class_acc: 0.0833 (0.1158)  loss_scale: 32768.0000 (50181.2729)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0006  max mem: 15572
Epoch: [4]  [ 980/1404]  eta: 0:04:10  lr: 0.000088  min_lr: 0.000001  loss: 4.6383 (4.6689)  class_acc: 0.1250 (0.1162)  loss_scale: 32768.0000 (50003.7676)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0009  max mem: 15572
[2025-01-16 21:22:42,788] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:22:42,789] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:22:42,790] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:22:42,790] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [ 990/1404]  eta: 0:04:03  lr: 0.000088  min_lr: 0.000001  loss: 4.6933 (4.6680)  class_acc: 0.1667 (0.1162)  loss_scale: 32768.0000 (49995.1726)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0008  max mem: 15572
[2025-01-16 21:22:51,023] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6616
[2025-01-16 21:22:51,023] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:22:51,023] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [1000/1404]  eta: 0:03:58  lr: 0.000088  min_lr: 0.000001  loss: 4.6944 (4.6679)  class_acc: 0.0833 (0.1163)  loss_scale: 65536.0000 (50117.6903)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0005  max mem: 15572
[2025-01-16 21:22:51,088] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6616
[2025-01-16 21:22:51,089] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [4]  [1010/1404]  eta: 0:03:52  lr: 0.000089  min_lr: 0.000001  loss: 4.6328 (4.6680)  class_acc: 0.0833 (0.1163)  loss_scale: 32768.0000 (49946.0811)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.0005  max mem: 15572
Epoch: [4]  [1020/1404]  eta: 0:03:46  lr: 0.000089  min_lr: 0.000001  loss: 4.6394 (4.6673)  class_acc: 0.1250 (0.1168)  loss_scale: 32768.0000 (49777.8335)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0006  max mem: 15572
Epoch: [4]  [1030/1404]  eta: 0:03:40  lr: 0.000089  min_lr: 0.000001  loss: 4.6437 (4.6676)  class_acc: 0.1250 (0.1166)  loss_scale: 32768.0000 (49612.8497)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0007  max mem: 15572
Epoch: [4]  [1040/1404]  eta: 0:03:34  lr: 0.000089  min_lr: 0.000001  loss: 4.6226 (4.6677)  class_acc: 0.0833 (0.1164)  loss_scale: 32768.0000 (49451.0355)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.0015  max mem: 15572
Epoch: [4]  [1050/1404]  eta: 0:03:28  lr: 0.000089  min_lr: 0.000001  loss: 4.6142 (4.6668)  class_acc: 0.0833 (0.1166)  loss_scale: 32768.0000 (49292.3007)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0015  max mem: 15572
Epoch: [4]  [1060/1404]  eta: 0:03:22  lr: 0.000089  min_lr: 0.000001  loss: 4.6487 (4.6666)  class_acc: 0.1667 (0.1174)  loss_scale: 32768.0000 (49136.5580)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0264  max mem: 15572
Epoch: [4]  [1070/1404]  eta: 0:03:16  lr: 0.000089  min_lr: 0.000001  loss: 4.6690 (4.6662)  class_acc: 0.1667 (0.1177)  loss_scale: 32768.0000 (48983.7236)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0374  max mem: 15572
Epoch: [4]  [1080/1404]  eta: 0:03:10  lr: 0.000089  min_lr: 0.000001  loss: 4.6690 (4.6663)  class_acc: 0.1250 (0.1180)  loss_scale: 32768.0000 (48833.7169)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0393  max mem: 15572
Epoch: [4]  [1090/1404]  eta: 0:03:04  lr: 0.000090  min_lr: 0.000001  loss: 4.7131 (4.6666)  class_acc: 0.0833 (0.1176)  loss_scale: 32768.0000 (48686.4601)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.0284  max mem: 15572
Epoch: [4]  [1100/1404]  eta: 0:02:59  lr: 0.000090  min_lr: 0.000001  loss: 4.7131 (4.6667)  class_acc: 0.0833 (0.1176)  loss_scale: 32768.0000 (48541.8783)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.0008  max mem: 15572
Epoch: [4]  [1110/1404]  eta: 0:02:53  lr: 0.000090  min_lr: 0.000001  loss: 4.7236 (4.6671)  class_acc: 0.0833 (0.1175)  loss_scale: 32768.0000 (48399.8992)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0007  max mem: 15572
Epoch: [4]  [1120/1404]  eta: 0:02:47  lr: 0.000090  min_lr: 0.000001  loss: 4.7067 (4.6664)  class_acc: 0.1250 (0.1176)  loss_scale: 32768.0000 (48260.4532)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0006  max mem: 15572
[2025-01-16 21:24:07,359] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:24:07,359] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:24:07,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:24:07,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [1130/1404]  eta: 0:02:41  lr: 0.000090  min_lr: 0.000001  loss: 4.6338 (4.6663)  class_acc: 0.1250 (0.1177)  loss_scale: 32768.0000 (48181.4182)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.0006  max mem: 15572
Epoch: [4]  [1140/1404]  eta: 0:02:35  lr: 0.000090  min_lr: 0.000001  loss: 4.6215 (4.6662)  class_acc: 0.1250 (0.1176)  loss_scale: 65536.0000 (48333.5180)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.0007  max mem: 15572
[2025-01-16 21:24:19,100] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6766
[2025-01-16 21:24:19,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:24:19,137] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6766
[2025-01-16 21:24:19,137] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:24:19,137] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [1150/1404]  eta: 0:02:29  lr: 0.000090  min_lr: 0.000001  loss: 4.5674 (4.6659)  class_acc: 0.0833 (0.1176)  loss_scale: 65536.0000 (48454.5056)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0008  max mem: 15572
Epoch: [4]  [1160/1404]  eta: 0:02:23  lr: 0.000091  min_lr: 0.000001  loss: 4.4476 (4.6637)  class_acc: 0.1667 (0.1181)  loss_scale: 32768.0000 (48319.3936)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0009  max mem: 15572
Epoch: [4]  [1170/1404]  eta: 0:02:17  lr: 0.000091  min_lr: 0.000001  loss: 4.4476 (4.6633)  class_acc: 0.1667 (0.1182)  loss_scale: 32768.0000 (48186.5892)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.0008  max mem: 15572
Epoch: [4]  [1180/1404]  eta: 0:02:11  lr: 0.000091  min_lr: 0.000001  loss: 4.6999 (4.6645)  class_acc: 0.0417 (0.1179)  loss_scale: 32768.0000 (48056.0339)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0008  max mem: 15572
Epoch: [4]  [1190/1404]  eta: 0:02:06  lr: 0.000091  min_lr: 0.000001  loss: 4.7055 (4.6635)  class_acc: 0.0833 (0.1178)  loss_scale: 32768.0000 (47927.6709)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0007  max mem: 15572
Epoch: [4]  [1200/1404]  eta: 0:02:00  lr: 0.000091  min_lr: 0.000001  loss: 4.5559 (4.6635)  class_acc: 0.0833 (0.1179)  loss_scale: 32768.0000 (47801.4455)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0006  max mem: 15572
Epoch: [4]  [1210/1404]  eta: 0:01:54  lr: 0.000091  min_lr: 0.000001  loss: 4.6678 (4.6633)  class_acc: 0.0833 (0.1180)  loss_scale: 32768.0000 (47677.3047)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0006  max mem: 15572
Epoch: [4]  [1220/1404]  eta: 0:01:48  lr: 0.000091  min_lr: 0.000001  loss: 4.6406 (4.6631)  class_acc: 0.1250 (0.1179)  loss_scale: 32768.0000 (47555.1974)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0006  max mem: 15572
Epoch: [4]  [1230/1404]  eta: 0:01:42  lr: 0.000091  min_lr: 0.000001  loss: 4.6098 (4.6623)  class_acc: 0.1250 (0.1181)  loss_scale: 32768.0000 (47435.0739)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0006  max mem: 15572
Epoch: [4]  [1240/1404]  eta: 0:01:36  lr: 0.000092  min_lr: 0.000001  loss: 4.6098 (4.6616)  class_acc: 0.1667 (0.1188)  loss_scale: 32768.0000 (47316.8864)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0007  max mem: 15572
Epoch: [4]  [1250/1404]  eta: 0:01:30  lr: 0.000092  min_lr: 0.000001  loss: 4.5887 (4.6613)  class_acc: 0.0833 (0.1186)  loss_scale: 32768.0000 (47200.5883)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0007  max mem: 15572
Epoch: [4]  [1260/1404]  eta: 0:01:24  lr: 0.000092  min_lr: 0.000001  loss: 4.5887 (4.6610)  class_acc: 0.0833 (0.1186)  loss_scale: 32768.0000 (47086.1348)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0006  max mem: 15572
Epoch: [4]  [1270/1404]  eta: 0:01:18  lr: 0.000092  min_lr: 0.000001  loss: 4.5982 (4.6610)  class_acc: 0.1250 (0.1187)  loss_scale: 32768.0000 (46973.4823)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.0006  max mem: 15572
[2025-01-16 21:25:34,053] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:25:34,053] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:25:34,099] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:25:34,100] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [1280/1404]  eta: 0:01:12  lr: 0.000092  min_lr: 0.000001  loss: 4.7357 (4.6616)  class_acc: 0.1667 (0.1191)  loss_scale: 32768.0000 (46913.7486)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.0006  max mem: 15572
Epoch: [4]  [1290/1404]  eta: 0:01:07  lr: 0.000092  min_lr: 0.000001  loss: 4.6399 (4.6605)  class_acc: 0.1667 (0.1192)  loss_scale: 65536.0000 (47057.9954)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0518  max mem: 15572
Epoch: [4]  [1300/1404]  eta: 0:01:01  lr: 0.000092  min_lr: 0.000001  loss: 4.5866 (4.6603)  class_acc: 0.0833 (0.1194)  loss_scale: 65536.0000 (47200.0246)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1174  max mem: 15572
Epoch: [4]  [1310/1404]  eta: 0:00:55  lr: 0.000093  min_lr: 0.000001  loss: 4.6347 (4.6601)  class_acc: 0.1250 (0.1196)  loss_scale: 65536.0000 (47339.8871)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1162  max mem: 15572
Epoch: [4]  [1320/1404]  eta: 0:00:49  lr: 0.000093  min_lr: 0.000001  loss: 4.6285 (4.6601)  class_acc: 0.1667 (0.1200)  loss_scale: 65536.0000 (47477.6321)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0727  max mem: 15572
Epoch: [4]  [1330/1404]  eta: 0:00:43  lr: 0.000093  min_lr: 0.000001  loss: 4.5876 (4.6595)  class_acc: 0.1667 (0.1202)  loss_scale: 65536.0000 (47613.3073)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0477  max mem: 15572
[2025-01-16 21:26:06,324] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6950
[2025-01-16 21:26:06,324] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:26:06,325] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6950
[2025-01-16 21:26:06,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:26:06,326] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [1340/1404]  eta: 0:00:37  lr: 0.000093  min_lr: 0.000001  loss: 4.5421 (4.6593)  class_acc: 0.1250 (0.1201)  loss_scale: 65536.0000 (47575.9105)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0348  max mem: 15572
Epoch: [4]  [1350/1404]  eta: 0:00:31  lr: 0.000093  min_lr: 0.000001  loss: 4.5814 (4.6583)  class_acc: 0.1250 (0.1203)  loss_scale: 32768.0000 (47466.3035)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0215  max mem: 15572
Epoch: [4]  [1360/1404]  eta: 0:00:25  lr: 0.000093  min_lr: 0.000001  loss: 4.5923 (4.6579)  class_acc: 0.1250 (0.1204)  loss_scale: 32768.0000 (47358.3071)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0218  max mem: 15572
Epoch: [4]  [1370/1404]  eta: 0:00:19  lr: 0.000093  min_lr: 0.000001  loss: 4.5654 (4.6572)  class_acc: 0.1250 (0.1209)  loss_scale: 32768.0000 (47251.8862)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.0099  max mem: 15572
Epoch: [4]  [1380/1404]  eta: 0:00:14  lr: 0.000093  min_lr: 0.000001  loss: 4.5876 (4.6579)  class_acc: 0.1667 (0.1212)  loss_scale: 32768.0000 (47147.0065)  weight_decay: 0.0500 (0.0500)  time: 0.6643  data: 0.0006  max mem: 15572
[2025-01-16 21:26:36,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=36, lr=[9.057462563736721e-07, 9.057462563736721e-07, 1.2939232233909602e-06, 1.2939232233909602e-06, 1.848461747701372e-06, 1.848461747701372e-06, 2.6406596395733886e-06, 2.6406596395733886e-06, 3.7723709136762698e-06, 3.7723709136762698e-06, 5.389101305251814e-06, 5.389101305251814e-06, 7.698716150359736e-06, 7.698716150359736e-06, 1.0998165929085337e-05, 1.0998165929085337e-05, 1.5711665612979053e-05, 1.5711665612979053e-05, 2.2445236589970078e-05, 2.2445236589970078e-05, 3.206462369995725e-05, 3.206462369995725e-05, 4.5806605285653225e-05, 4.5806605285653225e-05, 6.543800755093318e-05, 6.543800755093318e-05, 9.348286792990455e-05, 9.348286792990455e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 21:26:36,082] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=47.21841762532976, CurrSamplesPerSec=58.699290570051396, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [1390/1404]  eta: 0:00:08  lr: 0.000094  min_lr: 0.000001  loss: 4.5876 (4.6571)  class_acc: 0.1667 (0.1217)  loss_scale: 32768.0000 (47043.6348)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0006  max mem: 15572
[2025-01-16 21:26:41,380] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7011
[2025-01-16 21:26:41,380] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7011
[2025-01-16 21:26:41,380] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 21:26:41,380] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 21:26:41,380] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [4]  [1400/1404]  eta: 0:00:02  lr: 0.000094  min_lr: 0.000001  loss: 4.6414 (4.6572)  class_acc: 0.1667 (0.1218)  loss_scale: 32768.0000 (46871.5717)  weight_decay: 0.0500 (0.0500)  time: 0.4541  data: 0.0005  max mem: 15572
Epoch: [4]  [1403/1404]  eta: 0:00:00  lr: 0.000094  min_lr: 0.000001  loss: 4.6414 (4.6570)  class_acc: 0.1250 (0.1219)  loss_scale: 32768.0000 (46806.4274)  weight_decay: 0.0500 (0.0500)  time: 0.4145  data: 0.0004  max mem: 15572
Epoch: [4] Total time: 0:13:43 (0.5868 s / it)
Averaged stats: lr: 0.000094  min_lr: 0.000001  loss: 4.6414 (4.6560)  class_acc: 0.1250 (0.1215)  loss_scale: 32768.0000 (46806.4274)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:20  loss: 2.8528 (2.8528)  acc1: 50.0000 (50.0000)  acc5: 66.6667 (66.6667)  time: 4.1224  data: 3.9369  max mem: 15572
Val:  [ 10/136]  eta: 0:01:31  loss: 3.7668 (3.8438)  acc1: 5.5556 (13.1313)  acc5: 38.8889 (37.8788)  time: 0.7272  data: 0.5415  max mem: 15572
Val:  [ 20/136]  eta: 0:01:04  loss: 3.7668 (3.8607)  acc1: 5.5556 (12.4339)  acc5: 38.8889 (39.6825)  time: 0.3751  data: 0.1872  max mem: 15572
Val:  [ 30/136]  eta: 0:00:48  loss: 3.5186 (3.6664)  acc1: 11.1111 (22.7599)  acc5: 55.5556 (46.7742)  time: 0.3161  data: 0.1248  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 3.1829 (3.6316)  acc1: 16.6667 (22.0867)  acc5: 66.6667 (47.4255)  time: 0.3300  data: 0.1336  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 3.9667 (3.7495)  acc1: 5.5556 (18.9542)  acc5: 33.3333 (43.5730)  time: 0.3540  data: 0.1598  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 4.1286 (3.8296)  acc1: 0.0000 (16.6667)  acc5: 27.7778 (40.8925)  time: 0.3481  data: 0.1547  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 4.0409 (3.7818)  acc1: 5.5556 (19.3271)  acc5: 38.8889 (42.7230)  time: 0.3940  data: 0.1935  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 3.6085 (3.7701)  acc1: 22.2222 (19.4102)  acc5: 50.0000 (43.6214)  time: 0.4026  data: 0.1985  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 3.6621 (3.7765)  acc1: 11.1111 (19.2308)  acc5: 44.4444 (43.5287)  time: 0.3725  data: 0.1682  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.8400 (3.8019)  acc1: 5.5556 (18.7569)  acc5: 33.3333 (43.0693)  time: 0.3643  data: 0.1653  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.8761 (3.8121)  acc1: 16.6667 (19.4695)  acc5: 38.8889 (43.4434)  time: 0.3466  data: 0.1310  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 3.6447 (3.7808)  acc1: 33.3333 (21.1203)  acc5: 55.5556 (46.0055)  time: 0.3171  data: 0.1020  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 3.4417 (3.7665)  acc1: 33.3333 (22.6463)  acc5: 66.6667 (46.6921)  time: 0.2502  data: 0.0682  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 3.5260 (3.7742)  acc1: 27.7778 (22.5225)  acc5: 55.5556 (46.8059)  time: 0.2252  data: 0.0566  max mem: 15572
Val: Total time: 0:00:49 (0.3638 s / it)
* Acc@1 22.031 Acc@5 46.335 loss 3.797
Accuracy of the network on the 4883 val videos: 22.0%
[2025-01-16 21:27:34,734] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 21:27:34,736] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 21:27:34,736] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 21:27:34,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 21:27:37,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 21:27:37,198] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 22.03%
Epoch: [5]  [   0/1404]  eta: 3:08:54  lr: 0.000094  min_lr: 0.000001  loss: 4.8748 (4.8748)  class_acc: 0.1667 (0.1667)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 8.0729  data: 7.5836  max mem: 15572
Epoch: [5]  [  10/1404]  eta: 0:30:33  lr: 0.000094  min_lr: 0.000001  loss: 4.6307 (4.6301)  class_acc: 0.0833 (0.1250)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3154  data: 0.8212  max mem: 15572
Epoch: [5]  [  20/1404]  eta: 0:21:58  lr: 0.000094  min_lr: 0.000001  loss: 4.5535 (4.6109)  class_acc: 0.1250 (0.1567)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1033  max mem: 15572
Epoch: [5]  [  30/1404]  eta: 0:18:30  lr: 0.000094  min_lr: 0.000001  loss: 4.5535 (4.5657)  class_acc: 0.1250 (0.1505)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0385  max mem: 15572
Epoch: [5]  [  40/1404]  eta: 0:17:37  lr: 0.000094  min_lr: 0.000001  loss: 4.6044 (4.5935)  class_acc: 0.1250 (0.1423)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1132  max mem: 15572
Epoch: [5]  [  50/1404]  eta: 0:16:30  lr: 0.000094  min_lr: 0.000001  loss: 4.6632 (4.5987)  class_acc: 0.1250 (0.1356)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1270  max mem: 15572
Epoch: [5]  [  60/1404]  eta: 0:15:31  lr: 0.000094  min_lr: 0.000001  loss: 4.6082 (4.5967)  class_acc: 0.1250 (0.1393)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0218  max mem: 15572
Epoch: [5]  [  70/1404]  eta: 0:14:49  lr: 0.000094  min_lr: 0.000001  loss: 4.6226 (4.6121)  class_acc: 0.1250 (0.1397)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0122  max mem: 15572
Epoch: [5]  [  80/1404]  eta: 0:14:34  lr: 0.000094  min_lr: 0.000001  loss: 4.6403 (4.6119)  class_acc: 0.1250 (0.1415)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0737  max mem: 15572
Epoch: [5]  [  90/1404]  eta: 0:14:15  lr: 0.000094  min_lr: 0.000001  loss: 4.4611 (4.6017)  class_acc: 0.1250 (0.1378)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0936  max mem: 15572
Epoch: [5]  [ 100/1404]  eta: 0:14:04  lr: 0.000094  min_lr: 0.000001  loss: 4.5071 (4.6104)  class_acc: 0.1250 (0.1378)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0972  max mem: 15572
Epoch: [5]  [ 110/1404]  eta: 0:13:46  lr: 0.000094  min_lr: 0.000001  loss: 4.6619 (4.6107)  class_acc: 0.1250 (0.1393)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0762  max mem: 15572
[2025-01-16 21:28:53,717] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:28:53,718] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 21:28:53,723] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:28:53,724] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [5]  [ 120/1404]  eta: 0:13:31  lr: 0.000094  min_lr: 0.000001  loss: 4.6812 (4.6169)  class_acc: 0.1250 (0.1364)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0530  max mem: 15572
Epoch: [5]  [ 130/1404]  eta: 0:13:17  lr: 0.000094  min_lr: 0.000001  loss: 4.6155 (4.6152)  class_acc: 0.1250 (0.1361)  loss_scale: 32768.0000 (17759.7557)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0647  max mem: 15572
Epoch: [5]  [ 140/1404]  eta: 0:13:10  lr: 0.000094  min_lr: 0.000001  loss: 4.6161 (4.6197)  class_acc: 0.0833 (0.1318)  loss_scale: 32768.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.0860  max mem: 15572
Epoch: [5]  [ 150/1404]  eta: 0:13:04  lr: 0.000094  min_lr: 0.000001  loss: 4.6742 (4.6289)  class_acc: 0.0833 (0.1313)  loss_scale: 32768.0000 (19747.6026)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.1254  max mem: 15572
Epoch: [5]  [ 160/1404]  eta: 0:12:55  lr: 0.000094  min_lr: 0.000001  loss: 4.5887 (4.6167)  class_acc: 0.1250 (0.1333)  loss_scale: 32768.0000 (20556.3230)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1142  max mem: 15572
Epoch: [5]  [ 170/1404]  eta: 0:12:54  lr: 0.000094  min_lr: 0.000001  loss: 4.5887 (4.6187)  class_acc: 0.1667 (0.1357)  loss_scale: 32768.0000 (21270.4561)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.1236  max mem: 15572
Epoch: [5]  [ 180/1404]  eta: 0:12:38  lr: 0.000094  min_lr: 0.000001  loss: 4.6527 (4.6209)  class_acc: 0.1250 (0.1354)  loss_scale: 32768.0000 (21905.6796)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0717  max mem: 15572
Epoch: [5]  [ 190/1404]  eta: 0:12:34  lr: 0.000094  min_lr: 0.000001  loss: 4.5376 (4.6123)  class_acc: 0.1250 (0.1350)  loss_scale: 32768.0000 (22474.3874)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0611  max mem: 15572
Epoch: [5]  [ 200/1404]  eta: 0:12:30  lr: 0.000094  min_lr: 0.000001  loss: 4.5117 (4.6078)  class_acc: 0.1250 (0.1354)  loss_scale: 32768.0000 (22986.5075)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.1590  max mem: 15572
Epoch: [5]  [ 210/1404]  eta: 0:12:20  lr: 0.000094  min_lr: 0.000001  loss: 4.5756 (4.6103)  class_acc: 0.1250 (0.1365)  loss_scale: 32768.0000 (23450.0853)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1250  max mem: 15572
Epoch: [5]  [ 220/1404]  eta: 0:12:11  lr: 0.000094  min_lr: 0.000001  loss: 4.6177 (4.6081)  class_acc: 0.0833 (0.1359)  loss_scale: 32768.0000 (23871.7104)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0716  max mem: 15572
Epoch: [5]  [ 230/1404]  eta: 0:12:03  lr: 0.000094  min_lr: 0.000001  loss: 4.6177 (4.6090)  class_acc: 0.0833 (0.1351)  loss_scale: 32768.0000 (24256.8312)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0979  max mem: 15572
Epoch: [5]  [ 240/1404]  eta: 0:11:55  lr: 0.000094  min_lr: 0.000001  loss: 4.5438 (4.6080)  class_acc: 0.1250 (0.1357)  loss_scale: 32768.0000 (24609.9917)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0886  max mem: 15572
[2025-01-16 21:30:10,163] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:30:10,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:30:10,233] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:30:10,233] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:30:11,269] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7270
[2025-01-16 21:30:11,269] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:30:11,270] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:30:11,271] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7270
[2025-01-16 21:30:11,271] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [5]  [ 250/1404]  eta: 0:11:47  lr: 0.000094  min_lr: 0.000001  loss: 4.5256 (4.6054)  class_acc: 0.1250 (0.1356)  loss_scale: 32768.0000 (25196.1116)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0843  max mem: 15572
Epoch: [5]  [ 260/1404]  eta: 0:11:40  lr: 0.000094  min_lr: 0.000001  loss: 4.4637 (4.6037)  class_acc: 0.1250 (0.1370)  loss_scale: 32768.0000 (25486.2222)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0835  max mem: 15572
Epoch: [5]  [ 270/1404]  eta: 0:11:30  lr: 0.000094  min_lr: 0.000001  loss: 4.5152 (4.6011)  class_acc: 0.1667 (0.1378)  loss_scale: 32768.0000 (25754.9225)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0581  max mem: 15572
Epoch: [5]  [ 280/1404]  eta: 0:11:22  lr: 0.000094  min_lr: 0.000001  loss: 4.5567 (4.6020)  class_acc: 0.1667 (0.1376)  loss_scale: 32768.0000 (26004.4982)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0565  max mem: 15572
Epoch: [5]  [ 290/1404]  eta: 0:11:13  lr: 0.000094  min_lr: 0.000001  loss: 4.5950 (4.6028)  class_acc: 0.1667 (0.1387)  loss_scale: 32768.0000 (26236.9210)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0525  max mem: 15572
Epoch: [5]  [ 300/1404]  eta: 0:11:06  lr: 0.000094  min_lr: 0.000001  loss: 4.5250 (4.6000)  class_acc: 0.1667 (0.1390)  loss_scale: 32768.0000 (26453.9003)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0641  max mem: 15572
Epoch: [5]  [ 310/1404]  eta: 0:11:00  lr: 0.000094  min_lr: 0.000001  loss: 4.4390 (4.5957)  class_acc: 0.0833 (0.1373)  loss_scale: 32768.0000 (26656.9260)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1218  max mem: 15572
Epoch: [5]  [ 320/1404]  eta: 0:10:55  lr: 0.000094  min_lr: 0.000001  loss: 4.4638 (4.5927)  class_acc: 0.0833 (0.1375)  loss_scale: 32768.0000 (26847.3022)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1434  max mem: 15572
Epoch: [5]  [ 330/1404]  eta: 0:10:48  lr: 0.000094  min_lr: 0.000001  loss: 4.5929 (4.5965)  class_acc: 0.1250 (0.1377)  loss_scale: 32768.0000 (27026.1752)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1196  max mem: 15572
Epoch: [5]  [ 340/1404]  eta: 0:10:44  lr: 0.000094  min_lr: 0.000001  loss: 4.5929 (4.5934)  class_acc: 0.0833 (0.1365)  loss_scale: 32768.0000 (27194.5572)  weight_decay: 0.0500 (0.0500)  time: 0.6240  data: 0.1347  max mem: 15572
Epoch: [5]  [ 350/1404]  eta: 0:10:39  lr: 0.000094  min_lr: 0.000001  loss: 4.5792 (4.5954)  class_acc: 0.0833 (0.1366)  loss_scale: 32768.0000 (27353.3447)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.1544  max mem: 15572
Epoch: [5]  [ 360/1404]  eta: 0:10:30  lr: 0.000094  min_lr: 0.000001  loss: 4.5115 (4.5903)  class_acc: 0.1667 (0.1376)  loss_scale: 32768.0000 (27503.3352)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0817  max mem: 15572
Epoch: [5]  [ 370/1404]  eta: 0:10:24  lr: 0.000094  min_lr: 0.000001  loss: 4.5115 (4.5908)  class_acc: 0.1250 (0.1368)  loss_scale: 32768.0000 (27645.2399)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0509  max mem: 15572
[2025-01-16 21:31:25,977] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:31:25,977] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:31:25,989] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:31:25,989] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 380/1404]  eta: 0:10:15  lr: 0.000094  min_lr: 0.000001  loss: 4.6281 (4.5929)  class_acc: 0.1250 (0.1371)  loss_scale: 32768.0000 (27951.7060)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.0521  max mem: 15572
Epoch: [5]  [ 390/1404]  eta: 0:10:07  lr: 0.000094  min_lr: 0.000001  loss: 4.6012 (4.5920)  class_acc: 0.1667 (0.1379)  loss_scale: 65536.0000 (28912.9412)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0300  max mem: 15572
Epoch: [5]  [ 400/1404]  eta: 0:10:00  lr: 0.000094  min_lr: 0.000001  loss: 4.5887 (4.5941)  class_acc: 0.1667 (0.1385)  loss_scale: 65536.0000 (29826.2344)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0647  max mem: 15572
Epoch: [5]  [ 410/1404]  eta: 0:09:53  lr: 0.000094  min_lr: 0.000001  loss: 4.5638 (4.5926)  class_acc: 0.0833 (0.1380)  loss_scale: 65536.0000 (30695.0852)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0542  max mem: 15572
Epoch: [5]  [ 420/1404]  eta: 0:09:48  lr: 0.000094  min_lr: 0.000001  loss: 4.5638 (4.5920)  class_acc: 0.1250 (0.1386)  loss_scale: 65536.0000 (31522.6603)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0763  max mem: 15572
[2025-01-16 21:31:51,806] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7446
[2025-01-16 21:31:51,806] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:31:51,806] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7446
[2025-01-16 21:31:51,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:31:51,807] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [ 430/1404]  eta: 0:09:39  lr: 0.000094  min_lr: 0.000001  loss: 4.5991 (4.5905)  class_acc: 0.1250 (0.1384)  loss_scale: 65536.0000 (31931.6937)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.0819  max mem: 15572
Epoch: [5]  [ 440/1404]  eta: 0:09:34  lr: 0.000094  min_lr: 0.000001  loss: 4.5413 (4.5905)  class_acc: 0.1250 (0.1390)  loss_scale: 32768.0000 (31950.6576)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0543  max mem: 15572
Epoch: [5]  [ 450/1404]  eta: 0:09:28  lr: 0.000094  min_lr: 0.000001  loss: 4.5413 (4.5907)  class_acc: 0.0833 (0.1386)  loss_scale: 32768.0000 (31968.7805)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0846  max mem: 15572
Epoch: [5]  [ 460/1404]  eta: 0:09:24  lr: 0.000094  min_lr: 0.000001  loss: 4.6159 (4.5929)  class_acc: 0.1250 (0.1380)  loss_scale: 32768.0000 (31986.1171)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1328  max mem: 15572
Epoch: [5]  [ 470/1404]  eta: 0:09:17  lr: 0.000094  min_lr: 0.000001  loss: 4.6589 (4.5950)  class_acc: 0.1250 (0.1380)  loss_scale: 32768.0000 (32002.7176)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1220  max mem: 15572
Epoch: [5]  [ 480/1404]  eta: 0:09:12  lr: 0.000094  min_lr: 0.000001  loss: 4.5416 (4.5919)  class_acc: 0.1250 (0.1378)  loss_scale: 32768.0000 (32018.6279)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1168  max mem: 15572
Epoch: [5]  [ 490/1404]  eta: 0:09:06  lr: 0.000094  min_lr: 0.000001  loss: 4.5402 (4.5906)  class_acc: 0.1250 (0.1387)  loss_scale: 32768.0000 (32033.8900)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1111  max mem: 15572
Epoch: [5]  [ 500/1404]  eta: 0:08:59  lr: 0.000094  min_lr: 0.000001  loss: 4.6167 (4.5919)  class_acc: 0.1667 (0.1391)  loss_scale: 32768.0000 (32048.5429)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0531  max mem: 15572
Epoch: [5]  [ 510/1404]  eta: 0:08:54  lr: 0.000094  min_lr: 0.000001  loss: 4.6167 (4.5939)  class_acc: 0.1250 (0.1388)  loss_scale: 32768.0000 (32062.6223)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0990  max mem: 15572
Epoch: [5]  [ 520/1404]  eta: 0:08:47  lr: 0.000094  min_lr: 0.000001  loss: 4.6392 (4.5938)  class_acc: 0.1250 (0.1394)  loss_scale: 32768.0000 (32076.1612)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0822  max mem: 15572
Epoch: [5]  [ 530/1404]  eta: 0:08:40  lr: 0.000094  min_lr: 0.000001  loss: 4.6138 (4.5939)  class_acc: 0.1250 (0.1390)  loss_scale: 32768.0000 (32089.1902)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0428  max mem: 15572
Epoch: [5]  [ 540/1404]  eta: 0:08:34  lr: 0.000094  min_lr: 0.000001  loss: 4.5501 (4.5921)  class_acc: 0.1250 (0.1394)  loss_scale: 32768.0000 (32101.7375)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0849  max mem: 15572
Epoch: [5]  [ 550/1404]  eta: 0:08:29  lr: 0.000094  min_lr: 0.000001  loss: 4.5421 (4.5920)  class_acc: 0.1667 (0.1396)  loss_scale: 32768.0000 (32113.8294)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1127  max mem: 15572
[2025-01-16 21:33:09,467] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:33:09,468] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:33:09,502] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:33:09,503] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:33:11,081] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7578
[2025-01-16 21:33:11,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:33:11,150] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7578
[2025-01-16 21:33:11,151] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:33:11,151] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [ 560/1404]  eta: 0:08:23  lr: 0.000094  min_lr: 0.000001  loss: 4.6291 (4.5929)  class_acc: 0.1250 (0.1400)  loss_scale: 32768.0000 (32300.7201)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1218  max mem: 15572
Epoch: [5]  [ 570/1404]  eta: 0:08:17  lr: 0.000094  min_lr: 0.000001  loss: 4.7292 (4.5943)  class_acc: 0.1667 (0.1411)  loss_scale: 32768.0000 (32308.9037)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1170  max mem: 15572
Epoch: [5]  [ 580/1404]  eta: 0:08:11  lr: 0.000094  min_lr: 0.000001  loss: 4.5507 (4.5918)  class_acc: 0.2083 (0.1414)  loss_scale: 32768.0000 (32316.8055)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.0980  max mem: 15572
Epoch: [5]  [ 590/1404]  eta: 0:08:06  lr: 0.000094  min_lr: 0.000001  loss: 4.5498 (4.5925)  class_acc: 0.1250 (0.1419)  loss_scale: 32768.0000 (32324.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0998  max mem: 15572
Epoch: [5]  [ 600/1404]  eta: 0:07:59  lr: 0.000094  min_lr: 0.000001  loss: 4.6999 (4.5921)  class_acc: 0.1250 (0.1419)  loss_scale: 32768.0000 (32331.8203)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0679  max mem: 15572
Epoch: [5]  [ 610/1404]  eta: 0:07:53  lr: 0.000094  min_lr: 0.000001  loss: 4.5119 (4.5906)  class_acc: 0.2083 (0.1432)  loss_scale: 32768.0000 (32338.9591)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0569  max mem: 15572
Epoch: [5]  [ 620/1404]  eta: 0:07:48  lr: 0.000094  min_lr: 0.000001  loss: 4.5337 (4.5897)  class_acc: 0.1667 (0.1437)  loss_scale: 32768.0000 (32345.8680)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1190  max mem: 15572
Epoch: [5]  [ 630/1404]  eta: 0:07:42  lr: 0.000094  min_lr: 0.000001  loss: 4.6084 (4.5907)  class_acc: 0.1667 (0.1436)  loss_scale: 32768.0000 (32352.5578)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1233  max mem: 15572
Epoch: [5]  [ 640/1404]  eta: 0:07:35  lr: 0.000094  min_lr: 0.000001  loss: 4.6527 (4.5908)  class_acc: 0.1250 (0.1440)  loss_scale: 32768.0000 (32359.0390)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1006  max mem: 15572
Epoch: [5]  [ 650/1404]  eta: 0:07:29  lr: 0.000094  min_lr: 0.000001  loss: 4.6176 (4.5919)  class_acc: 0.1250 (0.1434)  loss_scale: 32768.0000 (32365.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0650  max mem: 15572
Epoch: [5]  [ 660/1404]  eta: 0:07:24  lr: 0.000094  min_lr: 0.000001  loss: 4.5653 (4.5896)  class_acc: 0.0833 (0.1433)  loss_scale: 32768.0000 (32371.4130)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1404  max mem: 15572
Epoch: [5]  [ 670/1404]  eta: 0:07:16  lr: 0.000094  min_lr: 0.000001  loss: 4.5653 (4.5898)  class_acc: 0.0833 (0.1433)  loss_scale: 32768.0000 (32377.3234)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1292  max mem: 15572
Epoch: [5]  [ 680/1404]  eta: 0:07:09  lr: 0.000094  min_lr: 0.000001  loss: 4.5781 (4.5895)  class_acc: 0.1250 (0.1430)  loss_scale: 32768.0000 (32383.0602)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0072  max mem: 15572
[2025-01-16 21:34:25,999] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:34:25,999] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:34:26,051] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:34:26,051] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 690/1404]  eta: 0:07:03  lr: 0.000094  min_lr: 0.000001  loss: 4.5590 (4.5891)  class_acc: 0.1250 (0.1424)  loss_scale: 32768.0000 (32578.3155)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0485  max mem: 15572
Epoch: [5]  [ 700/1404]  eta: 0:06:57  lr: 0.000094  min_lr: 0.000001  loss: 4.4385 (4.5857)  class_acc: 0.1250 (0.1432)  loss_scale: 65536.0000 (33048.4679)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.0981  max mem: 15572
Epoch: [5]  [ 710/1404]  eta: 0:06:52  lr: 0.000094  min_lr: 0.000001  loss: 4.4493 (4.5877)  class_acc: 0.2083 (0.1436)  loss_scale: 65536.0000 (33505.3952)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1057  max mem: 15572
Epoch: [5]  [ 720/1404]  eta: 0:06:46  lr: 0.000094  min_lr: 0.000001  loss: 4.6511 (4.5877)  class_acc: 0.2083 (0.1447)  loss_scale: 65536.0000 (33949.6477)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.0931  max mem: 15572
Epoch: [5]  [ 730/1404]  eta: 0:06:39  lr: 0.000094  min_lr: 0.000001  loss: 4.5302 (4.5875)  class_acc: 0.1667 (0.1449)  loss_scale: 65536.0000 (34381.7456)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0548  max mem: 15572
Epoch: [5]  [ 740/1404]  eta: 0:06:33  lr: 0.000094  min_lr: 0.000001  loss: 4.5940 (4.5880)  class_acc: 0.1250 (0.1445)  loss_scale: 65536.0000 (34802.1808)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0400  max mem: 15572
Epoch: [5]  [ 750/1404]  eta: 0:06:27  lr: 0.000094  min_lr: 0.000001  loss: 4.6559 (4.5885)  class_acc: 0.1250 (0.1444)  loss_scale: 65536.0000 (35211.4194)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0430  max mem: 15572
[2025-01-16 21:35:03,394] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7771
[2025-01-16 21:35:03,394] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:35:03,394] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:35:03,399] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7771
[2025-01-16 21:35:03,400] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [5]  [ 760/1404]  eta: 0:06:21  lr: 0.000094  min_lr: 0.000001  loss: 4.6231 (4.5881)  class_acc: 0.0833 (0.1444)  loss_scale: 32768.0000 (35179.3114)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0273  max mem: 15572
Epoch: [5]  [ 770/1404]  eta: 0:06:16  lr: 0.000094  min_lr: 0.000001  loss: 4.4692 (4.5878)  class_acc: 0.1250 (0.1445)  loss_scale: 32768.0000 (35148.0363)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0412  max mem: 15572
Epoch: [5]  [ 780/1404]  eta: 0:06:09  lr: 0.000094  min_lr: 0.000001  loss: 4.4657 (4.5858)  class_acc: 0.1250 (0.1448)  loss_scale: 32768.0000 (35117.5621)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0347  max mem: 15572
Epoch: [5]  [ 790/1404]  eta: 0:06:03  lr: 0.000094  min_lr: 0.000001  loss: 4.5562 (4.5854)  class_acc: 0.1250 (0.1453)  loss_scale: 32768.0000 (35087.8584)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.0010  max mem: 15572
Epoch: [5]  [ 800/1404]  eta: 0:05:58  lr: 0.000094  min_lr: 0.000001  loss: 4.5504 (4.5842)  class_acc: 0.1667 (0.1457)  loss_scale: 32768.0000 (35058.8964)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.0009  max mem: 15572
Epoch: [5]  [ 810/1404]  eta: 0:05:52  lr: 0.000094  min_lr: 0.000001  loss: 4.5437 (4.5839)  class_acc: 0.1250 (0.1454)  loss_scale: 32768.0000 (35030.6486)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.0006  max mem: 15572
Epoch: [5]  [ 820/1404]  eta: 0:05:46  lr: 0.000094  min_lr: 0.000001  loss: 4.5589 (4.5834)  class_acc: 0.1250 (0.1455)  loss_scale: 32768.0000 (35003.0889)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0006  max mem: 15572
Epoch: [5]  [ 830/1404]  eta: 0:05:40  lr: 0.000094  min_lr: 0.000001  loss: 4.5017 (4.5828)  class_acc: 0.1250 (0.1456)  loss_scale: 32768.0000 (34976.1925)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0006  max mem: 15572
Epoch: [5]  [ 840/1404]  eta: 0:05:34  lr: 0.000094  min_lr: 0.000001  loss: 4.4189 (4.5819)  class_acc: 0.1667 (0.1460)  loss_scale: 32768.0000 (34949.9358)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.0005  max mem: 15572
Epoch: [5]  [ 850/1404]  eta: 0:05:28  lr: 0.000094  min_lr: 0.000001  loss: 4.4881 (4.5828)  class_acc: 0.1667 (0.1461)  loss_scale: 32768.0000 (34924.2961)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0006  max mem: 15572
Epoch: [5]  [ 860/1404]  eta: 0:05:22  lr: 0.000094  min_lr: 0.000001  loss: 4.4881 (4.5819)  class_acc: 0.1667 (0.1463)  loss_scale: 32768.0000 (34899.2520)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0007  max mem: 15572
Epoch: [5]  [ 870/1404]  eta: 0:05:16  lr: 0.000094  min_lr: 0.000001  loss: 4.4805 (4.5823)  class_acc: 0.1667 (0.1461)  loss_scale: 32768.0000 (34874.7830)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0007  max mem: 15572
[2025-01-16 21:36:18,542] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:36:18,543] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:36:18,574] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:36:18,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 880/1404]  eta: 0:05:09  lr: 0.000094  min_lr: 0.000001  loss: 4.5967 (4.5828)  class_acc: 0.1250 (0.1460)  loss_scale: 32768.0000 (34888.0636)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.0007  max mem: 15572
Epoch: [5]  [ 890/1404]  eta: 0:05:03  lr: 0.000094  min_lr: 0.000001  loss: 4.6649 (4.5838)  class_acc: 0.0833 (0.1456)  loss_scale: 65536.0000 (35232.0359)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0187  max mem: 15572
[2025-01-16 21:36:29,527] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7919
[2025-01-16 21:36:29,527] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:36:29,537] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7919
[2025-01-16 21:36:29,537] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:36:29,537] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [ 900/1404]  eta: 0:04:57  lr: 0.000094  min_lr: 0.000001  loss: 4.5979 (4.5829)  class_acc: 0.1250 (0.1456)  loss_scale: 65536.0000 (35495.6360)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0630  max mem: 15572
Epoch: [5]  [ 910/1404]  eta: 0:04:51  lr: 0.000094  min_lr: 0.000001  loss: 4.5510 (4.5821)  class_acc: 0.1250 (0.1457)  loss_scale: 32768.0000 (35465.6948)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0782  max mem: 15572
Epoch: [5]  [ 920/1404]  eta: 0:04:45  lr: 0.000094  min_lr: 0.000001  loss: 4.5687 (4.5829)  class_acc: 0.1250 (0.1454)  loss_scale: 32768.0000 (35436.4039)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.0614  max mem: 15572
Epoch: [5]  [ 930/1404]  eta: 0:04:40  lr: 0.000094  min_lr: 0.000001  loss: 4.5730 (4.5828)  class_acc: 0.1250 (0.1459)  loss_scale: 32768.0000 (35407.7422)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.0973  max mem: 15572
Epoch: [5]  [ 940/1404]  eta: 0:04:34  lr: 0.000094  min_lr: 0.000001  loss: 4.5422 (4.5821)  class_acc: 0.1667 (0.1461)  loss_scale: 32768.0000 (35379.6897)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1417  max mem: 15572
Epoch: [5]  [ 950/1404]  eta: 0:04:28  lr: 0.000094  min_lr: 0.000001  loss: 4.5582 (4.5820)  class_acc: 0.1667 (0.1462)  loss_scale: 32768.0000 (35352.2271)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.1293  max mem: 15572
Epoch: [5]  [ 960/1404]  eta: 0:04:22  lr: 0.000094  min_lr: 0.000001  loss: 4.6270 (4.5824)  class_acc: 0.1250 (0.1462)  loss_scale: 32768.0000 (35325.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0739  max mem: 15572
Epoch: [5]  [ 970/1404]  eta: 0:04:16  lr: 0.000094  min_lr: 0.000001  loss: 4.6266 (4.5830)  class_acc: 0.1250 (0.1461)  loss_scale: 32768.0000 (35298.9990)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0537  max mem: 15572
[2025-01-16 21:37:17,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=42, lr=[9.074460811791398e-07, 9.074460811791398e-07, 1.2963515445416284e-06, 1.2963515445416284e-06, 1.8519307779166122e-06, 1.8519307779166122e-06, 2.6456153970237317e-06, 2.6456153970237317e-06, 3.77945056717676e-06, 3.77945056717676e-06, 5.3992150959668e-06, 5.3992150959668e-06, 7.713164422809715e-06, 7.713164422809715e-06, 1.1018806318299594e-05, 1.1018806318299594e-05, 1.5741151883285134e-05, 1.5741151883285134e-05, 2.248735983326448e-05, 2.248735983326448e-05, 3.21247997618064e-05, 3.21247997618064e-05, 4.589257108829486e-05, 4.589257108829486e-05, 6.556081584042124e-05, 6.556081584042124e-05, 9.365830834345891e-05, 9.365830834345891e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 21:37:17,712] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=47.5917860168389, CurrSamplesPerSec=46.10096265696988, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [5]  [ 980/1404]  eta: 0:04:10  lr: 0.000094  min_lr: 0.000001  loss: 4.5585 (4.5822)  class_acc: 0.1250 (0.1460)  loss_scale: 32768.0000 (35273.1988)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1248  max mem: 15572
Epoch: [5]  [ 990/1404]  eta: 0:04:04  lr: 0.000094  min_lr: 0.000001  loss: 4.5258 (4.5818)  class_acc: 0.1250 (0.1468)  loss_scale: 32768.0000 (35247.9193)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0918  max mem: 15572
Epoch: [5]  [1000/1404]  eta: 0:03:58  lr: 0.000094  min_lr: 0.000001  loss: 4.5893 (4.5827)  class_acc: 0.1667 (0.1475)  loss_scale: 32768.0000 (35223.1449)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0530  max mem: 15572
Epoch: [5]  [1010/1404]  eta: 0:03:52  lr: 0.000094  min_lr: 0.000001  loss: 4.6209 (4.5819)  class_acc: 0.1667 (0.1477)  loss_scale: 32768.0000 (35198.8605)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0601  max mem: 15572
Epoch: [5]  [1020/1404]  eta: 0:03:46  lr: 0.000094  min_lr: 0.000001  loss: 4.5982 (4.5819)  class_acc: 0.0833 (0.1472)  loss_scale: 32768.0000 (35175.0519)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0897  max mem: 15572
[2025-01-16 21:37:45,513] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:37:45,513] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:37:45,513] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:37:45,513] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1030/1404]  eta: 0:03:40  lr: 0.000094  min_lr: 0.000001  loss: 4.4401 (4.5803)  class_acc: 0.0833 (0.1480)  loss_scale: 32768.0000 (35247.0533)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1103  max mem: 15572
[2025-01-16 21:37:51,129] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8058
[2025-01-16 21:37:51,129] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:37:51,169] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8058
[2025-01-16 21:37:51,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:37:51,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [1040/1404]  eta: 0:03:34  lr: 0.000094  min_lr: 0.000001  loss: 4.4401 (4.5802)  class_acc: 0.2083 (0.1478)  loss_scale: 32768.0000 (35443.5812)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0683  max mem: 15572
Epoch: [5]  [1050/1404]  eta: 0:03:28  lr: 0.000094  min_lr: 0.000001  loss: 4.6107 (4.5800)  class_acc: 0.0833 (0.1474)  loss_scale: 32768.0000 (35418.1237)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0629  max mem: 15572
Epoch: [5]  [1060/1404]  eta: 0:03:22  lr: 0.000094  min_lr: 0.000001  loss: 4.5376 (4.5794)  class_acc: 0.0833 (0.1474)  loss_scale: 32768.0000 (35393.1461)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0530  max mem: 15572
Epoch: [5]  [1070/1404]  eta: 0:03:17  lr: 0.000094  min_lr: 0.000001  loss: 4.5044 (4.5789)  class_acc: 0.1250 (0.1475)  loss_scale: 32768.0000 (35368.6349)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0841  max mem: 15572
Epoch: [5]  [1080/1404]  eta: 0:03:11  lr: 0.000094  min_lr: 0.000001  loss: 4.5523 (4.5789)  class_acc: 0.1667 (0.1476)  loss_scale: 32768.0000 (35344.5772)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.1371  max mem: 15572
Epoch: [5]  [1090/1404]  eta: 0:03:05  lr: 0.000094  min_lr: 0.000001  loss: 4.6086 (4.5806)  class_acc: 0.1667 (0.1480)  loss_scale: 32768.0000 (35320.9606)  weight_decay: 0.0500 (0.0500)  time: 0.6471  data: 0.1616  max mem: 15572
Epoch: [5]  [1100/1404]  eta: 0:02:59  lr: 0.000094  min_lr: 0.000001  loss: 4.6371 (4.5813)  class_acc: 0.1250 (0.1477)  loss_scale: 32768.0000 (35297.7729)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0819  max mem: 15572
Epoch: [5]  [1110/1404]  eta: 0:02:53  lr: 0.000094  min_lr: 0.000001  loss: 4.5623 (4.5810)  class_acc: 0.1250 (0.1478)  loss_scale: 32768.0000 (35275.0027)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0006  max mem: 15572
Epoch: [5]  [1120/1404]  eta: 0:02:47  lr: 0.000094  min_lr: 0.000001  loss: 4.4359 (4.5801)  class_acc: 0.1667 (0.1483)  loss_scale: 32768.0000 (35252.6387)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0627  max mem: 15572
Epoch: [5]  [1130/1404]  eta: 0:02:41  lr: 0.000094  min_lr: 0.000001  loss: 4.4549 (4.5798)  class_acc: 0.1250 (0.1484)  loss_scale: 32768.0000 (35230.6702)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1263  max mem: 15572
Epoch: [5]  [1140/1404]  eta: 0:02:35  lr: 0.000094  min_lr: 0.000001  loss: 4.4906 (4.5796)  class_acc: 0.1667 (0.1488)  loss_scale: 32768.0000 (35209.0868)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0759  max mem: 15572
Epoch: [5]  [1150/1404]  eta: 0:02:29  lr: 0.000094  min_lr: 0.000001  loss: 4.6340 (4.5803)  class_acc: 0.1667 (0.1490)  loss_scale: 32768.0000 (35187.8784)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0843  max mem: 15572
Epoch: [5]  [1160/1404]  eta: 0:02:23  lr: 0.000094  min_lr: 0.000001  loss: 4.6007 (4.5798)  class_acc: 0.2083 (0.1493)  loss_scale: 32768.0000 (35167.0353)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1213  max mem: 15572
[2025-01-16 21:39:06,105] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:39:06,105] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:39:06,106] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:39:06,106] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1170/1404]  eta: 0:02:17  lr: 0.000094  min_lr: 0.000001  loss: 4.5151 (4.5795)  class_acc: 0.1667 (0.1493)  loss_scale: 32768.0000 (35258.4799)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0785  max mem: 15572
Epoch: [5]  [1180/1404]  eta: 0:02:11  lr: 0.000094  min_lr: 0.000001  loss: 4.5185 (4.5794)  class_acc: 0.1250 (0.1493)  loss_scale: 65536.0000 (35514.8518)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0784  max mem: 15572
[2025-01-16 21:39:15,489] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8204
[2025-01-16 21:39:15,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:39:15,524] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8204
[2025-01-16 21:39:15,525] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:39:15,525] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [1190/1404]  eta: 0:02:06  lr: 0.000094  min_lr: 0.000001  loss: 4.6132 (4.5796)  class_acc: 0.1667 (0.1495)  loss_scale: 65536.0000 (35574.3275)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0879  max mem: 15572
Epoch: [5]  [1200/1404]  eta: 0:02:00  lr: 0.000094  min_lr: 0.000001  loss: 4.6110 (4.5790)  class_acc: 0.1667 (0.1501)  loss_scale: 32768.0000 (35550.9609)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1349  max mem: 15572
Epoch: [5]  [1210/1404]  eta: 0:01:54  lr: 0.000094  min_lr: 0.000001  loss: 4.4486 (4.5779)  class_acc: 0.1667 (0.1503)  loss_scale: 32768.0000 (35527.9802)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0962  max mem: 15572
Epoch: [5]  [1220/1404]  eta: 0:01:48  lr: 0.000094  min_lr: 0.000001  loss: 4.4740 (4.5779)  class_acc: 0.1667 (0.1505)  loss_scale: 32768.0000 (35505.3759)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0320  max mem: 15572
Epoch: [5]  [1230/1404]  eta: 0:01:42  lr: 0.000094  min_lr: 0.000001  loss: 4.6026 (4.5773)  class_acc: 0.1667 (0.1507)  loss_scale: 32768.0000 (35483.1389)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.0824  max mem: 15572
Epoch: [5]  [1240/1404]  eta: 0:01:36  lr: 0.000094  min_lr: 0.000001  loss: 4.4842 (4.5770)  class_acc: 0.1250 (0.1506)  loss_scale: 32768.0000 (35461.2603)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.0510  max mem: 15572
Epoch: [5]  [1250/1404]  eta: 0:01:30  lr: 0.000094  min_lr: 0.000001  loss: 4.4842 (4.5772)  class_acc: 0.1250 (0.1505)  loss_scale: 32768.0000 (35439.7314)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0653  max mem: 15572
Epoch: [5]  [1260/1404]  eta: 0:01:24  lr: 0.000094  min_lr: 0.000001  loss: 4.5002 (4.5762)  class_acc: 0.1667 (0.1505)  loss_scale: 32768.0000 (35418.5440)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0948  max mem: 15572
Epoch: [5]  [1270/1404]  eta: 0:01:18  lr: 0.000094  min_lr: 0.000001  loss: 4.5858 (4.5766)  class_acc: 0.1667 (0.1505)  loss_scale: 32768.0000 (35397.6900)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.0925  max mem: 15572
Epoch: [5]  [1280/1404]  eta: 0:01:13  lr: 0.000094  min_lr: 0.000001  loss: 4.5098 (4.5758)  class_acc: 0.1250 (0.1505)  loss_scale: 32768.0000 (35377.1616)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0752  max mem: 15572
Epoch: [5]  [1290/1404]  eta: 0:01:07  lr: 0.000094  min_lr: 0.000001  loss: 4.5098 (4.5761)  class_acc: 0.1250 (0.1503)  loss_scale: 32768.0000 (35356.9512)  weight_decay: 0.0500 (0.0500)  time: 0.5060  data: 0.0127  max mem: 15572
Epoch: [5]  [1300/1404]  eta: 0:01:01  lr: 0.000094  min_lr: 0.000001  loss: 4.5165 (4.5759)  class_acc: 0.1250 (0.1502)  loss_scale: 32768.0000 (35337.0515)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.0006  max mem: 15572
Epoch: [5]  [1310/1404]  eta: 0:00:55  lr: 0.000094  min_lr: 0.000001  loss: 4.5078 (4.5764)  class_acc: 0.1250 (0.1502)  loss_scale: 32768.0000 (35317.4554)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0007  max mem: 15572
[2025-01-16 21:40:29,710] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:40:29,711] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:40:29,744] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:40:29,745] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1320/1404]  eta: 0:00:49  lr: 0.000094  min_lr: 0.000001  loss: 4.5027 (4.5759)  class_acc: 0.1250 (0.1501)  loss_scale: 32768.0000 (35496.5995)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0006  max mem: 15572
[2025-01-16 21:40:35,042] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8343
[2025-01-16 21:40:35,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:40:35,044] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8343
[2025-01-16 21:40:35,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:40:35,044] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [1330/1404]  eta: 0:00:43  lr: 0.000094  min_lr: 0.000001  loss: 4.4234 (4.5751)  class_acc: 0.1667 (0.1505)  loss_scale: 32768.0000 (35525.3373)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0698  max mem: 15572
Epoch: [5]  [1340/1404]  eta: 0:00:37  lr: 0.000094  min_lr: 0.000001  loss: 4.4777 (4.5751)  class_acc: 0.1250 (0.1503)  loss_scale: 32768.0000 (35504.7755)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1235  max mem: 15572
Epoch: [5]  [1350/1404]  eta: 0:00:31  lr: 0.000094  min_lr: 0.000001  loss: 4.4930 (4.5748)  class_acc: 0.1250 (0.1504)  loss_scale: 32768.0000 (35484.5181)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.1403  max mem: 15572
Epoch: [5]  [1360/1404]  eta: 0:00:25  lr: 0.000094  min_lr: 0.000001  loss: 4.5113 (4.5747)  class_acc: 0.1250 (0.1505)  loss_scale: 32768.0000 (35464.5584)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1025  max mem: 15572
Epoch: [5]  [1370/1404]  eta: 0:00:20  lr: 0.000094  min_lr: 0.000001  loss: 4.5942 (4.5751)  class_acc: 0.1250 (0.1507)  loss_scale: 32768.0000 (35444.8899)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0925  max mem: 15572
Epoch: [5]  [1380/1404]  eta: 0:00:14  lr: 0.000094  min_lr: 0.000001  loss: 4.5954 (4.5757)  class_acc: 0.1667 (0.1509)  loss_scale: 32768.0000 (35425.5062)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1351  max mem: 15572
Epoch: [5]  [1390/1404]  eta: 0:00:08  lr: 0.000094  min_lr: 0.000001  loss: 4.6159 (4.5759)  class_acc: 0.1667 (0.1509)  loss_scale: 32768.0000 (35406.4012)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0757  max mem: 15572
Epoch: [5]  [1400/1404]  eta: 0:00:02  lr: 0.000094  min_lr: 0.000001  loss: 4.4077 (4.5745)  class_acc: 0.1667 (0.1511)  loss_scale: 32768.0000 (35387.5689)  weight_decay: 0.0500 (0.0500)  time: 0.4546  data: 0.0170  max mem: 15572
Epoch: [5]  [1403/1404]  eta: 0:00:00  lr: 0.000094  min_lr: 0.000001  loss: 4.4103 (4.5752)  class_acc: 0.1667 (0.1511)  loss_scale: 32768.0000 (35381.9715)  weight_decay: 0.0500 (0.0500)  time: 0.4266  data: 0.0085  max mem: 15572
Epoch: [5] Total time: 0:13:43 (0.5865 s / it)
Averaged stats: lr: 0.000094  min_lr: 0.000001  loss: 4.4103 (4.5801)  class_acc: 0.1667 (0.1506)  loss_scale: 32768.0000 (35381.9715)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:14:46  loss: 2.5014 (2.5014)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 6.5188  data: 6.3278  max mem: 15572
Val:  [ 10/136]  eta: 0:01:40  loss: 3.7322 (3.6572)  acc1: 22.2222 (25.7576)  acc5: 50.0000 (40.9091)  time: 0.8006  data: 0.5977  max mem: 15572
Val:  [ 20/136]  eta: 0:01:04  loss: 3.6876 (3.6702)  acc1: 11.1111 (19.8413)  acc5: 50.0000 (45.2381)  time: 0.2572  data: 0.0500  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 3.3497 (3.4673)  acc1: 16.6667 (28.1362)  acc5: 61.1111 (52.1505)  time: 0.3532  data: 0.1498  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.8844 (3.4288)  acc1: 16.6667 (26.2873)  acc5: 77.7778 (54.2005)  time: 0.3737  data: 0.1789  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 3.5764 (3.5477)  acc1: 5.5556 (23.3115)  acc5: 38.8889 (50.1089)  time: 0.3719  data: 0.1730  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 3.9406 (3.6313)  acc1: 5.5556 (20.9472)  acc5: 27.7778 (47.0856)  time: 0.3595  data: 0.1610  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 3.9189 (3.5795)  acc1: 16.6667 (24.0219)  acc5: 44.4444 (49.2175)  time: 0.3638  data: 0.1758  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 3.3588 (3.5549)  acc1: 33.3333 (23.8683)  acc5: 55.5556 (49.7942)  time: 0.4227  data: 0.2342  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 3.5284 (3.5467)  acc1: 11.1111 (23.2601)  acc5: 50.0000 (50.4274)  time: 0.3824  data: 0.1913  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.6399 (3.5788)  acc1: 16.6667 (22.6073)  acc5: 44.4444 (49.6150)  time: 0.3591  data: 0.1634  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.7694 (3.5967)  acc1: 16.6667 (22.6727)  acc5: 33.3333 (49.1992)  time: 0.3485  data: 0.1581  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 3.3662 (3.5662)  acc1: 27.7778 (24.7016)  acc5: 55.5556 (51.4233)  time: 0.2933  data: 0.1005  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 3.2538 (3.5503)  acc1: 44.4444 (26.2087)  acc5: 66.6667 (51.9508)  time: 0.2042  data: 0.0276  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 3.3467 (3.5582)  acc1: 33.3333 (26.2080)  acc5: 61.1111 (51.9247)  time: 0.1776  data: 0.0275  max mem: 15572
Val: Total time: 0:00:50 (0.3683 s / it)
* Acc@1 25.594 Acc@5 51.495 loss 3.589
Accuracy of the network on the 4883 val videos: 25.6%
[2025-01-16 21:42:10,746] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 21:42:10,748] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 21:42:10,748] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 21:42:10,748] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 21:42:13,002] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 21:42:13,002] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 25.59%
Epoch: [6]  [   0/1404]  eta: 3:04:23  lr: 0.000094  min_lr: 0.000001  loss: 4.4710 (4.4710)  class_acc: 0.0833 (0.0833)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.8800  data: 5.7373  max mem: 15572
Epoch: [6]  [  10/1404]  eta: 0:27:34  lr: 0.000094  min_lr: 0.000001  loss: 4.5411 (4.5056)  class_acc: 0.1250 (0.1439)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1869  data: 0.5221  max mem: 15572
Epoch: [6]  [  20/1404]  eta: 0:20:10  lr: 0.000094  min_lr: 0.000001  loss: 4.5411 (4.5567)  class_acc: 0.1667 (0.1488)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0309  max mem: 15572
Epoch: [6]  [  30/1404]  eta: 0:18:14  lr: 0.000094  min_lr: 0.000001  loss: 4.6001 (4.5688)  class_acc: 0.1250 (0.1331)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0309  max mem: 15572
Epoch: [6]  [  40/1404]  eta: 0:16:51  lr: 0.000094  min_lr: 0.000001  loss: 4.6651 (4.5816)  class_acc: 0.1250 (0.1362)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0290  max mem: 15572
[2025-01-16 21:42:47,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:42:47,266] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:42:47,291] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:42:47,293] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [  50/1404]  eta: 0:16:06  lr: 0.000094  min_lr: 0.000001  loss: 4.6651 (4.5647)  class_acc: 0.1667 (0.1413)  loss_scale: 32768.0000 (34695.5294)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0901  max mem: 15572
[2025-01-16 21:42:51,208] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8477
[2025-01-16 21:42:51,209] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:42:51,209] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:42:51,237] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8477
[2025-01-16 21:42:51,238] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [  60/1404]  eta: 0:15:21  lr: 0.000094  min_lr: 0.000001  loss: 4.4980 (4.5468)  class_acc: 0.1667 (0.1496)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0707  max mem: 15572
Epoch: [6]  [  70/1404]  eta: 0:14:53  lr: 0.000094  min_lr: 0.000001  loss: 4.5186 (4.5480)  class_acc: 0.1667 (0.1532)  loss_scale: 32768.0000 (35075.6056)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0545  max mem: 15572
Epoch: [6]  [  80/1404]  eta: 0:14:43  lr: 0.000094  min_lr: 0.000001  loss: 4.5468 (4.5512)  class_acc: 0.1667 (0.1590)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1271  max mem: 15572
Epoch: [6]  [  90/1404]  eta: 0:14:12  lr: 0.000094  min_lr: 0.000001  loss: 4.5441 (4.5517)  class_acc: 0.1250 (0.1548)  loss_scale: 32768.0000 (34568.4396)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0857  max mem: 15572
Epoch: [6]  [ 100/1404]  eta: 0:14:06  lr: 0.000094  min_lr: 0.000001  loss: 4.4897 (4.5497)  class_acc: 0.0833 (0.1531)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0831  max mem: 15572
Epoch: [6]  [ 110/1404]  eta: 0:14:00  lr: 0.000094  min_lr: 0.000001  loss: 4.4897 (4.5437)  class_acc: 0.1250 (0.1498)  loss_scale: 32768.0000 (34244.0360)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1616  max mem: 15572
Epoch: [6]  [ 120/1404]  eta: 0:13:54  lr: 0.000094  min_lr: 0.000001  loss: 4.4818 (4.5458)  class_acc: 0.1250 (0.1525)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.1744  max mem: 15572
Epoch: [6]  [ 130/1404]  eta: 0:13:40  lr: 0.000094  min_lr: 0.000001  loss: 4.5235 (4.5518)  class_acc: 0.1667 (0.1543)  loss_scale: 32768.0000 (34018.6870)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1313  max mem: 15572
Epoch: [6]  [ 140/1404]  eta: 0:13:24  lr: 0.000094  min_lr: 0.000001  loss: 4.6214 (4.5548)  class_acc: 0.1250 (0.1537)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0503  max mem: 15572
Epoch: [6]  [ 150/1404]  eta: 0:13:18  lr: 0.000094  min_lr: 0.000001  loss: 4.6092 (4.5542)  class_acc: 0.1667 (0.1570)  loss_scale: 32768.0000 (33853.0331)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0868  max mem: 15572
Epoch: [6]  [ 160/1404]  eta: 0:13:01  lr: 0.000094  min_lr: 0.000001  loss: 4.5180 (4.5486)  class_acc: 0.2083 (0.1592)  loss_scale: 32768.0000 (33785.6398)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0759  max mem: 15572
Epoch: [6]  [ 170/1404]  eta: 0:12:49  lr: 0.000094  min_lr: 0.000001  loss: 4.5180 (4.5499)  class_acc: 0.1250 (0.1574)  loss_scale: 32768.0000 (33726.1287)  weight_decay: 0.0500 (0.0500)  time: 0.5274  data: 0.0236  max mem: 15572
Epoch: [6]  [ 180/1404]  eta: 0:12:37  lr: 0.000094  min_lr: 0.000001  loss: 4.5668 (4.5529)  class_acc: 0.1250 (0.1549)  loss_scale: 32768.0000 (33673.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0236  max mem: 15572
[2025-01-16 21:44:06,107] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:44:06,108] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:44:06,126] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:44:06,126] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 190/1404]  eta: 0:12:28  lr: 0.000094  min_lr: 0.000001  loss: 4.5643 (4.5478)  class_acc: 0.1250 (0.1564)  loss_scale: 32768.0000 (35169.8429)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0007  max mem: 15572
Epoch: [6]  [ 200/1404]  eta: 0:12:25  lr: 0.000094  min_lr: 0.000001  loss: 4.5643 (4.5504)  class_acc: 0.1667 (0.1582)  loss_scale: 65536.0000 (36680.5970)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0266  max mem: 15572
Epoch: [6]  [ 210/1404]  eta: 0:12:17  lr: 0.000094  min_lr: 0.000001  loss: 4.5576 (4.5473)  class_acc: 0.1667 (0.1580)  loss_scale: 65536.0000 (38048.1517)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.0266  max mem: 15572
Epoch: [6]  [ 220/1404]  eta: 0:12:07  lr: 0.000093  min_lr: 0.000001  loss: 4.4608 (4.5453)  class_acc: 0.1250 (0.1563)  loss_scale: 65536.0000 (39291.9457)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0007  max mem: 15572
[2025-01-16 21:44:29,705] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8646
[2025-01-16 21:44:29,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:44:29,705] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:44:29,707] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8646
[2025-01-16 21:44:29,707] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [ 230/1404]  eta: 0:11:58  lr: 0.000093  min_lr: 0.000001  loss: 4.4993 (4.5445)  class_acc: 0.1667 (0.1593)  loss_scale: 65536.0000 (39151.3766)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0007  max mem: 15572
Epoch: [6]  [ 240/1404]  eta: 0:11:52  lr: 0.000093  min_lr: 0.000001  loss: 4.5368 (4.5434)  class_acc: 0.1667 (0.1585)  loss_scale: 32768.0000 (38886.5062)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0007  max mem: 15572
Epoch: [6]  [ 250/1404]  eta: 0:11:43  lr: 0.000093  min_lr: 0.000001  loss: 4.5725 (4.5453)  class_acc: 0.1667 (0.1599)  loss_scale: 32768.0000 (38642.7410)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0006  max mem: 15572
Epoch: [6]  [ 260/1404]  eta: 0:11:37  lr: 0.000093  min_lr: 0.000001  loss: 4.3658 (4.5363)  class_acc: 0.1667 (0.1609)  loss_scale: 32768.0000 (38417.6552)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0458  max mem: 15572
Epoch: [6]  [ 270/1404]  eta: 0:11:28  lr: 0.000093  min_lr: 0.000001  loss: 4.4866 (4.5387)  class_acc: 0.1667 (0.1624)  loss_scale: 32768.0000 (38209.1808)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0658  max mem: 15572
Epoch: [6]  [ 280/1404]  eta: 0:11:20  lr: 0.000093  min_lr: 0.000001  loss: 4.5719 (4.5363)  class_acc: 0.1250 (0.1615)  loss_scale: 32768.0000 (38015.5445)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0452  max mem: 15572
Epoch: [6]  [ 290/1404]  eta: 0:11:15  lr: 0.000093  min_lr: 0.000001  loss: 4.4492 (4.5342)  class_acc: 0.1667 (0.1618)  loss_scale: 32768.0000 (37835.2165)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0893  max mem: 15572
Epoch: [6]  [ 300/1404]  eta: 0:11:08  lr: 0.000093  min_lr: 0.000001  loss: 4.5120 (4.5375)  class_acc: 0.1667 (0.1613)  loss_scale: 32768.0000 (37666.8704)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1063  max mem: 15572
Epoch: [6]  [ 310/1404]  eta: 0:11:03  lr: 0.000093  min_lr: 0.000001  loss: 4.5747 (4.5377)  class_acc: 0.1667 (0.1608)  loss_scale: 32768.0000 (37509.3505)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1138  max mem: 15572
Epoch: [6]  [ 320/1404]  eta: 0:10:56  lr: 0.000093  min_lr: 0.000001  loss: 4.6059 (4.5400)  class_acc: 0.1250 (0.1607)  loss_scale: 32768.0000 (37361.6449)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1161  max mem: 15572
Epoch: [6]  [ 330/1404]  eta: 0:10:50  lr: 0.000093  min_lr: 0.000001  loss: 4.5992 (4.5405)  class_acc: 0.1667 (0.1626)  loss_scale: 32768.0000 (37222.8640)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0993  max mem: 15572
Epoch: [6]  [ 340/1404]  eta: 0:10:42  lr: 0.000093  min_lr: 0.000001  loss: 4.4921 (4.5405)  class_acc: 0.1667 (0.1621)  loss_scale: 32768.0000 (37092.2229)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0558  max mem: 15572
Epoch: [6]  [ 350/1404]  eta: 0:10:36  lr: 0.000093  min_lr: 0.000001  loss: 4.4881 (4.5395)  class_acc: 0.1250 (0.1639)  loss_scale: 32768.0000 (36969.0256)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0664  max mem: 15572
[2025-01-16 21:45:45,582] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:45:45,583] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:45:45,583] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:45:45,583] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 360/1404]  eta: 0:10:29  lr: 0.000093  min_lr: 0.000001  loss: 4.4989 (4.5404)  class_acc: 0.2083 (0.1651)  loss_scale: 32768.0000 (37760.3546)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1108  max mem: 15572
Epoch: [6]  [ 370/1404]  eta: 0:10:24  lr: 0.000093  min_lr: 0.000001  loss: 4.4989 (4.5395)  class_acc: 0.2083 (0.1651)  loss_scale: 65536.0000 (38509.0243)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1047  max mem: 15572
[2025-01-16 21:46:01,786] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8803
[2025-01-16 21:46:01,786] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8803
[2025-01-16 21:46:01,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:46:01,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:46:01,786] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 380/1404]  eta: 0:10:15  lr: 0.000093  min_lr: 0.000001  loss: 4.4978 (4.5403)  class_acc: 0.1667 (0.1660)  loss_scale: 65536.0000 (39046.3832)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0600  max mem: 15572
Epoch: [6]  [ 390/1404]  eta: 0:10:08  lr: 0.000093  min_lr: 0.000001  loss: 4.4861 (4.5413)  class_acc: 0.1667 (0.1657)  loss_scale: 32768.0000 (38885.8107)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0330  max mem: 15572
Epoch: [6]  [ 400/1404]  eta: 0:10:02  lr: 0.000093  min_lr: 0.000001  loss: 4.4799 (4.5399)  class_acc: 0.1250 (0.1653)  loss_scale: 32768.0000 (38733.2469)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0848  max mem: 15572
Epoch: [6]  [ 410/1404]  eta: 0:09:54  lr: 0.000093  min_lr: 0.000001  loss: 4.5386 (4.5424)  class_acc: 0.1667 (0.1660)  loss_scale: 32768.0000 (38588.1071)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0718  max mem: 15572
Epoch: [6]  [ 420/1404]  eta: 0:09:49  lr: 0.000093  min_lr: 0.000001  loss: 4.6140 (4.5441)  class_acc: 0.2083 (0.1664)  loss_scale: 32768.0000 (38449.8622)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0901  max mem: 15572
Epoch: [6]  [ 430/1404]  eta: 0:09:43  lr: 0.000093  min_lr: 0.000001  loss: 4.5177 (4.5426)  class_acc: 0.1667 (0.1663)  loss_scale: 32768.0000 (38318.0325)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1382  max mem: 15572
Epoch: [6]  [ 440/1404]  eta: 0:09:38  lr: 0.000093  min_lr: 0.000001  loss: 4.5213 (4.5435)  class_acc: 0.1250 (0.1657)  loss_scale: 32768.0000 (38192.1814)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1132  max mem: 15572
Epoch: [6]  [ 450/1404]  eta: 0:09:32  lr: 0.000093  min_lr: 0.000001  loss: 4.5703 (4.5437)  class_acc: 0.1667 (0.1657)  loss_scale: 32768.0000 (38071.9113)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1096  max mem: 15572
Epoch: [6]  [ 460/1404]  eta: 0:09:26  lr: 0.000093  min_lr: 0.000001  loss: 4.6568 (4.5455)  class_acc: 0.0833 (0.1644)  loss_scale: 32768.0000 (37956.8590)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1370  max mem: 15572
Epoch: [6]  [ 470/1404]  eta: 0:09:19  lr: 0.000093  min_lr: 0.000001  loss: 4.4803 (4.5410)  class_acc: 0.1250 (0.1649)  loss_scale: 32768.0000 (37846.6921)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1022  max mem: 15572
Epoch: [6]  [ 480/1404]  eta: 0:09:11  lr: 0.000093  min_lr: 0.000001  loss: 4.4146 (4.5390)  class_acc: 0.2083 (0.1658)  loss_scale: 32768.0000 (37741.1060)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0297  max mem: 15572
Epoch: [6]  [ 490/1404]  eta: 0:09:05  lr: 0.000093  min_lr: 0.000001  loss: 4.5818 (4.5399)  class_acc: 0.1667 (0.1657)  loss_scale: 32768.0000 (37639.8208)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0333  max mem: 15572
Epoch: [6]  [ 500/1404]  eta: 0:09:00  lr: 0.000093  min_lr: 0.000001  loss: 4.5763 (4.5397)  class_acc: 0.1667 (0.1657)  loss_scale: 32768.0000 (37542.5788)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.0938  max mem: 15572
[2025-01-16 21:47:18,067] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:47:18,067] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:47:18,074] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:47:18,075] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 510/1404]  eta: 0:08:55  lr: 0.000093  min_lr: 0.000001  loss: 4.5763 (4.5399)  class_acc: 0.1250 (0.1645)  loss_scale: 32768.0000 (37641.5186)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1316  max mem: 15572
[2025-01-16 21:47:23,987] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8944
[2025-01-16 21:47:23,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:47:23,998] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8944
[2025-01-16 21:47:23,998] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:47:23,998] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 520/1404]  eta: 0:08:47  lr: 0.000093  min_lr: 0.000001  loss: 4.5401 (4.5383)  class_acc: 0.1250 (0.1643)  loss_scale: 65536.0000 (38114.0269)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0711  max mem: 15572
Epoch: [6]  [ 530/1404]  eta: 0:08:41  lr: 0.000093  min_lr: 0.000001  loss: 4.4459 (4.5367)  class_acc: 0.1667 (0.1646)  loss_scale: 32768.0000 (38013.3484)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0340  max mem: 15572
Epoch: [6]  [ 540/1404]  eta: 0:08:34  lr: 0.000093  min_lr: 0.000001  loss: 4.5067 (4.5379)  class_acc: 0.1667 (0.1637)  loss_scale: 32768.0000 (37916.3919)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0555  max mem: 15572
Epoch: [6]  [ 550/1404]  eta: 0:08:29  lr: 0.000093  min_lr: 0.000001  loss: 4.5967 (4.5379)  class_acc: 0.1667 (0.1649)  loss_scale: 32768.0000 (37822.9546)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1103  max mem: 15572
Epoch: [6]  [ 560/1404]  eta: 0:08:22  lr: 0.000093  min_lr: 0.000001  loss: 4.5567 (4.5370)  class_acc: 0.1250 (0.1646)  loss_scale: 32768.0000 (37732.8485)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0888  max mem: 15572
Epoch: [6]  [ 570/1404]  eta: 0:08:17  lr: 0.000093  min_lr: 0.000001  loss: 4.3537 (4.5348)  class_acc: 0.1250 (0.1646)  loss_scale: 32768.0000 (37645.8984)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0937  max mem: 15572
[2025-01-16 21:47:56,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=49, lr=[9.047079288391729e-07, 9.047079288391729e-07, 1.2924398983416757e-06, 1.2924398983416757e-06, 1.8463427119166798e-06, 1.8463427119166798e-06, 2.637632445595257e-06, 2.637632445595257e-06, 3.7680463508503673e-06, 3.7680463508503673e-06, 5.382923358357668e-06, 5.382923358357668e-06, 7.689890511939525e-06, 7.689890511939525e-06, 1.0985557874199324e-05, 1.0985557874199324e-05, 1.5693654105999034e-05, 1.5693654105999034e-05, 2.241950586571291e-05, 2.241950586571291e-05, 3.202786552244701e-05, 3.202786552244701e-05, 4.575409360349573e-05, 4.575409360349573e-05, 6.536299086213677e-05, 6.536299086213677e-05, 9.337570123162396e-05, 9.337570123162396e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 21:47:56,328] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=47.886962350843596, CurrSamplesPerSec=56.34813566323883, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [ 580/1404]  eta: 0:08:10  lr: 0.000093  min_lr: 0.000001  loss: 4.3378 (4.5331)  class_acc: 0.1667 (0.1652)  loss_scale: 32768.0000 (37561.9415)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1059  max mem: 15572
Epoch: [6]  [ 590/1404]  eta: 0:08:04  lr: 0.000093  min_lr: 0.000001  loss: 4.4995 (4.5337)  class_acc: 0.2083 (0.1656)  loss_scale: 32768.0000 (37480.8257)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0606  max mem: 15572
Epoch: [6]  [ 600/1404]  eta: 0:07:58  lr: 0.000093  min_lr: 0.000001  loss: 4.5719 (4.5347)  class_acc: 0.1667 (0.1662)  loss_scale: 32768.0000 (37402.4093)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1343  max mem: 15572
Epoch: [6]  [ 610/1404]  eta: 0:07:51  lr: 0.000093  min_lr: 0.000001  loss: 4.5876 (4.5349)  class_acc: 0.1667 (0.1663)  loss_scale: 32768.0000 (37326.5597)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0866  max mem: 15572
Epoch: [6]  [ 620/1404]  eta: 0:07:45  lr: 0.000093  min_lr: 0.000001  loss: 4.5735 (4.5342)  class_acc: 0.1667 (0.1669)  loss_scale: 32768.0000 (37253.1530)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0301  max mem: 15572
Epoch: [6]  [ 630/1404]  eta: 0:07:38  lr: 0.000093  min_lr: 0.000001  loss: 4.4803 (4.5339)  class_acc: 0.2083 (0.1669)  loss_scale: 32768.0000 (37182.0729)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0431  max mem: 15572
Epoch: [6]  [ 640/1404]  eta: 0:07:33  lr: 0.000093  min_lr: 0.000001  loss: 4.4641 (4.5332)  class_acc: 0.2083 (0.1667)  loss_scale: 32768.0000 (37113.2106)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0640  max mem: 15572
[2025-01-16 21:48:38,861] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:48:38,861] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:48:38,897] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:48:38,897] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 650/1404]  eta: 0:07:27  lr: 0.000093  min_lr: 0.000001  loss: 4.4802 (4.5334)  class_acc: 0.1667 (0.1669)  loss_scale: 32768.0000 (37147.1336)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1005  max mem: 15572
[2025-01-16 21:48:45,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9084
[2025-01-16 21:48:45,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:48:45,700] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9084
[2025-01-16 21:48:45,700] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:48:45,701] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 660/1404]  eta: 0:07:21  lr: 0.000093  min_lr: 0.000001  loss: 4.4877 (4.5313)  class_acc: 0.1250 (0.1664)  loss_scale: 65536.0000 (37527.0439)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1101  max mem: 15572
Epoch: [6]  [ 670/1404]  eta: 0:07:16  lr: 0.000093  min_lr: 0.000001  loss: 4.5276 (4.5334)  class_acc: 0.1250 (0.1662)  loss_scale: 32768.0000 (37456.1192)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1398  max mem: 15572
Epoch: [6]  [ 680/1404]  eta: 0:07:09  lr: 0.000093  min_lr: 0.000001  loss: 4.6122 (4.5327)  class_acc: 0.1667 (0.1665)  loss_scale: 32768.0000 (37387.2775)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0797  max mem: 15572
Epoch: [6]  [ 690/1404]  eta: 0:07:04  lr: 0.000093  min_lr: 0.000001  loss: 4.5882 (4.5335)  class_acc: 0.1667 (0.1662)  loss_scale: 32768.0000 (37320.4284)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0652  max mem: 15572
Epoch: [6]  [ 700/1404]  eta: 0:06:58  lr: 0.000093  min_lr: 0.000001  loss: 4.5403 (4.5336)  class_acc: 0.1667 (0.1661)  loss_scale: 32768.0000 (37255.4864)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.1416  max mem: 15572
Epoch: [6]  [ 710/1404]  eta: 0:06:52  lr: 0.000093  min_lr: 0.000001  loss: 4.5226 (4.5334)  class_acc: 0.1250 (0.1664)  loss_scale: 32768.0000 (37192.3713)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1213  max mem: 15572
Epoch: [6]  [ 720/1404]  eta: 0:06:45  lr: 0.000093  min_lr: 0.000001  loss: 4.4485 (4.5309)  class_acc: 0.1667 (0.1671)  loss_scale: 32768.0000 (37131.0069)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0449  max mem: 15572
Epoch: [6]  [ 730/1404]  eta: 0:06:39  lr: 0.000093  min_lr: 0.000001  loss: 4.4110 (4.5309)  class_acc: 0.1250 (0.1666)  loss_scale: 32768.0000 (37071.3215)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0405  max mem: 15572
Epoch: [6]  [ 740/1404]  eta: 0:06:33  lr: 0.000093  min_lr: 0.000001  loss: 4.5745 (4.5303)  class_acc: 0.1250 (0.1669)  loss_scale: 32768.0000 (37013.2470)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0698  max mem: 15572
Epoch: [6]  [ 750/1404]  eta: 0:06:27  lr: 0.000093  min_lr: 0.000001  loss: 4.4346 (4.5302)  class_acc: 0.1667 (0.1671)  loss_scale: 32768.0000 (36956.7190)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1140  max mem: 15572
Epoch: [6]  [ 760/1404]  eta: 0:06:22  lr: 0.000093  min_lr: 0.000001  loss: 4.4750 (4.5302)  class_acc: 0.1667 (0.1668)  loss_scale: 32768.0000 (36901.6767)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.1616  max mem: 15572
Epoch: [6]  [ 770/1404]  eta: 0:06:15  lr: 0.000093  min_lr: 0.000001  loss: 4.5981 (4.5298)  class_acc: 0.1250 (0.1666)  loss_scale: 32768.0000 (36848.0623)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0773  max mem: 15572
Epoch: [6]  [ 780/1404]  eta: 0:06:09  lr: 0.000093  min_lr: 0.000001  loss: 4.5981 (4.5300)  class_acc: 0.1667 (0.1663)  loss_scale: 32768.0000 (36795.8207)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0139  max mem: 15572
[2025-01-16 21:50:00,197] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:50:00,197] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:50:00,334] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:50:00,335] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 790/1404]  eta: 0:06:02  lr: 0.000093  min_lr: 0.000001  loss: 4.4686 (4.5284)  class_acc: 0.1250 (0.1662)  loss_scale: 32768.0000 (36827.7522)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0295  max mem: 15572
[2025-01-16 21:50:01,733] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9216
[2025-01-16 21:50:01,733] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:50:01,733] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:50:01,747] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9216
[2025-01-16 21:50:01,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [ 800/1404]  eta: 0:05:56  lr: 0.000093  min_lr: 0.000001  loss: 4.5473 (4.5310)  class_acc: 0.0833 (0.1654)  loss_scale: 32768.0000 (36817.9775)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0632  max mem: 15572
Epoch: [6]  [ 810/1404]  eta: 0:05:50  lr: 0.000093  min_lr: 0.000001  loss: 4.5842 (4.5307)  class_acc: 0.1250 (0.1656)  loss_scale: 32768.0000 (36768.0395)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0557  max mem: 15572
Epoch: [6]  [ 820/1404]  eta: 0:05:44  lr: 0.000093  min_lr: 0.000001  loss: 4.5512 (4.5316)  class_acc: 0.1667 (0.1661)  loss_scale: 32768.0000 (36719.3179)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0142  max mem: 15572
Epoch: [6]  [ 830/1404]  eta: 0:05:38  lr: 0.000093  min_lr: 0.000001  loss: 4.6182 (4.5328)  class_acc: 0.1667 (0.1663)  loss_scale: 32768.0000 (36671.7690)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0066  max mem: 15572
Epoch: [6]  [ 840/1404]  eta: 0:05:32  lr: 0.000093  min_lr: 0.000001  loss: 4.5515 (4.5333)  class_acc: 0.1250 (0.1660)  loss_scale: 32768.0000 (36625.3508)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0784  max mem: 15572
Epoch: [6]  [ 850/1404]  eta: 0:05:26  lr: 0.000093  min_lr: 0.000001  loss: 4.5312 (4.5334)  class_acc: 0.1667 (0.1665)  loss_scale: 32768.0000 (36580.0235)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1073  max mem: 15572
Epoch: [6]  [ 860/1404]  eta: 0:05:21  lr: 0.000093  min_lr: 0.000001  loss: 4.5880 (4.5340)  class_acc: 0.1667 (0.1664)  loss_scale: 32768.0000 (36535.7491)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1060  max mem: 15572
Epoch: [6]  [ 870/1404]  eta: 0:05:15  lr: 0.000093  min_lr: 0.000001  loss: 4.4898 (4.5331)  class_acc: 0.1667 (0.1666)  loss_scale: 32768.0000 (36492.4914)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.1618  max mem: 15572
Epoch: [6]  [ 880/1404]  eta: 0:05:09  lr: 0.000093  min_lr: 0.000001  loss: 4.4599 (4.5333)  class_acc: 0.2083 (0.1669)  loss_scale: 32768.0000 (36450.2157)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1171  max mem: 15572
Epoch: [6]  [ 890/1404]  eta: 0:05:03  lr: 0.000093  min_lr: 0.000001  loss: 4.4599 (4.5338)  class_acc: 0.2083 (0.1674)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0563  max mem: 15572
Epoch: [6]  [ 900/1404]  eta: 0:04:57  lr: 0.000093  min_lr: 0.000001  loss: 4.4514 (4.5330)  class_acc: 0.2083 (0.1678)  loss_scale: 32768.0000 (36368.4795)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0371  max mem: 15572
Epoch: [6]  [ 910/1404]  eta: 0:04:51  lr: 0.000093  min_lr: 0.000001  loss: 4.3886 (4.5309)  class_acc: 0.2083 (0.1681)  loss_scale: 32768.0000 (36328.9572)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1010  max mem: 15572
Epoch: [6]  [ 920/1404]  eta: 0:04:46  lr: 0.000093  min_lr: 0.000001  loss: 4.4228 (4.5309)  class_acc: 0.2083 (0.1679)  loss_scale: 32768.0000 (36290.2932)  weight_decay: 0.0500 (0.0500)  time: 0.6744  data: 0.1887  max mem: 15572
[2025-01-16 21:51:18,889] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:51:18,889] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:51:18,890] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:51:18,890] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:51:22,834] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9352
[2025-01-16 21:51:22,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:51:22,834] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:51:22,880] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9352
[2025-01-16 21:51:22,881] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [ 930/1404]  eta: 0:04:40  lr: 0.000093  min_lr: 0.000001  loss: 4.5866 (4.5324)  class_acc: 0.1250 (0.1678)  loss_scale: 32768.0000 (36498.8357)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1228  max mem: 15572
Epoch: [6]  [ 940/1404]  eta: 0:04:34  lr: 0.000093  min_lr: 0.000001  loss: 4.6279 (4.5332)  class_acc: 0.1250 (0.1672)  loss_scale: 32768.0000 (36459.1881)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0570  max mem: 15572
Epoch: [6]  [ 950/1404]  eta: 0:04:28  lr: 0.000093  min_lr: 0.000001  loss: 4.4670 (4.5318)  class_acc: 0.1250 (0.1678)  loss_scale: 32768.0000 (36420.3743)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0686  max mem: 15572
Epoch: [6]  [ 960/1404]  eta: 0:04:22  lr: 0.000093  min_lr: 0.000001  loss: 4.4372 (4.5317)  class_acc: 0.1667 (0.1677)  loss_scale: 32768.0000 (36382.3684)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0777  max mem: 15572
Epoch: [6]  [ 970/1404]  eta: 0:04:16  lr: 0.000093  min_lr: 0.000001  loss: 4.5336 (4.5315)  class_acc: 0.1250 (0.1674)  loss_scale: 32768.0000 (36345.1452)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0868  max mem: 15572
Epoch: [6]  [ 980/1404]  eta: 0:04:10  lr: 0.000093  min_lr: 0.000001  loss: 4.5596 (4.5316)  class_acc: 0.1250 (0.1671)  loss_scale: 32768.0000 (36308.6809)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.0729  max mem: 15572
Epoch: [6]  [ 990/1404]  eta: 0:04:04  lr: 0.000093  min_lr: 0.000001  loss: 4.5093 (4.5324)  class_acc: 0.1667 (0.1670)  loss_scale: 32768.0000 (36272.9526)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0766  max mem: 15572
Epoch: [6]  [1000/1404]  eta: 0:03:58  lr: 0.000093  min_lr: 0.000001  loss: 4.4818 (4.5328)  class_acc: 0.1667 (0.1675)  loss_scale: 32768.0000 (36237.9381)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1000  max mem: 15572
Epoch: [6]  [1010/1404]  eta: 0:03:52  lr: 0.000093  min_lr: 0.000001  loss: 4.5003 (4.5328)  class_acc: 0.1667 (0.1677)  loss_scale: 32768.0000 (36203.6162)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0712  max mem: 15572
Epoch: [6]  [1020/1404]  eta: 0:03:46  lr: 0.000093  min_lr: 0.000001  loss: 4.5239 (4.5334)  class_acc: 0.1250 (0.1674)  loss_scale: 32768.0000 (36169.9667)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0352  max mem: 15572
Epoch: [6]  [1030/1404]  eta: 0:03:40  lr: 0.000093  min_lr: 0.000001  loss: 4.4672 (4.5317)  class_acc: 0.1667 (0.1683)  loss_scale: 32768.0000 (36136.9699)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0688  max mem: 15572
Epoch: [6]  [1040/1404]  eta: 0:03:34  lr: 0.000093  min_lr: 0.000001  loss: 4.4247 (4.5324)  class_acc: 0.2083 (0.1688)  loss_scale: 32768.0000 (36104.6071)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0802  max mem: 15572
Epoch: [6]  [1050/1404]  eta: 0:03:28  lr: 0.000093  min_lr: 0.000001  loss: 4.5260 (4.5315)  class_acc: 0.1667 (0.1686)  loss_scale: 32768.0000 (36072.8601)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0659  max mem: 15572
[2025-01-16 21:52:36,766] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:52:36,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:52:36,766] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:52:36,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:52:37,275] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9482
[2025-01-16 21:52:37,275] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:52:37,276] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:52:37,330] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9482
[2025-01-16 21:52:37,331] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [1060/1404]  eta: 0:03:22  lr: 0.000093  min_lr: 0.000001  loss: 4.5260 (4.5327)  class_acc: 0.1250 (0.1682)  loss_scale: 32768.0000 (36072.5957)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0597  max mem: 15572
Epoch: [6]  [1070/1404]  eta: 0:03:16  lr: 0.000093  min_lr: 0.000001  loss: 4.5618 (4.5327)  class_acc: 0.1667 (0.1681)  loss_scale: 32768.0000 (36041.7404)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0794  max mem: 15572
Epoch: [6]  [1080/1404]  eta: 0:03:10  lr: 0.000093  min_lr: 0.000001  loss: 4.4660 (4.5317)  class_acc: 0.1667 (0.1685)  loss_scale: 32768.0000 (36011.4561)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0843  max mem: 15572
Epoch: [6]  [1090/1404]  eta: 0:03:04  lr: 0.000093  min_lr: 0.000001  loss: 4.4249 (4.5306)  class_acc: 0.1667 (0.1689)  loss_scale: 32768.0000 (35981.7269)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0454  max mem: 15572
Epoch: [6]  [1100/1404]  eta: 0:02:59  lr: 0.000093  min_lr: 0.000001  loss: 4.4764 (4.5307)  class_acc: 0.1667 (0.1686)  loss_scale: 32768.0000 (35952.5377)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.0294  max mem: 15572
Epoch: [6]  [1110/1404]  eta: 0:02:53  lr: 0.000093  min_lr: 0.000001  loss: 4.5834 (4.5313)  class_acc: 0.1250 (0.1685)  loss_scale: 32768.0000 (35923.8740)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.0113  max mem: 15572
Epoch: [6]  [1120/1404]  eta: 0:02:47  lr: 0.000093  min_lr: 0.000001  loss: 4.5126 (4.5310)  class_acc: 0.1667 (0.1686)  loss_scale: 32768.0000 (35895.7217)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0007  max mem: 15572
Epoch: [6]  [1130/1404]  eta: 0:02:41  lr: 0.000093  min_lr: 0.000001  loss: 4.4627 (4.5314)  class_acc: 0.1667 (0.1687)  loss_scale: 32768.0000 (35868.0672)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.0007  max mem: 15572
Epoch: [6]  [1140/1404]  eta: 0:02:35  lr: 0.000093  min_lr: 0.000001  loss: 4.5440 (4.5315)  class_acc: 0.1667 (0.1690)  loss_scale: 32768.0000 (35840.8975)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0006  max mem: 15572
Epoch: [6]  [1150/1404]  eta: 0:02:29  lr: 0.000093  min_lr: 0.000001  loss: 4.5754 (4.5304)  class_acc: 0.1667 (0.1691)  loss_scale: 32768.0000 (35814.1998)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0627  max mem: 15572
Epoch: [6]  [1160/1404]  eta: 0:02:23  lr: 0.000093  min_lr: 0.000001  loss: 4.4204 (4.5298)  class_acc: 0.1667 (0.1694)  loss_scale: 32768.0000 (35787.9621)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.0831  max mem: 15572
Epoch: [6]  [1170/1404]  eta: 0:02:17  lr: 0.000093  min_lr: 0.000001  loss: 4.3873 (4.5292)  class_acc: 0.2083 (0.1697)  loss_scale: 32768.0000 (35762.1725)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0418  max mem: 15572
Epoch: [6]  [1180/1404]  eta: 0:02:11  lr: 0.000093  min_lr: 0.000001  loss: 4.3702 (4.5283)  class_acc: 0.2083 (0.1700)  loss_scale: 32768.0000 (35736.8196)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0721  max mem: 15572
[2025-01-16 21:53:53,114] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:53:53,115] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:53:53,119] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:53:53,120] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [1190/1404]  eta: 0:02:05  lr: 0.000093  min_lr: 0.000001  loss: 4.4121 (4.5279)  class_acc: 0.2083 (0.1702)  loss_scale: 32768.0000 (35821.9446)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0665  max mem: 15572
Epoch: [6]  [1200/1404]  eta: 0:02:00  lr: 0.000093  min_lr: 0.000001  loss: 4.4569 (4.5282)  class_acc: 0.1667 (0.1698)  loss_scale: 65536.0000 (36069.3555)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0721  max mem: 15572
[2025-01-16 21:54:05,552] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9633
[2025-01-16 21:54:05,552] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:54:05,578] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9633
[2025-01-16 21:54:05,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:54:05,579] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [1210/1404]  eta: 0:01:54  lr: 0.000093  min_lr: 0.000001  loss: 4.3680 (4.5265)  class_acc: 0.1667 (0.1699)  loss_scale: 65536.0000 (36258.5632)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0878  max mem: 15572
Epoch: [6]  [1220/1404]  eta: 0:01:48  lr: 0.000093  min_lr: 0.000001  loss: 4.3762 (4.5261)  class_acc: 0.1667 (0.1702)  loss_scale: 32768.0000 (36229.9754)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1443  max mem: 15572
Epoch: [6]  [1230/1404]  eta: 0:01:42  lr: 0.000093  min_lr: 0.000001  loss: 4.5148 (4.5259)  class_acc: 0.1667 (0.1701)  loss_scale: 32768.0000 (36201.8522)  weight_decay: 0.0500 (0.0500)  time: 0.6589  data: 0.1767  max mem: 15572
Epoch: [6]  [1240/1404]  eta: 0:01:36  lr: 0.000093  min_lr: 0.000001  loss: 4.5446 (4.5267)  class_acc: 0.1250 (0.1702)  loss_scale: 32768.0000 (36174.1821)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1100  max mem: 15572
Epoch: [6]  [1250/1404]  eta: 0:01:30  lr: 0.000093  min_lr: 0.000001  loss: 4.6001 (4.5273)  class_acc: 0.1250 (0.1699)  loss_scale: 32768.0000 (36146.9544)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0508  max mem: 15572
Epoch: [6]  [1260/1404]  eta: 0:01:24  lr: 0.000093  min_lr: 0.000001  loss: 4.5510 (4.5272)  class_acc: 0.1250 (0.1697)  loss_scale: 32768.0000 (36120.1586)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0420  max mem: 15572
Epoch: [6]  [1270/1404]  eta: 0:01:18  lr: 0.000093  min_lr: 0.000001  loss: 4.5117 (4.5272)  class_acc: 0.1250 (0.1697)  loss_scale: 32768.0000 (36093.7844)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0789  max mem: 15572
Epoch: [6]  [1280/1404]  eta: 0:01:13  lr: 0.000093  min_lr: 0.000001  loss: 4.5117 (4.5281)  class_acc: 0.1250 (0.1698)  loss_scale: 32768.0000 (36067.8220)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0809  max mem: 15572
Epoch: [6]  [1290/1404]  eta: 0:01:07  lr: 0.000093  min_lr: 0.000001  loss: 4.5008 (4.5281)  class_acc: 0.1667 (0.1698)  loss_scale: 32768.0000 (36042.2618)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.0546  max mem: 15572
Epoch: [6]  [1300/1404]  eta: 0:01:01  lr: 0.000093  min_lr: 0.000001  loss: 4.4897 (4.5280)  class_acc: 0.1667 (0.1699)  loss_scale: 32768.0000 (36017.0945)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0837  max mem: 15572
Epoch: [6]  [1310/1404]  eta: 0:00:55  lr: 0.000093  min_lr: 0.000001  loss: 4.5421 (4.5277)  class_acc: 0.2083 (0.1702)  loss_scale: 32768.0000 (35992.3112)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1245  max mem: 15572
Epoch: [6]  [1320/1404]  eta: 0:00:49  lr: 0.000093  min_lr: 0.000001  loss: 4.4479 (4.5265)  class_acc: 0.2083 (0.1703)  loss_scale: 32768.0000 (35967.9031)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1026  max mem: 15572
Epoch: [6]  [1330/1404]  eta: 0:00:43  lr: 0.000093  min_lr: 0.000001  loss: 4.3000 (4.5257)  class_acc: 0.2083 (0.1706)  loss_scale: 32768.0000 (35943.8618)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.0811  max mem: 15572
[2025-01-16 21:55:22,710] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:55:22,710] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:55:22,775] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:55:22,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [1340/1404]  eta: 0:00:37  lr: 0.000093  min_lr: 0.000001  loss: 4.2719 (4.5250)  class_acc: 0.2083 (0.1708)  loss_scale: 32768.0000 (35993.4855)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0958  max mem: 15572
[2025-01-16 21:55:25,805] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9768
[2025-01-16 21:55:25,806] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:55:25,854] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9768
[2025-01-16 21:55:25,854] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:55:25,854] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [1350/1404]  eta: 0:00:31  lr: 0.000093  min_lr: 0.000001  loss: 4.3737 (4.5247)  class_acc: 0.1667 (0.1711)  loss_scale: 32768.0000 (36042.3745)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0617  max mem: 15572
Epoch: [6]  [1360/1404]  eta: 0:00:25  lr: 0.000093  min_lr: 0.000001  loss: 4.4720 (4.5256)  class_acc: 0.1667 (0.1709)  loss_scale: 32768.0000 (36018.3159)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0111  max mem: 15572
Epoch: [6]  [1370/1404]  eta: 0:00:19  lr: 0.000093  min_lr: 0.000001  loss: 4.5776 (4.5252)  class_acc: 0.1667 (0.1714)  loss_scale: 32768.0000 (35994.6083)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0296  max mem: 15572
Epoch: [6]  [1380/1404]  eta: 0:00:14  lr: 0.000093  min_lr: 0.000001  loss: 4.5613 (4.5256)  class_acc: 0.1667 (0.1714)  loss_scale: 32768.0000 (35971.2440)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0732  max mem: 15572
Epoch: [6]  [1390/1404]  eta: 0:00:08  lr: 0.000093  min_lr: 0.000001  loss: 4.5233 (4.5255)  class_acc: 0.1667 (0.1712)  loss_scale: 32768.0000 (35948.2157)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1152  max mem: 15572
Epoch: [6]  [1400/1404]  eta: 0:00:02  lr: 0.000093  min_lr: 0.000001  loss: 4.5038 (4.5253)  class_acc: 0.1250 (0.1714)  loss_scale: 32768.0000 (35925.5161)  weight_decay: 0.0500 (0.0500)  time: 0.4863  data: 0.0609  max mem: 15572
Epoch: [6]  [1403/1404]  eta: 0:00:00  lr: 0.000093  min_lr: 0.000001  loss: 4.5217 (4.5254)  class_acc: 0.2083 (0.1714)  loss_scale: 32768.0000 (35918.7692)  weight_decay: 0.0500 (0.0500)  time: 0.4667  data: 0.0608  max mem: 15572
Epoch: [6] Total time: 0:13:43 (0.5867 s / it)
Averaged stats: lr: 0.000093  min_lr: 0.000001  loss: 4.5217 (4.5256)  class_acc: 0.2083 (0.1731)  loss_scale: 32768.0000 (35918.7692)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:11:22  loss: 2.4148 (2.4148)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 5.0183  data: 4.8332  max mem: 15572
Val:  [ 10/136]  eta: 0:01:45  loss: 3.5212 (3.3852)  acc1: 16.6667 (24.2424)  acc5: 55.5556 (50.0000)  time: 0.8352  data: 0.6339  max mem: 15572
Val:  [ 20/136]  eta: 0:01:11  loss: 3.4987 (3.4472)  acc1: 16.6667 (20.6349)  acc5: 50.0000 (52.6455)  time: 0.3920  data: 0.1939  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 3.2684 (3.2770)  acc1: 16.6667 (28.3154)  acc5: 61.1111 (57.5269)  time: 0.2800  data: 0.0873  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 2.6906 (3.1731)  acc1: 44.4444 (30.4878)  acc5: 83.3333 (61.3821)  time: 0.2753  data: 0.0765  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 3.1429 (3.2547)  acc1: 22.2222 (29.1939)  acc5: 66.6667 (60.4575)  time: 0.3751  data: 0.1771  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 3.5850 (3.3420)  acc1: 16.6667 (27.1403)  acc5: 50.0000 (57.7413)  time: 0.3624  data: 0.1673  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 3.5763 (3.3258)  acc1: 22.2222 (28.3255)  acc5: 50.0000 (57.8247)  time: 0.3230  data: 0.1136  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 3.1488 (3.3100)  acc1: 33.3333 (28.3951)  acc5: 66.6667 (59.1907)  time: 0.3281  data: 0.1140  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 3.1975 (3.3172)  acc1: 27.7778 (28.2662)  acc5: 66.6667 (59.5238)  time: 0.3500  data: 0.1519  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.4961 (3.3563)  acc1: 16.6667 (26.8427)  acc5: 55.5556 (58.0858)  time: 0.3608  data: 0.1745  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.3720 (3.3521)  acc1: 22.2222 (27.7277)  acc5: 55.5556 (58.6086)  time: 0.3963  data: 0.2121  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 3.0229 (3.3113)  acc1: 33.3333 (29.6143)  acc5: 77.7778 (60.8356)  time: 0.3714  data: 0.1770  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.9682 (3.2948)  acc1: 50.0000 (30.9160)  acc5: 77.7778 (61.1111)  time: 0.2598  data: 0.0765  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 3.0847 (3.3048)  acc1: 27.7778 (30.4259)  acc5: 66.6667 (60.8108)  time: 0.2260  data: 0.0607  max mem: 15572
Val: Total time: 0:00:49 (0.3654 s / it)
* Acc@1 30.016 Acc@5 59.132 loss 3.331
Accuracy of the network on the 4883 val videos: 30.0%
[2025-01-16 21:56:46,488] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 21:56:46,490] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 21:56:46,490] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 21:56:46,490] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 21:56:48,920] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 21:56:48,921] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 30.02%
Epoch: [7]  [   0/1404]  eta: 3:04:11  lr: 0.000093  min_lr: 0.000001  loss: 4.3475 (4.3475)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.8712  data: 5.2409  max mem: 15572
Epoch: [7]  [  10/1404]  eta: 0:27:07  lr: 0.000093  min_lr: 0.000001  loss: 4.3475 (4.4065)  class_acc: 0.2083 (0.2045)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1676  data: 0.4768  max mem: 15572
Epoch: [7]  [  20/1404]  eta: 0:20:55  lr: 0.000093  min_lr: 0.000001  loss: 4.3411 (4.3839)  class_acc: 0.1667 (0.2004)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0007  max mem: 15572
Epoch: [7]  [  30/1404]  eta: 0:17:57  lr: 0.000093  min_lr: 0.000001  loss: 4.3411 (4.3718)  class_acc: 0.1667 (0.1935)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0009  max mem: 15572
Epoch: [7]  [  40/1404]  eta: 0:16:55  lr: 0.000093  min_lr: 0.000001  loss: 4.4207 (4.4115)  class_acc: 0.1667 (0.1911)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.0007  max mem: 15572
Epoch: [7]  [  50/1404]  eta: 0:16:19  lr: 0.000093  min_lr: 0.000001  loss: 4.5173 (4.4440)  class_acc: 0.2083 (0.1969)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.0005  max mem: 15572
Epoch: [7]  [  60/1404]  eta: 0:15:22  lr: 0.000093  min_lr: 0.000001  loss: 4.5176 (4.4442)  class_acc: 0.2083 (0.1974)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0008  max mem: 15572
[2025-01-16 21:57:35,110] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:57:35,111] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:57:35,112] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:57:35,112] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [  70/1404]  eta: 0:14:41  lr: 0.000093  min_lr: 0.000001  loss: 4.4176 (4.4368)  class_acc: 0.2083 (0.2031)  loss_scale: 32768.0000 (33691.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0007  max mem: 15572
[2025-01-16 21:57:38,380] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9902
[2025-01-16 21:57:38,380] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:57:38,380] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:57:38,380] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9902
[2025-01-16 21:57:38,381] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [  80/1404]  eta: 0:14:20  lr: 0.000093  min_lr: 0.000001  loss: 4.4624 (4.4513)  class_acc: 0.2083 (0.2011)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0005  max mem: 15572
Epoch: [7]  [  90/1404]  eta: 0:14:08  lr: 0.000093  min_lr: 0.000001  loss: 4.5453 (4.4573)  class_acc: 0.1667 (0.2005)  loss_scale: 32768.0000 (34568.4396)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0008  max mem: 15572
Epoch: [7]  [ 100/1404]  eta: 0:13:53  lr: 0.000093  min_lr: 0.000001  loss: 4.4710 (4.4641)  class_acc: 0.1667 (0.1972)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0424  max mem: 15572
Epoch: [7]  [ 110/1404]  eta: 0:13:40  lr: 0.000093  min_lr: 0.000001  loss: 4.5045 (4.4698)  class_acc: 0.1667 (0.1959)  loss_scale: 32768.0000 (34244.0360)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0660  max mem: 15572
Epoch: [7]  [ 120/1404]  eta: 0:13:31  lr: 0.000093  min_lr: 0.000001  loss: 4.5453 (4.4785)  class_acc: 0.1250 (0.1932)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0794  max mem: 15572
Epoch: [7]  [ 130/1404]  eta: 0:13:27  lr: 0.000093  min_lr: 0.000001  loss: 4.5453 (4.4733)  class_acc: 0.1667 (0.1966)  loss_scale: 32768.0000 (34018.6870)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1229  max mem: 15572
Epoch: [7]  [ 140/1404]  eta: 0:13:15  lr: 0.000093  min_lr: 0.000001  loss: 4.4477 (4.4777)  class_acc: 0.1667 (0.1947)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1010  max mem: 15572
Epoch: [7]  [ 150/1404]  eta: 0:13:03  lr: 0.000093  min_lr: 0.000001  loss: 4.4833 (4.4752)  class_acc: 0.1667 (0.1951)  loss_scale: 32768.0000 (33853.0331)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0640  max mem: 15572
Epoch: [7]  [ 160/1404]  eta: 0:12:57  lr: 0.000093  min_lr: 0.000001  loss: 4.4874 (4.4791)  class_acc: 0.1667 (0.1946)  loss_scale: 32768.0000 (33785.6398)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0892  max mem: 15572
Epoch: [7]  [ 170/1404]  eta: 0:12:46  lr: 0.000093  min_lr: 0.000001  loss: 4.4607 (4.4735)  class_acc: 0.1667 (0.1969)  loss_scale: 32768.0000 (33726.1287)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0707  max mem: 15572
[2025-01-16 21:58:35,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=56, lr=[9.001307943001361e-07, 9.001307943001361e-07, 1.2859011347144805e-06, 1.2859011347144805e-06, 1.8370016210206865e-06, 1.8370016210206865e-06, 2.624288030029552e-06, 2.624288030029552e-06, 3.7489829000422176e-06, 3.7489829000422176e-06, 5.355689857203168e-06, 5.355689857203168e-06, 7.650985510290241e-06, 7.650985510290241e-06, 1.092997930041463e-05, 1.092997930041463e-05, 1.561425614344947e-05, 1.561425614344947e-05, 2.230608020492782e-05, 2.230608020492782e-05, 3.18658288641826e-05, 3.18658288641826e-05, 4.552261266311801e-05, 4.552261266311801e-05, 6.50323038044543e-05, 6.50323038044543e-05, 9.290329114922043e-05, 9.290329114922043e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 21:58:35,625] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=48.05190912082207, CurrSamplesPerSec=56.01535836747232, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [ 180/1404]  eta: 0:12:38  lr: 0.000093  min_lr: 0.000001  loss: 4.4128 (4.4735)  class_acc: 0.2083 (0.1998)  loss_scale: 32768.0000 (33673.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0290  max mem: 15572
Epoch: [7]  [ 190/1404]  eta: 0:12:29  lr: 0.000093  min_lr: 0.000001  loss: 4.5292 (4.4786)  class_acc: 0.2083 (0.1979)  loss_scale: 32768.0000 (33625.8010)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.0354  max mem: 15572
Epoch: [7]  [ 200/1404]  eta: 0:12:21  lr: 0.000093  min_lr: 0.000001  loss: 4.5292 (4.4797)  class_acc: 0.1667 (0.1957)  loss_scale: 32768.0000 (33583.1244)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0559  max mem: 15572
[2025-01-16 21:58:54,182] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:58:54,182] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 21:58:54,189] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 21:58:54,189] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 210/1404]  eta: 0:12:10  lr: 0.000093  min_lr: 0.000001  loss: 4.4878 (4.4802)  class_acc: 0.1667 (0.1947)  loss_scale: 32768.0000 (34786.8815)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0528  max mem: 15572
[2025-01-16 21:58:58,624] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10039
[2025-01-16 21:58:58,625] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 21:58:58,625] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 21:58:58,644] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10039
[2025-01-16 21:58:58,645] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [ 220/1404]  eta: 0:12:05  lr: 0.000093  min_lr: 0.000001  loss: 4.4368 (4.4775)  class_acc: 0.1667 (0.1957)  loss_scale: 32768.0000 (34695.5294)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0714  max mem: 15572
Epoch: [7]  [ 230/1404]  eta: 0:11:54  lr: 0.000093  min_lr: 0.000001  loss: 4.4155 (4.4793)  class_acc: 0.0833 (0.1944)  loss_scale: 32768.0000 (34612.0866)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0616  max mem: 15572
Epoch: [7]  [ 240/1404]  eta: 0:11:49  lr: 0.000093  min_lr: 0.000001  loss: 4.5150 (4.4806)  class_acc: 0.0833 (0.1945)  loss_scale: 32768.0000 (34535.5685)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0836  max mem: 15572
Epoch: [7]  [ 250/1404]  eta: 0:11:44  lr: 0.000093  min_lr: 0.000001  loss: 4.5460 (4.4832)  class_acc: 0.2083 (0.1957)  loss_scale: 32768.0000 (34465.1474)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1485  max mem: 15572
Epoch: [7]  [ 260/1404]  eta: 0:11:37  lr: 0.000093  min_lr: 0.000001  loss: 4.4047 (4.4801)  class_acc: 0.2083 (0.1956)  loss_scale: 32768.0000 (34400.1226)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1098  max mem: 15572
Epoch: [7]  [ 270/1404]  eta: 0:11:31  lr: 0.000093  min_lr: 0.000001  loss: 4.3815 (4.4790)  class_acc: 0.1667 (0.1957)  loss_scale: 32768.0000 (34339.8967)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0962  max mem: 15572
Epoch: [7]  [ 280/1404]  eta: 0:11:20  lr: 0.000093  min_lr: 0.000001  loss: 4.5201 (4.4800)  class_acc: 0.2083 (0.1965)  loss_scale: 32768.0000 (34283.9573)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0646  max mem: 15572
Epoch: [7]  [ 290/1404]  eta: 0:11:18  lr: 0.000093  min_lr: 0.000001  loss: 4.4685 (4.4792)  class_acc: 0.1667 (0.1956)  loss_scale: 32768.0000 (34231.8625)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0956  max mem: 15572
Epoch: [7]  [ 300/1404]  eta: 0:11:10  lr: 0.000093  min_lr: 0.000001  loss: 4.4649 (4.4783)  class_acc: 0.1667 (0.1949)  loss_scale: 32768.0000 (34183.2292)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1201  max mem: 15572
Epoch: [7]  [ 310/1404]  eta: 0:11:05  lr: 0.000093  min_lr: 0.000001  loss: 4.5069 (4.4802)  class_acc: 0.1667 (0.1947)  loss_scale: 32768.0000 (34137.7235)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1103  max mem: 15572
Epoch: [7]  [ 320/1404]  eta: 0:10:57  lr: 0.000093  min_lr: 0.000001  loss: 4.5088 (4.4773)  class_acc: 0.1667 (0.1946)  loss_scale: 32768.0000 (34095.0530)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.0929  max mem: 15572
Epoch: [7]  [ 330/1404]  eta: 0:10:48  lr: 0.000093  min_lr: 0.000001  loss: 4.4870 (4.4759)  class_acc: 0.1667 (0.1964)  loss_scale: 32768.0000 (34054.9607)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0356  max mem: 15572
[2025-01-16 22:00:14,979] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:00:14,980] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:00:15,016] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:00:15,016] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 340/1404]  eta: 0:10:42  lr: 0.000093  min_lr: 0.000001  loss: 4.4163 (4.4759)  class_acc: 0.1667 (0.1951)  loss_scale: 32768.0000 (34113.3138)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0790  max mem: 15572
[2025-01-16 22:00:18,935] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10174
[2025-01-16 22:00:18,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:00:18,936] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:00:18,949] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10174
[2025-01-16 22:00:18,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [ 350/1404]  eta: 0:10:36  lr: 0.000093  min_lr: 0.000001  loss: 4.4754 (4.4775)  class_acc: 0.1667 (0.1949)  loss_scale: 32768.0000 (34541.7664)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.0971  max mem: 15572
Epoch: [7]  [ 360/1404]  eta: 0:10:30  lr: 0.000093  min_lr: 0.000001  loss: 4.4754 (4.4767)  class_acc: 0.1667 (0.1948)  loss_scale: 32768.0000 (34492.6316)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0954  max mem: 15572
Epoch: [7]  [ 370/1404]  eta: 0:10:23  lr: 0.000093  min_lr: 0.000001  loss: 4.5430 (4.4807)  class_acc: 0.0833 (0.1924)  loss_scale: 32768.0000 (34446.1456)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0989  max mem: 15572
Epoch: [7]  [ 380/1404]  eta: 0:10:15  lr: 0.000093  min_lr: 0.000001  loss: 4.5430 (4.4819)  class_acc: 0.1250 (0.1918)  loss_scale: 32768.0000 (34402.0997)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0428  max mem: 15572
Epoch: [7]  [ 390/1404]  eta: 0:10:09  lr: 0.000093  min_lr: 0.000001  loss: 4.4919 (4.4812)  class_acc: 0.1667 (0.1917)  loss_scale: 32768.0000 (34360.3069)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0006  max mem: 15572
Epoch: [7]  [ 400/1404]  eta: 0:10:01  lr: 0.000093  min_lr: 0.000001  loss: 4.4171 (4.4797)  class_acc: 0.1667 (0.1926)  loss_scale: 32768.0000 (34320.5985)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0006  max mem: 15572
Epoch: [7]  [ 410/1404]  eta: 0:09:56  lr: 0.000093  min_lr: 0.000001  loss: 4.3516 (4.4787)  class_acc: 0.1667 (0.1935)  loss_scale: 32768.0000 (34282.8224)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.0005  max mem: 15572
Epoch: [7]  [ 420/1404]  eta: 0:09:50  lr: 0.000093  min_lr: 0.000001  loss: 4.4906 (4.4797)  class_acc: 0.1667 (0.1924)  loss_scale: 32768.0000 (34246.8409)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.0006  max mem: 15572
Epoch: [7]  [ 430/1404]  eta: 0:09:42  lr: 0.000093  min_lr: 0.000001  loss: 4.5650 (4.4833)  class_acc: 0.1250 (0.1917)  loss_scale: 32768.0000 (34212.5290)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0041  max mem: 15572
Epoch: [7]  [ 440/1404]  eta: 0:09:38  lr: 0.000093  min_lr: 0.000001  loss: 4.5436 (4.4829)  class_acc: 0.1667 (0.1916)  loss_scale: 32768.0000 (34179.7732)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0181  max mem: 15572
Epoch: [7]  [ 450/1404]  eta: 0:09:30  lr: 0.000093  min_lr: 0.000001  loss: 4.4640 (4.4817)  class_acc: 0.1667 (0.1916)  loss_scale: 32768.0000 (34148.4701)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0172  max mem: 15572
Epoch: [7]  [ 460/1404]  eta: 0:09:23  lr: 0.000093  min_lr: 0.000001  loss: 4.4975 (4.4847)  class_acc: 0.1667 (0.1922)  loss_scale: 32768.0000 (34118.5249)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0137  max mem: 15572
Epoch: [7]  [ 470/1404]  eta: 0:09:18  lr: 0.000093  min_lr: 0.000001  loss: 4.4975 (4.4844)  class_acc: 0.1667 (0.1920)  loss_scale: 32768.0000 (34089.8514)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0498  max mem: 15572
[2025-01-16 22:01:33,481] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:01:33,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:01:33,486] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:01:33,487] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:01:33,943] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10304
[2025-01-16 22:01:33,944] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:01:33,944] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:01:33,984] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10304
[2025-01-16 22:01:33,985] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [ 480/1404]  eta: 0:09:14  lr: 0.000093  min_lr: 0.000001  loss: 4.4695 (4.4844)  class_acc: 0.1250 (0.1916)  loss_scale: 32768.0000 (34130.4948)  weight_decay: 0.0500 (0.0500)  time: 0.6633  data: 0.1199  max mem: 15572
Epoch: [7]  [ 490/1404]  eta: 0:09:06  lr: 0.000093  min_lr: 0.000001  loss: 4.3021 (4.4797)  class_acc: 0.2083 (0.1935)  loss_scale: 32768.0000 (34102.7454)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0960  max mem: 15572
Epoch: [7]  [ 500/1404]  eta: 0:09:01  lr: 0.000093  min_lr: 0.000001  loss: 4.4621 (4.4802)  class_acc: 0.2917 (0.1941)  loss_scale: 32768.0000 (34076.1038)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0724  max mem: 15572
Epoch: [7]  [ 510/1404]  eta: 0:08:54  lr: 0.000093  min_lr: 0.000001  loss: 4.4847 (4.4807)  class_acc: 0.2083 (0.1937)  loss_scale: 32768.0000 (34050.5049)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1045  max mem: 15572
Epoch: [7]  [ 520/1404]  eta: 0:08:48  lr: 0.000093  min_lr: 0.000001  loss: 4.5172 (4.4820)  class_acc: 0.1250 (0.1923)  loss_scale: 32768.0000 (34025.8887)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.0905  max mem: 15572
Epoch: [7]  [ 530/1404]  eta: 0:08:42  lr: 0.000093  min_lr: 0.000001  loss: 4.4788 (4.4806)  class_acc: 0.1667 (0.1925)  loss_scale: 32768.0000 (34002.1996)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0436  max mem: 15572
Epoch: [7]  [ 540/1404]  eta: 0:08:35  lr: 0.000093  min_lr: 0.000001  loss: 4.4489 (4.4800)  class_acc: 0.2083 (0.1928)  loss_scale: 32768.0000 (33979.3863)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0006  max mem: 15572
Epoch: [7]  [ 550/1404]  eta: 0:08:28  lr: 0.000093  min_lr: 0.000001  loss: 4.5080 (4.4778)  class_acc: 0.2083 (0.1944)  loss_scale: 32768.0000 (33957.4011)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0006  max mem: 15572
Epoch: [7]  [ 560/1404]  eta: 0:08:22  lr: 0.000093  min_lr: 0.000001  loss: 4.3859 (4.4769)  class_acc: 0.1667 (0.1930)  loss_scale: 32768.0000 (33936.1996)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0005  max mem: 15572
Epoch: [7]  [ 570/1404]  eta: 0:08:17  lr: 0.000093  min_lr: 0.000001  loss: 4.3964 (4.4755)  class_acc: 0.1250 (0.1928)  loss_scale: 32768.0000 (33915.7408)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.0006  max mem: 15572
Epoch: [7]  [ 580/1404]  eta: 0:08:11  lr: 0.000093  min_lr: 0.000001  loss: 4.3428 (4.4721)  class_acc: 0.1667 (0.1929)  loss_scale: 32768.0000 (33895.9862)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.0656  max mem: 15572
Epoch: [7]  [ 590/1404]  eta: 0:08:05  lr: 0.000093  min_lr: 0.000001  loss: 4.3369 (4.4703)  class_acc: 0.2083 (0.1932)  loss_scale: 32768.0000 (33876.9002)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0781  max mem: 15572
Epoch: [7]  [ 600/1404]  eta: 0:07:58  lr: 0.000093  min_lr: 0.000001  loss: 4.3616 (4.4705)  class_acc: 0.2083 (0.1929)  loss_scale: 32768.0000 (33858.4493)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0302  max mem: 15572
[2025-01-16 22:02:49,562] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:02:49,563] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:02:49,570] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:02:49,571] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:02:49,998] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10434
[2025-01-16 22:02:49,998] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:02:49,999] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10434
[2025-01-16 22:02:50,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:02:50,000] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 610/1404]  eta: 0:07:51  lr: 0.000093  min_lr: 0.000001  loss: 4.4383 (4.4697)  class_acc: 0.2083 (0.1931)  loss_scale: 32768.0000 (33894.2324)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0590  max mem: 15572
Epoch: [7]  [ 620/1404]  eta: 0:07:46  lr: 0.000093  min_lr: 0.000001  loss: 4.4395 (4.4718)  class_acc: 0.1667 (0.1925)  loss_scale: 32768.0000 (33876.0966)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1002  max mem: 15572
Epoch: [7]  [ 630/1404]  eta: 0:07:40  lr: 0.000093  min_lr: 0.000001  loss: 4.5340 (4.4742)  class_acc: 0.1667 (0.1924)  loss_scale: 32768.0000 (33858.5357)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1168  max mem: 15572
Epoch: [7]  [ 640/1404]  eta: 0:07:34  lr: 0.000093  min_lr: 0.000001  loss: 4.5340 (4.4743)  class_acc: 0.2083 (0.1930)  loss_scale: 32768.0000 (33841.5226)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.0819  max mem: 15572
Epoch: [7]  [ 650/1404]  eta: 0:07:27  lr: 0.000093  min_lr: 0.000001  loss: 4.5159 (4.4762)  class_acc: 0.2083 (0.1935)  loss_scale: 32768.0000 (33825.0323)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0240  max mem: 15572
Epoch: [7]  [ 660/1404]  eta: 0:07:21  lr: 0.000093  min_lr: 0.000001  loss: 4.4313 (4.4744)  class_acc: 0.1667 (0.1932)  loss_scale: 32768.0000 (33809.0408)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0007  max mem: 15572
Epoch: [7]  [ 670/1404]  eta: 0:07:15  lr: 0.000093  min_lr: 0.000001  loss: 4.3578 (4.4737)  class_acc: 0.2083 (0.1937)  loss_scale: 32768.0000 (33793.5261)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0007  max mem: 15572
Epoch: [7]  [ 680/1404]  eta: 0:07:10  lr: 0.000093  min_lr: 0.000001  loss: 4.4892 (4.4758)  class_acc: 0.2083 (0.1938)  loss_scale: 32768.0000 (33778.4670)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0006  max mem: 15572
Epoch: [7]  [ 690/1404]  eta: 0:07:03  lr: 0.000093  min_lr: 0.000001  loss: 4.4811 (4.4750)  class_acc: 0.1667 (0.1931)  loss_scale: 32768.0000 (33763.8437)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0005  max mem: 15572
Epoch: [7]  [ 700/1404]  eta: 0:06:57  lr: 0.000093  min_lr: 0.000001  loss: 4.4385 (4.4752)  class_acc: 0.1667 (0.1929)  loss_scale: 32768.0000 (33749.6377)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0006  max mem: 15572
Epoch: [7]  [ 710/1404]  eta: 0:06:52  lr: 0.000093  min_lr: 0.000001  loss: 4.3879 (4.4740)  class_acc: 0.1667 (0.1930)  loss_scale: 32768.0000 (33735.8312)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0006  max mem: 15572
Epoch: [7]  [ 720/1404]  eta: 0:06:46  lr: 0.000093  min_lr: 0.000001  loss: 4.3954 (4.4737)  class_acc: 0.1667 (0.1931)  loss_scale: 32768.0000 (33722.4078)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.0005  max mem: 15572
Epoch: [7]  [ 730/1404]  eta: 0:06:40  lr: 0.000093  min_lr: 0.000001  loss: 4.5129 (4.4741)  class_acc: 0.2083 (0.1937)  loss_scale: 32768.0000 (33709.3516)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0012  max mem: 15572
[2025-01-16 22:04:05,629] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:04:05,630] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:04:05,631] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:04:05,631] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 740/1404]  eta: 0:06:34  lr: 0.000093  min_lr: 0.000001  loss: 4.4086 (4.4715)  class_acc: 0.2083 (0.1939)  loss_scale: 32768.0000 (33961.9757)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0523  max mem: 15572
[2025-01-16 22:04:13,058] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10576
[2025-01-16 22:04:13,059] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:04:13,059] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:04:13,085] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10576
[2025-01-16 22:04:13,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [ 750/1404]  eta: 0:06:29  lr: 0.000093  min_lr: 0.000001  loss: 4.4236 (4.4738)  class_acc: 0.1667 (0.1941)  loss_scale: 65536.0000 (34251.5047)  weight_decay: 0.0500 (0.0500)  time: 0.6580  data: 0.1626  max mem: 15572
Epoch: [7]  [ 760/1404]  eta: 0:06:22  lr: 0.000093  min_lr: 0.000001  loss: 4.5672 (4.4737)  class_acc: 0.2083 (0.1939)  loss_scale: 32768.0000 (34232.0105)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1223  max mem: 15572
Epoch: [7]  [ 770/1404]  eta: 0:06:16  lr: 0.000093  min_lr: 0.000001  loss: 4.4233 (4.4728)  class_acc: 0.1667 (0.1936)  loss_scale: 32768.0000 (34213.0220)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0460  max mem: 15572
Epoch: [7]  [ 780/1404]  eta: 0:06:10  lr: 0.000093  min_lr: 0.000001  loss: 4.5654 (4.4745)  class_acc: 0.1250 (0.1928)  loss_scale: 32768.0000 (34194.5198)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0948  max mem: 15572
Epoch: [7]  [ 790/1404]  eta: 0:06:05  lr: 0.000093  min_lr: 0.000001  loss: 4.5595 (4.4732)  class_acc: 0.1250 (0.1927)  loss_scale: 32768.0000 (34176.4855)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.1373  max mem: 15572
Epoch: [7]  [ 800/1404]  eta: 0:05:58  lr: 0.000093  min_lr: 0.000001  loss: 4.4545 (4.4740)  class_acc: 0.1667 (0.1927)  loss_scale: 32768.0000 (34158.9014)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0974  max mem: 15572
Epoch: [7]  [ 810/1404]  eta: 0:05:52  lr: 0.000093  min_lr: 0.000001  loss: 4.5478 (4.4748)  class_acc: 0.1667 (0.1928)  loss_scale: 32768.0000 (34141.7509)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0575  max mem: 15572
Epoch: [7]  [ 820/1404]  eta: 0:05:46  lr: 0.000092  min_lr: 0.000001  loss: 4.4801 (4.4734)  class_acc: 0.2083 (0.1938)  loss_scale: 32768.0000 (34125.0183)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0860  max mem: 15572
Epoch: [7]  [ 830/1404]  eta: 0:05:40  lr: 0.000092  min_lr: 0.000001  loss: 4.2949 (4.4708)  class_acc: 0.2500 (0.1943)  loss_scale: 32768.0000 (34108.6883)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0823  max mem: 15572
Epoch: [7]  [ 840/1404]  eta: 0:05:34  lr: 0.000092  min_lr: 0.000001  loss: 4.3128 (4.4701)  class_acc: 0.1667 (0.1945)  loss_scale: 32768.0000 (34092.7467)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0342  max mem: 15572
Epoch: [7]  [ 850/1404]  eta: 0:05:28  lr: 0.000092  min_lr: 0.000001  loss: 4.4013 (4.4691)  class_acc: 0.1667 (0.1944)  loss_scale: 32768.0000 (34077.1798)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0747  max mem: 15572
Epoch: [7]  [ 860/1404]  eta: 0:05:22  lr: 0.000092  min_lr: 0.000001  loss: 4.4502 (4.4695)  class_acc: 0.1667 (0.1943)  loss_scale: 32768.0000 (34061.9744)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1145  max mem: 15572
Epoch: [7]  [ 870/1404]  eta: 0:05:16  lr: 0.000092  min_lr: 0.000001  loss: 4.5364 (4.4706)  class_acc: 0.2083 (0.1945)  loss_scale: 32768.0000 (34047.1183)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0499  max mem: 15572
[2025-01-16 22:05:28,853] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:05:28,853] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:05:28,853] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:05:28,853] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 880/1404]  eta: 0:05:10  lr: 0.000092  min_lr: 0.000001  loss: 4.5419 (4.4701)  class_acc: 0.2083 (0.1946)  loss_scale: 32768.0000 (34181.3757)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0679  max mem: 15572
[2025-01-16 22:05:32,627] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10710
[2025-01-16 22:05:32,627] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:05:32,627] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:05:32,650] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10710
[2025-01-16 22:05:32,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [ 890/1404]  eta: 0:05:04  lr: 0.000092  min_lr: 0.000001  loss: 4.4631 (4.4707)  class_acc: 0.2083 (0.1949)  loss_scale: 32768.0000 (34202.2896)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1064  max mem: 15572
Epoch: [7]  [ 900/1404]  eta: 0:04:59  lr: 0.000092  min_lr: 0.000001  loss: 4.4669 (4.4713)  class_acc: 0.1667 (0.1948)  loss_scale: 32768.0000 (34186.3707)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.1363  max mem: 15572
Epoch: [7]  [ 910/1404]  eta: 0:04:52  lr: 0.000092  min_lr: 0.000001  loss: 4.4724 (4.4714)  class_acc: 0.2083 (0.1958)  loss_scale: 32768.0000 (34170.8013)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0883  max mem: 15572
Epoch: [7]  [ 920/1404]  eta: 0:04:46  lr: 0.000092  min_lr: 0.000001  loss: 4.5605 (4.4722)  class_acc: 0.2083 (0.1957)  loss_scale: 32768.0000 (34155.5700)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0370  max mem: 15572
Epoch: [7]  [ 930/1404]  eta: 0:04:40  lr: 0.000092  min_lr: 0.000001  loss: 4.5528 (4.4725)  class_acc: 0.1667 (0.1958)  loss_scale: 32768.0000 (34140.6660)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0987  max mem: 15572
Epoch: [7]  [ 940/1404]  eta: 0:04:34  lr: 0.000092  min_lr: 0.000001  loss: 4.4310 (4.4711)  class_acc: 0.2083 (0.1962)  loss_scale: 32768.0000 (34126.0786)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0951  max mem: 15572
Epoch: [7]  [ 950/1404]  eta: 0:04:28  lr: 0.000092  min_lr: 0.000001  loss: 4.3502 (4.4714)  class_acc: 0.2500 (0.1965)  loss_scale: 32768.0000 (34111.7981)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0334  max mem: 15572
Epoch: [7]  [ 960/1404]  eta: 0:04:22  lr: 0.000092  min_lr: 0.000001  loss: 4.3754 (4.4702)  class_acc: 0.2500 (0.1965)  loss_scale: 32768.0000 (34097.8148)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0662  max mem: 15572
Epoch: [7]  [ 970/1404]  eta: 0:04:16  lr: 0.000092  min_lr: 0.000001  loss: 4.3447 (4.4697)  class_acc: 0.2083 (0.1967)  loss_scale: 32768.0000 (34084.1195)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0822  max mem: 15572
Epoch: [7]  [ 980/1404]  eta: 0:04:10  lr: 0.000092  min_lr: 0.000001  loss: 4.4936 (4.4698)  class_acc: 0.1667 (0.1964)  loss_scale: 32768.0000 (34070.7034)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0506  max mem: 15572
Epoch: [7]  [ 990/1404]  eta: 0:04:04  lr: 0.000092  min_lr: 0.000001  loss: 4.4963 (4.4685)  class_acc: 0.1667 (0.1961)  loss_scale: 32768.0000 (34057.5580)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0679  max mem: 15572
Epoch: [7]  [1000/1404]  eta: 0:03:58  lr: 0.000092  min_lr: 0.000001  loss: 4.4128 (4.4668)  class_acc: 0.1667 (0.1968)  loss_scale: 32768.0000 (34044.6753)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0757  max mem: 15572
Epoch: [7]  [1010/1404]  eta: 0:03:52  lr: 0.000092  min_lr: 0.000001  loss: 4.4735 (4.4682)  class_acc: 0.1667 (0.1965)  loss_scale: 32768.0000 (34032.0475)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0686  max mem: 15572
[2025-01-16 22:06:47,342] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:06:47,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:06:47,342] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:06:47,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:06:47,770] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10840
[2025-01-16 22:06:47,771] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:06:47,813] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10840
[2025-01-16 22:06:47,814] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:06:47,815] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [1020/1404]  eta: 0:03:46  lr: 0.000092  min_lr: 0.000001  loss: 4.5567 (4.4678)  class_acc: 0.1667 (0.1964)  loss_scale: 32768.0000 (34051.7610)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0755  max mem: 15572
Epoch: [7]  [1030/1404]  eta: 0:03:40  lr: 0.000092  min_lr: 0.000001  loss: 4.4348 (4.4673)  class_acc: 0.1667 (0.1961)  loss_scale: 32768.0000 (34039.3094)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0831  max mem: 15572
Epoch: [7]  [1040/1404]  eta: 0:03:35  lr: 0.000092  min_lr: 0.000001  loss: 4.5182 (4.4679)  class_acc: 0.1667 (0.1962)  loss_scale: 32768.0000 (34027.0970)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0784  max mem: 15572
Epoch: [7]  [1050/1404]  eta: 0:03:29  lr: 0.000092  min_lr: 0.000001  loss: 4.5167 (4.4684)  class_acc: 0.1667 (0.1959)  loss_scale: 32768.0000 (34015.1170)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0960  max mem: 15572
Epoch: [7]  [1060/1404]  eta: 0:03:23  lr: 0.000092  min_lr: 0.000001  loss: 4.4514 (4.4677)  class_acc: 0.1667 (0.1960)  loss_scale: 32768.0000 (34003.3629)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0637  max mem: 15572
Epoch: [7]  [1070/1404]  eta: 0:03:17  lr: 0.000092  min_lr: 0.000001  loss: 4.3303 (4.4674)  class_acc: 0.2083 (0.1958)  loss_scale: 32768.0000 (33991.8282)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0474  max mem: 15572
Epoch: [7]  [1080/1404]  eta: 0:03:11  lr: 0.000092  min_lr: 0.000001  loss: 4.4129 (4.4676)  class_acc: 0.1667 (0.1956)  loss_scale: 32768.0000 (33980.5069)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0787  max mem: 15572
Epoch: [7]  [1090/1404]  eta: 0:03:05  lr: 0.000092  min_lr: 0.000001  loss: 4.3130 (4.4662)  class_acc: 0.1667 (0.1957)  loss_scale: 32768.0000 (33969.3932)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0681  max mem: 15572
Epoch: [7]  [1100/1404]  eta: 0:02:59  lr: 0.000092  min_lr: 0.000001  loss: 4.2510 (4.4658)  class_acc: 0.1667 (0.1952)  loss_scale: 32768.0000 (33958.4814)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0376  max mem: 15572
Epoch: [7]  [1110/1404]  eta: 0:02:53  lr: 0.000092  min_lr: 0.000001  loss: 4.3561 (4.4647)  class_acc: 0.1667 (0.1955)  loss_scale: 32768.0000 (33947.7660)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.0710  max mem: 15572
Epoch: [7]  [1120/1404]  eta: 0:02:47  lr: 0.000092  min_lr: 0.000001  loss: 4.3758 (4.4644)  class_acc: 0.2083 (0.1959)  loss_scale: 32768.0000 (33937.2417)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0821  max mem: 15572
Epoch: [7]  [1130/1404]  eta: 0:02:41  lr: 0.000092  min_lr: 0.000001  loss: 4.3805 (4.4631)  class_acc: 0.2500 (0.1965)  loss_scale: 32768.0000 (33926.9036)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0240  max mem: 15572
Epoch: [7]  [1140/1404]  eta: 0:02:35  lr: 0.000092  min_lr: 0.000001  loss: 4.4313 (4.4643)  class_acc: 0.2083 (0.1967)  loss_scale: 32768.0000 (33916.7467)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0005  max mem: 15572
[2025-01-16 22:08:03,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:08:03,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:08:03,576] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:08:03,576] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:08:06,599] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10975
[2025-01-16 22:08:06,600] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:08:06,600] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10975
[2025-01-16 22:08:06,600] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:08:06,600] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [1150/1404]  eta: 0:02:29  lr: 0.000092  min_lr: 0.000001  loss: 4.5157 (4.4642)  class_acc: 0.1667 (0.1965)  loss_scale: 32768.0000 (34077.5812)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.0004  max mem: 15572
Epoch: [7]  [1160/1404]  eta: 0:02:24  lr: 0.000092  min_lr: 0.000001  loss: 4.4323 (4.4634)  class_acc: 0.1667 (0.1969)  loss_scale: 32768.0000 (34066.3015)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.0005  max mem: 15572
Epoch: [7]  [1170/1404]  eta: 0:02:17  lr: 0.000092  min_lr: 0.000001  loss: 4.4499 (4.4639)  class_acc: 0.2083 (0.1971)  loss_scale: 32768.0000 (34055.2143)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0007  max mem: 15572
[2025-01-16 22:08:20,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=64, lr=[8.93733379008233e-07, 8.93733379008233e-07, 1.2767619700117616e-06, 1.2767619700117616e-06, 1.823945671445374e-06, 1.823945671445374e-06, 2.6056366734933912e-06, 2.6056366734933912e-06, 3.7223381049905594e-06, 3.7223381049905594e-06, 5.317625864272228e-06, 5.317625864272228e-06, 7.596608377531754e-06, 7.596608377531754e-06, 1.0852297682188223e-05, 1.0852297682188223e-05, 1.550328240312603e-05, 1.550328240312603e-05, 2.2147546290180047e-05, 2.2147546290180047e-05, 3.163935184311435e-05, 3.163935184311435e-05, 4.5199074061591934e-05, 4.5199074061591934e-05, 6.45701058022742e-05, 6.45701058022742e-05, 9.224300828896315e-05, 9.224300828896315e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 22:08:20,361] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=48.08834889781069, CurrSamplesPerSec=52.14323431830366, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [1180/1404]  eta: 0:02:11  lr: 0.000092  min_lr: 0.000001  loss: 4.5552 (4.4652)  class_acc: 0.1667 (0.1965)  loss_scale: 32768.0000 (34044.3150)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0008  max mem: 15572
Epoch: [7]  [1190/1404]  eta: 0:02:06  lr: 0.000092  min_lr: 0.000001  loss: 4.5692 (4.4658)  class_acc: 0.1667 (0.1962)  loss_scale: 32768.0000 (34033.5987)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0006  max mem: 15572
Epoch: [7]  [1200/1404]  eta: 0:02:00  lr: 0.000092  min_lr: 0.000001  loss: 4.5383 (4.4660)  class_acc: 0.1667 (0.1964)  loss_scale: 32768.0000 (34023.0608)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0056  max mem: 15572
Epoch: [7]  [1210/1404]  eta: 0:01:54  lr: 0.000092  min_lr: 0.000001  loss: 4.4436 (4.4653)  class_acc: 0.2083 (0.1969)  loss_scale: 32768.0000 (34012.6969)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0351  max mem: 15572
Epoch: [7]  [1220/1404]  eta: 0:01:48  lr: 0.000092  min_lr: 0.000001  loss: 4.3779 (4.4658)  class_acc: 0.1667 (0.1965)  loss_scale: 32768.0000 (34002.5029)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.0691  max mem: 15572
Epoch: [7]  [1230/1404]  eta: 0:01:42  lr: 0.000092  min_lr: 0.000001  loss: 4.3735 (4.4650)  class_acc: 0.1667 (0.1965)  loss_scale: 32768.0000 (33992.4744)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0616  max mem: 15572
Epoch: [7]  [1240/1404]  eta: 0:01:36  lr: 0.000092  min_lr: 0.000001  loss: 4.4558 (4.4660)  class_acc: 0.1667 (0.1966)  loss_scale: 32768.0000 (33982.6076)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0536  max mem: 15572
Epoch: [7]  [1250/1404]  eta: 0:01:30  lr: 0.000092  min_lr: 0.000001  loss: 4.4578 (4.4656)  class_acc: 0.2083 (0.1970)  loss_scale: 32768.0000 (33972.8985)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0555  max mem: 15572
Epoch: [7]  [1260/1404]  eta: 0:01:24  lr: 0.000092  min_lr: 0.000001  loss: 4.3747 (4.4652)  class_acc: 0.2083 (0.1967)  loss_scale: 32768.0000 (33963.3434)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.0972  max mem: 15572
Epoch: [7]  [1270/1404]  eta: 0:01:19  lr: 0.000092  min_lr: 0.000001  loss: 4.4405 (4.4657)  class_acc: 0.1250 (0.1961)  loss_scale: 32768.0000 (33953.9386)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.0732  max mem: 15572
[2025-01-16 22:09:22,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:09:22,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:09:22,662] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:09:22,663] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:09:24,781] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11108
[2025-01-16 22:09:24,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:09:24,880] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11108
[2025-01-16 22:09:24,880] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:09:24,881] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [1280/1404]  eta: 0:01:13  lr: 0.000092  min_lr: 0.000001  loss: 4.5198 (4.4649)  class_acc: 0.1667 (0.1965)  loss_scale: 32768.0000 (34047.0008)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0006  max mem: 15572
Epoch: [7]  [1290/1404]  eta: 0:01:07  lr: 0.000092  min_lr: 0.000001  loss: 4.5198 (4.4658)  class_acc: 0.2500 (0.1970)  loss_scale: 32768.0000 (34037.0937)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.0006  max mem: 15572
Epoch: [7]  [1300/1404]  eta: 0:01:01  lr: 0.000092  min_lr: 0.000001  loss: 4.5917 (4.4658)  class_acc: 0.2083 (0.1971)  loss_scale: 32768.0000 (34027.3390)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0010  max mem: 15572
Epoch: [7]  [1310/1404]  eta: 0:00:55  lr: 0.000092  min_lr: 0.000001  loss: 4.5042 (4.4659)  class_acc: 0.1667 (0.1971)  loss_scale: 32768.0000 (34017.7330)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0009  max mem: 15572
Epoch: [7]  [1320/1404]  eta: 0:00:49  lr: 0.000092  min_lr: 0.000001  loss: 4.4964 (4.4662)  class_acc: 0.1667 (0.1970)  loss_scale: 32768.0000 (34008.2725)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0005  max mem: 15572
Epoch: [7]  [1330/1404]  eta: 0:00:43  lr: 0.000092  min_lr: 0.000001  loss: 4.4803 (4.4667)  class_acc: 0.2083 (0.1974)  loss_scale: 32768.0000 (33998.9542)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0005  max mem: 15572
Epoch: [7]  [1340/1404]  eta: 0:00:37  lr: 0.000092  min_lr: 0.000001  loss: 4.4879 (4.4666)  class_acc: 0.2500 (0.1976)  loss_scale: 32768.0000 (33989.7748)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.0006  max mem: 15572
Epoch: [7]  [1350/1404]  eta: 0:00:31  lr: 0.000092  min_lr: 0.000001  loss: 4.4672 (4.4667)  class_acc: 0.2083 (0.1978)  loss_scale: 32768.0000 (33980.7313)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0006  max mem: 15572
Epoch: [7]  [1360/1404]  eta: 0:00:25  lr: 0.000092  min_lr: 0.000001  loss: 4.4672 (4.4664)  class_acc: 0.1667 (0.1979)  loss_scale: 32768.0000 (33971.8207)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0007  max mem: 15572
Epoch: [7]  [1370/1404]  eta: 0:00:20  lr: 0.000092  min_lr: 0.000001  loss: 4.4961 (4.4663)  class_acc: 0.1667 (0.1978)  loss_scale: 32768.0000 (33963.0401)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.0009  max mem: 15572
Epoch: [7]  [1380/1404]  eta: 0:00:14  lr: 0.000092  min_lr: 0.000001  loss: 4.5181 (4.4670)  class_acc: 0.2083 (0.1977)  loss_scale: 32768.0000 (33954.3867)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0009  max mem: 15572
Epoch: [7]  [1390/1404]  eta: 0:00:08  lr: 0.000092  min_lr: 0.000001  loss: 4.5456 (4.4672)  class_acc: 0.1667 (0.1976)  loss_scale: 32768.0000 (33945.8577)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0007  max mem: 15572
Epoch: [7]  [1400/1404]  eta: 0:00:02  lr: 0.000092  min_lr: 0.000001  loss: 4.4751 (4.4670)  class_acc: 0.2083 (0.1977)  loss_scale: 32768.0000 (33937.4504)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0004  max mem: 15572
Epoch: [7]  [1403/1404]  eta: 0:00:00  lr: 0.000092  min_lr: 0.000001  loss: 4.4743 (4.4672)  class_acc: 0.2083 (0.1975)  loss_scale: 32768.0000 (33934.9516)  weight_decay: 0.0500 (0.0500)  time: 0.4851  data: 0.0004  max mem: 15572
Epoch: [7] Total time: 0:13:44 (0.5875 s / it)
Averaged stats: lr: 0.000092  min_lr: 0.000001  loss: 4.4743 (4.4715)  class_acc: 0.2083 (0.1975)  loss_scale: 32768.0000 (33934.9516)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:11:28  loss: 2.3525 (2.3525)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 5.0651  data: 4.8436  max mem: 15572
Val:  [ 10/136]  eta: 0:01:42  loss: 3.2219 (3.1957)  acc1: 27.7778 (28.2828)  acc5: 66.6667 (60.1010)  time: 0.8171  data: 0.6242  max mem: 15572
Val:  [ 20/136]  eta: 0:01:05  loss: 3.2457 (3.2692)  acc1: 22.2222 (24.8677)  acc5: 61.1111 (58.4656)  time: 0.3374  data: 0.1453  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 3.1377 (3.1142)  acc1: 22.2222 (31.5412)  acc5: 61.1111 (61.4695)  time: 0.2895  data: 0.0904  max mem: 15572
Val:  [ 40/136]  eta: 0:00:41  loss: 2.5563 (3.0271)  acc1: 50.0000 (34.4173)  acc5: 72.2222 (65.3117)  time: 0.2853  data: 0.0841  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 3.0766 (3.1022)  acc1: 38.8889 (33.7691)  acc5: 72.2222 (63.9434)  time: 0.3443  data: 0.1343  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 3.4673 (3.1861)  acc1: 22.2222 (31.2386)  acc5: 50.0000 (61.3843)  time: 0.3906  data: 0.1763  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 3.3478 (3.1757)  acc1: 27.7778 (32.0814)  acc5: 55.5556 (61.5023)  time: 0.3831  data: 0.1807  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.9914 (3.1532)  acc1: 33.3333 (31.8930)  acc5: 66.6667 (63.0316)  time: 0.3793  data: 0.1731  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 3.0668 (3.1636)  acc1: 27.7778 (31.2576)  acc5: 66.6667 (63.0647)  time: 0.3386  data: 0.1311  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.2609 (3.2151)  acc1: 16.6667 (29.6480)  acc5: 61.1111 (61.2211)  time: 0.3416  data: 0.1422  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.2875 (3.2204)  acc1: 22.2222 (29.7297)  acc5: 55.5556 (61.0110)  time: 0.3576  data: 0.1607  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 3.0205 (3.1805)  acc1: 33.3333 (31.9559)  acc5: 66.6667 (62.6263)  time: 0.3717  data: 0.1802  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.6704 (3.1532)  acc1: 55.5556 (33.5030)  acc5: 72.2222 (63.0195)  time: 0.2934  data: 0.1267  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.9985 (3.1631)  acc1: 33.3333 (33.2514)  acc5: 66.6667 (62.6945)  time: 0.1786  data: 0.0241  max mem: 15572
Val: Total time: 0:00:49 (0.3672 s / it)
* Acc@1 32.965 Acc@5 61.937 loss 3.190
Accuracy of the network on the 4883 val videos: 33.0%
[2025-01-16 22:11:23,769] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 22:11:23,771] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 22:11:23,771] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 22:11:23,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 22:11:26,157] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 22:11:26,157] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 32.96%
Epoch: [8]  [   0/1404]  eta: 2:21:40  lr: 0.000092  min_lr: 0.000001  loss: 4.9230 (4.9230)  class_acc: 0.0417 (0.0417)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.0542  data: 5.5898  max mem: 15572
[2025-01-16 22:11:36,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:11:36,266] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:11:36,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:11:36,267] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:11:37,421] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11239
[2025-01-16 22:11:37,421] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:11:37,421] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:11:37,443] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11239
[2025-01-16 22:11:37,444] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [  10/1404]  eta: 0:26:47  lr: 0.000092  min_lr: 0.000001  loss: 4.4804 (4.5123)  class_acc: 0.2083 (0.2121)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 1.1530  data: 0.5435  max mem: 15572
Epoch: [8]  [  20/1404]  eta: 0:19:42  lr: 0.000092  min_lr: 0.000001  loss: 4.4357 (4.4534)  class_acc: 0.1667 (0.2063)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0196  max mem: 15572
Epoch: [8]  [  30/1404]  eta: 0:17:37  lr: 0.000092  min_lr: 0.000001  loss: 4.4349 (4.4412)  class_acc: 0.1667 (0.2043)  loss_scale: 32768.0000 (34882.0645)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0069  max mem: 15572
Epoch: [8]  [  40/1404]  eta: 0:16:42  lr: 0.000092  min_lr: 0.000001  loss: 4.4674 (4.4542)  class_acc: 0.1667 (0.1972)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.0070  max mem: 15572
Epoch: [8]  [  50/1404]  eta: 0:15:38  lr: 0.000092  min_lr: 0.000001  loss: 4.4674 (4.4311)  class_acc: 0.1667 (0.1953)  loss_scale: 32768.0000 (34053.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0005  max mem: 15572
Epoch: [8]  [  60/1404]  eta: 0:15:18  lr: 0.000092  min_lr: 0.000001  loss: 4.2743 (4.4108)  class_acc: 0.2083 (0.1995)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0005  max mem: 15572
Epoch: [8]  [  70/1404]  eta: 0:14:42  lr: 0.000092  min_lr: 0.000001  loss: 4.3925 (4.4255)  class_acc: 0.2083 (0.2019)  loss_scale: 32768.0000 (33691.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0006  max mem: 15572
Epoch: [8]  [  80/1404]  eta: 0:14:20  lr: 0.000092  min_lr: 0.000001  loss: 4.5365 (4.4428)  class_acc: 0.2083 (0.1960)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0154  max mem: 15572
Epoch: [8]  [  90/1404]  eta: 0:14:14  lr: 0.000092  min_lr: 0.000001  loss: 4.4472 (4.4342)  class_acc: 0.2083 (0.2005)  loss_scale: 32768.0000 (33488.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0401  max mem: 15572
Epoch: [8]  [ 100/1404]  eta: 0:14:03  lr: 0.000092  min_lr: 0.000001  loss: 4.4117 (4.4404)  class_acc: 0.2083 (0.1984)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.0855  max mem: 15572
Epoch: [8]  [ 110/1404]  eta: 0:13:42  lr: 0.000092  min_lr: 0.000001  loss: 4.3828 (4.4347)  class_acc: 0.1667 (0.1959)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0652  max mem: 15572
Epoch: [8]  [ 120/1404]  eta: 0:13:34  lr: 0.000092  min_lr: 0.000001  loss: 4.3831 (4.4344)  class_acc: 0.1667 (0.1959)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0691  max mem: 15572
Epoch: [8]  [ 130/1404]  eta: 0:13:19  lr: 0.000092  min_lr: 0.000001  loss: 4.3414 (4.4286)  class_acc: 0.2083 (0.1991)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0901  max mem: 15572
[2025-01-16 22:12:52,711] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:12:52,711] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:12:52,732] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:12:52,732] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 140/1404]  eta: 0:13:13  lr: 0.000092  min_lr: 0.000001  loss: 4.3128 (4.4282)  class_acc: 0.1667 (0.1956)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0950  max mem: 15572
[2025-01-16 22:12:55,623] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11374
[2025-01-16 22:12:55,623] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:12:55,623] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:12:55,624] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11374
[2025-01-16 22:12:55,625] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [ 150/1404]  eta: 0:13:09  lr: 0.000092  min_lr: 0.000001  loss: 4.4609 (4.4340)  class_acc: 0.1667 (0.1984)  loss_scale: 32768.0000 (34504.0530)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.1626  max mem: 15572
Epoch: [8]  [ 160/1404]  eta: 0:12:55  lr: 0.000092  min_lr: 0.000001  loss: 4.5234 (4.4391)  class_acc: 0.2500 (0.2026)  loss_scale: 32768.0000 (34396.2236)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1253  max mem: 15572
Epoch: [8]  [ 170/1404]  eta: 0:12:46  lr: 0.000092  min_lr: 0.000001  loss: 4.4990 (4.4390)  class_acc: 0.2500 (0.2052)  loss_scale: 32768.0000 (34301.0058)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0714  max mem: 15572
Epoch: [8]  [ 180/1404]  eta: 0:12:42  lr: 0.000092  min_lr: 0.000001  loss: 4.4331 (4.4386)  class_acc: 0.2083 (0.2047)  loss_scale: 32768.0000 (34216.3094)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.0398  max mem: 15572
Epoch: [8]  [ 190/1404]  eta: 0:12:27  lr: 0.000092  min_lr: 0.000001  loss: 4.5965 (4.4468)  class_acc: 0.1250 (0.2031)  loss_scale: 32768.0000 (34140.4817)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0007  max mem: 15572
Epoch: [8]  [ 200/1404]  eta: 0:12:15  lr: 0.000092  min_lr: 0.000001  loss: 4.5432 (4.4439)  class_acc: 0.1667 (0.2044)  loss_scale: 32768.0000 (34072.1990)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0006  max mem: 15572
Epoch: [8]  [ 210/1404]  eta: 0:12:09  lr: 0.000092  min_lr: 0.000001  loss: 4.3909 (4.4466)  class_acc: 0.1667 (0.2032)  loss_scale: 32768.0000 (34010.3886)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0006  max mem: 15572
Epoch: [8]  [ 220/1404]  eta: 0:11:59  lr: 0.000092  min_lr: 0.000001  loss: 4.4174 (4.4434)  class_acc: 0.1250 (0.1998)  loss_scale: 32768.0000 (33954.1719)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0006  max mem: 15572
Epoch: [8]  [ 230/1404]  eta: 0:11:53  lr: 0.000092  min_lr: 0.000001  loss: 4.4688 (4.4457)  class_acc: 0.1667 (0.2000)  loss_scale: 32768.0000 (33902.8225)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0006  max mem: 15572
Epoch: [8]  [ 240/1404]  eta: 0:11:49  lr: 0.000092  min_lr: 0.000001  loss: 4.5033 (4.4455)  class_acc: 0.1667 (0.1985)  loss_scale: 32768.0000 (33855.7344)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0005  max mem: 15572
Epoch: [8]  [ 250/1404]  eta: 0:11:42  lr: 0.000092  min_lr: 0.000001  loss: 4.5169 (4.4506)  class_acc: 0.1667 (0.1974)  loss_scale: 32768.0000 (33812.3984)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.0004  max mem: 15572
Epoch: [8]  [ 260/1404]  eta: 0:11:33  lr: 0.000092  min_lr: 0.000001  loss: 4.5169 (4.4522)  class_acc: 0.1667 (0.1965)  loss_scale: 32768.0000 (33772.3831)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0005  max mem: 15572
Epoch: [8]  [ 270/1404]  eta: 0:11:28  lr: 0.000092  min_lr: 0.000001  loss: 4.6102 (4.4616)  class_acc: 0.1250 (0.1942)  loss_scale: 32768.0000 (33735.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.0005  max mem: 15572
[2025-01-16 22:14:12,637] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:14:12,637] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:14:12,637] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:14:12,637] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:14:13,102] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11504
[2025-01-16 22:14:13,102] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:14:13,102] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:14:13,173] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11504
[2025-01-16 22:14:13,173] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [ 280/1404]  eta: 0:11:22  lr: 0.000092  min_lr: 0.000001  loss: 4.5182 (4.4584)  class_acc: 0.1667 (0.1937)  loss_scale: 32768.0000 (33817.5089)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.0007  max mem: 15572
Epoch: [8]  [ 290/1404]  eta: 0:11:16  lr: 0.000092  min_lr: 0.000001  loss: 4.3050 (4.4561)  class_acc: 0.2083 (0.1946)  loss_scale: 32768.0000 (33781.4433)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.0006  max mem: 15572
Epoch: [8]  [ 300/1404]  eta: 0:11:08  lr: 0.000092  min_lr: 0.000001  loss: 4.3427 (4.4566)  class_acc: 0.2083 (0.1959)  loss_scale: 32768.0000 (33747.7741)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0007  max mem: 15572
Epoch: [8]  [ 310/1404]  eta: 0:11:01  lr: 0.000092  min_lr: 0.000001  loss: 4.3310 (4.4554)  class_acc: 0.2083 (0.1959)  loss_scale: 32768.0000 (33716.2701)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0008  max mem: 15572
Epoch: [8]  [ 320/1404]  eta: 0:10:57  lr: 0.000092  min_lr: 0.000001  loss: 4.3789 (4.4553)  class_acc: 0.1250 (0.1952)  loss_scale: 32768.0000 (33686.7290)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.0008  max mem: 15572
Epoch: [8]  [ 330/1404]  eta: 0:10:49  lr: 0.000092  min_lr: 0.000001  loss: 4.3789 (4.4537)  class_acc: 0.1667 (0.1957)  loss_scale: 32768.0000 (33658.9728)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0009  max mem: 15572
Epoch: [8]  [ 340/1404]  eta: 0:10:44  lr: 0.000092  min_lr: 0.000001  loss: 4.5164 (4.4574)  class_acc: 0.2083 (0.1962)  loss_scale: 32768.0000 (33632.8446)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0008  max mem: 15572
Epoch: [8]  [ 350/1404]  eta: 0:10:36  lr: 0.000092  min_lr: 0.000001  loss: 4.4646 (4.4527)  class_acc: 0.2083 (0.1980)  loss_scale: 32768.0000 (33608.2051)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0006  max mem: 15572
Epoch: [8]  [ 360/1404]  eta: 0:10:30  lr: 0.000092  min_lr: 0.000001  loss: 4.3634 (4.4545)  class_acc: 0.1667 (0.1969)  loss_scale: 32768.0000 (33584.9307)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0005  max mem: 15572
Epoch: [8]  [ 370/1404]  eta: 0:10:22  lr: 0.000092  min_lr: 0.000001  loss: 4.5215 (4.4527)  class_acc: 0.1250 (0.1967)  loss_scale: 32768.0000 (33562.9111)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0005  max mem: 15572
Epoch: [8]  [ 380/1404]  eta: 0:10:16  lr: 0.000092  min_lr: 0.000001  loss: 4.3715 (4.4510)  class_acc: 0.1667 (0.1964)  loss_scale: 32768.0000 (33542.0472)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0009  max mem: 15572
Epoch: [8]  [ 390/1404]  eta: 0:10:10  lr: 0.000092  min_lr: 0.000001  loss: 4.4387 (4.4517)  class_acc: 0.1667 (0.1960)  loss_scale: 32768.0000 (33522.2506)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0009  max mem: 15572
Epoch: [8]  [ 400/1404]  eta: 0:10:01  lr: 0.000092  min_lr: 0.000001  loss: 4.4988 (4.4501)  class_acc: 0.2083 (0.1967)  loss_scale: 32768.0000 (33503.4414)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0005  max mem: 15572
[2025-01-16 22:15:27,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:15:27,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:15:27,135] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:15:27,135] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:15:28,768] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11636
[2025-01-16 22:15:28,768] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:15:28,768] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:15:28,772] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11636
[2025-01-16 22:15:28,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [ 410/1404]  eta: 0:09:53  lr: 0.000092  min_lr: 0.000001  loss: 4.3822 (4.4501)  class_acc: 0.2083 (0.1971)  loss_scale: 32768.0000 (33724.7299)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0010  max mem: 15572
Epoch: [8]  [ 420/1404]  eta: 0:09:48  lr: 0.000092  min_lr: 0.000001  loss: 4.5296 (4.4528)  class_acc: 0.2083 (0.1978)  loss_scale: 32768.0000 (33702.0048)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0015  max mem: 15572
Epoch: [8]  [ 430/1404]  eta: 0:09:43  lr: 0.000092  min_lr: 0.000001  loss: 4.4853 (4.4511)  class_acc: 0.2500 (0.1994)  loss_scale: 32768.0000 (33680.3341)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.0884  max mem: 15572
Epoch: [8]  [ 440/1404]  eta: 0:09:37  lr: 0.000092  min_lr: 0.000001  loss: 4.2884 (4.4510)  class_acc: 0.2500 (0.2010)  loss_scale: 32768.0000 (33659.6463)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1202  max mem: 15572
[2025-01-16 22:15:51,565] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11674
[2025-01-16 22:15:51,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:15:51,566] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 22:15:51,574] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11674
[2025-01-16 22:15:51,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [8]  [ 450/1404]  eta: 0:09:31  lr: 0.000092  min_lr: 0.000001  loss: 4.3638 (4.4509)  class_acc: 0.2083 (0.2012)  loss_scale: 32768.0000 (33312.9224)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0874  max mem: 15572
Epoch: [8]  [ 460/1404]  eta: 0:09:23  lr: 0.000092  min_lr: 0.000001  loss: 4.4017 (4.4501)  class_acc: 0.2083 (0.2016)  loss_scale: 16384.0000 (32945.7007)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0551  max mem: 15572
Epoch: [8]  [ 470/1404]  eta: 0:09:17  lr: 0.000092  min_lr: 0.000001  loss: 4.4811 (4.4524)  class_acc: 0.2083 (0.2017)  loss_scale: 16384.0000 (32594.0722)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0263  max mem: 15572
Epoch: [8]  [ 480/1404]  eta: 0:09:11  lr: 0.000092  min_lr: 0.000001  loss: 4.4811 (4.4509)  class_acc: 0.1667 (0.2011)  loss_scale: 16384.0000 (32257.0644)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0814  max mem: 15572
Epoch: [8]  [ 490/1404]  eta: 0:09:04  lr: 0.000092  min_lr: 0.000001  loss: 4.3332 (4.4477)  class_acc: 0.2083 (0.2024)  loss_scale: 16384.0000 (31933.7841)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0817  max mem: 15572
Epoch: [8]  [ 500/1404]  eta: 0:08:58  lr: 0.000092  min_lr: 0.000001  loss: 4.4444 (4.4484)  class_acc: 0.2083 (0.2024)  loss_scale: 16384.0000 (31623.4092)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.0733  max mem: 15572
Epoch: [8]  [ 510/1404]  eta: 0:08:53  lr: 0.000092  min_lr: 0.000001  loss: 4.5332 (4.4510)  class_acc: 0.2083 (0.2037)  loss_scale: 16384.0000 (31325.1820)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1082  max mem: 15572
Epoch: [8]  [ 520/1404]  eta: 0:08:46  lr: 0.000092  min_lr: 0.000001  loss: 4.5882 (4.4522)  class_acc: 0.2083 (0.2031)  loss_scale: 16384.0000 (31038.4031)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0856  max mem: 15572
Epoch: [8]  [ 530/1404]  eta: 0:08:40  lr: 0.000092  min_lr: 0.000001  loss: 4.3979 (4.4520)  class_acc: 0.1667 (0.2029)  loss_scale: 16384.0000 (30762.4256)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0575  max mem: 15572
Epoch: [8]  [ 540/1404]  eta: 0:08:33  lr: 0.000092  min_lr: 0.000001  loss: 4.3807 (4.4522)  class_acc: 0.2083 (0.2032)  loss_scale: 16384.0000 (30496.6506)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0530  max mem: 15572
Epoch: [8]  [ 550/1404]  eta: 0:08:27  lr: 0.000092  min_lr: 0.000001  loss: 4.4250 (4.4511)  class_acc: 0.2083 (0.2036)  loss_scale: 16384.0000 (30240.5227)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0635  max mem: 15572
Epoch: [8]  [ 560/1404]  eta: 0:08:22  lr: 0.000092  min_lr: 0.000001  loss: 4.3715 (4.4494)  class_acc: 0.2500 (0.2050)  loss_scale: 16384.0000 (29993.5258)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1263  max mem: 15572
Epoch: [8]  [ 570/1404]  eta: 0:08:17  lr: 0.000092  min_lr: 0.000001  loss: 4.3715 (4.4500)  class_acc: 0.2083 (0.2045)  loss_scale: 16384.0000 (29755.1804)  weight_decay: 0.0500 (0.0500)  time: 0.6499  data: 0.1307  max mem: 15572
[2025-01-16 22:17:07,317] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:17:07,318] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:17:07,325] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:17:07,325] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [8]  [ 580/1404]  eta: 0:08:10  lr: 0.000092  min_lr: 0.000001  loss: 4.4123 (4.4492)  class_acc: 0.1667 (0.2042)  loss_scale: 16384.0000 (29807.0361)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0917  max mem: 15572
Epoch: [8]  [ 590/1404]  eta: 0:08:04  lr: 0.000092  min_lr: 0.000001  loss: 4.3907 (4.4485)  class_acc: 0.1667 (0.2040)  loss_scale: 32768.0000 (29857.1371)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.0764  max mem: 15572
Epoch: [8]  [ 600/1404]  eta: 0:07:58  lr: 0.000092  min_lr: 0.000001  loss: 4.3437 (4.4460)  class_acc: 0.1667 (0.2049)  loss_scale: 32768.0000 (29905.5707)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0538  max mem: 15572
Epoch: [8]  [ 610/1404]  eta: 0:07:52  lr: 0.000092  min_lr: 0.000001  loss: 4.3837 (4.4458)  class_acc: 0.2083 (0.2053)  loss_scale: 32768.0000 (29952.4190)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0726  max mem: 15572
Epoch: [8]  [ 620/1404]  eta: 0:07:45  lr: 0.000092  min_lr: 0.000001  loss: 4.5091 (4.4471)  class_acc: 0.1667 (0.2046)  loss_scale: 32768.0000 (29997.7585)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0517  max mem: 15572
Epoch: [8]  [ 630/1404]  eta: 0:07:39  lr: 0.000092  min_lr: 0.000001  loss: 4.5091 (4.4474)  class_acc: 0.2083 (0.2056)  loss_scale: 32768.0000 (30041.6609)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0611  max mem: 15572
Epoch: [8]  [ 640/1404]  eta: 0:07:33  lr: 0.000092  min_lr: 0.000001  loss: 4.4099 (4.4469)  class_acc: 0.2083 (0.2055)  loss_scale: 32768.0000 (30084.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0923  max mem: 15572
Epoch: [8]  [ 650/1404]  eta: 0:07:27  lr: 0.000092  min_lr: 0.000001  loss: 4.4264 (4.4473)  class_acc: 0.1667 (0.2051)  loss_scale: 32768.0000 (30125.4194)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0767  max mem: 15572
Epoch: [8]  [ 660/1404]  eta: 0:07:21  lr: 0.000091  min_lr: 0.000001  loss: 4.3443 (4.4460)  class_acc: 0.2083 (0.2052)  loss_scale: 32768.0000 (30165.3979)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0860  max mem: 15572
Epoch: [8]  [ 670/1404]  eta: 0:07:15  lr: 0.000091  min_lr: 0.000001  loss: 4.3443 (4.4479)  class_acc: 0.2083 (0.2051)  loss_scale: 32768.0000 (30204.1848)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0897  max mem: 15572
Epoch: [8]  [ 680/1404]  eta: 0:07:10  lr: 0.000091  min_lr: 0.000001  loss: 4.4237 (4.4466)  class_acc: 0.2083 (0.2061)  loss_scale: 32768.0000 (30241.8326)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0983  max mem: 15572
Epoch: [8]  [ 690/1404]  eta: 0:07:04  lr: 0.000091  min_lr: 0.000001  loss: 4.4180 (4.4471)  class_acc: 0.1667 (0.2057)  loss_scale: 32768.0000 (30278.3907)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0886  max mem: 15572
[2025-01-16 22:18:22,452] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:18:22,452] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:18:22,505] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:18:22,505] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:18:22,917] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11932
[2025-01-16 22:18:22,917] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11932
[2025-01-16 22:18:22,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:18:22,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:18:22,917] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 700/1404]  eta: 0:06:58  lr: 0.000091  min_lr: 0.000001  loss: 4.6130 (4.4492)  class_acc: 0.1667 (0.2059)  loss_scale: 32768.0000 (30360.6505)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.0941  max mem: 15572
Epoch: [8]  [ 710/1404]  eta: 0:06:52  lr: 0.000091  min_lr: 0.000001  loss: 4.6369 (4.4506)  class_acc: 0.2083 (0.2055)  loss_scale: 32768.0000 (30394.5091)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1231  max mem: 15572
Epoch: [8]  [ 720/1404]  eta: 0:06:46  lr: 0.000091  min_lr: 0.000001  loss: 4.5197 (4.4513)  class_acc: 0.2083 (0.2053)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1224  max mem: 15572
Epoch: [8]  [ 730/1404]  eta: 0:06:40  lr: 0.000091  min_lr: 0.000001  loss: 4.4172 (4.4508)  class_acc: 0.2083 (0.2061)  loss_scale: 32768.0000 (30459.4473)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.0965  max mem: 15572
Epoch: [8]  [ 740/1404]  eta: 0:06:34  lr: 0.000091  min_lr: 0.000001  loss: 4.3695 (4.4500)  class_acc: 0.2083 (0.2058)  loss_scale: 32768.0000 (30490.6019)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0705  max mem: 15572
Epoch: [8]  [ 750/1404]  eta: 0:06:27  lr: 0.000091  min_lr: 0.000001  loss: 4.3695 (4.4494)  class_acc: 0.1667 (0.2054)  loss_scale: 32768.0000 (30520.9268)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0470  max mem: 15572
Epoch: [8]  [ 760/1404]  eta: 0:06:21  lr: 0.000091  min_lr: 0.000001  loss: 4.3493 (4.4460)  class_acc: 0.2083 (0.2062)  loss_scale: 32768.0000 (30550.4547)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0867  max mem: 15572
[2025-01-16 22:19:02,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=71, lr=[8.855418217882891e-07, 8.855418217882891e-07, 1.2650597454118418e-06, 1.2650597454118418e-06, 1.8072282077312026e-06, 1.8072282077312026e-06, 2.581754582473147e-06, 2.581754582473147e-06, 3.6882208321044958e-06, 3.6882208321044958e-06, 5.268886903006422e-06, 5.268886903006422e-06, 7.526981290009176e-06, 7.526981290009176e-06, 1.0752830414298824e-05, 1.0752830414298824e-05, 1.5361186306141176e-05, 1.5361186306141176e-05, 2.194455186591597e-05, 2.194455186591597e-05, 3.1349359808451386e-05, 3.1349359808451386e-05, 4.478479972635913e-05, 4.478479972635913e-05, 6.397828532337019e-05, 6.397828532337019e-05, 9.139755046195741e-05, 9.139755046195741e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 22:19:02,512] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=47.999695954211504, CurrSamplesPerSec=43.322798941625884, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [ 770/1404]  eta: 0:06:16  lr: 0.000091  min_lr: 0.000001  loss: 4.3193 (4.4453)  class_acc: 0.2083 (0.2063)  loss_scale: 32768.0000 (30579.2166)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1398  max mem: 15572
Epoch: [8]  [ 780/1404]  eta: 0:06:10  lr: 0.000091  min_lr: 0.000001  loss: 4.4242 (4.4468)  class_acc: 0.1667 (0.2066)  loss_scale: 32768.0000 (30607.2420)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1235  max mem: 15572
Epoch: [8]  [ 790/1404]  eta: 0:06:04  lr: 0.000091  min_lr: 0.000001  loss: 4.4913 (4.4473)  class_acc: 0.1667 (0.2063)  loss_scale: 32768.0000 (30634.5588)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0521  max mem: 15572
Epoch: [8]  [ 800/1404]  eta: 0:05:57  lr: 0.000091  min_lr: 0.000001  loss: 4.4776 (4.4474)  class_acc: 0.2083 (0.2062)  loss_scale: 32768.0000 (30661.1935)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0007  max mem: 15572
Epoch: [8]  [ 810/1404]  eta: 0:05:51  lr: 0.000091  min_lr: 0.000001  loss: 4.5020 (4.4493)  class_acc: 0.2083 (0.2055)  loss_scale: 32768.0000 (30687.1714)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0662  max mem: 15572
Epoch: [8]  [ 820/1404]  eta: 0:05:46  lr: 0.000091  min_lr: 0.000001  loss: 4.6429 (4.4505)  class_acc: 0.2083 (0.2062)  loss_scale: 32768.0000 (30712.5164)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1253  max mem: 15572
[2025-01-16 22:19:38,730] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12061
[2025-01-16 22:19:38,731] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:19:38,743] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12061
[2025-01-16 22:19:38,744] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:19:38,744] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [8]  [ 830/1404]  eta: 0:05:40  lr: 0.000091  min_lr: 0.000001  loss: 4.4535 (4.4507)  class_acc: 0.2083 (0.2064)  loss_scale: 32768.0000 (30697.8195)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1225  max mem: 15572
Epoch: [8]  [ 840/1404]  eta: 0:05:34  lr: 0.000091  min_lr: 0.000001  loss: 4.4429 (4.4510)  class_acc: 0.2083 (0.2063)  loss_scale: 16384.0000 (30527.6195)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1044  max mem: 15572
Epoch: [8]  [ 850/1404]  eta: 0:05:27  lr: 0.000091  min_lr: 0.000001  loss: 4.4556 (4.4509)  class_acc: 0.2083 (0.2064)  loss_scale: 16384.0000 (30361.4195)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0633  max mem: 15572
Epoch: [8]  [ 860/1404]  eta: 0:05:21  lr: 0.000091  min_lr: 0.000001  loss: 4.4111 (4.4501)  class_acc: 0.1667 (0.2060)  loss_scale: 16384.0000 (30199.0801)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0224  max mem: 15572
Epoch: [8]  [ 870/1404]  eta: 0:05:15  lr: 0.000091  min_lr: 0.000001  loss: 4.3387 (4.4481)  class_acc: 0.1250 (0.2058)  loss_scale: 16384.0000 (30040.4684)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.0405  max mem: 15572
Epoch: [8]  [ 880/1404]  eta: 0:05:09  lr: 0.000091  min_lr: 0.000001  loss: 4.2844 (4.4470)  class_acc: 0.1667 (0.2060)  loss_scale: 16384.0000 (29885.4574)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1041  max mem: 15572
Epoch: [8]  [ 890/1404]  eta: 0:05:03  lr: 0.000091  min_lr: 0.000001  loss: 4.3885 (4.4469)  class_acc: 0.2083 (0.2065)  loss_scale: 16384.0000 (29733.9259)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1201  max mem: 15572
Epoch: [8]  [ 900/1404]  eta: 0:04:57  lr: 0.000091  min_lr: 0.000001  loss: 4.4045 (4.4467)  class_acc: 0.2083 (0.2068)  loss_scale: 16384.0000 (29585.7580)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0565  max mem: 15572
Epoch: [8]  [ 910/1404]  eta: 0:04:51  lr: 0.000091  min_lr: 0.000001  loss: 4.4095 (4.4450)  class_acc: 0.2083 (0.2072)  loss_scale: 16384.0000 (29440.8430)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0005  max mem: 15572
Epoch: [8]  [ 920/1404]  eta: 0:04:45  lr: 0.000091  min_lr: 0.000001  loss: 4.4061 (4.4453)  class_acc: 0.1667 (0.2069)  loss_scale: 16384.0000 (29299.0749)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1007  max mem: 15572
Epoch: [8]  [ 930/1404]  eta: 0:04:39  lr: 0.000091  min_lr: 0.000001  loss: 4.4299 (4.4454)  class_acc: 0.1667 (0.2069)  loss_scale: 16384.0000 (29160.3523)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1009  max mem: 15572
Epoch: [8]  [ 940/1404]  eta: 0:04:33  lr: 0.000091  min_lr: 0.000001  loss: 4.4094 (4.4440)  class_acc: 0.2500 (0.2077)  loss_scale: 16384.0000 (29024.5781)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0607  max mem: 15572
Epoch: [8]  [ 950/1404]  eta: 0:04:28  lr: 0.000091  min_lr: 0.000001  loss: 4.3404 (4.4439)  class_acc: 0.2500 (0.2082)  loss_scale: 16384.0000 (28891.6593)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.1432  max mem: 15572
[2025-01-16 22:20:53,214] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:20:53,214] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:20:53,215] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:20:53,215] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [8]  [ 960/1404]  eta: 0:04:22  lr: 0.000091  min_lr: 0.000001  loss: 4.3775 (4.4433)  class_acc: 0.2500 (0.2082)  loss_scale: 16384.0000 (28812.6535)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1330  max mem: 15572
Epoch: [8]  [ 970/1404]  eta: 0:04:16  lr: 0.000091  min_lr: 0.000001  loss: 4.3678 (4.4431)  class_acc: 0.1667 (0.2082)  loss_scale: 32768.0000 (28853.3883)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1199  max mem: 15572
Epoch: [8]  [ 980/1404]  eta: 0:04:10  lr: 0.000091  min_lr: 0.000001  loss: 4.5102 (4.4439)  class_acc: 0.1667 (0.2082)  loss_scale: 32768.0000 (28893.2926)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0817  max mem: 15572
Epoch: [8]  [ 990/1404]  eta: 0:04:04  lr: 0.000091  min_lr: 0.000001  loss: 4.5387 (4.4447)  class_acc: 0.2083 (0.2082)  loss_scale: 32768.0000 (28932.3915)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0457  max mem: 15572
Epoch: [8]  [1000/1404]  eta: 0:03:58  lr: 0.000091  min_lr: 0.000001  loss: 4.4401 (4.4447)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (28970.7093)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0878  max mem: 15572
Epoch: [8]  [1010/1404]  eta: 0:03:53  lr: 0.000091  min_lr: 0.000001  loss: 4.4368 (4.4453)  class_acc: 0.1667 (0.2083)  loss_scale: 32768.0000 (29008.2690)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1185  max mem: 15572
Epoch: [8]  [1020/1404]  eta: 0:03:47  lr: 0.000091  min_lr: 0.000001  loss: 4.3899 (4.4455)  class_acc: 0.2083 (0.2082)  loss_scale: 32768.0000 (29045.0930)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1131  max mem: 15572
Epoch: [8]  [1030/1404]  eta: 0:03:41  lr: 0.000091  min_lr: 0.000001  loss: 4.4409 (4.4454)  class_acc: 0.2083 (0.2085)  loss_scale: 32768.0000 (29081.2027)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.0936  max mem: 15572
Epoch: [8]  [1040/1404]  eta: 0:03:35  lr: 0.000091  min_lr: 0.000001  loss: 4.4409 (4.4448)  class_acc: 0.2083 (0.2088)  loss_scale: 32768.0000 (29116.6186)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1227  max mem: 15572
Epoch: [8]  [1050/1404]  eta: 0:03:29  lr: 0.000091  min_lr: 0.000001  loss: 4.3459 (4.4429)  class_acc: 0.2083 (0.2086)  loss_scale: 32768.0000 (29151.3606)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0864  max mem: 15572
Epoch: [8]  [1060/1404]  eta: 0:03:23  lr: 0.000091  min_lr: 0.000001  loss: 4.3552 (4.4424)  class_acc: 0.2083 (0.2090)  loss_scale: 32768.0000 (29185.4477)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0537  max mem: 15572
Epoch: [8]  [1070/1404]  eta: 0:03:17  lr: 0.000091  min_lr: 0.000001  loss: 4.4089 (4.4425)  class_acc: 0.2083 (0.2086)  loss_scale: 32768.0000 (29218.8982)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1339  max mem: 15572
Epoch: [8]  [1080/1404]  eta: 0:03:11  lr: 0.000091  min_lr: 0.000001  loss: 4.5246 (4.4432)  class_acc: 0.1250 (0.2081)  loss_scale: 32768.0000 (29251.7299)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0975  max mem: 15572
[2025-01-16 22:22:08,960] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:22:08,960] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:22:08,991] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:22:08,992] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:22:09,430] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12319
[2025-01-16 22:22:09,430] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:22:09,430] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:22:09,471] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12319
[2025-01-16 22:22:09,471] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [1090/1404]  eta: 0:03:05  lr: 0.000091  min_lr: 0.000001  loss: 4.5626 (4.4446)  class_acc: 0.2083 (0.2082)  loss_scale: 32768.0000 (29313.9945)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0646  max mem: 15572
Epoch: [8]  [1100/1404]  eta: 0:02:59  lr: 0.000091  min_lr: 0.000001  loss: 4.4763 (4.4431)  class_acc: 0.2500 (0.2086)  loss_scale: 32768.0000 (29345.3660)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0880  max mem: 15572
Epoch: [8]  [1110/1404]  eta: 0:02:53  lr: 0.000091  min_lr: 0.000001  loss: 4.4948 (4.4445)  class_acc: 0.2500 (0.2086)  loss_scale: 32768.0000 (29376.1728)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1239  max mem: 15572
[2025-01-16 22:22:27,110] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12349
[2025-01-16 22:22:27,110] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12349
[2025-01-16 22:22:27,110] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:22:27,110] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:22:27,110] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [8]  [1120/1404]  eta: 0:02:47  lr: 0.000091  min_lr: 0.000001  loss: 4.4719 (4.4433)  class_acc: 0.2083 (0.2086)  loss_scale: 32768.0000 (29347.9679)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0921  max mem: 15572
Epoch: [8]  [1130/1404]  eta: 0:02:41  lr: 0.000091  min_lr: 0.000001  loss: 4.3629 (4.4424)  class_acc: 0.2083 (0.2085)  loss_scale: 16384.0000 (29233.3439)  weight_decay: 0.0500 (0.0500)  time: 0.5188  data: 0.0007  max mem: 15572
Epoch: [8]  [1140/1404]  eta: 0:02:35  lr: 0.000091  min_lr: 0.000001  loss: 4.3146 (4.4410)  class_acc: 0.2500 (0.2091)  loss_scale: 16384.0000 (29120.7292)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0008  max mem: 15572
Epoch: [8]  [1150/1404]  eta: 0:02:29  lr: 0.000091  min_lr: 0.000001  loss: 4.2930 (4.4409)  class_acc: 0.2500 (0.2095)  loss_scale: 16384.0000 (29010.0712)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0568  max mem: 15572
Epoch: [8]  [1160/1404]  eta: 0:02:23  lr: 0.000091  min_lr: 0.000001  loss: 4.4826 (4.4405)  class_acc: 0.2500 (0.2100)  loss_scale: 16384.0000 (28901.3196)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0568  max mem: 15572
Epoch: [8]  [1170/1404]  eta: 0:02:17  lr: 0.000091  min_lr: 0.000001  loss: 4.4474 (4.4404)  class_acc: 0.2500 (0.2104)  loss_scale: 16384.0000 (28794.4253)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0584  max mem: 15572
Epoch: [8]  [1180/1404]  eta: 0:02:12  lr: 0.000091  min_lr: 0.000001  loss: 4.4396 (4.4407)  class_acc: 0.2500 (0.2105)  loss_scale: 16384.0000 (28689.3412)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1073  max mem: 15572
Epoch: [8]  [1190/1404]  eta: 0:02:06  lr: 0.000091  min_lr: 0.000001  loss: 4.4203 (4.4410)  class_acc: 0.2083 (0.2106)  loss_scale: 16384.0000 (28586.0218)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0963  max mem: 15572
Epoch: [8]  [1200/1404]  eta: 0:02:00  lr: 0.000091  min_lr: 0.000001  loss: 4.4203 (4.4411)  class_acc: 0.2083 (0.2111)  loss_scale: 16384.0000 (28484.4230)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0887  max mem: 15572
Epoch: [8]  [1210/1404]  eta: 0:01:54  lr: 0.000091  min_lr: 0.000001  loss: 4.3437 (4.4400)  class_acc: 0.2500 (0.2117)  loss_scale: 16384.0000 (28384.5021)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0574  max mem: 15572
Epoch: [8]  [1220/1404]  eta: 0:01:48  lr: 0.000091  min_lr: 0.000001  loss: 4.2580 (4.4386)  class_acc: 0.2500 (0.2122)  loss_scale: 16384.0000 (28286.2179)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0582  max mem: 15572
Epoch: [8]  [1230/1404]  eta: 0:01:42  lr: 0.000091  min_lr: 0.000001  loss: 4.3912 (4.4391)  class_acc: 0.2500 (0.2121)  loss_scale: 16384.0000 (28189.5305)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.0651  max mem: 15572
Epoch: [8]  [1240/1404]  eta: 0:01:36  lr: 0.000091  min_lr: 0.000001  loss: 4.4957 (4.4398)  class_acc: 0.2083 (0.2119)  loss_scale: 16384.0000 (28094.4013)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1082  max mem: 15572
[2025-01-16 22:23:42,895] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:23:42,896] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:23:42,897] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:23:42,897] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [8]  [1250/1404]  eta: 0:01:30  lr: 0.000091  min_lr: 0.000001  loss: 4.4306 (4.4399)  class_acc: 0.2083 (0.2123)  loss_scale: 16384.0000 (28066.2766)  weight_decay: 0.0500 (0.0500)  time: 0.6974  data: 0.2128  max mem: 15572
Epoch: [8]  [1260/1404]  eta: 0:01:24  lr: 0.000091  min_lr: 0.000001  loss: 4.3489 (4.4388)  class_acc: 0.2083 (0.2120)  loss_scale: 32768.0000 (28103.5623)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1538  max mem: 15572
Epoch: [8]  [1270/1404]  eta: 0:01:18  lr: 0.000091  min_lr: 0.000001  loss: 4.3549 (4.4392)  class_acc: 0.1667 (0.2118)  loss_scale: 32768.0000 (28140.2612)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0448  max mem: 15572
Epoch: [8]  [1280/1404]  eta: 0:01:13  lr: 0.000091  min_lr: 0.000001  loss: 4.4930 (4.4400)  class_acc: 0.1667 (0.2120)  loss_scale: 32768.0000 (28176.3872)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0368  max mem: 15572
Epoch: [8]  [1290/1404]  eta: 0:01:07  lr: 0.000091  min_lr: 0.000001  loss: 4.4921 (4.4400)  class_acc: 0.2083 (0.2120)  loss_scale: 32768.0000 (28211.9535)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0832  max mem: 15572
Epoch: [8]  [1300/1404]  eta: 0:01:01  lr: 0.000091  min_lr: 0.000001  loss: 4.4704 (4.4405)  class_acc: 0.1667 (0.2116)  loss_scale: 32768.0000 (28246.9731)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.1385  max mem: 15572
Epoch: [8]  [1310/1404]  eta: 0:00:55  lr: 0.000091  min_lr: 0.000001  loss: 4.4083 (4.4398)  class_acc: 0.2083 (0.2116)  loss_scale: 32768.0000 (28281.4584)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0743  max mem: 15572
Epoch: [8]  [1320/1404]  eta: 0:00:49  lr: 0.000091  min_lr: 0.000001  loss: 4.4223 (4.4403)  class_acc: 0.2083 (0.2116)  loss_scale: 32768.0000 (28315.4217)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0768  max mem: 15572
Epoch: [8]  [1330/1404]  eta: 0:00:43  lr: 0.000091  min_lr: 0.000001  loss: 4.5391 (4.4403)  class_acc: 0.2083 (0.2120)  loss_scale: 32768.0000 (28348.8745)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1394  max mem: 15572
Epoch: [8]  [1340/1404]  eta: 0:00:37  lr: 0.000091  min_lr: 0.000001  loss: 4.5391 (4.4406)  class_acc: 0.2083 (0.2121)  loss_scale: 32768.0000 (28381.8285)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1536  max mem: 15572
Epoch: [8]  [1350/1404]  eta: 0:00:31  lr: 0.000091  min_lr: 0.000001  loss: 4.5056 (4.4408)  class_acc: 0.2083 (0.2118)  loss_scale: 32768.0000 (28414.2946)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1247  max mem: 15572
Epoch: [8]  [1360/1404]  eta: 0:00:25  lr: 0.000091  min_lr: 0.000001  loss: 4.4892 (4.4409)  class_acc: 0.2083 (0.2119)  loss_scale: 32768.0000 (28446.2836)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0598  max mem: 15572
Epoch: [8]  [1370/1404]  eta: 0:00:20  lr: 0.000091  min_lr: 0.000001  loss: 4.3222 (4.4396)  class_acc: 0.2083 (0.2121)  loss_scale: 32768.0000 (28477.8060)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0824  max mem: 15572
[2025-01-16 22:24:56,966] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:24:56,966] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:24:56,991] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:24:56,992] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:24:57,553] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12607
[2025-01-16 22:24:57,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:24:57,553] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:24:57,554] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12607
[2025-01-16 22:24:57,555] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [1380/1404]  eta: 0:00:14  lr: 0.000091  min_lr: 0.000001  loss: 4.2894 (4.4392)  class_acc: 0.1667 (0.2120)  loss_scale: 32768.0000 (28532.5996)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.0861  max mem: 15572
Epoch: [8]  [1390/1404]  eta: 0:00:08  lr: 0.000091  min_lr: 0.000001  loss: 4.3422 (4.4385)  class_acc: 0.2083 (0.2119)  loss_scale: 32768.0000 (28563.0482)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0569  max mem: 15572
Epoch: [8]  [1400/1404]  eta: 0:00:02  lr: 0.000091  min_lr: 0.000001  loss: 4.3497 (4.4388)  class_acc: 0.2500 (0.2121)  loss_scale: 32768.0000 (28593.0621)  weight_decay: 0.0500 (0.0500)  time: 0.4387  data: 0.0275  max mem: 15572
Epoch: [8]  [1403/1404]  eta: 0:00:00  lr: 0.000091  min_lr: 0.000001  loss: 4.5029 (4.4390)  class_acc: 0.2500 (0.2121)  loss_scale: 32768.0000 (28601.9829)  weight_decay: 0.0500 (0.0500)  time: 0.4213  data: 0.0274  max mem: 15572
Epoch: [8] Total time: 0:13:44 (0.5873 s / it)
Averaged stats: lr: 0.000091  min_lr: 0.000001  loss: 4.5029 (4.4442)  class_acc: 0.2500 (0.2116)  loss_scale: 32768.0000 (28601.9829)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:11:22  loss: 1.9808 (1.9808)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 5.0211  data: 4.8449  max mem: 15572
Val:  [ 10/136]  eta: 0:01:38  loss: 3.2125 (3.1467)  acc1: 27.7778 (29.2929)  acc5: 66.6667 (59.5960)  time: 0.7830  data: 0.5778  max mem: 15572
Val:  [ 20/136]  eta: 0:01:01  loss: 3.2581 (3.2386)  acc1: 22.2222 (25.9259)  acc5: 61.1111 (58.2011)  time: 0.3100  data: 0.1154  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 3.1926 (3.0515)  acc1: 27.7778 (32.0789)  acc5: 61.1111 (62.5448)  time: 0.3571  data: 0.1675  max mem: 15572
Val:  [ 40/136]  eta: 0:00:41  loss: 2.4041 (2.9632)  acc1: 44.4444 (34.8238)  acc5: 83.3333 (65.3117)  time: 0.3318  data: 0.1333  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.9876 (3.0251)  acc1: 33.3333 (34.0959)  acc5: 72.2222 (64.9237)  time: 0.3064  data: 0.0991  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 3.2591 (3.0889)  acc1: 22.2222 (32.1494)  acc5: 55.5556 (63.4791)  time: 0.3590  data: 0.1525  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 3.1311 (3.0631)  acc1: 33.3333 (33.9593)  acc5: 61.1111 (64.0845)  time: 0.3289  data: 0.1357  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.9274 (3.0484)  acc1: 38.8889 (34.1564)  acc5: 72.2222 (65.2949)  time: 0.3448  data: 0.1518  max mem: 15572
Val:  [ 90/136]  eta: 0:00:17  loss: 2.9523 (3.0471)  acc1: 38.8889 (34.0049)  acc5: 72.2222 (65.8120)  time: 0.3361  data: 0.1401  max mem: 15572
Val:  [100/136]  eta: 0:00:13  loss: 3.1141 (3.0943)  acc1: 27.7778 (32.7833)  acc5: 66.6667 (64.3014)  time: 0.3460  data: 0.1558  max mem: 15572
Val:  [110/136]  eta: 0:00:09  loss: 3.1029 (3.0948)  acc1: 27.7778 (33.0831)  acc5: 61.1111 (64.4144)  time: 0.3565  data: 0.1700  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.9145 (3.0648)  acc1: 44.4444 (34.6189)  acc5: 72.2222 (65.7484)  time: 0.3890  data: 0.1874  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.8064 (3.0436)  acc1: 50.0000 (35.5386)  acc5: 77.7778 (66.1154)  time: 0.3109  data: 0.1113  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.8397 (3.0510)  acc1: 27.7778 (35.1761)  acc5: 72.2222 (65.8886)  time: 0.2441  data: 0.0647  max mem: 15572
Val: Total time: 0:00:49 (0.3629 s / it)
* Acc@1 34.541 Acc@5 64.885 loss 3.083
Accuracy of the network on the 4883 val videos: 34.5%
[2025-01-16 22:26:00,569] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 22:26:00,571] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 22:26:00,571] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 22:26:00,571] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 22:26:02,809] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 22:26:02,809] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 34.54%
Epoch: [9]  [   0/1404]  eta: 3:13:53  lr: 0.000091  min_lr: 0.000001  loss: 3.9563 (3.9563)  class_acc: 0.3333 (0.3333)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.2859  data: 5.5071  max mem: 15572
Epoch: [9]  [  10/1404]  eta: 0:29:08  lr: 0.000091  min_lr: 0.000001  loss: 4.2787 (4.2524)  class_acc: 0.2500 (0.2121)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2544  data: 0.5012  max mem: 15572
Epoch: [9]  [  20/1404]  eta: 0:21:34  lr: 0.000091  min_lr: 0.000001  loss: 4.3292 (4.3652)  class_acc: 0.1667 (0.1925)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0005  max mem: 15572
Epoch: [9]  [  30/1404]  eta: 0:19:20  lr: 0.000091  min_lr: 0.000001  loss: 4.4994 (4.3891)  class_acc: 0.1667 (0.2043)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0004  max mem: 15572
Epoch: [9]  [  40/1404]  eta: 0:17:37  lr: 0.000091  min_lr: 0.000001  loss: 4.4994 (4.3982)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0006  max mem: 15572
Epoch: [9]  [  50/1404]  eta: 0:16:33  lr: 0.000091  min_lr: 0.000001  loss: 4.5098 (4.3931)  class_acc: 0.2083 (0.2132)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0007  max mem: 15572
Epoch: [9]  [  60/1404]  eta: 0:15:49  lr: 0.000091  min_lr: 0.000001  loss: 4.3447 (4.3967)  class_acc: 0.1667 (0.2077)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.0005  max mem: 15572
Epoch: [9]  [  70/1404]  eta: 0:15:09  lr: 0.000091  min_lr: 0.000001  loss: 4.4020 (4.3990)  class_acc: 0.1667 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0005  max mem: 15572
Epoch: [9]  [  80/1404]  eta: 0:14:48  lr: 0.000091  min_lr: 0.000001  loss: 4.4623 (4.4146)  class_acc: 0.2083 (0.2063)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0006  max mem: 15572
Epoch: [9]  [  90/1404]  eta: 0:14:20  lr: 0.000091  min_lr: 0.000001  loss: 4.4654 (4.4178)  class_acc: 0.2500 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0006  max mem: 15572
[2025-01-16 22:27:08,432] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:27:08,432] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:27:08,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:27:08,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 100/1404]  eta: 0:14:06  lr: 0.000091  min_lr: 0.000001  loss: 4.3807 (4.4127)  class_acc: 0.2500 (0.2133)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0007  max mem: 15572
Epoch: [9]  [ 110/1404]  eta: 0:13:46  lr: 0.000091  min_lr: 0.000001  loss: 4.2040 (4.4042)  class_acc: 0.2083 (0.2125)  loss_scale: 65536.0000 (36015.2793)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.0011  max mem: 15572
[2025-01-16 22:27:15,943] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12749
[2025-01-16 22:27:15,944] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:27:15,944] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:27:15,945] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12749
[2025-01-16 22:27:15,945] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [9]  [ 120/1404]  eta: 0:13:32  lr: 0.000091  min_lr: 0.000001  loss: 4.3792 (4.3984)  class_acc: 0.2083 (0.2156)  loss_scale: 65536.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0267  max mem: 15572
Epoch: [9]  [ 130/1404]  eta: 0:13:30  lr: 0.000091  min_lr: 0.000001  loss: 4.4450 (4.3961)  class_acc: 0.2083 (0.2150)  loss_scale: 32768.0000 (36019.7863)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1126  max mem: 15572
Epoch: [9]  [ 140/1404]  eta: 0:13:23  lr: 0.000091  min_lr: 0.000001  loss: 4.3411 (4.3929)  class_acc: 0.2083 (0.2160)  loss_scale: 32768.0000 (35789.1631)  weight_decay: 0.0500 (0.0500)  time: 0.6529  data: 0.1585  max mem: 15572
Epoch: [9]  [ 150/1404]  eta: 0:13:09  lr: 0.000091  min_lr: 0.000001  loss: 4.3584 (4.3913)  class_acc: 0.2083 (0.2144)  loss_scale: 32768.0000 (35589.0861)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0970  max mem: 15572
Epoch: [9]  [ 160/1404]  eta: 0:12:59  lr: 0.000091  min_lr: 0.000001  loss: 4.3788 (4.3896)  class_acc: 0.2083 (0.2153)  loss_scale: 32768.0000 (35413.8634)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.0701  max mem: 15572
Epoch: [9]  [ 170/1404]  eta: 0:12:48  lr: 0.000091  min_lr: 0.000001  loss: 4.2171 (4.3781)  class_acc: 0.2083 (0.2193)  loss_scale: 32768.0000 (35259.1345)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0560  max mem: 15572
Epoch: [9]  [ 180/1404]  eta: 0:12:45  lr: 0.000091  min_lr: 0.000001  loss: 4.4103 (4.3823)  class_acc: 0.1667 (0.2155)  loss_scale: 32768.0000 (35121.5028)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0113  max mem: 15572
Epoch: [9]  [ 190/1404]  eta: 0:12:41  lr: 0.000091  min_lr: 0.000001  loss: 4.4221 (4.3795)  class_acc: 0.1667 (0.2164)  loss_scale: 32768.0000 (34998.2827)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.0027  max mem: 15572
Epoch: [9]  [ 200/1404]  eta: 0:12:27  lr: 0.000091  min_lr: 0.000001  loss: 4.4221 (4.3813)  class_acc: 0.2083 (0.2175)  loss_scale: 32768.0000 (34887.3234)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.0027  max mem: 15572
Epoch: [9]  [ 210/1404]  eta: 0:12:15  lr: 0.000091  min_lr: 0.000001  loss: 4.4900 (4.3893)  class_acc: 0.1667 (0.2176)  loss_scale: 32768.0000 (34786.8815)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0008  max mem: 15572
Epoch: [9]  [ 220/1404]  eta: 0:12:06  lr: 0.000091  min_lr: 0.000001  loss: 4.5574 (4.3907)  class_acc: 0.1667 (0.2178)  loss_scale: 32768.0000 (34695.5294)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0280  max mem: 15572
Epoch: [9]  [ 230/1404]  eta: 0:11:58  lr: 0.000091  min_lr: 0.000001  loss: 4.2659 (4.3854)  class_acc: 0.2500 (0.2206)  loss_scale: 32768.0000 (34612.0866)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0278  max mem: 15572
Epoch: [9]  [ 240/1404]  eta: 0:11:52  lr: 0.000091  min_lr: 0.000001  loss: 4.2626 (4.3810)  class_acc: 0.2500 (0.2203)  loss_scale: 32768.0000 (34535.5685)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0031  max mem: 15572
[2025-01-16 22:28:31,794] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:28:31,794] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:28:31,856] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:28:31,857] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:28:33,454] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12881
[2025-01-16 22:28:33,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:28:33,454] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:28:33,467] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12881
[2025-01-16 22:28:33,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [9]  [ 250/1404]  eta: 0:11:42  lr: 0.000090  min_lr: 0.000001  loss: 4.3116 (4.3784)  class_acc: 0.1667 (0.2190)  loss_scale: 32768.0000 (34856.7968)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0031  max mem: 15572
Epoch: [9]  [ 260/1404]  eta: 0:11:34  lr: 0.000090  min_lr: 0.000001  loss: 4.3477 (4.3766)  class_acc: 0.1667 (0.2176)  loss_scale: 32768.0000 (34776.7663)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0007  max mem: 15572
Epoch: [9]  [ 270/1404]  eta: 0:11:27  lr: 0.000090  min_lr: 0.000001  loss: 4.3649 (4.3763)  class_acc: 0.1667 (0.2169)  loss_scale: 32768.0000 (34702.6421)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0006  max mem: 15572
Epoch: [9]  [ 280/1404]  eta: 0:11:21  lr: 0.000090  min_lr: 0.000001  loss: 4.4060 (4.3810)  class_acc: 0.2083 (0.2181)  loss_scale: 32768.0000 (34633.7936)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0008  max mem: 15572
Epoch: [9]  [ 290/1404]  eta: 0:11:15  lr: 0.000090  min_lr: 0.000001  loss: 4.5534 (4.3881)  class_acc: 0.2083 (0.2168)  loss_scale: 32768.0000 (34569.6770)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0008  max mem: 15572
Epoch: [9]  [ 300/1404]  eta: 0:11:09  lr: 0.000090  min_lr: 0.000001  loss: 4.4936 (4.3853)  class_acc: 0.1667 (0.2157)  loss_scale: 32768.0000 (34509.8206)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0250  max mem: 15572
Epoch: [9]  [ 310/1404]  eta: 0:11:02  lr: 0.000090  min_lr: 0.000001  loss: 4.3163 (4.3846)  class_acc: 0.1667 (0.2158)  loss_scale: 32768.0000 (34453.8135)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0250  max mem: 15572
Epoch: [9]  [ 320/1404]  eta: 0:10:56  lr: 0.000090  min_lr: 0.000001  loss: 4.4286 (4.3877)  class_acc: 0.2083 (0.2169)  loss_scale: 32768.0000 (34401.2960)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0007  max mem: 15572
Epoch: [9]  [ 330/1404]  eta: 0:10:49  lr: 0.000090  min_lr: 0.000001  loss: 4.4749 (4.3936)  class_acc: 0.2083 (0.2176)  loss_scale: 32768.0000 (34351.9517)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.0008  max mem: 15572
Epoch: [9]  [ 340/1404]  eta: 0:10:44  lr: 0.000090  min_lr: 0.000001  loss: 4.5093 (4.3941)  class_acc: 0.2083 (0.2182)  loss_scale: 32768.0000 (34305.5015)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0008  max mem: 15572
Epoch: [9]  [ 350/1404]  eta: 0:10:39  lr: 0.000090  min_lr: 0.000001  loss: 4.4869 (4.3955)  class_acc: 0.2083 (0.2193)  loss_scale: 32768.0000 (34261.6980)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.0007  max mem: 15572
Epoch: [9]  [ 360/1404]  eta: 0:10:30  lr: 0.000090  min_lr: 0.000001  loss: 4.4209 (4.3916)  class_acc: 0.2083 (0.2203)  loss_scale: 32768.0000 (34220.3213)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0007  max mem: 15572
[2025-01-16 22:29:42,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=77, lr=[8.755895920446586e-07, 8.755895920446586e-07, 1.2508422743495125e-06, 1.2508422743495125e-06, 1.786917534785018e-06, 1.786917534785018e-06, 2.5527393354071685e-06, 2.5527393354071685e-06, 3.6467704791530984e-06, 3.6467704791530984e-06, 5.209672113075855e-06, 5.209672113075855e-06, 7.442388732965508e-06, 7.442388732965508e-06, 1.0631983904236441e-05, 1.0631983904236441e-05, 1.5188548434623486e-05, 1.5188548434623486e-05, 2.169792633517641e-05, 2.169792633517641e-05, 3.0997037621680587e-05, 3.0997037621680587e-05, 4.428148231668656e-05, 4.428148231668656e-05, 6.325926045240937e-05, 6.325926045240937e-05, 9.037037207487054e-05, 9.037037207487054e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 22:29:42,637] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=48.05476204397368, CurrSamplesPerSec=57.14570143966574, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [ 370/1404]  eta: 0:10:22  lr: 0.000090  min_lr: 0.000001  loss: 4.2522 (4.3907)  class_acc: 0.2500 (0.2210)  loss_scale: 32768.0000 (34181.1752)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0006  max mem: 15572
[2025-01-16 22:29:48,554] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:29:48,554] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:29:48,554] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:29:48,554] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:29:51,488] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13016
[2025-01-16 22:29:51,488] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:29:51,489] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13016
[2025-01-16 22:29:51,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:29:51,489] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 380/1404]  eta: 0:10:14  lr: 0.000090  min_lr: 0.000001  loss: 4.3831 (4.3940)  class_acc: 0.2083 (0.2198)  loss_scale: 32768.0000 (34660.1155)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0006  max mem: 15572
Epoch: [9]  [ 390/1404]  eta: 0:10:07  lr: 0.000090  min_lr: 0.000001  loss: 4.4378 (4.3934)  class_acc: 0.1667 (0.2187)  loss_scale: 32768.0000 (34611.7238)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0006  max mem: 15572
Epoch: [9]  [ 400/1404]  eta: 0:10:01  lr: 0.000090  min_lr: 0.000001  loss: 4.4138 (4.3951)  class_acc: 0.1667 (0.2188)  loss_scale: 32768.0000 (34565.7456)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0006  max mem: 15572
Epoch: [9]  [ 410/1404]  eta: 0:09:55  lr: 0.000090  min_lr: 0.000001  loss: 4.4730 (4.3971)  class_acc: 0.2083 (0.2183)  loss_scale: 32768.0000 (34522.0049)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0098  max mem: 15572
Epoch: [9]  [ 420/1404]  eta: 0:09:50  lr: 0.000090  min_lr: 0.000001  loss: 4.4109 (4.4000)  class_acc: 0.1667 (0.2177)  loss_scale: 32768.0000 (34480.3420)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.0408  max mem: 15572
Epoch: [9]  [ 430/1404]  eta: 0:09:44  lr: 0.000090  min_lr: 0.000001  loss: 4.3018 (4.3982)  class_acc: 0.2083 (0.2183)  loss_scale: 32768.0000 (34440.6125)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.0317  max mem: 15572
Epoch: [9]  [ 440/1404]  eta: 0:09:39  lr: 0.000090  min_lr: 0.000001  loss: 4.3580 (4.3987)  class_acc: 0.2500 (0.2191)  loss_scale: 32768.0000 (34402.6848)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.0006  max mem: 15572
Epoch: [9]  [ 450/1404]  eta: 0:09:33  lr: 0.000090  min_lr: 0.000001  loss: 4.3916 (4.3959)  class_acc: 0.2500 (0.2207)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.0007  max mem: 15572
Epoch: [9]  [ 460/1404]  eta: 0:09:27  lr: 0.000090  min_lr: 0.000001  loss: 4.3779 (4.3980)  class_acc: 0.2500 (0.2206)  loss_scale: 32768.0000 (34331.7657)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.0007  max mem: 15572
Epoch: [9]  [ 470/1404]  eta: 0:09:21  lr: 0.000090  min_lr: 0.000001  loss: 4.4802 (4.3994)  class_acc: 0.2500 (0.2210)  loss_scale: 32768.0000 (34298.5648)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0007  max mem: 15572
Epoch: [9]  [ 480/1404]  eta: 0:09:14  lr: 0.000090  min_lr: 0.000001  loss: 4.4013 (4.3981)  class_acc: 0.2500 (0.2219)  loss_scale: 32768.0000 (34266.7443)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0006  max mem: 15572
Epoch: [9]  [ 490/1404]  eta: 0:09:07  lr: 0.000090  min_lr: 0.000001  loss: 4.4598 (4.4027)  class_acc: 0.2500 (0.2217)  loss_scale: 32768.0000 (34236.2200)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0007  max mem: 15572
Epoch: [9]  [ 500/1404]  eta: 0:09:01  lr: 0.000090  min_lr: 0.000001  loss: 4.5320 (4.4025)  class_acc: 0.2083 (0.2211)  loss_scale: 32768.0000 (34206.9142)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0008  max mem: 15572
[2025-01-16 22:31:07,175] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:31:07,175] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:31:07,177] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:31:07,177] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 510/1404]  eta: 0:08:52  lr: 0.000090  min_lr: 0.000001  loss: 4.4326 (4.4034)  class_acc: 0.1667 (0.2207)  loss_scale: 32768.0000 (34307.0059)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0007  max mem: 15572
[2025-01-16 22:31:08,438] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13147
[2025-01-16 22:31:08,438] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13147
[2025-01-16 22:31:08,438] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:31:08,438] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:31:08,438] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:31:13,159] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13153
[2025-01-16 22:31:13,160] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:31:13,250] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13153
[2025-01-16 22:31:13,251] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:31:13,251] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [9]  [ 520/1404]  eta: 0:08:48  lr: 0.000090  min_lr: 0.000001  loss: 4.4231 (4.4048)  class_acc: 0.1667 (0.2202)  loss_scale: 32768.0000 (34151.6775)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0198  max mem: 15572
Epoch: [9]  [ 530/1404]  eta: 0:08:44  lr: 0.000090  min_lr: 0.000001  loss: 4.3741 (4.4024)  class_acc: 0.1667 (0.2206)  loss_scale: 16384.0000 (33817.0697)  weight_decay: 0.0500 (0.0500)  time: 0.6907  data: 0.0198  max mem: 15572
Epoch: [9]  [ 540/1404]  eta: 0:08:37  lr: 0.000090  min_lr: 0.000001  loss: 4.3899 (4.4025)  class_acc: 0.2083 (0.2201)  loss_scale: 16384.0000 (33494.8318)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.0006  max mem: 15572
Epoch: [9]  [ 550/1404]  eta: 0:08:30  lr: 0.000090  min_lr: 0.000001  loss: 4.4337 (4.4017)  class_acc: 0.2083 (0.2200)  loss_scale: 16384.0000 (33184.2904)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0005  max mem: 15572
Epoch: [9]  [ 560/1404]  eta: 0:08:25  lr: 0.000090  min_lr: 0.000001  loss: 4.4189 (4.4004)  class_acc: 0.2083 (0.2202)  loss_scale: 16384.0000 (32884.8200)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.0006  max mem: 15572
Epoch: [9]  [ 570/1404]  eta: 0:08:17  lr: 0.000090  min_lr: 0.000001  loss: 4.5116 (4.4028)  class_acc: 0.2500 (0.2204)  loss_scale: 16384.0000 (32595.8389)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0006  max mem: 15572
Epoch: [9]  [ 580/1404]  eta: 0:08:11  lr: 0.000090  min_lr: 0.000001  loss: 4.5116 (4.4046)  class_acc: 0.2083 (0.2200)  loss_scale: 16384.0000 (32316.8055)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0007  max mem: 15572
Epoch: [9]  [ 590/1404]  eta: 0:08:05  lr: 0.000090  min_lr: 0.000001  loss: 4.4652 (4.4060)  class_acc: 0.1667 (0.2200)  loss_scale: 16384.0000 (32047.2149)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.0008  max mem: 15572
Epoch: [9]  [ 600/1404]  eta: 0:07:59  lr: 0.000090  min_lr: 0.000001  loss: 4.4405 (4.4061)  class_acc: 0.2083 (0.2208)  loss_scale: 16384.0000 (31786.5957)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0007  max mem: 15572
Epoch: [9]  [ 610/1404]  eta: 0:07:52  lr: 0.000090  min_lr: 0.000001  loss: 4.3614 (4.4053)  class_acc: 0.2500 (0.2210)  loss_scale: 16384.0000 (31534.5074)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0005  max mem: 15572
Epoch: [9]  [ 620/1404]  eta: 0:07:47  lr: 0.000090  min_lr: 0.000001  loss: 4.4331 (4.4076)  class_acc: 0.2083 (0.2204)  loss_scale: 16384.0000 (31290.5378)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.0005  max mem: 15572
Epoch: [9]  [ 630/1404]  eta: 0:07:42  lr: 0.000090  min_lr: 0.000001  loss: 4.4391 (4.4072)  class_acc: 0.1667 (0.2200)  loss_scale: 16384.0000 (31054.3011)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.0009  max mem: 15572
Epoch: [9]  [ 640/1404]  eta: 0:07:35  lr: 0.000090  min_lr: 0.000001  loss: 4.3997 (4.4071)  class_acc: 0.2083 (0.2208)  loss_scale: 16384.0000 (30825.4353)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0009  max mem: 15572
[2025-01-16 22:32:27,945] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:32:27,946] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:32:27,958] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:32:27,958] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [9]  [ 650/1404]  eta: 0:07:29  lr: 0.000090  min_lr: 0.000001  loss: 4.3640 (4.4060)  class_acc: 0.2083 (0.2205)  loss_scale: 16384.0000 (30729.4378)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0007  max mem: 15572
Epoch: [9]  [ 660/1404]  eta: 0:07:23  lr: 0.000090  min_lr: 0.000001  loss: 4.3999 (4.4068)  class_acc: 0.1667 (0.2199)  loss_scale: 32768.0000 (30760.2784)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0009  max mem: 15572
Epoch: [9]  [ 670/1404]  eta: 0:07:17  lr: 0.000090  min_lr: 0.000001  loss: 4.3999 (4.4050)  class_acc: 0.2083 (0.2206)  loss_scale: 32768.0000 (30790.1997)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0009  max mem: 15572
Epoch: [9]  [ 680/1404]  eta: 0:07:11  lr: 0.000090  min_lr: 0.000001  loss: 4.2530 (4.4054)  class_acc: 0.2500 (0.2208)  loss_scale: 32768.0000 (30819.2423)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0008  max mem: 15572
Epoch: [9]  [ 690/1404]  eta: 0:07:05  lr: 0.000090  min_lr: 0.000001  loss: 4.4177 (4.4075)  class_acc: 0.1667 (0.2199)  loss_scale: 32768.0000 (30847.4443)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0007  max mem: 15572
Epoch: [9]  [ 700/1404]  eta: 0:06:58  lr: 0.000090  min_lr: 0.000001  loss: 4.4333 (4.4073)  class_acc: 0.1667 (0.2197)  loss_scale: 32768.0000 (30874.8417)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0006  max mem: 15572
Epoch: [9]  [ 710/1404]  eta: 0:06:52  lr: 0.000090  min_lr: 0.000001  loss: 4.4969 (4.4098)  class_acc: 0.2083 (0.2197)  loss_scale: 32768.0000 (30901.4684)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0006  max mem: 15572
Epoch: [9]  [ 720/1404]  eta: 0:06:46  lr: 0.000090  min_lr: 0.000001  loss: 4.5129 (4.4101)  class_acc: 0.1667 (0.2189)  loss_scale: 32768.0000 (30927.3564)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0007  max mem: 15572
Epoch: [9]  [ 730/1404]  eta: 0:06:39  lr: 0.000090  min_lr: 0.000001  loss: 4.4529 (4.4092)  class_acc: 0.1250 (0.2183)  loss_scale: 32768.0000 (30952.5363)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0007  max mem: 15572
Epoch: [9]  [ 740/1404]  eta: 0:06:34  lr: 0.000090  min_lr: 0.000001  loss: 4.4763 (4.4114)  class_acc: 0.1667 (0.2182)  loss_scale: 32768.0000 (30977.0364)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0008  max mem: 15572
Epoch: [9]  [ 750/1404]  eta: 0:06:28  lr: 0.000090  min_lr: 0.000001  loss: 4.4763 (4.4110)  class_acc: 0.2083 (0.2185)  loss_scale: 32768.0000 (31000.8842)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0008  max mem: 15572
Epoch: [9]  [ 760/1404]  eta: 0:06:22  lr: 0.000090  min_lr: 0.000001  loss: 4.3849 (4.4091)  class_acc: 0.2083 (0.2187)  loss_scale: 32768.0000 (31024.1051)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.0006  max mem: 15572
Epoch: [9]  [ 770/1404]  eta: 0:06:16  lr: 0.000090  min_lr: 0.000001  loss: 4.3550 (4.4080)  class_acc: 0.2083 (0.2189)  loss_scale: 32768.0000 (31046.7237)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0006  max mem: 15572
[2025-01-16 22:33:42,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:33:42,561] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:33:42,563] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:33:42,563] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:33:43,014] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13411
[2025-01-16 22:33:43,015] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:33:43,015] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:33:43,066] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13411
[2025-01-16 22:33:43,067] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [9]  [ 780/1404]  eta: 0:06:09  lr: 0.000090  min_lr: 0.000001  loss: 4.3106 (4.4066)  class_acc: 0.2083 (0.2183)  loss_scale: 32768.0000 (31110.7196)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0006  max mem: 15572
Epoch: [9]  [ 790/1404]  eta: 0:06:04  lr: 0.000090  min_lr: 0.000001  loss: 4.2733 (4.4050)  class_acc: 0.2083 (0.2181)  loss_scale: 32768.0000 (31131.6713)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0008  max mem: 15572
Epoch: [9]  [ 800/1404]  eta: 0:05:57  lr: 0.000090  min_lr: 0.000001  loss: 4.4295 (4.4064)  class_acc: 0.1667 (0.2170)  loss_scale: 32768.0000 (31152.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0008  max mem: 15572
Epoch: [9]  [ 810/1404]  eta: 0:05:51  lr: 0.000090  min_lr: 0.000001  loss: 4.4642 (4.4084)  class_acc: 0.1667 (0.2168)  loss_scale: 32768.0000 (31172.0247)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0339  max mem: 15572
Epoch: [9]  [ 820/1404]  eta: 0:05:45  lr: 0.000090  min_lr: 0.000001  loss: 4.4273 (4.4078)  class_acc: 0.2083 (0.2177)  loss_scale: 32768.0000 (31191.4641)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0770  max mem: 15572
Epoch: [9]  [ 830/1404]  eta: 0:05:39  lr: 0.000090  min_lr: 0.000001  loss: 4.3024 (4.4082)  class_acc: 0.2500 (0.2183)  loss_scale: 32768.0000 (31210.4356)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0890  max mem: 15572
Epoch: [9]  [ 840/1404]  eta: 0:05:33  lr: 0.000090  min_lr: 0.000001  loss: 4.4095 (4.4091)  class_acc: 0.2500 (0.2192)  loss_scale: 32768.0000 (31228.9560)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0960  max mem: 15572
Epoch: [9]  [ 850/1404]  eta: 0:05:27  lr: 0.000090  min_lr: 0.000001  loss: 4.4095 (4.4079)  class_acc: 0.2500 (0.2200)  loss_scale: 32768.0000 (31247.0411)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0660  max mem: 15572
Epoch: [9]  [ 860/1404]  eta: 0:05:22  lr: 0.000090  min_lr: 0.000001  loss: 4.4422 (4.4089)  class_acc: 0.2500 (0.2201)  loss_scale: 32768.0000 (31264.7062)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0779  max mem: 15572
Epoch: [9]  [ 870/1404]  eta: 0:05:16  lr: 0.000090  min_lr: 0.000001  loss: 4.4422 (4.4091)  class_acc: 0.1667 (0.2193)  loss_scale: 32768.0000 (31281.9656)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1027  max mem: 15572
Epoch: [9]  [ 880/1404]  eta: 0:05:09  lr: 0.000090  min_lr: 0.000001  loss: 4.4251 (4.4097)  class_acc: 0.1667 (0.2197)  loss_scale: 32768.0000 (31298.8331)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0512  max mem: 15572
Epoch: [9]  [ 890/1404]  eta: 0:05:03  lr: 0.000090  min_lr: 0.000001  loss: 4.2908 (4.4083)  class_acc: 0.1667 (0.2193)  loss_scale: 32768.0000 (31315.3221)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0210  max mem: 15572
Epoch: [9]  [ 900/1404]  eta: 0:04:58  lr: 0.000090  min_lr: 0.000001  loss: 4.3301 (4.4089)  class_acc: 0.2083 (0.2193)  loss_scale: 32768.0000 (31331.4451)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0691  max mem: 15572
[2025-01-16 22:34:58,111] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:34:58,111] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:34:58,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:34:58,118] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:35:01,107] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13544
[2025-01-16 22:35:01,107] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:35:01,107] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:35:01,109] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13544
[2025-01-16 22:35:01,110] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [9]  [ 910/1404]  eta: 0:04:52  lr: 0.000090  min_lr: 0.000001  loss: 4.3301 (4.4075)  class_acc: 0.2083 (0.2197)  loss_scale: 32768.0000 (31491.0911)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.0816  max mem: 15572
Epoch: [9]  [ 920/1404]  eta: 0:04:46  lr: 0.000090  min_lr: 0.000001  loss: 4.3253 (4.4068)  class_acc: 0.2083 (0.2198)  loss_scale: 32768.0000 (31504.9555)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0228  max mem: 15572
Epoch: [9]  [ 930/1404]  eta: 0:04:40  lr: 0.000090  min_lr: 0.000001  loss: 4.3666 (4.4056)  class_acc: 0.2083 (0.2203)  loss_scale: 32768.0000 (31518.5220)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0288  max mem: 15572
Epoch: [9]  [ 940/1404]  eta: 0:04:34  lr: 0.000090  min_lr: 0.000001  loss: 4.2205 (4.4042)  class_acc: 0.2083 (0.2202)  loss_scale: 32768.0000 (31531.8002)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1146  max mem: 15572
Epoch: [9]  [ 950/1404]  eta: 0:04:28  lr: 0.000090  min_lr: 0.000001  loss: 4.3873 (4.4042)  class_acc: 0.2083 (0.2200)  loss_scale: 32768.0000 (31544.7992)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1023  max mem: 15572
Epoch: [9]  [ 960/1404]  eta: 0:04:22  lr: 0.000090  min_lr: 0.000001  loss: 4.3600 (4.4032)  class_acc: 0.2083 (0.2200)  loss_scale: 32768.0000 (31557.5276)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1213  max mem: 15572
Epoch: [9]  [ 970/1404]  eta: 0:04:16  lr: 0.000090  min_lr: 0.000001  loss: 4.3422 (4.4041)  class_acc: 0.1667 (0.2197)  loss_scale: 32768.0000 (31569.9938)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1259  max mem: 15572
Epoch: [9]  [ 980/1404]  eta: 0:04:10  lr: 0.000090  min_lr: 0.000001  loss: 4.4221 (4.4050)  class_acc: 0.2083 (0.2199)  loss_scale: 32768.0000 (31582.2059)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0899  max mem: 15572
Epoch: [9]  [ 990/1404]  eta: 0:04:04  lr: 0.000090  min_lr: 0.000001  loss: 4.4221 (4.4064)  class_acc: 0.2083 (0.2197)  loss_scale: 32768.0000 (31594.1715)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0831  max mem: 15572
Epoch: [9]  [1000/1404]  eta: 0:03:58  lr: 0.000090  min_lr: 0.000001  loss: 4.4880 (4.4070)  class_acc: 0.1667 (0.2193)  loss_scale: 32768.0000 (31605.8981)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0544  max mem: 15572
Epoch: [9]  [1010/1404]  eta: 0:03:53  lr: 0.000090  min_lr: 0.000001  loss: 4.4880 (4.4066)  class_acc: 0.2083 (0.2201)  loss_scale: 32768.0000 (31617.3927)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1047  max mem: 15572
[2025-01-16 22:36:02,820] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13649
[2025-01-16 22:36:02,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:36:02,821] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 22:36:02,832] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13649
[2025-01-16 22:36:02,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [9]  [1020/1404]  eta: 0:03:47  lr: 0.000090  min_lr: 0.000001  loss: 4.3113 (4.4052)  class_acc: 0.2500 (0.2205)  loss_scale: 32768.0000 (31500.2860)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1259  max mem: 15572
Epoch: [9]  [1030/1404]  eta: 0:03:41  lr: 0.000090  min_lr: 0.000001  loss: 4.4357 (4.4058)  class_acc: 0.2083 (0.2202)  loss_scale: 16384.0000 (31353.6683)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0621  max mem: 15572
Epoch: [9]  [1040/1404]  eta: 0:03:34  lr: 0.000090  min_lr: 0.000001  loss: 4.5446 (4.4064)  class_acc: 0.1667 (0.2196)  loss_scale: 16384.0000 (31209.8674)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0008  max mem: 15572
Epoch: [9]  [1050/1404]  eta: 0:03:28  lr: 0.000090  min_lr: 0.000001  loss: 4.4519 (4.4066)  class_acc: 0.1667 (0.2196)  loss_scale: 16384.0000 (31068.8030)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0274  max mem: 15572
Epoch: [9]  [1060/1404]  eta: 0:03:23  lr: 0.000090  min_lr: 0.000001  loss: 4.3968 (4.4069)  class_acc: 0.2083 (0.2198)  loss_scale: 16384.0000 (30930.3977)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0736  max mem: 15572
Epoch: [9]  [1070/1404]  eta: 0:03:17  lr: 0.000090  min_lr: 0.000001  loss: 4.3581 (4.4063)  class_acc: 0.2083 (0.2200)  loss_scale: 16384.0000 (30794.5770)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.0580  max mem: 15572
Epoch: [9]  [1080/1404]  eta: 0:03:11  lr: 0.000090  min_lr: 0.000001  loss: 4.4542 (4.4073)  class_acc: 0.2083 (0.2203)  loss_scale: 16384.0000 (30661.2692)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0294  max mem: 15572
Epoch: [9]  [1090/1404]  eta: 0:03:05  lr: 0.000090  min_lr: 0.000001  loss: 4.4718 (4.4075)  class_acc: 0.2083 (0.2204)  loss_scale: 16384.0000 (30530.4051)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.1307  max mem: 15572
Epoch: [9]  [1100/1404]  eta: 0:02:59  lr: 0.000089  min_lr: 0.000001  loss: 4.4009 (4.4083)  class_acc: 0.2500 (0.2205)  loss_scale: 16384.0000 (30401.9183)  weight_decay: 0.0500 (0.0500)  time: 0.6690  data: 0.1781  max mem: 15572
Epoch: [9]  [1110/1404]  eta: 0:02:53  lr: 0.000089  min_lr: 0.000001  loss: 4.4009 (4.4094)  class_acc: 0.2083 (0.2204)  loss_scale: 16384.0000 (30275.7444)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0658  max mem: 15572
Epoch: [9]  [1120/1404]  eta: 0:02:47  lr: 0.000089  min_lr: 0.000001  loss: 4.5727 (4.4111)  class_acc: 0.2083 (0.2202)  loss_scale: 16384.0000 (30151.8216)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0138  max mem: 15572
Epoch: [9]  [1130/1404]  eta: 0:02:41  lr: 0.000089  min_lr: 0.000001  loss: 4.5477 (4.4109)  class_acc: 0.2083 (0.2201)  loss_scale: 16384.0000 (30030.0902)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.0502  max mem: 15572
Epoch: [9]  [1140/1404]  eta: 0:02:35  lr: 0.000089  min_lr: 0.000001  loss: 4.3777 (4.4114)  class_acc: 0.1667 (0.2194)  loss_scale: 16384.0000 (29910.4926)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0817  max mem: 15572
[2025-01-16 22:37:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:37:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:37:17,919] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:37:17,920] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [9]  [1150/1404]  eta: 0:02:30  lr: 0.000089  min_lr: 0.000001  loss: 4.3838 (4.4114)  class_acc: 0.1667 (0.2193)  loss_scale: 16384.0000 (29921.0843)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0962  max mem: 15572
Epoch: [9]  [1160/1404]  eta: 0:02:24  lr: 0.000089  min_lr: 0.000001  loss: 4.4798 (4.4121)  class_acc: 0.1667 (0.2192)  loss_scale: 32768.0000 (29945.6055)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.0763  max mem: 15572
Epoch: [9]  [1170/1404]  eta: 0:02:18  lr: 0.000089  min_lr: 0.000001  loss: 4.5200 (4.4123)  class_acc: 0.2083 (0.2199)  loss_scale: 32768.0000 (29969.7079)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0619  max mem: 15572
Epoch: [9]  [1180/1404]  eta: 0:02:12  lr: 0.000089  min_lr: 0.000001  loss: 4.4440 (4.4121)  class_acc: 0.2917 (0.2203)  loss_scale: 32768.0000 (29993.4022)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1066  max mem: 15572
Epoch: [9]  [1190/1404]  eta: 0:02:06  lr: 0.000089  min_lr: 0.000001  loss: 4.4396 (4.4120)  class_acc: 0.2083 (0.2204)  loss_scale: 32768.0000 (30016.6986)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1209  max mem: 15572
Epoch: [9]  [1200/1404]  eta: 0:02:00  lr: 0.000089  min_lr: 0.000001  loss: 4.3676 (4.4115)  class_acc: 0.2083 (0.2203)  loss_scale: 32768.0000 (30039.6070)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1135  max mem: 15572
[2025-01-16 22:37:54,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13838
[2025-01-16 22:37:54,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:37:54,236] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13838
[2025-01-16 22:37:54,236] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:37:54,236] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [9]  [1210/1404]  eta: 0:01:54  lr: 0.000089  min_lr: 0.000001  loss: 4.4420 (4.4124)  class_acc: 0.1667 (0.2200)  loss_scale: 32768.0000 (29940.3732)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0737  max mem: 15572
Epoch: [9]  [1220/1404]  eta: 0:01:48  lr: 0.000089  min_lr: 0.000001  loss: 4.5380 (4.4129)  class_acc: 0.1667 (0.2200)  loss_scale: 16384.0000 (29829.3464)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0464  max mem: 15572
Epoch: [9]  [1230/1404]  eta: 0:01:42  lr: 0.000089  min_lr: 0.000001  loss: 4.4159 (4.4131)  class_acc: 0.1667 (0.2200)  loss_scale: 16384.0000 (29720.1235)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1175  max mem: 15572
Epoch: [9]  [1240/1404]  eta: 0:01:36  lr: 0.000089  min_lr: 0.000001  loss: 4.4099 (4.4127)  class_acc: 0.2083 (0.2198)  loss_scale: 16384.0000 (29612.6608)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.0917  max mem: 15572
Epoch: [9]  [1250/1404]  eta: 0:01:30  lr: 0.000089  min_lr: 0.000001  loss: 4.3495 (4.4108)  class_acc: 0.2500 (0.2202)  loss_scale: 16384.0000 (29506.9161)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0495  max mem: 15572
Epoch: [9]  [1260/1404]  eta: 0:01:25  lr: 0.000089  min_lr: 0.000001  loss: 4.4057 (4.4108)  class_acc: 0.2500 (0.2202)  loss_scale: 16384.0000 (29402.8485)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0862  max mem: 15572
Epoch: [9]  [1270/1404]  eta: 0:01:19  lr: 0.000089  min_lr: 0.000001  loss: 4.4522 (4.4112)  class_acc: 0.1667 (0.2196)  loss_scale: 16384.0000 (29300.4186)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1118  max mem: 15572
Epoch: [9]  [1280/1404]  eta: 0:01:13  lr: 0.000089  min_lr: 0.000001  loss: 4.4763 (4.4114)  class_acc: 0.1667 (0.2195)  loss_scale: 16384.0000 (29199.5878)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.0985  max mem: 15572
Epoch: [9]  [1290/1404]  eta: 0:01:07  lr: 0.000089  min_lr: 0.000001  loss: 4.3905 (4.4104)  class_acc: 0.2083 (0.2200)  loss_scale: 16384.0000 (29100.3191)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0515  max mem: 15572
Epoch: [9]  [1300/1404]  eta: 0:01:01  lr: 0.000089  min_lr: 0.000001  loss: 4.3931 (4.4108)  class_acc: 0.2083 (0.2200)  loss_scale: 16384.0000 (29002.5765)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0733  max mem: 15572
Epoch: [9]  [1310/1404]  eta: 0:00:55  lr: 0.000089  min_lr: 0.000001  loss: 4.4197 (4.4108)  class_acc: 0.2500 (0.2203)  loss_scale: 16384.0000 (28906.3249)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1016  max mem: 15572
Epoch: [9]  [1320/1404]  eta: 0:00:49  lr: 0.000089  min_lr: 0.000001  loss: 4.2909 (4.4095)  class_acc: 0.2500 (0.2205)  loss_scale: 16384.0000 (28811.5307)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0548  max mem: 15572
Epoch: [9]  [1330/1404]  eta: 0:00:43  lr: 0.000089  min_lr: 0.000001  loss: 4.3164 (4.4088)  class_acc: 0.2083 (0.2206)  loss_scale: 16384.0000 (28718.1608)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0690  max mem: 15572
[2025-01-16 22:39:08,861] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:39:08,861] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:39:08,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:39:08,863] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [9]  [1340/1404]  eta: 0:00:37  lr: 0.000089  min_lr: 0.000001  loss: 4.3171 (4.4079)  class_acc: 0.2083 (0.2205)  loss_scale: 16384.0000 (28748.3609)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.0989  max mem: 15572
[2025-01-16 22:39:16,748] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13981
[2025-01-16 22:39:16,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:39:16,748] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 22:39:16,791] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13981
[2025-01-16 22:39:16,792] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [9]  [1350/1404]  eta: 0:00:31  lr: 0.000089  min_lr: 0.000001  loss: 4.4210 (4.4080)  class_acc: 0.2083 (0.2205)  loss_scale: 32768.0000 (28705.3501)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0878  max mem: 15572
Epoch: [9]  [1360/1404]  eta: 0:00:25  lr: 0.000089  min_lr: 0.000001  loss: 4.4250 (4.4083)  class_acc: 0.2500 (0.2210)  loss_scale: 16384.0000 (28614.8185)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0865  max mem: 15572
[2025-01-16 22:39:27,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=85, lr=[8.63917353010542e-07, 8.63917353010542e-07, 1.2341676471579173e-06, 1.2341676471579173e-06, 1.763096638797025e-06, 1.763096638797025e-06, 2.51870948399575e-06, 2.51870948399575e-06, 3.5981564057082143e-06, 3.5981564057082143e-06, 5.140223436726021e-06, 5.140223436726021e-06, 7.34317633818003e-06, 7.34317633818003e-06, 1.0490251911685759e-05, 1.0490251911685759e-05, 1.4986074159551083e-05, 1.4986074159551083e-05, 2.1408677370787264e-05, 2.1408677370787264e-05, 3.058382481541038e-05, 3.058382481541038e-05, 4.3691178307729115e-05, 4.3691178307729115e-05, 6.241596901104159e-05, 6.241596901104159e-05, 8.916567001577372e-05, 8.916567001577372e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 22:39:27,495] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=47.9868259812854, CurrSamplesPerSec=50.75215498008752, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [1370/1404]  eta: 0:00:20  lr: 0.000089  min_lr: 0.000001  loss: 4.4177 (4.4079)  class_acc: 0.2500 (0.2211)  loss_scale: 16384.0000 (28525.6076)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.0619  max mem: 15572
Epoch: [9]  [1380/1404]  eta: 0:00:14  lr: 0.000089  min_lr: 0.000001  loss: 4.4232 (4.4086)  class_acc: 0.2500 (0.2214)  loss_scale: 16384.0000 (28437.6886)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0704  max mem: 15572
Epoch: [9]  [1390/1404]  eta: 0:00:08  lr: 0.000089  min_lr: 0.000001  loss: 4.3710 (4.4081)  class_acc: 0.2083 (0.2215)  loss_scale: 16384.0000 (28351.0338)  weight_decay: 0.0500 (0.0500)  time: 0.5178  data: 0.0451  max mem: 15572
Epoch: [9]  [1400/1404]  eta: 0:00:02  lr: 0.000089  min_lr: 0.000001  loss: 4.3480 (4.4084)  class_acc: 0.2500 (0.2218)  loss_scale: 16384.0000 (28265.6160)  weight_decay: 0.0500 (0.0500)  time: 0.4387  data: 0.0005  max mem: 15572
Epoch: [9]  [1403/1404]  eta: 0:00:00  lr: 0.000089  min_lr: 0.000001  loss: 4.4110 (4.4086)  class_acc: 0.2500 (0.2218)  loss_scale: 16384.0000 (28240.2279)  weight_decay: 0.0500 (0.0500)  time: 0.4241  data: 0.0004  max mem: 15572
Epoch: [9] Total time: 0:13:44 (0.5872 s / it)
Averaged stats: lr: 0.000089  min_lr: 0.000001  loss: 4.4110 (4.4082)  class_acc: 0.2500 (0.2224)  loss_scale: 16384.0000 (28240.2279)  weight_decay: 0.0500 (0.0500)
[2025-01-16 22:39:47,293] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-9 is about to be saved!
[2025-01-16 22:39:47,294] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-9/mp_rank_00_model_states.pt
[2025-01-16 22:39:47,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-9/mp_rank_00_model_states.pt...
[2025-01-16 22:39:47,294] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
[2025-01-16 22:39:47,519] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-9/mp_rank_00_model_states.pt.
[2025-01-16 22:39:47,520] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
Val:  [  0/136]  eta: 0:11:35  loss: 2.1730 (2.1730)  acc1: 66.6667 (66.6667)  acc5: 72.2222 (72.2222)  time: 5.1108  data: 4.9231  max mem: 15572
Val:  [ 10/136]  eta: 0:01:38  loss: 2.9814 (3.0218)  acc1: 38.8889 (30.8081)  acc5: 66.6667 (62.1212)  time: 0.7840  data: 0.5743  max mem: 15572
Val:  [ 20/136]  eta: 0:01:05  loss: 3.1048 (3.1112)  acc1: 22.2222 (28.3069)  acc5: 61.1111 (61.6402)  time: 0.3351  data: 0.1360  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 3.0214 (2.9049)  acc1: 33.3333 (35.6631)  acc5: 66.6667 (67.0251)  time: 0.3609  data: 0.1583  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.4304 (2.8411)  acc1: 44.4444 (36.8564)  acc5: 77.7778 (68.8347)  time: 0.3514  data: 0.1357  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 2.7859 (2.8942)  acc1: 38.8889 (37.7996)  acc5: 77.7778 (69.2810)  time: 0.3360  data: 0.1284  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 3.0093 (2.9614)  acc1: 27.7778 (35.2459)  acc5: 72.2222 (67.7596)  time: 0.3743  data: 0.1688  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.9687 (2.9399)  acc1: 38.8889 (36.8545)  acc5: 66.6667 (67.9186)  time: 0.3516  data: 0.1513  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.8313 (2.9406)  acc1: 38.8889 (36.8999)  acc5: 66.6667 (67.6955)  time: 0.3541  data: 0.1429  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.9730 (2.9426)  acc1: 38.8889 (36.3858)  acc5: 72.2222 (67.7656)  time: 0.3515  data: 0.1428  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.0367 (3.0017)  acc1: 16.6667 (34.3784)  acc5: 61.1111 (65.9516)  time: 0.3306  data: 0.1308  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.1661 (3.0076)  acc1: 16.6667 (34.2342)  acc5: 61.1111 (66.0160)  time: 0.3314  data: 0.1278  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.7746 (2.9662)  acc1: 33.3333 (35.5372)  acc5: 77.7778 (67.7227)  time: 0.3672  data: 0.1781  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.5112 (2.9283)  acc1: 55.5556 (36.9381)  acc5: 83.3333 (68.4478)  time: 0.3125  data: 0.1500  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.6623 (2.9381)  acc1: 38.8889 (36.6503)  acc5: 77.7778 (68.1409)  time: 0.2085  data: 0.0607  max mem: 15572
Val: Total time: 0:00:50 (0.3703 s / it)
* Acc@1 36.036 Acc@5 67.793 loss 2.965
Accuracy of the network on the 4883 val videos: 36.0%
[2025-01-16 22:40:37,880] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 22:40:37,881] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 22:40:37,881] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 22:40:37,881] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 22:40:40,354] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 22:40:40,354] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 36.04%
Epoch: [10]  [   0/1404]  eta: 2:50:07  lr: 0.000089  min_lr: 0.000001  loss: 4.0824 (4.0824)  class_acc: 0.0417 (0.0417)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.2705  data: 5.5057  max mem: 15572
Epoch: [10]  [  10/1404]  eta: 0:25:42  lr: 0.000089  min_lr: 0.000001  loss: 4.3336 (4.3836)  class_acc: 0.2500 (0.2765)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1069  data: 0.5010  max mem: 15572
Epoch: [10]  [  20/1404]  eta: 0:20:46  lr: 0.000089  min_lr: 0.000001  loss: 4.2594 (4.2831)  class_acc: 0.2500 (0.2500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0964  max mem: 15572
Epoch: [10]  [  30/1404]  eta: 0:18:24  lr: 0.000089  min_lr: 0.000001  loss: 4.3219 (4.3739)  class_acc: 0.2500 (0.2419)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.1554  max mem: 15572
Epoch: [10]  [  40/1404]  eta: 0:17:29  lr: 0.000089  min_lr: 0.000001  loss: 4.3697 (4.3660)  class_acc: 0.2500 (0.2378)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1456  max mem: 15572
Epoch: [10]  [  50/1404]  eta: 0:16:35  lr: 0.000089  min_lr: 0.000001  loss: 4.2629 (4.3637)  class_acc: 0.2083 (0.2263)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1503  max mem: 15572
Epoch: [10]  [  60/1404]  eta: 0:16:03  lr: 0.000089  min_lr: 0.000001  loss: 4.3229 (4.3545)  class_acc: 0.2083 (0.2240)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1366  max mem: 15572
[2025-01-16 22:41:30,473] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:41:30,473] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:41:30,479] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:41:30,479] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [10]  [  70/1404]  eta: 0:15:41  lr: 0.000089  min_lr: 0.000001  loss: 4.3510 (4.3546)  class_acc: 0.2500 (0.2359)  loss_scale: 16384.0000 (16614.7606)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1566  max mem: 15572
Epoch: [10]  [  80/1404]  eta: 0:14:56  lr: 0.000089  min_lr: 0.000001  loss: 4.3830 (4.3643)  class_acc: 0.2917 (0.2418)  loss_scale: 32768.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0845  max mem: 15572
Epoch: [10]  [  90/1404]  eta: 0:14:32  lr: 0.000089  min_lr: 0.000001  loss: 4.3852 (4.3745)  class_acc: 0.2500 (0.2413)  loss_scale: 32768.0000 (20164.9231)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0341  max mem: 15572
Epoch: [10]  [ 100/1404]  eta: 0:14:11  lr: 0.000089  min_lr: 0.000001  loss: 4.4188 (4.3841)  class_acc: 0.2083 (0.2401)  loss_scale: 32768.0000 (21412.7525)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0547  max mem: 15572
Epoch: [10]  [ 110/1404]  eta: 0:14:05  lr: 0.000089  min_lr: 0.000001  loss: 4.3034 (4.3731)  class_acc: 0.2083 (0.2380)  loss_scale: 32768.0000 (22435.7477)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0984  max mem: 15572
Epoch: [10]  [ 120/1404]  eta: 0:13:55  lr: 0.000089  min_lr: 0.000001  loss: 4.2949 (4.3752)  class_acc: 0.2083 (0.2362)  loss_scale: 32768.0000 (23289.6529)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.1352  max mem: 15572
Epoch: [10]  [ 130/1404]  eta: 0:13:36  lr: 0.000089  min_lr: 0.000001  loss: 4.4259 (4.3827)  class_acc: 0.2500 (0.2385)  loss_scale: 32768.0000 (24013.1908)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0706  max mem: 15572
Epoch: [10]  [ 140/1404]  eta: 0:13:27  lr: 0.000089  min_lr: 0.000001  loss: 4.4653 (4.3879)  class_acc: 0.2083 (0.2364)  loss_scale: 32768.0000 (24634.0993)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0751  max mem: 15572
Epoch: [10]  [ 150/1404]  eta: 0:13:16  lr: 0.000089  min_lr: 0.000001  loss: 4.4718 (4.3982)  class_acc: 0.1667 (0.2321)  loss_scale: 32768.0000 (25172.7682)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1127  max mem: 15572
Epoch: [10]  [ 160/1404]  eta: 0:13:05  lr: 0.000089  min_lr: 0.000001  loss: 4.4718 (4.4021)  class_acc: 0.2083 (0.2290)  loss_scale: 32768.0000 (25644.5217)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0872  max mem: 15572
Epoch: [10]  [ 170/1404]  eta: 0:12:54  lr: 0.000089  min_lr: 0.000001  loss: 4.3429 (4.3956)  class_acc: 0.2500 (0.2327)  loss_scale: 32768.0000 (26061.0994)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0645  max mem: 15572
Epoch: [10]  [ 180/1404]  eta: 0:12:47  lr: 0.000089  min_lr: 0.000001  loss: 4.3679 (4.3975)  class_acc: 0.3333 (0.2373)  loss_scale: 32768.0000 (26431.6464)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1008  max mem: 15572
Epoch: [10]  [ 190/1404]  eta: 0:12:42  lr: 0.000089  min_lr: 0.000001  loss: 4.3585 (4.3920)  class_acc: 0.2917 (0.2360)  loss_scale: 32768.0000 (26763.3927)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1540  max mem: 15572
[2025-01-16 22:42:44,435] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:42:44,435] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:42:44,448] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:42:44,448] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:42:44,915] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14239
[2025-01-16 22:42:44,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:42:44,919] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14239
[2025-01-16 22:42:44,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:42:44,920] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 200/1404]  eta: 0:12:28  lr: 0.000089  min_lr: 0.000001  loss: 4.3315 (4.3975)  class_acc: 0.1250 (0.2330)  loss_scale: 32768.0000 (27225.1542)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0814  max mem: 15572
Epoch: [10]  [ 210/1404]  eta: 0:12:23  lr: 0.000089  min_lr: 0.000001  loss: 4.4277 (4.4007)  class_acc: 0.2083 (0.2338)  loss_scale: 32768.0000 (27487.8483)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0847  max mem: 15572
Epoch: [10]  [ 220/1404]  eta: 0:12:10  lr: 0.000089  min_lr: 0.000001  loss: 4.4277 (4.3998)  class_acc: 0.2083 (0.2342)  loss_scale: 32768.0000 (27726.7692)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1027  max mem: 15572
Epoch: [10]  [ 230/1404]  eta: 0:12:08  lr: 0.000089  min_lr: 0.000001  loss: 4.5075 (4.4038)  class_acc: 0.2083 (0.2352)  loss_scale: 32768.0000 (27945.0043)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1245  max mem: 15572
Epoch: [10]  [ 240/1404]  eta: 0:11:56  lr: 0.000089  min_lr: 0.000001  loss: 4.4534 (4.4045)  class_acc: 0.2500 (0.2374)  loss_scale: 32768.0000 (28145.1286)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1065  max mem: 15572
Epoch: [10]  [ 250/1404]  eta: 0:11:46  lr: 0.000089  min_lr: 0.000001  loss: 4.4239 (4.4021)  class_acc: 0.2083 (0.2359)  loss_scale: 32768.0000 (28329.3068)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0007  max mem: 15572
Epoch: [10]  [ 260/1404]  eta: 0:11:40  lr: 0.000089  min_lr: 0.000001  loss: 4.3817 (4.4021)  class_acc: 0.2083 (0.2367)  loss_scale: 32768.0000 (28499.3716)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0772  max mem: 15572
Epoch: [10]  [ 270/1404]  eta: 0:11:35  lr: 0.000089  min_lr: 0.000001  loss: 4.3474 (4.3990)  class_acc: 0.2500 (0.2366)  loss_scale: 32768.0000 (28656.8856)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1411  max mem: 15572
Epoch: [10]  [ 280/1404]  eta: 0:11:25  lr: 0.000089  min_lr: 0.000001  loss: 4.2452 (4.3963)  class_acc: 0.2500 (0.2375)  loss_scale: 32768.0000 (28803.1886)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0646  max mem: 15572
Epoch: [10]  [ 290/1404]  eta: 0:11:15  lr: 0.000089  min_lr: 0.000001  loss: 4.3638 (4.3969)  class_acc: 0.2500 (0.2373)  loss_scale: 32768.0000 (28939.4364)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0006  max mem: 15572
[2025-01-16 22:43:41,233] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14339
[2025-01-16 22:43:41,234] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:43:41,237] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14339
[2025-01-16 22:43:41,238] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:43:41,238] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [10]  [ 300/1404]  eta: 0:11:08  lr: 0.000089  min_lr: 0.000001  loss: 4.4231 (4.4002)  class_acc: 0.2083 (0.2356)  loss_scale: 32768.0000 (28957.7674)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0548  max mem: 15572
Epoch: [10]  [ 310/1404]  eta: 0:10:59  lr: 0.000089  min_lr: 0.000001  loss: 4.4158 (4.4009)  class_acc: 0.2083 (0.2346)  loss_scale: 16384.0000 (28553.4662)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0603  max mem: 15572
Epoch: [10]  [ 320/1404]  eta: 0:10:51  lr: 0.000089  min_lr: 0.000001  loss: 4.3891 (4.4021)  class_acc: 0.2083 (0.2346)  loss_scale: 16384.0000 (28174.3551)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0235  max mem: 15572
Epoch: [10]  [ 330/1404]  eta: 0:10:47  lr: 0.000089  min_lr: 0.000001  loss: 4.3891 (4.4019)  class_acc: 0.2083 (0.2354)  loss_scale: 16384.0000 (27818.1511)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0911  max mem: 15572
Epoch: [10]  [ 340/1404]  eta: 0:10:42  lr: 0.000089  min_lr: 0.000001  loss: 4.4036 (4.4017)  class_acc: 0.2500 (0.2374)  loss_scale: 16384.0000 (27482.8387)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.1378  max mem: 15572
Epoch: [10]  [ 350/1404]  eta: 0:10:34  lr: 0.000089  min_lr: 0.000001  loss: 4.3267 (4.3994)  class_acc: 0.2500 (0.2374)  loss_scale: 16384.0000 (27166.6325)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0976  max mem: 15572
Epoch: [10]  [ 360/1404]  eta: 0:10:27  lr: 0.000089  min_lr: 0.000001  loss: 4.3527 (4.3997)  class_acc: 0.2500 (0.2374)  loss_scale: 16384.0000 (26867.9446)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0708  max mem: 15572
Epoch: [10]  [ 370/1404]  eta: 0:10:22  lr: 0.000089  min_lr: 0.000001  loss: 4.3954 (4.4005)  class_acc: 0.2083 (0.2363)  loss_scale: 16384.0000 (26585.3585)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1047  max mem: 15572
Epoch: [10]  [ 380/1404]  eta: 0:10:16  lr: 0.000089  min_lr: 0.000001  loss: 4.3889 (4.3986)  class_acc: 0.2500 (0.2374)  loss_scale: 16384.0000 (26317.6063)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1273  max mem: 15572
Epoch: [10]  [ 390/1404]  eta: 0:10:12  lr: 0.000089  min_lr: 0.000001  loss: 4.4401 (4.4013)  class_acc: 0.2083 (0.2366)  loss_scale: 16384.0000 (26063.5499)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1530  max mem: 15572
Epoch: [10]  [ 400/1404]  eta: 0:10:05  lr: 0.000089  min_lr: 0.000001  loss: 4.3574 (4.3981)  class_acc: 0.2083 (0.2375)  loss_scale: 16384.0000 (25822.1646)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1325  max mem: 15572
Epoch: [10]  [ 410/1404]  eta: 0:09:56  lr: 0.000089  min_lr: 0.000001  loss: 4.3008 (4.4001)  class_acc: 0.2083 (0.2368)  loss_scale: 16384.0000 (25592.5255)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0399  max mem: 15572
Epoch: [10]  [ 420/1404]  eta: 0:09:49  lr: 0.000089  min_lr: 0.000001  loss: 4.3969 (4.3993)  class_acc: 0.2500 (0.2367)  loss_scale: 16384.0000 (25373.7957)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0153  max mem: 15572
[2025-01-16 22:44:57,708] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:44:57,708] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:44:57,722] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:44:57,723] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [10]  [ 430/1404]  eta: 0:09:43  lr: 0.000089  min_lr: 0.000001  loss: 4.4019 (4.4005)  class_acc: 0.2083 (0.2365)  loss_scale: 16384.0000 (25279.2575)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0674  max mem: 15572
Epoch: [10]  [ 440/1404]  eta: 0:09:39  lr: 0.000089  min_lr: 0.000001  loss: 4.3497 (4.3976)  class_acc: 0.2083 (0.2368)  loss_scale: 32768.0000 (25449.0703)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.1552  max mem: 15572
Epoch: [10]  [ 450/1404]  eta: 0:09:33  lr: 0.000089  min_lr: 0.000001  loss: 4.3758 (4.3987)  class_acc: 0.2083 (0.2366)  loss_scale: 32768.0000 (25611.3525)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.1580  max mem: 15572
Epoch: [10]  [ 460/1404]  eta: 0:09:28  lr: 0.000088  min_lr: 0.000001  loss: 4.4305 (4.3996)  class_acc: 0.1667 (0.2359)  loss_scale: 32768.0000 (25766.5944)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1313  max mem: 15572
Epoch: [10]  [ 470/1404]  eta: 0:09:19  lr: 0.000088  min_lr: 0.000001  loss: 4.3868 (4.3990)  class_acc: 0.2083 (0.2364)  loss_scale: 32768.0000 (25915.2442)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0763  max mem: 15572
Epoch: [10]  [ 480/1404]  eta: 0:09:12  lr: 0.000088  min_lr: 0.000001  loss: 4.3676 (4.3989)  class_acc: 0.2500 (0.2362)  loss_scale: 32768.0000 (26057.7131)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0007  max mem: 15572
Epoch: [10]  [ 490/1404]  eta: 0:09:08  lr: 0.000088  min_lr: 0.000001  loss: 4.3676 (4.3957)  class_acc: 0.1667 (0.2349)  loss_scale: 32768.0000 (26194.3788)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1077  max mem: 15572
Epoch: [10]  [ 500/1404]  eta: 0:09:00  lr: 0.000088  min_lr: 0.000001  loss: 4.3407 (4.3967)  class_acc: 0.1667 (0.2356)  loss_scale: 32768.0000 (26325.5888)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1115  max mem: 15572
Epoch: [10]  [ 510/1404]  eta: 0:08:53  lr: 0.000088  min_lr: 0.000001  loss: 4.4040 (4.3956)  class_acc: 0.2083 (0.2359)  loss_scale: 32768.0000 (26451.6634)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0332  max mem: 15572
Epoch: [10]  [ 520/1404]  eta: 0:08:47  lr: 0.000088  min_lr: 0.000001  loss: 4.4772 (4.3989)  class_acc: 0.2500 (0.2362)  loss_scale: 32768.0000 (26572.8983)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0796  max mem: 15572
Epoch: [10]  [ 530/1404]  eta: 0:08:41  lr: 0.000088  min_lr: 0.000001  loss: 4.4772 (4.3985)  class_acc: 0.2083 (0.2369)  loss_scale: 32768.0000 (26689.5669)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0907  max mem: 15572
Epoch: [10]  [ 540/1404]  eta: 0:08:34  lr: 0.000088  min_lr: 0.000001  loss: 4.2779 (4.3978)  class_acc: 0.2083 (0.2367)  loss_scale: 32768.0000 (26801.9224)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0841  max mem: 15572
Epoch: [10]  [ 550/1404]  eta: 0:08:30  lr: 0.000088  min_lr: 0.000001  loss: 4.4011 (4.3978)  class_acc: 0.2083 (0.2371)  loss_scale: 32768.0000 (26910.1996)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1363  max mem: 15572
[2025-01-16 22:46:12,823] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:46:12,823] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:46:12,826] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:46:12,827] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:46:13,307] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14597
[2025-01-16 22:46:13,308] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:46:13,308] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:46:13,339] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14597
[2025-01-16 22:46:13,340] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [10]  [ 560/1404]  eta: 0:08:22  lr: 0.000088  min_lr: 0.000001  loss: 4.2084 (4.3962)  class_acc: 0.2500 (0.2369)  loss_scale: 32768.0000 (27073.0267)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0966  max mem: 15572
Epoch: [10]  [ 570/1404]  eta: 0:08:17  lr: 0.000088  min_lr: 0.000001  loss: 4.5208 (4.4007)  class_acc: 0.2083 (0.2364)  loss_scale: 32768.0000 (27172.7636)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0830  max mem: 15572
Epoch: [10]  [ 580/1404]  eta: 0:08:11  lr: 0.000088  min_lr: 0.000001  loss: 4.4617 (4.4003)  class_acc: 0.2083 (0.2361)  loss_scale: 32768.0000 (27269.0671)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1329  max mem: 15572
Epoch: [10]  [ 590/1404]  eta: 0:08:04  lr: 0.000088  min_lr: 0.000001  loss: 4.4192 (4.4004)  class_acc: 0.2083 (0.2356)  loss_scale: 32768.0000 (27362.1117)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0738  max mem: 15572
Epoch: [10]  [ 600/1404]  eta: 0:07:59  lr: 0.000088  min_lr: 0.000001  loss: 4.4453 (4.3981)  class_acc: 0.2500 (0.2366)  loss_scale: 32768.0000 (27452.0599)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0956  max mem: 15572
Epoch: [10]  [ 610/1404]  eta: 0:07:52  lr: 0.000088  min_lr: 0.000001  loss: 4.4690 (4.4006)  class_acc: 0.2500 (0.2364)  loss_scale: 32768.0000 (27539.0638)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0764  max mem: 15572
Epoch: [10]  [ 620/1404]  eta: 0:07:46  lr: 0.000088  min_lr: 0.000001  loss: 4.5890 (4.4014)  class_acc: 0.2083 (0.2360)  loss_scale: 32768.0000 (27623.2657)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0529  max mem: 15572
Epoch: [10]  [ 630/1404]  eta: 0:07:40  lr: 0.000088  min_lr: 0.000001  loss: 4.2747 (4.3988)  class_acc: 0.2500 (0.2369)  loss_scale: 32768.0000 (27704.7987)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1157  max mem: 15572
Epoch: [10]  [ 640/1404]  eta: 0:07:34  lr: 0.000088  min_lr: 0.000001  loss: 4.1727 (4.3965)  class_acc: 0.2500 (0.2366)  loss_scale: 32768.0000 (27783.7878)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0983  max mem: 15572
Epoch: [10]  [ 650/1404]  eta: 0:07:27  lr: 0.000088  min_lr: 0.000001  loss: 4.2715 (4.3969)  class_acc: 0.2500 (0.2365)  loss_scale: 32768.0000 (27860.3502)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0796  max mem: 15572
Epoch: [10]  [ 660/1404]  eta: 0:07:21  lr: 0.000088  min_lr: 0.000001  loss: 4.2396 (4.3929)  class_acc: 0.2500 (0.2372)  loss_scale: 32768.0000 (27934.5961)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0559  max mem: 15572
Epoch: [10]  [ 670/1404]  eta: 0:07:14  lr: 0.000088  min_lr: 0.000001  loss: 4.2486 (4.3927)  class_acc: 0.2500 (0.2380)  loss_scale: 32768.0000 (28006.6289)  weight_decay: 0.0500 (0.0500)  time: 0.5158  data: 0.0116  max mem: 15572
Epoch: [10]  [ 680/1404]  eta: 0:07:09  lr: 0.000088  min_lr: 0.000001  loss: 4.3846 (4.3916)  class_acc: 0.2083 (0.2377)  loss_scale: 32768.0000 (28076.5463)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1134  max mem: 15572
[2025-01-16 22:47:27,932] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:47:27,933] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:47:27,934] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:47:27,934] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:47:29,335] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14729
[2025-01-16 22:47:29,336] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:47:29,336] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 22:47:29,344] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14729
[2025-01-16 22:47:29,344] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [10]  [ 690/1404]  eta: 0:07:02  lr: 0.000088  min_lr: 0.000001  loss: 4.3557 (4.3907)  class_acc: 0.1667 (0.2373)  loss_scale: 32768.0000 (28286.7033)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1134  max mem: 15572
Epoch: [10]  [ 700/1404]  eta: 0:06:57  lr: 0.000088  min_lr: 0.000001  loss: 4.3942 (4.3916)  class_acc: 0.2500 (0.2375)  loss_scale: 32768.0000 (28350.6305)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0058  max mem: 15572
Epoch: [10]  [ 710/1404]  eta: 0:06:51  lr: 0.000088  min_lr: 0.000001  loss: 4.3305 (4.3902)  class_acc: 0.2500 (0.2376)  loss_scale: 32768.0000 (28412.7595)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0335  max mem: 15572
Epoch: [10]  [ 720/1404]  eta: 0:06:45  lr: 0.000088  min_lr: 0.000001  loss: 4.2334 (4.3921)  class_acc: 0.2500 (0.2380)  loss_scale: 32768.0000 (28473.1650)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.0283  max mem: 15572
Epoch: [10]  [ 730/1404]  eta: 0:06:39  lr: 0.000088  min_lr: 0.000001  loss: 4.4786 (4.3930)  class_acc: 0.2500 (0.2384)  loss_scale: 32768.0000 (28531.9179)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0084  max mem: 15572
Epoch: [10]  [ 740/1404]  eta: 0:06:34  lr: 0.000088  min_lr: 0.000001  loss: 4.3041 (4.3912)  class_acc: 0.2500 (0.2382)  loss_scale: 32768.0000 (28589.0850)  weight_decay: 0.0500 (0.0500)  time: 0.6523  data: 0.0083  max mem: 15572
Epoch: [10]  [ 750/1404]  eta: 0:06:27  lr: 0.000088  min_lr: 0.000001  loss: 4.3041 (4.3932)  class_acc: 0.2083 (0.2382)  loss_scale: 32768.0000 (28644.7297)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0005  max mem: 15572
Epoch: [10]  [ 760/1404]  eta: 0:06:20  lr: 0.000088  min_lr: 0.000001  loss: 4.5074 (4.3954)  class_acc: 0.2083 (0.2377)  loss_scale: 32768.0000 (28698.9120)  weight_decay: 0.0500 (0.0500)  time: 0.4973  data: 0.0005  max mem: 15572
Epoch: [10]  [ 770/1404]  eta: 0:06:14  lr: 0.000088  min_lr: 0.000001  loss: 4.3876 (4.3932)  class_acc: 0.2500 (0.2381)  loss_scale: 32768.0000 (28751.6887)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0006  max mem: 15572
Epoch: [10]  [ 780/1404]  eta: 0:06:08  lr: 0.000088  min_lr: 0.000001  loss: 4.2519 (4.3926)  class_acc: 0.2500 (0.2379)  loss_scale: 32768.0000 (28803.1140)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0088  max mem: 15572
[2025-01-16 22:48:21,856] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14821
[2025-01-16 22:48:21,856] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:48:21,857] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 22:48:21,862] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14821
[2025-01-16 22:48:21,863] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [10]  [ 790/1404]  eta: 0:06:01  lr: 0.000088  min_lr: 0.000001  loss: 4.3430 (4.3907)  class_acc: 0.2083 (0.2373)  loss_scale: 16384.0000 (28646.1087)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0089  max mem: 15572
Epoch: [10]  [ 800/1404]  eta: 0:05:55  lr: 0.000088  min_lr: 0.000001  loss: 4.3430 (4.3910)  class_acc: 0.1667 (0.2370)  loss_scale: 16384.0000 (28493.0237)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0009  max mem: 15572
Epoch: [10]  [ 810/1404]  eta: 0:05:50  lr: 0.000088  min_lr: 0.000001  loss: 4.3965 (4.3917)  class_acc: 0.2083 (0.2368)  loss_scale: 16384.0000 (28343.7139)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0008  max mem: 15572
Epoch: [10]  [ 820/1404]  eta: 0:05:44  lr: 0.000088  min_lr: 0.000001  loss: 4.3690 (4.3898)  class_acc: 0.2500 (0.2371)  loss_scale: 16384.0000 (28198.0414)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0010  max mem: 15572
Epoch: [10]  [ 830/1404]  eta: 0:05:38  lr: 0.000088  min_lr: 0.000001  loss: 4.2075 (4.3888)  class_acc: 0.2083 (0.2369)  loss_scale: 16384.0000 (28055.8748)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0010  max mem: 15572
Epoch: [10]  [ 840/1404]  eta: 0:05:33  lr: 0.000088  min_lr: 0.000001  loss: 4.3489 (4.3892)  class_acc: 0.1667 (0.2368)  loss_scale: 16384.0000 (27917.0892)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.0735  max mem: 15572
Epoch: [10]  [ 850/1404]  eta: 0:05:27  lr: 0.000088  min_lr: 0.000001  loss: 4.3489 (4.3882)  class_acc: 0.1667 (0.2366)  loss_scale: 16384.0000 (27781.5652)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.1401  max mem: 15572
Epoch: [10]  [ 860/1404]  eta: 0:05:21  lr: 0.000088  min_lr: 0.000001  loss: 4.3007 (4.3871)  class_acc: 0.1667 (0.2365)  loss_scale: 16384.0000 (27649.1893)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1464  max mem: 15572
Epoch: [10]  [ 870/1404]  eta: 0:05:16  lr: 0.000088  min_lr: 0.000001  loss: 4.1406 (4.3841)  class_acc: 0.2500 (0.2368)  loss_scale: 16384.0000 (27519.8530)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1353  max mem: 15572
Epoch: [10]  [ 880/1404]  eta: 0:05:09  lr: 0.000088  min_lr: 0.000001  loss: 4.2359 (4.3845)  class_acc: 0.2500 (0.2370)  loss_scale: 16384.0000 (27393.4529)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0839  max mem: 15572
Epoch: [10]  [ 890/1404]  eta: 0:05:04  lr: 0.000088  min_lr: 0.000001  loss: 4.4274 (4.3853)  class_acc: 0.2500 (0.2369)  loss_scale: 16384.0000 (27269.8900)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0963  max mem: 15572
Epoch: [10]  [ 900/1404]  eta: 0:04:58  lr: 0.000088  min_lr: 0.000001  loss: 4.5779 (4.3871)  class_acc: 0.2083 (0.2368)  loss_scale: 16384.0000 (27149.0699)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1273  max mem: 15572
[2025-01-16 22:49:39,430] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:49:39,430] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:49:39,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:49:39,431] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [10]  [ 910/1404]  eta: 0:04:52  lr: 0.000088  min_lr: 0.000001  loss: 4.3416 (4.3866)  class_acc: 0.2500 (0.2371)  loss_scale: 16384.0000 (27048.8869)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0882  max mem: 15572
Epoch: [10]  [ 920/1404]  eta: 0:04:46  lr: 0.000088  min_lr: 0.000001  loss: 4.3409 (4.3867)  class_acc: 0.2500 (0.2366)  loss_scale: 32768.0000 (27110.9837)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0989  max mem: 15572
Epoch: [10]  [ 930/1404]  eta: 0:04:40  lr: 0.000088  min_lr: 0.000001  loss: 4.4379 (4.3865)  class_acc: 0.2500 (0.2374)  loss_scale: 32768.0000 (27171.7465)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1348  max mem: 15572
Epoch: [10]  [ 940/1404]  eta: 0:04:34  lr: 0.000088  min_lr: 0.000001  loss: 4.2968 (4.3846)  class_acc: 0.2500 (0.2370)  loss_scale: 32768.0000 (27231.2179)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0654  max mem: 15572
Epoch: [10]  [ 950/1404]  eta: 0:04:27  lr: 0.000088  min_lr: 0.000001  loss: 4.2671 (4.3842)  class_acc: 0.2083 (0.2371)  loss_scale: 32768.0000 (27289.4385)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0005  max mem: 15572
[2025-01-16 22:50:07,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=90, lr=[8.505727956044617e-07, 8.505727956044617e-07, 1.2151039937206597e-06, 1.2151039937206597e-06, 1.7358628481723713e-06, 1.7358628481723713e-06, 2.4798040688176734e-06, 2.4798040688176734e-06, 3.542577241168105e-06, 3.542577241168105e-06, 5.0608246302401504e-06, 5.0608246302401504e-06, 7.229749471771644e-06, 7.229749471771644e-06, 1.032821353110235e-05, 1.032821353110235e-05, 1.4754590758717642e-05, 1.4754590758717642e-05, 2.1077986798168063e-05, 2.1077986798168063e-05, 3.011140971166866e-05, 3.011140971166866e-05, 4.3016299588098086e-05, 4.3016299588098086e-05, 6.145185655442585e-05, 6.145185655442585e-05, 8.778836650632264e-05, 8.778836650632264e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 22:50:07,707] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=48.158665583480506, CurrSamplesPerSec=61.25098329374197, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [10]  [ 960/1404]  eta: 0:04:22  lr: 0.000088  min_lr: 0.000001  loss: 4.3831 (4.3837)  class_acc: 0.2083 (0.2367)  loss_scale: 32768.0000 (27346.4475)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0647  max mem: 15572
Epoch: [10]  [ 970/1404]  eta: 0:04:16  lr: 0.000088  min_lr: 0.000001  loss: 4.4166 (4.3840)  class_acc: 0.2083 (0.2363)  loss_scale: 32768.0000 (27402.2822)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.0930  max mem: 15572
Epoch: [10]  [ 980/1404]  eta: 0:04:10  lr: 0.000088  min_lr: 0.000001  loss: 4.3742 (4.3827)  class_acc: 0.2083 (0.2359)  loss_scale: 32768.0000 (27456.9786)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0781  max mem: 15572
Epoch: [10]  [ 990/1404]  eta: 0:04:04  lr: 0.000088  min_lr: 0.000001  loss: 4.3460 (4.3826)  class_acc: 0.2083 (0.2359)  loss_scale: 32768.0000 (27510.5711)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0524  max mem: 15572
Epoch: [10]  [1000/1404]  eta: 0:03:58  lr: 0.000088  min_lr: 0.000001  loss: 4.3708 (4.3823)  class_acc: 0.2500 (0.2363)  loss_scale: 32768.0000 (27563.0929)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0712  max mem: 15572
[2025-01-16 22:50:37,347] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15050
[2025-01-16 22:50:37,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:50:37,348] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [10]  [1010/1404]  eta: 0:03:52  lr: 0.000088  min_lr: 0.000001  loss: 4.4478 (4.3824)  class_acc: 0.2500 (0.2364)  loss_scale: 32768.0000 (27598.3699)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.1365  max mem: 15572
[2025-01-16 22:50:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15050
[2025-01-16 22:50:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [10]  [1020/1404]  eta: 0:03:46  lr: 0.000088  min_lr: 0.000001  loss: 4.4478 (4.3827)  class_acc: 0.2083 (0.2361)  loss_scale: 16384.0000 (27488.5328)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0776  max mem: 15572
Epoch: [10]  [1030/1404]  eta: 0:03:40  lr: 0.000088  min_lr: 0.000001  loss: 4.3119 (4.3809)  class_acc: 0.2083 (0.2359)  loss_scale: 16384.0000 (27380.8264)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0789  max mem: 15572
Epoch: [10]  [1040/1404]  eta: 0:03:34  lr: 0.000088  min_lr: 0.000001  loss: 4.2635 (4.3804)  class_acc: 0.2083 (0.2358)  loss_scale: 16384.0000 (27275.1892)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1318  max mem: 15572
Epoch: [10]  [1050/1404]  eta: 0:03:28  lr: 0.000088  min_lr: 0.000001  loss: 4.4861 (4.3822)  class_acc: 0.2500 (0.2365)  loss_scale: 16384.0000 (27171.5623)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0785  max mem: 15572
Epoch: [10]  [1060/1404]  eta: 0:03:22  lr: 0.000088  min_lr: 0.000001  loss: 4.3232 (4.3803)  class_acc: 0.2500 (0.2364)  loss_scale: 16384.0000 (27069.8888)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0543  max mem: 15572
Epoch: [10]  [1070/1404]  eta: 0:03:17  lr: 0.000088  min_lr: 0.000001  loss: 4.2057 (4.3802)  class_acc: 0.2500 (0.2367)  loss_scale: 16384.0000 (26970.1139)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0989  max mem: 15572
Epoch: [10]  [1080/1404]  eta: 0:03:11  lr: 0.000088  min_lr: 0.000001  loss: 4.2716 (4.3789)  class_acc: 0.2500 (0.2371)  loss_scale: 16384.0000 (26872.1850)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1091  max mem: 15572
Epoch: [10]  [1090/1404]  eta: 0:03:05  lr: 0.000088  min_lr: 0.000001  loss: 4.2403 (4.3784)  class_acc: 0.2500 (0.2374)  loss_scale: 16384.0000 (26776.0513)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0964  max mem: 15572
Epoch: [10]  [1100/1404]  eta: 0:02:59  lr: 0.000088  min_lr: 0.000001  loss: 4.2198 (4.3779)  class_acc: 0.2083 (0.2373)  loss_scale: 16384.0000 (26681.6639)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1080  max mem: 15572
Epoch: [10]  [1110/1404]  eta: 0:02:53  lr: 0.000088  min_lr: 0.000001  loss: 4.4173 (4.3790)  class_acc: 0.1667 (0.2368)  loss_scale: 16384.0000 (26588.9757)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1558  max mem: 15572
Epoch: [10]  [1120/1404]  eta: 0:02:47  lr: 0.000088  min_lr: 0.000001  loss: 4.4547 (4.3781)  class_acc: 0.2083 (0.2370)  loss_scale: 16384.0000 (26497.9411)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0962  max mem: 15572
Epoch: [10]  [1130/1404]  eta: 0:02:41  lr: 0.000088  min_lr: 0.000001  loss: 4.3949 (4.3786)  class_acc: 0.2083 (0.2368)  loss_scale: 16384.0000 (26408.5164)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0560  max mem: 15572
[2025-01-16 22:51:53,851] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:51:53,851] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:51:53,851] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:51:53,851] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [10]  [1140/1404]  eta: 0:02:35  lr: 0.000088  min_lr: 0.000001  loss: 4.4610 (4.3790)  class_acc: 0.2083 (0.2368)  loss_scale: 16384.0000 (26349.3777)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.1352  max mem: 15572
Epoch: [10]  [1150/1404]  eta: 0:02:29  lr: 0.000088  min_lr: 0.000001  loss: 4.4265 (4.3793)  class_acc: 0.1667 (0.2366)  loss_scale: 32768.0000 (26405.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0797  max mem: 15572
Epoch: [10]  [1160/1404]  eta: 0:02:23  lr: 0.000087  min_lr: 0.000001  loss: 4.4145 (4.3802)  class_acc: 0.2500 (0.2370)  loss_scale: 32768.0000 (26459.9483)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0007  max mem: 15572
Epoch: [10]  [1170/1404]  eta: 0:02:17  lr: 0.000087  min_lr: 0.000001  loss: 4.4918 (4.3807)  class_acc: 0.2917 (0.2377)  loss_scale: 32768.0000 (26513.8173)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0414  max mem: 15572
Epoch: [10]  [1180/1404]  eta: 0:02:11  lr: 0.000087  min_lr: 0.000001  loss: 4.4839 (4.3818)  class_acc: 0.2500 (0.2375)  loss_scale: 32768.0000 (26566.7739)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0916  max mem: 15572
Epoch: [10]  [1190/1404]  eta: 0:02:06  lr: 0.000087  min_lr: 0.000001  loss: 4.3922 (4.3813)  class_acc: 0.2083 (0.2376)  loss_scale: 32768.0000 (26618.8413)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1272  max mem: 15572
Epoch: [10]  [1200/1404]  eta: 0:02:00  lr: 0.000087  min_lr: 0.000001  loss: 4.3742 (4.3812)  class_acc: 0.2083 (0.2374)  loss_scale: 32768.0000 (26670.0416)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1360  max mem: 15572
Epoch: [10]  [1210/1404]  eta: 0:01:54  lr: 0.000087  min_lr: 0.000001  loss: 4.4301 (4.3815)  class_acc: 0.2083 (0.2374)  loss_scale: 32768.0000 (26720.3964)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0850  max mem: 15572
Epoch: [10]  [1220/1404]  eta: 0:01:48  lr: 0.000087  min_lr: 0.000001  loss: 4.3058 (4.3803)  class_acc: 0.2083 (0.2374)  loss_scale: 32768.0000 (26769.9263)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0852  max mem: 15572
Epoch: [10]  [1230/1404]  eta: 0:01:42  lr: 0.000087  min_lr: 0.000001  loss: 4.2341 (4.3790)  class_acc: 0.2500 (0.2375)  loss_scale: 32768.0000 (26818.6515)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1173  max mem: 15572
Epoch: [10]  [1240/1404]  eta: 0:01:36  lr: 0.000087  min_lr: 0.000001  loss: 4.3312 (4.3793)  class_acc: 0.2083 (0.2369)  loss_scale: 32768.0000 (26866.5915)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.0889  max mem: 15572
Epoch: [10]  [1250/1404]  eta: 0:01:30  lr: 0.000087  min_lr: 0.000001  loss: 4.4258 (4.3794)  class_acc: 0.1667 (0.2366)  loss_scale: 32768.0000 (26913.7650)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0631  max mem: 15572
[2025-01-16 22:53:00,340] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15295
[2025-01-16 22:53:00,340] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:53:00,340] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 22:53:00,455] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15295
[2025-01-16 22:53:00,455] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [10]  [1260/1404]  eta: 0:01:24  lr: 0.000087  min_lr: 0.000001  loss: 4.3857 (4.3796)  class_acc: 0.1667 (0.2364)  loss_scale: 32768.0000 (26882.2331)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0790  max mem: 15572
Epoch: [10]  [1270/1404]  eta: 0:01:18  lr: 0.000087  min_lr: 0.000001  loss: 4.4288 (4.3794)  class_acc: 0.1667 (0.2363)  loss_scale: 16384.0000 (26799.6349)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0982  max mem: 15572
Epoch: [10]  [1280/1404]  eta: 0:01:12  lr: 0.000087  min_lr: 0.000001  loss: 4.4351 (4.3791)  class_acc: 0.1667 (0.2360)  loss_scale: 16384.0000 (26718.3263)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1074  max mem: 15572
Epoch: [10]  [1290/1404]  eta: 0:01:07  lr: 0.000087  min_lr: 0.000001  loss: 4.3394 (4.3784)  class_acc: 0.2083 (0.2364)  loss_scale: 16384.0000 (26638.2773)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1407  max mem: 15572
Epoch: [10]  [1300/1404]  eta: 0:01:01  lr: 0.000087  min_lr: 0.000001  loss: 4.2633 (4.3783)  class_acc: 0.2083 (0.2366)  loss_scale: 16384.0000 (26559.4589)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1210  max mem: 15572
Epoch: [10]  [1310/1404]  eta: 0:00:55  lr: 0.000087  min_lr: 0.000001  loss: 4.2791 (4.3775)  class_acc: 0.1667 (0.2365)  loss_scale: 16384.0000 (26481.8429)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1204  max mem: 15572
Epoch: [10]  [1320/1404]  eta: 0:00:49  lr: 0.000087  min_lr: 0.000001  loss: 4.3127 (4.3773)  class_acc: 0.2083 (0.2364)  loss_scale: 16384.0000 (26405.4020)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1019  max mem: 15572
Epoch: [10]  [1330/1404]  eta: 0:00:43  lr: 0.000087  min_lr: 0.000001  loss: 4.3976 (4.3773)  class_acc: 0.2083 (0.2364)  loss_scale: 16384.0000 (26330.1097)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0288  max mem: 15572
Epoch: [10]  [1340/1404]  eta: 0:00:37  lr: 0.000087  min_lr: 0.000001  loss: 4.3976 (4.3779)  class_acc: 0.2083 (0.2361)  loss_scale: 16384.0000 (26255.9403)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0609  max mem: 15572
Epoch: [10]  [1350/1404]  eta: 0:00:31  lr: 0.000087  min_lr: 0.000001  loss: 4.4307 (4.3778)  class_acc: 0.2083 (0.2365)  loss_scale: 16384.0000 (26182.8690)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1147  max mem: 15572
Epoch: [10]  [1360/1404]  eta: 0:00:25  lr: 0.000087  min_lr: 0.000001  loss: 4.2770 (4.3763)  class_acc: 0.2083 (0.2363)  loss_scale: 16384.0000 (26110.8714)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1253  max mem: 15572
Epoch: [10]  [1370/1404]  eta: 0:00:20  lr: 0.000087  min_lr: 0.000001  loss: 4.2961 (4.3767)  class_acc: 0.2083 (0.2364)  loss_scale: 16384.0000 (26039.9241)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0862  max mem: 15572
Epoch: [10]  [1380/1404]  eta: 0:00:14  lr: 0.000087  min_lr: 0.000001  loss: 4.4818 (4.3778)  class_acc: 0.2083 (0.2362)  loss_scale: 16384.0000 (25970.0043)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0779  max mem: 15572
[2025-01-16 22:54:16,173] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:54:16,173] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:54:16,202] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:54:16,202] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [10]  [1390/1404]  eta: 0:00:08  lr: 0.000087  min_lr: 0.000001  loss: 4.4930 (4.3776)  class_acc: 0.2083 (0.2363)  loss_scale: 16384.0000 (25983.5399)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1121  max mem: 15572
Epoch: [10]  [1400/1404]  eta: 0:00:02  lr: 0.000087  min_lr: 0.000001  loss: 4.3575 (4.3778)  class_acc: 0.2083 (0.2361)  loss_scale: 32768.0000 (26031.9657)  weight_decay: 0.0500 (0.0500)  time: 0.4715  data: 0.0600  max mem: 15572
Epoch: [10]  [1403/1404]  eta: 0:00:00  lr: 0.000087  min_lr: 0.000001  loss: 4.4051 (4.3784)  class_acc: 0.2083 (0.2360)  loss_scale: 32768.0000 (26046.3590)  weight_decay: 0.0500 (0.0500)  time: 0.4551  data: 0.0600  max mem: 15572
Epoch: [10] Total time: 0:13:44 (0.5873 s / it)
Averaged stats: lr: 0.000087  min_lr: 0.000001  loss: 4.4051 (4.3802)  class_acc: 0.2083 (0.2379)  loss_scale: 32768.0000 (26046.3590)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:10:06  loss: 1.9715 (1.9715)  acc1: 72.2222 (72.2222)  acc5: 83.3333 (83.3333)  time: 4.4589  data: 4.0521  max mem: 15572
Val:  [ 10/136]  eta: 0:01:33  loss: 3.0185 (2.9728)  acc1: 27.7778 (31.8182)  acc5: 66.6667 (63.6364)  time: 0.7437  data: 0.5436  max mem: 15572
Val:  [ 20/136]  eta: 0:01:03  loss: 3.1654 (3.0358)  acc1: 27.7778 (30.6878)  acc5: 61.1111 (63.2275)  time: 0.3543  data: 0.1612  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.8473 (2.8332)  acc1: 27.7778 (36.3799)  acc5: 66.6667 (68.9964)  time: 0.3439  data: 0.1466  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.3611 (2.7861)  acc1: 44.4444 (37.8049)  acc5: 77.7778 (70.7317)  time: 0.3412  data: 0.1458  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.7731 (2.8285)  acc1: 44.4444 (37.5817)  acc5: 77.7778 (70.9150)  time: 0.3410  data: 0.1399  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.9334 (2.8914)  acc1: 33.3333 (35.7013)  acc5: 66.6667 (69.2168)  time: 0.3304  data: 0.1248  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.9748 (2.8879)  acc1: 27.7778 (35.8372)  acc5: 61.1111 (69.0923)  time: 0.3238  data: 0.1221  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.9168 (2.8897)  acc1: 27.7778 (35.3224)  acc5: 66.6667 (69.1358)  time: 0.3459  data: 0.1447  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.9346 (2.8913)  acc1: 27.7778 (34.5543)  acc5: 72.2222 (69.3529)  time: 0.3629  data: 0.1623  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.0598 (2.9420)  acc1: 16.6667 (32.6733)  acc5: 66.6667 (67.2167)  time: 0.3893  data: 0.1817  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.1626 (2.9506)  acc1: 27.7778 (32.9830)  acc5: 61.1111 (67.1672)  time: 0.3594  data: 0.1557  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.6018 (2.9063)  acc1: 44.4444 (34.8944)  acc5: 77.7778 (68.7328)  time: 0.3402  data: 0.1372  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.4478 (2.8739)  acc1: 50.0000 (36.0475)  acc5: 83.3333 (69.3808)  time: 0.3152  data: 0.1214  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.5386 (2.8756)  acc1: 44.4444 (35.9132)  acc5: 77.7778 (69.4513)  time: 0.2261  data: 0.0575  max mem: 15572
Val: Total time: 0:00:49 (0.3669 s / it)
* Acc@1 35.831 Acc@5 68.550 loss 2.905
Accuracy of the network on the 4883 val videos: 35.8%
Max accuracy: 36.04%
Epoch: [11]  [   0/1404]  eta: 3:00:35  lr: 0.000087  min_lr: 0.000001  loss: 4.3084 (4.3084)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.7177  data: 7.2046  max mem: 15572
Epoch: [11]  [  10/1404]  eta: 0:28:55  lr: 0.000087  min_lr: 0.000001  loss: 4.3084 (4.3073)  class_acc: 0.2083 (0.2311)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2448  data: 0.7485  max mem: 15572
Epoch: [11]  [  20/1404]  eta: 0:20:56  lr: 0.000087  min_lr: 0.000001  loss: 4.2280 (4.2684)  class_acc: 0.2083 (0.2460)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0844  max mem: 15572
Epoch: [11]  [  30/1404]  eta: 0:18:41  lr: 0.000087  min_lr: 0.000001  loss: 4.2166 (4.2481)  class_acc: 0.2500 (0.2487)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1128  max mem: 15572
Epoch: [11]  [  40/1404]  eta: 0:17:26  lr: 0.000087  min_lr: 0.000001  loss: 4.2306 (4.2683)  class_acc: 0.2500 (0.2470)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1472  max mem: 15572
Epoch: [11]  [  50/1404]  eta: 0:16:37  lr: 0.000087  min_lr: 0.000001  loss: 4.2846 (4.2820)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1183  max mem: 15572
Epoch: [11]  [  60/1404]  eta: 0:15:51  lr: 0.000087  min_lr: 0.000001  loss: 4.2846 (4.2991)  class_acc: 0.2500 (0.2555)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0986  max mem: 15572
Epoch: [11]  [  70/1404]  eta: 0:15:18  lr: 0.000087  min_lr: 0.000001  loss: 4.4099 (4.3067)  class_acc: 0.2500 (0.2518)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.0935  max mem: 15572
Epoch: [11]  [  80/1404]  eta: 0:14:59  lr: 0.000087  min_lr: 0.000001  loss: 4.3128 (4.3126)  class_acc: 0.2500 (0.2459)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1212  max mem: 15572
Epoch: [11]  [  90/1404]  eta: 0:14:33  lr: 0.000087  min_lr: 0.000001  loss: 4.2840 (4.3086)  class_acc: 0.2500 (0.2509)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1044  max mem: 15572
Epoch: [11]  [ 100/1404]  eta: 0:14:18  lr: 0.000087  min_lr: 0.000001  loss: 4.3776 (4.3153)  class_acc: 0.2917 (0.2508)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0836  max mem: 15572
[2025-01-16 22:56:25,400] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:56:25,401] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:56:25,417] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:56:25,418] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:56:25,943] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15553
[2025-01-16 22:56:25,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:56:26,027] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15553
[2025-01-16 22:56:26,027] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:56:26,028] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 110/1404]  eta: 0:14:00  lr: 0.000087  min_lr: 0.000001  loss: 4.5120 (4.3360)  class_acc: 0.2083 (0.2470)  loss_scale: 32768.0000 (33063.2072)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0802  max mem: 15572
Epoch: [11]  [ 120/1404]  eta: 0:13:44  lr: 0.000087  min_lr: 0.000001  loss: 4.4733 (4.3349)  class_acc: 0.1667 (0.2421)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0643  max mem: 15572
Epoch: [11]  [ 130/1404]  eta: 0:13:35  lr: 0.000087  min_lr: 0.000001  loss: 4.3824 (4.3312)  class_acc: 0.1667 (0.2405)  loss_scale: 32768.0000 (33018.1374)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1003  max mem: 15572
Epoch: [11]  [ 140/1404]  eta: 0:13:19  lr: 0.000087  min_lr: 0.000001  loss: 4.2605 (4.3274)  class_acc: 0.2500 (0.2405)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0845  max mem: 15572
Epoch: [11]  [ 150/1404]  eta: 0:13:02  lr: 0.000087  min_lr: 0.000001  loss: 4.2894 (4.3245)  class_acc: 0.2500 (0.2431)  loss_scale: 32768.0000 (32985.0066)  weight_decay: 0.0500 (0.0500)  time: 0.5184  data: 0.0478  max mem: 15572
Epoch: [11]  [ 160/1404]  eta: 0:12:51  lr: 0.000087  min_lr: 0.000001  loss: 4.3687 (4.3238)  class_acc: 0.2917 (0.2474)  loss_scale: 32768.0000 (32971.5280)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0359  max mem: 15572
Epoch: [11]  [ 170/1404]  eta: 0:12:48  lr: 0.000087  min_lr: 0.000001  loss: 4.3501 (4.3245)  class_acc: 0.2500 (0.2483)  loss_scale: 32768.0000 (32959.6257)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.0115  max mem: 15572
Epoch: [11]  [ 180/1404]  eta: 0:12:36  lr: 0.000087  min_lr: 0.000001  loss: 4.3365 (4.3288)  class_acc: 0.2083 (0.2436)  loss_scale: 32768.0000 (32949.0387)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0008  max mem: 15572
Epoch: [11]  [ 190/1404]  eta: 0:12:28  lr: 0.000087  min_lr: 0.000001  loss: 4.2812 (4.3259)  class_acc: 0.2083 (0.2450)  loss_scale: 32768.0000 (32939.5602)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0007  max mem: 15572
Epoch: [11]  [ 200/1404]  eta: 0:12:19  lr: 0.000087  min_lr: 0.000001  loss: 4.2509 (4.3282)  class_acc: 0.2500 (0.2467)  loss_scale: 32768.0000 (32931.0249)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0007  max mem: 15572
Epoch: [11]  [ 210/1404]  eta: 0:12:08  lr: 0.000087  min_lr: 0.000001  loss: 4.3844 (4.3329)  class_acc: 0.2083 (0.2449)  loss_scale: 32768.0000 (32923.2986)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0009  max mem: 15572
Epoch: [11]  [ 220/1404]  eta: 0:12:07  lr: 0.000087  min_lr: 0.000001  loss: 4.4057 (4.3358)  class_acc: 0.2083 (0.2447)  loss_scale: 32768.0000 (32916.2715)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.0007  max mem: 15572
Epoch: [11]  [ 230/1404]  eta: 0:11:57  lr: 0.000087  min_lr: 0.000001  loss: 4.4241 (4.3388)  class_acc: 0.2083 (0.2431)  loss_scale: 32768.0000 (32909.8528)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.0005  max mem: 15572
[2025-01-16 22:57:40,624] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:57:40,624] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:57:40,732] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:57:40,732] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 22:57:41,212] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15683
[2025-01-16 22:57:41,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:57:41,214] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15683
[2025-01-16 22:57:41,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 22:57:41,214] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 240/1404]  eta: 0:11:49  lr: 0.000087  min_lr: 0.000001  loss: 4.3209 (4.3331)  class_acc: 0.2083 (0.2443)  loss_scale: 32768.0000 (33039.9336)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0006  max mem: 15572
Epoch: [11]  [ 250/1404]  eta: 0:11:38  lr: 0.000087  min_lr: 0.000001  loss: 4.2582 (4.3357)  class_acc: 0.2083 (0.2430)  loss_scale: 32768.0000 (33029.0996)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0005  max mem: 15572
Epoch: [11]  [ 260/1404]  eta: 0:11:30  lr: 0.000087  min_lr: 0.000001  loss: 4.3841 (4.3380)  class_acc: 0.2083 (0.2423)  loss_scale: 32768.0000 (33019.0958)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0213  max mem: 15572
[2025-01-16 22:57:53,010] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15705
[2025-01-16 22:57:53,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:57:53,013] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15705
[2025-01-16 22:57:53,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:57:53,013] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [ 270/1404]  eta: 0:11:27  lr: 0.000087  min_lr: 0.000001  loss: 4.4109 (4.3427)  class_acc: 0.1667 (0.2406)  loss_scale: 16384.0000 (32405.2546)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1113  max mem: 15572
Epoch: [11]  [ 280/1404]  eta: 0:11:20  lr: 0.000087  min_lr: 0.000001  loss: 4.4158 (4.3421)  class_acc: 0.1667 (0.2413)  loss_scale: 16384.0000 (31835.1032)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1373  max mem: 15572
Epoch: [11]  [ 290/1404]  eta: 0:11:13  lr: 0.000087  min_lr: 0.000001  loss: 4.4004 (4.3445)  class_acc: 0.2500 (0.2410)  loss_scale: 16384.0000 (31304.1375)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1024  max mem: 15572
Epoch: [11]  [ 300/1404]  eta: 0:11:07  lr: 0.000087  min_lr: 0.000001  loss: 4.3329 (4.3427)  class_acc: 0.2083 (0.2411)  loss_scale: 16384.0000 (30808.4518)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0936  max mem: 15572
Epoch: [11]  [ 310/1404]  eta: 0:11:02  lr: 0.000087  min_lr: 0.000001  loss: 4.1865 (4.3435)  class_acc: 0.2500 (0.2405)  loss_scale: 16384.0000 (30344.6431)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1106  max mem: 15572
Epoch: [11]  [ 320/1404]  eta: 0:10:54  lr: 0.000087  min_lr: 0.000001  loss: 4.3853 (4.3420)  class_acc: 0.2500 (0.2414)  loss_scale: 16384.0000 (29909.7321)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1059  max mem: 15572
Epoch: [11]  [ 330/1404]  eta: 0:10:48  lr: 0.000087  min_lr: 0.000001  loss: 4.4230 (4.3426)  class_acc: 0.2500 (0.2421)  loss_scale: 16384.0000 (29501.0997)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0926  max mem: 15572
Epoch: [11]  [ 340/1404]  eta: 0:10:43  lr: 0.000087  min_lr: 0.000001  loss: 4.3722 (4.3435)  class_acc: 0.2083 (0.2421)  loss_scale: 16384.0000 (29116.4340)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.1335  max mem: 15572
Epoch: [11]  [ 350/1404]  eta: 0:10:35  lr: 0.000087  min_lr: 0.000001  loss: 4.4557 (4.3459)  class_acc: 0.2083 (0.2429)  loss_scale: 16384.0000 (28753.6866)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.0894  max mem: 15572
Epoch: [11]  [ 360/1404]  eta: 0:10:31  lr: 0.000087  min_lr: 0.000001  loss: 4.3874 (4.3446)  class_acc: 0.2500 (0.2452)  loss_scale: 16384.0000 (28411.0360)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0947  max mem: 15572
Epoch: [11]  [ 370/1404]  eta: 0:10:23  lr: 0.000087  min_lr: 0.000001  loss: 4.3153 (4.3463)  class_acc: 0.2500 (0.2435)  loss_scale: 16384.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1095  max mem: 15572
Epoch: [11]  [ 380/1404]  eta: 0:10:16  lr: 0.000087  min_lr: 0.000001  loss: 4.4473 (4.3519)  class_acc: 0.1667 (0.2416)  loss_scale: 16384.0000 (27779.6955)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0741  max mem: 15572
[2025-01-16 22:59:10,307] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:59:10,307] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 22:59:10,315] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 22:59:10,316] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [ 390/1404]  eta: 0:10:10  lr: 0.000087  min_lr: 0.000001  loss: 4.4495 (4.3527)  class_acc: 0.1667 (0.2413)  loss_scale: 16384.0000 (27530.1483)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0962  max mem: 15572
Epoch: [11]  [ 400/1404]  eta: 0:10:03  lr: 0.000086  min_lr: 0.000001  loss: 4.4063 (4.3552)  class_acc: 0.2083 (0.2409)  loss_scale: 32768.0000 (27660.7681)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0865  max mem: 15572
Epoch: [11]  [ 410/1404]  eta: 0:09:55  lr: 0.000086  min_lr: 0.000001  loss: 4.3917 (4.3545)  class_acc: 0.2083 (0.2411)  loss_scale: 32768.0000 (27785.0316)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0375  max mem: 15572
Epoch: [11]  [ 420/1404]  eta: 0:09:50  lr: 0.000086  min_lr: 0.000001  loss: 4.3909 (4.3571)  class_acc: 0.2500 (0.2415)  loss_scale: 32768.0000 (27903.3919)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0894  max mem: 15572
Epoch: [11]  [ 430/1404]  eta: 0:09:44  lr: 0.000086  min_lr: 0.000001  loss: 4.3433 (4.3570)  class_acc: 0.2500 (0.2416)  loss_scale: 32768.0000 (28016.2599)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.1468  max mem: 15572
[2025-01-16 22:59:36,364] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15879
[2025-01-16 22:59:36,365] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:59:36,367] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15879
[2025-01-16 22:59:36,368] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 22:59:36,368] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [ 440/1404]  eta: 0:09:37  lr: 0.000086  min_lr: 0.000001  loss: 4.3973 (4.3600)  class_acc: 0.2083 (0.2404)  loss_scale: 32768.0000 (27901.0975)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0921  max mem: 15572
Epoch: [11]  [ 450/1404]  eta: 0:09:31  lr: 0.000086  min_lr: 0.000001  loss: 4.4797 (4.3580)  class_acc: 0.2083 (0.2397)  loss_scale: 16384.0000 (27645.7295)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0529  max mem: 15572
Epoch: [11]  [ 460/1404]  eta: 0:09:24  lr: 0.000086  min_lr: 0.000001  loss: 4.2310 (4.3578)  class_acc: 0.2083 (0.2398)  loss_scale: 16384.0000 (27401.4403)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0664  max mem: 15572
Epoch: [11]  [ 470/1404]  eta: 0:09:18  lr: 0.000086  min_lr: 0.000001  loss: 4.2145 (4.3560)  class_acc: 0.2083 (0.2404)  loss_scale: 16384.0000 (27167.5244)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1065  max mem: 15572
Epoch: [11]  [ 480/1404]  eta: 0:09:13  lr: 0.000086  min_lr: 0.000001  loss: 4.2172 (4.3529)  class_acc: 0.2500 (0.2409)  loss_scale: 16384.0000 (26943.3347)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1242  max mem: 15572
Epoch: [11]  [ 490/1404]  eta: 0:09:05  lr: 0.000086  min_lr: 0.000001  loss: 4.2144 (4.3493)  class_acc: 0.2083 (0.2409)  loss_scale: 16384.0000 (26728.2770)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0702  max mem: 15572
Epoch: [11]  [ 500/1404]  eta: 0:08:58  lr: 0.000086  min_lr: 0.000001  loss: 4.2764 (4.3486)  class_acc: 0.2500 (0.2414)  loss_scale: 16384.0000 (26521.8044)  weight_decay: 0.0500 (0.0500)  time: 0.5229  data: 0.0078  max mem: 15572
Epoch: [11]  [ 510/1404]  eta: 0:08:52  lr: 0.000086  min_lr: 0.000001  loss: 4.3404 (4.3490)  class_acc: 0.2500 (0.2420)  loss_scale: 16384.0000 (26323.4129)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0415  max mem: 15572
Epoch: [11]  [ 520/1404]  eta: 0:08:47  lr: 0.000086  min_lr: 0.000001  loss: 4.4181 (4.3484)  class_acc: 0.2500 (0.2427)  loss_scale: 16384.0000 (26132.6372)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1373  max mem: 15572
Epoch: [11]  [ 530/1404]  eta: 0:08:41  lr: 0.000086  min_lr: 0.000001  loss: 4.3881 (4.3476)  class_acc: 0.2083 (0.2421)  loss_scale: 16384.0000 (25949.0471)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1272  max mem: 15572
Epoch: [11]  [ 540/1404]  eta: 0:08:34  lr: 0.000086  min_lr: 0.000001  loss: 4.1959 (4.3448)  class_acc: 0.2083 (0.2423)  loss_scale: 16384.0000 (25772.2440)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0670  max mem: 15572
Epoch: [11]  [ 550/1404]  eta: 0:08:28  lr: 0.000086  min_lr: 0.000001  loss: 4.2856 (4.3443)  class_acc: 0.2083 (0.2412)  loss_scale: 16384.0000 (25601.8584)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0929  max mem: 15572
[2025-01-16 23:00:45,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=96, lr=[8.35610443572728e-07, 8.35610443572728e-07, 1.1937292051038973e-06, 1.1937292051038973e-06, 1.7053274358627106e-06, 1.7053274358627106e-06, 2.436182051232444e-06, 2.436182051232444e-06, 3.480260073189206e-06, 3.480260073189206e-06, 4.9718001045560085e-06, 4.9718001045560085e-06, 7.102571577937155e-06, 7.102571577937155e-06, 1.0146530825624509e-05, 1.0146530825624509e-05, 1.4495044036606441e-05, 1.4495044036606441e-05, 2.070720576658063e-05, 2.070720576658063e-05, 2.9581722523686616e-05, 2.9581722523686616e-05, 4.2259603605266596e-05, 4.2259603605266596e-05, 6.0370862293238e-05, 6.0370862293238e-05, 8.624408899034001e-05, 8.624408899034001e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 23:00:45,949] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=48.33761449898292, CurrSamplesPerSec=49.57674293532382, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [ 560/1404]  eta: 0:08:22  lr: 0.000086  min_lr: 0.000001  loss: 4.4255 (4.3474)  class_acc: 0.1667 (0.2407)  loss_scale: 16384.0000 (25437.5472)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.0856  max mem: 15572
[2025-01-16 23:00:51,061] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:00:51,062] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:00:51,093] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:00:51,094] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [ 570/1404]  eta: 0:08:15  lr: 0.000086  min_lr: 0.000001  loss: 4.4582 (4.3467)  class_acc: 0.2500 (0.2420)  loss_scale: 16384.0000 (25479.8459)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0533  max mem: 15572
Epoch: [11]  [ 580/1404]  eta: 0:08:10  lr: 0.000086  min_lr: 0.000001  loss: 4.3943 (4.3475)  class_acc: 0.2917 (0.2420)  loss_scale: 32768.0000 (25605.2874)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0702  max mem: 15572
Epoch: [11]  [ 590/1404]  eta: 0:08:04  lr: 0.000086  min_lr: 0.000001  loss: 4.4293 (4.3484)  class_acc: 0.2083 (0.2415)  loss_scale: 32768.0000 (25726.4839)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0912  max mem: 15572
Epoch: [11]  [ 600/1404]  eta: 0:07:58  lr: 0.000086  min_lr: 0.000001  loss: 4.3013 (4.3469)  class_acc: 0.2083 (0.2418)  loss_scale: 32768.0000 (25843.6473)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1380  max mem: 15572
Epoch: [11]  [ 610/1404]  eta: 0:07:52  lr: 0.000086  min_lr: 0.000001  loss: 4.2185 (4.3449)  class_acc: 0.2083 (0.2415)  loss_scale: 32768.0000 (25956.9755)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1282  max mem: 15572
Epoch: [11]  [ 620/1404]  eta: 0:07:47  lr: 0.000086  min_lr: 0.000001  loss: 4.3279 (4.3467)  class_acc: 0.2083 (0.2413)  loss_scale: 32768.0000 (26066.6538)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1233  max mem: 15572
Epoch: [11]  [ 630/1404]  eta: 0:07:40  lr: 0.000086  min_lr: 0.000001  loss: 4.3779 (4.3477)  class_acc: 0.2083 (0.2412)  loss_scale: 32768.0000 (26172.8558)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1087  max mem: 15572
Epoch: [11]  [ 640/1404]  eta: 0:07:34  lr: 0.000086  min_lr: 0.000001  loss: 4.5612 (4.3498)  class_acc: 0.1667 (0.2406)  loss_scale: 32768.0000 (26275.7441)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0392  max mem: 15572
Epoch: [11]  [ 650/1404]  eta: 0:07:28  lr: 0.000086  min_lr: 0.000001  loss: 4.3749 (4.3492)  class_acc: 0.2500 (0.2421)  loss_scale: 32768.0000 (26375.4716)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0914  max mem: 15572
Epoch: [11]  [ 660/1404]  eta: 0:07:22  lr: 0.000086  min_lr: 0.000001  loss: 4.2370 (4.3476)  class_acc: 0.2917 (0.2420)  loss_scale: 32768.0000 (26472.1815)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1165  max mem: 15572
Epoch: [11]  [ 670/1404]  eta: 0:07:16  lr: 0.000086  min_lr: 0.000001  loss: 4.2777 (4.3469)  class_acc: 0.2500 (0.2427)  loss_scale: 32768.0000 (26566.0089)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0892  max mem: 15572
[2025-01-16 23:01:56,373] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16118
[2025-01-16 23:01:56,373] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:01:56,443] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16118
[2025-01-16 23:01:56,444] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:01:56,444] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [ 680/1404]  eta: 0:07:10  lr: 0.000086  min_lr: 0.000001  loss: 4.2777 (4.3458)  class_acc: 0.2500 (0.2427)  loss_scale: 32768.0000 (26488.6696)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.0962  max mem: 15572
Epoch: [11]  [ 690/1404]  eta: 0:07:04  lr: 0.000086  min_lr: 0.000001  loss: 4.3156 (4.3468)  class_acc: 0.2500 (0.2429)  loss_scale: 16384.0000 (26342.4370)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1012  max mem: 15572
Epoch: [11]  [ 700/1404]  eta: 0:06:58  lr: 0.000086  min_lr: 0.000001  loss: 4.4119 (4.3467)  class_acc: 0.2083 (0.2420)  loss_scale: 16384.0000 (26200.3766)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0716  max mem: 15572
Epoch: [11]  [ 710/1404]  eta: 0:06:52  lr: 0.000086  min_lr: 0.000001  loss: 4.3194 (4.3458)  class_acc: 0.1667 (0.2417)  loss_scale: 16384.0000 (26062.3122)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0573  max mem: 15572
Epoch: [11]  [ 720/1404]  eta: 0:06:45  lr: 0.000086  min_lr: 0.000001  loss: 4.3243 (4.3468)  class_acc: 0.2083 (0.2414)  loss_scale: 16384.0000 (25928.0777)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0656  max mem: 15572
Epoch: [11]  [ 730/1404]  eta: 0:06:40  lr: 0.000086  min_lr: 0.000001  loss: 4.2961 (4.3435)  class_acc: 0.2083 (0.2417)  loss_scale: 16384.0000 (25797.5157)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.0878  max mem: 15572
Epoch: [11]  [ 740/1404]  eta: 0:06:33  lr: 0.000086  min_lr: 0.000001  loss: 4.1095 (4.3429)  class_acc: 0.2917 (0.2424)  loss_scale: 16384.0000 (25670.4777)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0591  max mem: 15572
Epoch: [11]  [ 750/1404]  eta: 0:06:27  lr: 0.000086  min_lr: 0.000001  loss: 4.3469 (4.3436)  class_acc: 0.2500 (0.2418)  loss_scale: 16384.0000 (25546.8229)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0490  max mem: 15572
Epoch: [11]  [ 760/1404]  eta: 0:06:22  lr: 0.000086  min_lr: 0.000001  loss: 4.3469 (4.3430)  class_acc: 0.2083 (0.2416)  loss_scale: 16384.0000 (25426.4179)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1029  max mem: 15572
Epoch: [11]  [ 770/1404]  eta: 0:06:15  lr: 0.000086  min_lr: 0.000001  loss: 4.2084 (4.3407)  class_acc: 0.2500 (0.2422)  loss_scale: 16384.0000 (25309.1362)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0754  max mem: 15572
Epoch: [11]  [ 780/1404]  eta: 0:06:09  lr: 0.000086  min_lr: 0.000001  loss: 4.2223 (4.3404)  class_acc: 0.2083 (0.2419)  loss_scale: 16384.0000 (25194.8579)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0565  max mem: 15572
Epoch: [11]  [ 790/1404]  eta: 0:06:03  lr: 0.000086  min_lr: 0.000001  loss: 4.2646 (4.3398)  class_acc: 0.2083 (0.2412)  loss_scale: 16384.0000 (25083.4690)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0558  max mem: 15572
Epoch: [11]  [ 800/1404]  eta: 0:05:57  lr: 0.000086  min_lr: 0.000001  loss: 4.2662 (4.3406)  class_acc: 0.1667 (0.2408)  loss_scale: 16384.0000 (24974.8614)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0642  max mem: 15572
[2025-01-16 23:03:10,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:03:10,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:03:10,265] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:03:10,266] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [ 810/1404]  eta: 0:05:50  lr: 0.000086  min_lr: 0.000001  loss: 4.3392 (4.3409)  class_acc: 0.2500 (0.2418)  loss_scale: 16384.0000 (25030.5499)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0761  max mem: 15572
Epoch: [11]  [ 820/1404]  eta: 0:05:45  lr: 0.000086  min_lr: 0.000001  loss: 4.3392 (4.3415)  class_acc: 0.2500 (0.2419)  loss_scale: 32768.0000 (25124.7942)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.0626  max mem: 15572
Epoch: [11]  [ 830/1404]  eta: 0:05:38  lr: 0.000086  min_lr: 0.000001  loss: 4.3367 (4.3416)  class_acc: 0.2500 (0.2426)  loss_scale: 32768.0000 (25216.7702)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0556  max mem: 15572
Epoch: [11]  [ 840/1404]  eta: 0:05:33  lr: 0.000086  min_lr: 0.000001  loss: 4.3841 (4.3429)  class_acc: 0.2500 (0.2423)  loss_scale: 32768.0000 (25306.5589)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0602  max mem: 15572
Epoch: [11]  [ 850/1404]  eta: 0:05:27  lr: 0.000086  min_lr: 0.000001  loss: 4.4272 (4.3434)  class_acc: 0.2500 (0.2427)  loss_scale: 32768.0000 (25394.2374)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0608  max mem: 15572
Epoch: [11]  [ 860/1404]  eta: 0:05:21  lr: 0.000086  min_lr: 0.000001  loss: 4.3689 (4.3442)  class_acc: 0.2500 (0.2428)  loss_scale: 32768.0000 (25479.8792)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0313  max mem: 15572
Epoch: [11]  [ 870/1404]  eta: 0:05:15  lr: 0.000086  min_lr: 0.000001  loss: 4.3628 (4.3446)  class_acc: 0.2500 (0.2431)  loss_scale: 32768.0000 (25563.5545)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0476  max mem: 15572
Epoch: [11]  [ 880/1404]  eta: 0:05:09  lr: 0.000086  min_lr: 0.000001  loss: 4.3500 (4.3433)  class_acc: 0.2500 (0.2437)  loss_scale: 32768.0000 (25645.3303)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0835  max mem: 15572
Epoch: [11]  [ 890/1404]  eta: 0:05:03  lr: 0.000086  min_lr: 0.000001  loss: 4.3500 (4.3434)  class_acc: 0.2083 (0.2434)  loss_scale: 32768.0000 (25725.2705)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0414  max mem: 15572
Epoch: [11]  [ 900/1404]  eta: 0:04:57  lr: 0.000086  min_lr: 0.000001  loss: 4.2457 (4.3418)  class_acc: 0.2083 (0.2437)  loss_scale: 32768.0000 (25803.4362)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0606  max mem: 15572
Epoch: [11]  [ 910/1404]  eta: 0:04:51  lr: 0.000086  min_lr: 0.000001  loss: 4.1813 (4.3420)  class_acc: 0.2500 (0.2436)  loss_scale: 32768.0000 (25879.8858)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0948  max mem: 15572
Epoch: [11]  [ 920/1404]  eta: 0:04:46  lr: 0.000086  min_lr: 0.000001  loss: 4.3463 (4.3431)  class_acc: 0.2083 (0.2433)  loss_scale: 32768.0000 (25954.6754)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.1551  max mem: 15572
Epoch: [11]  [ 930/1404]  eta: 0:04:39  lr: 0.000086  min_lr: 0.000001  loss: 4.4611 (4.3442)  class_acc: 0.2500 (0.2432)  loss_scale: 32768.0000 (26027.8582)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1208  max mem: 15572
[2025-01-16 23:04:25,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:04:25,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 23:04:25,520] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:04:25,520] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 23:04:25,951] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16376
[2025-01-16 23:04:25,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 23:04:25,952] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 23:04:26,015] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16376
[2025-01-16 23:04:26,015] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [11]  [ 940/1404]  eta: 0:04:33  lr: 0.000086  min_lr: 0.000001  loss: 4.4411 (4.3436)  class_acc: 0.2083 (0.2433)  loss_scale: 32768.0000 (26134.3082)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0528  max mem: 15572
[2025-01-16 23:04:33,607] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16388
[2025-01-16 23:04:33,607] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:04:33,608] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16388
[2025-01-16 23:04:33,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:04:33,608] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [ 950/1404]  eta: 0:04:28  lr: 0.000086  min_lr: 0.000001  loss: 4.2838 (4.3427)  class_acc: 0.2500 (0.2435)  loss_scale: 32768.0000 (26083.4658)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0924  max mem: 15572
Epoch: [11]  [ 960/1404]  eta: 0:04:22  lr: 0.000086  min_lr: 0.000001  loss: 4.2813 (4.3419)  class_acc: 0.2500 (0.2436)  loss_scale: 16384.0000 (25982.5349)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0937  max mem: 15572
Epoch: [11]  [ 970/1404]  eta: 0:04:16  lr: 0.000086  min_lr: 0.000001  loss: 4.3835 (4.3431)  class_acc: 0.2083 (0.2437)  loss_scale: 16384.0000 (25883.6828)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1128  max mem: 15572
Epoch: [11]  [ 980/1404]  eta: 0:04:10  lr: 0.000086  min_lr: 0.000001  loss: 4.3922 (4.3419)  class_acc: 0.2500 (0.2441)  loss_scale: 16384.0000 (25786.8461)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0706  max mem: 15572
Epoch: [11]  [ 990/1404]  eta: 0:04:04  lr: 0.000086  min_lr: 0.000001  loss: 4.3477 (4.3426)  class_acc: 0.2917 (0.2440)  loss_scale: 16384.0000 (25691.9637)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0348  max mem: 15572
Epoch: [11]  [1000/1404]  eta: 0:03:58  lr: 0.000086  min_lr: 0.000001  loss: 4.3988 (4.3438)  class_acc: 0.2500 (0.2435)  loss_scale: 16384.0000 (25598.9770)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0675  max mem: 15572
Epoch: [11]  [1010/1404]  eta: 0:03:52  lr: 0.000085  min_lr: 0.000001  loss: 4.2224 (4.3400)  class_acc: 0.2500 (0.2444)  loss_scale: 16384.0000 (25507.8299)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0988  max mem: 15572
Epoch: [11]  [1020/1404]  eta: 0:03:46  lr: 0.000085  min_lr: 0.000001  loss: 4.1383 (4.3398)  class_acc: 0.2500 (0.2446)  loss_scale: 16384.0000 (25418.4682)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0955  max mem: 15572
Epoch: [11]  [1030/1404]  eta: 0:03:41  lr: 0.000085  min_lr: 0.000001  loss: 4.2467 (4.3397)  class_acc: 0.1667 (0.2439)  loss_scale: 16384.0000 (25330.8400)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.1499  max mem: 15572
Epoch: [11]  [1040/1404]  eta: 0:03:34  lr: 0.000085  min_lr: 0.000001  loss: 4.3719 (4.3404)  class_acc: 0.2083 (0.2437)  loss_scale: 16384.0000 (25244.8953)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1092  max mem: 15572
Epoch: [11]  [1050/1404]  eta: 0:03:28  lr: 0.000085  min_lr: 0.000001  loss: 4.4050 (4.3401)  class_acc: 0.2083 (0.2434)  loss_scale: 16384.0000 (25160.5861)  weight_decay: 0.0500 (0.0500)  time: 0.4945  data: 0.0005  max mem: 15572
Epoch: [11]  [1060/1404]  eta: 0:03:22  lr: 0.000085  min_lr: 0.000001  loss: 4.4050 (4.3403)  class_acc: 0.2083 (0.2430)  loss_scale: 16384.0000 (25077.8662)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0492  max mem: 15572
Epoch: [11]  [1070/1404]  eta: 0:03:16  lr: 0.000085  min_lr: 0.000001  loss: 4.4307 (4.3410)  class_acc: 0.2083 (0.2423)  loss_scale: 16384.0000 (24996.6909)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0493  max mem: 15572
[2025-01-16 23:05:47,755] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:05:47,756] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:05:47,814] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:05:47,814] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [1080/1404]  eta: 0:03:10  lr: 0.000085  min_lr: 0.000001  loss: 4.2846 (4.3408)  class_acc: 0.1667 (0.2418)  loss_scale: 16384.0000 (25038.2683)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0250  max mem: 15572
Epoch: [11]  [1090/1404]  eta: 0:03:04  lr: 0.000085  min_lr: 0.000001  loss: 4.2373 (4.3399)  class_acc: 0.1667 (0.2420)  loss_scale: 32768.0000 (25109.1182)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.0806  max mem: 15572
Epoch: [11]  [1100/1404]  eta: 0:02:58  lr: 0.000085  min_lr: 0.000001  loss: 4.2373 (4.3386)  class_acc: 0.2083 (0.2417)  loss_scale: 32768.0000 (25178.6812)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0563  max mem: 15572
Epoch: [11]  [1110/1404]  eta: 0:02:53  lr: 0.000085  min_lr: 0.000001  loss: 4.3216 (4.3395)  class_acc: 0.1667 (0.2413)  loss_scale: 32768.0000 (25246.9919)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0441  max mem: 15572
Epoch: [11]  [1120/1404]  eta: 0:02:47  lr: 0.000085  min_lr: 0.000001  loss: 4.4189 (4.3397)  class_acc: 0.2083 (0.2415)  loss_scale: 32768.0000 (25314.0839)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0863  max mem: 15572
Epoch: [11]  [1130/1404]  eta: 0:02:41  lr: 0.000085  min_lr: 0.000001  loss: 4.4531 (4.3403)  class_acc: 0.2500 (0.2417)  loss_scale: 32768.0000 (25379.9894)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1342  max mem: 15572
Epoch: [11]  [1140/1404]  eta: 0:02:35  lr: 0.000085  min_lr: 0.000001  loss: 4.4803 (4.3410)  class_acc: 0.2917 (0.2422)  loss_scale: 32768.0000 (25444.7397)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1036  max mem: 15572
Epoch: [11]  [1150/1404]  eta: 0:02:29  lr: 0.000085  min_lr: 0.000001  loss: 4.4811 (4.3422)  class_acc: 0.2083 (0.2421)  loss_scale: 32768.0000 (25508.3649)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0979  max mem: 15572
Epoch: [11]  [1160/1404]  eta: 0:02:23  lr: 0.000085  min_lr: 0.000001  loss: 4.4137 (4.3406)  class_acc: 0.2500 (0.2426)  loss_scale: 32768.0000 (25570.8941)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0996  max mem: 15572
Epoch: [11]  [1170/1404]  eta: 0:02:18  lr: 0.000085  min_lr: 0.000001  loss: 4.3352 (4.3411)  class_acc: 0.2500 (0.2429)  loss_scale: 32768.0000 (25632.3553)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1367  max mem: 15572
Epoch: [11]  [1180/1404]  eta: 0:02:12  lr: 0.000085  min_lr: 0.000001  loss: 4.3587 (4.3401)  class_acc: 0.2500 (0.2430)  loss_scale: 32768.0000 (25692.7756)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1233  max mem: 15572
Epoch: [11]  [1190/1404]  eta: 0:02:06  lr: 0.000085  min_lr: 0.000001  loss: 4.3419 (4.3410)  class_acc: 0.2083 (0.2427)  loss_scale: 32768.0000 (25752.1814)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0306  max mem: 15572
Epoch: [11]  [1200/1404]  eta: 0:02:00  lr: 0.000085  min_lr: 0.000001  loss: 4.5329 (4.3416)  class_acc: 0.2083 (0.2424)  loss_scale: 32768.0000 (25810.5978)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0768  max mem: 15572
[2025-01-16 23:07:03,589] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:07:03,589] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 23:07:03,624] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:07:03,624] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 23:07:04,951] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16647
[2025-01-16 23:07:04,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 23:07:04,957] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16647
[2025-01-16 23:07:04,957] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 23:07:04,958] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [1210/1404]  eta: 0:01:54  lr: 0.000085  min_lr: 0.000001  loss: 4.5347 (4.3423)  class_acc: 0.2083 (0.2424)  loss_scale: 32768.0000 (25922.1668)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0697  max mem: 15572
Epoch: [11]  [1220/1404]  eta: 0:01:48  lr: 0.000085  min_lr: 0.000001  loss: 4.3420 (4.3411)  class_acc: 0.2083 (0.2425)  loss_scale: 32768.0000 (25978.2342)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1064  max mem: 15572
Epoch: [11]  [1230/1404]  eta: 0:01:42  lr: 0.000085  min_lr: 0.000001  loss: 4.3164 (4.3413)  class_acc: 0.2083 (0.2425)  loss_scale: 32768.0000 (26033.3907)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0868  max mem: 15572
Epoch: [11]  [1240/1404]  eta: 0:01:36  lr: 0.000085  min_lr: 0.000001  loss: 4.3992 (4.3414)  class_acc: 0.2500 (0.2426)  loss_scale: 32768.0000 (26087.6583)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0623  max mem: 15572
Epoch: [11]  [1250/1404]  eta: 0:01:30  lr: 0.000085  min_lr: 0.000001  loss: 4.4861 (4.3434)  class_acc: 0.2500 (0.2426)  loss_scale: 32768.0000 (26141.0584)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.0591  max mem: 15572
Epoch: [11]  [1260/1404]  eta: 0:01:24  lr: 0.000085  min_lr: 0.000001  loss: 4.5129 (4.3446)  class_acc: 0.1250 (0.2415)  loss_scale: 32768.0000 (26193.6114)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0006  max mem: 15572
Epoch: [11]  [1270/1404]  eta: 0:01:18  lr: 0.000085  min_lr: 0.000001  loss: 4.4709 (4.3450)  class_acc: 0.1250 (0.2409)  loss_scale: 32768.0000 (26245.3375)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0347  max mem: 15572
Epoch: [11]  [1280/1404]  eta: 0:01:12  lr: 0.000085  min_lr: 0.000001  loss: 4.4065 (4.3446)  class_acc: 0.2083 (0.2411)  loss_scale: 32768.0000 (26296.2560)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0497  max mem: 15572
Epoch: [11]  [1290/1404]  eta: 0:01:07  lr: 0.000085  min_lr: 0.000001  loss: 4.2997 (4.3438)  class_acc: 0.2500 (0.2414)  loss_scale: 32768.0000 (26346.3857)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.1132  max mem: 15572
Epoch: [11]  [1300/1404]  eta: 0:01:01  lr: 0.000085  min_lr: 0.000001  loss: 4.3536 (4.3457)  class_acc: 0.2500 (0.2410)  loss_scale: 32768.0000 (26395.7448)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1167  max mem: 15572
Epoch: [11]  [1310/1404]  eta: 0:00:55  lr: 0.000085  min_lr: 0.000001  loss: 4.4605 (4.3458)  class_acc: 0.2500 (0.2414)  loss_scale: 32768.0000 (26444.3509)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0283  max mem: 15572
[2025-01-16 23:08:12,489] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16764
[2025-01-16 23:08:12,490] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:08:12,490] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 23:08:12,490] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16764
[2025-01-16 23:08:12,491] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [11]  [1320/1404]  eta: 0:00:49  lr: 0.000085  min_lr: 0.000001  loss: 4.4605 (4.3469)  class_acc: 0.2083 (0.2410)  loss_scale: 32768.0000 (26479.8183)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0206  max mem: 15572
Epoch: [11]  [1330/1404]  eta: 0:00:43  lr: 0.000085  min_lr: 0.000001  loss: 4.3950 (4.3467)  class_acc: 0.2083 (0.2408)  loss_scale: 16384.0000 (26403.9669)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0113  max mem: 15572
Epoch: [11]  [1340/1404]  eta: 0:00:37  lr: 0.000085  min_lr: 0.000001  loss: 4.3392 (4.3466)  class_acc: 0.2083 (0.2406)  loss_scale: 16384.0000 (26329.2468)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0006  max mem: 15572
Epoch: [11]  [1350/1404]  eta: 0:00:31  lr: 0.000085  min_lr: 0.000001  loss: 4.2992 (4.3456)  class_acc: 0.2500 (0.2407)  loss_scale: 16384.0000 (26255.6329)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0005  max mem: 15572
Epoch: [11]  [1360/1404]  eta: 0:00:25  lr: 0.000085  min_lr: 0.000001  loss: 4.2867 (4.3459)  class_acc: 0.2500 (0.2407)  loss_scale: 16384.0000 (26183.1007)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0443  max mem: 15572
Epoch: [11]  [1370/1404]  eta: 0:00:20  lr: 0.000085  min_lr: 0.000001  loss: 4.4154 (4.3461)  class_acc: 0.2500 (0.2407)  loss_scale: 16384.0000 (26111.6265)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0445  max mem: 15572
Epoch: [11]  [1380/1404]  eta: 0:00:14  lr: 0.000085  min_lr: 0.000001  loss: 4.2594 (4.3452)  class_acc: 0.2500 (0.2410)  loss_scale: 16384.0000 (26041.1875)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0009  max mem: 15572
Epoch: [11]  [1390/1404]  eta: 0:00:08  lr: 0.000085  min_lr: 0.000001  loss: 4.2606 (4.3461)  class_acc: 0.2500 (0.2412)  loss_scale: 16384.0000 (25971.7613)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.0009  max mem: 15572
Epoch: [11]  [1400/1404]  eta: 0:00:02  lr: 0.000085  min_lr: 0.000001  loss: 4.4026 (4.3462)  class_acc: 0.2083 (0.2410)  loss_scale: 16384.0000 (25903.3262)  weight_decay: 0.0500 (0.0500)  time: 0.4661  data: 0.0005  max mem: 15572
Epoch: [11]  [1403/1404]  eta: 0:00:00  lr: 0.000085  min_lr: 0.000001  loss: 4.3440 (4.3461)  class_acc: 0.2083 (0.2409)  loss_scale: 16384.0000 (25882.9858)  weight_decay: 0.0500 (0.0500)  time: 0.4194  data: 0.0004  max mem: 15572
Epoch: [11] Total time: 0:13:43 (0.5867 s / it)
Averaged stats: lr: 0.000085  min_lr: 0.000001  loss: 4.3440 (4.3542)  class_acc: 0.2083 (0.2421)  loss_scale: 16384.0000 (25882.9858)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:12:39  loss: 2.1546 (2.1546)  acc1: 55.5556 (55.5556)  acc5: 72.2222 (72.2222)  time: 5.5880  data: 5.3978  max mem: 15572
Val:  [ 10/136]  eta: 0:01:28  loss: 2.6989 (2.7877)  acc1: 33.3333 (31.3131)  acc5: 66.6667 (68.6869)  time: 0.7051  data: 0.4914  max mem: 15572
Val:  [ 20/136]  eta: 0:01:01  loss: 2.7885 (2.8971)  acc1: 27.7778 (30.9524)  acc5: 66.6667 (67.1958)  time: 0.2805  data: 0.0600  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 2.6717 (2.7532)  acc1: 38.8889 (35.4839)  acc5: 77.7778 (70.6093)  time: 0.3446  data: 0.1367  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.4080 (2.7265)  acc1: 44.4444 (36.8564)  acc5: 77.7778 (71.1382)  time: 0.3680  data: 0.1739  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.5634 (2.7397)  acc1: 44.4444 (38.6710)  acc5: 72.2222 (71.4597)  time: 0.3907  data: 0.1979  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 2.7834 (2.8158)  acc1: 38.8889 (36.8852)  acc5: 72.2222 (69.2168)  time: 0.4232  data: 0.2255  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.8271 (2.8074)  acc1: 27.7778 (37.4022)  acc5: 66.6667 (69.4836)  time: 0.3948  data: 0.1930  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.7482 (2.7998)  acc1: 27.7778 (37.4486)  acc5: 72.2222 (70.3018)  time: 0.3444  data: 0.1601  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.7482 (2.8122)  acc1: 27.7778 (36.2027)  acc5: 66.6667 (70.0244)  time: 0.3570  data: 0.1769  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.0225 (2.8534)  acc1: 22.2222 (34.8735)  acc5: 61.1111 (68.5919)  time: 0.3673  data: 0.1728  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.0428 (2.8570)  acc1: 33.3333 (35.5355)  acc5: 61.1111 (68.4685)  time: 0.4014  data: 0.2069  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.6245 (2.8205)  acc1: 38.8889 (36.5932)  acc5: 77.7778 (69.8347)  time: 0.3456  data: 0.1552  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.3668 (2.7884)  acc1: 50.0000 (37.7439)  acc5: 83.3333 (70.7379)  time: 0.2007  data: 0.0357  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.6387 (2.7997)  acc1: 38.8889 (37.5102)  acc5: 77.7778 (70.5160)  time: 0.1752  data: 0.0355  max mem: 15572
Val: Total time: 0:00:50 (0.3702 s / it)
* Acc@1 37.326 Acc@5 69.472 loss 2.841
Accuracy of the network on the 4883 val videos: 37.3%
[2025-01-16 23:09:48,955] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 23:09:48,957] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 23:09:48,957] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 23:09:48,957] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 23:09:51,387] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 23:09:51,387] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 37.33%
Epoch: [12]  [   0/1404]  eta: 2:47:59  lr: 0.000085  min_lr: 0.000001  loss: 4.9627 (4.9627)  class_acc: 0.2083 (0.2083)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.1788  data: 6.5927  max mem: 15572
Epoch: [12]  [  10/1404]  eta: 0:26:57  lr: 0.000085  min_lr: 0.000001  loss: 4.3236 (4.3366)  class_acc: 0.2083 (0.2576)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1600  data: 0.6514  max mem: 15572
Epoch: [12]  [  20/1404]  eta: 0:20:25  lr: 0.000085  min_lr: 0.000001  loss: 4.3743 (4.4038)  class_acc: 0.2917 (0.2679)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0289  max mem: 15572
Epoch: [12]  [  30/1404]  eta: 0:17:37  lr: 0.000085  min_lr: 0.000001  loss: 4.4299 (4.3855)  class_acc: 0.2500 (0.2594)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0005  max mem: 15572
Epoch: [12]  [  40/1404]  eta: 0:17:12  lr: 0.000085  min_lr: 0.000001  loss: 4.3396 (4.3762)  class_acc: 0.2500 (0.2663)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.0730  max mem: 15572
[2025-01-16 23:10:24,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:10:24,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:10:24,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:10:24,800] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [  50/1404]  eta: 0:15:59  lr: 0.000085  min_lr: 0.000001  loss: 4.2921 (4.3503)  class_acc: 0.2917 (0.2606)  loss_scale: 16384.0000 (18311.5294)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.0854  max mem: 15572
Epoch: [12]  [  60/1404]  eta: 0:15:55  lr: 0.000085  min_lr: 0.000001  loss: 4.2333 (4.3458)  class_acc: 0.2083 (0.2527)  loss_scale: 32768.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1286  max mem: 15572
Epoch: [12]  [  70/1404]  eta: 0:15:02  lr: 0.000085  min_lr: 0.000001  loss: 4.3626 (4.3542)  class_acc: 0.2083 (0.2488)  loss_scale: 32768.0000 (22383.7746)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1162  max mem: 15572
Epoch: [12]  [  80/1404]  eta: 0:14:27  lr: 0.000085  min_lr: 0.000001  loss: 4.3399 (4.3328)  class_acc: 0.2500 (0.2546)  loss_scale: 32768.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  time: 0.4857  data: 0.0079  max mem: 15572
Epoch: [12]  [  90/1404]  eta: 0:14:09  lr: 0.000085  min_lr: 0.000001  loss: 4.1899 (4.3325)  class_acc: 0.2500 (0.2505)  loss_scale: 32768.0000 (24666.0220)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0326  max mem: 15572
Epoch: [12]  [ 100/1404]  eta: 0:14:03  lr: 0.000085  min_lr: 0.000001  loss: 4.3264 (4.3369)  class_acc: 0.2500 (0.2529)  loss_scale: 32768.0000 (25468.1980)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1079  max mem: 15572
Epoch: [12]  [ 110/1404]  eta: 0:13:42  lr: 0.000085  min_lr: 0.000001  loss: 4.3205 (4.3294)  class_acc: 0.2500 (0.2523)  loss_scale: 32768.0000 (26125.8378)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.0896  max mem: 15572
Epoch: [12]  [ 120/1404]  eta: 0:13:31  lr: 0.000085  min_lr: 0.000001  loss: 4.2382 (4.3275)  class_acc: 0.2083 (0.2497)  loss_scale: 32768.0000 (26674.7769)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0701  max mem: 15572
[2025-01-16 23:11:12,451] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16976
[2025-01-16 23:11:12,451] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:11:12,452] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16976
[2025-01-16 23:11:12,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:11:12,452] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [12]  [ 130/1404]  eta: 0:13:21  lr: 0.000085  min_lr: 0.000001  loss: 4.2632 (4.3237)  class_acc: 0.2500 (0.2513)  loss_scale: 32768.0000 (26764.7023)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0866  max mem: 15572
Epoch: [12]  [ 140/1404]  eta: 0:13:16  lr: 0.000085  min_lr: 0.000001  loss: 4.2632 (4.3226)  class_acc: 0.2500 (0.2568)  loss_scale: 16384.0000 (26028.4823)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.0783  max mem: 15572
Epoch: [12]  [ 150/1404]  eta: 0:13:00  lr: 0.000085  min_lr: 0.000001  loss: 4.3070 (4.3196)  class_acc: 0.2500 (0.2566)  loss_scale: 16384.0000 (25389.7748)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0554  max mem: 15572
[2025-01-16 23:11:26,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=102, lr=[8.190914307140542e-07, 8.190914307140542e-07, 1.1701306153057918e-06, 1.1701306153057918e-06, 1.6716151647225601e-06, 1.6716151647225601e-06, 2.3880216638893716e-06, 2.3880216638893716e-06, 3.4114595198419597e-06, 3.4114595198419597e-06, 4.873513599774229e-06, 4.873513599774229e-06, 6.962162285391755e-06, 6.962162285391755e-06, 9.945946121988223e-06, 9.945946121988223e-06, 1.4208494459983175e-05, 1.4208494459983175e-05, 2.0297849228547397e-05, 2.0297849228547397e-05, 2.899692746935342e-05, 2.899692746935342e-05, 4.142418209907632e-05, 4.142418209907632e-05, 5.917740299868046e-05, 5.917740299868046e-05, 8.45391471409721e-05, 8.45391471409721e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 23:11:26,040] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=48.42679707640491, CurrSamplesPerSec=39.74392578639346, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [ 160/1404]  eta: 0:12:53  lr: 0.000085  min_lr: 0.000001  loss: 4.2411 (4.3154)  class_acc: 0.2500 (0.2591)  loss_scale: 16384.0000 (24830.4099)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0627  max mem: 15572
Epoch: [12]  [ 170/1404]  eta: 0:12:50  lr: 0.000085  min_lr: 0.000001  loss: 4.3090 (4.3188)  class_acc: 0.2500 (0.2585)  loss_scale: 16384.0000 (24336.4678)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.1540  max mem: 15572
Epoch: [12]  [ 180/1404]  eta: 0:12:43  lr: 0.000084  min_lr: 0.000001  loss: 4.3090 (4.3150)  class_acc: 0.2500 (0.2578)  loss_scale: 16384.0000 (23897.1050)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.1445  max mem: 15572
Epoch: [12]  [ 190/1404]  eta: 0:12:32  lr: 0.000084  min_lr: 0.000001  loss: 4.2738 (4.3110)  class_acc: 0.2500 (0.2592)  loss_scale: 16384.0000 (23503.7487)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0775  max mem: 15572
Epoch: [12]  [ 200/1404]  eta: 0:12:20  lr: 0.000084  min_lr: 0.000001  loss: 4.2503 (4.3110)  class_acc: 0.2500 (0.2597)  loss_scale: 16384.0000 (23149.5323)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0590  max mem: 15572
Epoch: [12]  [ 210/1404]  eta: 0:12:11  lr: 0.000084  min_lr: 0.000001  loss: 4.2511 (4.3138)  class_acc: 0.2500 (0.2577)  loss_scale: 16384.0000 (22828.8910)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0348  max mem: 15572
Epoch: [12]  [ 220/1404]  eta: 0:12:02  lr: 0.000084  min_lr: 0.000001  loss: 4.3056 (4.3141)  class_acc: 0.2083 (0.2575)  loss_scale: 16384.0000 (22537.2670)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0007  max mem: 15572
Epoch: [12]  [ 230/1404]  eta: 0:11:57  lr: 0.000084  min_lr: 0.000001  loss: 4.2885 (4.3116)  class_acc: 0.2500 (0.2579)  loss_scale: 16384.0000 (22270.8918)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0184  max mem: 15572
Epoch: [12]  [ 240/1404]  eta: 0:11:47  lr: 0.000084  min_lr: 0.000001  loss: 4.4199 (4.3219)  class_acc: 0.2083 (0.2555)  loss_scale: 16384.0000 (22026.6224)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0189  max mem: 15572
Epoch: [12]  [ 250/1404]  eta: 0:11:41  lr: 0.000084  min_lr: 0.000001  loss: 4.5420 (4.3212)  class_acc: 0.2083 (0.2570)  loss_scale: 16384.0000 (21801.8167)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0012  max mem: 15572
[2025-01-16 23:12:27,654] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:12:27,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:12:27,694] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:12:27,694] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [ 260/1404]  eta: 0:11:36  lr: 0.000084  min_lr: 0.000001  loss: 4.2784 (4.3235)  class_acc: 0.2917 (0.2575)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.0069  max mem: 15572
Epoch: [12]  [ 270/1404]  eta: 0:11:27  lr: 0.000084  min_lr: 0.000001  loss: 4.4061 (4.3299)  class_acc: 0.2500 (0.2569)  loss_scale: 32768.0000 (22248.3838)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.0068  max mem: 15572
Epoch: [12]  [ 280/1404]  eta: 0:11:20  lr: 0.000084  min_lr: 0.000001  loss: 4.3758 (4.3279)  class_acc: 0.2500 (0.2555)  loss_scale: 32768.0000 (22622.7473)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0154  max mem: 15572
Epoch: [12]  [ 290/1404]  eta: 0:11:14  lr: 0.000084  min_lr: 0.000001  loss: 4.3768 (4.3295)  class_acc: 0.2083 (0.2540)  loss_scale: 32768.0000 (22971.3814)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0153  max mem: 15572
[2025-01-16 23:12:51,247] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17145
[2025-01-16 23:12:51,248] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:12:51,248] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 23:12:51,250] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17145
[2025-01-16 23:12:51,251] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [12]  [ 300/1404]  eta: 0:11:09  lr: 0.000084  min_lr: 0.000001  loss: 4.4314 (4.3321)  class_acc: 0.1667 (0.2526)  loss_scale: 32768.0000 (23079.1229)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.0047  max mem: 15572
Epoch: [12]  [ 310/1404]  eta: 0:11:03  lr: 0.000084  min_lr: 0.000001  loss: 4.3174 (4.3366)  class_acc: 0.2083 (0.2532)  loss_scale: 16384.0000 (22863.8457)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0124  max mem: 15572
Epoch: [12]  [ 320/1404]  eta: 0:10:55  lr: 0.000084  min_lr: 0.000001  loss: 4.3879 (4.3402)  class_acc: 0.2500 (0.2534)  loss_scale: 16384.0000 (22661.9813)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0084  max mem: 15572
Epoch: [12]  [ 330/1404]  eta: 0:10:51  lr: 0.000084  min_lr: 0.000001  loss: 4.3879 (4.3408)  class_acc: 0.2083 (0.2534)  loss_scale: 16384.0000 (22472.3142)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0007  max mem: 15572
Epoch: [12]  [ 340/1404]  eta: 0:10:45  lr: 0.000084  min_lr: 0.000001  loss: 4.3824 (4.3369)  class_acc: 0.2083 (0.2542)  loss_scale: 16384.0000 (22293.7713)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.0007  max mem: 15572
Epoch: [12]  [ 350/1404]  eta: 0:10:36  lr: 0.000084  min_lr: 0.000001  loss: 4.2726 (4.3344)  class_acc: 0.2917 (0.2572)  loss_scale: 16384.0000 (22125.4017)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0007  max mem: 15572
Epoch: [12]  [ 360/1404]  eta: 0:10:29  lr: 0.000084  min_lr: 0.000001  loss: 4.2863 (4.3360)  class_acc: 0.2917 (0.2573)  loss_scale: 16384.0000 (21966.3601)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0009  max mem: 15572
Epoch: [12]  [ 370/1404]  eta: 0:10:23  lr: 0.000084  min_lr: 0.000001  loss: 4.3359 (4.3362)  class_acc: 0.2500 (0.2577)  loss_scale: 16384.0000 (21815.8922)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0011  max mem: 15572
Epoch: [12]  [ 380/1404]  eta: 0:10:17  lr: 0.000084  min_lr: 0.000001  loss: 4.3287 (4.3354)  class_acc: 0.2917 (0.2579)  loss_scale: 16384.0000 (21673.3228)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0009  max mem: 15572
Epoch: [12]  [ 390/1404]  eta: 0:10:11  lr: 0.000084  min_lr: 0.000001  loss: 4.2314 (4.3319)  class_acc: 0.2500 (0.2572)  loss_scale: 16384.0000 (21538.0460)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.0006  max mem: 15572
Epoch: [12]  [ 400/1404]  eta: 0:10:05  lr: 0.000084  min_lr: 0.000001  loss: 4.1888 (4.3318)  class_acc: 0.2083 (0.2557)  loss_scale: 16384.0000 (21409.5162)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0006  max mem: 15572
Epoch: [12]  [ 410/1404]  eta: 0:09:58  lr: 0.000084  min_lr: 0.000001  loss: 4.4920 (4.3334)  class_acc: 0.2083 (0.2558)  loss_scale: 16384.0000 (21287.2409)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0007  max mem: 15572
Epoch: [12]  [ 420/1404]  eta: 0:09:50  lr: 0.000084  min_lr: 0.000001  loss: 4.4920 (4.3330)  class_acc: 0.2500 (0.2560)  loss_scale: 16384.0000 (21170.7743)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0008  max mem: 15572
[2025-01-16 23:14:07,219] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:14:07,219] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:14:07,219] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:14:07,219] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [ 430/1404]  eta: 0:09:42  lr: 0.000084  min_lr: 0.000001  loss: 4.4298 (4.3342)  class_acc: 0.2500 (0.2564)  loss_scale: 16384.0000 (21249.7819)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0008  max mem: 15572
Epoch: [12]  [ 440/1404]  eta: 0:09:36  lr: 0.000084  min_lr: 0.000001  loss: 4.3384 (4.3328)  class_acc: 0.2500 (0.2566)  loss_scale: 32768.0000 (21510.9660)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0006  max mem: 15572
Epoch: [12]  [ 450/1404]  eta: 0:09:30  lr: 0.000084  min_lr: 0.000001  loss: 4.2150 (4.3298)  class_acc: 0.2500 (0.2575)  loss_scale: 32768.0000 (21760.5676)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0005  max mem: 15572
Epoch: [12]  [ 460/1404]  eta: 0:09:25  lr: 0.000084  min_lr: 0.000001  loss: 4.1645 (4.3286)  class_acc: 0.2500 (0.2574)  loss_scale: 32768.0000 (21999.3406)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.0006  max mem: 15572
Epoch: [12]  [ 470/1404]  eta: 0:09:17  lr: 0.000084  min_lr: 0.000001  loss: 4.2024 (4.3277)  class_acc: 0.2500 (0.2566)  loss_scale: 32768.0000 (22227.9745)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0007  max mem: 15572
Epoch: [12]  [ 480/1404]  eta: 0:09:11  lr: 0.000084  min_lr: 0.000001  loss: 4.2334 (4.3287)  class_acc: 0.2083 (0.2562)  loss_scale: 32768.0000 (22447.1019)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0007  max mem: 15572
Epoch: [12]  [ 490/1404]  eta: 0:09:06  lr: 0.000084  min_lr: 0.000001  loss: 4.2676 (4.3257)  class_acc: 0.2083 (0.2562)  loss_scale: 32768.0000 (22657.3035)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0005  max mem: 15572
Epoch: [12]  [ 500/1404]  eta: 0:09:01  lr: 0.000084  min_lr: 0.000001  loss: 4.3073 (4.3266)  class_acc: 0.2917 (0.2568)  loss_scale: 32768.0000 (22859.1138)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0006  max mem: 15572
Epoch: [12]  [ 510/1404]  eta: 0:08:54  lr: 0.000084  min_lr: 0.000001  loss: 4.4000 (4.3256)  class_acc: 0.2083 (0.2564)  loss_scale: 32768.0000 (23053.0254)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0007  max mem: 15572
[2025-01-16 23:15:00,448] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17365
[2025-01-16 23:15:00,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:15:00,449] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 23:15:00,454] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17365
[2025-01-16 23:15:00,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [12]  [ 520/1404]  eta: 0:08:48  lr: 0.000084  min_lr: 0.000001  loss: 4.3467 (4.3264)  class_acc: 0.2083 (0.2563)  loss_scale: 32768.0000 (23113.7044)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0006  max mem: 15572
Epoch: [12]  [ 530/1404]  eta: 0:08:41  lr: 0.000084  min_lr: 0.000001  loss: 4.3773 (4.3274)  class_acc: 0.2500 (0.2571)  loss_scale: 16384.0000 (22986.9680)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0007  max mem: 15572
Epoch: [12]  [ 540/1404]  eta: 0:08:35  lr: 0.000084  min_lr: 0.000001  loss: 4.3151 (4.3260)  class_acc: 0.2500 (0.2571)  loss_scale: 16384.0000 (22864.9168)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0008  max mem: 15572
Epoch: [12]  [ 550/1404]  eta: 0:08:31  lr: 0.000084  min_lr: 0.000001  loss: 4.3208 (4.3266)  class_acc: 0.2917 (0.2578)  loss_scale: 16384.0000 (22747.2958)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.0008  max mem: 15572
Epoch: [12]  [ 560/1404]  eta: 0:08:23  lr: 0.000084  min_lr: 0.000001  loss: 4.3618 (4.3259)  class_acc: 0.2917 (0.2578)  loss_scale: 16384.0000 (22633.8681)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0006  max mem: 15572
Epoch: [12]  [ 570/1404]  eta: 0:08:16  lr: 0.000084  min_lr: 0.000001  loss: 4.4003 (4.3280)  class_acc: 0.2500 (0.2573)  loss_scale: 16384.0000 (22524.4133)  weight_decay: 0.0500 (0.0500)  time: 0.4937  data: 0.0005  max mem: 15572
Epoch: [12]  [ 580/1404]  eta: 0:08:09  lr: 0.000084  min_lr: 0.000001  loss: 4.4105 (4.3289)  class_acc: 0.2500 (0.2568)  loss_scale: 16384.0000 (22418.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0007  max mem: 15572
Epoch: [12]  [ 590/1404]  eta: 0:08:03  lr: 0.000084  min_lr: 0.000001  loss: 4.3184 (4.3302)  class_acc: 0.2083 (0.2559)  loss_scale: 16384.0000 (22316.6159)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0007  max mem: 15572
Epoch: [12]  [ 600/1404]  eta: 0:07:57  lr: 0.000084  min_lr: 0.000001  loss: 4.2565 (4.3292)  class_acc: 0.2083 (0.2562)  loss_scale: 16384.0000 (22217.9035)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0006  max mem: 15572
Epoch: [12]  [ 610/1404]  eta: 0:07:51  lr: 0.000084  min_lr: 0.000001  loss: 4.2473 (4.3258)  class_acc: 0.2917 (0.2563)  loss_scale: 16384.0000 (22122.4223)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0007  max mem: 15572
Epoch: [12]  [ 620/1404]  eta: 0:07:44  lr: 0.000084  min_lr: 0.000001  loss: 4.3032 (4.3254)  class_acc: 0.2500 (0.2564)  loss_scale: 16384.0000 (22030.0161)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0006  max mem: 15572
Epoch: [12]  [ 630/1404]  eta: 0:07:39  lr: 0.000084  min_lr: 0.000001  loss: 4.4254 (4.3266)  class_acc: 0.2500 (0.2561)  loss_scale: 16384.0000 (21940.5388)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0007  max mem: 15572
Epoch: [12]  [ 640/1404]  eta: 0:07:33  lr: 0.000084  min_lr: 0.000001  loss: 4.4614 (4.3282)  class_acc: 0.2083 (0.2551)  loss_scale: 16384.0000 (21853.8534)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.0007  max mem: 15572
[2025-01-16 23:16:15,499] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:16:15,500] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:16:15,503] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:16:15,504] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [ 650/1404]  eta: 0:07:27  lr: 0.000084  min_lr: 0.000001  loss: 4.3344 (4.3282)  class_acc: 0.2500 (0.2561)  loss_scale: 16384.0000 (21895.6682)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.0009  max mem: 15572
Epoch: [12]  [ 660/1404]  eta: 0:07:22  lr: 0.000084  min_lr: 0.000001  loss: 4.3046 (4.3298)  class_acc: 0.2917 (0.2557)  loss_scale: 32768.0000 (22060.1513)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.0011  max mem: 15572
Epoch: [12]  [ 670/1404]  eta: 0:07:16  lr: 0.000084  min_lr: 0.000001  loss: 4.3270 (4.3288)  class_acc: 0.2083 (0.2555)  loss_scale: 32768.0000 (22219.7317)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.0008  max mem: 15572
Epoch: [12]  [ 680/1404]  eta: 0:07:10  lr: 0.000084  min_lr: 0.000001  loss: 4.3270 (4.3306)  class_acc: 0.2083 (0.2552)  loss_scale: 32768.0000 (22374.6256)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0007  max mem: 15572
Epoch: [12]  [ 690/1404]  eta: 0:07:04  lr: 0.000084  min_lr: 0.000001  loss: 4.3904 (4.3320)  class_acc: 0.2083 (0.2543)  loss_scale: 32768.0000 (22525.0362)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0007  max mem: 15572
Epoch: [12]  [ 700/1404]  eta: 0:06:58  lr: 0.000084  min_lr: 0.000001  loss: 4.2321 (4.3301)  class_acc: 0.2500 (0.2543)  loss_scale: 32768.0000 (22671.1555)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0006  max mem: 15572
Epoch: [12]  [ 710/1404]  eta: 0:06:52  lr: 0.000084  min_lr: 0.000001  loss: 4.2321 (4.3293)  class_acc: 0.2500 (0.2547)  loss_scale: 32768.0000 (22813.1646)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0007  max mem: 15572
Epoch: [12]  [ 720/1404]  eta: 0:06:45  lr: 0.000083  min_lr: 0.000001  loss: 4.3053 (4.3284)  class_acc: 0.2500 (0.2541)  loss_scale: 32768.0000 (22951.2344)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0006  max mem: 15572
Epoch: [12]  [ 730/1404]  eta: 0:06:40  lr: 0.000083  min_lr: 0.000001  loss: 4.3053 (4.3284)  class_acc: 0.2500 (0.2542)  loss_scale: 32768.0000 (23085.5267)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0006  max mem: 15572
Epoch: [12]  [ 740/1404]  eta: 0:06:33  lr: 0.000083  min_lr: 0.000001  loss: 4.3860 (4.3300)  class_acc: 0.2083 (0.2533)  loss_scale: 32768.0000 (23216.1943)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0006  max mem: 15572
Epoch: [12]  [ 750/1404]  eta: 0:06:27  lr: 0.000083  min_lr: 0.000001  loss: 4.4177 (4.3321)  class_acc: 0.2083 (0.2533)  loss_scale: 32768.0000 (23343.3822)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0006  max mem: 15572
Epoch: [12]  [ 760/1404]  eta: 0:06:22  lr: 0.000083  min_lr: 0.000001  loss: 4.4045 (4.3328)  class_acc: 0.2083 (0.2530)  loss_scale: 32768.0000 (23467.2273)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0008  max mem: 15572
Epoch: [12]  [ 770/1404]  eta: 0:06:16  lr: 0.000083  min_lr: 0.000001  loss: 4.2387 (4.3311)  class_acc: 0.2500 (0.2529)  loss_scale: 32768.0000 (23587.8599)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0008  max mem: 15572
[2025-01-16 23:17:30,811] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:17:30,811] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 23:17:30,817] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:17:30,818] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 23:17:31,338] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17623
[2025-01-16 23:17:31,339] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 23:17:31,361] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17623
[2025-01-16 23:17:31,361] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 23:17:31,362] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 780/1404]  eta: 0:06:09  lr: 0.000083  min_lr: 0.000001  loss: 4.2684 (4.3305)  class_acc: 0.2500 (0.2535)  loss_scale: 32768.0000 (23747.3598)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0008  max mem: 15572
Epoch: [12]  [ 790/1404]  eta: 0:06:04  lr: 0.000083  min_lr: 0.000001  loss: 4.3439 (4.3306)  class_acc: 0.2083 (0.2531)  loss_scale: 32768.0000 (23861.4008)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.0010  max mem: 15572
Epoch: [12]  [ 800/1404]  eta: 0:05:57  lr: 0.000083  min_lr: 0.000001  loss: 4.3575 (4.3312)  class_acc: 0.2500 (0.2534)  loss_scale: 32768.0000 (23972.5943)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0009  max mem: 15572
Epoch: [12]  [ 810/1404]  eta: 0:05:52  lr: 0.000083  min_lr: 0.000001  loss: 4.2820 (4.3321)  class_acc: 0.2500 (0.2527)  loss_scale: 32768.0000 (24081.0456)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0006  max mem: 15572
[2025-01-16 23:17:53,891] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17661
[2025-01-16 23:17:53,892] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:17:53,892] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 23:17:53,916] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17661
[2025-01-16 23:17:53,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [12]  [ 820/1404]  eta: 0:05:46  lr: 0.000083  min_lr: 0.000001  loss: 4.2820 (4.3321)  class_acc: 0.2500 (0.2536)  loss_scale: 32768.0000 (24027.2058)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0005  max mem: 15572
Epoch: [12]  [ 830/1404]  eta: 0:05:40  lr: 0.000083  min_lr: 0.000001  loss: 4.3588 (4.3323)  class_acc: 0.2083 (0.2530)  loss_scale: 16384.0000 (23935.2298)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0007  max mem: 15572
Epoch: [12]  [ 840/1404]  eta: 0:05:34  lr: 0.000083  min_lr: 0.000001  loss: 4.3588 (4.3306)  class_acc: 0.2083 (0.2537)  loss_scale: 16384.0000 (23845.4411)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0009  max mem: 15572
Epoch: [12]  [ 850/1404]  eta: 0:05:28  lr: 0.000083  min_lr: 0.000001  loss: 4.2014 (4.3284)  class_acc: 0.2500 (0.2534)  loss_scale: 16384.0000 (23757.7626)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0007  max mem: 15572
Epoch: [12]  [ 860/1404]  eta: 0:05:22  lr: 0.000083  min_lr: 0.000001  loss: 4.2014 (4.3276)  class_acc: 0.2500 (0.2534)  loss_scale: 16384.0000 (23672.1208)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0006  max mem: 15572
Epoch: [12]  [ 870/1404]  eta: 0:05:15  lr: 0.000083  min_lr: 0.000001  loss: 4.3634 (4.3288)  class_acc: 0.2500 (0.2539)  loss_scale: 16384.0000 (23588.4455)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0006  max mem: 15572
Epoch: [12]  [ 880/1404]  eta: 0:05:09  lr: 0.000083  min_lr: 0.000001  loss: 4.1756 (4.3264)  class_acc: 0.3333 (0.2551)  loss_scale: 16384.0000 (23506.6697)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0006  max mem: 15572
Epoch: [12]  [ 890/1404]  eta: 0:05:04  lr: 0.000083  min_lr: 0.000001  loss: 4.2462 (4.3289)  class_acc: 0.2917 (0.2548)  loss_scale: 16384.0000 (23426.7295)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0007  max mem: 15572
Epoch: [12]  [ 900/1404]  eta: 0:04:57  lr: 0.000083  min_lr: 0.000001  loss: 4.4283 (4.3289)  class_acc: 0.2083 (0.2547)  loss_scale: 16384.0000 (23348.5638)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0006  max mem: 15572
Epoch: [12]  [ 910/1404]  eta: 0:04:51  lr: 0.000083  min_lr: 0.000001  loss: 4.3213 (4.3294)  class_acc: 0.2083 (0.2543)  loss_scale: 16384.0000 (23272.1142)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0006  max mem: 15572
Epoch: [12]  [ 920/1404]  eta: 0:04:45  lr: 0.000083  min_lr: 0.000001  loss: 4.4032 (4.3300)  class_acc: 0.2083 (0.2543)  loss_scale: 16384.0000 (23197.3246)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0006  max mem: 15572
Epoch: [12]  [ 930/1404]  eta: 0:04:39  lr: 0.000083  min_lr: 0.000001  loss: 4.4032 (4.3303)  class_acc: 0.2083 (0.2546)  loss_scale: 16384.0000 (23124.1418)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0010  max mem: 15572
Epoch: [12]  [ 940/1404]  eta: 0:04:33  lr: 0.000083  min_lr: 0.000001  loss: 4.2865 (4.3297)  class_acc: 0.2083 (0.2541)  loss_scale: 16384.0000 (23052.5143)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0011  max mem: 15572
[2025-01-16 23:19:07,417] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:19:07,417] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:19:07,418] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:19:07,418] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [ 950/1404]  eta: 0:04:27  lr: 0.000083  min_lr: 0.000001  loss: 4.2612 (4.3299)  class_acc: 0.2083 (0.2542)  loss_scale: 16384.0000 (23137.4469)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0455  max mem: 15572
Epoch: [12]  [ 960/1404]  eta: 0:04:21  lr: 0.000083  min_lr: 0.000001  loss: 4.3015 (4.3304)  class_acc: 0.2500 (0.2540)  loss_scale: 32768.0000 (23237.6608)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0970  max mem: 15572
Epoch: [12]  [ 970/1404]  eta: 0:04:15  lr: 0.000083  min_lr: 0.000001  loss: 4.2808 (4.3284)  class_acc: 0.2500 (0.2545)  loss_scale: 32768.0000 (23335.8105)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0825  max mem: 15572
Epoch: [12]  [ 980/1404]  eta: 0:04:09  lr: 0.000083  min_lr: 0.000001  loss: 4.2332 (4.3281)  class_acc: 0.2917 (0.2547)  loss_scale: 32768.0000 (23431.9592)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0583  max mem: 15572
[2025-01-16 23:19:29,863] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17829
[2025-01-16 23:19:29,864] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:19:29,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17829
[2025-01-16 23:19:29,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:19:29,903] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [12]  [ 990/1404]  eta: 0:04:03  lr: 0.000083  min_lr: 0.000001  loss: 4.3159 (4.3294)  class_acc: 0.2083 (0.2547)  loss_scale: 16384.0000 (23360.8396)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.0282  max mem: 15572
Epoch: [12]  [1000/1404]  eta: 0:03:57  lr: 0.000083  min_lr: 0.000001  loss: 4.3854 (4.3292)  class_acc: 0.2083 (0.2549)  loss_scale: 16384.0000 (23291.1409)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0008  max mem: 15572
Epoch: [12]  [1010/1404]  eta: 0:03:52  lr: 0.000083  min_lr: 0.000001  loss: 4.3139 (4.3295)  class_acc: 0.2083 (0.2546)  loss_scale: 16384.0000 (23222.8210)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0242  max mem: 15572
Epoch: [12]  [1020/1404]  eta: 0:03:46  lr: 0.000083  min_lr: 0.000001  loss: 4.3749 (4.3295)  class_acc: 0.2083 (0.2551)  loss_scale: 16384.0000 (23155.8394)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.0242  max mem: 15572
Epoch: [12]  [1030/1404]  eta: 0:03:40  lr: 0.000083  min_lr: 0.000001  loss: 4.3782 (4.3299)  class_acc: 0.2500 (0.2550)  loss_scale: 16384.0000 (23090.1571)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0008  max mem: 15572
Epoch: [12]  [1040/1404]  eta: 0:03:33  lr: 0.000083  min_lr: 0.000001  loss: 4.2452 (4.3285)  class_acc: 0.2500 (0.2553)  loss_scale: 16384.0000 (23025.7368)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0115  max mem: 15572
Epoch: [12]  [1050/1404]  eta: 0:03:28  lr: 0.000083  min_lr: 0.000001  loss: 4.2489 (4.3285)  class_acc: 0.2500 (0.2555)  loss_scale: 16384.0000 (22962.5423)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0318  max mem: 15572
Epoch: [12]  [1060/1404]  eta: 0:03:22  lr: 0.000083  min_lr: 0.000001  loss: 4.3922 (4.3299)  class_acc: 0.2083 (0.2556)  loss_scale: 16384.0000 (22900.5391)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0920  max mem: 15572
Epoch: [12]  [1070/1404]  eta: 0:03:16  lr: 0.000083  min_lr: 0.000001  loss: 4.4316 (4.3308)  class_acc: 0.2083 (0.2556)  loss_scale: 16384.0000 (22839.6937)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.0991  max mem: 15572
Epoch: [12]  [1080/1404]  eta: 0:03:10  lr: 0.000083  min_lr: 0.000001  loss: 4.4163 (4.3305)  class_acc: 0.2083 (0.2555)  loss_scale: 16384.0000 (22779.9741)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0415  max mem: 15572
Epoch: [12]  [1090/1404]  eta: 0:03:04  lr: 0.000083  min_lr: 0.000001  loss: 4.4045 (4.3312)  class_acc: 0.2083 (0.2551)  loss_scale: 16384.0000 (22721.3492)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0141  max mem: 15572
Epoch: [12]  [1100/1404]  eta: 0:02:58  lr: 0.000083  min_lr: 0.000001  loss: 4.3545 (4.3316)  class_acc: 0.2083 (0.2552)  loss_scale: 16384.0000 (22663.7893)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0320  max mem: 15572
[2025-01-16 23:20:45,964] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:20:45,964] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:20:45,964] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:20:45,964] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [1110/1404]  eta: 0:02:53  lr: 0.000083  min_lr: 0.000001  loss: 4.2984 (4.3302)  class_acc: 0.2500 (0.2553)  loss_scale: 16384.0000 (22622.0126)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.0816  max mem: 15572
Epoch: [12]  [1120/1404]  eta: 0:02:47  lr: 0.000083  min_lr: 0.000001  loss: 4.2342 (4.3307)  class_acc: 0.2083 (0.2554)  loss_scale: 32768.0000 (22712.5210)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.0667  max mem: 15572
Epoch: [12]  [1130/1404]  eta: 0:02:41  lr: 0.000083  min_lr: 0.000001  loss: 4.2408 (4.3295)  class_acc: 0.2083 (0.2550)  loss_scale: 32768.0000 (22801.4288)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0623  max mem: 15572
Epoch: [12]  [1140/1404]  eta: 0:02:35  lr: 0.000083  min_lr: 0.000001  loss: 4.3056 (4.3287)  class_acc: 0.2083 (0.2548)  loss_scale: 32768.0000 (22888.7783)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0569  max mem: 15572
[2025-01-16 23:21:07,231] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17994
[2025-01-16 23:21:07,231] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:21:07,231] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 23:21:07,275] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17994
[2025-01-16 23:21:07,276] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [12]  [1150/1404]  eta: 0:02:29  lr: 0.000083  min_lr: 0.000001  loss: 4.2786 (4.3277)  class_acc: 0.2083 (0.2550)  loss_scale: 32768.0000 (22903.4370)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.0116  max mem: 15572
[2025-01-16 23:21:09,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=108, lr=[8.01083251096546e-07, 8.01083251096546e-07, 1.1444046444236373e-06, 1.1444046444236373e-06, 1.6348637777480533e-06, 1.6348637777480533e-06, 2.3355196824972193e-06, 2.3355196824972193e-06, 3.336456689281742e-06, 3.336456689281742e-06, 4.7663666989739175e-06, 4.7663666989739175e-06, 6.809095284248453e-06, 6.809095284248453e-06, 9.727278977497792e-06, 9.727278977497792e-06, 1.3896112824996845e-05, 1.3896112824996845e-05, 1.9851589749995496e-05, 1.9851589749995496e-05, 2.8359413928564995e-05, 2.8359413928564995e-05, 4.0513448469378566e-05, 4.0513448469378566e-05, 5.78763549562551e-05, 5.78763549562551e-05, 8.268050708036443e-05, 8.268050708036443e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 23:21:09,970] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=48.201998889639626, CurrSamplesPerSec=49.645007895793285, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [1160/1404]  eta: 0:02:23  lr: 0.000083  min_lr: 0.000001  loss: 4.2133 (4.3274)  class_acc: 0.2083 (0.2549)  loss_scale: 16384.0000 (22847.2834)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0220  max mem: 15572
Epoch: [12]  [1170/1404]  eta: 0:02:18  lr: 0.000083  min_lr: 0.000001  loss: 4.3135 (4.3278)  class_acc: 0.2083 (0.2548)  loss_scale: 16384.0000 (22792.0888)  weight_decay: 0.0500 (0.0500)  time: 0.6530  data: 0.0484  max mem: 15572
Epoch: [12]  [1180/1404]  eta: 0:02:12  lr: 0.000083  min_lr: 0.000001  loss: 4.3137 (4.3280)  class_acc: 0.2917 (0.2550)  loss_scale: 16384.0000 (22737.8290)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0271  max mem: 15572
Epoch: [12]  [1190/1404]  eta: 0:02:06  lr: 0.000083  min_lr: 0.000001  loss: 4.3859 (4.3293)  class_acc: 0.2917 (0.2548)  loss_scale: 16384.0000 (22684.4803)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0142  max mem: 15572
Epoch: [12]  [1200/1404]  eta: 0:02:00  lr: 0.000083  min_lr: 0.000001  loss: 4.4898 (4.3301)  class_acc: 0.2500 (0.2545)  loss_scale: 16384.0000 (22632.0200)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0144  max mem: 15572
Epoch: [12]  [1210/1404]  eta: 0:01:54  lr: 0.000083  min_lr: 0.000001  loss: 4.3523 (4.3305)  class_acc: 0.2917 (0.2551)  loss_scale: 16384.0000 (22580.4261)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0008  max mem: 15572
Epoch: [12]  [1220/1404]  eta: 0:01:48  lr: 0.000083  min_lr: 0.000001  loss: 4.3513 (4.3310)  class_acc: 0.2917 (0.2555)  loss_scale: 16384.0000 (22529.6773)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0005  max mem: 15572
Epoch: [12]  [1230/1404]  eta: 0:01:42  lr: 0.000083  min_lr: 0.000001  loss: 4.3982 (4.3317)  class_acc: 0.2500 (0.2549)  loss_scale: 16384.0000 (22479.7530)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.0152  max mem: 15572
Epoch: [12]  [1240/1404]  eta: 0:01:36  lr: 0.000083  min_lr: 0.000001  loss: 4.2951 (4.3309)  class_acc: 0.2083 (0.2549)  loss_scale: 16384.0000 (22430.6334)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0155  max mem: 15572
Epoch: [12]  [1250/1404]  eta: 0:01:30  lr: 0.000082  min_lr: 0.000001  loss: 4.3062 (4.3318)  class_acc: 0.2083 (0.2545)  loss_scale: 16384.0000 (22382.2990)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0009  max mem: 15572
Epoch: [12]  [1260/1404]  eta: 0:01:24  lr: 0.000082  min_lr: 0.000001  loss: 4.4599 (4.3324)  class_acc: 0.2083 (0.2544)  loss_scale: 16384.0000 (22334.7312)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0006  max mem: 15572
Epoch: [12]  [1270/1404]  eta: 0:01:18  lr: 0.000082  min_lr: 0.000001  loss: 4.3577 (4.3328)  class_acc: 0.2083 (0.2542)  loss_scale: 16384.0000 (22287.9119)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0006  max mem: 15572
[2025-01-16 23:22:23,966] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:22:23,967] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:22:23,971] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:22:23,972] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [1280/1404]  eta: 0:01:13  lr: 0.000082  min_lr: 0.000001  loss: 4.3577 (4.3329)  class_acc: 0.2083 (0.2543)  loss_scale: 16384.0000 (22318.5636)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.0006  max mem: 15572
Epoch: [12]  [1290/1404]  eta: 0:01:07  lr: 0.000082  min_lr: 0.000001  loss: 4.2939 (4.3322)  class_acc: 0.2917 (0.2551)  loss_scale: 32768.0000 (22399.5043)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0009  max mem: 15572
Epoch: [12]  [1300/1404]  eta: 0:01:01  lr: 0.000082  min_lr: 0.000001  loss: 4.3028 (4.3327)  class_acc: 0.2917 (0.2548)  loss_scale: 32768.0000 (22479.2006)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0009  max mem: 15572
Epoch: [12]  [1310/1404]  eta: 0:00:55  lr: 0.000082  min_lr: 0.000001  loss: 4.4291 (4.3343)  class_acc: 0.2917 (0.2552)  loss_scale: 32768.0000 (22557.6812)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0008  max mem: 15572
[2025-01-16 23:22:44,809] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18161
[2025-01-16 23:22:44,809] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:22:44,809] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 23:22:44,863] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18161
[2025-01-16 23:22:44,863] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [12]  [1320/1404]  eta: 0:00:49  lr: 0.000082  min_lr: 0.000001  loss: 4.3811 (4.3342)  class_acc: 0.2500 (0.2550)  loss_scale: 32768.0000 (22535.7517)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0011  max mem: 15572
Epoch: [12]  [1330/1404]  eta: 0:00:43  lr: 0.000082  min_lr: 0.000001  loss: 4.3154 (4.3345)  class_acc: 0.2500 (0.2553)  loss_scale: 16384.0000 (22489.5327)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0106  max mem: 15572
Epoch: [12]  [1340/1404]  eta: 0:00:37  lr: 0.000082  min_lr: 0.000001  loss: 4.4405 (4.3355)  class_acc: 0.2917 (0.2555)  loss_scale: 16384.0000 (22444.0030)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0788  max mem: 15572
Epoch: [12]  [1350/1404]  eta: 0:00:31  lr: 0.000082  min_lr: 0.000001  loss: 4.4573 (4.3370)  class_acc: 0.2083 (0.2551)  loss_scale: 16384.0000 (22399.1473)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.1250  max mem: 15572
Epoch: [12]  [1360/1404]  eta: 0:00:25  lr: 0.000082  min_lr: 0.000001  loss: 4.4573 (4.3365)  class_acc: 0.2083 (0.2553)  loss_scale: 16384.0000 (22354.9508)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.0565  max mem: 15572
Epoch: [12]  [1370/1404]  eta: 0:00:19  lr: 0.000082  min_lr: 0.000001  loss: 4.3593 (4.3368)  class_acc: 0.2500 (0.2554)  loss_scale: 16384.0000 (22311.3990)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0042  max mem: 15572
Epoch: [12]  [1380/1404]  eta: 0:00:14  lr: 0.000082  min_lr: 0.000001  loss: 4.2906 (4.3365)  class_acc: 0.2500 (0.2553)  loss_scale: 16384.0000 (22268.4779)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0315  max mem: 15572
Epoch: [12]  [1390/1404]  eta: 0:00:08  lr: 0.000082  min_lr: 0.000001  loss: 4.3179 (4.3366)  class_acc: 0.2500 (0.2552)  loss_scale: 16384.0000 (22226.1740)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0587  max mem: 15572
Epoch: [12]  [1400/1404]  eta: 0:00:02  lr: 0.000082  min_lr: 0.000001  loss: 4.3410 (4.3359)  class_acc: 0.2500 (0.2552)  loss_scale: 16384.0000 (22184.4739)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0312  max mem: 15572
Epoch: [12]  [1403/1404]  eta: 0:00:00  lr: 0.000082  min_lr: 0.000001  loss: 4.3179 (4.3359)  class_acc: 0.2500 (0.2552)  loss_scale: 16384.0000 (22172.0798)  weight_decay: 0.0500 (0.0500)  time: 0.4497  data: 0.0312  max mem: 15572
Epoch: [12] Total time: 0:13:43 (0.5863 s / it)
Averaged stats: lr: 0.000082  min_lr: 0.000001  loss: 4.3179 (4.3313)  class_acc: 0.2500 (0.2581)  loss_scale: 16384.0000 (22172.0798)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:23  loss: 2.1982 (2.1982)  acc1: 61.1111 (61.1111)  acc5: 83.3333 (83.3333)  time: 4.1460  data: 3.9635  max mem: 15572
Val:  [ 10/136]  eta: 0:01:32  loss: 2.8051 (2.7964)  acc1: 38.8889 (38.3838)  acc5: 72.2222 (70.2020)  time: 0.7358  data: 0.5278  max mem: 15572
Val:  [ 20/136]  eta: 0:01:04  loss: 2.9824 (2.8423)  acc1: 33.3333 (37.5661)  acc5: 72.2222 (70.3704)  time: 0.3756  data: 0.1768  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 2.7599 (2.6912)  acc1: 38.8889 (41.9355)  acc5: 77.7778 (72.4014)  time: 0.3390  data: 0.1383  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.3159 (2.6586)  acc1: 50.0000 (42.2764)  acc5: 77.7778 (72.8997)  time: 0.3489  data: 0.1385  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 2.6348 (2.6984)  acc1: 38.8889 (41.2854)  acc5: 77.7778 (72.9847)  time: 0.4023  data: 0.1922  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.9442 (2.7756)  acc1: 27.7778 (37.9781)  acc5: 66.6667 (70.8561)  time: 0.3451  data: 0.1430  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.8609 (2.7609)  acc1: 27.7778 (38.5759)  acc5: 66.6667 (71.2833)  time: 0.2835  data: 0.0973  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.6151 (2.7335)  acc1: 44.4444 (38.9575)  acc5: 77.7778 (72.4280)  time: 0.3382  data: 0.1398  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.6066 (2.7477)  acc1: 33.3333 (38.2173)  acc5: 77.7778 (71.9170)  time: 0.3716  data: 0.1610  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.0180 (2.8067)  acc1: 33.3333 (36.8537)  acc5: 66.6667 (70.2420)  time: 0.3621  data: 0.1487  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.9452 (2.8046)  acc1: 33.3333 (37.4875)  acc5: 66.6667 (70.4204)  time: 0.3714  data: 0.1648  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.4710 (2.7684)  acc1: 44.4444 (38.7511)  acc5: 77.7778 (71.6713)  time: 0.3911  data: 0.1977  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.3445 (2.7422)  acc1: 50.0000 (39.5674)  acc5: 83.3333 (72.3494)  time: 0.2762  data: 0.1020  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.5459 (2.7489)  acc1: 50.0000 (39.5987)  acc5: 77.7778 (72.2359)  time: 0.2285  data: 0.0770  max mem: 15572
Val: Total time: 0:00:49 (0.3665 s / it)
* Acc@1 38.391 Acc@5 70.332 loss 2.794
Accuracy of the network on the 4883 val videos: 38.4%
[2025-01-16 23:24:24,432] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 23:24:24,434] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 23:24:24,434] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 23:24:24,434] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 23:24:26,843] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 23:24:26,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 38.39%
Epoch: [13]  [   0/1404]  eta: 3:08:23  lr: 0.000082  min_lr: 0.000001  loss: 4.9981 (4.9981)  class_acc: 0.3333 (0.3333)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 8.0508  data: 7.5133  max mem: 15572
Epoch: [13]  [  10/1404]  eta: 0:29:07  lr: 0.000082  min_lr: 0.000001  loss: 4.3021 (4.4434)  class_acc: 0.2500 (0.2500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2537  data: 0.7443  max mem: 15572
Epoch: [13]  [  20/1404]  eta: 0:22:44  lr: 0.000082  min_lr: 0.000001  loss: 4.3021 (4.4250)  class_acc: 0.2083 (0.2579)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1304  max mem: 15572
Epoch: [13]  [  30/1404]  eta: 0:19:07  lr: 0.000082  min_lr: 0.000001  loss: 4.4233 (4.4286)  class_acc: 0.2917 (0.2809)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1183  max mem: 15572
[2025-01-16 23:24:56,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:24:56,659] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:24:56,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:24:56,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [  40/1404]  eta: 0:17:26  lr: 0.000082  min_lr: 0.000001  loss: 4.4233 (4.4069)  class_acc: 0.3333 (0.2957)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0622  max mem: 15572
Epoch: [13]  [  50/1404]  eta: 0:16:17  lr: 0.000082  min_lr: 0.000001  loss: 4.3943 (4.4049)  class_acc: 0.2917 (0.2810)  loss_scale: 32768.0000 (20560.3137)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0662  max mem: 15572
Epoch: [13]  [  60/1404]  eta: 0:15:41  lr: 0.000082  min_lr: 0.000001  loss: 4.3823 (4.3908)  class_acc: 0.2083 (0.2746)  loss_scale: 32768.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.0866  max mem: 15572
Epoch: [13]  [  70/1404]  eta: 0:15:20  lr: 0.000082  min_lr: 0.000001  loss: 4.2747 (4.3719)  class_acc: 0.2083 (0.2682)  loss_scale: 32768.0000 (23999.0986)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1315  max mem: 15572
Epoch: [13]  [  80/1404]  eta: 0:14:53  lr: 0.000082  min_lr: 0.000001  loss: 4.3070 (4.3604)  class_acc: 0.2500 (0.2737)  loss_scale: 32768.0000 (25081.6790)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1162  max mem: 15572
[2025-01-16 23:25:21,973] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18333
[2025-01-16 23:25:21,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:25:21,988] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18333
[2025-01-16 23:25:21,989] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:25:21,990] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [  90/1404]  eta: 0:14:31  lr: 0.000082  min_lr: 0.000001  loss: 4.2445 (4.3404)  class_acc: 0.2917 (0.2724)  loss_scale: 16384.0000 (24125.8901)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0861  max mem: 15572
Epoch: [13]  [ 100/1404]  eta: 0:14:22  lr: 0.000082  min_lr: 0.000001  loss: 4.2065 (4.3310)  class_acc: 0.2500 (0.2710)  loss_scale: 16384.0000 (23359.3663)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1088  max mem: 15572
Epoch: [13]  [ 110/1404]  eta: 0:14:17  lr: 0.000082  min_lr: 0.000001  loss: 4.3605 (4.3359)  class_acc: 0.2500 (0.2770)  loss_scale: 16384.0000 (22730.9550)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.1692  max mem: 15572
Epoch: [13]  [ 120/1404]  eta: 0:13:51  lr: 0.000082  min_lr: 0.000001  loss: 4.3960 (4.3304)  class_acc: 0.2917 (0.2789)  loss_scale: 16384.0000 (22206.4132)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1010  max mem: 15572
Epoch: [13]  [ 130/1404]  eta: 0:13:28  lr: 0.000082  min_lr: 0.000001  loss: 4.3960 (4.3430)  class_acc: 0.2083 (0.2742)  loss_scale: 16384.0000 (21761.9542)  weight_decay: 0.0500 (0.0500)  time: 0.4780  data: 0.0005  max mem: 15572
Epoch: [13]  [ 140/1404]  eta: 0:13:14  lr: 0.000082  min_lr: 0.000001  loss: 4.4371 (4.3533)  class_acc: 0.2083 (0.2736)  loss_scale: 16384.0000 (21380.5390)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0285  max mem: 15572
Epoch: [13]  [ 150/1404]  eta: 0:13:04  lr: 0.000082  min_lr: 0.000001  loss: 4.4269 (4.3550)  class_acc: 0.2083 (0.2690)  loss_scale: 16384.0000 (21049.6424)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0849  max mem: 15572
Epoch: [13]  [ 160/1404]  eta: 0:12:49  lr: 0.000082  min_lr: 0.000001  loss: 4.3359 (4.3495)  class_acc: 0.2083 (0.2699)  loss_scale: 16384.0000 (20759.8509)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0683  max mem: 15572
Epoch: [13]  [ 170/1404]  eta: 0:12:39  lr: 0.000082  min_lr: 0.000001  loss: 4.2261 (4.3396)  class_acc: 0.2500 (0.2675)  loss_scale: 16384.0000 (20503.9532)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0312  max mem: 15572
Epoch: [13]  [ 180/1404]  eta: 0:12:27  lr: 0.000082  min_lr: 0.000001  loss: 4.2490 (4.3358)  class_acc: 0.2500 (0.2696)  loss_scale: 16384.0000 (20276.3315)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0349  max mem: 15572
Epoch: [13]  [ 190/1404]  eta: 0:12:27  lr: 0.000082  min_lr: 0.000001  loss: 4.2303 (4.3315)  class_acc: 0.2500 (0.2696)  loss_scale: 16384.0000 (20072.5445)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.0660  max mem: 15572
Epoch: [13]  [ 200/1404]  eta: 0:12:15  lr: 0.000082  min_lr: 0.000001  loss: 4.1834 (4.3241)  class_acc: 0.2917 (0.2745)  loss_scale: 16384.0000 (19889.0348)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0590  max mem: 15572
[2025-01-16 23:26:35,772] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:26:35,772] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:26:35,818] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:26:35,818] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [ 210/1404]  eta: 0:12:09  lr: 0.000082  min_lr: 0.000001  loss: 4.2262 (4.3199)  class_acc: 0.2917 (0.2745)  loss_scale: 16384.0000 (19800.5687)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.0566  max mem: 15572
Epoch: [13]  [ 220/1404]  eta: 0:12:08  lr: 0.000082  min_lr: 0.000001  loss: 4.2262 (4.3183)  class_acc: 0.2500 (0.2736)  loss_scale: 32768.0000 (20387.3303)  weight_decay: 0.0500 (0.0500)  time: 0.6626  data: 0.1407  max mem: 15572
Epoch: [13]  [ 230/1404]  eta: 0:11:59  lr: 0.000082  min_lr: 0.000001  loss: 4.2836 (4.3144)  class_acc: 0.2500 (0.2727)  loss_scale: 32768.0000 (20923.2900)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1153  max mem: 15572
Epoch: [13]  [ 240/1404]  eta: 0:11:55  lr: 0.000082  min_lr: 0.000001  loss: 4.2452 (4.3126)  class_acc: 0.2500 (0.2726)  loss_scale: 32768.0000 (21414.7718)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0726  max mem: 15572
Epoch: [13]  [ 250/1404]  eta: 0:11:44  lr: 0.000082  min_lr: 0.000001  loss: 4.3348 (4.3164)  class_acc: 0.2500 (0.2701)  loss_scale: 32768.0000 (21867.0916)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.0501  max mem: 15572
Epoch: [13]  [ 260/1404]  eta: 0:11:34  lr: 0.000082  min_lr: 0.000001  loss: 4.4279 (4.3193)  class_acc: 0.2500 (0.2708)  loss_scale: 32768.0000 (22284.7510)  weight_decay: 0.0500 (0.0500)  time: 0.5151  data: 0.0126  max mem: 15572
Epoch: [13]  [ 270/1404]  eta: 0:11:29  lr: 0.000082  min_lr: 0.000001  loss: 4.3769 (4.3201)  class_acc: 0.2500 (0.2701)  loss_scale: 32768.0000 (22671.5867)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0712  max mem: 15572
Epoch: [13]  [ 280/1404]  eta: 0:11:23  lr: 0.000082  min_lr: 0.000001  loss: 4.3574 (4.3193)  class_acc: 0.2500 (0.2699)  loss_scale: 32768.0000 (23030.8897)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.0594  max mem: 15572
Epoch: [13]  [ 290/1404]  eta: 0:11:16  lr: 0.000082  min_lr: 0.000001  loss: 4.2563 (4.3143)  class_acc: 0.2917 (0.2712)  loss_scale: 32768.0000 (23365.4983)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0006  max mem: 15572
Epoch: [13]  [ 300/1404]  eta: 0:11:13  lr: 0.000082  min_lr: 0.000001  loss: 4.3330 (4.3154)  class_acc: 0.2917 (0.2713)  loss_scale: 32768.0000 (23677.8738)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.0856  max mem: 15572
Epoch: [13]  [ 310/1404]  eta: 0:11:03  lr: 0.000082  min_lr: 0.000001  loss: 4.3330 (4.3159)  class_acc: 0.2083 (0.2698)  loss_scale: 32768.0000 (23970.1608)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.0857  max mem: 15572
[2025-01-16 23:27:36,044] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18563
[2025-01-16 23:27:36,044] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18563
[2025-01-16 23:27:36,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:27:36,045] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:27:36,045] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [ 320/1404]  eta: 0:10:55  lr: 0.000082  min_lr: 0.000001  loss: 4.3610 (4.3150)  class_acc: 0.2500 (0.2701)  loss_scale: 16384.0000 (23733.8318)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0330  max mem: 15572
Epoch: [13]  [ 330/1404]  eta: 0:10:48  lr: 0.000082  min_lr: 0.000001  loss: 4.3610 (4.3150)  class_acc: 0.2917 (0.2711)  loss_scale: 16384.0000 (23511.7825)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0630  max mem: 15572
Epoch: [13]  [ 340/1404]  eta: 0:10:40  lr: 0.000082  min_lr: 0.000001  loss: 4.3379 (4.3166)  class_acc: 0.2500 (0.2709)  loss_scale: 16384.0000 (23302.7566)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0472  max mem: 15572
Epoch: [13]  [ 350/1404]  eta: 0:10:34  lr: 0.000081  min_lr: 0.000001  loss: 4.3323 (4.3162)  class_acc: 0.2083 (0.2708)  loss_scale: 16384.0000 (23105.6410)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0171  max mem: 15572
Epoch: [13]  [ 360/1404]  eta: 0:10:27  lr: 0.000081  min_lr: 0.000001  loss: 4.3050 (4.3156)  class_acc: 0.2500 (0.2725)  loss_scale: 16384.0000 (22919.4460)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0006  max mem: 15572
Epoch: [13]  [ 370/1404]  eta: 0:10:19  lr: 0.000081  min_lr: 0.000001  loss: 4.2576 (4.3138)  class_acc: 0.2500 (0.2723)  loss_scale: 16384.0000 (22743.2884)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0007  max mem: 15572
Epoch: [13]  [ 380/1404]  eta: 0:10:13  lr: 0.000081  min_lr: 0.000001  loss: 4.2484 (4.3111)  class_acc: 0.2500 (0.2726)  loss_scale: 16384.0000 (22576.3780)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0143  max mem: 15572
Epoch: [13]  [ 390/1404]  eta: 0:10:07  lr: 0.000081  min_lr: 0.000001  loss: 4.3283 (4.3144)  class_acc: 0.2083 (0.2716)  loss_scale: 16384.0000 (22418.0051)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0661  max mem: 15572
Epoch: [13]  [ 400/1404]  eta: 0:10:01  lr: 0.000081  min_lr: 0.000001  loss: 4.3392 (4.3143)  class_acc: 0.2500 (0.2718)  loss_scale: 16384.0000 (22267.5312)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0970  max mem: 15572
Epoch: [13]  [ 410/1404]  eta: 0:09:54  lr: 0.000081  min_lr: 0.000001  loss: 4.4030 (4.3173)  class_acc: 0.2500 (0.2712)  loss_scale: 16384.0000 (22124.3796)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0733  max mem: 15572
Epoch: [13]  [ 420/1404]  eta: 0:09:47  lr: 0.000081  min_lr: 0.000001  loss: 4.4443 (4.3200)  class_acc: 0.2500 (0.2712)  loss_scale: 16384.0000 (21988.0285)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0627  max mem: 15572
Epoch: [13]  [ 430/1404]  eta: 0:09:40  lr: 0.000081  min_lr: 0.000001  loss: 4.3211 (4.3191)  class_acc: 0.2500 (0.2712)  loss_scale: 16384.0000 (21858.0046)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0553  max mem: 15572
[2025-01-16 23:28:50,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:28:50,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [ 440/1404]  eta: 0:09:35  lr: 0.000081  min_lr: 0.000001  loss: 4.3715 (4.3241)  class_acc: 0.2083 (0.2693)  loss_scale: 16384.0000 (21771.0295)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.0708  max mem: 15572
[2025-01-16 23:28:50,232] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:28:50,234] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [ 450/1404]  eta: 0:09:29  lr: 0.000081  min_lr: 0.000001  loss: 4.4038 (4.3233)  class_acc: 0.2500 (0.2687)  loss_scale: 32768.0000 (22014.8647)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.0972  max mem: 15572
[2025-01-16 23:28:57,975] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18705
[2025-01-16 23:28:57,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:28:57,991] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18705
[2025-01-16 23:28:57,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:28:57,992] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [ 460/1404]  eta: 0:09:23  lr: 0.000081  min_lr: 0.000001  loss: 4.2745 (4.3229)  class_acc: 0.2500 (0.2681)  loss_scale: 32768.0000 (21963.8004)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0985  max mem: 15572
Epoch: [13]  [ 470/1404]  eta: 0:09:18  lr: 0.000081  min_lr: 0.000001  loss: 4.3047 (4.3227)  class_acc: 0.2083 (0.2685)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0870  max mem: 15572
Epoch: [13]  [ 480/1404]  eta: 0:09:13  lr: 0.000081  min_lr: 0.000001  loss: 4.2725 (4.3196)  class_acc: 0.2917 (0.2696)  loss_scale: 16384.0000 (21731.7921)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1011  max mem: 15572
Epoch: [13]  [ 490/1404]  eta: 0:09:08  lr: 0.000081  min_lr: 0.000001  loss: 4.1320 (4.3153)  class_acc: 0.2500 (0.2683)  loss_scale: 16384.0000 (21622.8758)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.1479  max mem: 15572
Epoch: [13]  [ 500/1404]  eta: 0:09:00  lr: 0.000081  min_lr: 0.000001  loss: 4.1781 (4.3158)  class_acc: 0.2917 (0.2693)  loss_scale: 16384.0000 (21518.3074)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1096  max mem: 15572
Epoch: [13]  [ 510/1404]  eta: 0:08:53  lr: 0.000081  min_lr: 0.000001  loss: 4.3202 (4.3154)  class_acc: 0.2917 (0.2706)  loss_scale: 16384.0000 (21417.8317)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0506  max mem: 15572
Epoch: [13]  [ 520/1404]  eta: 0:08:48  lr: 0.000081  min_lr: 0.000001  loss: 4.3968 (4.3181)  class_acc: 0.2083 (0.2691)  loss_scale: 16384.0000 (21321.2131)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0878  max mem: 15572
Epoch: [13]  [ 530/1404]  eta: 0:08:43  lr: 0.000081  min_lr: 0.000001  loss: 4.3912 (4.3173)  class_acc: 0.2083 (0.2692)  loss_scale: 16384.0000 (21228.2335)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1538  max mem: 15572
Epoch: [13]  [ 540/1404]  eta: 0:08:35  lr: 0.000081  min_lr: 0.000001  loss: 4.2406 (4.3147)  class_acc: 0.2083 (0.2685)  loss_scale: 16384.0000 (21138.6913)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1037  max mem: 15572
Epoch: [13]  [ 550/1404]  eta: 0:08:29  lr: 0.000081  min_lr: 0.000001  loss: 4.2597 (4.3134)  class_acc: 0.2500 (0.2688)  loss_scale: 16384.0000 (21052.3993)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0326  max mem: 15572
Epoch: [13]  [ 560/1404]  eta: 0:08:23  lr: 0.000081  min_lr: 0.000001  loss: 4.3293 (4.3140)  class_acc: 0.2500 (0.2688)  loss_scale: 16384.0000 (20969.1836)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0737  max mem: 15572
Epoch: [13]  [ 570/1404]  eta: 0:08:16  lr: 0.000081  min_lr: 0.000001  loss: 4.3420 (4.3152)  class_acc: 0.2500 (0.2689)  loss_scale: 16384.0000 (20888.8827)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0553  max mem: 15572
Epoch: [13]  [ 580/1404]  eta: 0:08:10  lr: 0.000081  min_lr: 0.000001  loss: 4.3428 (4.3160)  class_acc: 0.2917 (0.2697)  loss_scale: 16384.0000 (20811.3460)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0472  max mem: 15572
[2025-01-16 23:30:13,959] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:30:13,959] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:30:13,987] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:30:13,988] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [ 590/1404]  eta: 0:08:05  lr: 0.000081  min_lr: 0.000001  loss: 4.3696 (4.3168)  class_acc: 0.2917 (0.2697)  loss_scale: 16384.0000 (20985.9357)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1110  max mem: 15572
Epoch: [13]  [ 600/1404]  eta: 0:07:58  lr: 0.000081  min_lr: 0.000001  loss: 4.3358 (4.3159)  class_acc: 0.3333 (0.2709)  loss_scale: 32768.0000 (21181.9767)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0750  max mem: 15572
Epoch: [13]  [ 610/1404]  eta: 0:07:51  lr: 0.000081  min_lr: 0.000001  loss: 4.3455 (4.3166)  class_acc: 0.3333 (0.2711)  loss_scale: 32768.0000 (21371.6007)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0156  max mem: 15572
[2025-01-16 23:30:30,158] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18863
[2025-01-16 23:30:30,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:30:30,190] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18863
[2025-01-16 23:30:30,191] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:30:30,191] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [ 620/1404]  eta: 0:07:46  lr: 0.000081  min_lr: 0.000001  loss: 4.3783 (4.3159)  class_acc: 0.2500 (0.2707)  loss_scale: 16384.0000 (21291.2850)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.0944  max mem: 15572
Epoch: [13]  [ 630/1404]  eta: 0:07:39  lr: 0.000081  min_lr: 0.000001  loss: 4.3692 (4.3174)  class_acc: 0.1667 (0.2691)  loss_scale: 16384.0000 (21213.5151)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1034  max mem: 15572
Epoch: [13]  [ 640/1404]  eta: 0:07:33  lr: 0.000081  min_lr: 0.000001  loss: 4.3579 (4.3187)  class_acc: 0.1667 (0.2691)  loss_scale: 16384.0000 (21138.1716)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0519  max mem: 15572
Epoch: [13]  [ 650/1404]  eta: 0:07:27  lr: 0.000081  min_lr: 0.000001  loss: 4.3331 (4.3186)  class_acc: 0.2500 (0.2689)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1189  max mem: 15572
Epoch: [13]  [ 660/1404]  eta: 0:07:20  lr: 0.000081  min_lr: 0.000001  loss: 4.3205 (4.3181)  class_acc: 0.2917 (0.2690)  loss_scale: 16384.0000 (20994.3238)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0812  max mem: 15572
Epoch: [13]  [ 670/1404]  eta: 0:07:14  lr: 0.000081  min_lr: 0.000001  loss: 4.2978 (4.3175)  class_acc: 0.2917 (0.2690)  loss_scale: 16384.0000 (20925.6155)  weight_decay: 0.0500 (0.0500)  time: 0.5208  data: 0.0178  max mem: 15572
Epoch: [13]  [ 680/1404]  eta: 0:07:09  lr: 0.000081  min_lr: 0.000001  loss: 4.4227 (4.3192)  class_acc: 0.2917 (0.2689)  loss_scale: 16384.0000 (20858.9251)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0491  max mem: 15572
Epoch: [13]  [ 690/1404]  eta: 0:07:03  lr: 0.000081  min_lr: 0.000001  loss: 4.3624 (4.3179)  class_acc: 0.2917 (0.2693)  loss_scale: 16384.0000 (20794.1650)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.0321  max mem: 15572
Epoch: [13]  [ 700/1404]  eta: 0:06:58  lr: 0.000081  min_lr: 0.000001  loss: 4.2046 (4.3174)  class_acc: 0.2917 (0.2691)  loss_scale: 16384.0000 (20731.2525)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.0006  max mem: 15572
Epoch: [13]  [ 710/1404]  eta: 0:06:51  lr: 0.000081  min_lr: 0.000001  loss: 4.2046 (4.3148)  class_acc: 0.2500 (0.2690)  loss_scale: 16384.0000 (20670.1097)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0007  max mem: 15572
Epoch: [13]  [ 720/1404]  eta: 0:06:44  lr: 0.000081  min_lr: 0.000001  loss: 4.2768 (4.3149)  class_acc: 0.2917 (0.2696)  loss_scale: 16384.0000 (20610.6630)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0008  max mem: 15572
Epoch: [13]  [ 730/1404]  eta: 0:06:39  lr: 0.000081  min_lr: 0.000001  loss: 4.3290 (4.3149)  class_acc: 0.2500 (0.2692)  loss_scale: 16384.0000 (20552.8427)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.0418  max mem: 15572
[2025-01-16 23:31:45,025] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:31:45,025] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:31:45,038] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:31:45,039] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [ 740/1404]  eta: 0:06:32  lr: 0.000081  min_lr: 0.000001  loss: 4.3290 (4.3140)  class_acc: 0.2500 (0.2696)  loss_scale: 16384.0000 (20518.6937)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0418  max mem: 15572
[2025-01-16 23:31:49,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=113, lr=[7.816594832876371e-07, 7.816594832876371e-07, 1.1166564046966245e-06, 1.1166564046966245e-06, 1.5952234352808924e-06, 1.5952234352808924e-06, 2.2788906218298463e-06, 2.2788906218298463e-06, 3.255558031185495e-06, 3.255558031185495e-06, 4.65079718740785e-06, 4.65079718740785e-06, 6.643995982011215e-06, 6.643995982011215e-06, 9.491422831444594e-06, 9.491422831444594e-06, 1.3559175473492277e-05, 1.3559175473492277e-05, 1.937025067641754e-05, 1.937025067641754e-05, 2.7671786680596486e-05, 2.7671786680596486e-05, 3.9531123829423554e-05, 3.9531123829423554e-05, 5.647303404203365e-05, 5.647303404203365e-05, 8.067576291719094e-05, 8.067576291719094e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 23:31:49,426] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=48.224894267618325, CurrSamplesPerSec=45.0583962941063, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [ 750/1404]  eta: 0:06:26  lr: 0.000081  min_lr: 0.000001  loss: 4.2353 (4.3126)  class_acc: 0.2500 (0.2694)  loss_scale: 32768.0000 (20681.8003)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0008  max mem: 15572
[2025-01-16 23:31:55,250] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19009
[2025-01-16 23:31:55,250] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:31:55,251] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 23:31:55,252] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19009
[2025-01-16 23:31:55,252] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [13]  [ 760/1404]  eta: 0:06:20  lr: 0.000081  min_lr: 0.000001  loss: 4.3350 (4.3132)  class_acc: 0.2083 (0.2697)  loss_scale: 32768.0000 (20754.5020)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0007  max mem: 15572
Epoch: [13]  [ 770/1404]  eta: 0:06:14  lr: 0.000081  min_lr: 0.000001  loss: 4.3831 (4.3139)  class_acc: 0.2083 (0.2694)  loss_scale: 16384.0000 (20697.8158)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0183  max mem: 15572
Epoch: [13]  [ 780/1404]  eta: 0:06:08  lr: 0.000081  min_lr: 0.000001  loss: 4.3831 (4.3147)  class_acc: 0.2500 (0.2693)  loss_scale: 16384.0000 (20642.5813)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0269  max mem: 15572
Epoch: [13]  [ 790/1404]  eta: 0:06:02  lr: 0.000081  min_lr: 0.000001  loss: 4.3615 (4.3146)  class_acc: 0.2917 (0.2693)  loss_scale: 16384.0000 (20588.7434)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0093  max mem: 15572
Epoch: [13]  [ 800/1404]  eta: 0:05:56  lr: 0.000081  min_lr: 0.000001  loss: 4.3067 (4.3151)  class_acc: 0.2500 (0.2696)  loss_scale: 16384.0000 (20536.2497)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.0274  max mem: 15572
Epoch: [13]  [ 810/1404]  eta: 0:05:50  lr: 0.000081  min_lr: 0.000001  loss: 4.3058 (4.3156)  class_acc: 0.2500 (0.2698)  loss_scale: 16384.0000 (20485.0506)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0273  max mem: 15572
Epoch: [13]  [ 820/1404]  eta: 0:05:45  lr: 0.000081  min_lr: 0.000001  loss: 4.3626 (4.3156)  class_acc: 0.2500 (0.2689)  loss_scale: 16384.0000 (20435.0987)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0007  max mem: 15572
Epoch: [13]  [ 830/1404]  eta: 0:05:38  lr: 0.000081  min_lr: 0.000001  loss: 4.3985 (4.3167)  class_acc: 0.2083 (0.2689)  loss_scale: 16384.0000 (20386.3490)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0007  max mem: 15572
Epoch: [13]  [ 840/1404]  eta: 0:05:33  lr: 0.000080  min_lr: 0.000001  loss: 4.3042 (4.3149)  class_acc: 0.2083 (0.2693)  loss_scale: 16384.0000 (20338.7586)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0007  max mem: 15572
Epoch: [13]  [ 850/1404]  eta: 0:05:27  lr: 0.000080  min_lr: 0.000001  loss: 4.3042 (4.3174)  class_acc: 0.2500 (0.2686)  loss_scale: 16384.0000 (20292.2867)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0007  max mem: 15572
Epoch: [13]  [ 860/1404]  eta: 0:05:21  lr: 0.000080  min_lr: 0.000001  loss: 4.4854 (4.3189)  class_acc: 0.2083 (0.2681)  loss_scale: 16384.0000 (20246.8943)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0008  max mem: 15572
Epoch: [13]  [ 870/1404]  eta: 0:05:16  lr: 0.000080  min_lr: 0.000001  loss: 4.4067 (4.3186)  class_acc: 0.2500 (0.2681)  loss_scale: 16384.0000 (20202.5442)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.0007  max mem: 15572
Epoch: [13]  [ 880/1404]  eta: 0:05:09  lr: 0.000080  min_lr: 0.000001  loss: 4.1938 (4.3162)  class_acc: 0.2500 (0.2683)  loss_scale: 16384.0000 (20159.2009)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.0006  max mem: 15572
[2025-01-16 23:33:12,088] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:33:12,088] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:33:12,097] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:33:12,098] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [ 890/1404]  eta: 0:05:04  lr: 0.000080  min_lr: 0.000001  loss: 4.2086 (4.3165)  class_acc: 0.2917 (0.2688)  loss_scale: 16384.0000 (20208.7722)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0469  max mem: 15572
Epoch: [13]  [ 900/1404]  eta: 0:04:57  lr: 0.000080  min_lr: 0.000001  loss: 4.3409 (4.3164)  class_acc: 0.2500 (0.2683)  loss_scale: 32768.0000 (20348.1643)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0469  max mem: 15572
Epoch: [13]  [ 910/1404]  eta: 0:04:51  lr: 0.000080  min_lr: 0.000001  loss: 4.2290 (4.3142)  class_acc: 0.2917 (0.2692)  loss_scale: 32768.0000 (20484.4962)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0007  max mem: 15572
Epoch: [13]  [ 920/1404]  eta: 0:04:45  lr: 0.000080  min_lr: 0.000001  loss: 4.2724 (4.3145)  class_acc: 0.2500 (0.2688)  loss_scale: 32768.0000 (20617.8675)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0008  max mem: 15572
Epoch: [13]  [ 930/1404]  eta: 0:04:39  lr: 0.000080  min_lr: 0.000001  loss: 4.2724 (4.3138)  class_acc: 0.2083 (0.2684)  loss_scale: 32768.0000 (20748.3738)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0007  max mem: 15572
[2025-01-16 23:33:40,736] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19189
[2025-01-16 23:33:40,736] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19189
[2025-01-16 23:33:40,737] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:33:40,737] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:33:40,737] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [ 940/1404]  eta: 0:04:33  lr: 0.000080  min_lr: 0.000001  loss: 4.3241 (4.3137)  class_acc: 0.2083 (0.2682)  loss_scale: 32768.0000 (20806.4612)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0007  max mem: 15572
Epoch: [13]  [ 950/1404]  eta: 0:04:28  lr: 0.000080  min_lr: 0.000001  loss: 4.3825 (4.3161)  class_acc: 0.2083 (0.2675)  loss_scale: 16384.0000 (20759.9579)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0007  max mem: 15572
Epoch: [13]  [ 960/1404]  eta: 0:04:22  lr: 0.000080  min_lr: 0.000001  loss: 4.4799 (4.3176)  class_acc: 0.1667 (0.2670)  loss_scale: 16384.0000 (20714.4225)  weight_decay: 0.0500 (0.0500)  time: 0.6684  data: 0.0006  max mem: 15572
Epoch: [13]  [ 970/1404]  eta: 0:04:16  lr: 0.000080  min_lr: 0.000001  loss: 4.3303 (4.3185)  class_acc: 0.2083 (0.2672)  loss_scale: 16384.0000 (20669.8249)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0007  max mem: 15572
Epoch: [13]  [ 980/1404]  eta: 0:04:10  lr: 0.000080  min_lr: 0.000001  loss: 4.4624 (4.3206)  class_acc: 0.2917 (0.2672)  loss_scale: 16384.0000 (20626.1366)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0006  max mem: 15572
Epoch: [13]  [ 990/1404]  eta: 0:04:04  lr: 0.000080  min_lr: 0.000001  loss: 4.4470 (4.3194)  class_acc: 0.2500 (0.2672)  loss_scale: 16384.0000 (20583.3300)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.0007  max mem: 15572
Epoch: [13]  [1000/1404]  eta: 0:03:58  lr: 0.000080  min_lr: 0.000001  loss: 4.1288 (4.3181)  class_acc: 0.2500 (0.2669)  loss_scale: 16384.0000 (20541.3786)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.0007  max mem: 15572
Epoch: [13]  [1010/1404]  eta: 0:03:52  lr: 0.000080  min_lr: 0.000001  loss: 4.3355 (4.3179)  class_acc: 0.2500 (0.2671)  loss_scale: 16384.0000 (20500.2572)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.0005  max mem: 15572
Epoch: [13]  [1020/1404]  eta: 0:03:46  lr: 0.000080  min_lr: 0.000001  loss: 4.3031 (4.3165)  class_acc: 0.2917 (0.2676)  loss_scale: 16384.0000 (20459.9412)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0006  max mem: 15572
Epoch: [13]  [1030/1404]  eta: 0:03:40  lr: 0.000080  min_lr: 0.000001  loss: 4.1614 (4.3153)  class_acc: 0.2917 (0.2679)  loss_scale: 16384.0000 (20420.4074)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0007  max mem: 15572
Epoch: [13]  [1040/1404]  eta: 0:03:34  lr: 0.000080  min_lr: 0.000001  loss: 4.1614 (4.3151)  class_acc: 0.2500 (0.2680)  loss_scale: 16384.0000 (20381.6330)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0007  max mem: 15572
Epoch: [13]  [1050/1404]  eta: 0:03:28  lr: 0.000080  min_lr: 0.000001  loss: 4.3400 (4.3164)  class_acc: 0.2500 (0.2681)  loss_scale: 16384.0000 (20343.5966)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0456  max mem: 15572
Epoch: [13]  [1060/1404]  eta: 0:03:22  lr: 0.000080  min_lr: 0.000001  loss: 4.5278 (4.3174)  class_acc: 0.2500 (0.2679)  loss_scale: 16384.0000 (20306.2771)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0625  max mem: 15572
[2025-01-16 23:34:57,363] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:34:57,363] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:34:57,364] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:34:57,364] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [1070/1404]  eta: 0:03:17  lr: 0.000080  min_lr: 0.000001  loss: 4.4446 (4.3173)  class_acc: 0.2083 (0.2678)  loss_scale: 16384.0000 (20346.1438)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.0406  max mem: 15572
Epoch: [13]  [1080/1404]  eta: 0:03:11  lr: 0.000080  min_lr: 0.000001  loss: 4.3031 (4.3165)  class_acc: 0.2500 (0.2682)  loss_scale: 32768.0000 (20461.0546)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.0238  max mem: 15572
Epoch: [13]  [1090/1404]  eta: 0:03:05  lr: 0.000080  min_lr: 0.000001  loss: 4.2827 (4.3162)  class_acc: 0.2500 (0.2681)  loss_scale: 32768.0000 (20573.8588)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.0006  max mem: 15572
Epoch: [13]  [1100/1404]  eta: 0:02:59  lr: 0.000080  min_lr: 0.000001  loss: 4.4657 (4.3184)  class_acc: 0.2083 (0.2684)  loss_scale: 32768.0000 (20684.6140)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0006  max mem: 15572
Epoch: [13]  [1110/1404]  eta: 0:02:53  lr: 0.000080  min_lr: 0.000001  loss: 4.4895 (4.3190)  class_acc: 0.2500 (0.2681)  loss_scale: 32768.0000 (20793.3753)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0008  max mem: 15572
Epoch: [13]  [1120/1404]  eta: 0:02:47  lr: 0.000080  min_lr: 0.000001  loss: 4.2930 (4.3182)  class_acc: 0.2500 (0.2684)  loss_scale: 32768.0000 (20900.1963)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0008  max mem: 15572
Epoch: [13]  [1130/1404]  eta: 0:02:41  lr: 0.000080  min_lr: 0.000001  loss: 4.1899 (4.3176)  class_acc: 0.2917 (0.2686)  loss_scale: 32768.0000 (21005.1282)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0006  max mem: 15572
Epoch: [13]  [1140/1404]  eta: 0:02:35  lr: 0.000080  min_lr: 0.000001  loss: 4.1770 (4.3160)  class_acc: 0.2917 (0.2689)  loss_scale: 32768.0000 (21108.2209)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.0006  max mem: 15572
Epoch: [13]  [1150/1404]  eta: 0:02:29  lr: 0.000080  min_lr: 0.000001  loss: 4.1770 (4.3159)  class_acc: 0.2917 (0.2690)  loss_scale: 32768.0000 (21209.5222)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0008  max mem: 15572
Epoch: [13]  [1160/1404]  eta: 0:02:23  lr: 0.000080  min_lr: 0.000001  loss: 4.4194 (4.3168)  class_acc: 0.2500 (0.2686)  loss_scale: 32768.0000 (21309.0784)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0007  max mem: 15572
Epoch: [13]  [1170/1404]  eta: 0:02:17  lr: 0.000080  min_lr: 0.000001  loss: 4.3680 (4.3164)  class_acc: 0.2083 (0.2684)  loss_scale: 32768.0000 (21406.9342)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0007  max mem: 15572
Epoch: [13]  [1180/1404]  eta: 0:02:11  lr: 0.000080  min_lr: 0.000001  loss: 4.3146 (4.3163)  class_acc: 0.2500 (0.2685)  loss_scale: 32768.0000 (21503.1329)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.0007  max mem: 15572
Epoch: [13]  [1190/1404]  eta: 0:02:06  lr: 0.000080  min_lr: 0.000001  loss: 4.3809 (4.3164)  class_acc: 0.2500 (0.2685)  loss_scale: 32768.0000 (21597.7162)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.0006  max mem: 15572
[2025-01-16 23:36:11,955] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:36:11,956] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 23:36:11,998] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:36:11,998] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 23:36:12,920] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19448
[2025-01-16 23:36:12,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 23:36:12,920] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-16 23:36:12,922] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19448
[2025-01-16 23:36:12,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 23:36:14,459] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19451
[2025-01-16 23:36:14,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:36:14,460] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 23:36:14,461] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19451
[2025-01-16 23:36:14,461] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [13]  [1200/1404]  eta: 0:02:00  lr: 0.000080  min_lr: 0.000001  loss: 4.3809 (4.3166)  class_acc: 0.2500 (0.2684)  loss_scale: 32768.0000 (21718.0083)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0006  max mem: 15572
Epoch: [13]  [1210/1404]  eta: 0:01:54  lr: 0.000080  min_lr: 0.000001  loss: 4.3901 (4.3179)  class_acc: 0.2917 (0.2686)  loss_scale: 16384.0000 (21673.9620)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0010  max mem: 15572
Epoch: [13]  [1220/1404]  eta: 0:01:48  lr: 0.000080  min_lr: 0.000001  loss: 4.3800 (4.3177)  class_acc: 0.2083 (0.2686)  loss_scale: 16384.0000 (21630.6372)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0010  max mem: 15572
Epoch: [13]  [1230/1404]  eta: 0:01:42  lr: 0.000080  min_lr: 0.000001  loss: 4.2481 (4.3180)  class_acc: 0.2083 (0.2688)  loss_scale: 16384.0000 (21588.0162)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0006  max mem: 15572
Epoch: [13]  [1240/1404]  eta: 0:01:36  lr: 0.000080  min_lr: 0.000001  loss: 4.2867 (4.3178)  class_acc: 0.2500 (0.2691)  loss_scale: 16384.0000 (21546.0822)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0005  max mem: 15572
Epoch: [13]  [1250/1404]  eta: 0:01:30  lr: 0.000080  min_lr: 0.000001  loss: 4.3688 (4.3187)  class_acc: 0.2500 (0.2687)  loss_scale: 16384.0000 (21504.8185)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0006  max mem: 15572
Epoch: [13]  [1260/1404]  eta: 0:01:24  lr: 0.000080  min_lr: 0.000001  loss: 4.3760 (4.3188)  class_acc: 0.2083 (0.2685)  loss_scale: 16384.0000 (21464.2094)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0007  max mem: 15572
Epoch: [13]  [1270/1404]  eta: 0:01:18  lr: 0.000080  min_lr: 0.000001  loss: 4.3495 (4.3191)  class_acc: 0.2500 (0.2688)  loss_scale: 16384.0000 (21424.2392)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0009  max mem: 15572
Epoch: [13]  [1280/1404]  eta: 0:01:12  lr: 0.000080  min_lr: 0.000001  loss: 4.3360 (4.3181)  class_acc: 0.2917 (0.2686)  loss_scale: 16384.0000 (21384.8931)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0010  max mem: 15572
Epoch: [13]  [1290/1404]  eta: 0:01:07  lr: 0.000080  min_lr: 0.000001  loss: 4.4816 (4.3199)  class_acc: 0.1667 (0.2681)  loss_scale: 16384.0000 (21346.1565)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0008  max mem: 15572
Epoch: [13]  [1300/1404]  eta: 0:01:01  lr: 0.000080  min_lr: 0.000001  loss: 4.4963 (4.3195)  class_acc: 0.2083 (0.2679)  loss_scale: 16384.0000 (21308.0154)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.0007  max mem: 15572
Epoch: [13]  [1310/1404]  eta: 0:00:55  lr: 0.000079  min_lr: 0.000001  loss: 4.3926 (4.3196)  class_acc: 0.2083 (0.2674)  loss_scale: 16384.0000 (21270.4561)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.0007  max mem: 15572
Epoch: [13]  [1320/1404]  eta: 0:00:49  lr: 0.000079  min_lr: 0.000001  loss: 4.3815 (4.3198)  class_acc: 0.2083 (0.2669)  loss_scale: 16384.0000 (21233.4656)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0007  max mem: 15572
[2025-01-16 23:37:28,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:37:28,221] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:37:28,227] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:37:28,227] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:37:28,766] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19581
[2025-01-16 23:37:28,767] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:37:28,775] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19581
[2025-01-16 23:37:28,775] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:37:28,775] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [1330/1404]  eta: 0:00:43  lr: 0.000079  min_lr: 0.000001  loss: 4.3782 (4.3201)  class_acc: 0.2083 (0.2668)  loss_scale: 16384.0000 (21209.3403)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0009  max mem: 15572
Epoch: [13]  [1340/1404]  eta: 0:00:37  lr: 0.000079  min_lr: 0.000001  loss: 4.3533 (4.3192)  class_acc: 0.2083 (0.2663)  loss_scale: 16384.0000 (21173.3572)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0129  max mem: 15572
Epoch: [13]  [1350/1404]  eta: 0:00:31  lr: 0.000079  min_lr: 0.000001  loss: 4.2748 (4.3194)  class_acc: 0.2083 (0.2661)  loss_scale: 16384.0000 (21137.9067)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.0509  max mem: 15572
Epoch: [13]  [1360/1404]  eta: 0:00:25  lr: 0.000079  min_lr: 0.000001  loss: 4.3271 (4.3199)  class_acc: 0.2500 (0.2661)  loss_scale: 16384.0000 (21102.9772)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.0389  max mem: 15572
Epoch: [13]  [1370/1404]  eta: 0:00:20  lr: 0.000079  min_lr: 0.000001  loss: 4.3005 (4.3196)  class_acc: 0.2083 (0.2660)  loss_scale: 16384.0000 (21068.5573)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0006  max mem: 15572
Epoch: [13]  [1380/1404]  eta: 0:00:14  lr: 0.000079  min_lr: 0.000001  loss: 4.2551 (4.3206)  class_acc: 0.1667 (0.2656)  loss_scale: 16384.0000 (21034.6358)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0006  max mem: 15572
Epoch: [13]  [1390/1404]  eta: 0:00:08  lr: 0.000079  min_lr: 0.000001  loss: 4.2761 (4.3195)  class_acc: 0.2083 (0.2657)  loss_scale: 16384.0000 (21001.2020)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0010  max mem: 15572
Epoch: [13]  [1400/1404]  eta: 0:00:02  lr: 0.000079  min_lr: 0.000001  loss: 4.2545 (4.3181)  class_acc: 0.2917 (0.2663)  loss_scale: 16384.0000 (20968.2455)  weight_decay: 0.0500 (0.0500)  time: 0.4668  data: 0.0007  max mem: 15572
Epoch: [13]  [1403/1404]  eta: 0:00:00  lr: 0.000079  min_lr: 0.000001  loss: 4.2545 (4.3180)  class_acc: 0.2917 (0.2665)  loss_scale: 16384.0000 (20958.4501)  weight_decay: 0.0500 (0.0500)  time: 0.4434  data: 0.0007  max mem: 15572
Epoch: [13] Total time: 0:13:43 (0.5865 s / it)
Averaged stats: lr: 0.000079  min_lr: 0.000001  loss: 4.2545 (4.3249)  class_acc: 0.2917 (0.2636)  loss_scale: 16384.0000 (20958.4501)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:18  loss: 1.9777 (1.9777)  acc1: 55.5556 (55.5556)  acc5: 83.3333 (83.3333)  time: 4.1053  data: 3.9252  max mem: 15572
Val:  [ 10/136]  eta: 0:01:38  loss: 2.7336 (2.5926)  acc1: 50.0000 (44.4444)  acc5: 72.2222 (74.2424)  time: 0.7834  data: 0.5826  max mem: 15572
Val:  [ 20/136]  eta: 0:01:00  loss: 2.8254 (2.7428)  acc1: 33.3333 (38.0952)  acc5: 66.6667 (71.4286)  time: 0.3443  data: 0.1503  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 2.6316 (2.6242)  acc1: 33.3333 (40.3226)  acc5: 72.2222 (73.1183)  time: 0.2916  data: 0.0912  max mem: 15572
Val:  [ 40/136]  eta: 0:00:41  loss: 2.3022 (2.5948)  acc1: 44.4444 (42.1409)  acc5: 77.7778 (73.9837)  time: 0.3355  data: 0.1359  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.5008 (2.6382)  acc1: 38.8889 (41.5033)  acc5: 77.7778 (73.8562)  time: 0.3615  data: 0.1592  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.8124 (2.7133)  acc1: 33.3333 (38.5246)  acc5: 72.2222 (72.0401)  time: 0.3664  data: 0.1597  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.6797 (2.6916)  acc1: 33.3333 (39.9061)  acc5: 66.6667 (72.0657)  time: 0.3689  data: 0.1747  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.5569 (2.6993)  acc1: 44.4444 (38.9575)  acc5: 72.2222 (72.3594)  time: 0.3869  data: 0.1831  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.7540 (2.7100)  acc1: 38.8889 (38.9499)  acc5: 72.2222 (71.8559)  time: 0.3348  data: 0.1277  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.9108 (2.7513)  acc1: 27.7778 (37.8438)  acc5: 61.1111 (70.5721)  time: 0.3416  data: 0.1366  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.9556 (2.7375)  acc1: 33.3333 (38.9890)  acc5: 72.2222 (71.0711)  time: 0.3746  data: 0.1604  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.2575 (2.6878)  acc1: 55.5556 (40.2663)  acc5: 83.3333 (72.4518)  time: 0.3691  data: 0.1660  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.1858 (2.6571)  acc1: 55.5556 (41.2638)  acc5: 83.3333 (73.0280)  time: 0.2893  data: 0.1215  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.3539 (2.6583)  acc1: 44.4444 (41.2776)  acc5: 83.3333 (73.0958)  time: 0.1928  data: 0.0413  max mem: 15572
Val: Total time: 0:00:49 (0.3665 s / it)
* Acc@1 40.827 Acc@5 71.888 loss 2.699
Accuracy of the network on the 4883 val videos: 40.8%
[2025-01-16 23:39:00,100] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 23:39:00,101] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 23:39:00,102] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 23:39:00,102] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 23:39:02,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 23:39:02,445] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 40.83%
Epoch: [14]  [   0/1404]  eta: 2:46:36  lr: 0.000079  min_lr: 0.000001  loss: 3.8674 (3.8674)  class_acc: 0.2083 (0.2083)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.1197  data: 6.5740  max mem: 15572
Epoch: [14]  [  10/1404]  eta: 0:26:13  lr: 0.000079  min_lr: 0.000001  loss: 4.4665 (4.3294)  class_acc: 0.2083 (0.2311)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1290  data: 0.5983  max mem: 15572
Epoch: [14]  [  20/1404]  eta: 0:20:28  lr: 0.000079  min_lr: 0.000001  loss: 4.2532 (4.2645)  class_acc: 0.2083 (0.2361)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.0571  max mem: 15572
Epoch: [14]  [  30/1404]  eta: 0:17:30  lr: 0.000079  min_lr: 0.000001  loss: 4.1958 (4.2399)  class_acc: 0.2083 (0.2567)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0646  max mem: 15572
Epoch: [14]  [  40/1404]  eta: 0:16:40  lr: 0.000079  min_lr: 0.000001  loss: 4.2460 (4.2550)  class_acc: 0.2500 (0.2530)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0595  max mem: 15572
Epoch: [14]  [  50/1404]  eta: 0:16:02  lr: 0.000079  min_lr: 0.000001  loss: 4.2903 (4.2516)  class_acc: 0.2083 (0.2459)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.0955  max mem: 15572
[2025-01-16 23:39:41,510] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:39:41,511] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:39:41,512] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:39:41,512] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [  60/1404]  eta: 0:15:34  lr: 0.000079  min_lr: 0.000001  loss: 4.2748 (4.2548)  class_acc: 0.2083 (0.2575)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.0804  max mem: 15572
Epoch: [14]  [  70/1404]  eta: 0:14:49  lr: 0.000079  min_lr: 0.000001  loss: 4.3289 (4.2924)  class_acc: 0.2083 (0.2512)  loss_scale: 32768.0000 (20306.9296)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0369  max mem: 15572
Epoch: [14]  [  80/1404]  eta: 0:14:35  lr: 0.000079  min_lr: 0.000001  loss: 4.4716 (4.3197)  class_acc: 0.2083 (0.2531)  loss_scale: 32768.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0086  max mem: 15572
Epoch: [14]  [  90/1404]  eta: 0:14:18  lr: 0.000079  min_lr: 0.000001  loss: 4.3609 (4.3059)  class_acc: 0.2500 (0.2560)  loss_scale: 32768.0000 (23045.6264)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0086  max mem: 15572
[2025-01-16 23:40:06,410] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19754
[2025-01-16 23:40:06,410] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:40:06,444] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19754
[2025-01-16 23:40:06,444] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:40:06,445] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [14]  [ 100/1404]  eta: 0:13:57  lr: 0.000079  min_lr: 0.000001  loss: 4.1681 (4.3164)  class_acc: 0.2500 (0.2578)  loss_scale: 32768.0000 (23521.5842)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0185  max mem: 15572
Epoch: [14]  [ 110/1404]  eta: 0:13:52  lr: 0.000079  min_lr: 0.000001  loss: 4.4181 (4.3251)  class_acc: 0.2500 (0.2579)  loss_scale: 16384.0000 (22878.5586)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0729  max mem: 15572
Epoch: [14]  [ 120/1404]  eta: 0:13:40  lr: 0.000079  min_lr: 0.000001  loss: 4.3812 (4.3161)  class_acc: 0.2500 (0.2596)  loss_scale: 16384.0000 (22341.8182)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1039  max mem: 15572
Epoch: [14]  [ 130/1404]  eta: 0:13:33  lr: 0.000079  min_lr: 0.000001  loss: 4.2896 (4.3156)  class_acc: 0.2917 (0.2637)  loss_scale: 16384.0000 (21887.0229)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0966  max mem: 15572
Epoch: [14]  [ 140/1404]  eta: 0:13:19  lr: 0.000079  min_lr: 0.000001  loss: 4.3953 (4.3158)  class_acc: 0.2917 (0.2639)  loss_scale: 16384.0000 (21496.7376)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0818  max mem: 15572
Epoch: [14]  [ 150/1404]  eta: 0:13:09  lr: 0.000079  min_lr: 0.000001  loss: 4.3011 (4.3151)  class_acc: 0.2500 (0.2652)  loss_scale: 16384.0000 (21158.1457)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0840  max mem: 15572
Epoch: [14]  [ 160/1404]  eta: 0:13:00  lr: 0.000079  min_lr: 0.000001  loss: 4.3006 (4.3154)  class_acc: 0.2500 (0.2673)  loss_scale: 16384.0000 (20861.6149)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.0826  max mem: 15572
Epoch: [14]  [ 170/1404]  eta: 0:12:51  lr: 0.000079  min_lr: 0.000001  loss: 4.2919 (4.3101)  class_acc: 0.2500 (0.2678)  loss_scale: 16384.0000 (20599.7661)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0766  max mem: 15572
Epoch: [14]  [ 180/1404]  eta: 0:12:38  lr: 0.000079  min_lr: 0.000001  loss: 4.4118 (4.3140)  class_acc: 0.2500 (0.2650)  loss_scale: 16384.0000 (20366.8508)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0549  max mem: 15572
Epoch: [14]  [ 190/1404]  eta: 0:12:30  lr: 0.000079  min_lr: 0.000001  loss: 4.3849 (4.3145)  class_acc: 0.2500 (0.2653)  loss_scale: 16384.0000 (20158.3246)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0701  max mem: 15572
Epoch: [14]  [ 200/1404]  eta: 0:12:23  lr: 0.000079  min_lr: 0.000001  loss: 4.3849 (4.3130)  class_acc: 0.2917 (0.2697)  loss_scale: 16384.0000 (19970.5473)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1240  max mem: 15572
Epoch: [14]  [ 210/1404]  eta: 0:12:10  lr: 0.000079  min_lr: 0.000001  loss: 4.3350 (4.3128)  class_acc: 0.3333 (0.2707)  loss_scale: 16384.0000 (19800.5687)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0655  max mem: 15572
Epoch: [14]  [ 220/1404]  eta: 0:12:00  lr: 0.000079  min_lr: 0.000001  loss: 4.1973 (4.3068)  class_acc: 0.2917 (0.2717)  loss_scale: 16384.0000 (19645.9729)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0005  max mem: 15572
[2025-01-16 23:41:20,974] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:41:20,974] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:41:20,974] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:41:20,974] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [ 230/1404]  eta: 0:11:51  lr: 0.000079  min_lr: 0.000001  loss: 4.3217 (4.3080)  class_acc: 0.2083 (0.2698)  loss_scale: 16384.0000 (19788.4675)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0265  max mem: 15572
Epoch: [14]  [ 240/1404]  eta: 0:11:52  lr: 0.000079  min_lr: 0.000001  loss: 4.3463 (4.3052)  class_acc: 0.2083 (0.2688)  loss_scale: 32768.0000 (20327.0373)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.1499  max mem: 15572
[2025-01-16 23:41:32,887] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19901
[2025-01-16 23:41:32,887] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:41:32,903] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19901
[2025-01-16 23:41:32,904] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:41:32,904] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [14]  [ 250/1404]  eta: 0:11:42  lr: 0.000079  min_lr: 0.000001  loss: 4.2814 (4.3020)  class_acc: 0.2500 (0.2708)  loss_scale: 32768.0000 (20431.0438)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1500  max mem: 15572
Epoch: [14]  [ 260/1404]  eta: 0:11:38  lr: 0.000079  min_lr: 0.000001  loss: 4.2814 (4.3035)  class_acc: 0.2500 (0.2703)  loss_scale: 16384.0000 (20275.9847)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1160  max mem: 15572
Epoch: [14]  [ 270/1404]  eta: 0:11:29  lr: 0.000079  min_lr: 0.000001  loss: 4.4063 (4.3022)  class_acc: 0.2500 (0.2700)  loss_scale: 16384.0000 (20132.3690)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0899  max mem: 15572
Epoch: [14]  [ 280/1404]  eta: 0:11:22  lr: 0.000079  min_lr: 0.000001  loss: 4.4063 (4.3067)  class_acc: 0.2500 (0.2679)  loss_scale: 16384.0000 (19998.9751)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0526  max mem: 15572
Epoch: [14]  [ 290/1404]  eta: 0:11:12  lr: 0.000079  min_lr: 0.000001  loss: 4.3122 (4.3052)  class_acc: 0.2500 (0.2685)  loss_scale: 16384.0000 (19874.7491)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0531  max mem: 15572
Epoch: [14]  [ 300/1404]  eta: 0:11:04  lr: 0.000079  min_lr: 0.000001  loss: 4.2503 (4.3079)  class_acc: 0.2083 (0.2674)  loss_scale: 16384.0000 (19758.7774)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0222  max mem: 15572
Epoch: [14]  [ 310/1404]  eta: 0:10:57  lr: 0.000079  min_lr: 0.000001  loss: 4.3378 (4.3082)  class_acc: 0.2083 (0.2667)  loss_scale: 16384.0000 (19650.2637)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0394  max mem: 15572
Epoch: [14]  [ 320/1404]  eta: 0:10:52  lr: 0.000079  min_lr: 0.000001  loss: 4.2312 (4.3057)  class_acc: 0.2500 (0.2667)  loss_scale: 16384.0000 (19548.5109)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0553  max mem: 15572
Epoch: [14]  [ 330/1404]  eta: 0:10:46  lr: 0.000079  min_lr: 0.000001  loss: 4.1961 (4.3059)  class_acc: 0.2500 (0.2679)  loss_scale: 16384.0000 (19452.9063)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0376  max mem: 15572
Epoch: [14]  [ 340/1404]  eta: 0:10:38  lr: 0.000079  min_lr: 0.000001  loss: 4.1955 (4.3076)  class_acc: 0.2500 (0.2674)  loss_scale: 16384.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0006  max mem: 15572
[2025-01-16 23:42:29,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=120, lr=[7.608994897237253e-07, 7.608994897237253e-07, 1.0869992710338935e-06, 1.0869992710338935e-06, 1.5528561014769908e-06, 1.5528561014769908e-06, 2.218365859252844e-06, 2.218365859252844e-06, 3.1690940846469206e-06, 3.1690940846469206e-06, 4.527277263781315e-06, 4.527277263781315e-06, 6.4675389482590216e-06, 6.4675389482590216e-06, 9.239341354655746e-06, 9.239341354655746e-06, 1.3199059078079638e-05, 1.3199059078079638e-05, 1.8855798682970913e-05, 1.8855798682970913e-05, 2.693685526138702e-05, 2.693685526138702e-05, 3.848122180198146e-05, 3.848122180198146e-05, 5.497317400283066e-05, 5.497317400283066e-05, 7.853310571832952e-05, 7.853310571832952e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 23:42:29,001] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=48.09984816749111, CurrSamplesPerSec=59.80048701404784, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [ 350/1404]  eta: 0:10:35  lr: 0.000079  min_lr: 0.000001  loss: 4.2290 (4.3052)  class_acc: 0.2083 (0.2680)  loss_scale: 16384.0000 (19278.0399)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.0675  max mem: 15572
Epoch: [14]  [ 360/1404]  eta: 0:10:27  lr: 0.000078  min_lr: 0.000001  loss: 4.2919 (4.3088)  class_acc: 0.2917 (0.2687)  loss_scale: 16384.0000 (19197.8726)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.0874  max mem: 15572
Epoch: [14]  [ 370/1404]  eta: 0:10:22  lr: 0.000078  min_lr: 0.000001  loss: 4.4370 (4.3086)  class_acc: 0.2917 (0.2689)  loss_scale: 16384.0000 (19122.0270)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0773  max mem: 15572
[2025-01-16 23:42:47,894] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:42:47,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:42:47,933] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:42:47,934] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [ 380/1404]  eta: 0:10:15  lr: 0.000078  min_lr: 0.000001  loss: 4.1632 (4.3058)  class_acc: 0.2500 (0.2688)  loss_scale: 16384.0000 (19351.1811)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1067  max mem: 15572
Epoch: [14]  [ 390/1404]  eta: 0:10:08  lr: 0.000078  min_lr: 0.000001  loss: 4.3169 (4.3096)  class_acc: 0.2083 (0.2677)  loss_scale: 32768.0000 (19694.3223)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0815  max mem: 15572
Epoch: [14]  [ 400/1404]  eta: 0:10:02  lr: 0.000078  min_lr: 0.000001  loss: 4.3611 (4.3091)  class_acc: 0.2083 (0.2677)  loss_scale: 32768.0000 (20020.3491)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.0323  max mem: 15572
[2025-01-16 23:43:05,320] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20060
[2025-01-16 23:43:05,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:43:05,334] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20060
[2025-01-16 23:43:05,335] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:43:05,335] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [14]  [ 410/1404]  eta: 0:09:56  lr: 0.000078  min_lr: 0.000001  loss: 4.2764 (4.3091)  class_acc: 0.2083 (0.2668)  loss_scale: 32768.0000 (20051.4647)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0008  max mem: 15572
Epoch: [14]  [ 420/1404]  eta: 0:09:48  lr: 0.000078  min_lr: 0.000001  loss: 4.2427 (4.3065)  class_acc: 0.2500 (0.2661)  loss_scale: 16384.0000 (19964.3515)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0264  max mem: 15572
Epoch: [14]  [ 430/1404]  eta: 0:09:42  lr: 0.000078  min_lr: 0.000001  loss: 4.2745 (4.3061)  class_acc: 0.2500 (0.2657)  loss_scale: 16384.0000 (19881.2807)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.0448  max mem: 15572
Epoch: [14]  [ 440/1404]  eta: 0:09:35  lr: 0.000078  min_lr: 0.000001  loss: 4.2842 (4.3059)  class_acc: 0.2500 (0.2646)  loss_scale: 16384.0000 (19801.9773)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.0294  max mem: 15572
Epoch: [14]  [ 450/1404]  eta: 0:09:30  lr: 0.000078  min_lr: 0.000001  loss: 4.3147 (4.3063)  class_acc: 0.2500 (0.2632)  loss_scale: 16384.0000 (19726.1907)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0465  max mem: 15572
Epoch: [14]  [ 460/1404]  eta: 0:09:24  lr: 0.000078  min_lr: 0.000001  loss: 4.3003 (4.3058)  class_acc: 0.2500 (0.2632)  loss_scale: 16384.0000 (19653.6920)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.0361  max mem: 15572
Epoch: [14]  [ 470/1404]  eta: 0:09:20  lr: 0.000078  min_lr: 0.000001  loss: 4.3003 (4.3081)  class_acc: 0.2500 (0.2635)  loss_scale: 16384.0000 (19584.2718)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.0005  max mem: 15572
Epoch: [14]  [ 480/1404]  eta: 0:09:14  lr: 0.000078  min_lr: 0.000001  loss: 4.2399 (4.3075)  class_acc: 0.2917 (0.2646)  loss_scale: 16384.0000 (19517.7380)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.0006  max mem: 15572
Epoch: [14]  [ 490/1404]  eta: 0:09:06  lr: 0.000078  min_lr: 0.000001  loss: 4.1944 (4.3039)  class_acc: 0.3333 (0.2652)  loss_scale: 16384.0000 (19453.9145)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0007  max mem: 15572
Epoch: [14]  [ 500/1404]  eta: 0:08:59  lr: 0.000078  min_lr: 0.000001  loss: 4.2357 (4.3052)  class_acc: 0.2917 (0.2650)  loss_scale: 16384.0000 (19392.6387)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0007  max mem: 15572
Epoch: [14]  [ 510/1404]  eta: 0:08:54  lr: 0.000078  min_lr: 0.000001  loss: 4.3167 (4.3041)  class_acc: 0.2500 (0.2649)  loss_scale: 16384.0000 (19333.7613)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0007  max mem: 15572
Epoch: [14]  [ 520/1404]  eta: 0:08:47  lr: 0.000078  min_lr: 0.000001  loss: 4.2332 (4.3020)  class_acc: 0.2500 (0.2653)  loss_scale: 16384.0000 (19277.1440)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0007  max mem: 15572
Epoch: [14]  [ 530/1404]  eta: 0:08:40  lr: 0.000078  min_lr: 0.000001  loss: 4.2207 (4.3002)  class_acc: 0.2500 (0.2655)  loss_scale: 16384.0000 (19222.6591)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0007  max mem: 15572
[2025-01-16 23:44:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:44:20,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:44:20,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:44:20,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [ 540/1404]  eta: 0:08:35  lr: 0.000078  min_lr: 0.000001  loss: 4.1006 (4.2970)  class_acc: 0.2500 (0.2658)  loss_scale: 16384.0000 (19412.4658)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0007  max mem: 15572
Epoch: [14]  [ 550/1404]  eta: 0:08:28  lr: 0.000078  min_lr: 0.000001  loss: 4.2487 (4.2966)  class_acc: 0.2500 (0.2658)  loss_scale: 32768.0000 (19654.8530)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.0006  max mem: 15572
Epoch: [14]  [ 560/1404]  eta: 0:08:22  lr: 0.000078  min_lr: 0.000001  loss: 4.2487 (4.2943)  class_acc: 0.1667 (0.2660)  loss_scale: 32768.0000 (19888.5989)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0007  max mem: 15572
Epoch: [14]  [ 570/1404]  eta: 0:08:16  lr: 0.000078  min_lr: 0.000001  loss: 4.2037 (4.2960)  class_acc: 0.2500 (0.2661)  loss_scale: 32768.0000 (20114.1576)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0007  max mem: 15572
Epoch: [14]  [ 580/1404]  eta: 0:08:10  lr: 0.000078  min_lr: 0.000001  loss: 4.3292 (4.2955)  class_acc: 0.2500 (0.2661)  loss_scale: 32768.0000 (20331.9518)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0006  max mem: 15572
Epoch: [14]  [ 590/1404]  eta: 0:08:03  lr: 0.000078  min_lr: 0.000001  loss: 4.2847 (4.2958)  class_acc: 0.2083 (0.2658)  loss_scale: 32768.0000 (20542.3756)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0007  max mem: 15572
[2025-01-16 23:44:54,976] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20249
[2025-01-16 23:44:54,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:44:54,976] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 23:44:54,980] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20249
[2025-01-16 23:44:54,980] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [14]  [ 600/1404]  eta: 0:07:57  lr: 0.000078  min_lr: 0.000001  loss: 4.2847 (4.2961)  class_acc: 0.2500 (0.2664)  loss_scale: 32768.0000 (20527.7072)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0008  max mem: 15572
Epoch: [14]  [ 610/1404]  eta: 0:07:51  lr: 0.000078  min_lr: 0.000001  loss: 4.2728 (4.2967)  class_acc: 0.2500 (0.2662)  loss_scale: 16384.0000 (20459.8887)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.0238  max mem: 15572
Epoch: [14]  [ 620/1404]  eta: 0:07:45  lr: 0.000078  min_lr: 0.000001  loss: 4.2972 (4.2974)  class_acc: 0.2083 (0.2660)  loss_scale: 16384.0000 (20394.2544)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.0433  max mem: 15572
Epoch: [14]  [ 630/1404]  eta: 0:07:40  lr: 0.000078  min_lr: 0.000001  loss: 4.3446 (4.2994)  class_acc: 0.2083 (0.2649)  loss_scale: 16384.0000 (20330.7005)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0452  max mem: 15572
Epoch: [14]  [ 640/1404]  eta: 0:07:35  lr: 0.000078  min_lr: 0.000001  loss: 4.3446 (4.3011)  class_acc: 0.1667 (0.2637)  loss_scale: 16384.0000 (20269.1295)  weight_decay: 0.0500 (0.0500)  time: 0.6506  data: 0.0256  max mem: 15572
Epoch: [14]  [ 650/1404]  eta: 0:07:28  lr: 0.000078  min_lr: 0.000001  loss: 4.2598 (4.2989)  class_acc: 0.2500 (0.2652)  loss_scale: 16384.0000 (20209.4501)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0006  max mem: 15572
Epoch: [14]  [ 660/1404]  eta: 0:07:22  lr: 0.000078  min_lr: 0.000001  loss: 4.2659 (4.2997)  class_acc: 0.2917 (0.2654)  loss_scale: 16384.0000 (20151.5764)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0006  max mem: 15572
Epoch: [14]  [ 670/1404]  eta: 0:07:16  lr: 0.000078  min_lr: 0.000001  loss: 4.2659 (4.2996)  class_acc: 0.2083 (0.2649)  loss_scale: 16384.0000 (20095.4277)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.0005  max mem: 15572
Epoch: [14]  [ 680/1404]  eta: 0:07:10  lr: 0.000078  min_lr: 0.000001  loss: 4.2046 (4.2989)  class_acc: 0.1667 (0.2646)  loss_scale: 16384.0000 (20040.9280)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0006  max mem: 15572
Epoch: [14]  [ 690/1404]  eta: 0:07:03  lr: 0.000078  min_lr: 0.000001  loss: 4.2036 (4.2980)  class_acc: 0.2917 (0.2659)  loss_scale: 16384.0000 (19988.0058)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0006  max mem: 15572
Epoch: [14]  [ 700/1404]  eta: 0:06:58  lr: 0.000078  min_lr: 0.000001  loss: 4.1782 (4.2963)  class_acc: 0.3333 (0.2654)  loss_scale: 16384.0000 (19936.5934)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.0006  max mem: 15572
Epoch: [14]  [ 710/1404]  eta: 0:06:51  lr: 0.000078  min_lr: 0.000001  loss: 4.1280 (4.2955)  class_acc: 0.2500 (0.2652)  loss_scale: 16384.0000 (19886.6273)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0005  max mem: 15572
Epoch: [14]  [ 720/1404]  eta: 0:06:45  lr: 0.000078  min_lr: 0.000001  loss: 4.3706 (4.2972)  class_acc: 0.2500 (0.2647)  loss_scale: 16384.0000 (19838.0472)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0005  max mem: 15572
[2025-01-16 23:46:11,322] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:46:11,323] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:46:11,326] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:46:11,326] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [ 730/1404]  eta: 0:06:39  lr: 0.000078  min_lr: 0.000001  loss: 4.3759 (4.2963)  class_acc: 0.2500 (0.2651)  loss_scale: 16384.0000 (19992.5144)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0007  max mem: 15572
Epoch: [14]  [ 740/1404]  eta: 0:06:33  lr: 0.000078  min_lr: 0.000001  loss: 4.3151 (4.2967)  class_acc: 0.2500 (0.2646)  loss_scale: 32768.0000 (20164.9231)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0006  max mem: 15572
[2025-01-16 23:46:22,472] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20397
[2025-01-16 23:46:22,472] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:46:22,479] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20397
[2025-01-16 23:46:22,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:46:22,480] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [14]  [ 750/1404]  eta: 0:06:27  lr: 0.000078  min_lr: 0.000001  loss: 4.3661 (4.2984)  class_acc: 0.2083 (0.2646)  loss_scale: 16384.0000 (20114.5779)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0006  max mem: 15572
Epoch: [14]  [ 760/1404]  eta: 0:06:21  lr: 0.000078  min_lr: 0.000001  loss: 4.4103 (4.2980)  class_acc: 0.2500 (0.2646)  loss_scale: 16384.0000 (20065.5558)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0008  max mem: 15572
Epoch: [14]  [ 770/1404]  eta: 0:06:15  lr: 0.000078  min_lr: 0.000001  loss: 4.3418 (4.2993)  class_acc: 0.2917 (0.2646)  loss_scale: 16384.0000 (20017.8054)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0009  max mem: 15572
Epoch: [14]  [ 780/1404]  eta: 0:06:09  lr: 0.000078  min_lr: 0.000001  loss: 4.3360 (4.2985)  class_acc: 0.2500 (0.2652)  loss_scale: 16384.0000 (19971.2778)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0009  max mem: 15572
Epoch: [14]  [ 790/1404]  eta: 0:06:03  lr: 0.000078  min_lr: 0.000001  loss: 4.2186 (4.2969)  class_acc: 0.2917 (0.2656)  loss_scale: 16384.0000 (19925.9267)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0006  max mem: 15572
Epoch: [14]  [ 800/1404]  eta: 0:05:57  lr: 0.000078  min_lr: 0.000001  loss: 4.2908 (4.2987)  class_acc: 0.2500 (0.2653)  loss_scale: 16384.0000 (19881.7079)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0007  max mem: 15572
Epoch: [14]  [ 810/1404]  eta: 0:05:51  lr: 0.000077  min_lr: 0.000001  loss: 4.2963 (4.2973)  class_acc: 0.2500 (0.2657)  loss_scale: 16384.0000 (19838.5795)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0006  max mem: 15572
Epoch: [14]  [ 820/1404]  eta: 0:05:45  lr: 0.000077  min_lr: 0.000001  loss: 4.2614 (4.2980)  class_acc: 0.2500 (0.2653)  loss_scale: 16384.0000 (19796.5018)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0006  max mem: 15572
Epoch: [14]  [ 830/1404]  eta: 0:05:39  lr: 0.000077  min_lr: 0.000001  loss: 4.2614 (4.2984)  class_acc: 0.2500 (0.2654)  loss_scale: 16384.0000 (19755.4368)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.0008  max mem: 15572
Epoch: [14]  [ 840/1404]  eta: 0:05:33  lr: 0.000077  min_lr: 0.000001  loss: 4.2429 (4.2979)  class_acc: 0.2500 (0.2655)  loss_scale: 16384.0000 (19715.3484)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.0008  max mem: 15572
Epoch: [14]  [ 850/1404]  eta: 0:05:27  lr: 0.000077  min_lr: 0.000001  loss: 4.2464 (4.2988)  class_acc: 0.2500 (0.2651)  loss_scale: 16384.0000 (19676.2021)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0009  max mem: 15572
Epoch: [14]  [ 860/1404]  eta: 0:05:21  lr: 0.000077  min_lr: 0.000001  loss: 4.3196 (4.2993)  class_acc: 0.2500 (0.2649)  loss_scale: 16384.0000 (19637.9652)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0007  max mem: 15572
[2025-01-16 23:47:38,091] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:47:38,091] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [ 870/1404]  eta: 0:05:15  lr: 0.000077  min_lr: 0.000001  loss: 4.3641 (4.3003)  class_acc: 0.2500 (0.2647)  loss_scale: 16384.0000 (19619.4168)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0009  max mem: 15572
[2025-01-16 23:47:38,169] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:47:38,170] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [ 880/1404]  eta: 0:05:10  lr: 0.000077  min_lr: 0.000001  loss: 4.3921 (4.3015)  class_acc: 0.2500 (0.2648)  loss_scale: 32768.0000 (19768.6629)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.0010  max mem: 15572
Epoch: [14]  [ 890/1404]  eta: 0:05:04  lr: 0.000077  min_lr: 0.000001  loss: 4.3921 (4.3031)  class_acc: 0.2083 (0.2639)  loss_scale: 32768.0000 (19914.5589)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0005  max mem: 15572
Epoch: [14]  [ 900/1404]  eta: 0:04:58  lr: 0.000077  min_lr: 0.000001  loss: 4.3345 (4.3017)  class_acc: 0.2083 (0.2641)  loss_scale: 32768.0000 (20057.2164)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0005  max mem: 15572
[2025-01-16 23:47:56,329] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20557
[2025-01-16 23:47:56,330] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:47:56,330] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-16 23:47:56,330] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20557
[2025-01-16 23:47:56,331] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [14]  [ 910/1404]  eta: 0:04:52  lr: 0.000077  min_lr: 0.000001  loss: 4.2776 (4.3023)  class_acc: 0.2500 (0.2637)  loss_scale: 16384.0000 (20016.8957)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0041  max mem: 15572
Epoch: [14]  [ 920/1404]  eta: 0:04:46  lr: 0.000077  min_lr: 0.000001  loss: 4.3351 (4.3015)  class_acc: 0.2500 (0.2643)  loss_scale: 16384.0000 (19977.4506)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.0041  max mem: 15572
Epoch: [14]  [ 930/1404]  eta: 0:04:40  lr: 0.000077  min_lr: 0.000001  loss: 4.3471 (4.3027)  class_acc: 0.2500 (0.2641)  loss_scale: 16384.0000 (19938.8528)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0007  max mem: 15572
Epoch: [14]  [ 940/1404]  eta: 0:04:34  lr: 0.000077  min_lr: 0.000001  loss: 4.3548 (4.3020)  class_acc: 0.2500 (0.2643)  loss_scale: 16384.0000 (19901.0755)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0007  max mem: 15572
Epoch: [14]  [ 950/1404]  eta: 0:04:28  lr: 0.000077  min_lr: 0.000001  loss: 4.2944 (4.3017)  class_acc: 0.2500 (0.2645)  loss_scale: 16384.0000 (19864.0925)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0006  max mem: 15572
Epoch: [14]  [ 960/1404]  eta: 0:04:22  lr: 0.000077  min_lr: 0.000001  loss: 4.3340 (4.3025)  class_acc: 0.2500 (0.2647)  loss_scale: 16384.0000 (19827.8793)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0008  max mem: 15572
Epoch: [14]  [ 970/1404]  eta: 0:04:16  lr: 0.000077  min_lr: 0.000001  loss: 4.2979 (4.3024)  class_acc: 0.2500 (0.2644)  loss_scale: 16384.0000 (19792.4119)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0008  max mem: 15572
Epoch: [14]  [ 980/1404]  eta: 0:04:10  lr: 0.000077  min_lr: 0.000001  loss: 4.2979 (4.3023)  class_acc: 0.2500 (0.2646)  loss_scale: 16384.0000 (19757.6677)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0006  max mem: 15572
Epoch: [14]  [ 990/1404]  eta: 0:04:04  lr: 0.000077  min_lr: 0.000001  loss: 4.3377 (4.3018)  class_acc: 0.2500 (0.2642)  loss_scale: 16384.0000 (19723.6246)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0007  max mem: 15572
Epoch: [14]  [1000/1404]  eta: 0:03:58  lr: 0.000077  min_lr: 0.000001  loss: 4.3407 (4.3030)  class_acc: 0.2083 (0.2638)  loss_scale: 16384.0000 (19690.2617)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0007  max mem: 15572
Epoch: [14]  [1010/1404]  eta: 0:03:53  lr: 0.000077  min_lr: 0.000001  loss: 4.3278 (4.3027)  class_acc: 0.2500 (0.2648)  loss_scale: 16384.0000 (19657.5589)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.0009  max mem: 15572
Epoch: [14]  [1020/1404]  eta: 0:03:46  lr: 0.000077  min_lr: 0.000001  loss: 4.1707 (4.3020)  class_acc: 0.3333 (0.2649)  loss_scale: 16384.0000 (19625.4966)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0009  max mem: 15572
[2025-01-16 23:49:11,310] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:49:11,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:49:11,369] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:49:11,369] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [1030/1404]  eta: 0:03:40  lr: 0.000077  min_lr: 0.000001  loss: 4.2210 (4.3018)  class_acc: 0.3333 (0.2657)  loss_scale: 16384.0000 (19609.9476)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0007  max mem: 15572
Epoch: [14]  [1040/1404]  eta: 0:03:35  lr: 0.000077  min_lr: 0.000001  loss: 4.2715 (4.3013)  class_acc: 0.3333 (0.2662)  loss_scale: 32768.0000 (19736.3458)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0008  max mem: 15572
Epoch: [14]  [1050/1404]  eta: 0:03:29  lr: 0.000077  min_lr: 0.000001  loss: 4.3509 (4.3016)  class_acc: 0.2500 (0.2661)  loss_scale: 32768.0000 (19860.3387)  weight_decay: 0.0500 (0.0500)  time: 0.6736  data: 0.0010  max mem: 15572
Epoch: [14]  [1060/1404]  eta: 0:03:23  lr: 0.000077  min_lr: 0.000001  loss: 4.3298 (4.3017)  class_acc: 0.2083 (0.2658)  loss_scale: 32768.0000 (19981.9943)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0007  max mem: 15572
[2025-01-16 23:49:35,390] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20726
[2025-01-16 23:49:35,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:49:35,403] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20726
[2025-01-16 23:49:35,403] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:49:35,404] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [14]  [1070/1404]  eta: 0:03:17  lr: 0.000077  min_lr: 0.000001  loss: 4.3298 (4.3021)  class_acc: 0.2500 (0.2660)  loss_scale: 32768.0000 (20086.0803)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0006  max mem: 15572
Epoch: [14]  [1080/1404]  eta: 0:03:11  lr: 0.000077  min_lr: 0.000001  loss: 4.4079 (4.3027)  class_acc: 0.2917 (0.2663)  loss_scale: 16384.0000 (20051.8335)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0007  max mem: 15572
Epoch: [14]  [1090/1404]  eta: 0:03:05  lr: 0.000077  min_lr: 0.000001  loss: 4.3582 (4.3026)  class_acc: 0.2500 (0.2660)  loss_scale: 16384.0000 (20018.2145)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0007  max mem: 15572
Epoch: [14]  [1100/1404]  eta: 0:02:59  lr: 0.000077  min_lr: 0.000001  loss: 4.2597 (4.3032)  class_acc: 0.2500 (0.2657)  loss_scale: 16384.0000 (19985.2062)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0006  max mem: 15572
Epoch: [14]  [1110/1404]  eta: 0:02:53  lr: 0.000077  min_lr: 0.000001  loss: 4.3286 (4.3050)  class_acc: 0.2083 (0.2653)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.0006  max mem: 15572
Epoch: [14]  [1120/1404]  eta: 0:02:47  lr: 0.000077  min_lr: 0.000001  loss: 4.4746 (4.3063)  class_acc: 0.2083 (0.2655)  loss_scale: 16384.0000 (19920.9563)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.0006  max mem: 15572
Epoch: [14]  [1130/1404]  eta: 0:02:41  lr: 0.000077  min_lr: 0.000001  loss: 4.4209 (4.3075)  class_acc: 0.2500 (0.2653)  loss_scale: 16384.0000 (19889.6835)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0005  max mem: 15572
Epoch: [14]  [1140/1404]  eta: 0:02:35  lr: 0.000077  min_lr: 0.000001  loss: 4.3594 (4.3065)  class_acc: 0.2500 (0.2655)  loss_scale: 16384.0000 (19858.9588)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0006  max mem: 15572
Epoch: [14]  [1150/1404]  eta: 0:02:29  lr: 0.000077  min_lr: 0.000001  loss: 4.2812 (4.3069)  class_acc: 0.2500 (0.2651)  loss_scale: 16384.0000 (19828.7680)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0006  max mem: 15572
Epoch: [14]  [1160/1404]  eta: 0:02:23  lr: 0.000077  min_lr: 0.000001  loss: 4.4011 (4.3091)  class_acc: 0.2083 (0.2650)  loss_scale: 16384.0000 (19799.0973)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.0006  max mem: 15572
Epoch: [14]  [1170/1404]  eta: 0:02:17  lr: 0.000077  min_lr: 0.000001  loss: 4.3814 (4.3083)  class_acc: 0.2917 (0.2654)  loss_scale: 16384.0000 (19769.9334)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0007  max mem: 15572
Epoch: [14]  [1180/1404]  eta: 0:02:11  lr: 0.000077  min_lr: 0.000001  loss: 4.2236 (4.3088)  class_acc: 0.2917 (0.2653)  loss_scale: 16384.0000 (19741.2633)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0006  max mem: 15572
Epoch: [14]  [1190/1404]  eta: 0:02:06  lr: 0.000077  min_lr: 0.000001  loss: 4.3623 (4.3087)  class_acc: 0.2500 (0.2657)  loss_scale: 16384.0000 (19713.0747)  weight_decay: 0.0500 (0.0500)  time: 0.6573  data: 0.0006  max mem: 15572
[2025-01-16 23:50:50,591] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:50:50,593] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:50:50,594] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:50:50,594] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [1200/1404]  eta: 0:02:00  lr: 0.000077  min_lr: 0.000001  loss: 4.3623 (4.3094)  class_acc: 0.3333 (0.2661)  loss_scale: 16384.0000 (19712.6395)  weight_decay: 0.0500 (0.0500)  time: 0.6524  data: 0.0007  max mem: 15572
[2025-01-16 23:50:51,557] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20857
[2025-01-16 23:50:51,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:50:51,558] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20857
[2025-01-16 23:50:51,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:50:51,559] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [14]  [1210/1404]  eta: 0:01:54  lr: 0.000077  min_lr: 0.000001  loss: 4.2944 (4.3076)  class_acc: 0.3333 (0.2668)  loss_scale: 16384.0000 (19685.1528)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0008  max mem: 15572
Epoch: [14]  [1220/1404]  eta: 0:01:48  lr: 0.000077  min_lr: 0.000001  loss: 4.1609 (4.3063)  class_acc: 0.2500 (0.2665)  loss_scale: 16384.0000 (19658.1163)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0009  max mem: 15572
Epoch: [14]  [1230/1404]  eta: 0:01:42  lr: 0.000077  min_lr: 0.000001  loss: 4.0643 (4.3046)  class_acc: 0.2500 (0.2672)  loss_scale: 16384.0000 (19631.5191)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0006  max mem: 15572
Epoch: [14]  [1240/1404]  eta: 0:01:36  lr: 0.000077  min_lr: 0.000001  loss: 4.1497 (4.3038)  class_acc: 0.2500 (0.2671)  loss_scale: 16384.0000 (19605.3505)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.0006  max mem: 15572
Epoch: [14]  [1250/1404]  eta: 0:01:30  lr: 0.000076  min_lr: 0.000001  loss: 4.1951 (4.3033)  class_acc: 0.2500 (0.2672)  loss_scale: 16384.0000 (19579.6003)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.0006  max mem: 15572
Epoch: [14]  [1260/1404]  eta: 0:01:24  lr: 0.000076  min_lr: 0.000001  loss: 4.2757 (4.3033)  class_acc: 0.2500 (0.2670)  loss_scale: 16384.0000 (19554.2585)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0003  max mem: 15572
Epoch: [14]  [1270/1404]  eta: 0:01:18  lr: 0.000076  min_lr: 0.000001  loss: 4.3047 (4.3040)  class_acc: 0.2083 (0.2669)  loss_scale: 16384.0000 (19529.3155)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.0004  max mem: 15572
Epoch: [14]  [1280/1404]  eta: 0:01:13  lr: 0.000076  min_lr: 0.000001  loss: 4.3335 (4.3039)  class_acc: 0.2500 (0.2673)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.0006  max mem: 15572
Epoch: [14]  [1290/1404]  eta: 0:01:07  lr: 0.000076  min_lr: 0.000001  loss: 4.2107 (4.3030)  class_acc: 0.3333 (0.2681)  loss_scale: 16384.0000 (19480.5887)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0177  max mem: 15572
Epoch: [14]  [1300/1404]  eta: 0:01:01  lr: 0.000076  min_lr: 0.000001  loss: 4.2519 (4.3038)  class_acc: 0.2917 (0.2680)  loss_scale: 16384.0000 (19456.7871)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0389  max mem: 15572
Epoch: [14]  [1310/1404]  eta: 0:00:55  lr: 0.000076  min_lr: 0.000001  loss: 4.2519 (4.3035)  class_acc: 0.2083 (0.2680)  loss_scale: 16384.0000 (19433.3486)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0218  max mem: 15572
Epoch: [14]  [1320/1404]  eta: 0:00:49  lr: 0.000076  min_lr: 0.000001  loss: 4.1793 (4.3019)  class_acc: 0.2917 (0.2684)  loss_scale: 16384.0000 (19410.2650)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0849  max mem: 15572
[2025-01-16 23:52:06,663] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:52:06,663] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:52:06,678] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:52:06,678] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [1330/1404]  eta: 0:00:43  lr: 0.000076  min_lr: 0.000001  loss: 4.2408 (4.3007)  class_acc: 0.2917 (0.2686)  loss_scale: 16384.0000 (19399.8377)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0849  max mem: 15572
Epoch: [14]  [1340/1404]  eta: 0:00:37  lr: 0.000076  min_lr: 0.000001  loss: 4.3658 (4.3013)  class_acc: 0.2500 (0.2686)  loss_scale: 32768.0000 (19499.5257)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0089  max mem: 15572
[2025-01-16 23:52:13,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=126, lr=[7.38888092447839e-07, 7.38888092447839e-07, 1.0555544177826271e-06, 1.0555544177826271e-06, 1.5079348825466106e-06, 1.5079348825466106e-06, 2.154192689352301e-06, 2.154192689352301e-06, 3.077418127646144e-06, 3.077418127646144e-06, 4.396311610923064e-06, 4.396311610923064e-06, 6.280445158461519e-06, 6.280445158461519e-06, 8.972064512087886e-06, 8.972064512087886e-06, 1.2817235017268407e-05, 1.2817235017268407e-05, 1.8310335738954873e-05, 1.8310335738954873e-05, 2.6157622484221243e-05, 2.6157622484221243e-05, 3.736803212031606e-05, 3.736803212031606e-05, 5.3382903029022955e-05, 5.3382903029022955e-05, 7.626129004146137e-05, 7.626129004146137e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 23:52:13,126] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=47.89843891890058, CurrSamplesPerSec=50.930746369544956, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [1350/1404]  eta: 0:00:31  lr: 0.000076  min_lr: 0.000001  loss: 4.3658 (4.3015)  class_acc: 0.2500 (0.2687)  loss_scale: 32768.0000 (19597.7380)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0415  max mem: 15572
Epoch: [14]  [1360/1404]  eta: 0:00:25  lr: 0.000076  min_lr: 0.000001  loss: 4.3162 (4.3006)  class_acc: 0.2500 (0.2685)  loss_scale: 32768.0000 (19694.5070)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0744  max mem: 15572
Epoch: [14]  [1370/1404]  eta: 0:00:19  lr: 0.000076  min_lr: 0.000001  loss: 4.2862 (4.3006)  class_acc: 0.2917 (0.2687)  loss_scale: 32768.0000 (19789.8643)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0679  max mem: 15572
Epoch: [14]  [1380/1404]  eta: 0:00:14  lr: 0.000076  min_lr: 0.000001  loss: 4.2249 (4.3002)  class_acc: 0.2917 (0.2692)  loss_scale: 32768.0000 (19883.8407)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0427  max mem: 15572
Epoch: [14]  [1390/1404]  eta: 0:00:08  lr: 0.000076  min_lr: 0.000001  loss: 4.2248 (4.3001)  class_acc: 0.2500 (0.2694)  loss_scale: 32768.0000 (19976.4659)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0167  max mem: 15572
Epoch: [14]  [1400/1404]  eta: 0:00:02  lr: 0.000076  min_lr: 0.000001  loss: 4.2289 (4.3006)  class_acc: 0.3333 (0.2699)  loss_scale: 32768.0000 (20067.7687)  weight_decay: 0.0500 (0.0500)  time: 0.4775  data: 0.0005  max mem: 15572
Epoch: [14]  [1403/1404]  eta: 0:00:00  lr: 0.000076  min_lr: 0.000001  loss: 4.3289 (4.3012)  class_acc: 0.2917 (0.2698)  loss_scale: 32768.0000 (20094.9060)  weight_decay: 0.0500 (0.0500)  time: 0.4488  data: 0.0004  max mem: 15572
Epoch: [14] Total time: 0:13:42 (0.5858 s / it)
Averaged stats: lr: 0.000076  min_lr: 0.000001  loss: 4.3289 (4.2987)  class_acc: 0.2917 (0.2719)  loss_scale: 32768.0000 (20094.9060)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:12:47  loss: 1.7402 (1.7402)  acc1: 77.7778 (77.7778)  acc5: 83.3333 (83.3333)  time: 5.6397  data: 5.3026  max mem: 15572
Val:  [ 10/136]  eta: 0:01:41  loss: 2.8362 (2.5876)  acc1: 50.0000 (43.4343)  acc5: 72.2222 (73.7374)  time: 0.8093  data: 0.6132  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 2.8736 (2.7233)  acc1: 33.3333 (37.5661)  acc5: 66.6667 (70.3704)  time: 0.3181  data: 0.1319  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 2.6789 (2.5736)  acc1: 33.3333 (41.7563)  acc5: 72.2222 (74.0143)  time: 0.3515  data: 0.1560  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.2005 (2.5178)  acc1: 50.0000 (43.9024)  acc5: 83.3333 (75.4743)  time: 0.3599  data: 0.1567  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.4399 (2.5599)  acc1: 44.4444 (42.5926)  acc5: 77.7778 (75.4902)  time: 0.3300  data: 0.1252  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.7051 (2.6530)  acc1: 33.3333 (39.9818)  acc5: 72.2222 (73.2240)  time: 0.3300  data: 0.1320  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.7051 (2.6268)  acc1: 38.8889 (41.4710)  acc5: 72.2222 (74.1002)  time: 0.3402  data: 0.1310  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.4813 (2.6225)  acc1: 44.4444 (41.2894)  acc5: 83.3333 (74.7599)  time: 0.3532  data: 0.1282  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.6226 (2.6290)  acc1: 44.4444 (41.2698)  acc5: 77.7778 (74.4200)  time: 0.3291  data: 0.1046  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.8354 (2.6887)  acc1: 33.3333 (39.4389)  acc5: 66.6667 (72.6073)  time: 0.3222  data: 0.1180  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.7643 (2.6816)  acc1: 33.3333 (39.8899)  acc5: 66.6667 (72.7227)  time: 0.3560  data: 0.1660  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.4097 (2.6465)  acc1: 44.4444 (40.8632)  acc5: 83.3333 (73.7833)  time: 0.3766  data: 0.1724  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.2356 (2.6162)  acc1: 50.0000 (41.6879)  acc5: 83.3333 (74.4275)  time: 0.2903  data: 0.1047  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.3172 (2.6196)  acc1: 44.4444 (41.5643)  acc5: 77.7778 (74.2834)  time: 0.1802  data: 0.0206  max mem: 15572
Val: Total time: 0:00:49 (0.3651 s / it)
* Acc@1 41.319 Acc@5 73.423 loss 2.653
Accuracy of the network on the 4883 val videos: 41.3%
[2025-01-16 23:53:34,654] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 23:53:34,656] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 23:53:34,656] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 23:53:34,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-16 23:53:37,059] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 23:53:37,059] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.32%
Epoch: [15]  [   0/1404]  eta: 2:41:09  lr: 0.000076  min_lr: 0.000001  loss: 4.2928 (4.2928)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.8872  data: 6.3560  max mem: 15572
[2025-01-16 23:53:47,669] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21066
[2025-01-16 23:53:47,670] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:53:47,711] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21066
[2025-01-16 23:53:47,711] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:53:47,712] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [  10/1404]  eta: 0:27:42  lr: 0.000076  min_lr: 0.000001  loss: 4.2069 (4.1583)  class_acc: 0.2083 (0.2765)  loss_scale: 32768.0000 (25320.7273)  weight_decay: 0.0500 (0.0500)  time: 1.1926  data: 0.6280  max mem: 15572
Epoch: [15]  [  20/1404]  eta: 0:20:21  lr: 0.000076  min_lr: 0.000001  loss: 4.0977 (4.1560)  class_acc: 0.2917 (0.3115)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0698  max mem: 15572
Epoch: [15]  [  30/1404]  eta: 0:18:42  lr: 0.000076  min_lr: 0.000001  loss: 4.1730 (4.1953)  class_acc: 0.2917 (0.3038)  loss_scale: 16384.0000 (19555.0968)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0972  max mem: 15572
Epoch: [15]  [  40/1404]  eta: 0:17:16  lr: 0.000076  min_lr: 0.000001  loss: 4.3821 (4.2303)  class_acc: 0.2500 (0.3018)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1002  max mem: 15572
Epoch: [15]  [  50/1404]  eta: 0:16:28  lr: 0.000076  min_lr: 0.000001  loss: 4.3821 (4.2275)  class_acc: 0.2917 (0.3096)  loss_scale: 16384.0000 (18311.5294)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1043  max mem: 15572
Epoch: [15]  [  60/1404]  eta: 0:15:35  lr: 0.000076  min_lr: 0.000001  loss: 4.2874 (4.2230)  class_acc: 0.2917 (0.3026)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0775  max mem: 15572
Epoch: [15]  [  70/1404]  eta: 0:15:05  lr: 0.000076  min_lr: 0.000001  loss: 4.1351 (4.2296)  class_acc: 0.2500 (0.3016)  loss_scale: 16384.0000 (17768.5634)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0189  max mem: 15572
Epoch: [15]  [  80/1404]  eta: 0:14:38  lr: 0.000076  min_lr: 0.000001  loss: 4.1351 (4.2219)  class_acc: 0.2500 (0.3014)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0100  max mem: 15572
Epoch: [15]  [  90/1404]  eta: 0:14:23  lr: 0.000076  min_lr: 0.000001  loss: 4.2310 (4.2267)  class_acc: 0.2500 (0.2949)  loss_scale: 16384.0000 (17464.2637)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0100  max mem: 15572
Epoch: [15]  [ 100/1404]  eta: 0:13:57  lr: 0.000076  min_lr: 0.000001  loss: 4.2982 (4.2359)  class_acc: 0.2500 (0.2929)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0007  max mem: 15572
Epoch: [15]  [ 110/1404]  eta: 0:13:43  lr: 0.000076  min_lr: 0.000001  loss: 4.2836 (4.2384)  class_acc: 0.2500 (0.2947)  loss_scale: 16384.0000 (17269.6216)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0006  max mem: 15572
Epoch: [15]  [ 120/1404]  eta: 0:13:32  lr: 0.000076  min_lr: 0.000001  loss: 4.2075 (4.2347)  class_acc: 0.2500 (0.2913)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0006  max mem: 15572
Epoch: [15]  [ 130/1404]  eta: 0:13:23  lr: 0.000076  min_lr: 0.000001  loss: 4.3492 (4.2422)  class_acc: 0.2500 (0.2891)  loss_scale: 16384.0000 (17134.4122)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0072  max mem: 15572
[2025-01-16 23:55:02,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:55:02,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:55:02,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:55:02,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [ 140/1404]  eta: 0:13:10  lr: 0.000076  min_lr: 0.000001  loss: 4.3549 (4.2460)  class_acc: 0.2083 (0.2858)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.0119  max mem: 15572
Epoch: [15]  [ 150/1404]  eta: 0:13:01  lr: 0.000076  min_lr: 0.000001  loss: 4.2634 (4.2490)  class_acc: 0.2500 (0.2856)  loss_scale: 32768.0000 (18771.0728)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0054  max mem: 15572
Epoch: [15]  [ 160/1404]  eta: 0:12:57  lr: 0.000076  min_lr: 0.000001  loss: 4.2634 (4.2499)  class_acc: 0.2917 (0.2873)  loss_scale: 32768.0000 (19640.4472)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.0006  max mem: 15572
Epoch: [15]  [ 170/1404]  eta: 0:12:50  lr: 0.000076  min_lr: 0.000001  loss: 4.2503 (4.2516)  class_acc: 0.2500 (0.2873)  loss_scale: 32768.0000 (20408.1404)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.0007  max mem: 15572
Epoch: [15]  [ 180/1404]  eta: 0:12:39  lr: 0.000076  min_lr: 0.000001  loss: 4.2503 (4.2474)  class_acc: 0.2083 (0.2836)  loss_scale: 32768.0000 (21091.0055)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0010  max mem: 15572
Epoch: [15]  [ 190/1404]  eta: 0:12:27  lr: 0.000076  min_lr: 0.000001  loss: 4.2727 (4.2508)  class_acc: 0.2500 (0.2832)  loss_scale: 32768.0000 (21702.3665)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0008  max mem: 15572
Epoch: [15]  [ 200/1404]  eta: 0:12:19  lr: 0.000076  min_lr: 0.000001  loss: 4.3490 (4.2555)  class_acc: 0.2500 (0.2819)  loss_scale: 32768.0000 (22252.8955)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0007  max mem: 15572
Epoch: [15]  [ 210/1404]  eta: 0:12:11  lr: 0.000076  min_lr: 0.000001  loss: 4.3490 (4.2570)  class_acc: 0.2917 (0.2834)  loss_scale: 32768.0000 (22751.2417)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0007  max mem: 15572
Epoch: [15]  [ 220/1404]  eta: 0:12:06  lr: 0.000076  min_lr: 0.000001  loss: 4.3861 (4.2595)  class_acc: 0.2500 (0.2821)  loss_scale: 32768.0000 (23204.4887)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0007  max mem: 15572
Epoch: [15]  [ 230/1404]  eta: 0:11:55  lr: 0.000076  min_lr: 0.000001  loss: 4.2445 (4.2576)  class_acc: 0.2500 (0.2832)  loss_scale: 32768.0000 (23618.4935)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.0006  max mem: 15572
[2025-01-16 23:55:58,334] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21291
[2025-01-16 23:55:58,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:55:58,340] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21291
[2025-01-16 23:55:58,340] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:55:58,340] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [ 240/1404]  eta: 0:11:44  lr: 0.000076  min_lr: 0.000001  loss: 4.1905 (4.2550)  class_acc: 0.2917 (0.2835)  loss_scale: 16384.0000 (23318.3071)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0006  max mem: 15572
Epoch: [15]  [ 250/1404]  eta: 0:11:40  lr: 0.000076  min_lr: 0.000001  loss: 4.0344 (4.2445)  class_acc: 0.2917 (0.2852)  loss_scale: 16384.0000 (23042.0398)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0007  max mem: 15572
Epoch: [15]  [ 260/1404]  eta: 0:11:35  lr: 0.000076  min_lr: 0.000001  loss: 4.1862 (4.2464)  class_acc: 0.3333 (0.2882)  loss_scale: 16384.0000 (22786.9425)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.0008  max mem: 15572
Epoch: [15]  [ 270/1404]  eta: 0:11:28  lr: 0.000075  min_lr: 0.000001  loss: 4.4017 (4.2522)  class_acc: 0.2500 (0.2861)  loss_scale: 16384.0000 (22550.6716)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0009  max mem: 15572
Epoch: [15]  [ 280/1404]  eta: 0:11:20  lr: 0.000075  min_lr: 0.000001  loss: 4.3953 (4.2538)  class_acc: 0.2500 (0.2845)  loss_scale: 16384.0000 (22331.2171)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0009  max mem: 15572
Epoch: [15]  [ 290/1404]  eta: 0:11:11  lr: 0.000075  min_lr: 0.000001  loss: 4.3186 (4.2616)  class_acc: 0.2500 (0.2832)  loss_scale: 16384.0000 (22126.8454)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0007  max mem: 15572
Epoch: [15]  [ 300/1404]  eta: 0:11:07  lr: 0.000075  min_lr: 0.000001  loss: 4.3820 (4.2674)  class_acc: 0.1667 (0.2805)  loss_scale: 16384.0000 (21936.0532)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0006  max mem: 15572
Epoch: [15]  [ 310/1404]  eta: 0:11:00  lr: 0.000075  min_lr: 0.000001  loss: 4.4862 (4.2765)  class_acc: 0.1667 (0.2777)  loss_scale: 16384.0000 (21757.5305)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0008  max mem: 15572
Epoch: [15]  [ 320/1404]  eta: 0:10:58  lr: 0.000075  min_lr: 0.000001  loss: 4.5143 (4.2782)  class_acc: 0.2083 (0.2769)  loss_scale: 16384.0000 (21590.1308)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.0008  max mem: 15572
Epoch: [15]  [ 330/1404]  eta: 0:10:49  lr: 0.000075  min_lr: 0.000001  loss: 4.2760 (4.2771)  class_acc: 0.2500 (0.2764)  loss_scale: 16384.0000 (21432.8459)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0009  max mem: 15572
Epoch: [15]  [ 340/1404]  eta: 0:10:44  lr: 0.000075  min_lr: 0.000001  loss: 4.3123 (4.2831)  class_acc: 0.2500 (0.2761)  loss_scale: 16384.0000 (21284.7859)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.0010  max mem: 15572
Epoch: [15]  [ 350/1404]  eta: 0:10:36  lr: 0.000075  min_lr: 0.000001  loss: 4.4092 (4.2845)  class_acc: 0.2500 (0.2761)  loss_scale: 16384.0000 (21145.1624)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0007  max mem: 15572
[2025-01-16 23:57:15,189] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:57:15,189] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:57:15,189] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:57:15,190] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [ 360/1404]  eta: 0:10:30  lr: 0.000075  min_lr: 0.000001  loss: 4.3047 (4.2846)  class_acc: 0.2500 (0.2767)  loss_scale: 16384.0000 (21058.6593)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0006  max mem: 15572
Epoch: [15]  [ 370/1404]  eta: 0:10:23  lr: 0.000075  min_lr: 0.000001  loss: 4.3047 (4.2830)  class_acc: 0.2500 (0.2770)  loss_scale: 32768.0000 (21374.2749)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0007  max mem: 15572
Epoch: [15]  [ 380/1404]  eta: 0:10:19  lr: 0.000075  min_lr: 0.000001  loss: 4.3259 (4.2840)  class_acc: 0.2917 (0.2780)  loss_scale: 32768.0000 (21673.3228)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.0007  max mem: 15572
Epoch: [15]  [ 390/1404]  eta: 0:10:10  lr: 0.000075  min_lr: 0.000001  loss: 4.3369 (4.2825)  class_acc: 0.2917 (0.2786)  loss_scale: 32768.0000 (21957.0742)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0006  max mem: 15572
[2025-01-16 23:57:35,852] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21457
[2025-01-16 23:57:35,853] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:57:35,913] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21457
[2025-01-16 23:57:35,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:57:35,915] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [ 400/1404]  eta: 0:10:01  lr: 0.000075  min_lr: 0.000001  loss: 4.2684 (4.2805)  class_acc: 0.2917 (0.2796)  loss_scale: 32768.0000 (22063.2419)  weight_decay: 0.0500 (0.0500)  time: 0.4785  data: 0.0006  max mem: 15572
Epoch: [15]  [ 410/1404]  eta: 0:09:53  lr: 0.000075  min_lr: 0.000001  loss: 4.2392 (4.2797)  class_acc: 0.2917 (0.2806)  loss_scale: 16384.0000 (21925.0608)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0006  max mem: 15572
Epoch: [15]  [ 420/1404]  eta: 0:09:48  lr: 0.000075  min_lr: 0.000001  loss: 4.2392 (4.2789)  class_acc: 0.3333 (0.2819)  loss_scale: 16384.0000 (21793.4442)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0336  max mem: 15572
Epoch: [15]  [ 430/1404]  eta: 0:09:40  lr: 0.000075  min_lr: 0.000001  loss: 4.3114 (4.2807)  class_acc: 0.2917 (0.2830)  loss_scale: 16384.0000 (21667.9350)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0368  max mem: 15572
Epoch: [15]  [ 440/1404]  eta: 0:09:34  lr: 0.000075  min_lr: 0.000001  loss: 4.2441 (4.2785)  class_acc: 0.2917 (0.2833)  loss_scale: 16384.0000 (21548.1179)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0039  max mem: 15572
Epoch: [15]  [ 450/1404]  eta: 0:09:27  lr: 0.000075  min_lr: 0.000001  loss: 4.1586 (4.2750)  class_acc: 0.2500 (0.2836)  loss_scale: 16384.0000 (21433.6142)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0010  max mem: 15572
Epoch: [15]  [ 460/1404]  eta: 0:09:23  lr: 0.000075  min_lr: 0.000001  loss: 4.1729 (4.2760)  class_acc: 0.2500 (0.2832)  loss_scale: 16384.0000 (21324.0781)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.0594  max mem: 15572
Epoch: [15]  [ 470/1404]  eta: 0:09:15  lr: 0.000075  min_lr: 0.000001  loss: 4.3482 (4.2775)  class_acc: 0.2500 (0.2830)  loss_scale: 16384.0000 (21219.1932)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0718  max mem: 15572
Epoch: [15]  [ 480/1404]  eta: 0:09:10  lr: 0.000075  min_lr: 0.000001  loss: 4.1979 (4.2718)  class_acc: 0.2917 (0.2839)  loss_scale: 16384.0000 (21118.6694)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0512  max mem: 15572
Epoch: [15]  [ 490/1404]  eta: 0:09:04  lr: 0.000075  min_lr: 0.000001  loss: 4.1965 (4.2711)  class_acc: 0.2917 (0.2831)  loss_scale: 16384.0000 (21022.2403)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1065  max mem: 15572
Epoch: [15]  [ 500/1404]  eta: 0:08:58  lr: 0.000075  min_lr: 0.000001  loss: 4.1787 (4.2688)  class_acc: 0.2500 (0.2837)  loss_scale: 16384.0000 (20929.6607)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.0771  max mem: 15572
Epoch: [15]  [ 510/1404]  eta: 0:08:53  lr: 0.000075  min_lr: 0.000001  loss: 4.1674 (4.2703)  class_acc: 0.2500 (0.2831)  loss_scale: 16384.0000 (20840.7045)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1149  max mem: 15572
Epoch: [15]  [ 520/1404]  eta: 0:08:47  lr: 0.000075  min_lr: 0.000001  loss: 4.2688 (4.2701)  class_acc: 0.2500 (0.2836)  loss_scale: 16384.0000 (20755.1631)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.1673  max mem: 15572
[2025-01-16 23:58:52,136] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:58:52,136] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-16 23:58:52,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 23:58:52,142] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [ 530/1404]  eta: 0:08:41  lr: 0.000075  min_lr: 0.000001  loss: 4.2128 (4.2687)  class_acc: 0.2917 (0.2840)  loss_scale: 16384.0000 (20827.1186)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0999  max mem: 15572
[2025-01-16 23:58:56,834] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21594
[2025-01-16 23:58:56,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:58:56,834] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21594
[2025-01-16 23:58:56,835] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 23:58:56,835] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [ 540/1404]  eta: 0:08:34  lr: 0.000075  min_lr: 0.000001  loss: 4.1503 (4.2645)  class_acc: 0.2917 (0.2849)  loss_scale: 16384.0000 (20835.8447)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0684  max mem: 15572
Epoch: [15]  [ 550/1404]  eta: 0:08:29  lr: 0.000075  min_lr: 0.000001  loss: 4.1342 (4.2634)  class_acc: 0.2917 (0.2855)  loss_scale: 16384.0000 (20755.0490)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0359  max mem: 15572
Epoch: [15]  [ 560/1404]  eta: 0:08:22  lr: 0.000075  min_lr: 0.000001  loss: 4.2759 (4.2650)  class_acc: 0.2917 (0.2854)  loss_scale: 16384.0000 (20677.1337)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0149  max mem: 15572
Epoch: [15]  [ 570/1404]  eta: 0:08:16  lr: 0.000075  min_lr: 0.000001  loss: 4.3460 (4.2675)  class_acc: 0.2083 (0.2838)  loss_scale: 16384.0000 (20601.9475)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0535  max mem: 15572
Epoch: [15]  [ 580/1404]  eta: 0:08:09  lr: 0.000075  min_lr: 0.000001  loss: 4.3844 (4.2687)  class_acc: 0.2083 (0.2831)  loss_scale: 16384.0000 (20529.3494)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0844  max mem: 15572
Epoch: [15]  [ 590/1404]  eta: 0:08:04  lr: 0.000075  min_lr: 0.000001  loss: 4.3774 (4.2710)  class_acc: 0.2083 (0.2821)  loss_scale: 16384.0000 (20459.2081)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0866  max mem: 15572
Epoch: [15]  [ 600/1404]  eta: 0:07:58  lr: 0.000075  min_lr: 0.000001  loss: 4.3781 (4.2734)  class_acc: 0.2500 (0.2826)  loss_scale: 16384.0000 (20391.4010)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1221  max mem: 15572
Epoch: [15]  [ 610/1404]  eta: 0:07:53  lr: 0.000075  min_lr: 0.000001  loss: 4.3781 (4.2743)  class_acc: 0.3333 (0.2828)  loss_scale: 16384.0000 (20325.8134)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1487  max mem: 15572
Epoch: [15]  [ 620/1404]  eta: 0:07:46  lr: 0.000075  min_lr: 0.000001  loss: 4.3513 (4.2741)  class_acc: 0.2917 (0.2827)  loss_scale: 16384.0000 (20262.3382)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0740  max mem: 15572
Epoch: [15]  [ 630/1404]  eta: 0:07:40  lr: 0.000075  min_lr: 0.000001  loss: 4.3659 (4.2771)  class_acc: 0.2500 (0.2822)  loss_scale: 16384.0000 (20200.8748)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0253  max mem: 15572
Epoch: [15]  [ 640/1404]  eta: 0:07:34  lr: 0.000075  min_lr: 0.000001  loss: 4.3749 (4.2789)  class_acc: 0.2500 (0.2817)  loss_scale: 16384.0000 (20141.3292)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0787  max mem: 15572
Epoch: [15]  [ 650/1404]  eta: 0:07:29  lr: 0.000075  min_lr: 0.000001  loss: 4.3060 (4.2794)  class_acc: 0.2500 (0.2811)  loss_scale: 16384.0000 (20083.6129)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.1515  max mem: 15572
Epoch: [15]  [ 660/1404]  eta: 0:07:22  lr: 0.000075  min_lr: 0.000001  loss: 4.3448 (4.2812)  class_acc: 0.2500 (0.2805)  loss_scale: 16384.0000 (20027.6430)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0978  max mem: 15572
[2025-01-17 00:00:12,055] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:00:12,056] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:00:12,110] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:00:12,111] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [ 670/1404]  eta: 0:07:15  lr: 0.000075  min_lr: 0.000001  loss: 4.2746 (4.2799)  class_acc: 0.2500 (0.2801)  loss_scale: 16384.0000 (20168.6796)  weight_decay: 0.0500 (0.0500)  time: 0.5051  data: 0.0006  max mem: 15572
Epoch: [15]  [ 680/1404]  eta: 0:07:09  lr: 0.000075  min_lr: 0.000001  loss: 4.2474 (4.2784)  class_acc: 0.2917 (0.2810)  loss_scale: 32768.0000 (20353.6916)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0328  max mem: 15572
[2025-01-17 00:00:26,413] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21748
[2025-01-17 00:00:26,413] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:00:26,518] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21748
[2025-01-17 00:00:26,519] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:00:26,519] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [ 690/1404]  eta: 0:07:03  lr: 0.000074  min_lr: 0.000001  loss: 4.3716 (4.2793)  class_acc: 0.2917 (0.2808)  loss_scale: 32768.0000 (20462.2171)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0857  max mem: 15572
Epoch: [15]  [ 700/1404]  eta: 0:06:57  lr: 0.000074  min_lr: 0.000001  loss: 4.4130 (4.2816)  class_acc: 0.2083 (0.2803)  loss_scale: 16384.0000 (20404.0399)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1001  max mem: 15572
Epoch: [15]  [ 710/1404]  eta: 0:06:51  lr: 0.000074  min_lr: 0.000001  loss: 4.2390 (4.2804)  class_acc: 0.2083 (0.2795)  loss_scale: 16384.0000 (20347.4993)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0561  max mem: 15572
Epoch: [15]  [ 720/1404]  eta: 0:06:45  lr: 0.000074  min_lr: 0.000001  loss: 4.2390 (4.2803)  class_acc: 0.2083 (0.2798)  loss_scale: 16384.0000 (20292.5270)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0611  max mem: 15572
Epoch: [15]  [ 730/1404]  eta: 0:06:39  lr: 0.000074  min_lr: 0.000001  loss: 4.2546 (4.2797)  class_acc: 0.2917 (0.2795)  loss_scale: 16384.0000 (20239.0588)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1059  max mem: 15572
Epoch: [15]  [ 740/1404]  eta: 0:06:33  lr: 0.000074  min_lr: 0.000001  loss: 4.1933 (4.2787)  class_acc: 0.2500 (0.2799)  loss_scale: 16384.0000 (20187.0337)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0866  max mem: 15572
Epoch: [15]  [ 750/1404]  eta: 0:06:28  lr: 0.000074  min_lr: 0.000001  loss: 4.2548 (4.2792)  class_acc: 0.2917 (0.2801)  loss_scale: 16384.0000 (20136.3941)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1078  max mem: 15572
Epoch: [15]  [ 760/1404]  eta: 0:06:21  lr: 0.000074  min_lr: 0.000001  loss: 4.2602 (4.2791)  class_acc: 0.2500 (0.2804)  loss_scale: 16384.0000 (20087.0854)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0755  max mem: 15572
Epoch: [15]  [ 770/1404]  eta: 0:06:15  lr: 0.000074  min_lr: 0.000001  loss: 4.3724 (4.2816)  class_acc: 0.2500 (0.2801)  loss_scale: 16384.0000 (20039.0558)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0407  max mem: 15572
Epoch: [15]  [ 780/1404]  eta: 0:06:09  lr: 0.000074  min_lr: 0.000001  loss: 4.4093 (4.2832)  class_acc: 0.2083 (0.2796)  loss_scale: 16384.0000 (19992.2561)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0943  max mem: 15572
Epoch: [15]  [ 790/1404]  eta: 0:06:03  lr: 0.000074  min_lr: 0.000001  loss: 4.2980 (4.2813)  class_acc: 0.2083 (0.2801)  loss_scale: 16384.0000 (19946.6397)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0850  max mem: 15572
Epoch: [15]  [ 800/1404]  eta: 0:05:57  lr: 0.000074  min_lr: 0.000001  loss: 4.2916 (4.2812)  class_acc: 0.2500 (0.2801)  loss_scale: 16384.0000 (19902.1623)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0712  max mem: 15572
Epoch: [15]  [ 810/1404]  eta: 0:05:51  lr: 0.000074  min_lr: 0.000001  loss: 4.3394 (4.2816)  class_acc: 0.2500 (0.2801)  loss_scale: 16384.0000 (19858.7818)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.0917  max mem: 15572
[2025-01-17 00:01:42,282] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:01:42,282] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:01:42,283] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:01:42,284] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [ 820/1404]  eta: 0:05:45  lr: 0.000074  min_lr: 0.000001  loss: 4.3678 (4.2834)  class_acc: 0.2917 (0.2797)  loss_scale: 16384.0000 (19896.2826)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1225  max mem: 15572
Epoch: [15]  [ 830/1404]  eta: 0:05:39  lr: 0.000074  min_lr: 0.000001  loss: 4.2492 (4.2815)  class_acc: 0.2917 (0.2802)  loss_scale: 32768.0000 (20051.1769)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0713  max mem: 15572
Epoch: [15]  [ 840/1404]  eta: 0:05:32  lr: 0.000074  min_lr: 0.000001  loss: 4.2582 (4.2827)  class_acc: 0.2500 (0.2796)  loss_scale: 32768.0000 (20202.3876)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0130  max mem: 15572
Epoch: [15]  [ 850/1404]  eta: 0:05:26  lr: 0.000074  min_lr: 0.000001  loss: 4.2777 (4.2820)  class_acc: 0.2500 (0.2801)  loss_scale: 32768.0000 (20350.0447)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0586  max mem: 15572
Epoch: [15]  [ 860/1404]  eta: 0:05:21  lr: 0.000074  min_lr: 0.000001  loss: 4.2166 (4.2827)  class_acc: 0.3333 (0.2805)  loss_scale: 32768.0000 (20494.2718)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0864  max mem: 15572
Epoch: [15]  [ 870/1404]  eta: 0:05:15  lr: 0.000074  min_lr: 0.000001  loss: 4.3953 (4.2841)  class_acc: 0.2500 (0.2799)  loss_scale: 32768.0000 (20635.1871)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1058  max mem: 15572
Epoch: [15]  [ 880/1404]  eta: 0:05:09  lr: 0.000074  min_lr: 0.000001  loss: 4.3684 (4.2841)  class_acc: 0.2500 (0.2806)  loss_scale: 32768.0000 (20772.9035)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1308  max mem: 15572
[2025-01-17 00:02:21,236] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21944
[2025-01-17 00:02:21,236] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:02:21,269] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21944
[2025-01-17 00:02:21,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:02:21,271] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [ 890/1404]  eta: 0:05:03  lr: 0.000074  min_lr: 0.000001  loss: 4.2956 (4.2848)  class_acc: 0.2083 (0.2795)  loss_scale: 32768.0000 (20778.8103)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1382  max mem: 15572
Epoch: [15]  [ 900/1404]  eta: 0:04:58  lr: 0.000074  min_lr: 0.000001  loss: 4.3577 (4.2862)  class_acc: 0.2083 (0.2796)  loss_scale: 16384.0000 (20730.0333)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1446  max mem: 15572
Epoch: [15]  [ 910/1404]  eta: 0:04:52  lr: 0.000074  min_lr: 0.000001  loss: 4.4670 (4.2875)  class_acc: 0.2083 (0.2793)  loss_scale: 16384.0000 (20682.3271)  weight_decay: 0.0500 (0.0500)  time: 0.6498  data: 0.1567  max mem: 15572
Epoch: [15]  [ 920/1404]  eta: 0:04:46  lr: 0.000074  min_lr: 0.000001  loss: 4.3052 (4.2879)  class_acc: 0.2500 (0.2791)  loss_scale: 16384.0000 (20635.6569)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0852  max mem: 15572
Epoch: [15]  [ 930/1404]  eta: 0:04:40  lr: 0.000074  min_lr: 0.000001  loss: 4.3213 (4.2887)  class_acc: 0.2500 (0.2790)  loss_scale: 16384.0000 (20589.9893)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0173  max mem: 15572
[2025-01-17 00:02:52,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=132, lr=[7.157152265402117e-07, 7.157152265402117e-07, 1.0224503236288742e-06, 1.0224503236288742e-06, 1.4606433194698203e-06, 1.4606433194698203e-06, 2.086633313528315e-06, 2.086633313528315e-06, 2.9809047336118782e-06, 2.9809047336118782e-06, 4.258435333731255e-06, 4.258435333731255e-06, 6.083479048187507e-06, 6.083479048187507e-06, 8.690684354553583e-06, 8.690684354553583e-06, 1.2415263363647975e-05, 1.2415263363647975e-05, 1.773609051949711e-05, 1.773609051949711e-05, 2.5337272170710155e-05, 2.5337272170710155e-05, 3.619610310101451e-05, 3.619610310101451e-05, 5.1708718715735025e-05, 5.1708718715735025e-05, 7.386959816533575e-05, 7.386959816533575e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 00:02:52,782] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=47.898760835634775, CurrSamplesPerSec=58.318953560723074, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [15]  [ 940/1404]  eta: 0:04:34  lr: 0.000074  min_lr: 0.000001  loss: 4.3219 (4.2882)  class_acc: 0.2500 (0.2798)  loss_scale: 16384.0000 (20545.2922)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0644  max mem: 15572
Epoch: [15]  [ 950/1404]  eta: 0:04:28  lr: 0.000074  min_lr: 0.000001  loss: 4.3791 (4.2891)  class_acc: 0.2500 (0.2796)  loss_scale: 16384.0000 (20501.5352)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.1727  max mem: 15572
Epoch: [15]  [ 960/1404]  eta: 0:04:22  lr: 0.000074  min_lr: 0.000001  loss: 4.4004 (4.2888)  class_acc: 0.2083 (0.2794)  loss_scale: 16384.0000 (20458.6889)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1258  max mem: 15572
Epoch: [15]  [ 970/1404]  eta: 0:04:16  lr: 0.000074  min_lr: 0.000001  loss: 4.2044 (4.2885)  class_acc: 0.2500 (0.2800)  loss_scale: 16384.0000 (20416.7250)  weight_decay: 0.0500 (0.0500)  time: 0.5022  data: 0.0207  max mem: 15572
Epoch: [15]  [ 980/1404]  eta: 0:04:10  lr: 0.000074  min_lr: 0.000001  loss: 4.2277 (4.2879)  class_acc: 0.2917 (0.2804)  loss_scale: 16384.0000 (20375.6167)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1001  max mem: 15572
Epoch: [15]  [ 990/1404]  eta: 0:04:04  lr: 0.000074  min_lr: 0.000001  loss: 4.1975 (4.2873)  class_acc: 0.2917 (0.2798)  loss_scale: 16384.0000 (20335.3380)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1442  max mem: 15572
Epoch: [15]  [1000/1404]  eta: 0:03:58  lr: 0.000074  min_lr: 0.000001  loss: 4.2333 (4.2868)  class_acc: 0.2500 (0.2801)  loss_scale: 16384.0000 (20295.8641)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0916  max mem: 15572
Epoch: [15]  [1010/1404]  eta: 0:03:53  lr: 0.000074  min_lr: 0.000001  loss: 4.3143 (4.2883)  class_acc: 0.2500 (0.2795)  loss_scale: 16384.0000 (20257.1711)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1071  max mem: 15572
[2025-01-17 00:03:38,530] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:03:38,530] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:03:38,531] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:03:38,531] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [1020/1404]  eta: 0:03:47  lr: 0.000074  min_lr: 0.000001  loss: 4.3344 (4.2891)  class_acc: 0.2083 (0.2792)  loss_scale: 16384.0000 (20347.6121)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.1678  max mem: 15572
[2025-01-17 00:03:46,598] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22090
[2025-01-17 00:03:46,599] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:03:46,599] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [1030/1404]  eta: 0:03:40  lr: 0.000074  min_lr: 0.000001  loss: 4.2596 (4.2890)  class_acc: 0.2083 (0.2787)  loss_scale: 32768.0000 (20452.1901)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0883  max mem: 15572
[2025-01-17 00:03:46,616] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22090
[2025-01-17 00:03:46,616] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [15]  [1040/1404]  eta: 0:03:34  lr: 0.000074  min_lr: 0.000001  loss: 4.2596 (4.2879)  class_acc: 0.2083 (0.2786)  loss_scale: 16384.0000 (20413.1105)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0008  max mem: 15572
Epoch: [15]  [1050/1404]  eta: 0:03:28  lr: 0.000074  min_lr: 0.000001  loss: 4.2927 (4.2884)  class_acc: 0.2500 (0.2783)  loss_scale: 16384.0000 (20374.7745)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0630  max mem: 15572
Epoch: [15]  [1060/1404]  eta: 0:03:22  lr: 0.000074  min_lr: 0.000001  loss: 4.2548 (4.2869)  class_acc: 0.3333 (0.2794)  loss_scale: 16384.0000 (20337.1612)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1262  max mem: 15572
Epoch: [15]  [1070/1404]  eta: 0:03:16  lr: 0.000074  min_lr: 0.000001  loss: 4.2599 (4.2875)  class_acc: 0.3750 (0.2795)  loss_scale: 16384.0000 (20300.2502)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0744  max mem: 15572
Epoch: [15]  [1080/1404]  eta: 0:03:11  lr: 0.000074  min_lr: 0.000001  loss: 4.3253 (4.2870)  class_acc: 0.3333 (0.2797)  loss_scale: 16384.0000 (20264.0222)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0824  max mem: 15572
Epoch: [15]  [1090/1404]  eta: 0:03:05  lr: 0.000073  min_lr: 0.000001  loss: 4.2987 (4.2872)  class_acc: 0.2500 (0.2792)  loss_scale: 16384.0000 (20228.4583)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.1443  max mem: 15572
Epoch: [15]  [1100/1404]  eta: 0:02:59  lr: 0.000073  min_lr: 0.000001  loss: 4.2543 (4.2859)  class_acc: 0.2500 (0.2796)  loss_scale: 16384.0000 (20193.5404)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1309  max mem: 15572
Epoch: [15]  [1110/1404]  eta: 0:02:53  lr: 0.000073  min_lr: 0.000001  loss: 4.2210 (4.2855)  class_acc: 0.2500 (0.2791)  loss_scale: 16384.0000 (20159.2511)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0814  max mem: 15572
Epoch: [15]  [1120/1404]  eta: 0:02:47  lr: 0.000073  min_lr: 0.000001  loss: 4.2372 (4.2869)  class_acc: 0.1667 (0.2787)  loss_scale: 16384.0000 (20125.5736)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0743  max mem: 15572
Epoch: [15]  [1130/1404]  eta: 0:02:41  lr: 0.000073  min_lr: 0.000001  loss: 4.2390 (4.2858)  class_acc: 0.2083 (0.2784)  loss_scale: 16384.0000 (20092.4916)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0515  max mem: 15572
Epoch: [15]  [1140/1404]  eta: 0:02:35  lr: 0.000073  min_lr: 0.000001  loss: 4.2270 (4.2858)  class_acc: 0.2500 (0.2788)  loss_scale: 16384.0000 (20059.9895)  weight_decay: 0.0500 (0.0500)  time: 0.4944  data: 0.0005  max mem: 15572
Epoch: [15]  [1150/1404]  eta: 0:02:29  lr: 0.000073  min_lr: 0.000001  loss: 4.3327 (4.2860)  class_acc: 0.2083 (0.2783)  loss_scale: 16384.0000 (20028.0521)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0909  max mem: 15572
[2025-01-17 00:05:01,681] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:05:01,682] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:05:01,682] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:05:01,683] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [1160/1404]  eta: 0:02:23  lr: 0.000073  min_lr: 0.000001  loss: 4.2943 (4.2852)  class_acc: 0.2500 (0.2785)  loss_scale: 16384.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  time: 0.6539  data: 0.1435  max mem: 15572
Epoch: [15]  [1170/1404]  eta: 0:02:17  lr: 0.000073  min_lr: 0.000001  loss: 4.1407 (4.2839)  class_acc: 0.2917 (0.2786)  loss_scale: 32768.0000 (20133.7114)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0827  max mem: 15572
Epoch: [15]  [1180/1404]  eta: 0:02:12  lr: 0.000073  min_lr: 0.000001  loss: 4.2210 (4.2835)  class_acc: 0.2917 (0.2790)  loss_scale: 32768.0000 (20240.6909)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0669  max mem: 15572
Epoch: [15]  [1190/1404]  eta: 0:02:06  lr: 0.000073  min_lr: 0.000001  loss: 4.3202 (4.2836)  class_acc: 0.3333 (0.2794)  loss_scale: 32768.0000 (20345.8741)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0820  max mem: 15572
Epoch: [15]  [1200/1404]  eta: 0:02:00  lr: 0.000073  min_lr: 0.000001  loss: 4.2545 (4.2831)  class_acc: 0.2500 (0.2789)  loss_scale: 32768.0000 (20449.3056)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0884  max mem: 15572
Epoch: [15]  [1210/1404]  eta: 0:01:54  lr: 0.000073  min_lr: 0.000001  loss: 4.3553 (4.2837)  class_acc: 0.2500 (0.2786)  loss_scale: 32768.0000 (20551.0289)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.0548  max mem: 15572
Epoch: [15]  [1220/1404]  eta: 0:01:48  lr: 0.000073  min_lr: 0.000001  loss: 4.3041 (4.2843)  class_acc: 0.2083 (0.2781)  loss_scale: 32768.0000 (20651.0860)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.0116  max mem: 15572
Epoch: [15]  [1230/1404]  eta: 0:01:42  lr: 0.000073  min_lr: 0.000001  loss: 4.2682 (4.2840)  class_acc: 0.2083 (0.2783)  loss_scale: 32768.0000 (20749.5175)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0005  max mem: 15572
Epoch: [15]  [1240/1404]  eta: 0:01:36  lr: 0.000073  min_lr: 0.000001  loss: 4.1205 (4.2822)  class_acc: 0.3333 (0.2786)  loss_scale: 32768.0000 (20846.3626)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.0005  max mem: 15572
[2025-01-17 00:05:55,386] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22309
[2025-01-17 00:05:55,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:05:55,431] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22309
[2025-01-17 00:05:55,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:05:55,433] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [1250/1404]  eta: 0:01:30  lr: 0.000073  min_lr: 0.000001  loss: 4.0545 (4.2818)  class_acc: 0.2500 (0.2784)  loss_scale: 32768.0000 (20915.4660)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.0006  max mem: 15572
Epoch: [15]  [1260/1404]  eta: 0:01:24  lr: 0.000073  min_lr: 0.000001  loss: 4.0795 (4.2820)  class_acc: 0.2500 (0.2786)  loss_scale: 16384.0000 (20879.5305)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.0006  max mem: 15572
Epoch: [15]  [1270/1404]  eta: 0:01:19  lr: 0.000073  min_lr: 0.000001  loss: 4.3094 (4.2820)  class_acc: 0.2500 (0.2780)  loss_scale: 16384.0000 (20844.1605)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0006  max mem: 15572
Epoch: [15]  [1280/1404]  eta: 0:01:13  lr: 0.000073  min_lr: 0.000001  loss: 4.3436 (4.2829)  class_acc: 0.2500 (0.2779)  loss_scale: 16384.0000 (20809.3427)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0006  max mem: 15572
Epoch: [15]  [1290/1404]  eta: 0:01:07  lr: 0.000073  min_lr: 0.000001  loss: 4.2108 (4.2818)  class_acc: 0.2917 (0.2782)  loss_scale: 16384.0000 (20775.0643)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0007  max mem: 15572
Epoch: [15]  [1300/1404]  eta: 0:01:01  lr: 0.000073  min_lr: 0.000001  loss: 4.2062 (4.2823)  class_acc: 0.2917 (0.2783)  loss_scale: 16384.0000 (20741.3128)  weight_decay: 0.0500 (0.0500)  time: 0.5090  data: 0.0007  max mem: 15572
Epoch: [15]  [1310/1404]  eta: 0:00:55  lr: 0.000073  min_lr: 0.000001  loss: 4.3510 (4.2825)  class_acc: 0.2500 (0.2783)  loss_scale: 16384.0000 (20708.0763)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0007  max mem: 15572
Epoch: [15]  [1320/1404]  eta: 0:00:49  lr: 0.000073  min_lr: 0.000001  loss: 4.3510 (4.2829)  class_acc: 0.2500 (0.2780)  loss_scale: 16384.0000 (20675.3429)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.0008  max mem: 15572
Epoch: [15]  [1330/1404]  eta: 0:00:43  lr: 0.000073  min_lr: 0.000001  loss: 4.3899 (4.2844)  class_acc: 0.2500 (0.2778)  loss_scale: 16384.0000 (20643.1014)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0011  max mem: 15572
Epoch: [15]  [1340/1404]  eta: 0:00:37  lr: 0.000073  min_lr: 0.000001  loss: 4.4247 (4.2845)  class_acc: 0.2500 (0.2782)  loss_scale: 16384.0000 (20611.3408)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0011  max mem: 15572
Epoch: [15]  [1350/1404]  eta: 0:00:31  lr: 0.000073  min_lr: 0.000001  loss: 4.3379 (4.2849)  class_acc: 0.2083 (0.2778)  loss_scale: 16384.0000 (20580.0503)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0007  max mem: 15572
Epoch: [15]  [1360/1404]  eta: 0:00:25  lr: 0.000073  min_lr: 0.000001  loss: 4.2849 (4.2847)  class_acc: 0.2083 (0.2776)  loss_scale: 16384.0000 (20549.2197)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0008  max mem: 15572
Epoch: [15]  [1370/1404]  eta: 0:00:20  lr: 0.000073  min_lr: 0.000001  loss: 4.3065 (4.2848)  class_acc: 0.2500 (0.2775)  loss_scale: 16384.0000 (20518.8388)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0008  max mem: 15572
[2025-01-17 00:07:09,327] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:07:09,327] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:07:09,338] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:07:09,338] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [1380/1404]  eta: 0:00:14  lr: 0.000073  min_lr: 0.000001  loss: 4.3020 (4.2846)  class_acc: 0.2500 (0.2773)  loss_scale: 16384.0000 (20524.4895)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0007  max mem: 15572
Epoch: [15]  [1390/1404]  eta: 0:00:08  lr: 0.000073  min_lr: 0.000001  loss: 4.1337 (4.2837)  class_acc: 0.2500 (0.2773)  loss_scale: 32768.0000 (20612.5090)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0007  max mem: 15572
Epoch: [15]  [1400/1404]  eta: 0:00:02  lr: 0.000073  min_lr: 0.000001  loss: 4.1615 (4.2838)  class_acc: 0.2500 (0.2767)  loss_scale: 32768.0000 (20699.2719)  weight_decay: 0.0500 (0.0500)  time: 0.4564  data: 0.0004  max mem: 15572
Epoch: [15]  [1403/1404]  eta: 0:00:00  lr: 0.000073  min_lr: 0.000001  loss: 4.1997 (4.2838)  class_acc: 0.2500 (0.2765)  loss_scale: 32768.0000 (20725.0598)  weight_decay: 0.0500 (0.0500)  time: 0.4332  data: 0.0003  max mem: 15572
Epoch: [15] Total time: 0:13:44 (0.5869 s / it)
Averaged stats: lr: 0.000073  min_lr: 0.000001  loss: 4.1997 (4.2813)  class_acc: 0.2500 (0.2772)  loss_scale: 32768.0000 (20725.0598)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:14:09  loss: 1.9657 (1.9657)  acc1: 66.6667 (66.6667)  acc5: 77.7778 (77.7778)  time: 6.2449  data: 5.8835  max mem: 15572
Val:  [ 10/136]  eta: 0:01:44  loss: 2.4979 (2.6217)  acc1: 38.8889 (39.8990)  acc5: 77.7778 (73.2323)  time: 0.8289  data: 0.6169  max mem: 15572
Val:  [ 20/136]  eta: 0:01:07  loss: 2.7937 (2.6875)  acc1: 33.3333 (37.3016)  acc5: 72.2222 (72.2222)  time: 0.2985  data: 0.1065  max mem: 15572
Val:  [ 30/136]  eta: 0:00:52  loss: 2.6158 (2.5398)  acc1: 33.3333 (42.1147)  acc5: 77.7778 (73.8351)  time: 0.3165  data: 0.1263  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.1988 (2.5061)  acc1: 50.0000 (43.3604)  acc5: 77.7778 (74.3902)  time: 0.3262  data: 0.1221  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.4531 (2.5486)  acc1: 44.4444 (43.3551)  acc5: 77.7778 (74.1830)  time: 0.3408  data: 0.1279  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.6967 (2.6359)  acc1: 38.8889 (41.2568)  acc5: 66.6667 (71.8579)  time: 0.3356  data: 0.1373  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.6401 (2.6096)  acc1: 44.4444 (42.3318)  acc5: 72.2222 (72.5352)  time: 0.3343  data: 0.1376  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.3352 (2.5980)  acc1: 44.4444 (42.8669)  acc5: 77.7778 (73.5254)  time: 0.3562  data: 0.1590  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.5555 (2.6097)  acc1: 38.8889 (41.6361)  acc5: 77.7778 (73.2601)  time: 0.3518  data: 0.1578  max mem: 15572
Val:  [100/136]  eta: 0:00:13  loss: 2.8071 (2.6510)  acc1: 27.7778 (40.2640)  acc5: 72.2222 (72.1122)  time: 0.3290  data: 0.1332  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.6527 (2.6439)  acc1: 33.3333 (40.8909)  acc5: 72.2222 (72.1722)  time: 0.3437  data: 0.1500  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.2812 (2.6067)  acc1: 55.5556 (42.5620)  acc5: 83.3333 (73.2323)  time: 0.3705  data: 0.1799  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.0749 (2.5763)  acc1: 61.1111 (43.4690)  acc5: 88.8889 (74.0458)  time: 0.3008  data: 0.1298  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.2721 (2.5801)  acc1: 55.5556 (43.4070)  acc5: 83.3333 (74.0786)  time: 0.2349  data: 0.0805  max mem: 15572
Val: Total time: 0:00:49 (0.3656 s / it)
* Acc@1 42.322 Acc@5 72.973 loss 2.623
Accuracy of the network on the 4883 val videos: 42.3%
[2025-01-17 00:08:10,830] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 00:08:10,832] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 00:08:10,832] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 00:08:10,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 00:08:13,220] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 00:08:13,220] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 42.32%
[2025-01-17 00:08:20,828] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22464
[2025-01-17 00:08:20,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:08:20,829] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:08:20,835] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22464
[2025-01-17 00:08:20,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [16]  [   0/1404]  eta: 2:58:06  lr: 0.000073  min_lr: 0.000001  loss: 3.7302 (3.7302)  class_acc: 0.5833 (0.5833)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.6118  data: 7.0783  max mem: 15572
Epoch: [16]  [  10/1404]  eta: 0:26:43  lr: 0.000073  min_lr: 0.000001  loss: 4.0616 (3.9777)  class_acc: 0.2500 (0.3295)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1504  data: 0.6442  max mem: 15572
Epoch: [16]  [  20/1404]  eta: 0:20:23  lr: 0.000073  min_lr: 0.000001  loss: 4.0899 (4.0935)  class_acc: 0.2083 (0.2897)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0165  max mem: 15572
Epoch: [16]  [  30/1404]  eta: 0:18:03  lr: 0.000073  min_lr: 0.000001  loss: 4.2799 (4.1157)  class_acc: 0.2500 (0.3105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0220  max mem: 15572
Epoch: [16]  [  40/1404]  eta: 0:17:03  lr: 0.000073  min_lr: 0.000001  loss: 4.1142 (4.1489)  class_acc: 0.2917 (0.3039)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0062  max mem: 15572
Epoch: [16]  [  50/1404]  eta: 0:15:46  lr: 0.000073  min_lr: 0.000001  loss: 4.3101 (4.1953)  class_acc: 0.2500 (0.2835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0007  max mem: 15572
Epoch: [16]  [  60/1404]  eta: 0:15:02  lr: 0.000073  min_lr: 0.000001  loss: 4.3005 (4.2162)  class_acc: 0.2500 (0.2896)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0230  max mem: 15572
Epoch: [16]  [  70/1404]  eta: 0:14:39  lr: 0.000073  min_lr: 0.000001  loss: 4.3161 (4.2191)  class_acc: 0.2917 (0.2864)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0614  max mem: 15572
Epoch: [16]  [  80/1404]  eta: 0:14:19  lr: 0.000073  min_lr: 0.000001  loss: 4.2873 (4.1989)  class_acc: 0.2917 (0.2906)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0567  max mem: 15572
Epoch: [16]  [  90/1404]  eta: 0:14:09  lr: 0.000072  min_lr: 0.000001  loss: 4.2913 (4.2156)  class_acc: 0.2917 (0.2875)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0460  max mem: 15572
Epoch: [16]  [ 100/1404]  eta: 0:13:57  lr: 0.000072  min_lr: 0.000001  loss: 4.3681 (4.2229)  class_acc: 0.2500 (0.2875)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0412  max mem: 15572
Epoch: [16]  [ 110/1404]  eta: 0:13:48  lr: 0.000072  min_lr: 0.000001  loss: 4.3086 (4.2252)  class_acc: 0.2500 (0.2879)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0135  max mem: 15572
Epoch: [16]  [ 120/1404]  eta: 0:13:36  lr: 0.000072  min_lr: 0.000001  loss: 4.3224 (4.2279)  class_acc: 0.2917 (0.2899)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.0006  max mem: 15572
[2025-01-17 00:09:35,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:09:35,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:09:35,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:09:35,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [ 130/1404]  eta: 0:13:27  lr: 0.000072  min_lr: 0.000001  loss: 4.3224 (4.2240)  class_acc: 0.3333 (0.2958)  loss_scale: 16384.0000 (16634.1374)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.0007  max mem: 15572
Epoch: [16]  [ 140/1404]  eta: 0:13:17  lr: 0.000072  min_lr: 0.000001  loss: 4.3048 (4.2323)  class_acc: 0.2917 (0.2940)  loss_scale: 32768.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.0004  max mem: 15572
Epoch: [16]  [ 150/1404]  eta: 0:13:14  lr: 0.000072  min_lr: 0.000001  loss: 4.3785 (4.2454)  class_acc: 0.2500 (0.2933)  loss_scale: 32768.0000 (18771.0728)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.0005  max mem: 15572
Epoch: [16]  [ 160/1404]  eta: 0:12:58  lr: 0.000072  min_lr: 0.000001  loss: 4.3187 (4.2459)  class_acc: 0.2917 (0.2953)  loss_scale: 32768.0000 (19640.4472)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.0007  max mem: 15572
Epoch: [16]  [ 170/1404]  eta: 0:12:52  lr: 0.000072  min_lr: 0.000001  loss: 4.2715 (4.2519)  class_acc: 0.2917 (0.2929)  loss_scale: 32768.0000 (20408.1404)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0007  max mem: 15572
Epoch: [16]  [ 180/1404]  eta: 0:12:43  lr: 0.000072  min_lr: 0.000001  loss: 4.4338 (4.2601)  class_acc: 0.2500 (0.2919)  loss_scale: 32768.0000 (21091.0055)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.0008  max mem: 15572
Epoch: [16]  [ 190/1404]  eta: 0:12:36  lr: 0.000072  min_lr: 0.000001  loss: 4.4338 (4.2711)  class_acc: 0.2083 (0.2893)  loss_scale: 32768.0000 (21702.3665)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0008  max mem: 15572
Epoch: [16]  [ 200/1404]  eta: 0:12:29  lr: 0.000072  min_lr: 0.000001  loss: 4.4297 (4.2758)  class_acc: 0.2500 (0.2879)  loss_scale: 32768.0000 (22252.8955)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0008  max mem: 15572
Epoch: [16]  [ 210/1404]  eta: 0:12:22  lr: 0.000072  min_lr: 0.000001  loss: 4.2024 (4.2656)  class_acc: 0.2917 (0.2915)  loss_scale: 32768.0000 (22751.2417)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0007  max mem: 15572
Epoch: [16]  [ 220/1404]  eta: 0:12:11  lr: 0.000072  min_lr: 0.000001  loss: 4.1448 (4.2622)  class_acc: 0.3333 (0.2939)  loss_scale: 32768.0000 (23204.4887)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0007  max mem: 15572
Epoch: [16]  [ 230/1404]  eta: 0:11:59  lr: 0.000072  min_lr: 0.000001  loss: 4.2313 (4.2620)  class_acc: 0.3750 (0.2927)  loss_scale: 32768.0000 (23618.4935)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0007  max mem: 15572
Epoch: [16]  [ 240/1404]  eta: 0:11:47  lr: 0.000072  min_lr: 0.000001  loss: 4.2608 (4.2610)  class_acc: 0.2083 (0.2913)  loss_scale: 32768.0000 (23998.1411)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0007  max mem: 15572
[2025-01-17 00:10:41,230] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22707
[2025-01-17 00:10:41,230] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22707
[2025-01-17 00:10:41,230] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:10:41,230] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:10:41,230] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [ 250/1404]  eta: 0:11:36  lr: 0.000072  min_lr: 0.000001  loss: 4.2505 (4.2605)  class_acc: 0.2083 (0.2910)  loss_scale: 32768.0000 (23825.3386)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0006  max mem: 15572
Epoch: [16]  [ 260/1404]  eta: 0:11:29  lr: 0.000072  min_lr: 0.000001  loss: 4.3389 (4.2639)  class_acc: 0.2500 (0.2907)  loss_scale: 16384.0000 (23540.2299)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0236  max mem: 15572
Epoch: [16]  [ 270/1404]  eta: 0:11:29  lr: 0.000072  min_lr: 0.000001  loss: 4.4014 (4.2619)  class_acc: 0.2500 (0.2884)  loss_scale: 16384.0000 (23276.1624)  weight_decay: 0.0500 (0.0500)  time: 0.6660  data: 0.0293  max mem: 15572
Epoch: [16]  [ 280/1404]  eta: 0:11:22  lr: 0.000072  min_lr: 0.000001  loss: 4.4014 (4.2669)  class_acc: 0.2500 (0.2896)  loss_scale: 16384.0000 (23030.8897)  weight_decay: 0.0500 (0.0500)  time: 0.6681  data: 0.0063  max mem: 15572
Epoch: [16]  [ 290/1404]  eta: 0:11:13  lr: 0.000072  min_lr: 0.000001  loss: 4.4594 (4.2696)  class_acc: 0.3333 (0.2905)  loss_scale: 16384.0000 (22802.4742)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0008  max mem: 15572
Epoch: [16]  [ 300/1404]  eta: 0:11:08  lr: 0.000072  min_lr: 0.000001  loss: 4.3357 (4.2697)  class_acc: 0.2500 (0.2900)  loss_scale: 16384.0000 (22589.2359)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.0007  max mem: 15572
Epoch: [16]  [ 310/1404]  eta: 0:11:00  lr: 0.000072  min_lr: 0.000001  loss: 4.3206 (4.2656)  class_acc: 0.3750 (0.2922)  loss_scale: 16384.0000 (22389.7106)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0006  max mem: 15572
Epoch: [16]  [ 320/1404]  eta: 0:10:53  lr: 0.000072  min_lr: 0.000001  loss: 4.3818 (4.2725)  class_acc: 0.2500 (0.2909)  loss_scale: 16384.0000 (22202.6168)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.0005  max mem: 15572
Epoch: [16]  [ 330/1404]  eta: 0:10:46  lr: 0.000072  min_lr: 0.000001  loss: 4.3673 (4.2647)  class_acc: 0.2500 (0.2907)  loss_scale: 16384.0000 (22026.8278)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.0006  max mem: 15572
Epoch: [16]  [ 340/1404]  eta: 0:10:38  lr: 0.000072  min_lr: 0.000001  loss: 4.2665 (4.2693)  class_acc: 0.2500 (0.2884)  loss_scale: 16384.0000 (21861.3490)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0098  max mem: 15572
Epoch: [16]  [ 350/1404]  eta: 0:10:32  lr: 0.000072  min_lr: 0.000001  loss: 4.3414 (4.2690)  class_acc: 0.2083 (0.2879)  loss_scale: 16384.0000 (21705.2991)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0343  max mem: 15572
Epoch: [16]  [ 360/1404]  eta: 0:10:27  lr: 0.000072  min_lr: 0.000001  loss: 4.2894 (4.2678)  class_acc: 0.2500 (0.2872)  loss_scale: 16384.0000 (21557.8947)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0889  max mem: 15572
Epoch: [16]  [ 370/1404]  eta: 0:10:20  lr: 0.000072  min_lr: 0.000001  loss: 4.2574 (4.2682)  class_acc: 0.2500 (0.2873)  loss_scale: 16384.0000 (21418.4367)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1107  max mem: 15572
[2025-01-17 00:11:57,170] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:11:57,171] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:11:57,179] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:11:57,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [ 380/1404]  eta: 0:10:15  lr: 0.000072  min_lr: 0.000001  loss: 4.2195 (4.2645)  class_acc: 0.2500 (0.2874)  loss_scale: 16384.0000 (21673.3228)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0470  max mem: 15572
Epoch: [16]  [ 390/1404]  eta: 0:10:09  lr: 0.000072  min_lr: 0.000001  loss: 4.1473 (4.2601)  class_acc: 0.3333 (0.2882)  loss_scale: 32768.0000 (21957.0742)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.0007  max mem: 15572
[2025-01-17 00:12:12,308] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22860
[2025-01-17 00:12:12,308] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:12:12,309] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22860
[2025-01-17 00:12:12,310] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:12:12,310] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [ 400/1404]  eta: 0:10:04  lr: 0.000072  min_lr: 0.000001  loss: 4.2959 (4.2627)  class_acc: 0.3333 (0.2881)  loss_scale: 32768.0000 (22022.3840)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.0005  max mem: 15572
Epoch: [16]  [ 410/1404]  eta: 0:09:56  lr: 0.000072  min_lr: 0.000001  loss: 4.2902 (4.2591)  class_acc: 0.2917 (0.2892)  loss_scale: 16384.0000 (21885.1971)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0004  max mem: 15572
Epoch: [16]  [ 420/1404]  eta: 0:09:49  lr: 0.000072  min_lr: 0.000001  loss: 4.1384 (4.2550)  class_acc: 0.2500 (0.2887)  loss_scale: 16384.0000 (21754.5273)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0006  max mem: 15572
Epoch: [16]  [ 430/1404]  eta: 0:09:43  lr: 0.000072  min_lr: 0.000001  loss: 4.1842 (4.2569)  class_acc: 0.2500 (0.2879)  loss_scale: 16384.0000 (21629.9211)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0010  max mem: 15572
Epoch: [16]  [ 440/1404]  eta: 0:09:37  lr: 0.000072  min_lr: 0.000001  loss: 4.2692 (4.2572)  class_acc: 0.2917 (0.2882)  loss_scale: 16384.0000 (21510.9660)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.0008  max mem: 15572
Epoch: [16]  [ 450/1404]  eta: 0:09:32  lr: 0.000072  min_lr: 0.000001  loss: 4.2590 (4.2567)  class_acc: 0.2917 (0.2881)  loss_scale: 16384.0000 (21397.2860)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.0301  max mem: 15572
Epoch: [16]  [ 460/1404]  eta: 0:09:25  lr: 0.000072  min_lr: 0.000001  loss: 4.0278 (4.2514)  class_acc: 0.3333 (0.2890)  loss_scale: 16384.0000 (21288.5380)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0302  max mem: 15572
Epoch: [16]  [ 470/1404]  eta: 0:09:18  lr: 0.000072  min_lr: 0.000001  loss: 4.1102 (4.2515)  class_acc: 0.2917 (0.2887)  loss_scale: 16384.0000 (21184.4076)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0252  max mem: 15572
Epoch: [16]  [ 480/1404]  eta: 0:09:12  lr: 0.000072  min_lr: 0.000001  loss: 4.3053 (4.2509)  class_acc: 0.2500 (0.2885)  loss_scale: 16384.0000 (21084.6071)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0786  max mem: 15572
Epoch: [16]  [ 490/1404]  eta: 0:09:07  lr: 0.000071  min_lr: 0.000001  loss: 4.3226 (4.2548)  class_acc: 0.2500 (0.2873)  loss_scale: 16384.0000 (20988.8717)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1056  max mem: 15572
Epoch: [16]  [ 500/1404]  eta: 0:09:00  lr: 0.000071  min_lr: 0.000001  loss: 4.3170 (4.2536)  class_acc: 0.2083 (0.2867)  loss_scale: 16384.0000 (20896.9581)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0833  max mem: 15572
Epoch: [16]  [ 510/1404]  eta: 0:08:53  lr: 0.000071  min_lr: 0.000001  loss: 4.2340 (4.2543)  class_acc: 0.2917 (0.2873)  loss_scale: 16384.0000 (20808.6419)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0725  max mem: 15572
Epoch: [16]  [ 520/1404]  eta: 0:08:47  lr: 0.000071  min_lr: 0.000001  loss: 4.2901 (4.2565)  class_acc: 0.2917 (0.2863)  loss_scale: 16384.0000 (20723.7159)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0694  max mem: 15572
[2025-01-17 00:13:26,938] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:13:26,939] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:13:26,939] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:13:26,939] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [ 530/1404]  eta: 0:08:41  lr: 0.000071  min_lr: 0.000001  loss: 4.2901 (4.2566)  class_acc: 0.2500 (0.2864)  loss_scale: 16384.0000 (20827.1186)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0411  max mem: 15572
[2025-01-17 00:13:32,563] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22998
[2025-01-17 00:13:32,563] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:13:32,563] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:13:32,613] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22998
[2025-01-17 00:13:32,614] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:13:33,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=138, lr=[6.914755726578027e-07, 6.914755726578027e-07, 9.87822246654004e-07, 9.87822246654004e-07, 1.4111746380771487e-06, 1.4111746380771487e-06, 2.015963768681641e-06, 2.015963768681641e-06, 2.879948240973773e-06, 2.879948240973773e-06, 4.114211772819676e-06, 4.114211772819676e-06, 5.877445389742394e-06, 5.877445389742394e-06, 8.39635055677485e-06, 8.39635055677485e-06, 1.1994786509678356e-05, 1.1994786509678356e-05, 1.7135409299540513e-05, 1.7135409299540513e-05, 2.4479156142200732e-05, 2.4479156142200732e-05, 3.497022306028676e-05, 3.497022306028676e-05, 4.995746151469538e-05, 4.995746151469538e-05, 7.136780216385054e-05, 7.136780216385054e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 00:13:33,031] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=47.84660918378747, CurrSamplesPerSec=54.13722417325926, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [ 540/1404]  eta: 0:08:35  lr: 0.000071  min_lr: 0.000001  loss: 4.2836 (4.2565)  class_acc: 0.2917 (0.2865)  loss_scale: 16384.0000 (20835.8447)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0130  max mem: 15572
Epoch: [16]  [ 550/1404]  eta: 0:08:28  lr: 0.000071  min_lr: 0.000001  loss: 4.2497 (4.2571)  class_acc: 0.2917 (0.2861)  loss_scale: 16384.0000 (20755.0490)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0005  max mem: 15572
Epoch: [16]  [ 560/1404]  eta: 0:08:23  lr: 0.000071  min_lr: 0.000001  loss: 4.2382 (4.2572)  class_acc: 0.3750 (0.2884)  loss_scale: 16384.0000 (20677.1337)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0005  max mem: 15572
Epoch: [16]  [ 570/1404]  eta: 0:08:17  lr: 0.000071  min_lr: 0.000001  loss: 4.2346 (4.2564)  class_acc: 0.3750 (0.2877)  loss_scale: 16384.0000 (20601.9475)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0005  max mem: 15572
Epoch: [16]  [ 580/1404]  eta: 0:08:10  lr: 0.000071  min_lr: 0.000001  loss: 4.2309 (4.2564)  class_acc: 0.2083 (0.2869)  loss_scale: 16384.0000 (20529.3494)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0008  max mem: 15572
Epoch: [16]  [ 590/1404]  eta: 0:08:04  lr: 0.000071  min_lr: 0.000001  loss: 4.0776 (4.2535)  class_acc: 0.2500 (0.2872)  loss_scale: 16384.0000 (20459.2081)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0009  max mem: 15572
Epoch: [16]  [ 600/1404]  eta: 0:07:56  lr: 0.000071  min_lr: 0.000001  loss: 4.0858 (4.2539)  class_acc: 0.3333 (0.2883)  loss_scale: 16384.0000 (20391.4010)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0007  max mem: 15572
Epoch: [16]  [ 610/1404]  eta: 0:07:50  lr: 0.000071  min_lr: 0.000001  loss: 4.1743 (4.2529)  class_acc: 0.2500 (0.2878)  loss_scale: 16384.0000 (20325.8134)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0006  max mem: 15572
Epoch: [16]  [ 620/1404]  eta: 0:07:45  lr: 0.000071  min_lr: 0.000001  loss: 4.1743 (4.2512)  class_acc: 0.2917 (0.2884)  loss_scale: 16384.0000 (20262.3382)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.0006  max mem: 15572
Epoch: [16]  [ 630/1404]  eta: 0:07:39  lr: 0.000071  min_lr: 0.000001  loss: 4.1981 (4.2506)  class_acc: 0.2917 (0.2883)  loss_scale: 16384.0000 (20200.8748)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.0008  max mem: 15572
Epoch: [16]  [ 640/1404]  eta: 0:07:34  lr: 0.000071  min_lr: 0.000001  loss: 4.2767 (4.2514)  class_acc: 0.2917 (0.2880)  loss_scale: 16384.0000 (20141.3292)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0007  max mem: 15572
Epoch: [16]  [ 650/1404]  eta: 0:07:29  lr: 0.000071  min_lr: 0.000001  loss: 4.3779 (4.2526)  class_acc: 0.2500 (0.2884)  loss_scale: 16384.0000 (20083.6129)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.0006  max mem: 15572
Epoch: [16]  [ 660/1404]  eta: 0:07:22  lr: 0.000071  min_lr: 0.000001  loss: 4.2954 (4.2523)  class_acc: 0.2917 (0.2883)  loss_scale: 16384.0000 (20027.6430)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0006  max mem: 15572
[2025-01-17 00:14:47,971] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:14:47,971] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:14:47,972] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:14:47,972] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [ 670/1404]  eta: 0:07:15  lr: 0.000071  min_lr: 0.000001  loss: 4.1804 (4.2514)  class_acc: 0.2500 (0.2880)  loss_scale: 16384.0000 (20168.6796)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0007  max mem: 15572
Epoch: [16]  [ 680/1404]  eta: 0:07:09  lr: 0.000071  min_lr: 0.000001  loss: 4.1156 (4.2515)  class_acc: 0.2083 (0.2882)  loss_scale: 32768.0000 (20353.6916)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0007  max mem: 15572
Epoch: [16]  [ 690/1404]  eta: 0:07:04  lr: 0.000071  min_lr: 0.000001  loss: 4.3421 (4.2517)  class_acc: 0.2500 (0.2882)  loss_scale: 32768.0000 (20533.3488)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0005  max mem: 15572
[2025-01-17 00:15:08,376] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23160
[2025-01-17 00:15:08,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:15:08,377] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:15:08,377] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23160
[2025-01-17 00:15:08,378] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [16]  [ 700/1404]  eta: 0:06:58  lr: 0.000071  min_lr: 0.000001  loss: 4.3421 (4.2531)  class_acc: 0.3333 (0.2892)  loss_scale: 32768.0000 (20591.0185)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.0005  max mem: 15572
Epoch: [16]  [ 710/1404]  eta: 0:06:52  lr: 0.000071  min_lr: 0.000001  loss: 4.3844 (4.2523)  class_acc: 0.2917 (0.2886)  loss_scale: 16384.0000 (20531.8481)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.0007  max mem: 15572
Epoch: [16]  [ 720/1404]  eta: 0:06:46  lr: 0.000071  min_lr: 0.000001  loss: 4.3121 (4.2520)  class_acc: 0.2500 (0.2889)  loss_scale: 16384.0000 (20474.3190)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0007  max mem: 15572
Epoch: [16]  [ 730/1404]  eta: 0:06:40  lr: 0.000071  min_lr: 0.000001  loss: 4.3121 (4.2536)  class_acc: 0.2500 (0.2884)  loss_scale: 16384.0000 (20418.3639)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0007  max mem: 15572
Epoch: [16]  [ 740/1404]  eta: 0:06:33  lr: 0.000071  min_lr: 0.000001  loss: 4.3334 (4.2533)  class_acc: 0.2500 (0.2882)  loss_scale: 16384.0000 (20363.9190)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0006  max mem: 15572
Epoch: [16]  [ 750/1404]  eta: 0:06:28  lr: 0.000071  min_lr: 0.000001  loss: 4.2048 (4.2523)  class_acc: 0.2500 (0.2878)  loss_scale: 16384.0000 (20310.9241)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0005  max mem: 15572
Epoch: [16]  [ 760/1404]  eta: 0:06:21  lr: 0.000071  min_lr: 0.000001  loss: 4.2048 (4.2528)  class_acc: 0.2500 (0.2871)  loss_scale: 16384.0000 (20259.3219)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0005  max mem: 15572
Epoch: [16]  [ 770/1404]  eta: 0:06:15  lr: 0.000071  min_lr: 0.000001  loss: 4.2424 (4.2525)  class_acc: 0.2083 (0.2871)  loss_scale: 16384.0000 (20209.0584)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0006  max mem: 15572
Epoch: [16]  [ 780/1404]  eta: 0:06:09  lr: 0.000071  min_lr: 0.000001  loss: 4.2546 (4.2536)  class_acc: 0.2917 (0.2878)  loss_scale: 16384.0000 (20160.0819)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0047  max mem: 15572
Epoch: [16]  [ 790/1404]  eta: 0:06:03  lr: 0.000071  min_lr: 0.000001  loss: 4.3044 (4.2544)  class_acc: 0.2917 (0.2873)  loss_scale: 16384.0000 (20112.3439)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0046  max mem: 15572
Epoch: [16]  [ 800/1404]  eta: 0:05:57  lr: 0.000071  min_lr: 0.000001  loss: 4.3165 (4.2556)  class_acc: 0.2083 (0.2869)  loss_scale: 16384.0000 (20065.7978)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.0006  max mem: 15572
Epoch: [16]  [ 810/1404]  eta: 0:05:52  lr: 0.000071  min_lr: 0.000001  loss: 4.2617 (4.2551)  class_acc: 0.2500 (0.2871)  loss_scale: 16384.0000 (20020.3995)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.0007  max mem: 15572
Epoch: [16]  [ 820/1404]  eta: 0:05:46  lr: 0.000071  min_lr: 0.000001  loss: 4.0534 (4.2523)  class_acc: 0.2917 (0.2873)  loss_scale: 16384.0000 (19976.1072)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0005  max mem: 15572
[2025-01-17 00:16:22,640] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:16:22,640] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:16:22,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:16:22,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:16:24,099] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23291
[2025-01-17 00:16:24,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:16:24,102] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23291
[2025-01-17 00:16:24,102] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:16:24,102] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [ 830/1404]  eta: 0:05:39  lr: 0.000071  min_lr: 0.000001  loss: 4.1262 (4.2523)  class_acc: 0.3333 (0.2880)  loss_scale: 16384.0000 (19972.3129)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0005  max mem: 15572
Epoch: [16]  [ 840/1404]  eta: 0:05:34  lr: 0.000071  min_lr: 0.000001  loss: 4.1643 (4.2514)  class_acc: 0.2917 (0.2876)  loss_scale: 16384.0000 (19929.6457)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0007  max mem: 15572
Epoch: [16]  [ 850/1404]  eta: 0:05:28  lr: 0.000071  min_lr: 0.000001  loss: 4.2451 (4.2523)  class_acc: 0.2500 (0.2868)  loss_scale: 16384.0000 (19887.9812)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0008  max mem: 15572
Epoch: [16]  [ 860/1404]  eta: 0:05:21  lr: 0.000071  min_lr: 0.000001  loss: 4.2424 (4.2528)  class_acc: 0.2500 (0.2867)  loss_scale: 16384.0000 (19847.2846)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0007  max mem: 15572
Epoch: [16]  [ 870/1404]  eta: 0:05:15  lr: 0.000071  min_lr: 0.000001  loss: 4.2794 (4.2535)  class_acc: 0.2500 (0.2866)  loss_scale: 16384.0000 (19807.5224)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0006  max mem: 15572
Epoch: [16]  [ 880/1404]  eta: 0:05:09  lr: 0.000070  min_lr: 0.000001  loss: 4.2794 (4.2528)  class_acc: 0.2500 (0.2868)  loss_scale: 16384.0000 (19768.6629)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0006  max mem: 15572
Epoch: [16]  [ 890/1404]  eta: 0:05:03  lr: 0.000070  min_lr: 0.000001  loss: 4.2810 (4.2535)  class_acc: 0.2500 (0.2866)  loss_scale: 16384.0000 (19730.6756)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0008  max mem: 15572
Epoch: [16]  [ 900/1404]  eta: 0:04:57  lr: 0.000070  min_lr: 0.000001  loss: 4.3764 (4.2547)  class_acc: 0.2917 (0.2866)  loss_scale: 16384.0000 (19693.5316)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0008  max mem: 15572
Epoch: [16]  [ 910/1404]  eta: 0:04:51  lr: 0.000070  min_lr: 0.000001  loss: 4.1742 (4.2534)  class_acc: 0.2917 (0.2866)  loss_scale: 16384.0000 (19657.2031)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0422  max mem: 15572
Epoch: [16]  [ 920/1404]  eta: 0:04:45  lr: 0.000070  min_lr: 0.000001  loss: 4.1742 (4.2529)  class_acc: 0.2917 (0.2873)  loss_scale: 16384.0000 (19621.6634)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0422  max mem: 15572
Epoch: [16]  [ 930/1404]  eta: 0:04:39  lr: 0.000070  min_lr: 0.000001  loss: 4.2900 (4.2527)  class_acc: 0.2917 (0.2869)  loss_scale: 16384.0000 (19586.8872)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0007  max mem: 15572
Epoch: [16]  [ 940/1404]  eta: 0:04:33  lr: 0.000070  min_lr: 0.000001  loss: 4.2141 (4.2517)  class_acc: 0.2917 (0.2872)  loss_scale: 16384.0000 (19552.8502)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0007  max mem: 15572
Epoch: [16]  [ 950/1404]  eta: 0:04:27  lr: 0.000070  min_lr: 0.000001  loss: 4.1981 (4.2518)  class_acc: 0.2917 (0.2872)  loss_scale: 16384.0000 (19519.5289)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0007  max mem: 15572
[2025-01-17 00:17:37,849] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:17:37,850] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:17:37,927] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:17:37,927] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [ 960/1404]  eta: 0:04:21  lr: 0.000070  min_lr: 0.000001  loss: 4.2510 (4.2517)  class_acc: 0.2917 (0.2873)  loss_scale: 16384.0000 (19572.1457)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0084  max mem: 15572
Epoch: [16]  [ 970/1404]  eta: 0:04:15  lr: 0.000070  min_lr: 0.000001  loss: 4.2406 (4.2515)  class_acc: 0.2500 (0.2867)  loss_scale: 32768.0000 (19708.0453)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0083  max mem: 15572
Epoch: [16]  [ 980/1404]  eta: 0:04:09  lr: 0.000070  min_lr: 0.000001  loss: 4.2567 (4.2517)  class_acc: 0.2083 (0.2866)  loss_scale: 32768.0000 (19841.1743)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0233  max mem: 15572
Epoch: [16]  [ 990/1404]  eta: 0:04:03  lr: 0.000070  min_lr: 0.000001  loss: 4.3274 (4.2531)  class_acc: 0.2500 (0.2861)  loss_scale: 32768.0000 (19971.6165)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0662  max mem: 15572
Epoch: [16]  [1000/1404]  eta: 0:03:57  lr: 0.000070  min_lr: 0.000001  loss: 4.3274 (4.2534)  class_acc: 0.2500 (0.2863)  loss_scale: 32768.0000 (20099.4525)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0768  max mem: 15572
Epoch: [16]  [1010/1404]  eta: 0:03:51  lr: 0.000070  min_lr: 0.000001  loss: 4.2687 (4.2533)  class_acc: 0.2083 (0.2858)  loss_scale: 32768.0000 (20224.7596)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0635  max mem: 15572
[2025-01-17 00:18:10,789] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23477
[2025-01-17 00:18:10,789] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:18:10,790] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23477
[2025-01-17 00:18:10,790] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:18:10,790] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [1020/1404]  eta: 0:03:46  lr: 0.000070  min_lr: 0.000001  loss: 4.2309 (4.2526)  class_acc: 0.2083 (0.2854)  loss_scale: 32768.0000 (20219.2360)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.0814  max mem: 15572
Epoch: [16]  [1030/1404]  eta: 0:03:40  lr: 0.000070  min_lr: 0.000001  loss: 4.1362 (4.2516)  class_acc: 0.2500 (0.2860)  loss_scale: 16384.0000 (20182.0369)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0517  max mem: 15572
Epoch: [16]  [1040/1404]  eta: 0:03:34  lr: 0.000070  min_lr: 0.000001  loss: 4.3436 (4.2531)  class_acc: 0.2500 (0.2858)  loss_scale: 16384.0000 (20145.5524)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0661  max mem: 15572
Epoch: [16]  [1050/1404]  eta: 0:03:28  lr: 0.000070  min_lr: 0.000001  loss: 4.3616 (4.2535)  class_acc: 0.2500 (0.2858)  loss_scale: 16384.0000 (20109.7621)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.1475  max mem: 15572
Epoch: [16]  [1060/1404]  eta: 0:03:22  lr: 0.000070  min_lr: 0.000001  loss: 4.2991 (4.2545)  class_acc: 0.2500 (0.2858)  loss_scale: 16384.0000 (20074.6466)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0845  max mem: 15572
Epoch: [16]  [1070/1404]  eta: 0:03:16  lr: 0.000070  min_lr: 0.000001  loss: 4.1943 (4.2536)  class_acc: 0.2500 (0.2856)  loss_scale: 16384.0000 (20040.1867)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.0373  max mem: 15572
Epoch: [16]  [1080/1404]  eta: 0:03:11  lr: 0.000070  min_lr: 0.000001  loss: 4.1678 (4.2526)  class_acc: 0.2500 (0.2860)  loss_scale: 16384.0000 (20006.3645)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.0841  max mem: 15572
Epoch: [16]  [1090/1404]  eta: 0:03:05  lr: 0.000070  min_lr: 0.000001  loss: 4.2362 (4.2530)  class_acc: 0.3333 (0.2859)  loss_scale: 16384.0000 (19973.1622)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.0630  max mem: 15572
Epoch: [16]  [1100/1404]  eta: 0:02:59  lr: 0.000070  min_lr: 0.000001  loss: 4.3265 (4.2535)  class_acc: 0.2500 (0.2856)  loss_scale: 16384.0000 (19940.5631)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0760  max mem: 15572
Epoch: [16]  [1110/1404]  eta: 0:02:53  lr: 0.000070  min_lr: 0.000001  loss: 4.1724 (4.2524)  class_acc: 0.2500 (0.2854)  loss_scale: 16384.0000 (19908.5509)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.0915  max mem: 15572
Epoch: [16]  [1120/1404]  eta: 0:02:47  lr: 0.000070  min_lr: 0.000001  loss: 4.1724 (4.2529)  class_acc: 0.2917 (0.2855)  loss_scale: 16384.0000 (19877.1097)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0802  max mem: 15572
Epoch: [16]  [1130/1404]  eta: 0:02:41  lr: 0.000070  min_lr: 0.000001  loss: 4.2611 (4.2529)  class_acc: 0.2917 (0.2854)  loss_scale: 16384.0000 (19846.2246)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.0517  max mem: 15572
Epoch: [16]  [1140/1404]  eta: 0:02:35  lr: 0.000070  min_lr: 0.000001  loss: 4.3886 (4.2541)  class_acc: 0.2500 (0.2851)  loss_scale: 16384.0000 (19815.8808)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0294  max mem: 15572
[2025-01-17 00:19:27,156] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:19:27,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:19:27,164] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:19:27,164] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [1150/1404]  eta: 0:02:29  lr: 0.000070  min_lr: 0.000001  loss: 4.4112 (4.2551)  class_acc: 0.2500 (0.2848)  loss_scale: 16384.0000 (19914.1755)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0784  max mem: 15572
Epoch: [16]  [1160/1404]  eta: 0:02:23  lr: 0.000070  min_lr: 0.000001  loss: 4.3082 (4.2553)  class_acc: 0.2500 (0.2847)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1088  max mem: 15572
Epoch: [16]  [1170/1404]  eta: 0:02:18  lr: 0.000070  min_lr: 0.000001  loss: 4.1640 (4.2550)  class_acc: 0.2917 (0.2851)  loss_scale: 32768.0000 (20133.7114)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1026  max mem: 15572
Epoch: [16]  [1180/1404]  eta: 0:02:12  lr: 0.000070  min_lr: 0.000001  loss: 4.1640 (4.2545)  class_acc: 0.2500 (0.2850)  loss_scale: 32768.0000 (20240.6909)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0732  max mem: 15572
Epoch: [16]  [1190/1404]  eta: 0:02:06  lr: 0.000070  min_lr: 0.000001  loss: 4.2315 (4.2549)  class_acc: 0.2917 (0.2852)  loss_scale: 32768.0000 (20345.8741)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0376  max mem: 15572
Epoch: [16]  [1200/1404]  eta: 0:02:00  lr: 0.000070  min_lr: 0.000001  loss: 4.3437 (4.2557)  class_acc: 0.2500 (0.2849)  loss_scale: 32768.0000 (20449.3056)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1061  max mem: 15572
Epoch: [16]  [1210/1404]  eta: 0:01:54  lr: 0.000070  min_lr: 0.000001  loss: 4.3395 (4.2557)  class_acc: 0.2500 (0.2848)  loss_scale: 32768.0000 (20551.0289)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1356  max mem: 15572
Epoch: [16]  [1220/1404]  eta: 0:01:48  lr: 0.000070  min_lr: 0.000001  loss: 4.2310 (4.2549)  class_acc: 0.2917 (0.2850)  loss_scale: 32768.0000 (20651.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0374  max mem: 15572
Epoch: [16]  [1230/1404]  eta: 0:01:42  lr: 0.000070  min_lr: 0.000001  loss: 4.2269 (4.2548)  class_acc: 0.2917 (0.2851)  loss_scale: 32768.0000 (20749.5175)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0065  max mem: 15572
Epoch: [16]  [1240/1404]  eta: 0:01:36  lr: 0.000070  min_lr: 0.000001  loss: 4.3573 (4.2547)  class_acc: 0.2500 (0.2850)  loss_scale: 32768.0000 (20846.3626)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0515  max mem: 15572
Epoch: [16]  [1250/1404]  eta: 0:01:30  lr: 0.000070  min_lr: 0.000001  loss: 4.2898 (4.2554)  class_acc: 0.2917 (0.2847)  loss_scale: 32768.0000 (20941.6595)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0836  max mem: 15572
Epoch: [16]  [1260/1404]  eta: 0:01:24  lr: 0.000069  min_lr: 0.000001  loss: 4.3718 (4.2550)  class_acc: 0.2500 (0.2845)  loss_scale: 32768.0000 (21035.4449)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.1230  max mem: 15572
[2025-01-17 00:20:41,974] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23733
[2025-01-17 00:20:41,974] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:20:41,975] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23733
[2025-01-17 00:20:41,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:20:41,976] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [1270/1404]  eta: 0:01:19  lr: 0.000069  min_lr: 0.000001  loss: 4.1668 (4.2543)  class_acc: 0.2500 (0.2845)  loss_scale: 32768.0000 (21101.9732)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.1461  max mem: 15572
Epoch: [16]  [1280/1404]  eta: 0:01:13  lr: 0.000069  min_lr: 0.000001  loss: 4.2240 (4.2548)  class_acc: 0.2500 (0.2843)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0894  max mem: 15572
Epoch: [16]  [1290/1404]  eta: 0:01:07  lr: 0.000069  min_lr: 0.000001  loss: 4.3844 (4.2556)  class_acc: 0.2500 (0.2843)  loss_scale: 16384.0000 (21028.8830)  weight_decay: 0.0500 (0.0500)  time: 0.5224  data: 0.0284  max mem: 15572
Epoch: [16]  [1300/1404]  eta: 0:01:01  lr: 0.000069  min_lr: 0.000001  loss: 4.4088 (4.2563)  class_acc: 0.2500 (0.2840)  loss_scale: 16384.0000 (20993.1806)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0284  max mem: 15572
Epoch: [16]  [1310/1404]  eta: 0:00:55  lr: 0.000069  min_lr: 0.000001  loss: 4.2636 (4.2549)  class_acc: 0.2500 (0.2839)  loss_scale: 16384.0000 (20958.0229)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1121  max mem: 15572
Epoch: [16]  [1320/1404]  eta: 0:00:49  lr: 0.000069  min_lr: 0.000001  loss: 4.0627 (4.2553)  class_acc: 0.2083 (0.2838)  loss_scale: 16384.0000 (20923.3974)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1178  max mem: 15572
Epoch: [16]  [1330/1404]  eta: 0:00:43  lr: 0.000069  min_lr: 0.000001  loss: 4.3605 (4.2551)  class_acc: 0.2917 (0.2838)  loss_scale: 16384.0000 (20889.2923)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0788  max mem: 15572
Epoch: [16]  [1340/1404]  eta: 0:00:37  lr: 0.000069  min_lr: 0.000001  loss: 4.3577 (4.2558)  class_acc: 0.2917 (0.2839)  loss_scale: 16384.0000 (20855.6957)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1142  max mem: 15572
Epoch: [16]  [1350/1404]  eta: 0:00:31  lr: 0.000069  min_lr: 0.000001  loss: 4.3400 (4.2555)  class_acc: 0.2500 (0.2834)  loss_scale: 16384.0000 (20822.5966)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1388  max mem: 15572
Epoch: [16]  [1360/1404]  eta: 0:00:25  lr: 0.000069  min_lr: 0.000001  loss: 4.1809 (4.2542)  class_acc: 0.3333 (0.2840)  loss_scale: 16384.0000 (20789.9838)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0701  max mem: 15572
Epoch: [16]  [1370/1404]  eta: 0:00:20  lr: 0.000069  min_lr: 0.000001  loss: 4.1809 (4.2541)  class_acc: 0.3333 (0.2840)  loss_scale: 16384.0000 (20757.8468)  weight_decay: 0.0500 (0.0500)  time: 0.5161  data: 0.0188  max mem: 15572
Epoch: [16]  [1380/1404]  eta: 0:00:14  lr: 0.000069  min_lr: 0.000001  loss: 4.2420 (4.2539)  class_acc: 0.3333 (0.2841)  loss_scale: 16384.0000 (20726.1752)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0187  max mem: 15572
Epoch: [16]  [1390/1404]  eta: 0:00:08  lr: 0.000069  min_lr: 0.000001  loss: 4.2839 (4.2542)  class_acc: 0.2917 (0.2840)  loss_scale: 16384.0000 (20694.9590)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0412  max mem: 15572
[2025-01-17 00:21:54,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:21:54,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:21:54,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:21:54,458] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [1400/1404]  eta: 0:00:02  lr: 0.000069  min_lr: 0.000001  loss: 4.3496 (4.2549)  class_acc: 0.2083 (0.2838)  loss_scale: 16384.0000 (20699.2719)  weight_decay: 0.0500 (0.0500)  time: 0.4698  data: 0.0410  max mem: 15572
Epoch: [16]  [1403/1404]  eta: 0:00:00  lr: 0.000069  min_lr: 0.000001  loss: 4.3909 (4.2550)  class_acc: 0.2083 (0.2837)  loss_scale: 16384.0000 (20725.0598)  weight_decay: 0.0500 (0.0500)  time: 0.4538  data: 0.0409  max mem: 15572
Epoch: [16] Total time: 0:13:43 (0.5864 s / it)
Averaged stats: lr: 0.000069  min_lr: 0.000001  loss: 4.3909 (4.2652)  class_acc: 0.2083 (0.2837)  loss_scale: 16384.0000 (20725.0598)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:07:57  loss: 1.7614 (1.7614)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 3.5076  data: 3.2956  max mem: 15572
Val:  [ 10/136]  eta: 0:01:23  loss: 2.5643 (2.5113)  acc1: 44.4444 (42.4242)  acc5: 77.7778 (77.2727)  time: 0.6663  data: 0.4851  max mem: 15572
Val:  [ 20/136]  eta: 0:01:02  loss: 2.5962 (2.6051)  acc1: 38.8889 (41.0053)  acc5: 77.7778 (75.3968)  time: 0.3868  data: 0.1852  max mem: 15572
Val:  [ 30/136]  eta: 0:00:48  loss: 2.4048 (2.4840)  acc1: 44.4444 (44.9821)  acc5: 83.3333 (77.4194)  time: 0.3493  data: 0.1383  max mem: 15572
Val:  [ 40/136]  eta: 0:00:41  loss: 2.2866 (2.4671)  acc1: 50.0000 (45.7995)  acc5: 83.3333 (77.7778)  time: 0.3180  data: 0.1226  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.3560 (2.4881)  acc1: 50.0000 (45.0980)  acc5: 77.7778 (78.3224)  time: 0.3523  data: 0.1561  max mem: 15572
Val:  [ 60/136]  eta: 0:00:30  loss: 2.7056 (2.5881)  acc1: 38.8889 (42.3497)  acc5: 72.2222 (75.3188)  time: 0.3592  data: 0.1611  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.5634 (2.5615)  acc1: 38.8889 (42.8013)  acc5: 72.2222 (76.2128)  time: 0.3459  data: 0.1510  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.3062 (2.5552)  acc1: 50.0000 (43.1413)  acc5: 83.3333 (76.6118)  time: 0.3536  data: 0.1624  max mem: 15572
Val:  [ 90/136]  eta: 0:00:17  loss: 2.4697 (2.5614)  acc1: 38.8889 (42.5519)  acc5: 72.2222 (76.2515)  time: 0.3602  data: 0.1594  max mem: 15572
Val:  [100/136]  eta: 0:00:13  loss: 2.6691 (2.6064)  acc1: 38.8889 (41.4741)  acc5: 72.2222 (74.7525)  time: 0.3268  data: 0.1130  max mem: 15572
Val:  [110/136]  eta: 0:00:09  loss: 2.6290 (2.5996)  acc1: 38.8889 (41.6917)  acc5: 72.2222 (74.7748)  time: 0.3348  data: 0.1290  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.3027 (2.5617)  acc1: 50.0000 (42.7916)  acc5: 83.3333 (75.6198)  time: 0.3886  data: 0.1943  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.0429 (2.5320)  acc1: 55.5556 (43.6387)  acc5: 83.3333 (76.1238)  time: 0.3191  data: 0.1395  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.2406 (2.5366)  acc1: 44.4444 (43.7346)  acc5: 77.7778 (75.8395)  time: 0.1993  data: 0.0369  max mem: 15572
Val: Total time: 0:00:49 (0.3630 s / it)
* Acc@1 42.670 Acc@5 74.386 loss 2.574
Accuracy of the network on the 4883 val videos: 42.7%
[2025-01-17 00:22:46,518] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 00:22:46,520] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 00:22:46,520] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 00:22:46,520] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 00:22:48,968] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 00:22:48,968] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 42.67%
Epoch: [17]  [   0/1404]  eta: 3:06:32  lr: 0.000069  min_lr: 0.000001  loss: 4.5033 (4.5033)  class_acc: 0.3750 (0.3750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.9722  data: 7.4845  max mem: 15572
Epoch: [17]  [  10/1404]  eta: 0:27:50  lr: 0.000069  min_lr: 0.000001  loss: 4.1111 (4.1637)  class_acc: 0.2917 (0.2955)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1983  data: 0.6942  max mem: 15572
[2025-01-17 00:23:03,588] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23881
[2025-01-17 00:23:03,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:23:03,635] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23881
[2025-01-17 00:23:03,636] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:23:03,637] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [  20/1404]  eta: 0:20:45  lr: 0.000069  min_lr: 0.000001  loss: 4.1471 (4.1714)  class_acc: 0.2500 (0.2877)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0079  max mem: 15572
Epoch: [17]  [  30/1404]  eta: 0:18:01  lr: 0.000069  min_lr: 0.000001  loss: 4.2053 (4.1894)  class_acc: 0.2500 (0.2836)  loss_scale: 16384.0000 (23254.7097)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0186  max mem: 15572
Epoch: [17]  [  40/1404]  eta: 0:16:36  lr: 0.000069  min_lr: 0.000001  loss: 4.2479 (4.2297)  class_acc: 0.2917 (0.2835)  loss_scale: 16384.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0470  max mem: 15572
Epoch: [17]  [  50/1404]  eta: 0:15:59  lr: 0.000069  min_lr: 0.000001  loss: 4.3448 (4.2416)  class_acc: 0.2917 (0.2892)  loss_scale: 16384.0000 (20560.3137)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0844  max mem: 15572
Epoch: [17]  [  60/1404]  eta: 0:15:02  lr: 0.000069  min_lr: 0.000001  loss: 4.2872 (4.2459)  class_acc: 0.3333 (0.2958)  loss_scale: 16384.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0559  max mem: 15572
Epoch: [17]  [  70/1404]  eta: 0:14:38  lr: 0.000069  min_lr: 0.000001  loss: 4.2420 (4.2337)  class_acc: 0.2500 (0.2899)  loss_scale: 16384.0000 (19383.8873)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0554  max mem: 15572
Epoch: [17]  [  80/1404]  eta: 0:14:23  lr: 0.000069  min_lr: 0.000001  loss: 4.1200 (4.2371)  class_acc: 0.2500 (0.2891)  loss_scale: 16384.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1042  max mem: 15572
Epoch: [17]  [  90/1404]  eta: 0:14:21  lr: 0.000069  min_lr: 0.000001  loss: 4.2846 (4.2474)  class_acc: 0.2500 (0.2848)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.1484  max mem: 15572
Epoch: [17]  [ 100/1404]  eta: 0:14:13  lr: 0.000069  min_lr: 0.000001  loss: 4.3224 (4.2505)  class_acc: 0.2500 (0.2904)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.1756  max mem: 15572
Epoch: [17]  [ 110/1404]  eta: 0:14:01  lr: 0.000069  min_lr: 0.000001  loss: 4.2598 (4.2455)  class_acc: 0.2500 (0.2913)  loss_scale: 16384.0000 (18302.8468)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1374  max mem: 15572
Epoch: [17]  [ 120/1404]  eta: 0:13:40  lr: 0.000069  min_lr: 0.000001  loss: 4.1637 (4.2388)  class_acc: 0.2500 (0.2924)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0714  max mem: 15572
Epoch: [17]  [ 130/1404]  eta: 0:13:33  lr: 0.000069  min_lr: 0.000001  loss: 4.1341 (4.2322)  class_acc: 0.2917 (0.2936)  loss_scale: 16384.0000 (18009.8931)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0635  max mem: 15572
[2025-01-17 00:24:13,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=143, lr=[6.66268170184138e-07, 6.66268170184138e-07, 9.518116716916258e-07, 9.518116716916258e-07, 1.3597309595594655e-06, 1.3597309595594655e-06, 1.942472799370665e-06, 1.942472799370665e-06, 2.7749611419580934e-06, 2.7749611419580934e-06, 3.964230202797277e-06, 3.964230202797277e-06, 5.6631860039961095e-06, 5.6631860039961095e-06, 8.090265719994443e-06, 8.090265719994443e-06, 1.1557522457134919e-05, 1.1557522457134919e-05, 1.65107463673356e-05, 1.65107463673356e-05, 2.3586780524765142e-05, 2.3586780524765142e-05, 3.369540074966449e-05, 3.369540074966449e-05, 4.813628678523499e-05, 4.813628678523499e-05, 6.876612397890714e-05, 6.876612397890714e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 00:24:13,748] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=47.82607349369617, CurrSamplesPerSec=58.181730748667896, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [ 140/1404]  eta: 0:13:17  lr: 0.000069  min_lr: 0.000001  loss: 4.2221 (4.2417)  class_acc: 0.2500 (0.2837)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0852  max mem: 15572
[2025-01-17 00:24:19,026] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:24:19,027] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:24:19,073] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:24:19,074] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 150/1404]  eta: 0:13:01  lr: 0.000069  min_lr: 0.000001  loss: 4.2221 (4.2337)  class_acc: 0.2083 (0.2837)  loss_scale: 16384.0000 (18771.0728)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0324  max mem: 15572
[2025-01-17 00:24:24,551] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24020
[2025-01-17 00:24:24,551] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:24:24,551] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:24:24,560] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24020
[2025-01-17 00:24:24,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [17]  [ 160/1404]  eta: 0:12:59  lr: 0.000069  min_lr: 0.000001  loss: 4.1837 (4.2313)  class_acc: 0.3333 (0.2873)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1005  max mem: 15572
Epoch: [17]  [ 170/1404]  eta: 0:12:46  lr: 0.000069  min_lr: 0.000001  loss: 4.1626 (4.2269)  class_acc: 0.3333 (0.2900)  loss_scale: 16384.0000 (18587.6959)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1173  max mem: 15572
Epoch: [17]  [ 180/1404]  eta: 0:12:42  lr: 0.000069  min_lr: 0.000001  loss: 4.1626 (4.2298)  class_acc: 0.2500 (0.2891)  loss_scale: 16384.0000 (18465.9448)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1047  max mem: 15572
Epoch: [17]  [ 190/1404]  eta: 0:12:39  lr: 0.000069  min_lr: 0.000001  loss: 4.2523 (4.2305)  class_acc: 0.2500 (0.2886)  loss_scale: 16384.0000 (18356.9424)  weight_decay: 0.0500 (0.0500)  time: 0.6602  data: 0.1728  max mem: 15572
Epoch: [17]  [ 200/1404]  eta: 0:12:27  lr: 0.000069  min_lr: 0.000001  loss: 4.1790 (4.2352)  class_acc: 0.2917 (0.2886)  loss_scale: 16384.0000 (18258.7861)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1186  max mem: 15572
Epoch: [17]  [ 210/1404]  eta: 0:12:18  lr: 0.000069  min_lr: 0.000001  loss: 4.1790 (4.2341)  class_acc: 0.2917 (0.2891)  loss_scale: 16384.0000 (18169.9336)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0668  max mem: 15572
Epoch: [17]  [ 220/1404]  eta: 0:12:10  lr: 0.000069  min_lr: 0.000001  loss: 4.1341 (4.2323)  class_acc: 0.2917 (0.2877)  loss_scale: 16384.0000 (18089.1222)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0762  max mem: 15572
Epoch: [17]  [ 230/1404]  eta: 0:12:00  lr: 0.000069  min_lr: 0.000001  loss: 4.1255 (4.2266)  class_acc: 0.2500 (0.2890)  loss_scale: 16384.0000 (18015.3074)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0431  max mem: 15572
Epoch: [17]  [ 240/1404]  eta: 0:11:50  lr: 0.000068  min_lr: 0.000001  loss: 4.1572 (4.2278)  class_acc: 0.2500 (0.2882)  loss_scale: 16384.0000 (17947.6183)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0036  max mem: 15572
Epoch: [17]  [ 250/1404]  eta: 0:11:45  lr: 0.000068  min_lr: 0.000001  loss: 4.3251 (4.2315)  class_acc: 0.2500 (0.2883)  loss_scale: 16384.0000 (17885.3227)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0771  max mem: 15572
Epoch: [17]  [ 260/1404]  eta: 0:11:38  lr: 0.000068  min_lr: 0.000001  loss: 4.2834 (4.2342)  class_acc: 0.2917 (0.2894)  loss_scale: 16384.0000 (17827.8008)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1295  max mem: 15572
Epoch: [17]  [ 270/1404]  eta: 0:11:30  lr: 0.000068  min_lr: 0.000001  loss: 4.2951 (4.2376)  class_acc: 0.2500 (0.2883)  loss_scale: 16384.0000 (17774.5240)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0948  max mem: 15572
Epoch: [17]  [ 280/1404]  eta: 0:11:23  lr: 0.000068  min_lr: 0.000001  loss: 4.2900 (4.2376)  class_acc: 0.2500 (0.2890)  loss_scale: 16384.0000 (17725.0391)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0818  max mem: 15572
[2025-01-17 00:25:40,521] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:25:40,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:25:40,531] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:25:40,532] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 290/1404]  eta: 0:11:18  lr: 0.000068  min_lr: 0.000001  loss: 4.2318 (4.2396)  class_acc: 0.2500 (0.2889)  loss_scale: 16384.0000 (18241.9794)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1253  max mem: 15572
Epoch: [17]  [ 300/1404]  eta: 0:11:11  lr: 0.000068  min_lr: 0.000001  loss: 4.2962 (4.2413)  class_acc: 0.2500 (0.2888)  loss_scale: 32768.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1150  max mem: 15572
[2025-01-17 00:25:57,489] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24178
[2025-01-17 00:25:57,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:25:57,491] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [ 310/1404]  eta: 0:11:02  lr: 0.000068  min_lr: 0.000001  loss: 4.1979 (4.2362)  class_acc: 0.2917 (0.2899)  loss_scale: 32768.0000 (19123.4469)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.0516  max mem: 15572
[2025-01-17 00:25:57,535] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24178
[2025-01-17 00:25:57,536] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [17]  [ 320/1404]  eta: 0:10:55  lr: 0.000068  min_lr: 0.000001  loss: 4.2903 (4.2401)  class_acc: 0.2917 (0.2889)  loss_scale: 16384.0000 (19038.1059)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0649  max mem: 15572
Epoch: [17]  [ 330/1404]  eta: 0:10:50  lr: 0.000068  min_lr: 0.000001  loss: 4.3197 (4.2445)  class_acc: 0.2500 (0.2890)  loss_scale: 16384.0000 (18957.9215)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1186  max mem: 15572
Epoch: [17]  [ 340/1404]  eta: 0:10:42  lr: 0.000068  min_lr: 0.000001  loss: 4.3382 (4.2481)  class_acc: 0.2500 (0.2882)  loss_scale: 16384.0000 (18882.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0885  max mem: 15572
Epoch: [17]  [ 350/1404]  eta: 0:10:37  lr: 0.000068  min_lr: 0.000001  loss: 4.3878 (4.2526)  class_acc: 0.2500 (0.2886)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0940  max mem: 15572
Epoch: [17]  [ 360/1404]  eta: 0:10:32  lr: 0.000068  min_lr: 0.000001  loss: 4.4124 (4.2567)  class_acc: 0.2500 (0.2884)  loss_scale: 16384.0000 (18744.0222)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1508  max mem: 15572
Epoch: [17]  [ 370/1404]  eta: 0:10:23  lr: 0.000068  min_lr: 0.000001  loss: 4.2577 (4.2518)  class_acc: 0.2500 (0.2884)  loss_scale: 16384.0000 (18680.4097)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0923  max mem: 15572
Epoch: [17]  [ 380/1404]  eta: 0:10:17  lr: 0.000068  min_lr: 0.000001  loss: 4.0934 (4.2508)  class_acc: 0.2500 (0.2866)  loss_scale: 16384.0000 (18620.1365)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0799  max mem: 15572
Epoch: [17]  [ 390/1404]  eta: 0:10:09  lr: 0.000068  min_lr: 0.000001  loss: 4.2810 (4.2513)  class_acc: 0.2500 (0.2858)  loss_scale: 16384.0000 (18562.9463)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0849  max mem: 15572
Epoch: [17]  [ 400/1404]  eta: 0:10:03  lr: 0.000068  min_lr: 0.000001  loss: 4.2905 (4.2539)  class_acc: 0.2083 (0.2843)  loss_scale: 16384.0000 (18508.6085)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0748  max mem: 15572
Epoch: [17]  [ 410/1404]  eta: 0:09:56  lr: 0.000068  min_lr: 0.000001  loss: 4.3309 (4.2565)  class_acc: 0.2917 (0.2854)  loss_scale: 16384.0000 (18456.9148)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0882  max mem: 15572
Epoch: [17]  [ 420/1404]  eta: 0:09:51  lr: 0.000068  min_lr: 0.000001  loss: 4.2671 (4.2531)  class_acc: 0.3333 (0.2866)  loss_scale: 16384.0000 (18407.6770)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1186  max mem: 15572
Epoch: [17]  [ 430/1404]  eta: 0:09:43  lr: 0.000068  min_lr: 0.000001  loss: 4.1956 (4.2513)  class_acc: 0.3333 (0.2867)  loss_scale: 16384.0000 (18360.7239)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.0879  max mem: 15572
[2025-01-17 00:27:13,109] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:27:13,109] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:27:13,144] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:27:13,145] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 440/1404]  eta: 0:09:38  lr: 0.000068  min_lr: 0.000001  loss: 4.2746 (4.2539)  class_acc: 0.2917 (0.2866)  loss_scale: 16384.0000 (18390.2041)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0824  max mem: 15572
Epoch: [17]  [ 450/1404]  eta: 0:09:31  lr: 0.000068  min_lr: 0.000001  loss: 4.3468 (4.2561)  class_acc: 0.2500 (0.2874)  loss_scale: 32768.0000 (18709.0022)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1165  max mem: 15572
[2025-01-17 00:27:25,395] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24328
[2025-01-17 00:27:25,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:27:25,395] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [ 460/1404]  eta: 0:09:25  lr: 0.000068  min_lr: 0.000001  loss: 4.2386 (4.2539)  class_acc: 0.2500 (0.2882)  loss_scale: 32768.0000 (18978.4295)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0805  max mem: 15572
[2025-01-17 00:27:25,442] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24328
[2025-01-17 00:27:25,443] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [17]  [ 470/1404]  eta: 0:09:18  lr: 0.000068  min_lr: 0.000001  loss: 4.2301 (4.2532)  class_acc: 0.3333 (0.2892)  loss_scale: 16384.0000 (18923.3461)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0659  max mem: 15572
Epoch: [17]  [ 480/1404]  eta: 0:09:11  lr: 0.000068  min_lr: 0.000001  loss: 4.2389 (4.2519)  class_acc: 0.2917 (0.2895)  loss_scale: 16384.0000 (18870.5530)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0582  max mem: 15572
Epoch: [17]  [ 490/1404]  eta: 0:09:04  lr: 0.000068  min_lr: 0.000001  loss: 4.1330 (4.2513)  class_acc: 0.2917 (0.2898)  loss_scale: 16384.0000 (18819.9104)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0413  max mem: 15572
Epoch: [17]  [ 500/1404]  eta: 0:08:59  lr: 0.000068  min_lr: 0.000001  loss: 4.1330 (4.2520)  class_acc: 0.2500 (0.2893)  loss_scale: 16384.0000 (18771.2894)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0874  max mem: 15572
Epoch: [17]  [ 510/1404]  eta: 0:08:53  lr: 0.000068  min_lr: 0.000001  loss: 4.2218 (4.2498)  class_acc: 0.2500 (0.2902)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1317  max mem: 15572
Epoch: [17]  [ 520/1404]  eta: 0:08:49  lr: 0.000068  min_lr: 0.000001  loss: 4.1464 (4.2477)  class_acc: 0.2917 (0.2909)  loss_scale: 16384.0000 (18679.6468)  weight_decay: 0.0500 (0.0500)  time: 0.6579  data: 0.1626  max mem: 15572
Epoch: [17]  [ 530/1404]  eta: 0:08:42  lr: 0.000068  min_lr: 0.000001  loss: 4.1464 (4.2489)  class_acc: 0.2917 (0.2904)  loss_scale: 16384.0000 (18636.4143)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1423  max mem: 15572
Epoch: [17]  [ 540/1404]  eta: 0:08:35  lr: 0.000068  min_lr: 0.000001  loss: 4.3232 (4.2497)  class_acc: 0.2917 (0.2904)  loss_scale: 16384.0000 (18594.7800)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0548  max mem: 15572
Epoch: [17]  [ 550/1404]  eta: 0:08:29  lr: 0.000068  min_lr: 0.000001  loss: 4.3026 (4.2498)  class_acc: 0.2917 (0.2908)  loss_scale: 16384.0000 (18554.6570)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0862  max mem: 15572
Epoch: [17]  [ 560/1404]  eta: 0:08:23  lr: 0.000068  min_lr: 0.000001  loss: 4.1125 (4.2459)  class_acc: 0.2917 (0.2915)  loss_scale: 16384.0000 (18515.9643)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1008  max mem: 15572
Epoch: [17]  [ 570/1404]  eta: 0:08:17  lr: 0.000068  min_lr: 0.000001  loss: 4.2165 (4.2484)  class_acc: 0.2917 (0.2917)  loss_scale: 16384.0000 (18478.6270)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0584  max mem: 15572
Epoch: [17]  [ 580/1404]  eta: 0:08:10  lr: 0.000068  min_lr: 0.000001  loss: 4.2280 (4.2438)  class_acc: 0.2917 (0.2927)  loss_scale: 16384.0000 (18442.5749)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0357  max mem: 15572
[2025-01-17 00:28:40,112] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:28:40,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:28:40,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:28:40,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 590/1404]  eta: 0:08:03  lr: 0.000068  min_lr: 0.000001  loss: 4.1572 (4.2445)  class_acc: 0.2917 (0.2929)  loss_scale: 16384.0000 (18463.1878)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0592  max mem: 15572
Epoch: [17]  [ 600/1404]  eta: 0:07:57  lr: 0.000068  min_lr: 0.000001  loss: 4.2313 (4.2437)  class_acc: 0.2917 (0.2932)  loss_scale: 32768.0000 (18701.2047)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0832  max mem: 15572
Epoch: [17]  [ 610/1404]  eta: 0:07:51  lr: 0.000067  min_lr: 0.000001  loss: 4.1426 (4.2447)  class_acc: 0.2500 (0.2923)  loss_scale: 32768.0000 (18931.4304)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1020  max mem: 15572
Epoch: [17]  [ 620/1404]  eta: 0:07:47  lr: 0.000067  min_lr: 0.000001  loss: 4.3073 (4.2459)  class_acc: 0.2500 (0.2922)  loss_scale: 32768.0000 (19154.2415)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1711  max mem: 15572
Epoch: [17]  [ 630/1404]  eta: 0:07:41  lr: 0.000067  min_lr: 0.000001  loss: 4.4405 (4.2482)  class_acc: 0.3333 (0.2931)  loss_scale: 32768.0000 (19369.9905)  weight_decay: 0.0500 (0.0500)  time: 0.6633  data: 0.1946  max mem: 15572
Epoch: [17]  [ 640/1404]  eta: 0:07:34  lr: 0.000067  min_lr: 0.000001  loss: 4.4086 (4.2498)  class_acc: 0.2917 (0.2930)  loss_scale: 32768.0000 (19579.0078)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0896  max mem: 15572
[2025-01-17 00:29:12,687] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24512
[2025-01-17 00:29:12,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:29:12,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:29:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24512
[2025-01-17 00:29:12,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [17]  [ 650/1404]  eta: 0:07:27  lr: 0.000067  min_lr: 0.000001  loss: 4.3550 (4.2503)  class_acc: 0.2500 (0.2933)  loss_scale: 32768.0000 (19605.4316)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0183  max mem: 15572
Epoch: [17]  [ 660/1404]  eta: 0:07:21  lr: 0.000067  min_lr: 0.000001  loss: 4.1485 (4.2508)  class_acc: 0.2500 (0.2932)  loss_scale: 16384.0000 (19556.6959)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0660  max mem: 15572
Epoch: [17]  [ 670/1404]  eta: 0:07:16  lr: 0.000067  min_lr: 0.000001  loss: 4.1970 (4.2512)  class_acc: 0.2083 (0.2921)  loss_scale: 16384.0000 (19509.4128)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1024  max mem: 15572
Epoch: [17]  [ 680/1404]  eta: 0:07:10  lr: 0.000067  min_lr: 0.000001  loss: 4.2036 (4.2519)  class_acc: 0.2083 (0.2914)  loss_scale: 16384.0000 (19463.5184)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1001  max mem: 15572
Epoch: [17]  [ 690/1404]  eta: 0:07:03  lr: 0.000067  min_lr: 0.000001  loss: 4.1845 (4.2508)  class_acc: 0.2500 (0.2921)  loss_scale: 16384.0000 (19418.9522)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0697  max mem: 15572
Epoch: [17]  [ 700/1404]  eta: 0:06:58  lr: 0.000067  min_lr: 0.000001  loss: 4.1592 (4.2511)  class_acc: 0.2500 (0.2919)  loss_scale: 16384.0000 (19375.6576)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0935  max mem: 15572
Epoch: [17]  [ 710/1404]  eta: 0:06:52  lr: 0.000067  min_lr: 0.000001  loss: 4.2452 (4.2519)  class_acc: 0.2083 (0.2909)  loss_scale: 16384.0000 (19333.5809)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1089  max mem: 15572
Epoch: [17]  [ 720/1404]  eta: 0:06:46  lr: 0.000067  min_lr: 0.000001  loss: 4.2389 (4.2507)  class_acc: 0.2083 (0.2906)  loss_scale: 16384.0000 (19292.6713)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0630  max mem: 15572
Epoch: [17]  [ 730/1404]  eta: 0:06:39  lr: 0.000067  min_lr: 0.000001  loss: 4.2998 (4.2521)  class_acc: 0.2500 (0.2907)  loss_scale: 16384.0000 (19252.8810)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0499  max mem: 15572
Epoch: [17]  [ 740/1404]  eta: 0:06:34  lr: 0.000067  min_lr: 0.000001  loss: 4.3294 (4.2519)  class_acc: 0.3333 (0.2913)  loss_scale: 16384.0000 (19214.1646)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0939  max mem: 15572
Epoch: [17]  [ 750/1404]  eta: 0:06:27  lr: 0.000067  min_lr: 0.000001  loss: 4.2385 (4.2505)  class_acc: 0.3333 (0.2921)  loss_scale: 16384.0000 (19176.4794)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.0838  max mem: 15572
Epoch: [17]  [ 760/1404]  eta: 0:06:21  lr: 0.000067  min_lr: 0.000001  loss: 4.3170 (4.2527)  class_acc: 0.2083 (0.2915)  loss_scale: 16384.0000 (19139.7845)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0382  max mem: 15572
Epoch: [17]  [ 770/1404]  eta: 0:06:15  lr: 0.000067  min_lr: 0.000001  loss: 4.4008 (4.2537)  class_acc: 0.2500 (0.2920)  loss_scale: 16384.0000 (19104.0415)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0772  max mem: 15572
[2025-01-17 00:30:27,446] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:30:27,446] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:30:27,489] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:30:27,490] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 780/1404]  eta: 0:06:09  lr: 0.000067  min_lr: 0.000001  loss: 4.2366 (4.2544)  class_acc: 0.2500 (0.2911)  loss_scale: 16384.0000 (19237.0397)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1195  max mem: 15572
Epoch: [17]  [ 790/1404]  eta: 0:06:04  lr: 0.000067  min_lr: 0.000001  loss: 4.2366 (4.2544)  class_acc: 0.2083 (0.2909)  loss_scale: 32768.0000 (19408.1011)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1266  max mem: 15572
[2025-01-17 00:30:44,308] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24666
[2025-01-17 00:30:44,308] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:30:44,369] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24666
[2025-01-17 00:30:44,369] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:30:44,370] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [ 800/1404]  eta: 0:05:58  lr: 0.000067  min_lr: 0.000001  loss: 4.2364 (4.2533)  class_acc: 0.2500 (0.2914)  loss_scale: 32768.0000 (19513.5281)  weight_decay: 0.0500 (0.0500)  time: 0.6542  data: 0.1608  max mem: 15572
Epoch: [17]  [ 810/1404]  eta: 0:05:52  lr: 0.000067  min_lr: 0.000001  loss: 4.1730 (4.2547)  class_acc: 0.2500 (0.2909)  loss_scale: 16384.0000 (19474.9396)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0987  max mem: 15572
Epoch: [17]  [ 820/1404]  eta: 0:05:45  lr: 0.000067  min_lr: 0.000001  loss: 4.1638 (4.2535)  class_acc: 0.2500 (0.2910)  loss_scale: 16384.0000 (19437.2911)  weight_decay: 0.0500 (0.0500)  time: 0.5015  data: 0.0006  max mem: 15572
Epoch: [17]  [ 830/1404]  eta: 0:05:39  lr: 0.000067  min_lr: 0.000001  loss: 4.1697 (4.2544)  class_acc: 0.2917 (0.2904)  loss_scale: 16384.0000 (19400.5487)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0009  max mem: 15572
Epoch: [17]  [ 840/1404]  eta: 0:05:32  lr: 0.000067  min_lr: 0.000001  loss: 4.1798 (4.2529)  class_acc: 0.2500 (0.2905)  loss_scale: 16384.0000 (19364.6801)  weight_decay: 0.0500 (0.0500)  time: 0.5280  data: 0.0342  max mem: 15572
Epoch: [17]  [ 850/1404]  eta: 0:05:27  lr: 0.000067  min_lr: 0.000001  loss: 4.2475 (4.2545)  class_acc: 0.2500 (0.2903)  loss_scale: 16384.0000 (19329.6545)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1039  max mem: 15572
Epoch: [17]  [ 860/1404]  eta: 0:05:22  lr: 0.000067  min_lr: 0.000001  loss: 4.2897 (4.2549)  class_acc: 0.2917 (0.2903)  loss_scale: 16384.0000 (19295.4425)  weight_decay: 0.0500 (0.0500)  time: 0.6718  data: 0.0862  max mem: 15572
Epoch: [17]  [ 870/1404]  eta: 0:05:16  lr: 0.000067  min_lr: 0.000001  loss: 4.2393 (4.2557)  class_acc: 0.2500 (0.2897)  loss_scale: 16384.0000 (19262.0161)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0165  max mem: 15572
Epoch: [17]  [ 880/1404]  eta: 0:05:10  lr: 0.000067  min_lr: 0.000001  loss: 4.2978 (4.2569)  class_acc: 0.2083 (0.2884)  loss_scale: 16384.0000 (19229.3485)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0008  max mem: 15572
Epoch: [17]  [ 890/1404]  eta: 0:05:03  lr: 0.000067  min_lr: 0.000001  loss: 4.3449 (4.2574)  class_acc: 0.2083 (0.2885)  loss_scale: 16384.0000 (19197.4141)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.0009  max mem: 15572
Epoch: [17]  [ 900/1404]  eta: 0:04:58  lr: 0.000067  min_lr: 0.000001  loss: 4.2555 (4.2580)  class_acc: 0.2500 (0.2882)  loss_scale: 16384.0000 (19166.1887)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.0157  max mem: 15572
Epoch: [17]  [ 910/1404]  eta: 0:04:52  lr: 0.000067  min_lr: 0.000001  loss: 4.2345 (4.2573)  class_acc: 0.2500 (0.2882)  loss_scale: 16384.0000 (19135.6487)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.0153  max mem: 15572
Epoch: [17]  [ 920/1404]  eta: 0:04:46  lr: 0.000067  min_lr: 0.000001  loss: 4.1643 (4.2541)  class_acc: 0.2917 (0.2892)  loss_scale: 16384.0000 (19105.7720)  weight_decay: 0.0500 (0.0500)  time: 0.5305  data: 0.0005  max mem: 15572
[2025-01-17 00:31:58,075] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:31:58,076] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:31:58,075] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:31:58,076] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 930/1404]  eta: 0:04:40  lr: 0.000067  min_lr: 0.000001  loss: 3.9641 (4.2522)  class_acc: 0.2917 (0.2891)  loss_scale: 16384.0000 (19146.9302)  weight_decay: 0.0500 (0.0500)  time: 0.5166  data: 0.0007  max mem: 15572
Epoch: [17]  [ 940/1404]  eta: 0:04:33  lr: 0.000067  min_lr: 0.000001  loss: 4.2467 (4.2528)  class_acc: 0.2917 (0.2897)  loss_scale: 32768.0000 (19291.6812)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0008  max mem: 15572
Epoch: [17]  [ 950/1404]  eta: 0:04:27  lr: 0.000067  min_lr: 0.000001  loss: 4.3084 (4.2529)  class_acc: 0.3333 (0.2900)  loss_scale: 32768.0000 (19433.3880)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0007  max mem: 15572
Epoch: [17]  [ 960/1404]  eta: 0:04:22  lr: 0.000067  min_lr: 0.000001  loss: 4.3220 (4.2553)  class_acc: 0.3333 (0.2904)  loss_scale: 32768.0000 (19572.1457)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0006  max mem: 15572
[2025-01-17 00:32:18,759] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24831
[2025-01-17 00:32:18,759] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:32:18,761] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24831
[2025-01-17 00:32:18,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:32:18,761] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [ 970/1404]  eta: 0:04:16  lr: 0.000067  min_lr: 0.000001  loss: 4.3888 (4.2545)  class_acc: 0.2917 (0.2905)  loss_scale: 32768.0000 (19573.0587)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.0005  max mem: 15572
Epoch: [17]  [ 980/1404]  eta: 0:04:10  lr: 0.000066  min_lr: 0.000001  loss: 4.2324 (4.2534)  class_acc: 0.3333 (0.2912)  loss_scale: 16384.0000 (19540.5505)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0008  max mem: 15572
Epoch: [17]  [ 990/1404]  eta: 0:04:04  lr: 0.000066  min_lr: 0.000001  loss: 4.1148 (4.2516)  class_acc: 0.2917 (0.2912)  loss_scale: 16384.0000 (19508.6983)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0011  max mem: 15572
Epoch: [17]  [1000/1404]  eta: 0:03:58  lr: 0.000066  min_lr: 0.000001  loss: 4.1705 (4.2506)  class_acc: 0.2500 (0.2908)  loss_scale: 16384.0000 (19477.4825)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0009  max mem: 15572
Epoch: [17]  [1010/1404]  eta: 0:03:52  lr: 0.000066  min_lr: 0.000001  loss: 4.2125 (4.2506)  class_acc: 0.2083 (0.2899)  loss_scale: 16384.0000 (19446.8843)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0007  max mem: 15572
Epoch: [17]  [1020/1404]  eta: 0:03:46  lr: 0.000066  min_lr: 0.000001  loss: 4.3460 (4.2517)  class_acc: 0.2083 (0.2891)  loss_scale: 16384.0000 (19416.8854)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0007  max mem: 15572
Epoch: [17]  [1030/1404]  eta: 0:03:40  lr: 0.000066  min_lr: 0.000001  loss: 4.3983 (4.2517)  class_acc: 0.2500 (0.2894)  loss_scale: 16384.0000 (19387.4685)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0047  max mem: 15572
Epoch: [17]  [1040/1404]  eta: 0:03:34  lr: 0.000066  min_lr: 0.000001  loss: 4.1561 (4.2515)  class_acc: 0.2917 (0.2897)  loss_scale: 16384.0000 (19358.6167)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0213  max mem: 15572
Epoch: [17]  [1050/1404]  eta: 0:03:28  lr: 0.000066  min_lr: 0.000001  loss: 4.2559 (4.2510)  class_acc: 0.3333 (0.2902)  loss_scale: 16384.0000 (19330.3140)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0616  max mem: 15572
Epoch: [17]  [1060/1404]  eta: 0:03:22  lr: 0.000066  min_lr: 0.000001  loss: 4.1964 (4.2501)  class_acc: 0.2917 (0.2902)  loss_scale: 16384.0000 (19302.5448)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.0554  max mem: 15572
Epoch: [17]  [1070/1404]  eta: 0:03:16  lr: 0.000066  min_lr: 0.000001  loss: 4.2271 (4.2509)  class_acc: 0.2500 (0.2896)  loss_scale: 16384.0000 (19275.2941)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0297  max mem: 15572
Epoch: [17]  [1080/1404]  eta: 0:03:10  lr: 0.000066  min_lr: 0.000001  loss: 4.3293 (4.2517)  class_acc: 0.2083 (0.2893)  loss_scale: 16384.0000 (19248.5476)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0905  max mem: 15572
Epoch: [17]  [1090/1404]  eta: 0:03:05  lr: 0.000066  min_lr: 0.000001  loss: 4.3531 (4.2534)  class_acc: 0.2500 (0.2892)  loss_scale: 16384.0000 (19222.2915)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1062  max mem: 15572
[2025-01-17 00:33:34,056] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:33:34,056] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:33:34,058] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:33:34,058] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [1100/1404]  eta: 0:02:59  lr: 0.000066  min_lr: 0.000001  loss: 4.3531 (4.2538)  class_acc: 0.2500 (0.2894)  loss_scale: 16384.0000 (19330.4414)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0669  max mem: 15572
[2025-01-17 00:33:39,752] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24969
[2025-01-17 00:33:39,752] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:33:39,752] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24969
[2025-01-17 00:33:39,752] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:33:39,752] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [1110/1404]  eta: 0:02:53  lr: 0.000066  min_lr: 0.000001  loss: 4.3486 (4.2550)  class_acc: 0.2083 (0.2886)  loss_scale: 16384.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1006  max mem: 15572
Epoch: [17]  [1120/1404]  eta: 0:02:47  lr: 0.000066  min_lr: 0.000001  loss: 4.2026 (4.2541)  class_acc: 0.2083 (0.2885)  loss_scale: 16384.0000 (19277.8733)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0691  max mem: 15572
Epoch: [17]  [1130/1404]  eta: 0:02:41  lr: 0.000066  min_lr: 0.000001  loss: 4.1506 (4.2542)  class_acc: 0.2500 (0.2884)  loss_scale: 16384.0000 (19252.2865)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0361  max mem: 15572
[2025-01-17 00:33:55,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=150, lr=[6.401960125700905e-07, 6.401960125700905e-07, 9.145657322429866e-07, 9.145657322429866e-07, 1.3065224746328381e-06, 1.3065224746328381e-06, 1.8664606780469118e-06, 1.8664606780469118e-06, 2.666372397209874e-06, 2.666372397209874e-06, 3.8091034245855346e-06, 3.8091034245855346e-06, 5.441576320836478e-06, 5.441576320836478e-06, 7.773680458337827e-06, 7.773680458337827e-06, 1.1105257797625467e-05, 1.1105257797625467e-05, 1.586465399660781e-05, 1.586465399660781e-05, 2.2663791423725444e-05, 2.2663791423725444e-05, 3.2376844891036355e-05, 3.2376844891036355e-05, 4.6252635558623366e-05, 4.6252635558623366e-05, 6.607519365517624e-05, 6.607519365517624e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 00:33:55,736] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=47.904767693208456, CurrSamplesPerSec=54.4248012391969, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [1140/1404]  eta: 0:02:35  lr: 0.000066  min_lr: 0.000001  loss: 4.3495 (4.2549)  class_acc: 0.2500 (0.2883)  loss_scale: 16384.0000 (19227.1481)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0437  max mem: 15572
Epoch: [17]  [1150/1404]  eta: 0:02:29  lr: 0.000066  min_lr: 0.000001  loss: 4.3532 (4.2543)  class_acc: 0.2500 (0.2886)  loss_scale: 16384.0000 (19202.4466)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0172  max mem: 15572
Epoch: [17]  [1160/1404]  eta: 0:02:23  lr: 0.000066  min_lr: 0.000001  loss: 4.2673 (4.2553)  class_acc: 0.2500 (0.2886)  loss_scale: 16384.0000 (19178.1705)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0233  max mem: 15572
Epoch: [17]  [1170/1404]  eta: 0:02:17  lr: 0.000066  min_lr: 0.000001  loss: 4.2673 (4.2543)  class_acc: 0.2917 (0.2889)  loss_scale: 16384.0000 (19154.3091)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0249  max mem: 15572
Epoch: [17]  [1180/1404]  eta: 0:02:12  lr: 0.000066  min_lr: 0.000001  loss: 4.3439 (4.2545)  class_acc: 0.2917 (0.2891)  loss_scale: 16384.0000 (19130.8518)  weight_decay: 0.0500 (0.0500)  time: 0.6640  data: 0.0106  max mem: 15572
Epoch: [17]  [1190/1404]  eta: 0:02:06  lr: 0.000066  min_lr: 0.000001  loss: 4.3490 (4.2547)  class_acc: 0.2500 (0.2890)  loss_scale: 16384.0000 (19107.7884)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0131  max mem: 15572
Epoch: [17]  [1200/1404]  eta: 0:02:00  lr: 0.000066  min_lr: 0.000001  loss: 4.3466 (4.2557)  class_acc: 0.2083 (0.2886)  loss_scale: 16384.0000 (19085.1091)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0543  max mem: 15572
Epoch: [17]  [1210/1404]  eta: 0:01:54  lr: 0.000066  min_lr: 0.000001  loss: 4.2434 (4.2553)  class_acc: 0.2500 (0.2888)  loss_scale: 16384.0000 (19062.8043)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.1082  max mem: 15572
Epoch: [17]  [1220/1404]  eta: 0:01:48  lr: 0.000066  min_lr: 0.000001  loss: 4.2379 (4.2556)  class_acc: 0.2917 (0.2892)  loss_scale: 16384.0000 (19040.8649)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0806  max mem: 15572
[2025-01-17 00:34:55,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:34:55,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:34:55,613] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:34:55,613] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [1230/1404]  eta: 0:01:42  lr: 0.000066  min_lr: 0.000001  loss: 4.2502 (4.2553)  class_acc: 0.2917 (0.2891)  loss_scale: 16384.0000 (19032.5914)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1088  max mem: 15572
Epoch: [17]  [1240/1404]  eta: 0:01:36  lr: 0.000066  min_lr: 0.000001  loss: 4.1827 (4.2551)  class_acc: 0.2500 (0.2888)  loss_scale: 32768.0000 (19143.2716)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1233  max mem: 15572
Epoch: [17]  [1250/1404]  eta: 0:01:30  lr: 0.000066  min_lr: 0.000001  loss: 4.1787 (4.2548)  class_acc: 0.2917 (0.2892)  loss_scale: 32768.0000 (19252.1823)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0549  max mem: 15572
Epoch: [17]  [1260/1404]  eta: 0:01:24  lr: 0.000066  min_lr: 0.000001  loss: 4.2084 (4.2548)  class_acc: 0.3333 (0.2896)  loss_scale: 32768.0000 (19359.3656)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1111  max mem: 15572
Epoch: [17]  [1270/1404]  eta: 0:01:19  lr: 0.000066  min_lr: 0.000001  loss: 4.0935 (4.2532)  class_acc: 0.3333 (0.2898)  loss_scale: 32768.0000 (19464.8623)  weight_decay: 0.0500 (0.0500)  time: 0.6687  data: 0.2074  max mem: 15572
Epoch: [17]  [1280/1404]  eta: 0:01:13  lr: 0.000066  min_lr: 0.000001  loss: 4.0668 (4.2534)  class_acc: 0.2917 (0.2900)  loss_scale: 32768.0000 (19568.7119)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1231  max mem: 15572
Epoch: [17]  [1290/1404]  eta: 0:01:07  lr: 0.000066  min_lr: 0.000001  loss: 4.2386 (4.2532)  class_acc: 0.3333 (0.2900)  loss_scale: 32768.0000 (19670.9527)  weight_decay: 0.0500 (0.0500)  time: 0.5002  data: 0.0005  max mem: 15572
Epoch: [17]  [1300/1404]  eta: 0:01:01  lr: 0.000066  min_lr: 0.000001  loss: 4.2386 (4.2535)  class_acc: 0.3333 (0.2901)  loss_scale: 32768.0000 (19771.6218)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0005  max mem: 15572
[2025-01-17 00:35:38,673] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25175
[2025-01-17 00:35:38,674] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:35:38,674] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:35:38,703] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25175
[2025-01-17 00:35:38,704] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [17]  [1310/1404]  eta: 0:00:55  lr: 0.000066  min_lr: 0.000001  loss: 4.1908 (4.2527)  class_acc: 0.2083 (0.2902)  loss_scale: 32768.0000 (19820.7658)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0317  max mem: 15572
Epoch: [17]  [1320/1404]  eta: 0:00:49  lr: 0.000066  min_lr: 0.000001  loss: 4.1908 (4.2522)  class_acc: 0.2500 (0.2902)  loss_scale: 16384.0000 (19794.7494)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1274  max mem: 15572
Epoch: [17]  [1330/1404]  eta: 0:00:43  lr: 0.000066  min_lr: 0.000001  loss: 4.3105 (4.2527)  class_acc: 0.2083 (0.2898)  loss_scale: 16384.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  time: 0.6979  data: 0.2204  max mem: 15572
Epoch: [17]  [1340/1404]  eta: 0:00:37  lr: 0.000066  min_lr: 0.000001  loss: 4.2344 (4.2528)  class_acc: 0.2083 (0.2893)  loss_scale: 16384.0000 (19743.8807)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1318  max mem: 15572
Epoch: [17]  [1350/1404]  eta: 0:00:31  lr: 0.000065  min_lr: 0.000001  loss: 4.2241 (4.2529)  class_acc: 0.2500 (0.2892)  loss_scale: 16384.0000 (19719.0111)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0979  max mem: 15572
Epoch: [17]  [1360/1404]  eta: 0:00:25  lr: 0.000065  min_lr: 0.000001  loss: 4.2948 (4.2540)  class_acc: 0.2917 (0.2893)  loss_scale: 16384.0000 (19694.5070)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1289  max mem: 15572
Epoch: [17]  [1370/1404]  eta: 0:00:20  lr: 0.000065  min_lr: 0.000001  loss: 4.3429 (4.2535)  class_acc: 0.2917 (0.2895)  loss_scale: 16384.0000 (19670.3603)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0763  max mem: 15572
Epoch: [17]  [1380/1404]  eta: 0:00:14  lr: 0.000065  min_lr: 0.000001  loss: 4.2598 (4.2527)  class_acc: 0.2917 (0.2896)  loss_scale: 16384.0000 (19646.5634)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0821  max mem: 15572
Epoch: [17]  [1390/1404]  eta: 0:00:08  lr: 0.000065  min_lr: 0.000001  loss: 4.2598 (4.2522)  class_acc: 0.2083 (0.2893)  loss_scale: 16384.0000 (19623.1086)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0778  max mem: 15572
Epoch: [17]  [1400/1404]  eta: 0:00:02  lr: 0.000065  min_lr: 0.000001  loss: 4.2475 (4.2512)  class_acc: 0.2083 (0.2897)  loss_scale: 16384.0000 (19599.9886)  weight_decay: 0.0500 (0.0500)  time: 0.4563  data: 0.0338  max mem: 15572
Epoch: [17]  [1403/1404]  eta: 0:00:00  lr: 0.000065  min_lr: 0.000001  loss: 4.2475 (4.2512)  class_acc: 0.2083 (0.2897)  loss_scale: 16384.0000 (19593.1168)  weight_decay: 0.0500 (0.0500)  time: 0.4339  data: 0.0337  max mem: 15572
Epoch: [17] Total time: 0:13:44 (0.5873 s / it)
Averaged stats: lr: 0.000065  min_lr: 0.000001  loss: 4.2475 (4.2563)  class_acc: 0.2083 (0.2895)  loss_scale: 16384.0000 (19593.1168)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:15:19  loss: 1.7230 (1.7230)  acc1: 61.1111 (61.1111)  acc5: 83.3333 (83.3333)  time: 6.7635  data: 6.4337  max mem: 15572
Val:  [ 10/136]  eta: 0:01:44  loss: 2.5592 (2.4826)  acc1: 44.4444 (41.9192)  acc5: 77.7778 (77.2727)  time: 0.8329  data: 0.6406  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 2.6428 (2.6218)  acc1: 38.8889 (38.3598)  acc5: 72.2222 (73.2804)  time: 0.2643  data: 0.0809  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 2.5900 (2.5035)  acc1: 38.8889 (41.2186)  acc5: 77.7778 (75.8065)  time: 0.3440  data: 0.1478  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.1307 (2.4404)  acc1: 50.0000 (44.4444)  acc5: 83.3333 (77.2358)  time: 0.3539  data: 0.1509  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 2.2656 (2.4385)  acc1: 50.0000 (44.1176)  acc5: 83.3333 (78.3224)  time: 0.3699  data: 0.1680  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 2.4466 (2.5206)  acc1: 38.8889 (42.5319)  acc5: 77.7778 (76.0474)  time: 0.3878  data: 0.1742  max mem: 15572
Val:  [ 70/136]  eta: 0:00:29  loss: 2.5836 (2.5268)  acc1: 44.4444 (42.4100)  acc5: 77.7778 (76.0563)  time: 0.4084  data: 0.1888  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 2.4130 (2.5084)  acc1: 50.0000 (43.0041)  acc5: 77.7778 (76.7490)  time: 0.3952  data: 0.1936  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.4441 (2.5090)  acc1: 38.8889 (42.3077)  acc5: 77.7778 (76.9231)  time: 0.2675  data: 0.0647  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.7198 (2.5729)  acc1: 22.2222 (39.7140)  acc5: 72.2222 (75.1375)  time: 0.2705  data: 0.0727  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.7553 (2.5680)  acc1: 27.7778 (40.3904)  acc5: 72.2222 (75.0250)  time: 0.3313  data: 0.1461  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.4695 (2.5251)  acc1: 55.5556 (41.7815)  acc5: 77.7778 (75.8035)  time: 0.3519  data: 0.1563  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.0328 (2.4939)  acc1: 55.5556 (43.0874)  acc5: 83.3333 (76.4207)  time: 0.2616  data: 0.0827  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.2658 (2.5023)  acc1: 55.5556 (43.1613)  acc5: 77.7778 (76.1261)  time: 0.1770  data: 0.0174  max mem: 15572
Val: Total time: 0:00:49 (0.3652 s / it)
* Acc@1 42.527 Acc@5 74.980 loss 2.542
Accuracy of the network on the 4883 val videos: 42.5%
Max accuracy: 42.67%
Epoch: [18]  [   0/1404]  eta: 2:19:17  lr: 0.000065  min_lr: 0.000001  loss: 4.0198 (4.0198)  class_acc: 0.4583 (0.4583)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 5.9527  data: 5.4539  max mem: 15572
Epoch: [18]  [  10/1404]  eta: 0:29:17  lr: 0.000065  min_lr: 0.000001  loss: 4.2983 (4.3721)  class_acc: 0.2917 (0.3144)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2610  data: 0.6213  max mem: 15572
Epoch: [18]  [  20/1404]  eta: 0:20:41  lr: 0.000065  min_lr: 0.000001  loss: 4.3072 (4.3479)  class_acc: 0.2917 (0.3135)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.0719  max mem: 15572
Epoch: [18]  [  30/1404]  eta: 0:17:50  lr: 0.000065  min_lr: 0.000001  loss: 4.3072 (4.3381)  class_acc: 0.2917 (0.3024)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5141  data: 0.0032  max mem: 15572
[2025-01-17 00:37:48,755] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:37:48,755] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:37:48,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:37:48,758] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [  40/1404]  eta: 0:16:53  lr: 0.000065  min_lr: 0.000001  loss: 4.3003 (4.3045)  class_acc: 0.2917 (0.3140)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0783  max mem: 15572
[2025-01-17 00:37:56,143] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25317
[2025-01-17 00:37:56,144] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:37:56,144] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:37:56,189] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25317
[2025-01-17 00:37:56,190] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [18]  [  50/1404]  eta: 0:15:39  lr: 0.000065  min_lr: 0.000001  loss: 4.2506 (4.2910)  class_acc: 0.3333 (0.3137)  loss_scale: 32768.0000 (20560.3137)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0782  max mem: 15572
Epoch: [18]  [  60/1404]  eta: 0:15:15  lr: 0.000065  min_lr: 0.000001  loss: 4.2582 (4.3050)  class_acc: 0.2917 (0.3012)  loss_scale: 16384.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0709  max mem: 15572
Epoch: [18]  [  70/1404]  eta: 0:14:53  lr: 0.000065  min_lr: 0.000001  loss: 4.1802 (4.2935)  class_acc: 0.2917 (0.3063)  loss_scale: 16384.0000 (19383.8873)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1235  max mem: 15572
Epoch: [18]  [  80/1404]  eta: 0:14:29  lr: 0.000065  min_lr: 0.000001  loss: 4.1756 (4.2843)  class_acc: 0.2917 (0.2989)  loss_scale: 16384.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.0743  max mem: 15572
Epoch: [18]  [  90/1404]  eta: 0:14:04  lr: 0.000065  min_lr: 0.000001  loss: 4.2629 (4.2998)  class_acc: 0.2917 (0.2999)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0385  max mem: 15572
Epoch: [18]  [ 100/1404]  eta: 0:13:55  lr: 0.000065  min_lr: 0.000001  loss: 4.3806 (4.2946)  class_acc: 0.2917 (0.3020)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0756  max mem: 15572
Epoch: [18]  [ 110/1404]  eta: 0:13:37  lr: 0.000065  min_lr: 0.000001  loss: 4.1885 (4.2789)  class_acc: 0.3750 (0.3086)  loss_scale: 16384.0000 (18302.8468)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0844  max mem: 15572
Epoch: [18]  [ 120/1404]  eta: 0:13:37  lr: 0.000065  min_lr: 0.000001  loss: 4.1885 (4.2726)  class_acc: 0.2917 (0.3089)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.0756  max mem: 15572
Epoch: [18]  [ 130/1404]  eta: 0:13:22  lr: 0.000065  min_lr: 0.000001  loss: 4.2490 (4.2628)  class_acc: 0.2500 (0.3015)  loss_scale: 16384.0000 (18009.8931)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.0672  max mem: 15572
Epoch: [18]  [ 140/1404]  eta: 0:13:11  lr: 0.000065  min_lr: 0.000001  loss: 4.0696 (4.2440)  class_acc: 0.2500 (0.3061)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0179  max mem: 15572
Epoch: [18]  [ 150/1404]  eta: 0:13:05  lr: 0.000065  min_lr: 0.000001  loss: 4.0696 (4.2472)  class_acc: 0.3750 (0.3102)  loss_scale: 16384.0000 (17794.5430)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0338  max mem: 15572
Epoch: [18]  [ 160/1404]  eta: 0:12:56  lr: 0.000065  min_lr: 0.000001  loss: 4.3475 (4.2441)  class_acc: 0.3333 (0.3093)  loss_scale: 16384.0000 (17706.9317)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.0786  max mem: 15572
Epoch: [18]  [ 170/1404]  eta: 0:12:50  lr: 0.000065  min_lr: 0.000001  loss: 4.3212 (4.2522)  class_acc: 0.2083 (0.3036)  loss_scale: 16384.0000 (17629.5673)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0727  max mem: 15572
[2025-01-17 00:39:11,930] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:39:11,931] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:39:11,931] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:39:11,932] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [ 180/1404]  eta: 0:12:35  lr: 0.000065  min_lr: 0.000001  loss: 4.3208 (4.2468)  class_acc: 0.2500 (0.3034)  loss_scale: 16384.0000 (18194.3867)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0278  max mem: 15572
[2025-01-17 00:39:19,169] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25458
[2025-01-17 00:39:19,169] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:39:19,169] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:39:19,171] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25458
[2025-01-17 00:39:19,171] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [18]  [ 190/1404]  eta: 0:12:29  lr: 0.000065  min_lr: 0.000001  loss: 4.1853 (4.2430)  class_acc: 0.2917 (0.3061)  loss_scale: 32768.0000 (18528.5026)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0334  max mem: 15572
Epoch: [18]  [ 200/1404]  eta: 0:12:21  lr: 0.000065  min_lr: 0.000001  loss: 4.0847 (4.2380)  class_acc: 0.2917 (0.3074)  loss_scale: 16384.0000 (18421.8109)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0334  max mem: 15572
Epoch: [18]  [ 210/1404]  eta: 0:12:15  lr: 0.000065  min_lr: 0.000001  loss: 4.2905 (4.2452)  class_acc: 0.2917 (0.3039)  loss_scale: 16384.0000 (18325.2322)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.0175  max mem: 15572
Epoch: [18]  [ 220/1404]  eta: 0:12:05  lr: 0.000065  min_lr: 0.000001  loss: 4.2905 (4.2397)  class_acc: 0.2083 (0.3002)  loss_scale: 16384.0000 (18237.3937)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0175  max mem: 15572
Epoch: [18]  [ 230/1404]  eta: 0:11:59  lr: 0.000065  min_lr: 0.000001  loss: 4.1033 (4.2408)  class_acc: 0.2500 (0.3007)  loss_scale: 16384.0000 (18157.1602)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.0168  max mem: 15572
Epoch: [18]  [ 240/1404]  eta: 0:11:53  lr: 0.000065  min_lr: 0.000001  loss: 4.1541 (4.2400)  class_acc: 0.2500 (0.2986)  loss_scale: 16384.0000 (18083.5851)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.0287  max mem: 15572
Epoch: [18]  [ 250/1404]  eta: 0:11:46  lr: 0.000065  min_lr: 0.000001  loss: 4.0815 (4.2331)  class_acc: 0.2500 (0.3003)  loss_scale: 16384.0000 (18015.8725)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0195  max mem: 15572
Epoch: [18]  [ 260/1404]  eta: 0:11:38  lr: 0.000065  min_lr: 0.000001  loss: 4.0081 (4.2244)  class_acc: 0.3333 (0.3016)  loss_scale: 16384.0000 (17953.3487)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.0330  max mem: 15572
Epoch: [18]  [ 270/1404]  eta: 0:11:30  lr: 0.000065  min_lr: 0.000001  loss: 4.0628 (4.2257)  class_acc: 0.3333 (0.3004)  loss_scale: 16384.0000 (17895.4391)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.0296  max mem: 15572
Epoch: [18]  [ 280/1404]  eta: 0:11:26  lr: 0.000065  min_lr: 0.000001  loss: 4.2782 (4.2295)  class_acc: 0.2917 (0.3003)  loss_scale: 16384.0000 (17841.6512)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.0582  max mem: 15572
Epoch: [18]  [ 290/1404]  eta: 0:11:16  lr: 0.000065  min_lr: 0.000001  loss: 4.2755 (4.2248)  class_acc: 0.2917 (0.2997)  loss_scale: 16384.0000 (17791.5601)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.0547  max mem: 15572
Epoch: [18]  [ 300/1404]  eta: 0:11:06  lr: 0.000064  min_lr: 0.000001  loss: 4.1649 (4.2267)  class_acc: 0.3333 (0.3022)  loss_scale: 16384.0000 (17744.7973)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0133  max mem: 15572
Epoch: [18]  [ 310/1404]  eta: 0:11:00  lr: 0.000064  min_lr: 0.000001  loss: 4.2332 (4.2277)  class_acc: 0.3333 (0.3008)  loss_scale: 16384.0000 (17701.0418)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0687  max mem: 15572
[2025-01-17 00:40:33,987] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:40:33,987] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:40:33,988] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:40:33,988] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [ 320/1404]  eta: 0:10:52  lr: 0.000064  min_lr: 0.000001  loss: 4.2999 (4.2305)  class_acc: 0.2500 (0.3009)  loss_scale: 16384.0000 (17966.2555)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0560  max mem: 15572
Epoch: [18]  [ 330/1404]  eta: 0:10:44  lr: 0.000064  min_lr: 0.000001  loss: 4.3409 (4.2309)  class_acc: 0.3333 (0.3012)  loss_scale: 32768.0000 (18413.4381)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0261  max mem: 15572
Epoch: [18]  [ 340/1404]  eta: 0:10:38  lr: 0.000064  min_lr: 0.000001  loss: 4.3119 (4.2327)  class_acc: 0.3333 (0.3008)  loss_scale: 32768.0000 (18834.3930)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0809  max mem: 15572
Epoch: [18]  [ 350/1404]  eta: 0:10:35  lr: 0.000064  min_lr: 0.000001  loss: 4.2278 (4.2307)  class_acc: 0.2500 (0.3003)  loss_scale: 32768.0000 (19231.3618)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.1588  max mem: 15572
Epoch: [18]  [ 360/1404]  eta: 0:10:26  lr: 0.000064  min_lr: 0.000001  loss: 4.2465 (4.2308)  class_acc: 0.2083 (0.2986)  loss_scale: 32768.0000 (19606.3380)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1042  max mem: 15572
Epoch: [18]  [ 370/1404]  eta: 0:10:20  lr: 0.000064  min_lr: 0.000001  loss: 4.2465 (4.2303)  class_acc: 0.2500 (0.2992)  loss_scale: 32768.0000 (19961.0997)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0388  max mem: 15572
Epoch: [18]  [ 380/1404]  eta: 0:10:11  lr: 0.000064  min_lr: 0.000001  loss: 4.2081 (4.2283)  class_acc: 0.2500 (0.2986)  loss_scale: 32768.0000 (20297.2388)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0430  max mem: 15572
Epoch: [18]  [ 390/1404]  eta: 0:10:08  lr: 0.000064  min_lr: 0.000001  loss: 4.2190 (4.2259)  class_acc: 0.2500 (0.2987)  loss_scale: 32768.0000 (20616.1841)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0947  max mem: 15572
Epoch: [18]  [ 400/1404]  eta: 0:09:59  lr: 0.000064  min_lr: 0.000001  loss: 4.1222 (4.2228)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (20919.2219)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0966  max mem: 15572
Epoch: [18]  [ 410/1404]  eta: 0:09:54  lr: 0.000064  min_lr: 0.000001  loss: 4.1801 (4.2263)  class_acc: 0.2917 (0.2998)  loss_scale: 32768.0000 (21207.5134)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0881  max mem: 15572
Epoch: [18]  [ 420/1404]  eta: 0:09:48  lr: 0.000064  min_lr: 0.000001  loss: 4.2992 (4.2246)  class_acc: 0.3750 (0.3008)  loss_scale: 32768.0000 (21482.1093)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1338  max mem: 15572
[2025-01-17 00:41:40,770] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25702
[2025-01-17 00:41:40,771] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:41:40,771] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [18]  [ 430/1404]  eta: 0:09:41  lr: 0.000064  min_lr: 0.000001  loss: 4.0986 (4.2238)  class_acc: 0.3333 (0.3016)  loss_scale: 32768.0000 (21705.9490)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0783  max mem: 15572
[2025-01-17 00:41:40,792] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25702
[2025-01-17 00:41:40,794] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [18]  [ 440/1404]  eta: 0:09:34  lr: 0.000064  min_lr: 0.000001  loss: 4.2912 (4.2297)  class_acc: 0.2917 (0.3000)  loss_scale: 16384.0000 (21585.2698)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0670  max mem: 15572
Epoch: [18]  [ 450/1404]  eta: 0:09:28  lr: 0.000064  min_lr: 0.000001  loss: 4.2883 (4.2287)  class_acc: 0.2500 (0.3001)  loss_scale: 16384.0000 (21469.9424)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0807  max mem: 15572
Epoch: [18]  [ 460/1404]  eta: 0:09:22  lr: 0.000064  min_lr: 0.000001  loss: 4.2602 (4.2302)  class_acc: 0.2500 (0.2997)  loss_scale: 16384.0000 (21359.6182)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0926  max mem: 15572
Epoch: [18]  [ 470/1404]  eta: 0:09:17  lr: 0.000064  min_lr: 0.000001  loss: 4.3433 (4.2344)  class_acc: 0.2500 (0.2992)  loss_scale: 16384.0000 (21253.9788)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1090  max mem: 15572
Epoch: [18]  [ 480/1404]  eta: 0:09:10  lr: 0.000064  min_lr: 0.000001  loss: 4.3262 (4.2351)  class_acc: 0.2917 (0.2995)  loss_scale: 16384.0000 (21152.7318)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0854  max mem: 15572
Epoch: [18]  [ 490/1404]  eta: 0:09:04  lr: 0.000064  min_lr: 0.000001  loss: 4.3262 (4.2368)  class_acc: 0.3333 (0.3001)  loss_scale: 16384.0000 (21055.6090)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0859  max mem: 15572
Epoch: [18]  [ 500/1404]  eta: 0:08:57  lr: 0.000064  min_lr: 0.000001  loss: 4.1507 (4.2345)  class_acc: 0.2917 (0.3008)  loss_scale: 16384.0000 (20962.3633)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0702  max mem: 15572
Epoch: [18]  [ 510/1404]  eta: 0:08:51  lr: 0.000064  min_lr: 0.000001  loss: 4.2867 (4.2361)  class_acc: 0.2917 (0.3019)  loss_scale: 16384.0000 (20872.7671)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0561  max mem: 15572
Epoch: [18]  [ 520/1404]  eta: 0:08:47  lr: 0.000064  min_lr: 0.000001  loss: 4.3213 (4.2369)  class_acc: 0.2917 (0.3019)  loss_scale: 16384.0000 (20786.6104)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.1369  max mem: 15572
Epoch: [18]  [ 530/1404]  eta: 0:08:39  lr: 0.000064  min_lr: 0.000001  loss: 4.2453 (4.2380)  class_acc: 0.2917 (0.3015)  loss_scale: 16384.0000 (20703.6987)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1005  max mem: 15572
Epoch: [18]  [ 540/1404]  eta: 0:08:33  lr: 0.000064  min_lr: 0.000001  loss: 4.2038 (4.2368)  class_acc: 0.2500 (0.3014)  loss_scale: 16384.0000 (20623.8521)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0314  max mem: 15572
Epoch: [18]  [ 550/1404]  eta: 0:08:27  lr: 0.000064  min_lr: 0.000001  loss: 4.2038 (4.2365)  class_acc: 0.3333 (0.3031)  loss_scale: 16384.0000 (20546.9038)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.0861  max mem: 15572
[2025-01-17 00:42:55,695] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:42:55,695] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:42:55,697] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:42:55,698] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [ 560/1404]  eta: 0:08:20  lr: 0.000064  min_lr: 0.000001  loss: 4.3125 (4.2389)  class_acc: 0.3750 (0.3038)  loss_scale: 16384.0000 (20531.1087)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0611  max mem: 15572
Epoch: [18]  [ 570/1404]  eta: 0:08:13  lr: 0.000064  min_lr: 0.000001  loss: 4.1522 (4.2365)  class_acc: 0.2917 (0.3036)  loss_scale: 32768.0000 (20745.4151)  weight_decay: 0.0500 (0.0500)  time: 0.5188  data: 0.0092  max mem: 15572
Epoch: [18]  [ 580/1404]  eta: 0:08:07  lr: 0.000064  min_lr: 0.000001  loss: 4.1874 (4.2367)  class_acc: 0.2917 (0.3041)  loss_scale: 32768.0000 (20952.3442)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0541  max mem: 15572
Epoch: [18]  [ 590/1404]  eta: 0:08:02  lr: 0.000064  min_lr: 0.000001  loss: 4.2381 (4.2372)  class_acc: 0.2917 (0.3041)  loss_scale: 32768.0000 (21152.2707)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1140  max mem: 15572
Epoch: [18]  [ 600/1404]  eta: 0:07:55  lr: 0.000064  min_lr: 0.000001  loss: 4.1955 (4.2378)  class_acc: 0.1667 (0.3029)  loss_scale: 32768.0000 (21345.5441)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0945  max mem: 15572
Epoch: [18]  [ 610/1404]  eta: 0:07:50  lr: 0.000064  min_lr: 0.000001  loss: 4.2490 (4.2375)  class_acc: 0.2917 (0.3031)  loss_scale: 32768.0000 (21532.4910)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1171  max mem: 15572
[2025-01-17 00:43:27,268] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25884
[2025-01-17 00:43:27,269] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:43:27,269] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:43:27,316] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25884
[2025-01-17 00:43:27,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [18]  [ 620/1404]  eta: 0:07:44  lr: 0.000064  min_lr: 0.000001  loss: 4.0943 (4.2345)  class_acc: 0.3333 (0.3042)  loss_scale: 32768.0000 (21475.9678)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1394  max mem: 15572
Epoch: [18]  [ 630/1404]  eta: 0:07:40  lr: 0.000064  min_lr: 0.000001  loss: 3.9994 (4.2333)  class_acc: 0.3333 (0.3046)  loss_scale: 16384.0000 (21395.2710)  weight_decay: 0.0500 (0.0500)  time: 0.6630  data: 0.1674  max mem: 15572
Epoch: [18]  [ 640/1404]  eta: 0:07:35  lr: 0.000064  min_lr: 0.000001  loss: 4.0695 (4.2320)  class_acc: 0.2917 (0.3043)  loss_scale: 16384.0000 (21317.0920)  weight_decay: 0.0500 (0.0500)  time: 0.6913  data: 0.2035  max mem: 15572
Epoch: [18]  [ 650/1404]  eta: 0:07:27  lr: 0.000064  min_lr: 0.000001  loss: 4.1279 (4.2305)  class_acc: 0.2500 (0.3044)  loss_scale: 16384.0000 (21241.3149)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0845  max mem: 15572
Epoch: [18]  [ 660/1404]  eta: 0:07:22  lr: 0.000063  min_lr: 0.000001  loss: 4.1503 (4.2300)  class_acc: 0.2917 (0.3047)  loss_scale: 16384.0000 (21167.8306)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.0836  max mem: 15572
Epoch: [18]  [ 670/1404]  eta: 0:07:16  lr: 0.000063  min_lr: 0.000001  loss: 4.2847 (4.2314)  class_acc: 0.2500 (0.3035)  loss_scale: 16384.0000 (21096.5365)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1413  max mem: 15572
Epoch: [18]  [ 680/1404]  eta: 0:07:09  lr: 0.000063  min_lr: 0.000001  loss: 4.2099 (4.2302)  class_acc: 0.2917 (0.3035)  loss_scale: 16384.0000 (21027.3363)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0582  max mem: 15572
Epoch: [18]  [ 690/1404]  eta: 0:07:03  lr: 0.000063  min_lr: 0.000001  loss: 4.1473 (4.2309)  class_acc: 0.2500 (0.3028)  loss_scale: 16384.0000 (20960.1389)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0264  max mem: 15572
Epoch: [18]  [ 700/1404]  eta: 0:06:56  lr: 0.000063  min_lr: 0.000001  loss: 4.3030 (4.2309)  class_acc: 0.2083 (0.3033)  loss_scale: 16384.0000 (20894.8588)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0450  max mem: 15572
Epoch: [18]  [ 710/1404]  eta: 0:06:50  lr: 0.000063  min_lr: 0.000001  loss: 4.1884 (4.2298)  class_acc: 0.3333 (0.3044)  loss_scale: 16384.0000 (20831.4149)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0375  max mem: 15572
Epoch: [18]  [ 720/1404]  eta: 0:06:44  lr: 0.000063  min_lr: 0.000001  loss: 4.1986 (4.2305)  class_acc: 0.3333 (0.3044)  loss_scale: 16384.0000 (20769.7309)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0811  max mem: 15572
[2025-01-17 00:44:35,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=155, lr=[6.133656265189632e-07, 6.133656265189632e-07, 8.762366093128046e-07, 8.762366093128046e-07, 1.2517665847325782e-06, 1.2517665847325782e-06, 1.7882379781893974e-06, 1.7882379781893974e-06, 2.5546256831277107e-06, 2.5546256831277107e-06, 3.6494652616110157e-06, 3.6494652616110157e-06, 5.213521802301451e-06, 5.213521802301451e-06, 7.447888289002074e-06, 7.447888289002074e-06, 1.0639840412860105e-05, 1.0639840412860105e-05, 1.5199772018371582e-05, 1.5199772018371582e-05, 2.1713960026245116e-05, 2.1713960026245116e-05, 3.101994289463588e-05, 3.101994289463588e-05, 4.431420413519412e-05, 4.431420413519412e-05, 6.330600590742018e-05, 6.330600590742018e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 00:44:35,157] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=47.979756072440246, CurrSamplesPerSec=52.725407880477626, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [18]  [ 730/1404]  eta: 0:06:39  lr: 0.000063  min_lr: 0.000001  loss: 4.2184 (4.2274)  class_acc: 0.3750 (0.3057)  loss_scale: 16384.0000 (20709.7346)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.1367  max mem: 15572
Epoch: [18]  [ 740/1404]  eta: 0:06:32  lr: 0.000063  min_lr: 0.000001  loss: 4.2821 (4.2309)  class_acc: 0.3333 (0.3052)  loss_scale: 16384.0000 (20651.3576)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0923  max mem: 15572
[2025-01-17 00:44:42,449] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:44:42,449] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:44:42,468] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:44:42,468] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [ 750/1404]  eta: 0:06:26  lr: 0.000063  min_lr: 0.000001  loss: 4.3715 (4.2306)  class_acc: 0.2917 (0.3055)  loss_scale: 16384.0000 (20812.6977)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0725  max mem: 15572
Epoch: [18]  [ 760/1404]  eta: 0:06:21  lr: 0.000063  min_lr: 0.000001  loss: 4.2319 (4.2320)  class_acc: 0.2917 (0.3053)  loss_scale: 32768.0000 (20969.7976)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1092  max mem: 15572
Epoch: [18]  [ 770/1404]  eta: 0:06:14  lr: 0.000063  min_lr: 0.000001  loss: 4.2319 (4.2307)  class_acc: 0.2917 (0.3059)  loss_scale: 32768.0000 (21122.8223)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0644  max mem: 15572
Epoch: [18]  [ 780/1404]  eta: 0:06:09  lr: 0.000063  min_lr: 0.000001  loss: 4.2202 (4.2318)  class_acc: 0.2917 (0.3065)  loss_scale: 32768.0000 (21271.9283)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0628  max mem: 15572
Epoch: [18]  [ 790/1404]  eta: 0:06:02  lr: 0.000063  min_lr: 0.000001  loss: 4.2863 (4.2320)  class_acc: 0.2917 (0.3057)  loss_scale: 32768.0000 (21417.2642)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0533  max mem: 15572
Epoch: [18]  [ 800/1404]  eta: 0:05:56  lr: 0.000063  min_lr: 0.000001  loss: 4.3595 (4.2322)  class_acc: 0.3333 (0.3059)  loss_scale: 32768.0000 (21558.9713)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0007  max mem: 15572
[2025-01-17 00:45:17,854] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26074
[2025-01-17 00:45:17,855] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:45:17,855] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:45:17,898] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26074
[2025-01-17 00:45:17,898] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [18]  [ 810/1404]  eta: 0:05:50  lr: 0.000063  min_lr: 0.000001  loss: 4.2278 (4.2314)  class_acc: 0.3333 (0.3061)  loss_scale: 32768.0000 (21515.3637)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0260  max mem: 15572
Epoch: [18]  [ 820/1404]  eta: 0:05:44  lr: 0.000063  min_lr: 0.000001  loss: 4.2278 (4.2319)  class_acc: 0.3333 (0.3059)  loss_scale: 16384.0000 (21452.8624)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0393  max mem: 15572
Epoch: [18]  [ 830/1404]  eta: 0:05:38  lr: 0.000063  min_lr: 0.000001  loss: 4.3006 (4.2315)  class_acc: 0.2917 (0.3059)  loss_scale: 16384.0000 (21391.8652)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0332  max mem: 15572
Epoch: [18]  [ 840/1404]  eta: 0:05:33  lr: 0.000063  min_lr: 0.000001  loss: 4.1783 (4.2304)  class_acc: 0.2917 (0.3057)  loss_scale: 16384.0000 (21332.3187)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0300  max mem: 15572
Epoch: [18]  [ 850/1404]  eta: 0:05:26  lr: 0.000063  min_lr: 0.000001  loss: 4.2166 (4.2313)  class_acc: 0.3333 (0.3059)  loss_scale: 16384.0000 (21274.1716)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0257  max mem: 15572
Epoch: [18]  [ 860/1404]  eta: 0:05:20  lr: 0.000063  min_lr: 0.000001  loss: 4.3206 (4.2316)  class_acc: 0.3333 (0.3060)  loss_scale: 16384.0000 (21217.3751)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0342  max mem: 15572
Epoch: [18]  [ 870/1404]  eta: 0:05:15  lr: 0.000063  min_lr: 0.000001  loss: 4.1454 (4.2305)  class_acc: 0.2917 (0.3058)  loss_scale: 16384.0000 (21161.8829)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0545  max mem: 15572
Epoch: [18]  [ 880/1404]  eta: 0:05:09  lr: 0.000063  min_lr: 0.000001  loss: 4.2156 (4.2308)  class_acc: 0.2917 (0.3056)  loss_scale: 16384.0000 (21107.6504)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0674  max mem: 15572
Epoch: [18]  [ 890/1404]  eta: 0:05:03  lr: 0.000063  min_lr: 0.000001  loss: 4.2263 (4.2289)  class_acc: 0.2917 (0.3055)  loss_scale: 16384.0000 (21054.6352)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0987  max mem: 15572
Epoch: [18]  [ 900/1404]  eta: 0:04:56  lr: 0.000063  min_lr: 0.000001  loss: 4.2351 (4.2304)  class_acc: 0.2917 (0.3051)  loss_scale: 16384.0000 (21002.7969)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0672  max mem: 15572
Epoch: [18]  [ 910/1404]  eta: 0:04:51  lr: 0.000063  min_lr: 0.000001  loss: 4.2697 (4.2307)  class_acc: 0.2083 (0.3048)  loss_scale: 16384.0000 (20952.0966)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0308  max mem: 15572
Epoch: [18]  [ 920/1404]  eta: 0:04:45  lr: 0.000063  min_lr: 0.000001  loss: 4.2274 (4.2317)  class_acc: 0.2083 (0.3042)  loss_scale: 16384.0000 (20902.4973)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0785  max mem: 15572
Epoch: [18]  [ 930/1404]  eta: 0:04:39  lr: 0.000063  min_lr: 0.000001  loss: 4.3303 (4.2331)  class_acc: 0.2083 (0.3039)  loss_scale: 16384.0000 (20853.9635)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1304  max mem: 15572
[2025-01-17 00:46:33,827] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:46:33,827] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:46:33,830] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:46:33,831] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [ 940/1404]  eta: 0:04:33  lr: 0.000063  min_lr: 0.000001  loss: 4.2339 (4.2326)  class_acc: 0.2500 (0.3039)  loss_scale: 16384.0000 (20980.5739)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1215  max mem: 15572
Epoch: [18]  [ 950/1404]  eta: 0:04:28  lr: 0.000063  min_lr: 0.000001  loss: 4.2710 (4.2333)  class_acc: 0.2500 (0.3033)  loss_scale: 32768.0000 (21104.5216)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1195  max mem: 15572
Epoch: [18]  [ 960/1404]  eta: 0:04:22  lr: 0.000063  min_lr: 0.000001  loss: 4.2954 (4.2319)  class_acc: 0.2500 (0.3031)  loss_scale: 32768.0000 (21225.8897)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1239  max mem: 15572
Epoch: [18]  [ 970/1404]  eta: 0:04:15  lr: 0.000063  min_lr: 0.000001  loss: 4.2125 (4.2323)  class_acc: 0.2917 (0.3033)  loss_scale: 32768.0000 (21344.7580)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0439  max mem: 15572
Epoch: [18]  [ 980/1404]  eta: 0:04:09  lr: 0.000063  min_lr: 0.000001  loss: 4.1922 (4.2321)  class_acc: 0.2917 (0.3033)  loss_scale: 32768.0000 (21461.2029)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0340  max mem: 15572
Epoch: [18]  [ 990/1404]  eta: 0:04:04  lr: 0.000063  min_lr: 0.000001  loss: 4.2481 (4.2335)  class_acc: 0.2500 (0.3026)  loss_scale: 32768.0000 (21575.2977)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1249  max mem: 15572
Epoch: [18]  [1000/1404]  eta: 0:03:58  lr: 0.000063  min_lr: 0.000001  loss: 4.3451 (4.2341)  class_acc: 0.2500 (0.3027)  loss_scale: 32768.0000 (21687.1129)  weight_decay: 0.0500 (0.0500)  time: 0.6471  data: 0.1741  max mem: 15572
[2025-01-17 00:47:16,856] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26275
[2025-01-17 00:47:16,856] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:47:16,856] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:47:16,894] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26275
[2025-01-17 00:47:16,894] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [18]  [1010/1404]  eta: 0:03:52  lr: 0.000063  min_lr: 0.000001  loss: 4.2549 (4.2346)  class_acc: 0.2500 (0.3019)  loss_scale: 32768.0000 (21667.0702)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1130  max mem: 15572
Epoch: [18]  [1020/1404]  eta: 0:03:46  lr: 0.000062  min_lr: 0.000001  loss: 4.2392 (4.2362)  class_acc: 0.2083 (0.3012)  loss_scale: 16384.0000 (21615.3262)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0516  max mem: 15572
Epoch: [18]  [1030/1404]  eta: 0:03:40  lr: 0.000062  min_lr: 0.000001  loss: 4.2397 (4.2363)  class_acc: 0.2500 (0.3011)  loss_scale: 16384.0000 (21564.5858)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0548  max mem: 15572
Epoch: [18]  [1040/1404]  eta: 0:03:34  lr: 0.000062  min_lr: 0.000001  loss: 4.2272 (4.2351)  class_acc: 0.2917 (0.3016)  loss_scale: 16384.0000 (21514.8204)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0927  max mem: 15572
Epoch: [18]  [1050/1404]  eta: 0:03:28  lr: 0.000062  min_lr: 0.000001  loss: 4.2919 (4.2362)  class_acc: 0.3333 (0.3014)  loss_scale: 16384.0000 (21466.0019)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1048  max mem: 15572
Epoch: [18]  [1060/1404]  eta: 0:03:22  lr: 0.000062  min_lr: 0.000001  loss: 4.3168 (4.2367)  class_acc: 0.2917 (0.3011)  loss_scale: 16384.0000 (21418.1037)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0634  max mem: 15572
Epoch: [18]  [1070/1404]  eta: 0:03:16  lr: 0.000062  min_lr: 0.000001  loss: 4.2332 (4.2357)  class_acc: 0.2917 (0.3009)  loss_scale: 16384.0000 (21371.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5132  data: 0.0227  max mem: 15572
Epoch: [18]  [1080/1404]  eta: 0:03:10  lr: 0.000062  min_lr: 0.000001  loss: 4.2063 (4.2359)  class_acc: 0.2917 (0.3009)  loss_scale: 16384.0000 (21324.9658)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0680  max mem: 15572
Epoch: [18]  [1090/1404]  eta: 0:03:04  lr: 0.000062  min_lr: 0.000001  loss: 4.3532 (4.2378)  class_acc: 0.2500 (0.3006)  loss_scale: 16384.0000 (21279.6774)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0740  max mem: 15572
Epoch: [18]  [1100/1404]  eta: 0:02:58  lr: 0.000062  min_lr: 0.000001  loss: 4.2900 (4.2369)  class_acc: 0.2500 (0.3005)  loss_scale: 16384.0000 (21235.2116)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0841  max mem: 15572
Epoch: [18]  [1110/1404]  eta: 0:02:52  lr: 0.000062  min_lr: 0.000001  loss: 4.2671 (4.2376)  class_acc: 0.2500 (0.3001)  loss_scale: 16384.0000 (21191.5464)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0735  max mem: 15572
Epoch: [18]  [1120/1404]  eta: 0:02:46  lr: 0.000062  min_lr: 0.000001  loss: 4.4230 (4.2386)  class_acc: 0.2917 (0.3005)  loss_scale: 16384.0000 (21148.6601)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0789  max mem: 15572
Epoch: [18]  [1130/1404]  eta: 0:02:41  lr: 0.000062  min_lr: 0.000001  loss: 4.2665 (4.2381)  class_acc: 0.2917 (0.3005)  loss_scale: 16384.0000 (21106.5323)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1529  max mem: 15572
[2025-01-17 00:48:30,553] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:48:30,553] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:48:30,553] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:48:30,553] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:48:34,670] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26412
[2025-01-17 00:48:34,670] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:48:34,689] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26412
[2025-01-17 00:48:34,689] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:48:34,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [18]  [1140/1404]  eta: 0:02:35  lr: 0.000062  min_lr: 0.000001  loss: 4.2301 (4.2390)  class_acc: 0.2917 (0.3002)  loss_scale: 16384.0000 (21180.0175)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1127  max mem: 15572
Epoch: [18]  [1150/1404]  eta: 0:02:29  lr: 0.000062  min_lr: 0.000001  loss: 4.2786 (4.2392)  class_acc: 0.2083 (0.2999)  loss_scale: 16384.0000 (21138.3493)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0747  max mem: 15572
Epoch: [18]  [1160/1404]  eta: 0:02:23  lr: 0.000062  min_lr: 0.000001  loss: 4.2786 (4.2394)  class_acc: 0.2500 (0.2996)  loss_scale: 16384.0000 (21097.3988)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0751  max mem: 15572
Epoch: [18]  [1170/1404]  eta: 0:02:17  lr: 0.000062  min_lr: 0.000001  loss: 4.1870 (4.2383)  class_acc: 0.2500 (0.2996)  loss_scale: 16384.0000 (21057.1477)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1090  max mem: 15572
Epoch: [18]  [1180/1404]  eta: 0:02:11  lr: 0.000062  min_lr: 0.000001  loss: 4.1892 (4.2383)  class_acc: 0.2917 (0.2997)  loss_scale: 16384.0000 (21017.5783)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0705  max mem: 15572
Epoch: [18]  [1190/1404]  eta: 0:02:05  lr: 0.000062  min_lr: 0.000001  loss: 4.2610 (4.2392)  class_acc: 0.3333 (0.2997)  loss_scale: 16384.0000 (20978.6734)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0129  max mem: 15572
Epoch: [18]  [1200/1404]  eta: 0:01:59  lr: 0.000062  min_lr: 0.000001  loss: 4.1901 (4.2387)  class_acc: 0.3333 (0.3003)  loss_scale: 16384.0000 (20940.4163)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0563  max mem: 15572
Epoch: [18]  [1210/1404]  eta: 0:01:54  lr: 0.000062  min_lr: 0.000001  loss: 4.0874 (4.2378)  class_acc: 0.3750 (0.3009)  loss_scale: 16384.0000 (20902.7911)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0589  max mem: 15572
Epoch: [18]  [1220/1404]  eta: 0:01:48  lr: 0.000062  min_lr: 0.000001  loss: 4.2850 (4.2384)  class_acc: 0.2917 (0.3007)  loss_scale: 16384.0000 (20865.7821)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0156  max mem: 15572
Epoch: [18]  [1230/1404]  eta: 0:01:42  lr: 0.000062  min_lr: 0.000001  loss: 4.1902 (4.2379)  class_acc: 0.2917 (0.3009)  loss_scale: 16384.0000 (20829.3745)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0436  max mem: 15572
Epoch: [18]  [1240/1404]  eta: 0:01:36  lr: 0.000062  min_lr: 0.000001  loss: 4.1739 (4.2371)  class_acc: 0.3333 (0.3009)  loss_scale: 16384.0000 (20793.5536)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1563  max mem: 15572
Epoch: [18]  [1250/1404]  eta: 0:01:30  lr: 0.000062  min_lr: 0.000001  loss: 4.1930 (4.2362)  class_acc: 0.2917 (0.3009)  loss_scale: 16384.0000 (20758.3054)  weight_decay: 0.0500 (0.0500)  time: 0.6710  data: 0.1828  max mem: 15572
Epoch: [18]  [1260/1404]  eta: 0:01:24  lr: 0.000062  min_lr: 0.000001  loss: 4.1883 (4.2360)  class_acc: 0.3333 (0.3013)  loss_scale: 16384.0000 (20723.6162)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1097  max mem: 15572
[2025-01-17 00:49:50,932] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:49:50,933] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:49:50,945] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:49:50,946] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [1270/1404]  eta: 0:01:18  lr: 0.000062  min_lr: 0.000001  loss: 4.2805 (4.2376)  class_acc: 0.2500 (0.3007)  loss_scale: 16384.0000 (20715.2541)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0406  max mem: 15572
Epoch: [18]  [1280/1404]  eta: 0:01:12  lr: 0.000062  min_lr: 0.000001  loss: 4.3281 (4.2383)  class_acc: 0.2500 (0.3004)  loss_scale: 32768.0000 (20809.3427)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0416  max mem: 15572
Epoch: [18]  [1290/1404]  eta: 0:01:07  lr: 0.000062  min_lr: 0.000001  loss: 4.2494 (4.2391)  class_acc: 0.2917 (0.3003)  loss_scale: 32768.0000 (20901.9737)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1101  max mem: 15572
Epoch: [18]  [1300/1404]  eta: 0:01:01  lr: 0.000062  min_lr: 0.000001  loss: 4.4076 (4.2400)  class_acc: 0.2500 (0.3003)  loss_scale: 32768.0000 (20993.1806)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0993  max mem: 15572
Epoch: [18]  [1310/1404]  eta: 0:00:55  lr: 0.000062  min_lr: 0.000001  loss: 4.2570 (4.2404)  class_acc: 0.2500 (0.3003)  loss_scale: 32768.0000 (21082.9962)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1214  max mem: 15572
Epoch: [18]  [1320/1404]  eta: 0:00:49  lr: 0.000062  min_lr: 0.000001  loss: 4.4099 (4.2418)  class_acc: 0.2500 (0.3003)  loss_scale: 32768.0000 (21171.4519)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1303  max mem: 15572
Epoch: [18]  [1330/1404]  eta: 0:00:43  lr: 0.000062  min_lr: 0.000001  loss: 4.3023 (4.2414)  class_acc: 0.2917 (0.3007)  loss_scale: 32768.0000 (21258.5785)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0807  max mem: 15572
Epoch: [18]  [1340/1404]  eta: 0:00:37  lr: 0.000062  min_lr: 0.000001  loss: 4.1372 (4.2409)  class_acc: 0.2917 (0.3009)  loss_scale: 32768.0000 (21344.4057)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0735  max mem: 15572
Epoch: [18]  [1350/1404]  eta: 0:00:31  lr: 0.000062  min_lr: 0.000001  loss: 4.1059 (4.2403)  class_acc: 0.3750 (0.3014)  loss_scale: 32768.0000 (21428.9623)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0323  max mem: 15572
Epoch: [18]  [1360/1404]  eta: 0:00:25  lr: 0.000062  min_lr: 0.000001  loss: 4.2251 (4.2409)  class_acc: 0.3333 (0.3013)  loss_scale: 32768.0000 (21512.2763)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0621  max mem: 15572
[2025-01-17 00:50:48,830] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26640
[2025-01-17 00:50:48,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:50:48,830] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:50:48,836] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26640
[2025-01-17 00:50:48,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [18]  [1370/1404]  eta: 0:00:19  lr: 0.000061  min_lr: 0.000001  loss: 4.1503 (4.2398)  class_acc: 0.2917 (0.3015)  loss_scale: 32768.0000 (21558.5237)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0685  max mem: 15572
Epoch: [18]  [1380/1404]  eta: 0:00:14  lr: 0.000061  min_lr: 0.000001  loss: 4.1194 (4.2398)  class_acc: 0.2500 (0.3014)  loss_scale: 16384.0000 (21521.0543)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0747  max mem: 15572
Epoch: [18]  [1390/1404]  eta: 0:00:08  lr: 0.000061  min_lr: 0.000001  loss: 4.3122 (4.2406)  class_acc: 0.2917 (0.3013)  loss_scale: 16384.0000 (21484.1237)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0929  max mem: 15572
Epoch: [18]  [1400/1404]  eta: 0:00:02  lr: 0.000061  min_lr: 0.000001  loss: 4.3270 (4.2416)  class_acc: 0.2500 (0.3010)  loss_scale: 16384.0000 (21447.7202)  weight_decay: 0.0500 (0.0500)  time: 0.4514  data: 0.0250  max mem: 15572
Epoch: [18]  [1403/1404]  eta: 0:00:00  lr: 0.000061  min_lr: 0.000001  loss: 4.3444 (4.2417)  class_acc: 0.2500 (0.3008)  loss_scale: 16384.0000 (21436.9003)  weight_decay: 0.0500 (0.0500)  time: 0.4078  data: 0.0006  max mem: 15572
Epoch: [18] Total time: 0:13:43 (0.5863 s / it)
Averaged stats: lr: 0.000061  min_lr: 0.000001  loss: 4.3444 (4.2438)  class_acc: 0.2500 (0.2965)  loss_scale: 16384.0000 (21436.9003)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:12:17  loss: 1.8779 (1.8779)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.4206  data: 5.1997  max mem: 15572
Val:  [ 10/136]  eta: 0:01:25  loss: 2.4671 (2.4642)  acc1: 50.0000 (44.9495)  acc5: 77.7778 (74.2424)  time: 0.6788  data: 0.4734  max mem: 15572
Val:  [ 20/136]  eta: 0:01:04  loss: 2.5993 (2.5798)  acc1: 38.8889 (40.4762)  acc5: 72.2222 (71.9577)  time: 0.3094  data: 0.1091  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 2.4019 (2.4724)  acc1: 38.8889 (44.2652)  acc5: 72.2222 (74.5520)  time: 0.4091  data: 0.2168  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.2190 (2.4302)  acc1: 50.0000 (46.7480)  acc5: 83.3333 (75.8808)  time: 0.3630  data: 0.1602  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.5164 (2.4692)  acc1: 44.4444 (45.3159)  acc5: 77.7778 (75.4902)  time: 0.3159  data: 0.1111  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.5418 (2.5199)  acc1: 38.8889 (44.2623)  acc5: 72.2222 (74.8634)  time: 0.3765  data: 0.1572  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.5120 (2.5008)  acc1: 44.4444 (45.3052)  acc5: 72.2222 (75.1956)  time: 0.3604  data: 0.1329  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.3640 (2.4976)  acc1: 44.4444 (44.7874)  acc5: 77.7778 (75.7888)  time: 0.3395  data: 0.1292  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.5172 (2.5115)  acc1: 38.8889 (43.4676)  acc5: 72.2222 (75.3358)  time: 0.3574  data: 0.1590  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.7949 (2.5632)  acc1: 27.7778 (41.8042)  acc5: 66.6667 (73.7074)  time: 0.3388  data: 0.1487  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.7631 (2.5628)  acc1: 33.3333 (41.6917)  acc5: 66.6667 (73.7738)  time: 0.3772  data: 0.1875  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.2139 (2.5106)  acc1: 44.4444 (43.2966)  acc5: 83.3333 (74.9770)  time: 0.3643  data: 0.1803  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.0147 (2.4792)  acc1: 55.5556 (44.2748)  acc5: 88.8889 (75.6997)  time: 0.2656  data: 0.1015  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.2139 (2.4842)  acc1: 50.0000 (44.2670)  acc5: 83.3333 (75.5528)  time: 0.1678  data: 0.0244  max mem: 15572
Val: Total time: 0:00:49 (0.3675 s / it)
* Acc@1 43.264 Acc@5 74.795 loss 2.525
Accuracy of the network on the 4883 val videos: 43.3%
[2025-01-17 00:51:56,321] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 00:51:56,323] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 00:51:56,323] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 00:51:56,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 00:51:58,789] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 00:51:58,789] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 43.26%
Epoch: [19]  [   0/1404]  eta: 3:04:43  lr: 0.000061  min_lr: 0.000001  loss: 4.2936 (4.2936)  class_acc: 0.2500 (0.2500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.8942  data: 7.4551  max mem: 15572
Epoch: [19]  [  10/1404]  eta: 0:28:00  lr: 0.000061  min_lr: 0.000001  loss: 4.2867 (4.2023)  class_acc: 0.2500 (0.2727)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2054  data: 0.7110  max mem: 15572
Epoch: [19]  [  20/1404]  eta: 0:19:52  lr: 0.000061  min_lr: 0.000001  loss: 4.2678 (4.2492)  class_acc: 0.2500 (0.2718)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5099  data: 0.0185  max mem: 15572
Epoch: [19]  [  30/1404]  eta: 0:17:22  lr: 0.000061  min_lr: 0.000001  loss: 4.2496 (4.2255)  class_acc: 0.2500 (0.2661)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0373  max mem: 15572
Epoch: [19]  [  40/1404]  eta: 0:16:14  lr: 0.000061  min_lr: 0.000001  loss: 4.2560 (4.2283)  class_acc: 0.3333 (0.2846)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0648  max mem: 15572
Epoch: [19]  [  50/1404]  eta: 0:15:44  lr: 0.000061  min_lr: 0.000001  loss: 4.2651 (4.2373)  class_acc: 0.3333 (0.2794)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0281  max mem: 15572
Epoch: [19]  [  60/1404]  eta: 0:15:19  lr: 0.000061  min_lr: 0.000001  loss: 4.2342 (4.2191)  class_acc: 0.2500 (0.2807)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.0007  max mem: 15572
Epoch: [19]  [  70/1404]  eta: 0:15:02  lr: 0.000061  min_lr: 0.000001  loss: 4.2728 (4.2316)  class_acc: 0.2917 (0.2876)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.0008  max mem: 15572
Epoch: [19]  [  80/1404]  eta: 0:14:40  lr: 0.000061  min_lr: 0.000001  loss: 4.2126 (4.2091)  class_acc: 0.3333 (0.2953)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0008  max mem: 15572
Epoch: [19]  [  90/1404]  eta: 0:14:17  lr: 0.000061  min_lr: 0.000001  loss: 4.0592 (4.1984)  class_acc: 0.3333 (0.3036)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0007  max mem: 15572
[2025-01-17 00:53:00,352] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:53:00,353] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:53:00,354] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:53:00,354] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [ 100/1404]  eta: 0:14:00  lr: 0.000061  min_lr: 0.000001  loss: 4.1499 (4.2084)  class_acc: 0.3333 (0.3012)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0006  max mem: 15572
[2025-01-17 00:53:07,667] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26782
[2025-01-17 00:53:07,667] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:53:07,667] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:53:07,668] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26782
[2025-01-17 00:53:07,669] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [19]  [ 110/1404]  eta: 0:13:45  lr: 0.000061  min_lr: 0.000001  loss: 4.2876 (4.2232)  class_acc: 0.2917 (0.2999)  loss_scale: 32768.0000 (18302.8468)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0004  max mem: 15572
Epoch: [19]  [ 120/1404]  eta: 0:13:40  lr: 0.000061  min_lr: 0.000001  loss: 4.2876 (4.2173)  class_acc: 0.2917 (0.2975)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.0134  max mem: 15572
Epoch: [19]  [ 130/1404]  eta: 0:13:32  lr: 0.000061  min_lr: 0.000001  loss: 4.2612 (4.2233)  class_acc: 0.2917 (0.3009)  loss_scale: 16384.0000 (18009.8931)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.0134  max mem: 15572
Epoch: [19]  [ 140/1404]  eta: 0:13:17  lr: 0.000061  min_lr: 0.000001  loss: 4.1673 (4.2141)  class_acc: 0.3333 (0.3038)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0007  max mem: 15572
Epoch: [19]  [ 150/1404]  eta: 0:13:06  lr: 0.000061  min_lr: 0.000001  loss: 4.3128 (4.2285)  class_acc: 0.2500 (0.2999)  loss_scale: 16384.0000 (17794.5430)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0008  max mem: 15572
Epoch: [19]  [ 160/1404]  eta: 0:13:02  lr: 0.000061  min_lr: 0.000001  loss: 4.2575 (4.2160)  class_acc: 0.2500 (0.3028)  loss_scale: 16384.0000 (17706.9317)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0007  max mem: 15572
Epoch: [19]  [ 170/1404]  eta: 0:12:55  lr: 0.000061  min_lr: 0.000001  loss: 4.1251 (4.2129)  class_acc: 0.2917 (0.3038)  loss_scale: 16384.0000 (17629.5673)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.0006  max mem: 15572
Epoch: [19]  [ 180/1404]  eta: 0:12:39  lr: 0.000061  min_lr: 0.000001  loss: 4.2643 (4.2140)  class_acc: 0.2917 (0.3016)  loss_scale: 16384.0000 (17560.7514)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0008  max mem: 15572
Epoch: [19]  [ 190/1404]  eta: 0:12:34  lr: 0.000061  min_lr: 0.000001  loss: 4.2475 (4.2189)  class_acc: 0.2500 (0.2971)  loss_scale: 16384.0000 (17499.1414)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0007  max mem: 15572
Epoch: [19]  [ 200/1404]  eta: 0:12:27  lr: 0.000061  min_lr: 0.000001  loss: 4.2413 (4.2143)  class_acc: 0.2500 (0.2975)  loss_scale: 16384.0000 (17443.6617)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.0008  max mem: 15572
Epoch: [19]  [ 210/1404]  eta: 0:12:16  lr: 0.000061  min_lr: 0.000001  loss: 4.2413 (4.2155)  class_acc: 0.2500 (0.2964)  loss_scale: 16384.0000 (17393.4408)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0008  max mem: 15572
Epoch: [19]  [ 220/1404]  eta: 0:12:09  lr: 0.000061  min_lr: 0.000001  loss: 4.2966 (4.2146)  class_acc: 0.2917 (0.2981)  loss_scale: 16384.0000 (17347.7647)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0006  max mem: 15572
Epoch: [19]  [ 230/1404]  eta: 0:12:00  lr: 0.000061  min_lr: 0.000001  loss: 4.2705 (4.2186)  class_acc: 0.2500 (0.2949)  loss_scale: 16384.0000 (17306.0433)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0006  max mem: 15572
[2025-01-17 00:54:23,841] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:54:23,841] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:54:23,850] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:54:23,851] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [ 240/1404]  eta: 0:11:51  lr: 0.000061  min_lr: 0.000001  loss: 4.1296 (4.2110)  class_acc: 0.2917 (0.3000)  loss_scale: 16384.0000 (17675.6846)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.0007  max mem: 15572
Epoch: [19]  [ 250/1404]  eta: 0:11:46  lr: 0.000061  min_lr: 0.000001  loss: 4.1400 (4.2181)  class_acc: 0.3333 (0.2988)  loss_scale: 32768.0000 (18276.9721)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0007  max mem: 15572
Epoch: [19]  [ 260/1404]  eta: 0:11:39  lr: 0.000061  min_lr: 0.000001  loss: 4.1476 (4.2173)  class_acc: 0.2917 (0.3020)  loss_scale: 32768.0000 (18832.1839)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.0008  max mem: 15572
Epoch: [19]  [ 270/1404]  eta: 0:11:28  lr: 0.000061  min_lr: 0.000001  loss: 4.2571 (4.2234)  class_acc: 0.2500 (0.3010)  loss_scale: 32768.0000 (19346.4207)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0008  max mem: 15572
Epoch: [19]  [ 280/1404]  eta: 0:11:20  lr: 0.000061  min_lr: 0.000001  loss: 4.3517 (4.2265)  class_acc: 0.2500 (0.3012)  loss_scale: 32768.0000 (19824.0569)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0007  max mem: 15572
Epoch: [19]  [ 290/1404]  eta: 0:11:14  lr: 0.000061  min_lr: 0.000001  loss: 4.2714 (4.2293)  class_acc: 0.2500 (0.3021)  loss_scale: 32768.0000 (20268.8660)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0006  max mem: 15572
[2025-01-17 00:54:55,853] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26968
[2025-01-17 00:54:55,853] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:54:55,853] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 00:54:55,888] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26968
[2025-01-17 00:54:55,889] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [19]  [ 300/1404]  eta: 0:11:07  lr: 0.000061  min_lr: 0.000001  loss: 4.2670 (4.2281)  class_acc: 0.3333 (0.3030)  loss_scale: 32768.0000 (20194.2326)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0006  max mem: 15572
Epoch: [19]  [ 310/1404]  eta: 0:11:03  lr: 0.000061  min_lr: 0.000001  loss: 4.2685 (4.2316)  class_acc: 0.3333 (0.3031)  loss_scale: 16384.0000 (20071.7170)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.0007  max mem: 15572
Epoch: [19]  [ 320/1404]  eta: 0:10:57  lr: 0.000060  min_lr: 0.000001  loss: 4.2683 (4.2285)  class_acc: 0.3333 (0.3027)  loss_scale: 16384.0000 (19956.8349)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.0007  max mem: 15572
[2025-01-17 00:55:15,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=161, lr=[5.858866367352662e-07, 5.858866367352662e-07, 8.369809096218089e-07, 8.369809096218089e-07, 1.1956870137454413e-06, 1.1956870137454413e-06, 1.7081243053506307e-06, 1.7081243053506307e-06, 2.44017757907233e-06, 2.44017757907233e-06, 3.4859679701033285e-06, 3.4859679701033285e-06, 4.979954243004755e-06, 4.979954243004755e-06, 7.114220347149651e-06, 7.114220347149651e-06, 1.0163171924499501e-05, 1.0163171924499501e-05, 1.451881703499929e-05, 1.451881703499929e-05, 2.0741167192856128e-05, 2.0741167192856128e-05, 2.9630238846937328e-05, 2.9630238846937328e-05, 4.2328912638481896e-05, 4.2328912638481896e-05, 6.046987519783129e-05, 6.046987519783129e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 00:55:15,803] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=47.98604401499476, CurrSamplesPerSec=24.182502972865844, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [ 330/1404]  eta: 0:10:49  lr: 0.000060  min_lr: 0.000001  loss: 4.0660 (4.2248)  class_acc: 0.2917 (0.3035)  loss_scale: 16384.0000 (19848.8943)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0006  max mem: 15572
Epoch: [19]  [ 340/1404]  eta: 0:10:44  lr: 0.000060  min_lr: 0.000001  loss: 4.0837 (4.2226)  class_acc: 0.2917 (0.3029)  loss_scale: 16384.0000 (19747.2845)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0006  max mem: 15572
Epoch: [19]  [ 350/1404]  eta: 0:10:37  lr: 0.000060  min_lr: 0.000001  loss: 4.1985 (4.2217)  class_acc: 0.2500 (0.3020)  loss_scale: 16384.0000 (19651.4644)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0007  max mem: 15572
Epoch: [19]  [ 360/1404]  eta: 0:10:29  lr: 0.000060  min_lr: 0.000001  loss: 4.1985 (4.2218)  class_acc: 0.2500 (0.3030)  loss_scale: 16384.0000 (19560.9529)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0008  max mem: 15572
Epoch: [19]  [ 370/1404]  eta: 0:10:21  lr: 0.000060  min_lr: 0.000001  loss: 4.2082 (4.2242)  class_acc: 0.2917 (0.3024)  loss_scale: 16384.0000 (19475.3208)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0008  max mem: 15572
Epoch: [19]  [ 380/1404]  eta: 0:10:16  lr: 0.000060  min_lr: 0.000001  loss: 4.0876 (4.2209)  class_acc: 0.2917 (0.3028)  loss_scale: 16384.0000 (19394.1837)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0007  max mem: 15572
Epoch: [19]  [ 390/1404]  eta: 0:10:09  lr: 0.000060  min_lr: 0.000001  loss: 3.9822 (4.2137)  class_acc: 0.3750 (0.3052)  loss_scale: 16384.0000 (19317.1969)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0006  max mem: 15572
Epoch: [19]  [ 400/1404]  eta: 0:10:03  lr: 0.000060  min_lr: 0.000001  loss: 4.0982 (4.2146)  class_acc: 0.3333 (0.3054)  loss_scale: 16384.0000 (19244.0499)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0006  max mem: 15572
Epoch: [19]  [ 410/1404]  eta: 0:09:56  lr: 0.000060  min_lr: 0.000001  loss: 4.3433 (4.2181)  class_acc: 0.2917 (0.3042)  loss_scale: 16384.0000 (19174.4623)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0005  max mem: 15572
Epoch: [19]  [ 420/1404]  eta: 0:09:49  lr: 0.000060  min_lr: 0.000001  loss: 4.2616 (4.2186)  class_acc: 0.2500 (0.3035)  loss_scale: 16384.0000 (19108.1805)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0006  max mem: 15572
[2025-01-17 00:56:11,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:56:11,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:56:11,662] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:56:11,662] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [ 430/1404]  eta: 0:09:42  lr: 0.000060  min_lr: 0.000001  loss: 4.1997 (4.2190)  class_acc: 0.2500 (0.3034)  loss_scale: 16384.0000 (19425.1137)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0007  max mem: 15572
Epoch: [19]  [ 440/1404]  eta: 0:09:37  lr: 0.000060  min_lr: 0.000001  loss: 4.1980 (4.2215)  class_acc: 0.2083 (0.3011)  loss_scale: 32768.0000 (19727.6735)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0007  max mem: 15572
Epoch: [19]  [ 450/1404]  eta: 0:09:31  lr: 0.000060  min_lr: 0.000001  loss: 4.2495 (4.2207)  class_acc: 0.2500 (0.3016)  loss_scale: 32768.0000 (20016.8160)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0007  max mem: 15572
Epoch: [19]  [ 460/1404]  eta: 0:09:25  lr: 0.000060  min_lr: 0.000001  loss: 4.2694 (4.2231)  class_acc: 0.2917 (0.3015)  loss_scale: 32768.0000 (20293.4143)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0006  max mem: 15572
Epoch: [19]  [ 470/1404]  eta: 0:09:18  lr: 0.000060  min_lr: 0.000001  loss: 4.2718 (4.2200)  class_acc: 0.2917 (0.3014)  loss_scale: 32768.0000 (20558.2675)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0008  max mem: 15572
Epoch: [19]  [ 480/1404]  eta: 0:09:11  lr: 0.000060  min_lr: 0.000001  loss: 4.1442 (4.2193)  class_acc: 0.3333 (0.3025)  loss_scale: 32768.0000 (20812.1081)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.0009  max mem: 15572
Epoch: [19]  [ 490/1404]  eta: 0:09:05  lr: 0.000060  min_lr: 0.000001  loss: 4.3757 (4.2245)  class_acc: 0.3333 (0.3016)  loss_scale: 32768.0000 (21055.6090)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0007  max mem: 15572
Epoch: [19]  [ 500/1404]  eta: 0:08:59  lr: 0.000060  min_lr: 0.000001  loss: 4.3897 (4.2269)  class_acc: 0.2083 (0.3019)  loss_scale: 32768.0000 (21289.3892)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0005  max mem: 15572
Epoch: [19]  [ 510/1404]  eta: 0:08:52  lr: 0.000060  min_lr: 0.000001  loss: 4.2959 (4.2265)  class_acc: 0.2500 (0.3013)  loss_scale: 32768.0000 (21514.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0007  max mem: 15572
Epoch: [19]  [ 520/1404]  eta: 0:08:46  lr: 0.000060  min_lr: 0.000001  loss: 4.2584 (4.2275)  class_acc: 0.2500 (0.3006)  loss_scale: 32768.0000 (21730.0269)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0007  max mem: 15572
Epoch: [19]  [ 530/1404]  eta: 0:08:40  lr: 0.000060  min_lr: 0.000001  loss: 4.1453 (4.2264)  class_acc: 0.2917 (0.3009)  loss_scale: 32768.0000 (21937.8983)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.0006  max mem: 15572
Epoch: [19]  [ 540/1404]  eta: 0:08:35  lr: 0.000060  min_lr: 0.000001  loss: 4.1774 (4.2256)  class_acc: 0.2917 (0.3011)  loss_scale: 32768.0000 (22138.0850)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0007  max mem: 15572
[2025-01-17 00:57:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:57:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 00:57:26,308] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:57:26,308] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 00:57:26,745] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27226
[2025-01-17 00:57:26,746] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 00:57:26,746] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 550/1404]  eta: 0:08:28  lr: 0.000060  min_lr: 0.000001  loss: 4.2379 (4.2249)  class_acc: 0.3333 (0.3024)  loss_scale: 32768.0000 (22390.4755)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0009  max mem: 15572
[2025-01-17 00:57:26,758] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27226
[2025-01-17 00:57:26,758] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [19]  [ 560/1404]  eta: 0:08:23  lr: 0.000060  min_lr: 0.000001  loss: 4.1264 (4.2222)  class_acc: 0.3750 (0.3029)  loss_scale: 32768.0000 (22575.4581)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0011  max mem: 15572
Epoch: [19]  [ 570/1404]  eta: 0:08:17  lr: 0.000060  min_lr: 0.000001  loss: 4.0091 (4.2177)  class_acc: 0.2917 (0.3036)  loss_scale: 32768.0000 (22753.9615)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.0008  max mem: 15572
Epoch: [19]  [ 580/1404]  eta: 0:08:10  lr: 0.000060  min_lr: 0.000001  loss: 4.1391 (4.2190)  class_acc: 0.2917 (0.3032)  loss_scale: 32768.0000 (22926.3201)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.0006  max mem: 15572
Epoch: [19]  [ 590/1404]  eta: 0:08:04  lr: 0.000060  min_lr: 0.000001  loss: 4.2234 (4.2189)  class_acc: 0.2500 (0.3037)  loss_scale: 32768.0000 (23092.8460)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0006  max mem: 15572
Epoch: [19]  [ 600/1404]  eta: 0:07:58  lr: 0.000060  min_lr: 0.000001  loss: 4.2290 (4.2198)  class_acc: 0.3333 (0.3044)  loss_scale: 32768.0000 (23253.8303)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0007  max mem: 15572
Epoch: [19]  [ 610/1404]  eta: 0:07:50  lr: 0.000060  min_lr: 0.000001  loss: 4.1765 (4.2191)  class_acc: 0.3333 (0.3046)  loss_scale: 32768.0000 (23409.5450)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0007  max mem: 15572
Epoch: [19]  [ 620/1404]  eta: 0:07:45  lr: 0.000060  min_lr: 0.000001  loss: 4.2372 (4.2225)  class_acc: 0.2917 (0.3045)  loss_scale: 32768.0000 (23560.2448)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0006  max mem: 15572
Epoch: [19]  [ 630/1404]  eta: 0:07:40  lr: 0.000060  min_lr: 0.000001  loss: 4.3488 (4.2234)  class_acc: 0.3333 (0.3058)  loss_scale: 32768.0000 (23706.1680)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.0006  max mem: 15572
Epoch: [19]  [ 640/1404]  eta: 0:07:34  lr: 0.000060  min_lr: 0.000001  loss: 4.2905 (4.2248)  class_acc: 0.3750 (0.3061)  loss_scale: 32768.0000 (23847.5382)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.0006  max mem: 15572
Epoch: [19]  [ 650/1404]  eta: 0:07:28  lr: 0.000060  min_lr: 0.000001  loss: 4.2596 (4.2247)  class_acc: 0.2917 (0.3055)  loss_scale: 32768.0000 (23984.5653)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0007  max mem: 15572
Epoch: [19]  [ 660/1404]  eta: 0:07:22  lr: 0.000060  min_lr: 0.000001  loss: 4.3950 (4.2252)  class_acc: 0.2500 (0.3060)  loss_scale: 32768.0000 (24117.4463)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.0008  max mem: 15572
[2025-01-17 00:58:34,532] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27341
[2025-01-17 00:58:34,533] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27341
[2025-01-17 00:58:34,533] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:58:34,533] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 00:58:34,533] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [19]  [ 670/1404]  eta: 0:07:15  lr: 0.000059  min_lr: 0.000001  loss: 4.1164 (4.2251)  class_acc: 0.3333 (0.3061)  loss_scale: 32768.0000 (24099.8629)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0009  max mem: 15572
Epoch: [19]  [ 680/1404]  eta: 0:07:09  lr: 0.000059  min_lr: 0.000001  loss: 4.1164 (4.2235)  class_acc: 0.2500 (0.3065)  loss_scale: 16384.0000 (23986.5609)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0010  max mem: 15572
Epoch: [19]  [ 690/1404]  eta: 0:07:03  lr: 0.000059  min_lr: 0.000001  loss: 4.2182 (4.2237)  class_acc: 0.2917 (0.3065)  loss_scale: 16384.0000 (23876.5384)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.0008  max mem: 15572
Epoch: [19]  [ 700/1404]  eta: 0:06:57  lr: 0.000059  min_lr: 0.000001  loss: 4.1254 (4.2221)  class_acc: 0.2917 (0.3071)  loss_scale: 16384.0000 (23769.6548)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0006  max mem: 15572
Epoch: [19]  [ 710/1404]  eta: 0:06:50  lr: 0.000059  min_lr: 0.000001  loss: 4.2503 (4.2231)  class_acc: 0.3333 (0.3073)  loss_scale: 16384.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0007  max mem: 15572
Epoch: [19]  [ 720/1404]  eta: 0:06:44  lr: 0.000059  min_lr: 0.000001  loss: 4.3351 (4.2236)  class_acc: 0.3333 (0.3073)  loss_scale: 16384.0000 (23564.7822)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0008  max mem: 15572
Epoch: [19]  [ 730/1404]  eta: 0:06:39  lr: 0.000059  min_lr: 0.000001  loss: 4.1468 (4.2238)  class_acc: 0.2917 (0.3071)  loss_scale: 16384.0000 (23466.5499)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0007  max mem: 15572
Epoch: [19]  [ 740/1404]  eta: 0:06:33  lr: 0.000059  min_lr: 0.000001  loss: 4.1468 (4.2246)  class_acc: 0.2083 (0.3065)  loss_scale: 16384.0000 (23370.9690)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.0008  max mem: 15572
Epoch: [19]  [ 750/1404]  eta: 0:06:27  lr: 0.000059  min_lr: 0.000001  loss: 4.1903 (4.2250)  class_acc: 0.2917 (0.3070)  loss_scale: 16384.0000 (23277.9334)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0009  max mem: 15572
Epoch: [19]  [ 760/1404]  eta: 0:06:21  lr: 0.000059  min_lr: 0.000001  loss: 4.2117 (4.2264)  class_acc: 0.3333 (0.3071)  loss_scale: 16384.0000 (23187.3430)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0008  max mem: 15572
Epoch: [19]  [ 770/1404]  eta: 0:06:15  lr: 0.000059  min_lr: 0.000001  loss: 4.2406 (4.2256)  class_acc: 0.3333 (0.3069)  loss_scale: 16384.0000 (23099.1025)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0007  max mem: 15572
Epoch: [19]  [ 780/1404]  eta: 0:06:09  lr: 0.000059  min_lr: 0.000001  loss: 4.1915 (4.2244)  class_acc: 0.2500 (0.3061)  loss_scale: 16384.0000 (23013.1216)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0006  max mem: 15572
Epoch: [19]  [ 790/1404]  eta: 0:06:03  lr: 0.000059  min_lr: 0.000001  loss: 4.1331 (4.2233)  class_acc: 0.2500 (0.3064)  loss_scale: 16384.0000 (22929.3148)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0009  max mem: 15572
[2025-01-17 00:59:50,032] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:59:50,032] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 00:59:50,032] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 00:59:50,032] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [ 800/1404]  eta: 0:05:57  lr: 0.000059  min_lr: 0.000001  loss: 4.1085 (4.2223)  class_acc: 0.2500 (0.3060)  loss_scale: 16384.0000 (22990.7815)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0007  max mem: 15572
Epoch: [19]  [ 810/1404]  eta: 0:05:51  lr: 0.000059  min_lr: 0.000001  loss: 4.2742 (4.2250)  class_acc: 0.2500 (0.3056)  loss_scale: 32768.0000 (23111.3391)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0005  max mem: 15572
Epoch: [19]  [ 820/1404]  eta: 0:05:45  lr: 0.000059  min_lr: 0.000001  loss: 4.3030 (4.2252)  class_acc: 0.2500 (0.3057)  loss_scale: 32768.0000 (23228.9598)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0006  max mem: 15572
Epoch: [19]  [ 830/1404]  eta: 0:05:39  lr: 0.000059  min_lr: 0.000001  loss: 4.1808 (4.2245)  class_acc: 0.3333 (0.3064)  loss_scale: 32768.0000 (23343.7497)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0007  max mem: 15572
Epoch: [19]  [ 840/1404]  eta: 0:05:33  lr: 0.000059  min_lr: 0.000001  loss: 4.1676 (4.2229)  class_acc: 0.2917 (0.3066)  loss_scale: 32768.0000 (23455.8098)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0008  max mem: 15572
Epoch: [19]  [ 850/1404]  eta: 0:05:26  lr: 0.000059  min_lr: 0.000001  loss: 4.1676 (4.2240)  class_acc: 0.2917 (0.3067)  loss_scale: 32768.0000 (23565.2362)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0008  max mem: 15572
Epoch: [19]  [ 860/1404]  eta: 0:05:21  lr: 0.000059  min_lr: 0.000001  loss: 4.1476 (4.2236)  class_acc: 0.2500 (0.3065)  loss_scale: 32768.0000 (23672.1208)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0006  max mem: 15572
Epoch: [19]  [ 870/1404]  eta: 0:05:14  lr: 0.000059  min_lr: 0.000001  loss: 4.2749 (4.2244)  class_acc: 0.2500 (0.3057)  loss_scale: 32768.0000 (23776.5511)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0006  max mem: 15572
Epoch: [19]  [ 880/1404]  eta: 0:05:08  lr: 0.000059  min_lr: 0.000001  loss: 4.2555 (4.2239)  class_acc: 0.2500 (0.3059)  loss_scale: 32768.0000 (23878.6107)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0006  max mem: 15572
Epoch: [19]  [ 890/1404]  eta: 0:05:03  lr: 0.000059  min_lr: 0.000001  loss: 4.1486 (4.2240)  class_acc: 0.2917 (0.3066)  loss_scale: 32768.0000 (23978.3793)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0114  max mem: 15572
Epoch: [19]  [ 900/1404]  eta: 0:04:56  lr: 0.000059  min_lr: 0.000001  loss: 4.2472 (4.2249)  class_acc: 0.2917 (0.3070)  loss_scale: 32768.0000 (24075.9334)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0116  max mem: 15572
Epoch: [19]  [ 910/1404]  eta: 0:04:51  lr: 0.000059  min_lr: 0.000001  loss: 4.2356 (4.2255)  class_acc: 0.2917 (0.3071)  loss_scale: 32768.0000 (24171.3458)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0801  max mem: 15572
Epoch: [19]  [ 920/1404]  eta: 0:04:45  lr: 0.000059  min_lr: 0.000001  loss: 4.2504 (4.2257)  class_acc: 0.2917 (0.3074)  loss_scale: 32768.0000 (24264.6862)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.1590  max mem: 15572
[2025-01-17 01:01:03,124] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:01:03,124] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:01:03,124] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 01:01:03,124] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 01:01:04,338] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27599
[2025-01-17 01:01:04,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 01:01:04,338] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 01:01:04,353] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27599
[2025-01-17 01:01:04,354] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [19]  [ 930/1404]  eta: 0:04:39  lr: 0.000059  min_lr: 0.000001  loss: 4.3278 (4.2267)  class_acc: 0.2917 (0.3072)  loss_scale: 32768.0000 (24391.2180)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1088  max mem: 15572
[2025-01-17 01:01:11,622] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27612
[2025-01-17 01:01:11,623] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:01:11,623] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 01:01:11,635] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27612
[2025-01-17 01:01:11,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [19]  [ 940/1404]  eta: 0:04:33  lr: 0.000059  min_lr: 0.000001  loss: 4.3278 (4.2261)  class_acc: 0.3333 (0.3080)  loss_scale: 32768.0000 (24393.1817)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0667  max mem: 15572
Epoch: [19]  [ 950/1404]  eta: 0:04:27  lr: 0.000059  min_lr: 0.000001  loss: 4.2158 (4.2252)  class_acc: 0.3333 (0.3072)  loss_scale: 16384.0000 (24308.9632)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1100  max mem: 15572
Epoch: [19]  [ 960/1404]  eta: 0:04:21  lr: 0.000059  min_lr: 0.000001  loss: 4.2150 (4.2259)  class_acc: 0.2500 (0.3069)  loss_scale: 16384.0000 (24226.4974)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1239  max mem: 15572
Epoch: [19]  [ 970/1404]  eta: 0:04:16  lr: 0.000059  min_lr: 0.000001  loss: 4.2112 (4.2247)  class_acc: 0.3333 (0.3075)  loss_scale: 16384.0000 (24145.7302)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1307  max mem: 15572
Epoch: [19]  [ 980/1404]  eta: 0:04:10  lr: 0.000059  min_lr: 0.000001  loss: 4.2112 (4.2249)  class_acc: 0.2917 (0.3076)  loss_scale: 16384.0000 (24066.6096)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1318  max mem: 15572
Epoch: [19]  [ 990/1404]  eta: 0:04:04  lr: 0.000059  min_lr: 0.000001  loss: 4.3488 (4.2254)  class_acc: 0.2500 (0.3069)  loss_scale: 16384.0000 (23989.0858)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1001  max mem: 15572
Epoch: [19]  [1000/1404]  eta: 0:03:58  lr: 0.000059  min_lr: 0.000001  loss: 4.3488 (4.2251)  class_acc: 0.2500 (0.3073)  loss_scale: 16384.0000 (23913.1109)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0905  max mem: 15572
Epoch: [19]  [1010/1404]  eta: 0:03:52  lr: 0.000058  min_lr: 0.000001  loss: 4.2270 (4.2253)  class_acc: 0.3750 (0.3076)  loss_scale: 16384.0000 (23838.6390)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0814  max mem: 15572
Epoch: [19]  [1020/1404]  eta: 0:03:46  lr: 0.000058  min_lr: 0.000001  loss: 4.3570 (4.2263)  class_acc: 0.2500 (0.3069)  loss_scale: 16384.0000 (23765.6259)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.0738  max mem: 15572
Epoch: [19]  [1030/1404]  eta: 0:03:40  lr: 0.000058  min_lr: 0.000001  loss: 4.3597 (4.2272)  class_acc: 0.2083 (0.3064)  loss_scale: 16384.0000 (23694.0291)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0844  max mem: 15572
Epoch: [19]  [1040/1404]  eta: 0:03:34  lr: 0.000058  min_lr: 0.000001  loss: 4.1975 (4.2268)  class_acc: 0.2917 (0.3070)  loss_scale: 16384.0000 (23623.8079)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0794  max mem: 15572
Epoch: [19]  [1050/1404]  eta: 0:03:28  lr: 0.000058  min_lr: 0.000001  loss: 4.2845 (4.2266)  class_acc: 0.3333 (0.3070)  loss_scale: 16384.0000 (23554.9229)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0668  max mem: 15572
Epoch: [19]  [1060/1404]  eta: 0:03:22  lr: 0.000058  min_lr: 0.000001  loss: 4.2016 (4.2259)  class_acc: 0.3333 (0.3073)  loss_scale: 16384.0000 (23487.3365)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1358  max mem: 15572
[2025-01-17 01:02:28,514] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:02:28,515] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:02:28,551] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:02:28,551] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [1070/1404]  eta: 0:03:17  lr: 0.000058  min_lr: 0.000001  loss: 4.1676 (4.2246)  class_acc: 0.2917 (0.3071)  loss_scale: 16384.0000 (23512.7993)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1505  max mem: 15572
Epoch: [19]  [1080/1404]  eta: 0:03:10  lr: 0.000058  min_lr: 0.000001  loss: 4.1240 (4.2245)  class_acc: 0.2917 (0.3070)  loss_scale: 32768.0000 (23598.4163)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0783  max mem: 15572
Epoch: [19]  [1090/1404]  eta: 0:03:05  lr: 0.000058  min_lr: 0.000001  loss: 4.1352 (4.2235)  class_acc: 0.3750 (0.3077)  loss_scale: 32768.0000 (23682.4638)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0854  max mem: 15572
Epoch: [19]  [1100/1404]  eta: 0:02:59  lr: 0.000058  min_lr: 0.000001  loss: 4.1388 (4.2240)  class_acc: 0.3333 (0.3074)  loss_scale: 32768.0000 (23764.9846)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1184  max mem: 15572
Epoch: [19]  [1110/1404]  eta: 0:02:53  lr: 0.000058  min_lr: 0.000001  loss: 4.3288 (4.2240)  class_acc: 0.2917 (0.3079)  loss_scale: 32768.0000 (23846.0198)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.1312  max mem: 15572
Epoch: [19]  [1120/1404]  eta: 0:02:47  lr: 0.000058  min_lr: 0.000001  loss: 4.3696 (4.2251)  class_acc: 0.2500 (0.3075)  loss_scale: 32768.0000 (23925.6093)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0780  max mem: 15572
Epoch: [19]  [1130/1404]  eta: 0:02:41  lr: 0.000058  min_lr: 0.000001  loss: 4.3427 (4.2266)  class_acc: 0.2500 (0.3074)  loss_scale: 32768.0000 (24003.7913)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0547  max mem: 15572
Epoch: [19]  [1140/1404]  eta: 0:02:35  lr: 0.000058  min_lr: 0.000001  loss: 4.3032 (4.2264)  class_acc: 0.2500 (0.3072)  loss_scale: 32768.0000 (24080.6030)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1320  max mem: 15572
Epoch: [19]  [1150/1404]  eta: 0:02:29  lr: 0.000058  min_lr: 0.000001  loss: 4.1970 (4.2252)  class_acc: 0.2500 (0.3068)  loss_scale: 32768.0000 (24156.0799)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1136  max mem: 15572
[2025-01-17 01:03:21,326] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27833
[2025-01-17 01:03:21,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:03:21,326] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27833
[2025-01-17 01:03:21,326] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 01:03:21,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [19]  [1160/1404]  eta: 0:02:23  lr: 0.000058  min_lr: 0.000001  loss: 4.2676 (4.2272)  class_acc: 0.2500 (0.3063)  loss_scale: 32768.0000 (24173.8088)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0936  max mem: 15572
Epoch: [19]  [1170/1404]  eta: 0:02:17  lr: 0.000058  min_lr: 0.000001  loss: 4.3737 (4.2279)  class_acc: 0.2917 (0.3062)  loss_scale: 16384.0000 (24107.2861)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0970  max mem: 15572
Epoch: [19]  [1180/1404]  eta: 0:02:12  lr: 0.000058  min_lr: 0.000001  loss: 4.2285 (4.2281)  class_acc: 0.2917 (0.3058)  loss_scale: 16384.0000 (24041.8899)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0660  max mem: 15572
Epoch: [19]  [1190/1404]  eta: 0:02:06  lr: 0.000058  min_lr: 0.000001  loss: 4.3197 (4.2295)  class_acc: 0.2083 (0.3051)  loss_scale: 16384.0000 (23977.5919)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0994  max mem: 15572
Epoch: [19]  [1200/1404]  eta: 0:02:00  lr: 0.000058  min_lr: 0.000001  loss: 4.3067 (4.2285)  class_acc: 0.2083 (0.3049)  loss_scale: 16384.0000 (23914.3647)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0669  max mem: 15572
Epoch: [19]  [1210/1404]  eta: 0:01:54  lr: 0.000058  min_lr: 0.000001  loss: 4.2079 (4.2283)  class_acc: 0.3333 (0.3052)  loss_scale: 16384.0000 (23852.1817)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0204  max mem: 15572
Epoch: [19]  [1220/1404]  eta: 0:01:48  lr: 0.000058  min_lr: 0.000001  loss: 4.2631 (4.2279)  class_acc: 0.2917 (0.3047)  loss_scale: 16384.0000 (23791.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1001  max mem: 15572
Epoch: [19]  [1230/1404]  eta: 0:01:42  lr: 0.000058  min_lr: 0.000001  loss: 4.1971 (4.2277)  class_acc: 0.2500 (0.3050)  loss_scale: 16384.0000 (23730.8465)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1108  max mem: 15572
Epoch: [19]  [1240/1404]  eta: 0:01:36  lr: 0.000058  min_lr: 0.000001  loss: 4.2711 (4.2282)  class_acc: 0.2917 (0.3051)  loss_scale: 16384.0000 (23671.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0309  max mem: 15572
Epoch: [19]  [1250/1404]  eta: 0:01:30  lr: 0.000058  min_lr: 0.000001  loss: 4.2492 (4.2277)  class_acc: 0.2917 (0.3056)  loss_scale: 16384.0000 (23613.3909)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0502  max mem: 15572
Epoch: [19]  [1260/1404]  eta: 0:01:24  lr: 0.000058  min_lr: 0.000001  loss: 4.1491 (4.2267)  class_acc: 0.3750 (0.3057)  loss_scale: 16384.0000 (23556.0603)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1221  max mem: 15572
Epoch: [19]  [1270/1404]  eta: 0:01:18  lr: 0.000058  min_lr: 0.000001  loss: 4.1491 (4.2263)  class_acc: 0.3333 (0.3059)  loss_scale: 16384.0000 (23499.6318)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1086  max mem: 15572
Epoch: [19]  [1280/1404]  eta: 0:01:12  lr: 0.000058  min_lr: 0.000001  loss: 4.2299 (4.2266)  class_acc: 0.2917 (0.3060)  loss_scale: 16384.0000 (23444.0843)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0511  max mem: 15572
[2025-01-17 01:04:35,014] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:04:35,014] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:04:35,026] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:04:35,026] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [1290/1404]  eta: 0:01:07  lr: 0.000058  min_lr: 0.000001  loss: 4.3258 (4.2276)  class_acc: 0.2500 (0.3058)  loss_scale: 16384.0000 (23452.8521)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1300  max mem: 15572
Epoch: [19]  [1300/1404]  eta: 0:01:01  lr: 0.000058  min_lr: 0.000001  loss: 4.2918 (4.2274)  class_acc: 0.2917 (0.3060)  loss_scale: 32768.0000 (23524.4520)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1157  max mem: 15572
[2025-01-17 01:04:46,720] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27982
[2025-01-17 01:04:46,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:04:46,742] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27982
[2025-01-17 01:04:46,742] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:04:46,742] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [19]  [1310/1404]  eta: 0:00:55  lr: 0.000058  min_lr: 0.000001  loss: 4.2375 (4.2269)  class_acc: 0.3333 (0.3062)  loss_scale: 32768.0000 (23532.4729)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0225  max mem: 15572
Epoch: [19]  [1320/1404]  eta: 0:00:49  lr: 0.000058  min_lr: 0.000001  loss: 4.2643 (4.2270)  class_acc: 0.3333 (0.3062)  loss_scale: 16384.0000 (23478.3588)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0780  max mem: 15572
[2025-01-17 01:04:56,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=167, lr=[5.578713180155505e-07, 5.578713180155505e-07, 7.969590257365008e-07, 7.969590257365008e-07, 1.138512893909287e-06, 1.138512893909287e-06, 1.6264469912989816e-06, 1.6264469912989816e-06, 2.323495701855688e-06, 2.323495701855688e-06, 3.3192795740795545e-06, 3.3192795740795545e-06, 4.7418279629707926e-06, 4.7418279629707926e-06, 6.774039947101132e-06, 6.774039947101132e-06, 9.67719992443019e-06, 9.67719992443019e-06, 1.3824571320614558e-05, 1.3824571320614558e-05, 1.9749387600877938e-05, 1.9749387600877938e-05, 2.821341085839706e-05, 2.821341085839706e-05, 4.0304872654852946e-05, 4.0304872654852946e-05, 5.757838950693278e-05, 5.757838950693278e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 01:04:56,956] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=47.952345949323416, CurrSamplesPerSec=57.4442188446934, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [1330/1404]  eta: 0:00:43  lr: 0.000058  min_lr: 0.000001  loss: 4.3057 (4.2277)  class_acc: 0.2917 (0.3060)  loss_scale: 16384.0000 (23425.0579)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0562  max mem: 15572
Epoch: [19]  [1340/1404]  eta: 0:00:37  lr: 0.000058  min_lr: 0.000001  loss: 4.2363 (4.2269)  class_acc: 0.2500 (0.3058)  loss_scale: 16384.0000 (23372.5518)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0008  max mem: 15572
Epoch: [19]  [1350/1404]  eta: 0:00:31  lr: 0.000057  min_lr: 0.000001  loss: 4.0556 (4.2266)  class_acc: 0.2917 (0.3061)  loss_scale: 16384.0000 (23320.8231)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0353  max mem: 15572
Epoch: [19]  [1360/1404]  eta: 0:00:25  lr: 0.000057  min_lr: 0.000001  loss: 4.0556 (4.2259)  class_acc: 0.2500 (0.3057)  loss_scale: 16384.0000 (23269.8545)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0732  max mem: 15572
Epoch: [19]  [1370/1404]  eta: 0:00:19  lr: 0.000057  min_lr: 0.000001  loss: 4.1678 (4.2254)  class_acc: 0.2500 (0.3056)  loss_scale: 16384.0000 (23219.6295)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.0883  max mem: 15572
Epoch: [19]  [1380/1404]  eta: 0:00:14  lr: 0.000057  min_lr: 0.000001  loss: 4.2572 (4.2253)  class_acc: 0.2917 (0.3056)  loss_scale: 16384.0000 (23170.1318)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0758  max mem: 15572
Epoch: [19]  [1390/1404]  eta: 0:00:08  lr: 0.000057  min_lr: 0.000001  loss: 4.2809 (4.2263)  class_acc: 0.2917 (0.3057)  loss_scale: 16384.0000 (23121.3458)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0525  max mem: 15572
Epoch: [19]  [1400/1404]  eta: 0:00:02  lr: 0.000057  min_lr: 0.000001  loss: 4.3001 (4.2265)  class_acc: 0.2917 (0.3054)  loss_scale: 16384.0000 (23073.2562)  weight_decay: 0.0500 (0.0500)  time: 0.4598  data: 0.0268  max mem: 15572
Epoch: [19]  [1403/1404]  eta: 0:00:00  lr: 0.000057  min_lr: 0.000001  loss: 4.3001 (4.2267)  class_acc: 0.2500 (0.3054)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  time: 0.4357  data: 0.0191  max mem: 15572
Epoch: [19] Total time: 0:13:42 (0.5855 s / it)
Averaged stats: lr: 0.000057  min_lr: 0.000001  loss: 4.3001 (4.2284)  class_acc: 0.2500 (0.3034)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)
[2025-01-17 01:05:40,888] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2025-01-17 01:05:40,890] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-19/mp_rank_00_model_states.pt
[2025-01-17 01:05:40,890] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-19/mp_rank_00_model_states.pt...
[2025-01-17 01:05:40,890] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2025-01-17 01:05:41,116] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-19/mp_rank_00_model_states.pt.
[2025-01-17 01:05:41,117] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [  0/136]  eta: 0:12:36  loss: 1.9979 (1.9979)  acc1: 66.6667 (66.6667)  acc5: 77.7778 (77.7778)  time: 5.5589  data: 5.3741  max mem: 15572
Val:  [ 10/136]  eta: 0:01:35  loss: 2.3499 (2.4017)  acc1: 44.4444 (44.4444)  acc5: 77.7778 (77.2727)  time: 0.7586  data: 0.5662  max mem: 15572
Val:  [ 20/136]  eta: 0:01:02  loss: 2.5475 (2.5206)  acc1: 38.8889 (42.8571)  acc5: 77.7778 (75.9259)  time: 0.2853  data: 0.0937  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 2.4893 (2.4106)  acc1: 38.8889 (45.1613)  acc5: 77.7778 (77.4194)  time: 0.3660  data: 0.1595  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.0157 (2.3709)  acc1: 55.5556 (47.2900)  acc5: 83.3333 (77.6423)  time: 0.3653  data: 0.1561  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.2779 (2.4218)  acc1: 44.4444 (44.6623)  acc5: 77.7778 (76.9063)  time: 0.3284  data: 0.1350  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.6884 (2.5014)  acc1: 33.3333 (42.6230)  acc5: 66.6667 (75.5920)  time: 0.3409  data: 0.1498  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.5137 (2.4906)  acc1: 38.8889 (43.4272)  acc5: 77.7778 (75.9781)  time: 0.3447  data: 0.1471  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.3691 (2.4742)  acc1: 50.0000 (44.1701)  acc5: 83.3333 (76.8176)  time: 0.3692  data: 0.1644  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.4000 (2.4822)  acc1: 38.8889 (42.7961)  acc5: 83.3333 (76.4957)  time: 0.3599  data: 0.1485  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.6552 (2.5313)  acc1: 27.7778 (41.0891)  acc5: 66.6667 (75.0825)  time: 0.3277  data: 0.1114  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.7147 (2.5266)  acc1: 38.8889 (41.4915)  acc5: 77.7778 (75.4254)  time: 0.3314  data: 0.1252  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.2397 (2.4868)  acc1: 44.4444 (42.8834)  acc5: 83.3333 (76.2167)  time: 0.3635  data: 0.1704  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.1216 (2.4585)  acc1: 55.5556 (44.0628)  acc5: 83.3333 (76.5903)  time: 0.3072  data: 0.1328  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.2404 (2.4626)  acc1: 50.0000 (44.1032)  acc5: 77.7778 (76.6175)  time: 0.2054  data: 0.0451  max mem: 15572
Val: Total time: 0:00:49 (0.3670 s / it)
* Acc@1 42.977 Acc@5 75.082 loss 2.516
Accuracy of the network on the 4883 val videos: 43.0%
Max accuracy: 43.26%
Epoch: [20]  [   0/1404]  eta: 2:53:45  lr: 0.000057  min_lr: 0.000001  loss: 3.9238 (3.9238)  class_acc: 0.4167 (0.4167)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.4257  data: 5.4492  max mem: 15572
Epoch: [20]  [  10/1404]  eta: 0:26:49  lr: 0.000057  min_lr: 0.000001  loss: 4.4141 (4.3990)  class_acc: 0.2500 (0.2841)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1546  data: 0.5149  max mem: 15572
Epoch: [20]  [  20/1404]  eta: 0:20:18  lr: 0.000057  min_lr: 0.000001  loss: 4.3810 (4.2080)  class_acc: 0.2500 (0.3016)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0436  max mem: 15572
Epoch: [20]  [  30/1404]  eta: 0:18:46  lr: 0.000057  min_lr: 0.000001  loss: 4.2003 (4.1930)  class_acc: 0.2917 (0.2970)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.0843  max mem: 15572
[2025-01-17 01:06:56,903] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:06:56,903] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:06:56,958] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:06:56,959] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [  40/1404]  eta: 0:16:51  lr: 0.000057  min_lr: 0.000001  loss: 4.2003 (4.1481)  class_acc: 0.3333 (0.3069)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0518  max mem: 15572
Epoch: [20]  [  50/1404]  eta: 0:15:40  lr: 0.000057  min_lr: 0.000001  loss: 4.0240 (4.1313)  class_acc: 0.3333 (0.3047)  loss_scale: 32768.0000 (22809.0980)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0009  max mem: 15572
Epoch: [20]  [  60/1404]  eta: 0:15:16  lr: 0.000057  min_lr: 0.000001  loss: 4.1757 (4.1499)  class_acc: 0.2917 (0.3115)  loss_scale: 32768.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0572  max mem: 15572
[2025-01-17 01:07:14,874] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28144
[2025-01-17 01:07:14,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:07:14,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 01:07:14,887] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28144
[2025-01-17 01:07:14,888] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [20]  [  70/1404]  eta: 0:14:34  lr: 0.000057  min_lr: 0.000001  loss: 4.2626 (4.1662)  class_acc: 0.2917 (0.3093)  loss_scale: 32768.0000 (23999.0986)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0760  max mem: 15572
Epoch: [20]  [  80/1404]  eta: 0:14:28  lr: 0.000057  min_lr: 0.000001  loss: 4.2626 (4.1761)  class_acc: 0.2500 (0.3040)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0752  max mem: 15572
Epoch: [20]  [  90/1404]  eta: 0:14:14  lr: 0.000057  min_lr: 0.000001  loss: 4.2303 (4.1883)  class_acc: 0.2500 (0.3013)  loss_scale: 16384.0000 (22325.4505)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.0563  max mem: 15572
Epoch: [20]  [ 100/1404]  eta: 0:14:00  lr: 0.000057  min_lr: 0.000001  loss: 4.3104 (4.2058)  class_acc: 0.2917 (0.2991)  loss_scale: 16384.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0410  max mem: 15572
Epoch: [20]  [ 110/1404]  eta: 0:13:47  lr: 0.000057  min_lr: 0.000001  loss: 4.2593 (4.2074)  class_acc: 0.2917 (0.3018)  loss_scale: 16384.0000 (21254.9189)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0885  max mem: 15572
Epoch: [20]  [ 120/1404]  eta: 0:13:39  lr: 0.000057  min_lr: 0.000001  loss: 4.1511 (4.1953)  class_acc: 0.3333 (0.3044)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1232  max mem: 15572
Epoch: [20]  [ 130/1404]  eta: 0:13:22  lr: 0.000057  min_lr: 0.000001  loss: 4.1273 (4.1887)  class_acc: 0.3333 (0.3063)  loss_scale: 16384.0000 (20511.2672)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0960  max mem: 15572
Epoch: [20]  [ 140/1404]  eta: 0:13:16  lr: 0.000057  min_lr: 0.000001  loss: 4.1653 (4.1886)  class_acc: 0.3333 (0.3115)  loss_scale: 16384.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0451  max mem: 15572
Epoch: [20]  [ 150/1404]  eta: 0:13:08  lr: 0.000057  min_lr: 0.000001  loss: 4.1624 (4.1891)  class_acc: 0.2917 (0.3082)  loss_scale: 16384.0000 (19964.6093)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.0330  max mem: 15572
Epoch: [20]  [ 160/1404]  eta: 0:12:57  lr: 0.000057  min_lr: 0.000001  loss: 4.2568 (4.1943)  class_acc: 0.2500 (0.3082)  loss_scale: 16384.0000 (19742.2112)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0486  max mem: 15572
Epoch: [20]  [ 170/1404]  eta: 0:12:52  lr: 0.000057  min_lr: 0.000001  loss: 4.1124 (4.1835)  class_acc: 0.2917 (0.3138)  loss_scale: 16384.0000 (19545.8246)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.0402  max mem: 15572
Epoch: [20]  [ 180/1404]  eta: 0:12:42  lr: 0.000057  min_lr: 0.000001  loss: 4.1709 (4.1874)  class_acc: 0.2500 (0.3122)  loss_scale: 16384.0000 (19371.1381)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0010  max mem: 15572
Epoch: [20]  [ 190/1404]  eta: 0:12:30  lr: 0.000057  min_lr: 0.000001  loss: 4.2312 (4.1960)  class_acc: 0.2917 (0.3137)  loss_scale: 16384.0000 (19214.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0010  max mem: 15572
[2025-01-17 01:08:30,667] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:08:30,668] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:08:30,668] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:08:30,668] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [ 200/1404]  eta: 0:12:25  lr: 0.000057  min_lr: 0.000001  loss: 4.3123 (4.2030)  class_acc: 0.3750 (0.3157)  loss_scale: 16384.0000 (19726.0100)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0007  max mem: 15572
Epoch: [20]  [ 210/1404]  eta: 0:12:16  lr: 0.000057  min_lr: 0.000001  loss: 4.1561 (4.1980)  class_acc: 0.2917 (0.3146)  loss_scale: 32768.0000 (20344.1137)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0009  max mem: 15572
[2025-01-17 01:08:42,215] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28292
[2025-01-17 01:08:42,216] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:08:42,320] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28292
[2025-01-17 01:08:42,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:08:42,320] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [20]  [ 220/1404]  eta: 0:12:10  lr: 0.000057  min_lr: 0.000001  loss: 4.1227 (4.1999)  class_acc: 0.2500 (0.3133)  loss_scale: 32768.0000 (20239.0588)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0008  max mem: 15572
Epoch: [20]  [ 230/1404]  eta: 0:12:02  lr: 0.000057  min_lr: 0.000001  loss: 4.2343 (4.2008)  class_acc: 0.2917 (0.3122)  loss_scale: 16384.0000 (20072.1732)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0006  max mem: 15572
Epoch: [20]  [ 240/1404]  eta: 0:11:53  lr: 0.000057  min_lr: 0.000001  loss: 4.2338 (4.2009)  class_acc: 0.3333 (0.3133)  loss_scale: 16384.0000 (19919.1369)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0007  max mem: 15572
Epoch: [20]  [ 250/1404]  eta: 0:11:47  lr: 0.000057  min_lr: 0.000001  loss: 4.2473 (4.2056)  class_acc: 0.2917 (0.3136)  loss_scale: 16384.0000 (19778.2948)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0006  max mem: 15572
Epoch: [20]  [ 260/1404]  eta: 0:11:43  lr: 0.000057  min_lr: 0.000001  loss: 4.2473 (4.2049)  class_acc: 0.2917 (0.3129)  loss_scale: 16384.0000 (19648.2452)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.0006  max mem: 15572
Epoch: [20]  [ 270/1404]  eta: 0:11:35  lr: 0.000057  min_lr: 0.000001  loss: 4.1249 (4.2027)  class_acc: 0.2917 (0.3141)  loss_scale: 16384.0000 (19527.7934)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0006  max mem: 15572
Epoch: [20]  [ 280/1404]  eta: 0:11:25  lr: 0.000057  min_lr: 0.000001  loss: 4.2683 (4.2051)  class_acc: 0.2917 (0.3123)  loss_scale: 16384.0000 (19415.9146)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0008  max mem: 15572
Epoch: [20]  [ 290/1404]  eta: 0:11:19  lr: 0.000056  min_lr: 0.000001  loss: 4.3056 (4.2097)  class_acc: 0.2500 (0.3113)  loss_scale: 16384.0000 (19311.7251)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0008  max mem: 15572
Epoch: [20]  [ 300/1404]  eta: 0:11:10  lr: 0.000056  min_lr: 0.000001  loss: 4.2307 (4.2111)  class_acc: 0.2500 (0.3110)  loss_scale: 16384.0000 (19214.4585)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0009  max mem: 15572
Epoch: [20]  [ 310/1404]  eta: 0:11:01  lr: 0.000056  min_lr: 0.000001  loss: 4.0834 (4.2081)  class_acc: 0.3333 (0.3124)  loss_scale: 16384.0000 (19123.4469)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0014  max mem: 15572
Epoch: [20]  [ 320/1404]  eta: 0:10:54  lr: 0.000056  min_lr: 0.000001  loss: 4.0368 (4.2091)  class_acc: 0.3750 (0.3146)  loss_scale: 16384.0000 (19038.1059)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0013  max mem: 15572
Epoch: [20]  [ 330/1404]  eta: 0:10:47  lr: 0.000056  min_lr: 0.000001  loss: 4.1651 (4.2073)  class_acc: 0.3333 (0.3150)  loss_scale: 16384.0000 (18957.9215)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0008  max mem: 15572
Epoch: [20]  [ 340/1404]  eta: 0:10:42  lr: 0.000056  min_lr: 0.000001  loss: 4.2771 (4.2096)  class_acc: 0.2500 (0.3133)  loss_scale: 16384.0000 (18882.4399)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.0005  max mem: 15572
[2025-01-17 01:09:57,583] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:09:57,584] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:09:57,585] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:09:57,585] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [ 350/1404]  eta: 0:10:35  lr: 0.000056  min_lr: 0.000001  loss: 4.2785 (4.2110)  class_acc: 0.2500 (0.3133)  loss_scale: 16384.0000 (19278.0399)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0008  max mem: 15572
Epoch: [20]  [ 360/1404]  eta: 0:10:27  lr: 0.000056  min_lr: 0.000001  loss: 4.2699 (4.2121)  class_acc: 0.3333 (0.3152)  loss_scale: 32768.0000 (19651.7230)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0009  max mem: 15572
Epoch: [20]  [ 370/1404]  eta: 0:10:19  lr: 0.000056  min_lr: 0.000001  loss: 4.2796 (4.2143)  class_acc: 0.3333 (0.3145)  loss_scale: 32768.0000 (20005.2615)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0006  max mem: 15572
Epoch: [20]  [ 380/1404]  eta: 0:10:12  lr: 0.000056  min_lr: 0.000001  loss: 4.2355 (4.2146)  class_acc: 0.3333 (0.3145)  loss_scale: 32768.0000 (20340.2415)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0006  max mem: 15572
Epoch: [20]  [ 390/1404]  eta: 0:10:07  lr: 0.000056  min_lr: 0.000001  loss: 4.1238 (4.2099)  class_acc: 0.3333 (0.3151)  loss_scale: 32768.0000 (20658.0870)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.0007  max mem: 15572
Epoch: [20]  [ 400/1404]  eta: 0:09:59  lr: 0.000056  min_lr: 0.000001  loss: 4.1539 (4.2085)  class_acc: 0.2917 (0.3150)  loss_scale: 32768.0000 (20960.0798)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0008  max mem: 15572
[2025-01-17 01:10:33,618] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28485
[2025-01-17 01:10:33,618] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:10:33,621] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28485
[2025-01-17 01:10:33,621] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:10:33,621] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [20]  [ 410/1404]  eta: 0:09:51  lr: 0.000056  min_lr: 0.000001  loss: 4.2669 (4.2109)  class_acc: 0.2500 (0.3140)  loss_scale: 32768.0000 (21008.1946)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0097  max mem: 15572
Epoch: [20]  [ 420/1404]  eta: 0:09:45  lr: 0.000056  min_lr: 0.000001  loss: 4.2113 (4.2084)  class_acc: 0.2917 (0.3150)  loss_scale: 16384.0000 (20898.3563)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0551  max mem: 15572
Epoch: [20]  [ 430/1404]  eta: 0:09:40  lr: 0.000056  min_lr: 0.000001  loss: 4.2442 (4.2106)  class_acc: 0.3333 (0.3153)  loss_scale: 16384.0000 (20793.6148)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1171  max mem: 15572
Epoch: [20]  [ 440/1404]  eta: 0:09:32  lr: 0.000056  min_lr: 0.000001  loss: 4.2602 (4.2107)  class_acc: 0.2917 (0.3143)  loss_scale: 16384.0000 (20693.6236)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0980  max mem: 15572
Epoch: [20]  [ 450/1404]  eta: 0:09:26  lr: 0.000056  min_lr: 0.000001  loss: 4.1097 (4.2089)  class_acc: 0.2917 (0.3148)  loss_scale: 16384.0000 (20598.0665)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0269  max mem: 15572
Epoch: [20]  [ 460/1404]  eta: 0:09:21  lr: 0.000056  min_lr: 0.000001  loss: 4.1893 (4.2105)  class_acc: 0.2917 (0.3135)  loss_scale: 16384.0000 (20506.6551)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.0038  max mem: 15572
Epoch: [20]  [ 470/1404]  eta: 0:09:14  lr: 0.000056  min_lr: 0.000001  loss: 4.2723 (4.2112)  class_acc: 0.2917 (0.3140)  loss_scale: 16384.0000 (20419.1253)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0221  max mem: 15572
Epoch: [20]  [ 480/1404]  eta: 0:09:08  lr: 0.000056  min_lr: 0.000001  loss: 4.3646 (4.2147)  class_acc: 0.2917 (0.3135)  loss_scale: 16384.0000 (20335.2349)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0485  max mem: 15572
Epoch: [20]  [ 490/1404]  eta: 0:09:02  lr: 0.000056  min_lr: 0.000001  loss: 4.0951 (4.2089)  class_acc: 0.2917 (0.3144)  loss_scale: 16384.0000 (20254.7617)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0817  max mem: 15572
Epoch: [20]  [ 500/1404]  eta: 0:08:56  lr: 0.000056  min_lr: 0.000001  loss: 4.0951 (4.2106)  class_acc: 0.3333 (0.3144)  loss_scale: 16384.0000 (20177.5010)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1020  max mem: 15572
Epoch: [20]  [ 510/1404]  eta: 0:08:53  lr: 0.000056  min_lr: 0.000001  loss: 4.2897 (4.2120)  class_acc: 0.2917 (0.3144)  loss_scale: 16384.0000 (20103.2642)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.1673  max mem: 15572
Epoch: [20]  [ 520/1404]  eta: 0:08:45  lr: 0.000056  min_lr: 0.000001  loss: 4.2184 (4.2117)  class_acc: 0.2917 (0.3135)  loss_scale: 16384.0000 (20031.8772)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1262  max mem: 15572
Epoch: [20]  [ 530/1404]  eta: 0:08:39  lr: 0.000056  min_lr: 0.000001  loss: 4.1897 (4.2131)  class_acc: 0.2917 (0.3132)  loss_scale: 16384.0000 (19963.1789)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0743  max mem: 15572
[2025-01-17 01:11:49,937] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:11:49,937] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:11:49,939] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:11:49,939] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [ 540/1404]  eta: 0:08:33  lr: 0.000056  min_lr: 0.000001  loss: 4.3755 (4.2164)  class_acc: 0.2500 (0.3118)  loss_scale: 16384.0000 (20109.0129)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1218  max mem: 15572
[2025-01-17 01:11:57,047] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28625
[2025-01-17 01:11:57,048] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:11:57,048] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 01:11:57,068] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28625
[2025-01-17 01:11:57,069] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [20]  [ 550/1404]  eta: 0:08:28  lr: 0.000056  min_lr: 0.000001  loss: 4.2966 (4.2159)  class_acc: 0.2917 (0.3120)  loss_scale: 32768.0000 (20160.3485)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1423  max mem: 15572
Epoch: [20]  [ 560/1404]  eta: 0:08:21  lr: 0.000056  min_lr: 0.000001  loss: 4.1842 (4.2152)  class_acc: 0.3333 (0.3119)  loss_scale: 16384.0000 (20093.0339)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0858  max mem: 15572
Epoch: [20]  [ 570/1404]  eta: 0:08:15  lr: 0.000056  min_lr: 0.000001  loss: 4.2862 (4.2163)  class_acc: 0.2917 (0.3117)  loss_scale: 16384.0000 (20028.0771)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.0312  max mem: 15572
Epoch: [20]  [ 580/1404]  eta: 0:08:11  lr: 0.000056  min_lr: 0.000001  loss: 4.2862 (4.2172)  class_acc: 0.2917 (0.3114)  loss_scale: 16384.0000 (19965.3563)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.0312  max mem: 15572
Epoch: [20]  [ 590/1404]  eta: 0:08:04  lr: 0.000056  min_lr: 0.000001  loss: 4.1343 (4.2149)  class_acc: 0.3333 (0.3127)  loss_scale: 16384.0000 (19904.7580)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.0006  max mem: 15572
Epoch: [20]  [ 600/1404]  eta: 0:07:57  lr: 0.000056  min_lr: 0.000001  loss: 4.1343 (4.2161)  class_acc: 0.3333 (0.3127)  loss_scale: 16384.0000 (19846.1764)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0007  max mem: 15572
Epoch: [20]  [ 610/1404]  eta: 0:07:51  lr: 0.000056  min_lr: 0.000001  loss: 4.2783 (4.2157)  class_acc: 0.3333 (0.3129)  loss_scale: 16384.0000 (19789.5123)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0008  max mem: 15572
Epoch: [20]  [ 620/1404]  eta: 0:07:45  lr: 0.000056  min_lr: 0.000001  loss: 4.2783 (4.2165)  class_acc: 0.2917 (0.3135)  loss_scale: 16384.0000 (19734.6731)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0008  max mem: 15572
Epoch: [20]  [ 630/1404]  eta: 0:07:38  lr: 0.000055  min_lr: 0.000001  loss: 4.2040 (4.2168)  class_acc: 0.2917 (0.3127)  loss_scale: 16384.0000 (19681.5721)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0007  max mem: 15572
Epoch: [20]  [ 640/1404]  eta: 0:07:33  lr: 0.000055  min_lr: 0.000001  loss: 4.2040 (4.2168)  class_acc: 0.2500 (0.3125)  loss_scale: 16384.0000 (19630.1279)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0107  max mem: 15572
Epoch: [20]  [ 650/1404]  eta: 0:07:28  lr: 0.000055  min_lr: 0.000001  loss: 4.2415 (4.2173)  class_acc: 0.2500 (0.3121)  loss_scale: 16384.0000 (19580.2642)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.0145  max mem: 15572
Epoch: [20]  [ 660/1404]  eta: 0:07:22  lr: 0.000055  min_lr: 0.000001  loss: 4.2600 (4.2155)  class_acc: 0.2917 (0.3126)  loss_scale: 16384.0000 (19531.9092)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.0042  max mem: 15572
Epoch: [20]  [ 670/1404]  eta: 0:07:15  lr: 0.000055  min_lr: 0.000001  loss: 4.2220 (4.2173)  class_acc: 0.2917 (0.3126)  loss_scale: 16384.0000 (19484.9955)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0006  max mem: 15572
[2025-01-17 01:13:12,066] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:13:12,066] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:13:12,088] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:13:12,089] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [ 680/1404]  eta: 0:07:09  lr: 0.000055  min_lr: 0.000001  loss: 4.2220 (4.2172)  class_acc: 0.2917 (0.3129)  loss_scale: 16384.0000 (19607.8708)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0009  max mem: 15572
Epoch: [20]  [ 690/1404]  eta: 0:07:03  lr: 0.000055  min_lr: 0.000001  loss: 4.1851 (4.2162)  class_acc: 0.2917 (0.3127)  loss_scale: 32768.0000 (19798.3213)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0008  max mem: 15572
Epoch: [20]  [ 700/1404]  eta: 0:06:57  lr: 0.000055  min_lr: 0.000001  loss: 4.1651 (4.2151)  class_acc: 0.2500 (0.3125)  loss_scale: 32768.0000 (19983.3381)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0008  max mem: 15572
Epoch: [20]  [ 710/1404]  eta: 0:06:51  lr: 0.000055  min_lr: 0.000001  loss: 4.1370 (4.2161)  class_acc: 0.2917 (0.3128)  loss_scale: 32768.0000 (20163.1505)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0008  max mem: 15572
Epoch: [20]  [ 720/1404]  eta: 0:06:46  lr: 0.000055  min_lr: 0.000001  loss: 4.1336 (4.2142)  class_acc: 0.3750 (0.3134)  loss_scale: 32768.0000 (20337.9750)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0005  max mem: 15572
Epoch: [20]  [ 730/1404]  eta: 0:06:40  lr: 0.000055  min_lr: 0.000001  loss: 4.1372 (4.2136)  class_acc: 0.2917 (0.3131)  loss_scale: 32768.0000 (20508.0164)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.0007  max mem: 15572
Epoch: [20]  [ 740/1404]  eta: 0:06:33  lr: 0.000055  min_lr: 0.000001  loss: 4.2853 (4.2145)  class_acc: 0.2500 (0.3127)  loss_scale: 32768.0000 (20673.4683)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0008  max mem: 15572
[2025-01-17 01:13:52,002] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28822
[2025-01-17 01:13:52,003] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:13:52,004] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28822
[2025-01-17 01:13:52,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:13:52,004] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [20]  [ 750/1404]  eta: 0:06:27  lr: 0.000055  min_lr: 0.000001  loss: 4.3388 (4.2159)  class_acc: 0.2500 (0.3116)  loss_scale: 32768.0000 (20638.1678)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0070  max mem: 15572
Epoch: [20]  [ 760/1404]  eta: 0:06:21  lr: 0.000055  min_lr: 0.000001  loss: 4.2494 (4.2159)  class_acc: 0.2083 (0.3114)  loss_scale: 16384.0000 (20582.2654)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0068  max mem: 15572
Epoch: [20]  [ 770/1404]  eta: 0:06:15  lr: 0.000055  min_lr: 0.000001  loss: 4.2474 (4.2159)  class_acc: 0.3750 (0.3127)  loss_scale: 16384.0000 (20527.8132)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0005  max mem: 15572
Epoch: [20]  [ 780/1404]  eta: 0:06:09  lr: 0.000055  min_lr: 0.000001  loss: 4.1897 (4.2157)  class_acc: 0.4167 (0.3139)  loss_scale: 16384.0000 (20474.7554)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0005  max mem: 15572
Epoch: [20]  [ 790/1404]  eta: 0:06:04  lr: 0.000055  min_lr: 0.000001  loss: 4.2030 (4.2161)  class_acc: 0.3750 (0.3141)  loss_scale: 16384.0000 (20423.0392)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.0007  max mem: 15572
Epoch: [20]  [ 800/1404]  eta: 0:05:58  lr: 0.000055  min_lr: 0.000001  loss: 4.1450 (4.2165)  class_acc: 0.3750 (0.3146)  loss_scale: 16384.0000 (20372.6142)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0007  max mem: 15572
Epoch: [20]  [ 810/1404]  eta: 0:05:51  lr: 0.000055  min_lr: 0.000001  loss: 4.1199 (4.2173)  class_acc: 0.3750 (0.3147)  loss_scale: 16384.0000 (20323.4328)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0006  max mem: 15572
Epoch: [20]  [ 820/1404]  eta: 0:05:46  lr: 0.000055  min_lr: 0.000001  loss: 4.2580 (4.2174)  class_acc: 0.2917 (0.3152)  loss_scale: 16384.0000 (20275.4495)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0005  max mem: 15572
Epoch: [20]  [ 830/1404]  eta: 0:05:39  lr: 0.000055  min_lr: 0.000001  loss: 4.2343 (4.2170)  class_acc: 0.3333 (0.3157)  loss_scale: 16384.0000 (20228.6209)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0007  max mem: 15572
Epoch: [20]  [ 840/1404]  eta: 0:05:34  lr: 0.000055  min_lr: 0.000001  loss: 4.2434 (4.2182)  class_acc: 0.2917 (0.3147)  loss_scale: 16384.0000 (20182.9061)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0009  max mem: 15572
Epoch: [20]  [ 850/1404]  eta: 0:05:28  lr: 0.000055  min_lr: 0.000001  loss: 4.2294 (4.2168)  class_acc: 0.2917 (0.3156)  loss_scale: 16384.0000 (20138.2656)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.0008  max mem: 15572
Epoch: [20]  [ 860/1404]  eta: 0:05:22  lr: 0.000055  min_lr: 0.000001  loss: 4.2082 (4.2170)  class_acc: 0.3750 (0.3161)  loss_scale: 16384.0000 (20094.6620)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0007  max mem: 15572
Epoch: [20]  [ 870/1404]  eta: 0:05:16  lr: 0.000055  min_lr: 0.000001  loss: 4.0448 (4.2141)  class_acc: 0.3750 (0.3169)  loss_scale: 16384.0000 (20052.0597)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0005  max mem: 15572
[2025-01-17 01:15:08,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:15:08,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:15:08,068] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:15:08,068] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [ 880/1404]  eta: 0:05:10  lr: 0.000055  min_lr: 0.000001  loss: 4.1630 (4.2149)  class_acc: 0.3333 (0.3167)  loss_scale: 16384.0000 (20196.3950)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0006  max mem: 15572
Epoch: [20]  [ 890/1404]  eta: 0:05:04  lr: 0.000055  min_lr: 0.000001  loss: 4.3152 (4.2164)  class_acc: 0.2917 (0.3161)  loss_scale: 32768.0000 (20337.4905)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0007  max mem: 15572
Epoch: [20]  [ 900/1404]  eta: 0:04:58  lr: 0.000055  min_lr: 0.000001  loss: 4.3282 (4.2172)  class_acc: 0.2500 (0.3159)  loss_scale: 32768.0000 (20475.4539)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0009  max mem: 15572
Epoch: [20]  [ 910/1404]  eta: 0:04:52  lr: 0.000055  min_lr: 0.000001  loss: 4.1817 (4.2146)  class_acc: 0.2917 (0.3156)  loss_scale: 32768.0000 (20610.3886)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.0009  max mem: 15572
[2025-01-17 01:15:35,266] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=172, lr=[5.29434136511386e-07, 5.29434136511386e-07, 7.563344807305515e-07, 7.563344807305515e-07, 1.0804778296150736e-06, 1.0804778296150736e-06, 1.5435397565929626e-06, 1.5435397565929626e-06, 2.2050567951328037e-06, 2.2050567951328037e-06, 3.1500811359040052e-06, 3.1500811359040052e-06, 4.500115908434293e-06, 4.500115908434293e-06, 6.4287370120489915e-06, 6.4287370120489915e-06, 9.183910017212845e-06, 9.183910017212845e-06, 1.3119871453161209e-05, 1.3119871453161209e-05, 1.874267350451601e-05, 1.874267350451601e-05, 2.6775247863594308e-05, 2.6775247863594308e-05, 3.825035409084901e-05, 3.825035409084901e-05, 5.464336298692716e-05, 5.464336298692716e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 01:15:35,268] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=47.873393843109405, CurrSamplesPerSec=45.03229281394748, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [20]  [ 920/1404]  eta: 0:04:46  lr: 0.000055  min_lr: 0.000001  loss: 4.1814 (4.2154)  class_acc: 0.2917 (0.3152)  loss_scale: 32768.0000 (20742.3931)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0007  max mem: 15572
Epoch: [20]  [ 930/1404]  eta: 0:04:39  lr: 0.000055  min_lr: 0.000001  loss: 4.2496 (4.2147)  class_acc: 0.2917 (0.3154)  loss_scale: 32768.0000 (20871.5618)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0007  max mem: 15572
Epoch: [20]  [ 940/1404]  eta: 0:04:33  lr: 0.000055  min_lr: 0.000001  loss: 4.3266 (4.2158)  class_acc: 0.2917 (0.3153)  loss_scale: 32768.0000 (20997.9851)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0007  max mem: 15572
Epoch: [20]  [ 950/1404]  eta: 0:04:27  lr: 0.000055  min_lr: 0.000001  loss: 4.2462 (4.2157)  class_acc: 0.2083 (0.3149)  loss_scale: 32768.0000 (21121.7497)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0007  max mem: 15572
Epoch: [20]  [ 960/1404]  eta: 0:04:21  lr: 0.000055  min_lr: 0.000001  loss: 4.2081 (4.2150)  class_acc: 0.2083 (0.3146)  loss_scale: 32768.0000 (21242.9386)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0590  max mem: 15572
Epoch: [20]  [ 970/1404]  eta: 0:04:16  lr: 0.000054  min_lr: 0.000001  loss: 4.2708 (4.2161)  class_acc: 0.2083 (0.3136)  loss_scale: 32768.0000 (21361.6313)  weight_decay: 0.0500 (0.0500)  time: 0.6553  data: 0.1398  max mem: 15572
[2025-01-17 01:16:08,949] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29056
[2025-01-17 01:16:08,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:16:08,949] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 01:16:08,951] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29056
[2025-01-17 01:16:08,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [20]  [ 980/1404]  eta: 0:04:10  lr: 0.000054  min_lr: 0.000001  loss: 4.3956 (4.2182)  class_acc: 0.2083 (0.3138)  loss_scale: 32768.0000 (21394.3976)  weight_decay: 0.0500 (0.0500)  time: 0.6468  data: 0.1130  max mem: 15572
Epoch: [20]  [ 990/1404]  eta: 0:04:04  lr: 0.000054  min_lr: 0.000001  loss: 4.3039 (4.2197)  class_acc: 0.2917 (0.3132)  loss_scale: 16384.0000 (21343.8385)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.0857  max mem: 15572
Epoch: [20]  [1000/1404]  eta: 0:03:59  lr: 0.000054  min_lr: 0.000001  loss: 4.2965 (4.2208)  class_acc: 0.2500 (0.3133)  loss_scale: 16384.0000 (21294.2897)  weight_decay: 0.0500 (0.0500)  time: 0.6432  data: 0.1573  max mem: 15572
Epoch: [20]  [1010/1404]  eta: 0:03:52  lr: 0.000054  min_lr: 0.000001  loss: 4.2965 (4.2207)  class_acc: 0.2500 (0.3133)  loss_scale: 16384.0000 (21245.7211)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1035  max mem: 15572
Epoch: [20]  [1020/1404]  eta: 0:03:46  lr: 0.000054  min_lr: 0.000001  loss: 4.3319 (4.2216)  class_acc: 0.2500 (0.3127)  loss_scale: 16384.0000 (21198.1038)  weight_decay: 0.0500 (0.0500)  time: 0.5120  data: 0.0076  max mem: 15572
Epoch: [20]  [1030/1404]  eta: 0:03:40  lr: 0.000054  min_lr: 0.000001  loss: 4.2881 (4.2218)  class_acc: 0.2917 (0.3128)  loss_scale: 16384.0000 (21151.4103)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.0390  max mem: 15572
Epoch: [20]  [1040/1404]  eta: 0:03:34  lr: 0.000054  min_lr: 0.000001  loss: 4.1215 (4.2217)  class_acc: 0.2917 (0.3127)  loss_scale: 16384.0000 (21105.6138)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0452  max mem: 15572
Epoch: [20]  [1050/1404]  eta: 0:03:29  lr: 0.000054  min_lr: 0.000001  loss: 4.0921 (4.2200)  class_acc: 0.2917 (0.3130)  loss_scale: 16384.0000 (21060.6889)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0605  max mem: 15572
Epoch: [20]  [1060/1404]  eta: 0:03:23  lr: 0.000054  min_lr: 0.000001  loss: 4.2492 (4.2218)  class_acc: 0.2500 (0.3122)  loss_scale: 16384.0000 (21016.6107)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0602  max mem: 15572
Epoch: [20]  [1070/1404]  eta: 0:03:17  lr: 0.000054  min_lr: 0.000001  loss: 4.2492 (4.2213)  class_acc: 0.2083 (0.3117)  loss_scale: 16384.0000 (20973.3557)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.0648  max mem: 15572
Epoch: [20]  [1080/1404]  eta: 0:03:11  lr: 0.000054  min_lr: 0.000001  loss: 4.2298 (4.2226)  class_acc: 0.2500 (0.3113)  loss_scale: 16384.0000 (20930.9010)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1006  max mem: 15572
Epoch: [20]  [1090/1404]  eta: 0:03:05  lr: 0.000054  min_lr: 0.000001  loss: 4.2506 (4.2211)  class_acc: 0.2917 (0.3116)  loss_scale: 16384.0000 (20889.2246)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0494  max mem: 15572
Epoch: [20]  [1100/1404]  eta: 0:02:59  lr: 0.000054  min_lr: 0.000001  loss: 4.2289 (4.2212)  class_acc: 0.3750 (0.3120)  loss_scale: 16384.0000 (20848.3052)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0008  max mem: 15572
[2025-01-17 01:17:23,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:17:23,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:17:23,770] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:17:23,770] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [1110/1404]  eta: 0:02:53  lr: 0.000054  min_lr: 0.000001  loss: 4.2459 (4.2208)  class_acc: 0.3333 (0.3120)  loss_scale: 16384.0000 (20896.6049)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.0011  max mem: 15572
Epoch: [20]  [1120/1404]  eta: 0:02:47  lr: 0.000054  min_lr: 0.000001  loss: 4.2000 (4.2203)  class_acc: 0.3333 (0.3120)  loss_scale: 32768.0000 (21002.5049)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0010  max mem: 15572
Epoch: [20]  [1130/1404]  eta: 0:02:41  lr: 0.000054  min_lr: 0.000001  loss: 4.2000 (4.2196)  class_acc: 0.3333 (0.3118)  loss_scale: 32768.0000 (21106.5323)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0008  max mem: 15572
Epoch: [20]  [1140/1404]  eta: 0:02:35  lr: 0.000054  min_lr: 0.000001  loss: 4.2168 (4.2197)  class_acc: 0.2917 (0.3116)  loss_scale: 32768.0000 (21208.7362)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.0007  max mem: 15572
Epoch: [20]  [1150/1404]  eta: 0:02:29  lr: 0.000054  min_lr: 0.000001  loss: 4.2351 (4.2201)  class_acc: 0.2500 (0.3116)  loss_scale: 32768.0000 (21309.1642)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0008  max mem: 15572
Epoch: [20]  [1160/1404]  eta: 0:02:23  lr: 0.000054  min_lr: 0.000001  loss: 4.2059 (4.2197)  class_acc: 0.2917 (0.3119)  loss_scale: 32768.0000 (21407.8622)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0008  max mem: 15572
Epoch: [20]  [1170/1404]  eta: 0:02:17  lr: 0.000054  min_lr: 0.000001  loss: 4.1690 (4.2189)  class_acc: 0.3333 (0.3119)  loss_scale: 32768.0000 (21504.8745)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0006  max mem: 15572
Epoch: [20]  [1180/1404]  eta: 0:02:11  lr: 0.000054  min_lr: 0.000001  loss: 4.0948 (4.2182)  class_acc: 0.3333 (0.3120)  loss_scale: 32768.0000 (21600.2439)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0007  max mem: 15572
Epoch: [20]  [1190/1404]  eta: 0:02:06  lr: 0.000054  min_lr: 0.000001  loss: 4.1709 (4.2185)  class_acc: 0.2917 (0.3118)  loss_scale: 32768.0000 (21694.0118)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0006  max mem: 15572
[2025-01-17 01:18:16,903] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29277
[2025-01-17 01:18:16,904] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:18:16,906] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29277
[2025-01-17 01:18:16,907] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:18:16,907] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [20]  [1200/1404]  eta: 0:02:00  lr: 0.000054  min_lr: 0.000001  loss: 4.1942 (4.2184)  class_acc: 0.2917 (0.3120)  loss_scale: 32768.0000 (21731.6503)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.0006  max mem: 15572
Epoch: [20]  [1210/1404]  eta: 0:01:54  lr: 0.000054  min_lr: 0.000001  loss: 4.2910 (4.2195)  class_acc: 0.2500 (0.3112)  loss_scale: 16384.0000 (21687.4913)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.0254  max mem: 15572
Epoch: [20]  [1220/1404]  eta: 0:01:48  lr: 0.000054  min_lr: 0.000001  loss: 4.2910 (4.2194)  class_acc: 0.2083 (0.3108)  loss_scale: 16384.0000 (21644.0557)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0254  max mem: 15572
Epoch: [20]  [1230/1404]  eta: 0:01:42  lr: 0.000054  min_lr: 0.000001  loss: 4.2165 (4.2196)  class_acc: 0.2500 (0.3106)  loss_scale: 16384.0000 (21601.3258)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0007  max mem: 15572
Epoch: [20]  [1240/1404]  eta: 0:01:36  lr: 0.000054  min_lr: 0.000001  loss: 4.2709 (4.2196)  class_acc: 0.2500 (0.3101)  loss_scale: 16384.0000 (21559.2844)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0148  max mem: 15572
Epoch: [20]  [1250/1404]  eta: 0:01:30  lr: 0.000054  min_lr: 0.000001  loss: 4.2709 (4.2197)  class_acc: 0.2500 (0.3101)  loss_scale: 16384.0000 (21517.9153)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0217  max mem: 15572
Epoch: [20]  [1260/1404]  eta: 0:01:24  lr: 0.000054  min_lr: 0.000001  loss: 4.2175 (4.2196)  class_acc: 0.2917 (0.3099)  loss_scale: 16384.0000 (21477.2022)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.0366  max mem: 15572
Epoch: [20]  [1270/1404]  eta: 0:01:18  lr: 0.000054  min_lr: 0.000001  loss: 4.2365 (4.2196)  class_acc: 0.2917 (0.3100)  loss_scale: 16384.0000 (21437.1298)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.0298  max mem: 15572
Epoch: [20]  [1280/1404]  eta: 0:01:13  lr: 0.000054  min_lr: 0.000001  loss: 4.1371 (4.2181)  class_acc: 0.3750 (0.3108)  loss_scale: 16384.0000 (21397.6831)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0008  max mem: 15572
Epoch: [20]  [1290/1404]  eta: 0:01:07  lr: 0.000054  min_lr: 0.000001  loss: 4.1328 (4.2186)  class_acc: 0.3333 (0.3105)  loss_scale: 16384.0000 (21358.8474)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0631  max mem: 15572
Epoch: [20]  [1300/1404]  eta: 0:01:01  lr: 0.000054  min_lr: 0.000001  loss: 4.2640 (4.2185)  class_acc: 0.2917 (0.3107)  loss_scale: 16384.0000 (21320.6088)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1177  max mem: 15572
Epoch: [20]  [1310/1404]  eta: 0:00:55  lr: 0.000053  min_lr: 0.000001  loss: 4.2356 (4.2190)  class_acc: 0.3333 (0.3111)  loss_scale: 16384.0000 (21282.9535)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1075  max mem: 15572
Epoch: [20]  [1320/1404]  eta: 0:00:49  lr: 0.000053  min_lr: 0.000001  loss: 4.1058 (4.2179)  class_acc: 0.3333 (0.3115)  loss_scale: 16384.0000 (21245.8683)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0634  max mem: 15572
[2025-01-17 01:19:32,489] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:19:32,489] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:19:32,492] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:19:32,492] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [1330/1404]  eta: 0:00:43  lr: 0.000053  min_lr: 0.000001  loss: 4.0896 (4.2182)  class_acc: 0.3333 (0.3115)  loss_scale: 16384.0000 (21270.8881)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0375  max mem: 15572
[2025-01-17 01:19:36,548] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29412
[2025-01-17 01:19:36,548] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:19:36,603] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29412
[2025-01-17 01:19:36,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:19:36,603] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [20]  [1340/1404]  eta: 0:00:37  lr: 0.000053  min_lr: 0.000001  loss: 4.1612 (4.2179)  class_acc: 0.2917 (0.3115)  loss_scale: 16384.0000 (21246.6637)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1378  max mem: 15572
Epoch: [20]  [1350/1404]  eta: 0:00:31  lr: 0.000053  min_lr: 0.000001  loss: 4.1743 (4.2179)  class_acc: 0.2500 (0.3111)  loss_scale: 16384.0000 (21210.6706)  weight_decay: 0.0500 (0.0500)  time: 0.6560  data: 0.1271  max mem: 15572
Epoch: [20]  [1360/1404]  eta: 0:00:25  lr: 0.000053  min_lr: 0.000001  loss: 4.2181 (4.2180)  class_acc: 0.2917 (0.3111)  loss_scale: 16384.0000 (21175.2065)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0162  max mem: 15572
Epoch: [20]  [1370/1404]  eta: 0:00:20  lr: 0.000053  min_lr: 0.000001  loss: 4.1585 (4.2163)  class_acc: 0.3333 (0.3115)  loss_scale: 16384.0000 (21140.2597)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0175  max mem: 15572
Epoch: [20]  [1380/1404]  eta: 0:00:14  lr: 0.000053  min_lr: 0.000001  loss: 4.0140 (4.2165)  class_acc: 0.3333 (0.3116)  loss_scale: 16384.0000 (21105.8190)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0720  max mem: 15572
Epoch: [20]  [1390/1404]  eta: 0:00:08  lr: 0.000053  min_lr: 0.000001  loss: 4.1400 (4.2159)  class_acc: 0.2500 (0.3115)  loss_scale: 16384.0000 (21071.8735)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1204  max mem: 15572
Epoch: [20]  [1400/1404]  eta: 0:00:02  lr: 0.000053  min_lr: 0.000001  loss: 4.2243 (4.2166)  class_acc: 0.2500 (0.3112)  loss_scale: 16384.0000 (21038.4126)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0657  max mem: 15572
Epoch: [20]  [1403/1404]  eta: 0:00:00  lr: 0.000053  min_lr: 0.000001  loss: 4.3145 (4.2169)  class_acc: 0.2500 (0.3110)  loss_scale: 16384.0000 (21028.4672)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.0655  max mem: 15572
Epoch: [20] Total time: 0:13:44 (0.5873 s / it)
Averaged stats: lr: 0.000053  min_lr: 0.000001  loss: 4.3145 (4.2130)  class_acc: 0.2500 (0.3090)  loss_scale: 16384.0000 (21028.4672)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:08:42  loss: 1.7706 (1.7706)  acc1: 61.1111 (61.1111)  acc5: 77.7778 (77.7778)  time: 3.8432  data: 3.4719  max mem: 15572
Val:  [ 10/136]  eta: 0:01:29  loss: 2.4681 (2.4261)  acc1: 50.0000 (45.9596)  acc5: 72.2222 (76.7677)  time: 0.7095  data: 0.4879  max mem: 15572
Val:  [ 20/136]  eta: 0:01:00  loss: 2.5033 (2.4823)  acc1: 44.4444 (43.6508)  acc5: 72.2222 (75.9259)  time: 0.3521  data: 0.1493  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 2.4125 (2.3876)  acc1: 38.8889 (45.6989)  acc5: 83.3333 (77.4194)  time: 0.3412  data: 0.1461  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 2.0133 (2.3358)  acc1: 55.5556 (48.1030)  acc5: 83.3333 (78.8618)  time: 0.3640  data: 0.1638  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.3375 (2.3711)  acc1: 44.4444 (46.8410)  acc5: 83.3333 (78.8671)  time: 0.3651  data: 0.1648  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.4687 (2.4368)  acc1: 38.8889 (45.1730)  acc5: 72.2222 (77.1403)  time: 0.3658  data: 0.1754  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.3804 (2.4172)  acc1: 50.0000 (46.2441)  acc5: 77.7778 (77.8560)  time: 0.3366  data: 0.1490  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.3583 (2.4234)  acc1: 50.0000 (46.0219)  acc5: 77.7778 (78.0521)  time: 0.3878  data: 0.1810  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.4641 (2.4315)  acc1: 50.0000 (45.9096)  acc5: 77.7778 (77.7778)  time: 0.4028  data: 0.1850  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5144 (2.4976)  acc1: 33.3333 (43.8944)  acc5: 66.6667 (75.9626)  time: 0.3633  data: 0.1547  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.7123 (2.4969)  acc1: 33.3333 (44.0941)  acc5: 66.6667 (75.9760)  time: 0.3706  data: 0.1722  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.3486 (2.4655)  acc1: 44.4444 (44.9954)  acc5: 77.7778 (76.5840)  time: 0.3401  data: 0.1539  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.0455 (2.4337)  acc1: 50.0000 (46.0984)  acc5: 83.3333 (77.0992)  time: 0.2527  data: 0.0843  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.1177 (2.4337)  acc1: 50.0000 (46.1916)  acc5: 77.7778 (77.0680)  time: 0.1689  data: 0.0183  max mem: 15572
Val: Total time: 0:00:50 (0.3677 s / it)
* Acc@1 44.861 Acc@5 75.942 loss 2.480
Accuracy of the network on the 4883 val videos: 44.9%
[2025-01-17 01:21:05,675] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 01:21:05,677] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 01:21:05,677] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 01:21:05,677] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 01:21:08,101] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 01:21:08,102] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.86%
Epoch: [21]  [   0/1404]  eta: 2:57:23  lr: 0.000053  min_lr: 0.000001  loss: 3.9056 (3.9056)  class_acc: 0.4583 (0.4583)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.5808  data: 6.4983  max mem: 15572
Epoch: [21]  [  10/1404]  eta: 0:28:08  lr: 0.000053  min_lr: 0.000001  loss: 4.4439 (4.3859)  class_acc: 0.2917 (0.2689)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2113  data: 0.6106  max mem: 15572
Epoch: [21]  [  20/1404]  eta: 0:21:03  lr: 0.000053  min_lr: 0.000001  loss: 4.2905 (4.2921)  class_acc: 0.2917 (0.2917)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0112  max mem: 15572
Epoch: [21]  [  30/1404]  eta: 0:18:17  lr: 0.000053  min_lr: 0.000001  loss: 4.2905 (4.3019)  class_acc: 0.2917 (0.2917)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0006  max mem: 15572
Epoch: [21]  [  40/1404]  eta: 0:16:52  lr: 0.000053  min_lr: 0.000001  loss: 4.1406 (4.2436)  class_acc: 0.3333 (0.3252)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0084  max mem: 15572
Epoch: [21]  [  50/1404]  eta: 0:16:19  lr: 0.000053  min_lr: 0.000001  loss: 4.0594 (4.2138)  class_acc: 0.3750 (0.3317)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0481  max mem: 15572
[2025-01-17 01:21:48,227] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:21:48,227] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:21:48,227] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:21:48,227] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [  60/1404]  eta: 0:15:55  lr: 0.000053  min_lr: 0.000001  loss: 4.0837 (4.2075)  class_acc: 0.3333 (0.3238)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.0403  max mem: 15572
Epoch: [21]  [  70/1404]  eta: 0:15:16  lr: 0.000053  min_lr: 0.000001  loss: 4.0849 (4.1897)  class_acc: 0.3333 (0.3263)  loss_scale: 32768.0000 (19614.6479)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0006  max mem: 15572
Epoch: [21]  [  80/1404]  eta: 0:14:51  lr: 0.000053  min_lr: 0.000001  loss: 4.0900 (4.1944)  class_acc: 0.3333 (0.3251)  loss_scale: 32768.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0006  max mem: 15572
Epoch: [21]  [  90/1404]  eta: 0:14:45  lr: 0.000053  min_lr: 0.000001  loss: 4.2570 (4.2000)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (22505.4945)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.0010  max mem: 15572
Epoch: [21]  [ 100/1404]  eta: 0:14:21  lr: 0.000053  min_lr: 0.000001  loss: 4.1199 (4.1861)  class_acc: 0.3333 (0.3296)  loss_scale: 32768.0000 (23521.5842)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0010  max mem: 15572
Epoch: [21]  [ 110/1404]  eta: 0:13:59  lr: 0.000053  min_lr: 0.000001  loss: 4.1199 (4.1963)  class_acc: 0.3333 (0.3273)  loss_scale: 32768.0000 (24354.5946)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0006  max mem: 15572
Epoch: [21]  [ 120/1404]  eta: 0:13:47  lr: 0.000053  min_lr: 0.000001  loss: 4.2072 (4.1910)  class_acc: 0.2917 (0.3251)  loss_scale: 32768.0000 (25049.9174)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.0004  max mem: 15572
Epoch: [21]  [ 130/1404]  eta: 0:13:35  lr: 0.000053  min_lr: 0.000001  loss: 4.0272 (4.1805)  class_acc: 0.3333 (0.3302)  loss_scale: 32768.0000 (25639.0840)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0004  max mem: 15572
Epoch: [21]  [ 140/1404]  eta: 0:13:21  lr: 0.000053  min_lr: 0.000001  loss: 4.0799 (4.1882)  class_acc: 0.3750 (0.3342)  loss_scale: 32768.0000 (26144.6809)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0157  max mem: 15572
Epoch: [21]  [ 150/1404]  eta: 0:13:09  lr: 0.000053  min_lr: 0.000001  loss: 4.1868 (4.1958)  class_acc: 0.3333 (0.3331)  loss_scale: 32768.0000 (26583.3113)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0157  max mem: 15572
Epoch: [21]  [ 160/1404]  eta: 0:12:54  lr: 0.000053  min_lr: 0.000001  loss: 4.2364 (4.2020)  class_acc: 0.3333 (0.3354)  loss_scale: 32768.0000 (26967.4534)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0007  max mem: 15572
Epoch: [21]  [ 170/1404]  eta: 0:12:41  lr: 0.000053  min_lr: 0.000001  loss: 4.2406 (4.2053)  class_acc: 0.2917 (0.3348)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0062  max mem: 15572
[2025-01-17 01:22:59,383] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29663
[2025-01-17 01:22:59,384] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:22:59,392] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29663
[2025-01-17 01:22:59,393] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:22:59,394] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [ 180/1404]  eta: 0:12:35  lr: 0.000053  min_lr: 0.000001  loss: 4.3011 (4.2103)  class_acc: 0.3333 (0.3354)  loss_scale: 32768.0000 (27427.3591)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0654  max mem: 15572
Epoch: [21]  [ 190/1404]  eta: 0:12:24  lr: 0.000053  min_lr: 0.000001  loss: 4.2632 (4.2088)  class_acc: 0.2917 (0.3320)  loss_scale: 16384.0000 (26849.1728)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0599  max mem: 15572
Epoch: [21]  [ 200/1404]  eta: 0:12:20  lr: 0.000053  min_lr: 0.000001  loss: 4.2632 (4.2158)  class_acc: 0.2083 (0.3257)  loss_scale: 16384.0000 (26328.5174)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0734  max mem: 15572
Epoch: [21]  [ 210/1404]  eta: 0:12:15  lr: 0.000053  min_lr: 0.000001  loss: 4.3617 (4.2204)  class_acc: 0.2500 (0.3250)  loss_scale: 16384.0000 (25857.2133)  weight_decay: 0.0500 (0.0500)  time: 0.6376  data: 0.1267  max mem: 15572
Epoch: [21]  [ 220/1404]  eta: 0:12:04  lr: 0.000053  min_lr: 0.000001  loss: 4.3988 (4.2235)  class_acc: 0.3333 (0.3254)  loss_scale: 16384.0000 (25428.5611)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0800  max mem: 15572
Epoch: [21]  [ 230/1404]  eta: 0:12:01  lr: 0.000053  min_lr: 0.000001  loss: 4.1916 (4.2218)  class_acc: 0.3333 (0.3212)  loss_scale: 16384.0000 (25037.0216)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1214  max mem: 15572
Epoch: [21]  [ 240/1404]  eta: 0:11:52  lr: 0.000052  min_lr: 0.000001  loss: 4.2892 (4.2290)  class_acc: 0.3333 (0.3216)  loss_scale: 16384.0000 (24677.9751)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1240  max mem: 15572
Epoch: [21]  [ 250/1404]  eta: 0:11:44  lr: 0.000052  min_lr: 0.000001  loss: 4.2703 (4.2298)  class_acc: 0.3333 (0.3229)  loss_scale: 16384.0000 (24347.5378)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0585  max mem: 15572
Epoch: [21]  [ 260/1404]  eta: 0:11:35  lr: 0.000052  min_lr: 0.000001  loss: 4.2049 (4.2257)  class_acc: 0.3750 (0.3255)  loss_scale: 16384.0000 (24042.4215)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0652  max mem: 15572
Epoch: [21]  [ 270/1404]  eta: 0:11:26  lr: 0.000052  min_lr: 0.000001  loss: 4.2049 (4.2233)  class_acc: 0.3333 (0.3241)  loss_scale: 16384.0000 (23759.8229)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0715  max mem: 15572
Epoch: [21]  [ 280/1404]  eta: 0:11:21  lr: 0.000052  min_lr: 0.000001  loss: 4.1980 (4.2185)  class_acc: 0.2917 (0.3253)  loss_scale: 16384.0000 (23497.3381)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1045  max mem: 15572
Epoch: [21]  [ 290/1404]  eta: 0:11:14  lr: 0.000052  min_lr: 0.000001  loss: 4.2195 (4.2192)  class_acc: 0.3333 (0.3265)  loss_scale: 16384.0000 (23252.8935)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1235  max mem: 15572
Epoch: [21]  [ 300/1404]  eta: 0:11:10  lr: 0.000052  min_lr: 0.000001  loss: 4.1635 (4.2183)  class_acc: 0.3750 (0.3266)  loss_scale: 16384.0000 (23024.6910)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1320  max mem: 15572
[2025-01-17 01:24:16,022] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:24:16,022] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:24:16,022] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:24:16,022] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [ 310/1404]  eta: 0:11:04  lr: 0.000052  min_lr: 0.000001  loss: 4.1671 (4.2206)  class_acc: 0.2917 (0.3250)  loss_scale: 16384.0000 (22969.2090)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1141  max mem: 15572
Epoch: [21]  [ 320/1404]  eta: 0:10:55  lr: 0.000052  min_lr: 0.000001  loss: 4.2682 (4.2213)  class_acc: 0.2917 (0.3254)  loss_scale: 32768.0000 (23274.4673)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0468  max mem: 15572
Epoch: [21]  [ 330/1404]  eta: 0:10:45  lr: 0.000052  min_lr: 0.000001  loss: 4.2571 (4.2212)  class_acc: 0.2917 (0.3253)  loss_scale: 32768.0000 (23561.2810)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0101  max mem: 15572
Epoch: [21]  [ 340/1404]  eta: 0:10:39  lr: 0.000052  min_lr: 0.000001  loss: 4.3102 (4.2257)  class_acc: 0.2500 (0.3226)  loss_scale: 32768.0000 (23831.2727)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0407  max mem: 15572
Epoch: [21]  [ 350/1404]  eta: 0:10:32  lr: 0.000052  min_lr: 0.000001  loss: 4.3102 (4.2253)  class_acc: 0.2500 (0.3225)  loss_scale: 32768.0000 (24085.8803)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0579  max mem: 15572
[2025-01-17 01:24:40,201] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29837
[2025-01-17 01:24:40,202] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:24:40,237] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29837
[2025-01-17 01:24:40,238] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:24:40,238] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [ 360/1404]  eta: 0:10:29  lr: 0.000052  min_lr: 0.000001  loss: 4.2733 (4.2247)  class_acc: 0.3333 (0.3224)  loss_scale: 32768.0000 (23963.3019)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1387  max mem: 15572
Epoch: [21]  [ 370/1404]  eta: 0:10:22  lr: 0.000052  min_lr: 0.000001  loss: 4.1171 (4.2218)  class_acc: 0.3333 (0.3231)  loss_scale: 16384.0000 (23759.0081)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1567  max mem: 15572
Epoch: [21]  [ 380/1404]  eta: 0:10:14  lr: 0.000052  min_lr: 0.000001  loss: 4.1171 (4.2221)  class_acc: 0.3750 (0.3239)  loss_scale: 16384.0000 (23565.4383)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0546  max mem: 15572
Epoch: [21]  [ 390/1404]  eta: 0:10:07  lr: 0.000052  min_lr: 0.000001  loss: 4.2033 (4.2214)  class_acc: 0.2917 (0.3216)  loss_scale: 16384.0000 (23381.7698)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0564  max mem: 15572
Epoch: [21]  [ 400/1404]  eta: 0:10:03  lr: 0.000052  min_lr: 0.000001  loss: 4.0963 (4.2175)  class_acc: 0.2917 (0.3235)  loss_scale: 16384.0000 (23207.2618)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1154  max mem: 15572
Epoch: [21]  [ 410/1404]  eta: 0:09:57  lr: 0.000052  min_lr: 0.000001  loss: 4.1954 (4.2163)  class_acc: 0.3333 (0.3224)  loss_scale: 16384.0000 (23041.2457)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1390  max mem: 15572
Epoch: [21]  [ 420/1404]  eta: 0:09:50  lr: 0.000052  min_lr: 0.000001  loss: 4.2084 (4.2151)  class_acc: 0.2500 (0.3210)  loss_scale: 16384.0000 (22883.1164)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0747  max mem: 15572
Epoch: [21]  [ 430/1404]  eta: 0:09:44  lr: 0.000052  min_lr: 0.000001  loss: 4.2614 (4.2147)  class_acc: 0.2083 (0.3202)  loss_scale: 16384.0000 (22732.3248)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0690  max mem: 15572
Epoch: [21]  [ 440/1404]  eta: 0:09:36  lr: 0.000052  min_lr: 0.000001  loss: 4.2003 (4.2121)  class_acc: 0.2500 (0.3196)  loss_scale: 16384.0000 (22588.3719)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0724  max mem: 15572
Epoch: [21]  [ 450/1404]  eta: 0:09:30  lr: 0.000052  min_lr: 0.000001  loss: 4.1774 (4.2104)  class_acc: 0.2917 (0.3196)  loss_scale: 16384.0000 (22450.8027)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0728  max mem: 15572
Epoch: [21]  [ 460/1404]  eta: 0:09:25  lr: 0.000052  min_lr: 0.000001  loss: 4.2071 (4.2111)  class_acc: 0.3333 (0.3194)  loss_scale: 16384.0000 (22319.2017)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.0840  max mem: 15572
Epoch: [21]  [ 470/1404]  eta: 0:09:19  lr: 0.000052  min_lr: 0.000001  loss: 4.2594 (4.2131)  class_acc: 0.2917 (0.3179)  loss_scale: 16384.0000 (22193.1890)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.0805  max mem: 15572
Epoch: [21]  [ 480/1404]  eta: 0:09:13  lr: 0.000052  min_lr: 0.000001  loss: 4.2741 (4.2130)  class_acc: 0.2500 (0.3167)  loss_scale: 16384.0000 (22072.4158)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0940  max mem: 15572
[2025-01-17 01:25:57,310] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:25:57,310] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:25:57,314] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:25:57,315] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [ 490/1404]  eta: 0:09:06  lr: 0.000052  min_lr: 0.000001  loss: 4.3098 (4.2133)  class_acc: 0.2500 (0.3165)  loss_scale: 16384.0000 (22256.8798)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0615  max mem: 15572
[2025-01-17 01:26:06,447] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29982
[2025-01-17 01:26:06,447] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:26:06,447] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 01:26:06,471] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29982
[2025-01-17 01:26:06,472] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [21]  [ 500/1404]  eta: 0:08:59  lr: 0.000052  min_lr: 0.000001  loss: 4.3098 (4.2126)  class_acc: 0.2917 (0.3166)  loss_scale: 32768.0000 (22368.5749)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.0427  max mem: 15572
Epoch: [21]  [ 510/1404]  eta: 0:08:54  lr: 0.000052  min_lr: 0.000001  loss: 4.2462 (4.2162)  class_acc: 0.2500 (0.3154)  loss_scale: 16384.0000 (22251.4599)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0757  max mem: 15572
[2025-01-17 01:26:16,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=178, lr=[5.006912820388102e-07, 5.006912820388102e-07, 7.152732600554432e-07, 7.152732600554432e-07, 1.0218189429363475e-06, 1.0218189429363475e-06, 1.4597413470519252e-06, 1.4597413470519252e-06, 2.0853447815027504e-06, 2.0853447815027504e-06, 2.9790639735753574e-06, 2.9790639735753574e-06, 4.255805676536225e-06, 4.255805676536225e-06, 6.079722395051751e-06, 6.079722395051751e-06, 8.685317707216788e-06, 8.685317707216788e-06, 1.2407596724595412e-05, 1.2407596724595412e-05, 1.7725138177993445e-05, 1.7725138177993445e-05, 2.5321625968562067e-05, 2.5321625968562067e-05, 3.61737513836601e-05, 3.61737513836601e-05, 5.1676787690943e-05, 5.1676787690943e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 01:26:16,669] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=47.87854135987222, CurrSamplesPerSec=49.034131333712956, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [21]  [ 520/1404]  eta: 0:08:49  lr: 0.000052  min_lr: 0.000001  loss: 4.2778 (4.2174)  class_acc: 0.2500 (0.3149)  loss_scale: 16384.0000 (22138.8407)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.0530  max mem: 15572
Epoch: [21]  [ 530/1404]  eta: 0:08:41  lr: 0.000052  min_lr: 0.000001  loss: 4.2741 (4.2185)  class_acc: 0.2917 (0.3151)  loss_scale: 16384.0000 (22030.4633)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.0008  max mem: 15572
Epoch: [21]  [ 540/1404]  eta: 0:08:35  lr: 0.000052  min_lr: 0.000000  loss: 4.2448 (4.2189)  class_acc: 0.3333 (0.3150)  loss_scale: 16384.0000 (21926.0924)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0263  max mem: 15572
Epoch: [21]  [ 550/1404]  eta: 0:08:28  lr: 0.000052  min_lr: 0.000000  loss: 4.2352 (4.2189)  class_acc: 0.2500 (0.3150)  loss_scale: 16384.0000 (21825.5100)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0261  max mem: 15572
Epoch: [21]  [ 560/1404]  eta: 0:08:23  lr: 0.000052  min_lr: 0.000000  loss: 4.1073 (4.2171)  class_acc: 0.3750 (0.3161)  loss_scale: 16384.0000 (21728.5134)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0007  max mem: 15572
Epoch: [21]  [ 570/1404]  eta: 0:08:17  lr: 0.000052  min_lr: 0.000000  loss: 4.1073 (4.2180)  class_acc: 0.2917 (0.3150)  loss_scale: 16384.0000 (21634.9142)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.0008  max mem: 15572
Epoch: [21]  [ 580/1404]  eta: 0:08:11  lr: 0.000051  min_lr: 0.000000  loss: 4.2235 (4.2169)  class_acc: 0.2500 (0.3148)  loss_scale: 16384.0000 (21544.5370)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0008  max mem: 15572
Epoch: [21]  [ 590/1404]  eta: 0:08:04  lr: 0.000051  min_lr: 0.000000  loss: 4.2235 (4.2172)  class_acc: 0.2500 (0.3135)  loss_scale: 16384.0000 (21457.2183)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0008  max mem: 15572
Epoch: [21]  [ 600/1404]  eta: 0:07:57  lr: 0.000051  min_lr: 0.000000  loss: 4.0811 (4.2129)  class_acc: 0.2500 (0.3145)  loss_scale: 16384.0000 (21372.8053)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0008  max mem: 15572
Epoch: [21]  [ 610/1404]  eta: 0:07:52  lr: 0.000051  min_lr: 0.000000  loss: 4.1092 (4.2129)  class_acc: 0.3333 (0.3148)  loss_scale: 16384.0000 (21291.1555)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0008  max mem: 15572
Epoch: [21]  [ 620/1404]  eta: 0:07:46  lr: 0.000051  min_lr: 0.000000  loss: 4.2540 (4.2140)  class_acc: 0.2917 (0.3144)  loss_scale: 16384.0000 (21212.1353)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0009  max mem: 15572
[2025-01-17 01:27:23,058] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:27:23,058] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:27:23,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:27:23,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [ 630/1404]  eta: 0:07:41  lr: 0.000051  min_lr: 0.000000  loss: 4.3868 (4.2148)  class_acc: 0.2917 (0.3147)  loss_scale: 16384.0000 (21239.4802)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.0009  max mem: 15572
Epoch: [21]  [ 640/1404]  eta: 0:07:34  lr: 0.000051  min_lr: 0.000000  loss: 4.3026 (4.2129)  class_acc: 0.3333 (0.3145)  loss_scale: 32768.0000 (21419.3323)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.0006  max mem: 15572
Epoch: [21]  [ 650/1404]  eta: 0:07:29  lr: 0.000051  min_lr: 0.000000  loss: 4.2358 (4.2152)  class_acc: 0.2917 (0.3139)  loss_scale: 32768.0000 (21593.6590)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0005  max mem: 15572
Epoch: [21]  [ 660/1404]  eta: 0:07:21  lr: 0.000051  min_lr: 0.000000  loss: 4.1997 (4.2140)  class_acc: 0.2500 (0.3134)  loss_scale: 32768.0000 (21762.7110)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0005  max mem: 15572
[2025-01-17 01:27:42,281] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30147
[2025-01-17 01:27:42,282] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:27:42,348] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30147
[2025-01-17 01:27:42,349] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:27:42,349] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [ 670/1404]  eta: 0:07:16  lr: 0.000051  min_lr: 0.000000  loss: 4.1604 (4.2136)  class_acc: 0.2500 (0.3124)  loss_scale: 32768.0000 (21731.3860)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0007  max mem: 15572
Epoch: [21]  [ 680/1404]  eta: 0:07:10  lr: 0.000051  min_lr: 0.000000  loss: 4.1865 (4.2139)  class_acc: 0.2917 (0.3131)  loss_scale: 16384.0000 (21652.8634)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.0007  max mem: 15572
Epoch: [21]  [ 690/1404]  eta: 0:07:03  lr: 0.000051  min_lr: 0.000000  loss: 4.2658 (4.2155)  class_acc: 0.3333 (0.3135)  loss_scale: 16384.0000 (21576.6136)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0006  max mem: 15572
Epoch: [21]  [ 700/1404]  eta: 0:06:56  lr: 0.000051  min_lr: 0.000000  loss: 4.1879 (4.2146)  class_acc: 0.3333 (0.3129)  loss_scale: 16384.0000 (21502.5392)  weight_decay: 0.0500 (0.0500)  time: 0.4957  data: 0.0007  max mem: 15572
Epoch: [21]  [ 710/1404]  eta: 0:06:50  lr: 0.000051  min_lr: 0.000000  loss: 4.1879 (4.2160)  class_acc: 0.2917 (0.3125)  loss_scale: 16384.0000 (21430.5485)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0006  max mem: 15572
Epoch: [21]  [ 720/1404]  eta: 0:06:45  lr: 0.000051  min_lr: 0.000000  loss: 4.2692 (4.2169)  class_acc: 0.2917 (0.3131)  loss_scale: 16384.0000 (21360.5548)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.0007  max mem: 15572
Epoch: [21]  [ 730/1404]  eta: 0:06:39  lr: 0.000051  min_lr: 0.000000  loss: 4.2250 (4.2164)  class_acc: 0.3333 (0.3136)  loss_scale: 16384.0000 (21292.4761)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.0007  max mem: 15572
Epoch: [21]  [ 740/1404]  eta: 0:06:32  lr: 0.000051  min_lr: 0.000000  loss: 4.0626 (4.2143)  class_acc: 0.3333 (0.3137)  loss_scale: 16384.0000 (21226.2348)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0006  max mem: 15572
Epoch: [21]  [ 750/1404]  eta: 0:06:26  lr: 0.000051  min_lr: 0.000000  loss: 4.0693 (4.2135)  class_acc: 0.3333 (0.3149)  loss_scale: 16384.0000 (21161.7577)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0006  max mem: 15572
Epoch: [21]  [ 760/1404]  eta: 0:06:20  lr: 0.000051  min_lr: 0.000000  loss: 4.1909 (4.2146)  class_acc: 0.3333 (0.3148)  loss_scale: 16384.0000 (21098.9750)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0006  max mem: 15572
Epoch: [21]  [ 770/1404]  eta: 0:06:15  lr: 0.000051  min_lr: 0.000000  loss: 4.2987 (4.2161)  class_acc: 0.2500 (0.3145)  loss_scale: 16384.0000 (21037.8210)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0007  max mem: 15572
Epoch: [21]  [ 780/1404]  eta: 0:06:08  lr: 0.000051  min_lr: 0.000000  loss: 4.2685 (4.2175)  class_acc: 0.2500 (0.3138)  loss_scale: 16384.0000 (20978.2330)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0008  max mem: 15572
Epoch: [21]  [ 790/1404]  eta: 0:06:02  lr: 0.000051  min_lr: 0.000000  loss: 4.2685 (4.2172)  class_acc: 0.2500 (0.3134)  loss_scale: 16384.0000 (20920.1517)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.0007  max mem: 15572
[2025-01-17 01:28:57,023] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:28:57,023] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:28:57,030] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:28:57,030] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [ 800/1404]  eta: 0:05:56  lr: 0.000051  min_lr: 0.000000  loss: 4.2110 (4.2172)  class_acc: 0.2917 (0.3142)  loss_scale: 16384.0000 (21047.6105)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0072  max mem: 15572
Epoch: [21]  [ 810/1404]  eta: 0:05:50  lr: 0.000051  min_lr: 0.000000  loss: 4.1806 (4.2158)  class_acc: 0.2917 (0.3140)  loss_scale: 32768.0000 (21192.1282)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0746  max mem: 15572
[2025-01-17 01:29:11,606] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30302
[2025-01-17 01:29:11,606] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:29:11,606] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 01:29:11,609] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30302
[2025-01-17 01:29:11,610] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [21]  [ 820/1404]  eta: 0:05:44  lr: 0.000051  min_lr: 0.000000  loss: 4.1880 (4.2164)  class_acc: 0.2500 (0.3134)  loss_scale: 32768.0000 (21273.2570)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0681  max mem: 15572
Epoch: [21]  [ 830/1404]  eta: 0:05:38  lr: 0.000051  min_lr: 0.000000  loss: 4.2603 (4.2167)  class_acc: 0.2500 (0.3128)  loss_scale: 16384.0000 (21214.4212)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0260  max mem: 15572
Epoch: [21]  [ 840/1404]  eta: 0:05:32  lr: 0.000051  min_lr: 0.000000  loss: 4.2594 (4.2170)  class_acc: 0.2917 (0.3125)  loss_scale: 16384.0000 (21156.9845)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0392  max mem: 15572
Epoch: [21]  [ 850/1404]  eta: 0:05:26  lr: 0.000051  min_lr: 0.000000  loss: 4.2594 (4.2183)  class_acc: 0.2500 (0.3115)  loss_scale: 16384.0000 (21100.8978)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0744  max mem: 15572
Epoch: [21]  [ 860/1404]  eta: 0:05:21  lr: 0.000051  min_lr: 0.000000  loss: 4.3770 (4.2195)  class_acc: 0.2500 (0.3113)  loss_scale: 16384.0000 (21046.1138)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.0965  max mem: 15572
Epoch: [21]  [ 870/1404]  eta: 0:05:15  lr: 0.000051  min_lr: 0.000000  loss: 4.3404 (4.2186)  class_acc: 0.2917 (0.3114)  loss_scale: 16384.0000 (20992.5878)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.0897  max mem: 15572
Epoch: [21]  [ 880/1404]  eta: 0:05:09  lr: 0.000051  min_lr: 0.000000  loss: 4.2890 (4.2198)  class_acc: 0.3333 (0.3116)  loss_scale: 16384.0000 (20940.2770)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0544  max mem: 15572
Epoch: [21]  [ 890/1404]  eta: 0:05:03  lr: 0.000051  min_lr: 0.000000  loss: 4.2704 (4.2190)  class_acc: 0.3333 (0.3121)  loss_scale: 16384.0000 (20889.1403)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.0010  max mem: 15572
Epoch: [21]  [ 900/1404]  eta: 0:04:58  lr: 0.000051  min_lr: 0.000000  loss: 4.2688 (4.2196)  class_acc: 0.3333 (0.3126)  loss_scale: 16384.0000 (20839.1387)  weight_decay: 0.0500 (0.0500)  time: 0.6605  data: 0.0009  max mem: 15572
Epoch: [21]  [ 910/1404]  eta: 0:04:51  lr: 0.000050  min_lr: 0.000000  loss: 4.1598 (4.2180)  class_acc: 0.3333 (0.3129)  loss_scale: 16384.0000 (20790.2349)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0006  max mem: 15572
Epoch: [21]  [ 920/1404]  eta: 0:04:45  lr: 0.000050  min_lr: 0.000000  loss: 4.0653 (4.2172)  class_acc: 0.2917 (0.3124)  loss_scale: 16384.0000 (20742.3931)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0005  max mem: 15572
Epoch: [21]  [ 930/1404]  eta: 0:04:40  lr: 0.000050  min_lr: 0.000000  loss: 4.0653 (4.2154)  class_acc: 0.2917 (0.3128)  loss_scale: 16384.0000 (20695.5789)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0005  max mem: 15572
Epoch: [21]  [ 940/1404]  eta: 0:04:33  lr: 0.000050  min_lr: 0.000000  loss: 4.0853 (4.2154)  class_acc: 0.2917 (0.3124)  loss_scale: 16384.0000 (20649.7598)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0007  max mem: 15572
[2025-01-17 01:30:27,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:30:27,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:30:27,878] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:30:27,879] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [ 950/1404]  eta: 0:04:27  lr: 0.000050  min_lr: 0.000000  loss: 4.1117 (4.2149)  class_acc: 0.2500 (0.3122)  loss_scale: 16384.0000 (20673.8170)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0009  max mem: 15572
[2025-01-17 01:30:31,042] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30437
[2025-01-17 01:30:31,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:30:31,092] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30437
[2025-01-17 01:30:31,092] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:30:31,092] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [ 960/1404]  eta: 0:04:21  lr: 0.000050  min_lr: 0.000000  loss: 4.1117 (4.2154)  class_acc: 0.2500 (0.3118)  loss_scale: 16384.0000 (20663.2758)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0008  max mem: 15572
Epoch: [21]  [ 970/1404]  eta: 0:04:15  lr: 0.000050  min_lr: 0.000000  loss: 4.1133 (4.2142)  class_acc: 0.3333 (0.3125)  loss_scale: 16384.0000 (20619.2049)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0241  max mem: 15572
Epoch: [21]  [ 980/1404]  eta: 0:04:10  lr: 0.000050  min_lr: 0.000000  loss: 4.1569 (4.2150)  class_acc: 0.2917 (0.3123)  loss_scale: 16384.0000 (20576.0326)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0580  max mem: 15572
Epoch: [21]  [ 990/1404]  eta: 0:04:04  lr: 0.000050  min_lr: 0.000000  loss: 4.2741 (4.2155)  class_acc: 0.2917 (0.3123)  loss_scale: 16384.0000 (20533.7316)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0592  max mem: 15572
Epoch: [21]  [1000/1404]  eta: 0:03:57  lr: 0.000050  min_lr: 0.000000  loss: 4.1874 (4.2154)  class_acc: 0.3333 (0.3128)  loss_scale: 16384.0000 (20492.2757)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0317  max mem: 15572
Epoch: [21]  [1010/1404]  eta: 0:03:52  lr: 0.000050  min_lr: 0.000000  loss: 4.2174 (4.2164)  class_acc: 0.2917 (0.3126)  loss_scale: 16384.0000 (20451.6400)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0807  max mem: 15572
Epoch: [21]  [1020/1404]  eta: 0:03:46  lr: 0.000050  min_lr: 0.000000  loss: 4.2210 (4.2158)  class_acc: 0.2917 (0.3125)  loss_scale: 16384.0000 (20411.8002)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1032  max mem: 15572
Epoch: [21]  [1030/1404]  eta: 0:03:40  lr: 0.000050  min_lr: 0.000000  loss: 4.0632 (4.2147)  class_acc: 0.2917 (0.3126)  loss_scale: 16384.0000 (20372.7333)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0295  max mem: 15572
Epoch: [21]  [1040/1404]  eta: 0:03:34  lr: 0.000050  min_lr: 0.000000  loss: 3.9247 (4.2138)  class_acc: 0.2917 (0.3128)  loss_scale: 16384.0000 (20334.4169)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.0006  max mem: 15572
Epoch: [21]  [1050/1404]  eta: 0:03:28  lr: 0.000050  min_lr: 0.000000  loss: 4.0205 (4.2122)  class_acc: 0.2917 (0.3129)  loss_scale: 16384.0000 (20296.8297)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0006  max mem: 15572
Epoch: [21]  [1060/1404]  eta: 0:03:22  lr: 0.000050  min_lr: 0.000000  loss: 4.2066 (4.2123)  class_acc: 0.2500 (0.3124)  loss_scale: 16384.0000 (20259.9510)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0007  max mem: 15572
Epoch: [21]  [1070/1404]  eta: 0:03:16  lr: 0.000050  min_lr: 0.000000  loss: 4.1608 (4.2111)  class_acc: 0.2917 (0.3127)  loss_scale: 16384.0000 (20223.7610)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0264  max mem: 15572
Epoch: [21]  [1080/1404]  eta: 0:03:10  lr: 0.000050  min_lr: 0.000000  loss: 4.1030 (4.2099)  class_acc: 0.3333 (0.3131)  loss_scale: 16384.0000 (20188.2405)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.0829  max mem: 15572
[2025-01-17 01:31:46,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:31:46,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:31:46,013] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:31:46,013] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:31:46,978] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30568
[2025-01-17 01:31:46,979] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:31:47,026] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30568
[2025-01-17 01:31:47,027] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:31:47,028] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [1090/1404]  eta: 0:03:04  lr: 0.000050  min_lr: 0.000000  loss: 4.0553 (4.2086)  class_acc: 0.2917 (0.3131)  loss_scale: 16384.0000 (20183.4060)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.0929  max mem: 15572
Epoch: [21]  [1100/1404]  eta: 0:02:59  lr: 0.000050  min_lr: 0.000000  loss: 4.1471 (4.2092)  class_acc: 0.2917 (0.3128)  loss_scale: 16384.0000 (20148.8974)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.0466  max mem: 15572
Epoch: [21]  [1110/1404]  eta: 0:02:53  lr: 0.000050  min_lr: 0.000000  loss: 4.3411 (4.2105)  class_acc: 0.2917 (0.3130)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.0110  max mem: 15572
Epoch: [21]  [1120/1404]  eta: 0:02:47  lr: 0.000050  min_lr: 0.000000  loss: 4.2704 (4.2100)  class_acc: 0.2917 (0.3130)  loss_scale: 16384.0000 (20081.7270)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0790  max mem: 15572
Epoch: [21]  [1130/1404]  eta: 0:02:41  lr: 0.000050  min_lr: 0.000000  loss: 4.2324 (4.2109)  class_acc: 0.2917 (0.3131)  loss_scale: 16384.0000 (20049.0327)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1367  max mem: 15572
Epoch: [21]  [1140/1404]  eta: 0:02:35  lr: 0.000050  min_lr: 0.000000  loss: 4.2388 (4.2120)  class_acc: 0.2917 (0.3128)  loss_scale: 16384.0000 (20016.9115)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0584  max mem: 15572
Epoch: [21]  [1150/1404]  eta: 0:02:29  lr: 0.000050  min_lr: 0.000000  loss: 4.1768 (4.2111)  class_acc: 0.2500 (0.3126)  loss_scale: 16384.0000 (19985.3484)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0288  max mem: 15572
Epoch: [21]  [1160/1404]  eta: 0:02:23  lr: 0.000050  min_lr: 0.000000  loss: 4.1143 (4.2106)  class_acc: 0.3333 (0.3127)  loss_scale: 16384.0000 (19954.3290)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0543  max mem: 15572
Epoch: [21]  [1170/1404]  eta: 0:02:17  lr: 0.000050  min_lr: 0.000000  loss: 4.1733 (4.2117)  class_acc: 0.2917 (0.3125)  loss_scale: 16384.0000 (19923.8395)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0497  max mem: 15572
Epoch: [21]  [1180/1404]  eta: 0:02:11  lr: 0.000050  min_lr: 0.000000  loss: 4.3760 (4.2137)  class_acc: 0.2500 (0.3121)  loss_scale: 16384.0000 (19893.8662)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.0549  max mem: 15572
Epoch: [21]  [1190/1404]  eta: 0:02:05  lr: 0.000050  min_lr: 0.000000  loss: 4.2110 (4.2125)  class_acc: 0.2917 (0.3122)  loss_scale: 16384.0000 (19864.3963)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.0679  max mem: 15572
Epoch: [21]  [1200/1404]  eta: 0:01:59  lr: 0.000050  min_lr: 0.000000  loss: 4.0843 (4.2117)  class_acc: 0.3333 (0.3124)  loss_scale: 16384.0000 (19835.4172)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0372  max mem: 15572
Epoch: [21]  [1210/1404]  eta: 0:01:53  lr: 0.000050  min_lr: 0.000000  loss: 4.0903 (4.2116)  class_acc: 0.2917 (0.3126)  loss_scale: 16384.0000 (19806.9166)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0474  max mem: 15572
[2025-01-17 01:33:02,740] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:33:02,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:33:02,800] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:33:02,800] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [1220/1404]  eta: 0:01:48  lr: 0.000050  min_lr: 0.000000  loss: 4.0903 (4.2107)  class_acc: 0.2917 (0.3124)  loss_scale: 16384.0000 (19886.2310)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1082  max mem: 15572
Epoch: [21]  [1230/1404]  eta: 0:01:42  lr: 0.000050  min_lr: 0.000000  loss: 4.1015 (4.2108)  class_acc: 0.2083 (0.3116)  loss_scale: 32768.0000 (19990.8757)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0736  max mem: 15572
Epoch: [21]  [1240/1404]  eta: 0:01:36  lr: 0.000050  min_lr: 0.000000  loss: 4.1015 (4.2103)  class_acc: 0.2500 (0.3117)  loss_scale: 32768.0000 (20093.8340)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1261  max mem: 15572
Epoch: [21]  [1250/1404]  eta: 0:01:30  lr: 0.000049  min_lr: 0.000000  loss: 4.1756 (4.2106)  class_acc: 0.2917 (0.3114)  loss_scale: 32768.0000 (20195.1463)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.1560  max mem: 15572
Epoch: [21]  [1260/1404]  eta: 0:01:24  lr: 0.000049  min_lr: 0.000000  loss: 4.2299 (4.2115)  class_acc: 0.2500 (0.3111)  loss_scale: 32768.0000 (20294.8517)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1225  max mem: 15572
Epoch: [21]  [1270/1404]  eta: 0:01:18  lr: 0.000049  min_lr: 0.000000  loss: 4.2340 (4.2114)  class_acc: 0.2917 (0.3110)  loss_scale: 32768.0000 (20392.9882)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1002  max mem: 15572
Epoch: [21]  [1280/1404]  eta: 0:01:12  lr: 0.000049  min_lr: 0.000000  loss: 4.2843 (4.2117)  class_acc: 0.2917 (0.3107)  loss_scale: 32768.0000 (20489.5925)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0630  max mem: 15572
Epoch: [21]  [1290/1404]  eta: 0:01:07  lr: 0.000049  min_lr: 0.000000  loss: 4.2843 (4.2123)  class_acc: 0.2500 (0.3101)  loss_scale: 32768.0000 (20584.7002)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0898  max mem: 15572
Epoch: [21]  [1300/1404]  eta: 0:01:01  lr: 0.000049  min_lr: 0.000000  loss: 4.2381 (4.2128)  class_acc: 0.2500 (0.3098)  loss_scale: 32768.0000 (20678.3459)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1032  max mem: 15572
Epoch: [21]  [1310/1404]  eta: 0:00:55  lr: 0.000049  min_lr: 0.000000  loss: 4.2951 (4.2137)  class_acc: 0.2500 (0.3093)  loss_scale: 32768.0000 (20770.5629)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0858  max mem: 15572
Epoch: [21]  [1320/1404]  eta: 0:00:49  lr: 0.000049  min_lr: 0.000000  loss: 4.2867 (4.2145)  class_acc: 0.2917 (0.3094)  loss_scale: 32768.0000 (20861.3838)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0438  max mem: 15572
Epoch: [21]  [1330/1404]  eta: 0:00:43  lr: 0.000049  min_lr: 0.000000  loss: 4.3580 (4.2163)  class_acc: 0.2917 (0.3089)  loss_scale: 32768.0000 (20950.8400)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.0274  max mem: 15572
Epoch: [21]  [1340/1404]  eta: 0:00:37  lr: 0.000049  min_lr: 0.000000  loss: 4.3948 (4.2170)  class_acc: 0.2083 (0.3082)  loss_scale: 32768.0000 (21038.9620)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.0134  max mem: 15572
[2025-01-17 01:34:16,968] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:34:16,968] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 01:34:17,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:34:17,003] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 01:34:17,951] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30827
[2025-01-17 01:34:17,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 01:34:17,951] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 01:34:17,955] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30827
[2025-01-17 01:34:17,956] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [21]  [1350/1404]  eta: 0:00:31  lr: 0.000049  min_lr: 0.000000  loss: 4.2201 (4.2167)  class_acc: 0.2500 (0.3082)  loss_scale: 32768.0000 (21174.2887)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0008  max mem: 15572
Epoch: [21]  [1360/1404]  eta: 0:00:25  lr: 0.000049  min_lr: 0.000000  loss: 4.2125 (4.2170)  class_acc: 0.3333 (0.3085)  loss_scale: 32768.0000 (21259.4739)  weight_decay: 0.0500 (0.0500)  time: 0.6927  data: 0.0006  max mem: 15572
Epoch: [21]  [1370/1404]  eta: 0:00:19  lr: 0.000049  min_lr: 0.000000  loss: 4.2762 (4.2166)  class_acc: 0.3333 (0.3087)  loss_scale: 32768.0000 (21343.4165)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.0005  max mem: 15572
Epoch: [21]  [1380/1404]  eta: 0:00:14  lr: 0.000049  min_lr: 0.000000  loss: 4.2762 (4.2167)  class_acc: 0.2917 (0.3087)  loss_scale: 32768.0000 (21426.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0006  max mem: 15572
Epoch: [21]  [1390/1404]  eta: 0:00:08  lr: 0.000049  min_lr: 0.000000  loss: 4.2316 (4.2173)  class_acc: 0.2917 (0.3089)  loss_scale: 32768.0000 (21507.6808)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0009  max mem: 15572
Epoch: [21]  [1400/1404]  eta: 0:00:02  lr: 0.000049  min_lr: 0.000000  loss: 4.1305 (4.2169)  class_acc: 0.2917 (0.3084)  loss_scale: 32768.0000 (21588.0542)  weight_decay: 0.0500 (0.0500)  time: 0.4367  data: 0.0007  max mem: 15572
Epoch: [21]  [1403/1404]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000000  loss: 4.1197 (4.2169)  class_acc: 0.2500 (0.3083)  loss_scale: 32768.0000 (21611.9430)  weight_decay: 0.0500 (0.0500)  time: 0.4106  data: 0.0005  max mem: 15572
Epoch: [21] Total time: 0:13:43 (0.5864 s / it)
Averaged stats: lr: 0.000049  min_lr: 0.000000  loss: 4.1197 (4.2112)  class_acc: 0.2500 (0.3107)  loss_scale: 32768.0000 (21611.9430)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:08  loss: 1.8455 (1.8455)  acc1: 66.6667 (66.6667)  acc5: 77.7778 (77.7778)  time: 4.0326  data: 3.8444  max mem: 15572
Val:  [ 10/136]  eta: 0:01:25  loss: 2.4610 (2.4257)  acc1: 55.5556 (46.4646)  acc5: 77.7778 (74.7475)  time: 0.6801  data: 0.4865  max mem: 15572
Val:  [ 20/136]  eta: 0:00:59  loss: 2.5142 (2.5175)  acc1: 38.8889 (42.3280)  acc5: 72.2222 (75.6614)  time: 0.3387  data: 0.1417  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 2.4642 (2.4251)  acc1: 38.8889 (44.6237)  acc5: 83.3333 (76.7025)  time: 0.3443  data: 0.1399  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 2.0887 (2.3706)  acc1: 50.0000 (47.1545)  acc5: 83.3333 (77.5068)  time: 0.3730  data: 0.1738  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.3155 (2.3993)  acc1: 50.0000 (47.0588)  acc5: 83.3333 (77.5599)  time: 0.3540  data: 0.1621  max mem: 15572
Val:  [ 60/136]  eta: 0:00:30  loss: 2.4385 (2.4585)  acc1: 44.4444 (44.9909)  acc5: 77.7778 (76.7760)  time: 0.3248  data: 0.1339  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.3908 (2.4383)  acc1: 44.4444 (45.3834)  acc5: 77.7778 (77.6995)  time: 0.3518  data: 0.1526  max mem: 15572
Val:  [ 80/136]  eta: 0:00:21  loss: 2.3117 (2.4256)  acc1: 44.4444 (45.4733)  acc5: 83.3333 (78.1207)  time: 0.3309  data: 0.1287  max mem: 15572
Val:  [ 90/136]  eta: 0:00:17  loss: 2.3917 (2.4342)  acc1: 38.8889 (44.9328)  acc5: 72.2222 (77.3504)  time: 0.3309  data: 0.1392  max mem: 15572
Val:  [100/136]  eta: 0:00:13  loss: 2.5182 (2.4832)  acc1: 38.8889 (43.0693)  acc5: 72.2222 (75.9626)  time: 0.3644  data: 0.1761  max mem: 15572
Val:  [110/136]  eta: 0:00:09  loss: 2.6572 (2.4812)  acc1: 33.3333 (42.8428)  acc5: 77.7778 (76.0260)  time: 0.3785  data: 0.1771  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.2169 (2.4401)  acc1: 50.0000 (44.1690)  acc5: 83.3333 (77.0432)  time: 0.3612  data: 0.1453  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.9693 (2.4042)  acc1: 61.1111 (45.2502)  acc5: 88.8889 (77.7778)  time: 0.2971  data: 0.1028  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.0189 (2.3977)  acc1: 55.5556 (45.6593)  acc5: 83.3333 (77.9279)  time: 0.2298  data: 0.0499  max mem: 15572
Val: Total time: 0:00:49 (0.3635 s / it)
* Acc@1 44.615 Acc@5 76.618 loss 2.447
Accuracy of the network on the 4883 val videos: 44.6%
Max accuracy: 44.86%
Epoch: [22]  [   0/1404]  eta: 2:30:00  lr: 0.000049  min_lr: 0.000000  loss: 3.8744 (3.8744)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.4107  data: 5.1870  max mem: 15572
Epoch: [22]  [  10/1404]  eta: 0:27:06  lr: 0.000049  min_lr: 0.000000  loss: 4.1087 (4.1395)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1667  data: 0.4721  max mem: 15572
Epoch: [22]  [  20/1404]  eta: 0:20:22  lr: 0.000049  min_lr: 0.000000  loss: 4.0770 (4.0543)  class_acc: 0.2917 (0.3294)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0006  max mem: 15572
Epoch: [22]  [  30/1404]  eta: 0:18:16  lr: 0.000049  min_lr: 0.000000  loss: 4.0595 (4.0879)  class_acc: 0.2917 (0.3132)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0005  max mem: 15572
Epoch: [22]  [  40/1404]  eta: 0:17:18  lr: 0.000049  min_lr: 0.000000  loss: 4.2053 (4.1275)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.0007  max mem: 15572
Epoch: [22]  [  50/1404]  eta: 0:16:33  lr: 0.000049  min_lr: 0.000000  loss: 4.2333 (4.1667)  class_acc: 0.3333 (0.3178)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.0007  max mem: 15572
[2025-01-17 01:36:22,148] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30945
[2025-01-17 01:36:22,148] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:36:22,148] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30945
[2025-01-17 01:36:22,148] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:36:22,149] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [22]  [  60/1404]  eta: 0:15:46  lr: 0.000049  min_lr: 0.000000  loss: 4.2158 (4.1586)  class_acc: 0.2917 (0.3258)  loss_scale: 32768.0000 (31693.6393)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0006  max mem: 15572
Epoch: [22]  [  70/1404]  eta: 0:15:21  lr: 0.000049  min_lr: 0.000000  loss: 4.0542 (4.1514)  class_acc: 0.2917 (0.3251)  loss_scale: 16384.0000 (29537.3521)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0007  max mem: 15572
Epoch: [22]  [  80/1404]  eta: 0:14:55  lr: 0.000049  min_lr: 0.000000  loss: 4.1882 (4.1599)  class_acc: 0.3333 (0.3225)  loss_scale: 16384.0000 (27913.4815)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0009  max mem: 15572
Epoch: [22]  [  90/1404]  eta: 0:14:32  lr: 0.000049  min_lr: 0.000000  loss: 4.1882 (4.1671)  class_acc: 0.2917 (0.3201)  loss_scale: 16384.0000 (26646.5055)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0032  max mem: 15572
Epoch: [22]  [ 100/1404]  eta: 0:14:09  lr: 0.000049  min_lr: 0.000000  loss: 4.1058 (4.1606)  class_acc: 0.2917 (0.3218)  loss_scale: 16384.0000 (25630.4158)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0030  max mem: 15572
Epoch: [22]  [ 110/1404]  eta: 0:13:56  lr: 0.000049  min_lr: 0.000000  loss: 4.0783 (4.1670)  class_acc: 0.3333 (0.3270)  loss_scale: 16384.0000 (24797.4054)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0066  max mem: 15572
[2025-01-17 01:36:53,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=184, lr=[4.717601933451567e-07, 4.717601933451567e-07, 6.73943133350224e-07, 6.73943133350224e-07, 9.627759047860343e-07, 9.627759047860343e-07, 1.3753941496943348e-06, 1.3753941496943348e-06, 1.964848785277621e-06, 1.964848785277621e-06, 2.8069268361108873e-06, 2.8069268361108873e-06, 4.009895480158411e-06, 4.009895480158411e-06, 5.728422114512017e-06, 5.728422114512017e-06, 8.183460163588594e-06, 8.183460163588594e-06, 1.1690657376555136e-05, 1.1690657376555136e-05, 1.670093910936448e-05, 1.670093910936448e-05, 2.385848444194926e-05, 2.385848444194926e-05, 3.408354920278466e-05, 3.408354920278466e-05, 4.869078457540666e-05, 4.869078457540666e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 01:36:53,428] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=47.81673297601939, CurrSamplesPerSec=54.009564305278715, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [ 120/1404]  eta: 0:13:43  lr: 0.000049  min_lr: 0.000000  loss: 4.1806 (4.1663)  class_acc: 0.3333 (0.3323)  loss_scale: 16384.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0493  max mem: 15572
Epoch: [22]  [ 130/1404]  eta: 0:13:30  lr: 0.000049  min_lr: 0.000000  loss: 4.2814 (4.1727)  class_acc: 0.3333 (0.3314)  loss_scale: 16384.0000 (23512.9160)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0777  max mem: 15572
Epoch: [22]  [ 140/1404]  eta: 0:13:24  lr: 0.000049  min_lr: 0.000000  loss: 4.2552 (4.1719)  class_acc: 0.2917 (0.3322)  loss_scale: 16384.0000 (23007.3191)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1021  max mem: 15572
Epoch: [22]  [ 150/1404]  eta: 0:13:11  lr: 0.000049  min_lr: 0.000000  loss: 4.0936 (4.1729)  class_acc: 0.2917 (0.3270)  loss_scale: 16384.0000 (22568.6887)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1109  max mem: 15572
Epoch: [22]  [ 160/1404]  eta: 0:13:02  lr: 0.000049  min_lr: 0.000000  loss: 4.2880 (4.1816)  class_acc: 0.2500 (0.3204)  loss_scale: 16384.0000 (22184.5466)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0868  max mem: 15572
Epoch: [22]  [ 170/1404]  eta: 0:13:02  lr: 0.000049  min_lr: 0.000000  loss: 4.2880 (4.1855)  class_acc: 0.2500 (0.3173)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.1483  max mem: 15572
Epoch: [22]  [ 180/1404]  eta: 0:12:47  lr: 0.000048  min_lr: 0.000000  loss: 4.2167 (4.1898)  class_acc: 0.2500 (0.3165)  loss_scale: 16384.0000 (21543.6022)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1346  max mem: 15572
[2025-01-17 01:37:37,611] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:37:37,611] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:37:37,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:37:37,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [ 190/1404]  eta: 0:12:36  lr: 0.000048  min_lr: 0.000000  loss: 4.2167 (4.1891)  class_acc: 0.3333 (0.3192)  loss_scale: 16384.0000 (21702.3665)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0595  max mem: 15572
Epoch: [22]  [ 200/1404]  eta: 0:12:28  lr: 0.000048  min_lr: 0.000000  loss: 4.1805 (4.1912)  class_acc: 0.3333 (0.3211)  loss_scale: 32768.0000 (22252.8955)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0752  max mem: 15572
Epoch: [22]  [ 210/1404]  eta: 0:12:23  lr: 0.000048  min_lr: 0.000000  loss: 4.2225 (4.1922)  class_acc: 0.2500 (0.3197)  loss_scale: 32768.0000 (22751.2417)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1164  max mem: 15572
Epoch: [22]  [ 220/1404]  eta: 0:12:12  lr: 0.000048  min_lr: 0.000000  loss: 4.2225 (4.1947)  class_acc: 0.2500 (0.3186)  loss_scale: 32768.0000 (23204.4887)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.0915  max mem: 15572
Epoch: [22]  [ 230/1404]  eta: 0:12:00  lr: 0.000048  min_lr: 0.000000  loss: 4.2484 (4.1934)  class_acc: 0.2917 (0.3182)  loss_scale: 32768.0000 (23618.4935)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0260  max mem: 15572
Epoch: [22]  [ 240/1404]  eta: 0:11:56  lr: 0.000048  min_lr: 0.000000  loss: 4.1986 (4.1954)  class_acc: 0.2917 (0.3179)  loss_scale: 32768.0000 (23998.1411)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0951  max mem: 15572
Epoch: [22]  [ 250/1404]  eta: 0:11:50  lr: 0.000048  min_lr: 0.000000  loss: 4.1859 (4.1930)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (24347.5378)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1448  max mem: 15572
Epoch: [22]  [ 260/1404]  eta: 0:11:40  lr: 0.000048  min_lr: 0.000000  loss: 3.9820 (4.1923)  class_acc: 0.2917 (0.3212)  loss_scale: 32768.0000 (24670.1609)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0755  max mem: 15572
Epoch: [22]  [ 270/1404]  eta: 0:11:31  lr: 0.000048  min_lr: 0.000000  loss: 4.0184 (4.1919)  class_acc: 0.2917 (0.3206)  loss_scale: 32768.0000 (24968.9742)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0320  max mem: 15572
Epoch: [22]  [ 280/1404]  eta: 0:11:20  lr: 0.000048  min_lr: 0.000000  loss: 4.0184 (4.1877)  class_acc: 0.3333 (0.3209)  loss_scale: 32768.0000 (25246.5196)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0124  max mem: 15572
Epoch: [22]  [ 290/1404]  eta: 0:11:14  lr: 0.000048  min_lr: 0.000000  loss: 4.0045 (4.1883)  class_acc: 0.2917 (0.3210)  loss_scale: 32768.0000 (25504.9897)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0729  max mem: 15572
Epoch: [22]  [ 300/1404]  eta: 0:11:08  lr: 0.000048  min_lr: 0.000000  loss: 4.2501 (4.1883)  class_acc: 0.2917 (0.3213)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1307  max mem: 15572
Epoch: [22]  [ 310/1404]  eta: 0:10:58  lr: 0.000048  min_lr: 0.000000  loss: 4.2501 (4.1897)  class_acc: 0.2500 (0.3218)  loss_scale: 32768.0000 (25972.0643)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0586  max mem: 15572
[2025-01-17 01:38:50,415] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:38:50,415] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 01:38:50,501] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:38:50,501] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 01:38:51,916] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31205
[2025-01-17 01:38:51,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 01:38:51,930] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31205
[2025-01-17 01:38:51,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 01:38:51,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 320/1404]  eta: 0:10:51  lr: 0.000048  min_lr: 0.000000  loss: 4.0702 (4.1849)  class_acc: 0.3333 (0.3210)  loss_scale: 32768.0000 (26490.0187)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0440  max mem: 15572
Epoch: [22]  [ 330/1404]  eta: 0:10:47  lr: 0.000048  min_lr: 0.000000  loss: 4.0565 (4.1849)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (26679.6858)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1284  max mem: 15572
Epoch: [22]  [ 340/1404]  eta: 0:10:40  lr: 0.000048  min_lr: 0.000000  loss: 4.1469 (4.1821)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (26858.2287)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1124  max mem: 15572
Epoch: [22]  [ 350/1404]  eta: 0:10:33  lr: 0.000048  min_lr: 0.000000  loss: 4.1774 (4.1830)  class_acc: 0.2500 (0.3211)  loss_scale: 32768.0000 (27026.5983)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0784  max mem: 15572
Epoch: [22]  [ 360/1404]  eta: 0:10:27  lr: 0.000048  min_lr: 0.000000  loss: 4.2225 (4.1831)  class_acc: 0.2917 (0.3213)  loss_scale: 32768.0000 (27185.6399)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1071  max mem: 15572
Epoch: [22]  [ 370/1404]  eta: 0:10:19  lr: 0.000048  min_lr: 0.000000  loss: 4.1571 (4.1840)  class_acc: 0.3333 (0.3210)  loss_scale: 32768.0000 (27336.1078)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0832  max mem: 15572
Epoch: [22]  [ 380/1404]  eta: 0:10:15  lr: 0.000048  min_lr: 0.000000  loss: 4.1571 (4.1834)  class_acc: 0.2500 (0.3210)  loss_scale: 32768.0000 (27478.6772)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0271  max mem: 15572
Epoch: [22]  [ 390/1404]  eta: 0:10:10  lr: 0.000048  min_lr: 0.000000  loss: 4.1693 (4.1821)  class_acc: 0.2917 (0.3211)  loss_scale: 32768.0000 (27613.9540)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.0007  max mem: 15572
Epoch: [22]  [ 400/1404]  eta: 0:10:02  lr: 0.000048  min_lr: 0.000000  loss: 4.1909 (4.1805)  class_acc: 0.3333 (0.3217)  loss_scale: 32768.0000 (27742.4838)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0009  max mem: 15572
Epoch: [22]  [ 410/1404]  eta: 0:09:54  lr: 0.000048  min_lr: 0.000000  loss: 4.1115 (4.1779)  class_acc: 0.3750 (0.3240)  loss_scale: 32768.0000 (27864.7591)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0009  max mem: 15572
Epoch: [22]  [ 420/1404]  eta: 0:09:49  lr: 0.000048  min_lr: 0.000000  loss: 4.1792 (4.1783)  class_acc: 0.3333 (0.3237)  loss_scale: 32768.0000 (27981.2257)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0220  max mem: 15572
Epoch: [22]  [ 430/1404]  eta: 0:09:41  lr: 0.000048  min_lr: 0.000000  loss: 4.2911 (4.1838)  class_acc: 0.2917 (0.3234)  loss_scale: 32768.0000 (28092.2877)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0220  max mem: 15572
Epoch: [22]  [ 440/1404]  eta: 0:09:36  lr: 0.000048  min_lr: 0.000000  loss: 4.2494 (4.1818)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (28198.3129)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.0007  max mem: 15572
[2025-01-17 01:40:08,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:40:08,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 01:40:08,646] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:40:08,646] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 01:40:10,057] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31337
[2025-01-17 01:40:10,057] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 01:40:10,070] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31337
[2025-01-17 01:40:10,070] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 01:40:10,071] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 450/1404]  eta: 0:09:30  lr: 0.000048  min_lr: 0.000000  loss: 4.2494 (4.1845)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (28517.6053)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.0009  max mem: 15572
Epoch: [22]  [ 460/1404]  eta: 0:09:24  lr: 0.000048  min_lr: 0.000000  loss: 4.2709 (4.1844)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (28609.8048)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0281  max mem: 15572
[2025-01-17 01:40:20,499] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31355
[2025-01-17 01:40:20,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:40:20,500] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 01:40:20,500] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31355
[2025-01-17 01:40:20,501] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [22]  [ 470/1404]  eta: 0:09:21  lr: 0.000048  min_lr: 0.000000  loss: 4.1954 (4.1802)  class_acc: 0.3750 (0.3225)  loss_scale: 32768.0000 (28558.9469)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.0904  max mem: 15572
Epoch: [22]  [ 480/1404]  eta: 0:09:12  lr: 0.000048  min_lr: 0.000000  loss: 4.1642 (4.1787)  class_acc: 0.2917 (0.3214)  loss_scale: 16384.0000 (28305.8295)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0632  max mem: 15572
Epoch: [22]  [ 490/1404]  eta: 0:09:05  lr: 0.000048  min_lr: 0.000000  loss: 4.2401 (4.1800)  class_acc: 0.2083 (0.3205)  loss_scale: 16384.0000 (28063.0224)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0007  max mem: 15572
Epoch: [22]  [ 500/1404]  eta: 0:09:00  lr: 0.000048  min_lr: 0.000000  loss: 4.3240 (4.1841)  class_acc: 0.2083 (0.3199)  loss_scale: 16384.0000 (27829.9082)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0067  max mem: 15572
Epoch: [22]  [ 510/1404]  eta: 0:08:52  lr: 0.000047  min_lr: 0.000000  loss: 4.3647 (4.1848)  class_acc: 0.2917 (0.3197)  loss_scale: 16384.0000 (27605.9178)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.0123  max mem: 15572
Epoch: [22]  [ 520/1404]  eta: 0:08:46  lr: 0.000047  min_lr: 0.000000  loss: 4.2149 (4.1841)  class_acc: 0.2917 (0.3197)  loss_scale: 16384.0000 (27390.5259)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0300  max mem: 15572
Epoch: [22]  [ 530/1404]  eta: 0:08:40  lr: 0.000047  min_lr: 0.000000  loss: 4.0465 (4.1821)  class_acc: 0.3333 (0.3192)  loss_scale: 16384.0000 (27183.2467)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0421  max mem: 15572
Epoch: [22]  [ 540/1404]  eta: 0:08:34  lr: 0.000047  min_lr: 0.000000  loss: 4.1286 (4.1833)  class_acc: 0.2917 (0.3188)  loss_scale: 16384.0000 (26983.6303)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0184  max mem: 15572
Epoch: [22]  [ 550/1404]  eta: 0:08:27  lr: 0.000047  min_lr: 0.000000  loss: 4.2729 (4.1852)  class_acc: 0.2917 (0.3181)  loss_scale: 16384.0000 (26791.2595)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0008  max mem: 15572
Epoch: [22]  [ 560/1404]  eta: 0:08:21  lr: 0.000047  min_lr: 0.000000  loss: 4.2310 (4.1845)  class_acc: 0.2917 (0.3185)  loss_scale: 16384.0000 (26605.7469)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0007  max mem: 15572
Epoch: [22]  [ 570/1404]  eta: 0:08:14  lr: 0.000047  min_lr: 0.000000  loss: 4.2616 (4.1864)  class_acc: 0.2500 (0.3172)  loss_scale: 16384.0000 (26426.7320)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0071  max mem: 15572
Epoch: [22]  [ 580/1404]  eta: 0:08:08  lr: 0.000047  min_lr: 0.000000  loss: 4.3457 (4.1871)  class_acc: 0.2500 (0.3176)  loss_scale: 16384.0000 (26253.8795)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0504  max mem: 15572
Epoch: [22]  [ 590/1404]  eta: 0:08:02  lr: 0.000047  min_lr: 0.000000  loss: 4.2506 (4.1861)  class_acc: 0.3750 (0.3179)  loss_scale: 16384.0000 (26086.8765)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0700  max mem: 15572
[2025-01-17 01:41:35,379] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:41:35,379] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:41:35,416] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:41:35,416] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [ 600/1404]  eta: 0:07:56  lr: 0.000047  min_lr: 0.000000  loss: 4.1624 (4.1866)  class_acc: 0.3333 (0.3184)  loss_scale: 16384.0000 (26061.7371)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0795  max mem: 15572
Epoch: [22]  [ 610/1404]  eta: 0:07:50  lr: 0.000047  min_lr: 0.000000  loss: 4.1504 (4.1867)  class_acc: 0.3750 (0.3193)  loss_scale: 32768.0000 (26171.4959)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0534  max mem: 15572
Epoch: [22]  [ 620/1404]  eta: 0:07:45  lr: 0.000047  min_lr: 0.000000  loss: 4.1636 (4.1877)  class_acc: 0.3333 (0.3192)  loss_scale: 32768.0000 (26277.7198)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0628  max mem: 15572
Epoch: [22]  [ 630/1404]  eta: 0:07:40  lr: 0.000047  min_lr: 0.000000  loss: 4.2543 (4.1895)  class_acc: 0.2917 (0.3184)  loss_scale: 32768.0000 (26380.5769)  weight_decay: 0.0500 (0.0500)  time: 0.6549  data: 0.1635  max mem: 15572
[2025-01-17 01:41:57,472] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31520
[2025-01-17 01:41:57,473] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:41:57,473] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 01:41:57,483] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31520
[2025-01-17 01:41:57,483] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [22]  [ 640/1404]  eta: 0:07:33  lr: 0.000047  min_lr: 0.000000  loss: 4.3482 (4.1918)  class_acc: 0.2500 (0.3176)  loss_scale: 32768.0000 (26250.1841)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1281  max mem: 15572
Epoch: [22]  [ 650/1404]  eta: 0:07:27  lr: 0.000047  min_lr: 0.000000  loss: 4.1745 (4.1887)  class_acc: 0.2500 (0.3183)  loss_scale: 16384.0000 (26098.6298)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0889  max mem: 15572
Epoch: [22]  [ 660/1404]  eta: 0:07:20  lr: 0.000047  min_lr: 0.000000  loss: 4.0812 (4.1874)  class_acc: 0.2500 (0.3181)  loss_scale: 16384.0000 (25951.6611)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0811  max mem: 15572
Epoch: [22]  [ 670/1404]  eta: 0:07:15  lr: 0.000047  min_lr: 0.000000  loss: 4.2206 (4.1877)  class_acc: 0.2917 (0.3181)  loss_scale: 16384.0000 (25809.0730)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0836  max mem: 15572
Epoch: [22]  [ 680/1404]  eta: 0:07:09  lr: 0.000047  min_lr: 0.000000  loss: 4.2680 (4.1898)  class_acc: 0.3333 (0.3177)  loss_scale: 16384.0000 (25670.6725)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1048  max mem: 15572
Epoch: [22]  [ 690/1404]  eta: 0:07:03  lr: 0.000047  min_lr: 0.000000  loss: 4.3013 (4.1911)  class_acc: 0.2917 (0.3169)  loss_scale: 16384.0000 (25536.2779)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0806  max mem: 15572
Epoch: [22]  [ 700/1404]  eta: 0:06:56  lr: 0.000047  min_lr: 0.000000  loss: 4.2585 (4.1912)  class_acc: 0.2500 (0.3161)  loss_scale: 16384.0000 (25405.7175)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0680  max mem: 15572
Epoch: [22]  [ 710/1404]  eta: 0:06:50  lr: 0.000047  min_lr: 0.000000  loss: 4.2585 (4.1930)  class_acc: 0.2500 (0.3166)  loss_scale: 16384.0000 (25278.8298)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.0646  max mem: 15572
Epoch: [22]  [ 720/1404]  eta: 0:06:44  lr: 0.000047  min_lr: 0.000000  loss: 4.2280 (4.1926)  class_acc: 0.2917 (0.3169)  loss_scale: 16384.0000 (25155.4619)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0972  max mem: 15572
Epoch: [22]  [ 730/1404]  eta: 0:06:38  lr: 0.000047  min_lr: 0.000000  loss: 4.2333 (4.1941)  class_acc: 0.2917 (0.3159)  loss_scale: 16384.0000 (25035.4692)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0610  max mem: 15572
Epoch: [22]  [ 740/1404]  eta: 0:06:31  lr: 0.000047  min_lr: 0.000000  loss: 4.2519 (4.1937)  class_acc: 0.3333 (0.3160)  loss_scale: 16384.0000 (24918.7152)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0142  max mem: 15572
Epoch: [22]  [ 750/1404]  eta: 0:06:26  lr: 0.000047  min_lr: 0.000000  loss: 4.2455 (4.1947)  class_acc: 0.3333 (0.3158)  loss_scale: 16384.0000 (24805.0706)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0490  max mem: 15572
Epoch: [22]  [ 760/1404]  eta: 0:06:20  lr: 0.000047  min_lr: 0.000000  loss: 4.3416 (4.1968)  class_acc: 0.2917 (0.3161)  loss_scale: 16384.0000 (24694.4126)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0380  max mem: 15572
[2025-01-17 01:43:11,217] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:43:11,218] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:43:11,226] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:43:11,226] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [ 770/1404]  eta: 0:06:14  lr: 0.000047  min_lr: 0.000000  loss: 4.2545 (4.1965)  class_acc: 0.3333 (0.3159)  loss_scale: 16384.0000 (24799.1284)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0514  max mem: 15572
Epoch: [22]  [ 780/1404]  eta: 0:06:08  lr: 0.000047  min_lr: 0.000000  loss: 4.0648 (4.1965)  class_acc: 0.3333 (0.3153)  loss_scale: 32768.0000 (24901.1626)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0837  max mem: 15572
[2025-01-17 01:43:23,873] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31670
[2025-01-17 01:43:23,874] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:43:23,874] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 01:43:23,886] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31670
[2025-01-17 01:43:23,887] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [22]  [ 790/1404]  eta: 0:06:02  lr: 0.000047  min_lr: 0.000000  loss: 4.2268 (4.1956)  class_acc: 0.2917 (0.3160)  loss_scale: 32768.0000 (24814.1997)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1002  max mem: 15572
Epoch: [22]  [ 800/1404]  eta: 0:05:56  lr: 0.000047  min_lr: 0.000000  loss: 4.2268 (4.1967)  class_acc: 0.2917 (0.3155)  loss_scale: 16384.0000 (24708.9538)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0774  max mem: 15572
Epoch: [22]  [ 810/1404]  eta: 0:05:51  lr: 0.000047  min_lr: 0.000000  loss: 4.2050 (4.1985)  class_acc: 0.2500 (0.3151)  loss_scale: 16384.0000 (24606.3033)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0952  max mem: 15572
Epoch: [22]  [ 820/1404]  eta: 0:05:44  lr: 0.000047  min_lr: 0.000000  loss: 4.2028 (4.1985)  class_acc: 0.2500 (0.3146)  loss_scale: 16384.0000 (24506.1535)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1178  max mem: 15572
Epoch: [22]  [ 830/1404]  eta: 0:05:38  lr: 0.000047  min_lr: 0.000000  loss: 4.1780 (4.1990)  class_acc: 0.2083 (0.3142)  loss_scale: 16384.0000 (24408.4140)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.0804  max mem: 15572
Epoch: [22]  [ 840/1404]  eta: 0:05:32  lr: 0.000047  min_lr: 0.000000  loss: 4.2761 (4.2000)  class_acc: 0.2500 (0.3138)  loss_scale: 16384.0000 (24312.9988)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0704  max mem: 15572
Epoch: [22]  [ 850/1404]  eta: 0:05:26  lr: 0.000046  min_lr: 0.000000  loss: 4.1097 (4.1975)  class_acc: 0.2917 (0.3146)  loss_scale: 16384.0000 (24219.8261)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0517  max mem: 15572
Epoch: [22]  [ 860/1404]  eta: 0:05:20  lr: 0.000046  min_lr: 0.000000  loss: 4.0128 (4.1964)  class_acc: 0.2917 (0.3142)  loss_scale: 16384.0000 (24128.8177)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0808  max mem: 15572
Epoch: [22]  [ 870/1404]  eta: 0:05:14  lr: 0.000046  min_lr: 0.000000  loss: 4.1719 (4.1964)  class_acc: 0.2917 (0.3147)  loss_scale: 16384.0000 (24039.8990)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0834  max mem: 15572
Epoch: [22]  [ 880/1404]  eta: 0:05:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2927 (4.1982)  class_acc: 0.3333 (0.3141)  loss_scale: 16384.0000 (23952.9989)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0808  max mem: 15572
Epoch: [22]  [ 890/1404]  eta: 0:05:03  lr: 0.000046  min_lr: 0.000000  loss: 4.3290 (4.1980)  class_acc: 0.2917 (0.3140)  loss_scale: 16384.0000 (23868.0494)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1112  max mem: 15572
Epoch: [22]  [ 900/1404]  eta: 0:04:57  lr: 0.000046  min_lr: 0.000000  loss: 4.2737 (4.1998)  class_acc: 0.3333 (0.3140)  loss_scale: 16384.0000 (23784.9856)  weight_decay: 0.0500 (0.0500)  time: 0.6538  data: 0.1493  max mem: 15572
Epoch: [22]  [ 910/1404]  eta: 0:04:51  lr: 0.000046  min_lr: 0.000000  loss: 4.3034 (4.2013)  class_acc: 0.2917 (0.3135)  loss_scale: 16384.0000 (23703.7453)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0899  max mem: 15572
[2025-01-17 01:44:39,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:44:39,615] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:44:39,623] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:44:39,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [ 920/1404]  eta: 0:04:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2012 (4.2014)  class_acc: 0.2917 (0.3143)  loss_scale: 16384.0000 (23802.1629)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0261  max mem: 15572
Epoch: [22]  [ 930/1404]  eta: 0:04:39  lr: 0.000046  min_lr: 0.000000  loss: 4.2052 (4.2021)  class_acc: 0.4167 (0.3151)  loss_scale: 32768.0000 (23898.4662)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0635  max mem: 15572
[2025-01-17 01:44:55,715] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31828
[2025-01-17 01:44:55,715] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31828
[2025-01-17 01:44:55,715] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:44:55,715] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:44:55,715] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [22]  [ 940/1404]  eta: 0:04:33  lr: 0.000046  min_lr: 0.000000  loss: 4.2383 (4.2021)  class_acc: 0.3333 (0.3154)  loss_scale: 32768.0000 (23975.3114)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0491  max mem: 15572
Epoch: [22]  [ 950/1404]  eta: 0:04:27  lr: 0.000046  min_lr: 0.000000  loss: 4.2537 (4.2013)  class_acc: 0.3333 (0.3155)  loss_scale: 16384.0000 (23895.4869)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0668  max mem: 15572
Epoch: [22]  [ 960/1404]  eta: 0:04:21  lr: 0.000046  min_lr: 0.000000  loss: 4.1591 (4.2014)  class_acc: 0.3333 (0.3154)  loss_scale: 16384.0000 (23817.3236)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0818  max mem: 15572
Epoch: [22]  [ 970/1404]  eta: 0:04:15  lr: 0.000046  min_lr: 0.000000  loss: 4.1519 (4.2014)  class_acc: 0.3333 (0.3159)  loss_scale: 16384.0000 (23740.7703)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1213  max mem: 15572
Epoch: [22]  [ 980/1404]  eta: 0:04:09  lr: 0.000046  min_lr: 0.000000  loss: 4.2509 (4.2014)  class_acc: 0.3333 (0.3163)  loss_scale: 16384.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1193  max mem: 15572
Epoch: [22]  [ 990/1404]  eta: 0:04:03  lr: 0.000046  min_lr: 0.000000  loss: 4.2486 (4.2019)  class_acc: 0.2917 (0.3158)  loss_scale: 16384.0000 (23592.2987)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0647  max mem: 15572
Epoch: [22]  [1000/1404]  eta: 0:03:57  lr: 0.000046  min_lr: 0.000000  loss: 4.2025 (4.2009)  class_acc: 0.2917 (0.3165)  loss_scale: 16384.0000 (23520.2877)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0564  max mem: 15572
Epoch: [22]  [1010/1404]  eta: 0:03:51  lr: 0.000046  min_lr: 0.000000  loss: 4.1388 (4.2004)  class_acc: 0.3750 (0.3172)  loss_scale: 16384.0000 (23449.7013)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0163  max mem: 15572
Epoch: [22]  [1020/1404]  eta: 0:03:46  lr: 0.000046  min_lr: 0.000000  loss: 4.1706 (4.2005)  class_acc: 0.2917 (0.3167)  loss_scale: 16384.0000 (23380.4976)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1131  max mem: 15572
Epoch: [22]  [1030/1404]  eta: 0:03:40  lr: 0.000046  min_lr: 0.000000  loss: 4.1706 (4.2005)  class_acc: 0.2917 (0.3165)  loss_scale: 16384.0000 (23312.6363)  weight_decay: 0.0500 (0.0500)  time: 0.6811  data: 0.1821  max mem: 15572
Epoch: [22]  [1040/1404]  eta: 0:03:34  lr: 0.000046  min_lr: 0.000000  loss: 4.1480 (4.2012)  class_acc: 0.3333 (0.3170)  loss_scale: 16384.0000 (23246.0788)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1156  max mem: 15572
Epoch: [22]  [1050/1404]  eta: 0:03:28  lr: 0.000046  min_lr: 0.000000  loss: 4.3313 (4.2037)  class_acc: 0.2917 (0.3166)  loss_scale: 16384.0000 (23180.7878)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0987  max mem: 15572
Epoch: [22]  [1060/1404]  eta: 0:03:22  lr: 0.000046  min_lr: 0.000000  loss: 4.3313 (4.2036)  class_acc: 0.2500 (0.3164)  loss_scale: 16384.0000 (23116.7276)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1144  max mem: 15572
[2025-01-17 01:46:12,872] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:46:12,872] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:46:12,913] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:46:12,913] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [1070/1404]  eta: 0:03:16  lr: 0.000046  min_lr: 0.000000  loss: 4.0471 (4.2015)  class_acc: 0.2500 (0.3161)  loss_scale: 16384.0000 (23084.4594)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.0984  max mem: 15572
Epoch: [22]  [1080/1404]  eta: 0:03:11  lr: 0.000046  min_lr: 0.000000  loss: 4.0071 (4.2002)  class_acc: 0.2500 (0.3159)  loss_scale: 32768.0000 (23174.0389)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1270  max mem: 15572
Epoch: [22]  [1090/1404]  eta: 0:03:05  lr: 0.000046  min_lr: 0.000000  loss: 4.0889 (4.1992)  class_acc: 0.3333 (0.3163)  loss_scale: 32768.0000 (23261.9762)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0907  max mem: 15572
Epoch: [22]  [1100/1404]  eta: 0:02:58  lr: 0.000046  min_lr: 0.000000  loss: 4.1446 (4.1982)  class_acc: 0.3333 (0.3166)  loss_scale: 32768.0000 (23348.3161)  weight_decay: 0.0500 (0.0500)  time: 0.4890  data: 0.0005  max mem: 15572
Epoch: [22]  [1110/1404]  eta: 0:02:53  lr: 0.000046  min_lr: 0.000000  loss: 4.1804 (4.1979)  class_acc: 0.2917 (0.3163)  loss_scale: 32768.0000 (23433.1017)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0458  max mem: 15572
[2025-01-17 01:46:36,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=190, lr=[4.427590782729502e-07, 4.427590782729502e-07, 6.325129689613575e-07, 6.325129689613575e-07, 9.035899556590823e-07, 9.035899556590823e-07, 1.2908427937986889e-06, 1.2908427937986889e-06, 1.844061133998127e-06, 1.844061133998127e-06, 2.634373048568753e-06, 2.634373048568753e-06, 3.7633900693839335e-06, 3.7633900693839335e-06, 5.376271527691334e-06, 5.376271527691334e-06, 7.680387896701905e-06, 7.680387896701905e-06, 1.0971982709574152e-05, 1.0971982709574152e-05, 1.567426101367736e-05, 1.567426101367736e-05, 2.2391801448110516e-05, 2.2391801448110516e-05, 3.198828778301503e-05, 3.198828778301503e-05, 4.5697553975735754e-05, 4.5697553975735754e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 01:46:36,289] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=47.87680255626229, CurrSamplesPerSec=51.797757630862755, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [1120/1404]  eta: 0:02:46  lr: 0.000046  min_lr: 0.000000  loss: 4.2011 (4.1969)  class_acc: 0.2917 (0.3166)  loss_scale: 32768.0000 (23516.3747)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0458  max mem: 15572
[2025-01-17 01:46:42,291] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32011
[2025-01-17 01:46:42,291] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:46:42,291] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32011
[2025-01-17 01:46:42,291] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:46:42,291] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [22]  [1130/1404]  eta: 0:02:41  lr: 0.000046  min_lr: 0.000000  loss: 4.1335 (4.1962)  class_acc: 0.3333 (0.3168)  loss_scale: 32768.0000 (23482.2847)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1453  max mem: 15572
Epoch: [22]  [1140/1404]  eta: 0:02:35  lr: 0.000046  min_lr: 0.000000  loss: 4.0616 (4.1941)  class_acc: 0.3333 (0.3170)  loss_scale: 16384.0000 (23420.0736)  weight_decay: 0.0500 (0.0500)  time: 0.6411  data: 0.1716  max mem: 15572
Epoch: [22]  [1150/1404]  eta: 0:02:29  lr: 0.000046  min_lr: 0.000000  loss: 4.0699 (4.1941)  class_acc: 0.3333 (0.3174)  loss_scale: 16384.0000 (23358.9435)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0980  max mem: 15572
Epoch: [22]  [1160/1404]  eta: 0:02:23  lr: 0.000046  min_lr: 0.000000  loss: 4.1692 (4.1955)  class_acc: 0.2917 (0.3178)  loss_scale: 16384.0000 (23298.8665)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1311  max mem: 15572
Epoch: [22]  [1170/1404]  eta: 0:02:17  lr: 0.000046  min_lr: 0.000000  loss: 4.2835 (4.1957)  class_acc: 0.3333 (0.3181)  loss_scale: 16384.0000 (23239.8155)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1111  max mem: 15572
Epoch: [22]  [1180/1404]  eta: 0:02:11  lr: 0.000045  min_lr: 0.000000  loss: 4.2005 (4.1948)  class_acc: 0.3750 (0.3182)  loss_scale: 16384.0000 (23181.7646)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0539  max mem: 15572
Epoch: [22]  [1190/1404]  eta: 0:02:06  lr: 0.000045  min_lr: 0.000000  loss: 4.2005 (4.1959)  class_acc: 0.2917 (0.3180)  loss_scale: 16384.0000 (23124.6885)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1116  max mem: 15572
Epoch: [22]  [1200/1404]  eta: 0:02:00  lr: 0.000045  min_lr: 0.000000  loss: 4.2427 (4.1959)  class_acc: 0.2500 (0.3181)  loss_scale: 16384.0000 (23068.5629)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1095  max mem: 15572
Epoch: [22]  [1210/1404]  eta: 0:01:54  lr: 0.000045  min_lr: 0.000000  loss: 4.2427 (4.1966)  class_acc: 0.2917 (0.3182)  loss_scale: 16384.0000 (23013.3642)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0081  max mem: 15572
Epoch: [22]  [1220/1404]  eta: 0:01:48  lr: 0.000045  min_lr: 0.000000  loss: 4.1803 (4.1970)  class_acc: 0.2917 (0.3178)  loss_scale: 16384.0000 (22959.0696)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0412  max mem: 15572
Epoch: [22]  [1230/1404]  eta: 0:01:42  lr: 0.000045  min_lr: 0.000000  loss: 4.1008 (4.1961)  class_acc: 0.2917 (0.3179)  loss_scale: 16384.0000 (22905.6572)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0666  max mem: 15572
Epoch: [22]  [1240/1404]  eta: 0:01:36  lr: 0.000045  min_lr: 0.000000  loss: 4.1054 (4.1967)  class_acc: 0.2917 (0.3176)  loss_scale: 16384.0000 (22853.1056)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0989  max mem: 15572
Epoch: [22]  [1250/1404]  eta: 0:01:30  lr: 0.000045  min_lr: 0.000000  loss: 4.2382 (4.1967)  class_acc: 0.2917 (0.3184)  loss_scale: 16384.0000 (22801.3941)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.1498  max mem: 15572
[2025-01-17 01:47:59,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:47:59,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:47:59,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:47:59,615] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [1260/1404]  eta: 0:01:24  lr: 0.000045  min_lr: 0.000000  loss: 4.3491 (4.1978)  class_acc: 0.3333 (0.3181)  loss_scale: 16384.0000 (22867.4385)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1001  max mem: 15572
[2025-01-17 01:48:04,438] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32149
[2025-01-17 01:48:04,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:48:04,439] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 01:48:04,440] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32149
[2025-01-17 01:48:04,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [22]  [1270/1404]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000000  loss: 4.3353 (4.1984)  class_acc: 0.2500 (0.3180)  loss_scale: 16384.0000 (22816.4280)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0803  max mem: 15572
Epoch: [22]  [1280/1404]  eta: 0:01:13  lr: 0.000045  min_lr: 0.000000  loss: 4.2653 (4.1985)  class_acc: 0.2917 (0.3183)  loss_scale: 16384.0000 (22766.2139)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1308  max mem: 15572
Epoch: [22]  [1290/1404]  eta: 0:01:07  lr: 0.000045  min_lr: 0.000000  loss: 4.1131 (4.1990)  class_acc: 0.3333 (0.3183)  loss_scale: 16384.0000 (22716.7777)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1143  max mem: 15572
Epoch: [22]  [1300/1404]  eta: 0:01:01  lr: 0.000045  min_lr: 0.000000  loss: 4.2589 (4.1994)  class_acc: 0.2917 (0.3180)  loss_scale: 16384.0000 (22668.1015)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0482  max mem: 15572
Epoch: [22]  [1310/1404]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000000  loss: 4.2349 (4.1993)  class_acc: 0.3333 (0.3184)  loss_scale: 16384.0000 (22620.1678)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0341  max mem: 15572
Epoch: [22]  [1320/1404]  eta: 0:00:49  lr: 0.000045  min_lr: 0.000000  loss: 4.2062 (4.1998)  class_acc: 0.3333 (0.3183)  loss_scale: 16384.0000 (22572.9599)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0590  max mem: 15572
Epoch: [22]  [1330/1404]  eta: 0:00:43  lr: 0.000045  min_lr: 0.000000  loss: 4.1829 (4.1992)  class_acc: 0.2917 (0.3181)  loss_scale: 16384.0000 (22526.4613)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0409  max mem: 15572
Epoch: [22]  [1340/1404]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000000  loss: 4.1168 (4.1992)  class_acc: 0.3333 (0.3184)  loss_scale: 16384.0000 (22480.6562)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0562  max mem: 15572
Epoch: [22]  [1350/1404]  eta: 0:00:31  lr: 0.000045  min_lr: 0.000000  loss: 4.0899 (4.1974)  class_acc: 0.3333 (0.3186)  loss_scale: 16384.0000 (22435.5292)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0409  max mem: 15572
Epoch: [22]  [1360/1404]  eta: 0:00:25  lr: 0.000045  min_lr: 0.000000  loss: 4.1160 (4.1977)  class_acc: 0.2917 (0.3179)  loss_scale: 16384.0000 (22391.0654)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1036  max mem: 15572
Epoch: [22]  [1370/1404]  eta: 0:00:19  lr: 0.000045  min_lr: 0.000000  loss: 4.3134 (4.1976)  class_acc: 0.2917 (0.3178)  loss_scale: 16384.0000 (22347.2502)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1038  max mem: 15572
Epoch: [22]  [1380/1404]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000000  loss: 4.2428 (4.1981)  class_acc: 0.2917 (0.3178)  loss_scale: 16384.0000 (22304.0695)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0104  max mem: 15572
[2025-01-17 01:49:18,531] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:49:18,531] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [1390/1404]  eta: 0:00:08  lr: 0.000045  min_lr: 0.000000  loss: 4.3396 (4.1995)  class_acc: 0.2500 (0.3176)  loss_scale: 16384.0000 (22273.2883)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0474  max mem: 15572
[2025-01-17 01:49:18,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:49:18,546] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [1400/1404]  eta: 0:00:02  lr: 0.000045  min_lr: 0.000000  loss: 4.3581 (4.1999)  class_acc: 0.2917 (0.3176)  loss_scale: 32768.0000 (22348.1970)  weight_decay: 0.0500 (0.0500)  time: 0.4710  data: 0.0377  max mem: 15572
Epoch: [22]  [1403/1404]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 4.3581 (4.2002)  class_acc: 0.3333 (0.3177)  loss_scale: 32768.0000 (22370.4615)  weight_decay: 0.0500 (0.0500)  time: 0.4486  data: 0.0375  max mem: 15572
Epoch: [22] Total time: 0:13:42 (0.5858 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 4.3581 (4.2021)  class_acc: 0.3333 (0.3177)  loss_scale: 32768.0000 (22370.4615)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:10:01  loss: 1.9787 (1.9787)  acc1: 55.5556 (55.5556)  acc5: 83.3333 (83.3333)  time: 4.4201  data: 4.2400  max mem: 15572
Val:  [ 10/136]  eta: 0:01:28  loss: 2.4249 (2.4464)  acc1: 44.4444 (42.4242)  acc5: 77.7778 (76.7677)  time: 0.7063  data: 0.5177  max mem: 15572
Val:  [ 20/136]  eta: 0:01:00  loss: 2.5993 (2.5228)  acc1: 44.4444 (42.5926)  acc5: 72.2222 (75.6614)  time: 0.3266  data: 0.1369  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 2.4110 (2.4142)  acc1: 44.4444 (44.2652)  acc5: 77.7778 (77.5986)  time: 0.3312  data: 0.1327  max mem: 15572
Val:  [ 40/136]  eta: 0:00:41  loss: 2.1332 (2.3857)  acc1: 50.0000 (45.9350)  acc5: 83.3333 (77.6423)  time: 0.3302  data: 0.1254  max mem: 15572
Val:  [ 50/136]  eta: 0:00:35  loss: 2.2095 (2.3844)  acc1: 44.4444 (46.2963)  acc5: 83.3333 (78.5403)  time: 0.3467  data: 0.1363  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.3748 (2.4511)  acc1: 44.4444 (44.2623)  acc5: 77.7778 (77.2313)  time: 0.3691  data: 0.1573  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.3166 (2.4387)  acc1: 44.4444 (44.2880)  acc5: 77.7778 (77.7778)  time: 0.3543  data: 0.1496  max mem: 15572
Val:  [ 80/136]  eta: 0:00:21  loss: 2.2806 (2.4214)  acc1: 44.4444 (44.3073)  acc5: 83.3333 (78.1893)  time: 0.3439  data: 0.1496  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.3954 (2.4213)  acc1: 44.4444 (44.2002)  acc5: 77.7778 (78.0830)  time: 0.3747  data: 0.1839  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5841 (2.4701)  acc1: 38.8889 (42.5193)  acc5: 72.2222 (76.9527)  time: 0.3755  data: 0.1776  max mem: 15572
Val:  [110/136]  eta: 0:00:09  loss: 2.6160 (2.4607)  acc1: 38.8889 (43.2432)  acc5: 72.2222 (76.9269)  time: 0.3309  data: 0.1358  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1390 (2.4206)  acc1: 50.0000 (44.6281)  acc5: 83.3333 (77.9155)  time: 0.3618  data: 0.1723  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.0257 (2.3891)  acc1: 61.1111 (45.8015)  acc5: 88.8889 (78.4563)  time: 0.3318  data: 0.1660  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.0681 (2.3919)  acc1: 55.5556 (46.0278)  acc5: 83.3333 (78.4603)  time: 0.2074  data: 0.0580  max mem: 15572
Val: Total time: 0:00:49 (0.3674 s / it)
* Acc@1 44.881 Acc@5 77.416 loss 2.438
Accuracy of the network on the 4883 val videos: 44.9%
[2025-01-17 01:50:13,678] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 01:50:13,680] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 01:50:13,680] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 01:50:13,680] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 01:50:16,165] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 01:50:16,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.88%
Epoch: [23]  [   0/1404]  eta: 3:24:12  lr: 0.000045  min_lr: 0.000000  loss: 4.5798 (4.5798)  class_acc: 0.0833 (0.0833)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.7268  data: 6.8167  max mem: 15572
Epoch: [23]  [  10/1404]  eta: 0:28:49  lr: 0.000045  min_lr: 0.000000  loss: 4.2743 (4.2180)  class_acc: 0.2500 (0.2652)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2408  data: 0.6203  max mem: 15572
Epoch: [23]  [  20/1404]  eta: 0:20:24  lr: 0.000045  min_lr: 0.000000  loss: 4.2326 (4.2344)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4925  data: 0.0032  max mem: 15572
Epoch: [23]  [  30/1404]  eta: 0:18:17  lr: 0.000045  min_lr: 0.000000  loss: 4.2837 (4.2933)  class_acc: 0.2917 (0.2903)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0728  max mem: 15572
Epoch: [23]  [  40/1404]  eta: 0:17:09  lr: 0.000045  min_lr: 0.000000  loss: 4.2837 (4.2529)  class_acc: 0.2500 (0.2795)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1351  max mem: 15572
Epoch: [23]  [  50/1404]  eta: 0:16:21  lr: 0.000045  min_lr: 0.000000  loss: 4.2468 (4.2525)  class_acc: 0.2500 (0.2868)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1359  max mem: 15572
Epoch: [23]  [  60/1404]  eta: 0:15:58  lr: 0.000045  min_lr: 0.000000  loss: 4.2675 (4.2432)  class_acc: 0.2917 (0.2889)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1625  max mem: 15572
Epoch: [23]  [  70/1404]  eta: 0:15:17  lr: 0.000045  min_lr: 0.000000  loss: 4.2280 (4.2385)  class_acc: 0.2917 (0.2987)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1238  max mem: 15572
Epoch: [23]  [  80/1404]  eta: 0:14:55  lr: 0.000045  min_lr: 0.000000  loss: 4.2562 (4.2288)  class_acc: 0.3750 (0.3045)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0907  max mem: 15572
Epoch: [23]  [  90/1404]  eta: 0:14:35  lr: 0.000045  min_lr: 0.000000  loss: 4.3153 (4.2373)  class_acc: 0.3333 (0.3082)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0978  max mem: 15572
[2025-01-17 01:51:22,043] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32392
[2025-01-17 01:51:22,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:51:22,045] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [ 100/1404]  eta: 0:14:10  lr: 0.000045  min_lr: 0.000000  loss: 4.3641 (4.2478)  class_acc: 0.3333 (0.3115)  loss_scale: 32768.0000 (32605.7822)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0443  max mem: 15572
[2025-01-17 01:51:22,057] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32392
[2025-01-17 01:51:22,058] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [23]  [ 110/1404]  eta: 0:14:07  lr: 0.000044  min_lr: 0.000000  loss: 4.1663 (4.2267)  class_acc: 0.3750 (0.3153)  loss_scale: 16384.0000 (31144.3604)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1020  max mem: 15572
Epoch: [23]  [ 120/1404]  eta: 0:13:46  lr: 0.000044  min_lr: 0.000000  loss: 4.1312 (4.2153)  class_acc: 0.3750 (0.3147)  loss_scale: 16384.0000 (29924.4959)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1274  max mem: 15572
Epoch: [23]  [ 130/1404]  eta: 0:13:24  lr: 0.000044  min_lr: 0.000000  loss: 4.1612 (4.2141)  class_acc: 0.3333 (0.3149)  loss_scale: 16384.0000 (28890.8702)  weight_decay: 0.0500 (0.0500)  time: 0.5055  data: 0.0461  max mem: 15572
Epoch: [23]  [ 140/1404]  eta: 0:13:14  lr: 0.000044  min_lr: 0.000000  loss: 4.1145 (4.2056)  class_acc: 0.2917 (0.3177)  loss_scale: 16384.0000 (28003.8582)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0556  max mem: 15572
Epoch: [23]  [ 150/1404]  eta: 0:12:58  lr: 0.000044  min_lr: 0.000000  loss: 4.1935 (4.2068)  class_acc: 0.2917 (0.3151)  loss_scale: 16384.0000 (27234.3311)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0406  max mem: 15572
Epoch: [23]  [ 160/1404]  eta: 0:12:57  lr: 0.000044  min_lr: 0.000000  loss: 4.1623 (4.2028)  class_acc: 0.2917 (0.3157)  loss_scale: 16384.0000 (26560.3975)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.0119  max mem: 15572
Epoch: [23]  [ 170/1404]  eta: 0:12:49  lr: 0.000044  min_lr: 0.000000  loss: 4.0975 (4.1966)  class_acc: 0.2917 (0.3131)  loss_scale: 16384.0000 (25965.2865)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.0116  max mem: 15572
Epoch: [23]  [ 180/1404]  eta: 0:12:39  lr: 0.000044  min_lr: 0.000000  loss: 4.2024 (4.1992)  class_acc: 0.2500 (0.3124)  loss_scale: 16384.0000 (25435.9337)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.0006  max mem: 15572
Epoch: [23]  [ 190/1404]  eta: 0:12:24  lr: 0.000044  min_lr: 0.000000  loss: 4.2105 (4.1956)  class_acc: 0.2500 (0.3126)  loss_scale: 16384.0000 (24962.0105)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0007  max mem: 15572
Epoch: [23]  [ 200/1404]  eta: 0:12:12  lr: 0.000044  min_lr: 0.000000  loss: 4.1035 (4.1883)  class_acc: 0.3333 (0.3165)  loss_scale: 16384.0000 (24535.2438)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0173  max mem: 15572
Epoch: [23]  [ 210/1404]  eta: 0:12:08  lr: 0.000044  min_lr: 0.000000  loss: 4.1152 (4.1885)  class_acc: 0.2917 (0.3144)  loss_scale: 16384.0000 (24148.9289)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0696  max mem: 15572
Epoch: [23]  [ 220/1404]  eta: 0:12:00  lr: 0.000044  min_lr: 0.000000  loss: 4.1152 (4.1840)  class_acc: 0.3333 (0.3169)  loss_scale: 16384.0000 (23797.5747)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.0750  max mem: 15572
[2025-01-17 01:52:35,983] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:52:35,983] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:52:35,990] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:52:35,991] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [ 230/1404]  eta: 0:11:56  lr: 0.000044  min_lr: 0.000000  loss: 4.1110 (4.1862)  class_acc: 0.3333 (0.3178)  loss_scale: 16384.0000 (23618.4935)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0624  max mem: 15572
Epoch: [23]  [ 240/1404]  eta: 0:11:49  lr: 0.000044  min_lr: 0.000000  loss: 4.1641 (4.1858)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (23998.1411)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0512  max mem: 15572
[2025-01-17 01:52:48,585] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32540
[2025-01-17 01:52:48,585] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:52:48,601] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32540
[2025-01-17 01:52:48,602] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:52:48,602] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [ 250/1404]  eta: 0:11:45  lr: 0.000044  min_lr: 0.000000  loss: 3.9477 (4.1761)  class_acc: 0.4167 (0.3215)  loss_scale: 32768.0000 (24151.7131)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.0115  max mem: 15572
Epoch: [23]  [ 260/1404]  eta: 0:11:37  lr: 0.000044  min_lr: 0.000000  loss: 3.9069 (4.1710)  class_acc: 0.4167 (0.3231)  loss_scale: 16384.0000 (23854.0996)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0009  max mem: 15572
Epoch: [23]  [ 270/1404]  eta: 0:11:33  lr: 0.000044  min_lr: 0.000000  loss: 4.1341 (4.1673)  class_acc: 0.2500 (0.3210)  loss_scale: 16384.0000 (23578.4502)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0009  max mem: 15572
Epoch: [23]  [ 280/1404]  eta: 0:11:21  lr: 0.000044  min_lr: 0.000000  loss: 4.1887 (4.1719)  class_acc: 0.2500 (0.3200)  loss_scale: 16384.0000 (23322.4199)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0006  max mem: 15572
Epoch: [23]  [ 290/1404]  eta: 0:11:13  lr: 0.000044  min_lr: 0.000000  loss: 4.2690 (4.1731)  class_acc: 0.2917 (0.3182)  loss_scale: 16384.0000 (23083.9863)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0005  max mem: 15572
Epoch: [23]  [ 300/1404]  eta: 0:11:06  lr: 0.000044  min_lr: 0.000000  loss: 4.1118 (4.1708)  class_acc: 0.2917 (0.3194)  loss_scale: 16384.0000 (22861.3953)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0389  max mem: 15572
Epoch: [23]  [ 310/1404]  eta: 0:10:57  lr: 0.000044  min_lr: 0.000000  loss: 4.1118 (4.1729)  class_acc: 0.2917 (0.3187)  loss_scale: 16384.0000 (22653.1190)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0496  max mem: 15572
Epoch: [23]  [ 320/1404]  eta: 0:10:54  lr: 0.000044  min_lr: 0.000000  loss: 4.2508 (4.1728)  class_acc: 0.3333 (0.3193)  loss_scale: 16384.0000 (22457.8193)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1127  max mem: 15572
Epoch: [23]  [ 330/1404]  eta: 0:10:48  lr: 0.000044  min_lr: 0.000000  loss: 4.1330 (4.1693)  class_acc: 0.3333 (0.3192)  loss_scale: 16384.0000 (22274.3202)  weight_decay: 0.0500 (0.0500)  time: 0.6447  data: 0.1546  max mem: 15572
Epoch: [23]  [ 340/1404]  eta: 0:10:39  lr: 0.000044  min_lr: 0.000000  loss: 4.1474 (4.1716)  class_acc: 0.3333 (0.3195)  loss_scale: 16384.0000 (22101.5836)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0603  max mem: 15572
Epoch: [23]  [ 350/1404]  eta: 0:10:33  lr: 0.000044  min_lr: 0.000000  loss: 4.2897 (4.1732)  class_acc: 0.2500 (0.3179)  loss_scale: 16384.0000 (21938.6895)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0522  max mem: 15572
Epoch: [23]  [ 360/1404]  eta: 0:10:27  lr: 0.000044  min_lr: 0.000000  loss: 4.3051 (4.1734)  class_acc: 0.2917 (0.3181)  loss_scale: 16384.0000 (21784.8199)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0490  max mem: 15572
Epoch: [23]  [ 370/1404]  eta: 0:10:19  lr: 0.000044  min_lr: 0.000000  loss: 4.0320 (4.1721)  class_acc: 0.3333 (0.3193)  loss_scale: 16384.0000 (21639.2453)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0472  max mem: 15572
[2025-01-17 01:54:02,829] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:54:02,829] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:54:02,860] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:54:02,860] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [ 380/1404]  eta: 0:10:14  lr: 0.000044  min_lr: 0.000000  loss: 4.0712 (4.1752)  class_acc: 0.2917 (0.3176)  loss_scale: 16384.0000 (21673.3228)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0986  max mem: 15572
[2025-01-17 01:54:07,077] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32676
[2025-01-17 01:54:07,078] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:54:07,079] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32676
[2025-01-17 01:54:07,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:54:07,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [ 390/1404]  eta: 0:10:07  lr: 0.000044  min_lr: 0.000000  loss: 4.2805 (4.1798)  class_acc: 0.2500 (0.3172)  loss_scale: 16384.0000 (21663.7545)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0911  max mem: 15572
Epoch: [23]  [ 400/1404]  eta: 0:10:01  lr: 0.000044  min_lr: 0.000000  loss: 4.2552 (4.1799)  class_acc: 0.2917 (0.3193)  loss_scale: 16384.0000 (21532.0898)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.0722  max mem: 15572
Epoch: [23]  [ 410/1404]  eta: 0:09:54  lr: 0.000044  min_lr: 0.000000  loss: 4.1197 (4.1741)  class_acc: 0.3750 (0.3207)  loss_scale: 16384.0000 (21406.8321)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0838  max mem: 15572
Epoch: [23]  [ 420/1404]  eta: 0:09:48  lr: 0.000044  min_lr: 0.000000  loss: 3.9372 (4.1721)  class_acc: 0.3333 (0.3206)  loss_scale: 16384.0000 (21287.5249)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0870  max mem: 15572
Epoch: [23]  [ 430/1404]  eta: 0:09:42  lr: 0.000044  min_lr: 0.000000  loss: 4.1889 (4.1735)  class_acc: 0.2917 (0.3206)  loss_scale: 16384.0000 (21173.7541)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.0780  max mem: 15572
Epoch: [23]  [ 440/1404]  eta: 0:09:37  lr: 0.000044  min_lr: 0.000000  loss: 4.2521 (4.1773)  class_acc: 0.2500 (0.3192)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.0650  max mem: 15572
Epoch: [23]  [ 450/1404]  eta: 0:09:28  lr: 0.000043  min_lr: 0.000000  loss: 4.3511 (4.1808)  class_acc: 0.2917 (0.3190)  loss_scale: 16384.0000 (20961.3481)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0271  max mem: 15572
Epoch: [23]  [ 460/1404]  eta: 0:09:22  lr: 0.000043  min_lr: 0.000000  loss: 4.0823 (4.1774)  class_acc: 0.3333 (0.3196)  loss_scale: 16384.0000 (20862.0564)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0006  max mem: 15572
Epoch: [23]  [ 470/1404]  eta: 0:09:16  lr: 0.000043  min_lr: 0.000000  loss: 4.0535 (4.1756)  class_acc: 0.2917 (0.3193)  loss_scale: 16384.0000 (20766.9809)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0006  max mem: 15572
Epoch: [23]  [ 480/1404]  eta: 0:09:12  lr: 0.000043  min_lr: 0.000000  loss: 4.2204 (4.1783)  class_acc: 0.2917 (0.3181)  loss_scale: 16384.0000 (20675.8586)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.0007  max mem: 15572
Epoch: [23]  [ 490/1404]  eta: 0:09:04  lr: 0.000043  min_lr: 0.000000  loss: 4.2916 (4.1805)  class_acc: 0.3333 (0.3194)  loss_scale: 16384.0000 (20588.4481)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0007  max mem: 15572
Epoch: [23]  [ 500/1404]  eta: 0:08:58  lr: 0.000043  min_lr: 0.000000  loss: 4.2798 (4.1834)  class_acc: 0.3750 (0.3193)  loss_scale: 16384.0000 (20504.5269)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0005  max mem: 15572
Epoch: [23]  [ 510/1404]  eta: 0:08:53  lr: 0.000043  min_lr: 0.000000  loss: 4.1360 (4.1812)  class_acc: 0.3333 (0.3206)  loss_scale: 16384.0000 (20423.8904)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.0005  max mem: 15572
[2025-01-17 01:55:22,939] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:55:22,939] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:55:23,025] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:55:23,025] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [ 520/1404]  eta: 0:08:46  lr: 0.000043  min_lr: 0.000000  loss: 4.1028 (4.1814)  class_acc: 0.3333 (0.3217)  loss_scale: 16384.0000 (20597.9271)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0008  max mem: 15572
[2025-01-17 01:55:27,126] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32813
[2025-01-17 01:55:27,126] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32813
[2025-01-17 01:55:27,126] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:55:27,126] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:55:27,126] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [ 530/1404]  eta: 0:08:38  lr: 0.000043  min_lr: 0.000000  loss: 4.2744 (4.1833)  class_acc: 0.2917 (0.3207)  loss_scale: 16384.0000 (20518.5687)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0008  max mem: 15572
Epoch: [23]  [ 540/1404]  eta: 0:08:33  lr: 0.000043  min_lr: 0.000000  loss: 4.2553 (4.1830)  class_acc: 0.2500 (0.3198)  loss_scale: 16384.0000 (20442.1442)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0233  max mem: 15572
Epoch: [23]  [ 550/1404]  eta: 0:08:26  lr: 0.000043  min_lr: 0.000000  loss: 4.2279 (4.1840)  class_acc: 0.2917 (0.3203)  loss_scale: 16384.0000 (20368.4936)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0232  max mem: 15572
Epoch: [23]  [ 560/1404]  eta: 0:08:20  lr: 0.000043  min_lr: 0.000000  loss: 4.3425 (4.1849)  class_acc: 0.2917 (0.3194)  loss_scale: 16384.0000 (20297.4688)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0735  max mem: 15572
Epoch: [23]  [ 570/1404]  eta: 0:08:15  lr: 0.000043  min_lr: 0.000000  loss: 4.2088 (4.1851)  class_acc: 0.2917 (0.3190)  loss_scale: 16384.0000 (20228.9317)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.1200  max mem: 15572
Epoch: [23]  [ 580/1404]  eta: 0:08:10  lr: 0.000043  min_lr: 0.000000  loss: 4.1699 (4.1849)  class_acc: 0.2917 (0.3195)  loss_scale: 16384.0000 (20162.7539)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.1185  max mem: 15572
Epoch: [23]  [ 590/1404]  eta: 0:08:06  lr: 0.000043  min_lr: 0.000000  loss: 4.0010 (4.1804)  class_acc: 0.2917 (0.3198)  loss_scale: 16384.0000 (20098.8156)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.2119  max mem: 15572
Epoch: [23]  [ 600/1404]  eta: 0:07:58  lr: 0.000043  min_lr: 0.000000  loss: 4.0159 (4.1802)  class_acc: 0.2917 (0.3194)  loss_scale: 16384.0000 (20037.0050)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1404  max mem: 15572
Epoch: [23]  [ 610/1404]  eta: 0:07:53  lr: 0.000043  min_lr: 0.000000  loss: 4.1290 (4.1790)  class_acc: 0.2500 (0.3190)  loss_scale: 16384.0000 (19977.2177)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0607  max mem: 15572
Epoch: [23]  [ 620/1404]  eta: 0:07:46  lr: 0.000043  min_lr: 0.000000  loss: 4.2042 (4.1801)  class_acc: 0.2500 (0.3178)  loss_scale: 16384.0000 (19919.3559)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0958  max mem: 15572
Epoch: [23]  [ 630/1404]  eta: 0:07:40  lr: 0.000043  min_lr: 0.000000  loss: 4.2891 (4.1811)  class_acc: 0.2917 (0.3185)  loss_scale: 16384.0000 (19863.3281)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0670  max mem: 15572
Epoch: [23]  [ 640/1404]  eta: 0:07:33  lr: 0.000043  min_lr: 0.000000  loss: 4.0503 (4.1802)  class_acc: 0.3333 (0.3196)  loss_scale: 16384.0000 (19809.0484)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0374  max mem: 15572
[2025-01-17 01:56:42,344] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:56:42,344] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [ 650/1404]  eta: 0:07:27  lr: 0.000043  min_lr: 0.000000  loss: 4.1781 (4.1813)  class_acc: 0.3333 (0.3189)  loss_scale: 16384.0000 (19781.6037)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0343  max mem: 15572
[2025-01-17 01:56:42,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:56:42,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [ 660/1404]  eta: 0:07:20  lr: 0.000043  min_lr: 0.000000  loss: 4.1549 (4.1795)  class_acc: 0.2917 (0.3186)  loss_scale: 32768.0000 (19978.0696)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0527  max mem: 15572
Epoch: [23]  [ 670/1404]  eta: 0:07:14  lr: 0.000043  min_lr: 0.000000  loss: 4.0945 (4.1786)  class_acc: 0.2917 (0.3181)  loss_scale: 32768.0000 (20168.6796)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0477  max mem: 15572
Epoch: [23]  [ 680/1404]  eta: 0:07:10  lr: 0.000043  min_lr: 0.000000  loss: 4.1332 (4.1787)  class_acc: 0.2500 (0.3172)  loss_scale: 32768.0000 (20353.6916)  weight_decay: 0.0500 (0.0500)  time: 0.6694  data: 0.1529  max mem: 15572
Epoch: [23]  [ 690/1404]  eta: 0:07:03  lr: 0.000043  min_lr: 0.000000  loss: 4.0974 (4.1758)  class_acc: 0.3333 (0.3179)  loss_scale: 32768.0000 (20533.3488)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1295  max mem: 15572
Epoch: [23]  [ 700/1404]  eta: 0:06:56  lr: 0.000043  min_lr: 0.000000  loss: 4.1502 (4.1767)  class_acc: 0.3333 (0.3177)  loss_scale: 32768.0000 (20707.8802)  weight_decay: 0.0500 (0.0500)  time: 0.4859  data: 0.0007  max mem: 15572
[2025-01-17 01:57:14,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=33000, skipped=196, lr=[4.138064307813978e-07, 4.138064307813978e-07, 5.911520439734255e-07, 5.911520439734255e-07, 8.445029199620364e-07, 8.445029199620364e-07, 1.2064327428029093e-06, 1.2064327428029093e-06, 1.7234753468612991e-06, 1.7234753468612991e-06, 2.4621076383732845e-06, 2.4621076383732845e-06, 3.517296626247549e-06, 3.517296626247549e-06, 5.024709466067928e-06, 5.024709466067928e-06, 7.17815638009704e-06, 7.17815638009704e-06, 1.0254509114424345e-05, 1.0254509114424345e-05, 1.4649298734891921e-05, 1.4649298734891921e-05, 2.0927569621274175e-05, 2.0927569621274175e-05, 2.989652803039168e-05, 2.989652803039168e-05, 4.27093257577024e-05, 4.27093257577024e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 01:57:14,865] [INFO] [timer.py:260:stop] epoch=0/micro_step=33000/global_step=33000, RunningAvgSamplesPerSec=47.926209948760494, CurrSamplesPerSec=55.284655577877636, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [23]  [ 710/1404]  eta: 0:06:50  lr: 0.000043  min_lr: 0.000000  loss: 4.2512 (4.1762)  class_acc: 0.2917 (0.3180)  loss_scale: 32768.0000 (20877.5021)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0224  max mem: 15572
Epoch: [23]  [ 720/1404]  eta: 0:06:45  lr: 0.000043  min_lr: 0.000000  loss: 4.1292 (4.1758)  class_acc: 0.2917 (0.3176)  loss_scale: 32768.0000 (21042.4189)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1027  max mem: 15572
Epoch: [23]  [ 730/1404]  eta: 0:06:40  lr: 0.000043  min_lr: 0.000000  loss: 4.1659 (4.1780)  class_acc: 0.2500 (0.3171)  loss_scale: 32768.0000 (21202.8235)  weight_decay: 0.0500 (0.0500)  time: 0.6988  data: 0.1910  max mem: 15572
Epoch: [23]  [ 740/1404]  eta: 0:06:33  lr: 0.000043  min_lr: 0.000000  loss: 4.1868 (4.1771)  class_acc: 0.3333 (0.3170)  loss_scale: 32768.0000 (21358.8988)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1105  max mem: 15572
[2025-01-17 01:57:36,704] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33034
[2025-01-17 01:57:36,704] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33034
[2025-01-17 01:57:36,704] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:57:36,704] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 01:57:36,704] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [ 750/1404]  eta: 0:06:26  lr: 0.000043  min_lr: 0.000000  loss: 4.1868 (4.1791)  class_acc: 0.2500 (0.3156)  loss_scale: 32768.0000 (21314.4714)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0005  max mem: 15572
Epoch: [23]  [ 760/1404]  eta: 0:06:20  lr: 0.000043  min_lr: 0.000000  loss: 4.2785 (4.1810)  class_acc: 0.2500 (0.3150)  loss_scale: 16384.0000 (21249.6820)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0486  max mem: 15572
Epoch: [23]  [ 770/1404]  eta: 0:06:15  lr: 0.000043  min_lr: 0.000000  loss: 4.2694 (4.1816)  class_acc: 0.2500 (0.3147)  loss_scale: 16384.0000 (21186.5733)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1342  max mem: 15572
Epoch: [23]  [ 780/1404]  eta: 0:06:08  lr: 0.000042  min_lr: 0.000000  loss: 4.2008 (4.1821)  class_acc: 0.2500 (0.3149)  loss_scale: 16384.0000 (21125.0807)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0862  max mem: 15572
Epoch: [23]  [ 790/1404]  eta: 0:06:03  lr: 0.000042  min_lr: 0.000000  loss: 4.1438 (4.1814)  class_acc: 0.3333 (0.3152)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0852  max mem: 15572
Epoch: [23]  [ 800/1404]  eta: 0:05:56  lr: 0.000042  min_lr: 0.000000  loss: 4.1030 (4.1806)  class_acc: 0.3333 (0.3154)  loss_scale: 16384.0000 (21006.7016)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1027  max mem: 15572
Epoch: [23]  [ 810/1404]  eta: 0:05:51  lr: 0.000042  min_lr: 0.000000  loss: 4.2443 (4.1815)  class_acc: 0.3333 (0.3163)  loss_scale: 16384.0000 (20949.7016)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0815  max mem: 15572
Epoch: [23]  [ 820/1404]  eta: 0:05:45  lr: 0.000042  min_lr: 0.000000  loss: 4.2418 (4.1828)  class_acc: 0.2917 (0.3162)  loss_scale: 16384.0000 (20894.0901)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1131  max mem: 15572
Epoch: [23]  [ 830/1404]  eta: 0:05:39  lr: 0.000042  min_lr: 0.000000  loss: 4.1655 (4.1816)  class_acc: 0.2917 (0.3167)  loss_scale: 16384.0000 (20839.8171)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0710  max mem: 15572
Epoch: [23]  [ 840/1404]  eta: 0:05:32  lr: 0.000042  min_lr: 0.000000  loss: 4.0577 (4.1814)  class_acc: 0.3333 (0.3168)  loss_scale: 16384.0000 (20786.8347)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0219  max mem: 15572
Epoch: [23]  [ 850/1404]  eta: 0:05:26  lr: 0.000042  min_lr: 0.000000  loss: 4.1081 (4.1801)  class_acc: 0.3750 (0.3184)  loss_scale: 16384.0000 (20735.0975)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0533  max mem: 15572
Epoch: [23]  [ 860/1404]  eta: 0:05:20  lr: 0.000042  min_lr: 0.000000  loss: 4.2071 (4.1809)  class_acc: 0.3750 (0.3187)  loss_scale: 16384.0000 (20684.5621)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.0792  max mem: 15572
Epoch: [23]  [ 870/1404]  eta: 0:05:15  lr: 0.000042  min_lr: 0.000000  loss: 4.2943 (4.1819)  class_acc: 0.2917 (0.3186)  loss_scale: 16384.0000 (20635.1871)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0934  max mem: 15572
[2025-01-17 01:58:52,061] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:58:52,062] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 01:58:52,062] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 01:58:52,062] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [ 880/1404]  eta: 0:05:09  lr: 0.000042  min_lr: 0.000000  loss: 4.2788 (4.1823)  class_acc: 0.3333 (0.3191)  loss_scale: 16384.0000 (20772.9035)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1279  max mem: 15572
Epoch: [23]  [ 890/1404]  eta: 0:05:03  lr: 0.000042  min_lr: 0.000000  loss: 4.2224 (4.1814)  class_acc: 0.3333 (0.3197)  loss_scale: 32768.0000 (20907.5286)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1482  max mem: 15572
Epoch: [23]  [ 900/1404]  eta: 0:04:57  lr: 0.000042  min_lr: 0.000000  loss: 4.1373 (4.1822)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (21039.1654)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1281  max mem: 15572
Epoch: [23]  [ 910/1404]  eta: 0:04:51  lr: 0.000042  min_lr: 0.000000  loss: 4.1373 (4.1814)  class_acc: 0.2500 (0.3195)  loss_scale: 32768.0000 (21167.9122)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1141  max mem: 15572
Epoch: [23]  [ 920/1404]  eta: 0:04:45  lr: 0.000042  min_lr: 0.000000  loss: 4.1192 (4.1812)  class_acc: 0.3333 (0.3199)  loss_scale: 32768.0000 (21293.8632)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0929  max mem: 15572
Epoch: [23]  [ 930/1404]  eta: 0:04:39  lr: 0.000042  min_lr: 0.000000  loss: 3.9956 (4.1794)  class_acc: 0.3333 (0.3204)  loss_scale: 32768.0000 (21417.1085)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0860  max mem: 15572
Epoch: [23]  [ 940/1404]  eta: 0:04:33  lr: 0.000042  min_lr: 0.000000  loss: 3.9956 (4.1781)  class_acc: 0.3333 (0.3208)  loss_scale: 32768.0000 (21537.7343)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0987  max mem: 15572
Epoch: [23]  [ 950/1404]  eta: 0:04:27  lr: 0.000042  min_lr: 0.000000  loss: 4.0258 (4.1779)  class_acc: 0.2917 (0.3203)  loss_scale: 32768.0000 (21655.8233)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0502  max mem: 15572
Epoch: [23]  [ 960/1404]  eta: 0:04:22  lr: 0.000042  min_lr: 0.000000  loss: 4.1073 (4.1773)  class_acc: 0.2917 (0.3206)  loss_scale: 32768.0000 (21771.4547)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0728  max mem: 15572
Epoch: [23]  [ 970/1404]  eta: 0:04:16  lr: 0.000042  min_lr: 0.000000  loss: 4.2131 (4.1782)  class_acc: 0.3750 (0.3208)  loss_scale: 32768.0000 (21884.7044)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.0992  max mem: 15572
Epoch: [23]  [ 980/1404]  eta: 0:04:10  lr: 0.000042  min_lr: 0.000000  loss: 4.2257 (4.1790)  class_acc: 0.2917 (0.3203)  loss_scale: 32768.0000 (21995.6453)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0751  max mem: 15572
Epoch: [23]  [ 990/1404]  eta: 0:04:04  lr: 0.000042  min_lr: 0.000000  loss: 4.1949 (4.1791)  class_acc: 0.2917 (0.3205)  loss_scale: 32768.0000 (22104.3471)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0664  max mem: 15572
[2025-01-17 02:00:05,818] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:00:05,818] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 02:00:05,831] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:00:05,832] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 02:00:06,329] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33292
[2025-01-17 02:00:06,330] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 02:00:06,330] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [1000/1404]  eta: 0:03:58  lr: 0.000042  min_lr: 0.000000  loss: 4.3195 (4.1801)  class_acc: 0.2917 (0.3206)  loss_scale: 32768.0000 (22243.6124)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0633  max mem: 15572
[2025-01-17 02:00:06,377] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33292
[2025-01-17 02:00:06,378] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [23]  [1010/1404]  eta: 0:03:52  lr: 0.000042  min_lr: 0.000000  loss: 4.2110 (4.1794)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (22347.7112)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0700  max mem: 15572
Epoch: [23]  [1020/1404]  eta: 0:03:46  lr: 0.000042  min_lr: 0.000000  loss: 4.1292 (4.1797)  class_acc: 0.2917 (0.3205)  loss_scale: 32768.0000 (22449.7708)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0593  max mem: 15572
Epoch: [23]  [1030/1404]  eta: 0:03:39  lr: 0.000042  min_lr: 0.000000  loss: 4.2029 (4.1797)  class_acc: 0.3333 (0.3201)  loss_scale: 32768.0000 (22549.8506)  weight_decay: 0.0500 (0.0500)  time: 0.5224  data: 0.0170  max mem: 15572
Epoch: [23]  [1040/1404]  eta: 0:03:33  lr: 0.000042  min_lr: 0.000000  loss: 4.1841 (4.1804)  class_acc: 0.2917 (0.3200)  loss_scale: 32768.0000 (22648.0077)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0335  max mem: 15572
[2025-01-17 02:00:34,800] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33341
[2025-01-17 02:00:34,801] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:00:34,801] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 02:00:34,802] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33341
[2025-01-17 02:00:34,802] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [23]  [1050/1404]  eta: 0:03:28  lr: 0.000042  min_lr: 0.000000  loss: 4.2031 (4.1804)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (22713.1189)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1453  max mem: 15572
Epoch: [23]  [1060/1404]  eta: 0:03:22  lr: 0.000042  min_lr: 0.000000  loss: 4.2740 (4.1819)  class_acc: 0.3333 (0.3201)  loss_scale: 16384.0000 (22653.4665)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1433  max mem: 15572
Epoch: [23]  [1070/1404]  eta: 0:03:16  lr: 0.000042  min_lr: 0.000000  loss: 4.2325 (4.1819)  class_acc: 0.3333 (0.3203)  loss_scale: 16384.0000 (22594.9281)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0624  max mem: 15572
Epoch: [23]  [1080/1404]  eta: 0:03:10  lr: 0.000042  min_lr: 0.000000  loss: 4.1913 (4.1806)  class_acc: 0.3333 (0.3207)  loss_scale: 16384.0000 (22537.4727)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.0772  max mem: 15572
Epoch: [23]  [1090/1404]  eta: 0:03:04  lr: 0.000042  min_lr: 0.000000  loss: 4.3009 (4.1823)  class_acc: 0.3750 (0.3208)  loss_scale: 16384.0000 (22481.0706)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0536  max mem: 15572
Epoch: [23]  [1100/1404]  eta: 0:02:58  lr: 0.000042  min_lr: 0.000000  loss: 4.3099 (4.1832)  class_acc: 0.2917 (0.3209)  loss_scale: 16384.0000 (22425.6930)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0078  max mem: 15572
Epoch: [23]  [1110/1404]  eta: 0:02:52  lr: 0.000042  min_lr: 0.000000  loss: 4.1416 (4.1821)  class_acc: 0.2917 (0.3211)  loss_scale: 16384.0000 (22371.3123)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0147  max mem: 15572
Epoch: [23]  [1120/1404]  eta: 0:02:46  lr: 0.000041  min_lr: 0.000000  loss: 4.0400 (4.1808)  class_acc: 0.3333 (0.3212)  loss_scale: 16384.0000 (22317.9019)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0772  max mem: 15572
Epoch: [23]  [1130/1404]  eta: 0:02:41  lr: 0.000041  min_lr: 0.000000  loss: 4.1187 (4.1809)  class_acc: 0.3333 (0.3218)  loss_scale: 16384.0000 (22265.4359)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0882  max mem: 15572
Epoch: [23]  [1140/1404]  eta: 0:02:35  lr: 0.000041  min_lr: 0.000000  loss: 4.1812 (4.1799)  class_acc: 0.3750 (0.3218)  loss_scale: 16384.0000 (22213.8896)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0513  max mem: 15572
Epoch: [23]  [1150/1404]  eta: 0:02:29  lr: 0.000041  min_lr: 0.000000  loss: 4.1055 (4.1797)  class_acc: 0.3333 (0.3220)  loss_scale: 16384.0000 (22163.2389)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0261  max mem: 15572
Epoch: [23]  [1160/1404]  eta: 0:02:23  lr: 0.000041  min_lr: 0.000000  loss: 4.1228 (4.1804)  class_acc: 0.3333 (0.3222)  loss_scale: 16384.0000 (22113.4608)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.0007  max mem: 15572
Epoch: [23]  [1170/1404]  eta: 0:02:17  lr: 0.000041  min_lr: 0.000000  loss: 4.2425 (4.1803)  class_acc: 0.3333 (0.3224)  loss_scale: 16384.0000 (22064.5329)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0007  max mem: 15572
[2025-01-17 02:01:50,739] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:01:50,740] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:01:50,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:01:50,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [1180/1404]  eta: 0:02:11  lr: 0.000041  min_lr: 0.000000  loss: 4.2561 (4.1817)  class_acc: 0.2917 (0.3219)  loss_scale: 16384.0000 (22058.0525)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0007  max mem: 15572
Epoch: [23]  [1190/1404]  eta: 0:02:05  lr: 0.000041  min_lr: 0.000000  loss: 4.2591 (4.1821)  class_acc: 0.2500 (0.3215)  loss_scale: 32768.0000 (22147.9765)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0007  max mem: 15572
[2025-01-17 02:01:59,056] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33485
[2025-01-17 02:01:59,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:01:59,142] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33485
[2025-01-17 02:01:59,143] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:01:59,143] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [1200/1404]  eta: 0:02:00  lr: 0.000041  min_lr: 0.000000  loss: 4.1785 (4.1810)  class_acc: 0.2500 (0.3215)  loss_scale: 32768.0000 (22127.2673)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0006  max mem: 15572
Epoch: [23]  [1210/1404]  eta: 0:01:54  lr: 0.000041  min_lr: 0.000000  loss: 4.1836 (4.1823)  class_acc: 0.3333 (0.3214)  loss_scale: 16384.0000 (22079.8415)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.0005  max mem: 15572
Epoch: [23]  [1220/1404]  eta: 0:01:48  lr: 0.000041  min_lr: 0.000000  loss: 4.3248 (4.1831)  class_acc: 0.2917 (0.3207)  loss_scale: 16384.0000 (22033.1925)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0007  max mem: 15572
Epoch: [23]  [1230/1404]  eta: 0:01:42  lr: 0.000041  min_lr: 0.000000  loss: 4.1896 (4.1828)  class_acc: 0.2083 (0.3207)  loss_scale: 16384.0000 (21987.3014)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0006  max mem: 15572
Epoch: [23]  [1240/1404]  eta: 0:01:36  lr: 0.000041  min_lr: 0.000000  loss: 4.1585 (4.1833)  class_acc: 0.2917 (0.3207)  loss_scale: 16384.0000 (21942.1499)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0005  max mem: 15572
Epoch: [23]  [1250/1404]  eta: 0:01:30  lr: 0.000041  min_lr: 0.000000  loss: 4.1585 (4.1827)  class_acc: 0.3333 (0.3208)  loss_scale: 16384.0000 (21897.7202)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0006  max mem: 15572
Epoch: [23]  [1260/1404]  eta: 0:01:24  lr: 0.000041  min_lr: 0.000000  loss: 4.2169 (4.1831)  class_acc: 0.3333 (0.3209)  loss_scale: 16384.0000 (21853.9952)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.0005  max mem: 15572
Epoch: [23]  [1270/1404]  eta: 0:01:19  lr: 0.000041  min_lr: 0.000000  loss: 4.2169 (4.1829)  class_acc: 0.2500 (0.3206)  loss_scale: 16384.0000 (21810.9583)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.0005  max mem: 15572
Epoch: [23]  [1280/1404]  eta: 0:01:13  lr: 0.000041  min_lr: 0.000000  loss: 4.1175 (4.1820)  class_acc: 0.2917 (0.3210)  loss_scale: 16384.0000 (21768.5933)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.0006  max mem: 15572
Epoch: [23]  [1290/1404]  eta: 0:01:07  lr: 0.000041  min_lr: 0.000000  loss: 4.1255 (4.1819)  class_acc: 0.2917 (0.3206)  loss_scale: 16384.0000 (21726.8846)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.0006  max mem: 15572
Epoch: [23]  [1300/1404]  eta: 0:01:01  lr: 0.000041  min_lr: 0.000000  loss: 4.2585 (4.1828)  class_acc: 0.2917 (0.3207)  loss_scale: 16384.0000 (21685.8171)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0007  max mem: 15572
Epoch: [23]  [1310/1404]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000000  loss: 4.2700 (4.1832)  class_acc: 0.2917 (0.3204)  loss_scale: 16384.0000 (21645.3760)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0006  max mem: 15572
Epoch: [23]  [1320/1404]  eta: 0:00:49  lr: 0.000041  min_lr: 0.000000  loss: 4.2306 (4.1831)  class_acc: 0.2500 (0.3201)  loss_scale: 16384.0000 (21605.5473)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0006  max mem: 15572
[2025-01-17 02:03:15,510] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:03:15,511] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:03:15,588] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:03:15,589] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [1330/1404]  eta: 0:00:43  lr: 0.000041  min_lr: 0.000000  loss: 4.1576 (4.1823)  class_acc: 0.3333 (0.3205)  loss_scale: 16384.0000 (21677.1029)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0008  max mem: 15572
Epoch: [23]  [1340/1404]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000000  loss: 4.1576 (4.1829)  class_acc: 0.3333 (0.3202)  loss_scale: 32768.0000 (21759.8091)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.0007  max mem: 15572
Epoch: [23]  [1350/1404]  eta: 0:00:31  lr: 0.000041  min_lr: 0.000000  loss: 4.2346 (4.1822)  class_acc: 0.3750 (0.3207)  loss_scale: 32768.0000 (21841.2909)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0006  max mem: 15572
Epoch: [23]  [1360/1404]  eta: 0:00:25  lr: 0.000041  min_lr: 0.000000  loss: 3.9645 (4.1805)  class_acc: 0.3750 (0.3211)  loss_scale: 32768.0000 (21921.5753)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.0008  max mem: 15572
Epoch: [23]  [1370/1404]  eta: 0:00:20  lr: 0.000041  min_lr: 0.000000  loss: 4.1013 (4.1808)  class_acc: 0.3750 (0.3211)  loss_scale: 32768.0000 (22000.6885)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.0008  max mem: 15572
[2025-01-17 02:03:47,303] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33669
[2025-01-17 02:03:47,304] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:03:47,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 02:03:47,373] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33669
[2025-01-17 02:03:47,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [23]  [1380/1404]  eta: 0:00:14  lr: 0.000041  min_lr: 0.000000  loss: 4.2353 (4.1810)  class_acc: 0.3750 (0.3216)  loss_scale: 32768.0000 (22031.2006)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0010  max mem: 15572
Epoch: [23]  [1390/1404]  eta: 0:00:08  lr: 0.000041  min_lr: 0.000000  loss: 4.1636 (4.1805)  class_acc: 0.3333 (0.3216)  loss_scale: 16384.0000 (21990.6024)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0011  max mem: 15572
Epoch: [23]  [1400/1404]  eta: 0:00:02  lr: 0.000041  min_lr: 0.000000  loss: 4.0058 (4.1799)  class_acc: 0.3750 (0.3220)  loss_scale: 16384.0000 (21950.5839)  weight_decay: 0.0500 (0.0500)  time: 0.4404  data: 0.0005  max mem: 15572
Epoch: [23]  [1403/1404]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000000  loss: 4.1894 (4.1800)  class_acc: 0.3333 (0.3218)  loss_scale: 16384.0000 (21938.6895)  weight_decay: 0.0500 (0.0500)  time: 0.4193  data: 0.0004  max mem: 15572
Epoch: [23] Total time: 0:13:43 (0.5862 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000000  loss: 4.1894 (4.1876)  class_acc: 0.3333 (0.3222)  loss_scale: 16384.0000 (21938.6895)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:14:37  loss: 1.6968 (1.6968)  acc1: 61.1111 (61.1111)  acc5: 83.3333 (83.3333)  time: 6.4529  data: 6.2136  max mem: 15572
Val:  [ 10/136]  eta: 0:01:48  loss: 2.4675 (2.3899)  acc1: 55.5556 (46.4646)  acc5: 72.2222 (76.2626)  time: 0.8640  data: 0.6624  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 2.4986 (2.4758)  acc1: 44.4444 (44.7090)  acc5: 72.2222 (74.8677)  time: 0.2800  data: 0.0848  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 2.2754 (2.3618)  acc1: 44.4444 (47.1326)  acc5: 77.7778 (76.5233)  time: 0.3247  data: 0.1205  max mem: 15572
Val:  [ 40/136]  eta: 0:00:45  loss: 2.1697 (2.3400)  acc1: 50.0000 (48.9160)  acc5: 83.3333 (77.3713)  time: 0.3686  data: 0.1566  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.2113 (2.3413)  acc1: 50.0000 (48.9107)  acc5: 83.3333 (78.5403)  time: 0.3130  data: 0.1175  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.3583 (2.4163)  acc1: 44.4444 (46.2659)  acc5: 77.7778 (77.0492)  time: 0.3417  data: 0.1341  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.4204 (2.4032)  acc1: 44.4444 (46.5571)  acc5: 72.2222 (77.5430)  time: 0.3691  data: 0.1467  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.3032 (2.3888)  acc1: 44.4444 (46.6392)  acc5: 83.3333 (78.3951)  time: 0.3592  data: 0.1482  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.3162 (2.3950)  acc1: 44.4444 (46.0928)  acc5: 77.7778 (78.0830)  time: 0.3883  data: 0.1831  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.6139 (2.4519)  acc1: 33.3333 (43.8944)  acc5: 72.2222 (76.6227)  time: 0.3632  data: 0.1656  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.4828 (2.4413)  acc1: 33.3333 (44.5445)  acc5: 72.2222 (76.9269)  time: 0.3629  data: 0.1660  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1923 (2.4054)  acc1: 55.5556 (45.7300)  acc5: 83.3333 (77.8237)  time: 0.3602  data: 0.1743  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.0372 (2.3765)  acc1: 55.5556 (46.3953)  acc5: 88.8889 (78.4139)  time: 0.2360  data: 0.0782  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.0394 (2.3741)  acc1: 50.0000 (46.8468)  acc5: 88.8889 (78.5012)  time: 0.2243  data: 0.0781  max mem: 15572
Val: Total time: 0:00:50 (0.3690 s / it)
* Acc@1 45.373 Acc@5 77.416 loss 2.420
Accuracy of the network on the 4883 val videos: 45.4%
[2025-01-17 02:04:49,398] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 02:04:49,400] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 02:04:49,400] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 02:04:49,400] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 02:04:51,768] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 02:04:51,768] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.37%
Epoch: [24]  [   0/1404]  eta: 3:11:54  lr: 0.000041  min_lr: 0.000000  loss: 4.1818 (4.1818)  class_acc: 0.2083 (0.2083)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 8.2013  data: 6.2614  max mem: 15572
Epoch: [24]  [  10/1404]  eta: 0:27:49  lr: 0.000041  min_lr: 0.000000  loss: 4.1818 (4.2234)  class_acc: 0.3333 (0.3220)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1978  data: 0.5697  max mem: 15572
Epoch: [24]  [  20/1404]  eta: 0:20:34  lr: 0.000041  min_lr: 0.000000  loss: 4.1795 (4.1515)  class_acc: 0.3333 (0.3333)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0092  max mem: 15572
Epoch: [24]  [  30/1404]  eta: 0:18:00  lr: 0.000041  min_lr: 0.000000  loss: 3.9937 (4.0842)  class_acc: 0.3750 (0.3669)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0093  max mem: 15572
Epoch: [24]  [  40/1404]  eta: 0:16:57  lr: 0.000041  min_lr: 0.000000  loss: 4.0099 (4.1037)  class_acc: 0.4167 (0.3659)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0006  max mem: 15572
Epoch: [24]  [  50/1404]  eta: 0:15:43  lr: 0.000040  min_lr: 0.000000  loss: 4.2100 (4.1472)  class_acc: 0.2917 (0.3456)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0006  max mem: 15572
Epoch: [24]  [  60/1404]  eta: 0:15:05  lr: 0.000040  min_lr: 0.000000  loss: 4.1810 (4.1302)  class_acc: 0.2917 (0.3463)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0008  max mem: 15572
Epoch: [24]  [  70/1404]  eta: 0:14:44  lr: 0.000040  min_lr: 0.000000  loss: 4.0757 (4.1153)  class_acc: 0.3333 (0.3427)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0009  max mem: 15572
Epoch: [24]  [  80/1404]  eta: 0:14:15  lr: 0.000040  min_lr: 0.000000  loss: 4.0916 (4.1066)  class_acc: 0.2917 (0.3447)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0006  max mem: 15572
Epoch: [24]  [  90/1404]  eta: 0:14:17  lr: 0.000040  min_lr: 0.000000  loss: 4.1906 (4.1157)  class_acc: 0.2917 (0.3402)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.0007  max mem: 15572
Epoch: [24]  [ 100/1404]  eta: 0:14:04  lr: 0.000040  min_lr: 0.000000  loss: 4.2225 (4.1133)  class_acc: 0.3333 (0.3412)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6525  data: 0.0009  max mem: 15572
[2025-01-17 02:05:58,210] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:05:58,211] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:05:58,234] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:05:58,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [ 110/1404]  eta: 0:13:48  lr: 0.000040  min_lr: 0.000000  loss: 4.1445 (4.1248)  class_acc: 0.2917 (0.3348)  loss_scale: 16384.0000 (17712.4324)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0338  max mem: 15572
Epoch: [24]  [ 120/1404]  eta: 0:13:43  lr: 0.000040  min_lr: 0.000000  loss: 4.0705 (4.1203)  class_acc: 0.2917 (0.3357)  loss_scale: 32768.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1043  max mem: 15572
Epoch: [24]  [ 130/1404]  eta: 0:13:30  lr: 0.000040  min_lr: 0.000000  loss: 4.1742 (4.1290)  class_acc: 0.2917 (0.3327)  loss_scale: 32768.0000 (20010.9924)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0714  max mem: 15572
Epoch: [24]  [ 140/1404]  eta: 0:13:18  lr: 0.000040  min_lr: 0.000000  loss: 4.2594 (4.1377)  class_acc: 0.2917 (0.3313)  loss_scale: 32768.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0008  max mem: 15572
Epoch: [24]  [ 150/1404]  eta: 0:13:04  lr: 0.000040  min_lr: 0.000000  loss: 4.2189 (4.1364)  class_acc: 0.2917 (0.3320)  loss_scale: 32768.0000 (21700.6623)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.0007  max mem: 15572
Epoch: [24]  [ 160/1404]  eta: 0:12:58  lr: 0.000040  min_lr: 0.000000  loss: 4.2249 (4.1477)  class_acc: 0.2917 (0.3307)  loss_scale: 32768.0000 (22388.0745)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0153  max mem: 15572
Epoch: [24]  [ 170/1404]  eta: 0:12:44  lr: 0.000040  min_lr: 0.000000  loss: 4.4020 (4.1633)  class_acc: 0.2500 (0.3277)  loss_scale: 32768.0000 (22995.0877)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.0153  max mem: 15572
Epoch: [24]  [ 180/1404]  eta: 0:12:44  lr: 0.000040  min_lr: 0.000000  loss: 4.4185 (4.1693)  class_acc: 0.3333 (0.3273)  loss_scale: 32768.0000 (23535.0276)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0007  max mem: 15572
[2025-01-17 02:06:46,438] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33879
[2025-01-17 02:06:46,438] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:06:46,451] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33879
[2025-01-17 02:06:46,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:06:46,452] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [ 190/1404]  eta: 0:12:34  lr: 0.000040  min_lr: 0.000000  loss: 4.2665 (4.1698)  class_acc: 0.3333 (0.3301)  loss_scale: 32768.0000 (23332.1885)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.0007  max mem: 15572
Epoch: [24]  [ 200/1404]  eta: 0:12:20  lr: 0.000040  min_lr: 0.000000  loss: 4.1476 (4.1686)  class_acc: 0.3333 (0.3300)  loss_scale: 16384.0000 (22986.5075)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0006  max mem: 15572
Epoch: [24]  [ 210/1404]  eta: 0:12:08  lr: 0.000040  min_lr: 0.000000  loss: 4.1146 (4.1708)  class_acc: 0.3333 (0.3335)  loss_scale: 16384.0000 (22673.5924)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0006  max mem: 15572
Epoch: [24]  [ 220/1404]  eta: 0:12:02  lr: 0.000040  min_lr: 0.000000  loss: 4.1146 (4.1664)  class_acc: 0.4167 (0.3369)  loss_scale: 16384.0000 (22388.9955)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0006  max mem: 15572
Epoch: [24]  [ 230/1404]  eta: 0:11:52  lr: 0.000040  min_lr: 0.000000  loss: 4.1121 (4.1652)  class_acc: 0.3333 (0.3369)  loss_scale: 16384.0000 (22129.0390)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0006  max mem: 15572
Epoch: [24]  [ 240/1404]  eta: 0:11:47  lr: 0.000040  min_lr: 0.000000  loss: 4.1555 (4.1677)  class_acc: 0.2917 (0.3366)  loss_scale: 16384.0000 (21890.6556)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0683  max mem: 15572
Epoch: [24]  [ 250/1404]  eta: 0:11:38  lr: 0.000040  min_lr: 0.000000  loss: 4.1591 (4.1736)  class_acc: 0.2917 (0.3368)  loss_scale: 16384.0000 (21671.2669)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1003  max mem: 15572
Epoch: [24]  [ 260/1404]  eta: 0:11:33  lr: 0.000040  min_lr: 0.000000  loss: 4.1100 (4.1680)  class_acc: 0.2917 (0.3346)  loss_scale: 16384.0000 (21468.6897)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0773  max mem: 15572
Epoch: [24]  [ 270/1404]  eta: 0:11:27  lr: 0.000040  min_lr: 0.000000  loss: 4.0832 (4.1699)  class_acc: 0.2917 (0.3361)  loss_scale: 16384.0000 (21281.0627)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1175  max mem: 15572
Epoch: [24]  [ 280/1404]  eta: 0:11:18  lr: 0.000040  min_lr: 0.000000  loss: 4.1818 (4.1725)  class_acc: 0.2917 (0.3348)  loss_scale: 16384.0000 (21106.7900)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0820  max mem: 15572
Epoch: [24]  [ 290/1404]  eta: 0:11:12  lr: 0.000040  min_lr: 0.000000  loss: 4.1972 (4.1725)  class_acc: 0.2917 (0.3352)  loss_scale: 16384.0000 (20944.4948)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0538  max mem: 15572
Epoch: [24]  [ 300/1404]  eta: 0:11:07  lr: 0.000040  min_lr: 0.000000  loss: 4.1754 (4.1744)  class_acc: 0.2917 (0.3321)  loss_scale: 16384.0000 (20792.9834)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1056  max mem: 15572
[2025-01-17 02:07:55,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=34000, skipped=202, lr=[3.850205467988544e-07, 3.850205467988544e-07, 5.500293525697921e-07, 5.500293525697921e-07, 7.857562179568459e-07, 7.857562179568459e-07, 1.1225088827954943e-06, 1.1225088827954943e-06, 1.6035841182792775e-06, 1.6035841182792775e-06, 2.290834454684682e-06, 2.290834454684682e-06, 3.272620649549546e-06, 3.272620649549546e-06, 4.675172356499352e-06, 4.675172356499352e-06, 6.678817652141932e-06, 6.678817652141932e-06, 9.541168074488476e-06, 9.541168074488476e-06, 1.3630240106412107e-05, 1.3630240106412107e-05, 1.9471771580588725e-05, 1.9471771580588725e-05, 2.7816816543698182e-05, 2.7816816543698182e-05, 3.973830934814026e-05, 3.973830934814026e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 02:07:55,413] [INFO] [timer.py:260:stop] epoch=0/micro_step=34000/global_step=34000, RunningAvgSamplesPerSec=47.90982755512616, CurrSamplesPerSec=43.980375939948175, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [ 310/1404]  eta: 0:11:01  lr: 0.000040  min_lr: 0.000000  loss: 4.2317 (4.1770)  class_acc: 0.2083 (0.3309)  loss_scale: 16384.0000 (20651.2154)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1122  max mem: 15572
[2025-01-17 02:08:00,971] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:08:00,971] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:08:01,066] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:08:01,067] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [ 320/1404]  eta: 0:10:53  lr: 0.000040  min_lr: 0.000000  loss: 4.2317 (4.1727)  class_acc: 0.2917 (0.3326)  loss_scale: 16384.0000 (20977.6449)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0658  max mem: 15572
Epoch: [24]  [ 330/1404]  eta: 0:10:46  lr: 0.000040  min_lr: 0.000000  loss: 4.1538 (4.1760)  class_acc: 0.3333 (0.3332)  loss_scale: 32768.0000 (21333.8489)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0516  max mem: 15572
Epoch: [24]  [ 340/1404]  eta: 0:10:41  lr: 0.000040  min_lr: 0.000000  loss: 4.0930 (4.1710)  class_acc: 0.3333 (0.3343)  loss_scale: 32768.0000 (21669.1613)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0746  max mem: 15572
Epoch: [24]  [ 350/1404]  eta: 0:10:33  lr: 0.000040  min_lr: 0.000000  loss: 3.9744 (4.1688)  class_acc: 0.2917 (0.3336)  loss_scale: 32768.0000 (21985.3675)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0621  max mem: 15572
Epoch: [24]  [ 360/1404]  eta: 0:10:26  lr: 0.000040  min_lr: 0.000000  loss: 4.0392 (4.1653)  class_acc: 0.2917 (0.3338)  loss_scale: 32768.0000 (22284.0554)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0668  max mem: 15572
Epoch: [24]  [ 370/1404]  eta: 0:10:20  lr: 0.000040  min_lr: 0.000000  loss: 4.1467 (4.1681)  class_acc: 0.2917 (0.3328)  loss_scale: 32768.0000 (22566.6415)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0729  max mem: 15572
[2025-01-17 02:08:39,717] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34075
[2025-01-17 02:08:39,717] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34075
[2025-01-17 02:08:39,718] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:08:39,718] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:08:39,718] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [ 380/1404]  eta: 0:10:14  lr: 0.000040  min_lr: 0.000000  loss: 4.2159 (4.1677)  class_acc: 0.2917 (0.3319)  loss_scale: 32768.0000 (22748.3885)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0546  max mem: 15572
Epoch: [24]  [ 390/1404]  eta: 0:10:08  lr: 0.000039  min_lr: 0.000000  loss: 4.1933 (4.1653)  class_acc: 0.2917 (0.3318)  loss_scale: 16384.0000 (22585.6164)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.0351  max mem: 15572
Epoch: [24]  [ 400/1404]  eta: 0:10:01  lr: 0.000039  min_lr: 0.000000  loss: 4.1711 (4.1642)  class_acc: 0.3333 (0.3317)  loss_scale: 16384.0000 (22430.9626)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0240  max mem: 15572
Epoch: [24]  [ 410/1404]  eta: 0:09:56  lr: 0.000039  min_lr: 0.000000  loss: 4.2111 (4.1658)  class_acc: 0.2917 (0.3302)  loss_scale: 16384.0000 (22283.8345)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0575  max mem: 15572
Epoch: [24]  [ 420/1404]  eta: 0:09:50  lr: 0.000039  min_lr: 0.000000  loss: 4.2407 (4.1670)  class_acc: 0.2917 (0.3306)  loss_scale: 16384.0000 (22143.6960)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.0817  max mem: 15572
Epoch: [24]  [ 430/1404]  eta: 0:09:43  lr: 0.000039  min_lr: 0.000000  loss: 4.2151 (4.1715)  class_acc: 0.2917 (0.3303)  loss_scale: 16384.0000 (22010.0603)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0623  max mem: 15572
Epoch: [24]  [ 440/1404]  eta: 0:09:37  lr: 0.000039  min_lr: 0.000000  loss: 4.2389 (4.1747)  class_acc: 0.2917 (0.3287)  loss_scale: 16384.0000 (21882.4853)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0799  max mem: 15572
Epoch: [24]  [ 450/1404]  eta: 0:09:31  lr: 0.000039  min_lr: 0.000000  loss: 4.2193 (4.1771)  class_acc: 0.3333 (0.3287)  loss_scale: 16384.0000 (21760.5676)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1265  max mem: 15572
Epoch: [24]  [ 460/1404]  eta: 0:09:24  lr: 0.000039  min_lr: 0.000000  loss: 4.2251 (4.1763)  class_acc: 0.3333 (0.3285)  loss_scale: 16384.0000 (21643.9393)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1018  max mem: 15572
Epoch: [24]  [ 470/1404]  eta: 0:09:20  lr: 0.000039  min_lr: 0.000000  loss: 4.3121 (4.1788)  class_acc: 0.2917 (0.3267)  loss_scale: 16384.0000 (21532.2633)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.0731  max mem: 15572
Epoch: [24]  [ 480/1404]  eta: 0:09:11  lr: 0.000039  min_lr: 0.000000  loss: 4.1088 (4.1767)  class_acc: 0.2917 (0.3268)  loss_scale: 16384.0000 (21425.2308)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0432  max mem: 15572
Epoch: [24]  [ 490/1404]  eta: 0:09:04  lr: 0.000039  min_lr: 0.000000  loss: 4.0569 (4.1793)  class_acc: 0.3333 (0.3271)  loss_scale: 16384.0000 (21322.5580)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0255  max mem: 15572
Epoch: [24]  [ 500/1404]  eta: 0:09:00  lr: 0.000039  min_lr: 0.000000  loss: 4.2724 (4.1801)  class_acc: 0.3333 (0.3280)  loss_scale: 16384.0000 (21223.9840)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1058  max mem: 15572
[2025-01-17 02:09:56,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:09:56,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:09:56,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:09:56,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [ 510/1404]  eta: 0:08:53  lr: 0.000039  min_lr: 0.000000  loss: 4.1023 (4.1785)  class_acc: 0.3333 (0.3278)  loss_scale: 16384.0000 (21225.4560)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1216  max mem: 15572
Epoch: [24]  [ 520/1404]  eta: 0:08:48  lr: 0.000039  min_lr: 0.000000  loss: 4.0987 (4.1760)  class_acc: 0.3333 (0.3288)  loss_scale: 32768.0000 (21447.0019)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0967  max mem: 15572
Epoch: [24]  [ 530/1404]  eta: 0:08:41  lr: 0.000039  min_lr: 0.000000  loss: 4.0940 (4.1722)  class_acc: 0.3333 (0.3285)  loss_scale: 32768.0000 (21660.2034)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1128  max mem: 15572
Epoch: [24]  [ 540/1404]  eta: 0:08:36  lr: 0.000039  min_lr: 0.000000  loss: 4.0205 (4.1717)  class_acc: 0.3333 (0.3285)  loss_scale: 32768.0000 (21865.5231)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1434  max mem: 15572
Epoch: [24]  [ 550/1404]  eta: 0:08:29  lr: 0.000039  min_lr: 0.000000  loss: 4.1686 (4.1708)  class_acc: 0.2917 (0.3280)  loss_scale: 32768.0000 (22063.3902)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1135  max mem: 15572
Epoch: [24]  [ 560/1404]  eta: 0:08:22  lr: 0.000039  min_lr: 0.000000  loss: 4.1992 (4.1701)  class_acc: 0.2917 (0.3275)  loss_scale: 32768.0000 (22254.2032)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0273  max mem: 15572
[2025-01-17 02:10:29,173] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34261
[2025-01-17 02:10:29,173] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:10:29,173] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 02:10:29,176] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34261
[2025-01-17 02:10:29,176] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [24]  [ 570/1404]  eta: 0:08:16  lr: 0.000039  min_lr: 0.000000  loss: 4.1963 (4.1706)  class_acc: 0.3333 (0.3279)  loss_scale: 32768.0000 (22266.1716)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0303  max mem: 15572
Epoch: [24]  [ 580/1404]  eta: 0:08:10  lr: 0.000039  min_lr: 0.000000  loss: 4.1963 (4.1726)  class_acc: 0.2500 (0.3265)  loss_scale: 16384.0000 (22164.9294)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0998  max mem: 15572
Epoch: [24]  [ 590/1404]  eta: 0:08:05  lr: 0.000039  min_lr: 0.000000  loss: 4.1850 (4.1731)  class_acc: 0.2917 (0.3268)  loss_scale: 16384.0000 (22067.1134)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1170  max mem: 15572
Epoch: [24]  [ 600/1404]  eta: 0:07:57  lr: 0.000039  min_lr: 0.000000  loss: 4.1336 (4.1717)  class_acc: 0.3750 (0.3272)  loss_scale: 16384.0000 (21972.5524)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0523  max mem: 15572
Epoch: [24]  [ 610/1404]  eta: 0:07:52  lr: 0.000039  min_lr: 0.000000  loss: 4.1947 (4.1735)  class_acc: 0.3333 (0.3268)  loss_scale: 16384.0000 (21881.0867)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0889  max mem: 15572
Epoch: [24]  [ 620/1404]  eta: 0:07:47  lr: 0.000039  min_lr: 0.000000  loss: 4.1947 (4.1735)  class_acc: 0.2500 (0.3256)  loss_scale: 16384.0000 (21792.5668)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.1602  max mem: 15572
Epoch: [24]  [ 630/1404]  eta: 0:07:40  lr: 0.000039  min_lr: 0.000000  loss: 4.1282 (4.1744)  class_acc: 0.2917 (0.3246)  loss_scale: 16384.0000 (21706.8526)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1035  max mem: 15572
Epoch: [24]  [ 640/1404]  eta: 0:07:34  lr: 0.000039  min_lr: 0.000000  loss: 4.1567 (4.1739)  class_acc: 0.2917 (0.3251)  loss_scale: 16384.0000 (21623.8128)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0908  max mem: 15572
Epoch: [24]  [ 650/1404]  eta: 0:07:29  lr: 0.000039  min_lr: 0.000000  loss: 4.1500 (4.1726)  class_acc: 0.3750 (0.3263)  loss_scale: 16384.0000 (21543.3241)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1262  max mem: 15572
Epoch: [24]  [ 660/1404]  eta: 0:07:22  lr: 0.000039  min_lr: 0.000000  loss: 4.2936 (4.1727)  class_acc: 0.3333 (0.3265)  loss_scale: 16384.0000 (21465.2708)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0743  max mem: 15572
Epoch: [24]  [ 670/1404]  eta: 0:07:16  lr: 0.000039  min_lr: 0.000000  loss: 4.2701 (4.1727)  class_acc: 0.3333 (0.3271)  loss_scale: 16384.0000 (21389.5440)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0332  max mem: 15572
Epoch: [24]  [ 680/1404]  eta: 0:07:10  lr: 0.000039  min_lr: 0.000000  loss: 4.2449 (4.1736)  class_acc: 0.3333 (0.3261)  loss_scale: 16384.0000 (21316.0411)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0456  max mem: 15572
Epoch: [24]  [ 690/1404]  eta: 0:07:03  lr: 0.000039  min_lr: 0.000000  loss: 4.2644 (4.1749)  class_acc: 0.2917 (0.3263)  loss_scale: 16384.0000 (21244.6657)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0541  max mem: 15572
[2025-01-17 02:11:45,735] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:11:45,736] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:11:45,752] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:11:45,753] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [ 700/1404]  eta: 0:06:58  lr: 0.000039  min_lr: 0.000000  loss: 4.2644 (4.1756)  class_acc: 0.3333 (0.3267)  loss_scale: 16384.0000 (21338.9330)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1026  max mem: 15572
Epoch: [24]  [ 710/1404]  eta: 0:06:51  lr: 0.000039  min_lr: 0.000000  loss: 4.1771 (4.1757)  class_acc: 0.3333 (0.3275)  loss_scale: 32768.0000 (21499.6793)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0988  max mem: 15572
Epoch: [24]  [ 720/1404]  eta: 0:06:46  lr: 0.000039  min_lr: 0.000000  loss: 4.1389 (4.1749)  class_acc: 0.3750 (0.3278)  loss_scale: 32768.0000 (21655.9667)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0740  max mem: 15572
Epoch: [24]  [ 730/1404]  eta: 0:06:39  lr: 0.000038  min_lr: 0.000000  loss: 4.0757 (4.1740)  class_acc: 0.4167 (0.3292)  loss_scale: 32768.0000 (21807.9781)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0558  max mem: 15572
Epoch: [24]  [ 740/1404]  eta: 0:06:33  lr: 0.000038  min_lr: 0.000000  loss: 4.1310 (4.1727)  class_acc: 0.2917 (0.3290)  loss_scale: 32768.0000 (21955.8866)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0248  max mem: 15572
Epoch: [24]  [ 750/1404]  eta: 0:06:26  lr: 0.000038  min_lr: 0.000000  loss: 4.1509 (4.1733)  class_acc: 0.2500 (0.3276)  loss_scale: 32768.0000 (22099.8562)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0274  max mem: 15572
Epoch: [24]  [ 760/1404]  eta: 0:06:20  lr: 0.000038  min_lr: 0.000000  loss: 4.1234 (4.1722)  class_acc: 0.2500 (0.3271)  loss_scale: 32768.0000 (22240.0420)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0359  max mem: 15572
Epoch: [24]  [ 770/1404]  eta: 0:06:14  lr: 0.000038  min_lr: 0.000000  loss: 4.0793 (4.1701)  class_acc: 0.2917 (0.3272)  loss_scale: 32768.0000 (22376.5914)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0717  max mem: 15572
Epoch: [24]  [ 780/1404]  eta: 0:06:09  lr: 0.000038  min_lr: 0.000000  loss: 4.1122 (4.1706)  class_acc: 0.2500 (0.3260)  loss_scale: 32768.0000 (22509.6440)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1148  max mem: 15572
Epoch: [24]  [ 790/1404]  eta: 0:06:03  lr: 0.000038  min_lr: 0.000000  loss: 4.1863 (4.1723)  class_acc: 0.2500 (0.3254)  loss_scale: 32768.0000 (22639.3325)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.1517  max mem: 15572
Epoch: [24]  [ 800/1404]  eta: 0:05:58  lr: 0.000038  min_lr: 0.000000  loss: 4.1787 (4.1730)  class_acc: 0.2500 (0.3245)  loss_scale: 32768.0000 (22765.7828)  weight_decay: 0.0500 (0.0500)  time: 0.6592  data: 0.1492  max mem: 15572
Epoch: [24]  [ 810/1404]  eta: 0:05:51  lr: 0.000038  min_lr: 0.000000  loss: 4.1643 (4.1732)  class_acc: 0.2500 (0.3237)  loss_scale: 32768.0000 (22889.1147)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0717  max mem: 15572
Epoch: [24]  [ 820/1404]  eta: 0:05:45  lr: 0.000038  min_lr: 0.000000  loss: 4.2317 (4.1725)  class_acc: 0.2500 (0.3242)  loss_scale: 32768.0000 (23009.4421)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0487  max mem: 15572
[2025-01-17 02:12:58,880] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:12:58,881] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 02:12:58,914] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:12:58,914] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 02:12:59,371] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34519
[2025-01-17 02:12:59,371] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34519
[2025-01-17 02:12:59,371] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 02:12:59,371] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 02:12:59,372] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 02:13:04,956] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34526
[2025-01-17 02:13:04,957] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:13:04,957] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34526
[2025-01-17 02:13:04,959] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:13:04,959] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [ 830/1404]  eta: 0:05:40  lr: 0.000038  min_lr: 0.000000  loss: 4.1872 (4.1727)  class_acc: 0.4167 (0.3249)  loss_scale: 32768.0000 (23146.5897)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1591  max mem: 15572
Epoch: [24]  [ 840/1404]  eta: 0:05:33  lr: 0.000038  min_lr: 0.000000  loss: 4.1872 (4.1738)  class_acc: 0.3333 (0.3243)  loss_scale: 16384.0000 (23066.1784)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1163  max mem: 15572
Epoch: [24]  [ 850/1404]  eta: 0:05:27  lr: 0.000038  min_lr: 0.000000  loss: 4.1756 (4.1744)  class_acc: 0.3333 (0.3247)  loss_scale: 16384.0000 (22987.6569)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0256  max mem: 15572
Epoch: [24]  [ 860/1404]  eta: 0:05:21  lr: 0.000038  min_lr: 0.000000  loss: 4.1281 (4.1740)  class_acc: 0.3333 (0.3244)  loss_scale: 16384.0000 (22910.9593)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0387  max mem: 15572
Epoch: [24]  [ 870/1404]  eta: 0:05:15  lr: 0.000038  min_lr: 0.000000  loss: 4.0778 (4.1739)  class_acc: 0.3333 (0.3246)  loss_scale: 16384.0000 (22836.0230)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0247  max mem: 15572
Epoch: [24]  [ 880/1404]  eta: 0:05:09  lr: 0.000038  min_lr: 0.000000  loss: 4.1835 (4.1753)  class_acc: 0.2917 (0.3245)  loss_scale: 16384.0000 (22762.7877)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0938  max mem: 15572
Epoch: [24]  [ 890/1404]  eta: 0:05:03  lr: 0.000038  min_lr: 0.000000  loss: 4.3431 (4.1751)  class_acc: 0.2500 (0.3242)  loss_scale: 16384.0000 (22691.1964)  weight_decay: 0.0500 (0.0500)  time: 0.6468  data: 0.1406  max mem: 15572
Epoch: [24]  [ 900/1404]  eta: 0:04:58  lr: 0.000038  min_lr: 0.000000  loss: 4.1019 (4.1735)  class_acc: 0.3333 (0.3248)  loss_scale: 16384.0000 (22621.1942)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1234  max mem: 15572
Epoch: [24]  [ 910/1404]  eta: 0:04:52  lr: 0.000038  min_lr: 0.000000  loss: 4.1347 (4.1740)  class_acc: 0.2917 (0.3245)  loss_scale: 16384.0000 (22552.7289)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0997  max mem: 15572
Epoch: [24]  [ 920/1404]  eta: 0:04:46  lr: 0.000038  min_lr: 0.000000  loss: 4.2051 (4.1742)  class_acc: 0.3333 (0.3254)  loss_scale: 16384.0000 (22485.7503)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0727  max mem: 15572
Epoch: [24]  [ 930/1404]  eta: 0:04:40  lr: 0.000038  min_lr: 0.000000  loss: 4.2783 (4.1757)  class_acc: 0.3333 (0.3254)  loss_scale: 16384.0000 (22420.2105)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0671  max mem: 15572
Epoch: [24]  [ 940/1404]  eta: 0:04:34  lr: 0.000038  min_lr: 0.000000  loss: 4.1976 (4.1753)  class_acc: 0.2917 (0.3251)  loss_scale: 16384.0000 (22356.0638)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0408  max mem: 15572
Epoch: [24]  [ 950/1404]  eta: 0:04:28  lr: 0.000038  min_lr: 0.000000  loss: 3.9675 (4.1723)  class_acc: 0.3750 (0.3263)  loss_scale: 16384.0000 (22293.2660)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0476  max mem: 15572
[2025-01-17 02:14:19,394] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:14:19,395] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:14:19,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:14:19,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [ 960/1404]  eta: 0:04:22  lr: 0.000038  min_lr: 0.000000  loss: 4.0841 (4.1739)  class_acc: 0.3750 (0.3267)  loss_scale: 16384.0000 (22265.8730)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0592  max mem: 15572
Epoch: [24]  [ 970/1404]  eta: 0:04:16  lr: 0.000038  min_lr: 0.000000  loss: 4.2655 (4.1746)  class_acc: 0.3333 (0.3264)  loss_scale: 32768.0000 (22374.0309)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0520  max mem: 15572
Epoch: [24]  [ 980/1404]  eta: 0:04:10  lr: 0.000038  min_lr: 0.000000  loss: 4.2333 (4.1742)  class_acc: 0.2917 (0.3260)  loss_scale: 32768.0000 (22479.9837)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1024  max mem: 15572
Epoch: [24]  [ 990/1404]  eta: 0:04:04  lr: 0.000038  min_lr: 0.000000  loss: 4.1263 (4.1742)  class_acc: 0.2917 (0.3259)  loss_scale: 32768.0000 (22583.7982)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1470  max mem: 15572
Epoch: [24]  [1000/1404]  eta: 0:03:58  lr: 0.000038  min_lr: 0.000000  loss: 4.2838 (4.1752)  class_acc: 0.2917 (0.3263)  loss_scale: 32768.0000 (22685.5385)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0736  max mem: 15572
Epoch: [24]  [1010/1404]  eta: 0:03:52  lr: 0.000038  min_lr: 0.000000  loss: 4.2868 (4.1759)  class_acc: 0.2917 (0.3258)  loss_scale: 32768.0000 (22785.2661)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0315  max mem: 15572
Epoch: [24]  [1020/1404]  eta: 0:03:46  lr: 0.000038  min_lr: 0.000000  loss: 4.2868 (4.1771)  class_acc: 0.2500 (0.3251)  loss_scale: 32768.0000 (22883.0402)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0568  max mem: 15572
Epoch: [24]  [1030/1404]  eta: 0:03:40  lr: 0.000038  min_lr: 0.000000  loss: 4.3076 (4.1782)  class_acc: 0.2917 (0.3250)  loss_scale: 32768.0000 (22978.9176)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0649  max mem: 15572
Epoch: [24]  [1040/1404]  eta: 0:03:34  lr: 0.000038  min_lr: 0.000000  loss: 4.1701 (4.1776)  class_acc: 0.3750 (0.3258)  loss_scale: 32768.0000 (23072.9529)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0635  max mem: 15572
Epoch: [24]  [1050/1404]  eta: 0:03:28  lr: 0.000038  min_lr: 0.000000  loss: 4.1225 (4.1777)  class_acc: 0.3750 (0.3259)  loss_scale: 32768.0000 (23165.1989)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0404  max mem: 15572
Epoch: [24]  [1060/1404]  eta: 0:03:22  lr: 0.000038  min_lr: 0.000000  loss: 4.3353 (4.1795)  class_acc: 0.2500 (0.3254)  loss_scale: 32768.0000 (23255.7059)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0853  max mem: 15572
[2025-01-17 02:15:23,673] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34766
[2025-01-17 02:15:23,673] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:15:23,673] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [1070/1404]  eta: 0:03:16  lr: 0.000037  min_lr: 0.000000  loss: 4.4487 (4.1808)  class_acc: 0.2500 (0.3253)  loss_scale: 32768.0000 (23329.2250)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.1483  max mem: 15572
[2025-01-17 02:15:23,740] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34766
[2025-01-17 02:15:23,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [24]  [1080/1404]  eta: 0:03:11  lr: 0.000037  min_lr: 0.000000  loss: 4.3843 (4.1817)  class_acc: 0.2917 (0.3251)  loss_scale: 16384.0000 (23264.9769)  weight_decay: 0.0500 (0.0500)  time: 0.6551  data: 0.1666  max mem: 15572
Epoch: [24]  [1090/1404]  eta: 0:03:05  lr: 0.000037  min_lr: 0.000000  loss: 4.3012 (4.1817)  class_acc: 0.2917 (0.3251)  loss_scale: 16384.0000 (23201.9065)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.1355  max mem: 15572
Epoch: [24]  [1100/1404]  eta: 0:02:59  lr: 0.000037  min_lr: 0.000000  loss: 4.1895 (4.1819)  class_acc: 0.2917 (0.3250)  loss_scale: 16384.0000 (23139.9818)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0826  max mem: 15572
Epoch: [24]  [1110/1404]  eta: 0:02:53  lr: 0.000037  min_lr: 0.000000  loss: 4.1411 (4.1813)  class_acc: 0.2917 (0.3248)  loss_scale: 16384.0000 (23079.1719)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0352  max mem: 15572
Epoch: [24]  [1120/1404]  eta: 0:02:47  lr: 0.000037  min_lr: 0.000000  loss: 4.1980 (4.1826)  class_acc: 0.2917 (0.3248)  loss_scale: 16384.0000 (23019.4469)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0159  max mem: 15572
Epoch: [24]  [1130/1404]  eta: 0:02:41  lr: 0.000037  min_lr: 0.000000  loss: 4.2784 (4.1823)  class_acc: 0.2917 (0.3245)  loss_scale: 16384.0000 (22960.7781)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0823  max mem: 15572
Epoch: [24]  [1140/1404]  eta: 0:02:35  lr: 0.000037  min_lr: 0.000000  loss: 4.2667 (4.1830)  class_acc: 0.2083 (0.3241)  loss_scale: 16384.0000 (22903.1376)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1260  max mem: 15572
Epoch: [24]  [1150/1404]  eta: 0:02:29  lr: 0.000037  min_lr: 0.000000  loss: 4.2039 (4.1830)  class_acc: 0.3333 (0.3245)  loss_scale: 16384.0000 (22846.4987)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1396  max mem: 15572
Epoch: [24]  [1160/1404]  eta: 0:02:24  lr: 0.000037  min_lr: 0.000000  loss: 4.2194 (4.1841)  class_acc: 0.3333 (0.3244)  loss_scale: 16384.0000 (22790.8355)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1599  max mem: 15572
Epoch: [24]  [1170/1404]  eta: 0:02:17  lr: 0.000037  min_lr: 0.000000  loss: 4.1038 (4.1830)  class_acc: 0.3333 (0.3248)  loss_scale: 16384.0000 (22736.1230)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.0874  max mem: 15572
Epoch: [24]  [1180/1404]  eta: 0:02:12  lr: 0.000037  min_lr: 0.000000  loss: 4.0835 (4.1833)  class_acc: 0.3333 (0.3247)  loss_scale: 16384.0000 (22682.3370)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0477  max mem: 15572
Epoch: [24]  [1190/1404]  eta: 0:02:06  lr: 0.000037  min_lr: 0.000000  loss: 4.0587 (4.1809)  class_acc: 0.3333 (0.3254)  loss_scale: 16384.0000 (22629.4542)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0822  max mem: 15572
[2025-01-17 02:16:39,718] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:16:39,718] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:16:39,720] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:16:39,720] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [1200/1404]  eta: 0:02:00  lr: 0.000037  min_lr: 0.000000  loss: 4.0443 (4.1815)  class_acc: 0.3333 (0.3250)  loss_scale: 16384.0000 (22604.7361)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0945  max mem: 15572
[2025-01-17 02:16:46,556] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34906
[2025-01-17 02:16:46,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:16:46,557] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [1210/1404]  eta: 0:01:54  lr: 0.000037  min_lr: 0.000000  loss: 4.2533 (4.1837)  class_acc: 0.2500 (0.3249)  loss_scale: 32768.0000 (22675.1313)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0783  max mem: 15572
[2025-01-17 02:16:46,567] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34906
[2025-01-17 02:16:46,568] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [24]  [1220/1404]  eta: 0:01:48  lr: 0.000037  min_lr: 0.000000  loss: 4.2587 (4.1839)  class_acc: 0.2500 (0.3250)  loss_scale: 16384.0000 (22623.6069)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0264  max mem: 15572
Epoch: [24]  [1230/1404]  eta: 0:01:42  lr: 0.000037  min_lr: 0.000000  loss: 4.2504 (4.1839)  class_acc: 0.3333 (0.3246)  loss_scale: 16384.0000 (22572.9196)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0097  max mem: 15572
Epoch: [24]  [1240/1404]  eta: 0:01:36  lr: 0.000037  min_lr: 0.000000  loss: 4.1008 (4.1829)  class_acc: 0.2917 (0.3245)  loss_scale: 16384.0000 (22523.0492)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0667  max mem: 15572
Epoch: [24]  [1250/1404]  eta: 0:01:30  lr: 0.000037  min_lr: 0.000000  loss: 4.1467 (4.1834)  class_acc: 0.2917 (0.3242)  loss_scale: 16384.0000 (22473.9760)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1160  max mem: 15572
Epoch: [24]  [1260/1404]  eta: 0:01:24  lr: 0.000037  min_lr: 0.000000  loss: 4.1983 (4.1837)  class_acc: 0.2500 (0.3239)  loss_scale: 16384.0000 (22425.6812)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0803  max mem: 15572
Epoch: [24]  [1270/1404]  eta: 0:01:18  lr: 0.000037  min_lr: 0.000000  loss: 4.2303 (4.1842)  class_acc: 0.2500 (0.3236)  loss_scale: 16384.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0421  max mem: 15572
Epoch: [24]  [1280/1404]  eta: 0:01:13  lr: 0.000037  min_lr: 0.000000  loss: 4.2666 (4.1843)  class_acc: 0.2917 (0.3238)  loss_scale: 16384.0000 (22331.3536)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0604  max mem: 15572
Epoch: [24]  [1290/1404]  eta: 0:01:07  lr: 0.000037  min_lr: 0.000000  loss: 4.1802 (4.1833)  class_acc: 0.3333 (0.3242)  loss_scale: 16384.0000 (22285.2858)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0459  max mem: 15572
Epoch: [24]  [1300/1404]  eta: 0:01:01  lr: 0.000037  min_lr: 0.000000  loss: 4.2064 (4.1835)  class_acc: 0.3333 (0.3244)  loss_scale: 16384.0000 (22239.9262)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0768  max mem: 15572
[2025-01-17 02:17:39,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=35000, skipped=208, lr=[3.565190408844115e-07, 3.565190408844115e-07, 5.093129155491593e-07, 5.093129155491593e-07, 7.275898793559419e-07, 7.275898793559419e-07, 1.0394141133656313e-06, 1.0394141133656313e-06, 1.4848773048080449e-06, 1.4848773048080449e-06, 2.121253292582921e-06, 2.121253292582921e-06, 3.0303618465470305e-06, 3.0303618465470305e-06, 4.3290883522100446e-06, 4.3290883522100446e-06, 6.1844119317286345e-06, 6.1844119317286345e-06, 8.834874188183765e-06, 8.834874188183765e-06, 1.2621248840262521e-05, 1.2621248840262521e-05, 1.8030355486089318e-05, 1.8030355486089318e-05, 2.5757650694413313e-05, 2.5757650694413313e-05, 3.679664384916188e-05, 3.679664384916188e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 02:17:39,748] [INFO] [timer.py:260:stop] epoch=0/micro_step=35000/global_step=35000, RunningAvgSamplesPerSec=47.980304749468004, CurrSamplesPerSec=47.19205496714573, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [1310/1404]  eta: 0:00:55  lr: 0.000037  min_lr: 0.000000  loss: 4.2346 (4.1830)  class_acc: 0.3333 (0.3245)  loss_scale: 16384.0000 (22195.2586)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1204  max mem: 15572
Epoch: [24]  [1320/1404]  eta: 0:00:49  lr: 0.000037  min_lr: 0.000000  loss: 4.1088 (4.1828)  class_acc: 0.3333 (0.3243)  loss_scale: 16384.0000 (22151.2672)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1030  max mem: 15572
Epoch: [24]  [1330/1404]  eta: 0:00:43  lr: 0.000037  min_lr: 0.000000  loss: 4.1091 (4.1837)  class_acc: 0.3333 (0.3244)  loss_scale: 16384.0000 (22107.9369)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1349  max mem: 15572
[2025-01-17 02:18:03,433] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:18:03,433] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:18:03,498] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:18:03,498] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [1340/1404]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000000  loss: 4.2497 (4.1841)  class_acc: 0.3333 (0.3243)  loss_scale: 16384.0000 (22089.6883)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.1537  max mem: 15572
[2025-01-17 02:18:04,789] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35038
[2025-01-17 02:18:04,789] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35038
[2025-01-17 02:18:04,790] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:18:04,790] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:18:04,790] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [1350/1404]  eta: 0:00:31  lr: 0.000037  min_lr: 0.000000  loss: 4.2497 (4.1845)  class_acc: 0.2917 (0.3240)  loss_scale: 16384.0000 (22059.5825)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0725  max mem: 15572
Epoch: [24]  [1360/1404]  eta: 0:00:25  lr: 0.000037  min_lr: 0.000000  loss: 4.2128 (4.1838)  class_acc: 0.3333 (0.3242)  loss_scale: 16384.0000 (22017.8810)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0334  max mem: 15572
Epoch: [24]  [1370/1404]  eta: 0:00:20  lr: 0.000037  min_lr: 0.000000  loss: 4.1619 (4.1842)  class_acc: 0.2917 (0.3241)  loss_scale: 16384.0000 (21976.7877)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0703  max mem: 15572
Epoch: [24]  [1380/1404]  eta: 0:00:14  lr: 0.000037  min_lr: 0.000000  loss: 4.2245 (4.1845)  class_acc: 0.2917 (0.3239)  loss_scale: 16384.0000 (21936.2896)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0479  max mem: 15572
Epoch: [24]  [1390/1404]  eta: 0:00:08  lr: 0.000037  min_lr: 0.000000  loss: 4.2245 (4.1844)  class_acc: 0.2500 (0.3238)  loss_scale: 16384.0000 (21896.3738)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0111  max mem: 15572
Epoch: [24]  [1400/1404]  eta: 0:00:02  lr: 0.000037  min_lr: 0.000000  loss: 4.1773 (4.1841)  class_acc: 0.2917 (0.3237)  loss_scale: 16384.0000 (21857.0278)  weight_decay: 0.0500 (0.0500)  time: 0.4530  data: 0.0007  max mem: 15572
Epoch: [24]  [1403/1404]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000000  loss: 4.2355 (4.1845)  class_acc: 0.2500 (0.3236)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  time: 0.4297  data: 0.0006  max mem: 15572
Epoch: [24] Total time: 0:13:43 (0.5866 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000000  loss: 4.2355 (4.1697)  class_acc: 0.2500 (0.3296)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:10:33  loss: 1.8228 (1.8228)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 4.6613  data: 4.4475  max mem: 15572
Val:  [ 10/136]  eta: 0:01:36  loss: 2.4303 (2.3706)  acc1: 50.0000 (51.0101)  acc5: 77.7778 (78.2828)  time: 0.7640  data: 0.5626  max mem: 15572
Val:  [ 20/136]  eta: 0:01:02  loss: 2.5365 (2.4550)  acc1: 44.4444 (46.0317)  acc5: 77.7778 (77.2487)  time: 0.3321  data: 0.1405  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.3490 (2.3431)  acc1: 38.8889 (48.3871)  acc5: 77.7778 (77.9570)  time: 0.3257  data: 0.1461  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.0613 (2.3197)  acc1: 55.5556 (49.4580)  acc5: 83.3333 (78.9973)  time: 0.3557  data: 0.1718  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.1858 (2.3229)  acc1: 44.4444 (48.3660)  acc5: 83.3333 (79.3028)  time: 0.3764  data: 0.1809  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.4416 (2.3920)  acc1: 38.8889 (45.6284)  acc5: 77.7778 (77.7778)  time: 0.3717  data: 0.1759  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.3176 (2.3737)  acc1: 38.8889 (45.8529)  acc5: 77.7778 (78.4038)  time: 0.3636  data: 0.1646  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.2178 (2.3651)  acc1: 44.4444 (46.5021)  acc5: 83.3333 (78.6694)  time: 0.3621  data: 0.1634  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.3545 (2.3755)  acc1: 38.8889 (45.9096)  acc5: 77.7778 (78.4493)  time: 0.3692  data: 0.1767  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5640 (2.4358)  acc1: 33.3333 (44.2794)  acc5: 77.7778 (76.7877)  time: 0.3574  data: 0.1481  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.6509 (2.4326)  acc1: 38.8889 (44.4945)  acc5: 72.2222 (76.7768)  time: 0.3261  data: 0.1122  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1986 (2.3896)  acc1: 50.0000 (45.8219)  acc5: 83.3333 (77.7778)  time: 0.3384  data: 0.1434  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.9165 (2.3565)  acc1: 61.1111 (46.9466)  acc5: 83.3333 (78.4139)  time: 0.2678  data: 0.0966  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.0511 (2.3557)  acc1: 55.5556 (47.1335)  acc5: 83.3333 (78.5012)  time: 0.2152  data: 0.0614  max mem: 15572
Val: Total time: 0:00:49 (0.3659 s / it)
* Acc@1 46.192 Acc@5 77.355 loss 2.399
Accuracy of the network on the 4883 val videos: 46.2%
[2025-01-17 02:19:25,137] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 02:19:25,138] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 02:19:25,138] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 02:19:25,138] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 02:19:27,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 02:19:27,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.19%
Epoch: [25]  [   0/1404]  eta: 3:04:54  lr: 0.000037  min_lr: 0.000000  loss: 4.4599 (4.4599)  class_acc: 0.2083 (0.2083)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.9021  data: 7.4533  max mem: 15572
Epoch: [25]  [  10/1404]  eta: 0:28:54  lr: 0.000036  min_lr: 0.000000  loss: 4.2814 (4.1701)  class_acc: 0.3333 (0.3371)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2441  data: 0.7822  max mem: 15572
Epoch: [25]  [  20/1404]  eta: 0:20:17  lr: 0.000036  min_lr: 0.000000  loss: 4.1880 (4.2080)  class_acc: 0.2917 (0.3155)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0578  max mem: 15572
Epoch: [25]  [  30/1404]  eta: 0:17:28  lr: 0.000036  min_lr: 0.000000  loss: 4.1880 (4.1984)  class_acc: 0.2500 (0.2997)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0175  max mem: 15572
Epoch: [25]  [  40/1404]  eta: 0:17:03  lr: 0.000036  min_lr: 0.000000  loss: 4.1906 (4.2229)  class_acc: 0.2500 (0.2927)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0550  max mem: 15572
Epoch: [25]  [  50/1404]  eta: 0:16:09  lr: 0.000036  min_lr: 0.000000  loss: 4.2519 (4.2009)  class_acc: 0.2917 (0.3137)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.0383  max mem: 15572
Epoch: [25]  [  60/1404]  eta: 0:15:17  lr: 0.000036  min_lr: 0.000000  loss: 4.1212 (4.1903)  class_acc: 0.4167 (0.3238)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0134  max mem: 15572
[2025-01-17 02:20:14,376] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:20:14,376] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:20:14,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:20:14,408] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [  70/1404]  eta: 0:15:06  lr: 0.000036  min_lr: 0.000000  loss: 4.1288 (4.1964)  class_acc: 0.2917 (0.3198)  loss_scale: 16384.0000 (17307.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1019  max mem: 15572
Epoch: [25]  [  80/1404]  eta: 0:14:34  lr: 0.000036  min_lr: 0.000000  loss: 4.1727 (4.1967)  class_acc: 0.3333 (0.3246)  loss_scale: 32768.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0993  max mem: 15572
Epoch: [25]  [  90/1404]  eta: 0:14:17  lr: 0.000036  min_lr: 0.000000  loss: 4.1158 (4.1808)  class_acc: 0.3333 (0.3205)  loss_scale: 32768.0000 (20705.0549)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0607  max mem: 15572
Epoch: [25]  [ 100/1404]  eta: 0:14:02  lr: 0.000036  min_lr: 0.000000  loss: 4.1158 (4.1853)  class_acc: 0.3333 (0.3247)  loss_scale: 32768.0000 (21899.4059)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1118  max mem: 15572
Epoch: [25]  [ 110/1404]  eta: 0:13:44  lr: 0.000036  min_lr: 0.000000  loss: 4.3216 (4.2011)  class_acc: 0.3333 (0.3206)  loss_scale: 32768.0000 (22878.5586)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0987  max mem: 15572
Epoch: [25]  [ 120/1404]  eta: 0:13:35  lr: 0.000036  min_lr: 0.000000  loss: 4.2193 (4.2006)  class_acc: 0.2917 (0.3220)  loss_scale: 32768.0000 (23695.8678)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0817  max mem: 15572
Epoch: [25]  [ 130/1404]  eta: 0:13:29  lr: 0.000036  min_lr: 0.000000  loss: 4.2193 (4.2082)  class_acc: 0.3333 (0.3244)  loss_scale: 32768.0000 (24388.3969)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1205  max mem: 15572
Epoch: [25]  [ 140/1404]  eta: 0:13:18  lr: 0.000036  min_lr: 0.000000  loss: 4.2428 (4.2072)  class_acc: 0.3333 (0.3277)  loss_scale: 32768.0000 (24982.6950)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1186  max mem: 15572
Epoch: [25]  [ 150/1404]  eta: 0:13:14  lr: 0.000036  min_lr: 0.000000  loss: 4.2586 (4.2109)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (25498.2781)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1287  max mem: 15572
Epoch: [25]  [ 160/1404]  eta: 0:13:01  lr: 0.000036  min_lr: 0.000000  loss: 4.2586 (4.2056)  class_acc: 0.2917 (0.3253)  loss_scale: 32768.0000 (25949.8137)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0981  max mem: 15572
Epoch: [25]  [ 170/1404]  eta: 0:12:46  lr: 0.000036  min_lr: 0.000000  loss: 4.0975 (4.1974)  class_acc: 0.3750 (0.3265)  loss_scale: 32768.0000 (26348.5380)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0224  max mem: 15572
[2025-01-17 02:21:19,086] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35280
[2025-01-17 02:21:19,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:21:19,103] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35280
[2025-01-17 02:21:19,103] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:21:19,104] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [ 180/1404]  eta: 0:12:33  lr: 0.000036  min_lr: 0.000000  loss: 4.1081 (4.1869)  class_acc: 0.3750 (0.3285)  loss_scale: 32768.0000 (26612.6851)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0111  max mem: 15572
Epoch: [25]  [ 190/1404]  eta: 0:12:22  lr: 0.000036  min_lr: 0.000000  loss: 3.9820 (4.1828)  class_acc: 0.3333 (0.3283)  loss_scale: 16384.0000 (26077.1518)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0208  max mem: 15572
Epoch: [25]  [ 200/1404]  eta: 0:12:15  lr: 0.000036  min_lr: 0.000000  loss: 4.1385 (4.1832)  class_acc: 0.3333 (0.3306)  loss_scale: 16384.0000 (25594.9055)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0686  max mem: 15572
Epoch: [25]  [ 210/1404]  eta: 0:12:05  lr: 0.000036  min_lr: 0.000000  loss: 4.1771 (4.1821)  class_acc: 0.3750 (0.3329)  loss_scale: 16384.0000 (25158.3697)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0606  max mem: 15572
Epoch: [25]  [ 220/1404]  eta: 0:12:00  lr: 0.000036  min_lr: 0.000000  loss: 4.1245 (4.1794)  class_acc: 0.3750 (0.3352)  loss_scale: 16384.0000 (24761.3394)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0825  max mem: 15572
Epoch: [25]  [ 230/1404]  eta: 0:11:56  lr: 0.000036  min_lr: 0.000000  loss: 4.2566 (4.1836)  class_acc: 0.3333 (0.3339)  loss_scale: 16384.0000 (24398.6840)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.1090  max mem: 15572
Epoch: [25]  [ 240/1404]  eta: 0:11:50  lr: 0.000036  min_lr: 0.000000  loss: 4.2566 (4.1800)  class_acc: 0.3333 (0.3358)  loss_scale: 16384.0000 (24066.1245)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.0922  max mem: 15572
Epoch: [25]  [ 250/1404]  eta: 0:11:45  lr: 0.000036  min_lr: 0.000000  loss: 4.0056 (4.1793)  class_acc: 0.3333 (0.3357)  loss_scale: 16384.0000 (23760.0637)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.0819  max mem: 15572
Epoch: [25]  [ 260/1404]  eta: 0:11:34  lr: 0.000036  min_lr: 0.000000  loss: 3.9713 (4.1687)  class_acc: 0.3750 (0.3397)  loss_scale: 16384.0000 (23477.4559)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.0402  max mem: 15572
Epoch: [25]  [ 270/1404]  eta: 0:11:29  lr: 0.000036  min_lr: 0.000000  loss: 4.0407 (4.1711)  class_acc: 0.3750 (0.3396)  loss_scale: 16384.0000 (23215.7048)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0904  max mem: 15572
Epoch: [25]  [ 280/1404]  eta: 0:11:25  lr: 0.000036  min_lr: 0.000000  loss: 4.2368 (4.1765)  class_acc: 0.3333 (0.3390)  loss_scale: 16384.0000 (22972.5836)  weight_decay: 0.0500 (0.0500)  time: 0.6547  data: 0.1611  max mem: 15572
Epoch: [25]  [ 290/1404]  eta: 0:11:16  lr: 0.000036  min_lr: 0.000000  loss: 4.2368 (4.1750)  class_acc: 0.2917 (0.3391)  loss_scale: 16384.0000 (22746.1718)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0903  max mem: 15572
Epoch: [25]  [ 300/1404]  eta: 0:11:07  lr: 0.000036  min_lr: 0.000000  loss: 4.1608 (4.1697)  class_acc: 0.3333 (0.3403)  loss_scale: 16384.0000 (22534.8040)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0083  max mem: 15572
[2025-01-17 02:22:34,496] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:22:34,496] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:22:34,527] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:22:34,528] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [ 310/1404]  eta: 0:11:01  lr: 0.000036  min_lr: 0.000000  loss: 4.2111 (4.1715)  class_acc: 0.3333 (0.3387)  loss_scale: 16384.0000 (22442.3923)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0268  max mem: 15572
Epoch: [25]  [ 320/1404]  eta: 0:10:53  lr: 0.000036  min_lr: 0.000000  loss: 4.1750 (4.1674)  class_acc: 0.2917 (0.3376)  loss_scale: 32768.0000 (22764.0623)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0267  max mem: 15572
Epoch: [25]  [ 330/1404]  eta: 0:10:43  lr: 0.000036  min_lr: 0.000000  loss: 4.1339 (4.1691)  class_acc: 0.2917 (0.3359)  loss_scale: 32768.0000 (23066.2961)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0006  max mem: 15572
Epoch: [25]  [ 340/1404]  eta: 0:10:42  lr: 0.000036  min_lr: 0.000000  loss: 4.1286 (4.1686)  class_acc: 0.2917 (0.3365)  loss_scale: 32768.0000 (23350.8035)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1179  max mem: 15572
[2025-01-17 02:22:56,911] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35447
[2025-01-17 02:22:56,911] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:22:56,912] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 02:22:56,915] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35447
[2025-01-17 02:22:56,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [25]  [ 350/1404]  eta: 0:10:32  lr: 0.000035  min_lr: 0.000000  loss: 4.0678 (4.1641)  class_acc: 0.2917 (0.3346)  loss_scale: 32768.0000 (23432.3875)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1179  max mem: 15572
Epoch: [25]  [ 360/1404]  eta: 0:10:26  lr: 0.000035  min_lr: 0.000000  loss: 4.0678 (4.1643)  class_acc: 0.2917 (0.3336)  loss_scale: 16384.0000 (23237.1413)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0006  max mem: 15572
Epoch: [25]  [ 370/1404]  eta: 0:10:22  lr: 0.000035  min_lr: 0.000000  loss: 4.3447 (4.1702)  class_acc: 0.2500 (0.3308)  loss_scale: 16384.0000 (23052.4205)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.0006  max mem: 15572
Epoch: [25]  [ 380/1404]  eta: 0:10:16  lr: 0.000035  min_lr: 0.000000  loss: 4.3447 (4.1686)  class_acc: 0.2500 (0.3306)  loss_scale: 16384.0000 (22877.3963)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.0006  max mem: 15572
Epoch: [25]  [ 390/1404]  eta: 0:10:08  lr: 0.000035  min_lr: 0.000000  loss: 4.0988 (4.1695)  class_acc: 0.2917 (0.3300)  loss_scale: 16384.0000 (22711.3248)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0006  max mem: 15572
Epoch: [25]  [ 400/1404]  eta: 0:10:01  lr: 0.000035  min_lr: 0.000000  loss: 4.1481 (4.1703)  class_acc: 0.2917 (0.3298)  loss_scale: 16384.0000 (22553.5362)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0006  max mem: 15572
Epoch: [25]  [ 410/1404]  eta: 0:09:55  lr: 0.000035  min_lr: 0.000000  loss: 4.2470 (4.1719)  class_acc: 0.2917 (0.3306)  loss_scale: 16384.0000 (22403.4258)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0007  max mem: 15572
Epoch: [25]  [ 420/1404]  eta: 0:09:49  lr: 0.000035  min_lr: 0.000000  loss: 4.2307 (4.1697)  class_acc: 0.2917 (0.3301)  loss_scale: 16384.0000 (22260.4466)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0007  max mem: 15572
Epoch: [25]  [ 430/1404]  eta: 0:09:42  lr: 0.000035  min_lr: 0.000000  loss: 4.2307 (4.1694)  class_acc: 0.3333 (0.3303)  loss_scale: 16384.0000 (22124.1021)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0010  max mem: 15572
Epoch: [25]  [ 440/1404]  eta: 0:09:38  lr: 0.000035  min_lr: 0.000000  loss: 4.2462 (4.1688)  class_acc: 0.3333 (0.3300)  loss_scale: 16384.0000 (21993.9410)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.0009  max mem: 15572
Epoch: [25]  [ 450/1404]  eta: 0:09:31  lr: 0.000035  min_lr: 0.000000  loss: 4.2440 (4.1720)  class_acc: 0.3333 (0.3301)  loss_scale: 16384.0000 (21869.5521)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.0005  max mem: 15572
Epoch: [25]  [ 460/1404]  eta: 0:09:23  lr: 0.000035  min_lr: 0.000000  loss: 4.2644 (4.1727)  class_acc: 0.3750 (0.3313)  loss_scale: 16384.0000 (21750.5597)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0008  max mem: 15572
Epoch: [25]  [ 470/1404]  eta: 0:09:18  lr: 0.000035  min_lr: 0.000000  loss: 4.2465 (4.1728)  class_acc: 0.3333 (0.3310)  loss_scale: 16384.0000 (21636.6200)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0079  max mem: 15572
[2025-01-17 02:24:13,241] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:24:13,241] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:24:13,267] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:24:13,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [ 480/1404]  eta: 0:09:12  lr: 0.000035  min_lr: 0.000000  loss: 4.2465 (4.1740)  class_acc: 0.3333 (0.3303)  loss_scale: 16384.0000 (21697.7297)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0168  max mem: 15572
Epoch: [25]  [ 490/1404]  eta: 0:09:05  lr: 0.000035  min_lr: 0.000000  loss: 4.2604 (4.1758)  class_acc: 0.3333 (0.3304)  loss_scale: 32768.0000 (21923.1935)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0195  max mem: 15572
Epoch: [25]  [ 500/1404]  eta: 0:08:59  lr: 0.000035  min_lr: 0.000000  loss: 4.0926 (4.1762)  class_acc: 0.3333 (0.3299)  loss_scale: 32768.0000 (22139.6567)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0376  max mem: 15572
[2025-01-17 02:24:29,871] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35604
[2025-01-17 02:24:29,872] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:24:29,872] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 02:24:29,945] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35604
[2025-01-17 02:24:29,946] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [25]  [ 510/1404]  eta: 0:08:54  lr: 0.000035  min_lr: 0.000000  loss: 4.2332 (4.1773)  class_acc: 0.2917 (0.3288)  loss_scale: 32768.0000 (22123.2094)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0489  max mem: 15572
Epoch: [25]  [ 520/1404]  eta: 0:08:46  lr: 0.000035  min_lr: 0.000000  loss: 4.2261 (4.1761)  class_acc: 0.2917 (0.3283)  loss_scale: 16384.0000 (22013.0518)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0218  max mem: 15572
Epoch: [25]  [ 530/1404]  eta: 0:08:40  lr: 0.000035  min_lr: 0.000000  loss: 4.1568 (4.1769)  class_acc: 0.2917 (0.3279)  loss_scale: 16384.0000 (21907.0433)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0395  max mem: 15572
Epoch: [25]  [ 540/1404]  eta: 0:08:35  lr: 0.000035  min_lr: 0.000000  loss: 4.1700 (4.1772)  class_acc: 0.3333 (0.3283)  loss_scale: 16384.0000 (21804.9538)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0759  max mem: 15572
Epoch: [25]  [ 550/1404]  eta: 0:08:28  lr: 0.000035  min_lr: 0.000000  loss: 4.1260 (4.1743)  class_acc: 0.3333 (0.3292)  loss_scale: 16384.0000 (21706.5699)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0631  max mem: 15572
Epoch: [25]  [ 560/1404]  eta: 0:08:21  lr: 0.000035  min_lr: 0.000000  loss: 4.2035 (4.1774)  class_acc: 0.3333 (0.3288)  loss_scale: 16384.0000 (21611.6934)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0369  max mem: 15572
Epoch: [25]  [ 570/1404]  eta: 0:08:15  lr: 0.000035  min_lr: 0.000000  loss: 4.2310 (4.1743)  class_acc: 0.3333 (0.3298)  loss_scale: 16384.0000 (21520.1401)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0110  max mem: 15572
Epoch: [25]  [ 580/1404]  eta: 0:08:08  lr: 0.000035  min_lr: 0.000000  loss: 4.0748 (4.1724)  class_acc: 0.3333 (0.3301)  loss_scale: 16384.0000 (21431.7384)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0007  max mem: 15572
Epoch: [25]  [ 590/1404]  eta: 0:08:02  lr: 0.000035  min_lr: 0.000000  loss: 4.0740 (4.1721)  class_acc: 0.3750 (0.3304)  loss_scale: 16384.0000 (21346.3283)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0009  max mem: 15572
Epoch: [25]  [ 600/1404]  eta: 0:07:56  lr: 0.000035  min_lr: 0.000000  loss: 4.0604 (4.1713)  class_acc: 0.3750 (0.3308)  loss_scale: 16384.0000 (21263.7604)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0394  max mem: 15572
Epoch: [25]  [ 610/1404]  eta: 0:07:50  lr: 0.000035  min_lr: 0.000000  loss: 3.9930 (4.1692)  class_acc: 0.3333 (0.3310)  loss_scale: 16384.0000 (21183.8953)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0815  max mem: 15572
Epoch: [25]  [ 620/1404]  eta: 0:07:44  lr: 0.000035  min_lr: 0.000000  loss: 4.1225 (4.1685)  class_acc: 0.3750 (0.3324)  loss_scale: 16384.0000 (21106.6023)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0910  max mem: 15572
Epoch: [25]  [ 630/1404]  eta: 0:07:38  lr: 0.000035  min_lr: 0.000000  loss: 4.1942 (4.1706)  class_acc: 0.3750 (0.3325)  loss_scale: 16384.0000 (21031.7591)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0920  max mem: 15572
[2025-01-17 02:25:43,062] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:25:43,062] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:25:43,071] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:25:43,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:25:43,965] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35735
[2025-01-17 02:25:43,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:25:43,966] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35735
[2025-01-17 02:25:43,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:25:43,966] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [ 640/1404]  eta: 0:07:31  lr: 0.000035  min_lr: 0.000000  loss: 4.1817 (4.1683)  class_acc: 0.3750 (0.3340)  loss_scale: 16384.0000 (21010.3713)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0439  max mem: 15572
Epoch: [25]  [ 650/1404]  eta: 0:07:25  lr: 0.000035  min_lr: 0.000000  loss: 4.1657 (4.1706)  class_acc: 0.3333 (0.3327)  loss_scale: 16384.0000 (20939.3057)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0263  max mem: 15572
Epoch: [25]  [ 660/1404]  eta: 0:07:19  lr: 0.000035  min_lr: 0.000000  loss: 4.2469 (4.1703)  class_acc: 0.2083 (0.3326)  loss_scale: 16384.0000 (20870.3903)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0264  max mem: 15572
Epoch: [25]  [ 670/1404]  eta: 0:07:14  lr: 0.000035  min_lr: 0.000000  loss: 4.0848 (4.1686)  class_acc: 0.3333 (0.3337)  loss_scale: 16384.0000 (20803.5291)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0566  max mem: 15572
Epoch: [25]  [ 680/1404]  eta: 0:07:07  lr: 0.000035  min_lr: 0.000000  loss: 4.1034 (4.1678)  class_acc: 0.3750 (0.3343)  loss_scale: 16384.0000 (20738.6314)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0565  max mem: 15572
Epoch: [25]  [ 690/1404]  eta: 0:07:02  lr: 0.000034  min_lr: 0.000000  loss: 4.2132 (4.1702)  class_acc: 0.3333 (0.3340)  loss_scale: 16384.0000 (20675.6122)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0259  max mem: 15572
Epoch: [25]  [ 700/1404]  eta: 0:06:57  lr: 0.000034  min_lr: 0.000000  loss: 4.2132 (4.1701)  class_acc: 0.2917 (0.3329)  loss_scale: 16384.0000 (20614.3909)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.0404  max mem: 15572
Epoch: [25]  [ 710/1404]  eta: 0:06:50  lr: 0.000034  min_lr: 0.000000  loss: 4.1542 (4.1694)  class_acc: 0.2917 (0.3338)  loss_scale: 16384.0000 (20554.8917)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.0150  max mem: 15572
Epoch: [25]  [ 720/1404]  eta: 0:06:44  lr: 0.000034  min_lr: 0.000000  loss: 4.1221 (4.1701)  class_acc: 0.3750 (0.3340)  loss_scale: 16384.0000 (20497.0430)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0007  max mem: 15572
Epoch: [25]  [ 730/1404]  eta: 0:06:38  lr: 0.000034  min_lr: 0.000000  loss: 4.2956 (4.1721)  class_acc: 0.2917 (0.3331)  loss_scale: 16384.0000 (20440.7770)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0195  max mem: 15572
Epoch: [25]  [ 740/1404]  eta: 0:06:32  lr: 0.000034  min_lr: 0.000000  loss: 4.3226 (4.1749)  class_acc: 0.2917 (0.3327)  loss_scale: 16384.0000 (20386.0297)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0195  max mem: 15572
Epoch: [25]  [ 750/1404]  eta: 0:06:25  lr: 0.000034  min_lr: 0.000000  loss: 4.2364 (4.1740)  class_acc: 0.3333 (0.3328)  loss_scale: 16384.0000 (20332.7403)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0163  max mem: 15572
Epoch: [25]  [ 760/1404]  eta: 0:06:20  lr: 0.000034  min_lr: 0.000000  loss: 4.0656 (4.1734)  class_acc: 0.3750 (0.3340)  loss_scale: 16384.0000 (20280.8515)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0520  max mem: 15572
[2025-01-17 02:26:58,963] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:26:58,963] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:26:58,966] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:26:58,966] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [ 770/1404]  eta: 0:06:13  lr: 0.000034  min_lr: 0.000000  loss: 4.1889 (4.1735)  class_acc: 0.3750 (0.3345)  loss_scale: 16384.0000 (20379.0610)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0437  max mem: 15572
Epoch: [25]  [ 780/1404]  eta: 0:06:07  lr: 0.000034  min_lr: 0.000000  loss: 4.2118 (4.1737)  class_acc: 0.3750 (0.3355)  loss_scale: 32768.0000 (20537.6901)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0439  max mem: 15572
Epoch: [25]  [ 790/1404]  eta: 0:06:02  lr: 0.000034  min_lr: 0.000000  loss: 4.1466 (4.1715)  class_acc: 0.3750 (0.3354)  loss_scale: 32768.0000 (20692.3085)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1161  max mem: 15572
Epoch: [25]  [ 800/1404]  eta: 0:05:56  lr: 0.000034  min_lr: 0.000000  loss: 4.0102 (4.1712)  class_acc: 0.3333 (0.3354)  loss_scale: 32768.0000 (20843.0662)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.1442  max mem: 15572
Epoch: [25]  [ 810/1404]  eta: 0:05:50  lr: 0.000034  min_lr: 0.000000  loss: 4.0128 (4.1705)  class_acc: 0.3333 (0.3350)  loss_scale: 32768.0000 (20990.1060)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0947  max mem: 15572
Epoch: [25]  [ 820/1404]  eta: 0:05:44  lr: 0.000034  min_lr: 0.000000  loss: 4.1903 (4.1706)  class_acc: 0.3333 (0.3351)  loss_scale: 32768.0000 (21133.5639)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1086  max mem: 15572
Epoch: [25]  [ 830/1404]  eta: 0:05:38  lr: 0.000034  min_lr: 0.000000  loss: 4.1903 (4.1693)  class_acc: 0.3333 (0.3353)  loss_scale: 32768.0000 (21273.5692)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1108  max mem: 15572
Epoch: [25]  [ 840/1404]  eta: 0:05:33  lr: 0.000034  min_lr: 0.000000  loss: 4.0923 (4.1703)  class_acc: 0.2917 (0.3349)  loss_scale: 32768.0000 (21410.2449)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0823  max mem: 15572
[2025-01-17 02:27:45,319] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35941
[2025-01-17 02:27:45,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:27:45,320] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 02:27:45,361] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35941
[2025-01-17 02:27:45,361] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [25]  [ 850/1404]  eta: 0:05:26  lr: 0.000034  min_lr: 0.000000  loss: 4.2545 (4.1711)  class_acc: 0.2917 (0.3350)  loss_scale: 16384.0000 (21351.1821)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0813  max mem: 15572
Epoch: [25]  [ 860/1404]  eta: 0:05:21  lr: 0.000034  min_lr: 0.000000  loss: 4.2545 (4.1715)  class_acc: 0.3333 (0.3348)  loss_scale: 16384.0000 (21293.4913)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0941  max mem: 15572
Epoch: [25]  [ 870/1404]  eta: 0:05:15  lr: 0.000034  min_lr: 0.000000  loss: 4.0766 (4.1688)  class_acc: 0.3750 (0.3352)  loss_scale: 16384.0000 (21237.1251)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1292  max mem: 15572
Epoch: [25]  [ 880/1404]  eta: 0:05:09  lr: 0.000034  min_lr: 0.000000  loss: 3.8490 (4.1659)  class_acc: 0.3750 (0.3361)  loss_scale: 16384.0000 (21182.0386)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1066  max mem: 15572
Epoch: [25]  [ 890/1404]  eta: 0:05:04  lr: 0.000034  min_lr: 0.000000  loss: 4.1155 (4.1663)  class_acc: 0.3750 (0.3360)  loss_scale: 16384.0000 (21128.1886)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.1368  max mem: 15572
[2025-01-17 02:28:20,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=36000, skipped=214, lr=[3.284183656734508e-07, 3.284183656734508e-07, 4.6916909381921544e-07, 4.6916909381921544e-07, 6.702415625988792e-07, 6.702415625988792e-07, 9.574879465698275e-07, 9.574879465698275e-07, 1.3678399236711823e-06, 1.3678399236711823e-06, 1.9540570338159747e-06, 1.9540570338159747e-06, 2.7915100483085355e-06, 2.7915100483085355e-06, 3.9878714975836224e-06, 3.9878714975836224e-06, 5.696959282262318e-06, 5.696959282262318e-06, 8.13851326037474e-06, 8.13851326037474e-06, 1.1626447514821058e-05, 1.1626447514821058e-05, 1.6609210735458655e-05, 1.6609210735458655e-05, 2.372744390779808e-05, 2.372744390779808e-05, 3.389634843971155e-05, 3.389634843971155e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 02:28:20,935] [INFO] [timer.py:260:stop] epoch=0/micro_step=36000/global_step=36000, RunningAvgSamplesPerSec=47.99673576985747, CurrSamplesPerSec=45.866268619177085, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [25]  [ 900/1404]  eta: 0:04:58  lr: 0.000034  min_lr: 0.000000  loss: 4.1800 (4.1646)  class_acc: 0.3333 (0.3367)  loss_scale: 16384.0000 (21075.5339)  weight_decay: 0.0500 (0.0500)  time: 0.6605  data: 0.1532  max mem: 15572
Epoch: [25]  [ 910/1404]  eta: 0:04:52  lr: 0.000034  min_lr: 0.000000  loss: 4.0724 (4.1647)  class_acc: 0.3333 (0.3365)  loss_scale: 16384.0000 (21024.0351)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0732  max mem: 15572
Epoch: [25]  [ 920/1404]  eta: 0:04:46  lr: 0.000034  min_lr: 0.000000  loss: 4.1842 (4.1637)  class_acc: 0.3333 (0.3372)  loss_scale: 16384.0000 (20973.6547)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0454  max mem: 15572
Epoch: [25]  [ 930/1404]  eta: 0:04:39  lr: 0.000034  min_lr: 0.000000  loss: 4.2255 (4.1636)  class_acc: 0.3333 (0.3368)  loss_scale: 16384.0000 (20924.3566)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0477  max mem: 15572
Epoch: [25]  [ 940/1404]  eta: 0:04:34  lr: 0.000034  min_lr: 0.000000  loss: 4.1299 (4.1641)  class_acc: 0.3750 (0.3371)  loss_scale: 16384.0000 (20876.1063)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0854  max mem: 15572
Epoch: [25]  [ 950/1404]  eta: 0:04:27  lr: 0.000034  min_lr: 0.000000  loss: 4.2203 (4.1648)  class_acc: 0.2917 (0.3361)  loss_scale: 16384.0000 (20828.8707)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0770  max mem: 15572
Epoch: [25]  [ 960/1404]  eta: 0:04:22  lr: 0.000034  min_lr: 0.000000  loss: 4.1877 (4.1644)  class_acc: 0.2917 (0.3361)  loss_scale: 16384.0000 (20782.6181)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0467  max mem: 15572
[2025-01-17 02:29:01,079] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:29:01,079] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:29:01,088] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:29:01,088] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [ 970/1404]  eta: 0:04:16  lr: 0.000034  min_lr: 0.000000  loss: 4.1468 (4.1656)  class_acc: 0.3333 (0.3363)  loss_scale: 16384.0000 (20754.1916)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0819  max mem: 15572
Epoch: [25]  [ 980/1404]  eta: 0:04:10  lr: 0.000034  min_lr: 0.000000  loss: 4.1584 (4.1638)  class_acc: 0.3333 (0.3368)  loss_scale: 32768.0000 (20876.6565)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1157  max mem: 15572
Epoch: [25]  [ 990/1404]  eta: 0:04:04  lr: 0.000034  min_lr: 0.000000  loss: 4.1593 (4.1648)  class_acc: 0.3333 (0.3369)  loss_scale: 32768.0000 (20996.6498)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1257  max mem: 15572
Epoch: [25]  [1000/1404]  eta: 0:03:58  lr: 0.000034  min_lr: 0.000000  loss: 4.1593 (4.1646)  class_acc: 0.3333 (0.3371)  loss_scale: 32768.0000 (21114.2458)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0951  max mem: 15572
[2025-01-17 02:29:22,253] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36104
[2025-01-17 02:29:22,254] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:29:22,255] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36104
[2025-01-17 02:29:22,255] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:29:22,255] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [1010/1404]  eta: 0:03:52  lr: 0.000034  min_lr: 0.000000  loss: 4.1094 (4.1647)  class_acc: 0.3333 (0.3372)  loss_scale: 32768.0000 (21116.0752)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0866  max mem: 15572
Epoch: [25]  [1020/1404]  eta: 0:03:46  lr: 0.000034  min_lr: 0.000000  loss: 4.1422 (4.1648)  class_acc: 0.3750 (0.3373)  loss_scale: 16384.0000 (21069.7277)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1092  max mem: 15572
Epoch: [25]  [1030/1404]  eta: 0:03:41  lr: 0.000034  min_lr: 0.000000  loss: 4.1201 (4.1646)  class_acc: 0.3750 (0.3373)  loss_scale: 16384.0000 (21024.2793)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.1728  max mem: 15572
Epoch: [25]  [1040/1404]  eta: 0:03:35  lr: 0.000033  min_lr: 0.000000  loss: 4.1475 (4.1640)  class_acc: 0.3333 (0.3373)  loss_scale: 16384.0000 (20979.7041)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1113  max mem: 15572
Epoch: [25]  [1050/1404]  eta: 0:03:28  lr: 0.000033  min_lr: 0.000000  loss: 4.1203 (4.1632)  class_acc: 0.3750 (0.3379)  loss_scale: 16384.0000 (20935.9772)  weight_decay: 0.0500 (0.0500)  time: 0.5120  data: 0.0081  max mem: 15572
Epoch: [25]  [1060/1404]  eta: 0:03:22  lr: 0.000033  min_lr: 0.000000  loss: 4.1831 (4.1638)  class_acc: 0.3750 (0.3376)  loss_scale: 16384.0000 (20893.0745)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0278  max mem: 15572
Epoch: [25]  [1070/1404]  eta: 0:03:16  lr: 0.000033  min_lr: 0.000000  loss: 4.2643 (4.1650)  class_acc: 0.3750 (0.3388)  loss_scale: 16384.0000 (20850.9729)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0288  max mem: 15572
Epoch: [25]  [1080/1404]  eta: 0:03:11  lr: 0.000033  min_lr: 0.000000  loss: 4.2845 (4.1665)  class_acc: 0.3750 (0.3390)  loss_scale: 16384.0000 (20809.6503)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0903  max mem: 15572
Epoch: [25]  [1090/1404]  eta: 0:03:05  lr: 0.000033  min_lr: 0.000000  loss: 4.1586 (4.1659)  class_acc: 0.3333 (0.3394)  loss_scale: 16384.0000 (20769.0852)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1524  max mem: 15572
Epoch: [25]  [1100/1404]  eta: 0:02:59  lr: 0.000033  min_lr: 0.000000  loss: 4.0970 (4.1654)  class_acc: 0.3333 (0.3397)  loss_scale: 16384.0000 (20729.2570)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1297  max mem: 15572
Epoch: [25]  [1110/1404]  eta: 0:02:53  lr: 0.000033  min_lr: 0.000000  loss: 4.1579 (4.1660)  class_acc: 0.3333 (0.3397)  loss_scale: 16384.0000 (20690.1458)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0780  max mem: 15572
Epoch: [25]  [1120/1404]  eta: 0:02:47  lr: 0.000033  min_lr: 0.000000  loss: 4.2463 (4.1672)  class_acc: 0.3333 (0.3399)  loss_scale: 16384.0000 (20651.7324)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0555  max mem: 15572
Epoch: [25]  [1130/1404]  eta: 0:02:41  lr: 0.000033  min_lr: 0.000000  loss: 4.3663 (4.1669)  class_acc: 0.3333 (0.3399)  loss_scale: 16384.0000 (20613.9982)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0501  max mem: 15572
[2025-01-17 02:30:36,496] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:30:36,497] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:30:36,511] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:30:36,511] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [1140/1404]  eta: 0:02:35  lr: 0.000033  min_lr: 0.000000  loss: 4.1291 (4.1672)  class_acc: 0.3333 (0.3399)  loss_scale: 16384.0000 (20691.8002)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0521  max mem: 15572
[2025-01-17 02:30:41,223] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36241
[2025-01-17 02:30:41,223] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36241
[2025-01-17 02:30:41,223] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:30:41,223] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:30:41,223] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [1150/1404]  eta: 0:02:29  lr: 0.000033  min_lr: 0.000000  loss: 4.2104 (4.1678)  class_acc: 0.3750 (0.3401)  loss_scale: 16384.0000 (20654.3736)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.1080  max mem: 15572
Epoch: [25]  [1160/1404]  eta: 0:02:23  lr: 0.000033  min_lr: 0.000000  loss: 4.1859 (4.1679)  class_acc: 0.3750 (0.3401)  loss_scale: 16384.0000 (20617.5917)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1066  max mem: 15572
Epoch: [25]  [1170/1404]  eta: 0:02:18  lr: 0.000033  min_lr: 0.000000  loss: 4.1485 (4.1676)  class_acc: 0.3750 (0.3404)  loss_scale: 16384.0000 (20581.4381)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1080  max mem: 15572
Epoch: [25]  [1180/1404]  eta: 0:02:12  lr: 0.000033  min_lr: 0.000000  loss: 4.1537 (4.1680)  class_acc: 0.3750 (0.3404)  loss_scale: 16384.0000 (20545.8967)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0714  max mem: 15572
Epoch: [25]  [1190/1404]  eta: 0:02:05  lr: 0.000033  min_lr: 0.000000  loss: 4.1972 (4.1690)  class_acc: 0.3333 (0.3403)  loss_scale: 16384.0000 (20510.9521)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0006  max mem: 15572
Epoch: [25]  [1200/1404]  eta: 0:02:00  lr: 0.000033  min_lr: 0.000000  loss: 4.1470 (4.1685)  class_acc: 0.2917 (0.3400)  loss_scale: 16384.0000 (20476.5895)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0008  max mem: 15572
Epoch: [25]  [1210/1404]  eta: 0:01:54  lr: 0.000033  min_lr: 0.000000  loss: 4.2594 (4.1691)  class_acc: 0.3333 (0.3400)  loss_scale: 16384.0000 (20442.7944)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0843  max mem: 15572
Epoch: [25]  [1220/1404]  eta: 0:01:48  lr: 0.000033  min_lr: 0.000000  loss: 4.2594 (4.1686)  class_acc: 0.3750 (0.3401)  loss_scale: 16384.0000 (20409.5528)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1139  max mem: 15572
Epoch: [25]  [1230/1404]  eta: 0:01:42  lr: 0.000033  min_lr: 0.000000  loss: 4.1501 (4.1683)  class_acc: 0.2917 (0.3403)  loss_scale: 16384.0000 (20376.8513)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.0816  max mem: 15572
Epoch: [25]  [1240/1404]  eta: 0:01:36  lr: 0.000033  min_lr: 0.000000  loss: 4.0523 (4.1670)  class_acc: 0.3333 (0.3407)  loss_scale: 16384.0000 (20344.6769)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1068  max mem: 15572
Epoch: [25]  [1250/1404]  eta: 0:01:30  lr: 0.000033  min_lr: 0.000000  loss: 4.0382 (4.1658)  class_acc: 0.3750 (0.3410)  loss_scale: 16384.0000 (20313.0168)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1141  max mem: 15572
Epoch: [25]  [1260/1404]  eta: 0:01:24  lr: 0.000033  min_lr: 0.000000  loss: 4.1066 (4.1646)  class_acc: 0.2917 (0.3407)  loss_scale: 16384.0000 (20281.8588)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.0692  max mem: 15572
[2025-01-17 02:31:57,550] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:31:57,550] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [1270/1404]  eta: 0:01:19  lr: 0.000033  min_lr: 0.000000  loss: 4.1614 (4.1659)  class_acc: 0.3333 (0.3408)  loss_scale: 16384.0000 (20264.0818)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1071  max mem: 15572
[2025-01-17 02:31:57,561] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:31:57,562] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [1280/1404]  eta: 0:01:13  lr: 0.000033  min_lr: 0.000000  loss: 4.2209 (4.1665)  class_acc: 0.3333 (0.3406)  loss_scale: 32768.0000 (20361.6924)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.1706  max mem: 15572
Epoch: [25]  [1290/1404]  eta: 0:01:07  lr: 0.000033  min_lr: 0.000000  loss: 4.2087 (4.1663)  class_acc: 0.3333 (0.3410)  loss_scale: 32768.0000 (20457.7909)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0741  max mem: 15572
Epoch: [25]  [1300/1404]  eta: 0:01:01  lr: 0.000033  min_lr: 0.000000  loss: 4.2087 (4.1673)  class_acc: 0.3333 (0.3407)  loss_scale: 32768.0000 (20552.4120)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0514  max mem: 15572
Epoch: [25]  [1310/1404]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000000  loss: 4.3692 (4.1675)  class_acc: 0.3333 (0.3406)  loss_scale: 32768.0000 (20645.5896)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1166  max mem: 15572
Epoch: [25]  [1320/1404]  eta: 0:00:49  lr: 0.000033  min_lr: 0.000000  loss: 4.3095 (4.1679)  class_acc: 0.2500 (0.3403)  loss_scale: 32768.0000 (20737.3565)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1276  max mem: 15572
Epoch: [25]  [1330/1404]  eta: 0:00:43  lr: 0.000033  min_lr: 0.000000  loss: 4.1788 (4.1672)  class_acc: 0.3333 (0.3405)  loss_scale: 32768.0000 (20827.7446)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0625  max mem: 15572
Epoch: [25]  [1340/1404]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000000  loss: 4.1793 (4.1680)  class_acc: 0.3333 (0.3400)  loss_scale: 32768.0000 (20916.7845)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0723  max mem: 15572
Epoch: [25]  [1350/1404]  eta: 0:00:31  lr: 0.000033  min_lr: 0.000000  loss: 4.0793 (4.1666)  class_acc: 0.2917 (0.3403)  loss_scale: 32768.0000 (21004.5063)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0791  max mem: 15572
Epoch: [25]  [1360/1404]  eta: 0:00:25  lr: 0.000033  min_lr: 0.000000  loss: 4.0782 (4.1672)  class_acc: 0.3333 (0.3403)  loss_scale: 32768.0000 (21090.9390)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0709  max mem: 15572
Epoch: [25]  [1370/1404]  eta: 0:00:20  lr: 0.000033  min_lr: 0.000000  loss: 4.1701 (4.1666)  class_acc: 0.3333 (0.3405)  loss_scale: 32768.0000 (21176.1109)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1048  max mem: 15572
[2025-01-17 02:33:00,052] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36477
[2025-01-17 02:33:00,052] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:33:00,052] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 02:33:00,053] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36477
[2025-01-17 02:33:00,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [25]  [1380/1404]  eta: 0:00:14  lr: 0.000033  min_lr: 0.000000  loss: 4.1057 (4.1652)  class_acc: 0.3750 (0.3411)  loss_scale: 32768.0000 (21212.5938)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0804  max mem: 15572
Epoch: [25]  [1390/1404]  eta: 0:00:08  lr: 0.000032  min_lr: 0.000000  loss: 4.2703 (4.1664)  class_acc: 0.3333 (0.3407)  loss_scale: 16384.0000 (21177.8807)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0810  max mem: 15572
Epoch: [25]  [1400/1404]  eta: 0:00:02  lr: 0.000032  min_lr: 0.000000  loss: 4.3725 (4.1670)  class_acc: 0.2500 (0.3406)  loss_scale: 16384.0000 (21143.6631)  weight_decay: 0.0500 (0.0500)  time: 0.4726  data: 0.0416  max mem: 15572
Epoch: [25]  [1403/1404]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000000  loss: 4.3725 (4.1672)  class_acc: 0.2917 (0.3404)  loss_scale: 16384.0000 (21133.4929)  weight_decay: 0.0500 (0.0500)  time: 0.4499  data: 0.0416  max mem: 15572
Epoch: [25] Total time: 0:13:44 (0.5874 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000000  loss: 4.3725 (4.1645)  class_acc: 0.2917 (0.3368)  loss_scale: 16384.0000 (21133.4929)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:10:28  loss: 1.7584 (1.7584)  acc1: 55.5556 (55.5556)  acc5: 83.3333 (83.3333)  time: 4.6188  data: 4.2626  max mem: 15572
Val:  [ 10/136]  eta: 0:01:37  loss: 2.4274 (2.3588)  acc1: 50.0000 (45.4545)  acc5: 72.2222 (77.7778)  time: 0.7715  data: 0.5761  max mem: 15572
Val:  [ 20/136]  eta: 0:01:03  loss: 2.5480 (2.4502)  acc1: 38.8889 (43.6508)  acc5: 72.2222 (75.9259)  time: 0.3467  data: 0.1566  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 2.3529 (2.3639)  acc1: 44.4444 (45.6989)  acc5: 77.7778 (76.8817)  time: 0.2991  data: 0.1075  max mem: 15572
Val:  [ 40/136]  eta: 0:00:40  loss: 2.0889 (2.3363)  acc1: 50.0000 (47.6965)  acc5: 83.3333 (77.1003)  time: 0.2971  data: 0.1177  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.1206 (2.3193)  acc1: 55.5556 (48.0392)  acc5: 83.3333 (78.2135)  time: 0.3450  data: 0.1604  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.2770 (2.3825)  acc1: 44.4444 (45.9927)  acc5: 77.7778 (77.6867)  time: 0.3818  data: 0.1778  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.2770 (2.3641)  acc1: 44.4444 (46.4789)  acc5: 77.7778 (78.4038)  time: 0.3519  data: 0.1444  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.2415 (2.3601)  acc1: 50.0000 (46.7078)  acc5: 83.3333 (79.0123)  time: 0.3475  data: 0.1487  max mem: 15572
Val:  [ 90/136]  eta: 0:00:17  loss: 2.2910 (2.3672)  acc1: 44.4444 (46.2759)  acc5: 77.7778 (78.8767)  time: 0.3438  data: 0.1492  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5100 (2.4204)  acc1: 38.8889 (44.8845)  acc5: 72.2222 (77.7778)  time: 0.3639  data: 0.1661  max mem: 15572
Val:  [110/136]  eta: 0:00:09  loss: 2.4157 (2.4148)  acc1: 44.4444 (45.0450)  acc5: 72.2222 (77.7277)  time: 0.3548  data: 0.1564  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1515 (2.3747)  acc1: 50.0000 (46.3269)  acc5: 77.7778 (78.4206)  time: 0.3229  data: 0.1190  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.9225 (2.3439)  acc1: 55.5556 (47.1586)  acc5: 88.8889 (78.9652)  time: 0.3226  data: 0.1360  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.0272 (2.3394)  acc1: 55.5556 (47.5430)  acc5: 83.3333 (79.0745)  time: 0.2355  data: 0.0718  max mem: 15572
Val: Total time: 0:00:49 (0.3653 s / it)
* Acc@1 46.253 Acc@5 78.174 loss 2.382
Accuracy of the network on the 4883 val videos: 46.3%
[2025-01-17 02:34:02,019] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 02:34:02,020] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 02:34:02,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 02:34:02,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 02:34:04,538] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 02:34:04,538] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.25%
Epoch: [26]  [   0/1404]  eta: 3:04:34  lr: 0.000032  min_lr: 0.000000  loss: 4.3255 (4.3255)  class_acc: 0.2500 (0.2500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.8878  data: 7.3788  max mem: 15572
Epoch: [26]  [  10/1404]  eta: 0:28:59  lr: 0.000032  min_lr: 0.000000  loss: 3.9919 (4.0812)  class_acc: 0.2917 (0.3485)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2475  data: 0.7674  max mem: 15572
Epoch: [26]  [  20/1404]  eta: 0:21:21  lr: 0.000032  min_lr: 0.000000  loss: 4.0098 (4.1637)  class_acc: 0.2917 (0.3313)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0958  max mem: 15572
Epoch: [26]  [  30/1404]  eta: 0:18:15  lr: 0.000032  min_lr: 0.000000  loss: 4.1871 (4.0901)  class_acc: 0.2917 (0.3212)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0590  max mem: 15572
Epoch: [26]  [  40/1404]  eta: 0:17:23  lr: 0.000032  min_lr: 0.000000  loss: 4.0889 (4.1280)  class_acc: 0.2917 (0.3130)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1058  max mem: 15572
Epoch: [26]  [  50/1404]  eta: 0:16:40  lr: 0.000032  min_lr: 0.000000  loss: 4.1506 (4.1075)  class_acc: 0.3333 (0.3219)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.1797  max mem: 15572
Epoch: [26]  [  60/1404]  eta: 0:15:59  lr: 0.000032  min_lr: 0.000000  loss: 4.1506 (4.1123)  class_acc: 0.3333 (0.3231)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1375  max mem: 15572
Epoch: [26]  [  70/1404]  eta: 0:15:30  lr: 0.000032  min_lr: 0.000000  loss: 4.2283 (4.1133)  class_acc: 0.3333 (0.3281)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1054  max mem: 15572
Epoch: [26]  [  80/1404]  eta: 0:14:50  lr: 0.000032  min_lr: 0.000000  loss: 4.0699 (4.1055)  class_acc: 0.3333 (0.3287)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0640  max mem: 15572
Epoch: [26]  [  90/1404]  eta: 0:14:30  lr: 0.000032  min_lr: 0.000000  loss: 4.1409 (4.1179)  class_acc: 0.2917 (0.3237)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0551  max mem: 15572
Epoch: [26]  [ 100/1404]  eta: 0:14:15  lr: 0.000032  min_lr: 0.000000  loss: 4.1409 (4.1093)  class_acc: 0.2917 (0.3263)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1077  max mem: 15572
[2025-01-17 02:35:12,005] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:35:12,005] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:35:12,012] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:35:12,012] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [ 110/1404]  eta: 0:14:00  lr: 0.000032  min_lr: 0.000000  loss: 4.1426 (4.1198)  class_acc: 0.2917 (0.3285)  loss_scale: 16384.0000 (17712.4324)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1177  max mem: 15572
Epoch: [26]  [ 120/1404]  eta: 0:13:46  lr: 0.000032  min_lr: 0.000000  loss: 4.2187 (4.1235)  class_acc: 0.3750 (0.3313)  loss_scale: 32768.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1143  max mem: 15572
Epoch: [26]  [ 130/1404]  eta: 0:13:26  lr: 0.000032  min_lr: 0.000000  loss: 4.1538 (4.1160)  class_acc: 0.3750 (0.3349)  loss_scale: 32768.0000 (20010.9924)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0733  max mem: 15572
Epoch: [26]  [ 140/1404]  eta: 0:13:23  lr: 0.000032  min_lr: 0.000000  loss: 4.1008 (4.1160)  class_acc: 0.3750 (0.3384)  loss_scale: 32768.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1088  max mem: 15572
Epoch: [26]  [ 150/1404]  eta: 0:13:08  lr: 0.000032  min_lr: 0.000000  loss: 4.1834 (4.1230)  class_acc: 0.2917 (0.3361)  loss_scale: 32768.0000 (21700.6623)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1279  max mem: 15572
Epoch: [26]  [ 160/1404]  eta: 0:13:00  lr: 0.000032  min_lr: 0.000000  loss: 4.1834 (4.1220)  class_acc: 0.3750 (0.3390)  loss_scale: 32768.0000 (22388.0745)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0995  max mem: 15572
Epoch: [26]  [ 170/1404]  eta: 0:12:50  lr: 0.000032  min_lr: 0.000000  loss: 4.1648 (4.1288)  class_acc: 0.3750 (0.3382)  loss_scale: 32768.0000 (22995.0877)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1211  max mem: 15572
Epoch: [26]  [ 180/1404]  eta: 0:12:42  lr: 0.000032  min_lr: 0.000000  loss: 4.0797 (4.1217)  class_acc: 0.3333 (0.3402)  loss_scale: 32768.0000 (23535.0276)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1210  max mem: 15572
Epoch: [26]  [ 190/1404]  eta: 0:12:31  lr: 0.000032  min_lr: 0.000000  loss: 4.0106 (4.1240)  class_acc: 0.4583 (0.3449)  loss_scale: 32768.0000 (24018.4293)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0896  max mem: 15572
Epoch: [26]  [ 200/1404]  eta: 0:12:23  lr: 0.000032  min_lr: 0.000000  loss: 4.0577 (4.1244)  class_acc: 0.4167 (0.3466)  loss_scale: 32768.0000 (24453.7313)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0760  max mem: 15572
Epoch: [26]  [ 210/1404]  eta: 0:12:13  lr: 0.000032  min_lr: 0.000000  loss: 4.0582 (4.1257)  class_acc: 0.3333 (0.3476)  loss_scale: 32768.0000 (24847.7725)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0808  max mem: 15572
Epoch: [26]  [ 220/1404]  eta: 0:12:02  lr: 0.000032  min_lr: 0.000000  loss: 4.1741 (4.1275)  class_acc: 0.3333 (0.3471)  loss_scale: 32768.0000 (25206.1538)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0502  max mem: 15572
[2025-01-17 02:36:25,555] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:36:25,555] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:36:25,556] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 02:36:25,556] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 230/1404]  eta: 0:11:56  lr: 0.000032  min_lr: 0.000000  loss: 4.1507 (4.1230)  class_acc: 0.3333 (0.3490)  loss_scale: 32768.0000 (25675.3593)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0638  max mem: 15572
[2025-01-17 02:36:27,014] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36737
[2025-01-17 02:36:27,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 02:36:27,048] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36737
[2025-01-17 02:36:27,049] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 02:36:27,049] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 240/1404]  eta: 0:11:48  lr: 0.000032  min_lr: 0.000000  loss: 4.1257 (4.1251)  class_acc: 0.3333 (0.3477)  loss_scale: 32768.0000 (26241.5934)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0850  max mem: 15572
Epoch: [26]  [ 250/1404]  eta: 0:11:39  lr: 0.000032  min_lr: 0.000000  loss: 4.2277 (4.1298)  class_acc: 0.3333 (0.3469)  loss_scale: 32768.0000 (26501.6096)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0686  max mem: 15572
Epoch: [26]  [ 260/1404]  eta: 0:11:32  lr: 0.000032  min_lr: 0.000000  loss: 4.2581 (4.1349)  class_acc: 0.3333 (0.3448)  loss_scale: 32768.0000 (26741.7011)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0664  max mem: 15572
Epoch: [26]  [ 270/1404]  eta: 0:11:23  lr: 0.000032  min_lr: 0.000000  loss: 4.2187 (4.1332)  class_acc: 0.3750 (0.3479)  loss_scale: 32768.0000 (26964.0738)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0375  max mem: 15572
[2025-01-17 02:36:51,213] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36780
[2025-01-17 02:36:51,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:36:51,257] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36780
[2025-01-17 02:36:51,257] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:36:51,258] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [26]  [ 280/1404]  eta: 0:11:16  lr: 0.000032  min_lr: 0.000000  loss: 4.0050 (4.1330)  class_acc: 0.3750 (0.3480)  loss_scale: 32768.0000 (26879.0890)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0324  max mem: 15572
Epoch: [26]  [ 290/1404]  eta: 0:11:11  lr: 0.000032  min_lr: 0.000000  loss: 4.0061 (4.1318)  class_acc: 0.3333 (0.3465)  loss_scale: 16384.0000 (26518.4330)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0620  max mem: 15572
Epoch: [26]  [ 300/1404]  eta: 0:11:09  lr: 0.000032  min_lr: 0.000000  loss: 4.0356 (4.1324)  class_acc: 0.2917 (0.3455)  loss_scale: 16384.0000 (26181.7409)  weight_decay: 0.0500 (0.0500)  time: 0.6656  data: 0.0884  max mem: 15572
Epoch: [26]  [ 310/1404]  eta: 0:10:59  lr: 0.000032  min_lr: 0.000000  loss: 3.9949 (4.1301)  class_acc: 0.2917 (0.3441)  loss_scale: 16384.0000 (25866.7010)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0588  max mem: 15572
Epoch: [26]  [ 320/1404]  eta: 0:10:51  lr: 0.000032  min_lr: 0.000000  loss: 4.0953 (4.1265)  class_acc: 0.2917 (0.3444)  loss_scale: 16384.0000 (25571.2897)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0008  max mem: 15572
Epoch: [26]  [ 330/1404]  eta: 0:10:47  lr: 0.000032  min_lr: 0.000000  loss: 4.0953 (4.1255)  class_acc: 0.3750 (0.3449)  loss_scale: 16384.0000 (25293.7281)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0008  max mem: 15572
Epoch: [26]  [ 340/1404]  eta: 0:10:40  lr: 0.000031  min_lr: 0.000000  loss: 4.1830 (4.1282)  class_acc: 0.3750 (0.3447)  loss_scale: 16384.0000 (25032.4457)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.0007  max mem: 15572
Epoch: [26]  [ 350/1404]  eta: 0:10:36  lr: 0.000031  min_lr: 0.000000  loss: 4.3691 (4.1323)  class_acc: 0.3333 (0.3440)  loss_scale: 16384.0000 (24786.0513)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0011  max mem: 15572
Epoch: [26]  [ 360/1404]  eta: 0:10:27  lr: 0.000031  min_lr: 0.000000  loss: 4.2725 (4.1360)  class_acc: 0.2917 (0.3434)  loss_scale: 16384.0000 (24553.3075)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0012  max mem: 15572
Epoch: [26]  [ 370/1404]  eta: 0:10:23  lr: 0.000031  min_lr: 0.000000  loss: 4.2310 (4.1384)  class_acc: 0.3333 (0.3430)  loss_scale: 16384.0000 (24333.1105)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0007  max mem: 15572
Epoch: [26]  [ 380/1404]  eta: 0:10:18  lr: 0.000031  min_lr: 0.000000  loss: 4.2512 (4.1429)  class_acc: 0.3333 (0.3430)  loss_scale: 16384.0000 (24124.4724)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.0005  max mem: 15572
Epoch: [26]  [ 390/1404]  eta: 0:10:08  lr: 0.000031  min_lr: 0.000000  loss: 4.1916 (4.1435)  class_acc: 0.3333 (0.3432)  loss_scale: 16384.0000 (23926.5064)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0005  max mem: 15572
Epoch: [26]  [ 400/1404]  eta: 0:10:01  lr: 0.000031  min_lr: 0.000000  loss: 4.1916 (4.1450)  class_acc: 0.2917 (0.3427)  loss_scale: 16384.0000 (23738.4140)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0006  max mem: 15572
[2025-01-17 02:38:08,812] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:38:08,812] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:38:08,813] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:38:08,813] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [ 410/1404]  eta: 0:09:56  lr: 0.000031  min_lr: 0.000000  loss: 4.1823 (4.1450)  class_acc: 0.3750 (0.3446)  loss_scale: 16384.0000 (23798.6569)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0008  max mem: 15572
Epoch: [26]  [ 420/1404]  eta: 0:09:50  lr: 0.000031  min_lr: 0.000000  loss: 4.1127 (4.1428)  class_acc: 0.3750 (0.3454)  loss_scale: 32768.0000 (24011.7055)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.0007  max mem: 15572
Epoch: [26]  [ 430/1404]  eta: 0:09:45  lr: 0.000031  min_lr: 0.000000  loss: 4.1859 (4.1426)  class_acc: 0.3333 (0.3455)  loss_scale: 32768.0000 (24214.8677)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.0006  max mem: 15572
Epoch: [26]  [ 440/1404]  eta: 0:09:37  lr: 0.000031  min_lr: 0.000000  loss: 4.1887 (4.1424)  class_acc: 0.3333 (0.3455)  loss_scale: 32768.0000 (24408.8163)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0006  max mem: 15572
Epoch: [26]  [ 450/1404]  eta: 0:09:30  lr: 0.000031  min_lr: 0.000000  loss: 4.1809 (4.1432)  class_acc: 0.3333 (0.3452)  loss_scale: 32768.0000 (24594.1641)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0007  max mem: 15572
Epoch: [26]  [ 460/1404]  eta: 0:09:25  lr: 0.000031  min_lr: 0.000000  loss: 4.2828 (4.1502)  class_acc: 0.3333 (0.3437)  loss_scale: 32768.0000 (24771.4707)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0008  max mem: 15572
Epoch: [26]  [ 470/1404]  eta: 0:09:19  lr: 0.000031  min_lr: 0.000000  loss: 4.2325 (4.1473)  class_acc: 0.3333 (0.3440)  loss_scale: 32768.0000 (24941.2484)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0008  max mem: 15572
Epoch: [26]  [ 480/1404]  eta: 0:09:11  lr: 0.000031  min_lr: 0.000000  loss: 4.1276 (4.1462)  class_acc: 0.3333 (0.3426)  loss_scale: 32768.0000 (25103.9667)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0008  max mem: 15572
Epoch: [26]  [ 490/1404]  eta: 0:09:07  lr: 0.000031  min_lr: 0.000000  loss: 4.2080 (4.1462)  class_acc: 0.2917 (0.3427)  loss_scale: 32768.0000 (25260.0570)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0007  max mem: 15572
[2025-01-17 02:39:02,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=37000, skipped=219, lr=[3.008333360706352e-07, 3.008333360706352e-07, 4.2976190867233605e-07, 4.2976190867233605e-07, 6.139455838176229e-07, 6.139455838176229e-07, 8.770651197394614e-07, 8.770651197394614e-07, 1.2529501710563735e-06, 1.2529501710563735e-06, 1.7899288157948193e-06, 1.7899288157948193e-06, 2.5570411654211706e-06, 2.5570411654211706e-06, 3.6529159506016727e-06, 3.6529159506016727e-06, 5.2184513580023895e-06, 5.2184513580023895e-06, 7.454930511431987e-06, 7.454930511431987e-06, 1.0649900730617123e-05, 1.0649900730617123e-05, 1.5214143900881605e-05, 1.5214143900881605e-05, 2.1734491286973724e-05, 2.1734491286973724e-05, 3.104927326710532e-05, 3.104927326710532e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 02:39:02,517] [INFO] [timer.py:260:stop] epoch=0/micro_step=37000/global_step=37000, RunningAvgSamplesPerSec=48.039728928041434, CurrSamplesPerSec=49.153008020672296, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [26]  [ 500/1404]  eta: 0:09:01  lr: 0.000031  min_lr: 0.000000  loss: 4.2080 (4.1465)  class_acc: 0.3333 (0.3425)  loss_scale: 32768.0000 (25409.9162)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.0005  max mem: 15572
Epoch: [26]  [ 510/1404]  eta: 0:08:54  lr: 0.000031  min_lr: 0.000000  loss: 4.1344 (4.1448)  class_acc: 0.3333 (0.3434)  loss_scale: 32768.0000 (25553.9100)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0005  max mem: 15572
Epoch: [26]  [ 520/1404]  eta: 0:08:46  lr: 0.000031  min_lr: 0.000000  loss: 4.1470 (4.1458)  class_acc: 0.3750 (0.3437)  loss_scale: 32768.0000 (25692.3762)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0009  max mem: 15572
Epoch: [26]  [ 530/1404]  eta: 0:08:40  lr: 0.000031  min_lr: 0.000000  loss: 4.1527 (4.1475)  class_acc: 0.3333 (0.3434)  loss_scale: 32768.0000 (25825.6271)  weight_decay: 0.0500 (0.0500)  time: 0.5231  data: 0.0009  max mem: 15572
[2025-01-17 02:39:22,166] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:39:22,167] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 02:39:22,213] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:39:22,213] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 02:39:23,975] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37039
[2025-01-17 02:39:23,975] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37039
[2025-01-17 02:39:23,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 02:39:23,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 02:39:23,975] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 540/1404]  eta: 0:08:34  lr: 0.000031  min_lr: 0.000000  loss: 4.0802 (4.1476)  class_acc: 0.3333 (0.3435)  loss_scale: 32768.0000 (26075.0906)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0006  max mem: 15572
Epoch: [26]  [ 550/1404]  eta: 0:08:27  lr: 0.000031  min_lr: 0.000000  loss: 4.2130 (4.1485)  class_acc: 0.3750 (0.3438)  loss_scale: 32768.0000 (26196.5590)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0009  max mem: 15572
[2025-01-17 02:39:35,229] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37058
[2025-01-17 02:39:35,230] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:39:35,258] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37058
[2025-01-17 02:39:35,258] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:39:35,258] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [26]  [ 560/1404]  eta: 0:08:22  lr: 0.000031  min_lr: 0.000000  loss: 4.2350 (4.1509)  class_acc: 0.2917 (0.3435)  loss_scale: 32768.0000 (26109.2620)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0009  max mem: 15572
Epoch: [26]  [ 570/1404]  eta: 0:08:15  lr: 0.000031  min_lr: 0.000000  loss: 4.1442 (4.1508)  class_acc: 0.3333 (0.3435)  loss_scale: 16384.0000 (25938.9422)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.0008  max mem: 15572
Epoch: [26]  [ 580/1404]  eta: 0:08:10  lr: 0.000031  min_lr: 0.000000  loss: 4.2255 (4.1517)  class_acc: 0.3333 (0.3434)  loss_scale: 16384.0000 (25774.4854)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0007  max mem: 15572
Epoch: [26]  [ 590/1404]  eta: 0:08:03  lr: 0.000031  min_lr: 0.000000  loss: 4.1330 (4.1499)  class_acc: 0.3333 (0.3448)  loss_scale: 16384.0000 (25615.5939)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0007  max mem: 15572
Epoch: [26]  [ 600/1404]  eta: 0:07:57  lr: 0.000031  min_lr: 0.000000  loss: 4.0798 (4.1487)  class_acc: 0.3333 (0.3450)  loss_scale: 16384.0000 (25461.9900)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0006  max mem: 15572
Epoch: [26]  [ 610/1404]  eta: 0:07:52  lr: 0.000031  min_lr: 0.000000  loss: 4.1591 (4.1482)  class_acc: 0.3333 (0.3453)  loss_scale: 16384.0000 (25313.4141)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.0007  max mem: 15572
Epoch: [26]  [ 620/1404]  eta: 0:07:46  lr: 0.000031  min_lr: 0.000000  loss: 4.2108 (4.1503)  class_acc: 0.3333 (0.3450)  loss_scale: 16384.0000 (25169.6232)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.0007  max mem: 15572
Epoch: [26]  [ 630/1404]  eta: 0:07:39  lr: 0.000031  min_lr: 0.000000  loss: 4.3245 (4.1520)  class_acc: 0.2500 (0.3438)  loss_scale: 16384.0000 (25030.3899)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0007  max mem: 15572
Epoch: [26]  [ 640/1404]  eta: 0:07:32  lr: 0.000031  min_lr: 0.000000  loss: 4.1670 (4.1520)  class_acc: 0.2500 (0.3430)  loss_scale: 16384.0000 (24895.5008)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0008  max mem: 15572
Epoch: [26]  [ 650/1404]  eta: 0:07:27  lr: 0.000031  min_lr: 0.000000  loss: 4.1350 (4.1519)  class_acc: 0.2500 (0.3422)  loss_scale: 16384.0000 (24764.7558)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0008  max mem: 15572
Epoch: [26]  [ 660/1404]  eta: 0:07:22  lr: 0.000031  min_lr: 0.000000  loss: 4.2213 (4.1531)  class_acc: 0.3333 (0.3425)  loss_scale: 16384.0000 (24637.9667)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.0008  max mem: 15572
Epoch: [26]  [ 670/1404]  eta: 0:07:16  lr: 0.000031  min_lr: 0.000000  loss: 4.2619 (4.1560)  class_acc: 0.3333 (0.3418)  loss_scale: 16384.0000 (24514.9568)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.0005  max mem: 15572
Epoch: [26]  [ 680/1404]  eta: 0:07:09  lr: 0.000031  min_lr: 0.000000  loss: 4.2503 (4.1552)  class_acc: 0.2917 (0.3411)  loss_scale: 16384.0000 (24395.5595)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0004  max mem: 15572
[2025-01-17 02:40:49,965] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:40:49,966] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:40:49,983] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:40:49,985] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [ 690/1404]  eta: 0:07:02  lr: 0.000031  min_lr: 0.000000  loss: 4.1097 (4.1549)  class_acc: 0.3333 (0.3413)  loss_scale: 16384.0000 (24469.3025)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0006  max mem: 15572
Epoch: [26]  [ 700/1404]  eta: 0:06:56  lr: 0.000030  min_lr: 0.000000  loss: 4.0365 (4.1538)  class_acc: 0.3333 (0.3415)  loss_scale: 32768.0000 (24587.6862)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0009  max mem: 15572
Epoch: [26]  [ 710/1404]  eta: 0:06:49  lr: 0.000030  min_lr: 0.000000  loss: 4.0131 (4.1519)  class_acc: 0.3333 (0.3419)  loss_scale: 32768.0000 (24702.7398)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0009  max mem: 15572
Epoch: [26]  [ 720/1404]  eta: 0:06:43  lr: 0.000030  min_lr: 0.000000  loss: 4.0231 (4.1524)  class_acc: 0.2500 (0.3405)  loss_scale: 32768.0000 (24814.6019)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0007  max mem: 15572
Epoch: [26]  [ 730/1404]  eta: 0:06:37  lr: 0.000030  min_lr: 0.000000  loss: 4.2262 (4.1506)  class_acc: 0.2500 (0.3407)  loss_scale: 32768.0000 (24923.4036)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0068  max mem: 15572
Epoch: [26]  [ 740/1404]  eta: 0:06:32  lr: 0.000030  min_lr: 0.000000  loss: 4.1800 (4.1504)  class_acc: 0.2917 (0.3400)  loss_scale: 32768.0000 (25029.2686)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0067  max mem: 15572
Epoch: [26]  [ 750/1404]  eta: 0:06:25  lr: 0.000030  min_lr: 0.000000  loss: 4.1894 (4.1515)  class_acc: 0.3333 (0.3403)  loss_scale: 32768.0000 (25132.3142)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0119  max mem: 15572
Epoch: [26]  [ 760/1404]  eta: 0:06:19  lr: 0.000030  min_lr: 0.000000  loss: 4.0693 (4.1502)  class_acc: 0.3333 (0.3395)  loss_scale: 32768.0000 (25232.6518)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0225  max mem: 15572
[2025-01-17 02:41:38,820] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37273
[2025-01-17 02:41:38,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:41:38,821] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 02:41:38,827] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37273
[2025-01-17 02:41:38,827] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [26]  [ 770/1404]  eta: 0:06:13  lr: 0.000030  min_lr: 0.000000  loss: 3.9469 (4.1493)  class_acc: 0.2917 (0.3394)  loss_scale: 32768.0000 (25287.8859)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0362  max mem: 15572
Epoch: [26]  [ 780/1404]  eta: 0:06:07  lr: 0.000030  min_lr: 0.000000  loss: 4.0850 (4.1493)  class_acc: 0.2500 (0.3384)  loss_scale: 16384.0000 (25173.8796)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0255  max mem: 15572
Epoch: [26]  [ 790/1404]  eta: 0:06:02  lr: 0.000030  min_lr: 0.000000  loss: 4.0883 (4.1495)  class_acc: 0.2500 (0.3378)  loss_scale: 16384.0000 (25062.7560)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.0007  max mem: 15572
Epoch: [26]  [ 800/1404]  eta: 0:05:56  lr: 0.000030  min_lr: 0.000000  loss: 4.2032 (4.1501)  class_acc: 0.2917 (0.3381)  loss_scale: 16384.0000 (24954.4070)  weight_decay: 0.0500 (0.0500)  time: 0.6352  data: 0.0008  max mem: 15572
Epoch: [26]  [ 810/1404]  eta: 0:05:50  lr: 0.000030  min_lr: 0.000000  loss: 4.2065 (4.1503)  class_acc: 0.2917 (0.3374)  loss_scale: 16384.0000 (24848.7300)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0008  max mem: 15572
Epoch: [26]  [ 820/1404]  eta: 0:05:44  lr: 0.000030  min_lr: 0.000000  loss: 4.1631 (4.1504)  class_acc: 0.2917 (0.3372)  loss_scale: 16384.0000 (24745.6273)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0008  max mem: 15572
Epoch: [26]  [ 830/1404]  eta: 0:05:38  lr: 0.000030  min_lr: 0.000000  loss: 4.1113 (4.1506)  class_acc: 0.2917 (0.3368)  loss_scale: 16384.0000 (24645.0060)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0006  max mem: 15572
Epoch: [26]  [ 840/1404]  eta: 0:05:33  lr: 0.000030  min_lr: 0.000000  loss: 4.2534 (4.1505)  class_acc: 0.3333 (0.3368)  loss_scale: 16384.0000 (24546.7776)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.0007  max mem: 15572
Epoch: [26]  [ 850/1404]  eta: 0:05:27  lr: 0.000030  min_lr: 0.000000  loss: 4.2428 (4.1514)  class_acc: 0.3333 (0.3373)  loss_scale: 16384.0000 (24450.8578)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0007  max mem: 15572
Epoch: [26]  [ 860/1404]  eta: 0:05:21  lr: 0.000030  min_lr: 0.000000  loss: 4.2428 (4.1526)  class_acc: 0.3750 (0.3372)  loss_scale: 16384.0000 (24357.1661)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0006  max mem: 15572
Epoch: [26]  [ 870/1404]  eta: 0:05:15  lr: 0.000030  min_lr: 0.000000  loss: 4.1905 (4.1522)  class_acc: 0.2917 (0.3367)  loss_scale: 16384.0000 (24265.6257)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0007  max mem: 15572
Epoch: [26]  [ 880/1404]  eta: 0:05:09  lr: 0.000030  min_lr: 0.000000  loss: 4.0795 (4.1515)  class_acc: 0.3333 (0.3370)  loss_scale: 16384.0000 (24176.1635)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0008  max mem: 15572
Epoch: [26]  [ 890/1404]  eta: 0:05:03  lr: 0.000030  min_lr: 0.000000  loss: 4.1287 (4.1509)  class_acc: 0.3333 (0.3367)  loss_scale: 16384.0000 (24088.7093)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0006  max mem: 15572
[2025-01-17 02:42:54,930] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:42:54,930] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:42:54,991] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:42:54,992] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [ 900/1404]  eta: 0:04:57  lr: 0.000030  min_lr: 0.000000  loss: 4.1287 (4.1504)  class_acc: 0.2917 (0.3368)  loss_scale: 16384.0000 (24057.7492)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0006  max mem: 15572
Epoch: [26]  [ 910/1404]  eta: 0:04:51  lr: 0.000030  min_lr: 0.000000  loss: 4.1134 (4.1504)  class_acc: 0.3333 (0.3368)  loss_scale: 32768.0000 (24153.3611)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0008  max mem: 15572
Epoch: [26]  [ 920/1404]  eta: 0:04:45  lr: 0.000030  min_lr: 0.000000  loss: 4.1105 (4.1485)  class_acc: 0.3333 (0.3367)  loss_scale: 32768.0000 (24246.8969)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.0007  max mem: 15572
Epoch: [26]  [ 930/1404]  eta: 0:04:39  lr: 0.000030  min_lr: 0.000000  loss: 4.0623 (4.1493)  class_acc: 0.3333 (0.3368)  loss_scale: 32768.0000 (24338.4232)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0006  max mem: 15572
Epoch: [26]  [ 940/1404]  eta: 0:04:34  lr: 0.000030  min_lr: 0.000000  loss: 4.3184 (4.1516)  class_acc: 0.3750 (0.3370)  loss_scale: 32768.0000 (24428.0043)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0007  max mem: 15572
Epoch: [26]  [ 950/1404]  eta: 0:04:27  lr: 0.000030  min_lr: 0.000000  loss: 4.2384 (4.1505)  class_acc: 0.3333 (0.3370)  loss_scale: 32768.0000 (24515.7014)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0032  max mem: 15572
Epoch: [26]  [ 960/1404]  eta: 0:04:21  lr: 0.000030  min_lr: 0.000000  loss: 4.0773 (4.1500)  class_acc: 0.3333 (0.3362)  loss_scale: 32768.0000 (24601.5734)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0031  max mem: 15572
[2025-01-17 02:43:36,924] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37474
[2025-01-17 02:43:36,924] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:43:36,924] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [26]  [ 970/1404]  eta: 0:04:15  lr: 0.000030  min_lr: 0.000000  loss: 4.1047 (4.1502)  class_acc: 0.2917 (0.3358)  loss_scale: 32768.0000 (24668.8033)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0006  max mem: 15572
[2025-01-17 02:43:37,003] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37474
[2025-01-17 02:43:37,003] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [26]  [ 980/1404]  eta: 0:04:10  lr: 0.000030  min_lr: 0.000000  loss: 4.1906 (4.1499)  class_acc: 0.3333 (0.3363)  loss_scale: 16384.0000 (24584.3507)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.0006  max mem: 15572
Epoch: [26]  [ 990/1404]  eta: 0:04:03  lr: 0.000030  min_lr: 0.000000  loss: 4.0621 (4.1484)  class_acc: 0.4167 (0.3375)  loss_scale: 16384.0000 (24501.6024)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0006  max mem: 15572
Epoch: [26]  [1000/1404]  eta: 0:03:57  lr: 0.000030  min_lr: 0.000000  loss: 4.0844 (4.1485)  class_acc: 0.3333 (0.3375)  loss_scale: 16384.0000 (24420.5075)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0006  max mem: 15572
Epoch: [26]  [1010/1404]  eta: 0:03:52  lr: 0.000030  min_lr: 0.000000  loss: 4.1450 (4.1486)  class_acc: 0.3333 (0.3377)  loss_scale: 16384.0000 (24341.0168)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0006  max mem: 15572
Epoch: [26]  [1020/1404]  eta: 0:03:46  lr: 0.000030  min_lr: 0.000000  loss: 4.2662 (4.1495)  class_acc: 0.3333 (0.3377)  loss_scale: 16384.0000 (24263.0833)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0006  max mem: 15572
Epoch: [26]  [1030/1404]  eta: 0:03:40  lr: 0.000030  min_lr: 0.000000  loss: 4.2662 (4.1505)  class_acc: 0.3333 (0.3375)  loss_scale: 16384.0000 (24186.6615)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0007  max mem: 15572
Epoch: [26]  [1040/1404]  eta: 0:03:34  lr: 0.000030  min_lr: 0.000000  loss: 4.2272 (4.1500)  class_acc: 0.2917 (0.3370)  loss_scale: 16384.0000 (24111.7080)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0007  max mem: 15572
Epoch: [26]  [1050/1404]  eta: 0:03:28  lr: 0.000029  min_lr: 0.000000  loss: 4.1356 (4.1499)  class_acc: 0.2917 (0.3371)  loss_scale: 16384.0000 (24038.1808)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0005  max mem: 15572
Epoch: [26]  [1060/1404]  eta: 0:03:22  lr: 0.000029  min_lr: 0.000000  loss: 4.1618 (4.1495)  class_acc: 0.2917 (0.3374)  loss_scale: 16384.0000 (23966.0396)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.0006  max mem: 15572
Epoch: [26]  [1070/1404]  eta: 0:03:16  lr: 0.000029  min_lr: 0.000000  loss: 4.1724 (4.1509)  class_acc: 0.2917 (0.3372)  loss_scale: 16384.0000 (23895.2456)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0008  max mem: 15572
Epoch: [26]  [1080/1404]  eta: 0:03:10  lr: 0.000029  min_lr: 0.000000  loss: 4.2533 (4.1517)  class_acc: 0.2917 (0.3371)  loss_scale: 16384.0000 (23825.7613)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0008  max mem: 15572
Epoch: [26]  [1090/1404]  eta: 0:03:05  lr: 0.000029  min_lr: 0.000000  loss: 4.1941 (4.1516)  class_acc: 0.2917 (0.3372)  loss_scale: 16384.0000 (23757.5509)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0006  max mem: 15572
[2025-01-17 02:44:52,647] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:44:52,647] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:44:52,650] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:44:52,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [1100/1404]  eta: 0:02:59  lr: 0.000029  min_lr: 0.000000  loss: 4.2360 (4.1522)  class_acc: 0.2500 (0.3367)  loss_scale: 16384.0000 (23720.3415)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.0008  max mem: 15572
Epoch: [26]  [1110/1404]  eta: 0:02:53  lr: 0.000029  min_lr: 0.000000  loss: 4.1905 (4.1514)  class_acc: 0.3333 (0.3368)  loss_scale: 32768.0000 (23801.7786)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0009  max mem: 15572
Epoch: [26]  [1120/1404]  eta: 0:02:47  lr: 0.000029  min_lr: 0.000000  loss: 4.0712 (4.1509)  class_acc: 0.3333 (0.3364)  loss_scale: 32768.0000 (23881.7627)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.0006  max mem: 15572
Epoch: [26]  [1130/1404]  eta: 0:02:41  lr: 0.000029  min_lr: 0.000000  loss: 4.2371 (4.1516)  class_acc: 0.2500 (0.3362)  loss_scale: 32768.0000 (23960.3324)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.0006  max mem: 15572
Epoch: [26]  [1140/1404]  eta: 0:02:35  lr: 0.000029  min_lr: 0.000000  loss: 4.2774 (4.1532)  class_acc: 0.3333 (0.3362)  loss_scale: 32768.0000 (24037.5250)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0007  max mem: 15572
Epoch: [26]  [1150/1404]  eta: 0:02:29  lr: 0.000029  min_lr: 0.000000  loss: 4.3436 (4.1547)  class_acc: 0.3333 (0.3360)  loss_scale: 32768.0000 (24113.3762)  weight_decay: 0.0500 (0.0500)  time: 0.4881  data: 0.0008  max mem: 15572
Epoch: [26]  [1160/1404]  eta: 0:02:23  lr: 0.000029  min_lr: 0.000000  loss: 4.3083 (4.1550)  class_acc: 0.3333 (0.3363)  loss_scale: 32768.0000 (24187.9208)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0010  max mem: 15572
[2025-01-17 02:45:29,879] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37668
[2025-01-17 02:45:29,879] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:45:29,893] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37668
[2025-01-17 02:45:29,894] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:45:29,894] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [26]  [1170/1404]  eta: 0:02:17  lr: 0.000029  min_lr: 0.000000  loss: 4.2516 (4.1556)  class_acc: 0.3750 (0.3371)  loss_scale: 32768.0000 (24163.2519)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.0009  max mem: 15572
Epoch: [26]  [1180/1404]  eta: 0:02:11  lr: 0.000029  min_lr: 0.000000  loss: 4.2071 (4.1553)  class_acc: 0.3750 (0.3377)  loss_scale: 16384.0000 (24097.3819)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0008  max mem: 15572
Epoch: [26]  [1190/1404]  eta: 0:02:06  lr: 0.000029  min_lr: 0.000000  loss: 4.1649 (4.1555)  class_acc: 0.3750 (0.3377)  loss_scale: 16384.0000 (24032.6180)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.0007  max mem: 15572
Epoch: [26]  [1200/1404]  eta: 0:02:00  lr: 0.000029  min_lr: 0.000000  loss: 4.1992 (4.1566)  class_acc: 0.2917 (0.3375)  loss_scale: 16384.0000 (23968.9326)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0007  max mem: 15572
Epoch: [26]  [1210/1404]  eta: 0:01:54  lr: 0.000029  min_lr: 0.000000  loss: 4.2183 (4.1570)  class_acc: 0.3333 (0.3381)  loss_scale: 16384.0000 (23906.2989)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.0008  max mem: 15572
Epoch: [26]  [1220/1404]  eta: 0:01:48  lr: 0.000029  min_lr: 0.000000  loss: 4.0957 (4.1560)  class_acc: 0.3333 (0.3378)  loss_scale: 16384.0000 (23844.6912)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0008  max mem: 15572
Epoch: [26]  [1230/1404]  eta: 0:01:42  lr: 0.000029  min_lr: 0.000000  loss: 4.2786 (4.1581)  class_acc: 0.3333 (0.3379)  loss_scale: 16384.0000 (23784.0845)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0009  max mem: 15572
Epoch: [26]  [1240/1404]  eta: 0:01:36  lr: 0.000029  min_lr: 0.000000  loss: 4.1924 (4.1566)  class_acc: 0.3333 (0.3383)  loss_scale: 16384.0000 (23724.4545)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0009  max mem: 15572
Epoch: [26]  [1250/1404]  eta: 0:01:30  lr: 0.000029  min_lr: 0.000000  loss: 3.8971 (4.1541)  class_acc: 0.3333 (0.3389)  loss_scale: 16384.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0007  max mem: 15572
Epoch: [26]  [1260/1404]  eta: 0:01:24  lr: 0.000029  min_lr: 0.000000  loss: 3.9636 (4.1545)  class_acc: 0.3333 (0.3386)  loss_scale: 16384.0000 (23608.0317)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0007  max mem: 15572
Epoch: [26]  [1270/1404]  eta: 0:01:18  lr: 0.000029  min_lr: 0.000000  loss: 4.2286 (4.1548)  class_acc: 0.2917 (0.3385)  loss_scale: 16384.0000 (23551.1943)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0006  max mem: 15572
Epoch: [26]  [1280/1404]  eta: 0:01:12  lr: 0.000029  min_lr: 0.000000  loss: 4.1153 (4.1544)  class_acc: 0.3333 (0.3386)  loss_scale: 16384.0000 (23495.2443)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.0008  max mem: 15572
Epoch: [26]  [1290/1404]  eta: 0:01:07  lr: 0.000029  min_lr: 0.000000  loss: 4.2100 (4.1553)  class_acc: 0.3333 (0.3387)  loss_scale: 16384.0000 (23440.1611)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.0008  max mem: 15572
[2025-01-17 02:46:46,091] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:46:46,092] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:46:46,209] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:46:46,210] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [1300/1404]  eta: 0:01:01  lr: 0.000029  min_lr: 0.000000  loss: 4.2594 (4.1563)  class_acc: 0.3333 (0.3388)  loss_scale: 16384.0000 (23486.6718)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0010  max mem: 15572
Epoch: [26]  [1310/1404]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000000  loss: 4.1769 (4.1555)  class_acc: 0.3333 (0.3386)  loss_scale: 32768.0000 (23557.4676)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0009  max mem: 15572
Epoch: [26]  [1320/1404]  eta: 0:00:49  lr: 0.000029  min_lr: 0.000000  loss: 4.1769 (4.1558)  class_acc: 0.2917 (0.3387)  loss_scale: 32768.0000 (23627.1915)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.0007  max mem: 15572
Epoch: [26]  [1330/1404]  eta: 0:00:43  lr: 0.000029  min_lr: 0.000000  loss: 4.1893 (4.1544)  class_acc: 0.3750 (0.3390)  loss_scale: 32768.0000 (23695.8678)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0006  max mem: 15572
Epoch: [26]  [1340/1404]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000000  loss: 4.1265 (4.1548)  class_acc: 0.2917 (0.3388)  loss_scale: 32768.0000 (23763.5198)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0006  max mem: 15572
Epoch: [26]  [1350/1404]  eta: 0:00:31  lr: 0.000029  min_lr: 0.000000  loss: 4.1913 (4.1553)  class_acc: 0.2917 (0.3389)  loss_scale: 32768.0000 (23830.1702)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.0006  max mem: 15572
Epoch: [26]  [1360/1404]  eta: 0:00:25  lr: 0.000029  min_lr: 0.000000  loss: 4.1418 (4.1545)  class_acc: 0.3333 (0.3388)  loss_scale: 32768.0000 (23895.8413)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0005  max mem: 15572
Epoch: [26]  [1370/1404]  eta: 0:00:19  lr: 0.000029  min_lr: 0.000000  loss: 4.1418 (4.1540)  class_acc: 0.2917 (0.3390)  loss_scale: 32768.0000 (23960.5543)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0006  max mem: 15572
Epoch: [26]  [1380/1404]  eta: 0:00:14  lr: 0.000029  min_lr: 0.000000  loss: 4.2768 (4.1552)  class_acc: 0.2917 (0.3389)  loss_scale: 32768.0000 (24024.3302)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.0007  max mem: 15572
Epoch: [26]  [1390/1404]  eta: 0:00:08  lr: 0.000029  min_lr: 0.000000  loss: 4.3200 (4.1560)  class_acc: 0.2917 (0.3387)  loss_scale: 32768.0000 (24087.1891)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0009  max mem: 15572
Epoch: [26]  [1400/1404]  eta: 0:00:02  lr: 0.000029  min_lr: 0.000000  loss: 4.1771 (4.1542)  class_acc: 0.2917 (0.3391)  loss_scale: 32768.0000 (24149.1506)  weight_decay: 0.0500 (0.0500)  time: 0.4555  data: 0.0006  max mem: 15572
Epoch: [26]  [1403/1404]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000000  loss: 4.0559 (4.1539)  class_acc: 0.3333 (0.3391)  loss_scale: 32768.0000 (24167.5670)  weight_decay: 0.0500 (0.0500)  time: 0.4357  data: 0.0004  max mem: 15572
Epoch: [26] Total time: 0:13:43 (0.5862 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000000  loss: 4.0559 (4.1513)  class_acc: 0.3333 (0.3381)  loss_scale: 32768.0000 (24167.5670)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:14:11  loss: 1.7784 (1.7784)  acc1: 61.1111 (61.1111)  acc5: 83.3333 (83.3333)  time: 6.2611  data: 6.0852  max mem: 15572
Val:  [ 10/136]  eta: 0:01:48  loss: 2.3213 (2.2847)  acc1: 55.5556 (49.4949)  acc5: 77.7778 (78.7879)  time: 0.8586  data: 0.6655  max mem: 15572
Val:  [ 20/136]  eta: 0:01:09  loss: 2.4832 (2.4227)  acc1: 44.4444 (46.0317)  acc5: 72.2222 (76.4550)  time: 0.3186  data: 0.1287  max mem: 15572
Val:  [ 30/136]  eta: 0:00:57  loss: 2.3689 (2.3240)  acc1: 44.4444 (48.2079)  acc5: 77.7778 (78.1362)  time: 0.3661  data: 0.1810  max mem: 15572
Val:  [ 40/136]  eta: 0:00:46  loss: 2.0700 (2.2866)  acc1: 55.5556 (49.7290)  acc5: 83.3333 (78.9973)  time: 0.3710  data: 0.1877  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 2.0919 (2.2840)  acc1: 50.0000 (49.5643)  acc5: 83.3333 (79.8475)  time: 0.3472  data: 0.1594  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.3358 (2.3519)  acc1: 44.4444 (47.0856)  acc5: 83.3333 (78.3242)  time: 0.3162  data: 0.1240  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.3539 (2.3397)  acc1: 44.4444 (47.7308)  acc5: 77.7778 (78.6385)  time: 0.3172  data: 0.1226  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.2116 (2.3351)  acc1: 44.4444 (47.3937)  acc5: 83.3333 (79.1495)  time: 0.3888  data: 0.1878  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.2705 (2.3443)  acc1: 44.4444 (47.4359)  acc5: 77.7778 (78.8156)  time: 0.3353  data: 0.1378  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.4784 (2.4036)  acc1: 38.8889 (45.5996)  acc5: 72.2222 (77.2827)  time: 0.2666  data: 0.0701  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.4986 (2.4043)  acc1: 38.8889 (45.5455)  acc5: 72.2222 (77.3273)  time: 0.3380  data: 0.1350  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1727 (2.3701)  acc1: 50.0000 (46.5565)  acc5: 83.3333 (78.1451)  time: 0.3712  data: 0.1665  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.0322 (2.3401)  acc1: 55.5556 (47.4555)  acc5: 88.8889 (78.8380)  time: 0.2775  data: 0.1021  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.0638 (2.3426)  acc1: 50.0000 (47.6249)  acc5: 83.3333 (78.8698)  time: 0.1896  data: 0.0349  max mem: 15572
Val: Total time: 0:00:49 (0.3668 s / it)
* Acc@1 46.048 Acc@5 78.092 loss 2.388
Accuracy of the network on the 4883 val videos: 46.0%
Max accuracy: 46.25%
Epoch: [27]  [   0/1404]  eta: 2:46:37  lr: 0.000029  min_lr: 0.000000  loss: 3.8939 (3.8939)  class_acc: 0.6250 (0.6250)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.1211  data: 6.4112  max mem: 15572
Epoch: [27]  [  10/1404]  eta: 0:26:43  lr: 0.000028  min_lr: 0.000000  loss: 4.1678 (4.1627)  class_acc: 0.2917 (0.3788)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1503  data: 0.6362  max mem: 15572
[2025-01-17 02:48:54,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:48:54,003] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 02:48:54,103] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:48:54,104] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 02:48:54,524] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37926
[2025-01-17 02:48:54,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 02:48:54,525] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 02:48:54,527] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37926
[2025-01-17 02:48:54,527] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [27]  [  20/1404]  eta: 0:21:45  lr: 0.000028  min_lr: 0.000000  loss: 4.1678 (4.1702)  class_acc: 0.2917 (0.3611)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.1445  max mem: 15572
Epoch: [27]  [  30/1404]  eta: 0:18:34  lr: 0.000028  min_lr: 0.000000  loss: 4.1650 (4.1419)  class_acc: 0.3333 (0.3696)  loss_scale: 32768.0000 (33825.0323)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1347  max mem: 15572
Epoch: [27]  [  40/1404]  eta: 0:17:47  lr: 0.000028  min_lr: 0.000000  loss: 4.1038 (4.0979)  class_acc: 0.3333 (0.3669)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1260  max mem: 15572
Epoch: [27]  [  50/1404]  eta: 0:17:00  lr: 0.000028  min_lr: 0.000000  loss: 4.1038 (4.0868)  class_acc: 0.2917 (0.3603)  loss_scale: 32768.0000 (33410.5098)  weight_decay: 0.0500 (0.0500)  time: 0.6636  data: 0.1902  max mem: 15572
Epoch: [27]  [  60/1404]  eta: 0:15:48  lr: 0.000028  min_lr: 0.000000  loss: 4.0105 (4.0687)  class_acc: 0.2917 (0.3566)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0841  max mem: 15572
Epoch: [27]  [  70/1404]  eta: 0:15:21  lr: 0.000028  min_lr: 0.000000  loss: 4.0477 (4.0815)  class_acc: 0.3333 (0.3574)  loss_scale: 32768.0000 (33229.5211)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0543  max mem: 15572
Epoch: [27]  [  80/1404]  eta: 0:15:03  lr: 0.000028  min_lr: 0.000000  loss: 4.0993 (4.0822)  class_acc: 0.4167 (0.3719)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1275  max mem: 15572
[2025-01-17 02:49:37,143] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37997
[2025-01-17 02:49:37,143] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:49:37,143] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 02:49:37,143] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37997
[2025-01-17 02:49:37,144] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [27]  [  90/1404]  eta: 0:14:43  lr: 0.000028  min_lr: 0.000000  loss: 4.0993 (4.0993)  class_acc: 0.3750 (0.3686)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1289  max mem: 15572
[2025-01-17 02:49:39,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=38000, skipped=226, lr=[2.738766601344082e-07, 2.738766601344082e-07, 3.9125237162058317e-07, 3.9125237162058317e-07, 5.58931959457976e-07, 5.58931959457976e-07, 7.984742277971087e-07, 7.984742277971087e-07, 1.1406774682815838e-06, 1.1406774682815838e-06, 1.6295392404022628e-06, 1.6295392404022628e-06, 2.3279132005746613e-06, 2.3279132005746613e-06, 3.3255902865352305e-06, 3.3255902865352305e-06, 4.750843266478901e-06, 4.750843266478901e-06, 6.786918952112716e-06, 6.786918952112716e-06, 9.695598503018165e-06, 9.695598503018165e-06, 1.3850855004311667e-05, 1.3850855004311667e-05, 1.978693572044524e-05, 1.978693572044524e-05, 2.8267051029207487e-05, 2.8267051029207487e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 02:49:39,218] [INFO] [timer.py:260:stop] epoch=0/micro_step=38000/global_step=38000, RunningAvgSamplesPerSec=47.944604138278905, CurrSamplesPerSec=44.732502969552584, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [27]  [ 100/1404]  eta: 0:14:26  lr: 0.000028  min_lr: 0.000000  loss: 4.0400 (4.0908)  class_acc: 0.3750 (0.3725)  loss_scale: 16384.0000 (31145.8218)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1062  max mem: 15572
Epoch: [27]  [ 110/1404]  eta: 0:13:57  lr: 0.000028  min_lr: 0.000000  loss: 4.1129 (4.1022)  class_acc: 0.4583 (0.3754)  loss_scale: 16384.0000 (29815.9279)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0511  max mem: 15572
Epoch: [27]  [ 120/1404]  eta: 0:13:37  lr: 0.000028  min_lr: 0.000000  loss: 4.2031 (4.0973)  class_acc: 0.3750 (0.3764)  loss_scale: 16384.0000 (28705.8512)  weight_decay: 0.0500 (0.0500)  time: 0.4951  data: 0.0007  max mem: 15572
Epoch: [27]  [ 130/1404]  eta: 0:13:33  lr: 0.000028  min_lr: 0.000000  loss: 3.9278 (4.1005)  class_acc: 0.3750 (0.3801)  loss_scale: 16384.0000 (27765.2519)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0802  max mem: 15572
Epoch: [27]  [ 140/1404]  eta: 0:13:17  lr: 0.000028  min_lr: 0.000000  loss: 4.0840 (4.0949)  class_acc: 0.3750 (0.3797)  loss_scale: 16384.0000 (26958.0709)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1033  max mem: 15572
Epoch: [27]  [ 150/1404]  eta: 0:13:06  lr: 0.000028  min_lr: 0.000000  loss: 4.0921 (4.0973)  class_acc: 0.3750 (0.3827)  loss_scale: 16384.0000 (26257.8013)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0604  max mem: 15572
Epoch: [27]  [ 160/1404]  eta: 0:12:58  lr: 0.000028  min_lr: 0.000000  loss: 4.1304 (4.1005)  class_acc: 0.3750 (0.3843)  loss_scale: 16384.0000 (25644.5217)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1007  max mem: 15572
Epoch: [27]  [ 170/1404]  eta: 0:12:53  lr: 0.000028  min_lr: 0.000000  loss: 4.0766 (4.0986)  class_acc: 0.3333 (0.3791)  loss_scale: 16384.0000 (25102.9708)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1454  max mem: 15572
Epoch: [27]  [ 180/1404]  eta: 0:12:43  lr: 0.000028  min_lr: 0.000000  loss: 4.0574 (4.1003)  class_acc: 0.2917 (0.3771)  loss_scale: 16384.0000 (24621.2597)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1289  max mem: 15572
Epoch: [27]  [ 190/1404]  eta: 0:12:32  lr: 0.000028  min_lr: 0.000000  loss: 4.1823 (4.1034)  class_acc: 0.3333 (0.3748)  loss_scale: 16384.0000 (24189.9895)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0479  max mem: 15572
Epoch: [27]  [ 200/1404]  eta: 0:12:20  lr: 0.000028  min_lr: 0.000000  loss: 4.2189 (4.1155)  class_acc: 0.3333 (0.3738)  loss_scale: 16384.0000 (23801.6318)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0157  max mem: 15572
Epoch: [27]  [ 210/1404]  eta: 0:12:09  lr: 0.000028  min_lr: 0.000000  loss: 4.3617 (4.1211)  class_acc: 0.3333 (0.3707)  loss_scale: 16384.0000 (23450.0853)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0284  max mem: 15572
[2025-01-17 02:50:50,520] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:50:50,521] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:50:50,521] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:50:50,521] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [ 220/1404]  eta: 0:12:07  lr: 0.000028  min_lr: 0.000000  loss: 4.3512 (4.1279)  class_acc: 0.2917 (0.3665)  loss_scale: 16384.0000 (23352.7602)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1102  max mem: 15572
Epoch: [27]  [ 230/1404]  eta: 0:12:00  lr: 0.000028  min_lr: 0.000000  loss: 4.2739 (4.1254)  class_acc: 0.3333 (0.3654)  loss_scale: 32768.0000 (23760.3463)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1534  max mem: 15572
Epoch: [27]  [ 240/1404]  eta: 0:11:49  lr: 0.000028  min_lr: 0.000000  loss: 4.1422 (4.1294)  class_acc: 0.3333 (0.3638)  loss_scale: 32768.0000 (24134.1079)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.0709  max mem: 15572
Epoch: [27]  [ 250/1404]  eta: 0:11:38  lr: 0.000028  min_lr: 0.000000  loss: 4.3276 (4.1383)  class_acc: 0.2917 (0.3606)  loss_scale: 32768.0000 (24478.0876)  weight_decay: 0.0500 (0.0500)  time: 0.5090  data: 0.0145  max mem: 15572
Epoch: [27]  [ 260/1404]  eta: 0:11:31  lr: 0.000028  min_lr: 0.000000  loss: 4.1880 (4.1390)  class_acc: 0.2500 (0.3579)  loss_scale: 32768.0000 (24795.7088)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0008  max mem: 15572
Epoch: [27]  [ 270/1404]  eta: 0:11:28  lr: 0.000028  min_lr: 0.000000  loss: 4.0892 (4.1333)  class_acc: 0.2917 (0.3584)  loss_scale: 32768.0000 (25089.8893)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.0009  max mem: 15572
Epoch: [27]  [ 280/1404]  eta: 0:11:19  lr: 0.000028  min_lr: 0.000000  loss: 4.1050 (4.1361)  class_acc: 0.3750 (0.3602)  loss_scale: 32768.0000 (25363.1317)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0006  max mem: 15572
Epoch: [27]  [ 290/1404]  eta: 0:11:15  lr: 0.000028  min_lr: 0.000000  loss: 4.1420 (4.1362)  class_acc: 0.3750 (0.3594)  loss_scale: 32768.0000 (25617.5945)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.0008  max mem: 15572
[2025-01-17 02:51:37,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38206
[2025-01-17 02:51:37,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:51:37,902] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 02:51:37,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38206
[2025-01-17 02:51:37,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [27]  [ 300/1404]  eta: 0:11:09  lr: 0.000028  min_lr: 0.000000  loss: 4.1885 (4.1423)  class_acc: 0.3750 (0.3595)  loss_scale: 32768.0000 (25691.8538)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.0009  max mem: 15572
Epoch: [27]  [ 310/1404]  eta: 0:11:03  lr: 0.000028  min_lr: 0.000000  loss: 4.2512 (4.1413)  class_acc: 0.3750 (0.3589)  loss_scale: 16384.0000 (25392.5659)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.0008  max mem: 15572
Epoch: [27]  [ 320/1404]  eta: 0:10:55  lr: 0.000028  min_lr: 0.000000  loss: 4.0361 (4.1373)  class_acc: 0.3750 (0.3618)  loss_scale: 16384.0000 (25111.9252)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0006  max mem: 15572
Epoch: [27]  [ 330/1404]  eta: 0:10:47  lr: 0.000028  min_lr: 0.000000  loss: 4.0698 (4.1416)  class_acc: 0.3750 (0.3604)  loss_scale: 16384.0000 (24848.2417)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0006  max mem: 15572
Epoch: [27]  [ 340/1404]  eta: 0:10:42  lr: 0.000028  min_lr: 0.000000  loss: 4.2198 (4.1438)  class_acc: 0.3750 (0.3622)  loss_scale: 16384.0000 (24600.0235)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0006  max mem: 15572
Epoch: [27]  [ 350/1404]  eta: 0:10:35  lr: 0.000028  min_lr: 0.000000  loss: 4.1842 (4.1470)  class_acc: 0.3750 (0.3617)  loss_scale: 16384.0000 (24365.9487)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0008  max mem: 15572
Epoch: [27]  [ 360/1404]  eta: 0:10:28  lr: 0.000028  min_lr: 0.000000  loss: 4.2331 (4.1489)  class_acc: 0.3333 (0.3606)  loss_scale: 16384.0000 (24144.8421)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0252  max mem: 15572
Epoch: [27]  [ 370/1404]  eta: 0:10:24  lr: 0.000028  min_lr: 0.000000  loss: 4.2331 (4.1525)  class_acc: 0.1667 (0.3571)  loss_scale: 16384.0000 (23935.6550)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.0251  max mem: 15572
Epoch: [27]  [ 380/1404]  eta: 0:10:16  lr: 0.000027  min_lr: 0.000000  loss: 4.1835 (4.1471)  class_acc: 0.2500 (0.3575)  loss_scale: 16384.0000 (23737.4488)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0007  max mem: 15572
Epoch: [27]  [ 390/1404]  eta: 0:10:08  lr: 0.000027  min_lr: 0.000000  loss: 4.0723 (4.1478)  class_acc: 0.3750 (0.3574)  loss_scale: 16384.0000 (23549.3811)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0006  max mem: 15572
Epoch: [27]  [ 400/1404]  eta: 0:10:03  lr: 0.000027  min_lr: 0.000000  loss: 4.0723 (4.1447)  class_acc: 0.3750 (0.3588)  loss_scale: 16384.0000 (23370.6933)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0005  max mem: 15572
Epoch: [27]  [ 410/1404]  eta: 0:09:57  lr: 0.000027  min_lr: 0.000000  loss: 4.0447 (4.1402)  class_acc: 0.3750 (0.3584)  loss_scale: 16384.0000 (23200.7007)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.0006  max mem: 15572
Epoch: [27]  [ 420/1404]  eta: 0:09:50  lr: 0.000027  min_lr: 0.000000  loss: 4.0426 (4.1397)  class_acc: 0.3750 (0.3588)  loss_scale: 16384.0000 (23038.7838)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0005  max mem: 15572
[2025-01-17 02:52:53,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:52:53,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:52:53,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:52:53,657] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [ 430/1404]  eta: 0:09:43  lr: 0.000027  min_lr: 0.000000  loss: 4.1199 (4.1403)  class_acc: 0.3333 (0.3577)  loss_scale: 16384.0000 (23036.4362)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0008  max mem: 15572
Epoch: [27]  [ 440/1404]  eta: 0:09:38  lr: 0.000027  min_lr: 0.000000  loss: 4.2377 (4.1395)  class_acc: 0.2917 (0.3565)  loss_scale: 32768.0000 (23257.1066)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0007  max mem: 15572
Epoch: [27]  [ 450/1404]  eta: 0:09:33  lr: 0.000027  min_lr: 0.000000  loss: 4.1313 (4.1371)  class_acc: 0.2917 (0.3556)  loss_scale: 32768.0000 (23467.9911)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.0006  max mem: 15572
[2025-01-17 02:53:11,506] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38364
[2025-01-17 02:53:11,507] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:53:11,507] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 02:53:11,534] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38364
[2025-01-17 02:53:11,534] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [27]  [ 460/1404]  eta: 0:09:24  lr: 0.000027  min_lr: 0.000000  loss: 4.0224 (4.1361)  class_acc: 0.3333 (0.3561)  loss_scale: 32768.0000 (23492.0260)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0006  max mem: 15572
Epoch: [27]  [ 470/1404]  eta: 0:09:18  lr: 0.000027  min_lr: 0.000000  loss: 4.0695 (4.1367)  class_acc: 0.3333 (0.3554)  loss_scale: 16384.0000 (23341.1125)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0005  max mem: 15572
Epoch: [27]  [ 480/1404]  eta: 0:09:12  lr: 0.000027  min_lr: 0.000000  loss: 4.2128 (4.1400)  class_acc: 0.2917 (0.3530)  loss_scale: 16384.0000 (23196.4740)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0006  max mem: 15572
Epoch: [27]  [ 490/1404]  eta: 0:09:07  lr: 0.000027  min_lr: 0.000000  loss: 4.2595 (4.1418)  class_acc: 0.2917 (0.3530)  loss_scale: 16384.0000 (23057.7271)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.0007  max mem: 15572
Epoch: [27]  [ 500/1404]  eta: 0:09:02  lr: 0.000027  min_lr: 0.000000  loss: 4.0860 (4.1397)  class_acc: 0.3333 (0.3535)  loss_scale: 16384.0000 (22924.5190)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.0007  max mem: 15572
Epoch: [27]  [ 510/1404]  eta: 0:08:54  lr: 0.000027  min_lr: 0.000000  loss: 4.0459 (4.1382)  class_acc: 0.3750 (0.3554)  loss_scale: 16384.0000 (22796.5245)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0007  max mem: 15572
Epoch: [27]  [ 520/1404]  eta: 0:08:47  lr: 0.000027  min_lr: 0.000000  loss: 4.1868 (4.1404)  class_acc: 0.3333 (0.3532)  loss_scale: 16384.0000 (22673.4434)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0007  max mem: 15572
Epoch: [27]  [ 530/1404]  eta: 0:08:40  lr: 0.000027  min_lr: 0.000000  loss: 4.2065 (4.1397)  class_acc: 0.2917 (0.3535)  loss_scale: 16384.0000 (22554.9981)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0005  max mem: 15572
Epoch: [27]  [ 540/1404]  eta: 0:08:35  lr: 0.000027  min_lr: 0.000000  loss: 4.0986 (4.1383)  class_acc: 0.3333 (0.3540)  loss_scale: 16384.0000 (22440.9316)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0006  max mem: 15572
Epoch: [27]  [ 550/1404]  eta: 0:08:29  lr: 0.000027  min_lr: 0.000000  loss: 4.1032 (4.1373)  class_acc: 0.3333 (0.3538)  loss_scale: 16384.0000 (22331.0054)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0007  max mem: 15572
Epoch: [27]  [ 560/1404]  eta: 0:08:21  lr: 0.000027  min_lr: 0.000000  loss: 4.1032 (4.1350)  class_acc: 0.3333 (0.3540)  loss_scale: 16384.0000 (22224.9982)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0008  max mem: 15572
Epoch: [27]  [ 570/1404]  eta: 0:08:14  lr: 0.000027  min_lr: 0.000000  loss: 4.0457 (4.1344)  class_acc: 0.3750 (0.3545)  loss_scale: 16384.0000 (22122.7040)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0009  max mem: 15572
Epoch: [27]  [ 580/1404]  eta: 0:08:08  lr: 0.000027  min_lr: 0.000000  loss: 4.1013 (4.1350)  class_acc: 0.3333 (0.3540)  loss_scale: 16384.0000 (22023.9312)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0009  max mem: 15572
[2025-01-17 02:54:24,411] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:54:24,411] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:54:24,433] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:54:24,434] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [ 590/1404]  eta: 0:08:01  lr: 0.000027  min_lr: 0.000000  loss: 4.1565 (4.1363)  class_acc: 0.2917 (0.3527)  loss_scale: 16384.0000 (22094.8359)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0010  max mem: 15572
Epoch: [27]  [ 600/1404]  eta: 0:07:56  lr: 0.000027  min_lr: 0.000000  loss: 4.2394 (4.1386)  class_acc: 0.2500 (0.3520)  loss_scale: 32768.0000 (22272.4260)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.0521  max mem: 15572
Epoch: [27]  [ 610/1404]  eta: 0:07:50  lr: 0.000027  min_lr: 0.000000  loss: 4.1664 (4.1388)  class_acc: 0.2917 (0.3513)  loss_scale: 32768.0000 (22444.2029)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1102  max mem: 15572
Epoch: [27]  [ 620/1404]  eta: 0:07:45  lr: 0.000027  min_lr: 0.000000  loss: 4.1267 (4.1382)  class_acc: 0.2917 (0.3506)  loss_scale: 32768.0000 (22610.4477)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1225  max mem: 15572
Epoch: [27]  [ 630/1404]  eta: 0:07:39  lr: 0.000027  min_lr: 0.000000  loss: 4.1538 (4.1365)  class_acc: 0.3750 (0.3514)  loss_scale: 32768.0000 (22771.4231)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1225  max mem: 15572
Epoch: [27]  [ 640/1404]  eta: 0:07:32  lr: 0.000027  min_lr: 0.000000  loss: 4.0807 (4.1344)  class_acc: 0.4167 (0.3519)  loss_scale: 32768.0000 (22927.3760)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0587  max mem: 15572
Epoch: [27]  [ 650/1404]  eta: 0:07:26  lr: 0.000027  min_lr: 0.000000  loss: 4.0186 (4.1326)  class_acc: 0.3750 (0.3523)  loss_scale: 32768.0000 (23078.5376)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0176  max mem: 15572
Epoch: [27]  [ 660/1404]  eta: 0:07:22  lr: 0.000027  min_lr: 0.000000  loss: 4.0868 (4.1335)  class_acc: 0.3750 (0.3533)  loss_scale: 32768.0000 (23225.1256)  weight_decay: 0.0500 (0.0500)  time: 0.6506  data: 0.1065  max mem: 15572
Epoch: [27]  [ 670/1404]  eta: 0:07:15  lr: 0.000027  min_lr: 0.000000  loss: 4.0159 (4.1296)  class_acc: 0.3750 (0.3531)  loss_scale: 32768.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0897  max mem: 15572
Epoch: [27]  [ 680/1404]  eta: 0:07:08  lr: 0.000027  min_lr: 0.000000  loss: 3.9587 (4.1294)  class_acc: 0.2917 (0.3527)  loss_scale: 32768.0000 (23505.3862)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0156  max mem: 15572
[2025-01-17 02:55:24,037] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38594
[2025-01-17 02:55:24,037] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:55:24,038] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38594
[2025-01-17 02:55:24,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:55:24,039] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [27]  [ 690/1404]  eta: 0:07:03  lr: 0.000027  min_lr: 0.000000  loss: 4.2780 (4.1315)  class_acc: 0.3750 (0.3532)  loss_scale: 32768.0000 (23520.8799)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.0154  max mem: 15572
Epoch: [27]  [ 700/1404]  eta: 0:06:57  lr: 0.000027  min_lr: 0.000000  loss: 4.2809 (4.1329)  class_acc: 0.3333 (0.3534)  loss_scale: 16384.0000 (23419.0699)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.0005  max mem: 15572
Epoch: [27]  [ 710/1404]  eta: 0:06:51  lr: 0.000027  min_lr: 0.000000  loss: 4.1883 (4.1332)  class_acc: 0.2917 (0.3522)  loss_scale: 16384.0000 (23320.1238)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0006  max mem: 15572
Epoch: [27]  [ 720/1404]  eta: 0:06:45  lr: 0.000027  min_lr: 0.000000  loss: 4.1099 (4.1315)  class_acc: 0.3750 (0.3531)  loss_scale: 16384.0000 (23223.9223)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0006  max mem: 15572
Epoch: [27]  [ 730/1404]  eta: 0:06:39  lr: 0.000027  min_lr: 0.000000  loss: 3.9526 (4.1311)  class_acc: 0.3750 (0.3529)  loss_scale: 16384.0000 (23130.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0006  max mem: 15572
Epoch: [27]  [ 740/1404]  eta: 0:06:33  lr: 0.000027  min_lr: 0.000000  loss: 4.0432 (4.1297)  class_acc: 0.2917 (0.3525)  loss_scale: 16384.0000 (23039.3090)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0008  max mem: 15572
Epoch: [27]  [ 750/1404]  eta: 0:06:26  lr: 0.000026  min_lr: 0.000000  loss: 4.1338 (4.1307)  class_acc: 0.3333 (0.3530)  loss_scale: 16384.0000 (22950.6897)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.0008  max mem: 15572
Epoch: [27]  [ 760/1404]  eta: 0:06:20  lr: 0.000026  min_lr: 0.000000  loss: 4.1338 (4.1297)  class_acc: 0.3750 (0.3532)  loss_scale: 16384.0000 (22864.3995)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0006  max mem: 15572
Epoch: [27]  [ 770/1404]  eta: 0:06:14  lr: 0.000026  min_lr: 0.000000  loss: 4.1215 (4.1313)  class_acc: 0.3333 (0.3527)  loss_scale: 16384.0000 (22780.3476)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.0006  max mem: 15572
Epoch: [27]  [ 780/1404]  eta: 0:06:08  lr: 0.000026  min_lr: 0.000000  loss: 4.0891 (4.1307)  class_acc: 0.3333 (0.3531)  loss_scale: 16384.0000 (22698.4481)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0006  max mem: 15572
Epoch: [27]  [ 790/1404]  eta: 0:06:02  lr: 0.000026  min_lr: 0.000000  loss: 4.0621 (4.1307)  class_acc: 0.3333 (0.3523)  loss_scale: 16384.0000 (22618.6195)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0007  max mem: 15572
Epoch: [27]  [ 800/1404]  eta: 0:05:56  lr: 0.000026  min_lr: 0.000000  loss: 4.2099 (4.1309)  class_acc: 0.2500 (0.3521)  loss_scale: 16384.0000 (22540.7840)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.0058  max mem: 15572
Epoch: [27]  [ 810/1404]  eta: 0:05:50  lr: 0.000026  min_lr: 0.000000  loss: 4.1772 (4.1313)  class_acc: 0.3750 (0.3520)  loss_scale: 16384.0000 (22464.8681)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.0419  max mem: 15572
[2025-01-17 02:56:38,499] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:56:38,499] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:56:38,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:56:38,561] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [ 820/1404]  eta: 0:05:44  lr: 0.000026  min_lr: 0.000000  loss: 4.1119 (4.1317)  class_acc: 0.3333 (0.3519)  loss_scale: 16384.0000 (22510.5384)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0882  max mem: 15572
Epoch: [27]  [ 830/1404]  eta: 0:05:38  lr: 0.000026  min_lr: 0.000000  loss: 4.1609 (4.1326)  class_acc: 0.2917 (0.3510)  loss_scale: 32768.0000 (22633.9735)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0752  max mem: 15572
Epoch: [27]  [ 840/1404]  eta: 0:05:32  lr: 0.000026  min_lr: 0.000000  loss: 4.2125 (4.1338)  class_acc: 0.3333 (0.3513)  loss_scale: 32768.0000 (22754.4732)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0782  max mem: 15572
Epoch: [27]  [ 850/1404]  eta: 0:05:26  lr: 0.000026  min_lr: 0.000000  loss: 4.1460 (4.1335)  class_acc: 0.3333 (0.3512)  loss_scale: 32768.0000 (22872.1410)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1104  max mem: 15572
Epoch: [27]  [ 860/1404]  eta: 0:05:20  lr: 0.000026  min_lr: 0.000000  loss: 3.9892 (4.1306)  class_acc: 0.3333 (0.3511)  loss_scale: 32768.0000 (22987.0755)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0778  max mem: 15572
Epoch: [27]  [ 870/1404]  eta: 0:05:14  lr: 0.000026  min_lr: 0.000000  loss: 3.9894 (4.1303)  class_acc: 0.3750 (0.3513)  loss_scale: 32768.0000 (23099.3708)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0661  max mem: 15572
Epoch: [27]  [ 880/1404]  eta: 0:05:08  lr: 0.000026  min_lr: 0.000000  loss: 4.0748 (4.1317)  class_acc: 0.3750 (0.3515)  loss_scale: 32768.0000 (23209.1169)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0738  max mem: 15572
Epoch: [27]  [ 890/1404]  eta: 0:05:03  lr: 0.000026  min_lr: 0.000000  loss: 4.0530 (4.1311)  class_acc: 0.4167 (0.3517)  loss_scale: 32768.0000 (23316.3996)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0928  max mem: 15572
Epoch: [27]  [ 900/1404]  eta: 0:04:57  lr: 0.000026  min_lr: 0.000000  loss: 4.2090 (4.1313)  class_acc: 0.4167 (0.3527)  loss_scale: 32768.0000 (23421.3008)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0634  max mem: 15572
Epoch: [27]  [ 910/1404]  eta: 0:04:51  lr: 0.000026  min_lr: 0.000000  loss: 4.2090 (4.1310)  class_acc: 0.4167 (0.3529)  loss_scale: 32768.0000 (23523.8990)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0211  max mem: 15572
Epoch: [27]  [ 920/1404]  eta: 0:04:45  lr: 0.000026  min_lr: 0.000000  loss: 3.9759 (4.1287)  class_acc: 0.3333 (0.3535)  loss_scale: 32768.0000 (23624.2693)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.0507  max mem: 15572
Epoch: [27]  [ 930/1404]  eta: 0:04:39  lr: 0.000026  min_lr: 0.000000  loss: 3.9456 (4.1280)  class_acc: 0.5000 (0.3546)  loss_scale: 32768.0000 (23722.4834)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.0697  max mem: 15572
Epoch: [27]  [ 940/1404]  eta: 0:04:33  lr: 0.000026  min_lr: 0.000000  loss: 4.0288 (4.1270)  class_acc: 0.4583 (0.3550)  loss_scale: 32768.0000 (23818.6100)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0398  max mem: 15572
[2025-01-17 02:57:55,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:57:55,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 02:57:55,322] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:57:55,322] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 02:57:55,738] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38852
[2025-01-17 02:57:55,738] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 02:57:55,739] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 02:57:55,741] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38852
[2025-01-17 02:57:55,741] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [27]  [ 950/1404]  eta: 0:04:27  lr: 0.000026  min_lr: 0.000000  loss: 4.0288 (4.1259)  class_acc: 0.3750 (0.3553)  loss_scale: 32768.0000 (23947.1714)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0512  max mem: 15572
Epoch: [27]  [ 960/1404]  eta: 0:04:21  lr: 0.000026  min_lr: 0.000000  loss: 3.9153 (4.1257)  class_acc: 0.2917 (0.3547)  loss_scale: 32768.0000 (24038.9594)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0628  max mem: 15572
[2025-01-17 02:58:06,702] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38871
[2025-01-17 02:58:06,702] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:58:06,741] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38871
[2025-01-17 02:58:06,742] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 02:58:06,742] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [27]  [ 970/1404]  eta: 0:04:15  lr: 0.000026  min_lr: 0.000000  loss: 4.1963 (4.1271)  class_acc: 0.2917 (0.3546)  loss_scale: 32768.0000 (23993.8702)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0748  max mem: 15572
Epoch: [27]  [ 980/1404]  eta: 0:04:10  lr: 0.000026  min_lr: 0.000000  loss: 4.2501 (4.1281)  class_acc: 0.3333 (0.3551)  loss_scale: 16384.0000 (23916.2977)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1300  max mem: 15572
Epoch: [27]  [ 990/1404]  eta: 0:04:04  lr: 0.000026  min_lr: 0.000000  loss: 4.2332 (4.1292)  class_acc: 0.3333 (0.3549)  loss_scale: 16384.0000 (23840.2906)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1057  max mem: 15572
Epoch: [27]  [1000/1404]  eta: 0:03:58  lr: 0.000026  min_lr: 0.000000  loss: 4.2332 (4.1305)  class_acc: 0.2917 (0.3544)  loss_scale: 16384.0000 (23765.8022)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1201  max mem: 15572
Epoch: [27]  [1010/1404]  eta: 0:03:52  lr: 0.000026  min_lr: 0.000000  loss: 4.2728 (4.1315)  class_acc: 0.2917 (0.3541)  loss_scale: 16384.0000 (23692.7873)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1113  max mem: 15572
Epoch: [27]  [1020/1404]  eta: 0:03:46  lr: 0.000026  min_lr: 0.000000  loss: 4.2719 (4.1322)  class_acc: 0.2917 (0.3540)  loss_scale: 16384.0000 (23621.2027)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0657  max mem: 15572
Epoch: [27]  [1030/1404]  eta: 0:03:40  lr: 0.000026  min_lr: 0.000000  loss: 4.2423 (4.1334)  class_acc: 0.3333 (0.3543)  loss_scale: 16384.0000 (23551.0068)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1005  max mem: 15572
Epoch: [27]  [1040/1404]  eta: 0:03:34  lr: 0.000026  min_lr: 0.000000  loss: 4.1872 (4.1335)  class_acc: 0.3333 (0.3547)  loss_scale: 16384.0000 (23482.1595)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1068  max mem: 15572
Epoch: [27]  [1050/1404]  eta: 0:03:28  lr: 0.000026  min_lr: 0.000000  loss: 4.3731 (4.1353)  class_acc: 0.3333 (0.3538)  loss_scale: 16384.0000 (23414.6223)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1074  max mem: 15572
Epoch: [27]  [1060/1404]  eta: 0:03:22  lr: 0.000026  min_lr: 0.000000  loss: 4.0658 (4.1344)  class_acc: 0.2917 (0.3537)  loss_scale: 16384.0000 (23348.3582)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0796  max mem: 15572
Epoch: [27]  [1070/1404]  eta: 0:03:16  lr: 0.000026  min_lr: 0.000000  loss: 4.0658 (4.1347)  class_acc: 0.3333 (0.3540)  loss_scale: 16384.0000 (23283.3315)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0499  max mem: 15572
Epoch: [27]  [1080/1404]  eta: 0:03:10  lr: 0.000026  min_lr: 0.000000  loss: 4.3108 (4.1365)  class_acc: 0.3333 (0.3536)  loss_scale: 16384.0000 (23219.5079)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0872  max mem: 15572
Epoch: [27]  [1090/1404]  eta: 0:03:04  lr: 0.000026  min_lr: 0.000000  loss: 4.2982 (4.1375)  class_acc: 0.2917 (0.3537)  loss_scale: 16384.0000 (23156.8543)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0873  max mem: 15572
[2025-01-17 02:59:20,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=39000, skipped=231, lr=[2.4765847856973254e-07, 2.4765847856973254e-07, 3.5379782652818937e-07, 3.5379782652818937e-07, 5.054254664688421e-07, 5.054254664688421e-07, 7.220363806697743e-07, 7.220363806697743e-07, 1.0314805438139633e-06, 1.0314805438139633e-06, 1.4735436340199477e-06, 1.4735436340199477e-06, 2.105062334314211e-06, 2.105062334314211e-06, 3.0072319061631594e-06, 3.0072319061631594e-06, 4.296045580233085e-06, 4.296045580233085e-06, 6.13720797176155e-06, 6.13720797176155e-06, 8.767439959659357e-06, 8.767439959659357e-06, 1.2524914228084798e-05, 1.2524914228084798e-05, 1.7892734611549713e-05, 1.7892734611549713e-05, 2.5561049445071018e-05, 2.5561049445071018e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 02:59:20,848] [INFO] [timer.py:260:stop] epoch=0/micro_step=39000/global_step=39000, RunningAvgSamplesPerSec=47.925278087631916, CurrSamplesPerSec=60.638384539344685, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-17 02:59:21,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:59:21,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 02:59:21,440] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 02:59:21,440] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [1100/1404]  eta: 0:02:58  lr: 0.000026  min_lr: 0.000000  loss: 4.1406 (4.1362)  class_acc: 0.3750 (0.3543)  loss_scale: 16384.0000 (23229.2679)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0475  max mem: 15572
Epoch: [27]  [1110/1404]  eta: 0:02:53  lr: 0.000026  min_lr: 0.000000  loss: 3.7197 (4.1339)  class_acc: 0.4167 (0.3546)  loss_scale: 32768.0000 (23315.1251)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0644  max mem: 15572
Epoch: [27]  [1120/1404]  eta: 0:02:47  lr: 0.000025  min_lr: 0.000000  loss: 4.0078 (4.1333)  class_acc: 0.3333 (0.3544)  loss_scale: 32768.0000 (23399.4505)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1324  max mem: 15572
Epoch: [27]  [1130/1404]  eta: 0:02:41  lr: 0.000025  min_lr: 0.000000  loss: 4.1983 (4.1347)  class_acc: 0.2500 (0.3536)  loss_scale: 32768.0000 (23482.2847)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0842  max mem: 15572
Epoch: [27]  [1140/1404]  eta: 0:02:35  lr: 0.000025  min_lr: 0.000000  loss: 4.2060 (4.1350)  class_acc: 0.2917 (0.3532)  loss_scale: 32768.0000 (23563.6670)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0047  max mem: 15572
Epoch: [27]  [1150/1404]  eta: 0:02:29  lr: 0.000025  min_lr: 0.000000  loss: 4.0145 (4.1345)  class_acc: 0.3333 (0.3532)  loss_scale: 32768.0000 (23643.6351)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0793  max mem: 15572
Epoch: [27]  [1160/1404]  eta: 0:02:23  lr: 0.000025  min_lr: 0.000000  loss: 4.0775 (4.1350)  class_acc: 0.3750 (0.3532)  loss_scale: 32768.0000 (23722.2257)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1121  max mem: 15572
[2025-01-17 03:00:01,380] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39070
[2025-01-17 03:00:01,381] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:00:01,383] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39070
[2025-01-17 03:00:01,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:00:01,383] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [27]  [1170/1404]  eta: 0:02:17  lr: 0.000025  min_lr: 0.000000  loss: 4.2040 (4.1354)  class_acc: 0.3333 (0.3528)  loss_scale: 32768.0000 (23673.5508)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1196  max mem: 15572
Epoch: [27]  [1180/1404]  eta: 0:02:11  lr: 0.000025  min_lr: 0.000000  loss: 4.2122 (4.1366)  class_acc: 0.2917 (0.3523)  loss_scale: 16384.0000 (23611.8273)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1227  max mem: 15572
Epoch: [27]  [1190/1404]  eta: 0:02:06  lr: 0.000025  min_lr: 0.000000  loss: 4.2375 (4.1365)  class_acc: 0.2917 (0.3520)  loss_scale: 16384.0000 (23551.1402)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1068  max mem: 15572
Epoch: [27]  [1200/1404]  eta: 0:02:00  lr: 0.000025  min_lr: 0.000000  loss: 4.2072 (4.1380)  class_acc: 0.2917 (0.3515)  loss_scale: 16384.0000 (23491.4638)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1121  max mem: 15572
Epoch: [27]  [1210/1404]  eta: 0:01:54  lr: 0.000025  min_lr: 0.000000  loss: 4.1503 (4.1380)  class_acc: 0.2917 (0.3515)  loss_scale: 16384.0000 (23432.7729)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0853  max mem: 15572
Epoch: [27]  [1220/1404]  eta: 0:01:48  lr: 0.000025  min_lr: 0.000000  loss: 4.1056 (4.1379)  class_acc: 0.3333 (0.3516)  loss_scale: 16384.0000 (23375.0434)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0802  max mem: 15572
Epoch: [27]  [1230/1404]  eta: 0:01:42  lr: 0.000025  min_lr: 0.000000  loss: 4.0608 (4.1367)  class_acc: 0.3750 (0.3518)  loss_scale: 16384.0000 (23318.2518)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1151  max mem: 15572
Epoch: [27]  [1240/1404]  eta: 0:01:36  lr: 0.000025  min_lr: 0.000000  loss: 4.1033 (4.1367)  class_acc: 0.4167 (0.3520)  loss_scale: 16384.0000 (23262.3755)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1009  max mem: 15572
Epoch: [27]  [1250/1404]  eta: 0:01:30  lr: 0.000025  min_lr: 0.000000  loss: 4.0854 (4.1355)  class_acc: 0.3750 (0.3521)  loss_scale: 16384.0000 (23207.3925)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0832  max mem: 15572
Epoch: [27]  [1260/1404]  eta: 0:01:24  lr: 0.000025  min_lr: 0.000000  loss: 4.0854 (4.1359)  class_acc: 0.3750 (0.3521)  loss_scale: 16384.0000 (23153.2815)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0936  max mem: 15572
Epoch: [27]  [1270/1404]  eta: 0:01:18  lr: 0.000025  min_lr: 0.000000  loss: 4.2827 (4.1379)  class_acc: 0.2917 (0.3514)  loss_scale: 16384.0000 (23100.0220)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0360  max mem: 15572
Epoch: [27]  [1280/1404]  eta: 0:01:12  lr: 0.000025  min_lr: 0.000000  loss: 4.2732 (4.1378)  class_acc: 0.2917 (0.3514)  loss_scale: 16384.0000 (23047.5941)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0059  max mem: 15572
Epoch: [27]  [1290/1404]  eta: 0:01:06  lr: 0.000025  min_lr: 0.000000  loss: 4.1926 (4.1378)  class_acc: 0.2917 (0.3506)  loss_scale: 16384.0000 (22995.9783)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0689  max mem: 15572
[2025-01-17 03:01:17,012] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:01:17,012] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:01:17,012] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:01:17,013] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [1300/1404]  eta: 0:01:01  lr: 0.000025  min_lr: 0.000000  loss: 4.1794 (4.1372)  class_acc: 0.2917 (0.3505)  loss_scale: 16384.0000 (23071.0899)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1265  max mem: 15572
[2025-01-17 03:01:28,690] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39218
[2025-01-17 03:01:28,693] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:01:28,702] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39218
[2025-01-17 03:01:28,703] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:01:28,703] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [27]  [1310/1404]  eta: 0:00:55  lr: 0.000025  min_lr: 0.000000  loss: 4.0984 (4.1363)  class_acc: 0.3750 (0.3509)  loss_scale: 32768.0000 (23132.5584)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1008  max mem: 15572
Epoch: [27]  [1320/1404]  eta: 0:00:49  lr: 0.000025  min_lr: 0.000000  loss: 4.1235 (4.1363)  class_acc: 0.3333 (0.3506)  loss_scale: 16384.0000 (23081.4716)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0412  max mem: 15572
Epoch: [27]  [1330/1404]  eta: 0:00:43  lr: 0.000025  min_lr: 0.000000  loss: 4.1828 (4.1373)  class_acc: 0.2500 (0.3502)  loss_scale: 16384.0000 (23031.1525)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0009  max mem: 15572
Epoch: [27]  [1340/1404]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000000  loss: 4.1848 (4.1373)  class_acc: 0.3333 (0.3502)  loss_scale: 16384.0000 (22981.5839)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0300  max mem: 15572
Epoch: [27]  [1350/1404]  eta: 0:00:31  lr: 0.000025  min_lr: 0.000000  loss: 4.2426 (4.1385)  class_acc: 0.3333 (0.3497)  loss_scale: 16384.0000 (22932.7491)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0925  max mem: 15572
Epoch: [27]  [1360/1404]  eta: 0:00:25  lr: 0.000025  min_lr: 0.000000  loss: 4.2240 (4.1381)  class_acc: 0.3333 (0.3502)  loss_scale: 16384.0000 (22884.6319)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0867  max mem: 15572
Epoch: [27]  [1370/1404]  eta: 0:00:19  lr: 0.000025  min_lr: 0.000000  loss: 4.1273 (4.1375)  class_acc: 0.3333 (0.3500)  loss_scale: 16384.0000 (22837.2166)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0711  max mem: 15572
Epoch: [27]  [1380/1404]  eta: 0:00:14  lr: 0.000025  min_lr: 0.000000  loss: 4.1619 (4.1386)  class_acc: 0.2917 (0.3500)  loss_scale: 16384.0000 (22790.4881)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1101  max mem: 15572
Epoch: [27]  [1390/1404]  eta: 0:00:08  lr: 0.000025  min_lr: 0.000000  loss: 4.2121 (4.1390)  class_acc: 0.3333 (0.3501)  loss_scale: 16384.0000 (22744.4313)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1034  max mem: 15572
Epoch: [27]  [1400/1404]  eta: 0:00:02  lr: 0.000025  min_lr: 0.000000  loss: 4.2121 (4.1398)  class_acc: 0.2917 (0.3494)  loss_scale: 16384.0000 (22699.0321)  weight_decay: 0.0500 (0.0500)  time: 0.4674  data: 0.0404  max mem: 15572
Epoch: [27]  [1403/1404]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000000  loss: 4.1629 (4.1395)  class_acc: 0.3333 (0.3497)  loss_scale: 16384.0000 (22685.5385)  weight_decay: 0.0500 (0.0500)  time: 0.4547  data: 0.0404  max mem: 15572
Epoch: [27] Total time: 0:13:41 (0.5854 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000000  loss: 4.1629 (4.1474)  class_acc: 0.3333 (0.3435)  loss_scale: 16384.0000 (22685.5385)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:11:14  loss: 1.8410 (1.8410)  acc1: 61.1111 (61.1111)  acc5: 83.3333 (83.3333)  time: 4.9631  data: 4.6287  max mem: 15572
Val:  [ 10/136]  eta: 0:01:33  loss: 2.3654 (2.3011)  acc1: 55.5556 (47.9798)  acc5: 83.3333 (81.3131)  time: 0.7441  data: 0.5395  max mem: 15572
Val:  [ 20/136]  eta: 0:01:03  loss: 2.4449 (2.4213)  acc1: 44.4444 (45.5026)  acc5: 72.2222 (76.9841)  time: 0.3244  data: 0.1358  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 2.2952 (2.3483)  acc1: 44.4444 (46.5950)  acc5: 83.3333 (78.4946)  time: 0.3326  data: 0.1432  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.0382 (2.2978)  acc1: 55.5556 (48.7805)  acc5: 83.3333 (79.6748)  time: 0.3847  data: 0.1945  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.1734 (2.3076)  acc1: 50.0000 (48.2571)  acc5: 83.3333 (79.9564)  time: 0.3809  data: 0.1953  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.3046 (2.3737)  acc1: 44.4444 (45.8106)  acc5: 77.7778 (78.5064)  time: 0.3507  data: 0.1661  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.3714 (2.3613)  acc1: 44.4444 (46.7136)  acc5: 77.7778 (78.7950)  time: 0.3666  data: 0.1703  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.2534 (2.3546)  acc1: 50.0000 (46.5706)  acc5: 83.3333 (79.2181)  time: 0.3312  data: 0.1312  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.2636 (2.3579)  acc1: 44.4444 (45.9096)  acc5: 77.7778 (78.9988)  time: 0.3442  data: 0.1488  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5959 (2.4099)  acc1: 38.8889 (44.1694)  acc5: 72.2222 (77.5578)  time: 0.3947  data: 0.2069  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.5764 (2.4022)  acc1: 38.8889 (44.3944)  acc5: 77.7778 (77.5275)  time: 0.3881  data: 0.2114  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1100 (2.3620)  acc1: 50.0000 (45.8219)  acc5: 83.3333 (78.5124)  time: 0.3488  data: 0.1648  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8506 (2.3261)  acc1: 55.5556 (47.0314)  acc5: 94.4444 (79.3893)  time: 0.2354  data: 0.0651  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.9673 (2.3257)  acc1: 55.5556 (47.2563)  acc5: 88.8889 (79.4840)  time: 0.2151  data: 0.0649  max mem: 15572
Val: Total time: 0:00:49 (0.3673 s / it)
* Acc@1 46.192 Acc@5 78.296 loss 2.371
Accuracy of the network on the 4883 val videos: 46.2%
Max accuracy: 46.25%
Epoch: [28]  [   0/1404]  eta: 2:58:10  lr: 0.000025  min_lr: 0.000000  loss: 4.0833 (4.0833)  class_acc: 0.5417 (0.5417)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.6142  data: 6.4857  max mem: 15572
Epoch: [28]  [  10/1404]  eta: 0:28:27  lr: 0.000025  min_lr: 0.000000  loss: 4.1861 (4.1919)  class_acc: 0.3750 (0.3295)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2250  data: 0.6736  max mem: 15572
Epoch: [28]  [  20/1404]  eta: 0:21:14  lr: 0.000025  min_lr: 0.000000  loss: 4.1049 (4.1009)  class_acc: 0.3750 (0.3631)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1033  max mem: 15572
Epoch: [28]  [  30/1404]  eta: 0:18:56  lr: 0.000025  min_lr: 0.000000  loss: 4.0672 (4.0897)  class_acc: 0.3750 (0.3683)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1035  max mem: 15572
[2025-01-17 03:03:37,312] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:03:37,312] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:03:37,353] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:03:37,354] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [  40/1404]  eta: 0:17:07  lr: 0.000025  min_lr: 0.000000  loss: 4.1028 (4.0967)  class_acc: 0.2917 (0.3567)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0721  max mem: 15572
Epoch: [28]  [  50/1404]  eta: 0:16:28  lr: 0.000025  min_lr: 0.000000  loss: 4.1690 (4.1086)  class_acc: 0.2917 (0.3570)  loss_scale: 32768.0000 (21524.0784)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0941  max mem: 15572
[2025-01-17 03:03:49,483] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39368
[2025-01-17 03:03:49,484] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:03:49,484] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:03:49,505] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39368
[2025-01-17 03:03:49,506] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [28]  [  60/1404]  eta: 0:15:29  lr: 0.000025  min_lr: 0.000000  loss: 4.3060 (4.1474)  class_acc: 0.3333 (0.3572)  loss_scale: 32768.0000 (22024.3934)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0746  max mem: 15572
Epoch: [28]  [  70/1404]  eta: 0:14:59  lr: 0.000025  min_lr: 0.000000  loss: 4.2602 (4.1487)  class_acc: 0.3333 (0.3521)  loss_scale: 16384.0000 (21229.9718)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0513  max mem: 15572
Epoch: [28]  [  80/1404]  eta: 0:14:39  lr: 0.000025  min_lr: 0.000000  loss: 4.0861 (4.1379)  class_acc: 0.3333 (0.3513)  loss_scale: 16384.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1057  max mem: 15572
Epoch: [28]  [  90/1404]  eta: 0:14:20  lr: 0.000024  min_lr: 0.000000  loss: 4.0867 (4.1428)  class_acc: 0.2500 (0.3393)  loss_scale: 16384.0000 (20164.9231)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1132  max mem: 15572
Epoch: [28]  [ 100/1404]  eta: 0:13:58  lr: 0.000024  min_lr: 0.000000  loss: 4.1836 (4.1511)  class_acc: 0.2083 (0.3321)  loss_scale: 16384.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0779  max mem: 15572
Epoch: [28]  [ 110/1404]  eta: 0:13:49  lr: 0.000024  min_lr: 0.000000  loss: 4.2023 (4.1490)  class_acc: 0.2500 (0.3307)  loss_scale: 16384.0000 (19483.6757)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0796  max mem: 15572
Epoch: [28]  [ 120/1404]  eta: 0:13:33  lr: 0.000024  min_lr: 0.000000  loss: 4.1764 (4.1595)  class_acc: 0.2917 (0.3326)  loss_scale: 16384.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0659  max mem: 15572
Epoch: [28]  [ 130/1404]  eta: 0:13:27  lr: 0.000024  min_lr: 0.000000  loss: 4.1229 (4.1545)  class_acc: 0.3333 (0.3346)  loss_scale: 16384.0000 (19010.4427)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0658  max mem: 15572
Epoch: [28]  [ 140/1404]  eta: 0:13:15  lr: 0.000024  min_lr: 0.000000  loss: 4.1971 (4.1611)  class_acc: 0.3333 (0.3324)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0906  max mem: 15572
Epoch: [28]  [ 150/1404]  eta: 0:13:04  lr: 0.000024  min_lr: 0.000000  loss: 4.2941 (4.1653)  class_acc: 0.2917 (0.3331)  loss_scale: 16384.0000 (18662.5695)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0767  max mem: 15572
Epoch: [28]  [ 160/1404]  eta: 0:12:48  lr: 0.000024  min_lr: 0.000000  loss: 4.1205 (4.1622)  class_acc: 0.2917 (0.3323)  loss_scale: 16384.0000 (18521.0435)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0479  max mem: 15572
Epoch: [28]  [ 170/1404]  eta: 0:12:39  lr: 0.000024  min_lr: 0.000000  loss: 4.1205 (4.1668)  class_acc: 0.2917 (0.3321)  loss_scale: 16384.0000 (18396.0702)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0552  max mem: 15572
Epoch: [28]  [ 180/1404]  eta: 0:12:37  lr: 0.000024  min_lr: 0.000000  loss: 4.1539 (4.1627)  class_acc: 0.2917 (0.3329)  loss_scale: 16384.0000 (18284.9061)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1337  max mem: 15572
[2025-01-17 03:05:04,107] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:05:04,107] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:05:04,107] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:05:04,108] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [ 190/1404]  eta: 0:12:31  lr: 0.000024  min_lr: 0.000000  loss: 4.1539 (4.1665)  class_acc: 0.2917 (0.3325)  loss_scale: 16384.0000 (18700.0628)  weight_decay: 0.0500 (0.0500)  time: 0.6550  data: 0.1465  max mem: 15572
Epoch: [28]  [ 200/1404]  eta: 0:12:19  lr: 0.000024  min_lr: 0.000000  loss: 4.2902 (4.1750)  class_acc: 0.2917 (0.3323)  loss_scale: 32768.0000 (19399.9602)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0861  max mem: 15572
Epoch: [28]  [ 210/1404]  eta: 0:12:12  lr: 0.000024  min_lr: 0.000000  loss: 4.2421 (4.1700)  class_acc: 0.3750 (0.3329)  loss_scale: 32768.0000 (20033.5166)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0675  max mem: 15572
Epoch: [28]  [ 220/1404]  eta: 0:11:58  lr: 0.000024  min_lr: 0.000000  loss: 4.1564 (4.1677)  class_acc: 0.3333 (0.3335)  loss_scale: 32768.0000 (20609.7376)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0419  max mem: 15572
[2025-01-17 03:05:24,192] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39533
[2025-01-17 03:05:24,192] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:05:24,192] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39533
[2025-01-17 03:05:24,192] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:05:24,192] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [28]  [ 230/1404]  eta: 0:11:52  lr: 0.000024  min_lr: 0.000000  loss: 4.1564 (4.1676)  class_acc: 0.3333 (0.3330)  loss_scale: 16384.0000 (20426.8052)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0613  max mem: 15572
Epoch: [28]  [ 240/1404]  eta: 0:11:46  lr: 0.000024  min_lr: 0.000000  loss: 4.0125 (4.1617)  class_acc: 0.3750 (0.3339)  loss_scale: 16384.0000 (20259.0539)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1105  max mem: 15572
Epoch: [28]  [ 250/1404]  eta: 0:11:38  lr: 0.000024  min_lr: 0.000000  loss: 3.9985 (4.1543)  class_acc: 0.3750 (0.3355)  loss_scale: 16384.0000 (20104.6693)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0759  max mem: 15572
Epoch: [28]  [ 260/1404]  eta: 0:11:30  lr: 0.000024  min_lr: 0.000000  loss: 4.0747 (4.1535)  class_acc: 0.3333 (0.3351)  loss_scale: 16384.0000 (19962.1149)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0605  max mem: 15572
Epoch: [28]  [ 270/1404]  eta: 0:11:26  lr: 0.000024  min_lr: 0.000000  loss: 4.1074 (4.1599)  class_acc: 0.2917 (0.3336)  loss_scale: 16384.0000 (19830.0812)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1140  max mem: 15572
Epoch: [28]  [ 280/1404]  eta: 0:11:19  lr: 0.000024  min_lr: 0.000000  loss: 4.2585 (4.1618)  class_acc: 0.2500 (0.3319)  loss_scale: 16384.0000 (19707.4448)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1060  max mem: 15572
Epoch: [28]  [ 290/1404]  eta: 0:11:12  lr: 0.000024  min_lr: 0.000000  loss: 4.1755 (4.1605)  class_acc: 0.2500 (0.3309)  loss_scale: 16384.0000 (19593.2371)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0673  max mem: 15572
Epoch: [28]  [ 300/1404]  eta: 0:11:08  lr: 0.000024  min_lr: 0.000000  loss: 4.1383 (4.1621)  class_acc: 0.3750 (0.3340)  loss_scale: 16384.0000 (19486.6179)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1232  max mem: 15572
Epoch: [28]  [ 310/1404]  eta: 0:11:00  lr: 0.000024  min_lr: 0.000000  loss: 4.3145 (4.1669)  class_acc: 0.3750 (0.3321)  loss_scale: 16384.0000 (19386.8553)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1220  max mem: 15572
Epoch: [28]  [ 320/1404]  eta: 0:10:56  lr: 0.000024  min_lr: 0.000000  loss: 4.3089 (4.1702)  class_acc: 0.2917 (0.3315)  loss_scale: 16384.0000 (19293.3084)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1310  max mem: 15572
Epoch: [28]  [ 330/1404]  eta: 0:10:47  lr: 0.000024  min_lr: 0.000000  loss: 4.1812 (4.1694)  class_acc: 0.3333 (0.3330)  loss_scale: 16384.0000 (19205.4139)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1097  max mem: 15572
Epoch: [28]  [ 340/1404]  eta: 0:10:43  lr: 0.000024  min_lr: 0.000000  loss: 4.2772 (4.1730)  class_acc: 0.3333 (0.3325)  loss_scale: 16384.0000 (19122.6745)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0737  max mem: 15572
[2025-01-17 03:06:40,888] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:06:40,889] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:06:40,947] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:06:40,947] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [ 350/1404]  eta: 0:10:34  lr: 0.000024  min_lr: 0.000000  loss: 4.1786 (4.1712)  class_acc: 0.3333 (0.3342)  loss_scale: 16384.0000 (19091.3276)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0785  max mem: 15572
[2025-01-17 03:06:42,814] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39666
[2025-01-17 03:06:42,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:06:42,815] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:06:42,816] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39666
[2025-01-17 03:06:42,816] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [28]  [ 360/1404]  eta: 0:10:28  lr: 0.000024  min_lr: 0.000000  loss: 4.0774 (4.1687)  class_acc: 0.3750 (0.3339)  loss_scale: 16384.0000 (19152.4875)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0888  max mem: 15572
Epoch: [28]  [ 370/1404]  eta: 0:10:20  lr: 0.000024  min_lr: 0.000000  loss: 4.2011 (4.1688)  class_acc: 0.2917 (0.3337)  loss_scale: 16384.0000 (19077.8652)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0762  max mem: 15572
Epoch: [28]  [ 380/1404]  eta: 0:10:11  lr: 0.000024  min_lr: 0.000000  loss: 4.2070 (4.1705)  class_acc: 0.3333 (0.3339)  loss_scale: 16384.0000 (19007.1601)  weight_decay: 0.0500 (0.0500)  time: 0.5093  data: 0.0207  max mem: 15572
Epoch: [28]  [ 390/1404]  eta: 0:10:05  lr: 0.000024  min_lr: 0.000000  loss: 4.2636 (4.1727)  class_acc: 0.3750 (0.3346)  loss_scale: 16384.0000 (18940.0716)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0681  max mem: 15572
Epoch: [28]  [ 400/1404]  eta: 0:09:59  lr: 0.000024  min_lr: 0.000000  loss: 4.3344 (4.1725)  class_acc: 0.3750 (0.3365)  loss_scale: 16384.0000 (18876.3292)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0584  max mem: 15572
Epoch: [28]  [ 410/1404]  eta: 0:09:53  lr: 0.000024  min_lr: 0.000000  loss: 4.3904 (4.1780)  class_acc: 0.3333 (0.3347)  loss_scale: 16384.0000 (18815.6886)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0005  max mem: 15572
Epoch: [28]  [ 420/1404]  eta: 0:09:46  lr: 0.000024  min_lr: 0.000000  loss: 4.3405 (4.1801)  class_acc: 0.2500 (0.3334)  loss_scale: 16384.0000 (18757.9287)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0006  max mem: 15572
Epoch: [28]  [ 430/1404]  eta: 0:09:40  lr: 0.000024  min_lr: 0.000000  loss: 4.2287 (4.1789)  class_acc: 0.2917 (0.3328)  loss_scale: 16384.0000 (18702.8492)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0158  max mem: 15572
Epoch: [28]  [ 440/1404]  eta: 0:09:34  lr: 0.000024  min_lr: 0.000000  loss: 4.1479 (4.1786)  class_acc: 0.3333 (0.3330)  loss_scale: 16384.0000 (18650.2676)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0429  max mem: 15572
Epoch: [28]  [ 450/1404]  eta: 0:09:29  lr: 0.000024  min_lr: 0.000000  loss: 4.1314 (4.1771)  class_acc: 0.2917 (0.3326)  loss_scale: 16384.0000 (18600.0177)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0392  max mem: 15572
Epoch: [28]  [ 460/1404]  eta: 0:09:20  lr: 0.000024  min_lr: 0.000000  loss: 4.0773 (4.1715)  class_acc: 0.3333 (0.3344)  loss_scale: 16384.0000 (18551.9479)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0119  max mem: 15572
Epoch: [28]  [ 470/1404]  eta: 0:09:15  lr: 0.000024  min_lr: 0.000000  loss: 3.9673 (4.1694)  class_acc: 0.4167 (0.3341)  loss_scale: 16384.0000 (18505.9193)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0318  max mem: 15572
Epoch: [28]  [ 480/1404]  eta: 0:09:09  lr: 0.000023  min_lr: 0.000000  loss: 4.1723 (4.1710)  class_acc: 0.4167 (0.3371)  loss_scale: 16384.0000 (18461.8046)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.0774  max mem: 15572
[2025-01-17 03:07:57,958] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:07:57,958] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:07:57,960] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:07:57,960] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [ 490/1404]  eta: 0:09:04  lr: 0.000023  min_lr: 0.000000  loss: 4.2330 (4.1712)  class_acc: 0.3750 (0.3369)  loss_scale: 16384.0000 (18686.4358)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0855  max mem: 15572
Epoch: [28]  [ 500/1404]  eta: 0:08:57  lr: 0.000023  min_lr: 0.000000  loss: 4.1809 (4.1711)  class_acc: 0.2917 (0.3364)  loss_scale: 32768.0000 (18967.5050)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0400  max mem: 15572
Epoch: [28]  [ 510/1404]  eta: 0:08:51  lr: 0.000023  min_lr: 0.000000  loss: 4.1622 (4.1703)  class_acc: 0.2917 (0.3371)  loss_scale: 32768.0000 (19237.5734)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0412  max mem: 15572
Epoch: [28]  [ 520/1404]  eta: 0:08:45  lr: 0.000023  min_lr: 0.000000  loss: 4.2188 (4.1701)  class_acc: 0.3750 (0.3374)  loss_scale: 32768.0000 (19497.2745)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0738  max mem: 15572
Epoch: [28]  [ 530/1404]  eta: 0:08:40  lr: 0.000023  min_lr: 0.000000  loss: 4.1744 (4.1697)  class_acc: 0.3750 (0.3384)  loss_scale: 32768.0000 (19747.1940)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.0930  max mem: 15572
[2025-01-17 03:08:26,188] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39843
[2025-01-17 03:08:26,188] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:08:26,199] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39843
[2025-01-17 03:08:26,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:08:26,200] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [28]  [ 540/1404]  eta: 0:08:35  lr: 0.000023  min_lr: 0.000000  loss: 4.0947 (4.1708)  class_acc: 0.4167 (0.3408)  loss_scale: 16384.0000 (19685.0277)  weight_decay: 0.0500 (0.0500)  time: 0.6647  data: 0.1369  max mem: 15572
Epoch: [28]  [ 550/1404]  eta: 0:08:29  lr: 0.000023  min_lr: 0.000000  loss: 4.1966 (4.1722)  class_acc: 0.3333 (0.3404)  loss_scale: 16384.0000 (19625.1180)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1133  max mem: 15572
Epoch: [28]  [ 560/1404]  eta: 0:08:22  lr: 0.000023  min_lr: 0.000000  loss: 4.1779 (4.1703)  class_acc: 0.3333 (0.3405)  loss_scale: 16384.0000 (19567.3440)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0555  max mem: 15572
Epoch: [28]  [ 570/1404]  eta: 0:08:16  lr: 0.000023  min_lr: 0.000000  loss: 3.9572 (4.1689)  class_acc: 0.3333 (0.3422)  loss_scale: 16384.0000 (19511.5937)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0875  max mem: 15572
Epoch: [28]  [ 580/1404]  eta: 0:08:09  lr: 0.000023  min_lr: 0.000000  loss: 4.1855 (4.1700)  class_acc: 0.4167 (0.3422)  loss_scale: 16384.0000 (19457.7625)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0874  max mem: 15572
Epoch: [28]  [ 590/1404]  eta: 0:08:02  lr: 0.000023  min_lr: 0.000000  loss: 4.1455 (4.1650)  class_acc: 0.3750 (0.3427)  loss_scale: 16384.0000 (19405.7530)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0301  max mem: 15572
Epoch: [28]  [ 600/1404]  eta: 0:07:56  lr: 0.000023  min_lr: 0.000000  loss: 4.1840 (4.1657)  class_acc: 0.3750 (0.3423)  loss_scale: 16384.0000 (19355.4742)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0302  max mem: 15572
Epoch: [28]  [ 610/1404]  eta: 0:07:50  lr: 0.000023  min_lr: 0.000000  loss: 4.2339 (4.1664)  class_acc: 0.3333 (0.3427)  loss_scale: 16384.0000 (19306.8412)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0196  max mem: 15572
Epoch: [28]  [ 620/1404]  eta: 0:07:44  lr: 0.000023  min_lr: 0.000000  loss: 4.1985 (4.1644)  class_acc: 0.3750 (0.3437)  loss_scale: 16384.0000 (19259.7746)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.0007  max mem: 15572
Epoch: [28]  [ 630/1404]  eta: 0:07:38  lr: 0.000023  min_lr: 0.000000  loss: 4.1857 (4.1671)  class_acc: 0.3333 (0.3422)  loss_scale: 16384.0000 (19214.1997)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0354  max mem: 15572
Epoch: [28]  [ 640/1404]  eta: 0:07:32  lr: 0.000023  min_lr: 0.000000  loss: 4.2064 (4.1671)  class_acc: 0.2917 (0.3419)  loss_scale: 16384.0000 (19170.0468)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0505  max mem: 15572
Epoch: [28]  [ 650/1404]  eta: 0:07:26  lr: 0.000023  min_lr: 0.000000  loss: 4.1495 (4.1677)  class_acc: 0.3333 (0.3425)  loss_scale: 16384.0000 (19127.2504)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0159  max mem: 15572
[2025-01-17 03:09:40,904] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:09:40,905] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:09:40,922] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:09:40,922] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [ 660/1404]  eta: 0:07:20  lr: 0.000023  min_lr: 0.000000  loss: 4.0736 (4.1669)  class_acc: 0.3333 (0.3420)  loss_scale: 16384.0000 (19110.5356)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0247  max mem: 15572
Epoch: [28]  [ 670/1404]  eta: 0:07:13  lr: 0.000023  min_lr: 0.000000  loss: 4.2760 (4.1678)  class_acc: 0.2917 (0.3408)  loss_scale: 32768.0000 (19314.0745)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0400  max mem: 15572
Epoch: [28]  [ 680/1404]  eta: 0:07:08  lr: 0.000023  min_lr: 0.000000  loss: 4.2778 (4.1701)  class_acc: 0.2083 (0.3398)  loss_scale: 32768.0000 (19511.6358)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0161  max mem: 15572
[2025-01-17 03:09:56,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=40000, skipped=237, lr=[2.2228591471062704e-07, 2.2228591471062704e-07, 3.175513067294672e-07, 3.175513067294672e-07, 4.5364472389923893e-07, 4.5364472389923893e-07, 6.480638912846271e-07, 6.480638912846271e-07, 9.258055589780387e-07, 9.258055589780387e-07, 1.3225793699686267e-06, 1.3225793699686267e-06, 1.8893990999551814e-06, 1.8893990999551814e-06, 2.699141571364545e-06, 2.699141571364545e-06, 3.855916530520778e-06, 3.855916530520778e-06, 5.508452186458256e-06, 5.508452186458256e-06, 7.86921740922608e-06, 7.86921740922608e-06, 1.1241739156037258e-05, 1.1241739156037258e-05, 1.605962736576751e-05, 1.605962736576751e-05, 2.2942324808239304e-05, 2.2942324808239304e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 03:09:56,172] [INFO] [timer.py:260:stop] epoch=0/micro_step=40000/global_step=40000, RunningAvgSamplesPerSec=47.979727507919726, CurrSamplesPerSec=45.77845427056791, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-17 03:09:57,761] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40002
[2025-01-17 03:09:57,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:09:57,761] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [28]  [ 690/1404]  eta: 0:07:01  lr: 0.000023  min_lr: 0.000000  loss: 4.1844 (4.1688)  class_acc: 0.2917 (0.3404)  loss_scale: 32768.0000 (19679.7685)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0007  max mem: 15572
[2025-01-17 03:09:57,785] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40002
[2025-01-17 03:09:57,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [28]  [ 700/1404]  eta: 0:06:55  lr: 0.000023  min_lr: 0.000000  loss: 4.0385 (4.1679)  class_acc: 0.3333 (0.3409)  loss_scale: 16384.0000 (19632.7532)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0008  max mem: 15572
Epoch: [28]  [ 710/1404]  eta: 0:06:50  lr: 0.000023  min_lr: 0.000000  loss: 4.0385 (4.1656)  class_acc: 0.3333 (0.3419)  loss_scale: 16384.0000 (19587.0605)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0250  max mem: 15572
Epoch: [28]  [ 720/1404]  eta: 0:06:44  lr: 0.000023  min_lr: 0.000000  loss: 3.9455 (4.1623)  class_acc: 0.3750 (0.3437)  loss_scale: 16384.0000 (19542.6352)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.0360  max mem: 15572
Epoch: [28]  [ 730/1404]  eta: 0:06:38  lr: 0.000023  min_lr: 0.000000  loss: 3.9373 (4.1591)  class_acc: 0.3750 (0.3439)  loss_scale: 16384.0000 (19499.4254)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.0330  max mem: 15572
Epoch: [28]  [ 740/1404]  eta: 0:06:32  lr: 0.000023  min_lr: 0.000000  loss: 3.9356 (4.1569)  class_acc: 0.3750 (0.3451)  loss_scale: 16384.0000 (19457.3819)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.0744  max mem: 15572
Epoch: [28]  [ 750/1404]  eta: 0:06:26  lr: 0.000023  min_lr: 0.000000  loss: 4.1146 (4.1578)  class_acc: 0.4167 (0.3453)  loss_scale: 16384.0000 (19416.4581)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.0936  max mem: 15572
Epoch: [28]  [ 760/1404]  eta: 0:06:20  lr: 0.000023  min_lr: 0.000000  loss: 4.2005 (4.1576)  class_acc: 0.3750 (0.3459)  loss_scale: 16384.0000 (19376.6097)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0479  max mem: 15572
Epoch: [28]  [ 770/1404]  eta: 0:06:13  lr: 0.000023  min_lr: 0.000000  loss: 4.1663 (4.1592)  class_acc: 0.3750 (0.3461)  loss_scale: 16384.0000 (19337.7951)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0490  max mem: 15572
Epoch: [28]  [ 780/1404]  eta: 0:06:07  lr: 0.000023  min_lr: 0.000000  loss: 4.2571 (4.1602)  class_acc: 0.3333 (0.3456)  loss_scale: 16384.0000 (19299.9744)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0945  max mem: 15572
Epoch: [28]  [ 790/1404]  eta: 0:06:02  lr: 0.000023  min_lr: 0.000000  loss: 4.1616 (4.1584)  class_acc: 0.3333 (0.3458)  loss_scale: 16384.0000 (19263.1100)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1258  max mem: 15572
Epoch: [28]  [ 800/1404]  eta: 0:05:56  lr: 0.000023  min_lr: 0.000000  loss: 4.0793 (4.1575)  class_acc: 0.2500 (0.3450)  loss_scale: 16384.0000 (19227.1660)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.0736  max mem: 15572
Epoch: [28]  [ 810/1404]  eta: 0:05:50  lr: 0.000023  min_lr: 0.000000  loss: 4.1370 (4.1552)  class_acc: 0.2500 (0.3443)  loss_scale: 16384.0000 (19192.1085)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0008  max mem: 15572
[2025-01-17 03:11:14,371] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:11:14,371] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:11:14,373] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:11:14,373] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [ 820/1404]  eta: 0:05:45  lr: 0.000023  min_lr: 0.000000  loss: 4.1529 (4.1555)  class_acc: 0.2500 (0.3435)  loss_scale: 16384.0000 (19197.8173)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.0007  max mem: 15572
Epoch: [28]  [ 830/1404]  eta: 0:05:38  lr: 0.000023  min_lr: 0.000000  loss: 4.1996 (4.1549)  class_acc: 0.2500 (0.3431)  loss_scale: 32768.0000 (19361.1167)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0006  max mem: 15572
Epoch: [28]  [ 840/1404]  eta: 0:05:33  lr: 0.000023  min_lr: 0.000000  loss: 4.0912 (4.1536)  class_acc: 0.2917 (0.3428)  loss_scale: 32768.0000 (19520.5327)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0006  max mem: 15572
Epoch: [28]  [ 850/1404]  eta: 0:05:26  lr: 0.000023  min_lr: 0.000000  loss: 4.0913 (4.1526)  class_acc: 0.2500 (0.3425)  loss_scale: 32768.0000 (19676.2021)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0007  max mem: 15572
Epoch: [28]  [ 860/1404]  eta: 0:05:21  lr: 0.000022  min_lr: 0.000000  loss: 4.1425 (4.1534)  class_acc: 0.2500 (0.3419)  loss_scale: 32768.0000 (19828.2555)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.0007  max mem: 15572
Epoch: [28]  [ 870/1404]  eta: 0:05:15  lr: 0.000022  min_lr: 0.000000  loss: 4.2155 (4.1541)  class_acc: 0.3333 (0.3422)  loss_scale: 32768.0000 (19976.8175)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0008  max mem: 15572
Epoch: [28]  [ 880/1404]  eta: 0:05:08  lr: 0.000022  min_lr: 0.000000  loss: 4.0517 (4.1519)  class_acc: 0.3750 (0.3426)  loss_scale: 32768.0000 (20122.0068)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0008  max mem: 15572
Epoch: [28]  [ 890/1404]  eta: 0:05:03  lr: 0.000022  min_lr: 0.000000  loss: 4.0465 (4.1515)  class_acc: 0.3333 (0.3427)  loss_scale: 32768.0000 (20263.9371)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.0007  max mem: 15572
Epoch: [28]  [ 900/1404]  eta: 0:04:57  lr: 0.000022  min_lr: 0.000000  loss: 4.0802 (4.1508)  class_acc: 0.3750 (0.3431)  loss_scale: 32768.0000 (20402.7170)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.0008  max mem: 15572
Epoch: [28]  [ 910/1404]  eta: 0:04:51  lr: 0.000022  min_lr: 0.000000  loss: 4.1254 (4.1515)  class_acc: 0.3750 (0.3428)  loss_scale: 32768.0000 (20538.4501)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0009  max mem: 15572
Epoch: [28]  [ 920/1404]  eta: 0:04:45  lr: 0.000022  min_lr: 0.000000  loss: 4.1479 (4.1517)  class_acc: 0.2917 (0.3421)  loss_scale: 32768.0000 (20671.2356)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.0007  max mem: 15572
Epoch: [28]  [ 930/1404]  eta: 0:04:39  lr: 0.000022  min_lr: 0.000000  loss: 4.0930 (4.1493)  class_acc: 0.2917 (0.3425)  loss_scale: 32768.0000 (20801.1686)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0006  max mem: 15572
Epoch: [28]  [ 940/1404]  eta: 0:04:33  lr: 0.000022  min_lr: 0.000000  loss: 3.9261 (4.1482)  class_acc: 0.3333 (0.3426)  loss_scale: 32768.0000 (20928.3401)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0009  max mem: 15572
[2025-01-17 03:12:29,394] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:12:29,394] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:12:29,394] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 03:12:29,394] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 03:12:29,857] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40260
[2025-01-17 03:12:29,857] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 03:12:29,892] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40260
[2025-01-17 03:12:29,892] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 03:12:29,892] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 950/1404]  eta: 0:04:27  lr: 0.000022  min_lr: 0.000000  loss: 4.2421 (4.1506)  class_acc: 0.3333 (0.3424)  loss_scale: 32768.0000 (21087.2934)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.0008  max mem: 15572
[2025-01-17 03:12:33,966] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40267
[2025-01-17 03:12:33,966] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40267
[2025-01-17 03:12:33,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:12:33,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:12:33,967] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [28]  [ 960/1404]  eta: 0:04:21  lr: 0.000022  min_lr: 0.000000  loss: 4.3696 (4.1512)  class_acc: 0.2917 (0.3421)  loss_scale: 32768.0000 (21106.5473)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.0005  max mem: 15572
Epoch: [28]  [ 970/1404]  eta: 0:04:15  lr: 0.000022  min_lr: 0.000000  loss: 4.1801 (4.1520)  class_acc: 0.2917 (0.3417)  loss_scale: 16384.0000 (21057.9114)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0411  max mem: 15572
Epoch: [28]  [ 980/1404]  eta: 0:04:09  lr: 0.000022  min_lr: 0.000000  loss: 4.2592 (4.1544)  class_acc: 0.2500 (0.3412)  loss_scale: 16384.0000 (21010.2671)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0638  max mem: 15572
Epoch: [28]  [ 990/1404]  eta: 0:04:04  lr: 0.000022  min_lr: 0.000000  loss: 4.2296 (4.1542)  class_acc: 0.2917 (0.3412)  loss_scale: 16384.0000 (20963.5843)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0964  max mem: 15572
Epoch: [28]  [1000/1404]  eta: 0:03:58  lr: 0.000022  min_lr: 0.000000  loss: 4.1500 (4.1540)  class_acc: 0.2917 (0.3410)  loss_scale: 16384.0000 (20917.8342)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0896  max mem: 15572
Epoch: [28]  [1010/1404]  eta: 0:03:52  lr: 0.000022  min_lr: 0.000000  loss: 4.1500 (4.1534)  class_acc: 0.3750 (0.3417)  loss_scale: 16384.0000 (20872.9891)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0697  max mem: 15572
Epoch: [28]  [1020/1404]  eta: 0:03:46  lr: 0.000022  min_lr: 0.000000  loss: 4.1878 (4.1536)  class_acc: 0.3750 (0.3415)  loss_scale: 16384.0000 (20829.0225)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0890  max mem: 15572
Epoch: [28]  [1030/1404]  eta: 0:03:40  lr: 0.000022  min_lr: 0.000000  loss: 4.2058 (4.1543)  class_acc: 0.3333 (0.3419)  loss_scale: 16384.0000 (20785.9088)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0491  max mem: 15572
Epoch: [28]  [1040/1404]  eta: 0:03:34  lr: 0.000022  min_lr: 0.000000  loss: 4.2058 (4.1553)  class_acc: 0.4167 (0.3424)  loss_scale: 16384.0000 (20743.6234)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0675  max mem: 15572
Epoch: [28]  [1050/1404]  eta: 0:03:28  lr: 0.000022  min_lr: 0.000000  loss: 4.2523 (4.1546)  class_acc: 0.4167 (0.3430)  loss_scale: 16384.0000 (20702.1427)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0723  max mem: 15572
Epoch: [28]  [1060/1404]  eta: 0:03:22  lr: 0.000022  min_lr: 0.000000  loss: 4.1975 (4.1561)  class_acc: 0.3750 (0.3428)  loss_scale: 16384.0000 (20661.4439)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0573  max mem: 15572
Epoch: [28]  [1070/1404]  eta: 0:03:16  lr: 0.000022  min_lr: 0.000000  loss: 4.1875 (4.1557)  class_acc: 0.2917 (0.3424)  loss_scale: 16384.0000 (20621.5051)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.0508  max mem: 15572
Epoch: [28]  [1080/1404]  eta: 0:03:10  lr: 0.000022  min_lr: 0.000000  loss: 4.0686 (4.1558)  class_acc: 0.2917 (0.3427)  loss_scale: 16384.0000 (20582.3053)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0122  max mem: 15572
[2025-01-17 03:13:48,775] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:13:48,775] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:13:48,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:13:48,777] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [1090/1404]  eta: 0:03:04  lr: 0.000022  min_lr: 0.000000  loss: 4.2126 (4.1566)  class_acc: 0.2917 (0.3424)  loss_scale: 16384.0000 (20648.9459)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0008  max mem: 15572
Epoch: [28]  [1100/1404]  eta: 0:02:58  lr: 0.000022  min_lr: 0.000000  loss: 4.1794 (4.1556)  class_acc: 0.2917 (0.3424)  loss_scale: 32768.0000 (20759.0191)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0009  max mem: 15572
Epoch: [28]  [1110/1404]  eta: 0:02:53  lr: 0.000022  min_lr: 0.000000  loss: 4.1531 (4.1566)  class_acc: 0.2917 (0.3425)  loss_scale: 32768.0000 (20867.1107)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0524  max mem: 15572
Epoch: [28]  [1120/1404]  eta: 0:02:47  lr: 0.000022  min_lr: 0.000000  loss: 4.2039 (4.1572)  class_acc: 0.3333 (0.3426)  loss_scale: 32768.0000 (20973.2739)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0522  max mem: 15572
Epoch: [28]  [1130/1404]  eta: 0:02:41  lr: 0.000022  min_lr: 0.000000  loss: 4.1342 (4.1558)  class_acc: 0.3333 (0.3421)  loss_scale: 32768.0000 (21077.5597)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0227  max mem: 15572
Epoch: [28]  [1140/1404]  eta: 0:02:35  lr: 0.000022  min_lr: 0.000000  loss: 4.1451 (4.1560)  class_acc: 0.2917 (0.3424)  loss_scale: 32768.0000 (21180.0175)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.0228  max mem: 15572
[2025-01-17 03:14:23,901] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40456
[2025-01-17 03:14:23,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:14:23,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40456
[2025-01-17 03:14:23,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:14:23,902] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [28]  [1150/1404]  eta: 0:02:29  lr: 0.000022  min_lr: 0.000000  loss: 4.3237 (4.1577)  class_acc: 0.3333 (0.3420)  loss_scale: 32768.0000 (21181.0530)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0008  max mem: 15572
Epoch: [28]  [1160/1404]  eta: 0:02:23  lr: 0.000022  min_lr: 0.000000  loss: 4.1918 (4.1572)  class_acc: 0.2917 (0.3422)  loss_scale: 16384.0000 (21139.7347)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0007  max mem: 15572
Epoch: [28]  [1170/1404]  eta: 0:02:17  lr: 0.000022  min_lr: 0.000000  loss: 4.1459 (4.1582)  class_acc: 0.2917 (0.3419)  loss_scale: 16384.0000 (21099.1221)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0005  max mem: 15572
Epoch: [28]  [1180/1404]  eta: 0:02:11  lr: 0.000022  min_lr: 0.000000  loss: 4.2190 (4.1580)  class_acc: 0.3333 (0.3424)  loss_scale: 16384.0000 (21059.1973)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0005  max mem: 15572
Epoch: [28]  [1190/1404]  eta: 0:02:05  lr: 0.000022  min_lr: 0.000000  loss: 4.0778 (4.1573)  class_acc: 0.3750 (0.3423)  loss_scale: 16384.0000 (21019.9429)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0220  max mem: 15572
Epoch: [28]  [1200/1404]  eta: 0:01:59  lr: 0.000022  min_lr: 0.000000  loss: 4.0570 (4.1573)  class_acc: 0.3333 (0.3423)  loss_scale: 16384.0000 (20981.3422)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0384  max mem: 15572
Epoch: [28]  [1210/1404]  eta: 0:01:53  lr: 0.000022  min_lr: 0.000000  loss: 4.1762 (4.1586)  class_acc: 0.3333 (0.3420)  loss_scale: 16384.0000 (20943.3790)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0170  max mem: 15572
Epoch: [28]  [1220/1404]  eta: 0:01:48  lr: 0.000022  min_lr: 0.000000  loss: 4.0473 (4.1571)  class_acc: 0.2917 (0.3419)  loss_scale: 16384.0000 (20906.0377)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0344  max mem: 15572
Epoch: [28]  [1230/1404]  eta: 0:01:42  lr: 0.000022  min_lr: 0.000000  loss: 3.9975 (4.1559)  class_acc: 0.3750 (0.3423)  loss_scale: 16384.0000 (20869.3030)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0737  max mem: 15572
Epoch: [28]  [1240/1404]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000000  loss: 4.0086 (4.1556)  class_acc: 0.3750 (0.3431)  loss_scale: 16384.0000 (20833.1604)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.0657  max mem: 15572
Epoch: [28]  [1250/1404]  eta: 0:01:30  lr: 0.000022  min_lr: 0.000000  loss: 4.0938 (4.1548)  class_acc: 0.4167 (0.3435)  loss_scale: 16384.0000 (20797.5955)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.0374  max mem: 15572
Epoch: [28]  [1260/1404]  eta: 0:01:24  lr: 0.000021  min_lr: 0.000000  loss: 4.1492 (4.1551)  class_acc: 0.3333 (0.3434)  loss_scale: 16384.0000 (20762.5948)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0205  max mem: 15572
Epoch: [28]  [1270/1404]  eta: 0:01:18  lr: 0.000021  min_lr: 0.000000  loss: 4.2200 (4.1553)  class_acc: 0.3333 (0.3439)  loss_scale: 16384.0000 (20728.1448)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0349  max mem: 15572
[2025-01-17 03:15:38,005] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:15:38,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:15:38,049] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:15:38,050] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [1280/1404]  eta: 0:01:12  lr: 0.000021  min_lr: 0.000000  loss: 4.1688 (4.1554)  class_acc: 0.3333 (0.3434)  loss_scale: 16384.0000 (20796.5527)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.0665  max mem: 15572
Epoch: [28]  [1290/1404]  eta: 0:01:07  lr: 0.000021  min_lr: 0.000000  loss: 4.0625 (4.1554)  class_acc: 0.2500 (0.3432)  loss_scale: 32768.0000 (20889.2827)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1092  max mem: 15572
Epoch: [28]  [1300/1404]  eta: 0:01:01  lr: 0.000021  min_lr: 0.000000  loss: 4.0343 (4.1544)  class_acc: 0.3333 (0.3437)  loss_scale: 32768.0000 (20980.5872)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1083  max mem: 15572
[2025-01-17 03:15:58,330] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40618
[2025-01-17 03:15:58,330] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:15:58,330] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:15:58,331] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40618
[2025-01-17 03:15:58,331] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [28]  [1310/1404]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000000  loss: 4.1874 (4.1551)  class_acc: 0.3333 (0.3434)  loss_scale: 32768.0000 (21008.0122)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0541  max mem: 15572
Epoch: [28]  [1320/1404]  eta: 0:00:49  lr: 0.000021  min_lr: 0.000000  loss: 4.1512 (4.1544)  class_acc: 0.3333 (0.3439)  loss_scale: 16384.0000 (20973.0083)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0147  max mem: 15572
Epoch: [28]  [1330/1404]  eta: 0:00:43  lr: 0.000021  min_lr: 0.000000  loss: 4.1135 (4.1546)  class_acc: 0.3750 (0.3438)  loss_scale: 16384.0000 (20938.5304)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0516  max mem: 15572
Epoch: [28]  [1340/1404]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000000  loss: 4.1551 (4.1549)  class_acc: 0.3333 (0.3436)  loss_scale: 16384.0000 (20904.5667)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0516  max mem: 15572
Epoch: [28]  [1350/1404]  eta: 0:00:31  lr: 0.000021  min_lr: 0.000000  loss: 4.1276 (4.1551)  class_acc: 0.3333 (0.3435)  loss_scale: 16384.0000 (20871.1058)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0009  max mem: 15572
Epoch: [28]  [1360/1404]  eta: 0:00:25  lr: 0.000021  min_lr: 0.000000  loss: 4.2327 (4.1554)  class_acc: 0.2917 (0.3432)  loss_scale: 16384.0000 (20838.1367)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0127  max mem: 15572
Epoch: [28]  [1370/1404]  eta: 0:00:19  lr: 0.000021  min_lr: 0.000000  loss: 4.0698 (4.1541)  class_acc: 0.2917 (0.3428)  loss_scale: 16384.0000 (20805.6484)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0125  max mem: 15572
Epoch: [28]  [1380/1404]  eta: 0:00:14  lr: 0.000021  min_lr: 0.000000  loss: 3.9534 (4.1529)  class_acc: 0.3333 (0.3429)  loss_scale: 16384.0000 (20773.6307)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0009  max mem: 15572
Epoch: [28]  [1390/1404]  eta: 0:00:08  lr: 0.000021  min_lr: 0.000000  loss: 4.1501 (4.1533)  class_acc: 0.3750 (0.3433)  loss_scale: 16384.0000 (20742.0733)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.0019  max mem: 15572
Epoch: [28]  [1400/1404]  eta: 0:00:02  lr: 0.000021  min_lr: 0.000000  loss: 4.1930 (4.1530)  class_acc: 0.3750 (0.3437)  loss_scale: 16384.0000 (20710.9665)  weight_decay: 0.0500 (0.0500)  time: 0.4893  data: 0.0015  max mem: 15572
Epoch: [28]  [1403/1404]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 4.2040 (4.1533)  class_acc: 0.4167 (0.3437)  loss_scale: 16384.0000 (20701.7208)  weight_decay: 0.0500 (0.0500)  time: 0.4168  data: 0.0014  max mem: 15572
Epoch: [28] Total time: 0:13:42 (0.5861 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 4.2040 (4.1606)  class_acc: 0.4167 (0.3443)  loss_scale: 16384.0000 (20701.7208)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:14:34  loss: 1.7053 (1.7053)  acc1: 61.1111 (61.1111)  acc5: 83.3333 (83.3333)  time: 6.4315  data: 6.2589  max mem: 15572
Val:  [ 10/136]  eta: 0:01:42  loss: 2.2891 (2.2202)  acc1: 55.5556 (52.5253)  acc5: 77.7778 (81.8182)  time: 0.8110  data: 0.6214  max mem: 15572
Val:  [ 20/136]  eta: 0:01:03  loss: 2.4995 (2.4092)  acc1: 38.8889 (45.7672)  acc5: 77.7778 (77.7778)  time: 0.2518  data: 0.0656  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 2.3857 (2.3377)  acc1: 38.8889 (46.9534)  acc5: 77.7778 (78.6738)  time: 0.2968  data: 0.1045  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 2.0317 (2.2850)  acc1: 55.5556 (50.1355)  acc5: 83.3333 (79.5393)  time: 0.3445  data: 0.1369  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.0841 (2.2741)  acc1: 55.5556 (50.4357)  acc5: 83.3333 (80.2832)  time: 0.3393  data: 0.1343  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.2628 (2.3457)  acc1: 44.4444 (47.9964)  acc5: 77.7778 (78.7796)  time: 0.3460  data: 0.1352  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.2841 (2.3283)  acc1: 44.4444 (49.0610)  acc5: 77.7778 (79.3427)  time: 0.3508  data: 0.1392  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.2791 (2.3206)  acc1: 44.4444 (48.9026)  acc5: 83.3333 (79.9726)  time: 0.3750  data: 0.1718  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.3350 (2.3301)  acc1: 44.4444 (48.2906)  acc5: 83.3333 (79.7924)  time: 0.3780  data: 0.1688  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.4997 (2.3894)  acc1: 38.8889 (46.0946)  acc5: 77.7778 (78.4929)  time: 0.3412  data: 0.1466  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.5639 (2.3761)  acc1: 38.8889 (46.4965)  acc5: 77.7778 (78.8789)  time: 0.3804  data: 0.1948  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.0764 (2.3379)  acc1: 55.5556 (47.6584)  acc5: 88.8889 (79.5684)  time: 0.3670  data: 0.1779  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8868 (2.3065)  acc1: 55.5556 (48.5157)  acc5: 88.8889 (80.1951)  time: 0.2518  data: 0.0827  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.0385 (2.3047)  acc1: 55.5556 (48.6486)  acc5: 83.3333 (80.3030)  time: 0.1726  data: 0.0191  max mem: 15572
Val: Total time: 0:00:49 (0.3653 s / it)
* Acc@1 47.523 Acc@5 78.788 loss 2.345
Accuracy of the network on the 4883 val videos: 47.5%
[2025-01-17 03:17:41,997] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 03:17:41,998] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 03:17:41,998] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 03:17:41,998] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 03:17:44,476] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 03:17:44,476] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 47.52%
Epoch: [29]  [   0/1404]  eta: 3:13:54  lr: 0.000021  min_lr: 0.000000  loss: 4.0088 (4.0088)  class_acc: 0.2917 (0.2917)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 8.2869  data: 7.6151  max mem: 15572
Epoch: [29]  [  10/1404]  eta: 0:28:57  lr: 0.000021  min_lr: 0.000000  loss: 4.1094 (4.1963)  class_acc: 0.2917 (0.2727)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2466  data: 0.7479  max mem: 15572
Epoch: [29]  [  20/1404]  eta: 0:21:39  lr: 0.000021  min_lr: 0.000000  loss: 4.2456 (4.2361)  class_acc: 0.3333 (0.3234)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0393  max mem: 15572
Epoch: [29]  [  30/1404]  eta: 0:18:10  lr: 0.000021  min_lr: 0.000000  loss: 4.1974 (4.1895)  class_acc: 0.3750 (0.3226)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0090  max mem: 15572
[2025-01-17 03:18:09,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:18:09,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:18:09,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:18:09,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [  40/1404]  eta: 0:16:16  lr: 0.000021  min_lr: 0.000000  loss: 4.0500 (4.1353)  class_acc: 0.3333 (0.3394)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0006  max mem: 15572
Epoch: [29]  [  50/1404]  eta: 0:15:38  lr: 0.000021  min_lr: 0.000000  loss: 4.0500 (4.1603)  class_acc: 0.3750 (0.3382)  loss_scale: 32768.0000 (22809.0980)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0006  max mem: 15572
Epoch: [29]  [  60/1404]  eta: 0:14:57  lr: 0.000021  min_lr: 0.000000  loss: 4.2109 (4.1477)  class_acc: 0.3750 (0.3545)  loss_scale: 32768.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0007  max mem: 15572
Epoch: [29]  [  70/1404]  eta: 0:14:39  lr: 0.000021  min_lr: 0.000000  loss: 4.1568 (4.1511)  class_acc: 0.4167 (0.3627)  loss_scale: 32768.0000 (25614.4225)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0007  max mem: 15572
Epoch: [29]  [  80/1404]  eta: 0:14:17  lr: 0.000021  min_lr: 0.000000  loss: 4.3066 (4.1720)  class_acc: 0.4167 (0.3724)  loss_scale: 32768.0000 (26497.5802)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.0195  max mem: 15572
Epoch: [29]  [  90/1404]  eta: 0:14:10  lr: 0.000021  min_lr: 0.000000  loss: 4.3126 (4.1787)  class_acc: 0.3750 (0.3672)  loss_scale: 32768.0000 (27186.6374)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1030  max mem: 15572
Epoch: [29]  [ 100/1404]  eta: 0:14:09  lr: 0.000021  min_lr: 0.000000  loss: 4.1420 (4.1848)  class_acc: 0.3333 (0.3618)  loss_scale: 32768.0000 (27739.2475)  weight_decay: 0.0500 (0.0500)  time: 0.6658  data: 0.1742  max mem: 15572
Epoch: [29]  [ 110/1404]  eta: 0:13:56  lr: 0.000021  min_lr: 0.000000  loss: 4.0585 (4.1613)  class_acc: 0.3750 (0.3634)  loss_scale: 32768.0000 (28192.2883)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.1343  max mem: 15572
Epoch: [29]  [ 120/1404]  eta: 0:13:40  lr: 0.000021  min_lr: 0.000000  loss: 3.9551 (4.1458)  class_acc: 0.3750 (0.3660)  loss_scale: 32768.0000 (28570.4463)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.0753  max mem: 15572
[2025-01-17 03:19:03,791] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40837
[2025-01-17 03:19:03,791] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40837
[2025-01-17 03:19:03,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:19:03,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:19:03,791] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [ 130/1404]  eta: 0:13:33  lr: 0.000021  min_lr: 0.000000  loss: 4.1420 (4.1437)  class_acc: 0.3750 (0.3709)  loss_scale: 16384.0000 (27640.1832)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1092  max mem: 15572
Epoch: [29]  [ 140/1404]  eta: 0:13:22  lr: 0.000021  min_lr: 0.000000  loss: 4.1286 (4.1421)  class_acc: 0.4167 (0.3723)  loss_scale: 16384.0000 (26841.8723)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1349  max mem: 15572
Epoch: [29]  [ 150/1404]  eta: 0:13:09  lr: 0.000021  min_lr: 0.000000  loss: 4.1887 (4.1516)  class_acc: 0.3750 (0.3670)  loss_scale: 16384.0000 (26149.2980)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0939  max mem: 15572
Epoch: [29]  [ 160/1404]  eta: 0:12:58  lr: 0.000021  min_lr: 0.000000  loss: 4.2877 (4.1539)  class_acc: 0.2917 (0.3631)  loss_scale: 16384.0000 (25542.7578)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0761  max mem: 15572
Epoch: [29]  [ 170/1404]  eta: 0:12:47  lr: 0.000021  min_lr: 0.000000  loss: 3.9484 (4.1456)  class_acc: 0.3750 (0.3662)  loss_scale: 16384.0000 (25007.1579)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0789  max mem: 15572
Epoch: [29]  [ 180/1404]  eta: 0:12:39  lr: 0.000021  min_lr: 0.000000  loss: 3.9244 (4.1345)  class_acc: 0.3333 (0.3600)  loss_scale: 16384.0000 (24530.7403)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1015  max mem: 15572
Epoch: [29]  [ 190/1404]  eta: 0:12:33  lr: 0.000021  min_lr: 0.000000  loss: 4.0958 (4.1397)  class_acc: 0.3333 (0.3621)  loss_scale: 16384.0000 (24104.2094)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1336  max mem: 15572
Epoch: [29]  [ 200/1404]  eta: 0:12:27  lr: 0.000021  min_lr: 0.000000  loss: 4.2048 (4.1394)  class_acc: 0.3333 (0.3588)  loss_scale: 16384.0000 (23720.1194)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1556  max mem: 15572
Epoch: [29]  [ 210/1404]  eta: 0:12:15  lr: 0.000021  min_lr: 0.000000  loss: 4.0279 (4.1397)  class_acc: 0.3333 (0.3612)  loss_scale: 16384.0000 (23372.4360)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0841  max mem: 15572
Epoch: [29]  [ 220/1404]  eta: 0:12:01  lr: 0.000021  min_lr: 0.000000  loss: 4.0279 (4.1315)  class_acc: 0.3750 (0.3614)  loss_scale: 16384.0000 (23056.2172)  weight_decay: 0.0500 (0.0500)  time: 0.4914  data: 0.0005  max mem: 15572
Epoch: [29]  [ 230/1404]  eta: 0:11:55  lr: 0.000021  min_lr: 0.000000  loss: 4.0660 (4.1264)  class_acc: 0.3750 (0.3617)  loss_scale: 16384.0000 (22767.3766)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0467  max mem: 15572
Epoch: [29]  [ 240/1404]  eta: 0:11:46  lr: 0.000021  min_lr: 0.000000  loss: 4.1545 (4.1253)  class_acc: 0.3333 (0.3617)  loss_scale: 16384.0000 (22502.5062)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0753  max mem: 15572
[2025-01-17 03:20:16,647] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:20:16,647] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:20:16,693] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:20:16,694] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [ 250/1404]  eta: 0:11:39  lr: 0.000021  min_lr: 0.000000  loss: 4.2379 (4.1310)  class_acc: 0.3333 (0.3612)  loss_scale: 16384.0000 (22324.0159)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0678  max mem: 15572
Epoch: [29]  [ 260/1404]  eta: 0:11:30  lr: 0.000020  min_lr: 0.000000  loss: 4.3325 (4.1404)  class_acc: 0.3333 (0.3621)  loss_scale: 32768.0000 (22724.1686)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.0728  max mem: 15572
Epoch: [29]  [ 270/1404]  eta: 0:11:24  lr: 0.000020  min_lr: 0.000000  loss: 4.3352 (4.1400)  class_acc: 0.3333 (0.3616)  loss_scale: 32768.0000 (23094.7897)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0796  max mem: 15572
[2025-01-17 03:20:31,345] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40990
[2025-01-17 03:20:31,345] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:20:31,348] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40990
[2025-01-17 03:20:31,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:20:31,348] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [ 280/1404]  eta: 0:11:18  lr: 0.000020  min_lr: 0.000000  loss: 4.1291 (4.1393)  class_acc: 0.3333 (0.3620)  loss_scale: 32768.0000 (23030.8897)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0462  max mem: 15572
[2025-01-17 03:20:35,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=41000, skipped=244, lr=[1.9786263683120527e-07, 1.9786263683120527e-07, 2.826609097588647e-07, 2.826609097588647e-07, 4.038012996555211e-07, 4.038012996555211e-07, 5.768589995078873e-07, 5.768589995078873e-07, 8.240842850112676e-07, 8.240842850112676e-07, 1.1772632643018108e-06, 1.1772632643018108e-06, 1.6818046632883012e-06, 1.6818046632883012e-06, 2.402578090411859e-06, 2.402578090411859e-06, 3.4322544148740844e-06, 3.4322544148740844e-06, 4.903220592677264e-06, 4.903220592677264e-06, 7.004600846681806e-06, 7.004600846681806e-06, 1.0006572638116867e-05, 1.0006572638116867e-05, 1.4295103768738381e-05, 1.4295103768738381e-05, 2.0421576812483404e-05, 2.0421576812483404e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 03:20:35,764] [INFO] [timer.py:260:stop] epoch=0/micro_step=41000/global_step=41000, RunningAvgSamplesPerSec=47.96519572024703, CurrSamplesPerSec=50.72609410778212, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [ 290/1404]  eta: 0:11:14  lr: 0.000020  min_lr: 0.000000  loss: 4.0071 (4.1359)  class_acc: 0.3333 (0.3638)  loss_scale: 16384.0000 (22802.4742)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.0007  max mem: 15572
Epoch: [29]  [ 300/1404]  eta: 0:11:07  lr: 0.000020  min_lr: 0.000000  loss: 4.1434 (4.1409)  class_acc: 0.3333 (0.3620)  loss_scale: 16384.0000 (22589.2359)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0007  max mem: 15572
Epoch: [29]  [ 310/1404]  eta: 0:11:01  lr: 0.000020  min_lr: 0.000000  loss: 4.1940 (4.1411)  class_acc: 0.2917 (0.3619)  loss_scale: 16384.0000 (22389.7106)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0009  max mem: 15572
Epoch: [29]  [ 320/1404]  eta: 0:10:53  lr: 0.000020  min_lr: 0.000000  loss: 4.2481 (4.1462)  class_acc: 0.3333 (0.3621)  loss_scale: 16384.0000 (22202.6168)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0008  max mem: 15572
Epoch: [29]  [ 330/1404]  eta: 0:10:47  lr: 0.000020  min_lr: 0.000000  loss: 4.2740 (4.1447)  class_acc: 0.3333 (0.3614)  loss_scale: 16384.0000 (22026.8278)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0381  max mem: 15572
Epoch: [29]  [ 340/1404]  eta: 0:10:41  lr: 0.000020  min_lr: 0.000000  loss: 4.1785 (4.1442)  class_acc: 0.3750 (0.3631)  loss_scale: 16384.0000 (21861.3490)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.0528  max mem: 15572
Epoch: [29]  [ 350/1404]  eta: 0:10:34  lr: 0.000020  min_lr: 0.000000  loss: 4.1072 (4.1402)  class_acc: 0.3750 (0.3625)  loss_scale: 16384.0000 (21705.2991)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0397  max mem: 15572
Epoch: [29]  [ 360/1404]  eta: 0:10:29  lr: 0.000020  min_lr: 0.000000  loss: 4.0673 (4.1373)  class_acc: 0.3333 (0.3607)  loss_scale: 16384.0000 (21557.8947)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0801  max mem: 15572
Epoch: [29]  [ 370/1404]  eta: 0:10:24  lr: 0.000020  min_lr: 0.000000  loss: 4.0673 (4.1379)  class_acc: 0.2917 (0.3608)  loss_scale: 16384.0000 (21418.4367)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.1353  max mem: 15572
Epoch: [29]  [ 380/1404]  eta: 0:10:18  lr: 0.000020  min_lr: 0.000000  loss: 4.2306 (4.1417)  class_acc: 0.3333 (0.3603)  loss_scale: 16384.0000 (21286.2992)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1589  max mem: 15572
Epoch: [29]  [ 390/1404]  eta: 0:10:11  lr: 0.000020  min_lr: 0.000000  loss: 4.1144 (4.1400)  class_acc: 0.3333 (0.3607)  loss_scale: 16384.0000 (21160.9207)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1236  max mem: 15572
Epoch: [29]  [ 400/1404]  eta: 0:10:03  lr: 0.000020  min_lr: 0.000000  loss: 4.1144 (4.1431)  class_acc: 0.3333 (0.3605)  loss_scale: 16384.0000 (21041.7955)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0587  max mem: 15572
[2025-01-17 03:21:47,139] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:21:47,139] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:21:47,139] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:21:47,139] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [ 410/1404]  eta: 0:09:56  lr: 0.000020  min_lr: 0.000000  loss: 4.2146 (4.1417)  class_acc: 0.3333 (0.3605)  loss_scale: 16384.0000 (21247.3771)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0444  max mem: 15572
Epoch: [29]  [ 420/1404]  eta: 0:09:48  lr: 0.000020  min_lr: 0.000000  loss: 4.0732 (4.1417)  class_acc: 0.3333 (0.3592)  loss_scale: 32768.0000 (21521.0261)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0419  max mem: 15572
[2025-01-17 03:21:58,810] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41141
[2025-01-17 03:21:58,810] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41141
[2025-01-17 03:21:58,811] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:21:58,811] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:21:58,811] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [ 430/1404]  eta: 0:09:40  lr: 0.000020  min_lr: 0.000000  loss: 4.1983 (4.1455)  class_acc: 0.2917 (0.3569)  loss_scale: 32768.0000 (21553.8933)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0343  max mem: 15572
Epoch: [29]  [ 440/1404]  eta: 0:09:34  lr: 0.000020  min_lr: 0.000000  loss: 4.2299 (4.1461)  class_acc: 0.3333 (0.3574)  loss_scale: 16384.0000 (21436.6621)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0515  max mem: 15572
Epoch: [29]  [ 450/1404]  eta: 0:09:27  lr: 0.000020  min_lr: 0.000000  loss: 4.1190 (4.1470)  class_acc: 0.3750 (0.3575)  loss_scale: 16384.0000 (21324.6297)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0577  max mem: 15572
Epoch: [29]  [ 460/1404]  eta: 0:09:19  lr: 0.000020  min_lr: 0.000000  loss: 4.0652 (4.1454)  class_acc: 0.3750 (0.3577)  loss_scale: 16384.0000 (21217.4577)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0343  max mem: 15572
Epoch: [29]  [ 470/1404]  eta: 0:09:14  lr: 0.000020  min_lr: 0.000000  loss: 4.0689 (4.1448)  class_acc: 0.3333 (0.3569)  loss_scale: 16384.0000 (21114.8365)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0748  max mem: 15572
Epoch: [29]  [ 480/1404]  eta: 0:09:07  lr: 0.000020  min_lr: 0.000000  loss: 4.1720 (4.1446)  class_acc: 0.2917 (0.3555)  loss_scale: 16384.0000 (21016.4823)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0823  max mem: 15572
Epoch: [29]  [ 490/1404]  eta: 0:09:01  lr: 0.000020  min_lr: 0.000000  loss: 4.2329 (4.1461)  class_acc: 0.2917 (0.3548)  loss_scale: 16384.0000 (20922.1344)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0448  max mem: 15572
Epoch: [29]  [ 500/1404]  eta: 0:08:57  lr: 0.000020  min_lr: 0.000000  loss: 4.1468 (4.1430)  class_acc: 0.3750 (0.3551)  loss_scale: 16384.0000 (20831.5529)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.0665  max mem: 15572
Epoch: [29]  [ 510/1404]  eta: 0:08:51  lr: 0.000020  min_lr: 0.000000  loss: 4.0537 (4.1425)  class_acc: 0.3750 (0.3553)  loss_scale: 16384.0000 (20744.5166)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.1036  max mem: 15572
Epoch: [29]  [ 520/1404]  eta: 0:08:45  lr: 0.000020  min_lr: 0.000000  loss: 4.1454 (4.1407)  class_acc: 0.4167 (0.3567)  loss_scale: 16384.0000 (20660.8215)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1070  max mem: 15572
Epoch: [29]  [ 530/1404]  eta: 0:08:38  lr: 0.000020  min_lr: 0.000000  loss: 4.1683 (4.1421)  class_acc: 0.3750 (0.3562)  loss_scale: 16384.0000 (20580.2787)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0449  max mem: 15572
Epoch: [29]  [ 540/1404]  eta: 0:08:32  lr: 0.000020  min_lr: 0.000000  loss: 4.2278 (4.1420)  class_acc: 0.3333 (0.3557)  loss_scale: 16384.0000 (20502.7135)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0480  max mem: 15572
Epoch: [29]  [ 550/1404]  eta: 0:08:26  lr: 0.000020  min_lr: 0.000000  loss: 4.2021 (4.1411)  class_acc: 0.3333 (0.3562)  loss_scale: 16384.0000 (20427.9637)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1117  max mem: 15572
[2025-01-17 03:23:14,637] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:23:14,638] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:23:14,646] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:23:14,646] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [ 560/1404]  eta: 0:08:21  lr: 0.000020  min_lr: 0.000000  loss: 4.2021 (4.1439)  class_acc: 0.3333 (0.3564)  loss_scale: 16384.0000 (20560.3137)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1356  max mem: 15572
Epoch: [29]  [ 570/1404]  eta: 0:08:15  lr: 0.000020  min_lr: 0.000000  loss: 4.2877 (4.1478)  class_acc: 0.2917 (0.3552)  loss_scale: 32768.0000 (20774.1086)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1311  max mem: 15572
Epoch: [29]  [ 580/1404]  eta: 0:08:10  lr: 0.000020  min_lr: 0.000000  loss: 4.2587 (4.1490)  class_acc: 0.2917 (0.3538)  loss_scale: 32768.0000 (20980.5439)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.1663  max mem: 15572
[2025-01-17 03:23:33,465] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41301
[2025-01-17 03:23:33,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:23:33,474] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41301
[2025-01-17 03:23:33,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:23:33,475] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [ 590/1404]  eta: 0:08:04  lr: 0.000020  min_lr: 0.000000  loss: 4.1580 (4.1489)  class_acc: 0.2917 (0.3543)  loss_scale: 32768.0000 (21013.6582)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1161  max mem: 15572
Epoch: [29]  [ 600/1404]  eta: 0:07:57  lr: 0.000020  min_lr: 0.000000  loss: 4.1467 (4.1492)  class_acc: 0.3333 (0.3539)  loss_scale: 16384.0000 (20936.6256)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0319  max mem: 15572
Epoch: [29]  [ 610/1404]  eta: 0:07:50  lr: 0.000020  min_lr: 0.000000  loss: 4.2909 (4.1507)  class_acc: 0.3750 (0.3543)  loss_scale: 16384.0000 (20862.1146)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0541  max mem: 15572
Epoch: [29]  [ 620/1404]  eta: 0:07:44  lr: 0.000020  min_lr: 0.000000  loss: 4.0479 (4.1482)  class_acc: 0.3750 (0.3549)  loss_scale: 16384.0000 (20790.0032)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0826  max mem: 15572
Epoch: [29]  [ 630/1404]  eta: 0:07:39  lr: 0.000020  min_lr: 0.000000  loss: 3.8971 (4.1457)  class_acc: 0.3750 (0.3553)  loss_scale: 16384.0000 (20720.1775)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1291  max mem: 15572
Epoch: [29]  [ 640/1404]  eta: 0:07:33  lr: 0.000020  min_lr: 0.000000  loss: 4.1063 (4.1450)  class_acc: 0.3750 (0.3556)  loss_scale: 16384.0000 (20652.5304)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1247  max mem: 15572
Epoch: [29]  [ 650/1404]  eta: 0:07:26  lr: 0.000020  min_lr: 0.000000  loss: 4.1720 (4.1453)  class_acc: 0.3750 (0.3555)  loss_scale: 16384.0000 (20586.9616)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0541  max mem: 15572
Epoch: [29]  [ 660/1404]  eta: 0:07:21  lr: 0.000019  min_lr: 0.000000  loss: 4.2690 (4.1471)  class_acc: 0.2500 (0.3540)  loss_scale: 16384.0000 (20523.3767)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0855  max mem: 15572
Epoch: [29]  [ 670/1404]  eta: 0:07:14  lr: 0.000019  min_lr: 0.000000  loss: 4.2129 (4.1463)  class_acc: 0.2917 (0.3542)  loss_scale: 16384.0000 (20461.6870)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.0717  max mem: 15572
Epoch: [29]  [ 680/1404]  eta: 0:07:08  lr: 0.000019  min_lr: 0.000000  loss: 4.1436 (4.1468)  class_acc: 0.3333 (0.3538)  loss_scale: 16384.0000 (20401.8091)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0006  max mem: 15572
Epoch: [29]  [ 690/1404]  eta: 0:07:03  lr: 0.000019  min_lr: 0.000000  loss: 4.2350 (4.1493)  class_acc: 0.3333 (0.3534)  loss_scale: 16384.0000 (20343.6643)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0007  max mem: 15572
Epoch: [29]  [ 700/1404]  eta: 0:06:56  lr: 0.000019  min_lr: 0.000000  loss: 4.2845 (4.1509)  class_acc: 0.3333 (0.3544)  loss_scale: 16384.0000 (20287.1783)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0007  max mem: 15572
Epoch: [29]  [ 710/1404]  eta: 0:06:50  lr: 0.000019  min_lr: 0.000000  loss: 4.0783 (4.1498)  class_acc: 0.3333 (0.3544)  loss_scale: 16384.0000 (20232.2813)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0007  max mem: 15572
[2025-01-17 03:24:48,314] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:24:48,314] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:24:48,319] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:24:48,320] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [ 720/1404]  eta: 0:06:44  lr: 0.000019  min_lr: 0.000000  loss: 4.0559 (4.1490)  class_acc: 0.3333 (0.3546)  loss_scale: 16384.0000 (20337.9750)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0005  max mem: 15572
Epoch: [29]  [ 730/1404]  eta: 0:06:38  lr: 0.000019  min_lr: 0.000000  loss: 4.1493 (4.1509)  class_acc: 0.3333 (0.3541)  loss_scale: 32768.0000 (20508.0164)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.0006  max mem: 15572
[2025-01-17 03:25:00,589] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41450
[2025-01-17 03:25:00,589] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:25:00,589] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:25:00,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41450
[2025-01-17 03:25:00,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [29]  [ 740/1404]  eta: 0:06:33  lr: 0.000019  min_lr: 0.000000  loss: 4.2090 (4.1507)  class_acc: 0.3750 (0.3547)  loss_scale: 32768.0000 (20518.6937)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.0008  max mem: 15572
Epoch: [29]  [ 750/1404]  eta: 0:06:27  lr: 0.000019  min_lr: 0.000000  loss: 4.1527 (4.1499)  class_acc: 0.4167 (0.3557)  loss_scale: 16384.0000 (20463.6378)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.0041  max mem: 15572
Epoch: [29]  [ 760/1404]  eta: 0:06:21  lr: 0.000019  min_lr: 0.000000  loss: 4.1235 (4.1496)  class_acc: 0.3750 (0.3553)  loss_scale: 16384.0000 (20410.0289)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0064  max mem: 15572
Epoch: [29]  [ 770/1404]  eta: 0:06:15  lr: 0.000019  min_lr: 0.000000  loss: 4.1235 (4.1492)  class_acc: 0.3333 (0.3551)  loss_scale: 16384.0000 (20357.8106)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0028  max mem: 15572
Epoch: [29]  [ 780/1404]  eta: 0:06:09  lr: 0.000019  min_lr: 0.000000  loss: 4.0340 (4.1468)  class_acc: 0.3333 (0.3552)  loss_scale: 16384.0000 (20306.9296)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0004  max mem: 15572
Epoch: [29]  [ 790/1404]  eta: 0:06:03  lr: 0.000019  min_lr: 0.000000  loss: 4.0487 (4.1469)  class_acc: 0.3333 (0.3554)  loss_scale: 16384.0000 (20257.3350)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0005  max mem: 15572
Epoch: [29]  [ 800/1404]  eta: 0:05:57  lr: 0.000019  min_lr: 0.000000  loss: 4.2073 (4.1474)  class_acc: 0.3333 (0.3552)  loss_scale: 16384.0000 (20208.9788)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0008  max mem: 15572
Epoch: [29]  [ 810/1404]  eta: 0:05:51  lr: 0.000019  min_lr: 0.000000  loss: 4.1014 (4.1457)  class_acc: 0.3333 (0.3554)  loss_scale: 16384.0000 (20161.8150)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0010  max mem: 15572
Epoch: [29]  [ 820/1404]  eta: 0:05:45  lr: 0.000019  min_lr: 0.000000  loss: 3.9956 (4.1444)  class_acc: 0.3750 (0.3568)  loss_scale: 16384.0000 (20115.8002)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0008  max mem: 15572
Epoch: [29]  [ 830/1404]  eta: 0:05:39  lr: 0.000019  min_lr: 0.000000  loss: 4.1071 (4.1455)  class_acc: 0.3750 (0.3565)  loss_scale: 16384.0000 (20070.8929)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0005  max mem: 15572
Epoch: [29]  [ 840/1404]  eta: 0:05:33  lr: 0.000019  min_lr: 0.000000  loss: 4.2820 (4.1445)  class_acc: 0.3333 (0.3566)  loss_scale: 16384.0000 (20027.0535)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0006  max mem: 15572
Epoch: [29]  [ 850/1404]  eta: 0:05:27  lr: 0.000019  min_lr: 0.000000  loss: 4.1590 (4.1449)  class_acc: 0.3333 (0.3563)  loss_scale: 16384.0000 (19984.2444)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0007  max mem: 15572
Epoch: [29]  [ 860/1404]  eta: 0:05:21  lr: 0.000019  min_lr: 0.000000  loss: 4.1685 (4.1448)  class_acc: 0.3333 (0.3567)  loss_scale: 16384.0000 (19942.4297)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0006  max mem: 15572
[2025-01-17 03:26:14,491] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:26:14,492] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:26:14,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:26:14,548] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [ 870/1404]  eta: 0:05:15  lr: 0.000019  min_lr: 0.000000  loss: 4.2014 (4.1455)  class_acc: 0.3750 (0.3567)  loss_scale: 16384.0000 (20052.0597)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0009  max mem: 15572
Epoch: [29]  [ 880/1404]  eta: 0:05:09  lr: 0.000019  min_lr: 0.000000  loss: 4.2034 (4.1456)  class_acc: 0.3750 (0.3567)  loss_scale: 32768.0000 (20196.3950)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0010  max mem: 15572
Epoch: [29]  [ 890/1404]  eta: 0:05:03  lr: 0.000019  min_lr: 0.000000  loss: 4.0644 (4.1439)  class_acc: 0.3750 (0.3570)  loss_scale: 32768.0000 (20337.4905)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.0007  max mem: 15572
Epoch: [29]  [ 900/1404]  eta: 0:04:57  lr: 0.000019  min_lr: 0.000000  loss: 4.0435 (4.1420)  class_acc: 0.3333 (0.3564)  loss_scale: 32768.0000 (20475.4539)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0005  max mem: 15572
Epoch: [29]  [ 910/1404]  eta: 0:04:51  lr: 0.000019  min_lr: 0.000000  loss: 4.2305 (4.1439)  class_acc: 0.2917 (0.3562)  loss_scale: 32768.0000 (20610.3886)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0009  max mem: 15572
Epoch: [29]  [ 920/1404]  eta: 0:04:45  lr: 0.000019  min_lr: 0.000000  loss: 4.2941 (4.1442)  class_acc: 0.3333 (0.3565)  loss_scale: 32768.0000 (20742.3931)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.0009  max mem: 15572
Epoch: [29]  [ 930/1404]  eta: 0:04:40  lr: 0.000019  min_lr: 0.000000  loss: 4.0485 (4.1438)  class_acc: 0.3750 (0.3574)  loss_scale: 32768.0000 (20871.5618)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.0006  max mem: 15572
Epoch: [29]  [ 940/1404]  eta: 0:04:33  lr: 0.000019  min_lr: 0.000000  loss: 4.0080 (4.1424)  class_acc: 0.4167 (0.3572)  loss_scale: 32768.0000 (20997.9851)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0006  max mem: 15572
Epoch: [29]  [ 950/1404]  eta: 0:04:28  lr: 0.000019  min_lr: 0.000000  loss: 4.0329 (4.1432)  class_acc: 0.3750 (0.3568)  loss_scale: 32768.0000 (21121.7497)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.0006  max mem: 15572
Epoch: [29]  [ 960/1404]  eta: 0:04:22  lr: 0.000019  min_lr: 0.000000  loss: 4.2470 (4.1435)  class_acc: 0.3333 (0.3562)  loss_scale: 32768.0000 (21242.9386)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0007  max mem: 15572
Epoch: [29]  [ 970/1404]  eta: 0:04:15  lr: 0.000019  min_lr: 0.000000  loss: 3.9830 (4.1415)  class_acc: 0.3333 (0.3568)  loss_scale: 32768.0000 (21361.6313)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0007  max mem: 15572
Epoch: [29]  [ 980/1404]  eta: 0:04:10  lr: 0.000019  min_lr: 0.000000  loss: 4.0785 (4.1422)  class_acc: 0.3333 (0.3569)  loss_scale: 32768.0000 (21477.9042)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0006  max mem: 15572
Epoch: [29]  [ 990/1404]  eta: 0:04:03  lr: 0.000019  min_lr: 0.000000  loss: 4.1799 (4.1404)  class_acc: 0.3333 (0.3574)  loss_scale: 32768.0000 (21591.8305)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0132  max mem: 15572
[2025-01-17 03:27:29,103] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:27:29,103] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 03:27:29,103] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:27:29,103] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 03:27:29,622] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41708
[2025-01-17 03:27:29,622] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 03:27:29,623] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41708
[2025-01-17 03:27:29,624] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 03:27:29,625] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1000/1404]  eta: 0:03:57  lr: 0.000019  min_lr: 0.000000  loss: 4.1548 (4.1407)  class_acc: 0.4167 (0.3578)  loss_scale: 32768.0000 (21736.2158)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0133  max mem: 15572
Epoch: [29]  [1010/1404]  eta: 0:03:52  lr: 0.000019  min_lr: 0.000000  loss: 4.1571 (4.1414)  class_acc: 0.3750 (0.3577)  loss_scale: 32768.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1015  max mem: 15572
Epoch: [29]  [1020/1404]  eta: 0:03:46  lr: 0.000019  min_lr: 0.000000  loss: 4.1228 (4.1414)  class_acc: 0.3333 (0.3572)  loss_scale: 32768.0000 (21952.3134)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1163  max mem: 15572
Epoch: [29]  [1030/1404]  eta: 0:03:40  lr: 0.000019  min_lr: 0.000000  loss: 4.1216 (4.1417)  class_acc: 0.2917 (0.3572)  loss_scale: 32768.0000 (22057.2182)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0802  max mem: 15572
Epoch: [29]  [1040/1404]  eta: 0:03:34  lr: 0.000019  min_lr: 0.000000  loss: 4.1917 (4.1418)  class_acc: 0.3333 (0.3575)  loss_scale: 32768.0000 (22160.1076)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0652  max mem: 15572
Epoch: [29]  [1050/1404]  eta: 0:03:28  lr: 0.000019  min_lr: 0.000000  loss: 4.0617 (4.1404)  class_acc: 0.3750 (0.3576)  loss_scale: 32768.0000 (22261.0390)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0471  max mem: 15572
Epoch: [29]  [1060/1404]  eta: 0:03:22  lr: 0.000019  min_lr: 0.000000  loss: 4.0466 (4.1401)  class_acc: 0.3750 (0.3578)  loss_scale: 32768.0000 (22360.0679)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0692  max mem: 15572
Epoch: [29]  [1070/1404]  eta: 0:03:16  lr: 0.000019  min_lr: 0.000000  loss: 4.0118 (4.1393)  class_acc: 0.3333 (0.3573)  loss_scale: 32768.0000 (22457.2474)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.0926  max mem: 15572
Epoch: [29]  [1080/1404]  eta: 0:03:11  lr: 0.000018  min_lr: 0.000000  loss: 4.0206 (4.1386)  class_acc: 0.3333 (0.3575)  loss_scale: 32768.0000 (22552.6290)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1232  max mem: 15572
Epoch: [29]  [1090/1404]  eta: 0:03:04  lr: 0.000018  min_lr: 0.000000  loss: 4.1040 (4.1371)  class_acc: 0.3750 (0.3578)  loss_scale: 32768.0000 (22646.2621)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.0667  max mem: 15572
Epoch: [29]  [1100/1404]  eta: 0:02:59  lr: 0.000018  min_lr: 0.000000  loss: 4.1385 (4.1372)  class_acc: 0.3333 (0.3573)  loss_scale: 32768.0000 (22738.1944)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0785  max mem: 15572
[2025-01-17 03:28:36,005] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41820
[2025-01-17 03:28:36,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:28:36,005] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41820
[2025-01-17 03:28:36,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:28:36,006] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [1110/1404]  eta: 0:02:53  lr: 0.000018  min_lr: 0.000000  loss: 4.1088 (4.1366)  class_acc: 0.3750 (0.3583)  loss_scale: 32768.0000 (22725.2421)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0867  max mem: 15572
Epoch: [29]  [1120/1404]  eta: 0:02:47  lr: 0.000018  min_lr: 0.000000  loss: 4.0321 (4.1348)  class_acc: 0.4167 (0.3588)  loss_scale: 16384.0000 (22668.6744)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0570  max mem: 15572
Epoch: [29]  [1130/1404]  eta: 0:02:41  lr: 0.000018  min_lr: 0.000000  loss: 4.0389 (4.1357)  class_acc: 0.4167 (0.3593)  loss_scale: 16384.0000 (22613.1070)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0816  max mem: 15572
Epoch: [29]  [1140/1404]  eta: 0:02:35  lr: 0.000018  min_lr: 0.000000  loss: 4.1966 (4.1361)  class_acc: 0.3333 (0.3593)  loss_scale: 16384.0000 (22558.5136)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0600  max mem: 15572
Epoch: [29]  [1150/1404]  eta: 0:02:29  lr: 0.000018  min_lr: 0.000000  loss: 4.1769 (4.1345)  class_acc: 0.3333 (0.3591)  loss_scale: 16384.0000 (22504.8688)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0419  max mem: 15572
Epoch: [29]  [1160/1404]  eta: 0:02:23  lr: 0.000018  min_lr: 0.000000  loss: 3.9894 (4.1341)  class_acc: 0.3750 (0.3596)  loss_scale: 16384.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.1177  max mem: 15572
Epoch: [29]  [1170/1404]  eta: 0:02:17  lr: 0.000018  min_lr: 0.000000  loss: 4.2072 (4.1343)  class_acc: 0.3750 (0.3596)  loss_scale: 16384.0000 (22400.3279)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.1300  max mem: 15572
Epoch: [29]  [1180/1404]  eta: 0:02:11  lr: 0.000018  min_lr: 0.000000  loss: 4.1622 (4.1346)  class_acc: 0.2917 (0.3593)  loss_scale: 16384.0000 (22349.3853)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0496  max mem: 15572
Epoch: [29]  [1190/1404]  eta: 0:02:05  lr: 0.000018  min_lr: 0.000000  loss: 4.0904 (4.1340)  class_acc: 0.2917 (0.3587)  loss_scale: 16384.0000 (22299.2981)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0209  max mem: 15572
Epoch: [29]  [1200/1404]  eta: 0:02:00  lr: 0.000018  min_lr: 0.000000  loss: 4.1645 (4.1338)  class_acc: 0.2917 (0.3586)  loss_scale: 16384.0000 (22250.0450)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0502  max mem: 15572
Epoch: [29]  [1210/1404]  eta: 0:01:54  lr: 0.000018  min_lr: 0.000000  loss: 4.2118 (4.1356)  class_acc: 0.3333 (0.3585)  loss_scale: 16384.0000 (22201.6053)  weight_decay: 0.0500 (0.0500)  time: 0.6447  data: 0.1337  max mem: 15572
Epoch: [29]  [1220/1404]  eta: 0:01:48  lr: 0.000018  min_lr: 0.000000  loss: 4.2106 (4.1349)  class_acc: 0.3750 (0.3583)  loss_scale: 16384.0000 (22153.9590)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1018  max mem: 15572
Epoch: [29]  [1230/1404]  eta: 0:01:42  lr: 0.000018  min_lr: 0.000000  loss: 4.1457 (4.1349)  class_acc: 0.3333 (0.3582)  loss_scale: 16384.0000 (22107.0869)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0393  max mem: 15572
[2025-01-17 03:29:51,481] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:29:51,481] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:29:51,491] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:29:51,491] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [1240/1404]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 4.1762 (4.1356)  class_acc: 0.3333 (0.3580)  loss_scale: 16384.0000 (22166.5882)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1214  max mem: 15572
Epoch: [29]  [1250/1404]  eta: 0:01:30  lr: 0.000018  min_lr: 0.000000  loss: 4.2253 (4.1366)  class_acc: 0.3750 (0.3582)  loss_scale: 32768.0000 (22251.3317)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1180  max mem: 15572
[2025-01-17 03:30:07,077] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41976
[2025-01-17 03:30:07,077] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:30:07,112] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41976
[2025-01-17 03:30:07,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:30:07,112] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [1260/1404]  eta: 0:01:24  lr: 0.000018  min_lr: 0.000000  loss: 4.2042 (4.1360)  class_acc: 0.3333 (0.3579)  loss_scale: 32768.0000 (22321.7383)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.0923  max mem: 15572
Epoch: [29]  [1270/1404]  eta: 0:01:18  lr: 0.000018  min_lr: 0.000000  loss: 4.1514 (4.1359)  class_acc: 0.2917 (0.3574)  loss_scale: 16384.0000 (22275.0212)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1304  max mem: 15572
Epoch: [29]  [1280/1404]  eta: 0:01:13  lr: 0.000018  min_lr: 0.000000  loss: 4.0828 (4.1358)  class_acc: 0.3333 (0.3575)  loss_scale: 16384.0000 (22229.0336)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1100  max mem: 15572
[2025-01-17 03:30:21,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=42000, skipped=250, lr=[1.7448843457353946e-07, 1.7448843457353946e-07, 2.4926919224791356e-07, 2.4926919224791356e-07, 3.5609884606844795e-07, 3.5609884606844795e-07, 5.087126372406399e-07, 5.087126372406399e-07, 7.267323389152e-07, 7.267323389152e-07, 1.038189055593143e-06, 1.038189055593143e-06, 1.4831272222759185e-06, 1.4831272222759185e-06, 2.118753174679884e-06, 2.118753174679884e-06, 3.026790249542691e-06, 3.026790249542691e-06, 4.323986070775273e-06, 4.323986070775273e-06, 6.177122958250391e-06, 6.177122958250391e-06, 8.82446136892913e-06, 8.82446136892913e-06, 1.2606373384184473e-05, 1.2606373384184473e-05, 1.8009104834549248e-05, 1.8009104834549248e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 03:30:21,529] [INFO] [timer.py:260:stop] epoch=0/micro_step=42000/global_step=42000, RunningAvgSamplesPerSec=47.96177272686134, CurrSamplesPerSec=56.61825606182471, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [1290/1404]  eta: 0:01:07  lr: 0.000018  min_lr: 0.000000  loss: 4.0906 (4.1366)  class_acc: 0.3333 (0.3573)  loss_scale: 16384.0000 (22183.7583)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0825  max mem: 15572
Epoch: [29]  [1300/1404]  eta: 0:01:01  lr: 0.000018  min_lr: 0.000000  loss: 4.1722 (4.1369)  class_acc: 0.2917 (0.3573)  loss_scale: 16384.0000 (22139.1791)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1104  max mem: 15572
Epoch: [29]  [1310/1404]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 4.1311 (4.1360)  class_acc: 0.3333 (0.3572)  loss_scale: 16384.0000 (22095.2799)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0876  max mem: 15572
Epoch: [29]  [1320/1404]  eta: 0:00:49  lr: 0.000018  min_lr: 0.000000  loss: 4.1311 (4.1363)  class_acc: 0.3750 (0.3571)  loss_scale: 16384.0000 (22052.0454)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0793  max mem: 15572
Epoch: [29]  [1330/1404]  eta: 0:00:43  lr: 0.000018  min_lr: 0.000000  loss: 4.2062 (4.1376)  class_acc: 0.3750 (0.3571)  loss_scale: 16384.0000 (22009.4606)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0928  max mem: 15572
Epoch: [29]  [1340/1404]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 4.2297 (4.1382)  class_acc: 0.3333 (0.3569)  loss_scale: 16384.0000 (21967.5108)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0577  max mem: 15572
Epoch: [29]  [1350/1404]  eta: 0:00:31  lr: 0.000018  min_lr: 0.000000  loss: 4.1086 (4.1378)  class_acc: 0.3333 (0.3568)  loss_scale: 16384.0000 (21926.1821)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0670  max mem: 15572
Epoch: [29]  [1360/1404]  eta: 0:00:25  lr: 0.000018  min_lr: 0.000000  loss: 4.1086 (4.1377)  class_acc: 0.3333 (0.3567)  loss_scale: 16384.0000 (21885.4607)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1572  max mem: 15572
Epoch: [29]  [1370/1404]  eta: 0:00:20  lr: 0.000018  min_lr: 0.000000  loss: 4.0283 (4.1369)  class_acc: 0.3333 (0.3568)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1174  max mem: 15572
Epoch: [29]  [1380/1404]  eta: 0:00:14  lr: 0.000018  min_lr: 0.000000  loss: 4.1016 (4.1372)  class_acc: 0.3333 (0.3567)  loss_scale: 16384.0000 (21805.7871)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0618  max mem: 15572
[2025-01-17 03:31:22,320] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:31:22,320] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:31:22,345] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:31:22,345] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [1390/1404]  eta: 0:00:08  lr: 0.000018  min_lr: 0.000000  loss: 4.2165 (4.1378)  class_acc: 0.3333 (0.3571)  loss_scale: 16384.0000 (21790.3666)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0902  max mem: 15572
Epoch: [29]  [1400/1404]  eta: 0:00:02  lr: 0.000018  min_lr: 0.000000  loss: 4.2672 (4.1385)  class_acc: 0.3333 (0.3566)  loss_scale: 32768.0000 (21868.7223)  weight_decay: 0.0500 (0.0500)  time: 0.4469  data: 0.0288  max mem: 15572
Epoch: [29]  [1403/1404]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 4.2979 (4.1389)  class_acc: 0.3333 (0.3564)  loss_scale: 32768.0000 (21892.0114)  weight_decay: 0.0500 (0.0500)  time: 0.4073  data: 0.0003  max mem: 15572
Epoch: [29] Total time: 0:13:43 (0.5864 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 4.2979 (4.1407)  class_acc: 0.3333 (0.3498)  loss_scale: 32768.0000 (21892.0114)  weight_decay: 0.0500 (0.0500)
[2025-01-17 03:31:27,842] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-29 is about to be saved!
[2025-01-17 03:31:27,844] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-29/mp_rank_00_model_states.pt
[2025-01-17 03:31:27,844] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-29/mp_rank_00_model_states.pt...
[2025-01-17 03:31:27,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-29 is ready now!
[2025-01-17 03:31:28,070] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-29/mp_rank_00_model_states.pt.
[2025-01-17 03:31:28,070] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-29 is ready now!
Val:  [  0/136]  eta: 0:12:16  loss: 1.7584 (1.7584)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.4189  data: 5.2434  max mem: 15572
Val:  [ 10/136]  eta: 0:01:32  loss: 2.3673 (2.2341)  acc1: 50.0000 (50.0000)  acc5: 77.7778 (79.7980)  time: 0.7370  data: 0.5537  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 2.3840 (2.3787)  acc1: 38.8889 (46.5608)  acc5: 77.7778 (78.3069)  time: 0.3324  data: 0.1418  max mem: 15572
Val:  [ 30/136]  eta: 0:00:52  loss: 2.2996 (2.2929)  acc1: 38.8889 (47.6703)  acc5: 83.3333 (79.3907)  time: 0.3578  data: 0.1669  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 2.1361 (2.2611)  acc1: 55.5556 (49.7290)  acc5: 83.3333 (79.8103)  time: 0.3017  data: 0.1184  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.1876 (2.2743)  acc1: 50.0000 (49.8911)  acc5: 77.7778 (79.8475)  time: 0.3345  data: 0.1459  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.2843 (2.3493)  acc1: 44.4444 (47.1767)  acc5: 77.7778 (78.3242)  time: 0.3815  data: 0.1839  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.2843 (2.3289)  acc1: 50.0000 (48.0438)  acc5: 77.7778 (78.5603)  time: 0.3533  data: 0.1573  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.1424 (2.3186)  acc1: 50.0000 (48.3539)  acc5: 83.3333 (79.2181)  time: 0.3441  data: 0.1430  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.2601 (2.3265)  acc1: 44.4444 (47.4359)  acc5: 77.7778 (79.0598)  time: 0.3378  data: 0.1279  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5634 (2.3810)  acc1: 38.8889 (45.8196)  acc5: 72.2222 (77.6128)  time: 0.3412  data: 0.1322  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.4901 (2.3685)  acc1: 38.8889 (46.0961)  acc5: 77.7778 (78.0781)  time: 0.3638  data: 0.1524  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1278 (2.3285)  acc1: 50.0000 (47.1074)  acc5: 83.3333 (78.9715)  time: 0.3369  data: 0.1333  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.9496 (2.2979)  acc1: 55.5556 (48.1340)  acc5: 88.8889 (79.5165)  time: 0.2941  data: 0.1151  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.9498 (2.2946)  acc1: 55.5556 (48.4029)  acc5: 83.3333 (79.6478)  time: 0.2233  data: 0.0578  max mem: 15572
Val: Total time: 0:00:49 (0.3663 s / it)
* Acc@1 47.195 Acc@5 78.706 loss 2.340
Accuracy of the network on the 4883 val videos: 47.2%
Max accuracy: 47.52%
Epoch: [30]  [   0/1404]  eta: 3:16:29  lr: 0.000018  min_lr: 0.000000  loss: 4.1004 (4.1004)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.3974  data: 6.0217  max mem: 15572
Epoch: [30]  [  10/1404]  eta: 0:28:02  lr: 0.000018  min_lr: 0.000000  loss: 3.9158 (4.0238)  class_acc: 0.3333 (0.3826)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2069  data: 0.5483  max mem: 15572
Epoch: [30]  [  20/1404]  eta: 0:21:37  lr: 0.000018  min_lr: 0.000000  loss: 4.0852 (4.1244)  class_acc: 0.2917 (0.3115)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.0775  max mem: 15572
Epoch: [30]  [  30/1404]  eta: 0:19:06  lr: 0.000018  min_lr: 0.000000  loss: 4.1122 (4.1138)  class_acc: 0.2917 (0.3293)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.1414  max mem: 15572
Epoch: [30]  [  40/1404]  eta: 0:17:14  lr: 0.000018  min_lr: 0.000000  loss: 3.9828 (4.1062)  class_acc: 0.3333 (0.3161)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0870  max mem: 15572
Epoch: [30]  [  50/1404]  eta: 0:16:40  lr: 0.000018  min_lr: 0.000000  loss: 3.9828 (4.1045)  class_acc: 0.3333 (0.3252)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0925  max mem: 15572
Epoch: [30]  [  60/1404]  eta: 0:15:42  lr: 0.000018  min_lr: 0.000000  loss: 4.1488 (4.1261)  class_acc: 0.2917 (0.3197)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0702  max mem: 15572
Epoch: [30]  [  70/1404]  eta: 0:14:57  lr: 0.000018  min_lr: 0.000000  loss: 4.2985 (4.1380)  class_acc: 0.2500 (0.3128)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0079  max mem: 15572
Epoch: [30]  [  80/1404]  eta: 0:14:26  lr: 0.000018  min_lr: 0.000000  loss: 4.3404 (4.1537)  class_acc: 0.2500 (0.3107)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0205  max mem: 15572
Epoch: [30]  [  90/1404]  eta: 0:14:06  lr: 0.000018  min_lr: 0.000000  loss: 4.2708 (4.1638)  class_acc: 0.2917 (0.3159)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0458  max mem: 15572
Epoch: [30]  [ 100/1404]  eta: 0:13:53  lr: 0.000017  min_lr: 0.000000  loss: 4.1679 (4.1648)  class_acc: 0.3333 (0.3168)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0332  max mem: 15572
Epoch: [30]  [ 110/1404]  eta: 0:13:40  lr: 0.000017  min_lr: 0.000000  loss: 4.0658 (4.1543)  class_acc: 0.2917 (0.3138)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0007  max mem: 15572
[2025-01-17 03:33:31,038] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:33:31,039] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 03:33:31,045] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:33:31,045] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 03:33:32,126] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42235
[2025-01-17 03:33:32,126] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 03:33:32,126] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 03:33:32,224] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42235
[2025-01-17 03:33:32,225] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [30]  [ 120/1404]  eta: 0:13:33  lr: 0.000017  min_lr: 0.000000  loss: 4.0428 (4.1504)  class_acc: 0.2917 (0.3175)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0006  max mem: 15572
Epoch: [30]  [ 130/1404]  eta: 0:13:24  lr: 0.000017  min_lr: 0.000000  loss: 4.0308 (4.1408)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0007  max mem: 15572
Epoch: [30]  [ 140/1404]  eta: 0:13:11  lr: 0.000017  min_lr: 0.000000  loss: 4.1228 (4.1495)  class_acc: 0.3333 (0.3206)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.0007  max mem: 15572
Epoch: [30]  [ 150/1404]  eta: 0:13:04  lr: 0.000017  min_lr: 0.000000  loss: 4.1149 (4.1341)  class_acc: 0.3333 (0.3240)  loss_scale: 32768.0000 (33202.0132)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0007  max mem: 15572
Epoch: [30]  [ 160/1404]  eta: 0:12:53  lr: 0.000017  min_lr: 0.000000  loss: 4.0416 (4.1265)  class_acc: 0.3333 (0.3248)  loss_scale: 32768.0000 (33175.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.0030  max mem: 15572
[2025-01-17 03:33:59,596] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42282
[2025-01-17 03:33:59,596] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:33:59,596] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:33:59,622] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42282
[2025-01-17 03:33:59,622] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [30]  [ 170/1404]  eta: 0:12:41  lr: 0.000017  min_lr: 0.000000  loss: 4.0575 (4.1232)  class_acc: 0.3750 (0.3277)  loss_scale: 32768.0000 (32288.9357)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0029  max mem: 15572
Epoch: [30]  [ 180/1404]  eta: 0:12:37  lr: 0.000017  min_lr: 0.000000  loss: 4.1012 (4.1211)  class_acc: 0.3750 (0.3297)  loss_scale: 16384.0000 (31410.2099)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0006  max mem: 15572
Epoch: [30]  [ 190/1404]  eta: 0:12:31  lr: 0.000017  min_lr: 0.000000  loss: 4.0090 (4.1179)  class_acc: 0.3750 (0.3329)  loss_scale: 16384.0000 (30623.4974)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.0007  max mem: 15572
Epoch: [30]  [ 200/1404]  eta: 0:12:20  lr: 0.000017  min_lr: 0.000000  loss: 4.0598 (4.1228)  class_acc: 0.3333 (0.3344)  loss_scale: 16384.0000 (29915.0647)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0069  max mem: 15572
Epoch: [30]  [ 210/1404]  eta: 0:12:16  lr: 0.000017  min_lr: 0.000000  loss: 4.1997 (4.1284)  class_acc: 0.3333 (0.3341)  loss_scale: 16384.0000 (29273.7820)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0068  max mem: 15572
Epoch: [30]  [ 220/1404]  eta: 0:12:06  lr: 0.000017  min_lr: 0.000000  loss: 4.0419 (4.1219)  class_acc: 0.3750 (0.3377)  loss_scale: 16384.0000 (28690.5339)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0006  max mem: 15572
Epoch: [30]  [ 230/1404]  eta: 0:12:03  lr: 0.000017  min_lr: 0.000000  loss: 4.0990 (4.1267)  class_acc: 0.3750 (0.3402)  loss_scale: 16384.0000 (28157.7835)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0006  max mem: 15572
Epoch: [30]  [ 240/1404]  eta: 0:11:53  lr: 0.000017  min_lr: 0.000000  loss: 4.0990 (4.1207)  class_acc: 0.3750 (0.3432)  loss_scale: 16384.0000 (27669.2448)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0007  max mem: 15572
Epoch: [30]  [ 250/1404]  eta: 0:11:44  lr: 0.000017  min_lr: 0.000000  loss: 3.9397 (4.1180)  class_acc: 0.3750 (0.3430)  loss_scale: 16384.0000 (27219.6335)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0005  max mem: 15572
Epoch: [30]  [ 260/1404]  eta: 0:11:35  lr: 0.000017  min_lr: 0.000000  loss: 4.0500 (4.1169)  class_acc: 0.3333 (0.3428)  loss_scale: 16384.0000 (26804.4751)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.0192  max mem: 15572
Epoch: [30]  [ 270/1404]  eta: 0:11:29  lr: 0.000017  min_lr: 0.000000  loss: 4.0789 (4.1159)  class_acc: 0.3333 (0.3426)  loss_scale: 16384.0000 (26419.9557)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0317  max mem: 15572
Epoch: [30]  [ 280/1404]  eta: 0:11:24  lr: 0.000017  min_lr: 0.000000  loss: 4.1986 (4.1181)  class_acc: 0.2917 (0.3415)  loss_scale: 16384.0000 (26062.8043)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.0401  max mem: 15572
Epoch: [30]  [ 290/1404]  eta: 0:11:20  lr: 0.000017  min_lr: 0.000000  loss: 4.1987 (4.1215)  class_acc: 0.2917 (0.3409)  loss_scale: 16384.0000 (25730.1993)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.0867  max mem: 15572
[2025-01-17 03:35:16,066] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:35:16,067] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:35:16,068] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:35:16,068] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [30]  [ 300/1404]  eta: 0:11:11  lr: 0.000017  min_lr: 0.000000  loss: 4.1778 (4.1215)  class_acc: 0.3333 (0.3419)  loss_scale: 16384.0000 (25964.0133)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0834  max mem: 15572
Epoch: [30]  [ 310/1404]  eta: 0:11:02  lr: 0.000017  min_lr: 0.000000  loss: 4.1103 (4.1220)  class_acc: 0.3750 (0.3418)  loss_scale: 32768.0000 (26182.7910)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0499  max mem: 15572
[2025-01-17 03:35:29,348] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42436
[2025-01-17 03:35:29,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:35:29,350] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42436
[2025-01-17 03:35:29,351] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:35:29,351] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [ 320/1404]  eta: 0:10:57  lr: 0.000017  min_lr: 0.000000  loss: 4.0978 (4.1161)  class_acc: 0.3333 (0.3407)  loss_scale: 32768.0000 (26132.7352)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1030  max mem: 15572
Epoch: [30]  [ 330/1404]  eta: 0:10:48  lr: 0.000017  min_lr: 0.000000  loss: 4.0539 (4.1161)  class_acc: 0.2917 (0.3400)  loss_scale: 16384.0000 (25838.2115)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0906  max mem: 15572
Epoch: [30]  [ 340/1404]  eta: 0:10:41  lr: 0.000017  min_lr: 0.000000  loss: 4.0318 (4.1088)  class_acc: 0.3333 (0.3419)  loss_scale: 16384.0000 (25560.9619)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0395  max mem: 15572
Epoch: [30]  [ 350/1404]  eta: 0:10:35  lr: 0.000017  min_lr: 0.000000  loss: 4.0318 (4.1089)  class_acc: 0.3333 (0.3403)  loss_scale: 16384.0000 (25299.5100)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0263  max mem: 15572
Epoch: [30]  [ 360/1404]  eta: 0:10:28  lr: 0.000017  min_lr: 0.000000  loss: 4.1529 (4.1136)  class_acc: 0.3333 (0.3408)  loss_scale: 16384.0000 (25052.5429)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0006  max mem: 15572
Epoch: [30]  [ 370/1404]  eta: 0:10:21  lr: 0.000017  min_lr: 0.000000  loss: 4.1435 (4.1120)  class_acc: 0.3750 (0.3405)  loss_scale: 16384.0000 (24818.8895)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0145  max mem: 15572
Epoch: [30]  [ 380/1404]  eta: 0:10:13  lr: 0.000017  min_lr: 0.000000  loss: 4.1742 (4.1162)  class_acc: 0.3333 (0.3398)  loss_scale: 16384.0000 (24597.5013)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0332  max mem: 15572
Epoch: [30]  [ 390/1404]  eta: 0:10:07  lr: 0.000017  min_lr: 0.000000  loss: 4.1014 (4.1140)  class_acc: 0.3333 (0.3406)  loss_scale: 16384.0000 (24387.4373)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0194  max mem: 15572
Epoch: [30]  [ 400/1404]  eta: 0:10:02  lr: 0.000017  min_lr: 0.000000  loss: 4.1012 (4.1166)  class_acc: 0.3333 (0.3394)  loss_scale: 16384.0000 (24187.8504)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0354  max mem: 15572
Epoch: [30]  [ 410/1404]  eta: 0:09:55  lr: 0.000017  min_lr: 0.000000  loss: 4.1592 (4.1185)  class_acc: 0.3333 (0.3397)  loss_scale: 16384.0000 (23997.9757)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0592  max mem: 15572
Epoch: [30]  [ 420/1404]  eta: 0:09:48  lr: 0.000017  min_lr: 0.000000  loss: 4.3303 (4.1241)  class_acc: 0.3333 (0.3404)  loss_scale: 16384.0000 (23817.1211)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0243  max mem: 15572
Epoch: [30]  [ 430/1404]  eta: 0:09:43  lr: 0.000017  min_lr: 0.000000  loss: 4.1760 (4.1235)  class_acc: 0.2917 (0.3404)  loss_scale: 16384.0000 (23644.6589)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.0006  max mem: 15572
Epoch: [30]  [ 440/1404]  eta: 0:09:36  lr: 0.000017  min_lr: 0.000000  loss: 4.0441 (4.1204)  class_acc: 0.3750 (0.3421)  loss_scale: 16384.0000 (23480.0181)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.0006  max mem: 15572
[2025-01-17 03:36:45,857] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:36:45,857] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:36:45,918] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:36:45,919] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [30]  [ 450/1404]  eta: 0:09:31  lr: 0.000017  min_lr: 0.000000  loss: 4.1014 (4.1211)  class_acc: 0.3750 (0.3421)  loss_scale: 16384.0000 (23540.6475)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0005  max mem: 15572
Epoch: [30]  [ 460/1404]  eta: 0:09:24  lr: 0.000017  min_lr: 0.000000  loss: 4.1091 (4.1220)  class_acc: 0.4167 (0.3443)  loss_scale: 32768.0000 (23740.8069)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.0005  max mem: 15572
Epoch: [30]  [ 470/1404]  eta: 0:09:17  lr: 0.000017  min_lr: 0.000000  loss: 4.1079 (4.1230)  class_acc: 0.4167 (0.3446)  loss_scale: 32768.0000 (23932.4671)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0280  max mem: 15572
Epoch: [30]  [ 480/1404]  eta: 0:09:10  lr: 0.000017  min_lr: 0.000000  loss: 4.0738 (4.1241)  class_acc: 0.3333 (0.3436)  loss_scale: 32768.0000 (24116.1580)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0703  max mem: 15572
[2025-01-17 03:37:07,036] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42603
[2025-01-17 03:37:07,037] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:37:07,037] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42603
[2025-01-17 03:37:07,037] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:37:07,037] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [ 490/1404]  eta: 0:09:04  lr: 0.000017  min_lr: 0.000000  loss: 3.9458 (4.1192)  class_acc: 0.3333 (0.3446)  loss_scale: 32768.0000 (24025.4175)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0810  max mem: 15572
Epoch: [30]  [ 500/1404]  eta: 0:08:57  lr: 0.000017  min_lr: 0.000000  loss: 3.9476 (4.1193)  class_acc: 0.3333 (0.3442)  loss_scale: 16384.0000 (23872.8942)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0764  max mem: 15572
Epoch: [30]  [ 510/1404]  eta: 0:08:52  lr: 0.000017  min_lr: 0.000000  loss: 4.1343 (4.1201)  class_acc: 0.3333 (0.3447)  loss_scale: 16384.0000 (23726.3405)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1030  max mem: 15572
Epoch: [30]  [ 520/1404]  eta: 0:08:46  lr: 0.000017  min_lr: 0.000000  loss: 4.2856 (4.1233)  class_acc: 0.3333 (0.3440)  loss_scale: 16384.0000 (23585.4127)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1140  max mem: 15572
Epoch: [30]  [ 530/1404]  eta: 0:08:41  lr: 0.000017  min_lr: 0.000000  loss: 4.2000 (4.1231)  class_acc: 0.3333 (0.3442)  loss_scale: 16384.0000 (23449.7928)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1051  max mem: 15572
Epoch: [30]  [ 540/1404]  eta: 0:08:35  lr: 0.000016  min_lr: 0.000000  loss: 4.0851 (4.1214)  class_acc: 0.3750 (0.3440)  loss_scale: 16384.0000 (23319.1867)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1156  max mem: 15572
Epoch: [30]  [ 550/1404]  eta: 0:08:28  lr: 0.000016  min_lr: 0.000000  loss: 4.0783 (4.1183)  class_acc: 0.4167 (0.3447)  loss_scale: 16384.0000 (23193.3212)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.0698  max mem: 15572
Epoch: [30]  [ 560/1404]  eta: 0:08:21  lr: 0.000016  min_lr: 0.000000  loss: 4.1190 (4.1179)  class_acc: 0.3333 (0.3444)  loss_scale: 16384.0000 (23071.9430)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0308  max mem: 15572
Epoch: [30]  [ 570/1404]  eta: 0:08:17  lr: 0.000016  min_lr: 0.000000  loss: 4.1654 (4.1167)  class_acc: 0.3333 (0.3449)  loss_scale: 16384.0000 (22954.8161)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1240  max mem: 15572
Epoch: [30]  [ 580/1404]  eta: 0:08:10  lr: 0.000016  min_lr: 0.000000  loss: 4.1571 (4.1174)  class_acc: 0.3750 (0.3460)  loss_scale: 16384.0000 (22841.7212)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1395  max mem: 15572
Epoch: [30]  [ 590/1404]  eta: 0:08:05  lr: 0.000016  min_lr: 0.000000  loss: 3.9976 (4.1141)  class_acc: 0.3750 (0.3460)  loss_scale: 16384.0000 (22732.4535)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0921  max mem: 15572
Epoch: [30]  [ 600/1404]  eta: 0:07:58  lr: 0.000016  min_lr: 0.000000  loss: 4.0811 (4.1153)  class_acc: 0.3333 (0.3455)  loss_scale: 16384.0000 (22626.8220)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0726  max mem: 15572
Epoch: [30]  [ 610/1404]  eta: 0:07:52  lr: 0.000016  min_lr: 0.000000  loss: 4.1105 (4.1130)  class_acc: 0.3333 (0.3465)  loss_scale: 16384.0000 (22524.6481)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.0722  max mem: 15572
[2025-01-17 03:38:22,814] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:38:22,815] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:38:22,858] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:38:22,859] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [30]  [ 620/1404]  eta: 0:07:47  lr: 0.000016  min_lr: 0.000000  loss: 4.0291 (4.1147)  class_acc: 0.2917 (0.3461)  loss_scale: 16384.0000 (22663.2142)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.1373  max mem: 15572
Epoch: [30]  [ 630/1404]  eta: 0:07:41  lr: 0.000016  min_lr: 0.000000  loss: 4.1725 (4.1150)  class_acc: 0.3333 (0.3465)  loss_scale: 32768.0000 (22823.3534)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1190  max mem: 15572
Epoch: [30]  [ 640/1404]  eta: 0:07:34  lr: 0.000016  min_lr: 0.000000  loss: 4.0880 (4.1151)  class_acc: 0.3750 (0.3472)  loss_scale: 32768.0000 (22978.4961)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0628  max mem: 15572
Epoch: [30]  [ 650/1404]  eta: 0:07:28  lr: 0.000016  min_lr: 0.000000  loss: 4.0221 (4.1133)  class_acc: 0.3750 (0.3477)  loss_scale: 32768.0000 (23128.8725)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0728  max mem: 15572
Epoch: [30]  [ 660/1404]  eta: 0:07:22  lr: 0.000016  min_lr: 0.000000  loss: 3.9702 (4.1107)  class_acc: 0.3750 (0.3481)  loss_scale: 32768.0000 (23274.6989)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0691  max mem: 15572
Epoch: [30]  [ 670/1404]  eta: 0:07:16  lr: 0.000016  min_lr: 0.000000  loss: 4.1511 (4.1132)  class_acc: 0.3333 (0.3483)  loss_scale: 32768.0000 (23416.1788)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0870  max mem: 15572
[2025-01-17 03:38:59,358] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42794
[2025-01-17 03:38:59,358] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:38:59,358] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:38:59,388] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42794
[2025-01-17 03:38:59,388] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [30]  [ 680/1404]  eta: 0:07:09  lr: 0.000016  min_lr: 0.000000  loss: 4.2112 (4.1138)  class_acc: 0.2917 (0.3480)  loss_scale: 32768.0000 (23385.0925)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0659  max mem: 15572
Epoch: [30]  [ 690/1404]  eta: 0:07:03  lr: 0.000016  min_lr: 0.000000  loss: 4.1593 (4.1138)  class_acc: 0.3333 (0.3480)  loss_scale: 16384.0000 (23283.7742)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0321  max mem: 15572
Epoch: [30]  [ 700/1404]  eta: 0:06:56  lr: 0.000016  min_lr: 0.000000  loss: 4.2450 (4.1166)  class_acc: 0.2500 (0.3477)  loss_scale: 16384.0000 (23185.3466)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0464  max mem: 15572
Epoch: [30]  [ 710/1404]  eta: 0:06:50  lr: 0.000016  min_lr: 0.000000  loss: 4.2050 (4.1171)  class_acc: 0.2917 (0.3473)  loss_scale: 16384.0000 (23089.6878)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0505  max mem: 15572
Epoch: [30]  [ 720/1404]  eta: 0:06:45  lr: 0.000016  min_lr: 0.000000  loss: 4.1812 (4.1180)  class_acc: 0.2917 (0.3462)  loss_scale: 16384.0000 (22996.6824)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.0934  max mem: 15572
Epoch: [30]  [ 730/1404]  eta: 0:06:39  lr: 0.000016  min_lr: 0.000000  loss: 4.2417 (4.1193)  class_acc: 0.2917 (0.3456)  loss_scale: 16384.0000 (22906.2216)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0844  max mem: 15572
Epoch: [30]  [ 740/1404]  eta: 0:06:33  lr: 0.000016  min_lr: 0.000000  loss: 4.2270 (4.1197)  class_acc: 0.3750 (0.3463)  loss_scale: 16384.0000 (22818.2024)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0797  max mem: 15572
Epoch: [30]  [ 750/1404]  eta: 0:06:27  lr: 0.000016  min_lr: 0.000000  loss: 4.2217 (4.1211)  class_acc: 0.3750 (0.3462)  loss_scale: 16384.0000 (22732.5273)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0940  max mem: 15572
Epoch: [30]  [ 760/1404]  eta: 0:06:21  lr: 0.000016  min_lr: 0.000000  loss: 4.0814 (4.1203)  class_acc: 0.3333 (0.3459)  loss_scale: 16384.0000 (22649.1038)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1027  max mem: 15572
Epoch: [30]  [ 770/1404]  eta: 0:06:15  lr: 0.000016  min_lr: 0.000000  loss: 4.0200 (4.1204)  class_acc: 0.3750 (0.3458)  loss_scale: 16384.0000 (22567.8444)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0958  max mem: 15572
Epoch: [30]  [ 780/1404]  eta: 0:06:09  lr: 0.000016  min_lr: 0.000000  loss: 4.1203 (4.1186)  class_acc: 0.3750 (0.3459)  loss_scale: 16384.0000 (22488.6658)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0371  max mem: 15572
Epoch: [30]  [ 790/1404]  eta: 0:06:03  lr: 0.000016  min_lr: 0.000000  loss: 4.0734 (4.1190)  class_acc: 0.3333 (0.3454)  loss_scale: 16384.0000 (22411.4893)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0031  max mem: 15572
Epoch: [30]  [ 800/1404]  eta: 0:05:57  lr: 0.000016  min_lr: 0.000000  loss: 4.1717 (4.1206)  class_acc: 0.2917 (0.3452)  loss_scale: 16384.0000 (22336.2397)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0409  max mem: 15572
[2025-01-17 03:40:13,738] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:40:13,738] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:40:13,783] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:40:13,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [30]  [ 810/1404]  eta: 0:05:51  lr: 0.000016  min_lr: 0.000000  loss: 4.1836 (4.1209)  class_acc: 0.3333 (0.3456)  loss_scale: 16384.0000 (22424.4636)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.0601  max mem: 15572
Epoch: [30]  [ 820/1404]  eta: 0:05:45  lr: 0.000016  min_lr: 0.000000  loss: 4.0533 (4.1197)  class_acc: 0.3750 (0.3464)  loss_scale: 32768.0000 (22550.4507)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0245  max mem: 15572
[2025-01-17 03:40:24,860] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42942
[2025-01-17 03:40:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:40:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42942
[2025-01-17 03:40:24,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:40:24,862] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [ 830/1404]  eta: 0:05:39  lr: 0.000016  min_lr: 0.000000  loss: 4.0480 (4.1188)  class_acc: 0.3333 (0.3473)  loss_scale: 32768.0000 (22495.9615)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0119  max mem: 15572
Epoch: [30]  [ 840/1404]  eta: 0:05:33  lr: 0.000016  min_lr: 0.000000  loss: 4.0117 (4.1184)  class_acc: 0.3333 (0.3475)  loss_scale: 16384.0000 (22423.2866)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0074  max mem: 15572
Epoch: [30]  [ 850/1404]  eta: 0:05:27  lr: 0.000016  min_lr: 0.000000  loss: 4.0117 (4.1194)  class_acc: 0.3750 (0.3476)  loss_scale: 16384.0000 (22352.3196)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0045  max mem: 15572
Epoch: [30]  [ 860/1404]  eta: 0:05:21  lr: 0.000016  min_lr: 0.000000  loss: 4.1465 (4.1198)  class_acc: 0.3750 (0.3472)  loss_scale: 16384.0000 (22283.0012)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0139  max mem: 15572
Epoch: [30]  [ 870/1404]  eta: 0:05:15  lr: 0.000016  min_lr: 0.000000  loss: 4.1927 (4.1195)  class_acc: 0.3333 (0.3471)  loss_scale: 16384.0000 (22215.2744)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0673  max mem: 15572
[2025-01-17 03:40:58,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=43000, skipped=256, lr=[1.5225881122300058e-07, 1.5225881122300058e-07, 2.1751258746142944e-07, 2.1751258746142944e-07, 3.107322678020421e-07, 3.107322678020421e-07, 4.43903239717203e-07, 4.43903239717203e-07, 6.341474853102901e-07, 6.341474853102901e-07, 9.059249790147001e-07, 9.059249790147001e-07, 1.2941785414495716e-06, 1.2941785414495716e-06, 1.8488264877851026e-06, 1.8488264877851026e-06, 2.6411806968358607e-06, 2.6411806968358607e-06, 3.773115281194087e-06, 3.773115281194087e-06, 5.390164687420124e-06, 5.390164687420124e-06, 7.700235267743036e-06, 7.700235267743036e-06, 1.1000336096775766e-05, 1.1000336096775766e-05, 1.571476585253681e-05, 1.571476585253681e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 03:40:58,624] [INFO] [timer.py:260:stop] epoch=0/micro_step=43000/global_step=43000, RunningAvgSamplesPerSec=47.96602578570745, CurrSamplesPerSec=41.994956264682216, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [30]  [ 880/1404]  eta: 0:05:09  lr: 0.000016  min_lr: 0.000000  loss: 4.1506 (4.1196)  class_acc: 0.3333 (0.3470)  loss_scale: 16384.0000 (22149.0851)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.0897  max mem: 15572
Epoch: [30]  [ 890/1404]  eta: 0:05:03  lr: 0.000016  min_lr: 0.000000  loss: 4.1248 (4.1200)  class_acc: 0.2917 (0.3468)  loss_scale: 16384.0000 (22084.3816)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.0888  max mem: 15572
Epoch: [30]  [ 900/1404]  eta: 0:04:57  lr: 0.000016  min_lr: 0.000000  loss: 4.1464 (4.1201)  class_acc: 0.2917 (0.3465)  loss_scale: 16384.0000 (22021.1143)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1134  max mem: 15572
Epoch: [30]  [ 910/1404]  eta: 0:04:52  lr: 0.000016  min_lr: 0.000000  loss: 4.2586 (4.1222)  class_acc: 0.2500 (0.3455)  loss_scale: 16384.0000 (21959.2360)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1179  max mem: 15572
Epoch: [30]  [ 920/1404]  eta: 0:04:46  lr: 0.000016  min_lr: 0.000000  loss: 4.3186 (4.1236)  class_acc: 0.2917 (0.3458)  loss_scale: 16384.0000 (21898.7014)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0752  max mem: 15572
Epoch: [30]  [ 930/1404]  eta: 0:04:39  lr: 0.000016  min_lr: 0.000000  loss: 4.1935 (4.1246)  class_acc: 0.3333 (0.3457)  loss_scale: 16384.0000 (21839.4672)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0295  max mem: 15572
Epoch: [30]  [ 940/1404]  eta: 0:04:33  lr: 0.000016  min_lr: 0.000000  loss: 4.1278 (4.1230)  class_acc: 0.3750 (0.3458)  loss_scale: 16384.0000 (21781.4920)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0596  max mem: 15572
Epoch: [30]  [ 950/1404]  eta: 0:04:28  lr: 0.000016  min_lr: 0.000000  loss: 4.0102 (4.1231)  class_acc: 0.3750 (0.3458)  loss_scale: 16384.0000 (21724.7361)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1188  max mem: 15572
[2025-01-17 03:41:40,860] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:41:40,861] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:41:40,875] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:41:40,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [30]  [ 960/1404]  eta: 0:04:22  lr: 0.000016  min_lr: 0.000000  loss: 4.1218 (4.1233)  class_acc: 0.3333 (0.3452)  loss_scale: 16384.0000 (21839.6504)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0901  max mem: 15572
Epoch: [30]  [ 970/1404]  eta: 0:04:16  lr: 0.000016  min_lr: 0.000000  loss: 4.1895 (4.1244)  class_acc: 0.2917 (0.3455)  loss_scale: 32768.0000 (21952.1977)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1112  max mem: 15572
Epoch: [30]  [ 980/1404]  eta: 0:04:10  lr: 0.000015  min_lr: 0.000000  loss: 4.2180 (4.1256)  class_acc: 0.3750 (0.3457)  loss_scale: 32768.0000 (22062.4506)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.1247  max mem: 15572
Epoch: [30]  [ 990/1404]  eta: 0:04:04  lr: 0.000015  min_lr: 0.000000  loss: 4.2297 (4.1261)  class_acc: 0.2917 (0.3449)  loss_scale: 32768.0000 (22170.4783)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0834  max mem: 15572
Epoch: [30]  [1000/1404]  eta: 0:03:58  lr: 0.000015  min_lr: 0.000000  loss: 4.0953 (4.1252)  class_acc: 0.2917 (0.3452)  loss_scale: 32768.0000 (22276.3477)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0628  max mem: 15572
Epoch: [30]  [1010/1404]  eta: 0:03:52  lr: 0.000015  min_lr: 0.000000  loss: 4.0170 (4.1225)  class_acc: 0.3750 (0.3459)  loss_scale: 32768.0000 (22380.1227)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0555  max mem: 15572
Epoch: [30]  [1020/1404]  eta: 0:03:46  lr: 0.000015  min_lr: 0.000000  loss: 4.0095 (4.1220)  class_acc: 0.3333 (0.3450)  loss_scale: 32768.0000 (22481.8648)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0785  max mem: 15572
[2025-01-17 03:42:26,035] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43149
[2025-01-17 03:42:26,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:42:26,076] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43149
[2025-01-17 03:42:26,076] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:42:26,076] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [1030/1404]  eta: 0:03:40  lr: 0.000015  min_lr: 0.000000  loss: 4.0518 (4.1222)  class_acc: 0.2917 (0.3450)  loss_scale: 32768.0000 (22549.8506)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0633  max mem: 15572
Epoch: [30]  [1040/1404]  eta: 0:03:34  lr: 0.000015  min_lr: 0.000000  loss: 4.0443 (4.1209)  class_acc: 0.3333 (0.3456)  loss_scale: 16384.0000 (22490.6206)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0873  max mem: 15572
Epoch: [30]  [1050/1404]  eta: 0:03:28  lr: 0.000015  min_lr: 0.000000  loss: 4.0443 (4.1195)  class_acc: 0.4167 (0.3467)  loss_scale: 16384.0000 (22432.5176)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0794  max mem: 15572
Epoch: [30]  [1060/1404]  eta: 0:03:22  lr: 0.000015  min_lr: 0.000000  loss: 4.1646 (4.1212)  class_acc: 0.3333 (0.3455)  loss_scale: 16384.0000 (22375.5099)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0974  max mem: 15572
Epoch: [30]  [1070/1404]  eta: 0:03:16  lr: 0.000015  min_lr: 0.000000  loss: 4.2724 (4.1220)  class_acc: 0.2500 (0.3448)  loss_scale: 16384.0000 (22319.5668)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0916  max mem: 15572
Epoch: [30]  [1080/1404]  eta: 0:03:10  lr: 0.000015  min_lr: 0.000000  loss: 4.2132 (4.1230)  class_acc: 0.2917 (0.3441)  loss_scale: 16384.0000 (22264.6586)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0305  max mem: 15572
Epoch: [30]  [1090/1404]  eta: 0:03:05  lr: 0.000015  min_lr: 0.000000  loss: 4.2219 (4.1242)  class_acc: 0.3333 (0.3443)  loss_scale: 16384.0000 (22210.7571)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1061  max mem: 15572
Epoch: [30]  [1100/1404]  eta: 0:02:59  lr: 0.000015  min_lr: 0.000000  loss: 4.2507 (4.1252)  class_acc: 0.3333 (0.3441)  loss_scale: 16384.0000 (22157.8347)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.1449  max mem: 15572
Epoch: [30]  [1110/1404]  eta: 0:02:53  lr: 0.000015  min_lr: 0.000000  loss: 4.1371 (4.1254)  class_acc: 0.2917 (0.3444)  loss_scale: 16384.0000 (22105.8650)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1126  max mem: 15572
Epoch: [30]  [1120/1404]  eta: 0:02:47  lr: 0.000015  min_lr: 0.000000  loss: 4.0918 (4.1253)  class_acc: 0.3333 (0.3443)  loss_scale: 16384.0000 (22054.8225)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1342  max mem: 15572
Epoch: [30]  [1130/1404]  eta: 0:02:41  lr: 0.000015  min_lr: 0.000000  loss: 4.0581 (4.1253)  class_acc: 0.2917 (0.3439)  loss_scale: 16384.0000 (22004.6826)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.0920  max mem: 15572
Epoch: [30]  [1140/1404]  eta: 0:02:35  lr: 0.000015  min_lr: 0.000000  loss: 4.0581 (4.1260)  class_acc: 0.3333 (0.3442)  loss_scale: 16384.0000 (21955.4216)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.0640  max mem: 15572
Epoch: [30]  [1150/1404]  eta: 0:02:29  lr: 0.000015  min_lr: 0.000000  loss: 4.0566 (4.1254)  class_acc: 0.3333 (0.3442)  loss_scale: 16384.0000 (21907.0165)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0975  max mem: 15572
[2025-01-17 03:43:42,834] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:43:42,834] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:43:42,842] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:43:42,842] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [30]  [1160/1404]  eta: 0:02:24  lr: 0.000015  min_lr: 0.000000  loss: 4.0326 (4.1251)  class_acc: 0.3333 (0.3439)  loss_scale: 16384.0000 (21901.7812)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1138  max mem: 15572
Epoch: [30]  [1170/1404]  eta: 0:02:18  lr: 0.000015  min_lr: 0.000000  loss: 4.1993 (4.1254)  class_acc: 0.2917 (0.3434)  loss_scale: 32768.0000 (21994.5756)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1137  max mem: 15572
Epoch: [30]  [1180/1404]  eta: 0:02:12  lr: 0.000015  min_lr: 0.000000  loss: 4.1492 (4.1254)  class_acc: 0.3333 (0.3436)  loss_scale: 32768.0000 (22085.7985)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0531  max mem: 15572
Epoch: [30]  [1190/1404]  eta: 0:02:06  lr: 0.000015  min_lr: 0.000000  loss: 4.1766 (4.1246)  class_acc: 0.4167 (0.3441)  loss_scale: 32768.0000 (22175.4895)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0765  max mem: 15572
Epoch: [30]  [1200/1404]  eta: 0:02:00  lr: 0.000015  min_lr: 0.000000  loss: 4.0933 (4.1240)  class_acc: 0.2917 (0.3438)  loss_scale: 32768.0000 (22263.6869)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.1412  max mem: 15572
Epoch: [30]  [1210/1404]  eta: 0:01:54  lr: 0.000015  min_lr: 0.000000  loss: 4.0460 (4.1236)  class_acc: 0.2917 (0.3437)  loss_scale: 32768.0000 (22350.4277)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0654  max mem: 15572
Epoch: [30]  [1220/1404]  eta: 0:01:48  lr: 0.000015  min_lr: 0.000000  loss: 4.1039 (4.1239)  class_acc: 0.3333 (0.3438)  loss_scale: 32768.0000 (22435.7477)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0007  max mem: 15572
Epoch: [30]  [1230/1404]  eta: 0:01:42  lr: 0.000015  min_lr: 0.000000  loss: 4.1499 (4.1243)  class_acc: 0.3333 (0.3436)  loss_scale: 32768.0000 (22519.6816)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0426  max mem: 15572
Epoch: [30]  [1240/1404]  eta: 0:01:36  lr: 0.000015  min_lr: 0.000000  loss: 4.1692 (4.1250)  class_acc: 0.3333 (0.3439)  loss_scale: 32768.0000 (22602.2627)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0755  max mem: 15572
Epoch: [30]  [1250/1404]  eta: 0:01:30  lr: 0.000015  min_lr: 0.000000  loss: 4.1077 (4.1254)  class_acc: 0.3750 (0.3442)  loss_scale: 32768.0000 (22683.5236)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0604  max mem: 15572
Epoch: [30]  [1260/1404]  eta: 0:01:24  lr: 0.000015  min_lr: 0.000000  loss: 4.3716 (4.1269)  class_acc: 0.3750 (0.3445)  loss_scale: 32768.0000 (22763.4956)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0359  max mem: 15572
Epoch: [30]  [1270/1404]  eta: 0:01:18  lr: 0.000015  min_lr: 0.000000  loss: 4.3434 (4.1281)  class_acc: 0.3333 (0.3443)  loss_scale: 32768.0000 (22842.2093)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0615  max mem: 15572
[2025-01-17 03:44:52,419] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43397
[2025-01-17 03:44:52,419] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43397
[2025-01-17 03:44:52,420] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:44:52,420] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:44:52,420] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [1280/1404]  eta: 0:01:13  lr: 0.000015  min_lr: 0.000000  loss: 4.2455 (4.1287)  class_acc: 0.2917 (0.3443)  loss_scale: 32768.0000 (22868.5340)  weight_decay: 0.0500 (0.0500)  time: 0.6572  data: 0.1451  max mem: 15572
Epoch: [30]  [1290/1404]  eta: 0:01:07  lr: 0.000015  min_lr: 0.000000  loss: 4.2226 (4.1291)  class_acc: 0.3333 (0.3441)  loss_scale: 16384.0000 (22818.3052)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0927  max mem: 15572
Epoch: [30]  [1300/1404]  eta: 0:01:01  lr: 0.000015  min_lr: 0.000000  loss: 4.1684 (4.1296)  class_acc: 0.2917 (0.3439)  loss_scale: 16384.0000 (22768.8486)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0457  max mem: 15572
Epoch: [30]  [1310/1404]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 4.2347 (4.1301)  class_acc: 0.2917 (0.3439)  loss_scale: 16384.0000 (22720.1465)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0619  max mem: 15572
Epoch: [30]  [1320/1404]  eta: 0:00:49  lr: 0.000015  min_lr: 0.000000  loss: 4.1282 (4.1305)  class_acc: 0.2917 (0.3436)  loss_scale: 16384.0000 (22672.1817)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0239  max mem: 15572
Epoch: [30]  [1330/1404]  eta: 0:00:43  lr: 0.000015  min_lr: 0.000000  loss: 4.1242 (4.1309)  class_acc: 0.2917 (0.3434)  loss_scale: 16384.0000 (22624.9376)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.0334  max mem: 15572
Epoch: [30]  [1340/1404]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 4.2029 (4.1305)  class_acc: 0.2917 (0.3432)  loss_scale: 16384.0000 (22578.3982)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.0813  max mem: 15572
Epoch: [30]  [1350/1404]  eta: 0:00:31  lr: 0.000015  min_lr: 0.000000  loss: 4.2474 (4.1314)  class_acc: 0.3333 (0.3435)  loss_scale: 16384.0000 (22532.5477)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0774  max mem: 15572
Epoch: [30]  [1360/1404]  eta: 0:00:25  lr: 0.000015  min_lr: 0.000000  loss: 3.9956 (4.1297)  class_acc: 0.3333 (0.3434)  loss_scale: 16384.0000 (22487.3711)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0929  max mem: 15572
Epoch: [30]  [1370/1404]  eta: 0:00:20  lr: 0.000015  min_lr: 0.000000  loss: 3.9722 (4.1300)  class_acc: 0.3750 (0.3434)  loss_scale: 16384.0000 (22442.8534)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1027  max mem: 15572
Epoch: [30]  [1380/1404]  eta: 0:00:14  lr: 0.000015  min_lr: 0.000000  loss: 4.0149 (4.1290)  class_acc: 0.3750 (0.3440)  loss_scale: 16384.0000 (22398.9804)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0550  max mem: 15572
Epoch: [30]  [1390/1404]  eta: 0:00:08  lr: 0.000015  min_lr: 0.000000  loss: 4.1774 (4.1299)  class_acc: 0.4167 (0.3445)  loss_scale: 16384.0000 (22355.7383)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0235  max mem: 15572
Epoch: [30]  [1400/1404]  eta: 0:00:02  lr: 0.000015  min_lr: 0.000000  loss: 4.2665 (4.1306)  class_acc: 0.3750 (0.3447)  loss_scale: 16384.0000 (22313.1135)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0005  max mem: 15572
Epoch: [30]  [1403/1404]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 4.3109 (4.1312)  class_acc: 0.3750 (0.3449)  loss_scale: 16384.0000 (22300.4444)  weight_decay: 0.0500 (0.0500)  time: 0.4231  data: 0.0004  max mem: 15572
Epoch: [30] Total time: 0:13:44 (0.5871 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 4.3109 (4.1334)  class_acc: 0.3750 (0.3489)  loss_scale: 16384.0000 (22300.4444)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:12  loss: 1.7624 (1.7624)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 4.0653  data: 3.8261  max mem: 15572
Val:  [ 10/136]  eta: 0:01:38  loss: 2.3132 (2.2082)  acc1: 55.5556 (51.5152)  acc5: 83.3333 (82.3232)  time: 0.7836  data: 0.5701  max mem: 15572
Val:  [ 20/136]  eta: 0:01:01  loss: 2.5058 (2.3756)  acc1: 44.4444 (47.8836)  acc5: 77.7778 (78.5714)  time: 0.3576  data: 0.1553  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 2.3078 (2.3126)  acc1: 50.0000 (49.4624)  acc5: 77.7778 (79.2115)  time: 0.3116  data: 0.1183  max mem: 15572
Val:  [ 40/136]  eta: 0:00:41  loss: 2.0368 (2.2776)  acc1: 55.5556 (51.4905)  acc5: 83.3333 (79.8103)  time: 0.3221  data: 0.1214  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.1950 (2.2814)  acc1: 50.0000 (50.6536)  acc5: 83.3333 (80.1743)  time: 0.3439  data: 0.1338  max mem: 15572
Val:  [ 60/136]  eta: 0:00:30  loss: 2.2897 (2.3473)  acc1: 44.4444 (48.3607)  acc5: 77.7778 (79.0528)  time: 0.3605  data: 0.1525  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.2755 (2.3296)  acc1: 50.0000 (49.2175)  acc5: 77.7778 (79.5775)  time: 0.3404  data: 0.1443  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.1710 (2.3199)  acc1: 50.0000 (48.9026)  acc5: 88.8889 (80.2469)  time: 0.3653  data: 0.1813  max mem: 15572
Val:  [ 90/136]  eta: 0:00:17  loss: 2.2433 (2.3247)  acc1: 38.8889 (48.1074)  acc5: 83.3333 (79.9756)  time: 0.3451  data: 0.1660  max mem: 15572
Val:  [100/136]  eta: 0:00:13  loss: 2.4929 (2.3791)  acc1: 38.8889 (46.4246)  acc5: 72.2222 (78.5479)  time: 0.3459  data: 0.1596  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.5376 (2.3707)  acc1: 38.8889 (46.7467)  acc5: 72.2222 (78.6787)  time: 0.3791  data: 0.1856  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1449 (2.3341)  acc1: 55.5556 (47.6125)  acc5: 83.3333 (79.4766)  time: 0.3933  data: 0.2000  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.9378 (2.3044)  acc1: 55.5556 (48.3461)  acc5: 88.8889 (80.1951)  time: 0.2974  data: 0.1297  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.9941 (2.2996)  acc1: 55.5556 (48.5258)  acc5: 88.8889 (80.2621)  time: 0.1996  data: 0.0475  max mem: 15572
Val: Total time: 0:00:49 (0.3659 s / it)
* Acc@1 46.990 Acc@5 78.931 loss 2.346
Accuracy of the network on the 4883 val videos: 47.0%
Max accuracy: 47.52%
Epoch: [31]  [   0/1404]  eta: 2:49:25  lr: 0.000015  min_lr: 0.000000  loss: 4.0893 (4.0893)  class_acc: 0.2500 (0.2500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.2401  data: 5.9274  max mem: 15572
[2025-01-17 03:47:00,555] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:47:00,556] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:47:00,556] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:47:00,556] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:47:01,893] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43529
[2025-01-17 03:47:01,893] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:47:01,916] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43529
[2025-01-17 03:47:01,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:47:01,918] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [  10/1404]  eta: 0:27:47  lr: 0.000015  min_lr: 0.000000  loss: 4.0893 (4.1519)  class_acc: 0.4583 (0.4167)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  time: 1.1959  data: 0.5399  max mem: 15572
Epoch: [31]  [  20/1404]  eta: 0:20:50  lr: 0.000015  min_lr: 0.000000  loss: 4.1090 (4.1633)  class_acc: 0.4583 (0.4028)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0244  max mem: 15572
Epoch: [31]  [  30/1404]  eta: 0:17:39  lr: 0.000014  min_lr: 0.000000  loss: 4.1090 (4.1646)  class_acc: 0.3750 (0.3965)  loss_scale: 16384.0000 (17969.5484)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0366  max mem: 15572
Epoch: [31]  [  40/1404]  eta: 0:16:38  lr: 0.000014  min_lr: 0.000000  loss: 4.0241 (4.1149)  class_acc: 0.3750 (0.3963)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0699  max mem: 15572
Epoch: [31]  [  50/1404]  eta: 0:15:53  lr: 0.000014  min_lr: 0.000000  loss: 3.9655 (4.0787)  class_acc: 0.4167 (0.4020)  loss_scale: 16384.0000 (17347.7647)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0575  max mem: 15572
Epoch: [31]  [  60/1404]  eta: 0:15:23  lr: 0.000014  min_lr: 0.000000  loss: 3.9655 (4.0778)  class_acc: 0.3333 (0.3839)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0124  max mem: 15572
Epoch: [31]  [  70/1404]  eta: 0:14:50  lr: 0.000014  min_lr: 0.000000  loss: 4.0847 (4.0912)  class_acc: 0.3333 (0.3809)  loss_scale: 16384.0000 (17076.2817)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0124  max mem: 15572
Epoch: [31]  [  80/1404]  eta: 0:14:25  lr: 0.000014  min_lr: 0.000000  loss: 4.1428 (4.1054)  class_acc: 0.3333 (0.3771)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0213  max mem: 15572
Epoch: [31]  [  90/1404]  eta: 0:14:09  lr: 0.000014  min_lr: 0.000000  loss: 4.2237 (4.1194)  class_acc: 0.3750 (0.3755)  loss_scale: 16384.0000 (16924.1319)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0455  max mem: 15572
Epoch: [31]  [ 100/1404]  eta: 0:14:00  lr: 0.000014  min_lr: 0.000000  loss: 4.1772 (4.1141)  class_acc: 0.3333 (0.3746)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0575  max mem: 15572
Epoch: [31]  [ 110/1404]  eta: 0:13:57  lr: 0.000014  min_lr: 0.000000  loss: 4.0545 (4.1087)  class_acc: 0.3750 (0.3746)  loss_scale: 16384.0000 (16826.8108)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.1040  max mem: 15572
Epoch: [31]  [ 120/1404]  eta: 0:13:35  lr: 0.000014  min_lr: 0.000000  loss: 4.1704 (4.1077)  class_acc: 0.3333 (0.3685)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0793  max mem: 15572
Epoch: [31]  [ 130/1404]  eta: 0:13:20  lr: 0.000014  min_lr: 0.000000  loss: 4.1959 (4.1099)  class_acc: 0.2917 (0.3667)  loss_scale: 16384.0000 (16759.2061)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0369  max mem: 15572
[2025-01-17 03:48:17,426] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:48:17,426] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:48:17,426] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:48:17,427] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [ 140/1404]  eta: 0:13:13  lr: 0.000014  min_lr: 0.000000  loss: 3.9217 (4.0986)  class_acc: 0.3333 (0.3676)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0459  max mem: 15572
Epoch: [31]  [ 150/1404]  eta: 0:13:06  lr: 0.000014  min_lr: 0.000000  loss: 4.0233 (4.1077)  class_acc: 0.3750 (0.3640)  loss_scale: 32768.0000 (18554.0662)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0180  max mem: 15572
Epoch: [31]  [ 160/1404]  eta: 0:12:55  lr: 0.000014  min_lr: 0.000000  loss: 4.1348 (4.1104)  class_acc: 0.2917 (0.3610)  loss_scale: 32768.0000 (19436.9193)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.0011  max mem: 15572
Epoch: [31]  [ 170/1404]  eta: 0:12:48  lr: 0.000014  min_lr: 0.000000  loss: 4.1000 (4.1107)  class_acc: 0.3333 (0.3594)  loss_scale: 32768.0000 (20216.5146)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.0006  max mem: 15572
Epoch: [31]  [ 180/1404]  eta: 0:12:34  lr: 0.000014  min_lr: 0.000000  loss: 4.1578 (4.1141)  class_acc: 0.3333 (0.3598)  loss_scale: 32768.0000 (20909.9669)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0057  max mem: 15572
Epoch: [31]  [ 190/1404]  eta: 0:12:22  lr: 0.000014  min_lr: 0.000000  loss: 4.1135 (4.1127)  class_acc: 0.3333 (0.3575)  loss_scale: 32768.0000 (21530.8063)  weight_decay: 0.0500 (0.0500)  time: 0.5188  data: 0.0057  max mem: 15572
Epoch: [31]  [ 200/1404]  eta: 0:12:14  lr: 0.000014  min_lr: 0.000000  loss: 4.1438 (4.1205)  class_acc: 0.2917 (0.3543)  loss_scale: 32768.0000 (22089.8706)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0084  max mem: 15572
Epoch: [31]  [ 210/1404]  eta: 0:12:11  lr: 0.000014  min_lr: 0.000000  loss: 4.1611 (4.1144)  class_acc: 0.3333 (0.3558)  loss_scale: 32768.0000 (22595.9431)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.0809  max mem: 15572
[2025-01-17 03:49:06,930] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43744
[2025-01-17 03:49:06,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:49:06,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:49:06,931] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43744
[2025-01-17 03:49:06,932] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [31]  [ 220/1404]  eta: 0:12:02  lr: 0.000014  min_lr: 0.000000  loss: 4.0816 (4.1142)  class_acc: 0.2917 (0.3541)  loss_scale: 32768.0000 (22982.0814)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1155  max mem: 15572
Epoch: [31]  [ 230/1404]  eta: 0:11:59  lr: 0.000014  min_lr: 0.000000  loss: 4.1531 (4.1159)  class_acc: 0.2917 (0.3564)  loss_scale: 16384.0000 (22696.4502)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1218  max mem: 15572
Epoch: [31]  [ 240/1404]  eta: 0:11:52  lr: 0.000014  min_lr: 0.000000  loss: 4.1130 (4.1099)  class_acc: 0.3750 (0.3568)  loss_scale: 16384.0000 (22434.5228)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.1204  max mem: 15572
Epoch: [31]  [ 250/1404]  eta: 0:11:43  lr: 0.000014  min_lr: 0.000000  loss: 4.0343 (4.1089)  class_acc: 0.3750 (0.3579)  loss_scale: 16384.0000 (22193.4661)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0722  max mem: 15572
Epoch: [31]  [ 260/1404]  eta: 0:11:33  lr: 0.000014  min_lr: 0.000000  loss: 4.1801 (4.1104)  class_acc: 0.3750 (0.3586)  loss_scale: 16384.0000 (21970.8812)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0431  max mem: 15572
Epoch: [31]  [ 270/1404]  eta: 0:11:24  lr: 0.000014  min_lr: 0.000000  loss: 4.1037 (4.1106)  class_acc: 0.3333 (0.3565)  loss_scale: 16384.0000 (21764.7232)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0333  max mem: 15572
Epoch: [31]  [ 280/1404]  eta: 0:11:16  lr: 0.000014  min_lr: 0.000000  loss: 4.0709 (4.1107)  class_acc: 0.3333 (0.3578)  loss_scale: 16384.0000 (21573.2384)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0550  max mem: 15572
Epoch: [31]  [ 290/1404]  eta: 0:11:10  lr: 0.000014  min_lr: 0.000000  loss: 4.0678 (4.1150)  class_acc: 0.3750 (0.3592)  loss_scale: 16384.0000 (21394.9141)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0686  max mem: 15572
Epoch: [31]  [ 300/1404]  eta: 0:11:08  lr: 0.000014  min_lr: 0.000000  loss: 4.1952 (4.1211)  class_acc: 0.2917 (0.3560)  loss_scale: 16384.0000 (21228.4385)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.1362  max mem: 15572
Epoch: [31]  [ 310/1404]  eta: 0:11:00  lr: 0.000014  min_lr: 0.000000  loss: 4.2700 (4.1258)  class_acc: 0.2917 (0.3561)  loss_scale: 16384.0000 (21072.6688)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1333  max mem: 15572
Epoch: [31]  [ 320/1404]  eta: 0:10:55  lr: 0.000014  min_lr: 0.000000  loss: 4.0128 (4.1224)  class_acc: 0.3333 (0.3567)  loss_scale: 16384.0000 (20926.6044)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1071  max mem: 15572
Epoch: [31]  [ 330/1404]  eta: 0:10:47  lr: 0.000014  min_lr: 0.000000  loss: 4.0937 (4.1232)  class_acc: 0.3750 (0.3556)  loss_scale: 16384.0000 (20789.3656)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0922  max mem: 15572
Epoch: [31]  [ 340/1404]  eta: 0:10:44  lr: 0.000014  min_lr: 0.000000  loss: 4.1427 (4.1221)  class_acc: 0.3750 (0.3550)  loss_scale: 16384.0000 (20660.1760)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1102  max mem: 15572
[2025-01-17 03:50:23,181] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:50:23,181] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:50:23,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:50:23,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [ 350/1404]  eta: 0:10:35  lr: 0.000014  min_lr: 0.000000  loss: 3.9931 (4.1218)  class_acc: 0.3750 (0.3565)  loss_scale: 16384.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0934  max mem: 15572
Epoch: [31]  [ 360/1404]  eta: 0:10:27  lr: 0.000014  min_lr: 0.000000  loss: 4.0822 (4.1226)  class_acc: 0.3333 (0.3555)  loss_scale: 32768.0000 (20967.8892)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0313  max mem: 15572
Epoch: [31]  [ 370/1404]  eta: 0:10:20  lr: 0.000014  min_lr: 0.000000  loss: 4.1184 (4.1190)  class_acc: 0.3750 (0.3562)  loss_scale: 32768.0000 (21285.9515)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0775  max mem: 15572
Epoch: [31]  [ 380/1404]  eta: 0:10:13  lr: 0.000014  min_lr: 0.000000  loss: 4.1475 (4.1206)  class_acc: 0.3750 (0.3561)  loss_scale: 32768.0000 (21587.3176)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0861  max mem: 15572
Epoch: [31]  [ 390/1404]  eta: 0:10:07  lr: 0.000014  min_lr: 0.000000  loss: 4.2044 (4.1236)  class_acc: 0.3333 (0.3553)  loss_scale: 32768.0000 (21873.2685)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0763  max mem: 15572
Epoch: [31]  [ 400/1404]  eta: 0:10:00  lr: 0.000014  min_lr: 0.000000  loss: 4.2300 (4.1252)  class_acc: 0.2917 (0.3538)  loss_scale: 32768.0000 (22144.9576)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0748  max mem: 15572
[2025-01-17 03:50:55,520] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43928
[2025-01-17 03:50:55,521] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:50:55,612] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43928
[2025-01-17 03:50:55,612] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:50:55,612] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [ 410/1404]  eta: 0:09:55  lr: 0.000014  min_lr: 0.000000  loss: 4.2047 (4.1272)  class_acc: 0.2917 (0.3522)  loss_scale: 32768.0000 (22124.3796)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1148  max mem: 15572
Epoch: [31]  [ 420/1404]  eta: 0:09:49  lr: 0.000014  min_lr: 0.000000  loss: 4.1882 (4.1296)  class_acc: 0.3333 (0.3534)  loss_scale: 16384.0000 (21988.0285)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1188  max mem: 15572
Epoch: [31]  [ 430/1404]  eta: 0:09:42  lr: 0.000014  min_lr: 0.000000  loss: 4.2186 (4.1324)  class_acc: 0.3333 (0.3529)  loss_scale: 16384.0000 (21858.0046)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0733  max mem: 15572
Epoch: [31]  [ 440/1404]  eta: 0:09:37  lr: 0.000014  min_lr: 0.000000  loss: 4.1647 (4.1319)  class_acc: 0.2917 (0.3519)  loss_scale: 16384.0000 (21733.8776)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1113  max mem: 15572
Epoch: [31]  [ 450/1404]  eta: 0:09:32  lr: 0.000014  min_lr: 0.000000  loss: 4.0916 (4.1313)  class_acc: 0.3333 (0.3518)  loss_scale: 16384.0000 (21615.2550)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.1487  max mem: 15572
Epoch: [31]  [ 460/1404]  eta: 0:09:26  lr: 0.000014  min_lr: 0.000000  loss: 4.0916 (4.1327)  class_acc: 0.3333 (0.3514)  loss_scale: 16384.0000 (21501.7787)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1229  max mem: 15572
Epoch: [31]  [ 470/1404]  eta: 0:09:18  lr: 0.000014  min_lr: 0.000000  loss: 4.1439 (4.1323)  class_acc: 0.2917 (0.3507)  loss_scale: 16384.0000 (21393.1210)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0546  max mem: 15572
[2025-01-17 03:51:37,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=44000, skipped=261, lr=[1.3126459349697277e-07, 1.3126459349697277e-07, 1.875208478528183e-07, 1.875208478528183e-07, 2.678869255040261e-07, 2.678869255040261e-07, 3.826956078628945e-07, 3.826956078628945e-07, 5.467080112327065e-07, 5.467080112327065e-07, 7.810114446181522e-07, 7.810114446181522e-07, 1.115730635168789e-06, 1.115730635168789e-06, 1.5939009073839843e-06, 1.5939009073839843e-06, 2.2770012962628343e-06, 2.2770012962628343e-06, 3.2528589946611926e-06, 3.2528589946611926e-06, 4.6469414209445606e-06, 4.6469414209445606e-06, 6.6384877442065164e-06, 6.6384877442065164e-06, 9.483553920295024e-06, 9.483553920295024e-06, 1.3547934171850035e-05, 1.3547934171850035e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 03:51:37,188] [INFO] [timer.py:260:stop] epoch=0/micro_step=44000/global_step=44000, RunningAvgSamplesPerSec=48.00929441178484, CurrSamplesPerSec=51.61121915079611, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [31]  [ 480/1404]  eta: 0:09:12  lr: 0.000014  min_lr: 0.000000  loss: 4.2708 (4.1373)  class_acc: 0.2917 (0.3512)  loss_scale: 16384.0000 (21288.9813)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0315  max mem: 15572
Epoch: [31]  [ 490/1404]  eta: 0:09:07  lr: 0.000014  min_lr: 0.000000  loss: 4.2546 (4.1343)  class_acc: 0.3750 (0.3520)  loss_scale: 16384.0000 (21189.0835)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0972  max mem: 15572
Epoch: [31]  [ 500/1404]  eta: 0:08:59  lr: 0.000013  min_lr: 0.000000  loss: 4.0024 (4.1309)  class_acc: 0.4167 (0.3527)  loss_scale: 16384.0000 (21093.1737)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0663  max mem: 15572
Epoch: [31]  [ 510/1404]  eta: 0:08:52  lr: 0.000013  min_lr: 0.000000  loss: 4.2055 (4.1359)  class_acc: 0.3333 (0.3522)  loss_scale: 16384.0000 (21001.0176)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0071  max mem: 15572
Epoch: [31]  [ 520/1404]  eta: 0:08:45  lr: 0.000013  min_lr: 0.000000  loss: 4.2294 (4.1355)  class_acc: 0.3333 (0.3530)  loss_scale: 16384.0000 (20912.3992)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0285  max mem: 15572
Epoch: [31]  [ 530/1404]  eta: 0:08:39  lr: 0.000013  min_lr: 0.000000  loss: 4.0773 (4.1337)  class_acc: 0.3333 (0.3524)  loss_scale: 16384.0000 (20827.1186)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0596  max mem: 15572
[2025-01-17 03:52:09,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:52:09,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:52:09,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:52:09,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [ 540/1404]  eta: 0:08:32  lr: 0.000013  min_lr: 0.000000  loss: 4.0773 (4.1340)  class_acc: 0.2917 (0.3524)  loss_scale: 16384.0000 (20987.2680)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0422  max mem: 15572
Epoch: [31]  [ 550/1404]  eta: 0:08:27  lr: 0.000013  min_lr: 0.000000  loss: 4.0927 (4.1322)  class_acc: 0.2917 (0.3519)  loss_scale: 32768.0000 (21201.0744)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0049  max mem: 15572
Epoch: [31]  [ 560/1404]  eta: 0:08:20  lr: 0.000013  min_lr: 0.000000  loss: 4.1078 (4.1333)  class_acc: 0.3333 (0.3507)  loss_scale: 32768.0000 (21407.2585)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.0343  max mem: 15572
Epoch: [31]  [ 570/1404]  eta: 0:08:13  lr: 0.000013  min_lr: 0.000000  loss: 4.0481 (4.1303)  class_acc: 0.3333 (0.3507)  loss_scale: 32768.0000 (21606.2207)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0437  max mem: 15572
Epoch: [31]  [ 580/1404]  eta: 0:08:07  lr: 0.000013  min_lr: 0.000000  loss: 4.0481 (4.1327)  class_acc: 0.2917 (0.3508)  loss_scale: 32768.0000 (21798.3339)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0489  max mem: 15572
Epoch: [31]  [ 590/1404]  eta: 0:08:02  lr: 0.000013  min_lr: 0.000000  loss: 4.2316 (4.1332)  class_acc: 0.2917 (0.3505)  loss_scale: 32768.0000 (21983.9459)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1236  max mem: 15572
Epoch: [31]  [ 600/1404]  eta: 0:07:57  lr: 0.000013  min_lr: 0.000000  loss: 4.2316 (4.1344)  class_acc: 0.2917 (0.3507)  loss_scale: 32768.0000 (22163.3810)  weight_decay: 0.0500 (0.0500)  time: 0.6616  data: 0.1636  max mem: 15572
Epoch: [31]  [ 610/1404]  eta: 0:07:51  lr: 0.000013  min_lr: 0.000000  loss: 4.1215 (4.1346)  class_acc: 0.3333 (0.3510)  loss_scale: 32768.0000 (22336.9427)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1129  max mem: 15572
Epoch: [31]  [ 620/1404]  eta: 0:07:45  lr: 0.000013  min_lr: 0.000000  loss: 4.1392 (4.1368)  class_acc: 0.3333 (0.3519)  loss_scale: 32768.0000 (22504.9147)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0813  max mem: 15572
Epoch: [31]  [ 630/1404]  eta: 0:07:39  lr: 0.000013  min_lr: 0.000000  loss: 4.1234 (4.1361)  class_acc: 0.3750 (0.3524)  loss_scale: 32768.0000 (22667.5626)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0736  max mem: 15572
Epoch: [31]  [ 640/1404]  eta: 0:07:32  lr: 0.000013  min_lr: 0.000000  loss: 4.1234 (4.1375)  class_acc: 0.3750 (0.3511)  loss_scale: 32768.0000 (22825.1357)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0506  max mem: 15572
Epoch: [31]  [ 650/1404]  eta: 0:07:27  lr: 0.000013  min_lr: 0.000000  loss: 4.0665 (4.1351)  class_acc: 0.2917 (0.3510)  loss_scale: 32768.0000 (22977.8679)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1034  max mem: 15572
Epoch: [31]  [ 660/1404]  eta: 0:07:21  lr: 0.000013  min_lr: 0.000000  loss: 4.0665 (4.1357)  class_acc: 0.2917 (0.3500)  loss_scale: 32768.0000 (23125.9788)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1106  max mem: 15572
[2025-01-17 03:53:24,677] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:53:24,677] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 03:53:24,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:53:24,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 03:53:27,193] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44189
[2025-01-17 03:53:27,193] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 03:53:27,193] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44189
[2025-01-17 03:53:27,193] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 03:53:27,193] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 03:53:28,095] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44191
[2025-01-17 03:53:28,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:53:28,095] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:53:28,105] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44191
[2025-01-17 03:53:28,106] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [31]  [ 670/1404]  eta: 0:07:14  lr: 0.000013  min_lr: 0.000000  loss: 4.1527 (4.1348)  class_acc: 0.2917 (0.3500)  loss_scale: 32768.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0606  max mem: 15572
Epoch: [31]  [ 680/1404]  eta: 0:07:08  lr: 0.000013  min_lr: 0.000000  loss: 4.1728 (4.1361)  class_acc: 0.2917 (0.3486)  loss_scale: 16384.0000 (23264.7988)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0964  max mem: 15572
Epoch: [31]  [ 690/1404]  eta: 0:07:03  lr: 0.000013  min_lr: 0.000000  loss: 4.1728 (4.1358)  class_acc: 0.2917 (0.3490)  loss_scale: 16384.0000 (23165.2214)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1217  max mem: 15572
Epoch: [31]  [ 700/1404]  eta: 0:06:57  lr: 0.000013  min_lr: 0.000000  loss: 4.1438 (4.1364)  class_acc: 0.3750 (0.3497)  loss_scale: 16384.0000 (23068.4850)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1002  max mem: 15572
Epoch: [31]  [ 710/1404]  eta: 0:06:51  lr: 0.000013  min_lr: 0.000000  loss: 4.1340 (4.1373)  class_acc: 0.3333 (0.3488)  loss_scale: 16384.0000 (22974.4698)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1158  max mem: 15572
Epoch: [31]  [ 720/1404]  eta: 0:06:45  lr: 0.000013  min_lr: 0.000000  loss: 4.1760 (4.1381)  class_acc: 0.2500 (0.3483)  loss_scale: 16384.0000 (22883.0624)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0985  max mem: 15572
Epoch: [31]  [ 730/1404]  eta: 0:06:39  lr: 0.000013  min_lr: 0.000000  loss: 4.2084 (4.1385)  class_acc: 0.2917 (0.3477)  loss_scale: 16384.0000 (22794.1560)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0661  max mem: 15572
Epoch: [31]  [ 740/1404]  eta: 0:06:32  lr: 0.000013  min_lr: 0.000000  loss: 4.1937 (4.1389)  class_acc: 0.3750 (0.3481)  loss_scale: 16384.0000 (22707.6491)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0702  max mem: 15572
Epoch: [31]  [ 750/1404]  eta: 0:06:27  lr: 0.000013  min_lr: 0.000000  loss: 4.0916 (4.1384)  class_acc: 0.3750 (0.3483)  loss_scale: 16384.0000 (22623.4461)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1000  max mem: 15572
Epoch: [31]  [ 760/1404]  eta: 0:06:20  lr: 0.000013  min_lr: 0.000000  loss: 4.0863 (4.1358)  class_acc: 0.3750 (0.3487)  loss_scale: 16384.0000 (22541.4560)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0898  max mem: 15572
Epoch: [31]  [ 770/1404]  eta: 0:06:14  lr: 0.000013  min_lr: 0.000000  loss: 4.1536 (4.1357)  class_acc: 0.3750 (0.3492)  loss_scale: 16384.0000 (22461.5927)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0645  max mem: 15572
Epoch: [31]  [ 780/1404]  eta: 0:06:08  lr: 0.000013  min_lr: 0.000000  loss: 4.1536 (4.1353)  class_acc: 0.3750 (0.3496)  loss_scale: 16384.0000 (22383.7746)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0908  max mem: 15572
Epoch: [31]  [ 790/1404]  eta: 0:06:02  lr: 0.000013  min_lr: 0.000000  loss: 4.1754 (4.1370)  class_acc: 0.3333 (0.3491)  loss_scale: 16384.0000 (22307.9241)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0847  max mem: 15572
[2025-01-17 03:54:43,133] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:54:43,133] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:54:43,139] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:54:43,140] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [ 800/1404]  eta: 0:05:56  lr: 0.000013  min_lr: 0.000000  loss: 4.1717 (4.1362)  class_acc: 0.3333 (0.3497)  loss_scale: 16384.0000 (22336.2397)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0466  max mem: 15572
Epoch: [31]  [ 810/1404]  eta: 0:05:50  lr: 0.000013  min_lr: 0.000000  loss: 4.1061 (4.1363)  class_acc: 0.3750 (0.3494)  loss_scale: 32768.0000 (22464.8681)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0256  max mem: 15572
Epoch: [31]  [ 820/1404]  eta: 0:05:44  lr: 0.000013  min_lr: 0.000000  loss: 4.1929 (4.1346)  class_acc: 0.3333 (0.3495)  loss_scale: 32768.0000 (22590.3630)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0338  max mem: 15572
[2025-01-17 03:54:59,444] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44349
[2025-01-17 03:54:59,444] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:54:59,444] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:54:59,448] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44349
[2025-01-17 03:54:59,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [31]  [ 830/1404]  eta: 0:05:38  lr: 0.000013  min_lr: 0.000000  loss: 4.2000 (4.1368)  class_acc: 0.2917 (0.3486)  loss_scale: 32768.0000 (22594.5415)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.0224  max mem: 15572
Epoch: [31]  [ 840/1404]  eta: 0:05:33  lr: 0.000013  min_lr: 0.000000  loss: 4.2601 (4.1368)  class_acc: 0.2500 (0.3486)  loss_scale: 16384.0000 (22520.6944)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.0759  max mem: 15572
Epoch: [31]  [ 850/1404]  eta: 0:05:27  lr: 0.000013  min_lr: 0.000000  loss: 4.1865 (4.1374)  class_acc: 0.3333 (0.3486)  loss_scale: 16384.0000 (22448.5828)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1037  max mem: 15572
Epoch: [31]  [ 860/1404]  eta: 0:05:21  lr: 0.000013  min_lr: 0.000000  loss: 4.1627 (4.1376)  class_acc: 0.3333 (0.3486)  loss_scale: 16384.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0318  max mem: 15572
Epoch: [31]  [ 870/1404]  eta: 0:05:15  lr: 0.000013  min_lr: 0.000000  loss: 4.1382 (4.1368)  class_acc: 0.3333 (0.3483)  loss_scale: 16384.0000 (22309.3272)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.0011  max mem: 15572
Epoch: [31]  [ 880/1404]  eta: 0:05:10  lr: 0.000013  min_lr: 0.000000  loss: 4.1648 (4.1374)  class_acc: 0.3750 (0.3489)  loss_scale: 16384.0000 (22242.0704)  weight_decay: 0.0500 (0.0500)  time: 0.6521  data: 0.0011  max mem: 15572
Epoch: [31]  [ 890/1404]  eta: 0:05:03  lr: 0.000013  min_lr: 0.000000  loss: 4.1447 (4.1357)  class_acc: 0.3750 (0.3493)  loss_scale: 16384.0000 (22176.3232)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0007  max mem: 15572
Epoch: [31]  [ 900/1404]  eta: 0:04:58  lr: 0.000013  min_lr: 0.000000  loss: 4.1075 (4.1341)  class_acc: 0.3750 (0.3491)  loss_scale: 16384.0000 (22112.0355)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0006  max mem: 15572
Epoch: [31]  [ 910/1404]  eta: 0:04:52  lr: 0.000013  min_lr: 0.000000  loss: 4.1075 (4.1351)  class_acc: 0.3750 (0.3494)  loss_scale: 16384.0000 (22049.1592)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.0007  max mem: 15572
Epoch: [31]  [ 920/1404]  eta: 0:04:46  lr: 0.000013  min_lr: 0.000000  loss: 4.1227 (4.1351)  class_acc: 0.3750 (0.3499)  loss_scale: 16384.0000 (21987.6482)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0007  max mem: 15572
Epoch: [31]  [ 930/1404]  eta: 0:04:39  lr: 0.000013  min_lr: 0.000000  loss: 4.0738 (4.1340)  class_acc: 0.3333 (0.3500)  loss_scale: 16384.0000 (21927.4586)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0005  max mem: 15572
Epoch: [31]  [ 940/1404]  eta: 0:04:33  lr: 0.000013  min_lr: 0.000000  loss: 4.1454 (4.1349)  class_acc: 0.3333 (0.3500)  loss_scale: 16384.0000 (21868.5484)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0005  max mem: 15572
Epoch: [31]  [ 950/1404]  eta: 0:04:28  lr: 0.000013  min_lr: 0.000000  loss: 4.1629 (4.1340)  class_acc: 0.3333 (0.3497)  loss_scale: 16384.0000 (21810.8770)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0007  max mem: 15572
[2025-01-17 03:56:15,883] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:56:15,883] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:56:15,884] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:56:15,884] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [ 960/1404]  eta: 0:04:22  lr: 0.000013  min_lr: 0.000000  loss: 4.1527 (4.1336)  class_acc: 0.3333 (0.3496)  loss_scale: 16384.0000 (21873.7482)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0007  max mem: 15572
Epoch: [31]  [ 970/1404]  eta: 0:04:16  lr: 0.000013  min_lr: 0.000000  loss: 4.1527 (4.1341)  class_acc: 0.3750 (0.3495)  loss_scale: 32768.0000 (21985.9444)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0007  max mem: 15572
Epoch: [31]  [ 980/1404]  eta: 0:04:10  lr: 0.000013  min_lr: 0.000000  loss: 4.1126 (4.1335)  class_acc: 0.3333 (0.3495)  loss_scale: 32768.0000 (22095.8532)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0006  max mem: 15572
Epoch: [31]  [ 990/1404]  eta: 0:04:04  lr: 0.000012  min_lr: 0.000000  loss: 4.1126 (4.1333)  class_acc: 0.2917 (0.3493)  loss_scale: 32768.0000 (22203.5439)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0006  max mem: 15572
Epoch: [31]  [1000/1404]  eta: 0:03:58  lr: 0.000012  min_lr: 0.000000  loss: 4.1264 (4.1331)  class_acc: 0.2917 (0.3494)  loss_scale: 32768.0000 (22309.0829)  weight_decay: 0.0500 (0.0500)  time: 0.5111  data: 0.0007  max mem: 15572
[2025-01-17 03:56:47,049] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44533
[2025-01-17 03:56:47,049] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:56:47,049] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:56:47,060] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44533
[2025-01-17 03:56:47,060] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [31]  [1010/1404]  eta: 0:03:51  lr: 0.000012  min_lr: 0.000000  loss: 4.0470 (4.1311)  class_acc: 0.3750 (0.3501)  loss_scale: 32768.0000 (22380.1227)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0064  max mem: 15572
Epoch: [31]  [1020/1404]  eta: 0:03:46  lr: 0.000012  min_lr: 0.000000  loss: 4.1048 (4.1322)  class_acc: 0.3333 (0.3491)  loss_scale: 16384.0000 (22321.3947)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0744  max mem: 15572
Epoch: [31]  [1030/1404]  eta: 0:03:40  lr: 0.000012  min_lr: 0.000000  loss: 4.0739 (4.1316)  class_acc: 0.2917 (0.3494)  loss_scale: 16384.0000 (22263.8060)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.1542  max mem: 15572
Epoch: [31]  [1040/1404]  eta: 0:03:34  lr: 0.000012  min_lr: 0.000000  loss: 4.0263 (4.1300)  class_acc: 0.3750 (0.3499)  loss_scale: 16384.0000 (22207.3237)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1529  max mem: 15572
Epoch: [31]  [1050/1404]  eta: 0:03:28  lr: 0.000012  min_lr: 0.000000  loss: 4.0664 (4.1304)  class_acc: 0.3333 (0.3500)  loss_scale: 16384.0000 (22151.9163)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1070  max mem: 15572
Epoch: [31]  [1060/1404]  eta: 0:03:22  lr: 0.000012  min_lr: 0.000000  loss: 4.2278 (4.1312)  class_acc: 0.3333 (0.3500)  loss_scale: 16384.0000 (22097.5533)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0489  max mem: 15572
Epoch: [31]  [1070/1404]  eta: 0:03:16  lr: 0.000012  min_lr: 0.000000  loss: 4.1983 (4.1312)  class_acc: 0.3333 (0.3501)  loss_scale: 16384.0000 (22044.2054)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0418  max mem: 15572
Epoch: [31]  [1080/1404]  eta: 0:03:11  lr: 0.000012  min_lr: 0.000000  loss: 4.1828 (4.1319)  class_acc: 0.2917 (0.3500)  loss_scale: 16384.0000 (21991.8446)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0984  max mem: 15572
Epoch: [31]  [1090/1404]  eta: 0:03:05  lr: 0.000012  min_lr: 0.000000  loss: 4.1828 (4.1319)  class_acc: 0.2917 (0.3499)  loss_scale: 16384.0000 (21940.4436)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1009  max mem: 15572
Epoch: [31]  [1100/1404]  eta: 0:02:59  lr: 0.000012  min_lr: 0.000000  loss: 4.1890 (4.1321)  class_acc: 0.2917 (0.3496)  loss_scale: 16384.0000 (21889.9764)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1141  max mem: 15572
Epoch: [31]  [1110/1404]  eta: 0:02:53  lr: 0.000012  min_lr: 0.000000  loss: 4.1998 (4.1330)  class_acc: 0.3333 (0.3498)  loss_scale: 16384.0000 (21840.4176)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1128  max mem: 15572
Epoch: [31]  [1120/1404]  eta: 0:02:47  lr: 0.000012  min_lr: 0.000000  loss: 4.1480 (4.1339)  class_acc: 0.2917 (0.3488)  loss_scale: 16384.0000 (21791.7431)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0452  max mem: 15572
Epoch: [31]  [1130/1404]  eta: 0:02:41  lr: 0.000012  min_lr: 0.000000  loss: 4.2396 (4.1347)  class_acc: 0.2500 (0.3488)  loss_scale: 16384.0000 (21743.9293)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0244  max mem: 15572
[2025-01-17 03:58:02,694] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:58:02,694] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 03:58:02,700] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 03:58:02,700] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [1140/1404]  eta: 0:02:35  lr: 0.000012  min_lr: 0.000000  loss: 4.2296 (4.1347)  class_acc: 0.2917 (0.3482)  loss_scale: 16384.0000 (21740.0316)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0623  max mem: 15572
Epoch: [31]  [1150/1404]  eta: 0:02:29  lr: 0.000012  min_lr: 0.000000  loss: 4.1782 (4.1346)  class_acc: 0.3750 (0.3486)  loss_scale: 32768.0000 (21835.8436)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.0495  max mem: 15572
Epoch: [31]  [1160/1404]  eta: 0:02:23  lr: 0.000012  min_lr: 0.000000  loss: 4.1197 (4.1345)  class_acc: 0.3750 (0.3483)  loss_scale: 32768.0000 (21930.0052)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0005  max mem: 15572
Epoch: [31]  [1170/1404]  eta: 0:02:17  lr: 0.000012  min_lr: 0.000000  loss: 4.0290 (4.1336)  class_acc: 0.3750 (0.3487)  loss_scale: 32768.0000 (22022.5585)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.0007  max mem: 15572
Epoch: [31]  [1180/1404]  eta: 0:02:11  lr: 0.000012  min_lr: 0.000000  loss: 4.1044 (4.1341)  class_acc: 0.3750 (0.3488)  loss_scale: 32768.0000 (22113.5445)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0008  max mem: 15572
Epoch: [31]  [1190/1404]  eta: 0:02:06  lr: 0.000012  min_lr: 0.000000  loss: 4.1962 (4.1346)  class_acc: 0.3333 (0.3486)  loss_scale: 32768.0000 (22203.0025)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0006  max mem: 15572
Epoch: [31]  [1200/1404]  eta: 0:02:00  lr: 0.000012  min_lr: 0.000000  loss: 4.2064 (4.1355)  class_acc: 0.3333 (0.3484)  loss_scale: 32768.0000 (22290.9709)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0005  max mem: 15572
Epoch: [31]  [1210/1404]  eta: 0:01:54  lr: 0.000012  min_lr: 0.000000  loss: 4.2620 (4.1363)  class_acc: 0.3333 (0.3485)  loss_scale: 32768.0000 (22377.4864)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0006  max mem: 15572
Epoch: [31]  [1220/1404]  eta: 0:01:48  lr: 0.000012  min_lr: 0.000000  loss: 4.1436 (4.1366)  class_acc: 0.3333 (0.3488)  loss_scale: 32768.0000 (22462.5848)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0009  max mem: 15572
Epoch: [31]  [1230/1404]  eta: 0:01:42  lr: 0.000012  min_lr: 0.000000  loss: 4.0989 (4.1367)  class_acc: 0.3333 (0.3488)  loss_scale: 32768.0000 (22546.3006)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0009  max mem: 15572
Epoch: [31]  [1240/1404]  eta: 0:01:36  lr: 0.000012  min_lr: 0.000000  loss: 4.0370 (4.1365)  class_acc: 0.3333 (0.3486)  loss_scale: 32768.0000 (22628.6672)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0007  max mem: 15572
Epoch: [31]  [1250/1404]  eta: 0:01:30  lr: 0.000012  min_lr: 0.000000  loss: 4.2104 (4.1370)  class_acc: 0.3333 (0.3487)  loss_scale: 32768.0000 (22709.7170)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0006  max mem: 15572
[2025-01-17 03:59:11,658] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44778
[2025-01-17 03:59:11,658] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 03:59:11,658] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 03:59:11,797] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44778
[2025-01-17 03:59:11,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [31]  [1260/1404]  eta: 0:01:24  lr: 0.000012  min_lr: 0.000000  loss: 4.1006 (4.1367)  class_acc: 0.3333 (0.3488)  loss_scale: 32768.0000 (22698.5313)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0005  max mem: 15572
Epoch: [31]  [1270/1404]  eta: 0:01:18  lr: 0.000012  min_lr: 0.000000  loss: 4.2122 (4.1385)  class_acc: 0.3750 (0.3488)  loss_scale: 16384.0000 (22648.8497)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.0006  max mem: 15572
Epoch: [31]  [1280/1404]  eta: 0:01:12  lr: 0.000012  min_lr: 0.000000  loss: 4.2846 (4.1385)  class_acc: 0.3750 (0.3489)  loss_scale: 16384.0000 (22599.9438)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0008  max mem: 15572
Epoch: [31]  [1290/1404]  eta: 0:01:07  lr: 0.000012  min_lr: 0.000000  loss: 4.0600 (4.1373)  class_acc: 0.3750 (0.3491)  loss_scale: 16384.0000 (22551.7955)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0006  max mem: 15572
Epoch: [31]  [1300/1404]  eta: 0:01:01  lr: 0.000012  min_lr: 0.000000  loss: 4.0351 (4.1368)  class_acc: 0.3750 (0.3498)  loss_scale: 16384.0000 (22504.3874)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.0310  max mem: 15572
Epoch: [31]  [1310/1404]  eta: 0:00:55  lr: 0.000012  min_lr: 0.000000  loss: 4.1691 (4.1381)  class_acc: 0.3333 (0.3497)  loss_scale: 16384.0000 (22457.7025)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0855  max mem: 15572
Epoch: [31]  [1320/1404]  eta: 0:00:49  lr: 0.000012  min_lr: 0.000000  loss: 4.1691 (4.1374)  class_acc: 0.4167 (0.3502)  loss_scale: 16384.0000 (22411.7245)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.1473  max mem: 15572
Epoch: [31]  [1330/1404]  eta: 0:00:43  lr: 0.000012  min_lr: 0.000000  loss: 4.0688 (4.1376)  class_acc: 0.4167 (0.3504)  loss_scale: 16384.0000 (22366.4373)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1001  max mem: 15572
Epoch: [31]  [1340/1404]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000000  loss: 4.0757 (4.1382)  class_acc: 0.3750 (0.3510)  loss_scale: 16384.0000 (22321.8255)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0601  max mem: 15572
Epoch: [31]  [1350/1404]  eta: 0:00:31  lr: 0.000012  min_lr: 0.000000  loss: 4.1084 (4.1376)  class_acc: 0.3750 (0.3512)  loss_scale: 16384.0000 (22277.8742)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0718  max mem: 15572
Epoch: [31]  [1360/1404]  eta: 0:00:25  lr: 0.000012  min_lr: 0.000000  loss: 4.1084 (4.1369)  class_acc: 0.3750 (0.3514)  loss_scale: 16384.0000 (22234.5687)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0330  max mem: 15572
Epoch: [31]  [1370/1404]  eta: 0:00:19  lr: 0.000012  min_lr: 0.000000  loss: 4.1527 (4.1373)  class_acc: 0.3750 (0.3517)  loss_scale: 16384.0000 (22191.8950)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0592  max mem: 15572
Epoch: [31]  [1380/1404]  eta: 0:00:14  lr: 0.000012  min_lr: 0.000000  loss: 4.0835 (4.1372)  class_acc: 0.3750 (0.3519)  loss_scale: 16384.0000 (22149.8392)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0557  max mem: 15572
[2025-01-17 04:00:25,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:00:25,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:00:25,372] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:00:25,373] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [1390/1404]  eta: 0:00:08  lr: 0.000012  min_lr: 0.000000  loss: 4.1197 (4.1374)  class_acc: 0.3750 (0.3519)  loss_scale: 16384.0000 (22202.6168)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0347  max mem: 15572
Epoch: [31]  [1400/1404]  eta: 0:00:02  lr: 0.000012  min_lr: 0.000000  loss: 4.1930 (4.1372)  class_acc: 0.2917 (0.3517)  loss_scale: 32768.0000 (22278.0300)  weight_decay: 0.0500 (0.0500)  time: 0.4888  data: 0.0245  max mem: 15572
Epoch: [31]  [1403/1404]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 4.1930 (4.1368)  class_acc: 0.2917 (0.3519)  loss_scale: 32768.0000 (22300.4444)  weight_decay: 0.0500 (0.0500)  time: 0.4744  data: 0.0245  max mem: 15572
Epoch: [31] Total time: 0:13:42 (0.5862 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 4.1930 (4.1380)  class_acc: 0.2917 (0.3507)  loss_scale: 32768.0000 (22300.4444)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:08:22  loss: 1.7139 (1.7139)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 3.6932  data: 3.5041  max mem: 15572
Val:  [ 10/136]  eta: 0:01:35  loss: 2.2995 (2.2039)  acc1: 55.5556 (52.5253)  acc5: 83.3333 (82.3232)  time: 0.7608  data: 0.5749  max mem: 15572
Val:  [ 20/136]  eta: 0:00:59  loss: 2.5012 (2.3760)  acc1: 44.4444 (48.1481)  acc5: 77.7778 (78.5714)  time: 0.3508  data: 0.1657  max mem: 15572
Val:  [ 30/136]  eta: 0:00:47  loss: 2.2570 (2.2955)  acc1: 50.0000 (49.6416)  acc5: 77.7778 (79.3907)  time: 0.2720  data: 0.0772  max mem: 15572
Val:  [ 40/136]  eta: 0:00:40  loss: 1.9756 (2.2489)  acc1: 55.5556 (51.2195)  acc5: 83.3333 (80.3523)  time: 0.3275  data: 0.1203  max mem: 15572
Val:  [ 50/136]  eta: 0:00:35  loss: 2.1260 (2.2586)  acc1: 44.4444 (50.0000)  acc5: 83.3333 (80.6100)  time: 0.3506  data: 0.1437  max mem: 15572
Val:  [ 60/136]  eta: 0:00:30  loss: 2.2365 (2.3261)  acc1: 44.4444 (47.6321)  acc5: 77.7778 (79.1439)  time: 0.3597  data: 0.1611  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.2365 (2.3087)  acc1: 44.4444 (48.2003)  acc5: 77.7778 (79.6557)  time: 0.3773  data: 0.1663  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.1397 (2.3008)  acc1: 44.4444 (48.2853)  acc5: 88.8889 (80.5213)  time: 0.3873  data: 0.1486  max mem: 15572
Val:  [ 90/136]  eta: 0:00:17  loss: 2.2609 (2.3105)  acc1: 44.4444 (47.7411)  acc5: 77.7778 (80.3419)  time: 0.3518  data: 0.1290  max mem: 15572
Val:  [100/136]  eta: 0:00:13  loss: 2.5493 (2.3668)  acc1: 38.8889 (46.3696)  acc5: 77.7778 (78.8779)  time: 0.3350  data: 0.1453  max mem: 15572
Val:  [110/136]  eta: 0:00:09  loss: 2.4986 (2.3591)  acc1: 44.4444 (46.6967)  acc5: 83.3333 (78.9790)  time: 0.3526  data: 0.1520  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.2203 (2.3262)  acc1: 55.5556 (47.8421)  acc5: 83.3333 (79.5684)  time: 0.3757  data: 0.1724  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8214 (2.2933)  acc1: 55.5556 (48.7277)  acc5: 88.8889 (80.1951)  time: 0.3229  data: 0.1520  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.9113 (2.2899)  acc1: 55.5556 (48.8534)  acc5: 83.3333 (80.3030)  time: 0.2481  data: 0.0925  max mem: 15572
Val: Total time: 0:00:49 (0.3656 s / it)
* Acc@1 47.768 Acc@5 79.218 loss 2.331
Accuracy of the network on the 4883 val videos: 47.8%
[2025-01-17 04:01:25,017] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 04:01:25,019] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 04:01:25,019] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 04:01:25,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 04:01:27,518] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 04:01:27,519] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 47.77%
Epoch: [32]  [   0/1404]  eta: 2:20:48  lr: 0.000012  min_lr: 0.000000  loss: 4.2660 (4.2660)  class_acc: 0.3750 (0.3750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.0174  data: 5.5946  max mem: 15572
Epoch: [32]  [  10/1404]  eta: 0:25:14  lr: 0.000012  min_lr: 0.000000  loss: 4.1244 (4.1044)  class_acc: 0.3750 (0.3561)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0868  data: 0.6174  max mem: 15572
Epoch: [32]  [  20/1404]  eta: 0:19:47  lr: 0.000012  min_lr: 0.000000  loss: 4.1240 (4.1480)  class_acc: 0.3333 (0.3472)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1184  max mem: 15572
Epoch: [32]  [  30/1404]  eta: 0:17:52  lr: 0.000012  min_lr: 0.000000  loss: 4.1583 (4.1285)  class_acc: 0.3333 (0.3535)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1257  max mem: 15572
Epoch: [32]  [  40/1404]  eta: 0:17:01  lr: 0.000012  min_lr: 0.000000  loss: 4.1583 (4.1490)  class_acc: 0.2917 (0.3455)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.0938  max mem: 15572
Epoch: [32]  [  50/1404]  eta: 0:16:16  lr: 0.000012  min_lr: 0.000000  loss: 4.1486 (4.1436)  class_acc: 0.2917 (0.3489)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.0806  max mem: 15572
Epoch: [32]  [  60/1404]  eta: 0:15:31  lr: 0.000012  min_lr: 0.000000  loss: 4.0539 (4.1328)  class_acc: 0.3750 (0.3545)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0658  max mem: 15572
Epoch: [32]  [  70/1404]  eta: 0:14:56  lr: 0.000012  min_lr: 0.000000  loss: 4.1740 (4.1549)  class_acc: 0.3750 (0.3562)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0271  max mem: 15572
[2025-01-17 04:02:17,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=45000, skipped=266, lr=[1.1159156044128313e-07, 1.1159156044128313e-07, 1.5941651491611877e-07, 1.5941651491611877e-07, 2.2773787845159827e-07, 2.2773787845159827e-07, 3.253398263594261e-07, 3.253398263594261e-07, 4.6477118051346586e-07, 4.6477118051346586e-07, 6.639588293049513e-07, 6.639588293049513e-07, 9.485126132927875e-07, 9.485126132927875e-07, 1.3550180189896967e-06, 1.3550180189896967e-06, 1.9357400271281383e-06, 1.9357400271281383e-06, 2.7653428958973406e-06, 2.7653428958973406e-06, 3.950489851281915e-06, 3.950489851281915e-06, 5.643556930402736e-06, 5.643556930402736e-06, 8.062224186289624e-06, 8.062224186289624e-06, 1.1517463123270892e-05, 1.1517463123270892e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 04:02:17,239] [INFO] [timer.py:260:stop] epoch=0/micro_step=45000/global_step=45000, RunningAvgSamplesPerSec=48.01338018638757, CurrSamplesPerSec=59.33820905393847, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [32]  [  80/1404]  eta: 0:14:46  lr: 0.000011  min_lr: 0.000000  loss: 4.2303 (4.1575)  class_acc: 0.3750 (0.3580)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.0916  max mem: 15572
Epoch: [32]  [  90/1404]  eta: 0:14:28  lr: 0.000011  min_lr: 0.000000  loss: 4.1861 (4.1546)  class_acc: 0.3333 (0.3585)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1385  max mem: 15572
Epoch: [32]  [ 100/1404]  eta: 0:14:14  lr: 0.000011  min_lr: 0.000000  loss: 4.1512 (4.1535)  class_acc: 0.3333 (0.3585)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1169  max mem: 15572
[2025-01-17 04:02:37,096] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:02:37,096] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:02:37,096] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:02:37,096] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:02:38,070] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45037
[2025-01-17 04:02:38,070] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:02:38,071] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 04:02:38,079] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45037
[2025-01-17 04:02:38,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [32]  [ 110/1404]  eta: 0:13:49  lr: 0.000011  min_lr: 0.000000  loss: 4.0851 (4.1446)  class_acc: 0.2917 (0.3589)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.0554  max mem: 15572
Epoch: [32]  [ 120/1404]  eta: 0:13:31  lr: 0.000011  min_lr: 0.000000  loss: 4.1288 (4.1604)  class_acc: 0.2917 (0.3557)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0344  max mem: 15572
Epoch: [32]  [ 130/1404]  eta: 0:13:18  lr: 0.000011  min_lr: 0.000000  loss: 4.2458 (4.1609)  class_acc: 0.2917 (0.3585)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0700  max mem: 15572
[2025-01-17 04:02:51,272] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45060
[2025-01-17 04:02:51,272] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:02:51,273] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45060
[2025-01-17 04:02:51,273] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:02:51,273] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [ 140/1404]  eta: 0:13:06  lr: 0.000011  min_lr: 0.000000  loss: 4.1533 (4.1592)  class_acc: 0.4167 (0.3632)  loss_scale: 32768.0000 (32187.0071)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0673  max mem: 15572
Epoch: [32]  [ 150/1404]  eta: 0:13:01  lr: 0.000011  min_lr: 0.000000  loss: 4.1292 (4.1558)  class_acc: 0.3750 (0.3609)  loss_scale: 16384.0000 (31140.4503)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0838  max mem: 15572
Epoch: [32]  [ 160/1404]  eta: 0:12:50  lr: 0.000011  min_lr: 0.000000  loss: 4.1625 (4.1658)  class_acc: 0.3333 (0.3600)  loss_scale: 16384.0000 (30223.9006)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0785  max mem: 15572
Epoch: [32]  [ 170/1404]  eta: 0:12:42  lr: 0.000011  min_lr: 0.000000  loss: 4.1745 (4.1655)  class_acc: 0.2917 (0.3572)  loss_scale: 16384.0000 (29414.5497)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0813  max mem: 15572
Epoch: [32]  [ 180/1404]  eta: 0:12:31  lr: 0.000011  min_lr: 0.000000  loss: 4.2267 (4.1613)  class_acc: 0.3333 (0.3573)  loss_scale: 16384.0000 (28694.6298)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0747  max mem: 15572
Epoch: [32]  [ 190/1404]  eta: 0:12:26  lr: 0.000011  min_lr: 0.000000  loss: 4.1978 (4.1591)  class_acc: 0.3750 (0.3582)  loss_scale: 16384.0000 (28050.0942)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0746  max mem: 15572
Epoch: [32]  [ 200/1404]  eta: 0:12:16  lr: 0.000011  min_lr: 0.000000  loss: 4.1978 (4.1610)  class_acc: 0.4167 (0.3582)  loss_scale: 16384.0000 (27469.6915)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0934  max mem: 15572
Epoch: [32]  [ 210/1404]  eta: 0:12:13  lr: 0.000011  min_lr: 0.000000  loss: 4.1085 (4.1572)  class_acc: 0.4167 (0.3602)  loss_scale: 16384.0000 (26944.3033)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1226  max mem: 15572
Epoch: [32]  [ 220/1404]  eta: 0:12:02  lr: 0.000011  min_lr: 0.000000  loss: 4.0911 (4.1584)  class_acc: 0.3750 (0.3569)  loss_scale: 16384.0000 (26466.4615)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1035  max mem: 15572
Epoch: [32]  [ 230/1404]  eta: 0:11:56  lr: 0.000011  min_lr: 0.000000  loss: 4.0911 (4.1516)  class_acc: 0.2917 (0.3579)  loss_scale: 16384.0000 (26029.9913)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0967  max mem: 15572
Epoch: [32]  [ 240/1404]  eta: 0:11:47  lr: 0.000011  min_lr: 0.000000  loss: 4.0911 (4.1518)  class_acc: 0.3750 (0.3587)  loss_scale: 16384.0000 (25629.7427)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1090  max mem: 15572
Epoch: [32]  [ 250/1404]  eta: 0:11:42  lr: 0.000011  min_lr: 0.000000  loss: 4.1484 (4.1534)  class_acc: 0.3333 (0.3551)  loss_scale: 16384.0000 (25261.3865)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0889  max mem: 15572
Epoch: [32]  [ 260/1404]  eta: 0:11:35  lr: 0.000011  min_lr: 0.000000  loss: 4.0331 (4.1530)  class_acc: 0.2917 (0.3528)  loss_scale: 16384.0000 (24921.2567)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1172  max mem: 15572
[2025-01-17 04:04:08,379] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:04:08,379] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:04:08,380] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:04:08,381] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [ 270/1404]  eta: 0:11:32  lr: 0.000011  min_lr: 0.000000  loss: 4.0147 (4.1484)  class_acc: 0.3333 (0.3550)  loss_scale: 16384.0000 (25210.8044)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1462  max mem: 15572
Epoch: [32]  [ 280/1404]  eta: 0:11:21  lr: 0.000011  min_lr: 0.000000  loss: 4.1248 (4.1460)  class_acc: 0.3750 (0.3551)  loss_scale: 32768.0000 (25479.7438)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0907  max mem: 15572
[2025-01-17 04:04:21,850] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45214
[2025-01-17 04:04:21,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:04:21,868] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45214
[2025-01-17 04:04:21,868] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:04:21,869] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [ 290/1404]  eta: 0:11:14  lr: 0.000011  min_lr: 0.000000  loss: 4.1248 (4.1461)  class_acc: 0.3750 (0.3571)  loss_scale: 32768.0000 (25448.6873)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0578  max mem: 15572
Epoch: [32]  [ 300/1404]  eta: 0:11:09  lr: 0.000011  min_lr: 0.000000  loss: 4.0628 (4.1489)  class_acc: 0.3750 (0.3573)  loss_scale: 16384.0000 (25147.5349)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1270  max mem: 15572
Epoch: [32]  [ 310/1404]  eta: 0:11:02  lr: 0.000011  min_lr: 0.000000  loss: 4.3642 (4.1518)  class_acc: 0.3333 (0.3564)  loss_scale: 16384.0000 (24865.7492)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1308  max mem: 15572
Epoch: [32]  [ 320/1404]  eta: 0:10:56  lr: 0.000011  min_lr: 0.000000  loss: 4.1818 (4.1475)  class_acc: 0.3333 (0.3564)  loss_scale: 16384.0000 (24601.5202)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1207  max mem: 15572
Epoch: [32]  [ 330/1404]  eta: 0:10:52  lr: 0.000011  min_lr: 0.000000  loss: 4.1016 (4.1463)  class_acc: 0.3333 (0.3586)  loss_scale: 16384.0000 (24353.2568)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1481  max mem: 15572
Epoch: [32]  [ 340/1404]  eta: 0:10:44  lr: 0.000011  min_lr: 0.000000  loss: 4.1082 (4.1463)  class_acc: 0.3750 (0.3583)  loss_scale: 16384.0000 (24119.5543)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1067  max mem: 15572
Epoch: [32]  [ 350/1404]  eta: 0:10:35  lr: 0.000011  min_lr: 0.000000  loss: 4.0976 (4.1451)  class_acc: 0.3750 (0.3604)  loss_scale: 16384.0000 (23899.1681)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0219  max mem: 15572
Epoch: [32]  [ 360/1404]  eta: 0:10:27  lr: 0.000011  min_lr: 0.000000  loss: 4.0548 (4.1426)  class_acc: 0.3333 (0.3591)  loss_scale: 16384.0000 (23690.9917)  weight_decay: 0.0500 (0.0500)  time: 0.5280  data: 0.0310  max mem: 15572
Epoch: [32]  [ 370/1404]  eta: 0:10:19  lr: 0.000011  min_lr: 0.000000  loss: 4.0920 (4.1439)  class_acc: 0.3333 (0.3575)  loss_scale: 16384.0000 (23494.0377)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0506  max mem: 15572
Epoch: [32]  [ 380/1404]  eta: 0:10:14  lr: 0.000011  min_lr: 0.000000  loss: 4.1703 (4.1450)  class_acc: 0.3333 (0.3571)  loss_scale: 16384.0000 (23307.4226)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0718  max mem: 15572
Epoch: [32]  [ 390/1404]  eta: 0:10:06  lr: 0.000011  min_lr: 0.000000  loss: 4.1987 (4.1454)  class_acc: 0.3333 (0.3569)  loss_scale: 16384.0000 (23130.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0594  max mem: 15572
Epoch: [32]  [ 400/1404]  eta: 0:10:03  lr: 0.000011  min_lr: 0.000000  loss: 4.0768 (4.1390)  class_acc: 0.3750 (0.3590)  loss_scale: 16384.0000 (22962.1147)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1326  max mem: 15572
Epoch: [32]  [ 410/1404]  eta: 0:09:56  lr: 0.000011  min_lr: 0.000000  loss: 3.9665 (4.1373)  class_acc: 0.3750 (0.3586)  loss_scale: 16384.0000 (22802.0633)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.1712  max mem: 15572
[2025-01-17 04:05:36,881] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:05:36,882] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:05:36,943] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:05:36,944] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [ 420/1404]  eta: 0:09:51  lr: 0.000011  min_lr: 0.000000  loss: 4.0467 (4.1349)  class_acc: 0.3750 (0.3587)  loss_scale: 16384.0000 (22883.1164)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1064  max mem: 15572
Epoch: [32]  [ 430/1404]  eta: 0:09:43  lr: 0.000011  min_lr: 0.000000  loss: 4.0211 (4.1317)  class_acc: 0.3750 (0.3587)  loss_scale: 32768.0000 (23112.4640)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0606  max mem: 15572
Epoch: [32]  [ 440/1404]  eta: 0:09:38  lr: 0.000011  min_lr: 0.000000  loss: 4.1107 (4.1301)  class_acc: 0.3750 (0.3585)  loss_scale: 32768.0000 (23331.4104)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0624  max mem: 15572
Epoch: [32]  [ 450/1404]  eta: 0:09:31  lr: 0.000011  min_lr: 0.000000  loss: 3.8437 (4.1240)  class_acc: 0.3750 (0.3596)  loss_scale: 32768.0000 (23540.6475)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0627  max mem: 15572
Epoch: [32]  [ 460/1404]  eta: 0:09:23  lr: 0.000011  min_lr: 0.000000  loss: 3.8978 (4.1265)  class_acc: 0.3333 (0.3583)  loss_scale: 32768.0000 (23740.8069)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0029  max mem: 15572
Epoch: [32]  [ 470/1404]  eta: 0:09:15  lr: 0.000011  min_lr: 0.000000  loss: 4.1176 (4.1247)  class_acc: 0.2500 (0.3563)  loss_scale: 32768.0000 (23932.4671)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0026  max mem: 15572
Epoch: [32]  [ 480/1404]  eta: 0:09:09  lr: 0.000011  min_lr: 0.000000  loss: 4.1237 (4.1262)  class_acc: 0.2500 (0.3559)  loss_scale: 32768.0000 (24116.1580)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0307  max mem: 15572
Epoch: [32]  [ 490/1404]  eta: 0:09:04  lr: 0.000011  min_lr: 0.000000  loss: 4.1237 (4.1248)  class_acc: 0.3333 (0.3557)  loss_scale: 32768.0000 (24292.3666)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.0843  max mem: 15572
Epoch: [32]  [ 500/1404]  eta: 0:08:57  lr: 0.000011  min_lr: 0.000000  loss: 3.9734 (4.1237)  class_acc: 0.2917 (0.3538)  loss_scale: 32768.0000 (24461.5409)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0657  max mem: 15572
Epoch: [32]  [ 510/1404]  eta: 0:08:53  lr: 0.000011  min_lr: 0.000000  loss: 4.1645 (4.1255)  class_acc: 0.2500 (0.3536)  loss_scale: 32768.0000 (24624.0939)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1221  max mem: 15572
Epoch: [32]  [ 520/1404]  eta: 0:08:46  lr: 0.000011  min_lr: 0.000000  loss: 4.1486 (4.1223)  class_acc: 0.3333 (0.3540)  loss_scale: 32768.0000 (24780.4069)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.1555  max mem: 15572
Epoch: [32]  [ 530/1404]  eta: 0:08:42  lr: 0.000011  min_lr: 0.000000  loss: 4.0478 (4.1229)  class_acc: 0.3333 (0.3537)  loss_scale: 32768.0000 (24930.8324)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1175  max mem: 15572
Epoch: [32]  [ 540/1404]  eta: 0:08:36  lr: 0.000011  min_lr: 0.000000  loss: 4.2588 (4.1237)  class_acc: 0.3333 (0.3534)  loss_scale: 32768.0000 (25075.6969)  weight_decay: 0.0500 (0.0500)  time: 0.6506  data: 0.1480  max mem: 15572
[2025-01-17 04:06:52,647] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:06:52,648] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:06:52,674] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:06:52,674] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:06:53,157] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45472
[2025-01-17 04:06:53,157] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:06:53,158] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45472
[2025-01-17 04:06:53,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:06:53,159] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 550/1404]  eta: 0:08:29  lr: 0.000011  min_lr: 0.000000  loss: 4.2282 (4.1246)  class_acc: 0.3333 (0.3536)  loss_scale: 32768.0000 (25274.7731)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0759  max mem: 15572
Epoch: [32]  [ 560/1404]  eta: 0:08:23  lr: 0.000011  min_lr: 0.000000  loss: 4.1575 (4.1272)  class_acc: 0.3333 (0.3523)  loss_scale: 32768.0000 (25408.3422)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0669  max mem: 15572
Epoch: [32]  [ 570/1404]  eta: 0:08:17  lr: 0.000011  min_lr: 0.000000  loss: 4.2423 (4.1263)  class_acc: 0.2917 (0.3533)  loss_scale: 32768.0000 (25537.2329)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1182  max mem: 15572
Epoch: [32]  [ 580/1404]  eta: 0:08:11  lr: 0.000011  min_lr: 0.000000  loss: 4.1037 (4.1263)  class_acc: 0.3333 (0.3531)  loss_scale: 32768.0000 (25661.6867)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1067  max mem: 15572
Epoch: [32]  [ 590/1404]  eta: 0:08:04  lr: 0.000011  min_lr: 0.000000  loss: 4.1037 (4.1251)  class_acc: 0.2917 (0.3524)  loss_scale: 32768.0000 (25781.9289)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0799  max mem: 15572
[2025-01-17 04:07:21,749] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45522
[2025-01-17 04:07:21,749] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:07:21,750] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45522
[2025-01-17 04:07:21,751] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:07:21,751] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [ 600/1404]  eta: 0:07:59  lr: 0.000011  min_lr: 0.000000  loss: 4.1466 (4.1264)  class_acc: 0.3333 (0.3524)  loss_scale: 32768.0000 (25707.3411)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0837  max mem: 15572
Epoch: [32]  [ 610/1404]  eta: 0:07:51  lr: 0.000010  min_lr: 0.000000  loss: 4.0393 (4.1219)  class_acc: 0.3750 (0.3532)  loss_scale: 16384.0000 (25554.7496)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0593  max mem: 15572
Epoch: [32]  [ 620/1404]  eta: 0:07:45  lr: 0.000010  min_lr: 0.000000  loss: 3.9051 (4.1202)  class_acc: 0.3750 (0.3532)  loss_scale: 16384.0000 (25407.0725)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0283  max mem: 15572
Epoch: [32]  [ 630/1404]  eta: 0:07:40  lr: 0.000010  min_lr: 0.000000  loss: 4.1449 (4.1199)  class_acc: 0.3333 (0.3522)  loss_scale: 16384.0000 (25264.0761)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0768  max mem: 15572
Epoch: [32]  [ 640/1404]  eta: 0:07:33  lr: 0.000010  min_lr: 0.000000  loss: 4.0935 (4.1190)  class_acc: 0.2917 (0.3522)  loss_scale: 16384.0000 (25125.5413)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0491  max mem: 15572
Epoch: [32]  [ 650/1404]  eta: 0:07:28  lr: 0.000010  min_lr: 0.000000  loss: 4.0935 (4.1202)  class_acc: 0.3750 (0.3527)  loss_scale: 16384.0000 (24991.2627)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0518  max mem: 15572
Epoch: [32]  [ 660/1404]  eta: 0:07:21  lr: 0.000010  min_lr: 0.000000  loss: 4.1945 (4.1204)  class_acc: 0.3333 (0.3524)  loss_scale: 16384.0000 (24861.0469)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0643  max mem: 15572
Epoch: [32]  [ 670/1404]  eta: 0:07:15  lr: 0.000010  min_lr: 0.000000  loss: 4.1388 (4.1195)  class_acc: 0.3333 (0.3520)  loss_scale: 16384.0000 (24734.7124)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0774  max mem: 15572
Epoch: [32]  [ 680/1404]  eta: 0:07:09  lr: 0.000010  min_lr: 0.000000  loss: 4.1223 (4.1182)  class_acc: 0.3750 (0.3523)  loss_scale: 16384.0000 (24612.0881)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0648  max mem: 15572
Epoch: [32]  [ 690/1404]  eta: 0:07:03  lr: 0.000010  min_lr: 0.000000  loss: 4.1590 (4.1201)  class_acc: 0.3333 (0.3511)  loss_scale: 16384.0000 (24493.0130)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0006  max mem: 15572
Epoch: [32]  [ 700/1404]  eta: 0:06:57  lr: 0.000010  min_lr: 0.000000  loss: 4.0028 (4.1165)  class_acc: 0.2917 (0.3516)  loss_scale: 16384.0000 (24377.3352)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0009  max mem: 15572
Epoch: [32]  [ 710/1404]  eta: 0:06:51  lr: 0.000010  min_lr: 0.000000  loss: 3.9969 (4.1155)  class_acc: 0.3333 (0.3515)  loss_scale: 16384.0000 (24264.9114)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0203  max mem: 15572
Epoch: [32]  [ 720/1404]  eta: 0:06:45  lr: 0.000010  min_lr: 0.000000  loss: 4.0620 (4.1147)  class_acc: 0.3333 (0.3512)  loss_scale: 16384.0000 (24155.6061)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0201  max mem: 15572
[2025-01-17 04:08:37,144] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:08:37,145] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:08:37,163] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:08:37,164] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:08:38,255] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45653
[2025-01-17 04:08:38,255] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:08:38,255] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45653
[2025-01-17 04:08:38,255] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:08:38,255] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [ 730/1404]  eta: 0:06:40  lr: 0.000010  min_lr: 0.000000  loss: 4.1069 (4.1144)  class_acc: 0.3333 (0.3510)  loss_scale: 16384.0000 (24094.1176)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0006  max mem: 15572
Epoch: [32]  [ 740/1404]  eta: 0:06:33  lr: 0.000010  min_lr: 0.000000  loss: 4.1600 (4.1159)  class_acc: 0.2917 (0.3501)  loss_scale: 16384.0000 (23990.0675)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0006  max mem: 15572
Epoch: [32]  [ 750/1404]  eta: 0:06:27  lr: 0.000010  min_lr: 0.000000  loss: 4.1739 (4.1170)  class_acc: 0.2917 (0.3497)  loss_scale: 16384.0000 (23888.7883)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0008  max mem: 15572
Epoch: [32]  [ 760/1404]  eta: 0:06:21  lr: 0.000010  min_lr: 0.000000  loss: 4.2261 (4.1198)  class_acc: 0.2917 (0.3488)  loss_scale: 16384.0000 (23790.1708)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0006  max mem: 15572
Epoch: [32]  [ 770/1404]  eta: 0:06:16  lr: 0.000010  min_lr: 0.000000  loss: 4.1947 (4.1208)  class_acc: 0.3333 (0.3485)  loss_scale: 16384.0000 (23694.1115)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0004  max mem: 15572
Epoch: [32]  [ 780/1404]  eta: 0:06:09  lr: 0.000010  min_lr: 0.000000  loss: 4.1706 (4.1204)  class_acc: 0.3333 (0.3490)  loss_scale: 16384.0000 (23600.5122)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.0006  max mem: 15572
Epoch: [32]  [ 790/1404]  eta: 0:06:04  lr: 0.000010  min_lr: 0.000000  loss: 4.0693 (4.1205)  class_acc: 0.3750 (0.3489)  loss_scale: 16384.0000 (23509.2794)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0006  max mem: 15572
Epoch: [32]  [ 800/1404]  eta: 0:05:57  lr: 0.000010  min_lr: 0.000000  loss: 4.2419 (4.1225)  class_acc: 0.3333 (0.3485)  loss_scale: 16384.0000 (23420.3246)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0006  max mem: 15572
Epoch: [32]  [ 810/1404]  eta: 0:05:51  lr: 0.000010  min_lr: 0.000000  loss: 4.1578 (4.1214)  class_acc: 0.2917 (0.3478)  loss_scale: 16384.0000 (23333.5635)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0006  max mem: 15572
Epoch: [32]  [ 820/1404]  eta: 0:05:45  lr: 0.000010  min_lr: 0.000000  loss: 3.9950 (4.1220)  class_acc: 0.2917 (0.3474)  loss_scale: 16384.0000 (23248.9160)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0217  max mem: 15572
Epoch: [32]  [ 830/1404]  eta: 0:05:39  lr: 0.000010  min_lr: 0.000000  loss: 4.0411 (4.1216)  class_acc: 0.3333 (0.3484)  loss_scale: 16384.0000 (23166.3057)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0217  max mem: 15572
Epoch: [32]  [ 840/1404]  eta: 0:05:33  lr: 0.000010  min_lr: 0.000000  loss: 3.9810 (4.1215)  class_acc: 0.3333 (0.3480)  loss_scale: 16384.0000 (23085.6599)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0005  max mem: 15572
Epoch: [32]  [ 850/1404]  eta: 0:05:27  lr: 0.000010  min_lr: 0.000000  loss: 4.1963 (4.1219)  class_acc: 0.3333 (0.3476)  loss_scale: 16384.0000 (23006.9095)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0005  max mem: 15572
[2025-01-17 04:09:52,545] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:09:52,545] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:09:52,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:09:52,546] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [ 860/1404]  eta: 0:05:21  lr: 0.000010  min_lr: 0.000000  loss: 4.1963 (4.1223)  class_acc: 0.2917 (0.3471)  loss_scale: 16384.0000 (23063.1916)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0082  max mem: 15572
Epoch: [32]  [ 870/1404]  eta: 0:05:15  lr: 0.000010  min_lr: 0.000000  loss: 4.1198 (4.1207)  class_acc: 0.3750 (0.3481)  loss_scale: 32768.0000 (23174.6131)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0343  max mem: 15572
Epoch: [32]  [ 880/1404]  eta: 0:05:09  lr: 0.000010  min_lr: 0.000000  loss: 3.9337 (4.1194)  class_acc: 0.3750 (0.3484)  loss_scale: 32768.0000 (23283.5051)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1032  max mem: 15572
Epoch: [32]  [ 890/1404]  eta: 0:05:03  lr: 0.000010  min_lr: 0.000000  loss: 4.0562 (4.1194)  class_acc: 0.3333 (0.3483)  loss_scale: 32768.0000 (23389.9529)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1284  max mem: 15572
Epoch: [32]  [ 900/1404]  eta: 0:04:57  lr: 0.000010  min_lr: 0.000000  loss: 4.0812 (4.1199)  class_acc: 0.2917 (0.3477)  loss_scale: 32768.0000 (23494.0377)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0828  max mem: 15572
Epoch: [32]  [ 910/1404]  eta: 0:04:51  lr: 0.000010  min_lr: 0.000000  loss: 4.0955 (4.1204)  class_acc: 0.2917 (0.3478)  loss_scale: 32768.0000 (23595.8375)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0907  max mem: 15572
Epoch: [32]  [ 920/1404]  eta: 0:04:45  lr: 0.000010  min_lr: 0.000000  loss: 4.0864 (4.1202)  class_acc: 0.3333 (0.3478)  loss_scale: 32768.0000 (23695.4267)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1160  max mem: 15572
Epoch: [32]  [ 930/1404]  eta: 0:04:39  lr: 0.000010  min_lr: 0.000000  loss: 4.2249 (4.1202)  class_acc: 0.3333 (0.3478)  loss_scale: 32768.0000 (23792.8765)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1030  max mem: 15572
[2025-01-17 04:10:38,505] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45860
[2025-01-17 04:10:38,505] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:10:38,505] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 04:10:38,507] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45860
[2025-01-17 04:10:38,508] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [32]  [ 940/1404]  eta: 0:04:34  lr: 0.000010  min_lr: 0.000000  loss: 4.1306 (4.1180)  class_acc: 0.3750 (0.3484)  loss_scale: 32768.0000 (23731.5537)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1311  max mem: 15572
Epoch: [32]  [ 950/1404]  eta: 0:04:28  lr: 0.000010  min_lr: 0.000000  loss: 4.1306 (4.1197)  class_acc: 0.3750 (0.3478)  loss_scale: 16384.0000 (23654.2923)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1036  max mem: 15572
Epoch: [32]  [ 960/1404]  eta: 0:04:22  lr: 0.000010  min_lr: 0.000000  loss: 4.2701 (4.1198)  class_acc: 0.3333 (0.3484)  loss_scale: 16384.0000 (23578.6389)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0523  max mem: 15572
Epoch: [32]  [ 970/1404]  eta: 0:04:16  lr: 0.000010  min_lr: 0.000000  loss: 4.1297 (4.1207)  class_acc: 0.3333 (0.3481)  loss_scale: 16384.0000 (23504.5438)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0938  max mem: 15572
Epoch: [32]  [ 980/1404]  eta: 0:04:10  lr: 0.000010  min_lr: 0.000000  loss: 4.1873 (4.1209)  class_acc: 0.3333 (0.3482)  loss_scale: 16384.0000 (23431.9592)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1142  max mem: 15572
Epoch: [32]  [ 990/1404]  eta: 0:04:04  lr: 0.000010  min_lr: 0.000000  loss: 4.1136 (4.1197)  class_acc: 0.3333 (0.3480)  loss_scale: 16384.0000 (23360.8396)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0981  max mem: 15572
Epoch: [32]  [1000/1404]  eta: 0:03:58  lr: 0.000010  min_lr: 0.000000  loss: 4.0422 (4.1199)  class_acc: 0.2917 (0.3480)  loss_scale: 16384.0000 (23291.1409)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0720  max mem: 15572
Epoch: [32]  [1010/1404]  eta: 0:03:52  lr: 0.000010  min_lr: 0.000000  loss: 4.1424 (4.1208)  class_acc: 0.2917 (0.3473)  loss_scale: 16384.0000 (23222.8210)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1125  max mem: 15572
Epoch: [32]  [1020/1404]  eta: 0:03:46  lr: 0.000010  min_lr: 0.000000  loss: 4.1424 (4.1213)  class_acc: 0.3333 (0.3472)  loss_scale: 16384.0000 (23155.8394)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1386  max mem: 15572
Epoch: [32]  [1030/1404]  eta: 0:03:40  lr: 0.000010  min_lr: 0.000000  loss: 4.1002 (4.1217)  class_acc: 0.3333 (0.3472)  loss_scale: 16384.0000 (23090.1571)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0929  max mem: 15572
Epoch: [32]  [1040/1404]  eta: 0:03:34  lr: 0.000010  min_lr: 0.000000  loss: 4.1620 (4.1218)  class_acc: 0.2917 (0.3470)  loss_scale: 16384.0000 (23025.7368)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0710  max mem: 15572
Epoch: [32]  [1050/1404]  eta: 0:03:28  lr: 0.000010  min_lr: 0.000000  loss: 4.1620 (4.1225)  class_acc: 0.2917 (0.3471)  loss_scale: 16384.0000 (22962.5423)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0460  max mem: 15572
Epoch: [32]  [1060/1404]  eta: 0:03:22  lr: 0.000010  min_lr: 0.000000  loss: 4.1186 (4.1207)  class_acc: 0.4167 (0.3475)  loss_scale: 16384.0000 (22900.5391)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0714  max mem: 15572
[2025-01-17 04:11:53,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:11:53,657] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:11:53,662] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:11:53,663] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [1070/1404]  eta: 0:03:16  lr: 0.000010  min_lr: 0.000000  loss: 4.1105 (4.1209)  class_acc: 0.3333 (0.3471)  loss_scale: 16384.0000 (22992.6723)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1019  max mem: 15572
[2025-01-17 04:11:59,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=46000, skipped=273, lr=[9.332009295061709e-08, 9.332009295061709e-08, 1.3331441850088156e-07, 1.3331441850088156e-07, 1.904491692869737e-07, 1.904491692869737e-07, 2.7207024183853387e-07, 2.7207024183853387e-07, 3.886717740550484e-07, 3.886717740550484e-07, 5.55245391507212e-07, 5.55245391507212e-07, 7.9320770215316e-07, 7.9320770215316e-07, 1.1331538602188002e-06, 1.1331538602188002e-06, 1.6187912288840002e-06, 1.6187912288840002e-06, 2.3125588984057147e-06, 2.3125588984057147e-06, 3.303655569151021e-06, 3.303655569151021e-06, 4.719507955930031e-06, 4.719507955930031e-06, 6.742154222757187e-06, 6.742154222757187e-06, 9.631648889653125e-06, 9.631648889653125e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 04:11:59,390] [INFO] [timer.py:260:stop] epoch=0/micro_step=46000/global_step=46000, RunningAvgSamplesPerSec=48.05358236088019, CurrSamplesPerSec=53.49840588898992, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [32]  [1080/1404]  eta: 0:03:10  lr: 0.000010  min_lr: 0.000000  loss: 4.1414 (4.1205)  class_acc: 0.3333 (0.3470)  loss_scale: 32768.0000 (23083.1008)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0989  max mem: 15572
Epoch: [32]  [1090/1404]  eta: 0:03:05  lr: 0.000010  min_lr: 0.000000  loss: 4.1061 (4.1199)  class_acc: 0.3333 (0.3470)  loss_scale: 32768.0000 (23171.8717)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1475  max mem: 15572
Epoch: [32]  [1100/1404]  eta: 0:02:59  lr: 0.000010  min_lr: 0.000000  loss: 4.0462 (4.1207)  class_acc: 0.3333 (0.3473)  loss_scale: 32768.0000 (23259.0300)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0937  max mem: 15572
Epoch: [32]  [1110/1404]  eta: 0:02:53  lr: 0.000010  min_lr: 0.000000  loss: 4.1209 (4.1209)  class_acc: 0.3750 (0.3477)  loss_scale: 32768.0000 (23344.6193)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0006  max mem: 15572
Epoch: [32]  [1120/1404]  eta: 0:02:47  lr: 0.000010  min_lr: 0.000000  loss: 4.0588 (4.1195)  class_acc: 0.3750 (0.3481)  loss_scale: 32768.0000 (23428.6815)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0423  max mem: 15572
Epoch: [32]  [1130/1404]  eta: 0:02:41  lr: 0.000010  min_lr: 0.000000  loss: 3.9648 (4.1184)  class_acc: 0.3750 (0.3483)  loss_scale: 32768.0000 (23511.2573)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0898  max mem: 15572
Epoch: [32]  [1140/1404]  eta: 0:02:35  lr: 0.000010  min_lr: 0.000000  loss: 4.0697 (4.1180)  class_acc: 0.3750 (0.3485)  loss_scale: 32768.0000 (23592.3856)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0481  max mem: 15572
[2025-01-17 04:12:45,257] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46078
[2025-01-17 04:12:45,258] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:12:45,312] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46078
[2025-01-17 04:12:45,312] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:12:45,312] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [1150/1404]  eta: 0:02:29  lr: 0.000009  min_lr: 0.000000  loss: 4.0910 (4.1182)  class_acc: 0.3333 (0.3484)  loss_scale: 32768.0000 (23657.8697)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.0867  max mem: 15572
Epoch: [32]  [1160/1404]  eta: 0:02:23  lr: 0.000009  min_lr: 0.000000  loss: 4.1314 (4.1178)  class_acc: 0.2917 (0.3480)  loss_scale: 16384.0000 (23595.2179)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.1303  max mem: 15572
Epoch: [32]  [1170/1404]  eta: 0:02:17  lr: 0.000009  min_lr: 0.000000  loss: 4.1200 (4.1178)  class_acc: 0.2917 (0.3481)  loss_scale: 16384.0000 (23533.6362)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0443  max mem: 15572
Epoch: [32]  [1180/1404]  eta: 0:02:11  lr: 0.000009  min_lr: 0.000000  loss: 4.1210 (4.1179)  class_acc: 0.2917 (0.3478)  loss_scale: 16384.0000 (23473.0974)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0283  max mem: 15572
Epoch: [32]  [1190/1404]  eta: 0:02:05  lr: 0.000009  min_lr: 0.000000  loss: 4.1328 (4.1189)  class_acc: 0.2500 (0.3473)  loss_scale: 16384.0000 (23413.5751)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0586  max mem: 15572
Epoch: [32]  [1200/1404]  eta: 0:01:59  lr: 0.000009  min_lr: 0.000000  loss: 4.1398 (4.1182)  class_acc: 0.2917 (0.3473)  loss_scale: 16384.0000 (23355.0441)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0312  max mem: 15572
Epoch: [32]  [1210/1404]  eta: 0:01:54  lr: 0.000009  min_lr: 0.000000  loss: 4.0518 (4.1181)  class_acc: 0.3333 (0.3472)  loss_scale: 16384.0000 (23297.4798)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0845  max mem: 15572
Epoch: [32]  [1220/1404]  eta: 0:01:48  lr: 0.000009  min_lr: 0.000000  loss: 4.1757 (4.1180)  class_acc: 0.2917 (0.3468)  loss_scale: 16384.0000 (23240.8583)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1159  max mem: 15572
Epoch: [32]  [1230/1404]  eta: 0:01:42  lr: 0.000009  min_lr: 0.000000  loss: 4.2453 (4.1185)  class_acc: 0.2917 (0.3471)  loss_scale: 16384.0000 (23185.1568)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0702  max mem: 15572
Epoch: [32]  [1240/1404]  eta: 0:01:36  lr: 0.000009  min_lr: 0.000000  loss: 4.1755 (4.1179)  class_acc: 0.3333 (0.3473)  loss_scale: 16384.0000 (23130.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.0762  max mem: 15572
Epoch: [32]  [1250/1404]  eta: 0:01:30  lr: 0.000009  min_lr: 0.000000  loss: 4.0502 (4.1182)  class_acc: 0.3750 (0.3477)  loss_scale: 16384.0000 (23076.4253)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0381  max mem: 15572
Epoch: [32]  [1260/1404]  eta: 0:01:24  lr: 0.000009  min_lr: 0.000000  loss: 4.2121 (4.1191)  class_acc: 0.3333 (0.3474)  loss_scale: 16384.0000 (23023.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0714  max mem: 15572
Epoch: [32]  [1270/1404]  eta: 0:01:18  lr: 0.000009  min_lr: 0.000000  loss: 4.2121 (4.1199)  class_acc: 0.3333 (0.3476)  loss_scale: 16384.0000 (22971.1157)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.1633  max mem: 15572
[2025-01-17 04:14:00,531] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:14:00,531] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:14:00,531] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:14:00,531] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [1280/1404]  eta: 0:01:12  lr: 0.000009  min_lr: 0.000000  loss: 4.2739 (4.1215)  class_acc: 0.3750 (0.3481)  loss_scale: 16384.0000 (22945.2740)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.1527  max mem: 15572
Epoch: [32]  [1290/1404]  eta: 0:01:07  lr: 0.000009  min_lr: 0.000000  loss: 4.1694 (4.1217)  class_acc: 0.3750 (0.3482)  loss_scale: 32768.0000 (23021.3602)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1036  max mem: 15572
Epoch: [32]  [1300/1404]  eta: 0:01:01  lr: 0.000009  min_lr: 0.000000  loss: 4.0749 (4.1219)  class_acc: 0.3333 (0.3478)  loss_scale: 32768.0000 (23096.2767)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1123  max mem: 15572
Epoch: [32]  [1310/1404]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 4.0650 (4.1208)  class_acc: 0.3333 (0.3484)  loss_scale: 32768.0000 (23170.0503)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0881  max mem: 15572
Epoch: [32]  [1320/1404]  eta: 0:00:49  lr: 0.000009  min_lr: 0.000000  loss: 4.1258 (4.1207)  class_acc: 0.3333 (0.3487)  loss_scale: 32768.0000 (23242.7070)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1108  max mem: 15572
Epoch: [32]  [1330/1404]  eta: 0:00:43  lr: 0.000009  min_lr: 0.000000  loss: 4.2330 (4.1215)  class_acc: 0.3333 (0.3483)  loss_scale: 32768.0000 (23314.2720)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1322  max mem: 15572
Epoch: [32]  [1340/1404]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 4.1732 (4.1212)  class_acc: 0.2917 (0.3485)  loss_scale: 32768.0000 (23384.7696)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1119  max mem: 15572
Epoch: [32]  [1350/1404]  eta: 0:00:31  lr: 0.000009  min_lr: 0.000000  loss: 4.1094 (4.1215)  class_acc: 0.3750 (0.3489)  loss_scale: 32768.0000 (23454.2235)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1220  max mem: 15572
Epoch: [32]  [1360/1404]  eta: 0:00:25  lr: 0.000009  min_lr: 0.000000  loss: 4.2851 (4.1221)  class_acc: 0.3333 (0.3483)  loss_scale: 32768.0000 (23522.6569)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0589  max mem: 15572
[2025-01-17 04:14:48,439] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46289
[2025-01-17 04:14:48,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:14:48,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 04:14:48,469] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46289
[2025-01-17 04:14:48,470] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [32]  [1370/1404]  eta: 0:00:19  lr: 0.000009  min_lr: 0.000000  loss: 4.1780 (4.1223)  class_acc: 0.2500 (0.3480)  loss_scale: 16384.0000 (23470.5879)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0534  max mem: 15572
Epoch: [32]  [1380/1404]  eta: 0:00:14  lr: 0.000009  min_lr: 0.000000  loss: 4.0135 (4.1207)  class_acc: 0.3750 (0.3484)  loss_scale: 16384.0000 (23419.2730)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.0861  max mem: 15572
Epoch: [32]  [1390/1404]  eta: 0:00:08  lr: 0.000009  min_lr: 0.000000  loss: 4.1612 (4.1217)  class_acc: 0.3750 (0.3484)  loss_scale: 16384.0000 (23368.6959)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0594  max mem: 15572
Epoch: [32]  [1400/1404]  eta: 0:00:02  lr: 0.000009  min_lr: 0.000000  loss: 4.1895 (4.1226)  class_acc: 0.3333 (0.3485)  loss_scale: 16384.0000 (23318.8408)  weight_decay: 0.0500 (0.0500)  time: 0.4562  data: 0.0183  max mem: 15572
Epoch: [32]  [1403/1404]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 4.1895 (4.1223)  class_acc: 0.3333 (0.3485)  loss_scale: 16384.0000 (23304.0228)  weight_decay: 0.0500 (0.0500)  time: 0.4367  data: 0.0183  max mem: 15572
Epoch: [32] Total time: 0:13:42 (0.5860 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 4.1895 (4.1274)  class_acc: 0.3333 (0.3509)  loss_scale: 16384.0000 (23304.0228)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:11:58  loss: 1.7205 (1.7205)  acc1: 72.2222 (72.2222)  acc5: 83.3333 (83.3333)  time: 5.2830  data: 5.1014  max mem: 15572
Val:  [ 10/136]  eta: 0:01:42  loss: 2.2193 (2.1921)  acc1: 55.5556 (52.0202)  acc5: 83.3333 (83.3333)  time: 0.8096  data: 0.6137  max mem: 15572
Val:  [ 20/136]  eta: 0:01:10  loss: 2.4324 (2.3453)  acc1: 44.4444 (48.6772)  acc5: 77.7778 (79.6296)  time: 0.3754  data: 0.1719  max mem: 15572
Val:  [ 30/136]  eta: 0:00:55  loss: 2.3951 (2.2849)  acc1: 44.4444 (49.8208)  acc5: 83.3333 (79.7491)  time: 0.3660  data: 0.1672  max mem: 15572
Val:  [ 40/136]  eta: 0:00:48  loss: 2.0057 (2.2531)  acc1: 55.5556 (51.8970)  acc5: 83.3333 (80.2168)  time: 0.3888  data: 0.2024  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 2.1877 (2.2562)  acc1: 55.5556 (51.0893)  acc5: 83.3333 (81.2636)  time: 0.3599  data: 0.1671  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 2.3258 (2.3241)  acc1: 38.8889 (48.4517)  acc5: 83.3333 (79.9636)  time: 0.3233  data: 0.1277  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.3258 (2.3107)  acc1: 50.0000 (49.2175)  acc5: 77.7778 (80.0469)  time: 0.3651  data: 0.1650  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.1467 (2.3000)  acc1: 50.0000 (49.3141)  acc5: 83.3333 (80.5213)  time: 0.3680  data: 0.1645  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.2619 (2.3106)  acc1: 44.4444 (48.6569)  acc5: 77.7778 (80.2808)  time: 0.3573  data: 0.1616  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5352 (2.3622)  acc1: 38.8889 (46.9197)  acc5: 72.2222 (78.7129)  time: 0.3737  data: 0.1840  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.4682 (2.3539)  acc1: 38.8889 (46.8969)  acc5: 72.2222 (78.6787)  time: 0.3863  data: 0.2037  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1559 (2.3166)  acc1: 50.0000 (48.1175)  acc5: 83.3333 (79.3848)  time: 0.3094  data: 0.1301  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8141 (2.2846)  acc1: 61.1111 (48.9822)  acc5: 88.8889 (80.0254)  time: 0.1922  data: 0.0335  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.9263 (2.2823)  acc1: 55.5556 (49.0991)  acc5: 83.3333 (80.0983)  time: 0.1777  data: 0.0334  max mem: 15572
Val: Total time: 0:00:50 (0.3694 s / it)
* Acc@1 47.973 Acc@5 78.993 loss 2.326
Accuracy of the network on the 4883 val videos: 48.0%
[2025-01-17 04:16:00,472] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 04:16:00,474] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 04:16:00,474] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 04:16:00,474] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 04:16:02,736] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 04:16:02,736] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 47.97%
Epoch: [33]  [   0/1404]  eta: 2:52:34  lr: 0.000009  min_lr: 0.000000  loss: 4.1150 (4.1150)  class_acc: 0.2083 (0.2083)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.3753  data: 6.8889  max mem: 15572
Epoch: [33]  [  10/1404]  eta: 0:27:29  lr: 0.000009  min_lr: 0.000000  loss: 4.1637 (4.2484)  class_acc: 0.2917 (0.2879)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1830  data: 0.7224  max mem: 15572
Epoch: [33]  [  20/1404]  eta: 0:20:11  lr: 0.000009  min_lr: 0.000000  loss: 4.1108 (4.1616)  class_acc: 0.3333 (0.3512)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0942  max mem: 15572
Epoch: [33]  [  30/1404]  eta: 0:17:52  lr: 0.000009  min_lr: 0.000000  loss: 4.0422 (4.1263)  class_acc: 0.3750 (0.3414)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0911  max mem: 15572
Epoch: [33]  [  40/1404]  eta: 0:16:49  lr: 0.000009  min_lr: 0.000000  loss: 4.1703 (4.1670)  class_acc: 0.2917 (0.3262)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1059  max mem: 15572
Epoch: [33]  [  50/1404]  eta: 0:15:53  lr: 0.000009  min_lr: 0.000000  loss: 4.1204 (4.1482)  class_acc: 0.3333 (0.3301)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0897  max mem: 15572
Epoch: [33]  [  60/1404]  eta: 0:15:37  lr: 0.000009  min_lr: 0.000000  loss: 4.0425 (4.1518)  class_acc: 0.3333 (0.3258)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1115  max mem: 15572
Epoch: [33]  [  70/1404]  eta: 0:14:57  lr: 0.000009  min_lr: 0.000000  loss: 4.1119 (4.1486)  class_acc: 0.2917 (0.3239)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0958  max mem: 15572
Epoch: [33]  [  80/1404]  eta: 0:14:44  lr: 0.000009  min_lr: 0.000000  loss: 4.1789 (4.1417)  class_acc: 0.2917 (0.3189)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1053  max mem: 15572
[2025-01-17 04:17:00,173] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:17:00,173] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:17:00,174] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:17:00,174] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [  90/1404]  eta: 0:14:16  lr: 0.000009  min_lr: 0.000000  loss: 4.2061 (4.1373)  class_acc: 0.2917 (0.3205)  loss_scale: 16384.0000 (17284.2198)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1125  max mem: 15572
Epoch: [33]  [ 100/1404]  eta: 0:14:08  lr: 0.000009  min_lr: 0.000000  loss: 4.1105 (4.1268)  class_acc: 0.3750 (0.3263)  loss_scale: 32768.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1009  max mem: 15572
[2025-01-17 04:17:11,112] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46436
[2025-01-17 04:17:11,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:17:11,113] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46436
[2025-01-17 04:17:11,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:17:11,115] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [33]  [ 110/1404]  eta: 0:13:51  lr: 0.000009  min_lr: 0.000000  loss: 3.9508 (4.0971)  class_acc: 0.4167 (0.3375)  loss_scale: 32768.0000 (19040.8649)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1118  max mem: 15572
Epoch: [33]  [ 120/1404]  eta: 0:13:38  lr: 0.000009  min_lr: 0.000000  loss: 3.9950 (4.1006)  class_acc: 0.3750 (0.3378)  loss_scale: 16384.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0937  max mem: 15572
Epoch: [33]  [ 130/1404]  eta: 0:13:24  lr: 0.000009  min_lr: 0.000000  loss: 4.1480 (4.1028)  class_acc: 0.3333 (0.3387)  loss_scale: 16384.0000 (18635.2366)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0960  max mem: 15572
Epoch: [33]  [ 140/1404]  eta: 0:13:18  lr: 0.000009  min_lr: 0.000000  loss: 4.0141 (4.1026)  class_acc: 0.3333 (0.3413)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1079  max mem: 15572
Epoch: [33]  [ 150/1404]  eta: 0:13:12  lr: 0.000009  min_lr: 0.000000  loss: 4.0141 (4.0994)  class_acc: 0.3333 (0.3444)  loss_scale: 16384.0000 (18337.0596)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1357  max mem: 15572
Epoch: [33]  [ 160/1404]  eta: 0:13:04  lr: 0.000009  min_lr: 0.000000  loss: 4.0642 (4.0972)  class_acc: 0.3750 (0.3450)  loss_scale: 16384.0000 (18215.7516)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.0791  max mem: 15572
Epoch: [33]  [ 170/1404]  eta: 0:13:01  lr: 0.000009  min_lr: 0.000000  loss: 4.1353 (4.0973)  class_acc: 0.2917 (0.3423)  loss_scale: 16384.0000 (18108.6316)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.0136  max mem: 15572
Epoch: [33]  [ 180/1404]  eta: 0:12:47  lr: 0.000009  min_lr: 0.000000  loss: 4.1353 (4.0995)  class_acc: 0.2917 (0.3471)  loss_scale: 16384.0000 (18013.3481)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0006  max mem: 15572
Epoch: [33]  [ 190/1404]  eta: 0:12:36  lr: 0.000009  min_lr: 0.000000  loss: 4.0290 (4.0971)  class_acc: 0.3750 (0.3479)  loss_scale: 16384.0000 (17928.0419)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0005  max mem: 15572
Epoch: [33]  [ 200/1404]  eta: 0:12:26  lr: 0.000009  min_lr: 0.000000  loss: 4.1067 (4.0979)  class_acc: 0.3333 (0.3454)  loss_scale: 16384.0000 (17851.2239)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0005  max mem: 15572
Epoch: [33]  [ 210/1404]  eta: 0:12:16  lr: 0.000009  min_lr: 0.000000  loss: 4.0868 (4.0943)  class_acc: 0.3333 (0.3462)  loss_scale: 16384.0000 (17781.6872)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0006  max mem: 15572
Epoch: [33]  [ 220/1404]  eta: 0:12:07  lr: 0.000009  min_lr: 0.000000  loss: 4.1518 (4.1011)  class_acc: 0.2917 (0.3454)  loss_scale: 16384.0000 (17718.4434)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0006  max mem: 15572
Epoch: [33]  [ 230/1404]  eta: 0:12:00  lr: 0.000009  min_lr: 0.000000  loss: 4.2165 (4.1044)  class_acc: 0.2917 (0.3465)  loss_scale: 16384.0000 (17660.6753)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0006  max mem: 15572
[2025-01-17 04:18:25,806] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:18:25,807] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:18:25,841] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:18:25,842] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [ 240/1404]  eta: 0:11:48  lr: 0.000009  min_lr: 0.000000  loss: 4.1878 (4.1027)  class_acc: 0.3333 (0.3475)  loss_scale: 16384.0000 (18151.5685)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0007  max mem: 15572
Epoch: [33]  [ 250/1404]  eta: 0:11:39  lr: 0.000009  min_lr: 0.000000  loss: 4.1629 (4.1104)  class_acc: 0.2500 (0.3438)  loss_scale: 32768.0000 (18733.8964)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0098  max mem: 15572
Epoch: [33]  [ 260/1404]  eta: 0:11:31  lr: 0.000009  min_lr: 0.000000  loss: 4.0931 (4.1084)  class_acc: 0.2500 (0.3464)  loss_scale: 32768.0000 (19271.6015)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0394  max mem: 15572
Epoch: [33]  [ 270/1404]  eta: 0:11:26  lr: 0.000009  min_lr: 0.000000  loss: 4.1015 (4.1091)  class_acc: 0.3750 (0.3475)  loss_scale: 32768.0000 (19769.6236)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.0609  max mem: 15572
Epoch: [33]  [ 280/1404]  eta: 0:11:21  lr: 0.000009  min_lr: 0.000000  loss: 4.1015 (4.1104)  class_acc: 0.3750 (0.3479)  loss_scale: 32768.0000 (20232.1993)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.0996  max mem: 15572
[2025-01-17 04:18:57,873] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46621
[2025-01-17 04:18:57,874] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:18:57,912] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46621
[2025-01-17 04:18:57,912] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:18:57,912] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [33]  [ 290/1404]  eta: 0:11:15  lr: 0.000009  min_lr: 0.000000  loss: 3.8846 (4.0984)  class_acc: 0.3750 (0.3512)  loss_scale: 32768.0000 (20550.3780)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.0735  max mem: 15572
Epoch: [33]  [ 300/1404]  eta: 0:11:06  lr: 0.000009  min_lr: 0.000000  loss: 3.9807 (4.0986)  class_acc: 0.4167 (0.3533)  loss_scale: 16384.0000 (20411.9601)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0055  max mem: 15572
Epoch: [33]  [ 310/1404]  eta: 0:10:59  lr: 0.000008  min_lr: 0.000000  loss: 4.0991 (4.1012)  class_acc: 0.3333 (0.3506)  loss_scale: 16384.0000 (20282.4437)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0146  max mem: 15572
Epoch: [33]  [ 320/1404]  eta: 0:10:51  lr: 0.000008  min_lr: 0.000000  loss: 4.0991 (4.0996)  class_acc: 0.2917 (0.3506)  loss_scale: 16384.0000 (20160.9969)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0341  max mem: 15572
Epoch: [33]  [ 330/1404]  eta: 0:10:46  lr: 0.000008  min_lr: 0.000000  loss: 4.0824 (4.0983)  class_acc: 0.2917 (0.3499)  loss_scale: 16384.0000 (20046.8882)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0740  max mem: 15572
Epoch: [33]  [ 340/1404]  eta: 0:10:39  lr: 0.000008  min_lr: 0.000000  loss: 4.1095 (4.0982)  class_acc: 0.2917 (0.3491)  loss_scale: 16384.0000 (19939.4721)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0839  max mem: 15572
Epoch: [33]  [ 350/1404]  eta: 0:10:33  lr: 0.000008  min_lr: 0.000000  loss: 4.1095 (4.0973)  class_acc: 0.3333 (0.3486)  loss_scale: 16384.0000 (19838.1766)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0673  max mem: 15572
Epoch: [33]  [ 360/1404]  eta: 0:10:30  lr: 0.000008  min_lr: 0.000000  loss: 4.1298 (4.0983)  class_acc: 0.3333 (0.3486)  loss_scale: 16384.0000 (19742.4931)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.0689  max mem: 15572
Epoch: [33]  [ 370/1404]  eta: 0:10:21  lr: 0.000008  min_lr: 0.000000  loss: 4.1490 (4.0974)  class_acc: 0.3333 (0.3504)  loss_scale: 16384.0000 (19651.9677)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0320  max mem: 15572
Epoch: [33]  [ 380/1404]  eta: 0:10:15  lr: 0.000008  min_lr: 0.000000  loss: 4.0446 (4.0969)  class_acc: 0.3750 (0.3506)  loss_scale: 16384.0000 (19566.1942)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0403  max mem: 15572
Epoch: [33]  [ 390/1404]  eta: 0:10:09  lr: 0.000008  min_lr: 0.000000  loss: 4.0542 (4.0979)  class_acc: 0.3333 (0.3500)  loss_scale: 16384.0000 (19484.8082)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1162  max mem: 15572
Epoch: [33]  [ 400/1404]  eta: 0:10:03  lr: 0.000008  min_lr: 0.000000  loss: 4.0491 (4.0950)  class_acc: 0.4167 (0.3530)  loss_scale: 16384.0000 (19407.4813)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1212  max mem: 15572
Epoch: [33]  [ 410/1404]  eta: 0:09:57  lr: 0.000008  min_lr: 0.000000  loss: 3.8902 (4.0958)  class_acc: 0.4167 (0.3543)  loss_scale: 16384.0000 (19333.9173)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1004  max mem: 15572
[2025-01-17 04:20:13,990] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:20:13,990] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:20:13,991] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:20:13,991] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [ 420/1404]  eta: 0:09:49  lr: 0.000008  min_lr: 0.000000  loss: 4.0144 (4.0946)  class_acc: 0.4167 (0.3548)  loss_scale: 16384.0000 (19380.5986)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0557  max mem: 15572
Epoch: [33]  [ 430/1404]  eta: 0:09:44  lr: 0.000008  min_lr: 0.000000  loss: 4.1271 (4.0956)  class_acc: 0.3750 (0.3539)  loss_scale: 32768.0000 (19691.2111)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0806  max mem: 15572
Epoch: [33]  [ 440/1404]  eta: 0:09:38  lr: 0.000008  min_lr: 0.000000  loss: 4.1752 (4.0990)  class_acc: 0.3333 (0.3535)  loss_scale: 32768.0000 (19987.7370)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1275  max mem: 15572
Epoch: [33]  [ 450/1404]  eta: 0:09:32  lr: 0.000008  min_lr: 0.000000  loss: 4.2028 (4.0993)  class_acc: 0.3333 (0.3525)  loss_scale: 32768.0000 (20271.1131)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1186  max mem: 15572
Epoch: [33]  [ 460/1404]  eta: 0:09:25  lr: 0.000008  min_lr: 0.000000  loss: 4.0886 (4.0985)  class_acc: 0.2917 (0.3513)  loss_scale: 32768.0000 (20542.1952)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0857  max mem: 15572
Epoch: [33]  [ 470/1404]  eta: 0:09:19  lr: 0.000008  min_lr: 0.000000  loss: 4.1321 (4.1005)  class_acc: 0.2917 (0.3501)  loss_scale: 32768.0000 (20801.7665)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.0598  max mem: 15572
Epoch: [33]  [ 480/1404]  eta: 0:09:12  lr: 0.000008  min_lr: 0.000000  loss: 4.1256 (4.0997)  class_acc: 0.2917 (0.3494)  loss_scale: 32768.0000 (21050.5447)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0698  max mem: 15572
Epoch: [33]  [ 490/1404]  eta: 0:09:06  lr: 0.000008  min_lr: 0.000000  loss: 4.1611 (4.1038)  class_acc: 0.2917 (0.3485)  loss_scale: 32768.0000 (21289.1894)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0905  max mem: 15572
Epoch: [33]  [ 500/1404]  eta: 0:08:59  lr: 0.000008  min_lr: 0.000000  loss: 4.3215 (4.1074)  class_acc: 0.2917 (0.3486)  loss_scale: 32768.0000 (21518.3074)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1007  max mem: 15572
Epoch: [33]  [ 510/1404]  eta: 0:08:53  lr: 0.000008  min_lr: 0.000000  loss: 4.2627 (4.1102)  class_acc: 0.2917 (0.3478)  loss_scale: 32768.0000 (21738.4579)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0679  max mem: 15572
Epoch: [33]  [ 520/1404]  eta: 0:08:46  lr: 0.000008  min_lr: 0.000000  loss: 4.1132 (4.1075)  class_acc: 0.3333 (0.3488)  loss_scale: 32768.0000 (21950.1574)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0338  max mem: 15572
Epoch: [33]  [ 530/1404]  eta: 0:08:39  lr: 0.000008  min_lr: 0.000000  loss: 3.9883 (4.1044)  class_acc: 0.3750 (0.3482)  loss_scale: 32768.0000 (22153.8832)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0244  max mem: 15572
Epoch: [33]  [ 540/1404]  eta: 0:08:32  lr: 0.000008  min_lr: 0.000000  loss: 4.1271 (4.1066)  class_acc: 0.3750 (0.3488)  loss_scale: 32768.0000 (22350.0776)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0686  max mem: 15572
[2025-01-17 04:21:28,169] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:21:28,170] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:21:28,174] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:21:28,174] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:21:29,249] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46880
[2025-01-17 04:21:29,249] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:21:29,314] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46880
[2025-01-17 04:21:29,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:21:29,314] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 550/1404]  eta: 0:08:27  lr: 0.000008  min_lr: 0.000000  loss: 4.0607 (4.1035)  class_acc: 0.3750 (0.3497)  loss_scale: 32768.0000 (22658.0907)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0897  max mem: 15572
Epoch: [33]  [ 560/1404]  eta: 0:08:20  lr: 0.000008  min_lr: 0.000000  loss: 4.0180 (4.1032)  class_acc: 0.3750 (0.3497)  loss_scale: 32768.0000 (22838.3030)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0932  max mem: 15572
Epoch: [33]  [ 570/1404]  eta: 0:08:15  lr: 0.000008  min_lr: 0.000000  loss: 4.0180 (4.1002)  class_acc: 0.3750 (0.3503)  loss_scale: 32768.0000 (23012.2032)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1128  max mem: 15572
Epoch: [33]  [ 580/1404]  eta: 0:08:09  lr: 0.000008  min_lr: 0.000000  loss: 3.8845 (4.0978)  class_acc: 0.4167 (0.3522)  loss_scale: 32768.0000 (23180.1170)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.1262  max mem: 15572
Epoch: [33]  [ 590/1404]  eta: 0:08:03  lr: 0.000008  min_lr: 0.000000  loss: 4.0571 (4.0976)  class_acc: 0.4167 (0.3515)  loss_scale: 32768.0000 (23342.3486)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1045  max mem: 15572
Epoch: [33]  [ 600/1404]  eta: 0:07:57  lr: 0.000008  min_lr: 0.000000  loss: 4.2058 (4.0994)  class_acc: 0.3333 (0.3514)  loss_scale: 32768.0000 (23499.1814)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0792  max mem: 15572
Epoch: [33]  [ 610/1404]  eta: 0:07:52  lr: 0.000008  min_lr: 0.000000  loss: 4.2926 (4.1016)  class_acc: 0.3333 (0.3511)  loss_scale: 32768.0000 (23650.8805)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.0895  max mem: 15572
Epoch: [33]  [ 620/1404]  eta: 0:07:45  lr: 0.000008  min_lr: 0.000000  loss: 4.1722 (4.1022)  class_acc: 0.2917 (0.3503)  loss_scale: 32768.0000 (23797.6940)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0887  max mem: 15572
Epoch: [33]  [ 630/1404]  eta: 0:07:38  lr: 0.000008  min_lr: 0.000000  loss: 4.1427 (4.1020)  class_acc: 0.3333 (0.3497)  loss_scale: 32768.0000 (23939.8542)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0635  max mem: 15572
Epoch: [33]  [ 640/1404]  eta: 0:07:33  lr: 0.000008  min_lr: 0.000000  loss: 4.1595 (4.1050)  class_acc: 0.3333 (0.3502)  loss_scale: 32768.0000 (24077.5788)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0863  max mem: 15572
Epoch: [33]  [ 650/1404]  eta: 0:07:26  lr: 0.000008  min_lr: 0.000000  loss: 4.2101 (4.1053)  class_acc: 0.3750 (0.3500)  loss_scale: 32768.0000 (24211.0722)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0985  max mem: 15572
Epoch: [33]  [ 660/1404]  eta: 0:07:21  lr: 0.000008  min_lr: 0.000000  loss: 4.0723 (4.1054)  class_acc: 0.3333 (0.3492)  loss_scale: 32768.0000 (24340.5265)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1196  max mem: 15572
[2025-01-17 04:22:39,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=47000, skipped=278, lr=[7.652484534492424e-08, 7.652484534492424e-08, 1.0932120763560607e-07, 1.0932120763560607e-07, 1.5617315376515155e-07, 1.5617315376515155e-07, 2.2310450537878794e-07, 2.2310450537878794e-07, 3.187207219696971e-07, 3.187207219696971e-07, 4.553153170995673e-07, 4.553153170995673e-07, 6.504504529993819e-07, 6.504504529993819e-07, 9.292149328562599e-07, 9.292149328562599e-07, 1.3274499040803713e-06, 1.3274499040803713e-06, 1.896357005829102e-06, 1.896357005829102e-06, 2.7090814368987172e-06, 2.7090814368987172e-06, 3.870116338426739e-06, 3.870116338426739e-06, 5.528737626323913e-06, 5.528737626323913e-06, 7.898196609034162e-06, 7.898196609034162e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 04:22:39,876] [INFO] [timer.py:260:stop] epoch=0/micro_step=47000/global_step=47000, RunningAvgSamplesPerSec=48.104606919039256, CurrSamplesPerSec=43.09865870199619, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [33]  [ 670/1404]  eta: 0:07:15  lr: 0.000008  min_lr: 0.000000  loss: 4.0776 (4.1060)  class_acc: 0.3333 (0.3492)  loss_scale: 32768.0000 (24466.1222)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1310  max mem: 15572
[2025-01-17 04:22:44,866] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:22:44,866] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:22:44,868] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:22:44,868] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:22:45,302] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47010
[2025-01-17 04:22:45,302] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:22:45,331] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47010
[2025-01-17 04:22:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:22:45,332] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 680/1404]  eta: 0:07:08  lr: 0.000008  min_lr: 0.000000  loss: 4.0877 (4.1066)  class_acc: 0.3750 (0.3489)  loss_scale: 32768.0000 (24636.1468)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0520  max mem: 15572
Epoch: [33]  [ 690/1404]  eta: 0:07:02  lr: 0.000008  min_lr: 0.000000  loss: 4.1714 (4.1093)  class_acc: 0.2917 (0.3484)  loss_scale: 32768.0000 (24753.8292)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0138  max mem: 15572
Epoch: [33]  [ 700/1404]  eta: 0:06:56  lr: 0.000008  min_lr: 0.000000  loss: 4.2445 (4.1103)  class_acc: 0.2917 (0.3490)  loss_scale: 32768.0000 (24868.1541)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0303  max mem: 15572
Epoch: [33]  [ 710/1404]  eta: 0:06:51  lr: 0.000008  min_lr: 0.000000  loss: 4.1405 (4.1104)  class_acc: 0.2917 (0.3487)  loss_scale: 32768.0000 (24979.2630)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0882  max mem: 15572
Epoch: [33]  [ 720/1404]  eta: 0:06:44  lr: 0.000008  min_lr: 0.000000  loss: 4.1405 (4.1119)  class_acc: 0.2917 (0.3491)  loss_scale: 32768.0000 (25087.2899)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0959  max mem: 15572
Epoch: [33]  [ 730/1404]  eta: 0:06:38  lr: 0.000008  min_lr: 0.000000  loss: 4.1448 (4.1110)  class_acc: 0.3333 (0.3492)  loss_scale: 32768.0000 (25192.3611)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0814  max mem: 15572
Epoch: [33]  [ 740/1404]  eta: 0:06:32  lr: 0.000008  min_lr: 0.000000  loss: 4.1825 (4.1120)  class_acc: 0.3333 (0.3492)  loss_scale: 32768.0000 (25294.5965)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0974  max mem: 15572
Epoch: [33]  [ 750/1404]  eta: 0:06:26  lr: 0.000008  min_lr: 0.000000  loss: 4.1747 (4.1117)  class_acc: 0.3333 (0.3495)  loss_scale: 32768.0000 (25394.1092)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0659  max mem: 15572
Epoch: [33]  [ 760/1404]  eta: 0:06:20  lr: 0.000008  min_lr: 0.000000  loss: 4.1747 (4.1134)  class_acc: 0.3333 (0.3496)  loss_scale: 32768.0000 (25491.0066)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0674  max mem: 15572
Epoch: [33]  [ 770/1404]  eta: 0:06:14  lr: 0.000008  min_lr: 0.000000  loss: 4.1945 (4.1142)  class_acc: 0.3333 (0.3498)  loss_scale: 32768.0000 (25585.3904)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0807  max mem: 15572
Epoch: [33]  [ 780/1404]  eta: 0:06:08  lr: 0.000008  min_lr: 0.000000  loss: 4.1142 (4.1143)  class_acc: 0.2917 (0.3494)  loss_scale: 32768.0000 (25677.3572)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0784  max mem: 15572
Epoch: [33]  [ 790/1404]  eta: 0:06:03  lr: 0.000008  min_lr: 0.000000  loss: 4.1111 (4.1136)  class_acc: 0.3333 (0.3502)  loss_scale: 32768.0000 (25766.9987)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.1063  max mem: 15572
Epoch: [33]  [ 800/1404]  eta: 0:05:56  lr: 0.000008  min_lr: 0.000000  loss: 4.0530 (4.1127)  class_acc: 0.3750 (0.3506)  loss_scale: 32768.0000 (25854.4020)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0698  max mem: 15572
[2025-01-17 04:24:00,633] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:24:00,634] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:24:00,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:24:00,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:24:01,134] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47140
[2025-01-17 04:24:01,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:24:01,135] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 04:24:01,193] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47140
[2025-01-17 04:24:01,193] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [33]  [ 810/1404]  eta: 0:05:50  lr: 0.000008  min_lr: 0.000000  loss: 3.9931 (4.1126)  class_acc: 0.3750 (0.3507)  loss_scale: 32768.0000 (25980.0543)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0153  max mem: 15572
Epoch: [33]  [ 820/1404]  eta: 0:05:45  lr: 0.000008  min_lr: 0.000000  loss: 3.9669 (4.1118)  class_acc: 0.3333 (0.3507)  loss_scale: 32768.0000 (26062.7333)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0125  max mem: 15572
Epoch: [33]  [ 830/1404]  eta: 0:05:38  lr: 0.000008  min_lr: 0.000000  loss: 4.0712 (4.1123)  class_acc: 0.2500 (0.3497)  loss_scale: 32768.0000 (26143.4224)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0009  max mem: 15572
Epoch: [33]  [ 840/1404]  eta: 0:05:32  lr: 0.000008  min_lr: 0.000000  loss: 4.1325 (4.1122)  class_acc: 0.3750 (0.3507)  loss_scale: 32768.0000 (26222.1926)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0010  max mem: 15572
Epoch: [33]  [ 850/1404]  eta: 0:05:26  lr: 0.000008  min_lr: 0.000000  loss: 4.1582 (4.1125)  class_acc: 0.3750 (0.3504)  loss_scale: 32768.0000 (26299.1116)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0008  max mem: 15572
Epoch: [33]  [ 860/1404]  eta: 0:05:20  lr: 0.000008  min_lr: 0.000000  loss: 3.9830 (4.1108)  class_acc: 0.3333 (0.3509)  loss_scale: 32768.0000 (26374.2439)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0780  max mem: 15572
Epoch: [33]  [ 870/1404]  eta: 0:05:15  lr: 0.000008  min_lr: 0.000000  loss: 3.9401 (4.1110)  class_acc: 0.3750 (0.3511)  loss_scale: 32768.0000 (26447.6510)  weight_decay: 0.0500 (0.0500)  time: 0.6524  data: 0.1179  max mem: 15572
Epoch: [33]  [ 880/1404]  eta: 0:05:08  lr: 0.000008  min_lr: 0.000000  loss: 4.1358 (4.1108)  class_acc: 0.3750 (0.3520)  loss_scale: 32768.0000 (26519.3916)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0723  max mem: 15572
Epoch: [33]  [ 890/1404]  eta: 0:05:03  lr: 0.000008  min_lr: 0.000000  loss: 4.1803 (4.1120)  class_acc: 0.2917 (0.3510)  loss_scale: 32768.0000 (26589.5219)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1094  max mem: 15572
[2025-01-17 04:24:54,226] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47231
[2025-01-17 04:24:54,227] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:24:54,227] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 04:24:54,229] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47231
[2025-01-17 04:24:54,229] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [33]  [ 900/1404]  eta: 0:04:57  lr: 0.000008  min_lr: 0.000000  loss: 4.1144 (4.1107)  class_acc: 0.2917 (0.3515)  loss_scale: 32768.0000 (26621.7270)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1055  max mem: 15572
Epoch: [33]  [ 910/1404]  eta: 0:04:51  lr: 0.000008  min_lr: 0.000000  loss: 4.0800 (4.1115)  class_acc: 0.3333 (0.3516)  loss_scale: 16384.0000 (26509.3480)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0806  max mem: 15572
Epoch: [33]  [ 920/1404]  eta: 0:04:45  lr: 0.000007  min_lr: 0.000000  loss: 4.1606 (4.1116)  class_acc: 0.3333 (0.3517)  loss_scale: 16384.0000 (26399.4093)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0662  max mem: 15572
Epoch: [33]  [ 930/1404]  eta: 0:04:39  lr: 0.000007  min_lr: 0.000000  loss: 4.2305 (4.1124)  class_acc: 0.3333 (0.3515)  loss_scale: 16384.0000 (26291.8324)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0642  max mem: 15572
Epoch: [33]  [ 940/1404]  eta: 0:04:33  lr: 0.000007  min_lr: 0.000000  loss: 4.1997 (4.1123)  class_acc: 0.3333 (0.3520)  loss_scale: 16384.0000 (26186.5420)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1190  max mem: 15572
Epoch: [33]  [ 950/1404]  eta: 0:04:27  lr: 0.000007  min_lr: 0.000000  loss: 4.1861 (4.1114)  class_acc: 0.3333 (0.3518)  loss_scale: 16384.0000 (26083.4658)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.1110  max mem: 15572
Epoch: [33]  [ 960/1404]  eta: 0:04:22  lr: 0.000007  min_lr: 0.000000  loss: 4.0366 (4.1121)  class_acc: 0.3333 (0.3518)  loss_scale: 16384.0000 (25982.5349)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1254  max mem: 15572
Epoch: [33]  [ 970/1404]  eta: 0:04:16  lr: 0.000007  min_lr: 0.000000  loss: 4.1922 (4.1128)  class_acc: 0.3750 (0.3526)  loss_scale: 16384.0000 (25883.6828)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1065  max mem: 15572
Epoch: [33]  [ 980/1404]  eta: 0:04:09  lr: 0.000007  min_lr: 0.000000  loss: 4.1922 (4.1131)  class_acc: 0.3750 (0.3527)  loss_scale: 16384.0000 (25786.8461)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0635  max mem: 15572
Epoch: [33]  [ 990/1404]  eta: 0:04:04  lr: 0.000007  min_lr: 0.000000  loss: 4.0076 (4.1117)  class_acc: 0.3333 (0.3525)  loss_scale: 16384.0000 (25691.9637)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0897  max mem: 15572
Epoch: [33]  [1000/1404]  eta: 0:03:58  lr: 0.000007  min_lr: 0.000000  loss: 4.0230 (4.1118)  class_acc: 0.3333 (0.3526)  loss_scale: 16384.0000 (25598.9770)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0741  max mem: 15572
Epoch: [33]  [1010/1404]  eta: 0:03:52  lr: 0.000007  min_lr: 0.000000  loss: 4.1920 (4.1126)  class_acc: 0.2917 (0.3524)  loss_scale: 16384.0000 (25507.8299)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0369  max mem: 15572
Epoch: [33]  [1020/1404]  eta: 0:03:46  lr: 0.000007  min_lr: 0.000000  loss: 4.0628 (4.1114)  class_acc: 0.3750 (0.3528)  loss_scale: 16384.0000 (25418.4682)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0471  max mem: 15572
[2025-01-17 04:26:09,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:26:09,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:26:09,807] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:26:09,807] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [1030/1404]  eta: 0:03:40  lr: 0.000007  min_lr: 0.000000  loss: 4.0447 (4.1117)  class_acc: 0.3750 (0.3525)  loss_scale: 16384.0000 (25378.5141)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1141  max mem: 15572
Epoch: [33]  [1040/1404]  eta: 0:03:34  lr: 0.000007  min_lr: 0.000000  loss: 4.1062 (4.1112)  class_acc: 0.3333 (0.3530)  loss_scale: 32768.0000 (25449.4986)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1260  max mem: 15572
Epoch: [33]  [1050/1404]  eta: 0:03:28  lr: 0.000007  min_lr: 0.000000  loss: 4.0581 (4.1111)  class_acc: 0.3750 (0.3531)  loss_scale: 32768.0000 (25519.1323)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1261  max mem: 15572
Epoch: [33]  [1060/1404]  eta: 0:03:22  lr: 0.000007  min_lr: 0.000000  loss: 4.0047 (4.1092)  class_acc: 0.3750 (0.3533)  loss_scale: 32768.0000 (25587.4533)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0969  max mem: 15572
Epoch: [33]  [1070/1404]  eta: 0:03:16  lr: 0.000007  min_lr: 0.000000  loss: 4.0006 (4.1084)  class_acc: 0.3333 (0.3537)  loss_scale: 32768.0000 (25654.4986)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0493  max mem: 15572
Epoch: [33]  [1080/1404]  eta: 0:03:10  lr: 0.000007  min_lr: 0.000000  loss: 4.2191 (4.1099)  class_acc: 0.2917 (0.3532)  loss_scale: 32768.0000 (25720.3034)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0769  max mem: 15572
Epoch: [33]  [1090/1404]  eta: 0:03:04  lr: 0.000007  min_lr: 0.000000  loss: 4.2191 (4.1110)  class_acc: 0.3333 (0.3533)  loss_scale: 32768.0000 (25784.9019)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0636  max mem: 15572
Epoch: [33]  [1100/1404]  eta: 0:02:59  lr: 0.000007  min_lr: 0.000000  loss: 4.1845 (4.1114)  class_acc: 0.3750 (0.3533)  loss_scale: 32768.0000 (25848.3270)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1328  max mem: 15572
Epoch: [33]  [1110/1404]  eta: 0:02:53  lr: 0.000007  min_lr: 0.000000  loss: 4.0713 (4.1112)  class_acc: 0.3333 (0.3531)  loss_scale: 32768.0000 (25910.6103)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1144  max mem: 15572
Epoch: [33]  [1120/1404]  eta: 0:02:47  lr: 0.000007  min_lr: 0.000000  loss: 4.0713 (4.1096)  class_acc: 0.3333 (0.3539)  loss_scale: 32768.0000 (25971.7823)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0006  max mem: 15572
Epoch: [33]  [1130/1404]  eta: 0:02:41  lr: 0.000007  min_lr: 0.000000  loss: 4.0183 (4.1093)  class_acc: 0.3750 (0.3541)  loss_scale: 32768.0000 (26031.8727)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0086  max mem: 15572
Epoch: [33]  [1140/1404]  eta: 0:02:35  lr: 0.000007  min_lr: 0.000000  loss: 4.1646 (4.1099)  class_acc: 0.3333 (0.3535)  loss_scale: 32768.0000 (26090.9097)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0087  max mem: 15572
Epoch: [33]  [1150/1404]  eta: 0:02:29  lr: 0.000007  min_lr: 0.000000  loss: 4.0949 (4.1091)  class_acc: 0.3333 (0.3537)  loss_scale: 32768.0000 (26148.9209)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0412  max mem: 15572
[2025-01-17 04:27:23,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:27:23,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:27:23,727] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:27:23,727] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:27:24,222] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47489
[2025-01-17 04:27:24,222] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:27:24,226] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47489
[2025-01-17 04:27:24,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:27:24,226] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1160/1404]  eta: 0:02:23  lr: 0.000007  min_lr: 0.000000  loss: 3.8980 (4.1088)  class_acc: 0.3750 (0.3539)  loss_scale: 32768.0000 (26234.1568)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.0946  max mem: 15572
[2025-01-17 04:27:27,288] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47494
[2025-01-17 04:27:27,288] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:27:27,289] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 04:27:27,298] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47494
[2025-01-17 04:27:27,298] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [33]  [1170/1404]  eta: 0:02:17  lr: 0.000007  min_lr: 0.000000  loss: 4.1190 (4.1085)  class_acc: 0.3750 (0.3542)  loss_scale: 32768.0000 (26164.0307)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0790  max mem: 15572
Epoch: [33]  [1180/1404]  eta: 0:02:11  lr: 0.000007  min_lr: 0.000000  loss: 4.0998 (4.1076)  class_acc: 0.3750 (0.3544)  loss_scale: 16384.0000 (26081.2193)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0770  max mem: 15572
Epoch: [33]  [1190/1404]  eta: 0:02:05  lr: 0.000007  min_lr: 0.000000  loss: 4.0693 (4.1072)  class_acc: 0.3333 (0.3545)  loss_scale: 16384.0000 (25999.7985)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0782  max mem: 15572
Epoch: [33]  [1200/1404]  eta: 0:01:59  lr: 0.000007  min_lr: 0.000000  loss: 4.0320 (4.1057)  class_acc: 0.3333 (0.3548)  loss_scale: 16384.0000 (25919.7336)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0749  max mem: 15572
Epoch: [33]  [1210/1404]  eta: 0:01:54  lr: 0.000007  min_lr: 0.000000  loss: 4.1307 (4.1067)  class_acc: 0.3333 (0.3549)  loss_scale: 16384.0000 (25840.9909)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.0974  max mem: 15572
Epoch: [33]  [1220/1404]  eta: 0:01:48  lr: 0.000007  min_lr: 0.000000  loss: 4.1420 (4.1052)  class_acc: 0.3750 (0.3558)  loss_scale: 16384.0000 (25763.5381)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.0492  max mem: 15572
Epoch: [33]  [1230/1404]  eta: 0:01:42  lr: 0.000007  min_lr: 0.000000  loss: 4.1420 (4.1066)  class_acc: 0.3750 (0.3556)  loss_scale: 16384.0000 (25687.3436)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.0007  max mem: 15572
Epoch: [33]  [1240/1404]  eta: 0:01:36  lr: 0.000007  min_lr: 0.000000  loss: 4.2751 (4.1065)  class_acc: 0.3333 (0.3556)  loss_scale: 16384.0000 (25612.3771)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0007  max mem: 15572
Epoch: [33]  [1250/1404]  eta: 0:01:30  lr: 0.000007  min_lr: 0.000000  loss: 4.1756 (4.1072)  class_acc: 0.4167 (0.3561)  loss_scale: 16384.0000 (25538.6091)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0006  max mem: 15572
Epoch: [33]  [1260/1404]  eta: 0:01:24  lr: 0.000007  min_lr: 0.000000  loss: 4.2460 (4.1079)  class_acc: 0.3750 (0.3562)  loss_scale: 16384.0000 (25466.0111)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.0006  max mem: 15572
Epoch: [33]  [1270/1404]  eta: 0:01:18  lr: 0.000007  min_lr: 0.000000  loss: 4.2308 (4.1084)  class_acc: 0.2917 (0.3560)  loss_scale: 16384.0000 (25394.5555)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0193  max mem: 15572
Epoch: [33]  [1280/1404]  eta: 0:01:12  lr: 0.000007  min_lr: 0.000000  loss: 4.2308 (4.1093)  class_acc: 0.2917 (0.3559)  loss_scale: 16384.0000 (25324.2155)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0466  max mem: 15572
Epoch: [33]  [1290/1404]  eta: 0:01:07  lr: 0.000007  min_lr: 0.000000  loss: 4.2365 (4.1101)  class_acc: 0.3333 (0.3559)  loss_scale: 16384.0000 (25254.9651)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0801  max mem: 15572
[2025-01-17 04:28:44,402] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:28:44,403] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:28:44,469] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:28:44,469] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [1300/1404]  eta: 0:01:01  lr: 0.000007  min_lr: 0.000000  loss: 4.1443 (4.1103)  class_acc: 0.3333 (0.3555)  loss_scale: 16384.0000 (25312.7133)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1137  max mem: 15572
Epoch: [33]  [1310/1404]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 4.2439 (4.1113)  class_acc: 0.2500 (0.3551)  loss_scale: 32768.0000 (25369.5805)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1124  max mem: 15572
[2025-01-17 04:28:57,515] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47646
[2025-01-17 04:28:57,515] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:28:57,515] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 04:28:57,566] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47646
[2025-01-17 04:28:57,567] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [33]  [1320/1404]  eta: 0:00:49  lr: 0.000007  min_lr: 0.000000  loss: 4.1964 (4.1117)  class_acc: 0.3333 (0.3554)  loss_scale: 32768.0000 (25338.7676)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1018  max mem: 15572
Epoch: [33]  [1330/1404]  eta: 0:00:43  lr: 0.000007  min_lr: 0.000000  loss: 4.1769 (4.1123)  class_acc: 0.3333 (0.3555)  loss_scale: 16384.0000 (25271.4891)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0665  max mem: 15572
Epoch: [33]  [1340/1404]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 4.1377 (4.1118)  class_acc: 0.2917 (0.3551)  loss_scale: 16384.0000 (25205.2140)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0331  max mem: 15572
Epoch: [33]  [1350/1404]  eta: 0:00:31  lr: 0.000007  min_lr: 0.000000  loss: 4.0991 (4.1116)  class_acc: 0.2917 (0.3549)  loss_scale: 16384.0000 (25139.9201)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.0668  max mem: 15572
Epoch: [33]  [1360/1404]  eta: 0:00:25  lr: 0.000007  min_lr: 0.000000  loss: 4.2399 (4.1128)  class_acc: 0.2917 (0.3542)  loss_scale: 16384.0000 (25075.5856)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1146  max mem: 15572
Epoch: [33]  [1370/1404]  eta: 0:00:20  lr: 0.000007  min_lr: 0.000000  loss: 4.2069 (4.1133)  class_acc: 0.3333 (0.3545)  loss_scale: 16384.0000 (25012.1896)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.1700  max mem: 15572
Epoch: [33]  [1380/1404]  eta: 0:00:14  lr: 0.000007  min_lr: 0.000000  loss: 4.1097 (4.1132)  class_acc: 0.3750 (0.3544)  loss_scale: 16384.0000 (24949.7118)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1482  max mem: 15572
Epoch: [33]  [1390/1404]  eta: 0:00:08  lr: 0.000007  min_lr: 0.000000  loss: 4.1349 (4.1139)  class_acc: 0.2917 (0.3544)  loss_scale: 16384.0000 (24888.1323)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1245  max mem: 15572
Epoch: [33]  [1400/1404]  eta: 0:00:02  lr: 0.000007  min_lr: 0.000000  loss: 4.1556 (4.1135)  class_acc: 0.3750 (0.3546)  loss_scale: 16384.0000 (24827.4318)  weight_decay: 0.0500 (0.0500)  time: 0.4957  data: 0.0812  max mem: 15572
Epoch: [33]  [1403/1404]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 4.2139 (4.1143)  class_acc: 0.3750 (0.3546)  loss_scale: 16384.0000 (24809.3903)  weight_decay: 0.0500 (0.0500)  time: 0.3953  data: 0.0003  max mem: 15572
Epoch: [33] Total time: 0:13:44 (0.5875 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 4.2139 (4.1175)  class_acc: 0.3750 (0.3548)  loss_scale: 16384.0000 (24809.3903)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:13:13  loss: 1.6580 (1.6580)  acc1: 72.2222 (72.2222)  acc5: 83.3333 (83.3333)  time: 5.8309  data: 5.6278  max mem: 15572
Val:  [ 10/136]  eta: 0:01:34  loss: 2.2357 (2.2178)  acc1: 55.5556 (49.4949)  acc5: 77.7778 (82.3232)  time: 0.7525  data: 0.5596  max mem: 15572
Val:  [ 20/136]  eta: 0:01:01  loss: 2.4897 (2.3916)  acc1: 44.4444 (46.5608)  acc5: 77.7778 (78.3069)  time: 0.2615  data: 0.0724  max mem: 15572
Val:  [ 30/136]  eta: 0:00:48  loss: 2.3250 (2.3049)  acc1: 44.4444 (48.3871)  acc5: 77.7778 (78.6738)  time: 0.2923  data: 0.0879  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 1.9649 (2.2638)  acc1: 55.5556 (51.0840)  acc5: 83.3333 (79.4038)  time: 0.3481  data: 0.1475  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.1312 (2.2679)  acc1: 50.0000 (49.7821)  acc5: 83.3333 (80.1743)  time: 0.3767  data: 0.1937  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.2555 (2.3301)  acc1: 33.3333 (47.4499)  acc5: 83.3333 (79.3260)  time: 0.3666  data: 0.1759  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.2611 (2.3081)  acc1: 50.0000 (48.4351)  acc5: 77.7778 (79.8122)  time: 0.3618  data: 0.1682  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.1159 (2.2940)  acc1: 50.0000 (48.7654)  acc5: 83.3333 (80.3155)  time: 0.3693  data: 0.1730  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.2301 (2.3029)  acc1: 44.4444 (48.3516)  acc5: 83.3333 (80.0366)  time: 0.3722  data: 0.1784  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5486 (2.3585)  acc1: 38.8889 (46.4796)  acc5: 72.2222 (78.4378)  time: 0.3521  data: 0.1556  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.5439 (2.3534)  acc1: 38.8889 (46.5966)  acc5: 77.7778 (78.6787)  time: 0.3254  data: 0.1326  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1741 (2.3180)  acc1: 55.5556 (47.8421)  acc5: 88.8889 (79.4307)  time: 0.3422  data: 0.1294  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8274 (2.2881)  acc1: 61.1111 (48.6005)  acc5: 88.8889 (79.9830)  time: 0.3000  data: 0.0987  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.0282 (2.2853)  acc1: 50.0000 (48.7305)  acc5: 88.8889 (80.0573)  time: 0.2038  data: 0.0324  max mem: 15572
Val: Total time: 0:00:49 (0.3651 s / it)
* Acc@1 47.318 Acc@5 79.075 loss 2.326
Accuracy of the network on the 4883 val videos: 47.3%
Max accuracy: 47.97%
Epoch: [34]  [   0/1404]  eta: 2:42:53  lr: 0.000007  min_lr: 0.000000  loss: 3.9382 (3.9382)  class_acc: 0.3750 (0.3750)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 6.9609  data: 5.6103  max mem: 15572
Epoch: [34]  [  10/1404]  eta: 0:25:47  lr: 0.000007  min_lr: 0.000000  loss: 4.0231 (4.0879)  class_acc: 0.3750 (0.3864)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1102  data: 0.5321  max mem: 15572
Epoch: [34]  [  20/1404]  eta: 0:19:54  lr: 0.000007  min_lr: 0.000000  loss: 4.0983 (4.1039)  class_acc: 0.3333 (0.3472)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0125  max mem: 15572
Epoch: [34]  [  30/1404]  eta: 0:17:51  lr: 0.000007  min_lr: 0.000000  loss: 4.1614 (4.1155)  class_acc: 0.3333 (0.3508)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0006  max mem: 15572
[2025-01-17 04:31:07,276] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:31:07,277] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:31:07,277] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:31:07,277] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [  40/1404]  eta: 0:16:50  lr: 0.000007  min_lr: 0.000000  loss: 4.1452 (4.1403)  class_acc: 0.2917 (0.3455)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.0006  max mem: 15572
Epoch: [34]  [  50/1404]  eta: 0:15:40  lr: 0.000007  min_lr: 0.000000  loss: 4.1171 (4.1082)  class_acc: 0.2917 (0.3423)  loss_scale: 32768.0000 (20239.0588)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0006  max mem: 15572
Epoch: [34]  [  60/1404]  eta: 0:15:10  lr: 0.000007  min_lr: 0.000000  loss: 4.1278 (4.1363)  class_acc: 0.2917 (0.3415)  loss_scale: 32768.0000 (22292.9836)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0375  max mem: 15572
Epoch: [34]  [  70/1404]  eta: 0:15:05  lr: 0.000007  min_lr: 0.000000  loss: 4.1278 (4.1334)  class_acc: 0.3333 (0.3474)  loss_scale: 32768.0000 (23768.3380)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.0997  max mem: 15572
Epoch: [34]  [  80/1404]  eta: 0:14:48  lr: 0.000007  min_lr: 0.000000  loss: 4.0566 (4.1171)  class_acc: 0.4167 (0.3575)  loss_scale: 32768.0000 (24879.4074)  weight_decay: 0.0500 (0.0500)  time: 0.6536  data: 0.0718  max mem: 15572
Epoch: [34]  [  90/1404]  eta: 0:14:29  lr: 0.000007  min_lr: 0.000000  loss: 4.0988 (4.1268)  class_acc: 0.3333 (0.3503)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0098  max mem: 15572
Epoch: [34]  [ 100/1404]  eta: 0:14:01  lr: 0.000007  min_lr: 0.000000  loss: 4.1859 (4.1228)  class_acc: 0.2917 (0.3535)  loss_scale: 32768.0000 (26441.5050)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0007  max mem: 15572
Epoch: [34]  [ 110/1404]  eta: 0:13:50  lr: 0.000007  min_lr: 0.000000  loss: 4.1702 (4.1272)  class_acc: 0.3750 (0.3544)  loss_scale: 32768.0000 (27011.4595)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0516  max mem: 15572
[2025-01-17 04:31:50,129] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47847
[2025-01-17 04:31:50,129] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:31:50,139] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47847
[2025-01-17 04:31:50,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:31:50,140] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [ 120/1404]  eta: 0:13:41  lr: 0.000007  min_lr: 0.000000  loss: 4.1362 (4.1226)  class_acc: 0.3333 (0.3526)  loss_scale: 16384.0000 (26133.1570)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1038  max mem: 15572
Epoch: [34]  [ 130/1404]  eta: 0:13:25  lr: 0.000007  min_lr: 0.000000  loss: 4.2391 (4.1297)  class_acc: 0.3750 (0.3550)  loss_scale: 16384.0000 (25388.9466)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0786  max mem: 15572
Epoch: [34]  [ 140/1404]  eta: 0:13:15  lr: 0.000007  min_lr: 0.000000  loss: 4.2878 (4.1364)  class_acc: 0.3333 (0.3499)  loss_scale: 16384.0000 (24750.2979)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0740  max mem: 15572
Epoch: [34]  [ 150/1404]  eta: 0:13:06  lr: 0.000006  min_lr: 0.000000  loss: 4.2498 (4.1373)  class_acc: 0.3333 (0.3532)  loss_scale: 16384.0000 (24196.2384)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1095  max mem: 15572
Epoch: [34]  [ 160/1404]  eta: 0:12:57  lr: 0.000006  min_lr: 0.000000  loss: 4.1826 (4.1368)  class_acc: 0.3750 (0.3556)  loss_scale: 16384.0000 (23711.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1091  max mem: 15572
Epoch: [34]  [ 170/1404]  eta: 0:12:49  lr: 0.000006  min_lr: 0.000000  loss: 4.0495 (4.1359)  class_acc: 0.3750 (0.3540)  loss_scale: 16384.0000 (23282.5263)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0961  max mem: 15572
Epoch: [34]  [ 180/1404]  eta: 0:12:33  lr: 0.000006  min_lr: 0.000000  loss: 4.0194 (4.1323)  class_acc: 0.3333 (0.3515)  loss_scale: 16384.0000 (22901.3923)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0488  max mem: 15572
Epoch: [34]  [ 190/1404]  eta: 0:12:26  lr: 0.000006  min_lr: 0.000000  loss: 4.0019 (4.1232)  class_acc: 0.3333 (0.3536)  loss_scale: 16384.0000 (22560.1675)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0548  max mem: 15572
Epoch: [34]  [ 200/1404]  eta: 0:12:16  lr: 0.000006  min_lr: 0.000000  loss: 4.1484 (4.1246)  class_acc: 0.3333 (0.3532)  loss_scale: 16384.0000 (22252.8955)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0809  max mem: 15572
Epoch: [34]  [ 210/1404]  eta: 0:12:08  lr: 0.000006  min_lr: 0.000000  loss: 4.0756 (4.1200)  class_acc: 0.3333 (0.3525)  loss_scale: 16384.0000 (21974.7488)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0778  max mem: 15572
Epoch: [34]  [ 220/1404]  eta: 0:12:03  lr: 0.000006  min_lr: 0.000000  loss: 4.0716 (4.1218)  class_acc: 0.3750 (0.3537)  loss_scale: 16384.0000 (21721.7738)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1158  max mem: 15572
Epoch: [34]  [ 230/1404]  eta: 0:11:51  lr: 0.000006  min_lr: 0.000000  loss: 4.1209 (4.1246)  class_acc: 0.3750 (0.3557)  loss_scale: 16384.0000 (21490.7013)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0648  max mem: 15572
[2025-01-17 04:33:04,464] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:33:04,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:33:04,633] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:33:04,633] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [ 240/1404]  eta: 0:11:50  lr: 0.000006  min_lr: 0.000000  loss: 4.1209 (4.1211)  class_acc: 0.3750 (0.3560)  loss_scale: 16384.0000 (21346.7884)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.0159  max mem: 15572
Epoch: [34]  [ 250/1404]  eta: 0:11:38  lr: 0.000006  min_lr: 0.000000  loss: 4.1208 (4.1239)  class_acc: 0.3750 (0.3554)  loss_scale: 32768.0000 (21801.8167)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0161  max mem: 15572
Epoch: [34]  [ 260/1404]  eta: 0:11:27  lr: 0.000006  min_lr: 0.000000  loss: 4.1838 (4.1260)  class_acc: 0.3333 (0.3542)  loss_scale: 32768.0000 (22221.9770)  weight_decay: 0.0500 (0.0500)  time: 0.4871  data: 0.0009  max mem: 15572
[2025-01-17 04:33:15,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=48000, skipped=285, lr=[6.127444034370026e-08, 6.127444034370026e-08, 8.753491477671467e-08, 8.753491477671467e-08, 1.2504987825244955e-07, 1.2504987825244955e-07, 1.7864268321778507e-07, 1.7864268321778507e-07, 2.552038331682644e-07, 2.552038331682644e-07, 3.6457690452609205e-07, 3.6457690452609205e-07, 5.208241493229887e-07, 5.208241493229887e-07, 7.44034499032841e-07, 7.44034499032841e-07, 1.0629064271897728e-06, 1.0629064271897728e-06, 1.5184377531282471e-06, 1.5184377531282471e-06, 2.16919679018321e-06, 2.16919679018321e-06, 3.098852557404586e-06, 3.098852557404586e-06, 4.426932224863695e-06, 4.426932224863695e-06, 6.324188892662422e-06, 6.324188892662422e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 04:33:15,844] [INFO] [timer.py:260:stop] epoch=0/micro_step=48000/global_step=48000, RunningAvgSamplesPerSec=48.1317312492332, CurrSamplesPerSec=54.63319240392287, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [ 270/1404]  eta: 0:11:22  lr: 0.000006  min_lr: 0.000000  loss: 4.2467 (4.1307)  class_acc: 0.3333 (0.3546)  loss_scale: 32768.0000 (22611.1292)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0006  max mem: 15572
Epoch: [34]  [ 280/1404]  eta: 0:11:18  lr: 0.000006  min_lr: 0.000000  loss: 4.2749 (4.1375)  class_acc: 0.2917 (0.3516)  loss_scale: 32768.0000 (22972.5836)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.0006  max mem: 15572
Epoch: [34]  [ 290/1404]  eta: 0:11:12  lr: 0.000006  min_lr: 0.000000  loss: 4.1223 (4.1359)  class_acc: 0.2917 (0.3528)  loss_scale: 32768.0000 (23309.1959)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.0008  max mem: 15572
Epoch: [34]  [ 300/1404]  eta: 0:11:08  lr: 0.000006  min_lr: 0.000000  loss: 4.1618 (4.1354)  class_acc: 0.3333 (0.3512)  loss_scale: 32768.0000 (23623.4419)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.0009  max mem: 15572
Epoch: [34]  [ 310/1404]  eta: 0:10:58  lr: 0.000006  min_lr: 0.000000  loss: 4.1618 (4.1358)  class_acc: 0.2917 (0.3513)  loss_scale: 32768.0000 (23917.4791)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.0006  max mem: 15572
Epoch: [34]  [ 320/1404]  eta: 0:10:51  lr: 0.000006  min_lr: 0.000000  loss: 4.0952 (4.1327)  class_acc: 0.2917 (0.3498)  loss_scale: 32768.0000 (24193.1963)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0286  max mem: 15572
Epoch: [34]  [ 330/1404]  eta: 0:10:44  lr: 0.000006  min_lr: 0.000000  loss: 4.0919 (4.1322)  class_acc: 0.3333 (0.3489)  loss_scale: 32768.0000 (24452.2538)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0287  max mem: 15572
Epoch: [34]  [ 340/1404]  eta: 0:10:37  lr: 0.000006  min_lr: 0.000000  loss: 4.1591 (4.1357)  class_acc: 0.3333 (0.3500)  loss_scale: 32768.0000 (24696.1173)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0108  max mem: 15572
Epoch: [34]  [ 350/1404]  eta: 0:10:32  lr: 0.000006  min_lr: 0.000000  loss: 4.2752 (4.1347)  class_acc: 0.3750 (0.3511)  loss_scale: 32768.0000 (24926.0855)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0778  max mem: 15572
[2025-01-17 04:34:09,294] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48088
[2025-01-17 04:34:09,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:34:09,308] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48088
[2025-01-17 04:34:09,309] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:34:09,309] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [ 360/1404]  eta: 0:10:27  lr: 0.000006  min_lr: 0.000000  loss: 4.1625 (4.1332)  class_acc: 0.4167 (0.3538)  loss_scale: 32768.0000 (24734.8476)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.0676  max mem: 15572
Epoch: [34]  [ 370/1404]  eta: 0:10:21  lr: 0.000006  min_lr: 0.000000  loss: 4.0660 (4.1293)  class_acc: 0.3750 (0.3527)  loss_scale: 16384.0000 (24509.7574)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.0256  max mem: 15572
Epoch: [34]  [ 380/1404]  eta: 0:10:14  lr: 0.000006  min_lr: 0.000000  loss: 4.0660 (4.1309)  class_acc: 0.3333 (0.3542)  loss_scale: 16384.0000 (24296.4829)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0255  max mem: 15572
Epoch: [34]  [ 390/1404]  eta: 0:10:09  lr: 0.000006  min_lr: 0.000000  loss: 4.2203 (4.1354)  class_acc: 0.3333 (0.3542)  loss_scale: 16384.0000 (24094.1176)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0006  max mem: 15572
Epoch: [34]  [ 400/1404]  eta: 0:10:04  lr: 0.000006  min_lr: 0.000000  loss: 4.1639 (4.1340)  class_acc: 0.3333 (0.3543)  loss_scale: 16384.0000 (23901.8454)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.0007  max mem: 15572
Epoch: [34]  [ 410/1404]  eta: 0:09:58  lr: 0.000006  min_lr: 0.000000  loss: 4.1024 (4.1356)  class_acc: 0.3333 (0.3541)  loss_scale: 16384.0000 (23718.9294)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.0006  max mem: 15572
Epoch: [34]  [ 420/1404]  eta: 0:09:50  lr: 0.000006  min_lr: 0.000000  loss: 4.1102 (4.1357)  class_acc: 0.3333 (0.3536)  loss_scale: 16384.0000 (23544.7031)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0006  max mem: 15572
Epoch: [34]  [ 430/1404]  eta: 0:09:43  lr: 0.000006  min_lr: 0.000000  loss: 4.1102 (4.1342)  class_acc: 0.3333 (0.3542)  loss_scale: 16384.0000 (23378.5615)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0006  max mem: 15572
Epoch: [34]  [ 440/1404]  eta: 0:09:38  lr: 0.000006  min_lr: 0.000000  loss: 4.0678 (4.1311)  class_acc: 0.3750 (0.3550)  loss_scale: 16384.0000 (23219.9546)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0030  max mem: 15572
Epoch: [34]  [ 450/1404]  eta: 0:09:30  lr: 0.000006  min_lr: 0.000000  loss: 4.1459 (4.1331)  class_acc: 0.3750 (0.3557)  loss_scale: 16384.0000 (23068.3814)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0030  max mem: 15572
Epoch: [34]  [ 460/1404]  eta: 0:09:23  lr: 0.000006  min_lr: 0.000000  loss: 4.0366 (4.1271)  class_acc: 0.4167 (0.3572)  loss_scale: 16384.0000 (22923.3839)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0007  max mem: 15572
Epoch: [34]  [ 470/1404]  eta: 0:09:19  lr: 0.000006  min_lr: 0.000000  loss: 3.9233 (4.1243)  class_acc: 0.4167 (0.3585)  loss_scale: 16384.0000 (22784.5435)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0009  max mem: 15572
Epoch: [34]  [ 480/1404]  eta: 0:09:11  lr: 0.000006  min_lr: 0.000000  loss: 4.1449 (4.1265)  class_acc: 0.4167 (0.3587)  loss_scale: 16384.0000 (22651.4761)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0008  max mem: 15572
[2025-01-17 04:35:25,314] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:35:25,314] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:35:25,314] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:35:25,314] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [ 490/1404]  eta: 0:09:05  lr: 0.000006  min_lr: 0.000000  loss: 4.3460 (4.1286)  class_acc: 0.3333 (0.3578)  loss_scale: 16384.0000 (22857.5153)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0006  max mem: 15572
Epoch: [34]  [ 500/1404]  eta: 0:08:58  lr: 0.000006  min_lr: 0.000000  loss: 4.2064 (4.1273)  class_acc: 0.3333 (0.3573)  loss_scale: 32768.0000 (23055.3293)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0007  max mem: 15572
Epoch: [34]  [ 510/1404]  eta: 0:08:52  lr: 0.000006  min_lr: 0.000000  loss: 4.0568 (4.1270)  class_acc: 0.2917 (0.3572)  loss_scale: 32768.0000 (23245.4012)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0007  max mem: 15572
Epoch: [34]  [ 520/1404]  eta: 0:08:48  lr: 0.000006  min_lr: 0.000000  loss: 4.2055 (4.1271)  class_acc: 0.4167 (0.3588)  loss_scale: 32768.0000 (23428.1766)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.0698  max mem: 15572
Epoch: [34]  [ 530/1404]  eta: 0:08:41  lr: 0.000006  min_lr: 0.000000  loss: 4.2055 (4.1263)  class_acc: 0.3333 (0.3574)  loss_scale: 32768.0000 (23604.0678)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0999  max mem: 15572
Epoch: [34]  [ 540/1404]  eta: 0:08:34  lr: 0.000006  min_lr: 0.000000  loss: 4.2104 (4.1258)  class_acc: 0.2917 (0.3567)  loss_scale: 32768.0000 (23773.4566)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0309  max mem: 15572
Epoch: [34]  [ 550/1404]  eta: 0:08:28  lr: 0.000006  min_lr: 0.000000  loss: 4.2104 (4.1270)  class_acc: 0.2917 (0.3556)  loss_scale: 32768.0000 (23936.6969)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.0320  max mem: 15572
Epoch: [34]  [ 560/1404]  eta: 0:08:22  lr: 0.000006  min_lr: 0.000000  loss: 4.1782 (4.1268)  class_acc: 0.2917 (0.3549)  loss_scale: 32768.0000 (24094.1176)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0318  max mem: 15572
Epoch: [34]  [ 570/1404]  eta: 0:08:15  lr: 0.000006  min_lr: 0.000000  loss: 4.0368 (4.1230)  class_acc: 0.3750 (0.3562)  loss_scale: 32768.0000 (24246.0245)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0007  max mem: 15572
[2025-01-17 04:36:20,277] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48312
[2025-01-17 04:36:20,278] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:36:20,278] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 04:36:20,390] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48312
[2025-01-17 04:36:20,390] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [34]  [ 580/1404]  eta: 0:08:09  lr: 0.000006  min_lr: 0.000000  loss: 4.0168 (4.1245)  class_acc: 0.4167 (0.3566)  loss_scale: 32768.0000 (24251.7040)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0058  max mem: 15572
Epoch: [34]  [ 590/1404]  eta: 0:08:03  lr: 0.000006  min_lr: 0.000000  loss: 4.2025 (4.1256)  class_acc: 0.3750 (0.3570)  loss_scale: 16384.0000 (24118.5787)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0057  max mem: 15572
Epoch: [34]  [ 600/1404]  eta: 0:07:58  lr: 0.000006  min_lr: 0.000000  loss: 4.2025 (4.1248)  class_acc: 0.3750 (0.3579)  loss_scale: 16384.0000 (23989.8835)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.0007  max mem: 15572
Epoch: [34]  [ 610/1404]  eta: 0:07:53  lr: 0.000006  min_lr: 0.000000  loss: 4.1172 (4.1249)  class_acc: 0.3750 (0.3579)  loss_scale: 16384.0000 (23865.4010)  weight_decay: 0.0500 (0.0500)  time: 0.6817  data: 0.0006  max mem: 15572
Epoch: [34]  [ 620/1404]  eta: 0:07:46  lr: 0.000006  min_lr: 0.000000  loss: 4.0252 (4.1238)  class_acc: 0.2917 (0.3572)  loss_scale: 16384.0000 (23744.9275)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0006  max mem: 15572
Epoch: [34]  [ 630/1404]  eta: 0:07:39  lr: 0.000006  min_lr: 0.000000  loss: 4.1125 (4.1255)  class_acc: 0.2500 (0.3566)  loss_scale: 16384.0000 (23628.2726)  weight_decay: 0.0500 (0.0500)  time: 0.4877  data: 0.0007  max mem: 15572
Epoch: [34]  [ 640/1404]  eta: 0:07:32  lr: 0.000006  min_lr: 0.000000  loss: 4.2763 (4.1268)  class_acc: 0.2917 (0.3554)  loss_scale: 16384.0000 (23515.2574)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0006  max mem: 15572
Epoch: [34]  [ 650/1404]  eta: 0:07:25  lr: 0.000006  min_lr: 0.000000  loss: 4.1990 (4.1281)  class_acc: 0.3333 (0.3558)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0006  max mem: 15572
Epoch: [34]  [ 660/1404]  eta: 0:07:19  lr: 0.000006  min_lr: 0.000000  loss: 4.2369 (4.1312)  class_acc: 0.3750 (0.3555)  loss_scale: 16384.0000 (23299.4856)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0009  max mem: 15572
Epoch: [34]  [ 670/1404]  eta: 0:07:14  lr: 0.000006  min_lr: 0.000000  loss: 4.2800 (4.1304)  class_acc: 0.2917 (0.3550)  loss_scale: 16384.0000 (23196.4232)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0008  max mem: 15572
Epoch: [34]  [ 680/1404]  eta: 0:07:07  lr: 0.000006  min_lr: 0.000000  loss: 4.0877 (4.1294)  class_acc: 0.2917 (0.3555)  loss_scale: 16384.0000 (23096.3877)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0343  max mem: 15572
Epoch: [34]  [ 690/1404]  eta: 0:07:02  lr: 0.000006  min_lr: 0.000000  loss: 4.1042 (4.1295)  class_acc: 0.3333 (0.3557)  loss_scale: 16384.0000 (22999.2475)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0714  max mem: 15572
Epoch: [34]  [ 700/1404]  eta: 0:06:56  lr: 0.000006  min_lr: 0.000000  loss: 4.1797 (4.1295)  class_acc: 0.3333 (0.3549)  loss_scale: 16384.0000 (22904.8787)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0428  max mem: 15572
[2025-01-17 04:37:35,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:37:35,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:37:35,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:37:35,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [ 710/1404]  eta: 0:06:49  lr: 0.000006  min_lr: 0.000000  loss: 4.1220 (4.1302)  class_acc: 0.3333 (0.3549)  loss_scale: 16384.0000 (22951.4262)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0057  max mem: 15572
Epoch: [34]  [ 720/1404]  eta: 0:06:44  lr: 0.000006  min_lr: 0.000000  loss: 4.1413 (4.1308)  class_acc: 0.3750 (0.3550)  loss_scale: 32768.0000 (23087.5784)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0617  max mem: 15572
Epoch: [34]  [ 730/1404]  eta: 0:06:38  lr: 0.000006  min_lr: 0.000000  loss: 4.1304 (4.1289)  class_acc: 0.3333 (0.3554)  loss_scale: 32768.0000 (23220.0055)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1068  max mem: 15572
Epoch: [34]  [ 740/1404]  eta: 0:06:33  lr: 0.000006  min_lr: 0.000000  loss: 4.0058 (4.1296)  class_acc: 0.3333 (0.3555)  loss_scale: 32768.0000 (23348.8583)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1294  max mem: 15572
Epoch: [34]  [ 750/1404]  eta: 0:06:26  lr: 0.000006  min_lr: 0.000000  loss: 4.0556 (4.1287)  class_acc: 0.3333 (0.3554)  loss_scale: 32768.0000 (23474.2796)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1204  max mem: 15572
Epoch: [34]  [ 760/1404]  eta: 0:06:21  lr: 0.000006  min_lr: 0.000000  loss: 4.0556 (4.1285)  class_acc: 0.3333 (0.3551)  loss_scale: 32768.0000 (23596.4047)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0366  max mem: 15572
Epoch: [34]  [ 770/1404]  eta: 0:06:14  lr: 0.000006  min_lr: 0.000000  loss: 4.1911 (4.1295)  class_acc: 0.3750 (0.3553)  loss_scale: 32768.0000 (23715.3619)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0008  max mem: 15572
Epoch: [34]  [ 780/1404]  eta: 0:06:08  lr: 0.000006  min_lr: 0.000000  loss: 4.2088 (4.1278)  class_acc: 0.2917 (0.3544)  loss_scale: 32768.0000 (23831.2727)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0009  max mem: 15572
Epoch: [34]  [ 790/1404]  eta: 0:06:02  lr: 0.000006  min_lr: 0.000000  loss: 4.1024 (4.1274)  class_acc: 0.3333 (0.3552)  loss_scale: 32768.0000 (23944.2528)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0114  max mem: 15572
Epoch: [34]  [ 800/1404]  eta: 0:05:56  lr: 0.000006  min_lr: 0.000000  loss: 4.0706 (4.1271)  class_acc: 0.4583 (0.3558)  loss_scale: 32768.0000 (24054.4120)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0511  max mem: 15572
Epoch: [34]  [ 810/1404]  eta: 0:05:50  lr: 0.000006  min_lr: 0.000000  loss: 3.9601 (4.1248)  class_acc: 0.4167 (0.3568)  loss_scale: 32768.0000 (24161.8545)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0833  max mem: 15572
Epoch: [34]  [ 820/1404]  eta: 0:05:44  lr: 0.000006  min_lr: 0.000000  loss: 4.1178 (4.1254)  class_acc: 0.3750 (0.3564)  loss_scale: 32768.0000 (24266.6797)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0839  max mem: 15572
Epoch: [34]  [ 830/1404]  eta: 0:05:38  lr: 0.000006  min_lr: 0.000000  loss: 4.1881 (4.1252)  class_acc: 0.3333 (0.3564)  loss_scale: 32768.0000 (24368.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0879  max mem: 15572
[2025-01-17 04:38:51,147] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:38:51,147] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:38:51,148] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:38:51,148] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:38:51,643] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48570
[2025-01-17 04:38:51,644] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:38:51,645] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48570
[2025-01-17 04:38:51,646] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:38:51,646] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 840/1404]  eta: 0:05:33  lr: 0.000005  min_lr: 0.000000  loss: 4.1563 (4.1244)  class_acc: 0.3750 (0.3563)  loss_scale: 32768.0000 (24507.8145)  weight_decay: 0.0500 (0.0500)  time: 0.6468  data: 0.1668  max mem: 15572
Epoch: [34]  [ 850/1404]  eta: 0:05:27  lr: 0.000005  min_lr: 0.000000  loss: 4.1184 (4.1239)  class_acc: 0.3750 (0.3562)  loss_scale: 32768.0000 (24604.8790)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1443  max mem: 15572
Epoch: [34]  [ 860/1404]  eta: 0:05:21  lr: 0.000005  min_lr: 0.000000  loss: 4.2140 (4.1251)  class_acc: 0.3333 (0.3560)  loss_scale: 32768.0000 (24699.6887)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0592  max mem: 15572
Epoch: [34]  [ 870/1404]  eta: 0:05:15  lr: 0.000005  min_lr: 0.000000  loss: 4.2345 (4.1266)  class_acc: 0.2917 (0.3557)  loss_scale: 32768.0000 (24792.3215)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1080  max mem: 15572
Epoch: [34]  [ 880/1404]  eta: 0:05:09  lr: 0.000005  min_lr: 0.000000  loss: 4.1363 (4.1254)  class_acc: 0.2500 (0.3550)  loss_scale: 32768.0000 (24882.8513)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1266  max mem: 15572
Epoch: [34]  [ 890/1404]  eta: 0:05:03  lr: 0.000005  min_lr: 0.000000  loss: 4.0817 (4.1253)  class_acc: 0.2917 (0.3554)  loss_scale: 32768.0000 (24971.3490)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.0712  max mem: 15572
Epoch: [34]  [ 900/1404]  eta: 0:04:57  lr: 0.000005  min_lr: 0.000000  loss: 4.1112 (4.1249)  class_acc: 0.3750 (0.3553)  loss_scale: 32768.0000 (25057.8824)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1026  max mem: 15572
[2025-01-17 04:39:36,233] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48646
[2025-01-17 04:39:36,233] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:39:36,287] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48646
[2025-01-17 04:39:36,288] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:39:36,288] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [ 910/1404]  eta: 0:04:52  lr: 0.000005  min_lr: 0.000000  loss: 4.1112 (4.1260)  class_acc: 0.3750 (0.3559)  loss_scale: 32768.0000 (25124.5313)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1323  max mem: 15572
Epoch: [34]  [ 920/1404]  eta: 0:04:45  lr: 0.000005  min_lr: 0.000000  loss: 4.1198 (4.1260)  class_acc: 0.3750 (0.3556)  loss_scale: 16384.0000 (25029.6287)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0483  max mem: 15572
Epoch: [34]  [ 930/1404]  eta: 0:04:39  lr: 0.000005  min_lr: 0.000000  loss: 4.1198 (4.1258)  class_acc: 0.3750 (0.3557)  loss_scale: 16384.0000 (24936.7648)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0362  max mem: 15572
Epoch: [34]  [ 940/1404]  eta: 0:04:34  lr: 0.000005  min_lr: 0.000000  loss: 4.2313 (4.1280)  class_acc: 0.2917 (0.3550)  loss_scale: 16384.0000 (24845.8746)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0976  max mem: 15572
Epoch: [34]  [ 950/1404]  eta: 0:04:27  lr: 0.000005  min_lr: 0.000000  loss: 4.1993 (4.1280)  class_acc: 0.2917 (0.3548)  loss_scale: 16384.0000 (24756.8959)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.0821  max mem: 15572
Epoch: [34]  [ 960/1404]  eta: 0:04:22  lr: 0.000005  min_lr: 0.000000  loss: 4.0681 (4.1271)  class_acc: 0.3333 (0.3545)  loss_scale: 16384.0000 (24669.7690)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1025  max mem: 15572
Epoch: [34]  [ 970/1404]  eta: 0:04:16  lr: 0.000005  min_lr: 0.000000  loss: 4.1436 (4.1272)  class_acc: 0.3333 (0.3546)  loss_scale: 16384.0000 (24584.4367)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1414  max mem: 15572
Epoch: [34]  [ 980/1404]  eta: 0:04:10  lr: 0.000005  min_lr: 0.000000  loss: 3.9958 (4.1247)  class_acc: 0.3750 (0.3552)  loss_scale: 16384.0000 (24500.8440)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0781  max mem: 15572
Epoch: [34]  [ 990/1404]  eta: 0:04:04  lr: 0.000005  min_lr: 0.000000  loss: 4.0530 (4.1254)  class_acc: 0.4167 (0.3558)  loss_scale: 16384.0000 (24418.9384)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.0691  max mem: 15572
Epoch: [34]  [1000/1404]  eta: 0:03:58  lr: 0.000005  min_lr: 0.000000  loss: 4.0986 (4.1246)  class_acc: 0.3750 (0.3554)  loss_scale: 16384.0000 (24338.6693)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0727  max mem: 15572
Epoch: [34]  [1010/1404]  eta: 0:03:52  lr: 0.000005  min_lr: 0.000000  loss: 4.0136 (4.1234)  class_acc: 0.3333 (0.3554)  loss_scale: 16384.0000 (24259.9881)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0656  max mem: 15572
Epoch: [34]  [1020/1404]  eta: 0:03:46  lr: 0.000005  min_lr: 0.000000  loss: 4.0572 (4.1231)  class_acc: 0.3333 (0.3550)  loss_scale: 16384.0000 (24182.8482)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0961  max mem: 15572
Epoch: [34]  [1030/1404]  eta: 0:03:40  lr: 0.000005  min_lr: 0.000000  loss: 4.2422 (4.1238)  class_acc: 0.3333 (0.3551)  loss_scale: 16384.0000 (24107.2047)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1117  max mem: 15572
[2025-01-17 04:40:51,945] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:40:51,945] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:40:51,979] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:40:51,979] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [1040/1404]  eta: 0:03:34  lr: 0.000005  min_lr: 0.000000  loss: 4.2351 (4.1244)  class_acc: 0.3750 (0.3555)  loss_scale: 16384.0000 (24064.4918)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1064  max mem: 15572
Epoch: [34]  [1050/1404]  eta: 0:03:29  lr: 0.000005  min_lr: 0.000000  loss: 4.1227 (4.1228)  class_acc: 0.4167 (0.3556)  loss_scale: 32768.0000 (24147.3035)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.1701  max mem: 15572
Epoch: [34]  [1060/1404]  eta: 0:03:23  lr: 0.000005  min_lr: 0.000000  loss: 3.9016 (4.1216)  class_acc: 0.3750 (0.3560)  loss_scale: 32768.0000 (24228.5542)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1230  max mem: 15572
Epoch: [34]  [1070/1404]  eta: 0:03:16  lr: 0.000005  min_lr: 0.000000  loss: 4.0115 (4.1212)  class_acc: 0.3333 (0.3559)  loss_scale: 32768.0000 (24308.2876)  weight_decay: 0.0500 (0.0500)  time: 0.4843  data: 0.0009  max mem: 15572
Epoch: [34]  [1080/1404]  eta: 0:03:10  lr: 0.000005  min_lr: 0.000000  loss: 4.0115 (4.1202)  class_acc: 0.3750 (0.3564)  loss_scale: 32768.0000 (24386.5458)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0009  max mem: 15572
Epoch: [34]  [1090/1404]  eta: 0:03:04  lr: 0.000005  min_lr: 0.000000  loss: 4.0429 (4.1183)  class_acc: 0.3750 (0.3565)  loss_scale: 32768.0000 (24463.3694)  weight_decay: 0.0500 (0.0500)  time: 0.5186  data: 0.0259  max mem: 15572
Epoch: [34]  [1100/1404]  eta: 0:02:59  lr: 0.000005  min_lr: 0.000000  loss: 4.0041 (4.1187)  class_acc: 0.3750 (0.3568)  loss_scale: 32768.0000 (24538.7975)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1048  max mem: 15572
Epoch: [34]  [1110/1404]  eta: 0:02:53  lr: 0.000005  min_lr: 0.000000  loss: 4.0213 (4.1179)  class_acc: 0.3750 (0.3568)  loss_scale: 32768.0000 (24612.8677)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1401  max mem: 15572
Epoch: [34]  [1120/1404]  eta: 0:02:47  lr: 0.000005  min_lr: 0.000000  loss: 4.0264 (4.1171)  class_acc: 0.3750 (0.3574)  loss_scale: 32768.0000 (24685.6164)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0969  max mem: 15572
Epoch: [34]  [1130/1404]  eta: 0:02:41  lr: 0.000005  min_lr: 0.000000  loss: 4.1298 (4.1188)  class_acc: 0.3750 (0.3571)  loss_scale: 32768.0000 (24757.0787)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0772  max mem: 15572
Epoch: [34]  [1140/1404]  eta: 0:02:35  lr: 0.000005  min_lr: 0.000000  loss: 4.1722 (4.1191)  class_acc: 0.2917 (0.3569)  loss_scale: 32768.0000 (24827.2883)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0678  max mem: 15572
Epoch: [34]  [1150/1404]  eta: 0:02:29  lr: 0.000005  min_lr: 0.000000  loss: 4.2100 (4.1211)  class_acc: 0.2917 (0.3562)  loss_scale: 32768.0000 (24896.2780)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0919  max mem: 15572
Epoch: [34]  [1160/1404]  eta: 0:02:23  lr: 0.000005  min_lr: 0.000000  loss: 4.2296 (4.1213)  class_acc: 0.2917 (0.3560)  loss_scale: 32768.0000 (24964.0792)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1018  max mem: 15572
[2025-01-17 04:42:05,764] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:42:05,764] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:42:05,800] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:42:05,801] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:42:07,147] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48905
[2025-01-17 04:42:07,147] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48905
[2025-01-17 04:42:07,147] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:42:07,147] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:42:07,147] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1170/1404]  eta: 0:02:17  lr: 0.000005  min_lr: 0.000000  loss: 4.0821 (4.1216)  class_acc: 0.2917 (0.3553)  loss_scale: 32768.0000 (25086.6883)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0625  max mem: 15572
Epoch: [34]  [1180/1404]  eta: 0:02:12  lr: 0.000005  min_lr: 0.000000  loss: 4.2009 (4.1229)  class_acc: 0.2500 (0.3546)  loss_scale: 32768.0000 (25151.7290)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.0714  max mem: 15572
Epoch: [34]  [1190/1404]  eta: 0:02:06  lr: 0.000005  min_lr: 0.000000  loss: 4.2481 (4.1238)  class_acc: 0.3333 (0.3547)  loss_scale: 32768.0000 (25215.6776)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0460  max mem: 15572
Epoch: [34]  [1200/1404]  eta: 0:02:00  lr: 0.000005  min_lr: 0.000000  loss: 4.1796 (4.1237)  class_acc: 0.3333 (0.3544)  loss_scale: 32768.0000 (25278.5612)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0299  max mem: 15572
Epoch: [34]  [1210/1404]  eta: 0:01:54  lr: 0.000005  min_lr: 0.000000  loss: 4.2339 (4.1247)  class_acc: 0.2917 (0.3541)  loss_scale: 32768.0000 (25340.4063)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0302  max mem: 15572
Epoch: [34]  [1220/1404]  eta: 0:01:48  lr: 0.000005  min_lr: 0.000000  loss: 4.1016 (4.1241)  class_acc: 0.3750 (0.3546)  loss_scale: 32768.0000 (25401.2383)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0497  max mem: 15572
Epoch: [34]  [1230/1404]  eta: 0:01:42  lr: 0.000005  min_lr: 0.000000  loss: 3.9962 (4.1241)  class_acc: 0.4167 (0.3554)  loss_scale: 32768.0000 (25461.0820)  weight_decay: 0.0500 (0.0500)  time: 0.6513  data: 0.1410  max mem: 15572
Epoch: [34]  [1240/1404]  eta: 0:01:36  lr: 0.000005  min_lr: 0.000000  loss: 4.1016 (4.1244)  class_acc: 0.4583 (0.3555)  loss_scale: 32768.0000 (25519.9613)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1392  max mem: 15572
Epoch: [34]  [1250/1404]  eta: 0:01:30  lr: 0.000005  min_lr: 0.000000  loss: 4.1746 (4.1253)  class_acc: 0.2917 (0.3550)  loss_scale: 32768.0000 (25577.8993)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0989  max mem: 15572
Epoch: [34]  [1260/1404]  eta: 0:01:24  lr: 0.000005  min_lr: 0.000000  loss: 4.0956 (4.1251)  class_acc: 0.2917 (0.3553)  loss_scale: 32768.0000 (25634.9183)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0851  max mem: 15572
[2025-01-17 04:43:01,915] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48998
[2025-01-17 04:43:01,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:43:01,947] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48998
[2025-01-17 04:43:01,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:43:01,948] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 04:43:02,405] [INFO] [logging.py:96:log_dist] [Rank 0] step=49000, skipped=291, lr=[4.7631188684432676e-08, 4.7631188684432676e-08, 6.804455526347526e-08, 6.804455526347526e-08, 9.720650751925038e-08, 9.720650751925038e-08, 1.3886643931321484e-07, 1.3886643931321484e-07, 1.9838062759030693e-07, 1.9838062759030693e-07, 2.8340089655758133e-07, 2.8340089655758133e-07, 4.048584236536876e-07, 4.048584236536876e-07, 5.783691766481252e-07, 5.783691766481252e-07, 8.262416809258931e-07, 8.262416809258931e-07, 1.180345258465562e-06, 1.180345258465562e-06, 1.6862075120936596e-06, 1.6862075120936596e-06, 2.4088678744195143e-06, 2.4088678744195143e-06, 3.441239820599306e-06, 3.441239820599306e-06, 4.916056886570438e-06, 4.916056886570438e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 04:43:02,407] [INFO] [timer.py:260:stop] epoch=0/micro_step=49000/global_step=49000, RunningAvgSamplesPerSec=48.12186145296228, CurrSamplesPerSec=56.18942349395674, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [1270/1404]  eta: 0:01:18  lr: 0.000005  min_lr: 0.000000  loss: 4.0956 (4.1242)  class_acc: 0.4167 (0.3560)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0771  max mem: 15572
Epoch: [34]  [1280/1404]  eta: 0:01:13  lr: 0.000005  min_lr: 0.000000  loss: 4.2168 (4.1253)  class_acc: 0.4167 (0.3559)  loss_scale: 16384.0000 (25503.2756)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0959  max mem: 15572
Epoch: [34]  [1290/1404]  eta: 0:01:07  lr: 0.000005  min_lr: 0.000000  loss: 4.2572 (4.1255)  class_acc: 0.3333 (0.3559)  loss_scale: 16384.0000 (25432.6383)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0903  max mem: 15572
Epoch: [34]  [1300/1404]  eta: 0:01:01  lr: 0.000005  min_lr: 0.000000  loss: 4.1242 (4.1257)  class_acc: 0.4167 (0.3566)  loss_scale: 16384.0000 (25363.0869)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.0713  max mem: 15572
Epoch: [34]  [1310/1404]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 4.1075 (4.1248)  class_acc: 0.4167 (0.3567)  loss_scale: 16384.0000 (25294.5965)  weight_decay: 0.0500 (0.0500)  time: 0.5186  data: 0.0337  max mem: 15572
Epoch: [34]  [1320/1404]  eta: 0:00:49  lr: 0.000005  min_lr: 0.000000  loss: 4.0526 (4.1250)  class_acc: 0.3750 (0.3565)  loss_scale: 16384.0000 (25227.1431)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0008  max mem: 15572
Epoch: [34]  [1330/1404]  eta: 0:00:43  lr: 0.000005  min_lr: 0.000000  loss: 4.1612 (4.1259)  class_acc: 0.3333 (0.3564)  loss_scale: 16384.0000 (25160.7032)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1093  max mem: 15572
Epoch: [34]  [1340/1404]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 4.1612 (4.1256)  class_acc: 0.3750 (0.3568)  loss_scale: 16384.0000 (25095.2543)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1092  max mem: 15572
Epoch: [34]  [1350/1404]  eta: 0:00:31  lr: 0.000005  min_lr: 0.000000  loss: 4.0409 (4.1249)  class_acc: 0.3750 (0.3572)  loss_scale: 16384.0000 (25030.7742)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0008  max mem: 15572
Epoch: [34]  [1360/1404]  eta: 0:00:25  lr: 0.000005  min_lr: 0.000000  loss: 4.1376 (4.1250)  class_acc: 0.3750 (0.3569)  loss_scale: 16384.0000 (24967.2417)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0385  max mem: 15572
Epoch: [34]  [1370/1404]  eta: 0:00:19  lr: 0.000005  min_lr: 0.000000  loss: 4.1564 (4.1252)  class_acc: 0.3333 (0.3570)  loss_scale: 16384.0000 (24904.6360)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0890  max mem: 15572
Epoch: [34]  [1380/1404]  eta: 0:00:14  lr: 0.000005  min_lr: 0.000000  loss: 4.1009 (4.1251)  class_acc: 0.3750 (0.3569)  loss_scale: 16384.0000 (24842.9370)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0564  max mem: 15572
Epoch: [34]  [1390/1404]  eta: 0:00:08  lr: 0.000005  min_lr: 0.000000  loss: 4.1009 (4.1242)  class_acc: 0.3750 (0.3570)  loss_scale: 16384.0000 (24782.1251)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0124  max mem: 15572
[2025-01-17 04:44:14,992] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:44:14,992] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:44:14,997] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:44:14,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [1400/1404]  eta: 0:00:02  lr: 0.000005  min_lr: 0.000000  loss: 4.1439 (4.1244)  class_acc: 0.3333 (0.3567)  loss_scale: 16384.0000 (24839.1263)  weight_decay: 0.0500 (0.0500)  time: 0.4695  data: 0.0072  max mem: 15572
Epoch: [34]  [1403/1404]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 4.1304 (4.1245)  class_acc: 0.3333 (0.3566)  loss_scale: 32768.0000 (24856.0684)  weight_decay: 0.0500 (0.0500)  time: 0.4374  data: 0.0011  max mem: 15572
Epoch: [34] Total time: 0:13:42 (0.5858 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 4.1304 (4.1167)  class_acc: 0.3333 (0.3581)  loss_scale: 32768.0000 (24856.0684)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:15:27  loss: 1.7399 (1.7399)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 6.8182  data: 6.5267  max mem: 15572
Val:  [ 10/136]  eta: 0:01:41  loss: 2.2228 (2.1912)  acc1: 61.1111 (54.0404)  acc5: 83.3333 (83.8384)  time: 0.8087  data: 0.5940  max mem: 15572
Val:  [ 20/136]  eta: 0:01:01  loss: 2.4427 (2.3584)  acc1: 44.4444 (49.2063)  acc5: 77.7778 (79.3651)  time: 0.2123  data: 0.0102  max mem: 15572
Val:  [ 30/136]  eta: 0:00:48  loss: 2.2934 (2.2785)  acc1: 44.4444 (49.6416)  acc5: 77.7778 (80.2867)  time: 0.2706  data: 0.0806  max mem: 15572
Val:  [ 40/136]  eta: 0:00:41  loss: 2.0456 (2.2459)  acc1: 55.5556 (51.4905)  acc5: 83.3333 (80.3523)  time: 0.3395  data: 0.1472  max mem: 15572
Val:  [ 50/136]  eta: 0:00:34  loss: 2.1428 (2.2484)  acc1: 55.5556 (50.4357)  acc5: 83.3333 (80.8279)  time: 0.3210  data: 0.1285  max mem: 15572
Val:  [ 60/136]  eta: 0:00:30  loss: 2.3011 (2.3144)  acc1: 44.4444 (48.0874)  acc5: 83.3333 (79.6903)  time: 0.3337  data: 0.1425  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.2972 (2.2952)  acc1: 44.4444 (49.0610)  acc5: 77.7778 (79.9687)  time: 0.3712  data: 0.1659  max mem: 15572
Val:  [ 80/136]  eta: 0:00:21  loss: 2.1961 (2.2909)  acc1: 44.4444 (48.6283)  acc5: 83.3333 (80.5213)  time: 0.3450  data: 0.1405  max mem: 15572
Val:  [ 90/136]  eta: 0:00:17  loss: 2.2760 (2.2997)  acc1: 44.4444 (48.2295)  acc5: 77.7778 (80.1587)  time: 0.3481  data: 0.1541  max mem: 15572
Val:  [100/136]  eta: 0:00:13  loss: 2.5383 (2.3532)  acc1: 38.8889 (46.6447)  acc5: 72.2222 (78.8229)  time: 0.3583  data: 0.1652  max mem: 15572
Val:  [110/136]  eta: 0:00:09  loss: 2.5155 (2.3471)  acc1: 38.8889 (46.5966)  acc5: 77.7778 (78.8789)  time: 0.3494  data: 0.1527  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1490 (2.3101)  acc1: 55.5556 (47.8421)  acc5: 83.3333 (79.5225)  time: 0.3647  data: 0.1592  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8580 (2.2795)  acc1: 61.1111 (48.5157)  acc5: 88.8889 (80.3223)  time: 0.3324  data: 0.1415  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.9764 (2.2766)  acc1: 55.5556 (48.8124)  acc5: 88.8889 (80.3849)  time: 0.2326  data: 0.0590  max mem: 15572
Val: Total time: 0:00:49 (0.3643 s / it)
* Acc@1 47.768 Acc@5 79.484 loss 2.320
Accuracy of the network on the 4883 val videos: 47.8%
Max accuracy: 47.97%
Epoch: [35]  [   0/1404]  eta: 2:23:46  lr: 0.000005  min_lr: 0.000000  loss: 3.2592 (3.2592)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.1443  data: 5.4396  max mem: 15572
Epoch: [35]  [  10/1404]  eta: 0:25:27  lr: 0.000005  min_lr: 0.000000  loss: 3.9252 (3.8160)  class_acc: 0.3333 (0.3182)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0959  data: 0.5858  max mem: 15572
Epoch: [35]  [  20/1404]  eta: 0:20:00  lr: 0.000005  min_lr: 0.000000  loss: 4.1195 (3.9699)  class_acc: 0.2917 (0.3214)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1197  max mem: 15572
Epoch: [35]  [  30/1404]  eta: 0:17:48  lr: 0.000005  min_lr: 0.000000  loss: 4.3508 (4.0865)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1322  max mem: 15572
Epoch: [35]  [  40/1404]  eta: 0:16:52  lr: 0.000005  min_lr: 0.000000  loss: 4.2072 (4.0717)  class_acc: 0.2917 (0.3171)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1349  max mem: 15572
Epoch: [35]  [  50/1404]  eta: 0:16:15  lr: 0.000005  min_lr: 0.000000  loss: 3.9476 (4.0704)  class_acc: 0.2917 (0.3260)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1372  max mem: 15572
Epoch: [35]  [  60/1404]  eta: 0:15:16  lr: 0.000005  min_lr: 0.000000  loss: 3.9743 (4.0762)  class_acc: 0.3750 (0.3347)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0703  max mem: 15572
Epoch: [35]  [  70/1404]  eta: 0:14:54  lr: 0.000005  min_lr: 0.000000  loss: 4.0585 (4.0754)  class_acc: 0.3333 (0.3333)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0741  max mem: 15572
Epoch: [35]  [  80/1404]  eta: 0:14:34  lr: 0.000005  min_lr: 0.000000  loss: 4.1989 (4.0869)  class_acc: 0.2917 (0.3297)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1230  max mem: 15572
Epoch: [35]  [  90/1404]  eta: 0:14:18  lr: 0.000005  min_lr: 0.000000  loss: 4.2394 (4.0964)  class_acc: 0.3333 (0.3292)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1149  max mem: 15572
Epoch: [35]  [ 100/1404]  eta: 0:14:05  lr: 0.000005  min_lr: 0.000000  loss: 4.1068 (4.0814)  class_acc: 0.3750 (0.3387)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1129  max mem: 15572
Epoch: [35]  [ 110/1404]  eta: 0:13:59  lr: 0.000005  min_lr: 0.000000  loss: 3.8017 (4.0620)  class_acc: 0.3750 (0.3465)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1439  max mem: 15572
[2025-01-17 04:46:24,052] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:46:24,052] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:46:24,053] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:46:24,053] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:46:24,487] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49256
[2025-01-17 04:46:24,487] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49256
[2025-01-17 04:46:24,488] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:46:24,488] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:46:24,488] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 120/1404]  eta: 0:13:33  lr: 0.000005  min_lr: 0.000000  loss: 3.9114 (4.0775)  class_acc: 0.3750 (0.3488)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0919  max mem: 15572
[2025-01-17 04:46:29,461] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49264
[2025-01-17 04:46:29,462] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:46:29,463] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49264
[2025-01-17 04:46:29,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:46:29,464] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [35]  [ 130/1404]  eta: 0:13:24  lr: 0.000005  min_lr: 0.000000  loss: 4.2096 (4.0901)  class_acc: 0.3333 (0.3489)  loss_scale: 32768.0000 (32142.6565)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0557  max mem: 15572
Epoch: [35]  [ 140/1404]  eta: 0:13:12  lr: 0.000005  min_lr: 0.000000  loss: 4.3107 (4.0983)  class_acc: 0.2917 (0.3454)  loss_scale: 16384.0000 (31025.0213)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0980  max mem: 15572
Epoch: [35]  [ 150/1404]  eta: 0:13:12  lr: 0.000005  min_lr: 0.000000  loss: 4.0712 (4.0851)  class_acc: 0.2917 (0.3480)  loss_scale: 16384.0000 (30055.4172)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1592  max mem: 15572
Epoch: [35]  [ 160/1404]  eta: 0:13:03  lr: 0.000005  min_lr: 0.000000  loss: 4.0568 (4.0861)  class_acc: 0.2917 (0.3465)  loss_scale: 16384.0000 (29206.2609)  weight_decay: 0.0500 (0.0500)  time: 0.6471  data: 0.1744  max mem: 15572
Epoch: [35]  [ 170/1404]  eta: 0:12:46  lr: 0.000005  min_lr: 0.000000  loss: 4.2025 (4.0922)  class_acc: 0.2917 (0.3472)  loss_scale: 16384.0000 (28456.4211)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0631  max mem: 15572
Epoch: [35]  [ 180/1404]  eta: 0:12:44  lr: 0.000005  min_lr: 0.000000  loss: 4.1690 (4.0971)  class_acc: 0.3750 (0.3494)  loss_scale: 16384.0000 (27789.4365)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1026  max mem: 15572
Epoch: [35]  [ 190/1404]  eta: 0:12:31  lr: 0.000004  min_lr: 0.000000  loss: 4.0208 (4.0940)  class_acc: 0.3750 (0.3525)  loss_scale: 16384.0000 (27192.2932)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0975  max mem: 15572
Epoch: [35]  [ 200/1404]  eta: 0:12:26  lr: 0.000004  min_lr: 0.000000  loss: 4.0478 (4.0964)  class_acc: 0.3333 (0.3516)  loss_scale: 16384.0000 (26654.5672)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0623  max mem: 15572
Epoch: [35]  [ 210/1404]  eta: 0:12:14  lr: 0.000004  min_lr: 0.000000  loss: 4.1668 (4.0964)  class_acc: 0.3750 (0.3535)  loss_scale: 16384.0000 (26167.8104)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0737  max mem: 15572
Epoch: [35]  [ 220/1404]  eta: 0:12:10  lr: 0.000004  min_lr: 0.000000  loss: 4.1723 (4.1053)  class_acc: 0.3333 (0.3516)  loss_scale: 16384.0000 (25725.1041)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0956  max mem: 15572
Epoch: [35]  [ 230/1404]  eta: 0:12:01  lr: 0.000004  min_lr: 0.000000  loss: 4.2592 (4.1119)  class_acc: 0.3333 (0.3505)  loss_scale: 16384.0000 (25320.7273)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1162  max mem: 15572
Epoch: [35]  [ 240/1404]  eta: 0:11:51  lr: 0.000004  min_lr: 0.000000  loss: 4.2876 (4.1162)  class_acc: 0.2917 (0.3487)  loss_scale: 16384.0000 (24949.9087)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0601  max mem: 15572
Epoch: [35]  [ 250/1404]  eta: 0:11:43  lr: 0.000004  min_lr: 0.000000  loss: 4.2537 (4.1201)  class_acc: 0.2500 (0.3479)  loss_scale: 16384.0000 (24608.6375)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0609  max mem: 15572
[2025-01-17 04:47:44,434] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:47:44,434] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:47:44,444] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:47:44,445] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [ 260/1404]  eta: 0:11:37  lr: 0.000004  min_lr: 0.000000  loss: 4.2537 (4.1292)  class_acc: 0.2500 (0.3455)  loss_scale: 16384.0000 (24795.7088)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0874  max mem: 15572
Epoch: [35]  [ 270/1404]  eta: 0:11:32  lr: 0.000004  min_lr: 0.000000  loss: 4.1680 (4.1256)  class_acc: 0.2917 (0.3455)  loss_scale: 32768.0000 (25089.8893)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1393  max mem: 15572
Epoch: [35]  [ 280/1404]  eta: 0:11:24  lr: 0.000004  min_lr: 0.000000  loss: 4.0770 (4.1262)  class_acc: 0.3333 (0.3443)  loss_scale: 32768.0000 (25363.1317)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1090  max mem: 15572
[2025-01-17 04:48:04,545] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49426
[2025-01-17 04:48:04,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:48:04,547] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49426
[2025-01-17 04:48:04,547] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:48:04,547] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [35]  [ 290/1404]  eta: 0:11:16  lr: 0.000004  min_lr: 0.000000  loss: 4.0939 (4.1232)  class_acc: 0.3333 (0.3468)  loss_scale: 32768.0000 (25336.0825)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0573  max mem: 15572
Epoch: [35]  [ 300/1404]  eta: 0:11:10  lr: 0.000004  min_lr: 0.000000  loss: 4.1286 (4.1291)  class_acc: 0.4167 (0.3488)  loss_scale: 16384.0000 (25038.6711)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.0971  max mem: 15572
Epoch: [35]  [ 310/1404]  eta: 0:11:03  lr: 0.000004  min_lr: 0.000000  loss: 4.0692 (4.1269)  class_acc: 0.3750 (0.3482)  loss_scale: 16384.0000 (24760.3859)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0940  max mem: 15572
Epoch: [35]  [ 320/1404]  eta: 0:10:56  lr: 0.000004  min_lr: 0.000000  loss: 4.1328 (4.1304)  class_acc: 0.2917 (0.3459)  loss_scale: 16384.0000 (24499.4393)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0752  max mem: 15572
Epoch: [35]  [ 330/1404]  eta: 0:10:46  lr: 0.000004  min_lr: 0.000000  loss: 4.0816 (4.1251)  class_acc: 0.3333 (0.3489)  loss_scale: 16384.0000 (24254.2598)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0453  max mem: 15572
Epoch: [35]  [ 340/1404]  eta: 0:10:41  lr: 0.000004  min_lr: 0.000000  loss: 3.9673 (4.1199)  class_acc: 0.4167 (0.3502)  loss_scale: 16384.0000 (24023.4604)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0582  max mem: 15572
Epoch: [35]  [ 350/1404]  eta: 0:10:32  lr: 0.000004  min_lr: 0.000000  loss: 4.0706 (4.1246)  class_acc: 0.3750 (0.3498)  loss_scale: 16384.0000 (23805.8120)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0581  max mem: 15572
Epoch: [35]  [ 360/1404]  eta: 0:10:26  lr: 0.000004  min_lr: 0.000000  loss: 4.2570 (4.1243)  class_acc: 0.3750 (0.3504)  loss_scale: 16384.0000 (23600.2216)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0643  max mem: 15572
Epoch: [35]  [ 370/1404]  eta: 0:10:18  lr: 0.000004  min_lr: 0.000000  loss: 4.1312 (4.1287)  class_acc: 0.3750 (0.3505)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0899  max mem: 15572
Epoch: [35]  [ 380/1404]  eta: 0:10:14  lr: 0.000004  min_lr: 0.000000  loss: 4.1755 (4.1320)  class_acc: 0.2917 (0.3489)  loss_scale: 16384.0000 (23221.4173)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1100  max mem: 15572
Epoch: [35]  [ 390/1404]  eta: 0:10:09  lr: 0.000004  min_lr: 0.000000  loss: 4.0616 (4.1249)  class_acc: 0.2917 (0.3485)  loss_scale: 16384.0000 (23046.5473)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1680  max mem: 15572
Epoch: [35]  [ 400/1404]  eta: 0:10:02  lr: 0.000004  min_lr: 0.000000  loss: 4.0476 (4.1281)  class_acc: 0.3750 (0.3488)  loss_scale: 16384.0000 (22880.3990)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1286  max mem: 15572
Epoch: [35]  [ 410/1404]  eta: 0:09:54  lr: 0.000004  min_lr: 0.000000  loss: 4.1095 (4.1230)  class_acc: 0.3333 (0.3487)  loss_scale: 16384.0000 (22722.3358)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0454  max mem: 15572
[2025-01-17 04:49:19,313] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:49:19,313] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:49:19,313] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:49:19,313] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [ 420/1404]  eta: 0:09:48  lr: 0.000004  min_lr: 0.000000  loss: 4.0309 (4.1232)  class_acc: 0.3333 (0.3501)  loss_scale: 16384.0000 (22805.2827)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0431  max mem: 15572
Epoch: [35]  [ 430/1404]  eta: 0:09:44  lr: 0.000004  min_lr: 0.000000  loss: 4.1108 (4.1229)  class_acc: 0.3333 (0.3502)  loss_scale: 32768.0000 (23036.4362)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1342  max mem: 15572
Epoch: [35]  [ 440/1404]  eta: 0:09:37  lr: 0.000004  min_lr: 0.000000  loss: 4.0958 (4.1195)  class_acc: 0.2917 (0.3491)  loss_scale: 32768.0000 (23257.1066)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1195  max mem: 15572
Epoch: [35]  [ 450/1404]  eta: 0:09:30  lr: 0.000004  min_lr: 0.000000  loss: 4.1538 (4.1237)  class_acc: 0.2917 (0.3489)  loss_scale: 32768.0000 (23467.9911)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0665  max mem: 15572
Epoch: [35]  [ 460/1404]  eta: 0:09:23  lr: 0.000004  min_lr: 0.000000  loss: 4.1990 (4.1215)  class_acc: 0.3750 (0.3492)  loss_scale: 32768.0000 (23669.7267)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0930  max mem: 15572
Epoch: [35]  [ 470/1404]  eta: 0:09:18  lr: 0.000004  min_lr: 0.000000  loss: 4.2254 (4.1230)  class_acc: 0.3750 (0.3495)  loss_scale: 32768.0000 (23862.8960)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1103  max mem: 15572
Epoch: [35]  [ 480/1404]  eta: 0:09:10  lr: 0.000004  min_lr: 0.000000  loss: 4.1718 (4.1234)  class_acc: 0.3750 (0.3507)  loss_scale: 32768.0000 (24048.0333)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.0564  max mem: 15572
Epoch: [35]  [ 490/1404]  eta: 0:09:03  lr: 0.000004  min_lr: 0.000000  loss: 4.0747 (4.1216)  class_acc: 0.3750 (0.3506)  loss_scale: 32768.0000 (24225.6293)  weight_decay: 0.0500 (0.0500)  time: 0.4974  data: 0.0008  max mem: 15572
Epoch: [35]  [ 500/1404]  eta: 0:08:57  lr: 0.000004  min_lr: 0.000000  loss: 4.0918 (4.1243)  class_acc: 0.3333 (0.3501)  loss_scale: 32768.0000 (24396.1357)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0012  max mem: 15572
Epoch: [35]  [ 510/1404]  eta: 0:08:53  lr: 0.000004  min_lr: 0.000000  loss: 4.1776 (4.1268)  class_acc: 0.3333 (0.3502)  loss_scale: 32768.0000 (24559.9687)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.0012  max mem: 15572
Epoch: [35]  [ 520/1404]  eta: 0:08:47  lr: 0.000004  min_lr: 0.000000  loss: 4.1350 (4.1256)  class_acc: 0.3750 (0.3506)  loss_scale: 32768.0000 (24717.5125)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.0006  max mem: 15572
Epoch: [35]  [ 530/1404]  eta: 0:08:40  lr: 0.000004  min_lr: 0.000000  loss: 4.1350 (4.1265)  class_acc: 0.4167 (0.3525)  loss_scale: 32768.0000 (24869.1224)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0005  max mem: 15572
Epoch: [35]  [ 540/1404]  eta: 0:08:35  lr: 0.000004  min_lr: 0.000000  loss: 4.1508 (4.1268)  class_acc: 0.4167 (0.3530)  loss_scale: 32768.0000 (25015.1275)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0006  max mem: 15572
[2025-01-17 04:50:33,854] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:50:33,854] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:50:33,857] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:50:33,857] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [ 550/1404]  eta: 0:08:27  lr: 0.000004  min_lr: 0.000000  loss: 4.1579 (4.1285)  class_acc: 0.3750 (0.3531)  loss_scale: 32768.0000 (25631.5935)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0008  max mem: 15572
[2025-01-17 04:50:37,984] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49691
[2025-01-17 04:50:37,985] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:50:37,985] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 04:50:38,045] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49691
[2025-01-17 04:50:38,045] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [35]  [ 560/1404]  eta: 0:08:21  lr: 0.000004  min_lr: 0.000000  loss: 4.1729 (4.1281)  class_acc: 0.3750 (0.3532)  loss_scale: 32768.0000 (25758.8021)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0373  max mem: 15572
Epoch: [35]  [ 570/1404]  eta: 0:08:16  lr: 0.000004  min_lr: 0.000000  loss: 4.0245 (4.1254)  class_acc: 0.4167 (0.3557)  loss_scale: 32768.0000 (25881.5552)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.0999  max mem: 15572
Epoch: [35]  [ 580/1404]  eta: 0:08:10  lr: 0.000004  min_lr: 0.000000  loss: 4.0553 (4.1262)  class_acc: 0.4167 (0.3552)  loss_scale: 32768.0000 (26000.0826)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1148  max mem: 15572
[2025-01-17 04:50:58,484] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49725
[2025-01-17 04:50:58,484] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49725
[2025-01-17 04:50:58,484] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:50:58,484] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:50:58,484] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [35]  [ 590/1404]  eta: 0:08:05  lr: 0.000004  min_lr: 0.000000  loss: 4.0359 (4.1263)  class_acc: 0.3750 (0.3562)  loss_scale: 32768.0000 (25948.2640)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1074  max mem: 15572
Epoch: [35]  [ 600/1404]  eta: 0:07:57  lr: 0.000004  min_lr: 0.000000  loss: 4.0957 (4.1285)  class_acc: 0.3750 (0.3556)  loss_scale: 16384.0000 (25789.1248)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0560  max mem: 15572
Epoch: [35]  [ 610/1404]  eta: 0:07:51  lr: 0.000004  min_lr: 0.000000  loss: 4.1456 (4.1270)  class_acc: 0.3333 (0.3555)  loss_scale: 16384.0000 (25635.1948)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0411  max mem: 15572
Epoch: [35]  [ 620/1404]  eta: 0:07:45  lr: 0.000004  min_lr: 0.000000  loss: 4.0479 (4.1264)  class_acc: 0.4167 (0.3570)  loss_scale: 16384.0000 (25486.2222)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.0918  max mem: 15572
Epoch: [35]  [ 630/1404]  eta: 0:07:40  lr: 0.000004  min_lr: 0.000000  loss: 4.0847 (4.1274)  class_acc: 0.3750 (0.3566)  loss_scale: 16384.0000 (25341.9715)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.1112  max mem: 15572
Epoch: [35]  [ 640/1404]  eta: 0:07:34  lr: 0.000004  min_lr: 0.000000  loss: 4.0847 (4.1246)  class_acc: 0.3750 (0.3569)  loss_scale: 16384.0000 (25202.2215)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1051  max mem: 15572
Epoch: [35]  [ 650/1404]  eta: 0:07:29  lr: 0.000004  min_lr: 0.000000  loss: 4.1458 (4.1255)  class_acc: 0.3750 (0.3571)  loss_scale: 16384.0000 (25066.7650)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1173  max mem: 15572
Epoch: [35]  [ 660/1404]  eta: 0:07:22  lr: 0.000004  min_lr: 0.000000  loss: 4.2356 (4.1277)  class_acc: 0.3750 (0.3581)  loss_scale: 16384.0000 (24935.4070)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0727  max mem: 15572
Epoch: [35]  [ 670/1404]  eta: 0:07:15  lr: 0.000004  min_lr: 0.000000  loss: 4.1376 (4.1277)  class_acc: 0.3750 (0.3579)  loss_scale: 16384.0000 (24807.9642)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0195  max mem: 15572
Epoch: [35]  [ 680/1404]  eta: 0:07:09  lr: 0.000004  min_lr: 0.000000  loss: 4.1376 (4.1281)  class_acc: 0.2917 (0.3565)  loss_scale: 16384.0000 (24684.2643)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0352  max mem: 15572
Epoch: [35]  [ 690/1404]  eta: 0:07:03  lr: 0.000004  min_lr: 0.000000  loss: 4.1627 (4.1288)  class_acc: 0.2500 (0.3561)  loss_scale: 16384.0000 (24564.1447)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0456  max mem: 15572
Epoch: [35]  [ 700/1404]  eta: 0:06:57  lr: 0.000004  min_lr: 0.000000  loss: 4.1627 (4.1295)  class_acc: 0.3750 (0.3563)  loss_scale: 16384.0000 (24447.4522)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0968  max mem: 15572
Epoch: [35]  [ 710/1404]  eta: 0:06:51  lr: 0.000004  min_lr: 0.000000  loss: 4.1803 (4.1293)  class_acc: 0.2917 (0.3558)  loss_scale: 16384.0000 (24334.0422)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1098  max mem: 15572
[2025-01-17 04:52:13,057] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:52:13,057] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:52:13,057] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:52:13,057] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [ 720/1404]  eta: 0:06:45  lr: 0.000004  min_lr: 0.000000  loss: 4.1721 (4.1294)  class_acc: 0.3333 (0.3564)  loss_scale: 16384.0000 (24382.8460)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0904  max mem: 15572
Epoch: [35]  [ 730/1404]  eta: 0:06:39  lr: 0.000004  min_lr: 0.000000  loss: 4.1321 (4.1300)  class_acc: 0.3750 (0.3552)  loss_scale: 32768.0000 (24497.5540)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0967  max mem: 15572
Epoch: [35]  [ 740/1404]  eta: 0:06:35  lr: 0.000004  min_lr: 0.000000  loss: 4.1217 (4.1301)  class_acc: 0.2917 (0.3552)  loss_scale: 32768.0000 (24609.1660)  weight_decay: 0.0500 (0.0500)  time: 0.6772  data: 0.1642  max mem: 15572
Epoch: [35]  [ 750/1404]  eta: 0:06:28  lr: 0.000004  min_lr: 0.000000  loss: 4.0534 (4.1296)  class_acc: 0.2917 (0.3541)  loss_scale: 32768.0000 (24717.8056)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1156  max mem: 15572
Epoch: [35]  [ 760/1404]  eta: 0:06:21  lr: 0.000004  min_lr: 0.000000  loss: 4.1191 (4.1318)  class_acc: 0.3333 (0.3543)  loss_scale: 32768.0000 (24823.5900)  weight_decay: 0.0500 (0.0500)  time: 0.5028  data: 0.0092  max mem: 15572
Epoch: [35]  [ 770/1404]  eta: 0:06:15  lr: 0.000004  min_lr: 0.000000  loss: 4.0878 (4.1310)  class_acc: 0.3333 (0.3541)  loss_scale: 32768.0000 (24926.6304)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0528  max mem: 15572
Epoch: [35]  [ 780/1404]  eta: 0:06:09  lr: 0.000004  min_lr: 0.000000  loss: 4.0878 (4.1326)  class_acc: 0.3333 (0.3538)  loss_scale: 32768.0000 (25027.0320)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0844  max mem: 15572
Epoch: [35]  [ 790/1404]  eta: 0:06:03  lr: 0.000004  min_lr: 0.000000  loss: 4.1964 (4.1334)  class_acc: 0.3333 (0.3532)  loss_scale: 32768.0000 (25124.8951)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1206  max mem: 15572
Epoch: [35]  [ 800/1404]  eta: 0:05:57  lr: 0.000004  min_lr: 0.000000  loss: 4.2000 (4.1345)  class_acc: 0.3333 (0.3529)  loss_scale: 32768.0000 (25220.3146)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1000  max mem: 15572
Epoch: [35]  [ 810/1404]  eta: 0:05:51  lr: 0.000004  min_lr: 0.000000  loss: 4.2108 (4.1351)  class_acc: 0.3333 (0.3536)  loss_scale: 32768.0000 (25313.3810)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0717  max mem: 15572
[2025-01-17 04:53:13,975] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49957
[2025-01-17 04:53:13,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:53:13,982] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49957
[2025-01-17 04:53:13,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:53:13,983] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [35]  [ 820/1404]  eta: 0:05:45  lr: 0.000004  min_lr: 0.000000  loss: 4.1964 (4.1335)  class_acc: 0.4167 (0.3540)  loss_scale: 32768.0000 (25324.3557)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0595  max mem: 15572
Epoch: [35]  [ 830/1404]  eta: 0:05:39  lr: 0.000004  min_lr: 0.000000  loss: 4.0184 (4.1326)  class_acc: 0.4583 (0.3554)  loss_scale: 16384.0000 (25216.7702)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0325  max mem: 15572
Epoch: [35]  [ 840/1404]  eta: 0:05:33  lr: 0.000004  min_lr: 0.000000  loss: 4.1813 (4.1347)  class_acc: 0.3750 (0.3549)  loss_scale: 16384.0000 (25111.7432)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0725  max mem: 15572
Epoch: [35]  [ 850/1404]  eta: 0:05:28  lr: 0.000004  min_lr: 0.000000  loss: 4.3246 (4.1360)  class_acc: 0.3333 (0.3554)  loss_scale: 16384.0000 (25009.1845)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1313  max mem: 15572
[2025-01-17 04:53:40,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=50000, skipped=297, lr=[3.565083453080395e-08, 3.565083453080395e-08, 5.092976361543422e-08, 5.092976361543422e-08, 7.275680516490604e-08, 7.275680516490604e-08, 1.0393829309272291e-07, 1.0393829309272291e-07, 1.4848327584674703e-07, 1.4848327584674703e-07, 2.1211896549535291e-07, 2.1211896549535291e-07, 3.030270935647899e-07, 3.030270935647899e-07, 4.328958479496999e-07, 4.328958479496999e-07, 6.184226399281426e-07, 6.184226399281426e-07, 8.834609141830612e-07, 8.834609141830612e-07, 1.2620870202615158e-06, 1.2620870202615158e-06, 1.8029814575164513e-06, 1.8029814575164513e-06, 2.5756877964520737e-06, 2.5756877964520737e-06, 3.679553994931534e-06, 3.679553994931534e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 04:53:40,157] [INFO] [timer.py:260:stop] epoch=0/micro_step=50000/global_step=50000, RunningAvgSamplesPerSec=48.17505064488608, CurrSamplesPerSec=62.628114660571214, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [35]  [ 860/1404]  eta: 0:05:22  lr: 0.000004  min_lr: 0.000000  loss: 4.1393 (4.1333)  class_acc: 0.3750 (0.3556)  loss_scale: 16384.0000 (24909.0081)  weight_decay: 0.0500 (0.0500)  time: 0.6645  data: 0.1831  max mem: 15572
Epoch: [35]  [ 870/1404]  eta: 0:05:16  lr: 0.000004  min_lr: 0.000000  loss: 4.0030 (4.1332)  class_acc: 0.4167 (0.3565)  loss_scale: 16384.0000 (24811.1320)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0998  max mem: 15572
Epoch: [35]  [ 880/1404]  eta: 0:05:09  lr: 0.000004  min_lr: 0.000000  loss: 3.9921 (4.1312)  class_acc: 0.4583 (0.3577)  loss_scale: 16384.0000 (24715.4779)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0005  max mem: 15572
Epoch: [35]  [ 890/1404]  eta: 0:05:03  lr: 0.000004  min_lr: 0.000000  loss: 4.0059 (4.1308)  class_acc: 0.4167 (0.3581)  loss_scale: 16384.0000 (24621.9708)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0212  max mem: 15572
Epoch: [35]  [ 900/1404]  eta: 0:04:57  lr: 0.000004  min_lr: 0.000000  loss: 3.9777 (4.1274)  class_acc: 0.4167 (0.3589)  loss_scale: 16384.0000 (24530.5394)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0989  max mem: 15572
Epoch: [35]  [ 910/1404]  eta: 0:04:51  lr: 0.000004  min_lr: 0.000000  loss: 3.9777 (4.1270)  class_acc: 0.4167 (0.3591)  loss_scale: 16384.0000 (24441.1153)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1232  max mem: 15572
Epoch: [35]  [ 920/1404]  eta: 0:04:46  lr: 0.000004  min_lr: 0.000000  loss: 4.0998 (4.1267)  class_acc: 0.3333 (0.3586)  loss_scale: 16384.0000 (24353.6330)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1053  max mem: 15572
Epoch: [35]  [ 930/1404]  eta: 0:04:40  lr: 0.000004  min_lr: 0.000000  loss: 4.2253 (4.1279)  class_acc: 0.2917 (0.3578)  loss_scale: 16384.0000 (24268.0301)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1078  max mem: 15572
Epoch: [35]  [ 940/1404]  eta: 0:04:34  lr: 0.000004  min_lr: 0.000000  loss: 4.1931 (4.1277)  class_acc: 0.2917 (0.3578)  loss_scale: 16384.0000 (24184.2465)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0909  max mem: 15572
[2025-01-17 04:54:29,374] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:54:29,375] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:54:29,376] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:54:29,376] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [ 950/1404]  eta: 0:04:28  lr: 0.000004  min_lr: 0.000000  loss: 4.1931 (4.1281)  class_acc: 0.3750 (0.3580)  loss_scale: 16384.0000 (24188.3659)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0623  max mem: 15572
Epoch: [35]  [ 960/1404]  eta: 0:04:22  lr: 0.000004  min_lr: 0.000000  loss: 4.1899 (4.1286)  class_acc: 0.3750 (0.3586)  loss_scale: 32768.0000 (24277.6441)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0840  max mem: 15572
[2025-01-17 04:54:41,363] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50107
[2025-01-17 04:54:41,363] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50107
[2025-01-17 04:54:41,364] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:54:41,364] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 04:54:41,364] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [35]  [ 970/1404]  eta: 0:04:16  lr: 0.000004  min_lr: 0.000000  loss: 4.0789 (4.1270)  class_acc: 0.4167 (0.3593)  loss_scale: 32768.0000 (24297.5901)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1174  max mem: 15572
Epoch: [35]  [ 980/1404]  eta: 0:04:10  lr: 0.000004  min_lr: 0.000000  loss: 4.1266 (4.1279)  class_acc: 0.3750 (0.3592)  loss_scale: 16384.0000 (24216.9215)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0527  max mem: 15572
Epoch: [35]  [ 990/1404]  eta: 0:04:03  lr: 0.000004  min_lr: 0.000000  loss: 3.9971 (4.1257)  class_acc: 0.3750 (0.3598)  loss_scale: 16384.0000 (24137.8809)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0127  max mem: 15572
Epoch: [35]  [1000/1404]  eta: 0:03:58  lr: 0.000004  min_lr: 0.000000  loss: 3.9971 (4.1262)  class_acc: 0.3750 (0.3596)  loss_scale: 16384.0000 (24060.4196)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0643  max mem: 15572
Epoch: [35]  [1010/1404]  eta: 0:03:52  lr: 0.000004  min_lr: 0.000000  loss: 4.1627 (4.1256)  class_acc: 0.3333 (0.3597)  loss_scale: 16384.0000 (23984.4906)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0808  max mem: 15572
Epoch: [35]  [1020/1404]  eta: 0:03:46  lr: 0.000003  min_lr: 0.000000  loss: 4.1684 (4.1273)  class_acc: 0.3333 (0.3599)  loss_scale: 16384.0000 (23910.0490)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1170  max mem: 15572
Epoch: [35]  [1030/1404]  eta: 0:03:40  lr: 0.000003  min_lr: 0.000000  loss: 4.1197 (4.1268)  class_acc: 0.3333 (0.3596)  loss_scale: 16384.0000 (23837.0514)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1231  max mem: 15572
Epoch: [35]  [1040/1404]  eta: 0:03:34  lr: 0.000003  min_lr: 0.000000  loss: 3.9974 (4.1264)  class_acc: 0.3333 (0.3596)  loss_scale: 16384.0000 (23765.4563)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0764  max mem: 15572
Epoch: [35]  [1050/1404]  eta: 0:03:28  lr: 0.000003  min_lr: 0.000000  loss: 4.1563 (4.1266)  class_acc: 0.3333 (0.3597)  loss_scale: 16384.0000 (23695.2236)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.0852  max mem: 15572
Epoch: [35]  [1060/1404]  eta: 0:03:23  lr: 0.000003  min_lr: 0.000000  loss: 4.1280 (4.1253)  class_acc: 0.4167 (0.3604)  loss_scale: 16384.0000 (23626.3148)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1364  max mem: 15572
Epoch: [35]  [1070/1404]  eta: 0:03:16  lr: 0.000003  min_lr: 0.000000  loss: 4.0613 (4.1253)  class_acc: 0.3750 (0.3604)  loss_scale: 16384.0000 (23558.6928)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0929  max mem: 15572
Epoch: [35]  [1080/1404]  eta: 0:03:10  lr: 0.000003  min_lr: 0.000000  loss: 4.1180 (4.1241)  class_acc: 0.3333 (0.3601)  loss_scale: 16384.0000 (23492.3219)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0007  max mem: 15572
Epoch: [35]  [1090/1404]  eta: 0:03:04  lr: 0.000003  min_lr: 0.000000  loss: 4.1039 (4.1246)  class_acc: 0.3333 (0.3602)  loss_scale: 16384.0000 (23427.1677)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0196  max mem: 15572
[2025-01-17 04:55:55,065] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:55:55,066] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 04:55:55,067] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:55:55,067] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [1100/1404]  eta: 0:02:58  lr: 0.000003  min_lr: 0.000000  loss: 4.1951 (4.1251)  class_acc: 0.3750 (0.3604)  loss_scale: 16384.0000 (23437.6022)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0404  max mem: 15572
Epoch: [35]  [1110/1404]  eta: 0:02:53  lr: 0.000003  min_lr: 0.000000  loss: 4.2294 (4.1254)  class_acc: 0.3750 (0.3606)  loss_scale: 32768.0000 (23521.5842)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.1512  max mem: 15572
Epoch: [35]  [1120/1404]  eta: 0:02:47  lr: 0.000003  min_lr: 0.000000  loss: 4.1231 (4.1248)  class_acc: 0.3750 (0.3605)  loss_scale: 32768.0000 (23604.0678)  weight_decay: 0.0500 (0.0500)  time: 0.6708  data: 0.1828  max mem: 15572
Epoch: [35]  [1130/1404]  eta: 0:02:41  lr: 0.000003  min_lr: 0.000000  loss: 4.1137 (4.1249)  class_acc: 0.3750 (0.3604)  loss_scale: 32768.0000 (23685.0928)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0860  max mem: 15572
Epoch: [35]  [1140/1404]  eta: 0:02:35  lr: 0.000003  min_lr: 0.000000  loss: 4.1137 (4.1252)  class_acc: 0.3750 (0.3603)  loss_scale: 32768.0000 (23764.6976)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0890  max mem: 15572
Epoch: [35]  [1150/1404]  eta: 0:02:29  lr: 0.000003  min_lr: 0.000000  loss: 4.2554 (4.1263)  class_acc: 0.2917 (0.3597)  loss_scale: 32768.0000 (23842.9192)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1068  max mem: 15572
Epoch: [35]  [1160/1404]  eta: 0:02:23  lr: 0.000003  min_lr: 0.000000  loss: 4.2221 (4.1262)  class_acc: 0.3333 (0.3597)  loss_scale: 32768.0000 (23919.7933)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1343  max mem: 15572
Epoch: [35]  [1170/1404]  eta: 0:02:17  lr: 0.000003  min_lr: 0.000000  loss: 3.9776 (4.1243)  class_acc: 0.3333 (0.3594)  loss_scale: 32768.0000 (23995.3544)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1208  max mem: 15572
Epoch: [35]  [1180/1404]  eta: 0:02:11  lr: 0.000003  min_lr: 0.000000  loss: 3.9643 (4.1240)  class_acc: 0.3333 (0.3598)  loss_scale: 32768.0000 (24069.6359)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0684  max mem: 15572
Epoch: [35]  [1190/1404]  eta: 0:02:06  lr: 0.000003  min_lr: 0.000000  loss: 4.1231 (4.1228)  class_acc: 0.4167 (0.3598)  loss_scale: 32768.0000 (24142.6700)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0601  max mem: 15572
Epoch: [35]  [1200/1404]  eta: 0:02:00  lr: 0.000003  min_lr: 0.000000  loss: 4.1309 (4.1239)  class_acc: 0.3750 (0.3599)  loss_scale: 32768.0000 (24214.4879)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0772  max mem: 15572
Epoch: [35]  [1210/1404]  eta: 0:01:54  lr: 0.000003  min_lr: 0.000000  loss: 4.1466 (4.1236)  class_acc: 0.4167 (0.3601)  loss_scale: 32768.0000 (24285.1197)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.0969  max mem: 15572
Epoch: [35]  [1220/1404]  eta: 0:01:48  lr: 0.000003  min_lr: 0.000000  loss: 4.0871 (4.1236)  class_acc: 0.4167 (0.3605)  loss_scale: 32768.0000 (24354.5946)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0493  max mem: 15572
[2025-01-17 04:57:11,392] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:57:11,392] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:57:11,400] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:57:11,401] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:57:12,383] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50366
[2025-01-17 04:57:12,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:57:12,383] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 04:57:12,445] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50366
[2025-01-17 04:57:12,446] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [35]  [1230/1404]  eta: 0:01:42  lr: 0.000003  min_lr: 0.000000  loss: 4.0985 (4.1233)  class_acc: 0.4167 (0.3606)  loss_scale: 32768.0000 (24476.1787)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0476  max mem: 15572
Epoch: [35]  [1240/1404]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000000  loss: 4.0985 (4.1231)  class_acc: 0.3333 (0.3606)  loss_scale: 32768.0000 (24542.9944)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1382  max mem: 15572
Epoch: [35]  [1250/1404]  eta: 0:01:30  lr: 0.000003  min_lr: 0.000000  loss: 4.0052 (4.1223)  class_acc: 0.3333 (0.3608)  loss_scale: 32768.0000 (24608.7418)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1053  max mem: 15572
Epoch: [35]  [1260/1404]  eta: 0:01:24  lr: 0.000003  min_lr: 0.000000  loss: 4.2534 (4.1240)  class_acc: 0.3333 (0.3604)  loss_scale: 32768.0000 (24673.4465)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1112  max mem: 15572
Epoch: [35]  [1270/1404]  eta: 0:01:18  lr: 0.000003  min_lr: 0.000000  loss: 4.3454 (4.1256)  class_acc: 0.2500 (0.3599)  loss_scale: 32768.0000 (24737.1330)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0971  max mem: 15572
Epoch: [35]  [1280/1404]  eta: 0:01:12  lr: 0.000003  min_lr: 0.000000  loss: 4.2943 (4.1263)  class_acc: 0.2917 (0.3600)  loss_scale: 32768.0000 (24799.8251)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0007  max mem: 15572
Epoch: [35]  [1290/1404]  eta: 0:01:07  lr: 0.000003  min_lr: 0.000000  loss: 4.0970 (4.1267)  class_acc: 0.3333 (0.3596)  loss_scale: 32768.0000 (24861.5461)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0695  max mem: 15572
Epoch: [35]  [1300/1404]  eta: 0:01:01  lr: 0.000003  min_lr: 0.000000  loss: 4.1647 (4.1277)  class_acc: 0.2500 (0.3588)  loss_scale: 32768.0000 (24922.3182)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1214  max mem: 15572
Epoch: [35]  [1310/1404]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 4.2511 (4.1277)  class_acc: 0.3333 (0.3585)  loss_scale: 32768.0000 (24982.1632)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0782  max mem: 15572
Epoch: [35]  [1320/1404]  eta: 0:00:49  lr: 0.000003  min_lr: 0.000000  loss: 4.2801 (4.1291)  class_acc: 0.3333 (0.3583)  loss_scale: 32768.0000 (25041.1022)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0739  max mem: 15572
Epoch: [35]  [1330/1404]  eta: 0:00:43  lr: 0.000003  min_lr: 0.000000  loss: 4.2608 (4.1296)  class_acc: 0.3333 (0.3577)  loss_scale: 32768.0000 (25099.1555)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.1501  max mem: 15572
Epoch: [35]  [1340/1404]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000000  loss: 4.1902 (4.1293)  class_acc: 0.2917 (0.3578)  loss_scale: 32768.0000 (25156.3430)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1057  max mem: 15572
Epoch: [35]  [1350/1404]  eta: 0:00:31  lr: 0.000003  min_lr: 0.000000  loss: 3.9879 (4.1284)  class_acc: 0.3750 (0.3580)  loss_scale: 32768.0000 (25212.6839)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0711  max mem: 15572
[2025-01-17 04:58:29,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:58:29,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:58:29,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 04:58:29,578] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 04:58:30,030] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50496
[2025-01-17 04:58:30,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 04:58:30,030] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50496
[2025-01-17 04:58:30,031] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 04:58:30,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [35]  [1360/1404]  eta: 0:00:25  lr: 0.000003  min_lr: 0.000000  loss: 3.9716 (4.1285)  class_acc: 0.3750 (0.3578)  loss_scale: 32768.0000 (25292.2733)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.0677  max mem: 15572
Epoch: [35]  [1370/1404]  eta: 0:00:20  lr: 0.000003  min_lr: 0.000000  loss: 4.1598 (4.1285)  class_acc: 0.3333 (0.3578)  loss_scale: 32768.0000 (25346.8009)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0395  max mem: 15572
Epoch: [35]  [1380/1404]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 4.2101 (4.1287)  class_acc: 0.3333 (0.3574)  loss_scale: 32768.0000 (25400.5387)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1119  max mem: 15572
Epoch: [35]  [1390/1404]  eta: 0:00:08  lr: 0.000003  min_lr: 0.000000  loss: 4.2263 (4.1295)  class_acc: 0.3333 (0.3576)  loss_scale: 32768.0000 (25453.5040)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0730  max mem: 15572
Epoch: [35]  [1400/1404]  eta: 0:00:02  lr: 0.000003  min_lr: 0.000000  loss: 4.2328 (4.1303)  class_acc: 0.3333 (0.3573)  loss_scale: 32768.0000 (25505.7131)  weight_decay: 0.0500 (0.0500)  time: 0.4107  data: 0.0004  max mem: 15572
Epoch: [35]  [1403/1404]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 4.2120 (4.1303)  class_acc: 0.3333 (0.3575)  loss_scale: 32768.0000 (25521.2308)  weight_decay: 0.0500 (0.0500)  time: 0.3959  data: 0.0004  max mem: 15572
Epoch: [35] Total time: 0:13:44 (0.5870 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 4.2120 (4.1222)  class_acc: 0.3333 (0.3584)  loss_scale: 32768.0000 (25521.2308)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:13:15  loss: 1.6633 (1.6633)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.8489  data: 5.6571  max mem: 15572
Val:  [ 10/136]  eta: 0:01:47  loss: 2.2471 (2.1649)  acc1: 55.5556 (53.0303)  acc5: 83.3333 (81.8182)  time: 0.8493  data: 0.6595  max mem: 15572
Val:  [ 20/136]  eta: 0:01:08  loss: 2.4472 (2.3483)  acc1: 44.4444 (48.6772)  acc5: 72.2222 (76.9841)  time: 0.3240  data: 0.1251  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 2.2658 (2.2716)  acc1: 44.4444 (48.7455)  acc5: 77.7778 (78.8530)  time: 0.3197  data: 0.1102  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.0158 (2.2369)  acc1: 55.5556 (50.9485)  acc5: 83.3333 (79.4038)  time: 0.3052  data: 0.1022  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.1213 (2.2405)  acc1: 50.0000 (50.6536)  acc5: 83.3333 (79.7386)  time: 0.3327  data: 0.1389  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.2430 (2.3047)  acc1: 44.4444 (48.2696)  acc5: 77.7778 (78.8707)  time: 0.3766  data: 0.1877  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.2752 (2.2868)  acc1: 44.4444 (48.9828)  acc5: 77.7778 (79.4210)  time: 0.3424  data: 0.1485  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.1508 (2.2822)  acc1: 50.0000 (48.5597)  acc5: 83.3333 (79.7668)  time: 0.3279  data: 0.1297  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.1836 (2.2882)  acc1: 44.4444 (48.2906)  acc5: 77.7778 (79.4261)  time: 0.3348  data: 0.1383  max mem: 15572
Val:  [100/136]  eta: 0:00:13  loss: 2.5374 (2.3446)  acc1: 38.8889 (46.6447)  acc5: 72.2222 (77.9428)  time: 0.3363  data: 0.1301  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.4430 (2.3387)  acc1: 38.8889 (46.8468)  acc5: 72.2222 (77.8779)  time: 0.3659  data: 0.1547  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1710 (2.3049)  acc1: 55.5556 (47.9798)  acc5: 83.3333 (78.5583)  time: 0.3701  data: 0.1667  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.9108 (2.2727)  acc1: 61.1111 (48.9822)  acc5: 88.8889 (79.3893)  time: 0.2910  data: 0.1168  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.9319 (2.2688)  acc1: 55.5556 (49.2629)  acc5: 88.8889 (79.4840)  time: 0.2012  data: 0.0457  max mem: 15572
Val: Total time: 0:00:49 (0.3667 s / it)
* Acc@1 47.993 Acc@5 79.013 loss 2.313
Accuracy of the network on the 4883 val videos: 48.0%
[2025-01-17 04:59:43,727] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 04:59:43,729] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 04:59:43,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 04:59:43,729] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 04:59:46,112] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 04:59:46,112] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 47.99%
Epoch: [36]  [   0/1404]  eta: 3:01:04  lr: 0.000003  min_lr: 0.000000  loss: 4.7327 (4.7327)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.7384  data: 7.2301  max mem: 15572
Epoch: [36]  [  10/1404]  eta: 0:27:05  lr: 0.000003  min_lr: 0.000000  loss: 4.2739 (4.3500)  class_acc: 0.3333 (0.3295)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1659  data: 0.6579  max mem: 15572
Epoch: [36]  [  20/1404]  eta: 0:20:24  lr: 0.000003  min_lr: 0.000000  loss: 4.1631 (4.2288)  class_acc: 0.3333 (0.3274)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0215  max mem: 15572
Epoch: [36]  [  30/1404]  eta: 0:18:00  lr: 0.000003  min_lr: 0.000000  loss: 4.0834 (4.2222)  class_acc: 0.2917 (0.3172)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0290  max mem: 15572
Epoch: [36]  [  40/1404]  eta: 0:16:26  lr: 0.000003  min_lr: 0.000000  loss: 4.1914 (4.1959)  class_acc: 0.2917 (0.3232)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0083  max mem: 15572
Epoch: [36]  [  50/1404]  eta: 0:15:18  lr: 0.000003  min_lr: 0.000000  loss: 4.1914 (4.2051)  class_acc: 0.3333 (0.3243)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5111  data: 0.0079  max mem: 15572
Epoch: [36]  [  60/1404]  eta: 0:14:56  lr: 0.000003  min_lr: 0.000000  loss: 4.1013 (4.1815)  class_acc: 0.3750 (0.3415)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0607  max mem: 15572
Epoch: [36]  [  70/1404]  eta: 0:14:35  lr: 0.000003  min_lr: 0.000000  loss: 4.1013 (4.1930)  class_acc: 0.3750 (0.3468)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0536  max mem: 15572
Epoch: [36]  [  80/1404]  eta: 0:14:14  lr: 0.000003  min_lr: 0.000000  loss: 4.0025 (4.1498)  class_acc: 0.4167 (0.3611)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0376  max mem: 15572
[2025-01-17 05:00:39,012] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:00:39,013] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:00:39,013] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:00:39,014] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:00:39,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50626
[2025-01-17 05:00:39,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:00:39,481] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 05:00:39,548] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50626
[2025-01-17 05:00:39,548] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [36]  [  90/1404]  eta: 0:14:01  lr: 0.000003  min_lr: 0.000000  loss: 4.0021 (4.1306)  class_acc: 0.4167 (0.3663)  loss_scale: 32768.0000 (33128.0879)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0880  max mem: 15572
Epoch: [36]  [ 100/1404]  eta: 0:13:55  lr: 0.000003  min_lr: 0.000000  loss: 4.1775 (4.1550)  class_acc: 0.3750 (0.3626)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.1203  max mem: 15572
Epoch: [36]  [ 110/1404]  eta: 0:13:48  lr: 0.000003  min_lr: 0.000000  loss: 4.1775 (4.1413)  class_acc: 0.3333 (0.3630)  loss_scale: 32768.0000 (33063.2072)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1368  max mem: 15572
Epoch: [36]  [ 120/1404]  eta: 0:13:40  lr: 0.000003  min_lr: 0.000000  loss: 4.1559 (4.1513)  class_acc: 0.3750 (0.3623)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1470  max mem: 15572
Epoch: [36]  [ 130/1404]  eta: 0:13:43  lr: 0.000003  min_lr: 0.000000  loss: 4.1846 (4.1526)  class_acc: 0.2917 (0.3540)  loss_scale: 32768.0000 (33018.1374)  weight_decay: 0.0500 (0.0500)  time: 0.6814  data: 0.1894  max mem: 15572
Epoch: [36]  [ 140/1404]  eta: 0:13:25  lr: 0.000003  min_lr: 0.000000  loss: 4.1385 (4.1505)  class_acc: 0.2917 (0.3552)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1236  max mem: 15572
Epoch: [36]  [ 150/1404]  eta: 0:13:17  lr: 0.000003  min_lr: 0.000000  loss: 4.2395 (4.1602)  class_acc: 0.2917 (0.3507)  loss_scale: 32768.0000 (32985.0066)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0631  max mem: 15572
Epoch: [36]  [ 160/1404]  eta: 0:13:06  lr: 0.000003  min_lr: 0.000000  loss: 4.2678 (4.1680)  class_acc: 0.3333 (0.3538)  loss_scale: 32768.0000 (32971.5280)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0915  max mem: 15572
Epoch: [36]  [ 170/1404]  eta: 0:12:56  lr: 0.000003  min_lr: 0.000000  loss: 4.2210 (4.1695)  class_acc: 0.3333 (0.3521)  loss_scale: 32768.0000 (32959.6257)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0888  max mem: 15572
[2025-01-17 05:01:37,612] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50720
[2025-01-17 05:01:37,612] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:01:37,612] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 05:01:37,698] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50720
[2025-01-17 05:01:37,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [36]  [ 180/1404]  eta: 0:12:46  lr: 0.000003  min_lr: 0.000000  loss: 4.1738 (4.1714)  class_acc: 0.3333 (0.3515)  loss_scale: 32768.0000 (32496.4420)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0908  max mem: 15572
Epoch: [36]  [ 190/1404]  eta: 0:12:35  lr: 0.000003  min_lr: 0.000000  loss: 4.2123 (4.1800)  class_acc: 0.3333 (0.3503)  loss_scale: 16384.0000 (31652.8586)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0694  max mem: 15572
Epoch: [36]  [ 200/1404]  eta: 0:12:25  lr: 0.000003  min_lr: 0.000000  loss: 4.2944 (4.1827)  class_acc: 0.3333 (0.3530)  loss_scale: 16384.0000 (30893.2139)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0642  max mem: 15572
Epoch: [36]  [ 210/1404]  eta: 0:12:22  lr: 0.000003  min_lr: 0.000000  loss: 4.2055 (4.1785)  class_acc: 0.3750 (0.3529)  loss_scale: 16384.0000 (30205.5735)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.1205  max mem: 15572
Epoch: [36]  [ 220/1404]  eta: 0:12:09  lr: 0.000003  min_lr: 0.000000  loss: 4.0133 (4.1737)  class_acc: 0.3333 (0.3533)  loss_scale: 16384.0000 (29580.1629)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0814  max mem: 15572
Epoch: [36]  [ 230/1404]  eta: 0:12:00  lr: 0.000003  min_lr: 0.000000  loss: 4.0817 (4.1718)  class_acc: 0.3750 (0.3557)  loss_scale: 16384.0000 (29008.9004)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0388  max mem: 15572
Epoch: [36]  [ 240/1404]  eta: 0:11:52  lr: 0.000003  min_lr: 0.000000  loss: 4.0886 (4.1648)  class_acc: 0.4167 (0.3594)  loss_scale: 16384.0000 (28485.0456)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0822  max mem: 15572
Epoch: [36]  [ 250/1404]  eta: 0:11:42  lr: 0.000003  min_lr: 0.000000  loss: 4.0164 (4.1645)  class_acc: 0.3750 (0.3607)  loss_scale: 16384.0000 (28002.9323)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0682  max mem: 15572
Epoch: [36]  [ 260/1404]  eta: 0:11:38  lr: 0.000003  min_lr: 0.000000  loss: 4.0164 (4.1622)  class_acc: 0.3750 (0.3638)  loss_scale: 16384.0000 (27557.7625)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1090  max mem: 15572
Epoch: [36]  [ 270/1404]  eta: 0:11:29  lr: 0.000003  min_lr: 0.000000  loss: 4.1594 (4.1635)  class_acc: 0.3750 (0.3629)  loss_scale: 16384.0000 (27145.4465)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1138  max mem: 15572
Epoch: [36]  [ 280/1404]  eta: 0:11:20  lr: 0.000003  min_lr: 0.000000  loss: 4.1064 (4.1551)  class_acc: 0.3750 (0.3646)  loss_scale: 16384.0000 (26762.4769)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0567  max mem: 15572
Epoch: [36]  [ 290/1404]  eta: 0:11:15  lr: 0.000003  min_lr: 0.000000  loss: 4.0573 (4.1526)  class_acc: 0.3333 (0.3615)  loss_scale: 16384.0000 (26405.8282)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1001  max mem: 15572
Epoch: [36]  [ 300/1404]  eta: 0:11:07  lr: 0.000003  min_lr: 0.000000  loss: 4.1253 (4.1506)  class_acc: 0.3333 (0.3623)  loss_scale: 16384.0000 (26072.8771)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1080  max mem: 15572
[2025-01-17 05:02:51,177] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:02:51,178] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:02:51,231] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:02:51,232] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [ 310/1404]  eta: 0:10:59  lr: 0.000003  min_lr: 0.000000  loss: 4.1245 (4.1500)  class_acc: 0.3333 (0.3592)  loss_scale: 16384.0000 (26077.4277)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0611  max mem: 15572
Epoch: [36]  [ 320/1404]  eta: 0:10:51  lr: 0.000003  min_lr: 0.000000  loss: 4.1245 (4.1486)  class_acc: 0.2917 (0.3592)  loss_scale: 32768.0000 (26285.8567)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0596  max mem: 15572
Epoch: [36]  [ 330/1404]  eta: 0:10:43  lr: 0.000003  min_lr: 0.000000  loss: 4.1172 (4.1482)  class_acc: 0.2917 (0.3578)  loss_scale: 32768.0000 (26481.6918)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0421  max mem: 15572
Epoch: [36]  [ 340/1404]  eta: 0:10:35  lr: 0.000003  min_lr: 0.000000  loss: 4.1597 (4.1500)  class_acc: 0.2917 (0.3567)  loss_scale: 32768.0000 (26666.0411)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0330  max mem: 15572
Epoch: [36]  [ 350/1404]  eta: 0:10:29  lr: 0.000003  min_lr: 0.000000  loss: 4.1650 (4.1500)  class_acc: 0.3333 (0.3568)  loss_scale: 32768.0000 (26839.8860)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0523  max mem: 15572
Epoch: [36]  [ 360/1404]  eta: 0:10:21  lr: 0.000003  min_lr: 0.000000  loss: 4.1034 (4.1474)  class_acc: 0.3333 (0.3566)  loss_scale: 32768.0000 (27004.0997)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0489  max mem: 15572
Epoch: [36]  [ 370/1404]  eta: 0:10:16  lr: 0.000003  min_lr: 0.000000  loss: 4.1034 (4.1462)  class_acc: 0.2917 (0.3548)  loss_scale: 32768.0000 (27159.4609)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0565  max mem: 15572
Epoch: [36]  [ 380/1404]  eta: 0:10:10  lr: 0.000003  min_lr: 0.000000  loss: 4.2463 (4.1518)  class_acc: 0.2917 (0.3555)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.0440  max mem: 15572
[2025-01-17 05:03:38,687] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50932
[2025-01-17 05:03:38,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:03:38,701] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50932
[2025-01-17 05:03:38,702] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:03:38,702] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [36]  [ 390/1404]  eta: 0:10:05  lr: 0.000003  min_lr: 0.000000  loss: 4.2545 (4.1510)  class_acc: 0.3750 (0.3567)  loss_scale: 32768.0000 (27320.6343)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.0576  max mem: 15572
Epoch: [36]  [ 400/1404]  eta: 0:10:00  lr: 0.000003  min_lr: 0.000000  loss: 4.2260 (4.1488)  class_acc: 0.3750 (0.3559)  loss_scale: 16384.0000 (27047.9002)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.0952  max mem: 15572
Epoch: [36]  [ 410/1404]  eta: 0:09:52  lr: 0.000003  min_lr: 0.000000  loss: 4.2378 (4.1461)  class_acc: 0.2917 (0.3554)  loss_scale: 16384.0000 (26788.4380)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0641  max mem: 15572
Epoch: [36]  [ 420/1404]  eta: 0:09:47  lr: 0.000003  min_lr: 0.000000  loss: 4.2178 (4.1476)  class_acc: 0.3333 (0.3553)  loss_scale: 16384.0000 (26541.3017)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1017  max mem: 15572
Epoch: [36]  [ 430/1404]  eta: 0:09:40  lr: 0.000003  min_lr: 0.000000  loss: 4.1262 (4.1487)  class_acc: 0.4167 (0.3555)  loss_scale: 16384.0000 (26305.6334)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1135  max mem: 15572
Epoch: [36]  [ 440/1404]  eta: 0:09:35  lr: 0.000003  min_lr: 0.000000  loss: 4.0857 (4.1472)  class_acc: 0.3750 (0.3560)  loss_scale: 16384.0000 (26080.6531)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0975  max mem: 15572
Epoch: [36]  [ 450/1404]  eta: 0:09:28  lr: 0.000003  min_lr: 0.000000  loss: 4.0207 (4.1433)  class_acc: 0.4167 (0.3579)  loss_scale: 16384.0000 (25865.6497)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1029  max mem: 15572
[2025-01-17 05:04:18,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=51000, skipped=303, lr=[2.5382327710875498e-08, 2.5382327710875498e-08, 3.626046815839357e-08, 3.626046815839357e-08, 5.1800668797705105e-08, 5.1800668797705105e-08, 7.400095542529302e-08, 7.400095542529302e-08, 1.0571565060756145e-07, 1.0571565060756145e-07, 1.510223580108021e-07, 1.510223580108021e-07, 2.1574622572971727e-07, 2.1574622572971727e-07, 3.0820889389959615e-07, 3.0820889389959615e-07, 4.402984198565659e-07, 4.402984198565659e-07, 6.289977426522371e-07, 6.289977426522371e-07, 8.985682037889101e-07, 8.985682037889101e-07, 1.283668862555586e-06, 1.283668862555586e-06, 1.8338126607936943e-06, 1.8338126607936943e-06, 2.6197323725624208e-06, 2.6197323725624208e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 05:04:18,105] [INFO] [timer.py:260:stop] epoch=0/micro_step=51000/global_step=51000, RunningAvgSamplesPerSec=48.229217704902325, CurrSamplesPerSec=39.69643670486985, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [36]  [ 460/1404]  eta: 0:09:22  lr: 0.000003  min_lr: 0.000000  loss: 4.0328 (4.1441)  class_acc: 0.4167 (0.3571)  loss_scale: 16384.0000 (25659.9740)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0722  max mem: 15572
Epoch: [36]  [ 470/1404]  eta: 0:09:16  lr: 0.000003  min_lr: 0.000000  loss: 4.1754 (4.1423)  class_acc: 0.3750 (0.3574)  loss_scale: 16384.0000 (25463.0318)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0919  max mem: 15572
Epoch: [36]  [ 480/1404]  eta: 0:09:10  lr: 0.000003  min_lr: 0.000000  loss: 3.9781 (4.1395)  class_acc: 0.3750 (0.3573)  loss_scale: 16384.0000 (25274.2786)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1038  max mem: 15572
Epoch: [36]  [ 490/1404]  eta: 0:09:04  lr: 0.000003  min_lr: 0.000000  loss: 3.9466 (4.1370)  class_acc: 0.4167 (0.3587)  loss_scale: 16384.0000 (25093.2138)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1137  max mem: 15572
Epoch: [36]  [ 500/1404]  eta: 0:08:58  lr: 0.000003  min_lr: 0.000000  loss: 4.0165 (4.1394)  class_acc: 0.3750 (0.3580)  loss_scale: 16384.0000 (24919.3772)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1034  max mem: 15572
Epoch: [36]  [ 510/1404]  eta: 0:08:53  lr: 0.000003  min_lr: 0.000000  loss: 4.2574 (4.1399)  class_acc: 0.3333 (0.3584)  loss_scale: 16384.0000 (24752.3444)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1216  max mem: 15572
[2025-01-17 05:04:55,936] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:04:55,936] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:04:55,937] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:04:55,937] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [ 520/1404]  eta: 0:08:47  lr: 0.000003  min_lr: 0.000000  loss: 4.1055 (4.1380)  class_acc: 0.3750 (0.3582)  loss_scale: 16384.0000 (24717.5125)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1431  max mem: 15572
Epoch: [36]  [ 530/1404]  eta: 0:08:41  lr: 0.000003  min_lr: 0.000000  loss: 3.9800 (4.1356)  class_acc: 0.3750 (0.3588)  loss_scale: 32768.0000 (24869.1224)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1016  max mem: 15572
Epoch: [36]  [ 540/1404]  eta: 0:08:36  lr: 0.000003  min_lr: 0.000000  loss: 4.0969 (4.1362)  class_acc: 0.3333 (0.3579)  loss_scale: 32768.0000 (25015.1275)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1113  max mem: 15572
Epoch: [36]  [ 550/1404]  eta: 0:08:29  lr: 0.000003  min_lr: 0.000000  loss: 4.1875 (4.1364)  class_acc: 0.3333 (0.3570)  loss_scale: 32768.0000 (25155.8330)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0946  max mem: 15572
Epoch: [36]  [ 560/1404]  eta: 0:08:22  lr: 0.000003  min_lr: 0.000000  loss: 4.0882 (4.1367)  class_acc: 0.3333 (0.3572)  loss_scale: 32768.0000 (25291.5223)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0251  max mem: 15572
Epoch: [36]  [ 570/1404]  eta: 0:08:15  lr: 0.000003  min_lr: 0.000000  loss: 4.0311 (4.1355)  class_acc: 0.3750 (0.3569)  loss_scale: 32768.0000 (25422.4588)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0295  max mem: 15572
Epoch: [36]  [ 580/1404]  eta: 0:08:10  lr: 0.000002  min_lr: 0.000000  loss: 4.0356 (4.1353)  class_acc: 0.3333 (0.3562)  loss_scale: 32768.0000 (25548.8881)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0938  max mem: 15572
Epoch: [36]  [ 590/1404]  eta: 0:08:03  lr: 0.000002  min_lr: 0.000000  loss: 3.9947 (4.1316)  class_acc: 0.3333 (0.3565)  loss_scale: 32768.0000 (25671.0389)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0796  max mem: 15572
Epoch: [36]  [ 600/1404]  eta: 0:07:58  lr: 0.000002  min_lr: 0.000000  loss: 4.0719 (4.1322)  class_acc: 0.3750 (0.3563)  loss_scale: 32768.0000 (25789.1248)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0721  max mem: 15572
[2025-01-17 05:05:48,065] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51150
[2025-01-17 05:05:48,066] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:05:48,066] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 05:05:48,066] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51150
[2025-01-17 05:05:48,066] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [36]  [ 610/1404]  eta: 0:07:52  lr: 0.000002  min_lr: 0.000000  loss: 4.1638 (4.1330)  class_acc: 0.3333 (0.3564)  loss_scale: 32768.0000 (25769.2700)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1088  max mem: 15572
Epoch: [36]  [ 620/1404]  eta: 0:07:46  lr: 0.000002  min_lr: 0.000000  loss: 4.1662 (4.1338)  class_acc: 0.3333 (0.3559)  loss_scale: 16384.0000 (25618.1385)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0971  max mem: 15572
Epoch: [36]  [ 630/1404]  eta: 0:07:39  lr: 0.000002  min_lr: 0.000000  loss: 4.0965 (4.1316)  class_acc: 0.3333 (0.3560)  loss_scale: 16384.0000 (25471.7971)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0598  max mem: 15572
Epoch: [36]  [ 640/1404]  eta: 0:07:33  lr: 0.000002  min_lr: 0.000000  loss: 4.0594 (4.1310)  class_acc: 0.3333 (0.3559)  loss_scale: 16384.0000 (25330.0218)  weight_decay: 0.0500 (0.0500)  time: 0.5210  data: 0.0325  max mem: 15572
Epoch: [36]  [ 650/1404]  eta: 0:07:26  lr: 0.000002  min_lr: 0.000000  loss: 4.1440 (4.1314)  class_acc: 0.2917 (0.3552)  loss_scale: 16384.0000 (25192.6022)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0583  max mem: 15572
Epoch: [36]  [ 660/1404]  eta: 0:07:21  lr: 0.000002  min_lr: 0.000000  loss: 4.1267 (4.1314)  class_acc: 0.2917 (0.3555)  loss_scale: 16384.0000 (25059.3404)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0999  max mem: 15572
Epoch: [36]  [ 670/1404]  eta: 0:07:14  lr: 0.000002  min_lr: 0.000000  loss: 4.0566 (4.1293)  class_acc: 0.3750 (0.3558)  loss_scale: 16384.0000 (24930.0507)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0872  max mem: 15572
Epoch: [36]  [ 680/1404]  eta: 0:07:08  lr: 0.000002  min_lr: 0.000000  loss: 4.1167 (4.1301)  class_acc: 0.3750 (0.3567)  loss_scale: 16384.0000 (24804.5580)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0572  max mem: 15572
Epoch: [36]  [ 690/1404]  eta: 0:07:03  lr: 0.000002  min_lr: 0.000000  loss: 4.0597 (4.1259)  class_acc: 0.3750 (0.3571)  loss_scale: 16384.0000 (24682.6975)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0903  max mem: 15572
Epoch: [36]  [ 700/1404]  eta: 0:06:56  lr: 0.000002  min_lr: 0.000000  loss: 3.8823 (4.1237)  class_acc: 0.3333 (0.3575)  loss_scale: 16384.0000 (24564.3138)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0869  max mem: 15572
Epoch: [36]  [ 710/1404]  eta: 0:06:50  lr: 0.000002  min_lr: 0.000000  loss: 4.1580 (4.1216)  class_acc: 0.3750 (0.3577)  loss_scale: 16384.0000 (24449.2602)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0618  max mem: 15572
Epoch: [36]  [ 720/1404]  eta: 0:06:44  lr: 0.000002  min_lr: 0.000000  loss: 4.1105 (4.1211)  class_acc: 0.3750 (0.3581)  loss_scale: 16384.0000 (24337.3981)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1082  max mem: 15572
Epoch: [36]  [ 730/1404]  eta: 0:06:39  lr: 0.000002  min_lr: 0.000000  loss: 4.1105 (4.1206)  class_acc: 0.3333 (0.3582)  loss_scale: 16384.0000 (24228.5964)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1402  max mem: 15572
[2025-01-17 05:07:02,065] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:07:02,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:07:02,077] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:07:02,077] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [ 740/1404]  eta: 0:06:33  lr: 0.000002  min_lr: 0.000000  loss: 4.2032 (4.1205)  class_acc: 0.3750 (0.3593)  loss_scale: 16384.0000 (24255.3954)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1245  max mem: 15572
Epoch: [36]  [ 750/1404]  eta: 0:06:27  lr: 0.000002  min_lr: 0.000000  loss: 4.2995 (4.1243)  class_acc: 0.4167 (0.3581)  loss_scale: 32768.0000 (24368.7457)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0862  max mem: 15572
Epoch: [36]  [ 760/1404]  eta: 0:06:21  lr: 0.000002  min_lr: 0.000000  loss: 4.3799 (4.1253)  class_acc: 0.2083 (0.3579)  loss_scale: 32768.0000 (24479.1170)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0908  max mem: 15572
Epoch: [36]  [ 770/1404]  eta: 0:06:16  lr: 0.000002  min_lr: 0.000000  loss: 4.0806 (4.1239)  class_acc: 0.3750 (0.3578)  loss_scale: 32768.0000 (24586.6252)  weight_decay: 0.0500 (0.0500)  time: 0.6611  data: 0.1632  max mem: 15572
Epoch: [36]  [ 780/1404]  eta: 0:06:10  lr: 0.000002  min_lr: 0.000000  loss: 4.0767 (4.1228)  class_acc: 0.3750 (0.3579)  loss_scale: 32768.0000 (24691.3803)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1379  max mem: 15572
Epoch: [36]  [ 790/1404]  eta: 0:06:03  lr: 0.000002  min_lr: 0.000000  loss: 4.1434 (4.1215)  class_acc: 0.3333 (0.3574)  loss_scale: 32768.0000 (24793.4867)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0435  max mem: 15572
Epoch: [36]  [ 800/1404]  eta: 0:05:57  lr: 0.000002  min_lr: 0.000000  loss: 4.1460 (4.1225)  class_acc: 0.2917 (0.3568)  loss_scale: 32768.0000 (24893.0437)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0127  max mem: 15572
Epoch: [36]  [ 810/1404]  eta: 0:05:50  lr: 0.000002  min_lr: 0.000000  loss: 4.0750 (4.1223)  class_acc: 0.2917 (0.3567)  loss_scale: 32768.0000 (24990.1455)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0109  max mem: 15572
Epoch: [36]  [ 820/1404]  eta: 0:05:45  lr: 0.000002  min_lr: 0.000000  loss: 4.0750 (4.1224)  class_acc: 0.2917 (0.3559)  loss_scale: 32768.0000 (25084.8819)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0490  max mem: 15572
[2025-01-17 05:07:52,625] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51366
[2025-01-17 05:07:52,625] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:07:52,643] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51366
[2025-01-17 05:07:52,644] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:07:52,644] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [36]  [ 830/1404]  eta: 0:05:39  lr: 0.000002  min_lr: 0.000000  loss: 4.1062 (4.1218)  class_acc: 0.3333 (0.3563)  loss_scale: 32768.0000 (24999.8941)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0939  max mem: 15572
Epoch: [36]  [ 840/1404]  eta: 0:05:32  lr: 0.000002  min_lr: 0.000000  loss: 4.1062 (4.1202)  class_acc: 0.3333 (0.3562)  loss_scale: 16384.0000 (24897.4459)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0505  max mem: 15572
Epoch: [36]  [ 850/1404]  eta: 0:05:27  lr: 0.000002  min_lr: 0.000000  loss: 4.1283 (4.1212)  class_acc: 0.2917 (0.3557)  loss_scale: 16384.0000 (24797.4054)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0563  max mem: 15572
Epoch: [36]  [ 860/1404]  eta: 0:05:21  lr: 0.000002  min_lr: 0.000000  loss: 4.1963 (4.1214)  class_acc: 0.3333 (0.3557)  loss_scale: 16384.0000 (24699.6887)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1295  max mem: 15572
Epoch: [36]  [ 870/1404]  eta: 0:05:15  lr: 0.000002  min_lr: 0.000000  loss: 4.1313 (4.1213)  class_acc: 0.3333 (0.3555)  loss_scale: 16384.0000 (24604.2158)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0866  max mem: 15572
Epoch: [36]  [ 880/1404]  eta: 0:05:09  lr: 0.000002  min_lr: 0.000000  loss: 4.0646 (4.1213)  class_acc: 0.3333 (0.3549)  loss_scale: 16384.0000 (24510.9103)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0587  max mem: 15572
Epoch: [36]  [ 890/1404]  eta: 0:05:03  lr: 0.000002  min_lr: 0.000000  loss: 4.0881 (4.1209)  class_acc: 0.3333 (0.3555)  loss_scale: 16384.0000 (24419.6992)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0750  max mem: 15572
Epoch: [36]  [ 900/1404]  eta: 0:04:57  lr: 0.000002  min_lr: 0.000000  loss: 4.0950 (4.1226)  class_acc: 0.3333 (0.3553)  loss_scale: 16384.0000 (24330.5128)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0925  max mem: 15572
Epoch: [36]  [ 910/1404]  eta: 0:04:51  lr: 0.000002  min_lr: 0.000000  loss: 4.1699 (4.1217)  class_acc: 0.3333 (0.3558)  loss_scale: 16384.0000 (24243.2843)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1229  max mem: 15572
Epoch: [36]  [ 920/1404]  eta: 0:04:45  lr: 0.000002  min_lr: 0.000000  loss: 4.0442 (4.1193)  class_acc: 0.4167 (0.3557)  loss_scale: 16384.0000 (24157.9501)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0986  max mem: 15572
Epoch: [36]  [ 930/1404]  eta: 0:04:40  lr: 0.000002  min_lr: 0.000000  loss: 4.1279 (4.1199)  class_acc: 0.2917 (0.3557)  loss_scale: 16384.0000 (24074.4490)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1355  max mem: 15572
Epoch: [36]  [ 940/1404]  eta: 0:04:34  lr: 0.000002  min_lr: 0.000000  loss: 4.0930 (4.1191)  class_acc: 0.3333 (0.3566)  loss_scale: 16384.0000 (23992.7226)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1566  max mem: 15572
Epoch: [36]  [ 950/1404]  eta: 0:04:28  lr: 0.000002  min_lr: 0.000000  loss: 4.0805 (4.1203)  class_acc: 0.3333 (0.3559)  loss_scale: 16384.0000 (23912.7150)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0926  max mem: 15572
[2025-01-17 05:09:08,830] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:09:08,830] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:09:08,844] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:09:08,845] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [ 960/1404]  eta: 0:04:22  lr: 0.000002  min_lr: 0.000000  loss: 4.0998 (4.1199)  class_acc: 0.2917 (0.3560)  loss_scale: 16384.0000 (24004.8616)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1163  max mem: 15572
Epoch: [36]  [ 970/1404]  eta: 0:04:16  lr: 0.000002  min_lr: 0.000000  loss: 4.0502 (4.1190)  class_acc: 0.3333 (0.3562)  loss_scale: 32768.0000 (24095.1102)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1161  max mem: 15572
Epoch: [36]  [ 980/1404]  eta: 0:04:10  lr: 0.000002  min_lr: 0.000000  loss: 4.0549 (4.1180)  class_acc: 0.3750 (0.3564)  loss_scale: 32768.0000 (24183.5189)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1179  max mem: 15572
Epoch: [36]  [ 990/1404]  eta: 0:04:04  lr: 0.000002  min_lr: 0.000000  loss: 4.1004 (4.1187)  class_acc: 0.3750 (0.3563)  loss_scale: 32768.0000 (24270.1433)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0889  max mem: 15572
Epoch: [36]  [1000/1404]  eta: 0:03:58  lr: 0.000002  min_lr: 0.000000  loss: 4.1617 (4.1194)  class_acc: 0.2917 (0.3564)  loss_scale: 32768.0000 (24355.0370)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0804  max mem: 15572
[2025-01-17 05:09:39,497] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51546
[2025-01-17 05:09:39,497] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:09:39,497] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 05:09:39,559] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51546
[2025-01-17 05:09:39,560] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [36]  [1010/1404]  eta: 0:03:52  lr: 0.000002  min_lr: 0.000000  loss: 4.1431 (4.1183)  class_acc: 0.2917 (0.3564)  loss_scale: 32768.0000 (24292.3996)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0881  max mem: 15572
Epoch: [36]  [1020/1404]  eta: 0:03:46  lr: 0.000002  min_lr: 0.000000  loss: 4.0280 (4.1163)  class_acc: 0.3333 (0.3566)  loss_scale: 16384.0000 (24214.9422)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0762  max mem: 15572
Epoch: [36]  [1030/1404]  eta: 0:03:40  lr: 0.000002  min_lr: 0.000000  loss: 3.9698 (4.1150)  class_acc: 0.3750 (0.3570)  loss_scale: 16384.0000 (24138.9874)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0755  max mem: 15572
Epoch: [36]  [1040/1404]  eta: 0:03:34  lr: 0.000002  min_lr: 0.000000  loss: 4.0964 (4.1155)  class_acc: 0.3750 (0.3571)  loss_scale: 16384.0000 (24064.4918)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0112  max mem: 15572
Epoch: [36]  [1050/1404]  eta: 0:03:29  lr: 0.000002  min_lr: 0.000000  loss: 4.0964 (4.1141)  class_acc: 0.3333 (0.3568)  loss_scale: 16384.0000 (23991.4139)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.0679  max mem: 15572
Epoch: [36]  [1060/1404]  eta: 0:03:22  lr: 0.000002  min_lr: 0.000000  loss: 4.1369 (4.1151)  class_acc: 0.3750 (0.3573)  loss_scale: 16384.0000 (23919.7135)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0807  max mem: 15572
Epoch: [36]  [1070/1404]  eta: 0:03:17  lr: 0.000002  min_lr: 0.000000  loss: 4.1460 (4.1148)  class_acc: 0.4167 (0.3577)  loss_scale: 16384.0000 (23849.3520)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1237  max mem: 15572
Epoch: [36]  [1080/1404]  eta: 0:03:11  lr: 0.000002  min_lr: 0.000000  loss: 4.0233 (4.1135)  class_acc: 0.3750 (0.3577)  loss_scale: 16384.0000 (23780.2923)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1315  max mem: 15572
Epoch: [36]  [1090/1404]  eta: 0:03:05  lr: 0.000002  min_lr: 0.000000  loss: 4.1877 (4.1152)  class_acc: 0.3333 (0.3578)  loss_scale: 16384.0000 (23712.4986)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0568  max mem: 15572
Epoch: [36]  [1100/1404]  eta: 0:02:59  lr: 0.000002  min_lr: 0.000000  loss: 4.2793 (4.1165)  class_acc: 0.2917 (0.3571)  loss_scale: 16384.0000 (23645.9364)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0631  max mem: 15572
Epoch: [36]  [1110/1404]  eta: 0:02:53  lr: 0.000002  min_lr: 0.000000  loss: 4.1654 (4.1153)  class_acc: 0.2917 (0.3575)  loss_scale: 16384.0000 (23580.5725)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0276  max mem: 15572
Epoch: [36]  [1120/1404]  eta: 0:02:47  lr: 0.000002  min_lr: 0.000000  loss: 4.0410 (4.1155)  class_acc: 0.3333 (0.3572)  loss_scale: 16384.0000 (23516.3747)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0494  max mem: 15572
Epoch: [36]  [1130/1404]  eta: 0:02:41  lr: 0.000002  min_lr: 0.000000  loss: 4.1522 (4.1159)  class_acc: 0.3333 (0.3569)  loss_scale: 16384.0000 (23453.3121)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1228  max mem: 15572
[2025-01-17 05:10:53,849] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:10:53,849] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:10:53,849] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:10:53,849] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [1140/1404]  eta: 0:02:35  lr: 0.000002  min_lr: 0.000000  loss: 4.1830 (4.1152)  class_acc: 0.3750 (0.3572)  loss_scale: 16384.0000 (23534.9483)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1387  max mem: 15572
[2025-01-17 05:11:02,243] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51689
[2025-01-17 05:11:02,243] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:11:02,320] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51689
[2025-01-17 05:11:02,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:11:02,320] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [36]  [1150/1404]  eta: 0:02:29  lr: 0.000002  min_lr: 0.000000  loss: 4.0988 (4.1152)  class_acc: 0.3750 (0.3571)  loss_scale: 32768.0000 (23529.7585)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0794  max mem: 15572
Epoch: [36]  [1160/1404]  eta: 0:02:23  lr: 0.000002  min_lr: 0.000000  loss: 4.0988 (4.1153)  class_acc: 0.3750 (0.3578)  loss_scale: 16384.0000 (23468.2102)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0909  max mem: 15572
Epoch: [36]  [1170/1404]  eta: 0:02:17  lr: 0.000002  min_lr: 0.000000  loss: 4.0895 (4.1150)  class_acc: 0.3750 (0.3578)  loss_scale: 16384.0000 (23407.7131)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1060  max mem: 15572
Epoch: [36]  [1180/1404]  eta: 0:02:12  lr: 0.000002  min_lr: 0.000000  loss: 4.2151 (4.1158)  class_acc: 0.3750 (0.3583)  loss_scale: 16384.0000 (23348.2405)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1036  max mem: 15572
Epoch: [36]  [1190/1404]  eta: 0:02:06  lr: 0.000002  min_lr: 0.000000  loss: 4.2215 (4.1160)  class_acc: 0.3333 (0.3581)  loss_scale: 16384.0000 (23289.7666)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1158  max mem: 15572
Epoch: [36]  [1200/1404]  eta: 0:02:00  lr: 0.000002  min_lr: 0.000000  loss: 4.0634 (4.1155)  class_acc: 0.2500 (0.3573)  loss_scale: 16384.0000 (23232.2664)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0733  max mem: 15572
Epoch: [36]  [1210/1404]  eta: 0:01:54  lr: 0.000002  min_lr: 0.000000  loss: 4.0654 (4.1158)  class_acc: 0.2917 (0.3574)  loss_scale: 16384.0000 (23175.7159)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0870  max mem: 15572
Epoch: [36]  [1220/1404]  eta: 0:01:48  lr: 0.000002  min_lr: 0.000000  loss: 3.9995 (4.1139)  class_acc: 0.3750 (0.3576)  loss_scale: 16384.0000 (23120.0917)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0891  max mem: 15572
Epoch: [36]  [1230/1404]  eta: 0:01:42  lr: 0.000002  min_lr: 0.000000  loss: 3.9940 (4.1142)  class_acc: 0.3750 (0.3574)  loss_scale: 16384.0000 (23065.3712)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1104  max mem: 15572
Epoch: [36]  [1240/1404]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 3.9912 (4.1119)  class_acc: 0.2917 (0.3574)  loss_scale: 16384.0000 (23011.5326)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0954  max mem: 15572
Epoch: [36]  [1250/1404]  eta: 0:01:30  lr: 0.000002  min_lr: 0.000000  loss: 3.9992 (4.1119)  class_acc: 0.3333 (0.3574)  loss_scale: 16384.0000 (22958.5548)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0368  max mem: 15572
Epoch: [36]  [1260/1404]  eta: 0:01:24  lr: 0.000002  min_lr: 0.000000  loss: 4.1536 (4.1123)  class_acc: 0.3333 (0.3576)  loss_scale: 16384.0000 (22906.4171)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0592  max mem: 15572
Epoch: [36]  [1270/1404]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 4.2347 (4.1125)  class_acc: 0.3750 (0.3578)  loss_scale: 16384.0000 (22855.0999)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0994  max mem: 15572
[2025-01-17 05:12:17,864] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:12:17,865] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:12:17,867] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:12:17,867] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [1280/1404]  eta: 0:01:13  lr: 0.000002  min_lr: 0.000000  loss: 4.2322 (4.1139)  class_acc: 0.3333 (0.3579)  loss_scale: 16384.0000 (22894.1140)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1046  max mem: 15572
[2025-01-17 05:12:24,064] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51829
[2025-01-17 05:12:24,064] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51829
[2025-01-17 05:12:24,064] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:12:24,064] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:12:24,065] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [36]  [1290/1404]  eta: 0:01:07  lr: 0.000002  min_lr: 0.000000  loss: 4.1730 (4.1138)  class_acc: 0.3333 (0.3582)  loss_scale: 32768.0000 (22894.4508)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0755  max mem: 15572
Epoch: [36]  [1300/1404]  eta: 0:01:01  lr: 0.000002  min_lr: 0.000000  loss: 4.2166 (4.1154)  class_acc: 0.3333 (0.3575)  loss_scale: 16384.0000 (22844.4089)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1006  max mem: 15572
Epoch: [36]  [1310/1404]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 4.2166 (4.1154)  class_acc: 0.2500 (0.3572)  loss_scale: 16384.0000 (22795.1304)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0717  max mem: 15572
Epoch: [36]  [1320/1404]  eta: 0:00:49  lr: 0.000002  min_lr: 0.000000  loss: 4.1095 (4.1155)  class_acc: 0.2917 (0.3572)  loss_scale: 16384.0000 (22746.5980)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0897  max mem: 15572
Epoch: [36]  [1330/1404]  eta: 0:00:43  lr: 0.000002  min_lr: 0.000000  loss: 4.0757 (4.1156)  class_acc: 0.3333 (0.3574)  loss_scale: 16384.0000 (22698.7949)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0898  max mem: 15572
Epoch: [36]  [1340/1404]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 4.0751 (4.1155)  class_acc: 0.3333 (0.3574)  loss_scale: 16384.0000 (22651.7047)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0006  max mem: 15572
Epoch: [36]  [1350/1404]  eta: 0:00:31  lr: 0.000002  min_lr: 0.000000  loss: 4.2102 (4.1170)  class_acc: 0.3333 (0.3573)  loss_scale: 16384.0000 (22605.3116)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1208  max mem: 15572
Epoch: [36]  [1360/1404]  eta: 0:00:25  lr: 0.000002  min_lr: 0.000000  loss: 4.3304 (4.1170)  class_acc: 0.2917 (0.3572)  loss_scale: 16384.0000 (22559.6003)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1678  max mem: 15572
Epoch: [36]  [1370/1404]  eta: 0:00:20  lr: 0.000002  min_lr: 0.000000  loss: 4.3369 (4.1178)  class_acc: 0.2917 (0.3567)  loss_scale: 16384.0000 (22514.5558)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0581  max mem: 15572
Epoch: [36]  [1380/1404]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 4.1598 (4.1170)  class_acc: 0.2917 (0.3566)  loss_scale: 16384.0000 (22470.1636)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0287  max mem: 15572
Epoch: [36]  [1390/1404]  eta: 0:00:08  lr: 0.000002  min_lr: 0.000000  loss: 4.0960 (4.1175)  class_acc: 0.2917 (0.3565)  loss_scale: 16384.0000 (22426.4098)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0540  max mem: 15572
Epoch: [36]  [1400/1404]  eta: 0:00:02  lr: 0.000002  min_lr: 0.000000  loss: 4.2140 (4.1180)  class_acc: 0.3333 (0.3567)  loss_scale: 16384.0000 (22383.2805)  weight_decay: 0.0500 (0.0500)  time: 0.4691  data: 0.0362  max mem: 15572
Epoch: [36]  [1403/1404]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 4.0594 (4.1176)  class_acc: 0.3333 (0.3566)  loss_scale: 16384.0000 (22370.4615)  weight_decay: 0.0500 (0.0500)  time: 0.4453  data: 0.0362  max mem: 15572
Epoch: [36] Total time: 0:13:43 (0.5863 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 4.0594 (4.1168)  class_acc: 0.3333 (0.3559)  loss_scale: 16384.0000 (22370.4615)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:13:15  loss: 1.6769 (1.6769)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.8469  data: 5.6074  max mem: 15572
Val:  [ 10/136]  eta: 0:01:35  loss: 2.2406 (2.1808)  acc1: 61.1111 (52.0202)  acc5: 83.3333 (83.3333)  time: 0.7569  data: 0.5549  max mem: 15572
Val:  [ 20/136]  eta: 0:01:00  loss: 2.3865 (2.3404)  acc1: 38.8889 (47.6190)  acc5: 77.7778 (79.8942)  time: 0.2575  data: 0.0704  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.3203 (2.2729)  acc1: 44.4444 (48.2079)  acc5: 83.3333 (79.9283)  time: 0.3319  data: 0.1373  max mem: 15572
Val:  [ 40/136]  eta: 0:00:41  loss: 2.0440 (2.2429)  acc1: 55.5556 (50.0000)  acc5: 83.3333 (79.9458)  time: 0.3399  data: 0.1275  max mem: 15572
Val:  [ 50/136]  eta: 0:00:35  loss: 2.0710 (2.2498)  acc1: 55.5556 (49.3464)  acc5: 77.7778 (80.2832)  time: 0.3177  data: 0.1097  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.2724 (2.3172)  acc1: 44.4444 (47.2678)  acc5: 77.7778 (79.2350)  time: 0.3617  data: 0.1558  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.2724 (2.3047)  acc1: 44.4444 (48.1221)  acc5: 77.7778 (79.4992)  time: 0.3539  data: 0.1499  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.1542 (2.2965)  acc1: 50.0000 (48.1481)  acc5: 83.3333 (79.9040)  time: 0.3583  data: 0.1646  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.2614 (2.3057)  acc1: 50.0000 (47.8022)  acc5: 77.7778 (79.4261)  time: 0.3779  data: 0.1858  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5204 (2.3593)  acc1: 38.8889 (45.8746)  acc5: 72.2222 (77.9978)  time: 0.3598  data: 0.1601  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.4536 (2.3501)  acc1: 38.8889 (46.5966)  acc5: 77.7778 (78.0280)  time: 0.3782  data: 0.1790  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1659 (2.3128)  acc1: 55.5556 (47.8880)  acc5: 83.3333 (78.7879)  time: 0.3944  data: 0.1941  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8593 (2.2827)  acc1: 61.1111 (48.6429)  acc5: 88.8889 (79.3893)  time: 0.2786  data: 0.1043  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.8733 (2.2796)  acc1: 50.0000 (48.8534)  acc5: 88.8889 (79.5659)  time: 0.2238  data: 0.0755  max mem: 15572
Val: Total time: 0:00:49 (0.3669 s / it)
* Acc@1 47.789 Acc@5 78.747 loss 2.317
Accuracy of the network on the 4883 val videos: 47.8%
Max accuracy: 47.99%
Epoch: [37]  [   0/1404]  eta: 2:59:27  lr: 0.000002  min_lr: 0.000000  loss: 4.2473 (4.2473)  class_acc: 0.2083 (0.2083)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 7.6691  data: 5.9890  max mem: 15572
[2025-01-17 05:14:32,769] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:14:32,770] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:14:32,769] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:14:32,770] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [  10/1404]  eta: 0:28:16  lr: 0.000002  min_lr: 0.000000  loss: 4.0698 (4.0861)  class_acc: 0.2500 (0.2992)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  time: 1.2173  data: 0.5452  max mem: 15572
Epoch: [37]  [  20/1404]  eta: 0:19:53  lr: 0.000002  min_lr: 0.000000  loss: 4.0698 (4.0926)  class_acc: 0.2917 (0.3234)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0008  max mem: 15572
Epoch: [37]  [  30/1404]  eta: 0:18:03  lr: 0.000002  min_lr: 0.000000  loss: 4.1944 (4.1636)  class_acc: 0.3333 (0.3320)  loss_scale: 32768.0000 (27482.8387)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0117  max mem: 15572
Epoch: [37]  [  40/1404]  eta: 0:16:24  lr: 0.000002  min_lr: 0.000000  loss: 4.0858 (4.1174)  class_acc: 0.3750 (0.3435)  loss_scale: 32768.0000 (28771.9024)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.0116  max mem: 15572
Epoch: [37]  [  50/1404]  eta: 0:15:16  lr: 0.000002  min_lr: 0.000000  loss: 4.0711 (4.1259)  class_acc: 0.3750 (0.3391)  loss_scale: 32768.0000 (29555.4510)  weight_decay: 0.0500 (0.0500)  time: 0.5028  data: 0.0006  max mem: 15572
[2025-01-17 05:14:54,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=52000, skipped=308, lr=[1.686762371584848e-08, 1.686762371584848e-08, 2.4096605308354973e-08, 2.4096605308354973e-08, 3.442372186907854e-08, 3.442372186907854e-08, 4.917674552725506e-08, 4.917674552725506e-08, 7.025249361036437e-08, 7.025249361036437e-08, 1.0036070515766339e-07, 1.0036070515766339e-07, 1.4337243593951914e-07, 1.4337243593951914e-07, 2.048177656278845e-07, 2.048177656278845e-07, 2.92596808039835e-07, 2.92596808039835e-07, 4.179954400569072e-07, 4.179954400569072e-07, 5.971363429384388e-07, 5.971363429384388e-07, 8.530519184834842e-07, 8.530519184834842e-07, 1.2186455978335488e-06, 1.2186455978335488e-06, 1.7409222826193556e-06, 1.7409222826193556e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 05:14:54,454] [INFO] [timer.py:260:stop] epoch=0/micro_step=52000/global_step=52000, RunningAvgSamplesPerSec=48.28345786865296, CurrSamplesPerSec=46.276996405903915, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [37]  [  60/1404]  eta: 0:14:49  lr: 0.000002  min_lr: 0.000000  loss: 4.0543 (4.1029)  class_acc: 0.3333 (0.3402)  loss_scale: 32768.0000 (30082.0984)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0325  max mem: 15572
Epoch: [37]  [  70/1404]  eta: 0:15:03  lr: 0.000002  min_lr: 0.000000  loss: 4.0368 (4.1048)  class_acc: 0.3333 (0.3386)  loss_scale: 32768.0000 (30460.3944)  weight_decay: 0.0500 (0.0500)  time: 0.6786  data: 0.0417  max mem: 15572
Epoch: [37]  [  80/1404]  eta: 0:14:39  lr: 0.000002  min_lr: 0.000000  loss: 4.0368 (4.0923)  class_acc: 0.3750 (0.3426)  loss_scale: 32768.0000 (30745.2840)  weight_decay: 0.0500 (0.0500)  time: 0.6739  data: 0.0099  max mem: 15572
Epoch: [37]  [  90/1404]  eta: 0:14:36  lr: 0.000002  min_lr: 0.000000  loss: 3.9790 (4.0781)  class_acc: 0.4167 (0.3475)  loss_scale: 32768.0000 (30967.5604)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.0007  max mem: 15572
Epoch: [37]  [ 100/1404]  eta: 0:14:28  lr: 0.000002  min_lr: 0.000000  loss: 4.0962 (4.0719)  class_acc: 0.3333 (0.3465)  loss_scale: 32768.0000 (31145.8218)  weight_decay: 0.0500 (0.0500)  time: 0.6702  data: 0.0010  max mem: 15572
Epoch: [37]  [ 110/1404]  eta: 0:14:11  lr: 0.000002  min_lr: 0.000000  loss: 4.0245 (4.0697)  class_acc: 0.3333 (0.3465)  loss_scale: 32768.0000 (31291.9640)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0008  max mem: 15572
Epoch: [37]  [ 120/1404]  eta: 0:13:48  lr: 0.000002  min_lr: 0.000000  loss: 3.9764 (4.0635)  class_acc: 0.3750 (0.3550)  loss_scale: 32768.0000 (31413.9504)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0008  max mem: 15572
Epoch: [37]  [ 130/1404]  eta: 0:13:26  lr: 0.000002  min_lr: 0.000000  loss: 4.1665 (4.0853)  class_acc: 0.3333 (0.3511)  loss_scale: 32768.0000 (31517.3130)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0008  max mem: 15572
[2025-01-17 05:15:47,652] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:15:47,652] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:15:47,721] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:15:47,722] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:15:48,803] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52088
[2025-01-17 05:15:48,804] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:15:48,825] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52088
[2025-01-17 05:15:48,826] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:15:48,827] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 140/1404]  eta: 0:13:21  lr: 0.000002  min_lr: 0.000000  loss: 4.3127 (4.0806)  class_acc: 0.3333 (0.3549)  loss_scale: 32768.0000 (32070.8085)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0148  max mem: 15572
Epoch: [37]  [ 150/1404]  eta: 0:13:14  lr: 0.000002  min_lr: 0.000000  loss: 4.2921 (4.0901)  class_acc: 0.3750 (0.3513)  loss_scale: 32768.0000 (32116.9801)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.0349  max mem: 15572
Epoch: [37]  [ 160/1404]  eta: 0:13:12  lr: 0.000002  min_lr: 0.000000  loss: 4.1799 (4.0901)  class_acc: 0.3750 (0.3571)  loss_scale: 32768.0000 (32157.4161)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.0207  max mem: 15572
Epoch: [37]  [ 170/1404]  eta: 0:12:54  lr: 0.000002  min_lr: 0.000000  loss: 4.0986 (4.0951)  class_acc: 0.4167 (0.3550)  loss_scale: 32768.0000 (32193.1228)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0006  max mem: 15572
Epoch: [37]  [ 180/1404]  eta: 0:12:42  lr: 0.000002  min_lr: 0.000000  loss: 4.1520 (4.1046)  class_acc: 0.3333 (0.3538)  loss_scale: 32768.0000 (32224.8840)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0007  max mem: 15572
Epoch: [37]  [ 190/1404]  eta: 0:12:31  lr: 0.000002  min_lr: 0.000000  loss: 4.2336 (4.1081)  class_acc: 0.2917 (0.3506)  loss_scale: 32768.0000 (32253.3194)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0008  max mem: 15572
[2025-01-17 05:16:20,808] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52144
[2025-01-17 05:16:20,808] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:16:20,808] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52144
[2025-01-17 05:16:20,808] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:16:20,808] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [37]  [ 200/1404]  eta: 0:12:27  lr: 0.000002  min_lr: 0.000000  loss: 4.2285 (4.1178)  class_acc: 0.2917 (0.3478)  loss_scale: 32768.0000 (31871.3632)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0111  max mem: 15572
Epoch: [37]  [ 210/1404]  eta: 0:12:13  lr: 0.000002  min_lr: 0.000000  loss: 4.2285 (4.1249)  class_acc: 0.3333 (0.3474)  loss_scale: 16384.0000 (31137.3649)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0110  max mem: 15572
Epoch: [37]  [ 220/1404]  eta: 0:12:10  lr: 0.000002  min_lr: 0.000000  loss: 4.1766 (4.1237)  class_acc: 0.3750 (0.3471)  loss_scale: 16384.0000 (30469.7919)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0006  max mem: 15572
Epoch: [37]  [ 230/1404]  eta: 0:12:02  lr: 0.000002  min_lr: 0.000000  loss: 4.0941 (4.1260)  class_acc: 0.3333 (0.3452)  loss_scale: 16384.0000 (29860.0173)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.0008  max mem: 15572
Epoch: [37]  [ 240/1404]  eta: 0:11:54  lr: 0.000002  min_lr: 0.000000  loss: 4.0941 (4.1254)  class_acc: 0.2917 (0.3446)  loss_scale: 16384.0000 (29300.8465)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0134  max mem: 15572
Epoch: [37]  [ 250/1404]  eta: 0:11:50  lr: 0.000002  min_lr: 0.000000  loss: 4.0917 (4.1252)  class_acc: 0.3333 (0.3448)  loss_scale: 16384.0000 (28786.2311)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.0134  max mem: 15572
Epoch: [37]  [ 260/1404]  eta: 0:11:45  lr: 0.000002  min_lr: 0.000000  loss: 4.0379 (4.1195)  class_acc: 0.3750 (0.3458)  loss_scale: 16384.0000 (28311.0498)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.0008  max mem: 15572
Epoch: [37]  [ 270/1404]  eta: 0:11:34  lr: 0.000002  min_lr: 0.000000  loss: 4.1146 (4.1213)  class_acc: 0.3333 (0.3452)  loss_scale: 16384.0000 (27870.9373)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0007  max mem: 15572
Epoch: [37]  [ 280/1404]  eta: 0:11:25  lr: 0.000002  min_lr: 0.000000  loss: 4.1853 (4.1265)  class_acc: 0.3333 (0.3437)  loss_scale: 16384.0000 (27462.1495)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0005  max mem: 15572
Epoch: [37]  [ 290/1404]  eta: 0:11:20  lr: 0.000002  min_lr: 0.000000  loss: 4.2412 (4.1279)  class_acc: 0.3333 (0.3439)  loss_scale: 16384.0000 (27081.4570)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0005  max mem: 15572
Epoch: [37]  [ 300/1404]  eta: 0:11:09  lr: 0.000002  min_lr: 0.000000  loss: 4.1965 (4.1299)  class_acc: 0.3333 (0.3450)  loss_scale: 16384.0000 (26726.0598)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0005  max mem: 15572
Epoch: [37]  [ 310/1404]  eta: 0:11:01  lr: 0.000002  min_lr: 0.000000  loss: 4.1965 (4.1312)  class_acc: 0.4167 (0.3470)  loss_scale: 16384.0000 (26393.5177)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0005  max mem: 15572
Epoch: [37]  [ 320/1404]  eta: 0:10:58  lr: 0.000002  min_lr: 0.000000  loss: 4.1203 (4.1268)  class_acc: 0.3750 (0.3481)  loss_scale: 16384.0000 (26081.6947)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0006  max mem: 15572
[2025-01-17 05:17:37,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:17:37,070] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:17:37,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:17:37,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [ 330/1404]  eta: 0:10:51  lr: 0.000002  min_lr: 0.000000  loss: 3.9498 (4.1189)  class_acc: 0.3750 (0.3507)  loss_scale: 16384.0000 (26085.7039)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.0006  max mem: 15572
Epoch: [37]  [ 340/1404]  eta: 0:10:43  lr: 0.000002  min_lr: 0.000000  loss: 4.0626 (4.1178)  class_acc: 0.3333 (0.3512)  loss_scale: 32768.0000 (26281.6657)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0009  max mem: 15572
Epoch: [37]  [ 350/1404]  eta: 0:10:37  lr: 0.000002  min_lr: 0.000000  loss: 4.1345 (4.1182)  class_acc: 0.4167 (0.3521)  loss_scale: 32768.0000 (26466.4615)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0010  max mem: 15572
Epoch: [37]  [ 360/1404]  eta: 0:10:30  lr: 0.000002  min_lr: 0.000000  loss: 4.2241 (4.1231)  class_acc: 0.2917 (0.3500)  loss_scale: 32768.0000 (26641.0194)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0006  max mem: 15572
Epoch: [37]  [ 370/1404]  eta: 0:10:22  lr: 0.000001  min_lr: 0.000000  loss: 4.2829 (4.1256)  class_acc: 0.2917 (0.3507)  loss_scale: 32768.0000 (26806.1671)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0005  max mem: 15572
Epoch: [37]  [ 380/1404]  eta: 0:10:14  lr: 0.000001  min_lr: 0.000000  loss: 4.2714 (4.1298)  class_acc: 0.3333 (0.3507)  loss_scale: 32768.0000 (26962.6457)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0005  max mem: 15572
Epoch: [37]  [ 390/1404]  eta: 0:10:06  lr: 0.000001  min_lr: 0.000000  loss: 4.2630 (4.1301)  class_acc: 0.3333 (0.3510)  loss_scale: 32768.0000 (27111.1202)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0008  max mem: 15572
Epoch: [37]  [ 400/1404]  eta: 0:10:01  lr: 0.000001  min_lr: 0.000000  loss: 4.0209 (4.1274)  class_acc: 0.3333 (0.3516)  loss_scale: 32768.0000 (27252.1895)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0009  max mem: 15572
[2025-01-17 05:18:21,469] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52351
[2025-01-17 05:18:21,469] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:18:21,469] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 05:18:21,504] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52351
[2025-01-17 05:18:21,505] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [37]  [ 410/1404]  eta: 0:09:54  lr: 0.000001  min_lr: 0.000000  loss: 3.9697 (4.1277)  class_acc: 0.3750 (0.3539)  loss_scale: 32768.0000 (27067.4842)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0008  max mem: 15572
Epoch: [37]  [ 420/1404]  eta: 0:09:50  lr: 0.000001  min_lr: 0.000000  loss: 4.2083 (4.1305)  class_acc: 0.3750 (0.3532)  loss_scale: 16384.0000 (26813.7197)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.0010  max mem: 15572
Epoch: [37]  [ 430/1404]  eta: 0:09:43  lr: 0.000001  min_lr: 0.000000  loss: 4.2406 (4.1333)  class_acc: 0.2917 (0.3526)  loss_scale: 16384.0000 (26571.7309)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.0008  max mem: 15572
Epoch: [37]  [ 440/1404]  eta: 0:09:38  lr: 0.000001  min_lr: 0.000000  loss: 4.2104 (4.1333)  class_acc: 0.3333 (0.3517)  loss_scale: 16384.0000 (26340.7166)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0006  max mem: 15572
Epoch: [37]  [ 450/1404]  eta: 0:09:34  lr: 0.000001  min_lr: 0.000000  loss: 4.1157 (4.1332)  class_acc: 0.3750 (0.3528)  loss_scale: 16384.0000 (26119.9468)  weight_decay: 0.0500 (0.0500)  time: 0.6802  data: 0.0006  max mem: 15572
Epoch: [37]  [ 460/1404]  eta: 0:09:27  lr: 0.000001  min_lr: 0.000000  loss: 4.1129 (4.1345)  class_acc: 0.2917 (0.3520)  loss_scale: 16384.0000 (25908.7549)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.0006  max mem: 15572
Epoch: [37]  [ 470/1404]  eta: 0:09:20  lr: 0.000001  min_lr: 0.000000  loss: 4.2513 (4.1367)  class_acc: 0.2917 (0.3507)  loss_scale: 16384.0000 (25706.5308)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0005  max mem: 15572
Epoch: [37]  [ 480/1404]  eta: 0:09:13  lr: 0.000001  min_lr: 0.000000  loss: 4.2513 (4.1380)  class_acc: 0.3750 (0.3524)  loss_scale: 16384.0000 (25512.7152)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0006  max mem: 15572
Epoch: [37]  [ 490/1404]  eta: 0:09:07  lr: 0.000001  min_lr: 0.000000  loss: 4.2528 (4.1419)  class_acc: 0.4167 (0.3527)  loss_scale: 16384.0000 (25326.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0006  max mem: 15572
Epoch: [37]  [ 500/1404]  eta: 0:09:02  lr: 0.000001  min_lr: 0.000000  loss: 4.1799 (4.1401)  class_acc: 0.4167 (0.3541)  loss_scale: 16384.0000 (25148.2954)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0004  max mem: 15572
Epoch: [37]  [ 510/1404]  eta: 0:08:55  lr: 0.000001  min_lr: 0.000000  loss: 4.1386 (4.1404)  class_acc: 0.4167 (0.3554)  loss_scale: 16384.0000 (24976.7828)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.0004  max mem: 15572
Epoch: [37]  [ 520/1404]  eta: 0:08:48  lr: 0.000001  min_lr: 0.000000  loss: 4.1206 (4.1382)  class_acc: 0.3333 (0.3545)  loss_scale: 16384.0000 (24811.8541)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0007  max mem: 15572
Epoch: [37]  [ 530/1404]  eta: 0:08:43  lr: 0.000001  min_lr: 0.000000  loss: 4.1346 (4.1403)  class_acc: 0.2917 (0.3540)  loss_scale: 16384.0000 (24653.1375)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0008  max mem: 15572
[2025-01-17 05:19:38,470] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:19:38,470] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:19:38,534] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:19:38,535] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [ 540/1404]  eta: 0:08:36  lr: 0.000001  min_lr: 0.000000  loss: 4.1664 (4.1391)  class_acc: 0.2917 (0.3534)  loss_scale: 16384.0000 (24772.8503)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0005  max mem: 15572
Epoch: [37]  [ 550/1404]  eta: 0:08:29  lr: 0.000001  min_lr: 0.000000  loss: 4.1294 (4.1405)  class_acc: 0.3333 (0.3543)  loss_scale: 32768.0000 (24917.9528)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0005  max mem: 15572
Epoch: [37]  [ 560/1404]  eta: 0:08:25  lr: 0.000001  min_lr: 0.000000  loss: 3.9950 (4.1374)  class_acc: 0.3750 (0.3543)  loss_scale: 32768.0000 (25057.8824)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.0027  max mem: 15572
Epoch: [37]  [ 570/1404]  eta: 0:08:17  lr: 0.000001  min_lr: 0.000000  loss: 3.9742 (4.1378)  class_acc: 0.3750 (0.3545)  loss_scale: 32768.0000 (25192.9107)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0027  max mem: 15572
Epoch: [37]  [ 580/1404]  eta: 0:08:11  lr: 0.000001  min_lr: 0.000000  loss: 4.2054 (4.1368)  class_acc: 0.3750 (0.3554)  loss_scale: 32768.0000 (25323.2909)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0005  max mem: 15572
Epoch: [37]  [ 590/1404]  eta: 0:08:04  lr: 0.000001  min_lr: 0.000000  loss: 4.2269 (4.1387)  class_acc: 0.3750 (0.3546)  loss_scale: 32768.0000 (25449.2589)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0006  max mem: 15572
Epoch: [37]  [ 600/1404]  eta: 0:07:59  lr: 0.000001  min_lr: 0.000000  loss: 4.2800 (4.1422)  class_acc: 0.2917 (0.3546)  loss_scale: 32768.0000 (25571.0349)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0006  max mem: 15572
Epoch: [37]  [ 610/1404]  eta: 0:07:52  lr: 0.000001  min_lr: 0.000000  loss: 4.2800 (4.1437)  class_acc: 0.3333 (0.3551)  loss_scale: 32768.0000 (25688.8249)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0007  max mem: 15572
Epoch: [37]  [ 620/1404]  eta: 0:07:46  lr: 0.000001  min_lr: 0.000000  loss: 4.2061 (4.1443)  class_acc: 0.4167 (0.3553)  loss_scale: 32768.0000 (25802.8213)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0006  max mem: 15572
Epoch: [37]  [ 630/1404]  eta: 0:07:39  lr: 0.000001  min_lr: 0.000000  loss: 4.1616 (4.1439)  class_acc: 0.3750 (0.3553)  loss_scale: 32768.0000 (25913.2044)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0005  max mem: 15572
Epoch: [37]  [ 640/1404]  eta: 0:07:35  lr: 0.000001  min_lr: 0.000000  loss: 4.2276 (4.1444)  class_acc: 0.2917 (0.3541)  loss_scale: 32768.0000 (26020.1435)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.0007  max mem: 15572
Epoch: [37]  [ 650/1404]  eta: 0:07:28  lr: 0.000001  min_lr: 0.000000  loss: 4.0433 (4.1416)  class_acc: 0.3750 (0.3545)  loss_scale: 32768.0000 (26123.7972)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.0008  max mem: 15572
[2025-01-17 05:20:53,627] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:20:53,627] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:20:53,628] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:20:53,628] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 660/1404]  eta: 0:07:23  lr: 0.000001  min_lr: 0.000000  loss: 4.0545 (4.1430)  class_acc: 0.3750 (0.3546)  loss_scale: 32768.0000 (26273.8880)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0008  max mem: 15572
[2025-01-17 05:20:54,111] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52609
[2025-01-17 05:20:54,111] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52609
[2025-01-17 05:20:54,111] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:20:54,111] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:20:54,112] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 670/1404]  eta: 0:07:17  lr: 0.000001  min_lr: 0.000000  loss: 4.1467 (4.1435)  class_acc: 0.3333 (0.3548)  loss_scale: 32768.0000 (26370.6706)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0007  max mem: 15572
[2025-01-17 05:21:02,333] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52624
[2025-01-17 05:21:02,333] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52624
[2025-01-17 05:21:02,333] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:21:02,333] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:21:02,333] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [37]  [ 680/1404]  eta: 0:07:10  lr: 0.000001  min_lr: 0.000000  loss: 4.1154 (4.1416)  class_acc: 0.3750 (0.3562)  loss_scale: 32768.0000 (26344.3172)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0010  max mem: 15572
Epoch: [37]  [ 690/1404]  eta: 0:07:04  lr: 0.000001  min_lr: 0.000000  loss: 4.0973 (4.1427)  class_acc: 0.4583 (0.3567)  loss_scale: 16384.0000 (26200.1737)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0011  max mem: 15572
Epoch: [37]  [ 700/1404]  eta: 0:06:59  lr: 0.000001  min_lr: 0.000000  loss: 4.0973 (4.1418)  class_acc: 0.3750 (0.3569)  loss_scale: 16384.0000 (26060.1427)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.0007  max mem: 15572
Epoch: [37]  [ 710/1404]  eta: 0:06:52  lr: 0.000001  min_lr: 0.000000  loss: 4.0265 (4.1409)  class_acc: 0.3750 (0.3574)  loss_scale: 16384.0000 (25924.0506)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0008  max mem: 15572
Epoch: [37]  [ 720/1404]  eta: 0:06:45  lr: 0.000001  min_lr: 0.000000  loss: 4.0091 (4.1387)  class_acc: 0.3750 (0.3577)  loss_scale: 16384.0000 (25791.7337)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0009  max mem: 15572
Epoch: [37]  [ 730/1404]  eta: 0:06:39  lr: 0.000001  min_lr: 0.000000  loss: 3.9958 (4.1362)  class_acc: 0.4167 (0.3583)  loss_scale: 16384.0000 (25663.0369)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0009  max mem: 15572
Epoch: [37]  [ 740/1404]  eta: 0:06:33  lr: 0.000001  min_lr: 0.000000  loss: 4.0184 (4.1353)  class_acc: 0.3750 (0.3586)  loss_scale: 16384.0000 (25537.8138)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0009  max mem: 15572
Epoch: [37]  [ 750/1404]  eta: 0:06:27  lr: 0.000001  min_lr: 0.000000  loss: 4.1123 (4.1366)  class_acc: 0.2917 (0.3579)  loss_scale: 16384.0000 (25415.9254)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0008  max mem: 15572
Epoch: [37]  [ 760/1404]  eta: 0:06:21  lr: 0.000001  min_lr: 0.000000  loss: 4.1031 (4.1358)  class_acc: 0.3333 (0.3585)  loss_scale: 16384.0000 (25297.2405)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0007  max mem: 15572
Epoch: [37]  [ 770/1404]  eta: 0:06:15  lr: 0.000001  min_lr: 0.000000  loss: 4.0704 (4.1365)  class_acc: 0.3750 (0.3580)  loss_scale: 16384.0000 (25181.6342)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.0006  max mem: 15572
Epoch: [37]  [ 780/1404]  eta: 0:06:09  lr: 0.000001  min_lr: 0.000000  loss: 4.2443 (4.1379)  class_acc: 0.3750 (0.3581)  loss_scale: 16384.0000 (25068.9885)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0007  max mem: 15572
Epoch: [37]  [ 790/1404]  eta: 0:06:03  lr: 0.000001  min_lr: 0.000000  loss: 4.2436 (4.1368)  class_acc: 0.3333 (0.3575)  loss_scale: 16384.0000 (24959.1909)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0005  max mem: 15572
Epoch: [37]  [ 800/1404]  eta: 0:05:57  lr: 0.000001  min_lr: 0.000000  loss: 4.0074 (4.1353)  class_acc: 0.3333 (0.3577)  loss_scale: 16384.0000 (24852.1348)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0005  max mem: 15572
[2025-01-17 05:22:17,309] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:22:17,309] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:22:17,357] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:22:17,358] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [ 810/1404]  eta: 0:05:51  lr: 0.000001  min_lr: 0.000000  loss: 4.1863 (4.1357)  class_acc: 0.2917 (0.3571)  loss_scale: 16384.0000 (24868.9322)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0475  max mem: 15572
Epoch: [37]  [ 820/1404]  eta: 0:05:45  lr: 0.000001  min_lr: 0.000000  loss: 4.2070 (4.1360)  class_acc: 0.2917 (0.3580)  loss_scale: 32768.0000 (24965.1449)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0475  max mem: 15572
Epoch: [37]  [ 830/1404]  eta: 0:05:39  lr: 0.000001  min_lr: 0.000000  loss: 4.0179 (4.1341)  class_acc: 0.4583 (0.3587)  loss_scale: 32768.0000 (25059.0421)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.0381  max mem: 15572
Epoch: [37]  [ 840/1404]  eta: 0:05:33  lr: 0.000001  min_lr: 0.000000  loss: 3.9970 (4.1337)  class_acc: 0.4583 (0.3600)  loss_scale: 32768.0000 (25150.7063)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0382  max mem: 15572
Epoch: [37]  [ 850/1404]  eta: 0:05:27  lr: 0.000001  min_lr: 0.000000  loss: 4.0256 (4.1325)  class_acc: 0.4167 (0.3606)  loss_scale: 32768.0000 (25240.2162)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0630  max mem: 15572
[2025-01-17 05:22:45,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52802
[2025-01-17 05:22:45,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:22:45,228] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 05:22:45,229] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52802
[2025-01-17 05:22:45,230] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [37]  [ 860/1404]  eta: 0:05:21  lr: 0.000001  min_lr: 0.000000  loss: 4.0242 (4.1318)  class_acc: 0.4167 (0.3614)  loss_scale: 32768.0000 (25194.4437)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.0659  max mem: 15572
Epoch: [37]  [ 870/1404]  eta: 0:05:15  lr: 0.000001  min_lr: 0.000000  loss: 4.0242 (4.1305)  class_acc: 0.4167 (0.3617)  loss_scale: 16384.0000 (25093.2905)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0294  max mem: 15572
Epoch: [37]  [ 880/1404]  eta: 0:05:09  lr: 0.000001  min_lr: 0.000000  loss: 4.1356 (4.1316)  class_acc: 0.3333 (0.3623)  loss_scale: 16384.0000 (24994.4336)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0794  max mem: 15572
Epoch: [37]  [ 890/1404]  eta: 0:05:03  lr: 0.000001  min_lr: 0.000000  loss: 4.1356 (4.1311)  class_acc: 0.3750 (0.3623)  loss_scale: 16384.0000 (24897.7957)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0591  max mem: 15572
Epoch: [37]  [ 900/1404]  eta: 0:04:57  lr: 0.000001  min_lr: 0.000000  loss: 4.0340 (4.1295)  class_acc: 0.3750 (0.3632)  loss_scale: 16384.0000 (24803.3030)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0147  max mem: 15572
Epoch: [37]  [ 910/1404]  eta: 0:04:51  lr: 0.000001  min_lr: 0.000000  loss: 4.0428 (4.1298)  class_acc: 0.3750 (0.3627)  loss_scale: 16384.0000 (24710.8847)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0095  max mem: 15572
Epoch: [37]  [ 920/1404]  eta: 0:04:45  lr: 0.000001  min_lr: 0.000000  loss: 4.1133 (4.1300)  class_acc: 0.2917 (0.3627)  loss_scale: 16384.0000 (24620.4734)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0207  max mem: 15572
Epoch: [37]  [ 930/1404]  eta: 0:04:39  lr: 0.000001  min_lr: 0.000000  loss: 4.1415 (4.1293)  class_acc: 0.2917 (0.3623)  loss_scale: 16384.0000 (24532.0043)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0525  max mem: 15572
Epoch: [37]  [ 940/1404]  eta: 0:04:33  lr: 0.000001  min_lr: 0.000000  loss: 4.1647 (4.1292)  class_acc: 0.2917 (0.3615)  loss_scale: 16384.0000 (24445.4155)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.0726  max mem: 15572
Epoch: [37]  [ 950/1404]  eta: 0:04:27  lr: 0.000001  min_lr: 0.000000  loss: 4.1945 (4.1294)  class_acc: 0.3333 (0.3623)  loss_scale: 16384.0000 (24360.6477)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.0406  max mem: 15572
Epoch: [37]  [ 960/1404]  eta: 0:04:21  lr: 0.000001  min_lr: 0.000000  loss: 4.1310 (4.1288)  class_acc: 0.4167 (0.3628)  loss_scale: 16384.0000 (24277.6441)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0347  max mem: 15572
Epoch: [37]  [ 970/1404]  eta: 0:04:16  lr: 0.000001  min_lr: 0.000000  loss: 4.0770 (4.1289)  class_acc: 0.3750 (0.3623)  loss_scale: 16384.0000 (24196.3502)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1100  max mem: 15572
Epoch: [37]  [ 980/1404]  eta: 0:04:10  lr: 0.000001  min_lr: 0.000000  loss: 4.2735 (4.1308)  class_acc: 0.3750 (0.3626)  loss_scale: 16384.0000 (24116.7136)  weight_decay: 0.0500 (0.0500)  time: 0.6679  data: 0.1757  max mem: 15572
[2025-01-17 05:24:01,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:24:01,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:24:01,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:24:01,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [ 990/1404]  eta: 0:04:04  lr: 0.000001  min_lr: 0.000000  loss: 4.2826 (4.1315)  class_acc: 0.3750 (0.3626)  loss_scale: 16384.0000 (24170.9465)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1146  max mem: 15572
Epoch: [37]  [1000/1404]  eta: 0:03:58  lr: 0.000001  min_lr: 0.000000  loss: 4.0799 (4.1302)  class_acc: 0.3333 (0.3627)  loss_scale: 32768.0000 (24256.8312)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0148  max mem: 15572
Epoch: [37]  [1010/1404]  eta: 0:03:52  lr: 0.000001  min_lr: 0.000000  loss: 4.0125 (4.1282)  class_acc: 0.3750 (0.3629)  loss_scale: 32768.0000 (24341.0168)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0006  max mem: 15572
Epoch: [37]  [1020/1404]  eta: 0:03:46  lr: 0.000001  min_lr: 0.000000  loss: 3.9934 (4.1275)  class_acc: 0.3750 (0.3629)  loss_scale: 32768.0000 (24423.5534)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0006  max mem: 15572
Epoch: [37]  [1030/1404]  eta: 0:03:40  lr: 0.000001  min_lr: 0.000000  loss: 4.0690 (4.1286)  class_acc: 0.3333 (0.3627)  loss_scale: 32768.0000 (24504.4888)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0194  max mem: 15572
Epoch: [37]  [1040/1404]  eta: 0:03:34  lr: 0.000001  min_lr: 0.000000  loss: 4.0455 (4.1274)  class_acc: 0.3333 (0.3626)  loss_scale: 32768.0000 (24583.8694)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0195  max mem: 15572
Epoch: [37]  [1050/1404]  eta: 0:03:28  lr: 0.000001  min_lr: 0.000000  loss: 4.1704 (4.1282)  class_acc: 0.3333 (0.3623)  loss_scale: 32768.0000 (24661.7393)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0657  max mem: 15572
[2025-01-17 05:24:39,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=53000, skipped=314, lr=[1.0141512276577659e-08, 1.0141512276577659e-08, 1.4487874680825229e-08, 1.4487874680825229e-08, 2.069696382975033e-08, 2.069696382975033e-08, 2.9567091185357615e-08, 2.9567091185357615e-08, 4.223870169336803e-08, 4.223870169336803e-08, 6.034100241909718e-08, 6.034100241909718e-08, 8.620143202728168e-08, 8.620143202728168e-08, 1.231449028961167e-07, 1.231449028961167e-07, 1.759212898515953e-07, 1.759212898515953e-07, 2.513161283594219e-07, 2.513161283594219e-07, 3.5902304051345984e-07, 3.5902304051345984e-07, 5.128900578763712e-07, 5.128900578763712e-07, 7.327000826805304e-07, 7.327000826805304e-07, 1.0467144038293292e-06, 1.0467144038293292e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 05:24:39,191] [INFO] [timer.py:260:stop] epoch=0/micro_step=53000/global_step=53000, RunningAvgSamplesPerSec=48.20667657924648, CurrSamplesPerSec=58.33311370760108, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-17 05:24:42,692] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53004
[2025-01-17 05:24:42,693] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:24:42,693] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 05:24:42,722] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53004
[2025-01-17 05:24:42,723] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [37]  [1060/1404]  eta: 0:03:22  lr: 0.000001  min_lr: 0.000000  loss: 4.1941 (4.1284)  class_acc: 0.3333 (0.3625)  loss_scale: 32768.0000 (24660.9312)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1055  max mem: 15572
Epoch: [37]  [1070/1404]  eta: 0:03:16  lr: 0.000001  min_lr: 0.000000  loss: 4.0530 (4.1289)  class_acc: 0.3333 (0.3627)  loss_scale: 16384.0000 (24583.6489)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0670  max mem: 15572
Epoch: [37]  [1080/1404]  eta: 0:03:10  lr: 0.000001  min_lr: 0.000000  loss: 4.1306 (4.1297)  class_acc: 0.4167 (0.3628)  loss_scale: 16384.0000 (24507.7965)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0548  max mem: 15572
Epoch: [37]  [1090/1404]  eta: 0:03:04  lr: 0.000001  min_lr: 0.000000  loss: 4.1502 (4.1299)  class_acc: 0.3333 (0.3629)  loss_scale: 16384.0000 (24433.3346)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0396  max mem: 15572
Epoch: [37]  [1100/1404]  eta: 0:02:58  lr: 0.000001  min_lr: 0.000000  loss: 4.1028 (4.1288)  class_acc: 0.3333 (0.3629)  loss_scale: 16384.0000 (24360.2252)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0641  max mem: 15572
Epoch: [37]  [1110/1404]  eta: 0:02:53  lr: 0.000001  min_lr: 0.000000  loss: 3.9570 (4.1289)  class_acc: 0.2917 (0.3625)  loss_scale: 16384.0000 (24288.4320)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1215  max mem: 15572
Epoch: [37]  [1120/1404]  eta: 0:02:47  lr: 0.000001  min_lr: 0.000000  loss: 4.1559 (4.1290)  class_acc: 0.2917 (0.3625)  loss_scale: 16384.0000 (24217.9197)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.1307  max mem: 15572
Epoch: [37]  [1130/1404]  eta: 0:02:41  lr: 0.000001  min_lr: 0.000000  loss: 4.0979 (4.1273)  class_acc: 0.3333 (0.3619)  loss_scale: 16384.0000 (24148.6543)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1047  max mem: 15572
Epoch: [37]  [1140/1404]  eta: 0:02:35  lr: 0.000001  min_lr: 0.000000  loss: 4.0929 (4.1276)  class_acc: 0.2917 (0.3617)  loss_scale: 16384.0000 (24080.6030)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0804  max mem: 15572
Epoch: [37]  [1150/1404]  eta: 0:02:29  lr: 0.000001  min_lr: 0.000000  loss: 4.0994 (4.1273)  class_acc: 0.2917 (0.3613)  loss_scale: 16384.0000 (24013.7341)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0378  max mem: 15572
Epoch: [37]  [1160/1404]  eta: 0:02:23  lr: 0.000001  min_lr: 0.000000  loss: 4.1658 (4.1280)  class_acc: 0.2917 (0.3610)  loss_scale: 16384.0000 (23948.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0161  max mem: 15572
Epoch: [37]  [1170/1404]  eta: 0:02:17  lr: 0.000001  min_lr: 0.000000  loss: 4.2401 (4.1295)  class_acc: 0.2917 (0.3609)  loss_scale: 16384.0000 (23883.4227)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0540  max mem: 15572
Epoch: [37]  [1180/1404]  eta: 0:02:11  lr: 0.000001  min_lr: 0.000000  loss: 4.2022 (4.1282)  class_acc: 0.3333 (0.3609)  loss_scale: 16384.0000 (23819.9221)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0387  max mem: 15572
[2025-01-17 05:25:58,917] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:25:58,917] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:25:58,985] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:25:58,985] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [1190/1404]  eta: 0:02:06  lr: 0.000001  min_lr: 0.000000  loss: 4.1170 (4.1282)  class_acc: 0.3333 (0.3610)  loss_scale: 16384.0000 (23840.0269)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.0007  max mem: 15572
Epoch: [37]  [1200/1404]  eta: 0:02:00  lr: 0.000001  min_lr: 0.000000  loss: 4.1951 (4.1298)  class_acc: 0.2917 (0.3602)  loss_scale: 32768.0000 (23914.3647)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.0008  max mem: 15572
Epoch: [37]  [1210/1404]  eta: 0:01:54  lr: 0.000001  min_lr: 0.000000  loss: 4.1951 (4.1300)  class_acc: 0.2917 (0.3598)  loss_scale: 32768.0000 (23987.4748)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0008  max mem: 15572
Epoch: [37]  [1220/1404]  eta: 0:01:48  lr: 0.000001  min_lr: 0.000000  loss: 4.1055 (4.1298)  class_acc: 0.2917 (0.3596)  loss_scale: 32768.0000 (24059.3874)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0007  max mem: 15572
Epoch: [37]  [1230/1404]  eta: 0:01:42  lr: 0.000001  min_lr: 0.000000  loss: 4.1094 (4.1291)  class_acc: 0.3750 (0.3598)  loss_scale: 32768.0000 (24130.1316)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.0009  max mem: 15572
Epoch: [37]  [1240/1404]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 4.1094 (4.1299)  class_acc: 0.3333 (0.3594)  loss_scale: 32768.0000 (24199.7357)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.0008  max mem: 15572
Epoch: [37]  [1250/1404]  eta: 0:01:30  lr: 0.000001  min_lr: 0.000000  loss: 4.1597 (4.1295)  class_acc: 0.3333 (0.3598)  loss_scale: 32768.0000 (24268.2270)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0006  max mem: 15572
Epoch: [37]  [1260/1404]  eta: 0:01:24  lr: 0.000001  min_lr: 0.000000  loss: 4.0483 (4.1286)  class_acc: 0.4583 (0.3603)  loss_scale: 32768.0000 (24335.6320)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0007  max mem: 15572
Epoch: [37]  [1270/1404]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 4.0483 (4.1272)  class_acc: 0.4167 (0.3606)  loss_scale: 32768.0000 (24401.9764)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0037  max mem: 15572
Epoch: [37]  [1280/1404]  eta: 0:01:13  lr: 0.000001  min_lr: 0.000000  loss: 4.1402 (4.1271)  class_acc: 0.3333 (0.3606)  loss_scale: 32768.0000 (24467.2849)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0039  max mem: 15572
[2025-01-17 05:26:59,309] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53234
[2025-01-17 05:26:59,309] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:26:59,309] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 05:26:59,388] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53234
[2025-01-17 05:26:59,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [37]  [1290/1404]  eta: 0:01:07  lr: 0.000001  min_lr: 0.000000  loss: 4.1402 (4.1267)  class_acc: 0.3750 (0.3611)  loss_scale: 32768.0000 (24468.1270)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.0006  max mem: 15572
Epoch: [37]  [1300/1404]  eta: 0:01:01  lr: 0.000001  min_lr: 0.000000  loss: 3.9899 (4.1256)  class_acc: 0.3750 (0.3611)  loss_scale: 16384.0000 (24405.9892)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0008  max mem: 15572
Epoch: [37]  [1310/1404]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 3.9147 (4.1250)  class_acc: 0.3333 (0.3611)  loss_scale: 16384.0000 (24344.7994)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0010  max mem: 15572
Epoch: [37]  [1320/1404]  eta: 0:00:49  lr: 0.000001  min_lr: 0.000000  loss: 4.0378 (4.1238)  class_acc: 0.4167 (0.3617)  loss_scale: 16384.0000 (24284.5360)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0008  max mem: 15572
Epoch: [37]  [1330/1404]  eta: 0:00:43  lr: 0.000001  min_lr: 0.000000  loss: 4.0760 (4.1242)  class_acc: 0.3750 (0.3614)  loss_scale: 16384.0000 (24225.1781)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.0008  max mem: 15572
Epoch: [37]  [1340/1404]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 4.2410 (4.1241)  class_acc: 0.3333 (0.3618)  loss_scale: 16384.0000 (24166.7054)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0006  max mem: 15572
Epoch: [37]  [1350/1404]  eta: 0:00:31  lr: 0.000001  min_lr: 0.000000  loss: 4.1523 (4.1239)  class_acc: 0.3333 (0.3615)  loss_scale: 16384.0000 (24109.0984)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0009  max mem: 15572
Epoch: [37]  [1360/1404]  eta: 0:00:25  lr: 0.000001  min_lr: 0.000000  loss: 4.0539 (4.1234)  class_acc: 0.2917 (0.3616)  loss_scale: 16384.0000 (24052.3380)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0010  max mem: 15572
Epoch: [37]  [1370/1404]  eta: 0:00:19  lr: 0.000001  min_lr: 0.000000  loss: 4.0336 (4.1232)  class_acc: 0.3333 (0.3621)  loss_scale: 16384.0000 (23996.4055)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0008  max mem: 15572
Epoch: [37]  [1380/1404]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 4.0965 (4.1233)  class_acc: 0.3750 (0.3623)  loss_scale: 16384.0000 (23941.2831)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0007  max mem: 15572
Epoch: [37]  [1390/1404]  eta: 0:00:08  lr: 0.000001  min_lr: 0.000000  loss: 4.1388 (4.1226)  class_acc: 0.3333 (0.3625)  loss_scale: 16384.0000 (23886.9533)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0007  max mem: 15572
Epoch: [37]  [1400/1404]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000000  loss: 4.0809 (4.1221)  class_acc: 0.3333 (0.3629)  loss_scale: 16384.0000 (23833.3990)  weight_decay: 0.0500 (0.0500)  time: 0.4863  data: 0.0005  max mem: 15572
Epoch: [37]  [1403/1404]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 4.0084 (4.1218)  class_acc: 0.3750 (0.3632)  loss_scale: 16384.0000 (23817.4815)  weight_decay: 0.0500 (0.0500)  time: 0.4005  data: 0.0005  max mem: 15572
Epoch: [37] Total time: 0:13:44 (0.5871 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 4.0084 (4.1198)  class_acc: 0.3750 (0.3608)  loss_scale: 16384.0000 (23817.4815)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:11:54  loss: 1.7470 (1.7470)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.2500  data: 5.0545  max mem: 15572
Val:  [ 10/136]  eta: 0:01:34  loss: 2.2265 (2.1970)  acc1: 55.5556 (52.0202)  acc5: 83.3333 (83.3333)  time: 0.7511  data: 0.5742  max mem: 15572
Val:  [ 20/136]  eta: 0:01:00  loss: 2.4564 (2.3625)  acc1: 44.4444 (47.3545)  acc5: 77.7778 (79.3651)  time: 0.2830  data: 0.1065  max mem: 15572
Val:  [ 30/136]  eta: 0:00:47  loss: 2.2808 (2.2829)  acc1: 38.8889 (47.8495)  acc5: 83.3333 (80.2867)  time: 0.2763  data: 0.0877  max mem: 15572
Val:  [ 40/136]  eta: 0:00:40  loss: 2.0551 (2.2507)  acc1: 55.5556 (50.8130)  acc5: 83.3333 (80.6233)  time: 0.3214  data: 0.1298  max mem: 15572
Val:  [ 50/136]  eta: 0:00:35  loss: 2.1631 (2.2445)  acc1: 61.1111 (51.0893)  acc5: 83.3333 (80.9368)  time: 0.3623  data: 0.1737  max mem: 15572
Val:  [ 60/136]  eta: 0:00:30  loss: 2.2003 (2.3114)  acc1: 44.4444 (48.2696)  acc5: 77.7778 (79.9636)  time: 0.3629  data: 0.1714  max mem: 15572
Val:  [ 70/136]  eta: 0:00:26  loss: 2.2956 (2.2969)  acc1: 50.0000 (49.0610)  acc5: 77.7778 (80.2817)  time: 0.3522  data: 0.1495  max mem: 15572
Val:  [ 80/136]  eta: 0:00:21  loss: 2.1431 (2.2896)  acc1: 50.0000 (48.9712)  acc5: 83.3333 (80.5899)  time: 0.3414  data: 0.1375  max mem: 15572
Val:  [ 90/136]  eta: 0:00:17  loss: 2.2674 (2.2972)  acc1: 44.4444 (48.4737)  acc5: 77.7778 (80.2198)  time: 0.3654  data: 0.1647  max mem: 15572
Val:  [100/136]  eta: 0:00:13  loss: 2.5403 (2.3536)  acc1: 38.8889 (46.6997)  acc5: 72.2222 (78.7129)  time: 0.3614  data: 0.1598  max mem: 15572
Val:  [110/136]  eta: 0:00:09  loss: 2.4446 (2.3436)  acc1: 44.4444 (47.0971)  acc5: 77.7778 (78.9790)  time: 0.3463  data: 0.1561  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1426 (2.3072)  acc1: 55.5556 (48.3930)  acc5: 88.8889 (79.6143)  time: 0.3638  data: 0.1803  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8504 (2.2759)  acc1: 61.1111 (49.2791)  acc5: 88.8889 (80.3223)  time: 0.3107  data: 0.1422  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.9154 (2.2733)  acc1: 55.5556 (49.5086)  acc5: 83.3333 (80.3849)  time: 0.2328  data: 0.0807  max mem: 15572
Val: Total time: 0:00:49 (0.3629 s / it)
* Acc@1 47.768 Acc@5 79.423 loss 2.317
Accuracy of the network on the 4883 val videos: 47.8%
Max accuracy: 47.99%
Epoch: [38]  [   0/1404]  eta: 3:07:12  lr: 0.000001  min_lr: 0.000000  loss: 3.7316 (3.7316)  class_acc: 0.2500 (0.2500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 8.0002  data: 6.1772  max mem: 15572
Epoch: [38]  [  10/1404]  eta: 0:27:48  lr: 0.000001  min_lr: 0.000000  loss: 4.0588 (4.0317)  class_acc: 0.3750 (0.3220)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1966  data: 0.5627  max mem: 15572
[2025-01-17 05:29:06,870] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:29:06,870] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:29:06,871] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:29:06,871] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [38]  [  20/1404]  eta: 0:20:35  lr: 0.000001  min_lr: 0.000000  loss: 4.1289 (4.0520)  class_acc: 0.3750 (0.3373)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0010  max mem: 15572
Epoch: [38]  [  30/1404]  eta: 0:17:42  lr: 0.000001  min_lr: 0.000000  loss: 4.1289 (4.0737)  class_acc: 0.3750 (0.3723)  loss_scale: 32768.0000 (26954.3226)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0008  max mem: 15572
Epoch: [38]  [  40/1404]  eta: 0:16:21  lr: 0.000001  min_lr: 0.000000  loss: 3.9866 (4.0781)  class_acc: 0.4167 (0.3740)  loss_scale: 32768.0000 (28372.2927)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0348  max mem: 15572
Epoch: [38]  [  50/1404]  eta: 0:15:59  lr: 0.000001  min_lr: 0.000000  loss: 4.1403 (4.1071)  class_acc: 0.3333 (0.3619)  loss_scale: 32768.0000 (29234.1961)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0348  max mem: 15572
Epoch: [38]  [  60/1404]  eta: 0:15:38  lr: 0.000001  min_lr: 0.000000  loss: 4.1403 (4.1254)  class_acc: 0.3333 (0.3750)  loss_scale: 32768.0000 (29813.5082)  weight_decay: 0.0500 (0.0500)  time: 0.6542  data: 0.0007  max mem: 15572
Epoch: [38]  [  70/1404]  eta: 0:15:13  lr: 0.000001  min_lr: 0.000000  loss: 4.0910 (4.1166)  class_acc: 0.3750 (0.3697)  loss_scale: 32768.0000 (30229.6338)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.0005  max mem: 15572
Epoch: [38]  [  80/1404]  eta: 0:14:47  lr: 0.000001  min_lr: 0.000000  loss: 4.1905 (4.1264)  class_acc: 0.3750 (0.3668)  loss_scale: 32768.0000 (30543.0123)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0005  max mem: 15572
Epoch: [38]  [  90/1404]  eta: 0:14:29  lr: 0.000001  min_lr: 0.000000  loss: 4.1869 (4.1254)  class_acc: 0.3750 (0.3668)  loss_scale: 32768.0000 (30787.5165)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0006  max mem: 15572
Epoch: [38]  [ 100/1404]  eta: 0:14:12  lr: 0.000001  min_lr: 0.000000  loss: 4.1869 (4.1364)  class_acc: 0.3750 (0.3672)  loss_scale: 32768.0000 (30983.6040)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.0006  max mem: 15572
Epoch: [38]  [ 110/1404]  eta: 0:14:01  lr: 0.000001  min_lr: 0.000000  loss: 4.1981 (4.1385)  class_acc: 0.3750 (0.3637)  loss_scale: 32768.0000 (31144.3604)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.0006  max mem: 15572
Epoch: [38]  [ 120/1404]  eta: 0:13:49  lr: 0.000001  min_lr: 0.000000  loss: 4.1333 (4.1252)  class_acc: 0.3750 (0.3667)  loss_scale: 32768.0000 (31278.5455)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0006  max mem: 15572
Epoch: [38]  [ 130/1404]  eta: 0:13:38  lr: 0.000001  min_lr: 0.000000  loss: 4.1397 (4.1307)  class_acc: 0.3750 (0.3728)  loss_scale: 32768.0000 (31392.2443)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0006  max mem: 15572
[2025-01-17 05:30:22,025] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:30:22,025] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:30:22,075] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:30:22,075] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:30:23,221] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53492
[2025-01-17 05:30:23,222] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:30:23,222] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53492
[2025-01-17 05:30:23,222] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:30:23,222] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 140/1404]  eta: 0:13:26  lr: 0.000001  min_lr: 0.000000  loss: 4.2019 (4.1350)  class_acc: 0.3333 (0.3644)  loss_scale: 32768.0000 (31722.2128)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.0006  max mem: 15572
Epoch: [38]  [ 150/1404]  eta: 0:13:14  lr: 0.000001  min_lr: 0.000000  loss: 4.0558 (4.1350)  class_acc: 0.2500 (0.3634)  loss_scale: 32768.0000 (31791.4702)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0033  max mem: 15572
Epoch: [38]  [ 160/1404]  eta: 0:13:04  lr: 0.000001  min_lr: 0.000000  loss: 4.0432 (4.1317)  class_acc: 0.3750 (0.3641)  loss_scale: 32768.0000 (31852.1242)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0034  max mem: 15572
Epoch: [38]  [ 170/1404]  eta: 0:12:56  lr: 0.000001  min_lr: 0.000000  loss: 4.0892 (4.1325)  class_acc: 0.4167 (0.3667)  loss_scale: 32768.0000 (31905.6842)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0011  max mem: 15572
Epoch: [38]  [ 180/1404]  eta: 0:12:44  lr: 0.000001  min_lr: 0.000000  loss: 4.0915 (4.1343)  class_acc: 0.3750 (0.3649)  loss_scale: 32768.0000 (31953.3260)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0009  max mem: 15572
Epoch: [38]  [ 190/1404]  eta: 0:12:33  lr: 0.000001  min_lr: 0.000000  loss: 4.0783 (4.1329)  class_acc: 0.3750 (0.3641)  loss_scale: 32768.0000 (31995.9791)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0006  max mem: 15572
Epoch: [38]  [ 200/1404]  eta: 0:12:24  lr: 0.000001  min_lr: 0.000000  loss: 4.0826 (4.1343)  class_acc: 0.3750 (0.3642)  loss_scale: 32768.0000 (32034.3881)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0010  max mem: 15572
Epoch: [38]  [ 210/1404]  eta: 0:12:18  lr: 0.000001  min_lr: 0.000000  loss: 4.1159 (4.1300)  class_acc: 0.3333 (0.3633)  loss_scale: 32768.0000 (32069.1564)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0013  max mem: 15572
Epoch: [38]  [ 220/1404]  eta: 0:12:10  lr: 0.000001  min_lr: 0.000000  loss: 3.9759 (4.1265)  class_acc: 0.3333 (0.3635)  loss_scale: 32768.0000 (32100.7783)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0030  max mem: 15572
Epoch: [38]  [ 230/1404]  eta: 0:12:01  lr: 0.000001  min_lr: 0.000000  loss: 4.0582 (4.1237)  class_acc: 0.3333 (0.3622)  loss_scale: 32768.0000 (32129.6623)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0027  max mem: 15572
Epoch: [38]  [ 240/1404]  eta: 0:11:56  lr: 0.000001  min_lr: 0.000000  loss: 4.1987 (4.1299)  class_acc: 0.3333 (0.3598)  loss_scale: 32768.0000 (32156.1494)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0007  max mem: 15572
Epoch: [38]  [ 250/1404]  eta: 0:11:48  lr: 0.000001  min_lr: 0.000000  loss: 4.1987 (4.1267)  class_acc: 0.3333 (0.3606)  loss_scale: 32768.0000 (32180.5259)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0007  max mem: 15572
Epoch: [38]  [ 260/1404]  eta: 0:11:39  lr: 0.000001  min_lr: 0.000000  loss: 4.1042 (4.1276)  class_acc: 0.4167 (0.3605)  loss_scale: 32768.0000 (32203.0345)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0007  max mem: 15572
[2025-01-17 05:31:37,473] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:31:37,473] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:31:37,474] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:31:37,474] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:31:39,198] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53622
[2025-01-17 05:31:39,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:31:39,199] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53622
[2025-01-17 05:31:39,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:31:39,200] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 270/1404]  eta: 0:11:34  lr: 0.000001  min_lr: 0.000000  loss: 3.9941 (4.1159)  class_acc: 0.4167 (0.3621)  loss_scale: 32768.0000 (32344.7970)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0006  max mem: 15572
Epoch: [38]  [ 280/1404]  eta: 0:11:26  lr: 0.000001  min_lr: 0.000000  loss: 3.8852 (4.1112)  class_acc: 0.4583 (0.3636)  loss_scale: 32768.0000 (32359.8577)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0006  max mem: 15572
Epoch: [38]  [ 290/1404]  eta: 0:11:18  lr: 0.000001  min_lr: 0.000000  loss: 3.9489 (4.1074)  class_acc: 0.3750 (0.3653)  loss_scale: 32768.0000 (32373.8832)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0008  max mem: 15572
Epoch: [38]  [ 300/1404]  eta: 0:11:10  lr: 0.000001  min_lr: 0.000000  loss: 4.0069 (4.1121)  class_acc: 0.2917 (0.3638)  loss_scale: 32768.0000 (32386.9767)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0008  max mem: 15572
[2025-01-17 05:32:00,469] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53661
[2025-01-17 05:32:00,470] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:32:00,478] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53661
[2025-01-17 05:32:00,478] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:32:00,478] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [38]  [ 310/1404]  eta: 0:11:05  lr: 0.000001  min_lr: 0.000000  loss: 4.0587 (4.1044)  class_acc: 0.3333 (0.3658)  loss_scale: 32768.0000 (32293.8650)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0007  max mem: 15572
Epoch: [38]  [ 320/1404]  eta: 0:10:56  lr: 0.000001  min_lr: 0.000000  loss: 4.0031 (4.1004)  class_acc: 0.4583 (0.3690)  loss_scale: 16384.0000 (31798.2305)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0008  max mem: 15572
Epoch: [38]  [ 330/1404]  eta: 0:10:51  lr: 0.000001  min_lr: 0.000000  loss: 4.0968 (4.0983)  class_acc: 0.4583 (0.3707)  loss_scale: 16384.0000 (31332.5438)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0029  max mem: 15572
Epoch: [38]  [ 340/1404]  eta: 0:10:42  lr: 0.000001  min_lr: 0.000000  loss: 4.1183 (4.1006)  class_acc: 0.3750 (0.3696)  loss_scale: 16384.0000 (30894.1701)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0030  max mem: 15572
Epoch: [38]  [ 350/1404]  eta: 0:10:34  lr: 0.000001  min_lr: 0.000000  loss: 4.1429 (4.1024)  class_acc: 0.3333 (0.3704)  loss_scale: 16384.0000 (30480.7749)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0008  max mem: 15572
Epoch: [38]  [ 360/1404]  eta: 0:10:28  lr: 0.000001  min_lr: 0.000000  loss: 4.1324 (4.1025)  class_acc: 0.3750 (0.3704)  loss_scale: 16384.0000 (30090.2825)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0007  max mem: 15572
Epoch: [38]  [ 370/1404]  eta: 0:10:23  lr: 0.000001  min_lr: 0.000000  loss: 4.0790 (4.1036)  class_acc: 0.3750 (0.3705)  loss_scale: 16384.0000 (29720.8410)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.0006  max mem: 15572
Epoch: [38]  [ 380/1404]  eta: 0:10:16  lr: 0.000001  min_lr: 0.000000  loss: 4.0518 (4.1032)  class_acc: 0.3333 (0.3692)  loss_scale: 16384.0000 (29370.7927)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0005  max mem: 15572
Epoch: [38]  [ 390/1404]  eta: 0:10:09  lr: 0.000001  min_lr: 0.000000  loss: 4.0527 (4.1051)  class_acc: 0.3333 (0.3682)  loss_scale: 16384.0000 (29038.6496)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0006  max mem: 15572
Epoch: [38]  [ 400/1404]  eta: 0:10:04  lr: 0.000001  min_lr: 0.000000  loss: 4.1878 (4.1039)  class_acc: 0.3333 (0.3686)  loss_scale: 16384.0000 (28723.0723)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.0010  max mem: 15572
Epoch: [38]  [ 410/1404]  eta: 0:09:56  lr: 0.000001  min_lr: 0.000000  loss: 4.1308 (4.1057)  class_acc: 0.3333 (0.3677)  loss_scale: 16384.0000 (28422.8516)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0010  max mem: 15572
Epoch: [38]  [ 420/1404]  eta: 0:09:49  lr: 0.000001  min_lr: 0.000000  loss: 4.0865 (4.1042)  class_acc: 0.3333 (0.3677)  loss_scale: 16384.0000 (28136.8931)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0007  max mem: 15572
Epoch: [38]  [ 430/1404]  eta: 0:09:43  lr: 0.000001  min_lr: 0.000000  loss: 3.9545 (4.0989)  class_acc: 0.3333 (0.3676)  loss_scale: 16384.0000 (27864.2042)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0006  max mem: 15572
[2025-01-17 05:33:15,208] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:33:15,208] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:33:15,211] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:33:15,211] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [38]  [ 440/1404]  eta: 0:09:36  lr: 0.000001  min_lr: 0.000000  loss: 3.9386 (4.0970)  class_acc: 0.3333 (0.3676)  loss_scale: 16384.0000 (27715.3379)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0009  max mem: 15572
Epoch: [38]  [ 450/1404]  eta: 0:09:30  lr: 0.000001  min_lr: 0.000000  loss: 4.0289 (4.0983)  class_acc: 0.3333 (0.3667)  loss_scale: 32768.0000 (27827.3703)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0010  max mem: 15572
Epoch: [38]  [ 460/1404]  eta: 0:09:23  lr: 0.000001  min_lr: 0.000000  loss: 4.1068 (4.0974)  class_acc: 0.2917 (0.3661)  loss_scale: 32768.0000 (27934.5423)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0008  max mem: 15572
Epoch: [38]  [ 470/1404]  eta: 0:09:15  lr: 0.000001  min_lr: 0.000000  loss: 4.0871 (4.0972)  class_acc: 0.3333 (0.3665)  loss_scale: 32768.0000 (28037.1635)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0007  max mem: 15572
Epoch: [38]  [ 480/1404]  eta: 0:09:08  lr: 0.000001  min_lr: 0.000000  loss: 4.1246 (4.0975)  class_acc: 0.3333 (0.3663)  loss_scale: 32768.0000 (28135.5177)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0005  max mem: 15572
Epoch: [38]  [ 490/1404]  eta: 0:09:04  lr: 0.000001  min_lr: 0.000000  loss: 4.2904 (4.1015)  class_acc: 0.3333 (0.3651)  loss_scale: 32768.0000 (28229.8656)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0146  max mem: 15572
Epoch: [38]  [ 500/1404]  eta: 0:08:59  lr: 0.000001  min_lr: 0.000000  loss: 4.2682 (4.1024)  class_acc: 0.3333 (0.3652)  loss_scale: 32768.0000 (28320.4471)  weight_decay: 0.0500 (0.0500)  time: 0.6749  data: 0.0147  max mem: 15572
Epoch: [38]  [ 510/1404]  eta: 0:08:54  lr: 0.000001  min_lr: 0.000000  loss: 4.2618 (4.1027)  class_acc: 0.3750 (0.3659)  loss_scale: 32768.0000 (28407.4834)  weight_decay: 0.0500 (0.0500)  time: 0.6381  data: 0.0006  max mem: 15572
Epoch: [38]  [ 520/1404]  eta: 0:08:48  lr: 0.000001  min_lr: 0.000000  loss: 4.1860 (4.1027)  class_acc: 0.3750 (0.3660)  loss_scale: 32768.0000 (28491.1785)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.0008  max mem: 15572
Epoch: [38]  [ 530/1404]  eta: 0:08:43  lr: 0.000001  min_lr: 0.000000  loss: 4.1108 (4.1033)  class_acc: 0.2917 (0.3643)  loss_scale: 32768.0000 (28571.7213)  weight_decay: 0.0500 (0.0500)  time: 0.6509  data: 0.0007  max mem: 15572
Epoch: [38]  [ 540/1404]  eta: 0:08:37  lr: 0.000001  min_lr: 0.000000  loss: 4.1108 (4.1043)  class_acc: 0.2500 (0.3638)  loss_scale: 32768.0000 (28649.2865)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0007  max mem: 15572
Epoch: [38]  [ 550/1404]  eta: 0:08:31  lr: 0.000001  min_lr: 0.000000  loss: 4.0853 (4.1028)  class_acc: 0.3750 (0.3643)  loss_scale: 32768.0000 (28724.0363)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.0008  max mem: 15572
Epoch: [38]  [ 560/1404]  eta: 0:08:25  lr: 0.000001  min_lr: 0.000000  loss: 4.0622 (4.1031)  class_acc: 0.3750 (0.3650)  loss_scale: 32768.0000 (28796.1212)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0007  max mem: 15572
[2025-01-17 05:34:32,467] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:34:32,467] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:34:32,505] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:34:32,505] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:34:33,497] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53920
[2025-01-17 05:34:33,498] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:34:33,498] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 05:34:33,533] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53920
[2025-01-17 05:34:33,533] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [38]  [ 570/1404]  eta: 0:08:19  lr: 0.000001  min_lr: 0.000000  loss: 4.0944 (4.1035)  class_acc: 0.3750 (0.3664)  loss_scale: 32768.0000 (28980.4553)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.0010  max mem: 15572
Epoch: [38]  [ 580/1404]  eta: 0:08:13  lr: 0.000001  min_lr: 0.000000  loss: 4.1153 (4.1043)  class_acc: 0.3333 (0.3658)  loss_scale: 32768.0000 (29045.6454)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.0009  max mem: 15572
Epoch: [38]  [ 590/1404]  eta: 0:08:08  lr: 0.000001  min_lr: 0.000000  loss: 4.1544 (4.1073)  class_acc: 0.2917 (0.3641)  loss_scale: 32768.0000 (29108.6294)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0007  max mem: 15572
Epoch: [38]  [ 600/1404]  eta: 0:08:01  lr: 0.000001  min_lr: 0.000000  loss: 4.2120 (4.1091)  class_acc: 0.2917 (0.3645)  loss_scale: 32768.0000 (29169.5175)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0008  max mem: 15572
Epoch: [38]  [ 610/1404]  eta: 0:07:55  lr: 0.000001  min_lr: 0.000000  loss: 4.1604 (4.1080)  class_acc: 0.3333 (0.3646)  loss_scale: 32768.0000 (29228.4124)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0013  max mem: 15572
Epoch: [38]  [ 620/1404]  eta: 0:07:48  lr: 0.000001  min_lr: 0.000000  loss: 4.0124 (4.1072)  class_acc: 0.3333 (0.3645)  loss_scale: 32768.0000 (29285.4106)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0011  max mem: 15572
Epoch: [38]  [ 630/1404]  eta: 0:07:42  lr: 0.000001  min_lr: 0.000000  loss: 4.0577 (4.1070)  class_acc: 0.3750 (0.3655)  loss_scale: 32768.0000 (29340.6022)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0004  max mem: 15572
Epoch: [38]  [ 640/1404]  eta: 0:07:36  lr: 0.000001  min_lr: 0.000000  loss: 4.1482 (4.1069)  class_acc: 0.3750 (0.3640)  loss_scale: 32768.0000 (29394.0718)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0005  max mem: 15572
[2025-01-17 05:35:20,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=54000, skipped=320, lr=[5.231475218245322e-09, 5.231475218245322e-09, 7.473536026064748e-09, 7.473536026064748e-09, 1.0676480037235355e-08, 1.0676480037235355e-08, 1.525211433890765e-08, 1.525211433890765e-08, 2.1788734769868072e-08, 2.1788734769868072e-08, 3.112676395695439e-08, 3.112676395695439e-08, 4.4466805652791985e-08, 4.4466805652791985e-08, 6.352400807541714e-08, 6.352400807541714e-08, 9.074858296488161e-08, 9.074858296488161e-08, 1.2964083280697376e-07, 1.2964083280697376e-07, 1.852011897242482e-07, 1.852011897242482e-07, 2.6457312817749746e-07, 2.6457312817749746e-07, 3.779616116821393e-07, 3.779616116821393e-07, 5.399451595459133e-07, 5.399451595459133e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 05:35:20,315] [INFO] [timer.py:260:stop] epoch=0/micro_step=54000/global_step=54000, RunningAvgSamplesPerSec=48.12181881215986, CurrSamplesPerSec=47.31596497247905, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [38]  [ 650/1404]  eta: 0:07:29  lr: 0.000001  min_lr: 0.000000  loss: 4.0621 (4.1073)  class_acc: 0.3750 (0.3641)  loss_scale: 32768.0000 (29445.8986)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0007  max mem: 15572
[2025-01-17 05:35:25,887] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54010
[2025-01-17 05:35:25,888] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:35:25,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54010
[2025-01-17 05:35:25,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:35:25,903] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [38]  [ 660/1404]  eta: 0:07:24  lr: 0.000001  min_lr: 0.000000  loss: 4.2673 (4.1100)  class_acc: 0.3750 (0.3644)  loss_scale: 32768.0000 (29421.7973)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.0009  max mem: 15572
Epoch: [38]  [ 670/1404]  eta: 0:07:18  lr: 0.000001  min_lr: 0.000000  loss: 4.2425 (4.1077)  class_acc: 0.3750 (0.3646)  loss_scale: 16384.0000 (29227.4933)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0008  max mem: 15572
Epoch: [38]  [ 680/1404]  eta: 0:07:12  lr: 0.000001  min_lr: 0.000000  loss: 4.1712 (4.1098)  class_acc: 0.2917 (0.3626)  loss_scale: 16384.0000 (29038.8957)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.0007  max mem: 15572
Epoch: [38]  [ 690/1404]  eta: 0:07:05  lr: 0.000001  min_lr: 0.000000  loss: 4.2023 (4.1101)  class_acc: 0.2917 (0.3622)  loss_scale: 16384.0000 (28855.7569)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.0006  max mem: 15572
Epoch: [38]  [ 700/1404]  eta: 0:06:59  lr: 0.000001  min_lr: 0.000000  loss: 4.0341 (4.1082)  class_acc: 0.2917 (0.3618)  loss_scale: 16384.0000 (28677.8431)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0323  max mem: 15572
Epoch: [38]  [ 710/1404]  eta: 0:06:53  lr: 0.000001  min_lr: 0.000000  loss: 4.0384 (4.1089)  class_acc: 0.2917 (0.3612)  loss_scale: 16384.0000 (28504.9339)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0381  max mem: 15572
Epoch: [38]  [ 720/1404]  eta: 0:06:47  lr: 0.000001  min_lr: 0.000000  loss: 4.0808 (4.1092)  class_acc: 0.3750 (0.3618)  loss_scale: 16384.0000 (28336.8211)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0413  max mem: 15572
Epoch: [38]  [ 730/1404]  eta: 0:06:41  lr: 0.000001  min_lr: 0.000000  loss: 4.0808 (4.1097)  class_acc: 0.3333 (0.3609)  loss_scale: 16384.0000 (28173.3078)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.0359  max mem: 15572
Epoch: [38]  [ 740/1404]  eta: 0:06:34  lr: 0.000001  min_lr: 0.000000  loss: 4.1476 (4.1102)  class_acc: 0.2917 (0.3604)  loss_scale: 16384.0000 (28014.2078)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0010  max mem: 15572
Epoch: [38]  [ 750/1404]  eta: 0:06:28  lr: 0.000000  min_lr: 0.000000  loss: 4.1515 (4.1096)  class_acc: 0.3750 (0.3611)  loss_scale: 16384.0000 (27859.3449)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0433  max mem: 15572
Epoch: [38]  [ 760/1404]  eta: 0:06:23  lr: 0.000000  min_lr: 0.000000  loss: 4.0574 (4.1087)  class_acc: 0.3750 (0.3606)  loss_scale: 16384.0000 (27708.5519)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1171  max mem: 15572
Epoch: [38]  [ 770/1404]  eta: 0:06:16  lr: 0.000000  min_lr: 0.000000  loss: 3.9692 (4.1049)  class_acc: 0.3750 (0.3614)  loss_scale: 16384.0000 (27561.6706)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0810  max mem: 15572
Epoch: [38]  [ 780/1404]  eta: 0:06:11  lr: 0.000000  min_lr: 0.000000  loss: 3.8575 (4.1033)  class_acc: 0.3750 (0.3624)  loss_scale: 16384.0000 (27418.5506)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0165  max mem: 15572
[2025-01-17 05:36:41,875] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:36:41,875] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:36:41,943] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:36:41,943] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [38]  [ 790/1404]  eta: 0:06:04  lr: 0.000000  min_lr: 0.000000  loss: 4.0121 (4.1028)  class_acc: 0.2917 (0.3615)  loss_scale: 16384.0000 (27361.9014)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0100  max mem: 15572
Epoch: [38]  [ 800/1404]  eta: 0:05:58  lr: 0.000000  min_lr: 0.000000  loss: 4.0222 (4.1027)  class_acc: 0.3750 (0.3615)  loss_scale: 32768.0000 (27429.3933)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0008  max mem: 15572
Epoch: [38]  [ 810/1404]  eta: 0:05:52  lr: 0.000000  min_lr: 0.000000  loss: 4.0928 (4.1034)  class_acc: 0.3750 (0.3614)  loss_scale: 32768.0000 (27495.2207)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0460  max mem: 15572
Epoch: [38]  [ 820/1404]  eta: 0:05:46  lr: 0.000000  min_lr: 0.000000  loss: 4.1331 (4.1041)  class_acc: 0.3750 (0.3617)  loss_scale: 32768.0000 (27559.4446)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0690  max mem: 15572
Epoch: [38]  [ 830/1404]  eta: 0:05:41  lr: 0.000000  min_lr: 0.000000  loss: 4.1393 (4.1036)  class_acc: 0.4167 (0.3618)  loss_scale: 32768.0000 (27622.1227)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1396  max mem: 15572
Epoch: [38]  [ 840/1404]  eta: 0:05:34  lr: 0.000000  min_lr: 0.000000  loss: 4.2097 (4.1051)  class_acc: 0.3750 (0.3617)  loss_scale: 32768.0000 (27683.3103)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1250  max mem: 15572
[2025-01-17 05:37:16,016] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54199
[2025-01-17 05:37:16,017] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:37:16,074] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54199
[2025-01-17 05:37:16,075] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:37:16,075] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [38]  [ 850/1404]  eta: 0:05:28  lr: 0.000000  min_lr: 0.000000  loss: 4.2222 (4.1058)  class_acc: 0.3333 (0.3620)  loss_scale: 32768.0000 (27666.0494)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0385  max mem: 15572
Epoch: [38]  [ 860/1404]  eta: 0:05:21  lr: 0.000000  min_lr: 0.000000  loss: 4.1089 (4.1043)  class_acc: 0.3750 (0.3624)  loss_scale: 16384.0000 (27535.0151)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0300  max mem: 15572
Epoch: [38]  [ 870/1404]  eta: 0:05:16  lr: 0.000000  min_lr: 0.000000  loss: 4.1825 (4.1068)  class_acc: 0.3333 (0.3621)  loss_scale: 16384.0000 (27406.9897)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0693  max mem: 15572
Epoch: [38]  [ 880/1404]  eta: 0:05:10  lr: 0.000000  min_lr: 0.000000  loss: 4.2176 (4.1074)  class_acc: 0.3333 (0.3623)  loss_scale: 16384.0000 (27281.8706)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0910  max mem: 15572
Epoch: [38]  [ 890/1404]  eta: 0:05:04  lr: 0.000000  min_lr: 0.000000  loss: 4.1704 (4.1077)  class_acc: 0.3333 (0.3619)  loss_scale: 16384.0000 (27159.5600)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0916  max mem: 15572
Epoch: [38]  [ 900/1404]  eta: 0:04:58  lr: 0.000000  min_lr: 0.000000  loss: 4.1997 (4.1094)  class_acc: 0.2917 (0.3614)  loss_scale: 16384.0000 (27039.9645)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1187  max mem: 15572
Epoch: [38]  [ 910/1404]  eta: 0:04:52  lr: 0.000000  min_lr: 0.000000  loss: 4.1997 (4.1097)  class_acc: 0.2917 (0.3611)  loss_scale: 16384.0000 (26922.9945)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0800  max mem: 15572
Epoch: [38]  [ 920/1404]  eta: 0:04:46  lr: 0.000000  min_lr: 0.000000  loss: 4.1674 (4.1094)  class_acc: 0.2917 (0.3600)  loss_scale: 16384.0000 (26808.5646)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1149  max mem: 15572
Epoch: [38]  [ 930/1404]  eta: 0:04:40  lr: 0.000000  min_lr: 0.000000  loss: 4.1590 (4.1100)  class_acc: 0.2917 (0.3600)  loss_scale: 16384.0000 (26696.5929)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1516  max mem: 15572
Epoch: [38]  [ 940/1404]  eta: 0:04:34  lr: 0.000000  min_lr: 0.000000  loss: 4.1705 (4.1110)  class_acc: 0.2917 (0.3596)  loss_scale: 16384.0000 (26587.0011)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1022  max mem: 15572
Epoch: [38]  [ 950/1404]  eta: 0:04:28  lr: 0.000000  min_lr: 0.000000  loss: 4.1711 (4.1121)  class_acc: 0.3333 (0.3601)  loss_scale: 16384.0000 (26479.7140)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1090  max mem: 15572
Epoch: [38]  [ 960/1404]  eta: 0:04:23  lr: 0.000000  min_lr: 0.000000  loss: 4.1767 (4.1122)  class_acc: 0.3750 (0.3596)  loss_scale: 16384.0000 (26374.6597)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1436  max mem: 15572
Epoch: [38]  [ 970/1404]  eta: 0:04:17  lr: 0.000000  min_lr: 0.000000  loss: 4.1181 (4.1107)  class_acc: 0.2917 (0.3593)  loss_scale: 16384.0000 (26271.7693)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1515  max mem: 15572
[2025-01-17 05:38:32,573] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:38:32,574] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:38:32,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:38:32,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [38]  [ 980/1404]  eta: 0:04:11  lr: 0.000000  min_lr: 0.000000  loss: 4.1222 (4.1114)  class_acc: 0.3333 (0.3594)  loss_scale: 16384.0000 (26254.4832)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1052  max mem: 15572
Epoch: [38]  [ 990/1404]  eta: 0:04:05  lr: 0.000000  min_lr: 0.000000  loss: 4.1222 (4.1104)  class_acc: 0.3750 (0.3594)  loss_scale: 32768.0000 (26320.2099)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0640  max mem: 15572
Epoch: [38]  [1000/1404]  eta: 0:03:59  lr: 0.000000  min_lr: 0.000000  loss: 4.0479 (4.1088)  class_acc: 0.3333 (0.3593)  loss_scale: 32768.0000 (26384.6234)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0739  max mem: 15572
Epoch: [38]  [1010/1404]  eta: 0:03:53  lr: 0.000000  min_lr: 0.000000  loss: 4.1266 (4.1087)  class_acc: 0.3750 (0.3598)  loss_scale: 32768.0000 (26447.7626)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0418  max mem: 15572
Epoch: [38]  [1020/1404]  eta: 0:03:47  lr: 0.000000  min_lr: 0.000000  loss: 4.1266 (4.1075)  class_acc: 0.3750 (0.3605)  loss_scale: 32768.0000 (26509.6650)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0298  max mem: 15572
Epoch: [38]  [1030/1404]  eta: 0:03:41  lr: 0.000000  min_lr: 0.000000  loss: 4.2030 (4.1086)  class_acc: 0.3333 (0.3598)  loss_scale: 32768.0000 (26570.3666)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0605  max mem: 15572
Epoch: [38]  [1040/1404]  eta: 0:03:35  lr: 0.000000  min_lr: 0.000000  loss: 4.2262 (4.1086)  class_acc: 0.2917 (0.3595)  loss_scale: 32768.0000 (26629.9020)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.0839  max mem: 15572
Epoch: [38]  [1050/1404]  eta: 0:03:29  lr: 0.000000  min_lr: 0.000000  loss: 3.9978 (4.1076)  class_acc: 0.3750 (0.3608)  loss_scale: 32768.0000 (26688.3045)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0865  max mem: 15572
Epoch: [38]  [1060/1404]  eta: 0:03:23  lr: 0.000000  min_lr: 0.000000  loss: 3.9978 (4.1066)  class_acc: 0.4167 (0.3613)  loss_scale: 32768.0000 (26745.6060)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0944  max mem: 15572
Epoch: [38]  [1070/1404]  eta: 0:03:17  lr: 0.000000  min_lr: 0.000000  loss: 4.0030 (4.1059)  class_acc: 0.3750 (0.3615)  loss_scale: 32768.0000 (26801.8375)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0598  max mem: 15572
Epoch: [38]  [1080/1404]  eta: 0:03:11  lr: 0.000000  min_lr: 0.000000  loss: 4.0228 (4.1059)  class_acc: 0.3333 (0.3612)  loss_scale: 32768.0000 (26857.0287)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0400  max mem: 15572
Epoch: [38]  [1090/1404]  eta: 0:03:05  lr: 0.000000  min_lr: 0.000000  loss: 4.0228 (4.1058)  class_acc: 0.3333 (0.3609)  loss_scale: 32768.0000 (26911.2081)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0542  max mem: 15572
Epoch: [38]  [1100/1404]  eta: 0:02:59  lr: 0.000000  min_lr: 0.000000  loss: 3.9797 (4.1037)  class_acc: 0.3333 (0.3612)  loss_scale: 32768.0000 (26964.4033)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0223  max mem: 15572
[2025-01-17 05:39:45,927] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:39:45,927] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:39:45,928] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:39:45,928] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:39:46,960] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54458
[2025-01-17 05:39:46,960] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:39:46,960] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 05:39:47,011] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54458
[2025-01-17 05:39:47,011] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [38]  [1110/1404]  eta: 0:02:53  lr: 0.000000  min_lr: 0.000000  loss: 4.1129 (4.1045)  class_acc: 0.3333 (0.3610)  loss_scale: 32768.0000 (27075.6292)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.0793  max mem: 15572
Epoch: [38]  [1120/1404]  eta: 0:02:47  lr: 0.000000  min_lr: 0.000000  loss: 4.1288 (4.1040)  class_acc: 0.3333 (0.3611)  loss_scale: 32768.0000 (27126.4086)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1000  max mem: 15572
Epoch: [38]  [1130/1404]  eta: 0:02:42  lr: 0.000000  min_lr: 0.000000  loss: 4.1045 (4.1054)  class_acc: 0.3750 (0.3605)  loss_scale: 32768.0000 (27176.2900)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1033  max mem: 15572
Epoch: [38]  [1140/1404]  eta: 0:02:36  lr: 0.000000  min_lr: 0.000000  loss: 4.0324 (4.1040)  class_acc: 0.3750 (0.3611)  loss_scale: 32768.0000 (27225.2971)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1087  max mem: 15572
Epoch: [38]  [1150/1404]  eta: 0:02:30  lr: 0.000000  min_lr: 0.000000  loss: 4.0135 (4.1039)  class_acc: 0.3333 (0.3600)  loss_scale: 32768.0000 (27273.4526)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0909  max mem: 15572
Epoch: [38]  [1160/1404]  eta: 0:02:24  lr: 0.000000  min_lr: 0.000000  loss: 4.0611 (4.1029)  class_acc: 0.2917 (0.3602)  loss_scale: 32768.0000 (27320.7786)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1027  max mem: 15572
Epoch: [38]  [1170/1404]  eta: 0:02:18  lr: 0.000000  min_lr: 0.000000  loss: 4.0611 (4.1020)  class_acc: 0.3750 (0.3602)  loss_scale: 32768.0000 (27367.2963)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0681  max mem: 15572
Epoch: [38]  [1180/1404]  eta: 0:02:12  lr: 0.000000  min_lr: 0.000000  loss: 4.0745 (4.1012)  class_acc: 0.4167 (0.3604)  loss_scale: 32768.0000 (27413.0262)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0877  max mem: 15572
Epoch: [38]  [1190/1404]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 4.0745 (4.1014)  class_acc: 0.3333 (0.3602)  loss_scale: 32768.0000 (27457.9882)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1041  max mem: 15572
Epoch: [38]  [1200/1404]  eta: 0:02:00  lr: 0.000000  min_lr: 0.000000  loss: 4.0385 (4.1014)  class_acc: 0.2917 (0.3598)  loss_scale: 32768.0000 (27502.2015)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0747  max mem: 15572
Epoch: [38]  [1210/1404]  eta: 0:01:54  lr: 0.000000  min_lr: 0.000000  loss: 4.0399 (4.1010)  class_acc: 0.3333 (0.3600)  loss_scale: 32768.0000 (27545.6846)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0588  max mem: 15572
Epoch: [38]  [1220/1404]  eta: 0:01:48  lr: 0.000000  min_lr: 0.000000  loss: 4.0169 (4.1007)  class_acc: 0.3333 (0.3602)  loss_scale: 32768.0000 (27588.4554)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0731  max mem: 15572
Epoch: [38]  [1230/1404]  eta: 0:01:42  lr: 0.000000  min_lr: 0.000000  loss: 4.0169 (4.1001)  class_acc: 0.3750 (0.3603)  loss_scale: 32768.0000 (27630.5313)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0562  max mem: 15572
[2025-01-17 05:41:03,108] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:41:03,108] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:41:03,114] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:41:03,114] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:41:03,589] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54588
[2025-01-17 05:41:03,589] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:41:03,598] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54588
[2025-01-17 05:41:03,598] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:41:03,599] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [1240/1404]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 4.0054 (4.0991)  class_acc: 0.3750 (0.3607)  loss_scale: 32768.0000 (27698.3336)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0761  max mem: 15572
Epoch: [38]  [1250/1404]  eta: 0:01:30  lr: 0.000000  min_lr: 0.000000  loss: 4.0442 (4.0995)  class_acc: 0.3333 (0.3604)  loss_scale: 32768.0000 (27738.8585)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1224  max mem: 15572
Epoch: [38]  [1260/1404]  eta: 0:01:25  lr: 0.000000  min_lr: 0.000000  loss: 4.0881 (4.0976)  class_acc: 0.3333 (0.3606)  loss_scale: 32768.0000 (27778.7407)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1176  max mem: 15572
Epoch: [38]  [1270/1404]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 4.0429 (4.0979)  class_acc: 0.3750 (0.3612)  loss_scale: 32768.0000 (27817.9953)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.0615  max mem: 15572
Epoch: [38]  [1280/1404]  eta: 0:01:13  lr: 0.000000  min_lr: 0.000000  loss: 4.0912 (4.0972)  class_acc: 0.3750 (0.3611)  loss_scale: 32768.0000 (27856.6370)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0511  max mem: 15572
[2025-01-17 05:41:34,222] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54640
[2025-01-17 05:41:34,222] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:41:34,290] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54640
[2025-01-17 05:41:34,290] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:41:34,291] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [38]  [1290/1404]  eta: 0:01:07  lr: 0.000000  min_lr: 0.000000  loss: 4.1159 (4.0972)  class_acc: 0.2917 (0.3609)  loss_scale: 32768.0000 (27856.6073)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0769  max mem: 15572
Epoch: [38]  [1300/1404]  eta: 0:01:01  lr: 0.000000  min_lr: 0.000000  loss: 4.1556 (4.0979)  class_acc: 0.3333 (0.3613)  loss_scale: 16384.0000 (27768.4243)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0560  max mem: 15572
Epoch: [38]  [1310/1404]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 4.1437 (4.0972)  class_acc: 0.4583 (0.3619)  loss_scale: 16384.0000 (27681.5866)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0614  max mem: 15572
Epoch: [38]  [1320/1404]  eta: 0:00:49  lr: 0.000000  min_lr: 0.000000  loss: 4.1017 (4.0974)  class_acc: 0.4167 (0.3621)  loss_scale: 16384.0000 (27596.0636)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1093  max mem: 15572
Epoch: [38]  [1330/1404]  eta: 0:00:43  lr: 0.000000  min_lr: 0.000000  loss: 4.1017 (4.0970)  class_acc: 0.3333 (0.3618)  loss_scale: 16384.0000 (27511.8257)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0868  max mem: 15572
Epoch: [38]  [1340/1404]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 4.0913 (4.0979)  class_acc: 0.3333 (0.3620)  loss_scale: 16384.0000 (27428.8441)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0551  max mem: 15572
Epoch: [38]  [1350/1404]  eta: 0:00:31  lr: 0.000000  min_lr: 0.000000  loss: 4.0649 (4.0963)  class_acc: 0.3750 (0.3622)  loss_scale: 16384.0000 (27347.0910)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.0855  max mem: 15572
Epoch: [38]  [1360/1404]  eta: 0:00:25  lr: 0.000000  min_lr: 0.000000  loss: 3.9963 (4.0950)  class_acc: 0.3750 (0.3626)  loss_scale: 16384.0000 (27266.5393)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0623  max mem: 15572
Epoch: [38]  [1370/1404]  eta: 0:00:20  lr: 0.000000  min_lr: 0.000000  loss: 4.0172 (4.0955)  class_acc: 0.3333 (0.3626)  loss_scale: 16384.0000 (27187.1627)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0624  max mem: 15572
Epoch: [38]  [1380/1404]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 4.0541 (4.0960)  class_acc: 0.3333 (0.3626)  loss_scale: 16384.0000 (27108.9356)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0532  max mem: 15572
Epoch: [38]  [1390/1404]  eta: 0:00:08  lr: 0.000000  min_lr: 0.000000  loss: 3.9852 (4.0953)  class_acc: 0.3750 (0.3629)  loss_scale: 16384.0000 (27031.8332)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0107  max mem: 15572
Epoch: [38]  [1400/1404]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 3.9887 (4.0955)  class_acc: 0.3750 (0.3626)  loss_scale: 16384.0000 (26955.8315)  weight_decay: 0.0500 (0.0500)  time: 0.4468  data: 0.0004  max mem: 15572
Epoch: [38]  [1403/1404]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 4.0379 (4.0953)  class_acc: 0.3333 (0.3626)  loss_scale: 16384.0000 (26933.2422)  weight_decay: 0.0500 (0.0500)  time: 0.4265  data: 0.0004  max mem: 15572
Epoch: [38] Total time: 0:13:44 (0.5871 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 4.0379 (4.1043)  class_acc: 0.3333 (0.3642)  loss_scale: 16384.0000 (26933.2422)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:13:55  loss: 1.6901 (1.6901)  acc1: 61.1111 (61.1111)  acc5: 83.3333 (83.3333)  time: 6.1415  data: 5.9527  max mem: 15572
Val:  [ 10/136]  eta: 0:01:41  loss: 2.2786 (2.1790)  acc1: 61.1111 (56.0606)  acc5: 83.3333 (82.3232)  time: 0.8026  data: 0.6126  max mem: 15572
Val:  [ 20/136]  eta: 0:01:08  loss: 2.4257 (2.3573)  acc1: 44.4444 (48.6772)  acc5: 77.7778 (78.5714)  time: 0.3090  data: 0.0946  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 2.3151 (2.2862)  acc1: 38.8889 (48.7455)  acc5: 83.3333 (79.7491)  time: 0.3380  data: 0.1215  max mem: 15572
Val:  [ 40/136]  eta: 0:00:45  loss: 2.0465 (2.2514)  acc1: 55.5556 (50.4065)  acc5: 83.3333 (80.4878)  time: 0.3598  data: 0.1671  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.1140 (2.2443)  acc1: 55.5556 (50.6536)  acc5: 83.3333 (81.0458)  time: 0.3383  data: 0.1437  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.2088 (2.3122)  acc1: 44.4444 (48.2696)  acc5: 83.3333 (80.0546)  time: 0.3209  data: 0.1228  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.1965 (2.2908)  acc1: 50.0000 (49.0610)  acc5: 77.7778 (80.4382)  time: 0.3510  data: 0.1516  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.1063 (2.2823)  acc1: 50.0000 (48.9026)  acc5: 88.8889 (80.7956)  time: 0.3739  data: 0.1734  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.2062 (2.2905)  acc1: 44.4444 (48.4737)  acc5: 77.7778 (80.4029)  time: 0.3689  data: 0.1693  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5230 (2.3477)  acc1: 38.8889 (46.5897)  acc5: 77.7778 (78.9329)  time: 0.3335  data: 0.1300  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.4684 (2.3393)  acc1: 38.8889 (46.8969)  acc5: 77.7778 (79.0290)  time: 0.3505  data: 0.1440  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1443 (2.3023)  acc1: 55.5556 (47.9798)  acc5: 83.3333 (79.7980)  time: 0.3686  data: 0.1685  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8506 (2.2731)  acc1: 55.5556 (48.6853)  acc5: 88.8889 (80.4495)  time: 0.2615  data: 0.0870  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.9715 (2.2673)  acc1: 50.0000 (49.0991)  acc5: 88.8889 (80.6306)  time: 0.1828  data: 0.0272  max mem: 15572
Val: Total time: 0:00:49 (0.3661 s / it)
* Acc@1 47.912 Acc@5 79.197 loss 2.315
Accuracy of the network on the 4883 val videos: 47.9%
Max accuracy: 47.99%
Epoch: [39]  [   0/1404]  eta: 3:14:51  lr: 0.000000  min_lr: 0.000000  loss: 4.1357 (4.1357)  class_acc: 0.5000 (0.5000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 8.3274  data: 7.9124  max mem: 15572
Epoch: [39]  [  10/1404]  eta: 0:30:05  lr: 0.000000  min_lr: 0.000000  loss: 4.0259 (3.9906)  class_acc: 0.3750 (0.3598)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2955  data: 0.8329  max mem: 15572
[2025-01-17 05:43:42,972] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:43:42,972] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:43:42,973] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:43:42,973] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [  20/1404]  eta: 0:21:52  lr: 0.000000  min_lr: 0.000000  loss: 4.0259 (3.9914)  class_acc: 0.2917 (0.3591)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1159  max mem: 15572
Epoch: [39]  [  30/1404]  eta: 0:19:04  lr: 0.000000  min_lr: 0.000000  loss: 4.0872 (4.0132)  class_acc: 0.2917 (0.3508)  loss_scale: 32768.0000 (25897.2903)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1077  max mem: 15572
Epoch: [39]  [  40/1404]  eta: 0:17:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1184 (4.0376)  class_acc: 0.3333 (0.3506)  loss_scale: 32768.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0965  max mem: 15572
Epoch: [39]  [  50/1404]  eta: 0:16:28  lr: 0.000000  min_lr: 0.000000  loss: 4.1455 (4.0405)  class_acc: 0.3750 (0.3587)  loss_scale: 32768.0000 (28591.6863)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0909  max mem: 15572
Epoch: [39]  [  60/1404]  eta: 0:16:03  lr: 0.000000  min_lr: 0.000000  loss: 4.1271 (4.0746)  class_acc: 0.3750 (0.3634)  loss_scale: 32768.0000 (29276.3279)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1327  max mem: 15572
Epoch: [39]  [  70/1404]  eta: 0:15:22  lr: 0.000000  min_lr: 0.000000  loss: 4.0322 (4.0623)  class_acc: 0.3750 (0.3697)  loss_scale: 32768.0000 (29768.1127)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1087  max mem: 15572
Epoch: [39]  [  80/1404]  eta: 0:14:56  lr: 0.000000  min_lr: 0.000000  loss: 4.0322 (4.0686)  class_acc: 0.3333 (0.3611)  loss_scale: 32768.0000 (30138.4691)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0525  max mem: 15572
Epoch: [39]  [  90/1404]  eta: 0:14:37  lr: 0.000000  min_lr: 0.000000  loss: 4.1104 (4.0928)  class_acc: 0.2917 (0.3608)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0759  max mem: 15572
Epoch: [39]  [ 100/1404]  eta: 0:14:19  lr: 0.000000  min_lr: 0.000000  loss: 4.1721 (4.0993)  class_acc: 0.3750 (0.3601)  loss_scale: 32768.0000 (30659.1683)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.0619  max mem: 15572
Epoch: [39]  [ 110/1404]  eta: 0:14:02  lr: 0.000000  min_lr: 0.000000  loss: 4.1236 (4.0943)  class_acc: 0.3750 (0.3619)  loss_scale: 32768.0000 (30849.1532)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0435  max mem: 15572
Epoch: [39]  [ 120/1404]  eta: 0:13:52  lr: 0.000000  min_lr: 0.000000  loss: 4.1515 (4.1098)  class_acc: 0.3750 (0.3598)  loss_scale: 32768.0000 (31007.7355)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0944  max mem: 15572
Epoch: [39]  [ 130/1404]  eta: 0:13:31  lr: 0.000000  min_lr: 0.000000  loss: 4.1859 (4.1109)  class_acc: 0.2917 (0.3543)  loss_scale: 32768.0000 (31142.1069)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0651  max mem: 15572
Epoch: [39]  [ 140/1404]  eta: 0:13:15  lr: 0.000000  min_lr: 0.000000  loss: 4.1756 (4.1180)  class_acc: 0.2500 (0.3475)  loss_scale: 32768.0000 (31257.4184)  weight_decay: 0.0500 (0.0500)  time: 0.5133  data: 0.0109  max mem: 15572
[2025-01-17 05:44:56,549] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:44:56,550] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:44:56,697] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:44:56,697] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:44:57,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54898
[2025-01-17 05:44:57,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:44:57,182] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54898
[2025-01-17 05:44:57,183] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:44:57,183] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 05:44:59,128] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54902
[2025-01-17 05:44:59,128] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:44:59,128] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 05:44:59,133] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54902
[2025-01-17 05:44:59,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [39]  [ 150/1404]  eta: 0:12:57  lr: 0.000000  min_lr: 0.000000  loss: 4.1328 (4.1217)  class_acc: 0.2917 (0.3469)  loss_scale: 32768.0000 (31031.9470)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0110  max mem: 15572
Epoch: [39]  [ 160/1404]  eta: 0:12:52  lr: 0.000000  min_lr: 0.000000  loss: 4.1302 (4.1238)  class_acc: 0.3333 (0.3476)  loss_scale: 16384.0000 (30122.1366)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0135  max mem: 15572
Epoch: [39]  [ 170/1404]  eta: 0:12:41  lr: 0.000000  min_lr: 0.000000  loss: 4.1181 (4.1181)  class_acc: 0.3750 (0.3499)  loss_scale: 16384.0000 (29318.7368)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0427  max mem: 15572
Epoch: [39]  [ 180/1404]  eta: 0:12:39  lr: 0.000000  min_lr: 0.000000  loss: 4.0289 (4.1198)  class_acc: 0.3333 (0.3467)  loss_scale: 16384.0000 (28604.1105)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1219  max mem: 15572
Epoch: [39]  [ 190/1404]  eta: 0:12:31  lr: 0.000000  min_lr: 0.000000  loss: 4.0604 (4.1200)  class_acc: 0.4167 (0.3530)  loss_scale: 16384.0000 (27964.3141)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1514  max mem: 15572
Epoch: [39]  [ 200/1404]  eta: 0:12:24  lr: 0.000000  min_lr: 0.000000  loss: 4.1760 (4.1206)  class_acc: 0.4167 (0.3539)  loss_scale: 16384.0000 (27388.1791)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1244  max mem: 15572
Epoch: [39]  [ 210/1404]  eta: 0:12:16  lr: 0.000000  min_lr: 0.000000  loss: 4.0804 (4.1174)  class_acc: 0.3750 (0.3560)  loss_scale: 16384.0000 (26866.6540)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1078  max mem: 15572
Epoch: [39]  [ 220/1404]  eta: 0:12:05  lr: 0.000000  min_lr: 0.000000  loss: 4.0282 (4.1185)  class_acc: 0.3333 (0.3565)  loss_scale: 16384.0000 (26392.3258)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0549  max mem: 15572
Epoch: [39]  [ 230/1404]  eta: 0:11:59  lr: 0.000000  min_lr: 0.000000  loss: 4.0468 (4.1230)  class_acc: 0.3333 (0.3552)  loss_scale: 16384.0000 (25959.0649)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0383  max mem: 15572
Epoch: [39]  [ 240/1404]  eta: 0:11:51  lr: 0.000000  min_lr: 0.000000  loss: 4.0469 (4.1139)  class_acc: 0.3333 (0.3565)  loss_scale: 16384.0000 (25561.7593)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0261  max mem: 15572
[2025-01-17 05:45:56,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=55000, skipped=327, lr=[2.157574173979932e-09, 2.157574173979932e-09, 3.082248819971332e-09, 3.082248819971332e-09, 4.403212599959046e-09, 4.403212599959046e-09, 6.290303714227209e-09, 6.290303714227209e-09, 8.986148163181727e-09, 8.986148163181727e-09, 1.283735451883104e-08, 1.283735451883104e-08, 1.8339077884044344e-08, 1.8339077884044344e-08, 2.6198682691491923e-08, 2.6198682691491923e-08, 3.7426689559274175e-08, 3.7426689559274175e-08, 5.3466699370391684e-08, 5.3466699370391684e-08, 7.638099910055955e-08, 7.638099910055955e-08, 1.0911571300079936e-07, 1.0911571300079936e-07, 1.5587959000114197e-07, 1.5587959000114197e-07, 2.2268512857305995e-07, 2.2268512857305995e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 05:45:56,176] [INFO] [timer.py:260:stop] epoch=0/micro_step=55000/global_step=55000, RunningAvgSamplesPerSec=48.15759632195475, CurrSamplesPerSec=54.813799818019355, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [39]  [ 250/1404]  eta: 0:11:43  lr: 0.000000  min_lr: 0.000000  loss: 4.1170 (4.1201)  class_acc: 0.3333 (0.3549)  loss_scale: 16384.0000 (25196.1116)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0005  max mem: 15572
Epoch: [39]  [ 260/1404]  eta: 0:11:34  lr: 0.000000  min_lr: 0.000000  loss: 4.1978 (4.1202)  class_acc: 0.3333 (0.3534)  loss_scale: 16384.0000 (24858.4828)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0005  max mem: 15572
Epoch: [39]  [ 270/1404]  eta: 0:11:28  lr: 0.000000  min_lr: 0.000000  loss: 4.0218 (4.1179)  class_acc: 0.2917 (0.3527)  loss_scale: 16384.0000 (24545.7712)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0008  max mem: 15572
[2025-01-17 05:46:14,510] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:46:14,511] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:46:14,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:46:14,546] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [ 280/1404]  eta: 0:11:23  lr: 0.000000  min_lr: 0.000000  loss: 4.1682 (4.1231)  class_acc: 0.2917 (0.3536)  loss_scale: 16384.0000 (24605.1530)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.0008  max mem: 15572
Epoch: [39]  [ 290/1404]  eta: 0:11:14  lr: 0.000000  min_lr: 0.000000  loss: 4.0714 (4.1206)  class_acc: 0.2917 (0.3532)  loss_scale: 32768.0000 (24885.6632)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0007  max mem: 15572
Epoch: [39]  [ 300/1404]  eta: 0:11:07  lr: 0.000000  min_lr: 0.000000  loss: 4.0601 (4.1224)  class_acc: 0.2917 (0.3523)  loss_scale: 32768.0000 (25147.5349)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0005  max mem: 15572
Epoch: [39]  [ 310/1404]  eta: 0:11:04  lr: 0.000000  min_lr: 0.000000  loss: 4.0501 (4.1189)  class_acc: 0.4167 (0.3557)  loss_scale: 32768.0000 (25392.5659)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.0005  max mem: 15572
Epoch: [39]  [ 320/1404]  eta: 0:10:56  lr: 0.000000  min_lr: 0.000000  loss: 4.0305 (4.1197)  class_acc: 0.4167 (0.3550)  loss_scale: 32768.0000 (25622.3302)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.0006  max mem: 15572
Epoch: [39]  [ 330/1404]  eta: 0:10:49  lr: 0.000000  min_lr: 0.000000  loss: 4.1316 (4.1156)  class_acc: 0.3750 (0.3574)  loss_scale: 32768.0000 (25838.2115)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0007  max mem: 15572
Epoch: [39]  [ 340/1404]  eta: 0:10:43  lr: 0.000000  min_lr: 0.000000  loss: 4.1316 (4.1157)  class_acc: 0.4167 (0.3591)  loss_scale: 32768.0000 (26041.4311)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0007  max mem: 15572
Epoch: [39]  [ 350/1404]  eta: 0:10:37  lr: 0.000000  min_lr: 0.000000  loss: 4.1158 (4.1173)  class_acc: 0.3750 (0.3583)  loss_scale: 32768.0000 (26233.0712)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0006  max mem: 15572
Epoch: [39]  [ 360/1404]  eta: 0:10:29  lr: 0.000000  min_lr: 0.000000  loss: 4.0695 (4.1148)  class_acc: 0.4167 (0.3611)  loss_scale: 32768.0000 (26414.0942)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0006  max mem: 15572
Epoch: [39]  [ 370/1404]  eta: 0:10:25  lr: 0.000000  min_lr: 0.000000  loss: 4.0406 (4.1139)  class_acc: 0.3750 (0.3610)  loss_scale: 32768.0000 (26585.3585)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0007  max mem: 15572
Epoch: [39]  [ 380/1404]  eta: 0:10:16  lr: 0.000000  min_lr: 0.000000  loss: 4.0837 (4.1126)  class_acc: 0.3333 (0.3611)  loss_scale: 32768.0000 (26747.6325)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0007  max mem: 15572
Epoch: [39]  [ 390/1404]  eta: 0:10:11  lr: 0.000000  min_lr: 0.000000  loss: 4.0797 (4.1111)  class_acc: 0.3333 (0.3604)  loss_scale: 32768.0000 (26901.6061)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0008  max mem: 15572
Epoch: [39]  [ 400/1404]  eta: 0:10:02  lr: 0.000000  min_lr: 0.000000  loss: 4.0797 (4.1079)  class_acc: 0.2917 (0.3594)  loss_scale: 32768.0000 (27047.9002)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.0008  max mem: 15572
[2025-01-17 05:47:29,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:47:29,862] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:47:29,869] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:47:29,869] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:47:30,318] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55160
[2025-01-17 05:47:30,318] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55160
[2025-01-17 05:47:30,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:47:30,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:47:30,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 410/1404]  eta: 0:09:56  lr: 0.000000  min_lr: 0.000000  loss: 4.0101 (4.1066)  class_acc: 0.3333 (0.3594)  loss_scale: 32768.0000 (27266.8029)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0007  max mem: 15572
Epoch: [39]  [ 420/1404]  eta: 0:09:50  lr: 0.000000  min_lr: 0.000000  loss: 4.0253 (4.1061)  class_acc: 0.3750 (0.3605)  loss_scale: 32768.0000 (27397.4727)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0007  max mem: 15572
Epoch: [39]  [ 430/1404]  eta: 0:09:42  lr: 0.000000  min_lr: 0.000000  loss: 4.0253 (4.1034)  class_acc: 0.3750 (0.3610)  loss_scale: 32768.0000 (27522.0789)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0117  max mem: 15572
[2025-01-17 05:47:47,030] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55189
[2025-01-17 05:47:47,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:47:47,041] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55189
[2025-01-17 05:47:47,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:47:47,042] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [39]  [ 440/1404]  eta: 0:09:35  lr: 0.000000  min_lr: 0.000000  loss: 4.0049 (4.1023)  class_acc: 0.2917 (0.3592)  loss_scale: 32768.0000 (27343.8186)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0117  max mem: 15572
Epoch: [39]  [ 450/1404]  eta: 0:09:29  lr: 0.000000  min_lr: 0.000000  loss: 4.1858 (4.1050)  class_acc: 0.2500 (0.3578)  loss_scale: 16384.0000 (27100.8071)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0135  max mem: 15572
Epoch: [39]  [ 460/1404]  eta: 0:09:24  lr: 0.000000  min_lr: 0.000000  loss: 4.0342 (4.1024)  class_acc: 0.2917 (0.3570)  loss_scale: 16384.0000 (26868.3384)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0134  max mem: 15572
Epoch: [39]  [ 470/1404]  eta: 0:09:17  lr: 0.000000  min_lr: 0.000000  loss: 4.0342 (4.1058)  class_acc: 0.3333 (0.3570)  loss_scale: 16384.0000 (26645.7410)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.0006  max mem: 15572
Epoch: [39]  [ 480/1404]  eta: 0:09:11  lr: 0.000000  min_lr: 0.000000  loss: 4.2964 (4.1069)  class_acc: 0.2917 (0.3555)  loss_scale: 16384.0000 (26432.3992)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0007  max mem: 15572
Epoch: [39]  [ 490/1404]  eta: 0:09:04  lr: 0.000000  min_lr: 0.000000  loss: 4.2201 (4.1066)  class_acc: 0.3750 (0.3579)  loss_scale: 16384.0000 (26227.7475)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0006  max mem: 15572
Epoch: [39]  [ 500/1404]  eta: 0:08:58  lr: 0.000000  min_lr: 0.000000  loss: 4.1494 (4.1090)  class_acc: 0.4583 (0.3580)  loss_scale: 16384.0000 (26031.2655)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.0006  max mem: 15572
Epoch: [39]  [ 510/1404]  eta: 0:08:53  lr: 0.000000  min_lr: 0.000000  loss: 4.0811 (4.1060)  class_acc: 0.3333 (0.3576)  loss_scale: 16384.0000 (25842.4736)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0006  max mem: 15572
Epoch: [39]  [ 520/1404]  eta: 0:08:46  lr: 0.000000  min_lr: 0.000000  loss: 4.1104 (4.1067)  class_acc: 0.3333 (0.3583)  loss_scale: 16384.0000 (25660.9290)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0009  max mem: 15572
Epoch: [39]  [ 530/1404]  eta: 0:08:39  lr: 0.000000  min_lr: 0.000000  loss: 4.1580 (4.1078)  class_acc: 0.3750 (0.3580)  loss_scale: 16384.0000 (25486.2222)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0010  max mem: 15572
Epoch: [39]  [ 540/1404]  eta: 0:08:32  lr: 0.000000  min_lr: 0.000000  loss: 4.0494 (4.1062)  class_acc: 0.3750 (0.3593)  loss_scale: 16384.0000 (25317.9741)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0007  max mem: 15572
Epoch: [39]  [ 550/1404]  eta: 0:08:25  lr: 0.000000  min_lr: 0.000000  loss: 4.0012 (4.1057)  class_acc: 0.4167 (0.3612)  loss_scale: 16384.0000 (25155.8330)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0006  max mem: 15572
Epoch: [39]  [ 560/1404]  eta: 0:08:20  lr: 0.000000  min_lr: 0.000000  loss: 4.0890 (4.1059)  class_acc: 0.4167 (0.3620)  loss_scale: 16384.0000 (24999.4724)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0055  max mem: 15572
[2025-01-17 05:49:01,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:49:01,274] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:49:01,318] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:49:01,319] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [ 570/1404]  eta: 0:08:14  lr: 0.000000  min_lr: 0.000000  loss: 4.1618 (4.1059)  class_acc: 0.3333 (0.3608)  loss_scale: 16384.0000 (25106.8301)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.0056  max mem: 15572
[2025-01-17 05:49:08,135] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55329
[2025-01-17 05:49:08,136] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:49:08,136] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 05:49:08,194] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55329
[2025-01-17 05:49:08,194] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [39]  [ 580/1404]  eta: 0:08:09  lr: 0.000000  min_lr: 0.000000  loss: 4.2029 (4.1077)  class_acc: 0.3333 (0.3607)  loss_scale: 32768.0000 (25013.0947)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0006  max mem: 15572
Epoch: [39]  [ 590/1404]  eta: 0:08:02  lr: 0.000000  min_lr: 0.000000  loss: 4.3283 (4.1119)  class_acc: 0.3333 (0.3599)  loss_scale: 16384.0000 (24867.0863)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0008  max mem: 15572
Epoch: [39]  [ 600/1404]  eta: 0:07:57  lr: 0.000000  min_lr: 0.000000  loss: 4.2075 (4.1132)  class_acc: 0.3333 (0.3600)  loss_scale: 16384.0000 (24725.9368)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0009  max mem: 15572
Epoch: [39]  [ 610/1404]  eta: 0:07:52  lr: 0.000000  min_lr: 0.000000  loss: 4.1991 (4.1156)  class_acc: 0.3333 (0.3601)  loss_scale: 16384.0000 (24589.4075)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.0006  max mem: 15572
Epoch: [39]  [ 620/1404]  eta: 0:07:45  lr: 0.000000  min_lr: 0.000000  loss: 4.2095 (4.1172)  class_acc: 0.3333 (0.3594)  loss_scale: 16384.0000 (24457.2754)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0006  max mem: 15572
Epoch: [39]  [ 630/1404]  eta: 0:07:39  lr: 0.000000  min_lr: 0.000000  loss: 4.1638 (4.1156)  class_acc: 0.3333 (0.3597)  loss_scale: 16384.0000 (24329.3312)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0007  max mem: 15572
Epoch: [39]  [ 640/1404]  eta: 0:07:34  lr: 0.000000  min_lr: 0.000000  loss: 4.1397 (4.1167)  class_acc: 0.3750 (0.3598)  loss_scale: 16384.0000 (24205.3791)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0008  max mem: 15572
Epoch: [39]  [ 650/1404]  eta: 0:07:27  lr: 0.000000  min_lr: 0.000000  loss: 4.1397 (4.1171)  class_acc: 0.3333 (0.3596)  loss_scale: 16384.0000 (24085.2350)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0011  max mem: 15572
Epoch: [39]  [ 660/1404]  eta: 0:07:21  lr: 0.000000  min_lr: 0.000000  loss: 4.1364 (4.1173)  class_acc: 0.3333 (0.3599)  loss_scale: 16384.0000 (23968.7262)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0012  max mem: 15572
Epoch: [39]  [ 670/1404]  eta: 0:07:15  lr: 0.000000  min_lr: 0.000000  loss: 4.1992 (4.1167)  class_acc: 0.3333 (0.3598)  loss_scale: 16384.0000 (23855.6900)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0031  max mem: 15572
Epoch: [39]  [ 680/1404]  eta: 0:07:09  lr: 0.000000  min_lr: 0.000000  loss: 4.0952 (4.1151)  class_acc: 0.3750 (0.3598)  loss_scale: 16384.0000 (23745.9736)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0029  max mem: 15572
Epoch: [39]  [ 690/1404]  eta: 0:07:03  lr: 0.000000  min_lr: 0.000000  loss: 4.0821 (4.1143)  class_acc: 0.3750 (0.3603)  loss_scale: 16384.0000 (23639.4327)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0005  max mem: 15572
Epoch: [39]  [ 700/1404]  eta: 0:06:57  lr: 0.000000  min_lr: 0.000000  loss: 4.1561 (4.1146)  class_acc: 0.3750 (0.3603)  loss_scale: 16384.0000 (23535.9315)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.0007  max mem: 15572
[2025-01-17 05:50:24,090] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:50:24,090] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:50:24,091] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:50:24,091] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [ 710/1404]  eta: 0:06:51  lr: 0.000000  min_lr: 0.000000  loss: 4.2029 (4.1155)  class_acc: 0.3333 (0.3603)  loss_scale: 16384.0000 (23642.7342)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0008  max mem: 15572
[2025-01-17 05:50:30,735] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55469
[2025-01-17 05:50:30,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:50:30,738] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55469
[2025-01-17 05:50:30,739] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:50:30,739] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [39]  [ 720/1404]  eta: 0:06:45  lr: 0.000000  min_lr: 0.000000  loss: 4.1188 (4.1144)  class_acc: 0.3333 (0.3598)  loss_scale: 32768.0000 (23587.5062)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.0007  max mem: 15572
Epoch: [39]  [ 730/1404]  eta: 0:06:39  lr: 0.000000  min_lr: 0.000000  loss: 4.1188 (4.1140)  class_acc: 0.3333 (0.3603)  loss_scale: 16384.0000 (23488.9631)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0006  max mem: 15572
Epoch: [39]  [ 740/1404]  eta: 0:06:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1367 (4.1135)  class_acc: 0.3750 (0.3606)  loss_scale: 16384.0000 (23393.0796)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0006  max mem: 15572
Epoch: [39]  [ 750/1404]  eta: 0:06:27  lr: 0.000000  min_lr: 0.000000  loss: 4.1367 (4.1138)  class_acc: 0.2917 (0.3598)  loss_scale: 16384.0000 (23299.7497)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0006  max mem: 15572
Epoch: [39]  [ 760/1404]  eta: 0:06:21  lr: 0.000000  min_lr: 0.000000  loss: 4.1390 (4.1129)  class_acc: 0.3333 (0.3601)  loss_scale: 16384.0000 (23208.8725)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0006  max mem: 15572
Epoch: [39]  [ 770/1404]  eta: 0:06:15  lr: 0.000000  min_lr: 0.000000  loss: 4.0399 (4.1123)  class_acc: 0.3333 (0.3598)  loss_scale: 16384.0000 (23120.3528)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0006  max mem: 15572
Epoch: [39]  [ 780/1404]  eta: 0:06:09  lr: 0.000000  min_lr: 0.000000  loss: 4.1375 (4.1137)  class_acc: 0.2500 (0.3587)  loss_scale: 16384.0000 (23034.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0006  max mem: 15572
Epoch: [39]  [ 790/1404]  eta: 0:06:03  lr: 0.000000  min_lr: 0.000000  loss: 4.0545 (4.1133)  class_acc: 0.3333 (0.3596)  loss_scale: 16384.0000 (22950.0278)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0007  max mem: 15572
Epoch: [39]  [ 800/1404]  eta: 0:05:56  lr: 0.000000  min_lr: 0.000000  loss: 4.1833 (4.1150)  class_acc: 0.3750 (0.3596)  loss_scale: 16384.0000 (22868.0549)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0006  max mem: 15572
Epoch: [39]  [ 810/1404]  eta: 0:05:50  lr: 0.000000  min_lr: 0.000000  loss: 4.2371 (4.1157)  class_acc: 0.3333 (0.3592)  loss_scale: 16384.0000 (22788.1036)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0124  max mem: 15572
Epoch: [39]  [ 820/1404]  eta: 0:05:44  lr: 0.000000  min_lr: 0.000000  loss: 4.0960 (4.1152)  class_acc: 0.3750 (0.3602)  loss_scale: 16384.0000 (22710.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0165  max mem: 15572
Epoch: [39]  [ 830/1404]  eta: 0:05:38  lr: 0.000000  min_lr: 0.000000  loss: 4.1559 (4.1165)  class_acc: 0.3333 (0.3596)  loss_scale: 16384.0000 (22633.9735)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.0328  max mem: 15572
Epoch: [39]  [ 840/1404]  eta: 0:05:32  lr: 0.000000  min_lr: 0.000000  loss: 4.1948 (4.1163)  class_acc: 0.3333 (0.3596)  loss_scale: 16384.0000 (22559.6576)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0583  max mem: 15572
[2025-01-17 05:51:45,434] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:51:45,434] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:51:45,492] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:51:45,492] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [ 850/1404]  eta: 0:05:27  lr: 0.000000  min_lr: 0.000000  loss: 4.2208 (4.1184)  class_acc: 0.3333 (0.3591)  loss_scale: 16384.0000 (22660.3619)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0575  max mem: 15572
Epoch: [39]  [ 860/1404]  eta: 0:05:21  lr: 0.000000  min_lr: 0.000000  loss: 4.1943 (4.1176)  class_acc: 0.2917 (0.3587)  loss_scale: 32768.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  time: 0.6499  data: 0.1170  max mem: 15572
Epoch: [39]  [ 870/1404]  eta: 0:05:15  lr: 0.000000  min_lr: 0.000000  loss: 4.1829 (4.1187)  class_acc: 0.3333 (0.3587)  loss_scale: 32768.0000 (22892.4546)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1094  max mem: 15572
Epoch: [39]  [ 880/1404]  eta: 0:05:09  lr: 0.000000  min_lr: 0.000000  loss: 4.2672 (4.1207)  class_acc: 0.2917 (0.3581)  loss_scale: 32768.0000 (23004.5494)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0783  max mem: 15572
Epoch: [39]  [ 890/1404]  eta: 0:05:03  lr: 0.000000  min_lr: 0.000000  loss: 4.0917 (4.1192)  class_acc: 0.3750 (0.3591)  loss_scale: 32768.0000 (23114.1279)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1014  max mem: 15572
Epoch: [39]  [ 900/1404]  eta: 0:04:57  lr: 0.000000  min_lr: 0.000000  loss: 4.1714 (4.1211)  class_acc: 0.4167 (0.3595)  loss_scale: 32768.0000 (23221.2741)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.0433  max mem: 15572
Epoch: [39]  [ 910/1404]  eta: 0:04:51  lr: 0.000000  min_lr: 0.000000  loss: 4.3051 (4.1216)  class_acc: 0.4167 (0.3595)  loss_scale: 32768.0000 (23326.0681)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0005  max mem: 15572
Epoch: [39]  [ 920/1404]  eta: 0:04:45  lr: 0.000000  min_lr: 0.000000  loss: 4.1955 (4.1202)  class_acc: 0.4167 (0.3599)  loss_scale: 32768.0000 (23428.5863)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0323  max mem: 15572
Epoch: [39]  [ 930/1404]  eta: 0:04:39  lr: 0.000000  min_lr: 0.000000  loss: 4.1689 (4.1207)  class_acc: 0.4167 (0.3601)  loss_scale: 32768.0000 (23528.9023)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0623  max mem: 15572
Epoch: [39]  [ 940/1404]  eta: 0:04:33  lr: 0.000000  min_lr: 0.000000  loss: 4.1989 (4.1216)  class_acc: 0.2917 (0.3594)  loss_scale: 32768.0000 (23627.0861)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1072  max mem: 15572
Epoch: [39]  [ 950/1404]  eta: 0:04:27  lr: 0.000000  min_lr: 0.000000  loss: 4.1445 (4.1212)  class_acc: 0.2917 (0.3599)  loss_scale: 32768.0000 (23723.2050)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.1453  max mem: 15572
Epoch: [39]  [ 960/1404]  eta: 0:04:21  lr: 0.000000  min_lr: 0.000000  loss: 4.1012 (4.1203)  class_acc: 0.4167 (0.3603)  loss_scale: 32768.0000 (23817.3236)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0731  max mem: 15572
[2025-01-17 05:53:00,931] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:53:00,932] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 05:53:00,933] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:53:00,933] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 970/1404]  eta: 0:04:16  lr: 0.000000  min_lr: 0.000000  loss: 3.9630 (4.1190)  class_acc: 0.3750 (0.3602)  loss_scale: 32768.0000 (23943.2503)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0050  max mem: 15572
[2025-01-17 05:53:02,043] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55728
[2025-01-17 05:53:02,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 05:53:02,043] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 05:53:02,079] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55728
[2025-01-17 05:53:02,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [39]  [ 980/1404]  eta: 0:04:10  lr: 0.000000  min_lr: 0.000000  loss: 3.9731 (4.1178)  class_acc: 0.3750 (0.3610)  loss_scale: 32768.0000 (24066.6096)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.0007  max mem: 15572
Epoch: [39]  [ 990/1404]  eta: 0:04:04  lr: 0.000000  min_lr: 0.000000  loss: 4.0052 (4.1176)  class_acc: 0.3750 (0.3616)  loss_scale: 32768.0000 (24154.4137)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0008  max mem: 15572
Epoch: [39]  [1000/1404]  eta: 0:03:58  lr: 0.000000  min_lr: 0.000000  loss: 4.1164 (4.1174)  class_acc: 0.2917 (0.3612)  loss_scale: 32768.0000 (24240.4635)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0008  max mem: 15572
Epoch: [39]  [1010/1404]  eta: 0:03:52  lr: 0.000000  min_lr: 0.000000  loss: 4.1306 (4.1176)  class_acc: 0.2917 (0.3616)  loss_scale: 32768.0000 (24324.8111)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0009  max mem: 15572
Epoch: [39]  [1020/1404]  eta: 0:03:46  lr: 0.000000  min_lr: 0.000000  loss: 4.1365 (4.1166)  class_acc: 0.4167 (0.3619)  loss_scale: 32768.0000 (24407.5064)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0009  max mem: 15572
[2025-01-17 05:53:30,826] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55777
[2025-01-17 05:53:30,826] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:53:30,832] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55777
[2025-01-17 05:53:30,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:53:30,833] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [39]  [1030/1404]  eta: 0:03:40  lr: 0.000000  min_lr: 0.000000  loss: 4.0556 (4.1159)  class_acc: 0.3750 (0.3624)  loss_scale: 16384.0000 (24329.6838)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.0008  max mem: 15572
Epoch: [39]  [1040/1404]  eta: 0:03:34  lr: 0.000000  min_lr: 0.000000  loss: 4.0137 (4.1147)  class_acc: 0.3333 (0.3622)  loss_scale: 16384.0000 (24253.3564)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0007  max mem: 15572
Epoch: [39]  [1050/1404]  eta: 0:03:28  lr: 0.000000  min_lr: 0.000000  loss: 4.0655 (4.1165)  class_acc: 0.3333 (0.3620)  loss_scale: 16384.0000 (24178.4814)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0007  max mem: 15572
Epoch: [39]  [1060/1404]  eta: 0:03:22  lr: 0.000000  min_lr: 0.000000  loss: 4.2434 (4.1160)  class_acc: 0.3750 (0.3624)  loss_scale: 16384.0000 (24105.0179)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0007  max mem: 15572
Epoch: [39]  [1070/1404]  eta: 0:03:16  lr: 0.000000  min_lr: 0.000000  loss: 4.0503 (4.1150)  class_acc: 0.4583 (0.3633)  loss_scale: 16384.0000 (24032.9262)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0008  max mem: 15572
Epoch: [39]  [1080/1404]  eta: 0:03:10  lr: 0.000000  min_lr: 0.000000  loss: 4.1576 (4.1166)  class_acc: 0.4167 (0.3637)  loss_scale: 16384.0000 (23962.1684)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0008  max mem: 15572
Epoch: [39]  [1090/1404]  eta: 0:03:05  lr: 0.000000  min_lr: 0.000000  loss: 4.2043 (4.1169)  class_acc: 0.3750 (0.3639)  loss_scale: 16384.0000 (23892.7076)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0007  max mem: 15572
Epoch: [39]  [1100/1404]  eta: 0:02:59  lr: 0.000000  min_lr: 0.000000  loss: 4.2726 (4.1171)  class_acc: 0.2500 (0.3632)  loss_scale: 16384.0000 (23824.5086)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0007  max mem: 15572
Epoch: [39]  [1110/1404]  eta: 0:02:53  lr: 0.000000  min_lr: 0.000000  loss: 4.2802 (4.1184)  class_acc: 0.2500 (0.3627)  loss_scale: 16384.0000 (23757.5374)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0007  max mem: 15572
Epoch: [39]  [1120/1404]  eta: 0:02:47  lr: 0.000000  min_lr: 0.000000  loss: 4.2036 (4.1187)  class_acc: 0.3333 (0.3629)  loss_scale: 16384.0000 (23691.7609)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.0008  max mem: 15572
Epoch: [39]  [1130/1404]  eta: 0:02:41  lr: 0.000000  min_lr: 0.000000  loss: 4.0777 (4.1181)  class_acc: 0.3333 (0.3630)  loss_scale: 16384.0000 (23627.1477)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.0006  max mem: 15572
Epoch: [39]  [1140/1404]  eta: 0:02:35  lr: 0.000000  min_lr: 0.000000  loss: 4.0674 (4.1175)  class_acc: 0.3750 (0.3627)  loss_scale: 16384.0000 (23563.6670)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0006  max mem: 15572
[2025-01-17 05:54:46,107] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:54:46,108] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:54:46,109] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:54:46,109] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [1150/1404]  eta: 0:02:29  lr: 0.000000  min_lr: 0.000000  loss: 4.1304 (4.1173)  class_acc: 0.2917 (0.3626)  loss_scale: 16384.0000 (23515.5239)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0007  max mem: 15572
Epoch: [39]  [1160/1404]  eta: 0:02:23  lr: 0.000000  min_lr: 0.000000  loss: 4.1957 (4.1181)  class_acc: 0.3333 (0.3627)  loss_scale: 32768.0000 (23595.2179)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0007  max mem: 15572
Epoch: [39]  [1170/1404]  eta: 0:02:17  lr: 0.000000  min_lr: 0.000000  loss: 4.1763 (4.1179)  class_acc: 0.3750 (0.3628)  loss_scale: 32768.0000 (23673.5508)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0007  max mem: 15572
[2025-01-17 05:55:03,595] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55936
[2025-01-17 05:55:03,596] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:55:03,690] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55936
[2025-01-17 05:55:03,691] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:55:03,691] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [39]  [1180/1404]  eta: 0:02:11  lr: 0.000000  min_lr: 0.000000  loss: 4.0731 (4.1178)  class_acc: 0.3333 (0.3629)  loss_scale: 32768.0000 (23736.6842)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0007  max mem: 15572
Epoch: [39]  [1190/1404]  eta: 0:02:05  lr: 0.000000  min_lr: 0.000000  loss: 4.0891 (4.1184)  class_acc: 0.3333 (0.3629)  loss_scale: 16384.0000 (23674.9488)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0008  max mem: 15572
Epoch: [39]  [1200/1404]  eta: 0:02:00  lr: 0.000000  min_lr: 0.000000  loss: 4.2074 (4.1200)  class_acc: 0.3333 (0.3628)  loss_scale: 16384.0000 (23614.2415)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.0119  max mem: 15572
Epoch: [39]  [1210/1404]  eta: 0:01:54  lr: 0.000000  min_lr: 0.000000  loss: 4.2178 (4.1203)  class_acc: 0.2917 (0.3626)  loss_scale: 16384.0000 (23554.5367)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.0118  max mem: 15572
Epoch: [39]  [1220/1404]  eta: 0:01:48  lr: 0.000000  min_lr: 0.000000  loss: 4.1563 (4.1205)  class_acc: 0.2917 (0.3624)  loss_scale: 16384.0000 (23495.8100)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.0007  max mem: 15572
Epoch: [39]  [1230/1404]  eta: 0:01:42  lr: 0.000000  min_lr: 0.000000  loss: 4.2415 (4.1214)  class_acc: 0.3333 (0.3628)  loss_scale: 16384.0000 (23438.0374)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0007  max mem: 15572
Epoch: [39]  [1240/1404]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 4.2636 (4.1229)  class_acc: 0.4167 (0.3631)  loss_scale: 16384.0000 (23381.1958)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0007  max mem: 15572
[2025-01-17 05:55:39,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=56000, skipped=334, lr=[9.323686162031562e-10, 9.323686162031562e-10, 1.3319551660045092e-09, 1.3319551660045092e-09, 1.9027930942921564e-09, 1.9027930942921564e-09, 2.7182758489887946e-09, 2.7182758489887946e-09, 3.883251212841135e-09, 3.883251212841135e-09, 5.547501732630193e-09, 5.547501732630193e-09, 7.925002475185992e-09, 7.925002475185992e-09, 1.132143210740856e-08, 1.132143210740856e-08, 1.6173474439155085e-08, 1.6173474439155085e-08, 2.310496348450727e-08, 2.310496348450727e-08, 3.300709069215324e-08, 3.300709069215324e-08, 4.715298670307606e-08, 4.715298670307606e-08, 6.736140957582295e-08, 6.736140957582295e-08, 9.623058510831851e-08, 9.623058510831851e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 05:55:39,818] [INFO] [timer.py:260:stop] epoch=0/micro_step=56000/global_step=56000, RunningAvgSamplesPerSec=48.08398789379557, CurrSamplesPerSec=53.51009407814791, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [39]  [1250/1404]  eta: 0:01:30  lr: 0.000000  min_lr: 0.000000  loss: 4.2124 (4.1229)  class_acc: 0.4167 (0.3632)  loss_scale: 16384.0000 (23325.2630)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0162  max mem: 15572
Epoch: [39]  [1260/1404]  eta: 0:01:24  lr: 0.000000  min_lr: 0.000000  loss: 4.2124 (4.1242)  class_acc: 0.3333 (0.3629)  loss_scale: 16384.0000 (23270.2173)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.0227  max mem: 15572
Epoch: [39]  [1270/1404]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 4.2772 (4.1250)  class_acc: 0.3750 (0.3628)  loss_scale: 16384.0000 (23216.0378)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0072  max mem: 15572
Epoch: [39]  [1280/1404]  eta: 0:01:12  lr: 0.000000  min_lr: 0.000000  loss: 4.2219 (4.1255)  class_acc: 0.3333 (0.3622)  loss_scale: 16384.0000 (23162.7041)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0351  max mem: 15572
Epoch: [39]  [1290/1404]  eta: 0:01:07  lr: 0.000000  min_lr: 0.000000  loss: 4.1264 (4.1252)  class_acc: 0.2917 (0.3623)  loss_scale: 16384.0000 (23110.1967)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0350  max mem: 15572
Epoch: [39]  [1300/1404]  eta: 0:01:01  lr: 0.000000  min_lr: 0.000000  loss: 4.0223 (4.1253)  class_acc: 0.3750 (0.3624)  loss_scale: 16384.0000 (23058.4965)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0006  max mem: 15572
[2025-01-17 05:56:18,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:56:18,142] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 05:56:18,151] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 05:56:18,151] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [1310/1404]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 4.0190 (4.1248)  class_acc: 0.3750 (0.3625)  loss_scale: 16384.0000 (23032.5797)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0007  max mem: 15572
[2025-01-17 05:56:21,313] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56069
[2025-01-17 05:56:21,313] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56069
[2025-01-17 05:56:21,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:56:21,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 05:56:21,313] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [39]  [1320/1404]  eta: 0:00:49  lr: 0.000000  min_lr: 0.000000  loss: 4.0807 (4.1254)  class_acc: 0.3750 (0.3624)  loss_scale: 16384.0000 (23007.0553)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0498  max mem: 15572
Epoch: [39]  [1330/1404]  eta: 0:00:43  lr: 0.000000  min_lr: 0.000000  loss: 4.2183 (4.1254)  class_acc: 0.3333 (0.3622)  loss_scale: 16384.0000 (22957.2953)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1095  max mem: 15572
Epoch: [39]  [1340/1404]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 3.9755 (4.1246)  class_acc: 0.3333 (0.3620)  loss_scale: 16384.0000 (22908.2774)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.0998  max mem: 15572
Epoch: [39]  [1350/1404]  eta: 0:00:31  lr: 0.000000  min_lr: 0.000000  loss: 4.0843 (4.1252)  class_acc: 0.3333 (0.3618)  loss_scale: 16384.0000 (22859.9852)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1220  max mem: 15572
Epoch: [39]  [1360/1404]  eta: 0:00:25  lr: 0.000000  min_lr: 0.000000  loss: 4.1597 (4.1255)  class_acc: 0.3333 (0.3617)  loss_scale: 16384.0000 (22812.4026)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1266  max mem: 15572
Epoch: [39]  [1370/1404]  eta: 0:00:20  lr: 0.000000  min_lr: 0.000000  loss: 4.0615 (4.1234)  class_acc: 0.3333 (0.3617)  loss_scale: 16384.0000 (22765.5142)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.1554  max mem: 15572
Epoch: [39]  [1380/1404]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 4.0615 (4.1233)  class_acc: 0.3333 (0.3615)  loss_scale: 16384.0000 (22719.3049)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1162  max mem: 15572
Epoch: [39]  [1390/1404]  eta: 0:00:08  lr: 0.000000  min_lr: 0.000000  loss: 4.0728 (4.1233)  class_acc: 0.3333 (0.3617)  loss_scale: 16384.0000 (22673.7599)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0845  max mem: 15572
Epoch: [39]  [1400/1404]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 4.0154 (4.1230)  class_acc: 0.3750 (0.3611)  loss_scale: 16384.0000 (22628.8651)  weight_decay: 0.0500 (0.0500)  time: 0.4895  data: 0.0792  max mem: 15572
Epoch: [39]  [1403/1404]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 3.9280 (4.1227)  class_acc: 0.3333 (0.3609)  loss_scale: 16384.0000 (22615.5214)  weight_decay: 0.0500 (0.0500)  time: 0.3919  data: 0.0003  max mem: 15572
Epoch: [39] Total time: 0:13:44 (0.5875 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 3.9280 (4.1197)  class_acc: 0.3333 (0.3599)  loss_scale: 16384.0000 (22615.5214)  weight_decay: 0.0500 (0.0500)
[2025-01-17 05:57:12,290] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2025-01-17 05:57:12,292] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-39/mp_rank_00_model_states.pt
[2025-01-17 05:57:12,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-39/mp_rank_00_model_states.pt...
[2025-01-17 05:57:12,292] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2025-01-17 05:57:12,522] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-39/mp_rank_00_model_states.pt.
[2025-01-17 05:57:12,522] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [  0/136]  eta: 0:11:46  loss: 1.7461 (1.7461)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.1962  data: 4.9777  max mem: 15572
Val:  [ 10/136]  eta: 0:01:43  loss: 2.3057 (2.2069)  acc1: 55.5556 (53.5354)  acc5: 83.3333 (80.8081)  time: 0.8239  data: 0.6333  max mem: 15572
Val:  [ 20/136]  eta: 0:01:07  loss: 2.4106 (2.3592)  acc1: 50.0000 (49.7355)  acc5: 72.2222 (77.7778)  time: 0.3509  data: 0.1675  max mem: 15572
Val:  [ 30/136]  eta: 0:00:55  loss: 2.3943 (2.2946)  acc1: 44.4444 (49.1039)  acc5: 77.7778 (78.4946)  time: 0.3608  data: 0.1709  max mem: 15572
Val:  [ 40/136]  eta: 0:00:45  loss: 2.0589 (2.2622)  acc1: 50.0000 (51.2195)  acc5: 83.3333 (79.1328)  time: 0.3528  data: 0.1465  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 2.1708 (2.2543)  acc1: 55.5556 (51.6340)  acc5: 83.3333 (80.0654)  time: 0.3192  data: 0.1110  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.2044 (2.3215)  acc1: 38.8889 (49.0893)  acc5: 77.7778 (78.8707)  time: 0.3566  data: 0.1539  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.2044 (2.3045)  acc1: 44.4444 (49.7653)  acc5: 77.7778 (79.5775)  time: 0.3540  data: 0.1527  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 2.1689 (2.2957)  acc1: 50.0000 (49.7942)  acc5: 88.8889 (80.1783)  time: 0.3366  data: 0.1431  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.2401 (2.3025)  acc1: 50.0000 (49.1453)  acc5: 77.7778 (80.0366)  time: 0.3447  data: 0.1560  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5693 (2.3602)  acc1: 38.8889 (47.0297)  acc5: 77.7778 (78.5479)  time: 0.3584  data: 0.1683  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.5106 (2.3520)  acc1: 33.3333 (47.4975)  acc5: 77.7778 (78.4284)  time: 0.3690  data: 0.1766  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1646 (2.3155)  acc1: 50.0000 (48.3930)  acc5: 83.3333 (79.0634)  time: 0.3715  data: 0.1747  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8625 (2.2851)  acc1: 55.5556 (49.2366)  acc5: 88.8889 (79.8134)  time: 0.2612  data: 0.0926  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.8862 (2.2796)  acc1: 50.0000 (49.4267)  acc5: 88.8889 (79.9345)  time: 0.1507  data: 0.0054  max mem: 15572
Val: Total time: 0:00:50 (0.3679 s / it)
* Acc@1 48.321 Acc@5 79.177 loss 2.318
Accuracy of the network on the 4883 val videos: 48.3%
[2025-01-17 05:58:02,563] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 05:58:02,565] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 05:58:02,565] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 05:58:02,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 05:58:04,946] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_0_2gpus/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 05:58:04,947] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 48.32%
Test:  [   0/1221]  eta: 1:20:36  loss: 1.0798 (1.0798)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 3.9608  data: 3.6910  max mem: 15572
Test:  [  10/1221]  eta: 0:12:11  loss: 2.2332 (2.1657)  acc1: 50.0000 (53.7879)  acc5: 83.3333 (83.3333)  time: 0.6044  data: 0.4454  max mem: 15572
Test:  [  20/1221]  eta: 0:09:12  loss: 2.3057 (2.2733)  acc1: 50.0000 (48.0159)  acc5: 83.3333 (82.9365)  time: 0.2851  data: 0.1383  max mem: 15572
Test:  [  30/1221]  eta: 0:08:35  loss: 2.6402 (2.4610)  acc1: 33.3333 (42.4731)  acc5: 75.0000 (77.1505)  time: 0.3380  data: 0.1965  max mem: 15572
Test:  [  40/1221]  eta: 0:07:55  loss: 2.3544 (2.4007)  acc1: 33.3333 (43.6992)  acc5: 75.0000 (78.0488)  time: 0.3418  data: 0.2075  max mem: 15572
Test:  [  50/1221]  eta: 0:07:09  loss: 2.2152 (2.3810)  acc1: 41.6667 (44.2810)  acc5: 83.3333 (77.9412)  time: 0.2654  data: 0.1239  max mem: 15572
Test:  [  60/1221]  eta: 0:06:31  loss: 2.2779 (2.3621)  acc1: 41.6667 (44.8087)  acc5: 83.3333 (78.8251)  time: 0.2042  data: 0.0562  max mem: 15572
Test:  [  70/1221]  eta: 0:06:15  loss: 2.2936 (2.3478)  acc1: 41.6667 (44.9531)  acc5: 83.3333 (78.8732)  time: 0.2231  data: 0.0864  max mem: 15572
Test:  [  80/1221]  eta: 0:06:03  loss: 2.4405 (2.4048)  acc1: 33.3333 (43.1070)  acc5: 75.0000 (77.9835)  time: 0.2602  data: 0.1265  max mem: 15572
Test:  [  90/1221]  eta: 0:05:58  loss: 2.5422 (2.4175)  acc1: 41.6667 (42.7656)  acc5: 66.6667 (77.3810)  time: 0.2848  data: 0.1436  max mem: 15572
Test:  [ 100/1221]  eta: 0:05:47  loss: 2.2395 (2.3807)  acc1: 50.0000 (44.1419)  acc5: 83.3333 (78.3828)  time: 0.2745  data: 0.1314  max mem: 15572
Test:  [ 110/1221]  eta: 0:05:42  loss: 2.3115 (2.4050)  acc1: 50.0000 (43.6186)  acc5: 83.3333 (77.9279)  time: 0.2649  data: 0.1256  max mem: 15572
Test:  [ 120/1221]  eta: 0:05:37  loss: 2.3350 (2.3857)  acc1: 41.6667 (44.2837)  acc5: 83.3333 (78.7190)  time: 0.2889  data: 0.1497  max mem: 15572
Test:  [ 130/1221]  eta: 0:05:29  loss: 2.1493 (2.3853)  acc1: 41.6667 (44.2112)  acc5: 83.3333 (78.8804)  time: 0.2712  data: 0.1232  max mem: 15572
Test:  [ 140/1221]  eta: 0:05:27  loss: 2.4907 (2.4272)  acc1: 33.3333 (42.6123)  acc5: 75.0000 (77.6596)  time: 0.2807  data: 0.1269  max mem: 15572
Test:  [ 150/1221]  eta: 0:05:19  loss: 2.6352 (2.4480)  acc1: 25.0000 (42.0530)  acc5: 75.0000 (77.1523)  time: 0.2738  data: 0.1338  max mem: 15572
Test:  [ 160/1221]  eta: 0:05:18  loss: 2.3566 (2.4468)  acc1: 41.6667 (42.2360)  acc5: 75.0000 (77.2257)  time: 0.2816  data: 0.1421  max mem: 15572
Test:  [ 170/1221]  eta: 0:05:15  loss: 2.1655 (2.4299)  acc1: 50.0000 (42.9337)  acc5: 83.3333 (77.4854)  time: 0.3137  data: 0.1635  max mem: 15572
Test:  [ 180/1221]  eta: 0:05:12  loss: 1.8674 (2.4057)  acc1: 66.6667 (44.0147)  acc5: 91.6667 (78.1308)  time: 0.2976  data: 0.1447  max mem: 15572
Test:  [ 190/1221]  eta: 0:05:06  loss: 2.1657 (2.4060)  acc1: 50.0000 (44.2408)  acc5: 83.3333 (78.0105)  time: 0.2763  data: 0.1293  max mem: 15572
Test:  [ 200/1221]  eta: 0:05:05  loss: 2.0983 (2.3711)  acc1: 58.3333 (45.6053)  acc5: 83.3333 (78.6070)  time: 0.2978  data: 0.1682  max mem: 15572
Test:  [ 210/1221]  eta: 0:04:59  loss: 1.9646 (2.3695)  acc1: 58.3333 (45.7741)  acc5: 83.3333 (78.6335)  time: 0.2782  data: 0.1495  max mem: 15572
Test:  [ 220/1221]  eta: 0:04:55  loss: 2.3128 (2.3675)  acc1: 50.0000 (45.7014)  acc5: 83.3333 (78.6953)  time: 0.2521  data: 0.1164  max mem: 15572
Test:  [ 230/1221]  eta: 0:04:51  loss: 2.3811 (2.3777)  acc1: 41.6667 (45.4185)  acc5: 83.3333 (78.5354)  time: 0.2759  data: 0.1398  max mem: 15572
Test:  [ 240/1221]  eta: 0:04:48  loss: 2.2909 (2.3689)  acc1: 33.3333 (45.5048)  acc5: 83.3333 (78.8728)  time: 0.2755  data: 0.1397  max mem: 15572
Test:  [ 250/1221]  eta: 0:04:43  loss: 1.9848 (2.3540)  acc1: 58.3333 (46.0159)  acc5: 91.6667 (79.0837)  time: 0.2675  data: 0.1356  max mem: 15572
Test:  [ 260/1221]  eta: 0:04:39  loss: 1.8391 (2.3387)  acc1: 58.3333 (46.5198)  acc5: 83.3333 (79.1826)  time: 0.2620  data: 0.1257  max mem: 15572
Test:  [ 270/1221]  eta: 0:04:36  loss: 2.0360 (2.3426)  acc1: 58.3333 (46.5560)  acc5: 83.3333 (79.1513)  time: 0.2723  data: 0.1292  max mem: 15572
Test:  [ 280/1221]  eta: 0:04:35  loss: 2.4212 (2.3567)  acc1: 41.6667 (46.1447)  acc5: 83.3333 (78.8553)  time: 0.3138  data: 0.1682  max mem: 15572
Test:  [ 290/1221]  eta: 0:04:33  loss: 2.4751 (2.3633)  acc1: 41.6667 (46.0481)  acc5: 83.3333 (78.8087)  time: 0.3341  data: 0.1876  max mem: 15572
Test:  [ 300/1221]  eta: 0:04:28  loss: 2.3918 (2.3684)  acc1: 41.6667 (45.9579)  acc5: 75.0000 (78.7652)  time: 0.2816  data: 0.1381  max mem: 15572
Test:  [ 310/1221]  eta: 0:04:26  loss: 2.3432 (2.3649)  acc1: 41.6667 (46.1415)  acc5: 83.3333 (78.6978)  time: 0.2697  data: 0.1299  max mem: 15572
Test:  [ 320/1221]  eta: 0:04:23  loss: 2.2768 (2.3625)  acc1: 41.6667 (46.0021)  acc5: 83.3333 (78.7124)  time: 0.2898  data: 0.1548  max mem: 15572
Test:  [ 330/1221]  eta: 0:04:21  loss: 2.4152 (2.3679)  acc1: 33.3333 (45.8459)  acc5: 75.0000 (78.4240)  time: 0.3132  data: 0.1730  max mem: 15572
Test:  [ 340/1221]  eta: 0:04:19  loss: 2.5271 (2.3673)  acc1: 41.6667 (45.7967)  acc5: 66.6667 (78.3969)  time: 0.3421  data: 0.2007  max mem: 15572
Test:  [ 350/1221]  eta: 0:04:15  loss: 2.5215 (2.3745)  acc1: 41.6667 (45.4416)  acc5: 75.0000 (78.1814)  time: 0.2973  data: 0.1496  max mem: 15572
Test:  [ 360/1221]  eta: 0:04:12  loss: 2.5215 (2.3804)  acc1: 33.3333 (45.2909)  acc5: 66.6667 (78.0933)  time: 0.2619  data: 0.1143  max mem: 15572
Test:  [ 370/1221]  eta: 0:04:10  loss: 2.2330 (2.3808)  acc1: 41.6667 (45.3953)  acc5: 75.0000 (78.1222)  time: 0.3017  data: 0.1661  max mem: 15572
Test:  [ 380/1221]  eta: 0:04:08  loss: 1.9452 (2.3656)  acc1: 58.3333 (45.9536)  acc5: 83.3333 (78.3902)  time: 0.3373  data: 0.1993  max mem: 15572
Test:  [ 390/1221]  eta: 0:04:03  loss: 1.8891 (2.3677)  acc1: 50.0000 (45.9719)  acc5: 83.3333 (78.4101)  time: 0.2851  data: 0.1337  max mem: 15572
Test:  [ 400/1221]  eta: 0:04:01  loss: 1.8891 (2.3517)  acc1: 50.0000 (46.4464)  acc5: 83.3333 (78.6991)  time: 0.2611  data: 0.1126  max mem: 15572
Test:  [ 410/1221]  eta: 0:03:59  loss: 1.6957 (2.3427)  acc1: 66.6667 (46.7964)  acc5: 91.6667 (78.9132)  time: 0.3357  data: 0.2023  max mem: 15572
Test:  [ 420/1221]  eta: 0:03:55  loss: 1.9620 (2.3359)  acc1: 50.0000 (47.0507)  acc5: 91.6667 (79.1172)  time: 0.2890  data: 0.1442  max mem: 15572
Test:  [ 430/1221]  eta: 0:03:51  loss: 2.2859 (2.3420)  acc1: 50.0000 (46.8097)  acc5: 75.0000 (78.8670)  time: 0.2208  data: 0.0652  max mem: 15572
Test:  [ 440/1221]  eta: 0:03:48  loss: 2.6242 (2.3469)  acc1: 33.3333 (46.5420)  acc5: 66.6667 (78.7793)  time: 0.2717  data: 0.1239  max mem: 15572
Test:  [ 450/1221]  eta: 0:03:45  loss: 2.3408 (2.3457)  acc1: 41.6667 (46.5447)  acc5: 83.3333 (78.7879)  time: 0.2865  data: 0.1448  max mem: 15572
Test:  [ 460/1221]  eta: 0:03:41  loss: 2.0667 (2.3400)  acc1: 50.0000 (46.8004)  acc5: 83.3333 (78.8503)  time: 0.2714  data: 0.1228  max mem: 15572
Test:  [ 470/1221]  eta: 0:03:39  loss: 2.1835 (2.3401)  acc1: 50.0000 (46.7268)  acc5: 83.3333 (78.8570)  time: 0.2899  data: 0.1404  max mem: 15572
Test:  [ 480/1221]  eta: 0:03:35  loss: 2.3977 (2.3431)  acc1: 33.3333 (46.6216)  acc5: 75.0000 (78.7769)  time: 0.2851  data: 0.1405  max mem: 15572
Test:  [ 490/1221]  eta: 0:03:33  loss: 2.6052 (2.3545)  acc1: 33.3333 (46.3170)  acc5: 75.0000 (78.5132)  time: 0.3058  data: 0.1620  max mem: 15572
Test:  [ 500/1221]  eta: 0:03:30  loss: 2.7288 (2.3577)  acc1: 33.3333 (46.2076)  acc5: 66.6667 (78.4431)  time: 0.3124  data: 0.1686  max mem: 15572
Test:  [ 510/1221]  eta: 0:03:27  loss: 2.4054 (2.3623)  acc1: 41.6667 (46.1840)  acc5: 75.0000 (78.3757)  time: 0.2783  data: 0.1333  max mem: 15572
Test:  [ 520/1221]  eta: 0:03:24  loss: 2.2835 (2.3603)  acc1: 50.0000 (46.1292)  acc5: 83.3333 (78.4709)  time: 0.2720  data: 0.1239  max mem: 15572
Test:  [ 530/1221]  eta: 0:03:21  loss: 2.2835 (2.3601)  acc1: 33.3333 (46.0766)  acc5: 83.3333 (78.5625)  time: 0.2693  data: 0.1249  max mem: 15572
Test:  [ 540/1221]  eta: 0:03:17  loss: 2.1588 (2.3609)  acc1: 41.6667 (45.9797)  acc5: 83.3333 (78.5428)  time: 0.2607  data: 0.1099  max mem: 15572
Test:  [ 550/1221]  eta: 0:03:14  loss: 2.5539 (2.3742)  acc1: 33.3333 (45.5687)  acc5: 66.6667 (78.1760)  time: 0.2678  data: 0.1092  max mem: 15572
Test:  [ 560/1221]  eta: 0:03:11  loss: 2.5539 (2.3744)  acc1: 33.3333 (45.5585)  acc5: 75.0000 (78.1937)  time: 0.2931  data: 0.1451  max mem: 15572
Test:  [ 570/1221]  eta: 0:03:08  loss: 2.1659 (2.3768)  acc1: 41.6667 (45.5196)  acc5: 83.3333 (78.1670)  time: 0.2884  data: 0.1380  max mem: 15572
Test:  [ 580/1221]  eta: 0:03:06  loss: 2.1099 (2.3706)  acc1: 50.0000 (45.7975)  acc5: 83.3333 (78.2846)  time: 0.2871  data: 0.1443  max mem: 15572
Test:  [ 590/1221]  eta: 0:03:02  loss: 2.0326 (2.3673)  acc1: 58.3333 (45.8545)  acc5: 83.3333 (78.3982)  time: 0.2875  data: 0.1504  max mem: 15572
Test:  [ 600/1221]  eta: 0:02:59  loss: 2.1048 (2.3619)  acc1: 50.0000 (45.9789)  acc5: 83.3333 (78.4942)  time: 0.2787  data: 0.1328  max mem: 15572
Test:  [ 610/1221]  eta: 0:02:56  loss: 2.0886 (2.3561)  acc1: 58.3333 (46.2357)  acc5: 83.3333 (78.6416)  time: 0.2754  data: 0.1289  max mem: 15572
Test:  [ 620/1221]  eta: 0:02:54  loss: 2.1991 (2.3586)  acc1: 58.3333 (46.1889)  acc5: 83.3333 (78.5561)  time: 0.2827  data: 0.1327  max mem: 15572
Test:  [ 630/1221]  eta: 0:02:51  loss: 2.6064 (2.3623)  acc1: 41.6667 (46.1305)  acc5: 75.0000 (78.4733)  time: 0.3146  data: 0.1668  max mem: 15572
Test:  [ 640/1221]  eta: 0:02:47  loss: 2.6462 (2.3681)  acc1: 33.3333 (45.9308)  acc5: 75.0000 (78.3671)  time: 0.2712  data: 0.1306  max mem: 15572
Test:  [ 650/1221]  eta: 0:02:45  loss: 2.3806 (2.3665)  acc1: 41.6667 (46.0061)  acc5: 75.0000 (78.4178)  time: 0.2715  data: 0.1247  max mem: 15572
Test:  [ 660/1221]  eta: 0:02:41  loss: 2.2061 (2.3643)  acc1: 50.0000 (46.0792)  acc5: 83.3333 (78.4165)  time: 0.2756  data: 0.1106  max mem: 15572
Test:  [ 670/1221]  eta: 0:02:39  loss: 2.2061 (2.3644)  acc1: 50.0000 (46.1500)  acc5: 83.3333 (78.3656)  time: 0.2596  data: 0.0988  max mem: 15572
Test:  [ 680/1221]  eta: 0:02:35  loss: 2.5024 (2.3685)  acc1: 50.0000 (46.0352)  acc5: 75.0000 (78.3284)  time: 0.2769  data: 0.1247  max mem: 15572
Test:  [ 690/1221]  eta: 0:02:33  loss: 2.6732 (2.3775)  acc1: 33.3333 (45.7911)  acc5: 75.0000 (78.1356)  time: 0.2715  data: 0.1190  max mem: 15572
Test:  [ 700/1221]  eta: 0:02:30  loss: 2.7200 (2.3838)  acc1: 33.3333 (45.7085)  acc5: 66.6667 (77.9719)  time: 0.3155  data: 0.1632  max mem: 15572
Test:  [ 710/1221]  eta: 0:02:27  loss: 2.3068 (2.3804)  acc1: 50.0000 (45.8744)  acc5: 75.0000 (78.0356)  time: 0.2890  data: 0.1358  max mem: 15572
Test:  [ 720/1221]  eta: 0:02:24  loss: 2.2995 (2.3845)  acc1: 41.6667 (45.7235)  acc5: 75.0000 (77.9473)  time: 0.2522  data: 0.0985  max mem: 15572
Test:  [ 730/1221]  eta: 0:02:21  loss: 2.7272 (2.3869)  acc1: 33.3333 (45.6224)  acc5: 66.6667 (77.8956)  time: 0.2642  data: 0.1103  max mem: 15572
Test:  [ 740/1221]  eta: 0:02:18  loss: 2.6090 (2.3878)  acc1: 41.6667 (45.6703)  acc5: 75.0000 (77.8340)  time: 0.2989  data: 0.1589  max mem: 15572
Test:  [ 750/1221]  eta: 0:02:15  loss: 2.6035 (2.3929)  acc1: 41.6667 (45.4838)  acc5: 66.6667 (77.6298)  time: 0.2877  data: 0.1432  max mem: 15572
Test:  [ 760/1221]  eta: 0:02:12  loss: 2.6210 (2.3968)  acc1: 33.3333 (45.3351)  acc5: 75.0000 (77.5405)  time: 0.2494  data: 0.0970  max mem: 15572
Test:  [ 770/1221]  eta: 0:02:09  loss: 2.4016 (2.4002)  acc1: 41.6667 (45.2875)  acc5: 75.0000 (77.4535)  time: 0.2845  data: 0.1302  max mem: 15572
Test:  [ 780/1221]  eta: 0:02:07  loss: 2.4580 (2.4007)  acc1: 41.6667 (45.3692)  acc5: 75.0000 (77.4541)  time: 0.3333  data: 0.1787  max mem: 15572
Test:  [ 790/1221]  eta: 0:02:04  loss: 2.1570 (2.3957)  acc1: 50.0000 (45.5120)  acc5: 83.3333 (77.5706)  time: 0.3170  data: 0.1727  max mem: 15572
Test:  [ 800/1221]  eta: 0:02:01  loss: 2.2197 (2.3967)  acc1: 41.6667 (45.4952)  acc5: 83.3333 (77.5281)  time: 0.2751  data: 0.1346  max mem: 15572
Test:  [ 810/1221]  eta: 0:01:58  loss: 1.9043 (2.3906)  acc1: 50.0000 (45.6843)  acc5: 83.3333 (77.5997)  time: 0.2787  data: 0.1317  max mem: 15572
Test:  [ 820/1221]  eta: 0:01:55  loss: 1.8881 (2.3883)  acc1: 50.0000 (45.7674)  acc5: 91.6667 (77.6492)  time: 0.2924  data: 0.1503  max mem: 15572
Test:  [ 830/1221]  eta: 0:01:52  loss: 2.0432 (2.3855)  acc1: 50.0000 (45.8684)  acc5: 91.6667 (77.7477)  time: 0.3034  data: 0.1611  max mem: 15572
Test:  [ 840/1221]  eta: 0:01:49  loss: 2.0835 (2.3865)  acc1: 50.0000 (45.8284)  acc5: 83.3333 (77.7348)  time: 0.2659  data: 0.1196  max mem: 15572
Test:  [ 850/1221]  eta: 0:01:46  loss: 2.1462 (2.3845)  acc1: 33.3333 (45.8186)  acc5: 83.3333 (77.7713)  time: 0.2625  data: 0.1177  max mem: 15572
Test:  [ 860/1221]  eta: 0:01:43  loss: 2.0210 (2.3817)  acc1: 41.6667 (45.9156)  acc5: 83.3333 (77.8165)  time: 0.2854  data: 0.1418  max mem: 15572
Test:  [ 870/1221]  eta: 0:01:41  loss: 2.0157 (2.3774)  acc1: 58.3333 (46.0964)  acc5: 83.3333 (77.8798)  time: 0.2938  data: 0.1510  max mem: 15572
Test:  [ 880/1221]  eta: 0:01:38  loss: 2.1748 (2.3748)  acc1: 50.0000 (46.2070)  acc5: 83.3333 (77.9417)  time: 0.3075  data: 0.1556  max mem: 15572
Test:  [ 890/1221]  eta: 0:01:35  loss: 2.2133 (2.3754)  acc1: 50.0000 (46.1747)  acc5: 83.3333 (77.9461)  time: 0.2572  data: 0.1100  max mem: 15572
Test:  [ 900/1221]  eta: 0:01:32  loss: 2.4768 (2.3790)  acc1: 33.3333 (46.0599)  acc5: 75.0000 (77.8857)  time: 0.2818  data: 0.1433  max mem: 15572
Test:  [ 910/1221]  eta: 0:01:29  loss: 2.4050 (2.3770)  acc1: 33.3333 (46.0392)  acc5: 75.0000 (77.9821)  time: 0.2953  data: 0.1522  max mem: 15572
Test:  [ 920/1221]  eta: 0:01:26  loss: 2.1833 (2.3766)  acc1: 41.6667 (46.0731)  acc5: 83.3333 (77.9859)  time: 0.2671  data: 0.1246  max mem: 15572
Test:  [ 930/1221]  eta: 0:01:23  loss: 2.1833 (2.3750)  acc1: 50.0000 (46.0705)  acc5: 83.3333 (78.0344)  time: 0.3082  data: 0.1708  max mem: 15572
Test:  [ 940/1221]  eta: 0:01:20  loss: 2.1488 (2.3723)  acc1: 58.3333 (46.1654)  acc5: 83.3333 (78.1438)  time: 0.2949  data: 0.1646  max mem: 15572
Test:  [ 950/1221]  eta: 0:01:17  loss: 2.1584 (2.3731)  acc1: 50.0000 (46.1181)  acc5: 83.3333 (78.0757)  time: 0.2813  data: 0.1495  max mem: 15572
Test:  [ 960/1221]  eta: 0:01:15  loss: 2.3782 (2.3768)  acc1: 41.6667 (46.0024)  acc5: 75.0000 (77.9917)  time: 0.2864  data: 0.1380  max mem: 15572
Test:  [ 970/1221]  eta: 0:01:12  loss: 2.2987 (2.3780)  acc1: 41.6667 (45.9492)  acc5: 83.3333 (77.9694)  time: 0.2805  data: 0.1243  max mem: 15572
Test:  [ 980/1221]  eta: 0:01:09  loss: 2.2253 (2.3775)  acc1: 41.6667 (45.9395)  acc5: 83.3333 (77.9732)  time: 0.2972  data: 0.1440  max mem: 15572
Test:  [ 990/1221]  eta: 0:01:06  loss: 1.9086 (2.3710)  acc1: 58.3333 (46.1655)  acc5: 91.6667 (78.1282)  time: 0.2806  data: 0.1388  max mem: 15572
Test:  [1000/1221]  eta: 0:01:03  loss: 1.9089 (2.3712)  acc1: 58.3333 (46.0956)  acc5: 91.6667 (78.1219)  time: 0.2727  data: 0.1356  max mem: 15572
Test:  [1010/1221]  eta: 0:01:00  loss: 2.1943 (2.3647)  acc1: 50.0000 (46.3320)  acc5: 83.3333 (78.2559)  time: 0.2667  data: 0.1239  max mem: 15572
Test:  [1020/1221]  eta: 0:00:57  loss: 1.8901 (2.3630)  acc1: 66.6667 (46.4414)  acc5: 91.6667 (78.2893)  time: 0.2611  data: 0.1177  max mem: 15572
Test:  [1030/1221]  eta: 0:00:54  loss: 2.3641 (2.3640)  acc1: 50.0000 (46.4759)  acc5: 75.0000 (78.2412)  time: 0.2824  data: 0.1492  max mem: 15572
Test:  [1040/1221]  eta: 0:00:52  loss: 2.5039 (2.3666)  acc1: 41.6667 (46.3737)  acc5: 75.0000 (78.1940)  time: 0.3175  data: 0.1768  max mem: 15572
Test:  [1050/1221]  eta: 0:00:48  loss: 2.5842 (2.3699)  acc1: 33.3333 (46.2337)  acc5: 75.0000 (78.1399)  time: 0.2458  data: 0.0975  max mem: 15572
Test:  [1060/1221]  eta: 0:00:46  loss: 2.4086 (2.3681)  acc1: 41.6667 (46.2928)  acc5: 75.0000 (78.1653)  time: 0.2211  data: 0.0833  max mem: 15572
Test:  [1070/1221]  eta: 0:00:43  loss: 2.0028 (2.3658)  acc1: 50.0000 (46.3741)  acc5: 83.3333 (78.1746)  time: 0.2797  data: 0.1375  max mem: 15572
Test:  [1080/1221]  eta: 0:00:40  loss: 2.1564 (2.3676)  acc1: 41.6667 (46.3074)  acc5: 75.0000 (78.1529)  time: 0.2662  data: 0.1187  max mem: 15572
Test:  [1090/1221]  eta: 0:00:37  loss: 2.7016 (2.3711)  acc1: 41.6667 (46.2343)  acc5: 66.6667 (78.0629)  time: 0.3134  data: 0.1623  max mem: 15572
Test:  [1100/1221]  eta: 0:00:34  loss: 2.6770 (2.3761)  acc1: 25.0000 (46.0869)  acc5: 66.6667 (77.9140)  time: 0.2987  data: 0.1469  max mem: 15572
Test:  [1110/1221]  eta: 0:00:31  loss: 2.7865 (2.3788)  acc1: 33.3333 (46.0696)  acc5: 66.6667 (77.8578)  time: 0.2932  data: 0.1317  max mem: 15572
Test:  [1120/1221]  eta: 0:00:28  loss: 2.7231 (2.3806)  acc1: 41.6667 (46.0452)  acc5: 75.0000 (77.8174)  time: 0.2749  data: 0.1066  max mem: 15572
Test:  [1130/1221]  eta: 0:00:26  loss: 2.3694 (2.3803)  acc1: 41.6667 (46.0433)  acc5: 75.0000 (77.8367)  time: 0.2472  data: 0.0928  max mem: 15572
Test:  [1140/1221]  eta: 0:00:23  loss: 2.4673 (2.3825)  acc1: 41.6667 (45.9685)  acc5: 75.0000 (77.7900)  time: 0.3098  data: 0.1639  max mem: 15572
Test:  [1150/1221]  eta: 0:00:20  loss: 2.6056 (2.3829)  acc1: 41.6667 (45.9745)  acc5: 75.0000 (77.7295)  time: 0.2910  data: 0.1421  max mem: 15572
Test:  [1160/1221]  eta: 0:00:17  loss: 2.5617 (2.3879)  acc1: 41.6667 (45.8010)  acc5: 66.6667 (77.5768)  time: 0.2610  data: 0.1099  max mem: 15572
Test:  [1170/1221]  eta: 0:00:14  loss: 2.2887 (2.3873)  acc1: 33.3333 (45.7871)  acc5: 75.0000 (77.5761)  time: 0.2745  data: 0.1288  max mem: 15572
Test:  [1180/1221]  eta: 0:00:11  loss: 2.2371 (2.3892)  acc1: 41.6667 (45.7875)  acc5: 83.3333 (77.5543)  time: 0.2804  data: 0.1424  max mem: 15572
Test:  [1190/1221]  eta: 0:00:08  loss: 2.4259 (2.3883)  acc1: 50.0000 (45.8718)  acc5: 75.0000 (77.5539)  time: 0.2818  data: 0.1506  max mem: 15572
Test:  [1200/1221]  eta: 0:00:05  loss: 2.2585 (2.3871)  acc1: 50.0000 (45.8993)  acc5: 83.3333 (77.6089)  time: 0.2088  data: 0.0842  max mem: 15572
Test:  [1210/1221]  eta: 0:00:03  loss: 2.0417 (2.3857)  acc1: 50.0000 (45.9675)  acc5: 83.3333 (77.6493)  time: 0.1219  data: 0.0099  max mem: 15572
Test:  [1220/1221]  eta: 0:00:00  loss: 1.8257 (2.3823)  acc1: 58.3333 (46.0782)  acc5: 91.6667 (77.7254)  time: 0.1003  data: 0.0001  max mem: 15572
Test: Total time: 0:05:44 (0.2820 s / it)
* Acc@1 46.146 Acc@5 77.637 loss 2.383
Start merging results...
Reading individual output files
Computing final results
4883
Accuracy of the network on the 29298 test videos: Top-1: 50.01%, Top-5: 80.59%
Training time 9:49:52
