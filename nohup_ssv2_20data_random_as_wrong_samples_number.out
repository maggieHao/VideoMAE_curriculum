/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-13 10:16:02,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fc4950ae980>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-13 10:16:05,509] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-13 10:16:05,509] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-13 10:16:05,533] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.04765582084655762 seconds
[2025-01-13 10:16:05,856] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-13 10:16:05,857] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-13 10:16:05,861] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-13 10:16:05,862] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-13 10:16:05,873] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-13 10:16:05,873] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-13 10:16:05,873] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-13 10:16:05,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 10:16:05,874] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc494576e90>
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-13 10:16:05,875] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-13 10:16:05,876] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-13 10:16:05,877] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-13 10:16:05,877] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
Epoch: [0]  [   0/2809]  eta: 17:49:20  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 22.8410  data: 12.0271  max mem: 15572
Epoch: [0]  [  10/2809]  eta: 1:54:21  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 2.4515  data: 1.0938  max mem: 15572
Epoch: [0]  [  20/2809]  eta: 1:09:18  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4237  data: 0.0005  max mem: 15572
Epoch: [0]  [  30/2809]  eta: 0:53:21  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4374  data: 0.0006  max mem: 15572
Epoch: [0]  [  40/2809]  eta: 0:46:37  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5052  data: 0.0534  max mem: 15572
Epoch: [0]  [  50/2809]  eta: 0:42:17  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0796  max mem: 15572
Epoch: [0]  [  60/2809]  eta: 0:39:15  lr: 0.000000  min_lr: 0.000000  loss: 5.1600 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0734  max mem: 15572
Epoch: [0]  [  70/2809]  eta: 0:37:46  lr: 0.000000  min_lr: 0.000000  loss: 5.1599 (5.1600)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1517  max mem: 15572
Epoch: [0]  [  80/2809]  eta: 0:36:11  lr: 0.000000  min_lr: 0.000000  loss: 5.1598 (5.1600)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1661  max mem: 15572
Epoch: [0]  [  90/2809]  eta: 0:35:14  lr: 0.000000  min_lr: 0.000000  loss: 5.1597 (5.1599)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1411  max mem: 15572
Epoch: [0]  [ 100/2809]  eta: 0:34:38  lr: 0.000000  min_lr: 0.000000  loss: 5.1594 (5.1599)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.1665  max mem: 15572
Epoch: [0]  [ 110/2809]  eta: 0:33:31  lr: 0.000000  min_lr: 0.000000  loss: 5.1590 (5.1598)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1183  max mem: 15572
Epoch: [0]  [ 120/2809]  eta: 0:32:45  lr: 0.000000  min_lr: 0.000000  loss: 5.1588 (5.1597)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0899  max mem: 15572
[2025-01-13 10:17:40,499] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:17:40,499] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/2809]  eta: 0:32:26  lr: 0.000000  min_lr: 0.000000  loss: 5.1586 (5.1596)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1617  max mem: 15572
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2136399 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2136372 got signal: 1
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-13 10:17:45,977] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f8d641be980>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-13 10:17:49,398] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-13 10:17:49,398] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-13 10:17:49,430] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.05914592742919922 seconds
[2025-01-13 10:17:49,792] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-13 10:17:49,793] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-13 10:17:49,795] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-13 10:17:49,795] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-13 10:17:49,802] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-13 10:17:49,802] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-13 10:17:49,802] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-13 10:17:49,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 10:17:49,803] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-13 10:17:49,803] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-13 10:17:49,803] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-13 10:17:49,803] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-13 10:17:49,803] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-13 10:17:49,803] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-13 10:17:49,803] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-13 10:17:49,803] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-13 10:17:49,803] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-13 10:17:49,803] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8d33aaee90>
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-13 10:17:49,804] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-13 10:17:49,805] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-13 10:17:49,805] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
Epoch: [0]  [   0/2809]  eta: 12:45:16  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 16.3461  data: 7.6693  max mem: 15572
Epoch: [0]  [  10/2809]  eta: 1:26:50  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.8614  data: 0.6976  max mem: 15572
Epoch: [0]  [  20/2809]  eta: 0:55:40  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4403  data: 0.0006  max mem: 15572
Epoch: [0]  [  30/2809]  eta: 0:44:05  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4518  data: 0.0009  max mem: 15572
Epoch: [0]  [  40/2809]  eta: 0:38:24  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4485  data: 0.0010  max mem: 15572
Epoch: [0]  [  50/2809]  eta: 0:35:45  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0499  max mem: 15572
Epoch: [0]  [  60/2809]  eta: 0:33:31  lr: 0.000000  min_lr: 0.000000  loss: 5.1600 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0703  max mem: 15572
Epoch: [0]  [  70/2809]  eta: 0:33:30  lr: 0.000000  min_lr: 0.000000  loss: 5.1599 (5.1600)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1723  max mem: 15572
Epoch: [0]  [  80/2809]  eta: 0:31:56  lr: 0.000000  min_lr: 0.000000  loss: 5.1598 (5.1600)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1757  max mem: 15572
Epoch: [0]  [  90/2809]  eta: 0:31:47  lr: 0.000000  min_lr: 0.000000  loss: 5.1597 (5.1599)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1509  max mem: 15572
Epoch: [0]  [ 100/2809]  eta: 0:31:11  lr: 0.000000  min_lr: 0.000000  loss: 5.1594 (5.1599)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.2036  max mem: 15572
Epoch: [0]  [ 110/2809]  eta: 0:30:52  lr: 0.000000  min_lr: 0.000000  loss: 5.1590 (5.1598)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1712  max mem: 15572
Epoch: [0]  [ 120/2809]  eta: 0:30:09  lr: 0.000000  min_lr: 0.000000  loss: 5.1588 (5.1597)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1323  max mem: 15572
[2025-01-13 10:19:15,123] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:19:15,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/2809]  eta: 0:29:49  lr: 0.000000  min_lr: 0.000000  loss: 5.1586 (5.1596)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1201  max mem: 15572
Epoch: [0]  [ 140/2809]  eta: 0:29:31  lr: 0.000000  min_lr: 0.000000  loss: 5.1583 (5.1595)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1621  max mem: 15572
Epoch: [0]  [ 150/2809]  eta: 0:28:57  lr: 0.000001  min_lr: 0.000000  loss: 5.1583 (5.1595)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1178  max mem: 15572
Epoch: [0]  [ 160/2809]  eta: 0:28:44  lr: 0.000001  min_lr: 0.000000  loss: 5.1584 (5.1594)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1278  max mem: 15572
Epoch: [0]  [ 170/2809]  eta: 0:28:34  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1593)  loss_scale: 256.0000 (160.1871)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1768  max mem: 15572
Epoch: [0]  [ 180/2809]  eta: 0:28:19  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1680  max mem: 15572
Epoch: [0]  [ 190/2809]  eta: 0:27:41  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  loss_scale: 256.0000 (170.2199)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0815  max mem: 15572
Epoch: [0]  [ 200/2809]  eta: 0:27:04  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1591)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 0.4082  data: 0.0004  max mem: 15572
Epoch: [0]  [ 210/2809]  eta: 0:26:38  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1591)  loss_scale: 256.0000 (178.3507)  weight_decay: 0.0500 (0.0500)  time: 0.4283  data: 0.0005  max mem: 15572
Epoch: [0]  [ 220/2809]  eta: 0:26:20  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1590)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.0500 (0.0500)  time: 0.4881  data: 0.0012  max mem: 15572
Epoch: [0]  [ 230/2809]  eta: 0:26:20  lr: 0.000001  min_lr: 0.000000  loss: 5.1576 (5.1590)  loss_scale: 256.0000 (185.0736)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1083  max mem: 15572
Epoch: [0]  [ 240/2809]  eta: 0:26:25  lr: 0.000001  min_lr: 0.000000  loss: 5.1574 (5.1589)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.0500 (0.0500)  time: 0.6910  data: 0.2137  max mem: 15572
Epoch: [0]  [ 250/2809]  eta: 0:26:30  lr: 0.000001  min_lr: 0.000000  loss: 5.1574 (5.1588)  loss_scale: 256.0000 (190.7251)  weight_decay: 0.0500 (0.0500)  time: 0.7210  data: 0.2326  max mem: 15572
[2025-01-13 10:20:29,098] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:20:29,099] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [ 260/2809]  eta: 0:26:30  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1588)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.0500 (0.0500)  time: 0.7059  data: 0.2368  max mem: 15572
Epoch: [0]  [ 270/2809]  eta: 0:26:23  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1587)  loss_scale: 512.0000 (209.7122)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.1750  max mem: 15572
Epoch: [0]  [ 280/2809]  eta: 0:26:25  lr: 0.000001  min_lr: 0.000000  loss: 5.1571 (5.1587)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.0500 (0.0500)  time: 0.6665  data: 0.1832  max mem: 15572
Epoch: [0]  [ 290/2809]  eta: 0:26:25  lr: 0.000001  min_lr: 0.000000  loss: 5.1567 (5.1586)  loss_scale: 512.0000 (230.4880)  weight_decay: 0.0500 (0.0500)  time: 0.7071  data: 0.2298  max mem: 15572
Epoch: [0]  [ 300/2809]  eta: 0:26:24  lr: 0.000001  min_lr: 0.000000  loss: 5.1563 (5.1585)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 0.6922  data: 0.2135  max mem: 15572
Epoch: [0]  [ 310/2809]  eta: 0:26:27  lr: 0.000001  min_lr: 0.000000  loss: 5.1563 (5.1584)  loss_scale: 512.0000 (248.5916)  weight_decay: 0.0500 (0.0500)  time: 0.7173  data: 0.2362  max mem: 15572
Epoch: [0]  [ 320/2809]  eta: 0:26:21  lr: 0.000001  min_lr: 0.000000  loss: 5.1562 (5.1584)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.0500 (0.0500)  time: 0.6963  data: 0.2185  max mem: 15572
Epoch: [0]  [ 330/2809]  eta: 0:26:21  lr: 0.000001  min_lr: 0.000000  loss: 5.1555 (5.1583)  loss_scale: 512.0000 (264.5076)  weight_decay: 0.0500 (0.0500)  time: 0.6818  data: 0.2149  max mem: 15572
Epoch: [0]  [ 340/2809]  eta: 0:26:10  lr: 0.000001  min_lr: 0.000000  loss: 5.1554 (5.1582)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.1949  max mem: 15572
Epoch: [0]  [ 350/2809]  eta: 0:25:51  lr: 0.000001  min_lr: 0.000000  loss: 5.1553 (5.1581)  loss_scale: 512.0000 (278.6097)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0881  max mem: 15572
Epoch: [0]  [ 360/2809]  eta: 0:25:37  lr: 0.000001  min_lr: 0.000000  loss: 5.1548 (5.1580)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.0500 (0.0500)  time: 0.4924  data: 0.0523  max mem: 15572
Epoch: [0]  [ 370/2809]  eta: 0:25:38  lr: 0.000001  min_lr: 0.000000  loss: 5.1541 (5.1579)  loss_scale: 512.0000 (291.1914)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.1683  max mem: 15572
Epoch: [0]  [ 380/2809]  eta: 0:25:28  lr: 0.000001  min_lr: 0.000000  loss: 5.1541 (5.1578)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.0500 (0.0500)  time: 0.6510  data: 0.2066  max mem: 15572
[2025-01-13 10:21:51,718] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:21:51,719] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [ 390/2809]  eta: 0:25:18  lr: 0.000001  min_lr: 0.000000  loss: 5.1535 (5.1577)  loss_scale: 512.0000 (311.6522)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1268  max mem: 15572
Epoch: [0]  [ 400/2809]  eta: 0:25:14  lr: 0.000001  min_lr: 0.000000  loss: 5.1520 (5.1576)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1693  max mem: 15572
Epoch: [0]  [ 410/2809]  eta: 0:25:07  lr: 0.000001  min_lr: 0.000000  loss: 5.1539 (5.1575)  loss_scale: 1024.0000 (346.3163)  weight_decay: 0.0500 (0.0500)  time: 0.6434  data: 0.2023  max mem: 15572
Epoch: [0]  [ 420/2809]  eta: 0:24:56  lr: 0.000001  min_lr: 0.000000  loss: 5.1532 (5.1573)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1058  max mem: 15572
Epoch: [0]  [ 430/2809]  eta: 0:24:52  lr: 0.000001  min_lr: 0.000000  loss: 5.1515 (5.1572)  loss_scale: 1024.0000 (377.7633)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1204  max mem: 15572
Epoch: [0]  [ 440/2809]  eta: 0:24:39  lr: 0.000001  min_lr: 0.000000  loss: 5.1515 (5.1571)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1080  max mem: 15572
Epoch: [0]  [ 450/2809]  eta: 0:24:33  lr: 0.000002  min_lr: 0.000000  loss: 5.1515 (5.1569)  loss_scale: 1024.0000 (406.4213)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0763  max mem: 15572
Epoch: [0]  [ 460/2809]  eta: 0:24:24  lr: 0.000002  min_lr: 0.000000  loss: 5.1496 (5.1568)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1324  max mem: 15572
Epoch: [0]  [ 470/2809]  eta: 0:24:19  lr: 0.000002  min_lr: 0.000000  loss: 5.1490 (5.1566)  loss_scale: 1024.0000 (432.6454)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1457  max mem: 15572
Epoch: [0]  [ 480/2809]  eta: 0:24:05  lr: 0.000002  min_lr: 0.000000  loss: 5.1484 (5.1564)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0894  max mem: 15572
Epoch: [0]  [ 490/2809]  eta: 0:23:57  lr: 0.000002  min_lr: 0.000000  loss: 5.1474 (5.1562)  loss_scale: 1024.0000 (456.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0433  max mem: 15572
Epoch: [0]  [ 500/2809]  eta: 0:23:47  lr: 0.000002  min_lr: 0.000000  loss: 5.1458 (5.1560)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0491  max mem: 15572
Epoch: [0]  [ 510/2809]  eta: 0:23:35  lr: 0.000002  min_lr: 0.000000  loss: 5.1456 (5.1558)  loss_scale: 1024.0000 (478.9354)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0176  max mem: 15572
[2025-01-13 10:23:05,656] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:23:05,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [0]  [ 520/2809]  eta: 0:23:24  lr: 0.000002  min_lr: 0.000000  loss: 5.1441 (5.1556)  loss_scale: 1024.0000 (507.0864)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0377  max mem: 15572
Epoch: [0]  [ 530/2809]  eta: 0:23:17  lr: 0.000002  min_lr: 0.000000  loss: 5.1440 (5.1553)  loss_scale: 2048.0000 (536.1055)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0986  max mem: 15572
Epoch: [0]  [ 540/2809]  eta: 0:23:08  lr: 0.000002  min_lr: 0.000000  loss: 5.1419 (5.1551)  loss_scale: 2048.0000 (564.0518)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1049  max mem: 15572
Epoch: [0]  [ 550/2809]  eta: 0:23:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1549)  loss_scale: 2048.0000 (590.9837)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1112  max mem: 15572
Epoch: [0]  [ 560/2809]  eta: 0:22:55  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1547)  loss_scale: 2048.0000 (616.9554)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1427  max mem: 15572
Epoch: [0]  [ 570/2809]  eta: 0:22:50  lr: 0.000002  min_lr: 0.000000  loss: 5.1395 (5.1545)  loss_scale: 2048.0000 (642.0175)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1586  max mem: 15572
Epoch: [0]  [ 580/2809]  eta: 0:22:43  lr: 0.000002  min_lr: 0.000000  loss: 5.1396 (5.1543)  loss_scale: 2048.0000 (666.2169)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1447  max mem: 15572
Epoch: [0]  [ 590/2809]  eta: 0:22:37  lr: 0.000002  min_lr: 0.000000  loss: 5.1376 (5.1540)  loss_scale: 2048.0000 (689.5973)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1345  max mem: 15572
Epoch: [0]  [ 600/2809]  eta: 0:22:30  lr: 0.000002  min_lr: 0.000000  loss: 5.1369 (5.1537)  loss_scale: 2048.0000 (712.1997)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1409  max mem: 15572
Epoch: [0]  [ 610/2809]  eta: 0:22:24  lr: 0.000002  min_lr: 0.000000  loss: 5.1369 (5.1535)  loss_scale: 2048.0000 (734.0622)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1359  max mem: 15572
Epoch: [0]  [ 620/2809]  eta: 0:22:16  lr: 0.000002  min_lr: 0.000000  loss: 5.1334 (5.1531)  loss_scale: 2048.0000 (755.2206)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1204  max mem: 15572
Epoch: [0]  [ 630/2809]  eta: 0:22:09  lr: 0.000002  min_lr: 0.000000  loss: 5.1308 (5.1529)  loss_scale: 2048.0000 (775.7084)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1106  max mem: 15572
[2025-01-13 10:24:21,611] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:24:21,611] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 640/2809]  eta: 0:22:04  lr: 0.000002  min_lr: 0.000000  loss: 5.1382 (5.1527)  loss_scale: 2048.0000 (798.7520)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1598  max mem: 15572
Epoch: [0]  [ 650/2809]  eta: 0:21:56  lr: 0.000002  min_lr: 0.000000  loss: 5.1335 (5.1523)  loss_scale: 4096.0000 (849.4009)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1364  max mem: 15572
Epoch: [0]  [ 660/2809]  eta: 0:21:52  lr: 0.000002  min_lr: 0.000000  loss: 5.1286 (5.1520)  loss_scale: 4096.0000 (898.5174)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1372  max mem: 15572
Epoch: [0]  [ 670/2809]  eta: 0:21:43  lr: 0.000002  min_lr: 0.000000  loss: 5.1264 (5.1516)  loss_scale: 4096.0000 (946.1699)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1065  max mem: 15572
Epoch: [0]  [ 680/2809]  eta: 0:21:37  lr: 0.000002  min_lr: 0.000000  loss: 5.1207 (5.1512)  loss_scale: 4096.0000 (992.4229)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0900  max mem: 15572
Epoch: [0]  [ 690/2809]  eta: 0:21:30  lr: 0.000002  min_lr: 0.000000  loss: 5.1207 (5.1509)  loss_scale: 4096.0000 (1037.3372)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1109  max mem: 15572
Epoch: [0]  [ 700/2809]  eta: 0:21:23  lr: 0.000002  min_lr: 0.000000  loss: 5.1347 (5.1506)  loss_scale: 4096.0000 (1080.9700)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0834  max mem: 15572
Epoch: [0]  [ 710/2809]  eta: 0:21:15  lr: 0.000002  min_lr: 0.000000  loss: 5.1334 (5.1503)  loss_scale: 4096.0000 (1123.3755)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0858  max mem: 15572
Epoch: [0]  [ 720/2809]  eta: 0:21:08  lr: 0.000002  min_lr: 0.000000  loss: 5.1228 (5.1499)  loss_scale: 4096.0000 (1164.6047)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0982  max mem: 15572
Epoch: [0]  [ 730/2809]  eta: 0:21:01  lr: 0.000002  min_lr: 0.000000  loss: 5.1228 (5.1495)  loss_scale: 4096.0000 (1204.7059)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1179  max mem: 15572
Epoch: [0]  [ 740/2809]  eta: 0:20:56  lr: 0.000002  min_lr: 0.000000  loss: 5.1246 (5.1492)  loss_scale: 4096.0000 (1243.7247)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1697  max mem: 15572
Epoch: [0]  [ 750/2809]  eta: 0:20:52  lr: 0.000003  min_lr: 0.000000  loss: 5.1155 (5.1487)  loss_scale: 4096.0000 (1281.7044)  weight_decay: 0.0500 (0.0500)  time: 0.6546  data: 0.2303  max mem: 15572
Epoch: [0]  [ 760/2809]  eta: 0:20:48  lr: 0.000003  min_lr: 0.000000  loss: 5.1155 (5.1483)  loss_scale: 4096.0000 (1318.6859)  weight_decay: 0.0500 (0.0500)  time: 0.6740  data: 0.2221  max mem: 15572
[2025-01-13 10:25:39,599] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:25:39,599] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [0]  [ 770/2809]  eta: 0:20:43  lr: 0.000003  min_lr: 0.000000  loss: 5.1169 (5.1479)  loss_scale: 4096.0000 (1370.6459)  weight_decay: 0.0500 (0.0500)  time: 0.6664  data: 0.2043  max mem: 15572
Epoch: [0]  [ 780/2809]  eta: 0:20:36  lr: 0.000003  min_lr: 0.000000  loss: 5.1170 (5.1475)  loss_scale: 8192.0000 (1457.9872)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1568  max mem: 15572
Epoch: [0]  [ 790/2809]  eta: 0:20:27  lr: 0.000003  min_lr: 0.000000  loss: 5.1164 (5.1472)  loss_scale: 8192.0000 (1543.1201)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0826  max mem: 15572
Epoch: [0]  [ 800/2809]  eta: 0:20:20  lr: 0.000003  min_lr: 0.000000  loss: 5.1091 (5.1467)  loss_scale: 8192.0000 (1626.1273)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0801  max mem: 15572
Epoch: [0]  [ 810/2809]  eta: 0:20:14  lr: 0.000003  min_lr: 0.000000  loss: 5.1091 (5.1463)  loss_scale: 8192.0000 (1707.0875)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1209  max mem: 15572
Epoch: [0]  [ 820/2809]  eta: 0:20:08  lr: 0.000003  min_lr: 0.000000  loss: 5.1146 (5.1460)  loss_scale: 8192.0000 (1786.0755)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1378  max mem: 15572
Epoch: [0]  [ 830/2809]  eta: 0:20:02  lr: 0.000003  min_lr: 0.000000  loss: 5.1068 (5.1455)  loss_scale: 8192.0000 (1863.1625)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1672  max mem: 15572
Epoch: [0]  [ 840/2809]  eta: 0:19:55  lr: 0.000003  min_lr: 0.000000  loss: 5.1024 (5.1450)  loss_scale: 8192.0000 (1938.4162)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1761  max mem: 15572
Epoch: [0]  [ 850/2809]  eta: 0:19:49  lr: 0.000003  min_lr: 0.000000  loss: 5.1057 (5.1446)  loss_scale: 8192.0000 (2011.9013)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1458  max mem: 15572
Epoch: [0]  [ 860/2809]  eta: 0:19:45  lr: 0.000003  min_lr: 0.000000  loss: 5.1015 (5.1442)  loss_scale: 8192.0000 (2083.6794)  weight_decay: 0.0500 (0.0500)  time: 0.6629  data: 0.1883  max mem: 15572
Epoch: [0]  [ 870/2809]  eta: 0:19:36  lr: 0.000003  min_lr: 0.000000  loss: 5.1044 (5.1439)  loss_scale: 8192.0000 (2153.8094)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1219  max mem: 15572
Epoch: [0]  [ 880/2809]  eta: 0:19:30  lr: 0.000003  min_lr: 0.000000  loss: 5.1025 (5.1433)  loss_scale: 8192.0000 (2222.3473)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0561  max mem: 15572
Epoch: [0]  [ 890/2809]  eta: 0:19:23  lr: 0.000003  min_lr: 0.000000  loss: 5.0929 (5.1429)  loss_scale: 8192.0000 (2289.3468)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0997  max mem: 15572
[2025-01-13 10:26:53,188] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:26:53,188] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [0]  [ 900/2809]  eta: 0:19:17  lr: 0.000003  min_lr: 0.000000  loss: 5.1061 (5.1425)  loss_scale: 8192.0000 (2400.3196)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1333  max mem: 15572
Epoch: [0]  [ 910/2809]  eta: 0:19:11  lr: 0.000003  min_lr: 0.000000  loss: 5.1046 (5.1421)  loss_scale: 16384.0000 (2553.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1618  max mem: 15572
Epoch: [0]  [ 920/2809]  eta: 0:19:04  lr: 0.000003  min_lr: 0.000000  loss: 5.1040 (5.1418)  loss_scale: 16384.0000 (2703.9826)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1295  max mem: 15572
Epoch: [0]  [ 930/2809]  eta: 0:18:58  lr: 0.000003  min_lr: 0.000000  loss: 5.1048 (5.1414)  loss_scale: 16384.0000 (2850.9216)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1363  max mem: 15572
Epoch: [0]  [ 940/2809]  eta: 0:18:53  lr: 0.000003  min_lr: 0.000000  loss: 5.1055 (5.1410)  loss_scale: 16384.0000 (2994.7375)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.1724  max mem: 15572
Epoch: [0]  [ 950/2809]  eta: 0:18:48  lr: 0.000003  min_lr: 0.000000  loss: 5.1055 (5.1407)  loss_scale: 16384.0000 (3135.5289)  weight_decay: 0.0500 (0.0500)  time: 0.6619  data: 0.1928  max mem: 15572
Epoch: [0]  [ 960/2809]  eta: 0:18:41  lr: 0.000003  min_lr: 0.000000  loss: 5.1051 (5.1402)  loss_scale: 16384.0000 (3273.3902)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1499  max mem: 15572
Epoch: [0]  [ 970/2809]  eta: 0:18:36  lr: 0.000003  min_lr: 0.000000  loss: 5.1051 (5.1399)  loss_scale: 16384.0000 (3408.4119)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1468  max mem: 15572
Epoch: [0]  [ 980/2809]  eta: 0:18:28  lr: 0.000003  min_lr: 0.000000  loss: 5.1015 (5.1394)  loss_scale: 16384.0000 (3540.6809)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1236  max mem: 15572
Epoch: [0]  [ 990/2809]  eta: 0:18:22  lr: 0.000003  min_lr: 0.000000  loss: 5.0866 (5.1387)  loss_scale: 16384.0000 (3670.2805)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0969  max mem: 15572
[2025-01-13 10:27:57,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[3.2306541515702744e-08, 3.2306541515702744e-08, 4.615220216528964e-08, 4.615220216528964e-08, 6.59317173789852e-08, 6.59317173789852e-08, 9.418816768426457e-08, 9.418816768426457e-08, 1.3455452526323513e-07, 1.3455452526323513e-07, 1.9222075037605018e-07, 1.9222075037605018e-07, 2.7460107196578597e-07, 2.7460107196578597e-07, 3.922872456654086e-07, 3.922872456654086e-07, 5.604103509505837e-07, 5.604103509505837e-07, 8.005862156436911e-07, 8.005862156436911e-07, 1.1436945937767016e-06, 1.1436945937767016e-06, 1.6338494196810024e-06, 1.6338494196810024e-06, 2.3340705995442894e-06, 2.3340705995442894e-06, 3.3343865707775563e-06, 3.3343865707775563e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 10:27:57,292] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=27.52042647941961, CurrSamplesPerSec=29.492092827028944, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [1000/2809]  eta: 0:18:18  lr: 0.000003  min_lr: 0.000000  loss: 5.0870 (5.1383)  loss_scale: 16384.0000 (3797.2907)  weight_decay: 0.0500 (0.0500)  time: 0.6612  data: 0.1645  max mem: 15572
Epoch: [0]  [1010/2809]  eta: 0:18:10  lr: 0.000003  min_lr: 0.000000  loss: 5.0940 (5.1380)  loss_scale: 16384.0000 (3921.7883)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0949  max mem: 15572
Epoch: [0]  [1020/2809]  eta: 0:18:03  lr: 0.000003  min_lr: 0.000000  loss: 5.0888 (5.1375)  loss_scale: 16384.0000 (4043.8472)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0571  max mem: 15572
[2025-01-13 10:28:10,787] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:28:10,788] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [0]  [1030/2809]  eta: 0:17:57  lr: 0.000003  min_lr: 0.000000  loss: 5.0857 (5.1370)  loss_scale: 16384.0000 (4274.7779)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1116  max mem: 15572
Epoch: [0]  [1040/2809]  eta: 0:17:50  lr: 0.000003  min_lr: 0.000000  loss: 5.0919 (5.1368)  loss_scale: 32768.0000 (4548.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1151  max mem: 15572
Epoch: [0]  [1050/2809]  eta: 0:17:42  lr: 0.000004  min_lr: 0.000000  loss: 5.1028 (5.1364)  loss_scale: 32768.0000 (4816.9895)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0845  max mem: 15572
Epoch: [0]  [1060/2809]  eta: 0:17:38  lr: 0.000004  min_lr: 0.000000  loss: 5.0846 (5.1359)  loss_scale: 32768.0000 (5080.4298)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1254  max mem: 15572
Epoch: [0]  [1070/2809]  eta: 0:17:30  lr: 0.000004  min_lr: 0.000000  loss: 5.0781 (5.1354)  loss_scale: 32768.0000 (5338.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1177  max mem: 15572
Epoch: [0]  [1080/2809]  eta: 0:17:26  lr: 0.000004  min_lr: 0.000000  loss: 5.0781 (5.1348)  loss_scale: 32768.0000 (5592.6883)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1386  max mem: 15572
Epoch: [0]  [1090/2809]  eta: 0:17:20  lr: 0.000004  min_lr: 0.000000  loss: 5.0771 (5.1344)  loss_scale: 32768.0000 (5841.7745)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.1848  max mem: 15572
Epoch: [0]  [1100/2809]  eta: 0:17:13  lr: 0.000004  min_lr: 0.000000  loss: 5.0976 (5.1341)  loss_scale: 32768.0000 (6086.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0889  max mem: 15572
Epoch: [0]  [1110/2809]  eta: 0:17:08  lr: 0.000004  min_lr: 0.000000  loss: 5.0843 (5.1335)  loss_scale: 32768.0000 (6326.4950)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1257  max mem: 15572
Epoch: [0]  [1120/2809]  eta: 0:17:01  lr: 0.000004  min_lr: 0.000000  loss: 5.0730 (5.1331)  loss_scale: 32768.0000 (6562.3693)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1401  max mem: 15572
Epoch: [0]  [1130/2809]  eta: 0:16:54  lr: 0.000004  min_lr: 0.000000  loss: 5.0699 (5.1326)  loss_scale: 32768.0000 (6794.0725)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0771  max mem: 15572
Epoch: [0]  [1140/2809]  eta: 0:16:47  lr: 0.000004  min_lr: 0.000000  loss: 5.0726 (5.1321)  loss_scale: 32768.0000 (7021.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0801  max mem: 15572
Epoch: [0]  [1150/2809]  eta: 0:16:40  lr: 0.000004  min_lr: 0.000000  loss: 5.0859 (5.1317)  loss_scale: 32768.0000 (7245.4005)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0990  max mem: 15572
[2025-01-13 10:29:25,654] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:29:25,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [0]  [1160/2809]  eta: 0:16:34  lr: 0.000004  min_lr: 0.000000  loss: 5.0859 (5.1313)  loss_scale: 32768.0000 (7719.2489)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0953  max mem: 15572
Epoch: [0]  [1170/2809]  eta: 0:16:27  lr: 0.000004  min_lr: 0.000000  loss: 5.0784 (5.1307)  loss_scale: 65536.0000 (8212.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0758  max mem: 15572
Epoch: [0]  [1180/2809]  eta: 0:16:20  lr: 0.000004  min_lr: 0.000000  loss: 5.0701 (5.1303)  loss_scale: 65536.0000 (8698.3641)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1031  max mem: 15572
Epoch: [0]  [1190/2809]  eta: 0:16:16  lr: 0.000004  min_lr: 0.000000  loss: 5.0956 (5.1299)  loss_scale: 65536.0000 (9175.5903)  weight_decay: 0.0500 (0.0500)  time: 0.6587  data: 0.1940  max mem: 15572
Epoch: [0]  [1200/2809]  eta: 0:16:09  lr: 0.000004  min_lr: 0.000000  loss: 5.0828 (5.1295)  loss_scale: 65536.0000 (9644.8693)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.1671  max mem: 15572
Epoch: [0]  [1210/2809]  eta: 0:16:03  lr: 0.000004  min_lr: 0.000000  loss: 5.0916 (5.1291)  loss_scale: 65536.0000 (10106.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0949  max mem: 15572
Epoch: [0]  [1220/2809]  eta: 0:15:57  lr: 0.000004  min_lr: 0.000000  loss: 5.0774 (5.1286)  loss_scale: 65536.0000 (10560.3669)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1342  max mem: 15572
Epoch: [0]  [1230/2809]  eta: 0:15:52  lr: 0.000004  min_lr: 0.000000  loss: 5.0682 (5.1283)  loss_scale: 65536.0000 (11006.9602)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.1640  max mem: 15572
Epoch: [0]  [1240/2809]  eta: 0:15:46  lr: 0.000004  min_lr: 0.000000  loss: 5.0855 (5.1280)  loss_scale: 65536.0000 (11446.3562)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1583  max mem: 15572
Epoch: [0]  [1250/2809]  eta: 0:15:39  lr: 0.000004  min_lr: 0.000000  loss: 5.0822 (5.1276)  loss_scale: 65536.0000 (11878.7274)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1061  max mem: 15572
Epoch: [0]  [1260/2809]  eta: 0:15:32  lr: 0.000004  min_lr: 0.000000  loss: 5.0856 (5.1273)  loss_scale: 65536.0000 (12304.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0994  max mem: 15572
Epoch: [0]  [1270/2809]  eta: 0:15:27  lr: 0.000004  min_lr: 0.000000  loss: 5.0768 (5.1269)  loss_scale: 65536.0000 (12723.0590)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1326  max mem: 15572
[2025-01-13 10:30:42,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:30:42,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [0]  [1280/2809]  eta: 0:15:21  lr: 0.000004  min_lr: 0.000000  loss: 5.0672 (5.1266)  loss_scale: 65536.0000 (13186.4980)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1528  max mem: 15572
Epoch: [0]  [1290/2809]  eta: 0:15:15  lr: 0.000004  min_lr: 0.000000  loss: 5.0922 (5.1264)  loss_scale: 131072.0000 (14099.6313)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.1779  max mem: 15572
Epoch: [0]  [1300/2809]  eta: 0:15:10  lr: 0.000004  min_lr: 0.000000  loss: 5.0757 (5.1259)  loss_scale: 131072.0000 (14998.7271)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1641  max mem: 15572
Epoch: [0]  [1310/2809]  eta: 0:15:03  lr: 0.000004  min_lr: 0.000000  loss: 5.0489 (5.1253)  loss_scale: 131072.0000 (15884.1068)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1288  max mem: 15572
Epoch: [0]  [1320/2809]  eta: 0:14:58  lr: 0.000004  min_lr: 0.000000  loss: 5.0517 (5.1248)  loss_scale: 131072.0000 (16756.0818)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1462  max mem: 15572
Epoch: [0]  [1330/2809]  eta: 0:14:50  lr: 0.000004  min_lr: 0.000000  loss: 5.0613 (5.1244)  loss_scale: 131072.0000 (17614.9542)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0911  max mem: 15572
Epoch: [0]  [1340/2809]  eta: 0:14:44  lr: 0.000004  min_lr: 0.000000  loss: 5.0613 (5.1239)  loss_scale: 131072.0000 (18461.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5083  data: 0.0132  max mem: 15572
Epoch: [0]  [1350/2809]  eta: 0:14:38  lr: 0.000005  min_lr: 0.000000  loss: 5.0655 (5.1235)  loss_scale: 131072.0000 (19294.5551)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0807  max mem: 15572
Epoch: [0]  [1360/2809]  eta: 0:14:32  lr: 0.000005  min_lr: 0.000000  loss: 5.0695 (5.1231)  loss_scale: 131072.0000 (20115.8442)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1273  max mem: 15572
Epoch: [0]  [1370/2809]  eta: 0:14:25  lr: 0.000005  min_lr: 0.000000  loss: 5.0580 (5.1226)  loss_scale: 131072.0000 (20925.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0601  max mem: 15572
Epoch: [0]  [1380/2809]  eta: 0:14:18  lr: 0.000005  min_lr: 0.000000  loss: 5.0384 (5.1220)  loss_scale: 131072.0000 (21722.7400)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0201  max mem: 15572
Epoch: [0]  [1390/2809]  eta: 0:14:11  lr: 0.000005  min_lr: 0.000000  loss: 5.0521 (5.1216)  loss_scale: 131072.0000 (22508.8598)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0201  max mem: 15572
Epoch: [0]  [1400/2809]  eta: 0:14:05  lr: 0.000005  min_lr: 0.000000  loss: 5.0647 (5.1213)  loss_scale: 131072.0000 (23283.7573)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0012  max mem: 15572
[2025-01-13 10:31:57,497] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:31:57,498] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
Epoch: [0]  [1410/2809]  eta: 0:14:00  lr: 0.000005  min_lr: 0.000000  loss: 5.0692 (5.1209)  loss_scale: 131072.0000 (24326.3501)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.1280  max mem: 15572
Epoch: [0]  [1420/2809]  eta: 0:13:55  lr: 0.000005  min_lr: 0.000000  loss: 5.0708 (5.1204)  loss_scale: 262144.0000 (25999.9437)  weight_decay: 0.0500 (0.0500)  time: 0.6951  data: 0.1734  max mem: 15572
[2025-01-13 10:32:10,100] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1430
[2025-01-13 10:32:10,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
[2025-01-13 10:32:10,101] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144, reducing to 131072.0
Epoch: [0]  [1430/2809]  eta: 0:13:48  lr: 0.000005  min_lr: 0.000000  loss: 5.0685 (5.1199)  loss_scale: 262144.0000 (27558.5521)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0535  max mem: 15572
Epoch: [0]  [1440/2809]  eta: 0:13:44  lr: 0.000005  min_lr: 0.000000  loss: 5.0702 (5.1197)  loss_scale: 131072.0000 (28276.8966)  weight_decay: 0.0500 (0.0500)  time: 0.6963  data: 0.2221  max mem: 15572
Epoch: [0]  [1450/2809]  eta: 0:13:37  lr: 0.000005  min_lr: 0.000000  loss: 5.0621 (5.1192)  loss_scale: 131072.0000 (28985.3398)  weight_decay: 0.0500 (0.0500)  time: 0.6812  data: 0.2151  max mem: 15572
Epoch: [0]  [1460/2809]  eta: 0:13:31  lr: 0.000005  min_lr: 0.000000  loss: 5.0597 (5.1188)  loss_scale: 131072.0000 (29684.0849)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0856  max mem: 15572
Epoch: [0]  [1470/2809]  eta: 0:13:25  lr: 0.000005  min_lr: 0.000000  loss: 5.0780 (5.1186)  loss_scale: 131072.0000 (30373.3297)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1042  max mem: 15572
Epoch: [0]  [1480/2809]  eta: 0:13:19  lr: 0.000005  min_lr: 0.000000  loss: 5.0553 (5.1182)  loss_scale: 131072.0000 (31053.2667)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1211  max mem: 15572
Epoch: [0]  [1490/2809]  eta: 0:13:13  lr: 0.000005  min_lr: 0.000000  loss: 5.0651 (5.1178)  loss_scale: 131072.0000 (31724.0832)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1785  max mem: 15572
Epoch: [0]  [1500/2809]  eta: 0:13:07  lr: 0.000005  min_lr: 0.000000  loss: 5.0768 (5.1175)  loss_scale: 131072.0000 (32385.9614)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1348  max mem: 15572
Epoch: [0]  [1510/2809]  eta: 0:13:00  lr: 0.000005  min_lr: 0.000000  loss: 5.0635 (5.1170)  loss_scale: 131072.0000 (33039.0788)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1092  max mem: 15572
Epoch: [0]  [1520/2809]  eta: 0:12:54  lr: 0.000005  min_lr: 0.000000  loss: 5.0512 (5.1166)  loss_scale: 131072.0000 (33683.6082)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1073  max mem: 15572
Epoch: [0]  [1530/2809]  eta: 0:12:48  lr: 0.000005  min_lr: 0.000000  loss: 5.0512 (5.1163)  loss_scale: 131072.0000 (34319.7178)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0917  max mem: 15572
Epoch: [0]  [1540/2809]  eta: 0:12:42  lr: 0.000005  min_lr: 0.000000  loss: 5.0472 (5.1159)  loss_scale: 131072.0000 (34947.5717)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0614  max mem: 15572
Epoch: [0]  [1550/2809]  eta: 0:12:35  lr: 0.000005  min_lr: 0.000000  loss: 5.0354 (5.1154)  loss_scale: 131072.0000 (35567.3295)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0390  max mem: 15572
[2025-01-13 10:33:26,079] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:33:26,079] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1560/2809]  eta: 0:12:29  lr: 0.000005  min_lr: 0.000000  loss: 5.0520 (5.1152)  loss_scale: 131072.0000 (36347.0801)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0444  max mem: 15572
Epoch: [0]  [1570/2809]  eta: 0:12:23  lr: 0.000005  min_lr: 0.000000  loss: 5.0554 (5.1148)  loss_scale: 262144.0000 (37784.3616)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0764  max mem: 15572
Epoch: [0]  [1580/2809]  eta: 0:12:16  lr: 0.000005  min_lr: 0.000000  loss: 5.0238 (5.1142)  loss_scale: 262144.0000 (39203.4611)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.0937  max mem: 15572
Epoch: [0]  [1590/2809]  eta: 0:12:10  lr: 0.000005  min_lr: 0.000000  loss: 5.0186 (5.1136)  loss_scale: 262144.0000 (40604.7216)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0824  max mem: 15572
[2025-01-13 10:33:48,700] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1599
[2025-01-13 10:33:48,701] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:33:48,701] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1600/2809]  eta: 0:12:03  lr: 0.000005  min_lr: 0.000000  loss: 5.0194 (5.1131)  loss_scale: 262144.0000 (41824.7395)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0588  max mem: 15572
Epoch: [0]  [1610/2809]  eta: 0:11:58  lr: 0.000005  min_lr: 0.000000  loss: 5.0479 (5.1127)  loss_scale: 131072.0000 (42378.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1087  max mem: 15572
Epoch: [0]  [1620/2809]  eta: 0:11:52  lr: 0.000005  min_lr: 0.000000  loss: 5.0526 (5.1123)  loss_scale: 131072.0000 (42925.8779)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1686  max mem: 15572
Epoch: [0]  [1630/2809]  eta: 0:11:46  lr: 0.000005  min_lr: 0.000000  loss: 5.0335 (5.1119)  loss_scale: 131072.0000 (43466.3200)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.2040  max mem: 15572
Epoch: [0]  [1640/2809]  eta: 0:11:41  lr: 0.000005  min_lr: 0.000000  loss: 5.0410 (5.1115)  loss_scale: 131072.0000 (44000.1755)  weight_decay: 0.0500 (0.0500)  time: 0.6529  data: 0.2108  max mem: 15572
Epoch: [0]  [1650/2809]  eta: 0:11:35  lr: 0.000006  min_lr: 0.000000  loss: 5.0617 (5.1111)  loss_scale: 131072.0000 (44527.5639)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1512  max mem: 15572
Epoch: [0]  [1660/2809]  eta: 0:11:29  lr: 0.000006  min_lr: 0.000000  loss: 5.0134 (5.1104)  loss_scale: 131072.0000 (45048.6020)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1163  max mem: 15572
Epoch: [0]  [1670/2809]  eta: 0:11:23  lr: 0.000006  min_lr: 0.000000  loss: 5.0045 (5.1100)  loss_scale: 131072.0000 (45563.4039)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1281  max mem: 15572
Epoch: [0]  [1680/2809]  eta: 0:11:17  lr: 0.000006  min_lr: 0.000000  loss: 5.0359 (5.1098)  loss_scale: 131072.0000 (46072.0809)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1206  max mem: 15572
Epoch: [0]  [1690/2809]  eta: 0:11:11  lr: 0.000006  min_lr: 0.000000  loss: 5.0668 (5.1095)  loss_scale: 131072.0000 (46574.7416)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1505  max mem: 15572
Epoch: [0]  [1700/2809]  eta: 0:11:04  lr: 0.000006  min_lr: 0.000000  loss: 5.0668 (5.1094)  loss_scale: 131072.0000 (47071.4921)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1289  max mem: 15572
Epoch: [0]  [1710/2809]  eta: 0:10:59  lr: 0.000006  min_lr: 0.000000  loss: 5.0553 (5.1089)  loss_scale: 131072.0000 (47562.4360)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1808  max mem: 15572
Epoch: [0]  [1720/2809]  eta: 0:10:53  lr: 0.000006  min_lr: 0.000000  loss: 5.0238 (5.1084)  loss_scale: 131072.0000 (48047.6746)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1763  max mem: 15572
[2025-01-13 10:35:09,057] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:35:09,058] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1730/2809]  eta: 0:10:47  lr: 0.000006  min_lr: 0.000000  loss: 5.0297 (5.1080)  loss_scale: 131072.0000 (48754.4679)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1327  max mem: 15572
Epoch: [0]  [1740/2809]  eta: 0:10:41  lr: 0.000006  min_lr: 0.000000  loss: 5.0292 (5.1075)  loss_scale: 262144.0000 (49980.1401)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1096  max mem: 15572
Epoch: [0]  [1750/2809]  eta: 0:10:34  lr: 0.000006  min_lr: 0.000000  loss: 5.0127 (5.1070)  loss_scale: 262144.0000 (51191.8127)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0014  max mem: 15572
Epoch: [0]  [1760/2809]  eta: 0:10:28  lr: 0.000006  min_lr: 0.000000  loss: 5.0273 (5.1067)  loss_scale: 262144.0000 (52389.7240)  weight_decay: 0.0500 (0.0500)  time: 0.4974  data: 0.0012  max mem: 15572
Epoch: [0]  [1770/2809]  eta: 0:10:22  lr: 0.000006  min_lr: 0.000000  loss: 5.0398 (5.1062)  loss_scale: 262144.0000 (53574.1073)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0510  max mem: 15572
[2025-01-13 10:35:34,459] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1776
[2025-01-13 10:35:34,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:35:34,459] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1780/2809]  eta: 0:10:15  lr: 0.000006  min_lr: 0.000000  loss: 5.0520 (5.1059)  loss_scale: 262144.0000 (54377.2173)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0696  max mem: 15572
Epoch: [0]  [1790/2809]  eta: 0:10:10  lr: 0.000006  min_lr: 0.000000  loss: 5.0756 (5.1056)  loss_scale: 131072.0000 (54805.4405)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1102  max mem: 15572
Epoch: [0]  [1800/2809]  eta: 0:10:04  lr: 0.000006  min_lr: 0.000000  loss: 5.0756 (5.1054)  loss_scale: 131072.0000 (55228.9084)  weight_decay: 0.0500 (0.0500)  time: 0.6865  data: 0.1781  max mem: 15572
Epoch: [0]  [1810/2809]  eta: 0:09:58  lr: 0.000006  min_lr: 0.000000  loss: 5.0073 (5.1048)  loss_scale: 131072.0000 (55647.6996)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0872  max mem: 15572
Epoch: [0]  [1820/2809]  eta: 0:09:52  lr: 0.000006  min_lr: 0.000000  loss: 5.0470 (5.1046)  loss_scale: 131072.0000 (56061.8913)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0895  max mem: 15572
Epoch: [0]  [1830/2809]  eta: 0:09:46  lr: 0.000006  min_lr: 0.000000  loss: 5.0751 (5.1045)  loss_scale: 131072.0000 (56471.5587)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0959  max mem: 15572
Epoch: [0]  [1840/2809]  eta: 0:09:40  lr: 0.000006  min_lr: 0.000000  loss: 5.0669 (5.1042)  loss_scale: 131072.0000 (56876.7757)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0710  max mem: 15572
Epoch: [0]  [1850/2809]  eta: 0:09:33  lr: 0.000006  min_lr: 0.000000  loss: 5.0295 (5.1039)  loss_scale: 131072.0000 (57277.6143)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0922  max mem: 15572
Epoch: [0]  [1860/2809]  eta: 0:09:28  lr: 0.000006  min_lr: 0.000000  loss: 5.0140 (5.1034)  loss_scale: 131072.0000 (57674.1451)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0946  max mem: 15572
Epoch: [0]  [1870/2809]  eta: 0:09:21  lr: 0.000006  min_lr: 0.000000  loss: 5.0332 (5.1030)  loss_scale: 131072.0000 (58066.4372)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1022  max mem: 15572
Epoch: [0]  [1880/2809]  eta: 0:09:15  lr: 0.000006  min_lr: 0.000000  loss: 5.0503 (5.1027)  loss_scale: 131072.0000 (58454.5582)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0544  max mem: 15572
Epoch: [0]  [1890/2809]  eta: 0:09:09  lr: 0.000006  min_lr: 0.000000  loss: 5.0270 (5.1022)  loss_scale: 131072.0000 (58838.5743)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0442  max mem: 15572
Epoch: [0]  [1900/2809]  eta: 0:09:03  lr: 0.000006  min_lr: 0.000000  loss: 5.0328 (5.1021)  loss_scale: 131072.0000 (59218.5502)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0519  max mem: 15572
[2025-01-13 10:36:51,095] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:36:51,095] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-13 10:36:53,096] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1909
[2025-01-13 10:36:53,096] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:36:53,096] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1910/2809]  eta: 0:08:57  lr: 0.000006  min_lr: 0.000000  loss: 5.0328 (5.1017)  loss_scale: 131072.0000 (59868.9021)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1328  max mem: 15572
Epoch: [0]  [1920/2809]  eta: 0:08:51  lr: 0.000006  min_lr: 0.000000  loss: 5.0367 (5.1014)  loss_scale: 131072.0000 (60239.5586)  weight_decay: 0.0500 (0.0500)  time: 0.6615  data: 0.2000  max mem: 15572
Epoch: [0]  [1930/2809]  eta: 0:08:45  lr: 0.000006  min_lr: 0.000000  loss: 4.9997 (5.1008)  loss_scale: 131072.0000 (60606.3760)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1492  max mem: 15572
Epoch: [0]  [1940/2809]  eta: 0:08:39  lr: 0.000006  min_lr: 0.000000  loss: 4.9997 (5.1004)  loss_scale: 131072.0000 (60969.4137)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1190  max mem: 15572
Epoch: [0]  [1950/2809]  eta: 0:08:33  lr: 0.000007  min_lr: 0.000000  loss: 5.0086 (5.1000)  loss_scale: 131072.0000 (61328.7299)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0926  max mem: 15572
Epoch: [0]  [1960/2809]  eta: 0:08:27  lr: 0.000007  min_lr: 0.000000  loss: 4.9853 (5.0996)  loss_scale: 131072.0000 (61684.3814)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0812  max mem: 15572
Epoch: [0]  [1970/2809]  eta: 0:08:21  lr: 0.000007  min_lr: 0.000000  loss: 4.9785 (5.0991)  loss_scale: 131072.0000 (62036.4242)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0935  max mem: 15572
Epoch: [0]  [1980/2809]  eta: 0:08:15  lr: 0.000007  min_lr: 0.000000  loss: 5.0137 (5.0989)  loss_scale: 131072.0000 (62384.9127)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1592  max mem: 15572
Epoch: [0]  [1990/2809]  eta: 0:08:09  lr: 0.000007  min_lr: 0.000000  loss: 5.0183 (5.0985)  loss_scale: 131072.0000 (62729.9006)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1518  max mem: 15572
[2025-01-13 10:37:45,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=4, lr=[6.464542191180159e-08, 6.464542191180159e-08, 9.235060273114513e-08, 9.235060273114513e-08, 1.3192943247306448e-07, 1.3192943247306448e-07, 1.8847061781866357e-07, 1.8847061781866357e-07, 2.6924373974094795e-07, 2.6924373974094795e-07, 3.8463391391563995e-07, 3.8463391391563995e-07, 5.494770198794857e-07, 5.494770198794857e-07, 7.849671712564082e-07, 7.849671712564082e-07, 1.1213816732234404e-06, 1.1213816732234404e-06, 1.6019738188906293e-06, 1.6019738188906293e-06, 2.288534026986613e-06, 2.288534026986613e-06, 3.2693343242665907e-06, 3.2693343242665907e-06, 4.670477606095129e-06, 4.670477606095129e-06, 6.6721108658501856e-06, 6.6721108658501856e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 10:37:45,686] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=27.018769646970483, CurrSamplesPerSec=31.52802099711227, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [2000/2809]  eta: 0:08:03  lr: 0.000007  min_lr: 0.000000  loss: 5.0114 (5.0981)  loss_scale: 131072.0000 (63071.4403)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0987  max mem: 15572
Epoch: [0]  [2010/2809]  eta: 0:07:57  lr: 0.000007  min_lr: 0.000000  loss: 4.9854 (5.0975)  loss_scale: 131072.0000 (63409.5833)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1274  max mem: 15572
Epoch: [0]  [2020/2809]  eta: 0:07:51  lr: 0.000007  min_lr: 0.000000  loss: 4.9888 (5.0973)  loss_scale: 131072.0000 (63744.3800)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1146  max mem: 15572
Epoch: [0]  [2030/2809]  eta: 0:07:45  lr: 0.000007  min_lr: 0.000000  loss: 5.0313 (5.0971)  loss_scale: 131072.0000 (64075.8799)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1276  max mem: 15572
[2025-01-13 10:38:08,713] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:38:08,714] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2040/2809]  eta: 0:07:39  lr: 0.000007  min_lr: 0.000000  loss: 4.9939 (5.0964)  loss_scale: 131072.0000 (64596.7898)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1044  max mem: 15572
Epoch: [0]  [2050/2809]  eta: 0:07:33  lr: 0.000007  min_lr: 0.000000  loss: 4.9703 (5.0960)  loss_scale: 262144.0000 (65559.9649)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1085  max mem: 15572
Epoch: [0]  [2060/2809]  eta: 0:07:27  lr: 0.000007  min_lr: 0.000000  loss: 5.0084 (5.0957)  loss_scale: 262144.0000 (66513.7933)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1618  max mem: 15572
Epoch: [0]  [2070/2809]  eta: 0:07:21  lr: 0.000007  min_lr: 0.000000  loss: 5.0252 (5.0954)  loss_scale: 262144.0000 (67458.4104)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1537  max mem: 15572
Epoch: [0]  [2080/2809]  eta: 0:07:15  lr: 0.000007  min_lr: 0.000000  loss: 5.0044 (5.0949)  loss_scale: 262144.0000 (68393.9491)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1233  max mem: 15572
[2025-01-13 10:38:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2086
[2025-01-13 10:38:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:38:36,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2090/2809]  eta: 0:07:09  lr: 0.000007  min_lr: 0.000000  loss: 4.9915 (5.0944)  loss_scale: 262144.0000 (69007.1200)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1670  max mem: 15572
Epoch: [0]  [2100/2809]  eta: 0:07:03  lr: 0.000007  min_lr: 0.000000  loss: 5.0105 (5.0942)  loss_scale: 131072.0000 (69302.5264)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1591  max mem: 15572
Epoch: [0]  [2110/2809]  eta: 0:06:57  lr: 0.000007  min_lr: 0.000000  loss: 5.0079 (5.0937)  loss_scale: 131072.0000 (69595.1341)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1256  max mem: 15572
Epoch: [0]  [2120/2809]  eta: 0:06:51  lr: 0.000007  min_lr: 0.000000  loss: 4.9933 (5.0933)  loss_scale: 131072.0000 (69884.9826)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0827  max mem: 15572
Epoch: [0]  [2130/2809]  eta: 0:06:45  lr: 0.000007  min_lr: 0.000000  loss: 4.9997 (5.0930)  loss_scale: 131072.0000 (70172.1107)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1050  max mem: 15572
Epoch: [0]  [2140/2809]  eta: 0:06:39  lr: 0.000007  min_lr: 0.000000  loss: 5.0188 (5.0929)  loss_scale: 131072.0000 (70456.5567)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.2058  max mem: 15572
Epoch: [0]  [2150/2809]  eta: 0:06:33  lr: 0.000007  min_lr: 0.000000  loss: 5.0121 (5.0926)  loss_scale: 131072.0000 (70738.3580)  weight_decay: 0.0500 (0.0500)  time: 0.6501  data: 0.1989  max mem: 15572
Epoch: [0]  [2160/2809]  eta: 0:06:27  lr: 0.000007  min_lr: 0.000000  loss: 5.0001 (5.0922)  loss_scale: 131072.0000 (71017.5511)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1547  max mem: 15572
Epoch: [0]  [2170/2809]  eta: 0:06:21  lr: 0.000007  min_lr: 0.000000  loss: 5.0220 (5.0919)  loss_scale: 131072.0000 (71294.1723)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1039  max mem: 15572
Epoch: [0]  [2180/2809]  eta: 0:06:15  lr: 0.000007  min_lr: 0.000000  loss: 5.0319 (5.0918)  loss_scale: 131072.0000 (71568.2568)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1434  max mem: 15572
Epoch: [0]  [2190/2809]  eta: 0:06:09  lr: 0.000007  min_lr: 0.000000  loss: 5.0153 (5.0913)  loss_scale: 131072.0000 (71839.8393)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1523  max mem: 15572
Epoch: [0]  [2200/2809]  eta: 0:06:03  lr: 0.000007  min_lr: 0.000000  loss: 5.0022 (5.0910)  loss_scale: 131072.0000 (72108.9541)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1044  max mem: 15572
Epoch: [0]  [2210/2809]  eta: 0:05:57  lr: 0.000007  min_lr: 0.000000  loss: 5.0012 (5.0904)  loss_scale: 131072.0000 (72375.6346)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1011  max mem: 15572
[2025-01-13 10:39:54,621] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:39:54,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2220/2809]  eta: 0:05:51  lr: 0.000007  min_lr: 0.000000  loss: 4.9886 (5.0900)  loss_scale: 131072.0000 (72994.0027)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0687  max mem: 15572
Epoch: [0]  [2230/2809]  eta: 0:05:45  lr: 0.000007  min_lr: 0.000000  loss: 4.9899 (5.0894)  loss_scale: 262144.0000 (73841.8288)  weight_decay: 0.0500 (0.0500)  time: 0.4820  data: 0.0170  max mem: 15572
Epoch: [0]  [2240/2809]  eta: 0:05:39  lr: 0.000007  min_lr: 0.000000  loss: 5.0007 (5.0892)  loss_scale: 262144.0000 (74682.0884)  weight_decay: 0.0500 (0.0500)  time: 0.5158  data: 0.0631  max mem: 15572
Epoch: [0]  [2250/2809]  eta: 0:05:33  lr: 0.000008  min_lr: 0.000000  loss: 5.0239 (5.0888)  loss_scale: 262144.0000 (75514.8823)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1543  max mem: 15572
Epoch: [0]  [2260/2809]  eta: 0:05:27  lr: 0.000008  min_lr: 0.000000  loss: 5.0211 (5.0886)  loss_scale: 262144.0000 (76340.3096)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1295  max mem: 15572
[2025-01-13 10:40:21,434] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2263
[2025-01-13 10:40:21,435] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:40:21,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2270/2809]  eta: 0:05:21  lr: 0.000008  min_lr: 0.000000  loss: 4.9858 (5.0880)  loss_scale: 262144.0000 (76696.7433)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0384  max mem: 15572
Epoch: [0]  [2280/2809]  eta: 0:05:15  lr: 0.000008  min_lr: 0.000000  loss: 4.9672 (5.0876)  loss_scale: 131072.0000 (76935.1267)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0736  max mem: 15572
Epoch: [0]  [2290/2809]  eta: 0:05:09  lr: 0.000008  min_lr: 0.000000  loss: 5.0192 (5.0872)  loss_scale: 131072.0000 (77171.4291)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1145  max mem: 15572
Epoch: [0]  [2300/2809]  eta: 0:05:03  lr: 0.000008  min_lr: 0.000000  loss: 4.9885 (5.0867)  loss_scale: 131072.0000 (77405.6775)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1372  max mem: 15572
Epoch: [0]  [2310/2809]  eta: 0:04:57  lr: 0.000008  min_lr: 0.000000  loss: 4.9738 (5.0864)  loss_scale: 131072.0000 (77637.8987)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0964  max mem: 15572
Epoch: [0]  [2320/2809]  eta: 0:04:50  lr: 0.000008  min_lr: 0.000000  loss: 4.9738 (5.0861)  loss_scale: 131072.0000 (77868.1189)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0258  max mem: 15572
Epoch: [0]  [2330/2809]  eta: 0:04:45  lr: 0.000008  min_lr: 0.000000  loss: 4.9869 (5.0857)  loss_scale: 131072.0000 (78096.3638)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0912  max mem: 15572
Epoch: [0]  [2340/2809]  eta: 0:04:38  lr: 0.000008  min_lr: 0.000000  loss: 5.0125 (5.0856)  loss_scale: 131072.0000 (78322.6587)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1324  max mem: 15572
Epoch: [0]  [2350/2809]  eta: 0:04:33  lr: 0.000008  min_lr: 0.000000  loss: 5.0449 (5.0854)  loss_scale: 131072.0000 (78547.0285)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1722  max mem: 15572
Epoch: [0]  [2360/2809]  eta: 0:04:27  lr: 0.000008  min_lr: 0.000000  loss: 5.0186 (5.0851)  loss_scale: 131072.0000 (78769.4977)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.1902  max mem: 15572
Epoch: [0]  [2370/2809]  eta: 0:04:21  lr: 0.000008  min_lr: 0.000000  loss: 5.0500 (5.0852)  loss_scale: 131072.0000 (78990.0903)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1106  max mem: 15572
Epoch: [0]  [2380/2809]  eta: 0:04:15  lr: 0.000008  min_lr: 0.000000  loss: 5.0313 (5.0848)  loss_scale: 131072.0000 (79208.8299)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1339  max mem: 15572
Epoch: [0]  [2390/2809]  eta: 0:04:09  lr: 0.000008  min_lr: 0.000000  loss: 5.0198 (5.0846)  loss_scale: 131072.0000 (79425.7399)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1536  max mem: 15572
[2025-01-13 10:41:35,125] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:41:35,125] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-13 10:41:39,478] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2399
[2025-01-13 10:41:39,478] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:41:39,478] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2400/2809]  eta: 0:04:03  lr: 0.000008  min_lr: 0.000000  loss: 5.0043 (5.0841)  loss_scale: 131072.0000 (80022.9771)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1067  max mem: 15572
Epoch: [0]  [2410/2809]  eta: 0:03:57  lr: 0.000008  min_lr: 0.000000  loss: 4.9684 (5.0835)  loss_scale: 131072.0000 (80234.7109)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1316  max mem: 15572
Epoch: [0]  [2420/2809]  eta: 0:03:51  lr: 0.000008  min_lr: 0.000000  loss: 4.9712 (5.0834)  loss_scale: 131072.0000 (80444.6956)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1465  max mem: 15572
Epoch: [0]  [2430/2809]  eta: 0:03:45  lr: 0.000008  min_lr: 0.000000  loss: 5.0326 (5.0833)  loss_scale: 131072.0000 (80652.9527)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1262  max mem: 15572
Epoch: [0]  [2440/2809]  eta: 0:03:39  lr: 0.000008  min_lr: 0.000000  loss: 5.0418 (5.0831)  loss_scale: 131072.0000 (80859.5035)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0857  max mem: 15572
Epoch: [0]  [2450/2809]  eta: 0:03:33  lr: 0.000008  min_lr: 0.000000  loss: 5.0418 (5.0828)  loss_scale: 131072.0000 (81064.3688)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1351  max mem: 15572
Epoch: [0]  [2460/2809]  eta: 0:03:27  lr: 0.000008  min_lr: 0.000000  loss: 5.0174 (5.0826)  loss_scale: 131072.0000 (81267.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1307  max mem: 15572
Epoch: [0]  [2470/2809]  eta: 0:03:21  lr: 0.000008  min_lr: 0.000000  loss: 5.0133 (5.0823)  loss_scale: 131072.0000 (81469.1251)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0920  max mem: 15572
Epoch: [0]  [2480/2809]  eta: 0:03:15  lr: 0.000008  min_lr: 0.000000  loss: 4.9949 (5.0819)  loss_scale: 131072.0000 (81669.0560)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1185  max mem: 15572
Epoch: [0]  [2490/2809]  eta: 0:03:09  lr: 0.000008  min_lr: 0.000000  loss: 4.9702 (5.0815)  loss_scale: 131072.0000 (81867.3818)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1091  max mem: 15572
Epoch: [0]  [2500/2809]  eta: 0:03:03  lr: 0.000008  min_lr: 0.000000  loss: 5.0015 (5.0812)  loss_scale: 131072.0000 (82064.1216)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1448  max mem: 15572
Epoch: [0]  [2510/2809]  eta: 0:02:57  lr: 0.000008  min_lr: 0.000000  loss: 4.9884 (5.0809)  loss_scale: 131072.0000 (82259.2943)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1252  max mem: 15572
Epoch: [0]  [2520/2809]  eta: 0:02:51  lr: 0.000008  min_lr: 0.000000  loss: 4.9615 (5.0804)  loss_scale: 131072.0000 (82452.9187)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1229  max mem: 15572
[2025-01-13 10:42:54,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:42:54,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2530/2809]  eta: 0:02:45  lr: 0.000008  min_lr: 0.000000  loss: 4.9685 (5.0801)  loss_scale: 131072.0000 (82800.3730)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1749  max mem: 15572
Epoch: [0]  [2540/2809]  eta: 0:02:39  lr: 0.000008  min_lr: 0.000000  loss: 5.0214 (5.0798)  loss_scale: 262144.0000 (83506.1724)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1648  max mem: 15572
Epoch: [0]  [2550/2809]  eta: 0:02:34  lr: 0.000009  min_lr: 0.000000  loss: 5.0317 (5.0798)  loss_scale: 262144.0000 (84206.4383)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1863  max mem: 15572
Epoch: [0]  [2560/2809]  eta: 0:02:28  lr: 0.000009  min_lr: 0.000000  loss: 4.9903 (5.0794)  loss_scale: 262144.0000 (84901.2355)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1566  max mem: 15572
[2025-01-13 10:43:16,362] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2565
[2025-01-13 10:43:16,362] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:43:16,363] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2570/2809]  eta: 0:02:22  lr: 0.000009  min_lr: 0.000000  loss: 4.9903 (5.0792)  loss_scale: 262144.0000 (85284.7421)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1031  max mem: 15572
Epoch: [0]  [2580/2809]  eta: 0:02:16  lr: 0.000009  min_lr: 0.000000  loss: 4.9850 (5.0788)  loss_scale: 131072.0000 (85462.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1119  max mem: 15572
Epoch: [0]  [2590/2809]  eta: 0:02:10  lr: 0.000009  min_lr: 0.000000  loss: 4.9850 (5.0786)  loss_scale: 131072.0000 (85638.1752)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1557  max mem: 15572
Epoch: [0]  [2600/2809]  eta: 0:02:04  lr: 0.000009  min_lr: 0.000000  loss: 4.9907 (5.0780)  loss_scale: 131072.0000 (85812.8535)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1514  max mem: 15572
Epoch: [0]  [2610/2809]  eta: 0:01:58  lr: 0.000009  min_lr: 0.000000  loss: 4.9891 (5.0776)  loss_scale: 131072.0000 (85986.1938)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1513  max mem: 15572
Epoch: [0]  [2620/2809]  eta: 0:01:52  lr: 0.000009  min_lr: 0.000000  loss: 5.0390 (5.0776)  loss_scale: 131072.0000 (86158.2114)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1174  max mem: 15572
Epoch: [0]  [2630/2809]  eta: 0:01:46  lr: 0.000009  min_lr: 0.000000  loss: 5.0639 (5.0774)  loss_scale: 131072.0000 (86328.9213)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0223  max mem: 15572
Epoch: [0]  [2640/2809]  eta: 0:01:40  lr: 0.000009  min_lr: 0.000000  loss: 4.9870 (5.0772)  loss_scale: 131072.0000 (86498.3385)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0375  max mem: 15572
Epoch: [0]  [2650/2809]  eta: 0:01:34  lr: 0.000009  min_lr: 0.000000  loss: 5.0347 (5.0771)  loss_scale: 131072.0000 (86666.4776)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0824  max mem: 15572
Epoch: [0]  [2660/2809]  eta: 0:01:28  lr: 0.000009  min_lr: 0.000000  loss: 5.0035 (5.0768)  loss_scale: 131072.0000 (86833.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1017  max mem: 15572
Epoch: [0]  [2670/2809]  eta: 0:01:22  lr: 0.000009  min_lr: 0.000000  loss: 4.9875 (5.0765)  loss_scale: 131072.0000 (86998.9787)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0848  max mem: 15572
Epoch: [0]  [2680/2809]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000000  loss: 4.9701 (5.0761)  loss_scale: 131072.0000 (87163.3689)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1109  max mem: 15572
Epoch: [0]  [2690/2809]  eta: 0:01:10  lr: 0.000009  min_lr: 0.000000  loss: 4.9704 (5.0759)  loss_scale: 131072.0000 (87326.5373)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1216  max mem: 15572
[2025-01-13 10:44:31,929] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:44:31,930] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2700/2809]  eta: 0:01:04  lr: 0.000009  min_lr: 0.000000  loss: 5.0239 (5.0758)  loss_scale: 131072.0000 (87828.1881)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0878  max mem: 15572
Epoch: [0]  [2710/2809]  eta: 0:00:58  lr: 0.000009  min_lr: 0.000000  loss: 5.0170 (5.0756)  loss_scale: 262144.0000 (88471.1826)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0624  max mem: 15572
Epoch: [0]  [2720/2809]  eta: 0:00:52  lr: 0.000009  min_lr: 0.000000  loss: 5.0047 (5.0755)  loss_scale: 262144.0000 (89109.4509)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1034  max mem: 15572
Epoch: [0]  [2730/2809]  eta: 0:00:46  lr: 0.000009  min_lr: 0.000000  loss: 4.9830 (5.0752)  loss_scale: 262144.0000 (89743.0450)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.1680  max mem: 15572
Epoch: [0]  [2740/2809]  eta: 0:00:40  lr: 0.000009  min_lr: 0.000000  loss: 4.9566 (5.0748)  loss_scale: 262144.0000 (90372.0161)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1266  max mem: 15572
[2025-01-13 10:45:05,372] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2750
[2025-01-13 10:45:05,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:45:05,373] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2750/2809]  eta: 0:00:35  lr: 0.000009  min_lr: 0.000000  loss: 4.9599 (5.0745)  loss_scale: 262144.0000 (90948.7692)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0874  max mem: 15572
Epoch: [0]  [2760/2809]  eta: 0:00:29  lr: 0.000009  min_lr: 0.000000  loss: 5.0024 (5.0742)  loss_scale: 131072.0000 (91094.0905)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1000  max mem: 15572
Epoch: [0]  [2770/2809]  eta: 0:00:23  lr: 0.000009  min_lr: 0.000000  loss: 4.9534 (5.0738)  loss_scale: 131072.0000 (91238.3630)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1674  max mem: 15572
Epoch: [0]  [2780/2809]  eta: 0:00:17  lr: 0.000009  min_lr: 0.000000  loss: 5.0012 (5.0736)  loss_scale: 131072.0000 (91381.5980)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1392  max mem: 15572
Epoch: [0]  [2790/2809]  eta: 0:00:11  lr: 0.000009  min_lr: 0.000000  loss: 4.9633 (5.0731)  loss_scale: 131072.0000 (91523.8065)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0678  max mem: 15572
Epoch: [0]  [2800/2809]  eta: 0:00:05  lr: 0.000009  min_lr: 0.000000  loss: 4.9705 (5.0731)  loss_scale: 131072.0000 (91664.9996)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0356  max mem: 15572
Epoch: [0]  [2808/2809]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 5.0421 (5.0730)  loss_scale: 131072.0000 (91777.2303)  weight_decay: 0.0500 (0.0500)  time: 0.4336  data: 0.0006  max mem: 15572
Epoch: [0] Total time: 0:27:46 (0.5933 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 5.0421 (5.0730)  loss_scale: 131072.0000 (91777.2303)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 620
Indices to remove: tensor([   64,    69,   182,   268,   363,   463,   587,   659,   663,   799,
         1083,  1158,  1244,  1337,  1582,  1911,  1971,  1987,  2201,  2226,
         2252,  2353,  2396,  2430,  2567,  2645,  2669,  2701,  2731,  2743,
         2807,  3124,  3453,  3494,  3528,  4043,  4496,  4511,  4609,  4710,
         4807,  4815,  4935,  5321,  5354,  5468,  5506,  5742,  5813,  5957,
         6027,  6055,  6137,  6189,  6322,  6742,  6792,  6863,  7026,  7275,
         7285,  7557,  7728,  7864,  7982,  8051,  8590,  8600,  8606,  8622,
         8643,  8828,  8830,  8866,  8895,  8902,  8961,  8997,  9021,  9033,
         9060,  9064,  9085,  9116,  9192,  9308,  9320,  9325,  9348,  9370,
         9385,  9388,  9394,  9400,  9411,  9425,  9450,  9451,  9457,  9468,
         9475,  9482,  9486,  9495,  9506,  9510,  9515,  9518,  9519,  9525,
         9527,  9528,  9531,  9549,  9570,  9576,  9580,  9581,  9588,  9589,
         9590,  9591,  9596,  9599,  9600,  9601,  9602,  9616,  9625,  9637,
         9644,  9648,  9653,  9665,  9672,  9676,  9697,  9709,  9719,  9729,
         9739,  9741,  9744,  9756,  9763,  9764,  9770,  9773,  9788,  9805,
         9812,  9815,  9816,  9825,  9826,  9827,  9830,  9836,  9844,  9852,
         9857,  9862,  9865,  9866,  9881,  9894,  9907,  9908,  9915,  9917,
         9923,  9937,  9938, 10116, 10229, 10352, 10637, 10793, 10860, 10995,
        11389, 11443, 11463, 11946, 12647, 13208, 13389, 13437, 13676, 13770,
        14010, 14150, 14245, 14514, 14734, 14761, 14775, 14786, 14855, 15150,
        15226, 15256, 15343, 15405, 15483, 15523, 15580, 15695, 15951, 16086,
        16376, 16474, 16996, 17333, 17405, 17433, 17454, 17484, 17522, 17526,
        17568, 17579, 17635, 17695, 17719, 18015, 18124, 18224, 18496, 18520,
        18527, 18796, 18909, 18940, 18985, 19148, 19273, 19319, 19333, 19436,
        19464, 19567, 19570, 19801, 20151, 20157, 20201, 20240, 20463, 20502,
        20615, 20618, 20622, 20649, 20873, 20875, 20899, 20902, 20939, 21058,
        21062, 21067, 21072, 21073, 21077, 21078, 21079, 21080, 21081, 21084,
        21085, 21086, 21088, 21091, 21093, 21098, 21099, 21100, 21103, 21105,
        21109, 21116, 21118, 21119, 21123, 21124, 21125, 21127, 21129, 21133,
        21137, 21138, 21141, 21142, 21149, 21151, 21155, 21156, 21157, 21158,
        21160, 21161, 21166, 21167, 21168, 21169, 21170, 21173, 21176, 21178,
        21180, 21183, 21184, 21186, 21187, 21188, 21189, 21192, 21197, 21198,
        21202, 21203, 21204, 21205, 21210, 21211, 21213, 21214, 21215, 21217,
        21218, 21219, 21220, 21223, 21226, 21228, 21233, 21235, 21240, 21241,
        21243, 21246, 21247, 21249, 21251, 21254, 21257, 21258, 21263, 21265,
        21268, 21269, 21271, 21274, 21275, 21276, 21280, 21285, 21286, 21288,
        21289, 21291, 21294, 21300, 21301, 21306, 21310, 21320, 21321, 21324,
        21328, 21330, 21332, 21333, 21335, 21337, 21342, 21343, 21344, 21345,
        21346, 21347, 21351, 21353, 21354, 21357, 21358, 21362, 21363, 21364,
        21366, 21368, 21369, 21370, 21375, 21376, 21377, 21378, 21380, 21381,
        21382, 21384, 21385, 21386, 21390, 21392, 21393, 21397, 21401, 21407,
        21408, 21409, 21410, 21412, 21414, 21415, 21421, 21422, 21424, 21425,
        21426, 21427, 21431, 21434, 21435, 21440, 21442, 21444, 21446, 21449,
        21451, 21453, 21454, 21455, 21456, 21457, 21464, 21466, 21467, 21469,
        21471, 21473, 21474, 21475, 21476, 21480, 21481, 21483, 21484, 21492,
        21495, 21497, 21499, 21500, 21503, 21507, 21510, 21512, 21514, 21518,
        21519, 21520, 21522, 21524, 21526, 21529, 21530, 21531, 21533, 21534,
        21537, 21544, 21545, 21547, 21549, 21551, 21553, 21554, 21557, 21559,
        21561, 21564, 21568, 21569, 21571, 21572, 21575, 21576, 21577, 21582,
        21583, 21585, 21588, 21589, 21590, 21592, 21593, 21597, 21600, 21601,
        21602, 21603, 21605, 21612, 21614, 21615, 21616, 21617, 21618, 21620,
        21621, 21622, 21624, 21625, 21626, 21630, 21631, 21632, 21633, 21635,
        21636, 21637, 21640, 21642, 21645, 21650, 21651, 21653, 21654, 21656,
        21659, 21661, 21664, 21665, 21670, 21677, 21679, 21681, 21683, 21686,
        21687, 21691, 21696, 21697, 21698, 21703, 21705, 21706, 21709, 21710,
        21725, 21752, 22312, 22404, 22775, 22927, 23013, 23253, 23442, 23529,
        23561, 23569, 23582, 24104, 24344, 24851, 25209, 25477, 25592, 25645,
        25797, 25963, 26132, 26337, 26519, 26765, 26958, 26977, 27175, 27202,
        27411, 27487, 27843, 27881, 27943, 28091, 28230, 28239, 28402, 28416,
        28715, 29167, 29517, 29688, 29805, 29970, 30078, 30080, 30333, 30463,
        30577, 30768, 30917, 31099, 31213, 31246, 31353, 31488, 31530, 31563,
        31604, 31624, 31651, 31879, 32254, 32419, 32980, 32982, 33184, 33412],
       device='cuda:0')
length of data loader train is: 2757
num_training_steps_per_epoch is: 2757
Change step level LR scheduler!
Set warmup steps = 13785
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:19:08  loss: 4.9922 (4.9922)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 4.2218  data: 3.8745  max mem: 15572
Val:  [ 10/272]  eta: 0:03:20  loss: 5.1506 (5.0917)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.7659  data: 0.5440  max mem: 15572
Val:  [ 20/272]  eta: 0:02:11  loss: 5.1836 (5.0610)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3350  data: 0.1302  max mem: 15572
Val:  [ 30/272]  eta: 0:01:47  loss: 5.1022 (5.0353)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2701  data: 0.0730  max mem: 15572
Val:  [ 40/272]  eta: 0:01:39  loss: 4.8739 (4.9999)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3306  data: 0.1414  max mem: 15572
Val:  [ 50/272]  eta: 0:01:30  loss: 4.8286 (5.0215)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3519  data: 0.1627  max mem: 15572
Val:  [ 60/272]  eta: 0:01:23  loss: 4.8309 (5.0150)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3193  data: 0.1236  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 4.8320 (4.9991)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (4.8513)  time: 0.3245  data: 0.1257  max mem: 15572
Val:  [ 80/272]  eta: 0:01:12  loss: 4.9648 (4.9579)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (9.1221)  time: 0.3374  data: 0.1309  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 5.0204 (4.9828)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (8.1197)  time: 0.3422  data: 0.1409  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 5.1758 (5.0141)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (7.3157)  time: 0.3276  data: 0.1273  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 5.2500 (5.0352)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (6.6567)  time: 0.3349  data: 0.1241  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 5.1777 (5.0578)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (6.1065)  time: 0.3651  data: 0.1639  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 5.1120 (5.0564)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (7.5912)  time: 0.3327  data: 0.1427  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 4.8459 (5.0422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (9.0623)  time: 0.2886  data: 0.1054  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 4.8459 (5.0327)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (8.4621)  time: 0.2904  data: 0.0992  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 4.6328 (5.0026)  acc1: 0.0000 (1.8634)  acc5: 0.0000 (9.7999)  time: 0.2897  data: 0.0914  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.6250 (5.0105)  acc1: 0.0000 (1.9493)  acc5: 0.0000 (9.4217)  time: 0.2784  data: 0.0864  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 5.1155 (5.0132)  acc1: 0.0000 (1.8416)  acc5: 0.0000 (8.9012)  time: 0.3090  data: 0.1229  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 5.1745 (5.0205)  acc1: 0.0000 (1.7452)  acc5: 0.0000 (8.4351)  time: 0.3404  data: 0.1508  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 5.0762 (5.0293)  acc1: 0.0000 (1.6584)  acc5: 0.0000 (8.0155)  time: 0.3489  data: 0.1491  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 5.0757 (5.0345)  acc1: 0.0000 (1.5798)  acc5: 0.0000 (7.6356)  time: 0.3365  data: 0.1284  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.8477 (5.0207)  acc1: 0.0000 (1.5083)  acc5: 0.0000 (7.2901)  time: 0.3188  data: 0.1019  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.7385 (5.0101)  acc1: 0.0000 (1.4430)  acc5: 0.0000 (6.9745)  time: 0.3201  data: 0.1041  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 4.7630 (5.0088)  acc1: 0.0000 (1.3831)  acc5: 0.0000 (6.6851)  time: 0.3395  data: 0.1194  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 5.2500 (5.0253)  acc1: 0.0000 (1.3280)  acc5: 0.0000 (6.4188)  time: 0.3270  data: 0.1103  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 5.1847 (5.0217)  acc1: 0.0000 (1.2771)  acc5: 0.0000 (6.1728)  time: 0.3040  data: 0.1094  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 5.0892 (5.0235)  acc1: 0.0000 (1.2300)  acc5: 0.0000 (5.9451)  time: 0.2288  data: 0.0632  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 5.0948 (5.0255)  acc1: 0.0000 (1.2288)  acc5: 0.0000 (5.9390)  time: 0.2225  data: 0.0632  max mem: 15572
Val: Total time: 0:01:30 (0.3325 s / it)
* Acc@1 1.229 Acc@5 5.939 loss 5.025
Accuracy of the network on the 4883 val videos: 1.2%
[2025-01-13 10:47:07,603] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2025-01-13 10:47:07,619] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 10:47:07,619] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 10:47:08,118] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 10:47:08,119] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 1.23%
Epoch: [1]  [   0/2757]  eta: 5:32:58  lr: 0.000009  min_lr: 0.000000  loss: 5.0489 (5.0489)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 7.2465  data: 6.8179  max mem: 15572
Epoch: [1]  [  10/2757]  eta: 1:01:02  lr: 0.000009  min_lr: 0.000000  loss: 5.0353 (4.9600)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3334  data: 0.8769  max mem: 15572
Epoch: [1]  [  20/2757]  eta: 0:42:14  lr: 0.000009  min_lr: 0.000000  loss: 5.0284 (4.9839)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1716  max mem: 15572
Epoch: [1]  [  30/2757]  eta: 0:39:32  lr: 0.000009  min_lr: 0.000000  loss: 5.0284 (4.9972)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1960  max mem: 15572
Epoch: [1]  [  40/2757]  eta: 0:35:27  lr: 0.000010  min_lr: 0.000000  loss: 4.9625 (4.9832)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1975  max mem: 15572
Epoch: [1]  [  50/2757]  eta: 0:33:30  lr: 0.000010  min_lr: 0.000000  loss: 4.9278 (4.9800)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0927  max mem: 15572
Epoch: [1]  [  60/2757]  eta: 0:32:40  lr: 0.000010  min_lr: 0.000000  loss: 5.0143 (4.9907)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1266  max mem: 15572
[2025-01-13 10:47:58,732] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:47:58,733] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [  70/2757]  eta: 0:31:54  lr: 0.000010  min_lr: 0.000000  loss: 4.9807 (4.9936)  loss_scale: 131072.0000 (132918.0845)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1246  max mem: 15572
Epoch: [1]  [  80/2757]  eta: 0:31:00  lr: 0.000010  min_lr: 0.000000  loss: 4.9719 (4.9932)  loss_scale: 262144.0000 (148871.9012)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0980  max mem: 15572
Epoch: [1]  [  90/2757]  eta: 0:30:26  lr: 0.000010  min_lr: 0.000000  loss: 4.9746 (4.9927)  loss_scale: 262144.0000 (161319.3846)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1066  max mem: 15572
Epoch: [1]  [ 100/2757]  eta: 0:30:12  lr: 0.000010  min_lr: 0.000000  loss: 5.0178 (4.9982)  loss_scale: 262144.0000 (171302.0198)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.1314  max mem: 15572
Epoch: [1]  [ 110/2757]  eta: 0:29:21  lr: 0.000010  min_lr: 0.000000  loss: 5.0178 (4.9959)  loss_scale: 262144.0000 (179485.9820)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.0983  max mem: 15572
Epoch: [1]  [ 120/2757]  eta: 0:29:25  lr: 0.000010  min_lr: 0.000000  loss: 5.0135 (4.9987)  loss_scale: 262144.0000 (186317.2231)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1555  max mem: 15572
Epoch: [1]  [ 130/2757]  eta: 0:28:50  lr: 0.000010  min_lr: 0.000000  loss: 5.0135 (4.9994)  loss_scale: 262144.0000 (192105.5267)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1521  max mem: 15572
Epoch: [1]  [ 140/2757]  eta: 0:28:19  lr: 0.000010  min_lr: 0.000000  loss: 4.9778 (4.9988)  loss_scale: 262144.0000 (197072.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0563  max mem: 15572
Epoch: [1]  [ 150/2757]  eta: 0:28:19  lr: 0.000010  min_lr: 0.000000  loss: 5.0419 (5.0053)  loss_scale: 262144.0000 (201382.1457)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1295  max mem: 15572
Epoch: [1]  [ 160/2757]  eta: 0:27:59  lr: 0.000010  min_lr: 0.000000  loss: 5.0439 (5.0079)  loss_scale: 262144.0000 (205156.1739)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1525  max mem: 15572
Epoch: [1]  [ 170/2757]  eta: 0:27:50  lr: 0.000010  min_lr: 0.000000  loss: 4.9983 (5.0078)  loss_scale: 262144.0000 (208488.7953)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1190  max mem: 15572
Epoch: [1]  [ 180/2757]  eta: 0:27:38  lr: 0.000010  min_lr: 0.000000  loss: 5.0234 (5.0081)  loss_scale: 262144.0000 (211453.1713)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1122  max mem: 15572
[2025-01-13 10:49:09,372] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=9, lr=[9.710032249894423e-08, 9.710032249894423e-08, 1.387147464270632e-07, 1.387147464270632e-07, 1.9816392346723315e-07, 1.9816392346723315e-07, 2.8309131923890453e-07, 2.8309131923890453e-07, 4.0441617034129223e-07, 4.0441617034129223e-07, 5.77737386201846e-07, 5.77737386201846e-07, 8.253391231454944e-07, 8.253391231454944e-07, 1.1790558902078493e-06, 1.1790558902078493e-06, 1.6843655574397846e-06, 1.6843655574397846e-06, 2.4062365106282643e-06, 2.4062365106282643e-06, 3.4374807294689483e-06, 3.4374807294689483e-06, 4.9106867563842125e-06, 4.9106867563842125e-06, 7.01526679483459e-06, 7.01526679483459e-06, 1.0021809706906558e-05, 1.0021809706906558e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 10:49:09,374] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=27.0688910104883, CurrSamplesPerSec=30.850205794107822, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [ 190/2757]  eta: 0:27:08  lr: 0.000010  min_lr: 0.000000  loss: 5.0006 (5.0072)  loss_scale: 262144.0000 (214107.1414)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0607  max mem: 15572
[2025-01-13 10:49:12,709] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:49:12,710] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Epoch: [1]  [ 200/2757]  eta: 0:26:35  lr: 0.000010  min_lr: 0.000000  loss: 4.9734 (5.0065)  loss_scale: 262144.0000 (220409.6318)  weight_decay: 0.0500 (0.0500)  time: 0.4453  data: 0.0199  max mem: 15572
[2025-01-13 10:49:14,157] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3010
[2025-01-13 10:49:14,158] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
[2025-01-13 10:49:14,158] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Epoch: [1]  [ 210/2757]  eta: 0:26:10  lr: 0.000010  min_lr: 0.000000  loss: 5.0044 (5.0088)  loss_scale: 262144.0000 (222387.5640)  weight_decay: 0.0500 (0.0500)  time: 0.4466  data: 0.0009  max mem: 15572
Epoch: [1]  [ 220/2757]  eta: 0:25:49  lr: 0.000010  min_lr: 0.000000  loss: 5.0044 (5.0071)  loss_scale: 262144.0000 (224186.4977)  weight_decay: 0.0500 (0.0500)  time: 0.4767  data: 0.0009  max mem: 15572
[2025-01-13 10:49:26,332] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3032
[2025-01-13 10:49:26,333] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:49:26,334] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 230/2757]  eta: 0:25:49  lr: 0.000010  min_lr: 0.000000  loss: 4.9709 (5.0061)  loss_scale: 262144.0000 (221290.3896)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0845  max mem: 15572
Epoch: [1]  [ 240/2757]  eta: 0:26:02  lr: 0.000010  min_lr: 0.000000  loss: 4.9941 (5.0062)  loss_scale: 131072.0000 (217546.8880)  weight_decay: 0.0500 (0.0500)  time: 0.7334  data: 0.2219  max mem: 15572
Epoch: [1]  [ 250/2757]  eta: 0:25:54  lr: 0.000010  min_lr: 0.000000  loss: 5.0119 (5.0062)  loss_scale: 131072.0000 (214101.6733)  weight_decay: 0.0500 (0.0500)  time: 0.7024  data: 0.2069  max mem: 15572
Epoch: [1]  [ 260/2757]  eta: 0:25:52  lr: 0.000010  min_lr: 0.000000  loss: 5.0433 (5.0086)  loss_scale: 131072.0000 (210920.4598)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1434  max mem: 15572
Epoch: [1]  [ 270/2757]  eta: 0:25:43  lr: 0.000010  min_lr: 0.000000  loss: 5.0144 (5.0075)  loss_scale: 131072.0000 (207974.0221)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1435  max mem: 15572
Epoch: [1]  [ 280/2757]  eta: 0:25:49  lr: 0.000010  min_lr: 0.000000  loss: 4.9913 (5.0069)  loss_scale: 131072.0000 (205237.2954)  weight_decay: 0.0500 (0.0500)  time: 0.6774  data: 0.2085  max mem: 15572
Epoch: [1]  [ 290/2757]  eta: 0:25:40  lr: 0.000010  min_lr: 0.000000  loss: 4.9913 (5.0065)  loss_scale: 131072.0000 (202688.6598)  weight_decay: 0.0500 (0.0500)  time: 0.6769  data: 0.2065  max mem: 15572
Epoch: [1]  [ 300/2757]  eta: 0:25:40  lr: 0.000010  min_lr: 0.000000  loss: 4.9342 (5.0038)  loss_scale: 131072.0000 (200309.3688)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1762  max mem: 15572
Epoch: [1]  [ 310/2757]  eta: 0:25:40  lr: 0.000010  min_lr: 0.000000  loss: 4.9443 (5.0025)  loss_scale: 131072.0000 (198083.0868)  weight_decay: 0.0500 (0.0500)  time: 0.7027  data: 0.2275  max mem: 15572
Epoch: [1]  [ 320/2757]  eta: 0:25:39  lr: 0.000010  min_lr: 0.000000  loss: 4.9507 (5.0029)  loss_scale: 131072.0000 (195995.5140)  weight_decay: 0.0500 (0.0500)  time: 0.7028  data: 0.2040  max mem: 15572
Epoch: [1]  [ 330/2757]  eta: 0:25:38  lr: 0.000010  min_lr: 0.000000  loss: 5.0310 (5.0040)  loss_scale: 131072.0000 (194034.0785)  weight_decay: 0.0500 (0.0500)  time: 0.7002  data: 0.1946  max mem: 15572
Epoch: [1]  [ 340/2757]  eta: 0:25:19  lr: 0.000011  min_lr: 0.000000  loss: 5.0362 (5.0041)  loss_scale: 131072.0000 (192187.6833)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1283  max mem: 15572
Epoch: [1]  [ 350/2757]  eta: 0:25:03  lr: 0.000011  min_lr: 0.000000  loss: 4.9586 (5.0031)  loss_scale: 131072.0000 (190446.4957)  weight_decay: 0.0500 (0.0500)  time: 0.4742  data: 0.0198  max mem: 15572
[2025-01-13 10:50:48,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:50:48,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 360/2757]  eta: 0:24:53  lr: 0.000011  min_lr: 0.000000  loss: 4.9793 (5.0028)  loss_scale: 131072.0000 (192069.4958)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0476  max mem: 15572
[2025-01-13 10:50:54,990] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3171
[2025-01-13 10:50:54,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:50:54,990] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 370/2757]  eta: 0:24:47  lr: 0.000011  min_lr: 0.000000  loss: 5.0273 (5.0044)  loss_scale: 131072.0000 (190778.6523)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1461  max mem: 15572
Epoch: [1]  [ 380/2757]  eta: 0:24:36  lr: 0.000011  min_lr: 0.000000  loss: 5.0137 (5.0024)  loss_scale: 131072.0000 (189211.5486)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1344  max mem: 15572
Epoch: [1]  [ 390/2757]  eta: 0:24:32  lr: 0.000011  min_lr: 0.000000  loss: 4.9660 (5.0015)  loss_scale: 131072.0000 (187724.6036)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1481  max mem: 15572
Epoch: [1]  [ 400/2757]  eta: 0:24:24  lr: 0.000011  min_lr: 0.000000  loss: 4.9903 (5.0010)  loss_scale: 131072.0000 (186311.8204)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1747  max mem: 15572
Epoch: [1]  [ 410/2757]  eta: 0:24:22  lr: 0.000011  min_lr: 0.000000  loss: 5.0320 (5.0020)  loss_scale: 131072.0000 (184967.7859)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.1829  max mem: 15572
Epoch: [1]  [ 420/2757]  eta: 0:24:10  lr: 0.000011  min_lr: 0.000000  loss: 4.9700 (5.0012)  loss_scale: 131072.0000 (183687.6010)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1261  max mem: 15572
Epoch: [1]  [ 430/2757]  eta: 0:24:02  lr: 0.000011  min_lr: 0.000000  loss: 4.9599 (5.0008)  loss_scale: 131072.0000 (182466.8213)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0695  max mem: 15572
Epoch: [1]  [ 440/2757]  eta: 0:23:54  lr: 0.000011  min_lr: 0.000000  loss: 5.0264 (5.0011)  loss_scale: 131072.0000 (181301.4059)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1412  max mem: 15572
Epoch: [1]  [ 450/2757]  eta: 0:23:47  lr: 0.000011  min_lr: 0.000000  loss: 4.9554 (4.9999)  loss_scale: 131072.0000 (180187.6718)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1448  max mem: 15572
Epoch: [1]  [ 460/2757]  eta: 0:23:39  lr: 0.000011  min_lr: 0.000000  loss: 4.9905 (5.0008)  loss_scale: 131072.0000 (179122.2560)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1335  max mem: 15572
Epoch: [1]  [ 470/2757]  eta: 0:23:37  lr: 0.000011  min_lr: 0.000000  loss: 5.0065 (5.0004)  loss_scale: 131072.0000 (178102.0807)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1921  max mem: 15572
Epoch: [1]  [ 480/2757]  eta: 0:23:30  lr: 0.000011  min_lr: 0.000000  loss: 4.9483 (5.0011)  loss_scale: 131072.0000 (177124.3243)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.2019  max mem: 15572
Epoch: [1]  [ 490/2757]  eta: 0:23:20  lr: 0.000011  min_lr: 0.000000  loss: 5.0162 (5.0018)  loss_scale: 131072.0000 (176186.3951)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1156  max mem: 15572
[2025-01-13 10:52:12,154] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:52:12,154] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 500/2757]  eta: 0:23:19  lr: 0.000011  min_lr: 0.000000  loss: 5.0397 (5.0028)  loss_scale: 131072.0000 (177902.1158)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1618  max mem: 15572
Epoch: [1]  [ 510/2757]  eta: 0:23:07  lr: 0.000011  min_lr: 0.000000  loss: 5.0558 (5.0028)  loss_scale: 262144.0000 (179550.6849)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1219  max mem: 15572
Epoch: [1]  [ 520/2757]  eta: 0:22:55  lr: 0.000011  min_lr: 0.000000  loss: 4.9550 (5.0023)  loss_scale: 262144.0000 (181135.9693)  weight_decay: 0.0500 (0.0500)  time: 0.4862  data: 0.0011  max mem: 15572
Epoch: [1]  [ 530/2757]  eta: 0:22:47  lr: 0.000011  min_lr: 0.000000  loss: 4.9816 (5.0028)  loss_scale: 262144.0000 (182661.5443)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0562  max mem: 15572
[2025-01-13 10:52:38,553] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3345
[2025-01-13 10:52:38,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:52:38,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 540/2757]  eta: 0:22:41  lr: 0.000011  min_lr: 0.000000  loss: 4.9880 (5.0023)  loss_scale: 262144.0000 (182919.3346)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1239  max mem: 15572
Epoch: [1]  [ 550/2757]  eta: 0:22:37  lr: 0.000011  min_lr: 0.000000  loss: 4.9880 (5.0022)  loss_scale: 131072.0000 (181978.3666)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1715  max mem: 15572
Epoch: [1]  [ 560/2757]  eta: 0:22:28  lr: 0.000011  min_lr: 0.000000  loss: 4.9883 (5.0027)  loss_scale: 131072.0000 (181070.9447)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1357  max mem: 15572
Epoch: [1]  [ 570/2757]  eta: 0:22:23  lr: 0.000011  min_lr: 0.000000  loss: 4.9829 (5.0026)  loss_scale: 131072.0000 (180195.3065)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1027  max mem: 15572
Epoch: [1]  [ 580/2757]  eta: 0:22:14  lr: 0.000011  min_lr: 0.000000  loss: 5.0284 (5.0034)  loss_scale: 131072.0000 (179349.8107)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1069  max mem: 15572
Epoch: [1]  [ 590/2757]  eta: 0:22:06  lr: 0.000011  min_lr: 0.000000  loss: 4.9934 (5.0030)  loss_scale: 131072.0000 (178532.9272)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0832  max mem: 15572
Epoch: [1]  [ 600/2757]  eta: 0:22:00  lr: 0.000011  min_lr: 0.000000  loss: 4.9625 (5.0030)  loss_scale: 131072.0000 (177743.2280)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1327  max mem: 15572
Epoch: [1]  [ 610/2757]  eta: 0:21:53  lr: 0.000011  min_lr: 0.000000  loss: 4.9972 (5.0017)  loss_scale: 131072.0000 (176979.3781)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1418  max mem: 15572
Epoch: [1]  [ 620/2757]  eta: 0:21:47  lr: 0.000011  min_lr: 0.000000  loss: 5.0280 (5.0025)  loss_scale: 131072.0000 (176240.1288)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1248  max mem: 15572
Epoch: [1]  [ 630/2757]  eta: 0:21:38  lr: 0.000012  min_lr: 0.000000  loss: 5.0233 (5.0017)  loss_scale: 131072.0000 (175524.3106)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0999  max mem: 15572
Epoch: [1]  [ 640/2757]  eta: 0:21:34  lr: 0.000012  min_lr: 0.000000  loss: 4.9477 (5.0010)  loss_scale: 131072.0000 (174830.8268)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1235  max mem: 15572
Epoch: [1]  [ 650/2757]  eta: 0:21:24  lr: 0.000012  min_lr: 0.000000  loss: 4.9553 (5.0006)  loss_scale: 131072.0000 (174158.6482)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1113  max mem: 15572
Epoch: [1]  [ 660/2757]  eta: 0:21:15  lr: 0.000012  min_lr: 0.000000  loss: 4.9576 (5.0008)  loss_scale: 131072.0000 (173506.8079)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0189  max mem: 15572
[2025-01-13 10:53:53,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:53:53,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 670/2757]  eta: 0:21:06  lr: 0.000012  min_lr: 0.000000  loss: 5.0520 (5.0019)  loss_scale: 131072.0000 (174046.4262)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0397  max mem: 15572
[2025-01-13 10:53:57,061] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3482
[2025-01-13 10:53:57,061] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:53:57,061] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 680/2757]  eta: 0:21:00  lr: 0.000012  min_lr: 0.000000  loss: 5.0356 (5.0013)  loss_scale: 131072.0000 (173800.3172)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1247  max mem: 15572
Epoch: [1]  [ 690/2757]  eta: 0:20:58  lr: 0.000012  min_lr: 0.000000  loss: 5.0134 (5.0019)  loss_scale: 131072.0000 (173181.9624)  weight_decay: 0.0500 (0.0500)  time: 0.6733  data: 0.2120  max mem: 15572
Epoch: [1]  [ 700/2757]  eta: 0:20:50  lr: 0.000012  min_lr: 0.000000  loss: 5.0347 (5.0024)  loss_scale: 131072.0000 (172581.2496)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.1663  max mem: 15572
Epoch: [1]  [ 710/2757]  eta: 0:20:44  lr: 0.000012  min_lr: 0.000000  loss: 5.0479 (5.0030)  loss_scale: 131072.0000 (171997.4346)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1244  max mem: 15572
Epoch: [1]  [ 720/2757]  eta: 0:20:36  lr: 0.000012  min_lr: 0.000000  loss: 5.0260 (5.0027)  loss_scale: 131072.0000 (171429.8141)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1264  max mem: 15572
Epoch: [1]  [ 730/2757]  eta: 0:20:32  lr: 0.000012  min_lr: 0.000000  loss: 4.9302 (5.0023)  loss_scale: 131072.0000 (170877.7237)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1490  max mem: 15572
Epoch: [1]  [ 740/2757]  eta: 0:20:28  lr: 0.000012  min_lr: 0.000000  loss: 5.0513 (5.0030)  loss_scale: 131072.0000 (170340.5344)  weight_decay: 0.0500 (0.0500)  time: 0.6783  data: 0.2370  max mem: 15572
Epoch: [1]  [ 750/2757]  eta: 0:20:19  lr: 0.000012  min_lr: 0.000000  loss: 5.0108 (5.0029)  loss_scale: 131072.0000 (169817.6511)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1519  max mem: 15572
Epoch: [1]  [ 760/2757]  eta: 0:20:12  lr: 0.000012  min_lr: 0.000000  loss: 4.9951 (5.0028)  loss_scale: 131072.0000 (169308.5099)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0720  max mem: 15572
Epoch: [1]  [ 770/2757]  eta: 0:20:04  lr: 0.000012  min_lr: 0.000000  loss: 4.9781 (5.0028)  loss_scale: 131072.0000 (168812.5759)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0980  max mem: 15572
Epoch: [1]  [ 780/2757]  eta: 0:19:59  lr: 0.000012  min_lr: 0.000000  loss: 5.0131 (5.0032)  loss_scale: 131072.0000 (168329.3419)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1635  max mem: 15572
Epoch: [1]  [ 790/2757]  eta: 0:19:57  lr: 0.000012  min_lr: 0.000000  loss: 5.0023 (5.0028)  loss_scale: 131072.0000 (167858.3262)  weight_decay: 0.0500 (0.0500)  time: 0.7062  data: 0.2455  max mem: 15572
Epoch: [1]  [ 800/2757]  eta: 0:19:48  lr: 0.000012  min_lr: 0.000000  loss: 4.9512 (5.0021)  loss_scale: 131072.0000 (167399.0712)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1429  max mem: 15572
[2025-01-13 10:55:15,825] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:55:15,826] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 810/2757]  eta: 0:19:40  lr: 0.000012  min_lr: 0.000000  loss: 4.9829 (5.0019)  loss_scale: 131072.0000 (168405.7016)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0393  max mem: 15572
Epoch: [1]  [ 820/2757]  eta: 0:19:34  lr: 0.000012  min_lr: 0.000000  loss: 4.9566 (5.0015)  loss_scale: 262144.0000 (169547.4592)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0995  max mem: 15572
[2025-01-13 10:55:30,881] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3635
[2025-01-13 10:55:30,882] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:55:30,882] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 830/2757]  eta: 0:19:29  lr: 0.000012  min_lr: 0.000000  loss: 4.9566 (5.0013)  loss_scale: 262144.0000 (169873.0975)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1565  max mem: 15572
Epoch: [1]  [ 840/2757]  eta: 0:19:25  lr: 0.000012  min_lr: 0.000000  loss: 4.9828 (5.0018)  loss_scale: 131072.0000 (169411.7289)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.2001  max mem: 15572
Epoch: [1]  [ 850/2757]  eta: 0:19:18  lr: 0.000012  min_lr: 0.000000  loss: 5.0297 (5.0024)  loss_scale: 131072.0000 (168961.2033)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1801  max mem: 15572
Epoch: [1]  [ 860/2757]  eta: 0:19:10  lr: 0.000012  min_lr: 0.000000  loss: 4.9942 (5.0022)  loss_scale: 131072.0000 (168521.1429)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0836  max mem: 15572
Epoch: [1]  [ 870/2757]  eta: 0:19:03  lr: 0.000012  min_lr: 0.000000  loss: 4.9718 (5.0024)  loss_scale: 131072.0000 (168091.1871)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0695  max mem: 15572
Epoch: [1]  [ 880/2757]  eta: 0:18:56  lr: 0.000012  min_lr: 0.000000  loss: 5.0257 (5.0030)  loss_scale: 131072.0000 (167670.9921)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1075  max mem: 15572
Epoch: [1]  [ 890/2757]  eta: 0:18:49  lr: 0.000012  min_lr: 0.000000  loss: 5.0257 (5.0027)  loss_scale: 131072.0000 (167260.2290)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1226  max mem: 15572
Epoch: [1]  [ 900/2757]  eta: 0:18:43  lr: 0.000012  min_lr: 0.000000  loss: 4.9848 (5.0025)  loss_scale: 131072.0000 (166858.5838)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1445  max mem: 15572
Epoch: [1]  [ 910/2757]  eta: 0:18:37  lr: 0.000012  min_lr: 0.000000  loss: 4.9841 (5.0025)  loss_scale: 131072.0000 (166465.7563)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1557  max mem: 15572
Epoch: [1]  [ 920/2757]  eta: 0:18:30  lr: 0.000013  min_lr: 0.000000  loss: 4.9841 (5.0022)  loss_scale: 131072.0000 (166081.4593)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1188  max mem: 15572
Epoch: [1]  [ 930/2757]  eta: 0:18:24  lr: 0.000013  min_lr: 0.000000  loss: 4.9972 (5.0028)  loss_scale: 131072.0000 (165705.4178)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0962  max mem: 15572
Epoch: [1]  [ 940/2757]  eta: 0:18:19  lr: 0.000013  min_lr: 0.000000  loss: 5.0429 (5.0028)  loss_scale: 131072.0000 (165337.3688)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1483  max mem: 15572
Epoch: [1]  [ 950/2757]  eta: 0:18:11  lr: 0.000013  min_lr: 0.000000  loss: 4.9671 (5.0024)  loss_scale: 131072.0000 (164977.0599)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1222  max mem: 15572
[2025-01-13 10:56:46,742] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:56:46,743] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 960/2757]  eta: 0:18:05  lr: 0.000013  min_lr: 0.000000  loss: 4.9671 (5.0022)  loss_scale: 131072.0000 (165442.5973)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0951  max mem: 15572
[2025-01-13 10:56:52,879] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3777
[2025-01-13 10:56:52,879] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-13 10:56:52,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 970/2757]  eta: 0:17:57  lr: 0.000013  min_lr: 0.000000  loss: 4.9852 (5.0023)  loss_scale: 262144.0000 (166033.5324)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0756  max mem: 15572
Epoch: [1]  [ 980/2757]  eta: 0:17:51  lr: 0.000013  min_lr: 0.000000  loss: 4.9792 (5.0016)  loss_scale: 131072.0000 (165677.1458)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0758  max mem: 15572
[2025-01-13 10:57:01,579] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3792
[2025-01-13 10:57:01,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:57:01,580] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [ 990/2757]  eta: 0:17:45  lr: 0.000013  min_lr: 0.000000  loss: 4.9792 (5.0022)  loss_scale: 131072.0000 (164798.9021)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1153  max mem: 15572
Epoch: [1]  [1000/2757]  eta: 0:17:39  lr: 0.000013  min_lr: 0.000000  loss: 5.0383 (5.0025)  loss_scale: 65536.0000 (163807.2647)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0757  max mem: 15572
Epoch: [1]  [1010/2757]  eta: 0:17:30  lr: 0.000013  min_lr: 0.000000  loss: 5.0339 (5.0023)  loss_scale: 65536.0000 (162835.2443)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0288  max mem: 15572
Epoch: [1]  [1020/2757]  eta: 0:17:24  lr: 0.000013  min_lr: 0.000000  loss: 5.0256 (5.0025)  loss_scale: 65536.0000 (161882.2644)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0629  max mem: 15572
Epoch: [1]  [1030/2757]  eta: 0:17:17  lr: 0.000013  min_lr: 0.000000  loss: 5.0274 (5.0027)  loss_scale: 65536.0000 (160947.7711)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1058  max mem: 15572
Epoch: [1]  [1040/2757]  eta: 0:17:11  lr: 0.000013  min_lr: 0.000000  loss: 5.0154 (5.0025)  loss_scale: 65536.0000 (160031.2315)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1045  max mem: 15572
Epoch: [1]  [1050/2757]  eta: 0:17:06  lr: 0.000013  min_lr: 0.000000  loss: 4.9733 (5.0021)  loss_scale: 65536.0000 (159132.1332)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1611  max mem: 15572
Epoch: [1]  [1060/2757]  eta: 0:16:59  lr: 0.000013  min_lr: 0.000000  loss: 4.9733 (5.0021)  loss_scale: 65536.0000 (158249.9830)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1308  max mem: 15572
Epoch: [1]  [1070/2757]  eta: 0:16:51  lr: 0.000013  min_lr: 0.000000  loss: 4.9193 (5.0013)  loss_scale: 65536.0000 (157384.3063)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0355  max mem: 15572
Epoch: [1]  [1080/2757]  eta: 0:16:47  lr: 0.000013  min_lr: 0.000000  loss: 4.9193 (5.0015)  loss_scale: 65536.0000 (156534.6457)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1089  max mem: 15572
Epoch: [1]  [1090/2757]  eta: 0:16:40  lr: 0.000013  min_lr: 0.000000  loss: 5.0137 (5.0015)  loss_scale: 65536.0000 (155700.5610)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1353  max mem: 15572
Epoch: [1]  [1100/2757]  eta: 0:16:34  lr: 0.000013  min_lr: 0.000000  loss: 4.9588 (5.0010)  loss_scale: 65536.0000 (154881.6276)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1097  max mem: 15572
Epoch: [1]  [1110/2757]  eta: 0:16:29  lr: 0.000013  min_lr: 0.000000  loss: 4.9676 (5.0010)  loss_scale: 65536.0000 (154077.4365)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.1616  max mem: 15572
[2025-01-13 10:58:16,835] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:58:16,835] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1120/2757]  eta: 0:16:24  lr: 0.000013  min_lr: 0.000000  loss: 5.0032 (5.0013)  loss_scale: 65536.0000 (153813.7520)  weight_decay: 0.0500 (0.0500)  time: 0.6535  data: 0.1795  max mem: 15572
[2025-01-13 10:58:23,045] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3930
[2025-01-13 10:58:23,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 10:58:23,047] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1130/2757]  eta: 0:16:16  lr: 0.000013  min_lr: 0.000000  loss: 4.9655 (5.0006)  loss_scale: 65536.0000 (153033.2237)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1386  max mem: 15572
Epoch: [1]  [1140/2757]  eta: 0:16:10  lr: 0.000013  min_lr: 0.000000  loss: 4.9239 (5.0002)  loss_scale: 65536.0000 (152266.3769)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0969  max mem: 15572
Epoch: [1]  [1150/2757]  eta: 0:16:04  lr: 0.000013  min_lr: 0.000000  loss: 4.9377 (5.0007)  loss_scale: 65536.0000 (151512.8549)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1260  max mem: 15572
Epoch: [1]  [1160/2757]  eta: 0:15:57  lr: 0.000013  min_lr: 0.000000  loss: 5.0257 (5.0009)  loss_scale: 65536.0000 (150772.3135)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1353  max mem: 15572
Epoch: [1]  [1170/2757]  eta: 0:15:51  lr: 0.000013  min_lr: 0.000000  loss: 4.9929 (5.0007)  loss_scale: 65536.0000 (150044.4202)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1086  max mem: 15572
Epoch: [1]  [1180/2757]  eta: 0:15:45  lr: 0.000013  min_lr: 0.000000  loss: 4.9640 (5.0005)  loss_scale: 65536.0000 (149328.8535)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1307  max mem: 15572
[2025-01-13 10:59:02,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=18, lr=[1.300491933842324e-07, 1.300491933842324e-07, 1.8578456197747487e-07, 1.8578456197747487e-07, 2.654065171106784e-07, 2.654065171106784e-07, 3.791521673009692e-07, 3.791521673009692e-07, 5.416459532870989e-07, 5.416459532870989e-07, 7.737799332672842e-07, 7.737799332672842e-07, 1.105399904667549e-06, 1.105399904667549e-06, 1.5791427209536414e-06, 1.5791427209536414e-06, 2.2559181727909163e-06, 2.2559181727909163e-06, 3.2227402468441663e-06, 3.2227402468441663e-06, 4.603914638348809e-06, 4.603914638348809e-06, 6.577020911926871e-06, 6.577020911926871e-06, 9.39574415989553e-06, 9.39574415989553e-06, 1.3422491656993615e-05, 1.3422491656993615e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 10:59:02,908] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=27.08238628916542, CurrSamplesPerSec=25.34240991226869, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [1190/2757]  eta: 0:15:39  lr: 0.000013  min_lr: 0.000000  loss: 4.9610 (5.0002)  loss_scale: 65536.0000 (148625.3031)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1547  max mem: 15572
Epoch: [1]  [1200/2757]  eta: 0:15:34  lr: 0.000013  min_lr: 0.000000  loss: 4.9568 (4.9999)  loss_scale: 65536.0000 (147933.4688)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.1841  max mem: 15572
Epoch: [1]  [1210/2757]  eta: 0:15:28  lr: 0.000013  min_lr: 0.000000  loss: 5.0030 (4.9999)  loss_scale: 65536.0000 (147253.0603)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1651  max mem: 15572
Epoch: [1]  [1220/2757]  eta: 0:15:23  lr: 0.000014  min_lr: 0.000000  loss: 5.0121 (5.0002)  loss_scale: 65536.0000 (146583.7969)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1662  max mem: 15572
Epoch: [1]  [1230/2757]  eta: 0:15:17  lr: 0.000014  min_lr: 0.000000  loss: 4.9941 (5.0000)  loss_scale: 65536.0000 (145925.4070)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1905  max mem: 15572
Epoch: [1]  [1240/2757]  eta: 0:15:11  lr: 0.000014  min_lr: 0.000000  loss: 4.9672 (4.9994)  loss_scale: 65536.0000 (145277.6277)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1496  max mem: 15572
[2025-01-13 10:59:40,974] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 10:59:40,975] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1250/2757]  eta: 0:15:06  lr: 0.000014  min_lr: 0.000000  loss: 4.9517 (4.9992)  loss_scale: 65536.0000 (144692.5915)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1736  max mem: 15572
Epoch: [1]  [1260/2757]  eta: 0:14:58  lr: 0.000014  min_lr: 0.000000  loss: 4.9685 (4.9992)  loss_scale: 131072.0000 (144584.5773)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1276  max mem: 15572
Epoch: [1]  [1270/2757]  eta: 0:14:54  lr: 0.000014  min_lr: 0.000000  loss: 4.9832 (4.9994)  loss_scale: 131072.0000 (144478.2628)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1789  max mem: 15572
Epoch: [1]  [1280/2757]  eta: 0:14:48  lr: 0.000014  min_lr: 0.000000  loss: 4.9614 (4.9988)  loss_scale: 131072.0000 (144373.6081)  weight_decay: 0.0500 (0.0500)  time: 0.6756  data: 0.2227  max mem: 15572
Epoch: [1]  [1290/2757]  eta: 0:14:42  lr: 0.000014  min_lr: 0.000000  loss: 4.9322 (4.9988)  loss_scale: 131072.0000 (144270.5747)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1443  max mem: 15572
[2025-01-13 11:00:08,021] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4103
[2025-01-13 11:00:08,021] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:00:08,021] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1300/2757]  eta: 0:14:35  lr: 0.000014  min_lr: 0.000000  loss: 4.9322 (4.9985)  loss_scale: 131072.0000 (143816.5104)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1194  max mem: 15572
Epoch: [1]  [1310/2757]  eta: 0:14:30  lr: 0.000014  min_lr: 0.000000  loss: 4.9681 (4.9985)  loss_scale: 65536.0000 (143219.4050)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1158  max mem: 15572
Epoch: [1]  [1320/2757]  eta: 0:14:24  lr: 0.000014  min_lr: 0.000000  loss: 4.9940 (4.9989)  loss_scale: 65536.0000 (142631.3399)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.1510  max mem: 15572
Epoch: [1]  [1330/2757]  eta: 0:14:19  lr: 0.000014  min_lr: 0.000000  loss: 5.0431 (4.9993)  loss_scale: 65536.0000 (142052.1112)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.1713  max mem: 15572
Epoch: [1]  [1340/2757]  eta: 0:14:12  lr: 0.000014  min_lr: 0.000000  loss: 4.9825 (4.9988)  loss_scale: 65536.0000 (141481.5213)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0938  max mem: 15572
Epoch: [1]  [1350/2757]  eta: 0:14:05  lr: 0.000014  min_lr: 0.000000  loss: 4.9558 (4.9989)  loss_scale: 65536.0000 (140919.3782)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0549  max mem: 15572
Epoch: [1]  [1360/2757]  eta: 0:13:59  lr: 0.000014  min_lr: 0.000000  loss: 4.9631 (4.9987)  loss_scale: 65536.0000 (140365.4960)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1108  max mem: 15572
Epoch: [1]  [1370/2757]  eta: 0:13:53  lr: 0.000014  min_lr: 0.000000  loss: 4.9665 (4.9989)  loss_scale: 65536.0000 (139819.6937)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1304  max mem: 15572
Epoch: [1]  [1380/2757]  eta: 0:13:46  lr: 0.000014  min_lr: 0.000000  loss: 4.9902 (4.9990)  loss_scale: 65536.0000 (139281.7958)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1159  max mem: 15572
Epoch: [1]  [1390/2757]  eta: 0:13:39  lr: 0.000014  min_lr: 0.000000  loss: 4.9505 (4.9985)  loss_scale: 65536.0000 (138751.6319)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0707  max mem: 15572
Epoch: [1]  [1400/2757]  eta: 0:13:33  lr: 0.000014  min_lr: 0.000000  loss: 4.9434 (4.9988)  loss_scale: 65536.0000 (138229.0364)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0797  max mem: 15572
Epoch: [1]  [1410/2757]  eta: 0:13:27  lr: 0.000014  min_lr: 0.000000  loss: 4.9884 (4.9989)  loss_scale: 65536.0000 (137713.8483)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0930  max mem: 15572
Epoch: [1]  [1420/2757]  eta: 0:13:20  lr: 0.000014  min_lr: 0.000000  loss: 4.9601 (4.9989)  loss_scale: 65536.0000 (137205.9113)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0600  max mem: 15572
[2025-01-13 11:01:22,177] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:01:22,177] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1430/2757]  eta: 0:13:14  lr: 0.000014  min_lr: 0.000000  loss: 4.9748 (4.9988)  loss_scale: 65536.0000 (137071.4521)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1041  max mem: 15572
Epoch: [1]  [1440/2757]  eta: 0:13:08  lr: 0.000014  min_lr: 0.000000  loss: 4.9967 (4.9985)  loss_scale: 131072.0000 (137029.8182)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1471  max mem: 15572
[2025-01-13 11:01:36,740] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4258
[2025-01-13 11:01:36,741] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:01:36,741] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1450/2757]  eta: 0:13:02  lr: 0.000014  min_lr: 0.000000  loss: 4.9132 (4.9979)  loss_scale: 131072.0000 (136898.4259)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0782  max mem: 15572
Epoch: [1]  [1460/2757]  eta: 0:12:56  lr: 0.000014  min_lr: 0.000000  loss: 4.9065 (4.9978)  loss_scale: 65536.0000 (136409.9767)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0872  max mem: 15572
Epoch: [1]  [1470/2757]  eta: 0:12:50  lr: 0.000014  min_lr: 0.000000  loss: 4.9297 (4.9976)  loss_scale: 65536.0000 (135928.1686)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1510  max mem: 15572
Epoch: [1]  [1480/2757]  eta: 0:12:43  lr: 0.000014  min_lr: 0.000000  loss: 4.9297 (4.9974)  loss_scale: 65536.0000 (135452.8670)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0874  max mem: 15572
Epoch: [1]  [1490/2757]  eta: 0:12:39  lr: 0.000014  min_lr: 0.000000  loss: 4.9438 (4.9975)  loss_scale: 65536.0000 (134983.9410)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1617  max mem: 15572
Epoch: [1]  [1500/2757]  eta: 0:12:32  lr: 0.000014  min_lr: 0.000000  loss: 5.0045 (4.9976)  loss_scale: 65536.0000 (134521.2632)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.1963  max mem: 15572
Epoch: [1]  [1510/2757]  eta: 0:12:26  lr: 0.000015  min_lr: 0.000000  loss: 4.9961 (4.9973)  loss_scale: 65536.0000 (134064.7095)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0981  max mem: 15572
Epoch: [1]  [1520/2757]  eta: 0:12:20  lr: 0.000015  min_lr: 0.000000  loss: 4.9961 (4.9975)  loss_scale: 65536.0000 (133614.1591)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1156  max mem: 15572
Epoch: [1]  [1530/2757]  eta: 0:12:13  lr: 0.000015  min_lr: 0.000000  loss: 4.9656 (4.9974)  loss_scale: 65536.0000 (133169.4944)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0877  max mem: 15572
Epoch: [1]  [1540/2757]  eta: 0:12:07  lr: 0.000015  min_lr: 0.000000  loss: 4.9535 (4.9973)  loss_scale: 65536.0000 (132730.6009)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0735  max mem: 15572
Epoch: [1]  [1550/2757]  eta: 0:12:01  lr: 0.000015  min_lr: 0.000000  loss: 5.0006 (4.9974)  loss_scale: 65536.0000 (132297.3669)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1384  max mem: 15572
Epoch: [1]  [1560/2757]  eta: 0:11:55  lr: 0.000015  min_lr: 0.000000  loss: 5.0134 (4.9974)  loss_scale: 65536.0000 (131869.6835)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1504  max mem: 15572
Epoch: [1]  [1570/2757]  eta: 0:11:49  lr: 0.000015  min_lr: 0.000000  loss: 5.0134 (4.9974)  loss_scale: 65536.0000 (131447.4449)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1084  max mem: 15572
[2025-01-13 11:02:52,886] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:02:52,887] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1580/2757]  eta: 0:11:43  lr: 0.000015  min_lr: 0.000000  loss: 4.9630 (4.9971)  loss_scale: 65536.0000 (131154.9045)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1122  max mem: 15572
Epoch: [1]  [1590/2757]  eta: 0:11:37  lr: 0.000015  min_lr: 0.000000  loss: 4.9565 (4.9971)  loss_scale: 131072.0000 (131154.3834)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1188  max mem: 15572
Epoch: [1]  [1600/2757]  eta: 0:11:30  lr: 0.000015  min_lr: 0.000000  loss: 5.0239 (4.9974)  loss_scale: 131072.0000 (131153.8688)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1015  max mem: 15572
Epoch: [1]  [1610/2757]  eta: 0:11:25  lr: 0.000015  min_lr: 0.000000  loss: 5.0239 (4.9975)  loss_scale: 131072.0000 (131153.3606)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1396  max mem: 15572
[2025-01-13 11:03:16,650] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4427
[2025-01-13 11:03:16,650] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:03:16,650] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1620/2757]  eta: 0:11:19  lr: 0.000015  min_lr: 0.000000  loss: 4.9892 (4.9973)  loss_scale: 131072.0000 (131031.5706)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.1716  max mem: 15572
Epoch: [1]  [1630/2757]  eta: 0:11:13  lr: 0.000015  min_lr: 0.000000  loss: 4.9830 (4.9976)  loss_scale: 65536.0000 (130630.0037)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1164  max mem: 15572
Epoch: [1]  [1640/2757]  eta: 0:11:06  lr: 0.000015  min_lr: 0.000000  loss: 4.9374 (4.9973)  loss_scale: 65536.0000 (130233.3309)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0450  max mem: 15572
Epoch: [1]  [1650/2757]  eta: 0:10:59  lr: 0.000015  min_lr: 0.000000  loss: 4.9389 (4.9972)  loss_scale: 65536.0000 (129841.4634)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0301  max mem: 15572
Epoch: [1]  [1660/2757]  eta: 0:10:54  lr: 0.000015  min_lr: 0.000000  loss: 5.0032 (4.9976)  loss_scale: 65536.0000 (129454.3143)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1326  max mem: 15572
Epoch: [1]  [1670/2757]  eta: 0:10:47  lr: 0.000015  min_lr: 0.000000  loss: 5.0019 (4.9974)  loss_scale: 65536.0000 (129071.7989)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1496  max mem: 15572
Epoch: [1]  [1680/2757]  eta: 0:10:41  lr: 0.000015  min_lr: 0.000000  loss: 5.0019 (4.9974)  loss_scale: 65536.0000 (128693.8346)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0844  max mem: 15572
Epoch: [1]  [1690/2757]  eta: 0:10:35  lr: 0.000015  min_lr: 0.000000  loss: 4.9814 (4.9970)  loss_scale: 65536.0000 (128320.3406)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1185  max mem: 15572
Epoch: [1]  [1700/2757]  eta: 0:10:29  lr: 0.000015  min_lr: 0.000000  loss: 4.9616 (4.9974)  loss_scale: 65536.0000 (127951.2381)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1480  max mem: 15572
Epoch: [1]  [1710/2757]  eta: 0:10:23  lr: 0.000015  min_lr: 0.000000  loss: 4.9914 (4.9974)  loss_scale: 65536.0000 (127586.4500)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1477  max mem: 15572
Epoch: [1]  [1720/2757]  eta: 0:10:17  lr: 0.000015  min_lr: 0.000000  loss: 4.9335 (4.9971)  loss_scale: 65536.0000 (127225.9012)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1406  max mem: 15572
Epoch: [1]  [1730/2757]  eta: 0:10:12  lr: 0.000015  min_lr: 0.000000  loss: 4.9335 (4.9969)  loss_scale: 65536.0000 (126869.5182)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.1666  max mem: 15572
Epoch: [1]  [1740/2757]  eta: 0:10:06  lr: 0.000015  min_lr: 0.000000  loss: 4.9643 (4.9967)  loss_scale: 65536.0000 (126517.2292)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.2034  max mem: 15572
[2025-01-13 11:04:32,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:04:32,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1750/2757]  eta: 0:10:00  lr: 0.000015  min_lr: 0.000000  loss: 4.9643 (4.9969)  loss_scale: 65536.0000 (126318.6750)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.1912  max mem: 15572
[2025-01-13 11:04:34,964] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4561
[2025-01-13 11:04:34,964] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:04:34,964] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1760/2757]  eta: 0:09:54  lr: 0.000015  min_lr: 0.000000  loss: 5.0183 (4.9968)  loss_scale: 65536.0000 (126010.7303)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0911  max mem: 15572
Epoch: [1]  [1770/2757]  eta: 0:09:48  lr: 0.000015  min_lr: 0.000000  loss: 5.0316 (4.9971)  loss_scale: 65536.0000 (125669.2580)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1019  max mem: 15572
Epoch: [1]  [1780/2757]  eta: 0:09:42  lr: 0.000015  min_lr: 0.000000  loss: 5.0339 (4.9973)  loss_scale: 65536.0000 (125331.6204)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1283  max mem: 15572
Epoch: [1]  [1790/2757]  eta: 0:09:37  lr: 0.000015  min_lr: 0.000000  loss: 5.0013 (4.9974)  loss_scale: 65536.0000 (124997.7532)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1246  max mem: 15572
Epoch: [1]  [1800/2757]  eta: 0:09:30  lr: 0.000015  min_lr: 0.000000  loss: 4.9862 (4.9975)  loss_scale: 65536.0000 (124667.5936)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1088  max mem: 15572
Epoch: [1]  [1810/2757]  eta: 0:09:24  lr: 0.000016  min_lr: 0.000000  loss: 4.9861 (4.9973)  loss_scale: 65536.0000 (124341.0801)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0700  max mem: 15572
Epoch: [1]  [1820/2757]  eta: 0:09:17  lr: 0.000016  min_lr: 0.000000  loss: 4.9752 (4.9969)  loss_scale: 65536.0000 (124018.1527)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0543  max mem: 15572
Epoch: [1]  [1830/2757]  eta: 0:09:12  lr: 0.000016  min_lr: 0.000000  loss: 5.0140 (4.9971)  loss_scale: 65536.0000 (123698.7526)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0560  max mem: 15572
Epoch: [1]  [1840/2757]  eta: 0:09:05  lr: 0.000016  min_lr: 0.000000  loss: 5.0212 (4.9973)  loss_scale: 65536.0000 (123382.8224)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0670  max mem: 15572
Epoch: [1]  [1850/2757]  eta: 0:08:59  lr: 0.000016  min_lr: 0.000000  loss: 4.9911 (4.9968)  loss_scale: 65536.0000 (123070.3058)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0948  max mem: 15572
Epoch: [1]  [1860/2757]  eta: 0:08:53  lr: 0.000016  min_lr: 0.000000  loss: 4.9484 (4.9968)  loss_scale: 65536.0000 (122761.1478)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1534  max mem: 15572
Epoch: [1]  [1870/2757]  eta: 0:08:47  lr: 0.000016  min_lr: 0.000000  loss: 4.9486 (4.9967)  loss_scale: 65536.0000 (122455.2945)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1511  max mem: 15572
Epoch: [1]  [1880/2757]  eta: 0:08:41  lr: 0.000016  min_lr: 0.000000  loss: 4.9908 (4.9968)  loss_scale: 65536.0000 (122152.6932)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0813  max mem: 15572
[2025-01-13 11:05:47,904] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:05:47,904] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 11:05:52,622] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4699
[2025-01-13 11:05:52,622] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:05:52,623] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1890/2757]  eta: 0:08:35  lr: 0.000016  min_lr: 0.000000  loss: 4.9401 (4.9966)  loss_scale: 65536.0000 (122165.2036)  weight_decay: 0.0500 (0.0500)  time: 0.4923  data: 0.0398  max mem: 15572
Epoch: [1]  [1900/2757]  eta: 0:08:29  lr: 0.000016  min_lr: 0.000000  loss: 4.9247 (4.9963)  loss_scale: 65536.0000 (121867.3119)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0907  max mem: 15572
Epoch: [1]  [1910/2757]  eta: 0:08:23  lr: 0.000016  min_lr: 0.000000  loss: 4.9977 (4.9963)  loss_scale: 65536.0000 (121572.5379)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.1708  max mem: 15572
Epoch: [1]  [1920/2757]  eta: 0:08:17  lr: 0.000016  min_lr: 0.000000  loss: 5.0219 (4.9966)  loss_scale: 65536.0000 (121280.8329)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1408  max mem: 15572
Epoch: [1]  [1930/2757]  eta: 0:08:10  lr: 0.000016  min_lr: 0.000000  loss: 5.0327 (4.9967)  loss_scale: 65536.0000 (120992.1491)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0564  max mem: 15572
Epoch: [1]  [1940/2757]  eta: 0:08:05  lr: 0.000016  min_lr: 0.000000  loss: 4.9949 (4.9967)  loss_scale: 65536.0000 (120706.4400)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1722  max mem: 15572
[2025-01-13 11:06:25,605] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4755
[2025-01-13 11:06:25,605] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:06:25,605] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [1950/2757]  eta: 0:07:59  lr: 0.000016  min_lr: 0.000000  loss: 4.9644 (4.9964)  loss_scale: 65536.0000 (120339.6822)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1759  max mem: 15572
Epoch: [1]  [1960/2757]  eta: 0:07:53  lr: 0.000016  min_lr: 0.000000  loss: 4.9545 (4.9966)  loss_scale: 32768.0000 (119893.1158)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1188  max mem: 15572
Epoch: [1]  [1970/2757]  eta: 0:07:47  lr: 0.000016  min_lr: 0.000000  loss: 5.0069 (4.9968)  loss_scale: 32768.0000 (119451.0807)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1357  max mem: 15572
Epoch: [1]  [1980/2757]  eta: 0:07:41  lr: 0.000016  min_lr: 0.000000  loss: 5.0282 (4.9968)  loss_scale: 32768.0000 (119013.5083)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0945  max mem: 15572
Epoch: [1]  [1990/2757]  eta: 0:07:36  lr: 0.000016  min_lr: 0.000000  loss: 4.9575 (4.9965)  loss_scale: 32768.0000 (118580.3315)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1680  max mem: 15572
Epoch: [1]  [2000/2757]  eta: 0:07:30  lr: 0.000016  min_lr: 0.000000  loss: 4.9735 (4.9967)  loss_scale: 32768.0000 (118151.4843)  weight_decay: 0.0500 (0.0500)  time: 0.6513  data: 0.1740  max mem: 15572
Epoch: [1]  [2010/2757]  eta: 0:07:23  lr: 0.000016  min_lr: 0.000000  loss: 4.9932 (4.9965)  loss_scale: 32768.0000 (117726.9020)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0801  max mem: 15572
Epoch: [1]  [2020/2757]  eta: 0:07:17  lr: 0.000016  min_lr: 0.000000  loss: 4.9808 (4.9965)  loss_scale: 32768.0000 (117306.5215)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0573  max mem: 15572
Epoch: [1]  [2030/2757]  eta: 0:07:11  lr: 0.000016  min_lr: 0.000000  loss: 4.9839 (4.9964)  loss_scale: 32768.0000 (116890.2806)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0652  max mem: 15572
Epoch: [1]  [2040/2757]  eta: 0:07:05  lr: 0.000016  min_lr: 0.000000  loss: 5.0221 (4.9965)  loss_scale: 32768.0000 (116478.1186)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0816  max mem: 15572
Epoch: [1]  [2050/2757]  eta: 0:06:59  lr: 0.000016  min_lr: 0.000000  loss: 5.0061 (4.9963)  loss_scale: 32768.0000 (116069.9756)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1251  max mem: 15572
Epoch: [1]  [2060/2757]  eta: 0:06:53  lr: 0.000016  min_lr: 0.000000  loss: 4.9785 (4.9962)  loss_scale: 32768.0000 (115665.7933)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1655  max mem: 15572
Epoch: [1]  [2070/2757]  eta: 0:06:47  lr: 0.000016  min_lr: 0.000000  loss: 5.0025 (4.9962)  loss_scale: 32768.0000 (115265.5142)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1354  max mem: 15572
[2025-01-13 11:07:40,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:07:40,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2080/2757]  eta: 0:06:41  lr: 0.000016  min_lr: 0.000000  loss: 4.9996 (4.9961)  loss_scale: 32768.0000 (114963.5598)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1251  max mem: 15572
Epoch: [1]  [2090/2757]  eta: 0:06:36  lr: 0.000016  min_lr: 0.000000  loss: 4.9996 (4.9962)  loss_scale: 65536.0000 (114727.1774)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1653  max mem: 15572
Epoch: [1]  [2100/2757]  eta: 0:06:29  lr: 0.000017  min_lr: 0.000000  loss: 5.0135 (4.9963)  loss_scale: 65536.0000 (114493.0452)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1288  max mem: 15572
Epoch: [1]  [2110/2757]  eta: 0:06:24  lr: 0.000017  min_lr: 0.000000  loss: 4.9278 (4.9958)  loss_scale: 65536.0000 (114261.1312)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0903  max mem: 15572
Epoch: [1]  [2120/2757]  eta: 0:06:17  lr: 0.000017  min_lr: 0.000000  loss: 4.9167 (4.9956)  loss_scale: 65536.0000 (114031.4041)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0957  max mem: 15572
Epoch: [1]  [2130/2757]  eta: 0:06:12  lr: 0.000017  min_lr: 0.000000  loss: 4.9460 (4.9957)  loss_scale: 65536.0000 (113803.8329)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1063  max mem: 15572
Epoch: [1]  [2140/2757]  eta: 0:06:06  lr: 0.000017  min_lr: 0.000000  loss: 4.9498 (4.9954)  loss_scale: 65536.0000 (113578.3877)  weight_decay: 0.0500 (0.0500)  time: 0.6492  data: 0.1860  max mem: 15572
Epoch: [1]  [2150/2757]  eta: 0:06:00  lr: 0.000017  min_lr: 0.000000  loss: 4.9506 (4.9954)  loss_scale: 65536.0000 (113355.0386)  weight_decay: 0.0500 (0.0500)  time: 0.6592  data: 0.2028  max mem: 15572
Epoch: [1]  [2160/2757]  eta: 0:05:54  lr: 0.000017  min_lr: 0.000000  loss: 4.9690 (4.9954)  loss_scale: 65536.0000 (113133.7566)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.0864  max mem: 15572
Epoch: [1]  [2170/2757]  eta: 0:05:48  lr: 0.000017  min_lr: 0.000000  loss: 4.9861 (4.9953)  loss_scale: 65536.0000 (112914.5131)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0772  max mem: 15572
Epoch: [1]  [2180/2757]  eta: 0:05:42  lr: 0.000017  min_lr: 0.000000  loss: 4.9861 (4.9953)  loss_scale: 65536.0000 (112697.2801)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.1961  max mem: 15572
[2025-01-13 11:08:45,588] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4991
[2025-01-13 11:08:45,588] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:08:45,588] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-13 11:08:49,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=25, lr=[1.6299806426952057e-07, 1.6299806426952057e-07, 2.3285437752788657e-07, 2.3285437752788657e-07, 3.326491107541237e-07, 3.326491107541237e-07, 4.752130153630339e-07, 4.752130153630339e-07, 6.788757362329056e-07, 6.788757362329056e-07, 9.698224803327224e-07, 9.698224803327224e-07, 1.3854606861896035e-06, 1.3854606861896035e-06, 1.979229551699434e-06, 1.979229551699434e-06, 2.827470788142048e-06, 2.827470788142048e-06, 4.039243983060069e-06, 4.039243983060069e-06, 5.77034854722867e-06, 5.77034854722867e-06, 8.24335506746953e-06, 8.24335506746953e-06, 1.1776221524956472e-05, 1.1776221524956472e-05, 1.6823173607080675e-05, 1.6823173607080675e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 11:08:49,891] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=27.13429859777948, CurrSamplesPerSec=29.797132748895297, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [2190/2757]  eta: 0:05:36  lr: 0.000017  min_lr: 0.000000  loss: 4.9580 (4.9950)  loss_scale: 65536.0000 (112347.4286)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1596  max mem: 15572
Epoch: [1]  [2200/2757]  eta: 0:05:30  lr: 0.000017  min_lr: 0.000000  loss: 4.9051 (4.9946)  loss_scale: 32768.0000 (111985.8682)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1045  max mem: 15572
Epoch: [1]  [2210/2757]  eta: 0:05:24  lr: 0.000017  min_lr: 0.000000  loss: 4.9492 (4.9943)  loss_scale: 32768.0000 (111627.5785)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1209  max mem: 15572
Epoch: [1]  [2220/2757]  eta: 0:05:18  lr: 0.000017  min_lr: 0.000000  loss: 4.9903 (4.9941)  loss_scale: 32768.0000 (111272.5151)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.1594  max mem: 15572
Epoch: [1]  [2230/2757]  eta: 0:05:12  lr: 0.000017  min_lr: 0.000000  loss: 5.0159 (4.9940)  loss_scale: 32768.0000 (110920.6347)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1408  max mem: 15572
Epoch: [1]  [2240/2757]  eta: 0:05:06  lr: 0.000017  min_lr: 0.000000  loss: 4.9348 (4.9938)  loss_scale: 32768.0000 (110571.8947)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0776  max mem: 15572
Epoch: [1]  [2250/2757]  eta: 0:05:00  lr: 0.000017  min_lr: 0.000000  loss: 4.9922 (4.9937)  loss_scale: 32768.0000 (110226.2532)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0637  max mem: 15572
Epoch: [1]  [2260/2757]  eta: 0:04:54  lr: 0.000017  min_lr: 0.000000  loss: 4.9210 (4.9934)  loss_scale: 32768.0000 (109883.6692)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0667  max mem: 15572
Epoch: [1]  [2270/2757]  eta: 0:04:48  lr: 0.000017  min_lr: 0.000000  loss: 4.9195 (4.9933)  loss_scale: 32768.0000 (109544.1022)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1184  max mem: 15572
Epoch: [1]  [2280/2757]  eta: 0:04:42  lr: 0.000017  min_lr: 0.000000  loss: 4.9787 (4.9932)  loss_scale: 32768.0000 (109207.5125)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1530  max mem: 15572
Epoch: [1]  [2290/2757]  eta: 0:04:37  lr: 0.000017  min_lr: 0.000000  loss: 5.0041 (4.9935)  loss_scale: 32768.0000 (108873.8612)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1723  max mem: 15572
Epoch: [1]  [2300/2757]  eta: 0:04:31  lr: 0.000017  min_lr: 0.000000  loss: 5.0041 (4.9935)  loss_scale: 32768.0000 (108543.1100)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1468  max mem: 15572
Epoch: [1]  [2310/2757]  eta: 0:04:25  lr: 0.000017  min_lr: 0.000000  loss: 4.9789 (4.9934)  loss_scale: 32768.0000 (108215.2211)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1234  max mem: 15572
[2025-01-13 11:10:01,957] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:10:01,957] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2320/2757]  eta: 0:04:19  lr: 0.000017  min_lr: 0.000000  loss: 4.9425 (4.9932)  loss_scale: 32768.0000 (108031.3382)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1210  max mem: 15572
Epoch: [1]  [2330/2757]  eta: 0:04:13  lr: 0.000017  min_lr: 0.000000  loss: 4.9225 (4.9928)  loss_scale: 65536.0000 (107849.0330)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1725  max mem: 15572
Epoch: [1]  [2340/2757]  eta: 0:04:07  lr: 0.000017  min_lr: 0.000000  loss: 4.9243 (4.9925)  loss_scale: 65536.0000 (107668.2853)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.2016  max mem: 15572
Epoch: [1]  [2350/2757]  eta: 0:04:01  lr: 0.000017  min_lr: 0.000000  loss: 4.9378 (4.9921)  loss_scale: 65536.0000 (107489.0753)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1116  max mem: 15572
Epoch: [1]  [2360/2757]  eta: 0:03:55  lr: 0.000017  min_lr: 0.000000  loss: 4.9759 (4.9922)  loss_scale: 65536.0000 (107311.3833)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1081  max mem: 15572
Epoch: [1]  [2370/2757]  eta: 0:03:49  lr: 0.000017  min_lr: 0.000000  loss: 4.9759 (4.9920)  loss_scale: 65536.0000 (107135.1902)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1142  max mem: 15572
Epoch: [1]  [2380/2757]  eta: 0:03:43  lr: 0.000017  min_lr: 0.000000  loss: 4.9286 (4.9920)  loss_scale: 65536.0000 (106960.4771)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1362  max mem: 15572
Epoch: [1]  [2390/2757]  eta: 0:03:37  lr: 0.000018  min_lr: 0.000000  loss: 4.9192 (4.9917)  loss_scale: 65536.0000 (106787.2254)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1265  max mem: 15572
Epoch: [1]  [2400/2757]  eta: 0:03:31  lr: 0.000018  min_lr: 0.000000  loss: 4.9456 (4.9917)  loss_scale: 65536.0000 (106615.4169)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0915  max mem: 15572
Epoch: [1]  [2410/2757]  eta: 0:03:25  lr: 0.000018  min_lr: 0.000000  loss: 4.9916 (4.9918)  loss_scale: 65536.0000 (106445.0336)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1415  max mem: 15572
Epoch: [1]  [2420/2757]  eta: 0:03:19  lr: 0.000018  min_lr: 0.000000  loss: 4.9416 (4.9914)  loss_scale: 65536.0000 (106276.0578)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1153  max mem: 15572
Epoch: [1]  [2430/2757]  eta: 0:03:13  lr: 0.000018  min_lr: 0.000000  loss: 4.9631 (4.9915)  loss_scale: 65536.0000 (106108.4722)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0988  max mem: 15572
[2025-01-13 11:11:17,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:11:17,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2440/2757]  eta: 0:03:08  lr: 0.000018  min_lr: 0.000000  loss: 4.9134 (4.9909)  loss_scale: 65536.0000 (105995.9558)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1521  max mem: 15572
[2025-01-13 11:11:18,063] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5250
[2025-01-13 11:11:18,063] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:11:18,064] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2450/2757]  eta: 0:03:02  lr: 0.000018  min_lr: 0.000000  loss: 4.8871 (4.9907)  loss_scale: 65536.0000 (105830.8805)  weight_decay: 0.0500 (0.0500)  time: 0.6676  data: 0.2325  max mem: 15572
Epoch: [1]  [2460/2757]  eta: 0:02:56  lr: 0.000018  min_lr: 0.000000  loss: 4.9719 (4.9907)  loss_scale: 65536.0000 (105667.1467)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1825  max mem: 15572
Epoch: [1]  [2470/2757]  eta: 0:02:50  lr: 0.000018  min_lr: 0.000000  loss: 4.9303 (4.9905)  loss_scale: 65536.0000 (105504.7382)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1102  max mem: 15572
Epoch: [1]  [2480/2757]  eta: 0:02:44  lr: 0.000018  min_lr: 0.000000  loss: 4.9119 (4.9903)  loss_scale: 65536.0000 (105343.6389)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0717  max mem: 15572
Epoch: [1]  [2490/2757]  eta: 0:02:38  lr: 0.000018  min_lr: 0.000000  loss: 4.8585 (4.9900)  loss_scale: 65536.0000 (105183.8330)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0737  max mem: 15572
Epoch: [1]  [2500/2757]  eta: 0:02:32  lr: 0.000018  min_lr: 0.000000  loss: 4.9656 (4.9901)  loss_scale: 65536.0000 (105025.3051)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0821  max mem: 15572
Epoch: [1]  [2510/2757]  eta: 0:02:26  lr: 0.000018  min_lr: 0.000000  loss: 5.0064 (4.9897)  loss_scale: 65536.0000 (104868.0398)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1213  max mem: 15572
Epoch: [1]  [2520/2757]  eta: 0:02:20  lr: 0.000018  min_lr: 0.000000  loss: 4.9890 (4.9897)  loss_scale: 65536.0000 (104712.0222)  weight_decay: 0.0500 (0.0500)  time: 0.6570  data: 0.1959  max mem: 15572
Epoch: [1]  [2530/2757]  eta: 0:02:14  lr: 0.000018  min_lr: 0.000000  loss: 4.9563 (4.9895)  loss_scale: 65536.0000 (104557.2375)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0933  max mem: 15572
Epoch: [1]  [2540/2757]  eta: 0:02:08  lr: 0.000018  min_lr: 0.000000  loss: 4.9068 (4.9892)  loss_scale: 65536.0000 (104403.6710)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0527  max mem: 15572
Epoch: [1]  [2550/2757]  eta: 0:02:02  lr: 0.000018  min_lr: 0.000000  loss: 4.9101 (4.9890)  loss_scale: 65536.0000 (104251.3085)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1043  max mem: 15572
Epoch: [1]  [2560/2757]  eta: 0:01:56  lr: 0.000018  min_lr: 0.000000  loss: 4.9101 (4.9886)  loss_scale: 65536.0000 (104100.1359)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1519  max mem: 15572
[2025-01-13 11:12:33,013] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:12:33,014] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2570/2757]  eta: 0:01:50  lr: 0.000018  min_lr: 0.000000  loss: 4.8581 (4.9883)  loss_scale: 65536.0000 (103975.6297)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0907  max mem: 15572
[2025-01-13 11:12:35,967] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5383
[2025-01-13 11:12:35,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:12:35,967] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2580/2757]  eta: 0:01:44  lr: 0.000018  min_lr: 0.000000  loss: 4.9109 (4.9881)  loss_scale: 65536.0000 (103902.8718)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0369  max mem: 15572
Epoch: [1]  [2590/2757]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 4.9158 (4.9879)  loss_scale: 65536.0000 (103754.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0371  max mem: 15572
Epoch: [1]  [2600/2757]  eta: 0:01:32  lr: 0.000018  min_lr: 0.000000  loss: 4.9631 (4.9877)  loss_scale: 65536.0000 (103607.8554)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0587  max mem: 15572
Epoch: [1]  [2610/2757]  eta: 0:01:27  lr: 0.000018  min_lr: 0.000000  loss: 4.9631 (4.9876)  loss_scale: 65536.0000 (103462.0421)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1414  max mem: 15572
Epoch: [1]  [2620/2757]  eta: 0:01:21  lr: 0.000018  min_lr: 0.000000  loss: 5.0011 (4.9878)  loss_scale: 65536.0000 (103317.3415)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1509  max mem: 15572
Epoch: [1]  [2630/2757]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 5.0219 (4.9877)  loss_scale: 65536.0000 (103173.7408)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.1768  max mem: 15572
Epoch: [1]  [2640/2757]  eta: 0:01:09  lr: 0.000018  min_lr: 0.000000  loss: 4.9329 (4.9876)  loss_scale: 65536.0000 (103031.2276)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1561  max mem: 15572
Epoch: [1]  [2650/2757]  eta: 0:01:03  lr: 0.000018  min_lr: 0.000000  loss: 4.8936 (4.9874)  loss_scale: 65536.0000 (102889.7895)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1412  max mem: 15572
Epoch: [1]  [2660/2757]  eta: 0:00:57  lr: 0.000018  min_lr: 0.000000  loss: 4.9687 (4.9874)  loss_scale: 65536.0000 (102749.4145)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0994  max mem: 15572
Epoch: [1]  [2670/2757]  eta: 0:00:51  lr: 0.000018  min_lr: 0.000000  loss: 4.9687 (4.9873)  loss_scale: 65536.0000 (102610.0906)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1068  max mem: 15572
[2025-01-13 11:13:32,092] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5481
[2025-01-13 11:13:32,092] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:13:32,093] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [2680/2757]  eta: 0:00:45  lr: 0.000018  min_lr: 0.000000  loss: 4.8841 (4.9869)  loss_scale: 65536.0000 (102361.8053)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1425  max mem: 15572
Epoch: [1]  [2690/2757]  eta: 0:00:39  lr: 0.000019  min_lr: 0.000000  loss: 4.8663 (4.9866)  loss_scale: 32768.0000 (102103.1884)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.0876  max mem: 15572
Epoch: [1]  [2700/2757]  eta: 0:00:33  lr: 0.000019  min_lr: 0.000000  loss: 4.8855 (4.9864)  loss_scale: 32768.0000 (101846.4865)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1256  max mem: 15572
Epoch: [1]  [2710/2757]  eta: 0:00:27  lr: 0.000019  min_lr: 0.000000  loss: 4.9073 (4.9862)  loss_scale: 32768.0000 (101591.6783)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1439  max mem: 15572
Epoch: [1]  [2720/2757]  eta: 0:00:21  lr: 0.000019  min_lr: 0.000000  loss: 4.9439 (4.9862)  loss_scale: 32768.0000 (101338.7431)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1715  max mem: 15572
Epoch: [1]  [2730/2757]  eta: 0:00:15  lr: 0.000019  min_lr: 0.000000  loss: 4.9830 (4.9858)  loss_scale: 32768.0000 (101087.6602)  weight_decay: 0.0500 (0.0500)  time: 0.6590  data: 0.1543  max mem: 15572
Epoch: [1]  [2740/2757]  eta: 0:00:10  lr: 0.000019  min_lr: 0.000000  loss: 4.9021 (4.9856)  loss_scale: 32768.0000 (100838.4093)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1249  max mem: 15572
Epoch: [1]  [2750/2757]  eta: 0:00:04  lr: 0.000019  min_lr: 0.000000  loss: 4.8671 (4.9851)  loss_scale: 32768.0000 (100590.9706)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0855  max mem: 15572
Epoch: [1]  [2756/2757]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 4.9189 (4.9850)  loss_scale: 32768.0000 (100443.3689)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0854  max mem: 15572
Epoch: [1] Total time: 0:27:12 (0.5922 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 4.9189 (4.9850)  loss_scale: 32768.0000 (100443.3689)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 887
Indices to remove: tensor([  137,   281,   356,   458,   575,   842,  1087,  1094,  1117,  1225,
         1415,  1425,  1501,  1750,  1996,  2187,  2376,  2592,  2750,  2798,
         2825,  2889,  3102,  3501,  3581,  3585,  3597,  3606,  3609,  3610,
         3611,  3622,  3633,  3635,  3644,  3652,  3655,  3661,  3666,  3674,
         3682,  3685,  3692,  3709,  3711,  3720,  3729,  3733,  3747,  3748,
         3751,  3757,  3768,  3772,  3782,  3789,  3791,  3803,  3808,  3818,
         3829,  3833,  3846,  3855,  3869,  3871,  3872,  3875,  3900,  3911,
         3920,  3928,  3930,  4072,  4224,  4270,  4277,  4281,  4317,  4326,
         4411,  4543,  4685,  4794,  4888,  5057,  5058,  5099,  5119,  5143,
         5405,  5560,  5574,  5866,  5877,  5884,  5996,  6060,  6269,  6279,
         6449,  6621,  6772,  6775,  6887,  7026,  7085,  7201,  7594,  8048,
         8058,  8180,  8254,  8279,  8354,  8433,  8442,  8459,  8472,  8496,
         8653,  8703,  8747,  8769,  8776,  8837,  8856,  9151,  9164,  9169,
         9309,  9314,  9316,  9318,  9320,  9321,  9325,  9327,  9331,  9340,
         9341,  9346,  9347,  9350,  9354,  9365,  9373,  9374,  9383,  9384,
         9400,  9415,  9416,  9418,  9423,  9425,  9429,  9430,  9431,  9432,
         9434,  9439,  9443,  9444,  9446,  9448,  9457,  9458,  9459,  9462,
         9463,  9467,  9469,  9470,  9471,  9474,  9477,  9479,  9484,  9488,
         9489,  9490,  9491,  9497,  9498,  9505,  9508,  9511,  9513,  9514,
         9517,  9518,  9520,  9525,  9537,  9538,  9545,  9550,  9551,  9556,
         9558,  9564,  9565,  9571,  9574,  9581,  9582,  9587,  9597,  9599,
         9601,  9608,  9611,  9615,  9616,  9625,  9629,  9633,  9635,  9640,
         9644,  9654,  9657,  9659,  9661,  9662,  9670,  9674,  9677,  9678,
         9680,  9683,  9686,  9690,  9693,  9695,  9697,  9700,  9703,  9705,
         9708,  9710,  9726,  9737,  9740,  9744,  9749,  9756,  9757,  9759,
         9771,  9772,  9775,  9779,  9780,  9785,  9786,  9790,  9794,  9807,
         9810,  9811,  9812,  9814,  9816,  9818,  9819,  9833,  9835,  9839,
         9846,  9848,  9851,  9852,  9859,  9861,  9869,  9870,  9874,  9882,
         9884,  9885,  9886,  9889,  9897,  9899,  9905,  9907,  9908,  9919,
         9923,  9926,  9934,  9936,  9937, 10049, 10093, 10132, 10149, 10231,
        10248, 10479, 10555, 10723, 10763, 10848, 10929, 11058, 11117, 11157,
        11286, 11308, 11339, 11340, 11343, 11344, 11346, 11350, 11353, 11363,
        11366, 11373, 11379, 11392, 11409, 11415, 11417, 11428, 11444, 11448,
        11449, 11461, 11466, 11468, 11477, 11486, 11503, 11538, 11550, 11555,
        11569, 11592, 11628, 11631, 11647, 11655, 11688, 11710, 11722, 11742,
        11845, 11952, 12047, 12226, 12281, 12494, 12553, 12767, 12784, 12847,
        13095, 13243, 13248, 13274, 13361, 13716, 13753, 14040, 14133, 14192,
        14240, 14452, 14478, 14690, 14830, 14966, 15051, 15065, 15091, 15199,
        15813, 15965, 16030, 16073, 16536, 16595, 16699, 16716, 16723, 16725,
        16738, 16746, 16747, 16749, 16750, 16766, 16771, 16775, 16779, 16780,
        16787, 16791, 16796, 16808, 16827, 16828, 16849, 16859, 16870, 16877,
        16878, 16882, 16887, 16888, 16889, 16890, 16897, 16898, 16902, 16907,
        16908, 16918, 16919, 16921, 16935, 16945, 16950, 16971, 16977, 16979,
        16995, 17013, 17014, 17022, 17032, 17060, 17061, 17063, 17074, 17076,
        17096, 17098, 17115, 17117, 17118, 17121, 17125, 17139, 17144, 17147,
        17148, 17151, 17155, 17163, 17166, 17167, 17174, 17177, 17202, 17210,
        17217, 17238, 17246, 17248, 17250, 17258, 17261, 17286, 17287, 17291,
        17299, 17313, 17423, 17460, 17497, 17505, 17570, 17598, 17698, 17778,
        17798, 17936, 18036, 18175, 18255, 18259, 18287, 18293, 18453, 18483,
        18542, 18617, 18778, 19357, 19637, 19680, 19815, 19864, 19986, 20017,
        20211, 20304, 20371, 20451, 20792, 20801, 20860, 21034, 21058, 21059,
        21060, 21063, 21064, 21065, 21067, 21069, 21071, 21073, 21076, 21077,
        21078, 21079, 21080, 21081, 21082, 21083, 21087, 21088, 21094, 21095,
        21096, 21097, 21098, 21103, 21104, 21106, 21107, 21111, 21120, 21121,
        21124, 21129, 21130, 21131, 21132, 21133, 21135, 21136, 21138, 21139,
        21141, 21142, 21143, 21146, 21147, 21148, 21149, 21152, 21154, 21155,
        21158, 21160, 21165, 21168, 21172, 21175, 21180, 21183, 21185, 21186,
        21189, 21191, 21192, 21193, 21194, 21196, 21197, 21199, 21200, 21201,
        21203, 21207, 21208, 21210, 21215, 21217, 21222, 21223, 21232, 21237,
        21241, 21242, 21243, 21244, 21245, 21247, 21251, 21253, 21258, 21259,
        21262, 21266, 21267, 21269, 21270, 21271, 21273, 21274, 21275, 21277,
        21282, 21283, 21284, 21285, 21287, 21295, 21296, 21297, 21302, 21303,
        21304, 21306, 21310, 21311, 21312, 21313, 21318, 21321, 21324, 21325,
        21326, 21327, 21330, 21332, 21336, 21338, 21340, 21344, 21350, 21351,
        21352, 21353, 21355, 21356, 21358, 21362, 21365, 21369, 21370, 21380,
        21384, 21386, 21387, 21389, 21391, 21393, 21395, 21407, 21412, 21418,
        21419, 21420, 21423, 21424, 21426, 21430, 21438, 21440, 21441, 21443,
        21444, 21445, 21446, 21447, 21449, 21451, 21452, 21455, 21462, 21468,
        21471, 21472, 21475, 21476, 21477, 21478, 21479, 21480, 21485, 21489,
        21491, 21492, 21496, 21497, 21501, 21502, 21510, 21511, 21512, 21515,
        21516, 21517, 21518, 21519, 21520, 21521, 21528, 21533, 21534, 21537,
        21541, 21542, 21543, 21544, 21551, 21553, 21556, 21559, 21561, 21562,
        21564, 21565, 21568, 21569, 21575, 21577, 21578, 21583, 21587, 21592,
        21594, 21595, 21596, 21598, 21599, 21600, 21601, 21602, 21606, 21607,
        21610, 21611, 21615, 21616, 21618, 21620, 21623, 21624, 21631, 21633,
        21634, 21635, 21636, 21637, 21638, 21639, 21640, 21642, 21645, 21651,
        21652, 21653, 21657, 21658, 21659, 21660, 21662, 21663, 21664, 21666,
        21668, 21669, 21670, 21671, 21676, 21680, 21682, 21683, 21684, 21686,
        21688, 21690, 21692, 21693, 21694, 21696, 21701, 21704, 21706, 21707,
        21708, 21709, 21711, 21712, 21716, 21875, 21927, 22175, 22328, 22331,
        22346, 22664, 22975, 23014, 23104, 23368, 23484, 23654, 23703, 23717,
        24006, 24047, 24061, 24167, 24521, 24828, 24889, 25266, 25395, 25396,
        25732, 25825, 25905, 26092, 26349, 26415, 26551, 26553, 26713, 26718,
        26863, 27295, 27334, 27585, 27627, 27744, 27795, 27797, 27803, 27839,
        27860, 27947, 27987, 28068, 28360, 28396, 28533, 28560, 28594, 28635,
        28740, 28826, 28839, 28874, 29079, 29166, 29284, 29428, 29441, 29675,
        30120, 30189, 30448, 30702, 30731, 30805, 30840, 30841, 30927, 30979,
        30989, 31165, 31369, 31513, 31609, 31817, 32140, 32209, 32503, 32633,
        32638, 32745, 32803, 32847, 33106, 33231, 33665], device='cuda:0')
length of data loader train is: 2683
num_training_steps_per_epoch is: 2683
Change step level LR scheduler!
Set warmup steps = 13415
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:23:39  loss: 4.9225 (4.9225)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 5.2174  data: 5.0295  max mem: 15572
Val:  [ 10/272]  eta: 0:03:48  loss: 5.1651 (5.0149)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (17.1717)  time: 0.8726  data: 0.6722  max mem: 15572
Val:  [ 20/272]  eta: 0:02:21  loss: 5.0438 (4.9354)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (17.9894)  time: 0.3269  data: 0.1237  max mem: 15572
Val:  [ 30/272]  eta: 0:01:51  loss: 4.9878 (4.9187)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (16.3082)  time: 0.2354  data: 0.0166  max mem: 15572
Val:  [ 40/272]  eta: 0:01:43  loss: 4.6589 (4.8182)  acc1: 0.0000 (4.8780)  acc5: 0.0000 (21.9512)  time: 0.3253  data: 0.0913  max mem: 15572
Val:  [ 50/272]  eta: 0:01:37  loss: 4.5862 (4.8486)  acc1: 0.0000 (3.9216)  acc5: 0.0000 (18.3007)  time: 0.4010  data: 0.1730  max mem: 15572
Val:  [ 60/272]  eta: 0:01:27  loss: 4.6897 (4.8518)  acc1: 0.0000 (3.2787)  acc5: 0.0000 (15.6648)  time: 0.3459  data: 0.1386  max mem: 15572
Val:  [ 70/272]  eta: 0:01:23  loss: 4.8062 (4.8302)  acc1: 0.0000 (2.8169)  acc5: 0.0000 (17.2926)  time: 0.3549  data: 0.1314  max mem: 15572
Val:  [ 80/272]  eta: 0:01:14  loss: 4.9655 (4.7981)  acc1: 0.0000 (6.1043)  acc5: 0.0000 (19.3416)  time: 0.3062  data: 0.0909  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 5.0521 (4.8412)  acc1: 0.0000 (5.4335)  acc5: 0.0000 (17.2161)  time: 0.2400  data: 0.0451  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 5.0777 (4.8601)  acc1: 0.0000 (4.8955)  acc5: 0.0000 (17.1067)  time: 0.3094  data: 0.1124  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 4.9217 (4.8851)  acc1: 0.0000 (4.4545)  acc5: 0.0000 (15.5656)  time: 0.3332  data: 0.1296  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 5.1497 (4.9098)  acc1: 0.0000 (4.0863)  acc5: 0.0000 (14.2792)  time: 0.3292  data: 0.1157  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 5.0011 (4.8987)  acc1: 0.0000 (5.6404)  acc5: 0.0000 (15.0551)  time: 0.3095  data: 0.0941  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 4.7183 (4.8888)  acc1: 0.0000 (5.3980)  acc5: 0.0000 (15.8786)  time: 0.3031  data: 0.0849  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 4.7812 (4.8838)  acc1: 0.0000 (5.0405)  acc5: 0.0000 (14.9742)  time: 0.3020  data: 0.0879  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 4.5690 (4.8619)  acc1: 0.0000 (4.7619)  acc5: 0.0000 (16.0455)  time: 0.3060  data: 0.0913  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.5690 (4.8717)  acc1: 0.0000 (4.4834)  acc5: 0.0000 (15.4321)  time: 0.3589  data: 0.1490  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 4.9486 (4.8739)  acc1: 0.0000 (4.2357)  acc5: 0.0000 (14.5795)  time: 0.3584  data: 0.1565  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.9486 (4.8835)  acc1: 0.0000 (4.0140)  acc5: 0.0000 (13.8162)  time: 0.3386  data: 0.1443  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 4.8989 (4.8949)  acc1: 0.0000 (3.8143)  acc5: 0.0000 (13.1288)  time: 0.3373  data: 0.1376  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 4.9099 (4.9043)  acc1: 0.0000 (3.6335)  acc5: 0.0000 (12.5329)  time: 0.2676  data: 0.0541  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.8587 (4.8951)  acc1: 0.0000 (3.5194)  acc5: 0.0000 (13.0970)  time: 0.2717  data: 0.0604  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.6951 (4.8872)  acc1: 0.0000 (3.3670)  acc5: 0.0000 (12.5301)  time: 0.3221  data: 0.1059  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 4.7023 (4.8897)  acc1: 0.0000 (3.2273)  acc5: 0.0000 (12.0101)  time: 0.3435  data: 0.1257  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 5.2036 (4.9069)  acc1: 0.0000 (3.0987)  acc5: 0.0000 (11.6866)  time: 0.3662  data: 0.1613  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.9021 (4.8984)  acc1: 0.0000 (2.9800)  acc5: 0.0000 (11.3027)  time: 0.3149  data: 0.1266  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 4.7142 (4.8972)  acc1: 0.0000 (2.8700)  acc5: 0.0000 (11.4801)  time: 0.2051  data: 0.0453  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 4.7142 (4.8989)  acc1: 0.0000 (2.8671)  acc5: 0.0000 (11.4684)  time: 0.1966  data: 0.0453  max mem: 15572
Val: Total time: 0:01:30 (0.3335 s / it)
* Acc@1 2.867 Acc@5 11.468 loss 4.899
Accuracy of the network on the 4883 val videos: 2.9%
[2025-01-13 11:15:52,192] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 11:15:52,196] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 11:15:52,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 11:15:55,087] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 11:15:55,088] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 2.87%
Epoch: [2]  [   0/2683]  eta: 4:55:43  lr: 0.000019  min_lr: 0.000000  loss: 4.9849 (4.9849)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.6132  data: 6.1100  max mem: 15572
Epoch: [2]  [  10/2683]  eta: 0:51:36  lr: 0.000019  min_lr: 0.000000  loss: 4.9254 (4.9226)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1583  data: 0.7120  max mem: 15572
Epoch: [2]  [  20/2683]  eta: 0:41:01  lr: 0.000019  min_lr: 0.000000  loss: 4.9107 (4.8888)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.1957  max mem: 15572
Epoch: [2]  [  30/2683]  eta: 0:34:27  lr: 0.000019  min_lr: 0.000000  loss: 4.9012 (4.9025)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1100  max mem: 15572
Epoch: [2]  [  40/2683]  eta: 0:32:07  lr: 0.000019  min_lr: 0.000000  loss: 4.9304 (4.9242)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0672  max mem: 15572
[2025-01-13 11:16:28,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:16:28,290] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [  50/2683]  eta: 0:30:54  lr: 0.000019  min_lr: 0.000000  loss: 4.9502 (4.9347)  loss_scale: 32768.0000 (37265.5686)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1328  max mem: 15572
Epoch: [2]  [  60/2683]  eta: 0:29:48  lr: 0.000019  min_lr: 0.000000  loss: 4.9262 (4.9337)  loss_scale: 65536.0000 (41900.0656)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1216  max mem: 15572
Epoch: [2]  [  70/2683]  eta: 0:29:39  lr: 0.000019  min_lr: 0.000000  loss: 4.9449 (4.9329)  loss_scale: 65536.0000 (45229.0704)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1613  max mem: 15572
Epoch: [2]  [  80/2683]  eta: 0:29:24  lr: 0.000019  min_lr: 0.000000  loss: 4.9449 (4.9315)  loss_scale: 65536.0000 (47736.0988)  weight_decay: 0.0500 (0.0500)  time: 0.6651  data: 0.1848  max mem: 15572
Epoch: [2]  [  90/2683]  eta: 0:29:00  lr: 0.000019  min_lr: 0.000000  loss: 4.9591 (4.9342)  loss_scale: 65536.0000 (49692.1319)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.1411  max mem: 15572
Epoch: [2]  [ 100/2683]  eta: 0:28:43  lr: 0.000019  min_lr: 0.000000  loss: 4.9370 (4.9327)  loss_scale: 65536.0000 (51260.8317)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1345  max mem: 15572
Epoch: [2]  [ 110/2683]  eta: 0:28:24  lr: 0.000019  min_lr: 0.000000  loss: 4.8574 (4.9256)  loss_scale: 65536.0000 (52546.8829)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1415  max mem: 15572
Epoch: [2]  [ 120/2683]  eta: 0:28:14  lr: 0.000019  min_lr: 0.000000  loss: 4.8790 (4.9286)  loss_scale: 65536.0000 (53620.3636)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1531  max mem: 15572
Epoch: [2]  [ 130/2683]  eta: 0:27:46  lr: 0.000019  min_lr: 0.000000  loss: 4.9510 (4.9271)  loss_scale: 65536.0000 (54529.9542)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1407  max mem: 15572
Epoch: [2]  [ 140/2683]  eta: 0:27:29  lr: 0.000019  min_lr: 0.000000  loss: 4.8824 (4.9255)  loss_scale: 65536.0000 (55310.5248)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1224  max mem: 15572
Epoch: [2]  [ 150/2683]  eta: 0:27:18  lr: 0.000019  min_lr: 0.000000  loss: 4.9157 (4.9300)  loss_scale: 65536.0000 (55987.7086)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1436  max mem: 15572
Epoch: [2]  [ 160/2683]  eta: 0:26:49  lr: 0.000019  min_lr: 0.000000  loss: 4.9157 (4.9292)  loss_scale: 65536.0000 (56580.7702)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1040  max mem: 15572
Epoch: [2]  [ 170/2683]  eta: 0:26:30  lr: 0.000019  min_lr: 0.000000  loss: 4.9351 (4.9316)  loss_scale: 65536.0000 (57104.4678)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0695  max mem: 15572
[2025-01-13 11:17:44,952] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:17:44,953] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 11:17:45,433] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5739
[2025-01-13 11:17:45,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:17:45,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 180/2683]  eta: 0:26:25  lr: 0.000019  min_lr: 0.000000  loss: 4.9583 (4.9301)  loss_scale: 65536.0000 (57932.3757)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1305  max mem: 15572
Epoch: [2]  [ 190/2683]  eta: 0:26:12  lr: 0.000019  min_lr: 0.000000  loss: 4.8864 (4.9284)  loss_scale: 65536.0000 (58330.4712)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1435  max mem: 15572
Epoch: [2]  [ 200/2683]  eta: 0:26:05  lr: 0.000019  min_lr: 0.000000  loss: 4.9087 (4.9284)  loss_scale: 65536.0000 (58688.9552)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1461  max mem: 15572
Epoch: [2]  [ 210/2683]  eta: 0:25:46  lr: 0.000019  min_lr: 0.000000  loss: 4.9386 (4.9295)  loss_scale: 65536.0000 (59013.4597)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1308  max mem: 15572
Epoch: [2]  [ 220/2683]  eta: 0:25:47  lr: 0.000020  min_lr: 0.000000  loss: 4.8933 (4.9291)  loss_scale: 65536.0000 (59308.5973)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1568  max mem: 15572
[2025-01-13 11:18:16,898] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5791
[2025-01-13 11:18:16,898] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:18:16,898] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 230/2683]  eta: 0:25:46  lr: 0.000020  min_lr: 0.000000  loss: 4.9079 (4.9306)  loss_scale: 65536.0000 (58727.0649)  weight_decay: 0.0500 (0.0500)  time: 0.6831  data: 0.1953  max mem: 15572
Epoch: [2]  [ 240/2683]  eta: 0:25:34  lr: 0.000020  min_lr: 0.000000  loss: 4.9163 (4.9309)  loss_scale: 32768.0000 (57649.9253)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1454  max mem: 15572
Epoch: [2]  [ 250/2683]  eta: 0:25:08  lr: 0.000020  min_lr: 0.000000  loss: 4.9331 (4.9317)  loss_scale: 32768.0000 (56658.6135)  weight_decay: 0.0500 (0.0500)  time: 0.5015  data: 0.0661  max mem: 15572
Epoch: [2]  [ 260/2683]  eta: 0:24:44  lr: 0.000020  min_lr: 0.000000  loss: 4.9379 (4.9307)  loss_scale: 32768.0000 (55743.2644)  weight_decay: 0.0500 (0.0500)  time: 0.4274  data: 0.0006  max mem: 15572
Epoch: [2]  [ 270/2683]  eta: 0:24:28  lr: 0.000020  min_lr: 0.000000  loss: 4.9441 (4.9301)  loss_scale: 32768.0000 (54895.4686)  weight_decay: 0.0500 (0.0500)  time: 0.4656  data: 0.0009  max mem: 15572
Epoch: [2]  [ 280/2683]  eta: 0:24:12  lr: 0.000020  min_lr: 0.000000  loss: 4.9664 (4.9312)  loss_scale: 32768.0000 (54108.0142)  weight_decay: 0.0500 (0.0500)  time: 0.4930  data: 0.0010  max mem: 15572
Epoch: [2]  [ 290/2683]  eta: 0:24:02  lr: 0.000020  min_lr: 0.000000  loss: 4.8597 (4.9297)  loss_scale: 32768.0000 (53374.6804)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0420  max mem: 15572
Epoch: [2]  [ 300/2683]  eta: 0:24:08  lr: 0.000020  min_lr: 0.000000  loss: 4.8597 (4.9279)  loss_scale: 32768.0000 (52690.0731)  weight_decay: 0.0500 (0.0500)  time: 0.6584  data: 0.1886  max mem: 15572
Epoch: [2]  [ 310/2683]  eta: 0:24:08  lr: 0.000020  min_lr: 0.000000  loss: 4.9542 (4.9288)  loss_scale: 32768.0000 (52049.4920)  weight_decay: 0.0500 (0.0500)  time: 0.7193  data: 0.2492  max mem: 15572
Epoch: [2]  [ 320/2683]  eta: 0:24:03  lr: 0.000020  min_lr: 0.000000  loss: 4.9125 (4.9264)  loss_scale: 32768.0000 (51448.8224)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.1730  max mem: 15572
Epoch: [2]  [ 330/2683]  eta: 0:24:04  lr: 0.000020  min_lr: 0.000000  loss: 4.8123 (4.9240)  loss_scale: 32768.0000 (50884.4471)  weight_decay: 0.0500 (0.0500)  time: 0.6703  data: 0.1620  max mem: 15572
Epoch: [2]  [ 340/2683]  eta: 0:24:09  lr: 0.000020  min_lr: 0.000000  loss: 4.8417 (4.9231)  loss_scale: 32768.0000 (50353.1730)  weight_decay: 0.0500 (0.0500)  time: 0.7459  data: 0.2429  max mem: 15572
Epoch: [2]  [ 350/2683]  eta: 0:24:08  lr: 0.000020  min_lr: 0.000000  loss: 4.8975 (4.9228)  loss_scale: 32768.0000 (49852.1709)  weight_decay: 0.0500 (0.0500)  time: 0.7362  data: 0.2707  max mem: 15572
[2025-01-13 11:19:35,345] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:19:35,346] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 360/2683]  eta: 0:24:04  lr: 0.000020  min_lr: 0.000000  loss: 4.8697 (4.9210)  loss_scale: 32768.0000 (50014.3158)  weight_decay: 0.0500 (0.0500)  time: 0.6759  data: 0.1926  max mem: 15572
Epoch: [2]  [ 370/2683]  eta: 0:24:01  lr: 0.000020  min_lr: 0.000000  loss: 4.9545 (4.9225)  loss_scale: 65536.0000 (50432.6900)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.1776  max mem: 15572
Epoch: [2]  [ 380/2683]  eta: 0:23:57  lr: 0.000020  min_lr: 0.000000  loss: 4.9545 (4.9221)  loss_scale: 65536.0000 (50829.1024)  weight_decay: 0.0500 (0.0500)  time: 0.6665  data: 0.2004  max mem: 15572
Epoch: [2]  [ 390/2683]  eta: 0:23:54  lr: 0.000020  min_lr: 0.000000  loss: 4.9123 (4.9226)  loss_scale: 65536.0000 (51205.2379)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.2063  max mem: 15572
[2025-01-13 11:20:00,835] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5958
[2025-01-13 11:20:00,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:20:00,836] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 400/2683]  eta: 0:23:39  lr: 0.000020  min_lr: 0.000000  loss: 4.9035 (4.9209)  loss_scale: 65536.0000 (50827.1721)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1181  max mem: 15572
Epoch: [2]  [ 410/2683]  eta: 0:23:22  lr: 0.000020  min_lr: 0.000000  loss: 4.8600 (4.9196)  loss_scale: 32768.0000 (50387.7762)  weight_decay: 0.0500 (0.0500)  time: 0.4458  data: 0.0085  max mem: 15572
Epoch: [2]  [ 420/2683]  eta: 0:23:08  lr: 0.000020  min_lr: 0.000000  loss: 4.8756 (4.9190)  loss_scale: 32768.0000 (49969.2542)  weight_decay: 0.0500 (0.0500)  time: 0.4516  data: 0.0006  max mem: 15572
Epoch: [2]  [ 430/2683]  eta: 0:23:01  lr: 0.000020  min_lr: 0.000000  loss: 4.9030 (4.9182)  loss_scale: 32768.0000 (49570.1531)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0826  max mem: 15572
[2025-01-13 11:20:20,984] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=31, lr=[1.963408232595816e-07, 1.963408232595816e-07, 2.804868903708309e-07, 2.804868903708309e-07, 4.006955576726156e-07, 4.006955576726156e-07, 5.724222252465937e-07, 5.724222252465937e-07, 8.177460360665625e-07, 8.177460360665625e-07, 1.1682086229522321e-06, 1.1682086229522321e-06, 1.6688694613603317e-06, 1.6688694613603317e-06, 2.38409923051476e-06, 2.38409923051476e-06, 3.405856043592514e-06, 3.405856043592514e-06, 4.865508633703593e-06, 4.865508633703593e-06, 6.95072661957656e-06, 6.95072661957656e-06, 9.929609456537945e-06, 9.929609456537945e-06, 1.4185156366482778e-05, 1.4185156366482778e-05, 2.02645090949754e-05, 2.02645090949754e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 11:20:20,985] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=27.15328536493823, CurrSamplesPerSec=29.22809010295988, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [ 440/2683]  eta: 0:22:56  lr: 0.000020  min_lr: 0.000000  loss: 4.9537 (4.9198)  loss_scale: 32768.0000 (49189.1519)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1688  max mem: 15572
Epoch: [2]  [ 450/2683]  eta: 0:22:48  lr: 0.000020  min_lr: 0.000000  loss: 5.0038 (4.9196)  loss_scale: 32768.0000 (48825.0466)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1554  max mem: 15572
Epoch: [2]  [ 460/2683]  eta: 0:22:37  lr: 0.000020  min_lr: 0.000000  loss: 4.8993 (4.9198)  loss_scale: 32768.0000 (48476.7375)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0983  max mem: 15572
Epoch: [2]  [ 470/2683]  eta: 0:22:31  lr: 0.000020  min_lr: 0.000000  loss: 4.8531 (4.9186)  loss_scale: 32768.0000 (48143.2187)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1167  max mem: 15572
Epoch: [2]  [ 480/2683]  eta: 0:22:22  lr: 0.000020  min_lr: 0.000000  loss: 4.8998 (4.9188)  loss_scale: 32768.0000 (47823.5676)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1299  max mem: 15572
Epoch: [2]  [ 490/2683]  eta: 0:22:21  lr: 0.000020  min_lr: 0.000000  loss: 4.8575 (4.9178)  loss_scale: 32768.0000 (47516.9369)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1702  max mem: 15572
Epoch: [2]  [ 500/2683]  eta: 0:22:10  lr: 0.000020  min_lr: 0.000000  loss: 4.8207 (4.9169)  loss_scale: 32768.0000 (47222.5469)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1668  max mem: 15572
Epoch: [2]  [ 510/2683]  eta: 0:22:07  lr: 0.000021  min_lr: 0.000000  loss: 4.8144 (4.9150)  loss_scale: 32768.0000 (46939.6791)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1489  max mem: 15572
Epoch: [2]  [ 520/2683]  eta: 0:22:04  lr: 0.000021  min_lr: 0.000000  loss: 4.8580 (4.9142)  loss_scale: 32768.0000 (46667.6699)  weight_decay: 0.0500 (0.0500)  time: 0.6778  data: 0.1873  max mem: 15572
[2025-01-13 11:21:14,727] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:21:14,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 530/2683]  eta: 0:21:55  lr: 0.000021  min_lr: 0.000000  loss: 4.9544 (4.9148)  loss_scale: 32768.0000 (47023.0056)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1314  max mem: 15572
Epoch: [2]  [ 540/2683]  eta: 0:21:47  lr: 0.000021  min_lr: 0.000000  loss: 4.9544 (4.9154)  loss_scale: 65536.0000 (47365.2052)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1240  max mem: 15572
Epoch: [2]  [ 550/2683]  eta: 0:21:41  lr: 0.000021  min_lr: 0.000000  loss: 4.9392 (4.9157)  loss_scale: 65536.0000 (47694.9837)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1647  max mem: 15572
Epoch: [2]  [ 560/2683]  eta: 0:21:32  lr: 0.000021  min_lr: 0.000000  loss: 4.9045 (4.9152)  loss_scale: 65536.0000 (48013.0053)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1381  max mem: 15572
Epoch: [2]  [ 570/2683]  eta: 0:21:28  lr: 0.000021  min_lr: 0.000000  loss: 4.8291 (4.9145)  loss_scale: 65536.0000 (48319.8879)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1488  max mem: 15572
Epoch: [2]  [ 580/2683]  eta: 0:21:20  lr: 0.000021  min_lr: 0.000000  loss: 4.8291 (4.9133)  loss_scale: 65536.0000 (48616.2065)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1414  max mem: 15572
Epoch: [2]  [ 590/2683]  eta: 0:21:17  lr: 0.000021  min_lr: 0.000000  loss: 4.8834 (4.9127)  loss_scale: 65536.0000 (48902.4975)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1435  max mem: 15572
Epoch: [2]  [ 600/2683]  eta: 0:21:09  lr: 0.000021  min_lr: 0.000000  loss: 4.8731 (4.9126)  loss_scale: 65536.0000 (49179.2612)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1596  max mem: 15572
Epoch: [2]  [ 610/2683]  eta: 0:21:03  lr: 0.000021  min_lr: 0.000000  loss: 4.9209 (4.9132)  loss_scale: 65536.0000 (49446.9656)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1400  max mem: 15572
Epoch: [2]  [ 620/2683]  eta: 0:20:54  lr: 0.000021  min_lr: 0.000000  loss: 4.9225 (4.9139)  loss_scale: 65536.0000 (49706.0483)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1182  max mem: 15572
Epoch: [2]  [ 630/2683]  eta: 0:20:47  lr: 0.000021  min_lr: 0.000000  loss: 4.8995 (4.9139)  loss_scale: 65536.0000 (49956.9192)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1020  max mem: 15572
Epoch: [2]  [ 640/2683]  eta: 0:20:44  lr: 0.000021  min_lr: 0.000000  loss: 4.8783 (4.9136)  loss_scale: 65536.0000 (50199.9626)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.1987  max mem: 15572
[2025-01-13 11:22:31,637] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:22:31,637] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 650/2683]  eta: 0:20:38  lr: 0.000021  min_lr: 0.000000  loss: 4.8592 (4.9121)  loss_scale: 65536.0000 (50636.8786)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.2119  max mem: 15572
[2025-01-13 11:22:32,474] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6217
[2025-01-13 11:22:32,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:22:32,475] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 660/2683]  eta: 0:20:33  lr: 0.000021  min_lr: 0.000000  loss: 4.9225 (4.9126)  loss_scale: 65536.0000 (50862.2814)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1706  max mem: 15572
Epoch: [2]  [ 670/2683]  eta: 0:20:27  lr: 0.000021  min_lr: 0.000000  loss: 4.9028 (4.9118)  loss_scale: 65536.0000 (51080.9657)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1652  max mem: 15572
Epoch: [2]  [ 680/2683]  eta: 0:20:22  lr: 0.000021  min_lr: 0.000000  loss: 4.8733 (4.9118)  loss_scale: 65536.0000 (51293.2276)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1832  max mem: 15572
Epoch: [2]  [ 690/2683]  eta: 0:20:16  lr: 0.000021  min_lr: 0.000000  loss: 4.8733 (4.9117)  loss_scale: 65536.0000 (51499.3459)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.1940  max mem: 15572
Epoch: [2]  [ 700/2683]  eta: 0:20:08  lr: 0.000021  min_lr: 0.000000  loss: 4.9714 (4.9126)  loss_scale: 65536.0000 (51699.5835)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1507  max mem: 15572
Epoch: [2]  [ 710/2683]  eta: 0:20:03  lr: 0.000021  min_lr: 0.000000  loss: 4.9588 (4.9121)  loss_scale: 65536.0000 (51894.1885)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1461  max mem: 15572
Epoch: [2]  [ 720/2683]  eta: 0:19:57  lr: 0.000021  min_lr: 0.000000  loss: 4.8658 (4.9111)  loss_scale: 65536.0000 (52083.3953)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1474  max mem: 15572
Epoch: [2]  [ 730/2683]  eta: 0:19:50  lr: 0.000021  min_lr: 0.000000  loss: 4.8658 (4.9111)  loss_scale: 65536.0000 (52267.4254)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1085  max mem: 15572
Epoch: [2]  [ 740/2683]  eta: 0:19:42  lr: 0.000021  min_lr: 0.000000  loss: 4.8973 (4.9112)  loss_scale: 65536.0000 (52446.4885)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0950  max mem: 15572
Epoch: [2]  [ 750/2683]  eta: 0:19:36  lr: 0.000021  min_lr: 0.000000  loss: 4.8780 (4.9111)  loss_scale: 65536.0000 (52620.7830)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1248  max mem: 15572
Epoch: [2]  [ 760/2683]  eta: 0:19:29  lr: 0.000021  min_lr: 0.000000  loss: 4.8815 (4.9113)  loss_scale: 65536.0000 (52790.4967)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1353  max mem: 15572
Epoch: [2]  [ 770/2683]  eta: 0:19:20  lr: 0.000021  min_lr: 0.000000  loss: 4.9334 (4.9109)  loss_scale: 65536.0000 (52955.8080)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0761  max mem: 15572
[2025-01-13 11:23:48,943] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:23:48,944] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 780/2683]  eta: 0:19:13  lr: 0.000021  min_lr: 0.000000  loss: 4.8979 (4.9101)  loss_scale: 65536.0000 (53200.7990)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0569  max mem: 15572
[2025-01-13 11:23:50,378] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6349
[2025-01-13 11:23:50,379] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:23:50,379] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 790/2683]  eta: 0:19:07  lr: 0.000022  min_lr: 0.000000  loss: 4.8716 (4.9093)  loss_scale: 65536.0000 (53522.4475)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0916  max mem: 15572
Epoch: [2]  [ 800/2683]  eta: 0:19:00  lr: 0.000022  min_lr: 0.000000  loss: 4.8302 (4.9095)  loss_scale: 65536.0000 (53672.4295)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1116  max mem: 15572
Epoch: [2]  [ 810/2683]  eta: 0:18:59  lr: 0.000022  min_lr: 0.000000  loss: 4.8982 (4.9092)  loss_scale: 65536.0000 (53818.7127)  weight_decay: 0.0500 (0.0500)  time: 0.6930  data: 0.2419  max mem: 15572
Epoch: [2]  [ 820/2683]  eta: 0:18:49  lr: 0.000022  min_lr: 0.000000  loss: 4.9122 (4.9095)  loss_scale: 65536.0000 (53961.4324)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.1825  max mem: 15572
Epoch: [2]  [ 830/2683]  eta: 0:18:42  lr: 0.000022  min_lr: 0.000000  loss: 4.9221 (4.9103)  loss_scale: 65536.0000 (54100.7172)  weight_decay: 0.0500 (0.0500)  time: 0.5132  data: 0.0440  max mem: 15572
Epoch: [2]  [ 840/2683]  eta: 0:18:35  lr: 0.000022  min_lr: 0.000000  loss: 4.8979 (4.9095)  loss_scale: 65536.0000 (54236.6897)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0873  max mem: 15572
Epoch: [2]  [ 850/2683]  eta: 0:18:29  lr: 0.000022  min_lr: 0.000000  loss: 4.8636 (4.9089)  loss_scale: 65536.0000 (54369.4665)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1351  max mem: 15572
Epoch: [2]  [ 860/2683]  eta: 0:18:23  lr: 0.000022  min_lr: 0.000000  loss: 4.8198 (4.9080)  loss_scale: 65536.0000 (54499.1591)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1885  max mem: 15572
Epoch: [2]  [ 870/2683]  eta: 0:18:16  lr: 0.000022  min_lr: 0.000000  loss: 4.8198 (4.9074)  loss_scale: 65536.0000 (54625.8737)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1454  max mem: 15572
Epoch: [2]  [ 880/2683]  eta: 0:18:08  lr: 0.000022  min_lr: 0.000000  loss: 4.9464 (4.9081)  loss_scale: 65536.0000 (54749.7117)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0524  max mem: 15572
Epoch: [2]  [ 890/2683]  eta: 0:18:04  lr: 0.000022  min_lr: 0.000000  loss: 4.9190 (4.9072)  loss_scale: 65536.0000 (54870.7699)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1112  max mem: 15572
Epoch: [2]  [ 900/2683]  eta: 0:17:55  lr: 0.000022  min_lr: 0.000000  loss: 4.8519 (4.9073)  loss_scale: 65536.0000 (54989.1410)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1074  max mem: 15572
Epoch: [2]  [ 910/2683]  eta: 0:17:49  lr: 0.000022  min_lr: 0.000000  loss: 4.8838 (4.9070)  loss_scale: 65536.0000 (55104.9133)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0470  max mem: 15572
[2025-01-13 11:25:05,983] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:25:05,984] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 11:25:07,168] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6480
[2025-01-13 11:25:07,168] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:25:07,168] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 920/2683]  eta: 0:17:45  lr: 0.000022  min_lr: 0.000000  loss: 4.8838 (4.9072)  loss_scale: 65536.0000 (55360.4864)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.1372  max mem: 15572
Epoch: [2]  [ 930/2683]  eta: 0:17:36  lr: 0.000022  min_lr: 0.000000  loss: 4.9010 (4.9067)  loss_scale: 65536.0000 (55469.7830)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1045  max mem: 15572
Epoch: [2]  [ 940/2683]  eta: 0:17:31  lr: 0.000022  min_lr: 0.000000  loss: 4.8711 (4.9061)  loss_scale: 65536.0000 (55576.7566)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0718  max mem: 15572
Epoch: [2]  [ 950/2683]  eta: 0:17:24  lr: 0.000022  min_lr: 0.000000  loss: 4.8649 (4.9059)  loss_scale: 65536.0000 (55681.4805)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1043  max mem: 15572
Epoch: [2]  [ 960/2683]  eta: 0:17:17  lr: 0.000022  min_lr: 0.000000  loss: 4.8342 (4.9052)  loss_scale: 65536.0000 (55784.0250)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1122  max mem: 15572
[2025-01-13 11:25:34,680] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6527
[2025-01-13 11:25:34,680] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:25:34,681] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 970/2683]  eta: 0:17:12  lr: 0.000022  min_lr: 0.000000  loss: 4.8421 (4.9047)  loss_scale: 32768.0000 (55546.9907)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1535  max mem: 15572
Epoch: [2]  [ 980/2683]  eta: 0:17:06  lr: 0.000022  min_lr: 0.000000  loss: 4.8914 (4.9049)  loss_scale: 32768.0000 (55314.7890)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1480  max mem: 15572
Epoch: [2]  [ 990/2683]  eta: 0:17:00  lr: 0.000022  min_lr: 0.000000  loss: 4.8949 (4.9044)  loss_scale: 32768.0000 (55087.2735)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1405  max mem: 15572
Epoch: [2]  [1000/2683]  eta: 0:16:55  lr: 0.000022  min_lr: 0.000000  loss: 4.9220 (4.9047)  loss_scale: 32768.0000 (54864.3037)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.1678  max mem: 15572
Epoch: [2]  [1010/2683]  eta: 0:16:48  lr: 0.000022  min_lr: 0.000000  loss: 4.9363 (4.9044)  loss_scale: 32768.0000 (54645.7448)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1279  max mem: 15572
Epoch: [2]  [1020/2683]  eta: 0:16:42  lr: 0.000022  min_lr: 0.000000  loss: 4.9363 (4.9045)  loss_scale: 32768.0000 (54431.4672)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1428  max mem: 15572
Epoch: [2]  [1030/2683]  eta: 0:16:35  lr: 0.000022  min_lr: 0.000000  loss: 4.8575 (4.9038)  loss_scale: 32768.0000 (54221.3463)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1467  max mem: 15572
Epoch: [2]  [1040/2683]  eta: 0:16:29  lr: 0.000022  min_lr: 0.000000  loss: 4.8714 (4.9037)  loss_scale: 32768.0000 (54015.2622)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1131  max mem: 15572
Epoch: [2]  [1050/2683]  eta: 0:16:23  lr: 0.000022  min_lr: 0.000000  loss: 4.8380 (4.9026)  loss_scale: 32768.0000 (53813.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1270  max mem: 15572
Epoch: [2]  [1060/2683]  eta: 0:16:17  lr: 0.000022  min_lr: 0.000000  loss: 4.8380 (4.9020)  loss_scale: 32768.0000 (53614.7484)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1371  max mem: 15572
Epoch: [2]  [1070/2683]  eta: 0:16:11  lr: 0.000022  min_lr: 0.000000  loss: 4.8493 (4.9015)  loss_scale: 32768.0000 (53420.1008)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1599  max mem: 15572
Epoch: [2]  [1080/2683]  eta: 0:16:05  lr: 0.000023  min_lr: 0.000000  loss: 4.8484 (4.9003)  loss_scale: 32768.0000 (53229.0546)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1474  max mem: 15572
[2025-01-13 11:26:52,442] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:26:52,443] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1090/2683]  eta: 0:15:59  lr: 0.000023  min_lr: 0.000000  loss: 4.7518 (4.8990)  loss_scale: 32768.0000 (53071.5454)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1334  max mem: 15572
Epoch: [2]  [1100/2683]  eta: 0:15:53  lr: 0.000023  min_lr: 0.000000  loss: 4.7985 (4.8984)  loss_scale: 65536.0000 (53184.7557)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1419  max mem: 15572
Epoch: [2]  [1110/2683]  eta: 0:15:47  lr: 0.000023  min_lr: 0.000000  loss: 4.8319 (4.8981)  loss_scale: 65536.0000 (53295.9280)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1289  max mem: 15572
Epoch: [2]  [1120/2683]  eta: 0:15:41  lr: 0.000023  min_lr: 0.000000  loss: 4.9073 (4.8977)  loss_scale: 65536.0000 (53405.1169)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1219  max mem: 15572
Epoch: [2]  [1130/2683]  eta: 0:15:35  lr: 0.000023  min_lr: 0.000000  loss: 4.8392 (4.8969)  loss_scale: 65536.0000 (53512.3749)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1578  max mem: 15572
Epoch: [2]  [1140/2683]  eta: 0:15:29  lr: 0.000023  min_lr: 0.000000  loss: 4.9223 (4.8978)  loss_scale: 65536.0000 (53617.7528)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1656  max mem: 15572
Epoch: [2]  [1150/2683]  eta: 0:15:24  lr: 0.000023  min_lr: 0.000000  loss: 4.9378 (4.8981)  loss_scale: 65536.0000 (53721.2997)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1809  max mem: 15572
Epoch: [2]  [1160/2683]  eta: 0:15:17  lr: 0.000023  min_lr: 0.000000  loss: 4.9378 (4.8984)  loss_scale: 65536.0000 (53823.0629)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1317  max mem: 15572
Epoch: [2]  [1170/2683]  eta: 0:15:11  lr: 0.000023  min_lr: 0.000000  loss: 4.9309 (4.8978)  loss_scale: 65536.0000 (53923.0880)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1062  max mem: 15572
Epoch: [2]  [1180/2683]  eta: 0:15:04  lr: 0.000023  min_lr: 0.000000  loss: 4.7691 (4.8968)  loss_scale: 65536.0000 (54021.4191)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1434  max mem: 15572
Epoch: [2]  [1190/2683]  eta: 0:15:00  lr: 0.000023  min_lr: 0.000000  loss: 4.7953 (4.8970)  loss_scale: 65536.0000 (54118.0991)  weight_decay: 0.0500 (0.0500)  time: 0.6394  data: 0.1862  max mem: 15572
Epoch: [2]  [1200/2683]  eta: 0:14:54  lr: 0.000023  min_lr: 0.000000  loss: 4.8708 (4.8966)  loss_scale: 65536.0000 (54213.1690)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.1940  max mem: 15572
Epoch: [2]  [1210/2683]  eta: 0:14:47  lr: 0.000023  min_lr: 0.000000  loss: 4.8531 (4.8955)  loss_scale: 65536.0000 (54306.6689)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1193  max mem: 15572
[2025-01-13 11:28:10,058] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:28:10,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1220/2683]  eta: 0:14:41  lr: 0.000023  min_lr: 0.000000  loss: 4.8070 (4.8952)  loss_scale: 65536.0000 (54559.6593)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1127  max mem: 15572
[2025-01-13 11:28:14,089] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6791
[2025-01-13 11:28:14,089] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:28:14,089] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1230/2683]  eta: 0:14:34  lr: 0.000023  min_lr: 0.000000  loss: 4.8530 (4.8954)  loss_scale: 65536.0000 (54861.7774)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1273  max mem: 15572
Epoch: [2]  [1240/2683]  eta: 0:14:29  lr: 0.000023  min_lr: 0.000000  loss: 4.8420 (4.8946)  loss_scale: 65536.0000 (54947.7905)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1364  max mem: 15572
Epoch: [2]  [1250/2683]  eta: 0:14:22  lr: 0.000023  min_lr: 0.000000  loss: 4.8036 (4.8941)  loss_scale: 65536.0000 (55032.4285)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1189  max mem: 15572
Epoch: [2]  [1260/2683]  eta: 0:14:16  lr: 0.000023  min_lr: 0.000000  loss: 4.8431 (4.8939)  loss_scale: 65536.0000 (55115.7240)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0956  max mem: 15572
Epoch: [2]  [1270/2683]  eta: 0:14:10  lr: 0.000023  min_lr: 0.000000  loss: 4.8620 (4.8939)  loss_scale: 65536.0000 (55197.7089)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1338  max mem: 15572
Epoch: [2]  [1280/2683]  eta: 0:14:04  lr: 0.000023  min_lr: 0.000000  loss: 4.8746 (4.8936)  loss_scale: 65536.0000 (55278.4137)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.1676  max mem: 15572
Epoch: [2]  [1290/2683]  eta: 0:13:57  lr: 0.000023  min_lr: 0.000000  loss: 4.7820 (4.8926)  loss_scale: 65536.0000 (55357.8683)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1106  max mem: 15572
Epoch: [2]  [1300/2683]  eta: 0:13:52  lr: 0.000023  min_lr: 0.000000  loss: 4.7917 (4.8927)  loss_scale: 65536.0000 (55436.1015)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1194  max mem: 15572
Epoch: [2]  [1310/2683]  eta: 0:13:46  lr: 0.000023  min_lr: 0.000000  loss: 4.8445 (4.8922)  loss_scale: 65536.0000 (55513.1411)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1361  max mem: 15572
Epoch: [2]  [1320/2683]  eta: 0:13:40  lr: 0.000023  min_lr: 0.000000  loss: 4.8604 (4.8926)  loss_scale: 65536.0000 (55589.0144)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1171  max mem: 15572
Epoch: [2]  [1330/2683]  eta: 0:13:33  lr: 0.000023  min_lr: 0.000000  loss: 4.8670 (4.8924)  loss_scale: 65536.0000 (55663.7476)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0783  max mem: 15572
Epoch: [2]  [1340/2683]  eta: 0:13:27  lr: 0.000023  min_lr: 0.000000  loss: 4.8839 (4.8922)  loss_scale: 65536.0000 (55737.3661)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0727  max mem: 15572
Epoch: [2]  [1350/2683]  eta: 0:13:21  lr: 0.000023  min_lr: 0.000000  loss: 4.8234 (4.8920)  loss_scale: 65536.0000 (55809.8949)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1368  max mem: 15572
[2025-01-13 11:29:29,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:29:29,431] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 11:29:32,217] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6923
[2025-01-13 11:29:32,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:29:32,217] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1360/2683]  eta: 0:13:15  lr: 0.000024  min_lr: 0.000000  loss: 4.8234 (4.8915)  loss_scale: 65536.0000 (56025.8163)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1462  max mem: 15572
Epoch: [2]  [1370/2683]  eta: 0:13:08  lr: 0.000024  min_lr: 0.000000  loss: 4.8015 (4.8905)  loss_scale: 65536.0000 (56095.1831)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1325  max mem: 15572
Epoch: [2]  [1380/2683]  eta: 0:13:03  lr: 0.000024  min_lr: 0.000000  loss: 4.7542 (4.8902)  loss_scale: 65536.0000 (56163.5453)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1622  max mem: 15572
Epoch: [2]  [1390/2683]  eta: 0:12:56  lr: 0.000024  min_lr: 0.000000  loss: 4.9024 (4.8900)  loss_scale: 65536.0000 (56230.9245)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1447  max mem: 15572
Epoch: [2]  [1400/2683]  eta: 0:12:50  lr: 0.000024  min_lr: 0.000000  loss: 4.8874 (4.8893)  loss_scale: 65536.0000 (56297.3419)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1233  max mem: 15572
Epoch: [2]  [1410/2683]  eta: 0:12:43  lr: 0.000024  min_lr: 0.000000  loss: 4.7808 (4.8892)  loss_scale: 65536.0000 (56362.8179)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1026  max mem: 15572
Epoch: [2]  [1420/2683]  eta: 0:12:37  lr: 0.000024  min_lr: 0.000000  loss: 4.8343 (4.8889)  loss_scale: 65536.0000 (56427.3723)  weight_decay: 0.0500 (0.0500)  time: 0.5208  data: 0.0663  max mem: 15572
Epoch: [2]  [1430/2683]  eta: 0:12:31  lr: 0.000024  min_lr: 0.000000  loss: 4.8343 (4.8886)  loss_scale: 65536.0000 (56491.0245)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1417  max mem: 15572
[2025-01-13 11:30:15,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=37, lr=[2.3019852687392572e-07, 2.3019852687392572e-07, 3.288550383913225e-07, 3.288550383913225e-07, 4.697929119876036e-07, 4.697929119876036e-07, 6.711327314108624e-07, 6.711327314108624e-07, 9.587610448726605e-07, 9.587610448726605e-07, 1.3696586355323722e-06, 1.3696586355323722e-06, 1.956655193617675e-06, 1.956655193617675e-06, 2.795221705168107e-06, 2.795221705168107e-06, 3.993173864525867e-06, 3.993173864525867e-06, 5.704534092179811e-06, 5.704534092179811e-06, 8.14933441739973e-06, 8.14933441739973e-06, 1.1641906310571043e-05, 1.1641906310571043e-05, 1.6631294729387208e-05, 1.6631294729387208e-05, 2.3758992470553154e-05, 2.3758992470553154e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 11:30:15,329] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=27.211344194542097, CurrSamplesPerSec=29.19260864799089, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [1440/2683]  eta: 0:12:26  lr: 0.000024  min_lr: 0.000000  loss: 4.8533 (4.8881)  loss_scale: 65536.0000 (56553.7932)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.1894  max mem: 15572
Epoch: [2]  [1450/2683]  eta: 0:12:20  lr: 0.000024  min_lr: 0.000000  loss: 4.9056 (4.8885)  loss_scale: 65536.0000 (56615.6968)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.1807  max mem: 15572
Epoch: [2]  [1460/2683]  eta: 0:12:14  lr: 0.000024  min_lr: 0.000000  loss: 4.9376 (4.8883)  loss_scale: 65536.0000 (56676.7529)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1493  max mem: 15572
Epoch: [2]  [1470/2683]  eta: 0:12:07  lr: 0.000024  min_lr: 0.000000  loss: 4.9274 (4.8885)  loss_scale: 65536.0000 (56736.9789)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1281  max mem: 15572
Epoch: [2]  [1480/2683]  eta: 0:12:01  lr: 0.000024  min_lr: 0.000000  loss: 4.9274 (4.8884)  loss_scale: 65536.0000 (56796.3916)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1383  max mem: 15572
[2025-01-13 11:30:48,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:30:48,142] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 11:30:48,962] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7054
[2025-01-13 11:30:48,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:30:48,962] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1490/2683]  eta: 0:11:55  lr: 0.000024  min_lr: 0.000000  loss: 4.8394 (4.8883)  loss_scale: 65536.0000 (56942.9162)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1425  max mem: 15572
Epoch: [2]  [1500/2683]  eta: 0:11:49  lr: 0.000024  min_lr: 0.000000  loss: 4.8590 (4.8879)  loss_scale: 65536.0000 (57000.1652)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1127  max mem: 15572
Epoch: [2]  [1510/2683]  eta: 0:11:42  lr: 0.000024  min_lr: 0.000000  loss: 4.8525 (4.8877)  loss_scale: 65536.0000 (57056.6565)  weight_decay: 0.0500 (0.0500)  time: 0.5143  data: 0.0652  max mem: 15572
Epoch: [2]  [1520/2683]  eta: 0:11:35  lr: 0.000024  min_lr: 0.000000  loss: 4.8200 (4.8871)  loss_scale: 65536.0000 (57112.4050)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0679  max mem: 15572
Epoch: [2]  [1530/2683]  eta: 0:11:29  lr: 0.000024  min_lr: 0.000000  loss: 4.7892 (4.8865)  loss_scale: 65536.0000 (57167.4252)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1212  max mem: 15572
Epoch: [2]  [1540/2683]  eta: 0:11:23  lr: 0.000024  min_lr: 0.000000  loss: 4.8026 (4.8862)  loss_scale: 65536.0000 (57221.7313)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1006  max mem: 15572
Epoch: [2]  [1550/2683]  eta: 0:11:17  lr: 0.000024  min_lr: 0.000000  loss: 4.8622 (4.8864)  loss_scale: 65536.0000 (57275.3372)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1117  max mem: 15572
Epoch: [2]  [1560/2683]  eta: 0:11:12  lr: 0.000024  min_lr: 0.000000  loss: 4.8675 (4.8860)  loss_scale: 65536.0000 (57328.2562)  weight_decay: 0.0500 (0.0500)  time: 0.6536  data: 0.1512  max mem: 15572
Epoch: [2]  [1570/2683]  eta: 0:11:06  lr: 0.000024  min_lr: 0.000000  loss: 4.8589 (4.8859)  loss_scale: 65536.0000 (57380.5016)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1312  max mem: 15572
Epoch: [2]  [1580/2683]  eta: 0:11:00  lr: 0.000024  min_lr: 0.000000  loss: 4.8589 (4.8859)  loss_scale: 65536.0000 (57432.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0903  max mem: 15572
Epoch: [2]  [1590/2683]  eta: 0:10:54  lr: 0.000024  min_lr: 0.000000  loss: 4.9128 (4.8860)  loss_scale: 65536.0000 (57483.0220)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0954  max mem: 15572
Epoch: [2]  [1600/2683]  eta: 0:10:47  lr: 0.000024  min_lr: 0.000000  loss: 4.8346 (4.8852)  loss_scale: 65536.0000 (57533.3217)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0740  max mem: 15572
Epoch: [2]  [1610/2683]  eta: 0:10:41  lr: 0.000024  min_lr: 0.000000  loss: 4.7768 (4.8846)  loss_scale: 65536.0000 (57582.9969)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0734  max mem: 15572
[2025-01-13 11:32:03,078] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:32:03,079] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 11:32:04,111] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7185
[2025-01-13 11:32:04,111] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:32:04,112] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1620/2683]  eta: 0:10:35  lr: 0.000024  min_lr: 0.000000  loss: 4.7640 (4.8835)  loss_scale: 65536.0000 (57712.9180)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0984  max mem: 15572
Epoch: [2]  [1630/2683]  eta: 0:10:30  lr: 0.000024  min_lr: 0.000000  loss: 4.7751 (4.8834)  loss_scale: 65536.0000 (57760.8829)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1312  max mem: 15572
Epoch: [2]  [1640/2683]  eta: 0:10:23  lr: 0.000024  min_lr: 0.000000  loss: 4.8420 (4.8828)  loss_scale: 65536.0000 (57808.2633)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1423  max mem: 15572
Epoch: [2]  [1650/2683]  eta: 0:10:17  lr: 0.000025  min_lr: 0.000000  loss: 4.7457 (4.8821)  loss_scale: 65536.0000 (57855.0697)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1189  max mem: 15572
Epoch: [2]  [1660/2683]  eta: 0:10:11  lr: 0.000025  min_lr: 0.000000  loss: 4.7612 (4.8821)  loss_scale: 65536.0000 (57901.3125)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1393  max mem: 15572
Epoch: [2]  [1670/2683]  eta: 0:10:05  lr: 0.000025  min_lr: 0.000000  loss: 4.8541 (4.8816)  loss_scale: 65536.0000 (57947.0018)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0815  max mem: 15572
Epoch: [2]  [1680/2683]  eta: 0:09:59  lr: 0.000025  min_lr: 0.000000  loss: 4.8731 (4.8818)  loss_scale: 65536.0000 (57992.1475)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.0950  max mem: 15572
Epoch: [2]  [1690/2683]  eta: 0:09:52  lr: 0.000025  min_lr: 0.000000  loss: 4.8634 (4.8817)  loss_scale: 65536.0000 (58036.7593)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1108  max mem: 15572
[2025-01-13 11:32:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7263
[2025-01-13 11:32:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:32:50,344] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [1700/2683]  eta: 0:09:47  lr: 0.000025  min_lr: 0.000000  loss: 4.8648 (4.8818)  loss_scale: 65536.0000 (58003.7907)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1136  max mem: 15572
Epoch: [2]  [1710/2683]  eta: 0:09:41  lr: 0.000025  min_lr: 0.000000  loss: 4.8708 (4.8815)  loss_scale: 32768.0000 (57856.2992)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.1835  max mem: 15572
Epoch: [2]  [1720/2683]  eta: 0:09:35  lr: 0.000025  min_lr: 0.000000  loss: 4.8060 (4.8808)  loss_scale: 32768.0000 (57710.5218)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.1849  max mem: 15572
Epoch: [2]  [1730/2683]  eta: 0:09:29  lr: 0.000025  min_lr: 0.000000  loss: 4.8383 (4.8808)  loss_scale: 32768.0000 (57566.4287)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1393  max mem: 15572
Epoch: [2]  [1740/2683]  eta: 0:09:22  lr: 0.000025  min_lr: 0.000000  loss: 4.9079 (4.8807)  loss_scale: 32768.0000 (57423.9908)  weight_decay: 0.0500 (0.0500)  time: 0.5280  data: 0.0597  max mem: 15572
Epoch: [2]  [1750/2683]  eta: 0:09:16  lr: 0.000025  min_lr: 0.000000  loss: 4.8452 (4.8802)  loss_scale: 32768.0000 (57283.1799)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0796  max mem: 15572
Epoch: [2]  [1760/2683]  eta: 0:09:11  lr: 0.000025  min_lr: 0.000000  loss: 4.7980 (4.8796)  loss_scale: 32768.0000 (57143.9682)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1690  max mem: 15572
Epoch: [2]  [1770/2683]  eta: 0:09:04  lr: 0.000025  min_lr: 0.000000  loss: 4.7980 (4.8793)  loss_scale: 32768.0000 (57006.3286)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1525  max mem: 15572
Epoch: [2]  [1780/2683]  eta: 0:08:58  lr: 0.000025  min_lr: 0.000000  loss: 4.8564 (4.8794)  loss_scale: 32768.0000 (56870.2347)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1279  max mem: 15572
Epoch: [2]  [1790/2683]  eta: 0:08:53  lr: 0.000025  min_lr: 0.000000  loss: 4.8177 (4.8789)  loss_scale: 32768.0000 (56735.6605)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1524  max mem: 15572
Epoch: [2]  [1800/2683]  eta: 0:08:46  lr: 0.000025  min_lr: 0.000000  loss: 4.7595 (4.8781)  loss_scale: 32768.0000 (56602.5808)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1009  max mem: 15572
Epoch: [2]  [1810/2683]  eta: 0:08:40  lr: 0.000025  min_lr: 0.000000  loss: 4.7791 (4.8780)  loss_scale: 32768.0000 (56470.9707)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0701  max mem: 15572
Epoch: [2]  [1820/2683]  eta: 0:08:34  lr: 0.000025  min_lr: 0.000000  loss: 4.8193 (4.8774)  loss_scale: 32768.0000 (56340.8062)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1189  max mem: 15572
[2025-01-13 11:34:06,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:34:06,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1830/2683]  eta: 0:08:28  lr: 0.000025  min_lr: 0.000000  loss: 4.7953 (4.8768)  loss_scale: 32768.0000 (56301.5445)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1436  max mem: 15572
Epoch: [2]  [1840/2683]  eta: 0:08:22  lr: 0.000025  min_lr: 0.000000  loss: 4.7953 (4.8768)  loss_scale: 65536.0000 (56351.7045)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1284  max mem: 15572
Epoch: [2]  [1850/2683]  eta: 0:08:17  lr: 0.000025  min_lr: 0.000000  loss: 4.8008 (4.8765)  loss_scale: 65536.0000 (56401.3225)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1425  max mem: 15572
Epoch: [2]  [1860/2683]  eta: 0:08:10  lr: 0.000025  min_lr: 0.000000  loss: 4.8171 (4.8763)  loss_scale: 65536.0000 (56450.4073)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1353  max mem: 15572
Epoch: [2]  [1870/2683]  eta: 0:08:04  lr: 0.000025  min_lr: 0.000000  loss: 4.8777 (4.8762)  loss_scale: 65536.0000 (56498.9674)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1055  max mem: 15572
[2025-01-13 11:34:37,283] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7446
[2025-01-13 11:34:37,284] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:34:37,284] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [1880/2683]  eta: 0:07:58  lr: 0.000025  min_lr: 0.000000  loss: 4.8424 (4.8754)  loss_scale: 65536.0000 (56529.5906)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1349  max mem: 15572
Epoch: [2]  [1890/2683]  eta: 0:07:52  lr: 0.000025  min_lr: 0.000000  loss: 4.7980 (4.8752)  loss_scale: 32768.0000 (56403.9344)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1550  max mem: 15572
Epoch: [2]  [1900/2683]  eta: 0:07:46  lr: 0.000025  min_lr: 0.000000  loss: 4.8447 (4.8752)  loss_scale: 32768.0000 (56279.6002)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1646  max mem: 15572
Epoch: [2]  [1910/2683]  eta: 0:07:41  lr: 0.000025  min_lr: 0.000000  loss: 4.7995 (4.8745)  loss_scale: 32768.0000 (56156.5672)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1801  max mem: 15572
Epoch: [2]  [1920/2683]  eta: 0:07:35  lr: 0.000025  min_lr: 0.000000  loss: 4.7638 (4.8740)  loss_scale: 32768.0000 (56034.8152)  weight_decay: 0.0500 (0.0500)  time: 0.6559  data: 0.1849  max mem: 15572
Epoch: [2]  [1930/2683]  eta: 0:07:29  lr: 0.000025  min_lr: 0.000000  loss: 4.7254 (4.8733)  loss_scale: 32768.0000 (55914.3242)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1048  max mem: 15572
Epoch: [2]  [1940/2683]  eta: 0:07:23  lr: 0.000026  min_lr: 0.000000  loss: 4.7353 (4.8731)  loss_scale: 32768.0000 (55795.0747)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0885  max mem: 15572
Epoch: [2]  [1950/2683]  eta: 0:07:17  lr: 0.000026  min_lr: 0.000000  loss: 4.7353 (4.8724)  loss_scale: 32768.0000 (55677.0477)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0682  max mem: 15572
Epoch: [2]  [1960/2683]  eta: 0:07:10  lr: 0.000026  min_lr: 0.000000  loss: 4.8184 (4.8725)  loss_scale: 32768.0000 (55560.2244)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0170  max mem: 15572
Epoch: [2]  [1970/2683]  eta: 0:07:04  lr: 0.000026  min_lr: 0.000000  loss: 4.8204 (4.8726)  loss_scale: 32768.0000 (55444.5865)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0636  max mem: 15572
Epoch: [2]  [1980/2683]  eta: 0:06:58  lr: 0.000026  min_lr: 0.000000  loss: 4.8021 (4.8722)  loss_scale: 32768.0000 (55330.1161)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0843  max mem: 15572
Epoch: [2]  [1990/2683]  eta: 0:06:52  lr: 0.000026  min_lr: 0.000000  loss: 4.7224 (4.8715)  loss_scale: 32768.0000 (55216.7956)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1118  max mem: 15572
Epoch: [2]  [2000/2683]  eta: 0:06:46  lr: 0.000026  min_lr: 0.000000  loss: 4.7838 (4.8712)  loss_scale: 32768.0000 (55104.6077)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1014  max mem: 15572
[2025-01-13 11:35:50,996] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:35:50,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [2010/2683]  eta: 0:06:40  lr: 0.000026  min_lr: 0.000000  loss: 4.7838 (4.8707)  loss_scale: 32768.0000 (55026.1243)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0772  max mem: 15572
Epoch: [2]  [2020/2683]  eta: 0:06:34  lr: 0.000026  min_lr: 0.000000  loss: 4.8536 (4.8709)  loss_scale: 65536.0000 (55078.1277)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0740  max mem: 15572
Epoch: [2]  [2030/2683]  eta: 0:06:28  lr: 0.000026  min_lr: 0.000000  loss: 4.8422 (4.8705)  loss_scale: 65536.0000 (55129.6189)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1483  max mem: 15572
Epoch: [2]  [2040/2683]  eta: 0:06:22  lr: 0.000026  min_lr: 0.000000  loss: 4.7620 (4.8702)  loss_scale: 65536.0000 (55180.6056)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1254  max mem: 15572
Epoch: [2]  [2050/2683]  eta: 0:06:16  lr: 0.000026  min_lr: 0.000000  loss: 4.8287 (4.8697)  loss_scale: 65536.0000 (55231.0951)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0422  max mem: 15572
Epoch: [2]  [2060/2683]  eta: 0:06:10  lr: 0.000026  min_lr: 0.000000  loss: 4.7858 (4.8692)  loss_scale: 65536.0000 (55281.0946)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1572  max mem: 15572
Epoch: [2]  [2070/2683]  eta: 0:06:04  lr: 0.000026  min_lr: 0.000000  loss: 4.7867 (4.8689)  loss_scale: 65536.0000 (55330.6113)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1760  max mem: 15572
Epoch: [2]  [2080/2683]  eta: 0:05:58  lr: 0.000026  min_lr: 0.000000  loss: 4.8434 (4.8687)  loss_scale: 65536.0000 (55379.6521)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1149  max mem: 15572
Epoch: [2]  [2090/2683]  eta: 0:05:52  lr: 0.000026  min_lr: 0.000000  loss: 4.8280 (4.8686)  loss_scale: 65536.0000 (55428.2238)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1285  max mem: 15572
Epoch: [2]  [2100/2683]  eta: 0:05:46  lr: 0.000026  min_lr: 0.000000  loss: 4.8431 (4.8688)  loss_scale: 65536.0000 (55476.3332)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.1094  max mem: 15572
Epoch: [2]  [2110/2683]  eta: 0:05:40  lr: 0.000026  min_lr: 0.000000  loss: 4.8682 (4.8685)  loss_scale: 65536.0000 (55523.9867)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1297  max mem: 15572
Epoch: [2]  [2120/2683]  eta: 0:05:34  lr: 0.000026  min_lr: 0.000000  loss: 4.7880 (4.8678)  loss_scale: 65536.0000 (55571.1909)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1638  max mem: 15572
Epoch: [2]  [2130/2683]  eta: 0:05:28  lr: 0.000026  min_lr: 0.000000  loss: 4.8219 (4.8677)  loss_scale: 65536.0000 (55617.9521)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1316  max mem: 15572
[2025-01-13 11:37:06,910] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:37:06,911] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [2140/2683]  eta: 0:05:22  lr: 0.000026  min_lr: 0.000000  loss: 4.8058 (4.8673)  loss_scale: 65536.0000 (55786.7165)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1088  max mem: 15572
[2025-01-13 11:37:10,317] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7709
[2025-01-13 11:37:10,317] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:37:10,317] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2150/2683]  eta: 0:05:16  lr: 0.000026  min_lr: 0.000000  loss: 4.7514 (4.8670)  loss_scale: 65536.0000 (55892.9763)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0531  max mem: 15572
Epoch: [2]  [2160/2683]  eta: 0:05:10  lr: 0.000026  min_lr: 0.000000  loss: 4.7279 (4.8662)  loss_scale: 65536.0000 (55937.5993)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0678  max mem: 15572
Epoch: [2]  [2170/2683]  eta: 0:05:04  lr: 0.000026  min_lr: 0.000000  loss: 4.8055 (4.8663)  loss_scale: 65536.0000 (55981.8111)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1411  max mem: 15572
Epoch: [2]  [2180/2683]  eta: 0:04:59  lr: 0.000026  min_lr: 0.000000  loss: 4.8228 (4.8660)  loss_scale: 65536.0000 (56025.6176)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1408  max mem: 15572
Epoch: [2]  [2190/2683]  eta: 0:04:52  lr: 0.000026  min_lr: 0.000000  loss: 4.8210 (4.8660)  loss_scale: 65536.0000 (56069.0242)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1150  max mem: 15572
Epoch: [2]  [2200/2683]  eta: 0:04:47  lr: 0.000026  min_lr: 0.000000  loss: 4.7714 (4.8651)  loss_scale: 65536.0000 (56112.0363)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1684  max mem: 15572
Epoch: [2]  [2210/2683]  eta: 0:04:41  lr: 0.000026  min_lr: 0.000000  loss: 4.6733 (4.8645)  loss_scale: 65536.0000 (56154.6594)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.1820  max mem: 15572
Epoch: [2]  [2220/2683]  eta: 0:04:35  lr: 0.000027  min_lr: 0.000000  loss: 4.6884 (4.8640)  loss_scale: 65536.0000 (56196.8987)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1128  max mem: 15572
Epoch: [2]  [2230/2683]  eta: 0:04:29  lr: 0.000027  min_lr: 0.000000  loss: 4.7940 (4.8640)  loss_scale: 65536.0000 (56238.7593)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1215  max mem: 15572
Epoch: [2]  [2240/2683]  eta: 0:04:23  lr: 0.000027  min_lr: 0.000000  loss: 4.7940 (4.8634)  loss_scale: 65536.0000 (56280.2463)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1030  max mem: 15572
Epoch: [2]  [2250/2683]  eta: 0:04:17  lr: 0.000027  min_lr: 0.000000  loss: 4.7512 (4.8633)  loss_scale: 65536.0000 (56321.3647)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0998  max mem: 15572
Epoch: [2]  [2260/2683]  eta: 0:04:11  lr: 0.000027  min_lr: 0.000000  loss: 4.8126 (4.8630)  loss_scale: 65536.0000 (56362.1194)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1474  max mem: 15572
Epoch: [2]  [2270/2683]  eta: 0:04:05  lr: 0.000027  min_lr: 0.000000  loss: 4.7939 (4.8624)  loss_scale: 65536.0000 (56402.5152)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1577  max mem: 15572
[2025-01-13 11:38:26,507] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:38:26,507] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [2280/2683]  eta: 0:03:59  lr: 0.000027  min_lr: 0.000000  loss: 4.7790 (4.8622)  loss_scale: 65536.0000 (56701.1381)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1668  max mem: 15572
[2025-01-13 11:38:34,799] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7850
[2025-01-13 11:38:34,799] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:38:34,799] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2290/2683]  eta: 0:03:53  lr: 0.000027  min_lr: 0.000000  loss: 4.8446 (4.8622)  loss_scale: 131072.0000 (56825.5190)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1206  max mem: 15572
Epoch: [2]  [2300/2683]  eta: 0:03:47  lr: 0.000027  min_lr: 0.000000  loss: 4.8705 (4.8625)  loss_scale: 65536.0000 (56863.3742)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0755  max mem: 15572
Epoch: [2]  [2310/2683]  eta: 0:03:41  lr: 0.000027  min_lr: 0.000000  loss: 4.8991 (4.8626)  loss_scale: 65536.0000 (56900.9018)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0957  max mem: 15572
Epoch: [2]  [2320/2683]  eta: 0:03:35  lr: 0.000027  min_lr: 0.000000  loss: 4.7573 (4.8619)  loss_scale: 65536.0000 (56938.1060)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1170  max mem: 15572
Epoch: [2]  [2330/2683]  eta: 0:03:29  lr: 0.000027  min_lr: 0.000000  loss: 4.7091 (4.8613)  loss_scale: 65536.0000 (56974.9910)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0900  max mem: 15572
Epoch: [2]  [2340/2683]  eta: 0:03:23  lr: 0.000027  min_lr: 0.000000  loss: 4.7608 (4.8609)  loss_scale: 65536.0000 (57011.5609)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1151  max mem: 15572
Epoch: [2]  [2350/2683]  eta: 0:03:17  lr: 0.000027  min_lr: 0.000000  loss: 4.7645 (4.8603)  loss_scale: 65536.0000 (57047.8197)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1347  max mem: 15572
Epoch: [2]  [2360/2683]  eta: 0:03:11  lr: 0.000027  min_lr: 0.000000  loss: 4.7845 (4.8601)  loss_scale: 65536.0000 (57083.7713)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1050  max mem: 15572
Epoch: [2]  [2370/2683]  eta: 0:03:05  lr: 0.000027  min_lr: 0.000000  loss: 4.7824 (4.8598)  loss_scale: 65536.0000 (57119.4197)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0891  max mem: 15572
Epoch: [2]  [2380/2683]  eta: 0:02:59  lr: 0.000027  min_lr: 0.000000  loss: 4.7182 (4.8597)  loss_scale: 65536.0000 (57154.7686)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1314  max mem: 15572
Epoch: [2]  [2390/2683]  eta: 0:02:53  lr: 0.000027  min_lr: 0.000000  loss: 4.7311 (4.8593)  loss_scale: 65536.0000 (57189.8218)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1549  max mem: 15572
Epoch: [2]  [2400/2683]  eta: 0:02:48  lr: 0.000027  min_lr: 0.000000  loss: 4.7819 (4.8592)  loss_scale: 65536.0000 (57224.5831)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1540  max mem: 15572
Epoch: [2]  [2410/2683]  eta: 0:02:42  lr: 0.000027  min_lr: 0.000000  loss: 4.7998 (4.8589)  loss_scale: 65536.0000 (57259.0560)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1369  max mem: 15572
[2025-01-13 11:39:48,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:39:48,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 11:39:49,792] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7981
[2025-01-13 11:39:49,792] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:39:49,793] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2420/2683]  eta: 0:02:36  lr: 0.000027  min_lr: 0.000000  loss: 4.8196 (4.8588)  loss_scale: 65536.0000 (57347.3837)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1038  max mem: 15572
Epoch: [2]  [2430/2683]  eta: 0:02:30  lr: 0.000027  min_lr: 0.000000  loss: 4.8196 (4.8582)  loss_scale: 65536.0000 (57381.0679)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1039  max mem: 15572
[2025-01-13 11:40:00,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=44, lr=[2.6405623048826985e-07, 2.6405623048826985e-07, 3.7722318641181413e-07, 3.7722318641181413e-07, 5.388902663025916e-07, 5.388902663025916e-07, 7.69843237575131e-07, 7.69843237575131e-07, 1.0997760536787585e-06, 1.0997760536787585e-06, 1.5711086481125123e-06, 1.5711086481125123e-06, 2.244440925875018e-06, 2.244440925875018e-06, 3.2063441798214542e-06, 3.2063441798214542e-06, 4.5804916854592204e-06, 4.5804916854592204e-06, 6.54355955065603e-06, 6.54355955065603e-06, 9.347942215222899e-06, 9.347942215222899e-06, 1.3354203164604143e-05, 1.3354203164604143e-05, 1.9077433092291634e-05, 1.9077433092291634e-05, 2.725347584613091e-05, 2.725347584613091e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 11:40:00,512] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=27.21314606724119, CurrSamplesPerSec=25.33326420979464, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [2440/2683]  eta: 0:02:24  lr: 0.000027  min_lr: 0.000000  loss: 4.7248 (4.8576)  loss_scale: 65536.0000 (57414.4760)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1127  max mem: 15572
Epoch: [2]  [2450/2683]  eta: 0:02:18  lr: 0.000027  min_lr: 0.000000  loss: 4.7248 (4.8573)  loss_scale: 65536.0000 (57447.6116)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1662  max mem: 15572
Epoch: [2]  [2460/2683]  eta: 0:02:12  lr: 0.000027  min_lr: 0.000000  loss: 4.7801 (4.8573)  loss_scale: 65536.0000 (57480.4779)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1456  max mem: 15572
Epoch: [2]  [2470/2683]  eta: 0:02:06  lr: 0.000027  min_lr: 0.000000  loss: 4.7802 (4.8572)  loss_scale: 65536.0000 (57513.0781)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0659  max mem: 15572
Epoch: [2]  [2480/2683]  eta: 0:02:00  lr: 0.000027  min_lr: 0.000000  loss: 4.7575 (4.8565)  loss_scale: 65536.0000 (57545.4156)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0653  max mem: 15572
Epoch: [2]  [2490/2683]  eta: 0:01:54  lr: 0.000027  min_lr: 0.000000  loss: 4.6786 (4.8563)  loss_scale: 65536.0000 (57577.4934)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0748  max mem: 15572
Epoch: [2]  [2500/2683]  eta: 0:01:48  lr: 0.000027  min_lr: 0.000000  loss: 4.8038 (4.8562)  loss_scale: 65536.0000 (57609.3147)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1028  max mem: 15572
Epoch: [2]  [2510/2683]  eta: 0:01:42  lr: 0.000028  min_lr: 0.000000  loss: 4.7691 (4.8557)  loss_scale: 65536.0000 (57640.8825)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1624  max mem: 15572
Epoch: [2]  [2520/2683]  eta: 0:01:36  lr: 0.000028  min_lr: 0.000000  loss: 4.7950 (4.8556)  loss_scale: 65536.0000 (57672.1999)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1740  max mem: 15572
Epoch: [2]  [2530/2683]  eta: 0:01:30  lr: 0.000028  min_lr: 0.000000  loss: 4.8156 (4.8556)  loss_scale: 65536.0000 (57703.2699)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.1749  max mem: 15572
Epoch: [2]  [2540/2683]  eta: 0:01:24  lr: 0.000028  min_lr: 0.000000  loss: 4.7636 (4.8549)  loss_scale: 65536.0000 (57734.0952)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1643  max mem: 15572
[2025-01-13 11:41:06,034] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:41:06,034] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 11:41:09,548] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8114
[2025-01-13 11:41:09,548] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:41:09,548] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2550/2683]  eta: 0:01:18  lr: 0.000028  min_lr: 0.000000  loss: 4.7565 (4.8548)  loss_scale: 65536.0000 (57867.4402)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1621  max mem: 15572
Epoch: [2]  [2560/2683]  eta: 0:01:13  lr: 0.000028  min_lr: 0.000000  loss: 4.8479 (4.8546)  loss_scale: 65536.0000 (57897.3838)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1734  max mem: 15572
Epoch: [2]  [2570/2683]  eta: 0:01:07  lr: 0.000028  min_lr: 0.000000  loss: 4.8113 (4.8543)  loss_scale: 65536.0000 (57927.0945)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0918  max mem: 15572
Epoch: [2]  [2580/2683]  eta: 0:01:01  lr: 0.000028  min_lr: 0.000000  loss: 4.8327 (4.8543)  loss_scale: 65536.0000 (57956.5750)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0493  max mem: 15572
Epoch: [2]  [2590/2683]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000000  loss: 4.8257 (4.8541)  loss_scale: 65536.0000 (57985.8279)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0752  max mem: 15572
Epoch: [2]  [2600/2683]  eta: 0:00:49  lr: 0.000028  min_lr: 0.000000  loss: 4.7649 (4.8540)  loss_scale: 65536.0000 (58014.8558)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0817  max mem: 15572
Epoch: [2]  [2610/2683]  eta: 0:00:43  lr: 0.000028  min_lr: 0.000000  loss: 4.7458 (4.8537)  loss_scale: 65536.0000 (58043.6614)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1001  max mem: 15572
Epoch: [2]  [2620/2683]  eta: 0:00:37  lr: 0.000028  min_lr: 0.000000  loss: 4.8039 (4.8534)  loss_scale: 65536.0000 (58072.2472)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0959  max mem: 15572
Epoch: [2]  [2630/2683]  eta: 0:00:31  lr: 0.000028  min_lr: 0.000000  loss: 4.8272 (4.8532)  loss_scale: 65536.0000 (58100.6157)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1529  max mem: 15572
Epoch: [2]  [2640/2683]  eta: 0:00:25  lr: 0.000028  min_lr: 0.000000  loss: 4.8272 (4.8528)  loss_scale: 65536.0000 (58128.7694)  weight_decay: 0.0500 (0.0500)  time: 0.6557  data: 0.1732  max mem: 15572
Epoch: [2]  [2650/2683]  eta: 0:00:19  lr: 0.000028  min_lr: 0.000000  loss: 4.7579 (4.8527)  loss_scale: 65536.0000 (58156.7107)  weight_decay: 0.0500 (0.0500)  time: 0.6742  data: 0.1979  max mem: 15572
Epoch: [2]  [2660/2683]  eta: 0:00:13  lr: 0.000028  min_lr: 0.000000  loss: 4.7577 (4.8522)  loss_scale: 65536.0000 (58184.4419)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1770  max mem: 15572
Epoch: [2]  [2670/2683]  eta: 0:00:07  lr: 0.000028  min_lr: 0.000000  loss: 4.7122 (4.8517)  loss_scale: 65536.0000 (58211.9656)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0727  max mem: 15572
[2025-01-13 11:42:23,711] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:42:23,711] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [2680/2683]  eta: 0:00:01  lr: 0.000028  min_lr: 0.000000  loss: 4.8328 (4.8519)  loss_scale: 65536.0000 (58337.0623)  weight_decay: 0.0500 (0.0500)  time: 0.4340  data: 0.0222  max mem: 15572
[2025-01-13 11:42:25,251] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8247
[2025-01-13 11:42:25,251] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:42:25,251] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2682/2683]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000000  loss: 4.8328 (4.8517)  loss_scale: 65536.0000 (58342.4286)  weight_decay: 0.0500 (0.0500)  time: 0.4280  data: 0.0221  max mem: 15572
Epoch: [2] Total time: 0:26:30 (0.5929 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000000  loss: 4.8328 (4.8517)  loss_scale: 65536.0000 (58342.4286)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 1937
Indices to remove: tensor([   41,    55,   161,  ..., 33639, 33659, 33684], device='cuda:0')
length of data loader train is: 2522
num_training_steps_per_epoch is: 2522
Change step level LR scheduler!
Set warmup steps = 12610
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:23:56  loss: 3.8937 (3.8937)  acc1: 0.0000 (0.0000)  acc5: 50.0000 (50.0000)  time: 5.2802  data: 5.0778  max mem: 15572
Val:  [ 10/272]  eta: 0:03:19  loss: 5.1434 (4.7339)  acc1: 0.0000 (11.1111)  acc5: 0.0000 (21.7172)  time: 0.7609  data: 0.5660  max mem: 15572
Val:  [ 20/272]  eta: 0:02:17  loss: 4.8407 (4.6783)  acc1: 0.0000 (11.1111)  acc5: 0.0000 (19.3122)  time: 0.3069  data: 0.1060  max mem: 15572
Val:  [ 30/272]  eta: 0:01:51  loss: 4.7424 (4.6368)  acc1: 0.0000 (8.6022)  acc5: 0.0000 (20.7885)  time: 0.2966  data: 0.0987  max mem: 15572
Val:  [ 40/272]  eta: 0:01:40  loss: 4.1163 (4.4846)  acc1: 0.0000 (9.8916)  acc5: 22.2222 (26.9648)  time: 0.3153  data: 0.1212  max mem: 15572
Val:  [ 50/272]  eta: 0:01:28  loss: 4.0677 (4.4852)  acc1: 0.0000 (7.9521)  acc5: 27.7778 (28.1046)  time: 0.2997  data: 0.1105  max mem: 15572
Val:  [ 60/272]  eta: 0:01:27  loss: 4.0677 (4.4428)  acc1: 0.0000 (6.7395)  acc5: 11.1111 (27.2313)  time: 0.3655  data: 0.1804  max mem: 15572
Val:  [ 70/272]  eta: 0:01:21  loss: 4.2446 (4.3863)  acc1: 0.0000 (6.0250)  acc5: 11.1111 (27.8560)  time: 0.4234  data: 0.2271  max mem: 15572
Val:  [ 80/272]  eta: 0:01:15  loss: 4.2446 (4.3789)  acc1: 0.0000 (8.9163)  acc5: 0.0000 (28.3265)  time: 0.3369  data: 0.1387  max mem: 15572
Val:  [ 90/272]  eta: 0:01:10  loss: 4.8516 (4.4407)  acc1: 0.0000 (7.9365)  acc5: 0.0000 (25.2137)  time: 0.3278  data: 0.1356  max mem: 15572
Val:  [100/272]  eta: 0:01:05  loss: 4.8405 (4.4792)  acc1: 0.0000 (7.9758)  acc5: 0.0000 (24.3124)  time: 0.3290  data: 0.1414  max mem: 15572
Val:  [110/272]  eta: 0:01:00  loss: 4.8212 (4.5280)  acc1: 0.0000 (7.3073)  acc5: 0.0000 (22.8228)  time: 0.2928  data: 0.1053  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 4.9889 (4.5692)  acc1: 0.0000 (6.7034)  acc5: 0.0000 (20.9826)  time: 0.2634  data: 0.0681  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 4.8559 (4.5232)  acc1: 0.0000 (8.1001)  acc5: 0.0000 (23.8338)  time: 0.3179  data: 0.1214  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 4.4979 (4.5093)  acc1: 0.0000 (9.2593)  acc5: 5.5556 (24.0741)  time: 0.3421  data: 0.1514  max mem: 15572
Val:  [150/272]  eta: 0:00:44  loss: 4.5951 (4.5188)  acc1: 0.0000 (8.6461)  acc5: 0.0000 (22.7373)  time: 0.3572  data: 0.1670  max mem: 15572
Val:  [160/272]  eta: 0:00:41  loss: 4.4130 (4.5094)  acc1: 0.0000 (8.4886)  acc5: 11.1111 (23.7060)  time: 0.4286  data: 0.2341  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 4.4130 (4.5361)  acc1: 0.0000 (8.0572)  acc5: 16.6667 (22.7096)  time: 0.3585  data: 0.1687  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 4.8539 (4.5426)  acc1: 0.0000 (7.6120)  acc5: 0.0000 (21.7311)  time: 0.2388  data: 0.0508  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.8103 (4.5593)  acc1: 0.0000 (7.2135)  acc5: 0.0000 (20.6225)  time: 0.2547  data: 0.0623  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 4.8257 (4.5830)  acc1: 0.0000 (6.8546)  acc5: 0.0000 (19.5965)  time: 0.3409  data: 0.1554  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 4.8257 (4.5975)  acc1: 0.0000 (6.5298)  acc5: 0.0000 (18.9837)  time: 0.2854  data: 0.0938  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.5627 (4.5887)  acc1: 0.0000 (6.4103)  acc5: 11.1111 (19.3313)  time: 0.2739  data: 0.0816  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.4093 (4.5832)  acc1: 5.5556 (6.9505)  acc5: 33.3333 (20.0096)  time: 0.3432  data: 0.1406  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 4.4692 (4.5863)  acc1: 0.0000 (6.6851)  acc5: 11.1111 (19.5482)  time: 0.3828  data: 0.1776  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 4.8574 (4.6091)  acc1: 0.0000 (6.4188)  acc5: 0.0000 (18.9686)  time: 0.3978  data: 0.1861  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.4041 (4.5764)  acc1: 0.0000 (6.4496)  acc5: 27.7778 (20.8812)  time: 0.2967  data: 0.0853  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 4.1211 (4.5749)  acc1: 0.0000 (6.2526)  acc5: 44.4444 (21.2792)  time: 0.1940  data: 0.0176  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 4.1211 (4.5764)  acc1: 0.0000 (6.2462)  acc5: 44.4444 (21.2574)  time: 0.1870  data: 0.0175  max mem: 15572
Val: Total time: 0:01:31 (0.3375 s / it)
* Acc@1 6.246 Acc@5 21.257 loss 4.576
Accuracy of the network on the 4883 val videos: 6.2%
[2025-01-13 11:43:58,227] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 11:43:58,231] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 11:43:58,231] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 11:44:01,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 11:44:01,276] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 6.25%
Epoch: [3]  [   0/2522]  eta: 6:35:06  lr: 0.000028  min_lr: 0.000000  loss: 4.6115 (4.6115)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.3998  data: 8.8194  max mem: 15572
Epoch: [3]  [  10/2522]  eta: 0:55:36  lr: 0.000028  min_lr: 0.000000  loss: 4.7812 (4.7595)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3281  data: 0.8549  max mem: 15572
Epoch: [3]  [  20/2522]  eta: 0:40:11  lr: 0.000028  min_lr: 0.000000  loss: 4.8596 (4.7937)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0723  max mem: 15572
Epoch: [3]  [  30/2522]  eta: 0:34:37  lr: 0.000028  min_lr: 0.000000  loss: 4.8207 (4.8119)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0918  max mem: 15572
Epoch: [3]  [  40/2522]  eta: 0:32:07  lr: 0.000028  min_lr: 0.000000  loss: 4.7429 (4.7853)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1315  max mem: 15572
Epoch: [3]  [  50/2522]  eta: 0:30:26  lr: 0.000028  min_lr: 0.000000  loss: 4.7057 (4.7930)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1199  max mem: 15572
Epoch: [3]  [  60/2522]  eta: 0:29:10  lr: 0.000028  min_lr: 0.000000  loss: 4.8380 (4.7831)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0770  max mem: 15572
Epoch: [3]  [  70/2522]  eta: 0:28:14  lr: 0.000028  min_lr: 0.000000  loss: 4.8231 (4.7775)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0942  max mem: 15572
Epoch: [3]  [  80/2522]  eta: 0:28:13  lr: 0.000028  min_lr: 0.000000  loss: 4.6782 (4.7697)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.1707  max mem: 15572
Epoch: [3]  [  90/2522]  eta: 0:27:28  lr: 0.000028  min_lr: 0.000000  loss: 4.7348 (4.7712)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1687  max mem: 15572
Epoch: [3]  [ 100/2522]  eta: 0:26:46  lr: 0.000028  min_lr: 0.000000  loss: 4.7433 (4.7673)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0859  max mem: 15572
Epoch: [3]  [ 110/2522]  eta: 0:26:40  lr: 0.000029  min_lr: 0.000000  loss: 4.8054 (4.7776)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1387  max mem: 15572
Epoch: [3]  [ 120/2522]  eta: 0:26:29  lr: 0.000029  min_lr: 0.000000  loss: 4.7971 (4.7774)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.1821  max mem: 15572
[2025-01-13 11:45:24,772] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:45:24,773] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 11:45:25,184] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8377
[2025-01-13 11:45:25,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:45:25,185] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 130/2522]  eta: 0:25:46  lr: 0.000029  min_lr: 0.000000  loss: 4.7318 (4.7771)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0772  max mem: 15572
Epoch: [3]  [ 140/2522]  eta: 0:25:28  lr: 0.000029  min_lr: 0.000000  loss: 4.7331 (4.7737)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0558  max mem: 15572
Epoch: [3]  [ 150/2522]  eta: 0:25:16  lr: 0.000029  min_lr: 0.000000  loss: 4.7002 (4.7708)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1304  max mem: 15572
Epoch: [3]  [ 160/2522]  eta: 0:24:58  lr: 0.000029  min_lr: 0.000000  loss: 4.7419 (4.7716)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1058  max mem: 15572
Epoch: [3]  [ 170/2522]  eta: 0:24:51  lr: 0.000029  min_lr: 0.000000  loss: 4.6590 (4.7646)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1187  max mem: 15572
Epoch: [3]  [ 180/2522]  eta: 0:24:21  lr: 0.000029  min_lr: 0.000000  loss: 4.6359 (4.7599)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0881  max mem: 15572
Epoch: [3]  [ 190/2522]  eta: 0:24:16  lr: 0.000029  min_lr: 0.000000  loss: 4.6496 (4.7561)  loss_scale: 65536.0000 (65879.1204)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0722  max mem: 15572
Epoch: [3]  [ 200/2522]  eta: 0:23:55  lr: 0.000029  min_lr: 0.000000  loss: 4.7230 (4.7579)  loss_scale: 65536.0000 (65862.0498)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0945  max mem: 15572
Epoch: [3]  [ 210/2522]  eta: 0:23:50  lr: 0.000029  min_lr: 0.000000  loss: 4.8134 (4.7613)  loss_scale: 65536.0000 (65846.5972)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1036  max mem: 15572
Epoch: [3]  [ 220/2522]  eta: 0:23:37  lr: 0.000029  min_lr: 0.000000  loss: 4.7595 (4.7609)  loss_scale: 65536.0000 (65832.5430)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1389  max mem: 15572
Epoch: [3]  [ 230/2522]  eta: 0:23:37  lr: 0.000029  min_lr: 0.000000  loss: 4.7180 (4.7577)  loss_scale: 65536.0000 (65819.7056)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1667  max mem: 15572
Epoch: [3]  [ 240/2522]  eta: 0:23:30  lr: 0.000029  min_lr: 0.000000  loss: 4.6738 (4.7541)  loss_scale: 65536.0000 (65807.9336)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1999  max mem: 15572
Epoch: [3]  [ 250/2522]  eta: 0:23:20  lr: 0.000029  min_lr: 0.000000  loss: 4.7092 (4.7529)  loss_scale: 65536.0000 (65797.0996)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1502  max mem: 15572
[2025-01-13 11:46:40,885] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:46:40,886] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 260/2522]  eta: 0:23:15  lr: 0.000029  min_lr: 0.000000  loss: 4.7581 (4.7526)  loss_scale: 65536.0000 (66791.4789)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1447  max mem: 15572
[2025-01-13 11:46:43,145] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8510
[2025-01-13 11:46:43,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:46:43,145] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 270/2522]  eta: 0:23:12  lr: 0.000029  min_lr: 0.000000  loss: 4.7114 (4.7541)  loss_scale: 65536.0000 (66745.1513)  weight_decay: 0.0500 (0.0500)  time: 0.6434  data: 0.1741  max mem: 15572
Epoch: [3]  [ 280/2522]  eta: 0:23:06  lr: 0.000029  min_lr: 0.000000  loss: 4.8754 (4.7562)  loss_scale: 65536.0000 (66702.1210)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1439  max mem: 15572
Epoch: [3]  [ 290/2522]  eta: 0:22:58  lr: 0.000029  min_lr: 0.000000  loss: 4.8851 (4.7602)  loss_scale: 65536.0000 (66662.0481)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1270  max mem: 15572
[2025-01-13 11:47:07,499] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8548
[2025-01-13 11:47:07,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:47:07,500] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [ 300/2522]  eta: 0:22:56  lr: 0.000029  min_lr: 0.000000  loss: 4.8439 (4.7609)  loss_scale: 65536.0000 (66406.9103)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.1740  max mem: 15572
Epoch: [3]  [ 310/2522]  eta: 0:22:51  lr: 0.000029  min_lr: 0.000000  loss: 4.7926 (4.7637)  loss_scale: 32768.0000 (65325.2733)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.1688  max mem: 15572
Epoch: [3]  [ 320/2522]  eta: 0:22:37  lr: 0.000029  min_lr: 0.000000  loss: 4.7900 (4.7639)  loss_scale: 32768.0000 (64311.0280)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0942  max mem: 15572
Epoch: [3]  [ 330/2522]  eta: 0:22:32  lr: 0.000029  min_lr: 0.000000  loss: 4.7647 (4.7644)  loss_scale: 32768.0000 (63358.0665)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1208  max mem: 15572
Epoch: [3]  [ 340/2522]  eta: 0:22:27  lr: 0.000029  min_lr: 0.000000  loss: 4.7647 (4.7649)  loss_scale: 32768.0000 (62460.9971)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1797  max mem: 15572
Epoch: [3]  [ 350/2522]  eta: 0:22:21  lr: 0.000029  min_lr: 0.000000  loss: 4.7811 (4.7661)  loss_scale: 32768.0000 (61615.0427)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1742  max mem: 15572
Epoch: [3]  [ 360/2522]  eta: 0:22:02  lr: 0.000029  min_lr: 0.000000  loss: 4.7811 (4.7653)  loss_scale: 32768.0000 (60815.9557)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0875  max mem: 15572
Epoch: [3]  [ 370/2522]  eta: 0:21:46  lr: 0.000030  min_lr: 0.000000  loss: 4.7705 (4.7656)  loss_scale: 32768.0000 (60059.9461)  weight_decay: 0.0500 (0.0500)  time: 0.4246  data: 0.0005  max mem: 15572
Epoch: [3]  [ 380/2522]  eta: 0:21:32  lr: 0.000030  min_lr: 0.000000  loss: 4.7705 (4.7662)  loss_scale: 32768.0000 (59343.6220)  weight_decay: 0.0500 (0.0500)  time: 0.4564  data: 0.0007  max mem: 15572
Epoch: [3]  [ 390/2522]  eta: 0:21:24  lr: 0.000030  min_lr: 0.000000  loss: 4.7567 (4.7662)  loss_scale: 32768.0000 (58663.9386)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0275  max mem: 15572
Epoch: [3]  [ 400/2522]  eta: 0:21:23  lr: 0.000030  min_lr: 0.000000  loss: 4.7636 (4.7652)  loss_scale: 32768.0000 (58018.1546)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1269  max mem: 15572
Epoch: [3]  [ 410/2522]  eta: 0:21:27  lr: 0.000030  min_lr: 0.000000  loss: 4.6995 (4.7640)  loss_scale: 32768.0000 (57403.7956)  weight_decay: 0.0500 (0.0500)  time: 0.7557  data: 0.2483  max mem: 15572
Epoch: [3]  [ 420/2522]  eta: 0:21:19  lr: 0.000030  min_lr: 0.000000  loss: 4.6856 (4.7637)  loss_scale: 32768.0000 (56818.6223)  weight_decay: 0.0500 (0.0500)  time: 0.6915  data: 0.2022  max mem: 15572
[2025-01-13 11:48:23,868] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:48:23,868] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 430/2522]  eta: 0:21:17  lr: 0.000030  min_lr: 0.000000  loss: 4.7453 (4.7637)  loss_scale: 32768.0000 (56488.6868)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1608  max mem: 15572
Epoch: [3]  [ 440/2522]  eta: 0:21:18  lr: 0.000030  min_lr: 0.000000  loss: 4.7453 (4.7620)  loss_scale: 65536.0000 (56693.8413)  weight_decay: 0.0500 (0.0500)  time: 0.7225  data: 0.2384  max mem: 15572
Epoch: [3]  [ 450/2522]  eta: 0:21:18  lr: 0.000030  min_lr: 0.000000  loss: 4.7185 (4.7627)  loss_scale: 65536.0000 (56889.8980)  weight_decay: 0.0500 (0.0500)  time: 0.7496  data: 0.2687  max mem: 15572
Epoch: [3]  [ 460/2522]  eta: 0:21:13  lr: 0.000030  min_lr: 0.000000  loss: 4.7164 (4.7612)  loss_scale: 65536.0000 (57077.4490)  weight_decay: 0.0500 (0.0500)  time: 0.6984  data: 0.2129  max mem: 15572
Epoch: [3]  [ 470/2522]  eta: 0:21:09  lr: 0.000030  min_lr: 0.000000  loss: 4.6859 (4.7610)  loss_scale: 65536.0000 (57257.0361)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.1785  max mem: 15572
Epoch: [3]  [ 480/2522]  eta: 0:21:01  lr: 0.000030  min_lr: 0.000000  loss: 4.7118 (4.7607)  loss_scale: 65536.0000 (57429.1559)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1354  max mem: 15572
Epoch: [3]  [ 490/2522]  eta: 0:21:02  lr: 0.000030  min_lr: 0.000000  loss: 4.7603 (4.7612)  loss_scale: 65536.0000 (57594.2648)  weight_decay: 0.0500 (0.0500)  time: 0.6775  data: 0.1748  max mem: 15572
Epoch: [3]  [ 500/2522]  eta: 0:20:58  lr: 0.000030  min_lr: 0.000000  loss: 4.8002 (4.7610)  loss_scale: 65536.0000 (57752.7824)  weight_decay: 0.0500 (0.0500)  time: 0.7291  data: 0.2559  max mem: 15572
Epoch: [3]  [ 510/2522]  eta: 0:20:47  lr: 0.000030  min_lr: 0.000000  loss: 4.7612 (4.7603)  loss_scale: 65536.0000 (57905.0959)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1140  max mem: 15572
Epoch: [3]  [ 520/2522]  eta: 0:20:34  lr: 0.000030  min_lr: 0.000000  loss: 4.7362 (4.7613)  loss_scale: 65536.0000 (58051.5624)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0009  max mem: 15572
Epoch: [3]  [ 530/2522]  eta: 0:20:32  lr: 0.000030  min_lr: 0.000000  loss: 4.9271 (4.7645)  loss_scale: 65536.0000 (58192.5122)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1102  max mem: 15572
[2025-01-13 11:49:32,879] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8782
[2025-01-13 11:49:32,880] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:49:32,880] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [ 540/2522]  eta: 0:20:27  lr: 0.000030  min_lr: 0.000000  loss: 4.8136 (4.7650)  loss_scale: 65536.0000 (57843.6969)  weight_decay: 0.0500 (0.0500)  time: 0.6879  data: 0.2179  max mem: 15572
Epoch: [3]  [ 550/2522]  eta: 0:20:24  lr: 0.000030  min_lr: 0.000000  loss: 4.7598 (4.7629)  loss_scale: 32768.0000 (57388.6025)  weight_decay: 0.0500 (0.0500)  time: 0.6899  data: 0.2310  max mem: 15572
Epoch: [3]  [ 560/2522]  eta: 0:20:16  lr: 0.000030  min_lr: 0.000000  loss: 4.7008 (4.7604)  loss_scale: 32768.0000 (56949.7326)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.1808  max mem: 15572
Epoch: [3]  [ 570/2522]  eta: 0:20:13  lr: 0.000030  min_lr: 0.000000  loss: 4.7055 (4.7602)  loss_scale: 32768.0000 (56526.2347)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.1924  max mem: 15572
Epoch: [3]  [ 580/2522]  eta: 0:20:08  lr: 0.000030  min_lr: 0.000000  loss: 4.7579 (4.7607)  loss_scale: 32768.0000 (56117.3150)  weight_decay: 0.0500 (0.0500)  time: 0.6909  data: 0.2213  max mem: 15572
Epoch: [3]  [ 590/2522]  eta: 0:20:05  lr: 0.000030  min_lr: 0.000000  loss: 4.7755 (4.7596)  loss_scale: 32768.0000 (55722.2335)  weight_decay: 0.0500 (0.0500)  time: 0.6779  data: 0.1955  max mem: 15572
Epoch: [3]  [ 600/2522]  eta: 0:20:02  lr: 0.000030  min_lr: 0.000000  loss: 4.7189 (4.7588)  loss_scale: 32768.0000 (55340.2995)  weight_decay: 0.0500 (0.0500)  time: 0.7176  data: 0.2259  max mem: 15572
Epoch: [3]  [ 610/2522]  eta: 0:19:49  lr: 0.000030  min_lr: 0.000000  loss: 4.7469 (4.7599)  loss_scale: 32768.0000 (54970.8674)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1174  max mem: 15572
Epoch: [3]  [ 620/2522]  eta: 0:19:38  lr: 0.000030  min_lr: 0.000000  loss: 4.8285 (4.7605)  loss_scale: 32768.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.4437  data: 0.0008  max mem: 15572
Epoch: [3]  [ 630/2522]  eta: 0:19:28  lr: 0.000030  min_lr: 0.000000  loss: 4.7372 (4.7602)  loss_scale: 32768.0000 (54267.1315)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0010  max mem: 15572
Epoch: [3]  [ 640/2522]  eta: 0:19:23  lr: 0.000031  min_lr: 0.000000  loss: 4.6601 (4.7585)  loss_scale: 32768.0000 (53931.7317)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0730  max mem: 15572
Epoch: [3]  [ 650/2522]  eta: 0:19:20  lr: 0.000031  min_lr: 0.000000  loss: 4.7268 (4.7590)  loss_scale: 32768.0000 (53606.6359)  weight_decay: 0.0500 (0.0500)  time: 0.6980  data: 0.2011  max mem: 15572
Epoch: [3]  [ 660/2522]  eta: 0:19:14  lr: 0.000031  min_lr: 0.000000  loss: 4.7486 (4.7584)  loss_scale: 32768.0000 (53291.3767)  weight_decay: 0.0500 (0.0500)  time: 0.6858  data: 0.2137  max mem: 15572
[2025-01-13 11:50:52,745] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:50:52,746] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 670/2522]  eta: 0:19:09  lr: 0.000031  min_lr: 0.000000  loss: 4.8132 (4.7601)  loss_scale: 32768.0000 (53425.0253)  weight_decay: 0.0500 (0.0500)  time: 0.6352  data: 0.1572  max mem: 15572
Epoch: [3]  [ 680/2522]  eta: 0:19:05  lr: 0.000031  min_lr: 0.000000  loss: 4.8143 (4.7605)  loss_scale: 65536.0000 (53602.8664)  weight_decay: 0.0500 (0.0500)  time: 0.6721  data: 0.1780  max mem: 15572
Epoch: [3]  [ 690/2522]  eta: 0:18:57  lr: 0.000031  min_lr: 0.000000  loss: 4.7725 (4.7604)  loss_scale: 65536.0000 (53775.5601)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.1469  max mem: 15572
Epoch: [3]  [ 700/2522]  eta: 0:18:47  lr: 0.000031  min_lr: 0.000000  loss: 4.8203 (4.7617)  loss_scale: 65536.0000 (53943.3267)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0549  max mem: 15572
Epoch: [3]  [ 710/2522]  eta: 0:18:44  lr: 0.000031  min_lr: 0.000000  loss: 4.7926 (4.7618)  loss_scale: 65536.0000 (54106.3741)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1429  max mem: 15572
Epoch: [3]  [ 720/2522]  eta: 0:18:38  lr: 0.000031  min_lr: 0.000000  loss: 4.7833 (4.7626)  loss_scale: 65536.0000 (54264.8988)  weight_decay: 0.0500 (0.0500)  time: 0.6829  data: 0.2095  max mem: 15572
Epoch: [3]  [ 730/2522]  eta: 0:18:35  lr: 0.000031  min_lr: 0.000000  loss: 4.8234 (4.7617)  loss_scale: 65536.0000 (54419.0862)  weight_decay: 0.0500 (0.0500)  time: 0.6922  data: 0.2299  max mem: 15572
Epoch: [3]  [ 740/2522]  eta: 0:18:32  lr: 0.000031  min_lr: 0.000000  loss: 4.8382 (4.7620)  loss_scale: 65536.0000 (54569.1120)  weight_decay: 0.0500 (0.0500)  time: 0.7481  data: 0.2780  max mem: 15572
[2025-01-13 11:51:48,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=50, lr=[2.9953642135997035e-07, 2.9953642135997035e-07, 4.2790917337138624e-07, 4.2790917337138624e-07, 6.112988191019804e-07, 6.112988191019804e-07, 8.732840272885435e-07, 8.732840272885435e-07, 1.247548610412205e-06, 1.247548610412205e-06, 1.7822123005888645e-06, 1.7822123005888645e-06, 2.5460175722698063e-06, 2.5460175722698063e-06, 3.637167960385438e-06, 3.637167960385438e-06, 5.195954229122055e-06, 5.195954229122055e-06, 7.42279175588865e-06, 7.42279175588865e-06, 1.0603988222698071e-05, 1.0603988222698071e-05, 1.514855460385439e-05, 1.514855460385439e-05, 2.164079229122056e-05, 2.164079229122056e-05, 3.091541755888651e-05, 3.091541755888651e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 11:51:48,656] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=27.192146678481343, CurrSamplesPerSec=24.490272089545922, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [ 750/2522]  eta: 0:18:21  lr: 0.000031  min_lr: 0.000000  loss: 4.6998 (4.7610)  loss_scale: 65536.0000 (54715.1425)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1292  max mem: 15572
Epoch: [3]  [ 760/2522]  eta: 0:18:14  lr: 0.000031  min_lr: 0.000000  loss: 4.6646 (4.7608)  loss_scale: 65536.0000 (54857.3351)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0468  max mem: 15572
Epoch: [3]  [ 770/2522]  eta: 0:18:08  lr: 0.000031  min_lr: 0.000000  loss: 4.6846 (4.7610)  loss_scale: 65536.0000 (54995.8392)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1456  max mem: 15572
Epoch: [3]  [ 780/2522]  eta: 0:18:04  lr: 0.000031  min_lr: 0.000000  loss: 4.7066 (4.7610)  loss_scale: 65536.0000 (55130.7964)  weight_decay: 0.0500 (0.0500)  time: 0.6683  data: 0.2159  max mem: 15572
[2025-01-13 11:52:12,681] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:52:12,681] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 790/2522]  eta: 0:17:55  lr: 0.000031  min_lr: 0.000000  loss: 4.7316 (4.7605)  loss_scale: 65536.0000 (55345.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1169  max mem: 15572
[2025-01-13 11:52:13,194] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9040
[2025-01-13 11:52:13,194] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:52:13,194] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-13 11:52:13,576] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9041
[2025-01-13 11:52:13,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:52:13,576] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [ 800/2522]  eta: 0:17:46  lr: 0.000031  min_lr: 0.000000  loss: 4.7296 (4.7593)  loss_scale: 65536.0000 (55104.2397)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0262  max mem: 15572
Epoch: [3]  [ 810/2522]  eta: 0:17:41  lr: 0.000031  min_lr: 0.000000  loss: 4.6643 (4.7585)  loss_scale: 32768.0000 (54828.8237)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1042  max mem: 15572
Epoch: [3]  [ 820/2522]  eta: 0:17:35  lr: 0.000031  min_lr: 0.000000  loss: 4.6736 (4.7584)  loss_scale: 32768.0000 (54560.1169)  weight_decay: 0.0500 (0.0500)  time: 0.6531  data: 0.1703  max mem: 15572
Epoch: [3]  [ 830/2522]  eta: 0:17:32  lr: 0.000031  min_lr: 0.000000  loss: 4.6763 (4.7572)  loss_scale: 32768.0000 (54297.8773)  weight_decay: 0.0500 (0.0500)  time: 0.7044  data: 0.2036  max mem: 15572
Epoch: [3]  [ 840/2522]  eta: 0:17:24  lr: 0.000031  min_lr: 0.000000  loss: 4.6993 (4.7576)  loss_scale: 32768.0000 (54041.8740)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1444  max mem: 15572
Epoch: [3]  [ 850/2522]  eta: 0:17:17  lr: 0.000031  min_lr: 0.000000  loss: 4.8171 (4.7578)  loss_scale: 32768.0000 (53791.8872)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0969  max mem: 15572
Epoch: [3]  [ 860/2522]  eta: 0:17:11  lr: 0.000031  min_lr: 0.000000  loss: 4.7637 (4.7576)  loss_scale: 32768.0000 (53547.7073)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1385  max mem: 15572
Epoch: [3]  [ 870/2522]  eta: 0:17:05  lr: 0.000031  min_lr: 0.000000  loss: 4.7446 (4.7566)  loss_scale: 32768.0000 (53309.1343)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1166  max mem: 15572
Epoch: [3]  [ 880/2522]  eta: 0:16:56  lr: 0.000031  min_lr: 0.000000  loss: 4.8127 (4.7578)  loss_scale: 32768.0000 (53075.9773)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0430  max mem: 15572
Epoch: [3]  [ 890/2522]  eta: 0:16:49  lr: 0.000031  min_lr: 0.000000  loss: 4.7083 (4.7572)  loss_scale: 32768.0000 (52848.0539)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0489  max mem: 15572
Epoch: [3]  [ 900/2522]  eta: 0:16:44  lr: 0.000031  min_lr: 0.000000  loss: 4.6837 (4.7570)  loss_scale: 32768.0000 (52625.1898)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1560  max mem: 15572
Epoch: [3]  [ 910/2522]  eta: 0:16:36  lr: 0.000032  min_lr: 0.000000  loss: 4.6765 (4.7554)  loss_scale: 32768.0000 (52407.2184)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1080  max mem: 15572
Epoch: [3]  [ 920/2522]  eta: 0:16:28  lr: 0.000032  min_lr: 0.000000  loss: 4.7488 (4.7559)  loss_scale: 32768.0000 (52193.9805)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0012  max mem: 15572
[2025-01-13 11:53:30,221] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:53:30,221] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 930/2522]  eta: 0:16:22  lr: 0.000032  min_lr: 0.000000  loss: 4.7351 (4.7540)  loss_scale: 32768.0000 (52337.2889)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0900  max mem: 15572
Epoch: [3]  [ 940/2522]  eta: 0:16:16  lr: 0.000032  min_lr: 0.000000  loss: 4.6852 (4.7553)  loss_scale: 65536.0000 (52477.5515)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1544  max mem: 15572
Epoch: [3]  [ 950/2522]  eta: 0:16:10  lr: 0.000032  min_lr: 0.000000  loss: 4.7279 (4.7549)  loss_scale: 65536.0000 (52614.8644)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.1356  max mem: 15572
Epoch: [3]  [ 960/2522]  eta: 0:16:04  lr: 0.000032  min_lr: 0.000000  loss: 4.7544 (4.7547)  loss_scale: 65536.0000 (52749.3195)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.1467  max mem: 15572
Epoch: [3]  [ 970/2522]  eta: 0:15:59  lr: 0.000032  min_lr: 0.000000  loss: 4.7720 (4.7553)  loss_scale: 65536.0000 (52881.0051)  weight_decay: 0.0500 (0.0500)  time: 0.6667  data: 0.1894  max mem: 15572
Epoch: [3]  [ 980/2522]  eta: 0:15:54  lr: 0.000032  min_lr: 0.000000  loss: 4.7058 (4.7548)  loss_scale: 65536.0000 (53010.0061)  weight_decay: 0.0500 (0.0500)  time: 0.6879  data: 0.2135  max mem: 15572
Epoch: [3]  [ 990/2522]  eta: 0:15:50  lr: 0.000032  min_lr: 0.000000  loss: 4.7058 (4.7555)  loss_scale: 65536.0000 (53136.4036)  weight_decay: 0.0500 (0.0500)  time: 0.7193  data: 0.2412  max mem: 15572
Epoch: [3]  [1000/2522]  eta: 0:15:43  lr: 0.000032  min_lr: 0.000000  loss: 4.8276 (4.7556)  loss_scale: 65536.0000 (53260.2757)  weight_decay: 0.0500 (0.0500)  time: 0.6471  data: 0.1475  max mem: 15572
Epoch: [3]  [1010/2522]  eta: 0:15:35  lr: 0.000032  min_lr: 0.000000  loss: 4.6723 (4.7547)  loss_scale: 65536.0000 (53381.6973)  weight_decay: 0.0500 (0.0500)  time: 0.5305  data: 0.0073  max mem: 15572
Epoch: [3]  [1020/2522]  eta: 0:15:30  lr: 0.000032  min_lr: 0.000000  loss: 4.6500 (4.7544)  loss_scale: 65536.0000 (53500.7405)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1200  max mem: 15572
Epoch: [3]  [1030/2522]  eta: 0:15:26  lr: 0.000032  min_lr: 0.000000  loss: 4.7256 (4.7542)  loss_scale: 65536.0000 (53617.4743)  weight_decay: 0.0500 (0.0500)  time: 0.7203  data: 0.2587  max mem: 15572
Epoch: [3]  [1040/2522]  eta: 0:15:20  lr: 0.000032  min_lr: 0.000000  loss: 4.7518 (4.7543)  loss_scale: 65536.0000 (53731.9654)  weight_decay: 0.0500 (0.0500)  time: 0.6993  data: 0.2189  max mem: 15572
[2025-01-13 11:54:54,640] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:54:54,640] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 11:54:55,118] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9299
[2025-01-13 11:54:55,119] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 11:54:55,119] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1050/2522]  eta: 0:15:15  lr: 0.000032  min_lr: 0.000000  loss: 4.6667 (4.7532)  loss_scale: 65536.0000 (53906.6337)  weight_decay: 0.0500 (0.0500)  time: 0.6701  data: 0.1989  max mem: 15572
Epoch: [3]  [1060/2522]  eta: 0:15:10  lr: 0.000032  min_lr: 0.000000  loss: 4.6397 (4.7525)  loss_scale: 65536.0000 (54016.2413)  weight_decay: 0.0500 (0.0500)  time: 0.7286  data: 0.2342  max mem: 15572
Epoch: [3]  [1070/2522]  eta: 0:15:06  lr: 0.000032  min_lr: 0.000000  loss: 4.7711 (4.7541)  loss_scale: 65536.0000 (54123.8021)  weight_decay: 0.0500 (0.0500)  time: 0.7542  data: 0.2469  max mem: 15572
Epoch: [3]  [1080/2522]  eta: 0:15:00  lr: 0.000032  min_lr: 0.000000  loss: 4.6665 (4.7524)  loss_scale: 65536.0000 (54229.3728)  weight_decay: 0.0500 (0.0500)  time: 0.7259  data: 0.2334  max mem: 15572
Epoch: [3]  [1090/2522]  eta: 0:14:55  lr: 0.000032  min_lr: 0.000000  loss: 4.5959 (4.7516)  loss_scale: 65536.0000 (54333.0082)  weight_decay: 0.0500 (0.0500)  time: 0.7098  data: 0.2004  max mem: 15572
[2025-01-13 11:55:29,400] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9347
[2025-01-13 11:55:29,401] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:55:29,401] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [1100/2522]  eta: 0:14:49  lr: 0.000032  min_lr: 0.000000  loss: 4.7423 (4.7522)  loss_scale: 65536.0000 (54345.4750)  weight_decay: 0.0500 (0.0500)  time: 0.6590  data: 0.1554  max mem: 15572
Epoch: [3]  [1110/2522]  eta: 0:14:40  lr: 0.000032  min_lr: 0.000000  loss: 4.8013 (4.7526)  loss_scale: 32768.0000 (54151.2583)  weight_decay: 0.0500 (0.0500)  time: 0.5229  data: 0.0569  max mem: 15572
Epoch: [3]  [1120/2522]  eta: 0:14:32  lr: 0.000032  min_lr: 0.000000  loss: 4.7981 (4.7530)  loss_scale: 32768.0000 (53960.5067)  weight_decay: 0.0500 (0.0500)  time: 0.4740  data: 0.0009  max mem: 15572
Epoch: [3]  [1130/2522]  eta: 0:14:26  lr: 0.000032  min_lr: 0.000000  loss: 4.7248 (4.7523)  loss_scale: 32768.0000 (53773.1282)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0595  max mem: 15572
Epoch: [3]  [1140/2522]  eta: 0:14:18  lr: 0.000032  min_lr: 0.000000  loss: 4.7309 (4.7527)  loss_scale: 32768.0000 (53589.0342)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0904  max mem: 15572
Epoch: [3]  [1150/2522]  eta: 0:14:12  lr: 0.000032  min_lr: 0.000000  loss: 4.7365 (4.7527)  loss_scale: 32768.0000 (53408.1390)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0948  max mem: 15572
Epoch: [3]  [1160/2522]  eta: 0:14:06  lr: 0.000032  min_lr: 0.000000  loss: 4.7771 (4.7526)  loss_scale: 32768.0000 (53230.3600)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1536  max mem: 15572
Epoch: [3]  [1170/2522]  eta: 0:14:01  lr: 0.000032  min_lr: 0.000000  loss: 4.7806 (4.7533)  loss_scale: 32768.0000 (53055.6174)  weight_decay: 0.0500 (0.0500)  time: 0.6788  data: 0.1868  max mem: 15572
Epoch: [3]  [1180/2522]  eta: 0:13:54  lr: 0.000033  min_lr: 0.000000  loss: 4.7476 (4.7532)  loss_scale: 32768.0000 (52883.8340)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1439  max mem: 15572
Epoch: [3]  [1190/2522]  eta: 0:13:47  lr: 0.000033  min_lr: 0.000000  loss: 4.6869 (4.7530)  loss_scale: 32768.0000 (52714.9353)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1275  max mem: 15572
Epoch: [3]  [1200/2522]  eta: 0:13:42  lr: 0.000033  min_lr: 0.000000  loss: 4.7655 (4.7538)  loss_scale: 32768.0000 (52548.8493)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.1869  max mem: 15572
Epoch: [3]  [1210/2522]  eta: 0:13:37  lr: 0.000033  min_lr: 0.000000  loss: 4.8254 (4.7538)  loss_scale: 32768.0000 (52385.5062)  weight_decay: 0.0500 (0.0500)  time: 0.7137  data: 0.2204  max mem: 15572
Epoch: [3]  [1220/2522]  eta: 0:13:29  lr: 0.000033  min_lr: 0.000000  loss: 4.7655 (4.7534)  loss_scale: 32768.0000 (52224.8387)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1143  max mem: 15572
[2025-01-13 11:56:45,550] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:56:45,551] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [1230/2522]  eta: 0:13:23  lr: 0.000033  min_lr: 0.000000  loss: 4.7305 (4.7531)  loss_scale: 32768.0000 (52173.2575)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0433  max mem: 15572
Epoch: [3]  [1240/2522]  eta: 0:13:16  lr: 0.000033  min_lr: 0.000000  loss: 4.8087 (4.7543)  loss_scale: 65536.0000 (52280.9347)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0758  max mem: 15572
Epoch: [3]  [1250/2522]  eta: 0:13:08  lr: 0.000033  min_lr: 0.000000  loss: 4.8387 (4.7547)  loss_scale: 65536.0000 (52386.8905)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0560  max mem: 15572
Epoch: [3]  [1260/2522]  eta: 0:13:01  lr: 0.000033  min_lr: 0.000000  loss: 4.6941 (4.7529)  loss_scale: 65536.0000 (52491.1657)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0235  max mem: 15572
Epoch: [3]  [1270/2522]  eta: 0:12:56  lr: 0.000033  min_lr: 0.000000  loss: 4.6188 (4.7528)  loss_scale: 65536.0000 (52593.8002)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1450  max mem: 15572
Epoch: [3]  [1280/2522]  eta: 0:12:49  lr: 0.000033  min_lr: 0.000000  loss: 4.6983 (4.7527)  loss_scale: 65536.0000 (52694.8322)  weight_decay: 0.0500 (0.0500)  time: 0.6710  data: 0.2154  max mem: 15572
Epoch: [3]  [1290/2522]  eta: 0:12:44  lr: 0.000033  min_lr: 0.000000  loss: 4.7182 (4.7531)  loss_scale: 65536.0000 (52794.2990)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1751  max mem: 15572
Epoch: [3]  [1300/2522]  eta: 0:12:38  lr: 0.000033  min_lr: 0.000000  loss: 4.7136 (4.7524)  loss_scale: 65536.0000 (52892.2367)  weight_decay: 0.0500 (0.0500)  time: 0.6740  data: 0.2005  max mem: 15572
Epoch: [3]  [1310/2522]  eta: 0:12:32  lr: 0.000033  min_lr: 0.000000  loss: 4.7017 (4.7525)  loss_scale: 65536.0000 (52988.6804)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.1769  max mem: 15572
[2025-01-13 11:57:39,099] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9565
[2025-01-13 11:57:39,099] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 11:57:39,099] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [1320/2522]  eta: 0:12:25  lr: 0.000033  min_lr: 0.000000  loss: 4.7128 (4.7525)  loss_scale: 65536.0000 (52959.6366)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1139  max mem: 15572
Epoch: [3]  [1330/2522]  eta: 0:12:19  lr: 0.000033  min_lr: 0.000000  loss: 4.6131 (4.7520)  loss_scale: 32768.0000 (52807.9339)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1251  max mem: 15572
Epoch: [3]  [1340/2522]  eta: 0:12:12  lr: 0.000033  min_lr: 0.000000  loss: 4.6131 (4.7511)  loss_scale: 32768.0000 (52658.4937)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1455  max mem: 15572
Epoch: [3]  [1350/2522]  eta: 0:12:05  lr: 0.000033  min_lr: 0.000000  loss: 4.6193 (4.7501)  loss_scale: 32768.0000 (52511.2657)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0541  max mem: 15572
Epoch: [3]  [1360/2522]  eta: 0:11:58  lr: 0.000033  min_lr: 0.000000  loss: 4.6373 (4.7498)  loss_scale: 32768.0000 (52366.2013)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0007  max mem: 15572
Epoch: [3]  [1370/2522]  eta: 0:11:51  lr: 0.000033  min_lr: 0.000000  loss: 4.6447 (4.7488)  loss_scale: 32768.0000 (52223.2531)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0400  max mem: 15572
Epoch: [3]  [1380/2522]  eta: 0:11:46  lr: 0.000033  min_lr: 0.000000  loss: 4.6447 (4.7487)  loss_scale: 32768.0000 (52082.3751)  weight_decay: 0.0500 (0.0500)  time: 0.6620  data: 0.1508  max mem: 15572
Epoch: [3]  [1390/2522]  eta: 0:11:40  lr: 0.000033  min_lr: 0.000000  loss: 4.7207 (4.7479)  loss_scale: 32768.0000 (51943.5226)  weight_decay: 0.0500 (0.0500)  time: 0.6644  data: 0.1632  max mem: 15572
Epoch: [3]  [1400/2522]  eta: 0:11:35  lr: 0.000033  min_lr: 0.000000  loss: 4.7079 (4.7473)  loss_scale: 32768.0000 (51806.6524)  weight_decay: 0.0500 (0.0500)  time: 0.6896  data: 0.1831  max mem: 15572
Epoch: [3]  [1410/2522]  eta: 0:11:29  lr: 0.000033  min_lr: 0.000000  loss: 4.7079 (4.7473)  loss_scale: 32768.0000 (51671.7222)  weight_decay: 0.0500 (0.0500)  time: 0.7506  data: 0.2423  max mem: 15572
Epoch: [3]  [1420/2522]  eta: 0:11:23  lr: 0.000033  min_lr: 0.000000  loss: 4.6956 (4.7466)  loss_scale: 32768.0000 (51538.6911)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.1643  max mem: 15572
Epoch: [3]  [1430/2522]  eta: 0:11:18  lr: 0.000033  min_lr: 0.000000  loss: 4.6333 (4.7464)  loss_scale: 32768.0000 (51407.5192)  weight_decay: 0.0500 (0.0500)  time: 0.7027  data: 0.1944  max mem: 15572
Epoch: [3]  [1440/2522]  eta: 0:11:12  lr: 0.000033  min_lr: 0.000000  loss: 4.6558 (4.7464)  loss_scale: 32768.0000 (51278.1679)  weight_decay: 0.0500 (0.0500)  time: 0.7045  data: 0.2189  max mem: 15572
[2025-01-13 11:59:01,213] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 11:59:01,214] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [1450/2522]  eta: 0:11:06  lr: 0.000034  min_lr: 0.000000  loss: 4.7139 (4.7459)  loss_scale: 32768.0000 (51286.0979)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1494  max mem: 15572
Epoch: [3]  [1460/2522]  eta: 0:10:58  lr: 0.000034  min_lr: 0.000000  loss: 4.7139 (4.7453)  loss_scale: 65536.0000 (51383.6331)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0921  max mem: 15572
Epoch: [3]  [1470/2522]  eta: 0:10:51  lr: 0.000034  min_lr: 0.000000  loss: 4.6397 (4.7448)  loss_scale: 65536.0000 (51479.8423)  weight_decay: 0.0500 (0.0500)  time: 0.4571  data: 0.0208  max mem: 15572
Epoch: [3]  [1480/2522]  eta: 0:10:44  lr: 0.000034  min_lr: 0.000000  loss: 4.6397 (4.7444)  loss_scale: 65536.0000 (51574.7522)  weight_decay: 0.0500 (0.0500)  time: 0.4531  data: 0.0009  max mem: 15572
Epoch: [3]  [1490/2522]  eta: 0:10:37  lr: 0.000034  min_lr: 0.000000  loss: 4.5508 (4.7435)  loss_scale: 65536.0000 (51668.3890)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0009  max mem: 15572
Epoch: [3]  [1500/2522]  eta: 0:10:30  lr: 0.000034  min_lr: 0.000000  loss: 4.6913 (4.7438)  loss_scale: 65536.0000 (51760.7781)  weight_decay: 0.0500 (0.0500)  time: 0.5229  data: 0.0465  max mem: 15572
Epoch: [3]  [1510/2522]  eta: 0:10:24  lr: 0.000034  min_lr: 0.000000  loss: 4.7360 (4.7437)  loss_scale: 65536.0000 (51851.9444)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1454  max mem: 15572
Epoch: [3]  [1520/2522]  eta: 0:10:17  lr: 0.000034  min_lr: 0.000000  loss: 4.7473 (4.7441)  loss_scale: 65536.0000 (51941.9119)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1211  max mem: 15572
Epoch: [3]  [1530/2522]  eta: 0:10:10  lr: 0.000034  min_lr: 0.000000  loss: 4.7619 (4.7441)  loss_scale: 65536.0000 (52030.7041)  weight_decay: 0.0500 (0.0500)  time: 0.4615  data: 0.0221  max mem: 15572
Epoch: [3]  [1540/2522]  eta: 0:10:03  lr: 0.000034  min_lr: 0.000000  loss: 4.7095 (4.7436)  loss_scale: 65536.0000 (52118.3439)  weight_decay: 0.0500 (0.0500)  time: 0.4696  data: 0.0334  max mem: 15572
Epoch: [3]  [1550/2522]  eta: 0:09:55  lr: 0.000034  min_lr: 0.000000  loss: 4.6646 (4.7431)  loss_scale: 65536.0000 (52204.8536)  weight_decay: 0.0500 (0.0500)  time: 0.4511  data: 0.0333  max mem: 15572
Epoch: [3]  [1560/2522]  eta: 0:09:48  lr: 0.000034  min_lr: 0.000000  loss: 4.7424 (4.7435)  loss_scale: 65536.0000 (52290.2550)  weight_decay: 0.0500 (0.0500)  time: 0.4164  data: 0.0006  max mem: 15572
Epoch: [3]  [1570/2522]  eta: 0:09:41  lr: 0.000034  min_lr: 0.000000  loss: 4.7677 (4.7435)  loss_scale: 65536.0000 (52374.5691)  weight_decay: 0.0500 (0.0500)  time: 0.4588  data: 0.0008  max mem: 15572
[2025-01-13 12:00:03,778] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:00:03,779] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:00:04,205] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9823
[2025-01-13 12:00:04,205] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:00:04,205] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1580/2522]  eta: 0:09:34  lr: 0.000034  min_lr: 0.000000  loss: 4.7111 (4.7430)  loss_scale: 65536.0000 (52499.2688)  weight_decay: 0.0500 (0.0500)  time: 0.4901  data: 0.0009  max mem: 15572
Epoch: [3]  [1590/2522]  eta: 0:09:28  lr: 0.000034  min_lr: 0.000000  loss: 4.6920 (4.7429)  loss_scale: 65536.0000 (52581.2093)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0009  max mem: 15572
Epoch: [3]  [1600/2522]  eta: 0:09:21  lr: 0.000034  min_lr: 0.000000  loss: 4.7230 (4.7428)  loss_scale: 65536.0000 (52662.1262)  weight_decay: 0.0500 (0.0500)  time: 0.4901  data: 0.0011  max mem: 15572
Epoch: [3]  [1610/2522]  eta: 0:09:14  lr: 0.000034  min_lr: 0.000000  loss: 4.7632 (4.7429)  loss_scale: 65536.0000 (52742.0385)  weight_decay: 0.0500 (0.0500)  time: 0.4638  data: 0.0010  max mem: 15572
Epoch: [3]  [1620/2522]  eta: 0:09:07  lr: 0.000034  min_lr: 0.000000  loss: 4.8032 (4.7426)  loss_scale: 65536.0000 (52820.9648)  weight_decay: 0.0500 (0.0500)  time: 0.4409  data: 0.0006  max mem: 15572
Epoch: [3]  [1630/2522]  eta: 0:09:00  lr: 0.000034  min_lr: 0.000000  loss: 4.5879 (4.7418)  loss_scale: 65536.0000 (52898.9234)  weight_decay: 0.0500 (0.0500)  time: 0.4429  data: 0.0008  max mem: 15572
Epoch: [3]  [1640/2522]  eta: 0:08:53  lr: 0.000034  min_lr: 0.000000  loss: 4.6647 (4.7414)  loss_scale: 65536.0000 (52975.9317)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0008  max mem: 15572
Epoch: [3]  [1650/2522]  eta: 0:08:47  lr: 0.000034  min_lr: 0.000000  loss: 4.5997 (4.7406)  loss_scale: 65536.0000 (53052.0073)  weight_decay: 0.0500 (0.0500)  time: 0.4978  data: 0.0009  max mem: 15572
Epoch: [3]  [1660/2522]  eta: 0:08:40  lr: 0.000034  min_lr: 0.000000  loss: 4.6620 (4.7408)  loss_scale: 65536.0000 (53127.1668)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0011  max mem: 15572
Epoch: [3]  [1670/2522]  eta: 0:08:34  lr: 0.000034  min_lr: 0.000000  loss: 4.7565 (4.7409)  loss_scale: 65536.0000 (53201.4267)  weight_decay: 0.0500 (0.0500)  time: 0.5187  data: 0.0239  max mem: 15572
Epoch: [3]  [1680/2522]  eta: 0:08:27  lr: 0.000034  min_lr: 0.000000  loss: 4.6784 (4.7407)  loss_scale: 65536.0000 (53274.8031)  weight_decay: 0.0500 (0.0500)  time: 0.4757  data: 0.0235  max mem: 15572
Epoch: [3]  [1690/2522]  eta: 0:08:20  lr: 0.000034  min_lr: 0.000000  loss: 4.7258 (4.7405)  loss_scale: 65536.0000 (53347.3116)  weight_decay: 0.0500 (0.0500)  time: 0.4236  data: 0.0005  max mem: 15572
Epoch: [3]  [1700/2522]  eta: 0:08:13  lr: 0.000034  min_lr: 0.000000  loss: 4.7615 (4.7404)  loss_scale: 65536.0000 (53418.9677)  weight_decay: 0.0500 (0.0500)  time: 0.4177  data: 0.0005  max mem: 15572
[2025-01-13 12:01:04,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:01:04,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:01:06,209] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9956
[2025-01-13 12:01:06,209] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:01:06,210] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1710/2522]  eta: 0:08:06  lr: 0.000034  min_lr: 0.000000  loss: 4.6461 (4.7399)  loss_scale: 65536.0000 (53642.9971)  weight_decay: 0.0500 (0.0500)  time: 0.4112  data: 0.0004  max mem: 15572
Epoch: [3]  [1720/2522]  eta: 0:07:59  lr: 0.000035  min_lr: 0.000000  loss: 4.6250 (4.7397)  loss_scale: 65536.0000 (53712.1023)  weight_decay: 0.0500 (0.0500)  time: 0.4026  data: 0.0005  max mem: 15572
Epoch: [3]  [1730/2522]  eta: 0:07:52  lr: 0.000035  min_lr: 0.000000  loss: 4.6520 (4.7389)  loss_scale: 65536.0000 (53780.4090)  weight_decay: 0.0500 (0.0500)  time: 0.3953  data: 0.0004  max mem: 15572
Epoch: [3]  [1740/2522]  eta: 0:07:45  lr: 0.000035  min_lr: 0.000000  loss: 4.6520 (4.7388)  loss_scale: 65536.0000 (53847.9311)  weight_decay: 0.0500 (0.0500)  time: 0.3917  data: 0.0003  max mem: 15572
[2025-01-13 12:01:23,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=57, lr=[3.355557120477974e-07, 3.355557120477974e-07, 4.793653029254249e-07, 4.793653029254249e-07, 6.8480757560775e-07, 6.8480757560775e-07, 9.782965365825e-07, 9.782965365825e-07, 1.397566480832143e-06, 1.397566480832143e-06, 1.9965235440459186e-06, 1.9965235440459186e-06, 2.8521764914941695e-06, 2.8521764914941695e-06, 4.074537844991672e-06, 4.074537844991672e-06, 5.820768349988102e-06, 5.820768349988102e-06, 8.31538335712586e-06, 8.31538335712586e-06, 1.1879119081608372e-05, 1.1879119081608372e-05, 1.6970170116583392e-05, 1.6970170116583392e-05, 2.4243100166547703e-05, 2.4243100166547703e-05, 3.463300023792529e-05, 3.463300023792529e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 12:01:23,046] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=27.151141690581774, CurrSamplesPerSec=32.784071878987554, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [1750/2522]  eta: 0:07:38  lr: 0.000035  min_lr: 0.000000  loss: 4.7133 (4.7387)  loss_scale: 65536.0000 (53914.6819)  weight_decay: 0.0500 (0.0500)  time: 0.3832  data: 0.0002  max mem: 15572
Epoch: [3]  [1760/2522]  eta: 0:07:32  lr: 0.000035  min_lr: 0.000000  loss: 4.7344 (4.7391)  loss_scale: 65536.0000 (53980.6746)  weight_decay: 0.0500 (0.0500)  time: 0.3797  data: 0.0003  max mem: 15572
Epoch: [3]  [1770/2522]  eta: 0:07:25  lr: 0.000035  min_lr: 0.000000  loss: 4.7912 (4.7393)  loss_scale: 65536.0000 (54045.9221)  weight_decay: 0.0500 (0.0500)  time: 0.3791  data: 0.0002  max mem: 15572
Epoch: [3]  [1780/2522]  eta: 0:07:18  lr: 0.000035  min_lr: 0.000000  loss: 4.6216 (4.7387)  loss_scale: 65536.0000 (54110.4368)  weight_decay: 0.0500 (0.0500)  time: 0.3796  data: 0.0002  max mem: 15572
Epoch: [3]  [1790/2522]  eta: 0:07:11  lr: 0.000035  min_lr: 0.000000  loss: 4.6976 (4.7387)  loss_scale: 65536.0000 (54174.2312)  weight_decay: 0.0500 (0.0500)  time: 0.3827  data: 0.0003  max mem: 15572
Epoch: [3]  [1800/2522]  eta: 0:07:04  lr: 0.000035  min_lr: 0.000000  loss: 4.7486 (4.7385)  loss_scale: 65536.0000 (54237.3170)  weight_decay: 0.0500 (0.0500)  time: 0.3820  data: 0.0002  max mem: 15572
Epoch: [3]  [1810/2522]  eta: 0:06:58  lr: 0.000035  min_lr: 0.000000  loss: 4.6667 (4.7379)  loss_scale: 65536.0000 (54299.7062)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0002  max mem: 15572
Epoch: [3]  [1820/2522]  eta: 0:06:51  lr: 0.000035  min_lr: 0.000000  loss: 4.6692 (4.7377)  loss_scale: 65536.0000 (54361.4102)  weight_decay: 0.0500 (0.0500)  time: 0.3863  data: 0.0003  max mem: 15572
Epoch: [3]  [1830/2522]  eta: 0:06:45  lr: 0.000035  min_lr: 0.000000  loss: 4.7329 (4.7378)  loss_scale: 65536.0000 (54422.4402)  weight_decay: 0.0500 (0.0500)  time: 0.3914  data: 0.0004  max mem: 15572
[2025-01-13 12:01:56,125] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:01:56,126] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1840/2522]  eta: 0:06:38  lr: 0.000035  min_lr: 0.000000  loss: 4.7583 (4.7378)  loss_scale: 65536.0000 (54660.7974)  weight_decay: 0.0500 (0.0500)  time: 0.3942  data: 0.0005  max mem: 15572
[2025-01-13 12:01:58,128] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10090
[2025-01-13 12:01:58,128] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:01:58,128] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1850/2522]  eta: 0:06:31  lr: 0.000035  min_lr: 0.000000  loss: 4.7852 (4.7383)  loss_scale: 65536.0000 (54719.5505)  weight_decay: 0.0500 (0.0500)  time: 0.3986  data: 0.0005  max mem: 15572
Epoch: [3]  [1860/2522]  eta: 0:06:25  lr: 0.000035  min_lr: 0.000000  loss: 4.7581 (4.7378)  loss_scale: 65536.0000 (54777.6722)  weight_decay: 0.0500 (0.0500)  time: 0.3995  data: 0.0005  max mem: 15572
Epoch: [3]  [1870/2522]  eta: 0:06:18  lr: 0.000035  min_lr: 0.000000  loss: 4.6161 (4.7373)  loss_scale: 65536.0000 (54835.1726)  weight_decay: 0.0500 (0.0500)  time: 0.3894  data: 0.0004  max mem: 15572
Epoch: [3]  [1880/2522]  eta: 0:06:12  lr: 0.000035  min_lr: 0.000000  loss: 4.6671 (4.7375)  loss_scale: 65536.0000 (54892.0617)  weight_decay: 0.0500 (0.0500)  time: 0.3857  data: 0.0004  max mem: 15572
Epoch: [3]  [1890/2522]  eta: 0:06:06  lr: 0.000035  min_lr: 0.000000  loss: 4.7105 (4.7371)  loss_scale: 65536.0000 (54948.3490)  weight_decay: 0.0500 (0.0500)  time: 0.3888  data: 0.0004  max mem: 15572
Epoch: [3]  [1900/2522]  eta: 0:05:59  lr: 0.000035  min_lr: 0.000000  loss: 4.7105 (4.7371)  loss_scale: 65536.0000 (55004.0442)  weight_decay: 0.0500 (0.0500)  time: 0.3891  data: 0.0003  max mem: 15572
Epoch: [3]  [1910/2522]  eta: 0:05:53  lr: 0.000035  min_lr: 0.000000  loss: 4.6653 (4.7364)  loss_scale: 65536.0000 (55059.1565)  weight_decay: 0.0500 (0.0500)  time: 0.3940  data: 0.0003  max mem: 15572
Epoch: [3]  [1920/2522]  eta: 0:05:46  lr: 0.000035  min_lr: 0.000000  loss: 4.7111 (4.7363)  loss_scale: 65536.0000 (55113.6950)  weight_decay: 0.0500 (0.0500)  time: 0.3950  data: 0.0004  max mem: 15572
Epoch: [3]  [1930/2522]  eta: 0:05:40  lr: 0.000035  min_lr: 0.000000  loss: 4.7146 (4.7361)  loss_scale: 65536.0000 (55167.6686)  weight_decay: 0.0500 (0.0500)  time: 0.3916  data: 0.0004  max mem: 15572
Epoch: [3]  [1940/2522]  eta: 0:05:34  lr: 0.000035  min_lr: 0.000000  loss: 4.7160 (4.7361)  loss_scale: 65536.0000 (55221.0860)  weight_decay: 0.0500 (0.0500)  time: 0.3923  data: 0.0004  max mem: 15572
Epoch: [3]  [1950/2522]  eta: 0:05:27  lr: 0.000035  min_lr: 0.000000  loss: 4.7510 (4.7361)  loss_scale: 65536.0000 (55273.9559)  weight_decay: 0.0500 (0.0500)  time: 0.3887  data: 0.0003  max mem: 15572
Epoch: [3]  [1960/2522]  eta: 0:05:21  lr: 0.000035  min_lr: 0.000000  loss: 4.7214 (4.7358)  loss_scale: 65536.0000 (55326.2866)  weight_decay: 0.0500 (0.0500)  time: 0.3814  data: 0.0002  max mem: 15572
[2025-01-13 12:02:48,367] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:02:48,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1970/2522]  eta: 0:05:15  lr: 0.000035  min_lr: 0.000000  loss: 4.7227 (4.7363)  loss_scale: 65536.0000 (55411.3364)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0001  max mem: 15572
Epoch: [3]  [1980/2522]  eta: 0:05:09  lr: 0.000035  min_lr: 0.000000  loss: 4.7707 (4.7360)  loss_scale: 131072.0000 (55793.2680)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0001  max mem: 15572
[2025-01-13 12:02:55,880] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10239
[2025-01-13 12:02:55,880] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:02:55,880] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1990/2522]  eta: 0:05:02  lr: 0.000036  min_lr: 0.000000  loss: 4.7125 (4.7355)  loss_scale: 131072.0000 (56138.4470)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
Epoch: [3]  [2000/2522]  eta: 0:04:56  lr: 0.000036  min_lr: 0.000000  loss: 4.7631 (4.7358)  loss_scale: 65536.0000 (56185.4113)  weight_decay: 0.0500 (0.0500)  time: 0.3818  data: 0.0002  max mem: 15572
Epoch: [3]  [2010/2522]  eta: 0:04:50  lr: 0.000036  min_lr: 0.000000  loss: 4.7307 (4.7358)  loss_scale: 65536.0000 (56231.9085)  weight_decay: 0.0500 (0.0500)  time: 0.3813  data: 0.0002  max mem: 15572
Epoch: [3]  [2020/2522]  eta: 0:04:44  lr: 0.000036  min_lr: 0.000000  loss: 4.6371 (4.7353)  loss_scale: 65536.0000 (56277.9456)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [3]  [2030/2522]  eta: 0:04:38  lr: 0.000036  min_lr: 0.000000  loss: 4.5687 (4.7345)  loss_scale: 65536.0000 (56323.5293)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [3]  [2040/2522]  eta: 0:04:32  lr: 0.000036  min_lr: 0.000000  loss: 4.6130 (4.7339)  loss_scale: 65536.0000 (56368.6663)  weight_decay: 0.0500 (0.0500)  time: 0.3827  data: 0.0003  max mem: 15572
Epoch: [3]  [2050/2522]  eta: 0:04:26  lr: 0.000036  min_lr: 0.000000  loss: 4.6526 (4.7340)  loss_scale: 65536.0000 (56413.3632)  weight_decay: 0.0500 (0.0500)  time: 0.3903  data: 0.0004  max mem: 15572
Epoch: [3]  [2060/2522]  eta: 0:04:20  lr: 0.000036  min_lr: 0.000000  loss: 4.7640 (4.7339)  loss_scale: 65536.0000 (56457.6264)  weight_decay: 0.0500 (0.0500)  time: 0.3962  data: 0.0004  max mem: 15572
Epoch: [3]  [2070/2522]  eta: 0:04:14  lr: 0.000036  min_lr: 0.000000  loss: 4.5755 (4.7331)  loss_scale: 65536.0000 (56501.4621)  weight_decay: 0.0500 (0.0500)  time: 0.3973  data: 0.0004  max mem: 15572
Epoch: [3]  [2080/2522]  eta: 0:04:08  lr: 0.000036  min_lr: 0.000000  loss: 4.5755 (4.7326)  loss_scale: 65536.0000 (56544.8765)  weight_decay: 0.0500 (0.0500)  time: 0.3964  data: 0.0004  max mem: 15572
Epoch: [3]  [2090/2522]  eta: 0:04:02  lr: 0.000036  min_lr: 0.000000  loss: 4.7087 (4.7324)  loss_scale: 65536.0000 (56587.8757)  weight_decay: 0.0500 (0.0500)  time: 0.3938  data: 0.0003  max mem: 15572
Epoch: [3]  [2100/2522]  eta: 0:03:56  lr: 0.000036  min_lr: 0.000000  loss: 4.8637 (4.7332)  loss_scale: 65536.0000 (56630.4655)  weight_decay: 0.0500 (0.0500)  time: 0.3792  data: 0.0003  max mem: 15572
Epoch: [3]  [2110/2522]  eta: 0:03:50  lr: 0.000036  min_lr: 0.000000  loss: 4.7930 (4.7332)  loss_scale: 65536.0000 (56672.6518)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
[2025-01-13 12:03:45,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:03:45,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2120/2522]  eta: 0:03:44  lr: 0.000036  min_lr: 0.000000  loss: 4.6186 (4.7327)  loss_scale: 65536.0000 (56776.2376)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0002  max mem: 15572
[2025-01-13 12:03:47,442] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10373
[2025-01-13 12:03:47,443] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:03:47,443] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2130/2522]  eta: 0:03:38  lr: 0.000036  min_lr: 0.000000  loss: 4.6186 (4.7323)  loss_scale: 65536.0000 (56909.6049)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0002  max mem: 15572
Epoch: [3]  [2140/2522]  eta: 0:03:32  lr: 0.000036  min_lr: 0.000000  loss: 4.7093 (4.7322)  loss_scale: 65536.0000 (56949.8963)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0003  max mem: 15572
Epoch: [3]  [2150/2522]  eta: 0:03:26  lr: 0.000036  min_lr: 0.000000  loss: 4.7618 (4.7320)  loss_scale: 65536.0000 (56989.8131)  weight_decay: 0.0500 (0.0500)  time: 0.3800  data: 0.0002  max mem: 15572
Epoch: [3]  [2160/2522]  eta: 0:03:20  lr: 0.000036  min_lr: 0.000000  loss: 4.7516 (4.7315)  loss_scale: 65536.0000 (57029.3605)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0001  max mem: 15572
Epoch: [3]  [2170/2522]  eta: 0:03:14  lr: 0.000036  min_lr: 0.000000  loss: 4.6028 (4.7309)  loss_scale: 65536.0000 (57068.5435)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
Epoch: [3]  [2180/2522]  eta: 0:03:09  lr: 0.000036  min_lr: 0.000000  loss: 4.5801 (4.7304)  loss_scale: 65536.0000 (57107.3673)  weight_decay: 0.0500 (0.0500)  time: 0.3830  data: 0.0002  max mem: 15572
Epoch: [3]  [2190/2522]  eta: 0:03:03  lr: 0.000036  min_lr: 0.000000  loss: 4.6049 (4.7304)  loss_scale: 65536.0000 (57145.8366)  weight_decay: 0.0500 (0.0500)  time: 0.3872  data: 0.0003  max mem: 15572
Epoch: [3]  [2200/2522]  eta: 0:02:57  lr: 0.000036  min_lr: 0.000000  loss: 4.6574 (4.7301)  loss_scale: 65536.0000 (57183.9564)  weight_decay: 0.0500 (0.0500)  time: 0.3936  data: 0.0003  max mem: 15572
Epoch: [3]  [2210/2522]  eta: 0:02:51  lr: 0.000036  min_lr: 0.000000  loss: 4.6188 (4.7294)  loss_scale: 65536.0000 (57221.7313)  weight_decay: 0.0500 (0.0500)  time: 0.3968  data: 0.0004  max mem: 15572
Epoch: [3]  [2220/2522]  eta: 0:02:46  lr: 0.000036  min_lr: 0.000000  loss: 4.6188 (4.7291)  loss_scale: 65536.0000 (57259.1661)  weight_decay: 0.0500 (0.0500)  time: 0.3938  data: 0.0006  max mem: 15572
Epoch: [3]  [2230/2522]  eta: 0:02:40  lr: 0.000036  min_lr: 0.000000  loss: 4.8160 (4.7295)  loss_scale: 65536.0000 (57296.2654)  weight_decay: 0.0500 (0.0500)  time: 0.3938  data: 0.0006  max mem: 15572
Epoch: [3]  [2240/2522]  eta: 0:02:34  lr: 0.000036  min_lr: 0.000000  loss: 4.6808 (4.7293)  loss_scale: 65536.0000 (57333.0335)  weight_decay: 0.0500 (0.0500)  time: 0.3831  data: 0.0003  max mem: 15572
Epoch: [3]  [2250/2522]  eta: 0:02:29  lr: 0.000036  min_lr: 0.000000  loss: 4.6367 (4.7291)  loss_scale: 65536.0000 (57369.4749)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
[2025-01-13 12:04:37,008] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:04:37,008] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:04:37,771] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10504
[2025-01-13 12:04:37,771] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:04:37,771] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2260/2522]  eta: 0:02:23  lr: 0.000037  min_lr: 0.000000  loss: 4.6822 (4.7292)  loss_scale: 65536.0000 (57463.5648)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [3]  [2270/2522]  eta: 0:02:17  lr: 0.000037  min_lr: 0.000000  loss: 4.6976 (4.7286)  loss_scale: 65536.0000 (57499.1105)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0002  max mem: 15572
Epoch: [3]  [2280/2522]  eta: 0:02:12  lr: 0.000037  min_lr: 0.000000  loss: 4.6976 (4.7287)  loss_scale: 65536.0000 (57534.3446)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [3]  [2290/2522]  eta: 0:02:06  lr: 0.000037  min_lr: 0.000000  loss: 4.7043 (4.7283)  loss_scale: 65536.0000 (57569.2711)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [3]  [2300/2522]  eta: 0:02:00  lr: 0.000037  min_lr: 0.000000  loss: 4.7371 (4.7287)  loss_scale: 65536.0000 (57603.8940)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [3]  [2310/2522]  eta: 0:01:55  lr: 0.000037  min_lr: 0.000000  loss: 4.7371 (4.7284)  loss_scale: 65536.0000 (57638.2172)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [3]  [2320/2522]  eta: 0:01:49  lr: 0.000037  min_lr: 0.000000  loss: 4.6399 (4.7278)  loss_scale: 65536.0000 (57672.2447)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [3]  [2330/2522]  eta: 0:01:44  lr: 0.000037  min_lr: 0.000000  loss: 4.7248 (4.7279)  loss_scale: 65536.0000 (57705.9803)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0003  max mem: 15572
Epoch: [3]  [2340/2522]  eta: 0:01:38  lr: 0.000037  min_lr: 0.000000  loss: 4.5970 (4.7270)  loss_scale: 65536.0000 (57739.4276)  weight_decay: 0.0500 (0.0500)  time: 0.3896  data: 0.0005  max mem: 15572
Epoch: [3]  [2350/2522]  eta: 0:01:33  lr: 0.000037  min_lr: 0.000000  loss: 4.4692 (4.7265)  loss_scale: 65536.0000 (57772.5904)  weight_decay: 0.0500 (0.0500)  time: 0.3923  data: 0.0006  max mem: 15572
Epoch: [3]  [2360/2522]  eta: 0:01:27  lr: 0.000037  min_lr: 0.000000  loss: 4.6513 (4.7266)  loss_scale: 65536.0000 (57805.4723)  weight_decay: 0.0500 (0.0500)  time: 0.3926  data: 0.0005  max mem: 15572
Epoch: [3]  [2370/2522]  eta: 0:01:22  lr: 0.000037  min_lr: 0.000000  loss: 4.6378 (4.7260)  loss_scale: 65536.0000 (57838.0768)  weight_decay: 0.0500 (0.0500)  time: 0.3901  data: 0.0004  max mem: 15572
Epoch: [3]  [2380/2522]  eta: 0:01:16  lr: 0.000037  min_lr: 0.000000  loss: 4.6357 (4.7261)  loss_scale: 65536.0000 (57870.4074)  weight_decay: 0.0500 (0.0500)  time: 0.3894  data: 0.0004  max mem: 15572
[2025-01-13 12:05:27,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:05:27,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2390/2522]  eta: 0:01:11  lr: 0.000037  min_lr: 0.000000  loss: 4.7484 (4.7259)  loss_scale: 65536.0000 (58094.3338)  weight_decay: 0.0500 (0.0500)  time: 0.3821  data: 0.0003  max mem: 15572
Epoch: [3]  [2400/2522]  eta: 0:01:05  lr: 0.000037  min_lr: 0.000000  loss: 4.7522 (4.7258)  loss_scale: 131072.0000 (58398.2807)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
[2025-01-13 12:05:36,398] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10658
[2025-01-13 12:05:36,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:05:36,398] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2410/2522]  eta: 0:01:00  lr: 0.000037  min_lr: 0.000000  loss: 4.5971 (4.7252)  loss_scale: 131072.0000 (58645.3422)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0002  max mem: 15572
Epoch: [3]  [2420/2522]  eta: 0:00:54  lr: 0.000037  min_lr: 0.000000  loss: 4.6039 (4.7250)  loss_scale: 65536.0000 (58673.8042)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0002  max mem: 15572
Epoch: [3]  [2430/2522]  eta: 0:00:49  lr: 0.000037  min_lr: 0.000000  loss: 4.6737 (4.7252)  loss_scale: 65536.0000 (58702.0321)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0001  max mem: 15572
Epoch: [3]  [2440/2522]  eta: 0:00:43  lr: 0.000037  min_lr: 0.000000  loss: 4.7481 (4.7254)  loss_scale: 65536.0000 (58730.0287)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0002  max mem: 15572
Epoch: [3]  [2450/2522]  eta: 0:00:38  lr: 0.000037  min_lr: 0.000000  loss: 4.7607 (4.7255)  loss_scale: 65536.0000 (58757.7968)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0002  max mem: 15572
Epoch: [3]  [2460/2522]  eta: 0:00:33  lr: 0.000037  min_lr: 0.000000  loss: 4.7482 (4.7253)  loss_scale: 65536.0000 (58785.3393)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [3]  [2470/2522]  eta: 0:00:27  lr: 0.000037  min_lr: 0.000000  loss: 4.6764 (4.7249)  loss_scale: 65536.0000 (58812.6588)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0002  max mem: 15572
Epoch: [3]  [2480/2522]  eta: 0:00:22  lr: 0.000037  min_lr: 0.000000  loss: 4.6764 (4.7247)  loss_scale: 65536.0000 (58839.7582)  weight_decay: 0.0500 (0.0500)  time: 0.3888  data: 0.0003  max mem: 15572
Epoch: [3]  [2490/2522]  eta: 0:00:17  lr: 0.000037  min_lr: 0.000000  loss: 4.6841 (4.7246)  loss_scale: 65536.0000 (58866.6399)  weight_decay: 0.0500 (0.0500)  time: 0.3930  data: 0.0004  max mem: 15572
Epoch: [3]  [2500/2522]  eta: 0:00:11  lr: 0.000037  min_lr: 0.000000  loss: 4.7073 (4.7243)  loss_scale: 65536.0000 (58893.3067)  weight_decay: 0.0500 (0.0500)  time: 0.3940  data: 0.0004  max mem: 15572
Epoch: [3]  [2510/2522]  eta: 0:00:06  lr: 0.000037  min_lr: 0.000000  loss: 4.5987 (4.7239)  loss_scale: 65536.0000 (58919.7611)  weight_decay: 0.0500 (0.0500)  time: 0.3927  data: 0.0004  max mem: 15572
Epoch: [3]  [2520/2522]  eta: 0:00:01  lr: 0.000037  min_lr: 0.000000  loss: 4.6255 (4.7237)  loss_scale: 65536.0000 (58946.0056)  weight_decay: 0.0500 (0.0500)  time: 0.3823  data: 0.0003  max mem: 15572
Epoch: [3]  [2521/2522]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000000  loss: 4.6255 (4.7237)  loss_scale: 65536.0000 (58948.6186)  weight_decay: 0.0500 (0.0500)  time: 0.3818  data: 0.0003  max mem: 15572
Epoch: [3] Total time: 0:22:18 (0.5306 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000000  loss: 4.6255 (4.7237)  loss_scale: 65536.0000 (58948.6186)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 2997
Indices to remove: tensor([    2,     4,     5,  ..., 33674, 33698, 33705], device='cuda:0')
length of data loader train is: 2272
num_training_steps_per_epoch is: 2272
Change step level LR scheduler!
Set warmup steps = 11360
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:09:31  loss: 2.7996 (2.7996)  acc1: 44.4444 (44.4444)  acc5: 94.4444 (94.4444)  time: 2.1018  data: 1.9145  max mem: 15572
Val:  [ 10/272]  eta: 0:01:50  loss: 5.1393 (4.5395)  acc1: 0.0000 (14.6465)  acc5: 0.0000 (29.2929)  time: 0.4202  data: 0.2620  max mem: 15572
Val:  [ 20/272]  eta: 0:01:15  loss: 4.4769 (4.4028)  acc1: 0.0000 (11.6402)  acc5: 11.1111 (26.4550)  time: 0.2075  data: 0.0486  max mem: 15572
Val:  [ 30/272]  eta: 0:01:01  loss: 4.4644 (4.4072)  acc1: 0.0000 (8.2437)  acc5: 16.6667 (24.0143)  time: 0.1620  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:53  loss: 4.1329 (4.2698)  acc1: 0.0000 (9.6206)  acc5: 22.2222 (31.0298)  time: 0.1578  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:48  loss: 3.6949 (4.2373)  acc1: 0.0000 (7.8431)  acc5: 66.6667 (34.0959)  time: 0.1594  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:44  loss: 3.5519 (4.1570)  acc1: 0.0000 (9.5628)  acc5: 61.1111 (38.1603)  time: 0.1700  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:41  loss: 3.6558 (4.0977)  acc1: 16.6667 (10.1721)  acc5: 61.1111 (38.1847)  time: 0.1689  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:38  loss: 3.8368 (4.0922)  acc1: 0.0000 (12.0027)  acc5: 27.7778 (37.9287)  time: 0.1650  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:35  loss: 4.7509 (4.1750)  acc1: 0.0000 (10.6838)  acc5: 0.0000 (33.7607)  time: 0.1687  data: 0.0052  max mem: 15572
Val:  [100/272]  eta: 0:00:33  loss: 4.7509 (4.2328)  acc1: 0.0000 (10.2860)  acc5: 0.0000 (31.9032)  time: 0.1854  data: 0.0291  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 4.7687 (4.2927)  acc1: 0.0000 (9.3594)  acc5: 0.0000 (29.8298)  time: 0.1815  data: 0.0260  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 4.8414 (4.3473)  acc1: 0.0000 (8.5859)  acc5: 0.0000 (27.5482)  time: 0.1643  data: 0.0021  max mem: 15572
Val:  [130/272]  eta: 0:00:26  loss: 4.7418 (4.2918)  acc1: 0.0000 (9.7116)  acc5: 0.0000 (29.7710)  time: 0.1632  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:24  loss: 3.9832 (4.2649)  acc1: 0.0000 (10.9929)  acc5: 33.3333 (30.2600)  time: 0.1592  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:22  loss: 4.2804 (4.2767)  acc1: 0.0000 (10.2649)  acc5: 11.1111 (28.8079)  time: 0.1587  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:20  loss: 4.2521 (4.2652)  acc1: 0.0000 (10.7660)  acc5: 16.6667 (29.6411)  time: 0.1594  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:18  loss: 4.3822 (4.2934)  acc1: 0.0000 (10.2664)  acc5: 22.2222 (28.8174)  time: 0.1555  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 4.5920 (4.2981)  acc1: 0.0000 (9.6992)  acc5: 0.0000 (27.4095)  time: 0.1533  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:14  loss: 4.7741 (4.3226)  acc1: 0.0000 (9.1914)  acc5: 0.0000 (26.1780)  time: 0.1867  data: 0.0236  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 4.7539 (4.3236)  acc1: 0.0000 (8.8447)  acc5: 5.5556 (26.9486)  time: 0.2008  data: 0.0275  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 4.1365 (4.3397)  acc1: 0.0000 (8.7941)  acc5: 27.7778 (26.3823)  time: 0.1860  data: 0.0043  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 4.4581 (4.3407)  acc1: 0.0000 (8.5219)  acc5: 0.0000 (26.1941)  time: 0.1762  data: 0.0005  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 4.2558 (4.3361)  acc1: 5.5556 (8.8985)  acc5: 27.7778 (26.7917)  time: 0.1701  data: 0.0006  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 4.1238 (4.3278)  acc1: 11.1111 (8.8520)  acc5: 44.4444 (27.3628)  time: 0.1722  data: 0.0006  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 4.3331 (4.3510)  acc1: 0.0000 (8.4993)  acc5: 5.5556 (26.4719)  time: 0.1659  data: 0.0006  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 4.2988 (4.3011)  acc1: 0.0000 (9.2380)  acc5: 38.8889 (28.5441)  time: 0.1596  data: 0.0004  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.6943 (4.3006)  acc1: 11.1111 (9.2866)  acc5: 61.1111 (28.9463)  time: 0.1478  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.6943 (4.3030)  acc1: 11.1111 (9.2771)  acc5: 61.1111 (28.9166)  time: 0.1425  data: 0.0002  max mem: 15572
Val: Total time: 0:00:48 (0.1777 s / it)
* Acc@1 9.277 Acc@5 28.917 loss 4.303
Accuracy of the network on the 4883 val videos: 9.3%
[2025-01-13 12:07:08,383] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 12:07:08,386] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 12:07:08,386] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 12:07:10,956] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 12:07:10,956] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 9.28%
Epoch: [4]  [   0/2272]  eta: 2:45:12  lr: 0.000038  min_lr: 0.000000  loss: 4.8511 (4.8511)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.3628  data: 3.9723  max mem: 15572
Epoch: [4]  [  10/2272]  eta: 0:28:10  lr: 0.000038  min_lr: 0.000000  loss: 4.7224 (4.6835)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7474  data: 0.3614  max mem: 15572
[2025-01-13 12:07:21,448] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:07:21,448] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:07:21,814] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10788
[2025-01-13 12:07:21,814] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:07:21,814] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [  20/2272]  eta: 0:21:24  lr: 0.000038  min_lr: 0.000000  loss: 4.7095 (4.6669)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.3808  data: 0.0003  max mem: 15572
Epoch: [4]  [  30/2272]  eta: 0:18:56  lr: 0.000038  min_lr: 0.000000  loss: 4.6500 (4.6413)  loss_scale: 65536.0000 (67650.0645)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [4]  [  40/2272]  eta: 0:17:37  lr: 0.000038  min_lr: 0.000000  loss: 4.6789 (4.6432)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [4]  [  50/2272]  eta: 0:16:49  lr: 0.000038  min_lr: 0.000000  loss: 4.5856 (4.6244)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [4]  [  60/2272]  eta: 0:16:15  lr: 0.000038  min_lr: 0.000000  loss: 4.5856 (4.6290)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [4]  [  70/2272]  eta: 0:15:49  lr: 0.000038  min_lr: 0.000000  loss: 4.5903 (4.6116)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [4]  [  80/2272]  eta: 0:15:30  lr: 0.000038  min_lr: 0.000000  loss: 4.6199 (4.6224)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [4]  [  90/2272]  eta: 0:15:19  lr: 0.000038  min_lr: 0.000000  loss: 4.7217 (4.6361)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.3856  data: 0.0004  max mem: 15572
Epoch: [4]  [ 100/2272]  eta: 0:15:08  lr: 0.000038  min_lr: 0.000000  loss: 4.6404 (4.6290)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.3926  data: 0.0005  max mem: 15572
Epoch: [4]  [ 110/2272]  eta: 0:15:00  lr: 0.000038  min_lr: 0.000000  loss: 4.6636 (4.6317)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.3951  data: 0.0004  max mem: 15572
Epoch: [4]  [ 120/2272]  eta: 0:14:52  lr: 0.000038  min_lr: 0.000000  loss: 4.6841 (4.6393)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.3976  data: 0.0004  max mem: 15572
Epoch: [4]  [ 130/2272]  eta: 0:14:46  lr: 0.000038  min_lr: 0.000000  loss: 4.6841 (4.6389)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.3982  data: 0.0005  max mem: 15572
Epoch: [4]  [ 140/2272]  eta: 0:14:38  lr: 0.000038  min_lr: 0.000000  loss: 4.6202 (4.6432)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.3952  data: 0.0005  max mem: 15572
[2025-01-13 12:08:11,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:08:11,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:08:12,950] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10921
[2025-01-13 12:08:12,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:08:12,951] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 150/2272]  eta: 0:14:30  lr: 0.000038  min_lr: 0.000000  loss: 4.6202 (4.6405)  loss_scale: 65536.0000 (67706.0662)  weight_decay: 0.0500 (0.0500)  time: 0.3881  data: 0.0004  max mem: 15572
Epoch: [4]  [ 160/2272]  eta: 0:14:22  lr: 0.000038  min_lr: 0.000000  loss: 4.5783 (4.6400)  loss_scale: 65536.0000 (67571.2795)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0002  max mem: 15572
Epoch: [4]  [ 170/2272]  eta: 0:14:14  lr: 0.000038  min_lr: 0.000000  loss: 4.5760 (4.6366)  loss_scale: 65536.0000 (67452.2573)  weight_decay: 0.0500 (0.0500)  time: 0.3781  data: 0.0002  max mem: 15572
Epoch: [4]  [ 180/2272]  eta: 0:14:07  lr: 0.000038  min_lr: 0.000000  loss: 4.5977 (4.6418)  loss_scale: 65536.0000 (67346.3867)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0002  max mem: 15572
Epoch: [4]  [ 190/2272]  eta: 0:13:59  lr: 0.000038  min_lr: 0.000000  loss: 4.6156 (4.6405)  loss_scale: 65536.0000 (67251.6021)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [4]  [ 200/2272]  eta: 0:13:52  lr: 0.000038  min_lr: 0.000000  loss: 4.6781 (4.6479)  loss_scale: 65536.0000 (67166.2488)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [4]  [ 210/2272]  eta: 0:13:45  lr: 0.000038  min_lr: 0.000000  loss: 4.6963 (4.6463)  loss_scale: 65536.0000 (67088.9858)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [4]  [ 220/2272]  eta: 0:13:39  lr: 0.000038  min_lr: 0.000000  loss: 4.6397 (4.6457)  loss_scale: 65536.0000 (67018.7149)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
[2025-01-13 12:08:42,243] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=64, lr=[3.7248190626029385e-07, 3.7248190626029385e-07, 5.32117008943277e-07, 5.32117008943277e-07, 7.601671556332529e-07, 7.601671556332529e-07, 1.0859530794760755e-06, 1.0859530794760755e-06, 1.5513615421086795e-06, 1.5513615421086795e-06, 2.216230774440971e-06, 2.216230774440971e-06, 3.1660439634871013e-06, 3.1660439634871013e-06, 4.522919947838717e-06, 4.522919947838717e-06, 6.461314211198166e-06, 6.461314211198166e-06, 9.23044887314024e-06, 9.23044887314024e-06, 1.3186355533057485e-05, 1.3186355533057485e-05, 1.8837650761510694e-05, 1.8837650761510694e-05, 2.6910929659300993e-05, 2.6910929659300993e-05, 3.844418522757285e-05, 3.844418522757285e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 12:08:42,244] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=27.600614969987188, CurrSamplesPerSec=34.10939180466185, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [ 230/2272]  eta: 0:13:33  lr: 0.000038  min_lr: 0.000000  loss: 4.7075 (4.6496)  loss_scale: 65536.0000 (66954.5281)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0002  max mem: 15572
Epoch: [4]  [ 240/2272]  eta: 0:13:29  lr: 0.000038  min_lr: 0.000000  loss: 4.6043 (4.6454)  loss_scale: 65536.0000 (66895.6680)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0004  max mem: 15572
Epoch: [4]  [ 250/2272]  eta: 0:13:25  lr: 0.000039  min_lr: 0.000000  loss: 4.5956 (4.6442)  loss_scale: 65536.0000 (66841.4980)  weight_decay: 0.0500 (0.0500)  time: 0.3966  data: 0.0005  max mem: 15572
Epoch: [4]  [ 260/2272]  eta: 0:13:20  lr: 0.000039  min_lr: 0.000000  loss: 4.6256 (4.6423)  loss_scale: 65536.0000 (66791.4789)  weight_decay: 0.0500 (0.0500)  time: 0.3922  data: 0.0005  max mem: 15572
Epoch: [4]  [ 270/2272]  eta: 0:13:16  lr: 0.000039  min_lr: 0.000000  loss: 4.6276 (4.6427)  loss_scale: 65536.0000 (66745.1513)  weight_decay: 0.0500 (0.0500)  time: 0.3935  data: 0.0005  max mem: 15572
[2025-01-13 12:09:02,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:09:02,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 280/2272]  eta: 0:13:12  lr: 0.000039  min_lr: 0.000000  loss: 4.6527 (4.6442)  loss_scale: 65536.0000 (67168.5694)  weight_decay: 0.0500 (0.0500)  time: 0.3942  data: 0.0005  max mem: 15572
[2025-01-13 12:09:05,553] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11058
[2025-01-13 12:09:05,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:09:05,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 290/2272]  eta: 0:13:08  lr: 0.000039  min_lr: 0.000000  loss: 4.6162 (4.6427)  loss_scale: 65536.0000 (68463.7251)  weight_decay: 0.0500 (0.0500)  time: 0.3945  data: 0.0005  max mem: 15572
Epoch: [4]  [ 300/2272]  eta: 0:13:02  lr: 0.000039  min_lr: 0.000000  loss: 4.6138 (4.6410)  loss_scale: 65536.0000 (68366.4585)  weight_decay: 0.0500 (0.0500)  time: 0.3848  data: 0.0004  max mem: 15572
Epoch: [4]  [ 310/2272]  eta: 0:12:57  lr: 0.000039  min_lr: 0.000000  loss: 4.6220 (4.6408)  loss_scale: 65536.0000 (68275.4469)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0002  max mem: 15572
Epoch: [4]  [ 320/2272]  eta: 0:12:52  lr: 0.000039  min_lr: 0.000000  loss: 4.6331 (4.6390)  loss_scale: 65536.0000 (68190.1059)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0003  max mem: 15572
Epoch: [4]  [ 330/2272]  eta: 0:12:46  lr: 0.000039  min_lr: 0.000000  loss: 4.6331 (4.6396)  loss_scale: 65536.0000 (68109.9215)  weight_decay: 0.0500 (0.0500)  time: 0.3722  data: 0.0003  max mem: 15572
Epoch: [4]  [ 340/2272]  eta: 0:12:41  lr: 0.000039  min_lr: 0.000000  loss: 4.6979 (4.6414)  loss_scale: 65536.0000 (68034.4399)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
Epoch: [4]  [ 350/2272]  eta: 0:12:36  lr: 0.000039  min_lr: 0.000000  loss: 4.7231 (4.6400)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [4]  [ 360/2272]  eta: 0:12:31  lr: 0.000039  min_lr: 0.000000  loss: 4.5887 (4.6386)  loss_scale: 65536.0000 (67896.0222)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [4]  [ 370/2272]  eta: 0:12:27  lr: 0.000039  min_lr: 0.000000  loss: 4.6276 (4.6386)  loss_scale: 65536.0000 (67832.4097)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [4]  [ 380/2272]  eta: 0:12:22  lr: 0.000039  min_lr: 0.000000  loss: 4.6029 (4.6373)  loss_scale: 65536.0000 (67772.1365)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
Epoch: [4]  [ 390/2272]  eta: 0:12:18  lr: 0.000039  min_lr: 0.000000  loss: 4.5852 (4.6360)  loss_scale: 65536.0000 (67714.9463)  weight_decay: 0.0500 (0.0500)  time: 0.3846  data: 0.0003  max mem: 15572
Epoch: [4]  [ 400/2272]  eta: 0:12:14  lr: 0.000039  min_lr: 0.000000  loss: 4.5604 (4.6320)  loss_scale: 65536.0000 (67660.6085)  weight_decay: 0.0500 (0.0500)  time: 0.3939  data: 0.0004  max mem: 15572
Epoch: [4]  [ 410/2272]  eta: 0:12:11  lr: 0.000039  min_lr: 0.000000  loss: 4.6053 (4.6326)  loss_scale: 65536.0000 (67608.9148)  weight_decay: 0.0500 (0.0500)  time: 0.4066  data: 0.0004  max mem: 15572
[2025-01-13 12:09:54,969] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:09:54,970] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:09:56,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11190
[2025-01-13 12:09:56,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:09:56,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 420/2272]  eta: 0:12:08  lr: 0.000039  min_lr: 0.000000  loss: 4.7264 (4.6355)  loss_scale: 65536.0000 (68026.6793)  weight_decay: 0.0500 (0.0500)  time: 0.4103  data: 0.0005  max mem: 15572
Epoch: [4]  [ 430/2272]  eta: 0:12:04  lr: 0.000039  min_lr: 0.000000  loss: 4.7418 (4.6371)  loss_scale: 65536.0000 (67968.8910)  weight_decay: 0.0500 (0.0500)  time: 0.4006  data: 0.0005  max mem: 15572
Epoch: [4]  [ 440/2272]  eta: 0:12:00  lr: 0.000039  min_lr: 0.000000  loss: 4.6441 (4.6363)  loss_scale: 65536.0000 (67913.7234)  weight_decay: 0.0500 (0.0500)  time: 0.3936  data: 0.0004  max mem: 15572
Epoch: [4]  [ 450/2272]  eta: 0:11:55  lr: 0.000039  min_lr: 0.000000  loss: 4.6214 (4.6326)  loss_scale: 65536.0000 (67861.0022)  weight_decay: 0.0500 (0.0500)  time: 0.3823  data: 0.0003  max mem: 15572
[2025-01-13 12:10:09,703] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11225
[2025-01-13 12:10:09,703] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 12:10:09,703] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [ 460/2272]  eta: 0:11:51  lr: 0.000039  min_lr: 0.000000  loss: 4.5894 (4.6338)  loss_scale: 65536.0000 (67313.0065)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [4]  [ 470/2272]  eta: 0:11:46  lr: 0.000039  min_lr: 0.000000  loss: 4.6250 (4.6327)  loss_scale: 32768.0000 (66579.5669)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [4]  [ 480/2272]  eta: 0:11:41  lr: 0.000039  min_lr: 0.000000  loss: 4.6330 (4.6310)  loss_scale: 32768.0000 (65876.6237)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [4]  [ 490/2272]  eta: 0:11:37  lr: 0.000040  min_lr: 0.000000  loss: 4.6362 (4.6317)  loss_scale: 32768.0000 (65202.3136)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0002  max mem: 15572
Epoch: [4]  [ 500/2272]  eta: 0:11:33  lr: 0.000040  min_lr: 0.000000  loss: 4.6096 (4.6293)  loss_scale: 32768.0000 (64554.9222)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0002  max mem: 15572
Epoch: [4]  [ 510/2272]  eta: 0:11:28  lr: 0.000040  min_lr: 0.000000  loss: 4.4739 (4.6253)  loss_scale: 32768.0000 (63932.8689)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [4]  [ 520/2272]  eta: 0:11:24  lr: 0.000040  min_lr: 0.000000  loss: 4.6385 (4.6284)  loss_scale: 32768.0000 (63334.6948)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [4]  [ 530/2272]  eta: 0:11:19  lr: 0.000040  min_lr: 0.000000  loss: 4.7192 (4.6306)  loss_scale: 32768.0000 (62759.0508)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [4]  [ 540/2272]  eta: 0:11:15  lr: 0.000040  min_lr: 0.000000  loss: 4.6827 (4.6316)  loss_scale: 32768.0000 (62204.6876)  weight_decay: 0.0500 (0.0500)  time: 0.3859  data: 0.0003  max mem: 15572
Epoch: [4]  [ 550/2272]  eta: 0:11:12  lr: 0.000040  min_lr: 0.000000  loss: 4.6819 (4.6315)  loss_scale: 32768.0000 (61670.4465)  weight_decay: 0.0500 (0.0500)  time: 0.3981  data: 0.0004  max mem: 15572
Epoch: [4]  [ 560/2272]  eta: 0:11:08  lr: 0.000040  min_lr: 0.000000  loss: 4.5694 (4.6306)  loss_scale: 32768.0000 (61155.2513)  weight_decay: 0.0500 (0.0500)  time: 0.3971  data: 0.0004  max mem: 15572
Epoch: [4]  [ 570/2272]  eta: 0:11:04  lr: 0.000040  min_lr: 0.000000  loss: 4.5817 (4.6313)  loss_scale: 32768.0000 (60658.1016)  weight_decay: 0.0500 (0.0500)  time: 0.3935  data: 0.0005  max mem: 15572
Epoch: [4]  [ 580/2272]  eta: 0:11:00  lr: 0.000040  min_lr: 0.000000  loss: 4.5837 (4.6327)  loss_scale: 32768.0000 (60178.0654)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0005  max mem: 15572
[2025-01-13 12:10:59,083] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:10:59,084] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [ 590/2272]  eta: 0:10:56  lr: 0.000040  min_lr: 0.000000  loss: 4.7569 (4.6331)  loss_scale: 32768.0000 (60157.8342)  weight_decay: 0.0500 (0.0500)  time: 0.3914  data: 0.0005  max mem: 15572
Epoch: [4]  [ 600/2272]  eta: 0:10:52  lr: 0.000040  min_lr: 0.000000  loss: 4.6852 (4.6322)  loss_scale: 65536.0000 (60247.3211)  weight_decay: 0.0500 (0.0500)  time: 0.3882  data: 0.0003  max mem: 15572
Epoch: [4]  [ 610/2272]  eta: 0:10:48  lr: 0.000040  min_lr: 0.000000  loss: 4.6669 (4.6339)  loss_scale: 65536.0000 (60333.8789)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0002  max mem: 15572
Epoch: [4]  [ 620/2272]  eta: 0:10:44  lr: 0.000040  min_lr: 0.000000  loss: 4.6169 (4.6322)  loss_scale: 65536.0000 (60417.6490)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [4]  [ 630/2272]  eta: 0:10:39  lr: 0.000040  min_lr: 0.000000  loss: 4.5726 (4.6325)  loss_scale: 65536.0000 (60498.7639)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0002  max mem: 15572
Epoch: [4]  [ 640/2272]  eta: 0:10:35  lr: 0.000040  min_lr: 0.000000  loss: 4.6063 (4.6323)  loss_scale: 65536.0000 (60577.3479)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [4]  [ 650/2272]  eta: 0:10:31  lr: 0.000040  min_lr: 0.000000  loss: 4.6304 (4.6333)  loss_scale: 65536.0000 (60653.5177)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [4]  [ 660/2272]  eta: 0:10:27  lr: 0.000040  min_lr: 0.000000  loss: 4.6585 (4.6336)  loss_scale: 65536.0000 (60727.3828)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0002  max mem: 15572
Epoch: [4]  [ 670/2272]  eta: 0:10:22  lr: 0.000040  min_lr: 0.000000  loss: 4.6133 (4.6334)  loss_scale: 65536.0000 (60799.0462)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0002  max mem: 15572
Epoch: [4]  [ 680/2272]  eta: 0:10:18  lr: 0.000040  min_lr: 0.000000  loss: 4.6211 (4.6333)  loss_scale: 65536.0000 (60868.6050)  weight_decay: 0.0500 (0.0500)  time: 0.3812  data: 0.0002  max mem: 15572
Epoch: [4]  [ 690/2272]  eta: 0:10:15  lr: 0.000040  min_lr: 0.000000  loss: 4.6690 (4.6338)  loss_scale: 65536.0000 (60936.1505)  weight_decay: 0.0500 (0.0500)  time: 0.3919  data: 0.0003  max mem: 15572
Epoch: [4]  [ 700/2272]  eta: 0:10:11  lr: 0.000040  min_lr: 0.000000  loss: 4.6742 (4.6343)  loss_scale: 65536.0000 (61001.7689)  weight_decay: 0.0500 (0.0500)  time: 0.3995  data: 0.0004  max mem: 15572
Epoch: [4]  [ 710/2272]  eta: 0:10:07  lr: 0.000040  min_lr: 0.000000  loss: 4.6558 (4.6332)  loss_scale: 65536.0000 (61065.5415)  weight_decay: 0.0500 (0.0500)  time: 0.3972  data: 0.0004  max mem: 15572
[2025-01-13 12:11:48,171] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:11:48,171] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:11:50,184] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11487
[2025-01-13 12:11:50,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:11:50,184] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 720/2272]  eta: 0:10:04  lr: 0.000040  min_lr: 0.000000  loss: 4.6808 (4.6348)  loss_scale: 65536.0000 (61582.0250)  weight_decay: 0.0500 (0.0500)  time: 0.3975  data: 0.0005  max mem: 15572
Epoch: [4]  [ 730/2272]  eta: 0:10:00  lr: 0.000041  min_lr: 0.000000  loss: 4.7423 (4.6366)  loss_scale: 65536.0000 (61636.1149)  weight_decay: 0.0500 (0.0500)  time: 0.3926  data: 0.0005  max mem: 15572
Epoch: [4]  [ 740/2272]  eta: 0:09:56  lr: 0.000041  min_lr: 0.000000  loss: 4.6504 (4.6355)  loss_scale: 65536.0000 (61688.7449)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0003  max mem: 15572
Epoch: [4]  [ 750/2272]  eta: 0:09:51  lr: 0.000041  min_lr: 0.000000  loss: 4.4994 (4.6343)  loss_scale: 65536.0000 (61739.9734)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [4]  [ 760/2272]  eta: 0:09:47  lr: 0.000041  min_lr: 0.000000  loss: 4.5528 (4.6355)  loss_scale: 65536.0000 (61789.8555)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [4]  [ 770/2272]  eta: 0:09:43  lr: 0.000041  min_lr: 0.000000  loss: 4.6568 (4.6354)  loss_scale: 65536.0000 (61838.4436)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [4]  [ 780/2272]  eta: 0:09:39  lr: 0.000041  min_lr: 0.000000  loss: 4.6120 (4.6363)  loss_scale: 65536.0000 (61885.7875)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
Epoch: [4]  [ 790/2272]  eta: 0:09:35  lr: 0.000041  min_lr: 0.000000  loss: 4.5223 (4.6349)  loss_scale: 65536.0000 (61931.9343)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [4]  [ 800/2272]  eta: 0:09:31  lr: 0.000041  min_lr: 0.000000  loss: 4.5275 (4.6350)  loss_scale: 65536.0000 (61976.9288)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [4]  [ 810/2272]  eta: 0:09:27  lr: 0.000041  min_lr: 0.000000  loss: 4.6284 (4.6333)  loss_scale: 65536.0000 (62020.8138)  weight_decay: 0.0500 (0.0500)  time: 0.3792  data: 0.0003  max mem: 15572
Epoch: [4]  [ 820/2272]  eta: 0:09:23  lr: 0.000041  min_lr: 0.000000  loss: 4.6284 (4.6337)  loss_scale: 65536.0000 (62063.6297)  weight_decay: 0.0500 (0.0500)  time: 0.3797  data: 0.0003  max mem: 15572
Epoch: [4]  [ 830/2272]  eta: 0:09:18  lr: 0.000041  min_lr: 0.000000  loss: 4.6709 (4.6330)  loss_scale: 65536.0000 (62105.4152)  weight_decay: 0.0500 (0.0500)  time: 0.3795  data: 0.0003  max mem: 15572
Epoch: [4]  [ 840/2272]  eta: 0:09:15  lr: 0.000041  min_lr: 0.000000  loss: 4.6709 (4.6332)  loss_scale: 65536.0000 (62146.2069)  weight_decay: 0.0500 (0.0500)  time: 0.3861  data: 0.0004  max mem: 15572
[2025-01-13 12:12:39,097] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:12:39,097] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:12:39,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11618
[2025-01-13 12:12:39,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:12:39,903] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 850/2272]  eta: 0:09:11  lr: 0.000041  min_lr: 0.000000  loss: 4.6493 (4.6330)  loss_scale: 65536.0000 (62340.0611)  weight_decay: 0.0500 (0.0500)  time: 0.3923  data: 0.0004  max mem: 15572
Epoch: [4]  [ 860/2272]  eta: 0:09:07  lr: 0.000041  min_lr: 0.000000  loss: 4.6353 (4.6313)  loss_scale: 65536.0000 (62377.1800)  weight_decay: 0.0500 (0.0500)  time: 0.3931  data: 0.0004  max mem: 15572
Epoch: [4]  [ 870/2272]  eta: 0:09:03  lr: 0.000041  min_lr: 0.000000  loss: 4.6306 (4.6308)  loss_scale: 65536.0000 (62413.4466)  weight_decay: 0.0500 (0.0500)  time: 0.3922  data: 0.0004  max mem: 15572
[2025-01-13 12:12:49,744] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11643
[2025-01-13 12:12:49,745] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 12:12:49,745] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [ 880/2272]  eta: 0:08:59  lr: 0.000041  min_lr: 0.000000  loss: 4.6372 (4.6313)  loss_scale: 65536.0000 (62114.1430)  weight_decay: 0.0500 (0.0500)  time: 0.3891  data: 0.0005  max mem: 15572
Epoch: [4]  [ 890/2272]  eta: 0:08:55  lr: 0.000041  min_lr: 0.000000  loss: 4.6777 (4.6322)  loss_scale: 32768.0000 (61784.7811)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0004  max mem: 15572
Epoch: [4]  [ 900/2272]  eta: 0:08:51  lr: 0.000041  min_lr: 0.000000  loss: 4.6985 (4.6322)  loss_scale: 32768.0000 (61462.7303)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [4]  [ 910/2272]  eta: 0:08:47  lr: 0.000041  min_lr: 0.000000  loss: 4.5659 (4.6316)  loss_scale: 32768.0000 (61147.7497)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [4]  [ 920/2272]  eta: 0:08:43  lr: 0.000041  min_lr: 0.000000  loss: 4.4461 (4.6296)  loss_scale: 32768.0000 (60839.6091)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0002  max mem: 15572
Epoch: [4]  [ 930/2272]  eta: 0:08:39  lr: 0.000041  min_lr: 0.000000  loss: 4.6145 (4.6314)  loss_scale: 32768.0000 (60538.0881)  weight_decay: 0.0500 (0.0500)  time: 0.3787  data: 0.0003  max mem: 15572
Epoch: [4]  [ 940/2272]  eta: 0:08:35  lr: 0.000041  min_lr: 0.000000  loss: 4.7543 (4.6327)  loss_scale: 32768.0000 (60242.9756)  weight_decay: 0.0500 (0.0500)  time: 0.3794  data: 0.0002  max mem: 15572
Epoch: [4]  [ 950/2272]  eta: 0:08:31  lr: 0.000041  min_lr: 0.000000  loss: 4.5904 (4.6310)  loss_scale: 32768.0000 (59954.0694)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0002  max mem: 15572
Epoch: [4]  [ 960/2272]  eta: 0:08:27  lr: 0.000041  min_lr: 0.000000  loss: 4.5226 (4.6298)  loss_scale: 32768.0000 (59671.1759)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0002  max mem: 15572
Epoch: [4]  [ 970/2272]  eta: 0:08:23  lr: 0.000042  min_lr: 0.000000  loss: 4.5424 (4.6305)  loss_scale: 32768.0000 (59394.1092)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
Epoch: [4]  [ 980/2272]  eta: 0:08:19  lr: 0.000042  min_lr: 0.000000  loss: 4.6036 (4.6297)  loss_scale: 32768.0000 (59122.6911)  weight_decay: 0.0500 (0.0500)  time: 0.3868  data: 0.0003  max mem: 15572
Epoch: [4]  [ 990/2272]  eta: 0:08:16  lr: 0.000042  min_lr: 0.000000  loss: 4.6036 (4.6297)  loss_scale: 32768.0000 (58856.7508)  weight_decay: 0.0500 (0.0500)  time: 0.3968  data: 0.0004  max mem: 15572
Epoch: [4]  [1000/2272]  eta: 0:08:12  lr: 0.000042  min_lr: 0.000000  loss: 4.6268 (4.6297)  loss_scale: 32768.0000 (58596.1239)  weight_decay: 0.0500 (0.0500)  time: 0.4112  data: 0.0004  max mem: 15572
[2025-01-13 12:13:39,299] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:13:39,299] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [1010/2272]  eta: 0:08:09  lr: 0.000042  min_lr: 0.000000  loss: 4.6268 (4.6300)  loss_scale: 32768.0000 (58664.7676)  weight_decay: 0.0500 (0.0500)  time: 0.4134  data: 0.0005  max mem: 15572
Epoch: [4]  [1020/2272]  eta: 0:08:05  lr: 0.000042  min_lr: 0.000000  loss: 4.5550 (4.6290)  loss_scale: 65536.0000 (58732.0666)  weight_decay: 0.0500 (0.0500)  time: 0.3978  data: 0.0005  max mem: 15572
Epoch: [4]  [1030/2272]  eta: 0:08:01  lr: 0.000042  min_lr: 0.000000  loss: 4.5492 (4.6289)  loss_scale: 65536.0000 (58798.0601)  weight_decay: 0.0500 (0.0500)  time: 0.3928  data: 0.0006  max mem: 15572
Epoch: [4]  [1040/2272]  eta: 0:07:57  lr: 0.000042  min_lr: 0.000000  loss: 4.5404 (4.6283)  loss_scale: 65536.0000 (58862.7858)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0005  max mem: 15572
Epoch: [4]  [1050/2272]  eta: 0:07:53  lr: 0.000042  min_lr: 0.000000  loss: 4.6169 (4.6290)  loss_scale: 65536.0000 (58926.2797)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0004  max mem: 15572
Epoch: [4]  [1060/2272]  eta: 0:07:49  lr: 0.000042  min_lr: 0.000000  loss: 4.6546 (4.6286)  loss_scale: 65536.0000 (58988.5768)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0003  max mem: 15572
Epoch: [4]  [1070/2272]  eta: 0:07:45  lr: 0.000042  min_lr: 0.000000  loss: 4.6399 (4.6290)  loss_scale: 65536.0000 (59049.7106)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0003  max mem: 15572
Epoch: [4]  [1080/2272]  eta: 0:07:41  lr: 0.000042  min_lr: 0.000000  loss: 4.6188 (4.6289)  loss_scale: 65536.0000 (59109.7132)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0003  max mem: 15572
Epoch: [4]  [1090/2272]  eta: 0:07:37  lr: 0.000042  min_lr: 0.000000  loss: 4.5488 (4.6275)  loss_scale: 65536.0000 (59168.6159)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
Epoch: [4]  [1100/2272]  eta: 0:07:33  lr: 0.000042  min_lr: 0.000000  loss: 4.5916 (4.6276)  loss_scale: 65536.0000 (59226.4487)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0002  max mem: 15572
Epoch: [4]  [1110/2272]  eta: 0:07:29  lr: 0.000042  min_lr: 0.000000  loss: 4.6860 (4.6285)  loss_scale: 65536.0000 (59283.2403)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0003  max mem: 15572
Epoch: [4]  [1120/2272]  eta: 0:07:25  lr: 0.000042  min_lr: 0.000000  loss: 4.6860 (4.6284)  loss_scale: 65536.0000 (59339.0187)  weight_decay: 0.0500 (0.0500)  time: 0.3795  data: 0.0004  max mem: 15572
[2025-01-13 12:14:28,324] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:14:28,325] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1130/2272]  eta: 0:07:21  lr: 0.000042  min_lr: 0.000000  loss: 4.5512 (4.6280)  loss_scale: 65536.0000 (59509.7011)  weight_decay: 0.0500 (0.0500)  time: 0.3923  data: 0.0005  max mem: 15572
[2025-01-13 12:14:29,101] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11902
[2025-01-13 12:14:29,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:14:29,101] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1140/2272]  eta: 0:07:18  lr: 0.000042  min_lr: 0.000000  loss: 4.5559 (4.6271)  loss_scale: 65536.0000 (59562.5171)  weight_decay: 0.0500 (0.0500)  time: 0.3977  data: 0.0005  max mem: 15572
Epoch: [4]  [1150/2272]  eta: 0:07:14  lr: 0.000042  min_lr: 0.000000  loss: 4.5094 (4.6265)  loss_scale: 65536.0000 (59614.4153)  weight_decay: 0.0500 (0.0500)  time: 0.3934  data: 0.0005  max mem: 15572
Epoch: [4]  [1160/2272]  eta: 0:07:10  lr: 0.000042  min_lr: 0.000000  loss: 4.5094 (4.6258)  loss_scale: 65536.0000 (59665.4195)  weight_decay: 0.0500 (0.0500)  time: 0.3965  data: 0.0005  max mem: 15572
Epoch: [4]  [1170/2272]  eta: 0:07:06  lr: 0.000042  min_lr: 0.000000  loss: 4.6248 (4.6259)  loss_scale: 65536.0000 (59715.5525)  weight_decay: 0.0500 (0.0500)  time: 0.4012  data: 0.0005  max mem: 15572
Epoch: [4]  [1180/2272]  eta: 0:07:02  lr: 0.000042  min_lr: 0.000000  loss: 4.5628 (4.6248)  loss_scale: 65536.0000 (59764.8366)  weight_decay: 0.0500 (0.0500)  time: 0.3935  data: 0.0004  max mem: 15572
Epoch: [4]  [1190/2272]  eta: 0:06:58  lr: 0.000042  min_lr: 0.000000  loss: 4.5975 (4.6255)  loss_scale: 65536.0000 (59813.2930)  weight_decay: 0.0500 (0.0500)  time: 0.3831  data: 0.0003  max mem: 15572
Epoch: [4]  [1200/2272]  eta: 0:06:55  lr: 0.000042  min_lr: 0.000000  loss: 4.6360 (4.6250)  loss_scale: 65536.0000 (59860.9425)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0002  max mem: 15572
Epoch: [4]  [1210/2272]  eta: 0:06:51  lr: 0.000042  min_lr: 0.000000  loss: 4.5621 (4.6250)  loss_scale: 65536.0000 (59907.8051)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0002  max mem: 15572
Epoch: [4]  [1220/2272]  eta: 0:06:47  lr: 0.000043  min_lr: 0.000000  loss: 4.5927 (4.6240)  loss_scale: 65536.0000 (59953.9001)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0002  max mem: 15572
[2025-01-13 12:15:06,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=71, lr=[4.1246493612936786e-07, 4.1246493612936786e-07, 5.892356230419541e-07, 5.892356230419541e-07, 8.417651757742203e-07, 8.417651757742203e-07, 1.2025216796774577e-06, 1.2025216796774577e-06, 1.7178881138249397e-06, 1.7178881138249397e-06, 2.454125876892771e-06, 2.454125876892771e-06, 3.505894109846816e-06, 3.505894109846816e-06, 5.008420156924023e-06, 5.008420156924023e-06, 7.15488593846289e-06, 7.15488593846289e-06, 1.022126562637556e-05, 1.022126562637556e-05, 1.460180803767937e-05, 1.460180803767937e-05, 2.0859725768113388e-05, 2.0859725768113388e-05, 2.9799608240161983e-05, 2.9799608240161983e-05, 4.257086891451712e-05, 4.257086891451712e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 12:15:06,644] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=27.983345592774974, CurrSamplesPerSec=34.18185127408628, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [1230/2272]  eta: 0:06:43  lr: 0.000043  min_lr: 0.000000  loss: 4.6228 (4.6244)  loss_scale: 65536.0000 (59999.2461)  weight_decay: 0.0500 (0.0500)  time: 0.3788  data: 0.0003  max mem: 15572
Epoch: [4]  [1240/2272]  eta: 0:06:39  lr: 0.000043  min_lr: 0.000000  loss: 4.7258 (4.6247)  loss_scale: 65536.0000 (60043.8614)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0003  max mem: 15572
Epoch: [4]  [1250/2272]  eta: 0:06:35  lr: 0.000043  min_lr: 0.000000  loss: 4.6580 (4.6241)  loss_scale: 65536.0000 (60087.7634)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
[2025-01-13 12:15:18,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:15:18,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1260/2272]  eta: 0:06:31  lr: 0.000043  min_lr: 0.000000  loss: 4.5127 (4.6230)  loss_scale: 65536.0000 (60182.9405)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
[2025-01-13 12:15:20,951] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12037
[2025-01-13 12:15:20,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:15:20,952] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1270/2272]  eta: 0:06:27  lr: 0.000043  min_lr: 0.000000  loss: 4.5645 (4.6229)  loss_scale: 65536.0000 (60482.8702)  weight_decay: 0.0500 (0.0500)  time: 0.3831  data: 0.0004  max mem: 15572
Epoch: [4]  [1280/2272]  eta: 0:06:23  lr: 0.000043  min_lr: 0.000000  loss: 4.6085 (4.6225)  loss_scale: 65536.0000 (60522.3169)  weight_decay: 0.0500 (0.0500)  time: 0.3940  data: 0.0006  max mem: 15572
Epoch: [4]  [1290/2272]  eta: 0:06:19  lr: 0.000043  min_lr: 0.000000  loss: 4.6085 (4.6231)  loss_scale: 65536.0000 (60561.1526)  weight_decay: 0.0500 (0.0500)  time: 0.3941  data: 0.0006  max mem: 15572
Epoch: [4]  [1300/2272]  eta: 0:06:15  lr: 0.000043  min_lr: 0.000000  loss: 4.6376 (4.6233)  loss_scale: 65536.0000 (60599.3912)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0007  max mem: 15572
Epoch: [4]  [1310/2272]  eta: 0:06:12  lr: 0.000043  min_lr: 0.000000  loss: 4.6274 (4.6233)  loss_scale: 65536.0000 (60637.0465)  weight_decay: 0.0500 (0.0500)  time: 0.3919  data: 0.0006  max mem: 15572
Epoch: [4]  [1320/2272]  eta: 0:06:08  lr: 0.000043  min_lr: 0.000000  loss: 4.5836 (4.6236)  loss_scale: 65536.0000 (60674.1317)  weight_decay: 0.0500 (0.0500)  time: 0.3916  data: 0.0005  max mem: 15572
Epoch: [4]  [1330/2272]  eta: 0:06:04  lr: 0.000043  min_lr: 0.000000  loss: 4.6080 (4.6229)  loss_scale: 65536.0000 (60710.6597)  weight_decay: 0.0500 (0.0500)  time: 0.3879  data: 0.0004  max mem: 15572
Epoch: [4]  [1340/2272]  eta: 0:06:00  lr: 0.000043  min_lr: 0.000000  loss: 4.5655 (4.6224)  loss_scale: 65536.0000 (60746.6428)  weight_decay: 0.0500 (0.0500)  time: 0.3820  data: 0.0003  max mem: 15572
Epoch: [4]  [1350/2272]  eta: 0:05:56  lr: 0.000043  min_lr: 0.000000  loss: 4.5791 (4.6221)  loss_scale: 65536.0000 (60782.0933)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0003  max mem: 15572
Epoch: [4]  [1360/2272]  eta: 0:05:52  lr: 0.000043  min_lr: 0.000000  loss: 4.5763 (4.6218)  loss_scale: 65536.0000 (60817.0228)  weight_decay: 0.0500 (0.0500)  time: 0.3812  data: 0.0003  max mem: 15572
Epoch: [4]  [1370/2272]  eta: 0:05:48  lr: 0.000043  min_lr: 0.000000  loss: 4.4803 (4.6206)  loss_scale: 65536.0000 (60851.4427)  weight_decay: 0.0500 (0.0500)  time: 0.3808  data: 0.0004  max mem: 15572
Epoch: [4]  [1380/2272]  eta: 0:05:44  lr: 0.000043  min_lr: 0.000000  loss: 4.4803 (4.6211)  loss_scale: 65536.0000 (60885.3642)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0003  max mem: 15572
Epoch: [4]  [1390/2272]  eta: 0:05:40  lr: 0.000043  min_lr: 0.000000  loss: 4.5344 (4.6203)  loss_scale: 65536.0000 (60918.7980)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0003  max mem: 15572
[2025-01-13 12:16:10,670] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:16:10,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1400/2272]  eta: 0:05:36  lr: 0.000043  min_lr: 0.000000  loss: 4.4920 (4.6195)  loss_scale: 65536.0000 (61232.4226)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0004  max mem: 15572
[2025-01-13 12:16:12,910] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12172
[2025-01-13 12:16:12,910] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:16:12,910] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1410/2272]  eta: 0:05:33  lr: 0.000043  min_lr: 0.000000  loss: 4.5335 (4.6190)  loss_scale: 65536.0000 (61262.9227)  weight_decay: 0.0500 (0.0500)  time: 0.3773  data: 0.0004  max mem: 15572
Epoch: [4]  [1420/2272]  eta: 0:05:29  lr: 0.000043  min_lr: 0.000000  loss: 4.5031 (4.6175)  loss_scale: 65536.0000 (61292.9937)  weight_decay: 0.0500 (0.0500)  time: 0.3815  data: 0.0005  max mem: 15572
Epoch: [4]  [1430/2272]  eta: 0:05:25  lr: 0.000043  min_lr: 0.000000  loss: 4.5031 (4.6171)  loss_scale: 65536.0000 (61322.6443)  weight_decay: 0.0500 (0.0500)  time: 0.3893  data: 0.0005  max mem: 15572
Epoch: [4]  [1440/2272]  eta: 0:05:21  lr: 0.000043  min_lr: 0.000000  loss: 4.5280 (4.6169)  loss_scale: 65536.0000 (61351.8834)  weight_decay: 0.0500 (0.0500)  time: 0.3978  data: 0.0005  max mem: 15572
Epoch: [4]  [1450/2272]  eta: 0:05:17  lr: 0.000043  min_lr: 0.000000  loss: 4.6067 (4.6165)  loss_scale: 65536.0000 (61380.7195)  weight_decay: 0.0500 (0.0500)  time: 0.3925  data: 0.0005  max mem: 15572
Epoch: [4]  [1460/2272]  eta: 0:05:13  lr: 0.000044  min_lr: 0.000000  loss: 4.6338 (4.6173)  loss_scale: 65536.0000 (61409.1608)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0005  max mem: 15572
Epoch: [4]  [1470/2272]  eta: 0:05:09  lr: 0.000044  min_lr: 0.000000  loss: 4.6609 (4.6175)  loss_scale: 65536.0000 (61437.2155)  weight_decay: 0.0500 (0.0500)  time: 0.3893  data: 0.0004  max mem: 15572
[2025-01-13 12:16:43,524] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12251
[2025-01-13 12:16:43,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 12:16:43,524] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [1480/2272]  eta: 0:05:06  lr: 0.000044  min_lr: 0.000000  loss: 4.5734 (4.6167)  loss_scale: 65536.0000 (61442.7657)  weight_decay: 0.0500 (0.0500)  time: 0.3803  data: 0.0004  max mem: 15572
Epoch: [4]  [1490/2272]  eta: 0:05:02  lr: 0.000044  min_lr: 0.000000  loss: 4.6162 (4.6170)  loss_scale: 32768.0000 (61250.4467)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0003  max mem: 15572
Epoch: [4]  [1500/2272]  eta: 0:04:58  lr: 0.000044  min_lr: 0.000000  loss: 4.6154 (4.6166)  loss_scale: 32768.0000 (61060.6902)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0004  max mem: 15572
Epoch: [4]  [1510/2272]  eta: 0:04:54  lr: 0.000044  min_lr: 0.000000  loss: 4.5603 (4.6163)  loss_scale: 32768.0000 (60873.4454)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0004  max mem: 15572
Epoch: [4]  [1520/2272]  eta: 0:04:50  lr: 0.000044  min_lr: 0.000000  loss: 4.5603 (4.6162)  loss_scale: 32768.0000 (60688.6627)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0003  max mem: 15572
Epoch: [4]  [1530/2272]  eta: 0:04:46  lr: 0.000044  min_lr: 0.000000  loss: 4.6701 (4.6161)  loss_scale: 32768.0000 (60506.2939)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [4]  [1540/2272]  eta: 0:04:42  lr: 0.000044  min_lr: 0.000000  loss: 4.6129 (4.6158)  loss_scale: 32768.0000 (60326.2920)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0003  max mem: 15572
Epoch: [4]  [1550/2272]  eta: 0:04:38  lr: 0.000044  min_lr: 0.000000  loss: 4.5764 (4.6160)  loss_scale: 32768.0000 (60148.6112)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0003  max mem: 15572
Epoch: [4]  [1560/2272]  eta: 0:04:34  lr: 0.000044  min_lr: 0.000000  loss: 4.5891 (4.6159)  loss_scale: 32768.0000 (59973.2069)  weight_decay: 0.0500 (0.0500)  time: 0.3875  data: 0.0005  max mem: 15572
Epoch: [4]  [1570/2272]  eta: 0:04:31  lr: 0.000044  min_lr: 0.000000  loss: 4.5949 (4.6163)  loss_scale: 32768.0000 (59800.0356)  weight_decay: 0.0500 (0.0500)  time: 0.3976  data: 0.0006  max mem: 15572
Epoch: [4]  [1580/2272]  eta: 0:04:27  lr: 0.000044  min_lr: 0.000000  loss: 4.6687 (4.6162)  loss_scale: 32768.0000 (59629.0550)  weight_decay: 0.0500 (0.0500)  time: 0.3978  data: 0.0005  max mem: 15572
Epoch: [4]  [1590/2272]  eta: 0:04:23  lr: 0.000044  min_lr: 0.000000  loss: 4.5412 (4.6155)  loss_scale: 32768.0000 (59460.2238)  weight_decay: 0.0500 (0.0500)  time: 0.3928  data: 0.0005  max mem: 15572
Epoch: [4]  [1600/2272]  eta: 0:04:19  lr: 0.000044  min_lr: 0.000000  loss: 4.5412 (4.6158)  loss_scale: 32768.0000 (59293.5016)  weight_decay: 0.0500 (0.0500)  time: 0.3901  data: 0.0005  max mem: 15572
[2025-01-13 12:17:33,148] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:17:33,149] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [1610/2272]  eta: 0:04:15  lr: 0.000044  min_lr: 0.000000  loss: 4.5893 (4.6147)  loss_scale: 32768.0000 (59169.5295)  weight_decay: 0.0500 (0.0500)  time: 0.3874  data: 0.0004  max mem: 15572
Epoch: [4]  [1620/2272]  eta: 0:04:11  lr: 0.000044  min_lr: 0.000000  loss: 4.4798 (4.6146)  loss_scale: 65536.0000 (59208.8044)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0003  max mem: 15572
Epoch: [4]  [1630/2272]  eta: 0:04:07  lr: 0.000044  min_lr: 0.000000  loss: 4.5254 (4.6136)  loss_scale: 65536.0000 (59247.5978)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0003  max mem: 15572
Epoch: [4]  [1640/2272]  eta: 0:04:04  lr: 0.000044  min_lr: 0.000000  loss: 4.5290 (4.6144)  loss_scale: 65536.0000 (59285.9183)  weight_decay: 0.0500 (0.0500)  time: 0.3831  data: 0.0003  max mem: 15572
Epoch: [4]  [1650/2272]  eta: 0:04:00  lr: 0.000044  min_lr: 0.000000  loss: 4.7176 (4.6138)  loss_scale: 65536.0000 (59323.7747)  weight_decay: 0.0500 (0.0500)  time: 0.3837  data: 0.0004  max mem: 15572
Epoch: [4]  [1660/2272]  eta: 0:03:56  lr: 0.000044  min_lr: 0.000000  loss: 4.5948 (4.6138)  loss_scale: 65536.0000 (59361.1752)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0003  max mem: 15572
Epoch: [4]  [1670/2272]  eta: 0:03:52  lr: 0.000044  min_lr: 0.000000  loss: 4.5955 (4.6141)  loss_scale: 65536.0000 (59398.1281)  weight_decay: 0.0500 (0.0500)  time: 0.3817  data: 0.0003  max mem: 15572
Epoch: [4]  [1680/2272]  eta: 0:03:48  lr: 0.000044  min_lr: 0.000000  loss: 4.7087 (4.6146)  loss_scale: 65536.0000 (59434.6413)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0003  max mem: 15572
Epoch: [4]  [1690/2272]  eta: 0:03:44  lr: 0.000044  min_lr: 0.000000  loss: 4.5206 (4.6132)  loss_scale: 65536.0000 (59470.7226)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
Epoch: [4]  [1700/2272]  eta: 0:03:40  lr: 0.000045  min_lr: 0.000000  loss: 4.3954 (4.6128)  loss_scale: 65536.0000 (59506.3798)  weight_decay: 0.0500 (0.0500)  time: 0.3835  data: 0.0003  max mem: 15572
Epoch: [4]  [1710/2272]  eta: 0:03:36  lr: 0.000045  min_lr: 0.000000  loss: 4.6526 (4.6134)  loss_scale: 65536.0000 (59541.6201)  weight_decay: 0.0500 (0.0500)  time: 0.3935  data: 0.0005  max mem: 15572
Epoch: [4]  [1720/2272]  eta: 0:03:33  lr: 0.000045  min_lr: 0.000000  loss: 4.6526 (4.6129)  loss_scale: 65536.0000 (59576.4509)  weight_decay: 0.0500 (0.0500)  time: 0.3913  data: 0.0004  max mem: 15572
Epoch: [4]  [1730/2272]  eta: 0:03:29  lr: 0.000045  min_lr: 0.000000  loss: 4.5385 (4.6128)  loss_scale: 65536.0000 (59610.8793)  weight_decay: 0.0500 (0.0500)  time: 0.3913  data: 0.0004  max mem: 15572
[2025-01-13 12:18:22,383] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:18:22,384] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1740/2272]  eta: 0:03:25  lr: 0.000045  min_lr: 0.000000  loss: 4.6296 (4.6130)  loss_scale: 65536.0000 (59795.4831)  weight_decay: 0.0500 (0.0500)  time: 0.3949  data: 0.0004  max mem: 15572
[2025-01-13 12:18:23,984] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12512
[2025-01-13 12:18:23,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:18:23,984] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1750/2272]  eta: 0:03:21  lr: 0.000045  min_lr: 0.000000  loss: 4.6296 (4.6134)  loss_scale: 65536.0000 (59828.2673)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0005  max mem: 15572
Epoch: [4]  [1760/2272]  eta: 0:03:17  lr: 0.000045  min_lr: 0.000000  loss: 4.6291 (4.6129)  loss_scale: 65536.0000 (59860.6792)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0004  max mem: 15572
Epoch: [4]  [1770/2272]  eta: 0:03:13  lr: 0.000045  min_lr: 0.000000  loss: 4.5401 (4.6122)  loss_scale: 65536.0000 (59892.7250)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0003  max mem: 15572
Epoch: [4]  [1780/2272]  eta: 0:03:09  lr: 0.000045  min_lr: 0.000000  loss: 4.5922 (4.6124)  loss_scale: 65536.0000 (59924.4110)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [4]  [1790/2272]  eta: 0:03:06  lr: 0.000045  min_lr: 0.000000  loss: 4.5454 (4.6115)  loss_scale: 65536.0000 (59955.7432)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0003  max mem: 15572
Epoch: [4]  [1800/2272]  eta: 0:03:02  lr: 0.000045  min_lr: 0.000000  loss: 4.4166 (4.6106)  loss_scale: 65536.0000 (59986.7274)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [4]  [1810/2272]  eta: 0:02:58  lr: 0.000045  min_lr: 0.000000  loss: 4.5332 (4.6112)  loss_scale: 65536.0000 (60017.3694)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [4]  [1820/2272]  eta: 0:02:54  lr: 0.000045  min_lr: 0.000000  loss: 4.6711 (4.6111)  loss_scale: 65536.0000 (60047.6749)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [4]  [1830/2272]  eta: 0:02:50  lr: 0.000045  min_lr: 0.000000  loss: 4.5960 (4.6110)  loss_scale: 65536.0000 (60077.6494)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0003  max mem: 15572
Epoch: [4]  [1840/2272]  eta: 0:02:46  lr: 0.000045  min_lr: 0.000000  loss: 4.5960 (4.6104)  loss_scale: 65536.0000 (60107.2982)  weight_decay: 0.0500 (0.0500)  time: 0.3813  data: 0.0004  max mem: 15572
Epoch: [4]  [1850/2272]  eta: 0:02:42  lr: 0.000045  min_lr: 0.000000  loss: 4.5007 (4.6102)  loss_scale: 65536.0000 (60136.6267)  weight_decay: 0.0500 (0.0500)  time: 0.3910  data: 0.0005  max mem: 15572
Epoch: [4]  [1860/2272]  eta: 0:02:38  lr: 0.000045  min_lr: 0.000000  loss: 4.5878 (4.6105)  loss_scale: 65536.0000 (60165.6400)  weight_decay: 0.0500 (0.0500)  time: 0.3976  data: 0.0005  max mem: 15572
[2025-01-13 12:19:13,186] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:19:13,187] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1870/2272]  eta: 0:02:35  lr: 0.000045  min_lr: 0.000000  loss: 4.5878 (4.6104)  loss_scale: 65536.0000 (60229.3704)  weight_decay: 0.0500 (0.0500)  time: 0.3976  data: 0.0004  max mem: 15572
[2025-01-13 12:19:13,572] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12642
[2025-01-13 12:19:13,572] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:19:13,573] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1880/2272]  eta: 0:02:31  lr: 0.000045  min_lr: 0.000000  loss: 4.5440 (4.6101)  loss_scale: 65536.0000 (60257.5821)  weight_decay: 0.0500 (0.0500)  time: 0.3955  data: 0.0004  max mem: 15572
Epoch: [4]  [1890/2272]  eta: 0:02:27  lr: 0.000045  min_lr: 0.000000  loss: 4.5520 (4.6098)  loss_scale: 65536.0000 (60285.4955)  weight_decay: 0.0500 (0.0500)  time: 0.3975  data: 0.0004  max mem: 15572
Epoch: [4]  [1900/2272]  eta: 0:02:23  lr: 0.000045  min_lr: 0.000000  loss: 4.5520 (4.6097)  loss_scale: 65536.0000 (60313.1152)  weight_decay: 0.0500 (0.0500)  time: 0.3917  data: 0.0004  max mem: 15572
Epoch: [4]  [1910/2272]  eta: 0:02:19  lr: 0.000045  min_lr: 0.000000  loss: 4.6001 (4.6104)  loss_scale: 65536.0000 (60340.4458)  weight_decay: 0.0500 (0.0500)  time: 0.3821  data: 0.0004  max mem: 15572
Epoch: [4]  [1920/2272]  eta: 0:02:15  lr: 0.000045  min_lr: 0.000000  loss: 4.5619 (4.6096)  loss_scale: 65536.0000 (60367.4919)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
Epoch: [4]  [1930/2272]  eta: 0:02:11  lr: 0.000045  min_lr: 0.000000  loss: 4.5451 (4.6108)  loss_scale: 65536.0000 (60394.2579)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [4]  [1940/2272]  eta: 0:02:08  lr: 0.000046  min_lr: 0.000000  loss: 4.6047 (4.6108)  loss_scale: 65536.0000 (60420.7481)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0003  max mem: 15572
Epoch: [4]  [1950/2272]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000000  loss: 4.5348 (4.6106)  loss_scale: 65536.0000 (60446.9667)  weight_decay: 0.0500 (0.0500)  time: 0.3779  data: 0.0003  max mem: 15572
Epoch: [4]  [1960/2272]  eta: 0:02:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4471 (4.6097)  loss_scale: 65536.0000 (60472.9179)  weight_decay: 0.0500 (0.0500)  time: 0.3773  data: 0.0003  max mem: 15572
Epoch: [4]  [1970/2272]  eta: 0:01:56  lr: 0.000046  min_lr: 0.000000  loss: 4.4573 (4.6094)  loss_scale: 65536.0000 (60498.6058)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
Epoch: [4]  [1980/2272]  eta: 0:01:52  lr: 0.000046  min_lr: 0.000000  loss: 4.5397 (4.6091)  loss_scale: 65536.0000 (60524.0343)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [4]  [1990/2272]  eta: 0:01:48  lr: 0.000046  min_lr: 0.000000  loss: 4.5397 (4.6090)  loss_scale: 65536.0000 (60549.2074)  weight_decay: 0.0500 (0.0500)  time: 0.3825  data: 0.0003  max mem: 15572
[2025-01-13 12:20:03,219] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:20:03,219] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2000/2272]  eta: 0:01:44  lr: 0.000046  min_lr: 0.000000  loss: 4.5829 (4.6087)  loss_scale: 65536.0000 (60606.8806)  weight_decay: 0.0500 (0.0500)  time: 0.4032  data: 0.0024  max mem: 15572
[2025-01-13 12:20:04,801] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12775
[2025-01-13 12:20:04,801] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:20:04,802] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2010/2272]  eta: 0:01:41  lr: 0.000046  min_lr: 0.000000  loss: 4.5370 (4.6077)  loss_scale: 65536.0000 (60729.1576)  weight_decay: 0.0500 (0.0500)  time: 0.4104  data: 0.0024  max mem: 15572
Epoch: [4]  [2020/2272]  eta: 0:01:37  lr: 0.000046  min_lr: 0.000000  loss: 4.4606 (4.6070)  loss_scale: 65536.0000 (60752.9421)  weight_decay: 0.0500 (0.0500)  time: 0.3977  data: 0.0005  max mem: 15572
Epoch: [4]  [2030/2272]  eta: 0:01:33  lr: 0.000046  min_lr: 0.000000  loss: 4.4662 (4.6068)  loss_scale: 65536.0000 (60776.4924)  weight_decay: 0.0500 (0.0500)  time: 0.3964  data: 0.0004  max mem: 15572
Epoch: [4]  [2040/2272]  eta: 0:01:29  lr: 0.000046  min_lr: 0.000000  loss: 4.4662 (4.6066)  loss_scale: 65536.0000 (60799.8119)  weight_decay: 0.0500 (0.0500)  time: 0.3983  data: 0.0004  max mem: 15572
Epoch: [4]  [2050/2272]  eta: 0:01:25  lr: 0.000046  min_lr: 0.000000  loss: 4.4426 (4.6060)  loss_scale: 65536.0000 (60822.9039)  weight_decay: 0.0500 (0.0500)  time: 0.3892  data: 0.0003  max mem: 15572
Epoch: [4]  [2060/2272]  eta: 0:01:21  lr: 0.000046  min_lr: 0.000000  loss: 4.5749 (4.6063)  loss_scale: 65536.0000 (60845.7720)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0003  max mem: 15572
Epoch: [4]  [2070/2272]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000000  loss: 4.6314 (4.6063)  loss_scale: 65536.0000 (60868.4191)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
Epoch: [4]  [2080/2272]  eta: 0:01:14  lr: 0.000046  min_lr: 0.000000  loss: 4.7022 (4.6068)  loss_scale: 65536.0000 (60890.8486)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0003  max mem: 15572
Epoch: [4]  [2090/2272]  eta: 0:01:10  lr: 0.000046  min_lr: 0.000000  loss: 4.7003 (4.6067)  loss_scale: 65536.0000 (60913.0636)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0003  max mem: 15572
Epoch: [4]  [2100/2272]  eta: 0:01:06  lr: 0.000046  min_lr: 0.000000  loss: 4.6251 (4.6072)  loss_scale: 65536.0000 (60935.0671)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [4]  [2110/2272]  eta: 0:01:02  lr: 0.000046  min_lr: 0.000000  loss: 4.6251 (4.6070)  loss_scale: 65536.0000 (60956.8622)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0003  max mem: 15572
Epoch: [4]  [2120/2272]  eta: 0:00:58  lr: 0.000046  min_lr: 0.000000  loss: 4.5981 (4.6069)  loss_scale: 65536.0000 (60978.4517)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [4]  [2130/2272]  eta: 0:00:54  lr: 0.000046  min_lr: 0.000000  loss: 4.7169 (4.6072)  loss_scale: 65536.0000 (60999.8386)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0003  max mem: 15572
[2025-01-13 12:20:54,226] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:20:54,226] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:20:56,559] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12910
[2025-01-13 12:20:56,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:20:56,559] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2140/2272]  eta: 0:00:50  lr: 0.000046  min_lr: 0.000000  loss: 4.6158 (4.6068)  loss_scale: 65536.0000 (61204.6857)  weight_decay: 0.0500 (0.0500)  time: 0.3861  data: 0.0004  max mem: 15572
Epoch: [4]  [2150/2272]  eta: 0:00:47  lr: 0.000046  min_lr: 0.000000  loss: 4.4630 (4.6068)  loss_scale: 65536.0000 (61224.8219)  weight_decay: 0.0500 (0.0500)  time: 0.3911  data: 0.0006  max mem: 15572
Epoch: [4]  [2160/2272]  eta: 0:00:43  lr: 0.000046  min_lr: 0.000000  loss: 4.4236 (4.6059)  loss_scale: 65536.0000 (61244.7719)  weight_decay: 0.0500 (0.0500)  time: 0.3903  data: 0.0007  max mem: 15572
Epoch: [4]  [2170/2272]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000000  loss: 4.4739 (4.6058)  loss_scale: 65536.0000 (61264.5380)  weight_decay: 0.0500 (0.0500)  time: 0.3902  data: 0.0005  max mem: 15572
Epoch: [4]  [2180/2272]  eta: 0:00:35  lr: 0.000046  min_lr: 0.000000  loss: 4.5539 (4.6060)  loss_scale: 65536.0000 (61284.1229)  weight_decay: 0.0500 (0.0500)  time: 0.3912  data: 0.0004  max mem: 15572
Epoch: [4]  [2190/2272]  eta: 0:00:31  lr: 0.000047  min_lr: 0.000000  loss: 4.6278 (4.6062)  loss_scale: 65536.0000 (61303.5290)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0004  max mem: 15572
Epoch: [4]  [2200/2272]  eta: 0:00:27  lr: 0.000047  min_lr: 0.000000  loss: 4.6513 (4.6061)  loss_scale: 65536.0000 (61322.7587)  weight_decay: 0.0500 (0.0500)  time: 0.3821  data: 0.0003  max mem: 15572
Epoch: [4]  [2210/2272]  eta: 0:00:23  lr: 0.000047  min_lr: 0.000000  loss: 4.6513 (4.6062)  loss_scale: 65536.0000 (61341.8146)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0003  max mem: 15572
Epoch: [4]  [2220/2272]  eta: 0:00:20  lr: 0.000047  min_lr: 0.000000  loss: 4.5912 (4.6052)  loss_scale: 65536.0000 (61360.6988)  weight_decay: 0.0500 (0.0500)  time: 0.3819  data: 0.0002  max mem: 15572
[2025-01-13 12:21:30,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=78, lr=[4.5244796599844187e-07, 4.5244796599844187e-07, 6.463542371406314e-07, 6.463542371406314e-07, 9.233631959151878e-07, 9.233631959151878e-07, 1.3190902798788397e-06, 1.3190902798788397e-06, 1.8844146855411998e-06, 1.8844146855411998e-06, 2.692020979344571e-06, 2.692020979344571e-06, 3.84574425620653e-06, 3.84574425620653e-06, 5.49392036600933e-06, 5.49392036600933e-06, 7.848457665727614e-06, 7.848457665727614e-06, 1.1212082379610878e-05, 1.1212082379610878e-05, 1.6017260542301254e-05, 1.6017260542301254e-05, 2.2881800774716078e-05, 2.2881800774716078e-05, 3.268828682102297e-05, 3.268828682102297e-05, 4.669755260146139e-05, 4.669755260146139e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 12:21:30,989] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=28.31611353638864, CurrSamplesPerSec=30.90377409116837, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [2230/2272]  eta: 0:00:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5250 (4.6049)  loss_scale: 65536.0000 (61379.4137)  weight_decay: 0.0500 (0.0500)  time: 0.3802  data: 0.0002  max mem: 15572
Epoch: [4]  [2240/2272]  eta: 0:00:12  lr: 0.000047  min_lr: 0.000000  loss: 4.6568 (4.6051)  loss_scale: 65536.0000 (61397.9616)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0002  max mem: 15572
Epoch: [4]  [2250/2272]  eta: 0:00:08  lr: 0.000047  min_lr: 0.000000  loss: 4.6390 (4.6054)  loss_scale: 65536.0000 (61416.3447)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [4]  [2260/2272]  eta: 0:00:04  lr: 0.000047  min_lr: 0.000000  loss: 4.5898 (4.6052)  loss_scale: 65536.0000 (61434.5652)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
[2025-01-13 12:21:45,834] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:21:45,834] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2270/2272]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5884 (4.6048)  loss_scale: 65536.0000 (61539.1986)  weight_decay: 0.0500 (0.0500)  time: 0.3674  data: 0.0001  max mem: 15572
Epoch: [4]  [2271/2272]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5760 (4.6048)  loss_scale: 65536.0000 (61569.8028)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0001  max mem: 15572
Epoch: [4] Total time: 0:14:36 (0.3856 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000000  loss: 4.5760 (4.6048)  loss_scale: 65536.0000 (61569.8028)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 3623
Indices to remove: tensor([    0,     3,     6,  ..., 33462, 33525, 33577], device='cuda:0')
length of data loader train is: 1970
num_training_steps_per_epoch is: 1970
Change step level LR scheduler!
Set warmup steps = 9850
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:09:49  loss: 1.5748 (1.5748)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1679  data: 1.9848  max mem: 15572
Val:  [ 10/272]  eta: 0:02:02  loss: 4.5939 (4.1209)  acc1: 0.0000 (20.7071)  acc5: 5.5556 (28.7879)  time: 0.4658  data: 0.2990  max mem: 15572
Val:  [ 20/272]  eta: 0:01:22  loss: 4.2586 (4.1039)  acc1: 0.0000 (14.8148)  acc5: 11.1111 (26.7196)  time: 0.2341  data: 0.0656  max mem: 15572
Val:  [ 30/272]  eta: 0:01:06  loss: 4.2610 (4.1613)  acc1: 0.0000 (10.0358)  acc5: 11.1111 (24.7312)  time: 0.1707  data: 0.0006  max mem: 15572
Val:  [ 40/272]  eta: 0:00:57  loss: 3.9546 (4.0771)  acc1: 0.0000 (10.4336)  acc5: 22.2222 (28.8618)  time: 0.1666  data: 0.0006  max mem: 15572
Val:  [ 50/272]  eta: 0:00:51  loss: 3.6721 (4.0699)  acc1: 0.0000 (9.8039)  acc5: 38.8889 (31.9172)  time: 0.1629  data: 0.0005  max mem: 15572
Val:  [ 60/272]  eta: 0:00:46  loss: 3.3290 (3.9556)  acc1: 11.1111 (15.3916)  acc5: 61.1111 (36.6120)  time: 0.1636  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:42  loss: 3.3114 (3.8771)  acc1: 22.2222 (17.4491)  acc5: 50.0000 (38.2629)  time: 0.1627  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:39  loss: 3.4926 (3.8831)  acc1: 11.1111 (18.1756)  acc5: 44.4444 (38.0658)  time: 0.1661  data: 0.0006  max mem: 15572
Val:  [ 90/272]  eta: 0:00:37  loss: 4.5352 (3.9601)  acc1: 0.0000 (16.2393)  acc5: 5.5556 (34.3101)  time: 0.1785  data: 0.0006  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 4.5737 (4.0200)  acc1: 0.0000 (15.7316)  acc5: 0.0000 (32.2882)  time: 0.1779  data: 0.0006  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 4.5002 (4.0667)  acc1: 0.0000 (14.3143)  acc5: 0.0000 (31.3313)  time: 0.1707  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 4.4717 (4.1036)  acc1: 0.0000 (13.1313)  acc5: 11.1111 (30.1194)  time: 0.1685  data: 0.0005  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 4.4334 (4.0486)  acc1: 0.0000 (13.8253)  acc5: 22.2222 (32.0187)  time: 0.1615  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 3.8831 (4.0286)  acc1: 0.0000 (14.7754)  acc5: 38.8889 (32.5059)  time: 0.1613  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 4.0242 (4.0392)  acc1: 0.0000 (13.8705)  acc5: 22.2222 (31.4570)  time: 0.1686  data: 0.0005  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 4.0140 (4.0198)  acc1: 0.0000 (14.7688)  acc5: 27.7778 (33.2298)  time: 0.1747  data: 0.0007  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 4.0283 (4.0464)  acc1: 0.0000 (14.0676)  acc5: 33.3333 (32.6511)  time: 0.1724  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 4.2882 (4.0495)  acc1: 0.0000 (13.9349)  acc5: 16.6667 (32.3204)  time: 0.1610  data: 0.0005  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 4.4895 (4.0754)  acc1: 0.0000 (13.2344)  acc5: 11.1111 (31.3264)  time: 0.1598  data: 0.0005  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 4.3902 (4.0744)  acc1: 0.0000 (12.8800)  acc5: 16.6667 (32.5594)  time: 0.1666  data: 0.0005  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.8007 (4.0866)  acc1: 0.0000 (12.8225)  acc5: 44.4444 (32.2275)  time: 0.1680  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 4.3100 (4.0909)  acc1: 0.0000 (12.4686)  acc5: 11.1111 (32.0261)  time: 0.1738  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 4.0551 (4.0701)  acc1: 5.5556 (14.0693)  acc5: 38.8889 (33.3333)  time: 0.1738  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 3.4236 (4.0487)  acc1: 33.3333 (14.6150)  acc5: 88.8889 (34.9700)  time: 0.1635  data: 0.0006  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 3.9353 (4.0761)  acc1: 0.0000 (14.0770)  acc5: 33.3333 (34.2851)  time: 0.1597  data: 0.0005  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 3.9353 (4.0191)  acc1: 5.5556 (15.9004)  acc5: 66.6667 (36.2495)  time: 0.1522  data: 0.0004  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.5391 (4.0196)  acc1: 33.3333 (15.6622)  acc5: 66.6667 (36.2239)  time: 0.1402  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.5391 (4.0223)  acc1: 33.3333 (15.6461)  acc5: 61.1111 (36.1868)  time: 0.1346  data: 0.0001  max mem: 15572
Val: Total time: 0:00:48 (0.1777 s / it)
* Acc@1 15.646 Acc@5 36.187 loss 4.022
Accuracy of the network on the 4883 val videos: 15.6%
[2025-01-13 12:22:36,242] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 12:22:36,244] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 12:22:36,244] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 12:22:38,712] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 12:22:38,712] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 15.65%
Epoch: [5]  [   0/1970]  eta: 1:48:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5330 (4.5330)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 3.2941  data: 2.8914  max mem: 15572
[2025-01-13 12:22:42,931] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13044
[2025-01-13 12:22:42,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:22:42,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [  10/1970]  eta: 0:22:43  lr: 0.000047  min_lr: 0.000000  loss: 4.6452 (4.6234)  loss_scale: 65536.0000 (71493.8182)  weight_decay: 0.0500 (0.0500)  time: 0.6955  data: 0.3075  max mem: 15572
[2025-01-13 12:22:49,862] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13062
[2025-01-13 12:22:49,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 12:22:49,862] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [  20/1970]  eta: 0:17:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5522 (4.5569)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4089  data: 0.0248  max mem: 15572
Epoch: [5]  [  30/1970]  eta: 0:16:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5120 (4.5625)  loss_scale: 32768.0000 (54965.6774)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0004  max mem: 15572
Epoch: [5]  [  40/1970]  eta: 0:15:09  lr: 0.000047  min_lr: 0.000000  loss: 4.6675 (4.5984)  loss_scale: 32768.0000 (49551.6098)  weight_decay: 0.0500 (0.0500)  time: 0.3926  data: 0.0005  max mem: 15572
Epoch: [5]  [  50/1970]  eta: 0:14:37  lr: 0.000047  min_lr: 0.000000  loss: 4.6446 (4.6062)  loss_scale: 32768.0000 (46260.7059)  weight_decay: 0.0500 (0.0500)  time: 0.3980  data: 0.0005  max mem: 15572
Epoch: [5]  [  60/1970]  eta: 0:14:10  lr: 0.000047  min_lr: 0.000000  loss: 4.4898 (4.6101)  loss_scale: 32768.0000 (44048.7869)  weight_decay: 0.0500 (0.0500)  time: 0.3914  data: 0.0005  max mem: 15572
Epoch: [5]  [  70/1970]  eta: 0:13:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5361 (4.6054)  loss_scale: 32768.0000 (42459.9437)  weight_decay: 0.0500 (0.0500)  time: 0.3896  data: 0.0006  max mem: 15572
Epoch: [5]  [  80/1970]  eta: 0:13:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6229 (4.6065)  loss_scale: 32768.0000 (41263.4074)  weight_decay: 0.0500 (0.0500)  time: 0.3990  data: 0.0006  max mem: 15572
Epoch: [5]  [  90/1970]  eta: 0:13:27  lr: 0.000047  min_lr: 0.000000  loss: 4.5107 (4.5848)  loss_scale: 32768.0000 (40329.8462)  weight_decay: 0.0500 (0.0500)  time: 0.3983  data: 0.0006  max mem: 15572
Epoch: [5]  [ 100/1970]  eta: 0:13:12  lr: 0.000047  min_lr: 0.000000  loss: 4.3744 (4.5805)  loss_scale: 32768.0000 (39581.1485)  weight_decay: 0.0500 (0.0500)  time: 0.3832  data: 0.0004  max mem: 15572
Epoch: [5]  [ 110/1970]  eta: 0:13:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5236 (4.5793)  loss_scale: 32768.0000 (38967.3514)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [5]  [ 120/1970]  eta: 0:12:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4878 (4.5655)  loss_scale: 32768.0000 (38455.0083)  weight_decay: 0.0500 (0.0500)  time: 0.3789  data: 0.0003  max mem: 15572
Epoch: [5]  [ 130/1970]  eta: 0:12:41  lr: 0.000047  min_lr: 0.000000  loss: 4.4338 (4.5590)  loss_scale: 32768.0000 (38020.8855)  weight_decay: 0.0500 (0.0500)  time: 0.3807  data: 0.0003  max mem: 15572
Epoch: [5]  [ 140/1970]  eta: 0:12:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5028 (4.5572)  loss_scale: 32768.0000 (37648.3404)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0003  max mem: 15572
[2025-01-13 12:23:39,797] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:23:39,797] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 150/1970]  eta: 0:12:24  lr: 0.000047  min_lr: 0.000000  loss: 4.5111 (4.5524)  loss_scale: 32768.0000 (37976.1589)  weight_decay: 0.0500 (0.0500)  time: 0.3812  data: 0.0003  max mem: 15572
Epoch: [5]  [ 160/1970]  eta: 0:12:17  lr: 0.000047  min_lr: 0.000000  loss: 4.5718 (4.5529)  loss_scale: 65536.0000 (39687.9503)  weight_decay: 0.0500 (0.0500)  time: 0.3810  data: 0.0002  max mem: 15572
Epoch: [5]  [ 170/1970]  eta: 0:12:09  lr: 0.000047  min_lr: 0.000000  loss: 4.7085 (4.5655)  loss_scale: 65536.0000 (41199.5322)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0002  max mem: 15572
Epoch: [5]  [ 180/1970]  eta: 0:12:03  lr: 0.000047  min_lr: 0.000000  loss: 4.6888 (4.5677)  loss_scale: 65536.0000 (42544.0884)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0003  max mem: 15572
Epoch: [5]  [ 190/1970]  eta: 0:11:57  lr: 0.000047  min_lr: 0.000000  loss: 4.5422 (4.5654)  loss_scale: 65536.0000 (43747.8534)  weight_decay: 0.0500 (0.0500)  time: 0.3823  data: 0.0004  max mem: 15572
Epoch: [5]  [ 200/1970]  eta: 0:11:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5062 (4.5569)  loss_scale: 65536.0000 (44831.8408)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0004  max mem: 15572
Epoch: [5]  [ 210/1970]  eta: 0:11:48  lr: 0.000047  min_lr: 0.000000  loss: 4.5390 (4.5562)  loss_scale: 65536.0000 (45813.0806)  weight_decay: 0.0500 (0.0500)  time: 0.3948  data: 0.0005  max mem: 15572
Epoch: [5]  [ 220/1970]  eta: 0:11:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5390 (4.5540)  loss_scale: 65536.0000 (46705.5204)  weight_decay: 0.0500 (0.0500)  time: 0.3946  data: 0.0005  max mem: 15572
Epoch: [5]  [ 230/1970]  eta: 0:11:38  lr: 0.000047  min_lr: 0.000000  loss: 4.5867 (4.5557)  loss_scale: 65536.0000 (47520.6926)  weight_decay: 0.0500 (0.0500)  time: 0.3908  data: 0.0004  max mem: 15572
Epoch: [5]  [ 240/1970]  eta: 0:11:32  lr: 0.000047  min_lr: 0.000000  loss: 4.6003 (4.5540)  loss_scale: 65536.0000 (48268.2158)  weight_decay: 0.0500 (0.0500)  time: 0.3818  data: 0.0003  max mem: 15572
Epoch: [5]  [ 250/1970]  eta: 0:11:26  lr: 0.000047  min_lr: 0.000000  loss: 4.6003 (4.5593)  loss_scale: 65536.0000 (48956.1753)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
Epoch: [5]  [ 260/1970]  eta: 0:11:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6094 (4.5570)  loss_scale: 65536.0000 (49591.4176)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
Epoch: [5]  [ 270/1970]  eta: 0:11:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5339 (4.5530)  loss_scale: 65536.0000 (50179.7786)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0002  max mem: 15572
[2025-01-13 12:24:28,721] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:24:28,721] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 280/1970]  eta: 0:11:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5591 (4.5538)  loss_scale: 65536.0000 (51892.3843)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [5]  [ 290/1970]  eta: 0:11:04  lr: 0.000047  min_lr: 0.000000  loss: 4.6917 (4.5549)  loss_scale: 131072.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
[2025-01-13 12:24:36,999] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13341
[2025-01-13 12:24:36,999] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:24:36,999] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 300/1970]  eta: 0:11:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5323 (4.5529)  loss_scale: 131072.0000 (56500.3056)  weight_decay: 0.0500 (0.0500)  time: 0.3788  data: 0.0003  max mem: 15572
Epoch: [5]  [ 310/1970]  eta: 0:10:54  lr: 0.000047  min_lr: 0.000000  loss: 4.5070 (4.5549)  loss_scale: 65536.0000 (56790.8424)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0003  max mem: 15572
Epoch: [5]  [ 320/1970]  eta: 0:10:50  lr: 0.000047  min_lr: 0.000000  loss: 4.5528 (4.5562)  loss_scale: 65536.0000 (57063.2773)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0004  max mem: 15572
Epoch: [5]  [ 330/1970]  eta: 0:10:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6194 (4.5588)  loss_scale: 65536.0000 (57319.2508)  weight_decay: 0.0500 (0.0500)  time: 0.3910  data: 0.0005  max mem: 15572
Epoch: [5]  [ 340/1970]  eta: 0:10:43  lr: 0.000047  min_lr: 0.000000  loss: 4.6372 (4.5606)  loss_scale: 65536.0000 (57560.2111)  weight_decay: 0.0500 (0.0500)  time: 0.3985  data: 0.0005  max mem: 15572
Epoch: [5]  [ 350/1970]  eta: 0:10:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6133 (4.5613)  loss_scale: 65536.0000 (57787.4416)  weight_decay: 0.0500 (0.0500)  time: 0.3983  data: 0.0006  max mem: 15572
Epoch: [5]  [ 360/1970]  eta: 0:10:35  lr: 0.000047  min_lr: 0.000000  loss: 4.4211 (4.5539)  loss_scale: 65536.0000 (58002.0831)  weight_decay: 0.0500 (0.0500)  time: 0.3942  data: 0.0007  max mem: 15572
Epoch: [5]  [ 370/1970]  eta: 0:10:31  lr: 0.000047  min_lr: 0.000000  loss: 4.3375 (4.5532)  loss_scale: 65536.0000 (58205.1536)  weight_decay: 0.0500 (0.0500)  time: 0.3954  data: 0.0006  max mem: 15572
Epoch: [5]  [ 380/1970]  eta: 0:10:27  lr: 0.000047  min_lr: 0.000000  loss: 4.5230 (4.5529)  loss_scale: 65536.0000 (58397.5643)  weight_decay: 0.0500 (0.0500)  time: 0.3943  data: 0.0004  max mem: 15572
Epoch: [5]  [ 390/1970]  eta: 0:10:22  lr: 0.000047  min_lr: 0.000000  loss: 4.6153 (4.5545)  loss_scale: 65536.0000 (58580.1330)  weight_decay: 0.0500 (0.0500)  time: 0.3871  data: 0.0003  max mem: 15572
Epoch: [5]  [ 400/1970]  eta: 0:10:18  lr: 0.000047  min_lr: 0.000000  loss: 4.6259 (4.5565)  loss_scale: 65536.0000 (58753.5960)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0003  max mem: 15572
Epoch: [5]  [ 410/1970]  eta: 0:10:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5672 (4.5548)  loss_scale: 65536.0000 (58918.6180)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0003  max mem: 15572
Epoch: [5]  [ 420/1970]  eta: 0:10:09  lr: 0.000047  min_lr: 0.000000  loss: 4.4494 (4.5522)  loss_scale: 65536.0000 (59075.8005)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
[2025-01-13 12:25:26,897] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:25:26,897] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:25:28,030] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13473
[2025-01-13 12:25:28,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:25:28,030] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 430/1970]  eta: 0:10:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4494 (4.5522)  loss_scale: 65536.0000 (59681.8561)  weight_decay: 0.0500 (0.0500)  time: 0.3780  data: 0.0003  max mem: 15572
Epoch: [5]  [ 440/1970]  eta: 0:10:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5772 (4.5520)  loss_scale: 65536.0000 (59814.6032)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0003  max mem: 15572
Epoch: [5]  [ 450/1970]  eta: 0:09:55  lr: 0.000047  min_lr: 0.000000  loss: 4.6709 (4.5543)  loss_scale: 65536.0000 (59941.4634)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0003  max mem: 15572
Epoch: [5]  [ 460/1970]  eta: 0:09:51  lr: 0.000047  min_lr: 0.000000  loss: 4.6642 (4.5542)  loss_scale: 65536.0000 (60062.8200)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [5]  [ 470/1970]  eta: 0:09:47  lr: 0.000047  min_lr: 0.000000  loss: 4.4440 (4.5527)  loss_scale: 65536.0000 (60179.0234)  weight_decay: 0.0500 (0.0500)  time: 0.3829  data: 0.0004  max mem: 15572
Epoch: [5]  [ 480/1970]  eta: 0:09:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5875 (4.5546)  loss_scale: 65536.0000 (60290.3950)  weight_decay: 0.0500 (0.0500)  time: 0.3929  data: 0.0006  max mem: 15572
Epoch: [5]  [ 490/1970]  eta: 0:09:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6360 (4.5549)  loss_scale: 65536.0000 (60397.2301)  weight_decay: 0.0500 (0.0500)  time: 0.3932  data: 0.0006  max mem: 15572
Epoch: [5]  [ 500/1970]  eta: 0:09:35  lr: 0.000047  min_lr: 0.000000  loss: 4.5530 (4.5545)  loss_scale: 65536.0000 (60499.8004)  weight_decay: 0.0500 (0.0500)  time: 0.3957  data: 0.0005  max mem: 15572
Epoch: [5]  [ 510/1970]  eta: 0:09:31  lr: 0.000047  min_lr: 0.000000  loss: 4.5366 (4.5525)  loss_scale: 65536.0000 (60598.3562)  weight_decay: 0.0500 (0.0500)  time: 0.3956  data: 0.0006  max mem: 15572
Epoch: [5]  [ 520/1970]  eta: 0:09:28  lr: 0.000047  min_lr: 0.000000  loss: 4.5366 (4.5529)  loss_scale: 65536.0000 (60693.1286)  weight_decay: 0.0500 (0.0500)  time: 0.3926  data: 0.0007  max mem: 15572
Epoch: [5]  [ 530/1970]  eta: 0:09:23  lr: 0.000047  min_lr: 0.000000  loss: 4.4961 (4.5528)  loss_scale: 65536.0000 (60784.3315)  weight_decay: 0.0500 (0.0500)  time: 0.3847  data: 0.0005  max mem: 15572
Epoch: [5]  [ 540/1970]  eta: 0:09:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5501 (4.5511)  loss_scale: 65536.0000 (60872.1627)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0003  max mem: 15572
Epoch: [5]  [ 550/1970]  eta: 0:09:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5373 (4.5476)  loss_scale: 65536.0000 (60956.8058)  weight_decay: 0.0500 (0.0500)  time: 0.3780  data: 0.0003  max mem: 15572
[2025-01-13 12:26:17,615] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:26:17,615] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 560/1970]  eta: 0:09:10  lr: 0.000047  min_lr: 0.000000  loss: 4.4213 (4.5439)  loss_scale: 65536.0000 (61272.0713)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0003  max mem: 15572
[2025-01-13 12:26:21,363] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13612
[2025-01-13 12:26:21,363] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:26:21,364] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 570/1970]  eta: 0:09:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5584 (4.5462)  loss_scale: 65536.0000 (62264.9387)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [5]  [ 580/1970]  eta: 0:09:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6062 (4.5462)  loss_scale: 65536.0000 (62321.2392)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
Epoch: [5]  [ 590/1970]  eta: 0:08:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5760 (4.5472)  loss_scale: 65536.0000 (62375.6345)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0003  max mem: 15572
Epoch: [5]  [ 600/1970]  eta: 0:08:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5345 (4.5450)  loss_scale: 65536.0000 (62428.2196)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [5]  [ 610/1970]  eta: 0:08:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3455 (4.5411)  loss_scale: 65536.0000 (62479.0835)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0003  max mem: 15572
Epoch: [5]  [ 620/1970]  eta: 0:08:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4734 (4.5410)  loss_scale: 65536.0000 (62528.3092)  weight_decay: 0.0500 (0.0500)  time: 0.3933  data: 0.0004  max mem: 15572
Epoch: [5]  [ 630/1970]  eta: 0:08:42  lr: 0.000047  min_lr: 0.000000  loss: 4.4734 (4.5389)  loss_scale: 65536.0000 (62575.9746)  weight_decay: 0.0500 (0.0500)  time: 0.3996  data: 0.0023  max mem: 15572
Epoch: [5]  [ 640/1970]  eta: 0:08:38  lr: 0.000047  min_lr: 0.000000  loss: 4.5342 (4.5407)  loss_scale: 65536.0000 (62622.1529)  weight_decay: 0.0500 (0.0500)  time: 0.4002  data: 0.0022  max mem: 15572
Epoch: [5]  [ 650/1970]  eta: 0:08:35  lr: 0.000047  min_lr: 0.000000  loss: 4.5639 (4.5398)  loss_scale: 65536.0000 (62666.9124)  weight_decay: 0.0500 (0.0500)  time: 0.4029  data: 0.0005  max mem: 15572
Epoch: [5]  [ 660/1970]  eta: 0:08:31  lr: 0.000047  min_lr: 0.000000  loss: 4.4713 (4.5382)  loss_scale: 65536.0000 (62710.3177)  weight_decay: 0.0500 (0.0500)  time: 0.4003  data: 0.0006  max mem: 15572
Epoch: [5]  [ 670/1970]  eta: 0:08:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4965 (4.5389)  loss_scale: 65536.0000 (62752.4292)  weight_decay: 0.0500 (0.0500)  time: 0.3918  data: 0.0005  max mem: 15572
Epoch: [5]  [ 680/1970]  eta: 0:08:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5793 (4.5389)  loss_scale: 65536.0000 (62793.3040)  weight_decay: 0.0500 (0.0500)  time: 0.3795  data: 0.0004  max mem: 15572
Epoch: [5]  [ 690/1970]  eta: 0:08:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5288 (4.5393)  loss_scale: 65536.0000 (62832.9957)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0003  max mem: 15572
[2025-01-13 12:27:11,325] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:27:11,326] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:27:12,069] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13743
[2025-01-13 12:27:12,070] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:27:12,070] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 700/1970]  eta: 0:08:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5192 (4.5399)  loss_scale: 65536.0000 (63058.5335)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0003  max mem: 15572
Epoch: [5]  [ 710/1970]  eta: 0:08:10  lr: 0.000047  min_lr: 0.000000  loss: 4.4553 (4.5397)  loss_scale: 65536.0000 (63093.3783)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0003  max mem: 15572
Epoch: [5]  [ 720/1970]  eta: 0:08:06  lr: 0.000047  min_lr: 0.000000  loss: 4.4247 (4.5387)  loss_scale: 65536.0000 (63127.2566)  weight_decay: 0.0500 (0.0500)  time: 0.3779  data: 0.0003  max mem: 15572
Epoch: [5]  [ 730/1970]  eta: 0:08:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6541 (4.5410)  loss_scale: 65536.0000 (63160.2079)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0002  max mem: 15572
Epoch: [5]  [ 740/1970]  eta: 0:07:58  lr: 0.000047  min_lr: 0.000000  loss: 4.6925 (4.5406)  loss_scale: 65536.0000 (63192.2699)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0002  max mem: 15572
Epoch: [5]  [ 750/1970]  eta: 0:07:54  lr: 0.000047  min_lr: 0.000000  loss: 4.5828 (4.5412)  loss_scale: 65536.0000 (63223.4780)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0002  max mem: 15572
Epoch: [5]  [ 760/1970]  eta: 0:07:50  lr: 0.000047  min_lr: 0.000000  loss: 4.5541 (4.5401)  loss_scale: 65536.0000 (63253.8660)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0003  max mem: 15572
Epoch: [5]  [ 770/1970]  eta: 0:07:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4911 (4.5398)  loss_scale: 65536.0000 (63283.4656)  weight_decay: 0.0500 (0.0500)  time: 0.3856  data: 0.0005  max mem: 15572
Epoch: [5]  [ 780/1970]  eta: 0:07:42  lr: 0.000047  min_lr: 0.000000  loss: 4.4911 (4.5385)  loss_scale: 65536.0000 (63312.3073)  weight_decay: 0.0500 (0.0500)  time: 0.3915  data: 0.0006  max mem: 15572
Epoch: [5]  [ 790/1970]  eta: 0:07:39  lr: 0.000047  min_lr: 0.000000  loss: 4.4040 (4.5361)  loss_scale: 65536.0000 (63340.4197)  weight_decay: 0.0500 (0.0500)  time: 0.4010  data: 0.0006  max mem: 15572
Epoch: [5]  [ 800/1970]  eta: 0:07:35  lr: 0.000047  min_lr: 0.000000  loss: 4.4454 (4.5382)  loss_scale: 65536.0000 (63367.8302)  weight_decay: 0.0500 (0.0500)  time: 0.4004  data: 0.0007  max mem: 15572
Epoch: [5]  [ 810/1970]  eta: 0:07:31  lr: 0.000047  min_lr: 0.000000  loss: 4.7112 (4.5383)  loss_scale: 65536.0000 (63394.5647)  weight_decay: 0.0500 (0.0500)  time: 0.3935  data: 0.0006  max mem: 15572
Epoch: [5]  [ 820/1970]  eta: 0:07:27  lr: 0.000047  min_lr: 0.000000  loss: 4.6725 (4.5398)  loss_scale: 65536.0000 (63420.6480)  weight_decay: 0.0500 (0.0500)  time: 0.3983  data: 0.0005  max mem: 15572
[2025-01-13 12:28:01,989] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:28:01,989] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 830/1970]  eta: 0:07:23  lr: 0.000047  min_lr: 0.000000  loss: 4.6794 (4.5418)  loss_scale: 65536.0000 (63603.8315)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0004  max mem: 15572
[2025-01-13 12:28:03,480] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13876
[2025-01-13 12:28:03,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:28:03,481] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 840/1970]  eta: 0:07:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5960 (4.5399)  loss_scale: 65536.0000 (63782.6587)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0003  max mem: 15572
Epoch: [5]  [ 850/1970]  eta: 0:07:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5847 (4.5404)  loss_scale: 65536.0000 (63803.2620)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0003  max mem: 15572
Epoch: [5]  [ 860/1970]  eta: 0:07:11  lr: 0.000047  min_lr: 0.000000  loss: 4.4695 (4.5378)  loss_scale: 65536.0000 (63823.3868)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [5]  [ 870/1970]  eta: 0:07:07  lr: 0.000047  min_lr: 0.000000  loss: 4.3830 (4.5367)  loss_scale: 65536.0000 (63843.0494)  weight_decay: 0.0500 (0.0500)  time: 0.3780  data: 0.0003  max mem: 15572
Epoch: [5]  [ 880/1970]  eta: 0:07:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4342 (4.5354)  loss_scale: 65536.0000 (63862.2656)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
Epoch: [5]  [ 890/1970]  eta: 0:06:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5167 (4.5348)  loss_scale: 65536.0000 (63881.0505)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0003  max mem: 15572
Epoch: [5]  [ 900/1970]  eta: 0:06:55  lr: 0.000047  min_lr: 0.000000  loss: 4.5786 (4.5363)  loss_scale: 65536.0000 (63899.4184)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [5]  [ 910/1970]  eta: 0:06:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5786 (4.5351)  loss_scale: 65536.0000 (63917.3831)  weight_decay: 0.0500 (0.0500)  time: 0.3868  data: 0.0003  max mem: 15572
Epoch: [5]  [ 920/1970]  eta: 0:06:47  lr: 0.000047  min_lr: 0.000000  loss: 4.2729 (4.5319)  loss_scale: 65536.0000 (63934.9577)  weight_decay: 0.0500 (0.0500)  time: 0.3922  data: 0.0004  max mem: 15572
Epoch: [5]  [ 930/1970]  eta: 0:06:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4534 (4.5313)  loss_scale: 65536.0000 (63952.1547)  weight_decay: 0.0500 (0.0500)  time: 0.3891  data: 0.0006  max mem: 15572
Epoch: [5]  [ 940/1970]  eta: 0:06:40  lr: 0.000047  min_lr: 0.000000  loss: 4.5380 (4.5331)  loss_scale: 65536.0000 (63968.9862)  weight_decay: 0.0500 (0.0500)  time: 0.3959  data: 0.0006  max mem: 15572
Epoch: [5]  [ 950/1970]  eta: 0:06:36  lr: 0.000047  min_lr: 0.000000  loss: 4.6148 (4.5334)  loss_scale: 65536.0000 (63985.4637)  weight_decay: 0.0500 (0.0500)  time: 0.4012  data: 0.0006  max mem: 15572
[2025-01-13 12:28:50,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=85, lr=[4.5395205741487987e-07, 4.5395205741487987e-07, 6.485029391641142e-07, 6.485029391641142e-07, 9.264327702344489e-07, 9.264327702344489e-07, 1.3234753860492128e-06, 1.3234753860492128e-06, 1.890679122927447e-06, 1.890679122927447e-06, 2.7009701756106387e-06, 2.7009701756106387e-06, 3.858528822300913e-06, 3.858528822300913e-06, 5.512184031858447e-06, 5.512184031858447e-06, 7.874548616940638e-06, 7.874548616940638e-06, 1.1249355167058056e-05, 1.1249355167058056e-05, 1.6070507381511507e-05, 1.6070507381511507e-05, 2.2957867687873585e-05, 2.2957867687873585e-05, 3.2796953839819414e-05, 3.2796953839819414e-05, 4.685279119974202e-05, 4.685279119974202e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 12:28:50,850] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=28.605156497610096, CurrSamplesPerSec=32.13755162400104, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [5]  [ 960/1970]  eta: 0:06:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5871 (4.5326)  loss_scale: 65536.0000 (64001.5983)  weight_decay: 0.0500 (0.0500)  time: 0.4003  data: 0.0006  max mem: 15572
[2025-01-13 12:28:53,233] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:28:53,233] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:28:56,237] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14013
[2025-01-13 12:28:56,237] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:28:56,237] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 970/1970]  eta: 0:06:28  lr: 0.000047  min_lr: 0.000000  loss: 4.5203 (4.5312)  loss_scale: 65536.0000 (64557.3471)  weight_decay: 0.0500 (0.0500)  time: 0.3901  data: 0.0004  max mem: 15572
Epoch: [5]  [ 980/1970]  eta: 0:06:24  lr: 0.000047  min_lr: 0.000000  loss: 4.4431 (4.5299)  loss_scale: 65536.0000 (64567.3231)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [5]  [ 990/1970]  eta: 0:06:20  lr: 0.000047  min_lr: 0.000000  loss: 4.4366 (4.5295)  loss_scale: 65536.0000 (64577.0979)  weight_decay: 0.0500 (0.0500)  time: 0.3724  data: 0.0002  max mem: 15572
Epoch: [5]  [1000/1970]  eta: 0:06:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4684 (4.5295)  loss_scale: 65536.0000 (64586.6773)  weight_decay: 0.0500 (0.0500)  time: 0.3836  data: 0.0003  max mem: 15572
Epoch: [5]  [1010/1970]  eta: 0:06:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5250 (4.5302)  loss_scale: 65536.0000 (64596.0673)  weight_decay: 0.0500 (0.0500)  time: 0.3839  data: 0.0003  max mem: 15572
Epoch: [5]  [1020/1970]  eta: 0:06:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5441 (4.5295)  loss_scale: 65536.0000 (64605.2733)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
Epoch: [5]  [1030/1970]  eta: 0:06:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4672 (4.5293)  loss_scale: 65536.0000 (64614.3007)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0003  max mem: 15572
Epoch: [5]  [1040/1970]  eta: 0:06:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5255 (4.5298)  loss_scale: 65536.0000 (64623.1547)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [5]  [1050/1970]  eta: 0:05:56  lr: 0.000047  min_lr: 0.000000  loss: 4.5724 (4.5299)  loss_scale: 65536.0000 (64631.8402)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0003  max mem: 15572
Epoch: [5]  [1060/1970]  eta: 0:05:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5339 (4.5300)  loss_scale: 65536.0000 (64640.3619)  weight_decay: 0.0500 (0.0500)  time: 0.3913  data: 0.0020  max mem: 15572
Epoch: [5]  [1070/1970]  eta: 0:05:49  lr: 0.000047  min_lr: 0.000000  loss: 4.4622 (4.5290)  loss_scale: 65536.0000 (64648.7246)  weight_decay: 0.0500 (0.0500)  time: 0.4024  data: 0.0021  max mem: 15572
Epoch: [5]  [1080/1970]  eta: 0:05:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4361 (4.5281)  loss_scale: 65536.0000 (64656.9325)  weight_decay: 0.0500 (0.0500)  time: 0.3979  data: 0.0005  max mem: 15572
Epoch: [5]  [1090/1970]  eta: 0:05:41  lr: 0.000047  min_lr: 0.000000  loss: 4.5204 (4.5276)  loss_scale: 65536.0000 (64664.9899)  weight_decay: 0.0500 (0.0500)  time: 0.3961  data: 0.0004  max mem: 15572
[2025-01-13 12:29:45,927] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:29:45,927] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1100/1970]  eta: 0:05:37  lr: 0.000047  min_lr: 0.000000  loss: 4.6181 (4.5290)  loss_scale: 65536.0000 (64791.9491)  weight_decay: 0.0500 (0.0500)  time: 0.3927  data: 0.0004  max mem: 15572
[2025-01-13 12:29:47,485] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14146
[2025-01-13 12:29:47,485] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:29:47,485] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1110/1970]  eta: 0:05:33  lr: 0.000047  min_lr: 0.000000  loss: 4.6953 (4.5297)  loss_scale: 65536.0000 (64916.6229)  weight_decay: 0.0500 (0.0500)  time: 0.3802  data: 0.0003  max mem: 15572
Epoch: [5]  [1120/1970]  eta: 0:05:29  lr: 0.000047  min_lr: 0.000000  loss: 4.6016 (4.5295)  loss_scale: 65536.0000 (64922.1481)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [5]  [1130/1970]  eta: 0:05:25  lr: 0.000047  min_lr: 0.000000  loss: 4.5537 (4.5288)  loss_scale: 65536.0000 (64927.5756)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
Epoch: [5]  [1140/1970]  eta: 0:05:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5223 (4.5290)  loss_scale: 65536.0000 (64932.9080)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [5]  [1150/1970]  eta: 0:05:17  lr: 0.000047  min_lr: 0.000000  loss: 4.6330 (4.5299)  loss_scale: 65536.0000 (64938.1477)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0003  max mem: 15572
Epoch: [5]  [1160/1970]  eta: 0:05:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5987 (4.5300)  loss_scale: 65536.0000 (64943.2972)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0004  max mem: 15572
Epoch: [5]  [1170/1970]  eta: 0:05:09  lr: 0.000047  min_lr: 0.000000  loss: 4.4251 (4.5289)  loss_scale: 65536.0000 (64948.3587)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0003  max mem: 15572
Epoch: [5]  [1180/1970]  eta: 0:05:06  lr: 0.000047  min_lr: 0.000000  loss: 4.4251 (4.5293)  loss_scale: 65536.0000 (64953.3345)  weight_decay: 0.0500 (0.0500)  time: 0.3773  data: 0.0002  max mem: 15572
Epoch: [5]  [1190/1970]  eta: 0:05:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5171 (4.5287)  loss_scale: 65536.0000 (64958.2267)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0003  max mem: 15572
Epoch: [5]  [1200/1970]  eta: 0:04:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5460 (4.5292)  loss_scale: 65536.0000 (64963.0375)  weight_decay: 0.0500 (0.0500)  time: 0.3846  data: 0.0004  max mem: 15572
Epoch: [5]  [1210/1970]  eta: 0:04:54  lr: 0.000047  min_lr: 0.000000  loss: 4.5709 (4.5293)  loss_scale: 65536.0000 (64967.7688)  weight_decay: 0.0500 (0.0500)  time: 0.3982  data: 0.0005  max mem: 15572
Epoch: [5]  [1220/1970]  eta: 0:04:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4732 (4.5282)  loss_scale: 65536.0000 (64972.4226)  weight_decay: 0.0500 (0.0500)  time: 0.4017  data: 0.0005  max mem: 15572
Epoch: [5]  [1230/1970]  eta: 0:04:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4180 (4.5276)  loss_scale: 65536.0000 (64977.0008)  weight_decay: 0.0500 (0.0500)  time: 0.3967  data: 0.0005  max mem: 15572
[2025-01-13 12:30:36,845] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:30:36,845] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:30:37,994] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14278
[2025-01-13 12:30:37,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:30:37,994] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1240/1970]  eta: 0:04:42  lr: 0.000047  min_lr: 0.000000  loss: 4.4236 (4.5270)  loss_scale: 65536.0000 (65139.9323)  weight_decay: 0.0500 (0.0500)  time: 0.3929  data: 0.0005  max mem: 15572
Epoch: [5]  [1250/1970]  eta: 0:04:39  lr: 0.000047  min_lr: 0.000000  loss: 4.4884 (4.5275)  loss_scale: 65536.0000 (65143.0983)  weight_decay: 0.0500 (0.0500)  time: 0.3909  data: 0.0005  max mem: 15572
Epoch: [5]  [1260/1970]  eta: 0:04:35  lr: 0.000047  min_lr: 0.000000  loss: 4.4913 (4.5273)  loss_scale: 65536.0000 (65146.2141)  weight_decay: 0.0500 (0.0500)  time: 0.3893  data: 0.0004  max mem: 15572
Epoch: [5]  [1270/1970]  eta: 0:04:31  lr: 0.000047  min_lr: 0.000000  loss: 4.4165 (4.5260)  loss_scale: 65536.0000 (65149.2809)  weight_decay: 0.0500 (0.0500)  time: 0.3827  data: 0.0003  max mem: 15572
Epoch: [5]  [1280/1970]  eta: 0:04:27  lr: 0.000047  min_lr: 0.000000  loss: 4.3447 (4.5264)  loss_scale: 65536.0000 (65152.2998)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0003  max mem: 15572
Epoch: [5]  [1290/1970]  eta: 0:04:23  lr: 0.000047  min_lr: 0.000000  loss: 4.3767 (4.5256)  loss_scale: 65536.0000 (65155.2719)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [5]  [1300/1970]  eta: 0:04:19  lr: 0.000047  min_lr: 0.000000  loss: 4.4213 (4.5252)  loss_scale: 65536.0000 (65158.1983)  weight_decay: 0.0500 (0.0500)  time: 0.3721  data: 0.0002  max mem: 15572
Epoch: [5]  [1310/1970]  eta: 0:04:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5443 (4.5254)  loss_scale: 65536.0000 (65161.0801)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0003  max mem: 15572
Epoch: [5]  [1320/1970]  eta: 0:04:11  lr: 0.000047  min_lr: 0.000000  loss: 4.5997 (4.5255)  loss_scale: 65536.0000 (65163.9182)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0003  max mem: 15572
Epoch: [5]  [1330/1970]  eta: 0:04:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5339 (4.5253)  loss_scale: 65536.0000 (65166.7137)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0003  max mem: 15572
Epoch: [5]  [1340/1970]  eta: 0:04:03  lr: 0.000047  min_lr: 0.000000  loss: 4.3666 (4.5241)  loss_scale: 65536.0000 (65169.4676)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0003  max mem: 15572
Epoch: [5]  [1350/1970]  eta: 0:03:59  lr: 0.000047  min_lr: 0.000000  loss: 4.3414 (4.5233)  loss_scale: 65536.0000 (65172.1806)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0004  max mem: 15572
Epoch: [5]  [1360/1970]  eta: 0:03:56  lr: 0.000047  min_lr: 0.000000  loss: 4.5019 (4.5229)  loss_scale: 65536.0000 (65174.8538)  weight_decay: 0.0500 (0.0500)  time: 0.4007  data: 0.0005  max mem: 15572
[2025-01-13 12:31:27,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:31:27,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:31:28,733] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14410
[2025-01-13 12:31:28,733] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:31:28,733] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1370/1970]  eta: 0:03:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5328 (4.5224)  loss_scale: 65536.0000 (65320.8928)  weight_decay: 0.0500 (0.0500)  time: 0.4025  data: 0.0004  max mem: 15572
Epoch: [5]  [1380/1970]  eta: 0:03:48  lr: 0.000047  min_lr: 0.000000  loss: 4.6195 (4.5225)  loss_scale: 65536.0000 (65322.4504)  weight_decay: 0.0500 (0.0500)  time: 0.4058  data: 0.0004  max mem: 15572
Epoch: [5]  [1390/1970]  eta: 0:03:44  lr: 0.000047  min_lr: 0.000000  loss: 4.6026 (4.5236)  loss_scale: 65536.0000 (65323.9856)  weight_decay: 0.0500 (0.0500)  time: 0.4059  data: 0.0005  max mem: 15572
Epoch: [5]  [1400/1970]  eta: 0:03:40  lr: 0.000047  min_lr: 0.000000  loss: 4.6026 (4.5241)  loss_scale: 65536.0000 (65325.4989)  weight_decay: 0.0500 (0.0500)  time: 0.3938  data: 0.0005  max mem: 15572
Epoch: [5]  [1410/1970]  eta: 0:03:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5609 (4.5245)  loss_scale: 65536.0000 (65326.9908)  weight_decay: 0.0500 (0.0500)  time: 0.3806  data: 0.0003  max mem: 15572
Epoch: [5]  [1420/1970]  eta: 0:03:33  lr: 0.000047  min_lr: 0.000000  loss: 4.4855 (4.5244)  loss_scale: 65536.0000 (65328.4616)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [5]  [1430/1970]  eta: 0:03:29  lr: 0.000047  min_lr: 0.000000  loss: 4.4855 (4.5243)  loss_scale: 65536.0000 (65329.9119)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0002  max mem: 15572
Epoch: [5]  [1440/1970]  eta: 0:03:25  lr: 0.000047  min_lr: 0.000000  loss: 4.4640 (4.5245)  loss_scale: 65536.0000 (65331.3421)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0002  max mem: 15572
Epoch: [5]  [1450/1970]  eta: 0:03:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4640 (4.5243)  loss_scale: 65536.0000 (65332.7526)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0003  max mem: 15572
Epoch: [5]  [1460/1970]  eta: 0:03:17  lr: 0.000047  min_lr: 0.000000  loss: 4.4810 (4.5235)  loss_scale: 65536.0000 (65334.1437)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0003  max mem: 15572
Epoch: [5]  [1470/1970]  eta: 0:03:13  lr: 0.000047  min_lr: 0.000000  loss: 4.4499 (4.5224)  loss_scale: 65536.0000 (65335.5160)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [5]  [1480/1970]  eta: 0:03:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5664 (4.5235)  loss_scale: 65536.0000 (65336.8697)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [5]  [1490/1970]  eta: 0:03:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5820 (4.5227)  loss_scale: 65536.0000 (65338.2052)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0002  max mem: 15572
[2025-01-13 12:32:18,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:32:18,134] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:32:19,322] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14542
[2025-01-13 12:32:19,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:32:19,323] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1500/1970]  eta: 0:03:01  lr: 0.000047  min_lr: 0.000000  loss: 4.3826 (4.5209)  loss_scale: 65536.0000 (65470.5077)  weight_decay: 0.0500 (0.0500)  time: 0.3879  data: 0.0004  max mem: 15572
Epoch: [5]  [1510/1970]  eta: 0:02:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4467 (4.5215)  loss_scale: 65536.0000 (65470.9411)  weight_decay: 0.0500 (0.0500)  time: 0.3949  data: 0.0005  max mem: 15572
Epoch: [5]  [1520/1970]  eta: 0:02:54  lr: 0.000047  min_lr: 0.000000  loss: 4.4917 (4.5216)  loss_scale: 65536.0000 (65471.3688)  weight_decay: 0.0500 (0.0500)  time: 0.4024  data: 0.0005  max mem: 15572
Epoch: [5]  [1530/1970]  eta: 0:02:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4641 (4.5219)  loss_scale: 65536.0000 (65471.7910)  weight_decay: 0.0500 (0.0500)  time: 0.4020  data: 0.0005  max mem: 15572
Epoch: [5]  [1540/1970]  eta: 0:02:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4641 (4.5213)  loss_scale: 65536.0000 (65472.2077)  weight_decay: 0.0500 (0.0500)  time: 0.3956  data: 0.0004  max mem: 15572
Epoch: [5]  [1550/1970]  eta: 0:02:42  lr: 0.000047  min_lr: 0.000000  loss: 4.4689 (4.5214)  loss_scale: 65536.0000 (65472.6190)  weight_decay: 0.0500 (0.0500)  time: 0.3959  data: 0.0005  max mem: 15572
Epoch: [5]  [1560/1970]  eta: 0:02:38  lr: 0.000047  min_lr: 0.000000  loss: 4.4689 (4.5213)  loss_scale: 65536.0000 (65473.0250)  weight_decay: 0.0500 (0.0500)  time: 0.3867  data: 0.0004  max mem: 15572
Epoch: [5]  [1570/1970]  eta: 0:02:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4724 (4.5213)  loss_scale: 65536.0000 (65473.4258)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
Epoch: [5]  [1580/1970]  eta: 0:02:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4075 (4.5204)  loss_scale: 65536.0000 (65473.8216)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0004  max mem: 15572
Epoch: [5]  [1590/1970]  eta: 0:02:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4075 (4.5197)  loss_scale: 65536.0000 (65474.2124)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0003  max mem: 15572
Epoch: [5]  [1600/1970]  eta: 0:02:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5520 (4.5194)  loss_scale: 65536.0000 (65474.5984)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [5]  [1610/1970]  eta: 0:02:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5520 (4.5193)  loss_scale: 65536.0000 (65474.9795)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0003  max mem: 15572
Epoch: [5]  [1620/1970]  eta: 0:02:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5267 (4.5187)  loss_scale: 65536.0000 (65475.3560)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0003  max mem: 15572
[2025-01-13 12:33:09,051] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:33:09,052] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:33:09,800] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14673
[2025-01-13 12:33:09,800] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:33:09,800] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1630/1970]  eta: 0:02:11  lr: 0.000047  min_lr: 0.000000  loss: 4.4484 (4.5182)  loss_scale: 65536.0000 (65556.0907)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0002  max mem: 15572
Epoch: [5]  [1640/1970]  eta: 0:02:07  lr: 0.000047  min_lr: 0.000000  loss: 4.4070 (4.5184)  loss_scale: 65536.0000 (65555.9683)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0002  max mem: 15572
Epoch: [5]  [1650/1970]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4456 (4.5183)  loss_scale: 65536.0000 (65555.8474)  weight_decay: 0.0500 (0.0500)  time: 0.3878  data: 0.0004  max mem: 15572
Epoch: [5]  [1660/1970]  eta: 0:01:59  lr: 0.000047  min_lr: 0.000000  loss: 4.4456 (4.5181)  loss_scale: 65536.0000 (65555.7279)  weight_decay: 0.0500 (0.0500)  time: 0.3929  data: 0.0005  max mem: 15572
Epoch: [5]  [1670/1970]  eta: 0:01:56  lr: 0.000047  min_lr: 0.000000  loss: 4.5008 (4.5181)  loss_scale: 65536.0000 (65555.6098)  weight_decay: 0.0500 (0.0500)  time: 0.3928  data: 0.0004  max mem: 15572
Epoch: [5]  [1680/1970]  eta: 0:01:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5008 (4.5179)  loss_scale: 65536.0000 (65555.4932)  weight_decay: 0.0500 (0.0500)  time: 0.3982  data: 0.0005  max mem: 15572
Epoch: [5]  [1690/1970]  eta: 0:01:48  lr: 0.000047  min_lr: 0.000000  loss: 4.4904 (4.5175)  loss_scale: 65536.0000 (65555.3779)  weight_decay: 0.0500 (0.0500)  time: 0.3940  data: 0.0006  max mem: 15572
Epoch: [5]  [1700/1970]  eta: 0:01:44  lr: 0.000047  min_lr: 0.000000  loss: 4.4758 (4.5170)  loss_scale: 65536.0000 (65555.2640)  weight_decay: 0.0500 (0.0500)  time: 0.3872  data: 0.0005  max mem: 15572
Epoch: [5]  [1710/1970]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4481 (4.5169)  loss_scale: 65536.0000 (65555.1514)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0004  max mem: 15572
Epoch: [5]  [1720/1970]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000000  loss: 4.4661 (4.5166)  loss_scale: 65536.0000 (65555.0401)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0003  max mem: 15572
Epoch: [5]  [1730/1970]  eta: 0:01:32  lr: 0.000047  min_lr: 0.000000  loss: 4.2287 (4.5152)  loss_scale: 65536.0000 (65554.9301)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0003  max mem: 15572
Epoch: [5]  [1740/1970]  eta: 0:01:28  lr: 0.000047  min_lr: 0.000000  loss: 4.2287 (4.5135)  loss_scale: 65536.0000 (65554.8214)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [5]  [1750/1970]  eta: 0:01:25  lr: 0.000047  min_lr: 0.000000  loss: 4.3646 (4.5137)  loss_scale: 65536.0000 (65554.7139)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0003  max mem: 15572
[2025-01-13 12:33:59,183] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:33:59,183] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1760/1970]  eta: 0:01:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4917 (4.5140)  loss_scale: 65536.0000 (65629.0380)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0003  max mem: 15572
[2025-01-13 12:34:00,683] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14806
[2025-01-13 12:34:00,683] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:34:00,683] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1770/1970]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000000  loss: 4.6018 (4.5133)  loss_scale: 65536.0000 (65702.5229)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0003  max mem: 15572
Epoch: [5]  [1780/1970]  eta: 0:01:13  lr: 0.000047  min_lr: 0.000000  loss: 4.3698 (4.5124)  loss_scale: 65536.0000 (65701.5879)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0003  max mem: 15572
Epoch: [5]  [1790/1970]  eta: 0:01:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5596 (4.5121)  loss_scale: 65536.0000 (65700.6633)  weight_decay: 0.0500 (0.0500)  time: 0.3796  data: 0.0003  max mem: 15572
Epoch: [5]  [1800/1970]  eta: 0:01:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5075 (4.5117)  loss_scale: 65536.0000 (65699.7490)  weight_decay: 0.0500 (0.0500)  time: 0.3898  data: 0.0004  max mem: 15572
Epoch: [5]  [1810/1970]  eta: 0:01:01  lr: 0.000047  min_lr: 0.000000  loss: 4.4165 (4.5111)  loss_scale: 65536.0000 (65698.8448)  weight_decay: 0.0500 (0.0500)  time: 0.3911  data: 0.0004  max mem: 15572
Epoch: [5]  [1820/1970]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4436 (4.5106)  loss_scale: 65536.0000 (65697.9506)  weight_decay: 0.0500 (0.0500)  time: 0.3895  data: 0.0004  max mem: 15572
Epoch: [5]  [1830/1970]  eta: 0:00:54  lr: 0.000047  min_lr: 0.000000  loss: 4.4436 (4.5101)  loss_scale: 65536.0000 (65697.0661)  weight_decay: 0.0500 (0.0500)  time: 0.3927  data: 0.0004  max mem: 15572
Epoch: [5]  [1840/1970]  eta: 0:00:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4889 (4.5107)  loss_scale: 65536.0000 (65696.1912)  weight_decay: 0.0500 (0.0500)  time: 0.3931  data: 0.0004  max mem: 15572
Epoch: [5]  [1850/1970]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.5643 (4.5103)  loss_scale: 65536.0000 (65695.3258)  weight_decay: 0.0500 (0.0500)  time: 0.3884  data: 0.0003  max mem: 15572
Epoch: [5]  [1860/1970]  eta: 0:00:42  lr: 0.000047  min_lr: 0.000000  loss: 4.3309 (4.5097)  loss_scale: 65536.0000 (65694.4696)  weight_decay: 0.0500 (0.0500)  time: 0.3822  data: 0.0003  max mem: 15572
Epoch: [5]  [1870/1970]  eta: 0:00:38  lr: 0.000047  min_lr: 0.000000  loss: 4.5289 (4.5108)  loss_scale: 65536.0000 (65693.6227)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0003  max mem: 15572
Epoch: [5]  [1880/1970]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6415 (4.5102)  loss_scale: 65536.0000 (65692.7847)  weight_decay: 0.0500 (0.0500)  time: 0.3797  data: 0.0002  max mem: 15572
Epoch: [5]  [1890/1970]  eta: 0:00:30  lr: 0.000047  min_lr: 0.000000  loss: 4.3843 (4.5098)  loss_scale: 65536.0000 (65691.9556)  weight_decay: 0.0500 (0.0500)  time: 0.3796  data: 0.0002  max mem: 15572
[2025-01-13 12:34:50,250] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:34:50,251] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:34:51,376] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14938
[2025-01-13 12:34:51,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:34:51,376] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1900/1970]  eta: 0:00:27  lr: 0.000047  min_lr: 0.000000  loss: 4.5285 (4.5101)  loss_scale: 65536.0000 (65794.5587)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [5]  [1910/1970]  eta: 0:00:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5501 (4.5101)  loss_scale: 65536.0000 (65793.2057)  weight_decay: 0.0500 (0.0500)  time: 0.3794  data: 0.0003  max mem: 15572
Epoch: [5]  [1920/1970]  eta: 0:00:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5174 (4.5100)  loss_scale: 65536.0000 (65791.8667)  weight_decay: 0.0500 (0.0500)  time: 0.3808  data: 0.0003  max mem: 15572
Epoch: [5]  [1930/1970]  eta: 0:00:15  lr: 0.000047  min_lr: 0.000000  loss: 4.4335 (4.5093)  loss_scale: 65536.0000 (65790.5417)  weight_decay: 0.0500 (0.0500)  time: 0.3794  data: 0.0016  max mem: 15572
Epoch: [5]  [1940/1970]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.4921 (4.5098)  loss_scale: 65536.0000 (65789.2303)  weight_decay: 0.0500 (0.0500)  time: 0.3862  data: 0.0017  max mem: 15572
Epoch: [5]  [1950/1970]  eta: 0:00:07  lr: 0.000047  min_lr: 0.000000  loss: 4.6298 (4.5097)  loss_scale: 65536.0000 (65787.9323)  weight_decay: 0.0500 (0.0500)  time: 0.3899  data: 0.0005  max mem: 15572
[2025-01-13 12:35:14,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=93, lr=[4.5326690419707024e-07, 4.5326690419707024e-07, 6.475241488529576e-07, 6.475241488529576e-07, 9.250344983613681e-07, 9.250344983613681e-07, 1.3214778548019544e-06, 1.3214778548019544e-06, 1.887825506859935e-06, 1.887825506859935e-06, 2.6968935812284785e-06, 2.6968935812284785e-06, 3.852705116040684e-06, 3.852705116040684e-06, 5.503864451486692e-06, 5.503864451486692e-06, 7.862663502123846e-06, 7.862663502123846e-06, 1.1232376431605496e-05, 1.1232376431605496e-05, 1.6046252045150706e-05, 1.6046252045150706e-05, 2.2923217207358156e-05, 2.2923217207358156e-05, 3.2747453153368796e-05, 3.2747453153368796e-05, 4.6782075933384e-05, 4.6782075933384e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 12:35:14,816] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=28.864557812683614, CurrSamplesPerSec=33.4795716777286, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [5]  [1960/1970]  eta: 0:00:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4877 (4.5096)  loss_scale: 65536.0000 (65786.6476)  weight_decay: 0.0500 (0.0500)  time: 0.3863  data: 0.0005  max mem: 15572
Epoch: [5]  [1969/1970]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4877 (4.5101)  loss_scale: 65536.0000 (65785.5025)  weight_decay: 0.0500 (0.0500)  time: 0.3813  data: 0.0004  max mem: 15572
Epoch: [5] Total time: 0:12:41 (0.3863 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000000  loss: 4.4877 (4.5101)  loss_scale: 65536.0000 (65785.5025)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 3987
Indices to remove: tensor([    0,     1,     2,  ..., 33676, 33698, 33705], device='cuda:0')
length of data loader train is: 1638
num_training_steps_per_epoch is: 1638
Change step level LR scheduler!
Set warmup steps = 8190
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:15:00  loss: 1.1394 (1.1394)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.3090  data: 3.1393  max mem: 15572
Val:  [ 10/272]  eta: 0:02:00  loss: 4.5946 (4.0555)  acc1: 0.0000 (22.7273)  acc5: 5.5556 (28.7879)  time: 0.4596  data: 0.2860  max mem: 15572
Val:  [ 20/272]  eta: 0:01:23  loss: 4.0981 (4.0034)  acc1: 0.0000 (16.6667)  acc5: 16.6667 (28.5714)  time: 0.1812  data: 0.0007  max mem: 15572
Val:  [ 30/272]  eta: 0:01:06  loss: 4.1156 (4.0721)  acc1: 0.0000 (11.6487)  acc5: 22.2222 (28.4946)  time: 0.1750  data: 0.0006  max mem: 15572
Val:  [ 40/272]  eta: 0:00:57  loss: 3.8715 (3.9401)  acc1: 0.0000 (12.4661)  acc5: 44.4444 (32.7913)  time: 0.1619  data: 0.0005  max mem: 15572
Val:  [ 50/272]  eta: 0:00:51  loss: 3.4502 (3.9007)  acc1: 5.5556 (11.2200)  acc5: 50.0000 (36.1656)  time: 0.1643  data: 0.0005  max mem: 15572
Val:  [ 60/272]  eta: 0:00:47  loss: 3.0064 (3.7565)  acc1: 11.1111 (17.2131)  acc5: 72.2222 (40.8015)  time: 0.1688  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:42  loss: 2.9138 (3.6540)  acc1: 33.3333 (19.4836)  acc5: 72.2222 (43.3490)  time: 0.1635  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:39  loss: 3.4032 (3.6752)  acc1: 11.1111 (19.7531)  acc5: 50.0000 (42.4554)  time: 0.1627  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:36  loss: 4.5709 (3.7737)  acc1: 0.0000 (17.5824)  acc5: 0.0000 (38.2784)  time: 0.1678  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 4.4868 (3.8341)  acc1: 0.0000 (16.8317)  acc5: 0.0000 (36.1936)  time: 0.1668  data: 0.0005  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 4.4207 (3.9001)  acc1: 0.0000 (15.3153)  acc5: 5.5556 (34.1842)  time: 0.1669  data: 0.0006  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 4.4238 (3.9416)  acc1: 0.0000 (14.1873)  acc5: 11.1111 (33.0579)  time: 0.1660  data: 0.0005  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 4.2296 (3.8849)  acc1: 0.0000 (14.8855)  acc5: 27.7778 (35.1569)  time: 0.1637  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 3.7802 (3.8668)  acc1: 0.0000 (15.9180)  acc5: 33.3333 (35.3428)  time: 0.1634  data: 0.0005  max mem: 15572
Val:  [150/272]  eta: 0:00:22  loss: 3.9215 (3.8767)  acc1: 0.0000 (15.0478)  acc5: 22.2222 (35.0625)  time: 0.1637  data: 0.0005  max mem: 15572
Val:  [160/272]  eta: 0:00:20  loss: 3.8237 (3.8610)  acc1: 5.5556 (15.9420)  acc5: 38.8889 (36.6115)  time: 0.1709  data: 0.0006  max mem: 15572
Val:  [170/272]  eta: 0:00:18  loss: 4.0119 (3.8919)  acc1: 0.0000 (15.1722)  acc5: 33.3333 (35.8350)  time: 0.1721  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 4.0808 (3.8913)  acc1: 0.0000 (14.8864)  acc5: 27.7778 (36.4027)  time: 0.1720  data: 0.0006  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 4.3556 (3.9192)  acc1: 0.0000 (14.1652)  acc5: 11.1111 (34.9331)  time: 0.1762  data: 0.0005  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 4.3377 (3.9250)  acc1: 0.0000 (14.1791)  acc5: 16.6667 (35.5721)  time: 0.1687  data: 0.0005  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.8729 (3.9366)  acc1: 5.5556 (14.0864)  acc5: 44.4444 (35.6240)  time: 0.1760  data: 0.0007  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 4.0618 (3.9497)  acc1: 5.5556 (13.8009)  acc5: 33.3333 (35.1684)  time: 0.1832  data: 0.0008  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 4.0390 (3.9368)  acc1: 11.1111 (15.0072)  acc5: 33.3333 (36.2193)  time: 0.1776  data: 0.0007  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 3.4696 (3.9190)  acc1: 22.2222 (15.3296)  acc5: 77.7778 (37.6671)  time: 0.1671  data: 0.0005  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 3.7878 (3.9434)  acc1: 0.0000 (15.0066)  acc5: 44.4444 (36.8526)  time: 0.1575  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 3.6890 (3.8775)  acc1: 33.3333 (17.0498)  acc5: 50.0000 (38.6335)  time: 0.1549  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.5037 (3.8770)  acc1: 33.3333 (16.9947)  acc5: 66.6667 (38.8889)  time: 0.1433  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.5037 (3.8797)  acc1: 33.3333 (16.9977)  acc5: 66.6667 (38.8695)  time: 0.1375  data: 0.0001  max mem: 15572
Val: Total time: 0:00:48 (0.1789 s / it)
* Acc@1 16.998 Acc@5 38.870 loss 3.880
Accuracy of the network on the 4883 val videos: 17.0%
[2025-01-13 12:36:09,477] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 12:36:09,478] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 12:36:09,478] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 12:36:11,879] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 12:36:11,879] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 17.00%
Epoch: [6]  [   0/1638]  eta: 2:03:17  lr: 0.000047  min_lr: 0.000000  loss: 4.6015 (4.6015)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.5160  data: 4.1114  max mem: 15572
Epoch: [6]  [  10/1638]  eta: 0:20:26  lr: 0.000047  min_lr: 0.000000  loss: 4.4889 (4.3209)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7535  data: 0.3740  max mem: 15572
Epoch: [6]  [  20/1638]  eta: 0:15:40  lr: 0.000047  min_lr: 0.000000  loss: 4.2373 (4.3680)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3845  data: 0.0003  max mem: 15572
Epoch: [6]  [  30/1638]  eta: 0:13:48  lr: 0.000047  min_lr: 0.000000  loss: 4.3554 (4.3856)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3838  data: 0.0003  max mem: 15572
Epoch: [6]  [  40/1638]  eta: 0:12:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3554 (4.3866)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [6]  [  50/1638]  eta: 0:12:12  lr: 0.000047  min_lr: 0.000000  loss: 4.3827 (4.3876)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0002  max mem: 15572
[2025-01-13 12:36:36,960] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:36:36,960] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:36:37,772] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15069
[2025-01-13 12:36:37,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:36:37,773] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [  60/1638]  eta: 0:11:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3958 (4.3922)  loss_scale: 65536.0000 (67684.7213)  weight_decay: 0.0500 (0.0500)  time: 0.3855  data: 0.0003  max mem: 15572
Epoch: [6]  [  70/1638]  eta: 0:11:31  lr: 0.000047  min_lr: 0.000000  loss: 4.3939 (4.4066)  loss_scale: 65536.0000 (67382.0845)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0004  max mem: 15572
Epoch: [6]  [  80/1638]  eta: 0:11:17  lr: 0.000047  min_lr: 0.000000  loss: 4.4579 (4.4180)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  time: 0.3896  data: 0.0005  max mem: 15572
Epoch: [6]  [  90/1638]  eta: 0:11:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5714 (4.4300)  loss_scale: 65536.0000 (66976.3516)  weight_decay: 0.0500 (0.0500)  time: 0.3925  data: 0.0005  max mem: 15572
Epoch: [6]  [ 100/1638]  eta: 0:10:55  lr: 0.000047  min_lr: 0.000000  loss: 4.5788 (4.4365)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  time: 0.3904  data: 0.0004  max mem: 15572
Epoch: [6]  [ 110/1638]  eta: 0:10:46  lr: 0.000047  min_lr: 0.000000  loss: 4.3915 (4.4335)  loss_scale: 65536.0000 (66716.8288)  weight_decay: 0.0500 (0.0500)  time: 0.3883  data: 0.0003  max mem: 15572
Epoch: [6]  [ 120/1638]  eta: 0:10:35  lr: 0.000047  min_lr: 0.000000  loss: 4.3588 (4.4342)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.3817  data: 0.0003  max mem: 15572
Epoch: [6]  [ 130/1638]  eta: 0:10:26  lr: 0.000047  min_lr: 0.000000  loss: 4.4288 (4.4363)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [6]  [ 140/1638]  eta: 0:10:17  lr: 0.000047  min_lr: 0.000000  loss: 4.4984 (4.4489)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [6]  [ 150/1638]  eta: 0:10:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5440 (4.4502)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [6]  [ 160/1638]  eta: 0:10:02  lr: 0.000047  min_lr: 0.000000  loss: 4.3162 (4.4519)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [6]  [ 170/1638]  eta: 0:09:55  lr: 0.000047  min_lr: 0.000000  loss: 4.4136 (4.4502)  loss_scale: 65536.0000 (66302.5029)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [6]  [ 180/1638]  eta: 0:09:49  lr: 0.000047  min_lr: 0.000000  loss: 4.5098 (4.4550)  loss_scale: 65536.0000 (66260.1547)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
[2025-01-13 12:37:26,896] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:37:26,896] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:37:28,395] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15202
[2025-01-13 12:37:28,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:37:28,395] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 190/1638]  eta: 0:09:42  lr: 0.000047  min_lr: 0.000000  loss: 4.5268 (4.4563)  loss_scale: 65536.0000 (67594.7225)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [6]  [ 200/1638]  eta: 0:09:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5308 (4.4632)  loss_scale: 65536.0000 (67492.2985)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [6]  [ 210/1638]  eta: 0:09:31  lr: 0.000047  min_lr: 0.000000  loss: 4.5124 (4.4616)  loss_scale: 65536.0000 (67399.5829)  weight_decay: 0.0500 (0.0500)  time: 0.3813  data: 0.0003  max mem: 15572
Epoch: [6]  [ 220/1638]  eta: 0:09:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4026 (4.4627)  loss_scale: 65536.0000 (67315.2579)  weight_decay: 0.0500 (0.0500)  time: 0.3929  data: 0.0004  max mem: 15572
Epoch: [6]  [ 230/1638]  eta: 0:09:23  lr: 0.000047  min_lr: 0.000000  loss: 4.4060 (4.4590)  loss_scale: 65536.0000 (67238.2338)  weight_decay: 0.0500 (0.0500)  time: 0.3976  data: 0.0005  max mem: 15572
Epoch: [6]  [ 240/1638]  eta: 0:09:18  lr: 0.000047  min_lr: 0.000000  loss: 4.3952 (4.4581)  loss_scale: 65536.0000 (67167.6017)  weight_decay: 0.0500 (0.0500)  time: 0.3946  data: 0.0005  max mem: 15572
Epoch: [6]  [ 250/1638]  eta: 0:09:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3949 (4.4586)  loss_scale: 65536.0000 (67102.5976)  weight_decay: 0.0500 (0.0500)  time: 0.3936  data: 0.0004  max mem: 15572
Epoch: [6]  [ 260/1638]  eta: 0:09:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5011 (4.4621)  loss_scale: 65536.0000 (67042.5747)  weight_decay: 0.0500 (0.0500)  time: 0.3901  data: 0.0003  max mem: 15572
Epoch: [6]  [ 270/1638]  eta: 0:09:04  lr: 0.000047  min_lr: 0.000000  loss: 4.5047 (4.4599)  loss_scale: 65536.0000 (66986.9815)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0003  max mem: 15572
Epoch: [6]  [ 280/1638]  eta: 0:08:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5127 (4.4627)  loss_scale: 65536.0000 (66935.3452)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0002  max mem: 15572
Epoch: [6]  [ 290/1638]  eta: 0:08:55  lr: 0.000047  min_lr: 0.000000  loss: 4.3518 (4.4586)  loss_scale: 65536.0000 (66887.2577)  weight_decay: 0.0500 (0.0500)  time: 0.3814  data: 0.0002  max mem: 15572
Epoch: [6]  [ 300/1638]  eta: 0:08:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4434 (4.4593)  loss_scale: 65536.0000 (66842.3654)  weight_decay: 0.0500 (0.0500)  time: 0.3818  data: 0.0003  max mem: 15572
Epoch: [6]  [ 310/1638]  eta: 0:08:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4519 (4.4596)  loss_scale: 65536.0000 (66800.3601)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0003  max mem: 15572
[2025-01-13 12:38:18,127] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:38:18,127] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 320/1638]  eta: 0:08:41  lr: 0.000047  min_lr: 0.000000  loss: 4.3256 (4.4540)  loss_scale: 65536.0000 (67373.4579)  weight_decay: 0.0500 (0.0500)  time: 0.3819  data: 0.0002  max mem: 15572
Epoch: [6]  [ 330/1638]  eta: 0:08:36  lr: 0.000047  min_lr: 0.000000  loss: 4.3807 (4.4539)  loss_scale: 131072.0000 (69297.8852)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0002  max mem: 15572
[2025-01-13 12:38:24,975] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15349
[2025-01-13 12:38:24,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:38:24,975] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 340/1638]  eta: 0:08:32  lr: 0.000047  min_lr: 0.000000  loss: 4.4891 (4.4528)  loss_scale: 131072.0000 (70148.5044)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
Epoch: [6]  [ 350/1638]  eta: 0:08:27  lr: 0.000047  min_lr: 0.000000  loss: 4.5371 (4.4542)  loss_scale: 65536.0000 (70017.0940)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0003  max mem: 15572
Epoch: [6]  [ 360/1638]  eta: 0:08:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5634 (4.4587)  loss_scale: 65536.0000 (69892.9640)  weight_decay: 0.0500 (0.0500)  time: 0.3890  data: 0.0004  max mem: 15572
Epoch: [6]  [ 370/1638]  eta: 0:08:19  lr: 0.000047  min_lr: 0.000000  loss: 4.6034 (4.4606)  loss_scale: 65536.0000 (69775.5256)  weight_decay: 0.0500 (0.0500)  time: 0.3892  data: 0.0004  max mem: 15572
Epoch: [6]  [ 380/1638]  eta: 0:08:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5997 (4.4616)  loss_scale: 65536.0000 (69664.2520)  weight_decay: 0.0500 (0.0500)  time: 0.3879  data: 0.0004  max mem: 15572
Epoch: [6]  [ 390/1638]  eta: 0:08:11  lr: 0.000047  min_lr: 0.000000  loss: 4.4556 (4.4612)  loss_scale: 65536.0000 (69558.6701)  weight_decay: 0.0500 (0.0500)  time: 0.3878  data: 0.0004  max mem: 15572
Epoch: [6]  [ 400/1638]  eta: 0:08:07  lr: 0.000047  min_lr: 0.000000  loss: 4.4556 (4.4624)  loss_scale: 65536.0000 (69458.3541)  weight_decay: 0.0500 (0.0500)  time: 0.3841  data: 0.0004  max mem: 15572
Epoch: [6]  [ 410/1638]  eta: 0:08:02  lr: 0.000047  min_lr: 0.000000  loss: 4.4738 (4.4609)  loss_scale: 65536.0000 (69362.9197)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0003  max mem: 15572
Epoch: [6]  [ 420/1638]  eta: 0:07:58  lr: 0.000047  min_lr: 0.000000  loss: 4.1856 (4.4554)  loss_scale: 65536.0000 (69272.0190)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0002  max mem: 15572
Epoch: [6]  [ 430/1638]  eta: 0:07:53  lr: 0.000047  min_lr: 0.000000  loss: 4.3791 (4.4564)  loss_scale: 65536.0000 (69185.3364)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0002  max mem: 15572
Epoch: [6]  [ 440/1638]  eta: 0:07:49  lr: 0.000047  min_lr: 0.000000  loss: 4.4097 (4.4552)  loss_scale: 65536.0000 (69102.5850)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0002  max mem: 15572
Epoch: [6]  [ 450/1638]  eta: 0:07:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3996 (4.4542)  loss_scale: 65536.0000 (69023.5033)  weight_decay: 0.0500 (0.0500)  time: 0.3716  data: 0.0002  max mem: 15572
Epoch: [6]  [ 460/1638]  eta: 0:07:40  lr: 0.000047  min_lr: 0.000000  loss: 4.3599 (4.4544)  loss_scale: 65536.0000 (68947.8525)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
[2025-01-13 12:39:14,082] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:39:14,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 470/1638]  eta: 0:07:36  lr: 0.000047  min_lr: 0.000000  loss: 4.4309 (4.4563)  loss_scale: 65536.0000 (69710.2675)  weight_decay: 0.0500 (0.0500)  time: 0.3792  data: 0.0002  max mem: 15572
[2025-01-13 12:39:16,366] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15484
[2025-01-13 12:39:16,366] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:39:16,366] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 480/1638]  eta: 0:07:32  lr: 0.000047  min_lr: 0.000000  loss: 4.4521 (4.4588)  loss_scale: 65536.0000 (69623.4844)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [6]  [ 490/1638]  eta: 0:07:28  lr: 0.000047  min_lr: 0.000000  loss: 4.4013 (4.4574)  loss_scale: 65536.0000 (69540.2363)  weight_decay: 0.0500 (0.0500)  time: 0.3811  data: 0.0003  max mem: 15572
Epoch: [6]  [ 500/1638]  eta: 0:07:24  lr: 0.000047  min_lr: 0.000000  loss: 4.4346 (4.4584)  loss_scale: 65536.0000 (69460.3114)  weight_decay: 0.0500 (0.0500)  time: 0.3911  data: 0.0005  max mem: 15572
Epoch: [6]  [ 510/1638]  eta: 0:07:20  lr: 0.000047  min_lr: 0.000000  loss: 4.4346 (4.4563)  loss_scale: 65536.0000 (69383.5147)  weight_decay: 0.0500 (0.0500)  time: 0.3991  data: 0.0005  max mem: 15572
Epoch: [6]  [ 520/1638]  eta: 0:07:17  lr: 0.000047  min_lr: 0.000000  loss: 4.3676 (4.4555)  loss_scale: 65536.0000 (69309.6660)  weight_decay: 0.0500 (0.0500)  time: 0.4033  data: 0.0005  max mem: 15572
Epoch: [6]  [ 530/1638]  eta: 0:07:13  lr: 0.000047  min_lr: 0.000000  loss: 4.4050 (4.4554)  loss_scale: 65536.0000 (69238.5989)  weight_decay: 0.0500 (0.0500)  time: 0.4016  data: 0.0005  max mem: 15572
Epoch: [6]  [ 540/1638]  eta: 0:07:09  lr: 0.000047  min_lr: 0.000000  loss: 4.4705 (4.4555)  loss_scale: 65536.0000 (69170.1590)  weight_decay: 0.0500 (0.0500)  time: 0.3962  data: 0.0004  max mem: 15572
Epoch: [6]  [ 550/1638]  eta: 0:07:05  lr: 0.000047  min_lr: 0.000000  loss: 4.3681 (4.4527)  loss_scale: 65536.0000 (69104.2033)  weight_decay: 0.0500 (0.0500)  time: 0.3828  data: 0.0003  max mem: 15572
Epoch: [6]  [ 560/1638]  eta: 0:07:01  lr: 0.000047  min_lr: 0.000000  loss: 4.3681 (4.4522)  loss_scale: 65536.0000 (69040.5989)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [6]  [ 570/1638]  eta: 0:06:56  lr: 0.000047  min_lr: 0.000000  loss: 4.5155 (4.4502)  loss_scale: 65536.0000 (68979.2224)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [6]  [ 580/1638]  eta: 0:06:52  lr: 0.000047  min_lr: 0.000000  loss: 4.4351 (4.4493)  loss_scale: 65536.0000 (68919.9587)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
Epoch: [6]  [ 590/1638]  eta: 0:06:48  lr: 0.000047  min_lr: 0.000000  loss: 4.4079 (4.4476)  loss_scale: 65536.0000 (68862.7005)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
[2025-01-13 12:40:06,088] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:40:06,088] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 600/1638]  eta: 0:06:44  lr: 0.000047  min_lr: 0.000000  loss: 4.4897 (4.4475)  loss_scale: 65536.0000 (68916.3927)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [6]  [ 610/1638]  eta: 0:06:40  lr: 0.000047  min_lr: 0.000000  loss: 4.5454 (4.4495)  loss_scale: 131072.0000 (69933.6694)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0003  max mem: 15572
[2025-01-13 12:40:10,609] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15625
[2025-01-13 12:40:10,609] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:40:10,609] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 620/1638]  eta: 0:06:36  lr: 0.000047  min_lr: 0.000000  loss: 4.4804 (4.4484)  loss_scale: 131072.0000 (69968.3865)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0002  max mem: 15572
Epoch: [6]  [ 630/1638]  eta: 0:06:32  lr: 0.000047  min_lr: 0.000000  loss: 4.3518 (4.4482)  loss_scale: 65536.0000 (69898.1426)  weight_decay: 0.0500 (0.0500)  time: 0.3808  data: 0.0003  max mem: 15572
Epoch: [6]  [ 640/1638]  eta: 0:06:28  lr: 0.000047  min_lr: 0.000000  loss: 4.4022 (4.4490)  loss_scale: 65536.0000 (69830.0905)  weight_decay: 0.0500 (0.0500)  time: 0.3870  data: 0.0005  max mem: 15572
Epoch: [6]  [ 650/1638]  eta: 0:06:24  lr: 0.000047  min_lr: 0.000000  loss: 4.4307 (4.4472)  loss_scale: 65536.0000 (69764.1290)  weight_decay: 0.0500 (0.0500)  time: 0.3922  data: 0.0004  max mem: 15572
Epoch: [6]  [ 660/1638]  eta: 0:06:20  lr: 0.000047  min_lr: 0.000000  loss: 4.4438 (4.4483)  loss_scale: 65536.0000 (69700.1634)  weight_decay: 0.0500 (0.0500)  time: 0.3937  data: 0.0005  max mem: 15572
Epoch: [6]  [ 670/1638]  eta: 0:06:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4933 (4.4469)  loss_scale: 65536.0000 (69638.1043)  weight_decay: 0.0500 (0.0500)  time: 0.3977  data: 0.0005  max mem: 15572
Epoch: [6]  [ 680/1638]  eta: 0:06:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5065 (4.4466)  loss_scale: 65536.0000 (69577.8678)  weight_decay: 0.0500 (0.0500)  time: 0.4011  data: 0.0023  max mem: 15572
Epoch: [6]  [ 690/1638]  eta: 0:06:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5567 (4.4481)  loss_scale: 65536.0000 (69519.3748)  weight_decay: 0.0500 (0.0500)  time: 0.3858  data: 0.0022  max mem: 15572
Epoch: [6]  [ 700/1638]  eta: 0:06:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5476 (4.4477)  loss_scale: 65536.0000 (69462.5506)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [6]  [ 710/1638]  eta: 0:06:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5076 (4.4494)  loss_scale: 65536.0000 (69407.3249)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0002  max mem: 15572
Epoch: [6]  [ 720/1638]  eta: 0:05:56  lr: 0.000047  min_lr: 0.000000  loss: 4.4330 (4.4497)  loss_scale: 65536.0000 (69353.6311)  weight_decay: 0.0500 (0.0500)  time: 0.3773  data: 0.0002  max mem: 15572
Epoch: [6]  [ 730/1638]  eta: 0:05:52  lr: 0.000047  min_lr: 0.000000  loss: 4.3651 (4.4481)  loss_scale: 65536.0000 (69301.4063)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [6]  [ 740/1638]  eta: 0:05:48  lr: 0.000047  min_lr: 0.000000  loss: 4.4548 (4.4485)  loss_scale: 65536.0000 (69250.5911)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0002  max mem: 15572
[2025-01-13 12:41:00,204] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:41:00,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:41:01,363] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15757
[2025-01-13 12:41:01,363] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:41:01,363] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 750/1638]  eta: 0:05:44  lr: 0.000047  min_lr: 0.000000  loss: 4.5087 (4.4492)  loss_scale: 65536.0000 (69462.9241)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
Epoch: [6]  [ 760/1638]  eta: 0:05:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4878 (4.4482)  loss_scale: 65536.0000 (69411.3219)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [6]  [ 770/1638]  eta: 0:05:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5812 (4.4483)  loss_scale: 65536.0000 (69361.0584)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [6]  [ 780/1638]  eta: 0:05:32  lr: 0.000047  min_lr: 0.000000  loss: 4.6117 (4.4501)  loss_scale: 65536.0000 (69312.0819)  weight_decay: 0.0500 (0.0500)  time: 0.3802  data: 0.0003  max mem: 15572
Epoch: [6]  [ 790/1638]  eta: 0:05:29  lr: 0.000047  min_lr: 0.000000  loss: 4.4204 (4.4503)  loss_scale: 65536.0000 (69264.3439)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0003  max mem: 15572
Epoch: [6]  [ 800/1638]  eta: 0:05:25  lr: 0.000047  min_lr: 0.000000  loss: 4.3472 (4.4497)  loss_scale: 65536.0000 (69217.7978)  weight_decay: 0.0500 (0.0500)  time: 0.3952  data: 0.0004  max mem: 15572
Epoch: [6]  [ 810/1638]  eta: 0:05:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5107 (4.4511)  loss_scale: 65536.0000 (69172.3995)  weight_decay: 0.0500 (0.0500)  time: 0.3943  data: 0.0004  max mem: 15572
Epoch: [6]  [ 820/1638]  eta: 0:05:17  lr: 0.000047  min_lr: 0.000000  loss: 4.5796 (4.4509)  loss_scale: 65536.0000 (69128.1072)  weight_decay: 0.0500 (0.0500)  time: 0.3903  data: 0.0004  max mem: 15572
Epoch: [6]  [ 830/1638]  eta: 0:05:13  lr: 0.000047  min_lr: 0.000000  loss: 4.4555 (4.4493)  loss_scale: 65536.0000 (69084.8809)  weight_decay: 0.0500 (0.0500)  time: 0.3843  data: 0.0003  max mem: 15572
Epoch: [6]  [ 840/1638]  eta: 0:05:09  lr: 0.000047  min_lr: 0.000000  loss: 4.2960 (4.4496)  loss_scale: 65536.0000 (69042.6825)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0002  max mem: 15572
Epoch: [6]  [ 850/1638]  eta: 0:05:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4894 (4.4511)  loss_scale: 65536.0000 (69001.4759)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
Epoch: [6]  [ 860/1638]  eta: 0:05:01  lr: 0.000047  min_lr: 0.000000  loss: 4.5429 (4.4509)  loss_scale: 65536.0000 (68961.2265)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0002  max mem: 15572
Epoch: [6]  [ 870/1638]  eta: 0:04:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4858 (4.4520)  loss_scale: 65536.0000 (68921.9013)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
[2025-01-13 12:41:50,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:41:50,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:41:53,249] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15893
[2025-01-13 12:41:53,249] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:41:53,249] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 880/1638]  eta: 0:04:53  lr: 0.000047  min_lr: 0.000000  loss: 4.4858 (4.4502)  loss_scale: 65536.0000 (69404.1862)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [6]  [ 890/1638]  eta: 0:04:49  lr: 0.000047  min_lr: 0.000000  loss: 4.2683 (4.4491)  loss_scale: 65536.0000 (69360.7722)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [6]  [ 900/1638]  eta: 0:04:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3617 (4.4488)  loss_scale: 65536.0000 (69318.3219)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [6]  [ 910/1638]  eta: 0:04:41  lr: 0.000047  min_lr: 0.000000  loss: 4.2959 (4.4472)  loss_scale: 65536.0000 (69276.8035)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
Epoch: [6]  [ 920/1638]  eta: 0:04:37  lr: 0.000047  min_lr: 0.000000  loss: 4.3542 (4.4468)  loss_scale: 65536.0000 (69236.1868)  weight_decay: 0.0500 (0.0500)  time: 0.3837  data: 0.0003  max mem: 15572
Epoch: [6]  [ 930/1638]  eta: 0:04:33  lr: 0.000047  min_lr: 0.000000  loss: 4.3590 (4.4463)  loss_scale: 65536.0000 (69196.4425)  weight_decay: 0.0500 (0.0500)  time: 0.3907  data: 0.0005  max mem: 15572
Epoch: [6]  [ 940/1638]  eta: 0:04:30  lr: 0.000047  min_lr: 0.000000  loss: 4.3430 (4.4461)  loss_scale: 65536.0000 (69157.5430)  weight_decay: 0.0500 (0.0500)  time: 0.3960  data: 0.0005  max mem: 15572
Epoch: [6]  [ 950/1638]  eta: 0:04:26  lr: 0.000047  min_lr: 0.000000  loss: 4.4637 (4.4468)  loss_scale: 65536.0000 (69119.4616)  weight_decay: 0.0500 (0.0500)  time: 0.3943  data: 0.0019  max mem: 15572
Epoch: [6]  [ 960/1638]  eta: 0:04:22  lr: 0.000047  min_lr: 0.000000  loss: 4.4875 (4.4473)  loss_scale: 65536.0000 (69082.1727)  weight_decay: 0.0500 (0.0500)  time: 0.3911  data: 0.0018  max mem: 15572
Epoch: [6]  [ 970/1638]  eta: 0:04:18  lr: 0.000047  min_lr: 0.000000  loss: 4.3697 (4.4459)  loss_scale: 65536.0000 (69045.6519)  weight_decay: 0.0500 (0.0500)  time: 0.3864  data: 0.0003  max mem: 15572
Epoch: [6]  [ 980/1638]  eta: 0:04:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3013 (4.4453)  loss_scale: 65536.0000 (69009.8756)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
[2025-01-13 12:42:33,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=100, lr=[4.518260476443017e-07, 4.518260476443017e-07, 6.454657823490025e-07, 6.454657823490025e-07, 9.220939747842894e-07, 9.220939747842894e-07, 1.3172771068346993e-06, 1.3172771068346993e-06, 1.8818244383352847e-06, 1.8818244383352847e-06, 2.688320626193264e-06, 2.688320626193264e-06, 3.8404580374189485e-06, 3.8404580374189485e-06, 5.486368624884213e-06, 5.486368624884213e-06, 7.837669464120304e-06, 7.837669464120304e-06, 1.1196670663029007e-05, 1.1196670663029007e-05, 1.5995243804327152e-05, 1.5995243804327152e-05, 2.2850348291895934e-05, 2.2850348291895934e-05, 3.264335470270848e-05, 3.264335470270848e-05, 4.663336386101212e-05, 4.663336386101212e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 12:42:33,872] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=29.099273725653156, CurrSamplesPerSec=34.356636559233735, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [ 990/1638]  eta: 0:04:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5120 (4.4466)  loss_scale: 65536.0000 (68974.8214)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [6]  [1000/1638]  eta: 0:04:06  lr: 0.000047  min_lr: 0.000000  loss: 4.4784 (4.4443)  loss_scale: 65536.0000 (68940.4675)  weight_decay: 0.0500 (0.0500)  time: 0.3870  data: 0.0002  max mem: 15572
[2025-01-13 12:42:42,727] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:42:42,727] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1010/1638]  eta: 0:04:02  lr: 0.000047  min_lr: 0.000000  loss: 4.4449 (4.4451)  loss_scale: 65536.0000 (69036.4392)  weight_decay: 0.0500 (0.0500)  time: 0.3871  data: 0.0002  max mem: 15572
[2025-01-13 12:42:43,901] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16025
[2025-01-13 12:42:43,901] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:42:43,901] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1020/1638]  eta: 0:03:59  lr: 0.000047  min_lr: 0.000000  loss: 4.4449 (4.4443)  loss_scale: 65536.0000 (69066.3428)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0002  max mem: 15572
Epoch: [6]  [1030/1638]  eta: 0:03:55  lr: 0.000047  min_lr: 0.000000  loss: 4.3963 (4.4440)  loss_scale: 65536.0000 (69032.1009)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
Epoch: [6]  [1040/1638]  eta: 0:03:51  lr: 0.000047  min_lr: 0.000000  loss: 4.3644 (4.4428)  loss_scale: 65536.0000 (68998.5168)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [6]  [1050/1638]  eta: 0:03:47  lr: 0.000047  min_lr: 0.000000  loss: 4.3644 (4.4415)  loss_scale: 65536.0000 (68965.5718)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0002  max mem: 15572
Epoch: [6]  [1060/1638]  eta: 0:03:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5370 (4.4428)  loss_scale: 65536.0000 (68933.2479)  weight_decay: 0.0500 (0.0500)  time: 0.3833  data: 0.0003  max mem: 15572
Epoch: [6]  [1070/1638]  eta: 0:03:39  lr: 0.000047  min_lr: 0.000000  loss: 4.5578 (4.4439)  loss_scale: 65536.0000 (68901.5275)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0004  max mem: 15572
Epoch: [6]  [1080/1638]  eta: 0:03:35  lr: 0.000047  min_lr: 0.000000  loss: 4.4860 (4.4445)  loss_scale: 65536.0000 (68870.3941)  weight_decay: 0.0500 (0.0500)  time: 0.3937  data: 0.0004  max mem: 15572
Epoch: [6]  [1090/1638]  eta: 0:03:31  lr: 0.000047  min_lr: 0.000000  loss: 4.4860 (4.4447)  loss_scale: 65536.0000 (68839.8313)  weight_decay: 0.0500 (0.0500)  time: 0.3922  data: 0.0004  max mem: 15572
Epoch: [6]  [1100/1638]  eta: 0:03:28  lr: 0.000047  min_lr: 0.000000  loss: 4.4850 (4.4444)  loss_scale: 65536.0000 (68809.8238)  weight_decay: 0.0500 (0.0500)  time: 0.3953  data: 0.0003  max mem: 15572
Epoch: [6]  [1110/1638]  eta: 0:03:24  lr: 0.000047  min_lr: 0.000000  loss: 4.4197 (4.4439)  loss_scale: 65536.0000 (68780.3564)  weight_decay: 0.0500 (0.0500)  time: 0.3920  data: 0.0003  max mem: 15572
Epoch: [6]  [1120/1638]  eta: 0:03:20  lr: 0.000047  min_lr: 0.000000  loss: 4.4087 (4.4435)  loss_scale: 65536.0000 (68751.4148)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0003  max mem: 15572
Epoch: [6]  [1130/1638]  eta: 0:03:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4671 (4.4436)  loss_scale: 65536.0000 (68722.9850)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [6]  [1140/1638]  eta: 0:03:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5143 (4.4432)  loss_scale: 65536.0000 (68695.0535)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
[2025-01-13 12:43:33,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:43:33,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:43:34,410] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16157
[2025-01-13 12:43:34,410] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:43:34,410] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1150/1638]  eta: 0:03:08  lr: 0.000047  min_lr: 0.000000  loss: 4.4625 (4.4433)  loss_scale: 65536.0000 (68838.4222)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [6]  [1160/1638]  eta: 0:03:04  lr: 0.000047  min_lr: 0.000000  loss: 4.3959 (4.4416)  loss_scale: 65536.0000 (68809.9776)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [6]  [1170/1638]  eta: 0:03:00  lr: 0.000047  min_lr: 0.000000  loss: 4.2900 (4.4409)  loss_scale: 65536.0000 (68782.0188)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0002  max mem: 15572
Epoch: [6]  [1180/1638]  eta: 0:02:56  lr: 0.000047  min_lr: 0.000000  loss: 4.2900 (4.4401)  loss_scale: 65536.0000 (68754.5334)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [6]  [1190/1638]  eta: 0:02:52  lr: 0.000047  min_lr: 0.000000  loss: 4.4229 (4.4406)  loss_scale: 65536.0000 (68727.5097)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [6]  [1200/1638]  eta: 0:02:49  lr: 0.000047  min_lr: 0.000000  loss: 4.4623 (4.4399)  loss_scale: 65536.0000 (68700.9359)  weight_decay: 0.0500 (0.0500)  time: 0.3812  data: 0.0003  max mem: 15572
Epoch: [6]  [1210/1638]  eta: 0:02:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4771 (4.4399)  loss_scale: 65536.0000 (68674.8010)  weight_decay: 0.0500 (0.0500)  time: 0.3906  data: 0.0004  max mem: 15572
Epoch: [6]  [1220/1638]  eta: 0:02:41  lr: 0.000047  min_lr: 0.000000  loss: 4.4761 (4.4391)  loss_scale: 65536.0000 (68649.0942)  weight_decay: 0.0500 (0.0500)  time: 0.3983  data: 0.0004  max mem: 15572
Epoch: [6]  [1230/1638]  eta: 0:02:37  lr: 0.000047  min_lr: 0.000000  loss: 4.4360 (4.4388)  loss_scale: 65536.0000 (68623.8050)  weight_decay: 0.0500 (0.0500)  time: 0.3966  data: 0.0004  max mem: 15572
Epoch: [6]  [1240/1638]  eta: 0:02:33  lr: 0.000047  min_lr: 0.000000  loss: 4.4143 (4.4375)  loss_scale: 65536.0000 (68598.9234)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0004  max mem: 15572
Epoch: [6]  [1250/1638]  eta: 0:02:29  lr: 0.000047  min_lr: 0.000000  loss: 4.3562 (4.4368)  loss_scale: 65536.0000 (68574.4396)  weight_decay: 0.0500 (0.0500)  time: 0.3860  data: 0.0004  max mem: 15572
Epoch: [6]  [1260/1638]  eta: 0:02:25  lr: 0.000047  min_lr: 0.000000  loss: 4.3973 (4.4367)  loss_scale: 65536.0000 (68550.3442)  weight_decay: 0.0500 (0.0500)  time: 0.3804  data: 0.0003  max mem: 15572
Epoch: [6]  [1270/1638]  eta: 0:02:22  lr: 0.000047  min_lr: 0.000000  loss: 4.3960 (4.4369)  loss_scale: 65536.0000 (68526.6279)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
[2025-01-13 12:44:23,758] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:44:23,759] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:44:24,879] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16289
[2025-01-13 12:44:24,879] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:44:24,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1280/1638]  eta: 0:02:18  lr: 0.000047  min_lr: 0.000000  loss: 4.3406 (4.4361)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [6]  [1290/1638]  eta: 0:02:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3406 (4.4355)  loss_scale: 65536.0000 (68632.5887)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0002  max mem: 15572
Epoch: [6]  [1300/1638]  eta: 0:02:10  lr: 0.000047  min_lr: 0.000000  loss: 4.4484 (4.4365)  loss_scale: 65536.0000 (68608.7871)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0002  max mem: 15572
Epoch: [6]  [1310/1638]  eta: 0:02:06  lr: 0.000047  min_lr: 0.000000  loss: 4.4927 (4.4376)  loss_scale: 65536.0000 (68585.3486)  weight_decay: 0.0500 (0.0500)  time: 0.3790  data: 0.0002  max mem: 15572
Epoch: [6]  [1320/1638]  eta: 0:02:02  lr: 0.000047  min_lr: 0.000000  loss: 4.4796 (4.4378)  loss_scale: 65536.0000 (68562.2650)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0002  max mem: 15572
Epoch: [6]  [1330/1638]  eta: 0:01:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5684 (4.4386)  loss_scale: 65536.0000 (68539.5282)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0002  max mem: 15572
Epoch: [6]  [1340/1638]  eta: 0:01:54  lr: 0.000047  min_lr: 0.000000  loss: 4.4923 (4.4380)  loss_scale: 65536.0000 (68517.1305)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0002  max mem: 15572
Epoch: [6]  [1350/1638]  eta: 0:01:51  lr: 0.000047  min_lr: 0.000000  loss: 4.3948 (4.4380)  loss_scale: 65536.0000 (68495.0644)  weight_decay: 0.0500 (0.0500)  time: 0.3881  data: 0.0003  max mem: 15572
Epoch: [6]  [1360/1638]  eta: 0:01:47  lr: 0.000047  min_lr: 0.000000  loss: 4.4949 (4.4391)  loss_scale: 65536.0000 (68473.3226)  weight_decay: 0.0500 (0.0500)  time: 0.3941  data: 0.0004  max mem: 15572
Epoch: [6]  [1370/1638]  eta: 0:01:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4949 (4.4384)  loss_scale: 65536.0000 (68451.8979)  weight_decay: 0.0500 (0.0500)  time: 0.3948  data: 0.0005  max mem: 15572
Epoch: [6]  [1380/1638]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000000  loss: 4.4055 (4.4380)  loss_scale: 65536.0000 (68430.7835)  weight_decay: 0.0500 (0.0500)  time: 0.3974  data: 0.0004  max mem: 15572
Epoch: [6]  [1390/1638]  eta: 0:01:35  lr: 0.000047  min_lr: 0.000000  loss: 4.4644 (4.4378)  loss_scale: 65536.0000 (68409.9727)  weight_decay: 0.0500 (0.0500)  time: 0.3944  data: 0.0004  max mem: 15572
Epoch: [6]  [1400/1638]  eta: 0:01:31  lr: 0.000047  min_lr: 0.000000  loss: 4.4644 (4.4380)  loss_scale: 65536.0000 (68389.4590)  weight_decay: 0.0500 (0.0500)  time: 0.3922  data: 0.0004  max mem: 15572
[2025-01-13 12:45:14,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:45:14,613] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1410/1638]  eta: 0:01:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4740 (4.4380)  loss_scale: 65536.0000 (68647.9150)  weight_decay: 0.0500 (0.0500)  time: 0.3826  data: 0.0003  max mem: 15572
[2025-01-13 12:45:16,867] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16424
[2025-01-13 12:45:16,867] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:45:16,867] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1420/1638]  eta: 0:01:24  lr: 0.000047  min_lr: 0.000000  loss: 4.4873 (4.4389)  loss_scale: 65536.0000 (68626.0155)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [6]  [1430/1638]  eta: 0:01:20  lr: 0.000047  min_lr: 0.000000  loss: 4.7221 (4.4404)  loss_scale: 65536.0000 (68604.4221)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [6]  [1440/1638]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4534 (4.4385)  loss_scale: 65536.0000 (68583.1284)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [6]  [1450/1638]  eta: 0:01:12  lr: 0.000047  min_lr: 0.000000  loss: 4.2215 (4.4371)  loss_scale: 65536.0000 (68562.1282)  weight_decay: 0.0500 (0.0500)  time: 0.3728  data: 0.0002  max mem: 15572
Epoch: [6]  [1460/1638]  eta: 0:01:08  lr: 0.000047  min_lr: 0.000000  loss: 4.2980 (4.4372)  loss_scale: 65536.0000 (68541.4155)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [6]  [1470/1638]  eta: 0:01:04  lr: 0.000047  min_lr: 0.000000  loss: 4.2980 (4.4359)  loss_scale: 65536.0000 (68520.9844)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0003  max mem: 15572
Epoch: [6]  [1480/1638]  eta: 0:01:00  lr: 0.000047  min_lr: 0.000000  loss: 4.3630 (4.4353)  loss_scale: 65536.0000 (68500.8292)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
Epoch: [6]  [1490/1638]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000000  loss: 4.3630 (4.4352)  loss_scale: 65536.0000 (68480.9443)  weight_decay: 0.0500 (0.0500)  time: 0.3838  data: 0.0003  max mem: 15572
Epoch: [6]  [1500/1638]  eta: 0:00:53  lr: 0.000047  min_lr: 0.000000  loss: 4.3703 (4.4361)  loss_scale: 65536.0000 (68461.3245)  weight_decay: 0.0500 (0.0500)  time: 0.3922  data: 0.0005  max mem: 15572
Epoch: [6]  [1510/1638]  eta: 0:00:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3703 (4.4358)  loss_scale: 65536.0000 (68441.9643)  weight_decay: 0.0500 (0.0500)  time: 0.3944  data: 0.0006  max mem: 15572
Epoch: [6]  [1520/1638]  eta: 0:00:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3816 (4.4351)  loss_scale: 65536.0000 (68422.8586)  weight_decay: 0.0500 (0.0500)  time: 0.3934  data: 0.0005  max mem: 15572
Epoch: [6]  [1530/1638]  eta: 0:00:41  lr: 0.000047  min_lr: 0.000000  loss: 4.3182 (4.4346)  loss_scale: 65536.0000 (68404.0026)  weight_decay: 0.0500 (0.0500)  time: 0.3964  data: 0.0004  max mem: 15572
[2025-01-13 12:46:06,277] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:46:06,277] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1540/1638]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000000  loss: 4.4289 (4.4351)  loss_scale: 65536.0000 (68427.9195)  weight_decay: 0.0500 (0.0500)  time: 0.3916  data: 0.0004  max mem: 15572
Epoch: [6]  [1550/1638]  eta: 0:00:33  lr: 0.000047  min_lr: 0.000000  loss: 4.4289 (4.4340)  loss_scale: 131072.0000 (68831.8143)  weight_decay: 0.0500 (0.0500)  time: 0.3830  data: 0.0003  max mem: 15572
[2025-01-13 12:46:10,466] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16564
[2025-01-13 12:46:10,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:46:10,467] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1560/1638]  eta: 0:00:30  lr: 0.000047  min_lr: 0.000000  loss: 4.3035 (4.4335)  loss_scale: 65536.0000 (68810.7008)  weight_decay: 0.0500 (0.0500)  time: 0.3792  data: 0.0003  max mem: 15572
Epoch: [6]  [1570/1638]  eta: 0:00:26  lr: 0.000047  min_lr: 0.000000  loss: 4.4167 (4.4328)  loss_scale: 65536.0000 (68789.8561)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0003  max mem: 15572
Epoch: [6]  [1580/1638]  eta: 0:00:22  lr: 0.000047  min_lr: 0.000000  loss: 4.5246 (4.4336)  loss_scale: 65536.0000 (68769.2751)  weight_decay: 0.0500 (0.0500)  time: 0.3804  data: 0.0002  max mem: 15572
Epoch: [6]  [1590/1638]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000000  loss: 4.4461 (4.4331)  loss_scale: 65536.0000 (68748.9529)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0002  max mem: 15572
Epoch: [6]  [1600/1638]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000000  loss: 4.4163 (4.4331)  loss_scale: 65536.0000 (68728.8844)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [6]  [1610/1638]  eta: 0:00:10  lr: 0.000047  min_lr: 0.000000  loss: 4.4787 (4.4329)  loss_scale: 65536.0000 (68709.0652)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0002  max mem: 15572
Epoch: [6]  [1620/1638]  eta: 0:00:06  lr: 0.000047  min_lr: 0.000000  loss: 4.4884 (4.4335)  loss_scale: 65536.0000 (68689.4904)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0002  max mem: 15572
Epoch: [6]  [1630/1638]  eta: 0:00:03  lr: 0.000047  min_lr: 0.000000  loss: 4.5225 (4.4339)  loss_scale: 65536.0000 (68670.1557)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0001  max mem: 15572
Epoch: [6]  [1637/1638]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4506 (4.4335)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.3701  data: 0.0001  max mem: 15572
Epoch: [6] Total time: 0:10:31 (0.3852 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 4.4506 (4.4335)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 3792
Indices to remove: tensor([    0,     2,     6,  ..., 33684, 33691, 33705], device='cuda:0')
length of data loader train is: 1322
num_training_steps_per_epoch is: 1322
Change step level LR scheduler!
Set warmup steps = 6610
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:17:23  loss: 0.8348 (0.8348)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.8373  data: 3.6476  max mem: 15572
Val:  [ 10/272]  eta: 0:02:15  loss: 4.1557 (3.7847)  acc1: 0.0000 (22.2222)  acc5: 11.1111 (29.7980)  time: 0.5181  data: 0.3451  max mem: 15572
Val:  [ 20/272]  eta: 0:01:29  loss: 3.9790 (3.8001)  acc1: 5.5556 (17.7249)  acc5: 27.7778 (34.9206)  time: 0.1795  data: 0.0077  max mem: 15572
Val:  [ 30/272]  eta: 0:01:12  loss: 3.9790 (3.9105)  acc1: 5.5556 (12.7240)  acc5: 33.3333 (33.5125)  time: 0.1769  data: 0.0006  max mem: 15572
Val:  [ 40/272]  eta: 0:01:02  loss: 3.8979 (3.8639)  acc1: 5.5556 (12.6016)  acc5: 33.3333 (35.3659)  time: 0.1781  data: 0.0007  max mem: 15572
Val:  [ 50/272]  eta: 0:00:55  loss: 3.5252 (3.8090)  acc1: 5.5556 (13.2898)  acc5: 44.4444 (37.7996)  time: 0.1773  data: 0.0008  max mem: 15572
Val:  [ 60/272]  eta: 0:00:50  loss: 2.7469 (3.6291)  acc1: 33.3333 (19.7632)  acc5: 72.2222 (42.2587)  time: 0.1817  data: 0.0006  max mem: 15572
Val:  [ 70/272]  eta: 0:00:46  loss: 2.7109 (3.5300)  acc1: 44.4444 (21.5180)  acc5: 66.6667 (45.6182)  time: 0.1726  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:42  loss: 3.2178 (3.5359)  acc1: 11.1111 (22.2908)  acc5: 61.1111 (45.1303)  time: 0.1621  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:39  loss: 4.2926 (3.6209)  acc1: 0.0000 (19.9634)  acc5: 16.6667 (41.5751)  time: 0.1685  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:36  loss: 4.2766 (3.6846)  acc1: 0.0000 (19.2519)  acc5: 16.6667 (40.1540)  time: 0.1686  data: 0.0005  max mem: 15572
Val:  [110/272]  eta: 0:00:33  loss: 4.1721 (3.7412)  acc1: 0.0000 (17.5175)  acc5: 16.6667 (38.7888)  time: 0.1622  data: 0.0005  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 4.1314 (3.7749)  acc1: 0.0000 (16.3912)  acc5: 22.2222 (38.4298)  time: 0.1687  data: 0.0006  max mem: 15572
Val:  [130/272]  eta: 0:00:28  loss: 4.0672 (3.7233)  acc1: 5.5556 (17.5148)  acc5: 38.8889 (40.1187)  time: 0.1739  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:26  loss: 3.3242 (3.7026)  acc1: 22.2222 (18.5185)  acc5: 50.0000 (40.5043)  time: 0.1703  data: 0.0005  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.7281 (3.7125)  acc1: 5.5556 (17.7704)  acc5: 33.3333 (40.0662)  time: 0.1649  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.6616 (3.7000)  acc1: 11.1111 (18.6680)  acc5: 44.4444 (41.4424)  time: 0.1604  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.6964 (3.7267)  acc1: 11.1111 (17.8687)  acc5: 61.1111 (41.1956)  time: 0.1607  data: 0.0041  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.7089 (3.7241)  acc1: 0.0000 (17.8637)  acc5: 38.8889 (41.6513)  time: 0.1747  data: 0.0179  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 4.1182 (3.7553)  acc1: 0.0000 (16.9866)  acc5: 33.3333 (40.5759)  time: 0.1755  data: 0.0143  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 4.0718 (3.7553)  acc1: 0.0000 (17.2471)  acc5: 33.3333 (41.5146)  time: 0.1753  data: 0.0005  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.5496 (3.7638)  acc1: 16.6667 (17.2459)  acc5: 50.0000 (41.6272)  time: 0.1853  data: 0.0006  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.8656 (3.7697)  acc1: 5.5556 (17.4962)  acc5: 44.4444 (41.6792)  time: 0.1816  data: 0.0006  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.4992 (3.7448)  acc1: 27.7778 (18.8793)  acc5: 61.1111 (43.0014)  time: 0.1778  data: 0.0006  max mem: 15572
Val:  [240/272]  eta: 0:00:06  loss: 3.0304 (3.7225)  acc1: 38.8889 (19.4099)  acc5: 88.8889 (44.3522)  time: 0.1788  data: 0.0006  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.5173 (3.7446)  acc1: 11.1111 (19.0792)  acc5: 50.0000 (43.8468)  time: 0.1786  data: 0.0030  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 3.2731 (3.6713)  acc1: 44.4444 (21.4134)  acc5: 72.2222 (45.6364)  time: 0.1637  data: 0.0027  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.8611 (3.6666)  acc1: 50.0000 (21.6482)  acc5: 77.7778 (45.8795)  time: 0.1462  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.8611 (3.6694)  acc1: 50.0000 (21.6260)  acc5: 72.2222 (45.8530)  time: 0.1400  data: 0.0002  max mem: 15572
Val: Total time: 0:00:50 (0.1852 s / it)
* Acc@1 21.626 Acc@5 45.853 loss 3.669
Accuracy of the network on the 4883 val videos: 21.6%
[2025-01-13 12:47:34,132] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 12:47:34,136] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 12:47:34,136] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 12:47:36,856] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 12:47:36,857] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 21.63%
Epoch: [7]  [   0/1322]  eta: 1:31:18  lr: 0.000046  min_lr: 0.000000  loss: 3.8245 (3.8245)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.1444  data: 3.7351  max mem: 15572
Epoch: [7]  [  10/1322]  eta: 0:16:13  lr: 0.000046  min_lr: 0.000000  loss: 4.0725 (4.1780)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7418  data: 0.3505  max mem: 15572
Epoch: [7]  [  20/1322]  eta: 0:12:25  lr: 0.000046  min_lr: 0.000000  loss: 4.2440 (4.2852)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3937  data: 0.0062  max mem: 15572
Epoch: [7]  [  30/1322]  eta: 0:11:02  lr: 0.000046  min_lr: 0.000000  loss: 4.3583 (4.3103)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3874  data: 0.0003  max mem: 15572
Epoch: [7]  [  40/1322]  eta: 0:10:14  lr: 0.000046  min_lr: 0.000000  loss: 4.2637 (4.3227)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0003  max mem: 15572
[2025-01-13 12:47:57,317] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:47:57,317] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:47:57,686] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16694
[2025-01-13 12:47:57,686] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:47:57,686] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [  50/1322]  eta: 0:09:46  lr: 0.000046  min_lr: 0.000000  loss: 4.4915 (4.3686)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0003  max mem: 15572
Epoch: [7]  [  60/1322]  eta: 0:09:23  lr: 0.000046  min_lr: 0.000000  loss: 4.5338 (4.3720)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.3792  data: 0.0003  max mem: 15572
Epoch: [7]  [  70/1322]  eta: 0:09:06  lr: 0.000046  min_lr: 0.000000  loss: 4.3034 (4.3500)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0003  max mem: 15572
Epoch: [7]  [  80/1322]  eta: 0:08:56  lr: 0.000046  min_lr: 0.000000  loss: 4.3412 (4.3583)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.3865  data: 0.0004  max mem: 15572
Epoch: [7]  [  90/1322]  eta: 0:08:46  lr: 0.000046  min_lr: 0.000000  loss: 4.5163 (4.3598)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.3948  data: 0.0004  max mem: 15572
Epoch: [7]  [ 100/1322]  eta: 0:08:38  lr: 0.000046  min_lr: 0.000000  loss: 4.5073 (4.3721)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.3917  data: 0.0004  max mem: 15572
Epoch: [7]  [ 110/1322]  eta: 0:08:30  lr: 0.000046  min_lr: 0.000000  loss: 4.4905 (4.3832)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.3908  data: 0.0005  max mem: 15572
Epoch: [7]  [ 120/1322]  eta: 0:08:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4903 (4.3897)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.3923  data: 0.0005  max mem: 15572
Epoch: [7]  [ 130/1322]  eta: 0:08:16  lr: 0.000046  min_lr: 0.000000  loss: 4.4453 (4.3912)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.3944  data: 0.0005  max mem: 15572
Epoch: [7]  [ 140/1322]  eta: 0:08:09  lr: 0.000046  min_lr: 0.000000  loss: 4.3790 (4.3922)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.3863  data: 0.0004  max mem: 15572
Epoch: [7]  [ 150/1322]  eta: 0:08:02  lr: 0.000046  min_lr: 0.000000  loss: 4.2965 (4.3837)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0016  max mem: 15572
Epoch: [7]  [ 160/1322]  eta: 0:07:55  lr: 0.000046  min_lr: 0.000000  loss: 4.2742 (4.3854)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0016  max mem: 15572
Epoch: [7]  [ 170/1322]  eta: 0:07:49  lr: 0.000046  min_lr: 0.000000  loss: 4.3177 (4.3837)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
[2025-01-13 12:48:47,288] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:48:47,288] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [ 180/1322]  eta: 0:07:43  lr: 0.000046  min_lr: 0.000000  loss: 4.3644 (4.3905)  loss_scale: 65536.0000 (69156.7735)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0004  max mem: 15572
Epoch: [7]  [ 190/1322]  eta: 0:07:37  lr: 0.000046  min_lr: 0.000000  loss: 4.4941 (4.3994)  loss_scale: 131072.0000 (72398.4084)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0004  max mem: 15572
[2025-01-13 12:48:54,853] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16843
[2025-01-13 12:48:54,853] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:48:54,853] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 200/1322]  eta: 0:07:31  lr: 0.000046  min_lr: 0.000000  loss: 4.4941 (4.3992)  loss_scale: 131072.0000 (72383.0448)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0003  max mem: 15572
Epoch: [7]  [ 210/1322]  eta: 0:07:26  lr: 0.000046  min_lr: 0.000000  loss: 4.3162 (4.3899)  loss_scale: 65536.0000 (72058.5403)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
Epoch: [7]  [ 220/1322]  eta: 0:07:21  lr: 0.000046  min_lr: 0.000000  loss: 4.3900 (4.3921)  loss_scale: 65536.0000 (71763.4027)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [7]  [ 230/1322]  eta: 0:07:16  lr: 0.000046  min_lr: 0.000000  loss: 4.3905 (4.3894)  loss_scale: 65536.0000 (71493.8182)  weight_decay: 0.0500 (0.0500)  time: 0.3814  data: 0.0003  max mem: 15572
Epoch: [7]  [ 240/1322]  eta: 0:07:12  lr: 0.000046  min_lr: 0.000000  loss: 4.3545 (4.3854)  loss_scale: 65536.0000 (71246.6058)  weight_decay: 0.0500 (0.0500)  time: 0.3902  data: 0.0004  max mem: 15572
Epoch: [7]  [ 250/1322]  eta: 0:07:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2001 (4.3757)  loss_scale: 65536.0000 (71019.0916)  weight_decay: 0.0500 (0.0500)  time: 0.3927  data: 0.0005  max mem: 15572
Epoch: [7]  [ 260/1322]  eta: 0:07:03  lr: 0.000046  min_lr: 0.000000  loss: 4.2001 (4.3712)  loss_scale: 65536.0000 (70809.0115)  weight_decay: 0.0500 (0.0500)  time: 0.3938  data: 0.0005  max mem: 15572
Epoch: [7]  [ 270/1322]  eta: 0:06:59  lr: 0.000046  min_lr: 0.000000  loss: 4.3209 (4.3697)  loss_scale: 65536.0000 (70614.4354)  weight_decay: 0.0500 (0.0500)  time: 0.3923  data: 0.0004  max mem: 15572
Epoch: [7]  [ 280/1322]  eta: 0:06:55  lr: 0.000046  min_lr: 0.000000  loss: 4.3326 (4.3677)  loss_scale: 65536.0000 (70433.7082)  weight_decay: 0.0500 (0.0500)  time: 0.3909  data: 0.0003  max mem: 15572
Epoch: [7]  [ 290/1322]  eta: 0:06:50  lr: 0.000046  min_lr: 0.000000  loss: 4.2923 (4.3660)  loss_scale: 65536.0000 (70265.4021)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0003  max mem: 15572
Epoch: [7]  [ 300/1322]  eta: 0:06:45  lr: 0.000046  min_lr: 0.000000  loss: 4.4491 (4.3698)  loss_scale: 65536.0000 (70108.2791)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0003  max mem: 15572
Epoch: [7]  [ 310/1322]  eta: 0:06:40  lr: 0.000046  min_lr: 0.000000  loss: 4.4757 (4.3759)  loss_scale: 65536.0000 (69961.2605)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [7]  [ 320/1322]  eta: 0:06:36  lr: 0.000046  min_lr: 0.000000  loss: 4.3969 (4.3744)  loss_scale: 65536.0000 (69823.4019)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
[2025-01-13 12:49:44,221] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:49:44,221] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:49:46,090] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16977
[2025-01-13 12:49:46,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:49:46,090] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 330/1322]  eta: 0:06:31  lr: 0.000046  min_lr: 0.000000  loss: 4.3501 (4.3745)  loss_scale: 65536.0000 (70683.8429)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0003  max mem: 15572
Epoch: [7]  [ 340/1322]  eta: 0:06:27  lr: 0.000046  min_lr: 0.000000  loss: 4.4923 (4.3759)  loss_scale: 65536.0000 (70532.8798)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
[2025-01-13 12:49:54,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=108, lr=[4.495022418074378e-07, 4.495022418074378e-07, 6.421460597249112e-07, 6.421460597249112e-07, 9.173515138927303e-07, 9.173515138927303e-07, 1.3105021627039007e-06, 1.3105021627039007e-06, 1.872145946719858e-06, 1.872145946719858e-06, 2.674494209599797e-06, 2.674494209599797e-06, 3.8207060137139965e-06, 3.8207060137139965e-06, 5.458151448162853e-06, 5.458151448162853e-06, 7.797359211661218e-06, 7.797359211661218e-06, 1.1139084588087456e-05, 1.1139084588087456e-05, 1.591297798298208e-05, 1.591297798298208e-05, 2.27328256899744e-05, 2.27328256899744e-05, 3.2475465271392e-05, 3.2475465271392e-05, 4.6393521816274294e-05, 4.6393521816274294e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 12:49:54,364] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=29.31127822314084, CurrSamplesPerSec=34.01658127317739, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [ 350/1322]  eta: 0:06:22  lr: 0.000046  min_lr: 0.000000  loss: 4.3232 (4.3727)  loss_scale: 65536.0000 (70390.5185)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0003  max mem: 15572
Epoch: [7]  [ 360/1322]  eta: 0:06:18  lr: 0.000046  min_lr: 0.000000  loss: 4.2690 (4.3709)  loss_scale: 65536.0000 (70256.0443)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0003  max mem: 15572
Epoch: [7]  [ 370/1322]  eta: 0:06:14  lr: 0.000046  min_lr: 0.000000  loss: 4.2966 (4.3669)  loss_scale: 65536.0000 (70128.8194)  weight_decay: 0.0500 (0.0500)  time: 0.3848  data: 0.0004  max mem: 15572
Epoch: [7]  [ 380/1322]  eta: 0:06:10  lr: 0.000046  min_lr: 0.000000  loss: 4.3368 (4.3653)  loss_scale: 65536.0000 (70008.2730)  weight_decay: 0.0500 (0.0500)  time: 0.3946  data: 0.0004  max mem: 15572
Epoch: [7]  [ 390/1322]  eta: 0:06:06  lr: 0.000046  min_lr: 0.000000  loss: 4.2398 (4.3622)  loss_scale: 65536.0000 (69893.8926)  weight_decay: 0.0500 (0.0500)  time: 0.3933  data: 0.0004  max mem: 15572
Epoch: [7]  [ 400/1322]  eta: 0:06:02  lr: 0.000046  min_lr: 0.000000  loss: 4.2552 (4.3621)  loss_scale: 65536.0000 (69785.2170)  weight_decay: 0.0500 (0.0500)  time: 0.3931  data: 0.0004  max mem: 15572
Epoch: [7]  [ 410/1322]  eta: 0:05:58  lr: 0.000046  min_lr: 0.000000  loss: 4.2839 (4.3608)  loss_scale: 65536.0000 (69681.8297)  weight_decay: 0.0500 (0.0500)  time: 0.3951  data: 0.0005  max mem: 15572
Epoch: [7]  [ 420/1322]  eta: 0:05:54  lr: 0.000046  min_lr: 0.000000  loss: 4.2543 (4.3582)  loss_scale: 65536.0000 (69583.3539)  weight_decay: 0.0500 (0.0500)  time: 0.3932  data: 0.0005  max mem: 15572
Epoch: [7]  [ 430/1322]  eta: 0:05:50  lr: 0.000046  min_lr: 0.000000  loss: 4.3031 (4.3570)  loss_scale: 65536.0000 (69489.4478)  weight_decay: 0.0500 (0.0500)  time: 0.3916  data: 0.0005  max mem: 15572
Epoch: [7]  [ 440/1322]  eta: 0:05:47  lr: 0.000046  min_lr: 0.000000  loss: 4.2742 (4.3542)  loss_scale: 65536.0000 (69399.8005)  weight_decay: 0.0500 (0.0500)  time: 0.3963  data: 0.0005  max mem: 15572
Epoch: [7]  [ 450/1322]  eta: 0:05:43  lr: 0.000046  min_lr: 0.000000  loss: 4.3488 (4.3566)  loss_scale: 65536.0000 (69314.1286)  weight_decay: 0.0500 (0.0500)  time: 0.3972  data: 0.0005  max mem: 15572
[2025-01-13 12:50:36,347] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:50:36,347] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:50:37,115] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17108
[2025-01-13 12:50:37,115] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:50:37,115] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 460/1322]  eta: 0:05:39  lr: 0.000046  min_lr: 0.000000  loss: 4.3359 (4.3523)  loss_scale: 65536.0000 (69516.4946)  weight_decay: 0.0500 (0.0500)  time: 0.3913  data: 0.0004  max mem: 15572
Epoch: [7]  [ 470/1322]  eta: 0:05:35  lr: 0.000046  min_lr: 0.000000  loss: 4.3247 (4.3561)  loss_scale: 65536.0000 (69431.9830)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0005  max mem: 15572
Epoch: [7]  [ 480/1322]  eta: 0:05:31  lr: 0.000046  min_lr: 0.000000  loss: 4.5319 (4.3585)  loss_scale: 65536.0000 (69350.9854)  weight_decay: 0.0500 (0.0500)  time: 0.3971  data: 0.0005  max mem: 15572
Epoch: [7]  [ 490/1322]  eta: 0:05:27  lr: 0.000046  min_lr: 0.000000  loss: 4.5172 (4.3578)  loss_scale: 65536.0000 (69273.2872)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0004  max mem: 15572
Epoch: [7]  [ 500/1322]  eta: 0:05:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4957 (4.3611)  loss_scale: 65536.0000 (69198.6906)  weight_decay: 0.0500 (0.0500)  time: 0.3802  data: 0.0003  max mem: 15572
Epoch: [7]  [ 510/1322]  eta: 0:05:18  lr: 0.000046  min_lr: 0.000000  loss: 4.5728 (4.3662)  loss_scale: 65536.0000 (69127.0137)  weight_decay: 0.0500 (0.0500)  time: 0.3811  data: 0.0003  max mem: 15572
Epoch: [7]  [ 520/1322]  eta: 0:05:14  lr: 0.000046  min_lr: 0.000000  loss: 4.3852 (4.3657)  loss_scale: 65536.0000 (69058.0883)  weight_decay: 0.0500 (0.0500)  time: 0.3791  data: 0.0003  max mem: 15572
Epoch: [7]  [ 530/1322]  eta: 0:05:10  lr: 0.000046  min_lr: 0.000000  loss: 4.2627 (4.3635)  loss_scale: 65536.0000 (68991.7589)  weight_decay: 0.0500 (0.0500)  time: 0.3832  data: 0.0003  max mem: 15572
Epoch: [7]  [ 540/1322]  eta: 0:05:06  lr: 0.000046  min_lr: 0.000000  loss: 4.1956 (4.3607)  loss_scale: 65536.0000 (68927.8817)  weight_decay: 0.0500 (0.0500)  time: 0.3813  data: 0.0003  max mem: 15572
Epoch: [7]  [ 550/1322]  eta: 0:05:02  lr: 0.000046  min_lr: 0.000000  loss: 4.3846 (4.3636)  loss_scale: 65536.0000 (68866.3230)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0002  max mem: 15572
Epoch: [7]  [ 560/1322]  eta: 0:04:58  lr: 0.000046  min_lr: 0.000000  loss: 4.4039 (4.3634)  loss_scale: 65536.0000 (68806.9590)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [7]  [ 570/1322]  eta: 0:04:54  lr: 0.000046  min_lr: 0.000000  loss: 4.4442 (4.3646)  loss_scale: 65536.0000 (68749.6743)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0003  max mem: 15572
Epoch: [7]  [ 580/1322]  eta: 0:04:50  lr: 0.000046  min_lr: 0.000000  loss: 4.5299 (4.3665)  loss_scale: 65536.0000 (68694.3614)  weight_decay: 0.0500 (0.0500)  time: 0.3881  data: 0.0004  max mem: 15572
[2025-01-13 12:51:26,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:51:26,613] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [ 590/1322]  eta: 0:04:46  lr: 0.000046  min_lr: 0.000000  loss: 4.4175 (4.3661)  loss_scale: 65536.0000 (69195.3706)  weight_decay: 0.0500 (0.0500)  time: 0.3907  data: 0.0005  max mem: 15572
Epoch: [7]  [ 600/1322]  eta: 0:04:42  lr: 0.000046  min_lr: 0.000000  loss: 4.3550 (4.3654)  loss_scale: 131072.0000 (70224.9318)  weight_decay: 0.0500 (0.0500)  time: 0.3917  data: 0.0004  max mem: 15572
[2025-01-13 12:51:32,901] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17253
[2025-01-13 12:51:32,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:51:32,902] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 610/1322]  eta: 0:04:38  lr: 0.000046  min_lr: 0.000000  loss: 4.4911 (4.3679)  loss_scale: 131072.0000 (70255.4501)  weight_decay: 0.0500 (0.0500)  time: 0.3950  data: 0.0005  max mem: 15572
Epoch: [7]  [ 620/1322]  eta: 0:04:34  lr: 0.000046  min_lr: 0.000000  loss: 4.3259 (4.3664)  loss_scale: 65536.0000 (70179.4525)  weight_decay: 0.0500 (0.0500)  time: 0.3937  data: 0.0005  max mem: 15572
Epoch: [7]  [ 630/1322]  eta: 0:04:30  lr: 0.000046  min_lr: 0.000000  loss: 4.2873 (4.3673)  loss_scale: 65536.0000 (70105.8637)  weight_decay: 0.0500 (0.0500)  time: 0.3850  data: 0.0005  max mem: 15572
Epoch: [7]  [ 640/1322]  eta: 0:04:26  lr: 0.000046  min_lr: 0.000000  loss: 4.5052 (4.3693)  loss_scale: 65536.0000 (70034.5710)  weight_decay: 0.0500 (0.0500)  time: 0.3797  data: 0.0003  max mem: 15572
Epoch: [7]  [ 650/1322]  eta: 0:04:22  lr: 0.000046  min_lr: 0.000000  loss: 4.3530 (4.3680)  loss_scale: 65536.0000 (69965.4685)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0003  max mem: 15572
Epoch: [7]  [ 660/1322]  eta: 0:04:18  lr: 0.000046  min_lr: 0.000000  loss: 4.3530 (4.3680)  loss_scale: 65536.0000 (69898.4569)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [7]  [ 670/1322]  eta: 0:04:14  lr: 0.000046  min_lr: 0.000000  loss: 4.4273 (4.3693)  loss_scale: 65536.0000 (69833.4426)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [7]  [ 680/1322]  eta: 0:04:10  lr: 0.000046  min_lr: 0.000000  loss: 4.4733 (4.3712)  loss_scale: 65536.0000 (69770.3377)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [7]  [ 690/1322]  eta: 0:04:06  lr: 0.000046  min_lr: 0.000000  loss: 4.4872 (4.3722)  loss_scale: 65536.0000 (69709.0593)  weight_decay: 0.0500 (0.0500)  time: 0.3773  data: 0.0003  max mem: 15572
Epoch: [7]  [ 700/1322]  eta: 0:04:02  lr: 0.000046  min_lr: 0.000000  loss: 4.3930 (4.3726)  loss_scale: 65536.0000 (69649.5292)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0003  max mem: 15572
Epoch: [7]  [ 710/1322]  eta: 0:03:58  lr: 0.000046  min_lr: 0.000000  loss: 4.4311 (4.3721)  loss_scale: 65536.0000 (69591.6737)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [7]  [ 720/1322]  eta: 0:03:54  lr: 0.000046  min_lr: 0.000000  loss: 4.3460 (4.3695)  loss_scale: 65536.0000 (69535.4230)  weight_decay: 0.0500 (0.0500)  time: 0.3831  data: 0.0004  max mem: 15572
Epoch: [7]  [ 730/1322]  eta: 0:03:50  lr: 0.000046  min_lr: 0.000000  loss: 4.1404 (4.3651)  loss_scale: 65536.0000 (69480.7114)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0004  max mem: 15572
[2025-01-13 12:52:22,102] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:52:22,102] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:52:22,949] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17384
[2025-01-13 12:52:22,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:52:22,949] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 740/1322]  eta: 0:03:46  lr: 0.000046  min_lr: 0.000000  loss: 4.1666 (4.3651)  loss_scale: 65536.0000 (69604.3617)  weight_decay: 0.0500 (0.0500)  time: 0.3965  data: 0.0005  max mem: 15572
Epoch: [7]  [ 750/1322]  eta: 0:03:42  lr: 0.000046  min_lr: 0.000000  loss: 4.4197 (4.3652)  loss_scale: 65536.0000 (69550.1891)  weight_decay: 0.0500 (0.0500)  time: 0.3986  data: 0.0005  max mem: 15572
Epoch: [7]  [ 760/1322]  eta: 0:03:39  lr: 0.000046  min_lr: 0.000000  loss: 4.4392 (4.3657)  loss_scale: 65536.0000 (69497.4402)  weight_decay: 0.0500 (0.0500)  time: 0.3975  data: 0.0005  max mem: 15572
Epoch: [7]  [ 770/1322]  eta: 0:03:35  lr: 0.000046  min_lr: 0.000000  loss: 4.3424 (4.3647)  loss_scale: 65536.0000 (69446.0597)  weight_decay: 0.0500 (0.0500)  time: 0.3983  data: 0.0005  max mem: 15572
Epoch: [7]  [ 780/1322]  eta: 0:03:31  lr: 0.000046  min_lr: 0.000000  loss: 4.3106 (4.3644)  loss_scale: 65536.0000 (69395.9949)  weight_decay: 0.0500 (0.0500)  time: 0.3875  data: 0.0004  max mem: 15572
Epoch: [7]  [ 790/1322]  eta: 0:03:27  lr: 0.000046  min_lr: 0.000000  loss: 4.3853 (4.3652)  loss_scale: 65536.0000 (69347.1960)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
Epoch: [7]  [ 800/1322]  eta: 0:03:23  lr: 0.000046  min_lr: 0.000000  loss: 4.3722 (4.3640)  loss_scale: 65536.0000 (69299.6155)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0002  max mem: 15572
Epoch: [7]  [ 810/1322]  eta: 0:03:19  lr: 0.000046  min_lr: 0.000000  loss: 4.3485 (4.3642)  loss_scale: 65536.0000 (69253.2084)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [7]  [ 820/1322]  eta: 0:03:15  lr: 0.000046  min_lr: 0.000000  loss: 4.4016 (4.3645)  loss_scale: 65536.0000 (69207.9318)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0003  max mem: 15572
Epoch: [7]  [ 830/1322]  eta: 0:03:11  lr: 0.000046  min_lr: 0.000000  loss: 4.4268 (4.3655)  loss_scale: 65536.0000 (69163.7449)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0003  max mem: 15572
Epoch: [7]  [ 840/1322]  eta: 0:03:07  lr: 0.000046  min_lr: 0.000000  loss: 4.4553 (4.3669)  loss_scale: 65536.0000 (69120.6088)  weight_decay: 0.0500 (0.0500)  time: 0.3787  data: 0.0002  max mem: 15572
Epoch: [7]  [ 850/1322]  eta: 0:03:03  lr: 0.000046  min_lr: 0.000000  loss: 4.4553 (4.3658)  loss_scale: 65536.0000 (69078.4865)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [7]  [ 860/1322]  eta: 0:02:59  lr: 0.000046  min_lr: 0.000000  loss: 4.3417 (4.3656)  loss_scale: 65536.0000 (69037.3426)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0003  max mem: 15572
[2025-01-13 12:53:12,421] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:53:12,421] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:53:14,370] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17518
[2025-01-13 12:53:14,370] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:53:14,370] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 870/1322]  eta: 0:02:55  lr: 0.000046  min_lr: 0.000000  loss: 4.4145 (4.3666)  loss_scale: 65536.0000 (69373.3548)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0004  max mem: 15572
Epoch: [7]  [ 880/1322]  eta: 0:02:51  lr: 0.000046  min_lr: 0.000000  loss: 4.4145 (4.3668)  loss_scale: 65536.0000 (69329.7980)  weight_decay: 0.0500 (0.0500)  time: 0.3955  data: 0.0006  max mem: 15572
Epoch: [7]  [ 890/1322]  eta: 0:02:47  lr: 0.000046  min_lr: 0.000000  loss: 4.3412 (4.3657)  loss_scale: 65536.0000 (69287.2189)  weight_decay: 0.0500 (0.0500)  time: 0.3949  data: 0.0005  max mem: 15572
Epoch: [7]  [ 900/1322]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000000  loss: 4.2708 (4.3640)  loss_scale: 65536.0000 (69245.5849)  weight_decay: 0.0500 (0.0500)  time: 0.3926  data: 0.0005  max mem: 15572
Epoch: [7]  [ 910/1322]  eta: 0:02:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2553 (4.3639)  loss_scale: 65536.0000 (69204.8650)  weight_decay: 0.0500 (0.0500)  time: 0.3932  data: 0.0004  max mem: 15572
Epoch: [7]  [ 920/1322]  eta: 0:02:36  lr: 0.000046  min_lr: 0.000000  loss: 4.4745 (4.3660)  loss_scale: 65536.0000 (69165.0293)  weight_decay: 0.0500 (0.0500)  time: 0.3893  data: 0.0004  max mem: 15572
Epoch: [7]  [ 930/1322]  eta: 0:02:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4530 (4.3664)  loss_scale: 65536.0000 (69126.0494)  weight_decay: 0.0500 (0.0500)  time: 0.3825  data: 0.0004  max mem: 15572
Epoch: [7]  [ 940/1322]  eta: 0:02:28  lr: 0.000046  min_lr: 0.000000  loss: 4.4107 (4.3666)  loss_scale: 65536.0000 (69087.8980)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0003  max mem: 15572
Epoch: [7]  [ 950/1322]  eta: 0:02:24  lr: 0.000046  min_lr: 0.000000  loss: 4.4139 (4.3673)  loss_scale: 65536.0000 (69050.5489)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0003  max mem: 15572
Epoch: [7]  [ 960/1322]  eta: 0:02:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3709 (4.3684)  loss_scale: 65536.0000 (69013.9771)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0003  max mem: 15572
Epoch: [7]  [ 970/1322]  eta: 0:02:16  lr: 0.000046  min_lr: 0.000000  loss: 4.4539 (4.3687)  loss_scale: 65536.0000 (68978.1586)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0003  max mem: 15572
Epoch: [7]  [ 980/1322]  eta: 0:02:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4812 (4.3695)  loss_scale: 65536.0000 (68943.0703)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0003  max mem: 15572
Epoch: [7]  [ 990/1322]  eta: 0:02:08  lr: 0.000046  min_lr: 0.000000  loss: 4.5723 (4.3703)  loss_scale: 65536.0000 (68908.6902)  weight_decay: 0.0500 (0.0500)  time: 0.3797  data: 0.0002  max mem: 15572
[2025-01-13 12:54:03,813] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:54:03,813] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:54:04,180] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17648
[2025-01-13 12:54:04,181] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:54:04,181] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1000/1322]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000000  loss: 4.3569 (4.3698)  loss_scale: 65536.0000 (68940.4675)  weight_decay: 0.0500 (0.0500)  time: 0.3879  data: 0.0003  max mem: 15572
Epoch: [7]  [1010/1322]  eta: 0:02:01  lr: 0.000046  min_lr: 0.000000  loss: 4.4116 (4.3705)  loss_scale: 65536.0000 (68906.7933)  weight_decay: 0.0500 (0.0500)  time: 0.3876  data: 0.0003  max mem: 15572
Epoch: [7]  [1020/1322]  eta: 0:01:57  lr: 0.000046  min_lr: 0.000000  loss: 4.4723 (4.3699)  loss_scale: 65536.0000 (68873.7786)  weight_decay: 0.0500 (0.0500)  time: 0.3843  data: 0.0003  max mem: 15572
Epoch: [7]  [1030/1322]  eta: 0:01:53  lr: 0.000046  min_lr: 0.000000  loss: 4.2916 (4.3691)  loss_scale: 65536.0000 (68841.4045)  weight_decay: 0.0500 (0.0500)  time: 0.3903  data: 0.0004  max mem: 15572
Epoch: [7]  [1040/1322]  eta: 0:01:49  lr: 0.000046  min_lr: 0.000000  loss: 4.2916 (4.3686)  loss_scale: 65536.0000 (68809.6523)  weight_decay: 0.0500 (0.0500)  time: 0.3884  data: 0.0004  max mem: 15572
Epoch: [7]  [1050/1322]  eta: 0:01:45  lr: 0.000046  min_lr: 0.000000  loss: 4.3237 (4.3683)  loss_scale: 65536.0000 (68778.5043)  weight_decay: 0.0500 (0.0500)  time: 0.3854  data: 0.0004  max mem: 15572
Epoch: [7]  [1060/1322]  eta: 0:01:41  lr: 0.000046  min_lr: 0.000000  loss: 4.3237 (4.3674)  loss_scale: 65536.0000 (68747.9434)  weight_decay: 0.0500 (0.0500)  time: 0.3887  data: 0.0004  max mem: 15572
Epoch: [7]  [1070/1322]  eta: 0:01:37  lr: 0.000046  min_lr: 0.000000  loss: 4.2827 (4.3669)  loss_scale: 65536.0000 (68717.9533)  weight_decay: 0.0500 (0.0500)  time: 0.3882  data: 0.0003  max mem: 15572
Epoch: [7]  [1080/1322]  eta: 0:01:33  lr: 0.000046  min_lr: 0.000000  loss: 4.2203 (4.3655)  loss_scale: 65536.0000 (68688.5180)  weight_decay: 0.0500 (0.0500)  time: 0.3793  data: 0.0003  max mem: 15572
Epoch: [7]  [1090/1322]  eta: 0:01:29  lr: 0.000046  min_lr: 0.000000  loss: 4.3589 (4.3669)  loss_scale: 65536.0000 (68659.6224)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [7]  [1100/1322]  eta: 0:01:26  lr: 0.000046  min_lr: 0.000000  loss: 4.4868 (4.3675)  loss_scale: 65536.0000 (68631.2516)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [7]  [1110/1322]  eta: 0:01:22  lr: 0.000046  min_lr: 0.000000  loss: 4.4435 (4.3681)  loss_scale: 65536.0000 (68603.3915)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0003  max mem: 15572
Epoch: [7]  [1120/1322]  eta: 0:01:18  lr: 0.000046  min_lr: 0.000000  loss: 4.3621 (4.3677)  loss_scale: 65536.0000 (68576.0285)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0003  max mem: 15572
[2025-01-13 12:54:53,751] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:54:53,751] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1130/1322]  eta: 0:01:14  lr: 0.000046  min_lr: 0.000000  loss: 4.3015 (4.3675)  loss_scale: 65536.0000 (68838.8753)  weight_decay: 0.0500 (0.0500)  time: 0.3815  data: 0.0003  max mem: 15572
Epoch: [7]  [1140/1322]  eta: 0:01:10  lr: 0.000046  min_lr: 0.000000  loss: 4.3591 (4.3673)  loss_scale: 131072.0000 (69384.3015)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0003  max mem: 15572
Epoch: [7]  [1150/1322]  eta: 0:01:06  lr: 0.000046  min_lr: 0.000000  loss: 4.3591 (4.3673)  loss_scale: 131072.0000 (69920.2502)  weight_decay: 0.0500 (0.0500)  time: 0.3796  data: 0.0003  max mem: 15572
[2025-01-13 12:55:03,984] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17804
[2025-01-13 12:55:03,985] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:55:03,985] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1160/1322]  eta: 0:01:02  lr: 0.000046  min_lr: 0.000000  loss: 4.4136 (4.3686)  loss_scale: 131072.0000 (69995.3833)  weight_decay: 0.0500 (0.0500)  time: 0.3817  data: 0.0004  max mem: 15572
Epoch: [7]  [1170/1322]  eta: 0:00:58  lr: 0.000046  min_lr: 0.000000  loss: 4.3064 (4.3658)  loss_scale: 65536.0000 (69957.3015)  weight_decay: 0.0500 (0.0500)  time: 0.3896  data: 0.0005  max mem: 15572
Epoch: [7]  [1180/1322]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000000  loss: 4.1721 (4.3654)  loss_scale: 65536.0000 (69919.8645)  weight_decay: 0.0500 (0.0500)  time: 0.3951  data: 0.0006  max mem: 15572
Epoch: [7]  [1190/1322]  eta: 0:00:51  lr: 0.000046  min_lr: 0.000000  loss: 4.3428 (4.3659)  loss_scale: 65536.0000 (69883.0563)  weight_decay: 0.0500 (0.0500)  time: 0.3912  data: 0.0005  max mem: 15572
Epoch: [7]  [1200/1322]  eta: 0:00:47  lr: 0.000046  min_lr: 0.000000  loss: 4.5048 (4.3669)  loss_scale: 65536.0000 (69846.8609)  weight_decay: 0.0500 (0.0500)  time: 0.3885  data: 0.0004  max mem: 15572
Epoch: [7]  [1210/1322]  eta: 0:00:43  lr: 0.000046  min_lr: 0.000000  loss: 4.4269 (4.3663)  loss_scale: 65536.0000 (69811.2634)  weight_decay: 0.0500 (0.0500)  time: 0.3819  data: 0.0003  max mem: 15572
Epoch: [7]  [1220/1322]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000000  loss: 4.2612 (4.3654)  loss_scale: 65536.0000 (69776.2490)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
Epoch: [7]  [1230/1322]  eta: 0:00:35  lr: 0.000046  min_lr: 0.000000  loss: 4.3620 (4.3656)  loss_scale: 65536.0000 (69741.8034)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0003  max mem: 15572
Epoch: [7]  [1240/1322]  eta: 0:00:31  lr: 0.000046  min_lr: 0.000000  loss: 4.2940 (4.3639)  loss_scale: 65536.0000 (69707.9130)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0003  max mem: 15572
Epoch: [7]  [1250/1322]  eta: 0:00:27  lr: 0.000046  min_lr: 0.000000  loss: 4.3023 (4.3638)  loss_scale: 65536.0000 (69674.5643)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0003  max mem: 15572
Epoch: [7]  [1260/1322]  eta: 0:00:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4413 (4.3643)  loss_scale: 65536.0000 (69641.7446)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0003  max mem: 15572
Epoch: [7]  [1270/1322]  eta: 0:00:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3702 (4.3632)  loss_scale: 65536.0000 (69609.4414)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0003  max mem: 15572
Epoch: [7]  [1280/1322]  eta: 0:00:16  lr: 0.000046  min_lr: 0.000000  loss: 4.2832 (4.3637)  loss_scale: 65536.0000 (69577.6425)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0003  max mem: 15572
[2025-01-13 12:55:53,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:55:53,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1290/1322]  eta: 0:00:12  lr: 0.000046  min_lr: 0.000000  loss: 4.2876 (4.3635)  loss_scale: 65536.0000 (70003.2099)  weight_decay: 0.0500 (0.0500)  time: 0.3804  data: 0.0003  max mem: 15572
[2025-01-13 12:55:59,406] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17949
[2025-01-13 12:55:59,406] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:55:59,406] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1300/1322]  eta: 0:00:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2589 (4.3631)  loss_scale: 131072.0000 (70321.4881)  weight_decay: 0.0500 (0.0500)  time: 0.3883  data: 0.0004  max mem: 15572
Epoch: [7]  [1310/1322]  eta: 0:00:04  lr: 0.000046  min_lr: 0.000000  loss: 4.3623 (4.3632)  loss_scale: 65536.0000 (70284.9855)  weight_decay: 0.0500 (0.0500)  time: 0.3887  data: 0.0005  max mem: 15572
Epoch: [7]  [1320/1322]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000000  loss: 4.3334 (4.3630)  loss_scale: 65536.0000 (70249.0356)  weight_decay: 0.0500 (0.0500)  time: 0.3814  data: 0.0004  max mem: 15572
Epoch: [7]  [1321/1322]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000000  loss: 4.3623 (4.3634)  loss_scale: 65536.0000 (70245.4705)  weight_decay: 0.0500 (0.0500)  time: 0.3808  data: 0.0004  max mem: 15572
Epoch: [7] Total time: 0:08:31 (0.3869 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 4.3623 (4.3634)  loss_scale: 65536.0000 (70245.4705)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 3488
Indices to remove: tensor([    0,     1,     2,  ..., 33646, 33675, 33676], device='cuda:0')
length of data loader train is: 1031
num_training_steps_per_epoch is: 1031
Change step level LR scheduler!
Set warmup steps = 5155
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:15:17  loss: 0.5785 (0.5785)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.3746  data: 3.1891  max mem: 15572
Val:  [ 10/272]  eta: 0:02:04  loss: 4.2582 (3.7579)  acc1: 0.0000 (24.7475)  acc5: 27.7778 (33.8384)  time: 0.4770  data: 0.2906  max mem: 15572
Val:  [ 20/272]  eta: 0:01:22  loss: 3.8707 (3.6750)  acc1: 5.5556 (21.4286)  acc5: 33.3333 (39.6825)  time: 0.1746  data: 0.0007  max mem: 15572
Val:  [ 30/272]  eta: 0:01:07  loss: 3.8800 (3.8070)  acc1: 5.5556 (15.4122)  acc5: 38.8889 (38.1720)  time: 0.1727  data: 0.0006  max mem: 15572
Val:  [ 40/272]  eta: 0:00:59  loss: 3.8320 (3.7418)  acc1: 5.5556 (15.1762)  acc5: 38.8889 (40.5149)  time: 0.1785  data: 0.0006  max mem: 15572
Val:  [ 50/272]  eta: 0:00:52  loss: 3.4580 (3.6750)  acc1: 11.1111 (16.9935)  acc5: 55.5556 (42.8105)  time: 0.1736  data: 0.0006  max mem: 15572
Val:  [ 60/272]  eta: 0:00:47  loss: 2.4832 (3.5017)  acc1: 38.8889 (23.3151)  acc5: 72.2222 (46.7213)  time: 0.1684  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:43  loss: 2.5974 (3.4003)  acc1: 50.0000 (24.4131)  acc5: 72.2222 (50.7042)  time: 0.1618  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:40  loss: 3.1394 (3.4205)  acc1: 11.1111 (24.7599)  acc5: 72.2222 (49.6571)  time: 0.1702  data: 0.0006  max mem: 15572
Val:  [ 90/272]  eta: 0:00:37  loss: 4.4934 (3.5364)  acc1: 0.0000 (22.1612)  acc5: 5.5556 (45.1770)  time: 0.1771  data: 0.0006  max mem: 15572
Val:  [100/272]  eta: 0:00:35  loss: 4.3290 (3.5948)  acc1: 0.0000 (21.5622)  acc5: 11.1111 (44.0044)  time: 0.1745  data: 0.0005  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 4.1104 (3.6659)  acc1: 0.0000 (19.6196)  acc5: 22.2222 (42.1421)  time: 0.1813  data: 0.0026  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 4.2631 (3.7057)  acc1: 0.0000 (18.4573)  acc5: 22.2222 (41.3223)  time: 0.1814  data: 0.0026  max mem: 15572
Val:  [130/272]  eta: 0:00:28  loss: 4.0146 (3.6488)  acc1: 5.5556 (20.0594)  acc5: 33.3333 (42.8753)  time: 0.1751  data: 0.0007  max mem: 15572
Val:  [140/272]  eta: 0:00:26  loss: 3.3034 (3.6298)  acc1: 11.1111 (20.7644)  acc5: 50.0000 (43.0260)  time: 0.1777  data: 0.0007  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.8531 (3.6440)  acc1: 5.5556 (20.0515)  acc5: 33.3333 (42.4945)  time: 0.1765  data: 0.0006  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.7438 (3.6347)  acc1: 11.1111 (20.6694)  acc5: 38.8889 (43.4092)  time: 0.1721  data: 0.0026  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.7901 (3.6613)  acc1: 5.5556 (19.6556)  acc5: 44.4444 (42.5601)  time: 0.1694  data: 0.0026  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.8229 (3.6521)  acc1: 0.0000 (19.8895)  acc5: 44.4444 (43.5543)  time: 0.1707  data: 0.0006  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 4.0814 (3.6843)  acc1: 0.0000 (18.9354)  acc5: 38.8889 (42.5247)  time: 0.1793  data: 0.0007  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 4.1504 (3.6822)  acc1: 5.5556 (18.9331)  acc5: 38.8889 (43.5047)  time: 0.1817  data: 0.0007  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.6645 (3.7022)  acc1: 16.6667 (18.6940)  acc5: 44.4444 (43.0753)  time: 0.1749  data: 0.0007  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 4.0622 (3.7052)  acc1: 11.1111 (18.9291)  acc5: 38.8889 (43.0116)  time: 0.1714  data: 0.0006  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.2609 (3.6826)  acc1: 44.4444 (20.2261)  acc5: 61.1111 (44.2520)  time: 0.1755  data: 0.0006  max mem: 15572
Val:  [240/272]  eta: 0:00:06  loss: 3.0294 (3.6596)  acc1: 50.0000 (21.0466)  acc5: 77.7778 (45.5740)  time: 0.1724  data: 0.0007  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.4278 (3.6779)  acc1: 16.6667 (20.7393)  acc5: 61.1111 (45.2634)  time: 0.1622  data: 0.0005  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 3.1584 (3.6090)  acc1: 38.8889 (22.9034)  acc5: 72.2222 (46.8072)  time: 0.1488  data: 0.0002  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.6817 (3.6049)  acc1: 61.1111 (23.0422)  acc5: 77.7778 (46.9865)  time: 0.1366  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.6817 (3.6087)  acc1: 61.1111 (23.0186)  acc5: 77.7778 (46.9588)  time: 0.1314  data: 0.0001  max mem: 15572
Val: Total time: 0:00:49 (0.1831 s / it)
* Acc@1 23.019 Acc@5 46.959 loss 3.609
Accuracy of the network on the 4883 val videos: 23.0%
[2025-01-13 12:56:58,818] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 12:56:58,820] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 12:56:58,820] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 12:57:01,203] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 12:57:01,203] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 23.02%
Epoch: [8]  [   0/1031]  eta: 1:17:35  lr: 0.000046  min_lr: 0.000000  loss: 3.8513 (3.8513)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.5156  data: 4.1351  max mem: 15572
Epoch: [8]  [  10/1031]  eta: 0:12:57  lr: 0.000046  min_lr: 0.000000  loss: 4.1383 (4.1179)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7610  data: 0.3762  max mem: 15572
Epoch: [8]  [  20/1031]  eta: 0:09:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2692 (4.2389)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3824  data: 0.0003  max mem: 15572
[2025-01-13 12:57:15,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=115, lr=[4.458548033881126e-07, 4.458548033881126e-07, 6.369354334115896e-07, 6.369354334115896e-07, 9.099077620165566e-07, 9.099077620165566e-07, 1.2998682314522238e-06, 1.2998682314522238e-06, 1.8569546163603199e-06, 1.8569546163603199e-06, 2.6527923090861714e-06, 2.6527923090861714e-06, 3.7897032986945304e-06, 3.7897032986945304e-06, 5.4138618552779015e-06, 5.4138618552779015e-06, 7.734088364682715e-06, 7.734088364682715e-06, 1.1048697663832453e-05, 1.1048697663832453e-05, 1.5783853805474934e-05, 1.5783853805474934e-05, 2.2548362579249907e-05, 2.2548362579249907e-05, 3.221194654178558e-05, 3.221194654178558e-05, 4.601706648826512e-05, 4.601706648826512e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 12:57:15,660] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=29.49596069950644, CurrSamplesPerSec=33.926103775562545, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [  30/1031]  eta: 0:08:35  lr: 0.000046  min_lr: 0.000000  loss: 4.3966 (4.2637)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0002  max mem: 15572
Epoch: [8]  [  40/1031]  eta: 0:07:55  lr: 0.000046  min_lr: 0.000000  loss: 4.3156 (4.2816)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
Epoch: [8]  [  50/1031]  eta: 0:07:31  lr: 0.000046  min_lr: 0.000000  loss: 4.3472 (4.3263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [8]  [  60/1031]  eta: 0:07:14  lr: 0.000046  min_lr: 0.000000  loss: 4.1983 (4.2838)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3795  data: 0.0002  max mem: 15572
Epoch: [8]  [  70/1031]  eta: 0:07:03  lr: 0.000046  min_lr: 0.000000  loss: 4.1755 (4.2938)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3904  data: 0.0003  max mem: 15572
Epoch: [8]  [  80/1031]  eta: 0:06:54  lr: 0.000046  min_lr: 0.000000  loss: 4.3819 (4.3051)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4005  data: 0.0004  max mem: 15572
Epoch: [8]  [  90/1031]  eta: 0:06:46  lr: 0.000046  min_lr: 0.000000  loss: 4.3047 (4.3014)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4037  data: 0.0004  max mem: 15572
Epoch: [8]  [ 100/1031]  eta: 0:06:38  lr: 0.000046  min_lr: 0.000000  loss: 4.3287 (4.3152)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3994  data: 0.0004  max mem: 15572
[2025-01-13 12:57:46,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:57:46,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:57:48,384] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18083
[2025-01-13 12:57:48,384] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:57:48,384] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 110/1031]  eta: 0:06:31  lr: 0.000046  min_lr: 0.000000  loss: 4.4309 (4.3281)  loss_scale: 65536.0000 (68488.0721)  weight_decay: 0.0500 (0.0500)  time: 0.3915  data: 0.0004  max mem: 15572
Epoch: [8]  [ 120/1031]  eta: 0:06:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4670 (4.3425)  loss_scale: 65536.0000 (68244.0992)  weight_decay: 0.0500 (0.0500)  time: 0.3839  data: 0.0003  max mem: 15572
Epoch: [8]  [ 130/1031]  eta: 0:06:16  lr: 0.000046  min_lr: 0.000000  loss: 4.5269 (4.3553)  loss_scale: 65536.0000 (68037.3740)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [8]  [ 140/1031]  eta: 0:06:09  lr: 0.000046  min_lr: 0.000000  loss: 4.5156 (4.3574)  loss_scale: 65536.0000 (67859.9716)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [8]  [ 150/1031]  eta: 0:06:02  lr: 0.000046  min_lr: 0.000000  loss: 4.2952 (4.3532)  loss_scale: 65536.0000 (67706.0662)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0002  max mem: 15572
Epoch: [8]  [ 160/1031]  eta: 0:05:56  lr: 0.000046  min_lr: 0.000000  loss: 4.1903 (4.3428)  loss_scale: 65536.0000 (67571.2795)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [8]  [ 170/1031]  eta: 0:05:50  lr: 0.000046  min_lr: 0.000000  loss: 4.2358 (4.3420)  loss_scale: 65536.0000 (67452.2573)  weight_decay: 0.0500 (0.0500)  time: 0.3725  data: 0.0002  max mem: 15572
Epoch: [8]  [ 180/1031]  eta: 0:05:45  lr: 0.000046  min_lr: 0.000000  loss: 4.3061 (4.3443)  loss_scale: 65536.0000 (67346.3867)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [8]  [ 190/1031]  eta: 0:05:39  lr: 0.000046  min_lr: 0.000000  loss: 4.3273 (4.3395)  loss_scale: 65536.0000 (67251.6021)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [8]  [ 200/1031]  eta: 0:05:34  lr: 0.000046  min_lr: 0.000000  loss: 4.3689 (4.3442)  loss_scale: 65536.0000 (67166.2488)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
Epoch: [8]  [ 210/1031]  eta: 0:05:29  lr: 0.000046  min_lr: 0.000000  loss: 4.3915 (4.3400)  loss_scale: 65536.0000 (67088.9858)  weight_decay: 0.0500 (0.0500)  time: 0.3819  data: 0.0003  max mem: 15572
Epoch: [8]  [ 220/1031]  eta: 0:05:25  lr: 0.000046  min_lr: 0.000000  loss: 4.3731 (4.3449)  loss_scale: 65536.0000 (67018.7149)  weight_decay: 0.0500 (0.0500)  time: 0.3882  data: 0.0004  max mem: 15572
Epoch: [8]  [ 230/1031]  eta: 0:05:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3538 (4.3458)  loss_scale: 65536.0000 (66954.5281)  weight_decay: 0.0500 (0.0500)  time: 0.3885  data: 0.0004  max mem: 15572
[2025-01-13 12:58:37,348] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:58:37,349] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 240/1031]  eta: 0:05:16  lr: 0.000046  min_lr: 0.000000  loss: 4.4136 (4.3490)  loss_scale: 65536.0000 (67439.5353)  weight_decay: 0.0500 (0.0500)  time: 0.3919  data: 0.0004  max mem: 15572
[2025-01-13 12:58:40,111] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18219
[2025-01-13 12:58:40,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:58:40,112] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 250/1031]  eta: 0:05:12  lr: 0.000046  min_lr: 0.000000  loss: 4.2830 (4.3433)  loss_scale: 65536.0000 (68669.1952)  weight_decay: 0.0500 (0.0500)  time: 0.3958  data: 0.0005  max mem: 15572
Epoch: [8]  [ 260/1031]  eta: 0:05:07  lr: 0.000046  min_lr: 0.000000  loss: 4.2506 (4.3419)  loss_scale: 65536.0000 (68549.1494)  weight_decay: 0.0500 (0.0500)  time: 0.3877  data: 0.0004  max mem: 15572
Epoch: [8]  [ 270/1031]  eta: 0:05:03  lr: 0.000046  min_lr: 0.000000  loss: 4.3080 (4.3457)  loss_scale: 65536.0000 (68437.9631)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0002  max mem: 15572
Epoch: [8]  [ 280/1031]  eta: 0:04:58  lr: 0.000046  min_lr: 0.000000  loss: 4.4113 (4.3470)  loss_scale: 65536.0000 (68334.6904)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0002  max mem: 15572
Epoch: [8]  [ 290/1031]  eta: 0:04:53  lr: 0.000046  min_lr: 0.000000  loss: 4.4113 (4.3509)  loss_scale: 65536.0000 (68238.5155)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0003  max mem: 15572
Epoch: [8]  [ 300/1031]  eta: 0:04:49  lr: 0.000046  min_lr: 0.000000  loss: 4.5385 (4.3536)  loss_scale: 65536.0000 (68148.7309)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0002  max mem: 15572
Epoch: [8]  [ 310/1031]  eta: 0:04:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2971 (4.3521)  loss_scale: 65536.0000 (68064.7203)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0002  max mem: 15572
Epoch: [8]  [ 320/1031]  eta: 0:04:40  lr: 0.000046  min_lr: 0.000000  loss: 4.3902 (4.3557)  loss_scale: 65536.0000 (67985.9439)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0002  max mem: 15572
Epoch: [8]  [ 330/1031]  eta: 0:04:36  lr: 0.000046  min_lr: 0.000000  loss: 4.3902 (4.3527)  loss_scale: 65536.0000 (67911.9275)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [8]  [ 340/1031]  eta: 0:04:32  lr: 0.000046  min_lr: 0.000000  loss: 4.2868 (4.3540)  loss_scale: 65536.0000 (67842.2522)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0002  max mem: 15572
Epoch: [8]  [ 350/1031]  eta: 0:04:28  lr: 0.000046  min_lr: 0.000000  loss: 4.2920 (4.3523)  loss_scale: 65536.0000 (67776.5470)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0003  max mem: 15572
Epoch: [8]  [ 360/1031]  eta: 0:04:24  lr: 0.000046  min_lr: 0.000000  loss: 4.2920 (4.3491)  loss_scale: 65536.0000 (67714.4820)  weight_decay: 0.0500 (0.0500)  time: 0.3868  data: 0.0004  max mem: 15572
Epoch: [8]  [ 370/1031]  eta: 0:04:20  lr: 0.000046  min_lr: 0.000000  loss: 4.1955 (4.3440)  loss_scale: 65536.0000 (67655.7628)  weight_decay: 0.0500 (0.0500)  time: 0.3882  data: 0.0004  max mem: 15572
[2025-01-13 12:59:29,168] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 12:59:29,169] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 12:59:30,316] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18351
[2025-01-13 12:59:30,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 12:59:30,317] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 380/1031]  eta: 0:04:15  lr: 0.000046  min_lr: 0.000000  loss: 4.2137 (4.3464)  loss_scale: 65536.0000 (68116.1575)  weight_decay: 0.0500 (0.0500)  time: 0.3882  data: 0.0004  max mem: 15572
Epoch: [8]  [ 390/1031]  eta: 0:04:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4703 (4.3442)  loss_scale: 65536.0000 (68050.1688)  weight_decay: 0.0500 (0.0500)  time: 0.3916  data: 0.0004  max mem: 15572
Epoch: [8]  [ 400/1031]  eta: 0:04:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2201 (4.3401)  loss_scale: 65536.0000 (67987.4713)  weight_decay: 0.0500 (0.0500)  time: 0.3903  data: 0.0003  max mem: 15572
Epoch: [8]  [ 410/1031]  eta: 0:04:03  lr: 0.000046  min_lr: 0.000000  loss: 4.2404 (4.3423)  loss_scale: 65536.0000 (67927.8248)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0002  max mem: 15572
Epoch: [8]  [ 420/1031]  eta: 0:03:59  lr: 0.000046  min_lr: 0.000000  loss: 4.3447 (4.3445)  loss_scale: 65536.0000 (67871.0119)  weight_decay: 0.0500 (0.0500)  time: 0.3773  data: 0.0002  max mem: 15572
Epoch: [8]  [ 430/1031]  eta: 0:03:55  lr: 0.000046  min_lr: 0.000000  loss: 4.4422 (4.3480)  loss_scale: 65536.0000 (67816.8353)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0002  max mem: 15572
Epoch: [8]  [ 440/1031]  eta: 0:03:51  lr: 0.000046  min_lr: 0.000000  loss: 4.4422 (4.3512)  loss_scale: 65536.0000 (67765.1156)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
Epoch: [8]  [ 450/1031]  eta: 0:03:47  lr: 0.000046  min_lr: 0.000000  loss: 4.3417 (4.3460)  loss_scale: 65536.0000 (67715.6896)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0002  max mem: 15572
Epoch: [8]  [ 460/1031]  eta: 0:03:43  lr: 0.000046  min_lr: 0.000000  loss: 4.1064 (4.3454)  loss_scale: 65536.0000 (67668.4078)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0003  max mem: 15572
Epoch: [8]  [ 470/1031]  eta: 0:03:39  lr: 0.000046  min_lr: 0.000000  loss: 4.3193 (4.3456)  loss_scale: 65536.0000 (67623.1338)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [8]  [ 480/1031]  eta: 0:03:34  lr: 0.000046  min_lr: 0.000000  loss: 4.3003 (4.3408)  loss_scale: 65536.0000 (67579.7422)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [8]  [ 490/1031]  eta: 0:03:31  lr: 0.000046  min_lr: 0.000000  loss: 4.2180 (4.3396)  loss_scale: 65536.0000 (67538.1181)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0003  max mem: 15572
Epoch: [8]  [ 500/1031]  eta: 0:03:27  lr: 0.000046  min_lr: 0.000000  loss: 4.2180 (4.3387)  loss_scale: 65536.0000 (67498.1557)  weight_decay: 0.0500 (0.0500)  time: 0.3897  data: 0.0004  max mem: 15572
[2025-01-13 13:00:19,434] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:00:19,434] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 13:00:19,810] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18481
[2025-01-13 13:00:19,810] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:00:19,810] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 510/1031]  eta: 0:03:23  lr: 0.000046  min_lr: 0.000000  loss: 4.2092 (4.3386)  loss_scale: 65536.0000 (67588.0078)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0004  max mem: 15572
Epoch: [8]  [ 520/1031]  eta: 0:03:19  lr: 0.000046  min_lr: 0.000000  loss: 4.2092 (4.3353)  loss_scale: 65536.0000 (67548.6219)  weight_decay: 0.0500 (0.0500)  time: 0.3852  data: 0.0004  max mem: 15572
Epoch: [8]  [ 530/1031]  eta: 0:03:15  lr: 0.000046  min_lr: 0.000000  loss: 4.4029 (4.3399)  loss_scale: 65536.0000 (67510.7194)  weight_decay: 0.0500 (0.0500)  time: 0.3850  data: 0.0004  max mem: 15572
Epoch: [8]  [ 540/1031]  eta: 0:03:11  lr: 0.000046  min_lr: 0.000000  loss: 4.4052 (4.3390)  loss_scale: 65536.0000 (67474.2181)  weight_decay: 0.0500 (0.0500)  time: 0.3812  data: 0.0003  max mem: 15572
Epoch: [8]  [ 550/1031]  eta: 0:03:07  lr: 0.000046  min_lr: 0.000000  loss: 4.2470 (4.3372)  loss_scale: 65536.0000 (67439.0417)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0003  max mem: 15572
Epoch: [8]  [ 560/1031]  eta: 0:03:03  lr: 0.000046  min_lr: 0.000000  loss: 4.2250 (4.3333)  loss_scale: 65536.0000 (67405.1194)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [8]  [ 570/1031]  eta: 0:02:59  lr: 0.000046  min_lr: 0.000000  loss: 4.0746 (4.3282)  loss_scale: 65536.0000 (67372.3853)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0002  max mem: 15572
Epoch: [8]  [ 580/1031]  eta: 0:02:55  lr: 0.000046  min_lr: 0.000000  loss: 4.2564 (4.3300)  loss_scale: 65536.0000 (67340.7780)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0002  max mem: 15572
Epoch: [8]  [ 590/1031]  eta: 0:02:51  lr: 0.000046  min_lr: 0.000000  loss: 4.3615 (4.3291)  loss_scale: 65536.0000 (67310.2403)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [8]  [ 600/1031]  eta: 0:02:47  lr: 0.000046  min_lr: 0.000000  loss: 4.3039 (4.3298)  loss_scale: 65536.0000 (67280.7188)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0002  max mem: 15572
Epoch: [8]  [ 610/1031]  eta: 0:02:43  lr: 0.000046  min_lr: 0.000000  loss: 4.3374 (4.3316)  loss_scale: 65536.0000 (67252.1637)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0002  max mem: 15572
Epoch: [8]  [ 620/1031]  eta: 0:02:39  lr: 0.000046  min_lr: 0.000000  loss: 4.4069 (4.3340)  loss_scale: 65536.0000 (67224.5282)  weight_decay: 0.0500 (0.0500)  time: 0.3769  data: 0.0002  max mem: 15572
Epoch: [8]  [ 630/1031]  eta: 0:02:35  lr: 0.000046  min_lr: 0.000000  loss: 4.4069 (4.3353)  loss_scale: 65536.0000 (67197.7686)  weight_decay: 0.0500 (0.0500)  time: 0.3860  data: 0.0003  max mem: 15572
[2025-01-13 13:01:08,916] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:01:08,916] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 640/1031]  eta: 0:02:31  lr: 0.000046  min_lr: 0.000000  loss: 4.5403 (4.3372)  loss_scale: 65536.0000 (67580.8050)  weight_decay: 0.0500 (0.0500)  time: 0.3985  data: 0.0004  max mem: 15572
[2025-01-13 13:01:11,267] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18616
[2025-01-13 13:01:11,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:01:11,267] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 650/1031]  eta: 0:02:27  lr: 0.000046  min_lr: 0.000000  loss: 4.1412 (4.3332)  loss_scale: 65536.0000 (67750.7343)  weight_decay: 0.0500 (0.0500)  time: 0.3963  data: 0.0004  max mem: 15572
Epoch: [8]  [ 660/1031]  eta: 0:02:24  lr: 0.000046  min_lr: 0.000000  loss: 4.0506 (4.3330)  loss_scale: 65536.0000 (67717.2284)  weight_decay: 0.0500 (0.0500)  time: 0.3890  data: 0.0004  max mem: 15572
Epoch: [8]  [ 670/1031]  eta: 0:02:20  lr: 0.000046  min_lr: 0.000000  loss: 4.2517 (4.3331)  loss_scale: 65536.0000 (67684.7213)  weight_decay: 0.0500 (0.0500)  time: 0.3925  data: 0.0005  max mem: 15572
Epoch: [8]  [ 680/1031]  eta: 0:02:16  lr: 0.000046  min_lr: 0.000000  loss: 4.3031 (4.3312)  loss_scale: 65536.0000 (67653.1689)  weight_decay: 0.0500 (0.0500)  time: 0.3945  data: 0.0005  max mem: 15572
Epoch: [8]  [ 690/1031]  eta: 0:02:12  lr: 0.000046  min_lr: 0.000000  loss: 4.2946 (4.3308)  loss_scale: 65536.0000 (67622.5297)  weight_decay: 0.0500 (0.0500)  time: 0.3843  data: 0.0004  max mem: 15572
Epoch: [8]  [ 700/1031]  eta: 0:02:08  lr: 0.000046  min_lr: 0.000000  loss: 4.3472 (4.3314)  loss_scale: 65536.0000 (67592.7646)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0003  max mem: 15572
Epoch: [8]  [ 710/1031]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000000  loss: 4.3712 (4.3299)  loss_scale: 65536.0000 (67563.8368)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [8]  [ 720/1031]  eta: 0:02:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2516 (4.3284)  loss_scale: 65536.0000 (67535.7115)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [8]  [ 730/1031]  eta: 0:01:56  lr: 0.000046  min_lr: 0.000000  loss: 4.2700 (4.3298)  loss_scale: 65536.0000 (67508.3557)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [8]  [ 740/1031]  eta: 0:01:52  lr: 0.000046  min_lr: 0.000000  loss: 4.3017 (4.3292)  loss_scale: 65536.0000 (67481.7382)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [8]  [ 750/1031]  eta: 0:01:48  lr: 0.000046  min_lr: 0.000000  loss: 4.2602 (4.3281)  loss_scale: 65536.0000 (67455.8296)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [8]  [ 760/1031]  eta: 0:01:44  lr: 0.000046  min_lr: 0.000000  loss: 4.2602 (4.3266)  loss_scale: 65536.0000 (67430.6018)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [8]  [ 770/1031]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2092 (4.3257)  loss_scale: 65536.0000 (67406.0285)  weight_decay: 0.0500 (0.0500)  time: 0.3808  data: 0.0003  max mem: 15572
[2025-01-13 13:02:00,409] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:02:00,409] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 780/1031]  eta: 0:01:37  lr: 0.000046  min_lr: 0.000000  loss: 4.2771 (4.3245)  loss_scale: 65536.0000 (68137.3009)  weight_decay: 0.0500 (0.0500)  time: 0.3912  data: 0.0004  max mem: 15572
[2025-01-13 13:02:05,192] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18757
[2025-01-13 13:02:05,192] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:02:05,192] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 790/1031]  eta: 0:01:33  lr: 0.000046  min_lr: 0.000000  loss: 4.2771 (4.3245)  loss_scale: 131072.0000 (68352.9709)  weight_decay: 0.0500 (0.0500)  time: 0.4020  data: 0.0005  max mem: 15572
Epoch: [8]  [ 800/1031]  eta: 0:01:29  lr: 0.000046  min_lr: 0.000000  loss: 4.3574 (4.3257)  loss_scale: 65536.0000 (68317.8027)  weight_decay: 0.0500 (0.0500)  time: 0.4011  data: 0.0005  max mem: 15572
Epoch: [8]  [ 810/1031]  eta: 0:01:25  lr: 0.000046  min_lr: 0.000000  loss: 4.3515 (4.3262)  loss_scale: 65536.0000 (68283.5018)  weight_decay: 0.0500 (0.0500)  time: 0.3972  data: 0.0005  max mem: 15572
Epoch: [8]  [ 820/1031]  eta: 0:01:21  lr: 0.000046  min_lr: 0.000000  loss: 4.2938 (4.3254)  loss_scale: 65536.0000 (68250.0365)  weight_decay: 0.0500 (0.0500)  time: 0.3952  data: 0.0005  max mem: 15572
Epoch: [8]  [ 830/1031]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000000  loss: 4.2606 (4.3253)  loss_scale: 65536.0000 (68217.3767)  weight_decay: 0.0500 (0.0500)  time: 0.3936  data: 0.0006  max mem: 15572
Epoch: [8]  [ 840/1031]  eta: 0:01:14  lr: 0.000046  min_lr: 0.000000  loss: 4.2606 (4.3252)  loss_scale: 65536.0000 (68185.4935)  weight_decay: 0.0500 (0.0500)  time: 0.3855  data: 0.0004  max mem: 15572
Epoch: [8]  [ 850/1031]  eta: 0:01:10  lr: 0.000046  min_lr: 0.000000  loss: 4.3776 (4.3263)  loss_scale: 65536.0000 (68154.3596)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [8]  [ 860/1031]  eta: 0:01:06  lr: 0.000046  min_lr: 0.000000  loss: 4.4117 (4.3269)  loss_scale: 65536.0000 (68123.9489)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
Epoch: [8]  [ 870/1031]  eta: 0:01:02  lr: 0.000045  min_lr: 0.000000  loss: 4.3397 (4.3261)  loss_scale: 65536.0000 (68094.2365)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0002  max mem: 15572
Epoch: [8]  [ 880/1031]  eta: 0:00:58  lr: 0.000045  min_lr: 0.000000  loss: 4.2618 (4.3249)  loss_scale: 65536.0000 (68065.1986)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [8]  [ 890/1031]  eta: 0:00:54  lr: 0.000045  min_lr: 0.000000  loss: 4.3048 (4.3259)  loss_scale: 65536.0000 (68036.8126)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [8]  [ 900/1031]  eta: 0:00:50  lr: 0.000045  min_lr: 0.000000  loss: 4.4578 (4.3258)  loss_scale: 65536.0000 (68009.0566)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0002  max mem: 15572
Epoch: [8]  [ 910/1031]  eta: 0:00:46  lr: 0.000045  min_lr: 0.000000  loss: 4.4523 (4.3276)  loss_scale: 65536.0000 (67981.9100)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
[2025-01-13 13:02:54,631] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:02:54,631] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 13:02:57,335] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18893
[2025-01-13 13:02:57,335] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:02:57,335] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 920/1031]  eta: 0:00:42  lr: 0.000045  min_lr: 0.000000  loss: 4.4566 (4.3291)  loss_scale: 65536.0000 (68453.4549)  weight_decay: 0.0500 (0.0500)  time: 0.3787  data: 0.0002  max mem: 15572
Epoch: [8]  [ 930/1031]  eta: 0:00:39  lr: 0.000045  min_lr: 0.000000  loss: 4.5003 (4.3299)  loss_scale: 65536.0000 (68422.1182)  weight_decay: 0.0500 (0.0500)  time: 0.3849  data: 0.0004  max mem: 15572
Epoch: [8]  [ 940/1031]  eta: 0:00:35  lr: 0.000045  min_lr: 0.000000  loss: 4.3956 (4.3277)  loss_scale: 65536.0000 (68391.4474)  weight_decay: 0.0500 (0.0500)  time: 0.3860  data: 0.0004  max mem: 15572
Epoch: [8]  [ 950/1031]  eta: 0:00:31  lr: 0.000045  min_lr: 0.000000  loss: 4.2439 (4.3272)  loss_scale: 65536.0000 (68361.4217)  weight_decay: 0.0500 (0.0500)  time: 0.3849  data: 0.0004  max mem: 15572
Epoch: [8]  [ 960/1031]  eta: 0:00:27  lr: 0.000045  min_lr: 0.000000  loss: 4.3762 (4.3271)  loss_scale: 65536.0000 (68332.0208)  weight_decay: 0.0500 (0.0500)  time: 0.3885  data: 0.0004  max mem: 15572
Epoch: [8]  [ 970/1031]  eta: 0:00:23  lr: 0.000045  min_lr: 0.000000  loss: 4.1872 (4.3264)  loss_scale: 65536.0000 (68303.2255)  weight_decay: 0.0500 (0.0500)  time: 0.3845  data: 0.0003  max mem: 15572
Epoch: [8]  [ 980/1031]  eta: 0:00:19  lr: 0.000045  min_lr: 0.000000  loss: 4.1872 (4.3260)  loss_scale: 65536.0000 (68275.0173)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0002  max mem: 15572
Epoch: [8]  [ 990/1031]  eta: 0:00:15  lr: 0.000045  min_lr: 0.000000  loss: 4.2015 (4.3252)  loss_scale: 65536.0000 (68247.3784)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [8]  [1000/1031]  eta: 0:00:11  lr: 0.000045  min_lr: 0.000000  loss: 4.3571 (4.3266)  loss_scale: 65536.0000 (68220.2917)  weight_decay: 0.0500 (0.0500)  time: 0.3879  data: 0.0003  max mem: 15572
Epoch: [8]  [1010/1031]  eta: 0:00:08  lr: 0.000045  min_lr: 0.000000  loss: 4.4245 (4.3259)  loss_scale: 65536.0000 (68193.7409)  weight_decay: 0.0500 (0.0500)  time: 0.3912  data: 0.0002  max mem: 15572
Epoch: [8]  [1020/1031]  eta: 0:00:04  lr: 0.000045  min_lr: 0.000000  loss: 4.3291 (4.3259)  loss_scale: 65536.0000 (68167.7101)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0001  max mem: 15572
[2025-01-13 13:03:37,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=122, lr=[4.39736378740275e-07, 4.39736378740275e-07, 6.281948267718216e-07, 6.281948267718216e-07, 8.974211811026023e-07, 8.974211811026023e-07, 1.2820302587180034e-06, 1.2820302587180034e-06, 1.8314717981685762e-06, 1.8314717981685762e-06, 2.616388283097966e-06, 2.616388283097966e-06, 3.737697547282809e-06, 3.737697547282809e-06, 5.339567924689728e-06, 5.339567924689728e-06, 7.627954178128182e-06, 7.627954178128182e-06, 1.0897077397325976e-05, 1.0897077397325976e-05, 1.5567253424751394e-05, 1.5567253424751394e-05, 2.2238933463930565e-05, 2.2238933463930565e-05, 3.176990494847224e-05, 3.176990494847224e-05, 4.5385578497817487e-05, 4.5385578497817487e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 13:03:37,797] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=29.670377968948394, CurrSamplesPerSec=34.90168351942795, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [1030/1031]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 4.3130 (4.3251)  loss_scale: 65536.0000 (68142.1843)  weight_decay: 0.0500 (0.0500)  time: 0.3657  data: 0.0001  max mem: 15572
Epoch: [8] Total time: 0:06:38 (0.3862 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 4.3130 (4.3251)  loss_scale: 65536.0000 (68142.1843)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 2892
Indices to remove: tensor([    0,     2,     6,  ..., 33660, 33670, 33705], device='cuda:0')
length of data loader train is: 790
num_training_steps_per_epoch is: 790
Change step level LR scheduler!
Set warmup steps = 3950
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:09:37  loss: 0.4739 (0.4739)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1234  data: 1.9713  max mem: 15572
Val:  [ 10/272]  eta: 0:01:41  loss: 4.0386 (3.8292)  acc1: 0.0000 (19.1919)  acc5: 27.7778 (27.7778)  time: 0.3855  data: 0.2351  max mem: 15572
Val:  [ 20/272]  eta: 0:01:18  loss: 3.9402 (3.8116)  acc1: 0.0000 (15.0794)  acc5: 33.3333 (32.2751)  time: 0.2193  data: 0.0649  max mem: 15572
Val:  [ 30/272]  eta: 0:01:06  loss: 3.8404 (3.8346)  acc1: 0.0000 (12.9032)  acc5: 38.8889 (34.0502)  time: 0.2113  data: 0.0431  max mem: 15572
Val:  [ 40/272]  eta: 0:00:58  loss: 3.7183 (3.7651)  acc1: 5.5556 (13.8211)  acc5: 50.0000 (37.9404)  time: 0.1901  data: 0.0138  max mem: 15572
Val:  [ 50/272]  eta: 0:00:52  loss: 3.5081 (3.6822)  acc1: 11.1111 (15.2505)  acc5: 50.0000 (41.3943)  time: 0.1809  data: 0.0051  max mem: 15572
Val:  [ 60/272]  eta: 0:00:48  loss: 2.5007 (3.4832)  acc1: 38.8889 (22.0401)  acc5: 72.2222 (45.9016)  time: 0.1768  data: 0.0006  max mem: 15572
Val:  [ 70/272]  eta: 0:00:44  loss: 2.5007 (3.3721)  acc1: 38.8889 (23.2394)  acc5: 77.7778 (50.7042)  time: 0.1745  data: 0.0006  max mem: 15572
Val:  [ 80/272]  eta: 0:00:41  loss: 3.1025 (3.3939)  acc1: 11.1111 (23.4568)  acc5: 77.7778 (49.3827)  time: 0.1749  data: 0.0007  max mem: 15572
Val:  [ 90/272]  eta: 0:00:38  loss: 4.5025 (3.5162)  acc1: 0.0000 (20.9402)  acc5: 11.1111 (44.8107)  time: 0.1780  data: 0.0007  max mem: 15572
Val:  [100/272]  eta: 0:00:35  loss: 4.3873 (3.5764)  acc1: 0.0000 (20.5721)  acc5: 11.1111 (43.8944)  time: 0.1760  data: 0.0008  max mem: 15572
Val:  [110/272]  eta: 0:00:33  loss: 3.9745 (3.6335)  acc1: 5.5556 (19.2192)  acc5: 38.8889 (42.5926)  time: 0.1770  data: 0.0008  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 3.9976 (3.6648)  acc1: 0.0000 (18.2736)  acc5: 38.8889 (42.3783)  time: 0.1805  data: 0.0008  max mem: 15572
Val:  [130/272]  eta: 0:00:28  loss: 3.6800 (3.6133)  acc1: 11.1111 (19.7201)  acc5: 38.8889 (43.8083)  time: 0.1850  data: 0.0006  max mem: 15572
Val:  [140/272]  eta: 0:00:26  loss: 3.4690 (3.6009)  acc1: 22.2222 (20.4886)  acc5: 38.8889 (43.7352)  time: 0.1834  data: 0.0006  max mem: 15572
Val:  [150/272]  eta: 0:00:24  loss: 3.8418 (3.6145)  acc1: 5.5556 (19.6100)  acc5: 33.3333 (43.0831)  time: 0.1816  data: 0.0008  max mem: 15572
Val:  [160/272]  eta: 0:00:22  loss: 3.6718 (3.6024)  acc1: 11.1111 (20.5659)  acc5: 50.0000 (44.3754)  time: 0.1877  data: 0.0008  max mem: 15572
Val:  [170/272]  eta: 0:00:20  loss: 3.6877 (3.6322)  acc1: 11.1111 (19.6881)  acc5: 50.0000 (43.6972)  time: 0.1818  data: 0.0007  max mem: 15572
Val:  [180/272]  eta: 0:00:18  loss: 3.8508 (3.6285)  acc1: 5.5556 (19.3370)  acc5: 44.4444 (44.2603)  time: 0.1771  data: 0.0007  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 4.0280 (3.6591)  acc1: 5.5556 (18.7900)  acc5: 38.8889 (43.3973)  time: 0.1718  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 4.0079 (3.6573)  acc1: 5.5556 (18.6567)  acc5: 38.8889 (43.9193)  time: 0.1691  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.2635 (3.6590)  acc1: 16.6667 (18.7730)  acc5: 66.6667 (44.4708)  time: 0.1679  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.4182 (3.6588)  acc1: 16.6667 (19.1302)  acc5: 55.5556 (44.4193)  time: 0.1617  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.2650 (3.6282)  acc1: 27.7778 (20.6349)  acc5: 61.1111 (45.6229)  time: 0.1590  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:06  loss: 2.8013 (3.5973)  acc1: 38.8889 (21.3001)  acc5: 77.7778 (47.0263)  time: 0.1643  data: 0.0025  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.1276 (3.6172)  acc1: 16.6667 (21.0049)  acc5: 61.1111 (46.5471)  time: 0.1672  data: 0.0025  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.6715 (3.5324)  acc1: 38.8889 (23.5632)  acc5: 77.7778 (48.3397)  time: 0.1558  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.6715 (3.5290)  acc1: 50.0000 (23.5342)  acc5: 77.7778 (48.5240)  time: 0.1421  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.9269 (3.5304)  acc1: 50.0000 (23.5511)  acc5: 72.2222 (48.5357)  time: 0.1363  data: 0.0001  max mem: 15572
Val: Total time: 0:00:50 (0.1840 s / it)
* Acc@1 23.551 Acc@5 48.536 loss 3.530
Accuracy of the network on the 4883 val videos: 23.6%
[2025-01-13 13:04:29,691] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 13:04:29,693] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 13:04:29,693] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 13:04:32,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 13:04:32,076] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 23.55%
Epoch: [9]  [  0/790]  eta: 0:50:02  lr: 0.000045  min_lr: 0.000000  loss: 4.3604 (4.3604)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.8010  data: 3.4019  max mem: 15572
Epoch: [9]  [ 10/790]  eta: 0:09:07  lr: 0.000045  min_lr: 0.000000  loss: 4.3328 (4.4013)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7018  data: 0.3096  max mem: 15572
[2025-01-13 13:04:42,806] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:04:42,806] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 20/790]  eta: 0:07:02  lr: 0.000045  min_lr: 0.000000  loss: 4.3328 (4.4379)  loss_scale: 65536.0000 (74898.2857)  weight_decay: 0.0500 (0.0500)  time: 0.3857  data: 0.0003  max mem: 15572
[2025-01-13 13:04:44,335] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19026
[2025-01-13 13:04:44,335] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:04:44,336] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 30/790]  eta: 0:06:12  lr: 0.000045  min_lr: 0.000000  loss: 4.3041 (4.3522)  loss_scale: 65536.0000 (73992.2581)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
Epoch: [9]  [ 40/790]  eta: 0:05:46  lr: 0.000045  min_lr: 0.000000  loss: 4.1734 (4.3366)  loss_scale: 65536.0000 (71929.7561)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [9]  [ 50/790]  eta: 0:05:29  lr: 0.000045  min_lr: 0.000000  loss: 4.1970 (4.3189)  loss_scale: 65536.0000 (70676.0784)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0002  max mem: 15572
Epoch: [9]  [ 60/790]  eta: 0:05:16  lr: 0.000045  min_lr: 0.000000  loss: 4.2804 (4.3310)  loss_scale: 65536.0000 (69833.4426)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [9]  [ 70/790]  eta: 0:05:07  lr: 0.000045  min_lr: 0.000000  loss: 4.2549 (4.3234)  loss_scale: 65536.0000 (69228.1690)  weight_decay: 0.0500 (0.0500)  time: 0.3834  data: 0.0003  max mem: 15572
Epoch: [9]  [ 80/790]  eta: 0:04:59  lr: 0.000045  min_lr: 0.000000  loss: 4.3561 (4.3398)  loss_scale: 65536.0000 (68772.3457)  weight_decay: 0.0500 (0.0500)  time: 0.3870  data: 0.0004  max mem: 15572
Epoch: [9]  [ 90/790]  eta: 0:04:52  lr: 0.000045  min_lr: 0.000000  loss: 4.2046 (4.3185)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)  time: 0.3846  data: 0.0004  max mem: 15572
Epoch: [9]  [100/790]  eta: 0:04:46  lr: 0.000045  min_lr: 0.000000  loss: 4.3278 (4.3401)  loss_scale: 65536.0000 (68131.4851)  weight_decay: 0.0500 (0.0500)  time: 0.3896  data: 0.0004  max mem: 15572
Epoch: [9]  [110/790]  eta: 0:04:40  lr: 0.000045  min_lr: 0.000000  loss: 4.5413 (4.3591)  loss_scale: 65536.0000 (67897.6577)  weight_decay: 0.0500 (0.0500)  time: 0.3910  data: 0.0004  max mem: 15572
Epoch: [9]  [120/790]  eta: 0:04:35  lr: 0.000045  min_lr: 0.000000  loss: 4.4607 (4.3533)  loss_scale: 65536.0000 (67702.4793)  weight_decay: 0.0500 (0.0500)  time: 0.3884  data: 0.0004  max mem: 15572
Epoch: [9]  [130/790]  eta: 0:04:29  lr: 0.000045  min_lr: 0.000000  loss: 4.2845 (4.3463)  loss_scale: 65536.0000 (67537.0992)  weight_decay: 0.0500 (0.0500)  time: 0.3810  data: 0.0003  max mem: 15572
Epoch: [9]  [140/790]  eta: 0:04:24  lr: 0.000045  min_lr: 0.000000  loss: 4.2730 (4.3467)  loss_scale: 65536.0000 (67395.1773)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [9]  [150/790]  eta: 0:04:18  lr: 0.000045  min_lr: 0.000000  loss: 4.2730 (4.3438)  loss_scale: 65536.0000 (67272.0530)  weight_decay: 0.0500 (0.0500)  time: 0.3806  data: 0.0003  max mem: 15572
[2025-01-13 13:05:33,562] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:05:33,562] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 13:05:35,434] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19160
[2025-01-13 13:05:35,435] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:05:35,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [160/790]  eta: 0:04:13  lr: 0.000045  min_lr: 0.000000  loss: 4.2441 (4.3377)  loss_scale: 65536.0000 (69199.5031)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0002  max mem: 15572
Epoch: [9]  [170/790]  eta: 0:04:08  lr: 0.000045  min_lr: 0.000000  loss: 4.2441 (4.3322)  loss_scale: 65536.0000 (68985.2632)  weight_decay: 0.0500 (0.0500)  time: 0.3731  data: 0.0002  max mem: 15572
Epoch: [9]  [180/790]  eta: 0:04:03  lr: 0.000045  min_lr: 0.000000  loss: 4.2624 (4.3293)  loss_scale: 65536.0000 (68794.6961)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0002  max mem: 15572
Epoch: [9]  [190/790]  eta: 0:03:58  lr: 0.000045  min_lr: 0.000000  loss: 4.3265 (4.3257)  loss_scale: 65536.0000 (68624.0838)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0003  max mem: 15572
Epoch: [9]  [200/790]  eta: 0:03:54  lr: 0.000045  min_lr: 0.000000  loss: 4.1439 (4.3214)  loss_scale: 65536.0000 (68470.4478)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [9]  [210/790]  eta: 0:03:50  lr: 0.000045  min_lr: 0.000000  loss: 4.2955 (4.3270)  loss_scale: 65536.0000 (68331.3744)  weight_decay: 0.0500 (0.0500)  time: 0.3843  data: 0.0003  max mem: 15572
Epoch: [9]  [220/790]  eta: 0:03:46  lr: 0.000045  min_lr: 0.000000  loss: 4.4428 (4.3325)  loss_scale: 65536.0000 (68204.8869)  weight_decay: 0.0500 (0.0500)  time: 0.3949  data: 0.0004  max mem: 15572
Epoch: [9]  [230/790]  eta: 0:03:42  lr: 0.000045  min_lr: 0.000000  loss: 4.3193 (4.3272)  loss_scale: 65536.0000 (68089.3506)  weight_decay: 0.0500 (0.0500)  time: 0.3951  data: 0.0005  max mem: 15572
Epoch: [9]  [240/790]  eta: 0:03:38  lr: 0.000045  min_lr: 0.000000  loss: 4.2467 (4.3252)  loss_scale: 65536.0000 (67983.4025)  weight_decay: 0.0500 (0.0500)  time: 0.3947  data: 0.0005  max mem: 15572
Epoch: [9]  [250/790]  eta: 0:03:34  lr: 0.000045  min_lr: 0.000000  loss: 4.2113 (4.3241)  loss_scale: 65536.0000 (67885.8964)  weight_decay: 0.0500 (0.0500)  time: 0.3980  data: 0.0005  max mem: 15572
Epoch: [9]  [260/790]  eta: 0:03:30  lr: 0.000045  min_lr: 0.000000  loss: 4.2600 (4.3232)  loss_scale: 65536.0000 (67795.8621)  weight_decay: 0.0500 (0.0500)  time: 0.4014  data: 0.0005  max mem: 15572
Epoch: [9]  [270/790]  eta: 0:03:26  lr: 0.000045  min_lr: 0.000000  loss: 4.3002 (4.3178)  loss_scale: 65536.0000 (67712.4723)  weight_decay: 0.0500 (0.0500)  time: 0.3944  data: 0.0004  max mem: 15572
Epoch: [9]  [280/790]  eta: 0:03:21  lr: 0.000045  min_lr: 0.000000  loss: 4.1984 (4.3134)  loss_scale: 65536.0000 (67635.0178)  weight_decay: 0.0500 (0.0500)  time: 0.3815  data: 0.0003  max mem: 15572
[2025-01-13 13:06:25,259] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:06:25,260] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [290/790]  eta: 0:03:17  lr: 0.000045  min_lr: 0.000000  loss: 4.2327 (4.3145)  loss_scale: 65536.0000 (68914.1443)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
[2025-01-13 13:06:27,487] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19295
[2025-01-13 13:06:27,487] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:06:27,487] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [300/790]  eta: 0:03:13  lr: 0.000045  min_lr: 0.000000  loss: 4.3641 (4.3172)  loss_scale: 65536.0000 (68801.9136)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [9]  [310/790]  eta: 0:03:09  lr: 0.000045  min_lr: 0.000000  loss: 4.4106 (4.3180)  loss_scale: 65536.0000 (68696.9003)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0002  max mem: 15572
Epoch: [9]  [320/790]  eta: 0:03:04  lr: 0.000045  min_lr: 0.000000  loss: 4.3374 (4.3128)  loss_scale: 65536.0000 (68598.4299)  weight_decay: 0.0500 (0.0500)  time: 0.3761  data: 0.0002  max mem: 15572
Epoch: [9]  [330/790]  eta: 0:03:00  lr: 0.000045  min_lr: 0.000000  loss: 4.1722 (4.3087)  loss_scale: 65536.0000 (68505.9094)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0002  max mem: 15572
Epoch: [9]  [340/790]  eta: 0:02:56  lr: 0.000045  min_lr: 0.000000  loss: 4.3050 (4.3137)  loss_scale: 65536.0000 (68418.8152)  weight_decay: 0.0500 (0.0500)  time: 0.3803  data: 0.0003  max mem: 15572
Epoch: [9]  [350/790]  eta: 0:02:52  lr: 0.000045  min_lr: 0.000000  loss: 4.3098 (4.3138)  loss_scale: 65536.0000 (68336.6838)  weight_decay: 0.0500 (0.0500)  time: 0.3792  data: 0.0003  max mem: 15572
Epoch: [9]  [360/790]  eta: 0:02:48  lr: 0.000045  min_lr: 0.000000  loss: 4.2882 (4.3120)  loss_scale: 65536.0000 (68259.1025)  weight_decay: 0.0500 (0.0500)  time: 0.3835  data: 0.0004  max mem: 15572
[2025-01-13 13:06:54,051] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19365
[2025-01-13 13:06:54,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 13:06:54,051] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [370/790]  eta: 0:02:44  lr: 0.000045  min_lr: 0.000000  loss: 4.2766 (4.3077)  loss_scale: 32768.0000 (67302.4690)  weight_decay: 0.0500 (0.0500)  time: 0.3987  data: 0.0005  max mem: 15572
Epoch: [9]  [380/790]  eta: 0:02:40  lr: 0.000045  min_lr: 0.000000  loss: 4.2039 (4.3025)  loss_scale: 32768.0000 (66396.0525)  weight_decay: 0.0500 (0.0500)  time: 0.4015  data: 0.0005  max mem: 15572
Epoch: [9]  [390/790]  eta: 0:02:37  lr: 0.000045  min_lr: 0.000000  loss: 4.1014 (4.2977)  loss_scale: 32768.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3998  data: 0.0004  max mem: 15572
Epoch: [9]  [400/790]  eta: 0:02:33  lr: 0.000045  min_lr: 0.000000  loss: 4.1682 (4.2987)  loss_scale: 32768.0000 (64718.8429)  weight_decay: 0.0500 (0.0500)  time: 0.3939  data: 0.0004  max mem: 15572
Epoch: [9]  [410/790]  eta: 0:02:29  lr: 0.000045  min_lr: 0.000000  loss: 4.2976 (4.3033)  loss_scale: 32768.0000 (63941.4501)  weight_decay: 0.0500 (0.0500)  time: 0.3894  data: 0.0004  max mem: 15572
Epoch: [9]  [420/790]  eta: 0:02:25  lr: 0.000045  min_lr: 0.000000  loss: 4.3582 (4.3043)  loss_scale: 32768.0000 (63200.9881)  weight_decay: 0.0500 (0.0500)  time: 0.3876  data: 0.0004  max mem: 15572
Epoch: [9]  [430/790]  eta: 0:02:21  lr: 0.000045  min_lr: 0.000000  loss: 4.3283 (4.3050)  loss_scale: 32768.0000 (62494.8863)  weight_decay: 0.0500 (0.0500)  time: 0.3823  data: 0.0003  max mem: 15572
Epoch: [9]  [440/790]  eta: 0:02:17  lr: 0.000045  min_lr: 0.000000  loss: 4.3738 (4.3085)  loss_scale: 32768.0000 (61820.8073)  weight_decay: 0.0500 (0.0500)  time: 0.3787  data: 0.0002  max mem: 15572
Epoch: [9]  [450/790]  eta: 0:02:13  lr: 0.000045  min_lr: 0.000000  loss: 4.4180 (4.3108)  loss_scale: 32768.0000 (61176.6208)  weight_decay: 0.0500 (0.0500)  time: 0.3750  data: 0.0002  max mem: 15572
Epoch: [9]  [460/790]  eta: 0:02:09  lr: 0.000045  min_lr: 0.000000  loss: 4.4763 (4.3116)  loss_scale: 32768.0000 (60560.3818)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [9]  [470/790]  eta: 0:02:05  lr: 0.000045  min_lr: 0.000000  loss: 4.3882 (4.3131)  loss_scale: 32768.0000 (59970.3100)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0002  max mem: 15572
Epoch: [9]  [480/790]  eta: 0:02:01  lr: 0.000045  min_lr: 0.000000  loss: 4.3718 (4.3146)  loss_scale: 32768.0000 (59404.7734)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
[2025-01-13 13:07:43,645] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:07:43,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [490/790]  eta: 0:01:57  lr: 0.000045  min_lr: 0.000000  loss: 4.3408 (4.3148)  loss_scale: 32768.0000 (58929.0102)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [9]  [500/790]  eta: 0:01:53  lr: 0.000045  min_lr: 0.000000  loss: 4.2394 (4.3136)  loss_scale: 65536.0000 (59060.8862)  weight_decay: 0.0500 (0.0500)  time: 0.3794  data: 0.0003  max mem: 15572
Epoch: [9]  [510/790]  eta: 0:01:49  lr: 0.000045  min_lr: 0.000000  loss: 4.2519 (4.3141)  loss_scale: 65536.0000 (59187.6008)  weight_decay: 0.0500 (0.0500)  time: 0.3919  data: 0.0003  max mem: 15572
Epoch: [9]  [520/790]  eta: 0:01:45  lr: 0.000045  min_lr: 0.000000  loss: 4.2467 (4.3138)  loss_scale: 65536.0000 (59309.4511)  weight_decay: 0.0500 (0.0500)  time: 0.3951  data: 0.0004  max mem: 15572
Epoch: [9]  [530/790]  eta: 0:01:41  lr: 0.000045  min_lr: 0.000000  loss: 4.2807 (4.3167)  loss_scale: 65536.0000 (59426.7119)  weight_decay: 0.0500 (0.0500)  time: 0.3909  data: 0.0004  max mem: 15572
Epoch: [9]  [540/790]  eta: 0:01:37  lr: 0.000045  min_lr: 0.000000  loss: 4.3814 (4.3147)  loss_scale: 65536.0000 (59539.6377)  weight_decay: 0.0500 (0.0500)  time: 0.3929  data: 0.0004  max mem: 15572
Epoch: [9]  [550/790]  eta: 0:01:33  lr: 0.000045  min_lr: 0.000000  loss: 4.3050 (4.3159)  loss_scale: 65536.0000 (59648.4646)  weight_decay: 0.0500 (0.0500)  time: 0.3945  data: 0.0004  max mem: 15572
Epoch: [9]  [560/790]  eta: 0:01:29  lr: 0.000045  min_lr: 0.000000  loss: 4.2772 (4.3124)  loss_scale: 65536.0000 (59753.4118)  weight_decay: 0.0500 (0.0500)  time: 0.3980  data: 0.0004  max mem: 15572
Epoch: [9]  [570/790]  eta: 0:01:25  lr: 0.000045  min_lr: 0.000000  loss: 4.2793 (4.3136)  loss_scale: 65536.0000 (59854.6830)  weight_decay: 0.0500 (0.0500)  time: 0.3869  data: 0.0003  max mem: 15572
Epoch: [9]  [580/790]  eta: 0:01:21  lr: 0.000045  min_lr: 0.000000  loss: 4.2718 (4.3106)  loss_scale: 65536.0000 (59952.4682)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.0002  max mem: 15572
Epoch: [9]  [590/790]  eta: 0:01:17  lr: 0.000045  min_lr: 0.000000  loss: 4.1594 (4.3101)  loss_scale: 65536.0000 (60046.9442)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [9]  [600/790]  eta: 0:01:13  lr: 0.000045  min_lr: 0.000000  loss: 4.3016 (4.3125)  loss_scale: 65536.0000 (60138.2762)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [9]  [610/790]  eta: 0:01:10  lr: 0.000045  min_lr: 0.000000  loss: 4.3016 (4.3125)  loss_scale: 65536.0000 (60226.6187)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0002  max mem: 15572
[2025-01-13 13:08:33,040] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:08:33,040] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 13:08:33,410] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19623
[2025-01-13 13:08:33,410] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:08:33,410] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [620/790]  eta: 0:01:06  lr: 0.000045  min_lr: 0.000000  loss: 4.3103 (4.3143)  loss_scale: 65536.0000 (60417.6490)  weight_decay: 0.0500 (0.0500)  time: 0.3800  data: 0.0002  max mem: 15572
Epoch: [9]  [630/790]  eta: 0:01:02  lr: 0.000045  min_lr: 0.000000  loss: 4.4613 (4.3141)  loss_scale: 65536.0000 (60498.7639)  weight_decay: 0.0500 (0.0500)  time: 0.3798  data: 0.0002  max mem: 15572
Epoch: [9]  [640/790]  eta: 0:00:58  lr: 0.000045  min_lr: 0.000000  loss: 4.2205 (4.3104)  loss_scale: 65536.0000 (60577.3479)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0002  max mem: 15572
Epoch: [9]  [650/790]  eta: 0:00:54  lr: 0.000045  min_lr: 0.000000  loss: 4.1044 (4.3074)  loss_scale: 65536.0000 (60653.5177)  weight_decay: 0.0500 (0.0500)  time: 0.3803  data: 0.0003  max mem: 15572
Epoch: [9]  [660/790]  eta: 0:00:50  lr: 0.000045  min_lr: 0.000000  loss: 4.2048 (4.3074)  loss_scale: 65536.0000 (60727.3828)  weight_decay: 0.0500 (0.0500)  time: 0.3943  data: 0.0003  max mem: 15572
Epoch: [9]  [670/790]  eta: 0:00:46  lr: 0.000045  min_lr: 0.000000  loss: 4.3423 (4.3092)  loss_scale: 65536.0000 (60799.0462)  weight_decay: 0.0500 (0.0500)  time: 0.4014  data: 0.0004  max mem: 15572
Epoch: [9]  [680/790]  eta: 0:00:42  lr: 0.000045  min_lr: 0.000000  loss: 4.3423 (4.3091)  loss_scale: 65536.0000 (60868.6050)  weight_decay: 0.0500 (0.0500)  time: 0.3950  data: 0.0004  max mem: 15572
Epoch: [9]  [690/790]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000000  loss: 4.3013 (4.3066)  loss_scale: 65536.0000 (60936.1505)  weight_decay: 0.0500 (0.0500)  time: 0.3953  data: 0.0004  max mem: 15572
Epoch: [9]  [700/790]  eta: 0:00:35  lr: 0.000045  min_lr: 0.000000  loss: 4.2754 (4.3067)  loss_scale: 65536.0000 (61001.7689)  weight_decay: 0.0500 (0.0500)  time: 0.3980  data: 0.0004  max mem: 15572
Epoch: [9]  [710/790]  eta: 0:00:31  lr: 0.000045  min_lr: 0.000000  loss: 4.2215 (4.3037)  loss_scale: 65536.0000 (61065.5415)  weight_decay: 0.0500 (0.0500)  time: 0.3888  data: 0.0004  max mem: 15572
Epoch: [9]  [720/790]  eta: 0:00:27  lr: 0.000045  min_lr: 0.000000  loss: 4.2645 (4.3064)  loss_scale: 65536.0000 (61127.5451)  weight_decay: 0.0500 (0.0500)  time: 0.3767  data: 0.0002  max mem: 15572
Epoch: [9]  [730/790]  eta: 0:00:23  lr: 0.000045  min_lr: 0.000000  loss: 4.3256 (4.3062)  loss_scale: 65536.0000 (61187.8523)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [9]  [740/790]  eta: 0:00:19  lr: 0.000045  min_lr: 0.000000  loss: 4.2813 (4.3048)  loss_scale: 65536.0000 (61246.5317)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0002  max mem: 15572
[2025-01-13 13:09:23,133] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:09:23,133] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [750/790]  eta: 0:00:15  lr: 0.000045  min_lr: 0.000000  loss: 4.2935 (4.3058)  loss_scale: 65536.0000 (61565.4434)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0002  max mem: 15572
Epoch: [9]  [760/790]  eta: 0:00:11  lr: 0.000045  min_lr: 0.000000  loss: 4.3846 (4.3067)  loss_scale: 131072.0000 (62478.8016)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0002  max mem: 15572
[2025-01-13 13:09:28,802] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19767
[2025-01-13 13:09:28,802] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:09:28,802] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [770/790]  eta: 0:00:07  lr: 0.000045  min_lr: 0.000000  loss: 4.3846 (4.3055)  loss_scale: 131072.0000 (62688.4565)  weight_decay: 0.0500 (0.0500)  time: 0.3732  data: 0.0002  max mem: 15572
Epoch: [9]  [780/790]  eta: 0:00:03  lr: 0.000045  min_lr: 0.000000  loss: 4.3173 (4.3063)  loss_scale: 65536.0000 (62724.9168)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0002  max mem: 15572
Epoch: [9]  [789/790]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 4.3509 (4.3046)  loss_scale: 65536.0000 (62756.9418)  weight_decay: 0.0500 (0.0500)  time: 0.3653  data: 0.0001  max mem: 15572
Epoch: [9] Total time: 0:05:06 (0.3878 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 4.3509 (4.3046)  loss_scale: 65536.0000 (62756.9418)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 2316
Indices to remove: tensor([    1,    13,    14,  ..., 33604, 33660, 33682], device='cuda:0')
length of data loader train is: 597
num_training_steps_per_epoch is: 597
Change step level LR scheduler!
Set warmup steps = 2985
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
[2025-01-13 13:09:38,714] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-9 is about to be saved!
[2025-01-13 13:09:38,718] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-9/mp_rank_00_model_states.pt
[2025-01-13 13:09:38,718] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-9/mp_rank_00_model_states.pt...
[2025-01-13 13:09:39,148] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-9/mp_rank_00_model_states.pt.
[2025-01-13 13:09:39,149] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
Val:  [  0/272]  eta: 0:14:15  loss: 0.5255 (0.5255)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.1439  data: 2.9711  max mem: 15572
Val:  [ 10/272]  eta: 0:02:07  loss: 4.1157 (3.7207)  acc1: 0.0000 (18.6869)  acc5: 22.2222 (30.3030)  time: 0.4853  data: 0.3205  max mem: 15572
Val:  [ 20/272]  eta: 0:01:23  loss: 3.7925 (3.6584)  acc1: 0.0000 (15.6085)  acc5: 27.7778 (37.5661)  time: 0.1892  data: 0.0280  max mem: 15572
Val:  [ 30/272]  eta: 0:01:07  loss: 3.7925 (3.7451)  acc1: 5.5556 (12.1864)  acc5: 38.8889 (38.7097)  time: 0.1670  data: 0.0006  max mem: 15572
Val:  [ 40/272]  eta: 0:00:58  loss: 3.7378 (3.6895)  acc1: 5.5556 (13.1436)  acc5: 38.8889 (41.3279)  time: 0.1727  data: 0.0006  max mem: 15572
Val:  [ 50/272]  eta: 0:00:52  loss: 3.4656 (3.5927)  acc1: 16.6667 (16.2309)  acc5: 50.0000 (44.5534)  time: 0.1708  data: 0.0006  max mem: 15572
Val:  [ 60/272]  eta: 0:00:47  loss: 2.2703 (3.4074)  acc1: 50.0000 (22.9508)  acc5: 72.2222 (48.2696)  time: 0.1690  data: 0.0006  max mem: 15572
Val:  [ 70/272]  eta: 0:00:44  loss: 2.3437 (3.3015)  acc1: 50.0000 (24.4131)  acc5: 77.7778 (51.7214)  time: 0.1712  data: 0.0006  max mem: 15572
Val:  [ 80/272]  eta: 0:00:40  loss: 2.9151 (3.3221)  acc1: 16.6667 (24.1427)  acc5: 72.2222 (50.7545)  time: 0.1683  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:37  loss: 4.3620 (3.4468)  acc1: 0.0000 (21.5507)  acc5: 16.6667 (46.2149)  time: 0.1724  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:35  loss: 4.1858 (3.4944)  acc1: 0.0000 (21.2871)  acc5: 16.6667 (46.2046)  time: 0.1753  data: 0.0006  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 3.8967 (3.5601)  acc1: 0.0000 (19.4695)  acc5: 38.8889 (44.6446)  time: 0.1659  data: 0.0007  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 4.0423 (3.6002)  acc1: 0.0000 (18.5032)  acc5: 27.7778 (43.8017)  time: 0.1674  data: 0.0006  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.9363 (3.5409)  acc1: 5.5556 (20.4411)  acc5: 38.8889 (45.2502)  time: 0.1761  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 3.4215 (3.5366)  acc1: 16.6667 (21.1190)  acc5: 50.0000 (44.9961)  time: 0.1763  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.5413 (3.5391)  acc1: 5.5556 (20.2723)  acc5: 50.0000 (45.3642)  time: 0.1697  data: 0.0005  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.4404 (3.5177)  acc1: 16.6667 (21.4286)  acc5: 61.1111 (46.8599)  time: 0.1735  data: 0.0006  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.6208 (3.5479)  acc1: 16.6667 (20.4678)  acc5: 55.5556 (46.1014)  time: 0.1727  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.6954 (3.5463)  acc1: 0.0000 (20.8410)  acc5: 44.4444 (46.8692)  time: 0.1770  data: 0.0005  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.8267 (3.5728)  acc1: 5.5556 (19.9825)  acc5: 50.0000 (45.9570)  time: 0.1775  data: 0.0007  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.6153 (3.5689)  acc1: 0.0000 (19.9281)  acc5: 50.0000 (46.6003)  time: 0.1639  data: 0.0006  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.2484 (3.5740)  acc1: 11.1111 (19.9842)  acc5: 66.6667 (47.0774)  time: 0.1664  data: 0.0005  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.6484 (3.5687)  acc1: 11.1111 (20.1860)  acc5: 55.5556 (47.3856)  time: 0.1679  data: 0.0005  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 2.9914 (3.5283)  acc1: 44.4444 (22.0298)  acc5: 72.2222 (48.7013)  time: 0.1654  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.6120 (3.5038)  acc1: 44.4444 (22.5450)  acc5: 83.3333 (49.7695)  time: 0.1763  data: 0.0006  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.3470 (3.5240)  acc1: 16.6667 (22.0673)  acc5: 61.1111 (49.3139)  time: 0.1786  data: 0.0007  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.6259 (3.4426)  acc1: 38.8889 (24.6062)  acc5: 77.7778 (50.9791)  time: 0.1630  data: 0.0006  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.6259 (3.4473)  acc1: 44.4444 (24.3337)  acc5: 77.7778 (50.9840)  time: 0.1438  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.7026 (3.4510)  acc1: 44.4444 (24.3088)  acc5: 72.2222 (50.9523)  time: 0.1372  data: 0.0002  max mem: 15572
Val: Total time: 0:00:49 (0.1821 s / it)
* Acc@1 24.309 Acc@5 50.952 loss 3.451
Accuracy of the network on the 4883 val videos: 24.3%
[2025-01-13 13:10:28,670] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 13:10:28,672] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 13:10:28,672] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 13:10:31,145] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 13:10:31,145] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 24.31%
Epoch: [10]  [  0/597]  eta: 0:43:57  lr: 0.000045  min_lr: 0.000000  loss: 3.4767 (3.4767)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.4172  data: 3.9982  max mem: 15572
Epoch: [10]  [ 10/597]  eta: 0:07:20  lr: 0.000045  min_lr: 0.000000  loss: 4.2786 (4.1724)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7501  data: 0.3638  max mem: 15572
Epoch: [10]  [ 20/597]  eta: 0:05:30  lr: 0.000045  min_lr: 0.000000  loss: 4.1390 (4.1408)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3813  data: 0.0003  max mem: 15572
Epoch: [10]  [ 30/597]  eta: 0:04:48  lr: 0.000045  min_lr: 0.000000  loss: 4.0709 (4.1452)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0003  max mem: 15572
Epoch: [10]  [ 40/597]  eta: 0:04:26  lr: 0.000044  min_lr: 0.000000  loss: 4.2800 (4.2087)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3805  data: 0.0004  max mem: 15572
Epoch: [10]  [ 50/597]  eta: 0:04:12  lr: 0.000044  min_lr: 0.000000  loss: 4.3730 (4.2361)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0005  max mem: 15572
Epoch: [10]  [ 60/597]  eta: 0:04:01  lr: 0.000044  min_lr: 0.000000  loss: 4.2560 (4.2109)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3884  data: 0.0005  max mem: 15572
Epoch: [10]  [ 70/597]  eta: 0:03:52  lr: 0.000044  min_lr: 0.000000  loss: 4.3310 (4.2264)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3906  data: 0.0005  max mem: 15572
Epoch: [10]  [ 80/597]  eta: 0:03:45  lr: 0.000044  min_lr: 0.000000  loss: 4.3668 (4.2277)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3916  data: 0.0005  max mem: 15572
Epoch: [10]  [ 90/597]  eta: 0:03:37  lr: 0.000044  min_lr: 0.000000  loss: 4.2740 (4.2309)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3872  data: 0.0005  max mem: 15572
Epoch: [10]  [100/597]  eta: 0:03:30  lr: 0.000044  min_lr: 0.000000  loss: 4.1959 (4.2374)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3786  data: 0.0004  max mem: 15572
[2025-01-13 13:11:14,744] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:11:14,744] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 13:11:16,993] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19902
[2025-01-13 13:11:16,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:11:16,993] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [110/597]  eta: 0:03:24  lr: 0.000044  min_lr: 0.000000  loss: 4.1959 (4.2407)  loss_scale: 65536.0000 (69078.4865)  weight_decay: 0.0500 (0.0500)  time: 0.3730  data: 0.0003  max mem: 15572
Epoch: [10]  [120/597]  eta: 0:03:18  lr: 0.000044  min_lr: 0.000000  loss: 4.4042 (4.2426)  loss_scale: 65536.0000 (68785.7190)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [10]  [130/597]  eta: 0:03:12  lr: 0.000044  min_lr: 0.000000  loss: 4.2952 (4.2393)  loss_scale: 65536.0000 (68537.6489)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0003  max mem: 15572
Epoch: [10]  [140/597]  eta: 0:03:07  lr: 0.000044  min_lr: 0.000000  loss: 4.2372 (4.2412)  loss_scale: 65536.0000 (68324.7660)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0003  max mem: 15572
Epoch: [10]  [150/597]  eta: 0:03:02  lr: 0.000044  min_lr: 0.000000  loss: 4.2079 (4.2362)  loss_scale: 65536.0000 (68140.0795)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0003  max mem: 15572
Epoch: [10]  [160/597]  eta: 0:02:57  lr: 0.000044  min_lr: 0.000000  loss: 4.2151 (4.2464)  loss_scale: 65536.0000 (67978.3354)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
Epoch: [10]  [170/597]  eta: 0:02:52  lr: 0.000044  min_lr: 0.000000  loss: 4.3320 (4.2449)  loss_scale: 65536.0000 (67835.5088)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0003  max mem: 15572
Epoch: [10]  [180/597]  eta: 0:02:47  lr: 0.000044  min_lr: 0.000000  loss: 4.1508 (4.2421)  loss_scale: 65536.0000 (67708.4641)  weight_decay: 0.0500 (0.0500)  time: 0.3800  data: 0.0004  max mem: 15572
Epoch: [10]  [190/597]  eta: 0:02:43  lr: 0.000044  min_lr: 0.000000  loss: 4.2862 (4.2392)  loss_scale: 65536.0000 (67594.7225)  weight_decay: 0.0500 (0.0500)  time: 0.3907  data: 0.0005  max mem: 15572
Epoch: [10]  [200/597]  eta: 0:02:39  lr: 0.000044  min_lr: 0.000000  loss: 4.2933 (4.2399)  loss_scale: 65536.0000 (67492.2985)  weight_decay: 0.0500 (0.0500)  time: 0.3946  data: 0.0005  max mem: 15572
[2025-01-13 13:11:53,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=129, lr=[4.285709859200856e-07, 4.285709859200856e-07, 6.122442656001223e-07, 6.122442656001223e-07, 8.746346651430319e-07, 8.746346651430319e-07, 1.2494780930614742e-06, 1.2494780930614742e-06, 1.7849687043735348e-06, 1.7849687043735348e-06, 2.5499552919621924e-06, 2.5499552919621924e-06, 3.6427932742317038e-06, 3.6427932742317038e-06, 5.203990391759578e-06, 5.203990391759578e-06, 7.434271988227968e-06, 7.434271988227968e-06, 1.0620388554611385e-05, 1.0620388554611385e-05, 1.5171983649444835e-05, 1.5171983649444835e-05, 2.1674262356349766e-05, 2.1674262356349766e-05, 3.096323193764252e-05, 3.096323193764252e-05, 4.4233188482346465e-05, 4.4233188482346465e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 13:11:53,952] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=29.82513327693737, CurrSamplesPerSec=32.9709559822999, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [10]  [210/597]  eta: 0:02:35  lr: 0.000044  min_lr: 0.000000  loss: 4.1800 (4.2409)  loss_scale: 65536.0000 (67399.5829)  weight_decay: 0.0500 (0.0500)  time: 0.3945  data: 0.0005  max mem: 15572
Epoch: [10]  [220/597]  eta: 0:02:31  lr: 0.000044  min_lr: 0.000000  loss: 4.1163 (4.2333)  loss_scale: 65536.0000 (67315.2579)  weight_decay: 0.0500 (0.0500)  time: 0.3988  data: 0.0005  max mem: 15572
Epoch: [10]  [230/597]  eta: 0:02:27  lr: 0.000044  min_lr: 0.000000  loss: 4.1364 (4.2350)  loss_scale: 65536.0000 (67238.2338)  weight_decay: 0.0500 (0.0500)  time: 0.3966  data: 0.0005  max mem: 15572
[2025-01-13 13:12:06,542] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:12:06,542] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 13:12:06,917] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20032
[2025-01-13 13:12:06,918] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:12:06,918] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [240/597]  eta: 0:02:22  lr: 0.000044  min_lr: 0.000000  loss: 4.3252 (4.2430)  loss_scale: 65536.0000 (67439.5353)  weight_decay: 0.0500 (0.0500)  time: 0.3872  data: 0.0004  max mem: 15572
Epoch: [10]  [250/597]  eta: 0:02:18  lr: 0.000044  min_lr: 0.000000  loss: 4.2466 (4.2390)  loss_scale: 65536.0000 (67363.6972)  weight_decay: 0.0500 (0.0500)  time: 0.3788  data: 0.0003  max mem: 15572
Epoch: [10]  [260/597]  eta: 0:02:14  lr: 0.000044  min_lr: 0.000000  loss: 4.2466 (4.2432)  loss_scale: 65536.0000 (67293.6705)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0002  max mem: 15572
Epoch: [10]  [270/597]  eta: 0:02:10  lr: 0.000044  min_lr: 0.000000  loss: 4.2964 (4.2457)  loss_scale: 65536.0000 (67228.8118)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0003  max mem: 15572
Epoch: [10]  [280/597]  eta: 0:02:05  lr: 0.000044  min_lr: 0.000000  loss: 4.3302 (4.2531)  loss_scale: 65536.0000 (67168.5694)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.0003  max mem: 15572
Epoch: [10]  [290/597]  eta: 0:02:01  lr: 0.000044  min_lr: 0.000000  loss: 4.3836 (4.2585)  loss_scale: 65536.0000 (67112.4674)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0016  max mem: 15572
Epoch: [10]  [300/597]  eta: 0:01:57  lr: 0.000044  min_lr: 0.000000  loss: 4.4019 (4.2586)  loss_scale: 65536.0000 (67060.0930)  weight_decay: 0.0500 (0.0500)  time: 0.3784  data: 0.0016  max mem: 15572
Epoch: [10]  [310/597]  eta: 0:01:53  lr: 0.000044  min_lr: 0.000000  loss: 4.3092 (4.2607)  loss_scale: 65536.0000 (67011.0868)  weight_decay: 0.0500 (0.0500)  time: 0.3789  data: 0.0003  max mem: 15572
Epoch: [10]  [320/597]  eta: 0:01:49  lr: 0.000044  min_lr: 0.000000  loss: 4.2750 (4.2584)  loss_scale: 65536.0000 (66965.1340)  weight_decay: 0.0500 (0.0500)  time: 0.3808  data: 0.0002  max mem: 15572
[2025-01-13 13:12:39,901] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20119
[2025-01-13 13:12:39,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 13:12:39,902] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [330/597]  eta: 0:01:45  lr: 0.000044  min_lr: 0.000000  loss: 4.2355 (4.2605)  loss_scale: 65536.0000 (66327.9758)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0004  max mem: 15572
Epoch: [10]  [340/597]  eta: 0:01:41  lr: 0.000044  min_lr: 0.000000  loss: 4.2897 (4.2602)  loss_scale: 32768.0000 (65343.8123)  weight_decay: 0.0500 (0.0500)  time: 0.3938  data: 0.0006  max mem: 15572
Epoch: [10]  [350/597]  eta: 0:01:37  lr: 0.000044  min_lr: 0.000000  loss: 4.2751 (4.2589)  loss_scale: 32768.0000 (64415.7265)  weight_decay: 0.0500 (0.0500)  time: 0.3964  data: 0.0005  max mem: 15572
Epoch: [10]  [360/597]  eta: 0:01:33  lr: 0.000044  min_lr: 0.000000  loss: 4.3411 (4.2645)  loss_scale: 32768.0000 (63539.0582)  weight_decay: 0.0500 (0.0500)  time: 0.3971  data: 0.0004  max mem: 15572
Epoch: [10]  [370/597]  eta: 0:01:29  lr: 0.000044  min_lr: 0.000000  loss: 4.3193 (4.2631)  loss_scale: 32768.0000 (62709.6496)  weight_decay: 0.0500 (0.0500)  time: 0.3944  data: 0.0005  max mem: 15572
Epoch: [10]  [380/597]  eta: 0:01:25  lr: 0.000044  min_lr: 0.000000  loss: 4.2277 (4.2652)  loss_scale: 32768.0000 (61923.7795)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0005  max mem: 15572
Epoch: [10]  [390/597]  eta: 0:01:21  lr: 0.000044  min_lr: 0.000000  loss: 4.0669 (4.2588)  loss_scale: 32768.0000 (61178.1074)  weight_decay: 0.0500 (0.0500)  time: 0.3768  data: 0.0003  max mem: 15572
Epoch: [10]  [400/597]  eta: 0:01:17  lr: 0.000044  min_lr: 0.000000  loss: 4.0669 (4.2606)  loss_scale: 32768.0000 (60469.6259)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0003  max mem: 15572
Epoch: [10]  [410/597]  eta: 0:01:13  lr: 0.000044  min_lr: 0.000000  loss: 4.4214 (4.2625)  loss_scale: 32768.0000 (59795.6204)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0003  max mem: 15572
Epoch: [10]  [420/597]  eta: 0:01:09  lr: 0.000044  min_lr: 0.000000  loss: 4.2002 (4.2572)  loss_scale: 32768.0000 (59153.6342)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0004  max mem: 15572
Epoch: [10]  [430/597]  eta: 0:01:05  lr: 0.000044  min_lr: 0.000000  loss: 4.1836 (4.2553)  loss_scale: 32768.0000 (58541.4385)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0004  max mem: 15572
Epoch: [10]  [440/597]  eta: 0:01:01  lr: 0.000044  min_lr: 0.000000  loss: 4.1908 (4.2537)  loss_scale: 32768.0000 (57957.0068)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
Epoch: [10]  [450/597]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000000  loss: 4.3812 (4.2578)  loss_scale: 32768.0000 (57398.4922)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0003  max mem: 15572
[2025-01-13 13:13:29,315] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:13:29,315] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [460/597]  eta: 0:00:53  lr: 0.000044  min_lr: 0.000000  loss: 4.3058 (4.2543)  loss_scale: 32768.0000 (57361.7701)  weight_decay: 0.0500 (0.0500)  time: 0.3765  data: 0.0003  max mem: 15572
Epoch: [10]  [470/597]  eta: 0:00:49  lr: 0.000044  min_lr: 0.000000  loss: 4.0626 (4.2553)  loss_scale: 65536.0000 (57535.3206)  weight_decay: 0.0500 (0.0500)  time: 0.3864  data: 0.0003  max mem: 15572
Epoch: [10]  [480/597]  eta: 0:00:45  lr: 0.000044  min_lr: 0.000000  loss: 4.3055 (4.2537)  loss_scale: 65536.0000 (57701.6549)  weight_decay: 0.0500 (0.0500)  time: 0.3946  data: 0.0005  max mem: 15572
Epoch: [10]  [490/597]  eta: 0:00:41  lr: 0.000044  min_lr: 0.000000  loss: 4.3078 (4.2555)  loss_scale: 65536.0000 (57861.2138)  weight_decay: 0.0500 (0.0500)  time: 0.3944  data: 0.0004  max mem: 15572
Epoch: [10]  [500/597]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000000  loss: 4.4084 (4.2574)  loss_scale: 65536.0000 (58014.4032)  weight_decay: 0.0500 (0.0500)  time: 0.3959  data: 0.0006  max mem: 15572
Epoch: [10]  [510/597]  eta: 0:00:34  lr: 0.000044  min_lr: 0.000000  loss: 4.2697 (4.2550)  loss_scale: 65536.0000 (58161.5969)  weight_decay: 0.0500 (0.0500)  time: 0.3961  data: 0.0006  max mem: 15572
Epoch: [10]  [520/597]  eta: 0:00:30  lr: 0.000044  min_lr: 0.000000  loss: 4.2648 (4.2547)  loss_scale: 65536.0000 (58303.1401)  weight_decay: 0.0500 (0.0500)  time: 0.3930  data: 0.0005  max mem: 15572
Epoch: [10]  [530/597]  eta: 0:00:26  lr: 0.000044  min_lr: 0.000000  loss: 4.1851 (4.2548)  loss_scale: 65536.0000 (58439.3522)  weight_decay: 0.0500 (0.0500)  time: 0.3849  data: 0.0004  max mem: 15572
Epoch: [10]  [540/597]  eta: 0:00:22  lr: 0.000044  min_lr: 0.000000  loss: 4.1851 (4.2558)  loss_scale: 65536.0000 (58570.5287)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0003  max mem: 15572
Epoch: [10]  [550/597]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000000  loss: 4.2001 (4.2563)  loss_scale: 65536.0000 (58696.9437)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0003  max mem: 15572
Epoch: [10]  [560/597]  eta: 0:00:14  lr: 0.000044  min_lr: 0.000000  loss: 4.2509 (4.2549)  loss_scale: 65536.0000 (58818.8520)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0003  max mem: 15572
Epoch: [10]  [570/597]  eta: 0:00:10  lr: 0.000044  min_lr: 0.000000  loss: 4.1914 (4.2517)  loss_scale: 65536.0000 (58936.4904)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
Epoch: [10]  [580/597]  eta: 0:00:06  lr: 0.000044  min_lr: 0.000000  loss: 4.2218 (4.2553)  loss_scale: 65536.0000 (59050.0792)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0002  max mem: 15572
[2025-01-13 13:14:18,503] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:14:18,503] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 13:14:20,324] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20381
[2025-01-13 13:14:20,324] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:14:20,324] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [590/597]  eta: 0:00:02  lr: 0.000044  min_lr: 0.000000  loss: 4.4387 (4.2564)  loss_scale: 65536.0000 (59714.2741)  weight_decay: 0.0500 (0.0500)  time: 0.3677  data: 0.0001  max mem: 15572
Epoch: [10]  [596/597]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000000  loss: 4.3981 (4.2573)  loss_scale: 65536.0000 (59772.7839)  weight_decay: 0.0500 (0.0500)  time: 0.3656  data: 0.0001  max mem: 15572
Epoch: [10] Total time: 0:03:52 (0.3896 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000000  loss: 4.3981 (4.2573)  loss_scale: 65536.0000 (59772.7839)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 1897
Indices to remove: tensor([    1,     8,    13,  ..., 33575, 33623, 33639], device='cuda:0')
length of data loader train is: 439
num_training_steps_per_epoch is: 439
Change step level LR scheduler!
Set warmup steps = 2195
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:09:00  loss: 0.5642 (0.5642)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.9866  data: 1.8208  max mem: 15572
Val:  [ 10/272]  eta: 0:01:56  loss: 4.0137 (3.5856)  acc1: 0.0000 (22.7273)  acc5: 16.6667 (33.8384)  time: 0.4450  data: 0.2732  max mem: 15572
Val:  [ 20/272]  eta: 0:01:17  loss: 3.7620 (3.5631)  acc1: 5.5556 (18.5185)  acc5: 33.3333 (41.0053)  time: 0.2250  data: 0.0594  max mem: 15572
Val:  [ 30/272]  eta: 0:01:04  loss: 3.7620 (3.6611)  acc1: 0.0000 (13.7993)  acc5: 50.0000 (42.2939)  time: 0.1685  data: 0.0005  max mem: 15572
Val:  [ 40/272]  eta: 0:00:56  loss: 3.6649 (3.5986)  acc1: 11.1111 (14.4986)  acc5: 50.0000 (44.7154)  time: 0.1776  data: 0.0006  max mem: 15572
Val:  [ 50/272]  eta: 0:00:50  loss: 3.3061 (3.5196)  acc1: 11.1111 (18.1917)  acc5: 61.1111 (46.7320)  time: 0.1718  data: 0.0006  max mem: 15572
Val:  [ 60/272]  eta: 0:00:46  loss: 2.2624 (3.3285)  acc1: 55.5556 (24.4991)  acc5: 72.2222 (50.6375)  time: 0.1736  data: 0.0006  max mem: 15572
Val:  [ 70/272]  eta: 0:00:43  loss: 2.3589 (3.2267)  acc1: 44.4444 (25.5869)  acc5: 77.7778 (54.7731)  time: 0.1745  data: 0.0006  max mem: 15572
Val:  [ 80/272]  eta: 0:00:40  loss: 2.9700 (3.2668)  acc1: 22.2222 (25.4458)  acc5: 72.2222 (53.6351)  time: 0.1788  data: 0.0006  max mem: 15572
Val:  [ 90/272]  eta: 0:00:37  loss: 4.3600 (3.3866)  acc1: 0.0000 (22.7717)  acc5: 16.6667 (48.7790)  time: 0.1808  data: 0.0007  max mem: 15572
Val:  [100/272]  eta: 0:00:35  loss: 4.1987 (3.4563)  acc1: 0.0000 (21.9472)  acc5: 16.6667 (47.5798)  time: 0.1755  data: 0.0006  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 4.0171 (3.5248)  acc1: 0.0000 (20.0200)  acc5: 33.3333 (45.7457)  time: 0.1735  data: 0.0005  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 4.0033 (3.5692)  acc1: 0.0000 (18.5950)  acc5: 33.3333 (45.2250)  time: 0.1705  data: 0.0005  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.8125 (3.5115)  acc1: 0.0000 (20.5683)  acc5: 38.8889 (46.4377)  time: 0.1705  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 3.1717 (3.4990)  acc1: 22.2222 (21.3554)  acc5: 55.5556 (46.5721)  time: 0.1734  data: 0.0005  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.3223 (3.4948)  acc1: 11.1111 (20.7873)  acc5: 55.5556 (47.2406)  time: 0.1739  data: 0.0005  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.3223 (3.4729)  acc1: 16.6667 (22.0152)  acc5: 61.1111 (48.7233)  time: 0.1721  data: 0.0005  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.4792 (3.5017)  acc1: 11.1111 (20.9877)  acc5: 55.5556 (47.7908)  time: 0.1674  data: 0.0005  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.5672 (3.4990)  acc1: 0.0000 (21.0252)  acc5: 44.4444 (48.3118)  time: 0.1639  data: 0.0005  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.8921 (3.5346)  acc1: 0.0000 (20.0698)  acc5: 38.8889 (47.0622)  time: 0.1748  data: 0.0023  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.8921 (3.5305)  acc1: 0.0000 (20.0663)  acc5: 38.8889 (47.5954)  time: 0.1720  data: 0.0024  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.1330 (3.5389)  acc1: 11.1111 (20.1948)  acc5: 66.6667 (47.7883)  time: 0.1690  data: 0.0006  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.4335 (3.5304)  acc1: 16.6667 (20.5631)  acc5: 61.1111 (48.2655)  time: 0.1729  data: 0.0007  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 2.9262 (3.4929)  acc1: 44.4444 (22.2463)  acc5: 66.6667 (49.5190)  time: 0.1690  data: 0.0006  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.5933 (3.4674)  acc1: 44.4444 (22.8907)  acc5: 83.3333 (50.6685)  time: 0.1614  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.2952 (3.4834)  acc1: 16.6667 (22.5985)  acc5: 55.5556 (50.2877)  time: 0.1582  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.4980 (3.4072)  acc1: 72.2222 (25.0958)  acc5: 77.7778 (51.8306)  time: 0.1579  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.2695 (3.4065)  acc1: 61.1111 (25.0308)  acc5: 77.7778 (51.8655)  time: 0.1457  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.2695 (3.4105)  acc1: 55.5556 (25.0051)  acc5: 77.7778 (51.8329)  time: 0.1391  data: 0.0001  max mem: 15572
Val: Total time: 0:00:49 (0.1805 s / it)
* Acc@1 25.005 Acc@5 51.833 loss 3.411
Accuracy of the network on the 4883 val videos: 25.0%
[2025-01-13 13:15:12,939] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 13:15:12,941] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 13:15:12,941] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 13:15:15,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 13:15:15,348] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 25.01%
Epoch: [11]  [  0/439]  eta: 0:29:46  lr: 0.000044  min_lr: 0.000000  loss: 4.1814 (4.1814)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.0690  data: 3.6618  max mem: 15572
Epoch: [11]  [ 10/439]  eta: 0:05:10  lr: 0.000044  min_lr: 0.000000  loss: 4.4027 (4.3547)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7240  data: 0.3391  max mem: 15572
Epoch: [11]  [ 20/439]  eta: 0:03:54  lr: 0.000044  min_lr: 0.000000  loss: 4.2825 (4.2653)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3851  data: 0.0035  max mem: 15572
Epoch: [11]  [ 30/439]  eta: 0:03:25  lr: 0.000043  min_lr: 0.000000  loss: 4.2282 (4.2594)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3813  data: 0.0002  max mem: 15572
Epoch: [11]  [ 40/439]  eta: 0:03:08  lr: 0.000043  min_lr: 0.000000  loss: 4.2425 (4.2457)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3821  data: 0.0002  max mem: 15572
Epoch: [11]  [ 50/439]  eta: 0:02:57  lr: 0.000043  min_lr: 0.000000  loss: 4.1042 (4.2127)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3836  data: 0.0003  max mem: 15572
Epoch: [11]  [ 60/439]  eta: 0:02:48  lr: 0.000043  min_lr: 0.000000  loss: 4.1042 (4.1813)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3877  data: 0.0004  max mem: 15572
Epoch: [11]  [ 70/439]  eta: 0:02:41  lr: 0.000043  min_lr: 0.000000  loss: 4.1657 (4.2015)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3936  data: 0.0004  max mem: 15572
Epoch: [11]  [ 80/439]  eta: 0:02:35  lr: 0.000043  min_lr: 0.000000  loss: 4.2139 (4.2041)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3941  data: 0.0004  max mem: 15572
Epoch: [11]  [ 90/439]  eta: 0:02:29  lr: 0.000043  min_lr: 0.000000  loss: 4.2066 (4.2084)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3895  data: 0.0005  max mem: 15572
Epoch: [11]  [100/439]  eta: 0:02:23  lr: 0.000043  min_lr: 0.000000  loss: 4.2507 (4.2014)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3906  data: 0.0005  max mem: 15572
Epoch: [11]  [110/439]  eta: 0:02:18  lr: 0.000043  min_lr: 0.000000  loss: 4.1682 (4.1993)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3857  data: 0.0003  max mem: 15572
[2025-01-13 13:16:05,361] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:16:05,362] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [120/439]  eta: 0:02:12  lr: 0.000043  min_lr: 0.000000  loss: 4.0888 (4.1976)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0002  max mem: 15572
[2025-01-13 13:16:09,523] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20521
[2025-01-13 13:16:09,523] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:16:09,523] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [130/439]  eta: 0:02:07  lr: 0.000043  min_lr: 0.000000  loss: 4.0888 (4.1994)  loss_scale: 131072.0000 (71039.0229)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
Epoch: [11]  [140/439]  eta: 0:02:02  lr: 0.000043  min_lr: 0.000000  loss: 4.2862 (4.2083)  loss_scale: 65536.0000 (70648.7376)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0002  max mem: 15572
Epoch: [11]  [150/439]  eta: 0:01:57  lr: 0.000043  min_lr: 0.000000  loss: 4.3479 (4.2180)  loss_scale: 65536.0000 (70310.1457)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0002  max mem: 15572
Epoch: [11]  [160/439]  eta: 0:01:53  lr: 0.000043  min_lr: 0.000000  loss: 4.3931 (4.2221)  loss_scale: 65536.0000 (70013.6149)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [11]  [170/439]  eta: 0:01:48  lr: 0.000043  min_lr: 0.000000  loss: 4.0913 (4.2097)  loss_scale: 65536.0000 (69751.7661)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0003  max mem: 15572
Epoch: [11]  [180/439]  eta: 0:01:44  lr: 0.000043  min_lr: 0.000000  loss: 4.0969 (4.2196)  loss_scale: 65536.0000 (69518.8508)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0002  max mem: 15572
Epoch: [11]  [190/439]  eta: 0:01:40  lr: 0.000043  min_lr: 0.000000  loss: 4.3166 (4.2258)  loss_scale: 65536.0000 (69310.3246)  weight_decay: 0.0500 (0.0500)  time: 0.3782  data: 0.0002  max mem: 15572
Epoch: [11]  [200/439]  eta: 0:01:35  lr: 0.000043  min_lr: 0.000000  loss: 4.3166 (4.2292)  loss_scale: 65536.0000 (69122.5473)  weight_decay: 0.0500 (0.0500)  time: 0.3831  data: 0.0003  max mem: 15572
Epoch: [11]  [210/439]  eta: 0:01:31  lr: 0.000043  min_lr: 0.000000  loss: 4.2955 (4.2356)  loss_scale: 65536.0000 (68952.5687)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0004  max mem: 15572
Epoch: [11]  [220/439]  eta: 0:01:27  lr: 0.000043  min_lr: 0.000000  loss: 4.2215 (4.2339)  loss_scale: 65536.0000 (68797.9729)  weight_decay: 0.0500 (0.0500)  time: 0.3933  data: 0.0005  max mem: 15572
Epoch: [11]  [230/439]  eta: 0:01:23  lr: 0.000043  min_lr: 0.000000  loss: 4.2482 (4.2405)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.4004  data: 0.0006  max mem: 15572
Epoch: [11]  [240/439]  eta: 0:01:19  lr: 0.000043  min_lr: 0.000000  loss: 4.3153 (4.2403)  loss_scale: 65536.0000 (68527.2697)  weight_decay: 0.0500 (0.0500)  time: 0.3995  data: 0.0006  max mem: 15572
Epoch: [11]  [250/439]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000000  loss: 4.1199 (4.2391)  loss_scale: 65536.0000 (68408.0956)  weight_decay: 0.0500 (0.0500)  time: 0.3985  data: 0.0004  max mem: 15572
[2025-01-13 13:16:59,187] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:16:59,187] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [260/439]  eta: 0:01:11  lr: 0.000043  min_lr: 0.000000  loss: 4.3909 (4.2484)  loss_scale: 65536.0000 (68800.2452)  weight_decay: 0.0500 (0.0500)  time: 0.3878  data: 0.0003  max mem: 15572
[2025-01-13 13:17:00,318] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20653
[2025-01-13 13:17:00,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:17:00,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [270/439]  eta: 0:01:07  lr: 0.000043  min_lr: 0.000000  loss: 4.3909 (4.2471)  loss_scale: 65536.0000 (68921.6236)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [11]  [280/439]  eta: 0:01:03  lr: 0.000043  min_lr: 0.000000  loss: 4.3129 (4.2528)  loss_scale: 65536.0000 (68801.1388)  weight_decay: 0.0500 (0.0500)  time: 0.3786  data: 0.0003  max mem: 15572
Epoch: [11]  [290/439]  eta: 0:00:59  lr: 0.000043  min_lr: 0.000000  loss: 4.3877 (4.2538)  loss_scale: 65536.0000 (68688.9347)  weight_decay: 0.0500 (0.0500)  time: 0.3774  data: 0.0003  max mem: 15572
Epoch: [11]  [300/439]  eta: 0:00:55  lr: 0.000043  min_lr: 0.000000  loss: 4.2512 (4.2497)  loss_scale: 65536.0000 (68584.1860)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0003  max mem: 15572
Epoch: [11]  [310/439]  eta: 0:00:51  lr: 0.000043  min_lr: 0.000000  loss: 4.1180 (4.2495)  loss_scale: 65536.0000 (68486.1736)  weight_decay: 0.0500 (0.0500)  time: 0.3738  data: 0.0003  max mem: 15572
Epoch: [11]  [320/439]  eta: 0:00:46  lr: 0.000043  min_lr: 0.000000  loss: 4.3104 (4.2531)  loss_scale: 65536.0000 (68394.2679)  weight_decay: 0.0500 (0.0500)  time: 0.3746  data: 0.0002  max mem: 15572
Epoch: [11]  [330/439]  eta: 0:00:42  lr: 0.000043  min_lr: 0.000000  loss: 4.3104 (4.2504)  loss_scale: 65536.0000 (68307.9154)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0003  max mem: 15572
Epoch: [11]  [340/439]  eta: 0:00:38  lr: 0.000043  min_lr: 0.000000  loss: 4.2731 (4.2512)  loss_scale: 65536.0000 (68226.6276)  weight_decay: 0.0500 (0.0500)  time: 0.3791  data: 0.0002  max mem: 15572
Epoch: [11]  [350/439]  eta: 0:00:35  lr: 0.000043  min_lr: 0.000000  loss: 4.3388 (4.2501)  loss_scale: 65536.0000 (68149.9715)  weight_decay: 0.0500 (0.0500)  time: 0.3885  data: 0.0003  max mem: 15572
Epoch: [11]  [360/439]  eta: 0:00:31  lr: 0.000043  min_lr: 0.000000  loss: 4.2857 (4.2516)  loss_scale: 65536.0000 (68077.5623)  weight_decay: 0.0500 (0.0500)  time: 0.3950  data: 0.0004  max mem: 15572
Epoch: [11]  [370/439]  eta: 0:00:27  lr: 0.000043  min_lr: 0.000000  loss: 4.1601 (4.2512)  loss_scale: 65536.0000 (68009.0566)  weight_decay: 0.0500 (0.0500)  time: 0.3942  data: 0.0004  max mem: 15572
Epoch: [11]  [380/439]  eta: 0:00:23  lr: 0.000043  min_lr: 0.000000  loss: 4.1293 (4.2490)  loss_scale: 65536.0000 (67944.1470)  weight_decay: 0.0500 (0.0500)  time: 0.3937  data: 0.0005  max mem: 15572
Epoch: [11]  [390/439]  eta: 0:00:19  lr: 0.000043  min_lr: 0.000000  loss: 4.3333 (4.2508)  loss_scale: 65536.0000 (67882.5575)  weight_decay: 0.0500 (0.0500)  time: 0.3881  data: 0.0004  max mem: 15572
[2025-01-13 13:17:49,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:17:49,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 13:17:51,250] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20786
[2025-01-13 13:17:51,250] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:17:51,250] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [400/439]  eta: 0:00:15  lr: 0.000043  min_lr: 0.000000  loss: 4.2943 (4.2502)  loss_scale: 65536.0000 (68477.7656)  weight_decay: 0.0500 (0.0500)  time: 0.3818  data: 0.0004  max mem: 15572
Epoch: [11]  [410/439]  eta: 0:00:11  lr: 0.000042  min_lr: 0.000000  loss: 4.1817 (4.2500)  loss_scale: 65536.0000 (68406.1898)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0003  max mem: 15572
Epoch: [11]  [420/439]  eta: 0:00:07  lr: 0.000042  min_lr: 0.000000  loss: 4.1817 (4.2499)  loss_scale: 65536.0000 (68338.0143)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [11]  [430/439]  eta: 0:00:03  lr: 0.000042  min_lr: 0.000000  loss: 4.2140 (4.2505)  loss_scale: 65536.0000 (68273.0023)  weight_decay: 0.0500 (0.0500)  time: 0.3697  data: 0.0002  max mem: 15572
Epoch: [11]  [438/439]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000000  loss: 4.1966 (4.2464)  loss_scale: 65536.0000 (68223.1253)  weight_decay: 0.0500 (0.0500)  time: 0.3663  data: 0.0001  max mem: 15572
Epoch: [11] Total time: 0:02:51 (0.3918 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000000  loss: 4.1966 (4.2464)  loss_scale: 65536.0000 (68223.1253)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 1392
Indices to remove: tensor([   13,    14,    21,  ..., 33475, 33578, 33636], device='cuda:0')
length of data loader train is: 323
num_training_steps_per_epoch is: 323
Change step level LR scheduler!
Set warmup steps = 1615
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:12:51  loss: 0.4989 (0.4989)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.8378  data: 2.6529  max mem: 15572
Val:  [ 10/272]  eta: 0:01:54  loss: 3.9355 (3.5368)  acc1: 0.0000 (22.7273)  acc5: 11.1111 (32.3232)  time: 0.4359  data: 0.2739  max mem: 15572
Val:  [ 20/272]  eta: 0:01:16  loss: 3.5943 (3.4815)  acc1: 5.5556 (20.6349)  acc5: 38.8889 (43.3862)  time: 0.1781  data: 0.0182  max mem: 15572
Val:  [ 30/272]  eta: 0:01:02  loss: 3.7493 (3.6023)  acc1: 5.5556 (15.4122)  acc5: 50.0000 (41.9355)  time: 0.1630  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:54  loss: 3.5666 (3.5442)  acc1: 5.5556 (16.2602)  acc5: 50.0000 (44.7154)  time: 0.1632  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:49  loss: 3.3499 (3.4655)  acc1: 16.6667 (19.1721)  acc5: 55.5556 (47.1678)  time: 0.1603  data: 0.0005  max mem: 15572
Val:  [ 60/272]  eta: 0:00:44  loss: 2.3346 (3.2795)  acc1: 50.0000 (25.7741)  acc5: 72.2222 (50.8197)  time: 0.1612  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:41  loss: 2.1829 (3.1851)  acc1: 55.5556 (26.7606)  acc5: 77.7778 (54.7731)  time: 0.1606  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:38  loss: 2.8989 (3.2177)  acc1: 16.6667 (26.5432)  acc5: 72.2222 (53.4979)  time: 0.1680  data: 0.0098  max mem: 15572
Val:  [ 90/272]  eta: 0:00:36  loss: 4.1311 (3.3241)  acc1: 0.0000 (23.9927)  acc5: 16.6667 (49.1453)  time: 0.1847  data: 0.0194  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 4.1277 (3.3892)  acc1: 5.5556 (23.2673)  acc5: 22.2222 (48.4048)  time: 0.2102  data: 0.0323  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 3.9931 (3.4526)  acc1: 5.5556 (21.5716)  acc5: 33.3333 (46.9469)  time: 0.2076  data: 0.0229  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 3.9931 (3.4956)  acc1: 0.0000 (20.2020)  acc5: 33.3333 (46.0514)  time: 0.1889  data: 0.0007  max mem: 15572
Val:  [130/272]  eta: 0:00:28  loss: 3.8330 (3.4454)  acc1: 11.1111 (22.1798)  acc5: 38.8889 (47.1162)  time: 0.1864  data: 0.0008  max mem: 15572
Val:  [140/272]  eta: 0:00:26  loss: 3.2235 (3.4434)  acc1: 27.7778 (22.8526)  acc5: 50.0000 (46.9661)  time: 0.1817  data: 0.0008  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.4202 (3.4486)  acc1: 5.5556 (21.8175)  acc5: 50.0000 (47.1670)  time: 0.1735  data: 0.0006  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.3111 (3.4227)  acc1: 16.6667 (23.0504)  acc5: 66.6667 (48.5852)  time: 0.1650  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.4054 (3.4521)  acc1: 11.1111 (22.0273)  acc5: 61.1111 (47.7583)  time: 0.1729  data: 0.0005  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.5075 (3.4511)  acc1: 5.5556 (21.9460)  acc5: 44.4444 (48.3118)  time: 0.1747  data: 0.0006  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.7414 (3.4834)  acc1: 0.0000 (21.0006)  acc5: 44.4444 (47.2368)  time: 0.1754  data: 0.0006  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.6574 (3.4797)  acc1: 5.5556 (21.0061)  acc5: 38.8889 (47.8441)  time: 0.1765  data: 0.0007  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.1631 (3.4906)  acc1: 11.1111 (21.0900)  acc5: 61.1111 (48.2096)  time: 0.1786  data: 0.0008  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.4420 (3.4851)  acc1: 22.2222 (21.4681)  acc5: 55.5556 (48.4163)  time: 0.1820  data: 0.0008  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.0167 (3.4530)  acc1: 38.8889 (23.0159)  acc5: 66.6667 (49.5911)  time: 0.1793  data: 0.0007  max mem: 15572
Val:  [240/272]  eta: 0:00:06  loss: 2.6654 (3.4269)  acc1: 44.4444 (23.8359)  acc5: 77.7778 (50.7607)  time: 0.1785  data: 0.0006  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.2502 (3.4452)  acc1: 22.2222 (23.5945)  acc5: 66.6667 (50.4869)  time: 0.1723  data: 0.0005  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.4980 (3.3677)  acc1: 66.6667 (26.0324)  acc5: 77.7778 (52.1286)  time: 0.1591  data: 0.0004  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.3331 (3.3685)  acc1: 55.5556 (25.8508)  acc5: 77.7778 (52.1115)  time: 0.1465  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.3331 (3.3722)  acc1: 55.5556 (25.8243)  acc5: 77.7778 (52.0786)  time: 0.1405  data: 0.0002  max mem: 15572
Val: Total time: 0:00:50 (0.1848 s / it)
* Acc@1 25.824 Acc@5 52.079 loss 3.372
Accuracy of the network on the 4883 val videos: 25.8%
[2025-01-13 13:18:57,680] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 13:18:57,684] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 13:18:57,684] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 13:19:00,281] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 13:19:00,281] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 25.82%
Epoch: [12]  [  0/323]  eta: 0:23:38  lr: 0.000042  min_lr: 0.000000  loss: 4.3580 (4.3580)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.3914  data: 3.9990  max mem: 15572
Epoch: [12]  [ 10/323]  eta: 0:03:53  lr: 0.000042  min_lr: 0.000000  loss: 4.4186 (4.3874)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7471  data: 0.3639  max mem: 15572
Epoch: [12]  [ 20/323]  eta: 0:02:53  lr: 0.000042  min_lr: 0.000000  loss: 4.3302 (4.2796)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3828  data: 0.0004  max mem: 15572
Epoch: [12]  [ 30/323]  eta: 0:02:28  lr: 0.000042  min_lr: 0.000000  loss: 4.2798 (4.3131)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3766  data: 0.0003  max mem: 15572
Epoch: [12]  [ 40/323]  eta: 0:02:14  lr: 0.000042  min_lr: 0.000000  loss: 4.2798 (4.2783)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0003  max mem: 15572
Epoch: [12]  [ 50/323]  eta: 0:02:04  lr: 0.000042  min_lr: 0.000000  loss: 4.2444 (4.2710)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0003  max mem: 15572
Epoch: [12]  [ 60/323]  eta: 0:01:56  lr: 0.000042  min_lr: 0.000000  loss: 4.0708 (4.2349)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.0003  max mem: 15572
Epoch: [12]  [ 70/323]  eta: 0:01:49  lr: 0.000042  min_lr: 0.000000  loss: 4.1629 (4.2461)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3756  data: 0.0003  max mem: 15572
Epoch: [12]  [ 80/323]  eta: 0:01:44  lr: 0.000042  min_lr: 0.000000  loss: 4.1908 (4.2284)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3904  data: 0.0004  max mem: 15572
[2025-01-13 13:19:37,057] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:19:37,058] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 13:19:38,638] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20919
[2025-01-13 13:19:38,639] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:19:38,639] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 90/323]  eta: 0:01:39  lr: 0.000042  min_lr: 0.000000  loss: 4.2579 (4.2367)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)  time: 0.4003  data: 0.0004  max mem: 15572
Epoch: [12]  [100/323]  eta: 0:01:34  lr: 0.000042  min_lr: 0.000000  loss: 4.3575 (4.2364)  loss_scale: 65536.0000 (68131.4851)  weight_decay: 0.0500 (0.0500)  time: 0.3965  data: 0.0004  max mem: 15572
Epoch: [12]  [110/323]  eta: 0:01:29  lr: 0.000042  min_lr: 0.000000  loss: 4.1007 (4.2127)  loss_scale: 65536.0000 (67897.6577)  weight_decay: 0.0500 (0.0500)  time: 0.3996  data: 0.0005  max mem: 15572
Epoch: [12]  [120/323]  eta: 0:01:25  lr: 0.000042  min_lr: 0.000000  loss: 4.0431 (4.2104)  loss_scale: 65536.0000 (67702.4793)  weight_decay: 0.0500 (0.0500)  time: 0.3993  data: 0.0005  max mem: 15572
Epoch: [12]  [130/323]  eta: 0:01:20  lr: 0.000042  min_lr: 0.000000  loss: 4.1306 (4.2031)  loss_scale: 65536.0000 (67537.0992)  weight_decay: 0.0500 (0.0500)  time: 0.3999  data: 0.0005  max mem: 15572
Epoch: [12]  [140/323]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000000  loss: 4.1318 (4.2018)  loss_scale: 65536.0000 (67395.1773)  weight_decay: 0.0500 (0.0500)  time: 0.3891  data: 0.0004  max mem: 15572
Epoch: [12]  [150/323]  eta: 0:01:11  lr: 0.000042  min_lr: 0.000000  loss: 4.1634 (4.1917)  loss_scale: 65536.0000 (67272.0530)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0003  max mem: 15572
Epoch: [12]  [160/323]  eta: 0:01:06  lr: 0.000042  min_lr: 0.000000  loss: 4.2967 (4.2062)  loss_scale: 65536.0000 (67164.2236)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0003  max mem: 15572
[2025-01-13 13:20:09,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=136, lr=[4.0437910169161685e-07, 4.0437910169161685e-07, 5.776844309880242e-07, 5.776844309880242e-07, 8.252634728400346e-07, 8.252634728400346e-07, 1.1789478183429065e-06, 1.1789478183429065e-06, 1.6842111690612952e-06, 1.6842111690612952e-06, 2.4060159558018504e-06, 2.4060159558018504e-06, 3.4371656511455007e-06, 3.4371656511455007e-06, 4.910236644493573e-06, 4.910236644493573e-06, 7.014623777847961e-06, 7.014623777847961e-06, 1.0020891111211375e-05, 1.0020891111211375e-05, 1.4315558730301964e-05, 1.4315558730301964e-05, 2.0450798186145663e-05, 2.0450798186145663e-05, 2.9215425980208092e-05, 2.9215425980208092e-05, 4.173632282886871e-05, 4.173632282886871e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 13:20:09,659] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=29.96647674854536, CurrSamplesPerSec=34.25751030312105, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [170/323]  eta: 0:01:02  lr: 0.000042  min_lr: 0.000000  loss: 4.3928 (4.2111)  loss_scale: 65536.0000 (67069.0058)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0003  max mem: 15572
Epoch: [12]  [180/323]  eta: 0:00:58  lr: 0.000042  min_lr: 0.000000  loss: 4.3422 (4.2142)  loss_scale: 65536.0000 (66984.3094)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [12]  [190/323]  eta: 0:00:53  lr: 0.000042  min_lr: 0.000000  loss: 4.2036 (4.2068)  loss_scale: 65536.0000 (66908.4817)  weight_decay: 0.0500 (0.0500)  time: 0.3748  data: 0.0003  max mem: 15572
Epoch: [12]  [200/323]  eta: 0:00:49  lr: 0.000042  min_lr: 0.000000  loss: 4.1183 (4.2071)  loss_scale: 65536.0000 (66840.1990)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [12]  [210/323]  eta: 0:00:45  lr: 0.000042  min_lr: 0.000000  loss: 4.2314 (4.2016)  loss_scale: 65536.0000 (66778.3886)  weight_decay: 0.0500 (0.0500)  time: 0.3793  data: 0.0003  max mem: 15572
[2025-01-13 13:20:28,145] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:20:28,145] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 13:20:28,547] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21049
[2025-01-13 13:20:28,548] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:20:28,548] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [220/323]  eta: 0:00:41  lr: 0.000042  min_lr: 0.000000  loss: 4.1075 (4.1973)  loss_scale: 65536.0000 (67018.7149)  weight_decay: 0.0500 (0.0500)  time: 0.3789  data: 0.0004  max mem: 15572
Epoch: [12]  [230/323]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000000  loss: 4.2129 (4.2018)  loss_scale: 65536.0000 (66954.5281)  weight_decay: 0.0500 (0.0500)  time: 0.3936  data: 0.0004  max mem: 15572
Epoch: [12]  [240/323]  eta: 0:00:33  lr: 0.000041  min_lr: 0.000000  loss: 4.1663 (4.1987)  loss_scale: 65536.0000 (66895.6680)  weight_decay: 0.0500 (0.0500)  time: 0.4021  data: 0.0004  max mem: 15572
Epoch: [12]  [250/323]  eta: 0:00:29  lr: 0.000041  min_lr: 0.000000  loss: 4.0511 (4.1934)  loss_scale: 65536.0000 (66841.4980)  weight_decay: 0.0500 (0.0500)  time: 0.3949  data: 0.0005  max mem: 15572
Epoch: [12]  [260/323]  eta: 0:00:25  lr: 0.000041  min_lr: 0.000000  loss: 4.1242 (4.1915)  loss_scale: 65536.0000 (66791.4789)  weight_decay: 0.0500 (0.0500)  time: 0.3943  data: 0.0005  max mem: 15572
Epoch: [12]  [270/323]  eta: 0:00:21  lr: 0.000041  min_lr: 0.000000  loss: 4.1317 (4.1883)  loss_scale: 65536.0000 (66745.1513)  weight_decay: 0.0500 (0.0500)  time: 0.3929  data: 0.0005  max mem: 15572
Epoch: [12]  [280/323]  eta: 0:00:17  lr: 0.000041  min_lr: 0.000000  loss: 4.0572 (4.1823)  loss_scale: 65536.0000 (66702.1210)  weight_decay: 0.0500 (0.0500)  time: 0.3878  data: 0.0004  max mem: 15572
Epoch: [12]  [290/323]  eta: 0:00:13  lr: 0.000041  min_lr: 0.000000  loss: 4.0325 (4.1841)  loss_scale: 65536.0000 (66662.0481)  weight_decay: 0.0500 (0.0500)  time: 0.3799  data: 0.0003  max mem: 15572
Epoch: [12]  [300/323]  eta: 0:00:09  lr: 0.000041  min_lr: 0.000000  loss: 4.1422 (4.1811)  loss_scale: 65536.0000 (66624.6379)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0003  max mem: 15572
Epoch: [12]  [310/323]  eta: 0:00:05  lr: 0.000041  min_lr: 0.000000  loss: 4.1420 (4.1800)  loss_scale: 65536.0000 (66589.6334)  weight_decay: 0.0500 (0.0500)  time: 0.3747  data: 0.0002  max mem: 15572
Epoch: [12]  [320/323]  eta: 0:00:01  lr: 0.000041  min_lr: 0.000000  loss: 4.1923 (4.1846)  loss_scale: 65536.0000 (66556.8100)  weight_decay: 0.0500 (0.0500)  time: 0.3671  data: 0.0001  max mem: 15572
Epoch: [12]  [322/323]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000000  loss: 4.4313 (4.1876)  loss_scale: 65536.0000 (66550.4892)  weight_decay: 0.0500 (0.0500)  time: 0.3661  data: 0.0001  max mem: 15572
Epoch: [12] Total time: 0:02:08 (0.3965 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000000  loss: 4.4313 (4.1876)  loss_scale: 65536.0000 (66550.4892)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 1064
Indices to remove: tensor([   13,    14,    21,  ..., 33550, 33603, 33668], device='cuda:0')
length of data loader train is: 234
num_training_steps_per_epoch is: 234
Change step level LR scheduler!
Set warmup steps = 1170
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:12:05  loss: 0.4972 (0.4972)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6683  data: 2.5110  max mem: 15572
Val:  [ 10/272]  eta: 0:01:43  loss: 4.0390 (3.6305)  acc1: 0.0000 (22.2222)  acc5: 22.2222 (32.3232)  time: 0.3954  data: 0.2438  max mem: 15572
Val:  [ 20/272]  eta: 0:01:10  loss: 3.9144 (3.6719)  acc1: 11.1111 (20.1058)  acc5: 27.7778 (35.7143)  time: 0.1616  data: 0.0087  max mem: 15572
Val:  [ 30/272]  eta: 0:00:59  loss: 3.8890 (3.7696)  acc1: 11.1111 (15.4122)  acc5: 33.3333 (36.5591)  time: 0.1622  data: 0.0065  max mem: 15572
Val:  [ 40/272]  eta: 0:00:53  loss: 3.6842 (3.7068)  acc1: 5.5556 (14.9051)  acc5: 44.4444 (39.7019)  time: 0.1735  data: 0.0189  max mem: 15572
Val:  [ 50/272]  eta: 0:00:48  loss: 3.5176 (3.6031)  acc1: 11.1111 (18.0828)  acc5: 50.0000 (42.3747)  time: 0.1780  data: 0.0169  max mem: 15572
Val:  [ 60/272]  eta: 0:00:44  loss: 2.1353 (3.3849)  acc1: 50.0000 (24.7723)  acc5: 72.2222 (46.7213)  time: 0.1740  data: 0.0046  max mem: 15572
Val:  [ 70/272]  eta: 0:00:40  loss: 2.2469 (3.2504)  acc1: 55.5556 (26.1346)  acc5: 83.3333 (51.6432)  time: 0.1596  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:38  loss: 2.8558 (3.2697)  acc1: 22.2222 (26.0631)  acc5: 77.7778 (51.0974)  time: 0.1648  data: 0.0064  max mem: 15572
Val:  [ 90/272]  eta: 0:00:35  loss: 4.0087 (3.3608)  acc1: 5.5556 (23.6264)  acc5: 22.2222 (47.2527)  time: 0.1716  data: 0.0065  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 4.0087 (3.4197)  acc1: 5.5556 (23.1573)  acc5: 22.2222 (46.8647)  time: 0.1939  data: 0.0279  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 3.9148 (3.4792)  acc1: 5.5556 (21.2212)  acc5: 33.3333 (45.3954)  time: 0.1916  data: 0.0279  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 3.9426 (3.5175)  acc1: 0.0000 (19.9265)  acc5: 33.3333 (44.9036)  time: 0.1706  data: 0.0006  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.7926 (3.4653)  acc1: 11.1111 (21.7557)  acc5: 38.8889 (46.0136)  time: 0.1779  data: 0.0007  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 3.0549 (3.4574)  acc1: 27.7778 (22.3798)  acc5: 55.5556 (46.2175)  time: 0.1702  data: 0.0006  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.2654 (3.4548)  acc1: 5.5556 (21.4496)  acc5: 50.0000 (46.7255)  time: 0.1695  data: 0.0005  max mem: 15572
Val:  [160/272]  eta: 0:00:20  loss: 3.2223 (3.4298)  acc1: 16.6667 (22.5673)  acc5: 72.2222 (48.1712)  time: 0.1664  data: 0.0006  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.2827 (3.4556)  acc1: 22.2222 (21.7024)  acc5: 66.6667 (47.5309)  time: 0.1653  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.3626 (3.4568)  acc1: 11.1111 (22.0074)  acc5: 55.5556 (48.1891)  time: 0.1670  data: 0.0006  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 4.0132 (3.4940)  acc1: 0.0000 (20.9715)  acc5: 33.3333 (47.0332)  time: 0.1716  data: 0.0006  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 4.0132 (3.4942)  acc1: 5.5556 (21.0614)  acc5: 38.8889 (47.5401)  time: 0.1710  data: 0.0005  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.2220 (3.5074)  acc1: 16.6667 (21.1427)  acc5: 61.1111 (47.6567)  time: 0.1721  data: 0.0006  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.5957 (3.5008)  acc1: 16.6667 (21.5686)  acc5: 50.0000 (48.0644)  time: 0.1756  data: 0.0007  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 2.9065 (3.4643)  acc1: 50.0000 (23.0640)  acc5: 72.2222 (49.2785)  time: 0.1680  data: 0.0006  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.7378 (3.4415)  acc1: 50.0000 (23.6745)  acc5: 77.7778 (50.2766)  time: 0.1729  data: 0.0005  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 3.2199 (3.4588)  acc1: 16.6667 (23.2846)  acc5: 55.5556 (49.9115)  time: 0.1731  data: 0.0005  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.7000 (3.3826)  acc1: 44.4444 (25.6492)  acc5: 77.7778 (51.4687)  time: 0.1594  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.6249 (3.3887)  acc1: 50.0000 (25.3383)  acc5: 77.7778 (51.2095)  time: 0.1464  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.6249 (3.3923)  acc1: 44.4444 (25.3123)  acc5: 66.6667 (51.1776)  time: 0.1392  data: 0.0001  max mem: 15572
Val: Total time: 0:00:48 (0.1791 s / it)
* Acc@1 25.312 Acc@5 51.178 loss 3.392
Accuracy of the network on the 4883 val videos: 25.3%
Max accuracy: 25.82%
Epoch: [13]  [  0/234]  eta: 0:16:36  lr: 0.000041  min_lr: 0.000000  loss: 4.2240 (4.2240)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.2597  data: 3.8370  max mem: 15572
Epoch: [13]  [ 10/234]  eta: 0:02:49  lr: 0.000041  min_lr: 0.000000  loss: 4.2240 (4.1017)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7570  data: 0.3674  max mem: 15572
Epoch: [13]  [ 20/234]  eta: 0:02:04  lr: 0.000041  min_lr: 0.000000  loss: 4.1920 (4.1705)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3994  data: 0.0104  max mem: 15572
[2025-01-13 13:22:11,338] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:22:11,338] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [ 30/234]  eta: 0:01:45  lr: 0.000041  min_lr: 0.000000  loss: 4.0528 (4.1376)  loss_scale: 65536.0000 (78220.3871)  weight_decay: 0.0500 (0.0500)  time: 0.3881  data: 0.0003  max mem: 15572
[2025-01-13 13:22:13,609] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21184
[2025-01-13 13:22:13,609] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:22:13,609] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 40/234]  eta: 0:01:33  lr: 0.000041  min_lr: 0.000000  loss: 4.1943 (4.2008)  loss_scale: 65536.0000 (75126.6341)  weight_decay: 0.0500 (0.0500)  time: 0.3800  data: 0.0003  max mem: 15572
Epoch: [13]  [ 50/234]  eta: 0:01:25  lr: 0.000041  min_lr: 0.000000  loss: 4.3603 (4.2195)  loss_scale: 65536.0000 (73246.1176)  weight_decay: 0.0500 (0.0500)  time: 0.3796  data: 0.0002  max mem: 15572
Epoch: [13]  [ 60/234]  eta: 0:01:18  lr: 0.000041  min_lr: 0.000000  loss: 4.1098 (4.1813)  loss_scale: 65536.0000 (71982.1639)  weight_decay: 0.0500 (0.0500)  time: 0.3805  data: 0.0002  max mem: 15572
Epoch: [13]  [ 70/234]  eta: 0:01:12  lr: 0.000041  min_lr: 0.000000  loss: 4.0203 (4.1781)  loss_scale: 65536.0000 (71074.2535)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0002  max mem: 15572
Epoch: [13]  [ 80/234]  eta: 0:01:06  lr: 0.000041  min_lr: 0.000000  loss: 4.1523 (4.1796)  loss_scale: 65536.0000 (70390.5185)  weight_decay: 0.0500 (0.0500)  time: 0.3803  data: 0.0002  max mem: 15572
Epoch: [13]  [ 90/234]  eta: 0:01:01  lr: 0.000041  min_lr: 0.000000  loss: 4.1873 (4.1884)  loss_scale: 65536.0000 (69857.0549)  weight_decay: 0.0500 (0.0500)  time: 0.3842  data: 0.0004  max mem: 15572
Epoch: [13]  [100/234]  eta: 0:00:56  lr: 0.000040  min_lr: 0.000000  loss: 4.2867 (4.2052)  loss_scale: 65536.0000 (69429.2277)  weight_decay: 0.0500 (0.0500)  time: 0.3894  data: 0.0005  max mem: 15572
Epoch: [13]  [110/234]  eta: 0:00:52  lr: 0.000040  min_lr: 0.000000  loss: 4.3262 (4.2047)  loss_scale: 65536.0000 (69078.4865)  weight_decay: 0.0500 (0.0500)  time: 0.3981  data: 0.0006  max mem: 15572
Epoch: [13]  [120/234]  eta: 0:00:47  lr: 0.000040  min_lr: 0.000000  loss: 4.1572 (4.2115)  loss_scale: 65536.0000 (68785.7190)  weight_decay: 0.0500 (0.0500)  time: 0.3954  data: 0.0005  max mem: 15572
Epoch: [13]  [130/234]  eta: 0:00:43  lr: 0.000040  min_lr: 0.000000  loss: 4.2090 (4.2267)  loss_scale: 65536.0000 (68537.6489)  weight_decay: 0.0500 (0.0500)  time: 0.3889  data: 0.0004  max mem: 15572
Epoch: [13]  [140/234]  eta: 0:00:39  lr: 0.000040  min_lr: 0.000000  loss: 4.1704 (4.2133)  loss_scale: 65536.0000 (68324.7660)  weight_decay: 0.0500 (0.0500)  time: 0.3888  data: 0.0004  max mem: 15572
Epoch: [13]  [150/234]  eta: 0:00:34  lr: 0.000040  min_lr: 0.000000  loss: 4.0278 (4.2070)  loss_scale: 65536.0000 (68140.0795)  weight_decay: 0.0500 (0.0500)  time: 0.3834  data: 0.0003  max mem: 15572
[2025-01-13 13:23:03,253] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:23:03,254] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [160/234]  eta: 0:00:30  lr: 0.000040  min_lr: 0.000000  loss: 3.9009 (4.1790)  loss_scale: 65536.0000 (68385.3913)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0002  max mem: 15572
[2025-01-13 13:23:04,364] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21316
[2025-01-13 13:23:04,364] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:23:04,364] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [170/234]  eta: 0:00:26  lr: 0.000040  min_lr: 0.000000  loss: 3.9732 (4.1695)  loss_scale: 65536.0000 (68985.2632)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
Epoch: [13]  [180/234]  eta: 0:00:21  lr: 0.000040  min_lr: 0.000000  loss: 4.1239 (4.1701)  loss_scale: 65536.0000 (68794.6961)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0002  max mem: 15572
Epoch: [13]  [190/234]  eta: 0:00:17  lr: 0.000040  min_lr: 0.000000  loss: 4.1656 (4.1784)  loss_scale: 65536.0000 (68624.0838)  weight_decay: 0.0500 (0.0500)  time: 0.3740  data: 0.0002  max mem: 15572
Epoch: [13]  [200/234]  eta: 0:00:13  lr: 0.000040  min_lr: 0.000000  loss: 4.1656 (4.1789)  loss_scale: 65536.0000 (68470.4478)  weight_decay: 0.0500 (0.0500)  time: 0.3778  data: 0.0003  max mem: 15572
Epoch: [13]  [210/234]  eta: 0:00:09  lr: 0.000040  min_lr: 0.000000  loss: 4.2710 (4.1894)  loss_scale: 65536.0000 (68331.3744)  weight_decay: 0.0500 (0.0500)  time: 0.3754  data: 0.0003  max mem: 15572
Epoch: [13]  [220/234]  eta: 0:00:05  lr: 0.000040  min_lr: 0.000000  loss: 4.2842 (4.1922)  loss_scale: 65536.0000 (68204.8869)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.0003  max mem: 15572
Epoch: [13]  [230/234]  eta: 0:00:01  lr: 0.000040  min_lr: 0.000000  loss: 4.1503 (4.1863)  loss_scale: 65536.0000 (68089.3506)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0002  max mem: 15572
Epoch: [13]  [233/234]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000000  loss: 4.1557 (4.1881)  loss_scale: 65536.0000 (68056.6154)  weight_decay: 0.0500 (0.0500)  time: 0.3703  data: 0.0002  max mem: 15572
Epoch: [13] Total time: 0:01:33 (0.3999 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000000  loss: 4.1557 (4.1881)  loss_scale: 65536.0000 (68056.6154)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 781
Indices to remove: tensor([   13,    14,    21,    36,    64,   126,   127,   148,   149,   164,
          230,   423,   740,   966,  1061,  1097,  1131,  1163,  1207,  1212,
         1255,  1278,  1287,  1291,  1353,  1394,  1436,  1450,  1461,  1595,
         1793,  1966,  1969,  1978,  2074,  2553,  2588,  2776,  2870,  2908,
         2957,  2962,  3004,  3042,  3242,  3286,  3392,  3428,  3511,  3607,
         3719,  3810,  3835,  3841,  3881,  3921,  4031,  4082,  4096,  4242,
         4265,  4422,  4721,  4960,  5021,  5022,  5145,  5184,  5211,  5367,
         5379,  5382,  5424,  5458,  5466,  5489,  5501,  5512,  5515,  5537,
         5547,  5596,  5606,  5664,  5667,  5720,  5733,  5745,  5762,  5788,
         5811,  5877,  5937,  5976,  5984,  6007,  6050,  6084,  6089,  6101,
         6139,  6172,  6178,  6226,  6229,  6258,  6271,  6284,  6285,  6286,
         6298,  6323,  6324,  6355,  6392,  6427,  6435,  6437,  6471,  6562,
         6642,  6647,  6653,  6712,  6737,  6749,  6753,  6759,  6769,  6783,
         6822,  6841,  7130,  7150,  7247,  7262,  7306,  7324,  7347,  7358,
         7361,  7365,  7367,  7401,  7409,  7449,  7482,  7490,  7545,  7557,
         7560,  7595,  7619,  7627,  7676,  7680,  7681,  7686,  7701,  7708,
         7790,  7808,  7825,  7848,  7925,  7960,  8022,  8244,  8252,  8272,
         8288,  8303,  8343,  8358,  8362,  8529,  8592,  8600,  8636,  8665,
         8717,  8727,  8735,  8741,  8769,  8823,  8830,  8835,  8838,  8867,
         8877,  8906,  8922,  8943,  9002,  9007,  9010,  9034,  9035,  9049,
         9053,  9056,  9068,  9087,  9110,  9116,  9143,  9169,  9206,  9249,
         9311,  9355,  9367,  9383,  9398,  9443,  9462,  9493,  9498,  9520,
         9530,  9538,  9541,  9555,  9580,  9598,  9663,  9665,  9666,  9677,
         9693,  9713,  9720,  9726,  9761,  9834,  9843,  9882,  9885,  9890,
         9898,  9923,  9950, 10011, 10066, 10286, 10694, 10866, 11008, 11148,
        11224, 11258, 11262, 11280, 11349, 11361, 11376, 11399, 11400, 11408,
        11418, 11479, 11486, 11497, 11505, 11532, 11574, 11582, 11600, 11635,
        11641, 11725, 11901, 11935, 11951, 11999, 12011, 12234, 12302, 12387,
        12626, 12656, 12717, 12730, 12851, 13047, 13083, 13087, 13104, 13180,
        13281, 13345, 13480, 13611, 13734, 13865, 13956, 14012, 14035, 14051,
        14055, 14084, 14114, 14189, 14298, 14350, 14351, 14390, 14459, 14502,
        14523, 14574, 14718, 14812, 14885, 15046, 15111, 15148, 15213, 15266,
        15271, 15404, 15438, 15740, 15748, 15754, 15766, 15785, 15804, 15820,
        15828, 15842, 15871, 16006, 16024, 16034, 16090, 16111, 16144, 16211,
        16223, 16287, 16325, 16550, 16595, 16616, 16643, 16713, 16714, 16721,
        16762, 16803, 16845, 16847, 16872, 16899, 16914, 16927, 16941, 16959,
        16975, 17040, 17062, 17067, 17084, 17135, 17163, 17168, 17185, 17186,
        17206, 17208, 17211, 17222, 17224, 17330, 17358, 17361, 17473, 17493,
        17497, 17522, 17528, 17531, 17532, 17569, 17572, 17574, 17575, 17577,
        17594, 17600, 17621, 17625, 17639, 17669, 17686, 17738, 17744, 17748,
        17761, 17767, 17777, 17801, 17805, 17816, 17891, 18147, 18419, 18451,
        18515, 18537, 18622, 18640, 18657, 18728, 18732, 18734, 18744, 18775,
        18792, 18795, 18866, 18917, 18931, 18998, 19057, 19112, 19169, 19197,
        19242, 19453, 19759, 19840, 19882, 19967, 20020, 20201, 20214, 20217,
        20257, 20260, 20310, 20320, 20323, 20434, 20451, 20555, 20564, 20576,
        20634, 20648, 20656, 20670, 20689, 20701, 20711, 20723, 20724, 20732,
        20760, 20782, 20791, 20812, 20830, 20887, 20888, 20894, 20949, 20961,
        20966, 20999, 21062, 21063, 21084, 21087, 21102, 21122, 21155, 21252,
        21258, 21270, 21327, 21337, 21360, 21368, 21398, 21419, 21438, 21481,
        21512, 21513, 21574, 21580, 21630, 21686, 21687, 21884, 22013, 22261,
        22365, 22374, 22399, 22431, 22442, 22510, 22546, 22682, 22856, 23043,
        23100, 23123, 23156, 23165, 23215, 23249, 23258, 23286, 23330, 23402,
        23420, 23495, 23516, 23580, 23584, 23644, 23677, 23753, 23794, 23796,
        23999, 24036, 24239, 24268, 24310, 24449, 24571, 24605, 24779, 24898,
        25140, 25243, 25276, 25342, 25445, 25487, 25488, 25794, 25803, 25819,
        25904, 25944, 26072, 26106, 26125, 26165, 26224, 26283, 26363, 26367,
        26373, 26456, 26459, 26469, 26477, 26516, 26535, 26561, 26616, 26635,
        26640, 26647, 26671, 26698, 26716, 26790, 26898, 26944, 26947, 27016,
        27017, 27039, 27040, 27041, 27062, 27154, 27176, 27185, 27201, 27213,
        27232, 27242, 27322, 27396, 27443, 27490, 27526, 27621, 27685, 27693,
        27719, 27806, 27846, 27849, 27851, 27857, 27879, 27986, 28038, 28070,
        28087, 28126, 28152, 28153, 28244, 28418, 28434, 28501, 28518, 28520,
        28529, 28533, 28544, 28593, 28617, 28639, 28703, 28794, 28805, 28819,
        28841, 28866, 28880, 28903, 28908, 28921, 28933, 28935, 28943, 28944,
        28979, 28989, 29016, 29037, 29046, 29067, 29206, 29220, 29239, 29252,
        29302, 29320, 29325, 29333, 29435, 29518, 29563, 29576, 29598, 29615,
        29630, 29660, 29662, 29676, 29737, 29745, 29752, 29772, 29778, 29841,
        29892, 29916, 29973, 30019, 30076, 30112, 30135, 30203, 30258, 30272,
        30278, 30387, 30568, 30595, 30615, 30684, 30712, 30724, 30778, 30866,
        31052, 31072, 31149, 31180, 31255, 31278, 31312, 31401, 31597, 31616,
        31629, 31653, 31689, 31719, 31738, 31739, 31753, 31809, 31831, 31879,
        31888, 31891, 31950, 31965, 31987, 31999, 32027, 32089, 32093, 32108,
        32109, 32129, 32134, 32135, 32155, 32168, 32169, 32170, 32172, 32173,
        32184, 32197, 32210, 32213, 32216, 32238, 32289, 32301, 32308, 32317,
        32349, 32402, 32404, 32428, 32429, 32434, 32435, 32439, 32444, 32452,
        32460, 32562, 32566, 32598, 32636, 32640, 32662, 32672, 32674, 32678,
        32680, 32698, 32751, 32752, 32755, 32801, 32958, 32992, 33037, 33168,
        33216, 33268, 33290, 33297, 33302, 33305, 33387, 33442, 33477, 33510,
        33664], device='cuda:0')
length of data loader train is: 169
num_training_steps_per_epoch is: 169
Change step level LR scheduler!
Set warmup steps = 845
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:11:51  loss: 0.5264 (0.5264)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6157  data: 2.4211  max mem: 15572
Val:  [ 10/272]  eta: 0:02:09  loss: 4.1329 (3.5688)  acc1: 0.0000 (23.2323)  acc5: 16.6667 (33.3333)  time: 0.4932  data: 0.3194  max mem: 15572
Val:  [ 20/272]  eta: 0:01:26  loss: 3.9215 (3.5362)  acc1: 11.1111 (22.7513)  acc5: 22.2222 (38.8889)  time: 0.2289  data: 0.0586  max mem: 15572
Val:  [ 30/272]  eta: 0:01:10  loss: 3.6300 (3.6331)  acc1: 11.1111 (17.7419)  acc5: 44.4444 (40.8602)  time: 0.1791  data: 0.0044  max mem: 15572
Val:  [ 40/272]  eta: 0:01:01  loss: 3.5420 (3.5808)  acc1: 11.1111 (16.8022)  acc5: 44.4444 (42.8184)  time: 0.1813  data: 0.0007  max mem: 15572
Val:  [ 50/272]  eta: 0:00:54  loss: 3.4401 (3.5048)  acc1: 11.1111 (18.8453)  acc5: 50.0000 (45.4248)  time: 0.1772  data: 0.0007  max mem: 15572
Val:  [ 60/272]  eta: 0:00:49  loss: 2.3098 (3.3202)  acc1: 50.0000 (24.9545)  acc5: 72.2222 (49.2714)  time: 0.1745  data: 0.0006  max mem: 15572
Val:  [ 70/272]  eta: 0:00:45  loss: 2.3844 (3.2063)  acc1: 50.0000 (26.2128)  acc5: 77.7778 (53.8341)  time: 0.1690  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:41  loss: 2.8298 (3.2160)  acc1: 22.2222 (26.2689)  acc5: 77.7778 (52.9492)  time: 0.1636  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:38  loss: 4.2423 (3.3416)  acc1: 0.0000 (23.5043)  acc5: 16.6667 (48.6569)  time: 0.1616  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:35  loss: 4.1133 (3.3985)  acc1: 0.0000 (22.6073)  acc5: 16.6667 (48.4598)  time: 0.1665  data: 0.0005  max mem: 15572
Val:  [110/272]  eta: 0:00:33  loss: 3.9531 (3.4709)  acc1: 0.0000 (20.7708)  acc5: 33.3333 (46.5966)  time: 0.1772  data: 0.0006  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 4.0871 (3.5181)  acc1: 0.0000 (19.2837)  acc5: 27.7778 (45.5005)  time: 0.1806  data: 0.0008  max mem: 15572
Val:  [130/272]  eta: 0:00:28  loss: 3.9289 (3.4670)  acc1: 5.5556 (21.3316)  acc5: 33.3333 (46.3953)  time: 0.1849  data: 0.0009  max mem: 15572
Val:  [140/272]  eta: 0:00:26  loss: 2.8746 (3.4543)  acc1: 22.2222 (22.1434)  acc5: 55.5556 (46.6115)  time: 0.1892  data: 0.0008  max mem: 15572
Val:  [150/272]  eta: 0:00:24  loss: 3.2920 (3.4580)  acc1: 5.5556 (21.2288)  acc5: 55.5556 (46.9831)  time: 0.1704  data: 0.0006  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.2025 (3.4265)  acc1: 22.2222 (22.8088)  acc5: 61.1111 (48.4127)  time: 0.1574  data: 0.0006  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.2025 (3.4548)  acc1: 22.2222 (21.8973)  acc5: 55.5556 (47.6933)  time: 0.1637  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.5353 (3.4506)  acc1: 11.1111 (22.3143)  acc5: 50.0000 (48.2505)  time: 0.1665  data: 0.0005  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.8087 (3.4767)  acc1: 5.5556 (21.3496)  acc5: 33.3333 (47.3240)  time: 0.1801  data: 0.0005  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.8087 (3.4681)  acc1: 0.0000 (21.6694)  acc5: 38.8889 (48.0929)  time: 0.1782  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.0720 (3.4706)  acc1: 27.7778 (22.0116)  acc5: 66.6667 (48.6835)  time: 0.1707  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.6017 (3.4666)  acc1: 16.6667 (22.1719)  acc5: 55.5556 (48.9442)  time: 0.1717  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 2.9777 (3.4263)  acc1: 44.4444 (23.8336)  acc5: 61.1111 (50.0962)  time: 0.1707  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:06  loss: 2.6246 (3.4027)  acc1: 44.4444 (24.2278)  acc5: 77.7778 (51.0834)  time: 0.1712  data: 0.0005  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.2325 (3.4195)  acc1: 22.2222 (23.9265)  acc5: 55.5556 (50.5976)  time: 0.1734  data: 0.0005  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.4293 (3.3382)  acc1: 66.6667 (26.4155)  acc5: 77.7778 (52.1711)  time: 0.1646  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.2617 (3.3378)  acc1: 55.5556 (26.2608)  acc5: 83.3333 (52.3370)  time: 0.1434  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.2617 (3.3422)  acc1: 55.5556 (26.2339)  acc5: 83.3333 (52.3039)  time: 0.1353  data: 0.0001  max mem: 15572
Val: Total time: 0:00:50 (0.1842 s / it)
* Acc@1 26.234 Acc@5 52.304 loss 3.342
Accuracy of the network on the 4883 val videos: 26.2%
[2025-01-13 13:24:20,905] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 13:24:20,906] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 13:24:20,906] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 13:24:23,500] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 13:24:23,500] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 26.23%
Epoch: [14]  [  0/169]  eta: 0:09:29  lr: 0.000040  min_lr: 0.000000  loss: 4.3526 (4.3526)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.3712  data: 2.9676  max mem: 15572
Epoch: [14]  [ 10/169]  eta: 0:01:46  lr: 0.000040  min_lr: 0.000000  loss: 4.2535 (4.1484)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6673  data: 0.2730  max mem: 15572
Epoch: [14]  [ 20/169]  eta: 0:01:20  lr: 0.000039  min_lr: 0.000000  loss: 4.1001 (4.1087)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3959  data: 0.0020  max mem: 15572
Epoch: [14]  [ 30/169]  eta: 0:01:08  lr: 0.000039  min_lr: 0.000000  loss: 4.1300 (4.1202)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3974  data: 0.0005  max mem: 15572
Epoch: [14]  [ 40/169]  eta: 0:01:00  lr: 0.000039  min_lr: 0.000000  loss: 4.2018 (4.1410)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3985  data: 0.0006  max mem: 15572
Epoch: [14]  [ 50/169]  eta: 0:00:54  lr: 0.000039  min_lr: 0.000000  loss: 4.2936 (4.1449)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3928  data: 0.0004  max mem: 15572
[2025-01-13 13:24:49,769] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:24:49,770] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [ 60/169]  eta: 0:00:48  lr: 0.000039  min_lr: 0.000000  loss: 4.1780 (4.1296)  loss_scale: 65536.0000 (68759.0820)  weight_decay: 0.0500 (0.0500)  time: 0.3881  data: 0.0003  max mem: 15572
Epoch: [14]  [ 70/169]  eta: 0:00:42  lr: 0.000039  min_lr: 0.000000  loss: 4.1780 (4.1277)  loss_scale: 131072.0000 (77535.5493)  weight_decay: 0.0500 (0.0500)  time: 0.3825  data: 0.0003  max mem: 15572
[2025-01-13 13:24:57,693] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21466
[2025-01-13 13:24:57,693] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:24:57,694] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 80/169]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000000  loss: 4.0778 (4.1186)  loss_scale: 131072.0000 (82526.8148)  weight_decay: 0.0500 (0.0500)  time: 0.3764  data: 0.0003  max mem: 15572
Epoch: [14]  [ 90/169]  eta: 0:00:33  lr: 0.000039  min_lr: 0.000000  loss: 4.1348 (4.1286)  loss_scale: 65536.0000 (80659.6923)  weight_decay: 0.0500 (0.0500)  time: 0.3735  data: 0.0002  max mem: 15572
Epoch: [14]  [100/169]  eta: 0:00:28  lr: 0.000039  min_lr: 0.000000  loss: 4.1526 (4.1208)  loss_scale: 65536.0000 (79162.2970)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [14]  [110/169]  eta: 0:00:24  lr: 0.000039  min_lr: 0.000000  loss: 4.1360 (4.1296)  loss_scale: 65536.0000 (77934.7027)  weight_decay: 0.0500 (0.0500)  time: 0.3762  data: 0.0002  max mem: 15572
Epoch: [14]  [120/169]  eta: 0:00:20  lr: 0.000039  min_lr: 0.000000  loss: 4.1360 (4.1239)  loss_scale: 65536.0000 (76910.0165)  weight_decay: 0.0500 (0.0500)  time: 0.3751  data: 0.0002  max mem: 15572
Epoch: [14]  [130/169]  eta: 0:00:15  lr: 0.000038  min_lr: 0.000000  loss: 3.9584 (4.1186)  loss_scale: 65536.0000 (76041.7710)  weight_decay: 0.0500 (0.0500)  time: 0.3755  data: 0.0002  max mem: 15572
Epoch: [14]  [140/169]  eta: 0:00:11  lr: 0.000038  min_lr: 0.000000  loss: 4.0812 (4.1289)  loss_scale: 65536.0000 (75296.6809)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.0002  max mem: 15572
Epoch: [14]  [150/169]  eta: 0:00:07  lr: 0.000038  min_lr: 0.000000  loss: 4.0812 (4.1214)  loss_scale: 65536.0000 (74650.2781)  weight_decay: 0.0500 (0.0500)  time: 0.3794  data: 0.0003  max mem: 15572
Epoch: [14]  [160/169]  eta: 0:00:03  lr: 0.000038  min_lr: 0.000000  loss: 4.1140 (4.1245)  loss_scale: 65536.0000 (74084.1739)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0003  max mem: 15572
Epoch: [14]  [168/169]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000000  loss: 4.1140 (4.1183)  loss_scale: 65536.0000 (73679.5266)  weight_decay: 0.0500 (0.0500)  time: 0.3797  data: 0.0003  max mem: 15572
Epoch: [14] Total time: 0:01:07 (0.4016 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000000  loss: 4.1140 (4.1183)  loss_scale: 65536.0000 (73679.5266)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 608
Indices to remove: tensor([   21,    81,   148,   149,   155,   164,   165,   237,   456,   550,
          843,   939,  1061,  1097,  1130,  1131,  1155,  1207,  1212,  1255,
         1284,  1436,  1461,  1940,  1966,  1999,  2007,  2037,  2087,  2199,
         2253,  2343,  2391,  2588,  2697,  2723,  2762,  2784,  2893,  2957,
         3010,  3150,  3168,  3242,  3252,  3286,  3300,  3376,  3471,  3607,
         3719,  3807,  3810,  3826,  3921,  4238,  4246,  4306,  4362,  4735,
         4754,  5145,  5184,  5211,  5466,  5517,  5537,  5606,  5616,  5621,
         5656,  5664,  5720,  5722,  5745,  5811,  5826,  5937,  5960,  5976,
         5982,  6050,  6089,  6139,  6189,  6220,  6226,  6266,  6285,  6288,
         6373,  6392,  6427,  6435,  6438,  6512,  6647,  6653,  6667,  6753,
         6759,  6829,  6861,  6939,  6978,  6993,  7288,  7302,  7306,  7358,
         7360,  7363,  7392,  7409,  7446,  7449,  7482,  7576,  7595,  7598,
         7619,  7624,  7627,  7641,  7670,  7676,  7692,  7699,  7701,  7715,
         7820,  7848,  7853,  7928,  8014,  8116,  8121,  8159,  8290,  8322,
         8358,  8585,  8665,  8727,  8735,  8744,  8745,  8830,  8835,  8838,
         8855,  8922,  9002,  9010,  9034,  9049,  9053,  9056,  9068,  9110,
         9121,  9137,  9146,  9160,  9206,  9257,  9357,  9367,  9398,  9417,
         9462,  9538,  9543,  9564,  9657,  9663,  9665,  9666,  9671,  9726,
         9781,  9834,  9843,  9882,  9890,  9898,  9923, 10140, 10372, 10390,
        10401, 10694, 10841, 11052, 11055, 11150, 11217, 11218, 11278, 11326,
        11349, 11399, 11444, 11519, 11574, 11580, 11600, 11612, 11665, 11699,
        11786, 11802, 11954, 11986, 12090, 12334, 12346, 12527, 12573, 12716,
        12730, 12832, 12940, 12959, 13104, 13111, 13154, 13206, 13281, 13320,
        13488, 13499, 13537, 13650, 13881, 13983, 14051, 14084, 14114, 14189,
        14196, 14253, 14298, 14315, 14324, 14350, 14354, 14367, 14411, 14425,
        14613, 14812, 14964, 15027, 15111, 15115, 15133, 15138, 15147, 15181,
        15194, 15197, 15266, 15439, 15740, 15748, 15754, 15785, 15820, 15826,
        15828, 15842, 15865, 15928, 16005, 16012, 16028, 16052, 16054, 16111,
        16144, 16211, 16373, 16604, 16684, 16720, 16721, 16762, 16769, 16845,
        16847, 16872, 16899, 16914, 16927, 16928, 16941, 16975, 17043, 17084,
        17135, 17163, 17185, 17194, 17224, 17330, 17361, 17365, 17386, 17419,
        17473, 17492, 17531, 17577, 17600, 17625, 17639, 17652, 17656, 17660,
        17669, 17686, 17738, 17744, 17777, 17805, 17816, 18013, 18107, 18144,
        18210, 18297, 18462, 18476, 18677, 18728, 18775, 18792, 18993, 19008,
        19049, 19057, 19169, 19316, 19416, 19676, 19745, 19769, 19779, 19847,
        20035, 20150, 20206, 20217, 20260, 20310, 20359, 20541, 20555, 20564,
        20604, 20623, 20624, 20634, 20654, 20656, 20672, 20696, 20724, 20730,
        20782, 20791, 20830, 20949, 20966, 21062, 21087, 21177, 21181, 21208,
        21213, 21227, 21252, 21295, 21386, 21419, 21437, 21438, 21512, 21513,
        21514, 21574, 21580, 21587, 21597, 21612, 21630, 21679, 21784, 21787,
        22027, 22322, 22377, 22431, 22831, 23095, 23114, 23240, 23249, 23270,
        23393, 23420, 23449, 23481, 23495, 23650, 23754, 23977, 24036, 24096,
        24168, 24177, 24186, 24197, 24239, 24240, 24241, 24317, 24345, 24449,
        24542, 25137, 25276, 25366, 25417, 25443, 25445, 25455, 25551, 25558,
        25589, 25803, 26066, 26106, 26114, 26125, 26165, 26210, 26228, 26373,
        26397, 26475, 26500, 26501, 26535, 26554, 26647, 26671, 26698, 26898,
        26908, 26943, 26944, 27016, 27070, 27083, 27143, 27154, 27176, 27201,
        27213, 27232, 27273, 27322, 27419, 27444, 27550, 27583, 27701, 27797,
        27851, 27907, 28073, 28087, 28104, 28112, 28435, 28501, 28518, 28529,
        28541, 28544, 28546, 28617, 28776, 28805, 28806, 28808, 28819, 28866,
        28903, 28921, 28943, 28944, 28993, 29016, 29028, 29037, 29067, 29163,
        29178, 29220, 29325, 29360, 29388, 29417, 29444, 29518, 29523, 29575,
        29660, 29668, 29682, 29737, 29772, 29778, 29816, 29841, 29935, 29975,
        29991, 30005, 30019, 30056, 30254, 30269, 30433, 30498, 30517, 30643,
        30684, 30728, 30860, 30950, 31072, 31180, 31232, 31289, 31312, 31321,
        31397, 31653, 31689, 31692, 31719, 31809, 31888, 31915, 31945, 31965,
        32089, 32093, 32108, 32129, 32134, 32155, 32167, 32169, 32172, 32173,
        32193, 32197, 32213, 32223, 32230, 32317, 32326, 32327, 32429, 32434,
        32444, 32452, 32460, 32470, 32487, 32551, 32557, 32562, 32566, 32598,
        32634, 32662, 32672, 32698, 32751, 32754, 32932, 33179, 33194, 33216,
        33268, 33302, 33305, 33367, 33387, 33411, 33475, 33564],
       device='cuda:0')
length of data loader train is: 119
num_training_steps_per_epoch is: 119
Change step level LR scheduler!
Set warmup steps = 595
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:14:49  loss: 0.5271 (0.5271)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.2701  data: 3.0570  max mem: 15572
Val:  [ 10/272]  eta: 0:02:06  loss: 3.8890 (3.4997)  acc1: 0.0000 (22.2222)  acc5: 22.2222 (34.8485)  time: 0.4845  data: 0.3032  max mem: 15572
Val:  [ 20/272]  eta: 0:01:25  loss: 3.8890 (3.5333)  acc1: 5.5556 (20.3704)  acc5: 27.7778 (39.9471)  time: 0.1908  data: 0.0143  max mem: 15572
Val:  [ 30/272]  eta: 0:01:09  loss: 3.6334 (3.6417)  acc1: 5.5556 (15.5914)  acc5: 44.4444 (40.8602)  time: 0.1817  data: 0.0008  max mem: 15572
Val:  [ 40/272]  eta: 0:01:00  loss: 3.6629 (3.6513)  acc1: 5.5556 (15.5827)  acc5: 38.8889 (41.7344)  time: 0.1840  data: 0.0007  max mem: 15572
Val:  [ 50/272]  eta: 0:00:54  loss: 3.6512 (3.5446)  acc1: 16.6667 (19.2810)  acc5: 44.4444 (44.9891)  time: 0.1771  data: 0.0005  max mem: 15572
Val:  [ 60/272]  eta: 0:00:49  loss: 2.2382 (3.3381)  acc1: 55.5556 (25.6831)  acc5: 72.2222 (48.9982)  time: 0.1687  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:44  loss: 2.2389 (3.2214)  acc1: 55.5556 (26.9953)  acc5: 77.7778 (53.1299)  time: 0.1602  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:41  loss: 2.7285 (3.2494)  acc1: 22.2222 (26.4746)  acc5: 72.2222 (52.2634)  time: 0.1680  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:38  loss: 4.1395 (3.3564)  acc1: 0.0000 (23.6874)  acc5: 22.2222 (48.2295)  time: 0.1668  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:35  loss: 4.0240 (3.4111)  acc1: 0.0000 (23.4323)  acc5: 22.2222 (48.1298)  time: 0.1579  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 3.8907 (3.4767)  acc1: 0.0000 (21.5215)  acc5: 44.4444 (46.8468)  time: 0.1666  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 4.0252 (3.5196)  acc1: 0.0000 (20.2020)  acc5: 27.7778 (45.7300)  time: 0.1633  data: 0.0003  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.8814 (3.4593)  acc1: 11.1111 (22.0526)  acc5: 33.3333 (46.7345)  time: 0.1559  data: 0.0003  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 3.0085 (3.4502)  acc1: 27.7778 (22.8526)  acc5: 55.5556 (46.9267)  time: 0.1572  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.3147 (3.4492)  acc1: 11.1111 (22.0751)  acc5: 61.1111 (47.2774)  time: 0.1647  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.2458 (3.4183)  acc1: 22.2222 (23.5680)  acc5: 66.6667 (48.6542)  time: 0.1674  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.3115 (3.4474)  acc1: 16.6667 (22.6121)  acc5: 61.1111 (47.9857)  time: 0.1671  data: 0.0005  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.3809 (3.4411)  acc1: 5.5556 (23.1430)  acc5: 55.5556 (48.6495)  time: 0.1677  data: 0.0005  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.9854 (3.4783)  acc1: 0.0000 (21.9604)  acc5: 33.3333 (47.7312)  time: 0.1783  data: 0.0093  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.9854 (3.4721)  acc1: 0.0000 (21.9182)  acc5: 38.8889 (48.2587)  time: 0.1810  data: 0.0121  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.8913 (3.4695)  acc1: 22.2222 (22.4065)  acc5: 72.2222 (48.9468)  time: 0.1658  data: 0.0032  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.3347 (3.4636)  acc1: 22.2222 (22.5239)  acc5: 61.1111 (49.3715)  time: 0.1631  data: 0.0004  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 2.9650 (3.4215)  acc1: 50.0000 (24.2905)  acc5: 66.6667 (50.5291)  time: 0.1714  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.6075 (3.3973)  acc1: 50.0000 (24.6657)  acc5: 83.3333 (51.5906)  time: 0.1686  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.2157 (3.4118)  acc1: 22.2222 (24.3471)  acc5: 55.5556 (51.1510)  time: 0.1607  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.3745 (3.3316)  acc1: 72.2222 (26.7986)  acc5: 77.7778 (52.7246)  time: 0.1582  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.3812 (3.3304)  acc1: 55.5556 (26.4658)  acc5: 83.3333 (53.0135)  time: 0.1439  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.3812 (3.3341)  acc1: 44.4444 (26.4387)  acc5: 83.3333 (53.0002)  time: 0.1369  data: 0.0001  max mem: 15572
Val: Total time: 0:00:48 (0.1792 s / it)
* Acc@1 26.439 Acc@5 53.000 loss 3.334
Accuracy of the network on the 4883 val videos: 26.4%
[2025-01-13 13:26:20,194] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 13:26:20,196] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 13:26:20,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 13:26:22,586] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 13:26:22,587] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 26.44%
Epoch: [15]  [  0/119]  eta: 0:10:33  lr: 0.000038  min_lr: 0.000000  loss: 3.6531 (3.6531)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 5.3208  data: 4.9148  max mem: 15572
Epoch: [15]  [ 10/119]  eta: 0:01:31  lr: 0.000038  min_lr: 0.000000  loss: 4.0669 (4.1042)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8412  data: 0.4472  max mem: 15572
Epoch: [15]  [ 20/119]  eta: 0:01:02  lr: 0.000038  min_lr: 0.000000  loss: 4.0827 (4.1206)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3925  data: 0.0004  max mem: 15572
Epoch: [15]  [ 30/119]  eta: 0:00:48  lr: 0.000038  min_lr: 0.000000  loss: 4.2127 (4.1687)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3903  data: 0.0004  max mem: 15572
[2025-01-13 13:26:43,030] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:26:43,030] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [ 40/119]  eta: 0:00:40  lr: 0.000038  min_lr: 0.000000  loss: 4.2127 (4.1478)  loss_scale: 65536.0000 (68732.8780)  weight_decay: 0.0500 (0.0500)  time: 0.3820  data: 0.0004  max mem: 15572
[2025-01-13 13:26:44,518] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21599
[2025-01-13 13:26:44,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:26:44,518] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 50/119]  eta: 0:00:33  lr: 0.000037  min_lr: 0.000000  loss: 4.2068 (4.1220)  loss_scale: 65536.0000 (70676.0784)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0003  max mem: 15572
Epoch: [15]  [ 60/119]  eta: 0:00:27  lr: 0.000037  min_lr: 0.000000  loss: 4.0802 (4.0889)  loss_scale: 65536.0000 (69833.4426)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0002  max mem: 15572
Epoch: [15]  [ 70/119]  eta: 0:00:22  lr: 0.000037  min_lr: 0.000000  loss: 4.1212 (4.1167)  loss_scale: 65536.0000 (69228.1690)  weight_decay: 0.0500 (0.0500)  time: 0.3793  data: 0.0003  max mem: 15572
Epoch: [15]  [ 80/119]  eta: 0:00:17  lr: 0.000037  min_lr: 0.000000  loss: 4.3004 (4.1346)  loss_scale: 65536.0000 (68772.3457)  weight_decay: 0.0500 (0.0500)  time: 0.3757  data: 0.0002  max mem: 15572
Epoch: [15]  [ 90/119]  eta: 0:00:12  lr: 0.000037  min_lr: 0.000000  loss: 4.2874 (4.1498)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0002  max mem: 15572
Epoch: [15]  [100/119]  eta: 0:00:08  lr: 0.000037  min_lr: 0.000000  loss: 4.1737 (4.1494)  loss_scale: 65536.0000 (68131.4851)  weight_decay: 0.0500 (0.0500)  time: 0.3772  data: 0.0002  max mem: 15572
Epoch: [15]  [110/119]  eta: 0:00:03  lr: 0.000036  min_lr: 0.000000  loss: 4.1323 (4.1409)  loss_scale: 65536.0000 (67897.6577)  weight_decay: 0.0500 (0.0500)  time: 0.3709  data: 0.0002  max mem: 15572
Epoch: [15]  [118/119]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000000  loss: 4.2308 (4.1457)  loss_scale: 65536.0000 (67738.8908)  weight_decay: 0.0500 (0.0500)  time: 0.3650  data: 0.0001  max mem: 15572
Epoch: [15] Total time: 0:00:50 (0.4212 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000000  loss: 4.2308 (4.1457)  loss_scale: 65536.0000 (67738.8908)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 433
Indices to remove: tensor([   14,    64,    75,    81,   148,   155,   165,   230,   231,   415,
          769,   906,  1061,  1097,  1130,  1207,  1255,  1327,  1436,  1659,
         1678,  1734,  1793,  1916,  1940,  1966,  1999,  2138,  2199,  2253,
         2391,  2431,  2460,  2762,  2908,  3042,  3043,  3109,  3134,  3165,
         3242,  3376,  3529,  3607,  3814,  3815,  3817,  3856,  3921,  4062,
         4081,  4545,  4644,  4971,  4984,  5139,  5164,  5379,  5517,  5537,
         5589,  5606,  5656,  5690,  5720,  5733,  5745,  5762,  5788,  5976,
         6027,  6052,  6076,  6101,  6130,  6139,  6172,  6229,  6258,  6266,
         6271,  6299,  6361,  6382,  6389,  6427,  6437,  6471,  6495,  6616,
         6653,  6737,  6749,  6753,  6829,  6841,  6870,  7198,  7288,  7302,
         7360,  7365,  7367,  7392,  7415,  7449,  7482,  7493,  7545,  7576,
         7595,  7666,  7705,  7715,  7808,  7821,  7836,  7974,  7990,  8014,
         8135,  8246,  8303,  8440,  8592,  8594,  8665,  8727,  8735,  8745,
         8830,  8906,  9041,  9049,  9056,  9068,  9087,  9146,  9160,  9164,
         9206,  9257,  9311,  9339,  9398,  9480,  9498,  9543,  9666,  9693,
         9713,  9726,  9772,  9777,  9834,  9843,  9890,  9923, 10565, 10694,
        11217, 11225, 11349, 11497, 11574, 11580, 11632, 11647, 11656, 11735,
        11828, 11901, 11935, 12387, 12731, 12832, 13224, 13499, 13726, 13833,
        13859, 13883, 13983, 14137, 14161, 14245, 14253, 14323, 14367, 14523,
        15016, 15138, 15748, 15785, 15820, 15828, 15928, 16012, 16052, 16144,
        16216, 16275, 16287, 16323, 16325, 16604, 16699, 16720, 16762, 16770,
        16847, 16872, 16899, 16928, 16950, 16975, 17043, 17084, 17185, 17194,
        17213, 17224, 17358, 17360, 17361, 17419, 17492, 17497, 17522, 17532,
        17575, 17600, 17639, 17652, 17744, 17805, 17816, 17941, 18013, 18079,
        18182, 18560, 18673, 18677, 18732, 18734, 18775, 19008, 19049, 19112,
        19125, 19169, 19288, 19316, 19769, 19942, 20139, 20178, 20201, 20214,
        20257, 20275, 20359, 20463, 20672, 20740, 20812, 20851, 20949, 20966,
        21087, 21213, 21252, 21270, 21368, 21419, 21468, 21497, 21500, 21511,
        21512, 21587, 21592, 21607, 21612, 21738, 21765, 22322, 22339, 22348,
        22510, 22609, 22789, 23114, 23270, 23330, 23393, 23402, 23481, 23584,
        23622, 23644, 23796, 23838, 24172, 24230, 24239, 24241, 24257, 24396,
        24449, 24817, 25366, 25391, 25417, 25488, 25629, 25781, 25945, 25957,
        25983, 26066, 26165, 26363, 26789, 26898, 26908, 27040, 27051, 27083,
        27176, 27185, 27201, 27273, 27342, 27873, 27907, 28226, 28374, 28418,
        28501, 28504, 28776, 28794, 28808, 28844, 28851, 28887, 28905, 28921,
        28944, 28976, 28993, 29016, 29028, 29037, 29147, 29320, 29423, 29435,
        29444, 29486, 29518, 29563, 29586, 29603, 29662, 29668, 29676, 29682,
        29745, 29772, 29778, 29906, 29960, 29973, 30012, 30056, 30170, 30213,
        30583, 30599, 30684, 30860, 30862, 30986, 31073, 31078, 31180, 31208,
        31289, 31629, 31719, 31809, 31888, 31909, 31915, 32093, 32108, 32134,
        32169, 32193, 32201, 32223, 32327, 32336, 32349, 32402, 32429, 32439,
        32444, 32452, 32460, 32470, 32562, 32566, 32592, 32598, 32634, 32640,
        32672, 32698, 32754, 32958, 33168, 33194, 33216, 33297, 33305, 33387,
        33437, 33448, 33705], device='cuda:0')
length of data loader train is: 82
num_training_steps_per_epoch is: 82
Change step level LR scheduler!
Set warmup steps = 410
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:16:09  loss: 0.6232 (0.6232)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.5654  data: 3.4078  max mem: 15572
Val:  [ 10/272]  eta: 0:02:03  loss: 3.9698 (3.5377)  acc1: 0.0000 (22.2222)  acc5: 22.2222 (35.3535)  time: 0.4698  data: 0.3101  max mem: 15572
Val:  [ 20/272]  eta: 0:01:22  loss: 3.9476 (3.5890)  acc1: 5.5556 (20.6349)  acc5: 22.2222 (38.0952)  time: 0.1641  data: 0.0004  max mem: 15572
Val:  [ 30/272]  eta: 0:01:07  loss: 3.7386 (3.6575)  acc1: 5.5556 (16.1290)  acc5: 44.4444 (40.3226)  time: 0.1744  data: 0.0005  max mem: 15572
Val:  [ 40/272]  eta: 0:00:59  loss: 3.4830 (3.5850)  acc1: 11.1111 (16.1247)  acc5: 44.4444 (43.0894)  time: 0.1817  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:53  loss: 3.3700 (3.5001)  acc1: 11.1111 (18.5185)  acc5: 50.0000 (46.0784)  time: 0.1797  data: 0.0005  max mem: 15572
Val:  [ 60/272]  eta: 0:00:48  loss: 2.2720 (3.3073)  acc1: 44.4444 (24.9545)  acc5: 72.2222 (49.4536)  time: 0.1726  data: 0.0025  max mem: 15572
Val:  [ 70/272]  eta: 0:00:44  loss: 2.3232 (3.2050)  acc1: 50.0000 (26.0563)  acc5: 77.7778 (53.4429)  time: 0.1690  data: 0.0025  max mem: 15572
Val:  [ 80/272]  eta: 0:00:41  loss: 2.8006 (3.2128)  acc1: 22.2222 (26.6118)  acc5: 77.7778 (52.5377)  time: 0.1730  data: 0.0029  max mem: 15572
Val:  [ 90/272]  eta: 0:00:38  loss: 4.2196 (3.3255)  acc1: 0.0000 (23.8706)  acc5: 22.2222 (48.4127)  time: 0.1723  data: 0.0030  max mem: 15572
Val:  [100/272]  eta: 0:00:35  loss: 4.0241 (3.3818)  acc1: 5.5556 (23.1023)  acc5: 22.2222 (48.3498)  time: 0.1707  data: 0.0006  max mem: 15572
Val:  [110/272]  eta: 0:00:33  loss: 3.9397 (3.4475)  acc1: 5.5556 (21.2212)  acc5: 38.8889 (47.2973)  time: 0.1819  data: 0.0008  max mem: 15572
Val:  [120/272]  eta: 0:00:30  loss: 3.9986 (3.4894)  acc1: 0.0000 (19.9265)  acc5: 27.7778 (46.4187)  time: 0.1752  data: 0.0007  max mem: 15572
Val:  [130/272]  eta: 0:00:28  loss: 3.9408 (3.4441)  acc1: 5.5556 (21.7557)  acc5: 33.3333 (47.2434)  time: 0.1592  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:26  loss: 3.0012 (3.4363)  acc1: 22.2222 (22.4192)  acc5: 55.5556 (47.2419)  time: 0.1741  data: 0.0005  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.3866 (3.4458)  acc1: 5.5556 (21.4496)  acc5: 44.4444 (47.0935)  time: 0.1868  data: 0.0005  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.3606 (3.4169)  acc1: 11.1111 (23.0159)  acc5: 50.0000 (48.4472)  time: 0.1737  data: 0.0006  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.3753 (3.4463)  acc1: 11.1111 (22.0923)  acc5: 55.5556 (47.8233)  time: 0.1711  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.4386 (3.4398)  acc1: 5.5556 (22.5599)  acc5: 55.5556 (48.5267)  time: 0.1759  data: 0.0005  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.7580 (3.4648)  acc1: 0.0000 (21.4951)  acc5: 38.8889 (47.8185)  time: 0.1710  data: 0.0004  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.7580 (3.4574)  acc1: 5.5556 (21.5589)  acc5: 38.8889 (48.5904)  time: 0.1582  data: 0.0003  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.0114 (3.4574)  acc1: 22.2222 (22.0642)  acc5: 66.6667 (49.2101)  time: 0.1610  data: 0.0004  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.4154 (3.4536)  acc1: 22.2222 (22.1971)  acc5: 61.1111 (49.6983)  time: 0.1745  data: 0.0005  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.0220 (3.4121)  acc1: 44.4444 (24.0260)  acc5: 61.1111 (50.8899)  time: 0.1728  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.5337 (3.3841)  acc1: 50.0000 (24.5274)  acc5: 83.3333 (52.0516)  time: 0.1662  data: 0.0005  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.0909 (3.4014)  acc1: 16.6667 (24.1479)  acc5: 66.6667 (51.5715)  time: 0.1636  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.5360 (3.3220)  acc1: 55.5556 (26.6284)  acc5: 77.7778 (53.1290)  time: 0.1597  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.3106 (3.3234)  acc1: 55.5556 (26.4453)  acc5: 77.7778 (53.2390)  time: 0.1441  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.3106 (3.3283)  acc1: 55.5556 (26.4182)  acc5: 77.7778 (53.2050)  time: 0.1371  data: 0.0001  max mem: 15572
Val: Total time: 0:00:49 (0.1821 s / it)
* Acc@1 26.418 Acc@5 53.205 loss 3.328
Accuracy of the network on the 4883 val videos: 26.4%
Max accuracy: 26.44%
Epoch: [16]  [ 0/82]  eta: 0:04:58  lr: 0.000036  min_lr: 0.000000  loss: 4.2857 (4.2857)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.6375  data: 3.2321  max mem: 15572
Epoch: [16]  [10/82]  eta: 0:00:50  lr: 0.000036  min_lr: 0.000000  loss: 4.1324 (3.9254)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6976  data: 0.3165  max mem: 15572
Epoch: [16]  [20/82]  eta: 0:00:33  lr: 0.000036  min_lr: 0.000000  loss: 3.9276 (4.0252)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3929  data: 0.0126  max mem: 15572
Epoch: [16]  [30/82]  eta: 0:00:25  lr: 0.000036  min_lr: 0.000000  loss: 4.0272 (4.0653)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3786  data: 0.0003  max mem: 15572
Epoch: [16]  [40/82]  eta: 0:00:19  lr: 0.000035  min_lr: 0.000000  loss: 4.2301 (4.1113)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3726  data: 0.0002  max mem: 15572
Epoch: [16]  [50/82]  eta: 0:00:14  lr: 0.000035  min_lr: 0.000000  loss: 4.1948 (4.1014)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3835  data: 0.0004  max mem: 15572
[2025-01-13 13:28:26,379] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:28:26,379] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 13:28:27,947] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21732
[2025-01-13 13:28:27,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:28:27,947] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [60/82]  eta: 0:00:09  lr: 0.000035  min_lr: 0.000000  loss: 4.3136 (4.1308)  loss_scale: 65536.0000 (69833.4426)  weight_decay: 0.0500 (0.0500)  time: 0.3956  data: 0.0005  max mem: 15572
Epoch: [16]  [70/82]  eta: 0:00:05  lr: 0.000035  min_lr: 0.000000  loss: 4.0535 (4.1108)  loss_scale: 65536.0000 (69228.1690)  weight_decay: 0.0500 (0.0500)  time: 0.3880  data: 0.0003  max mem: 15572
Epoch: [16]  [80/82]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000000  loss: 4.0406 (4.1158)  loss_scale: 65536.0000 (68772.3457)  weight_decay: 0.0500 (0.0500)  time: 0.3777  data: 0.0002  max mem: 15572
Epoch: [16]  [81/82]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000000  loss: 4.0535 (4.1207)  loss_scale: 65536.0000 (68732.8780)  weight_decay: 0.0500 (0.0500)  time: 0.3771  data: 0.0002  max mem: 15572
Epoch: [16] Total time: 0:00:34 (0.4262 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000000  loss: 4.0535 (4.1207)  loss_scale: 65536.0000 (68732.8780)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 297
Indices to remove: tensor([   36,    75,   112,   126,   148,   155,   165,   230,   274,   733,
          906,   939,  1207,  1255,  1327,  1916,  1966,  1999,  2087,  2459,
         2908,  3042,  3607,  3720,  3810,  3815,  3826,  4325,  4644,  4721,
         5025,  5364,  5606,  5621,  5656,  5690,  5788,  5811,  6050,  6052,
         6089,  6102,  6139,  6226,  6258,  6324,  6471,  6637,  6737,  6753,
         6841,  7302,  7449,  7493,  7545,  7576,  7595,  7624,  7666,  7686,
         7809,  7869,  7928,  8003,  8053,  8303,  8358,  8496,  8592,  8638,
         8735,  8745,  8830,  9002,  9034,  9049,  9053,  9146,  9160,  9211,
         9249,  9257,  9311,  9339,  9398,  9480,  9498,  9665,  9726,  9769,
         9777,  9843, 10724, 10866, 11150, 11225, 11301, 11497, 11574, 11632,
        11647, 11656, 11735, 12011, 12573, 12940, 12990, 13068, 13077, 13087,
        13110, 13237, 13281, 13883, 14012, 14137, 14161, 14411, 14523, 14812,
        14960, 15133, 15194, 15237, 15271, 15439, 16012, 16028, 16075, 16275,
        16287, 16323, 16604, 16847, 16872, 16878, 16899, 16914, 16950, 16975,
        17084, 17365, 17419, 17531, 17575, 17577, 17639, 17652, 17656, 17660,
        17744, 17805, 18182, 18700, 18792, 18912, 19032, 19125, 19169, 19256,
        19288, 19676, 19705, 19759, 19769, 19779, 19879, 20275, 20404, 20463,
        20623, 20696, 20763, 21062, 21252, 21419, 21438, 21478, 21512, 21514,
        21563, 21597, 21607, 21784, 22064, 22199, 22340, 22365, 22510, 22676,
        23043, 23054, 23165, 23393, 23474, 23650, 23753, 23796, 23984, 23988,
        24096, 24177, 24239, 24496, 24882, 25199, 25783, 25826, 25938, 25957,
        26210, 26228, 26304, 26363, 26397, 26486, 26679, 26898, 26908, 26944,
        26947, 27017, 27041, 27051, 27083, 27201, 27213, 27273, 27336, 27387,
        27419, 28087, 28100, 28529, 28593, 28639, 28794, 28806, 28808, 28844,
        28851, 28866, 28903, 28905, 28993, 29252, 29423, 29444, 29518, 29586,
        29603, 29662, 29668, 29739, 29772, 29778, 29960, 29964, 30107, 30207,
        30213, 30272, 30387, 30477, 31052, 31059, 31073, 31232, 31294, 31360,
        31629, 31692, 31719, 31909, 31915, 32089, 32108, 32134, 32169, 32223,
        32230, 32327, 32336, 32452, 32460, 32470, 32487, 32566, 32582, 32592,
        32634, 32672, 32698, 33216, 33263, 33297, 33475], device='cuda:0')
length of data loader train is: 58
num_training_steps_per_epoch is: 58
Change step level LR scheduler!
Set warmup steps = 290
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:10:45  loss: 0.5943 (0.5943)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3728  data: 2.2115  max mem: 15572
Val:  [ 10/272]  eta: 0:02:01  loss: 4.1074 (3.5616)  acc1: 0.0000 (21.2121)  acc5: 16.6667 (33.8384)  time: 0.4623  data: 0.3110  max mem: 15572
Val:  [ 20/272]  eta: 0:01:18  loss: 3.9589 (3.5969)  acc1: 5.5556 (19.8413)  acc5: 16.6667 (37.8307)  time: 0.2098  data: 0.0607  max mem: 15572
Val:  [ 30/272]  eta: 0:01:03  loss: 3.7084 (3.6695)  acc1: 5.5556 (15.2330)  acc5: 50.0000 (40.5018)  time: 0.1544  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:55  loss: 3.6003 (3.6397)  acc1: 5.5556 (14.3631)  acc5: 44.4444 (42.0054)  time: 0.1599  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:49  loss: 3.5794 (3.5381)  acc1: 11.1111 (17.5381)  acc5: 44.4444 (44.5534)  time: 0.1618  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:45  loss: 2.1567 (3.3409)  acc1: 44.4444 (24.2259)  acc5: 72.2222 (48.3607)  time: 0.1700  data: 0.0063  max mem: 15572
Val:  [ 70/272]  eta: 0:00:42  loss: 2.3453 (3.2264)  acc1: 55.5556 (25.5869)  acc5: 77.7778 (52.8169)  time: 0.1769  data: 0.0139  max mem: 15572
Val:  [ 80/272]  eta: 0:00:39  loss: 2.7728 (3.2439)  acc1: 27.7778 (25.3772)  acc5: 77.7778 (52.1262)  time: 0.1677  data: 0.0081  max mem: 15572
Val:  [ 90/272]  eta: 0:00:36  loss: 4.1153 (3.3520)  acc1: 0.0000 (22.8327)  acc5: 16.6667 (48.1074)  time: 0.1657  data: 0.0083  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 3.9960 (3.4123)  acc1: 5.5556 (22.4422)  acc5: 27.7778 (47.6348)  time: 0.1778  data: 0.0154  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 3.9566 (3.4800)  acc1: 0.0000 (20.7207)  acc5: 38.8889 (46.0961)  time: 0.1776  data: 0.0075  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 4.0129 (3.5221)  acc1: 0.0000 (19.4215)  acc5: 27.7778 (45.2709)  time: 0.1705  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.9576 (3.4757)  acc1: 5.5556 (21.1620)  acc5: 33.3333 (46.1832)  time: 0.1628  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:24  loss: 3.1081 (3.4635)  acc1: 27.7778 (21.9464)  acc5: 50.0000 (46.0599)  time: 0.1608  data: 0.0005  max mem: 15572
Val:  [150/272]  eta: 0:00:22  loss: 3.3321 (3.4631)  acc1: 5.5556 (21.0817)  acc5: 50.0000 (46.4312)  time: 0.1647  data: 0.0005  max mem: 15572
Val:  [160/272]  eta: 0:00:20  loss: 3.2165 (3.4293)  acc1: 16.6667 (22.5673)  acc5: 55.5556 (48.0331)  time: 0.1656  data: 0.0005  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.2522 (3.4558)  acc1: 11.1111 (21.7349)  acc5: 61.1111 (47.4984)  time: 0.1747  data: 0.0092  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.3477 (3.4493)  acc1: 5.5556 (22.2529)  acc5: 55.5556 (48.2198)  time: 0.1931  data: 0.0239  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.9023 (3.4826)  acc1: 0.0000 (21.2042)  acc5: 33.3333 (47.2659)  time: 0.1806  data: 0.0152  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.9023 (3.4762)  acc1: 5.5556 (21.2825)  acc5: 33.3333 (47.9547)  time: 0.1576  data: 0.0004  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.9718 (3.4739)  acc1: 27.7778 (21.9326)  acc5: 66.6667 (48.5519)  time: 0.1864  data: 0.0007  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.3366 (3.4658)  acc1: 27.7778 (22.1719)  acc5: 55.5556 (49.0950)  time: 0.2105  data: 0.0010  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 2.8835 (3.4210)  acc1: 38.8889 (24.0741)  acc5: 72.2222 (50.3367)  time: 0.1970  data: 0.0008  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.6722 (3.3962)  acc1: 50.0000 (24.3430)  acc5: 83.3333 (51.4062)  time: 0.1856  data: 0.0007  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.0956 (3.4143)  acc1: 11.1111 (23.9265)  acc5: 61.1111 (50.8411)  time: 0.1854  data: 0.0007  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.4978 (3.3347)  acc1: 55.5556 (26.3729)  acc5: 77.7778 (52.4053)  time: 0.1699  data: 0.0005  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.3337 (3.3331)  acc1: 61.1111 (26.2198)  acc5: 83.3333 (52.6445)  time: 0.1450  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.3337 (3.3370)  acc1: 55.5556 (26.1929)  acc5: 83.3333 (52.6111)  time: 0.1379  data: 0.0002  max mem: 15572
Val: Total time: 0:00:50 (0.1839 s / it)
* Acc@1 26.193 Acc@5 52.611 loss 3.337
Accuracy of the network on the 4883 val videos: 26.2%
Max accuracy: 26.44%
Epoch: [17]  [ 0/58]  eta: 0:04:14  lr: 0.000035  min_lr: 0.000000  loss: 3.8273 (3.8273)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.3902  data: 3.9284  max mem: 15572
Epoch: [17]  [10/58]  eta: 0:00:41  lr: 0.000034  min_lr: 0.000000  loss: 4.1375 (4.1652)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8676  data: 0.4481  max mem: 15572
Epoch: [17]  [20/58]  eta: 0:00:24  lr: 0.000034  min_lr: 0.000000  loss: 3.9280 (4.0562)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4460  data: 0.0502  max mem: 15572
Epoch: [17]  [30/58]  eta: 0:00:15  lr: 0.000034  min_lr: 0.000000  loss: 3.9995 (4.1034)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3786  data: 0.0003  max mem: 15572
Epoch: [17]  [40/58]  eta: 0:00:09  lr: 0.000033  min_lr: 0.000000  loss: 3.9956 (4.0408)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3803  data: 0.0002  max mem: 15572
Epoch: [17]  [50/58]  eta: 0:00:03  lr: 0.000033  min_lr: 0.000000  loss: 4.0940 (4.0850)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3720  data: 0.0001  max mem: 15572
Epoch: [17]  [57/58]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000000  loss: 4.1781 (4.0987)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3668  data: 0.0001  max mem: 15572
Epoch: [17] Total time: 0:00:27 (0.4700 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000000  loss: 4.1781 (4.0987)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 216
Indices to remove: tensor([   36,   112,   126,   148,   165,   230,  1207,  1278,  1595,  1659,
         1916,  1966,  2908,  3159,  3242,  3264,  3270,  3471,  3607,  3841,
         3856,  4216,  4778,  4984,  5379,  5589,  5606,  5733,  5906,  5976,
         6050,  6089,  6130,  6139,  6220,  6229,  6266,  6324,  6361,  6495,
         6609,  6616,  6637,  6829,  6887,  7198,  7262,  7302,  7361,  7446,
         7493,  7545,  7576,  7624,  7666,  7808,  7853,  7871,  7974,  8496,
         8505,  8592,  8665,  8906,  9002,  9087,  9146,  9160,  9480,  9498,
         9538,  9665,  9843,  9882, 10117, 10223, 10815, 11349, 12374, 12832,
        12858, 12990, 13281, 13320, 13537, 13883, 14012, 14064, 14367, 14396,
        14622, 15111, 15115, 15266, 15820, 15928, 16075, 16144, 16928, 16941,
        16975, 17194, 17213, 17358, 17361, 17365, 17492, 17497, 17531, 17532,
        17577, 17816, 18035, 18249, 18487, 18677, 19049, 19053, 19110, 19112,
        19169, 19377, 19676, 19769, 19779, 19919, 19967, 20071, 20217, 20332,
        20919, 20949, 20966, 21062, 21200, 21208, 21213, 21258, 21270, 21478,
        21500, 21580, 21734, 21837, 21977, 23043, 23052, 23516, 23988, 24135,
        24317, 24396, 25366, 25391, 25691, 25764, 25957, 25975, 26113, 26135,
        26165, 26363, 26459, 26663, 26685, 26908, 27213, 27322, 27419, 27429,
        27541, 28087, 28439, 28501, 28518, 28794, 28805, 28806, 28808, 28844,
        28944, 29042, 29302, 29325, 29662, 29772, 29778, 29781, 30097, 30213,
        30477, 30599, 30600, 30643, 30879, 31078, 31180, 31666, 31692, 31909,
        32089, 32108, 32169, 32182, 32193, 32429, 32470, 32487, 32582, 32634,
        33263, 33277, 33297, 33448, 33477, 33705], device='cuda:0')
length of data loader train is: 40
num_training_steps_per_epoch is: 40
Change step level LR scheduler!
Set warmup steps = 200
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:10:37  loss: 0.5957 (0.5957)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3455  data: 2.1887  max mem: 15572
Val:  [ 10/272]  eta: 0:01:44  loss: 4.3016 (3.6692)  acc1: 0.0000 (18.1818)  acc5: 27.7778 (34.3434)  time: 0.3994  data: 0.2434  max mem: 15572
Val:  [ 20/272]  eta: 0:01:13  loss: 3.8092 (3.6312)  acc1: 11.1111 (17.7249)  acc5: 27.7778 (39.4180)  time: 0.1900  data: 0.0357  max mem: 15572
Val:  [ 30/272]  eta: 0:01:00  loss: 3.6629 (3.7081)  acc1: 11.1111 (14.5161)  acc5: 38.8889 (40.3226)  time: 0.1650  data: 0.0114  max mem: 15572
Val:  [ 40/272]  eta: 0:00:52  loss: 3.5514 (3.6377)  acc1: 11.1111 (14.2276)  acc5: 44.4444 (42.5474)  time: 0.1578  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:47  loss: 3.4434 (3.5124)  acc1: 11.1111 (17.8649)  acc5: 50.0000 (46.5142)  time: 0.1603  data: 0.0005  max mem: 15572
Val:  [ 60/272]  eta: 0:00:45  loss: 2.1956 (3.3164)  acc1: 44.4444 (24.3169)  acc5: 72.2222 (49.9089)  time: 0.1924  data: 0.0254  max mem: 15572
Val:  [ 70/272]  eta: 0:00:42  loss: 2.3012 (3.2008)  acc1: 50.0000 (25.6651)  acc5: 77.7778 (54.1471)  time: 0.1933  data: 0.0254  max mem: 15572
Val:  [ 80/272]  eta: 0:00:39  loss: 2.7643 (3.2259)  acc1: 22.2222 (25.2401)  acc5: 77.7778 (53.2236)  time: 0.1750  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:36  loss: 4.0782 (3.3380)  acc1: 0.0000 (22.6496)  acc5: 22.2222 (49.3895)  time: 0.1844  data: 0.0006  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 4.0713 (3.4023)  acc1: 5.5556 (22.1672)  acc5: 22.2222 (48.4598)  time: 0.1771  data: 0.0007  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 4.0428 (3.4685)  acc1: 5.5556 (20.8208)  acc5: 33.3333 (47.2973)  time: 0.1814  data: 0.0007  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 4.0428 (3.5088)  acc1: 0.0000 (19.7888)  acc5: 33.3333 (46.1433)  time: 0.1830  data: 0.0005  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.9211 (3.4632)  acc1: 11.1111 (21.5437)  acc5: 33.3333 (47.0314)  time: 0.1767  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 3.1634 (3.4553)  acc1: 22.2222 (22.2222)  acc5: 55.5556 (46.8085)  time: 0.1798  data: 0.0006  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.4510 (3.4554)  acc1: 11.1111 (21.6336)  acc5: 50.0000 (47.1670)  time: 0.1782  data: 0.0007  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.3205 (3.4274)  acc1: 22.2222 (22.6708)  acc5: 55.5556 (48.5507)  time: 0.1690  data: 0.0006  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.4677 (3.4561)  acc1: 11.1111 (21.7349)  acc5: 50.0000 (47.7908)  time: 0.1713  data: 0.0005  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.6188 (3.4494)  acc1: 0.0000 (22.0688)  acc5: 44.4444 (48.4960)  time: 0.1804  data: 0.0006  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.9487 (3.4850)  acc1: 0.0000 (20.9715)  acc5: 33.3333 (47.4985)  time: 0.1698  data: 0.0005  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.9487 (3.4707)  acc1: 0.0000 (21.1719)  acc5: 38.8889 (48.5075)  time: 0.1651  data: 0.0003  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 3.0135 (3.4754)  acc1: 22.2222 (21.6430)  acc5: 66.6667 (48.9731)  time: 0.1816  data: 0.0005  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.2589 (3.4734)  acc1: 33.3333 (21.8451)  acc5: 66.6667 (49.2459)  time: 0.1786  data: 0.0006  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.0393 (3.4348)  acc1: 38.8889 (23.5690)  acc5: 72.2222 (50.4329)  time: 0.1683  data: 0.0006  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.6443 (3.4086)  acc1: 55.5556 (24.1125)  acc5: 83.3333 (51.4984)  time: 0.1696  data: 0.0005  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.0707 (3.4264)  acc1: 22.2222 (23.5945)  acc5: 66.6667 (50.9517)  time: 0.1670  data: 0.0005  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.6233 (3.3501)  acc1: 44.4444 (25.9259)  acc5: 77.7778 (52.4479)  time: 0.1571  data: 0.0004  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.4614 (3.3544)  acc1: 50.0000 (25.5638)  acc5: 77.7778 (52.4805)  time: 0.1458  data: 0.0003  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.4614 (3.3589)  acc1: 38.8889 (25.5376)  acc5: 77.7778 (52.4473)  time: 0.1397  data: 0.0002  max mem: 15572
Val: Total time: 0:00:49 (0.1820 s / it)
* Acc@1 25.538 Acc@5 52.447 loss 3.359
Accuracy of the network on the 4883 val videos: 25.5%
Max accuracy: 26.44%
Epoch: [18]  [ 0/40]  eta: 0:02:43  lr: 0.000033  min_lr: 0.000000  loss: 4.2375 (4.2375)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.0966  data: 3.6409  max mem: 15572
Epoch: [18]  [10/40]  eta: 0:00:23  lr: 0.000032  min_lr: 0.000000  loss: 4.1050 (3.9937)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7716  data: 0.3846  max mem: 15572
Epoch: [18]  [20/40]  eta: 0:00:11  lr: 0.000032  min_lr: 0.000000  loss: 3.9350 (3.9643)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4100  data: 0.0296  max mem: 15572
Epoch: [18]  [30/40]  eta: 0:00:05  lr: 0.000031  min_lr: 0.000000  loss: 3.9350 (3.9881)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3741  data: 0.0002  max mem: 15572
Epoch: [18]  [39/40]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000000  loss: 3.9168 (3.9775)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3665  data: 0.0001  max mem: 15572
Epoch: [18] Total time: 0:00:19 (0.4849 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000000  loss: 3.9168 (3.9775)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 160
Indices to remove: tensor([   75,   112,   165,   237,  1916,  3042,  3270,  3471,  3607,  3826,
         5022,  5382,  5606,  5788,  5906,  5937,  6076,  6089,  6130,  6178,
         6226,  6229,  6324,  6637,  6841,  6870,  6990,  7302,  7360,  7449,
         7545,  7666,  7853,  8518,  8529,  8594,  8906,  9002,  9087,  9146,
         9160,  9211,  9249,  9257,  9483,  9538,  9541,  9543,  9777,  9843,
         9967, 10090, 10245, 10485, 10602, 11224, 11574, 11622, 11632, 11647,
        11735, 13083, 13865, 13883, 14396, 14592, 14960, 15299, 15586, 15820,
        15826, 16144, 16323, 16847, 16899, 16928, 16941, 16950, 17084, 17194,
        17213, 17358, 17361, 17531, 17532, 17577, 17660, 18534, 18677, 18744,
        18792, 18912, 19288, 19316, 19371, 19815, 19882, 20763, 21062, 21068,
        21208, 21252, 21438, 21497, 21500, 21580, 21607, 21734, 22059, 22513,
        23095, 23796, 24281, 24382, 24856, 25366, 25551, 25983, 26165, 26629,
        26663, 26944, 26947, 27213, 27273, 27386, 27419, 27618, 27695, 28100,
        28518, 28805, 28806, 28808, 28844, 28903, 28944, 29163, 29417, 29423,
        29435, 29518, 29662, 29781, 30767, 30791, 31208, 31278, 31831, 32134,
        32182, 32193, 32201, 32223, 32327, 32429, 32452, 32672, 33277, 33705],
       device='cuda:0')
length of data loader train is: 26
num_training_steps_per_epoch is: 26
Change step level LR scheduler!
Set warmup steps = 130
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:10:38  loss: 0.7467 (0.7467)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3487  data: 2.1593  max mem: 15572
Val:  [ 10/272]  eta: 0:01:58  loss: 4.2594 (3.7045)  acc1: 0.0000 (17.1717)  acc5: 16.6667 (31.3131)  time: 0.4514  data: 0.2868  max mem: 15572
Val:  [ 20/272]  eta: 0:01:17  loss: 3.8920 (3.6690)  acc1: 5.5556 (15.0794)  acc5: 22.2222 (36.5079)  time: 0.2066  data: 0.0499  max mem: 15572
Val:  [ 30/272]  eta: 0:01:02  loss: 3.6827 (3.7213)  acc1: 5.5556 (11.6487)  acc5: 44.4444 (38.8889)  time: 0.1515  data: 0.0003  max mem: 15572
Val:  [ 40/272]  eta: 0:00:54  loss: 3.5428 (3.6388)  acc1: 5.5556 (12.4661)  acc5: 50.0000 (42.1409)  time: 0.1546  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:50  loss: 3.3828 (3.5343)  acc1: 11.1111 (16.3399)  acc5: 50.0000 (45.2070)  time: 0.1773  data: 0.0154  max mem: 15572
Val:  [ 60/272]  eta: 0:00:46  loss: 2.1809 (3.3371)  acc1: 50.0000 (23.3151)  acc5: 72.2222 (48.8160)  time: 0.1868  data: 0.0199  max mem: 15572
Val:  [ 70/272]  eta: 0:00:43  loss: 2.2980 (3.2223)  acc1: 50.0000 (24.7261)  acc5: 77.7778 (52.8951)  time: 0.1888  data: 0.0168  max mem: 15572
Val:  [ 80/272]  eta: 0:00:41  loss: 2.8344 (3.2398)  acc1: 22.2222 (24.8285)  acc5: 72.2222 (51.9890)  time: 0.2010  data: 0.0242  max mem: 15572
Val:  [ 90/272]  eta: 0:00:39  loss: 4.1477 (3.3488)  acc1: 0.0000 (22.2833)  acc5: 16.6667 (48.1074)  time: 0.2203  data: 0.0512  max mem: 15572
Val:  [100/272]  eta: 0:00:36  loss: 4.0892 (3.4152)  acc1: 0.0000 (21.4521)  acc5: 22.2222 (47.2497)  time: 0.1988  data: 0.0392  max mem: 15572
Val:  [110/272]  eta: 0:00:33  loss: 4.0219 (3.4768)  acc1: 0.0000 (20.1201)  acc5: 33.3333 (46.1962)  time: 0.1634  data: 0.0005  max mem: 15572
Val:  [120/272]  eta: 0:00:31  loss: 4.0219 (3.5153)  acc1: 0.0000 (19.0083)  acc5: 27.7778 (45.2709)  time: 0.1753  data: 0.0007  max mem: 15572
Val:  [130/272]  eta: 0:00:28  loss: 3.8721 (3.4682)  acc1: 11.1111 (20.8227)  acc5: 33.3333 (46.1408)  time: 0.1694  data: 0.0006  max mem: 15572
Val:  [140/272]  eta: 0:00:26  loss: 3.2027 (3.4606)  acc1: 22.2222 (21.5524)  acc5: 55.5556 (46.1387)  time: 0.1656  data: 0.0006  max mem: 15572
Val:  [150/272]  eta: 0:00:24  loss: 3.5085 (3.4649)  acc1: 11.1111 (20.9345)  acc5: 44.4444 (46.0633)  time: 0.1764  data: 0.0007  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.3754 (3.4374)  acc1: 16.6667 (22.0842)  acc5: 50.0000 (47.4810)  time: 0.1718  data: 0.0007  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.5760 (3.4654)  acc1: 11.1111 (21.2151)  acc5: 44.4444 (46.4587)  time: 0.1663  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.6662 (3.4609)  acc1: 5.5556 (21.5470)  acc5: 33.3333 (47.0534)  time: 0.1739  data: 0.0006  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.9354 (3.4972)  acc1: 0.0000 (20.4770)  acc5: 27.7778 (45.8988)  time: 0.1702  data: 0.0005  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.9354 (3.4846)  acc1: 0.0000 (20.5915)  acc5: 27.7778 (46.8214)  time: 0.1729  data: 0.0027  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.8749 (3.4834)  acc1: 27.7778 (21.3270)  acc5: 72.2222 (47.4197)  time: 0.1780  data: 0.0027  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.1854 (3.4785)  acc1: 27.7778 (21.5184)  acc5: 61.1111 (47.8381)  time: 0.1774  data: 0.0006  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 2.9905 (3.4366)  acc1: 44.4444 (23.3045)  acc5: 72.2222 (49.1342)  time: 0.1776  data: 0.0006  max mem: 15572
Val:  [240/272]  eta: 0:00:06  loss: 2.7281 (3.4128)  acc1: 38.8889 (23.5823)  acc5: 77.7778 (50.2766)  time: 0.1688  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.1033 (3.4325)  acc1: 11.1111 (23.0190)  acc5: 61.1111 (49.7344)  time: 0.1588  data: 0.0004  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.6560 (3.3537)  acc1: 33.3333 (25.4789)  acc5: 83.3333 (51.3623)  time: 0.1508  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.5003 (3.3567)  acc1: 55.5556 (25.2153)  acc5: 77.7778 (51.4555)  time: 0.1434  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.5003 (3.3606)  acc1: 50.0000 (25.1894)  acc5: 77.7778 (51.4233)  time: 0.1381  data: 0.0001  max mem: 15572
Val: Total time: 0:00:50 (0.1840 s / it)
* Acc@1 25.189 Acc@5 51.423 loss 3.361
Accuracy of the network on the 4883 val videos: 25.2%
Max accuracy: 26.44%
Epoch: [19]  [ 0/26]  eta: 0:01:25  lr: 0.000031  min_lr: 0.000000  loss: 3.3691 (3.3691)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.2968  data: 2.8761  max mem: 15572
[2025-01-13 13:31:59,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 13:31:59,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [10/26]  eta: 0:00:11  lr: 0.000030  min_lr: 0.000000  loss: 3.9508 (3.9559)  loss_scale: 65536.0000 (95325.0909)  weight_decay: 0.0500 (0.0500)  time: 0.7155  data: 0.3240  max mem: 15572
[2025-01-13 13:32:01,744] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21866
[2025-01-13 13:32:01,744] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 13:32:01,745] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [20/26]  eta: 0:00:03  lr: 0.000029  min_lr: 0.000000  loss: 4.2161 (4.0870)  loss_scale: 65536.0000 (81139.8095)  weight_decay: 0.0500 (0.0500)  time: 0.4120  data: 0.0344  max mem: 15572
Epoch: [19]  [25/26]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000000  loss: 4.1042 (4.0784)  loss_scale: 65536.0000 (78139.0769)  weight_decay: 0.0500 (0.0500)  time: 0.3737  data: 0.0001  max mem: 15572
Epoch: [19] Total time: 0:00:13 (0.5188 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000000  loss: 4.1042 (4.0784)  loss_scale: 65536.0000 (78139.0769)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 99
Indices to remove: tensor([  112,   230,  1916,  1999,  2087,  2908,  3270,  3403,  3719,  3841,
         4545,  5466,  5606,  5745,  5976,  6027,  6076,  6089,  6130,  6178,
         6266,  7302,  7360,  7545,  7598,  7624,  8496,  8518,  8735,  8906,
         9002,  9053,  9087,  9160,  9249,  9257,  9285,  9480,  9538,  9769,
         9777,  9843,  9882, 10117, 10264, 11622, 11647, 11656, 13077, 13883,
        15820, 15826, 15928, 16028, 16075, 16128, 16899, 16950, 17084, 17492,
        17532, 17660, 18462, 18677, 18775, 19049, 19288, 20763, 20782, 21252,
        21270, 21438, 21497, 21607, 21764, 22610, 23636, 23796, 23999, 24541,
        25826, 25835, 25975, 26373, 26944, 27181, 28593, 28808, 28944, 29435,
        29662, 29781, 30213, 31078, 31909, 32134, 32230, 32429, 32452],
       device='cuda:0')
length of data loader train is: 18
num_training_steps_per_epoch is: 18
Change step level LR scheduler!
Set warmup steps = 90
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
[2025-01-13 13:32:07,003] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2025-01-13 13:32:07,004] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-19/mp_rank_00_model_states.pt
[2025-01-13 13:32:07,004] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-19/mp_rank_00_model_states.pt...
[2025-01-13 13:32:07,230] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_random_as_wrong_samples/checkpoint-19/mp_rank_00_model_states.pt.
[2025-01-13 13:32:07,230] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [  0/272]  eta: 0:12:41  loss: 0.7668 (0.7668)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.8013  data: 2.6409  max mem: 15572
Val:  [ 10/272]  eta: 0:01:43  loss: 4.2771 (3.7183)  acc1: 5.5556 (17.1717)  acc5: 16.6667 (32.3232)  time: 0.3968  data: 0.2404  max mem: 15572
Val:  [ 20/272]  eta: 0:01:12  loss: 3.7384 (3.6477)  acc1: 5.5556 (14.5503)  acc5: 27.7778 (37.5661)  time: 0.1607  data: 0.0004  max mem: 15572
Val:  [ 30/272]  eta: 0:01:02  loss: 3.6730 (3.7140)  acc1: 5.5556 (11.2903)  acc5: 44.4444 (39.7849)  time: 0.1849  data: 0.0198  max mem: 15572
Val:  [ 40/272]  eta: 0:00:54  loss: 3.5758 (3.6460)  acc1: 5.5556 (11.9241)  acc5: 50.0000 (42.8184)  time: 0.1835  data: 0.0198  max mem: 15572
Val:  [ 50/272]  eta: 0:00:49  loss: 3.4035 (3.5396)  acc1: 16.6667 (15.7952)  acc5: 55.5556 (45.8606)  time: 0.1622  data: 0.0005  max mem: 15572
Val:  [ 60/272]  eta: 0:00:44  loss: 2.1739 (3.3423)  acc1: 55.5556 (22.6776)  acc5: 72.2222 (49.1803)  time: 0.1623  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:41  loss: 2.3351 (3.2270)  acc1: 55.5556 (24.2567)  acc5: 77.7778 (53.2864)  time: 0.1634  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:38  loss: 2.8914 (3.2418)  acc1: 22.2222 (24.8285)  acc5: 72.2222 (52.5377)  time: 0.1667  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:35  loss: 4.1850 (3.3556)  acc1: 5.5556 (22.3443)  acc5: 16.6667 (48.2906)  time: 0.1645  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:33  loss: 4.1850 (3.4226)  acc1: 5.5556 (21.7822)  acc5: 16.6667 (47.1947)  time: 0.1744  data: 0.0044  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 4.0604 (3.4885)  acc1: 0.0000 (20.3203)  acc5: 27.7778 (46.0460)  time: 0.2055  data: 0.0239  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 4.0802 (3.5303)  acc1: 0.0000 (19.1919)  acc5: 27.7778 (45.0872)  time: 0.1972  data: 0.0200  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.8844 (3.4828)  acc1: 5.5556 (20.9075)  acc5: 33.3333 (46.0560)  time: 0.1761  data: 0.0008  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 3.1251 (3.4735)  acc1: 27.7778 (21.6312)  acc5: 55.5556 (46.1387)  time: 0.1767  data: 0.0008  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.4092 (3.4749)  acc1: 11.1111 (21.0449)  acc5: 50.0000 (46.2840)  time: 0.1795  data: 0.0007  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.3575 (3.4456)  acc1: 22.2222 (22.2222)  acc5: 61.1111 (47.6536)  time: 0.1718  data: 0.0005  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.5818 (3.4734)  acc1: 11.1111 (21.2151)  acc5: 44.4444 (46.8161)  time: 0.1673  data: 0.0005  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.6260 (3.4697)  acc1: 5.5556 (21.5470)  acc5: 44.4444 (47.3603)  time: 0.1699  data: 0.0025  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 4.0254 (3.5073)  acc1: 0.0000 (20.4479)  acc5: 27.7778 (46.1024)  time: 0.1622  data: 0.0025  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 4.0254 (3.4926)  acc1: 0.0000 (20.5915)  acc5: 27.7778 (47.1531)  time: 0.1670  data: 0.0005  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.7864 (3.4899)  acc1: 22.2222 (21.3007)  acc5: 72.2222 (47.8673)  time: 0.1715  data: 0.0006  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.2311 (3.4870)  acc1: 27.7778 (21.4429)  acc5: 61.1111 (48.2906)  time: 0.1732  data: 0.0005  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.0391 (3.4466)  acc1: 38.8889 (23.2083)  acc5: 72.2222 (49.4949)  time: 0.1748  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.7497 (3.4226)  acc1: 44.4444 (23.4901)  acc5: 77.7778 (50.5302)  time: 0.1729  data: 0.0006  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.1229 (3.4420)  acc1: 16.6667 (23.0633)  acc5: 66.6667 (49.9779)  time: 0.1735  data: 0.0006  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.6907 (3.3638)  acc1: 44.4444 (25.5215)  acc5: 77.7778 (51.5964)  time: 0.1633  data: 0.0005  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.4478 (3.3669)  acc1: 55.5556 (25.1948)  acc5: 77.7778 (51.7015)  time: 0.1464  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.4478 (3.3720)  acc1: 44.4444 (25.1690)  acc5: 77.7778 (51.6691)  time: 0.1387  data: 0.0002  max mem: 15572
Val: Total time: 0:00:49 (0.1810 s / it)
* Acc@1 25.169 Acc@5 51.669 loss 3.372
Accuracy of the network on the 4883 val videos: 25.2%
Max accuracy: 26.44%
Epoch: [20]  [ 0/18]  eta: 0:01:19  lr: 0.000029  min_lr: 0.000000  loss: 4.0123 (4.0123)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.4216  data: 4.0221  max mem: 15572
Epoch: [20]  [10/18]  eta: 0:00:05  lr: 0.000028  min_lr: 0.000000  loss: 4.0123 (3.9935)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7458  data: 0.3694  max mem: 15572
Epoch: [20]  [17/18]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000000  loss: 4.1340 (4.0467)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.2258  max mem: 15572
Epoch: [20] Total time: 0:00:10 (0.6042 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000000  loss: 4.1340 (4.0467)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 69
Indices to remove: tensor([ 1907,  1999,  3042,  3442,  3720,  3810,  5382,  5656,  5745,  5906,
         6089,  6178,  6220,  7545,  7598,  8638,  8665,  8906,  9053,  9087,
         9285,  9665,  9666, 11622, 11647, 11735, 12990, 15111, 15820, 15826,
        15928, 16899, 17577, 18677, 19467, 20763, 21270, 21438, 21580, 21764,
        24108, 24712, 25298, 25551, 25886, 26413, 27176, 27213, 27322, 27386,
        27696, 28100, 28593, 28806, 28808, 28851, 28944, 29662, 29781, 30755,
        31059, 31831, 32089, 32134, 32169, 32182, 32201, 32230, 32429],
       device='cuda:0')
length of data loader train is: 12
num_training_steps_per_epoch is: 12
Change step level LR scheduler!
Set warmup steps = 60
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:09:14  loss: 0.7757 (0.7757)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0378  data: 1.8796  max mem: 15572
Val:  [ 10/272]  eta: 0:01:54  loss: 4.3830 (3.7654)  acc1: 0.0000 (15.6566)  acc5: 16.6667 (31.8182)  time: 0.4377  data: 0.2848  max mem: 15572
Val:  [ 20/272]  eta: 0:01:16  loss: 3.7751 (3.6709)  acc1: 0.0000 (14.8148)  acc5: 27.7778 (37.3016)  time: 0.2173  data: 0.0629  max mem: 15572
Val:  [ 30/272]  eta: 0:01:03  loss: 3.6544 (3.7375)  acc1: 5.5556 (11.8280)  acc5: 38.8889 (39.6057)  time: 0.1649  data: 0.0005  max mem: 15572
Val:  [ 40/272]  eta: 0:00:54  loss: 3.5436 (3.6588)  acc1: 5.5556 (12.4661)  acc5: 50.0000 (42.8184)  time: 0.1620  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:49  loss: 3.4000 (3.5449)  acc1: 11.1111 (16.0131)  acc5: 55.5556 (46.4052)  time: 0.1596  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:45  loss: 2.2230 (3.3476)  acc1: 50.0000 (23.2240)  acc5: 72.2222 (49.7268)  time: 0.1689  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:41  loss: 2.2269 (3.2288)  acc1: 55.5556 (24.8044)  acc5: 72.2222 (53.6776)  time: 0.1641  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:38  loss: 2.8800 (3.2302)  acc1: 22.2222 (25.1715)  acc5: 72.2222 (53.2236)  time: 0.1596  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:35  loss: 4.2482 (3.3489)  acc1: 0.0000 (22.5885)  acc5: 16.6667 (48.9011)  time: 0.1595  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:33  loss: 4.1228 (3.4136)  acc1: 5.5556 (22.0572)  acc5: 22.2222 (48.1848)  time: 0.1643  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:30  loss: 4.0279 (3.4825)  acc1: 0.0000 (20.6707)  acc5: 33.3333 (46.8468)  time: 0.1666  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:28  loss: 4.0887 (3.5270)  acc1: 0.0000 (19.5133)  acc5: 27.7778 (45.9137)  time: 0.1599  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.9691 (3.4778)  acc1: 5.5556 (21.3740)  acc5: 38.8889 (46.7769)  time: 0.2240  data: 0.0620  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 3.2086 (3.4730)  acc1: 22.2222 (22.1040)  acc5: 55.5556 (46.6509)  time: 0.2439  data: 0.0622  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.4140 (3.4744)  acc1: 11.1111 (21.5968)  acc5: 50.0000 (46.8359)  time: 0.1866  data: 0.0006  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.3532 (3.4472)  acc1: 22.2222 (22.6708)  acc5: 55.5556 (48.2057)  time: 0.1678  data: 0.0005  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.6157 (3.4782)  acc1: 5.5556 (21.6374)  acc5: 44.4444 (47.2060)  time: 0.1689  data: 0.0057  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.6560 (3.4720)  acc1: 5.5556 (22.0994)  acc5: 33.3333 (47.8821)  time: 0.1911  data: 0.0294  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.9427 (3.5049)  acc1: 0.0000 (21.0588)  acc5: 27.7778 (46.6550)  time: 0.1837  data: 0.0242  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.9427 (3.4881)  acc1: 0.0000 (21.2548)  acc5: 27.7778 (47.7612)  time: 0.1671  data: 0.0005  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.8162 (3.4826)  acc1: 27.7778 (22.1959)  acc5: 77.7778 (48.4729)  time: 0.1693  data: 0.0005  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.2275 (3.4784)  acc1: 33.3333 (22.3479)  acc5: 61.1111 (48.9442)  time: 0.1762  data: 0.0006  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.0916 (3.4383)  acc1: 38.8889 (24.0260)  acc5: 61.1111 (50.1203)  time: 0.1698  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.8005 (3.4139)  acc1: 38.8889 (24.2047)  acc5: 83.3333 (51.4292)  time: 0.1592  data: 0.0005  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.0882 (3.4343)  acc1: 11.1111 (23.7052)  acc5: 72.2222 (50.9075)  time: 0.1658  data: 0.0006  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.5799 (3.3545)  acc1: 44.4444 (26.1388)  acc5: 77.7778 (52.4691)  time: 0.1614  data: 0.0004  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.4472 (3.3579)  acc1: 55.5556 (25.8098)  acc5: 77.7778 (52.4805)  time: 0.1477  data: 0.0003  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.4472 (3.3621)  acc1: 50.0000 (25.7833)  acc5: 77.7778 (52.4473)  time: 0.1424  data: 0.0002  max mem: 15572
Val: Total time: 0:00:49 (0.1826 s / it)
* Acc@1 25.783 Acc@5 52.447 loss 3.362
Accuracy of the network on the 4883 val videos: 25.8%
Max accuracy: 26.44%
Epoch: [21]  [ 0/12]  eta: 0:00:51  lr: 0.000027  min_lr: 0.000000  loss: 3.9111 (3.9111)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.2628  data: 3.8625  max mem: 15572
Epoch: [21]  [10/12]  eta: 0:00:01  lr: 0.000025  min_lr: 0.000000  loss: 3.9111 (3.8190)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7188  data: 0.3512  max mem: 15572
Epoch: [21]  [11/12]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000000  loss: 3.7157 (3.8104)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.3219  max mem: 15572
Epoch: [21] Total time: 0:00:08 (0.7009 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000000  loss: 3.7157 (3.8104)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 59
Indices to remove: tensor([  112,  1483,  1907,  1916,  2841,  3270,  3810,  4960,  5606,  5906,
         5937,  6324,  8638,  9053,  9087,  9285,  9483,  9538,  9541, 11647,
        12990, 13956, 14411, 15928, 16604, 16899, 16950, 18534, 19032, 19467,
        19676, 19757, 20404, 21270, 21580, 22609, 23481, 24382, 25104, 25886,
        26165, 26413, 26947, 27419, 28808, 28944, 29423, 29662, 30520, 30767,
        31059, 31078, 31372, 31909, 32089, 32169, 32230, 32429, 33277],
       device='cuda:0')
length of data loader train is: 7
num_training_steps_per_epoch is: 7
Change step level LR scheduler!
Set warmup steps = 35
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:10:42  loss: 0.7707 (0.7707)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3616  data: 2.1957  max mem: 15572
Val:  [ 10/272]  eta: 0:01:55  loss: 4.2908 (3.7573)  acc1: 0.0000 (15.1515)  acc5: 11.1111 (30.8081)  time: 0.4401  data: 0.2835  max mem: 15572
Val:  [ 20/272]  eta: 0:01:17  loss: 3.8737 (3.6826)  acc1: 5.5556 (15.6085)  acc5: 27.7778 (36.5079)  time: 0.2045  data: 0.0463  max mem: 15572
Val:  [ 30/272]  eta: 0:01:03  loss: 3.6144 (3.7178)  acc1: 5.5556 (12.0072)  acc5: 50.0000 (39.9642)  time: 0.1647  data: 0.0018  max mem: 15572
Val:  [ 40/272]  eta: 0:00:55  loss: 3.5188 (3.6338)  acc1: 5.5556 (13.0081)  acc5: 50.0000 (43.3604)  time: 0.1648  data: 0.0018  max mem: 15572
Val:  [ 50/272]  eta: 0:00:49  loss: 3.3630 (3.5342)  acc1: 11.1111 (16.2309)  acc5: 50.0000 (46.5142)  time: 0.1607  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:44  loss: 2.2399 (3.3403)  acc1: 44.4444 (23.2240)  acc5: 72.2222 (49.9089)  time: 0.1563  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:41  loss: 2.2585 (3.2276)  acc1: 50.0000 (24.6479)  acc5: 77.7778 (53.7559)  time: 0.1601  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:38  loss: 2.8411 (3.2313)  acc1: 22.2222 (24.7599)  acc5: 72.2222 (53.2236)  time: 0.1693  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:35  loss: 4.2119 (3.3497)  acc1: 0.0000 (22.1612)  acc5: 22.2222 (49.0842)  time: 0.1652  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:33  loss: 4.1969 (3.4166)  acc1: 0.0000 (21.3421)  acc5: 22.2222 (48.1298)  time: 0.1763  data: 0.0113  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 4.0565 (3.4842)  acc1: 0.0000 (19.9199)  acc5: 33.3333 (46.8468)  time: 0.1723  data: 0.0113  max mem: 15572
Val:  [120/272]  eta: 0:00:28  loss: 4.0889 (3.5293)  acc1: 0.0000 (18.7787)  acc5: 27.7778 (45.6841)  time: 0.1556  data: 0.0003  max mem: 15572
Val:  [130/272]  eta: 0:00:26  loss: 3.9528 (3.4799)  acc1: 5.5556 (20.6531)  acc5: 27.7778 (46.6073)  time: 0.1715  data: 0.0106  max mem: 15572
Val:  [140/272]  eta: 0:00:24  loss: 3.1832 (3.4760)  acc1: 27.7778 (21.3160)  acc5: 55.5556 (46.5327)  time: 0.1847  data: 0.0223  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.4848 (3.4777)  acc1: 5.5556 (20.7138)  acc5: 50.0000 (46.6887)  time: 0.2167  data: 0.0595  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.4485 (3.4550)  acc1: 16.6667 (21.6701)  acc5: 50.0000 (47.9641)  time: 0.2007  data: 0.0479  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.6680 (3.4870)  acc1: 5.5556 (20.7602)  acc5: 44.4444 (47.0110)  time: 0.1598  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.7197 (3.4792)  acc1: 5.5556 (21.3321)  acc5: 38.8889 (47.6366)  time: 0.1707  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.9189 (3.5097)  acc1: 5.5556 (20.3025)  acc5: 33.3333 (46.5678)  time: 0.1725  data: 0.0005  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.9189 (3.4934)  acc1: 0.0000 (20.5362)  acc5: 33.3333 (47.6230)  time: 0.1687  data: 0.0005  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.7865 (3.4868)  acc1: 27.7778 (21.4323)  acc5: 77.7778 (48.4202)  time: 0.1708  data: 0.0005  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.1931 (3.4810)  acc1: 27.7778 (21.6692)  acc5: 61.1111 (48.8688)  time: 0.1750  data: 0.0006  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.0609 (3.4379)  acc1: 33.3333 (23.4247)  acc5: 66.6667 (50.0962)  time: 0.1826  data: 0.0006  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.7369 (3.4127)  acc1: 44.4444 (23.7206)  acc5: 83.3333 (51.3370)  time: 0.1779  data: 0.0006  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.0750 (3.4328)  acc1: 16.6667 (23.2404)  acc5: 72.2222 (50.7304)  time: 0.1667  data: 0.0005  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.6230 (3.3516)  acc1: 44.4444 (25.6918)  acc5: 77.7778 (52.3201)  time: 0.1588  data: 0.0016  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.4685 (3.3537)  acc1: 55.5556 (25.4613)  acc5: 83.3333 (52.5625)  time: 0.1470  data: 0.0014  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.4685 (3.3583)  acc1: 55.5556 (25.4352)  acc5: 83.3333 (52.5292)  time: 0.1404  data: 0.0014  max mem: 15572
Val: Total time: 0:00:49 (0.1812 s / it)
* Acc@1 25.435 Acc@5 52.529 loss 3.358
Accuracy of the network on the 4883 val videos: 25.4%
Max accuracy: 26.44%
Epoch: [22]  [0/7]  eta: 0:00:24  lr: 0.000025  min_lr: 0.000000  loss: 4.3364 (4.3364)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.4286  data: 3.0291  max mem: 15572
Epoch: [22]  [6/7]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000000  loss: 4.1957 (4.0915)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8097  data: 0.4329  max mem: 15572
Epoch: [22] Total time: 0:00:05 (0.8270 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000000  loss: 4.1957 (4.0915)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 31
Indices to remove: tensor([ 1916,  5745,  6027,  6220,  9483,  9538,  9541, 10501, 14350, 16899,
        16950, 18534, 20919, 21270, 21784, 23481, 24108, 24571, 25413, 26947,
        28593, 28806, 28808, 30125, 30520, 30767, 31909, 32134, 32429, 32460,
        33277], device='cuda:0')
length of data loader train is: 5
num_training_steps_per_epoch is: 5
Change step level LR scheduler!
Set warmup steps = 25
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:08:50  loss: 0.7929 (0.7929)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 1.9506  data: 1.7937  max mem: 15572
Val:  [ 10/272]  eta: 0:01:51  loss: 4.2831 (3.7562)  acc1: 0.0000 (16.1616)  acc5: 16.6667 (30.3030)  time: 0.4250  data: 0.2674  max mem: 15572
Val:  [ 20/272]  eta: 0:01:15  loss: 3.8097 (3.6832)  acc1: 0.0000 (16.1376)  acc5: 33.3333 (36.7725)  time: 0.2151  data: 0.0576  max mem: 15572
Val:  [ 30/272]  eta: 0:01:01  loss: 3.7638 (3.7214)  acc1: 5.5556 (12.5448)  acc5: 44.4444 (39.4265)  time: 0.1612  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:54  loss: 3.5566 (3.6364)  acc1: 5.5556 (13.1436)  acc5: 50.0000 (43.2249)  time: 0.1691  data: 0.0005  max mem: 15572
Val:  [ 50/272]  eta: 0:00:48  loss: 3.3855 (3.5366)  acc1: 11.1111 (16.0131)  acc5: 55.5556 (46.5142)  time: 0.1668  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:45  loss: 2.2981 (3.3481)  acc1: 44.4444 (22.9508)  acc5: 72.2222 (49.9089)  time: 0.1667  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:41  loss: 2.3731 (3.2347)  acc1: 50.0000 (24.5696)  acc5: 77.7778 (53.7559)  time: 0.1763  data: 0.0005  max mem: 15572
Val:  [ 80/272]  eta: 0:00:39  loss: 2.8371 (3.2394)  acc1: 22.2222 (24.6914)  acc5: 77.7778 (53.2922)  time: 0.1766  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:36  loss: 4.2048 (3.3572)  acc1: 0.0000 (22.1001)  acc5: 22.2222 (49.1453)  time: 0.1697  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:33  loss: 4.1651 (3.4251)  acc1: 0.0000 (21.4521)  acc5: 22.2222 (47.9098)  time: 0.1682  data: 0.0029  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 3.9940 (3.4927)  acc1: 0.0000 (19.9700)  acc5: 27.7778 (46.4464)  time: 0.1772  data: 0.0096  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 4.0463 (3.5343)  acc1: 0.0000 (18.9164)  acc5: 27.7778 (45.5464)  time: 0.1761  data: 0.0072  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.9506 (3.4823)  acc1: 11.1111 (20.8227)  acc5: 33.3333 (46.4801)  time: 0.1853  data: 0.0173  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 3.2665 (3.4798)  acc1: 27.7778 (21.5130)  acc5: 50.0000 (46.2569)  time: 0.1924  data: 0.0248  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.4748 (3.4821)  acc1: 5.5556 (20.6770)  acc5: 44.4444 (46.4312)  time: 0.1720  data: 0.0079  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.4716 (3.4602)  acc1: 16.6667 (21.8081)  acc5: 55.5556 (47.6881)  time: 0.1620  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.7297 (3.4923)  acc1: 11.1111 (20.8577)  acc5: 38.8889 (46.6862)  time: 0.1757  data: 0.0005  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.7684 (3.4854)  acc1: 5.5556 (21.2707)  acc5: 38.8889 (47.3910)  time: 0.1772  data: 0.0006  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.9204 (3.5152)  acc1: 0.0000 (20.2734)  acc5: 33.3333 (46.2187)  time: 0.1651  data: 0.0005  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.9204 (3.4982)  acc1: 0.0000 (20.5362)  acc5: 33.3333 (47.2360)  time: 0.2006  data: 0.0355  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.7888 (3.4902)  acc1: 27.7778 (21.5113)  acc5: 83.3333 (48.0779)  time: 0.2198  data: 0.0356  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.1507 (3.4828)  acc1: 27.7778 (21.6440)  acc5: 61.1111 (48.5168)  time: 0.1904  data: 0.0006  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.0355 (3.4388)  acc1: 38.8889 (23.4969)  acc5: 66.6667 (49.7595)  time: 0.1675  data: 0.0004  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.6910 (3.4128)  acc1: 38.8889 (23.6515)  acc5: 83.3333 (50.9912)  time: 0.1598  data: 0.0004  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 2.9942 (3.4335)  acc1: 11.1111 (23.2182)  acc5: 72.2222 (50.3099)  time: 0.1718  data: 0.0044  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.6200 (3.3514)  acc1: 44.4444 (25.6705)  acc5: 77.7778 (51.9157)  time: 0.1692  data: 0.0043  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.5294 (3.3533)  acc1: 55.5556 (25.4408)  acc5: 77.7778 (52.1320)  time: 0.1504  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.5294 (3.3577)  acc1: 55.5556 (25.4147)  acc5: 77.7778 (52.0991)  time: 0.1437  data: 0.0002  max mem: 15572
Val: Total time: 0:00:50 (0.1842 s / it)
* Acc@1 25.415 Acc@5 52.099 loss 3.358
Accuracy of the network on the 4883 val videos: 25.4%
Max accuracy: 26.44%
Epoch: [23]  [0/5]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000000  loss: 3.8329 (3.8329)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.4912  data: 3.0967  max mem: 15572
Epoch: [23]  [4/5]  eta: 0:00:01  lr: 0.000021  min_lr: 0.000000  loss: 3.8857 (4.0294)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0018  data: 0.6195  max mem: 15572
Epoch: [23] Total time: 0:00:05 (1.0270 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 3.8857 (4.0294)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 23
Indices to remove: tensor([ 1907,  2841,  6089,  8192,  9538, 10724, 18534, 18775, 19757, 21270,
        23165, 25986, 26165, 26947, 27213, 28806, 28808, 29662, 30213, 32089,
        32169, 32429, 32460], device='cuda:0')
length of data loader train is: 3
num_training_steps_per_epoch is: 3
Change step level LR scheduler!
Set warmup steps = 15
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:13:47  loss: 0.7798 (0.7798)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 3.0408  data: 2.8585  max mem: 15572
Val:  [ 10/272]  eta: 0:01:57  loss: 4.2044 (3.7602)  acc1: 0.0000 (18.1818)  acc5: 16.6667 (30.3030)  time: 0.4467  data: 0.2904  max mem: 15572
Val:  [ 20/272]  eta: 0:01:18  loss: 3.8651 (3.6867)  acc1: 0.0000 (16.4021)  acc5: 33.3333 (34.6561)  time: 0.1752  data: 0.0170  max mem: 15572
Val:  [ 30/272]  eta: 0:01:04  loss: 3.6713 (3.7247)  acc1: 5.5556 (12.5448)  acc5: 38.8889 (37.4552)  time: 0.1662  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:55  loss: 3.5473 (3.6419)  acc1: 5.5556 (13.1436)  acc5: 44.4444 (41.1924)  time: 0.1624  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:49  loss: 3.3749 (3.5362)  acc1: 16.6667 (16.3399)  acc5: 55.5556 (44.5534)  time: 0.1583  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:45  loss: 2.2722 (3.3475)  acc1: 44.4444 (23.0419)  acc5: 72.2222 (48.1785)  time: 0.1635  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:41  loss: 2.3363 (3.2385)  acc1: 50.0000 (24.4914)  acc5: 72.2222 (52.1909)  time: 0.1656  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:38  loss: 2.8145 (3.2414)  acc1: 22.2222 (24.7599)  acc5: 66.6667 (51.7147)  time: 0.1688  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:36  loss: 4.2109 (3.3532)  acc1: 0.0000 (22.1612)  acc5: 16.6667 (47.6190)  time: 0.1743  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:33  loss: 4.0910 (3.4226)  acc1: 0.0000 (21.7272)  acc5: 16.6667 (46.8647)  time: 0.1633  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 4.0202 (3.4898)  acc1: 0.0000 (20.2202)  acc5: 33.3333 (45.7457)  time: 0.1527  data: 0.0003  max mem: 15572
Val:  [120/272]  eta: 0:00:28  loss: 4.0659 (3.5352)  acc1: 0.0000 (19.1001)  acc5: 27.7778 (44.7658)  time: 0.1631  data: 0.0066  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.9957 (3.4836)  acc1: 11.1111 (21.2468)  acc5: 27.7778 (45.8015)  time: 0.1917  data: 0.0331  max mem: 15572
Val:  [140/272]  eta: 0:00:24  loss: 3.1798 (3.4798)  acc1: 27.7778 (21.9464)  acc5: 55.5556 (45.7053)  time: 0.1849  data: 0.0269  max mem: 15572
Val:  [150/272]  eta: 0:00:22  loss: 3.4619 (3.4822)  acc1: 5.5556 (21.0817)  acc5: 50.0000 (46.1369)  time: 0.1595  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:20  loss: 3.4257 (3.4590)  acc1: 11.1111 (22.2567)  acc5: 61.1111 (47.4465)  time: 0.1651  data: 0.0003  max mem: 15572
Val:  [170/272]  eta: 0:00:18  loss: 3.6016 (3.4916)  acc1: 11.1111 (21.2801)  acc5: 44.4444 (46.4912)  time: 0.1663  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:16  loss: 3.7554 (3.4865)  acc1: 5.5556 (21.6083)  acc5: 38.8889 (47.1148)  time: 0.1688  data: 0.0005  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.8867 (3.5164)  acc1: 0.0000 (20.5934)  acc5: 33.3333 (45.9860)  time: 0.1725  data: 0.0006  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.8867 (3.4982)  acc1: 0.0000 (20.9784)  acc5: 38.8889 (47.1531)  time: 0.1720  data: 0.0006  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.7448 (3.4886)  acc1: 33.3333 (21.9853)  acc5: 77.7778 (47.9989)  time: 0.1804  data: 0.0008  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.1761 (3.4827)  acc1: 27.7778 (22.0714)  acc5: 66.6667 (48.5420)  time: 0.1800  data: 0.0029  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.0905 (3.4384)  acc1: 38.8889 (23.9298)  acc5: 66.6667 (49.7595)  time: 0.1779  data: 0.0029  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.7064 (3.4125)  acc1: 33.3333 (23.9511)  acc5: 77.7778 (51.0604)  time: 0.1851  data: 0.0009  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 2.9956 (3.4322)  acc1: 11.1111 (23.4617)  acc5: 72.2222 (50.3763)  time: 0.1838  data: 0.0007  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.6514 (3.3500)  acc1: 38.8889 (25.9259)  acc5: 77.7778 (52.0009)  time: 0.1628  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.6681 (3.3521)  acc1: 55.5556 (25.6253)  acc5: 83.3333 (52.3165)  time: 0.1424  data: 0.0001  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.6681 (3.3566)  acc1: 50.0000 (25.5990)  acc5: 83.3333 (52.2834)  time: 0.1370  data: 0.0001  max mem: 15572
Val: Total time: 0:00:48 (0.1800 s / it)
* Acc@1 25.599 Acc@5 52.283 loss 3.357
Accuracy of the network on the 4883 val videos: 25.6%
Max accuracy: 26.44%
Epoch: [24]  [0/3]  eta: 0:00:09  lr: 0.000020  min_lr: 0.000000  loss: 3.6005 (3.6005)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.2158  data: 2.8107  max mem: 15572
Epoch: [24]  [2/3]  eta: 0:00:01  lr: 0.000019  min_lr: 0.000000  loss: 4.2360 (4.1260)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3234  data: 0.9370  max mem: 15572
Epoch: [24] Total time: 0:00:04 (1.3630 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 4.2360 (4.1260)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 10
Indices to remove: tensor([ 5367,  9002, 24108, 26165, 27419, 28806, 29423, 32089, 32460, 33277],
       device='cuda:0')
length of data loader train is: 2
num_training_steps_per_epoch is: 2
Change step level LR scheduler!
Set warmup steps = 10
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:14:38  loss: 0.7837 (0.7837)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 3.2291  data: 3.0533  max mem: 15572
Val:  [ 10/272]  eta: 0:02:02  loss: 4.2484 (3.7620)  acc1: 0.0000 (15.6566)  acc5: 11.1111 (29.2929)  time: 0.4676  data: 0.3037  max mem: 15572
Val:  [ 20/272]  eta: 0:01:21  loss: 3.7941 (3.6660)  acc1: 0.0000 (15.8730)  acc5: 33.3333 (36.2434)  time: 0.1769  data: 0.0147  max mem: 15572
Val:  [ 30/272]  eta: 0:01:05  loss: 3.6824 (3.7083)  acc1: 5.5556 (12.5448)  acc5: 38.8889 (38.7097)  time: 0.1636  data: 0.0005  max mem: 15572
Val:  [ 40/272]  eta: 0:00:57  loss: 3.5196 (3.6411)  acc1: 5.5556 (13.1436)  acc5: 50.0000 (42.2764)  time: 0.1672  data: 0.0005  max mem: 15572
Val:  [ 50/272]  eta: 0:00:51  loss: 3.4180 (3.5394)  acc1: 11.1111 (16.2309)  acc5: 55.5556 (45.4248)  time: 0.1678  data: 0.0005  max mem: 15572
Val:  [ 60/272]  eta: 0:00:46  loss: 2.2645 (3.3520)  acc1: 50.0000 (23.3151)  acc5: 72.2222 (48.6339)  time: 0.1694  data: 0.0005  max mem: 15572
Val:  [ 70/272]  eta: 0:00:43  loss: 2.3651 (3.2415)  acc1: 50.0000 (24.7261)  acc5: 77.7778 (52.6604)  time: 0.1744  data: 0.0006  max mem: 15572
Val:  [ 80/272]  eta: 0:00:39  loss: 2.8589 (3.2452)  acc1: 22.2222 (24.9657)  acc5: 72.2222 (52.1948)  time: 0.1676  data: 0.0005  max mem: 15572
Val:  [ 90/272]  eta: 0:00:36  loss: 4.1494 (3.3576)  acc1: 0.0000 (22.2833)  acc5: 16.6667 (48.1074)  time: 0.1564  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 4.1444 (3.4247)  acc1: 0.0000 (21.6172)  acc5: 16.6667 (47.1947)  time: 0.1600  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 4.0069 (3.4932)  acc1: 0.0000 (20.0200)  acc5: 27.7778 (45.7958)  time: 0.1662  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 4.0868 (3.5381)  acc1: 0.0000 (18.9624)  acc5: 27.7778 (44.9036)  time: 0.1666  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.9712 (3.4879)  acc1: 5.5556 (20.9075)  acc5: 33.3333 (45.9712)  time: 0.1679  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:24  loss: 3.1753 (3.4826)  acc1: 33.3333 (21.6706)  acc5: 55.5556 (45.9023)  time: 0.1646  data: 0.0004  max mem: 15572
Val:  [150/272]  eta: 0:00:22  loss: 3.4762 (3.4844)  acc1: 5.5556 (21.1185)  acc5: 50.0000 (46.1737)  time: 0.1644  data: 0.0004  max mem: 15572
Val:  [160/272]  eta: 0:00:20  loss: 3.4524 (3.4623)  acc1: 16.6667 (22.1877)  acc5: 55.5556 (47.3430)  time: 0.1635  data: 0.0004  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.6561 (3.4943)  acc1: 5.5556 (21.2151)  acc5: 44.4444 (46.3288)  time: 0.1807  data: 0.0205  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.6978 (3.4878)  acc1: 5.5556 (21.7618)  acc5: 38.8889 (47.0841)  time: 0.1824  data: 0.0206  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.8845 (3.5177)  acc1: 0.0000 (20.7097)  acc5: 33.3333 (45.9570)  time: 0.1716  data: 0.0075  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.8845 (3.4985)  acc1: 0.0000 (21.0890)  acc5: 33.3333 (47.1255)  time: 0.2057  data: 0.0397  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.7118 (3.4874)  acc1: 33.3333 (22.0906)  acc5: 77.7778 (47.9200)  time: 0.1988  data: 0.0355  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.1362 (3.4799)  acc1: 27.7778 (22.2474)  acc5: 61.1111 (48.3409)  time: 0.1689  data: 0.0034  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.0875 (3.4368)  acc1: 38.8889 (24.0500)  acc5: 61.1111 (49.5430)  time: 0.1736  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.7330 (3.4110)  acc1: 38.8889 (24.1355)  acc5: 83.3333 (50.7607)  time: 0.1720  data: 0.0005  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 2.9912 (3.4303)  acc1: 11.1111 (23.6388)  acc5: 66.6667 (50.2213)  time: 0.1698  data: 0.0006  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.5422 (3.3483)  acc1: 44.4444 (26.0749)  acc5: 77.7778 (51.8093)  time: 0.1587  data: 0.0004  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.5460 (3.3500)  acc1: 55.5556 (25.8303)  acc5: 83.3333 (52.0295)  time: 0.1423  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.5460 (3.3544)  acc1: 55.5556 (25.8038)  acc5: 83.3333 (51.9967)  time: 0.1372  data: 0.0001  max mem: 15572
Val: Total time: 0:00:49 (0.1814 s / it)
* Acc@1 25.804 Acc@5 51.997 loss 3.354
Accuracy of the network on the 4883 val videos: 25.8%
Max accuracy: 26.44%
Epoch: [25]  [0/2]  eta: 0:00:06  lr: 0.000018  min_lr: 0.000000  loss: 4.5134 (4.5134)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.4223  data: 3.0154  max mem: 15572
Epoch: [25]  [1/2]  eta: 0:00:01  lr: 0.000017  min_lr: 0.000000  loss: 3.8512 (4.1823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.9060  data: 1.5077  max mem: 15572
Epoch: [25] Total time: 0:00:03 (1.9730 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 3.8512 (4.1823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 7
Indices to remove: tensor([  371,  1907,  5589,  9285,  9538, 25413, 27213], device='cuda:0')
length of data loader train is: 2
num_training_steps_per_epoch is: 2
Change step level LR scheduler!
Set warmup steps = 10
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:14:55  loss: 0.8018 (0.8018)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 3.2910  data: 3.0553  max mem: 15572
Val:  [ 10/272]  eta: 0:02:11  loss: 4.2722 (3.7727)  acc1: 0.0000 (16.1616)  acc5: 22.2222 (30.3030)  time: 0.5034  data: 0.3286  max mem: 15572
Val:  [ 20/272]  eta: 0:01:24  loss: 3.8220 (3.6908)  acc1: 0.0000 (15.3439)  acc5: 27.7778 (35.9788)  time: 0.1875  data: 0.0282  max mem: 15572
Val:  [ 30/272]  eta: 0:01:07  loss: 3.6311 (3.7202)  acc1: 5.5556 (11.8280)  acc5: 44.4444 (37.9928)  time: 0.1578  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:58  loss: 3.5357 (3.6523)  acc1: 5.5556 (12.4661)  acc5: 50.0000 (41.7344)  time: 0.1658  data: 0.0004  max mem: 15572
Val:  [ 50/272]  eta: 0:00:52  loss: 3.4155 (3.5471)  acc1: 11.1111 (15.9041)  acc5: 55.5556 (44.6623)  time: 0.1667  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:47  loss: 2.2624 (3.3561)  acc1: 44.4444 (22.9508)  acc5: 72.2222 (48.3607)  time: 0.1660  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:43  loss: 2.2964 (3.2438)  acc1: 50.0000 (24.5696)  acc5: 77.7778 (52.6604)  time: 0.1689  data: 0.0023  max mem: 15572
Val:  [ 80/272]  eta: 0:00:40  loss: 2.8569 (3.2477)  acc1: 22.2222 (24.8285)  acc5: 77.7778 (52.1948)  time: 0.1711  data: 0.0023  max mem: 15572
Val:  [ 90/272]  eta: 0:00:37  loss: 4.1636 (3.3579)  acc1: 0.0000 (22.2222)  acc5: 22.2222 (48.2295)  time: 0.1677  data: 0.0005  max mem: 15572
Val:  [100/272]  eta: 0:00:34  loss: 4.0785 (3.4253)  acc1: 0.0000 (21.6172)  acc5: 22.2222 (47.5248)  time: 0.1624  data: 0.0004  max mem: 15572
Val:  [110/272]  eta: 0:00:32  loss: 4.0106 (3.4963)  acc1: 0.0000 (20.0701)  acc5: 33.3333 (46.0460)  time: 0.1602  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 4.1266 (3.5431)  acc1: 0.0000 (18.9624)  acc5: 33.3333 (45.2250)  time: 0.1633  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:27  loss: 3.9565 (3.4911)  acc1: 5.5556 (20.9500)  acc5: 33.3333 (46.2680)  time: 0.1652  data: 0.0004  max mem: 15572
Val:  [140/272]  eta: 0:00:25  loss: 3.1263 (3.4859)  acc1: 27.7778 (21.6706)  acc5: 44.4444 (46.2569)  time: 0.1702  data: 0.0005  max mem: 15572
Val:  [150/272]  eta: 0:00:23  loss: 3.4935 (3.4864)  acc1: 11.1111 (21.0817)  acc5: 44.4444 (46.4680)  time: 0.1818  data: 0.0006  max mem: 15572
Val:  [160/272]  eta: 0:00:21  loss: 3.4391 (3.4641)  acc1: 11.1111 (22.1532)  acc5: 55.5556 (47.6881)  time: 0.1814  data: 0.0006  max mem: 15572
Val:  [170/272]  eta: 0:00:19  loss: 3.7094 (3.4958)  acc1: 11.1111 (21.1501)  acc5: 44.4444 (46.7511)  time: 0.1732  data: 0.0006  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.7348 (3.4904)  acc1: 5.5556 (21.5777)  acc5: 44.4444 (47.2990)  time: 0.1678  data: 0.0006  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.9741 (3.5205)  acc1: 0.0000 (20.5352)  acc5: 33.3333 (46.1606)  time: 0.1721  data: 0.0006  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.9741 (3.5014)  acc1: 0.0000 (20.9232)  acc5: 33.3333 (47.3190)  time: 0.1797  data: 0.0005  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.7544 (3.4918)  acc1: 27.7778 (21.8536)  acc5: 83.3333 (48.1043)  time: 0.1754  data: 0.0005  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.1318 (3.4859)  acc1: 27.7778 (21.9708)  acc5: 66.6667 (48.5671)  time: 0.1677  data: 0.0005  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.0820 (3.4424)  acc1: 38.8889 (23.7855)  acc5: 66.6667 (49.8076)  time: 0.1678  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.7265 (3.4164)  acc1: 38.8889 (23.8589)  acc5: 83.3333 (50.9682)  time: 0.1664  data: 0.0005  max mem: 15572
Val:  [250/272]  eta: 0:00:04  loss: 3.0116 (3.4362)  acc1: 11.1111 (23.3953)  acc5: 66.6667 (50.3763)  time: 0.1612  data: 0.0005  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.4989 (3.3540)  acc1: 55.5556 (25.8621)  acc5: 77.7778 (51.9796)  time: 0.1565  data: 0.0004  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.5046 (3.3561)  acc1: 55.5556 (25.5023)  acc5: 83.3333 (52.2550)  time: 0.1456  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.5046 (3.3609)  acc1: 50.0000 (25.4761)  acc5: 83.3333 (52.2220)  time: 0.1380  data: 0.0001  max mem: 15572
Val: Total time: 0:00:49 (0.1803 s / it)
* Acc@1 25.476 Acc@5 52.222 loss 3.361
Accuracy of the network on the 4883 val videos: 25.5%
Max accuracy: 26.44%
Epoch: [26]  [0/2]  eta: 0:00:06  lr: 0.000016  min_lr: 0.000000  loss: 4.0041 (4.0041)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.0814  data: 2.6843  max mem: 15572
Epoch: [26]  [1/2]  eta: 0:00:01  lr: 0.000015  min_lr: 0.000000  loss: 3.8179 (3.9110)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.7268  data: 1.3422  max mem: 15572
Epoch: [26] Total time: 0:00:03 (1.7882 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 3.8179 (3.9110)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 9
Indices to remove: tensor([ 5589,  9002,  9538, 10485, 18249, 23165, 26947, 27213, 32460],
       device='cuda:0')
length of data loader train is: 1
num_training_steps_per_epoch is: 1
Change step level LR scheduler!
Set warmup steps = 5
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:10:28  loss: 0.8084 (0.8084)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 2.3110  data: 2.0817  max mem: 15572
Val:  [ 10/272]  eta: 0:02:03  loss: 4.2770 (3.7457)  acc1: 0.0000 (16.6667)  acc5: 11.1111 (30.3030)  time: 0.4711  data: 0.3046  max mem: 15572
Val:  [ 20/272]  eta: 0:01:22  loss: 3.8148 (3.6654)  acc1: 0.0000 (15.6085)  acc5: 33.3333 (36.2434)  time: 0.2266  data: 0.0637  max mem: 15572
Val:  [ 30/272]  eta: 0:01:05  loss: 3.6509 (3.7061)  acc1: 11.1111 (12.9032)  acc5: 38.8889 (38.7097)  time: 0.1630  data: 0.0004  max mem: 15572
Val:  [ 40/272]  eta: 0:00:56  loss: 3.5484 (3.6479)  acc1: 5.5556 (13.5501)  acc5: 44.4444 (41.3279)  time: 0.1556  data: 0.0003  max mem: 15572
Val:  [ 50/272]  eta: 0:00:50  loss: 3.4365 (3.5464)  acc1: 11.1111 (16.5577)  acc5: 44.4444 (44.7712)  time: 0.1569  data: 0.0004  max mem: 15572
Val:  [ 60/272]  eta: 0:00:46  loss: 2.2325 (3.3560)  acc1: 44.4444 (23.4062)  acc5: 72.2222 (48.2696)  time: 0.1653  data: 0.0004  max mem: 15572
Val:  [ 70/272]  eta: 0:00:42  loss: 2.2944 (3.2428)  acc1: 50.0000 (24.8044)  acc5: 77.7778 (52.1127)  time: 0.1653  data: 0.0004  max mem: 15572
Val:  [ 80/272]  eta: 0:00:39  loss: 2.8645 (3.2454)  acc1: 22.2222 (25.2401)  acc5: 66.6667 (51.7833)  time: 0.1635  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:36  loss: 4.1630 (3.3572)  acc1: 0.0000 (22.5275)  acc5: 16.6667 (47.6801)  time: 0.1688  data: 0.0004  max mem: 15572
Val:  [100/272]  eta: 0:00:33  loss: 4.0769 (3.4237)  acc1: 0.0000 (21.9472)  acc5: 16.6667 (46.6997)  time: 0.1666  data: 0.0005  max mem: 15572
Val:  [110/272]  eta: 0:00:31  loss: 4.0538 (3.4946)  acc1: 0.0000 (20.4705)  acc5: 27.7778 (45.3954)  time: 0.1592  data: 0.0005  max mem: 15572
Val:  [120/272]  eta: 0:00:29  loss: 4.1408 (3.5406)  acc1: 0.0000 (19.3297)  acc5: 27.7778 (44.4444)  time: 0.1653  data: 0.0064  max mem: 15572
Val:  [130/272]  eta: 0:00:26  loss: 3.9542 (3.4888)  acc1: 5.5556 (21.2892)  acc5: 27.7778 (45.5471)  time: 0.1647  data: 0.0063  max mem: 15572
Val:  [140/272]  eta: 0:00:24  loss: 3.1972 (3.4826)  acc1: 33.3333 (22.1040)  acc5: 55.5556 (45.5871)  time: 0.1678  data: 0.0105  max mem: 15572
Val:  [150/272]  eta: 0:00:22  loss: 3.4360 (3.4828)  acc1: 11.1111 (21.5232)  acc5: 50.0000 (46.1001)  time: 0.1742  data: 0.0150  max mem: 15572
Val:  [160/272]  eta: 0:00:20  loss: 3.4298 (3.4597)  acc1: 16.6667 (22.5328)  acc5: 61.1111 (47.4810)  time: 0.1791  data: 0.0215  max mem: 15572
Val:  [170/272]  eta: 0:00:18  loss: 3.7231 (3.4927)  acc1: 11.1111 (21.5400)  acc5: 44.4444 (46.4263)  time: 0.1753  data: 0.0171  max mem: 15572
Val:  [180/272]  eta: 0:00:17  loss: 3.7799 (3.4864)  acc1: 5.5556 (22.0074)  acc5: 38.8889 (47.0534)  time: 0.1716  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:15  loss: 3.9360 (3.5182)  acc1: 0.0000 (20.9715)  acc5: 33.3333 (45.9570)  time: 0.1733  data: 0.0005  max mem: 15572
Val:  [200/272]  eta: 0:00:13  loss: 3.9360 (3.4996)  acc1: 0.0000 (21.3101)  acc5: 33.3333 (47.1531)  time: 0.1720  data: 0.0006  max mem: 15572
Val:  [210/272]  eta: 0:00:11  loss: 2.7911 (3.4897)  acc1: 27.7778 (22.0379)  acc5: 77.7778 (47.9726)  time: 0.1751  data: 0.0006  max mem: 15572
Val:  [220/272]  eta: 0:00:09  loss: 3.1840 (3.4841)  acc1: 27.7778 (22.1468)  acc5: 66.6667 (48.5420)  time: 0.1722  data: 0.0006  max mem: 15572
Val:  [230/272]  eta: 0:00:07  loss: 3.0450 (3.4406)  acc1: 38.8889 (24.0019)  acc5: 66.6667 (49.7836)  time: 0.1716  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:05  loss: 2.7249 (3.4149)  acc1: 44.4444 (24.1586)  acc5: 77.7778 (51.0143)  time: 0.1706  data: 0.0006  max mem: 15572
Val:  [250/272]  eta: 0:00:03  loss: 2.9631 (3.4351)  acc1: 11.1111 (23.6609)  acc5: 72.2222 (50.3984)  time: 0.1680  data: 0.0006  max mem: 15572
Val:  [260/272]  eta: 0:00:02  loss: 2.5481 (3.3521)  acc1: 44.4444 (26.0962)  acc5: 77.7778 (52.0434)  time: 0.1599  data: 0.0004  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.5756 (3.3540)  acc1: 55.5556 (25.7893)  acc5: 83.3333 (52.2755)  time: 0.1471  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.5756 (3.3584)  acc1: 55.5556 (25.7629)  acc5: 83.3333 (52.2425)  time: 0.1400  data: 0.0001  max mem: 15572
Val: Total time: 0:00:48 (0.1792 s / it)
* Acc@1 25.763 Acc@5 52.242 loss 3.358
Accuracy of the network on the 4883 val videos: 25.8%
Max accuracy: 26.44%
Epoch: [27]  [0/1]  eta: 0:00:02  lr: 0.000014  min_lr: 0.000000  loss: 3.3427 (3.3427)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 2.8903  data: 2.5033  max mem: 15572
Epoch: [27] Total time: 0:00:03 (3.0055 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 3.3427 (3.3427)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 7
Indices to remove: tensor([ 1907,  5589,  9002,  9538, 26947, 27213, 27419], device='cuda:0')
length of data loader train is: 0
num_training_steps_per_epoch is: 0
Change step level LR scheduler!
Set warmup steps = 0
Set warmup steps = 0
Traceback (most recent call last):
  File "/home/maggie/VideoMAE_curriculum/run_class_finetuning.py", line 972, in <module>
    main(opts, ds_init)
  File "/home/maggie/VideoMAE_curriculum/run_class_finetuning.py", line 561, in main
    print("Max WD = %.7f, Min WD = %.7f" % (max(wd_schedule_values), min(wd_schedule_values)))
ValueError: max() arg is an empty sequence
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2141395) of binary: /home/maggie/miniconda3/envs/timesformer/bin/python
Traceback (most recent call last):
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_class_finetuning.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-13_13:39:29
  host      : maggie-Tornado-R5Q-iCUE-Certified
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2141395)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
